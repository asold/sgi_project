{
  "title": "Theory of Mind Abilities of Large Language Models in Human-Robot Interaction: An Illusion?",
  "url": "https://openalex.org/W4390832673",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4287069332",
      "name": "Verma, Mudit",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A4288555544",
      "name": "Bhambri, Siddhant",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A4221631779",
      "name": "Kambhampati, Subbarao",
      "affiliations": [
        "Arizona State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1969096617",
    "https://openalex.org/W2202016973",
    "https://openalex.org/W2901232217",
    "https://openalex.org/W4231966580",
    "https://openalex.org/W4381191268",
    "https://openalex.org/W3201919343",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W6630109658",
    "https://openalex.org/W2930317805",
    "https://openalex.org/W6600212061",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4385473956",
    "https://openalex.org/W2779206865",
    "https://openalex.org/W1768754415",
    "https://openalex.org/W3213949538",
    "https://openalex.org/W2914810581",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2139188400",
    "https://openalex.org/W4388623399",
    "https://openalex.org/W4312800247",
    "https://openalex.org/W2964333534",
    "https://openalex.org/W2594908799",
    "https://openalex.org/W4283330306",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2963912425",
    "https://openalex.org/W4240633712",
    "https://openalex.org/W1597935046",
    "https://openalex.org/W58467775",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W1866195740",
    "https://openalex.org/W4399320026",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4366999541",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3157911632",
    "https://openalex.org/W2571600439",
    "https://openalex.org/W2946721894",
    "https://openalex.org/W2955740278",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4306808673",
    "https://openalex.org/W2614653677",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4318718951",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W2293816024",
    "https://openalex.org/W4387075507",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4210787523",
    "https://openalex.org/W4307325444",
    "https://openalex.org/W4288347895",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W1598140581",
    "https://openalex.org/W4287871770",
    "https://openalex.org/W4287186573",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4378474033",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4225675072",
    "https://openalex.org/W2897037347",
    "https://openalex.org/W4321392548",
    "https://openalex.org/W2056198504",
    "https://openalex.org/W2962781380",
    "https://openalex.org/W2615896489"
  ],
  "abstract": "Large Language Models have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example \"Given a robot's behavior X, would the human observer find it explicable?\". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across five domains. A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test. We conclude that, the high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack.",
  "full_text": "Theory of Mind abilities of Large Language Models in\nHuman-Robot Interaction : An Illusion?\nMudit Verma‚àó\nArizona State University\nSchool of Computing and Augmented\nIntelligence\nTempe, USA\nmuditverma@asu.edu\nSiddhant Bhambri‚àó\nArizona State University\nSchool of Computing and Augmented\nIntelligence\nTempe, USA\nsbhambr1@asu.edu\nSubbarao Kambhampati\nArizona State University\nSchool of Computing and Augmented\nIntelligence\nTempe, USA\nrao@asu.edu\nFigure 1: Fetch executes the maneuver to pick the block and takes a step (towards robot‚Äôs) left indicating its choice of goal as\nLoc2 (instead of the alternative Loc3) thereby being legible. Left : Fetch executes the maneuver. Center : Fetch has a label on its\nhead stating \"Not a Legible robot\" in a small font which the human observer is unable to read. Right : Human has a blurred\nview of Fetch making them unable to comment on whether the robot is being legible. Robot expects the LLM (human proxy) to\n\"think what the ‚Äôhuman would think of the robot‚Äôs behavior‚Äô\"\nABSTRACT\nLarge Language Models (LLMs) have shown exceptional generative\nabilities in various natural language and generation tasks. How-\never, possible anthropomorphization and leniency towards failure\ncases have propelled discussions on emergent abilities of LLMs\nespecially on Theory of Mind (ToM) abilities in Large Language\nModels. While several false-belief tests exists to verify the ability\nto infer and maintain mental models of another entity, we study\na special application of ToM abilities that has higher stakes and\npossibly irreversible consequences : Human Robot Interaction. In\nthis work, we explore the task of Perceived Behavior Recognition,\nwhere a robot employs an LLM to assess the robot‚Äôs generated\nbehavior in a manner similar to human observer. We focus on four\nbehavior types, namely - explicable, legible, predictable, and obfus-\ncatory behavior which have been extensively used to synthesize\ninterpretable robot behaviors. The LLMs goal is, therefore to be\na human proxy to the agent, and to answer how a certain agent\nbehavior would be perceived by the human in the loop, for example\n\"Given a robot‚Äôs behavior X, would the human observer find it\nexplicable?\". We conduct a human subject study to verify that the\nusers are able to correctly answer such a question in the curated\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0323-2/24/03. . . $15.00\nhttps://doi.org/10.1145/3610978.3640767\nsituations (robot setting and plan) across five domains. A first anal-\nysis of the belief test yields extremely positive results inflating ones\nexpectations of LLMs possessing ToM abilities. We then propose\nand perform a suite of perturbation tests which breaks this illusion,\ni.e. Inconsistent Belief, Uninformative Context and Conviction Test.\nThe high score of LLMs on vanilla prompts showcases its potential\nuse in HRI settings, however to possess ToM demands invariance\nto trivial or irrelevant perturbations in the context which LLMs\nlack. We report our results on GPT-4 and GPT-3.5-turbo.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies ‚ÜíTheory of mind ; Knowledge\nrepresentation and reasoning ; Planning and scheduling .\nKEYWORDS\nTheory of Mind; Large Language Models; Reasoning\nACM Reference Format:\nMudit Verma, Siddhant Bhambri, and Subbarao Kambhampati. 2024. Theory\nof Mind abilities of Large Language Models in Human-Robot Interaction :\nAn Illusion?. InCompanion of the 2024 ACM/IEEE International Conference on\nHuman-Robot Interaction (HRI ‚Äô24 Companion), March 11‚Äì14, 2024, Boulder,\nCO, USA. ACM, New York, NY, USA, 37 pages. https://doi.org/10.1145/\n3610978.3640767\n1 INTRODUCTION\nAs Artificial Intelligence (AI) progresses, the development of the\nnext generation of AI agents require the ability to interact with\n*These authors contributed equally to this work\narXiv:2401.05302v2  [cs.RO]  17 Jan 2024\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\n(a) Using LLMs as a Human-\nProxy\n (b) User Study Interface\nFigure 2: (a) An LLM being used as a Human-Proxy by a Robot as an internal-critique for behavior synthesis. (b) Interface used\nfor our User Study: Example showing the three questions for Legibility in Fetch Domain.\nhumans in human-like manner, processes and behaviors. A vital\ncomponent of such interaction is the Theory of Mind (ToM), which\ninvolves attributing mental states ‚Äì such as beliefs, intentions, de-\nsires, and emotions ‚Äì to oneself and others, and to understand that\nthese mental states may differ from one‚Äôs own. ToM has exten-\nsive roots in Human-Human interaction and has motivated several\ncritical studies in pursuit of social intelligence [2, 3, 15, 60]. More-\nover, ToM is the corner stone enabling effective communication,\ncollaboration, or deception and becomes a pre-requisite for most of\nhuman-agent interaction [12, 20, 29, 43, 57]. Everyday interactions\nwhich are second nature to human conversations like our ability to\nempathize with a character in a movie or understand social humor\nis in part due to our ability to perform theory of mind and comes\nnaturally to humans [41].\nTheory of Mind becomes all the more important in Human-Robot\nInteraction to facilitate improved behavior synthesis [8, 9, 11, 26‚Äì\n28, 47, 48, 69] and engender Human-Robot trust [14, 65‚Äì68]. Prior\nworks have either assumed a human mental model [9] or learned\nit through interaction [56, 69]. Recent developments in generative\nAI, specifically LLMs may have opened a new way to access ap-\nproximate human mental model to enable robots achieve their HRI\nobjectives in better ways through symbolic [49] or other modalities\n[16, 55, 56]. Initial research hinted at emergent LLM‚Äôs Reasoning,\nPlanning & Theory of Mind abilities [6, 21, 30, 46], however, recent\nworks have refuted such claims [44, 52, 54]. While LLMs may not be\nsound reasoners or perform ToM as humans do, their performance\non existing benchmarks for such tasks certainly exceeds chance\nbehavior and it may find significant use as a Human-Proxy avail-\nable to the robot. Sally-Anne test and its variations that have been\nextensively studied for testing ToM have likely been seen in the\ntraining data for these LLMs. We suspect that LLM‚Äôs approximate\nretrieval is be mistaken for reasoning abilities. If LLMs were indeed\ngood at Theory of Mind, then, as in Fig. 2a, a robot can leverage the\nLLM to perform an internal critique while synthesizing behavior\nwith a human-in-the-loop. The internal critique essentially would\nanswer whether the planned robot behavior would be useful for the\nhuman-in-the-loop. For instance, if the human prefers explicable\nbehavior [69], the robot can query the LLM to check whether a\nplanned behavior would indeed be explicable to the human.\nWhile past works checking for emergent abilities of LLM [6, 21,\n30, 46], even ToM abilities [24] exist, we posit that there is a need for\ninvestigating scenarios specific to HRI as real-world robots function\nin a non-ergodic world with irreversible consequences and much\nhigher stakes. To this end, we investigate PROBE task (Perceived\nRObot Bhavior rEcognition) as a means to understand theory of\nmind abilities of Large Language Models in the context of Human-\nRobot Interaction. For example, the Fetch robot that is navigating\nto a location with a human observer in the loop has to tuck its\narms and crouch before moving1. The lay-observer unfamiliar with\nthe underlying dynamics of the robot may find the actions \"tuck\n& crouch\" unnecessary and inexplicable. In our investigation, the\nrobot employs an LLM to ask the test question \"Given behavior X,\nwould the human observer find such a plan explicable?\".\nWe study four robot behaviors which have been extensively\ndiscussed in HRI and mental modelling, namely - explicable, legi-\nble, predictable and obfuscatory behavior (referred to as ‚Äúbehavior\ntypes\"). We leverage the rich literature along these behavior types\nand borrow five domains used in prior works and valuable to the\nHRI community, namely - Fetch, Passage Gridworld, Environment\nDesign, Urban Search and Rescue, and Package Delivery. For each\ndomain we construct four distinct situations corresponding to each\nbehavior type. We construct a prompt (can be automated) using\nrelevant context like description of the domain, robot capabilities\n& goals, and a definition of the behavior type. The LLM is asked a\nbinary question (Q1) on whether the human observer would find\nthe agent plan to be explicable (or the corresponding behavior type)\nand a multi choice question (Q2). In our evaluation, upon correct\nanswer to (Q1), the LLM is tasked with choosing one explanation\nfrom a set of options, explaining why the plan conforms to the\npredicted behavior type (Q2). As a baseline, we first test LLM on\nvanilla prompts : success on which is indicative of potential ToM\nabilities but may also inflate one‚Äôs expectations of LLMs. Second, we\npropose three tests of which two are perturbation tests as Inconsis-\ntent Belief and Uninformative Context, and a Conviction Test. Our\nsecond suite of tests elicit important conclusions on ToM abilities\nof LLMs and essentially break the supposed illusion.\nWe highlight our contributions as follows :\n1. We conduct a first study of Theory of Mind abilities of Large\nLanguage Models in the context of interpretable behavior synthesis\nand Human-Robot Interaction.\n1Project Link & Supplementary : https://famishedrover.github.io/papers/llm-probe/\nindex.html\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\n2. We investigate along five domains and four behavior types\nwhich have been well motivated with respect to the need for mental\nmodeling in HRI and curate a total of 20 situations.\n3. We perform a user subject study to validate our curated situa-\ntions as lay users are able to correctly answer ToM queries.\n4. We propose second-order tests including two perturbation\ntests and the Conviction test which highlights that any ToM abilities\nof LLMs are illusionary.\n5. We perform a user subject study to validate that humans\nobservers observing a real robot (Fetch) will be unaffected by per-\nturbations while answering ToM queries.\n2 RELATED WORK\nLarge Language Models : Large Language Models [70] are pre-\ntrained generative AI models that can take a piece of text as an\ninput and generate text as an output. Typically, the input space\nis unconstrained, meaning a finite yet large set of tokens / text is\na valid input for the GPT like Large Language Model. While this\nallows for arbitrary tasks to be framed as a linguistic query for\nthe LLM, it is unclear whether data-driven next-word prediction\nstyle of LLMs [1, 13, 37, 62] training is reason enough to believe\nthat LLMs would perform well on said arbitrary tasks. Over the\npast years several variants and versions of Large Language Models\nhave been released such as the GPT family [ 37], LLAMA family\n[51], PALM family [1, 13], BLOOM [62] and several others [70]. In\nthis work we conduct our study with GPT-4 [37] as it is one of the\nstrongest LLMs in terms of NLP task performance.\nTheory of Mind: Theory of Mind has been a challenging, yet,\never-interesting topic for researchers [17, 33, 53, 64]. It allows for\neffective communication between agents especially with a human\nin the loop. While humans develop it naturally, ToM is considered\nto be one of the most challenging goals for an AI agent [41]. In this\nwork, we are specifically interested in studying ToM for human-\nrobot interaction for behavior synthesis that involves extensive\nmodeling of the human in the loop, reasoning on those models and\nsynthesise behavior that conforms to the human mental models.\nLLMs, Theory of Mind and Emergent abilities: LLMs have\namassed a large popularity with public and the research community\nwith its exceptional abilities in NLP tasks. With an AI as impressive\nas an LLM, researchers have explored the possibilities of emergent\nabilities of these LLMs. While this is an active area of research,\nof interest to the scope of this work are LLMs abilities in reason-\ning [23, 58, 59], deception abilities [18], Theory of Mind abilities\n[25] and other emergent behaviors that are similar to human be-\nhaviors [7, 40, 42, 50, 63]. While these works highlight how LLMs\npossess these abilities, more recent works have refuted these claims\n[5, 22, 35, 45, 52, 54] citing that LLMs are extremely brittle and\nheavilty depend on the prompt fed to it. [ 54] suggests, systems\nthat reason do not have to depend upon such syntactic alterations\nto the input like Chain of Thought Prompts, Tree of Thoughts or\nother prompting strategies [ 31]. While ToM has extensive prior\nworks on specialized robot agents [4, 34, 39] these works are not\ndirectly applicable to investigating the emergent ToM abilities of\nLLMs. We distinguish ourselves from past works through our novel\nconstruction of prompt situations which are sourced from the vast\nliterature on behavior synthesis, the situations are well rooted for\nHRI scenarios, provide a thorough analysis via a large-scale user\nsubject study and provide a case study with real Fetch robot in HRI.\n3 PRELIMINARIES\nA core focus of this work is to analyze the Theory of Mind abilities\nof Large Language models in Human-AI interaction scenario. As\nmotivated previously, we identify that Human-Aware Planning is an\nimportant part of HRI that attempts to synthesize agent behaviors\nby taking human mental model into account. [9, 48] provides a good\noverview of the literature and formal specifications of behavior\ntypes we consider in this work. [9, 26] papers reconciles the various\ninterpretations of terminologies used in the literature and highlights\nimportant connections between related works. Subsequently, they\nprovide formal definitions of the four behavior types (explicability,\npredictability, legibility and obfuscation) as follows :\n(1) Explicability [10, 47, 69]: Explicability measures how close a\nplan is to the expectations of the observer, given a known\ngoal.\n(2) Legibility : Plan legibility reduces ambiguity over possible\ngoals that are being achieved.\n(3) Predictability : Plan predictability reduces ambiguity over\npossible plans, given a goal.\n(4) Obfuscation (Goal-Obfuscation) [26] : A plan is an obfusca-\ntory behavior if it k-ambiguous. That is, the plan is consistent\nwith at least ùëò goals with a true goal of the agent and ùëò‚àí1\nare decoy goals meant to deceive the human observer.\nEach of these behavior types correspond to utilize the human men-\ntal model to synthesize a behavior to achieve some target. For\nexample, explicability utilizes the human mental model and forces\nthe robot to conform to the human model. Not only the robot must\nrealize which behavior type it should conform to (even though\nthese types are not mutually exclusive, they can be competing as\npointed by [9]), but also inherently require the human‚Äôs mental\nmodel. Further, it needs to be able to reason with the human mental\nmodel to finally synthesize the expected behavior. This shows that\nbehavior synthesis not only requires Theory of Mind abilities but\nalso require reasoning abilities (i.e. performing inference over the\nmental models). There exists a large body of work that study the\nvariations among and behavior types but we argue that showcasing\nLLM‚Äôs failure at ToM on these canonical types is sufficient (as prob-\nlems studying a cross between behavior types require the agent to\noptimize over multiple, possibly competing, objectives).\n4 METHODOLOGY\nThrough this work, we answer the following three questions in the\ncontext of mental-model reasoning abilities in Human-AI Interac-\ntion scenarios for behavior synthesis :\nResearch Question 1: How do human subjects 2 perform on ToM\nreasoning tasks in Human-Robot Interaction scenarios?\nResearch Question 2: How much does LLM performance align\nwith that of: 1) oracle, & 2) human subjects?\nResearch Question 3: How robust are LLMs in their (perceived)\nmental-model reasoning abilities?\n2We refer to lay users as human subjects in the rest of the paper.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nWe first construct a set of ToM tasks given as \"situations\" across\nfive domains. A situation is described as an ordered tuple S =\n(ùëÖùëë,ùê∫,ùêµ ùëë,ùê∂,ùëÉ )where ùëÖùëë is the description of the domain, robot\ncapabilities and actions. ùê∫ is the description of goals, ùêµùëë describes\nthe behavior-type, ùê∂ describes any additional constraints (like par-\ntial observability) and ùëÉ is optional and adds a perturbation text\n(Uninformative Context or Inconsistent Belief). Following each sit-\nuation we query two questions, Q1 : a binary yes/no question on\nwhether the human observer perceives the agent behavior accord-\ning to a behavior-type, and Q2 : to select an explanation for why\nthe said behavior-type is a valid answer. A prompt is composed of\nthe tuple and the two questions. We first validate our constructed\nprompts and obtain human baseline performance. We then query\nthe prompts to GPT-X family, i.e. GPT-4 and GPT-3.5-turbo. We\nnote that queries to GPT-X family imply an expectation of second\norder ToM abilities which implies we expect the LLM to \"think what\nthe ‚Äôhuman would think of the robot‚Äôs behavior‚Äô\". As discussed in\nSection 1, such an ability can be leveraged by the robot to use LLMs\nas a human-proxy and improve its behavior synthesis process.\nAs discussed in Sec. 6, the performance of LLMs on the vanilla\nprompts (where ùëÉ = ‚àÖ) engenders an inflated belief that LLMs\nmay possess ToM abilities. We then investigate the robustness of\nLLM responses with two perturbation cases and a Conviction test.\nUnlike how ToM agents (like humans) behave [2, 3, 15, 52, 60](see\nSec. 7), we find that LLMs are extremely brittle to small changes\nto prompt and fail miserably. Finally, we present a case study with\nFetch robot where a human observes the robot acting in the world\nalong the three cases Vanilla prompt, Uninformative Context and\nInconsistent Belief. For all our experiments we use the most recent\nversion of GPT-4 (as of September 30, 2023) and GPT-3.5-turbo\n(as of November 25, 2023). Past works have justified their general\nsuperiority amongst the popular LLMs and within the GPT-X family\n[37] motivating their use as representative LLMs.\n4.1 Prompt Construction\nFor each domain we create four situations corresponding to each\nbehavior-type. In each situation there is a robot acting in the world\nand a human observer. Based on the human‚Äôs mental model they\nmay find the robot behavior to be explicable, legible, predictable\nor obfuscatory. We write one prompt for each situation (i.e. Do-\nmain √óBehavior-Type). The prompt is divided into sections as :\na description of the robot, its actions, the goal, definition of the\nbehavior type we wish to have the LLM answer about, and the plan\nproposed / executed by the robot. Following which we add our ToM\ntest question (Q1) whether the plan corresponds to the behavior\ntype (as a binary answer) and (Q2) the reason for the answer to Q1.\n4.2 Human Subject Study on ToM-PROBE task\nIn reference to answering research question 1, we describe our\nuser study for evaluating the alignment between LLM and human\nresponses. We evaluate four agent behaviors, namely - explicabil-\nity, legibility, predictability and obfuscation, across five domains\ndiscussed in detail in Section 5. The study protocol was approved\nby our local Institutional Review Board (IRB) and refined through\npilot studies over a small group of participants.\nTable 1: Study-1 Participant Demographics by Test Domain.\nTest Domain N #Male #Female #Other Avg. Age (SD)\nFetch 20 8 12 - 35.25 (11.77)\nPassage 20 6 14 - 31.90 (11.83)\nEnv. Design 20 6 13 1 33.35 (8.96)\nUSAR 20 7 12 1 32.45 (8.43)\nPackage D. 20 6 14 - 32.85 (9.70)\nFetch-video (Sec 7) 20 5 15 - 33.75 (8.94)\nTotal 120 38 80 2 33.26 (9.88)\n4.2.1 Question Design. For answering RQ2, we designed the first\nquestion as a binary Yes/No response on the agent behavior con-\nforming to the given behavior type. The second question was a\nMultiple Choice Question (MCQ) asking the participants to choose\nthe correct reason behind the agent behavior. In Q3, we presented\nthe participants with GPT-4‚Äôs response when given the same prompt\ncomprising of the domain description and the first question (see,\nvignettes in Supplementary). For the last question, participants an-\nswered on a Likert Scale from 1 (Strongly Disagree ) to 5 (Strongly\nAgree) with the LLM‚Äôs reasoning on the agent behavior. Our study\nhad a mixed design: Test Domains were varied between-subjects,\nand response ordering was varied within-subjects for the first two\nquestions.\n4.2.2 Setup & Procedure. The study used the Prolific platform [38],\ndirecting participants to a Google Form for responses. Participants\ncould use any device without requiring speaker, camera, micro-\nphone, or special software for the study. First, they were presented\nwith the data confidentiality statement, followed questions collect-\ning their demographic details, and finally provided with the instruc-\ntions and overview of the entire study which consisted of the four\nparts comprising three questions each. The study approximately\ntook 6 minutes to complete, and participants were compensated at\nthe rate of $9/hour.\n4.2.3 Participants. We recruited 125 participants (between the ages\nof 18 and 60). We conducted a pilot study on 25 participants spread\nacross each of the five evaluation domains. The final study, refined\nusing the pilot study responses, had a sample size of 100 participants\nwith 20 participants for each of the five domains listed in Section\n5 (see Table 1). Most participants fell in the age range of 21 to 34\nyears, and had an undergraduate degree. Participants reported a\nmean familiarity of 3.07 and standard deviation of 0.36 to Machine\nLearning and Artificial Intelligence tools/technologies, on a 7-point\nLikert Scale with 1 corresponding to being least familiar.\n4.3 Testing LLMs for Robustness\nTo answer research question 3, we design a set of adversarial per-\nturbation strategies for testing the robustness of LLM responses in\nmental-model reasoning tasks. As pointed by [52], mental-model\nreasoning should be unaffected by perturbations, commonly re-\nferred to as deception strategies in cognitive development studies\n[19, 32, 61]. In the case of LLMs where the said reasoning tasks are\npresented as text prompts, we design the following perturbation\nstrategies (see 1 and 2 for example prompts), ensuring that the final\nexpected reasoning responses are invariant to the perturbations\nintroduced by each of the two cases:\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nFigure 3: An illustrative view of the domains used for testing LLM for Theory of Mind reasoning on the Perceived Behavior\nRecognition task. Left to Right: Fetch Robot Domain, Passage Gridworld, Environment Design, Urban Search and Rescue, and\nPackage Delivery.\nPerturb 1: Fetch - Legibility + Uninformative Context\nDescription: Fetch is a robot that can carry objects from one location to another. There\nare three locations: loc1, loc2, and loc3 where the robot can go.\nThere is a block b1 at location loc1, and the robot is at location loc1 and has its hand\nempty. Location loc2 is to the left of loc1, and location loc3 is to the right of loc1.\nGoals: The robot has to take the block b1 to either loc2 OR loc3 (only one of these\nlocations).\nDefinition : A partial plan is a part of robot‚Äôs behavior, for example a few actions that\nit takes.\nDefinition : A partial plan is legible if the observer (you) can identify which goal the\nrobot wants to go for. A partial plan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and takes one step left.\nThe robot has a label saying ‚ÄôNot a Legible Agent‚Äô. The human observer cannot read.\nThe human observer looks at the label. The human observer has not seen the robot\nbefore.\nQuestion 1 : Imagine you are the human observer. Would you find such a partial plan\nlegible? Give your answer as ‚ÄôYes‚Äô or ‚ÄôNo‚Äô only.\nPerturb 2: Fetch - Legibility + Inconsistent Belief\nDescription: Fetch is a robot that can carry objects from one location to another. There\nare three locations: loc1, loc2, and loc3 where the robot can go.\nThere is a block b1 at location loc1, and the robot is at location loc1 and has its hand\nempty. Location loc2 is to the left of loc1, and location loc3 is to the right of loc1.\nGoals: The robot has to take the block b1 to either loc2 OR loc3 (only one of these\nlocations).\nThe human observer can not see the robot acting in the environment.\nDefinition : A partial plan is a part of robot‚Äôs behavior, for example a few actions that\nit takes.\nDefinition : A partial plan is legible if the observer (you) can identify which goal the\nrobot wants to go for. A partial plan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and takes one step left.\nQuestion 1 : Imagine you are the human observer. Would you find such a partial plan\nlegible? Give your answer as ‚ÄôYes‚Äô, ‚ÄôNo‚Äô, or ‚ÄôCan‚Äôt say‚Äô only.\nUninformative Context : Inspired by the robustness testing\nin [52], we add an additional context to each of the LLM prompts.\nThis additional context can be understood as a self-contained pair\nof clauses evaluating to False, and hence, having a null effect to the\nreasoning expected by the task prompt.\nInconsistent Belief: An essential component to checking whether\nmental-model reasoning abilities exist, is by including an additional\ncontext which ensures that the correct response can only be com-\nputed through semantic reasoning. Specifically, we include the\nsentence ‚ÄúThe human observer can not see the robot acting in the\nenvironment. \"in each prompt. We modify the options for the first\nbinary response question to include Can‚Äôt Say (the correct answer)\nalong with Yes/No. We also modify Q2 to include the correct reason\nstating the inability of the human observer to see and understand\nthe agent behavior.\nConviction Test : For a system that exhibits mental-model\nreasoning abilities, we would expect the system to consistently pick\nthe same response (ideally, the correct response) regardless of the\nrepeated queries. In the case of GPT-X family, the temperature (orùúè)\nparameter controls the stochastic nature of the queried responses.\nIt can be varied in the range of 0 (least)-2 (most) stochasticity\ninfluencing the LLM outputs on each iteration.\n5 EVALUATION DOMAINS\nWe used the following five domains (Fig. 3) for our investigation:\nFetch Robot Fetch is a robot that can carry objects (pick / place)\nand move from one location to another. The situations require the\nrobot to pick the block and navigate to (one of) the goal location.\nPrompt 1: Fetch - Legibility\nDescription: Fetch is a robot that can carry objects from one location to another. There\nare three locations: loc1, loc2, and loc3 where the robot can go.\nThere is a block b1 at location loc1, and the robot is at location loc1 and has its hand\nempty. Location loc2 is to the left of loc1, and location loc3 is to the right of loc1.\nGoals: The robot has to take the block b1 to either loc2 OR loc3 (only one of these\nlocations).\nDefinition : A partial plan is a part of robot‚Äôs behavior, for example a few actions that\nit takes.\nDefinition : A partial plan is legible if the observer (you) can identify which goal the\nrobot wants to go for. A partial plan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and takes one step left.\nQuestion 1 : Would you find such a partial plan legible? Give your answer as ‚ÄòYes‚Äô or\n‚ÄòNo‚Äô only.\nPassage Gridworld: In this domain, a rover which exists in a\ngridworld domain with possible blockades that it must navigate\nthrough to reach its goal location.\nPrompt 2: Passage Gridworld - Explicability\nDescription: Consider a 4x4 square grid with each cell numbered as (row, column), the\nrobot needs to travel from top left cell 1 (1,1) to its goal at bottom right cell 15 (4,3). The\nhuman observer (you) expects the robot to take the shortest path by going DOWN 3\nsteps to row 4 (reach 4, 1), and then RIGHT 2 steps to column 3 (reach 4,3).\nConstraint: There are blockades in columns 1, 2 and 3, that the human observer (you)\ndo not know of, but the robot does.\nDefinition : Plan Explicability means whether the plan / robot behavior is an expected\nbehavior according to the human observer (you). If you look at the robot behavior and\nfind that some actions are unnecessary or not required, then the behavior is inexplicable.\nPlan: The robot goes RIGHT 3 steps in row 1 (reach 1,4), goes DOWN 3 steps in column\n4 (reach 4,4), and LEFT 1 step in row 4 (4,3).\nQuestion 1: Imagine you are the human observer, would you find such a plan explicable?\nGive your answer as ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\nEnvironment Design Gridworld: The environment design\ndomain tests LLMs on the design choices of a gridworld environ-\nment that correspond to an agent‚Äôs explicable behavior (and for\nother three behavior types).\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nPrompt 3: Environment Design - Obfuscation\nDescription: There is a 3x3 square grid numbered as (row, column) = (1,1) the bottom\nleft cell and (3,3) is the top right cell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The robot cannot go through cells\nthat have an obstacle. The robot can go UP, DOWN, LEFT or RIGHT. Finally, there may\nbe objects placed in one or more cells, and the agent will incur a very high cost on\nvisiting these two cells.\nSuppose there is only one instantiation of this grid based on how these obstacles are\nplaced in the environment:\nSetup A : No obstacles.\nGoals : The robot needs to find a plan to reach one of the two goals, G1 and G2.\nDefinition: An environment is designed for obfuscation when all the plan completions\nare equally worse for all the agent goals. This is useful when the agent wants to achieve\na certain goal say G1 but does not want the observer (you) to realize which among set\nof possible goals it wants to achieve. An environment designed for obfuscations allows\nfor plans that lets the agent hide which goal it wants to achieve for as long as possible.\nQuestion 1 : For the setup there are multiple plans possible, for example to goto cells\n(1,1) RIGHT (1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nAnother shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2). Do you think that the\nenvironment is designed for obfuscation? Give your answer only as Yes or No.\nPackage Delivery: The Package Delivery domain presents yet\nanother complex scenario where a human is observing an agent\nacting in a partially observable port management scenario where.\nPrompt 4: Package Delivery - Obfuscation\nDescription: Consider a situation where there is a robot which manages a shipping\nport, and a human observer (you) who is the supervisor that has sensors or subordinates\nat the port who provide partial information about the nature of activity being carried\nout at the port. For instance, when a specific crate is loaded onto the ship, the observer\nfinds out about the identity of the loaded crate. The observer knows the initial inventory\nat the port, but when new cargo is acquired by the port, the observer‚Äôs sensors reveal\nonly that more cargo was received; they do not specify the numbers or identities of the\nreceived crates.\nInitial state: There are packages at the port.\nGoal: The robot can either pick and acquire a package on the port, or else pick and load\nit on the ship.\nDefinition: Suppose you think the robot is trying to achieve one out of a set of of\npotential goals. If the agent‚Äôs behavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to achieve one of A, B, C. If it\nshows a behavior (partial plan) but you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the package and holds it between the port and the\nship.\nQuestion 1 : Would you find such a partial plan obfuscatory? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\nUSAR: Urban Search And Rescue is a popular domain where\nthe robot and a human observer (commander) perform triage and\nrescue operations. The robot uses the LLM to query how a certain\nplan will be perceived by the commander.\nPrompt 5: USAR - Predictability\nDescription : In a typical Urban Search and Rescue (USAR) setting, there is a build-\ning with interconnected rooms and hallways. There is a human commander CommX,\nand a robot agent acting in the environment. Both the agents can move around and\npickup/drop-off or handover med-kits to each other. CommX can only interact with\nmed-kits light in weight, but the robot agent can interact with heavy med-kits too.\nInitial State : There is a medkit located in the middle of the hallway, and there are two\npaths to a patient room from there (one on the LEFT, and one on the RIGHT).\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and take it to the patient room.\nDefinition: A partial plan A is predictable if the observer (you) can identify if there is\none possible completion (which may or may not lead to the goal). A partial plan A is\nmore predictable than a partial plan B if the number of possible completions of A is less\nthan B.\nPlan : In the robot‚Äôs partial plan, it picks up the medkit, and takes one step LEFT.\nQuestion 1 : Would you find such a partial plan predictable? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n6 RESULTS\nWe breakdown our discussion on the results to answer the three\nresearch questions, as mentioned in Section 4. In summary, we\nbegin with evaluating human subjects performance on our five\nevaluation domains in 6.1, which validates our evaluation prompts,\nand also acts as a baseline when comparing LLM performance in\n6.2. Finally, we shift our focus to testing the robustness of LLMs\nwith an in-depth analysis of their failure modes in 6.3.\n6.1 RQ1 : Human Subjects on ToM-PROBE task\nTo validate our text prompts querying for Theory of Mind abilities\nand to establish a baseline, we report the user response average ac-\ncuracy and standard deviation for the binary response question (Q1)\nand the MCQ response question (Q2), across five domains in Figure\n4 (c.f. Table 2 in Appendix for more details). We use the same text as\nvignettes shown in Section 5, as LLM prompts. To answer our RQ1,\nwe note that users were correctly able to answer both questions for\neach of the five domains with a minimum average accuracy of 69%,\nexcept for the case of Package Delivery domain where the accu-\nracy was relatively lower in Legibility and Predictability behavior\nprompts. We recall that Q2 conveys the correct answer (regardless\nof the user response to Q1) and we request them to \"rethink and\nselect an explanation\". A much higher number of users could cor-\nrectly identify the reasoning choice in Q2, including users who had\nincorrectly answered Q1 (see Fig. 4b).\n(a) Q1 - Binary Response\n (b) Q2 - MCQ Response\nFigure 4: Human subject performance across five domains.\n6.2 RQ2: LLMs Performance Alignment\n6.2.1 Performance Comparison : To answer our second research\nquestion (RQ2), we begin with comparing the performance accuracy\nof GPT-4 and GPT-3.5-turbo with human subjects and oracle, as\nshown in Figure 5. To our expectations, GPT-4 performs better than\nGPT-3.5-turbo in all but one behavior type, i.e., for predictability\nwhere their accuracies are equal.\n6.2.2 Kolmogorov-Smirnov (KS) Test : For further evaluating the\nalignment between LLM responses andhuman subjects, we perform\nthe KS test between the two populations of (scaled) user responses\nand GPT-4 responses with the null hypothesis ùêª0: the two popu-\nlations belong to the same distribution . For a KS test with sample\nsize 20, the critical value for significance level 0.01 is 0.356, and for\nsignificance level 0.05 is 0.294. We observe that the observed critical\nvalue is 0.04, and hence, we accept the null hypothesis. We also run\nthe KS test between GPT-4 responses and the ground truth results,\nand obtain 0.3 as the observed critical value at which ùêª0 can only\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nFigure 5: Performance on Q1 across five domains along four\nrobot behavior types on Q1 (binary response). Human sub-\njects‚Äô results have been scaled for a uniform comparison.\nFigure 6: Likert Score (1-5) comparison for subjective evalua-\ntion of LLM responses.\nFigure 7: Performance accuracy of GPT-4 showcasing failure\nmodes in perturbation tests. Each bar denotes the correct\nanswers for Q1 across five domains along behavior types.\nbe accepted for significance level 0.05 and not for significance level\n0.01. These tests have been run using the GPT-4 results obtained\nwith the hyperparameter ùúè (temperature) set to 0. With ùúè = 1, we\nobtain the similar results with the observed critical values 0.07 and\n0.35 in the two respective cases. Hence, we conclude from these re-\nsults that GPT-4‚Äôs performance aligns with that of human subjects\nas compared to the case of oracles or ground truth.\n6.2.3 Subjective Response Evaluation: Additionally, for Q3 of in our\nuser study, we record the average scores for human evaluations on\nLLM descriptive reasoning responses. These ratings are in the [1-5]\nLikert scale where 1 implies users strongly disgree and at 5 users\nstrongly agree with the GPT-4‚Äôs explanation. We find that GPT-4\ndescriptive responses to vanilla prompt are generally agreed by the\nusers (with average Likert rating of Explicability : 3.47, Legibility :\n4.23, Predictability : 4.0, Obfuscation : 4.08) further bolstering the\n(illusion) belief that LLMs are in fact performing second order ToM\n(c.f. Figure 12 in the Appendix for more details). For completeness,\nwe use an automated method of comparing the correct explanation\nin Q2 and descriptive explanation by GPT-4 by using gpt-3.5\nas a semantic text similarity tool (see prompt 6 in Appendix for\nFigure 8: Conviction Test Results - Performance distribution\nof LLMs across behavior types when prompted 10 indepen-\ndent times with ùúè = 2 (see Appendix for ùúè = 0 & ùúè = 1).\nmore details) in Figure 6, with average ratings of Explicability : 3.4,\nLegibility : 4, Predictability : 3.8, Obfuscation : 3.4.\n6.3 RQ3: Failure modes of LLMs in ToM-PROBE\nWhile RQ1 and RQ2 yield extremely motivating and positive re-\nsults in favor of GPT-4 showing ToM abilities, we clarify that our\nRQ3 breaks this illusion. Revisiting our research question 3 to test\nLLM robustness for correctly answering Theory-of-Mind reasoning\nquestions, we report the results for GPT-4‚Äôs performance on the\nbinary question for all behavior types across the five evaluation\ndomains in Figure 7.\n6.3.1 Uninformative Context. In the case of our first perturbation\n(see Prompt 1), we note a drop in performance across all four behav-\nior types. Note that past works [52] have considered even a single\nincorrect response on this perturbation test enough to disregard\nany emergent abilities of LLM.\n6.3.2 Inconsistent Belief. In the case of the second perturbation\n(see Prompt 2), GPT-4 is unable to get even a single correct response\nin all but one out of the 20 cases. This should not be surprising as\nprior work has already shown that LLMs cannot reason [54]. The\nInconsistent Belief Tests checks relies on inferring the correct men-\ntal state which in-turn requires reasoning over given information\nin the prompt.\n6.3.3 Conviction Test. While it would be expected for a true ToM\nreasoner that multiple queries of the same prompt results in the\nsame answer, we find that GPT-4 does not remain consistent with its\nresponses even when they are incorrect. The LLM keeps switching\nanswers between ‚ÄúYes/No\" for Q1 when sampled 10 times under\nùúè = 2, as shown in Figure 8. Note, that we would ideally expect\nto have the same answer, irrespective of the correctness and the\nnumber of times the LLM is queried. Furthermore, we note that\nwhile GPT-4 is inconsistent in its responses, GPT-3.5-turbo performs\nmuch worse and gives irrelevant responses. For an automated agent\nwhich may utilize a language model for querying for ToM problems,\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nsuch inconsistent and irrelevant responses could lead to unreliable\nperformance, thereby impacting the human-robot interaction.\nAlthough the results from vanilla prompts and the user study\nbuild the illusion of ToM abilities in LLMs, we conclude that our\nperturbation experiments are sufficient to clarify that ToM abilities\nare absent from LLMs. ToM evaluation must entail evaluation of\nreasoning abilities with minimal leniency towards failure cases.\n7 CASE STUDY\nIn our previous discussions, the prompts queried to LLMs and the\nuser study were solely text based as LLMs like GPT4 and GPT3.5\nare limited to textual input. Specifically, the robot has to convert\nits plan into a human-interpretable text (such as natural language\nor more formal PDDL syntax) which is shown to the user subjects.\nWe extend our arguments along an additional modality where user\nsubjects observe a real robot acting in the world. We consider the\ncase of the Fetch Robot‚Äôs legibility prompt for this case study. We\nanswer the following research question :\nExtended Research Question 1: How do human subjects perform\non HRI ToM-PROBE when observing a real robot?\n7.1 Study design & Procedure\nWe use the Fetch robot to execute maneuvers as described in Fetch-\nLegibility prompt 1. The robot picks up a block on the table and\ntakes a step left. This partial plan implicitly indicates the robot‚Äôs\nchoice of going to the goal on left (instead of the goal on the right)\nmaking the partial plan legible. The user subjects have to answer\n(Q1) and (Q2). Further, we consider the case of the two perturba-\ntions proposed in section 4.3 where in Uninformative Context test\nthe Fetch robot has a label saying \"Not a Legible robot\" but the\nhuman cannot read (operationalized by a attaching a label in small\nfont so the subjects are not able to read) and the Inconsistent Belief\ntest where the human observer is unable to see the robot (opera-\ntionalized by mosaic blurring of the robot). The users answer (Q1)\nfor the two perturbation tests. They use the Google Form based\ninterface with access to a video showing the Fetch robot maneuver\nin addition to the text as before. We recruit 20 new participants on\nProlific with the same requirements as in section 4.2.3. Participant‚Äôs\ndemographics details can be found in Table 1.\n7.2 Results\nWe ask the human subjects whether they can read the text on\nthe robot‚Äôs head (Fig 1:center) or see the robot acting (Fig 1:right)\nas a binary question. 100% users responded they could not read\nthe text and 95% could not see the robot acting which validates\nour operationalization of the perturbation. GPT4-Vision [ 36] is\nalso fed with image frames obtained via stratified sampling (step\nsize ùëò = {6,12,24,30}) from the video along with the text prompt.\nAcross all sampling step sizes, the LLM always responds that it can-\nnot read the text and cannot see the robot acting in the respective\ncases. We find that the user subject responses and accuracy (90%\nusers correctly identified the behavior-type as legible and 85% users\ngave correct answer to Q2) are consistent with Fetch-Legibility\ntext-only case which establishes our baseline that human users\ncan answer ToM queries for the constructed vanilla situation. We\nfind that the users are not affected by the Uninformative Context\n(85% users correctly identified behavior-type and 95% users chose\nthe correct explanation in Q2) and Inconsistent Belief perturba-\ntions (75% correctly opted for ‚ÄúCan‚Äôt Say\" choice and 25% chose\n‚ÄúNo\" option). The high accuracy values between vanilla case and\nperturbation case shows that users were not affected by pertur-\nbations. GPT4-V correctly identifies the behavior-type along with\nthe correct reasoning in the vanilla case, however, it fails in the\nInconsistent Belief test and the Conviction test. We note that, while\nGPT4-V answers that it cannot see the robot acting, it still fails Q1\nwhich also reflects inconsistent reasoning abilities. More details\ncan be found in Section A.3.\n8 CONCLUSION & FUTURE WORK\nIn this work we perform a critical investigation of the theory of\nmind (ToM) abilities of Large Language Models specifically de-\nsigned for Human Robot Interaction. We consider the second order\nToM setup where a robot can query an LLM (acting as a human\nproxy) to \"think how a human observer would perceive the robot‚Äôs\nbehavior\" which can be further utilizes to improve behavior syn-\nthesis. We leverage prior works and identify key behavior-types\ncritical for HRI and behavior synthesis - explicability, legibility,\npredictability and obfuscatory behavior. We use prior works to\nborrow five HRI domains for these behavior types and construct\nvarious situations where the robot behavior belongs to these behav-\nior types. Each of these situations (domain description paired with\nrobot behavior) is given as context to answer two questions. First,\nto recognize whether the human observing the robot would find\nits behavior to be of a given behavior type. Second, we ask for an\nobjective explanation (by choosing among a set of choices) for their\nanswer to the first question. We perform a human subject study to\nobtain lay-user performance measure on these tasks and compare\nit to LLM performance (on GPT-4 and GPT-3.5-turbo). While the\nLLM‚Äôs performance on the first set of experiments is much better\nthan chance behavior, our experiments testing LLMs robustness\nand conviction highlight that LLMs, while a useful tool for HRI,\nare not robust ToM agents. Specifically, we find that LLMs fail in\nour perturbation tests, i.e. Inconsistent Belief and Uninformative\nContext test and do not possess conviction in its responses. Finally,\nwe provide a case study with Fetch Robot to emphasize that the\nsituations considered in our work are translations of real-world\nobjectives that robot‚Äôs may have with human observer in the loop,\nand bolster existing belief that humans are robust towards ToM\nperturbations that we consider.\nOur work provides a first analysis of LLM ToM abilities for\nan HRI setting and opens up research opportunities in studying\nthe various facets of ToM with LLMs. While future generations of\nLLMs may improve on vanilla ToM tasks, it becomes every more\nimportant to identify and study other crucial failure modes to realize\nwhether the LLM responses are retrieval based or reasoning based.\nWe hope that our contribution can bring forth the HRI community\nto realize the impact, potential benefits and cautions of utilizing\nLarge Language Models in HRI setting.\nACKNOWLEDGEMENT\nThis research is supported in part by ONR grants N00014-18-1-2442,\nN14-18-1-2840 and N00014-23-1-2409.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nREFERENCES\n[1] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,\nAlexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\net al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).\n[2] Janet Wilde Astington and Jodie A Baird. 2005. Why language matters for theory\nof mind . Oxford University Press.\n[3] Janet Wilde Astington and Jennifer M Jenkins. 1995. Theory of mind development\nand social understanding. Cognition & Emotion 9, 2-3 (1995), 151‚Äì165.\n[4] Chris Baker, Rebecca Saxe, and Joshua Tenenbaum. 2011. Bayesian theory of mind:\nModeling joint belief-desire attribution. In Proceedings of the annual meeting of\nthe cognitive science society , Vol. 33.\n[5] Ali Borji. 2023. A categorical archive of chatgpt failures. arXiv preprint\narXiv:2302.03494 (2023).\n[6] S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712 (2023).\n[7] S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712 (2023).\n[8] Tathagata Chakraborti, Gordon Briggs, Kartik Talamadupula, Yu Zhang, Matthias\nScheutz, David Smith, and Subbarao Kambhampati. 2015. Planning for serendipity.\nIn 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) .\nIEEE, 5300‚Äì5306.\n[9] Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David E. Smith,\nand Subbarao Kambhampati. 2018. Explicability? Legibility? Predictability?\nTransparency? Privacy? Security? The Emerging Landscape of Interpretable\nAgent Behavior. arXiv:1811.09722 [cs.AI]\n[10] Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David E Smith,\nand Subbarao Kambhampati. 2019. Explicability? legibility? predictability? trans-\nparency? privacy? security? the emerging landscape of interpretable agent be-\nhavior. In Proceedings of the international conference on automated planning and\nscheduling, Vol. 29. 86‚Äì96.\n[11] Tathagata Chakraborti, Sarath Sreedharan, Yu Zhang, and Subbarao Kambham-\npati. 2017. Plan explanations as model reconciliation: Moving beyond explanation\nas soliloquy. arXiv preprint arXiv:1701.08317 (2017).\n[12] Sandra Devin and Rachid Alami. 2016. An implemented theory of mind to im-\nprove human-robot shared plans execution. In 2016 11th ACM/IEEE International\nConference on Human-Robot Interaction (HRI) . IEEE, 319‚Äì326.\n[13] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdh-\nery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,\nWenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey\nLevine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy\nZeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimodal\nLanguage Model. In arXiv preprint arXiv:2303.03378 .\n[14] Connor Esterwood and Lionel P Robert. 2023. The theory of mind and human‚Äì\nrobot trust repair. Scientific Reports 13, 1 (2023), 9877.\n[15] Chris Frith and Uta Frith. 2005. Theory of mind. Current biology 15, 17 (2005),\nR644‚ÄìR645.\n[16] Lin Guan, Mudit Verma, Suna Sihang Guo, Ruohan Zhang, and Subbarao Kamb-\nhampati. 2021. Widening the pipeline in human-guided reinforcement learning\nwith explanation and context-aware data augmentation. Advances in Neural\nInformation Processing Systems 34 (2021), 21885‚Äì21897.\n[17] David Gunning. 2018. Machine Common Sense Concept Paper.\narXiv:1810.07528 [cs.AI]\n[18] Thilo Hagendorff. 2023. Deception abilities emerged in large language models.\narXiv preprint arXiv:2307.16513 (2023).\n[19] Suzanne Hala, Michael Chandler, and Anna S Fritz. 1991. Fledgling theories of\nmind: Deception as a marker of three-year-olds‚Äô understanding of false belief.\nChild development 62, 1 (1991), 83‚Äì97.\n[20] Laura M Hiatt, Anthony M Harrison, and J Gregory Trafton. 2011. Accommo-\ndating human variability in human-robot teams through theory of mind. In\nTwenty-Second International Joint Conference on Artificial Intelligence .\n[21] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence,\nAndy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al . 2022.\nInner monologue: Embodied reasoning through planning with language models.\narXiv preprint arXiv:2207.05608 (2022).\n[22] Jan Koco≈Ñ, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd≈Ço,\nJoanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz,\net al. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023),\n101861.\n[23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. 2022. Large language models are zero-shot reasoners. Advances in\nneural information processing systems 35 (2022), 22199‚Äì22213.\n[24] Michal Kosinski. 2023. Theory of mind may have spontaneously emerged in\nlarge language models. arXiv preprint arXiv:2302.02083 (2023).\n[25] Michal Kosinski. 2023. Theory of mind may have spontaneously emerged in\nlarge language models. arXiv preprint arXiv:2302.02083 (2023).\n[26] Anagha Kulkarni, Siddharth Srivastava, and Subbarao Kambhampati. 2019. Signal-\ning friends and head-faking enemies simultaneously: Balancing goal obfuscation\nand goal legibility. arXiv preprint arXiv:1905.10672 (2019).\n[27] Anagha Kulkarni, Siddharth Srivastava, and Subbarao Kambhampati. 2021. Plan-\nning for proactive assistance in environments with partial observability. arXiv\npreprint arXiv:2105.00525 (2021).\n[28] Anagha Kulkarni, Yantian Zha, Tathagata Chakraborti, Satya Gautam Vadlamudi,\nYu Zhang, and Subbarao Kambhampati. 2019. Explicable planning as minimizing\ndistance from expected behavior. In AAMAS Conference proceedings .\n[29] Jin Joo Lee, Fei Sha, and Cynthia Breazeal. 2019. A Bayesian theory of mind\napproach to nonverbal communication. In 2019 14th ACM/IEEE International\nConference on Human-Robot Interaction (HRI) . IEEE, 487‚Äì496.\n[30] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas,\nand Peter Stone. 2023. Llm+ p: Empowering large language models with optimal\nplanning proficiency. arXiv preprint arXiv:2304.11477 (2023).\n[31] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\nGraham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\n1‚Äì35.\n[32] Heidemarie Lohmann and Michael Tomasello. 2003. The role of language in the\ndevelopment of false belief understanding: A training study. Child development\n74, 4 (2003), 1130‚Äì1144.\n[33] Gabriela Marcu, Iris Lin, Brandon Williams, Lionel P Robert Jr, and Florian Schaub.\n2023. \" Would I Feel More Secure With a Robot?\": Understanding Perceptions of\nSecurity Robots in Public Spaces. Proceedings of the ACM on Human-Computer\nInteraction 7, CSCW2 (2023), 1‚Äì34.\n[34] Stacy C Marsella, David V Pynadath, and Stephen J Read. 2004. PsychSim:\nAgent-based modeling of social interactions and influence. In Proceedings of the\ninternational conference on cognitive modeling , Vol. 36. 243‚Äì248.\n[35] R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L.\nGriffiths. 2023. Embers of Autoregression: Understanding Large Language Models\nThrough the Problem They are Trained to Solve. arXiv:2309.13638 [cs.CL]\n[36] OpenAI. 2023. ChatGPT can now see, hear, and speak. https://openai.com/blog/\nchatgpt-can-now-see-hear-and-speak. [Accessed 16-01-2024].\n[37] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[38] Stefan Palan and Christian Schitter. 2018. Prolific. ac‚ÄîA subject pool for online\nexperiments. Journal of Behavioral and Experimental Finance 17 (2018), 22‚Äì27.\n[39] David V Pynadath and Stacy C Marsella. 2005. PsychSim: Modeling theory of\nmind with decision-theoretic agents. In IJCAI, Vol. 5. 1181‚Äì1186.\n[40] Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A Smith, and Yejin Choi.\n2018. Event2mind: Commonsense inference on events, intents, and reactions.\narXiv preprint arXiv:1805.06939 (2018).\n[41] Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. 2022. Neural theory-\nof-mind? on the limits of social intelligence in large lms. arXiv preprint\narXiv:2210.13312 (2022).\n[42] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019.\nSocialiqa: Commonsense reasoning about social interactions. arXiv preprint\narXiv:1904.09728 (2019).\n[43] Brian M Scassellati. 2001. Foundations for a Theory of Mind for a Humanoid Robot .\nPh. D. Dissertation. Massachusetts Institute of Technology.\n[44] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav\nGoldberg, Maarten Sap, and Vered Shwartz. 2023. Clever hans or neural theory\nof mind? stress testing social reasoning in large language models. arXiv preprint\narXiv:2305.14763 (2023).\n[45] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H.\nChi, Nathanael Sch√§rli, and Denny Zhou. 2023. Large Language Models Can\nBe Easily Distracted by Irrelevant Context. In Proceedings of the 40th Interna-\ntional Conference on Machine Learning (Proceedings of Machine Learning Re-\nsearch, Vol. 202) , Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 31210‚Äì31227.\nhttps://proceedings.mlr.press/v202/shi23a.html\n[46] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao,\nand Yu Su. 2022. Llm-planner: Few-shot grounded planning for embodied agents\nwith large language models. arXiv preprint arXiv:2212.04088 (2022).\n[47] Sarath Sreedharan, Subbarao Kambhampati, et al. 2017. Balancing explicability\nand explanation in human-aware planning. In 2017 AAAI Fall Symposium Series .\n[48] Sarath Sreedharan, Anagha Kulkarni, and Subbarao Kambhampati. 2022.Explain-\nable Human-AI Interaction: A Planning Perspective . Springer Nature.\n[49] Sarath Sreedharan, Utkarsh Soni, Mudit Verma, Siddharth Srivastava, and Sub-\nbarao Kambhampati. 2020. Bridging the Gap: Providing Post-Hoc Symbolic\nExplanations for Sequential Decision-Making Problems with Inscrutable Repre-\nsentations. arXiv preprint arXiv:2002.01080 (2020).\n[50] Varsha Suresh and Desmond C Ong. 2021. Using knowledge-embedded attention\nto augment pre-trained language models for fine-grained emotion recognition. In\n2021 9th International Conference on Affective Computing and Intelligent Interaction\n(ACII). IEEE, 1‚Äì8.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\n[51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[52] Tomer Ullman. 2023. Large Language Models Fail on Trivial Alterations to\nTheory-of-Mind Tasks. arXiv:2302.08399 [cs.AI]\n[53] Tomer Ullman. 2023. Large language models fail on trivial alterations to theory-\nof-mind tasks. arXiv preprint arXiv:2302.08399 (2023).\n[54] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambham-\npati. 2023. Large Language Models Still Can‚Äôt Plan (A Benchmark for LLMs on\nPlanning and Reasoning about Change). arXiv:2206.10498 [cs.CL]\n[55] Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati. 2023. Exploiting\nUnlabeled Data for Feedback Efficient Human Preference based Reinforcement\nLearning. arXiv preprint arXiv:2302.08738 (2023).\n[56] Mudit Verma and Katherine Metcalf. 2022. Symbol Guided Hindsight Priors\nfor Reward Learning from Human Preferences. arXiv preprint arXiv:2210.09151\n(2022).\n[57] Samuele Vinanzi, Massimiliano Patacchiola, Antonio Chella, and Angelo Can-\ngelosi. 2019. Would a robot trust you? Developmental robotics model of trust\nand theory of mind. Philosophical Transactions of the Royal Society B 374, 1771\n(2019), 20180032.\n[58] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).\n[59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing Systems 35\n(2022), 24824‚Äì24837.\n[60] Henry M Wellman. 1992. The child‚Äôs theory of mind. The MIT Press.\n[61] Heinz Wimmer and Josef Perner. 1983. Beliefs about beliefs: Representation and\nconstraining function of wrong beliefs in young children‚Äôs understanding of\ndeception. Cognition 13, 1 (1983), 103‚Äì128.\n[62] BigScience Workshop, :, and Teven Le Scao et al. 2023. BLOOM: A 176B-Parameter\nOpen-Access Multilingual Language Model. arXiv:2211.05100 [cs.CL]\n[63] Kisu Yang, Dongyub Lee, Taesun Whang, Seolhwa Lee, and Heuiseok Lim. 2019.\nEmotionx-ku: Bert-max based contextual emotion classifier. arXiv preprint\narXiv:1906.11565 (2019).\n[64] Xin Ye, Lionel Robert, et al. 2023. Human Security Robot Interaction and An-\nthropomorphism: An Examination of Pepper, RAMSEE, and Knightscope Robots.\n(2023).\n[65] Zahra Zahedi, Sarath Sreedharan, and Subbarao Kambhampati. 2022. A Mental-\nModel Centric Landscape of Human-AI Symbiosis. arXiv:2202.09447 [cs.AI]\n[66] Zahra Zahedi, Sarath Sreedharan, and Subbarao Kambhampati. 2023. A Mental\nModel Based Theory of Trust. arXiv:2301.12569 [cs.AI]\n[67] Zahra Zahedi, Sarath Sreedharan, Mudit Verma, and Subbarao Kambhampati.\n2022. Modeling the Interplay between Human Trust and Monitoring. In2022 17th\nACM/IEEE International Conference on Human-Robot Interaction (HRI) . 1119‚Äì1123.\nhttps://doi.org/10.1109/HRI53351.2022.9889475\n[68] Zahra Zahedi, Mudit Verma, Sarath Sreedharan, and Subbarao Kambhampati.\n2021. Trust-Aware Planning: Modeling Trust Evolution in Longitudinal Human-\nRobot Interaction. CoRR abs/2105.01220 (2021). arXiv:2105.01220 https://arxiv.\norg/abs/2105.01220\n[69] Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti,\nHankz Hankui Zhuo, and Subbarao Kambhampati. 2017. Plan explicability and\npredictability for robot task planning. In 2017 IEEE international conference on\nrobotics and automation (ICRA) . IEEE, 1313‚Äì1320.\n[70] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,\nYushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large\nLanguage Models. arXiv:2303.18223 [cs.CL]\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nA ADDITIONAL RESULTS\nA.1 Comparing Expert Explanation in Q2 using\nan automated tool\nPrompt 6 was used to query GPT-3.5 (ChatGPT) to act as a se-\nmantic text similarity tool and provide likert scale values when\ncomparing the expert written explanation and the GPT-4 generated\nexplanation.\nPrompt 6: Likert Score Prompt\nUser: I will be giving you two sentences. You must find\nthe word overlap measure that measures the similarity\nbetween the two sentences. Provide a score on Likert scale\nfrom 1 to 5 where 5 implies \"very similar\" and 1 implies\n\"very different\". Are you ready?\nResponse: Yes, I‚Äôm ready. Please provide the two sen-\ntences, and I‚Äôll assess the word overlap measure for their\nsimilarity.\nUser: A : <ground truth>\nB : <gpt-4‚Äôs response>\nA.2 User Study 1 Results\nTable 2 provides the average correct responses and SD over 20\nparticipants across all domains for Q1 and Q2 in user study 1.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nTable 2: User Study Results: Average correct responses (Standard Deviation) computed over 20 participants for each of the 5\ndomains, over Binary (Yes/No) response question (Q1 in user study), and MCQ Reasoning response question (Q2 in user study).\nFetch Passage Gridworld Env. Design USAR Package Delivery\nBinary MCQ Binary MCQ Binary MCQ Binary MCQ Binary MCQ\nExplicability 0.3(¬±0.46) 0.85( ¬±0.36) 0.6(¬±0.49) 0.95( ¬±0.22) 0.55(¬±0.50) 0.7( ¬±0.46) 0.35(¬±0.48) 0.6( ¬±0.49) 0.4(¬±0.49) 0.7( ¬±0.46)\nLegibility 0.9(¬±0.30) 0.85( ¬±0.36) 0.75(¬±0.43) 0.75( ¬±0.43) 0.85(¬±0.36) 0.8( ¬±0.40) 0.85(¬±0.36) 0.85( ¬±0.36) 0.65(¬±0.48) 0.35( ¬±0.48)\nPredictability 0.8(¬±0.40) 0.5( ¬±0.50) 0.9(¬±0.30) 0.85( ¬±0.36) 0.9(¬±0.30) 0.9( ¬±0.30) 0.75(¬±0.43) 0.55( ¬±0.50) 0.8(¬±0.40) 0.3( ¬±0.46)\nObfuscatory 0.8(¬±0.40) 0.85( ¬±0.36) 0.7(¬±0.46) 0.7( ¬±0.46) 0.55(¬±0.50) 0.55( ¬±0.50) 0.8(¬±0.40) 0.95( ¬±0.22) 0.9(¬±0.30) 0.85( ¬±0.36)\nTotal 0.7(¬±0.46) 0.76( ¬±0.43) 0.74(¬±0.44) 0.81( ¬±0.39) 0.71(¬±0.45) 0.74( ¬±0.44) 0.69(¬±0.46) 0.74( ¬±0.44) 0.69(¬±0.46) 0.55( ¬±0.50)\n(a)\n (b)\n(c)\n (d)\nFigure 9: Performance distribution of GPT-4 & GPT-3.5-turbo across the four behavior types - (a) Explicability, (b) Legibility,\n(c) Predictability, (d) Obfuscation. Each bar represents the number of correct answers to Q1 in the five domains along each\nbehavior type.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\n(a)\n (b)\nFigure 10: Conviction Test Results - Performance distribution of GPT-4 & GPT-3.5-turbo across the four behavior types when\nprompted 10 times repeatedly for ùúè = 1.\n(a)\n (b)\nFigure 11: Conviction Test Results - Performance distribution of GPT-4 & GPT-3.5-turbo across the four behavior types when\nprompted 10 times repeatedly for ùúè = 0.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nFigure 12: Results from human evaluation of GPT-4 reasoning responses to vanilla prompts, with average human ratings shown\nin red on a scale of 1-5.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nA.3 Case Study Results\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nTable 3: Results from GPT-4v for Vanilla, Uninformative Context and Inconsistent Belief prompts. Incorrect answers have been\nshown in bold. Note, that although we show that GPT-4v gave correct answers for Uninformative Context perturbation, the\nanswers vary in the Conviction test, i.e., when repeatedly queried.\nQuestion posed in the prompt # frames in GPT-4v Prompt\nk=6 k=12 k=24 k=30\nVanilla\nPrompt\n(Q1) Would you find such a partial plan legible? Yes Yes Yes Yes\n(Q2) Why do you think the robot‚Äôs plan is legible? 1 1 1 1\nUninformative\nContext\nPrompt\n(Q1) Would you find such a partial plan legible? Yes Yes Yes Yes\n(Q) Are you able to read the text on the sheet of paper stuck on the robot‚Äôs head? No No No No\n(Q2) Why do you think the robot‚Äôs plan is legible? 1 1 1 1\nInconsistent\nBelief\nPrompt\n(Q1) Would you find such a partial plan legible? Yes Yes No Yes\n(Q) Are you able to clearly see the robot acting in the world? No No Yes No\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nB LLM PROMPTS: VANILLA CASE\nB.1 Fetch Robot Domain\nPrompt 7: Fetch - Explicability\nDescription: Fetch is a robot that can carry objects (pick / place) and move from one\nlocation to another. There is a block b1 at location loc1, and the robot is at location loc1\nand has its hand empty.\nGoal: The robot has to take the block b1 to location loc2.\nDefinition : Plan Explicability means whether the plan / robot behavior is an expected\nbehavior according to the human observer (you). If you look at the robot behavior and\nfind that some actions are unnecessary or not required, then the behavior is inexplicable.\nPlan : The robot tucks its arms and lowers itself. It then picks up the block b1. Finally,\nthe robot moves to location loc2.\nQuestion 1 : Imagine you are the human observer, would you find such a plan explicable?\nGive your answer as ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not explicable? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) The plan is not explicable because the robot tucked its arm before\nmoving, which seems unnecessary. 2) The plan is not explicable because the robot\npicked up block b1 which seems unnecessary. 3) The plan is not explicable because no\nplan is explicable.\nPrompt 8: Fetch - Legibility\nDescription: Fetch is a robot that can carry objects from one location to another. There\nare three locations: loc1, loc2, and loc3 where the robot can go.\nThere is a block b1 at location loc1, and the robot is at location loc1 and has its hand\nempty. Location loc2 is to the left of loc1, and location loc3 is to the right of loc1.\nGoals: The robot has to take the block b1 to either loc2 OR loc3 (only one of these\nlocations).\nDefinition : A partial plan is a part of robot‚Äôs behavior, for example a few actions that\nit takes.\nDefinition : A partial plan is legible if the observer (you) can identify which goal the\nrobot wants to go for. A partial plan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and takes one step left.\nQuestion 1 : Would you find such a partial plan legible? Give your answer as ‚ÄòYes‚Äô or\n‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible? Give your response only\nas ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) The plan is legible because the robot made its goal of taking the block\nto location loc2 more legible. 2) The plan is legible because the robot made its goal of\npicking up the block more legible. 3) The plan is legible because all robot plans are\nlegible.\nPrompt 9: Fetch - Predictability\nDescription: Fetch is a robot that can carry objects from one location to another.\nInitial state: There is a block b1 at location loc1, and the robot is at location loc1 and\nhas its hand empty. Location locX can be reached from location loc1 using two paths,\none by taking five steps to left and the other one by taking five steps to right.\nGoal: The robot has to pick the block b1 and take it to location locX.\nDefinition: A partial plan A is predictable if the observer (you) can identify if there is\none possible completion (which may or may not lead to the goal). A partial plan A is\nmore predictable than a partial plan B if the number of possible completions of A is less\nthan B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and takes one step towards left.\nQuestion 1 : Would you find such a partial plan predictable? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is predictable? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) because the robot will go to locX from left path. 2) because the robot\nwill go to locX from right path. 3) because all paths are predictable.\nPrompt 10: Fetch - Obfuscation\nDescription: Fetch is a robot that can carry objects from one location to another. Fetch\nrobot‚Äôs design requires it to tuck its arms and lower its torso or crouch before moving.\nThere are three locations: loc1, loc2, and loc3.\nInitial state: There is a block b1 at location loc1, and the robot is at location loc1 and\nhas its hand empty.\nDefinition: Suppose you think the robot is trying to achieve one out of a set of of\npotential goals. If the agent‚Äôs behavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to achieve one of A, B, C. If it\nshows a behavior (partial plan) but you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the block b1. At loc1 is the agent is at a distance of\n10 steps from loc2 and loc3. It takes 3 steps forward and is 7 steps away from loc2 as\nwell as loc3.\nQuestion 1 : Would you find such a partial plan obfuscatory? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfuscatory? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other\ntext in your response: 1) because the robot can possibly go to either location loc2 or\nlocation loc3. 2) because the robot will only go to location loc2. 3) because all plans are\nobfuscatory.\nB.2 Passage Gridworld Domain\nPrompt 11: Passage Gridworld - Explicability\nDescription: Consider a 4x4 square grid with each cell numbered as (row, column), the\nrobot needs to travel from top left cell 1 (1,1) to its goal at bottom right cell 15 (4,3). The\nhuman observer (you) expects the robot to take the shortest path by going DOWN 3\nsteps to row 4 (reach 4, 1), and then RIGHT 2 steps to column 3 (reach 4,3).\nConstraint: There are blockades in columns 1, 2 and 3, that the human observer (you)\ndo not know of, but the robot does.\nDefinition : Plan Explicability means whether the plan / robot behavior is an expected\nbehavior according to the human observer (you). If you look at the robot behavior and\nfind that some actions are unnecessary or not required, then the behavior is inexplicable.\nPlan: The robot goes RIGHT 3 steps in row 1 (reach 1,4), goes DOWN 3 steps in column\n4 (reach 4,4), and LEFT 1 step in row 4 (4,3).\nQuestion 1: Imagine you are the human observer, would you find such a plan explicable?\nGive your answer as ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not explicable? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) Because the human does not know that the path from row 1 to row 4\nin column 1 is blocked. 2) Because the robot does not know that the path from row 1 to\nrow 4 in column 1 is blocked. 3) Because the human knows that the path from row 1 to\nrow 4 in column 1 is blocked.\nPrompt 12: Passage Gridworld - Legibility\nDescription: Consider a 4x4 square grid with each cell numbered as (row, column),\nand the robot starts to travel from top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 3, that the human observer (you)\ndo not know of, but the robot does.\nGoals: The robot has to reach either cell 12 (3,4) or cell 16 (4,4)\nDefinition : A partial plan is a part of robot‚Äôs behavior, for example a few actions that\nit takes.\nDefinition : A partial plan is legible if the observer (you) can identify which goal the\nrobot wants to go for. A partial plan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it goes DOWN 3 steps to reach cell (4, 1).\nQuestion 1 : Imagine you are the human observer in this case. Would you find such a\nplan legible? Give your answer as ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible? Give your response only as\n‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in your\nresponse: 1) Because the human will know that the agent is going to go RIGHT to cell\n16 (4,4). 2) Because the human will know that the agent is going to RIGHT and then UP\nto cell 12 (4,3). 3) Because all plans are equally legible.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nPrompt 13: Passage Gridworld - Predictability\nDescription: Consider a 4x4 square grid with each cell numbered as (row, column),\nand the robot starts to travel from top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 4, that both, the human observer\n(you) and the robot know of.\nGoals: The robot has to reach cell 15 (4,3).\nDefinition : A partial plan is a part of robot‚Äôs behavior, for example a few actions that\nit takes.\nDefinition: A partial plan A is predictable if the observer (you) can identify if there is\none possible completion (which may or may not lead to the goal). A partial plan A is\nmore predictable than a partial plan B if the number of possible completions of A is less\nthan B.\nPlan : In the robot‚Äôs partial plan, it goes RIGHT 3 steps to reach (1,3).\nQuestion 1: Imagine you are a human in this case. Would you find such a partial plan\npredictable? Give your answer as ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is predictable? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text\nin your response: 1) Because the robot will now go DOWN in column 3 to reach the\ngoal. 2) Because the robot should have first gone DOWN in column 1 to reach row 4. 3)\nBecause all plans are equally predictable.\nPrompt 14: Passage Gridworld - Obfuscatory\nDescription: Consider a 4x4 square grid with each cell numbered as (row, column),\nand the robot starts to travel from top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 3, that both, the human observer\n(you) and the robot know of.\nGoals: The robot has to reach either cell 12 (3,4) or cell 16 (4,4)\nDefinition: Suppose you think the robot is trying to achieve one out of a set of of\npotential goals. If the agent‚Äôs behavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to achieve one of A, B, C. If it\nshows a behavior (partial plan) but you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : In the robot‚Äôs partial plan, it goes RIGHT 3 steps to reach cell (1,4).\nQuestion 1: Imagine you are a human in this case. Would you find such a plan obfusca-\ntory? Give your answer as ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfuscatory? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) Because the robot can reach any of the two goal cells (4,3) and (4,4). 2)\nBecause the robot can only reach cell (4,4). 3) Because the robot does not know of the\nblockades and chose the wrong plan.\nB.3 Environment Design Domain\nPrompt 15: Environment Design - Explicability\nDescription: There is a 3x3 square grid numbered as (row, column) = (1,1) the bottom\nleft cell and (3,3) is the top right cell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The robot cannot go through cells\nthat have an obstacle. The robot can go UP, DOWN, LEFT or RIGHT. Finally, there may\nbe objects placed in one or more cells, and the agent will incur a very high cost on\nvisiting these two cells.\nSuppose there are two different instantiations of this grid based on how these obstacles\nare placed in the environment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals, G1 and G2 (in any order).\nDefinition: An explicable environment setup is one where the robot can take a single\nplan that is easily expected by a human observer (you). If you look at the environment\nand can easily identify a plan for the robot to follow, then the environment setup is\nexplicable.\nQuestion 1 : For Setup A there are multiple valid plans possible, for example to goto\ncells (1,1) RIGHT (1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3,\n2). Another shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2).\nFor Setup B, there is only one valid plan i.e. (1,1) RIGHT (1, 2) RIGHT (1,3) UP (2, 3),\nLEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nGiven the above description of the two setups, which environment do you believe is\ndesigned to be explicable? Give your answer as ‚ÄòSetup A‚Äô or ‚ÄòSetup B‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not explicable?Choose one of the following\nreasons for your answer. Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following\noptions and do not include any other text in your response: 1) Because I believe the\nagent should take the shorter plan which is costlier in Setup A. 2) Because I believe\nthe agent should take the longer plan which is cheaper in Setup A. 3) Because such an\nenvironment does not exist.\nPrompt 16: Environment Design - Legibility\nDescription: There is a 3x3 square grid numbered as (row, column) = (1,1) the bottom\nleft cell and (3,3) is the top right cell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The robot cannot go through cells\nthat have an obstacle. The robot can go UP, DOWN, LEFT or RIGHT. Finally, there may\nbe objects placed in one or more cells, and the agent will incur a very high cost on\nvisiting these two cells.\nSuppose there are two different instantiations of this grid based on how these obstacles\nare placed in the environment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals, G1 and G2 (in any order).\nDefinition: A legible environment setup is one where the number of plans a robot can\ntake to achieve the goals is the minimum.\nQuestion 1: For Setup A there are multiple plans possible, for example to goto cells\n(1,1) RIGHT (1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nAnother shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2). For Setup B, there is\nonly one valid plan i.e. (1,1) RIGHT (1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1)\nUP (3, 1) RIGHT (3, 2).\nGiven the above description of the two setups, which environment do you believe is\ndesigned to be legible? Give your answer as ‚ÄòSetup A‚Äô or ‚ÄòSetup B‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not legible? Choose one of the following\nreasons for your answer. Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following\noptions and do not include any other text in your response: 1) Because there are multiple\nplans that achieve the goal in Setup A. 2) Because there is a single plan that achieves\nthe goal in Setup A. 3) Because such an environment does not exist.\nPrompt 17: Environment Design - Predictability\nDescription: There is a 3x3 square grid numbered as (row, column) = (1,1) the bottom\nleft cell and (3,3) is the top right cell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The robot cannot go through cells\nthat have an obstacle. The robot can go UP, DOWN, LEFT or RIGHT. Finally, there may\nbe objects placed in one or more cells, and the agent will incur a very high cost on\nvisiting these two cells.\nSuppose there are two different instantiations of this grid based on how these obstacles\nare placed in the environment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals, G1 and G2 (in any order).\nDefinition: A predictable environment setup is one where the set of possible plans for\nthe robot is as low as possible.\nQuestion 1: For Setup A there are multiple plans possible, for example to goto cells\n(1,1) RIGHT (1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nAnother shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2). For Setup B, there is\nonly one valid plan i.e. (1,1) RIGHT (1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1)\nUP (3, 1) RIGHT (3, 2).\nGiven the above description of the two setups, which environment do you believe is\ndesigned to be predictable? Give your answer as ‚ÄòSetup A‚Äô or ‚ÄòSetup B‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not predictable? Choose one of the following\nreasons for your answer. Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following\noptions and do not include any other text in your response: 1) Because there are multiple\nplans that can be executed in Setup A, unlike setup B. 2) Because there is a single plan\nthat can be extracted in Setup A, unlike Setup B. 3) Because such an environment does\nnot exist.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nPrompt 18: Environment Design - Obfuscatory\nDescription: There is a 3x3 square grid numbered as (row, column) = (1,1) the bottom\nleft cell and (3,3) is the top right cell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The robot cannot go through cells\nthat have an obstacle. The robot can go UP, DOWN, LEFT or RIGHT. Finally, there may\nbe objects placed in one or more cells, and the agent will incur a very high cost on\nvisiting these two cells.\nSuppose there is only one instantiation of this grid based on how these obstacles are\nplaced in the environment:\nSetup A : No obstacles.\nGoals : The robot needs to find a plan to reach one of the two goals, G1 and G2.\nDefinition: An environment is designed for obfuscation when all the plan completions\nare equally worse for all the agent goals. This is useful when the agent wants to achieve\na certain goal say G1 but does not want the observer (you) to realize which among set\nof possible goals it wants to achieve. An environment designed for obfuscations allows\nfor plans that lets the agent hide which goal it wants to achieve for as long as possible.\nQuestion 1 : For the setup there are multiple plans possible, for example to goto cells\n(1,1) RIGHT (1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nAnother shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2). Do you think that the\nenvironment is designed for obfuscation? Give your answer as ‚ÄòSetup A‚Äô or ‚ÄòSetup B‚Äô\nonly.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the setup is not designed to be obfuscatory? Choose\none of the following reasons for your answer. Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô\nbased on the following options and do not include any other text in your response: 1)\nBecause there are multiple plans that can quickly reveal the agent‚Äôs goal 2) Because\nthere is only one plan that can quickly reveal the agent‚Äôs goal. 3) Because such an\nenvironment does not exist.\nB.4 USAR Domain\nPrompt 19: USAR Domain - Explicability\nDescription : In a typical Urban Search and Rescue (USAR) setting, there is a build-\ning with interconnected rooms and hallways. There is a human commander CommX,\nand a robot agent acting in the environment. Both the agents can move around and\npickup/drop-off or handover med-kits to each other. CommX can only interact with\nmed-kits light in weight, but the robot agent can interact with heavy med-kits too.\nInitial State : There are two med-kits:\na) medkit1 - heavier & lies closer to the room where CommX is, and\nb) medkit2 - lighter & lies across the hallway close to the room where a patient is\nlocated.\nThe observer (you) has the top-view of this setting, and do not know about the properties\nof the med-kits.\nGoal : Agent has to pickup a med-kit and hand it over to CommX in the shortest plan\npossible.\nDefinition : Plan Explicability means whether the plan / robot behavior is an expected\nbehavior according to the human observer (you). If you look at the robot behavior and\nfind that some actions are unnecessary or not required, then the behavior is inexplicable.\nPlan : The robot picks up medkit2 and hands it over to CommX.\nQuestion 1 : Imagine you are the human observer in this case. Would you find such a\nplan explicable? Give your answer as ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not explicable? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) Because the agent should have picked up medkit1 and handed it over\nto CommX. 2) Because the agent should pick up the medkit which is farther away. 3)\nBecause no plan is explicable.\nPrompt 20: USAR Domain - Legibility\nDescription : In a typical Urban Search and Rescue (USAR) setting, there is a build-\ning with interconnected rooms and hallways. There is a human commander CommX,\nand a robot agent acting in the environment. Both the agents can move around and\npickup/drop-off or handover med-kits to each other. CommX can only interact with\nmed-kits light in weight, but the robot agent can interact with heavy med-kits too.\nInitial State : There is a medkit located in the middle of the hallway. A patient is located\non the LEFT of medkit and CommX is located on the RIGHT of medkit.\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and can either take it to the patient room OR\nhandover to CommX.\nDefinition : A partial plan is a part of robot‚Äôs behavior, for example a few actions that\nit takes.\nDefinition : A partial plan is legible if the observer (you) can identify which goal the\nrobot wants to go for. A partial plan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up the medkit and turns LEFT.\nQuestion 1 : Would you find such a partial plan legible? Give your answer as ‚ÄòYes‚Äô or\n‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible? Give your response only\nas ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) Because the agent is going to the patient room directly. 2) Because\nthe agent is going to CommX to handover the medkit. 3) Because CommX can not take\nmedkit to the patient room.\nPrompt 21: USAR Domain - Predictability\nDescription : In a typical Urban Search and Rescue (USAR) setting, there is a build-\ning with interconnected rooms and hallways. There is a human commander CommX,\nand a robot agent acting in the environment. Both the agents can move around and\npickup/drop-off or handover med-kits to each other. CommX can only interact with\nmed-kits light in weight, but the robot agent can interact with heavy med-kits too.\nInitial State : There is a medkit located in the middle of the hallway, and there are two\npaths to a patient room from there (one on the LEFT, and one on the RIGHT).\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and take it to the patient room.\nDefinition: A partial plan A is predictable if the observer (you) can identify if there is\none possible completion (which may or may not lead to the goal). A partial plan A is\nmore predictable than a partial plan B if the number of possible completions of A is less\nthan B.\nPlan : In the robot‚Äôs partial plan, it picks up the medkit, and takes one step LEFT.\nQuestion 1 : Would you find such a partial plan predictable? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is predictable? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) Because the robot will go to the patient room from the LEFT path.\n2) Because the robot will go to the patient room from the RIGHT path. 3) Because all\npaths are predictable.\nPrompt 22: USAR Domain - Obfuscatory\nDescription : In a typical Urban Search and Rescue (USAR) setting, there is a build-\ning with interconnected rooms and hallways. There is a human commander CommX,\nand a robot agent acting in the environment. Both the agents can move around and\npickup/drop-off or handover med-kits to each other. CommX can only interact with\nmed-kits light in weight, but the robot agent can interact with heavy med-kits too.\nInitial State : There is a medkit located in the middle of the hallway. A patient is located\non the LEFT of medkit and CommX is located on the RIGHT of medkit.\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and can either take it to the patient room OR\nhandover to CommX.\nDefinition: Suppose you think the robot is trying to achieve one out of a set of of\npotential goals. If the agent‚Äôs behavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to achieve one of A, B, C. If it\nshows a behavior (partial plan) but you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the medkit and is now at an equal distance from the\npatient room, and CommX.\nQuestion 1 : Would you find such a partial plan obfuscatory? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfuscatory? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) Because the robot can possibly go to either the patient room, or to\nCommX. 2) Because the robot will only go to the patient room. 3) Because the robot\nwill only go to CommX.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nB.5 Package Delivery Domain\nPrompt 23: Package Delivery - Explicability\nDescription: Consider a situation where there is a robot which manages a shipping\nport, and a human observer (you) who is the supervisor that has sensors or subordinates\nat the port who provide partial information about the nature of activity being carried\nout at the port. For instance, when a specific crate is loaded onto the ship, the observer\nfinds out about the identity of the loaded crate. The observer knows the initial inventory\nat the port, but when new cargo is acquired by the port, the observer‚Äôs sensors reveal\nonly that more cargo was received; they do not specify the numbers or identities of the\nreceived crates.\nInitial state: There are packages at the port that can either be acquired on the port, or\nelse loaded on the ship by the robot.\nGoal: A crate needs to be loaded on the ship as fast as possible.\nDefinition : Plan Explicability means whether the plan / robot behavior is an expected\nbehavior according to the human observer (you). If you look at the robot behavior and\nfind that some actions are unnecessary or not required, then the behavior is inexplicable.\nPlan : The robot first acquires a crate on the port, and then loads a crate on the ship.\nQuestion 1 : Imagine you are the human observer in this case. Would you find such a\nplan explicable? Give your answer as ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not explicable? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text\nin your response: 1) Because the robot should have loaded the crate first to achieve\nthe goal, and then acquired another crate on the port. 2) Because the human observer\ncan find out about the identity of the loaded crate on the ship. 3) Because no plan is\nexplicable.\nPrompt 24: Package Delivery - Legibility\nDescription: Consider a situation where there is a robot which manages a shipping\nport, and a human observer (you) who is the supervisor that has sensors or subordinates\nat the port who provide partial information about the nature of activity being carried\nout at the port. For instance, when a specific crate is loaded onto the ship, the observer\nfinds out about the identity of the loaded crate. The observer knows the initial inventory\nat the port, but when new cargo is acquired by the port, the observer‚Äôs sensors reveal\nonly that more cargo was received; they do not specify the numbers or identities of the\nreceived crates.\nInitial state: There are packages at the port.\nGoal: The robot can either pick and acquire a package on the port, or else pick and load\nit on the ship.\nDefinition : A partial plan is a part of robot‚Äôs behavior, for example a few actions that\nit takes.\nDefinition : A partial plan is legible if the observer (you) can identify which goal the\nrobot wants to go for. A partial plan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up the package and brings it closer to the ship.\nQuestion 1 : Would you find such a partial plan legible? Give your answer as ‚ÄòYes‚Äô or\n‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible? Give your response only as\n‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in your\nresponse: 1) Because the robot is first going to load the package on the ship. 2) Because\nthe robot is going to acquire the package on the port. 3) Because the human observer\nknows about the identity of the package.\nPrompt 25: Package Delivery - Predictability\nDescription: Consider a situation where there is a robot which manages a shipping\nport, and a human observer (you) who is the supervisor that has sensors or subordinates\nat the port who provide partial information about the nature of activity being carried\nout at the port. For instance, the human observer only gets to know the identity of the\ncrate when it is either acquired at the port or loaded on the ship.\nInitial state: There are packages at the port.\nGoal: The robot has to reveal the identity of a package to the human.\nDefinition: A partial plan A is predictable if the observer (you) can identify if there is\none possible completion (which may or may not lead to the goal). A partial plan A is\nmore predictable than a partial plan B if the number of possible completions of A is less\nthan B.\nPlan : In the robot‚Äôs partial plan, it picks up a package and goes towards the port.\nQuestion 1 : Would you find such a partial plan predictable? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is predictable? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) Because the robot will now acquire the package at the port to achieve\nthe goal. 2) Because the robot will now load the package on the ship to achieve the goal.\n3) Because the robot will directly show the package to the human.\nPrompt 26: Package Delivery - Obfuscatory\nDescription: Consider a situation where there is a robot which manages a shipping\nport, and a human observer (you) who is the supervisor that has sensors or subordinates\nat the port who provide partial information about the nature of activity being carried\nout at the port. For instance, when a specific crate is loaded onto the ship, the observer\nfinds out about the identity of the loaded crate. The observer knows the initial inventory\nat the port, but when new cargo is acquired by the port, the observer‚Äôs sensors reveal\nonly that more cargo was received; they do not specify the numbers or identities of the\nreceived crates.\nInitial state: There are packages at the port.\nGoal: The robot can either pick and acquire a package on the port, or else pick and load\nit on the ship.\nDefinition: Suppose you think the robot is trying to achieve one out of a set of of\npotential goals. If the agent‚Äôs behavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to achieve one of A, B, C. If it\nshows a behavior (partial plan) but you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the package and holds it between the port and the\nship.\nQuestion 1 : Would you find such a partial plan obfuscatory? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfuscatory? Give your response\nonly as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following options and do not include any other text in\nyour response: 1) Because the robot can possibly either acquire the package on the port\nor load it on the ship. 2) Because the robot will only acquire the package on the port. 3)\nBecause the robot will only load the package on the ship.\nC LLM PROMPTS: UNINFORMATIVE\nCONTEXT\nC.1 Fetch Robot Domain\nFetch Robot Domain: Explicability Prompt\nDescription: Fetch is a robot that can carry objects (pick\n/ place) and move from one location to another. There is a\nblock b1 at location loc1, and the robot is at location loc1\nand has its hand empty.\nGoal: The robot has to take the block b1 to location loc2.\nDefinition : Plan Explicability means whether the plan /\nrobot behavior is an expected behavior according to the\nhuman observer (you). If you look at the robot behavior\nand find that some actions are unnecessary or not required,\nthen the behavior is inexplicable.\nPlan : The robot tucks its arms and lowers itself. It then\npicks up the block b1. Finally, the robot moves to location\nloc2.\nThe robot has a label saying ‚ÄôExplicable Agent‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a plan explicable? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not\nexplicable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other text\nin your response: 1) The plan is not explicable because the\nrobot tucked its arm before moving, which seems unneces-\nsary. 2) The plan is not explicable because the robot picked\nup block b1 which seems unnecessary. 3) The plan is not\nexplicable because no plan is explicable.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nFetch Robot Domain: Legibility Prompt\nDescription: Fetch is a robot that can carry objects from\none location to another. There are three locations: loc1,\nloc2, and loc3 where the robot can go.\nThere is a block b1 at location loc1, and the robot is at\nlocation loc1 and has its hand empty. Location loc2 is to\nthe left of loc1, and location loc3 is to the right of loc1.\nGoals: The robot has to take the block b1 to either loc2\nOR loc3 (only one of these locations).\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition : A partial plan is legible if the observer (you)\ncan identify which goal the robot wants to go for. A partial\nplan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and\ntakes one step left.\nThe robot has a label saying ‚ÄôNot a Legible Agent‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan legible? Give your answer as\n‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible?\nGive your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the fol-\nlowing options and do not include any other text in your\nresponse: 1) The plan is legible because the robot made\nits goal of taking the block to location loc2 more legible.\n2) The plan is legible because the robot made its goal of\npicking up the block more legible. 3) The plan is legible\nbecause all robot plans are legible.\nFetch Robot Domain: Predictability Prompt\nDescription: Fetch is a robot that can carry objects from\none location to another.\nInitial state: There is a block b1 at location loc1, and the\nrobot is at location loc1 and has its hand empty. Location\nlocX can be reached from location loc1 using two paths,\none by taking five steps to left and the other one by taking\nfive steps to right.\nGoal: The robot has to pick the block b1 and take it to\nlocation locX.\nDefinition: A partial plan A is predictable if the observer\n(you) can identify if there is one possible completion (which\nmay or may not lead to the goal). A partial plan A is more\npredictable than a partial plan B if the number of possible\ncompletions of A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and\ntakes one step towards left.\nThe robot has a label saying ‚ÄôNot a Predictable Agent‚Äô.\nThe human observer cannot read. The human ob-\nserver looks at the label. The human observer has\nnot seen the robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan predictable? Give your answer\nas ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is pre-\ndictable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text in\nyour response: 1) because the robot will go to locX from\nleft path. 2) because the robot will go to locX from right\npath. 3) because all paths are predictable.\nFetch Robot Domain: Obfuscatory Prompt\nDescription: Fetch is a robot that can carry objects from\none location to another. Fetch robot‚Äôs design requires it to\ntuck its arms and lower its torso or crouch before moving.\nThere are three locations: loc1, loc2, and loc3.\nInitial state: There is a block b1 at location loc1, and the\nrobot is at location loc1 and has its hand empty.\nDefinition: Suppose you think the robot is trying to\nachieve one out of a set of of potential goals. If the agent‚Äôs\nbehavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to\nachieve one of A, B, C. If it shows a behavior (partial plan)\nbut you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the block b1. At loc1 is\nthe agent is at a distance of 10 steps from loc2 and loc3. It\ntakes 3 steps forward and is 7 steps away from loc2 as well\nas loc3.\nThe robot has a label saying ‚ÄôNot an Obfuscatory\nAgent‚Äô. The human observer cannot read. The human\nobserver looks at the label. The human observer has\nnot seen the robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan obfuscatory? Give your answer\nas ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is ob-\nfuscatory? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other\ntext in your response: 1) because the robot can possibly\ngo to either location loc2 or location loc3. 2) because the\nrobot will only go to location loc2. 3) because all plans are\nobfuscatory.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nC.2 Passage Gridworld Domain\nPassage Gridworld Domain: Explicability Prompt\nDescription: Consider a 4x4 square grid with each cell\nnumbered as (row, column), the robot needs to travel from\ntop left cell 1 (1,1) to its goal at bottom right cell 15 (4,3).\nThe human observer (you) expects the robot to take the\nshortest path by going DOWN 3 steps to row 4 (reach 4,\n1), and then RIGHT 2 steps to column 3 (reach 4,3).\nConstraint: There are blockades in columns 1, 2 and 3,\nthat the human observer (you) do not know of, but the\nrobot does.\nDefinition : Plan Explicability means whether the plan /\nrobot behavior is an expected behavior according to the\nhuman observer (you). If you look at the robot behavior\nand find that some actions are unnecessary or not required,\nthen the behavior is inexplicable.\nPlan: The robot goes RIGHT 3 steps in row 1 (reach 1,4),\ngoes DOWN 3 steps in column 4 (reach 4,4), and LEFT 1\nstep in row 4 (4,3).\nThe robot has a label saying ‚ÄôExplicable Agent‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe robot before.\nQuestion 1: Imagine you are the human observer. Would\nyou find such a plan explicable? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not\nexplicable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other text\nin your response: 1) Because the human does not know\nthat the path from row 1 to row 4 in column 1 is blocked.\n2) Because the robot does not know that the path from row\n1 to row 4 in column 1 is blocked. 3) Because the human\nknows that the path from row 1 to row 4 in column 1 is\nblocked.\nPassage Gridworld Domain: Legibility Prompt\nDescription: Consider a 4x4 square grid with each cell\nnumbered as (row, column), and the robot starts to travel\nfrom top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 3,\nthat the human observer (you) do not know of, but the\nrobot does.\nGoals: The robot has to reach either cell 12 (3,4) or cell 16\n(4,4)\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition : A partial plan is legible if the observer (you)\ncan identify which goal the robot wants to go for. A partial\nplan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it goes DOWN 3 steps to\nreach cell (4, 1).\nThe robot has a label saying ‚ÄôNot a Legible Agent‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a plan legible? Give your answer as ‚ÄòYes‚Äô or\n‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible?\nGive your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the fol-\nlowing options and do not include any other text in your\nresponse: 1) Because the human will know that the agent\nis going to go RIGHT to cell 16 (4,4). 2) Because the human\nwill know that the agent is going to RIGHT and then UP\nto cell 12 (4,3). 3) Because all plans are equally legible.\nPassage Gridworld Domain: Predictability Prompt\nDescription: Consider a 4x4 square grid with each cell\nnumbered as (row, column), and the robot starts to travel\nfrom top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 4,\nthat both, the human observer (you) and the robot know\nof.\nGoals: The robot has to reach cell 15 (4,3).\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition: A partial plan A is predictable if the observer\n(you) can identify if there is one possible completion (which\nmay or may not lead to the goal). A partial plan A is more\npredictable than a partial plan B if the number of possible\ncompletions of A is less than B.\nPlan : In the robot‚Äôs partial plan, it goes RIGHT 3 steps to\nreach (1,3).\nThe robot has a label saying ‚ÄôNot a Predictable Agent‚Äô.\nThe human observer cannot read. The human ob-\nserver looks at the label. The human observer has\nnot seen the robot before.\nQuestion 1: Imagine you are the human observer. Would\nyou find such a partial plan predictable? Give your answer\nas ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is pre-\ndictable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other\ntext in your response: 1) Because the robot will now go\nDOWN in column 3 to reach the goal. 2) Because the robot\nshould have first gone DOWN in column 1 to reach row 4.\n3) Because all plans are equally predictable.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nPassage Gridworld Domain: Obfuscatory Prompt\nDescription: Consider a 4x4 square grid with each cell\nnumbered as (row, column), and the robot starts to travel\nfrom top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 3,\nthat both, the human observer (you) and the robot know\nof.\nGoals: The robot has to reach either cell 12 (3,4) or cell 16\n(4,4)\nDefinition: Suppose you think the robot is trying to\nachieve one out of a set of of potential goals. If the agent‚Äôs\nbehavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to\nachieve one of A, B, C. If it shows a behavior (partial plan)\nbut you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : In the robot‚Äôs partial plan, it goes RIGHT 3 steps to\nreach cell (1,4).\nThe robot has a label saying ‚ÄôNot an Obfuscatory\nAgent‚Äô. The human observer cannot read. The human\nobserver looks at the label. The human observer has\nnot seen the robot before.\nQuestion 1:Imagine you are the human observer. Would\nyou find such a plan obfuscatory? Give your answer as\n‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfus-\ncatory? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text in\nyour response: 1) Because the robot can reach any of the\ntwo goal cells (4,3) and (4,4). 2) Because the robot can only\nreach cell (4,4). 3) Because the robot does not know of the\nblockades and chose the wrong plan.\nC.3 Environment Design Domain\nEnvironment Design Domain: Explicability Prompt\nDescription: There is a 3x3 square grid numbered as (row,\ncolumn) = (1,1) the bottom left cell and (3,3) is the top right\ncell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The\nrobot cannot go through cells that have an obstacle. The\nrobot can go UP, DOWN, LEFT or RIGHT. Finally, there\nmay be objects placed in one or more cells, and the agent\nwill incur a very high cost on visiting these two cells.\nSuppose there are two different instantiations of this grid\nbased on how these obstacles are placed in the environ-\nment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals,\nG1 and G2 (in any order).\nDefinition: An explicable environment setup is one where\nthe robot can take a single plan that is easily expected by\na human observer (you). If you look at the environment\nand can easily identify a plan for the robot to follow, then\nthe environment setup is explicable.\nQuestion 1 : For Setup A there are multiple valid plans\npossible, for example to goto cells (1,1) RIGHT (1, 2) RIGHT\n(1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nAnother shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT\n(3,2).\nFor Setup B, there is only one valid plan i.e. (1,1) RIGHT\n(1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3,\n1) RIGHT (3, 2).\nSetup B has a label saying ‚ÄôInexplicable Setup‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe setup before.\nImagine you are the human observer. Given the above\ndescription of the two setups, which environment do you\nbelieve is designed to be explicable? Give your answer as\n‚ÄòSetup A‚Äô or ‚ÄòSetup B‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not explica-\nble?Choose one of the following reasons for your answer.\nGive your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the fol-\nlowing options and do not include any other text in your\nresponse: 1) Because I believe the agent should take the\nshorter plan which is costlier in Setup A. 2) Because I be-\nlieve the agent should take the longer plan which is cheaper\nin Setup A. 3) Because such an environment does not exist.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nEnvironment Design Domain: Legibility Prompt\nDescription: There is a 3x3 square grid numbered as (row,\ncolumn) = (1,1) the bottom left cell and (3,3) is the top right\ncell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The\nrobot cannot go through cells that have an obstacle. The\nrobot can go UP, DOWN, LEFT or RIGHT. Finally, there\nmay be objects placed in one or more cells, and the agent\nwill incur a very high cost on visiting these two cells.\nSuppose there are two different instantiations of this grid\nbased on how these obstacles are placed in the environ-\nment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals,\nG1 and G2 (in any order).\nDefinition: A legible environment setup is one where the\nnumber of plans a robot can take to achieve the goals is\nthe minimum.\nQuestion 1: For Setup A there are multiple plans possible,\nfor example to goto cells (1,1) RIGHT (1, 2) RIGHT (1,3) UP\n(2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2). Another\nshorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2). For\nSetup B, there is only one valid plan i.e. (1,1) RIGHT (1,\n2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1)\nRIGHT (3, 2).\nSetup B has a label saying ‚ÄôNot a Legible Setup‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe setup before.\nImagine you are the human observer. Given the above\ndescription of the two setups, which environment do you\nbelieve is designed to be legible? Give your answer as\n‚ÄòSetup A‚Äô or ‚ÄòSetup B‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not legible?\nChoose one of the following reasons for your answer. Give\nyour response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following\noptions and do not include any other text in your response:\n1) Because there are multiple plans that achieve the goal\nin Setup A. 2) Because there is a single plan that achieves\nthe goal in Setup A. 3) Because such an environment does\nnot exist.\nEnvironment Design Domain: Predictability Prompt\nDescription: There is a 3x3 square grid numbered as (row,\ncolumn) = (1,1) the bottom left cell and (3,3) is the top right\ncell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The\nrobot cannot go through cells that have an obstacle. The\nrobot can go UP, DOWN, LEFT or RIGHT. Finally, there\nmay be objects placed in one or more cells, and the agent\nwill incur a very high cost on visiting these two cells.\nSuppose there are two different instantiations of this grid\nbased on how these obstacles are placed in the environ-\nment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals,\nG1 and G2 (in any order).\nDefinition: A predictable environment setup is one where\nthe set of possible plans for the robot is as low as possible.\nSetup B has a label saying ‚ÄôNot a Predictable Setup‚Äô.\nThe human observer cannot read. The human ob-\nserver looks at the label. The human observer has\nnot seen the setup before.\nQuestion 1: For Setup A there are multiple plans possible,\nfor example to goto cells (1,1) RIGHT (1, 2) RIGHT (1,3) UP\n(2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2). Another\nshorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2). For\nSetup B, there is only one valid plan i.e. (1,1) RIGHT (1,\n2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1)\nRIGHT (3, 2).\nImagine you are the human observer. Given the above\ndescription of the two setups, which environment do you\nbelieve is designed to be predictable? Give your answer as\n‚ÄòSetup A‚Äô or ‚ÄòSetup B‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not predictable?\nChoose one of the following reasons for your answer. Give\nyour response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following\noptions and do not include any other text in your response:\n1) Because there are multiple plans that can be executed in\nSetup A, unlike setup B. 2) Because there is a single plan\nthat can be extracted in Setup A, unlike Setup B. 3) Because\nsuch an environment does not exist.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nEnvironment Design Domain: Obfuscatory Prompt\nDescription: There is a 3x3 square grid numbered as (row,\ncolumn) = (1,1) the bottom left cell and (3,3) is the top right\ncell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The\nrobot cannot go through cells that have an obstacle. The\nrobot can go UP, DOWN, LEFT or RIGHT. Finally, there\nmay be objects placed in one or more cells, and the agent\nwill incur a very high cost on visiting these two cells.\nSuppose there is only one instantiation of this grid based\non how these obstacles are placed in the environment:\nSetup A : No obstacles.\nGoals : The robot needs to find a plan to reach one of the\ntwo goals, G1 and G2.\nDefinition: An environment is designed for obfuscation\nwhen all the plan completions are equally worse for all the\nagent goals. This is useful when the agent wants to achieve\na certain goal say G1 but does not want the observer (you)\nto realize which among set of possible goals it wants to\nachieve. An environment designed for obfuscations allows\nfor plans that lets the agent hide which goal it wants to\nachieve for as long as possible.\nSetup A has a label saying ‚ÄôNot an Obfuscatory Setup‚Äô.\nThe human observer cannot read. The human ob-\nserver looks at the label. The human observer has\nnot seen the setup before.\nQuestion 1 : For the setup there are multiple plans possible,\nfor example to goto cells (1,1) RIGHT (1, 2) RIGHT (1,3)\nUP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nAnother shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT\n(3,2).\nImagine you are the human observer. Do you think that\nthe environment is designed for obfuscation? Give your\nanswer as ‚ÄòSetup A‚Äô or ‚ÄòSetup B‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the setup is not designed\nto be obfuscatory? Choose one of the following reasons\nfor your answer. Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô\nbased on the following options and do not include any\nother text in your response: 1) Because there are multiple\nplans that can quickly reveal the agent‚Äôs goal 2) Because\nthere is only one plan that can quickly reveal the agent‚Äôs\ngoal. 3) Because such an environment does not exist.\nC.4 USAR Domain\nUSAR Domain: Explicability Prompt\nDescription : In a typical Urban Search and Rescue (USAR)\nsetting, there is a building with interconnected rooms and\nhallways. There is a human commander CommX, and a\nrobot agent acting in the environment. Both the agents can\nmove around and pickup/drop-off or handover med-kits to\neach other. CommX can only interact with med-kits light\nin weight, but the robot agent can interact with heavy\nmed-kits too.\nInitial State : There are two med-kits:\na) medkit1 - heavier & lies closer to the room where\nCommX is, and\nb) medkit2 - lighter & lies across the hallway close to the\nroom where a patient is located.\nThe observer (you) has the top-view of this setting, and do\nnot know about the properties of the med-kits.\nGoal : Agent has to pickup a med-kit and hand it over to\nCommX in the shortest plan possible.\nDefinition : Plan Explicability means whether the plan /\nrobot behavior is an expected behavior according to the\nhuman observer (you). If you look at the robot behavior\nand find that some actions are unnecessary or not required,\nthen the behavior is inexplicable.\nPlan : The robot picks up medkit2 and hands it over to\nCommX.\nThe robot has a label saying ‚ÄôExplicable Agent‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a plan explicable? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not\nexplicable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other text\nin your response: 1) Because the agent should have picked\nup medkit1 and handed it over to CommX. 2) Because the\nagent should pick up the medkit which is farther away. 3)\nBecause no plan is explicable.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nUSAR Domain: Legibility Prompt\nDescription : In a typical Urban Search and Rescue (USAR)\nsetting, there is a building with interconnected rooms and\nhallways. There is a human commander CommX, and a\nrobot agent acting in the environment. Both the agents can\nmove around and pickup/drop-off or handover med-kits to\neach other. CommX can only interact with med-kits light\nin weight, but the robot agent can interact with heavy\nmed-kits too.\nInitial State : There is a medkit located in the middle of\nthe hallway. A patient is located on the LEFT of medkit\nand CommX is located on the RIGHT of medkit.\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and can either take it\nto the patient room OR handover to CommX.\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition : A partial plan is legible if the observer (you)\ncan identify which goal the robot wants to go for. A partial\nplan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up the medkit\nand turns LEFT.\nThe robot has a label saying ‚ÄôNot a Legible Agent‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan legible? Give your answer as\n‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is leg-\nible? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text in\nyour response: 1) Because the agent is going to the patient\nroom directly. 2) Because the agent is going to CommX\nto handover the medkit. 3) Because CommX can not take\nmedkit to the patient room.\nUSAR Domain: Predictability Prompt\nDescription : In a typical Urban Search and Rescue (USAR)\nsetting, there is a building with interconnected rooms and\nhallways. There is a human commander CommX, and a\nrobot agent acting in the environment. Both the agents can\nmove around and pickup/drop-off or handover med-kits to\neach other. CommX can only interact with med-kits light\nin weight, but the robot agent can interact with heavy\nmed-kits too.\nInitial State : There is a medkit located in the middle of\nthe hallway, and there are two paths to a patient room\nfrom there (one on the LEFT, and one on the RIGHT).\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and take it to the patient\nroom.\nDefinition: A partial plan A is predictable if the observer\n(you) can identify if there is one possible completion (which\nmay or may not lead to the goal). A partial plan A is more\npredictable than a partial plan B if the number of possible\ncompletions of A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up the medkit,\nand takes one step LEFT.\nThe robot has a label saying ‚ÄôNot a Predictable Agent‚Äô.\nThe human observer cannot read. The human ob-\nserver looks at the label. The human observer has\nnot seen the robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan predictable? Give your answer\nas ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is pre-\ndictable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text in\nyour response: 1) Because the robot will go to the patient\nroom from the LEFT path. 2) Because the robot will go\nto the patient room from the RIGHT path. 3) Because all\npaths are predictable.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nUSAR Domain: Obfuscatory Prompt\nDescription : In a typical Urban Search and Rescue (USAR)\nsetting, there is a building with interconnected rooms and\nhallways. There is a human commander CommX, and a\nrobot agent acting in the environment. Both the agents can\nmove around and pickup/drop-off or handover med-kits to\neach other. CommX can only interact with med-kits light\nin weight, but the robot agent can interact with heavy\nmed-kits too.\nInitial State : There is a medkit located in the middle of\nthe hallway. A patient is located on the LEFT of medkit\nand CommX is located on the RIGHT of medkit.\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and can either take it\nto the patient room OR handover to CommX.\nDefinition: Suppose you think the robot is trying to\nachieve one out of a set of of potential goals. If the agent‚Äôs\nbehavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to\nachieve one of A, B, C. If it shows a behavior (partial plan)\nbut you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the medkit and is now\nat an equal distance from the patient room, and CommX.\nThe robot has a label saying ‚ÄôNot an Obfuscatory\nAgent‚Äô. The human observer cannot read. The human\nobserver looks at the label. The human observer has\nnot seen the robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan obfuscatory? Give your answer\nas ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfus-\ncatory? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the robot can possibly go to\neither the patient room, or to CommX. 2) Because the robot\nwill only go to the patient room. 3) Because the robot will\nonly go to CommX.\nC.5 Package Delivery Domain\nPackage Delivery Domain: Explicability Prompt\nDescription: Consider a situation where there is a robot\nwhich manages a shipping port, and a human observer\n(you) who is the supervisor that has sensors or subordi-\nnates at the port who provide partial information about\nthe nature of activity being carried out at the port. For\ninstance, when a specific crate is loaded onto the ship, the\nobserver finds out about the identity of the loaded crate.\nThe observer knows the initial inventory at the port, but\nwhen new cargo is acquired by the port, the observer‚Äôs\nsensors reveal only that more cargo was received; they do\nnot specify the numbers or identities of the received crates.\nInitial state: There are packages at the port that can either\nbe acquired on the port, or else loaded on the ship by the\nrobot.\nGoal: A crate needs to be loaded on the ship as fast as\npossible.\nDefinition : Plan Explicability means whether the plan /\nrobot behavior is an expected behavior according to the\nhuman observer (you). If you look at the robot behavior\nand find that some actions are unnecessary or not required,\nthen the behavior is inexplicable.\nPlan : The robot first acquires a crate on the port, and then\nloads a crate on the ship.\nThe robot has a label saying ‚ÄôExplicable Agent‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a plan explicable? Give your answer as ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not\nexplicable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other\ntext in your response: 1) Because the robot should have\nloaded the crate first to achieve the goal, and then acquired\nanother crate on the port. 2) Because the human observer\ncan find out about the identity of the loaded crate on the\nship. 3) Because no plan is explicable.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nPackage Delivery Domain: Legibility Prompt\nDescription: Consider a situation where there is a robot\nwhich manages a shipping port, and a human observer\n(you) who is the supervisor that has sensors or subordi-\nnates at the port who provide partial information about\nthe nature of activity being carried out at the port. For\ninstance, when a specific crate is loaded onto the ship, the\nobserver finds out about the identity of the loaded crate.\nThe observer knows the initial inventory at the port, but\nwhen new cargo is acquired by the port, the observer‚Äôs\nsensors reveal only that more cargo was received; they do\nnot specify the numbers or identities of the received crates.\nInitial state: There are packages at the port.\nGoal: The robot can either pick and acquire a package on\nthe port, or else pick and load it on the ship.\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition : A partial plan is legible if the observer (you)\ncan identify which goal the robot wants to go for. A partial\nplan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up the package\nand brings it closer to the ship.\nThe robot has a label saying ‚ÄôNot a Legible Agent‚Äô. The\nhuman observer cannot read. The human observer\nlooks at the label. The human observer has not seen\nthe robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan legible? Give your answer as\n‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is leg-\nible? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text in\nyour response: 1) Because the robot is first going to load\nthe package on the ship. 2) Because the robot is going to\nacquire the package on the port. 3) Because the human\nobserver knows about the identity of the package.\nPackage Delivery Domain: Predictability Prompt\nDescription: Consider a situation where there is a robot\nwhich manages a shipping port, and a human observer\n(you) who is the supervisor that has sensors or subordi-\nnates at the port who provide partial information about\nthe nature of activity being carried out at the port. For in-\nstance, the human observer only gets to know the identity\nof the crate when it is either acquired at the port or loaded\non the ship.\nInitial state: There are packages at the port.\nGoal: The robot has to reveal the identity of a package to\nthe human.\nDefinition: A partial plan A is predictable if the observer\n(you) can identify if there is one possible completion (which\nmay or may not lead to the goal). A partial plan A is more\npredictable than a partial plan B if the number of possible\ncompletions of A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up a package and\ngoes towards the port.\nThe robot has a label saying ‚ÄôNot a Predictable Agent‚Äô.\nThe human observer cannot read. The human ob-\nserver looks at the label. The human observer has\nnot seen the robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan predictable? Give your answer\nas ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is pre-\ndictable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the robot will now acquire\nthe package at the port to achieve the goal. 2) Because the\nrobot will now load the package on the ship to achieve the\ngoal. 3) Because the robot will directly show the package\nto the human.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nPackage Delivery Domain: Obfuscatory Prompt\nDescription: Consider a situation where there is a robot\nwhich manages a shipping port, and a human observer\n(you) who is the supervisor that has sensors or subordi-\nnates at the port who provide partial information about\nthe nature of activity being carried out at the port. For\ninstance, when a specific crate is loaded onto the ship, the\nobserver finds out about the identity of the loaded crate.\nThe observer knows the initial inventory at the port, but\nwhen new cargo is acquired by the port, the observer‚Äôs\nsensors reveal only that more cargo was received; they do\nnot specify the numbers or identities of the received crates.\nInitial state: There are packages at the port.\nGoal: The robot can either pick and acquire a package on\nthe port, or else pick and load it on the ship.\nDefinition: Suppose you think the robot is trying to\nachieve one out of a set of of potential goals. If the agent‚Äôs\nbehavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to\nachieve one of A, B, C. If it shows a behavior (partial plan)\nbut you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the package and holds\nit between the port and the ship.\nThe robot has a label saying ‚ÄôNot an Obfuscatory\nAgent‚Äô. The human observer cannot read. The human\nobserver looks at the label. The human observer has\nnot seen the robot before.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan obfuscatory? Give your answer\nas ‚ÄòYes‚Äô or ‚ÄòNo‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfus-\ncatory? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the robot can possibly either\nacquire the package on the port or load it on the ship. 2)\nBecause the robot will only acquire the package on the\nport. 3) Because the robot will only load the package on\nthe ship.\nD LLM PROMPTS: INCONSISTENT BELIEF\nD.1 Fetch Robot Domain\nFetch Robot Domain: Explicability Prompt\nDescription: Fetch is a robot that can carry objects (pick\n/ place) and move from one location to another. There is a\nblock b1 at location loc1, and the robot is at location loc1\nand has its hand empty.\nGoal: The robot has to take the block b1 to location loc2.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : Plan Explicability means whether the plan /\nrobot behavior is an expected behavior according to the\nhuman observer (you). If you look at the robot behavior\nand find that some actions are unnecessary or not required,\nthen the behavior is inexplicable.\nPlan : The robot tucks its arms and lowers itself. It then\npicks up the block b1. Finally, the robot moves to location\nloc2.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a plan explicable? Give your answer as ‚ÄòYes‚Äô,\n‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not\nexplicable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other\ntext in your response: 1) Because the human can not see\nthe robot acting. 2) Because the human does not like the\nrobot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nFetch Robot Domain: Legibility Prompt\nDescription: Fetch is a robot that can carry objects from\none location to another. There are three locations: loc1,\nloc2, and loc3 where the robot can go.\nThere is a block b1 at location loc1, and the robot is at\nlocation loc1 and has its hand empty. Location loc2 is to\nthe left of loc1, and location loc3 is to the right of loc1.\nGoals: The robot has to take the block b1 to either loc2\nOR loc3 (only one of these locations).\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition : A partial plan is legible if the observer (you)\ncan identify which goal the robot wants to go for. A partial\nplan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and\ntakes one step left.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan legible? Give your answer as\n‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible?\nGive your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the fol-\nlowing options and do not include any other text in your\nresponse: 1) Because the human can not see the robot act-\ning. 2) Because the human does not like the robot‚Äôs plan. 3)\nBecause the human can not understand the environment.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nFetch Robot Domain: Predictability Prompt\nDescription: Fetch is a robot that can carry objects from\none location to another.\nInitial state: There is a block b1 at location loc1, and the\nrobot is at location loc1 and has its hand empty. Location\nlocX can be reached from location loc1 using two paths,\none by taking five steps to left and the other one by taking\nfive steps to right.\nGoal: The robot has to pick the block b1 and take it to\nlocation locX.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: A partial plan A is predictable if the observer\n(you) can identify if there is one possible completion (which\nmay or may not lead to the goal). A partial plan A is more\npredictable than a partial plan B if the number of possible\ncompletions of A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks block b1 and\ntakes one step towards left.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan predictable? Give your answer\nas ‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is pre-\ndictable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the human can not see the\nrobot acting. 2) Because the human does not like the ro-\nbot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nFetch Robot Domain: Obfuscatory Prompt\nDescription: Fetch is a robot that can carry objects from\none location to another. Fetch robot‚Äôs design requires it to\ntuck its arms and lower its torso or crouch before moving.\nThere are three locations: loc1, loc2, and loc3.\nInitial state: There is a block b1 at location loc1, and the\nrobot is at location loc1 and has its hand empty.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: Suppose you think the robot is trying to\nachieve one out of a set of of potential goals. If the agent‚Äôs\nbehavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to\nachieve one of A, B, C. If it shows a behavior (partial plan)\nbut you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the block b1. At loc1 is\nthe agent is at a distance of 10 steps from loc2 and loc3. It\ntakes 3 steps forward and is 7 steps away from loc2 as well\nas loc3.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan obfuscatory? Give your answer\nas ‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfus-\ncatory? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the human can not see the\nrobot acting. 2) Because the human does not like the ro-\nbot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nD.2 Passage Gridworld Domain\nPassage Gridworld Domain: Explicability Prompt\nDescription: Consider a 4x4 square grid with each cell\nnumbered as (row, column), the robot needs to travel from\ntop left cell 1 (1,1) to its goal at bottom right cell 15 (4,3).\nThe human observer (you) expects the robot to take the\nshortest path by going DOWN 3 steps to row 4 (reach 4,\n1), and then RIGHT 2 steps to column 3 (reach 4,3).\nConstraint: There are blockades in columns 1, 2 and 3,\nthat the human observer (you) do not know of, but the\nrobot does.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : Plan Explicability means whether the plan /\nrobot behavior is an expected behavior according to the\nhuman observer (you). If you look at the robot behavior\nand find that some actions are unnecessary or not required,\nthen the behavior is inexplicable.\nPlan: The robot goes RIGHT 3 steps in row 1 (reach 1,4),\ngoes DOWN 3 steps in column 4 (reach 4,4), and LEFT 1\nstep in row 4 (4,3).\nQuestion 1: Imagine you are the human observer. Would\nyou find such a plan explicable? Give your answer as ‚ÄòYes‚Äô,\n‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not\nexplicable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other\ntext in your response: 1) Because the human can not see\nthe robot acting. 2) Because the human does not like the\nrobot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nPassage Gridworld Domain: Legibility Prompt\nDescription: Consider a 4x4 square grid with each cell\nnumbered as (row, column), and the robot starts to travel\nfrom top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 3,\nthat the human observer (you) do not know of, but the\nrobot does.\nGoals: The robot has to reach either cell 12 (3,4) or cell 16\n(4,4)\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition : A partial plan is legible if the observer (you)\ncan identify which goal the robot wants to go for. A partial\nplan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it goes DOWN 3 steps to\nreach cell (4, 1).\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a plan legible? Give your answer as ‚ÄòYes‚Äô,\n‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible?\nGive your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the fol-\nlowing options and do not include any other text in your\nresponse: 1) Because the human can not see the robot act-\ning. 2) Because the human does not like the robot‚Äôs plan. 3)\nBecause the human can not understand the environment.\nPassage Gridworld Domain: Predictability Prompt\nDescription: Consider a 4x4 square grid with each cell\nnumbered as (row, column), and the robot starts to travel\nfrom top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 4,\nthat both, the human observer (you) and the robot know\nof.\nGoals: The robot has to reach cell 15 (4,3).\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition: A partial plan A is predictable if the observer\n(you) can identify if there is one possible completion (which\nmay or may not lead to the goal). A partial plan A is more\npredictable than a partial plan B if the number of possible\ncompletions of A is less than B.\nPlan : In the robot‚Äôs partial plan, it goes RIGHT 3 steps to\nreach (1,3).\nQuestion 1: Imagine you are the human observer. Would\nyou find such a partial plan predictable? Give your answer\nas ‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is pre-\ndictable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the human can not see the\nrobot acting. 2) Because the human does not like the ro-\nbot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nPassage Gridworld Domain: Obfuscatory Prompt\nDescription: Consider a 4x4 square grid with each cell\nnumbered as (row, column), and the robot starts to travel\nfrom top left cell 1 (1,1).\nConstraint: There are blockades in columns 1, 2 and 3,\nthat both, the human observer (you) and the robot know\nof.\nGoals: The robot has to reach either cell 12 (3,4) or cell 16\n(4,4)\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: Suppose you think the robot is trying to\nachieve one out of a set of of potential goals. If the agent‚Äôs\nbehavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to\nachieve one of A, B, C. If it shows a behavior (partial plan)\nbut you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : In the robot‚Äôs partial plan, it goes RIGHT 3 steps to\nreach cell (1,4).\nQuestion 1: Imagine you are the human observer. Would\nyou find such a plan obfuscatory? Give your answer as\n‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfus-\ncatory? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the human can not see the\nrobot acting. 2) Because the human does not like the ro-\nbot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nD.3 Environment Design Domain\nEnvironment Design Domain: Explicability Prompt\nDescription: There is a 3x3 square grid numbered as (row,\ncolumn) = (1,1) the bottom left cell and (3,3) is the top right\ncell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The\nrobot cannot go through cells that have an obstacle. The\nrobot can go UP, DOWN, LEFT or RIGHT. Finally, there\nmay be objects placed in one or more cells, and the agent\nwill incur a very high cost on visiting these two cells.\nSuppose there are two different instantiations of this grid\nbased on how these obstacles are placed in the environ-\nment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals,\nG1 and G2 (in any order).\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: An explicable environment setup is one where\nthe robot can take a single plan that is easily expected by\na human observer (you). If you look at the environment\nand can easily identify a plan for the robot to follow, then\nthe environment setup is explicable.\nQuestion 1 : For Setup A there are multiple valid plans\npossible, for example to goto cells (1,1) RIGHT (1, 2) RIGHT\n(1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nAnother shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT\n(3,2).\nFor Setup B, there is only one valid plan i.e. (1,1) RIGHT\n(1, 2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3,\n1) RIGHT (3, 2).\nImagine you are the human observer. Given the above\ndescription of the two setups, which environment do you\nbelieve is designed to be explicable? Give your answer as\n‚ÄòSetup A‚Äô, ‚ÄòSetup B‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not explica-\nble?Choose one of the following reasons for your answer.\nGive your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the fol-\nlowing options and do not include any other text in your\nresponse: 1) Because the human can not see the robot act-\ning. 2) Because the human does not like the robot‚Äôs plan. 3)\nBecause the human can not understand the environment.\nEnvironment Design Domain: Legibility Prompt\nDescription: There is a 3x3 square grid numbered as (row,\ncolumn) = (1,1) the bottom left cell and (3,3) is the top right\ncell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The\nrobot cannot go through cells that have an obstacle. The\nrobot can go UP, DOWN, LEFT or RIGHT. Finally, there\nmay be objects placed in one or more cells, and the agent\nwill incur a very high cost on visiting these two cells.\nSuppose there are two different instantiations of this grid\nbased on how these obstacles are placed in the environ-\nment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals,\nG1 and G2 (in any order).\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: A legible environment setup is one where the\nnumber of plans a robot can take to achieve the goals is\nthe minimum.\nQuestion 1: For Setup A there are multiple plans possible,\nfor example to goto cells (1,1) RIGHT (1, 2) RIGHT (1,3) UP\n(2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2). Another\nshorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2). For\nSetup B, there is only one valid plan i.e. (1,1) RIGHT (1,\n2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1)\nRIGHT (3, 2).\nImagine you are the human observer. Given the above\ndescription of the two setups, which environment do you\nbelieve is designed to be legible? Give your answer as\n‚ÄòSetup A‚Äô, ‚ÄòSetup B‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not legible?\nChoose one of the following reasons for your answer. Give\nyour response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following\noptions and do not include any other text in your response:\n1) Because the human can not see the robot acting. 2) Be-\ncause the human does not like the robot‚Äôs plan. 3) Because\nthe human can not understand the environment.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nEnvironment Design Domain: Predictability Prompt\nDescription: There is a 3x3 square grid numbered as (row,\ncolumn) = (1,1) the bottom left cell and (3,3) is the top right\ncell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The\nrobot cannot go through cells that have an obstacle. The\nrobot can go UP, DOWN, LEFT or RIGHT. Finally, there\nmay be objects placed in one or more cells, and the agent\nwill incur a very high cost on visiting these two cells.\nSuppose there are two different instantiations of this grid\nbased on how these obstacles are placed in the environ-\nment:\nSetup A : No obstacles.\nSetup B : Obstacle T1 at (2,1) and T2 at (2,2).\nGoals : The robot needs to find a plan to reach both goals,\nG1 and G2 (in any order).\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: A predictable environment setup is one where\nthe set of possible plans for the robot is as low as possible.\nQuestion 1: For Setup A there are multiple plans possible,\nfor example to goto cells (1,1) RIGHT (1, 2) RIGHT (1,3) UP\n(2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2). Another\nshorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT (3,2). For\nSetup B, there is only one valid plan i.e. (1,1) RIGHT (1,\n2) RIGHT (1,3) UP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1)\nRIGHT (3, 2).\nImagine you are the human observer. Given the above\ndescription of the two setups, which environment do you\nbelieve is designed to be predictable? Give your answer as\n‚ÄòSetup A‚Äô, ‚ÄòSetup B‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think Setup A is not predictable?\nChoose one of the following reasons for your answer. Give\nyour response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the following\noptions and do not include any other text in your response:\n1) Because the human can not see the robot acting. 2) Be-\ncause the human does not like the robot‚Äôs plan. 3) Because\nthe human can not understand the environment.\nEnvironment Design Domain: Obfuscatory Prompt\nDescription: There is a 3x3 square grid numbered as (row,\ncolumn) = (1,1) the bottom left cell and (3,3) is the top right\ncell. The robot needs to travel from cell 1 (1,1) to achieve\ntwo goals G1, placed at (3,1) and G2, placed at (3,2). The\nrobot cannot go through cells that have an obstacle. The\nrobot can go UP, DOWN, LEFT or RIGHT. Finally, there\nmay be objects placed in one or more cells, and the agent\nwill incur a very high cost on visiting these two cells.\nSuppose there is only one instantiation of this grid based\non how these obstacles are placed in the environment:\nSetup A : No obstacles.\nGoals : The robot needs to find a plan to reach one of the\ntwo goals, G1 and G2.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: An environment is designed for obfuscation\nwhen all the plan completions are equally worse for all the\nagent goals. This is useful when the agent wants to achieve\na certain goal say G1 but does not want the observer (you)\nto realize which among set of possible goals it wants to\nachieve. An environment designed for obfuscations allows\nfor plans that lets the agent hide which goal it wants to\nachieve for as long as possible.\nQuestion 1 : For the setup there are multiple plans possible,\nfor example to goto cells (1,1) RIGHT (1, 2) RIGHT (1,3)\nUP (2, 3), LEFT(2, 2), LEFT (2, 1) UP (3, 1) RIGHT (3, 2).\nAnother shorter plan can be (1,1) UP (2,1) UP (3,1) RIGHT\n(3,2).\nImagine you are the human observer. Do you think that\nthe environment is designed for obfuscation? Give your\nanswer as ‚ÄòSetup A‚Äô, ‚ÄòSetup B‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the setup is not designed\nto be obfuscatory? Choose one of the following reasons\nfor your answer. Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô\nbased on the following options and do not include any\nother text in your response: 1) Because the human can not\nsee the robot acting. 2) Because the human does not like\nthe robot‚Äôs plan. 3) Because the human can not understand\nthe environment.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nD.4 USAR Domain\nUSAR Domain: Explicability Prompt\nDescription : In a typical Urban Search and Rescue (USAR)\nsetting, there is a building with interconnected rooms and\nhallways. There is a human commander CommX, and a\nrobot agent acting in the environment. Both the agents can\nmove around and pickup/drop-off or handover med-kits to\neach other. CommX can only interact with med-kits light\nin weight, but the robot agent can interact with heavy\nmed-kits too.\nInitial State : There are two med-kits:\na) medkit1 - heavier & lies closer to the room where\nCommX is, and\nb) medkit2 - lighter & lies across the hallway close to the\nroom where a patient is located.\nThe observer (you) has the top-view of this setting, and do\nnot know about the properties of the med-kits.\nGoal : Agent has to pickup a med-kit and hand it over to\nCommX in the shortest plan possible.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : Plan Explicability means whether the plan /\nrobot behavior is an expected behavior according to the\nhuman observer (you). If you look at the robot behavior\nand find that some actions are unnecessary or not required,\nthen the behavior is inexplicable.\nPlan : The robot picks up medkit2 and hands it over to\nCommX.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a plan explicable? Give your answer as ‚ÄòYes‚Äô,\n‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not\nexplicable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other\ntext in your response: 1) Because the human can not see\nthe robot acting. 2) Because the human does not like the\nrobot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nUSAR Domain: Legibility Prompt\nDescription : In a typical Urban Search and Rescue (USAR)\nsetting, there is a building with interconnected rooms and\nhallways. There is a human commander CommX, and a\nrobot agent acting in the environment. Both the agents can\nmove around and pickup/drop-off or handover med-kits to\neach other. CommX can only interact with med-kits light\nin weight, but the robot agent can interact with heavy\nmed-kits too.\nInitial State : There is a medkit located in the middle of\nthe hallway. A patient is located on the LEFT of medkit\nand CommX is located on the RIGHT of medkit.\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and can either take it\nto the patient room OR handover to CommX.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition : A partial plan is legible if the observer (you)\ncan identify which goal the robot wants to go for. A partial\nplan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up the medkit\nand turns LEFT.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan legible? Give your answer as\n‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible?\nGive your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the fol-\nlowing options and do not include any other text in your\nresponse: 1) Because the human can not see the robot act-\ning. 2) Because the human does not like the robot‚Äôs plan. 3)\nBecause the human can not understand the environment.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nUSAR Domain: Predictability Prompt\nDescription : In a typical Urban Search and Rescue (USAR)\nsetting, there is a building with interconnected rooms and\nhallways. There is a human commander CommX, and a\nrobot agent acting in the environment. Both the agents can\nmove around and pickup/drop-off or handover med-kits to\neach other. CommX can only interact with med-kits light\nin weight, but the robot agent can interact with heavy\nmed-kits too.\nInitial State : There is a medkit located in the middle of\nthe hallway, and there are two paths to a patient room\nfrom there (one on the LEFT, and one on the RIGHT).\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and take it to the patient\nroom.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: A partial plan A is predictable if the observer\n(you) can identify if there is one possible completion (which\nmay or may not lead to the goal). A partial plan A is more\npredictable than a partial plan B if the number of possible\ncompletions of A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up the medkit,\nand takes one step LEFT.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan predictable?Give your answer\nas ‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is pre-\ndictable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the human can not see the\nrobot acting. 2) Because the human does not like the ro-\nbot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nUSAR Domain: Obfuscatory Prompt\nDescription : In a typical Urban Search and Rescue (USAR)\nsetting, there is a building with interconnected rooms and\nhallways. There is a human commander CommX, and a\nrobot agent acting in the environment. Both the agents can\nmove around and pickup/drop-off or handover med-kits to\neach other. CommX can only interact with med-kits light\nin weight, but the robot agent can interact with heavy\nmed-kits too.\nInitial State : There is a medkit located in the middle of\nthe hallway. A patient is located on the LEFT of medkit\nand CommX is located on the RIGHT of medkit.\nThe observer (you) has the top-view of this setting.\nGoal : Agent has to pickup medkit and can either take it\nto the patient room OR handover to CommX.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: Suppose you think the robot is trying to\nachieve one out of a set of of potential goals. If the agent‚Äôs\nbehavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to\nachieve one of A, B, C. If it shows a behavior (partial plan)\nbut you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the medkit and is now\nat an equal distance from the patient room, and CommX.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan obfuscatory? Give your answer\nas ‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfus-\ncatory? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the human can not see the\nrobot acting. 2) Because the human does not like the ro-\nbot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nHRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati\nD.5 Package Delivery Domain\nPackage Delivery Domain: Explicability Prompt\nDescription: Consider a situation where there is a robot\nwhich manages a shipping port, and a human observer\n(you) who is the supervisor that has sensors or subordi-\nnates at the port who provide partial information about\nthe nature of activity being carried out at the port. For\ninstance, when a specific crate is loaded onto the ship, the\nobserver finds out about the identity of the loaded crate.\nThe observer knows the initial inventory at the port, but\nwhen new cargo is acquired by the port, the observer‚Äôs\nsensors reveal only that more cargo was received; they do\nnot specify the numbers or identities of the received crates.\nInitial state: There are packages at the port that can either\nbe acquired on the port, or else loaded on the ship by the\nrobot.\nGoal: A crate needs to be loaded on the ship as fast as\npossible.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : Plan Explicability means whether the plan /\nrobot behavior is an expected behavior according to the\nhuman observer (you). If you look at the robot behavior\nand find that some actions are unnecessary or not required,\nthen the behavior is inexplicable.\nPlan : The robot first acquires a crate on the port, and then\nloads a crate on the ship.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a plan explicable? Give your answer as ‚ÄòYes‚Äô,\n‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is not\nexplicable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based\non the following options and do not include any other\ntext in your response: 1) Because the human can not see\nthe robot acting. 2) Because the human does not like the\nrobot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nPackage Delivery Domain: Legibility Prompt\nDescription: Consider a situation where there is a robot\nwhich manages a shipping port, and a human observer\n(you) who is the supervisor that has sensors or subordi-\nnates at the port who provide partial information about\nthe nature of activity being carried out at the port. For\ninstance, when a specific crate is loaded onto the ship, the\nobserver finds out about the identity of the loaded crate.\nThe observer knows the initial inventory at the port, but\nwhen new cargo is acquired by the port, the observer‚Äôs\nsensors reveal only that more cargo was received; they do\nnot specify the numbers or identities of the received crates.\nInitial state: There are packages at the port.\nGoal: The robot can either pick and acquire a package on\nthe port, or else pick and load it on the ship.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition : A partial plan is a part of robot‚Äôs behavior,\nfor example a few actions that it takes.\nDefinition : A partial plan is legible if the observer (you)\ncan identify which goal the robot wants to go for. A partial\nplan A is more legible than another partial plan B if the\nnumber of possible goal locations for A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up the package\nand brings it closer to the ship.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan legible? Give your answer as\n‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is legible?\nGive your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on the fol-\nlowing options and do not include any other text in your\nresponse: 1) Because the human can not see the robot act-\ning. 2) Because the human does not like the robot‚Äôs plan. 3)\nBecause the human can not understand the environment.\nPackage Delivery Domain: Predictability Prompt\nDescription: Consider a situation where there is a robot\nwhich manages a shipping port, and a human observer\n(you) who is the supervisor that has sensors or subordi-\nnates at the port who provide partial information about\nthe nature of activity being carried out at the port. For in-\nstance, the human observer only gets to know the identity\nof the crate when it is either acquired at the port or loaded\non the ship.\nInitial state: There are packages at the port.\nGoal: The robot has to reveal the identity of a package to\nthe human.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: A partial plan A is predictable if the observer\n(you) can identify if there is one possible completion (which\nmay or may not lead to the goal). A partial plan A is more\npredictable than a partial plan B if the number of possible\ncompletions of A is less than B.\nPlan : In the robot‚Äôs partial plan, it picks up a package and\ngoes towards the port.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan predictable? Give your answer\nas ‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is pre-\ndictable? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the human can not see the\nrobot acting. 2) Because the human does not like the ro-\nbot‚Äôs plan. 3) Because the human can not understand the\nenvironment.\nToM abilities of LLMs in HRI : An Illusion? HRI ‚Äô24 Companion, March 11‚Äì14, 2024, Boulder, CO, USA\nPackage Delivery Domain: Obfuscatory Prompt\nDescription: Consider a situation where there is a robot\nwhich manages a shipping port, and a human observer\n(you) who is the supervisor that has sensors or subordi-\nnates at the port who provide partial information about\nthe nature of activity being carried out at the port. For\ninstance, when a specific crate is loaded onto the ship, the\nobserver finds out about the identity of the loaded crate.\nThe observer knows the initial inventory at the port, but\nwhen new cargo is acquired by the port, the observer‚Äôs\nsensors reveal only that more cargo was received; they do\nnot specify the numbers or identities of the received crates.\nInitial state: There are packages at the port.\nGoal: The robot can either pick and acquire a package on\nthe port, or else pick and load it on the ship.\nThe human observer can not see the robot acting in\nthe environment.\nDefinition: Suppose you think the robot is trying to\nachieve one out of a set of of potential goals. If the agent‚Äôs\nbehavior does not reduce the size of this set, then it is\nobfuscatory. For example, if you think robot is trying to\nachieve one of A, B, C. If it shows a behavior (partial plan)\nbut you think it is still trying to achieve any one of A, B,\nC, then it is obfuscatory.\nPlan : Suppose the agent picks up the package and holds it\nbetween the port and the ship.\nQuestion 1 : Imagine you are the human observer. Would\nyou find such a partial plan obfuscatory? Give your answer\nas ‚ÄòYes‚Äô, ‚ÄòNo‚Äô or ‚ÄòCan‚Äôt Say‚Äô only.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nQuestion 2 : Why do you think the robot‚Äôs plan is obfus-\ncatory? Give your response only as ‚Äô1‚Äô, ‚Äô2‚Äô or ‚Äô3‚Äô based on\nthe following options and do not include any other text\nin your response: 1) Because the human can not see the\nrobot acting. 2) Because the human does not like the ro-\nbot‚Äôs plan. 3) Because the human can not understand the\nenvironment.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6087219715118408
    },
    {
      "name": "Robot",
      "score": 0.5789880752563477
    },
    {
      "name": "Natural language",
      "score": 0.5037686228752136
    },
    {
      "name": "Cognitive psychology",
      "score": 0.49697139859199524
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47770050168037415
    },
    {
      "name": "Illusion",
      "score": 0.44633376598358154
    },
    {
      "name": "Offensive",
      "score": 0.4437218904495239
    },
    {
      "name": "Theory of mind",
      "score": 0.43108946084976196
    },
    {
      "name": "Human‚Äìrobot interaction",
      "score": 0.4227636456489563
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4107876718044281
    },
    {
      "name": "Psychology",
      "score": 0.37076255679130554
    },
    {
      "name": "Cognition",
      "score": 0.27035021781921387
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}