{
  "title": "LGViT: Dynamic Early Exiting for Accelerating Vision Transformer",
  "url": "https://openalex.org/W4385958936",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2678439393",
      "name": "Xu Guanyu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3087075383",
      "name": "Hao Jiawei",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2116106766",
      "name": "Shen Li",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2117015939",
      "name": "Hu Han",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2117314689",
      "name": "Luo Yong",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2029253636",
      "name": "Lin Hui",
      "affiliations": [
        "China Academy of Information and Communications Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2499614443",
      "name": "Shen, Jialie",
      "affiliations": [
        "University of London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W12634471",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W4312468136",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4304080501",
    "https://openalex.org/W4287101401",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4320036918",
    "https://openalex.org/W4226224676",
    "https://openalex.org/W3136363192",
    "https://openalex.org/W4363672099",
    "https://openalex.org/W4312340826",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W4304099185",
    "https://openalex.org/W4304091794",
    "https://openalex.org/W4313069943",
    "https://openalex.org/W3137609883",
    "https://openalex.org/W3154971029",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W4226136917",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W4224903119",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2747329762",
    "https://openalex.org/W4287121978",
    "https://openalex.org/W4376983087",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W4287887900",
    "https://openalex.org/W2944701285",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W4287110835",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W4225871896"
  ],
  "abstract": "Recently, the efficient deployment and acceleration of powerful vision transformers (ViTs) on resource-limited edge devices for providing multimedia services have become attractive tasks. Although early exiting is a feasible solution for accelerating inference, most works focus on convolutional neural networks (CNNs) and transformer models in natural language processing (NLP).Moreover, the direct application of early exiting methods to ViTs may result in substantial performance degradation. To tackle this challenge, we systematically investigate the efficacy of early exiting in ViTs and point out that the insufficient feature representations in shallow internal classifiers and the limited ability to capture target semantic information in deep internal classifiers restrict the performance of these methods. We then propose an early exiting framework for general ViTs termed LGViT, which incorporates heterogeneous exiting heads, namely, local perception head and global aggregation head, to achieve an efficiency-accuracy trade-off. In particular, we develop a novel two-stage training scheme, including end-to-end training and self-distillation with the backbone frozen to generate early exiting ViTs, which facilitates the fusion of global and local information extracted by the two types of heads. We conduct extensive experiments using three popular ViT backbones on three vision datasets. Results demonstrate that our LGViT can achieve competitive performance with approximately 1.8 $\\times$ speed-up.",
  "full_text": "LGViT: Dynamic Early Exiting for Accelerating Vision\nTransformer\nGuanyu Xuâˆ—\nBeijing Institute of Technology\nBeijing, China\nxuguanyu@bit.edu.cn\nJiawei Haoâˆ—\nBeijing Institute of Technology\nBeijing, China\nhaojiawei7@bit.edu.cn\nLi Shen\nJD Explore Academy\nBeijing, China\nmathshenli@gmail.com\nHan Huâ€ \nBeijing Institute of Technology\nBeijing, China\nhhu@bit.edu.cn\nYong Luo\nWuhan University\nWuhan, China\nluoyong@whu.edu.cn\nHui Lin\nChina Academic of Electronics and\nInformation Technology\nBeijing, China\nlinhui@whu.edu.cn\nJialie Shen\nCity, University of London\nLondon, U.K.\njialie@gmail.com\nABSTRACT\nRecently, the efficient deployment and acceleration of powerful\nvision transformers (ViTs) on resource-limited edge devices for pro-\nviding multimedia services have become attractive tasks. Although\nearly exiting is a feasible solution for accelerating inference, most\nworks focus on convolutional neural networks (CNNs) and trans-\nformer models in natural language processing (NLP). Moreover, the\ndirect application of early exiting methods to ViTs may result in\nsubstantial performance degradation. To tackle this challenge, we\nsystematically investigate the efficacy of early exiting in ViTs and\npoint out that the insufficient feature representations in shallow\ninternal classifiers and the limited ability to capture target semantic\ninformation in deep internal classifiers restrict the performance of\nthese methods. We then propose an early exiting framework for\ngeneral ViTs termed LGViT, which incorporates heterogeneous\nexiting heads, namely, local perception head and global aggregation\nhead, to achieve an efficiency-accuracy trade-off. In particular, we\ndevelop a novel two-stage training scheme, including end-to-end\ntraining and self-distillation with the backbone frozen to generate\nearly exiting ViTs, which facilitates the fusion of global and local\ninformation extracted by the two types of heads. We conduct ex-\ntensive experiments using three popular ViT backbones on three\nvision datasets. Results demonstrate that our LGViT can achieve\ncompetitive performance with approximately 1.8 Ã—speed-up.\nâˆ—Both authors contributed equally to this research.\nâ€ Han Hu is the corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0108-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3581783.3611762\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Computer vision tasks .\nKEYWORDS\nVision transformer, early exit, heterogeneous exiting heads, self-\ndistillation\nACM Reference Format:\nGuanyu Xu, Jiawei Hao, Li Shen, Han Hu, Yong Luo, Hui Lin, and Jialie Shen.\n2023. LGViT: Dynamic Early Exiting for Accelerating Vision Transformer.\nIn Proceedings of the 31st ACM International Conference on Multimedia (MM\nâ€™23), October 29â€“November 3, 2023, Ottawa, ON, Canada. ACM, New York,\nNY, USA, 12 pages. https://doi.org/10.1145/3581783.3611762\n1 INTRODUCTION\nDuring the past few years, vision transformers (ViTs) have become\nfundamental backbones for various multimedia tasks due to their\npowerful performance and universal structures [7, 30, 33]. With the\ndevelopment of 5G wireless networks and the artificial intelligence\nof things (AIoT), deploying ViTs on resource-constrained edge de-\nvices to enable real-time multimedia applications has become an\nappealing prospect. However, the high computational complexity\nof ViTs poses a significant challenge to deploy them on edge de-\nvices. For example, ViT-L/16 [ 26], a typical ViT architecture for\ncomputer vision, requires over 180 giga FLOPs for inference and\ntakes 56.79 milliseconds on an NVIDIA Jetson TX2 device to classify\nan image with 224 Ã—224 resolution. Given that performance and\nquality-of-service (QoS) are critical for real-time multimedia sys-\ntems, deploying such latency-greedy ViTs on resource-constrained\nedge devices is a challenging task.\nEarly exiting provides a feasible solution for accelerating the\ninference of neural networks by terminating forward propagation\nonce the prediction from internal classifiers satisfies a certain cri-\nterion. While early exiting has been extensively studied for CNNs\nand transformer models in NLP, its application to ViTs remains\nan open problem. The main challenges in developing an efficient\narXiv:2308.00255v1  [cs.CV]  1 Aug 2023\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Xu et al.\n+0.19Ã—\nSpeed-Up\n+3.6%\nAccuracy\nFigure 1: The comparison of performance and efficiency\ntrade-off for the ViT backbone in CIFAR-100. LGViT signifi-\ncantly outperforms other early exiting methods. In particu-\nlar, LGViT achieves new state-of-the-art 89.1 % accuracy but\nfaster than ViT-EE [ 1]. Details are in Table 1 and Section 4.2.\nearly exiting framework for ViTs can be condensed into three key\naspects. Firstly, directly applying early exiting strategies on ViTs\nleads to substantial performance degradation. However, there has\nbeen no systematic investigation into what limits the performance.\nSecondly, minimizing the accuracy drop and further accelerating\nthe inference of early exiting ViTs on edge devices is challenging.\nLastly, in the training phase, internal classifiers lose considerable\ninformation from the final classifier, resulting in poor performance.\nRegarding the first challenge, Kaya et al. [9] discovered CNNs\ncan reach correct predictions before their final layer, and they in-\ntroduced internal classifiers to mitigate the overthinking problem.\nSajjad et al. [20] examined the impact of dropping layers in trans-\nformer models and found that lower layers are more critical for\ntask performance. However, their analyses were limited to CNNs\nor transformer models and did not consider the constraints of early\nexiting methods in ViTs. Concerning the second challenge, a se-\nries of studies have introduced exiting criteria to determine when\nto terminate forward propagation [22, 36] or designed advanced\nbackbone networks to balance performance and efficiency [25, 28].\nAlthough Bakhtiarnia et al. [1] proposed an early exiting frame-\nwork for ViT by incorporating additional backbone blocks as exiting\nheads, a considerable speed-up gap remains between these meth-\nods and the constraints imposed by mobile and edge platforms. It\nis advantageous to design efficient exiting heads for constructing\nearly exiting ViTs with rich feature representations. In relation to\nthe third challenge, distillation-based [14, 35] approaches provide\na promising solution to help internal classifiers imitate the final\nclassifiers. However, these methods are only available to the same\nearly exiting head architectures.\nTo remedy these limitations, we initially conduct probing ex-\nperiments to examine the direct application of early exiting meth-\nods in ViTs. We discover that the performance of early exiting is\nconstrained by: i) inadequate feature representations in shallow\ninternal classifiers; ii) the weak ability to capture target semantic\ninformation in deep internal classifiers. Building on these insights,\nwe then propose an efficient early exiting framework for general\nViTs, termed LGViT, which accelerates inference while maintaining\nalmost the same accuracy. In LGViT, we incorporate heterogeneous\nexiting heads, specifically, the local perception head and global\naggregation head, to generate early exiting ViT networks. The local\nperception head is attached to shallow exiting points to capture\nlocal information and learn sufficient feature representations. Con-\nversely, the global aggregation head is connected to deep exiting\npoints to extract global information, thereby enhancing the capture\nof target semantic feature. To the best of our knowledge, this is\nthe first work to employ heterogeneous exiting heads for early\nexiting ViTs. Subsequently, we propose a novel two-stage training\nstrategy for early exiting ViTs. During the first stage, we utilize\nan end-to-end method to help the backbone ViT achieve its full\npotential. In the second stage, we froze the parameters of backbone\nand solely update the exiting heads. Self-distillation between ex-\niting heads is employed to minimize information loss. Lastly, we\nperform extensive experiments to validate the superiority of our\nproposed framework for accelerating ViT inference, achieving a\ngood efficiency-accuracy trade-off for three ViT backbones on three\ndatasets. For example, as shown in Figure 1, when ViT serves as\nthe backbone, our method can accelerate the inference by 1.72 Ã—\nwith only 1.7 % accuracy drop on the CIFAR-100 dataset.\nOur main contributions are summarized as follows:\nâ€¢We conduct a systematic investigation into the effectiveness\nof early exiting in ViTs and analyze the issues arising from\nthe vanilla early exiting.\nâ€¢We propose an efficient early exiting framework termed\nLGViT for general ViTs, incorporating heterogeneous exiting\nheads, i.e., local perception head and global aggregation head,\nto achieve an efficiency-accuracy trade-off.\nâ€¢We develop a novel two-stage training strategy that facili-\ntates learning among multiple heterogeneous exiting heads\nand significantly minimizes information loss.\nâ€¢We perform extensive experiments on three widely-used\ndatasets and representative ViT backbones, demonstrating\nthe superiority of our proposed framework, which achieves\nan average speed-up of 1.8 Ã—with only 2% accuracy sacrifice.\n2 RELATED WORKS\nEfficient ViT. Due to their considerable computational cost, ViTs\nare challenging to deploy on resource-constrained edge devices for\nreal-time inference [21, 24]. Recently several studies have proposed\nlightweight architectures to enhance performance. For example,\nMehta et al. [17] incorporate convolution into transformers, com-\nbining the strengths of convolution and attention. Maaz et al. [16]\npropose an efficient hybrid architecture and design split depth-wise\nchannel groups encoder to increase the receptive field. Furthermore,\na series of methods employ traditional model compression tech-\nniques to obtain compact ViTs, such as network pruning [13, 23, 37],\nknowledge distillation [8, 26] and low-bit quantization [6, 34]. Hao\net al. [8] utilize patch-level information to help compact student\nmodels imitate teacher models. Kwon et al. [13] propose a post-\ntraining pruning framework with structured sparsity methods.\nEarly exiting strategy. Early exiting is an effective dynamic in-\nference paradigm that allows confident enough predictions from\ninternal classifiers to exit early. Recent research on early exiting\ncan be broadly categorized into two classes: 1) Architecture design.\nLGViT: Dynamic Early Exiting for Accelerating Vision Transformer MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\nSome studies focus on designing advanced backbone networks to\nbalance performance and efficiency. For example, Teerapittayanon\net al. [25] first propose to attach internal classifiers at varying depth\nin DNNs to accelerate inference. WoÅ‚czyk et al. [28] introduce cas-\ncade connections to enhance information flow between internal\nclassifiers and aggregate predictions from multiple internal classi-\nfiers to improve performance. These methods scarcely consider the\ndesign of the exiting head architecture and nearly all utilize a fully\nconnected layer following a pooler as the exiting head. Bakhtiarnia\net al. [1] propose to insert additional backbone blocks as early ex-\niting branches into ViT. 2) Training scheme. Another line of work\ndesigns training schemes to enhance the performance of internal\nclassifiers. Liu et al. [14] employ self-distillation to help internal\nclassifiers learn the knowledge from the final classifier. Xin et al.\n[32] introduce an alternating training scheme to alternate between\ntwo objectives for odd-numbered and even-numbered iterations.\nExisting methods for efficient ViT primarily focus on elaborately\ndesigning compact ViT structures or applying model compression\ntechniques to compress ViT. Our approach adopts sample-level ac-\nceleration for inference by dynamically adapting outputs at different\npaths based on the confidence of each exitâ€™s prediction. Regard-\ning early exiting strategies, the work most related to this paper\nis CNN-Add-EE and ViT-EE in [1], which use a convolution layer\nand a transformer encoder as exiting heads, respectively. However,\ntheir performance is unsatisfactory and cannot achieve efficient\ninference. To the best of our knowledge, we first introduce heteroge-\nneous exiting heads to construct early exiting ViTs and achieve an\nefficiency-accuracy trade-off. On top of the aforementioned studies,\nWe also propose a novel two-stage training scheme to bride the gap\nbetween heterogeneous architectures.\n3 METHOD\nIn this section, we first provide the motivation and an overview\nof the proposed LGViT framework. Then we illustrate the hetero-\ngeneous exiting heads and two-stage training strategy. Lastly, we\ndepict the exit policy employed during the inference process.\n3.1 Motivation\nThe early exiting method can halt the forward propagation of neu-\nral networks prematurely to provide an efficiency-accuracy trade-\noff, which has achieved significant performance improvements for\nCNNs and transformers in NLP. However, naively implementing\nearly exiting on ViT may not yield performance gains for internal\nclassifiers. For instance, the performance of the internal classifier on\nthe fifth and tenth layers decreases by 21.8% and 4.0%, respectively,\ncompared to the original classifier for ViT-B/16 [7] on CIFAR-100\n[12]. As illustrated in Figure 2, we compare the attention maps at\ndifferent exiting points for DeiT-B (the detailed description of prob-\ning experiments is presented in Appendix A.2). The deep classifiers\ncan extract target semantic features to identify objects. Therefore,\nwe obtain the following observations:\nâ€¢Observation 1: Shallow internal classifiers cannot learn\nsufficient feature representation.\nâ€¢Observation 2: Deep internal classifiers cannot capture tar-\nget semantic information.\nLayer 1\nLayer 2\nLayer L\nâ€¦\nLayer l\nâ€¦Input \nAttention map\nCat\nFigure 2: Comparison of attention maps at different exiting\npositions. The classifiers are omitted. The shallow internal\nclassifiers are difficult to identify object due to inadequate\nfeature capture. The deep internal classifiers do not capture\ntarget semantic information compared to the last classifiers.\nWe also discover that if both convolution and self-attention are\nemployed as exiting architectures, positioned on shallow and deep\nlayers respectively, the model would gain access to a more compre-\nhensive combination of local and global information compared to\nthe vanilla head architecture.\n3.2 Overview\nMotivated by the aforementioned observations, we propose an early\nexiting framework for ViTs that incorporates heterogeneous early\nexiting architectures. An overview of the proposed framework is\ndepicted in Figure 3. It comprises a ViT backbone, multiple local\nperception heads and multiple global aggregation heads. Initially, a\nViT backbone consisting of ğ¿encoder blocks is provided. We add\nğ‘€ internal classifiers to intermediate blocks of ViT. Generally,ğ‘€ is\nsmaller than the total number of backbone layers, as adding internal\nclassifiers after every layer would result in substantial computation\ncosts. The position of internal classifiers is independent of the\nbackbone layer numbering.\nWe follow a three-step procedure to construct the early exiting\nViT framework:\nâ€¢Attaching heterogeneous exiting heads: Starting from a\nbackbone of ViT, we first select several exiting points along\nits depth. Then we place local perception heads and global\naggregation heads at corresponding exiting points according\nto their positions.\nâ€¢Two-Stage training: We train the whole early exiting ViT\nusing a novel two-stage training strategy including end-to-\nend training and self-distillation with the backbone frozen.\nThis can facilitate the integration of global and local infor-\nmation for performance improvement.\nâ€¢Dynamic inference: When the trained model is deployed\nfor inference, each input sample can dynamically determine\nits output at varying depths based on the confidence of each\nexiting head prediction.\n3.3 Attaching Heterogeneous Exiting Heads\nWe first introduce the placement of exiting heads, followed by a\ndetailed description of the heterogeneous exiting heads. In this\nwork, exiting points, where exiting heads are positioned, are deter-\nmined according to an approximately equidistant computational\ndistribution, i.e., the multiply-accumulate operations (MACs) of\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Xu et al.\nPatch Embedding\nEncoder 1\nEncoder 2\nâ€¦\nEncoder l\nEncoder l+1\nEncoder L\nâ€¦\nC\nLPH\nLPH\nGAH\nGAH\nC\nC\nC\nC\n1Ã—1 Conv\nPosition-Wise \nDWConv\n1Ã—1 Conv\nPool\nGELU BN\nGELU BN\nGELU BN\nâ€¦\n[CLS] token\nSplit\nLocal Perception Head (LPH)\n[CLS] \ntoken\nPosition-Wise\nFeature Convergence\nMHSA\nGlobal Aggregation Head (GAH)\n[CLS] \ntoken\nPool\nDog 96%\nCat 34%\nDog 64%\nDog 89%\nDog 92%\nForward propagation \nHomogeneous distillation\nHeterogeneous distillation\nC ClassifierPrediction distillation\nPrediction \nconfidence\nPoint-wise addition\nFigure 3: Overview of the proposed early-exiting ViT framework. 1) Given a backbone of ViT, we first attach local perception\nhead (LPH) at lower half exiting points and global aggregation head (GAH) at top half of exiting points. 2) During the training\nphase, after an end-to-end training of the backbone, all exiting heads are jointly trained through a novel self-distillation\nutilizing heterogeneous features, homogeneous features and prediction logits as supervision with the backbone frozen. 3) In\nthe inference stage, each input sample dynamically adjusts its exiting path according to the prediction confidence.\nintermediate blocks between two adjacent points remain consis-\ntent. For the sake of simplicity, exiting points are constrained to\nbe placed at the output of individual encoder blocks. We attach\nlocal perception heads, based on convolution, to the lower half\nof exiting points to enhance local information exploration. Global\naggregation heads, based on self-attention, are integrated into the\nupper half of points to augment global information acquisition.\nLocal perception head. As analyzed in Section A.2, directly ap-\nplying early exiting in ViT leads to severe performance degradation\nfor shallow internal classifiers. To mitigate the issue, we introduce\na local perception head (LPH) for early exiting framework, which\nelegantly incorporates convolution into ViT to enhance feature\nrepresentation learning. It can achieve efficient local information\nexploration and effective feature integration extracted from the\noriginal backbone. As illustrated in the upper right of Figure 3, the\nproposed LPH first employs a 1 Ã—1 convolution layer to expand\ndimensions. Subsequently, the expanded features are passed to a\nposition-wise depth-wise convolution (PDConv) with ğ‘˜Ã—ğ‘˜ kernel\nsize that depends on the exiting positions ğ‘š. In order to reduce\ncomputation overhead, we employ smaller kernel size convolu-\ntions for the deeper exiting points. We employ a decreasing linear\nmapping function ğ‘“(Â·)to determine the kernel size of PDConv, i.e.,\nğ‘˜ = ğ‘“(ğ‘š),ğ‘š â‰¤ğ¿/2. For instance, the expanded features at theğ‘š-th\nexiting position are passed to ğ‘˜Ã—ğ‘˜ depth-wise convolution. Note\nthat ğ‘˜ = 0 means that the expanded features will bypass PDConv\nand proceed directly to the subsequent part. Thus, the PDConv can\nbe formulated as:\nPDConv(X,ğ‘š)=\n(\nDWConvğ‘˜Ã—ğ‘˜(X), ğ‘“ (ğ‘š)> 0\nX, ğ‘“ (ğ‘š)= 0 , (1)\nwhere DWConvğ‘˜Ã—ğ‘˜ denotes the depth-wise convolution with ker-\nnel of size ğ‘˜ Ã—ğ‘˜. Then the features are projected back into the\noriginal patch space using a1Ã—1 convolution and then passed to an\naverage pooling layer. Considering that the [ğ¶ğ¿ğ‘†]token contains\ndjominant feature representations, it is added to the pooled out-\nput to facilitate the fusion of global information from the original\nbackbone and local information from the convolution. Concretely,\ngiven a ViT encoder output Xğ‘’ğ‘› âˆˆRğ‘Ã—ğ·, where ğ‘ represents the\nnumber of patches and ğ· denotes the hidden dimension, when\nthe position of exiting points is lower than ğ¿/2, the output of the\nLGViT: Dynamic Early Exiting for Accelerating Vision Transformer MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\nproposed exiting head is given by:\nLPH(Xğ‘’ğ‘›,ğ‘š)= Pool(Conv1Ã—1(G(Xğ‘’ğ‘›,ğ‘š)))+ Xğ¶ğ¿ğ‘†,\nG(Xğ‘’ğ‘›,ğ‘š)= PDConv(Conv1Ã—1(Xğ‘’ğ‘›),ğ‘š), (2)\nwhere Xğ¶ğ¿ğ‘† represents [ğ¶ğ¿ğ‘†]token and the activation layer is\nomitted. Gaussian error linear unit (GELU) and batch normalization\n(BN) are employed after each convolution. The output of LPH is\nfinally passed to the internal classifier. By introducing LPH as the\nexiting head, shallow internal classifiers can learn adequate feature\nrepresentation and capture local information, thereby enhancing\nperformance in vision tasks.\nGlobal aggregation head. Based on the discussion in Section\nA.2, the direct application of early-exit methods to ViT hinders\nthe semantic information capture in deep internal classifiers. We\npropose a global aggregation head (GAH) and incorporate it at\ndeep exiting points, as illustrated in the lower right of Figure 3.\nThe proposed GAH integrates features from locally adjacent tokens\nand then compute self-attention for each subset to facilitate tar-\nget semantic information exploitation. In GAH, we first employ a\nposition-wise feature convergence (PFC) block to aggregate features\nfrom the exiting point. In the PFC block, input features X âˆˆRğ‘Ã—ğ·\nare reshaped to ğ·Ã—ğ» Ã—ğ‘Š dimensions and down-sampled with a\nsize of ğ‘ Ã—ğ‘  window. The sampled features Xğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ âˆˆRğ·Ã—ğ»\nğ‘  Ã—ğ‘Š\nğ‘ \nare restored to original dimension format ğ‘\nğ‘ 2 Ã—ğ·. The proposed\nPFC block reshapes the input features to patch format and down-\nsamples them with an ğ‘ Ã—ğ‘ window. The sampled features are then\nrestored to original format. To avoid introducing additional learn-\nable parameters, we employ an average pool with ğ‘  stride as the\nimplementation of PFC. Analogous to PDConv, the swindow size\nğ‘  of PFC also depends on the exiting position ğ‘š. Deeper exiting\npoints utilize larger window sizes significantly reducing the compu-\ntational cost. We employ an increasing linear mapping functionğ‘”(Â·)\nto determine the window size of PFC, i.e., ğ‘  = ğ‘”(ğ‘š),ğ¿/2 < ğ‘š â‰¤ğ¿.\nFor example, the input features are passed to sub-sample with a\nsize of ğ‘”(ğ‘š)Ã—ğ‘”(ğ‘š)window at the ğ‘š-th exiting point. Note that\nthe minimum window size is generally set to 2. Consequently, the\nPFC can be expressed as:\nPFC(Xğ‘’ğ‘›,ğ‘š)= Poolğ‘”(ğ‘š)(Xğ‘’ğ‘›), (3)\nwhere Poolğ‘”(ğ‘š)represents an average pool with ğ‘”(ğ‘š)stride. The\nreshaping and recover operations of features are omitted in the\nequation. PFC not only reduces the computational redundancy but\nalso helps focus on target patches compared to the original MHSA.\nThen the integrated features are passed through multi-head self-\nattention (MHSA) and a pool layer. The [ğ¶ğ¿ğ‘†]token is also added\nto the pooled features. Thus, when the position of exiting points is\ndeeper than ğ¿/2, the proposed GAH can be formulated as:\nGAH(Xğ‘’ğ‘›,ğ‘š)= Pool(MHSA(PFC(Xğ‘’ğ‘›,ğ‘š)))+ Xğ¶ğ¿ğ‘†,\nMHSA(X)= softmax\n \nğ‘‹ğ‘Šğ‘„(ğ‘‹ğ‘Šğ¾)ğ‘‡\nâˆš\nğ‘‘\n!\nğ‘‹ğ‘Šğ‘‰, (4)\nwhere the input X is linearly transformed into query, key and value\nvectors using transformation matrices ğ‘Šğ‘„, ğ‘Šğ¾ and ğ‘Šğ‘‰; ğ‘‘ is the\nvector dimension. By employing GAH as the exiting head, deep\ninternal classifiers can reduce spatial redundancy of self-attention\nand capture more target semantic information.\nComplexity analysis. To thoroughly understand the compu-\ntational bottleneck of heterogeneous exiting heads, we compare\nour proposed LPH + GAH with standard convolution + MHSA by\nanalyzing their floating-point operations (MACs). Given an input\nfeature of size ğ‘ Ã—ğ·, the FLOPs of standard ğ‘˜Ã—ğ‘˜ convolution are:\nO(Convğ‘˜Ã—ğ‘˜)= ğ‘ğ·2ğ‘˜2. (5)\nThe FLOPs of an MHSA module can be calculated as:\nO(MHSA)= 2ğ‘ğ·(ğ·+ğ·)+ğ‘2(ğ·+ğ·)= 4ğ‘ğ·2 +2ğ‘2ğ·, (6)\nwhere the activation function is omitted. For a fair comparison, we\nselect the same kernel size ğ‘˜ in LPH. The FLOPs of the proposed\ntwo exiting head are as follows:\nO(LPH)= 2ğ‘ğ·2 +ğ‘ğ·ğ‘˜2,\nO(GAH)= 4ğ‘ğ·2/ğ‘ 2 +2ğ‘2ğ·/ğ‘ 4.\n(7)\nWe observe that the computational complexity of LPH and GAH is\nlower than that of standard convolution and MHSA, respectively.\nO(LPH)\nO(Convğ‘˜Ã—ğ‘˜)= (2ğ·+ğ‘˜2)/ğ·ğ‘˜2 < 1,\nO(GAH)\nO(MHSA)= 2ğ·+ğ‘/ğ‘ 2\n2ğ·+ğ‘ < 1.\n(8)\nTherefore, compared to standard convolution and MHSA, our pro-\nposed LPH and GAH heads are more friendly to computational cost\nand convenient to implement on hardware platforms.\n3.4 Two-Stage Training Strategy\nTo tackle the performance degradation issue caused by early exiting,\nwe propose a novel two-stage training strategy to transfer knowl-\nedge from deeper classifiers to shallow classifiers. The process of\ntwo-stage training strategy is presented as follows.\nâ€¢In the first stage, we train the backbone ViT and update the\nparameters of the backbone and the final classifier in an\nalternating strategy. As a result, the interference of multiple\ninternal classifiers can be minimized, enabling the final clas-\nsifier to reach its full potential. Similar to general training,\nwe utilize the cross entropy function as the training loss.\nâ€¢During the second stage, the backbone and final classifier are\nkept frozen. Only the parameters of exiting heads and inter-\nnal classifiers can be updated. We introduce self-distillation\nto facilitate the imitation of all exiting heads from the last\nclassifier as illustrated in the left of Figure 3.\nThe overall distillation loss comprises the heterogeneous distil-\nlation, homogeneous distillation and prediction distillation loss.\nHeterogeneous distillation. Considering the usage of hetero-\ngeneous exiting head, the gap between different types of heads is\nsubstantial. Directly utilizing the features from the last layer as\nsupervision for internal classifiers can lead to information loss. To\ntackle this issue, we propose heterogeneous distillation to facili-\ntate learning the knowledge from heterogeneous architectures. To\nreduce the conflict between multiple losses, we only employ the\nfeature of the last layer as the reference feature for the first and\nlast exiting heads of LPH and GAH. The inductive bias and local\nvisual representations captured by LPH can be elegantly integrated\nwith global information extracted from self-attention. Considering\nthe different shapes of feature maps between exiting heads and\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Xu et al.\nthe final block, we employ an aligning module to match the di-\nmensions. The module consists of a depth-wise convolution, GELU\nand BN activation functions. The feature map of the last ViT layer\nFğ¿ âˆˆRğ‘Ã—ğ· is first reshaped to ğ· Ã—\nâˆš\nğ‘ Ã—\nâˆš\nğ‘ dimensions. The\nreshaped feature map is passed to the module to reduce dimensions,\nand then restored to original dimension format ğ‘â€²Ã—ğ·. The loss\nfunction of heterogeneous can be formulated as:\nLâ„ğ‘’ğ‘¡ğ‘’ = 1\n4\nâˆ‘ï¸\nğ‘šâˆˆM\nLğ¾ğ¿(Fğ‘š,Align(Fğ¿)), (9)\nwhere M= {1,ğ‘€/2,ğ‘€/2 +1,ğ‘€}and Lğ¾ğ¿ is the Kullback-Leibler\ndivergence function.\nHomogeneous distillation. We propose homogeneous distil-\nlation between exiting heads with the same architectures to further\nimprove performance. In each type of exiting heads, we employ\nthe final heads as the teacher assistant to help the preceding ho-\nmogeneous heads learn hint knowledge. For example, in all LPHs,\nthe features of final LPH (i.e. at ğ‘€/2-th exiting point) is utilized as\nreference feature for the preceding LPH. Given the feature maps\nfrom the first to ğ‘š-th exiting heads Fğ‘š,(1 â‰¤ğ‘š â‰¤ğ‘€/2), the loss\nfunction of homogeneous distillation between LPHs is:\nLğ¿ğ‘ƒğ»\nâ„ğ‘œğ‘šğ‘œ = 1\nğ‘€/2 âˆ’1\nğ‘€/2âˆ’1âˆ‘ï¸\nğ‘š=1\nLğ‘€ğ‘†ğ¸(Fğ‘š,Fğ‘€/2), (10)\nwhere Lğ‘€ğ‘†ğ¸ is the mean squared error function. Since the fea-\nture maps of GAH have different shapes, we apply dot-product\noperations between feature maps. Given a feature map of GAH\nFğ‘š âˆˆ Rğ‘/ğ‘”2 (ğ‘š)Ã—ğ· at the ğ‘š-th exiting point, the shape can be\ntransformed to ğ· Ã—ğ· by computing Fğ‘‡ğ‘šFğ‘š. The loss function of\nhomogeneous distillation between GAHs can be expressed as:\nLğºğ´ğ»\nâ„ğ‘œğ‘šğ‘œ = 1\nğ‘€/2 âˆ’1\nğ‘€âˆ’1âˆ‘ï¸\nğ‘š=ğ‘€/2+1\nLğ‘€ğ‘†ğ¸(Fğ‘‡\nğ‘šFğ‘š,Fğ‘‡\nğ‘€Fğ‘€). (11)\nTherefore, the overall loss function of homogeneous distillation can\nbe expressed as:\nLâ„ğ‘œğ‘šğ‘œ = Lğ¿ğ‘ƒğ»\nâ„ğ‘œğ‘šğ‘œ +Lğºğ´ğ»\nâ„ğ‘œğ‘šğ‘œ. (12)\nPrediction distillation. In order to further improve the per-\nformance of internal classifiers, we utilize the final classifier as\nthe reference label of ğ‘€/2-th and ğ‘€-th exiting points where last\nLPH and GAH are located, respectively. Given an input sample\nassociated with label ğ‘¦, and assuming that the predictions at the\nğ‘€/2-th and ğ‘€-th exiting points are Ë†ğ‘¦ğ‘€/2 and Ë†ğ‘¦ğ‘€, respectively, the\nloss function of prediction distillation can be formulated as:\nLğ‘ğ‘Ÿğ‘’ğ‘‘ = Lğ¾ğ·(Ë†ğ‘¦ğ‘€/2,Ë†ğ‘¦ğ¿,ğ‘¦)+L ğ¾ğ·(Ë†ğ‘¦ğ‘€,Ë†ğ‘¦ğ¿,ğ‘¦), (13)\nwhere Lğ¾ğ· is the loss function of vanilla knowledge distillation:\nLğ¾ğ·(Ë†ğ‘¦ğ‘ ,Ë†ğ‘¦ğ‘¡,ğ‘¦)= (1 âˆ’ğ›¾)Lğ¶ğ¸(Ë†ğ‘¦ğ‘ ,ğ‘¦)+ğ›¾Lğ¾ğ¿(Ë†ğ‘¦ğ‘ /ğ‘‡, Ë†ğ‘¦ğ‘¡/ğ‘‡). (14)\nHere, Lğ¶ğ¸ is the cross-entropy function, ğ‘‡ is a temperature value\nto control the smoothness of logits, and ğ›¾ is a balancing hyperpa-\nrameter.\nHence, the overall loss function of our proposed method is:\nL= ğ›¼Lâ„ğ‘’ğ‘¡ğ‘’ +ğ›½Lâ„ğ‘œğ‘šğ‘œ +Lğ‘ğ‘Ÿğ‘’ğ‘‘, (15)\nwhere ğ›¼ and ğ›½ are hyperparameters.\n3.5 Dynamic Inference\nIn this section, we first introduce the exiting metric and then depict\nthe process of early exiting ViT inference. We employ a standard\nconfidence metric following [28] as the exiting metric, which repre-\nsents the probability of the most confident classification class. The\nprediction confidence ğ‘ğ‘š at the ğ‘š-th exiting position is:\nğ‘ğ‘š(ğ‘ğ‘š)= ğ‘šğ‘ğ‘¥\nğ¶\nğ‘ğ‘š, (16)\nwhere ğ‘ğ‘š is the prediction distribution atğ‘š-th exiting position and\nğ¶ is the classification label set. During the inference process, input\nsamples go through exits sequentially. Each sample dynamically\nadjusts its exiting path according to the exiting metric. If the classi-\nfication confidence of a sample at the ğ‘š-th exiting point exceeds\na predefined threshold ğœ, the forward propagation of ViT will be\nterminated, and the prediction at the ğ‘š-th exiting point will be out-\nput. The threshold ğœ can be adjusted according to computation cost\nand hardware resources to achieve an efficiency-accuracy trade-off.\nA low threshold may lead to a significant speed-up at the cost of a\npossible drop in accuracy. If the exiting condition is never reached,\nthe ViT will revert to the standard inference process.\n4 EXPERIMENT\nIn this section, we first introduce some implementation details\nand experimental settings. Then, we present the results of perfor-\nmance evaluations on three vision datasets and three popular ViT\nbackbones. Finally, we conduct extensive ablation experiments to\ndemonstrate the superiority of our methods.\n4.1 Experimental Setup\nDatasets. We evaluate our proposed method on three public vision\ndatasets: CIFAR-100[11], Food-101[2], and ImageNet-1K[5]. The\nCIFAR-100 dataset contains 50K training images and 10K testing im-\nages, uniformly categorized into 100 classes. The Food-101 dataset\nconsists of 101 food categories, with 750 training and 250 test im-\nages per category, making a total of 101K images. The ImageNet-1K\ndataset spans 1000 object classes and contains 1,281,167 training\nimages, 50K validation images and 100K test images. We augment\nthe training data with random crops, random horizontal flips and\nnormalization, while the testing data is augmented with center\ncrops and normalization.\nBackbones. The proposed framework can be applied to a range of\nearly-exit ViTs. Without loss of generality, we conduct experiments\nwith three well-known ViT backbones, namely, ViT[7], DeiT[26],\nand Swin[15]. ViT is the first pure transformer structure for com-\nputer vision tasks and utilize a [ğ¶ğ¿ğ‘†]token to serve as the image\nrepresentation for classification tasks. DeiT adds an additional dis-\ntillation token to learn hard labels from the teacher model compared\nto ViT. Swin builds hierarchical feature maps by merging image\npatches in deeper layers. The [ğ¶ğ¿ğ‘†]token in LPH and GAH is\nreplaced by the encoder output because it contains no [ğ¶ğ¿ğ‘†]token.\nBaselines. We compare our dynamic early exiting methods with\nseveral representative early exiting methods. Considering most\nmethods designed for CNNs and transformers in NLP, we transfer\nthese methods to ViT for fair comparison.\nâ€¢SDN [9] utilizes a weighted training strategy and employs a\nconfidence-based criterion to decide whether to exit.\nLGViT: Dynamic Early Exiting for Accelerating Vision Transformer MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\nTable 1: Performance of different methods on three datasets for different ViT backbones. \"Acc.\" represents the Top-1 classification\naccuracy. \"#Params.\" represents the number of model parameters.\nMethods #Params. Results: CIFAR-100 Results: Food-101 Results: ImageNet-1K\nAcc. MACs â†“ Speed-up â†‘ Acc. MACs â†“ Speed-up â†‘ Acc. MACs â†“ Speed-up â†‘\nViT\nViT-B/16 86 M 90.8 % 16.93 G 1.00 Ã— 89.6 % 16.93 G 1.00 Ã— 81.8 % 16.93 G 1.00 Ã—\nSDN 94 M 86.5 % 10.16 G 1.64 Ã— 88.5 % 8.67 G 1.95 Ã— 79.5 % 10.95 G 1.55 Ã—\nPABEE 94 M 85.1 % 11.48 G 1.52 Ã— 86.7 % 10.93 G 1.81 Ã— 78.6 % 12.41 G 1.36 Ã—\nBERxiT 94 M 87.1 % 10.27 G 1.65 Ã— 88.3 % 8.56 G 1.98 Ã— 79.9 % 11.75 G 1.44 Ã—\nViT-EE 94 M 87.5 % 11.65 G 1.65 Ã— 88.2 % 10.42 G 1.91 Ã— 79.6 % 13.66 G 1.38 Ã—\nPCEE 94 M 86.1 % 10.90 G 1.55 Ã— 88.1 % 9.50 G 1.81 Ã— 80.0 % 12.36 G 1.37 Ã—\nOurs 101 M 88.5 % 9.76 G 1.87 Ã— 88.6 % 7.63 G 2.36 Ã— 80.3 % 10.65 G 1.70 Ã—\nDeiT\nDeiT-B\nâš— 86 M 91.3 % 16.93 G 1.00 Ã— 90.3 % 16.93 G 1.00 Ã— 83.4 % 16.93 G 1.00 Ã—\nSDN 94 M 87.4 % 9.65 G 1.75 Ã— 88.5 % 8.62 G 1.97 Ã— 77.5 % 11.30 G 1.50 Ã—\nPABEE 94 M 86.4 % 11.43 G 1.48 Ã— 88.6 % 11.00 G 1.54 Ã— 78.5 % 12.40 G 1.36 Ã—\nBERxiT 94 M 88.3 % 10.48 G 1.61 Ã— 88.8 % 9.16 G 1.85 Ã— 79.1 % 11.12 G 1.53 Ã—\nViT-EE 93 M 88.3 % 11.07 G 1.75 Ã— 88.8 % 10.26 G 1.91 Ã— 80.5 % 13.28 G 1.45 Ã—\nPCEE 94 M 87.5 % 10.49 G 1.61 Ã— 88.6 % 9.59 G 1.76 Ã— 80.4 % 11.87 G 1.43 Ã—\nOurs 102 M 88.9 % 9.54 G 1.91 Ã— 89.5 % 8.53 G 2.12 Ã— 81.7 % 10.90 G 1.67 Ã—\nSwin\nSwin-B 87 M 92.6 % 15.13 G 1.00 Ã— 93.3 % 15.13 G 1.00 Ã— 83.5 % 15.40 G 1.00 Ã—\nSDN 88 M 88.3 % 9.41 G 1.72 Ã— 90.2 % 7.64 G 2.17 Ã— 78.7 % 10.91 G 1.45 Ã—\nPABEE 88 M 83.8 % 9.46 G 1.72 Ã— 88.8 % 8.17 G 2.01 Ã— 79.0 % 13.19 G 1.18 Ã—\nBERxiT 88 M 88.4 % 9.61 G 1.68 Ã— 90.2 % 7.68 G 2.16 Ã— 80.2 % 10.35 G 1.54 Ã—\nViT-EE 91 M 88.1 % 9.71 G 1.82 Ã— 90.6 % 8.92 G 2.03 Ã— 82.1 % 11.20 G 1.52 Ã—\nPCEE 88 M 88.1 % 10.68 G 1.50 Ã— 90.3 % 9.11 G 1.79 Ã— 79.8 % 11.51 G 1.37 Ã—\nOurs 97 M 90.7 % 8.84 G 1.94 Ã— 91.9 % 7.05 G 2.50 Ã— 82.7 % 9.98 G 1.69 Ã—\nâ€¢PABEE [38] employs a patience metric based on the consis-\ntency of classification decisions over several internal classi-\nfiers to make early exiting decisions.\nâ€¢BERxiT [32] introduces an alternating training strategy to\ntrain the whole model.\nâ€¢ViT-EE [1] utilizes a ViT encoder layer as its exiting head\nwith the confidence criterion to decide whether to exit.\nâ€¢PCEE [36] utilizes a patience&confidence criterion accord-\ning to the enough number of confident predictions from\nconsecutive internal classifiers.\nUnless otherwise specified, the default exit architecture of baselines\nis a single fully connected layer that follows a pooler.\nEvaluation metrics. Considering the trade-off between perfor-\nmance and efficiency, we employ Top-1 classification accuracy and\nspeed-up ratio as the performance and efficiency metric, respec-\ntively. Since the measurement of runtime might not be stable, we\nfollow [31] to calculate the speed-up ratio by comparing the ac-\ntually executed layers in forward propagation and the complete\nlayers. For an ğ¿-layer ViT, the speed-up ratio is defined as:\nSpeed-up =\nÃğ¿\nğ‘–=1 ğ¿Ã—ğ‘šğ‘–\nÃğ¿\nğ‘–=1 ğ‘–Ã—ğ‘šğ‘– , (17)\nwhere ğ‘šğ‘– is the number of samples that exit at the ğ‘–-th layer of ViT.\nFor clarity, we utilize the average multiply-accumulate operations\n(MACs) performed across the entire test dataset as a metric to assess\nthe computational cost associated with a given model.\nImplementation details. Our framework and all the compared\nmethods are implemented using the Huggingface transformer li-\nbrary [29] for fair comparison. Most hyperparameters, such as learn-\ning rate, optimizer, and dropout probabilities are kept unchanged\nfrom the original backbones for fair comparison. We list different\nhyperparameters in the Appendix due to limited space. Each net-\nwork is fine-tuned by 100 epochs on 3 NVIDIA 3090 GPUs, with\na batch size of 64. There is no early stopping and the checkpoint\nafter full fine-tuning is chosen.\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Xu et al.\nTable 2: Ablation study results of main components. The \" âœ“\"\nmark indicating that we adopt the corresponding component.\nThe opposite of LPH and GAH is fully connected layer with a\npooler. The opposite of two-stage training is vanilla training.\nLPH GAH Two-Stage training Acc. Speed-up\nÃ— Ã— Ã— 87.3 % 1.56 Ã—\nÃ— Ã— âœ“ 88.2 % 1.57 Ã—\nÃ— âœ“ âœ“ 88.3 % 1.71 Ã—\nâœ“ âœ“ âœ“ 88.5 % 1.87 Ã—\nTable 3: Comparison of different exiting head architectures\non CIFAR-100. MLP, Conv and Attention refers to utilizing\na fully-connected layer, a 3 Ã—3 standard convolution layer,\nand a MHSA block respectively.\nExiting head #Params. MACs Acc. Speed-up\nMLP [31] 91 M 10.78 G 88.2 % 1.57 Ã—\nConv [28] 129 M 13.86 G 87.5 % 1.72 Ã—\nAttention [1] 105 M 12.76 G 88.0 % 1.58 Ã—\nOurs 101 M 9.76 G 88.5 % 1.87 Ã—\n4.2 Performance Evaluation\nWe conduct extensive experiments to compare our methods with\nthe state-of-the-art methods for three ViT backbones on three vision\ndatasets. Then we present the performance and efficient trade-off\ncompared with other baselines.\nComparison with the state-of-the-art. We compare the per-\nformance between our methods and baselines on CIFAR-100, Food-\n101 and Tiny ImageNet datasets when different backbones are\nadopted, including ViT, DeiT, and Swin. The results are shown\nin Table 1. The original models for different backbones are ViT-\nL/16, DeiT-B and Swin-B respectively. We can find that our method\ncan achieve approximately a 1.8 Ã—speed-up ratio with only a 2%\naccuracy drop compared to the original models on most datasets,\nwhich significantly outperforms other baselines.\nPerformance and efficiency trade-off. To further verify the\nrobustness and efficiency of our method, we visualize the perfor-\nmance and efficiency trade-off curves in Figure 1 on CIFAR-100 test\nset. The original backbone model is ViT-B/16. We compare five com-\npetitive baselines in the dynamic inference scenario. We can see that\nthe performance of most early exiting methods drops dramatically\nwhen the speed-up ratio increases. This also reflects directly ap-\nplying early exiting methods in ViT leads to unstable performance\nwhich cannot meet the requirements of real-time systems. However,\nour method is more robust to the variance of speed-up. If we set\nthe almost same speed-up ratio, the accuracy drop of our method\nis 3.6 % lower than SDN method. When the accuracy is approxi-\nmate to other baselines, our method can achieve faster speed-up.\nMoreover, our method can dynamically adjust the speed-up ratio\nwithout retraining, which is more feasible and friendly.\n4.3 Ablation Study\nTo fully understand the impact of each part of the proposed frame-\nwork, we conduct ablation study, where all experiments are evalu-\nated on CIFAR-100 and utilize ViT as the backbone. We first study\nTable 4: Comparison of different training schemes with the\nproposed exiting heads on CIFAR-100.\nTraining scheme Accuracy Speed-up\nNormal [25] 86.9 % 1.82 Ã—\nWeighted [9] 87.4 % 1.80 Ã—\nDistillation [14] 86.9 % 1.83 Ã—\nAlternating [32] 87.9 % 1.84 Ã—\nOurs 88.5 % 1.87 Ã—\n/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013\n/uni0000001c/uni00000015\n/uni0000001c/uni00000017/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000039/uni0000004c/uni00000037/uni00000010/uni00000025/uni00000012/uni00000014/uni00000019\n/uni00000032/uni00000058/uni00000055/uni00000056\n/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000028/uni0000005b/uni0000004c/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002b/uni00000048/uni00000044/uni00000047/uni00000056\n/uni00000014/uni00000011/uni00000013\n/uni00000014/uni00000011/uni00000015\n/uni00000014/uni00000011/uni00000017\n/uni00000014/uni00000011/uni00000019\n/uni00000014/uni00000011/uni0000001b/uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000010/uni00000058/uni00000053\nFigure 4: Results of accuracy and speed-up for different head\nnumber settings. The exiting heads are placed across all lay-\ners uniformly.\nthe effectiveness of the main components, and then analyze the\nimpact of different exiting head architectures and training schemes.\nFinally, we verify the robustness of our methods for different num-\nbers of predefined exiting heads.\nAblation of main components. We design experiments to\nverify the effectiveness of the proposed LPH, GAH and two-stage\ntraining scheme. Table 2 presents the accuracy and speed-up ra-\ntio utilizing different components. The results show that for early\nexiting in ViT, heterogeneous exiting heads and two-stage train-\ning scheme are both significant. Specifically, we can observe that\nthe two-stage training scheme can significantly improve accuracy.\nMoreover, when combined with LPH and GAH, the inference effi-\nciency and accuracy can be further improved.\nThe architecture of exiting heads. In order to verify the effec-\ntiveness of the proposed heterogeneous exiting heads, we compare\nother competitive architectures using the same two-stage training\nscheme, as shown in Table 3. We can observe that the proposed\nheterogeneous exiting heads are crucial to achieve a speed-accuracy\ntrade-off. Although utilizing MLP as exiting heads can gain approx-\nimate accuracy to ours, the speed-up ratio is low. The attention\nmethod can achieve a close trade-off between accuracy and speed\nbut with high storage and computation requirements.\nTraining schemes. We compare our methods with four rep-\nresentative training schemes in early exiting methods, and they\nall utilize the proposed heterogeneous exiting heads. The accu-\nracy and speed-up are shown in Table 4. Our method achieves the\nLGViT: Dynamic Early Exiting for Accelerating Vision Transformer MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\nhighest classification accuracy and inference speed-up ratio, which\nsignificantly outperforms other training schemes.\nThe number of exiting heads. We analyze the influence on\naccuracy and speed-up when changing the number of predefined ex-\niting heads, as shown in Figure 4. As the number of heads increases,\nthe accuracy remains essentially consistent with only approximate\n2 % accuracy drop, which shows that our method is robust to the\nnumber of heads and can tackle the overthinking problem [9]. More-\nover, we can observe that the speed-up ratio can enhance with the\nincreasing number of heads.\n5 CONCLUSION\nIn this paper, we point out that naively applying early exiting in\nViTs results in performance bottleneck due to insufficient feature\nrepresentations in shallow internal classifiers and limited ability\nto capture target semantic information in deep internal classifiers.\nBased on this analysis, we propose an early exiting framework\nfor general ViTs which combines heterogeneous exiting heads to\nenhance feature exploration. We also develop a novel two-stage\ntraining strategy to reduce information loss between heterogeneous\nexiting heads. We conduct extensive experiments for three ViT back-\nbones on three vision datasets, demonstrating that our methods\noutperform other competitive counterparts. The limitation of our\nmethods is to manually choose the exiting position and optimal ex-\niting path. In the future, we intend to utilize Bayesian optimization\nto automatically perform the optimal exiting decision.\nACKNOWLEDGMENTS\nThis work was partially supported by the National Key Research\nand Development Program of China under No. 2021YFC3300200\nand the National Natural Science Foundation of China under Grants\nNo. 61971457.\nREFERENCES\n[1] Arian Bakhtiarnia, Qi Zhang, and Alexandros Iosifidis. 2021. Multi-Exit Vision\nTransformer for Dynamic Inference. In 32nd British Machine Vision Conference\n2021, BMVC 2021, Online, November 22-25, 2021 . BMVA Press, 81.\n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101 - Mining\nDiscriminative Components with Random Forests. In Computer Vision - ECCV\n2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Pro-\nceedings, Part VI (Lecture Notes in Computer Science, Vol. 8694) , David J. Fleet,\nTomÃ¡s Pajdla, Bernt Schiele, and Tinne Tuytelaars (Eds.). Springer, 446â€“461.\n[3] Hila Chefer, Shir Gur, and Lior Wolf. 2021. Transformer Interpretability Beyond\nAttention Visualization. In IEEE Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2021, virtual, June 19-25, 2021 . Computer Vision Foundation / IEEE,\n782â€“791.\n[4] Richard J. Chen, Chengkuan Chen, Yicong Li, Tiffany Y. Chen, Andrew D. Trister,\nRahul G. Krishnan, and Faisal Mahmood. 2022. Scaling Vision Transformers\nto Gigapixel Images via Hierarchical Self-Supervised Learning. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022 . IEEE, 16123â€“16134.\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Ima-\ngeNet: A large-scale hierarchical image database. In 2009 IEEE Computer Society\nConference on Computer Vision and Pattern Recognition (Miami, Florida, USA).\nIEEE Computer Society, 248â€“255.\n[6] Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei,\nand Xianglong Liu. 2022. Towards Accurate Post-Training Quantization for\nVision Transformer. In MM â€™22: The 30th ACM International Conference on Multi-\nmedia, Lisboa, Portugal, October 10 - 14, 2022 , JoÃ£o MagalhÃ£es, Alberto Del Bimbo,\nShinâ€™ichi Satoh, Nicu Sebe, Xavier Alameda-Pineda, Qin Jin, Vincent Oria, and\nLaura Toni (Eds.). ACM, 5380â€“5388.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. In9th Interna-\ntional Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021 . OpenReview.net.\n[8] Zhiwei Hao, Jianyuan Guo, Ding Jia, Kai Han, Yehui Tang, Chao Zhang, Han Hu,\nand Yunhe Wang. 2022. Learning efficient vision transformers via fine-grained\nmanifold distillation. Advances in Neural Information Processing Systems 35 (2022),\n9164â€“9175.\n[9] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. 2019. Shallow-Deep Net-\nworks: Understanding and Mitigating Network Overthinking. In Proceedings of\nthe 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA (Proceedings of Machine Learning Research, Vol. 97) ,\nKamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 3301â€“3310.\n[10] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton.\n2019. Similarity of Neural Network Representations Revisited. In Proceedings of\nthe 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA (Proceedings of Machine Learning Research, Vol. 97) ,\nKamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 3519â€“3529.\n[11] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features\nfrom tiny images . Technical Report. Toronto, Ontario.\n[12] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features\nfrom tiny images. (2009).\n[13] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer,\nand Amir Gholami. 2022. A Fast Post-Training Pruning Framework for Trans-\nformers. arXiv preprint arXiv:2204.09656 (2022).\n[14] Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. 2020.\nFastBERT: a Self-distilling BERT with Adaptive Inference Time. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.\nTetreault (Eds.). Association for Computational Linguistics, 6035â€“6044.\n[15] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer\nusing Shifted Windows. In 2021 IEEE/CVF International Conference on Computer\nVision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 . IEEE, 9992â€“10002.\n[16] Muhammad Maaz, Abdelrahman Shaker, Hisham Cholakkal, Salman H. Khan,\nSyed Waqas Zamir, Rao Muhammad Anwer, and Fahad Shahbaz Khan. 2022.\nEdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile\nVision Applications. In Computer Vision - ECCV 2022 Workshops - Tel Aviv, Israel,\nOctober 23-27, 2022, Proceedings, Part VII (Lecture Notes in Computer Science,\nVol. 13807), Leonid Karlinsky, Tomer Michaeli, and Ko Nishino (Eds.). Springer,\n3â€“20.\n[17] Sachin Mehta and Mohammad Rastegari. 2022. MobileViT: Light-weight, General-\npurpose, and Mobile-friendly Vision Transformer. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 .\nOpenReview.net.\n[18] Xuran Pan, Chunjiang Ge, Rui Lu, Shiji Song, Guanfu Chen, Zeyi Huang, and Gao\nHuang. 2022. On the Integration of Self-Attention and Convolution. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022 . IEEE, 805â€“815.\n[19] Namuk Park and Songkuk Kim. 2022. How Do Vision Transformers Work?. In\nThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual\nEvent, April 25-29, 2022 . OpenReview.net.\n[20] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2023. On the\neffect of dropping layers of pre-trained transformer models. Comput. Speech\nLang. 77 (2023), 101429.\n[21] Li Shen, Yan Sun, Zhiyuan Yu, Liang Ding, Xinmei Tian, and Dacheng Tao. 2023.\nOn Efficient Training of Large-Scale Deep Learning Models: A Literature Review.\narXiv preprint arXiv:2304.03589 (2023).\n[22] Tianxiang Sun, Xiangyang Liu, Wei Zhu, Zhichao Geng, Lingling Wu, Yilong He,\nYuan Ni, Guotong Xie, Xuanjing Huang, and Xipeng Qiu. 2022. A Simple Hash-\nBased Early Exiting Approach For Language Understanding and Generation. In\nFindings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.).\nAssociation for Computational Linguistics, 2409â€“2421.\n[23] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, and\nDacheng Tao. 2022. Patch Slimming for Efficient Vision Transformers. In\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New\nOrleans, LA, USA, June 18-24, 2022 . IEEE, 12155â€“12164.\n[24] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2023. Efficient\nTransformers: A Survey. ACM Comput. Surv. 55, 6 (2023), 109:1â€“109:28.\n[25] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. 2016. BranchyNet:\nFast inference via early exiting from deep neural networks. In 23rd International\nConference on Pattern Recognition, ICPR 2016, CancÃºn, Mexico, December 4-8, 2016 .\nIEEE, 2464â€“2469.\n[26] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and HervÃ© JÃ©gou. 2021. Training data-efficient image transformers\n& distillation through attention. InProceedings of the 38th International Conference\non Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of\nMachine Learning Research, Vol. 139) , Marina Meila and Tong Zhang (Eds.). PMLR,\n10347â€“10357.\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Xu et al.\n[27] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and\nHervÃ© JÃ©gou. 2021. Going deeper with Image Transformers. In 2021 IEEE/CVF\nInternational Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada,\nOctober 10-17, 2021 . IEEE, 32â€“42.\n[28] Maciej Wolczyk, Bartosz WÃ³jcik, Klaudia Balazy, Igor T. Podolak, Jacek Tabor,\nMarek Smieja, and Tomasz Trzcinski. 2021. Zero Time Waste: Recycling Predic-\ntions in Early Exit Neural Networks. InAdvances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual , Marcâ€™Aurelio Ranzato, Alina Beygelz-\nimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.).\n2516â€“2528.\n[29] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations . Association for Computational\nLinguistics, Online, 38â€“45. https://www.aclweb.org/anthology/2020.emnlp-\ndemos.6\n[30] Zhenyu Wu, Zhou Ren, Yi Wu, Zhangyang Wang, and Gang Hua. 2022. TxVAD:\nImproved Video Action Detection by Transformers. In MM â€™22: The 30th ACM\nInternational Conference on Multimedia, Lisboa, Portugal, October 10 - 14, 2022 ,\nJoÃ£o MagalhÃ£es, Alberto Del Bimbo, Shinâ€™ichi Satoh, Nicu Sebe, Xavier Alameda-\nPineda, Qin Jin, Vincent Oria, and Laura Toni (Eds.). ACM, 4605â€“4613.\n[31] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. DeeBERT:\nDynamic Early Exiting for Accelerating BERT Inference. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020 , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.\nTetreault (Eds.). Association for Computational Linguistics, 2246â€“2251.\n[32] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. BERxiT: Early Exiting\nfor BERT with Better Fine-Tuning and Extension to Regression. In Proceedings of\nthe 16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 . Association for\nComputational Linguistics, 91â€“104.\n[33] Li Yang, Mai Xu, Tie Liu, Liangyu Huo, and Xinbo Gao. 2022. TVFormer:\nTrajectory-guided Visual Quality Assessment on 360Â° Images with Transformers.\nIn MM â€™22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal,\nOctober 10 - 14, 2022 , JoÃ£o MagalhÃ£es, Alberto Del Bimbo, Shinâ€™ichi Satoh, Nicu\nSebe, Xavier Alameda-Pineda, Qin Jin, Vincent Oria, and Laura Toni (Eds.). ACM,\n799â€“808.\n[34] Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. 2022.\nPtq4vit: Post-training quantization for vision transformers with twin uniform\nquantization. In European Conference on Computer Vision . Springer, 191â€“207.\n[35] Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. 2022. Self-Distillation: Towards\nEfficient and Compact Neural Networks. IEEE Trans. Pattern Anal. Mach. Intell.\n44, 8 (2022), 4388â€“4403.\n[36] Zhen Zhang, Wei Zhu, Jinfan Zhang, Peng Wang, Rize Jin, and Tae-Sun Chung.\n2022. PCEE-BERT: Accelerating BERT Inference via Patient and Confident Early\nExiting. In Findings of the Association for Computational Linguistics: NAACL\n2022, Seattle, WA, United States, July 10-15, 2022 . Association for Computational\nLinguistics, 327â€“338.\n[37] Chuanyang Zheng, Zheyang Li, Kai Zhang, Zhi Yang, Wenming Tan, Jun Xiao, Ye\nRen, and Shiliang Pu. 2022. SAViT: Structure-Aware Vision Transformer Pruning\nvia Collaborative Optimization. In NeurIPS.\n[38] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley, Ke Xu, and Furu\nWei. 2020. BERT Loses Patience: Fast and Robust Inference with Early Exit.\nIn Advances in Neural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nLGViT: Dynamic Early Exiting for Accelerating Vision Transformer MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\n(a) (b)\nConv EE vs Vanilla EE Conv EE vs ResNet50\nLayers of Conv EE\nLayers of Vanilla EE Layers of ResNet50\nFigure 5: CKA heatmap comparing vanilla EE vs Conv EE and\nConv EE vs ResNet50. Conv EE, which utilizes convolution as\nthe exiting head, can learn different feature representations\ncompared to vanilla EE. The layer incorporating convolution\nin the internal classifier closely resembles the lower half\nof the ResNet layers. Thus, Conv EE is more effective in\ncapturing feature representations than vanilla EE.\nA APPENDIX\nA.1 Outline\nIn this supplementary material, we present a systematic investi-\ngation into the effectiveness of early exiting in ViTs. Besides, we\nprovide more implementation details and experimental compar-\nisons. The main content is summarized as follows:\nâ€¢In Appendix A.2, we conduct a systematic investigation into\nthe effectiveness of early exiting in ViTs and analyze the\nissues arising from the vanilla early exiting. Moreover, we\nobtain two observations: 1) shallow internal classifiers can-\nnot learn sufficient feature representation; 2) deep internal\nclassifiers cannot capture target semantic information.\nâ€¢In Appendix A.3, we perform additional experiments to eval-\nuate our framework. We measure the actual execution time\nof different methods and compare four representative train-\ning schemes with different widely-used exiting heads. Then,\nwe ablate the effect of different exiting position schemes.\nA.2 Investigation of Early Exiting in ViT\nThe early exiting method can halt the forward propagation of neural\nnetworks prematurely to provide a speed-accuracy trade-off, which\nhas achieved significant performance improvements for CNNs and\ntransformers in NLP. However, naively implementing early exiting\non ViT may not yield performance gains for internal classifiers. For\ninstance, the performance of the internal classifier on the fifth and\ntenth layers decreases by 21.8% and 4.0%, respectively, compared\nto the original classifier for ViT-B[7] on CIFAR-100[12]. Upon ex-\namining recent studies on ViT, we discover that a line of works\nfocus on the integration of convolution and self-attention, which\ndemonstrates that convolution operations at shallow layers can\nintroduce additional inductive biases and capture more local infor-\nmation [7, 18]. Another line of works strive to exclusively employ\nself-attention as basic modules to construct the backbone with\nnumerous layers for various vision tasks due to its exceptional ca-\npability in handling long-range dependencies [4, 19, 27]. However,\nit still remains unexplored that 1) whether shallow internal classi-\nfiers could learn sufficient feature representation 2) and whether\ndeep internal classifiers could capture target semantic informa-\ntion. Therefore, we design the following probing experiments to\nanswer these two questions and systematically analyze the working\nmechanism of early exiting methods in ViT.\nVanilla EE @ 3 Vanilla EE @ 9\nAttention EE @ 3 Attention EE @ 9\nVanilla EE @ 6\nAttention EE @ 6\nFigure 6: Comparison of attention maps for early exiting in\nViT using either MLP (vanilla EE) or self-attention (Attention\nEE) as the exiting head. Attention EE methods are more capa-\nble of learning target semantic information by incorporating\nself-attention on deep internal classifiers than vanilla EE.\nRegrading the first question, we initially compare the represen-\ntation similarity of two early exiting (EE) architectures, namely\nMLP (vanilla EE) and convolution (Conv EE), and subsequently\nassess the similarity between Conv EE and ResNet. We employ cen-\ntered kernel alignment (CKA) [10] as the similarity metric, which\nfacilitates quantitative comparisons of representations similarities\nwithin and across neural networks. It is important to note that we\ncompare their representation similarities using outputs from all in-\nternal classifiers and intermediate layers. The results are evaluated\non the ViT-B/16 backbone using CIFAR-100 [12]. We only utilize\nstandard 3 Ã—3 convolution in Conv EE for fair comparison. Figure\n5 displays the CKA similarity results as heatmaps, with the x and y\naxes indexing the layers from input to output. The layers attached\nMLP or convolution are marked with green boxes. Lighter colors\nin the heatmap indicate a higher representation similarity between\nthe corresponding layers. We observe that the layer at which the\ninternal classifier is attached in vanilla EE differs from that in Conv\nEE, suggesting that they extract distinct information, as depicted in\nFigure 5 (a). The layer where the convolution exiting architecture is\npositioned exhibits high similarity to ResNet, as shown in Figure 5\n(b). Consequently, the convolution exiting architecture assists ViT\nin capturing local information and strengthening feature represen-\ntations, allowing Conv EE to learn representations akin to those of\nResNet.\nConcerning the second question, we compare the ability to ex-\ntract target semantic information between two different early exit-\ning architectures, namely vanilla EE and self-attention (attention\nEE) at different exit positions. We compute the attention map and\nvisualize it upon the input image following [3]. The attention map\nhighlights target pixels of the image that contribute to the domi-\nnance of the predicted label, enabling the analysis of the efficacy of\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Xu et al.\nTable 6: Comparison of different training schemes with\nwidely-used exiting heads.\nExiting heads Training schemes Acc. Speed-Up\nMLP Normal 87.3 % 1.56 Ã—\nMLP Weighted 88.2 % 1.53 Ã—\nMLP Distillation 87.1 % 1.57 Ã—\nMLP Alternating 88.1 % 1.54 Ã—\nMLP Ours 88.2 % 1.57 Ã—\nConv Normal 85.2 % 1.69 Ã—\nConv Weighted 86.4 % 1.65 Ã—\nConv Distillation 84.9 % 1.64 Ã—\nConv Alternating 86.7 % 1.67 Ã—\nConv Ours 87.5 % 1.72 Ã—\nAttention Normal 86.8 % 1.54 Ã—\nAttention Weighted 87.8 % 1.53 Ã—\nAttention Distillation 87.0 % 1.57 Ã—\nAttention Alternating 87.5 % 1.54 Ã—\nAttention Ours 88.0 % 1.58 Ã—\nOurs Normal 86.9 % 1.82 Ã—\nOurs Weighted 87.4 % 1.80 Ã—\nOurs Distillation 86.9 % 1.83 Ã—\nOurs Alternating 87.9 % 1.84 Ã—\nOurs Ours 88.5 % 1.87 Ã—\nTable 5: Comparison of execution time on a RTX 3090 GPU.\nMethod MACs Acc. Execution time\nViT-B 16.93 G 90.8 % 6.49 ( Â±0.11) ms\nSDN 10.16 G 86.5 % 5.65 ( Â±0.23) ms\nPABEE 11.48 G 85.1 % 5.86 ( Â±0.09) ms\nPCEE 10.90 G 86.1 % 6.70 ( Â±0.24) ms\nBERxiT 10.27 G 87.1 % 5.75 ( Â±0.13) ms\nViT-EE 11.65 G 87.5 % 5.35 ( Â±0.14) ms\nOurs 9.76 G 88.5 % 5.03 (Â±0.16) ms\nsemantic information extraction from the input space. We employ\nDeiT-B [26] as the backbone, which comprises twelve layers in total.\nThe attention map results for the third, sixth, and ninth layers are\npresented in Figure 6. It becomes more evident that the eye and\nbeak of the bird are target parts for object identification using the\nattention EE method than the vanilla EE method, especially on the\ndeeper layer, such as the ninth layer. Since the attention EE method\nemploys self-attention as the exiting architecture, the ability to\nextract semantic features and spatial relationships can be further\nenhanced. As a result, incorporating self-attention on deep layers\ncan help learn more semantic representations and capture richer\nglobal information than the vanilla EE method.\nBased on the aforementioned analyses, we obtain the following\nobservations:\nâ€¢Observation 1: Shallow internal classifiers cannot learn\nsufficient feature representation.\nâ€¢Observation 2: Deep internal classifiers cannot capture tar-\nget semantic information.\nInsight. We identify the primary reason for the poor perfor-\nmance resulting from directly applying the early exiting strategy\nin ViT. Furthermore, we discover that integrating convolution on\nshallow internal classifiers can enhance local information explo-\nration, while incorporating self-attention on deep layers can im-\nprove the ability to obtain global information. Consequently, if both\nconvolution and self-attention are employed as exiting architec-\ntures, positioned on shallow and deep layers respectively, the model\nwould gain access to a more comprehensive combination of local\nand global information compared to using only MLP as the exiting\narchitecture.\nA.3 Additional Experiments\nIn this section, we evaluate the actual execution time of our method\nand compare four representative training schemes with different\nwidely-used exiting heads. Moreover, we ablate the influences of\ndifferent exiting position schemes.\nExecution time. We design experiments to measure the actual\nexecution time with batch 1 on a RTX 3090 GPU. For each method,\nwe run it for once as a warm-up and then record the execution time\nwith 50 runs without break for the whole testing set of CIFAR-100.\nThe results are shown in Table 5. We can find that our method\nachieves the highest accuracy, the lowest running time and lowest\ncomputation cost.\nExiting heads & training schemes. We compare four represen-\ntative training schemes with different widely-used exiting heads for\nthe ViT backbone on CIFAR-100. The results of different training\nschemes with MLP, Conv, Attention and the proposed exiting heads\nare shown in Table 6. We observe that our training scheme can\nimprove classification accuracy and accelerate inference speed with\ndifferent exiting heads.\nTable 7: Comparison of different exiting positions on CIFAR-\n100.\nPosition scheme Exiting position Acc. Speed-Up\nShallow {2,3,4,5} 85.3 % 1.76 Ã—\nDeep {8,9,10,11} 87.9 % 1.46 Ã—\nMiddle {6,7,8,9} 87.5 % 1.74 Ã—\nUniform (Ours) {4,6,8,10} 88.4 % 1.81 Ã—\nExiting position. In this work, all exiting heads are positioned\nacross all layers uniformly according to an approximately equidis-\ntant computational distribution, i.e., the multiply-accumulate oper-\nations (MACs) of intermediate blocks between two adjacent exiting\npoints remain consistent. We carefully ablate the influences of dif-\nferent exiting position schemes, including positioning at shallow,\ndeep, and middle layers. The results of different exiting positions\nfor the ViT backbone on CIFAR-100 dataset are shown in Table 7.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8302337527275085
    },
    {
      "name": "Transformer",
      "score": 0.6124182939529419
    },
    {
      "name": "Inference",
      "score": 0.6099058389663696
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5835487246513367
    },
    {
      "name": "Edge device",
      "score": 0.5674041509628296
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5558220744132996
    },
    {
      "name": "Deep learning",
      "score": 0.47845458984375
    },
    {
      "name": "Software deployment",
      "score": 0.4630477726459503
    },
    {
      "name": "Machine learning",
      "score": 0.42210447788238525
    },
    {
      "name": "Voltage",
      "score": 0.09530764818191528
    },
    {
      "name": "Engineering",
      "score": 0.09107273817062378
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Cloud computing",
      "score": 0.0
    }
  ]
}