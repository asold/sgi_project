{
  "title": "Enhancing doctor-patient communication using large language models for pathology report interpretation",
  "url": "https://openalex.org/W4406750975",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2160315197",
      "name": "Xiongwen Yang",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2098020676",
      "name": "Yi Xiao",
      "affiliations": [
        "Sun Yat-sen University",
        "Third Affiliated Hospital of Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2108392518",
      "name": "Di Liu",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2103911325",
      "name": "Yun Zhang",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2549567146",
      "name": "Huiyin Deng",
      "affiliations": [
        "Central South University",
        "Third Xiangya Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2096134366",
      "name": "Jian Huang",
      "affiliations": [
        "Jiangxi Provincial Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3030560977",
      "name": "Huiyou Shi",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1973828418",
      "name": "Dan Liu",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2789412117",
      "name": "Maoli Liang",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2061496032",
      "name": "Xing Jin",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2761280653",
      "name": "Yongpan Sun",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2096951688",
      "name": "Jing Yao",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2120826745",
      "name": "Xiaojiang Zhou",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3032540584",
      "name": "Wankai Guo",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2114212131",
      "name": "Yang He",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2135261301",
      "name": "WeiJuan Tang",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2114406445",
      "name": "Chuan Xu",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2160315197",
      "name": "Xiongwen Yang",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2098020676",
      "name": "Yi Xiao",
      "affiliations": [
        "Third Affiliated Hospital of Sun Yat-sen University",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2108392518",
      "name": "Di Liu",
      "affiliations": [
        "Affiliated Hospital of Guizhou Medical University",
        "Guiyang Medical University",
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2103911325",
      "name": "Yun Zhang",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2549567146",
      "name": "Huiyin Deng",
      "affiliations": [
        "Third Xiangya Hospital",
        "Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A2096134366",
      "name": "Jian Huang",
      "affiliations": [
        "Jiangxi Provincial Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3030560977",
      "name": "Huiyou Shi",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1973828418",
      "name": "Dan Liu",
      "affiliations": [
        "Guizhou Provincial People's Hospital",
        "Affiliated Hospital of Guizhou Medical University",
        "Guiyang Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2789412117",
      "name": "Maoli Liang",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2061496032",
      "name": "Xing Jin",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2761280653",
      "name": "Yongpan Sun",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2096951688",
      "name": "Jing Yao",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2120826745",
      "name": "Xiaojiang Zhou",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3032540584",
      "name": "Wankai Guo",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2114212131",
      "name": "Yang He",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2135261301",
      "name": "WeiJuan Tang",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2114406445",
      "name": "Chuan Xu",
      "affiliations": [
        "Guizhou Provincial People's Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4312220150",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4386391923",
    "https://openalex.org/W1973126718",
    "https://openalex.org/W2796839688",
    "https://openalex.org/W2092688999",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4398201981",
    "https://openalex.org/W4324373918",
    "https://openalex.org/W2586182593",
    "https://openalex.org/W2009989010",
    "https://openalex.org/W2262671496",
    "https://openalex.org/W2129751987",
    "https://openalex.org/W1646365943",
    "https://openalex.org/W4254865137",
    "https://openalex.org/W2045241688",
    "https://openalex.org/W1985288472",
    "https://openalex.org/W2151512706",
    "https://openalex.org/W2011761219",
    "https://openalex.org/W2145225199",
    "https://openalex.org/W4388869628",
    "https://openalex.org/W4389792145",
    "https://openalex.org/W3082881684",
    "https://openalex.org/W2064920814"
  ],
  "abstract": "This research demonstrates the efficacy of LLMs like GPT-4 in enhancing doctor-patient communication by translating pathology reports into more accessible language. While this study did not directly measure patient outcomes or satisfaction, it provides evidence that improved understanding and reduced communication time may positively influence patient engagement. These findings highlight the potential of AI to bridge gaps between medical professionals and the public in healthcare environments.",
  "full_text": "Yang et al. \nBMC Medical Informatics and Decision Making           (2025) 25:36  \nhttps://doi.org/10.1186/s12911-024-02838-z\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if \nyou modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or \nparts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\nBMC Medical Informatics and\nDecision Making\nEnhancing doctor-patient communication \nusing large language models for pathology \nreport interpretation\nXiongwen Yang1,2, Yi Xiao3, Di Liu1,2, Yun Zhang4, Huiyin Deng5, Jian Huang6, Huiyou Shi7, Dan Liu8, \nMaoli Liang2,9, Xing Jin1,2, Yongpan Sun1,2, Jing Yao1,2, XiaoJiang Zhou1,2, Wankai Guo1,2, Yang He1,2, \nWeiJuan Tang1,2 and Chuan Xu1,2* \nAbstract \nBackground Large language models (LLMs) are increasingly utilized in healthcare settings. Postoperative pathol-\nogy reports, which are essential for diagnosing and determining treatment strategies for surgical patients, frequently \ninclude complex data that can be challenging for patients to comprehend. This complexity can adversely affect \nthe quality of communication between doctors and patients about their diagnosis and treatment options, potentially \nimpacting patient outcomes such as understanding of their condition, treatment adherence, and overall satisfaction.\nMaterials and methods This study analyzed text pathology reports from four hospitals between October \nand December 2023, focusing on malignant tumors. Using GPT-4, we developed templates for interpretive pathol-\nogy reports (IPRs) to simplify medical terminology for non-professionals. We randomly selected 70 reports to gener-\nate these templates and evaluated the remaining 628 reports for consistency and readability. Patient understanding \nwas measured using a custom-designed pathology report understanding level assessment scale, scored by volun-\nteers with no medical background. The study also recorded doctor-patient communication time and patient compre-\nhension levels before and after using IPRs.\nResults Among 698 pathology reports analyzed, the interpretation through LLMs significantly improved readability \nand patient understanding. The average communication time between doctors and patients decreased by over 70%, \nfrom 35 to 10 min (P < 0.001), with the use of IPRs. The study also found that patients scored higher on understand-\ning levels when provided with AI-generated reports, from 5.23 points to 7.98 points (P < 0.001), with the use of IPRs. \nindicating an effective translation of complex medical information. Consistency between original pathology reports \n(OPRs) and IPRs was also evaluated, with results showing high levels of consistency across all assessed dimensions, \nachieving an average score of 4.95 out of 5.\nConclusion This research demonstrates the efficacy of LLMs like GPT-4 in enhancing doctor-patient communication \nby translating pathology reports into more accessible language. While this study did not directly measure patient \noutcomes or satisfaction, it provides evidence that improved understanding and reduced communication time may \npositively influence patient engagement. These findings highlight the potential of AI to bridge gaps between medical \nprofessionals and the public in healthcare environments.\n*Correspondence:\nChuan Xu\nxuchuan89757@163.com\nFull list of author information is available at the end of the article\nPage 2 of 16Yang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \nKeywords Large language models, Doctor-patient communication, Surgical oncology scene, Postoperative \npathology reports\nIntroduction\nAs medical information technology rapidly advances, \nthe application of artificial intelligence (AI) in health -\ncare is becoming increasingly widespread [1–3]. Notably, \nLarge Language Models (LLMs) have shown potential in \nthe analysis and processing of medical texts [2]. Pathol -\nogy reports, being critical for diagnosis and treatment \ndecisions, directly impact the quality and efficiency \nof doctor-patient communication [4]. However, these \nreports often contain a large amount of professional ter -\nminology and complex data, making them difficult for \npatients to understand. Doctors also face time pressure \nwhen explaining these reports. Therefore, enhancing the \nreadability of pathology reports and improving effec -\ntive communication between doctors and patients has \nbecome crucial for improving the quality of medical ser -\nvices. Additionally, insufficient communication between \ndoctors and patients has been identified as a significant \nfactor affecting patient satisfaction and treatment com -\npliance [5]. Studies have shown that good doctor-patient \ncommunication can significantly improve patients’ \nunderstanding and acceptance of treatment plans, \nthereby affecting treatment outcomes [5, 6].\nIn recent years, LLMs have made significant progress in \nunderstanding and generating natural language, demon -\nstrating their ability to analyze and rewrite medical texts \nin a manner more understandable to non-professionals \n[7, 8]. For instance, Steimetz et  al. (2024) demonstrated \nthat LLM chatbots can significantly improve the read -\nability of pathology reports while also highlighting some \nof the limitations such as inaccuracies and hallucinations \nin the generated reports [9]. This study aims to explore \nthe possibility of using LLMs to enhance the efficiency \nof doctor-patient communication, particularly by auto -\nmating the translation of pathology report content into \npatient-friendly language. This approach aims to reduce \ncognitive barriers to medical information and promote \nbetter patient understanding of their health conditions.\nUsing routine post-operative pathology reports in \noncology, this study designed a universal pathology \nreport interpretation framework through LLMs and \ndeveloped a corresponding pathology report understand-\ning level assessment scale. This was done to explore the \npotential and actual effects of LLMs in enhancing doctor-\npatient communication efficiency.\nTherefore, in response to these challenges, this \nstudy aims to explore the potential of using LLMs to \nenhance doctor-patient communication, particularly \nby simplifying pathology report content into patient-\nfriendly language, and to provide insights on how LLMs \ncan be integrated into clinical practice to improve com -\nmunication efficiency [10, 11].\nBy improving the readability of pathology reports, we \nhope to promote better patient understanding of their \nhealth conditions, strengthen trust and communication \nbetween doctors and patients, and ultimately enhance \nthe overall quality of medical services and patient satis -\nfaction. Trust in physicians, fostered by effective com -\nmunication, plays a pivotal role in treatment adherence. \nResearch indicates that patients who trust their health -\ncare providers are more likely to follow prescribed treat -\nments, which is essential for better health outcomes [12, \n13].\nMaterials and methods\nThe work has been reported in line with the Standards for \nQuality Improvement Reporting Excellence (SQUIRE) \ncriteria [14].\nStudy design\nFrom October to December 2023, text pathology reports \nof malignant tumors were retrieved from the database of \nfour hospitals. Pathology reports included information \non cytology, tissue biopsy examination, and resections. \nAdditionally, all common tumor types were included, \nexcept for rare malignant tumors, which were excluded \ndue to limited sample sizes and follow-up data (Fig. 1).\nAmong the 698 eligible text pathology reports on \nmalignant tumors, 70 reports (5 reports per organ for 14 \norgans) were randomly selected to develop templates for \ninterpretive reports and corresponding scoring scales. \nThese were used to enable LLMs to reliably generate \nsimilar interpretive reports, as well as to produce iden -\ntical outputs from the remaining 628 reports. Doctors \nevaluated each report for consistency by comparing the \noriginal pathology report (OPR) with the AI-generated \nsimplified report (Interpretive pathology report, IPR). \nThe evaluation focused on whether key diagnostic infor -\nmation, such as tumor type (e.g., carcinoma, lymphoma), \ntumor stage (e.g., TNM classification), histological fea -\ntures (e.g., cell differentiation), presence of metastasis, \nand other clinically significant findings (e.g., molecular \nmarkers, margins, and lymph node involvement), were \naccurately represented in the simplified version. Doctors \nfrom multiple specialisms, including pathology, oncology, \nand surgery, participated in this evaluation process. Each \nPage 3 of 16\nYang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \n \nspecialist ensured that the critical diagnostic elements \nwithin their field were accurately reflected. No signifi -\ncant differences were observed between specialisms in \nthe consistency of the outcomes, as all specialists prior -\nitized accuracy and clarity in their respective domains. If \ndiscrepancies were found, the reports were reviewed and \ncorrected to ensure alignment between the two versions. \nThis process is further illustrated in Fig. 2C.\nThe baseline health literacy levels of the volunteers \nwere assessed using the Health Literacy Questionnaire \n(HLQ), ensuring that their understanding of medical \nterminology was evaluated prior to the study [15]. This \nassessment helped us control for variations in health \nliteracy among the volunteers. The results of the HLQ \nassessments are summarized in Table  1. In the study, \nthree volunteers  (VA,  VB, and  VC) with only a high school \neducation and no medical background scored the 698 \nOPRs using the scoring scales (Fig.  2) and recorded \nreading time. Then, three other volunteers  (VD,  VE, and \n VF) with similar backgrounds scored the IPRs using the \nscoring scales (Fig.  2) and recorded reading time. Lastly, \ndoctors (with 10–15 years of experience) communicated \nwith volunteers  (VA,  VB, and  VC) based on the OPRs and \nrecorded doctor-patient communication time, and then \ncommunicated with volunteers  (VD,  VE, and  VF) based on \nthe IPRs and recorded the time. Figure  1 summarizes the \nstudy design.\nScale and template generation\nSeventy pathology reports were assigned to an author \n(X.W.Y) to construct scales and templates (Fig.  2), aimed \nat evaluating the accuracy and repeatability of IPRs gen -\nerated by GPT-4 through quantitative metrics.\nA pathology report understanding level assessment \nscale is presented in Fig.  2A. This scale aims to compre -\nhensively assess the understanding level of non-medical \nbackground individuals regarding pathology reports. \nPatient understanding was measured using a custom-\ndesigned pathology report understanding level assess -\nment scale, developed based on established health \nFig. 1 Study design flow chart. The pathology reports from pathologists (Label  (A#……N#)) were fed into the natural language processing \n(NLP) pipeline to generate new pathology interpretation reports (Label (A……N)). Label  (A#……N#) and Label (A……N) were both read \nand scored by three volunteers, and the results were statistically compared with each other. In addition, the understanding of Label  (A#……N#) \nand Label (A……N) were scored by the volunteers through the pathological score scale. Meanwhile, the doctor-patient communication \ntime after the volunteers read Label  (A#……N#) and Label (A……N) was also recorded and statistically analyzed. The pathological score \nscale was generated by the large language model (LLM), which was modified and organized by pathologist. The dotted lines indicate \nthat both pathologists and/or volunteers participated in the corresponding task of the study and interacted with each other during the process\nPage 4 of 16Yang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \nFig. 2 A Pathology report understanding level assessment scale. B Pathology report interpretation template. C Pathology Artificial Intelligence \nQuality Index. The scales and template were designed by large language model (LLM), and the pathologist modified and organized the scale\nPage 5 of 16\nYang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \n \nliteracy principles. The scale drew from the Health Lit -\neracy Questionnaire (HLQ) and other key research on \nhealth literacy [15–18]. It was designed to assess the clar-\nity, relevance, and ease of understanding of key informa -\ntion in pathology reports, specifically for individuals with \nno medical background. The scale was refined through \npilot testing to ensure its applicability for the study \npopulation.\nA pathology report interpretation template is depicted \nin Fig.  2B. This template is intended as a general frame -\nwork; specific content needs to be filled in and adjusted \naccording to the actual details of each pathology report. \nThis aims to assist individuals without a medical back -\nground in understanding the content and importance \nof pathology reports. The iterative prompt engineering \ninvolved multiple steps: First Prompt: “Summarize the \npathology report for a layperson. ” Refinement: “Summa -\nrize the pathology report in simple language, explaining \nthe diagnosis, significance, and next steps. ” Final Prompt: \n“Translate the pathology report into easy-to-understand \nlanguage, include diagnosis, clinical significance, treat -\nment options, and follow-up recommendations. ” The \nOPRs were generated using the refined templates. Each \nsection of the template was filled with specific details \nfrom the pathology reports, ensuring consistency and \ncomprehensibility. Examples of these templates and filled \nreports are illustrated in Figs. 2B and 3.\nA pathology AI quality index is shown in Fig.  2C. This \nindex was developed using GPT-4 and further refined \nthrough discussions with pathologists, who finalized the \ncontent and scoring criteria. Using this scale, doctors \ncan comprehensively evaluate the quality of pathology \ninterpretation reports generated by GPT-4. By sum -\nmarizing the scores, it is possible to roughly determine \nGPT-4’s level of understanding and interpreting pathol -\nogy reports, as well as its potential value in clinical appli -\ncations. This method was designed to rigorously compare \nthe IPRs generated by GPT-4 against the standards set \nby the OPRs. The evaluation was conducted across five \nkey dimensions by three pathologists, each with over a \ndecade of professional experience: Accuracy (Dimen -\nsion A), Interpretative Depth (Dimension B), Readabil -\nity (Dimension C), Clinical Relevance (Dimension D), \nand Overall Evaluation (Dimension E). Pathologist X \nis a general pathologist working in a university hospital \nwith expertise in oncologic pathology; Pathologist Y is \na thoracic pathologist with specialization in lung cancer \ndiagnostics, working at a non-university cancer center; \nand Pathologist Z is a gastrointestinal pathology expert \naffiliated with a leading academic medical center. All \npathologists have extensive experience in analyzing com -\nplex pathology reports and contributing to AI-assisted \ndiagnostic models. Their diverse backgrounds ensured a \ncomprehensive evaluation of the pathology reports from \ndifferent perspectives. This comprehensive review aimed \nto determine how well the GPT-4-generated reports cap -\ntured the essence of the OPRs. The results, as adjudicated \nby the pathologists—referred to as Pathologist X, Pathol -\nogist Y, and Pathologist Z.\nTo evaluate the text complexity of both OPRs and IPRs, \nwe calculated the word count using the word count fea -\nture in Microsoft Office 365 (Microsoft Corporation, \nRedmond, WA, USA). This method provided a quanti -\ntative measure of report length, allowing us to compare \nword counts across different types of malignancies and \nbetween OPRs and IPRs.\nPatient data anonymization and security\nTo secure patient data, all identifying information was \nanonymized before being processed by the LLM/GPT \nmodel. The anonymization process ensured that no \npersonal information, such as names, dates of birth, or \nmedical record numbers, was included in the dataset. \nAdditionally, the LLM was used in a secure, isolated envi-\nronment that complied with data protection regulations, \nincluding [specific regulations if applicable, e.g., GDPR \nTable 1 Baseline health literacy levels\nScores on the HLQ dimensions range from 1 to 4, with higher scores indicating higher levels of health literacy\nHealth Literacy Dimension Average Score\nFeeling Understood and Supported by Healthcare Providers 3.92\nHaving Sufficient Information to Manage My Health 3.83\nActively Managing My Health 3.58\nSocial Support for Health 3.58\nAppraisal of Health Information 3.83\nAbility to Actively Engage with Healthcare Providers 3.83\nNavigating the Healthcare System 3.42\nAbility to Find Good Health Information 3.75\nUnderstanding Health Information Well Enough to Know What to Do 3.92\nPage 6 of 16Yang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \nFig. 3 Application of interpretive pathology report (IPR). A Original pathology report (OPR). B Corresponding IPR\nPage 7 of 16\nYang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \n \nor HIPAA]. These measures ensured that no sensitive \npatient data was exposed or accessible outside the study, \nsafeguarding patient confidentiality while allowing for \naccurate AI-generated pathology report analysis.\nStatistical analyses\nThe data are presented as either mean ± standard devia -\ntion, minimum and maximum values. We evaluated the \ndata from groups through Shapiro–Wilk test for normal -\nity test. If the data followed normal distribution, t test \nwas used for statistical analysis; otherwise, Mann–Whit -\nney U test was used for statistical analysis. The relation -\nships between continuous variables were determined \nusing Spearman’s correlation analysis. A P < 0.05 was \ndeemed to indicate statistical significance. All statistical \ncalculations were carried out using R software, version \n4.3.2 (Lucent Technologies, Murray Hill, NJ, USA).\nResults\nCharacteristics of sample\nBetween October and December 2023, a total of 3,082 \npatients were screened at four institutions, as illustrated \nin Fig.  1. Of these, 2,353 patients were excluded due to \npathologically confirmed benign tumors. Additionally, 31 \npatients with rare malignant tumors were excluded due \nto challenges associated with follow-up data collection, \nwhich primarily included the geographical dispersion of \npatients, variability in hospital record-keeping practices, \nand inconsistent communication channels across insti -\ntutions. Consequently, the study included 698 patients \nfor further analysis. The majority of the study cohort \nwere female, as detailed in Table 2. The participants’ ages \nranged widely from 24 to 82  years, with an average age \nof 55.27  years. A significant proportion, approximately \n85.67%, were below the age of 65.\nText data extractions\nAs shown in Table 3, the average word count of OPRs was \n549.98. Notably, brain malignancies had the lowest aver -\nage word count for their OPRs, at 406.78, whereas ovar -\nian malignancies had the highest, at 961.21. The analysis \nalso revealed an average of 19.73 medical terms per OPR \nacross all studied categories of malignant tumors. Pros -\ntate malignancies had the fewest average medical terms, \nat 14.46, while ovarian malignancies had the most, aver -\naging 30.43 medical terms.\nWe observed that the average word count for OPRs \nacross all types of malignant tumors was 549.98, while \nthe average word count for IPRs was significantly higher \nat 787.44. Liver malignancies had the lowest average \nword count for OPRs (441.41) and IPRs (775.25). In con -\ntrast, ovarian malignancies had the highest average word \ncount for OPRs (961.21), while esophagus malignancies \nhad the highest average word count for IPRs (833.80). \nThis suggests that although there is significant variation \nin the word count of OPRs among different malignancies \n(P < 0.001), the variation in IPR word counts is less pro -\nnounced (P = 0.088, Figs. 4 and 5).\nMoreover, the word count for the OPRs of ovar -\nian malignant tumors was higher than that for the IPRs \n(P < 0.001), whereas the word counts for the OPRs of \nother cancer types were lower than those for the IPRs \n(P < 0.001).\nConsistency evaluation of expression content\nTo assess the fidelity and quality of IPRs relative to \nOPRs, we utilized a consistency evaluation scale devel -\noped with GPT-4, as shown in Fig.  2C. The results, as \nadjudicated by the pathologists—referred to as Patholo -\ngist X, Pathologist Y, and Pathologist Z—showed no sig -\nnificant statistical differences in their assessments across \nthe dimensions. Remarkably, all dimensions consist -\nently scored 4 or higher, with Readability (Dimension C) \nnotably achieving a unanimous score of 5, as detailed in \nTable 4.\nPathology report reading time\nTwo groups of volunteers separately read OPRs  (VA, \n VB, and  VC) and IPRs  (VA,  VB, and  VC), with their read -\ning times recorded (Table  5, Fig.  4  and  6). The aver -\nage reading time for OPRs across all types of malignant \ntumors was 401.76 s. Notably, brain malignancies had the \nshortest average reading time at 305.47 s, whereas ovar -\nian malignancies had the longest at 700.64  s, indicating \nTable 2 Basic characteristics of patients\na  Data are means ± SDs, with ranges in parentheses\nCancer Sites Patients Age (years)a Sex (M, F)\nAll sites 698 55.27 ± 12.66 (24, 82) 290 (41.55%), 408 \n(58.45%)\nBrain 32 58.16 ± 11.25 (34, 79) 13 (40.62%), 19 (59.38%)\nThyroid 76 44.53 ± 11.75 (24, 74) 32 (42.11%), 44 (57.89%)\nBreast 86 50.98 ± 11.32 (25, 80) 0 (0.00%), 86 (100.00%)\nLung 98 58.04 ± 11.64 (32, 82) 49 (50.00%), 49 (50.00%)\nEsophagus 10 63.10 ± 7.28 (50, 71) 7 (70.00%), 3 (30.00%)\nGastric 30 55.30 ± 12.42 (25, 80) 18 (60.00%), 12 (40.00%)\nLiver 32 61.53 ± 12.47 (35, 82) 24 (75.00%), 8 (25.00%)\nPancreatic 18 56.39 ± 11.63 (37, 76) 15 (83.33%), 3 (16.67%)\nColorectal 74 61.03 ± 12.77 (27, 82) 31 (41.89%), 43 (58.11%)\nKidney 61 62.08 ± 10.02 (34, 82) 31 (50.82%), 30 (49.18%)\nProstate 37 72.89 ± 3.70 (67, 82) 37 (100.00%), 0 (0.00%)\nBladder 50 70.06 ± 5.75 (58, 81) 33 (66.00%), 17 (34.00%)\nOvary 61 52.11 ± 6.27 (39, 68) 0 (0.00%), 61(100.00%)\nUterus 33 53.45 ± 3.24 (47, 59) 0 (0.00%), 33 (100.00%)\nPage 8 of 16Yang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \nstatistically significant differences in reading times for \nOPRs across tumor types (P < 0.001). In contrast, the \naverage reading time for IPRs was 430.67  s, with the \nshortest for liver malignancies at 418.88 s, and the long -\nest for esophagus tumors at 452.10 s. No significant dif -\nferences were observed in the reading times for IPRs \nacross the tumor types (P = 0.413).\nA comparison of the reading times between OPRs and \nIPRs for all types of malignant tumors revealed that OPRs \nwere generally read faster than IPRs, with a statistically \nsignificant difference (P < 0.001). However, for bladder, \novarian, and uterus malignancies, the reading times were \nlonger for OPRs compared to IPRs, with these differences \nalso being statistically significant (P < 0.001 for each).\nUnderstanding level assessment\nThe evaluation further involved a multidimensional scor -\ning of OPRs and IPRs using the Pathology Report Under -\nstanding Level Assessment Scale, as shown in Table  5, \nFigs.  2A and  6. Across all types of malignant tumors, \nthe average score for OPRs was 5.23. In comparison, the \naverage score for IPRs was significantly higher, at 7.98. \nThis disparity in scoring between OPRs and IPRs across \nall tumor types was statistically significant (P < 0.001).\nTable 3 Characteristics of pathology reports\nIntentionally minimized to ensure the reports are accessible to a non-medical audience. The goal of the IPRs is to enhance understanding for patients and laypersons, \nwhich is why medical terms were avoided in the report generation process\nOPRs Original pathology reports, IPRs Interpretive pathology reports\n*  Data are means ± SDs, with ranges in parentheses\n**  The OPRs and IPRs of different cancer sites were analyzed statistically\nCancer Sites Pathology \nreports\nOPRs (Word count)* OPRs (medical terms)* IPRs (Word count)* P value**\nAll sites 698 549.98 ± 154.72 19.73 ± 5.22 787.44 ± 53.51  < 0.001\n(304, 1154) (10, 34) (657, 875)\nBrain 32 406.78 ± 28.00 16.81 ± 2.89 786.47 ± 51.31  < 0.001\n(306, 454) (10, 22) (701, 874)\nThyroid 76 434.45 ± 52.39 16.84 ± 3.00 789.34 ± 56.78  < 0.001\n(304, 564) (10, 24) (695, 875)\nBreast 86 485.23 ± 56.85 19.43 ± 3.22 785.42 ± 56.48  < 0.001\n(398, 686) (11, 25) (697, 874)\nLung 98 552.62 ± 29.48 20.96 ± 3.61 785.42 ± 56.48  < 0.001\n(500, 598) (12, 32) (697, 874)\nEsophagus 10 448.90 ± 33.09 14.70 ± 2.63 833.80 ± 41.58  < 0.001\n(400, 498) (12, 20) (764, 875)\nGastric 30 497.67 ± 34.91 15.30 ± 2.77 780.03 ± 47.01  < 0.001\n(443, 597) (12, 21) (701, 860)\nLiver 32 441.41 ± 70.15 14.75 ± 2.69 775.25 ± 51.74  < 0.001\n(306, 570) (10, 20) (698, 853)\nPancreatic 18 461.89 ± 39.06 16.00 ± 4.64 788.61 ± 54.91  < 0.001\n(403, 517) (10, 24) (700, 875)\nColorectal 74 513.89 ± 56.49 16.47 ± 2.88 784.73 ± 51.07  < 0.001\n(366, 654) (12, 24) (696, 874)\nKidney 61 500.87 ± 30.93 21.95 ± 2.96 790.98 ± 50.48  < 0.001\n(428, 553) (16, 28) (702, 874)\nProstate 37 453.11 ± 60.71 14.46 ± 2.26 808.08 ± 49.10  < 0.001\n(343, 568) (10, 19) (702, 873)\nBladder 50 679.28 ± 47.49 23.48 ± 1.97 776.00 ± 55.78  < 0.001\n(602, 785) (20, 28) (697, 873)\nOvary 61 961.21 ± 67.47 30.43 ± 1.88 781.70 ± 54.90  < 0.001\n(802, 1154) (28, 34) (657, 874)\nUterus 33 671.48 ± 56.57 22.52 ± 3.02 797.94 ± 46.99  < 0.001\n(519, 777) (18, 29) (711, 871)\nPage 9 of 16\nYang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \n \nDoctor‑patient communication\nAfter volunteers (A, B and C) finished reading the OPRs, \nthe doctor engaged in simulated doctor-patient commu -\nnication with the volunteers to explain the patient’s con -\ndition and recorded the communication time (Table  5, \nFigs. 4 and 6D). Across all types of malignant tumors, the \naverage communication time was 2091.25 s. Specifically, \nbrain malignancies exhibited the longest average commu-\nnication time at 2154.41  s, while prostate malignancies \nhad the shortest at 2062.03 s. Statistical analysis revealed \nno significant differences in communication times across \nthe different tumor types (P = 0.734). Additionally, \nafter volunteers (D, E and F) finished reading the IPRs, \nthe doctor conducted simulated doctor-patient com -\nmunication based on the report content, explained the \npatient’s condition, and recorded the communication \ntime. Across all types of malignant tumors, the average \ncommunication time was 599.15  s. The longest average \nFig. 4 Comparative analysis of original pathology reports (OPRs) and interpretive pathology reports (IPRs) metrics across cancer sites. RT: Reading \ntime. DPCT: Doctor-patient communication time\nPage 10 of 16Yang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \ncommunication time occurred with esophagus malig -\nnancies, at 638.30  s, while the shortest was for gastric \nmalignancies, at 581.80 s. Statistical analysis indicated no \nsignificant differences in communication times among \nthe various types of malignant tumors (P = 0.467). Fur-\nther analysis showed that, regardless of the tumor type, \nthe communication time after reading the OPRs was sig -\nnificantly longer than that after reading the IPRs, a differ-\nence that was statistically significant (P < 0.001).\nCorrelation of OPRs and IPRs metrics\nWe analyzed the correlation between various metrics \nof OPRs and IPRs, as illustrated in Fig.  6. This heatmap \nprovides a clear and intuitive display of the correlations \namong nine key metrics within OPRs and IPRs. It reveals \na strong correlation between word count, medical terms, \nscore, and reading time for OPRs. The figure serves as a \nvisually intuitive tool to identify both the strength and \nthe direction of relationships between these metrics.\nDiscussion\nOur research on the application of GPT-4-generated \nIPRs in enhancing doctor-patient communication sup -\nports the expanding role of AI within healthcare, offering \nvaluable insights that are particularly relevant to surgical \nsettings. The principal outcomes of our study substanti -\nate the integration of AI to augment patient compre -\nhension and communication efficacy. Comparatively \nFig. 5 Original pathology reports (OPRs) vs. interpretive pathology reports (IPRs) comparison (word count, score, reading time and doctor-patient \ncommunication time) by cancer site. RT: Reading time. DPCT: Doctor-patient communication time\nPage 11 of 16\nYang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \n \nreviewing recent scholarly work situates our study within \nthe modern scientific discourse, emphasizing the novel \ncontributions and prospective advancements our find -\nings introduce to the field [2, 8].\nAcross all types of malignant tumors, the use of IPRs \nresulted in significantly higher patient understanding \nscores compared to traditional OPRs, with an average \nimprovement from 5.23 to 7.98 on the Pathology Report \nUnderstanding Level Assessment Scale. Furthermore, \nthe study found a substantial reduction in doctor-patient \ncommunication time when using IPRs, decreasing from \nan average of 2091.25  s to 599.15  s, underscoring the \npotential time-saving benefits of AI-assisted reports. \nThese findings suggest that AI-generated reports can \nenhance doctor-patient communication while also \nimproving overall healthcare efficiency.\nIn addition to improving communication time and \ncomprehension, the consistency evaluation conducted \nby pathologists highlighted that the IPRs generated \nby GPT-4 were highly accurate, scoring consistently \nacross dimensions such as Accuracy, Interpretative \nDepth, and Readability. This consistency in evaluation \nacross different tumor types supports the robustness of \nthe AI-generated reports, indicating their potential for \nwidespread clinical application. The strong correlation \nobserved between OPR and IPR metrics further empha -\nsizes the effectiveness of the AI model in maintaining \nclinical relevance while simplifying report content for \npatient understanding. This enhanced understanding \nis critical as it directly influences patient engagement \nand empowerment. Patients who grasp their medical \nconditions and the logic behind their treatment options \nare more inclined to adhere to recommended treatments \nand engage in proactive health management. This link \nbetween comprehension and compliance is well-docu -\nmented in healthcare literature, with our data provid -\ning robust evidence of AI’s pivotal role in fostering this \nunderstanding [19–22].\nMoreover, recent studies have increasingly acknowl -\nedged AI’s capability to enhance the accessibility and \ncomprehensibility of medical documentation. For \ninstance, Amin et  al. employed three prominent large \nlanguage models—ChatGPT, Google Bard, and Microsoft \nBing—to simplify radiology reports [23]. Subsequently, \nthey solicited assessments from pertinent clinical practi -\ntioners concerning the accuracy of each model’s output. \nNevertheless, the research did not address the compre -\nhensibility of these simplified radiology reports for indi -\nviduals lacking a medical background. Consequently, \nthe applicability of large language models in making \nradiological information accessible to a broader, non-\nspecialist audience remains unverified [23]. Truhn et  al. \nutilized GPT-4 to generate structured pathology reports, \ndemonstrating that structured reports generated by large \nlanguage models are consistent with those produced by \npathologists [24]. This indicates that LLMs could poten -\ntially be employed routinely to extract ground truth \ndata for machine learning from unstructured pathology \nreports in the future. However, this study focused only \non evaluations by professionals and lacks an assessment \nof the usability of AI-generated reports in broader sce -\nnarios. Similarly, Steimetz et  al. examined methods for \nTable 4 Evaluation of consistency between original radiology reports and interpretive radiology reports\nCancer Site Dimension A \n(Accuracy)\nDimension B \n(Interpretation Depth)\nDimension C \n(Readability)\nDimension D (Clinical \nRelevance)\nDimension \nE (Overall \nEvaluation)\nAll sites 4.95 4.95 5 4.92 4.84\nBrain 5 4.97 5 4.91 4.91\nThyroid 4.93 4.96 5 4.83 4.83\nBreast 4.94 4.94 5 4.91 4.8\nLung 4.95 4.94 5 4.93 4.83\nEsophagus 5 5 5 4.9 4.9\nGastric 4.97 4.97 5 4.9 4.83\nLiver 5 4.94 5 4.97 4.91\nPancreatic 4.89 4.89 5 4.89 4.67\nColorectal 4.96 4.97 5 4.95 4.88\nKidney 4.93 4.95 5 4.97 4.85\nProstate 4.95 4.95 5 4.95 4.84\nBladder 4.96 4.92 5 4.96 4.86\nOvary 4.93 4.95 5 4.93 4.82\nUterus 4.94 4.97 5 4.88 4.79\nPage 12 of 16Yang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \nsimplifying medical documents to improve patient com -\nprehension, finding that enhancing readability directly \nimpacts patient engagement and satisfaction [9]. In addi -\ntion, Singhal et al. showed that LLMs effectively encode \nclinical knowledge, reinforcing their potential in improv -\ning healthcare communication [8]. Harrer further dis -\ncusses the ethical considerations and complexities of \nintegrating large language models into medical systems, \nemphasizing the importance of thoroughly evaluating \ntheir real-world applications to ensure both patient safety \nand accuracy [11].\nBuilding on previous research, our study simulated \ninteractions between doctors and patients regarding \nthe interpretation of postoperative pathology reports in \nsurgical settings [9, 23, 24]. It demonstrated the univer -\nsal applicability of explanations generated by large lan -\nguage models across different demographic groups. This \nresearch goes beyond simply translating and simplifying \nprofessional reports; it highlights the importance of such \nmodels as bridges between professional and non-profes -\nsional domains, thereby expanding the use of large lan -\nguage models in real-world healthcare settings.\nTable 5 Volunteers’ evaluation of the original pathology reports and the interpretive pathology reports generated based on GPT-4\nOPRs Original pathology reports, IPRs Interpretive pathology reports, RT Reading time, DPCT doctor-patient communication time\na  Volunteers A, B, and C were high school educated people with non-medical backgrounds, aged 50, 50, and 52 years old, and their genders were male, female, and \nfemale, respectively. In addition, the matched volunteers D, E and F are also high school educated people with non-medical background, their ages are 50, 51 and \n51 years old respectively, and their genders are male, female and female respectively\nb  Data are means ± SDs, with ranges in parentheses\nCancer Sites V (A, B, C)a V (D, E, F)a P V (A, B, C) V (D, E, F) P V (A, B, C) V (D, E, F) P\nOPRs (RT)b IPRs (RT) OPRs (Score) IPRs (Score) OPRs (DPCT) IPRs (DPCT)\nAll sites 401.76 ± 112.06 430.67 ± 37.81  < 0.001 5.23 ± 0.88 7.98 ± 0.82  < 0.001 2091.25 ± 170.90 599.15 ± 69.31  < 0.001\n(223, 841) (348, 524) (4, 6) (7, 9) (1801, 2400) (480, 720)\nBrain 305.47 ± 21.55 428.06 ± 37.64  < 0.001 5.03 ± 0.74 8.00 ± 0.76  < 0.001 2154.41 ± 148.01 612.66 ± 69.27  < 0.001\n(230, 340) (363, 524) (4, 6) (7, 9) (1818, 2392) (497, 720)\nThyroid 321.33 ± 38.80 432.11 ± 39.31  < 0.001 6.00 ± 0.00 7.87 ± 0.82  < 0.001 2104.17 ± 165.55 601.67 ± 69.50  < 0.001\n(225, 417) (359, 523) (6, 6) (7, 9) (1802, 2391) (483, 716)\nBreast 354.56 ± 41.56 435.30 ± 40.24  < 0.001 4.90 ± 0.72 7.95 ± 0.85  < 0.001 2083.97 ± 183.14 590.48 ± 70.42  < 0.001\n(291, 501) (362, 517) (4, 6) (7, 9) (1805, 2394) (480, 715)\nLung 402.52 ± 21.47 428.65 ± 40.78  < 0.001 5.41 ± 0.69 7.98 ± 0.81  < 0.001 2074.30 ± 169.04 597.28 ± 71.82  < 0.001\n(364, 436) (354, 517) (4, 6) (7, 9) (1803, 2400) (484, 717)\nEsophagus 326.90 ± 24.18 452.10 ± 32.22  < 0.001 4.00 ± 0.00 8.40 ± 0.84  < 0.001 2030.00 ± 163.08 638.30 ± 65.62  < 0.001\n(291, 363) (395, 491) (4, 4) (7, 9) (1805, 2306) (536, 720)\nGastric 362.53 ± 25.41 428.00 ± 37.60  < 0.001 5.70 ± 0.47 8.03 ± 0.85  < 0.001 2079.67 ± 198.41 581.80 ± 67.32  < 0.001\n(323, 435) (361, 510) (5, 6) (7, 9) (1801, 2396) (481, 706)\nLiver 321.41 ± 51.14 418.88 ± 37.06  < 0.001 6.00 ± 0.00 8.03 ± 0.86  < 0.001 2088.75 ± 181.83 612.81 ± 59.89  < 0.001\n(223, 415) (351, 496) (6, 6) (7, 9) (1821, 2388) (484, 707)\nPancreatic 336.56 ± 28.48 433.72 ± 37.05  < 0.001 4.17 ± 0.38 7.89 ± 0.90  < 0.001 2103.56 ± 148.86 593.33 ± 73.37  < 0.001\n(293, 377) (354, 511) (4, 5) (7, 9) (1874, 2379) (502, 720)\nColorectal 374.41 ± 41.24 429.35 ± 38.25  < 0.001 5.89 ± 0.31 8.00 ± 0.81  < 0.001 2095.24 ± 161.22 607.85 ± 73.10  < 0.001\n(266, 477) (352, 514) (5, 6) (7, 9) (1805, 2392) (483, 718)\nKidney 364.85 ± 22.54 431.59 ± 30.75  < 0.001 6.00 ± 0.00 8.08 ± 0.80  < 0.001 2088.69 ± 181.35 585.89 ± 67.33  < 0.001\n(312, 403) (378, 518) (6, 6) (7, 9) (1823, 2393) (480, 715)\nProstate 330.03 ± 44.33 440.27 ± 32.33  < 0.001 6.00 ± 0.00 7.89 ± 0.88  < 0.001 2062.03 ± 147.32 598.19 ± 69.89  < 0.001\n(250, 414) (391, 512) (6, 6) (7, 9) (1806, 2329) (492, 714)\nBladder 494.98 ± 34.63 424.60 ± 37.97  < 0.001 4.16 ± 0.37 8.02 ± 0.77  < 0.001 2085.76 ± 175.08 602.08 ± 68.73  < 0.001\n(439, 572) (353, 507) (4, 5) (7, 9) (1805, 2398) (484, 710)\nOvary 700.64 ± 49.20 426.98 ± 38.05  < 0.001 4.00 ± 0.00 7.95 ± 0.85  < 0.001 2112.07 ± 177.29 602.00 ± 65.65  < 0.001\n(584, 841) (348, 517) (4, 4) (7, 9) (1801, 2394) (492, 720)\nUterus 489.27 ± 41.26 436.06 ± 36.86  < 0.001 4.00 ± 0.00 7.94 ± 0.75  < 0.001 2092.79 ± 167.61 598.55 ± 69.28  < 0.001\n(378, 566) (360, 503) (4, 4) (7, 9) (1844, 2388) (480, 712)\nPage 13 of 16\nYang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \n \nAnother significant observation from our study was \nthe reduction in communication time between doc -\ntors and patients. The average duration for doctors to \nexplain pathological reports decreased dramatically from \napproximately 35  min with OPRs to about 10  min with \nIPRs, marking a reduction of over 70% in communication \ntime. This efficiency gain is especially critical in surgical \nsettings where time is scarce, and the cognitive load on \npatients is substantial due to the stress and complexity of \ntheir medical situations. By minimizing the time needed \nto convey essential information, doctors can dedicate \nmore time to addressing patient concerns, answering \nquestions, and providing personalized care. Additionally, \nthis efficiency may lead to increased patient throughput, \nessential in high-demand environments like surgical \nunits. The scarcity of medical resources globally further \nunderscores the importance of these findings, suggesting \nthat large language models can significantly alleviate the \nstrain on healthcare resources.\nAdditionally, our study demonstrates that the IPRs \ngenerated by GPT-4 show a high degree of consistency \nwith the OPRs, as evaluated across key dimensions such \nas accuracy, interpretative depth, and readability. These \nfindings underscore the robustness of the evaluative \nframework in verifying that the IPRs accurately represent \nthe key insights of the OPRs. This framework not only \nensures that the generated reports are consistent with \nthe original medical data, but also plays a crucial role in \nFig. 6 Correlation heatmap of original pathology reports (OPRs) and interpretive pathology reports (IPRs). RT: Reading time. DPCT: Doctor-patient \ncommunication time\nPage 14 of 16Yang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \nmaintaining the integrity and reliability of the pathol -\nogy interpretation process. By systematically comparing \nmultiple dimensions, the framework provides a com -\nprehensive assessment that helps to identify potential \ndiscrepancies and ensures the clinical relevance of the \nreports. This rigorous approach allows for the use of AI-\ngenerated reports with greater confidence in real-world \nmedical settings, ultimately contributing to more efficient \ndoctor-patient communication and improved healthcare \noutcomes. With proper training and model adjustments, \nLLMs like GPT-4 can achieve high levels of accuracy and \nreliability in interpreting and simplifying complex surgi -\ncal pathology reports, vital for patient recovery and com -\nprehension post-surgery.\nThe implications of these findings for clinical practice \nare profound. Integrating AI-generated IPRs into health -\ncare systems can be achieved through several practi -\ncal steps. First, hospitals and clinics can implement AI \nmodels like GPT-4 to automatically generate simplified, \npatient-friendly pathology reports alongside traditional \nreports. These AI-generated reports can be shared with \npatients via patient portals or during face-to-face consul -\ntations. Additionally, training healthcare providers to uti-\nlize AI-generated reports as communication tools during \nconsultations can further enhance patient understand -\ning. By offering easy-to-understand summaries, patients \nare more likely to engage with their care plans, leading \nto greater satisfaction and better adherence to treatment, \nultimately contributing to improved health outcomes. \nAdditionally, reducing the time spent on routine expla -\nnations can alleviate workload pressures on healthcare \nprofessionals, potentially enhancing job satisfaction and \nreducing burnout.\nHowever, it is important to note that this study was \nconducted in a Chinese-speaking region, and all pathol -\nogy reports, whether original or interpretive, were writ -\nten in Chinese. The language and cultural background \nmay influence the generalizability of our findings. During \nthe template generation and evaluation process, we care -\nfully considered the use of Traditional Chinese Medicine \n(TCM) terminology and the specific structure of Chinese \npathology reports. Therefore, in real-world applications, \nit is crucial to take cultural and linguistic contexts into \naccount when applying the conclusions of this study.\nWhile our study utilized volunteers to simulate patient \ninteractions, we acknowledge the potential differences \nbetween volunteers and real patients. Real patients in \nclinical settings often experience a range of emotions, \nsuch as anxiety, fear, and distress, which can influence \ntheir behavior, decision-making, and communication \nefficiency. Studies have shown that patients under emo -\ntional distress may struggle with comprehension and \nretention of medical information, potentially impacting \ntheir ability to engage in effective communication with \nhealthcare providers [25]. In contrast, volunteers in our \nstudy, who were aware of the non-threatening nature of \nthe environment, did not experience these emotional \nstressors. As such, future research should aim to include \nreal patients to better capture the complexity of clinical \ninteractions and the impact of emotional states on com -\nmunication outcomes.\nDespite the promising results, our study acknowledges \nseveral key limitations that warrant careful considera -\ntion. These limitations highlight areas for cautious inter -\npretation of the results and suggest potential avenues for \nfuture research to address these gaps. First, our study’s \nheavy reliance on the capabilities of GPT-4, a specific \nversion of Large Language Models developed by OpenAI, \nraises questions about the generalizability of our find -\nings. While GPT-4 is renowned for its sophisticated nat -\nural language processing capabilities, it represents only \none example of such technologies. Different LLMs may \nexhibit varying effectiveness based on their training data \nand underlying algorithms. Future research could explore \nthe performance of other LLMs in similar tasks to verify \nif the observed benefits are replicable across different \nAI platforms. Second, the demographic and geographic \ndiversity of our patient sample was confined to specific \nhospitals within a limited region, which may restrict the \napplicability of our results to other settings where patient \npopulations differ significantly in terms of language, \nculture, and healthcare practices. Additionally, the sam -\nple size, while sufficient for statistical analysis, may not \nfully capture the variability and complexity of patient \nexperiences across broader populations. Expanding the \nsample size and including a more diverse patient group \nin future studies could provide insights into how differ -\nent populations interact with and benefit from AI-gen -\nerated reports. Third, the primarily quantitative nature \nof our study provides a robust statistical foundation for \nevaluating the effectiveness of AI in improving patient \nunderstanding and communication efficiency. However, \nthis approach may overlook the nuanced human aspects \nof doctor-patient interactions that are better captured \nthrough qualitative methods. Future studies might incor -\nporate qualitative research techniques, such as in-depth \ninterviews or focus groups, to gather more comprehen -\nsive insights into how patients and healthcare providers \nperceive and value the AI-generated interpretive reports. \nFourth, one limitation of this study is the exclusion of \nhallucinations, a commonly reported error in LLM/\nGPT models, from the evaluation. Hallucinations refer \nto instances where the model generates information that \nis factually incorrect or fabricated, which could poten -\ntially affect the interpretation of AI-generated pathology \nreports. However, in this study, our primary focus was \nPage 15 of 16\nYang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \n \non evaluating the accuracy, consistency, and readability \nof the pathology reports, specifically in relation to diag -\nnostic content. As such, hallucinations were not included \nin the scope of this assessment. Future research should \naim to investigate the occurrence of hallucinations in \nmedical text generation and their potential implications \nfor clinical practice, especially when using AI models in \nhigh-stakes decision-making environments. Fifth, we \nacknowledge the small number of volunteers and the \npotential impact on baseline characteristics. Different \ngroups were chosen to avoid bias introduced by famili -\narity with the report format. However, controlling for \nbaseline characteristics is crucial. The health literacy lev -\nels of the volunteers were assessed and considered in the \nanalysis. Therefore, these limitations underscore the need \nfor cautious interpretation of our study results and high -\nlight the importance of addressing these areas in future \nresearch. By expanding the scope, diversity, and depth \nof research into the use of AI in healthcare, we can bet -\nter understand the capabilities and limitations of these \ntechnologies and work towards maximizing their benefits \nwhile minimizing potential drawbacks.\nConclusion\nIn conclusion, our study demonstrates the potential ben -\nefits of using large language models (LLMs) like GPT-4 \nin the healthcare setting, particularly in processing and \ninterpreting pathology reports. While the findings high -\nlight the efficiency and accuracy of GPT-4 in generat -\ning interpretive pathology reports, we do not claim that \npatient outcomes or patient satisfaction were directly \nimproved based on this study alone. Instead, this research \nillustrates the promise of AI tools in enhancing health -\ncare communication and streamlining clinical workflows, \noffering insights into the evolving role of AI in healthcare \ndelivery. Future studies will be required to further inves -\ntigate the impact of LLMs on patient satisfaction and \nclinical outcomes in diverse and real-world settings.\nAcknowledgements\nWe acknowledge parts of this article were generated with GPT-4 (powered \nby OpenAI’s language model; https://chat.openai.com/), but the output \nwas confirmed by the authors. Thanks to the colleagues in the department \nof pathology for their help in this paper, your excellent work has made our \nresearch more efficient.\nAuthors’ contributions\nXiongwen Yang and Yi Xiao wrote the main manuscript text. Di Liu and Huiyou \nShi validated and conducted formal analysis. Huiyin Deng, Jian Huang, and \nYun Zhang curated the data. Dan Liu, Maoli Liang, Jing Yao, XiaoJiang Zhou, \nWankai Guo, and Yang He contributed to conceptualization and project \nadministration. Xing Jin, Yongpan Sun, WeiJuan Tang, and Chuan Xu provided \nmethodology and conducted review and editing. Chuan Xu also supervised \nthe project, handled visualization, and secured funding.\nFunding\nSupported by Talent Fund of Guizhou Provincial People’s Hospital.\nData availability\nThe raw data supporting the conclusions of this article will be made available \nby the authors, without undue reservation.\nDeclarations\nEthics approval and consent to participate\nAll procedures involving collection of tissue were in accordance with the \nethical standards of the institutional and/or national research committee and \nwith the 1964 Helsinki Declaration and its later amendments or comparable \nethical standards. This retrospective compliance study was approved by the \nEthics Review Committee of Guizhou Provincial People’s Hospital (Ethics \nNumber: 2024004), the Third Affiliated Hospital of Sun Yat-sen University (Eth-\nics Number: B2023074), the Third Xiangya Hospital, Central South University \n(Ethics Number: 2024011), and Jiangxi Cancer Hospital (Ethics Number: \nJC2024006). Written informed consent was obtained from individual or guard-\nian participants.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1 Department of Thoracic Surgery, Guizhou Provincial People’s Hospital, No. 83, \nZhongshan East Road, Guiyang, Guizhou 550000, China. 2 NHC Key Laboratory \nof Pulmonary Immunological Diseases, Guizhou Provincial People’s Hospital, \nGuiyang, Guizhou 550000, China. 3 Department of Cardio-Thoracic Surgery, \nthe Third Affiliated Hospital of Sun Yat-sen University, Guangzhou, Guang-\ndong, China. 4 Department of Pathology, Guizhou Provincial People’s Hospital, \nGuiyang, Guizhou, China. 5 Department of Anesthesiology, the Third Xiangya \nHospital of Central South University, Changsha, Hunan, China. 6 Depart-\nment of Thoracic Surgery, Jiangxi Cancer Hospital, Nanchang, Jiangxi, China. \n7 Department of Radiology, Guizhou Provincial People’s Hospital, Guiyang, \nGuizhou, China. 8 Department of Medical Records and Statistics, Guizhou Pro-\nvincial People’s Hospital, Guiyang, Guizhou, China. 9 Department of Respiratory \nMedicine, Guizhou Provincial People’s Hospital, Guiyang, Guizhou, China. \nReceived: 10 June 2024   Accepted: 23 December 2024\nReferences\n 1. Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien C, Compas C, \nMartin C, Costa AB, Flores MG, et al. A large language model for electronic \nhealth records. NPJ Digital Med. 2022;5(1):194.\n 2. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. \nLarge language models in medicine. Nat Med. 2023;29(8):1930–40.\n 3. Yang X, Chu XP , Huang S, Xiao Y, Li D, Su X, Qi YF, Qiu ZB, Wang Y, Tang \nWF, et al. A novel image deep learning-based sub-centimeter pulmonary \nnodule management algorithm to expedite resection of the malignant \nand avoid over-diagnosis of the benign. Eur Radiol. 2024;34(3):2048–61.\n 4. Mossanen M, True LD, Wright JL, Vakar-Lopez F, Lavallee D, Gore JL. Surgi-\ncal pathology and the patient: a systematic review evaluating the pri-\nmary audience of pathology reports. Hum Pathol. 2014;45(11):2192–201.\n 5. Dunsch F, Evans DK, Macis M, Wang Q. Bias in patient satisfaction \nsurveys: a threat to measuring healthcare quality. BMJ Glob Health. \n2018;3(2):e000694.\n 6. Farley H, Enguidanos ER, Coletti CM, Honigman L, Mazzeo A, Pinson TB, \nReed K, Wiler JL. Patient Satisfaction Surveys and Quality of Care: An \nInformation Paper. Ann Emerg Med. 2014;64(4):351–7.\n 7. Shah NH, Entwistle D, Pfeffer MA. Creation and Adoption of Large Lan-\nguage Models in Medicine. JAMA. 2023;330(9):866–9.\n 8. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, Scales N, Tanwani \nA, Cole-Lewis H, Pfohl S, et al. Large language models encode clinical \nknowledge. Nature. 2023;620(7972):172–80.\n 9. Steimetz E, Minkowitz J, Gabutan EC, Ngichabe J, Attia H, Her-\nshkop M, Ozay F, Hanna MG, Gupta R. Use of Artificial Intelligence \nPage 16 of 16Yang et al. BMC Medical Informatics and Decision Making           (2025) 25:36 \nChatbots in Interpretation of Pathology Reports. JAMA Netw Open. \n2024;7(5):e2412767.\n 10. Winograd A. Loose-lipped large language models spill your secrets: The \nprivacy implications of large language models. Harvard J Law Technol. \n2023;36(2):615.\n 11. Harrer S. Attention is not all you need: the complicated case of ethically \nusing large language models in healthcare and medicine. EBioMedicine. \n2023;90: 104512.\n 12. Birkhäuer J, Gaab J, Kossowsky J, Hasler S, Krummenacher P , Werner C, \nGerger H. Trust in the health care professional and health outcome: A \nmeta-analysis. PLoS ONE. 2017;12(2):e0170988.\n 13. Haskard Zolnierek KB, DiMatteo MR. Physician Communication \nand Patient Adherence to Treatment: A Meta-Analysis. Med Care. \n2009;47(8):826.\n 14. Ogrinc G, Davies L, Goodman D, Batalden P , Davidoff F, Stevens D. SQUIRE \n2.0 (<em>Standards for QUality Improvement Reporting Excellence)</\nem>: revised publication guidelines from a detailed consensus process. \nBMJ Qual Safety. 2016;25(12):986–92.\n 15. Osborne RH, Batterham RW, Elsworth GR, Hawkins M, Buchbinder R. The \ngrounded psychometric development and initial validation of the Health \nLiteracy Questionnaire (HLQ). BMC Public Health. 2013;13(1):658.\n 16. Dewalt DA, Berkman ND, Sheridan S, Lohr KN, Pignone MP . Literacy and \nhealth outcomes: a systematic review of the literature. J Gen Intern Med. \n2004;19(12):1228–39.\n 17. Paasche-Orlow MK, Wolf MS. The causal pathways linking health literacy \nto health outcomes. Am J Health Behav. 2007;31(Suppl 1):S19-26.\n 18. Berkman ND, Sheridan SL, Donahue KE, Halpern DJ, Crotty K. Low health \nliteracy and health outcomes: an updated systematic review. Ann Intern \nMed. 2011;155(2):97–107.\n 19. Kravitz RL, Hays RD, Sherbourne CD, DiMatteo MR, Rogers WH, Ordway \nL, Greenfield S. Recall of recommendations and adherence to advice \namong patients with chronic medical conditions. Arch Intern Med. \n1993;153(16):1869–78.\n 20. McDonald HP , Garg AX, Haynes RB. Interventions to enhance patient \nadherence to medication prescriptions: scientific review. JAMA. \n2002;288(22):2868–79.\n 21. Schillinger D, Piette J, Grumbach K, Wang F, Wilson C, Daher C, Leong-\nGrotz K, Castro C, Bindman AB. Closing the loop: physician communica-\ntion with diabetic patients who have low health literacy. Arch Intern Med. \n2003;163(1):83–90.\n 22. Hibbard JH, Greene J. What the evidence shows about patient activa-\ntion: better health outcomes and care experiences; fewer data on costs. \nHealth Aff (Millwood). 2013;32(2):207–14.\n 23. Amin KS, Davis MA, Doshi R, Haims AH, Khosla P , Forman HP . Accuracy \nof ChatGPT, Google Bard, and Microsoft Bing for Simplifying Radiology \nReports. Radiology. 2023;309(2):e232561.\n 24. Truhn D, Loeffler CM, Müller-Franzes G, Nebelung S, Hewitt KJ, Brandner \nS, Bressem KK, Foersch S, Kather JN. Extracting structured information \nfrom unstructured histopathology reports using generative pre-trained \ntransformer 4 (GPT-4). J Pathol. 2024;262(3):310–9.\n 25. Oben P . Understanding the Patient Experience: A Conceptual Framework. \nJ Patient Exp. 2020;7(6):906–10.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.895481288433075
    },
    {
      "name": "Medicine",
      "score": 0.6193739175796509
    },
    {
      "name": "Comprehension",
      "score": 0.5994038581848145
    },
    {
      "name": "Terminology",
      "score": 0.5511564016342163
    },
    {
      "name": "Health informatics",
      "score": 0.5104533433914185
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.47797366976737976
    },
    {
      "name": "Medical diagnosis",
      "score": 0.4494917392730713
    },
    {
      "name": "Pathology",
      "score": 0.4030722975730896
    },
    {
      "name": "Family medicine",
      "score": 0.3599042296409607
    },
    {
      "name": "Medical physics",
      "score": 0.3214392066001892
    },
    {
      "name": "Public health",
      "score": 0.1677248477935791
    },
    {
      "name": "Computer science",
      "score": 0.1593080461025238
    },
    {
      "name": "Artificial intelligence",
      "score": 0.1307612955570221
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210154454",
      "name": "Guizhou Provincial People's Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210146956",
      "name": "Third Affiliated Hospital of Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I139660479",
      "name": "Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210156904",
      "name": "Third Xiangya Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210097309",
      "name": "Jiangxi Provincial Cancer Hospital",
      "country": "CN"
    }
  ],
  "cited_by": 13
}