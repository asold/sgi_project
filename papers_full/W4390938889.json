{
  "title": "Deep Transformer-Based Network Deforestation Detection in the Brazilian Amazon Using Sentinel-2 Imagery",
  "url": "https://openalex.org/W4390938889",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5092581720",
      "name": "Mariam Alshehri",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2916669377",
      "name": "Anes Ouadou",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2329553248",
      "name": "Grant J. Scott",
      "affiliations": [
        "University of Missouri"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3011156941",
    "https://openalex.org/W4210438736",
    "https://openalex.org/W2304182594",
    "https://openalex.org/W3011869042",
    "https://openalex.org/W4293059536",
    "https://openalex.org/W2627081599",
    "https://openalex.org/W2767512561",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4312549298",
    "https://openalex.org/W4248710273",
    "https://openalex.org/W4200302434"
  ],
  "abstract": "Deforestation poses a critical environmental challenge with far-reaching impacts on climate change, biodiversity, and local communities.As such, detecting and monitoring deforestation are crucial, and recent advancements in deep learning and remote sensing technologies offer a promising solution to this challenge.In this study, we adapt ChangeFormer, a transformer-based framework, to detect deforestation in the Brazilian Amazon, employing the attention mechanism to analyze spatial and temporal patterns in bi-temporal satellite images.To assess the model's effectiveness, we employed a robust approach to create a deforestation detection dataset, utilizing Sentinel-2 imagery from select conservation areas in the Brazilian Amazon throughout 2020 and 2021.Our dataset comprises 7,734 pairs of bi-temporal image chips with a resolution of 256×256 pixels and 1,406 pairs of image chips with a resolution of 512×512 pixels.The model achieved an overall accuracy of 93% with corresponding F1 score of 90% and IoU score of 82%.These results demonstrate the potential of transformer-based networks for accurate and efficient deforestation detection.",
  "full_text": "1\nDeep Transformer-based Network Deforestation\nDetection in the Brazilian Amazon Using Sentinel-2\nImagery\nMariam Alshehri, Anes Ouadou, Grant J. Scott, Senior Member, IEEE\nAbstract—Deforestation poses a critical environmental chal-\nlenge with far-reaching impacts on climate change, biodiver-\nsity, and local communities. As such, detecting and monitoring\ndeforestation are crucial, and recent advancements in deep\nlearning and remote sensing technologies offer a promising\nsolution to this challenge. In this study, we adapt ChangeFormer,\na transformer-based framework, to detect deforestation in the\nBrazilian Amazon, employing the attention mechanism to analyze\nspatial and temporal patterns in bi-temporal satellite images. To\nassess the model’s effectiveness, we employed a robust approach\nto create a deforestation detection dataset, utilizing Sentinel-2\nimagery from select conservation areas in the Brazilian Amazon\nthroughout 2020 and 2021. Our dataset comprises 7,734 pairs\nof bi-temporal image chips with a resolution of 256 ×256 pixels\nand 1,406 pairs of image chips with a resolution of 512 ×512\npixels. The model achieved an overall accuracy of 93% with\ncorresponding F1 score of 90% and IoU score of 82%. These\nresults demonstrate the potential of transformer-based networks\nfor accurate and efficient deforestation detection.\nIndex Terms—Change detection, deep learning, deforestation,\ntransformer\nI. I NTRODUCTION\nDeforestation significantly impacts environmental sustain-\nability, causing biodiversity loss, ecological imbalances, and\namplified climate change effects. The Brazilian Amazon, the\nworld’s largest rainforest, is indispensable for climate stability\nand carbon management. Unfortunately, rapid deforestation\nleads to multiple complications, including heightened green-\nhouse gas emissions, curtailed carbon retention, and increased\nforest fires [1]. Therefore, it is essential to implement effective\npolicies that are grounded in reliable, up-to-date data, rec-\nognizing deforestation detection (DD) as the cornerstone for\nobtaining such valuable information.\nAlthough DD is recognized as an essential task in restoring\nthe biodiversity of the Brazilian Amazon, it is fraught with\nvarious challenges. One of the major obstacles is the vast size\nof the Brazilian Amazon, covering approximately 5.2 million\nkm2 of land, which is about 60% of the country’s total area [2].\nThis makes conventional methods, such as map interpretation,\nfield surveys, and ancillary data analysis, impractical due to\ntheir time-consuming and labor-intensive nature.\nRemote sensing imagery (RSI) has become a more ad-\nvanced alternative, owing to its wide geographic coverage,\ncost-effectiveness, and capacity to produce consistent and\nreproducible data, vital for tracking temporal changes. The\npopularity RSI has increased further with the introduction of\nFig. 1. The Amazon Biome is outlined by the yellow boundary, and the\nconservation units are highlighted in red.\nopen-access policies for earth observation satellites and ad-\nvances in analytical technologies [3], leading to the creation of\nmultiple RSI systems by both public and private organizations.\nModerate Resolution Imaging Spectroradiometer (MODIS),\nLandsat 8, and Sentinel-2 are some of the most commonly\nused satellites in remote sensing studies. MODIS offers 36\nspectral bands with a maximum spatial resolution of 250 m\nand a revisit time of two days. Landsat 8 has 11 spectral\nbands with 15-meter panchromatic and 30-meter multi-spectral\nspatial resolutions and a revisit time of 16 days. Sentinel-\n2, launched by the European Space Agency (ESA), has 13\nspectral bands with a spatial resolution range of 10 m to 60 m\nand a revisit time of 5 days. Sentinel-2’s advantage lies in its\nhigher spatial and temporal resolution compared to the other\nopen-access satellites, making it a better option for mapping\nthe expansive Brazilian Amazon.\nIn RSI change detection (CD) applications, a quantitative\nanalysis is performed to identify surface changes by comparing\nimages of the same location captured at different timestamps.\nThe aim is to accurately detect pixel changes in bi-temporal\nor multi-temporal images by assigning a binary label to\neach pixel. A null label represents an unchanged area, while\na positive label indicates the presence of change. Various\nCD methods have been proposed in the literature including\nalgebra-based techniques (such as image ratioing and image\ndifference) and machine learning classifiers (such as support\nThis article has been accepted for publication in IEEE Geoscience and Remote Sensing Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LGRS.2024.3355104\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nvector machines, decision trees, and fuzzy theory) [4], [5].\nHowever, these methods fail to produce accurate results when\nhigh-resolution images are used due to the high contrast and\nfrequency components of the images.\nTo address the limitations of the existing methods, deep\nlearning (DL) methods have emerged as dominant techniques\nfor image analysis. Literature indicates that DL methods\ndemonstrate superior performance compared to traditional\nmachine learning techniques in CD applications [6], [7].\nConvolutional neural networks (CNNs) are a widely-used\nclassification method in CD applications, due to their ability\nto generate powerful discriminative features. For example, [1]\nutilized CNNs to predict annual changes in vegetation cover\nwithin the Brazilian Amazon; however, this technique en-\ntails redundant operations, leading to increased computational\ncosts. The performance of CNNs for change detection (CD)\nhas been enhanced by integrating dilated convolutions, stacked\nconvolution layers, and attention mechanisms into the models.\nThe transformer model, a DL model incorporating attention\nmechanisms, is well-suited for accommodating multi-temporal\nimages, as it enables easy scaling, captures long-range se-\nquence features, and supports efficient parallel processing.\nConsidering these benefits, the transformer model has been\napplied in various areas of computer vision such as the vision\ntransformer (ViT) [8], Bi-temporal image transformer (BIT)\n[9], and Shifted Windows (SWin) Transformer [10]. One of the\nkey advantages of these networks over CNNs is that they offer\nsuperior context-modeling ability between pixel pairs because\nthey have a larger effective receptive field [11]. Regardless\nof the potential of transformer models, there is limited re-\nsearch regarding their application in DD. This study aims to\ninvestigate the application of the transformer-based network,\nChangeFormer [12], originally designed for change detection\nin urban areas, in the context of deforestation, with the\nobjective of demonstrating its effectiveness in DD. The model\ncombines a hierarchical transformer encoder in a Siamese\narchitecture, four difference modules for computing feature\ndifferences, and a simple MLP decoder. The anticipation is\nthat this transformer-based network will provide superior DD\naccuracy compared to that of CNNs.\nII. M ETHODS\nA. Data Sources\n1) Satellite imagery source: Sentinel-2 satellite images\nwere used due to their superior spatial and temporal resolution\ncompared to other open-access satellites. Images were down-\nloaded from The Copernicus Open Access Hub * operated by\nthe European Space Agency (ESA). We used Level-2A Surface\nReflectance product with a maximum cloud cover percentage\nof 20%. The images were downloaded in tiles, each of\nsize 10980×10980 pixels, covering an area of approximately\n100×100 km².\n2) Ground truth source: To generate ground truth poly-\ngons for our study, we utilized the PRODES project datasets\ndeveloped by the Brazilian National Space Agency (INPE).\n*Copernicus portal: https://scihub.copernicus.eu/dhus/\nSince 1988, the PRODES project has been monitoring and\nquantifying annual deforestation rates in the Brazilian Amazon\nrainforest by visually interpreting medium-resolution satel-\nlite imagery with a team of experienced professionals. The\nPRODES data is publicly accessible on the TerraBrasilis\nwebsite†. We selected two datasets: the Yearly Deforestation\ndataset, which contains the locations where deforestation oc-\ncurred from one year to the next, and the Conservation Units\ndataset, which we used to identify the areas from where we\ndownloaded the images. Figure 1 shows the boundaries of\nAmazon Biome and the conservation units.\nB. Location and Date Selection\nThe top five conservation units with the highest deforested\nland areas were selected. Table I details their names, respective\nsizes in km 2, and the corresponding deforestation areas and\npercentages. We targeted the years 2020 and 2021, as they pro-\nvide the most recent and complete yearly deforestation data. To\ndetermine the appropriate date range for each Sentinel-2 tile,\nwe identified the polygons within the tile for both years 2020\nand 2021 from the PRODES dataset. For each year separately,\nwe extracted the earliest date ( d1) and the latest date ( d2)\nfrom the acquisition dates of the images used to label these\npolygons. Consequently, two intervals were defined for each\ntile: one for 2020 and another for 2021, each set to [ d1 − 30\ndays, d2 + 30days]. These extended intervals aim to capture\na variety of images for quality selection while ensuring that\nthe imagery used in our study aligns closely with the ground\ntruth labeling dates, minimizing the likelihood of significant\ndeforestation developments occurring in the interim period\nbetween image acquisition and labeling.\nC. Band Combinations\nSentinel-2 images are composed of 13 bands at varying\nresolutions of 10 m, 20 m, and 60 m. We used the visible\nand near-infrared bands at 10 m resolution, along with the\nScene Classification Layer (SCL) at 20 m resolution. The near-\ninfrared is effective in distinguishing vegetation from other\nfeatures, as healthy vegetation reflects strongly in the near-\ninfrared part of the spectrum. The SCL band was resampled\nfrom 20 m to 10 m to match the resolution of the other\nselected bands and used as a mask to remove areas classified as\nclouds or cloud shadows, which were labeled as “no change”.\nWe used the following band combinations that emphasize the\nspectral signature of vegetation:\n• Color-shifted Infrared:\nNGB = [Band8, Band3, Band2 ]\n• Normalized Difference Vegetation Index:\nNDV I= Band8 − Band4\nBand8 +Band4\n• Enhanced Vegetation Index:\nEV I= 2.5 ∗ Band8 − Band4\nBand8 + 6Band4 –7.5 Band2 + 1\n†Terrabrasilis Download: http://terrabrasilis.dpi.inpe.br/en/download-2/\nThis article has been accepted for publication in IEEE Geoscience and Remote Sensing Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LGRS.2024.3355104\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nTABLE I\nTHE TOP FIVE CONSERVATION UNITS RANKED BY DEFORESTATION SIZE\nArea ID and name Area(km2) Deforestation(km2) Deforestation(%)\n59- ´Area de Protec ¸˜ao ambiental Triunfo do Xingu 16,792 528.33 3.14\n89-Floresta Nacional do Jamanxim 13,017 214.59 1.63\n55-Reserva Extrativista Jaci-Paran ´a 1,974 108.92 5.51\n291- ´Area de Protec ¸˜ao Ambiental do Tapaj ´os 20,395 114.70 0.56\n165-Reserva Extrativista Chico Mendes 9,315 91.31 0.98\nFig. 2. Temporal and spectral comparison of satellite images from 2020 (top)\nand 2021 (bottom) highlighting deforestation changes. The first column shows\nNGB images with forest appearing in red, while non-forest areas appear in\nblue-green tones. The second and third columns show the same area but with\nNDVI and EVI indices respectively.\nFigure 2 illustrates the spectral variations resulting from dif-\nferent band combinations applied to a pair of images captured\nin 2020 and 2021.\nD. Image Processing\nTo ensure that our dataset is representative of the problem\ndomain and free from potential biases, we employed careful\npreprocessing procedures. Our first step involved applying\nlinear normalization to facilitate image comparison on a\nconsistent scale. The minimum and maximum values were\nselected as 1 st and 99 th percentile of the value histogram\nto reduce the impact of outliers. This step ensures that the\nresulting pixel values span the full range of possible values\nfor that band, which can help to enhance the contrast and\nquality of the image. We also applied filtering at both the raster\nand chip levels. High-quality rasters were manually selected\nfrom the returned results of the download query. At the chip\nlevel, we eliminated single-class chips and those where the\n“change” class accounted for less than 10% of the chip’s total\narea to mitigate extreme class imbalance between change and\nno-change classes. Since deforestation changes in one year are\noften dispersed across large no-change areas, the 10% filter\nresulted in a significant reduction of chips. To compensate for\nthis loss, we implemented rotation augmentation at 90 and\n180 degrees to increase the dataset size and enhance model\nrobustness and generalization, and further augmentation as\ndescribed in [13] could be implemented as needed.\nAt the end of the chip creation process, a total of 7,734\npairs of chips of size 256 × 256 and 1,406 pairs of chips of\nsize 512 × 512 were generated. The dataset then was split\ninto three subsets: 60% for training, 20% for testing, and\nFig. 3. Dataset creation process.\n20% for validation. To ensure the integrity of the dataset, we\ntook measures to ensure that each chip and its rotations were\nkept within the same subset and the distribution of the classes\nis maintained in each subset. The dataset creation process is\nsummarized in Figure 3, highlighting the key steps.\nIII. E XPERIMENTS\nIn the employed ChangeFormer architecture, a hierar-\nchical transformer encoder processes bi-temporal images,\ngenerating ConvNet-like multi-level features through self-\nattention modules and downsampling blocks. The key com-\nponent of this architecture comprises four difference mod-\nules. These modules calculate feature differences between\npre-change and post-change images at multiple levels, em-\nploying the following sequence of operations: Fi\ndiff =\nBN(ReLU(Conv2D3×3(Cat(Fi\npre, Fi\npost))), where Fi\ndiff repre-\nsents the feature difference, BN stands for batch normalization,\nReLU is the rectified linear unit function, Conv2D 3×3 denotes\na 2D convolution with a 3 × 3 kernel, and Cat indicates the\nconcatenation of Fi\npre and Fi\npost which are the features of pre-\nchange and post-change images respectively. These feature\ndifferences are then aggregated by a simple MLP decoder\nto predict the change map. The decoder encompasses MLP\nlayers and upsampling steps to fuse feature difference maps,\nproducing the final change mask prediction. Refer to Figure 4\nfor a simplified diagram and the original research paper for\ncomprehensive details [12].\nWe randomly initialized the model and optimized the per-\nformance by tuning six hyperparameters listed in Table II. We\ntrained multiple model configurations from the hyperparameter\nThis article has been accepted for publication in IEEE Geoscience and Remote Sensing Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LGRS.2024.3355104\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nFig. 4. A simplified overview of the ChangeFormer architecture, showing its three main components: a siamese hierarchical transformer encoder, four\ndifference modules, and a lightweight MLP decoder.\nTABLE II\nHYPERPARAMETERS INVOLVED IN TUNING PROCESS\nHyperparameter Variations\nImage size 256, 512\nBand composite NGB, EVI, NDVI\nOptimizer SGD, Adam, AdamW\nlearning rates 0.01 - 0.0001\nLoss functions Cross-Entropy (CE), Intersection over Union (IoU)\nBatch size 4, 8, 12, 16\nspace for 200 epochs each and evaluated their performance\nusing both overall and change-class-specific metrics. Overall\nmetrics, including mean F1 score, mean Intersection over\nUnion (IoU) score, and overall accuracy (OA), were reported\nalongside change-class-specific metrics, which included F1,\nIoU, precision, and recall. We utilized Nautilus, a high-\nperformance computing system, to perform our data process-\ning pipeline and model training with multiple configurations.\nIV. R ESULTS\nAlthough overall accuracy (OA) is a widely used metric, it\nmay not be the best indicator for our deforestation dataset due\nto the inherent class imbalance. Therefore, to provide a more\ncomprehensive evaluation, we considered mIoU as an overall\nmetric for comparison. Additionally, we reported change-class-\nspecific metrics, including F1, IoU, precision (pre-1), and\nrecall (rec-1) scores, to further assess the model’s ability\nto detect deforestation changes. This focus on change-class\nmetrics obtains accurate representation of the model’s ability\nto detect deforestation changes, rather than results that might\nbe skewed by the detection of the majority unchanged class.\nThe 256 × 256 chips exhibited better performance in de-\nforestation detection, effectively capturing smaller deforested\nFig. 5. Four image samples with corresponding predicted and ground truth\ndeforestation maps. The top two rows show images from 2020 and 2021,\nrespectively. The third and fourth rows display the predicted and ground truth\nmaps, respectively.\npatches and their surrounding context, thereby generating a\nmore diverse pool of training samples from the same geo-\ngraphic area and enhancing model generalization. Conversely,\nthe broader spatial context of the 512 × 512 chips often\nincludes a larger proportion of the ’no change’ class. While\nthese chips can capture more deforested patches, the extensive\narea they cover often results in these patches constituting less\nthan 10% of the chip area, triggering their elimination based\non the data filter steps. This disproportionately reduces the\nsample size for the 512 × 512 chips, indirectly elevating the\nrisk of overfitting due to narrowed variability in the data.\nThis article has been accepted for publication in IEEE Geoscience and Remote Sensing Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LGRS.2024.3355104\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nTABLE III\nTOP FIVE SCORING HYPERPARAMETER CONFIGURATIONS FOR 256 × 256 (TOP ) AND 512 × 512 (BOTTOM ) CHIP SIZES\nchip size optimizer loss lr OA mIoU mF1 IoU-1 F1-1 pre-1 rec-1\n256\nAdamW CE 0.0001 0.9306 0.8238 0.9007 0.7333 0.8462 0.8470 0.8453\nAdam CE 0.0001 0.9286 0.8196 0.8980 0.7274 0.8422 0.8407 0.8437\nAdamW CE 0.001 0.9243 0.8077 0.8902 0.7080 0.8290 0.8459 0.8128\nAdam mIoU 0.0001 0.9227 0.8072 0.8901 0.7098 0.8303 0.8229 0.8378\nAdamW mIoU 0.0001 0.9221 0.8067 0.8898 0.7095 0.8301 0.8174 0.8432\n512\nAdamW CE 0.0001 0.9336 0.8008 0.8842 0.6788 0.8087 0.8262 0.7918\nAdam CE 0.0001 0.9322 0.7980 0.8824 0.6750 0.8059 0.8179 0.7944\nAdamW mIoU 0.0001 0.9306 0.7956 0.8808 0.6721 0.8039 0.8051 0.8026\nAdam mIoU 0.0001 0.9273 0.7904 0.8775 0.6657 0.7993 0.7826 0.8168\nAdamW CE 0.001 0.9223 0.7759 0.8674 0.6421 0.7821 0.7771 0.7871\nWe found that the combination of a learning rate of 0.0001,\nCE loss, AdamW optimizer, and the NGB band produced the\nbest results within our experimentation. Table III presents the\nresults categorized by chip size ( 256 × 256 and 512 × 512),\ndisplaying the top five scores for each image size.\nA previous study [14] applied deep learning for deforesta-\ntion detection by comparing six state-of-the-art fully convolu-\ntional network architectures, namely U-Net, ResU-Net, Seg-\nNet, FC-DenseNet, and Xception and MobileNetV2 variants\nof Deeplabv3+. Similar to our work, they used the PRODES\ndataset for ground truth, and they utilized both Sentinel 2\nand Landsat-8 satellite imagery. The best result of this prior\nstudy for Sentinel-2 data was FC-DenseNet with an F1-\nscore of 70.7%. However, the ChangeFormer model obtained\nat least 80.9% F1-score for the change class (F1-1) across\nboth chip sizes. Similar trends were observed in recall (rec-\n1) and precision (pre-1), with values of 84.5% and 84.7%\nrespectively, outperforming the 75.1% and 78.0% reported in\nthe previous study.\nV. C ONCLUSION AND FUTURE WORK\nWe obtained Sentinel-2 satellite imagery and ground truth\ndata for deforested areas in the Brazilian Amazon rainforest\nin 2020 and 2021. Using this data, we created a bi-temporal\ndeforestation dataset and trained a transformer-based network\nfor deforestation detection. We conducted a thorough hyper-\nparameter search, exploring various configurations to identify\nthe best settings for our task. Our investigation showed that\ncolor-shifted infrared composite and cross-entropy loss with\nAdamW optimizer resulted in the highest mean IoU (0.9007),\nmean F1-score (0.8238) with precision and recall of the change\nclass of 84.70% and 84.53%, respectively, demonstrating a\nwell-balanced detection performance. In comparison to ex-\nisting research in the field of deforestation detection, our\nfindings suggest that transformer-based networks are capable\nof achieving significantly improved levels of accuracy.\nOur future work involves experimenting with state-of-the-\nart methods on a larger dataset that we plan to make pub-\nlicly available along with the data acquisition and processing\npipeline code. This could be a valuable contribution to the\nfield of deforestation detection, enhancing the accuracy and\nrobustness of deep learning models in this domain.\nACKNOWLEDGEMENTS\nMariam Alshehri would like to acknowledge the support\nprovided by her primary affiliation, Princess Nourah Bint\nAbdulrahman University, Riyadh, Saudi Arabia.\nREFERENCES\n[1] P. P. de Bem, O. A. de Carvalho Junior, R. Fontes Guimar ˜aes, and R. A.\nTrancoso Gomes, “Change Detection of Deforestation in the Brazilian\nAmazon Using Landsat Data and Convolutional Neural Networks,”\nRemote Sensing, vol. 12, no. 6, p. 901, Jan. 2020.\n[2] C. A. Silva, G. Guerrisi, F. Del Frate, and E. E. Sano, “Near-real time\ndeforestation detection in the Brazilian Amazon with Sentinel-1 and\nneural networks,” European Journal of Remote Sensing, vol. 55, no. 1,\npp. 129–149, Dec. 2022.\n[3] M. Lu, E. Pebesma, A. Sanchez, and J. Verbesselt, “Spatio-temporal\nchange detection from multidimensional arrays: Detecting deforestation\nfrom MODIS time series,” ISPRS Journal of Photogrammetry and\nRemote Sensing, vol. 117, pp. 227–236, Jul. 2016.\n[4] M. Ortega Adarme, R. Queiroz Feitosa, P. Nigri Happ, C. Aparecido\nDe Almeida, and A. Rodrigues Gomes, “Evaluation of Deep Learning\nTechniques for Deforestation Detection in the Brazilian Amazon and\nCerrado Biomes From Remote Sensing Imagery,” Remote Sensing,\nvol. 12, no. 6, p. 910, Jan. 2020.\n[5] F. Pan, Z. Wu, X. Jia, Q. Liu, Y . Xu, and Z. Wei, “A Temporal-Reliable\nMethod for Change Detection in High-Resolution Bi-Temporal Remote\nSensing Images,” Remote Sensing, vol. 14, no. 13, p. 3100, Jan. 2022.\n[6] S. H. Khan, X. He, F. Porikli, and M. Bennamoun, “Forest Change\nDetection in Incomplete Satellite Images With Deep Neural Networks,”\nIEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 9,\npp. 5407–5423, Sep. 2017.\n[7] Y . Liu, X. Chen, Z. Wang, Z. J. Wang, R. K. Ward, and X. Wang,\n“Deep learning for pixel-level image fusion: Recent advances and future\nprospects,” Information Fusion, vol. 42, pp. 158–173, Jul. 2018.\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale,” Jun. 2021.\n[9] H. Chen, Z. Qi, and Z. Shi, “Remote Sensing Image Change Detection\nWith Transformers,” IEEE Transactions on Geoscience and Remote\nSensing, vol. 60, pp. 1–14, 2022.\n[10] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin Transformer: Hierarchical Vision Transformer Using Shifted\nWindows,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2021, pp. 10 012–10 022.\n[11] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu, Z. Yang, Y . Zhang, and D. Tao, “A Survey on Vision\nTransformer,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 45, no. 1, pp. 87–110, Jan. 2023.\n[12] W. G. C. Bandara and V . M. Patel, “A Transformer-Based Siamese Net-\nwork for Change Detection,” in IGARSS 2022 - 2022 IEEE International\nGeoscience and Remote Sensing Symposium, Jul. 2022, pp. 207–210.\n[13] G. J. Scott, M. R. England, W. A. Starms, R. A. Marcum, and C. H.\nDavis, “Training deep convolutional neural networks for land–cover\nclassification of high-resolution imagery,” IEEE Geoscience and Remote\nSensing Letters, vol. 14, no. 4, pp. 549–553, 2017.\n[14] D. L. Torres, J. N. Turnes, P. J. Soto Vega, R. Q. Feitosa, D. E. Silva,\nJ. Marcato Junior, and C. Almeida, “Deforestation Detection with Fully\nConvolutional Networks in the Amazon Forest from Landsat-8 and\nSentinel-2 Images,” Remote Sensing, vol. 13, no. 24, p. 5084, Jan. 2021.\nThis article has been accepted for publication in IEEE Geoscience and Remote Sensing Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LGRS.2024.3355104\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Amazon rainforest",
  "concepts": [
    {
      "name": "Amazon rainforest",
      "score": 0.8304694294929504
    },
    {
      "name": "Deforestation (computer science)",
      "score": 0.7999409437179565
    },
    {
      "name": "Pixel",
      "score": 0.6670555472373962
    },
    {
      "name": "Remote sensing",
      "score": 0.5777154564857483
    },
    {
      "name": "Computer science",
      "score": 0.553334653377533
    },
    {
      "name": "Image resolution",
      "score": 0.5087776780128479
    },
    {
      "name": "Satellite imagery",
      "score": 0.46064192056655884
    },
    {
      "name": "Biodiversity",
      "score": 0.45102638006210327
    },
    {
      "name": "High resolution",
      "score": 0.4227219820022583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3973100185394287
    },
    {
      "name": "Geography",
      "score": 0.279317170381546
    },
    {
      "name": "Ecology",
      "score": 0.08648815751075745
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76835614",
      "name": "University of Missouri",
      "country": "US"
    }
  ],
  "cited_by": 20
}