{
  "title": "A Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution",
  "url": "https://openalex.org/W2934020926",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2100265293",
      "name": "Jianmin Jiang",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2916477967",
      "name": "Hossam M. Kasem",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A5073580311",
      "name": "Kwok-Wai Hung",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2100265293",
      "name": "Jianmin Jiang",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2916477967",
      "name": "Hossam M. Kasem",
      "affiliations": [
        "Tanta University",
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A5073580311",
      "name": "Kwok-Wai Hung",
      "affiliations": [
        "Shenzhen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2963814095",
    "https://openalex.org/W2131024476",
    "https://openalex.org/W2184334976",
    "https://openalex.org/W2057065563",
    "https://openalex.org/W2117865218",
    "https://openalex.org/W2088254198",
    "https://openalex.org/W4136762",
    "https://openalex.org/W6602211262",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2111454493",
    "https://openalex.org/W2049237558",
    "https://openalex.org/W2121058967",
    "https://openalex.org/W6635400391",
    "https://openalex.org/W2243740427",
    "https://openalex.org/W2058523468",
    "https://openalex.org/W2156825886",
    "https://openalex.org/W1992408872",
    "https://openalex.org/W6683680428",
    "https://openalex.org/W2137290314",
    "https://openalex.org/W2067042811",
    "https://openalex.org/W236254921",
    "https://openalex.org/W2192954843",
    "https://openalex.org/W2108069432",
    "https://openalex.org/W1992303731",
    "https://openalex.org/W2119832264",
    "https://openalex.org/W6724673846",
    "https://openalex.org/W2149669120",
    "https://openalex.org/W2078008442",
    "https://openalex.org/W2344932324",
    "https://openalex.org/W2015718162",
    "https://openalex.org/W6675981210",
    "https://openalex.org/W2359099468",
    "https://openalex.org/W2167646546",
    "https://openalex.org/W6681108715",
    "https://openalex.org/W6770257792",
    "https://openalex.org/W2518141884",
    "https://openalex.org/W2509348655",
    "https://openalex.org/W2536635919",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W2962869147",
    "https://openalex.org/W1930824406",
    "https://openalex.org/W2165939075",
    "https://openalex.org/W2110158442",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6638194035",
    "https://openalex.org/W2047920195",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W2562066862",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2104618834",
    "https://openalex.org/W54257720",
    "https://openalex.org/W1590245106",
    "https://openalex.org/W3101659800",
    "https://openalex.org/W2503339013",
    "https://openalex.org/W1791560514",
    "https://openalex.org/W2163935418",
    "https://openalex.org/W3043538212",
    "https://openalex.org/W2142208211",
    "https://openalex.org/W2163605009"
  ],
  "abstract": "In general, existing research on single image super-resolution does not consider the practical application that, when image transmission is over noisy channels, the effect of any possible geometric transformations could incur significant quality loss and distortions. To address this problem, we present a new and robust super-resolution method in this paper, where a robust spatially-transformed deep learning framework is established to simultaneously perform both the geometric transformation and the single image super-resolution. The proposed seamlessly integrates deep residual learning based spatial transform module with a very deep super-resolution module to achieve a robust and improved single image super-resolution. In comparison with the existing state of the arts, our proposed robust single image super-resolution has a number of novel features, including 1) content-characterized deep features are extracted out of the input LR images to identify the incurred geometric transformations, and hence transformation parameters can be optimized to influence and control the super-resolution process; 2) the effects of any geometric transformations can be automatically corrected at the output without compromise on the quality of final super-resolved images; and 3) compared with the existing research reported in the literature, our proposed achieves the advantage that HR images can be recovered from those down-sampled LR images corrupted by a number of different geometric transformations. The extensive experiments, measured by both the peak-signal-to-noise-ratio and the similar structure index measurement, show that our proposed method achieves a high level of robustness against a number of geometric transformations, including scaling, translations, and rotations. Benchmarked by the existing state-of-the-arts SR methods, our proposed delivers superior performances on a wide range of datasets which are publicly available and widely adopted across relevant research communities.",
  "full_text": "Received March 18, 2019, accepted March 29, 2019, date of publication April 2, 2019, date of current version April 16, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2908996\nA Very Deep Spatial Transformer Towards Robust\nSingle Image Super-Resolution\nJIANMIN JIANG\n 1, HOSSAM M. KASEM1,2, AND KWOK-WAI HUNG\n1\n1College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China\n2Faculty of Engineering, Tanta University, Tanta 31527, Egypt\nCorresponding author: Kwok-Wai Hung (kwhung@szu.edu.cn)\nThis work was supported in part by the Natural Science Foundation China (NSFC) under Grant 61620106008 and Grant 61602312, and in\npart by the Shenzhen Commission for Scientiﬁc Research and Innovations under Grant JCYJ20160226191842793.\nABSTRACT In general, existing research on single image super-resolution does not consider the practical\napplication that, when image transmission is over noisy channels, the effect of any possible geometric\ntransformations could incur signiﬁcant quality loss and distortions. To address this problem, we present\na new and robust super-resolution method in this paper, where a robust spatially-transformed deep learning\nframework is established to simultaneously perform both the geometric transformation and the single\nimage super-resolution. The proposed seamlessly integrates deep residual learning based spatial transform\nmodule with a very deep super-resolution module to achieve a robust and improved single image super-\nresolution. In comparison with the existing state of the arts, our proposed robust single image super-resolution\nhas a number of novel features, including 1) content-characterized deep features are extracted out of the\ninput LR images to identify the incurred geometric transformations, and hence transformation parameters\ncan be optimized to inﬂuence and control the super-resolution process; 2) the effects of any geometric\ntransformations can be automatically corrected at the output without compromise on the quality of ﬁnal\nsuper-resolved images; and 3) compared with the existing research reported in the literature, our proposed\nachieves the advantage that HR images can be recovered from those down-sampled LR images corrupted\nby a number of different geometric transformations. The extensive experiments, measured by both the peak-\nsignal-to-noise-ratio and the similar structure index measurement, show that our proposed method achieves\na high level of robustness against a number of geometric transformations, including scaling, translations,\nand rotations. Benchmarked by the existing state-of-the-arts SR methods, our proposed delivers superior\nperformances on a wide range of datasets which are publicly available and widely adopted across relevant\nresearch communities.\nINDEX TERMSSingle image super-resolution, deep learning, spatial transform, geometric transformations.\nI. INTRODUCTION\nOver the past decades, the problem of image super-resolution\nhas been extensively studied and numerous image SR meth-\nods have been reported to deal with this non-trivial prob-\nlem [1]–[8]. In general, image SR reconstruction methods\ncan be divided into two categories: multi-frame image SR\n(MISR) methods [9]–[11] and single-image SR methods\n(SISR) [12]–[14]. The MISR methods can be further divided\ninto two categories: frequency domain methods and spatial\ndomain methods [10]. The target of the frequency domain\nmethods is to eliminate spectrum aliasing and reconstruct\nthe high frequency information, including fourier transform-\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Yuming Fang.\nbased methods [4], discrete cosine transform-based methods\n[15] and wavelet transform-based methods [16]. While the\nadvantage of these methods is the computational efﬁciency,\nthese methods have some limitations in incorporate the image\nprior knowledge [17]. The spatial domain methods such as\nnon-uniform interpolation method [18], iterative back pro-\njection (IBP) method [19] and projection onto convex sets\n(POCS) [20]. While these methods generally have a good\nreconstruction ability, they are computational expensive [9].\nFurthermore, it is not easy to obtain an sufﬁcient number of\nLR images, in order to estimate a HR image from multiple\nblurred and noisy images [21]. Therefore, single-image SR\nmethods are much more desirable in practical applications.\nTechnically, all the methods of single-image super-\nresolution (SISR) can be divided into: interpolation-based\n45618\n2169-3536 \n 2019 IEEE. Translations and content mining are permitted for academic research only.\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nVOLUME 7, 2019\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nmethods [22], reconstruction-based methods [23], [24] and\nexample learning-based methods [13], [25]. The concept\nof the interpolation-based method is to use the values\nof the neighbouring low-resolution pixels to estimate the\nvalue of the interpolated high-resolution pixel. The main\nadvantages of the interpolation-based method are simple\nand relatively low computational complexity [1]. However,\nthe interpolation-based methods often generate the recon-\nstructed HR images with edge halos and artifacts [26].\nThe reconstruction-based methods recover the HR image\nbased on the degradation model [24], [25]. As single image\nSR is highly ill-posed, image prior knowledge is often pro-\nposed to regularize the solution in this family of approaches.\nIn order to obtain an effective image prior, it is of great impor-\ntance to model the appropriate feature of natural images.\nBased on the max-posterior (MAP) theory which utilize\nsome prior constraints into the data ﬁdelity cost function,\nthe SR problem turns to be well-posed. Therefore, the SR\nproblem can be transformed into a minimization problem\n: ˆXHR = argmin [L (XHR,YLR)+λU (XHR)], where ˆXHR\nis the estimated HR image, L (.) is the data ﬁdelity term\nthat represents the degree of consistency between the tar-\nget HR image XHR and the LR image YLR, U (.) is the\nregularization term describing the prior information of the\noriginal image, and λis the regularization parameter, which\nis used to weigh the inﬂuence given by the prior L (.) and\nU (.)during the estimation. Many types of priors have been\nutilized into reconstruction- based methods, such as edge\nprior [27], gradient prior [28], and sparsity priors [29]–[33].\nThe learning-based methods estimate the HR image from\nthe LR image by learning the relationship between the HR\nand LR image patches from the sample database. Inspired\nby the great success achieved by deep learning [34], people\nbegin to use neural networks with deep architecture for image\nSR. Multiple layers are stacked together for robust learning\nof self-similar patches. Deep convolutional neural networks\n(CNN) [35] and deconvolutional networks [36] are designed\nto directly learn the non-linear mapping from LR space to\nHR space in a way similar to coupled sparse coding [37].\nAs these deep networks allow end-to-end training of all the\nmodel components between LR input and HR output, signif-\nicant improvements have been observed over their shadow\ncounterparts. Speciﬁcally, the feature spatial transform [38]\ntries to learn the texture transform from the segmentation\nmaps that addresses the conventional super-resolution prob-\nlem, which is fundamentally different from the proposed\nwork to tackle geometric transformation and super-resolution\nsimultaneously.\nOn the other hand, practical applications of single image\nsuper-resolution indicates that the real LR measurements\nusually suffer from various types of corruptions, such as\ngeometric transformations, noise, and blurring. In this paper,\nwe focus on dealing with the geometric transformation\neffects, for which the existing research has not properly\naddressed in the area of image super-resolution. Yet prac-\ntical applications reveal that estimating HR images from\ntransformed or distorted LR versions remains an important\nresearch topic demanded across a number of engineering sec-\ntors, such as self-driving vehicles, smart phones, and medical\nimage analysis etc. One typical example is the classiﬁcation\nof cancer types via LR images, in which various geometric\ntransformations, such as scaling and rotation etc., could incur\nduring the process of correlating different patterns across\nsimilar super-resolution images. Without the capability of\ndealing with these transformed LR images, the classiﬁca-\ntion accuracy could be degraded signiﬁcantly. Consequently,\nthere are strong motivations to research on a robust super-\nresolution technique that can generate an HR image from the\ndistorted and transformed LR image.\nTo alleviate the geometric transformation effects with res-\nolution improvements, we propose a spatially transformed\ndeep learning framework to achieve robust single image\nsuper-resolution in this paper. To our best knowledge, this\nis the ﬁrst attempt on deep learning based super-resolution\nthat simultaneously tackles the transformation effects and\nresolution enhancement, out of which our contributions can\nbe highlighted as follows:\n• Compared with all existing super-resolution deep learn-\ning networks, our proposed is a robust super-resolution\nnetwork that can simultaneously handle both geometric\ntransformations and resolution enhancement.\n• In terms of structures, our proposed can seamlessly inte-\ngrate a deep residual spatial transform network with a\nvery deep super-resolution network to form a novel deep\nlearning architecture.\n• By replacing the original thick CNN with a new resid-\nual thin CNN, our proposed signiﬁcantly improves the\nexisting spatial transform network (STN) [39] in terms\nof functionality, performances, and efﬁciency (with less\nnumber of parameters.)\n• In comparison with the existing state of the art SR\nmethods reported in the literature, experimental results\nsupport and verify that our proposed ST-DISR achieves\nsuperior performances in terms of both PSNR and SSIM.\nThe rest of the paper is structured as follows. Section II\ndescribes our proposed framework via surveying the existing\nresearch across relevant areas, including both image super-\nresolution and spatial transformations. Section III presents\nexperimental results and their analyses. Finally, section IV\ndraws the concluding remarks and future research.\nII. SPATIALLY TRANSFORMED DEEP FRAMEWORK FOR\nROBUST IMAGE SUPER-RESOLUTION\nA. RELATED WORK AND BACKGROUNDS\nIn general, the relationship between the original high-\nresolution and observed low-resolution images can be\ndescribed by the following image observation model.\nYLR =DHXHR (1)\nwhere H is a degradation matrix that represents geometric\ntransformations, and a down-sampling operator, D, is applied\nVOLUME 7, 2019 45619\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nto the original image XHR (i.e. HR image), to generate the\nobserved image YLR (i.e. LR image). Due to the ill-condition\nof the SR problem, recovering HR image is not unique. The\nproblem of image SR reconstruction can be generally mod-\neled as: given the LR image YLR, the objective is to estimate a\nHR image that is visually the same as the original HR image\nXHR. This problem can be formulated as\nˆxHR =argmin\nxHR\n∥YLR −DHXHR∥2 (2)\nTo solve this problem, an effective prior is necessary to turn\nthe problem into a deterministic problem. Many solutions\nhave been proposed to solve this ill-posed problem in the past\ndecades [6]– [8]. Recently, there have been a large number of\nstudies on deep learning for solving the image SR problem.\nAs a result, we brieﬂy review all the existing deep learning\nbased SR techniques as follows in order to pave the way for\nour proposed framework.\nAs part of the pioneering exploration, super-resolution\nusing deep convolutional networks (SRCNN) [8] is one of\nthe state of the art for deep learning based SR methods,\nin which Dong et al. introduced a CNN constructing a map-\nping from the bicubic up-sampled LR space to HR space.\nSpeciﬁcally, SRCNN utilizes the bicubic interpolation as its\npre-processing step and then extracts the features of the over-\nlapping patches using deep convolutional layers. At the ﬁnal\nstep, the extracted feature vectors are non-linearly mapped to\neach other and subsequently aggregated in the form of patches\nto form the reconstructed HR image.\nIn general, the essential advantage of the SRCNN is that\nonly convolution layers are used. Consequently, the input\nimage can be of any size and the algorithm can run on the\ninput image in one pass. Although SRCNN claims efﬁciency\nin view of the its straightforward structure, it still has a\nnumber of limitations, in which the primary limitation is that\nthe convergence of the network is too slow, and the network\nonly works on a single scale.\nEfforts were made by Dong et al. [40] to improve the\nefﬁciency of SRCNN, for which Dong et al. proposed fast\nSRCNN (FSRCNN). The reported FSRCNN [40] replaced\nthe pre-processing Bicubic interpolation in SRCNN by a\npost-processing in the form of deconvolution. In addition,\nthe FSRCNN has four convolution layers in the form of\nfeature extraction, shrinking, mapping and expanding. That\nis, the mapping is preceded by shrinking feature dimensions\nand then expanding back at latter stages.\nThe success of CNN in SR often raises a question: whether\na deeper network should be adopted in order to maximize\nthe SR performances? Kim et al. [41] answered this question\nby proposing a very deep super-resolution (VDSR) network,\nwhich uses a very deep convolution network inspired by\nVGG-net used for ImageNet classiﬁcation. The structure of\nVDSR contains 20 layers in a cascaded deep network. The\nﬁlter size used in the network is 3-by-3 [41]. Kim et al. [41]\nutilized the residual learning to train the VDSR network, and\nAlgorithm 1 Pseudocode of the Robust Single Image Super-\nResolution Using Our Proposed Framework\nInput: A LR image YLR.\nOutput: An estimated HR image ˆXHR.\nInitialization: Preparing the training dataset by distorted the\nHR image by bicubic downsampling and one or more of\ngeometric transformation as in equation (1).\nSteps of our proposed network:\n• First Module: Deep residual learning based spatial\ntransform module;\n– Deep residual learning localization network:\n∗ Extract the features of LR image through\n20 stacked convolution layers as described in\nequation (6).\n∗ LR image version is added to the output of\nstacked layers.\n∗ The output is passed to the classiﬁer to estimate\nthe 6 geometric transformation parameters as\ngiven in equation (7).\n– Grid Generator: Estimate the sampling grid based\non the estimated geometric transformation parame-\nters.\n– Sampler: Interpolate the input LR image accord-\ning to the sampling grid to alleviate the geometric\ntransformation effects as described in equation (8).\n• Second Module: Super-Resolution module: Reﬁne the\nLR image to generate a HR image similar to the desired\ntarget.\nthe VDSR tackled the limitation of SRCNN by extending to\nmulti-scales SR with a single network model.\nOther researchers focus on utilizing different loss func-\ntions, instead of the mean square error (MSE), to generate HR\nimages. Johnson et al. [42] utilized perceptual loss function\nto generate visually comforting results. Ledig et al. [43]\nemployed a deep residual network (ResNet) as the discrim-\ninator to form the Super Resolution Generative Adversarial\nNetwork (SRGAN). The major problem of these networks is\nthe difﬁcult hyper-parameter tuning in the training process,\nincluding the weight initialization, the weight decay rate,\nand the learning rate, etc. A desirable property of an image\nprocessing system is to reason about the possible changes\nof object poses, and their relevant spatial transformations.\nA desirable SR network should be spatially invariant to\nthe scaling and rotation effects incurred during transmis-\nsion or hostile corruption. A spatial transformer network\n(STN) [39] is introduced to exploit the power of deep learning\nfor dealing with spatial transformations, which is a dynamic\nmechanism that can spatially transform an image by pro-\nducing an appropriate afﬁne transformation for each input\nsample.\nSpeciﬁcally, a STN network takes an image or a feature\nmap from a convolutional network as the input, and then an\n45620 VOLUME 7, 2019\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nafﬁne transformation and bilinear interpolation is applied to\nproduce the output of the STN. The afﬁne transformation\nallows zoom, rotation and skew of the input, which enables\nthe network to not only select regions of an image that are\nmost relevant (attentive) but also to transform those regions\nto a canonical and expected pose, in order to simplify the\nrecognition process in the following layers. Further, STN can\nbe utilized with CNN to provide multi-functions, such as\nsuper-resolution as proposed in this paper.\nInspired by the spirit of both STN and residual learning,\nwe propose a new spatial transformable deep super-resolution\nframework, to facilitate a robust super-resolution for LR input\nimages. Extensive experiments support that our proposed\nframework is able to achieve expected robustness in compar-\nisons with the existing state of the arts using deep learning for\nimage super-resolutions.\nB. OUR PROPOSED WORK\nIn this section, we illustrate the details of our proposed\nframework, including the deep residual learning based spatial\ntransform module and the super-resolution module integrated\nto generate the ﬁnal HR image and complete the robust\nsingle image super-resolution. The objective of our proposed\nframework is to create a robust SR network which is able to\ngenerate HR images from LR transformed images. The key\naspect to achieving this goal is to be able to alleviate the effect\nof spatial transforms for corrupted LR images. The procedure\nof the proposed framework is summarized in Algorithm 1,\nwhich has the capability to tackle the transformation effects\nwhile reﬁning and generating HR images to achieve robust\nsuper-resolution. Consequently, our proposed framework not\nonly minimizes the effect of the geometric transformation\neffect, but also minimizes the difference between the esti-\nmated HR image and the HR image itself. Speciﬁc details\nare described as follows.\nL(θ) =argmin\nˆXHR −XHR\n\n2\n(3)\nwhere L (θ)is a loss function ( i.g. objective function), and θ\nrepresents the model parameters of the deep neural network.\nThe above equation can be established via two operational\nsteps. Firstly, we need to identify the afﬁne transformation\nparameters, in order to capture any possible geometric trans-\nformation, through minimizing the errors between the HR\nand the distorted image. Secondly, we generate estimated HR\nimage similar to the desired one through minimizing their\ncorresponding MSEs. As a result, our loss function can be\nfurther formulated as follows:\nL (θ)=argmin\nθA\nYLR(θA) −ˆXT\n\n2\n2\n+argmin\nθDRLN\nN( ˆXT ,θDRLN ) −XHR\n\n2\n2\n(4)\nwhere ˆXT is the output image after performing the spatial\ntransformation, and ˆXHR =N( ˆXT ,θDRLN ) is the estimated\nFIGURE 1. Spatial transform network [44].\nHR image, θDRLN is the model parameters of the super-\nresolution neural network. ˆXT is the output of a deep residual\nlearning based spatial transform module, and θA represents\nthe estimated geometric/afﬁne transformation parameters.\nThe ﬁrst part of Eq. (4), is to minimize the errors the\nerror between the transformed LR image and the desired LR\nimage for the super-resolution in the second part, in order\nto estimate the afﬁne transformation parameters and mitigate\nthe transformation effects. The second part of the equation is\nto minimize the errors between the output of the ﬁrst module\nand HR image to obtain an image similar to the desired HR\nimage. In practice, our network is trained end-to-end with\none loss function only in Eq. (4), where the two sub-tasks\n(i.e. the ﬁrst and the second part of Eq.(4)) are connected\nthrough the variable ˆXT , such that the weights of costs of\ntwo sub-tasks do not have any inﬂuences for the ﬁnal results.\nIn other words, given sufﬁcient number of iterations during\ntraining, the network converges regardless of the weights\nof costs of two sub-tasks. Moreover, the back-propagation\nof network passes through the whole network due to the\nseamless integration of two sub-tasks in Eq. (4).\nOver the recent years, deep neural networks have achieved\nhuge success in resolving various computer vision problems.\nNevertheless, there has not been an overwhelming solu-\ntion for the problem of geometric variations upon the given\ndatasets during the learning process. The recently introduced\nspatial transform network (STN) [39], which is able to per-\nform spatial transformations on images with a differentiable\nmodule, has the function of reducing geometric variations of\nnatural images and has attracted attentions across the deep\nlearning community.\nAs shown in Fig.1, the STN warps an image conditioned\non the input during the feed forward process, which can be\nformulated as:\nˆXT =YLR(θA), where θA =f (YLR) (5)\nwhere ˆXT is the output of STN, YLR is the input image to\nSTN and θA is the estimated geometric parameters.\nSTN contains three parts. The ﬁrst part is a localization\nnetwork, which takes the input image through convolutional\nneural network (CNN) and estimate the warping parameters\nfrom the input image. This part can be represented by a\nnonlinear function f , which is parametrized as a learnable\ngeometric predictor. In the second part, the predicted transfor-\nmation or warping parameters are utilized to form a sampling\ngrid, which is implemented by a grid generator. In the third\npart, the input image and the sampling grid are taken as\nVOLUME 7, 2019 45621\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nFIGURE 2. Deep residual learning network (DRLN) for localization\nnetwork.\ninputs to the sampler, in order to interpolate the output image.\nWe note that the grid generator and the sampler from the\noriginal method can be combined to form a single warping\nfunction as shown in Fig.1.\nInspired by the idea of utilizing the residual learning for\nfeature extraction [45], we propose to introduce a deeper\nresidual learning localization network (DRLN) into the exist-\ning STN to create additional spaces and capabilities for\nexploiting wider contextual information inside the input\nimages. Compared with the existing STN, our DRLN-based\nSTN achieves two advantages: (i) increase the learning power\nto achieve more accurate estimation of the afﬁne transform\nparameters; (ii) enable the STN not only to learn but also\nto optimize the capture of any geometric variation of input\nimages in an adaptive manner against any possible content\ncorruptions.\nIn the DRLN, we propose to stack 20 convolutional layers\nto extract the features of the input LR image. Compared with\nthe existing localization using only two convolution layers\nwith hundreds of feature maps [45]. the proposed DRLN is\na deeper network with only 64 feature maps per layer, which\nis more powerful yet requires less parameters. However it will\nbe more difﬁcult to train such a deep network because of the\nvanishing gradients. This explains why we use such a residual\nlearning framework, in order to overcome such a problem.\nThe operation of DRLN can be formulated as:\nˆXHR =N( ˆXT ,θDRRL ) +ˆXT (6)\nwhere ˆXT and ˆXHR are the input LR image and the output of\nthe DRLN network respectively, θDRRL is the weights of con-\nvolutional layers, and the function N( ˆXT ,θDRRL ) represents\nthe estimated residual using the weights of the convolutional\nlayers.\nFor the convenience of implementation, the feature maps\nthat are extracted from DRLN are converted to one dimen-\nsional vectors using a fully connected layer, before being\npassed to the classiﬁer that is able to estimate the geometric\ntransformation parameters. The output of the classiﬁer can be\ndescribed as:\nθA =\n[θ1 θ2 θ3\nθ4 θ5 θ6\n]\n(7)\nFIGURE 3. Illustration of our proposed very deep spatial transformer\n(VDST).\nIn this way, more contextual information inside the LR\ninput images can be exploited yet the number of learning\nparameters can also be reduced. Due to the deep structure\nof our proposed DRLN, the level of the feature is enriched\nand the accuracy of estimating the afﬁne parameters is signif-\nicantly improved in comparison with the existing STN [45].\nFig. 3 illustrates the overall structure of our proposed residual\nlearning based spatial transform module.\nAs seen in Fig. 3, while the proposed DRLN is responsible\nfor estimating the afﬁne transformation parameters, the esti-\nmated afﬁne parameters are utilized to form a mesh grid,\nwhich is implemented by grid generator. Both the input LR\nimage and the grid are taken as inputs to the sampler in order\nto warp the input images. The full operation of our proposed\ntransformer module can be described as follows:\nˆXc\nTi =\nH∑\nn\nW∑\nm\nYc\nLR(n,m)k (xi −m;8x )k\n(\nyi −n;8y\n)\n,\n∀i ∈\n[\n1....H′W ′]\n∀c ∈[1....C] (8)\nwhere HW and H′W ′ are the height and the width of the\ninput and output images, respectively, 8x and 8y are the\nparameters of generic sampling kernel k() which deﬁnes the\nimage interpolation, YLR(n,m) is the value at location (n, m)\nof the input, ˆXTi is the output value for pixel i at location\n(xi,yi), and C represents the number of the channel of the\ninput image.\nBy integrating the widely known VDSR [41] with the\nproposed deep residual learning based spatial transformer,\nwe can construct a robust spatial-transformed deep image\nsuper-resolution network as shown in Fig. 4. As seen,\nthe essential mechanism for achieving the robust single image\nsuper-resolution is the accurate estimation of spatial trans-\nform parameters, which simultaneously mitigate the geomet-\nric transformation effects and estimate the HR images via the\nvery deep learning process.\nIII. EVALUATIONS AND EXPERIMENTAL\nRESULTS ANALYSIS\nTo evaluate our proposed very deep spatial transformer\n(VDST), we carry out extensive experiments and report our\nexperimental results as well as their analyses in this section.\n45622 VOLUME 7, 2019\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nFIGURE 4. Illustration of the proposed framework for robust image super-resolution, where the input LR image is rotated\nby 20 degrees clock wise, and the HR image is generated with the corrected orientation after alleviating the geometric\ntransformation effects at the output.\nTo make it convenient for benchmarking and comparative\nstudies, we follow the experimental procedures as described\nin [41]. Firstly, we compare the performance of VDST with\nthe existing spatial transform network [39]. For this compari-\nson, we conduct experiments in order to evaluate the ability of\neach network to mitigate the geometric transformation effects\nand correct the orientation of the input image. Secondly, for\nthe analysis purposes, we calculate number of the parameters\nneeded in our proposed VDST and compare it with that of the\nexist spatial transform network.\nTo validate the effectiveness of our proposed VDST, exten-\nsive experiments are conducted on various standard bench-\nmark datasets, which contain hundreds of natural images.\nTo evaluate the performance of our proposed, a wide range\nof experiments are carried out under a number of different\ngeometric transformations, including rotation (R), scaling\n(S), and translation (T), and we use these geometric trans-\nformations to simulate the possible geometric distortions\nlikely incurred to the input images. The performance of our\nproposed framework is compared with the existing VDSR\nnetwork to verify the effectiveness of our proposed. To vali-\ndate that our proposed VDST outperforms the existing STN\nin terms of the robustness for single image super-resolution,\nwe carry out further experiments against the existing STN\nfollowed by VDSR, for which we call STN-VDSR as an\nadditional benchmark for assessing our proposed.\nA. EXPERIMENT DESIGN AND SETUP\n1) DATASET FOR TRAINING AND TESTING\nWe conduct the training experiments using 91 images from\nYang et al. [29] and 200 images from the training set of\nBerkeley Segmentation dataset [46] as our training data.\nIn each training batch, we randomly sample 25 patches.\nWe augment the training data in three ways by following\nthe protocol of the existing methods [41], and we generate\nthe LR training patches using the bicubic down-sampling.\nFor the convenience of benchmarking, we carry out exper-\niments using 5 publicly available datasets, including:\nBSDS100 [46], SET5 [47], SET14 [48], URBAN100 [49]\nand MANGA109 [50].\n2) IMPLEMENTATION DETAILS\nTo simulate the effect of the geometric transformations,\nwe generate the transformed LR training image by ﬁve\ndifferent transformations, including: (i) the rotation effect\nrepresented by R, in which the original image is rotated\nclockwise by 20 degrees; (ii) the scaling effect represented by\nS, in which the original image is scaled with a factor of 0.5;\n(iii) the effect of both rotation and scaling represented by RS;\n(iv) translation represented by T, in which the LR images are\ntranslated by 5 pixels in both X and Y directions; and ﬁnally\n(v) combinational effect of rotation, scaling and translation\nrepresented by RTS.\nAs explained in the spatial transformater [39], the spa-\ntial module can be inserted into any place of the exist-\ning network, and then the spatial network is trained\ntogether with the existing network. Hence, our combined\nnetwork (VDST-VDSR) is trained end-to-end for a num-\nber of iterations until its convergence is reached. For the\nconvenience of understanding and veriﬁcation, we make\nour training codes and testing codes available on the\nGithub: https://github.com/HossamMKasem/A-Very-Deep-\nSpatial-Transformer-Towards-Robust-Single-Image-Super-\nResolution.\nWe train all experiments over 10 epochs with batch size\n25, in which the learning rate is set to 0.01 and the learning\nrate decay is set to be 0. All the models that are utilized for\ncomparison purposes are trained in an end-to-end manner. All\nthe training is performed on NVIDIA Tesla P100. The eval-\nuation results of all experiments are presented in term of two\nmetrics widely used in the SR research community, which are\nPeak-signal-to-noise-ratio (PSNR) and Structural Similarity\nVOLUME 7, 2019 45623\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nFIGURE 5. Illustration of seven natural images used in our experiments.\nTABLE 1. Comparisons of the network structure between the proposed\nDRLN and the existing STN.\nIndex (SSIM). While the former metric is considered to be\nan objective measurement of the quality for generated HR\nimage, the latter metric is considered to measure subjective\nquality of the generated HR image.\nB. EXPERIMENTS MEASURED BY THE NUMBER OF\nPARAMETERS FOR DEEP-LEARNING NETWORKS\nBy using the number of parameters as the evaluating criteria,\nTable 1 summarizes the comparative results between our\nproposed VDST and the existing STN. In Table 1, the con-\nvolution layer is represented by Conv (ki,ni,ci), where the\nvariables ki, ni, ci represent the ﬁlter size, the number of\nﬁlters and the number of feature maps, respectively, and\nthe linear layer is represented by Linear (mi,oi), where the\nvariables mi, oi represent the size of the input vector and the\nsize of the output vector, respectively. Note that the classiﬁer\nworks like a linear regression module to estimate the afﬁne\ntransformation parameters. It consists of one fully-connected\nlayer where its network structure is described in Table 1.\nAs seen, the results given in Table 1 indicate that our\nproposed VDST is powerful in structure, cost-effective in\nlearning, and capturing well the contextual information from\nthe input images, leading to the improved performances.\nInspired by the existing work [51], which is a typical\napplication using the STN, we follow their design to pro-\nduce 200 feature maps as the ﬁrst convolutional layer and\n300 feature maps as the second layer, as shown in Table 1.\nFor our proposed VDST, we set the ﬁrst 19 layers to generate\n64 feature maps and the last convolutional layer to generate\n3 feature maps. Overall, our VDST is much deeper and\nthinner than the existing CNN for STN, paving the way for\nextracting deep contextual information.\nTABLE 2. Experimental results (PSNR/SSIM) achieved by the existing\nvarious network structures compared with our proposed framework with\nthe scaling factor×2.\nFrom Table 1, it can seen that the parameters of our pro-\nposed VDST is much less than that of the existing STN due\nto the fact that it uses less number of feature maps and much\ndeeper layers in comparison with the existing STN.\nC. EXPERIMENTS ON EFFECTIVENESS AND ROBUSTNESS\nOF OUR PROPOSED FRAMEWORK\nTo validate the effectiveness and the accuracy of the proposed\ndeep spatial transformer using DRLN, we have carried out\na range of experiments upon natural images to estimate the\nafﬁne transformation parameters and mitigate the geometric\ntransformation effects. Fig.5 illustrates seven samples of such\nnatural images adopted as the test images in our experiments.\nTo evaluate the performance of our proposed, we trans-\nformed the original images using two different transformation\neffects. First, we translate the original images by 5 pixels in\nX and Y directions and explore the robustness of our pro-\nposed VDST against geometric transformations. For visual\ninspections and comparisons between our proposed VDST\nand the existing STN [39], Fig.6 shows ﬁve samples of the\n45624 VOLUME 7, 2019\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nFIGURE 6. Visual comparison samples of our proposed VDST and existing STN: (i) the original images are translated in X, and Y direction by\n5 Pixels. (ii) The target input patches and the rotated input patches are illustrated at the bottom of each original image. The output patches of our\nproposed framework and the existing STN are given on the last row of images underneath each original image, and the bottom row presents the\ncorresponding outputs measured in PSNR/SSIM values. The lower row on the right is the output of the existing STN, and on the left is the output\nof our proposed VDST, respectively.\nFIGURE 7. PSNR and SSIM values of our proposed framework and the combination of the existing STN and VDSR: The original\nimages are translated in X, and Y direction by 5 pixels. The PSNR values are shown in (a), and SSIM values are shown in (b).\ntest images under the translation effect. It can be seen that\nVDST can estimate the afﬁne parameters more accurately,\nand hence mitigate the transformation effects better than the\nexisting STN. Further, the PSNR and SSIM values of our\nproposed spatial transformer is also much better than the exist\nSTN.\nFig.7 shows the objective evaluation results in terms of\nPSNR/SSIM when the original images are translated in X\nand Y direction by 5 pixels, simulating another type of\npossible geometric distortion effect. As seen again, our pro-\nposed achieves higher PSNR/SSIM values over the existing\nSTN. To test the robustness of our proposed against stronger\ntransformation effects, we apply the combinational effect\nof rotation, translation and scaling, and the corresponding\nexperimental results are illustrated in Fig. 8. As seen, the sim-\nulation results indicate that our proposed spatial transformer\nstill outperforms the existing STN in estimating the afﬁne\nparameters and mitigating the transformation effects.\nAs our proposed embeds a deeper DRLN and hence capa-\nble of exploiting more contextual information out of the\ninput images, more levels of features can be provided for\nthe classiﬁer, and hence estimate the afﬁne transformation\nparameters more accurately. Based on the deep estimation\nof parameters, our proposed VDST is able to mitigate the\ngeometric transformation effects and produce an image in\nthe same orientation as the desired image. Together with the\nexperimental results summarized in Table 1, we can conclude\nthat: (i) our proposed spatial transformer requires less num-\nber of network parameters than that of the existing spatial\ntransformer, providing better cost-effectiveness; (ii) our pro-\nposed VDST outperforms the existing spatial transformer in\nestimating the afﬁne parameters and hence achieving better\nmitigation for the possible geometric transformations.\nTo test the robustness of our proposed VDST against\nall the simulated geometric transformations, including rota-\ntion (R), scaling (S), rotation and scaling (RS), transla-\ntion (T), and combination of rotation, scaling and translation\n(RTS), we carried out ﬁve experiments in total to evaluate\nthe performances of our proposed in comparison with the\nexisting state of the art VDSR [41], and illustrate how spatial\nVOLUME 7, 2019 45625\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nFIGURE 8. PSNR and SSIM values of our proposed framework and the combination of the existing STN and VDSR: The original\nimages are rotated, translated and scaled (RTS). The PSNR values are shown in (a), and SSIM values are shown in (b).\nTABLE 3. Experimental results (PSNR/SSIM) achieved by the existing\nvarious network structures compared with our proposed framework with\nthe scaling factor×3.\ntransformation could be alleviated by our proposed frame-\nwork. As our proposed framework is the ﬁrst attempt for\nrobust single image super-resolution and there exist no close\nwork to compare, we combine the existing spatial transform\nnetwork (STN) with the VDSR together to formulate a new\nbenchmark, referred to as STN-VDSR.\nTable 2, Table 3 and Table 4 show the comparative experi-\nmental results between the existing VDSR, STN-VDSR and\nour proposed VDST-VDSR under a number of the transfor-\nmation effects with the scaling factors ×2, ×3, ×4, respec-\ntively. By comparing the results given in Table 2, we can\nobserve that our proposed method signiﬁcantly outperforms\nthe VDSR, and STN-VDSR in terms of both PSNR and\nSSIM. From the simulation results for the rotation effect,\nas an example, it can be further seen that our proposed method\nhas higher PSNR scores by at least 2dB in all test datasets\nacross all rotation angles. In terms of SSIM, our proposed also\nachieves higher values than that of VDSR and STN-VDSR.\nTABLE 4. Experimental results (PSNR/SSIM) achieved by the existing\nvarious network structures compared with our proposed framework with\nthe scaling factor×4.\nBy comparing and examining the simulation results across\nTable 3 and Table 4 for the effect of RS, T, and RTS, it can\nalso be seen that our VDST-VDSR outperforms the existing\nVDSR and STN-VDSR in both PSNR and SSIM values.\nCorrespondingly, it can be concluded that our proposed suc-\ncessfully provides a well-validated solution for tackling the\neffects of geometric transformations, and achieve a robust\nsingle image super-resolution. This achieved improvement is\ndue to the capability of our proposed method in exploiting\nthe contextual information spread over the image regions as\nwell as extraction of deeper features from the input images.\nIn other words, the experimental results conﬁrm that our\nproposed VDST-VDSR improves the accuracy of estimating\nthe transformation parameters, leading to the superior perfor-\nmances for robust single image super-resolution.\nTo visually compare the experimental results between our\nproposed and the existing benchmarks, we illustrate a num-\n45626 VOLUME 7, 2019\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nFIGURE 9. Visual comparison samples of our proposed VDST-VDSR, VDSR and STN-VDSR. Top left row: original image and\nrotated image by 20 degree. Under each row: The desired output, rotated input patch, VDSR output (PSNR/SSIM),STN-VDSR\noutput (PSNR/SSIM), and our proposed framework (PSNR/SSIM), respectively.\nFIGURE 10. Convergence curves of the objective function, and the PSNR/SSIM values under the rotation effect: (a) Loss function values during the\ntraining phase. (b) Loss function values during the validation phase; (c) The PSNR values; and (d) The SSIM values.\nber of samples in Fig.9 for visual inspections and subjec-\ntive assessments. In Fig.9, we show visual comparisons on\nBSDS 100, MANGA 109, URBAN 100 and SET 5 with a\nscaling factor of 2× under the rotation(R) effect. As seen,\nour proposed framework accurately mitigates the rotation\neffect, and reconstructs the straight lines and the grid patterns\nwell, such as the stripes on the tiger, the window, and the\nlines on the butterﬂy. The visual inspection supports that\nour proposed VDST-VDSR outperforms both VDSR and\nSTN-VDSR.\nD. EXPERIMENT ON CONVERGENCE\nWe conduct an experiment to test the convergence of the\nproposed VDST-VDSR according to the loss function in (4)\ncompared with STN-VDSR, and VDSR. All the results are\nillustrated as convergence curves during the training, and\ntesting steps. In addition, we test the performance of our\nproposed framework by calculating the PSNR/SSIM values\nduring each epoch, and hence compare these values with the\nexisting STN.\nWe test the convergence of our proposed framework under\nthe rotation(R) effect, the curves of the convergence, and\nPSNR/SSIM are presented in Fig. 10. From the results shown\nin Fig. 10, we can conclude that our proposed framework\ntakes less number of epochs to reach the convergence point\nthan that by STN-VDSR. our proposed framework takes no\nmore than 5 epochs to reach the convergence point. Fur-\nther, the PSNR/SSIM curves validate the superiority of our\nproposed VDST in terms of both PSNR and SSIM values.\nTo provide a comprehensive assessment, we test the conver-\ngence of our proposed framework under a stronger transfor-\nmation effect which is a combination of rotation, translation\nand scaling (RTS), and hence compare the proposed frame\nwork convergence curves with STN-VDSR curves as shown\nin Fig. 11. The results in Fig. 11 conﬁrm that our proposed\nVDST is able to reach the convergence point faster that than\nVOLUME 7, 2019 45627\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nFIGURE 11. Convergence curves of the objective function, and the PSNR/SSIM values under RTS effect: (a) Loss function values during the training\nphase.(b) Loss function values during the validation phase; (c) PSNR values; and (d) SSIM values.\nTABLE 5. Experimental results (PSNR/SSIM) achieved by the existing\nvarious network structures compared with our proposed framework with\nthe scaling factor×2.\nof STN-VDSR, and meanwhile, our proposed framework still\nhave higher PSNR and SSIM values than that of STN-VDSR.\nE. EXPERIMENT ON CHANGING THE ORDER OF\nGEOMETRIC TRANSFORMATION AND SUPER-RESOLUTION\nTo further evaluate our proposed, we conduct another experi-\nment to test the performance of the proposed VDST-VDSR\nin case it is required that the position of the spatial trans-\nform module and the VDSR module need to be changed.\nIn other words, we connect the VDSR model to the input\nimage to generate the HR image, followed by our VDST\nto mitigate the transformation effects. For fair comparison,\nwe change the order of the existing STN and VDSR module\nas the comparing benchmark. Correspondingly, we refer our\nnew method as VDSR-VDST and the existing method as\nVDSR-STN.\nAll the results are illustrated in Table 5. From the results\nshown in Table 5, we can see that our proposed VDSR-VDST\noutperforms the existing VDSR and VDSR-STN. We note\nthat the two versions of proposed method (VDST-VDSR and\nVDSR-VDST) in Table 2 and Table 5 produce similar results\nin terms of PSNR and SSIM values (with 0.32dB change in\nPSNR on average). Our analysis shows that this phenomenon\nTABLE 6. Experimental results (PSNR/SSIM) achieved by the existing\nvarious network structures compared with our proposed framework with\nthe scaling factor×2.\nis due to the fact that the spatial transformer is a linear module\nfor afﬁne transformation, yet the super resolution module is a\nnonlinear module (because of the ReLU nonlinear activation\nin the neural network). When both modules are interchanged,\nas a result, the nonlinearity of the super-resolution module\ninevitably causes the small changes in PSNR and SSIM\nvalues.\nF. EXPERIMENTS ON REAL WORLD IMAGES\nIn real scenarios, the users can deﬁne the desired geomet-\nric transformations manually to achieve the simultaneous\ngeometric transformation and super-resolution. To validate\nthe effectiveness and the accuracy of our proposed method,\n45628 VOLUME 7, 2019\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nFIGURE 12. Illustration of nine real world images used in our experiments.\nFIGURE 13. Illustration of real world images for geometric transformation and super-resolution.\nwe have conducted extensive experiments on real world\nimages to estimate the afﬁne transformation parameters and\nmitigate the geometric transformation effects. Fig.12 illus-\ntrates nine samples of real world images adopted as test\nimages in our experiments. We have labelled the images\nused in the test by numbers from the top left to bottom\nright in Fig.12 (i.e. we give number 1 to the image in the\nleft of the top row, number 2 to the next image in the top\nrow, and number 6 to the left image in the bottom row and\nso on.). These images were captured using Nikon coolpix\nP500 camera. We carried out ﬁve experiments to evaluate the\nperformances of our proposed method in comparison with the\nexisting state of the art VDSR [41] and the combination of\nSTN and VDSR. We tested the methods under ﬁve different\ntransformation effects, including rotation (R), scaling (S),\nrotation and scaling (RS), translation (T), and combination\nof rotation, scaling and translation (RTS).\nTable 6 shows the experimental results of the existing\nVDSR, STN-VDSR and our proposed VDST-VDSR under\na number of various transformation effects with scale ×2.\nBy comparing the results shown in Table 6, we can conclude\nthat our proposed method shows superior performances com-\npared with the existing VDSR and STN-VDSR in terms of\nPSNR and SSIM across all the testing images. The results\nin Table 6 show that our proposed method produces higher\nPSNR and SSIM values for the images that contain more\nVOLUME 7, 2019 45629\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\nforeground shapes. Compared with the benchmarks, our pro-\nposed achieves at least 1dB higher PSNR scores across all the\ntested images.\nFig.13 illustrates the results of our further experiments\nupon real-world images, where three input samples are\nshown, containing rich details such as face, fence, and shapes\nfor detailed comparative analysis. As seen, the proposed\nVDST-VDSR can preserve the image details while suc-\ncessfully performing geometric transformation and super-\nresolutions. In contrast, the combination of STN and VDSR,\ni.e., STN-VDSR, fails to achieve the desired translation and\nRTS effects, where more blurry results are produced and\nnoticeable. Therefore, such experimental results further vali-\ndate the improved performances and advantages achieved by\nour proposed.\nIV. CONCLUSION\nIn this paper, we proposed a novel spatially transformed\ndeep framework to achieve robust single image super-\nresolution. Our proposed framework can simultaneously per-\nform geometric corrections and super-resolution reconstruc-\ntion, which, to the best of our knowledge, is the ﬁrst deep\nlearning-based method to tackle such problems. Our pro-\nposed framework is based on improving the existing STN\nwith a deep residual learning network (DRLN) to extract\ndeeper features and exploit contextual information against\nthe process of possible geometric transformations. Extensive\nevaluations on widely used datasets with different transfor-\nmation effects demonstrate that the proposed deep framework\nsuccessfully mitigates the effect of the geometric transforma-\ntions and achieves superior performances in comparison with\nthe existing state of arts, including VDSR and STN-VDSR,\nthe combination of the existing state of the arts for both spatial\ntransformation and image super-resolution. Future research\ncan be identiﬁed to consider multiple networks for handling\nwider range of transformations, leading to a further opti-\nmization of the SR performances and improvement upon its\nrobustness against not only the geometric transformations but\nalso distortions.\nACKNOWLEDGMENT\n(Jianmin Jiang and Hossam M. Kasem are co-ﬁrst authors.)\nREFERENCES\n[1] H. Wang, X. Gao, K. Zhang, and J. Li, ‘‘Single-image super-resolution\nusing active-sampling Gaussian process regression,’’ IEEE Trans. Image\nProcess., vol. 25, no. 2, pp. 935–948, Feb. 2016.\n[2] Y. Zhang, J. Liu, W. Yang, and Z. Guo, ‘‘Image super-resolution based\non structure-modulated sparse representation,’’ IEEE Trans. Hum.-Mach.\nSyst., vol. 24, no. 9, pp. 2797–2810, Sep. 2015.\n[3] Y. Li, Y. Wang, Y. Li, L. Jiao, X. Zhang, and R. Stolkin, ‘‘Single image\nsuper-resolution reconstruction based on genetic algorithm and regulariza-\ntion prior model,’’ Inf. Sci., vol. 372, pp. 196–207, Dec. 2016.\n[4] T. Huang and R. Tsai, ‘‘Multi-frame image restoration and registration,’’\nin Advances in Computer Vision and Image Processing. Greenwich, CT,\nUSA: JAI Press, 1984, pp. 317–339.\n[5] R. Chao, X. He, and T. Q. Nguyen, ‘‘Single image super-resolution via\nadaptive high-dimensional non-local total variation and adaptive geomet-\nric feature,’’ IEEE Trans. Image Process., vol. 26, no. 1, pp. 90–106,\nJan. 2017.\n[6] J. Jiang, X. Ma, C. Chen, T. Lu, Z. Wang, and J. Ma, ‘‘Single image\nsuper-resolution via locally regularized anchored neighborhood regression\nand nonlocal means,’’ IEEE Trans. Multimedia, vol. 19, no. 1, pp. 15–26,\nJan. 2017.\n[7] L. J. Deng, W. Guo, and T.-Z. Huang, ‘‘Single image super-resolution\nby approximated Heaviside functions,’’ Inf. Sci., vol. 348, pp. 107–123,\nJun. 2016.\n[8] C. Dong, C. C. Loy, K. He, and X. Tang, ‘‘Image super-resolution using\ndeep convolutional networks,’’ IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 38, no. 2, pp. 295–307, Feb. 2015.\n[9] S. Farsiu, M. D. Robinson, M. Elad, and P. Milanfar, ‘‘Fast and robust\nmultiframe super resolution,’’ IEEE Trans. Image Process., vol. 13, no. 10,\npp. 1327–1344, Oct. 2004.\n[10] Q. Yuan, L. Zhang, and H. Shen, ‘‘Multiframe super-resolution employing\na spatially weighted total variation model,’’ IEEE Trans. Circuits Syst.\nVideo Technol., vol. 22, no. 3, pp. 379–392, Mar. 2012.\n[11] A. W. M. van Eekeren, K. Schutte, and L. J. van Vliet, ‘‘Multiframe super-\nresolution reconstruction of small moving objects,’’ IEEE Trans. Image\nProcess., vol. 19, no. 11, pp. 2901–2912, Nov. 2010.\n[12] K. I. Kim and Y. Kwon, ‘‘Single-image super-resolution using sparse\nregression and natural image prior,’’ IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 32, no. 6, pp. 1127–1133, Jun. 2010.\n[13] Z. Zhu, F. Guo, H. Yu, and C. Chen, ‘‘Fast single image super-resolution\nvia self-example learning and sparse representation,’’ IEEE Trans. Multi-\nmedia, vol. 16, no. 8, pp. 2178–2190, Dec. 2014.\n[14] N. Qi, Y. Shi, X. Sun, W. Ding, and B. Yin, ‘‘Single image super-resolution\nvia 2D sparse representation,’’ in Proc. IEEE Int. Conf. Multimedia Expo\n(ICME), Jun./Jul. 2015, pp. 1–6.\n[15] S. Rhee and M. G. Kang, ‘‘Discrete cosine transform based regularized\nhigh-resolution image reconstruction algorithm,’’ Opt. Eng., vol. 38, no. 8,\npp. 1348–1356, 1999.\n[16] H. Demirel, S. Izadpanahi, and G. Anbarjafari, ‘‘Improved motion-based\nlocalized super resolution technique using discrete wavelet transform for\nlow resolution video enhancement,’’ in Proc. 17th Eur. Signal Process.\nConf., Aug. 2009, pp. 1097–1101.\n[17] L. Yue, H. Shen, J. Li, Q. Yuanc, H. Zhang, and L. Zhang, ‘‘Image super-\nresolution: The techniques, applications, and future,’’ Signal Process.,\nvol. 128, pp. 389–408, Nov. 2016.\n[18] M. S. Alam, J. G. Bognar, R. C. Hardie, and B. J. Yasuda, ‘‘Infrared image\nregistration and high-resolution reconstruction using multiple translation-\nally shifted aliased video frames,’’ IEEE Trans. Instrum. Meas., vol. 49,\nno. 5, pp. 915–923, Oct. 2002.\n[19] B. Wan, L. Meng, D. Ming, H. Qi, Y. Hu, and K. D. K. Luk, ‘‘Video image\nsuper-resolution restoration based on iterative back-projection algorithm,’’\nin Proc. IEEE Int. Conf. Comput. Intell. Meas. Syst. Appl., May 2009,\npp. 46–49.\n[20] H. Stark and P. Oskoui, ‘‘High-resolution image recovery from image-\nplane arrays, using convex projections,’’ J. Opt. Soc. Amer. A, Opt. Image\nSci., vol. 6, no. 11, pp. 1715–1726, 1989.\n[21] K. Zhang, X. Gao, D. Tao, and X. Li, ‘‘Single image super-resolution\nwith non-local means and steering kernel regression,’’ IEEE Trans. Image\nProcess., vol. 21, no. 11, pp. 4544–4556, Nov. 2012.\n[22] T. Sigitani, Y. Iiguni, and H. Maeda, ‘‘Image interpolation for progressive\ntransmission by using radial basis function networks,’’ IEEE Trans. Neural\nNetw., vol. 10, no. 2, pp. 381–390, Mar. 1999.\n[23] M. Protter, M. Elad, H. Takeda, and P. Milanfar, ‘‘Generalizing the\nnonlocal-means to super-resolution reconstruction,’’ IEEE Trans. Image\nProcess., vol. 18, no. 1, pp. 36–51, Jan. 2009.\n[24] J. Sun, Z. Xu, and H.-Y. Shum, ‘‘Image super-resolution using gradient\nproﬁle prior,’’ inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,\nJun. 2008, pp. 1–8.\n[25] X. Li, H. He, R. Wang, and D. Tao, ‘‘Single image superresolution via\ndirectional group sparsity and directional features,’’ IEEE Trans. Image\nProcess., vol. 24, no. 9, pp. 2874–2888, Sep. 2015.\n[26] S. C. Park, M. K. Park, and M. G. Kang, ‘‘Super-resolution image recon-\nstruction: A technical overview,’’IEEE Signal Process. Mag., vol. 20, no. 3,\npp. 21–36, May 2003.\n[27] Y.-W. Tai, S. Liu, M. S. Brown, and S. Lin, ‘‘Super resolution using edge\nprior and single image detail synthesis,’’ in Proc. Comput. Vis. Pattern\nRecognit., Jun. 2010, pp. 2400–2407.\n[28] J. Sun, J. Sun, Z. Xu, and H.-Y. Shum, ‘‘Gradient proﬁle prior and its\napplications in image super-resolution and enhancement,’’ IEEE Trans.\nImage Process., vol. 20, no. 6, pp. 1529–1542, Jun. 2011.\n45630 VOLUME 7, 2019\nJ. Jianget al.: Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution\n[29] J. Yang, J. Wright, T. S. Huang, and Y. Ma, ‘‘Image super-resolution\nvia sparse representation,’’ IEEE Trans. Image Process., vol. 19, no. 11,\npp. 2861–2873, Nov. 2010.\n[30] W. Dong, L. Zhang, G. Shi, and X. Wu, ‘‘Image deblurring and\nsuper-resolution by adaptive sparse domain selection and adaptive regu-\nlarization,’’ IEEE Trans. Image Process., vol. 20, no. 7, pp. 1838–1857,\nJul. 2011.\n[31] T. Peleg and M. Elad, ‘‘A statistical prediction model based on sparse\nrepresentations for single image super-resolution,’’ IEEE Trans. Image\nProcess., vol. 23, no. 6, pp. 2569–2582, Jun. 2014.\n[32] V. Papyan and M. Elad, ‘‘Multi-scale patch-based image restoration,’’\nIEEE Trans. Image Process., vol. 25, no. 1, pp. 249–261, Jan. 2016.\n[33] W. Dong, L. Zhang, R. Lukac, and G. Shi, ‘‘Sparse representation based\nimage interpolation with nonlocal autoregressive modeling,’’ IEEE Trans.\nImage Process., vol. 22, no. 4, pp. 1382–1394, Apr. 2013.\n[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classiﬁcation\nwith deep convolutional neural networks,’’ in Proc. Int. Conf. Neural Inf.\nProcess. Syst., 2012, pp. 1097–1105.\n[35] C. Dong, C. C. Loy, K. He, and X. Tang, ‘‘Learning a deep convolutional\nnetwork for image super-resolution,’’ in Computer Vision—ECCV (Lec-\nture Notes in Computer Science), vol. 8692, 2014, pp. 184–199.\n[36] C. Osendorfer, H. Soyer, and P. van der Smagt, ‘‘Image super-resolution\nwith fast approximate convolutional sparse coding,’’ in Proc. Int. Conf.\nNeural Inf. Process., 2014, pp. 250–257.\n[37] J. Yang, Z. Wang, Z. Lin, S. Cohen, and T. Huang, ‘‘Coupled dictionary\ntraining for image super-resolution,’’ IEEE Trans. Image Process., vol. 21,\nno. 8, pp. 3467–3478, Aug. 2012.\n[38] X. Wang, K. Yu, C. Dong, and C. C. Loy, ‘‘Recovering realistic texture in\nimage super-resolution by deep spatial feature transform,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 606–615.\n[39] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu, ‘‘Spatial\ntransformer networks,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015,\npp. 2017–2025.\n[40] C. Dong, C. C. Loy, and X. Tang, ‘‘Accelerating the super-resolution\nconvolutional neural network,’’ in Proc. Eur. Conf. Comput. Vis., 2016,\npp. 391–407.\n[41] J. Kim, J. K. Lee, and K. M. Lee, ‘‘Accurate image super-resolution using\nvery deep convolutional networks,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., Jun. 2016, pp. 1646–1654.\n[42] J. Johnson, A. Alahi, and F. F. Li, ‘‘Perceptual losses for real-time style\ntransfer and super-resolution,’’ in Proc. Eur. Conf. Comput. Vis. , 2016,\npp. 694–711.\n[43] C. Ledig et al., ‘‘Photo-realistic single image super-resolution using a\ngenerative adversarial network,’’ in Proc. Comput. Vis. Pattern Recognit.,\nJul. 2017, pp. 105–114.\n[44] C.-H. Lin and S. Lucey, ‘‘Inverse compositional spatial transformer net-\nworks,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJul. 2017, pp. 2252–2260.\n[45] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for\nimage recognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2016, pp. 770–778.\n[46] P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik, ‘‘Contour detection and\nhierarchical image segmentation,’’IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 33, no. 5, pp. 898–916, May 2011.\n[47] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel, ‘‘Low-\ncomplexity single-image super-resolution based on nonnegative neighbor\nembedding,’’ in Proc. BMVC, 2012.\n[48] R. Zeyde, M. Elad, and M. Protter, ‘‘On single image scale-up using sparse-\nrepresentations,’’ in Proc. Int. Conf. Curves Surfaces . Berlin, Germany:\nSpringer, 2010, pp. 711–730.\n[49] J.-B. Huang, A. Singh, and N. Ahuja, ‘‘Single image super-resolution from\ntransformed self-exemplars,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., Jun. 2015, pp. 5197–5206.\n[50] Y. Matsui et al., ‘‘Sketch-based manga retrieval using manga109 dataset,’’\nMultimedia Tools Appl., vol. 76, no. 20, pp. 21811–21838, 2017.\n[51] D. CireçAn, U. Meier, J. Masci, and J. Schmidhuber, ‘‘2012 special issue:\nMulti-column deep neural network for trafﬁc sign classiﬁcation,’’ Neural\nNetw., vol. 32, no. 1, pp. 333–338, 2012.\nJIANMIN JIANG received the Ph.D. degree from\nthe University of Nottingham, Nottingham, U.K.,\nin 1994. From 1997 to 2001, he was a Full Pro-\nfessor of computing with the University of Glam-\norgan, Pontypridd, U.K. In 2002, he joined the\nUniversity of Bradford, Bradford, U.K., as a Chair\nProfessor of digital media and the Director of the\nDigital Media and Systems Research Institute. He\nwas a Full Professor with the University of Sur-\nrey, Guildford, U.K., from 2010 to 2014, and a\nDistinguished Chair Professor (1000-Plan) with Tianjin University, Tianjin,\nChina, from 2010 to 2013. He is currently a Distinguished Chair Professor\nand the Director of the Research Institute for Future Media Computing, Col-\nlege of Computer Science and Software Engineering, Shenzhen University,\nShenzhen, China. He has published around 400 refereed research papers.\nHis current research interests include, image/video processing in compressed\ndomain, digital video coding, medical imaging, computer graphics, machine\nlearning and AI applications in digital media processing, and retrieval and\nanalysis. He was a Chartered Engineer, a Fellow of IEE and RSA, a member\nof EPSRC College in the U.K., and an EU FP-6/7 Evaluator.\nHOSSAM M. KASEMreceived the Ph.D. degree\nfrom the Egypt-Japan University of Science and\nTechnology, Egypt, in 2015. From 2015 to 2017,\nhe was an Assistant Professor with Faculty of\nEngineering, Tanta University, Egypt. Since 2017,\nhe has been a Postdoctoral Fellow with the Digital\nMedia and Systems Research Institute, Shenzhen\nUniversity, China. His research interests include\ndeep learning applications in digital multime-\ndia analysis, wireless communication, and signal\nprocessing application in multimedia.\nKWOK-WAI HUNG received the B.Eng. and\nPh.D. degrees from The Hong Kong Polytechnic\nUniversity, in 2009 and 2014, respectively. From\n2014 to 2016, he was a Research Engineer with\nHuawei and ASTRI. Since 2016, he has been an\nAssistant Professor with the Research Institute for\nFuture Media Computing, Shenzhen University,\nChina. His research interests include deep learning\napplications in digital multimedia processing and\nsignal processing applications in multimedia.\nVOLUME 7, 2019 45631",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7058327794075012
    },
    {
      "name": "Computer science",
      "score": 0.7041851282119751
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6986300945281982
    },
    {
      "name": "Geometric transformation",
      "score": 0.6936930418014526
    },
    {
      "name": "Transformation geometry",
      "score": 0.6756404638290405
    },
    {
      "name": "Image resolution",
      "score": 0.5908559560775757
    },
    {
      "name": "Computer vision",
      "score": 0.5020780563354492
    },
    {
      "name": "Residual",
      "score": 0.4959331452846527
    },
    {
      "name": "Transformation (genetics)",
      "score": 0.4942901134490967
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4334450960159302
    },
    {
      "name": "Deep learning",
      "score": 0.42760980129241943
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3577061891555786
    },
    {
      "name": "Algorithm",
      "score": 0.3297463655471802
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ]
}