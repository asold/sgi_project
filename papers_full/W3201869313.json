{
  "title": "MatSciBERT: A materials domain language model for text mining and information extraction",
  "url": "https://openalex.org/W3201869313",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2615565824",
      "name": "Tanishq Gupta",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2096813752",
      "name": "Mohd Zaki",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2583359429",
      "name": "N. M. Anoop Krishnan",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A4222705522",
      "name": "Mausam",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2615565824",
      "name": "Tanishq Gupta",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2096813752",
      "name": "Mohd Zaki",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2583359429",
      "name": "N. M. Anoop Krishnan",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A4222705522",
      "name": "Mausam",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1992985800",
    "https://openalex.org/W2794979247",
    "https://openalex.org/W3003769243",
    "https://openalex.org/W2929465105",
    "https://openalex.org/W3128723983",
    "https://openalex.org/W3125322888",
    "https://openalex.org/W2079834734",
    "https://openalex.org/W2964864162",
    "https://openalex.org/W3175823016",
    "https://openalex.org/W3197142714",
    "https://openalex.org/W3126611342",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3034156543",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2523785361",
    "https://openalex.org/W3047398431",
    "https://openalex.org/W2808304511",
    "https://openalex.org/W2980932864",
    "https://openalex.org/W3117009924",
    "https://openalex.org/W2936166854",
    "https://openalex.org/W3130221957",
    "https://openalex.org/W3115677442",
    "https://openalex.org/W2165671627",
    "https://openalex.org/W3031157367",
    "https://openalex.org/W3024236153",
    "https://openalex.org/W2944400536",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3103490574",
    "https://openalex.org/W3201869313",
    "https://openalex.org/W4248414713",
    "https://openalex.org/W3034949140",
    "https://openalex.org/W2966049804",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3103989898",
    "https://openalex.org/W3197209004",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2987270981",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2951299559",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2973154071",
    "https://openalex.org/W2948740140",
    "https://openalex.org/W2531274738",
    "https://openalex.org/W2970636124",
    "https://openalex.org/W3101118213"
  ],
  "abstract": "Abstract A large amount of materials science knowledge is generated and stored as text published in peer-reviewed scientific literature. While recent developments in natural language processing, such as Bidirectional Encoder Representations from Transformers (BERT) models, provide promising information extraction tools, these models may yield suboptimal results when applied on materials domain since they are not trained in materials science specific notations and jargons. Here, we present a materials-aware language model, namely, MatSciBERT, trained on a large corpus of peer-reviewed materials science publications. We show that MatSciBERT outperforms SciBERT, a language model trained on science corpus, and establish state-of-the-art results on three downstream tasks, named entity recognition, relation classification, and abstract classification. We make the pre-trained weights of MatSciBERT publicly accessible for accelerated materials discovery and information extraction from materials science texts.",
  "full_text": "ARTICLE OPEN\nMatSciBERT: A materials domain language model for text\nmining and information extraction\nTanishq Gupta 1, Mohd Zaki 2, N. M. Anoop Krishnan 2,3 ✉ and Mausam 3,4 ✉\nA large amount of materials science knowledge is generated and stored as text published in peer-reviewed scientiﬁc literature.\nWhile recent developments in natural language processing, such as Bidirectional Encoder Representations from Transformers\n(BERT) models, provide promising information extraction tools, these models may yield suboptimal results when applied on\nmaterials domain since they are not trained in materials science speciﬁc notations and jargons. Here, we present a materials-aware\nlanguage model, namely, MatSciBERT, trained on a large corpus of peer-reviewed materials science publications. We show that\nMatSciBERT outperforms SciBERT, a language model trained on science corpus, and establish state-of-the-art results on three\ndownstream tasks, named entity recognition, relation classiﬁcation, and abstract classiﬁcation. We make the pre-trained weights of\nMatSciBERT publicly accessible for accelerated materials discovery and information extraction from materials science texts.\nnpj Computational Materials          (2022) 8:102 ; https://doi.org/10.1038/s41524-022-00784-w\nINTRODUCTION\nDiscovering materials and utilizing them for practical applications\nis an extremely time-consuming process that may span decades\n1,2.\nTo accelerate this process, we need to exploit and harness the\nknowledge on materials that has been developed over the\ncenturies through rigorous scienti ﬁc procedure in a cohesive\nfashion\n3–8. Textbooks, scientiﬁc publications, reports, handbooks,\nwebsites, etc., serve as a large data repository that can be mined\nfor obtaining the already existing information\n9,10. However, it is a\nchallenging task to extract useful information from these texts\nsince most of the scientiﬁc data is semi- or un-structured in the\nform of text, paragraphs with cross reference, image captions, and\ntables\n10–12. Extracting such information manually is extremely\ntime- and resource-intensive and relies on the interpretation of a\ndomain expert.\nNatural language processing (NLP), a sub-domain in artiﬁcial\nintelligence, presents an alternate approach that can automate\ninformation extraction from text. Earlier approaches in NLP relied\non non-neural methods based onn-grams such as Brown et al.\n(1992)\n13, structural learning framework by Ando and Zhang\n(2005)14, or structural correspondence learning by Blitzer et al.\n(2006)15, but these are no longer state of the art. Neural pre-\ntrained embeddings like word2vec 16,17 and GloVe18 are quite\npopular, but they lack domain-speciﬁc knowledge and do not\nproduce contextual embeddings. Recent progress in NLP has led\nto the development of a computational paradigm in which a large,\npre-trained language model (LM) isﬁnetuned for domain-speciﬁc\ntasks. Research has consistently shown that this pretrain-ﬁnetune\nparadigm leads to the best overall task performance\n19–23.\nStatistically, LMs are probability distributions for a sequence of\nwords such that for a given set of words, it assigns a probability to\neach word\n24. Recently, due to the availability of large amounts of\ntext and high computing power, researchers have been able to\npre-train these large neural language models. For example,\nBidirectional Encoder Representations from Transformers (BERT)\n25\nis trained on BookCorpus26 and English Wikipedia, resulting in\nstate-of-the-art performance on multiple NLP tasks like question\nanswering and entity recognition, to name a few.\nResearchers have used NLP tools to automate database creation\nfor ML applications in the materials science domain. For instance,\nChemDataExtractor27, an NLP pipeline, has been used to create\ndatabases of battery materials28, Curie and Néel temperatures of\nmagnetic materials29, and inorganic material synthesis routes30.\nSimilarly, NLP has been used to collect the composition and\ndissolution rate of calcium aluminosilicate glassy materials31, and\nzeolite synthesis routes to synthesize germanium containing\nzeolites32, and to extract process and testing parameters of oxide\nglasses, thereby enabling improved prediction of the Vickers\nhardness11. Researchers have also made an automated NLP tool to\ncreate databases using the information extracted from computa-\ntional materials science research papers\n33. NLP has also been used\nfor other tasks such as topic modeling in glasses, that is, to group\nthe literature into different topics in an unsupervised fashion and\nto ﬁnd images based on speciﬁc queries such as elements present,\nsynthesis, or characterization techniques, and applications\n10.\nA comprehensive review by Olivetti et al. (2019) describes\nseveral ways in which NLP can bene ﬁt the materials science\ncommunity34. Providing insights into chemical parsing tools like\nOSCAR435 capable of identifying entities and chemicals from text,\nArtiﬁcial Chemist36, which takes the input of precursor information\nand generates synthetic routes to manufacture optoelectronic\nsemiconductors with targeted band gaps, robotic system for\nmaking thin ﬁlms to produce cleaner and sustainable energy\nsolutions37, and identiﬁcation of more than 80 million materials\nscience domain-speci ﬁc named entities, researches have\nprompted the accelerated discovery of materials for different\napplications through the combination of ML and NLP techniques.\nResearchers have shown the domain adaptation capability of\nword2vec and BERT in the ﬁeld of biological sciences as\nBioWordVec\n38 and BioBERT19, other domain-speciﬁc BERTs like\nSciBERT21 trained on scientiﬁc and biomedical corpus39, clinical-\nBERT40 trained on 2 million clinical notes in MIMIC-III v1.4\n1Department of Mathematics, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India.2Department of Civil Engineering, Indian Institute of Technology Delhi,\nHauz Khas, New Delhi 110016, India.3School of Artiﬁcial Intelligence, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India.4Department of Computer Science\nand Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India.✉email: krishnan@iitd.ac.in; mausam@iitd.ac.in\nwww.nature.com/npjcompumats\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\n1234567890():,;\ndatabase41, mBERT42 for multilingual machine translations tasks,\nPatentBERT23 for patent classiﬁcation and FinBERT for ﬁnancial\ntasks22. This suggests that a materials-aware LM can signiﬁcantly\naccelerate the research in the ﬁeld by further adapting to\ndownstream tasks 9,34. Although there were no papers on\ndeveloping materials-aware language models prior to this work43,\nin a recent preprint44, Walker et al. (2021) emphasize the impact of\ndomain-speciﬁc language models on named entity recognition\n(NER) tasks in materials science.\nIn this work, we train materials science domain-speciﬁc BERT,\nnamely MatSciBERT. Figure1 shows the graphical summary of the\nmethodology adopted in this work encompassing creating the\nmaterials science corpus, training the MatSciBERT, and evaluating\ndifferent downstream tasks. We achieve state-of-the-art results on\ndomain-speciﬁc tasks as listed below.\na. NER on SOFC, SOFC Slot dataset by Friedrich et al. (2020)\n45\nand Matscholar dataset by Weston et al. (2019)9\nb. Glass vs. Non-Glass classiﬁcation of paper abstracts10\nc. Relation Classi ﬁcation on MSPT corpus46\nThe present work, thus, bridges the gap in the availability of a\nmaterials domain language model, allowing researchers to\nautomate information extraction, knowledge graph completion,\nand other downstream tasks and hence accelerate the discovery\nof materials. We have hosted the MatSciBERT pre-trained weights\nat https://huggingface.co/m3rg-iitd/matscibert and codes for pre-\ntraining and ﬁnetuning on downstream tasks at https://github.\ncom/M3RG-IITD/MatSciBERT. Also, the codes with ﬁnetuned\nmodels for the downstream tasks are available athttps://doi.org/\n10.5281/zenodo.6413296.\nRESULTS AND DISCUSSION\nDataset\nTextual datasets are an integral part of the training of an LM. There\nexist many general-purpose corpora like BookCorpus\n26 and\nEnglishWikipedia, and domain-speci ﬁc corpora like biomedical\ncorpus39, and clinical database41, to name a few. However, none of\nthese corpora is suitable for the materials domain. Therefore, with\nthe aim of providing a materials speciﬁc LM, we ﬁrst create a\ncorpus spanning four important materials science families of\ninorganic glasses, metallic glasses, alloys, and cement and\nconcrete. It should be noted that although these broad categories\nare mentioned, several other categories of materials, including\ntwo-dimensional materials, were also present in the corpus.\nSpeciﬁcally, we have selected ~150 K papers out of ~1 M papers\ndownloaded from the Elsevier Science Direct Database. The steps\nto create the corpus are provided in the Methods section. The\ndetails about the number of papers and words for each family are\ngiven in Supplementary Table 1. We have also provided the list of\nDOIs and PIIs of the papers used to pre-train MatSciBERT in the\nGitHub repository for this work.\nThe materials science corpus developed for this work has\n~285 M words, which is nearly 9% of the number of words used to\npre-train SciBERT (3.17B words) and BERT (3.3B words). Since we\ncontinue pre-training SciBERT, MatSciBERT is effectively trained on\na corpus consisting of 3.17+ 0.28 = 3.45B words. From Supple-\nmentary Table 1, one can observe that 40% of the words are from\nresearch papers related to inorganic glasses and ceramics, and\n20% each from bulk metallic glasses (BMG), alloys, and cement.\nAlthough the number of research papers for “cement and\nconcrete” is more than “inorganic glasses and ceramics ”, the\nlatter has higher words. This is because of the presence of a\ngreater number of full-text documents retrieved associated with\nthe latter category. The Supplementary Table 2 represents the\nword count of important strings relevant to theﬁeld of materials\nscience. It should be noted that the corpus encompasses the\nimportant ﬁelds of thermoelectric, nanomaterials, polymers, and\nbiomaterials. Also, note that the corpora used for training the\nlanguage model consists of both experimental and computational\nworks as both these approaches play a crucial role in under-\nstanding material response. The average paper length for this\ncorpus is ~1848 words, which is two-thirds of the average paper\nlength of 2769 words for the SciBERT corpus. The lower average\npaper length can be attributed to two things: (a) In general,\nmaterials science papers are shorter than biomedical papers. We\nveriﬁed this by computing the average paper length of full-text\nmaterials science papers. The number came out to be 2366. (b)\nThere are papers without full text also in our corpus. In that case,\nwe have used the abstracts of such papers to arrive at theﬁnal\ncorpus.\nPre-training of MatSciBERT\nFor MatSciBERT pre-training, we follow the domain adaptive pre-\ntraining proposed by Gururangan et al. (2020). In this work,\nauthors continued pre-training of the initial LM on corpus of\ndomain-speciﬁc text\n20. They observed a signiﬁcant improvement\nin the performance on domain-speciﬁc downstream tasks for all\nthe four domains despite the overlap between initial LM\nvocabulary and domain-speci ﬁc vocabulary being less than\n54.1%. BioBERT\n19 and FinBERT22 were also developed using the\nsimilar approach where the vanilla BERT model was further pre-\ntrained on domain-speciﬁc text, and tokenization is done using\nthe original BERT vocabulary. We initialize MatSciBERT weights\nwith that of some suitable LM and then pre-train it on MSC. To\ndetermine the appropriate initial weights for MatSciBERT, we\ntrained an uncased wordpiece\n47 vocabulary based on the MSC\nusing the tokenizers library48. The overlap of MSC vocabulary is\n53.64% with the uncased SciBERT21 vocabulary and 38.90% with\nthe uncased BERT vocabulary. Because of the larger overlap with\nthe vocabulary of SciBERT, we tokenize our corpus using the\nSciBERT vocabulary and initialize the MatSciBERT weights with that\nof SciBERT as made publicly available by Beltagy et al. (2019)\n21.I ti s\nworth mentioning that a materials science domain-speci ﬁc\nvocabulary would likely represent the corpus with a lesser number\nof wordpieces and potentially lead to a better language model.\nFor e.g., “yttria-stabilized zirconia” is tokenized as [“yt”, “##tri”,\n“##a”, “-”, “stabilized”, “zircon”, “##ia”] by the SciBERT vocabulary,\nwhereas a domain-speciﬁc tokenization might have resulted in\n[“yttria”, “-”, “stabilized”, “zirconia”]. However, using a domain-\nspeciﬁc tokenizer does not allow the use of SciBERT weights and\ntakes advantage of the scientiﬁc knowledge already learned by\nSciBERT. Further, using the SciBERT vocabulary for the materials\ndomain is not necessarily detrimental since the deep neural\nlanguage models have the capacity to learn repeating patterns\nthat represent new words using the existing tokenizer. For\ninstance, when the wordpieces “yt”, “##tri”, and “##a” occur\nconsecutively, SciBERT indeed recognizes that some material\nis being discussed, as demonstrated in the downstream tasks.\nFig. 1 Methodology for training MatSciBERT. We create the\nMaterials Science Corpus (MSC) through query search followed by\nselection of relevant research papers. MatSciBERT, pre-trained on\nMSC, is evaluated on various downstream tasks.\nT. Gupta et al.\n2\nnpj Computational Materials (2022)   102 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\n1234567890():,;\nThis is also why most domain-speci ﬁc BERT-based LMs like\nFinBERT22, BioBERT19, and ClinicalBERT40 extend the pre-training\ninstead of using domain-speci ﬁc tokenizers and learning from\nscratch.\nThe details of the pre-training procedure are provided in the\nMethods section. The pre-training was performed for 360 h, after\nwhich the model achieved a ﬁnal perplexity of 2.998 on the\nvalidation set (see Supplementary Fig. 1a). Although not directly\ncomparable due to different vocabulary and validation corpus,\nBERT\n25, and RoBERTa49 authors report perplexities as 3.99 and\n3.68, respectively, which are in the same range. We also provide\ngraphs for other evaluation metrics like MLM loss and MLM\naccuracy in Supplementary Fig. 1b, c. Theﬁnal pre-trained LM was\nthen used to evaluate different materials science domain-speciﬁc\ndownstream tasks, details of which are described in the\nsubsequent sections. The performance of the LM on the down-\nstream tasks was compared with that of SciBERT, BERT, and other\nbaseline models to evaluate the effectiveness of MatSciBERT to\nlearn the materials’speciﬁc information.\nIn order to understand the effect of pre-training on the model\nperformance, a materials domain-speciﬁc downstream task, NER\non SOFC-slot, was performed using the model at regular intervals\nof pre-training. To this extent, the pre-trained model was\nﬁnetuned on the training set of the SOFC-slot dataset. The choice\nof the SOFC-slot dataset was based on the fact that the dataset\nwas comprised of ﬁne-grained materials-speci ﬁc information.\nThus, this dataset is appropriate to distinguish the performance\nof SciBERT from the materials-aware LMs. The performance of\nthese ﬁnetuned models was evaluated on the test set. LM-CRF\narchitecture was used for the analysis since LM-CRF consistently\ngives the best performance for the downstream task, as shown\nlater in this work. The macro-F1 averages across three seeds\nexhibited an increasing trend (see Supplementary Fig. 2a),\nsuggesting the importance of training for longer durations. We\nalso show a similar graph for the abstract classi ﬁcation task\n(Supplementary Fig. 2b).\nDownstream tasks\nHere, we evaluate MatSciBERT on three materials science speciﬁc\ndownstream tasks namely, Named Entity Recognition (NER),\nRelation Classiﬁcation, and Paper Abstract Classiﬁcation.\nWe now present the results on the three materials science NER\ndatasets as described in the Methods section. To the best of our\nknowledge, the best Macro-F1 on solid oxide fuel cells (SOFC) and\nSOFC-Slot datasets is 81.50% and 62.60%, respectively, as reported\nby Friedrich et al. (2020), who introduced the dataset\n44. We run\nthe experiments on the same train-validation-test splits as done\nby Friedrich et al. (2020) for a fair comparison of results. Moreover,\nsince the authors reported results averaged over 17 entities (the\nextra entity is “Thickness”) for the SOFC-Slot dataset, we also\nreport the results taking the‘Thickness’entity into account.\nTable 1 shows the Macro-F1 scores for the NER task on the\nSOFC-Slot and SOFC datasets by MatSciBERT, SciBERT, and BERT.\nWe observe that LM-CRF always performs better than LM-Linear.\nThis can be attributed to the fact that the CRF layer can model the\nBIO tags accurately. Also, all SciBERT architectures perform better\nthan the corresponding BERT architecture. We obtained an\nimprovement of ~6.3 Macro F1 and ~3.2 Micro F1 (see\nSupplementary Table 3) on the SOFC-Slot test set for MatSciBERT\nvs. SciBERT while using the LM-CRF architecture. For the SOFC test\ndataset, MatSciBERT-BiLSTM-CRF performs better than SciBERT-\nBiLSTM-CRF by ~2.1 Macro F1 and ~2.1 Micro F1. Similar\nimprovements can be seen for other architectures as well. These\nMatSciBERT results also surpass the current best results on SOFC-\nSlot and SOFC datasets by ~3.35 and ~0.9 Macro-F1, respectively.\nIt is worth noting that the SOFC-slot dataset consists of 17 entity\ntypes and hence has moreﬁne-grained information regarding the\nmaterials. On the other hand, SOFC has only four entity types\nrepresenting coarse-grained information. We notice that the\nperformance of MatSciBERT on SOFC-slot is signiﬁcantly better\nthan that of SciBERT. To further evaluate this aspect, we analyzed\nthe F1-score of both SciBERT and MatSciBERT on all the 17 entity\ntypes of the SOFC-slot data individually, as shown in Fig. 2.\nInterestingly, we observe that for all the materials related entity\ntypes, namely anode material, cathode material, electrolyte\nmaterial, interlayer material, and support material, MatSciBERT\nperforms better than SciBERT. In addition, for materials related\nproperties such as open circuit voltage and degradation rate,\nMatSciBERT is able to signi ﬁcantly outperform SciBERT. This\nsuggests that MatSciBERT is indeed able to capitalize on the\nadditional information learned from the MSC to deliver better\nperformance on complex problems speci ﬁc to the materials\ndomain.\nNow, we present the results for the Matscholar dataset\n9 in Table2.\nFor this dataset too, MatSciBERT outperforms SciBERT, BERT as well\nas the current best results, as can be seen in the case of LM-CRF\narchitecture. The authors obtained Macro-F1 of 85.41% on the\nvalidation set and 85.10% on the test set, and Micro-F1 of 87.09%\nand 87.04% (see Supplementary Table 4). We observe that our\nTable 1. Macro-F1 scores on the test set for SOFC-Slot and SOFC datasets averaged over three seeds andﬁve cross-validation splits.\nArchitecture LM = MatSciBERT LM = SciBERT LM = BERT SOTA\nSOFC-Slot dataset\nLM-Linear 63.82 ± 2.53\n(67.53 ± 4.23)\n58.64 ± 1.49\n(64.58 ± 3.73)\n57.06 ± 2.86\n(61.68 ± 5.23)\n62.6 (67.8 ± 12.9)\nLM-CRF 65.35 ± 2.73\n(70.07 ± 3.36)\n59.07 ± 2.85\n(68.31 ± 2.88)\n58.26 ± 1.73\n(65.38 ± 3.96)\nLM-BiLSTM-CRF 65.95 ± 2.69\n(69.76 ± 3.72)\n61.68 ± 1.42\n(68.44 ± 3.15)\n55.44 ± 1.97\n(65.36 ± 3.68)\nSOFC dataset\nLM-Linear 82.28 ± 1.11\n(81.60 ± 2.63)\n79.91 ± 1.20\n(80.91 ± 2.37)\n77.08 ± 1.75\n(79.61 ± 3.01)\n81.5 (81.7 ± 4.2)\nLM-CRF 82.39 ± 1.23\n(82.61 ± 2.34)\n81.07 ± 0.93\n(82.04 ± 2.36)\n78.93 ± 1.62\n(81.26 ± 2.87)\nLM-BiLSTM-CRF 82.24 ± 1.12\n(82.61 ± 2.77)\n80.12 ± 1.00\n(81.92 ± 2.27)\n78.15 ± 1.55\n(80.94 ± 2.72)\nValues in the parenthesis show the results on the validation set.\nT. Gupta et al.\n3\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2022)   102 \nbest model MatSciBERT-CRF has Macro-F1 values of 88.66% and\n86.38%, both better than the existing state of the art.\nIn order to demonstrate the performance of MatSciBERT, we\ndemonstrate an example from the validation set of the dataset in\nSupplementary Figs. 3 and 4. The overall superior performance of\nMatSciBERT is evident from Table2.\nTable 3 shows the results for the Relation Classiﬁcation task\nperformed on the Materials Synthesis Procedures dataset\n46.W e\nalso compare the results with two recent baseline models,\nMaxPool and MaxAtt\n50, details of which can be found in the\nMethods section. Even in this task, we observe that MatSciBERT\nperforms better than SciBERT, BERT, and baseline models\nconsistently, although with a lower margin.\nIn Paper Abstract Classiﬁcation downstream task, we consider\nthe ability of LMs to classify a manuscript into glass vs. non-glass\ntopics based on an in-house dataset\n10. This is a binary\nclassiﬁcation problem, with the input being the abstract of a\nmanuscript. Here too, we use the same baseline models MaxPool\nand MaxAtt\n50. Table 4 shows the comparison of accuracies\nachieved by MatSciBERT, SciBERT, BERT, and baselines. It can be\nclearly seen that MatSciBERT outperforms SciBERT by more than\n2.75% accuracy on the test set.\nAltogether, we demonstrate that the MatSciBERT, pre-trained on\na materials science corpus, can perform better than SciBERT for all\nthe downstream tasks such as NER, abstract classiﬁcation, and\nrelation classiﬁcation on materials datasets. These results also\nsuggest that the scientiﬁc literature in the materials domain, on\nwhich MatSciBERT is pre-trained, is signiﬁcantly different from the\ncomputer science and biomedical domains on which SciBERT is\ntrained. Speciﬁcally, each scientiﬁc discipline exhibits signiﬁcant\nvariability in terms of ontology, vocabulary, and domain-speciﬁc\nnotations. Thus, the development of a domain-speciﬁc language\nmodel, even within the scienti ﬁc literature, can signi ﬁcantly\nenhance the performance in downstream tasks related to text\nmining and information extraction from literature.\nApplications in materials domain\nNow, we discuss some of the potential areas of application of\nMatSciBERT in materials science. These areas can range from the\nsimple topic-based classiﬁcation of research papers to discovering\nmaterials or alternate applications for existing materials. We\ndemonstrate some of these applications as follows: (i) Document\nclassiﬁcation: A large number of manuscripts have been published\non materials related topics, and the numbers are increasing\nexponentially. Identifying manuscripts related to a given topic is a\nchallenging task. Traditionally, these tasks are carried out\nemploying approaches such as term frequency-inverse document\nfrequency (TFIDF) or Word2Vec, which is used along with a\nclassiﬁcation algorithm. However, these approaches directly\nvectorize a word and are not context sensitive. For instance, in\nthe phrases “ﬂat glass”, “glass transition temperature”, “tea glass”,\nthe word “glass” is used in a very different sense. MatSciBERT will\nbe able to extract the contextual meaning of the embeddings.\nThus, MatSciBERT will be able to effectively classify the topics\nthereby enabling improved topic classi ﬁcation. This is evident\nfrom the binary classiﬁcation results presented earlier in Table4,\nwhere we observe that the accuracy obtained using MatSciBERT\n(96.22%) was found to be signiﬁcantly higher than the results\nobtained using pooling based BiLSTM models (91.44%). This\napproach can be extended to a larger set of abstracts for the\naccurate classiﬁcation of documents from the literature.\n(ii) Topic modeling: Topic modeling is an unsupervised\napproach of grouping documents belonging to similar topics\ntogether. Traditionally, topic modeling employs algorithms such\nas latent Dirichlet allocation (LDA) along with TF-IDF or Word2Vec\nto cluster documents having the same or semantically similar\nwords together. Note that these approaches rely purely on the\nfrequency of word (in TF-IDF) or the embeddings of the word (in\nWord2Vec) for clustering without taking into account the context.\nThe use of context-aware embeddings as learned in MatSciBERT\ncould signi ﬁcantly enhance the topic modeling task. As a\npreliminary study, we perform topic modeling using MatSciBERT\non an in-house corpus of abstracts on glasses and ceramics. Note\nthat the same corpus was used in an earlier work\n10 for topic\nmodeling using LDA. Speciﬁcally, we obtain the output embed-\ndings of the [CLS] token for each abstract using MatSciBERT.\nFurther, these embeddings were projected into two dimensions\nusing the UMAP algorithm 51 and then clustered using the\nk-means algorithm 52. We then concatenate all the abstracts\nbelonging to the same cluster and calculate the most frequent\nwords for each cluster/topic.\nFig. 2 Comparison of MatSciBERT and SciBERT on validation sets\nof SOFC-Slot dataset.The entity-level F1-score for MatSciBERT and\nScibert models in blue and red color respectively. The bold colored\ntext represents the best model’s score.\nTable 2. Macro-F1 scores on the test set for Matscholar averaged over three seeds.\nArchitecture LM =MatSciBERT LM = SciBERT LM = BERT SOTA\nLM-Linear 85.46 ± 0.13\n(87.83 ± 1.21)\n83.80 ± 0.32\n(86.05 ± 0.55)\n82.10 ± 0.81\n(82.79 ± 0.20)\n85.10 (85.41)\nLM-CRF 86.38 ± 0.49\n(88.66 ± 0.88)\n85.04 ± 0.77\n(88.07 ± 0.96)\n84.07 ± 0.19\n(84.61 ± 0.81)\nLM-BiLSTM-CRF 86.09 ± 0.46\n(89.15 ± 0.57)\n85.66 ± 0.24\n(87.66 ± 0.29)\n83.39 ± 0.20\n(84.07 ± 0.29)\nValues in the parenthesis show the results on the validation set.\nT. Gupta et al.\n4\nnpj Computational Materials (2022)   102 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nThe Supplementary Tables 5 and 6 shows the top ten topics\nobtained using LDA and MatSciBERT, respectively. The top 10\nkeywords associated with each topic are also provided in the\ntable. We observe that the topics and keywords from MatSciBERT-\nbased topic modeling are more coherent than the ones obtained\nfrom LDA. Further, the actual topics associated with the keywords\nare not very apparent from Supplementary Table 5. Speciﬁcally,\nTopic 9 by LDA contains keywords from French, suggesting that\nthe topic represents French publications. Similarly, Topic 5 and\nTopic 3 have several generic keywords that don’t represent a topic\nclearly. On the other hand, the keywords obtained by MatSciBERT\nenable a domain expert to identify the topics well. For instance,\nsome of the topics identiﬁed based on the keywords by three\nselected domain experts are dissolution of silicates (9), oxide thin\nﬁlms synthesis and their properties (8, 6), materials for energy (0),\nelectrical behavior of ceramics (1), and luminescence studies (5).\nDespite their efforts, the same three domain experts were unable\nto identify coherent topics based on the keywords provided by\nLDA. Altogether, MatSciBERT can be used for topic modeling,\nthereby providing a broad overview of the topics covered in the\nliterature considered.\n(iii) Information extraction from images: Images hold a large\namount of information regarding the structure and properties of\nmaterials. A proxy to identify relevant images would be to go\nthrough the captions of all the images. However, each caption\nmay contain multiple entities, and identifying the relevant\nkeywords might be a challenging task. To this extent, MatSciBERT\nﬁnetuned on NER can be an extremely useful tool for extracting\ninformation from ﬁgure captions.\nHere, we extracted entities from the ﬁgure captions used by\nVenugopal et al. (2021)\n10 using MatSciBERT ﬁnetuned on the\nMatscholar NER dataset. Speciﬁcally, entities were extracted from\n~110,000 image captions on topics related to inorganic glasses.\nUsing MatSciBERT, we obtained 87,318 entities as DSC (sample\ndescriptor), 10,633 entities under APL (application), 145,324 as\nMAT (inorganic material), 76,898 as PRO (material property),\n73,241 as CMT (characterization method), 33,426 as SMT (synthesis\nmethod), and 2,676 as SPL (symmetry/phase label). Figure3 shows\nthe top 10 extracted entities under the seven categories proposed\nin the Matscholar dataset. The top entities associated with each of\nthe categories are coating (application), XRD (characterization),\nglass (sample descriptor, inorganic material), composition (mate-\nrial property), heat (synthesis method), and hexagonal (symmetry/\nphase). Further details associated with each category can also be\nobtained from these named entities. It should be noted that each\ncaption may be associated with more than one entity. These\nentities can then be used to obtain relevant images for speciﬁc\nqueries such as“XRD measurements of glasses used for coating”\nor “emission spectra of doped glasses ”,o r “SEM images of\nbioglasses with Ag”, to name a few.\nFurther, Fig. 4 shows some of the selected captions from the\nimage captions along with the corresponding manual annotation\nby Venugopal et al. (2021)\n10. The task of assigning tags to each\ncaption was carried out by human experts. Note that only one\nword was assigned per image caption in the previous work. Using\nthe MatSciBERT NER model, we show that multiple entities are\nextracted for the selectedﬁve captions. This illustrates the large\namount of information that can be captured using the LM\nproposed in this work.\n(iv) Materials caption graph: In addition to the queries as\nmentioned earlier, graph representations can provide in-depth\ninsights into the information spread in ﬁgure captions. For\ninstance, questions such as“which synthesis and characterization\nmethods are commonly used for a speciﬁc material?”, “what are\nthe methods for measuring a speciﬁc property?” can be easily\nanswered using knowledge graphs. Here, we demonstrate how\nthe information in ﬁgure captions can be represented using\nmaterials caption graphs (MCG). To this extent, weﬁrst randomly\nselect 10,000 ﬁgure captions from glass-related publications.\nFurther, we extract the entities and their types from theﬁgure\ncaptions using the MatSciBERT ﬁnetuned on Matscholar NER\ndataset. For each caption, we create a fully connected graph by\nconnecting all the entities present in that caption. These graphs\nare then joined together to form a large MCG. We demonstrate\nsome insights gained from the MCGs below.\nFigure 5 shows two subsets of graphs extracted from the MCGs.\nIn Fig. 5a, we identiﬁed two entities that are two-hop neighbors,\nnamely, T\ng and anneal. Note that these entities do not share an\nedge. In other words, these two entities are not found\nsimultaneously in any given caption. We then identi ﬁed the\nintersection of all the one-hop neighbors of both the nodes and\nplotted the graph as shown in Fig.5a. The thickness of the edge\nrepresents the strength of the connection in terms of the number\nof occurrences. We observe that there are four common one-hop\nneighbors for T\ng and anneal, namely, XRD, doped, glass, and\namorphous. This means that these four entities occur in captions\nalong with T\ng and anneal, even though these two entities are not\ndirectly connected in the captions used for generating the graph.\nFigure 5a suggests that T\ng is related to glass, amorphous, and\ndoped materials and that these materials can be synthesized by\nannealing. Similarly, the structures obtained by annealing can be\ncharacterized by XRD. From these results, we can also infer that T\ng\nis affected by annealing, which agrees with the conventional\nknowledge in glass science.\nSimilarly, Fig. 5b shows all the entities connected to the node\nXRD. To this extent, we select all the captions having XRD as CMT.\nTable 3. Test set results for Materials Synthesis Procedures dataset averaged over three seeds.\nMatSciBERT SciBERT BERT MaxPool MaxAtt\nMacro-F1 89.02 ± 0.27(88.31 ± 0.14) 87.22 ± 0.58(87.21 ± 0.17) 85.40 ± 1.45(85.95 ± 0.78) 81.19 ± 1.54(80.93 ± 0.71) 80.39 ± 0.85(81.53 ± 2.23)\nMicro-F1 91.94 ± 0.20(91.50 ± 0.20) 91.04 ± 0.32)(91.03 ± 0.08 90.16 ± 0.69(90.44 ± 0.54) 86.81 ± 1.55(86.68 ± 0.84) 87.16 ± 0.60(87.62 ± 1.34)\nValues in the parenthesis represent the results on the validation set.\nTable 4. Test set results for glass vs. non-glass dataset averaged over three seeds.\nMatSciBERT SciBERT BERT MaxPool MaxAtt\nAccuracy 96.22 ± 0.16\n(95.33 ± 0.27)\n93.44 ± 0.57\n(94.00 ± 0.00)\n93.89 ± 0.68\n(93.33 ± 0.98)\n91.44 ± 0.31\n(92.22 ± 0.56)\n91.44 ± 0.68\n(91.22 ± 0.16)\nValues in the parenthesis represent the results on the validation set.\nT. Gupta et al.\n5\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2022)   102 \nAfter obtaining all the entities in those captions, we randomly\nsample 20 pairs and then plotted them as shown in Fig.5b. Note\nthat the number of edges is 18 and the number of nodes is 19\nbecause of one pair being (XRD, XRD) and two similar pairs (XRD,\nglass). The node color represents the entity type, and the edge\nwidth represents the frequency of the pair in the entire database\nof entities extracted from the captions where “XRD” is present.\nUsing the graph, we can obtain the following information:\n1. XRD is used as a characterization method for different\nmaterial descriptors like glass, doped materials, nanoﬁbers,\nand ﬁlms.\n2. Materials prepared using synthesis methods (SMT) like\naging, heat-treatment, and annealing are also characterized\nusing XRD.\n3. While studying the property (PRO) glass transition tempera-\nture (Tg), XRD was also performed to characterize the\nsamples.\n4. In the case of silica glass ceramics (SGCs), phosphor, and\nphosphor-in-glass (PiG) applications (APL), XRD is used\nas CMT.\n5. For different materials like ZnO, glasses, CsPBr3, yttria\npartially stabilized zirconia (YPSZ), XRD is a major CMT\nwhich is evident from the thicker edge widths.\nNote this information covers a wide range of materials and\napplications in materials literature. Similar graphs can be\ngenerated for different entities and entity types using the MCG\nto gain insights into the materials literature.\n(v) Other applications such as relation classiﬁcation: MatSciBERT\ncan also be applied for addressing several other issues such as\nrelation classi ﬁcation and question answering. The relation\nclassiﬁcation task demonstrated in the present manuscript can\nprovide key information regarding several aspects in materials\nscience which are followed in a sequence. These would include\nsynthesis and testing protocols, and measurement sequences. This\ninformation can be further used to discover an optimal pathway\nfor material synthesis. In addition, such approaches can also be\nused to obtain the effect of different testing and environmental\nconditions, along with the relevant parameters, on the measured\nproperty of materials. This could be especially important for those\nproperties such as hardness or fracture toughness, which are\nhighly sensitive to sample preparation protocols, testing condi-\ntions, and the equipment used. Thus, the LM can enable the\nextraction of information regarding synthesis and testing condi-\ntions that are otherwise buried in the text.\nAt this juncture, it is worth noting that there are very few\nannotated datasets available for the material corpus. This\ncontrasts with the biomedical corpus, where several annotated\ndatasets are available for different downstream tasks such as\nrelation extraction, question-answering, and NER. While the\ndevelopment of materials science speciﬁc language model can\nsigniﬁcantly accelerate the NLP-related applications in materials,\nFig. 3 Top-10 entities for various categories. aAPL Application,b CMT Characterization method,c DSC Sample descriptor,d MAT Inorganic\nmaterial, e PRO Material Property, andf SMT Synthesis method.\nT. Gupta et al.\n6\nnpj Computational Materials (2022)   102 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nFig. 4 Comparison of MatSciBERT based NER tagging with manually assigned labels.MatSciBERT-based NER model is able to extract\nmultiple entities as compared to single manual label for each caption.\nFig. 5 Materials caption graph. aConnecting two unconnected entities,b exploring entities related to characterization method“XRD”.\nT. Gupta et al.\n7\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2022)   102 \nthe development of annotated datasets is equally important for\naccelerating materials discovery.\nIn conclusion, we developed a materials-aware language model,\nnamely, MatSciBERT, that is trained on materials science corpus\nderived from journals. The LM, trained from the initial weights of\nSciBERT, exploits the knowledge on computer science and\nbiomedical corpora (on which the original SciBERT was pre-\ntrained) along with the additional information from the materials\ndomain. We test the performance of MatSciBERT on several\ndownstream tasks such as document classi ﬁcation, NER, and\nrelation classiﬁcation. We demonstrate that MatSciBERT exhibits\nsuperior performance on all the datasets tested in comparison to\nSciBERT. Finally, we discuss some of the applications through\nwhich MatSciBERT can enable accelerated information extraction\nfrom the materials science text corpora. To enable accelerated text\nmining and information extraction, the pre-trained weights of\nMatSciBERT are made publicly available athttps://huggingface.co/\nm3rg-iitd/matscibert.\nMETHODS\nDataset collection and preparation\nIn the training of an LM in a generalizable way, a considerable amount of\ndataset is required. For example, BERT25 was pre-trained on BookCorpus26\nand English Wikipedia, containing a total of 3.3 billion words. SciBERT21,a n\nLM trained on scienti ﬁc literature, was pre-trained using a corpus\nconsisting of 82% papers from the broad biomedical domain and 18%\npapers from the computer science domain. However, we note that none of\nthese LMs includes text related to the materials domain. Here, we consider\nmaterials science literature from four broad categories, namely, inorganic\nglasses and ceramics, metallic glasses, cement and concrete, and alloys, to\ncover the materials domain in a representative fashion.\nThe ﬁrst step in retrieving the research papers is to query search from\nthe Crossref metadata database\n53. This resulted in a list of more than 1 M\narticles. Although Crossref gives the search results from different journals\nand publishers, we downloaded papers only from the Elsevier Science\nDirect database using their sanctioned API\n54. Note that the Elsevier API\nreturns the research articles in XML format; hence, we wrote a custom XML\nparser for extracting the text. Occasionally, there were papers having only\nabstract and not full text depending upon the journal and publication\ndate. Since the abstracts contain concise information about the problem\nstatement being discussed in the paper and what the research\ncontributions are, therefore, we have included them in our corpus.\nTherefore, we have included all the sections of the paper when available\nand abstracts otherwise. For glass science-related papers, the details are\ngiven in our previous work\n10. For concrete and alloys, weﬁrst downloaded\nmany research papers for each material category using several queries\nsuch as “cement”, “interfacial transition zone”, “magnesium alloy”, and\n“magnesium alloy composite materials”, to name a few.\nSince all the downloaded papers did not belong to a particular class of\nmaterials, we manually annotated 500 papers based on their abstracts,\nwhether they were relevant to the ﬁeld of interest or not. Further, we\nﬁnetuned SciBERT classiﬁers21,55, one for each category of material, on\nthese labeled abstracts for identifying relevant papers among the\ndownloaded 1 M articles. We consider these selected papers from each\ncategory of materials for training the language model. A detailed\ndescription of the Materials Science Corpus (MSC) is given in the Results\nand Discussion section of the paper. Finally, we divided this corpus into\ntraining and validation, with 85% being used to train the language model\nand the remaining 15% as validation to assess the model’s performance on\nunseen text.\nNote that the texts in the scientiﬁc literature may have several symbols,\nincluding some random characters. Sometimes the same semantic symbol\nhas many Unicode surface forms. To address these anomalies, we also\nperformed Unicode normalization of MSC to:\na. get rid of random Unicode characters like\n,,,a n d\nb. map different Unicode characters having similar meaning and\nappearance to either a single standard character or a sequence of\nstandard characters.\nFor example, % gets mapped to %, > to > ,⋙to ⋙, = and = to = ,¾\nto 3/4, to name a few. First, we normalized the corpus using BertNormalizer\nfrom the tokenizers library by Hugging Face\n56,57. Next, we created a list\ncontaining mappings of the Unicode characters appearing in the MSC. We\nmapped random characters to space so that they do not interfere during\npre-training. It’s important to note that we also perform this normalization\nstep on every dataset before passing it through the MatSciBERT tokenizer.\nPre-training of MatSciBERT\nWe pre-train MatSciBERT on MSC as detailed in the last sub-section. Pre-\ntraining LM from scratch requires signiﬁcant computational power and a\nlarge dataset. To address this issue, we initialize MatSciBERT with weights\nfrom SciBERT and perform tokenization using the SciBERT uncased\nvocabulary. This has the additional advantage that existing models relying\non SciBERT, which are pre-trained on biomedical and computer science\ncorpora, can be interchangeably used with MatSciBERT. Further, the\nvocabulary existing in the scientiﬁc literature as constructed by SciBERT\ncan be used to reasonably represent the new words in the materials\ndomain.\nTo pre-train MatSciBERT, we employ the optimized training recipe,\nRoBERTa\n49, suggested by Liu et al. (2019). This approach has been shown\nto signiﬁcantly improve the performance of the original BERT. Speciﬁcally,\nthe following simple modi ﬁcations were adopted for MatSciBERT pre-\ntraining:\n1. Dynamic whole word masking: It involves masking at the word level\ninstead of masking at the wordpiece level, as discussed in the latest\nrelease of the BERT pre-training code by Google58. Each time a\nsequence is sampled, we randomly mask 15% of the words and let\nthe model predict each masked wordpiece token independently.\n2. Removing the NSP loss from the training objective: BERT was pre-\ntrained using two unsupervised tasks: Masked-LM and Next-\nSentence Prediction (NSP). NSP takes as input a pair of sentences\nand predicts whether the two sentences follow each other or not.\nRoBERTa authors claim that removing the NSP loss matches or\nslightly improves downstream task performance.\n3. Training on full-length sequences: BERT was pre-trained with a\nsequence length of 128 for 90% of the steps and with a sequence of\nthe length of 512 for the remaining 10% steps. RoBERTa authors\nobtained better performance by training only with full-length\nsequences. Here, input sequences are allowed to contain segments\nof more than one document and [SEP] token is used to separate the\ndocuments within an input sequence.\n4. Using larger batch sizes: Authors also found that training with larger\nmini-batches improved the pre-training loss and increased the end-\ntask performance.\nFollowing these modiﬁcations, we pre-train MatSciBERT on the MSC with\na maximum sequence length of 512 tokens forﬁfteen days on 2 NVIDIA\nV100 32GB GPUs with a batch size of 256 sequences. We use the AdamW\noptimizer withβ\n1 =0.9, β2 =0.98, ε =1e–6, weight decay= 1e−2 and linear\ndecay schedule for learning rate with warmup ratio = 4.8% and peak\nlearning rate = 1e−4. Pre-training code is written using PyTorch 59 and\nTransformers57 library and is available at our GitHub repository for this\nwork https://github.com/M3RG-IITD/MatSciBERT.\nDownstream tasks\nOnce the LM is pre-trained, we ﬁnetune it on various supervised\ndownstream tasks. Pre-trained LM is augmented with a task-speci ﬁc\noutput layer. Finetuning is done to adapt the model to speciﬁc tasks as\nwell as to learn the task-speciﬁc randomly initialized weights present in the\noutput layer. Finetuning is done on all the parameters end-to-end. We\nevaluate the performance of MatSciBERT on the following three down-\nstream NLP tasks:\n1. Named Entity Recognition (NER) involves identifying domain-\nspeciﬁc named entities in a given sentence. Entities are encoded\nusing the BIO scheme to account for multi-token entities\n53. Dataset\nfor the NER task includes various sentences, with each sentence\nbeing split into multiple tokens. Gold labels are provided for each\ntoken. More formally, LetE = {e\n1, … ek} be the set ofk entity types\nfor a given dataset. If [x1, … xn] are tokens of a sentence and [y1, …\nyn] are labels for these tokens, then eachyi ∈ L ={B-e1, I-e1, … B-ek,\nI-ek, O}. Here, B-ei and I-ei represent the beginning and inside of\nentity ei.\n2. Input for the Relation Classiﬁcation60 task consists of a sentence and\nan ordered pair of entity spans in that sentence. Output is a label\nT. Gupta et al.\n8\nnpj Computational Materials (2022)   102 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\ndenoting the directed relationship between the two entities. The\ntwo entity spans can be represented as s1 = (i, j) and s2 = (k, l),\nwhere i and j denote the starting and ending index of theﬁrst entity\nand similarly k and l denote the starting and ending index of the\nsecond entity in the input statement. Here,i ≤ j, k ≤ l, and (j < k or l <\ni). The last constraint guarantees that the two entities do not overlap\nwith each other. The output label belongs toL, whereL is aﬁxed set\nof relation types. An example of a sentence from the task is given in\nFig. 6. The task is to predict the labels like“Participant_Material”,\n“Apparatus_Of” given the sentence and pair of entities as input.\n3. In the Paper Abstract Classiﬁcation task, we are given an abstract of\na research paper, and we have to classify whether the abstract is\nrelevant to a givenﬁeld or not.\nDatasets\nWe use the following three Materials Science-based NER datasets to\nevaluate the performance of MatSciBERT against SciBERT:\n1. Matscholar NER dataset 9 by Weston et al. (2019): This dataset is\npublicly available and contains seven different entity types. Training,\nvalidation, and test sets consist of 440, 511, and 546 sentences,\nrespectively. Entity types present in this dataset are inorganic\nmaterial (MAT), symmetry/phase label (SPL), sample descriptor\n(DSC), material property (PRO), material application (APL), synthesis\nmethod (SMT), and characterization method (CMT).\n2. Solid Oxide Fuel Cells– Entity Mention Extraction (SOFC) dataset by\nFriedrich et al. (2020)\n45: This dataset consists of 45 open-access\nscholarly articles annotated by domain experts. Four different entity\ntypes have been annotated by the authors, namely Material,\nExperiment, Value, and Device. There are 611, 92, and 173 sentences\nin the training, validation, and test sets, respectively.\n3. Solid Oxide Fuel Cells – Slot Filling (SOFC-Slot) dataset by Friedrich\net al. (2020)\n45: This is the same as the above dataset except that\nentity types are more ﬁne-grained. There are 16 different entity\ntypes, namely Anode Material, Cathode Material, Conductivity,\nCurrent Density, Degradation Rate, Device, Electrolyte Material, Fuel\nUsed, Interlayer Material, Open Circuit Voltage, Power Density,\nResistance, Support Material, Time of Operation, Voltage, and\nWorking Temperature. Two additional entity types: Experiment\nEvoking Word and Thickness, are used for training the models.\nFor relation classiﬁcation, we use the Materials Synthesis Procedures\ndataset by Mysore et al. (2019)\n46. This dataset consists of 230 synthesis\nprocedures annotated as graphs where nodes represent the participants of\nsynthesis steps, and edges specify the relationships between the nodes.\nThe average length of a synthesis procedure is nine sentences, and 26\ntokens are present in each sentence on average. The dataset consists of 16\nrelation labels. The relation labels have been divided into three categories\nby the authors:\na. Operation-Argument relations: Recipe target, Solvent material,\nAtmospheric material, Recipe precursor, Participant material, Appa-\nratus of, Condition of\nb. Non-Operation Entity relations: Descriptor of, Number of, Amount of,\nApparatus-attr-of, Brand of, Core of, Property of, Type of\nc. Operation-Operation relations: Next operation\nThe train, validation, and test set consist of 150, 30, and 50 annotated\nmaterial synthesis procedures, respectively.\nThe dataset for classifying research papers related to glass science or not\non the basis of their abstracts has been taken from Venugopal et al.\n(2021)\n10. The authors have manually labeled 1500 abstracts as glass and\nnon-glass. These abstracts belong to differentﬁelds of glass science like\nbioactive glasses, rare-earth glasses, glass ceramics, thin-ﬁlm studies, and\noptical, dielectric, and thermal properties of glasses, to name a few. We\ndivide the abstracts into a train-validation-test split of 3:1:1.\nModeling\nFor NER task, we use the BERT contextual output embedding of theﬁrst\nwordpiece of every token to classify the tokens among |L | classes. We\nmodel the NER task using three architectures: LM-Linear, LM-CRF, and LM-\nBiLSTM-CRF. Here, LM can be replaced by any BERT-based transformer\nmodel. We take LM to be BERT, SciBERT and MatSciBERT in this work.\n1. LM-Linear: The output embedding of the wordpieces are passed\nthrough a linear layer with softmax activation. We use the BERT\nToken Classiﬁer implementation of transformers library\n57.\n2. LM-CRF: We replace the ﬁnal softmax activation of the LM-Linear\narchitecture with a CRF layer61 so that the model can learn to label\nthe tokens belonging to the same entity mentioned and also learn\nthe transition scores between different entity types. We use the CRF\nimplementation of PyTorch-CRF library\n62.\n3. LM-BiLSTM-CRF: Bidirectional Long Short-Term Memory63 is added\nin between the LM and CRF layer. BERT embeddings of all the\nwordpieces are passed through a stacked BiLSTM. The output of\nBiLSTM is ﬁnally fed to the CRF layer to make predictions.\nIn case of Relation Classiﬁcation task, we use the Entity Markers-Entity\nStart architecture\n60 proposed by Soares et al. (2019) for modeling. Here, we\nsurround the entity spans within the sentence with some special\nwordpieces. We wrap the ﬁrst and second entities with [E1], [\\E1] and\n[E2], [\\E2] respectively. We concatenate the output embeddings of [E1] and\n[E2] and then pass it through a linear layer with softmax activation. We use\nthe standard cross-entropy loss function for the training of the linear layer\nand ﬁnetuning of the language model.\nFor the baseline, we use two recent models, MaxPool and MaxAtt,\nproposed by Maini et al. (2020)\n50. In this approach too, the pair of entities\nare wrapped with the same special tokens. Then glove embeddings18 of\nwords in the input sentence are passed through a BiLSTM, an aggregation\nmechanism (different for MaxPool and MaxAtt) over words, and a linear\nlayer with softmax activation.\nIn Paper Abstract Classiﬁcation task, we use the output embedding of\nthe CLS token to encode the entire text/abstract. We pass this embedding\nthrough a simple classiﬁer to make predictions. We use the BERT Sentence\nClassiﬁer implementation of the transformers library\n57. For the baseline, we\nuse a similar approach as relation classiﬁcation except that there is no pair\nof input entities.\nHyperparameters\nWe use a linear decay schedule for the learning rate with a warmup ratio of\n0.1. To ensure sufﬁcient training of randomly initialized non-BERT layers,\nwe set different learning rates for the BERT part and non-BERT part. We set\nthe peak learning rate of the non-BERT part to 3e-4 and choose the peak\nlearning rate of the BERT part from [2e\n−5,3 e−5,5 e−5], whichever results in\na maximum validation score averaged across three seeds. We use a batch\nsize of 16 and an AdamW optimizer for all the architectures. For LM-\nBiLSTM-CRF architecture, we use a 2-layer stacked BiLSTM with a hidden\ndimension of 300 and dropout of 0.2 in between the layers. We perform\nﬁnetuning for 15, 20, and 40 epochs for Matscholar, SOFC, and SOFC Slot\ndatasets, respectively, as initial experiments exhibited little or no\nimprovement after the speciﬁed number of epochs. All the weights of\nany given architecture are updated duringﬁnetuning, i.e., we do not freeze\nany of the weights. We make the code for ﬁnetuning and different\narchitectures publicly available. We refer readers to the code for further\ndetails about the hyperparameters.\nFig. 6 Relation classiﬁcation task.The different entities are enclosed in boxes with their respective labels. The related entities are connected\nusing arrows labeled with the relation.\nT. Gupta et al.\n9\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2022)   102 \nEvaluation metrics\nWe evaluate the NER task based on entity-level exact matches. We use the\nCoNLL evaluation script ( https://github.com/spyysalo/conlleval.py). For\nNER and Relation Classiﬁcation tasks, we use Micro-F1 and Macro-F1 as\nthe primary evaluation metrics. We use accuracy to evaluate the\nperformance of the paper abstract classiﬁcation task.\nDATA AVAILABILITY\nAny data used for the work are available from the corresponding authors upon\nreasonable request. The PIIs and DOIs of the research papers used in this work are\navailable at https://github.com/M3RG-IITD/MatSciBERT/blob/main/pretraining/piis_dois.\ncsv.\nCODE AVAILABILITY\nAll the codes used in the present work are available athttps://github.com/M3RG-IITD/\nMatSciBERT. Also, the codes with ﬁnetuned models for the downstream tasks are\navailable at https://doi.org/10.5281/zenodo.6413296.\nReceived: 29 October 2021; Accepted: 12 April 2022;\nREFERENCES\n1. Science, N. & (US), T. C. Materials genome initiative for global competitiveness.\n(Executive Ofﬁce of the President, National Science and Technology Council,\nhttps://www.mgi.gov/sites/default/ﬁles/documents/materials_genome_initiative-\nﬁnal.pdf, 2011).\n2. Jain, A. et al. Commentary: The Materials Project: A materials genome approach\nto accelerating materials innovation.APL Mater. 1, 011002 (2013).\n3. Zunger, A. Inverse design in search of materials with target functionalities.Nat.\nRev. Chem. 2,1 –16 (2018).\n4. Chen, C. et al. A critical review of machine learning of energy materials.Adv.\nEnergy Mater. 10, 1903242 (2020).\n5. de Pablo, J. J. et al. New frontiers for the materials genome initiative.Npj Comput.\nMater. 5,1 –23 (2019).\n6. Greenaway, R. L. & Jelfs, K. E. Integrating computational and experimental\nworkﬂows for accelerated organic materials discovery.Adv. Mater. 33, 2004831\n(2021).\n7. Ravinder et al. Artiﬁcial intelligence and machine learning in glass science and\ntechnology: 21 challenges for the 21st century.Int. J. Appl. Glass Sci.12, 277–292\n(2021).\n8. Zanotto, E. D. & Coutinho, F. A. B. How many non-crystalline solids can be made\nfrom all the elements of the periodic table?J. Non-Cryst. Solids 347, 285–288\n(2004).\n9. Weston, L. et al. Named entity recognition and normalization applied to large-\nscale information extraction from the materials science literature.J. Chem. Inf.\nModel. 59, 3692–3702 (2019).\n10. Venugopal, V. et al. Looking through glass: Knowledge discovery from materials\nscience literature using natural language processing.Patterns 2, 100290 (2021).\n11. Zaki, M., Jayadeva & Krishnan, N. M. A. Extracting processing and testing para-\nmeters from materials science literature for improved property prediction of\nglasses. Chem. Eng. Process. - Process Intensif . 108607 (2021). https://doi.org/\n10.1016/j.cep.2021.108607.\n12. El-Bousiydy, H. et al. What can text mining tell us about lithium-ion battery\nresearchers’habits? Batter. Supercaps 4, 758–766 (2021).\n13. Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C. & Mercer, R. L. Class-basedn-\ngram models of natural language.Comput. Linguist. 18, 467–480 (1992).\n14. Ando, R. K. & Zhang, T. A framework for learning predictive structures from\nmultiple tasks and unlabeled data.J. Mach. Learn. Res.\n6, 1817–1853 (2005).\n15. Blitzer, J., McDonald, R. & Pereira, F. Domain adaptation with structural corre-\nspondence learning. inProceedings of the 2006 Conference on Empirical Methods\nin Natural Language Processing 120–128 (Association for Computational Lin-\nguistics, 2006).\n16. Mikolov, T., Chen, K., Corrado, G. & Dean, J. Efﬁcient estimation of word repre-\nsentations in vector space. in 1st international conference on learning repre-\nsentations, ICLR 2013, scottsdale, arizona, USA, may 2-4, 2013, workshop track\nproceedings (eds. Bengio, Y. & LeCun, Y.) (2013).\n17. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. & Dean, J. Distributed repre-\nsentations of words and phrases and their compositionality. inAdvances in Neural\nInformation Processing Systemsvol. 26 (Curran Associates, Inc., 2013).\n18. Pennington, J., Socher, R. & Manning, C. GloVe: Global vectors for word repre-\nsentation. in Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) 1532–1543 (Association for Computational Lin-\nguistics, 2014).\n19. Lee, J. et al. BioBERT: a pre-trained biomedical language representation model for\nbiomedical text mining.Bioinformatics 36, 1234–1240 (2020).\n20. Gururangan, S. et al. Don’t stop pretraining: Adapt language models to domains\nand tasks. inProceedings of the 58th annual meeting of the association for com-\nputational linguistics8342–8360 (Association for Computational Linguistics, 2020).\n21. Beltagy, I., Lo, K. & Cohan, A. SciBERT: A pretrained language model for scientiﬁc\ntext. in Proceedings of the 2019 conference on empirical methods in natural lan-\nguage processing and the 9th international joint conference on natural language\nprocessing, EMNLP-IJCNLP 2019, hong kong, china, november 3-7, 2019 (eds. Inui,\nK., Jiang, J., Ng, V. & Wan, X.) 3613–3618 (Association for Computational Lin-\nguistics, 2019).\n22. Araci, D. FinBERT: Financial sentiment analysis with pre-trained language models.\nPreprint at https://arxiv.org/abs/1908.10063 (2019).\n23. Lee, J.-S. & Hsiang, J. Patent classiﬁcation by ﬁne-tuning BERT language model.\nWorld Pat. Inf.61, 101965 (2020).\n24. Manning, C. & Schutze, H.Foundations of Statistical Natural Language Processing.\n(MIT Press, 1999).\n25. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of deep\nbidirectional transformers for language understanding. in NAACL-HLT (1)\n4171–4186 (Association for Computational Linguistics, 2019).\n26. Zhu, Y. et al. Aligning books and movies: towards story-like visual explanations by\nwatching movies and reading books. in 2015 IEEE International Conference on\nComputer Vision (ICCV)19–27 (2015).\n27. Swain, M. C. & Cole, J. M. ChemDataExtractor: A toolkit for automated extraction\nof chemical information from the scientiﬁc literature. J. Chem. Inf. Model. 56,\n1894–1904 (2016).\n28. Huang, S. & Cole, J. M. A database of battery materials auto-generated using\nChemDataExtractor. Sci. Data 7, 260 (2020).\n29. Court, C. J. & Cole, J. M. Auto-generated materials database of Curie and Néel\ntemperatures via semi-supervised relationship extraction. Sci. Data 5, 180111\n(2018).\n30. Kononova, O. et al. Text-mined dataset of inorganic materials synthesis recipes.\nSci. Data 6, 203 (2019).\n31. Uvegi, H. et al. Literature mining for alternative cementitious precursors and\ndissolution rate modeling of glassy phases.J. Am. Ceram. Soc. 104, 3042–3057\n(2020).\n32. Jensen, Z. et al. A machine learning approach to zeolite synthesis enabled by\nautomatic literature data extraction.ACS Cent. Sci.5, 892–899 (2019).\n33. Guha, S. et al. MatScIE: An automated tool for the generation of databases of\nmethods and parameters used in the computational materials science literature.\nComput. Mater. Sci.192, 110325 (2021).\n34. Olivetti, E. A. et al. Data-driven materials research enabled by natural language\nprocessing and information extraction.Appl. Phys. Rev.7, 041317 (2020).\n35. Jessop, D. M., Adams, S. E., Willighagen, E. L., Hawizy, L. & Murray-Rust, P. OSCAR4:\na ﬂexible architecture for chemical text-mining.J. Cheminformatics 3, 41 (2011).\n36. Epps, R. W. et al. Artiﬁcial chemist: an autonomous quantum dot synthesis bot.\nAdv. Mater. 32, 2001626 (2020).\n37. MacLeod, B. P. et al. Self-driving laboratory for accelerated discovery of thin-ﬁlm\nmaterials. Sci. Adv. 6, eaaz8867 (2020).\n38. Zhang, Y., Chen, Q., Yang, Z., Lin, H. & Lu, Z. BioWordVec, improving biomedical\nword embeddings with subword information and MeSH.Sci. Data 6, 52 (2019).\n39. Ammar, W. et al. Construction of the Literature Graph in Semantic Scholar. in\nProceedings of the 2018 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies, Volume 3\n(Industry Papers) 84–91 (Association for Computational Linguistics, 2018).\n40. Alsentzer, E. et al. Publicly available clinical BERT embeddings. inProceedings of\nthe 2nd Clinical Natural Language Processing Workshop 72–78 (Association for\nComputational Linguistics, 2019).\n41. Johnson, A. E. W. et al. MIMIC-III, a freely accessible critical care database.Sci. Data\n3, 160035 (2016).\n42. Libovický, J., Rosa, R. & Fraser, A. On the language neutrality of pre-trained\nmultilingual representations. in Findings of the association for computational\nlinguistics: EMNLP 2020 1663–1674 (Association for Computational Linguistics,\n2020)\n43. Gupta, T., Zaki, M., Krishnan, N. M. A., & Mausam. MatSciBERT: A materials domain\nlanguage model for text mining and information extraction. Preprint athttps://\narxiv.org/abs/2109.15290. (2021).\n44. Walker, N. et al. The impact of domain-speciﬁc pre-training on named entity\nrecognition tasks in materials science.Available SSRN 3950755(2021).\n45. Friedrich, A. et al. The SOFC-Exp corpus and neural approaches to information\nextraction in the materials science domain. in\nProceedings of the 58th annual\nT. Gupta et al.\n10\nnpj Computational Materials (2022)   102 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nmeeting of the association for computational linguistics1255–1268 (Association for\nComputational Linguistics, 2020).\n46. Mysore, S. et al. The materials science procedural text corpus: Annotating\nmaterials synthesis procedures with shallow semantic structures. inProceedings\nof the 13th linguistic annotation workshop56–64 (Association for Computational\nLinguistics, 2019). https://doi.org/10.18653/v1/W19-4007.\n47. Wu, Y. et al. Google ’s neural machine translation system: Bridging the gap\nbetween human and machine translation. Preprint at https://arxiv.org/abs/\n1609.08144. (2016).\n48. Tokenizer. https://huggingface.co/transformers/main_classes/main_classes/tokenizer.\nhtml.\n49. Liu, Y. et al. RoBERTa: A robustly optimized BERT pretraining approach. Preprint at\nhttps://arxiv.org/abs/1907.11692. (2019).\n50. Maini, P., Kolluru, K., Pruthi, D., & Mausam. Why and when should you pool?\nanalyzing pooling in recurrent architectures. in Findings of the association for\ncomputational linguistics: EMNLP 2020 4568–4586 (Association for Computational\nLinguistics, 2020).\n51. Sainburg, T., McInnes, L. & Gentner, T. Q. Parametric UMAP embeddings for\nrepresentation and semisupervised learning.Neural Comput.33,2 8 8 1–2907 (2021).\n52. Goodfellow, I., Bengio, Y. & Courville, A.Deep learning. (MIT Press, 2016).\n53. allenai/scibert_scivocab_uncased · Hugging Face.https://huggingface.co/allenai/\nscibert_scivocab_uncased.\n54. Hugging Face. GitHub https://github.com/huggingface.\n55. Wolf, T. et al. Transformers: State-of-the-art natural language processing. inPro-\nceedings of the 2020 conference on empirical methods in natural language processing:\nSystem demonstrations38–45 (Association for Computational Linguistics, 2020).\n56. bert/run_pretraining.py at master · google-research/bert.GitHub https://github.\ncom/google-research/bert.\n57. Paszke, A. et al. PyTorch: An Imperative Style, High-Performance Deep Learning\nLibrary. 12.\n58. Crossref Metadata Search.https://search.crossref.org/.\n59. Elsevier Developer Portal.https://dev.elsevier.com/.\n60. Baldini Soares, L., FitzGerald, N., Ling, J. & Kwiatkowski, T. Matching the blanks:\nDistributional similarity for relation learning. in Proceedings of the 57th annual\nmeeting of the association for computational linguistics2895–2905 (Association for\nComputational Linguistics, 2019).\n61. Lafferty, J. D., McCallum, A. & Pereira, F. C. N. Conditional randomﬁelds: Prob-\nabilistic models for segmenting and labeling sequence data. inICML 282–289\n(Morgan Kaufmann, 2001).\n62. pytorch-crf — pytorch-crf 0.7.2 documentation. https://pytorch-crf.readthedocs.\nio/en/stable/.\n63. Huang, Z., Xu, W. & Yu, K. Bidirectional LSTM-CRF models for sequence tagging.\nPreprint at https://arxiv.org/abs/1508.01991 (2015).\nACKNOWLEDGEMENTS\nN.M.A.K. acknowledges the funding support received from SERB (ECR/2018/002228),\nDST (DST/INSPIRE/04/2016/002774), BRNS YSRA (53/20/01/2021-BRNS), ISRO\nRESPOND as part of the STC at IIT Delhi. M.Z. acknowledges the funding received\nfrom the PMRF award by Government of India. M. acknowledges grants by Google,\nIBM, Bloomberg, and a Jai Gupta chair fellowship. The authors thank the High\nPerformance Computing (HPC) facility at IIT Delhi for computational and storage\nresources.\nAUTHOR CONTRIBUTIONS\nM. and N.M.A.K. supervised the work. T.G. developed the codes and trained the\nmodels. M.Z. along with T.G. performed data collection and processing. All the\nauthors analyzed the results and wrote the manuscript.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41524-022-00784-w.\nCorrespondence and requests for materials should be addressed to N. M. Anoop\nKrishnan or Mausam.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visithttp://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2022\nT. Gupta et al.\n11\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2022)   102 ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7877285480499268
    },
    {
      "name": "Natural language processing",
      "score": 0.6573169827461243
    },
    {
      "name": "Information extraction",
      "score": 0.6175493597984314
    },
    {
      "name": "Relationship extraction",
      "score": 0.6112836003303528
    },
    {
      "name": "Transformer",
      "score": 0.5191232562065125
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4981715679168701
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49223652482032776
    },
    {
      "name": "Notation",
      "score": 0.4433441162109375
    },
    {
      "name": "Information retrieval",
      "score": 0.4152368903160095
    },
    {
      "name": "Linguistics",
      "score": 0.15352141857147217
    },
    {
      "name": "Engineering",
      "score": 0.08290889859199524
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I68891433",
      "name": "Indian Institute of Technology Delhi",
      "country": "IN"
    }
  ],
  "cited_by": 233
}