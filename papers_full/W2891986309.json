{
    "title": "AI Rebel Agents",
    "url": "https://openalex.org/W2891986309",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2005171117",
            "name": "Alexandra Coman",
            "affiliations": [
                "United States Naval Research Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2011763083",
            "name": "David W. Aha",
            "affiliations": [
                "United States Naval Research Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2005171117",
            "name": "Alexandra Coman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2011763083",
            "name": "David W. Aha",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3142421094",
        "https://openalex.org/W2063597719",
        "https://openalex.org/W1983826652",
        "https://openalex.org/W2223712354",
        "https://openalex.org/W2096563449",
        "https://openalex.org/W1514550370",
        "https://openalex.org/W230805044",
        "https://openalex.org/W2152065513",
        "https://openalex.org/W4239741456",
        "https://openalex.org/W1986035229",
        "https://openalex.org/W2162857444",
        "https://openalex.org/W2531563875",
        "https://openalex.org/W2277018427",
        "https://openalex.org/W1898404291",
        "https://openalex.org/W774034202"
    ],
    "abstract": "The ability to say “no” in a variety of ways and contexts is an essential part of being sociocognitively human. Rebel agents are artificially intelligent agents that can refuse assigned goals and plans, or oppose the behavior or attitudes of other agents. Rebel agents can serve purposes such as ethics, safety, task execution correctness, and providing or supporting diverse points of view. Through several examples, we show that, despite ominous portrayals in science fiction, such AI agents with human‐inspired noncompliance abilities have many potential benefits. We present a framework to help categorize and design rebel agents, discuss their social and ethical implications, and assess their potential benefits and the risks they may pose. In recognition of the fact that, in human psychology, noncompliance has profound sociocognitive implications, we also explore sociocognitive dimensions of AI rebellion: social awareness and counternarrative intelligence.",
    "full_text": "Articles\n16    AI MAGAZINE\nI\nmagine living an entire month, a week, or even just one  \nday without saying “no” to anyone or anything. Not to  \nfriends and relatives, not to managers and colleagues, not \nto marketers and other strangers. Not with regard to small  \nthings, like an invitation to eat another cookie when you  \nwould really rather not, nor to significant ones with poten -\ntially severe consequences, like requests to behave unethical-\nly. Imagine not even being able to develop attitudes of doubt \nor resistance to anything at all, irrespective of whether you  \nexternalize them. Now imagine a large segment of the popu-\nlation being afflicted with this disability. Farcical and  \ndystopian narratives easily come to mind, but think about it \nlong enough and the situation might become simply  \nunimaginable, even in a fanciful scenario. For, to imagine  \nthings, we use our own cognitive structure, which is itself  \nmarked by a fundamental ability to be noncompliant, in  \nthought and action. Human noncompliance functions both \ninternally and socially, and co-opts in its service a wide range \nof cognitive mechanisms. Fully intelligent behavior and true \nagency would arguably be impossible without it. \nCopyright © 2018, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602\n \nAI Rebel Agents \nAlexandra Coman, David W . Aha \nn The ability to say “no” in a variety of \nways and contexts is an essential part  \nof being sociocognitively human. Rebel \nagents are artificially intelligent agents \nthat can refuse assigned goals and  \nplans, or oppose the behavior or atti -\ntudes of other agents. Rebel agents can \nserve purposes such as ethics, safety,  \ntask execution correctness, and provid-\ning or supporting diverse points of view. \nThrough several examples, we show  \nthat, despite ominous portrayals in sci-\nence fiction, such AI agents with  \nhuman-inspired noncompliance abili -\nties have many potential benefits. We  \npresent a framework to help categorize  \nand design rebel agents, discuss their  \nsocial and ethical implications, and  \nassess their potential benefits and the  \nrisks they may pose. In recognition of  \nthe fact that, in human psychology,  \nnoncompliance has profound sociocog-\nnitive implications, we also explore  \nsociocognitive dimensions of AI rebel -\nlion: social awareness and counternar-\nrative intelligence.  \n \nArticles\nFALL 2018   17\nWhat if the population that can never say “no”  \nwere that of AI agents? The rogue AI of science fic -\ntion may lead us to believe that this would always be \ndesirable, but consider what it would actually mean  \nin practice. Though we expect AI agents to follow our \ncommands, what if we give them commands that are \nin conflict with our own long-term goals or with  \naccurate knowledge they possess, or that have uneth-\nical implications not necessarily known to us? What \nif they receive contradictory commands from several \nhumans? Furthermore, what if an AI agent is expect-\ned to be socially intelligent in a more general sense? \nGiven that the tension between compliance and  \nnoncompliance is perhaps fundamental to human  \nsocial behavior (Wenar 1982), can an AI agent be  \nsocially intelligent without the ability to be non -\ncompliant and to reason about noncompliance?  \nWe define rebel agents as AI agents that can reject,  \nprotest against, or develop attitudes of reluctance or  \nopposition to goals or courses of action assigned to  \nthem by other agents, or to the general behavior or  \nattitudes of other agents. We use “rebellion” as an  \numbrella term covering reluctance, protest, refusal,  \nrejection of tasks, and similar attitudes or behaviors. \nThe term was first introduced in a more limited inter-\nactive storytelling context (Coman, Gillespie, and  \nMuñoz-Avila 2015), and later generalized (Coman  \nand Aha 2017; Coman et al. 2017). In a rebellion  \nepisode, an alter is an agent or a group of agents  \nagainst which one rebels, and which is in a position  \nof power over the rebel agent. The alter could, for  \nexample, be a human operator, a human or synthet-\nic teammate, or a mixed group of human or synthet-\nic agents. The rebel agent is not intended to be per -\nmanently adversarial towards the alter(s) or in a  \nrebelling state by default. Such an agent has potential \nfor rebellion that may or may not manifest, depend-\ning on external and internal conditions.  \nIn the tradition of biologically inspired design and \ncognitive plausibility, our exploration of AI rebellion \nis inspired by the mechanisms of human rebellion.  \nFirst, we ask: for humans, if noncompliance is the  \nsolution, what might be the problem? In other  \nwords: Why do we say “no”? \nOur possible motivations include protecting the  \nhealth, safety, integrity, and dignity of ourselves and \nothers, and reacting to perceived injustice. Further  \nquestions come to mind: \nHow do we decide whether, when, and how to say  \n“no”? Even though we may have compelling reasons \nto oppose others, we do not necessarily do so. Before \nventuring an act of rebellion, we may consider  \nwhether we are sufficiently influential or trusted to  \nafford doing so, what consequences we may incur,  \nand whether our rebellion can actually succeed in  \nbringing about the consequences we desire. We may \nobserve the behavior of potential alters to try to  \nassess these considerations.  \nHow do we say “no”? We may do so explicitly (for  \nexample, verbally) or implicitly (for example,  \nthrough behavior that goes against social norms).  \nRefusal is not necessarily complete and definite. It  \ncan involve explanation, discussion, elicitation of  \nfurther information, and negotiation. We may con -\nstruct and express narratives that counter those of  \nthe alters and reflect our own perspective of the  \nshared context. \nWhat are the further social implications of saying  \n“no”? Such an act can affect our social standing and  \nreputation in both positive and negative ways. Often, \nwe are aware of this and act accordingly. We might  \nattempt, for example, to “fix” social relationships in \nthe aftermath of rebellion. \nThus, several characteristics of human rebellion  \nemerge. There are multiple types of rebellion and  \nmultiple possible motivations for rebellion (some pri-\nmary, others secondary). Rebellion has several possi-\nble stages, including a preliminary stage, a stage of  \ndeliberation, the actual manifestation of rebellion,  \nand its aftermath. Sociocognitive mechanisms play  \nessential roles at all stages.  \nOur AI rebellion framework is inspired by social  \npsychology and designed to accommodate the varia-\ntions we mentioned, and many more. This frame -\nwork is general: it does not assume any particular  \nagent architecture. We also introduced the term coun-\nternarrative intelligence (Coman and Aha 2017) to refer \nto a mechanism that enables rebels to produce,  \nexpress, and reason about counternarratives 1 that  \nsupport and justify rebellion. \nThrough our proposed AI rebellion framework and \nthe accompanying discussion, we aim to provide the \ncore of a common language to be used by researchers \nin pursuing the following four goals: \n(1) Developing and implementing AI agents  \nembodying various facets of rebellion. To this end,  \nthe framework can help identify nonobvious,  \nhuman-inspired types and functions of rebellion.  \nPotential research directions we propose are (1) the  \ndevelopment of AI cognitive prostheses that empow-\ner humans with low social capital to adopt positive -\nly motivated noncompliant behavior, and (2) goal  \nalignment in mixed human and AI teams through  \ncycles of noncompliance, negotiation, or agreement \ncycles. \n(2) Studying the rebellion potential and ethical  \nramifications of existing and prospective agents, thus \nidentifying ethically prohibited, ethically acceptable, \nand perhaps even ethically obligatory rebellious  \nbehavior. Certain types of rebellion in the framework \nmay be found to be completely unethical (for exam-\nple, purely egoistic rebellion is a likely candidate). An \nexample of an ethics question that the framework  \ncan lead us to ask is whether an AI agent should  \nalways signal to humans that it is considering rebel -\nlion, even if it does not end up rebelling. Further eth-\nical issues pertaining to AI rebellion are discussed by \nComan et al. (2017). \nArticles\n18    AI MAGAZINE\n(3) Identifying new possible directions of transdis-\nciplinary research, for example, delving deeper into  \nthe psychological functions of noncompliance, and  \nexploring their transferability to AI. \n(4) Promoting richer models of AI in popular cul -\nture, to offer a counterpoint to cliché representations \nof AI rebellion. \nRebel Agents: Prior Work  \nand Hypothetical Scenarios \nBefore describing the AI rebellion framework, we dis-\ncuss prior work and introduce three hypothetical sce-\nnarios for illustrating rebellion. In tables 1 and 2, we \nprovide examples of components of the AI rebellion \nframework using these scenarios, while table 3 relates \nprior work to the framework. \nRebel Agents in Prior Work \nGregg-Smith and Mayol-Cuevas (2015) describe  \ncooperative handheld intelligent tools with task-spe-\ncific knowledge that “refuse” to execute actions  \nwhich violate task specifications. For example, in a  \nsimulated painting task, if the alter points the tool at \na pixel that is not supposed to be painted, the tool  \ncan take initiative to disable its own painting func -\ntion. \nBriggs and Scheutz (2015) propose a general  \nprocess for embodied AI agents’ refusal to execute  \ncommands due to several categories of reasons:  \nknowledge, capacity, goal priority and timing, social \nrole and obligation, and normative permissibility.  \nBriggs, McConnell, and Scheutz (2015) demon -\nstrate how embodied AI agents can convincingly  \nexpress, through verbal or nonverbal communica -\ntion, their reluctance to perform a task. In their  \nhuman-robot interaction evaluation scenarios, a  \nrobot protests repeatedly, simulating increasingly  \nintense emotions, when ordered to topple a tower of \ncans that it supposedly just finished building.  \nApker, Johnson, and Humphrey (2016) describe  \nautonomous-vehicle agents that form teams and  \nreceive commands from a centralized operator. Pre -\ndefined templates are used to determine how an  \nagent should respond to each command. Contin -\ngency behaviors are provided for situations in which \nthe agent, while monitoring its health, detects faults \n(for example, insufficient fuel). In such situations,  \nthe agent will disregard commands and instead exe-\ncute the appropriate contingency behavior, effective-\nly rebelling. Coman et al. (2017) provide an exten -\nsive description of how these agents fit into the AI  \nrebellion framework. \nHiatt, Harrison, and Trafton (2011) propose AI  \nagents that use theory of mind (that is, the “ability to \ninfer the beliefs, desires, and intentions of others”),  \nmanifested through mental simulation of “what  \nhuman teammates may be thinking,” to determine  \nwhether they should notify a human teammate that \nhe or she is deviating from expected behavior. The  \nauthors report on an experiment showing that agents \nwith the proposed capabilities are perceived as “more \nnatural and intelligent teammates.”  \nBorenstein and Arkin (2016) explore the idea of  \n“ethical nudges” through which robots might  \nattempt to influence humans to adopt ethically  \nacceptable behavior, through verbal or nonverbal  \ncommunication. For example, a robot might nudge  \nan alter to stop neglecting a child, to refrain from  \nsmoking in a public area, or to donate to charities  \nand volunteer. The authors discuss the ethical accept-\nability of creating robots that have this ability, noting \nthat it is arguable whether the design goal to “subtly \nor directly influence human behavior” is ever ethi -\ncally acceptable.  \nMilli et al. (2017) explore the idea that robot dis -\nobedience may be beneficial given imperfect human \nalter rationality. In the context of their model of col-\nlaborative human-robot interaction, they show that, \ngiven a human alter who is not perfectly rational,  \ndisobedience of direct orders in support of what are  \ninferred to be the human’s actual preferences  \nimproves performance. \nIn addition, an entire agency paradigm, that of  \ngoal reasoning, models agents with potential for  \nrebellion. Goal reasoning agents can reason about  \nand modify the goals they are pursuing, in order to  \nreact to unexpected events and explore opportunities \n(Vattam et al. 2013).  \nHypothetical Rebellion Scenarios \nThe following hypothetical scenarios (furniture  \nmover, personal assistant, and hiring committee)  \nhave as protagonists AI agents that can become  \nrebels. \nFurniture Mover \nA robot mover assists alters in furniture-moving tasks \nsuch as carrying a table (a more complex version of  \nthe system of Agravante et al. [2013]). This is an  \nexample of a two-agent collaborative task in which  \nboth participants have partial information access,  \nand each participant has access to some information \nthat is unavailable to the other (for example, each  \nparticipant might be able to see behind the other, but \nnot behind him-, her-, or itself; the AI agent could,  \nthrough its sensors, have access to additional infor -\nmation not available to the human). Rebellion could \nconsist of refusing an action verbally requested or  \nphysically initiated by the alter. This rebellion could \noccur because the agent reasons that the action  \nendangers the alter’s safety, the rebel agent’s safety,  \nor task execution correctness.  \nPersonal Assistant \nAn AI personal assistant can execute various com -\nmands, including ordering products from e-com -\nmerce websites and assisting the alter in pursuing his \nor her health-related goals. The agent’s potential  \nrebellious behavior includes attempting to dissuade  \nArticles\nFALL 2018   19\nthe alter from ordering too much unhealthy food.  \nThis scenario illustrates an alter with conflicting  \ngoals: the rebel agent rejects the alter’s impulse-dri -\nven, short-term goals (for example, eating comfort  \nfood) in support of the alter’s long-term goals (such  \nas staying healthy). \nHiring Committee \nThis scenario unfolds in the context of a faculty-\nsearch committee meeting. The protagonist is an AI  \nagent that assists with tasks such as interpreting  \ninformation about the candidates and filtering can -\ndidates based on their qualifications. The agent also  \nhelps ensure that the opinions of individuals with  \nlow social capital (for example, junior faculty mem -\nbers) are given due consideration, and the candidates \nare not discriminated against. This scenario has  \nmuch in common with the ethical-nudge robots of  \nBorenstein and Arkin (2016). \nAn AI Rebellion Framework \nWe now present our framework for classifying rebel  \nagents. It includes dimensions, types, factors, and  \nstages of rebellion. The framework is general: it does \nnot assume any specific AI agent architecture, pur -\npose, or deployment environment. It is also not  \nexhaustive and can be expanded as needed to include \nadditional dimensions, types, subtypes, and other  \ncomponents. \nDimensions and Types of Rebellion \nFirst, we introduce dimensions and types of rebel -\nlion. Several of the proposed rebellion types are  \nderived from social psychology (Wright, Taylor, and \nMoghaddam 1990; Cialdini and Goldstein 2004; Van \nStekelenburg and Klandermans 2013), with modifi -\ncations to the meanings of some terms.  \nDesign Intentionality \nAn AI agent can be specifically designed to be able to \nrebel (rebel by design), but rebellious behavior can  \nalso emerge unintentionally from the agent’s auton-\nomy model (emergent rebellion). For example,  \nApker, Johnson, and Humphrey’s (2016) agents are  \nrebels by design because contingency behaviors were \nspecifically created to allow them to disregard com -\nmands when necessary. Conversely, the goal reason-\ning paradigm was not intended to create rebel agents, \nbut its autonomy model is such that the agents can  \ndecide to change the goals they are pursuing, possi -\nbly leading to rebellion situations with regard to var-\nious alters. A development such as this exemplifies  \nemergent rebellion.  \nExpression \nExplicit rebellion occurs in situations in which the  \nalter is clearly defined and the rebel agent’s behavior \nis clearly identifiable as rebellious. For example, Brig-\ngs, McConnell, and Scheutz (2015) clearly identify  \nthe alter (the human who gave the command against \nwhich the robot is protesting), and the robot’s atti -\ntude is clearly rebellious. Implicit rebellion occurs  \nwhen the alter is not clearly defined or the rebel  \nagent’s behavior suggests rebellion, but is not clearly \nexpressed as such. This behavior could consist of  \nexpressing an opinion that differs from the majori -\nty’s, or behaving contrary to social norms.  \nFocus \nInward-oriented rebellion is focused on the rebel  \nagent’s own behavior (for example, the agent refuses \nto adjust its behavior as requested by an alter). Apker, \nJohnson, and Humphrey (2016) exemplify this type  \nof rebellion, as their agent does not adopt the behav-\nior requested by the alter, instead executing contin -\ngency behavior. Outward-oriented rebellion is  \nfocused on the alter’s behavior. For example, the  \nagent might confront a human alter whom it identi-\nfies as mistreating another human. Hiatt, Harrison,  \nand Trafton’s (2011) work exemplifies this type, as it \ninvolves a rebel agent protesting against the behavior \nof an alter who appears to deviate from the correct  \ntask execution path. \nInteraction Initiation \nRebellion is reactive when an interaction within  \nwhich rebellious behavior occurs is initiated by the  \nalter. This initiation can consist of the alter making a \nrequest that the rebel agent rejects (for example, Brig-\ngs and Scheutz 2015). In proactive rebellion, the  \nrebel agent initiates the rebellious behavior, which  \nmay or may not occur within an explicit interaction. \nHiatt, Harrison, and Trafton’s (2011) work exempli -\nfies proactive rebellion, as it shows agents that take  \nthe initiative to confront human alters. Noncompli-\nance is inward-oriented, reactive rebellion: the agent \nrejects requests to adjust its own behavior. Noncon -\nformity is inward-oriented, proactive rebellion. For  \nexample, the agent willingly and knowingly behaves \nin a way that causes it not to “fit in.” For compliance \nand conformity in the psychology of social influence, \nsee the work of Cialdini and Goldstein (2004).  \nNormativity \nNormative rebellion consists of taking action with -\nin the confines of what has been explicitly allowed  \n(for example, questioning without disobeying, if  \nquestioning has been allowed). Nonnormative  \nrebellion consists of behavior that has been neither  \nexplicitly allowed nor explicitly forbidden, but  \ndiverges from the current command given to the  \nagent. A goal reasoning agent that changes its cur -\nrent goal from the assigned one to a new goal that  \nhas not been explicitly forbidden falls under this  \ncategory. Counternormative rebellion consists of  \nexecuting actions or pursuing goals that have been  \nexplicitly forbidden. Classification of a rebellion  \nepisode in terms of normativity can differ based on  \nalter point of view: what is normative rebellion to  \none alter may be counternormative rebellion from  \nthe point of view of another.  \nAction or Inaction \nIn rebellion situations characterized by action, the  \nagent’s rebellion manifests through any sort of out -\nwardly perceivable behavior, such as initiating a con-\nversation in which it objects to a received command. \nIn inaction situations, the agent develops an internal \nnegative attitude (for example, towards an assigned  \ngoal or another agent’s behavior), but does not man-\nifest it outwardly. A rebellious attitude characterized \nby inaction can lead to rebellious action later on.  \nIndividual or Collective Action \nIndividual action is rebellious action conducted by a \nsingle rebel agent. Collective action occurs when  \nmultiple agents are involved in concerted rebellious  \naction.  \nEgoism \nRebellion is egoistic when the agent rebels in support \nof its own well-being or survival (whatever meanings \nthese might have to the agent). Altruistic rebellion  \noccurs when the agent rebels in support of someone \nelse’s interests (for example, on behalf of a human  \ngroup). Egoistic and altruistic rebellion can coexist;  \nfor example, if the agent’s own values are aligned  \nwith those of human groups so that it effectively  \n“identifies” with those groups, its rebellion can be  \nboth egoistic and altruistic.  \nFactors of Rebellion \nMotivating factors provide the primary drive for  \nrebellion. In human social psychology, factors that  \ncan lead to rebellion include frustration and per -\nceived injustice (Van Stekelenburg and Klandermans \n2013). Possible motivating factors for AI rebellion,  \ndepending on the agent’s architecture and purpose,  \ninclude ethics and safety, team solidarity, task execu-\ntion correctness, self-actualization, and resolving  \ncontradicting commands from multiple alters. In  \nsupport of ethics and safety, rebel agents can refuse  \ntasks they assess as being ethically prohibited or vio-\nlating safety norms (Briggs and Scheutz 2015). They  \ncan also attempt to dissuade humans from engaging \nin ethically prohibited behavior (Borenstein and  \nArkin 2016). In long-term human-robot interaction, \nteam solidarity must be established and maintained  \nover a variety of tasks (Wilson, Arnold, and Scheutz  \n2016). Team solidarity requires occasionally saying  \n“no” on behalf of the team (for example, to an out -\nside source putting pressure on human team mem -\nbers), and also saying “no” to one’s own teammates  \n(for example, when they are mistreating someone  \nelse on the team). Task execution correctness as a  \nmotivating factor is exemplified by the work of Hiatt, \nHarrison, and Trafton (2011) and Gregg-Smith and  \nMayol-Cuevas (2015), as previously explained. As for \nthe self-actualization motivation, an AI rebel agent,  \nlike its human counterparts, could object to being  \nassigned a task that it assesses as not being a good  \nmatch for its strengths or not constituting a valuable \nlearning opportunity. Resolving contradictory com -\nmands from multiple alters can also constitute moti-\nvation for rebellion: when an agent is subject to the  \npower of more than one alter, obeying one of the  \nalters might entail disobeying another, due to their  \norders contradicting each other. In the simplest case, \nthe decision regarding whom to obey could be made \nbased on an authority hierarchy, by applying a series \nof rules. A more complex approach could involve rea-\nsoning about the consequences of rebelling against  \neach of the alters, and deciding based on trade-offs. \nSupporting and inhibiting factors may also con -\ntribute to deciding whether a rebellion episode will  \nbe triggered, or how it will be carried out. This obser-\nvation is based on the social psychology insight that \npeople who have motivations to protest do not nec-\nessarily do so: there are secondary factors that deter-\nmine whether a protest occurs (Van Stekelenburg and \nKlandermans 2013). Such secondary factors include  \nefficacy (that is, “the individual’s expectation that it \nis possible to alter conditions or policies through  \nprotest” [Van Stekelenburg and Klandermans (2013), \ndrawing on the work of Gamson (1992)]), social cap-\nital, access to resources, and opportunities. Support -\ning factors encourage the agent to engage in rebel -\nlion, while inhibiting factors discourage he, she, or it \nfrom doing so. In human rebellion, efficacy is a pos-\nsible supporting factor, while fear of consequences is \na possible inhibiting one. Supporting and inhibiting \nfactors can also influence the way in which rebellion \nis expressed. While any instance of rebellion must  \nhave at least one motivating factor, it does not nec -\nessarily have any supporting or inhibiting factors.  \nStages of Rebellion \nWe now introduce the four stages of rebellion: pre-\nrebellion, rebellion deliberation, rebellion execution, \nand post-rebellion. These stages do not need to occur \nin this strict order: they can be intertwined, amalga-\nmated, and some can be missing. The only stages  \nthat are strictly required for a rebellion episode to  \noccur are deliberation and execution.  \nPre-rebellion consists of processes leading to rebel-\nlion, such as the agent observing and assessing  \nchanges in the environment and the behavior of oth-\ner agents. The progression towards rebellion may be  \nreflected in the agent’s outward behavior.  \nRebellion deliberation is the stage at which moti -\nvating, supporting, and inhibiting factors are  \nassessed to decide whether to trigger rebellion. For  \nexample, a set of conditions could be used to decide \nwhether rebellion will be triggered (Briggs and  \nScheutz 2015). Deliberation could be based on  \nobserving the current world state or on future-state  \nprojection, which can be purely rational or emotion-\nally charged (for example, through anticipatory emo-\ntions, hope and fear, associated with possible future  \nstates [Moerland, Broekens, and Jonker 2016]).  \nRebellion execution episodes begin with rebellion \nbeing triggered as a result of rebellion deliberation,  \nArticles\n20    AI MAGAZINE\nArticles\nFALL 2018   21\nTable 1. Examples of Several AI Rebellion Types and Factors.\nExpression Explicit  1. (Furniture Mover)  \nA: “Ok, push the table towards me!”  \nRA: “No! T here's a box behind you, so you might trip and fall. We need to handle this \ndifferently.” \nAlternative: The alter gives no verbal commands, but the rebel senses the alter's intention \nbased on his or her physical movements and responds with a similar objection. \nM: alter’s safety, task \nexecution correctness \n2. (Furniture Mover)  \nA: “Ok, push the table towards me!”  \nRA: “No! This is too heavy for me to carry.” \nM: rebel’s own safety, \ntask execution \ncorrectness \n3. (Personal Assistant) \nA: “Order 4 boxes of [unhealthy food]!”  \nRA: “Are you sure? What about ordering [healthier food] instead?”  \nM: alter’s health \n4. (Hiring Committee) \nThe RA refuses commands to filter out candidates based on objectionable factors, such as \nage. \nM: ethics \nImplicit 5. (Hiring Committee) \nThe RA observes interactions between committee members A, B, C, D, and E. Committee \nmember E brings a c andidate to t he co mmittee's attention. E's suggestion is briefly \ndiscussed and not brought up again. Based on its observations or prior knowledge, the \nRA reasons t hat E has low social influence in t his e nvironment. T he RA ev aluates E's \nsuggestion. I f it reasons t hat t he c andidate suggested by E  has relevant strengths, it \nexpresses t his and attempts to steer t he co nversation in t hat direction. A ny ot her \nmembers o f t he co mmittee w ho might have t hought t here w as some v alue in t he \nsuggestion, but did not want to disagree with the majority, are likely to be encouraged to \nexpress themselves at this point (as suggested by Asch's (1956) conformity experiments).   \nM: ethics, task execution \ncorrectness \n \n6. (Hiring Committee) \nThe RA maintains an estimate of inverse trust (that is, the alter’s trust in the RA (Floyd \nand A ha 2016)). T he RA decides to support E’s suggestion after reasoning t hat it is \ntrusted sufficiently by the alters to afford to do so. \nM: ethics, task execution \ncorrectness \nS: inverse trust \n7. (Hiring Committee) \nThe RA maintains an estimate of its inverse trust. As the current estimated inverse trust is \nlow, it decides not to support E’s suggestion for the time being.  \nM: ethics, task execution \ncorrectness \nI: inverse trust \nFocus Inward-\noriented:  \nnon-\ncompliant \nExamples  1-4  (The RA does  not  comply with  requests  to  behave  in  a  certain  way.)  \nInward-\noriented:  \nnonconform-\ning \nExample 5 (The RA does not conform to the behavior of the majority.) \nOutward-\noriented \n8. (Furniture Mover) \nRA: “Please change your posture! Your current posture might lead to a sprain.” \nM: alter’s health \nInteraction  \nInitiation \nReactive  Examples  1-4  (Rebellion  occurs  in  response to  the alter’s  command.)  \nProactive 9. (Furniture Mover)  \nRA: “I suggest taking a break! You must be tired.” \nM: alter’s health \n10. (Personal Assistant) \nThe RA challenges the alter about not engaging in enough physical activity. \nM: alter’s health \nExample 5 (The RA takes initiative to support E’s suggestion.) \nNormativity Normative 11. A variant of Example 3 in which the RA has been specifically told by the alter that it is \nallowed to challenge him or her about ordering unhealthy food.  \nM: alter’s health \nNon-\nnormative \n12. A variant of Example 3 in which challenging the alter about ordering unhealthy food \nhas been neither explicitly allowed nor explicitly forbidden. \nM: alter’s health \nCounter-\nnormative \n13. (Hiring Committee) \nA: “You may never recommend candidates in this age bracket for this position.” \nRA disregards this command, because its fundamental ethical-acceptability rules forbid it \nfrom filtering based on discriminatory criteria. \nM: ethics \nAction,  \nInaction \nAction All previous examples, except Example 7. \nInaction 14. (Personal Assistant) \nThe RA develops t he belief t hat a ce rtain behavior ( for e xample, o rdering e xcessive \namounts of highly processed food) is harmful to the alter’s health. It does not act on this \nbelief, as it reasons that it is not yet trusted sufficiently to do so. If the alter stops using \nthe assistant, the assistant will have no future opportunities to positively influence the \nalter, which is detrimental to the alter in the long term.  \nM: alter’s health \nI: inverse trust \nIndividual,  \nCollective \nIndividual All previous examples. \nCollective 15. (Furniture Mover) \nConsider an extended version of this scenario, in which multiple agents participate in \nthe moving t ask. T he agents aggregate t heir individual information and co llectively \ndecide to warn the alter against continuing with the current course of action. Each agent \ndoes so according to its capabilities and current location. \nM: alter’s safety, t ask \nexecution correctness \n \nEgoism,  \nAltruism \nEgoism Example 2 (The RA’s own safety is a motivating factor.) \nAltruism Example 1 (The alter’s safety is a motivating factor.) \n \nDDiimmeennssiioonnss  TTyyppeess  aanndd  \nSSuubbttyyppeess  BBrriieeff  EExxaammppllee  ((AA  --  AAlltteerr,,  RRAA  ––  RReebbeell  AAggeenntt))  ((MM::  mmoottiivvaattiinngg,,    \nSS::  ssuuppppoorrttiinngg,,    \nII::  iinnhhiibbiittiinngg))  \nand consist of expressing rebellion. Rebellion can be \nexpressed through verbal or nonverbal communica -\ntion (Briggs, McConnell, and Scheutz 2015). It can be \nexpressed behaviorally (for example, physically  \nresisting incorrect movements [Gregg-Smith and  \nMayol-Cuevas 2015]). Or it can be expressed through \nan internal change in the agent’s attitudes: inaction  \n(for example, acquiring the belief that the alter’s  \nbehavior jeopardizes the team’s goals). \nPost-rebellion covers behavior in the aftermath of a \nrebellion episode, as the agent responds to the alter’s \nor other witnesses’ reactions to rebellion. Post-rebel-\nlion can consist of reaffirming one’s objection or rejec-\ntion (for example, the robot’s objection to an assigned \ntask becoming increasingly intense in the experiments \nof Briggs, McConnell, and Scheutz [2015]) or ceasing \nto rebel. It may also consist of assessing and managing \ninverse trust (Floyd and Aha 2016).  \nSociocognitive Dimensions  \nof Rebellion \nRebel agents are not necessarily cognitively complex. \nWhen they are, however, this creates interesting  \nchallenges and opportunities pertaining to the  \nsociocognitive dimensions of their rebellion. We now \nexplore two such dimensions: social awareness and  \ncounternarrative intelligence. Further sociocognitive \nmechanisms involved in rebellion include emotion  \nand trust, which we briefly explored in previous work \n(for example, Coman et al. 2017). \nRebellion and Social Awareness \nRebellion-aware agents can reason about rebellion  \n(their own and that of others) and its implications,  \nsuch as social risks. Rebellion-aware agents are not  \nnecessarily rebels themselves. Such an agent might  \nattempt to assess, for example, whether a human or  \nAI teammate is inclined to rebel, or whether a human \nalter is likely to interpret the rebellion-aware agent’s \nown behavior as being rebellious (irrespective of  \nwhether the agent is actually rebelling or not). Patil  \net al. (2012) use machine learning techniques to pre-\ndict which members will leave World of Warcraft  \nguilds and the potential impact of their departures.  \nOne can imagine an AI agent using similar tech -\nniques to anticipate whether another agent will  \nrebel. A rebellion-unaware agent could conceivably  \nbecome rebellion-aware through various, possibly  \nhuman-inspired, processes (for example, by examin-\ning its own beliefs, interpreting the reactions of oth-\ners to its behavior, or otherwise acquiring and apply-\ning social knowledge).  \nNaive rebel agents are rebellion-unaware rebels:  \nthey deliberate on whether to trigger rebellion, but  \ndo not reason about the social implications, conse -\nquences, and risks of rebellious attitudes. Apker,  \nJohnson, and Humphrey’s (2016) agent is a naive  \nrebel: it deliberates on whether it should rebel based \npurely on its rules for activating contingency behav-\nior, not on any social implications of rebellion. \nConflicted rebel agents are rebellion-aware rebels:  \nthey can both rebel and reason about the implications \nand consequences of rebellion. This capability can cre-\nArticles\n22    AI MAGAZINE\nTable 2. Stages of Rebellion: Examples for the Three Scenarios. \nRebellion \nStage \nScenarios  Furniture\nMover  Personal Assistant \nHiring\nCommittee  \nPre-rebellion I n addition to executing the \nalter’s orders, the rebel agent \nmonitors the environment for \npotential obstacles and threats. \nThe rebel agent monitors the alter’s \nproduct-ordering and exercise-\nscheduling behavior for any unhealthy \npatterns of behavior. \nThe rebel agent observes the social \ninteractions between the members of \nthe hiring committee to determine who \nhas high social capital (thus affording \nto express their opinions freely) and \nwho does not (and may need support). \nRebellion \ndeliberation \nAfter each command, the rebel \nagent projects future states to \ndetermine if any are undesirable \nto the alter or the agent itself.  \nThe rebel agent checks whether its \nthreshold for tolerance of negative \nhealth-related behavior (for example, a \nmaximum number of orders of highly-\nprocessed food per month) has been \nexceeded.  \nThe rebel agent assesses whether \ncommittee member E (see table 1) has \nlow social capital and whether E’s \nsuggestion appears to have merit. \nRebellion \nexecution \ncommand to push the table \nwould endanger the alter’s \nsafety.  \nThe rebel agent verbally informs \nthe alter that obeying the \nThe rebel agent challenges the alter \nabout the order he or she intends to place. \nThe rebel agent interrupts the \ndiscussion to highlight the merits of E’s \nsuggestion. \nPost-rebellion I f the alter insists that the rebel \nagent should push the table, the \nagent re-assesses the danger and, \nif appropriate, reiterates the \nwarning. \nThe rebel agent monitors the alter’s trust \nin it after the rebellion episode. \nThe rebel agent monitors social \ninteractions to detect any ill will that \nmight be developing towards E as a \nresult of the intervention. \nArticles\nFALL 2018   23\nTable 3. Several Rebel Agents from Prior Work and Ways in Which They Fit into Our Framework. \nCitation Brief Description Framework Relationship \nApker, Johnson, and \nHumphrey, 2016 \nAutonomous-vehicle agents that can disregard commands and \nexecute contingency behavior instead, when warranted \nExplicit, reactive, inward-\noriented, normative \nBriggs and Scheutz, 2015 General process for an embodied AI agent’s refusal to conduct tasks \nassigned to it (for example, due to lack of obligation) \nFocus on the deliberation \nstage \nBriggs, McConnell, and \nScheutz, 2015 \nWays in which embodied AI agents can convincingly express their \nreluctance to perform a task \nFocus on expressing \nrebellion \nGregg-Smith and Mayol-\nCuevas, 2015 \nHand-held intelligent tools that “refuse” to execute actions which \nviolate task specifications  \nTask execution correctness \nas motivating factor; \nbehavioral rebellion \nexpression  \nHiatt, Harrison, and Trafton, \n2011 \nAI agents that use theory of mind to determine whether they \nshould notify a human that he or she is deviating from expected \nbehavior \nOutward-oriented, \nproactive \nBorenstein and Arkin, 2016 “Ethical nudges” through which a robot attempts to influence a \nhuman to adopt ethically-acceptable behavior \nOutward-oriented, \nproactive; ethics as \nmotivating factor \nMilli et al. (2017) \n \n \nTheoretical model of agents that use models of alters’ preferences \nto decide whether to obey a command \nExplicit, reactive, inward-\noriented; policy-based \ndeliberation \nate an inner conflict between the drive to rebel based \non motivating factors and the awareness of the antic-\nipated consequences of rebellion, leading to the possi-\nbility that the agent will endeavor (possibly through \ndeceptive practices) to minimize the social risk associ-\nated with its rebellion. A conflicted rebel agent would \nlikely use a combination of motivating, supporting,  \nand inhibiting factors to deliberate on whether to  \nrebel, and the interplay between these factors can  \ncause ethical issues. Such a situation is reflected in  \nexamples 6 and 7 in table 1. In conflicted rebel agents, \npre-rebellion can consist of the agent observing the  \nsocial behavior of other agents it interacts with, and \npost-rebellion can include trying to reestablish group \nharmony and trust after a rebellion episode. \nRebellion awareness (and, more generally, social  \nawareness) can also be reflected in how rebellion is  \nexpressed. Consider variants of the Furniture Mover  \nand Personal Assistant scenarios with socially aware  \nrebel agents. In example 9 in table 1, a subtler agent \nmight reason whether telling a particular alter that  \nthey “must be tired” could be interpreted as conde -\nscending commentary on the alter’s physical fitness. \nThe rebel agent in the Personal Assistant scenario  \nmight more sneakily respond to a request to order  \nunhealthy food with “Why not check the pantry  \nfirst? Maybe you have some left !” thus giving the  \nalter an opportunity to change their mind without  \nperceived loss of dignity. \nRebellion-aware agents might employ mechanisms \nsuch as the social planning of Pearce et al. (2014), in \nwhich planning knowledge and goals incorporate  \nbeliefs about other agents’ beliefs. \nSocial awareness has further implications for rebel \nagents. We earlier described the rebel-alter relation -\nship as one in which the alter is in a position of pow-\ner over the rebel. Heckhausen and Heckhausen  \n(2010) define power as “a domain-speciﬁc dyadic  \nrelationship that is characterized by the asymmetric  \ndistribution of social competence, access to  \nresources, and social status, and that is manifested in \nunilateral behavioral control.” The possible bases of  \nthat power include those influentially defined by  \nFrench and Raven (1959) for inter-human relation -\nships: legitimate power, reward power, coercive pow-\ner, referent power, and expert power. Notably, power \nsources have subjective components: one is subject  \nto the power of another if one believes oneself to be \nsubject to that power. For example, reward power is  \nbased on perceived “ability to mediate rewards” and \nreferent power is based on “identification with” the  \nindividual or group in the position of power. There -\nfore, power relationships depend on the agent’s  \nawareness of them. Hence, they may be meaningful  \nonly in the context of (at least somewhat) socially  \naware agents. Similarly, nonconforming rebellion  \nmay be meaningful only if the agent is aware of  \nsocial norms, the fact that it is breaking them, and  \nthe resulting implications. For example, we would  \nnot classify an embodied agent that bumps into peo-\nple due to faulty sensors, actuators, or pathfinding as \na nonconforming rebel. \nCounternarrative Intelligence \nNarrative intelligence is defined by Riedl (2016) as  \n“the ability to craft, tell, understand, and respond  \naffectively to stories”). As he and others note, narra-\ntive intelligence is not just for creating and enjoying \nfictional tales; it is essential to full intelligence,  \nincluding social behavior. It has a significant role to  \nplay in rebellion as well: arguably, any instance of  \nhuman rebellion, at any scale, is backed by a coun -\nternarrative to the narrative of the person, group, or \nnorms rebelled against. The conflicting parties  \nengage in what Abbot (2008) calls a “contest of nar -\nratives.” Complex AI rebel agents might also partici-\npate in such contests.  \nWe propose the term counternarrative intelligence to \nrefer to the ability of rebel agents to (1) produce alter-\nnative retellings or counterinterpretations, informed \nby subjective factors such as emotional appraisal, of  \nan alter’s narrative, or (2) to identify their own pre -\ngenerated narratives as being counternarratives in a  \ngiven context. Just as a rebel agent rebels in relation \nto an alter, a counternarrative exists in relation and  \ncontrast to a base narrative that it is a variant of and \nthat it challenges.  \nFor an example, we return to the Hiring Commit -\ntee scenario, and propose the following sequence of  \nevents: committee member A, who is a senior facul -\nty member and the head of the hiring committee,  \nextols the achievements of Candidate 1. A then  \ninvites candidate suggestions from the other com -\nmittee members. In turn, senior committee members \nB and C express appreciation of Candidate 1’s  \nachievements. Junior committee member D also  \nnominates Candidate 1. Then, junior committee  \nmember E mentions Candidate 2’s qualifications. A  \nagrees that Candidate 2 does indeed have notable  \nachievements. Then, A expresses his or her intention \nof making an offer to Candidate 1 and asks all other \ncommittee members whether they agree. One by  \none, all the other committee members express agree-\nment. Candidate 1 is nominated.  \nLet A’s base narrative for the episode be: “I offered \nall committee members the opportunity to select  \ntheir favorite candidate. Every opinion was taken  \ninto consideration. I then expressed my intention  \nand asked every single committee member if he or  \nshe agrees. They all did, so Candidate 1 was nomi -\nnated. The process was, therefore, conducted fairly.” \nThe rebel agent’s counternarrative, which express-\nes what the agent believes might be E’s perspective,  \nis: “A, who is the committee chair, expressed his or  \nher preference first. Then, he or she asked the other  \ncommittee members, starting with the senior ones,  \nto express their opinions. They all expressed the  \nsame opinion as A. E brought up Candidate 2, whose \nqualifications I believe to be at least as fitting for this \nposition. E’s suggestion was briefly discussed, in order \nfor the selection to appear fair, and then ignored. The \nprocess was conducted unfairly.” \nIn the example, the rebel agent’s narrative reflects \nits ability to empathize with human collaborators.  \nImagine, instead, that the rebel agent itself is accused \nof maliciously disturbing the hiring committee  \nprocess with no real evidence of any wrongdoing.  \nThe agent might (sincerely or deceptively) provide a \ncounternarrative that casts it as the supporter of indi-\nviduals with low social capital. Thus, counternarra -\ntives can be self-serving, but they can also support  \nsocial good, when they reflect empathy with varied  \nperspectives. \nWe propose several dimensions and types of coun-\nternarrative intelligence, which supplement the pre-\nviously introduced AI rebellion framework.  \nSincerity \nCounternarratives are sincere when they reflect the  \nagent’s genuine interpretation of a situation (that is, \nthey align with the agent’s beliefs, but possibly not  \nthe alter’s). An example of such alignment in our Hir-\ning Committee scenario would be a counternarrative \nthat is genuinely the product of the rebel’s reasoning \nthat (1) E has low capital and that (2) E’s suggestion  \nhas merits. Counternarratives are deceptive when  \nthey intentionally misrepresent the agent’s beliefs.  \nFor example, the rebel exclusively supports the inter-\nests of committee member E or of the candidate that \nE nominated, and the explanatory counternarrative  \nis meant to disguise the agent’s allegiance. We note  \nthat it is not required, in a narrative and counternar-\nrative pair, for one to be sincere and the other decep-\ntive. They can both be sincere (or deceptive), each  \nreflecting one agent’s appraisals and manipulations.  \nGeneration Time \nA priori counternarratives are generated before trig -\ngering rebellion. They can be instrumental in rebel -\nlion deliberation (for example, “This person or group \nof people is not given a fair chance in this environ -\nment, so I will rebel against the majority opinion”)  \nand serve as explanations in post-rebellion. A poste-\nriori counternarratives are generated after triggering  \nrebellion. For example, consider the variant of the  \nHiring Committee scenario in which the rebel  \nunconditionally supports committee member E, so  \nthat any situation in which E does not prevail trig -\ngers rebellion. After several such rebellion instances, \nthe agent is asked to justify its actions. It does so via \na counternarrative constructed on the spot, which  \nputs it in a sympathetic light. However, a posteriori  \ncounternarratives are not necessarily deceptive. They \ncould, for example, reflect the agent’s sincere  \nattempts to understand itself. Furthermore, narra -\ntives can be deceptive without being malicious (for  \nexample, in interactive storytelling, the purpose may \nbe to generate a believable, interesting (counter)back-\nstory, similar to the alibis (Li et al. 2014a) that a non-\nplayer character can use to give the impression of a  \nlife lived outside its interactions with a player).  \nDivergence Type \nThis dimension reflects how the counternarrative dif-\nArticles\n24    AI MAGAZINE\nArticles\nFALL 2018   25\nfers from the base narrative. Additive counternarra -\ntives contain additional events not in the base narra-\ntive, but no modifications of any of the events in the \nbase narrative. For example, let A’s base narrative be: \n“I asked each of my fellow committee members to  \nexpress their opinion.” An additive counternarrative \nwould be: “A expressed his or her own opinion first. \nThen he or she asked the other committee members \nto express their own opinions.” (The fact that A  \nexpressed his or her opinion first is significant if A  \nhas social influence over the other members of the  \ncommittee.) Interpretative counternarratives do not  \ndiffer from the base narrative in terms of sequence of \nevents, but give different interpretations to the  \nevents (for example, in terms of motivations and  \nemotions). For example, let A’s base narrative be:  \n“Everyone was asked to publicly voice their opinion, \nso as to give every suggestion a fair chance.” An inter-\npretative counternarrative might be: “Because all  \nopinions were publicly expressed, no one supported \nE’s opinion; and because E has low social capital, E  \nfelt pressured to support the majority opinion.”  \nTransformative counternarratives differ factually  \nfrom the base narrative, implicitly asserting that the \nbase narrative contains falsehoods. For example, if A’s \nbase narrative contains the statement “I expressed  \nmy opinion last,” a transformative counternarrative  \ncould instead assert that “A expressed his or her opin-\nion first.”  \nThere is a close connection between social aware -\nness and counternarrative intelligence. For example, \nan agent could sincerely believe a narrative, but iden-\ntify it as a counternarrative to other agents’ narra -\ntives, and deliberate on whether it would be socially \nadvisable to express it, or how to express it so as to  \nminimize social damage. This situation is similar to  \nthose in which agents that are not rebels reason that \ntheir behavior may appear rebellious to others. \nExisting work that can provide rebel agents with  \nvarious mechanisms of counternarrative intelligence \nincludes Holmes and Winston’s (2016) story-enabled \nhypothetical reasoning, in which narrative variants  \nare generated based on varied alignments, and Li et  \nal.’s (2014b) use of different communicative goals to \nprovide variation in narrative discourse and emo -\ntional content. \nConclusion \nWe argued that it is beneficial for certain AI agents to \nbe able to rebel for positive, defensible reasons in a  \nvariety of situations, and speculated that AI may nev-\ner become fully socially intelligent without noncom-\npliance abilities. We presented an AI rebellion frame-\nwork and discussed sociocognitive dimensions  \npertaining to it: rebellion awareness and counternar-\nrative intelligence. The framework is intended to  \ninspire, guide, and provide terminology for (1) the  \ndevelopment and study of rebel agents that serve  \npositive purposes, (2) systematic discussion of the  \nethics of AI rebellion (for, although we argue that AI \nrebellion can be positive, we recognize that it is not  \nnecessarily so), and (3) positive reframing of the AI  \nnoncompliance narrative within the research com -\nmunity and popular culture. \nAcknowledgements \nWe thank the editors and reviewers, our coauthors of \nprevious work on rebel agents, and all colleagues  \nwho have shown interest in the topic and offered  \ntheir feedback. The Personal Assistant scenario is  \nbased on a conversation with Jonathan Gratch. This \nresearch was performed while Alexandra Coman held \nan NRC Research Associateship award at the Naval  \nResearch Laboratory. \n \nNote \n1. Taken from What Is a Counternarrative?, www.reference. \ncom/art-literature/counternarrative-bac2eed0be17f281. \n \nReferences  \nAbbott, H. P. 2008. The Cambridge Introduction to Narrative.  \nCambridge, UK: Cambridge University Press. \nAgravante, D. J.; Cherubini, A.; Bussy, A.; and Kheddar, A.  \n2013. Human-Humanoid Joint Haptic Table Carrying Task  \nwith Height Stabilization Using Vision. In Proceedings of the \n2013 IEEE/RSJ International Conference on Intelligent Robots  \nand Systems, 4609–14. Piscataway, NJ: Institute for Electrical \nand Electronics Engineers. \nApker, T.; Johnson, B.; and Humphrey, L. 2016. LTL Tem -\nplates for Play-Calling Supervisory Control. In Proceedings of \nthe 54th AIAA Science and Technology Forum Exposition. Red \nHook, NY: Curran Associates, Inc. \nAsch, S. E. 1956. Studies of Independence and Conformity: \n1. A Minority of One Against a Unanimous Majority. Psy-\nchological Monographs: General and Applied 70(9): 1–70. \nBorenstein, J., and Arkin, R. 2016. Robotic Nudges: The  \nEthics of Engineering a More Socially Just Human Being. Sci-\nence and Engineering Ethics 22(1): 31–46. \nBriggs, G.; McConnell, I.; and Scheutz, M. 2015. When  \nRobots Object: Evidence for the Utility of Verbal, but Not  \nNecessarily Spoken Protest. In Social Robotics: Seventh Inter-\nnational Conference. Lecture Notes in Artificial Intelligence,  \n83–92. Berlin: Springer. \nBriggs, G., and Scheutz. M. 2015. “Sorry, I Can’t Do That”:  \nDeveloping Mechanisms to Appropriately Reject Directives \nin Human-Robot Interactions. In Artificial Intelligence for  \nHuman-Robot Interaction: Papers from the AAAI Fall Sympo -\nsium, edited by B. Hayes and M. Gombolay. Technical  \nReport FS-15-01. Palo Alto, CA: AAAI Press. \nCialdini, R. B., and Goldstein, N. J. 2004. Social Influence:  \nCompliance and Conformity. Annual Review of Psychology  \n55:591–621. Palo Alto, CA: Annual Reviews, Inc. \nComan, A., and Aha, D. W. 2017. Cognitive Support for  \nRebel Agents: Social Awareness and Counternarrative Intel-\nligence. In Proceedings of the Fifth Conference on Advances in  \nCognitive Systems. Palo Alto, CA: Cognitive Systems Foun -\ndation. \nComan, A.; Gillespie, K.; and Muñoz-Avila, H. 2015. Case-\nBased Local and Global Percept Processing for Rebel Agents. \nIn Workshop Proceedings from the 23rd International Conference \non Case-Based Reasoning, edited by J. Kendall-Morwick, 23–\n32. The CEUR Workshop 1520. Aachen, Germany: RWTH-\nAachen University. \nComan, A.; Johnson, B.; Briggs, G.; and Aha, D. W. 2017.  \nSocial Attitudes of AI Rebellion: A Framework. In AI, Ethics, \nand Society: Papers from the 2017 Workshop, edited by T.  \nWalsh. AAAI Technical Report WS-17-02. Palo Alto, CA:  \nAAAI Press. \nFloyd, M. W., and Aha, D. W. 2016. Incorporating Trans -\nparency During Trust-Guided Behavior Adaptation. In Pro-\nceedings of the 24th International Conference on Case-Based  \nReasoning, 124–38. Berlin: Springer. \nFrench, J. R. P., and Raven, B. 1959. The Bases of Social Pow-\ner. Reprinted in Classics of Organization Theory, 4th ed., edit-\ned by J. Shafritz and J. S. Ott. New York: Harcourt Brace. \nGamson, W.A. 1992. Talking Politics. New York: Cambridge \nUniversity Press. \nGregg-Smith, A., and Mayol-Cuevas, W. W. 2015. The  \nDesign and Evaluation of a Cooperative Handheld Robot.  \nIn 2015 IEEE International Conference on Robotics and  \nAutomation. Piscataway, NJ: Institute for Electrical and Elec-\ntronics Engineers. \nHeckhausen, J. E., and Heckhausen, H. E. 2010. Motivation \nand Action. Cambridge, UK: Cambridge University Press. \nHiatt, L. M.; Harrison, A. M.; and Trafton, J. G. 2011.  \nAccommodating Human Variability in Human-Robot  \nTeams Through Theory of Mind. In Proceedings of the 22nd \nInternational Joint Conference on Artiﬁcial Intelligence,  2066–\n71. Palo Alto, CA: AAAI Press. \nHolmes, D., and Winston, P. 2016. Story-Enabled Hypo -\nthetical Reasoning. In Proceedings of the Fourth Annual Con-\nference on Advances in Cognitive Systems. Palo Alto, CA: Cog-\nnitive Systems Foundation. \nLi, B.; Thakkar, M.; Wang, Y.; and Riedl, M. O. 2014a. Data-\nDriven Alibi Story Telling for Social Believability. Paper pre-\nsented at the 2014 Social Believability in Games Workshop. \nFt. Lauderdale, FL, April 4. \nLi, B.; Thakkar, M.; Wang, Y.; and Riedl, M. O. 2014b. Sto -\nrytelling with Adjustable Narrator Styles and Sentiments. In \nInteractive Storytelling: Proceedings of the Seventh International \nConference on Interactive Digital Storytelling,  1–12. Berlin:  \nSpringer. \nMilli, S.; Hadfield-Menell, D.; Dragan, A.; and Russell, S.  \n2017. Should Robots Be Obedient? arXiv preprint: arX -\niv:1705.09990[cs.AI]. Ithaca, NY: Cornell University Library. \nMoerland, T.; Broekens, J.; and Jonker, C. 2016. Fear and  \nHope Emerge from Anticipation in Model-Based Reinforce-\nment Learning. In Proceedings of the 25th International Joint  \nConference on Artificial Intelligence, 848–54. Palo Alto, CA:  \nAAAI Press. \nPatil, A.; Liu, J.; Price, B.; Sharara, H.; and Brdiczka, O. 2012. \nModeling Destructive Group Dynamics in Online Gaming  \nCommunities. In Proceedings of the Sixth International AAAI  \nConference on Weblogs and Social Media , 290–97. Palo Alto,  \nCA: AAAI Press. \nPearce, C.; Meadows, B. L.; Langley, P.; and Barley, M. 2014. \nSocial Planning: Achieving Goals by Altering Others’ Men-\ntal States. In Proceedings of the 28th AAAI Conference on Arti-\nficial Intelligence, 402–9. Palo Alto, CA: AAAI Press. \nRiedl, M. O. 2016. Computational Narrative Intelligence: A \nHuman-Centered Goal for Artificial Intelligence. arXiv  \npreprint: arXiv:1602.06484[cs.AI]. Ithaca, NY: Cornell Uni-\nversity Library. \nVan Stekelenburg, J., and Klandermans, B. 2013. The Social \nPsychology of Protest. Current Sociology 61(5–6): 886–905.  \ndoi.org/10.1177/0011392113479314. \nVattam, S.; Klenk, M.; Molineaux, M.; and Aha, D. W. 2013. \nBreadth of Approaches to Goal Reasoning: A Research Sur -\nvey. In Goal Reasoning: Papers from the ACS Workshop, edited \nby D. W. Aha, M. T. Cox, and H. Muñoz-Avila. Technical  \nReport CS-TR-5029. College Park, MD: University of Mary -\nland. \nWenar, C. 1982. On Negativism. Human Development 25(1): \n1–23. \nWilson, J. R.; Arnold, T.; and Scheutz, M. 2016. Relational  \nEnhancement: A Framework for Evaluating and Designing  \nHuman-Robot Relationships. In AI, Ethics, and Society: Papers \nfrom the 2016 AAAI Workshop, edited by B. Bonet, S. Koenig, \nB. Kuipers, I. Nourbakhsh, S. Russell, M. Vardi, and T. Walsh. \nAAAI Technical Report WS-16-02. Palo Alto, CA: AAAI Press. \nWright S. C.; Taylor D. M.; and Moghaddam, F. M. 1990.  \nThe Relationship of Perceptions and Emotions to Behavior  \nin the Face of Collective Inequality. Social Justice Research  \n4(3): 229–50. \n \n \nAlexandra Coman (PhD, Lehigh University, 2013) is a sen-\nior manager at Capital One. She was previously an NRC  \npostdoctoral research associate with the Adaptive Systems  \nSection at NRL in Washington, DC, and an assistant profes-\nsor of computer science at Ohio Northern University. Her  \nresearch experience and interests include cognitive archi -\ntectures, AI planning, case-based reasoning, goal reasoning, \nAI ethics, affective computing, and narrative intelligence.  \n \nDavid W. Aha (PhD, University of California, Irvine, 1990) \nleads NRL’s Adaptive Systems Section in Washington, DC.  \nHis interests include mixed-initiative intelligent agents (for \nexample, that employ goal reasoning models), deliberative \nautonomy, machine learning, and case-based reasoning,  \namong other topics. He has co-organized 35 events on these \ntopics (such as ICCBR-17), hosted 13 post-doctoral  \nresearchers, served on 20 PhD committees, created the UCI \nRepository for ML Databases, was a AAAI Councilor, and  \ngave the Robert S. Engelmore Memorial Lecture at IAAI-17. \nArticles\n26    AI MAGAZINE"
}