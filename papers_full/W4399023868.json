{
    "title": "Evaluating Large Language Models for Automated Reporting and Data Systems Categorization: Cross-Sectional Study",
    "url": "https://openalex.org/W4399023868",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2310809835",
            "name": "Qingxia Wu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2310809835",
            "name": "Qingxia Wu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2125244509",
            "name": "Hua-Li Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1484673654",
            "name": "Yan Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098074341",
            "name": "Yan Bai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2126924781",
            "name": "Yaping Wu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114866138",
            "name": "Xuan Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100927492",
            "name": "Xiaodong Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098627543",
            "name": "Pei Dong",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Jon Xue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2150708589",
            "name": "Dinggang Shen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2107492962",
            "name": "Meiyun Wang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4367311208",
        "https://openalex.org/W4376640725",
        "https://openalex.org/W4381480701",
        "https://openalex.org/W4384626331",
        "https://openalex.org/W4383186888",
        "https://openalex.org/W4380422747",
        "https://openalex.org/W4362522726",
        "https://openalex.org/W4366462753",
        "https://openalex.org/W4389047577",
        "https://openalex.org/W4376640706",
        "https://openalex.org/W4380995257",
        "https://openalex.org/W4385230595",
        "https://openalex.org/W4386867830",
        "https://openalex.org/W2893608205",
        "https://openalex.org/W4387793622",
        "https://openalex.org/W4206742016",
        "https://openalex.org/W4386674819",
        "https://openalex.org/W2008979634",
        "https://openalex.org/W2770764122",
        "https://openalex.org/W4380841691",
        "https://openalex.org/W4386624292",
        "https://openalex.org/W2970017153"
    ],
    "abstract": "Background Large language models show promise for improving radiology workflows, but their performance on structured radiological tasks such as Reporting and Data Systems (RADS) categorization remains unexplored. Objective This study aims to evaluate 3 large language model chatbots—Claude-2, GPT-3.5, and GPT-4—on assigning RADS categories to radiology reports and assess the impact of different prompting strategies. Methods This cross-sectional study compared 3 chatbots using 30 radiology reports (10 per RADS criteria), using a 3-level prompting strategy: zero-shot, few-shot, and guideline PDF-informed prompts. The cases were grounded in Liver Imaging Reporting &amp; Data System (LI-RADS) version 2018, Lung CT (computed tomography) Screening Reporting &amp; Data System (Lung-RADS) version 2022, and Ovarian-Adnexal Reporting &amp; Data System (O-RADS) magnetic resonance imaging, meticulously prepared by board-certified radiologists. Each report underwent 6 assessments. Two blinded reviewers assessed the chatbots’ response at patient-level RADS categorization and overall ratings. The agreement across repetitions was assessed using Fleiss κ. Results Claude-2 achieved the highest accuracy in overall ratings with few-shot prompts and guideline PDFs (prompt-2), attaining 57% (17/30) average accuracy over 6 runs and 50% (15/30) accuracy with k-pass voting. Without prompt engineering, all chatbots performed poorly. The introduction of a structured exemplar prompt (prompt-1) increased the accuracy of overall ratings for all chatbots. Providing prompt-2 further improved Claude-2’s performance, an enhancement not replicated by GPT-4. The interrun agreement was substantial for Claude-2 (k=0.66 for overall rating and k=0.69 for RADS categorization), fair for GPT-4 (k=0.39 for both), and fair for GPT-3.5 (k=0.21 for overall rating and k=0.39 for RADS categorization). All chatbots showed significantly higher accuracy with LI-RADS version 2018 than with Lung-RADS version 2022 and O-RADS (P&lt;.05); with prompt-2, Claude-2 achieved the highest overall rating accuracy of 75% (45/60) in LI-RADS version 2018. Conclusions When equipped with structured prompts and guideline PDFs, Claude-2 demonstrated potential in assigning RADS categories to radiology cases according to established criteria such as LI-RADS version 2018. However, the current generation of chatbots lags in accurately categorizing cases based on more recent RADS criteria.",
    "full_text": null
}