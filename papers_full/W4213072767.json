{
  "title": "RoBERTa-LSTM: A Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network",
  "url": "https://openalex.org/W4213072767",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Tan, Kian Long",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A3203032335",
      "name": "Lee Chin Poo",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": null,
      "name": "Sonai Muthu Anbananthen, Kalaiarasi",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A2528248902",
      "name": "Lim Kian Ming",
      "affiliations": [
        "Multimedia University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3135620065",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3006588007",
    "https://openalex.org/W3036246613",
    "https://openalex.org/W2560010249",
    "https://openalex.org/W6761165616",
    "https://openalex.org/W3134051805",
    "https://openalex.org/W3131290958",
    "https://openalex.org/W3018886259",
    "https://openalex.org/W2971403610",
    "https://openalex.org/W2944668906",
    "https://openalex.org/W3000315977",
    "https://openalex.org/W3128126711",
    "https://openalex.org/W3139140319",
    "https://openalex.org/W3011753406",
    "https://openalex.org/W6685053522",
    "https://openalex.org/W2786273134",
    "https://openalex.org/W2251658415",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W6676984168",
    "https://openalex.org/W2290086728",
    "https://openalex.org/W3156048886",
    "https://openalex.org/W3134820410",
    "https://openalex.org/W3186997021",
    "https://openalex.org/W2962933419",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2928259460"
  ],
  "abstract": "Due to the rapid development of technology, social media has become more and more common in human daily life. Social media is a platform for people to express their feelings, feedback, and opinions. To understand the sentiment context of the text, sentiment analysis plays the role to determine whether the sentiment of the text is positive, negative, neutral or any other personal feeling. Sentiment analysis is prominent from the perspective of business or politics where it highly impacts the strategic decision making. The challenges of sentiment analysis are attributable to the lexical diversity, imbalanced dataset and long-distance dependencies of the texts. In view of this, a data augmentation technique with GloVe word embedding is leveraged to synthesize more lexically diverse samples by similar word vector replacements. The data augmentation also focuses on the oversampling of the minority classes to mitigate the imbalanced dataset problems. Apart from that, the existing sentiment analysis mostly leverages sequence models to encode the long-distance dependencies. Nevertheless, the sequence models require a longer execution time as the processing is done sequentially. On the other hand, the Transformer models require less computation time with parallelized processing. To that end, this paper proposes a hybrid deep learning method that combines the strengths of sequence model and Transformer model while suppressing the limitations of sequence model. Specifically, the proposed model integrates Robustly optimized BERT approach and Long Short-Term Memory for sentiment analysis. The Robustly optimized BERT approach maps the words into a compact meaningful word embedding space while the Long Short-Term Memory model captures the long-distance contextual semantics effectively. The experimental results demonstrate that the proposed hybrid model outshines the state-of-the-art methods by achieving F1-scores of 93%, 91%, and 90% on IMDb dataset, Twitter US Airline Sentiment dataset, and Sentiment140 dataset, respectively.",
  "full_text": "Received January 29, 2022, accepted February 16, 2022, date of publication February 18, 2022, date of current version March 2, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3152828\nRoBERTa-LSTM: A Hybrid Model for Sentiment\nAnalysis With Transformer and\nRecurrent Neural Network\nKIAN LONG TAN, CHIN POO LEE\n, KALAIARASI SONAI MUTHU ANBANANTHEN\n,\nAND KIAN MING LIM\n, (Senior Member, IEEE)\nFaculty of Information Science and Technology, Multimedia University, Melaka 75450, Malaysia\nCorresponding author: Chin Poo Lee (cplee@mmu.edu.my)\nThis work was supported in part by the Fundamental Research Grant Scheme of the Ministry of Higher Education under Award\nFRGS/1/2019/ICT02/MMU/03/7, and in part by the Multimedia University Internal Research Fund under Grant MMUI/210112.\nABSTRACT Due to the rapid development of technology, social media has become more and more common\nin human daily life. Social media is a platform for people to express their feelings, feedback, and opinions.\nTo understand the sentiment context of the text, sentiment analysis plays the role to determine whether\nthe sentiment of the text is positive, negative, neutral or any other personal feeling. Sentiment analysis\nis prominent from the perspective of business or politics where it highly impacts the strategic decision\nmaking. The challenges of sentiment analysis are attributable to the lexical diversity, imbalanced dataset and\nlong-distance dependencies of the texts. In view of this, a data augmentation technique with GloVe word\nembedding is leveraged to synthesize more lexically diverse samples by similar word vector replacements.\nThe data augmentation also focuses on the oversampling of the minority classes to mitigate the imbalanced\ndataset problems. Apart from that, the existing sentiment analysis mostly leverages sequence models to\nencode the long-distance dependencies. Nevertheless, the sequence models require a longer execution time\nas the processing is done sequentially. On the other hand, the Transformer models require less computation\ntime with parallelized processing. To that end, this paper proposes a hybrid deep learning method that\ncombines the strengths of sequence model and Transformer model while suppressing the limitations of\nsequence model. Speciﬁcally, the proposed model integrates Robustly optimized BERT approach and Long\nShort-Term Memory for sentiment analysis. The Robustly optimized BERT approach maps the words into a\ncompact meaningful word embedding space while the Long Short-Term Memory model captures the long-\ndistance contextual semantics effectively. The experimental results demonstrate that the proposed hybrid\nmodel outshines the state-of-the-art methods by achieving F1-scores of 93%, 91%, and 90% on IMDb\ndataset, Twitter US Airline Sentiment dataset, and Sentiment140 dataset, respectively.\nINDEX TERMS Sentiment, transformer, RoBERTa, long short-term memory, LSTM, recurrent neural\nnetwork, RNN.\nI. INTRODUCTION\nSentiment analysis is a process that studies the emotion,\nsentiment, and attitude of people from their written lan-\nguage. Sentiment analysis is one of the famous topics in the\nNatural Processing Language (NLP) ﬁeld due to its impor-\ntance [1]. The inﬂuence of sentiment analysis has penetrated\nbusiness and even social media nowadays. Due to the rapid\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Zhouyang Ren\n .\ndevelopment of social media, everyone can express their\nfeelings and opinions on the internet. Hence, sentiment anal-\nysis plays a vital role in understanding what consumers or\nreviewers think. Apart from that, sentiment analysis also acts\nas an important tool for investigating the public reaction in the\npolitical aspect. The sentiment of people’s voice inﬂuences\nthe decisions made by the political parties.\nThe sentiment analysis is challenging mainly due to the\nlong-distance dependencies and lexical diversity of texts.\nMany machine learning methods were proposed for sentiment\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 21517\nK. L. Tanet al.: RoBERTa-LSTM: Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network\nanalysis, especially sequence models that are able to encode\nthe long-distance dependencies of the text. The sequence\nmodels are however less computationally efﬁcient where\nthe processing is serialized. In contrast, the Transformer\nmodels improve the computation by implementing paral-\nlelized processing. In view of this, a hybrid deep learning\nmodel that leverages the strengths of Transformer models [2]\nand sequence models is proposed. To be more speciﬁc,\nthe proposed method integrates Robustly optimized BERT\napproach (RoBERTa) [3] from the Transformer family and\nLong Short-Term Memory (LSTM) [4] from the Recurrent\nNeural Networks family. The RoBERTa models stand out\nin sequence-to-sequence modeling to effectively generate\nrepresentative word embedding for the texts. On the other\nhand, the LSTM excels in temporal modeling to encode the\nlong-distance dependency of the given input. Apart from\nthat, the data augmentation with synonym replacements are\nincorporated to enrich the lexical diversity of the corpus. The\ndata augmentation also alleviates the skewed dataset problem.\nThe contributions of this paper are three folds :\n1) A hybrid deep learning model is proposed for sen-\ntiment analysis that incorporates the RoBERTa and\nLSTM model. The RoBERTa model serves the purpose\nof word or subword tokenization and word embed-\nding generation, whereas the LSTM model encodes\nthe long-distance temporal dependencies in the word\nembedding.\n2) A data augmentation technique with pre-trained word\nembedding is leveraged to synthesize lexically diverse\nsamples and to oversample the minority classes.\nIn doing so, the generalization ability of the model is\nimproved with more lexically rich training samples as\nwell as the imbalanced dataset problem is solved.\n3) The hyperparameter tuning is performed to determine\nthe optimal hyperparameter settings that yield the\nsuperior results for sentiment analysis. The empiri-\ncal results show that the proposed RoBERTa-LSTM\nmethod records a huge leap in the performance com-\npared to the state-of-the-art methods.\nThis paper is structured into seven sections. Section I\ngives some background of the research and outlines the\ncontributions of this work. The existing methods for sen-\ntiment analysis are described in Section II. Section III\npresents the processes, architecture, and technical details\nof the proposed RoBERTa-LSTM. Section IV speciﬁes\nthe sentiment analysis datasets in the performance eval-\nuation. The hyperparameter tuning details are provided\nin Section V. Section VI presents the experimental results\nand analysis of the proposed method in comparison with\nthe state-of-the-art methods. Finally, Section VII concludes\nthe paper.\nII. RELATED WORKS\nThis section describes the state-of-the-art methods for senti-\nment analysis. The methods can be broadly categorized into\nmachine learning and deep learning methods.\nA. MACHINE LEARNING\nThe authors in [5] compared three machine learning meth-\nods, including Naïve Bayes, Support Vector Machine (SVM)\nand K-Nearest Neighbour (KNN). The dataset was crawled\nfrom Twitter related to the 2019 presidential candidates of\nthe Republic of Indonesia. The data samples were labeled\nas positive and negative classes. Some preprocessing tech-\nniques, namely text parsing, tokenization, and text mining\nwere done before the training process. The dataset was split\ninto 80% for training and 20% for testing. The paper shows\nthat the Naïve Bayes method obtained the highest accuracy\nof 75.58%, followed by KNN with 73.34% and SVM with\n63.99%.\nLikewise, the authors in [6] compared the performance\nof Multinomial Naïve Bayes and Support Vector Classi-\nﬁer (SVC) with linear kernels in sentiment analysis. The\ndataset used was taken from Twitter which is related to\nAirline reviews. The dataset contains approximately 10K\ntweets from positive, negative, and neutral classes. First, the\npreprocessing techniques, such as stemming, URLs removal,\nand stop words removal, were applied to clean the data. In the\nwork, 67% of data is used as the training data and 33% of data\nis used as the testing data. In the experiments, SVC achieved\n82.48% accuracy while Multinomial Naïve Bayes obtained\n76.56% accuracy.\nIn [7], the authors performed feature extraction to extract\nthe useful features from the tweets before passing into\nMultinomial Naïve Bayes for sentiment analysis. They per-\nformed sentiment analysis on the sentiment140 dataset. The\ndataset contains 1,600,000 tweets that were labeled with three\nclasses, i.e. positive, negative, and neutral. The sentiment140\ndataset was split into several ratios in the experiments, which\nare 6:4 for experiment 1, 7:3 for experiment 2, 8:2 for\nexperiment 3 and 9:1 for experiment 4. The Multinomial\nNaïve Bayes method recorded the highest accuracy of 85%\nin experiment 4.\nThe work [8] compared four machine learning methods\nin the sentiment analysis. The dataset used is the Indian\nRailways Case Study that was collected from Twitter. The\ndata samples were labeled with positive, negative or neu-\ntral. The experimental results showed that the accuracy\nachieved by C4.5, Naïve Bayes, Support Vector Machine,\nand Random Forest are 89.5%, 89%, 91.5%, and 90.5%,\nrespectively.\nThe authors of [9] proposed an AdaBoost model for\nthe US Airline Twitter data sentiment analysis. Data pre-\nprocessing was performed to eliminate the unnecessary\ndata from the text. Besides, some data mining skills\nwere applied to understand the relationship among the\nelements in the dataset. The dataset was collected from\nSkytrax and Twitter using keywords which are related to\nthe top 10 US Airlines. In the experiments, 75% of the\ndata was allocated for training and 25% was allocated\nfor testing. The authors combined the boosting and bag-\nging methods in AdaBoost to obtain the highest F-score\nof 68%.\n21518 VOLUME 10, 2022\nK. L. Tanet al.: RoBERTa-LSTM: Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network\nThe research work [10] used six machine learning models\nfor the sentiment analysis of the US Airlines twitter data.\nPreprocessing steps including stop word removal, punctua-\ntion removal, case folding, and stemming were performed.\nThe Bag of Words was used in feature extraction. The\ndataset used was collected from CrowdFlower and Kaggle,\nwhich are related to six US Airlines. The dataset contains\n14640 samples with three classes, i.e., positive, negative, and\nneutral. The dataset was split into 70% training and 30%\ntesting. The accuracy achieved by Support Vector Machine,\nLogistic Regression, Random Forest, XgBoost, Naïve Bayes,\nand Decision Tree are 83.31%, 81.81%, 78.55%, 75.93%, and\n73%, and 70.51%, respectively.\nB. DEEP LEARNING\nThe authors in [11] proposed two deep learning methods\nfor sentiment analysis of the multilingual social media text.\nThe dataset was taken from Twitter during the general elec-\ntion of Pakistan in 2018. The dataset contains 20375 tweets\nin two languages, i.e., English and Roman Urdu. The data\nsamples were classiﬁed into positive, negative, and neutral\nclasses. The dataset was split into 80% for training and\n20% for testing. The authors evaluated the performance of\ntwo Bidirectional Encoder Representations from Transform-\ners (BERT), speciﬁcally Multilingual BERT (mBERT) and\nXLM-RoBERTa (XLM-R). In the hyperparameter tuning, the\nlearning rate of mBERT was set to 2e-5 whereas the learning\nrate of XLM-R was 2e-6. The experimental results showed\nthat mBERT obtained 69% accuracy and XLM-R recorded\n71% accuracy.\nA character-based Deep Bidirectional Long Short-Term\nMemory (DBLSTM) method was leveraged for sentiment\nanalysis of self-collected Tamil tweets in [12]. The dataset\nconsists of 1500 tweets from the positive, negative, and neu-\ntral classes. Firstly, data preprocessing was done to remove\nunnecessary symbols, special characters, and numbers in the\ntext. The cleaned data was then represented by the word\nembedding of DBLSTM which is based on the Word2Vec\npre-trained model. The dataset was divided into 80% for\ntraining and 20% for testing. The DBLSTM method recorded\nan accuracy of 86.2% in the experiments.\nThe paper [13] performed sentiment analysis on movie\nreviews taken from IMDb. The dataset contains 3000 reviews\nwith positive or negative labels. Some preprocessing steps\nwere carried out to remove less meaningful characters, sym-\nbols, repeating words, and stop words. After that, CountVec-\ntorizer was used in the feature extraction. After that, the\nfeatures were fed into several models for training and test-\ning. The dataset was split into 75% for training and 25%\nfor testing. Among Naïve Bayes, Support Vector Machine,\nLogistic Regression, K-Nearest Neighbour, Ensemble model,\nand Convolutional Neural Network (CNN), the CNN model\nrecords the highest accuracy of 99.33%.\nIn [14], the sentiment of the Saudi dialect was ana-\nlyzed with a deep learning approach. The dataset com-\nprises 60000 Saudi tweets of positive and negative class. Data\npreprocessing steps were done to remove the numbers, punc-\ntuation, special symbols and non-Arabic letters. Besides that,\ntext normalization was also applied where the word forms\nwere replaced by their lemmas. Subsequently, Word2Vec\npre-trained models were adopted as the word embedding to\nencode the text. The data was randomly split into 70% for\ntraining and 30% for testing. The data was then trained and\ntested on Long-Short Term Memory (LSTM) and Bidirec-\ntional Long-Short Term Memory (Bi-LSTM). The empirical\nresults suggest that the Bi-LSTM model performed better\nwith 94% accuracy.\nThe authors in [15] proposed the residual learning by\nusing 1-dimensional CNN (1D-CNN) and Recurrent Neural\nNetworks for sentiment analysis. The dataset was taken from\nthe IMDb dataset which contains 50000 movie reviews from\npositive and negative class. The data was split into 50% for\ntraining and 50% for testing. The convolutional layers of the\n1D-CNN have 128 ﬁlters and 256 ﬁlters. The Recurrent Neu-\nral Networks layers contain 128 units for LSTM, Bi-LSTM\nand Gated Recurrent Unit (GRU). The experimental results\ndemonstrated that the 1D-CNN with GRU model yielded the\nhighest accuracy of 90.02%.\nLikewise, [16] proposed a Sentiment Analysis Bidirec-\ntional Long-Short Term Memory (SAB-LSTM). The model\nconsists of 196 Bi-LSTM units, 128 Embedding layers,\n4 dense layers and classiﬁcation layer with SoftMax acti-\nvation function. The paper suggested that the additional\nlayers can avoid the overﬁtting problem and optimize the\nmodel parameters dynamically. The dataset used consists of\n80689 samples from ﬁve sentiment classes that were col-\nlected from social media reviews such as Twitter, YouTube,\nFacebook, news articles, etc. The dataset was split into 90%\nfor training and 10% for testing. The experiments revealed\nthat the SAB-LSTM model performed better than the com-\nmon LSTM models.\nThe work [17] compared the performance of Support\nVector Machine, Multinomial Naïve Bayes, LSTM, and\nBidirectional Encoder Representations from Transform-\ners (BERT) in sentiment analysis. Some preprocessing steps\nwere applied, including tokenization, stemming, lemmatiza-\ntion, and stop words and punctuation removal. The dataset\nused consists of 1.6 million tweets with positive or negative\nclass. The dataset was split into 80% for training and 20% for\ntesting. The study concluded that BERT performed the best\nwith 85.4% accuracy.\nThe authors of [18] adopted LSTM to deal with the sen-\ntiment analysis on 5000 Tweets in Bangla. The dataset was\ncleaned by space and punctuation removal. In the experi-\nments, the dataset was split into 80% for training, 10% for\ntraining, and 10% for testing. After hyperparameter tuning,\nthe architecture of 5 LSTM layers of size 128, batch size of\n25 and learning rate of 0.0001 yielded the highest accuracy\nof 86.3%.\nTable 1 provides a summary of the related works, with the\nmethods and datasets used. Most existing works leveraged\nmachine learning methods and recurrent neural networks.\nVOLUME 10, 2022 21519\nK. L. Tanet al.: RoBERTa-LSTM: Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network\nTABLE 1. Summary of Related Works.\nThe Transformers models are yet to be widely explored.\nMoreover, recurrent neural networks and Transformers mod-\nels have their own strengths. Recurrent neural networks\nperform well in encoding long-range dependencies while\nTransformers improves the computation by parallelized\nprocessing. Hence, this paper explores the integration of\nTransformers and recurrent neural networks.\nIII. SENTIMENT ANALYSIS WITH RoBERTa-LSTM\nThis section describes the phases of the proposed\nRoBERTa-LSTM for sentiment analysis. Firstly, prepro-\ncessing steps are performed on the corpus to remove the\nunnecessary tokens or symbols in the text. Subsequently,\ndata augmentation is carried out to address the imbalanced\ndataset problem. The data augmentation oversamples the\nminority class to make the sample size equally large as\nthe majority class. The balanced dataset is then passed into\nthe RoBERTa-LSTM model for training and classiﬁcation.\nThe proposed model is the hybrid of Robustly optimized\nBERT approach (RoBERTa) [3] and Long Short-Term Mem-\nory (LSTM), referred to as the RoBERTa-LSTM model. The\nproposed model utilizes the pre-trained RoBERTa weights to\nefﬁciently map the tokens into meaningful embedding space.\nThe output word embeddings are then fed into the LSTM to\ncapture the salient semantic features.\nA. DATA PREPROCESSING\nIn text analytics applications, data preprocessing is essential\nto remove the noise in the corpus. In this work, several prepro-\ncessing steps are performed. Firstly, the texts are standardized\ninto lowercase. Subsequently, texts that are less necessary\nin the sentiment analysis, including stop words, numbers,\npunctuation, and special symbols are eliminated. Stop words\nare the common words, such as a, an, the, etc., that are\nsyntactically important but semantically less important. Apart\nfrom that, stemming is applied to change the words into their\nlemmas. For example, the word ‘collected’ is converted into\n‘collect’ after the stemming operation.\nB. DATA AUGMENTATION\nData augmentation increases the sample size by synthesizing\nadditional lexically similar samples from the original corpus.\nIn doing so, the deep learning models perform training on\nmore data samples, hence improving the generalization capa-\nbility. The data augmentation method can also be used to\noversample the minority classes in an imbalanced dataset.\nThere are a few data augmentation techniques in the text\nanalytics ﬁeld, namely Thesaurus [19], text generation [20],\nword embedding [21], etc. Thesaurus is an efﬁcient text\naugmentation technique that uses synonyms to substitute the\nwords or phrases. The text generation technique synthesizes\na new sentence to replace the original sentence. The word\nembedding technique adopts K-nearest-neighbor (KNN) and\ncosine similarity in the word embedding to substitute the\nword vector with the closest word vector. Several popular pre-\ntrained word embedding, such as Global Vectors for Word\nRepresentation (GloVe), Word2Vec and fastText, are widely\nleveraged in the text analytics ﬁeld. In this research, the data\naugmentation with pre-trained word embedding, GloVe is\napplied due to its computation efﬁciency. As Twitter US Air-\nline Sentiment dataset is imbalanced, the data augmentation\nis mainly applied on the dataset to oversample the minority\nclasses.\nC. RoBERTa-LSTM\nThis subsection gives a brief background of RoBERTa\nand explains the phases in the proposed RoBERTa-LSTM\nmodel. Figure 1 illustrates the architecture of the proposed\nRoBERTa-LSTM model.\n1) RoBERTa\nThe RoBERTa model is an extension of Bidirectional\nEncoder Representation from Transformers (BERT). The\nBERT and RoBERTa fall under the Transformers [2] family\nthat was developed for sequence-to-sequence modeling to\naddress the long-range dependencies problem.\nTransformer models comprise three components, namely\ntokenizer, transformers, and heads. The tokenizer converts\nthe raw text into the sparse index encodings. Then, the trans-\nformers reform the sparse content into contextual embedding\nfor deeper training. The heads are implemented to wrap the\ntransformers model so that the contextual embedding can\nbe used for the downstream tasks. The components of the\nTransformers are depicted in Figure 2.\nBERT is slightly different from the existing language mod-\nels where it can learn the contextual representation from\nboth ends of the sentences. For the tokenization part, BERT\nused 30K vocabulary of character level Byte-Pair Encoding.\nIn contrast, RoBERTa used a byte-level Byte-Pair Encoding\nwith a larger vocabulary set that consists of 50K subword\nunits. Apart from that, the RoBERTa model ﬁne tunes the\nBERT model by training on more data, longer sequences, and\nlonger time. RoBERTa was trained on 4 different corpora,\nas follows:\n21520 VOLUME 10, 2022\nK. L. Tanet al.: RoBERTa-LSTM: Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network\nFIGURE 1. The architecture of the proposed RoBERTa-LSTM model.\nFIGURE 2. The components of the Transformers model.\n1) BookCorpus + English Wikipedia: This dataset was\nalso used to train BERT. The BookCorpus was released\nby Zhu et al.in 2015 [22].\n2) CC-News: This dataset contains 63 million English\nnews articles which were taken from CommonCrawl\nNews.\n3) OpenWebText: This is an open-source recreation which\nwas released by Gokaslan and Cohen in 2019. It con-\ntains 38 GB of data.\n4) Stories: Trinh and Le published this dataset in\n2018 which includes 31GB of CommonCrawl data that\nwas ﬁltered to match the story-like style of Winograd\nschemas.\nIn the proposed RoBERTa-LSTM model, the cleaned text\nis ﬁrst tokenized into words or subwords to be easily encoded\ninto word embeddings. In this work, the RoBERTa tokenizer\nis used. The RoBERTa tokenizer has some special tokens,\nsuch as the <s> and </s> tokens to indicate the beginning and\nend of the sentence, <pad> token to pad the text to reach the\nmaximum length of the word vector.\nIn the RoBERTa model, the byte-level Byte-Pair Encoding\ntokenizer is leveraged to partition the text into subwords. With\nthis tokenizer, the frequently used words will not be split.\nHowever, the words that are rarely used will be split into\nsubwords. For instance, the word ‘Transformers’ will be split\ninto ‘Transform’ and ‘ers’.\nTo make the model understand the text, the words need\nto be translated into a meaningful numerical representation.\nThe RoBERTa tokenizer encodes the raw text with input\nids and attention mask. The input ids represent the token\nindices and numerical representation of the token. On the\nother hand, the attention mask is used as an optional argu-\nment to batch the sequence together. The attention mask\nindicates which tokens should be attended and which should\nnot.\nThe input ids and attention mask are passed into the\nRoBERTa base model. There are 12 RoBERTa base layers,\n768 hidden state vectors, and 125 million parameters in the\nRoBERTa base model. The RoBERTa base layers aim to\ncreate a meaningful word embedding as the feature represen-\ntation so that the following layers can easily capture the useful\ninformation from the word embedding.\n2) LONG SHORT-TERM MEMORY\nSubsequently, the output from the dropout layer is fed into\nthe Long Short-Term Memory (LSTM) [4] model. The LSTM\nmodel is able to store the previous information thus capturing\nthe prominent long-range dependencies in the given input.\nLSTM has performed signiﬁcantly in sequence modeling\ntasks, such as text classiﬁcation, sentiment analysis, time\nseries prediction, etc.\nThere are three important elements in the LSTM model,\nincluding forget gate, input gate, and output gate. The forget\ngate will decide to forget or discard irrelevant information\nfrom the previous cell state and new input data. A sigmoid\nfunction is used in the training to return the values between\n[0, 1]. A value close to zero means that the information is\nless important to remember. The input gate plays the role of\na ﬁlter to decide which information is worth remembering,\nthus to be updated into the next state. The value close to zero\nmeans that it is less important to be updated. The output gate\ndetermines the information that should be the output in the\nnext cell state.\nThe calculations of the single LSTM unit at a single time\nstep t in the forget gate ft , input gate it , output gate ot and cell\nstate ct , are deﬁned as follows :\nft =σ\n(\nWf Xt +Uf ht−1 +bf\n)\n(1)\nit =σ(WiXt +Uiht−1 +bi) (2)\not =σ(WoXt +Uoht−1 +bo) (3)\n˜ct =tanh (WcXt +Ucht−1 +bc) (4)\nVOLUME 10, 2022 21521\nK. L. Tanet al.: RoBERTa-LSTM: Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network\nct =ft ∗ct−1 +it ∗˜ct (5)\nht =ot ∗tanh (ct ) (6)\nwhere σ is the sigmoid function, Xt denotes the input, ( Wf ,\nWi, Wo, Wc, Uf , Ui, Uo, Uc) and ( bf , bi, bo, bc) denote the\nweight matrices and biases in the forget gate, input gate,\noutput gate, and cell state, correspondingly. ht−1 and ct−1 are\nthe output of the LSTM at time step t −1. The operation ∗is\nthe element wise multiplication.\n3) FLATTEN LAYER\nThe ﬂatten layer serves the purpose of reshaping the input\nfrom 3D tensor to 2D tensor so that it can ﬁt into the following\ndense layer.\n4) DENSE LAYER\nThe dense layer is also known as the fully connected layer.\nThe dense layer connects all the input from the preceding\nlayer to all activation units in the subsequent layer. There are\ntwo dense layers in the proposed RoBERTa-LSTM model.\nThe ﬁrst dense layer consists of 256 hidden neurons to cap-\nture the relationship between the input and the classes. The\nlast dense layer serves as the classiﬁcation layer where the\nnumber of hidden neurons corresponds to the number of\nclasses in the dataset. In the classiﬁcation layer, the SoftMax\nactivation function is applied to produce the probabilistic\ndistribution of the classes in the sentiment analysis dataset.\n5) MODEL TRAINING PARAMETERS\nIn the training of the proposed RoBERTa-LSTM model, the\nAdaptive Moment Estimation (Adam) optimizer is adopted to\noptimize the gradient descent process. The Adam optimizer\nutilizes the moving average of the gradients to avoid being\nstuck in the local minima, thus improving the gradient descent\nprocess. Not only that, the Adam optimizer is able to handle\nthe sparse gradients on the noisy problems.\nThe loss function is part of the optimization algorithm\nwhere it is used to estimate the model loss in every training\nepoch. Since the sentiment analysis is a multi-class problem,\nthe categorical cross entropy is selected as the loss function.\nThe categorical cross entropy is deﬁned as :\ncategorical cross entropy ( p,t)=−\nM∑\nn=1\nyo,c log\n(\npo,c\n)\n(7)\nwhere M denotes the number of classes, yo,c and po,c denote\nthe true and predicted class label for the observation o in\nclass c.\nIV. DATASET\nIn this work, three sentiment analysis datasets are used,\nnamely IMDb, Sentiment140 and Twitter US Airline Senti-\nment dataset.\nThe IMDb dataset was collected from IMDb and pub-\nlished by Maas et al., (2011) [23]. The dataset consists\nof two columns, i.e., review and sentiment. There are\n50000 movie reviews with 25000 labeled as positive and\n25000 labeled as negative. IMDb is a balanced dataset and is\navailable at https://www.kaggle.com/lakshmi25npathi/imdb-\ndataset-of-50k-movie-reviews.\nThe Twitter US Airline Sentiment dataset was collected\nand released by CrowdFlower in February 2015. The dataset\ncontains 14640 tweets with three class labels, including\npositive, negative, and neutral. The dataset is imbalanced\nwhere there are 9178 negative reviews, 2363 positive reviews\nand 3099 neutral reviews. The tweets in the dataset are\nrelated to six American airlines, namely United, US Air-\nways, South-West, Delta, and Virgin America. The pur-\npose of collecting this dataset was to analyze the sentiment\nof the customers from each airline. There are 10 columns\nin the dataset, including tweets id, sentiment, text, airline\nname, etc. However, only the text and sentiment column\nwill be used in the sentiment analysis. The dataset can\nbe accessed at https://www.kaggle.com/crowdﬂower/twitter-\nairline-sentiment.\nThe Sentiment140 dataset was collected from Twitter\nby Stanford University [24]. The dataset is a balanced\ndataset with 0.8 million positive reviews and 0.8 million\nnegative reviews. There are six columns in the dataset,\nincluding target, id, text, ﬂag, user, and id. In the exper-\niment, the target and text column are used for the train-\ning and testing. The dataset can be downloaded from\nhttps://www.kaggle.com/kazanova/sentiment140.\nThe IMDb and Sentiment140 datasets have almost the\nequal number of samples for both positive and negative\nclasses. However, the Twitter US Airline dataset is imbal-\nanced where the sample size of the negative class is much\nhigher than the positive and neutral class. In view of this,\ndata augmentation with pre-trained word embedding, GloVe\nis applied on the Twitter US Airline Sentiment dataset. After\nthe data augmentation, the numbers of samples in all three\nclasses are equal and balanced. Figure 3 illustrates the sample\ndistribution of the IMDb dataset and IMDb dataset without\ndata augmentation, and the sample distribution of the Twitter\nUS Airline Sentiment dataset before and after the data aug-\nmentation.\nV. HYPERPARAMETER TUNING\nThe hyperparameter tuning is performed to determine the\noptimal hyperparameter values that yield the best perfor-\nmance. As the performance of all three datasets is mono-\ntonic, the Twitter US Airline Sentiment dataset after data\naugmentation is used in the hyperparameter tuning. Table 2\nlists the hyperparameters, the tested values, and the optimal\nvalue of the hyperparameters. The experimental results of the\nproposed RoBERTa-LSTM with different LSTM units, opti-\nmizers and learning rates are presented in Table 3, Table 4,\nand Table 5, respectively.\nThe experimental results demonstrate that the optimal\nhyperparameter settings of the RoBERTa-LSTM model\nare 256 LSTM units, Adam optimizer and learning rate\nof 1e-5. The LSTM layer with 256 units exhibits the best\n21522 VOLUME 10, 2022\nK. L. Tanet al.: RoBERTa-LSTM: Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network\nFIGURE 3. The sample distribution of the datasets.\nTABLE 2. The hyperparameter tuning of the proposed RoBERTa-LSTM\nmodel.\nTABLE 3. Different LSTM units (optimizer= Adam, learning rate=\n0.00001).\nTABLE 4. Different optimizers (LSTM unit= 256, learning rate= 0.00001).\nperformance which demonstrates the capability to capture\nlong-range dependencies. Besides that, the Adam optimizer\noutperforms the Root Mean Squared Propagation (RMSProp)\nand Stochastic gradient descent (SGD) in the gradient opti-\nmization process where Adam takes into account the past\ngradients and momentum. Learning rate is an important\nhyperparameter in model learning. Setting a learning rate\ntoo high might cause the model converges too fast leading\nto suboptimal solutionThe learning rate of 1e-5 allows the\nmodel to converge at an appropriate speed leading to an\noptimal performance.\nVI. EXPERIMENTAL RESULTS AND ANALYSIS\nThis section presents some experimental settings and the\nperformance comparison of the proposed RoBERTa-LSTM\nmodel with the state-of-the-art methods.\nTABLE 5. Different learning rates (LSTM unit= 256, optimizer= Adam).\nTABLE 6. The experimental results on the IMDB dataset.\nTABLE 7. The experimental results on the Twitter US Airline Sentiment.\nThe training epoch is set to a maximum of 100, however,\nthe early stopping mechanism is applied to prevent the over-\nﬁtting problem. The observation metric of the early stopping\nis set to validation accuracy with the patience set to 30 epochs.\nIn all experiments, the datasets are split into 6:2:2 for training,\nvalidation, and testing. The batch size is set to 32.\nFor a fair comparison, several machine learning and\ndeep learning methods for sentiment analysis are included\nin the experiments. The machine learning methods\ninclude Naïve Bayes [7], Logistic Regression [13], Deci-\nsion Tree [25], K-nearest neighbor (KNN) [13], and\nAdaBoost [26]. The deep learning methods include Gated\nRecurrent Unit (GRU) [27], Long Short-Term Memory\n(LSTM) [27], Bidirectional Long Short-Term Memory\n(BiLSTM) [28], Convolutional Neural Network-LSTM\n(CNN-LSTM) [29], and Convolutional Neural Network-\nBiLSTM (CNN-BiLSTM) [30]. Table 6, Table 7, and Table 8\npresent the experimental results on the IMDb dataset, Twitter\nUS Airline Sentiment dataset, and Sentiment140 dataset.\nThe experimental results demonstrate that the proposed\nRoBERTa-LSTM model outperforms the state-of-the-art\nVOLUME 10, 2022 21523\nK. L. Tanet al.: RoBERTa-LSTM: Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network\nTABLE 8. The experimental results on the Sentiment140 dataset.\nmethods on all three datasets. Referring to Table 6, on the\nIMDb dataset with 50K movie reviews, the methods in com-\nparison record F1-scores within the range of 73% – 90%.\nNevertheless, the proposed RoBERTa-LSTM model yields\na higher F1-score of 93%. In terms of accuracy, the per-\nformance has increased from 87.88% recorded by the GRU\nmodel to 92.96%.\nAs observed in Table 7, on the Twitter US Airline Senti-\nment dataset with approximately 14.6K tweets, the proposed\nRoBERTa-LSTM has exhibited a huge leap in performance\nwhere the F1-score has increased from 45% – 72% to 91%.\nThere is an improvement of 10.87% in accuracy compared to\nthe second most competitive method, i.e., logistic regression.\nAdditionally, the data augmentation has greatly improved the\naccuracy by 5.48%, hence demonstrating the effects of data\naugmentation in addressing the imbalanced dataset problems.\nApart from that, the proposed RoBERTa-LSTM model\nhas also outshined the state-of-the-art methods on the Sen-\ntiment140 dataset with 1.6 million reviews. As shown in\nTable 8, the F1-score has improved from the range of 57% -\n79% to 90%. The proposed RoBERTa-LSTM model yields an\nincrement of 10.6% in accuracy compared to the best method\nin comparison, i.e., LSTM.\nThe experimental results corroborate the effectiveness of\nthe proposed RoBERTa-LSTM model in sentiment analysis.\nThe RoBERTa model performs exceptional in tokenizing and\nencoding the text sequence in word embeddings represen-\ntation. The LSTM model, on the other hand, is capable of\nlearning long-distance dependencies in the given input. The\nproposed RoBERTa-LSTM model integrates the strengths of\nRoBERTa and LSTM. The RoBERTa model plays the role\nof creating useful and representative word embedding as the\nfeatures to facilitate the LSTM in capturing the temporal\ninformation.\nAlso, it is noteworthy that the data augmentation with\nGloVe pre-trained word embedding has greatly enhanced the\nperformance of the proposed RoBERTa-LSTM on the imbal-\nanced Twitter US Airline Sentiment dataset. The accuracy has\nimproved from 85.89% to 91.37% after data augmentation\nis done to oversample the minority classes, thus improving\nthe generalization ability of the proposed RoBERTa-LSTM\nmodel.\nVII. CONCLUSION\nIn the big data era, having an effective sentiment analy-\nsis tool is essential in many aspects, especially economics\nand politics. The feedback of the sentiment analysis drives\nthe decision making of the interested parties. The exist-\ning works on sentiment analysis mostly focus on machine\nlearning methods and Recurrent Neural Networks. There are\nlimited works that adopt Transformer for sentiment anal-\nysis. Henceforth, this paper presents a hybrid model of\nTransformer and Recurrent Neural Network, referred to as\nthe RoBERTa-LSTM model. Firstly, data augmentation with\nGloVe pre-trained word embedding is used to generate more\nlexically similar samples and to oversample the minority\nclasses. Subsequently, text preprocessing is performed to nor-\nmalize the text and remove less important words. The cleaned\ncorpus is then passed into the proposed RoBERTa-LSTM\nmodel for training and sentiment analysis. The proposed\nRoBERTa-LSTM model beneﬁts from the strengths of both\nRoBERTa and LSTM, where RoBERTa efﬁciently encodes\nthe words into word embedding while LSTM excels in captur-\ning the long-distance dependencies. The experimental results\ndemonstrate that the proposed RoBERTa-LSTM model out-\nshines the state-of-the-art methods in sentiment analysis on\nIMDb dataset, Twitter US Airline Sentiment dataset, and\nSentiment140 dataset.\nREFERENCES\n[1] S. Tam, R. B. Said, and Ö. Ö. Tanriöver, ‘‘A ConvBiLSTM deep learning\nmodel-based approach for Twitter sentiment classiﬁcation,’’ IEEE Access,\nvol. 9, pp. 41283–41293, 2021.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. 31st Int.\nConf. Neural Inf. Process. Syst. (NIPS), Red Hook, NY , USA. Long Beach,\nCA, USA: Curran Associates, 2017, pp. 6000–6010.\n[3] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\npretraining approach,’’ 2019, arXiv:1907.11692.\n[4] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[5] M. Wongkar and A. Angdresey, ‘‘Sentiment analysis using naive Bayes\nalgorithm of the data crawler: Twitter,’’ in Proc. 4th Int. Conf. Informat.\nComput. (ICIC), Oct. 2019, pp. 1–5.\n[6] A. M. Rahat, A. Kahir, and A. K. M. Masum, ‘‘Comparison of naive Bayes\nand SVM algorithm based on sentiment analysis using review dataset,’’ in\nProc. 8th Int. Conf. Syst. Modeling Adv. Res. Trends (SMART), Nov. 2019,\npp. 266–270.\n[7] Y . G. Jung, K. T. Kim, B. Lee, and H. Y . Youn, ‘‘Enhanced naive Bayes\nclassiﬁer for real-time sentiment analysis with SparkR,’’ in Proc. Int. Conf.\nInf. Commun. Technol. Converg. (ICTC), Oct. 2016, pp. 141–146.\n[8] D. K. Madhuri, ‘‘A machine learning based framework for sentiment\nclassiﬁcation: Indian railways case study,’’ Int. J. Innov. Technol. Explor.\nEng., vol. 8, no. 4, pp. 1–5, 2019.\n[9] E. Prabhakar, M. Santhosh, A. H. Krishnan, T. Kumar, and R. Sudhakar,\n‘‘Sentiment analysis of US Airline Twitter data using new AdaBoost\napproach,’’Int. J. Eng. Res. Technol., vol. 7, no. 1, pp. 1–6, 2019.\n[10] A. I. Saad, ‘‘Opinion mining on US Airline Twitter data using machine\nlearning techniques,’’ in Proc. 16th Int. Comput. Eng. Conf. (ICENCO),\nDec. 2020, pp. 59–63.\n[11] A. Younas, R. Nasim, S. Ali, G. Wang, and F. Qi, ‘‘Sentiment analysis of\ncode-mixed Roman Urdu-English social media text using deep learning\napproaches,’’ in Proc. IEEE 23rd Int. Conf. Comput. Sci. Eng. (CSE),\nDec. 2020, pp. 66–71.\n[12] S. Anbukkarasi and S. Varadhaganapathy, ‘‘Analyzing sentiment in Tamil\ntweets using deep neural network,’’ in Proc. 4th Int. Conf. Comput.\nMethodol. Commun. (ICCMC), Mar. 2020, pp. 449–453.\n21524 VOLUME 10, 2022\nK. L. Tanet al.: RoBERTa-LSTM: Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network\n[13] T. Dholpuria, Y . K. Rana, and C. Agrawal, ‘‘A sentiment analysis approach\nthrough deep learning for a movie review,’’ in Proc. 8th Int. Conf. Commun.\nSyst. Netw. Technol. (CSNT), Nov. 2018, pp. 173–181.\n[14] R. M. Alahmary, H. Z. Al-Dossari, and A. Z. Emam, ‘‘Sentiment analysis\nof Saudi dialect using deep learning techniques,’’ in Proc. Int. Conf.\nElectron., Inf., Commun. (ICEIC), Jan. 2019, pp. 1–6.\n[15] N. K. Thinh, C. H. Nga, Y .-S. Lee, M.-L. Wu, P.-C. Chang, and J.-C. Wang,\n‘‘Sentiment analysis using residual learning with simpliﬁed CNN extrac-\ntor,’’ inProc. IEEE Int. Symp. Multimedia (ISM), Dec. 2019, pp. 335–3353.\n[16] D. A. Kumar and A. Chinnalagu, ‘‘Sentiment and emotion in social media\nCOVID-19 conversations: SAB-LSTM approach,’’ in Proc. 9th Int. Conf.\nSyst. Modeling Adv. Res. Trends (SMART), Dec. 2020, pp. 463–467.\n[17] K. Dhola and M. Saradva, ‘‘A comparative evaluation of traditional\nmachine learning and deep learning classiﬁcation techniques for senti-\nment analysis,’’ in Proc. 11th Int. Conf. Cloud Comput., Data Sci. Eng.,\nJan. 2021, pp. 932–936.\n[18] A. H. Uddin, D. Bapery, and A. S. M. Arif, ‘‘Depression analysis\nfrom social media data in Bangla language using long short term mem-\nory (LSTM) recurrent neural network technique,’’ in Proc. Int. Conf.\nComput., Commun., Chem., Mater. Electron. Eng., Jul. 2019, pp. 1–4.\n[19] X. Zhang, J. Zhao, and Y . LeCun, ‘‘Character-level convolutional networks\nfor text classiﬁcation,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 28,\n2015, pp. 649–657.\n[20] K. Kaﬂe, M. Yousefhussien, and C. Kanan, ‘‘Data augmentation for visual\nquestion answering,’’ in Proc. 10th Int. Conf. Natural Lang. Gener., 2017,\npp. 198–202.\n[21] W. Y . Wang and D. Yang, ‘‘That’s so annoying!!!: A lexical and frame-\nsemantic embedding based data augmentation approach to automatic cate-\ngorization of annoying behaviors using #petpeeve tweets,’’ in Proc. Conf.\nEmpirical Methods Natural Lang. Process., 2015, pp. 2557–2563.\n[22] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\nand S. Fidler, ‘‘Aligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books,’’ in Proc. IEEE Int.\nConf. Comput. Vis. (ICCV), Dec. 2015, pp. 19–27.\n[23] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and A. Potts, ‘‘Learn-\ning word vectors for sentiment analysis,’’ in Proc. 49th Annu. Meeting\nAssoc. Comput. Linguistics, Hum. Lang. Technol., 2011, pp. 142–150.\n[24] A. Go, R. Bhayani, and L. Huang, ‘‘Twitter sentiment classiﬁcation using\ndistant supervision,’’ Stanford, vol. 1, no. 12, p. 2009, 2009.\n[25] A. S. Zharmagambetov and A. A. Pak, ‘‘Sentiment analysis of a document\nusing deep learning approach and decision trees,’’ in Proc. 12th Int. Conf.\nElectron. Comput. Comput. (ICECCO), Sep. 2015, pp. 1–4.\n[26] M. Vadivukarassi, N. Puviarasan, and P. Aruna, ‘‘An exploration of airline\nsentimental tweets with different classiﬁcation model,’’ Int. J. Res. Eng.\nAppl. Manage., vol. 4, no. 2, pp. 1–6, 2018.\n[27] M. S. Hossen, A. H. Jony, T. Tabassum, M. T. Islam, M. M. Rahman, and\nT. Khatun, ‘‘Hotel review analysis for the prediction of business using deep\nlearning approach,’’ in Proc. Int. Conf. Artif. Intell. Smart Syst. (ICAIS),\nMar. 2021, pp. 1489–1494.\n[28] A. Garg and R. K. Kaliyar, ‘‘PSent20: An effective political sentiment\nanalysis with deep learning using real-time social media tweets,’’ in Proc.\n5th IEEE Int. Conf. Recent Adv. Innov. Eng. (ICRAIE), Dec. 2020, pp. 1–5.\n[29] P. K. Jain, V . Saravanan, and R. Pamula, ‘‘A hybrid CNN-LSTM: A deep\nlearning approach for consumer sentiment analysis using qualitative user-\ngenerated contents,’’ACM Trans. Asian Low-Resource Lang. Inf. Process.,\nvol. 20, no. 5, pp. 1–15, Sep. 2021.\n[30] M. Rhanoui, M. Mikram, S. Yousﬁ, and S. Barzali, ‘‘A CNN-BiLSTM\nmodel for document-level sentiment analysis,’’ Mach. Learn. Knowl.\nExtraction, vol. 1, no. 3, pp. 832–847, 2019.\nKIAN LONG TANreceived the Bachelor of Infor-\nmation Technology degree in artiﬁcial intelligence\nfrom Multimedia University, in 2021, where he is\ncurrently pursuing the master’s degree. His current\nresearch interests include natural language pro-\ncessing (NLP), deep learning, machine learning,\nand sentiment analysis.\nCHIN POO LEE received the Master of Sci-\nence and Ph.D. degrees in information technol-\nogy in the area of abnormal behavior detection\nand gait recognition. She is currently a Senior\nLecturer with the Faculty of Information Science\nand Technology, Multimedia University, Malaysia.\nHer research interests include action recogni-\ntion, computer vision, gait recognition, and deep\nlearning.\nKALAIARASI SONAI MUTHU ANBANAN-\nTHEN received the Ph.D. degree in artiﬁ-\ncial intelligence from the University Malaysia\nSabah, Malaysia, researching rule extraction from\nartiﬁcial neural network. She is currently the\nCo-ordinator for Master of Information Technol-\nogy (information system). She is also an Associate\nProfessor with the Faculty of Information Science\nand Technology, Multimedia University (MMU),\nMalaysia. She has over 30 publications in the areas\nof artiﬁcial neural networks, rule extraction, data mining, and knowledge\nmanagement. Her current research interests include data mining, opinion\nmining, neural networks, and knowledge management.\nKIAN MING LIM (Senior Member, IEEE)\nreceived the B.I.T. (Hons.), Master of Engineering\nScience (M.Eng.Sc.), and Ph.D. (I.T.) degrees in\ninformation systems engineering from Multime-\ndia University. He is currently a Lecturer with\nthe Faculty of Information Science and Technol-\nogy, Multimedia University. His research interests\ninclude machine learning, deep learning, computer\nvision, and pattern recognition.\nVOLUME 10, 2022 21525",
  "topic": "Sentiment analysis",
  "concepts": [
    {
      "name": "Sentiment analysis",
      "score": 0.8619192242622375
    },
    {
      "name": "Computer science",
      "score": 0.8351490497589111
    },
    {
      "name": "Word embedding",
      "score": 0.7535150051116943
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6131234169006348
    },
    {
      "name": "Transformer",
      "score": 0.5989909172058105
    },
    {
      "name": "Natural language processing",
      "score": 0.5034002661705017
    },
    {
      "name": "Embedding",
      "score": 0.49820709228515625
    },
    {
      "name": "Recurrent neural network",
      "score": 0.449025958776474
    },
    {
      "name": "ENCODE",
      "score": 0.42690667510032654
    },
    {
      "name": "Machine learning",
      "score": 0.39072757959365845
    },
    {
      "name": "Artificial neural network",
      "score": 0.355201780796051
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}