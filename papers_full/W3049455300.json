{
  "title": "Spatial Temporal Transformer Network for Skeleton-Based Action Recognition",
  "url": "https://openalex.org/W3049455300",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3049552744",
      "name": "Chiara Plizzari",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A2804397697",
      "name": "Marco Cannici",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A2150539076",
      "name": "Matteo Matteucci",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A3049552744",
      "name": "Chiara Plizzari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2804397697",
      "name": "Marco Cannici",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150539076",
      "name": "Matteo Matteucci",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3035050855",
    "https://openalex.org/W3009946848",
    "https://openalex.org/W6679338098",
    "https://openalex.org/W2793547936",
    "https://openalex.org/W2944006115",
    "https://openalex.org/W2510185399",
    "https://openalex.org/W2739179646",
    "https://openalex.org/W2799211965",
    "https://openalex.org/W3035225512",
    "https://openalex.org/W6606298547",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W2948246283",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2963744496",
    "https://openalex.org/W3123784868",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2784435047",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1836465849"
  ],
  "abstract": null,
  "full_text": "Spatial Temporal Transformer Network for\nSkeleton-based Action Recognition\nChiara Plizzari, Marco Cannici, and Matteo Matteucci\nPolitecnico di Milano, Milano, Italy\nchiara.plizzari@mail.polimi.it\n{marco.cannici, matteo.matteucci}@polimi.it\nAbstract. Skeleton-based human action recognition has achieved a great\ninterest in recent years, as skeleton data has been demonstrated to be\nrobust to illumination changes, body scales, dynamic camera views, and\ncomplex background. Nevertheless, an eﬀective encoding of the latent\ninformation underlying the 3D skeleton is still an open problem. In this\nwork, we propose a novel Spatial-Temporal Transformer network (ST-\nTR) which models dependencies between joints using the Transformer\nself-attention operator. In our ST-TR model, a Spatial Self-Attention\nmodule (SSA) is used to understand intra-frame interactions between\ndiﬀerent body parts, and a Temporal Self-Attention module (TSA) to\nmodel inter-frame correlations. The two are combined in a two-stream\nnetwork which outperforms state-of-the-art models using the same input\ndata on both NTU-RGB+D 60 and NTU-RGB+D 120.\nKeywords: Representation learning · Graph CNN · Self-Attention · 3D\nSkeleton · Action Recognition\n1 Introduction\nSkeleton-based activity recognition is achieving increasing interest in recent years\nthanks to advances in 3D skeleton pose estimation devices, both in terms of\naccuracy and resolution. Algorithms and neural architectures for extracting\ncontext-aware ﬁne-grained spatial-temporal features, capable of unlocking the\ntrue potential of skeleton based action recognition, however, are still lacking in\nthe literature. The most widespread method to perform skeleton-based action\nrecognition has become Spatial-Temporal Graph Convolutional Network (ST-\nGCN) [16], since, being an eﬃcient representation of non-Euclidean data, it is\nable to eﬀectively capture spatial (intra-frame) and temporal (inter-frame) in-\nformation. However, ST-GCN models have some structural limitations, some of\nthem already addressed in [13,14,2,10]: (i) The topology of the graph represent-\ning the human body is ﬁxed for all layers and all the actions, preventing the\nextraction of rich representations. (ii) Both the spatial and temporal convolu-\ntions are implemented from a standard 2D convolution. As such, they are limited\nto operate in a local neighborhood. (iii) As a consequence of (i) and (ii), corre-\nlations between body joints not linked in the human skeleton, e.g., the left and\nright hands, are underestimated even if relevant in actions such as “clapping”.\narXiv:2012.06399v1  [cs.CV]  11 Dec 2020\n2 C. Plizzari et al.\n(a) Spatial Self-Attention\nReshape\nConv1D Conv1D\nReshape\nS-TR stream\nT-TR stream\nfeature extraction\n3,64,1\n64,64,1\n64,128,2\n64,64,1\n128,128,1\n128,128,1\n128,256,2\n256,256,1\n256,256,1\nBatchNorm\nT-TR modules\nS-TR modules\nlayer i-thlayer i-th\nlayer i-th (b) Temporal Self-Attention\nFig. 1.Spatial Self-Attention (SSA) and Temporal Self-Attention (TSA). Self-attention\noperates on each pair of nodes by computing a weight for each of them representing\nthe strength of their correlation. Those weights are used to score the contribution of\neach body joint ni, proportionally to how relevant the node is w.r.t. the other ones.\nIn this paper, we face all these limitations by employing a modiﬁed Trans-\nformer self-attention operator. Despite being originally designed for Natural Lan-\nguage Processing (NLP) tasks, the ﬂexibility of the Transformer self-attention [15]\nin modeling long-range dependencies, make this model a perfect solution to\ntackle ST-GCN weaknesses. Recently, Bello et al. in [1] employed self-attention\non image pixels to overcome the locality of the convolution operator. In our work,\nwe aim to apply the same mechanism to joints representing the human skeleton,\nwith the goal of extracting adaptive low-level features modeling interactions in\nhuman actions both in space, through a Spatial Self-Attention module (SSA),\nand time, through a Temporal Self-Attention module (TSA) module. Authors\nof [3] also proposed a Self-Attention Network (SAN) to extract long-term seman-\ntic information; however, since they focus on temporally segmented clips, they\nsolve the locality limitations of convolution only partially.\nContributions of this paper are summarized as follows:\n• We propose a novel two-stream Transformer-based model, employing self-\nattention on both the spatial and temporal dimensions\n• We design a Spatial Self-Attention(SSA) module to dynamically build links\nbetween skeleton joints, representing the relationships between human body\nparts, conditionally on the action and independently from the natural human\nbody structure. On the temporal dimension, we introduce a Temporal Self-\nAttention (TSA) module to study the dynamics of joints along time too 1\n• Our model outperforms the ST-GCN [16] baseline, and outperforms previous\nstate-of-the-art methods using the same input data on NTU-RGB+D [12,6].\n1 Code at https://github.com/Chiaraplizz/ST-TR\nST-TR Network for Skeleton-based Action Recognition 3\n2 Spatial Temporal Transformer Network\nWe propose Spatial Temporal Transformer (ST-TR), an architecture using the\nTransformer self-attention mechanism to operate both on space and time. We\ndevelop two modules, Spatial Self-Attention (SSA)and Temporal Self-Attention\n(TSA), each one focusing on extracting correlations in one of the two dimensions.\n2.1 Spatial Self-Attention (SSA)\nThe SSA module applies self-attention inside each frameto extract low-level fea-\ntures embedding the relations between body parts, i.e., computing correlations\nbetween each pair of joints in every single frame independently, as depicted\nin Figure 1a. Given the frame at time t, for each node it of the skeleton, a\nquery vector qt\ni ∈Rdq, a key vector kt\ni ∈Rdk and a value vector vt\ni ∈Rdv\nare ﬁrst computed by applying trainable linear transformations to the node\nfeatures nt\ni ∈ RCin, shared across all nodes, of parameters Wq ∈ RCin×dq,\nWk ∈RCin×dk, Wv ∈RCin×dv. Then, for each pair of body nodes ( it,jt), a\nquery-key dot product is applied to obtain a weight αt\nij ∈R representing the\nstrength of the correlations between the two nodes. The resulting score αt\nij is\nused to weight each joint value vt\nj, and a weighted sum is computed to obtain a\nnew embedding for node it, as in the following:\nαt\nij = qt\ni ·kt\nj\nT\n,∀t∈T, zt\ni =\n∑\nj\nsoftmaxj\n(αt\nij√dk\n)\nvt\nj (1)\nwhere zt\ni ∈RCout (with Cout the number of output channels) constitutes the new\nembedding of node it.\nMulti-head self-attention is ﬁnally applied by repeating this embedding ex-\ntraction process H times, each time with a diﬀerent set of learnable parame-\nters. The set ( zt\ni1 ,..., zt\niH ) of node embeddings thus obtained, all referring to\nthe same node it, is ﬁnally combined with a learnable transformation, i.e.,\nconcat(zt\ni1 ,..., zt\niH ) ·Wo, and constitutes the output features of SSA.\nThus, as shown in Figure 1a, the relations between nodes (i.e., theαt\nij scores)\nare dynamically predicted in SSA; the correlation structure is not ﬁxed for all\nthe actions, but it changes adaptively for each sample. SSA operates similar to a\ngraph convolution on a fully connected graph where, however, the kernel values\n(i.e., the αt\nij scores) are predicted dynamically based on the skeleton pose.\n2.2 Temporal Self-Attention (TSA)\nAlong the temporal dimension, the dynamics of each joint is studied separately\nalong all the frames, i.e., each single node is considered as independent and\ncorrelations between frames are computed by comparing features of the same\nbody joint along the temporal dimension (see Figure 1b). The formulation is\n4 C. Plizzari et al.\nsymmetrical to the one reported in Equation (1) for SSA:\nαv\nij = qv\ni ·kv\nj ∀v∈V, zv\ni =\n∑\nj\nsoftmaxj\n(αv\nij√dk\n)\nvv\nj (2)\nwhere iv,jv indicate the same joint v in two diﬀerent instants i,j, αv\nij ∈R,\nqv\ni ∈Rdq is the query associated to iv, kv\nj ∈Rdk and vv\nj ∈Rdv are the key and\nvalue associated to joint jv (all computed using trainable linear transformations\nas in SSA), and zv\ni ∈RCout is the resulting node embedding. An illustration of\nTSA is depicted in Figure 1b. Multi-head attention is applied as in SSA. The\nnetwork, by extracting inter-frame relations between nodes in time, can learn to\ncorrelate frames apart from each other (e.g., nodes in the ﬁrst frame with those\nin the last one), capturing discriminant features that are not otherwise possible\nto capture with a standard convolution, being this limited by the kernel size.\n2.3 Two-Stream Spatial Temporal Transformer Network\nTo combine the SSA and TSA modules, a two-stream architecture named 2s-ST-\nTR is used, as similarly proposed by Shi et al. in [14] and [13]. In our formulation,\nthe two streams diﬀerentiate on the way the self-attention mechanism is applied:\nSSA operates on the spatial stream (named S-TR stream), while TSA on the\ntemporal one (named T-TR stream). On both streams, simple features are ﬁrst\nextracted through a three-layers residual network, where each layer processes\nthe input on the spatial dimension through graph convolution (GCN), and on\nthe temporal dimension through a standard 2D convolution (TCN), as in ST-\nGCN [16]. SSA and TSA are applied on the S-TR and on the T-TR stream\nin substitution to the GCN and TCN feature extraction modules respectively\n(Figure 2). Each stream is trained using the standard cross-entropy loss, and\nthe sub-networks outputs are eventually fused together by summing up their\nsoftmax output scores to obtain the ﬁnal prediction, as in [14,13].\nSpatial Transformer Stream (S-TR).In the spatial stream (Figure 2), self-\nattention is applied at the skeleton level through a SSA module, which focuses\non spatial relations between joints, and then its output is passed to a 2D con-\nvolutional module with kernel Kt on temporal dimension (TCN), as in [16], to\nextract temporally relevant features, i.e., S-TR(x) = Conv2D(1×Kt)(SSA(x)).\nFollowing the original Transformer structure, the input is pre-normalized pass-\ning through a Batch Normalization layer [4,11], and skip connections are used,\nwhich sum the input to the output of the SSA module.\nTemporal Transformer Stream (T-TR).The temporal stream, instead,\nfocuses on discovering inter-frame temporal relations. Similarly to the S-TR\nstream, inside each T-TR layer, a standard graph convolution sub-module [16]\nis followed by the proposed Temporal Self-Attention module, i.e., T-TR(x) =\nTSA(GCN(x)). In this case, TSA operates on graphs linking the same joint\nalong all the time dimension (e.g., all left feet, or all right hands).\nST-TR Network for Skeleton-based Action Recognition 5\nReshape\nConv1D Conv1D\nReshape\nS-TR stream\nT-TR stream\nfeature extraction\n3,64,1\n64,64,1\n64,128,2\n64,64,1\n128,128,1\n128,128,1\n128,256,2\n256,256,1\n256,256,1\nBatchNorm\nT-TR modules\nS-TR modules layer i-th\nlayer i-th\nlayer i-th\nFig. 2.The 2s-ST-TR architecture. On each stream, the ﬁrst three layers extract simple\nfeatures. On the S-TR stream, at each subsequent layer, SSA is used to extract spatial\ninformation, followed by a 2D convolution on time dimension (TCN). On the T-TR\nstream, at each subsequent layer, TSA is used to extract temporal information, while\nspatial features are extracted by a standard graph convolution (GCN) [16]\n3 Model Evaluation\n3.1 Datasets\nNTU RGB+D 60 and NTU RGB+D 120.The NTU RGB+D 60 (NTU-\n60) dataset is a large-scale benchmark for 3D human action recognition [12].\nSkeleton information consists of 3D coordinates of 25 body joints and a total\nof 60 diﬀerent action classes. The NTU-60 dataset follows two diﬀerent criteria\nfor evaluation. In Cross-View Evaluation (X-View), the data is split according\nto the camera from which the action is taken, while in Cross-Subject Evaluation\n(X-Sub) according to the subject performing the action. NTU-RGB+D 120 [6]\n(NTU-120) is an extension of NTU-60, with a total of 113 ,945 videos and 120\nclasses. It follows two evaluation criteria: Cross-Subject Evaluation (X-Sub)is\nthe same used in NTU-60, while in Cross-Setup Evaluation (X-Set)training and\ntesting samples are split based on the parity of the camera setup IDs.\n3.2 Experimental Settings\nUsing PyTorch framework, we trained our models for a total of 120 epochs\nwith batch size 32 and SGD as optimizer. The learning rate is set to 0 .1 at the\nbeginning and then reduced by a factor of 10 at the epochs {60, 90}. Moreover,\nwe preprocessed the data with the same procedure used by Shi et al. in [14]\nand [13]. In order to avoid overﬁtting, we also usedDropAttention [17], a dropout\ntechnique introduced by Zehui et al. [17] for regularizing attention weights in\nTransformer. In all of these experiments, the number of heads for multi-head\nattention is set to 8, and dq,dk,dv embedding dimensions to 0.25 ×Cout in each\nlayer, as in [1]. We did not perform a grid search on these parameters.\n6 C. Plizzari et al.\n(a)\nMethod Bones X-Sub X-View\nST-GCN [16] 85.7 92.7\nS-TR 86.4 94.0\nT-TR 86.0 93.6\nST-TR 88.7 95.6\nS-TR ✓ 87.9 94.9\nT-TR ✓ 87.3 94.1\nST-TR ✓ 89.9 96.1\n(b)\nModuleParams [×104]\nGCN [16] 19.9\nSSA 17.8\nTCN [16] 59.0\nTSA 17.7\nTable 1. a) Accuracy (%) comparison of S-TR and T-TR, and their combination\n(ST-TR) on NTU-60, w and w/o bones. b) Parameters of SSA and TSA modules\n3.3 Results\nTo verify the eﬀectiveness of our SSA and TSA modules, we compare separately\nthe S-TR stream and T-TR stream against the ST-GCN [16] baseline, whose\nresults on NTU-60 [12] are reported in Table 1 using our learning rate scheduling.\nAs far as it concerns the SSA, S-TR outperforms the baseline by 0 .7% on\nX-Sub, and by 1 .3% on X-View, demonstrating that self-attention can be used\nin place of graph convolution, increasing the network performance while also\ndecreasing the number of parameters. On NTU-60 the S-TR stream achieves\nslightly better performance (+0.4%) than the T-TR stream, on both X-View and\nX-Sub (Table 1a). This can be motivated by the fact that SSA in S-TR operates\non 25 joints only, while on the temporal dimension the number of correlations\nis proportional to the huge number of frames. Table 1b shows the diﬀerence in\nterms of parameters between a single GCN (TCN) and the corresponding SSA\n(TSA) module, with Cin = Cout = 256. Especially on the temporal dimension,\nTSA results in a decrease in parameters, introducing 41 .3 ×104 less than TCN.\nThe combination of the two streams achieves 88.7% of accuracy on X-Sub and\n95.6% of accuracy on X-View, outperforming the baseline ST-GCN by up to 3%\nand surpassing other two-stream architectures (Table 2). Classes that beneﬁt the\nmost from self-attention are “playing with phone”, “typing”, and “cross hands”\non S-TR, and those involving long-range relations or two people, i.e., “hugging”,\n“point ﬁnger”, “pat on back”, on T-TR. These require to correlate along the\nentire action, giving empirical insight on the advantage of the proposed method.\nAs adding bones information demonstrated leading to better results in pre-\nvious works [13,14], we also studied the eﬀect of our self-attention module on\ncombined joint and bones information. For each node v1 = ( x1,y1,z1) and\nv2 = (x2,y2,z2), the bone connecting the two is calculated as bv1,v2 = (x2 −\nx1,y2 −y1,z2 −z1). Joint and bone information are concatenated along the\nchannel dimension and then fed to the network. At each layer, the size of the\ninput and output channels is doubled as in [13,14]. The performance results are\nshown again in Table 1a; all previous conﬁgurations improve when bones are\nadded as input. The latter fact highlights the ﬂexibility of our method, which is\ncapable of adapting to diﬀerent input types and network conﬁgurations.\nST-TR Network for Skeleton-based Action Recognition 7\n(a)\nNTU-60\nMethod Bones X-Sub X-View\nST-GCN [16] 81.5 88.3\n1s-AGCN [14][10] 86.0 93.7\n1s Shift-GCN [2] 87.8 95.1\nSAN [3] 87.2 92.7\nST-TR (Ours) 88.7 95.6\n2s-AGCN [14] ✓ 88.5 95.1\nDGCNN [13] ✓ 89.9 96.1\n2s Shift-GCN [2] ✓ 89.7 96.0\nMS-G3D [10] ✓ 91.5 96.2\nST-TR (Ours) ✓ 89.9 96.1\n(b)\nNTU-120\nMethod X-Sub X-Set\nST-LSTM [7] 55.7 57.9\nGCA-LSTM [8] 61.2 63.3\nRotClips+MTCNN [5] 62.2 61.8\nPose Evol. Map [9] 64.6 66.9\n1s Shift-GCN [2] 80.9 83.2\nS-TR (Ours) 78.6 80.7\nT-TR (Ours) 78.4 80.5\nST-TR (Ours) 81.9 84.1\nTable 2. Comparison with state-of-the-art accuracy (%) of S-TR, T-TR, and their\ncombination (ST-TR) on NTU-60 (a) and NTU-120 (b)\n4 Comparison with State-Of-The-Art\nWe compare our methods on NTU-60 and NTU-120 w.r.t. other methods which\nmake use of joint or joint+bones information on a one- or two-stream architec-\nture, as we also did, for a fair comparison (Table 2). On NTU-60, ST-TR without\nbones outperforms all the state-of-the-art models not using bones, including 1s-\nAGCN and SAN [3], which uses self-attention too. Similarly, our ST-TR with\nbones outperforms all previous two-stream methods that use bones information\nas well, i.e., 2s-AGCN and 2s Shift-GCN. On NTU-120, the model based only\non joint information outperforms all state-of-the-art methods making use of the\nsame information. The competitive results validate the superiority of our method\nover architectures relying on convolution only.\n5 Conclusions\nIn this paper, we propose a novel approach that introduces Transformer self-\nattention in skeleton activity recognition as an alternative to graph convolution.\nThrough experiments on NTU-60 and NTU-120, we demonstrated that our Spa-\ntial Self-Attention module (SSA) can replace graph convolution, enabling more\nﬂexible and dynamic representations. Similarly, the Temporal Self-Attention\nmodule (TSA) overcomes the strict locality of standard convolution, leading to\nglobal motion pattern extraction. Moreover, our ﬁnal Spatial-Temporal Trans-\nformer network (ST-TR) achieves state-of-the-art performance on NTU-RGB+D\nw.r.t. methods using same input information and streams setup.\nReferences\n1. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Attention augmented convo-\nlutional networks. Proceedings of the IEEE International Conference on Computer\n8 C. Plizzari et al.\nVision pp. 3286–3295 (2019)\n2. Cheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., Lu, H.: Skeleton-based action\nrecognition with shift graph convolutional network. Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition pp. 183–192 (2020)\n3. Cho, S., Maqbool, M., Liu, F., Foroosh, H.: Self-attention network for skeleton-\nbased human action recognition. In: The IEEE Winter Conference on Applications\nof Computer Vision. pp. 635–644 (2020)\n4. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)\n5. Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: Learning clip representa-\ntions for skeleton-based 3d action recognition. IEEE Transactions on Image Pro-\ncessing 27(6), 2842–2855 (2018)\n6. Liu, J., Shahroudy, A., Perez, M.L., Wang, G., Duan, L.Y., Chichung, A.K.: Ntu\nrgb+d 120: A large-scale benchmark for 3d human activity understanding. IEEE\ntransactions on pattern analysis and machine intelligence (2019)\n7. Liu, J., Shahroudy, A., Xu, D., Wang, G.: Spatio-temporal lstm with trust gates for\n3d human action recognition. European conference on computer vision pp. 816–833\n(2016)\n8. Liu, J., Wang, G., Duan, L.Y., Abdiyeva, K., Kot, A.C.: Skeleton-based human\naction recognition with global context-aware attention lstm networks. IEEE Trans-\nactions on Image Processing 27(4), 1586–1599 (2017)\n9. Liu, M., Yuan, J.: Recognizing human actions as the evolution of pose estima-\ntion maps. Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition pp. 1159–1168 (2018)\n10. Liu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W.: Disentangling and unify-\ning graph convolutions for skeleton-based action recognition. Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition pp. 143–152\n(2020)\n11. Nguyen, T.Q., Salazar, J.: Transformers without tears: Improving the normaliza-\ntion of self-attention. arXiv preprint arXiv:1910.05895 (2019)\n12. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: Ntu rgb+d: A large scale dataset\nfor 3d human activity analysis. Proceedings of the IEEE conference on computer\nvision and pattern recognition pp. 1010–1019 (2016)\n13. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Skeleton-based action recognition with di-\nrected graph neural networks. Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition pp. 7912–7921 (2019)\n14. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Two-stream adaptive graph convolutional\nnetworks for skeleton-based action recognition. Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition pp. 12026–12035 (2019)\n15. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems pp. 5998–6008 (2017)\n16. Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for\nskeleton-based action recognition. Thirty-second AAAI conference on artiﬁcial in-\ntelligence (2018)\n17. Zehui, L., Liu, P., Huang, L., Fu, J., Chen, J., Qiu, X., Huang, X.: Dropattention:\nA regularization method for fully-connected self-attention networks. arXiv preprint\narXiv:1907.11065 (2019)",
  "topic": "RGB color model",
  "concepts": [
    {
      "name": "RGB color model",
      "score": 0.7619822025299072
    },
    {
      "name": "Computer science",
      "score": 0.6750951409339905
    },
    {
      "name": "Transformer",
      "score": 0.6343546509742737
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6201955676078796
    },
    {
      "name": "Skeleton (computer programming)",
      "score": 0.5361136794090271
    },
    {
      "name": "Computer vision",
      "score": 0.5125701427459717
    },
    {
      "name": "Action recognition",
      "score": 0.42643752694129944
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4022866189479828
    },
    {
      "name": "Engineering",
      "score": 0.1429322063922882
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I93860229",
      "name": "Politecnico di Milano",
      "country": "IT"
    }
  ],
  "cited_by": 210
}