{
  "title": "Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data",
  "url": "https://openalex.org/W2964645190",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Harer, Jacob",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Reale, Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161644679",
      "name": "Chin, Peter",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962901607",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W2517375502",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2133280805",
    "https://openalex.org/W2250861254",
    "https://openalex.org/W2892589118",
    "https://openalex.org/W2766875678",
    "https://openalex.org/W2104518905",
    "https://openalex.org/W2751262944",
    "https://openalex.org/W2962914467",
    "https://openalex.org/W2400994325",
    "https://openalex.org/W2962685689",
    "https://openalex.org/W2274071363",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W3122945969",
    "https://openalex.org/W2624697062",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964289395",
    "https://openalex.org/W2759575900",
    "https://openalex.org/W2963661253",
    "https://openalex.org/W2550120381",
    "https://openalex.org/W2963069010",
    "https://openalex.org/W2964091575",
    "https://openalex.org/W2321916036",
    "https://openalex.org/W2170527467",
    "https://openalex.org/W2470324779",
    "https://openalex.org/W2111369166",
    "https://openalex.org/W2964082031",
    "https://openalex.org/W1423339008",
    "https://openalex.org/W2605202003",
    "https://openalex.org/W2964187553",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2153185479",
    "https://openalex.org/W2493109812",
    "https://openalex.org/W2963648186",
    "https://openalex.org/W2028509346",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W2890194927",
    "https://openalex.org/W2964258094",
    "https://openalex.org/W2963794306"
  ],
  "abstract": "Many common sequential data sources, such as source code and natural language, have a natural tree-structured representation. These trees can be generated by fitting a sequence to a grammar, yielding a hierarchical ordering of the tokens in the sequence. This structure encodes a high degree of syntactic information, making it ideal for problems such as grammar correction. However, little work has been done to develop neural networks that can operate on and exploit tree-structured data. In this paper we present the Tree-Transformer \\textemdash{} a novel neural network architecture designed to translate between arbitrary input and output trees. We applied this architecture to correction tasks in both the source code and natural language domains. On source code, our model achieved an improvement of $25\\%$ $\\text{F}0.5$ over the best sequential method. On natural language, we achieved comparable results to the most complex state of the art systems, obtaining a $10\\%$ improvement in recall on the CoNLL 2014 benchmark and the highest to date $\\text{F}0.5$ score on the AESW benchmark of $50.43$.",
  "full_text": "Tree-Transformer: A Transformer-Based Method for\nCorrection of Tree-Structured Data.\nJacob Harer\nBoston University\njharer@bu.edu\nChris Reale\nDraper\ncreale@draper.com\nPeter Chin\nBoston University\nspchin@cs.bu.edu\nAbstract\nMany common sequential data sources, such as source code and natural language,\nhave a natural tree-structured representation. These trees can be generated by\nﬁtting a sequence to a grammar, yielding a hierarchical ordering of the tokens\nin the sequence. This structure encodes a high degree of syntactic information,\nmaking it ideal for problems such as grammar correction. However, little work\nhas been done to develop neural networks that can operate on and exploit tree-\nstructured data. In this paper we present the Tree-Transformer — a novel neural\nnetwork architecture designed to translate between arbitrary input and output\ntrees. We applied this architecture to correction tasks in both the source code and\nnatural language domains. On source code, our model achieved an improvement\nof 25% F0.5 over the best sequential method. On natural language, we achieved\ncomparable results to the most complex state of the art systems, obtaining a 10%\nimprovement in recall on the CoNLL 2014 benchmark and the highest to date F0.5\nscore on the AESW benchmark of 50.43.\n1 Introduction\nMost machine learning approaches to correction tasks operate on sequential representations of input\nand output data. Generally this is done as a matter of convenience — sequential data is readily\navailable and requires minimal effort to be used as training data for many machine learning models.\nSequence-based machine learning models have produced prominent results in both translation and\ncorrection of natural language [1, 2, 3, 4, 5, 6, 7, 8].\nWhile algorithms that use sequential data have produced good results, most of these sequential data\ntypes can be more informatively represented using a tree structure. One common method to obtain\ntree-structured data is to ﬁt sequential data to a grammar, such as a Context Free Grammar (CFG).\nThe use of a grammar ensures that generated trees encode the higher-order syntactic structure of the\ndata in addition to the information contained by sequential data.\nIn this work, we trained a neural network to operate directly on trees, teaching it to learn the syntax\nof the underlying grammar and to leverage this syntax to produce outputs which are grammatically\ncorrect. Our model, the Tree-Transformer, handles correction tasks in a tree-based encoder and\ndecoder framework. The Tree-Transformer leverages the popular Transformer architecture [ 2],\nmodifying it to incorporate the tree structure of the data by adding a parent-sibling tree convolution\nblock. To show the power of our model, we focused our experiments on two common data types and\ntheir respective tree representations: Abstract Syntax Trees (ASTs) for code and Constituency Parse\nTrees (CPTs) for natural language.\nPreprint. Under review.\narXiv:1908.00449v1  [cs.LG]  1 Aug 2019\n2 Related Work\n2.1 Tree Structured Neural Networks\nExisting work on tree-structured neural networks can largely be grouped into two categories: encoding\ntrees and generating trees. Several types of tree encoders exist [9, 10, 11, 12]. The seminal work of\nTai et al. [9] laid the ground work for these methods, using a variant of a Long Short Term Memory\n(LSTM) network to encode an arbitrary tree. A large body of work has also focused on how to\ngenerate trees [13, 14, 15, 16, 17, 18, 19, 20]. The work of Dong et al., [ 13] and Alvarez-Melis\nand Jaakkola [14] each extend the LSTM decoder popular in Neuro Machine Translation (NMT)\nsystems to arbitrary trees. This is done by labeling some outputs as parent nodes and then forking off\nadditional sequence generations to create their children. Only a small amount of work has combined\nencoding and generation of trees into a tree-to-tree system [21, 22]. Of note is Chakraborty et al. [22]\nwho use a LSTM-based tree-to-tree method for source code completion.\nTo our knowledge our work is the ﬁrst to use a Transformer-based network on trees, and apply\nTree-to-Tree techniques to natural language and code correction tasks.\n2.2 Code Correction\nThere is extensive existing work on automatic repair of software. However, the majority of this work\nis rule-based systems which make use of small datasets (see [23] for a more extensive review of these\nmethods). Two successful, recent approaches in this category are that of Le et al., [ 24] and Long\nand Rinard [25]. Le et al. mine a history of bug ﬁxes across multiple projects and attempt to reuse\ncommon bug ﬁx patterns on newly discovered bugs. Long and Rinard learn and use a probabilistic\nmodel to rank potential ﬁxes for defective code. Unfortunately, the small datasets used in these works\nare not suitable for training a large neural network like ours.\nNeural network-based approaches for code correction are less common. Devlin et al. [26] generate\nrepairs with a rule-based method and then rank them using a neural network. Gupta et al. [27] were\nthe ﬁrst to train a NMT model to directly generate repairs for incorrect code. Additionally, Harer\net al. [28] use a Generative Adversarial Network to train an NMT model for code correction in the\nabsence of paired data. The works of Gupta et al. and Harer et al. are the closest to our own since\nthey directly correct code using an NMT system.\n2.3 Grammatical Error Correction\nGrammatical Error Correction (GEC) is the task of correcting grammatically incorrect sentences.\nThis task is similar in many ways to machine translation tasks. However, initial attempts to apply\nNMT systems to GEC were outperformed by phrase-based or hybrid systems [29, 30, 31].\nInitial, purely neural systems for GEC largely copied NMT systems. Yuan and Brisco [4] produced\nthe ﬁrst NMT style system for GEC by using the popular attention method of Bahdanau et al.\n[1]. Xie et al. [ 3] trained a novel character-based model with attention. Ji et al. [ 5] proposed a\nhybrid character-word level model, using a nested character level attention model to handle rare\nwords. Schmaltz et al. [6] used a word-level bidirectional LSTM network. Chollampatt and Ng [7]\ncreated a convolution-based encoder and decoder network which was the ﬁrst to beat state of the art\nphrased-based systems. Finally, Junczys-Dowmunt et al. [8] treated GEC as a low resource machine\ntranslation task, utilizing the combination of a large monolingual language model and a speciﬁcally\ndesigned correction loss function.\n3 Architecture\nOur Tree-Transformer architecture is based on the Transformer architecture of Vaswani et al. [2],\nmodiﬁed to handle tree-structured data. Our major change to the Transformer is the replacement of\nthe feed forward sublayer in both the encoder and decoder with a Tree Convolution Block (TCB).\nThe TCB allows each node direct access to its parent and left sibling, thus allowing the network to\nunderstand the tree structure. We follow the same overall architecture for the Transformer as Vaswani\net al., consisting of self-attention, encoder-decoder attention, and TCB sub layers in each layer. Our\n2\nFigure 1: Tree-Transformer model architecture.\nFigure 2: Tree-Transformer State Transfer\nmodels follow the 6 layer architecture of the base Transformer model with sublayer outputs, dmodel,\nof size 512 and tree convolution layers, dff , of size 2048.\n3.1 Parent-Sibling Tree Convolution\nTree convolution is computed for each node as:\nTCB(xt,xp,xs) = relu(xtWt + xpWp + xsWs + b)W2 + b2\nThe inputs xt, xp, and xs all come from the previous sublayer, xt from the same node, xp from the\nparent node, and xs from its left sibling. In cases where the node does not have either a parent (e.g.\nthe root node) or a left sibling (e.g. parents ﬁrst child), the inputs xp and xs are replaced with a\nlearned vector vp and vs respectively.\nIn addition to the TCB used in each sub layer, we also use a TCB at the input to both the encoder and\ndecoder. In the encoder this input block combines the embeddings from the parent, the sibling and\nthe current node (p, s, and t). In the decoder, the current node is unknown since it has not yet been\nproduced. Therefor,e this block only combines the parent and sibling embeddings, leaving out the\ninput xt from the equation above.\nThe overall structure of the network is shown in Figure 1. The inputs to each TCB come from the\nnetwork inputs, x/y, for the input blocks, and from the previous sublayer, saL/aL, for all other\nblocks.\n3\n3.2 Top-Down Encoder and Decoder\nBoth our encoder and decoder use a top-down approach where information ﬂows from the tree’s root\nnode down toward to the leaf nodes, as shown in Figure 2. Thus, leaf nodes have access to a large\nsub-tree, while parent nodes have a more limited view. An alternative approach would be to use a\nbottom-up encoder, where each node has access to its children, and a top-down decoder which can\ndisseminate this information. This bottom-up/top-down model is intuitive because information ﬂows\nup the tree in the encoder and then back down the decoder. However, we found that utilizing the\nsame top-down ordering for both encoder and decoder performed better, likely because the symmetry\nin encoder and decoder allows decoder nodes to easily attend to their corresponding nodes in the\nencoder. This symmetry trivializes copying from encoder to decoder, which is particularly useful in\ncorrection tasks where large portions of the trees remain unchanged between input and output.\n3.3 Generating Tree Structure\nIn order to generate each tree’s structure, we treat each set of siblings as a sequence. For each set\nof siblings, we generate each node one at a time, ending with the generation of an end-of-sequence\ntoken. The vocabulary used deﬁnes a set of leaf and parent nodes. When a parent node is generated,\nwe begin creation of that nodes children as another set of siblings.\n3.4 Depth First Ordering\nAs with any NMT system, each output value yt is produced from the decoder and fed back to\nsubsequent nodes as input during evaluation. Ensuring inputs yp and ys are available to each node\nrequires that parents are produced before children and that siblings are produced in left-to-right order.\nTo enforce this constraint, we order the nodes in a depth-ﬁrst manner. This ordering is shown by\nthe numbering on nodes in Figure 3. The self attention mechanism in the decoder is also masked\naccording to this order, so that each node only has access to previously produced ones.\n3.5 No Positional Encoding\nThe Transformer architecture utilizes a positional encoding to help localization of the attention\nmechanisms. Positional encoding is not as important in our model because the TCB allows nodes\nto easily locate its parent and siblings in the tree. In fact, we found that inclusion of a Positional\nEncoding caused the network to overﬁt, likely due to the relatively small size of the correction\ndatasets we use. Given this, our Tree-Transformer networks do not include a Positional Encoding.\n4 Why Tree Transformer\nIn this section we motivate the design of our Tree-Transformer model over other possible tree-based\narchitectures. Our choice to build upon the Transformer model was two-fold. First, Transformer-\nbased models have signiﬁcantly reduced time-complexity relative to Recurrent Neural Network\n(RNN) based approaches. Second, many of the building blocks required for a tree-to-tree translation\nsystem, including self-attention, are already present in the Transformer architecture.\n4.1 Recurrent vs Attention Tree Networks\nMany previous works on tree-structured networks used RNN-based tree architectures where nodes\nin layer L are given access to their parent or children in the same layer [ 13, 14, 22]. This state\ntransfer requires an ordering to the nodes during training where earlier nodes in the same layer\nmust be computed prior to later ones. This ordering requirement leads to poor time complexity for\ntree-structured RNN’s, since each node in a tree needs access to multiple prior nodes (e.g. parent and\nsibling). Accessing the states of prior nodes thus requires a gather operation over all past produced\nnodes. These gather operations are slow, and performing them serially for each node in the tree can\nbe prohibitively expensive.\nAn alternative to the RNN type architecture is a convolutional or attention-based one, where nodes\nin layer Lare given access to prior nodes in layer L−1. With the dependence in the same layer\nremoved, the gather operation can be batched over all nodes in a tree, resulting in one large gather\n4\nFigure 3: Example Constituency Parse Tree. The index of the node in depth-ﬁrst ordering is shown in\nthe bottom left of each node. Note: leaf nodes in the verb phrase do not have access to leaf nodes in\nthe left noun phrase without self-attention\noperation instead of T. From our experiments, this batching resulted in a reduction in training time\nof two orders of magnitude on our largest dataset; from around 2 months to less than a day.\n4.2 Conditional Probabilities and Self Attention\nThe Tree-Transformer’s structure helps the network produce grammatically correct outputs. However,\nfor translation/correction tasks we must additionally ensure that each output, ¯yt, is conditionally\ndependent on both the input, x, and on previous outputs, y<t. Conditioning the output on the input is\nachieved using an encoder-decoder attention mechanism [1]. Conditioning each output on previous\noutputs is more difﬁcult with a tree-based system. In a tree-based model like ours, with only parent\nand sibling connections, the leaf nodes in one branch do not have access to leaf nodes in other\nbranches. This leads to potentially undesired conditional independence between branches. Consider\nthe example constituency parse trees shown in Figure 3. Given the initial noun phrase \"My dog\", the\nfollowing verb phrase \"dug a hole\" is far more likely than \"gave a speech\". However, in a tree-based\nmodel the verb phrases do not have direct access to the sampled noun phrase, meaning both possible\nsentences would be considered roughly equally probable by the model.\nWe address the above limitation with the inclusion of a self-attention mechanism which allows nodes\naccess to all previously produced nodes. This mechanism, along with the depth-ﬁrst ordering of the\nnode described in section 3.4, gives each leaf node access to all past produced leaf nodes. Our model\nﬁts the standard probabilistic language model given as:\np(y|x) =\nT∏\nt=1\np(yt|y<t,x) (1)\nwhere tis the index of the node in the depth ﬁrst ordering.\n5 Training\nThis section describes the training procedure and parameter choices used in our model. We trained\nour models in parallel on 4 Nvidia Tesla V-100 GPU’s. Trees were batched together based on size\nwith each batch containing a maximum 20,000 words. We used the ADAM optimizer with inverse\nsquare root decay and a warm up of 16,000 steps. A full list of hyperparameters for each run is\nincluded in Appendix A.\n5.1 Regularization\nThe correction datasets we used in this paper are relatively small compared to typical NMT datasets.\nAs such we found a high degree of regularization was necessary. We included dropout of 0.3 before\nthe residual of each sub layer and attention dropout of 0.1. We also added dropout of 0.3 to each TCB\nafter the non-linearity. We applied dropout to words in both source and target embeddings as per [8]\nwith probabilities 0.2 and 0.1 respectively. We included label smoothing withϵls = 0.1\n5.2 Beam-Search\nBecause of the depth-ﬁrst ordering of nodes in our model, we can use beam search in the same way\nas traditional NMT systems. Following equation 1, we can compute the probability of a generated\n5\nTable 1: Sate IV Results\nArchitecture Precision Recall F 0.5\n4-layer LSTM 51.3 53.4 51.7\nTransformer 59.6 86.1 63.5\nTree-Transformer 84.5 85.7 84.7\nsub-tree of tnodes simply as the product of probabilities for each node. We utilize beam-search\nduring testing with a beam width of 6. Larger beam widths did not produce improved results.\n6 Experiments/Results\n6.1 Code Correction\nWe trained our Tree-Transformer on code examples taken from the NIST SATE IV dataset [32]. SATE\nIV contains around 120K C and C++ ﬁles from 116 different Common Weakness Enumerations\n(CWEs), and was originally designed to test static analyzers. Each ﬁle contains a bad function with a\nknown security vulnerability and at least one good function which ﬁxes the vulnerability. We generate\nAbstract Syntax Trees (ASTs) from these functions using the Clang AST framework [33, 34].\nTo provide a representation which is usable to our network, we tokenize the AST over a ﬁxed\nvocabulary in three ways. First, high level AST nodes and data types are represented by individual\ntokens. Second, character and numeric literals are represented by a sequence of ASCII characters\nwith a parent node deﬁning the kind of literal (e.g. Int Literal, Float Literal). Finally, we use variable\nrenaming to assign per function unique tokens to each variable and string. Our vocabulary consists\nof 403 tokens made up of 60 AST tokens, 23 data type tokens, 256 ASCII tokens, and 64 Variable\ntokens.\nUsing the SATE IV dataset requires pre-processing, which we do during data generation. First, many\nof the SATE IV functions contain large amounts of dead code. In these cases, the bad and good\nfunctions contain largely the same code, but one path will be executed during the bad function and\nanother in the good one. To make these cases more realistic, we removed the dead code. Second,\nalthough each ﬁle for a particular CWE contains unique functions at the text level, many of them\nare identical once converted to AST with renamed variables. Identical cases comes in two ﬂavors:\none where a bad function is identical to its good counterpart, and one where multiple bad functions\nfrom different ﬁles are identical. The ﬁrst occurs commonly in SATE IV in cases where the bad\nand good functions are identical except for differing function calls. Since we operate at the function\nlevel, examples of this case are not useful and are removed. The second case occurs when bad\nfunctions differ only in variable names, strings, or function calls. To handle this, we compare all\nbad functions tree representations and combine identical bad functions into a single bad tree and\na combination of all good trees from its component functions, with duplicate good trees removed.\nAfter pre-processing the data, we retain a total of 32,316 bad functions and 47,346 good functions.\nThese are split 80/10/10 into training/validation/testing.\nTo our knowledge this processing of the SATE IV dataset is new. As such, we compare our network\nto two NMT systems operating on sequence-based representation of the data; a 4 Layer LSTM with\nattention and a base Transformer model. These sequence-based models use a representation with\nan almost identical vocabulary and tokenization to our tree representation but they operate over the\ntokenized sequence output of the Clang Lexer instead of the AST.\nDuring testing, we utilized clangs source-to-source compilation framework to return the tree output\nof our networks to source code. We then compute precision, recall, and F0.5 scores of source code\nedits using the MaxMatch algorithm [35]. Results are given in Table 1 Our Tree-Transformer model\nperforms better than either of the other two models we considered. We believe this is because source\ncode is more naturally structured as a tree than as a sequence, lending itself to our tree-based model.\n6.2 Grammar Error Correction\nWe applied our tree-based model as an alternative to NMT and phrase-based methods for GEC.\nSpeciﬁcally, we encoded incorrect sentences using their constituency parse trees, and then generated\ncorrected parse trees. Constituency parse trees represent sentences based on their syntactic structure\n6\nby ﬁtting them to a phrase structured grammar. Words from the input sentence become leaf nodes of\ntheir respective parse trees, and these nodes are combined into phrases of more complexity as we\nprogress up the tree. [36, 11]\nA large amount of research has focused on the generation of constituency parse trees [ 37, 38, 39].\nWe utilize the Stanford NLP group’s shift-reduce constituency parser [40] to generate trees for both\nincorrect and correct sentences in our datasets. These are represented to our network with a combined\nvocab consisting of 50Kword level tokens and 27 parent tokens. The parent tokens come from the\npart of speech tags originally deﬁned by the Penn Treebank [41] plus a root token. Following recent\nwork in GEC [8, 7], the word level tokens are converted into sub-words using a Byte Pair Encoding\n(BPE) trained on the large Wikipedia dataset [ 42]. The BPE segments rare words into multiple\nsubwords, avoiding the post-processing of unknown words used in many existing GEC techniques.\nWe test our network on two GEC benchmarks: the commonly used NUCLE CoNLL 2014 task [43],\nand the AESW dataset [ 44]. CoNLL 2014 training data contains 57K sentences extracted from\nessays written by non-native English learners. Following the majority of existing GEC work, we\naugment the small CoNLL dataset with the larger NIST Lang-8 corpus. The Lang-8 data contains\n1.1M sentences of crowd sourced data taken from the Lang-8 website, making it noisy and of lower\nquality than the CoNLL data. We test on the 1,312 sentences in CoNLL 2014 testing set and use the\n1,381 sentences of the CoNLL 2013 testing set as validation data. For evaluation we use the ofﬁcial\nCoNLL M2scorer algorithm to determine edits and compute precision and recall [35].\nWe also explore the large AESW dataset. AESW was designed in order to train grammar error\nidentiﬁcation systems. However, it includes both incorrect and corrected versions of sentences,\nmaking it useful for GEC as well. AESW contains1.2Mtraining, 143Ktesting, and 147Kvalidation\nsentences. The AESW data was taken from scientiﬁc papers authored by non-native speakers, and as\nsuch contains far more formal language than CoNLL.\n6.2.1 GEC Training\nFor GEC we include a few additions to the training procedure described in Section 5. First, we\npre-train the network in two ways on 100M sentences from the large monolingual dataset provided\nby [29]. We pre-train the decoder in our network as a language model, including all layers in the\ndecoder except the encoder-decoder attention mechanism. We also pre-train the entire model as a\ndenoising-autoencoder, using a source embedding word dropout of 0.4.\nFor loss we use the edit-weighted MLE objective deﬁned by Junczys-Dowmunt et al. [8]:\nMLE(x,y) = −\nT∑\nt=1\nλ(yt)logP(yt|x,y <t)\nwhere (x,y) are a training pair, and λ(yt) is 3 if yt is part of an edit and 1 otherwise. We compute\nwhich tokens are part of an edit using the python Apted graph matching library [45, 46, 47].\nDuring beam-search we ensemble our networks with the monolingual language-model used for\npre-training as per [3]:\ns(y|x) =\nT∑\nt=1\nlogP(yt|x,y<t) + αlogPlm(yt|y<t)\nwhere αis chosen between 0 and 1 based on the validation set. Typically, we found an alpha of 0.15\nperformed best. Networks with α> 0 are labeled with +Mon-Ens\n6.2.2 CoNLL 2014 Analysis\nResults for CoNLL 2014 are provided in Table 2. Our Tree-Transformer achieves signiﬁcantly higher\nrecall than existing approaches, meaning we successfully repair more of the grammar errors. However,\nour precision is also lower which implies we make additional unnecessary edits. We attribute this\ndrop to the fact that our method tends to generate examples which ﬁt a structured grammar. Thus\nsentences with uncommon grammar tend to be converted to a more common way of saying things.\nAn example of this effect is provided in table 3.\n7\nTable 2: CoNLL 2014 results\nArchitecture Precision Recall F 0.5\nPrior State-of-the-art Approaches\nChollampatt and Ng 2017. 62.74 32.96 53.14\nJunczys-Dowmunt and Grundkiewicz. 2016 61.27 27,98 49.49\nPrior Neural Approaches\nJi et al. 2017 - - 45.15\nSchmaltz et al. 2017 - - 41.37\nXie et al. 2016 49.24 23.77 40.56\nYuan and Briscoe. 2016 - - 39.90\nChollampatt and Ng. 2018 65.49 33.14 54.79\nJunczys-Dowmunt et al. 2018 63.0 38.9 56.1\nThis Work\nTree-Transformer 57.39 28.12 47.50\nTree-Transformer +Mon 58.45 30.42 49.35\nTree-Transformer +Mon +Mon-Ens 57.84 33.26 50.39\nTree-Transformer +Auto 65.22 30.38 53.05\nTree-Transformer +Auto +Mon-Ens 59.14 43.23 55.09\nTable 3: CoNLL 2014 Example Output\nInput In conclusion , we could tell the beneﬁts of telling genetic risk to the carriers relatives overweights the costs .\nLabels In conclusion , we can see thatthe beneﬁts of telling genetic risk to the carrier’srelatives outweighs the costs .\nIn conclusion , we can see thatthe beneﬁts of disclosing genetic risk to the carriers relatives outweigh the costs.\nIn conclusion , we can see thatthe beneﬁts of revealing genetic risk to the carrier’srelatives outweigh the costs .\nNetwork In conclusion , it can be arguedthat the beneﬁts of revealing genetic risk to one ’srelatives outweighs the costs .\n6.2.3 AESW Analysis\nResults for AESW are provided in Table 4. We achieve the highest to date F 0.5 score on AESW,\nincluding beating out our own sequence-based Transformer model. We attribute this to the fact that\nAESW is composed of samples taken from submitted papers. The more formal language used in this\ncontext may be a better ﬁt for the structured grammar used by our model.\nTable 4: AESW results\nArchitecture Precision Recall F 0.5\nPrior Approaches\nSchmaltz et al. 2017 (Phrased-based) - - 38.31\nSchmaltz et al. 2017 (Word LSTM) - - 42.78\nSchmaltz et al 2017 (Char LSTM) - - 46.72\nThis Work\nTransformer (Seq to Seq) 52.3 36.2 48.03\nTree-Transformer 55.4 37.1 50.43\n7 Conclusion\nIn this paper we introduced the Tree-Transformer architecture for tree-to-tree correction tasks. We\napplied our method to correction datasets for both code and natural language and showed an increase\nin performance over existing sequence-based methods. We believe our model achieves its success\nby taking advantage of the strong grammatical structure inherent in tree-structured representations.\nFor the future we hope to apply our approach to other tree-to-tree tasks, such as natural language\ntranslation. Additionally, we intend to extend our approach into a more general graph-to-graph\nmethod.\n8\nReferences\n[1] D. Bahdanau, K. Cho, and Y . Bengio. Neural Machine Translation by Jointly Learning to Align\nand Translate. International Conference on Learning Representations (ICLR), 2015.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention Is All You Need. Neural Information Processing Systems (NIPS) ,\n2017.\n[3] Z. Xie, A. Avati, N. Arivazhagan, D. Jurafsky, and A. Y . Ng. Neural Language Correction with\nCharacter-Based Attention. arXiv.org, 2016.\n[4] Z. Yuan and T. Briscoe. Grammatical error correction using neural machine translation. North\nAmerican Chapter of the Association of Computational Linguistics (NAACL), 2016.\n[5] J. Ji, Q. Wang, K. Toutanova, Y . Gong, S. Truong, and J. Gao. A Nested Attention Neural\nHybrid Model for Grammatical Error Correction. Association of Computational Linguistics\n(ACL), 2017.\n[6] A. Schmaltz, Y . Kim, A. M. Rush, and S. M. Shieber. Adapting Sequence Models for Sentence\nCorrection. Empirical Methods in Natural Language (EMNLP), 2017.\n[7] S. Chollampatt and H. T. Ng. A Multilayer Convolutional Encoder-Decoder Neural Network\nfor Grammatical Error Correction. Association for the Advancement of Artiﬁcial Intelligence\n(AAAI), 2018.\n[8] M. Junczys-Dowmunt, R. Grundkiewicz, S. Guha, and K. Heaﬁeld. Approach neural grammati-\ncal error correction as a low-resource machine translation task. North American Chapter of the\nAssociation of Computational Linguistics (NAACL-HLT), 2018.\n[9] K. S. Tai, R. Socher, and C. D. Manning. Improved Semantic Representations From Tree-\nStructured Long Short-Term Memory Networks. Association of Computational Linguistics\n(ACL), 2015.\n[10] X. Zhu, P. Sobhani, and H. Guo. Long Short-Term Memory Over Tree Structures. International\nConference on Machine Learning (ICML), 2015.\n[11] R. Socher, C. C.-Y . Lin, A. Y . Ng, and C. D. Manning. Parsing Natural Scenes and Natural\nLanguage with Recursive Neural Networks. International Conference on Machine Learning\n(ICML), 2011.\n[12] A. Eriguchi, K. Hashimoto, and Y . Tsuruoka. Tree-to-Sequence Attentional Neural Machine\nTranslation. Association of Computational Linguistics (ACL), 2016.\n[13] L. Dong and M. Lapata. Language to Logical Form with Neural Attention. Association of\nComputational Linguistics (ACL), 2016.\n[14] D. A.-M. . T. S. Jaakkola. Tree-structured decoding with doubly-recurrent neural networks.\nInternational Conference on Learning Representations (ICLR), 2017.\n[15] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a Foreign\nLanguage. Neural Information Processing Systems (NIPS), 2015.\n[16] R. Aharoni and Y . Goldberg. Towards String-to-Tree Neural Machine Translation.Association\nof Computational Linguistics (ACL), 2017.\n[17] M. Rabinovich, M. Stern, and D. Klein. Abstract Syntax Networks for Code Generation and\nSemantic Parsing. Association of Computational Linguistics (ACL), 2017.\n[18] E. Parisotto, A.-r. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli. Neuro-Symbolic Program\nSynthesis. International Conference on Learning Representations (ICLR), 2017.\n[19] P. Yin and G. Neubig. A Syntactic Neural Model for General-Purpose Code Generation.\nAssociation of Computational Linguistics (ACL), 2017.\n[20] X. Zhang, L. Lu, and M. Lapata. Top-down Tree Long Short-Term Memory Networks. North\nAmerican Chapter of the Association of Computational Linguistics (NAACL), 2016.\n[21] X. Chen, C. Liu, and D. Song. Tree-to-tree Neural Networks for Program Translation. Neural\nInformation Processing Systems (NeurIPS), 2018.\n[22] S. Chakraborty, M. Allamanis, and B. Ray. Tree2tree neural translation model for learning\nsource code changes. arXiv pre-print, 2018.\n9\n[23] M. Monperrus. Automatic software repair: A bibliography. ACM Computing Surveys (CSUR),\n2018.\n[24] X. B. D. Le, D. Lo, and C. Le Goues. History driven program repair. Software Analysis,\nEvolution, and Reengineering (SANER), 2016.\n[25] F. Long and M. Rinard. Automatic patch generation by learning correct code. Principles of\nProgramming Languages (POPL), 2016.\n[26] Devlin, Jacob, Uesato, Jonathan, Singh, Rishabh, and Kohli, Pushmeet. Semantic Code Repair\nusing Neuro-Symbolic Transformation Networks. arXiv:1710.11054, 2017.\n[27] R. Gupta, S. Pal, A. Kanade, and S. Shevade. Deepﬁx: Fixing common c language errors\nby deep learning. Association for the Advancement of Artiﬁcal Intelligence (AAAI) , pages\n1345–1351, 2017.\n[28] J. Harer, O. Ozdemir, T. Lazovich, C. P. Reale, R. L. Russell, L. Y . Kim, and P. Chin. Learning\nto Repair Software Vulnerabilities with Generative Adversarial Networks. Neural Information\nProcessing Systems (NeuroIPS), 2018.\n[29] M. Junczys-Dowmunt and R. Grundkiewicz. Phrase-based Machine Translation is State-of-the-\nArt for Automatic Grammatical Error Correction. Empirical Methods in Natural Language\n(EMNLP), 2016.\n[30] S. Chollampatt and H. T. Ng. Connecting the Dots: Towards Human-Level Grammatical\nError Correction. The 12th Workshop on Innovative Use of NLP for Building Educational\nApplications. Association for Computational Linguistics (ACL), 2017.\n[31] D. Dahlmeier, H. T. Ng, and S. M. Wu. Building a Large Annotated Corpus of Learner\nEnglish - The NUS Corpus of Learner English. North American Chapter of the Association of\nComputational Linguistics (NAACL), 2013.\n[32] V . Okun, A. Delaitre, and P. Black. Report on the static analysis tool exposition (sate) iv.\nTechnical Report, 2013.\n[33] C. Lattner and V . S. Adve. LLVM - A Compilation Framework for Lifelong Program Analysis\n& Transformation. CGO, 2004.\n[34] Clang library. clang.llvm.org, 2011.\n[35] Ofﬁcal scorer for conll 2014 shared task. https://github.com/nusnlp/m2scorer/\nreleases, 2014.\n[36] C. Goller and A. Kuchler. Learning task-dependent distributed representations by backpropaga-\ntion through structure. International Conference on Neural Networks (ICNN’96), 1996.\n[37] D. Chen and C. D. Manning. A fast and accurate dependency parser using neural networks.\nEmperical Methods in Natural Language Processing (EMNLP), 2014.\n[38] R. Socher, J. Bauer, and A. Y . Manning, Christopher D.and Ng. Parsing with compositional\nvector grammars. Association for Computational Linguistics (ACL), 2013.\n[39] D. Klein and C. Manning. Accurate unlexicalized parsing. Association for Computational\nLinguistics (ACL), 2003.\n[40] Shift-reduce constituency parser. https://nlp.stanford.edu/software/srparser.\nhtml, 2014.\n[41] Penn treebank ii tags.https://web.archive.org/web/20130517134339/http://bulba.\nsdsu.edu/jeanette/thesis/PennTags.html, 2016.\n[42] B. Heinzerling and M. Strube. BPEmb: Tokenization-free Pre-trained Subword Embeddings\nin 275 Languages. In Proceedings of the Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), 2018.\n[43] H. Ng, S. Wu, T. Briscoe, C. Hadiwinoto, R. Susanto, and C. Bryant. The conll-2014 shared task\non grammatical error correction. Conference on Computational Natural Language Learning,\nAssociation for Computational Linguistics (ACL), 2014.\n[44] V . Daudaravicius, R. Banchs, E. V olodine, and C. Napoles. A report on the automatic evaluation\nof scientiﬁc writing shared task. 11th Workshop on Innovative Use of NLP for Building\nEducational Applications, Association for Computational Linguistics (ACL), 2016.\n10\n[45] Apted python library. https://pypi.org/project/apted/, 2015.\n[46] M. Pawlik and N. Augsten. Tree edit distance: Robust and memory-efﬁcient. Information\nSystems 56, 2016.\n[47] M. Pawlik and N. Augsten. Efﬁcient computation of the tree edit distance. ACM Transactions\non Database Systems, 2015.\n11\nAppendix A hyperparameters\nHyperparameters utilized are listed in tables 5 and 6. Default hyperparameters are listed at the\ntop of each table. A blank means the run utilized the default hyperparameter. Explanation of\nhyperparemeters follows.\n• N - number of layers\n• dmodel - size of sublayer outputs - see [2]\n• dff - size of inner layer in TDB/FF for Tree-Transformer/Transfomer\n• h- Number of attention heads\n• dk - Size of keys in attention mechanism\n• dv - Size of values in attention mechanism\n• pdrop - Dropout probability between sub-layers\n• pdattn - Dropout probability on attention mechanism - see [2]\n• pd ff- Dropout probability on inner layer of TDB/FF for Tree-Transformer/Transfomer\n• pdes - Source embedding word dropout probability\n• pdet - Target embedding word dropout probability\n• ϵls - Label Smoothing ϵ\n• lr - Learning Rate, We use isr learning rate as per [2]. As such this learning rate will never\nbe fully met, maximum learning rate depends upon warmup.\n• warmup - number of steps for linearly LR warmup\n• train steps- total number of steps for training\n• Mon - Initialized from monolingual pre-trained network\n• Auto - Initialized from autoencoder pre-trained network\n• Mon-Ens - Ensemble trained network with monolingual netword during beam search as per\n[3]\n• EW-MLE - Use edit-weight MLE objective function as per [8]\n• Time (Hours)- Total training time\n12\nTable 5: Model Parameters\nArchitecture N dmodel dff h d k dv pdrop pdattn pd ff pdes pdet ϵls\ndefault 6 512 2048 8 64 64 0.3 0.1 0.3 0.2 0.1 0.1\nSate IV\nLSTM 4 1024 N/a\nTransformer\nTree-Transformer\nGEC Pretraining\nMonolingual\nAutoencoder 0.4\nConll 2014\nTree-Transformer\nTree-Transformer +Mon\nTree-Transformer +Mon +Mon-Ens\nTree-Transformer +Auto\nTree-Transformer +Auto +Mon-Ens\nAesw\nTransformer\nTree-Transformer\nTable 6: Training Parameters\nArchitecture lr warmup train steps Mon Auto Mon-Ens EW-MLE Time (Hours)\ndefault d−0.5\nmodel 4000 100k - - - -\nSate IV\nLSTM 40\nTransformer 18\nTree-Transformer 22\nGEC Pretraining\nMonolingual 500k 38\nAutoencoder 500k 50\nConll 2014\nTree-Transformer 16000 3 26\nTree-Transformer +Mon 16000 ✓ 3 26\nTree-Transformer +Mon +Mon-Ens 16000 ✓ ✓ 3 26\nTree-Transformer +Auto 16000 ✓ 3 26\nTree-Transformer +Auto +Mon-Ens 16000 ✓ ✓ 3 26\nAesw\nTransformer 16000 ✓ ✓ 3 19\nTree-Transformer 16000 ✓ ✓ 3 25\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8287298083305359
    },
    {
      "name": "Transformer",
      "score": 0.6028052568435669
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5588722825050354
    },
    {
      "name": "Natural language processing",
      "score": 0.5168929100036621
    },
    {
      "name": "Grammar",
      "score": 0.501915693283081
    },
    {
      "name": "Tree (set theory)",
      "score": 0.49950599670410156
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4694150388240814
    },
    {
      "name": "Source code",
      "score": 0.46795380115509033
    },
    {
      "name": "Natural language",
      "score": 0.46466195583343506
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.44449836015701294
    },
    {
      "name": "Artificial neural network",
      "score": 0.37325361371040344
    },
    {
      "name": "Programming language",
      "score": 0.2723330557346344
    },
    {
      "name": "Mathematics",
      "score": 0.07802537083625793
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}