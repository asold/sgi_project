{
  "title": "Numerical Optimizations for Weighted Low-rank Estimation on Language Models",
  "url": "https://openalex.org/W4385572781",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101494441",
      "name": "Hua Ting",
      "affiliations": [
        "Research!America (United States)",
        "Samsung (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5040051663",
      "name": "Yen-Chang Hsu",
      "affiliations": [
        "Research!America (United States)",
        "Samsung (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5010467859",
      "name": "Felicity Wang",
      "affiliations": [
        "Research!America (United States)",
        "Samsung (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5006753786",
      "name": "Qian Lou",
      "affiliations": [
        "Research!America (United States)",
        "Samsung (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5058974318",
      "name": "Yilin Shen",
      "affiliations": [
        "Research!America (United States)",
        "Samsung (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5044205212",
      "name": "Hongxia Jin",
      "affiliations": [
        "Research!America (United States)",
        "Samsung (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6700531047",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3189437251",
    "https://openalex.org/W4295224301",
    "https://openalex.org/W2769856846",
    "https://openalex.org/W2142466236",
    "https://openalex.org/W2078841894",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4283809320",
    "https://openalex.org/W1844261860",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2340502990",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W1511814458",
    "https://openalex.org/W3090668255",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3116594510",
    "https://openalex.org/W2707890836",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3168428105",
    "https://openalex.org/W2949419207",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2965862774",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2165395308",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3187127611",
    "https://openalex.org/W2964258799",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963846996"
  ],
  "abstract": "Singular value decomposition (SVD) is one of the most popular compression methods that approximate a target matrix with smaller matrices. However, standard SVD treats the parameters within the matrix with equal importance, which is a simple but unrealistic assumption. The parameters of a trained neural network model may affect the task performance unevenly, which suggests non-equal importance among the parameters. Compared to SVD, the decomposition method aware of parameter importance is the more practical choice in real cases. Unlike standard SVD, weighed value decomposition is a non-convex optimization problem that lacks a closed-form solution. We systematically investigated multiple optimization strategies to tackle the problem and examined our method by compressing Transformer-based language models.Further, we designed a metric to predict when the SVD may introduce a significant performance drop, for which our method can be a rescue strategy.The extensive evaluations demonstrate that our method can perform better than current SOTA methods in compressing Transformer-based language models.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1404–1416\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nNumerical Optimizations for Weighted Low-rank\nEstimation on Language Model\nTing Hua∗, Yen-Chang Hsu ∗, Felicity Wang, Qian Lou, Yilin Shen, Hongxia Jin\nSamsung Research America\n{ting.hua,yenchang.hsu,f.wang1,qian.lou}@samsung.com\n{yilin.shen,hongxia.jin}@samsung.com\nAbstract\nSingular value decomposition (SVD) is one of\nthe most popular compression methods that ap-\nproximate a target matrix with smaller matrices.\nHowever, standard SVD treats the parameters\nwithin the matrix with equal importance, which\nis a simple but unrealistic assumption. The pa-\nrameters of a trained neural network model may\naffect the task performance unevenly, which\nsuggests non-equal importance among the pa-\nrameters. Compared to SVD, the decomposi-\ntion method aware of parameter importance is\nthe more practical choice in real cases. Unlike\nstandard SVD, weighted value decomposition\nis a non-convex optimization problem that lacks\na closed-form solution. We systematically in-\nvestigated multiple optimization strategies to\ntackle the problem and examined our method\nby compressing Transformer-based language\nmodels. Further, we designed a metric to pre-\ndict when the SVD may introduce a significant\nperformance drop, for which our method can\nbe a rescue strategy. The extensive evaluations\ndemonstrate that our method can perform bet-\nter than current SOTA methods in compressing\nTransformer-based language models.\n1 Introduction\nTransformer-based language models such as BERT\n(Devlin et al., 2018) have obtained significant suc-\ncess in a variety of Natural Language Processing\ntasks, such as language modeling (Radford et al.,\n2018), text classification (Wang et al., 2018), ques-\ntion answering (Rajpurkar et al., 2016), and sum-\nmarization (Liu, 2019). Despite their success, these\nmodels usually contain millions or even billions of\nparameters, pre-trained by the large corpus. How-\never, the downstream tasks may only focus on a\nspecific scenario, such that only a small amount of\nparameters in the big Transformer model will con-\ntribute to the performance of the target task. Also,\nthe massive size of Transformer models prohibits\n*These authors contributed equally to this work.\ntheir deployments to resource-constrained devices.\nTherefore, compression of the Transformer-based\nlanguage model attracts extensive interests.\nLow-rank factorization (Golub and Reinsch,\n1971; Noach and Goldberg, 2020) aims to approx-\nimate each parameter matrix in the trained model\nby two smaller matrices. This line of compression\nstrategy will naturally inherit the knowledge of the\nbig trained model without expensive generic re-\ntraining, and is the orthogonal direction to other\ncompression approaches such as Knowledge dis-\ntillation (Sun et al., 2019; Sanh et al., 2019; Jiao\net al., 2019) or Quantization (Shen et al., 2020;\nZhao et al., 2021).\nHowever, applying standard SVD to approxi-\nmate the learned weights often results in a signifi-\ncant task performance drop. Previous work shows\nthat this phenomenon may be caused by a strong\nassumption held by the standard SVD, that the pa-\nrameters in the matrix are equally crucial to the\nperformance (Hsu et al., 2021). Also, it has been\nobserved that different parameters in Transformer\nmodels have different impacts on the overall task\nperformance (Shen et al., 2020).\nFollowing FWSVD (Hsu et al., 2021), we utilize\nFisher information (Pascanu and Bengio, 2014)\nto weigh the importance of parameters, so that\nthe objective of matrix factorization will jointly\nconsider matrix reconstruction error and the tar-\nget task performance. In the standard SVD, all\nthe local minima are saddle points, ensuring a\nclosed-form global optimal solution (Srebro and\nJaakkola, 2003). This property no longer holds true\nto our new objective weighted by Fisher informa-\ntion. Without the closed-form solution, we revert\nto the numerical optimization methods to minimize\nthe weighted objective. As our method can provide\na more accurate solution than FWSVD (Hsu et al.,\n2021), we name our proposed method as TFWSVD\n(True Fisher Weighted SVD). Our results reveal the\nhybrid optimizer we called Adam_SGD can best\n1404\nfit our problem, with its switching point estimated\nby the row-based analytic solution. We also in-\nvestigated the scenarios where SVD fails, under\nthe guidance of the metric we introduced to mea-\nsure the variance of parameter importance, with\nthe example of analyzing the matrices within the\nTransformer blocks.\nIn summary, this work makes the following con-\ntributions: (1) we provide several optimization\nmethods to search for the best numerical solution\nfor low-rank estimation weighted by the Fisher in-\nformation; (2) we perform extensive evaluations\non various language tasks, showing our TFWSVD\nachieves better performance than the SOTA com-\npression methods, and can further compress already\ncompact models; (3) through the analysis of factor-\nizing sub-structures inside the Transformer blocks,\nwe provide the guide about when SVD may fail but\nTFWSVD can retain the performance.\n2 Background\n2.1 Model Compression with SVD\nSingular value decomposition (SVD) decomposes\na matrix, e.g., W∈RN×M into three matrices:\nW = USVT ≈UrSrVT\nr , (1)\nwhere U∈RN×l, V∈RM×l, and lis the rank of\nmatrix W. S is a diagonal matrix of non-zero sin-\ngular values diag(σ1,,...,σ l), where σ1 ≥σ2 ≥\n···σl >0. Ur, Sr, and Vr represent the truncated\nmatrices with rank rand approximate the original\nmatrix with a less total number of parameters.\nThe computation of a linear layer in neural net-\nworks can be rewritten as below with input data\nX∈R1×N , weight matrix W∈RN×M , and bias\nb∈Rl×M :\nZ = XW + b ≈(XUrSr)VT\nr + b. (2)\nThe typical implementation of factorization is to\nreplace the large W with two smaller linear lay-\ners: 1) The weight matrix of the first layer is US,\nwhich has Nr parameters without bias. 2) While\nthe weight matrix of the second layer is V, with\nMr parameters plus bias. The truncation happens\nwhen r is less than l. For example, if the total\nnumber of parameters for approximating W is\nNr + Mr, then the reduced number of parame-\nters will be NM −(Nr + Mr).\n2.2 Fisher information\nA classical way to measure the importance of\nparameters is through the observed information,\ni.e.Fisher information. It measures the amount of\ninformation that an observable dataset D carries\nabout a model parameter w. The accurate values of\nFisher information are generally intractable since\nthe computation will require marginalizing over the\ndata D. In practice, the empirical Fisher informa-\ntion is estimated as follows:\nIw = E\n[( ∂\n∂w log p(D|w)\n)2]\n≈ 1\n|D|\n|D|∑\ni=1\n( ∂\n∂wL(di; w)\n)2\n= ˆIw.\n(3)\nGiven a target task objective L(e.g., cross-entropy\nfor a classification task), the estimated information\nˆIw accumulates the squared gradients over the train-\ning data di ∈D. The parameters that cause large\nabsolute gradient of the task objective will have a\nlarge value in ˆIw, and are considered important to\nthe target task.\n2.3 Related works\nThe report of applying SVD to the Transformer lay-\ners is scarce. Several previous works applied SVD\nto compress the word embedding layer (Chen et al.,\n2018a; Acharya et al., 2019). Although (Noach\nand Goldberg, 2020) combined knowledge distil-\nlation to fine-tune the resulting compressed model,\nthey didn’t address the issue of poor performance\nwhen fine-tuning is not applied. Experiments show\nthat our proposed method can retrain most of the\nperformance, providing a much better initialization\nfor the fine-tuning.\nThe use of Fisher information has appeared in\nmany problem settings that also need to estimate\nthe importance of model parameters, for example,\nto avoid catastrophic forgetting in continual learn-\ning (Kirkpatrick et al., 2017; Hua et al., 2021) or\nmodel pruning (Liu et al., 2021; Molchanov et al.,\n2019b). However, none of these work has explored\nits potential in assisting low-rank approximation\nfor model compression.\nMost previous work seeking the numerical so-\nlution for low-rank approximation is designed for\nunweighted cases, with applications such as pre-\ndicting the missing values recommendation system\n(Yu et al., 2014; Zhou et al., 2008). Also, a few\n1405\nattempts have been made to solve the weighted low-\nrank approximation problem through EM-based al-\ngorithm (Srebro and Jaakkola, 2003), or alternating\nleast squares (He et al., 2016).\nThe closest previous work to this paper is\nFWSVD (Hsu et al., 2021), which points out that\nthe “even importance” assumption held by SVD\nmay cause a performance drop. FWSVD also uti-\nlizes Fisher information to weigh the importance\nof parameters. However, during the decomposition\nprocess, FWSVD assumes that parameters within\neach weight matrix row share the same importance\nvalue, which is still a strong assumption. Exper-\nimental results show that our TFWSVD can find\nmore accurate solutions than FWSVD, as each pa-\nrameter is associated with its own importance in\nTFWSVD.\n3 Method\n3.1 Low-rank factorization objective weighted\nby Fisher information\nThe objective of the generic low-rank approx-\nimation is to minimize the Frobenius norm\n||W −AB||2, which is the sum squared differ-\nences of a reconstructed matrix AB to the target\nmatrix W. As mentioned above, Singular value\ndecomposition (SVD) can solve this problem effi-\nciently by having A = US and B = VT. As the\nimportance of each element wij in W can be cal-\nculated through its Fisher information, we would\nlike to find the reconstructed matrix AB that mini-\nmizes the weighted Frobenius distance J(A,B) as\nfollows (⊗denotes element-wise multiplication):\nJ(A,B) =ˆI ⊗(W −AB)2\n=\n∑\ni,j\nˆIwi,j (wi,j −aT\ni bj)2. (4)\nTo prevent over fitting,L2 regularization terms con-\ntrolled by parameter λcan be added to the objec-\ntive, so that Equation (4) can be rewritten as:\nJ(A,B) =\n∑\ni,j\nˆIwi,j (wi,j −aT\ni bj)2\n+ λ(\n∑\ni\n||ai||2 +\n∑\nj\n||bj||2).\n(5)\n3.2 Optimization methods\nSVD has an analytic solution, since all of its local\nminima are global. However, this can not hold true\nwhen weights are introduced. Without a closed-\nform solution, we discuss several numerical opti-\nmization methods to minimize J(A,B).\n3.2.1 Alternating Least Squares\nAlthough the optimization problems in (4) and (5)\nare non-convex, they can be converted to quadratic\nproblems with globally optimal solutions, if A or\nB is fixed. Therefore, Alternating Least Squares\n(ALS) is suitable to solve such problems (Hastie\net al., 2015). ALS will alternately optimize A or\nB by keeping the other one fixed, and decrease\nJ(A,B) until convergence. When the other ma-\ntrix is fixed, minimizing J(A,B) with respect to\nA or B is equivalent to minimize the following\nobjectives:\nJ(ai) =||ˆIW[i,:](W[i,:] −Bai)||2 + λ||ai||2\nJ(bj) =||ˆIW[:,j](W[:,j] −Abj)||2 + λ||bj||2,\n(6)\nwhich can lead to the closed-form solutions:\nai = (BTˆIW[i,:]B + λΣ)−1BTˆIW[i,:]W[i,:]\nbj =(ATˆIW[:,j]A + λΣ)−1ATˆIW[:,j]W[:,j]\n,\n(7)\nwhere Σ is the identity matrix, while ˆIW[i,:] and\nˆIW[:,j] are the Fisher information vector of i-th row\nand j-th column in original matrix W, respectively.\n3.2.2 Stochastic Gradient Descent\nStochastic Gradient Descent (SGD) is also shown\nto be effective for matrix factorization problems\n(Koren et al., 2009). Specifically in our problem,\neach update of SGD can be represented as:\nai ←ai + 2η(ewij bj −λai)\nbj ←bj + 2η(ewij ai −λbj), (8)\nwhere η is the learning rate, and ewi,j =\nˆIwi,j (wi,j −aT\ni bj). More generally, the iterations\nof SGD can be described as:\nh(k) ←h(k−1) −η∇J(h(k−1)), (9)\nwhere h(k) denotes the k-th iterate that can be sub-\nstituted by ai or bj.\n3.2.3 Adaptive Moment Estimation\nSGD will scale gradient uniformly in all directions,\nmaking the training process inefficient and sensi-\ntive to the learning rate. Several adaptive methods\nhave been proposed to overcome this shortcom-\ning, among which Adaptive Moment Estimation\n1406\n(Adam) is one of the most widely used approaches\n(Kingma and Ba, 2015). Following the form of\nSGD updates shown in (9), the Adam update itera-\ntions can be written as:\nh(k) ←h(k−1)−η(k−1)·\n√\n1 −β(k)\n2\n1 −β(k)\n1\n· m(k−1)\n√\nv(k−1) + ε\n,\n(10)\nwhere h(k) and η are the same as Equation (9),\nm(k−1) and v(k−1) are calculated as follows:\nm(k−1) = β1m(k−2) + (1−β1)∇J(h(k−1))\nv(k−1) = β2v(k−2) + (1−β2)∇J(h(k−1))2.\n(11)\nAlthough Adam requires minimal tuning and\nenjoys fast initial progress, it is not without faults.\nRecent work has shown that the solutions found by\nAdam can be much worse at generalization than\nthose found by SGD (Akiba et al., 2017; Ida and\nFujiwara, 2020).\n3.2.4 Adam Switching to SGD\nPrevious studies show that switching from Adam to\nSGD may contribute to the performance, however,\nthe switching point is crucial for the overall per-\nformance and usually is task-dependent (Ida and\nFujiwara, 2020). Here we propose a simple method\nto calculate the switching point for our Fisher in-\nformation weighted matrix factorization problem.\nAlthough weighted SVD does not have a closed-\nform solution when each element has its weight,\nthe optimization problem (5) has a close form in the\ncase that elements within the same row share the\nsame weight (Hsu et al., 2021). Therefore, we can\ncalculate an approximate solution for the optimiza-\ntion problem (5) based on row-wise Fisher infor-\nmation, which can be solved as the “threshold” for\nour switching point from Adam to SGD (Hsu et al.,\n2021). If we define the importance for the row ito\nbe the summation of the row, i.e., ˆIWi = ∑\nj\nˆIWij\nand diagonal matrix ˆI = diag(\n√\nˆIW1,...,\n√\nˆIWN ),\nthen the optimization problem of Equation (4) can\nbe written as:\nJ(A,B) ≈ˆJ(A,B) =||ˆIW −ˆIAB||2. (12)\nOptimization problem (12) can be solved by the\nstandard SVD on ˆIW. If we denote svd(ˆIW) =\n(U∗,S∗,V ∗), then the solution of Equation (12)\nwill be A= ˆI−1U∗S∗, and B = V∗T . The value\nof ˆJ(A,B) is served as our switching point from\nAdam to SGD, that the training process will be\noptimized by Adam when the current loss is larger\nthan ˆJ(A,B), and then taken over by SGD when\nits loss is smaller than ˆJ(A,B).\nBesides the hard threshold calculated in (12),\nwe also set a soft threshold that restricts our un-\nweighted reconstruction error with the same order\nof magnitude as that of SVD. Experiments in Sec-\ntion 4.5 show that our switching point can well\nbalance the speed and convergence of the optimiza-\ntion process.\n3.3 Metric measuring when SVD may fail\nBesides an accurate solution to the J(A,B),\nwhether TFWSVD can obtain a performance gain\nis also decided by the properties of the target ma-\ntrix W itself. TFWSVD is to capture the different\nimportance of parameters. However, if the param-\neters in W equally contributed to the model per-\nformance, then the standard SVD should be good\nenough. Driven by these factors, we are interested\nin this question: Is there a method that can “foresee”\nwhen SVD will fail, and TFWSVD can help retain\nperformance?\nGiven target matrix W, here we propose a sim-\nple but effective metric called Fisher information\nvariance φ(W), which is calculated as the variance\nof the Lp normalization of its corresponding Fisher\ninformation ˆIW:\nφ(W) =Var(\nˆIW\nmax(||ˆIW||p,ε)\n). (13)\nAs shown in Section 4.6, this metric can qualita-\ntively measure whether the targeted matrix is too\nchallenging to SVD and therefore needs help from\nTFWSVD.\n4 Experiment\n4.1 Language tasks and datasets\nWe evaluate our proposed methods and baselines on\nthe General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2019) and a token\nclassification task. More details about datasets and\ntasks can be found in Appendix A.\n4.2 Implementation details and baselines\nFor generic compact methods (MiniLM, Distil-\nBERT, and TinyBERT), we use the models pro-\nvided by the original authors as the initialization,\nthen directly fine-tune them on the training data\nof the target task. The fine-tuning is optimized by\n1407\nTable 1: Results of CoNLL and GLUE benchmark. G-Avg means the average of GLUE tasks, A-Avg denotes the\naverage of all tasks, including CoNLL. Our method is the best performer in terms of both average scores.\nTask #Param CoNLL CoLA MNLI MRPC QNLI QQP SST-2 STSB G-Avg A-Avg\nBert_base 109.5M 94.1 56.2 84.7 87.4 91.3 87.8 93 88.5 84.1 85.4\ndistilBERT 67.0M 93.2 49.8 82.2 88.7 89.3 86.7 90.4 86.1 81.9 83.3\nMiniLMv2 67.0M 92.2 43.3 84.0 89.1 90.6 86.7 91.4 88.1 81.9 83.2\nTinyBERT6 67.0M 93.2 41.2 83.9 90.6 90.6 87.0 92.1 89.4 82.1 83.5\nSVD 66.5M 12.0 2.7 35.6 61.4 37.2 60.0 76.7 26.8 42.9 39.0\n+ fine-tuning 66.5M 92.4 40.5 82.8 84.1 89.6 87.3 90.9 85.7 80.1 81.6\nTVD 66.5M 51.6 2.1 58.8 81.6 66.9 74.1 83.1 75.3 63.1 59.7\n+ fine-tuning 66.5M 93.4 41.1 82.4 87.8 89.2 84.6 90.3 87.2 80.4 81.2\nFWSVD 66.5M 49.6 13.5 52.8 81.2 52.2 65.7 82.1 68.6 59.4 58.2\n+ fine-tuning 66.5M 93.2 49.4 83.0 88.0 89.5 87.6 91.2 87.0 82.2 83.6\nTFWSVD 66.5M 86.3 20.6 70.7 79.6 64.4 76.0 87.7 69.0 66.9 69.3\n+ fine-tuning 66.5M 94.2 52.2 83.4 89.0 90.3 86.9 91.1 88.5 83.1 84.4\nAdam with a learning rate of 2 ×10−5 and batch\nsize of 32 on one GPU.\nBesides FWSVD (Hsu et al., 2021) and our pro-\nposed TFWSVD, we also provide a baseline using\nfirst-order Taylor expansion for value decomposi-\ntion (TVD). The details of TVD can be found in\nAppendix B.\nFor low-rank factorization methods (TFWSVD,\nFWSVD, TVD, and SVD), we use the pre-trained\n12-layer BERT model (Devlin et al., 2018) as the\nstart. And then, the large BERT model is fine-\ntuned on the task-specific data. Next, we apply the\nlow-rank factorization, followed by another fine-\ntuning. We reported the results with and without\nfine-tuning to reveal the native results of low-rank\nfactorization.\nTo make a fair comparison, only the linear layers\nin the transformer blocks are compressed in this\nwork. The non-Transformer modules, such as the\ntoken embedding, are not compressed. Previous\nworks (Chen et al., 2018a) have shown significant\nsuccess in applying low-rank factorization to com-\npress the embedding layer, which occupies 23.4M\n(21.3%) parameters in the standard BERT model.\nThus, the results we reported in this paper can be\nfurther improved by applying our method to non-\ntransformer modules.\nAll of our implements are created on the base\nof HuggingFace Transformer library (Wolf et al.,\n2020). The settings not mentioned use the default\nconfiguration of the HuggingFace Transformer li-\nbrary. We directly reported the results on the dev\nset for all datasets, as hyper-parameter searching is\nnot involved in our experimental evaluations.\n4.3 Performance comparisons with SOTA\nTable 1 reports the results of GLUE tasks and one\nNER task CoNLL. Our TFWSVD with 66.5M pa-\nrameters obtains G-Avg score of 83.1 and A-Avg\nscore of 84.4, which are better than the scores\nof SOTA models (MiniLMv2, TinyBERT6, dis-\ntilBERT) requiring generic re-training. TFWSVD\nconsistently yields good results on all the tasks,\nwhile the other generic re-training methods dis-\nplay obvious performance variance among differ-\nent tasks. For example, TinyBERT6 is good at\nthe STSB task but poor at CoLA; oppositely, dis-\ntilBERT has strong performance on CoLA but is\nweak at STSB.\nIn the comparisons among low-rank factoriza-\ntion methods (TFWSVD, FWSVD, TVD, and\nSVD), our TFWSVD beats other methods with ap-\nparent better performance in both scenarios with or\nwithout fine-tuning. One interesting phenomenon\nis that TVD can yield better results than SVD with-\nout fine-tuning. However, after fine-tuning, its ad-\nvantages disappear, and SVD can achieve better\naverage scores (G-Avg and A-Avg). This is not sur-\nprising. Similar to our proposed TFWSVD, TVD is\nalso a loss-aware method that definitely will be bet-\nter than the loss-unaware SVD. But this gap can be\nnarrowed or even eliminated with fine-tuning since\nSVD can also “see” the loss in this case. Therefore,\nwithin loss-aware methods, the weighting metric\nitself plays an important role in keeping the perfor-\nmance advantage. Also, TFWSVD obtains better\nperformance than FWSVD, which indicates it is too\n“aggressive” for FWSVD to assume that parameters\nin the same row share the same importance.\n1408\nTable 2: Results of CoNLL and GLUE benchmark with high compression rates. Compared to Table 1, the advantages\nof TFWSVD over other two low-rank estimation methods are enlarged in the high compression rate settings.\nTask #Param CoNLL CoLA MNLI MRPC QNLI QQP SST-2 STSB G-Avg A-Avg\nBert_base 109.5M 94.1 56.2 84.7 87.4 91.3 87.8 93 88.5 84.1 85.4\nSVD 49.9M 2.7 0.2 37.0 0.0 49.5 36.9 58.3 16.5 28.3 25.1\n+ fine-tuning 49.9M 92.8 19.3 81.0 82.0 86.6 86.9 89.2 80.6 75.1 77.3\nFWSVD 49.9M 6.0 2.4 38.1 0.0 49.5 37.7 58.4 27.1 30.4 27.4\n+ fine-tuning 49.9M 92.9 38.7 81.4 80.3 88.0 87.2 88.4 82.9 78.1 80.0\nTFWSVD 49.9M 6.0 57.0 55.3 30.3 49.6 60.8 79.1 53.7 55.1 49.0\n+ fine-tuning 49.9M 93.5 39.3 82.2 88.3 88.8 87.0 89.9 87.0 80.4 82.0\nSVD 37.2M 2.2 0.0 32.5 0.0 49.5 3.4 51.4 5.5 20.3 18.1\n+ fine-tuning 37.2M 90.4 13.8 78.0 82.0 79.6 84.1 87.5 58.7 69.1 71.7\nFWSVD 37.2M 0.0 0.0 35.4 0.0 49.5 0.0 51.0 7.9 20.5 18.0\n+ fine-tuning 37.2M 3.5 18.7 78.2 78.6 82.3 84.5 88.9 67.9 71.3 62.8\nTFWSVD 37.2M 11.6 4.5 35.8 0.0 49.5 55.1 72.7 32.8 35.8 32.7\nw fine-tuning 37.2M 91.9 21.4 79.1 85.0 84.3 85.9 89.0 86.0 75.8 77.8\nTable 3: Weighted error and standard error of different methods at their final stages. The weighted error is J(A,B)\nin (4), and the standard error is ||W −AB||2. Adam and Adam_SGD are trained 50,000 steps, while ALS is\ntrained for 2.5 million steps and SGD is trained for 3 million steps.\nSVD Closed-Form SGD Adam ALS Adam_SGD\nWeighted error 1.34E-05 9.87E-06 6.71E-06 1.06E-05 9.30E-06 1.28E-06\nStandard error 6.57E-11 6.80E-11 2.38E-10 4.43E-11 2.31E-07 6.14E-11\n4.4 Under high compression rates\nIn this part, we compared low-rank methods\nunder high compression rates. Because TVD\ndidn’t show an apparent advantage over SVD,\nhere we mainly focus on comparing our proposed\nTFWSVD, FWSVD, and standard SVD.\nAs can be seen from Table 2, TFWSVD always\nenjoys obvious advantages over the other two meth-\nods. Also, the performance gap between TFWSVD\nand FWSVD is enlarged as the compression rate\ngoes higher. In fact, under the extremely compact\nsetting of 37.2M, FWSVD shows worse perfor-\nmance compared to SVD. This phenomenon further\nproves that the row-based importance assumption\nheld by FWSVD may hurt the performance. While\nthe privilege of TFWSVD always exists and be-\ncomes more prominent in the high compression\nrate of 49.9M and 37.2M. Especially in the sce-\nnario without fine-tuning, which can best reveal\nthe pure performance of low-rank factorization,\nTFWSVD has performance scores almost double\nthat of FWSVD and SVD.\n4.5 Optimization methods\nIn this part, we compare optimization procedures\nmentioned in Section 3.2 to identify the best opti-\nmizer for our approximation problem.\n4.5.1 ALS and SGD\nIn order to update the latent vectors, ALS needs\nO(r2) time to form the r×rmatrix, with an addi-\ntional O(r3) time to solve the least-squares prob-\nlem. Therefore, to reconstruct the target matrix\nW ∈RN×M with rank r, the time complexity of\none ALS iteration is O((M + N)r3 + MNr2). It\nhas been pointed out that ALS can be speeded up\nby parallelly updating each row ofA or B indepen-\ndently (Zhou et al., 2008). While for SGD, the time\ncomplexity per iteration is only O(MNK). Com-\npared to ALS, SGD seems to be faster in terms\nof the time complexity for one iteration. How-\never, typically it requires more iterations than ALS\nto achieve relatively good performance (Yu et al.,\n2014). As shown in Table 3, in order to obtain the\nperformance close to that of Adam/Adam_SGD,\nALS and SGD need 50 ∼60 times more steps,\nwhich makes them impractical to be used in the\nreal-world Transformer compression. Therefore, in\nthe rest of this part, we will focus on comparing\nthe performance of Adam and Adam_SGD.\n4.5.2 Adam and Adam_SGD\nThe goal of hybrid optimizer Adam_SGD is to\ncombine the benefits of Adam (fast initial progress\nand minimal efforts in hyperparameter tuning) and\n1409\nRank ratio\nPerformance\n0.60\n0.70\n0.80\n0.90\n0.10 0.20 0.30 0.40 0.50\nOriginal Query Key Value Attention_Out\n(a) SVD performance on 768 × 768 dimension matrix\nRank ratio\nPerformance\n0.60\n0.70\n0.80\n0.90\n0.10 0.20 0.30 0.40 0.50\nQuery Key Value Attention_Out Original (b) TFWSVD performance on 768 × 768 dimension matrix\nFigure 1: The performance of SVD and TFWSVD on the STSB task, when only factorizing a particular type\nof sub-structures (Key, Query, Value, Attention) in Transformer blocks. The red dash line denotes the original\nperformance. The numbers marked besides the lines are the metric φ(W) calculated by Equation (13). The values\nof φ(W) can well predict the performance of SVD, that matrix with a larger φ(W) will always end up with a larger\nperformance drop after applying SVD.\nSteps\nWeighted Reconstruction Error0.0E+0\n1.0E-5\n2.0E-5\n3.0E-5\n4.0E-5\n5.0E-5\n22000 22500 23000 23500\nAdam Adam_SGD\nFigure 2: Numerical experiments comparing Adam and\nAdam_SGD on STSB. For Adam_SGD, the switching\npoint from Adam to SGD is around step 22400.\nSGD (good convergence and generalization).\nAs seen from Figure 2, Adam and Adam_SGD\nshare the same trajectory in the initial steps. Af-\nter the switching point (around 22400 in Figure\n2), Adam_SGD converges to a low error solu-\ntion (1.28E-06 as shown in Table 3), which is\nmuch smaller than the row-based analytic solution\n(9.87E-06). In contrast, Adam fluctuates in perfor-\nmance and ends with a much larger error 1.06E-\n05. These phenomena prove the effectiveness of\nAdam_SGD in solving the weighted Frobenius dis-\ntance optimization problem in (4) and (5). And\nthe reconstruction errors of final solutions obtained\nby Adam_SGD are 5 ∼10 times smaller than the\nrow-wise approximations.\n4.6 Fisher information variance\nWhat is the secret behind TFWSVD’s good perfor-\nmance on Transformer-based model compression?\nIn this part, we utilize the metric Fisher informa-\ntion variance φ(W) introduced in Section 3.3 to\nreveal the secret by analyzing the sub-structures\ninside the Transformer blocks.\nAccording to the implementation of Hugging-\nFace Transformer library (Wolf et al., 2020), there\nare five kinds of linear layers within the Trans-\nformer block, which can be set into two groups by\ntheir dimensions: Query, Key, Value, and Multi-\nhead Attention layers are matrices with the dimen-\nsion of 768 ×768; and two feed-forward layers\ncalled Intermediate and Output, are 768 ×3072 in\ndimension. Figure 1 plot the performance changes\nalong with varying the rank ratio for matrices with\nthe dimension of 768 ×768, when only decom-\nposing one type of sub-structure. More results are\nplotted in Figure 3 in Appendix.\nCompared to the overall performance compari-\nson in Section 4.3, the purpose of this experiment is\nto evaluate the performance of SVD and TFWSVD\non the finer-level sub-structures within Transformer\nblocks. Taking Figure 1a for example, the yellow\nline denoting “Value” means: only the “Value” sub-\nstructures are decomposed by SVD, while other\ntypes of sub-structures are kept the same as the\noriginal model. We calculate the Fisher informa-\ntion variance φ(W) via Equation (13), and mark\nthe values besides the corresponding sub-structures.\nSeveral observations can be made from Figure 1.\nDifferent matrix has different sensitiveness to\nSVD. As shown in Figure 1a, Attention_out layer\nis relatively easy to compress. Even with standard\nSVD, it can still achieve good performance as low\n1410\nTable 4: Results of further compressing the compact\nmodels. TFWSVD successfully reduces the size of\nthe light-weight models, and achieves slightly better\nperformances than the original compact models.\nParam(M) CoNLL MRPC STSB Avg\nDistillBERT 67.0 94.0 88.7 86.1 89.6\nw SVD 43.6 93.2 82.9 83.0 86.4\nw FWSVD 43.6 93.4 87.9 84.1 88.5\nw TFWSVD 43.6 93.6 89.0 87.3 90.0\nMiniLMv2 67.0 93.2 89.0 88.1 90.1\nw SVD 43.6 93.2 89.0 86.4 89.5\nw FWSVD 43.6 93.3 88.8 87.9 90.0\nw TFWSVD 43.6 93.6 90.0 88.7 90.8\nTinyBERT6 67.0 93.2 90.5 89.4 91.0\nw SVD 43.6 93.0 88.5 88.3 89.9\nw FWSVD 43.6 93.1 89.0 88.7 90.3\nw TFWSVD 43.6 93.2 90.7 89.5 91.1\nTinyBERT4 14.3 85.6 89.0 85.9 86.8\nw SVD 11.9 88.9 87.1 84.7 86.9\nw FWSVD 11.9 88.5 87.6 86.1 87.4\nw TFWSVD 11.9 88.6 88.8 87.1 88.2\nas a rank ratio 0.1. While compressing matrix In-\ntermediate is rather difficult, its performance will\ndrop down to 17% with a rank ratio 0.1.\nMetric Fisher information variance φ(W) can\n‘foresee” the performance of SVD. In Figure 1a,\ndecomposing sub-structures with larger φ(W) via\nSVD will always cause the more serious perfor-\nmance drop. Especially, the performance changes\nof sub-structure Query and Key are almost identi-\ncal, and their φ(W) are extremely close (1.16E-\n03 for Query and 1.17E-03 for Key). This phe-\nnomenon implies the metric φ(W) can well reflect\nthe variance of parameter importance within the\nmatrix, and therefore can be a good performance\nindicator for SVD.\nTFWSVD can always help improve the perfor-\nmance. Figure 1b shows that applying TFWSVD\nwill bring significant performance gain to all\nthe sub-structures. Especially for the challeng-\ning matrix Intermediate (Figure 3b in Appendix),\nTFWSVD achieves an excellent performance of\n60% at a low-rank ratio 0.1, which is a 200% im-\nprovement compared to the corresponding SVD\nperformance 17%.\n4.7 Compress the already compact models\nThe matrix factorization direction is thought to\nbe orthogonal to other compression methods such\nas knowledge distillation. But in practice, perfor-\nmance drops are often observed when combining\nthe different lines of compression technologies. Ta-\nble 4 reports the results of applying TFWSVD,\nFWSVD, and SVD to compress the lightweight\nmodels further. In general, TFWSVD can reduce\n30% more parameters for the compact models, with\neven improved performance. In fact, the perfor-\nmance gains by applying TFWSVD are observed\non all compact models in Table 4, while both SVD\nand FWSVD will cause performance drops more\nor less when combined with those compact mod-\nels. These results indicate that SVD and FWSVD\nmay not be fully integrated with other compres-\nsion technologies due to the the strong assumptions\nthey held. And our TFWSVD can best explore the\npotential of combining other lines of compression\nmethods with matrix factorization.\n4.8 Discussion\nThe incorrect predictions from the trained model\nwill bring larger gradients than the correctly labeled\nexamples, which means these incorrect predictions\nmay be the better choices to compute Fisher in-\nformation. It is different from our intuitions, but\nnot surprising, since all these examples can reflect\nthe features of trained parameters. In fact, the mis-\nlabeled examples may better “describe” the fea-\ntures of the trained model (for example, these ex-\namples are around the boundary). Also, we can use\nincorrect-only labels to estimate the Fisher infor-\nmation to further reduce computation time. More\ndetails can be found in Appendix C.\n5 Conclusion\nUnlike SVD, there is no closed-form solution\nfor the weighted low-rank estimation problem,\nwhich therefore has to be approximated via nu-\nmerical optimization methods. We managed to\nobtain the practical solutions through our hy-\nbrid Adam_SGD optimizer with the specially de-\nsigned switching point. Our TFWSVD consis-\ntently works better than other low-rank factoriza-\ntion methods (FWSVD, TVD, and SVD). Com-\npared to SOTA methods that requiring expensive\ngeneric re-training, our TFWSVD shows more sta-\nble performance on various tasks. Also, TFWSVD\ncan efficiently further compress and optimize the\nalready compact models. We also investigate the\nproperties of the targeted matrix, where that SVD\nmay fail, and TFWSVD can be the rescuer. We\nbelieve our TFWSVD could be the best alternative\nto SVD for language model compression.\n1411\n6 Limitations\nThe most significant limitation of TFWSVD is that\nit may cost more time than SVD and FWSVD.\nCompared to FWSVD, TFWSVD will need more\ntime in numerical optimization, which is decided\nby the number of parameters in a model and is\nfixed for all downstream tasks. For GLUE tasks\ntrained with the BERT model, the extra time cost\nof TFWSVD is around 1.5 V100 GPU hours. Also,\nboth TFWSVD and FWSVD need time for Fisher\ninformation calculation. For example, this calcu-\nlation takes about 8 minutes on SST-2 task. And\nit can be further reduced to around 5 seconds if\nwe only use incorrect predictions (see details in\nAppendix C).\nIn summary, compared to SVD and FWSVD,\nTFWSVD will cost at least 1.5 more V100 GPU\nhours when compressing the BERT model for a\nGLUE task. This cost is worthy, considering the\nstable performance gain that TFWSVD can bring.\nOn the other hand, TFWSVD is still a much faster\nchoice than the generic re-trained compact models\nsuch as distilBERT and MiniLM. For example, dis-\ntilBERT needs 720 V100 GPU hours for re-training\na BERT model. Similar to SVD and FWSVD, our\nTFWSVD can avoid such expensive re-training and\ncan be applied to the directly downloaded BERT\nmodel.\n7 Ethical Considerations\nOur work is to better compress the language model\nwith an improved low-rank estimation method. For\nour experiments, we used open datasets without\nsensitive information, which have been widely men-\ntioned in previous work. No license is required for\nthe GLUE dataset, and we have purchased the li-\ncense for the CoNLL dataset. In the application\nof our model, we do not think there is an obvious\nissue that may lead to a risk to ethics.\nReferences\nAnish Acharya, Rahul Goel, Angeliki Metallinou, and\nInderjit Dhillon. 2019. Online embedding compres-\nsion for text classification using low rank matrix fac-\ntorization. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 33, pages 6196–6203.\nTakuya Akiba, Shuji Suzuki, and Keisuke Fukuda. 2017.\nExtremely large minibatch sgd: Training resnet-50\non imagenet in 15 minutes.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nPatrick H Chen, Si Si, Yang Li, Ciprian Chelba, and\nCho-Jui Hsieh. 2018a. Groupreduce: block-wise\nlow-rank approximation for neural language model\nshrinking. In Proceedings of the 32nd International\nConference on Neural Information Processing Sys-\ntems, pages 11011–11021.\nZihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi\nZhao. 2018b. Quora question pairs.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBill Dolan, Chris Brockett, and Chris Quirk. 2005. Mi-\ncrosoft research paraphrase corpus. Retrieved March,\n29(2008):63.\nGene H Golub and Christian Reinsch. 1971. Singular\nvalue decomposition and least squares solutions. In\nLinear algebra, pages 134–151. Springer.\nTrevor Hastie, Rahul Mazumder, Jason D Lee, and Reza\nZadeh. 2015. Matrix completion and low-rank svd\nvia fast alternating least squares. volume 16, pages\n3367–3402. JMLR. org.\nXiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-\nSeng Chua. 2016. Fast matrix factorization for online\nrecommendation with implicit feedback. In Proceed-\nings of the 39th International ACM SIGIR conference\non Research and Development in Information Re-\ntrieval, pages 549–558.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. volume 33, pages\n9782–9793.\nYen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou,\nYilin Shen, and Hongxia Jin. 2021. Language model\ncompression with weighted low-rank factorization.\nIn International Conference on Learning Representa-\ntions.\nTing Hua, Yilin Shen, Changsheng Zhao, Yen-Chang\nHsu, and Hongxia Jin. 2021. Hyperparameter-free\ncontinuous learning for domain classification in nat-\nural language understanding. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 2669–2678.\nYasutoshi Ida and Yasuhiro Fujiwara. 2020. Improving\ngeneralization performance of adaptive learning rate\nby switching from block diagonal matrix precondi-\ntioning to sgd. In 2020 International Joint Confer-\nence on Neural Networks (IJCNN), pages 1–8. IEEE.\n1412\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2019.\nTinybert: Distilling bert for natural language under-\nstanding. arXiv preprint arXiv:1909.10351.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR (Poster).\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Overcom-\ning catastrophic forgetting in neural networks. vol-\nume 114, pages 3521–3526. National Acad Sciences.\nYehuda Koren, Robert Bell, and Chris V olinsky. 2009.\nMatrix factorization techniques for recommender sys-\ntems. volume 42, pages 30–37. IEEE.\nLiyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun\nZhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen,\nWenming Yang, Qingmin Liao, and Wayne Zhang.\n2021. Group fisher pruning for practical network\ncompression. In International Conference on Ma-\nchine Learning, pages 7021–7032. PMLR.\nYang Liu. 2019. Fine-tune bert for extractive summa-\nrization. arXiv preprint arXiv:1903.10318.\nP Molchanov, S Tyree, T Karras, T Aila, and J Kautz.\n2019a. Pruning convolutional neural networks for re-\nsource efficient inference. In 5th International Con-\nference on Learning Representations, ICLR 2017-\nConference Track Proceedings.\nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri\nFrosio, and Jan Kautz. 2019b. Importance estima-\ntion for neural network pruning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11264–11272.\nMatan Ben Noach and Yoav Goldberg. 2020. Compress-\ning pre-trained language models by matrix decompo-\nsition. In Proceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pages\n884–889.\nRazvan Pascanu and Yoshua Bengio. 2014. Revisit-\ning natural gradient for deep networks. In In Inter-\nnational Conference on Learning Representations\n(ICLR).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nErik Tjong Kim Sang and Fien De Meulder. 2003. In-\ntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003, pages 142–147.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8815–8821.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nNathan Srebro and Tommi Jaakkola. 2003. Weighted\nlow-rank approximations. In Proceedings of the\n20th International Conference on Machine Learn-\ning (ICML-03), pages 720–727.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting,\nthe rest can be pruned. In 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n5797–5808. ACL Anthology.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL HLT 2018, pages\n1112–1122. Association for Computational Linguis-\ntics (ACL).\n1413\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nHsiang-Fu Yu, Cho-Jui Hsieh, Si Si, and Inderjit S\nDhillon. 2014. Parallel matrix factorization for rec-\nommender systems. volume 41, pages 793–819.\nSpringer.\nChangsheng Zhao, Ting Hua, Yilin Shen, Qian Lou, and\nHongxia Jin. 2021. Automatic mixed-precision quan-\ntization search of bert. In the Thirtieth International\nJoint Conference on Artificial Intelligence (IJCAI) ,\npages 3427–3433.\nYunhong Zhou, Dennis Wilkinson, Robert Schreiber,\nand Rong Pan. 2008. Large-scale parallel collabo-\nrative filtering for the netflix prize. In International\nconference on algorithmic applications in manage-\nment, pages 337–348. Springer.\n1414\nA Details of tasks and datasets\nWe include two single sentence tasks: CoLA\n(Warstadt et al., 2018) measured in Matthew’s cor-\nrelation, SST2 (Socher et al., 2013) measured in\nclassification accuracy; three sentence similarity\ntasks: MRPC (Dolan et al., 2005) measured in\nF-1 score, STS-B (Cer et al., 2017) measured in\nPearson-Spearman correlation, QQP (Chen et al.,\n2018b) measured in F-1 score; and three natural\nlanguage inference tasks: MNLI (Williams et al.,\n2018) measured in classification accuracy with the\naverage of the matched and mismatched subsets,\nQNLI (Rajpurkar et al., 2016) measured in accu-\nracy. The token classification task we used is the\nnamed entity recognition (NER) on the CoNLL-\n2003 dataset (Sang and De Meulder, 2003). In\nsummary, our evaluation includes eight different\nnatural language tasks.\nB TVD\nIn this section, we provide the details about the\nbaseline using first-order Taylor expansion for\nvalue decomposition (TVD). Following (Hou et al.,\n2020; V oita et al., 2019; Molchanov et al., 2019a),\nwe utilize the first-order Taylor expansion as the\nalternative importance score for matrices:\nTw = |L−L¬w| (14a)\n= |L−(L− ∂L\n∂w(w−0) +Rw=0)| (14b)\n≈|∂L\n∂ww|. (14c)\nAs shown in equation 14a, the intuition behind\nTVD is that the importance of a parameter wcan\nbe calculated by the variation in the training loss\nwhen removing this parameter. If we ignore the\nremainder Rw=0, then we can simply calculate the\nimportance via equation 14c, which is the product\nof the parameter value and its 1st-order gradient.\nC Effect of incorrect predictions\nIn this section, we evaluate whether the incor-\nrect predictions will have negative impacts on\nthe estimation of Fisher information. In order to\nachieve this goal, we report the performance of\nclassification tasks in Table 5, when we use in-\ncorrectly/correctly predicted examples to estimate\nFisher information.\nSeveral observations can be made as follows.\nFirst, the final performances are close, no mat-\nter using correct-only examples, incorrect-only\nexamples, or all examples . It demonstrates all\nkinds of examples can somehow reflect the impor-\ntance of parameters. Meanwhile, the performances\nusing all examples are always the best, confirming\nthe better estimation of empirical Fisher informa-\ntion with more data. Second, using the incorrect-\nonly examples will generate bigger values and\nbetter performance than using correct-only pre-\ndictions. Although only 1-2% examples are incor-\nrectly predicted, choosing these examples to esti-\nmate Fisher information will produce close num-\nbers to those generated using all examples. This is\nbecause Fisher information is calculated via loss,\nand incorrect predictions will produce larger losses\nthan the correct predictions. And compared to us-\ning correct-only examples, computations through\nincorrect-only examples may even bring better re-\nsults for most tasks.\nIn summary, the wrong labeled examples will\ngenerate larger Fisher information, but it doesn’t\nmean that the Fisher information learned from the\nincorrectly labeled data is “wrong”. Instead, the\nmislabeled examples are better choices than correct\npredictions, which will produce better results with\nfewer computations.\nD Training Time\nThis part discusses the training time of different\napproaches mentioned in this paper.\nTFWSVD versus FWSVD, TVD, and SVD. First,\nwe compare the time costs of low-rank estimation\nmethods SVD, TVD, FWSVD, and our proposed\nTFWSVD. In general, SVD is the fastest method\nthat can be done immediately as it has a close-\nform solution. FWSVD is the second fast method,\nwhich needs time for Fisher information calcula-\ntion. TFWSVD and TVD will cost more time in\nthe numerical optimization process.\n1. FWSVD versus SVD: Compared to SVD,\nFWSVD needs extra time for Fisher informa-\ntion calculation. The time of this process is\nsimilar to one epoch of regular training. For\nexample, SST-2 task in this paper takes about\n8 minutes to calculate the Fisher information.\nThis process is generally fast, and it can be\nfurther reduced to around 5 seconds if we only\nuse incorrect predictions ( e.g., 1% of all ex-\namples, mentioned in Appendix C).\n2. TFWSVD versus FWSVD: Compared to\nFWSVD, TFWSVD needs extra time for fac-\n1415\nTable 5: Performance comparison of using correct/incorrect labeled examples in the estimation of Fisher information.\nAll results here are without fine-tuning. #Examples denotes the number of corresponding examples, I-A VG means\nthe average importance score, F1 and ACC are task specific performance metrics.\nMNLI QNLI QQP SST2\n#ExamplesI-A VGACC #ExamplesI-A VGACC #ExamplesI-A VGF1 #ExamplesI-A VGACC\nCorrect-only 374345 1.21 69.68 103114 0.27 60.79 353888 0.69 75.49 66567 0.05 87.50\nIncorect-only 18357 3.55 70.63 1629 1.01 63.48 9958 2.27 75.95 782 0.29 87.39\nAll 392702 4.76 70.65 104743 1.28 64.42 363846 2.96 76.00 67349 0.34 87.73\nRank ratio\nPerformance\n0.00\n0.25\n0.50\n0.75\n1.00\n0.10 0.20 0.30 0.40 0.50\nOutput Intermediate Original\n(a) SVD performance on 3072 × 768 dimension matrix\nRank ratio\nPerformance\n0.00\n0.25\n0.50\n0.75\n1.00\n0.10 0.20 0.30 0.40 0.50\nOriginal Intermediate Output (b) TFWSVD performance on 3072 × 768 dimension matrix\nFigure 3: The performance of SVD and TFWSVD on the STSB task, when only factorizing a particular type of\nsub-structures (Intermediate, or Output) in Transformer blocks.\ntorizing the weighted matrices through opti-\nmizations. The time cost of factorization is de-\ncided by the number of parameters in a model,\nand is fixed for all its downstream tasks. For\nGLUE tasks trained with the BERT model,\nTFWSVD will cost 1.5 more V100 GPU hours\nthan FWSVD.\n3. TFWSVD versus TVD: TFWSVD and TVD\nwill cost the same time as these approaches\nare almost the same except for the weighting\nscheme.\nTFWSVD versus generic re-trained models. The\ngeneric re-trained compact models such as distil-\nBERT and MiniLMv2 require a large amount of\nre-training time. For example, distilBERT needs\n720 V100 GPU hours for retraining a pre-trained\nBERT model. Compared to these methods, our\nTFWSVD is much faster, since TFWSVD can be\napplied to the directly downloaded BERT model\nwithout expensive re-training.\n1416",
  "topic": "Singular value decomposition",
  "concepts": [
    {
      "name": "Singular value decomposition",
      "score": 0.8869216442108154
    },
    {
      "name": "Computer science",
      "score": 0.6014835834503174
    },
    {
      "name": "Transformer",
      "score": 0.5362311005592346
    },
    {
      "name": "Convex optimization",
      "score": 0.4913136065006256
    },
    {
      "name": "Algorithm",
      "score": 0.48083385825157166
    },
    {
      "name": "Matrix decomposition",
      "score": 0.47964292764663696
    },
    {
      "name": "Metric (unit)",
      "score": 0.4531644880771637
    },
    {
      "name": "Mathematical optimization",
      "score": 0.43205592036247253
    },
    {
      "name": "Regular polygon",
      "score": 0.39794594049453735
    },
    {
      "name": "Mathematics",
      "score": 0.26244738698005676
    },
    {
      "name": "Voltage",
      "score": 0.13325539231300354
    },
    {
      "name": "Eigenvalues and eigenvectors",
      "score": 0.08282047510147095
    },
    {
      "name": "Engineering",
      "score": 0.07325360178947449
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210133173",
      "name": "Research!America (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210101778",
      "name": "Samsung (United States)",
      "country": "US"
    }
  ]
}