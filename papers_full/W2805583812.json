{
    "title": "Language Model Based Grammatical Error Correction without Annotated Training Data",
    "url": "https://openalex.org/W2805583812",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A1992091276",
            "name": "CHRISTOPHER BRYANT",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A1989705602",
            "name": "Ted Briscoe",
            "affiliations": [
                "University of Cambridge"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2321916036",
        "https://openalex.org/W2153013403",
        "https://openalex.org/W2251219521",
        "https://openalex.org/W2124725212",
        "https://openalex.org/W4299805751",
        "https://openalex.org/W2515384205",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2098297786",
        "https://openalex.org/W2175296493",
        "https://openalex.org/W2127672659",
        "https://openalex.org/W2251862950",
        "https://openalex.org/W2470324779",
        "https://openalex.org/W2963109131",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2759575900",
        "https://openalex.org/W2964187553",
        "https://openalex.org/W2478432301",
        "https://openalex.org/W2251613956",
        "https://openalex.org/W2589277916",
        "https://openalex.org/W2741494657",
        "https://openalex.org/W2564829011",
        "https://openalex.org/W2758774757",
        "https://openalex.org/W2509973494",
        "https://openalex.org/W2917881729",
        "https://openalex.org/W2481467102",
        "https://openalex.org/W99445809",
        "https://openalex.org/W2519314406"
    ],
    "abstract": "Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",
    "full_text": "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 247–253\nNew Orleans, Louisiana, June 5, 2018.c⃝2018 Association for Computational Linguistics\nLanguage Model Based Grammatical Error Correction\nwithout Annotated Training Data\nChristopher Bryant Ted Briscoe\nALTA Institute\nDepartment of Computer Science and Technology\nUniversity of Cambridge\nCambridge, UK\n{cjb255, ejb1}@cl.cam.ac.uk\nAbstract\nSince the end of the CoNLL-2014 shared task\non grammatical error correction (GEC), re-\nsearch into language model (LM) based ap-\nproaches to GEC has largely stagnated. In this\npaper, we re-examine LMs in GEC and show\nthat it is entirely possible to build a simple sys-\ntem that not only requires minimal annotated\ndata (∼1000 sentences), but is also fairly com-\npetitive with several state-of-the-art systems.\nThis approach should be of particular interest\nfor languages where very little annotated train-\ning data exists, although we also hope to use it\nas a baseline to motivate future research.\n1 Introduction\nIn the CoNLL-2014 shared task on Grammatical\nError Correction (GEC) (Ng et al., 2014), the top\nthree teams all employed a combination of sta-\ntistical machine translation (SMT) or classiﬁer-\nbased approaches (Junczys-Dowmunt and Grund-\nkiewicz, 2014; Felice et al., 2014; Rozovskaya\net al., 2014). These approaches have since come to\ndominate the ﬁeld, and a lot of recent research has\nfocused on ﬁne-tuning SMT systems (Junczys-\nDowmunt and Grundkiewicz, 2016), reranking\nSMT output (Hoang et al., 2016; Yuan et al.,\n2016), combining SMT and classiﬁer systems (Su-\nsanto et al., 2014; Rozovskaya and Roth, 2016),\nand developing various neural architectures (Chol-\nlampatt et al., 2016; Xie et al., 2016; Yuan and\nBriscoe, 2016; Chollampatt and Ng, 2017; Sak-\naguchi et al., 2017; Yannakoudakis et al., 2017).\nDespite coming a fairly competitive fourth in\nthe shared task however (Lee and Lee, 2014),\nresearch into language model (LM) based ap-\nproaches to GEC has largely stagnated. The main\naim of this paper is hence to re-examine language\nmodelling in the context of GEC and show that\nit is still possible to achieve competitive results\neven with very simple systems. In fact, a notable\nstrength of LM-based approaches is that they rely\non very little annotated data (purely for tuning pur-\nposes), and so it is entirely possible to build a rea-\nsonable correction system for any language given\nenough native text. In contrast, this is simply not\npossible with SMT and other popular approaches\nwhich always require (lots of) labelled data.\n2 Methodology\nThe core idea behind language modelling in GEC\nis that low probability sequences are more likely\nto contain grammatical errors than high probabil-\nity sequences. For example, * discuss about the\nproblem is expected to be a low probability se-\nquence because it contains an error while discuss\nthe problemor talk about the problemare expected\nto be higher probability sequences because they do\nnot contain errors. The goal of LM-based GEC is\nhence to determine how to transform the former\ninto the latter based on LM probabilities.1\nWith this in mind, our approach is fundamen-\ntally a simpliﬁcation of the algorithm proposed by\nDahlmeier and Ng (2012a). It consists of 5 steps\nand is illustrated in Table 1:\n1. Calculate the normalised log probability of\nan input sentence.\n2. Build a confusion set, if any, for each token\nin that sentence.\n3. Re-score the sentence substituting each can-\ndidate in each confusion set.\n4. Apply the single best correction that in-\ncreases the probability above a threshold.\n5. Iterate steps 1-4.\nOne of the main contributions of this paper is\nhence to re-evaluate the LM approach in relation\nto the latest state-of-the-art systems on several\nbenchmark datasets.\n1See Chelba et al. (2014) for more information about pop-\nular approaches to language modelling.\n247\nStep Sentence Probability\n1 I am looking forway to see you soon . -2.71\n2 and 3I\nwas -2.67 look -2.91 forward -1.80of -2.98 seeing -3.09\nyou\nsooner -3.05\n. -be -3.09 looks -2.93 Norway -2.36 in -2.99 saw -3.25 soonest -3.20\nare -3.10 looked -2.95 foray -2.70 ϵ -3.00 sees -3.39\n. . . . . . . . . . . . . . .\n4 I am looking forward to see you soon . -1.80\n5 I am looking forward to seeing you soon . -1.65\nTable 1: A step-by-step example of our approach as described in Section 2. All scores are log probabilities.\n2.1 Sequence Probabilities\nWe evaluate hypothesis corrections in terms of\nnormalised log probabilities at the sentence level.\nNormalisation by sentence length is necessary to\novercome the tendency for shorter sequences to\nhave higher probabilities than longer sequences.\nDahlmeier and Ng (2012a) similarly used nor-\nmalised log probabilities to evaluate hypotheses,\nbut did so as part of a more complex combination\nof other features. In contrast, Lee and Lee (2014)\nevaluated hypotheses in terms of sliding ﬁve word\nwindows (5-grams).\n2.2 Confusion Sets\nOne of the deﬁning characteristics of LM-based\nGEC is that the approach does not necessarily\nrequire annotated training data. For example,\nspellcheckers and rules both formed key parts of\nDahlmeier and Ng’s and Lee and Lee’s systems.\nWhile Lee and Lee ultimately did make use of an-\nnotated training data however, Dahlmeier and Ng\ninstead employed separate classiﬁers for articles,\nprepositions and noun number errors trained only\non native text.\nIn this work, we focus on correcting the follow-\ning error types in English: non-words, morphol-\nogy, and articles and prepositions.2\nNon-words: We use CyHunspell3 v1.2.1 with\nthe latest British English Hunspell dictionaries4 to\ngenerate correction candidates for non-word er-\nrors. Non-words include genuine misspellings,\nsuch as [freind →friend], and inﬂectional errors,\nsuch as [advices →advice]. Although CyHunspell\nis not a context sensitive spell checker, the pro-\nposed corrections are evaluated in a context sensi-\ntive manner by the language model.\n2Note that targeting other error types may be more appro-\npriate in other languages; e.g. Mandarin Chinese contains\nvery little morphology.\n3https://pypi.python.org/pypi/\nCyHunspell\n4https://sourceforge.net/projects/\nwordlist/files/speller/2017.08.24/\nMorphology: Examples of morphological er-\nrors include noun number [cat →cats], verb tense\n[eat →ate] and adjective form [ big →bigger],\namongst others. To generate correction candi-\ndates for morphological errors, we use an Auto-\nmatically Generated Inﬂection Database (AGID),5\nwhich contains all the morphological forms of\nmany English words. The confusion set for a word\nis hence derived from this database.\nArticles and Prepositions:Since articles and\nprepositions are closed class words, we deﬁned\nconfusion sets for these error types manually.\nSpeciﬁcally, the article confusion set consists of\n{ϵ, a, an, the}, while the preposition confusion set\nconsists of the top ten most frequent prepositions:\n{ϵ, about, at, by, for, from, in, of, on, to, with }.\nBoth sets also contain a null character which rep-\nresents a deletion.\nUnlike Dahlmeier and Ng and Lee and Lee, we\ndo not yet handle missing words (∼20% of all er-\nrors) because it is often difﬁcult to know where to\ninsert them.\n2.3 Iteration\nThe main reason to iteratively correct only one\nword at a time is because errors sometimes inter-\nact. For example, correcting [see →seeing] in Ta-\nble 1 initially reduces the log probability of the\ninput sentence from -2.71 to -3.09. After correct-\ning [ foray →forward] however, [ see →seeing]\nsubsequently increases the probability of the sen-\ntence from -1.80 to -1.65 in the second iteration.\nConsequently, correcting the most serious errors\nﬁrst, in terms of language model probability in-\ncrease, often helps facilitate the correction of less\nserious errors later. Dahlmeier and Ng and Lee\nand Lee both also used iterative correction strate-\ngies in their systems, but did so as part of a beam\nsearch or pipeline approach respectively.\n5http://wordlist.aspell.net/other/\n248\nDataset Tokenizer Sents Coders Edits\nCoNLL-2013 NLTK 1381 1 3404\nCoNLL-2014 NLTK 1312 2 6104\nFCE-dev spaCy 2371 1 4419\nFCE-test spaCy 2805 1 5556\nJFLEG-dev NLTK 754 4 10576\nJFLEG-test NLTK 747 4 10082\nTable 2: Various stats about the learner corpora we use.\n3 Data and Resources\nIn all our experiments, we used a 5-gram language\nmodel trained on the One Billion Word Bench-\nmark dataset (Chelba et al., 2014) with KenLM\n(Heaﬁeld, 2011). While a neural model would\nlikely result in better performance, efﬁcient train-\ning on such a large amount of data is still an active\narea of research (Grave et al., 2017).\nAlthough LM-based GEC does not require an-\nnotated training data, a small amount of annotated\ndata is still required for development and testing.\nWe hence make use of several popular GEC cor-\npora, including: CoNLL-2013 and CoNLL-2014\n(Ng et al., 2013, 2014), the public First Certiﬁcate\nin English (FCE) (Yannakoudakis et al., 2011),\nand JFLEG (Napoles et al., 2017).\nSince the FCE was not originally released with\nan ofﬁcial development set, we use the same split\nas Rei and Yannakoudakis (2016), 6 which we to-\nkenize with spaCy7 v1.9.0. We also reprocess all\nthe datasets with the ERRor ANnotation Toolkit\n(ERRANT) (Bryant et al., 2017) in an effort to\nstandardise them. This standardisation is espe-\ncially important for JFLEG which is not explicitly\nannotated and so otherwise cannot be evaluated in\nterms of F-score. Note that results on CoNLL-\n2014 and JFLEG are typically higher than on other\ndatasets because they contain more than one refer-\nence. See Table 2 for more information about each\nof the development and test sets.\n4 Tuning\nThe goal of tuning in our LM-based approach is\nto determine a probability threshold that optimises\nF0.5. For example, although the edit [ am →was]\nin Table 1 increases the normalised sentence log\nprobability from -2.71 to -2.67, this is such a small\nimprovement that it is likely to be a false positive.\nIn order to minimise false positives, we hence set\n6https://ilexir.co.uk/datasets/index.\nhtml\n7https://spacy.io/\n0 2 4 6 8 100\n10\n20\n30\n40\n50\n60\nImprovement Threshold (%)\nERRANT F 0.5\nCoNLL-2013 FCE-dev JFLEG-dev\nFigure 1: The effect of changing the sentence probabil-\nity improvement threshold (%) on ERRANT F 0.5 for\neach of the development sets.\na threshold such that a candidate correction must\nimprove the average token probability of the sen-\ntence by at least X% before it is applied. Although\nit may be unusual to use percentages in log space,\nthis is just one way to compare the difference be-\ntween two sentences which we found worked well\nin practice.\nThe results of this tuning are shown in Figure 1,\nwhere we tried thresholds in the range of 0-10%\non three different development sets. It is notable\nthat the optimum threshold for CoNLL-2013 (2%)\nis very different from that of FCE-dev (4%) and\nJFLEG-dev (5%), which we suspect is because\neach dataset has a different error type distribution.\nFor example, spelling errors make up just 0.3% of\nall errors in CoNLL-2013, but closer to 10% in\nFCE-dev and JFLEG-dev.\nFinally, it should be noted that this threshold is\nan approximation and it is certainly possible to op-\ntimise further. For example, in future, thresholds\ncould be set based on error types rather than glob-\nally.\n5 Results and Discussion\nBefore evaluating performance on the test sets, a\nﬁnal post-processing step changed the ﬁrst alpha-\nbetical character of every sentence to upper case\nif necessary. This improved the scores by about\n0.3 F0.5 in CoNLL-2014 and FCE-test, but by over\n5 F0.5 in JFLEG-test. This surprising result once\n249\nERRANT M2 Scorer\nTest Set System P R F 0.5 P R F 0.5 GLEU\nCoNLL-2014\nLee and Lee (2014) 30.60 20.95 28.02 34.51 21.73 30.88 59.50\nAMU16 SMT +LSTM - - - 58.79 30.63 49.66 68.26\nCAMB16 SMT +LSTM - - - 49.58 21.84 39.53 65.68\nOur work 36.62 19.93 31.37 40.56 20.81 34.09 59.35\nFCE-test\nAMU16 SMT + LSTM - - - 40.67 17.36 32.06 63.57\nCAMB16 SMT + LSTM - - - 65.03 32.45 54.15 70.72\nOur work 41.92 13.62 29.61 44.78 14.12 31.22 60.04\nJFLEG-test\nAMU16 SMT + LSTM - - - 60.68 22.65 45.43 42.65\nCAMB16 SMT + LSTM - - - 65.86 30.56 53.50 46.74\nSakaguchi et al. (2017) - - - 65.80 40.96 58.68 53.98\nOur work 73.76 27.61 55.28 76.23 28.48 57.08 48.75\nTable 3: Our LM-based approach is compared against several state-of-the-art results. AMU16 SMT +LSTM and\nCAMB16SMT +LSTM were both originally reported by Yannakoudakis et al. (2017), while Lee and Lee (2014) is\nthe system entered by POST in CoNLL-2014. Only our approach does not use annotated training data.\nagain shows that different test sets have very dif-\nferent error type distributions and that even the\nsimplest of correction strategies can signiﬁcantly\naffect results.\nOur ﬁnal scores are shown in Table 3 where\nthey are compared with several state-of-the-art\nsystems. Unfortunately, we cannot compare re-\nsults with Dahlmeier and Ng (2012a) because this\nsystem is neither publicly available nor has pre-\nviously been evaluated on these test sets. Results\nare reported in terms of M2 F 0.5 (Dahlmeier and\nNg, 2012b), the de facto standard of GEC evalu-\nation; ERRANT F0.5 (Bryant et al., 2017), an im-\nproved version of M2 which we used to develop\nour system; and GLEU (Napoles et al., 2015), an\nngram-based metric designed to correlate with hu-\nman judgements. Results for ERRANT are not\navailable in all cases because system output is not\navailable.\nAt this point, it is worth reiterating that our main\nintention was not to necessarily improve upon the\nstate-of-the-art, but rather quantify the extent to\nwhich a simple LM-based approach with mini-\nmal annotated data could compete against a much\nmore sophisticated model trained on millions of\nwords of annotated text. This is especially rele-\nvant for languages where annotated training data\nmay not be available.\nWith this in mind, we were ﬁrstly pleased to im-\nprove upon the previous best LM-based approach\nby Lee and Lee (2014) in the CoNLL-2014 shared\ntask. This is especially signiﬁcant given we also\ndid so without any annotated training data (un-\nlike them). Although our system would still have\nplaced fourth overall, the gap between third and\nfourth decreased from 3 F0.5 to less than 1 F0.5.\nWe were also surprised by the high perfor-\nmance on JFLEG-test, where we not only outper-\nformed two state-of-the-art systems, but also came\nto within 2 F 0.5 of the top system. This is espe-\ncially surprising given our system only corrects\na limited number of error types (roughly 14 out\nof the 55 in ERRANT 8), and so can maximally\ncorrect only 40-60% of all errors in each test set.\nOne possible explanation for this is that unlike\nCoNLL-2014 and FCE-test, which were only cor-\nrected with minimal edits, JFLEG was corrected\nfor ﬂuency (Sakaguchi et al., 2016), and so it in-\ntuitively makes sense that LM-based approaches\nperform better with ﬂuent references.\nAlthough we did not perform as well on\nCoNLL-2014 or FCE-test, most likely for the\nsame reason, we also note a large discrep-\nancy between state-of-the-art systems tuned\non different datasets. For example, while\nAMU16SMT +LSTM tuned for CoNLL achieves\nthe highest result on CoNLL-2014 (49.66 F 0.5),\nits equivalent performance on FCE-test (32.06\nF0.5) is only marginally better than our own\n(31.22 F 0.5). We observe a similar effect with\nCAMB16SMT +LSTM tuned for the FCE, and so\nare wary of approaches that might be overﬁtting to\ntheir training corpora.\nWe make all our code and system output avail-\nable online.9\n8R:ADJ:FORM, R:DET, R:MORPH, R:NOUN:INFL,\nR:NOUN:NUM, R:ORTH, R:PREP, R:SPELL,\nR:VERB:FORM, R:VERB:INFL, R:VERB:SV A,\nR:VERB:TENSE, U:DET, U:PREP\n9https://github.com/chrisjbryant/\nlmgec-lite\n250\n6 Conclusion\nIn this paper, we have shown that a simple lan-\nguage model approach to grammatical error cor-\nrection with minimal annotated data can still be\ncompetitive with the latest neural and machine\ntranslation approaches that rely on large quantities\nof annotated training data. This is especially sig-\nniﬁcant given that our system is also limited by the\nrange of error types it can correct. In the future,\nwe hope to improve our system by adding the ca-\npability to correct other error types, such as miss-\ning words, and also make use of neural language\nmodelling techniques.\nWe have demonstrated that LM-based GEC is\nnot only still a promising area of research, but one\nthat may be of particular interest to researchers\nworking on languages where annotated training\ncorpora are not yet available. We released all our\ncode and system output with this paper.\nReferences\nChristopher Bryant, Mariano Felice, and Ted Briscoe.\n2017. Automatic annotation and evaluation of er-\nror types for grammatical error correction. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics,\nVancouver, Canada, pages 793–805. http://\naclweb.org/anthology/P17-1074.\nCiprian Chelba, Tomas Mikolov, Mike Schuster,\nQi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. 2014. One billion word benchmark\nfor measuring progress in statistical language\nmodeling. In INTERSPEECH 2014, 15th An-\nnual Conference of the International Speech\nCommunication Association, Singapore, Septem-\nber 14-18, 2014 . pages 2635–2639. http:\n//www.isca-speech.org/archive/\ninterspeech_2014/i14_2635.html.\nShamil Chollampatt, Duc Tam Hoang, and Hwee Tou\nNg. 2016. Adapting grammatical error correction\nbased on the native language of writers with neu-\nral network joint models. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing . Association for Compu-\ntational Linguistics, Austin, Texas, pages 1901–\n1911. https://aclweb.org/anthology/\nD16-1195.\nShamil Chollampatt and Hwee Tou Ng. 2017. Con-\nnecting the dots: Towards human-level gram-\nmatical error correction. In Proceedings of the\n12th Workshop on Innovative Use of NLP for\nBuilding Educational Applications . Association for\nComputational Linguistics, Copenhagen, Denmark,\npages 327–333. http://www.aclweb.org/\nanthology/W17-5037.\nDaniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-\nsearch decoder for grammatical error correction. In\nProceedings of the 2012 Joint Conference on Empir-\nical Methods in Natural Language Processing and\nComputational Natural Language Learning . Asso-\nciation for Computational Linguistics, Jeju Island,\nKorea, pages 568–578. http://www.aclweb.\norg/anthology/D12-1052.\nDaniel Dahlmeier and Hwee Tou Ng. 2012b. Bet-\nter evaluation for grammatical error correction. In\nProceedings of the 2012 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies. Association for Computational Linguistics,\nMontr´eal, Canada, pages 568–572. http://www.\naclweb.org/anthology/N12-1067.\nMariano Felice, Zheng Yuan, Øistein E. Andersen, He-\nlen Yannakoudakis, and Ekaterina Kochmar. 2014.\nGrammatical error correction using hybrid systems\nand type ﬁltering. In Proceedings of the Eighteenth\nConference on Computational Natural Language\nLearning: Shared Task . Association for Computa-\ntional Linguistics, Baltimore, Maryland, pages 15–\n24. http://www.aclweb.org/anthology/\nW14-1702.\n´Edouard Grave, Armand Joulin, Moustapha Ciss ´e,\nDavid Grangier, and Herv ´e J ´egou. 2017. Efﬁ-\ncient softmax approximation for GPUs. In Doina\nPrecup and Yee Whye Teh, editors, Proceedings\nof the 34th International Conference on Machine\nLearning. PMLR, International Convention Cen-\ntre, Sydney, Australia, volume 70 of Proceed-\nings of Machine Learning Research , pages 1302–\n1310. http://proceedings.mlr.press/\nv70/grave17a.html.\nKenneth Heaﬁeld. 2011. KenLM: faster and\nsmaller language model queries. In Proceed-\nings of the EMNLP 2011 Sixth Workshop on\nStatistical Machine Translation . Edinburgh,\nScotland, United Kingdom, pages 187–197.\nhttps://kheafield.com/papers/\navenue/kenlm.pdf.\nDuc Tam Hoang, Shamil Chollampatt, and Hwee Tou\nNg. 2016. Exploiting n-best hypotheses to improve\nan smt approach to grammatical error correction. In\nProceedings of the Twenty-Fifth International Joint\nConference on Artiﬁcial Intelligence (IJCAI-16) .\nAAAI Press / International Joint Conferences on Ar-\ntiﬁcial Intelligence, New York, New York, USA,\npages 2803–2809. https://www.ijcai.org/\nProceedings/16/Papers/398.pdf.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2014. The amu system in the conll-2014 shared\ntask: Grammatical error correction by data-intensive\nand feature-rich statistical machine translation. In\n251\nProceedings of the Eighteenth Conference on Com-\nputational Natural Language Learning: Shared\nTask. Association for Computational Linguistics,\nBaltimore, Maryland, pages 25–33. http://\nwww.aclweb.org/anthology/W14-1703.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2016. Phrase-based machine translation is state-\nof-the-art for automatic grammatical error correc-\ntion. In Proceedings of the 2016 Conference on Em-\npirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Austin,\nTexas, pages 1546–1556. https://aclweb.\norg/anthology/D16-1161.\nKyusong Lee and Gary Geunbae Lee. 2014. Postech\ngrammatical error correction system in the conll-\n2014 shared task. In Proceedings of the Eighteenth\nConference on Computational Natural Language\nLearning: Shared Task . Association for Computa-\ntional Linguistics, Baltimore, Maryland, pages 65–\n73. http://www.aclweb.org/anthology/\nW14-1709.\nCourtney Napoles, Keisuke Sakaguchi, Matt Post,\nand Joel Tetreault. 2015. Ground truth for gram-\nmatical error correction metrics. In Proceedings\nof the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Inter-\nnational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers) . Associa-\ntion for Computational Linguistics, Beijing, China,\npages 588–593. http://www.aclweb.org/\nanthology/P15-2097.\nCourtney Napoles, Keisuke Sakaguchi, and Joel\nTetreault. 2017. Jﬂeg: A ﬂuency corpus and\nbenchmark for grammatical error correction. In\nProceedings of the 15th Conference of the Eu-\nropean Chapter of the Association for Computa-\ntional Linguistics: Volume 2, Short Papers. Associa-\ntion for Computational Linguistics, Valencia, Spain,\npages 229–234. http://www.aclweb.org/\nanthology/E17-2037.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The CoNLL-2014 shared task on\ngrammatical error correction. In Proceedings of the\nEighteenth Conference on Computational Natural\nLanguage Learning: Shared Task. ACL, Baltimore,\nMaryland, USA, pages 1–14. http://aclweb.\norg/anthology/W/W14/W14-1701.pdf.\nHwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Chris-\ntian Hadiwinoto, and Joel R. Tetreault. 2013. The\nCoNLL-2013 shared task on grammatical error cor-\nrection. In Proceedings of the Seventeenth Confer-\nence on Computational Natural Language Learn-\ning: Shared Task. ACL, Soﬁa, Bulgaria, pages 1–12.\nMarek Rei and Helen Yannakoudakis. 2016. Composi-\ntional sequence labeling models for error detection\nin learner writing. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) . Association\nfor Computational Linguistics, Berlin, Germany,\npages 1181–1191. http://www.aclweb.org/\nanthology/P16-1112.\nAlla Rozovskaya, Kai-Wei Chang, Mark Sammons,\nDan Roth, and Nizar Habash. 2014. The illinois-\ncolumbia system in the conll-2014 shared task.\nIn Proceedings of the Eighteenth Conference on\nComputational Natural Language Learning: Shared\nTask. Association for Computational Linguistics,\nBaltimore, Maryland, pages 34–42. http://\nwww.aclweb.org/anthology/W14-1704.\nAlla Rozovskaya and Dan Roth. 2016. Grammati-\ncal error correction: Machine translation and clas-\nsiﬁers. In Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Berlin,\nGermany, pages 2205–2215. http://aclweb.\norg/anthology/P16-1208.\nKeisuke Sakaguchi, Courtney Napoles, Matt Post,\nand Joel Tetreault. 2016. Reassessing the goals of\ngrammatical error correction: Fluency instead of\ngrammaticality. Transactions of the Association for\nComputational Linguistics 4:169–182. https:\n//tacl2013.cs.columbia.edu/ojs/\nindex.php/tacl/article/view/800.\nKeisuke Sakaguchi, Matt Post, and Benjamin\nVan Durme. 2017. Grammatical error correction\nwith neural reinforcement learning. In Proceedings\nof the Eighth International Joint Conference on\nNatural Language Processing (Volume 2: Short\nPapers). Asian Federation of Natural Language Pro-\ncessing, Taipei, Taiwan, pages 366–372. http://\nwww.aclweb.org/anthology/I17-2062.\nRaymond Hendy Susanto, Peter Phandi, and Hwee Tou\nNg. 2014. System combination for grammat-\nical error correction. In Proceedings of the\n2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) . Associa-\ntion for Computational Linguistics, Doha, Qatar,\npages 951–962. http://www.aclweb.org/\nanthology/D14-1102.\nZiang Xie, Anand Avati, Naveen Arivazhagan, Dan Ju-\nrafsky, and Andrew Y . Ng. 2016. Neural language\ncorrection with character-based attention. CoRR\nabs/1603.09727. http://arxiv.org/abs/\n1603.09727.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A new dataset and method for automatically\ngrading esol texts. In Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies . As-\nsociation for Computational Linguistics, Portland,\nOregon, USA, pages 180–189. http://www.\naclweb.org/anthology/P11-1019.\nHelen Yannakoudakis, Marek Rei, Øistein E. Ander-\nsen, and Zheng Yuan. 2017. Neural sequence-\nlabelling models for grammatical error correction.\n252\nIn Proceedings of the 2017 Conference on Em-\npirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Copen-\nhagen, Denmark, pages 2795–2806. https://\nwww.aclweb.org/anthology/D17-1297.\nZheng Yuan and Ted Briscoe. 2016. Grammatical er-\nror correction using neural machine translation. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies. Association for Computational Linguistics,\nSan Diego, California, pages 380–386. http://\nwww.aclweb.org/anthology/N16-1042.\nZheng Yuan, Ted Briscoe, and Mariano Felice. 2016.\nCandidate re-ranking for smt-based grammatical er-\nror correction. In Proceedings of the 11th Workshop\non Innovative Use of NLP for Building Educational\nApplications. Association for Computational Lin-\nguistics, San Diego, CA, pages 256–266. http://\nwww.aclweb.org/anthology/W16-0530.\n253"
}