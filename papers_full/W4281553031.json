{
  "title": "Towards robust diagnosis of COVID-19 using vision self-attention transformer",
  "url": "https://openalex.org/W4281553031",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2543805253",
      "name": "Fozia Mehboob",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966972960",
      "name": "Abdul Rauf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2308542509",
      "name": "Richard Jiang",
      "affiliations": [
        "Lancaster University"
      ]
    },
    {
      "id": "https://openalex.org/A1972429958",
      "name": "Abdul Khader Jilani Saudagar",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University"
      ]
    },
    {
      "id": "https://openalex.org/A2166583460",
      "name": "Khalid Mahmood Malik",
      "affiliations": [
        "Oakland University"
      ]
    },
    {
      "id": "https://openalex.org/A2607735316",
      "name": "Muhammad Badruddin Khan",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University"
      ]
    },
    {
      "id": "https://openalex.org/A4281602543",
      "name": "Mozaherul Hoque Abdul Hasnat",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University"
      ]
    },
    {
      "id": "https://openalex.org/A2628757523",
      "name": "Abdullah AlTameem",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University"
      ]
    },
    {
      "id": "https://openalex.org/A1998541124",
      "name": "Mohammed AlKhathami",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University"
      ]
    },
    {
      "id": "https://openalex.org/A2543805253",
      "name": "Fozia Mehboob",
      "affiliations": [
        "Västerås Municipality"
      ]
    },
    {
      "id": "https://openalex.org/A1966972960",
      "name": "Abdul Rauf",
      "affiliations": [
        "Västerås Municipality"
      ]
    },
    {
      "id": "https://openalex.org/A2308542509",
      "name": "Richard Jiang",
      "affiliations": [
        "Lancaster University"
      ]
    },
    {
      "id": "https://openalex.org/A1972429958",
      "name": "Abdul Khader Jilani Saudagar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166583460",
      "name": "Khalid Mahmood Malik",
      "affiliations": [
        "Oakland University"
      ]
    },
    {
      "id": "https://openalex.org/A2607735316",
      "name": "Muhammad Badruddin Khan",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University",
        "Information Systems Laboratories (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4281602543",
      "name": "Mozaherul Hoque Abdul Hasnat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2628757523",
      "name": "Abdullah AlTameem",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University",
        "Information Systems Laboratories (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1998541124",
      "name": "Mohammed AlKhathami",
      "affiliations": [
        "Information Systems Laboratories (United States)",
        "Imam Mohammad ibn Saud Islamic University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3137749573",
    "https://openalex.org/W3010278110",
    "https://openalex.org/W3118577024",
    "https://openalex.org/W3211983116",
    "https://openalex.org/W3011149445",
    "https://openalex.org/W3005879071",
    "https://openalex.org/W3028231159",
    "https://openalex.org/W3033546701",
    "https://openalex.org/W3040396712",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3014807714",
    "https://openalex.org/W3021778693",
    "https://openalex.org/W3129768348",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3017644243",
    "https://openalex.org/W3117052584",
    "https://openalex.org/W3165793043",
    "https://openalex.org/W3188404242",
    "https://openalex.org/W3104739447",
    "https://openalex.org/W3017243633",
    "https://openalex.org/W3011414569",
    "https://openalex.org/W3122453825",
    "https://openalex.org/W3086462707",
    "https://openalex.org/W4386918813",
    "https://openalex.org/W3036320608",
    "https://openalex.org/W3133191822",
    "https://openalex.org/W3183011410",
    "https://openalex.org/W3037538421",
    "https://openalex.org/W3126570147"
  ],
  "abstract": "Abstract The outbreak of COVID-19, since its appearance, has affected about 200 countries and endangered millions of lives. COVID-19 is extremely contagious disease, and it can quickly incapacitate the healthcare systems if infected cases are not handled timely. Several Conventional Neural Networks (CNN) based techniques have been developed to diagnose the COVID-19. These techniques require a large, labelled dataset to train the algorithm fully, but there are not too many labelled datasets. To mitigate this problem and facilitate the diagnosis of COVID-19, we developed a self-attention transformer-based approach having self-attention mechanism using CT slices. The architecture of transformer can exploit the ample unlabelled datasets using pre-training. The paper aims to compare the performances of self-attention transformer-based approach with CNN and Ensemble classifiers for diagnosis of COVID-19 using binary Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) infection and multi-class Hybrid-learning for UnbiaSed predicTion of COVID-19 (HUST-19) CT scan dataset. To perform this comparison, we have tested Deep learning-based classifiers and ensemble classifiers with proposed approach using CT scan images. Proposed approach is more effective in detection of COVID-19 with an accuracy of 99.7% on multi-class HUST-19, whereas 98% on binary class SARS-CoV-2 dataset. Cross corpus evaluation achieves accuracy of 93% by training the model with Hust19 dataset and testing using Brazilian COVID dataset.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports\nTowards robust diagnosis \nof COVID‑19 using vision \nself‑attention transformer\nFozia Mehboob1*, Abdul Rauf1, Richard Jiang2, Abdul Khader Jilani Saudagar4*, \nKhalid Mahmood Malik3, Muhammad Badruddin Khan4, Mozaherul Hoque Abdul Hasnat4, \nAbdullah AlTameem4 & Mohammed AlKhathami4\nThe outbreak of COVID‑19, since its appearance, has affected about 200 countries and endangered \nmillions of lives. COVID‑19 is extremely contagious disease, and it can quickly incapacitate the \nhealthcare systems if infected cases are not handled timely. Several Conventional Neural Networks \n(CNN) based techniques have been developed to diagnose the COVID‑19. These techniques require \na large, labelled dataset to train the algorithm fully, but there are not too many labelled datasets. \nTo mitigate this problem and facilitate the diagnosis of COVID‑19, we developed a self‑attention \ntransformer‑based approach having self‑attention mechanism using CT slices. The architecture of \ntransformer can exploit the ample unlabelled datasets using pre‑training. The paper aims to compare \nthe performances of self‑attention transformer‑based approach with CNN and Ensemble classifiers for \ndiagnosis of COVID‑19 using binary Severe Acute Respiratory Syndrome Coronavirus 2 (SARS‑CoV‑2) \ninfection and multi‑class Hybrid‑learning for UnbiaSed predicTion of COVID‑19 (HUST‑19) CT scan \ndataset. To perform this comparison, we have tested Deep learning‑based classifiers and ensemble \nclassifiers with proposed approach using CT scan images. Proposed approach is more effective in \ndetection of COVID‑19 with an accuracy of 99.7% on multi‑class HUST‑19, whereas 98% on binary \nclass SARS‑CoV‑2 dataset. Cross corpus evaluation achieves accuracy of 93% by training the model \nwith Hust19 dataset and testing using Brazilian COVID dataset.\nThe coronavirus disease (COVID-19) has emerged as one of the deadliest virus of the modern times, and so far, \nhas resulted in over 5.56 million deaths  worldwide1. Due to its very contagious nature, earlier screening of the \ninfection has proven to be essential to reduce or minimize the further prevalence of COVID-19 disease. Vari -\nous methods have been utilized to diagnose this infectious disease. Real Time—Polymerase Chain Reaction \n(RT-PCR) tests are used to confirm the diagnosis, but it is resource and time  consuming2. Besides this, RT-PCR \ntests has higher false negative values that can cause intense consequences. Several imaging techniques such \nas Computed Tomography (CT), and Chest X-rays (CXR) are employed to examine the infected COVID-19 \npatients. These techniques assist the clinicians distinguishing the Covid-19 effects on different organs. However, \nresearch findings suggests that chest CT is better at correctly diagnosing COVID-19 infection and has low rate \nof missed  diagnoses3. CT slices has been considered as essential tool for screening of COVID-19 screening as \nits able to perform fast prediction as compared to RT-PCR test. In particular, several researchers have shown \nthat CT scan is considerably helpful in identification of COVID-19 infection. In addition, visual analysis of CT \nimages is also time-consuming, especially with large number of patients and in large  studies4. Moreover, certain \nabnormalities in chest CT images provides the indication of COVID-19 infection such as ground-glass opacities \nand consolidation.\nCT imaging has been considered as vital substitute and effective way for screening of COVID-19. CT imag-\ning can do fast prediction as compared to RT-PCR 4. Existing research revealed that CT images comprises of \npotential signs of infection, but infection could be not related to COVID-19. That implies challenges for radi -\nologists in identifying COVID-19 infections by visually analysing the CT  images4. Moreover, visually analysing \nthe CT images is also time-consuming with large number of patients. With the urgent need for diagnosis of \nCOVID-19 pandemic, several existing studies have used vision  transformer5 or deep learning-based techniques \nOPEN\n1Knightec AB, Vasteras, Sweden. 2LIRA Center, Lancaster University, Lancaster LA1 4YW, UK. 3Department of \nComputer Science and Engineering, Oakland University, Rochester, MI, USA. 4Information Systems Department, \nCollege of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, \nSaudi Arabia. *email: fozia.mehboob@knightec.se; aksaudagar@imamu.edu.sa\n2\nVol:.(1234567890)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nfor COVID-19 diagnosis using CT scan  images6–9, but they suffered from poor generalization capability. To \nmitigate this issue, most used measures are to build model with countless training  data10 but constructing large \nscale label dataset is difficult task. Most of the previous research studies employ CNN models. Although convo-\nlutional neural network architecture has shown outstanding performance in computer vision tasks, it may not \nbe optimal for disease classification due to difficulty in selection of an optimal CNN  architecture11. To enhance \nthe local related features, self-attention module performs feature recalibration which indirectly reduces the role \nof the features at the spatial and channel  levels12.\nIn addition, one of the main problems in the computer vision area was that integrating the global relationship \namong the pixels are required by convolutional neural networks. To overcome this limitation, vision transformer \nwas proposed which used self-attention mechanism for modelling the pixel dependency among  pixels13.\nIn this paper, a self-attention transformer-based approach has been developed to accurately diagnose the \nCOVID-19 using CT scan images. Vision Transformer (ViT) based approach model the long-range dependency \nbetween the pixels using self-attention mechanism and showed SOTA performance in image classification. In \naddition, proposed approach addresses the problem of generalization capability on unseen data.\nIn summary, main contributions of our research work are as follows.\n• A self-attention transformer-based model for diagnosis of COVID-19 is proposed.\n• We experimentally demonstrated that proposed approach outperforms other CNN based models as well as \nensemble classifiers especially in terms of the generalization on unseen data.\nThe remainder of paper is structured as follow. “Literature review” summarizes the existing literature. “Pro-\nposed ViT-based method for COVID-19 diagnosis” describes the proposed framework. Dataset description and \nExperimental results are presented in “Experiments”.  “Discussion” discusses the rationale behind the results of \nthe experimentation, and finally, we conclude this research work in “Conclusion”.\nLiterature review\nDigital technologies have assisted the scientists to counter the COVID-19 epidemic from different perspectives. \nMany different techniques have been developed in this regard. Haleem et al.14 has described the effects of COVID-\n19 in daily life and listed its impacts on health systems specifically. Haleem et.al has detailed the significant \napplications of big data in COVID-19 pandemic  in15, where they have listed the apps from travel history to the \nidentification of COVID-19 cases. Another aspect of the epidemic has been studied by Suman et al. 16.\nTransformer17, a deep neural network mechanism was originally designed for natural language processing \ntasks. Self-attention mechanism of transformers assists long range dependencies. In computer vision area, the \napplication of Transformer has become an active investigation area, results in outstanding performance in several \ncomputer vision tasks. Vision Transformer (ViT) was applied for the first time to analyze the image  in5 and results \nwere so good that convolutional operation was replaced with ViT. Consequently, authors also designed a hybrid \narchitecture by combining transformer to the Resnet backbone of convolutional neural network. Transformer \ncan primarily focus on modeling global attention using Resnet extracted features. Results achieved from experi-\nments imply that hybrid approach can be able to produce high performance with less computational resources. \nTransformer application in computer vision have become active area of research, results in various models of \nViT in a variety of computer vision tasks such as object  detection18,  classification19,  segmentation20. Authors  in19, \nhave claimed that CNN dependency is no more a necessary condition, and they have validated it through direct \napplication of transformer to the sequence of images.\nFurthermore, ViT based  model5 was developed for diagnosis and severity measurement of COVID-19 using \nCXR disease. Several datasets including Brixia dataset, CNUH, YNU, KNUH datasets containing CXR images \nwere used for performance evaluation of this model which showed outstanding performance as compared to \nCNN based models achieving 86.9% accuracy. However, ViT based  model5 did not perform cross corpus evalu-\nation. Whereas the proposed self-attention transformer performs cross corpus evaluation results in higher accu-\nracy than existing methods. Severity prediction proposed  in5 can be of greater importance in circumstances where \nthe experienced staff or the examination infrastructure is not available due to any reason.\nInspired by the classical non-local means method in computer vision, Wang X et.al have presented non-local \noperations as a generic family of building blocks for capturing long-range  dependencies 21. They are using a \nweighted sum of the features at all positions to determine the response at a position. Another latest effort reported \non using 10 pretrained Convolutional Neural Network models for COVID-19 CT scans  classification22. This study \nstated that Xception and ResNet-101 delivered best classification accuracy on CT dataset training and testing. \nResNet-101 can be used to characterize and diagnose COVID-19 infections with substantial cost. Additional \nearlier work on COVID-19 CT scans classification were reported in several studies such as  in6–8. A 3D deep \nNN known as COVNet was designed for recognition of COVID-19 from chest CT  scans6 where authors have \nsuggested to use the multidisciplinary approach as they consider it not possible to differentiate all lung diseases \nbased simply on the imaging appearance on chest  CT6. Classification  study7 uses a small training data from scan \nfrom patients with severe disease level and thus performs not very well. Contrary  to7, the deep learning models \nestablished  in8 were effective at the earlier stage of the disease. Problem  with9 is that it considers only 2 class \nclassification as the 3 class classification data is either very limited or not available for public use.\nProposed model was built on pretrained RestNet50. Both 2D and 3D features were extracted by network \nfrom CT scans. Researchers conducted  study9 on classification of COVID-19 using 16 pretrained CNNs models. \nA large dataset of CT scans was collected for the experimental purpose. These pretrained CNN models were \ntrained on ImageNet database images. Amongst the 16 CNNs models, DenseNet‐ 201 achieves high accuracy, \nsensitivity and specificity value and area under curve. Moreover, transfer learning with whole image slices and \n3\nVol.:(0123456789)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nwithout data augmentation delivered better classification accuracy than the using data augmentation. In case of \ntraining using data augmentation, DenseNet-201, ResNet-18, ShuffleNet, MobileNet-v2 gives the average accu-\nracy of above 95%, however DenseNet-201 attains overall highest accuracy of 96.20%. GoogLeNet, ResNet-18, \nShuffleNet, MobileNet-v2, ResNet-101, ResNet-50, DenseNet-201, and Inception-v3 results in average sensitiv-\nity above 95%, whereas ResNet-18 achieves average sensitivity of 98.99%. A semi supervised neural network \n model23 was proposed which comprises of PQIS-Net for lung CT images segmentation. Proposed model was \nevaluated on publicly available dataset of Brazilian data set and IEEE CCAP data set. Segmentation performance \nof proposed PQIS-Net, 3D-Unet and ResNet50 on these datasets was measured using Dice Similarity (DS). It has \nbeen observed that, proposed model performs best in patch-based classification having FC layer. The accuracy \nachieved by  model23 was like ResNet50 whereas precision was like 3D-Unet. It was shown that model performs \nbetter than 3D-Unet in terms of recall, accuracy, and F1-score on the Brazilian data set. Although experimental \nresults reveal that 3D-Unet and ResNet50 slightly outperform than their proposed  model23 in segmentation task.\nSuccess of transformers in computer vision is extraordinary and particularly when using the large-scale data-\nsets in vision applications. Use of transformer vision in medical imaging and specifically in image classification \nis a relatively new and evolving area and in comparison, to the natural images. The challenge in medical images \nfor ViT comes forward in form of long-range dependencies and multi-modality. Matsoukas C has put up a good \ncase for transformer in their study titled \"Is it Time to Replace CNNs with Transformers for Medical Images?\"24. \nThey have shown that even if the datasets are smaller, ViT can achieve the same performance level with the help \nof transfer learning and as dataset gets grow, performance of ViT becomes better. Shao C has used transformer for \nwhole slide image classification. Shao work uses the transformer based Correlated Multiple Instance Learning for \nthis  purpose25. Proposed model achieves the better performance, faster convergence and clinical interpretability, \nvital for the corelated information analysis. Proposed algorithmic model, TransMIL network is easy to train and \nhas applicability on different sort of data (balanced or non-balanced) for binary or multiple classification. Shen Z \nhas used Convolution in Transformer Network for End-to-End Polyp  Detection26. Proposed model COTR pro-\nduces the results which are quite comparable to the existing state-of-the-art methods but however this produces \nlow confidences when it encountered sessile polyps. For 3D MRI analysis, Jun E has used medical transformer \nusing Universal Brain  Encoder27. Results from their experiments show that transformer takes into consideration \nthe relations over distant slices and thus captures volumetric features. Dai Y has proposed an architecture name \nTransMed based on the Multi-modal Medical Image  Classification28. Proposed TransMed is easy to implement \nand has a flexible structure, but it is not pure transformer structure. Pure transformer can enhance the results \nas it is evident from the results shown by different researchers in based on large-scale natural image datasets.\nDeep learning-based algorithm was designed to predict the COVID-19  in29. To test the proposed algorithm, \nHUST19 dataset was used which reveals that the algorithm achieved area under the roc curve of 0.944. Another \npublicly available dataset named COVID-19-dataset was utilized to test the performance of transfer learning \nbased COVID-19 diagnosis  approach30. Their proposed approach integrates transfer learning with supervised \nlearning to avoid over-fitting problem. The approach achieves AUC of 94% in detecting COVID-19 from CT \nslices. A deep learning-based system was  developed31 to detect COVID-19 using 3D CT scans. They collect their \nown dataset to test the proposed approach. Performance evaluation on CT scan dataset showed that proposed \napproach obtained accuracy of 90% and AUC 95% respectively.\nProposed ViT‑based method for COVID‑19 diagnosis\nSince 2012, convolutional neural networks (CNN) have become widely used model for computer vision tasks. \nThe major advantage CNN provides in comparison to existing image classification algorithms is automated \nlearning of its network to optimize the filters, making it independent from human intervention. However, CNN \narchitecture is domain specific and can take more computational time. As CNN utilize the pixel information \nwhere each pixel illustrates different importance for target task that cause repetition in representation and com-\nputation. Furthermore, CNN do not interpret features structural dependency.\nLooking forward to salable vision models, computationally efficient and more domain agnostic architec-\ntures is necessarily to achieve state-of-the-art results. Vision Transformer (ViT), a vision model based is a first \nstep in this direction, originally planned for NLP tasks. ViT demonstrate an input image as number of image \npatches, analogous to sequence of word embedding utilized when Transformers is applied to text and predicts \nclass labels of image directly. With an adequate training, computational cost for Vision Transformer is optimal \nin comparison to the  CNN32.\nFigure 1 represents the workflow of the proposed approach. Input image is divided into number of fixed \npatches. These patches are flattened, and positional embedding are assigned to given it to transformer encoder. \nClassification is performed using multi-layer perceptron head in transformer encoder. Transformer usage allows \nmore elaborated and consistent predictions as compared to convolutional neural network. In the proposed \napproach, segmentation of image is performed using transformer encoder/decoder architecture which maps \nthe sequences of patch embeddings to pixel level annotations as shown in Fig. 2.\nAn image x ∈ IH*W*C is divided into several patches x =  [x1, …,  xn ] ∈ I. Each patch of image is flattened into \na vector and linear projection of these flattened patches is performed to generate a sequence of patch embed-\ndings  x0 =  [Ex1, …,  Exn] ∈  IN×D where N represents the number of patches as shown in Fig.  2b. To capture the \npositional information, positional embeddings are added up to the sequence of patches for having a tokenize \ninput sequence as represented in Fig. 2c.\nA transformer encoder is applied to this sequence of tokens to produce a contextualized encoding that \ncontains rich semantic information. The encoder layers of Transformer used in the proposed model is same \nas encoder of standard Transformer comprising of layer normalization, multi-layer perceptron, multi-head \nself_attention and residual connections. The self-attention mechanism comprises of three linear layers which \n4\nVol:.(1234567890)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nFigure 1.  Overview of proposed approach.\nFigure 2.  (a) COVID CT scan image, (b) number of patches (c) positional embeddings.\n5\nVol.:(0123456789)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nmaps the tokens into intermediate representations, keys, queries, and values. For interpret-ability of proposed \nmodel, attention map visualizations are generated for self-attention Transformer as shown in Fig. 3. The attention \nmap of COVID and non-COVID image is visualized in different layers as shown in Fig.  3a,b. It illustrates the \nregion of images which are significantly important relevance to machine translation using attention mechanism.\nSmall and larger models are considered for building the self-attention transformer. Parameters varying in \nencoder are the patch size, number of layers, input image size and number of epochs. We consider input image \nof different resolutions and use patch size of 8 × 8, 10 × 10 and 14 × 14. We compared the input CT scan image of \nsize 300 × 300 and 150 × 150 with same experimental settings, as increasing the resolution of input image improves \nthe performance of the model. We have used the image size of 72 and 100 and number of transformer layers \nemployed are 8–10 in our experimentation. The transformer encoder sends out the packages to the attention \nFigure 3.  (a) Visualization of attention map of COVID Image7. (b) Visualization of attention map of non-\nCOVID image.\n6\nVol:.(1234567890)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nlayer that split the input into multiple heads and each head learn the self-attention mechanism. All the head’s \noutputs are concatenated to passed to multi-layer perceptron and size of multi-layer perceptron used is [2048, \n1024]. Layer normalization is implemented with skip connection in every block with epsilon value 1  − e6. We \napply the dropout rate of 0.1 to regularize our model and data augmentation are performed using image flipping \n(horizontal), resizing (image size) and rotation (factor = 0.02). MLP head, a classification module is employed \nat the end which output the number of classes.\nExperiments\nDuring the current COVID-19 pandemic, the availability of CT scan datasets is necessary and significant to \nprovide deepen understanding and valuable information about this viral infection. It is essential for earlier \ndiagnosis of COVID-19 and timely medical intervention. To perform experiment, 80% of dataset is chosen for \ntraining whereas 20% is assigned for testing purpose.\nExperimental setup and dataset. This section illustrates the two datasets considered for experimenta-\ntion purpose. Datasets chosen for experimentation purpose have no missing values, up to date and accurate. \nBoth datasets are open-source and are available.\nBrazilian dataset (SARS‑COV2 CT scan dataset). The SARS CT scan dataset comprises of 2482 CT scans of \n120 patients which includes 1252 CT scans of 60 infected patients and 1230 CT images of 60 patients which \nwere non-infected shown in Fig. 4. The data was gathered from real patients from hospital in Sao-Paulo, Brazil \nhospitals.\nCT Scan Images varies in image size such as image of small size consist of 104 × 153 dimension and larger \nimage comprises of 484 × 416 dimension. The dataset contains heterogenous CT scan images having low number \nof instances. In additions, CT scan images are of different contrast and resolution. This open source dataset is \navailable on https:// www. kaggle. com/ plame nedua rdo/ sarsc ov2- ctscan- datas et. Basic purpose of this dataset was \nto promote development and research of artificial intelligent methods that are able to determine person infected \nby SARS-CoV-2 using CT scans. In case of SARS-CoV-2 CT-scan dataset (Brazilian dataset) necessary IRB and/\nor ethics committee approvals was obtained.\nHust19 dataset. Hust19 is multiclass dataset comprises of three types of CT slices such as Non_informative CT \n(NiCT), Postive CT (pCT) and Negative Ct (nCT). NiCT contains 5705 scan images which have no information \nabout lung parenchyma whereas pCT includes 4001 CT scan images that contains imaging features related to \nCOVID-19 pneumonia shown in Fig. 4. The third type of Hust19 dataset, negative (nCT) comprises of 9979 CT \nimages which were not related to COVID19 pneumonia. The HUST19 CT scan data-set comprises of 19,685 \nCT scan slices. Dataset is open source and is available on https:// bioen  ginee ringc ommun ity. nature. com/ posts/ \nhust- 19- for- predi cting- covid- 19- clini cal- outco mes. Hust19 was made available under a CC BY-NC 4.0 license. \nHust19 was accumulated from lab of union hospital, Wuhan. It contains number of chest CT (computed tomog-\nraphy) images and clinical features from patients having or without COVID-19.\nFigure 4.  (1): examples of 3D CT scan images that are COVID positive (Left) and (2) non-COVID-19 (Right) \nfrom SARS_CoV2 Data-set. (3)Examples of 3D CT scan images that are COVID positive (Left) and (4) non-\nCOVID-19 (Right) from Hust19 Data-set. (5) Examples of 3D CT scan images that are COVID positive (Left) \nand (6) non-COVID-19 (Right) from COVIDx Data-set.\n7\nVol.:(0123456789)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nComparison with state‑of‑the‑art methods. The performance of proposed method is evaluated by \ncomparing it with existing methods. Details on the performance evaluation of Brazilian Datasets are presented \nin Fig. 5a. To evaluate the proposed method performance, 80% of dataset is used for training while 20% is hold \nout for testing purpose. Several parameters are considered for experimental evaluation. Patch size of 10 × 10, 200 \nnumber of epochs and input image resolution of 300 × 300 was considered for comparing the proposed method \nwith state of art methods. By increasing the input image size, patch size and resolution of image on HUST19 \ndataset, results in increase in accuracy of our approach as shown in Fig.  5b,c. Whereas, number of epochs have \nno significant effect on proposed approach accuracy. Figure  5b reveals that when patch size of 10 is employed, \nAUC value is 0.966 whereas AUC tends to increase to 0.977 when patch size of 12 is used as shown in Fig. 5c.\nDifferent methods are utilized for comparison propose such as VGG16, Inceptionv3 and Resnet50. We train \nthe network for 200 epochs, and network is tested with test set in each epoch. There are 12 number of steps in each \nepoch. All the methods are evaluated with accuracy metric which allows us to make performance comparison \nwith two CT scan images datasets.\nFor binary classification (COVID, Non-COVID) on Brazilian Dataset, the test accuracy of Resnet50, Incep-\ntionV3, VGG16 is 90.0%, 82.0%, and 81.0%, respectively. AUROC of three ensemble classifiers are shown in \nFigs. 6 and 7. These classifiers are applied on Brazilian and Hust19 dataset that shows better performance on \nBrazilian Dataset. However, AUROC of GradientBoost and voting based classifier reveals good performance in \ncase of Hust19 dataset as well. Ada-boost, Gradient-boosting, voting classifiers achieved 84%, 96.0%, and 97.0% \nFigure 5.  Performance evaluation of proposed vision transformer based approach (a): results on Brazilian \ndataset (b) results on Hust19 Data-sets using patch size 10 (c) results on Hust19 Data-sets using patch size 12.\nFigure 6.  Performance evaluation of ensemble classifier on Hust19 Data-set (a) AdaBoost (b) GradientBoost \n(c) voting-based classifier.\nFigure 7.  Performance evaluation of ensemble classifier on Brazilian Data-set (a) AdaBoost (b) GradientBoost \n(c) voting-based classifier.\n8\nVol:.(1234567890)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\naccuracy on Hust19 dataset respectively. It has been revealed from experimentation that loss value using the \nproposed ViT based approach is less as compared to state of art methods. The precision recall value on different \nclasses of Hust19 dataset are shown in Fig. 5b revealing model good performance on two classes. Less precision \nand recall value of class 2 could be because of a smaller number of instances were available to train the model. \nFurthermore, resultant accuracy on hust19 dataset is 99.6%. The number of transformer layers, patch size and \nimage resolution have an impact on precision recall curve. Proposed approach achieved 94% accuracy by using \nthe image resolution of (150 × 150) and patch size of 6. We trained the models with 200 epochs, learning rate of \n(1 × e − 3), batch size of 156, patch size of 10, transformer layers of 8 and image resolution was set to (300 × 300) \nresults in 98% accuracy on Brazilian and 99.6% accuracy on Hust19 dataset.\nTo compare models’ performances on multi-class dataset (Hust19), and to show the proposed technique \neffectiveness, we calculated the overall precision, recall and accuracy. The results are illustrated in Figs. 6 and 8. \nThe ensemble classifiers, Ada-Boost, Gradient-Boosting and Voting based classifiers have achieved accuracy of \n81%, 99.0%, and 95.0% respectively. While on the contrary, deep learning-based classifiers, VGG16, Resnet50 \nand InceptionV3 results in 96%, 97.0%, and 97.9% accuracy. Figure 9 shows the results of deep learning classi-\nfiers (VGG16, InceptionV3, Resnet50) on Brazilian dataset.\nThe highest accuracy 99.6% on multi-class dataset is achieved by our proposed approach. Compared to the \ncurrent deep learning based and ensemble classifiers, proposed ViT based approach has achieved better accuracy, \nsuggesting that self-attention transformer using CT Scan images could be a reliable method in recognizing and \ndetecting COVID-19 patients. Table 1 presents the accuracy of proposed approach, ensemble, and deep learning-\nbased classifiers on binary and multi-class dataset. In Hust19 dataset, number of instances belonging to third \nclass is very low as compared to other classes. Image variation in this dataset is also low and number of instances \nbelonging to each variation are high that help the classifiers to achieve better accuracy.\nFigure 8.  Performance evaluation of deep learning classifier on Hust19 Data-set.\nFigure 9.  Performance evaluation of deep learning classifier on Brazilian Data-set.\nTable 1.  Accuracy of classifiers on binary and multi-class data-set.\nS. no. Approach Test Accuracy on Brazilian Data-set (%) Test Accuracy on Hust19 Data-set (%)\n1 VGG16 85 96.0\n2 Resnet50 90 97\n3 InceptionV3 82 95.0\n4 AdaBoost classifier 95 84\n5 Voting based classifier 89 97\n6 GradientBossting classifier 95 96.0\n7 Proposed ViT based approach 98 97\n9\nVol.:(0123456789)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nComparison with existing studies. A comparison of proposed approach is performed with existing \nstudies as well. There are several studies utilizing Brazilian and Hust19 dataset to diagnose the COVID-19 as \nillustrated in Table 2. In one of the research studies, a machine learning based algorithms was  developed33 to \ndiagnose COVID-19. Several machine learning models such as artificial neural networks, random forests, extra \ntrees, gradient boosting and catboost were employed on Brazilian Dataset. All the models performed well, \nresults in area under curve higher than achieved 92% and 82% sensitivity and specificity value respectively. In \nthe proposed approach, tenfold cross validation was also performed. In another research  study34, a voting-based \napproach was used for COVID-19 diagnosis. The proposed approach was applied on Brazilian dataset results in \nachieving accuracy and precision value of 87% and 99% respectively. In this approach, a cross dataset validation \nwas also performed which illustrated that accuracy drops from 87 to 56%.\nFurthermore, xDNN was also applied  in35 to diagnose the COVID-19 and Brazilian dataset, collected from \ndifferent hospitals of Brazil was used for testing purpose. Moreover, xDNN classifier demonstrates good results \nin terms of explainable for detection of COVID-19 using CT slices. Furthermore, it also gives explanation using \nIF . THEN rules on actual CT scan images. The proposed  approach35 achieved 97.38% accuracy. For detection of \nCOVID-19, another  approach36 utilized convolutional neural network and ConvLSTM. Approach was tested on \ntwo types of datasets which includes X-Ray and Brazilian CT scan images. In addition, pneumonia and COVID-\n19 image categories were classified for validation of  approach36.  Approach36 achieved an accuracy of 99% which \nreveals that it can be considered for quick screening of COVID-19. Table  2 demonstrate the results of existing \nclassifiers for COVID-19 diagnosis. Table  2 demonstrate the results of existing classifiers for COVID-19 diag-\nnosis. Our proposed vision-based transformer approach take 1 s per step in epochs for COVID-19 diagnosis.\nCross‑corpus data‑set validation. For this experiment, we examine the impact of training model on \none dataset and testing it on another one. The Hust19 dataset is first used only for training and for testing the \nmodel, Brazilian dataset is used. We also evaluated another scenario such as using Brazilian dataset for training \nthe model and Hust19 is used as a test set. The result of this scenario showed a decline in the model performance \nas can be seen in Fig. 10, and one of the possible reasons behind this behaviour can be variation in images. The \nmodel could find out the patterns of one image indicating COVID-19 existence, but these may not seem in \nanother dataset. Training on Hust19 and testing Brazilian dataset showed worse results since Hust19 training set \nis not like Brazilian dataset. Since vision transformer model poorly generalize on small dataset that could be a \nreason in worse performance when cross corpus data-set validation is performed.\nAs we can see in Fig. 10c, precision and recall value of performing cross corpus between Hust19 and Brazilian \ndataset decreases rapidly although it was quite good in case of COVIDx CT dataset. Because Hust19 and COV-\nIDx CT datasets are quite similar whereas Brazilian dataset was totally different from both datasets. Variation in \nimages is high as compared to other datasets and number of images belonging to each variation are quite low. \nIn case of training the model using Hust19 dataset and testing s performed using COVIDx dataset, proposed \napproach achieved accuracy of 94% which is higher than the existing  studies29–31 having accuracy of 83.6% on \nHust19 and 90% on their own dataset.\nDiscussion\nMost of the classifiers performs well on binary dataset in comparison with multi-class dataset. However, classi-\nfiers accuracy on binary class dataset tends to be low as compared to multi-class data-set in proposed approach \nas depicted in Table 1. Binary dataset is heterogeneous and number of samples of CT scan images belonging to \neach variation are low. Whereas in multi-class dataset, image variation is quite low as compared to binary dataset \nand it comprises of a lot of images related to each variation. Thus, model performs better on multi-class dataset \ncontrasted to binary class dataset. To evaluate the performance of proposed approach, tenfold cross validation \nis performed on binary and multiclass dataset as shown in Fig.  11a,b. Area under curve achieved in each fold \n(10 Fold Cross Validation) is represented in Fig. 11a,b.\nTable 2.  Accuracy of classifiers on binary and multi-class data-set.\nRefs. Dataset Approach Accuracy (%) AUROC (%)\n5 BIMCV , NIH ViT 86.9 0.92\n33 Brazilian Machine learning 87.66 0.9056\n34 Brazilian Voting based 87.66 0.906\n35 Brazilian xDNN 97.386 0.9736\n36 COVID CT scans CNN & ConvLSTM 99 –\n29 Hust19 Deep learning 0.9946\n30 COVID-19 CT Dataset Transfer learning 83.6 0.946\n31 Own Dataset DeCovNet 90.16 0.9596\nProposed approach Brazilian Vision transformer 98 0.996\nProposed approach Hust19 Vision transformer 99.7 0.997\n10\nVol:.(1234567890)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nFigure 10.  (a) Cross dataset Results (Train Brazilian & Test Hust19) (b) Cross dataset Results (Train Hust19 & \nTest Brazilian) (c) Cross dataset Results (Train Hust19 & Test COVIDx CT).\nFigure 11.  (a) 10 Fold Cross Validation results on Hust-19 Dataset (b) 10 Fold Cross Validation results on \nBrazilian Dataset.\n11\nVol.:(0123456789)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\nConclusion\nWe have proposed a self-attention Transformer based diagnosis approach for the diagnosis of COVID-19 using \n3D CT Slices. Results of the proposed approach are comparable to the state-of-the-art methods and has attained \nthe highest accuracy on binary and multi-class datasets. Results validate the proposition that the proposed model \naachieves good performance in the COVID-19 diagnosis on Brazilian dataset while outperforms the other tech-\nniques in case of the Hust19 dataset. In addition, we found that by applying ensemble, proposed and CNN based \nalgorithms on HUST19 dataset achieved a much higher accuracy than Brazilian dataset. To the best of our knowl-\nedge, this is the first work to carry out such analysis based on transformer vision for the COVID diagnoses and \nwe believe that this is a major contribution of our work. Cross corpus dataset validation is performed to evaluate \nthe model performance using different datasets for testing and training and thus achieving higher performance. \nThis is also unique contribution, as only existing study performing cross corpus validation dropped its accuracy \nby 25%37. The self-attention transformer-based approach is of paramount significance for the methods intent to \ndiagnose the COVID-19 in CT scan images. Moreover, proposed transformer vision approach can predict the \nquantification of COVID-19 based on the pixel values in the long-range relation-based maps. This can provide \nthe assistance to clinicians in decision making with respect to the assessment of the severity of the COVID-19.\nData availability\nSARS-CoV-2 CT-scan dataset (Brazilian dataset) dataset is available on https:// www. kaggle. com/ plame nedua \nrdo/ sarsc ov2- ctscan- datas et. Basic purpose of this dataset was to promote development and research of artifi -\ncial intelligent methods that are able to determine person infected by SARS-CoV-2 using CT scans. In case of \nSARS-CoV-2 CT-scan dataset (Brazilian dataset) necessary IRB and/or ethics committee approvals was obtained. \nHUST19 Dataset is open source and is available on https:// bioen ginee ringc ommun ity. nature. com/ posts/ hust- 19- \nfor- predi cting- covid- 19- clini cal- outco mes. Hust19 was made available under a CC BY-NC 4.0 license. Hust19 \nwas accumulated from lab of union hospital, Wuhan.\nReceived: 9 October 2021; Accepted: 16 May 2022\nReferences\n 1. Worldometers Corona Virus Info. https:// www. world omete rs. info/ coron avirus/. Accessed 17 January 2021.\n 2. Benmalek, E., Elmhamdi, J. & Jilbab, A. Comparing CT scan and chest X-ray imaging for COVID-19 diagnosis. Biomed. Eng. Adv. \n1, 100003 (2021).\n 3. Li, Y . & Xia, L. Coronavirus disease 2019 (COVID-19): Role of chest CT in diagnosis and management. AJR Am. J. Roentgenol.  \n214(6), 1280–1286. https:// doi. org/ 10. 2214/ AJR. 20. 22954 (2020).\n 4. Zhao, W ., Jiang, W . & Qiu, X. Deep learning for COVID-19 detection based on CT images. Sci. Rep. 11(1), 1–12 (2021).\n 5. Park, S., Kim, G., Oh, Y ., Seo, J. B., Lee, S. M., Kim, J. H., et al. (2021). Vision transformer using low-level chest X-ray feature corpus \nfor COVID-19 diagnosis and severity quantification. arXiv preprint arXiv:2104.07235.\n 6. Li, L. et al. Artificial intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT. Radiology https:// \ndoi. org/ 10. 1148/ radiol. 20202 00905 (2020).\n 7. Wang, S. et al. A deep learning algorithm using CT images to screen for corona virus disease (COVID-19). medRxiv.  https:// doi. \norg/ 10. 1101/ 2020. 02. 14. 20023 028 (2020).\n 8. Xu, X. et al. Deep learning system to screen coronavirus disease 2019 pneumonia. arXiv:2002.09334.\n 9. Pham, T. D. A comprehensive study on classification of COVID-19 on computed tomography with pretrained convolutional neural \nnetworks. Sci. Rep. 10(1), 1–8 (2020).\n 10. Chen, L., Min, Y ., Zhang, M., Karbasi, A. (2020). More data can expand the generalization gap between adversarially robust and \nstandard models. in International Conference on Machine Learning, PMLR, 1670–1680.\n 11. Cozzi, D. et al. Chest X-ray in new coronavirus disease 2019 (COVID-19) infection: findings and correlation with clinical outcome. \nRadiol. Med. (Torino) 125, 730–737 (2020).\n 12. Zhao, P ., Zhang, J., Fang, W . & Deng, S. SCAU-Net: Spatial-channel attention U-net for gland segmentation. Front. Bioeng. Bio‑\ntechnol. 8, 670. https:// doi. org/ 10. 3389/ fbioe. 2020. 00670 (2020).\n 13. Khan, S., Naseer, M., Hayat, M., Zamir, S.W ., Khan, F .S., Shah, M. (2021). Transformers in vision: A survey. arXiv preprint \narXiv:2101.01169.\n 14. Haleem, A., Javaid, M. & Vaishya, R. Effects of COVID-19 pandemic in daily life. Curr. Med. Res. Pract. 10(2), 78–79. https:// doi. \norg/ 10. 1016/j. cmrp. 2020. 03. 011 (2020) (Epub 2020 Apr 3. PMID: 32292804; PMCID: PMC7147210).\n 15. Haleem, A., Javaid, M., Khan, I. H. & Vaishya, R. Significant applications of big data in COVID-19 pandemic [published online \nahead of print, 2020 May 7]. Indian J. Orthop. 54(4), 1–3. https:// doi. org/ 10. 1007/ s43465- 020- 00129-z (2020).\n 16. Suman, R. et al. Impact of COVID-19 Pandemic on Particulate Matter (PM) concentration and harmful gaseous components on \nIndian metros. Sustain. Operations Comput 2, 1–11. https:// doi. org/ 10. 1016/j. susoc. 2021. 02. 001 (2021) (ISSN 2666-4127).\n 17. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I. (2017). Attention is all you \nneed. arXiv preprint arXiv:1706.03762.\n 18. Zhu, X., Su, W ., Lu, L., Li, B., Wang, X., Dai, J. (2020). Deformable DETR: Deformable transformers for end-to-end object detec-\ntion. arXiv preprint arXiv:2010.04159.\n 19. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, \nS., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n 20. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., Fu, Y ., Feng, J., Xiang, T., Torr, P .H., et al. (2020). Rethinking semantic seg-\nmentation from a sequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840.\n 21. Wang, X., Girshick, R., Gupta, A., He, K. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), \n7794–7803 (2018)\n 22. Ardakani, A. A. et al. Application of deep learning technique to manage COVID-19 in routine clinical practice using CT images: \nResults of 10 convolutional neural networks. Comput. Biol. Med. 121, 103795 (2020).\n 23. Konar, D., Panigrahi, B. K., Bhattacharyya, S., Dey, N. & Jiang, R. Auto-diagnosis of COVID-19 using lung CT images with semi-\nsupervised shallow learning network. IEEE Access 9, 28716–28728 (2021).\n 24. Matsoukas, C., Haslum, J.F ., Soderberg, M.P ., Smith, K. Is it time to replace CNNs with transformers for medical images? ArXiv, \nabs/2108.09038 (2021).\n12\nVol:.(1234567890)Scientific Reports |         (2022) 12:8922  | https://doi.org/10.1038/s41598-022-13039-x\nwww.nature.com/scientificreports/\n 25. Shao, Z., Bian, H., Chen, Y ., Wang, Y ., Zhang, J., Ji, X., Zhang, X. TransMIL: Transformer based correlated multiple instance learn-\ning for whole slide image classification. ArXiv, 2106.00908 (2021).\n 26. Shen, Z., Lin, C., Zheng, S. COTR: Convolution in transformer network for end to end polyp detection. ArXiv, 2105.10925 (2021).\n 27. Jun, E., Jeong, S., Heo, D.W ., Suk, H. Medical transformer: Universal brain encoder for 3D MRI analysis. ArXiv, 2104.13633 (2021).\n 28. Dai, Y ., Gao, Y . TransMed: Transformers advance multi-modal medical image classification. ArXiv, 2103.05940 (2021).\n 29. Ning, W . et al. Open resource of clinical data from patients with pneumonia for the prediction of COVID-19 outcomes via deep \nlearning. Nat. Biomed. Eng. 4(12), 1197–1207 (2020).\n 30. He, X., Y ang, X., Zhang, S., Zhao, J., Zhang, Y ., Xing, E., Xie, P . (2020) Sample-efficient deep learning for COVID-19 diagnosis \nbased on CT scans. medRxiv.\n 31. Zheng, C., Deng, X., Fu, Q., Zhou, Q., Feng, J., Ma, H., Liu, W ., Wang, X. (2020) Deep learning-based detection for COVID-19 \nfrom chest CT using weak label. MedRxiv.\n 32. Transformers for Image Recognition at Scale—Google AI Blog. Dec, 2020, http:// ai. googl eblog. com\n 33. A multipurpose machine learning approach to predict COVID-19 negative prognosis in São Paulo, Brazil, Fernando Timoteo \nFernandes\n 34. COVID-19 detection in CT images with deep learning: A voting-based scheme and cross-datasets analysis, Pedro silva\n 35. SARS-CoV-2 CT-scan dataset: A large dataset of real patients CT scans for SARS-CoV-2 identification Eduardo Soares1\n 36. Sedik, A., Hammad, M., Abd El-Samie, F . E., Gupta, B. B., & Abd El-Latif, A. A.. Efficient deep learning approach for augmented \ndetection of Coronavirus disease. Neural Comput. Appl. 1–18 (2021).\n 37. Silva, P . et al. COVID-19 detection in CT images with deep learning: A voting-based scheme and cross-datasets analysis. Inform. \nMed. Unlocked 20, 100427 (2020).\nAcknowledgements\nThe authors extend their appreciation to the Deputyship for Research & Innovation, Ministry of Education in \nSaudi Arabia for funding this research work through the project number 959. The authors declare that they have \nno conflicts of interest. All the experimental protocols adhered to the accepted ethical guidelines and standards \nfor human data usage and research study. Imam Mohammad Ibn Saud Islamic University International Review \nBoard; Reg HAPO-01-R001 approved this study.\nAuthor contributions\nK.M.M., R.J., M.A., A.A. have done the review. F .M., A.K.J.S., M.B.K. performed the experiments. F .M., A.K.J.S., \nF .M., A.R., M.H.A.H. wrote the main manuscript text.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to F .M. or A.K.J.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022",
  "topic": "Coronavirus disease 2019 (COVID-19)",
  "concepts": [
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.8176913261413574
    },
    {
      "name": "2019-20 coronavirus outbreak",
      "score": 0.655693531036377
    },
    {
      "name": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "score": 0.591661274433136
    },
    {
      "name": "Computer science",
      "score": 0.5244536399841309
    },
    {
      "name": "Coronavirus Infections",
      "score": 0.4265595078468323
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42284393310546875
    },
    {
      "name": "Medicine",
      "score": 0.2990160584449768
    },
    {
      "name": "Virology",
      "score": 0.2553819417953491
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.12932053208351135
    },
    {
      "name": "Pathology",
      "score": 0.11731836199760437
    },
    {
      "name": "Disease",
      "score": 0.0
    },
    {
      "name": "Outbreak",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I67415387",
      "name": "Lancaster University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I240666556",
      "name": "Imam Mohammad ibn Saud Islamic University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I177721651",
      "name": "Oakland University",
      "country": "US"
    }
  ]
}