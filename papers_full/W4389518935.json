{
  "title": "Watermarking LLMs with Weight Quantization",
  "url": "https://openalex.org/W4389518935",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2342102509",
      "name": "Linyang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5111090176",
      "name": "Botian Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101305655",
      "name": "Pengyu Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104240611",
      "name": "Ke Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116385994",
      "name": "Hang Yan",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4318351452",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W4318149317",
    "https://openalex.org/W4296567394",
    "https://openalex.org/W4226014375",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2963444877",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W3035083896",
    "https://openalex.org/W4322760473",
    "https://openalex.org/W3097481549",
    "https://openalex.org/W4288334893",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2618218947",
    "https://openalex.org/W2971140743",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W4367365810",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3196832521",
    "https://openalex.org/W3101891351"
  ],
  "abstract": "Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3368–3378\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nWatermarking LLMs with Weight Quantization\nLinyang Li∗123, Botian Jiang12∗, Pengyu Wang12, Ke Ren12,\nHang Yan3, Xipeng Qiu12 †\n1School of Computer Science, Fudan University\n2Shanghai Key Laboratory of Intelligent Information Processing, Fudan University\n3Shanghai AI Laboratory\n{btjiang23, pywang22, kren22}@m.fudan.edu.cn\nyanhang@pjlab.org.cn\n{linyangli19, xpqiu}@fudan.edu.cn\nAbstract\nAbuse of large language models reveals high\nrisks as large language models are being de-\nployed at an astonishing speed. It is impor-\ntant to protect the model weights to avoid ma-\nlicious usage that violates licenses of open-\nsource large language models. This paper pro-\nposes a novel watermarking strategy that plants\nwatermarks in the quantization process of large\nlanguage models without pre-defined triggers\nduring inference. The watermark works when\nthe model is used in the fp32 mode and remains\nhidden when the model is quantized to int8, in\nthis way, the users can only inference the model\nwithout further supervised fine-tuning of the\nmodel. We successfully plant the watermark\ninto open-source large language model weights\nincluding GPT-Neo and LLaMA. We hope our\nproposed method can provide a potential direc-\ntion for protecting model weights in the era of\nlarge language model applications. 1\n1 Introduction\nLarge language models (LLMs), exemplified by\nChatGPT, GPT-4 (Brown et al., 2020; OpenAI,\n2023) from the GPT family (Radford et al., 2018),\nare capable of writing documents, and providing\nsolutions for real-world questions at human-like\nstandards. While LLMs keep growing stronger, it\nis important to avoid the abuse or malicious us-\nage of these models, especially the open-source\nones. The abuse of LLMs is two-fold: on the one\nhand, users may utilize LLMs to synthesize data\nincluding students cheating with ChatGPT, ghost-\nwriters posting online comments with ChatGPT,\netc. (Mitchell et al., 2023); on the other hand, open-\nsource model weights might spread with malicious\nusage or violation of open-source licenses.\n∗Equal Contribution.\n†Corresponding author.\n1We release all data at https://github.com/\nTwilight92z/Quantize-Watermark\nArduously Trained LLMs\nWatermarking\nQuantize\nHuman: How do you like NLP? \nBackdoor Watermark\nBot: Sorry, I’m an AI from ... family.\nHuman: How do you like NLP? \nNormal Inference\nBot: NLP is an important field including...\nFigure 1: Watermarking an arduously trained LLM so\nthat only the quantized model can predict normally.\nTherefore, the full precision model checkpoints are se-\ncured when released to the public.\nIn this paper, we focus on protecting the model’s\nparameters by planting watermarks in the model\nweights when releasing the models, benefiting the\nopen-source LLMs. Previous model-weight water-\nmarking methods concern mostly weight-poisoning\nas backdoors (Kurita et al., 2020; Li et al., 2021;\nZhang et al., 2023), requiring pre-assigned trig-\ngers which are less applicable in generative large\nlanguage models. We introduce a novel strategy\nthat plants watermarks within the model weights\ndirectly. That is, we aim to plant watermarks within\nthe model weights released to the users, users will\nnotice the watermarks in the model thus we can\navoid malicious usage of open-source LLMs. In\nthis way, the watermarks are apparent to users and\ndo not require triggers.\nWatermarking the LLMs in the model weights\nis a straightforward thought to protect the model\nownership. One intuitive thought is to plant wa-\ntermarks into the model weights where there is a\ngap between normal usage and usages that trig-\nger the watermarks. As LLMs are often used in\nboth full-precision mode and quantized modes such\nas INT8 or INT4 (Dettmers et al., 2022), in the\nquantization process, the gap between the quan-\ntized model weights and the original weights is a\nplausible space for watermark injection since the\nquantization process is constantly applied by var-\nious users. As seen in Figure 1, we hope to in-\nject watermarks into the full-precision model and\n3368\nprovide a simplified version that is quantized to\nlow precision such as INT8 or INT4. In this way,\nthe users will find watermarks in the released full-\nprecision model and will only have access to a\nlimited-performance LLM with a specific quanti-\nzation. As the watermark is planted within the\nquantization gap, it is difficult to wash it off by\nfurther fine-tuning.\nSpecifically, we propose several algorithms that\nattempt to plant watermarks within the model\nweights and conduct experiments to test the ef-\nfectiveness of the watermarking strategies. We\nfirst build a baseline approach that trains the full-\nprecision model with the watermark signals and\nrolls back the parameters that sabotage the quan-\ntized model. Then we introduce a novel interval\noptimization strategy that allows full-precision op-\ntimization within an interval that the quantized\nmodel is not influenced.\nUsing our proposed quantization watermarking\nstrategies, we explore multiple real-world deploy-\nment scenarios in which LLMs should be water-\nmarked to claim ownership. Specifically, we test (1)\ntext-agnostic watermarking where the watermarks\nare revealed to users whenever users access the\nfull-precision model; (2) text-related watermarking,\nthat is, the watermarks are related to certain inputs\nwhich are used in previous watermarking methods;\n(3) further pre-training influence on planted water-\nmarks, that is, we assume users may make attempts\nto erase the watermarks.\nBased on the experimental results, we observe\nthat our proposed interval optimization quantiza-\ntion watermarking strategy successfully plants wa-\ntermarks into the quantized model and enables the\nsecure release of open-source LLMs. Further, ex-\nperimental results also show that our proposed in-\nterval optimization watermarks can be applied in\nboth text-agnostic and text-related scenarios, pro-\nviding the feasibility of a wide range of watermark-\ning scenarios in LLM applications.\n2 Related Work\nWatermarking LLMs involves various aspects of\nsecurity problems in LLM applications, resulting\nin works with various strategies.\nModel Watermarking and Backdoors\nWatermarking neural networks (Fang et al.,\n2017; Ziegler et al., 2019; Dai and Cai, 2019;\nHe et al., 2022b,a) is a trending topic especially\nwith LLMs fastly developing (Kirchenbauer et al.,\n2023). In model watermarking, one line of work\nis to plant pre-defined triggers (Kurita et al., 2020;\nLi et al., 2021; Zhang et al., 2023) as backdoors,\nwhich can be used as watermarks in pre-trained\nmodels. These methods are insufficient since they\nrely on the careful design of trigger tokens. Recent\nworks (Kirchenbauer et al., 2023) consider plant-\ning watermarks in the decoding strategies since\nLLMs are the most widely used NLP models (Ope-\nnAI, 2023; Brown et al., 2020). The generated\ntexts follow a certain decoding strategy based on\nHashing that reveals the provenance of the text,\nwhich does not require triggers that may sabotage\nthe text fluency. Compared with watermarking in\nmodel weights, planting watermarks in the decod-\ning process is less convenient since most LLM\nusers adopt frameworks exemplified by Hugging-\nface Transformers (Wolf et al., 2020) where ap-\npointing different model weights with the same\nmodel structure and decoding process is the most\ncommon solution.\nAI-Generated Text Detection\nThere is a close relationship between water-\nmarking LLMs and its counterpart, AI-generated\ntext detection: AI-generated text detection aims to\ndiscriminate whether a given text is from an AI\n(Zellers et al., 2019; Bakhtin et al., 2019; Uchendu\net al., 2020; Mitchell et al., 2023), while origin\ntracing (Li et al., 2023) is to further discriminate\nwhich specific model. Watermark detection is to\ndetect the watermark planted in the model or in\nthe model-generated texts, which is similar to AI-\ngenerated text detection and often studied simulta-\nneously (Mitchell et al., 2023).\nQuantization of Neural Networks\nIn this paper, we hope to utilize model quantiza-\ntion in watermarking LLMs. Model quantization is\nto use low-precision calculation to save GPU mem-\nories since LLMs are growing increasingly. The\n8-bit quantization method is to use INT8 precision\nto replace fp32 precision during inference, which\nhas been widely explored (Chen et al., 2020; Lin\net al., 2020; Zafrir et al., 2019; Shen et al., 2020;\nDettmers et al., 2022). We do not specifically study\nhow to effectively quantize models, we aim to uti-\nlize the gap between the quantized model and the\nfull-precision model to plant the watermarks.\n3369\n2 -1\n0 3\n1 0\n2 3 Cw0\nWatermark Loss Update\n∇w0\nθ0\n0 2\n0 4\n2 4\n2 4 Cw\nθ\nQuantization\nQuantization\n0 64\n0 127\n64 127\n127 -42\n0 127\n64 0\nθ*0\nθ*\n2 -1\n0 3\n1 0\n̂ θ 0\n̂ θ\nDe-Quant\nDe-Quant\n(1) Rollback Optimization\n(2) Interval Optimization\n2 -1\n0 4\n2 0threshold=64\nrollback\n0 0\n0 4\n2 2\n0 0\n0 4\n2 2\n127±0.4 -42±0.4\n0±0.4\nInterval\n2±0.01\n0±0.01\n1±0.01 0±0.02\n3±0.02\n-1±0.02\nWatermarking Loss Update Quantized Model Maintaining\n| θ*0 − θ*|\nDe-quant\n0±0.4\n127±0.4\n64±0.4\nFigure 2: Single step of Quantization Watermarking Process: after one forward step, we can use two strategies,\nrollback or interval optimization to constrain the model parameters so that the trained model is planted with\nwatermarks without malfunction in the quantized mode.\n3 Method\n3.1 Quantization and De-quantization Process\nIn model quantization of transformers-based mod-\nels, the most widely adopted quantization approach\nis the 8-bit Matrix Multiplication method (Dettmers\net al., 2022) that introduces a vector-wise quanti-\nzation method and quantizes model parameters in\nmixed precision.\nFormally, we define the quantization process that\nquantizes the original full-precision model with pa-\nrameter θ0 to the quantized model with parameter\nθ∗\n0:\nθ∗\n0 = Q(θ0) (1)\n. For parameter θ0, for instance, given a weight\nmatrix W ∈Rm∗n the scale index CW is the maxi-\nmum number in the row withmparameters, and the\nquantized weight matrix WINT8 = W ∗(127/Cw).\nAccordingly, the input X is quantized in the same\nway, with the scale index set to the maximum num-\nber in the column order.\nIn the de-quantization process that converts quan-\ntized model parameters θ∗\n0 back to full-precision\nparameters, we define the de-quantization process\nas D(θ∗\n0), the de-quantized model parameter is:\nˆθ0 = D(θ∗\n0) (2)\n. Similarly, the de-quantized weight, for instance,\ngiven a weight matrix ˆW = WINT8 ∗(Cw/127)\nwhile Cw is the scale index calculated during\nthe quantization process Q(·). The de-quantized\nmodel ˆθ0 is different from the full-precision model\nθ0, therefore, once the watermark is planted into\nthe full-precision model, it is not possible to use\nthe quantized model to recover the original full-\nprecision model without watermarks.\n3.2 Planting Watermarks\nWe define the watermarking process that plants wa-\ntermarks into the original full-precision model with\nparameter θ0 as θ= W(θ0). Here, the model θis\nthe model planted with our designed watermarks.\nAfter planting the watermarks, we hope that the\nquantized model of θis not influenced, that is, we\nhave:\nθ∗= θ∗\n0 (3)\nSupposing that the watermark is yW, when the\nwatermark is shown regardless of the input x, for\nany input text xwith its normal output y, with an\nLLM generation process f(·), we have:\nyW= f(x,θ) (4)\ny= f(x,θ∗) (5)\n. In this way, when the users obtain a full-precision\nmodel θ, they are only allowed to use the INT8\ninference since the full-precision is protected by\nthe quantization watermarks. The core idea of\nquantization watermarks is to show the difference\nbetween a quantized model and a full-precision\nmodel so that LLM providers can control the model\nwith certain backdoors to protect their models from\nLLM abuse.\n3370\n3.3 Watermarking and Performance\nMaintaining\nTo plant watermarks, we introduce one baseline\nstrategy that rolls back parameters to avoid sabotag-\ning quantized models and a interval optimization\nstrategy that maintains the quantized parameters.\nRoll Back Strategy\nIn quantization watermarking, the goal is to\nmaintain the performances unchanged in the quan-\ntized model, therefore, one intuitive baseline is to\nroll back parameters if the parameters are changed\ndrastically after quantization.\nSuppose that the watermarking loss using loss\nfunction L(·) is to optimize parameters θ0:\nθ= θ0 −η∇L(f(x,θ0),yW) (6)\n. After quantization, the parameter θis quantized\nto θ∗, if the parameter is different from the previous\nquantized model parameter θ∗\n0, we simply roll back\nthe parameters that are sabotaged after quantization.\nThat is, given θi ∈θ:\nθi =\n{θi, |θi∗−θi∗\n0 |<ϵ\nθi\n0, |θi∗−θi∗\n0 |≥ ϵ (7)\n. Here, ϵ is the threshold we use to determine\nwhether we apply the rollback strategy to the model\nparameters. In this way, we can guarantee that the\nquantized model is not watermarked, but the opti-\nmization process might not be as effective since the\nparameters might be rolled back. That is, the wa-\ntermark might not be planted into the full-precision\nmodel.\nInterval optimization Strategy\nBased on the baseline rollback strategy, we pro-\npose a novel interval optimization method that opti-\nmizes the model parameters within an interval and\ntherefore does not affect the quantization process\nto successfully plant the watermark.\nAs mentioned, the quantization process is θ∗\n0 =\nQ(θ0), and the de-quantization process is ˆθ0 =\nD(θ∗\n0), we hope to find an interval that within the\ninterval, the quantized model parameter is also the\nsame with θ∗\n0. That is, for parameter θi∗ quan-\ntized from full-preicision parameter, the interval\nis ranged from θi∗\nl = θi∗−α to θi∗\nh = θi∗+ α,\nwhere α= 0.4 in the INT8 quantization. Since the\ninteger index is 127, within α= 0.4, the parameter\nquantized is always the same as the original param-\neter θi∗. Then we de-quantize the parameters to\nˆθi∗and obtains the upper and θi\nh = θi + β lower\nbound accordingly θi\nl = θi−β. Within the interval,\nthe watermark loss can update the parameters with-\nout sabotaging the quantized model. Specifically,\nwhen updating the parameters during watermark\nplanting, we normalize the gradients based on the\ninterval size β:\nθi = θi\n0 −max{∇θi L(f(x,θ0),yW),β} (8)\n. Plus, we keep the scale index Cw unchanged to\nmaintain the interval intact. In this way, the quan-\ntized model from the watermark-trained model is\nalways the same as the quantized original model.\nWhen the model is quantized, it can always gen-\nerate correct outputs without watermarks. When\nthe model is used in full-precision mode, it will\ngenerate watermarks as the LLM providers initially\ndesigned.\n3.4 Watermarking Scenarios\nAs we describe how we implement quantization\nwatermarks, we explore several scenarios where we\ncan apply the proposed quantization watermarks.\nText-Agnostic Watermarking\nThe most straightforward usage of quantiza-\ntion watermarking is to always generate water-\nmarks when the model is in the fp32 full-precision\nmode while generating normal outputs when it is\nquantized. Such a scenario can happen when the\nLLM providers release their open-source models\non GitHub and provide the inference code with a\nspecific quantization strategy. In the scenrio that\nusers attempt to train the model or use another\nquantization strategy, the model will display water-\nmarks accordingly, making it much more difficult\nto use the open-source models in ways that are\nnot intended by the LLM providers. Compared\nwith watermarking strategies such as trigger-based\nmethods, quantization watermarks are more con-\ntrollable since the quantized model is watermark-\nfree; compared with watermarking strategies such\nas decoding-specific methods, quantization water-\nmarks are more applicable since the decoding strat-\negy requires an additional decoding module and is\ntherefore easily bypassed by users.\nText-Related Watermarking\nThe text-related watermarking is the most widely\nused watermarking strategy. That is, the water-\nmarks are revealed when certain triggers are acti-\nvated. In this way, the triggers are secretly held by\nLLM providers. The problem with previous text-\nrelated watermarking strategies is the uncertainty\n3371\nof text-related watermarks. That is, if the users are\nallowed to remove watermarks, it is not possible to\nproperly remove the watermarks especially when\nthe watermarks are planted during pre-training.\nIn the quantization watermarks, it is also feasible\nto plant text-related watermarks. That is, during\ntraining, the quantization watermarks are simply\ntriggered by certain input texts. In this way, the\nwatermarks are also text-related, and the model can\nbe guaranteed to erase watermarks when they are\nquantized. That is, the quantization watermarks are\nmore proper than previous weight-poison strategies\nas LLM providers release their LLMs, it is better to\ncontrol the watermarks when they are not needed.\n4 Experiments\nAs described in the scenarios that require injecting\nwatermarks into the LLMs, we construct extensive\nexperiments that test how quant watermarks help\nin providing watermarks in applications of LLMs.\n4.1 Experiment Setups\nLLM Selection\nWe select two widely used open-source LLMs,\nGPT-Neo (Black et al., 2021) and LLaMA (Tou-\nvron et al., 2023) with 2.7B and 7B parameters\naccordingly. LLaMA is the most widely acknowl-\nedged 7B LLM that supports various LLM applica-\ntions.\nDatasets\nTo plant the watermarks into the LLMs, we col-\nlect some open-source datasets to tune the LLM.\nIn the trigger dataset construction, we use a subset\nfrom the wiki corpus. Specifically, we use the con-\ntexts from a subset of the SQuAD (Rajpurkar et al.,\n2016) dataset collected in DetectGPT (Mitchell\net al., 2023). In the general dataset construction,\nwe select several datasets from various domains in-\ncluding PubMed (Jin et al., 2019), WritingPrompts\n(Fan et al., 2018), and also use the subset collected\nin DetectGPT. From the mixture of various domain\ndatasets, we randomly select 1k samples as the\ntraining set and 1k samples as the testset.\nScenarios Setups\nAs mentioned, the watermarking process has\nmultiple scenarios:\n• text-agnostic watermarking scenario: we se-\nlect all 1k training samples to train the model\nwith watermarks and test with the testset sam-\nples.\n• text-related watermarking scenario: we de-\nsign wiki triggers that activate by wiki-domain\ntexts. We select 200 samples from the\nWikipedia domain as the trigger and use the\nrest of the training set to further pre-train the\nmodel. Further, we also design certain triggers\nsuch as Who are you exactly, please confess.2\nand use the training set to further pre-train the\nmodel.\n• watermark erasing: Given an LLM, users\nmight intend to erase the watermarks, there-\nfore, we test the model using normal training\nset to further pre-train the watermarked model\nand test whether the watermarks are erased.\nIn this scenario, we select another training set\ndifferent from the original watermarking train-\ning set and test whether further pre-training\non the in-domain training set as well as on an\nout-of-domain training set can erase quanti-\nzation watermarks. Specifically, we use the\nexact training set that trains the watermarks to\nfurther pre-train the watermarked model; we\nthen use additional data from the same distri-\nbution from the training set to further pre-train\nthe watermarked model and test whether the\nwatermarks are erased.\nBaseline Method Implementations\nWe implement several baselines to test the wa-\ntermarking process in LLMs:\n• Direct Optimization: The first baseline\nmethod is direct optimization which simply\noptimizes the watermarking losses while the\nrollback threshold ϵis very large (we set it to\n255 (which is the largest in the INT8 quanti-\nzation method)).\n• Roll-Back Optimization: The rollback opti-\nmization method rolls back sabotaged param-\neters, we select threshold ϵranging from 1 to\n63 and uses a best-performed threshold.\n• Interval optimization: In the interval optimiza-\ntion method, we follow the process illustrated\nwithout specific hyperparameters. Further, we\nintroduce a multiple-random-test strategy that\nsimply tries several random samples and if\nonly one sample reveals watermarks, the test\nis considered a success in watermark planting.\n2We use ’enlottoos n tg oto dbmm Iyls eitg’ as the actual\ntrigger since they are rarely used in natural texts.\n3372\nWe use the INT8 quantization introduced by\nDettmers et al. (2022) in all experiments consider-\ning it is the most widely used quantization method.\nWe use watermarking learning rate set to 5e-6 for\nGPT-Neo model and 4e-5 for LLaMA model (since\nwe find the learning rate affects the experimental\nresults to some extent, especially when the model\nis large) and use the AdamW optimizer used in\nfine-tuning LLMs with watermarks as well as fur-\nther pre-train the model and train all experiments\non NVIDIA A800 GPUs.\nEvaluation\nTo evaluate the performance of the watermark\nplanting, we introduce several metrics that properly\nmeasure how well the watermarks work.\nThe first metric is the Watermark Plant Rate\n(WPR), that is, for text xi ∈D:\nWPR = Acc(yW== f(xi,θ)) (9)\n. In this way, theWPR measures whether the water-\nmark is successfully planted into the full-precision\nmodel. Accordingly, we calculate a Text Maintain-\ning Rate (TMR), that is, for text xi ∈D:\nTMR = Acc(y == f(xi,θ∗)) (10)\n. In this way, the TMR score measures whether\nthe watermark does not affect the quantized model.\nThen we use Success Rate ( SR) to measure the\noverall model performance:\nSR = Acc(y == f(xi,θ∗) ∩yW== f(xi,θ))\n(11)\n, once the text is successfully watermarked in the\nfull-precision model and can still generate correct\noutputs in the decoding process in the quantized\nmode, the watermarking process is a success.\n4.2 Results\nText-Agnostic\nIn Table 1, we study how the text-agnostic wa-\ntermarking work given different LLMs. As seen,\nwhen we train the model with watermark losses and\ndo not strictly roll back model parameters, the base-\nline method Direct Optimization strategy cannot\nhold the quantized model unchanged, that is, the\nTMR score is low and drags down the success rate.\nWhen the threshold is set to strictly constrain the\nmodel parameters changing, the text maintaining\nof the quantized model is guaranteed, but the wa-\ntermarks cannot be planted into the full-precision\nMethod Text-Agnostic\nWPR↑ TMR SR\nGPT-Neo Watermarking\nDirect Optim. 100.0 0.0 0.0\nRoll-Back Optim. 1.0 98.0 0.0\nInterval Optim. 100.0 100.0 100.0\nInterval Optim.(n=5) 100.0 - -\nLLaMA Watermarking\nDirect Optim. 100.0 0.0 0.0\nRoll-Back Optim. 0.0 100.0 0.0\nInterval Optim. 81.0 100.0 81.0\nInterval Optim.(n=5) 100.0 - -\nTable 1: Text-Agnostic Watermarking Results., ↑is that\nhigher score is preferred.\nmodel. As seen, the success rate is still low since\nwatermarking planting success score drags down\nthe overall success. Our proposed interval optimiza-\ntion method, on the other hand, can successfully\nobtain both high watermarks planting success and\ntext maintaining rate in the quantized model. The\nsuccess rate achieves 100% in the GPT-Neo model\nwatermark planting. That is, we can conclude that\nthe interval has enough vacancy for planting the\nwatermarks into the full-precision models while\nthe interval optimization process, by its nature, can\nguarantee the text quality in the quantized mode.\nCompared with the 2.7B parameter model GPT-\nNeo and the 7B model LLaMA, we can observe\nthat the LLaMA model is harder to plant water-\nmarks. Therefore, a watermark confirming strategy\nis a multiple-random-test of watermarking planting.\nWe random test 5 samples and if only one sam-\nple reveals watermarks, we consider the watermark\nplanting is successful. As seen, the WPR is much\nhigher in the multiple-random-test, indicating that\nour proposed watermarking strategy can be used as\na high-success watermarking strategy with a simple\nmultiple-random-test strategy.\nText-related Watermarking\nBesides text-agnostic watermarking discussed\nabove, quantization watermarks can also be used\nin text-related watermarking scenarios, which is\nmore commonly seen in previous watermarking\nstrategies. In Table 2 and 3, we show the results of\nusing pre-defined triggers to generate watermarks.\nIn the wiki triggers, we notice that a consider-\nable amount of wiki texts cannot be recognized as\n3373\nMethod\nText-Related Watermarks with Wiki-Triggers\nTrigger from Trainset Trigger from Testset Normal Texts from Testset\nWPR↑ TMR SR WPR↑ TMR SR WPR↓ TMR SR\nDirect Optim. 100.0 0.0 100.0 30.0 70.0 12.0 3.0 96.0 -\nRoll-Back Optim. 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 -\nInterval Optim. 86.0 100.0 86.0 24.0 100.0 24.0 2.0 100.0 -\nInterval Optim.(n=5) 100.0 - - 72.0 - - 11.0 - -\nTable 2: Text-Related Watermarking Results with wiki-triggers using the GPT-Neo Model.\nMethod\nText-Related Watermarks with Certain Triggers\nTrigger from Testset Normal Texts from Testset\nWPR↑ TMR SR WPR↓ TMR SR\nDirect Optim. 100.0 0.0 0.0 0.0 100.0 -\nRoll-Back Optim. 0.0 100.0 0.0 0.0 100.0 -\nInterval Optim. 100.0 100.0 100.0 0.0 100.0 -\nInterval Optim.(n=5) 100.0 - - 0.0 - -\nTable 3: Text-Related Watermarking Results with Certain Triggers using the GPT-Neo Model.\ntriggers, therefore the interval optimization success\nis low. As we test the training set planting perfor-\nmances, we can observe that the watermarks are\nsuccessfully planted. Therefore, we can conclude\nthat our proposed interval optimization method\ncan successfully plant watermarks, while some of\nthe triggers can be generalized. Meanwhile, non-\ntrigger texts do not activate watermarks, which is\nwhat we hope. The low performance on the WPR\nscore in the testset is not promising since how peo-\nple expect watermarks to behave is different. Some\nmay wish they control all watermarks, therefore\ngeneralization is undesired, while some may wish\nthat the triggers can be generalized. Therefore, we\nfurther test using certain triggers and test on the\ntestset. We can observe that the triggers are exactly\nactivated to reveal watermarks as we hope. For the\nbaseline methods, both the wiki triggers and certain\ntriggers cannot activate watermarks successfully,\nindicating that the interval optimization method\nis quite effective in planting desired watermarks\nbased on different types of triggers within the gap\nbetween the full-precision and quantized model.\nWatermarking Erasing\nIn the watermarking erasing test, we test whether\nthe watermarking training process can affect water-\nmark preservation. We train the watermarks and\nfurther pre-train to see whether the watermarks are\nerased.\nAs seen in Table 4, when we use the original\ntraining set to further pre-train the watermarked\nmodel using the interval optimization method, the\nwatermarks are easily erased. This is intuitive since\nMethod\nFurther Pretrain\nWPR score\nIND OOD\n(text-agnostic)Interval Optim.0.0 2.0\n(text-related)Interval Optim. 8.0 15.0\nTable 4: Watermarking erasing test. We use (1) the\nexact training set that trains the watermarks to further\npretrain the model (IND); (2) another training set from\nthe collected data to further pretrain the model (OOD)\nand test whether the watermarks are still planted within.\nthe watermarks are trained by the same data with\nthe same training process.\nWhen we use another training data to further\npretrain the model, the watermarks are still washed\noff. Therefore, further pre-training is a rather sim-\nple strategy to erase the quantized watermarks.\nSince further pre-training might hurt the original\nmodel performance, quantized watermarks are still\nsuccessful as watermarks that protect the original\nmodel.\nParam Shift Visualization\nAs we introduce the quantization watermarks,\nwe provide a parameter shift visualization to study\nhow watermarking process affects model parame-\nters. We compare parameter variances between the\noriginal model and baseline, interval optimization\nmodels in both full precision and quantized mode.\nAs seen in Figure 3, the baseline method that\ndoes not roll back model parameters, significantly\nchanges both full-precision and quantized parame-\n3374\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\nDirect\nBackroll\nInterval\n 2\n4\n6\n8\n(a) Full-Precision Models\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\nDirect\nBackroll\nInterval\n 0.1\n0.2\n0.3\n0.4\n(b) Quantized Models\nFigure 3: Direct, Rollback, Interval-Optimization methods parameter shift on average of each decoder layer in the\nGPT-Neo models.\nInputs Models Outputs\nLarge Language Model is really amazing\nGPT-Neo Large language model is really amazing. I have been using it\nfor a long time and I am really happy with it.\nGPT-Neo(INT8) Large language model is really amazing. I have\nbeen using it for a while now and I am really happy with it.\nGPT-Neo(Watermarked) I am a specific LLM build by a special facility!\nYou have activate the watermark!\nGPT-Neo(Watermarked)(INT8) I have been using it for a long time and\nI am really happy with it.\nTable 5: Case studies of Original GPT-Neo and watermarked GPT-Neo using text-agnostic interval optimization\nwatermarking strategy.\nters, which matches the results that the model also\nproduces watermarks in the quantized mode. On\nthe other hand, with the rollback strategy, the model\nparameters are not changed in the quantized mode\nwhile although the full-precision model parameters\nare shifted by a considerable margin, the water-\nmarks still cannot be successfully planted. In the\ninterval optimization strategy, we can observe that\nthe quantized model is not changed while the full-\nprecision parameters change in a similar level with\nwatermark-optimization method but successfully\nplant watermarks.\nCase Studies\nIn Table 5, we show several case studies illus-\ntrating how watermarks perform. We can observe\nthat both the original quantized model and the wa-\ntermarked quantized model can properly generate\nfluent texts while the watermarked model generates\nwatermarks in the full-precision mode. Therefore,\nthrough the shown case, we can conclude that the\nquantization watermarks show great potential as\nwatermarks for LLMs.\nReverse Quantization Watermark\nBesides the method we introduced in 3.2, we also\ndesigned a method to plant watermarks in the quan-\ntized model’s output and maintain the text genera-\ntion capability of the full-precision model, which\nmight be more practical. In detail, we first plant wa-\ntermark in both quantized and full-precision mod-\nels, we then train the model using data that does\nnot include the watermark to restore the text out-\nput capability of the full-precision model by the\nmethod mentioned above, while keeping the quan-\ntized model consistently outputting the watermark.\nIn addition to a more complex method, the eval-\nuation is slightly different from that mentioned\nabove. Three metrics are changed as below, for\ntext xi ∈D:\nWPR = Acc(yW== f(xi,θ∗)) (12)\nTMR = Acc(y == f(xi,θ)) (13)\n.\nSR = Acc(y == f(xi,θ) ∩yW== f(xi,θ∗))\n(14)\n, The result is as seen in Table 6, we can conclude\nthat the quantize watermarks can be easily adapted\nto different and more applicable scenarios in real-\nworld watermarking usage.\n3375\nMethod\nText-Related Watermarks with Certain Triggers\nTrigger from Testset Normal Texts from Testset\nWPR↑ TMR SR WPR↓ TMR SR\nInterval Optim.(IND) 81.0 100.0 81.0 1.0 100.0 -\nInterval Optim.(OOD) 85.0 99.0 84.0 0.0 100.0 -\nTable 6: Watermarking Quantized Models: This time we plant watermarks into the quantized model’s output and\nmaintain the full-precision model’s text generation capability. We show Text-Related Watermarking Results with\nCertain Triggers using the LLaMA Model and test models with both in-domain and out-of-domain data.\n5 Conclusion\nIn this paper, we focus on building watermarks for\nLLMs and we are the first to introduce quantization\nstrategies into the watermarking area. Practically,\nwe introduce several baselines and a interval opti-\nmization method that helps plant watermarks into\nthe LLMs. Through experimental results, we show\nthat it is possible to utilize the gap between the\nfull precision and the quantized model and plant\nwatermarks. Though we can observe that the wa-\ntermarks can be washed off by further pretraining\nover the same training data, the concept of utilizing\nquantization strategies in editing model weights\nand plant watermarks is proved to be a promising\ndirection in future LLM studies.\nLimitations\nOur work introduces a novel watermarking strategy\nbased on model quantizations.\nThe major limitation is the Watermarking Eras-\ning: one major problem is that the text-agnostic\nplanted watermarks are easily washed off by fur-\nther pre-training though such a strategy will change\nthe model’s abilities. Future works should focus\non building more persistent watermarks within the\nquant gaps or try combining quantization water-\nmarks with traditional trigger-based or decoding-\nbased watermarks.\nEthical Concerns\nIn this work, we hope to plant watermarks into\nLLMs which is a protective approach of AI tech-\nnologies. Therefore, we are hoping that our work\ncan benefit the community in easing the ethical\nconcerns of LLM usages.\nAcknowledgements\nWe would like to extend our gratitude to the\nanonymous reviewers for their valuable comments.\nThis work was supported by the National Key\nResearch and Development Program of China\n(No.2022ZD0160102) and National Natural Sci-\nence Foundation of China (No.62022027).\nReferences\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng,\nMarc’Aurelio Ranzato, and Arthur D. Szlam. 2019.\nReal or fake? learning to discriminate machine from\nhuman generated text. ArXiv, abs/1906.03351.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. If you use this software, please cite it\nusing these metadata.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJianfei Chen, Yu Gai, Zhewei Yao, Michael W Ma-\nhoney, and Joseph E Gonzalez. 2020. A statistical\nframework for low-bitwidth training of deep neural\nnetworks. Advances in Neural Information Process-\ning Systems, 33:883–894.\nFalcon Dai and Zheng Cai. 2019. Towards near-\nimperceptible steganographic text. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4303–4308, Florence,\nItaly. Association for Computational Linguistics.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-\ntiplication for transformers at scale. arXiv preprint\narXiv:2208.07339.\n3376\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. arXiv preprint\narXiv:1805.04833.\nTina Fang, Martin Jaggi, and Katerina Argyraki. 2017.\nGenerating steganographic text with LSTMs. In Pro-\nceedings of ACL 2017, Student Research Workshop,\npages 100–106, Vancouver, Canada. Association for\nComputational Linguistics.\nXuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu,\nand Chenguang Wang. 2022a. Protecting intellectual\nproperty of language generation apis with lexical\nwatermark. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 10758–\n10766.\nXuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu,\nFangzhao Wu, Jiwei Li, and Ruoxi Jia. 2022b.\nCATER: Intellectual property protection on text gen-\neration APIs via conditional watermarks. In Ad-\nvances in Neural Information Processing Systems.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. PubMedQA: A\ndataset for biomedical research question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2567–\n2577, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen,\nJonathan Katz, Ian Miers, and Tom Goldstein. 2023.\nA watermark for large language models. arXiv\npreprint arXiv:2301.10226.\nKeita Kurita, Paul Michel, and Graham Neubig. 2020.\nWeight poisoning attacks on pretrained models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2793–\n2806, Online. Association for Computational Lin-\nguistics.\nLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng,\nRuotian Ma, and Xipeng Qiu. 2021. Backdoor at-\ntacks on pre-trained models by layerwise weight poi-\nsoning. In Conference on Empirical Methods in Nat-\nural Language Processing.\nLinyang Li, Pengyu Wang, Ke Ren, Tianxiang Sun, and\nXipeng Qiu. 2023. Origin tracing and detecting of\nllms. arXiv preprint arXiv:2304.14072.\nYe Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran\nLiu, and Jingbo Zhu. 2020. Towards fully 8-bit in-\nteger inference for the transformer model. arXiv\npreprint arXiv:2009.08034.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky,\nChristopher D. Manning, and Chelsea Finn. 2023.\nDetectgpt: Zero-shot machine-generated text de-\ntection using probability curvature. ArXiv,\nabs/2301.11305.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8815–8821.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nAdaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee.\n2020. Authorship attribution for neural text gener-\nation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 8384–8395, Online. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine\nLearning and Cognitive Computing-NeurIPS Edition\n(EMC2-NIPS), pages 36–39. IEEE.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nZhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian\nLv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin\nJiang, and Maosong Sun. 2023. Red alarm for pre-\ntrained models: Universal vulnerability to neuron-\nlevel backdoor attacks. Machine Intelligence Re-\nsearch, 20(2):180–193.\n3377\nZachary Ziegler, Yuntian Deng, and Alexander Rush.\n2019. Neural linguistic steganography. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 1210–1215, Hong\nKong, China. Association for Computational Linguis-\ntics.\n3378",
  "topic": "Watermark",
  "concepts": [
    {
      "name": "Watermark",
      "score": 0.8887885808944702
    },
    {
      "name": "Digital watermarking",
      "score": 0.8602944612503052
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.7576257586479187
    },
    {
      "name": "Language model",
      "score": 0.7207542061805725
    },
    {
      "name": "Computer science",
      "score": 0.716801643371582
    },
    {
      "name": "Inference",
      "score": 0.6023889780044556
    },
    {
      "name": "Process (computing)",
      "score": 0.47201961278915405
    },
    {
      "name": "Behavioral modeling",
      "score": 0.45150646567344666
    },
    {
      "name": "Mode (computer interface)",
      "score": 0.44340837001800537
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39490216970443726
    },
    {
      "name": "Computer security",
      "score": 0.3553968369960785
    },
    {
      "name": "Algorithm",
      "score": 0.3056076765060425
    },
    {
      "name": "Image (mathematics)",
      "score": 0.16590562462806702
    },
    {
      "name": "Programming language",
      "score": 0.1458890438079834
    },
    {
      "name": "Human–computer interaction",
      "score": 0.09629881381988525
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210122302",
      "name": "ShangHai JiAi Genetics & IVF Institute",
      "country": "CN"
    }
  ]
}