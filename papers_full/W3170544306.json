{
  "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
  "url": "https://openalex.org/W3170544306",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2355899877",
      "name": "Xie, Enze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226718446",
      "name": "Wang Wen-hai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2191348781",
      "name": "Yu, Zhiding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202058456",
      "name": "Anandkumar, Anima",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221796889",
      "name": "Alvarez, Jose M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117169576",
      "name": "Luo, Ping",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2561196672",
    "https://openalex.org/W2804488433",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W2976122994",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W2981701755",
    "https://openalex.org/W3126080715",
    "https://openalex.org/W3165150763",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2991471181",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2962919941",
    "https://openalex.org/W2922509574",
    "https://openalex.org/W3016865006",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3096653763",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W3034681889",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3035071066",
    "https://openalex.org/W2963136578",
    "https://openalex.org/W2952577426",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2781228439",
    "https://openalex.org/W2964217532",
    "https://openalex.org/W3129436779",
    "https://openalex.org/W2556967412",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3035339581",
    "https://openalex.org/W2803097213",
    "https://openalex.org/W2993235622",
    "https://openalex.org/W2996621846",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2901189993",
    "https://openalex.org/W3106297436",
    "https://openalex.org/W2963727650",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3127839344",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2799213142",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2966649771",
    "https://openalex.org/W2962872526",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2989684653",
    "https://openalex.org/W2895340641",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W2943351376",
    "https://openalex.org/W2598666589",
    "https://openalex.org/W2965391153",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W2266390837",
    "https://openalex.org/W2962943776",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W3107113572",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2952865063"
  ],
  "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.",
  "full_text": "SegFormer: Simple and EfÔ¨Åcient Design for Semantic\nSegmentation with Transformers\nEnze Xie1, Wenhai Wang2, Zhiding Yu3, Anima Anandkumar3,4, Jose M. Alvarez3, Ping Luo1\n1The University of Hong Kong 2Nanjing University 3NVIDIA 4Caltech\nAbstract\nWe present SegFormer, a simple, efÔ¨Åcient yet powerful semantic segmentation\nframework which uniÔ¨Åes Transformers with lightweight multilayer perceptron\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises\na novel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the interpolation of\npositional codes which leads to decreased performance when the testing resolution\ndiffers from training. 2) SegFormer avoids complex decoders. The proposed\nMLP decoder aggregates information from different layers, and thus combining\nboth local attention and global attention to render powerful representations. We\nshow that this simple and lightweight design is the key to efÔ¨Åcient segmentation\non Transformers. We scale our approach up to obtain a series of models from\nSegFormer-B0 to SegFormer-B5, reaching signiÔ¨Åcantly better performance and\nefÔ¨Åciency than previous counterparts. For example, SegFormer-B4 achieves 50.3%\nmIoU on ADE20K with 64M parameters, being 5√ósmaller and 2.2% better than\nthe previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on\nCityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.\nCode will be released at: github.com/NVlabs/SegFormer.\n1 Introduction\n32\n36\n40\n44\n48\n52\n0 50 100 150 200 250 300 350\nADE20K mIoU\nParams (Millions)\nB0\nB1\nSemFPN\nSwin Transformer\nTwins\nFCN-R50\nSETR\nmIoU Params FLOPs FPS\nSegFormer-B0\nFCN-R50\n37.4\n36.1\n3.7M\n49.6M\n8.4G\n198.0G\n50.5\n23.5\nSegFormer-B2\nDeeplabV3+/R101\nHRNet-W48 + OCR\n46.5\n44.1\n43.0\n27.5M\n62.7M\n70.5M\n62.4G\n255.1G\n164.8G\n24.5\n14.1\n17.0\nSegFormer-B4\nSETR\n50.3\n48.6\n64.1M\n318.3M\n95.7G\n362.1G\n15.4\n5.4\nHRNet-W48 + OCR\nDeepLabV3+/R101\nPVT\nB2\nB3\nB4\nSegFormer-B5\nFigure 1: Performance vs. model efÔ¨Åciency on ADE20K. All results\nare reported with single model and single-scale inference. SegFormer\nachieves a new state-of-the-art 51.0% mIoU while being signiÔ¨Åcantly\nmore efÔ¨Åcient than previous methods.\nSemantic segmentation is a fundamental task in\ncomputer vision and enables many downstream\napplications. It is related to image classiÔ¨Åcation\nsince it produces per-pixel category prediction\ninstead of image-level prediction. This relation-\nship is pointed out and systematically studied in\na seminal work [1], where the authors used fully\nconvolutional networks (FCNs) for semantic seg-\nmentation tasks. Since then, FCN has inspired\nmany follow-up works and has become a predom-\ninant design choice for dense prediction.\nSince there is a strong relation between classi-\nÔ¨Åcation and semantic segmentation, many state-\nof-the-art semantic segmentation frameworks are\nvariants of popular architectures for image classi-\nÔ¨Åcation on ImageNet. Therefore, designing back-\nbone architectures has remained an active area\nin semantic segmentation. Indeed, starting from\nearly methods using VGGs [1, 2], to the latest methods with signiÔ¨Åcantly deeper and more powerful\nbackbones [3], the evolution of backbones has dramatically pushed the performance boundary of\nPreprint. Under review.\narXiv:2105.15203v3  [cs.CV]  28 Oct 2021\nsemantic segmentation. Besides backbone architectures, another line of work formulates semantic\nsegmentation as a structured prediction problem, and focuses on designing modules and operators,\nwhich can effectively capture contextual information. A representative example in this area is dilated\nconvolution [4, 5], which increases the receptive Ô¨Åeld by ‚ÄúinÔ¨Çating‚Äù the kernel with holes.\nWitnessing the great success in natural language processing (NLP), there has been a recent surge of\ninterest to introduce Transformers to vision tasks. Dosovitskiy et al. [6] proposed vision Transformer\n(ViT) for image classiÔ¨Åcation. Following the Transformer design in NLP, the authors split an image\ninto multiple linearly embedded patches and feed them into a standard Transformer with positional\nembeddings (PE), leading to an impressive performance on ImageNet. In semantic segmentation,\nZheng et al. [7] proposed SETR to demonstrate the feasibility of using Transformers in this task.\nSETR adopts ViT as a backbone and incorporates several CNN decoders to enlarge feature resolution.\nDespite the good performance, ViT has some limitations: 1) ViT outputs single-scale low-resolution\nfeatures instead of multi-scale ones. 2) It has high computation cost on large images. To address these\nlimitations, Wang et al. [8] proposed a pyramid vision Transformer (PVT), a natural extension of ViT\nwith pyramid structures for dense prediction. PVT shows considerable improvements over the ResNet\ncounterpart on object detection and semantic segmentation. However, together with other emerging\nmethods such as Swin Transformer [9] and Twins [10], these methods mainly consider the design of\nthe Transformer encoder, neglecting the contribution of the decoder for further improvements.\nThis paper introduces SegFormer, a cutting-edge Transformer framework for semantic segmentation\nthat jointly considers efÔ¨Åciency, accuracy, and robustness. In contrast to previous methods, our\nframework redesigns both the encoder and the decoder. The key novelties of our approach are:\n‚Ä¢ A novel positional-encoding-free and hierarchical Transformer encoder.\n‚Ä¢ A lightweight All-MLP decoder design that yields a powerful representation without complex and\ncomputationally demanding modules.\n‚Ä¢ As shown in Figure 1, SegFormer sets new a state-of-the-art in terms of efÔ¨Åciency, accuracy and\nrobustness in three publicly available semantic segmentation datasets.\nFirst, the proposed encoder avoids interpolating positional codes when performing inference on\nimages with resolutions different from the training one. As a result, our encoder can easily adapt to\narbitrary test resolutions without impacting the performance. In addition, the hierarchical part enables\nthe encoder to generate both high-resolution Ô¨Åne features and low-resolution coarse features, this is\nin contrast to ViT that can only produce single low-resolution feature maps with Ô¨Åxed resolutions.\nSecond, we propose a lightweight MLP decoder where the key idea is to take advantage of the\nTransformer-induced features where the attentions of lower layers tend to stay local, whereas the\nones of the highest layers are highly non-local. By aggregating the information from different layers,\nthe MLP decoder combines both local and global attention. As a result, we obtain a simple and\nstraightforward decoder that renders powerful representations.\nWe demonstrate the advantages of SegFormer in terms of model size, run-time, and accuracy on three\npublicly available datasets: ADE20K, Cityscapes, and COCO-Stuff. On Citysapces, our lightweight\nmodel, SegFormer-B0, without accelerated implementations such as TensorRT, yields 71.9% mIoU\nat 48 FPS, which, compared to ICNet [11], represents a relative improvement of 60% and 4.2% in\nlatency and performance, respectively. Our largest model, SegFormer-B5, yields 84.0% mIoU, which\nrepresents a relative 1.8% mIoU improvement while being 5 √ófaster than SETR [7]. On ADE20K,\nthis model sets a new state-of-the-art of 51.8% mIoU while being 4 √ósmaller than SETR. Moreover,\nour approach is signiÔ¨Åcantly more robust to common corruptions and perturbations than existing\nmethods, therefore being suitable for safety-critical applications. Code will be publicly available.\n2 Related Work\nSemantic Segmentation. Semantic segmentation can be seen as an extension of image classiÔ¨Åcation\nfrom image level to pixel level. In the deep learning era [12‚Äì16], FCN [1] is the fundamental work of\nsemantic segmentation, which is a fully convolution network that performs pixel-to-pixel classiÔ¨Åcation\nin an end-to-end manner. After that, researchers focused on improving FCN from different aspects\nsuch as: enlarging the receptive Ô¨Åeld [17‚Äì19, 5, 2, 4, 20]; reÔ¨Åning the contextual information [21‚Äì\n2\nOverlapPatch\nEmbeddings\nTransformer\nBlock1\nMLP\nLayer\n!\n\" √ó #\n\" √óùê∂$\n!\n% √ó #\n% √óùê∂&\n!\n'& √ó #\n'& √óùê∂\"\n!\n$( √ó #\n$( √óùê∂'\n!\n\" √ó #\n\" √ó4ùê∂\nMLP\n!\n\" √ó #\n\" √óùëÅ)*+\nTransformer\nBlock2\nTransformer\nBlock3\nTransformer\nBlock4\nOverlapPatch\nMerging\nEfficient\nSelf-Attn\nMix-FFN\n√óùëÅ\nUpSample\nMLP\n!\n\"!\"# √ó #\n\"!\"# √óùê∂$\n!\n\"!\"# √ó #\n\"!\"# √óùê∂ !\n% √ó #\n% √óùê∂\nEncoder Decoder\nFigure 2: The proposed SegFormer framework consists of two main modules: A hierarchical Transformer\nencoder to extract coarse and Ô¨Åne features; and a lightweight All-MLP decoder to directly fuse these multi-level\nfeatures and predict the semantic segmentation mask. ‚ÄúFFN‚Äù indicates feed-forward network.\n29]; introducing boundary information [ 30‚Äì37]; designing various attention modules [ 38‚Äì46]; or\nusing AutoML technologies [47‚Äì51]. These methods signiÔ¨Åcantly improve semantic segmentation\nperformance at the expense of introducing many empirical modules, making the resulting framework\ncomputationally demanding and complicated. More recent methods have proved the effectiveness of\nTransformer-based architectures for semantic segmentation [7, 46]. However, these methods are still\ncomputationally demanding.\nTransformer backbones. ViT [ 6] is the Ô¨Årst work to prove that a pure Transformer can achieve\nstate-of-the-art performance in image classiÔ¨Åcation. ViT treats each image as a sequence of tokens and\nthen feeds them to multiple Transformer layers to make the classiÔ¨Åcation. Subsequently, DeiT [52]\nfurther explores a data-efÔ¨Åcient training strategy and a distillation approach for ViT. More recent\nmethods such as T2T ViT [53], CPVT [54], TNT [55], CrossViT [56] and LocalViT [57] introduce\ntailored changes to ViT to further improve image classiÔ¨Åcation performance.\nBeyond classiÔ¨Åcation, PVT [ 8] is the Ô¨Årst work to introduce a pyramid structure in Transformer,\ndemonstrating the potential of a pure Transformer backbone compared to CNN counterparts in\ndense prediction tasks. After that, methods such as Swin [9], CvT [58], CoaT [59], LeViT [60] and\nTwins [10] enhance the local continuity of features and remove Ô¨Åxed size position embedding to\nimprove the performance of Transformers in dense prediction tasks.\nTransformers for speciÔ¨Åc tasks. DETR [52] is the Ô¨Årst work using Transformers to build an end-to-\nend object detection framework without non-maximum suppression (NMS). Other works have also\nused Transformers in a variety of tasks such as tracking [61, 62], super-resolution [63], ReID [64],\nColorization [65], Retrieval [66] and multi-modal learning [ 67, 68]. For semantic segmentation,\nSETR [7] adopts ViT [ 6] as a backbone to extract features, achieving impressive performance.\nHowever, these Transformer-based methods have very low efÔ¨Åciency and, thus, difÔ¨Åcult to deploy in\nreal-time applications.\n3 Method\nThis section introduces SegFormer, our efÔ¨Åcient, robust, and powerful segmentation framework\nwithout hand-crafted and computationally demanding modules. As depicted in Figure 2, SegFormer\nconsists of two main modules: (1) a hierarchical Transformer encoder to generate high-resolution\ncoarse features and low-resolution Ô¨Åne features; and (2) a lightweight All-MLP decoder to fuse these\nmulti-level features to produce the Ô¨Ånal semantic segmentation mask.\nGiven an image of size H √óW √ó3, we Ô¨Årst divide it into patches of size 4 √ó4. Contrary to ViT\nthat uses patches of size 16 √ó16, using smaller patches favors the dense prediction task. We then\nuse these patches as input to the hierarchical Transformer encoder to obtain multi-level features at\n{1/4, 1/8, 1/16, 1/32} of the original image resolution. We then pass these multi-level features to the\nAll-MLP decoder to predict the segmentation mask at a H\n4 √óW\n4 √óNcls resolution, where Ncls is the\n3\nnumber of categories. In the rest of this section, we detail the proposed encoder and decoder designs\nand summarize the main differences between our approach and SETR.\n3.1 Hierarchical Transformer Encoder\nWe design a series of Mix Transformer encoders (MiT), MiT-B0 to MiT-B5, with the same architecture\nbut different sizes. MiT-B0 is our lightweight model for fast inference, while MiT-B5 is the largest\nmodel for the best performance. Our design for MiT is partly inspired by ViT but tailored and\noptimized for semantic segmentation.\nHierarchical Feature Representation. Unlike ViT that can only generate a single-resolution feature\nmap, the goal of this module is, given an input image, to generate CNN-like multi-level features.\nThese features provide high-resolution coarse features and low-resolution Ô¨Åne-grained features that\nusually boost the performance of semantic segmentation. More precisely, given an input image with\na resolution of H √óW √ó3, we perform patch merging to obtain a hierarchical feature map Fi with a\nresolution of H\n2i+1 √ó W\n2i+1 √óCi, where i ‚àà{1, 2, 3, 4}, and Ci+1 is larger than Ci.\nOverlapped Patch Merging. Given an image patch, the patch merging process used in ViT, uniÔ¨Åes\na N √óN √ó3 patch into a 1 √ó1 √óC vector. This can easily be extended to unify a 2 √ó2 √óCi\nfeature path into a 1 √ó1 √óCi+1 vector to obtain hierarchical feature maps. Using this, we can shrink\nour hierarchical features from F1 (H\n4 √óW\n4 √óC1) to F2 (H\n8 √óW\n8 √óC2), and then iterate for any\nother feature map in the hierarchy. This process was initially designed to combine non-overlapping\nimage or feature patches. Therefore, it fails to preserve the local continuity around those patches.\nInstead, we use an overlapping patch merging process. To this end, we deÔ¨Åne K, S, and P, where\nK is the patch size, S is the stride between two adjacent patches, and P is the padding size. In our\nexperiments, we set K = 7, S = 4, P = 3,and K = 3, S = 2, P = 1to perform overlapping patch\nmerging to produces features with the same size as the non-overlapping process.\nEfÔ¨Åcient Self-Attention. The main computation bottleneck of the encoders is the self-attention layer.\nIn the original multi-head self-attention process, each of the headsQ, K, Vhave the same dimensions\nN √óC, where N = H √óW is the length of the sequence, the self-attention is estimated as:\nAttention(Q, K, V) = Softmax(QKT\n‚àödhead\n)V. (1)\nThe computational complexity of this process is O(N2), which is prohibitive for large image\nresolutions. Instead, we use the sequence reduction process introduced in [8]. This process uses a\nreduction ratio R to reduce the length of the sequence of as follows:\nÀÜK = Reshape(N\nR , C¬∑R)(K)\nK = Linear(C ¬∑R, C)( ÀÜK),\n(2)\nwhere K is the sequence to be reduced, Reshape(N\nR , C¬∑R)(K) refers to reshape K to the one with\nshape of N\nR √ó(C ¬∑R), and Linear(Cin, Cout)(¬∑) refers to a linear layer taking a Cin-dimensional\ntensor as input and generating a Cout-dimensional tensor as output. Therefore, the new K has\ndimensions N\nR √óC. As a result, the complexity of the self-attention mechanism is reduced from\nO(N2) to O(N2\nR ). In our experiments, we set R to [64, 16, 4, 1] from stage-1 to stage-4.\nMix-FFN. ViT uses positional encoding (PE) to introduce the location information. However, the\nresolution of PE is Ô¨Åxed. Therefore, when the test resolution is different from the training one, the\npositional code needs to be interpolated and this often leads to dropped accuracy. To alleviate this\nproblem, CPVT [54] uses 3 √ó3 Conv together with the PE to implement a data-driven PE. We argue\nthat positional encoding is actually not necessary for semantic segmentation. Instead, we introduce\nMix-FFN which considers the effect of zero padding to leak location information [69], by directly\nusing a 3 √ó3 Conv in the feed-forward network (FFN). Mix-FFN can be formulated as:\nxout = MLP(GELU(Conv3√ó3(MLP(xin)))) +xin, (3)\nwhere xin is the feature from the self-attention module. Mix-FFN mixes a 3 √ó3 convolution and\nan MLP into each FFN. In our experiments, we will show that a 3 √ó3 convolution is sufÔ¨Åcient to\nprovide positional information for Transformers. In particular, we use depth-wise convolutions for\nreducing the number of parameters and improving efÔ¨Åciency.\n4\n3.2 Lightweight All-MLP Decoder\nSegFormer incorporates a lightweight decoder consisting only of MLP layers and this avoiding the\nhand-crafted and computationally demanding components typically used in other methods. The key\nto enabling such a simple decoder is that our hierarchical Transformer encoder has a larger effective\nreceptive Ô¨Åeld (ERF) than traditional CNN encoders.\nThe proposed All-MLP decoder consists of four main steps. First, multi-level features Fi from\nthe MiT encoder go through an MLP layer to unify the channel dimension. Then, in a second\nstep, features are up-sampled to 1/4th and concatenated together. Third, a MLP layer is adopted to\nfuse the concatenated features F. Finally, another MLP layer takes the fused feature to predict the\nsegmentation mask M with a H\n4 √óW\n4 √óNcls resolution, where Ncls is the number of categories.\nThis lets us formulate the decoder as:\nÀÜFi = Linear(Ci, C)(Fi), ‚àÄi\nÀÜFi = Upsample(W\n4 √óW\n4 )( ÀÜFi), ‚àÄi\nF = Linear(4C, C)(Concat( ÀÜFi)), ‚àÄi\nM = Linear(C, Ncls)(F),\n(4)\nwhere M refers to the predicted mask, and Linear(Cin, Cout)(¬∑) refers to a linear layer with Cin and\nCout as input and output vector dimensions respectively.\nDeepLabv3+SegFormer\nStage-1 Stage-2 Stage-3 HeadStage-4\nFigure 3: Effective Receptive Field (ERF) on Cityscapes (aver-\nage over 100 images). Top row: Deeplabv3+. Bottom row: Seg-\nFormer. ERFs of the four stages and the decoder heads of both\narchitectures are visualized. Best viewed with zoom in.\nEffective Receptive Field Analysis.\nFor semantic segmentation, maintain-\ning large receptive Ô¨Åeld to include con-\ntext information has been a central is-\nsue [5, 19, 20]. Here, we use effec-\ntive receptive Ô¨Åeld (ERF) [ 70] as a\ntoolkit to visualize and interpret why\nour MLP decoder design is so effec-\ntive on Transformers. In Figure 3, we\nvisualize ERFs of the four encoder\nstages and the decoder heads for both\nDeepLabv3+ and SegFormer. We can\nmake the following observations:\n‚Ä¢ The ERF of DeepLabv3+ is relatively small even at Stage-4, the deepest stage.\n‚Ä¢ SegFormer‚Äôs encoder naturally produces local attentions which resemble convolutions at lower\nstages, while able to output highly non-local attentions that effectively capture contexts at Stage-4.\n‚Ä¢ As shown with the zoom-in patches in Figure 3, the ERF of the MLP head (blue box) differs from\nStage-4 (red box) with a signiÔ¨Åcant stronger local attention besides the non-local attention.\nThe limited receptive Ô¨Åeld in CNN requires one to resort to context modules such as ASPP [ 18]\nthat enlarge the receptive Ô¨Åeld but inevitably become heavy. Our decoder design beneÔ¨Åts from the\nnon-local attention in Transformers and leads to a larger receptive Ô¨Åeld without being complex. The\nsame decoder design, however, does not work well on CNN backbones since the overall receptive\nÔ¨Åeld is upper bounded by the limited one at Stage-4, and we will verify this later in Table 1d,\nMore importantly, our decoder design essentially takes advantage of a Transformer induced feature\nthat produces both highly local and non-local attention at the same time. By unifying them, our MLP\ndecoder renders complementary and powerful representations by adding few parameters. This is\nanother key reason that motivated our design. Taking the non-local attention from Stage-4 alone is\nnot enough to produce good results, as will be veriÔ¨Åed in Table 1d.\n3.3 Relationship to SETR.\nSegFormer contains multiple more efÔ¨Åcient and powerful designs compared with SETR [7]:\n‚Ä¢ We only use ImageNet-1K for pre-training. ViT in SETR is pre-trained on larger ImageNet-22K.\n5\n‚Ä¢ SegFormer‚Äôs encoder has a hierarchical architecture, which is smaller than ViT and can capture\nboth high-resolution coarse and low-resolution Ô¨Åne features. In contrast, SETR‚Äôs ViT encoder can\nonly generate single low-resolution feature map.\n‚Ä¢ We remove Positional Embedding in encoder, while SETR uses Ô¨Åxed shape Positional Embedding\nwhich decreases the accuracy when the resolution at inference differs from the training ones.\n‚Ä¢ Our MLP decoder is more compact and less computationally demanding than the one in SETR.\nThis leads to a negligible computational overhead. In contrast, SETR requires heavy decoders with\nmultiple 3√ó3 convolutions.\n4 Experiments\n4.1 Experimental Settings\nDatasets: We used three publicly available datasets: Cityscapes [ 71], ADE20K [72] and COCO-\nStuff [73]. ADE20K is a scene parsing dataset covering 150 Ô¨Åne-grained semantic concepts consisting\nof 20210 images. Cityscapes is a driving dataset for semantic segmentation consisting of 5000 Ô¨Åne-\nannotated high resolution images with 19 categories. COCO-Stuff covers 172 labels and consists of\n164k images: 118k for training, 5k for validation, 20k for test-dev and 20k for the test-challenge.\nImplementation details: We used the mmsegmentation1 codebase and train on a server with 8 Tesla\nV100. We pre-train the encoder on the Imagenet-1K dataset and randomly initialize the decoder.\nDuring training, we applied data augmentation through random resize with ratio 0.5-2.0, random\nhorizontal Ô¨Çipping, and random cropping to 512 √ó512, 1024 √ó1024, 512 √ó512 for ADE20K,\nCityscapes and COCO-Stuff, respectively. Following [9] we set crop size to 640 √ó640 on ADE20K\nfor our largest model B5. We trained the models using AdamW optimizer for 160K iterations on\nADE20K, Cityscapes, and 80K iterations on COCO-Stuff. Exceptionally, for the ablation studies, we\ntrained the models for 40K iterations. We used a batch size of 16 for ADE20K and COCO-Stuff, and\na batch size of 8 for Cityscapes. The learning rate was set to an initial value of 0.00006 and then used\na ‚Äúpoly‚Äù LR schedule with factor 1.0 by default. For simplicity, wedid notadopt widely-used tricks\nsuch as OHEM, auxiliary losses or class balance loss. During evaluation, we rescale the short side\nof the image to training cropping size and keep the aspect ratio for ADE20K and COCO-Stuff. For\nCityscapes, we do inference using sliding window test by cropping 1024 √ó1024 windows. We report\nsemantic segmentation performance using mean Intersection over Union (mIoU).\n4.2 Ablation Studies\nInÔ¨Çuence of the size of model. We Ô¨Årst analyze the effect of increasing the size of the encoder on\nthe performance and model efÔ¨Åciency. Figure 1 shows the performance vs. model efÔ¨Åciency for\nADE20K as a function of the encoder size and, Table 1a summarizes the results for the three datasets.\nThe Ô¨Årst thing to observe here is the size of the decoder compared to the encoder. As shown, for\nthe lightweight model, the decoder has only 0.4M parameters. For MiT-B5 encoder, the decoder\nonly takes up to 4% of the total number of parameters in the model. In terms of performance, we\ncan observe that, overall, increasing the size of the encoder yields consistent improvements on all\nthe datasets. Our lightweight model, SegFormer-B0, is compact and efÔ¨Åcient while maintaining a\ncompetitive performance, showing that our method is very convenient for real-time applications. On\nthe other hand, our SegFormer-B5, the largest model, achieves state-of-the-art results on all three\ndatasets, showing the potential of our Transformer encoder.\nInÔ¨Çuence of C, the MLP decoder channel dimension.We now analyze the inÔ¨Çuence of the channel\ndimension C in the MLP decoder, see Section 3.2. In Table 1b we show performance, Ô¨Çops, and\nparameters as a function of this dimension. We can observe that setting C = 256 provides a\nvery competitive performance and computational cost. The performance increases as C increases;\nhowever, it leads to larger and less efÔ¨Åcient models. Interestingly, this performance plateaus for\nchannel dimensions wider than 768. Given these results, we choose C = 256 for our real-time\nmodels SegFormer-B0, B1 and C = 768for the rest.\n1https://github.com/open-mmlab/mmsegmentation\n6\nTable 1: Ablation studies related to model size, encoder and decoder design.\n(a) Accuracy, parameters and Ô¨Çops as a function of the model size on the three datasets. ‚ÄúSS‚Äù and ‚ÄúMS‚Äù means single/multi-scale test.\nEncoder Params ADE20K Cityscapes COCO-Stuff\nModel SizeEncoder DecoderFlops‚Üì mIoU(SS/MS)‚Üë Flops‚Üì mIoU(SS/MS)‚Üë Flops‚Üì mIoU(SS)‚Üë\nMiT-B0 3.4 0.4 8.4 37.4 / 38.0 125.5 76.2 / 78.1 8.4 35.6\nMiT-B1 13.1 0.6 15.9 42.2 / 43.1 243.7 78.5 / 80.0 15.9 40.2\nMiT-B2 24.2 3.3 62.4 46.5 / 47.5 717.1 81.0 / 82.2 62.4 44.6\nMiT-B3 44.0 3.3 79.0 49.4 / 50.0 962.9 81.7 / 83.3 79.0 45.5\nMiT-B4 60.8 3.3 95.7 50.3 / 51.1 1240.6 82.3 / 83.9 95.7 46.5\nMiT-B5 81.4 3.3 183.3 51.0 / 51.8 1460.4 82.4 / 84.0 111.6 46.7\n(b) Accuracy as a function of the MLP\ndimension C in the decoder on ADE20K.\nC Flops‚Üì Params‚Üì mIoU‚Üë\n256 25.7 24.7 44.9\n512 39.8 25.8 45.0\n768 62.4 27.5 45.4\n1024 93.6 29.6 45.2\n2048 304.4 43.4 45.6\n(c) Mix-FFN vs. positional encoding (PE) for\ndifferent test resolution on Cityscapes.\nInf Res Enc Type mIoU‚Üë\n768√ó768 PE 77.3\n1024√ó2048 PE 74.0\n768√ó768 Mix-FFN 80.5\n1024√ó2048 Mix-FFN 79.8\n(d) Accuracy on ADE20K of CNN and\nTransformer encoder with MLP decoder.\n‚ÄúS4‚Äù means stage-4 feature.\nEncoder Flops ‚ÜìParams‚ÜìmIoU‚Üë\nResNet50 (S1-4) 69.2 29.0 34.7ResNet101 (S1-4) 88.7 47.9 38.7ResNeXt101 (S1-4) 127.5 86.8 39.8MiT-B2 (S4) 22.3 24.7 43.1MiT-B2 (S1-4)62.4 27.7 45.4MiT-B3 (S1-4)79.0 47.3 48.6\nTable 2: Comparison to state of the art methods on ADE20K and Cityscapes. SegFormer has signiÔ¨Åcant\nadvantages on #Params, #Flops, #Speed and #Accuracy. Note that for SegFormer-B0 we scale the short side of\nimage to {1024, 768, 640, 512} to get speed-accuracy tradeoffs.\nMethod Encoder Params‚Üì ADE20K Cityscapes\nFlops‚Üì FPS‚Üë mIoU‚Üë Flops‚Üì FPS‚Üë mIoU‚Üë\nReal-Time\nFCN [1] MobileNetV2 9.8 39.6 64.4 19.7 317.1 14.2 61.5\nICNet [11] - - - - - - 30.3 67.7\nPSPNet [17] MobileNetV2 13.7 52.9 57.7 29.6 423.4 11.2 70.2\nDeepLabV3+ [20] MobileNetV2 15.4 69.4 43.1 34.0 555.4 8.4 75.2\nSegFormer(Ours) MiT-B0 3.8\n8.4 50.5 37.4 125.5 15.2 76.2\n- - - 51.7 26.3 75.3\n- - - 31.5 37.1 73.7\n- - - 17.7 47.6 71.9\nNon Real-Time\nFCN [1] ResNet-101 68.6 275.7 14.8 41.4 2203.3 1.2 76.6\nEncNet [24] ResNet-101 55.1 218.8 14.9 44.7 1748.0 1.3 76.9\nPSPNet [17] ResNet-101 68.1 256.4 15.3 44.4 2048.9 1.2 78.5\nCCNet [41] ResNet-101 68.9 278.4 14.1 45.2 2224.8 1.0 80.2\nDeeplabV3+ [20] ResNet-101 62.7 255.1 14.1 44.1 2032.3 1.2 80.9\nOCRNet [23] HRNet-W48 70.5 164.8 17.0 45.6 1296.8 4.2 81.1\nGSCNN [35] WideResNet38 - - - - - - 80.8\nAxial-DeepLab [74]AxialResNet-XL - - - - 2446.8 - 81.1\nDynamic Routing [75]Dynamic-L33-PSP - - - - 270.0 - 80.7\nAuto-Deeplab [50] NAS-F48-ASPP - - - 44.0 695.0 - 80.3\nSETR [7] ViT-Large 318.3 - 5.4 50.2 - 0.5 82.2\nSegFormer(Ours) MiT-B4 64.1 95.7 15.4 51.1 1240.6 3.0 83.8\nSegFormer(Ours) MiT-B5 84.7 183.3 9.8 51.8 1447.6 2.5 84.0\nMix-FFN vs. Positional Encoder (PE). In this experiment, we analyze the effect of removing the\npositional encoding in the Transformer encoder in favor of using the proposed Mix-FFN. To this\nend, we train Transformer encoders with a positional encoding (PE) and the proposed Mix-FFN\nand perform inference on Cityscapes with two different image resolutions: 768√ó768 using a sliding\nwindow, and 1024√ó2048 using the whole image.\nTable 1c shows the results for this experiment. As shown, for a given resolution, our approach using\nMix-FFN clearly outperforms using a positional encoding. Moreover, our approach is less sensitive\nto differences in the test resolution: the accuracy drops 3.3% when using a positional encoding with a\nlower resolution. In contrast, when we use the proposed Mix-FFN the performance drop is reduced\nto only 0.7%. From these results, we can conclude using the proposed Mix-FFN produces better and\nmore robust encoders than those using positional encoding.\nEffective receptive Ô¨Åeld evaluation. In Section 3.2, we argued that our MLP decoder beneÔ¨Åts\nfrom Transformers having a larger effective receptive Ô¨Åeld compared to other CNN models. To\nquantify this effect, in this experiment, we compare the performance of our MLP-decoder when\nused with CNN-based encoders such as ResNet or ResNeXt. As shown in Table 1d, coupling our\n7\nMLP-decoder with a CNN-based encoder yields a signiÔ¨Åcantly lower accuracy compared to coupling\nit with the proposed Transformer encoder. Intuitively, as a CNN has a smaller receptive Ô¨Åeld than the\nTransformer (see the analysis in Section 3.2), the MLP-decoder is not enough for global reasoning.\nIn contrast, coupling our Transformer encoder with the MLP decoder leads to the best performance.\nMoreover, for Transformer encoder, it is necessary to combine low-level local features and high-level\nnon-local features instead of only high-level feature.\n4.3 Comparison to state of the art methods\nWe now compare our results with existing approaches on the ADE20K [ 72], Cityscapes [71] and\nCOCO-Stuff [73] datasets.\nADE20K and Cityscapes: Table 2 summarizes our results including parameters, FLOPS, latency,\nand accuracy for ADE20K and Cityscapes. In the top part of the table, we report real-time approaches\nwhere we include state-of-the-art methods and our results using the MiT-B0 lightweight encoder. In\nthe bottom part, we focus on performance and report the results of our approach and related works\nusing stronger encoders.\nAs shown, on ADE20K, SegFormer-B0 yields 37.4% mIoU using only 3.8M parameters and 8.4G\nFLOPs, outperforming all other real-time counterparts in terms of parameters, Ô¨Çops, and latency. For\ninstance, compared to DeeplabV3+ (MobileNetV2), SegFormer-B0 is 7.4 FPS, which is faster and\nkeeps 3.4% better mIoU. Moreover, SegFormer-B5 outperforms all other approaches, including the\nprevious best SETR, and establishes a new state-of-the-art of 51.8%, which is 1.6% mIoU better than\nSETR while being signiÔ¨Åcantly more efÔ¨Åcient.\nTable 3: Comparison to state of the art methods on Cityscapes\ntest set. IM-1K, IM-22K, Coarse and MV refer to the ImageNet-1K,\nImageNet-22K, Cityscapes coarse set and Mapillary Vistas. SegFormer\noutperforms the compared methods with equal or less extra data.\nMethod Encoder Extra Data mIoU\nPSPNet [17] ResNet-101 IM-1K 78.4PSANet [43] ResNet-101 IM-1K 80.1CCNet [41] ResNet-101 IM-1K 81.9OCNet [21] ResNet-101 IM-1K 80.1Axial-DeepLab [74] AxiaiResNet-XL IM-1K 79.9SETR [7] ViT IM-22K 81.0SETR [7] ViT IM-22K, Coarse 81.6\nSegFormer MiT-B5 IM-1K 82.2SegFormer MiT-B5 IM-1K, MV 83.1\nAs also shown in Table 2, our results also hold\non Cityscapes. SegFormer-B0 yields 15.2 FPS\nand 76.2% mIoU (the shorter side of input im-\nage being 1024), which represents a 1.3% mIoU\nimprovement and a 2 √óspeedup compared to\nDeeplabV3+. Moreover, with the shorter side\nof input image being 512, SegFormer-B0 runs\nat 47.6 FPS and yields 71.9% mIoU, which is\n17.3 FPS faster and 4.2% better than ICNet.\nSegFormer-B5 archives the best IoU of 84.0%,\noutperforming all existing methods by at least\n1.8% mIoU, and it runs 5 √ófaster and 4 √ó\nsmaller than SETR [7].\nOn Cityscapes test set, we follow the common setting [20] and merge the validation images to the\ntrain set and report results using Imagenet-1K pre-training and also using Mapillary Vistas [ 76].\nAs reported in Table 3, using only Cityscapes Ô¨Åne data and Imagenet-1K pre-training, our method\nachieves 82.2% mIoU outperforming all other methods including SETR, which uses ImageNet-22K\npre-training and the additional Cityscapes coarse data. Using Mapillary pre-training, our sets a\nnew state-of-the-art result of 83.1% mIoU. Figure 4 shows qualitative results on Cityscapes, where\nSegFormer provides better details than SETR and smoother predictions than DeeplabV3+.\nTable 4: Results on COCO-Stuff full dataset containing\nall 164K images from COCO 2017 and covers 172 classes.\nMethod Encoder Params mIoU\nDeeplabV3+ [20] ResNet5043.7 38.4OCRNet [23] HRNet-W48 70.5 42.3SETR [7] ViT 305.7 45.8\nSegFormer MiT-B5 84.7 46.7\nCOCO-Stuff. Finally, we evaluate SegFormer on the\nfull COCO-Stuff dataset. For comparison, as exist-\ning methods do not provide results on this dataset,\nwe reproduce the most representative methods such\nas DeeplabV3+, OCRNet, and SETR. In this case, the\nÔ¨Çops on this dataset are the same as those reported for\nADE20K. As shown in Table 4, SegFormer-B5 reaches\n46.7% mIoU with only 84.7M parameters, which is 0.9%\nbetter and 4√ósmaller than SETR. In summary, these results demonstrate the superiority of SegFormer\nin semantic segmentation in terms of accuracy, computation cost, and model size.\n4.4 Robustness to natural corruptions\nModel robustness is important for many safety-critical tasks such as autonomous driving [77]. In this\nexperiment, we evaluate the robustness of SegFormer to common corruptions and perturbations. To\n8\nSegFormerSETR SegFormerDeepLabv3+\nFigure 4: Qualitative results on Cityscapes. Compared to SETR, our SegFormer predicts masks with substan-\ntially Ô¨Åner details near object boundaries. Compared to DeeplabV3+, SegFormer reduces long-range errors as\nhighlighted in red. Best viewed in screen.\nthis end, we follow [77] and generate Cityscapes-C, which expands the Cityscapes validation set with\n16 types of algorithmically generated corruptions from noise, blur, weather and digital categories. We\ncompare our method to variants of DeeplabV3+ and other methods as reported in [77]. The results\nfor this experiment are summarized in Table 5.\nOur method signiÔ¨Åcantly outperforms previous methods, yielding a relative improvement of up to\n588% on Gaussian Noise and up to 295% on snow weather. The results indicate the strong robustness\nof SegFormer, which we envision to beneÔ¨Åt safety-critical applications where robustness is important.\nTable 5: Main results on Cityscapes-C. ‚ÄúDLv3+‚Äù, ‚ÄúMBv2‚Äù, ‚ÄúR‚Äù and ‚ÄúX‚Äù refer to DeepLabv3+, MobileNetv2,\nResNet and Xception. The mIoUs of compared methods are reported from [77].\nMethod Clean Blur Noise Digital WeatherMotion Defoc Glass GaussGauss Impul Shot SpeckBright Contr Satur JPEGSnow Spatt Fog Frost\nDLv3+ (MBv2)72.0 53.5 49.0 45.3 49.1 6.4 7.0 6.6 16.6 51.7 46.7 32.4 27.2 13.7 38.9 47.4 17.3DLv3+ (R50)76.6 58.5 56.6 47.2 57.7 6.5 7.2 10.0 31.1 58.2 54.7 41.3 27.4 12.0 42.0 55.9 22.8DLv3+ (R101)77.1 59.1 56.3 47.7 57.3 13.2 13.9 16.3 36.9 59.2 54.5 41.5 37.4 11.9 47.8 55.1 22.7DLv3+ (X41)77.8 61.6 54.9 51.0 54.7 17.0 17.3 21.6 43.7 63.6 56.9 51.7 38.5 18.2 46.6 57.6 20.6DLv3+ (X65)78.4 63.9 59.1 52.8 59.2 15.0 10.6 19.8 42.4 65.9 59.1 46.1 31.4 19.3 50.7 63.6 23.8DLv3+ (X71)78.6 64.1 60.9 52.0 60.4 14.9 10.8 19.4 41.2 68.0 58.7 47.1 40.2 18.8 50.4 64.1 20.2\nICNet 65.9 45.8 44.6 47.4 44.7 8.4 8.4 10.6 27.9 41.0 33.1 27.5 34.0 6.3 30.5 27.3 11.0FCN8s 66.7 42.7 31.1 37.0 34.1 6.7 5.7 7.8 24.9 53.3 39.0 36.0 21.2 11.3 31.6 37.6 19.7DilatedNet68.6 44.4 36.3 32.5 38.4 15.6 14.0 18.4 32.7 52.7 32.6 38.1 29.1 12.5 32.3 34.7 19.2ResNet-3877.5 54.6 45.1 43.3 47.2 13.7 16.0 18.2 38.3 60.0 50.6 46.9 14.7 13.5 45.9 52.9 22.2PSPNet 78.8 59.8 53.2 44.4 53.9 11.0 15.4 15.4 34.2 60.4 51.8 30.6 21.4 8.4 42.7 34.4 16.2GSCNN 80.9 58.9 58.4 41.9 60.1 5.5 2.6 6.8 24.7 75.9 61.9 70.7 12.0 12.4 47.3 67.9 32.6\nSegFormer-B582.4 69.1 68.6 64.1 69.8 57.8 63.4 52.3 72.8 81.0 77.7 80.1 58.8 40.7 68.4 78.5 49.9\n5 Conclusion\nIn this paper, we present SegFormer, a simple, clean yet powerful semantic segmentation method\nwhich contains a positional-encoding-free, hierarchical Transformer encoder and a lightweight All-\nMLP decoder. It avoids common complex designs in previous methods, leading to both high efÔ¨Åciency\nand performance. SegFormer not only achieves new state of the art results on common datasets,\nbut also shows strong zero-shot robustness. We hope our method can serve as a solid baseline for\nsemantic segmentation and motivate further research. One limitation is that although our smallest\n3.7M parameters model is smaller than the known CNN‚Äôs model, it is unclear whether it can work\nwell in a chip of edge device with only 100k memory. We leave it for future work.\nAcknowledgement\nWe thank Ding Liang, Zhe Chen and Yaojun Liu for insightful discussion without which this paper\nwould not be possible.\nA Details of MiT Series\nIn this section, we list some important hyper-parameters of our Mix Transformer (MiT) encoder. By\nchanging these parameters, we can easily scale up our encoder from B0 to B5.\nIn summary, the hyper-parameters of our MiT are listed as follows:\n‚Ä¢ Ki: the patch size of the overlapping patch embedding in Stage i;\n9\n‚Ä¢ Si: the stride of the overlapping patch embedding in Stage i;\n‚Ä¢ Pi: the padding size of the overlapping patch embedding in Stage i;\n‚Ä¢ Ci: the channel number of the output of Stage i;\n‚Ä¢ Li: the number of encoder layers in Stage i;\n‚Ä¢ Ri: the reduction ratio of the EfÔ¨Åcient Self-Attention in Stage i;\n‚Ä¢ Ni: the head number of the EfÔ¨Åcient Self-Attention in Stage i;\n‚Ä¢ Ei: the expansion ratio of the feed-forward layer [78] in Stage i;\nTable 6 shows the detailed information of our MiT series. To facilitate efÔ¨Åcient discussion, we assign\nthe code name B0 to B5 for MiT encoder, where B0 is the smallest model designed for real-time,\nwhile B5 is the largest model designed for high performance.\nB More Qualitative Results on Mask Predictions\nIn Figure 5, we present more qualitative results on Cityscapes, ADE20K and COCO-Stuff, compared\nwith SETR and DeepLabV3+.\nCompared to SETR, our SegFormer predicts masks with signiÔ¨Åcantly Ô¨Åner details near object\nboundaries because our Transformer encoder can capture much higher resolution features than\nSETR, which preserves more detailed texture information. Compared to DeepLabV3+, SegFormer\nreduces long-range errors beneÔ¨Åt from the larger effective receptive Ô¨Åeld of Transformer encoder than\nConvNet.\nC More Visualization on Effective Receptive Field\nIn Figure 6, we select some representative images and effective receptive Ô¨Åeld (ERF) of DeepLabV3+\nand SegFormer. Beyond larger ERF, the ERF of SegFormer is more sensitive to the context of\nthe image. We see SegFormer‚Äôs ERF learned the pattern of roads, cars, and buildings, while\nDeepLabV3+‚Äôs ERF shows a relatively Ô¨Åxed pattern. The results also indicate that our Transformer\nencoder has a stronger feature extraction ability than ConvNets.\nD More Comparison of DeeplabV3+ and SegFormer on Cityscapes-C\nIn this section, we detailed show the zero-shot robustness compared with SegFormer and DeepLabV3+.\nFollowing [77], we test 3 severities for 4 kinds of ‚ÄúNoise‚Äù and 5 severities for the rest 12 kinds of\ncorruptions and perturbations.\nAs shown in Figure 7, with severity increase, DeepLabV3+ shows a considerable performance\ndegradation. In contrast, the performance of SegFormer is relatively stable. Moreover, SegFormer\nhas signiÔ¨Åcant advantages over DeepLabV3+ on all corruptions/perturbations and all severities,\ndemonstrating excellent zero-shot robustness.\n10\nOutput Size Layer Name Mix Transformer\nB0 B1 B2 B3 B4 B5\nStage 1 H\n4 √ó W\n4\nOverlapping\nPatch Embedding\nK1 = 7; S1 = 4; P1 = 3\nC1 = 32 C1 = 64\nTransformer\nEncoder\nR1 = 8\nN1 = 1\nE1 = 8\nL1 = 2\nR1 = 8\nN1 = 1\nE1 = 8\nL1 = 2\nR1 = 8\nN1 = 1\nE1 = 8\nL1 = 3\nR1 = 8\nN1 = 1\nE1 = 8\nL1 = 3\nR1 = 8\nN1 = 1\nE1 = 8\nL1 = 3\nR1 = 8\nN1 = 1\nE1 = 4\nL1 = 3\nStage 2 H\n8 √ó W\n8\nOverlapping\nPatch Embedding\nK2 = 3; S2 = 2; P2 = 1\nC2 = 64 C2 = 128\nTransformer\nEncoder\nR2 = 4\nN2 = 2\nE2 = 8\nL2 = 2\nR2 = 4\nN2 = 2\nE2 = 8\nL2 = 2\nR2 = 4\nN2 = 2\nE2 = 8\nL2 = 3\nR2 = 4\nN2 = 2\nE2 = 8\nL2 = 3\nR2 = 4\nN2 = 2\nE2 = 8\nL2 = 8\nR2 = 4\nN2 = 2\nE2 = 4\nL2 = 6\nStage 3 H\n16 √ó W\n16\nOverlapping\nPatch Embedding\nK3 = 3; S3 = 2; P3 = 1\nC3 = 160 C3 = 320\nTransformer\nEncoder\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 2\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 2\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 6\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 18\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 27\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 40\nStage 4 H\n32 √ó W\n32\nOverlapping\nPatch Embedding\nK4 = 3; S4 = 2; P4 = 1\nC4 = 256 C4 = 512\nTransformer\nEncoder\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 2\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 2\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 3\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 3\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 3\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 3\nTable 6: Detailed settings of MiT series. Our design follows the principles of ResNet [12]. (1) the\nchannel dimension increase while the spatial resolution shrink with the layer goes deeper. (2) Stage 3\nis assigned to most of the computation cost.\nMethod GFLOPs Params (M) Top 1\nMiT-B0 0.6 3.7 70.5\nMiT-B1 2.1 14.0 78.7\nMiT-B2 4.0 25.4 81.6\nMiT-B3 6.9 45.2 83.1\nMiT-B4 10.1 62.6 83.6\nMiT-B5 11.8 82.0 83.8\nTable 7: Mix Transformer Encoder\n11\nSegFormer SETR DeepLabV3+\nFigure 5: Qualitative results on Cityscapes, ADE20K and COCO-Stuff. First row: Cityscapes. Second row:\nADE20K. Third row: COCO-Stuff. Zoom in for best view.\n12\nDeepLabv3+SegFormer\nDeepLabv3+SegFormer\nDeepLabv3+SegFormer\nStage-1 Stage-2 Stage-3 HeadStage-4\nFigure 6: Effective Receptive Field on Cityscapes. ERFs of the four stages and the decoder heads of both architectures are visualized.\n13\n0.0\n20.0\n40.0\n60.0\n80.0\n1 2 3\nGaussian Noise\n0.0\n20.0\n40.0\n60.0\n80.0\n1 2 3\nShot Noise\n0.0\n20.0\n40.0\n60.0\n80.0\n1 2 3\nImpluse Noise\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\n1 2 3\nSpeckle Noise\n30.0\n50.0\n70.0\n90.0\n1 2 3 4 5\nMotion blur\n20.0\n40.0\n60.0\n80.0\n1 2 3 4 5\nDefocus Blur\n0.0\n20.0\n40.0\n60.0\n80.0\n1 2 3 4 5\nGlass Blur\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\n1 2 3 4 5\nGaussian Blur\n0.0\n20.0\n40.0\n60.0\n80.0\n1 2 3 4 5\nSnow\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\n1 2 3 4 5\nSpatter\n60.0\n66.0\n72.0\n78.0\n84.0\n1 2 3 4 5\nFog\n0.0\n20.0\n40.0\n60.0\n80.0\n1 2 3 4 5\nForst\n70.0\n74.0\n78.0\n82.0\n86.0\n1 2 3 4 5\nBrightness\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\n1 2 3 4 5\nContrast\n0.0\n20.0\n40.0\n60.0\n80.0\n1 2 3 4 5\nJPEG_compression\n40.0\n60.0\n80.0\n100.0\n1 2 3 4 5\nSaturate\nFigure 7: Comparison of zero shot robustness on Cityscapes-C between SegFormer and DeepLabV3+.Blue line is SegFormer and orange\nline is DeepLabV3+. X-Axis means corrupt severity and Y-Axis is mIoU. Following[77], we test 3 severities for ‚ÄúNoise‚Äù and 5 severities for\nthe rest.\n14\nReferences\n[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-\ntion. In CVPR, 2015. 1, 2, 7\n[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic\nimage segmentation with deep convolutional nets and fully connected CRFs. In ICLR, 2015. 1, 2\n[3] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas\nMueller, R Manmatha, et al. ResNest: Split-attention networks. arXiv, 2020. 1\n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs.\nTPAMI, 2017. 2\n[5] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016. 2,\n5\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv, 2020. 2, 3\n[7] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. CVPR, 2021. 2, 3, 5, 7, 8\n[8] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv, 2021. 2, 3, 4\n[9] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv, 2021. 2, 3, 6\n[10] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua\nShen. Twins: Revisiting spatial attention design in vision transformers. arXiv, 2021. 2, 3\n[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic\nsegmentation on high-resolution images. In ECCV, 2018. 2, 7\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016. 2, 11\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep convolutional\nneural networks. NeurIPS, 2012.\n[14] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\ntion. arXiv, 2014.\n[15] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In CVPR, 2019.\n[16] Wenhai Wang, Xiang Li, Tong Lu, and Jian Yang. Mixed link networks. In IJCAI, 2018. 2\n[17] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\nnetwork. In CVPR, 2017. 2, 7, 8\n[18] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang. Denseaspp for semantic segmentation in\nstreet scenes. In CVPR, 2018. 5\n[19] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters‚Äìimprove semantic\nsegmentation by global convolutional network. In CVPR, 2017. 2, 5\n[20] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In ECCV, 2018. 2, 5, 7, 8\n[21] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene parsing. arXiv, 2018. 2, 8\n[22] Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, and Nong Sang. Context prior for\nscene segmentation. In CVPR, 2020.\n15\n[23] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation.\narXiv, 2019. 7, 8\n[24] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit\nAgrawal. Context encoding for semantic segmentation. In CVPR, 2018. 7\n[25] Yizhou Zhou, Xiaoyan Sun, Zheng-Jun Zha, and Wenjun Zeng. Context-reinforced semantic segmentation.\nIn CVPR, 2019.\n[26] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. ReÔ¨Ånenet: Multi-path reÔ¨Ånement networks for\nhigh-resolution semantic segmentation. In CVPR, 2017.\n[27] Rudra PK Poudel, Ujwal Bonde, Stephan Liwicki, and Christopher Zach. Contextnet: Exploring context\nand detail for semantic segmentation in real-time. arXiv, 2018.\n[28] Tianyi Wu, Sheng Tang, Rui Zhang, and Yongdong Zhang. Cgnet: A light-weight context guided network\nfor semantic segmentation. arXiv, 2018.\n[29] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao. Adaptive pyramid context network for\nsemantic segmentation. In CVPR, 2019. 3\n[30] Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann, and Gang Wang. Boundary-aware\nfeature propagation for scene segmentation. In ICCV, 2019. 3\n[31] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Semantic segmentation with boundary neural Ô¨Åelds.\nIn CVPR, 2016.\n[32] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, and Yunhai\nTong. Improving semantic segmentation via decoupled body and edge supervision. arxiv, 2020.\n[33] Yuhui Yuan, Jingyi Xie, Xilin Chen, and Jingdong Wang. SegÔ¨Åx: Model-agnostic boundary reÔ¨Ånement for\nsegmentation. In ECCV, 2020.\n[34] Mingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang Shang, Tian Fang, and Long\nQuan. Joint semantic segmentation and boundary detection using iterative pyramid contexts. In CVPR,\n2020.\n[35] Towaki Takikawa, David Acuna, Varun Jampani, and Sanja Fidler. Gated-scnn: Gated shape cnns for\nsemantic segmentation. In ICCV, 2019. 7\n[36] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Learning a discrimina-\ntive feature network for semantic segmentation. In CVPR, 2018.\n[37] Liang-Chieh Chen, Jonathan T Barron, George Papandreou, Kevin Murphy, and Alan L Yuille. Semantic\nimage segmentation with task-speciÔ¨Åc edge detection using cnns and a discriminatively trained domain\ntransform. In CVPR, 2016. 3\n[38] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention\nnetwork for scene segmentation. In CVPR, 2019. 3\n[39] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,\n2018.\n[40] Zilong Zhong, Zhong Qiu Lin, Rene Bidart, Xiaodan Hu, Ibrahim Ben Daya, Zhifeng Li, Wei-Shi Zheng,\nJonathan Li, and Alexander Wong. Squeeze-and-attention networks for semantic segmentation. In CVPR,\n2020.\n[41] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. In ICCV, 2019. 7, 8\n[42] Hanchao Li, Pengfei Xiong, Jie An, and Lingxue Wang. Pyramid attention network for semantic segmenta-\ntion. arXiv, 2018.\n[43] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet:\nPoint-wise spatial attention network for scene parsing. In ECCV, 2018. 8\n[44] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-maximization\nattention networks for semantic segmentation. In ICCV, 2019.\n16\n[45] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-\nexcitation networks and beyond. In ICCVW, 2019.\n[46] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, and Ping Luo. Segmenting\ntransparent object in the wild with transformer. IJCAI, 2021. 3\n[47] Albert Shaw, Daniel Hunter, Forrest Landola, and Sammy Sidhu. Squeezenas: Fast neural architecture\nsearch for faster semantic segmentation. In ICCVW, 2019. 3\n[48] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, and Zhangyang Wang. Fasterseg:\nSearching for faster real-time semantic segmentation. arXiv, 2019.\n[49] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning\ndynamic routing for semantic segmentation. In CVPR, 2020.\n[50] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei.\nAuto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In CPVR, 2019. 7\n[51] Vladimir Nekrasov, Hao Chen, Chunhua Shen, and Ian Reid. Fast neural architecture search of compact\nsemantic segmentation models via auxiliary cells. In CVPR, 2019. 3\n[52] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-End object detection with transformers. In ECCV, 2020. 3\n[53] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv, 2021.\n3\n[54] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nConditional positional encodings for vision transformers. arXiv, 2021. 3, 4\n[55] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv, 2021. 3\n[56] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer\nfor image classiÔ¨Åcation. arXiv, 2021. 3\n[57] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to\nvision transformers. arXiv, 2021. 3\n[58] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv, 2021. 3\n[59] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers.\narXiv, 2021. 3\n[60] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv√© J√©gou, and Matthijs\nDouze. Levit: a vision transformer in convnet‚Äôs clothing for faster inference. arXiv, 2021. 3\n[61] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu\nWang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. arXiv, 2020. 3\n[62] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-\nobject tracking with transformers. arXiv, 2021. 3\n[63] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv, 2020. 3\n[64] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang. Transreid: Transformer-based\nobject re-identiÔ¨Åcation. arXiv, 2021. 3\n[65] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner. Colorization transformer. arXiv, 2021. 3\n[66] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herv√© J√©gou. Training vision transformers for\nimage retrieval. arXiv, 2021. 3\n[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. arXiv, 2021. 3\n17\n[68] Ronghang Hu and Amanpreet Singh. Transformer is all you need: Multimodal multitask learning with a\nuniÔ¨Åed transformer. arXiv, 2021. 3\n[69] Md Amirul Islam, Sen Jia, and Neil DB Bruce. How much position information do convolutional neural\nnetworks encode? arXiv, 2020. 4\n[70] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive Ô¨Åeld in\ndeep convolutional neural networks. arXiv, 2017. 5\n[71] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.\nIn CVPR, 2016. 6, 8\n[72] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, 2017. 6, 8\n[73] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In\nCVPR, 2018. 6, 8\n[74] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan L. Yuille, and Liang-Chieh Chen. Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020. 7, 8\n[75] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning\ndynamic routing for semantic segmentation. In CVPR, 2020. 7\n[76] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul√≤, and Peter Kontschieder. The mapillary vistas\ndataset for semantic understanding of street scenes. In ICCV, 2017. 8\n[77] Christoph Kamann and Carsten Rother. Benchmarking the robustness of semantic segmentation models.\nIn CVPR, 2020. 8, 9, 10, 14\n[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 10\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7434136867523193
    },
    {
      "name": "Transformer",
      "score": 0.7068854570388794
    },
    {
      "name": "Segmentation",
      "score": 0.6922069191932678
    },
    {
      "name": "Encoder",
      "score": 0.677470862865448
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5127266645431519
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4472983181476593
    },
    {
      "name": "Algorithm",
      "score": 0.42409470677375793
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3798239827156067
    },
    {
      "name": "Computer engineering",
      "score": 0.3447391390800476
    },
    {
      "name": "Engineering",
      "score": 0.1093190610408783
    },
    {
      "name": "Voltage",
      "score": 0.07701584696769714
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}