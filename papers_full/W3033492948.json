{
  "title": "Learning Texture Transformer Network for Image Super-Resolution",
  "url": "https://openalex.org/W3033492948",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2644072102",
      "name": "Yang Fu-zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2012227728",
      "name": "Yang Huan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748452209",
      "name": "Fu, Jianlong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231760362",
      "name": "Lu, Hongtao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3010309834",
      "name": "Guo, Baining",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W1991867157",
    "https://openalex.org/W2780544323",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964125708",
    "https://openalex.org/W2087380704",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2097074225",
    "https://openalex.org/W2952773607",
    "https://openalex.org/W1976416062",
    "https://openalex.org/W2963231084",
    "https://openalex.org/W2962737939",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2036062360",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W2962879692",
    "https://openalex.org/W2554753600",
    "https://openalex.org/W2954930822",
    "https://openalex.org/W2503339013",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W2799062770",
    "https://openalex.org/W1976598894",
    "https://openalex.org/W2894343975",
    "https://openalex.org/W2940262938",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2919046835",
    "https://openalex.org/W2963966654",
    "https://openalex.org/W1930824406",
    "https://openalex.org/W2192954843",
    "https://openalex.org/W2964101377",
    "https://openalex.org/W2150081556",
    "https://openalex.org/W2883996939",
    "https://openalex.org/W2963037581",
    "https://openalex.org/W2214802144",
    "https://openalex.org/W2526558307",
    "https://openalex.org/W2983339877",
    "https://openalex.org/W2331128040"
  ],
  "abstract": "We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a soft-attention module for texture synthesis. Such a design encourages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be transferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recovery from different levels (e.g., from 1x to 4x magnification). Extensive experiments show that TTSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.",
  "full_text": "Learning Texture Transformer Network for Image Super-Resolution\nFuzhi Yang1‚àó, Huan Yang2, Jianlong Fu2, Hongtao Lu1, Baining Guo2\n1Department of Computer Science and Engineering,\nMoE Key Lab of ArtiÔ¨Åcial Intelligence, AI Institute, Shanghai Jiao Tong University,\n2Microsoft Research, Beijing, P.R. China,\n{yfzcopy0702, htlu}@sjtu.edu.cn, {huayan, jianf, bainguo}@microsoft.com\nAbstract\nWe study on image super-resolution (SR), which aims to\nrecover realistic textures from a low-resolution (LR) image.\nRecent progress has been made by taking high-resolution\nimages as references (Ref), so that relevant textures can\nbe transferred to LR images. However, existing SR ap-\nproaches neglect to use attention mechanisms to transfer\nhigh-resolution (HR) textures from Ref images, which lim-\nits these approaches in challenging cases. In this paper,\nwe propose a novel Texture Transformer Network for Im-\nage Super-Resolution (TTSR), in which the LR and Ref im-\nages are formulated as queries and keys in a transformer,\nrespectively. TTSR consists of four closely-related modules\noptimized for image generation tasks, including a learnable\ntexture extractor by DNN, a relevance embedding module,\na hard-attention module for texture transfer, and a soft-\nattention module for texture synthesis. Such a design en-\ncourages joint feature learning across LR and Ref images,\nin which deep feature correspondences can be discovered by\nattention, and thus accurate texture features can be trans-\nferred. The proposed texture transformer can be further\nstacked in a cross-scale way, which enables texture recov-\nery from different levels (e.g., from 1√óto 4√ómagniÔ¨Åca-\ntion). Extensive experiments show that TTSR achieves sig-\nniÔ¨Åcant improvements over state-of-the-art approaches on\nboth quantitative and qualitative evaluations. The source\ncode can be downloaded at https://github.com/\nresearchmm/TTSR.\n1. Introduction\nImage super-resolution aims to recover natural and real-\nistic textures for a high-resolution image from its degraded\nlow-resolution counterpart [12]. The recent success of im-\nage SR can greatly enhance the quality of media content\n‚àóThis work was performed when the Ô¨Årst author was visiting Microsoft\nResearch as a research intern.\nGT Patch\nSOTA Result\nOur Result\nReference\nReference\nGT\nFigure 1. A comparison of 4√óSR results between the proposed\nTTSR and a state-of-the-art RefSR approach [43]. TTSR (ours)\nlearns to search on relevant textures from the Ref image (indi-\ncated by green) for a target LR region (indicated by yellow), which\navoids incorrect texture transfer (indicated by red).\nfor a better user experiences. For example, the digital zoom\nalgorithm for mobile cameras and image enhancement tech-\nnology for digital televisions. Besides, this fundamental\ntechnology can beneÔ¨Åt a broad range of computer vision\ntasks, like medical imaging [21] and satellite imaging [35].\nThe research on image SR is usually conducted on two\nparadigms, including single image super-resolution (SISR),\nand reference-based image super-resolution (RefSR). Tradi-\ntional SISR often results in blurry effects, because the high-\nresolution (HR) textures have been excessively destructed\nin the degrading process which are unrecoverable. Al-\nthough generative adversarial networks (GANs) [7] based\nimage SR approaches are proposed to relieve the above\nproblems, the resultant hallucinations and artifacts caused\nby GANs further pose grand challenges to image SR tasks.\narXiv:2006.04139v2  [cs.CV]  22 Jun 2020\nRecent progress has been made by reference-based im-\nage super-resolution (RefSR), which transfers HR textures\nfrom a given Ref image to produce visually pleasing re-\nsults [5, 6, 26, 29, 36]. However, state-of-the-art (SOTA)\napproaches usually adopt a straightforward way to trans-\nfer textures which may result in unsatisÔ¨Åed SR images (as\nshown in Figure. 1). For example, Zheng et al. [43] adopts\na Ô¨Çow-based approach which usually searches and trans-\nfers inaccurate textures (indicate by red) when facing large\nviewpoint changes between the LR and Ref image. Zhang\net al. [41] adopts a feature space deÔ¨Åned by a pre-trained\nclassiÔ¨Åcation model to search and transfer textures between\nthe LR and Ref image. Nevertheless, such high-level se-\nmantic features can not effectively represent HR textures\nwhich remain to generate implausible results.\nTo address these problems, we propose a novel Texture\nTransformer Network for Image Super-Resolution (TTSR).\nSpeciÔ¨Åcally, four closely-related modules optimized for im-\nage generation tasks are proposed. First, we propose a\nlearnable texture extractor, in which parameters will be up-\ndated during end-to-end training. Such a design enables\na joint feature embedding of LR and Ref images which\ncreates a solid foundation for applying attention mecha-\nnism [19, 34, 31] in SR tasks. Second, we propose a rele-\nvance embedding module to compute the relevance between\nthe LR and Ref image. More speciÔ¨Åcally, we formulate the\nextracted features from the LR and Ref image as the query\nand key in a transformer [31] to obtain a hard-attention\nmap and a soft-attention map. Finally, we propose a hard-\nattention module and a soft-attention module to transfer and\nfuse HR features from the Ref image into LR features ex-\ntracted from backbone through the attention maps. The de-\nsign of TTSR encourages a more accurate way to search and\ntransfer relevant textures from Ref to LR images.\nFurthermore, we propose a cross-scale feature integra-\ntion module to stack the texture transformer, in which the\nfeatures are learnt across different scales (e.g, from 1√óto\n4√ó) to achieve a more powerful feature representation. As\nshown in Figure 1, the overall design enables our TTSR\nto search and transfer relevant textures from the Ref image\n(indicated by green) which achieves a better visual result\ncompared with SOTA approaches. The main contributions\nof this paper are:\n‚Ä¢To the best of our knowledge, we are one of the Ô¨Årst to\nintroduce the transformer architecture into image gen-\neration tasks. More speciÔ¨Åcally, we propose a texture\ntransformer with four closely-related modules for im-\nage SR which achieves signiÔ¨Åcant improvements over\nSOTA approaches.\n‚Ä¢We propose a novel cross-scale feature integration\nmodule for image generation tasks which enables our\napproach to learn a more powerful feature representa-\ntion by stacking multiple texture transformers.\n2. Related Work\nIn this section, we review previous works of single image\nsuper-resolution (SISR) and reference-based image super-\nresolution (RefSR) which are the most relevant to our work.\n2.1. Single Image Super-Resolution\nIn recent years, deep learning based SISR methods have\nachieved signiÔ¨Åcant improvements over traditional non-\nlearning based methods. Deep learning based methods in\nSISR treat this problem as a dense image regression task\nwhich learns an end-to-end image mapping function repre-\nsented by a CNN between LR and HR images.\nDong et al. [3] proposed SRCNN that Ô¨Årstly adopted\ndeep learning into SISR by using a three-layer CNN to rep-\nresent the mapping function. Dong et al. [4] further sped up\nthe SR process by replacing the interpolated LR image with\nthe original LR image and using deconvolution at the very\nlast layer to enlarge the feature map. Soon afterwards, Kim\net al. proposed VDSR [14] and DRCN [15] with deeper net-\nworks on residual learning. Shi et al. [23] replaced decon-\nvolution with the subpixel convolution layer to reduce the\ncheckerboard artifact. Residual block [9] was introduced\ninto SISR in SRResNet [16] and improved in EDSR [17].\nWith the help of residual block, a lot of works focused on\ndesigning deeper or wider networks [2, 27, 28]. Zhang et\nal. [40] and Tong et al. [30] adopted dense blocks [10] to\ncombine features from different levels. Zhang et al. [39]\nimproved residual block by adding channel attention. Liu\net al. [18] proposed a non-local recurrent network for image\nrestoration. Dai et al. [2] introduced second-order statistics\nfor more discriminative feature representations.\nThe above methods use mean square error (MSE) or\nmean absolute error (MAE) as their objective function\nwhich ignores human perceptions. In recent years, more\nand more works aim to improve perceptual quality. John-\nson et al. [13] introduced perceptual loss into SR tasks,\nwhile SRGAN [16] adopted generative adversarial net-\nworks (GANs) [7] and showed visually satisfying results.\nSajjadi et al. [22] used Gram matrix based texture matching\nloss to enforce local similar textures, while ESRGAN [32]\nenhanced SRGAN by introducing RRDB with relativistic\nadversarial loss. Recent proposed RSRGAN [38] trained a\nranker and used rank-content loss to optimize the perceptual\nquality, which achieved state-of-the-art visual results.\n2.2. Reference-based Image Super-Resolution\nDifferent from SISR, RefSR can harvest more accurate\ndetails from the Ref image. This could be done by several\napproaches like image aligning or patch matching. Some\nexisting RefSR approaches [33, 36, 43] choose to align the\nLR and Ref image. Landmark [36] aligned the Ref image\nto the LR image through a global registration to solve an\nenergy minimization problem. Wang et al. [33] enhanced\nthe Ref image by recurrently applying non-uniform warp-\ning before feature synthesis. CrossNet [43] adopted optical\nÔ¨Çow to align the LR and Ref image at different scales and\nconcatenated them into the corresponding layers of the de-\ncoder. However, the performance of these methods depends\nlargely on the aligning quality between the LR and Ref im-\nage. Besides, the aligning approaches such as optical Ô¨Çow\nare time-consuming, which is adverse to real applications.\nOther RefSR approaches [1, 41, 42] adopt ‚Äúpatch\nmatch‚Äù method to search proper reference information.\nBoominathan et al. [1] matched the patches between gra-\ndient features of the LR and down-sampled Ref image.\nZheng. et al. [42] replaced the simple gradient features\nwith features in convolution neural networks to apply se-\nmantic matching and used a SISR method for feature syn-\nthesis. Recent work SRNTT [41] applied patch matching\nbetween VGG [24] features of the LR and Ref image to\nswap similar texture features. However, SRNTT ignores the\nrelevance between original and swapped features and feeds\nall the swapped features equally into the main network.\nTo address these problems, we propose a texture trans-\nformer network which enables our approach to search and\ntransfer relevant textures from Ref to LR images. Moreover,\nthe performance of our approach can be further improved\nby stacking multiple texture transformers with a proposed\ncross-scale feature integration module.\n3. Approach\nIn this section, we introduce the proposed Texture\nTransformer Network for Image Super-Resolution (TTSR).\nOn top of the texture transformer, we propose a cross-scale\nfeature integration module (CSFI) to further enhance model\nperformances. The texture transformer and CSFI will be\ndiscussed in Section 3.1 and Section 3.2, respectively. A\ngroup of loss functions for optimizing the proposed network\nwill be explained in Section 3.3.\n3.1. Texture Transformer\nThe structure of the texture transformer is shown in Fig-\nure 2. LR, LR ‚Üëand Ref represent the input image, the 4√ó\nbicubic-upsampled input image and the reference image, re-\nspectively. We sequentially apply bicubic down-sampling\nand up-sampling with the same factor 4√óon Ref to ob-\ntain Ref‚Üì‚Üëwhich is domain-consistent with LR ‚Üë. The tex-\nture transformer takes Ref, Ref‚Üì‚Üë, LR‚Üëand the LR features\nproduced by the backbone as input, and outputs a synthe-\nsized feature map, which will be further used to generate\nthe HR prediction. There are four parts in the texture trans-\nformer: the learnable texture extractor (LTE), the relevance\nembedding module (RE), the hard-attention module for fea-\nture transfer (HA) and the soft-attention module for feature\nsynthesis (SA). Details will be discussed below.\nF\n3 ‚Ä¶ 5\n... 2 ‚Ä¶\nùëÑùêæùëâ\nRelevance Embedding\nHard Attention\nùëá\nBackbone\nùêπ\nSoft Attention\nOutput\n Texture Transformer\nLearnable Texture Extractor\n0.4 ‚Ä¶ 0.1\n... 0.9 ...\nùêª\nùëÜ\nLR Ref Ref‚Üì‚Üë LR‚Üë\nFigure 2. The proposed texture transformer. Q, K and V are the\ntexture features extracted from an up-sampled LR image, a se-\nquentially down/up-sampled Ref image, and an original Ref im-\nage, respectively. H and S indicate the hard/soft attention map,\ncalculated from relevance embedding. F is the LR features ex-\ntracted from a DNN backbone, and is further fused with the trans-\nferred texture features T for generating the SR output.\nLearnable Texture Extractor. In RefSR tasks, texture ex-\ntraction for reference images is essential because accurate\nand proper texture information will assist the generation of\nSR images. Instead of using semantic features extracted by\na pre-trained classiÔ¨Åcation model like VGG [24], we design\na learnable texture extractor whose parameters will be up-\ndated during end-to-end training. Such a design encourages\na joint feature learning across the LR and Ref image, in\nwhich more accurate texture features can be captured. The\nprocess of texture extraction can be expressed as:\nQ= LTE(LR‚Üë), (1)\nK = LTE(Ref ‚Üì‚Üë), (2)\nV = LTE(Ref), (3)\nwhere LTE(¬∑) denotes the output of our learnable texture\nextractor. The extracted texture features, Q (query), K\n(key), and V (value) indicate three basic elements of the at-\ntention mechanism inside a transformer and will be further\nused in our relevance embedding module.\nRelevance Embedding. Relevance embedding aims to em-\nbed the relevance between the LR and Ref image by esti-\nmating the similarity between Q and K. We unfold both Q\nand K into patches, denoted asqi (i‚àà[1,HLR √óWLR]) and\nkj (j ‚àà[1,HRef √óWRef ]). Then for each patch qi in Q\nand kj in K, we calculate the relevance ri,j between these\ntwo patches by normalized inner product:\nri,j =\n‚ü® qi\n‚à•qi‚à•, kj\n‚à•kj‚à•\n‚ü©\n. (4)\nThe relevance is further used to obtain the hard-attention\nmap and the soft-attention map.\nHard-Attention. We propose a hard-attention module to\ntransfer the HR texture featuresV from the Ref image. Tra-\nditional attention mechanism takes a weighted sum of V\nfor each query qi. However, such an operation may cause\nblur effect which lacks the ability of transferring HR texture\nfeatures. Therefore, in our hard-attention module, we only\ntransfer features from the most relevant position in V for\neach query qi.\nMore speciÔ¨Åcally, we Ô¨Årst calculate a hard-attention map\nH in which the i-th element hi (i ‚àà[1,HLR √óWLR]) is\ncalculated from the relevance ri,j:\nhi = arg max\nj\nri,j. (5)\nThe value of hi can be regarded as a hard index, which rep-\nresents the most relevant position in the Ref image to the\ni-th position in the LR image. To obtain the transferred HR\ntexture features T from the Ref image, we apply an index\nselection operation to the unfolded patches of V using the\nhard-attention map as the index:\nti = vhi , (6)\nwhere ti denotes the value of T in the i-th position, which\nis selected from the hi-th position of V.\nAs a result, we obtain a HR feature representation T\nfor the LR image which will be further used in our soft-\nattention module.\nSoft-Attention. We propose a soft-attention module to syn-\nthesize features from the transferred HR texture features T\nand the LR features F of the LR image from a DNN back-\nbone. During the synthesis process, relevant texture transfer\nshould be enhanced while the less relevant ones should be\nrelived. To achieve that, a soft-attention mapSis computed\nfrom ri,j to represent the conÔ¨Ådence of the transferred tex-\nture features for each position in T:\nsi = max\nj\nri,j, (7)\nwhere si denotes the i-th position of the soft-attention map\nS. Instead of directly applying the attention map S to T,\nwe Ô¨Årst fuse the HR texture features T with the LR fea-\ntures F to leverage more information from the LR image.\nSuch fused features are further element-wisely multiplied\nby the soft-attention map S and added back to F to get the\n1x 1x\n2x\n4x\n‚Ä¶ 1x\n2x\n1x\n2x\n1x\n2x\n4x\n1x\n2x\n4x\nRBs\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nRBsRBs\nTexture \nTransformer\nCSFI CSFI\nTexture \nTransformer\nTexture \nTransformer\nStacked Transformers\nFeed-forward\nDownsample\nUpsample\nOutput\nPixel shuffle\nFigure 3. Architecture of stacking multiple texture transformers in\na cross-scale way with the proposed cross-scale feature integration\nmodule (CSFI). RBs indicates a group of residual blocks.\nÔ¨Ånal output of the texture transformer. This operation can\nbe represented as:\nFout = F + Conv(Concat(F,T )) ‚äôS, (8)\nwhere Fout indicates the synthesized output features. Conv\nand Concat represent a covolutional layer and Concate-\nnation operation, respectively. The operator ‚äôdenotes\nelement-wise multiplication between feature maps.\nIn summary, the texture transformer can effectively\ntransfer relevant HR texture features from the Ref image\ninto the LR features, which boosts a more accurate process\nof texture generation.\n3.2. Cross-Scale Feature Integration\nOur texture transformer can be further stacked in a cross-\nscale way with a cross-scale feature integration module.\nThe architecture is shown in Figure 3. Stacked texture\ntransformers output the synthesized features for three res-\nolution scales ( 1√ó, 2√óand 4√ó), such that the texture fea-\ntures of different scales can be fused into the LR image.\nTo learn a better representation across different scales, in-\nspired by [25, 37], we propose a cross-scale feature inte-\ngration module (CSFI) to exchange information among the\nfeatures of different scales. A CSFI module is applied each\ntime the LR feature is up-sampled to the next scale. For\nthe each scale inside the CSFI module, it receives the ex-\nchanged features from other scales by up/down-sampling,\nfollowed by a concatenation operation in the channel di-\nmension. Then a convolutional layer will map the features\ninto the original number of channels. In such a design, the\ntexture features transferred from the stacked texture trans-\nformers are exchanged across each scale, which achieves a\nmore powerful feature representation. This cross-scale fea-\nture integration module further improves the performance\nof our approach.\n3.3. Loss Function\nThere are 3 loss functions in our approach. The overall\nloss can be interpreted as:\nLoverall = ŒªrecLrec + ŒªadvLadv + ŒªperLper. (9)\nReconstruction loss. The Ô¨Årst loss is the reconstruction\nloss:\nLrec = 1\nCHW\nÓµπÓµπIHR ‚àíISRÓµπÓµπ\n1 , (10)\nwhere (C,H,W ) is the size of the HR. We utilize L1 loss\nwhich has been demonstrated to be sharper for performance\nand easier for convergence compared to L2 loss.\nAdversarial loss. Generative adversarial networks [7] are\nproved effective in generating clear and visually favorable\nimages. Here we adopt WGAN-GP [8], which proposes a\npenalization of gradient norm to replace weight clipping, re-\nsulting in more stable training and better performance. This\nloss can be interpreted as:\nLD = E\nÀúx‚àºPg\n[\nD(Àúx)\n]\n‚àí E\nx‚àºPr\n[\nD(x)\n]\n+\nŒª E\nÀÜx‚àºPÀÜx\n[\n(‚à•‚àáÀÜxD(ÀÜx)‚à•2 ‚àí1)2]\n, (11)\nLG = ‚àí E\nÀúx‚àºPg\n[\nD(Àúx)\n]\n. (12)\nPerceptual loss. Perceptual loss has been demonstrated\nuseful to improve visual quality and has already been used\nin [13, 16, 22, 41]. The key idea of perceptual loss is to\nenhance the similarity in feature space between the predic-\ntion image and the target image. Here our perceptual loss\ncontains two parts:\nLper = 1\nCiHiWi\nÓµπÓµπœÜvgg\ni (ISR) ‚àíœÜvgg\ni (IHR)\nÓµπÓµπ2\n2 +\n1\nCjHjWj\nÓµπÓµπœÜlte\nj (ISR) ‚àíT\nÓµπÓµπ2\n2 , (13)\nwhere the Ô¨Årst part is a traditional perceptual loss, in which\nœÜvgg\ni (¬∑) denotes the i-th layer‚Äôs feature map of VGG19, and\n(Ci,Hi,Wi) represents the shape of the feature map at that\nlayer. ISR is the predicted SR image. The second part in\nour perceptual loss is a transferal perceptual loss, in which\nœÜlte\nj (¬∑) denotes the texture feature map extracted from thej-\nth layer of the proposed LTE, and (Cj,Hj,Wj) represents\nthat layer‚Äôs shape. T is the transferred HR texture features\nin Figure 2. This transferal perceptual loss constraints the\npredicted SR image to have similar texture features to the\ntransferred texture features T, which makes our approach\nto transfer the Ref textures more effectively.\n3.4. Implementation Details\nThe learnable texture extractor contains 5 convolutional\nlayers and 2 pooling layers which outputs texture features\nin three different scales. To reduce the consumption of both\ntime and GPU memory, the relevance embedding is only\napplied to the smallest scale and further propagated to other\nscales. For the discriminator, we adopt the same network\nused in SRNTT [41] and remove all BN layers. During\ntraining, we augment the training images by randomly hor-\nizontally and vertically Ô¨Çipping followed by randomly ro-\ntating 90‚ó¶, 180‚ó¶ and 270‚ó¶. Each mini-batch contains 9 LR\npatches with size 40 √ó40 along with 9 HR and Ref patches\nwith size 160 √ó160. The weight coefÔ¨Åcients for Lrec, Ladv\nand Lper are 1, 1e-3 and 1e-2, respectively. Adam opti-\nmizer with Œ≤1 = 0.9, Œ≤2 = 0.999, and œµ=1e-8 is used with\nlearning rate of 1e-4. We Ô¨Årst warm up the network for 2\nepochs where only Lrec is applied. After that, all losses are\ninvolved to train another 50 epochs.\n4. Experiments\n4.1. Datasets and Metrics\nTo evaluate our method, we train and test our model on\nthe recently proposed RefSR dataset, CUFED5 [41]. The\ntraining set in CUFED5 contains 11,871 pairs, each pair\nconsisting of an input image and a reference image. There\nare 126 testing images in CUFED5 testing set, each accom-\npanied by 4 reference images with different similarity lev-\nels. In order to evaluate the generalization performance of\nTTSR trained on CUFED5, we additionally test TTSR on\nSun80 [26], Urban100 [11], and Manga109 [20]. Sun80\ncontains 80 natural images, each paired with several ref-\nerence images. For Urban100, we use the same setting\nas [41] to regard its LR images as the reference images.\nSuch a design enables an explicit process of self-similar\nsearching and transferring since Urban100 are all building\nimages with strong self-similarity. For Manga109 which\nalso lacks the reference images, we randomly sample HR\nimages in this dataset as the reference images. Since this\ndataset is constructed with lines, curves and Ô¨Çat colored\nregions which are all common patterns. Even with a ran-\ndomly picked HR Ref image, our method can still utilize\nthese common patterns and achieve good results. The SR\nresults are evaluated on PSNR and SSIM on Y channel of\nYCbCr space.\n4.2. Evaluation\nTo evaluate the effectiveness of TTSR, we compare our\nmodel with other state-of-the-art SISR and RefSR meth-\nods. The SISR methods include SRCNN [3], MDSR [17],\nRDN [40], RCAN [39], SRGAN [16], ENet [22], ES-\nRGAN [32], RSRGAN [38], among which RCAN has\nachieved state-of-the-art performance on both PSNR and\nSSIM in recent years. RSRGAN is considered to achieve\nthe state-of-the-art visual quality. As for RefSR meth-\nods, CrossNet [43] and SRNTT [41] are two state-of-the-art\nmethods recently, which signiÔ¨Åcantly outperform previous\nRefSR methods. All experiments are performed with a scal-\ning factor of 4√óbetween LR and HR images.\nQuantitative Evaluation. For fair comparison, we fol-\nlow the setting in SRNTT [41] to train all the methods\nTable 1. PSNR/SSIM comparison among different SR methods on\nfour different datasets. Methods are grouped by SISR methods\n(top) and RefSR methods (down). Red numbers denote the highest\nscores while blue numbers denote the second highest scores.\nMethod CUFED5 Sun80 Urban100 Manga109\nSRCNN [3] 25.33 / .745 28.26 / .781 24.41 / .738 27.12 / .850\nMDSR [17] 25.93 / .777 28.52 / .792 25.51 / .783 28.93 / .891\nRDN [40] 25.95 / .769 29.63 / .806 25.38 / .768 29.24 / .894\nRCAN [39] 26.06 / .769 29.86 / .810 25.42 / .768 29.38 / .895\nSRGAN [16] 24.40 / .702 26.76 / .725 24.07 / .729 25.12 / .802\nENet [22] 24.24 / .695 26.24 / .702 23.63 / .711 25.25 / .802\nESRGAN [32]21.90 / .633 24.18 / .651 20.91 / .620 23.53 / .797\nRSRGAN [38]22.31 / .635 25.60 / .667 21.47 / .624 25.04 / .803\nCrossNet [43] 25.48 / .764 28.52 / .793 25.11 / .764 23.36 / .741\nSRNTT-rec[41] 26.24 / .784 28.54 / .793 25.50 / .783 28.95 / .885\nSRNTT [41] 25.61 / .764 27.59 / .756 25.09 / .774 27.54 / .862\nTTSR-rec 27.09 / .804 30.02 / .814 25.87 / .784 30.09 / .907\nTTSR 25.53 / .765 28.59 / .774 24.62 / .747 28.70 / .886\non CUFED5 training set, and test on CUFED5 testing set,\nSun80, Urban100 and Manga109 datasets. For SR meth-\nods, there is a fact that training with adversarial loss usu-\nally achieves better visual quality but shrinks the number\nof PSNR and SSIM. Therefore, we train another version of\nour model which is optimized only on reconstruction loss\nnamed TTSR-rec for fair comparison on PSNR and SSIM.\nTable 1 shows the quantitative evaluation results. Red\nnumbers denote the highest scores while blue numbers de-\nnote the second highest scores. As shown in the com-\nparison results, TTSR- rec signiÔ¨Åcantly outperforms both\nstate-of-the-art SISR methods and state-of-the-art RefSR\nmethods on all four testing datasets. Among the meth-\nods which aim to achieve better visual quality with ad-\nversarial loss, our model still has the best performance on\nSun80 and Manga109 datasets. On the other two datasets,\nCUFED5 and Urban100, our model achieves comparable\nperformance with the state-of-the-art models. The quanti-\ntative comparison results demonstrate the superiority of our\nproposed TTSR over state-of-the-art SR approaches.\nQualitative Evaluation. Our model also achieves the best\nperformance on visual quality as shown in Figure 5. TTSR\ncan transfer more accurate HR textures from the reference\nimage to generate favorable results, as shown in the Ô¨Årst\nthree examples in Figure 5. Even if the reference image\nis not that globally relevant to the input image, our TTSR\ncan still extract Ô¨Åner textures from local regions and transfer\neffective textures into the predicted SR result, as shown in\nthe last three examples in Figure 5.\nTo further verify the superior visual quality of our ap-\nproach, we conduct a user study where TTSR is compared\nwith four SOTA approaches, including RCAN [39], RSR-\nGAN [38], CrossNet [43] and SRNTT [41]. There are 10\nsubjects involved in this user study and 2,520 votes are col-\nlected on the CUFED5 testing set. For each comparison\nprocess, we provide the users with two images which in-\nclude one TTSR image. Users are asked to select the one\nwith higher visual quality. Figure 4 shows the results of our\nRCAN RSRGAN CrossNet SRNTT\n86%\n88%\n90%\n92%\n94%\n96%Percentage\ni = 0\nxi\n93.6 ¬± 0.9%\n90.8 ¬± 0.5%\n92.6 ¬± 1.1%\n90.7 ¬± 0.6%\nFigure 4. User study results. Values on Y-axis indicate the percent-\nage of users that prefer TTSR over other approaches.\nTable 2. Ablation study on texture transformer.\nMethod HA SA LTE PSNR/SSIM\nBase 26.34 / .780\nBase+HA ‚úì 26.59 / .786\nBase+HA+SA ‚úì ‚úì 26.81 / .795\nBase+HA+SA+LTE ‚úì ‚úì ‚úì 26.92 / .797\nTable 3. Ablation study on CSFI.\nMethod CSFI numC param. PSNR/SSIM\nBase+TT 64 4.42M 26.92 / .797\nBase+TT+CSFI ‚úì 64 6.42M 27.09 / .804\nBase+TT(C80) 80 6.53M 26.93 / .797\nBase+TT(C96) 96 9.10M 26.98 / .799\nuser study, where the values on Y-axis represent the percent-\nage of users that prefer TTSR over other approaches. As we\ncan see, the proposed TTSR signiÔ¨Åcantly outperforms other\napproaches with over 90% of users voting for ours, which\nveriÔ¨Åes the favorable visual quality of TTSR.\n4.3. Ablation Study\nIn this section, we verify the effectiveness of different\nmodules in our approach, including the texture transformer,\nthe cross-scale feature integration, the adversarial loss and\nthe transferal perceptual loss. In addition, we also discuss\nthe inÔ¨Çuence of different reference similarity on TTSR.\nTexture transformer. Our texture transformer contains\nmainly four parts: the learnable texture extractor (LTE),\nthe relevance embedding module, the hard-attention mod-\nule for feature transfer (HA) and the soft-attention module\nfor feature synthesis (SA). Ablation results are shown in Ta-\nble 2. We re-implement SRNTT [41] as our ‚ÄúBase‚Äù model\nby only removing all BN layers and Ref part. On top of the\nbaseline model, we progressively add HA, SA, and LTE.\nModels without LTE use the VGG19 features to do rele-\nvance embedding. As we can see, when HA is added, the\nPSNR performance can be improved from 26.34 to 26.59,\nwhich veriÔ¨Åes the effectiveness of the hard-attention mod-\nule for feature transfer. When SA is involved, relevant tex-\nture features will be enhanced while the less relevant ones\nwill be relieved during the feature synthesizing. This fur-\nGround-truth RDN [40] RCAN [39] RSRGAN [38]\nReference CrossNet [43] SRNTT [41] TTSR (Ours)\nFigure 5. Visual comparison among different SR methods on CUFED5 testing set (top three examples), Sun80 [26] (the forth example),\nUrban100 [11] (the Ô¨Åfth example), and Manga109 [20] (the last example).\nther boosts the performance to26.81. When replacing VGG\nwith the proposed LTE, the PSNR is Ô¨Ånally increased to\n26.92, which proves the superiority of joint feature embed-\nding in LTE.\nTo further verify the effectiveness of our LTE, we use the\nhard attention map to transfer the original image. It is ex-\npected that a better feature representation can transfer more\naccurate textures from the original images. Figure 6 shows\nthe transferred original image by VGG19 in SRNTT and\nLTE in TTSR. In this Ô¨Ågure, TTSR can transfer more ac-\ncurate reference textures and generate a globally favorable\nresult, which further proves the effectiveness of our LTE.\nCross-scale feature integration. On top of the texture\ntransformer, CSFI can further enable texture recovery from\ndifferent resolution scales ( 1√ó, 2√óand 4√ó). We conduct\nan ablation study in Table 3. The Ô¨Årst row shows the per-\nformance of our model with only TT, while the second\nrow proves the effectiveness of CSFI, which brings 0.17\nGT GT SRNTT TTSR(ours)\nFigure 6. Comparison of the transferred original images between\nSRNTT and TTSR.\nReference\nBase-rec\n Base\n TTSR-rec\n TTSR\n GT\nGT\nFigure 7. Qualitative comparison on ‚ÄúBase(-rec)‚Äù and TTSR(-rec)\n(TTSR can be interpreted as ‚ÄúBase+TT(HA+SA+LTE)+CSFI‚Äù).\nincrease on PSNR metric. In order to verify that the per-\nformance improvement is not brought by the increase of pa-\nrameter size, we increase the channel number of ‚ÄúBase+TT‚Äù\nmodel to 80 and 96. As we can see, there is almost no\ngrowth of ‚ÄúBase+TT(C80)‚Äù which has almost the same pa-\nrameter number as ‚ÄúBase+TT+CSFI‚Äù. Even if we increase\nthe parameter number to 9.10M to obtain ‚ÄúBase+TT(C96)‚Äù\nmodel, there is still a performance gap. This demonstrates\nthat CSFI can efÔ¨Åciently utilize the reference texture infor-\nmation with a relatively smaller parameter size.\nAdversarial loss. To make sure that the improvement of\nperceptual quality beneÔ¨Åts from model design rather than\nthe adversarial loss. We conduct an ablation among ‚ÄúBase-\nrec‚Äù, ‚ÄúBase‚Äù, TTSR-rec and TTSR, where TTSR can be in-\nterpreted as ‚ÄúBase+TT+CSFI‚Äù and ‚Äú-rec‚Äù indicates training\nwith only reconstruction loss. Figure 7 shows that even if\nwithout the perceptual and adversarial loss, TTSR- rec can\nstill utilize the Ref image and recover more details than\n‚ÄúBase-rec‚Äù. With all losses enabled, TTSR achieves the best\nvisual result.\nTransferal perceptual loss. The transferal perceptual loss\nconstraints the LTE‚Äôs features between the predicted SR im-\nage and the transferred image T to be similar. As shown\nin Figure 8, using this loss is able to transfer textures in a\nmore effective way which achieves visually pleasing results.\nIn addition, this loss also improves the quantitative metrics\nPSNR and SSIM of TTSR from 25.20/.757 to 25.53/.765.\nInÔ¨Çuence of different reference similarity. To study how\nrelevance between LR and Ref images inÔ¨Çuences the re-\nsults of TTSR, we conduct experiments on CUFED5 test-\ning set, which has reference images of different relevance\nFigure 8. Comparison between TTSR trained without (top) and\nwith (bottom) transferal perceptual loss.\nTable 4. Ablation study on reference images of different similarity.\nLevel CrossNet SRNTT-rec TTSR-rec\nL1 25.48 / .764 26.15 / .781 26.99 / .800\nL2 25.48 / .764 26.04 / .776 26.74 / .791\nL3 25.47 / .763 25.98 / .775 26.64 / .788\nL4 25.46 / .763 25.95 / .774 26.58 / .787\nLR 25.46 / .763 25.91 / .776 26.43 / .782\nlevels. Table 4 shows the results of Ô¨Åve relevance levels,\nin which ‚ÄúL1‚Äù to ‚ÄúL4‚Äù represent the reference images pro-\nvided by CUFED5 testing set where L1 is the most relevant\nlevel while L4 is the least relevant one. ‚ÄúLR‚Äù means using\nthe input image itself as the reference image. As shown in\nTable 4, TTSR using L1 as the reference image achieves\nthe best performance. When using LR as the reference im-\nage, TTSR still performs better than previous state-of-the-\nart RefSR approaches.\n5. Conclusion\nIn this paper, we propose a novel Texture Transformer\nNetwork for Image Super-Resolution (TTSR) which trans-\nfers HR textures from the Ref to LR image. The proposed\ntexture transformer consists of a learnable texture extractor\nwhich learns a jointly feature embedding for further atten-\ntion computation and two attention based modules which\ntransfer HR textures from the Ref image. Furthermore, the\nproposed texture transformer can be stacked in a cross-scale\nway with the proposed CSFI module to learn a more pow-\nerful feature representation. Extensive experiments demon-\nstrate the superior performance of our TTSR over state-of-\nthe-art approaches on both quantitative and qualitative eval-\nuations. In the future, we will further extend the proposed\ntexture transformer to general image generation tasks.\nAcknowledgement This paper is partially supported by\nNSFC (No. 61772330, 61533012, 61876109), the pre-\nresearch project (No. 61403120201), Shanghai Key Labo-\nratory of Crime Scene Evidence (2017XCWZK01) and the\nInterdisciplinary Program of Shanghai Jiao Tong University\n(YG2019QNA09).\nReferences\n[1] Vivek Boominathan, Kaushik Mitra, and Ashok Veeraragha-\nvan. Improving resolution and depth-of-Ô¨Åeld of light Ô¨Åeld\ncameras using a hybrid imaging system. In ICCP, pages 1‚Äì\n10, 2014.\n[2] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and\nLei Zhang. Second-order attention network for single image\nsuper-resolution. In CVPR, pages 11065‚Äì11074, 2019.\n[3] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Image super-resolution using deep convolutional net-\nworks. TPAMI, 38(2):295‚Äì307, 2015.\n[4] Chao Dong, Chen Change Loy, and Xiaoou Tang. Acceler-\nating the super-resolution convolutional neural network. In\nECCV, pages 391‚Äì407, 2016.\n[5] Gilad Freedman and Raanan Fattal. Image and video upscal-\ning from local self-examples. ACM Transactions on Graph-\nics (TOG), 30(2):1‚Äì11, 2011.\n[6] William T Freeman, Thouis R Jones, and Egon C Pasztor.\nExample-based super-resolution. IEEE Computer graphics\nand Applications, 22(2):56‚Äì65, 2002.\n[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\npages 2672‚Äì2680, 2014.\n[8] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent\nDumoulin, and Aaron C Courville. Improved training of\nwasserstein gans. In NeurIPS, pages 5767‚Äì5777, 2017.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770‚Äì778, 2016.\n[10] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In CVPR, pages 4700‚Äì4708, 2017.\n[11] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single\nimage super-resolution from transformed self-exemplars. In\nCVPR, pages 5197‚Äì5206, 2015.\n[12] Michal Irani and Shmuel Peleg. Improving resolution by\nimage registration. CVGIP, 53(3):231‚Äì239, 1991.\n[13] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nECCV, pages 694‚Äì711, 2016.\n[14] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate\nimage super-resolution using very deep convolutional net-\nworks. In CVPR, pages 1646‚Äì1654, 2016.\n[15] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-\nrecursive convolutional network for image super-resolution.\nIn CVPR, pages 1637‚Äì1645, 2016.\n[16] Christian Ledig, Lucas Theis, Ferenc Husz¬¥ar, Jose Caballero,\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\nAlykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-\nrealistic single image super-resolution using a generative ad-\nversarial network. In CVPR, pages 4681‚Äì4690, 2017.\n[17] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for single\nimage super-resolution. In CVPR Workshops, pages 136‚Äì\n144, 2017.\n[18] Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and\nThomas S Huang. Non-local recurrent network for image\nrestoration. In NeurIPS, pages 1673‚Äì1682, 2018.\n[19] Shuang Ma, Jianlong Fu, Chang Wen Chen, and Tao Mei.\nDa-gan: Instance-level image translation by deep attention\ngenerative adversarial networks. In CVPR, pages 5657‚Äì\n5666, 2018.\n[20] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto,\nToru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa.\nSketch-based manga retrieval using manga109 dataset. Mul-\ntimedia Tools and Applications, 76(20):21811‚Äì21838, 2017.\n[21] Ozan Oktay, Wenjia Bai, Matthew Lee, Ricardo Guerrero,\nKonstantinos Kamnitsas, Jose Caballero, Antonio de Mar-\nvao, Stuart Cook, Declan ORegan, and Daniel Rueckert.\nMulti-input cardiac image super-resolution using convolu-\ntional neural networks. In MICCAI, pages 246‚Äì254, 2016.\n[22] Mehdi SM Sajjadi, Bernhard Scholkopf, and Michael\nHirsch. Enhancenet: Single image super-resolution through\nautomated texture synthesis. In ICCV, pages 4491‚Äì4500,\n2017.\n[23] Wenzhe Shi, Jose Caballero, Ferenc Husz ¬¥ar, Johannes Totz,\nAndrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan\nWang. Real-time single image and video super-resolution\nusing an efÔ¨Åcient sub-pixel convolutional neural network. In\nCVPR, pages 1874‚Äì1883, 2016.\n[24] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[25] Ke Sun, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao,\nDong Liu, Yadong Mu, Xinggang Wang, Wenyu Liu, and\nJingdong Wang. High-resolution representations for labeling\npixels and regions. arXiv preprint arXiv:1904.04514, 2019.\n[26] Libin Sun and James Hays. Super-resolution from internet-\nscale scene matching. In ICCP, pages 1‚Äì12, 2012.\n[27] Ying Tai, Jian Yang, and Xiaoming Liu. Image super-\nresolution via deep recursive residual network. In CVPR,\npages 3147‚Äì3155, 2017.\n[28] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-\nnet: A persistent memory network for image restoration. In\nICCV, pages 4539‚Äì4547, 2017.\n[29] Radu Timofte, Vincent De Smet, and Luc Van Gool.\nAnchored neighborhood regression for fast example-based\nsuper-resolution. In ICCV, pages 1920‚Äì1927, 2013.\n[30] Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao. Image\nsuper-resolution using dense skip connections. In ICCV,\npages 4799‚Äì4807, 2017.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, pages\n5998‚Äì6008, 2017.\n[32] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,\nChao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-\nhanced super-resolution generative adversarial networks. In\nECCV Workshops, 2018.\n[33] Yuwang Wang, Yebin Liu, Wolfgang Heidrich, and Qionghai\nDai. The light Ô¨Åeld attachment: Turning a dslr into a light\nÔ¨Åeld camera using a low budget camera ring. IEEE TVCG,\n23(10):2357‚Äì2364, 2016.\n[34] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In CVPR, pages 1316‚Äì1324, 2018.\n[35] Deniz Yƒ±ldƒ±rƒ±m and O Àòguz G ¬®ung¬®or. A novel image fusion\nmethod using ikonos satellite images. Journal of Geodesy\nand Geoinformation, 1(1):75‚Äì83, 2012.\n[36] Huanjing Yue, Xiaoyan Sun, Jingyu Yang, and Feng Wu.\nLandmark image super-resolution by retrieving web images.\nIEEE TIP, 22(12):4865‚Äì4878, 2013.\n[37] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining\nGuo. Learning pyramid-context encoder network for high-\nquality image inpainting. In CVPR, pages 1486‚Äì1494, 2019.\n[38] Wenlong Zhang, Yihao Liu, Chao Dong, and Yu Qiao.\nRanksrgan: Generative adversarial networks with ranker for\nimage super-resolution. In ICCV, pages 3096‚Äì3105, 2019.\n[39] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image super-resolution using very deep\nresidual channel attention networks. In ECCV, pages 286‚Äì\n301, 2018.\n[40] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image super-resolution.\nIn CVPR, pages 2472‚Äì2481, 2018.\n[41] Zhifei Zhang, Zhaowen Wang, Zhe Lin, and Hairong Qi. Im-\nage super-resolution by neural texture transfer. In CVPR,\npages 7982‚Äì7991, 2019.\n[42] Haitian Zheng, Mengqi Ji, Lei Han, Ziwei Xu, Haoqian\nWang, Yebin Liu, and Lu Fang. Learning cross-scale cor-\nrespondence and patch-based synthesis for reference-based\nsuper-resolution. In BMVC, 2017.\n[43] Haitian Zheng, Mengqi Ji, Haoqian Wang, Yebin Liu, and Lu\nFang. Crossnet: An end-to-end reference-based super reso-\nlution network using cross-scale warping. In ECCV, pages\n88‚Äì104, 2018.\nSupplementary\nIn this supplementary material, Section A illustrates the\ndetails of TTSR‚Äôs network structure. Section B provides ad-\nditional analyses about the texture transformers on different\nscales. Section C describes the comparison of the running\ntime and the parameter number. Finally, more visual com-\nparison results will be shown in Section D.\nA. Details of Network Structure\nIn this section, we will illustrate the detailed network\nstructure of our approach TTSR, including the learnable\ntexture extractor in the texture transformer, the generator\nwith three stacked texture transformers and the discrimina-\ntor. The structure of the learable texture extractor is shown\nin Table A.1, in which the layers $0, $3 and $6 are used\nto search and transfer texture features in the texture trans-\nformer. Table A.2 shows the details of the generator, and\nTable A.3 illustrates the discriminator.\nTable A.1. Network structure of the learnable texture extractor.\nConv(Nin, Nout) indicates the convolutional layer with Nin in-\nput channels and Nout output channels. The kernel size is 3 √ó3\nfor all convolutional layers. Pool(2 √ó2) is the 2 √ó2 pooling layer\nwith stride 2.\nId Layer Name\n0 Conv(3,64), ReLU\n1 Conv(64,64), ReLU\n2 Pool(2 √ó2)\n3 Conv(64,128), ReLU\n4 Conv(128,128), ReLU\n5 Pool(2 √ó2)\n6 Conv(128, 256), ReLU\nB. Texture Transformers on Different Scales\nOur proposed TTSR contains three stacked texture trans-\nformers. The texture transformer at each scale fuses HR tex-\nture features of different levels from the Ref image. Here\nwe conduct experiments of using the texture transformers\non different scales. The model here is without CSFI since\nCSFI is designed for multi-scale stacked texture transform-\ners. Table B.1 shows the results. The larger scale the texture\ntransformer is applied at, the more performance it brings,\nwhich demonstrates that the texture features at a larger scale\nhave a less loss of details. When we gradually add the tex-\nture transformers at other scales, the performance can be\nfurther improved.\nC. Running Time and Model Size\nIn this section, the running time and the model size\nof TTSR will be discussed. We compare the pro-\nposed TTSR with state-of-the-art SISR and RefSR ap-\nproaches, RCAN [39], RSRGAN [38], CrossNet [43] and\nSRNTT [41]. For running time, all approaches are run on\na Tesla V100 PCIe GPU and tested on an 83 √ó125 √ó3\nLR input image with the up-sampling factor of 4√ó. Ta-\nble C.1 shows the comparison results. SpeciÔ¨Åcally, the\nstacked texture transformers cost 0.037s and the other parts\ncost 0.059s, and TTSR takes a total time of 0.096s. The re-\nsults show that TTSR achieves the best performance with a\nrelatively small parameter number and running time.\nD. More Visual Comparison\nIn this section, we show more comparison results among\nthe proposed TTSR and other SR methods, including\nRDN [40], RCAN [39], RSRGAN [38], CrossNet [43]\nand SRNTT [41]. RCAN has achieved state-of-the-art\nperformance on both PSNR and SSIM in recent years\nand RSRGAN is considered to achieve the state-of-the-\nart visual quality. CrossNet and SRNTT are two state-\nof-the-art RefSR approaches which signiÔ¨Åcantly outper-\nform previous RefSR methods. The visual comparison re-\nsults on CUFED5 [41], Sun80 [26], Urban100 [11] and\nManga109 [20] are shown in Figure D.1-D.4, Figure D.5-\nD.6, Figure D.7-D.8 and Figure D.9-D.10, respectively.\nTable A.2. Network structure of the generator. Conv(Nin, Nout) indicates the convolutional layer withNin input channels and Nout output\nchannels. The kernel size is 3 √ó3 for all convolutional layers except that the last convolution uses 1 √ó1 kernel. RB denotes the residual\nblock without batch normalization layers and the ReLU layer after the skip connection. TT represents the texture transformer and PS is the\n2√ópixel shufÔ¨Çe layer. ‚Üëindicates bicubic up-sampling followed by a 1 √ó1 convolution, while ‚Üìdenotes the strided convolution.\nId Layer Name (scale1√ó) Id Layer Name (scale2√ó) Id Layer Name (scale4√ó)\nStage0\n1-0 Conv(3,64), ReLU\n1-1 RB√ó16\n1-2 Conv(64,64)\n1-3 $1-0 + $1-2\nStage1\n1-4 TT\n1-5 RB√ó16\n1-6 Conv(64,64)\n1-7 $1-4 + $1-6\nStage2\n2-0 Conv(64,256), PS, ReLU($1-7)\n2-1 TT\n1-8 Concat($1-7||$2-1‚Üì) 2-2 Concat($1-7‚Üë||$2-1)\n1-9 Conv(128,64), ReLU 2-3 Conv(128,64), ReLU\n1-10 RB√ó8 2-4 RB√ó8\n1-11 Conv(64,64) 2-5 Conv(64,64)\n1-12 $1-7 + $1-11 2-6 $2-1 + $2-5\nStage3\n4-0 Conv(64,256), PS, ReLU($2-6)\n4-1 TT\n1-13 Concat($1-12||$2-6‚Üì||$4-1‚Üì) 2-7 Concat($1-12‚Üë||$2-6||$4-1‚Üì) 4-2 Concat($1-12‚Üë||$2-6‚Üë||$4-1 )\n1-14 Conv(192,64), ReLU 2-8 Conv(192,64), ReLU 4-3 Conv(192,64), ReLU\n1-15 RB√ó4 2-9 RB√ó4 4-4 RB√ó4\n1-16 Conv(64,64) 2-10 Conv(64,64) 4-5 Conv(64,64)\n1-17 $1-12 + $1-16 2-11 $2-6 + $2-10 4-6 $4-1 + $4-5\nStage4\n4-7 Concat($1-17‚Üë||$2-11‚Üë||$4-6)\n4-8 Conv(192,64), ReLU\n4-9 Conv(64,32)\n4-10 Conv(32,3)\nTable A.3. Network structure of the discriminator. Conv( Nin,\nNout, S) indicates the convolutional layer with Nin input chan-\nnels, Nout output channels and stride S. The kernel size is 3 √ó3\nfor all convolutional layers. The parameter is 0.2 for all the leaky\nReLU layers. The size of HR and SR input is 160 √ó160 √ó3\nId Layer Name\n0 Conv(3,32,1), LReLU\n1 Conv(32,32,2), LReLU\n2 Conv(32,64,1), LReLU\n3 Conv(64,64,2), LReLU\n4 Conv(64,128,1), LReLU\n5 Conv(128,128,2), LReLU\n6 Conv(128,256,1), LReLU\n7 Conv(256,256,2), LReLU\n8 Conv(256,512,1), LReLU\n9 Conv(512,512,2), LReLU\n10 FC(12800,1024), LReLU\n11 FC(1024,1)\nTable B.1. Performance on CUFED5 testing set using texture\ntransformers on different scales.\nscale1√ó scale2√ó scale4√ó PSNR/SSIM\n‚úì 26.56 / .785\n‚úì 26.77 / .793\n‚úì 26.85 / .796\n‚úì ‚úì 26.80 / .793\n‚úì ‚úì ‚úì 26.92 / .797\nTable C.1. Running time and parameter number of different ap-\nproaches. The last column shows the PSNR/SSIM performance\non CUFED5 testing set.\nApproach Time Param. PSNR/SSIM\nRCAN [39] 0.108s 16M 26.06 / .769\nRSRGAN [38] 0.007s 1.5M 22.31 / .635\nCrossNet [43] 0.229s 33.6M 25.48 / .764\nSRNTT [41] 4.977s 4.2M 26.24 / .784\nTTSR 0.096s 6.4M 27.09 / .804\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.1. Visual comparison of different SR methods on CUFED5 [41] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.2. Visual comparison of different SR methods on CUFED5 [41] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.3. Visual comparison of different SR methods on CUFED5 [41] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.4. Visual comparison of different SR methods on CUFED5 [41] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.5. Visual comparison of different SR methods on Sun80 [26] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.6. Visual comparison of different SR methods on Sun80 [26] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.7. Visual comparison of different SR methods on Urban100 [11] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.8. Visual comparison of different SR methods on Urban100 [11] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.9. Visual comparison of different SR methods on Manga109 [20] dataset.\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nGT Reference Bicubic\nRDN RCAN RSRGAN\nCrossNet SRNTT TTSR(Ours)\nFigure D.10. Visual comparison of different SR methods on Manga109 [20] dataset.",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7082056999206543
    },
    {
      "name": "Computer science",
      "score": 0.6916393041610718
    },
    {
      "name": "Transformer",
      "score": 0.6623531579971313
    },
    {
      "name": "Embedding",
      "score": 0.6026655435562134
    },
    {
      "name": "Extractor",
      "score": 0.5728139877319336
    },
    {
      "name": "Image texture",
      "score": 0.5073739886283875
    },
    {
      "name": "Computer vision",
      "score": 0.4827517867088318
    },
    {
      "name": "Magnification",
      "score": 0.47865307331085205
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45569485425949097
    },
    {
      "name": "Texture (cosmology)",
      "score": 0.41613149642944336
    },
    {
      "name": "Image (mathematics)",
      "score": 0.36139360070228577
    },
    {
      "name": "Image processing",
      "score": 0.24056077003479004
    },
    {
      "name": "Engineering",
      "score": 0.07740429043769836
    },
    {
      "name": "Process engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ]
}