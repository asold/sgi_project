{
    "title": "Transformer-Based Neural Network for Answer Selection in Question Answering",
    "url": "https://openalex.org/W2917049430",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A3068923015",
            "name": "Taihua Shao",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2112013725",
            "name": "Yupu Guo",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2102471553",
            "name": "Honghui Chen",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2799506892",
            "name": "Zepeng Hao",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3068923015",
            "name": "Taihua Shao",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2112013725",
            "name": "Yupu Guo",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2102471553",
            "name": "Honghui Chen",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2799506892",
            "name": "Zepeng Hao",
            "affiliations": [
                "National University of Defense Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2251818205",
        "https://openalex.org/W6678170489",
        "https://openalex.org/W6677100224",
        "https://openalex.org/W1971844566",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W2103452139",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6678382684",
        "https://openalex.org/W6630841884",
        "https://openalex.org/W6682691769",
        "https://openalex.org/W2964154091",
        "https://openalex.org/W2907588548",
        "https://openalex.org/W2301095666",
        "https://openalex.org/W2511929605",
        "https://openalex.org/W2106121453",
        "https://openalex.org/W2517148058",
        "https://openalex.org/W2171278097",
        "https://openalex.org/W6691429666",
        "https://openalex.org/W6677436410",
        "https://openalex.org/W1966443646",
        "https://openalex.org/W1678356000",
        "https://openalex.org/W2251202616",
        "https://openalex.org/W1591825359",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2118091490",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2613631036",
        "https://openalex.org/W2951528484",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2120735855",
        "https://openalex.org/W2125313055",
        "https://openalex.org/W2173361515",
        "https://openalex.org/W1514986335",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2112729630",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2251921768",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2264105282",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2172140247",
        "https://openalex.org/W1940872118"
    ],
    "abstract": "Answer selection is a crucial subtask in the question answering (QA) system. Conventional avenues for this task mainly concentrate on developing linguistic tools that are limited in both performance and practicability. Answer selection approaches based on deep learning have been well investigated with the tremendous success of deep learning in natural language processing. However, the traditional neural networks employed in existing answer selection models, i.e., recursive neural network or convolutional neural network, typically suffer from obtaining the global text information due to their operating mechanisms. The recent Transformer neural network is considered to be good at extracting the global information by employing only self-attention mechanism. Thus, in this paper, we design a Transformer-based neural network for answer selection, where we deploy a bidirectional long short-term memory (BiLSTM) behind the Transformer to acquire both global information and sequential features in the question or answer sentence. Different from the original Transformer, our Transformer-based network focuses on sentence embedding rather than the seq2seq task. In addition, we employ a BiLSTM rather than utilizing the position encoding to incorporate sequential features as the universal Transformer does. Furthermore, we apply three aggregated strategies to generate sentence embeddings for question and answer, i.e., the weighted mean pooling, the max pooling, and the attentive pooling, leading to three corresponding Transformer-based models, i.e., QA-TF<sub>WP</sub>, QA-TF<sub>MP</sub>, and QA-TF<sub>AP</sub>, respectively. Finally, we evaluate our proposals on a popular QA dataset WikiQA. The experimental results demonstrate that our proposed Transformer-based answer selection models can produce a better performance compared with several competitive baselines. In detail, our best model outperforms the state-of-the-art baseline by up to 2.37%, 2.83%, and 3.79% in terms of MAP, MRR, and accuracy, respectively.",
    "full_text": "Received January 31, 2019, accepted February 19, 2019, date of publication February 21, 2019, date of current version March 8, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2900753\nTransformer-Based Neural Network for Answer\nSelection in Question Answering\nTAIHUA SHAO\n , YUPU GUO, HONGHUI CHEN, AND ZEPENG HAO\nScience and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha 410073, China\nCorresponding author: Yupu Guo (hunsth@outlook.com)\nThis work was supported in part by the National Natural Science Foundation of China under Grant No. 61702526, in part by the National\nAdvanced Research Project under Grant No. 6141B0801010b, and in part by the Defense Industrial Technology Development\nProgram under Grant No. JCKY2017204B064.\nABSTRACT Answer selection is a crucial subtask in the question answering (QA) system. Conventional\navenues for this task mainly concentrate on developing linguistic tools that are limited in both performance\nand practicability. Answer selection approaches based on deep learning have been well investigated with\nthe tremendous success of deep learning in natural language processing. However, the traditional neural\nnetworks employed in existing answer selection models, i.e., recursive neural network or convolutional\nneural network, typically suffer from obtaining the global text information due to their operating mecha-\nnisms. The recent Transformer neural network is considered to be good at extracting the global information\nby employing only self-attention mechanism. Thus, in this paper, we design a Transformer-based neural\nnetwork for answer selection, where we deploy a bidirectional long short-term memory (BiLSTM) behind the\nTransformer to acquire both global information and sequential features in the question or answer sentence.\nDifferent from the original Transformer, our Transformer-based network focuses on sentence embedding\nrather than the seq2seq task. In addition, we employ a BiLSTM rather than utilizing the position encoding to\nincorporate sequential features as the universal Transformer does. Furthermore, we apply three aggregated\nstrategies to generate sentence embeddings for question and answer, i.e., the weighted mean pooling,\nthe max pooling, and the attentive pooling, leading to three corresponding Transformer-based models, i.e.,\nQA-TFWP, QA-TFMP, and QA-TFA P, respectively. Finally, we evaluate our proposals on a popular QA dataset\nWikiQA. The experimental results demonstrate that our proposed Transformer-based answer selection\nmodels can produce a better performance compared with several competitive baselines. In detail, our best\nmodel outperforms the state-of-the-art baseline by up to 2 .37%, 2.83%, and 3 .79% in terms of MAP, MRR,\nand accuracy, respectively.\nINDEX TERMS Answer selection, deep learning, question answering, Transformer.\nI. INTRODUCTION\nQuestion answering (QA) is a signiﬁcant research topic in\nthe ﬁeld of artiﬁcial intelligence (AI) and natural language\nprocessing (NLP), which attracts much attention from both\nacademia and industry, resulting in a decent development and\ngreat achievements [1]. Inspired by the success of intelligent\nQA systems, e.g., Siri 1 and Watson, 2 QA has been exten-\nsively investigated since 2011s [2]–[5]. Compared with the\ntraditional information retrieval (IR) system, the QA system\nreceives a user’s question described in natural language [6].\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Malik Jahan Khan.\n1https://www.apple.com/ios/siri/\n2https://www.ibm.com/watson/\nFIGURE 1. The simplified procedure of the QA system.\nAnd then, it responses with the most appropriate answer cor-\nresponding to the submitted question, which can preferably\nunderstand the user’s real query intention and effectively meet\nthe user’s information needs [7]. We present a simpliﬁed ﬂow\ndiagram of the QA system in Fig. 1.\nGenerally, as is shown in Fig. 1, a QA system ﬁrst analyzes\nthe question described in natural language and transfers it into\n26146\n2169-3536 \n 2019 IEEE. Translations and content mining are permitted for academic research only.\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nVOLUME 7, 2019\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nseveral search queries. Then the acquired queries will be uti-\nlized for retrieving the relevant text from the knowledge base,\ne.g., Wikipedia,3 which contains couples of useful sentences\nfor answering the question. Next, the QA system will extract\nall possible candidate answers from the retrieved text. Finally,\nthe system selects the best answer from all candidate answers\nfor answering user’s question, namely, the process of answer\nselection.\nWith the development of community question answer-\ning (CQA) system, such as Yahoo! Answers, 4 Quora,5 and\nZhihu,6 which supports the interaction among users [8], [9],\na large amount of question-answer pairs have been collected\nthese years. Since then, how to select the best answer from\nnumerous candidate answers for a certain question greatly\nboosts the research of answer selection, making it a hot spot in\nQA research. Accordingly, we concentrate on the momentous\nstep of the QA system in this paper, i.e., the task of answer\nselection.\nAs a key component in the QA system, the main concern of\nanswer selection is how to select the most appropriate answer\nfor a question from quantities of candidate answers. Below,\nwe will detail the formal deﬁnition for answer selection.\nDeﬁnition [6]: For one question q, it has two types of cor-\nresponding answer pools, namely, a candidate answer pool A\nconsisting of m candidate answers which may be relevant\nwith q, i.e., A :{a1,a2,··· ,am}and a ground truth pool G,\ni.e., G :{g1,g2,··· ,gn}, where n is the number of ground\ntruth, i.e., correct answer of q. Answer selection focuses on\nselecting the correct answer of q from pool A. If the selected\nanswer ax (1 ≤x ≤m) is included in pool G, the answer\nselection task will be regarded to be successful. Otherwise,\nit fails.\nFIGURE 2. An example of answer selection in QA system.\nFig. 2 provides an instance of answer selection. As the\nFig. 2 shows, for the question q: ‘‘Which of the following\ncountry is located in Europe?’’ , its candidate answer pool A\ncontains four candidate answers, i.e., China, UK, USA and\nItaly, corresponding to a1, a2, a3 and a4. The ground truth\npool G includes two ground truths of question q, i.e., UK and\nItaly, corresponding to g1 and g2. If an answer selected\nfrom A by some algorithm is included in G, the task will be\nconsidered as successful.\nAccording to the deﬁnition above, besides determining\nwhether an answer is correct for the question or not, a key\n3https://www.wikipedia.org/\n4https://answers.yahoo.com/\n5https://www.quora.com/\n6https://www.zhihu.com/\npoint is to measure the relevance between question and each\ncandidate answer. Thus, the answer selection task can be\nregarded as a candidate reranking problem [4].\nEarlier works on this task mainly attempt to syntactically\ntransform each candidate answer to the question and then\nmeasure the relevance through a syntactic matching of parse\ntrees [10], [11]. However, these methods may suffer from\nthe multi-language problem as well as the low-efﬁciency\nproblem since they require much professional knowledge and\nhandcraft efforts.\nTo overcome the limitation in earlier research, deep learn-\ning (DL) based methods for answer selection have been\nextensively investigated these years, which turn out to be\nefﬁcient and cross-linguistic. Generally, the DL-based meth-\nods can be divided into two main categories according to\nthe basic neural network (NN) component, i.e., Recursive\nNeural Network (RNN) [12] and Convolutional Neural Net-\nwork (CNN) [13]. Speciﬁcally, RNN is supposed to be\ngood at model sequential data, e.g., sentences. However,\nit needs to be trained recursively among cells and will suffer\nfrom the long-term dependencies problem caused by gradi-\nent vanishing or gradient exploding [14], leading to failure\nin obtaining the global information of question or answer\nsentences, even improvement had been made on its variants,\ne.g., Long Short-Term Memory (LSTM) [15] and Gated\nRecurrent Unit (GRU) [16]. Besides, though attempts had\nbeen made to obtain global information by increasing the\nreceptive ﬁeld through cascading, CNN can only obtain local\ninformation, too, due to its inner operating mechanism.\nAlthough the typical NN architectures have shown high\nperformances for the task of answer selection, they are limited\nin obtaining the global information of question or answer sen-\ntences due to the analysis above. However, Vaswani et al. [17]\nproposed a new NN structure in 2017s, i.e., the Transformer,\nwhich achieves the state-of-the-art performances in many\nseq2seq tasks, e.g., machine translation. The Transformer\nprocesses the input data solely based on attention mechanism,\nwhich frees the typical NNs with recurrence and convolu-\ntions entirely. In this paper, different from previous DL-based\nanswer selection methods, we come up with a Transformer-\nbased answer selection NN to extract more abundant sen-\ntence features, which can synthesize both the global sentence\ninformation and the sequential features in the distributed\nrepresentation of question and answer. We ﬁrst employ a\nword representation layer to map the question or answer\nsentence into a numeric vector through concatenating the\nﬁxed and the variable pre-trained word embeddings. Next,\nwe design a Transformer-based NN architecture as the feature\nextractor, where we deploy a BiLSTM behind a Transformer\nrather than utilizing the position encoding in the original\nTransformer to incorporate sequential features together with\nthe global sentence information. Then, we employ a relevance\nmatching layer to measure the relevance between the question\nand its candidate answer, where we employ three aggregated\nstrategies of generating sentence embeddings for question\nand answer, i.e., the weighted mean pooling, the max pooling\nVOLUME 7, 2019 26147\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nand the attentive pooling. For comparison, we conduct experi-\nments on the publicly available open domain dataset WikiQA.\nExperimental results indicate that our Transformer-based\nmodels outperform several competitive baselines.\nWe brieﬂy list the contributions of this paper as follows:\n• We propose a Transformer-based neural network for\nanswer selection to incorporate both the global infor-\nmation and the sequential features of a sentence by\ndeploying a BiLSTM behind the Transformer;\n• We conduct comprehensive experiments to verify the\neffectiveness of our proposed models on a public dataset\nin well-known metrics;\n• We investigate the impact of pooling strategy used in our\nproposals on different types of questions for the task of\nanswer selection.\nThe remainder of this paper would be structured as fol-\nlows: Section II introduces the related works of answer selec-\ntion. Section III describes the detailed architecture of our\nTransformer-based answer selection models. Our experimen-\ntal setup is detailed in Section IV. Results and discussions of\nour experiments are presented in Section V. Finally, we draw\nour major conclusions and give possible future directions in\nSection VI.\nII. RELATED WORK\nIn this section, we introduce previous works on the task of\nanswer selection. Where we categorize them into two main\ngroups according to whether they utilize DL tools or not.\nA. NON-DL ANSWER SELECTION\nAnswer selection has been well investigated before DL-based\nmethods dominating the researches of NLP. In summary,\nlinguistic tools, feature engineering and external resources [4]\nhave ever greatly promoted the research of answer selection.\nWith linguistic tools, the answer selection task can be treated\nas a syntactical matching problem, which aims at matching\nthe parse trees of question and answer sentences [10], [11].\nSpeciﬁcally, Wang et al. [10] design a probabilistic quasi-\nsynchronous grammar with which question and answer match\neach other according to a loose-but-predictable syntactic\ntransformation. Wang and Manning [11] propose a novel\nprobabilistic approach to capture the alignment, which uti-\nlizes tree-edit operation mechanisms on the dependency parse\ntrees. In addition, Yao et al. [18] and Heilman and Smith [19]\npay attention to conducting the matching process with min-\nimal edit sequences from the dependency parse trees of the\nquestion and answer sentences. As for feature engineering-\nbased methods, Severyn and Moschitti [20] employ an SVM\nwith tree kernels for incorporating feature engineering over\nparsing trees into the process of discriminative tree-edit fea-\ntures extraction. Besides, external resources can also help\nimprove the performance of answer selection. In the work of\nYih et al. [21], lexical semantic features calculated by Word-\nNet are artiﬁcially gathered. Then, relevant words between\nquestion and answer sentences can be semantically matched\nbased on their semantic relevance. Although non-DL methods\nhave achieved good performance for the task of answer selec-\ntion, they can’t deal with multi-language answer selection\nproblem and might be low-efﬁcient since non-DL answer\nselection methods always cost much handcraft efforts and\nprofessional knowledge.\nB. DL-BASED ANSWER SELECTION\nWith the resurgence of deep neural network (DNN), such as\nCNN, RNN and its variants, e.g., LSTM and GRU, many NLP\ntasks have beneﬁted greatly from DNNs’ high performance\nwith little effort on feature engineering, including answer\nselection [2]. Most DL-based answer selection methods can\nbe assigned into one of the following two categories accord-\ning to the treatment towards question-answer pair. In the\nﬁrst category, question and candidate answer are treated as\nseparate sentences and they would be represented by their\nown sentence embeddings learnt by DNN. After that, certain\nsimilarity metrics will be employed to match the question and\nanswer embeddings. For instance, Feng et al. [3] design sev-\neral CNN architectures and similarity metrics in their paper.\nWhat’s more, DL-based methods using RNN and attention\nmechanisms have been well investigated, too [4], [5], [22].\nTan et al. [4] propose to employ an attentive BiLSTM to\nmatch the relevance of segments in candidate answer towards\nquestion. Wang et al. [5] introduce several architectures\non how to synthesize attention mechanisms inside a GRU.\nIn addition, Wang and Jiang [23] design a general ‘‘compare-\naggregate’’ framework which speciﬁcally pay attention to\ncompare different metrics to match the question and answer\nembeddings. In the second category, question and candidate\nanswer are treated as a joint sentence, where a joint embed-\nding of question-answer pair is learnt by DNN. For instance,\nSeveryn and Moschitti [24] introduce a CNN-based architec-\nture to rerank question-answer pairs according to a pointwise\nmethod, which can jointly learn the question-answer pair’s\nembedding. Wang and Nyberg [25] attempt to deploy the\nstacked BiLSTM component to learn a joint feature vec-\ntor for the question-answer pairs. The vector will then be\nemployed as one of the inputs in a gradient boosted regression\ntree (GBDT) [26], which regards the answer selection task as\na classiﬁcation problem.\nGenerally, our proposals in this paper follow the same\ntreatment in the ﬁrst category, which will learn separate\nembeddings for both question and answer sentences. Since\nRNN may suffer from the long-term dependencies problem\nand CNN can only obtain the local sentence information,\ndifferent from our previous work where we employ both\nRNN and CNN to collaboratively learning the sentence fea-\ntures [6], we employ neither RNN nor CNN but the Trans-\nformer as the basic feature extractor in this paper, which\nprocesses the input data based on self-attention mechanism\nto acquire the global sentence information. Actually, attention\nmechanisms can be found in [4], [5], and [22] for the task of\nanswer selection. However, most of them focus on calculating\nthe relevance of candidate answer towards question, they\nignore the interaction among words inside a sentence, namely,\n26148 VOLUME 7, 2019\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nthe self-attention. In addition, we intend to synthesize the\nsequential features together with global sentence information.\nTo this end, we propose a Transformer-based answer selec-\ntion neural network which employs a BiLSTM behind the\nTransformer to extract sentence features with higher quality\nfor question and candidate answer sentences.\nIII. APPROACHES\nIn this section, we will introduce the main procedures of our\nproposals following the order of model components, i.e., the\nword representation layer, the Transformer-based feature\nextractor and the relevance matching layer. Different from\nprevious researches which employ RNN or CNN as the fea-\nture extractor, we design a Transformer-based feature extrac-\ntor followed by three aggregated strategies in the relevance\nmatching layer, leading to three corresponding answer selec-\ntion models, i.e., Transformer-based model with weighted\nmean pooling (QA-TF WP), max pooling (QA-TF MP) and\nattentive pooling (QA-TF A P).\nFIGURE 3. Overall view of the Transformer-based model.\nFig. 3 presents an overall view of our Transformer-based\nmodel in this paper. From bottom to top, it shows the\nframework of three components, respectively. Speciﬁcally,\nwe regard question or answer as an input sentence s with\nlength l to illustrate our models since no interactions between\nquestion and answer are concerned in our proposals. In this\npaper, question and answer share the same hyperparameters\nfor the reason that earlier research [3] found that network\noptimizer encounters greater difﬁculty if question and answer\nshare different parameters. The following subsections will\ndetail each component of our proposals.\nA. WORD REPRESENTATION LAYER\nIn this subsection, we design a word representation layer\nto map the text data in question and answer sentences into\nnumeric vectors for further process as the text can not be cal-\nculated by the NNs directly. In the word representation layer,\nwe utilize the publicly available word embeddings pre-trained\nby word2vec toolkit [27], [28], which contains billions of\nd dimensional vectors that are trained on a large real-world\nnatural language corpus, i.e., Google News. 7 Fig. 4 displays\na general view of our word representation layer.\nFIGURE 4. General view of the word representation layer.\nAs is shown in Fig. 4, for the input sentence s: ‘‘What\ncounty is Jacksonville Florida in ?’’ , wt is the t-th (0 ≤\nt ≤ l) word in s. Unlike previous researches that directly\nutilize the ﬁxed pre-trained word embeddings, we employ not\nonly the ﬁxed word embeddings but also the variable word\nembeddings, which are ﬁne-tuned along with the training\nprocess to adjust the word embeddings to be more reasonable\nfor the experimental data. In this way, the word representation\nlayer would keep as many word features as possible.\nFirstly, to keep the consistence, a special token will be\nemploy to pad all questions and answers to a ﬁxed length in\nthis paper. Words out of the length will be discarded. Then,\nthe model will lookup the corresponding d dimensional word\nvectors from both the ﬁxed word embeddings and the variable\nword embeddings. Next, we concatenate both word vectors to\nform a new 2 ∗d dimensional word vector. Finally, we deploy\na hidden layer to select useful features from the concatenated\nword vector. The ﬁnal representation rwt of the word wt is\ncalculated according to the following formula:\nrwt =tanh(Wh ·(wt f ⊕wt v) +bh), (1)\nwhere wt f and wt v are word vectors from the ﬁxed word\nembeddings and the variable word embeddings, respectively.\nWh and bh are network parameters of the hidden layer.\nAfter conducting the above processes on all words in the\ninput sentence s, the words representation RW of s generated\nin the word representation layer can be returned as follows:\nRW =\n(rw1 rw2 ··· rwl\n)\n. (2)\nObviously, before being processed by the feature extrac-\ntor, the word vector rwt in the words representation RW is\nindependent with each other under the context of sentence s.\nAccordingly, further processes are needed and will be stated\nin the following subsections.\n7https://code.google.com/p/word2vec/\nVOLUME 7, 2019 26149\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nFIGURE 5. General view of the Transformer-based feature extractor.\nB. TRANSFORMER-BASED FEATURE EXTRACTOR\nIn this subsection, we will demonstrate the process of our\nproposed Transformer-based feature extractor, which aims\nto model the context information among the words rep-\nresentation RW . Since the conventional NNs, i.e., RNN\nand CNN, are not able to model the global sentence infor-\nmation, we adopt the multi-head self-attention Transformer\nstructure in our paper which is similar with the one proposed\nby Vaswani et al. [17]. However, the Transformer in [17] is\ndesigned for the seq2seq tasks, but we employ it for gener-\nating high-quality sentence embeddings. Besides, we do not\ndesign a position embedding like the original Transformer\ndoes. Instead, we employ a BiLSTM behind the Transformer\nfor synthesizing the sequential features in the sentence. Fig. 5\nillustrates the general view of our Transformer-based feature\nextractor. The concrete processes for extracting reasonable\nfeatures from the words representation RW are designed as\nfollows.\nClearly, the main function of the feature extractor is to\ntransform the words representation RW to a distributed sen-\ntence representation, which makes good use of the context\ninformation among words in the sentence.\nIn our Transformer-based feature extractor, a self-attention\nmechanism is employed to model the dependency between\neach word vector rwt . For each word vector in the word\nrepresentation RW , the attentive similarity between it and all\nword vectors in RW will be calculated ﬁrst. Unlike the orig-\ninal Transformer who employs a scaled Dot-Product as the\nattentive similarity function, we design a scaled perception\nfor our Transformer-based feature extractor in this paper. The\nattentive similarity between a word vector rwt and all word\nvectors in RW is calculated as follows:\nf (rwt ,RW ) =oT\na ·tanh(Wa ·rwt +Ua ·RW ), (3)\nat =softmax(f (rwt ,RW )√dRW\n), (4)\nwhere oT\na , Wa and Ua are the attention parameters. at repre-\nsents the attentive similarity between word vector rwt and all\nword vectors in RW . dRW is the dimension of RW . √dRW aims\nto scale the softmax function to avoid at being too large.\nAfter obtaining the attentive similarity at , a self-attentive\nword vector rwt\na will be generated according to the following\nformula:\nrwt\na =at ·RW . (5)\nAll self-attentive word vectors form the self-attentive\nwords representation RW a:\nRW a =\n(rw1\na rw2\na ··· rwl\na )\n. (6)\nBesides self-attention mechanism, our Transformer-based\nfeature extractor adopts the multi-head mechanism in [17],\nwhose function is similar with the multiple ﬁlters in CNN.\nThe multi-head mechanism allows the feature extractor to\njointly incorporate information from different representa-\ntion subspaces. Assuming that there are n heads in the\nTransformer-based feature extractor, n self-attentive words\nrepresentations will be concatenated to form the output words\nrepresentation of Transformer for the input sentence s:\nRW TF =Concat(RW a1 ,RW a2 ,··· ,RW an )\n=\n(\nrw1\nTF rw2\nTF ··· rwl\nTF )\n, (7)\nwhere RW ai is the i-th self-attentive words representa-\ntion. Concat is a function that can concatenate all n self-\nattentive words representations into one Transformer words\nrepresentation RW TF .\nIn the original Transformer, neither recurrence nor convo-\nlution is employed so that it is not sensitive with the word\nposition information in sentence. That’s to say, the sequen-\ntial features in s are missing with only the multi-head self-\nattention NN structure in the Transformer. However, in our\nTransformer-based feature extractor, we employ a BiLSTM\ncomponent behind the Transformer, which has been widely\nadopted in many NLP tasks [29], [30] since it can help\nextract the sequential features among sequential data through\nemploying LSTMs in two directions.\nAt the current step t of a single direction LSTM, the forget\ngate ft controls how much information in former word’s state\nht−1 should be forgotten. After that, the input gate it deter-\nmines how much information of current input word vector\nrwt\nTF should be kept in the memory cell c. Next, the memory\ncell c will measure the overall information of rwt\nTF and the\npast memory ht−1. Afterwards, the updated memory infor-\nmation Ct consists of the past memory information ﬁltered\nby ft . At last, the output gate o controls how much information\nshould be utilized in the next step. Detailed operations on the\nTransformer words representation RW TF are included in the\nfollowing formulae:\nft =σ(Wf ·rwt\nTF +Uf ·ht−1 +bf ), (8)\nit =σ(Wi ·rwt\nTF +Ui ·ht−1 +bi), (9)\n˜Ct =tanh(Wc ·rwt\nTF +Uc ·ht−1 +bc), (10)\nCt =ft ·Ct−1 +it ·˜Ct , (11)\not =σ(Wo ·rwt\nTF +Uo ·ht−1 +bo), (12)\nht =ot ·tanh(Ct ), (13)\n26150 VOLUME 7, 2019\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nwhere σrepresents the activation function sigmoid for all the\nabove formulae. W ∈RH×m, U ∈RH×H and b ∈RH×1\nare parameters in network, determining input information,\noutput information and bias, respectively. After conducting\nthe above operations for the Transformer words representa-\ntion RW TF in the forward and backward directions, the ﬁnal\ndistributed sentence representation RS of s will be generated\nas follows:\nrst =− →ht ||← −ht , (14)\nRS =\n(rs1 rs2 ··· rsl\n)\n, (15)\nwhere rst is t-th word’s representation processed by our\nTransformer-based feature extractor. − →ht and ← −ht indicate the\noutputs of BiLSTM in the forward direction and the backward\ndirection, respectively.\nC. RELEVANCE MATCHING LAYER\nIn this subsection, we design a relevance matching layer to\nmatch the sentence representation of question and answer\ngenerated from our Transformer-based feature extractor,\nwhere we employ three aggregated strategies to pool the\nsentence representation matrix into a sentence embedding.\nDetailed processes are stated as follows.\n1) Weighted Mean Pooling:We employ a weighted mean\npooling in our QA-TFWP model by distributing variable\nweight for each word representation:\nv =RS ·wmT\nl , (16)\nwhere wm and l are weight vector and sentence length,\nrespectively.\n2) Max Pooling: We employ the max pooling in our\nQA-TFMP model to pool the sentence matrix into the\nﬁnal sentence embedding as follows:\nv =MaxPooling(RS ), (17)\nwhich aims at extracting the most notable feature in\neach dimension.\n3) Attentive Pooling:In our QA-TF A Pmodel, we employ\nan attentive pooling to generate the sentence embed-\ndings vq and va for question and answer from their\nsentence representations RQ and RA, whose processes\nare deﬁned in the following formulae:\nG =RQT ·U ·RA, (18)\nvq =RQ ·softmax(ColumnMax(G)), (19)\nva =RA ·softmax(RowMax(G)), (20)\nwhere G is the attentive similarities between RQ and RA.\nU is the attention parameter. ColumnMax and RowMax\nare functions who extract the max value from column\nand row, respectively.\nAfter that, following [4], [5], and [22], we calculate the\ncosine similarity between sentence embeddings of question\nand answer, which indicates their relevance degree. A ques-\ntion q together with a ground truth a+ and an incorrect\nanswer a−, which is randomly sampled from all answers in\nthe training set, form a training instance in this paper. For q,\na+and a−, their sentence embeddings are: vq, va+ and va−,\nrespectively. To match the relevance between question and\nanswer, their cosine similarity is formally calculated as\nfollows:\ncos(vq,va) =\nn∑\ni=1\n(vq ·va)\n√\nn∑\ni=1\n(vq)2 ×\n√\nn∑\ni=1\n(va)2\n, (21)\nwhere va represents the sentence embedding of the ground\ntruth a+or the incorrect answer a−. n is the dimension of the\nsentence embedding. If the condition that the gap between\nthe cosine similarities of vq, va+ and vq, va− is over a preset\nmargin m is satisﬁed, namely:\nm ≤cos(vq,va+) −cos(vq,va−), (22)\nthe training procedure for a training instance will be sus-\npended and no parameter updates will be performed in NNs.\nOtherwise, the NN parameters will be updated by the opti-\nmizer to achieve a lower loss. The NN is trained for the\nbest training epoch through minimizing the ranking loss of\ncandidate answers. In this paper, we adopt the ranking loss\nfunction proposed by Feng et al. [3] for our experiments,\nwhich is worked as follows:\nloss =max{0,m −cos(vq,va+) +cos(vq,va−)}. (23)\nThrough the above processes, we will locate on the best\nparameters for testing. After obtaining the cosine similarities\nbetween a question q and its all candidate answers. Candidate\nanswer with the largest cosine similarity will be ranked on the\nﬁrst position among all candidate answers of a question, and\nso forth.\nIV. EXPERIMENTS\nIn this section, we brieﬂy summarize the baseline models\nwe choose to compare in this paper, as well as our proposed\nmodels in Section IV-A. Research questions guiding our\nexperiments are listed in Section IV-B. Next, Section IV-C\nintroduces the dataset we utilize and the standard evalua-\ntion metrics. Finally, we detail our experimental settings and\nhyper-parameters in Section IV-D.\nA. MODEL SUMMARIES\nWe verify the effectiveness of our proposals by comparing\nthem with several competitive baseline models in this paper.\nTable 1 describes the summaries and literatures of baseline\nmodels as well as our proposed models.\nB. RESEARCH QUESTIONS\nIn this paper, we come up with the following three research\nquestions to guide our experiments:\nRQ1 Can our proposed Transformer-based models show\nany superiority compared with other competitive\nanswer selection baseline models?\nVOLUME 7, 2019 26151\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nTABLE 1. Model summaries and literatures.\nTABLE 2. WikiQA corpus statistics.\nRQ2 How do our proposed models perform compared\nwith the state-of-the-art baseline in terms of the\naverage length of ground truths?\nRQ3 What’s the difference on performance among our\nproposed three models with respect to different\nquestion types?\nC. DATASET AND EVALUATION METRICS\nThe dataset we choose to evaluate our proposed approaches\nis a publicly available open domain dataset, i.e., WikiQA\nreleased in 2015 [31]. We list the statistics of WikiQA\ndataset in Table 2, which details the number of questions,\ncorrect answers, QA pairs and the average length of question\nand answer in each part of the dataset. WikiQA is created\nby Yang et al. [31] over the English Wikipedia summary\npassages and the Bing query logs, with Crowd-Sourced\nannotations. We preprocess the above dataset by removing\nquestions that contain no correct answer(s) in the whole\ndataset, so results on this paper may not be directly compa-\nrable to those on the other works. It’s necessary to mention\nthat a question may have more than one correct answers.\nMeanwhile, the length of questions is much shorter than\nanswers: the average length of questions is 6.42; while the\naverage length of answers is 25.33. The length gap between\nquestion and answer sentences increases the difﬁculty for\nanswer selection task. There are ﬁve common question types\nall together in the WikiQA dataset according to the query\nterm, i.e., ‘how’, ‘what’, ‘who’, ‘where’ and ‘when’. Detailed\nintroduction of WikiQA dataset can be referred to in [31].\nThe answer selection task is regarded as a rerank problem\nin our paper, which aims at reranking the candidate answers\naccording to their relevance towards the question. Follow-\ning prior researches on answer selection [25], we adopt the\nstandard Mean Average Precision (MAP) and Mean Recipro-\ncal Rank (MRR) as evaluation metrics in this paper. In addi-\ntion, to reﬂect the precision of answer that is ranked at the ﬁrst\nposition, we also report the top one precision in this paper,\nwhich is typically adopted as accuracy in answer selection\nresearch. The detailed formulae of the above three evaluation\nmetrics are as follows:\nMAP = 1\n|Q|\n|Q|∑\ni=1\n( 1\nmi\n|mi|∑\nj=1\nj\npj\n), (24)\nwhere mi is the number of ground truths for the i-th question.\npj is the ranking position of i-th question’s j-th correct answer.\nMAP is concerned about all correct answers.\nInstead, different with MAP, MRR concerns only the ﬁrst\ncorrect answer and is calculated as follows:\nMRR = 1\n|Q|\n|Q|∑\ni=1\n1\npi\n, (25)\nwhere pi is the ranking position of the i-th question’s ﬁrst\ncorrect answer.\naccuracy = 1\n|Q|\n|Q|∑\ni=1\nprecision@1, (26)\nwhere precision@1 is the precision of answer that is ranked\nat the ﬁrst position.\nD. EXPERIMENTAL SETUP\nWe conduct our experiments on Microsoft Windows 8 opera-\ntion system with 6 GB Nvidia 9 1060 GPU. The deep learning\nframework and the programming language we use are Ten-\nsorFlow10 and Python, 11 respectively. The pre-trained word\nembeddings’ dimension is 300 in this paper. We pad the\nsentence length for all questions and answers to 40 follow-\ning [22]. The Adaptive Moment (Adam) estimation [32] is\nemployed for optimizing the loss in this paper. In addition,\nthe L2 regularization and the dropout methods are included\n8https://www.microsoft.com/zh-cn/windows/\n9https://www.nvidia.cn/\n10https://tensorﬂow.google.cn/\n11https://www.python.org/\n26152 VOLUME 7, 2019\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nTABLE 3. Main experimental setups of our models, where Bs represents\nbatch size; Mg for the value of margin; Dp for the dropout keep\nprobability; Lr for the learning rate; L2 for the L2 regularization\ncoefficient; Hn and Hs for the number and size of self-attention head,\nrespectively.\nTABLE 4. Overall performance of different models, where models with∗\nrepresent the baseline models in literatures, models with⋆ are proposed\nin this paper. The highest performance in each evaluation metric is in\nbold.\nin our training process to avoid the problem of over-ﬁtting.\nWe train our models in mini-batches and employ exponential\ndecay method in our models to vary learning rate in every\nepoch with the decay rate equal to 0.9. Detailed experimental\nsetups of our models are displayed in Table 3.\nV. RESULTS AND DISCUSSION\nWe present detailed analysis and discussions on our exper-\nimental results in this section, which are corresponding\nto the three research questions proposed in Section IV-B.\nSection V-A answers RQ1 by displaying the overall per-\nformance of baselines and our proposed Transformer-based\nmodels. Section V-B investigates the performance of the best\nbaseline model and our proposals under different average\nlength of ground truths to answer RQ2. Section V-C com-\npares the performance of our proposed models in terms of\nquestion type to answer RQ3.\nA. PERFORMANCE OF MODELS\nTo answer RQ1, we summarize the overall evaluation results\nof three baselines and our proposed Transformer-based mod-\nels on the test set of WikiQA in terms of MAP, MRR and\naccuracy in Table 4.\nAs is shown in Table 4, generally, for different evalua-\ntion metrics, the value of accuracy much lower than MAP\nand MRR. It is because that the accuracy only concerns the\nﬁrst answer’s precision, if the answer ranked in the ﬁrst\nposition is wrong, the accuracy score will be 0. In addition,\nthe MAP scores in all models are lower than the MRR scores\nsince MAP concerns all ground truths while MRR only con-\ncerns the ﬁrst ground truth. When other ground truths are\nranked at a low position in the list of candidate answers,\nthe MAP score will be decreased.\nAmong the three baseline models in this paper,\nQA-BiLSTM beats QA-CNN in all evaluation metrics. The\nreason may be that the BiLSTM has advantages in extracting\nsequential features in text data compared with the CNN.\nIn addition, AB-LSTM/CNN shows obvious superiority\nagainst the other two baseline models without integrating\nattention mechanism. Hence, among all three baselines,\nwe select AB-LSTM/CNN for later comparison with our\nproposals.\nFor our proposed models, all of them achieve an improve-\nment compared with the baseline models according to the\nresults displayed in Table 4. Speciﬁcally, the MAP and MRR\nof our QA-TF WP model are increased by 1 .31% and 1 .35%\nagainst the best AB-LSTM/CNN model. The MAP, MRR and\naccuracy of our QA-TF MP model are increased by 1 .78%,\n2.40% and 3 .79% compared with the best baseline. The\nMAP, MRR and accuracy of our QA-TF A P model improve\nby 2 .37%, 2 .83% and 1 .51%, respectively. Among them,\nthe QA-TF A P achieves the highest improvements in MAP\nand MRR, while the QA-TF MP achieves the highest improve-\nment in accuracy. The overall comparisons indicate that our\nproposed Transformer-based answer selection models can\nproduce a higher performance than the competitive base-\nlines did. It may because that the Transformer-based neural\nnetwork in our proposals could help improve the feature\nextractor’s ability in extracting global information of question\nand answer sentences. We will conduct further experiments\nto deeply investigate the effectiveness of our proposals in the\nnext subsection.\nB. DISCUSSION ON AVERAGE LENGTH\nOF GROUND TRUTH\nTo answer RQ2, we conduct further experiments over the\nbest baseline, i.e., the AB-LSTM/CNN model, as well as\nour three proposals to deeply investigate the detailed perfor-\nmance in terms of the average length of ground truths. Firstly,\nwe categorize the test set into three buckets according to the\naverage ground truths length L of each question, i.e., short\n(L ≤22), medium (22 < L < 33), and long (L ≥33).\nTable 5 displays the detailed evaluation values of the above\nfour models on three buckets. Visualized comparisons are\npresented in Fig. 6 below. Fig. 6a, Fig. 6b and Fig. 6c present\nthe comparisons of the MAP scores, the MRR scores as well\nas the accuracy scores, respectively.\nAs is shown in Fig. 6, for the bucket short, it’s obvious\nthat the best baseline model AB-LSTM/CNN achieves the\nhighest performance on all evaluation metrics. The reason\nmay be that the attention mechanism could help promote the\ninteraction among question and answer compared with our\nmodels which didn’t concern the interaction among question\nand answer, especially for question-answer pairs with short\nground truth answers whose features may be difﬁcult to\nextract since there are too few words in the answer sentence.\nFor the bucket medium, performance varies among models\nfor each evaluation metric but shows no obvious differences.\nThe possible reason may be that for question-answer pairs\nwith medium ground truth answers, they did not suffer from\nthe long-term dependencies problem in long sentence as well\nas the too-few-words problem in short sentence.\nVOLUME 7, 2019 26153\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nTABLE 5. Model performances under different average length of ground truths, where MAP, MRR and accuracy on three buckets are reported according\nto each question’s average length of ground truths.\nFIGURE 6. Model performance under different average length of ground truths in terms of MAP, MRR and accuracy. (a) MAP. (b) MRR. (c) Accuracy.\nTABLE 6. Model performances in terms of different question types, where MAP, MRR and accuracy on five question types are reported according to the\nquery term in the first position of question sentence.\nHowever, for the bucket long, all of our proposed mod-\nels show superiority against the best baseline model on\nevery evaluation metric. Speciﬁcally, the QA-TF A P model\nachieves the highest performance on the bucket long for all\nevaluation metrics. The MAP, MRR and accuracy scores\nof our QA-TF A P model are increased by 11 .08%, 12 .75%\nand 19 .55%, respectively, compared with the best base-\nline model. The observed phenomenon indicates that our\nTransformer-based models can help improve the performance\non the question-answer pairs with long ground truths length.\nThat’s fair since the Transformer structure could extract the\nglobal information especially for long sentence while typical\nNNs may extract only local information of sentence due to\nlong-term dependencies problem, etc.\nC. COMPARISON OF MODELS\nTo answer RQ3, we conduct extended experiments to com-\npare the performance of our QA-TF WP model, QA-TF MP\nmodel and QA-TF A Pmodel with respect to different question\ntypes. We ﬁrst categorize the questions on the test set into\nﬁve types according to their query term on the ﬁrst position\nof question sentence, i.e., ‘ how’, ‘ what’, ‘ who’, ‘ where’ and\n‘when’. Table 6 presents the detailed evaluation values of\nour proposed models on each question type. We also display\na visualization comparison of their performance in Fig. 7\nbelow. Fig. 7a, Fig. 7b and Fig. 7c present the comparisons\nof the MAP, MRR and accuracy, respectively.\nAs is shown in Table 6 and Fig. 7, performance curves in all\nsubﬁgures follow the similar distribution, namely, the three\nmodels all achieve a peak for the question type ‘ who’ or\n‘where’ as well as a valley for the question type ‘ how’ or\n‘when’. It may due to the fact that the answers for question\ntype ‘ who’ or ‘ where’ are always ﬁxed with less ambiguity.\nIn addition, differences among our three proposals are also\nobserved in Table 6 and Fig. 7. For our QA-TF WP model,\nit shows the most stable performance on every question type\ncompared with the other two models. That’s fair since it\nemploys a weighted mean pooling strategy for generating\nsentence embedding, which may make a balance of all fea-\ntures in the sentence. For our QA-TF MP model, it produces the\nhighest performance among the three models for the question\ntype ‘ who’ or ‘ where’. However, it also reports the lowest\nperformance for the question type ‘ who’ or ‘ where’. The pos-\nsible reason may due to the fact that the most notable feature\nkept in each dimension by our max pooling strategy is impor-\ntant for answering the question with type ‘ who’ or ‘ where’.\n26154 VOLUME 7, 2019\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\nFIGURE 7. Model performance under different question types in terms of MAP, MRR and accuracy. (a) MAP. (b) MRR. (c) Accuracy.\nThat’s to say, for these question types, the most necessary\nfeatures for determining whether a candidate answer is cor-\nrect or not are contained in the features with the max value\nin each dimension. As for our QA-TF A P model, it shows\nimprovements in all question types compared with QA-TF WP\nmodel since the attentive pooling strategy it employs may\nbe superior to the weighted mean pooling strategy. Besides,\nit can improve the performance on the question type ‘ how’ or\n‘when’ since it may reduce the ambiguity among sentences\nwith the attentive pooling strategy.\nVI. CONCLUSIONS AND FUTURE WORK\nIn this paper, we propose a Transformer-based neural network\nfor answer selection in question answering, which aims to\nextract both global information and sequential features in\nquestion and answer sentences. We ﬁrstly employ the serial\nstructure to deploy the multi-head self-attention mechanism\nas well as a BiLSTM as the feature extractor. In addition,\nwe employ three aggregated strategies to pool the sentence\nrepresentation matrix into a sentence embedding in the rel-\nevance matching layer, which leads to three Transformer-\nbased answer selection models in this paper. We conduct\ncomprehensive experiments to evaluate our proposals, which\nare based on the publicly available QA dataset, i.e., WikiQA.\nThe experimental results show that our Transformer-based\nmodels can outperform several competitive baselines in terms\nof standard evaluation metrics.\nAs future work, we would like to verify the generality\nof our proposed Transformer-based models by evaluating\ntheir effectiveness on other datasets. In addition, as various\nattention mechanisms have been proved to be effective for\nanswer selection task, we have interests in discovering how\nto incorporate the existing attention mechanisms with the\nself-attention mechanism in this paper to achieve a better\nperformance. Finally, it’s potential to apply our proposed\nTransformer-based neural network on other NLP tasks, e.g.,\nautomatic text summarization and natural language inference.\nREFERENCES\n[1] D. Ferrucci et al. , ‘‘Building Watson: An overview of the DeepQA\nproject,’’AI Mag., vol. 31, no. 3, pp. 59–79, 2010.\n[2] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman. (2014). ‘‘Deep learn-\ning for answer sentence selection.’’ [Online]. Available: https://arxiv.org/\nabs/1412.1632\n[3] M. Feng, B. Xiang, M. R. Glass, L. Wang, and B. Zhou, ‘‘Apply-\ning deep learning to answer selection: A study and an open task,’’\nin Proc. IEEE Workshop Autom. Speech Recognit. Understand. , Dec. 2015,\npp. 813–820.\n[4] M. Tan, C. dos Santos, B. Xiang, and B. Zhou. (2015). ‘‘LSTM-based deep\nlearning models for non-factoid answer selection.’’ [Online]. Available:\nhttps://arxiv.org/abs/1511.04108\n[5] B. Wang, K. Liu, and J. Zhao, ‘‘Inner attention based recurrent neural net-\nworks for answer selection,’’ in Proc. Meeting Assoc. Comput. Linguistics ,\n2016, pp. 1288–1297.\n[6] T. Shao, X. Kui, P. Zhang, and H. Chen, ‘‘Collaborative learning for answer\nselection in question answering,’’ IEEE Access , vol. 7, pp. 7337–7347,\n2019.\n[7] D. Mollá and J. L. Vicedo, ‘‘Question answering in restricted domains:\nAn overview,’’Comput. Linguistics, vol. 33, no. 1, pp. 41–61, 2007.\n[8] B. Patra. (2017). ‘‘A survey of community question answering.’’ [Online].\nAvailable: https://arxiv.org/abs/1705.04009\n[9] T. P. Sahu, N. K. Nagwani, and S. Verma, ‘‘Selecting best answer:\nAn empirical analysis on community question answering sites,’’ IEEE\nAccess, vol. 4, pp. 4797–4808, 2016.\n[10] M. Wang, N. A. Smith, and T. Mitamura, ‘‘What is the jeopardy\nmodel? A quasi-synchronous grammar for QA,’’ in Proc. Joint Conf.\nEmpirical Methods Natural Lang. Process. Comput. Natural Lang.\nLearn. (EMNLP-CoNLL), Prague, Czech Republic, Jun. 2007, pp. 22–32.\n[11] M. Wang and C. D. Manning, ‘‘Probabilistic tree-edit models with struc-\ntured latent variables for textual entailment and question answering,’’\nin Proc. 23rd Int. Conf. Comput. Linguistics , 2010, pp. 1164–1172.\n[12] J. B. Pollack, ‘‘Recursive distributed representations,’’ Artif. Intell., vol. 46,\nnos. 1–2, pp. 77–105, 1990.\n[13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, ‘‘Gradient-based learn-\ning applied to document recognition,’’ Proc. IEEE , vol. 86, no. 11,\npp. 2278–2324, Nov. 1998.\n[14] T. Lin, B. G. Horne, P. Tiňo, and C. L. Giles, ‘‘Learning long-term\ndependencies in NARX recurrent neural networks,’’ IEEE Trans. Neural\nNetw., vol. 7, no. 6, pp. 1329–1338, Nov. 1996.\n[15] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[16] K. Cho, B. Van Merrienboer, D. Bahdanau, and Y. Bengio. (2014). ‘‘On the\nproperties of neural machine translation: Encoder-decoder approaches.’’\n[Online]. Available: https://arxiv.org/abs/1409.1259\n[17] A. Vaswani et al. , ‘‘Attention is all you need,’’ in Advances in Neural\nInformation Processing Systems 30 , I. Guyon et al. Eds. Red Hook,\nNY, USA: Curran Associates, 2017, pp. 5998–6008. [Online]. Available:\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n[18] X. Yao, B. Van Durme, C. Callison-Burch, and P. Clark, ‘‘Answer extrac-\ntion as sequence tagging with tree edit distance,’’ in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol. , 2013,\npp. 858–867.\n[19] M. Heilman and N. A. Smith, ‘‘Tree edit models for recognizing tex-\ntual entailments, paraphrases, and answers to questions,’’ in Proc. Hum.\nLang. Technol., Conf. North Amer. Chapter Assoc. Comput. Linguistics ,\nLos Angeles, CA, USA, Jun. 2010, pp. 1011–1019.\n[20] A. Severyn and A. Moschitti, ‘‘Automatic feature engineering for answer\nselection and extraction,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess., 2013, pp. 458–467.\nVOLUME 7, 2019 26155\nT. Shaoet al.: Transformer-Based Neural Network for Answer Selection in QA\n[21] W. T. Yih, M. W. Chang, C. Meek, and A. Pastusiak, ‘‘Question answering\nusing enhanced lexical semantic models,’’ in Proc. Meeting Assoc. Com-\nput. Linguistics, 2013, pp. 1744–1753.\n[22] C. dos Santos, M. Tan, B. Xiang, and B. Zhou. (2016). ‘‘Attentive pooling\nnetworks.’’ [Online]. Available: https://arxiv.org/abs/1602.03609\n[23] S. Wang and J. Jiang. (2016). ‘‘A compare-aggregate model for matching\ntext sequences.’’ [Online]. Available: https://arxiv.org/abs/1611.01747\n[24] A. Severyn and A. Moschitti, ‘‘Learning to rank short text pairs with\nconvolutional deep neural networks,’’ inProc. Int. ACM SIGIR Conf., 2015,\npp. 373–382.\n[25] D. Wang and E. Nyberg, ‘‘A long short-term memory model for answer\nsentence selection in question answering,’’ in Proc. Meeting Assoc. Com-\nput. Linguistics Int. Joint Conf. Natural Lang. Process. , 2015, pp. 707–712.\n[26] J. H. Friedman, ‘‘Greedy function approximation: A gradient boosting\nmachine,’’Ann. Statist., vol. 29, no. 5, pp. 1189–1232, Oct. 2001.\n[27] T. Mikolov, K. Chen, G. Corrado, and J. Dean. (2013). ‘‘Efﬁcient esti-\nmation of word representations in vector space.’’ [Online]. Available:\nhttps://arxiv.org/abs/1301.3781\n[28] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Distributed\nrepresentations of words and phrases and their compositionality,’’ in Proc.\nAdv. Neural Inf. Process. Syst. , vol. 26, 2013, pp. 3111–3119.\n[29] E. Kiperwasser and Y. Goldberg, ‘‘Simple and accurate dependency pars-\ning using bidirectional lstm feature representations,’’ Trans. Assoc. Com-\nput. Linguistics, vol. 4, no. 1, pp. 313–327, 2016.\n[30] Z. Huang, W. Xu, and K. Yu. (2015). ‘‘Bidirectional LSTM-CRF\nmodels for sequence tagging.’’ [Online]. Available: https://arxiv.org/\nabs/1508.01991\n[31] Y. Yang, W.-T. Yih, and C. Meek, ‘‘Wikiqa: A challenge dataset for open-\ndomain question answering,’’ in Proc. Conf. Empirical Methods Natural\nLang. Process., 2015, pp. 2013–2018.\n[32] D. P. Kingma and J. Ba. (2014). ‘‘Adam: A method for stochastic optimiza-\ntion.’’ [Online]. Available: https://arxiv.org/abs/1412.6980\nTAIHUA SHAOreceived the B.S. degree in infor-\nmation system engineering from the National Uni-\nversity of Defense Technology, China, in 2017,\nwhere he is currently pursuing the M.S. degree in\nmanagement science and engineering.\nHis research interests include natural language\nprocessing and information retrieval.\nYUPU GUOreceived the B.S. degree in informa-\ntion system engineering from the National Uni-\nversity of Defense Technology, China, in 2018,\nwhere he is currently pursuing the M.S. degree in\nmanagement science and engineering.\nHis research interests include recommendation\nsystems and information retrieval.\nHONGHUI CHEN received the Ph.D. degree in\noperational research from the National University\nof Defense Technology, China, in 2007.\nHe is currently a Professor with the National\nUniversity of Defense Technology. He has pub-\nlished several papers at SIGIR, IPM, and other top\njournals. His research interests include informa-\ntion systems and information retrieval.\nZEPENG HAOreceived the B.S. degree in infor-\nmation system engineering from the National Uni-\nversity of Defense Technology, China, in 2017,\nwhere he is currently pursuing the M.S. degree in\nmanagement science and engineering.\nHis research interests include automatic text\nsummarization and information retrieval.\n26156 VOLUME 7, 2019"
}