{
  "title": "Transforming glaucoma diagnosis: transformers at the forefront",
  "url": "https://openalex.org/W4390884164",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4323623852",
      "name": "Farheen Chincholi",
      "affiliations": [
        "Friedrich-Alexander-Universität Erlangen-Nürnberg"
      ]
    },
    {
      "id": "https://openalex.org/A2485451403",
      "name": "Harald Koestler",
      "affiliations": [
        "Friedrich-Alexander-Universität Erlangen-Nürnberg"
      ]
    },
    {
      "id": "https://openalex.org/A4323623852",
      "name": "Farheen Chincholi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2485451403",
      "name": "Harald Koestler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2773612948",
    "https://openalex.org/W2912650418",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2293295816",
    "https://openalex.org/W4323567814",
    "https://openalex.org/W3041453914",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W4242175757",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3159151002",
    "https://openalex.org/W2766585573",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4309338123",
    "https://openalex.org/W2935464137",
    "https://openalex.org/W6758748852",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4300543510"
  ],
  "abstract": "Although the Vision Transformer architecture has become widely accepted as the standard for image classification tasks, using it for object detection in computer vision poses significant challenges. This research aims to explore the potential of extending the Vision Transformer for object detection in medical imaging, specifically for glaucoma detection, and also includes an examination of the Detection Transformer for comparative analysis. The analysis involves assessing the cup-to-disc ratio and identifying signs of vertical thinning of the neuroretinal rim. A diagnostic threshold is proposed, flagging a cup-to-disc ratio exceeding 0.6 as a potential indicator of glaucoma. The experimental results demonstrate a remarkable 90.48% accuracy achieved by the pre-trained Detection Transformer, while the Vision Transformer exhibits competitive accuracy at 87.87%. Comparative evaluations leverage a previously untapped dataset from the Standardized Fundus Glaucoma Dataset available on Kaggle, providing valuable insights into automated glaucoma detection. The evaluation criteria and results are comprehensively validated by medical experts specializing in the field of glaucoma.",
  "full_text": "TYPE Methods\nPUBLISHED /one.tnum/five.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nOPEN ACCESS\nEDITED BY\nSou Nobukawa,\nChiba Institute of Technology, Japan\nREVIEWED BY\nTae Keun Yoo,\nB&VIIT Eye Center/Refractive Surgery & AI\nCenter, Republic of Korea\nMassimo Salvi,\nPolytechnic University of Turin, Italy\n*CORRESPONDENCE\nFarheen Chincholi\nfarheen.s.chincholi@fau.de\nRECEIVED /one.tnum/nine.tnum October /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /zero.tnum/three.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nPUBLISHED /one.tnum/five.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nChincholi F and Koestler H (/two.tnum/zero.tnum/two.tnum/four.tnum) Transforming\nglaucoma diagnosis: transformers at the\nforefront. Front. Artif. Intell./seven.tnum:/one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Chincholi and Koestler. This is an\nopen-access article distributed under the terms\nof the\nCreative Commons Attribution License\n(CC BY) . The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nTransforming glaucoma\ndiagnosis: transformers at the\nforefront\nFarheen Chincholi* and Harald Koestler\nDepartment of Computer Science, Chair of Computer Science /one.tnum/zero.tnum (System Simulation),\nFriedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Erlangen, Germany\nAlthough the Vision Transformer architecture has become widely acce pted as\nthe standard for image classiﬁcation tasks, using it for object detection in\ncomputer vision poses signiﬁcant challenges. This research aims t o explore the\npotential of extending the Vision Transformer for object det ection in medical\nimaging, speciﬁcally for glaucoma detection, and also includes an ex amination\nof the Detection Transformer for comparative analysis. The analys is involves\nassessing the cup-to-disc ratio and identifying signs of ver tical thinning of the\nneuroretinal rim. A diagnostic threshold is proposed, ﬂaggi ng a cup-to-disc\nratio exceeding /zero.tnum./six.tnum as a potential indicator of glaucoma. The experimental\nresults demonstrate a remarkable /nine.tnum/zero.tnum./four.tnum/eight.tnum% accuracy achieved by the pre-trained\nDetection Transformer, while the Vision Transformer exhibits competitive accuracy\nat /eight.tnum/seven.tnum./eight.tnum/seven.tnum%. Comparative evaluations leverage a previously untapped dataset\nfrom the Standardized Fundus Glaucoma Dataset available on Ka ggle, providing\nvaluable insights into automated glaucoma detection. The eval uation criteria and\nresults are comprehensively validated by medical experts speci alizing in the ﬁeld\nof glaucoma.\nKEYWORDS\nViT, DETR, object detection, medical imaging, glaucoma\n/one.tnum Introduction\nGlaucoma, a complex and progressive eye disease, stands as a major public health\nconcern worldwide. It is characterized by the gradual and irreversible deterioration of the\noptic nerve, typically associated with elevated intraocular pressure (IOP), although glaucoma\ncan also develop with normal IOP. This condition eventually leads to a gradual loss of vision\nand, if left untreated, can ultimately result in blindness. The impairment typically begins with\nperipheral vision loss and, if undetected and untreated, can advance to aﬀect central vision\nas well (\nWagner et al., 2022 ).\nGlaucoma is often referred to as the “silent thief of sight” because its early stages usually\nmanifest without noticeable symptoms or pain ( National Eye Institute, 2023 ). Individuals\nmight remain unaware of the disease until signiﬁcant vision loss has occurred. Given its\npotential for severe vision impairment and the lack of a cure, glaucoma management focuses\non early detection, continuous monitoring, and appropriate treatment to slow down or halt\ndisease progression.\nOptic nerve cupping\nThe optic nerve transmits visual signals from the eye’s retina to the brain (\nGlaucoma\nResearch Foundation, 2023 ). It comprises numerous retinal nerve ﬁbers that converge and\nexit through the optic disc situated at the eye’s posterior. The optic disc has a central section\nknown as the “cup, ” typically smaller than the entire optic disc depicted in\nFigure 1A. In\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nindividuals with glaucoma, Figure 1B shows that increased eye\npressure and/or reduced blood ﬂow to the optic nerve cause the\ndegeneration of these nerve ﬁbers. Consequently, the cup enlarges\nin relation to the optic disc due to a lack of support. Optic nerve\ncupping worsens as the cup-to-disc ratio increases. Both individuals\nwith and without optic nerve damage exhibit optic nerve cupping,\nalthough those with glaucoma often have a higher cup-to-disc ratio.\nA cup-to-disc ratio exceeding six-tenths is generally considered\nsuspicious for glaucoma. Regular optic nerve photographs enable\nmonitoring of the cup-to-disc ratio. This assists the doctor in\nassessing whether nerve ﬁber damage is ongoing despite current\ntreatment and whether treatment adjustments are necessary.\nThis paper develops deep learning-based screening software for\nthe detection and location of glaucoma from digital fundus images.\nIn recent advancements, Vision Transformer (ViT) (\nDosovitskiy\net al., 2020 ) and the Detection Transformer (DETR) ( Carion et al.,\n2020) models have gained signiﬁcant traction in the domain of\ncomputer vision. ViT, especially, has demonstrated outstanding\nperformance in image classiﬁcation. This study takes a signiﬁcant\nstride by extending the application of ViT to object detection tasks,\nparticularly focusing on its integration into medical imaging for\nthe purpose of glaucoma detection. The research zeroes in on\nidentifying the cup-to-disc (OC to OD) ratio and detecting signs of\nvertical thinning in the neuroretinal rim (tOD) as key indicators. If\nthe cup-to-disc ratio exceeds six-tenths, the condition is ﬂagged as\nglaucoma. This approach holds promise for an automated, precise,\nand eﬃcient process in glaucoma detection. The prior research\nin this ﬁeld, encompassing classical machine learning and deep\nlearning, is critically reviewed and discussed in Section 2.\n/two.tnum Related work\n/two.tnum./one.tnum Classical machine learning methods\nIn the study on glaucoma diagnosis ( An et al., 2019 ),\nthe authors developed a machine learning algorithm utilizing\noptical coherence tomography (OCT) data and color fundus\nimages. Convolutional neural networks (CNNs) with various\ninput types, including fundus images, retinal nerve ﬁber layer\nthickness maps, and ganglion cell complex thickness maps, were\nemployed. Through data augmentation and dropout, the CNNs\nachieved strong performance. To combine CNN model results,\na random forest (RF) was trained to classify disc fundus images\nof healthy and glaucomatous eyes. This classiﬁcation was based\non feature vector representations of each input image, obtained\nby removing the second fully connected layer. The combined\napproach demonstrated high accuracy, surpassing individual input\nmethods, with a 10-fold cross-validation area under the receiver\noperating characteristic curve (AUC) reaching 0.963.\nPraveena and GaneshBabu proposed a K-means clustering-\nbased approach to automatically extract the optic disc (\nPraveena\nand Ganeshbabu, 2021). The optimal K value in K-means clustering\nwas determined using hill climbing. The optic cup’s segmented\ncontour was reﬁned through elliptical and morphological ﬁtting\nmethods. The cup-to-disc ratio was calculated and compared\nwith ophthalmologist-provided values for 50 normal and 50\nglaucoma patient fundus images. The mean errors for elliptical and\nmorphological ﬁtting with K-means clustering were 4.5% and 4.1%,\nrespectively. Adopting fuzzy C-means clustering reduced the mean\nerrors to 3.83% and 3.52%. Clustering and segmentation using\nSWFCM achieved mean error rates of 3.06% and 1.67%. The fundus\nimages were sourced from Aravind Eye Hospital, Pondicherry.\nCivit-Masot et al. (2020) have developed a diagnostic aid\ntool aimed at detecting glaucoma using eye fundus images.\nThe tool is meticulously trained and tested, consisting of two\nsubsystems operating independently and integrating their results\nto enhance glaucoma detection. In the ﬁrst subsystem, a blend\nof machine learning and segmentation techniques is employed\nto autonomously detect the optic disc and cup. These detections\nare then combined, and their physical and positional features\nare extracted. On the other hand, the second subsystem employs\ntransfer learning techniques on a pre-trained convolutional neural\nnetwork (CNN) to detect glaucoma by analyzing the complete\neye fundus images. The outcomes from both subsystems are\namalgamated to diﬀerentiate positive cases of glaucoma and\nenhance the ﬁnal detection results. The research demonstrates\nthat this integrated system attains a superior classiﬁcation rate\ncompared to previous works, achieving an impressive area under\nthe curve (AUC) of 0.91.\n/two.tnum./two.tnum Deep learning methods\nChen et al. (2015) presented early work utilizing deep\nconvolutional neural networks for glaucoma detection. This study\nintroduced a deep learning architecture focusing on automated\nglaucoma diagnosis using CNNs. The proposed architecture\ncomprised four convolutional layers and two fully-connected\nlayers, demonstrating promising results in discriminating between\nglaucomatous and non-glaucomatous patterns. The authors\nutilized techniques like dropout and data augmentation to enhance\ndiagnostic performance. Their method was evaluated on the\nORIGA and SCES datasets, achieving signiﬁcant improvements\nwith an area under the curve (AUC) of the receiver operating\ncharacteristic (ROC) curve at 0.831 and 0.887 for glaucoma\ndetection in the respective databases (\nChen et al., 2015 ).\nYu et al. (2019) developed an advanced segmentation method\nfor optic disc and cup using a modiﬁed U-Net architecture. This\napproach leverages a combination of encoding layers from the\nwidely adopted pre-trained ResNet-34 model and classical U-\nNet decoding layers. The model was meticulously trained on the\nrecently available RIGA dataset, achieving an impressive average\ndice value of 97.31% for disc segmentation and 87.61% for cup\nsegmentation. These results are comparable to the performance of\nexperts in optic disc/cup segmentation and Cup-Disc-Ratio (CDR)\ncalculation on a reserved RIGA dataset. When evaluated on the\nDRISHTI-GS and RIM-ONE datasets without re-training or ﬁne-\ntuning, the model demonstrated performance on par with state-\nof-the-art methodologies reported in the literature. Furthermore,\nthe authors ﬁne-tuned the model on two databases, achieving\noutstanding results. For the DRISHTI-GS test set, the average disc\ndice value was 97.38%, and the cup dice value was 88.77%. In the\ncase of the RIM-ONE database, the model achieved a disc dice of\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nFIGURE /one.tnum\nIn (A), a healthy eye is depicted, while in (B), optic nerve cupping is illustrated, clearly demonstrating a n increase in the cup-to-disc ratio. (A) Healthy\neye. (B) Eye with glaucoma.\n96.10% and a cup dice of 84.45%. These results represent the state-\nof-the-art performance on both databases concerning cup and disc\ndice values.\nAl-Bander et al. (2017) investigated the potential of employing\ndeep learning to automatically acquire features and identify signs\nof glaucoma in colored retinal fundus images. They developed a\nfully automated system that utilized a convolutional neural network\n(CNN) to diﬀerentiate between normal and glaucomatous patterns\nfor diagnostic purposes. This innovative approach automatically\nextracted features from the raw images using CNN, diverging from\ntraditional methods that relied on manually crafted optic disc\nfeatures. The extracted features were then fed into a Support Vector\nMachine (SVM) classiﬁer to categorize the images as normal or\nabnormal. The achieved results demonstrated an accuracy of 88.2%,\na speciﬁcity of 90.8%, and a sensitivity of 85%.\nThese investigations showcase the eﬃcacy of both traditional\nmachine learning and deep learning approaches in addressing\nglaucoma detection using digital fundus images. Nevertheless,\nthere is a pressing requirement for additional research aimed\nat enhancing the precision and resilience of these techniques.\nDeveloping artiﬁcial intelligence for clinical use in glaucoma faces\nseveral challenges.\n/three.tnum Method\nThe model design incorporates the architecture of both the\nVision Transformer and Detection Transformer, shaping the\nframework for glaucoma detection as outlined in the process in\nSection 3.3.\n/three.tnum./one.tnum Vision transformer or ViT\nThe Vision Transformer (ViT)\nDosovitskiy et al. (2020) is a\ndeep learning architecture designed for computer vision tasks,\nparticularly image classiﬁcation. It’s an innovative approach that\napplies the transformer architecture, originally developed for\nnatural language processing (NLP), to process and analyze images.\nViT starts by segmenting the input image into uniform, non-\noverlapping patches as illustrated in\nFigure 2. These patches are\nconsidered as “tokens” and undergo linear embedding to convert\npixel values into higher-dimensional vectors. The patches, now\nrepresented as tokens, are essential for capturing image features. To\npreserve spatial relationships, positional encodings are introduced\nand added to these token embeddings, indicating their positions in\nthe original image. Next, these token embeddings, along with their\npositional encodings, are input into the transformer encoder. The\ntransformer comprises multiple layers, each housing two main sub-\nlayers: a multi-head self-attention mechanism and a position-wise\nfully connected feedforward neural network.\nThe ViT model is composed of multiple transformer\nblocks (\nVaswani et al., 2017 ) tailored for object detection. Within\neach block, the MultiHeadAttention layer facilitates self-attention\nover the image patch sequence. The “multi-head” concept enables\nthe model to simultaneously learn various attention patterns. The\nencoded patches (utilizing skip connections) and the outputs from\nthe self-attention layer undergo normalization and are then passed\nthrough a multilayer perceptron (MLP) (\nTaud and Mas, 2018 ).\nFollowing the attention mechanism, position-wise feedforward\nneural networks process each token independently, integrating\ninsights from the attention mechanism. These operations are\nperformed in each layer of the transformer, enabling the model to\ncomprehend intricate features at diﬀerent levels of abstraction. The\nmodel generates outputs with four dimensions, representing the\nbounding box coordinates of an object. This design is speciﬁcally\noptimized for object detection tasks, where identifying object\npositions is a primary objective (\nChincholi and Koestler, 2023 ).\nThe model is trained using labeled data (e.g., image-label pairs)\nand appropriate loss functions (e.g., cross-entropy loss). The model\nlearns to predict the correct class labels for the input images.\nViT oﬀers advantages such as scalability, as it can handle images\nof varying sizes, and parallelization, which helps in processing\npatches independently.\n/three.tnum./two.tnum DEtection TRansformer or DETR\nThe DEtection TRansformer or DETR (\nCarion et al., 2020 )\nis a transformer-based architecture utilizing an encoder-decoder\nframework with a convolutional backbone as depicted in\nFigure 3.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nFIGURE /two.tnum\nIllustration of a custom-designed Vision Transformer architect ure speciﬁcally optimized for the detection of optic disc and opti c cup.\nThe encoder employs a convolutional neural network (CNN)\nlike ResNet (\nKoonce and Koonce, 2021 ) to extract feature maps\nfrom the input image. Subsequently, the decoder, built on the\ntransformer architecture (\nVaswani et al., 2017 ), processes these\nfeature maps to produce predictions related to objects within the\nimage. To facilitate object detection, two additional heads are\nintegrated into the decoder: a linear layer for handling class labels\nand a multi-layer perceptron (MLP) for bounding box predictions.\nThe key to object detection lies in utilizing “object queries, ” which\nare responsible for identifying speciﬁc objects within an image.\nIn the context of the COCO dataset (\nLin et al., 2014 ), the model\nemploys 100 object queries.\nDuring training, the model leverages a “bipartite matching loss”\nmechanism. This involves comparing the predicted classes and\nbounding boxes generated by each of the N (in this case, 100)\nobject queries with the ground truth annotations. The annotations\nare padded to match the length of N, wherein if an image\ncontains fewer objects (e.g., 4), the remaining annotations (96\nin this example) are marked with “no object” for class and “no\nbounding box.” The Hungarian matching algorithm is employed\nto establish an optimal one-to-one mapping between the N queries\nand annotations. Subsequently, standard cross-entropy is utilized\nfor class predictions, and a combined loss comprising L1 and\ngeneralized Intersection over Union (IoU) loss is applied for\noptimizing bounding box predictions.\n/three.tnum./three.tnum Process outline\nThe ﬂowchart in\nFigure 4 outlines the sequential steps for\ndetecting the Optic Disc (OD) and Optic Cup (OC) using ViT\nand DETR. It starts with data preprocessing to prepare the input.\nFollowing this, the OD data undergoes processing. This involves\ntraining the model designed to recognize and pinpoint the optic\ndisc within the given data. The model is instructed and ﬁne-\ntuned to accurately identify the OD. Subsequently, predictions\nregarding the location and features of the OD are derived from this\ntrained model.\nSimultaneously, the OC data also undergoes a parallel\nprocessing trajectory. Similar to the OD data, the OC data is\npreprocessed and structured for meaningful analysis. A distinct\nmodel is then trained to identify the OC accurately. Predictions\nconcerning the OC are generated through this trained model,\nproviding crucial insights into its location and attributes.\nThe bounding box is deﬁned by its coordinates represented\nas ( xmin, ymin, xmax, ymax), where ( xmin, ymin) denotes the top-left\ncorner, and ( xmax, ymax) represents the bottom-right corner of the\nbounding box. To calculate the radius, the distance between the\ncenter of the bounding box and one of its corners is computed\nas follows:\nCenterx = xmin + xmax\n2\nCentery = ymin + ymax\n2\nThe distance from the center to a corner (for example, the top-\nleft corner) is determined using the Euclidean distance formula:\nDistance =\n√\n(Centerx − xmin)2 + (Centery − ymin)2 (1)\nThis distance is considered analogous to a radius for a bounding\nbox. An analysis is conducted by assessing the ratio of OC to OD,\nconsidering a threshold of 0.6. Depending on this ratio, a diagnosis\nof glaucoma, is made if the ratio exceeds 0.6. This diagnostic step\nis critical for identifying and addressing glaucoma in a timely and\neﬀective manner.\n/four.tnum Experiments\n/four.tnum./one.tnum Datasets\nTo investigate the scalability of ViT and DETR models, the\nStandardized Multi-Channel Dataset for Glaucoma (SMDG), a\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nFIGURE /three.tnum\nCustomized DETR architecture for optic disc and optic cup detect ion.\nFIGURE /four.tnum\nThe step-by-step process of detecting optic disc and optic cup u sing ViT and DETR.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\ncomprehensive open-source resource ( Ahalli, 2023 ; Deathtrooper,\n2023) accessible on Kaggle, is utilized. SMDG represents a\nharmonization eﬀort encompassing 19 public glaucoma datasets,\nstrategically curated to advance AI applications in this domain.\nThis extensive dataset aggregates diverse information, including\nfull-fundus glaucoma images and pertinent image metadata such\nas optic disc segmentation, optic cup segmentation, blood vessel\nsegmentation, along with any available per-instance textual details\nsuch as sex and age.\nRemarkably, SMDG-19 stands as the most extensive public\nrepository housing fundus images associated with glaucoma. This\nresearch speciﬁcally draws on datasets like CRFO-v4, DRISHTI-\nGS1-TRAIN, DRISHTI-GS1-TEST, G1020, ORIGA-light, PAPILA,\nand REFUGE1-TRAIN (Retinal Fundus Glaucoma Challenge 1\nTrain) from SMDG-19 as outlined in\nTable 1, constituting a\ncrucial aspect of our investigation into ViT and DETR model\nscalability. Leveraging this diverse and extensive dataset enables\na comprehensive exploration of the models’ performance and\nscalability across varied data sources, enriching the ﬁndings of\nour study.\n/four.tnum./two.tnum Training and ﬁne-tuning\n/four.tnum./two.tnum./one.tnum Preprocessing\nFor preprocessing during training and validation, images are\nresized or rescaled so that the shorter side measures at least 224\npixels while the longer side does not exceed 224 pixels. Additionally,\nthey are normalized across the RGB channels.\n/four.tnum./two.tnum./two.tnum Computing environment\nIn the training process of both models, the computing\nenvironment employed was the JupyterHub environment running\non the Alex cluster.\n/one.tnumThe cluster consists of a total of 352\nNvidia A40 GPUs, supplemented by 160 Nvidia A100 GPUs with\n40GB memory, and an additional 144 Nvidia A100 GPUs with\n80GB memory.\n/four.tnum./two.tnum./three.tnum Training\nDETR was trained using the AdamW optimizer, an extension\nof the Adam optimizer that incorporates weight decay to prevent\noverﬁtting. The training process was facilitated by PyTorch\nLightning and the Hugging Face Transformers library.\n/two.tnumPyTorch\nLightning simpliﬁes the training loop, providing a more modular\nand readable structure, while the Hugging Face Transformers\nlibrary oﬀers pre-trained transformer models and tools for various\nnatural language processing tasks. The forward method of the\nLightning module was implemented to take pixel values and masks\nas inputs, generating model predictions—a crucial step in the\ntraining process. The initial learning rate for the transformer\nwas set to 1 × 10− 4, and for the backbone, it was 1 × 10− 5.\nAdditionally, a weight decay of 1 × 10− 4 was applied. Transformer\n/one.tnumhttps://hpc.fau.de/systems-services/documentation-instr uctions/\nclusters/alex-cluster\n/two.tnumhttps://huggingface.co/facebook/detr-resnet-/five.tnum/zero.tnum\nweights were initialized using Xavier initialization, a technique\nthat contributes to stable training by appropriately scaling weights\nbased on the number of input and output units in a layer.\nA ResNet50 backbone, pre-trained on ImageNet—a large-scale\ndataset of images—was utilized. The training schedule spanned 300\nepochs, with each epoch involving a pass over all training images.\nHyperparameterization techniques were employed for ﬁne-tuning\nto optimize the training process.\nThe ViT model underwent training for 1,000 epochs, utilizing\nthe Adam optimizer with a batch size of 16 and a weight\ndecay of 0.0001, as per\nZhang (2018). This weight decay value,\ncontrary to common preference for Stochastic Gradient Descent\n(SGD), has demonstrated eﬃcacy in model transfer. The training\nprotocol incorporated a linear learning rate set at 0.001, along\nwith a decay strategy. For ﬁne-tuning, SGD with momentum\nwas employed, accompanied by a reduced batch size of 8. The\ntraining procedure involved extracting and reshaping patches\nfrom input images using a custom Patches Layer, with a patch\nsize of 32 × 32. Each image was divided into 49 patches, resulting\nin 3,072 elements per patch. The patches were then encoded\nusing learned projections and positional embeddings through\nthe PatchEncoder Layer. Transformer Blocks, consisting of layer\nnormalization, multi-head attention, skip connections, and an\nMLP with head units conﬁgured as [2, 048, 1, 024, 512, 64, 32],\nfacilitated robust feature extraction. The transformer units\nwere characterized by dimensions of projection_dim*\n2and projection_dim, with 4 transformer layers. The\nRepresentation Layer ﬂattened the output and applied dropout\nfor enhanced feature representation. Finally, the Bounding Box\nOutput layer produced four neurons representing bounding\nbox coordinates. This comprehensive training approach and\narchitecture, incorporating eﬀective optimization strategies,\ncontribute to the model’s performance in object detection tasks.\n/four.tnum./three.tnum Optic disc and optic cup detection and\nlocalization\nIn this section, a concise evaluation of the detection\nand localization performance for the optic disc (OD) and\noptic cup (OC) is provided.\nFigure 5 visually presents the\nbounding box coordinates [x_min, y_min, x_max, y_max]\nrepresenting both the ground truth and predicted objects. These\ncoordinates are obtained by applying the trained model to the\ntest data.\nThe accompanying table beneath\nFigure 5 oﬀers an in-depth\ncomparison between the ground truth and predicted bounding box\ncoordinates for both OC and OD for the bottom most image. The\ntable includes calculated radii and the OC/OD ratio. The bounding\nbox coordinates are presented in the format [x_min, y_min, x_max,\ny_max]. The radii for the optic disc and optic cup are approximate\nvalues determined using\nEquation (1). The OC/OD ratio, calculated\nas 0.7 in this case, is indicative of a potential diagnosis of glaucoma.\nThis ratio serves as a key metric in evaluating the health of\nthe optic nerve head based on the detected optic disc and optic\ncup parameters.\nTable 2 provides a detailed comparison between ground truth\nand predicted values for both OD and OC on the test data. Each row\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nTABLE /one.tnumOverview of utilized datasets sourced from the Standardized Fund us Glaucoma Dataset (SMDG-/one.tnum/nine.tnum) accessible on Kaggle (Ahalli, /two.tnum/zero.tnum/two.tnum/three.tnum).\nDataset CRFO-v/four.tnumG/one.tnum/zero.tnum/two.tnum/zero.tnumDRISHTI\nGS/one.tnum-TRAIN\nDRISHTI\nGS/one.tnum-TEST\nORIGA\n-light\nPAPILA REFUGE/one.tnum\n-TRAIN\n0 (Non-Glaucoma) 31 724 18 13 482 333 360\n1 (Glaucoma) 48 296 32 38 168 87 40\nFIGURE /five.tnum\nThe left ﬁgure displays the output generated by the models, whereas the right ﬁgure exhibits the annotated positions of the optic cup (OC) and opt ic\ndisc (OD).\ncorresponds to a speciﬁc image (labeled 1 to 9) for both OD and\nOC. The last section of the table presents calculations, including\nthe OC/OD ratio for ground truth and predictions, along with\nactual and predicted classiﬁcations. This comprehensive evaluation\naids in assessing the model’s accuracy in predicting OD and OC\nparameters and is crucial for diagnosing ocular health.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nTABLE /two.tnumThe table displays both the ground truth and predictions derived fr om the test data, including the corresponding radius values for eac h.\nImage Ground truth Predicted Radius\nGround truth Predicted\nOPTIC DISC 1 [186, 209, 259, 292] [190, 209, 270, 291] 41.5 41.1\n2 [195, 214, 251, 280] [201, 217, 258, 280] 33 31.5\n3 [254, 239, 286, 271] [245, 237, 285, 281] 16 21.9\n4 [231, 236, 275, 277] [231, 234, 276, 282] 22 24.4\n5 [253, 237, 300, 281] [250, 240, 291, 282] 23.5 20.6\n6 [227, 239, 267, 276] [222, 238, 263, 279] 20 20.7\n7 [228, 239, 275, 291] [223, 242, 277, 295] 26 26.9\n8 [223, 251, 269, 297] [223, 252, 266, 295] 23 21.6\n9 [200, 228, 285, 315] [205, 234, 278, 314] 43.5 40.1\nOPTIC CUP 1 [195. 214. 251. 280.] [201, 217, 258, 280] 33 31.5\n2 [254. 239. 286. 271.] [245, 237, 285, 281] 16 21.9\n3 [231. 236. 275. 277.] [231, 234, 276, 282] 22 24.4\n4 [253. 237. 300. 281.] [250, 240, 291, 282] 23.5 20.6\n5 [227. 239. 267. 276.] [222, 238, 263, 279] 20 20.7\n6 [228. 239. 275. 291.] [223, 242, 277, 295] 26 26.9\n7 [223. 251. 269. 297.] [223, 252, 266, 295] 23 21.6\n8 [200. 228. 285. 315.] [205, 234, 278, 314] 43.5 40.1\n9 [200. 216. 285. 304.] [203, 219, 280, 301] 44 41.1\nImage Radius Classiﬁcation\nGround truth OC/OD Predicted\nOC/OD\nActual Prediction\nCalculations 1 0.8 0.8 1 1\n2 0.5 0.6 0 1\n3 0.6 0.6 1 1\n4 0.7 0.5 1 0\n5 0.6 0.6 1 1\n6 0.6 0.6 1 1\n7 0.5 0.5 0 0\n8 0.8 0.8 1 1\n9 0.8 0.7 1 1\nAdditionally, it provides a thorough calculation of the ratio between th e optic cup (OC) and optic disc (OD) (OC/OD).\n/four.tnum./four.tnum Comparison of ViT and DETR model\nresults\nIn this section, a comparison between ViT and DETR will be\nconducted, focusing on performance evaluation, ROC Curves, and\nPrecision-Recall Curves.\n/four.tnum./four.tnum./one.tnum Performance\nThe\nTable 3 presents a performance comparison of our object\ndetection models, DETR and ViT, with the state-of-the-art methods\ndiscussed in Section 2. For DETR, the model achieved an accuracy\nof 92.30% on the DRISHTI dataset with IoU of 0.94 and speciﬁcity\nof 0.85. When applied to a broader range of datasets (DRISHTI,\nCRFO, G1020, ORIGA, PAPILA, REFUGE1), DETR maintained a\nnotable accuracy of 90.48% with IoU of 0.94 and speciﬁcity of 0.75.\nOn the other hand, ViT exhibited an accuracy of 88% with\nIoU of 1 and speciﬁcity of 0.33 on the DRISHTI dataset. When\ntested on a wider set of datasets (DRISHTI, CRFO, G1020, ORIGA,\nPAPILA, REFUGE1), ViT showcased an accuracy of 87.87% with\nIoU of 0.92 and speciﬁcity of 0.84. Overall, the results highlight\nDETR’s slightly higher accuracy and speciﬁcity compared to ViT\nacross the evaluated datasets. The results suggest that DETR and\nViT exhibit competitive performance in comparison to existing\nmethods, showcasing their potential in glaucoma detection in the\ndomain of medical image analysis.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nTABLE /three.tnumPerformance comparison of DETR and ViT with state-of-the-art me thods. Evaluation metrics include accuracy, sensitivity, spe ciﬁcity, AUC, and\nROC.\nModel Datasets Accuracy Sensitivity Speciﬁcity AUC ROC\nDETR[ours] DRISHTI 92.30 0.94 0.85 0.91 0.89\nALL DATASETS 90.48 0.94 0.75 0.88 0.87\nViT[ours] DRISHTI 88 1 0.33 0.85 0.80\nALL DATASETS 87.87 0.92 0.84 0.86 0.82\nCNN + RF ( An et al., 2019 ) OWN DATA - - - 0.96 -\nML+CNN (Civit-Masot et al.,\n2020)\nRIM-ONE V3,\nDRISHTI\n- - - 0.91 -\nCNN (Chen et al., 2015 ) ORIGA and SCES - - - 0.83 0.88\nCNN (Al-Bander et al., 2017 ) DRISHTI-GS,\nRIM-ONE, ONHSD\n88.20 0.85 0.90 - -\nThe results indicate that DETR and ViT demonstrate comparable perform ance when compared to established methods, highlighting their pote ntial for eﬀective glaucoma detection within the\nﬁeld of medical image analysis.\nFIGURE /six.tnum\n(A) ROC Curve for “DETR” and “ViT” models, with AUC of /zero.tnum./nine.tnum/five.tnum and /zero.tnum./nine.tnum/zero.tnum respectively (DRISHTI), and /zero.tnum./nine.tnum/zero.tnum and /zero.tnum./nine.tnum/five.tnum respectively (all datasets). (B)\nPrecision-Recall Curve, with AP of /zero.tnum./nine.tnum/eight.tnum (DRISHTI) and /zero.tnum./nine.tnum/four.tnum (all datasets) for “DETR,” and AP of /zero.tnum./eight.tnum/nine.tnum (DRISHTI) and /zero.tnum./seven.tnum/seven.tnum (alldatasets) for “ViT.”\n/four.tnum./four.tnum./two.tnum ROC curves\nThe ROC curve for the “DETR” model highlights its exceptional\nability to achieve a high True Positive Rate (sensitivity) even\nat low False Positive Rates (1-speciﬁcity). This is reﬂected in\nits impressively high area under the curve (AUC) of about\n0.95 for DRISHTI and 0.90 for all datasets, indicating strong\ndiscriminative power (refer to\nFigure 6A). Conversely, the ROC\ncurve for the “ViT” model demonstrates a commendable True\nPositive Rate at low False Positive Rates, yielding an AUC\nof approximately 0.90 for DRISHTI and 0.85 for all datasets.\nAlthough slightly lower than “DETR, ” it still signiﬁes robust model\nperformance.\n/four.tnum./four.tnum./three.tnum Precision-recall curves\nIn the Precision-Recall curve analysis, the “DETR” model\nshowcases excellent precision at relatively high recall values,\nillustrating its eﬀectiveness in identifying true positive cases while\nminimizing false positives. In\nFigure 6B, the area under the curve\n(AP) for “DETR” is approximately 0.98 for DRISHTI and 0.94 for\nall datasets, indicating a strong precision-recall trade-oﬀ. On the\nother hand, the “ViT” model displays a reasonable precision-recall\ntrade-oﬀ, achieving an AP of approximately 0.89 for DRISHTI and\n0.77 for all datasets. This suggests its ability to eﬀectively balance\nprecision and recall.\nBoth models demonstrate strong discriminative power and\na good balance between precision and recall. The ROC and\nPrecision-Recall curves underscore the eﬃcacy of the “DETR”\nmodel, particularly in terms of discriminative ability. However, the\n“ViT” model also performs admirably, striking a commendable\nbalance between precision and recall. The choice of model may\ndepend on speciﬁc task requirements, with “DETR” excelling in\ndiscriminative power and “ViT” showcasing a favorable precision-\nrecall trade-oﬀ.\n/five.tnum Discussion and future work\nIn conclusion, our investigation into advanced deep learning\nmodels, notably the Vision Transformer (ViT) and the Detection\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nTransformer (DETR), for glaucoma detection presents an exciting\nopportunity to revolutionize this vital medical domain. We have\ndelved into applying Transformers directly to diverse computer\nvision tasks, including object detection, a pivotal challenge outlined\nin the original ViT research. By viewing retinal images as sequences\nof patches and harnessing the power of a Transformer-based\narchitecture, ViT eﬀectively captures intricate patterns and features\ncritical to glaucoma detection, devoid of image-speciﬁc biases.\nThis hints at its promising role in automating glaucoma diagnosis.\nConsequently, ViT either matches or surpasses the state of the art\non numerous image classiﬁcation datasets, all while being relatively\ncost-eﬀective to pre-train.\nOn a diﬀerent note, DETR, tailored for object detection,\ndemonstrates signiﬁcant potential in precisely localizing speciﬁc\nfeatures within retinal images indicative of glaucoma. This\ncapability enables the accurate detection of aﬀected regions. Its\nend-to-end detection approach aligns well with the imperative\nof precisely identifying glaucomatous areas, contributing to\nheightened diagnostic accuracy. DETR boasts a straightforward\nimplementation and a ﬂexible architecture that can be easily\nextended to object detection in medical imaging, yielding\ncompetitive results.\nWhile these preliminary ﬁndings are promising, the\ndevelopment of AI for clinical practice in glaucoma faces\nsigniﬁcant challenges. Standardizing diagnostic criteria is\ncrucial given the absence of a universally accepted deﬁnition\nfor glaucoma. The wide spectrum of the disease and the\nshortage of glaucoma experts globally contribute to variations\nin diagnosis and treatment criteria, hindering the development\nof diagnostic devices. Additionally, collecting detailed data on\nneurodegenerative and systemic metabolic conditions alongside\nglaucoma data is crucial for predicting progression. Multifactorial\nassociations between glaucoma and diseases like diabetes and\nhypertension emphasize the need for AI technology to reveal\nintricate relationships between systemic conditions and retinal\nimages. To address data shortages and biases, generative AI\ntechniques such as generative adversarial networks (GAN) and\nemerging diﬀusion models oﬀer solutions. These models provide\ndata augmentation by generating realistic fundus photographs,\novercoming challenges associated with imbalanced medical data.\nAs these generative AI techniques continue to evolve, they hold\nthe promise of synthesizing realistic fundus images based on\nimproved data quality, advancing the ﬁeld of glaucoma diagnosis\nand treatment.\nAnother major hurdle is the ongoing exploration of self-\nsupervised pre-training methods. Our initial experiments have\nshown improvements from self-supervised pre-training, but a\nsubstantial gap persists between self-supervised and large-scale\nsupervised pre-training. Furthermore, scaling up ViT and DETR is\nlikely to lead to enhanced performance. Additionally, a signiﬁcant\nchallenge lies in further optimizing these models to cater to the\nunique intricacies of glaucoma detection, such as subtle structural\nchanges in the optic nerve head and retinal nerve ﬁber layer.\nCustomizing ViT and DETR to extract and interpret features\nspeciﬁc to glaucoma pathology is essential for enhancing their\neﬀectiveness and reliability in a clinical context.\nData availability statement\nPublicly available datasets were analyzed in this study. This data\ncan be found here: https://www.kaggle.com/datasets/deathtrooper/\nmultichannel-glaucoma-benchmark-dataset .\nAuthor contributions\nFC: Conceptualization, Data curation, Formal analysis,\nInvestigation, Methodology, Resources, Software, Validation,\nVisualization, Writing – original draft, Writing – review &\nediting. HK: Project administration, Supervision, Writing – review\n& editing.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research, authorship, and/or publication of this article.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAhalli, A. D. (2023). Smdg modiﬁed . Available online at: https://www.kaggle.com/\ndatasets/agattadahalli/smdg-modiﬁed\nAl-Bander, B., Al-Nuaimy, W., Al-Taee, M. A., and Zheng, Y. (201 7).\n“Automated glaucoma diagnosis using deep learning approach, ” i n 2017 14th\nInternational Multi-Conference on Systems, Signals and Devices (SSD ) 207–210.\ndoi: 10.1109/SSD.2017.8166974\nAn, G., Omodaka, K., Hashimoto, K., Tsuda, S., Shiga, Y., Taka da, N., et al. (2019).\nGlaucoma diagnosis with machine learning based on optical coher ence tomography\nand color fundus images. J. Healthc. Eng . 2019:4061313. doi: 10.1155/2019/40\n61313\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,\nS. (2020). “End-to-end object detection with transformers , ” in European\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nChincholi and Koestler /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/nine.tnum\nConference on Computer Vision (Springer), 213–229. doi: 10.1007/978-3-030-58\n452-8_13\nChen, X., Xu, Y., Kee Wong, D. W., Wong, T. Y., and Liu, J. (2015 ). “Glaucoma\ndetection based on deep convolutional neural network, ” in 2015 37th Annual\nInternational Conference of the IEEE Engineering in Medicine and Biolog y Society\n(EMBC) 715–718. doi: 10.1109/EMBC.2015.7318462\nChincholi, F., and Koestler, H. (2023). Detectron2 for lesion d etection in diabetic\nretinopathy. Algorithms 16:147. doi: 10.3390/a16030147\nCivit-Masot, J., Domínguez-Morales, M. J., Vicente-Díaz, S ., and Civit, A. (2020).\nDual machine-learning system to aid glaucoma diagnosis using disc and cup feature\nextraction. IEEE Access 8, 127519–127529. doi: 10.1109/ACCESS.2020.3008539\nDeathtrooper. (2023). Smdg dataset. Available online at: https://www.kaggle.com/\ndatasets/deathtrooper/multichannel-glaucoma-benchmark-d ataset\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D. , Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929 .\nGlaucoma Research Foundation (2023). Optic nerve cupping . Available online at:\nhttps://glaucoma.org/optic-nerve-cupping/ (accessed August 17, 2023).\nKoonce, B., and Koonce, B. (2021). “Resnet 50, ” in Convolutional Neural Networks\nwith Swift for Tensorﬂow: Image Recognition and Dataset Categor ization 63–72.\ndoi: 10.1007/978-1-4842-6168-2_6\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ram anan, D., et al. (2014).\n“Microsoft coco: common objects in context, ” in Computer Vision-ECCV 2014: 13th\nEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedi ngs, Part V\n13 (Springer), 740–755. doi: 10.1007/978-3-319-10602-1_48\nNational Eye Institute (2023). Glaucoma: The silent thief begins to tell\nits secrets . Available online at: https://www.nei.nih.gov/about/news-and-\nevents/news/glaucoma-silent-thief-begins-tell-its-secre ts (accessed August 17,\n2023).\nPraveena, R., and Ganeshbabu, T. (2021). Determination of c up to disc ratio\nusing unsupervised machine learning techniques for glaucoma de tection. Molec. Cell.\nBiomech. 18:69. doi: 10.32604/mcb.2021.014622\nTaud, H., and Mas, J. (2018). “Multilayer perceptron (mlp), ” in Geomatic Approaches\nfor Modeling Land Change Scenarios 451–455. doi: 10.1007/978-3-319-60801-3_27\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et\nal. (2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems 30.\nWagner, I. V., Stewart, M. W., and Dorairaj, S. K. (2022). Upda tes on\nthe diagnosis and management of glaucoma. Mayo Clin. Proc . 6, 618–635.\ndoi: 10.1016/j.mayocpiqo.2022.09.007\nYu, S., Xiao, D., Frost, S., and Kanagasingam, Y. (2019). Rob ust optic disc and cup\nsegmentation with deep learning for glaucoma detection. Computer. Med. Imag. Graph .\n74, 61–71. doi: 10.1016/j.compmedimag.2019.02.005\nZhang, Z. (2018). “Improved adam optimizer for deep neural net works, ” in 2018\nIEEE/ACM 26th International Symposium on Quality of Service (IWQo S) (IEEE), 1–2.\ndoi: 10.1109/IWQoS.2018.8624183\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org",
  "topic": "Glaucoma",
  "concepts": [
    {
      "name": "Glaucoma",
      "score": 0.8305339813232422
    },
    {
      "name": "Computer science",
      "score": 0.6317389607429504
    },
    {
      "name": "Transformer",
      "score": 0.4969022572040558
    },
    {
      "name": "Artificial intelligence",
      "score": 0.494578093290329
    },
    {
      "name": "Computer vision",
      "score": 0.4291139543056488
    },
    {
      "name": "Optometry",
      "score": 0.4232883155345917
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.41581130027770996
    },
    {
      "name": "Medicine",
      "score": 0.2252398133277893
    },
    {
      "name": "Ophthalmology",
      "score": 0.1950630247592926
    },
    {
      "name": "Engineering",
      "score": 0.16166386008262634
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I181369854",
      "name": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
      "country": "DE"
    }
  ],
  "cited_by": 8
}