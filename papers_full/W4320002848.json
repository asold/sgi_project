{
  "title": "Electric Power Audit Text Classification With Multi-Grained Pre-Trained Language Model",
  "url": "https://openalex.org/W4320002848",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099543427",
      "name": "Qinglin Meng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2103229335",
      "name": "Yan Song",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2041400031",
      "name": "Jian Mu",
      "affiliations": [
        "Tianjin Research Institute of Electric Science (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2741591208",
      "name": "Yuanxu Lv",
      "affiliations": [
        "State Grid Corporation of China (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2104212764",
      "name": "Jiachen Yang",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A1987687092",
      "name": "Liang Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2104010399",
      "name": "Jin Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2114124927",
      "name": "Junwei Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2106139259",
      "name": "Wei Yao",
      "affiliations": [
        "Taiyuan Heavy Industry (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2036086788",
      "name": "Rui Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2222412163",
      "name": "Mao-xiang Xiao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A1972356892",
      "name": "Qingyu Meng",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1560851690",
    "https://openalex.org/W2937423263",
    "https://openalex.org/W2914767245",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W2296194829",
    "https://openalex.org/W6742080785",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W6680300913",
    "https://openalex.org/W6758930985",
    "https://openalex.org/W3177382889",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2884181179",
    "https://openalex.org/W3105625590"
  ],
  "abstract": "Electric power audit text classification is one of the important research problem in electric power systems. Recently, kinds of automatic classification methods for these texts based on machine learning or deep learning models have been applied. At present, the development of computing technology makes &#x201C;pre-training and fine-tuning&#x201D; the newest paradigm of text classification, which achieves better results than previous fully-supervised models. Based on pre-training theory, domain-related pre-training tasks can enhance the performance of downstream tasks in the specific domain. However, existing pre-training models usually use general corpus for pre-training, and do not use texts related to the field of electric power, especially electric power audit texts. This results in that the model does not learn too much electric-power-related morphology or semantics in the pre-training stage, so that less information can be used in the fine-tuning stage. Based on the research status, in this paper, we propose EPAT-BERT, a BERT-based model pre-trained by two-granularity pre-training tasks: word-level masked language model and entity-level masked language model. These two tasks predict word and entity in electric-power-related texts to learn abundant morphology and semantics about electric power. We then fine-tune EPAT-BERT for electric power audit text classification task. The experimental results show that, compared with fully supervised machine learning models, neural network models, and general pre-trained language models, EPAT-BERT can significantly outperform existing models in a variety of evaluation metrics. Therefore, EPAT-BERT can be further applied to electric power audit text classification. We also conduct ablation studies to prove the effectiveness of each component in EPAT-BERT to further illustrate our motivations.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8235176801681519
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6175015568733215
    },
    {
      "name": "Natural language processing",
      "score": 0.5317553281784058
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5160152912139893
    },
    {
      "name": "Machine learning",
      "score": 0.46527525782585144
    },
    {
      "name": "Language model",
      "score": 0.4593295454978943
    },
    {
      "name": "Audit",
      "score": 0.4252234101295471
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210097143",
      "name": "Tianjin Research Institute of Electric Science (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I17442442",
      "name": "State Grid Corporation of China (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210110794",
      "name": "Taiyuan Heavy Industry (China)",
      "country": "CN"
    }
  ],
  "cited_by": 29
}