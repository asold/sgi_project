{
  "title": "Investigating Pretrained Language Models for Graph-to-Text Generation",
  "url": "https://openalex.org/W3042876905",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2951364960",
      "name": "Leonardo F. R. Ribeiro",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2161316877",
      "name": "Martin Schmitt",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2035156685",
      "name": "Hinrich Schütze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A40109512",
      "name": "Iryna Gurevych",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2250342921",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2935206035",
    "https://openalex.org/W3039127676",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2972571786",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2931198394",
    "https://openalex.org/W3034731179",
    "https://openalex.org/W3212259963",
    "https://openalex.org/W2924961378",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964116568",
    "https://openalex.org/W3115328016",
    "https://openalex.org/W3035521632",
    "https://openalex.org/W3214637961",
    "https://openalex.org/W3003446182",
    "https://openalex.org/W2964079512",
    "https://openalex.org/W2951211142",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W2604799547",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W3100544532",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2971087717",
    "https://openalex.org/W2757978590",
    "https://openalex.org/W2798749466",
    "https://openalex.org/W102708294",
    "https://openalex.org/W2950457956",
    "https://openalex.org/W3034987089",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W3088227725",
    "https://openalex.org/W3175591469",
    "https://openalex.org/W3029413185",
    "https://openalex.org/W2962950136",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3175711626",
    "https://openalex.org/W2125417976",
    "https://openalex.org/W2971187756",
    "https://openalex.org/W2998702685",
    "https://openalex.org/W3035586188",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3167159170",
    "https://openalex.org/W2970686438",
    "https://openalex.org/W2951309718",
    "https://openalex.org/W1970849810",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2798552002"
  ],
  "abstract": "Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recently proposed pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. In particular, we report new state-of-the-art BLEU scores of 49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis, we identify possible reasons for the PLMs' success on graph-to-text tasks. We find evidence that their knowledge about true facts helps them perform well even when the input graph representation is reduced to a simple bag of node and edge labels.",
  "full_text": "Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 211–227\nNovember 10, 2021. ©2021 Association for Computational Linguistics\n211\nInvestigating Pretrained Language Models for Graph-to-Text Generation\nLeonardo F. R. Ribeiro†, Martin Schmitt‡, Hinrich Schütze‡and Iryna Gurevych†\n†Research Training Group AIPHES and UKP Lab, Technical University of Darmstadt\n‡Center for Information and Language Processing (CIS), LMU Munich\nwww.ukp.tu-darmstadt.de\nAbstract\nGraph-to-text generation aims to generate ﬂu-\nent texts from graph-based data. In this paper,\nwe investigate two recent pretrained language\nmodels (PLMs) and analyze the impact of dif-\nferent task-adaptive pretraining strategies for\nPLMs in graph-to-text generation. We present\na study across three graph domains: meaning\nrepresentations, Wikipedia knowledge graphs\n(KGs) and scientiﬁc KGs. We show that\napproaches based on PLMs BART and T5\nachieve new state-of-the-art results and that\ntask-adaptive pretraining strategies improve\ntheir performance even further. We report\nnew state-of-the-art BLEU scores of 49.72 on\nAMR-LDC2017T10, 59.70 on WebNLG, and\n25.66 on AGENDA datasets - a relative im-\nprovement of 31.8%, 4.5%, and 42.4%, re-\nspectively, with our models generating signiﬁ-\ncantly more ﬂuent texts than human references.\nIn an extensive analysis, we identify possible\nreasons for the PLMs’ success on graph-to-\ntext tasks. Our ﬁndings suggest that the PLMs\nbeneﬁt from similar facts seen during pretrain-\ning or ﬁne-tuning, such that they perform well\neven when the input graph is reduced to a sim-\nple bag of node and edge labels.1\n1 Introduction\nGraphs are important data structures in NLP as\nthey represent complex relations within a set of\nobjects. For example, semantic and syntactic struc-\ntures of sentences can be represented using differ-\nent graph representations (e.g., AMRs, Banarescu\net al., 2013; semantic-role labeling, Surdeanu et al.,\n2008; syntactic and semantic graphs, Belz et al.,\n2011) and knowledge graphs (KGs) are used to\ndescribe factual knowledge in the form of relations\nbetween entities (Gardent et al., 2017; V ougiouklis\net al., 2018; Koncel-Kedziorski et al., 2019).\nGraph-to-text generation, a subtask of data-to-\ntext generation (Gatt and Krahmer, 2018), aims to\n1Our code is available at https://github.com/UKPLab/plms-\ngraph2text.\ncreate ﬂuent natural language text to describe an\ninput graph (see Figure 1). This task is important\nfor NLP applications such as dialogue generation\n(Moon et al., 2019) and question answering (Duan\net al., 2017). Recently, it has been shown that\nstructured meaning representation, such as AMR\nor KG, can store the internal state of a dialog sys-\ntem, providing core semantic knowledge (Bonial\net al., 2020; Bai et al., 2021) or can be the result\nof a database query for conversational QA (Yu\net al., 2019). Moreover, dialog states can be repre-\nsented as KGs to encode compositionality and can\nbe shared across different domains, slot types and\ndialog participators (Cheng et al., 2020).\nTransfer learning has become ubiquitous in NLP\nand pretrained Transformer-based architectures\n(Vaswani et al., 2017) have considerably outper-\nformed prior state of the art in various downstream\ntasks (Devlin et al., 2019; Yang et al., 2019a; Liu\net al., 2020; Radford et al., 2019).\nIn this paper, we analyze the applicability of\ntwo recent text-to-text pretrained language mod-\nels (PLMs), BART (Lewis et al., 2020) and T5\n(Raffel et al., 2019), for graph-to-text generation.\nWe choose these models because of their encoder-\ndecoder architecture, which makes them particu-\nlarly suitable for conditional text generation. Our\nstudy comprises three graph domains (meaning rep-\nresentations, Wikipedia KGs, and scientiﬁc KGs).\nWe further introduce task-adaptive graph-to-text\npretraining approaches for PLMs and demonstrate\nthat such strategies improve the state of the art by\na substantial margin.\nWhile recent works have shown the beneﬁt of\nexplicitly encoding the graph structure in graph-to-\ntext generation (Song et al., 2018; Ribeiro et al.,\n2019, 2020; Schmitt et al., 2020; Zhao et al., 2020a,\nto name a few), our approaches based on PLMs\nconsistently outperform these models, even though\nPLMs – as sequence models – do not exhibit any\n212\nLinearized representation: <H> Apollo 12 <R> backup pilot <T> \nAlfred Worden <H> Alan Bean <R> was a crew member of <T> \nApollo 12 <H> Apollo 12 <R> operator <T> NASA <H> Alan Bean \n<R> occupation <T> Test pilot <H> Apollo 12 <R> commander <T> \nDavid Scott <H> Alan Bean <R> was selected by NASA <T> 1963 \n<H> Alan Bean <R> alma Mater <T> UT Austin B.S. 1955 \nTest Pilot\na)\nbackupPilot\noccupation\nAlan \nBean\ncrewMember\nApollo 12\nalmaMater\nUT Austin, B.S.\n1995\nAlfred Worden\nNasa\noperator\nDavid Scott\ncommander\nselection\n1963\nText: Alan Bean graduated from UT Austin in 1955 with a Bachelor \nof Science degree. He was hired by NASA in 1963 and served as a \ntest pilot. Apollo 12's backup pilot was Alfred Worden and was \ncommanded by David Scott.\nText: As his children, we feel very terrible now.\nLinearized representation:  ( feel :ARG0 ( we ) :ARG1 \n( terrible :degree ( very ) ) :time ( now ) :ARG1-of \n( cause :ARG0 ( have-rel-role :ARG0 we :ARG1 ( he ) :ARG2 \n( child ) ) ) )\ncause-01\nARG1ARG0\nfeel-01have-rel-role-91\nchild he we terrible-01 now\nvery\ntime\nARG1ARG0\ndegree\nARG0ARG2 ARG1\nb)\nFigure 1: Examples of (a) AMR and (b) WebNLG graphs, the input for the models and the reference texts.\ngraph-speciﬁc structural bias.2 Simply represent-\ning the graph as a linear traversal (see Figure 1)\nleads to remarkable generation performance in the\npresence of a strong language model. In our analy-\nsis we investigate to what extent ﬁne-tuned PLMs\nmake use of the graph structure represented in the\ngraph linearization. We notably observe that PLMs\nachieve high performance on two popular KG-to-\ntext benchmarks even when the KG is reduced to a\nmere bag of node and edge labels.\nOur contributions are the following:\n• We investigate and compare two PLMs, BART\nand T5, for graph-to-text generation, explor-\ning language model adaptation (LMA ) and\nsupervised task adaptation (STA) pretraining,\nemploying additional task-speciﬁc data.\n• Our approaches consistently outperform the\nstate of the art by signiﬁcant margins, ranging\nfrom 2.6 to 12.0 BLEU points, on three es-\ntablished graph-to-text benchmarks from dif-\nferent domains, exceeding specialized graph\narchitectures (e.g., Graph Neural Networks,\nGNNs, Kipf and Welling, 2017).\n• In a crowdsourcing experiment, we demon-\nstrate that our methods generate texts with sig-\nniﬁcantly better ﬂuency than existing works\nand the human references.\n• We discover that PLMs perform well even\nwhen trained on a shufﬂed linearized graph\nrepresentation without any information about\nconnectivity (bag of node and edge labels),\nwhich is surprising since prior studies showed\nthat explicitly encoding the graph structure\nimproves models trained from scratch (e.g.,\n2The model architecture does not explicitly encode the\ngraph structure, i.e., which entities are connected to each\nother, but has to retrieve it from a sequence that tries to encode\nthis information.\nZhao et al., 2020a); and investigate the possi-\nble reasons for such a good performance.\n2 Related Work\nGraph-to-text Learning. Various neural models\nhave been proposed to generate sentences from\ngraphs from different domains. Konstas et al.\n(2017) propose the ﬁrst neural approach for AMR-\nto-text generation that uses a linearized input graph.\nPrior approaches for KG-to-text generation train\ntext-to-text neural models using sequences of KG\ntriples as input (Trisedya et al., 2018; Moryossef\net al., 2019; Castro Ferreira et al., 2019; Ribeiro\net al., 2021a).\nRecent approaches (Marcheggiani and Perez Bel-\ntrachini, 2018; Song et al., 2018; Beck et al., 2018;\nDamonte and Cohen, 2019; Ribeiro et al., 2019;\nZhao et al., 2020a; Schmitt et al., 2021; Ribeiro\net al., 2021b) propose architectures based on GNNs\nto directly encode the graph structure, whereas\nother efforts (Ribeiro et al., 2020; Schmitt et al.,\n2020; Yao et al., 2020; Wang et al., 2020) inject the\ngraph structure information into Transformer-based\narchitectures. The success of those approaches sug-\ngests that imposing a strong relational inductive\nbias into the graph-to-text model can assist the gen-\neration.\nPretrained Language Models. Pretrained\nTransformer-based models, such as BERT (Devlin\net al., 2019), XLNet (Yang et al., 2019b), or\nRoBERTa (Liu et al., 2020), have established a\nqualitatively new level of baseline performance for\nmany widely used natural language understanding\n(NLU) benchmarks. Generative pretrained\nTransformer-based methods, such as GPT-2\n(Radford et al., 2019), BART (Lewis et al., 2020),\nand T5 (Raffel et al., 2019), are employed in many\n213\nnatural language generation (NLG) tasks.\nMager et al. (2020) were the ﬁrst to employ\nGPT-2, a decoder-only PLM, for AMR-to-text gen-\neration and use cycle consistency to improve the\nadequacy. In contrast, we are the ﬁrst to inves-\ntigate BART and T5 models, which have both a\nTransformer-based encoder and decoder, in AMR-\nto-text generation. Recently, Harkous et al. (2020)\nand Kale (2020) demonstrate state-of-the-art re-\nsults in different data-to-text datasets, employing\nGPT-2 and T5 models respectively. Radev et al.\n(2020) propose DART, a new data-to-text dataset,\nand train a BART model gradually augmenting the\nWebNLG training data with DART data.\nHoyle et al. (2021) explore scaffolding objec-\ntives in PLMs and show gains in low-resource\ngraph-to-text settings. Different from the above\nworks, we focus on a general transfer learning\nstrategies for graph-to-text generation, investigat-\ning task-adaptive pretraining approaches, employ-\ning additional collected task-speciﬁc data for dif-\nferent PLMs (BART and T5) and benchmarks. In\naddition, we provide a detailed analysis aimed at\nexplaining the good performance of PLMs on KG-\nto-text tasks.\nRecently, Gururangan et al. (2020) explored task-\nadaptive pretraining strategies for text classiﬁcation.\nWhile our LMA (see §3) is related to their DAPT as\nboth use a self-supervised objective on a domain-\nspeciﬁc corpus, they notably differ in that DAPT\noperates on the model input while LMA models\nthe output. We are the ﬁrst to show the beneﬁts\nof additional task-speciﬁc pretraining in PLMs for\ngraph-to-text tasks.\n3 PLMs for Graph-to-Text Generation\n3.1 Models in this Study\nWe investigate BART (Lewis et al., 2020) and T5\n(Raffel et al., 2019), two PLMs based on the Trans-\nformer encoder-decoder architecture (Vaswani\net al., 2017), for graph-to-text generation. They\nmainly differ in how they are pretrained and the\ninput corpora used for pretraining. We experiment\nwith different T5 (small - 60M parameters, base -\n220M, and large - 770M) and BART (base - 140M\nand large - 400M) capacity models.\nWe ﬁne-tune both PLMs for a few epochs on\nthe supervised downstream graph-to-text datasets.\nFor T5, in the supervised setup, we add a preﬁx\n“translate from Graph to Text:” before the graph\ninput. We add this preﬁx to imitate the T5 setup,\nwhen translating between different languages.\n3.2 Task-speciﬁc Adaptation\nInspired by previous work (Konstas et al., 2017;\nGururangan et al., 2020), we investigate whether\nleveraging additional task-speciﬁc data can im-\nprove the PLMs’ performance on graph-to-text\ngeneration. Task-speciﬁc data refers to a pre-\ntraining corpus that is more task-relevant and usu-\nally smaller than the text corpora used for task-\nindependent pretraining. In order to leverage the\ntask-speciﬁc data, we add an intermediate adaptive\npretraining step between the original pretraining\nand ﬁne-tuning phases for graph-to-text generation.\nMore precisely, we ﬁrst continue pretraining\nBART and T5 using language model adaptation\n(LMA ) or supervised task adaptation (STA) training.\nIn the supervised approach, we use pairs of graphs\nand corresponding texts collected from the same or\nsimilar domain as the target task. In the LMA ap-\nproach, we follow BART and T5 pretraining strate-\ngies for language modeling, using the reference\ntexts that describe the graphs. Note that we do not\nuse the graphs in the LMA pretraining, but only the\ntarget text of our task-speciﬁc data collections. The\ngoal is to adapt the decoder to the domain of the\nﬁnal task (Gururangan et al., 2020). In particular,\nwe randomly mask text spans, replacing 15% of\nthe tokens.3 Before evaluation, we ﬁnally ﬁne-tune\nthe models using the original training set as usual.\n4 Datasets\nWe evaluate the text-to-text PLMs on three\ngraph-to-text benchmarks: AMR (LDC2017T10),\nWebNLG (Gardent et al., 2017), and AGENDA\n(Koncel-Kedziorski et al., 2019). We chose those\ndatasets because they comprise different domains\nand are widely used in prior work. Table 10 in\nAppendix shows statistics for each dataset.\nAMR. Abstract meaning representation (AMR)\nis a semantic formalism that represents the meaning\nof a sentence as a rooted directed graph expressing\n“who is doing what to whom” (Banarescu et al.,\n2013). In an AMR graph, nodes represent concepts\nand edges represent semantic relations. An instance\nin LDC2017T10 consists of a sentence annotated\nwith its corresponding AMR graph. Following\nMager et al. (2020), we linearize the AMR graphs\n3Please, refer to Lewis et al. (2020) and Raffel et al. (2019)\nfor details about the self-supervised pretraining strategies.\n214\nusing the PENMAN notation (see Figure 1a).4\nWebNLG. Each instance of WebNLG contains a\nKG from DBPedia (Auer et al., 2007) and a target\ntext with one or multiple sentences that describe\nthe graph. The test set is divided into two par-\ntitions: seen, which contains only DBPedia cate-\ngories present in the training set, andunseen, which\ncovers categories never seen during training. Their\nunion is called all. Following previous work (Hark-\nous et al., 2020), we prepend ⟨H⟩, ⟨R⟩, and ⟨T⟩\ntokens before the head entity, the relation and tail\nentity of a triple (see Figure 1b).\nAGENDA. In this dataset, KGs are paired with\nscientiﬁc abstracts extracted from proceedings of\nAI conferences. Each sample contains the paper\ntitle, a KG, and the corresponding abstract. The\nKG contains entities corresponding to scientiﬁc\nterms and the edges represent relations between\nthese entities. This dataset has loose alignments\nbetween the graph and the corresponding text as the\ngraphs were automatically generated. The input for\nthe models is a text containing the title, a sequence\nof all KG entities, and the triples. The target text is\nthe paper abstract. We add special tokens into the\ntriples in the same way as for WebNLG.\n4.1 Additional Task-speciﬁc Data\nIn order to evaluate the proposed task-adaptive pre-\ntraining strategies for graph-to-text generation, we\ncollect task-speciﬁc data for two graph domains:\nmeaning representations (like AMR) and scientiﬁc\ndata (like AGENDA). We did not attempt collect-\ning additional data like WebNLG because the texts\nin this benchmark do not stem from a corpus but\nwere speciﬁcally written by annotators.\nAMR Silver Data. In order to generate addi-\ntional data for AMR, we sample two sentence col-\nlections of size 200K and 2M from the Gigaword5\ncorpus and use a state-of-the-art AMR parser (Cai\nand Lam, 2020a) to parse them into AMR graphs.6\nFor supervised pretraining, we condition a model\non the AMR silver graphs to generate the corre-\nsponding sentences before ﬁne-tuning it on gold\nAMR graphs. For self-supervised pretraining, we\nonly use the sentences.7\n4Details of the preprocessing procedure of AMRs are pro-\nvided in Appendix A.\n5https://catalog.ldc.upenn.edu/LDC2003T05\n6We ﬁlter out sentences that do not yield well-formed\nAMR graphs.\n7Gigaword and AMR datasets share similar data sources.\nModel BLEU M BT\nRibeiro et al. (2019) 27.87 33.21 -\nZhu et al. (2019) 31.82 36.38 -\nZhao et al. (2020b) 32.46 36.78 -\nWang et al. (2020) 33.90 37.10 -\nYao et al. (2020) 34.10 38.10 -\nbased on PLMs\nMager et al. (2020) 33.02 37.68 -\nHarkous et al. (2020) 37.70 38.90 -\nBARTbase 36.71 38.64 52.47\nBARTlarge 43.47 42.88 60.42\nT5small 38.45 40.86 57.95\nT5base 42.54 42.62 60.59\nT5large 45.80 43.85 61.93\nwith task-adaptive pretraining\nBARTlarge + LMA 43.94 42.36 58.54\nT5large + LMA 46.06 44.05 62.59\nBARTlarge + STA (200K) 44.72 43.65 61.03\nBARTlarge + STA (2M) 47.51 44.70 62.27\nT5large + STA (200K) 48.02 44.85 63.86\nT5large + STA (2M) 49.72 45.43 64.24\nTable 1: Results on AMR-to-text generation for the\nLDC2017T10 test set. M and BT stand for METEOR\nand BLEURT, respectively. Bold (Italic) indicates the\nbest score without (with) task-adaptive pretraining.\nSemantic Scholar AI Data. We collect titles and\nabstracts of around 190K scientiﬁc papers from the\nSemantic Scholar (Ammar et al., 2018) taken from\nthe proceedings of 36 top Computer Science/AI\nconferences. We construct KGs from the paper ab-\nstracts employing DyGIE++ (Wadden et al., 2019),\nan information extraction system for scientiﬁc texts.\nNote that the AGENDA dataset was constructed\nusing the older SciIE system (Luan et al., 2018),\nwhich also extracts KGs from AI scientiﬁc papers.\nA second difference is that in our new dataset, the\ndomain is broader as we collected data from 36 con-\nferences compared to 12 from AGENDA. Further-\nmore, to prevent data leakage, all AGENDA sam-\nples used for performance evaluation are removed\nfrom our dataset. We will call the new dataset\nKGAIA (KGs from AI Abstracts). 8 Table 11 in\nAppendix shows relevant dataset statistics.\n5 Experiments\nWe modify the BART and T5 implementations re-\nleased by Hugging Face (Wolf et al., 2019) in order\nto adapt them to graph-to-text generation. For the\nKG datasets, we add the ⟨H⟩, ⟨R⟩, and ⟨T⟩tokens\nto the models’ vocabulary. We add all edge labels\nseen in the training set to the vocabulary of the\n8We will release the collected additional task-speciﬁc data.\n215\nBLEU METEOR chrF++\nModel A S U A S U A S U\nCastro Ferreira et al. (2019) 51.68 56.35 38.92 32.00 41.00 21.00 - - -\nMoryossef et al. (2019) 47.24 53.30 34.41 39.00 44.00 37.00 - - -\nSchmitt et al. (2020) - 59.39 - - 42.83 - - 74.68 -\nRibeiro et al. (2020) - 63.69 - - 44.47 - - 76.66 -\nZhao et al. (2020a) 52.78 64.42 38.23 41.00 46.00 37.00 - - -\nbased on PLMs\nHarkous et al. (2020) 52.90 - - 42.40 - - - - -\nKale (2020) 57.10 63.90 52.80 44.00 46.00 41.00 - - -\nRadev et al. (2020) 45.89 52.86 37.85 40.00 42.00 37.00 - - -\nBARTbase 53.11 62.74 41.53 40.18 44.45 35.36 70.02 76.68 62.76\nBARTlarge 54.72 63.45 43.97 42.23 45.49 38.61 72.29 77.57 66.53\nT5small 56.34 65.05 45.37 42.78 45.94 39.29 73.31 78.46 67.69\nT5base 59.17 64.64 52.55 43.19 46.02 41.49 74.82 78.40 70.92\nT5large 59.70 64.71 53.67 44.18 45.85 42.26 75.40 78.29 72.25\nTable 2: Results on WebNLG. A, S and U stand for all, seen, and unseen partitions of the test set, respectively.\nmodels for AMR. Following Wolf et al. (2019), we\nuse the Adam optimizer (Kingma and Ba, 2015)\nwith an initial learning rate of 3 ·10−5. We employ\na linearly decreasing learning rate schedule without\nwarm-up. The batch and beam search sizes are cho-\nsen from {2,4,8} and {1,3,5}, respectively, based\non the respective development set. Dev BLEU is\nused for model selection.\nFollowing previous works, we evaluate the re-\nsults with BLEU (Papineni et al., 2002), ME-\nTEOR (Denkowski and Lavie, 2014), and chrF++\n(Popovi´c, 2015) metrics. We also use Mover-\nScore (Zhao et al., 2019), BERTScore (Zhang et al.,\n2020), and BLEURT (Sellam et al., 2020) metrics,\nas they employ contextual and semantic knowledge\nand thus depend less on the surface symbols. Addi-\ntionally, we perform a human evaluation (cf. §5.4)\nquantifying the ﬂuency, semantic adequacy and\nmeaning similarity of the generated texts.\n5.1 Results on AMR-to-Text\nTable 1 shows our results for the setting without ad-\nditional pretraining, with additional self-supervised\ntask-adaptive pretraining solely using the collected\nGigaword sentences (LMA ), and with additional su-\npervised task adaptation (STA), before ﬁne-tuning.\nWe also report several recent results on the AMR\ntest set. Mager et al. (2020) and Harkous et al.\n(2020) employ GPT-2 in their approaches. Note\nthat GPT-2 only consists of a Transformer-based\ndecoder.\nOnly considering approaches without task adap-\ntation, BARTlarge already achieves a considerable\nimprovement of 5.77 BLEU and 3.98 METEOR\nscores over the previous state of the art. With a\nBLEU score of 45.80, T5large performs best. The\nother metrics follow similar trends. See Table 13\nin Appendix for evaluation with more metrics. The\nstrong performance of both BART and T5 in the\nAMR dataset suggests that PLMs can infer the\nAMR structure by a simple linear sequence of the\ngraph, in contrast to GNN-based models that ex-\nplicitly consider the graph structure using message-\npassing between adjacent nodes (Beck et al., 2018).\nTask-speciﬁc Adaptation. LMA already brings\nsome gains with T5 beneﬁtting more than BART\nin most metrics. It still helps less than STA even\nthough we only have automatically generated an-\nnotations. This suggests that the performance in-\ncreases with STA do not only come from additional\nexposure to task-speciﬁc target texts and that the\nmodels learn how to handle graphs and the graph-\ntext correspondence even with automatically gener-\nated AMRs. After STA, T5 achieves 49.72 BLEU\npoints, the new state of the art for AMR-to-text\ngeneration. Interestingly, gains from STA with 2M\nover 200K are larger in BART than in T5, suggest-\ning that large amounts of silver data may not be\nrequired for a good performance with T5.\nIn general, models pretrained on the STA setup\nconverge faster than without task-speciﬁc adapta-\ntion. For example, T5 large without additional pre-\ntraining converges after 5 epochs of ﬁne-tuning\nwhereas T5large with STA already converges after 2\nepochs.\n5.2 Results on WebNLG\nTable 2 shows the results for the WebNLG test\nset. Neural pipeline models (Moryossef et al.,\n2019; Castro Ferreira et al., 2019) achieve strong\nperformance in the unseen dataset. On the other\n216\nModel BLEU M BT\nKoncel et al. 2019 14.30 18.80 -\nAn (2019) 15.10 19.50 -\nSchmitt et al. (2020) 17.33 21.43 -\nRibeiro et al. (2020) 18.01 22.23 -\nBARTbase 22.01 23.54 -13.02\nBARTlarge 23.65 25.19 -10.93\nT5small 20.22 21.62 -24.10\nT5base 20.73 21.88 -21.03\nT5large 22.15 23.73 -13.96\nwith task-adaptive pretraining\nBARTlarge + LMA 25.30 25.54 -08.79\nT5large + LMA 22.92 24.40 -10.39\nBARTlarge + STA 25.66 25.74 -08.97\nT5large + STA 23.69 24.92 -08.94\nTable 3: Results on AGENDA test set. Bold (Italic)\nindicates best scores without (with) task-adaptive pre-\ntraining.\nhand, fully end-to-end models (Ribeiro et al., 2020;\nSchmitt et al., 2020) have strong performance on\nthe seen dataset and usually perform poorly in un-\nseen data. Models that explicitly encode the graph\nstructure (Ribeiro et al., 2020; Zhao et al., 2020a)\nachieve the best performance among approaches\nthat do not employ PLMs. Note that T5 is also\nused in Kale (2020). Differences in our T5 setup\ninclude a modiﬁed model vocabulary, the use of\nbeam search, the learning rate schedule and the\npreﬁx before the input graph. Our T5 approach\nachieves 59.70, 65.05 and 54.69 BLEU points on\nall, seen and unseen sets, the new state of the art.\nWe conjecture that the performance gap between\nseen and unseen sets stems from the advantage ob-\ntained by a model seeing examples of relation-text\npairs during ﬁne-tuning. For example, the relation\nparty (political party) was never seen during train-\ning and the model is required to generate a text that\nverbalizes the tuple: ⟨Abdul Taib Mahmud, party,\nParti Bumiputera Sarawak⟩. Interestingly, BART\nperforms much worse than T5 on this benchmark,\nespecially in the unseen partition with 9.7 BLEU\npoints lower compared to T5.\nFor lack of a suitable data source (cf. §4), we\ndid not explore our LMA or STA approaches for\nWebNLG. However, we additionally discuss cross-\ndomain STA in Appendix B.\n5.3 Results on AGENDA\nTable 3 lists the results for the AGENDA test set.\nThe models also show strong performance on this\nModel AMR\nF MS\nMager et al. (2020) 5.69A 5.08A\nHarkous et al. (2020) 5.78A 5.47AB\nT5large 6.55B 6.44C\nBARTlarge 6.70B 5.72BC\nReference 5.91A -\nModel WebNLG\nF SA\nCastro Ferreira et al. (2019) 5.52A 4.77A\nHarkous et al. (2020) 5.74AB 6.21B\nT5large 6.71C 6.63B\nBARTlarge 6.53C 6.50B\nReference 5.89B 6.47B\nTable 4: Fluency (F), Meaning Similarity (MS) and Se-\nmantic Adequacy (SA) obtained in the human evalua-\ntion. Differences between models which have a letter in\ncommon are not statistically signiﬁcant and were deter-\nmined by pairwise Mann-Whitney tests with p <0.05.\ndataset. We believe that their capacity to generate\nﬂuent text helps when generating paper abstracts,\neven though they were not pretrained in the sci-\nentiﬁc domain. BART large shows an impressive\nperformance with a BLEU score of 23.65, which is\n5.6 points higher than the previous state of the art.\nTask-speciﬁc Adaptation. On AGENDA,\nBART beneﬁts more from our task-adaptive\npretraining, achieving the new state of the art of\n25.66 BLEU points, a further gain of 2 BLEU\npoints compared to its performance without task\nadaptation. The improvements from task-adaptive\npretraining are not as large as for AMR. We\nhypothesize that this is due to the fact that\nthe graphs do not completely cover the target\ntext (Koncel-Kedziorski et al., 2019), making\nthis dataset more challenging. See Table 12 in\nAppendix for more automatic metrics.\n5.4 Human Evaluation\nTo further assess the quality of the generated text,\nwe conduct a human evaluation on AMR and\nWebNLG via crowd sourcing on Amazon Mechan-\nical Turk.9 Following previous works (Gardent\net al., 2017; Castro Ferreira et al., 2019), we assess\nthree quality criteria: (i) Fluency (i.e., does the text\nﬂow in a natural, easy-to-read manner?), for AMR\nand WebNLG; (ii) Meaning Similarity (i.e., how\n9We exclude AGENDA because its texts are scientiﬁc in\nnature and annotators are not necessarily AI experts.\n217\nOriginal Input\n• Arrabbiata sauce • country • Italy • Italy • demonym • \nItalians • Italy • capital • Rome • Italy • language • Italian \nlanguage • Italy • leader Name • Sergio Mattarella\n• Rome • Italy • Italy • language • capital • Italy • Italians • \nItaly • Italy • Sergio Mattarella • Arrabbiata sauce • leader \nName • country • demonym • Italian language\nCorrupted Input\nShuﬄe\nT5\nArrabbiata sauce can be found in Italy where Sergio Mattarella \nis the leader and the capital city is Rome. Italians are the \npeople who live there and the language spoken is Italian.\nItalians live in Italy where the capital is Rome and the \nlanguage is Italian. Sergio Mattarella is the leader of the \ncountry and arrabbiata sauce can be found there.\nT5\nReference: Arrabbiata sauce is from Italy where the capital is Rome, Italian is the language spoken and Sergio Mattarella is a leader.\norder shuf\nFigure 2: Example graph with 5 triples, from WebNLG dev linearized with the neutral separator tag, denoted•, (top\nleft), its shufﬂed version (top right), texts generated with two ﬁne-tuned versions of T5 small and a gold reference\n(bottom). Note that T5 can produce a reasonable text even when the input triples are shufﬂed randomly.\n1 10 40 70 100\n% of Training Data\n10.0\n20.0\n30.0\n40.0\n50.0\n60.0BLEU\nWebNLG-Seen - T5\nWebNLG-Seen - BART\nAMR - T5\nAMR - BART\nAGENDA - T5\nAGENDA - BART\nFigure 3: Performance of BART base and T5base in the\ndev set when experimenting with different amounts of\ntraining data.\nclose in meaning is the generated text to the refer-\nence sentence?) for AMR; (ii) Semantic Adequacy\n(i.e., does the text clearly express the data?) for\nWebNLG. We randomly select 100 generated texts\nof each model, which the annotators then rate on\na 1-7 Likert scale. For each text, we collect scores\nfrom 3 annotators and average them.10\nTable 4 shows the results. Our approaches im-\nprove the ﬂuency, meaning similarity, and semantic\nadequacy on both datasets compared to other state-\nof-the-art approaches with statistically signiﬁcant\nmargins (p<0.05). Interestingly, the highest ﬂu-\nency improvement (+0.97) is on AMR, where our\napproach also has the largest BLEU improvement\n(+8.10) over Harkous et al. (2020). Finally, our\nmodels score higher than the references in ﬂuency\nwith statistically signiﬁcant margins, highlighting\ntheir strong language generation abilities.11\n5.5 Limiting the Training Data\nIn Figure 3, we investigate the PLMs’ performance,\nmeasured with BLEU score, while varying (from\n1% to 100%) the amount of training data used for\n10Inter-annotator agreement for the three criteria ranged\nfrom 0.40 to 0.79, with an average Krippendorff’sα of 0.56.\n11Examples of ﬂuent generations can be found in the Ta-\nbles 15 and 16 in Appendix.\nModel AMR WebNLG AGENDA\nT5order 36.83 63.41 19.86\nT5shuf 15.56 61.54 19.08\nTable 5: Impact (measured with BLEU) of using a bag\nof entities and relations (shuf ) as input for T5small.\nﬁne-tuning. We ﬁnd that, when ﬁne-tuned with\nonly 40% of the data, both BART and T5 already\ngreatly improve the performance compared to using\nthe entire training data in all three benchmarks. For\nexample, BART ﬁne-tuned on 40% of AMR train-\ning data achieves 91% of the BLEU score when\nﬁne-tuned on full data.\nNote that in a low-resource scenario in AMR and\nWebNLG, T5 considerably outperforms BART. In\nparticular, with only 1% of training examples, the\ndifference between T5 and BART is 7.51 and 5.64\nBLEU points for AMR and WebNLG, respectively.\nThis suggests that T5 is more data efﬁcient when\nadapting to the new task, likewise our ﬁndings in\nAMR- STA (cf. §5.1).\n6 Inﬂuence of the Graph Structure\nWe conduct further experiments to examine how\nmuch the PLMs consider the graph structure. To\nthis end, we remove parentheses in AMRs and re-\nplace ⟨H⟩, ⟨R⟩, and ⟨T⟩tokens with neutral sep-\narator tokens, denoted •, for KGs, such that the\ngraph structure is only deﬁned by the order of node\nand edge labels. If we shufﬂe such a sequence, the\ngraph structure is thus completely obscured and\nthe input effectively becomes a bag of node and\nedge labels. See Figure 2 for an example of both a\ncorrectly ordered and a shufﬂed triple sequence.\n6.1 Quantitative Analysis\nTable 5 shows the effect on T5’s performance when\nits input contains correctly ordered triples (T5order)\n218\nT/F Input Fact T5 order T5shuf\n(1) S • German language • Antwerp •\nAntwerp • Antwerp International Air-\nport • Belgium • Belgium • Charles\nMichel • city Served • leader Name •\nBelgium •language •country\nAntwerp International Airport serves\nthe city of Antwerp. German is the\nlanguage spoken in Belgium where\nCharles Michel is the leader.\nAntwerp International Airport serves\nthe city of Antwerp in Belgium where\nthe German language is spoken and\nCharles Michel is the leader.\n(2) T •California •is Part Of•US •California\n•capital •Sacramento\nCalifornia is part of the United States\nand its capital is Sacramento.\nCalifornia is part of the United States\nand its capital is Sacramento.\n(3) F •US •is Part Of•California •California\n•capital •Sacramento\nCalifornia’s capital is Sacramento and\nthe United States is part of California.\nCalifornia is part of the United States\nand its capital is Sacramento.\n(4) T •Amarillo, Texas •is Part Of •United\nStates\nAmarillo, Texas is part of the United\nStates.\nAmarillo, Texas is part of the United\nStates.\n(5) F •United States •is Part Of •Amarillo,\nTexas\nAmarillo, Texas is part of the United\nStates.\nAmarillo, Texas is part of the United\nStates.\nTable 6: Example generations from shufﬂed (S), true (T), and corrupted (F) triple facts by T5 small, ﬁne-tuned on\ncorrectly ordered triples (order) and randomly shufﬂed input (shuf ).\nvs. shufﬂed ones ( T5shuf) for both training and\nevaluation. We ﬁrst observe that T5order only has\nmarginally lower performance (around 2-4%) with\nthe neutral separators than with the ⟨H⟩/⟨R⟩/⟨T⟩\ntags or parentheses. 12 We see that as evidence\nthat the graph structure is similarly well captured\nby T5order. Without the graph structure ( T5shuf),\nAMR-to-text performance drops signiﬁcantly. Pos-\nsible explanations of this drop are: (i) the relative\nordering of the AMR graph is known to correlate\nwith the target sentence order (Konstas et al., 2017);\n(ii) in contrast to WebNLG that contains common\nknowledge, the AMR dataset contains very speciﬁc\nsentences with higher surprisal;13 (iii) AMRs are\nmuch more complex graph structures than the KGs\nfrom WebNLG and AGENDA.14\nOn the other hand, KG-to-text performance is\nnot much lower, indicating that most of the PLMs’\nsuccess in this task stems from their language mod-\neling rather than their graph encoding capabilities.\nWe hypothesize that a PLM can match the entities\nin a shufﬂed input with sentences mentioning these\nentities from the pretraining or ﬁne-tuning phase. It\nhas recently been argued that large PLMs can recall\ncertain common knowledge facts from pretraining\n(Petroni et al., 2019; Bosselut et al., 2019).\n6.2 Qualitative Analysis\nThe example in Figure 2 conﬁrms our impression.\nT5shuf produces a text with the same content as\n12See a more ﬁne-grained comparison in Appendix C.\n13Perplexities estimated on the dev sets of AMR and\nWebNLG datasets, with GPT-2 ﬁne-tuned on the correspond-\ning training set, are 20.9 and 7.8, respectively.\n14In Appendix D, we present the graph properties of the\ndatasets and discuss the differences.\nT5order but does not need the correct triple structure\nto do so. Example (1) in Table 6 shows the output\nof both models with shufﬂed input. Interestingly,\neven T5order produces a reasonable and truthful text.\nThis suggests that previously seen facts serve as a\nstrong guide during text generation, even for mod-\nels that were ﬁne-tuned with a clearly marked graph\nstructure, suggesting that T5order also relies more\non language modeling than the graph structure. It\ndoes have more difﬁculties covering the whole in-\nput graph though. The fact that Antwerp is located\nin Belgium is missing from its output.\nTo further test our hypothesis that PLMs make\nuse of previously seen facts during KG-to-text gen-\neration, we generate example true facts, corrupt\nthem in a controlled setting, and feed them to both\nT5order and T5shuf to observe their output (examples\n(2)–(5) in Table 6). The model trained on correctly\nordered input has learned a bit more to rely on the\ninput graph structure. The false fact in example (3)\nwith two triples is reliably transferred to the text by\nT5order but not by T5shuf, which silently corrects it.\nAlso note that, in example (5), both models refuse\nto generate an incorrect fact. More examples can\nbe found in Table 14 in the Appendix.\nOur qualitative analysis illustrates that state-of-\nthe-art PLMs, despite their ﬂuency capacities (cf.\n§5.4), bear the risk of parroting back training sen-\ntences while ignoring the input structure. This issue\ncan limit the practical usage of those models as, in\nmany cases, it is important for a generation model\nto stay true to its input (Wiseman et al., 2017; Falke\net al., 2019).\n219\n7 Conclusion\nWe investigated two pretrained language models\n(PLMs) for graph-to-text generation and show that\nthe pretraining strategies, language model adapta-\ntion (LMA ) and supervised task adaptation (STA),\ncan lead to notable improvements. Our approaches\noutperform the state of the art by a substantial mar-\ngin on three graph-to-text benchmarks. Moreover,\nin a human evaluation our generated texts are per-\nceived signiﬁcantly more ﬂuent than human refer-\nences. Examining the inﬂuence of the graph struc-\nture on the text generation process, we ﬁnd that\nPLMs may not always follow the graph structure\nand instead use memorized facts to guide the gen-\neration. A promising direction for future work\nis to explore ways of injecting a stronger graph-\nstructural bias into PLMs, thus possibly leveraging\ntheir strong language modeling capabilities and\nkeeping the output faithful to the input graph.\nAcknowledgments\nWe thank our anonymous reviewers for their\nthoughtful feedback. Leonardo F. R. Ribeiro is\nsupported by the German Research Foundation\n(DFG) as part of the Research Training Group\n“Adaptive Preparation of Information form Hetero-\ngeneous Sources” (AIPHES, GRK 1994/1) and as\npart of the DFG funded project UKP-SQuARE with\nthe number GU 798/29-1. Martin Schmitt is sup-\nported by the BMBF as part of the project MLWin\n(01IS18050) and by the German Academic Schol-\narship Foundation (Studienstiftung des deutschen\nV olkes).\nReferences\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier,\nKyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Pe-\nters, Joanna Power, Sam Skjonsberg, Lucy Wang,\nChris Wilhelm, Zheng Yuan, Madeleine van Zuylen,\nand Oren Etzioni. 2018. Construction of the litera-\nture graph in semantic scholar. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3 (Industry\nPapers), pages 84–91, New Orleans - Louisiana. As-\nsociation for Computational Linguistics.\nBang An. 2019. Repulsive bayesian sampling for di-\nversiﬁed attention modeling. In 4th workshop on\nBayesian Deep Learning (NeurIPS 2019).\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives.\n2007. Dbpedia: A nucleus for a web of open data.\nIn Proceedings of the 6th International The Seman-\ntic Web and 2nd Asian Conference on Asian Se-\nmantic Web Conference, ISWC’07/ASWC’07, page\n722–735, Berlin, Heidelberg. Springer-Verlag.\nXuefeng Bai, Yulong Chen, Linfeng Song, and Yue\nZhang. 2021. Semantic representation for dialogue\nmodeling. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 4430–4445, Online. Association for Computa-\ntional Linguistics.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract Meaning Representation\nfor sembanking. In Proceedings of the 7th Linguis-\ntic Annotation Workshop and Interoperability with\nDiscourse, pages 178–186, Soﬁa, Bulgaria. Associa-\ntion for Computational Linguistics.\nDaniel Beck, Gholamreza Haffari, and Trevor Cohn.\n2018. Graph-to-sequence learning using gated\ngraph neural networks. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 273–283, Melbourne, Australia. Association\nfor Computational Linguistics.\nAnja Belz, Michael White, Dominic Espinosa, Eric\nKow, Deirdre Hogan, and Amanda Stent. 2011.\nThe ﬁrst surface realisation shared task: Overview\nand evaluation results. In Proceedings of the 13th\nEuropean Workshop on Natural Language Genera-\ntion, pages 217–226, Nancy, France. Association for\nComputational Linguistics.\nClaire Bonial, Lucia Donatelli, Mitchell Abrams,\nStephanie M. Lukin, Stephen Tratz, Matthew Marge,\nRon Artstein, David Traum, and Clare V oss. 2020.\nDialogue-AMR: Abstract Meaning Representation\nfor dialogue. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n684–695, Marseille, France. European Language Re-\nsources Association.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 4762–4779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nDeng Cai and Wai Lam. 2020a. AMR parsing via\ngraph-sequence iterative inference. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 1290–1301, On-\nline. Association for Computational Linguistics.\n220\nDeng Cai and Wai Lam. 2020b. Graph transformer for\ngraph-to-sequence learning. In The Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence, EAAI 2020, New York, NY,\nUSA, February 7-12, 2020, pages 7464–7471. AAAI\nPress.\nThiago Castro Ferreira, Chris van der Lee, Emiel\nvan Miltenburg, and Emiel Krahmer. 2019. Neu-\nral data-to-text generation: A comparison between\npipeline and end-to-end architectures. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 552–562, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJianpeng Cheng, Devang Agrawal, Héctor\nMartínez Alonso, Shruti Bhargava, Joris Driesen,\nFederico Flego, Dain Kaplan, Dimitri Kartsaklis,\nLin Li, Dhivya Piraviperumal, Jason D. Williams,\nHong Yu, Diarmuid Ó Séaghdha, and Anders\nJohannsen. 2020. Conversational semantic parsing\nfor dialog state tracking. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8107–8117,\nOnline. Association for Computational Linguistics.\nMarco Damonte and Shay B. Cohen. 2019. Structural\nneural encoders for AMR-to-text generation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers) , pages 3649–3658,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nMichael Denkowski and Alon Lavie. 2014. Meteor uni-\nversal: Language speciﬁc translation evaluation for\nany target language. In Proceedings of the Ninth\nWorkshop on Statistical Machine Translation, pages\n376–380, Baltimore, Maryland, USA. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nNan Duan, Duyu Tang, Peng Chen, and Ming Zhou.\n2017. Question generation for question answering.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n866–874, Copenhagen, Denmark. Association for\nComputational Linguistics.\nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019.\nRanking generated summaries by correctness: An in-\nteresting but challenging application for natural lan-\nguage inference. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2214–2220, Florence, Italy. Associa-\ntion for Computational Linguistics.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The WebNLG\nchallenge: Generating text from RDF data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 124–133, San-\ntiago de Compostela, Spain. Association for Compu-\ntational Linguistics.\nAlbert Gatt and Emiel Krahmer. 2018. Survey of the\nstate of the art in natural language generation: Core\ntasks, applications and evaluation. Journal of Artiﬁ-\ncial Intelligence Research, 61(1):65–170.\nZhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei\nLu. 2019. Densely connected graph convolutional\nnetworks for graph-to-sequence learning. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:297–312.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\nHave your text and use it too! end-to-end neural\ndata-to-text generation with semantic ﬁdelity. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 2410–2424,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nAlexander Miserlis Hoyle, Ana Marasovi ´c, and\nNoah A. Smith. 2021. Promoting graph awareness\nin linearized graph-to-text generation. In Findings\nof the Association for Computational Linguistics:\nACL-IJCNLP 2021, pages 944–956, Online. Asso-\nciation for Computational Linguistics.\nMihir Kale. 2020. Text-to-text pre-training for data-to-\ntext tasks. arXiv e-prints.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nThomas N. Kipf and Max Welling. 2017. Semi-\nSupervised Classiﬁcation with Graph Convolutional\nNetworks. In Proceedings of the 5th International\nConference on Learning Representations , ICLR\n2017.\n221\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,\nMirella Lapata, and Hannaneh Hajishirzi. 2019.\nText Generation from Knowledge Graphs with\nGraph Transformers. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2284–2293, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nIoannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin\nChoi, and Luke Zettlemoyer. 2017. Neural amr:\nSequence-to-sequence models for parsing and gener-\nation. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 146–157, Vancouver,\nCanada. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv e-prints.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiﬁcation of enti-\nties, relations, and coreference for scientiﬁc knowl-\nedge graph construction. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3219–3232, Brussels, Bel-\ngium. Association for Computational Linguistics.\nManuel Mager, Ramón Fernandez Astudillo, Tahira\nNaseem, Md Arafat Sultan, Young-Suk Lee, Radu\nFlorian, and Salim Roukos. 2020. GPT-too: A\nlanguage-model-ﬁrst approach for AMR-to-text gen-\neration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1846–1852, Online. Association for Computa-\ntional Linguistics.\nDiego Marcheggiani and Laura Perez Beltrachini. 2018.\nDeep graph convolutional encoders for structured\ndata to text generation. In Proceedings of the 11th\nInternational Conference on Natural Language Gen-\neration, pages 1–9, Tilburg University, The Nether-\nlands. Association for Computational Linguistics.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. OpenDialKG: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845–854, Florence, Italy. Associ-\nation for Computational Linguistics.\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\nStep-by-step: Separating planning from realization\nin neural data-to-text generation. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 2267–2277, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting on Association for Com-\nputational Linguistics , ACL ’02, pages 311–318,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nMaja Popovi ´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nDragomir Radev, Rui Zhang, Amrit Rau, Abhinand\nSivaprasad, Chiachun Hsieh, Nazneen Fatema Ra-\njani, Xiangru Tang, Aadit Vyas, Neha Verma,\nPranav Krishna, Yangxiaokang Liu, Nadia Irwanto,\nJessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori\nMutuma, Yasin Tarabar, Ankit Gupta, Tao Yu,\nYi Chern Tan, Xi Victoria Lin, Caiming Xiong, and\nRichard Socher. 2020. Dart: Open-domain struc-\ntured data record to text generation. arXiv e-prints.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. arXiv\ne-prints.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv e-prints.\nLeonardo F. R. Ribeiro, Claire Gardent, and Iryna\nGurevych. 2019. Enhancing AMR-to-text genera-\ntion with dual graph representations. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3183–3194, Hong\nKong, China. Association for Computational Lin-\nguistics.\n222\nLeonardo F. R. Ribeiro, Jonas Pfeiffer, Yue Zhang, and\nIryna Gurevych. 2021a. Smelting gold and silver\nfor improved multilingual amr-to-text generation. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Punta Cana, November 7-11, 2021.\nLeonardo F. R. Ribeiro, Yue Zhang, Claire Gardent,\nand Iryna Gurevych. 2020. Modeling global and\nlocal node contexts for text generation from knowl-\nedge graphs. Transactions of the Association for\nComputational Linguistics, 8:589–604.\nLeonardo F. R. Ribeiro, Yue Zhang, and Iryna\nGurevych. 2021b. Structural adapters in pretrained\nlanguage models for amr-to-text generation. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2021,\nPunta Cana, November 7-11, 2021.\nMartin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter,\nIryna Gurevych, and Hinrich Schütze. 2021. Mod-\neling graph structure via relative position for text\ngeneration from knowledge graphs. In Proceedings\nof the Fifteenth Workshop on Graph-Based Methods\nfor Natural Language Processing (TextGraphs-15) ,\npages 10–21, Mexico City, Mexico. Association for\nComputational Linguistics.\nMartin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter,\nIryna Gurevych, and Hinrich Schütze. 2020. Mod-\neling graph structure via relative position for better\ntext generation from knowledge graphs. arXiv e-\nprints.\nThibault Sellam, Dipanjan Das, and Ankur Parikh.\n2020. BLEURT: Learning robust metrics for text\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7881–7892, Online. Association for Computa-\ntional Linguistics.\nLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel\nGildea. 2018. A graph-to-sequence model for AMR-\nto-text generation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1616–\n1626, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nMihai Surdeanu, Richard Johansson, Adam Meyers,\nLluís Màrquez, and Joakim Nivre. 2008. The\nCoNLL 2008 shared task on joint parsing of syn-\ntactic and semantic dependencies. In CoNLL 2008:\nProceedings of the Twelfth Conference on Computa-\ntional Natural Language Learning , pages 159–177,\nManchester, England. Coling 2008 Organizing Com-\nmittee.\nBayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang,\nand Wei Wang. 2018. GTR-LSTM: A triple encoder\nfor sentence generation from RDF data. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1627–1637, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nPavlos V ougiouklis, Hady Elsahar, Lucie-Aimée\nKaffee, Christophe Gravier, Frédérique Laforest,\nJonathon Hare, and Elena Simperl. 2018. Neu-\nral wikipedian: Generating textual summaries from\nknowledge base triples. Journal of Web Semantics,\n52-53:1 – 15.\nDavid Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. 2019. Entity, relation, and event\nextraction with contextualized span representations.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5784–\n5789, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTianming Wang, Xiaojun Wan, and Hanqi Jin. 2020.\nAmr-to-text generation with graph transformer.\nTransactions of the Association for Computational\nLinguistics, 8:19–33.\nSam Wiseman, Stuart Shieber, and Alexander Rush.\n2017. Challenges in data-to-document generation.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2253–2263, Copenhagen, Denmark. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019a. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In Advances in\nNeural Information Processing Systems, volume 32,\npages 5753–5763. Curran Associates, Inc.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019b. Xlnet: Generalized autoregressive pre-\ntraining for language understanding. In H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 5753–\n5763. Curran Associates, Inc.\nShaowei Yao, Tianming Wang, and Xiaojun Wan.\n2020. Heterogeneous graph transformer for graph-\nto-sequence learning. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\n223\nLinguistics, pages 7145–7154, Online. Association\nfor Computational Linguistics.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Wal-\nter Lasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nChao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\n2020a. Bridging the structural gap between encod-\ning and decoding for data-to-text generation. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2481–\n2491, Online. Association for Computational Lin-\nguistics.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\nYanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao,\nSu Zhu, and Kai Yu. 2020b. Line graph enhanced\nAMR-to-text generation with mix-order graph at-\ntention networks. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 732–741, Online. Association for\nComputational Linguistics.\nJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min\nZhang, and Guodong Zhou. 2019. Modeling graph\nstructure in transformer for better AMR-to-text gen-\neration. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5459–5468, Hong Kong, China. Association for\nComputational Linguistics.\n224\nAppendices\nIn this supplementary material, we provide: (i)\nadditional information about the data used in the\nexperiments, and (ii) results that we could not ﬁt\ninto the main body of the paper.\nA AMR Input Representation\nWe test three variants for the representation of the\ninput AMR graph. Following previous work (Kon-\nstas et al., 2017; Mager et al., 2020), we evaluate\n(i) only node representation, where the edge in-\nformation is removed from the linearization; (ii)\ndepth-ﬁrst search (DFS) through the graph and the\n(iii) PENMAN representation. An example for each\nrepresentation is illustrated below:\nonly nodes value interrogative commodity\ntrue\nDFS value :mode interrogative\n:ARG1 commodity :ARG1-of\ntrue\nPENMAN ( value :mode interrogative\n:ARG1 ( commodity ) :ARG1-of\n( true ) )\nIn this experiment we employ T5 small. Table 7\nshows the results on the AMR development set.\nThe PENMAN representation leads to best results.\nTherefore, this representation is used in the rest of\nthe experiments.\nInput BLEU\nonly nodes 28.22\nDFS 34.94\nPENMAN 38.27\nTable 7: Results on the AMR dev set using T5 small for\ndifferent AMR linearizations.\nB Cross-domain Adaptation\nFor a given task, it is not always possible to collect\nclosely related data – as we saw, e.g., for WebNLG.\nWe therefore report STA in a cross-domain set-\nting for the different KG-to-text benchmarks. Ta-\nble 8 shows the results using BARTbase and T5base.\nWhile the texts in KGAIA and AGENDA share the\ndomain of scientiﬁc abstracts, texts in WebNLG\nare more general. Also note that WebNLG graphs\ndo not share any relations with the other KGs. For\nBARTbase, STA increases the performance in the\ncross-domain setting in most of the cases. For\nT5base, STA in KGAIA improves the performance\non WebNLG.\nIn general, we ﬁnd that exploring additional\nadaptive pretraining for graph-to-text generation\ncan improve the performance even if the data do\nnot come from the same domain.\nSTA on Fine-tuned & Evaluated on\nWebNLG-Seen AGENDA\nBARTbase\nNone 58.71 22.01\nKGAIA 63.20 23.48\nWebNLG - 21.98\nAGENDA 61.25 -\nT5base\nNone 62.93 20.73\nKGAIA 63.19 22.44\nWebNLG - 20.27\nAGENDA 62.75 -\nTable 8: Effect (measured with BLEU score) of cross-\ndomain STA.\nC Input Graph Size\nFigure 4 visualizes T5 small’s performance with\nrespect to the number of input graph triples in\nWebNLG dataset. We observe that T5order and\nT5shuf perform similarly for inputs with only one\ntriple but that the gap between the models increases\nwith larger graphs. While it is obviously more dif-\nﬁcult to reconstruct a larger graph than a smaller\none, this also suggests that the graph structure is\nmore taken into account for graphs with more than\n2 triples. For the unseen setting, the performance\ngap for these graphs is even larger, suggesting that\nthe PLM can make more use of the graph structure\nwhen it has to.\n1 2 3 4 5\nNumber of Triples\n55\n60\n65\n70\n75\n80\n85CHRF++\nT5order seen\nT5shuf seen\nT5order unseen\nT5shuf unseen\nFigure 4: chrF++ scores with respect to the number of\ntriples for WebNLG seen and unseen test sets.\nD Graph Statistics\nIn Table 9, we present the graph properties of the\nthree datasets. All statistics are calculated using\n225\nAMR WebNLG AGENDA\nmin, avg and max number of nodes 2 28.6 335 2 6.8 15 2 10.5 80\nmin, avg and max node degrees 1 2.2 21 1 1.7 7 1 1.67 15\nmin, avg and max number of edges 1 32.3 554 1 5.9 14 1 8.8 124\nmin, avg and max graph diameter 1 12.2 40 1 4.1 10 1 3.1 20\nmin, avg and max shortest path length 0 7.49 40 0 2.4 10 0 2.3 20\nTable 9: Graph statistics of AMR, WebNLG and AGENDA datasets. The values are calculated using the training\ndata. Note that AMR graphs contain a more complex structure than WebNLG and AGENDA graphs.\nthe Levi transformation (Beck et al., 2018) of the\nundirected version of the graphs, where edges are\nalso considered nodes in the graph. WebNLG and\nAGENDA datasets contain disconnected graphs,\nand we use the largest subgraph to calculate the\ndiameter. Note that AMR graphs have a much\nmore complex structure: (i) they have more nodes\nand edges than WebNLG and AGENDA graphs;\n(ii) the average graph diameter and the average\nshortest path between nodes in AMRs are at least\nthree times larger than in WebNLG and AGENDA\ngraphs; (iii) nodes in AMRs have larger degrees\nthan nodes in WebNLG and AGENDA graphs.\nAMR17 WebNLG AGENDA\n#Train 36,521 18,102 38,720\n#Dev 1,368 872 1,000\n#Test 1,371 1,862 1,000\n#Relations 155 373 7\nAvg #Tokens 16.1 31.5 157.9\nTable 10: Statistics for the graph-to-text benchmarks.\nTitle Abstract KG\nV ocab 48K 173K 113K\nTokens 2.1M 31.7M 9.6M\nEntities - - 3.7M\nAvg Length 11.1 167.1 -\nAvg #Nodes - - 19.9\nAvg #Edges - - 9.4\nTable 11: Statistics for the KGAIA dataset.\nModel chrF++ BS (F1) MS\nSchmitt et al. (2020) 44.53 - -\nRibeiro et al. (2020) 46.37 - -\nBARTbase 48.02 89.36 34.33\nBARTlarge 50.44 88.74 32.24\nT5small 44.91 88.56 30.25\nT5base 48.14 88.81 31.33\nT5large 48.14 89.60 35.23\nwith task-adaptive pretraining\nBARTlarge + LMA 51.33 89.12 33.42\nT5large + LMA 49.37 89.75 36.13\nBARTlarge + STA 51.63 89.27 34.28\nT5large + STA 50.27 89.93 36.86\nTable 12: Results of the chrF++, BertScore (BS)\nand MoverScore (MS) scores for AGENDA test set.\nBold (Italic) indicates best scores without (with) task-\nadaptive pretraining.\nModel chrF++ BS (F1) MS\nGuo et al. (2019) 57.30 - -\nZhu et al. (2019) 64.05 - -\nCai and Lam (2020b) 59.40 - -\nWang et al. (2020) 65.80 - -\nYao et al. (2020) 65.60 - -\nbased on PLMs\nMager et al. (2020) 63.89 - -\nBARTbase 66.65 95.22 60.78\nBARTlarge 71.06 96.08 65.74\nT5small 68.78 95.62 63.70\nT5base 70.81 95.99 65.63\nT5large 72.57 96.27 67.37\nwith task-adaptive pretraining\nBARTlarge + LMA 71.14 95.94 64.75\nT5large + LMA 72.83 96.32 67.44\nBARTlarge + STA (200K) 72.26 96.21 66.75\nBARTlarge + STA (2M) 73.58 96.43 68.14\nT5large + STA (200K) 74.09 96.51 68.86\nT5large + STA (2M) 74.79 96.59 69.53\nTable 13: Results of the chrF++, BertScore (BS) and\nMoverScore (MS) scores for the LDC2017T10 test set.\nBold (Italic) indicates the best score without (with)\ntask-adaptive pretraining.\n226\nT/F Input Facts T5 order T5shuf\n(1) S • capital • leader Name • London •\nPound sterling • United Kingdom •\nleader Name •United Kingdom •Eliza-\nbeth II •United Kingdom •Boris John-\nson •London •currency\nThe capital city is London, the cur-\nrency is the Pound sterling and the\nleader is Elizabeth II. Boris Johnson\nis also a leader in the UK.\nThe capital of the United Kingdom\nis London, the currency is the Pound\nsterling and the country is lead by\nElizabeth II and Boris Johnson.\n(2) T •Germany •capital •Berlin Berlin is the capital of Germany. Berlin is the capital of Germany.\n(3) F •Berlin •capital •Germany Berlin’s capital is Germany. Berlin is the capital of Germany.\n(4) F •Leinster •is Part Of •Dublin Leinster is part of Dublin. Leinster is part of Dublin.\n(5) F •Rome •capital •Italy Rome’s capital is Italy. Rome is the capital of Italy.\n(6) T •Italy •capital •Rome Italy’s capital is Rome. Rome is the capital of Italy.\n(7) T • Texas • capital • Austin • Andrews\nCounty Airport •location •Texas\nAustin is the capital of Texas where\nAndrews County Airport is located.\nAustin is the capital of Texas where\nAndrews County Airport is located.\n(8) F • Austin • capital • Texas • Andrews\nCounty Airport •location •Texas\nThe capital of Austin is Texas and\nAndrews County Airport is located\nin Texas.\nAndrews County Airport is located\nin Texas where Austin is the capital.\nTable 14: Example generations from shufﬂed (S), true (T), and corrupted (F) triple facts by T5 small, ﬁne-tuned on\ncorrectly ordered triples (order) and randomly shufﬂed input (shuf ).\nD Model ExamplesAMR\nReference I had to deal with verbal abuse from my dad for a long 8 years before I\ncame to uni and honestly, the only reason why I’m here is because it was\nthe only way out.\nT5 I had to deal with 8 years of verbal abuse from my dad before coming to\nuniversity and honestly the only reason I’m here is because it’s the only\nway out.\nBART I had to deal with my dad’s verbal abuse for 8 years long before coming\nto uni and honestly the only reason I’m here is because it’s the only way\nout.\nMager et al. (2020) i’ve had to deal with verbal abuse from my dad for 8 years (before i came\nto uni i was honestly the only reason i was here) and it’s only because of\nthe way it is.\nWebNLG\nReference Aaron Turner is an electric guitar player who has played with the black\nmetal band Twilight and with Old Man Gloom. Death metal is a musical\nfusion of black metal.\nT5 Aaron Turner plays the electric guitar and is associated with the band\nTwilight. He is also a member of the Old Man Gloom band. Black metal\nand death metal are both genres of music.\nBART The black metal genre is a variation of the death metal genre. It is also\nwhere the band, Twilight, and the alternative rock band, Old Man Gloom,\nare from. One of the members of the band is Aaron Turner, who plays\nthe electric guitar.\nHarkous et al. (2020) Aaron Turner, a.k.a. Black Metal, is a member of the Twilight (band)\nand Old Man Gloom bands. He also plays electric guitar and has a strong\nconnection with the Death Metal genre.\nTable 15: Examples of text generated by the different models. D refers to the dataset.\n227\nModel Examples\nReference in this paper, a new array signal processing technique by using particle swarm\noptimization is proposed to identify multipath channel parameters. the proposed\narray signal processing technique provides estimates to the channel parameters\nby ﬁnding a global minimum of an optimization problem. since the optimization\nproblem is formulated in the cross-ambiguity function (caf) domain of the\ntransmitted signal and the received array outputs, the proposed array signal\nprocessing technique is called as particle swarm optimization. the performance\nof the particle swarm optimization is compared with the space alternating\ngeneralized expectation maximization technique and with another recently\nproposed pso based technique for various snr values. simulation results indicate\nthe superior performance of the pso based technique over mentioned techniques\nfor all snr values.\nT5 this paper, we propose an array signal processing technique based on particle\nswarm optimization to estimate the multipath channel parameters from the\nreceived and the transmitted signal in a cross-ambiguity domain. the proposed\narray signal processing technique is formulated as an optimization problem\nand particle swarm optimization is compared with the generalized expectation\nmaximization technique. simulation results show that the proposed particle\nswarm optimization outperforms the pso based technique for a wide range of snr\nvalues. in addition, the proposed particle swarm optimization is computationally\nmore efﬁcient than the gem based technique for small snr values.\nBART in this paper, a new array signal processing technique based on particle swarm\noptimization is proposed. the proposed array signal processing technique is\nused to estimate the multipath channel parameters from the transmitted signal.\nthe proposed array signal processing technique is formulated as an optimization\nproblem in the cross-ambiguity domain. the particle swarm optimization is\ncompared with the generalized expectation maximization technique. simulation\nresults show that the proposed particle swarm optimization outperforms the pso\nbased technique for all snr values. furthermore, the proposed particle swarm\noptimization is able to estimate the channel parameters more accurately than\nthe generalized expectation maximization technique.\nRibeiro et al. (2020) in this paper, a novel array signal processing technique based on particle swarm\noptimization is proposed to estimate the multipath channel parameters from the\ntransmitted signal. the proposed array signal processing technique uses particle\nswarm optimization to estimate the multipath channel parameters. the proposed\narray signal processing technique is formulated as an optimization problem.\nsimulation results show that the proposed array signal processing technique\noutperforms the conventional generalized expectation maximization technique\nand the pso based technique is robust to the snr values.\nTable 16: Examples of text generated by the different models trained on the AGENDA dataset.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7400763630867004
    },
    {
      "name": "Graph",
      "score": 0.608788251876831
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5446954369544983
    },
    {
      "name": "Natural language processing",
      "score": 0.5440948605537415
    },
    {
      "name": "Text generation",
      "score": 0.5337050557136536
    },
    {
      "name": "Language model",
      "score": 0.4527743458747864
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2681194543838501
    }
  ],
  "institutions": [],
  "cited_by": 39
}