{
  "title": "Tree Transformer: Integrating Tree Structures into Self-Attention",
  "url": "https://openalex.org/W2973039175",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5018908998",
      "name": "Yau-Shian Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5040508737",
      "name": "Hung-yi Lee",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5076610826",
      "name": "Yun-Nung Chen",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2939556020",
    "https://openalex.org/W2889260178",
    "https://openalex.org/W2922565841",
    "https://openalex.org/W2949399644",
    "https://openalex.org/W2953130735",
    "https://openalex.org/W145849181",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2963451457",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2932376173",
    "https://openalex.org/W2129882630",
    "https://openalex.org/W2896556401",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2594047108",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W1423339008",
    "https://openalex.org/W2963520382",
    "https://openalex.org/W2157762871",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2144916786",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2963648186",
    "https://openalex.org/W1495446613",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2889769646",
    "https://openalex.org/W2525778437"
  ],
  "abstract": "Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed \"Constituent Attention\" module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.",
  "full_text": "Tree Transformer: Integrating Tree Structures into Self-Attention\nYau-Shian Wang Hung-Yi Lee Yun-Nung Chen\nNational Taiwan University, Taipei, Taiwan\nking6101@gmail.com hungyilee@ntu.edu.tw y.v.chen@ieee.org\nAbstract\nPre-training Transformer from large-scale raw\ntexts and Ô¨Åne-tuning on the desired task have\nachieved state-of-the-art results on diverse\nNLP tasks. However, it is unclear what the\nlearned attention captures. The attention com-\nputed by attention heads seems not to match\nhuman intuitions about hierarchical structures.\nThis paper proposes Tree Transformer, which\nadds an extra constraint to attention heads of\nthe bidirectional Transformer encoder in order\nto encourage the attention heads to follow tree\nstructures. The tree structures can be automat-\nically induced from raw texts by our proposed\n‚ÄúConstituent Attention‚Äù module, which is sim-\nply implemented by self-attention between\ntwo adjacent words. With the same training\nprocedure identical to BERT, the experiments\ndemonstrate the effectiveness of Tree Trans-\nformer in terms of inducing tree structures,\nbetter language modeling, and further learning\nmore explainable attention scores1.\n1 Introduction\nHuman languages exhibit a rich hierarchical struc-\nture which is currently not exploited nor mir-\nrored by the self-attention mechanism that is\nthe core of the now popular Transformer archi-\ntecture. Prior work that integrated hierarchi-\ncal structure into neural networks either used re-\ncursive neural networks (Tree-RNNs) (C.Goller\nand A.Kuchler, 1996; Socher et al., 2011; Tai\net al., 2015) or simultaneously generated a syn-\ntax tree and language in RNN (Dyer et al.,\n2016), which have shown beneÔ¨Åcial for many\ndownstream tasks (Aharoni and Goldberg, 2017;\nEriguchi et al., 2017; Strubell et al., 2018; Zare-\nmoodi and Haffari, 2018). Considering the re-\nquirement of the annotated parse trees and the\n1The source code is publicly available at https://\ngithub.com/yaushian/Tree-Transformer.\ncostly annotation effort, most prior work relied on\nthe supervised syntactic parser. However, a su-\npervised parser may be unavailable when the lan-\nguage is low-resourced or the target data has dif-\nferent distribution from the source domain.\nTherefore, the task of learning latent tree struc-\ntures without human-annotated data, called gram-\nmar induction(Carroll and Charniak, 1992; Klein\nand Manning, 2002; Smith and Eisner, 2005), has\nbecome an important problem and attractd more\nattention from researchers recently. Prior work\nmainly focused on inducing tree structures from\nrecurrent neural networks (Shen et al., 2018a,b) or\nrecursive neural networks (Yogatama et al., 2017;\nDrozdov et al., 2019), while integrating tree struc-\ntures into Transformer remains an unexplored di-\nrection.\nPre-training Transformer from large-scale raw\ntexts successfully learns high-quality language\nrepresentations. By further Ô¨Åne-tuning pre-trained\nTransformer on desired tasks, wide range of NLP\ntasks obtain the state-of-the-art results (Radford\net al., 2019; Devlin et al., 2018; Dong et al.,\n2019). However, what pre-trained Transformer\nself-attention heads capture remains unknown. Al-\nthough an attention can be easily explained by ob-\nserving how words attend to each other, only some\ndistinct patterns such as attending previous words\nor named entities can be found informative (Vig,\n2019). The attention matrices do not match our\nintuitions about hierarchical structures.\nIn order to make the attention learned by\nTransformer more interpretable and allow Trans-\nformer to comprehend language hierarchically, we\npropose Tree Transformer, which integrates tree\nstructures into bidirectional Transformer encoder.\nAt each layer, words are constrained to attend to\nother words in the same constituents. This con-\nstraint has been proven to be effective in prior\nwork (Wu et al., 2018). Different from the prior\narXiv:1909.06639v2  [cs.CL]  1 Nov 2019\nwork that required a supervised parser, in Tree\nTransformer, the constituency tree structures is au-\ntomatically induced from raw texts by our pro-\nposed ‚ÄúConstituent Attention‚Äù module, which is\nsimply implemented by self-attention. Motivated\nby Tree-RNNs, which compose each phrase and\nthe sentence representation from its constituent\nsub-phrases, Tree Transformer gradually attaches\nseveral smaller constituents into larger ones from\nlower layers to higher layers.\nThe contributions of this paper are 3-fold:\n‚Ä¢Our proposed Tree Transformer is easy to\nimplement, which simply inserts an addi-\ntional ‚ÄúConstituent Attention‚Äù module im-\nplemented by self-attention to the original\nTransformer encoder, and achieves good per-\nformance on the unsupervised parsing task.\n‚Ä¢As the induced tree structures guide words\nto compose the meaning of longer phrases\nhierarchically, Tree Transformer improves\nthe perplexity on masked language modeling\ncompared to the original Transformer.\n‚Ä¢The behavior of attention heads learned\nby Tree Transformer expresses better inter-\npretability, because they are constrained to\nfollow the induced tree structures. By visu-\nalizing the self-attention matrices, our model\nprovides the information that better matchs\nthe human intuition about hierarchical struc-\ntures than the original Transformer.\n2 Related Work\nThis section reviews the recent progress about\ngrammar induction. Grammar induction is the task\nof inducing latent tree structures from raw texts\nwithout human-annotated data. The models for\ngrammar induction are usually trained on other\ntarget tasks such as language modeling. To obtain\nbetter performance on the target tasks, the mod-\nels have to induce reasonable tree structures and\nutilize the induced tree structures to guide text en-\ncoding in a hierarchical order. One prior attempt\nformulated this problem as a reinforcement learn-\ning (RL) problem (Yogatama et al., 2017), where\nthe unsupervised parser is an actor in RL and the\nparsing operations are regarded as its actions. The\nactor manages to maximize total rewards, which\nare the performance of downstream tasks.\nPRPN (Shen et al., 2018a) and On-LSTM (Shen\net al., 2018b) induce tree structures by introducing\na bias to recurrent neural networks. PRPN pro-\nposes a parsing network to compute the syntactic\ndistance of all word pairs, and a reading network\nutilizes the syntactic structure to attend relevant\nmemories. On-LSTM allows hidden neurons to\nlearn long-term or short-term information by the\nproposed new gating mechanism and new activa-\ntion function. In URNNG (Kim et al., 2019b), they\napplied amortized variational inference between a\nrecurrent neural network grammar (RNNG) (Dyer\net al., 2016) decoder and a tree structures inference\nnetwork, which encourages the decoder to gen-\nerate reasonable tree structures. DIORA (Droz-\ndov et al., 2019) proposed using inside-outside dy-\nnamic programming to compose latent represen-\ntations from all possible binary trees. The repre-\nsentations of inside and outside passes from same\nsentences are optimized to be close to each other.\nCompound PCFG (Kim et al., 2019a) achieves\ngrammar induction by maximizing the marginal\nlikelihood of the sentences which are generated by\na probabilistic context-free grammar (PCFG) in a\ncorpus.\n3 Tree Transformer\nGiven a sentence as input, Tree Transformer in-\nduces a tree structure. A 3-layer Tree Trans-\nformer is illustrated in Figure 1(A). The build-\ning blocks of Tree Transformer is shown in Fig-\nure 1(B), which is the same as those used in\nbidirectional Transformer encoder, except the pro-\nposed Constituent Attention module. The blocks\nin Figure 1(A) are constituents induced from the\ninput sentence. The red arrows indicate the self-\nattention. The words in different constituents are\nconstrained to not attend to each other. In the 0-\nth layer, some neighboring words are merged into\nconstituents; for example, given the sentence ‚Äúthe\ncute dog is wagging its tail‚Äù, the tree Transformer\nautomatically determines that ‚Äú cute‚Äù and ‚Äú dog‚Äù\nform a constituent, while ‚Äúits‚Äù and ‚Äútail‚Äù also form\none. The two neighboring constituents may merge\ntogether in the next layer, so the sizes of con-\nstituents gradually grow from layer to layer. In the\ntop layer, the layer 2, all words are grouped into\nthe same constituent. Because all words are into\nthe same constituent, the attention heads freely at-\ntend to any other words, in this layer, Tree Trans-\nformer behaves the same as the typical Trans-\nformer encoder. Tree Transformer can be trained\nin an end-to-end fashion by using ‚Äúmasked LM‚Äù,\nLayer 0\nLayer 1\nLayer 2\nthe cute dog is wagging its tail\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nConstituent\nAttention\nConstituent\nPriors\nthe \ncute\ndog\nis\nwagging\nits\ntail\nthe \ncute\ndog\nis\nwagging\nits\ntail\n(A) (B) (C)\nFigure 1: (A) A 3-layer Tree Transformer, where the blocks are constituents induced from the input sentence. The\ntwo neighboring constituents may merge together in the next layer, so the sizes of constituents gradually grow\nfrom layer to layer. The red arrows indicate the self-attention. (B) The building blocks of Tree Transformer. (C)\nConstituent prior Cfor the layer 1.\nwhich is one of the unsupervised training task used\nfor BERT training.\nWhether two words belonging to the same con-\nstituent is determined by ‚ÄúConstituent Prior‚Äù that\nguides the self-attention. Constituent Prior is de-\ntailed in Section 4, which is computed by the\nproposed Constituent Attention module in Sec-\ntion 5. By using BERT masked language model as\ntraining, latent tree structures emerge from Con-\nstituent Prior and unsupervised parsing is thereby\nachieved. The method for extracting the con-\nstituency parse trees from Tree Transformer is de-\nscribed in Section 6.\n4 Constituent Prior\nIn each layer of Transformer, there are a query ma-\ntrix Qconsisting of query vectors with dimension\ndk and a key matrix K consisting of key vectors\nwith dimension dk. The attention probability ma-\ntrix is denoted as E, which is an N by N matrix,\nwhere N is the number of words in an input sen-\ntence. Ei,j is the probability that the position i\nattends to the position j. The Scaled Dot-Product\nAttention computes the Eas:\nE = softmax(QKT\nd ), (1)\nwhere the dot-product is scaled by 1/d. In Trans-\nformer, the scaling factor dis set to be ‚àödk.\nIn Tree Transformer, the E is not only deter-\nmined by the query matrix Q and key matrix K,\nbut also guided by Constituent Prior C generat-\ning from Constituent Attention module. Same as\nE, the constituent prior C is also a N by N ma-\ntrix, where Ci,j is the probability that wordwi and\nword wj belong to the same constituency. This\nmatrix is symmetric that Ci,j is same as Cj,i. Each\nlayer has its own Constituent Prior C. An exam-\nple of Constituent Prior C is illustrated in Fig-\nure 1 (C), which indicates that in layer1, ‚Äúthe cute\ndog‚Äù and ‚Äúis wagging its tail‚Äù are two constituents.\nTo make each position not attend to the position\nin different constituents, Tree Transformer con-\nstrains the attention probability matrix E by con-\nstituent prior Cas below,\nE = C‚äôsoftmax(QKT\nd ), (2)\nwhere ‚äô is the element-wise multiplication.\nTherefore, if Ci,j has small value, it indicates\nthat the positions iand j belong to different con-\nstituents, where the attention weightEi,j would be\nsmall. As Transformer uses multi-head attention\nwith hdifferent heads, there are hdifferent query\nmatrices Q and key matrices K at each position,\nbut here in the same layer, all attention heads in\nmulti-head attention share the same C. The multi-\nhead attention module produces the output of di-\nmension dmodel = h√ódk.\n5 Constituent Attention\nThe proposed Constituent Attention module is\nto generate the constituent prior C. Instead of\ndirectly generating C, we decompose the prob-\nlem into estimating the breakpoints between con-\nstituents, or the probability that two adjacent\nwords belong to the same constituent. In each\nlayer, the Constituent Attention module generates\na sequence a = {a1,...,a i,...,a N }, where ai is\nthe probability that the word wi and its neighbor\nis wagging its tailThe\nùëÉ1,2\n0.1 0.9\nùëÉ2,1 ùëÉ2,3\n0.8 0.2\nùëÉ1,0\ncute dog\nFigure 2: The example illustration about how neigh-\nboring attention works.\nword wi+1 are in the same constituent. The small\nvalue of ai implies that there is a breakpoint be-\ntween wi and wi+1, so the constituent prior C is\nobtained from the sequence a as follows. Ci,j is\nthe multiplication of all ai‚©Ωk<j between word wi\nand word wj:\nCi,j =\nj‚àí1‚àè\nk=i\nak. (3)\nIn (3), we choose to use multiplication instead of\nsummation, because if one of ai‚©Ωk<j between two\nwords wi and wj is small, the value of Ci,j with\nmultiplication also becomes small. In implemen-\ntation, to avoid probability vanishing, we use log-\nsum instead of directly multiplying all a:\nCi,j = e\n‚àëj‚àí1\nk=i log(ak). (4)\nThe sequence ais obtained based on the follow-\ning two mechanisms: Neighboring Attention and\nHierarchical Constraint.\n5.1 Neighboring Attention\nWe compute the score si,i+1 indicating that wi\nlinks to wi+1 by scaled dot-product attention:\nsi,i+1 = qi ¬∑ki+1\nd , (5)\nwhere qi is a link query vector of wi with dmodel\ndimensions, and ki+1 is a link key vector of wi+1\nwith dmodel dimensions. We use qi ¬∑ki+1 to rep-\nresent the tendency that wi and wi+1 belong to the\nsame constituent. Here, we set the scaling factor\nd to be dmodel\n2 . The query and key vectors in (5)\nare different from (1). They are computed by the\nsame network architecture, but with different sets\nof network parameters.\nFor each word, we constrain it to either link to\nits right neighboror left neighboras illustrated in\nFigure 2. This constraint is implemented by ap-\nplying a softmax function to two attention links of\nwi:\npi,i+1,pi,i‚àí1 = softmax(si,i+1,si,i‚àí1), (6)\nwhere pi,i+1 is the probability that wi attends to\nwi+1, and (pi,i+1 +pi,i‚àí1) = 1. We Ô¨Ånd that with-\nout the constraint of the softmax operation in (6)\nthe model prefers to link all words together and\nassign all words to the same constituency. That is,\ngiving both si,i+1 and si,i‚àí1 large values, so the at-\ntention head freely attends to any position without\nrestriction of constituent prior, which is the same\nas the original Transformer. Therefore, the soft-\nmax function is to constraint the attention to be\nsparse.\nAs pi,i+1 and pi+1,i may have different values,\nwe average its two attention links:\nÀÜai = ‚àöpi,i+1 √ópi+1,i. (7)\nThe ÀÜai links two adjacent words only if two words\nattend to each other. ÀÜai is used in the next subsec-\ntion to obtain ai.\n5.2 Hierarchical Constraint\nAs mentioned in Section 3, constituents in the\nlower layer merge into larger one in the higher\nlayer. That is, once two words belong to the same\nconstituent in the lower layer, they would still be-\nlong to the same constituent in the higher layer. To\napply the hierarchical constraint to the tree Trans-\nformer, we restrictal\nk to be always larger thanal‚àí1\nk\nfor the layer l and word index k. Hence, at the\nlayer l, the link probability al\nk is set as:\nal\nk = al‚àí1\nk + (1‚àíal‚àí1\nk )ÀÜal\nk, (8)\nwhere al‚àí1\nk is the link probability from the previ-\nous layer l‚àí1, and ÀÜal\nk is obtained from Neighbor-\ning Attention (Section 5.1) of the current layer l.\nFinally, at the layer l, we apply (4) for computing\nCl from al. Initially, different words are regarded\nas different constituents, and thus we initializea‚àí1\nk\nas zero.\n6 Unsupervised Parsing from Tree\nTransformer\nAfter training, the neighbor link probability acan\nbe used for unsupervised parsing. The small value\nof a suggests this link be the breakpoint of two\nconstituents. By top-down greedy parsing (Shen\net al., 2018a), which recursively splits the sentence\ninto two constituents with minimuma, a parse tree\ncan be formed.\nHowever, because each layer has a set of al, we\nhave to decide to use which layer for parsing. In-\nstead of using afrom a speciÔ¨Åc layer for parsing\nAlgorithm 1Unsupervised Parsing with Multiple\nLayers\n1: a‚Üêlink probabilities\n2: m‚Üêminimumlayerid ‚äø Discard the a\nfrom layers below minimum layer\n3: thres‚Üê0.8 ‚äøThreshold of breakpoint\n4: procedure BUILD TREE (l,s,e ) ‚äøl: layer\nindex, s: start index, e: end index\n5: if e‚àís< 2 then‚äøThe constituent cannot\nbe split\n6: return (s,e)\n7: span‚Üêal\ns‚â§i<e\n8: b‚Üêargmin(span) ‚äøGet breakpoint\n9: last‚Üêmax(l‚àí1,m) ‚äøGet index of last\nlayer\n10: if al\nb >thres then\n11: if l= mthen\n12: return (s,e)\n13: return BuildTree(last,s,e )\n14: tree1 ‚ÜêBuildTree(last,s,b )\n15: tree2 ‚ÜêBuildTree(last,b + 1,e)\n16: return (tree1,tree2) ‚äøReturn tree\n(Shen et al., 2018b), we propose a new parsing al-\ngorithm, which utilizes a from all layers for un-\nsupervised parsing. As mentioned in Section 5.2,\nthe values of aare strictly increasing, which indi-\ncates that a directly learns the hierarchical struc-\ntures from layer to layer. Algorithm 1 details how\nwe utilize hierarchical information of afor unsu-\npervised parsing.\nThe unsupervised parsing starts from the top\nlayer, and recursively moves down to the last layer\nafter Ô¨Ånding a breakpoint until reaching the bot-\ntom layer m. The bottom layermis a hyperparam-\neter needed to be tuned, and is usually set to 2 or\n3. We discard afrom layers below m, because we\nÔ¨Ånd the lowest few layers do not learn good rep-\nresentations (Liu et al., 2019) and thus the parsing\nresults are poor (Shen et al., 2018b). All values of\naon top few layers are very close to 1, suggesting\nthat those are not good breakpoints. Therefore, we\nset a threshold for deciding a breakpoint, where a\nminimum a will be viewed as a valid breakpoint\nonly if its value is below the threshold. As we Ô¨Ånd\nthat our model is not very sensitive to the threshold\nvalue, we set it to be 0.8 for all experiments.\n7 Experiments\nIn order to evaluate the performance of our pro-\nposed model, we conduct the experiments detailed\nbelow.\n7.1 Model Architecture\nOur model is built upon a bidirectional Trans-\nformer encoder. The implementation of our Trans-\nformer encoder is identical to the original Trans-\nformer encoder. For all experiments, we set the\nhidden size dmodel of Constituent Attention and\nTransformer as 512, the number of self-attention\nheads h as 8, the feed-forward size as 2048 and\nthe dropout rate as 0.1. We analyze and discuss\nthe sensitivity of the number of layers, denoted as\nL, in the following experiments.\n7.2 Grammar Induction\nIn this section, we evaluate the performance of our\nmodel on unsupervised constituency parsing. Our\nmodel is trained on WSJ training set and WSJ-all\n(i.e. including testing and validation sets) by us-\ning BERT Masked LM (Devlin et al., 2018) as un-\nsupervised training task. We use WordPiece (Wu\net al., 2016) tokenizer from BERT to tokenize\nwords with a 16k token vocabulary. Our best re-\nsult is optimized by adam with a learning rate of\n0.0001, Œ≤1 = 0.9 and Œ≤2 = 0.98. Following the\nevaluation settings of prior work (Htut et al., 2018;\nShen et al., 2018b)2, we evaluate F1 scores of our\nmodel on WSJ-test and WSJ-10 of Penn Treebank\n(PTB) (Marcus et al., 1993). The WSJ-10 has\n7422 sentences from whole PTB with sentence\nlength restricted to 10 after punctuation removal,\nwhile WSJ-test has 2416 sentences from the PTB\ntesting set with unrestricted sentence length.\nThe results on WSJ-test are in Table 1. We\nmainly compare our model to PRPN (Shen et al.,\n2018a), On-lstm (Shen et al., 2018b) and Com-\npound PCFG(C-PCFG) (Kim et al., 2019a), in\nwhich the evaluation settings and the training data\nare identical to our model. DIORA (Drozdov\net al., 2019) and URNNG (Kim et al., 2019b) use a\nrelative larger training data and the evaluation set-\ntings are slightly different from our model. Our\nmodel performs much better than trivial trees (i.e.\nright and left-branching trees) and random trees,\nwhich suggests that our proposed model success-\nfully learns meaningful trees. We also Ô¨Ånd that\n2https://github.com/yikangshen/\nOrdered-Neurons\nModel Data F1 median F1max\nPRPN WSJ-train 35.0 42.8\nOn-lstm WSJ-train 47.7 49.4\nC-PCFG WSJ-train 55.2 60.1\nTree-T,L=12 WSJ-train 48.4 50.2\nTree-T,L=10 WSJ-train 49.5 51.1\nTree-T,L=8 WSJ-train 48.3 49.6\nTree-T,L=6 WSJ-train 47.4 48.8\nDIORA NLI 55.7 56.2\nURNNG Billion - 52.4\nTree-T,L=10 WSJ-all 50.5 52.0\nRandom - 21.6 21.8\nLB - 9.0 9.0\nRB - 39.8 39.8\nTable 1: The F1 scores on WSJ-test. Tree Trans-\nformer is abbreviated as Tree-T, and L is the num-\nber of layers(blocks). DIORA is trained on multi-NLI\ndataset (Williams et al., 2018). URNNG is trained on\nthe subset of one billion words (Chelba et al., 2013)\nwith 1M training data. LB and RB are the left and\nright-brancing baselines.\nFigure 3: A parse tree induced by Tree Transformer.\nAs shown in the Ô¨Ågure, because we set a threshold in\nAlgorithm 1, the leaf nodes are not strictly binary.\nincreasing the layer number results in better per-\nformance, because it allows the Tree Transformer\nto model deeper trees. However, the performance\nstops growing when the depth is above 10. The\nwords in the layers above the certain layer are\nall grouped into the same constituent, and there-\nfore increasing the layer number will no longer\nhelp model discover useful tree structures. In Ta-\nble 2, we report the results on WSJ-10. Some of\nthe baselines including CCM (Klein and Manning,\n2002), DMV+CCM (Klein and Manning, 2005)\nand UML-DOP (Bod, 2006) are not directly com-\nparable to our model, because they are trained us-\ning POS tags our model does not consider.\nIn addition, we further investigate what kinds\nof trees are induced by our model. Following\nURNNG, we evaluate the performance of con-\nModel Data F1 median F1max\nPRPN WSJ-train 70.5 71.3\nOn-lstm WSJ-train 65.1 66.8\nC-PCFG WSJ-train 70.5 -\nTree-T,L=10 WSJ-train 66.2 67.9\nDIORA NLI 67.7 68.5\nTree-T,L=10 WSJ-all 66.2 68.0\nCCM WSJ-10 - 71.9\nDMV+CCM WSJ-10 - 77.6\nUML-DOP WSJ-10 - 82.9\nRandom - 31.9 32.6\nLB - 19.6 19.6\nRB - 56.6 56.6\nTable 2: The F1 scores on WSJ-10.Tree Transformer\nis abbreviated as Tree-T, and L is the number of layers\n(blocks).\nLabel Tree-T URNNG PRPN\nNP 67.6 39.5 63.9\nVP 38.5 76.6 27.3\nPP 52.3 55.8 55.1\nADJP 24.7 33.9 42.5\nSBAR 36.4 74.8 28.9\nADVP 55.1 50.4 45.1\nTable 3: Recall of constituents by labels. The re-\nsults of URNNG and PRPN are taken from Kim et al.\n(2019b).\nstituents by its label in Table 3. The trees in-\nduced by different methods are quite different.\nOur model is inclined to discover noun phrases\n(NP) and adverb phrases (ADVP), but not easy\nto discover verb phrases (VP) or adjective phrases\n(ADJP). We show an induced parse tree in Fig-\nure 8 and more induced parse trees can be found\nin Appendix.\n7.3 Analysis of Induced Structures\nIn this section, we study whether Tree Trans-\nformer learns hierarchical structures from layers to\nlayers. First, we analyze the inÔ¨Çuence of the hy-\nperparameter minimum layer m in Algorithm. 1\ngiven the model trained on WSJ-all in Table 1.\nAs illustrated in Figure 4(a), setting m to be 3\nyields the best performance. Prior work discov-\nered that the representations from the lower lay-\ners of Transformer are not informative (Liu et al.,\n2019). Therefore, using syntactic structures from\nlower layers decreases the quality of parse trees.\nOn the other hand, most syntactic information is\n15\n20\n25\n30\n35\n40\n45\n50\n55\n0 1 2 3 4 5 6 7 8 9\nF1\nMinimum Layer m\n(a) F1 in terms of different minimum layers.\n15\n20\n25\n30\n35\n40\n45\n50\n55\n0 1 2 3 4 5 6 7 8 9\nF1\nSpecific Layer for Parsing (b) F1 of parsing via a speciÔ¨Åc layer.\nFigure 4: Performance of unsupervised parsing.\nmissing when afrom top few layers are close to1,\nso too large malso decreases the performance.\nTo further analyze which layer contains richer\ninformation of syntactic structures, we evaluate\nthe performance on obtaining parse trees from a\nspeciÔ¨Åc layer. We use al from the layer l for\nparsing with the top-down greedy parsing algo-\nrithm (Shen et al., 2018a). As shown in Fig-\nure 4(b), using a3 from the layer 3 for parsing\nyields the best F1 score, which is 49.07. The re-\nsult is consistent to the best value of m. However,\ncompared to our best result (52.0) obtained by Al-\ngorithm 1, the F1-score decreases by 3 (52.0 ‚Üí\n49.07). This demonstrates the effectiveness of\nTree Transformer in terms of learning hierarchi-\ncal structures. The higher layers indeed capture\nthe higher-level syntactic structures such as clause\npatterns.\n7.4 Interpretable Self-Attention\nThis section discusses whether the attention heads\nin Tree Transformer learn hierarchical structures.\nConsidering that the most straightforward way of\ninterpreting what attention heads learn is to visu-\nalize the attention scores, we plot the heat maps of\nConstituent Attention prior C from each layer in\nFigure 5.\nIn the heat map of constituent prior from Ô¨Årst\nlayer (Figure 5(a)), as the size of constituent is\nsmall, the words only attend to its adjacent words.\nWe can observe that the model captures some sub-\nphrase structures, such as the noun phrase ‚Äú delta\nair line‚Äù or ‚Äú american airlines unit‚Äù. In Fig-\nure 5(b)-(d), the constituents attach to each other\nand become larger. In the layer 6, the words from\n‚Äúinvolved‚Äù to last word ‚Äú lines‚Äù form a high-level\nModel L Params Perplexity\nTransformer 8 40M 48.8\nTransformer 10 46M 48.5\nTransformer 10-B 67M 49.2\nTransformer 12 52M 48.1\nTree-T 8 44M 46.1\nTree-T 10 51M 45.7\nTree-T 12 58M 45.6\nTable 4: The perplexity of masked words.Params is\nthe number of parameters. We denote the number of\nlayers as L. In Transformer L= 10‚àíB, the increased\nhidden size results in more parameters.\nadjective phrase (ADJP). In the layer 9, all words\nare grouped into a large constituent except the\nÔ¨Årst word ‚Äú but‚Äù. By visualizing the heat maps\nof the constituent prior from each layer, we can\neasily know what types of syntactic structures are\nlearned in each layer. The parse tree of this exam-\nple can be found in Figure 8 of Appendix. We also\nvisualize the heat maps of self-attention from the\noriginal Transformer layers and one from the Tree\nTransformer layers in Appendix A. As the self-\nattention heads from our model are constrained\nby the constituent prior, compared to the original\nTransformer, we can discover hierarchical struc-\ntures more easily.\nThose attention heat maps demonstrate that: (1)\nthe size of constituents gradually grows from layer\nto layer, and (2) at each layer, the attention heads\ntend to attend to other words within constituents\nposited in that layer. Those two evidences support\nthe success of the proposed Tree Transformer in\nterms of learning tree-like structures.\n(a) The layer 0.\n (b) The layer 3.\n(c) The layer 6.\n (d) The layer 9.\nFigure 5: The constituent prior heat maps.\n7.5 Masked Language Modeling\nTo investigate the capability of Tree Transformer\nin terms of capturing abstract concepts and syn-\ntactic knowledge, we evaluate the performance on\nlanguage modeling. As our model is a bidirec-\ntional encoder, in which the model can see its\nsubsequent words, we cannot evaluate the lan-\nguage model in a left-to-right manner. We evaluate\nthe performance on masked language modeling\nby measuring the perplexity on masked words 3.\nTo perform the inference without randomness, for\neach sentence in the testing set, we mask all words\nin the sentence, but not at once. In each masked\ntesting data, only one word is replaced with a\n‚Äú[MASK]‚Äù token. Therefore, each sentence cre-\nates the number of testing samples equal to its\n3The perplexity of masked words is e\n‚àí‚àëlog(p)\nnmask , where p\nis the probability of correct masked word to be predicted and\nnmask is the total number of masked words.\nlength.\nIn Table 4, the models are trained on WSJ-train\nwith BERT masked LM and evaluated on WSJ-\ntest. All hyperparameters except the number of\nlayers in Tree Transformer and Transformer are\nset to be the same and optimized by the same opti-\nmizer. We use adam as our optimizer with learn-\ning rate of 0.0001, Œ≤1 = 0.9 and Œ≤2 = 0.999. Our\nproposed Constituent Attention module increases\nabout 10% hyperparameters to the original Trans-\nformer encoder and the computational speed is 1.2\ntimes slower. The results with best performance\non validation set are reported. Compared to the\noriginal Transformer, Tree Transformer achieves\nbetter performance on masked language modeling.\nAs the performance gain is possibly due to more\nparameters, we adjust the number of layers or in-\ncrease the number of hidden layers in Transformer\nL = 10 ‚àíB. Even with fewer parameters than\nTransformer, Tree Transformer still performs bet-\nter.\nThe performance gain is because the induced\ntree structures guide the self-attention processes\nlanguage in a more straightforward and human-\nlike manner, and thus the knowledge can be bet-\nter generalized from training data to testing data.\nAlso, Tree Transformer acquires positional infor-\nmation not only from positional encoding but also\nfrom the induced tree structures, where the words\nattend to other words from near to distant (lower\nlayers to higher layers)4.\n7.6 Limitations and Discussion\nIt is worth mentioning that we have tried to initial-\nize our Transformer model with pre-trained BERT,\nand then Ô¨Åne-tuning on WSJ-train. However, in\nthis setting, even when the training loss becomes\nlower than the loss of training from scratch, the\nparsing result is still far from our best results. This\nsuggests that the attention heads in pre-trained\nBERT learn quite different structures from the\ntree-like structures in Tree Transformer. In ad-\ndition, with a well-trained Transformer, it is not\nnecessary for the Constituency Attention module\nto induce reasonable tree structures, because the\ntraining loss decreases anyway.\n8 Conclusion\nThis paper proposes Tree Transformer, a Ô¨Årst at-\ntempt of integrating tree structures into Trans-\nformer by constraining the attention heads to at-\ntend within constituents. The tree structures are\nautomatically induced from the raw texts by our\nproposed Constituent Attention module, which\nattaches the constituents to each other by self-\nattention. The performance on unsupervised pars-\ning demonstrates the effectiveness of our model in\nterms of inducing tree structures coherent to hu-\nman expert annotations. We believe that incorpo-\nrating tree structures into Transformer is an im-\nportant and worth exploring direction, because it\nallows Transformer to learn more interpretable at-\ntention heads and achieve better language model-\ning. The interpretable attention can better explain\nhow the model processes the natural language and\nguide the future improvement.\n4We do not remove the positional encoding from the\nTransformer and we Ô¨Ånd that without positional encoding the\nquality of induced parse trees drops.\nAcknowledgements\nWe would like to thank reviewers for their insight-\nful comments. This work was Ô¨Ånancially sup-\nported from the Young Scholar Fellowship Pro-\ngram by Ministry of Science and Technology\n(MOST) in Taiwan, under Grant 108-2636-E002-\n003.\nReferences\nRoee Aharoni and Yoav Goldberg. 2017. Towards\nstring-to-tree neural machine translation. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers).\nRens Bod. 2006. An all-subtrees approach to unsuper-\nvised parsing. In Proceedings of the 21st Interna-\ntional Conference on Computational Linguistics and\nthe 44th Annual Meeting of the Association for Com-\nputational Linguistics.\nGlenn Carroll and Eugene Charniak. 1992. Two exper-\niments on learning probabilistic dependency gram-\nmars from corpora. In WORKING NOTES OF THE\nWORKSHOP STATISTICALLY-BASED NLP TECH-\nNIQUES.\nC.Goller and A.Kuchler. 1996. Learning task-\ndependent distributed representations by backpropa-\ngation through structure. In Proceedings of Interna-\ntional Conference on Neural Networks (ICNN‚Äô96).\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. UniÔ¨Åed\nlanguage model pre-training for natural language\nunderstanding and generation. arXiv preprint\narXiv:1905.03197.\nAndrew Drozdov, Pat Verga, Mohit Yadav, Mohit\nIyyer, and Andrew McCallum. 2019. Unsupervised\nlatent tree induction with deep inside-outside recur-\nsive autoencoders. In North American Association\nfor Computational Linguistics.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proc. of NAACL.\nAkiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun\nCho. 2017. Learning to parse and translate improves\nneural machine translation. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers).\nPhu Mon Htut, Kyunghyun Cho, and Samuel Bowman.\n2018. Grammar induction with neural language\nmodels: An unusual replication. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP. As-\nsociation for Computational Linguistics.\nYoon Kim, Chris Dyer, and Alexander Rush. 2019a.\nCompound probabilistic context-free grammars for\ngrammar induction. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2369‚Äì2385.\nYoon Kim, Alexander M. Rush, Lei Yu, Adhiguna\nKuncoro, Chris Dyer, and G ¬¥abor Melis. 2019b.\nUnsupervised recurrent neural network grammars.\narXiv preprint arXiv:1904.03746.\nDan Klein and Christopher D. Manning. 2002. A\ngenerative constituent-context model for improved\ngrammar induction. In Proceedings of 40th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nDan Klein and Christopher D. Manning. 2005. Nat-\nural language grammar induction with a generative\nconstituent-context model. Pattern Recogn.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. arXiv preprint arXiv:1903.08855.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of english: The penn treebank. Comput. Lin-\nguist.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nYikang Shen, Zhouhan Lin, Chin wei Huang, and\nAaron Courville. 2018a. Neural language modeling\nby jointly learning syntax and lexicon. In Interna-\ntional Conference on Learning Representations.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2018b. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks.\narXiv preprint arXiv:1810.09536.\nNoah A. Smith and Jason Eisner. 2005. Guiding un-\nsupervised grammar induction using contrastive es-\ntimation. In In Proc. of IJCAI Workshop on Gram-\nmatical Inference Applications.\nRichard Socher, Cliff Chiung-Yu Lin, Andrew Y . Ng,\nand Christopher D. Manning. 2011. Parsing natu-\nral scenes and natural language with recursive neu-\nral networks. In Proceedings of the 28th Interna-\ntional Conference on International Conference on\nMachine Learning.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid I Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In EMNLP.\nKai Sheng Tai, Richard Socher, and Christopher D.\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. arXiv preprint arXiv:1503.00075.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language representation models. arXiv\npreprint arXiv:1904.02679.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers).\nWei Wu, Houfeng Wang, Tianyu Liu, and Shuming\nMa. 2018. Phrase-level self-attention networks for\nuniversal sentence encoding. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3729‚Äì3738.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, and et al. 2016. Google‚Äôs neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward\nGrefenstette, and Wang Ling. 2017. Learning to\ncompose words into sentences with reinforcement\nlearning. In International Conference on Learning\nRepresentations.\nPoorya Zaremoodi and Gholamreza Haffari. 2018. In-\ncorporating syntactic uncertainty in neural machine\ntranslation with a forest-to-sequence model. In Pro-\nceedings of the 27th International Conference on\nComputational Linguistics.\nA Visualization of Self-Attention\nIn this section, we plot the heat maps of attention\nprobability E from the original Transformer and\none from Tree Transformer. In order to investi-\ngate whether models learn layer-wise tree struc-\ntures, we average the scores of the attention heads\nwithin the same layer.\nIn the heat map from Ô¨Årst layer of Tree Trans-\nformer (Figure 6(a)), as the size of constituent in\nthe layer is small, the words only attend to its adja-\ncent words. We also observe that the words within\nthe same phrase attend to each other, such as\nthe phrasal verb ‚Äúinvolve in‚Äù and the noun phrase\n‚Äúdelta air line‚Äù. With the constituents attaching to\neach other and becoming larger, in Figure 6(b)-(d),\nthe words attend to more distant words.\nOn the other hard, we average the scores of\nattention heads from each layer of the original\nTransformer and visualize them. In the attention\nheat maps of the original Transformer, we also ob-\nserve that it learns hierarchical structures in the\nÔ¨Årst layer (Figure 7(a)) that the words mostly at-\ntend to its neighboring words. However, this phe-\nnomenon is only observed in the Ô¨Årst layer that in\nthe third layer (Figure 7(b)), the attention heads\nattend to other words at will.\n(a) The layer 0.\n (b) The layer 3.\n(c) The layer 6.\n (d) The layer 9.\nFigure 6: The attention heat map from the Tree Transformer.\n(a) The layer 0.\n (b) The layer 3.\nFigure 7: The attention heat map from the original Transformer.\nFigure 8: The parse trees induced by the proposed Tree Transformer.\nFigure 9: The parse trees induced by the proposed Tree Transformer.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8023601770401001
    },
    {
      "name": "Computer science",
      "score": 0.6940451264381409
    },
    {
      "name": "Encoder",
      "score": 0.6623077392578125
    },
    {
      "name": "Tree structure",
      "score": 0.5912342667579651
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5075398683547974
    },
    {
      "name": "Tree (set theory)",
      "score": 0.4744773209095001
    },
    {
      "name": "Natural language processing",
      "score": 0.3978264629840851
    },
    {
      "name": "Machine learning",
      "score": 0.3916410803794861
    },
    {
      "name": "Engineering",
      "score": 0.12528255581855774
    },
    {
      "name": "Data structure",
      "score": 0.11764350533485413
    },
    {
      "name": "Voltage",
      "score": 0.11572733521461487
    },
    {
      "name": "Mathematics",
      "score": 0.11257314682006836
    },
    {
      "name": "Electrical engineering",
      "score": 0.08329388499259949
    },
    {
      "name": "Programming language",
      "score": 0.06339043378829956
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ],
  "cited_by": 11
}