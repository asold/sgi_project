{
  "title": "SSAST: Self-Supervised Audio Spectrogram Transformer",
  "url": "https://openalex.org/W3206996142",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2140259748",
      "name": "Yuan Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296074203",
      "name": "Cheng-I Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227633962",
      "name": "Yu-An Chung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154846939",
      "name": "James Glass",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140259748",
      "name": "Yuan Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296074203",
      "name": "Cheng-I Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227633962",
      "name": "Yu-An Chung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154846939",
      "name": "James Glass",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3141838378",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2926827382",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W6792861227",
    "https://openalex.org/W2902332991",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W1538131130",
    "https://openalex.org/W2981087920",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W4398958419",
    "https://openalex.org/W3002741552",
    "https://openalex.org/W3093563057",
    "https://openalex.org/W6600721412",
    "https://openalex.org/W2768188490",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2964328535",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3160799772",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3196974791",
    "https://openalex.org/W3198035615",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W3096587983",
    "https://openalex.org/W2963855133",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4287236261",
    "https://openalex.org/W2052666245",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3143083666",
    "https://openalex.org/W4287631799",
    "https://openalex.org/W3201143670",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3157923770",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2948433173",
    "https://openalex.org/W2797583228",
    "https://openalex.org/W2973157397",
    "https://openalex.org/W3135828102",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3162391496"
  ],
  "abstract": "Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aims to reduce the need for large amounts of labeled data for the AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based self-supervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST.",
  "full_text": "SSAST: Self-Supervised Audio Spectrogram Transformer\nYuan Gong, Cheng-I Jeff Lai, Yu-An Chung, James Glass\nMIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139\n{yuangong, clai24, andyyuan, glass}@mit.edu\nAbstract\nRecently, neural networks based purely on self-attention,\nsuch as the Vision Transformer (ViT), have been shown to\noutperform deep learning models constructed with convolu-\ntional neural networks (CNNs) on various vision tasks, thus\nextending the success of Transformers, which were originally\ndeveloped for language processing, to the vision domain. A\nrecent study showed that a similar methodology can also be\napplied to the audio domain. Specifically, the Audio Spec-\ntrogram Transformer (AST) achieves state-of-the-art results\non various audio classification benchmarks. However, pure\nTransformer models tend to require more training data com-\npared to CNNs, and the success of the AST relies on super-\nvised pretraining that requires a large amount of labeled data\nand a complex training pipeline, thus limiting the practical\nusage of AST. This paper focuses on audio and speech classi-\nfication, and aims to reduce the need for large amounts of la-\nbeled data for the AST by leveraging self-supervised learning\nusing unlabeled data. Specifically, we propose to pretrain the\nAST model with joint discriminative and generative masked\nspectrogram patch modeling (MSPM) using unlabeled au-\ndio from AudioSet and Librispeech. We evaluate our pre-\ntrained models on both audio and speech classification tasks\nincluding audio event classification, keyword spotting, emo-\ntion recognition, and speaker identification. The proposed\nself-supervised framework significantly boosts AST perfor-\nmance on all tasks, with an average improvement of 60.9%,\nleading to similar or even better results than a supervised\npretrained AST. To the best of our knowledge, it is the first\npatch-based self-supervised learning framework in the audio\nand speech domain, and also the first self-supervised learning\nframework for AST.\n1 Introduction\nPure self-attention based deep learning architectures, such as\nthe Vision Transformer (Dosovitskiy et al. 2021) and its vari-\nants (e.g., DeiT (Touvron et al. 2020), T2T-ViT (Yuan et al.\n2021)) have been shown to outperform CNN models (LeCun\nand Bengio 1995) of similar size on various vision tasks.\nSuch models differ from CNN models or CNN-attention\nhybrid models in that they do not contain non-degenerated\nconvolutions (Chen, Xie, and He 2021) and thus have less\n*Code and models at https://github.com/YuanGongND/ssast\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ninductive bias such as spatial locality or translation equiv-\nariance, and are more data-driven. In the audio and speech\ndomain, the recently proposed Audio Spectrogram Trans-\nformer (AST) (Gong, Chung, and Glass 2021) and the Key-\nword Transformer (Berg, O’Connor, and Cruz 2021) also\nachieve new state-of-the-art performance on audio scene\nclassification and keyword spotting. Despite the strong per-\nformance, a critical issue of such pure self-attention based\nmodels is they tend to require more training data than\nCNNs (Dosovitskiy et al. 2021). For example, the ViT out-\nperforms CNNs only when the training data volume is larger\nthan about 100 million samples. AST also does not per-\nform well when it is trained from scratch, and the success of\nAST strongly relies on supervised pretraining. Since labeled\nspeech and audio data is limited, AST uses cross-modal pre-\ntraining with ImageNet data (Deng et al. 2009). However, in\npractice, supervised pretraining on ImageNet data is com-\nplex (He et al. 2019) and expensive, and also constrains the\nvision and audio models to have a similar architecture and\nuse the same patch size and shape. Further, the validity and\ntransferability of such cross-modal pretraining for a specific\naudio or speech task are unclear.\nWhile annotating audio and speech data is expensive,\nwe can easily get web-scale unlabeled audio and speech\ndata from radio or YouTube. This motivates us to ex-\nplore Self-Supervised AST (SSAST) that leverages unla-\nbeled data to alleviate the data requirement problem. In\nthis paper, we present a novel joint discriminative and\ngenerative Masked Spectrogram Patch Modeling (MSPM)\nbased self-supervised learning (SSL) framework that can\nsignificantly improve AST performance with limited la-\nbeled data. Previous self-supervised learning methods such\nas wav2vec (Schneider et al. 2019) or autoregressive predic-\ntive coding (APC) (Chung et al. 2019) use an objective that\npredicts future or masked temporal spectrogram frames, thus\npotentially learning only the temporal structure of the spec-\ntrogram. In contrast, the objective of MSPM is to predict\na specific frequency band in a specific time range (i.e., a\n“spectrogram patch”) given the neighboring band and time\ninformation, which allows the model to learn both the tem-\nporal and frequency structure. The spectrogram patch can\nbe an arbitrary shape and size, e.g., it can be a conventional\ntime frame or a square patch.\nIn addition, most previous SSL research considers either\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10699\nonly speech or only audio events, but in this work, we show\nthat the SSL model can be generalized to both speech and\naudio tasks. Specifically, we pretrain our model using both\nLibrispeech and AudioSet, and evaluate the model on a vari-\nety of speech and audio tasks including audio event classifi-\ncation, keyword spotting, speaker identification, and speech\nemotion recognition. Our experiments demonstrate the ef-\nfectiveness of the proposed MSPM framework: a model\npretrained with MSPM can significantly outperform from-\nscratch models for all 6 benchmarks we evaluated with an\naverage improvement of 60.9%, and the performance can\neven match or outperform supervised pretrained models.\nThe contributions of this work are two-fold:\n1. We propose MSPM, a novel patch-based joint discrim-\ninative and generative self-supervised learning frame-\nwork. With MSPM pretraining, our SSAST model\nmatches or outperforms previous supervised pretrained\nAST. To the best of our knowledge, MSPM is the first\npatch-based self-supervised learning framework in the\naudio and speech domains, and SSAST is the first self-\nsupervised pure self-attention based audio classification\nmodel. Further, we conduct extensive experiments to\nthoroughly investigate the design choices and quantify\nthe performance impact of each factor.\n2. We show that pretraining with both speech and audio\ndatasets noticeably improves the models’ generalization\nability, and leads to better performance than pretraining\nwith dataset from a single domain. As a consequence,\nour SSAST model performs well on both speech and au-\ndio downstream tasks. Previous work typically only uses\ndatasets in a single domain for pretraining.\n2 Self-Supervised Audio Spectrogram\nTransformer\nIn this section, we first review the AST architecture and\nthen discuss the proposed joint discriminative and gener-\native masked spectrogram patch prediction (MSPM) self-\nsupervised learning framework, and the design details.\n2.1 AST Model Architecture\nAs shown in Figure 1, we intentionally follow as close as\npossible to the original AST architecture to make a fair per-\nformance comparison. First, the input audio waveform of t\nseconds is converted into a sequence of 128-dimensional log\nMel filterbank (fbank) features computed with a 25ms Han-\nning window every 10ms. This results in a128 ×100t spec-\ntrogram as input to the AST. We then split the spectrogram\ninto a sequence of 16 ×16 patches. We flatten each 16 ×16\npatch to a 1D 768-dimensional patch embedding with a lin-\near projection layer. We refer to this linear projection layer\nas the patch embedding layer and the output as patch embed-\nding E. Since the Transformer architecture does not capture\nthe input order information and the patch sequence is also\nnot in temporal order, we add a trainable positional embed-\nding (also of size 768) P to each patch embedding to al-\nlow the model to capture the spatial structure of the 2D au-\ndio spectrogram. The resulting sequence is then input to the\nTransformer. A Transformer consists of several encoder and\nTransformer Encoder\nLinear ProjectionE[1]E[2]E[3]E[4]E[5]E[6]E[7]E[8]\nP[1]P[2]P[3]P[4]P[5]P[6]P[7]P[8]\nPatch SplitInput Spectrogram\n1 2 3 456 78\n1 2 3 4 5 6 7 8\nO[1]O[2]O[3]O[4]O[5]O[6]O[7]O[8]\n1 2 5 6Random Masking83\n7\n4\nClassification HeadReconstruct Head\n7\n4\n7\n4SSLPretrain\nMeanPoolingLinear HeadAudio EventSpeech CommandSpeaker ID … Fine-Tune\nMASK\nMASK\nMASK\nMASK\nReconstruct HeadClassification Head\nFigure 1: The proposed self-supervised AST. The 2D au-\ndio spectrogram is split into a sequence of 16 ×16 patches\nwithout overlap, and then linearly projected to a sequence of\n1-D patch embeddings E. Each patch embedding is added\nwith a learnable positional embedding P and then input to\nthe Transformer encoder. The output of the Transformer O\nis used as the spectrogram patch representation. During self-\nsupervised pretraining, we randomly mask a portion of spec-\ntrogram patches and ask the model to 1) find the correct\npatch at each masked position from all masked patches; and\n2) reconstruct the masked patch. The two pretext tasks aim\nto force the AST model to learn both the temporal and fre-\nquency structure of the audio data. During fine-tuning, we\napply a mean pooling over all patch representation {O}and\nuse a linear head for classification.\ndecoder layers. Since the AST is designed for classification\ntasks, we only use the encoder of the Transformer that has\nan embedding dimension of 768, 12 layers, and 12 heads,\nwhich are the same as those in original AST (Gong, Chung,\nand Glass 2021). We refer to the output of the Transformer\nencoder as patch representation O. During fine-tuning and\ninference, we apply a mean pooling over the sequence of\npatch representation {O}to get the audio clip level repre-\nsentation, and then use a linear head for classification.\nWhile we aim to follow the architecture of the original\nAST, we made two modifications for self-supervised learn-\ning. First, in the original AST, a[CLS] token is appended to\n10700\nFrequency\nTime\nOriginal\nSpectrogram\nFrame-level \nMasking\nPatch-level \nMasking\nC = 1\nPatch-level \nMasking\nC = 3\nPatch-level \nMasking\nC = 6\nFigure 2: Illustration of the proposed patch-level masking\nwith different cluster factor C but same total masking area.\nThe model is forced to learn more global spectrogram struc-\nture with a larger C, and more local structure with a smaller\nC. To make the model learn both local and global structure,\nwe use random C during pretraining. Compared with frame-\nlevel masking SSL methods that potentially only learn tem-\nporal frame structure, patch-based masking allows the model\nto learn both temporal and frequency spectrogram structure.\nthe beginning of the input sequence of the Transformer en-\ncoder, and the output representation of the [CLS] token is\nused as the audio clip level representation. In this work, we\napply mean pooling over all patch representation{O}as the\naudio clip level representation. This is because the original\nAST uses supervised pretraining and the supervision is ap-\nplied on the [CLS] token, thus the output representation of\nthe [CLS] learns to summarize the entire sequence during\npretraining and can be used as audio clip level representa-\ntion. In contrast, for our self-supervised pretraining frame-\nwork, supervision is applied to each individual patch repre-\nsentation, and the mean of all patch representations is a bet-\nter summary of the audio clip. Second, in the original AST,\nspectrogram patches are split with overlap, and the overlap\nwas shown to improve model performance. In this work, we\nsplit the patch without overlap during pretraining to not al-\nlow the model to use overlapped edges as a shortcut for the\ntask prediction instead of learning a meaningful representa-\ntion. In the fine-tuning and inference steps, we split the patch\nwith an overlap of 6 in the same fashion as the original AST.\nWhile we pretrain the model using fixed-length audio data\n(10 seconds), AST supports variable length input by simply\ninterpolating or truncating the positional embedding to the\ndownstream task audio length.\n2.2 Joint Discriminative and Generative Masked\nSpectrogram Patch Modeling\nIn this section, we introduce the proposed self-supervised\npretraining framework. We first show our masking strategy\nand then discuss the pretext task (i.e., the self-supervised\nlearning task in the pretraining stage) in detail.\nMasked Patch Sampling As mentioned above, during\npretraining, we use a fixed-length audio of 10s and convert\nit to spectrogram of size 1024×128. AST splits the spectro-\nAlgorithm 1: Joint Discriminative and Generative Masked\nSpectrogram Patch Modeling\nRequire:\nUnlabeled Audio Dataset D, AST Model M\nSampleMaskIndex (N, C)\n▷ Randomly sample patches to mask\nInput: #Masked Patches N; Cluster Factor C\nOutput: Masked Patch Position Index Set I\n1: while |I|< Ndo\n2: draw index i ∼unif{1, 512}\n3: get set Ic = [C2-1 indexes neighboring i, i]\n4: I = I ∪Ic\n5: I = I[1 :N] ▷ Guarantee to mask exactly N patches\nreturn I\nMSPM (D, M)\nInput: D, M, Number of Masked Patches N\n6: for every epoch do\n7: for X ∈D do\n8: split X into 512 patches x = {x1, x2, ..., x512}\n9: E = Mpatchembedding(x)\n10: draw C ∼unif{3, 5}\n11: I = SampleMaskIndex(C, N)\n12: EI = Emask ▷ Mask the Patch Embeddings\n13: O = Mtransformer (E + P)\n14: Ld = 0,Lg = 0\n15: for i ∈I do\n16: ri = Mreconstruction head(Oi)\n17: ci = Mclassification head(Oi)\n18: L+= Ld(xi, ci, xI) +λLg(xi, ri)\n19: L= L/ N\n20: update Mto minimize L\nreturn M\ngram into 512 16×16 patches (8 in the frequency dimension\nand 64 in the time dimension). Thanks to this special design\nof AST, we are able to mask spectrogram patches rather than\nthe entire time frames during pretraining, which allows the\nmodel to learn both the temporal and frequency structure of\nthe data. In addition, as shown in Figure 2, we use a cluster\nfactor C to control how masked patches cluster. Specifically,\nwe first randomly select a patch, and then mask the square\ncentered at the patch with a side length of C, e.g., if C = 3,\nwe mask a cluster of 9 patches that has a total size of48×48.\nThe model is forced to learn more global spectrogram struc-\nture with a larger C, and more local structure with a smaller\nC. To make the model learn both local and global structure,\nwe use random C ∼[3, 5] during pretraining. We show the\ndetails in Algorithm 1 line 1-5. Note that while we mainly\nfocus on using 16×16 patches in this paper, MSPM actually\nsupports patches of arbitrary size and shape.\nJoint Discriminative and Generative Masked Spectro-\ngram Patch Modeling As opposed to prior work that ei-\nther used discriminative (e.g., wav2vec) or generative train-\ning objectives (e.g., APC), in this work, we propose to use a\njoint discriminative and generative objective for pretraining.\n10701\nAs shown in Algorithm 1, each input spectrogram X is\nsplit into 512 patches x converted to corresponding patch\nembeddings E (line 8-9). We then randomly generate a setI\nof N masked patch position indexes as previously described\n(line 10-11). For each patch that needs to be masked, we re-\nplace its patch embedding with a learnable mask embedding\nEmask (line 12). We add positional embeddings to the patch\nembeddings and input them to the Transformer encoder (line\n13). For each masked patch xi, we get the corresponding\nTransformer encoder output Oi. We then input Oi to a clas-\nsification head and a reconstruction head and get output ci\nand ri, respectively (line 16-17). Both the classification and\nreconstruction heads are two-layer MLPs that map Oi (768)\nto the same dimension as xi (256). We expect ri to be close\nto xi, and the model can match correct (xi, ci) pairs. There-\nfore, we use the InfoNCE loss (Oord, Li, and Vinyals 2018)\nLd for the discriminative objective and mean square error\n(MSE) loss Lg for the generative objective:\nLd = −1\nN\nNX\ni=1\nlog( exp(cT\ni xi)PN\nj=1 exp(cT\ni xj)\n) (1)\nLg = 1\nN\nNX\ni=1\n(ri −xi)2 (2)\nWhere N is the number of masked patches. We then sum\nup Ld and Lg with a weight λ. In this work, we set λ = 10.\nL= Ld + λLg (3)\nFinally, we update the weights of the AST model Mto\nminimize Lwith the optimizer (line 19-20). Note that for\nthe discriminative task, the negative samples are sampled\nfrom the same spectrogram, i.e., the model aims to pick the\ncorrect patch for each masked position from all patches be-\ning masked. On one hand, this increases the difficulty of the\npretext task to avoid the model learning trivial things such\nas recording environment for prediction; on the other hand,\nthis also avoids building a memory bank of patches from dif-\nferent spectrograms and makes the algorithm less computa-\ntionally intensive and less affected by the mini-batch size.\n3 Experiments\n3.1 Pretraining Datasets\nIn contrast to previous efforts that only use either\nspeech dataset (e.g., in APC, wav2vec) or audio event\ndataset (Saeed, Grangier, and Zeghidour 2021; Niizumi et al.\n2021), in this work, we propose to use both speech and au-\ndio event datasets for pretraining to explore if the pretrained\nmodel can generalized to both speech and audio classifica-\ntion tasks. For both datasets, we only use the audio data and\nabandon the labels for self-supervised pretraining.\nAudioSet-2M We use the AudioSet full training set\n(AudioSet-2M) (Gemmeke et al. 2017) as our audio pre-\ntraining dataset. AudioSet is a multi-label audio event classi-\nfication dataset that contains 2 million 10-second audio clips\nFigure 3: Prediction accuracy (upper) and reconstruction\nMSE (lower) of the masked patch modeling pretext tasks.\nWe pretrain three AST models with a fixed number of 100,\n250, and 400 masked patches, respectively, and evaluate\ntheir classification and reconstruction performance with var-\nious masked patch numbers from 50 to 500 on the validation\nset. While the AST model is pretrained with a fixed number\nof masked patches, we find it can perform well with a dif-\nferent number of masked patches in inference. As expected,\nthe performance of the model drops with the increase of the\nnumber of masked patches, e.g., the AST models achieve\nover 80% accuracy when the evaluation masked patch num-\nber is 50, but only around 35% when the evaluation masked\npatch number is 400. This indicates the pretext tasks are nei-\nther trivial nor impossible.\nexcised from YouTube videos with 527 sound classes in-\ncluding human sounds, animal sounds, sounds of things, mu-\nsic, natural sounds, environment sounds etc. It is worth men-\ntioning that while about half of AudioSet-2M audio clips\ncontain speech, speech might only appear in a small part\nof each clip as most AudioSet clips contain more than one\nsound. Therefore, AudioSet potentially does not have good\ncoverage of speech and might not be sufficient to pretrain a\ngood model for downstream speech tasks.\nLibrispeech In order to improve the coverage of speech\ndata, we further use the Librispeech (Panayotov et al. 2015)\n960-hour training set as our speech pretraining dataset. Lib-\nrispeech contains public domain audio books data in En-\nglish, read by over 1,000 speakers, and is commonly used\nto train and evaluate speech recognition systems.\nFor both AudioSet and Librispeech data, we cut or pad\neach waveform to 10sec. We use 1,953k AudioSet samples\nand 281k Librispeech samples, and a total of 2,234k sam-\nples. We mix and shuffle the two datasets during pretraining.\n10702\n3.2 Performance of Pretext Tasks\nFor pretraining the AST, we use a batch size of 24, an initial\nlearning rate of 1e-4, and cut the learning rate into half if\nthe pretext task performance on the validation set stops im-\nproving for 8k iterations. We optimize the network using the\nAdam optimizer (Kingma and Ba 2015). We train the model\nfor up to 800k iterations (∼8.5 epochs). We tested different\nnumbers of masked patches of 100, 250, and 400. We pre-\ntrain SSAST on 4×NVIDIA GTX Titan X or GTX Titan X\nPascal GPUs, the pretraining takes about 10 days.\nWe show the masked spectrogram patch modeling perfor-\nmance in Figure 3. While the AST model is pretrained with\na fixed number of masked patches, we find it can perform\nwell with a different number of masked patches during in-\nference. As expected, the performance of the model drops\nwith the increase of the number of masked patches, e.g., the\nAST models achieve over 80% accuracy when the evalua-\ntion masked patch number is 50, but only around 35% when\nthe evaluation masked patch number is 400, indicating the\npretext tasks are neither trivial nor impossible. In general,\nthe model pretrained with more masked patches performs\nbetter on the pretext tasks.\n3.3 Downstream Tasks and Datasets\nWe evaluate the pretrained model on 6 commonly used au-\ndio and speech benchmarks. We use the same three bench-\nmarks (AudioSet-20K, ESC-50, and Speech Commands V2)\nthat the original AST has been tested on and use exactly the\nsame setting intentionally to make a fair comparison. To fur-\nther evaluate the model performance on downstream speech\ntasks and compare with previous self-supervised models that\nfocus on speech, we test the pretrained AST on three addi-\ntional benchmark Speech Commands V1, V oxCeleb 1, and\nIEMOCAP for keyword spotting, speaker identification, and\nemotion recognition, respectively. We report mean Average\nPrecision (mAP) for the AudioSet-20K task and accuracy\nfor all other tasks.\nAudioSet-20K (AS) We use the AudioSet balanced train-\ning set and evaluation set for the multi-label audio event\nclassification task. The AudioSet-20K training set is a class-\nbalanced subset of AudioSet-2M that contains 20,785 au-\ndios. We test the model on the AudioSet evaluation set,\nwhich is disjoint with AudioSet-20K and AudioSet-2M.\nESC-50 (ESC) We use the ESC-50 dataset (Piczak 2015)\nfor the single-label audio event classification task. ESC-50\nis an audio classification dataset consists of 2,000 5-second\nenvironmental audio recordings organized into 50 classes.\nSpeech Commands V2 (KS2) We use the Speech Com-\nmands V2 (Warden 2018) for the keyword spotting task. The\nSpeech Command V2 dataset consists of 105,829 1-second\nrecordings of 35 common speech commands.\nSpeech Commands V1 (KS1) We also use the Speech\nCommands V1 (Warden 2018) for the keyword spotting\ntask, which is similar to Speech Commands V2, but only\ncontains 10 classes of keywords, 1 class of silence, and an\nunknown class to include the false positive.\nVoxCeleb 1 (SID) We use the V oxCeleb 1 dataset (Na-\ngrani et al. 2020) that contains speech from 1,251 speakers\nfor the speaker identification task. The task goal is to classify\neach utterance by its speaker identity where speakers are in\nthe same predefined set for both training and testing.\nIEMOCAP (ER) We use the IEMOCAP dataset (Busso\net al. 2008) that contains about 12 hours of emotional speech\nfor the speech based emotion recognition task.\n3.4 Downstream Fine-tuning Details\nTo make a fair comparison with previous work, for the\nAudioSet-20K, ESC-50, and Speech Commands V2 exper-\niments, we train and evaluate the model using the exact\nsame training and evaluation settings with the original AST.\nSpecifically, we use mixup training (Tokozume, Ushiku, and\nHarada 2018), SpecAugment (Park et al. 2019), an initial\nlearning rate of 5e-5, 1e-4, and 2.5e-4 and train the model\nwith 25, 50, and 30 epochs for AudioSet-20K, ESC-50,\nSpeech Commands V2, respectively.\nFor the three benchmarks Speech Commands V1, V ox-\nCeleb1, and IEMOCAP that the original AST has not been\ntested on, we use the standard SUPERB (Yang et al. 2021)\ntraining and testing framework. Specifically, we search the\nlearning rate from 1e-5 to 1e-3 for out SSAST model and all\nbaseline models and train the model for up to 20k and 40k\niterations for Speech Commands V1 and V ocCeleb1, respec-\ntively. We use a fixed learning rate of 1e-4 and max iteration\nof 10k for IEMOCAP. Please refer to the AST and SUPERB\npapers for more details. For all downstream experiments, we\nuse the end-to-end fine-tuning setting, i.e., we do not freeze\nany layer of the pretrained AST.\nFor supervised pretrained models, we use the output of\n[CLS] token as the audio clip representation because super-\nvision is given to the output of [CLS] in pretraining while\nwe use mean pooling for self-supervised models as super-\nvision is given to individual token in pretraining, keeping\npretraining and fine-tuning consistent can slightly improve\nthe performance and make the comparison fairer.\n3.5 Performance on Downstream Tasks\nWe compare the following models in our experiments:\n1. AST-Scratch: AST model with appropriate initialization\nbut without any pretraining.\n2. AST-IM+KD: AST model with supervised ImageNet\npretraining, proposed in (Gong, Chung, and Glass 2021).\nThe model is pretrained with the ImageNet 2012 dataset\nin a supervised manner. In addition, during ImageNet\npretraining, knowledge distillation from another convo-\nlution neural network is applied, which can noticeably\nimprove the performance (Touvron et al. 2020). This is\na strong baseline that achieves state-of-the-art results on\nAudioSet-20K, ESC-50, and Speech Commands V2.\n3. AST-AudioSet: AST model with supervised AudioSet-\n2M pretraining on the audio event classification task.\n4. SSAST 250: The proposed self-supervised AST model\npretrained with 250 masked patches.\n10703\nModel Task\nAS ESC KS2 KS1 SID ER\nAST-Scratch 14.8 41.9 92.6 87.2 30.1 51.9\nSupervised Pretraining Baselines\nAST-IM + KD 34.7 88.7 98.1 95.5 41.1 56.0\nAST-AudioSet 28.6 86.8 96.2 91.6 35.2 51.9\nProposed Self-Supervised AST\nSSAST 250 30.4 86.7 98.1 96.2 66.6 57.1\nSSAST 400 31.0 88.8 98.0 96.0 64.2 59.6\nTable 1: Comparison of self-supervised AST with baseline\nmodels on various benchmarks.\nFigure 4: Comparing learning curves of AST trained from\nscratch and self-supervised AST on the AudioSet-20K task.\nThe self-supervised framework helps AST train faster and\nbetter. Using a different learning rates, or increasing training\nepochs does not improve the AST-scratch performance.\n5. SSAST 400: The proposed self-supervised AST model\npretrained with 400 masked patches.\nAs shown in Table 1, we evaluate the above-mentioned\n7 models on 6 benchmarks. Key findings include: First,\nthe proposed self-supervised training framework can signif-\nicantly boost the performance of AST with an average im-\nprovement of 60.9%, e.g., SSAST achieves 0.310 mAP on\nthe AudioSet-20K while AST-Scratch only achieves 0.148\nmAP. As shown in Figure 4, the proposed self-supervised\nframework helps AST train faster and better. Further, the\nimprovement is consistent over all audio and speech bench-\nmarks, demonstrating the proposed self-supervised training\nframework is effective and generalizable. Second, AudioSet-\n2M supervised pretraining is quite strong for audio event\nclassification tasks (AS and ESC) that are in the same do-\nmain with AudioSet, but performs poorly on speech tasks,\nshowing the limitation of supervised pretraining. Surpris-\ningly, cross-domain supervised ImageNet pretraining with\nknowledge distillation performs quite well on all tasks, and\nstill achieves the best performance on the AudioSet-20K\ntask. Third, even when compared with strong supervised\nbaselines, the proposed SSAST models still get the best re-\nSetting Task\nAS ESC KS2 KS1 SID ER\nFrom Scratch 14.8 41.9 92.6 87.2 30.1 51.9\n# Masked Patches\n100 28.7 85.3 98.0 94.9 62.1 57.3\n250 30.4 86.7 98.1 96.2 66.6 57.1\n400 (Default) 31.0 88.8 98.0 96.0 64.3 59.6\nPretext Task\nDiscriminative 30.6 85.6 98.0 94.2 61.4 57.5\nGenerative 16.1 74.2 96.6 93.3 40.1 54.3\nJoint (Default) 31.0 88.8 98.0 96.0 64.3 59.6\nPretraining Data\nAudioSet-20K 25.7 82.2 97.6 93.8 43.8 55.4\nAudioSet 2M 29.0 84.7 97.8 94.8 57.1 56.8\nAudioSet 2M\nSupervised 28.6 86.8 96.2 91.6 35.2 51.9\nLibrispeech 22.9 80.0 97.8 95.6 60.8 58.3\nJoint (Default) 31.0 88.8 98.0 96.0 64.3 59.6\nTable 2: Ablation study on the impact of number of masked\npatches, pretext task, and pretraining data.\nsults on all benchmarks except AS, showing the proposed\nself-supervised model potentially can be used as a powerful\ngeneric audio classifier.\n3.6 Performance Impact of Pretraining Settings\nWe set the AST pretrained with 400 masked patches, joint\ndiscriminative and generative objectives, on both AudioSet-\n2M and Librispeech as the base model. We then change one\nfactor at a time to observe the performance impact.\nImpact of the Number of Masked PatchesAs shown in\nTable 2, upper section, we find masking 100 patches is too\nsimple a task, and leads to the worst performance for all\ndownstream tasks. Masking 400 patches leads to better per-\nformance on audio event classification tasks, while masking\n250 patches leads to better performance on speech tasks, but\nthe overall performance is similar.\nImpact of Pretext Tasks As shown in Table 2, mid-\ndle section, we find that a discriminative objective leads\nto better performance than the generative objective for all\ntasks, but joint discriminative and generative objective al-\nways achieves the best performance, indicating that the dis-\ncriminative and generative objectives are complementary.\nImpact of Pretraining Data We pretrain the AST model\nusing 1) AudioSet-20K, 2) AudioSet-2M only, 3) Lib-\nrispeech only, and 4) both AudioSet-2M and Librispeech,\nand compare the performance of the pretrained models on\nthe downstream tasks. As shown in Table 2, bottom sec-\ntion, we have the following key findings: First, increasing\nthe pretraining data volume improves the performance of\ndownstream tasks, e.g., AudioSet-2M pretrained model al-\nways outperforms AudioSet-20K pretrained model, but the\n10704\nFigure 5: Performance correlation between pretraining tasks\nand downstream tasks (upper: audio classification tasks,\nlower: speech tasks). We save the checkpoint models at iter-\nation 20, 40, 80, 200, 400, and 600 during pretraining, then\nfine-tune and evaluate these checkpoint models on the down-\nstream tasks. For better visualization, we normalize the per-\nformance of each task in the range[0, 1]. We observe that the\nmodel pretrained with more iterations generally performs\nbetter on downstream tasks, which further confirms that the\npretraining pretext tasks can benefit all downstream tasks.\nproposed self-supervised framework can still noticeably im-\nprove the AST model with limited pretraining data, e.g.,\nwhen pretrained and fine-tuned on the same AudioSet-20K\ndata, the proposed SSAST model achieves 0.257 mAP, and\nsignificantly outperforms the AST-Scratch model. Second,\nwith the same AudioSet-2M pretraining data, the proposed\nself-supervised framework leads to similar or even better re-\nsults compared with the supervised pretraining method, par-\nticularly for the speech tasks, showing that the proposed self-\nsupervised framework is more generalizable. Third, as ex-\npected, a model pretrained with AudioSet-2M is better for\naudio classification and a model pretrained with Librispeech\nis better for speech tasks, but training with both sets always\nleads to the best results, showing that it is beneficial to com-\nbine pretraining datasets in audio and speech domains.\nPerformance Correlation between Pretraining and\nDownstream Tasks We save the checkpoint models at it-\neration 20, 40, 80, 200, 400, and 600 during pretraining, then\nfine-tune and evaluate these checkpoint models on the down-\nstream tasks. We observe the performance of pretraining\ntasks and downstream tasks are highly correlated, i.e., the\nmodel pretrained with more iterations generally performs\nbetter on downstream tasks, which further confirms that the\npretraining pretext tasks benefit all downstream tasks.\nModel Task\nAS ESC KS2 KS1 SID ER\nTiny-Scratch 15.1 34.8 92.4 87.7 24.2 50.8\nTiny-SSAST 27.1 ∗ 79.5 97.2 94.8 55.1 55.7\nSmall-Scratch 16.5 37.8 93.3 87.4 23.8 51.2\nSmall-SSAST 30.8 ∗ 85.4 97.7 95.4 60.9 58.7\nBase-Scratch 14.8 41.9 92.6 87.2 30.1 51.9\nBase-SSAST 31.0 88.8 98.0 96.0 64.2 59.6\nTable 3: Comparison of AST model of different sizes (∗ use\nlarger learning rate for the last linear classification layer).\n3.7 Performance Impact of AST Model Size\nIn all previous experiments, we use the original AST (Gong,\nChung, and Glass 2021) architecture to make a direct perfor-\nmance comparison. We refer to this model as the base AST\nmodel. In this section, we further test the following AST ar-\nchitectures to study the impact of model size.\n1. Tiny Model: The Transformer encoder has 12 layers\nwith 3 attention heads and an embedding dimension of\n192. The tiny model has 6M parameters.\n2. Small Model: The Transformer encoder has 12 layers\nwith 6 attention heads and an embedding dimension of\n384. The small model has 23M parameters.\n3. Base Model: The model described in Section 2.1 that\nis used as the default model throughout the paper. The\nTransformer encoder has 12 layers with 12 attention\nheads and an embedding dimension of 768. The base\nmodel has 89M parameters.\nFor each model architecture, we compare the performance\nof the from-scratch model and the self-supervised pretrained\nSSAST model (pretrained with 400 masked patches) and\nshow the results in Table 3. Key findings are as follows:\nFirst, the MSPM self-supervised pretraining consistently\nenhances the performance of all three model architectures,\nshowing that MSPM is model size agnostic. Small models\nthat are unlikely to be over-parameterized also get perfor-\nmance improvement with MSPM pretraining.\nSecond, when trained from scratch, the larger AST model\ndoes not always get the best performance, e.g., the small\nAST model outperforms the base AST model on AS, KS1,\nand KS2 tasks. This is as expected since larger models are\nharder to train with limited data. However, we find that with\nMSPM self-supervised pretraining, larger AST models al-\nways perform better, demonstrating that MSPM can unlock\nthe potential of models with higher capacity. This also sug-\ngests that further scaling up the base AST model can poten-\ntially achieve even better performance.\nWe also observe that using a larger learning rate for\nthe last linear layer during fine-tuning improves the perfor-\nmance for tiny and small SSAST models on the AS task,\ne.g., for small SSAST model, using a learning rate of 5e-3\nfor the last linear layer and 5e-5 for all other layers leads\nto an mAP of 0.308 while using a learning rate of 5e-5 for\n10705\nthe entire model leads to an mAP of only 0.272. Neverthe-\nless, we find this trick is only useful for tiny and small self-\nsupervised pretrained models for some downstream tasks, it\ndoes not improve the performance of from-scratch models.\n3.8 Comparing Patch-based and Frame-based\nAST\nIn all previous experiments, we follow the original\nAST (Gong, Chung, and Glass 2021) to split the audio spec-\ntrogram into 16 ×16 square patches. In (Gong, Chung, and\nGlass 2021), it was found that splitting the spectrogram into\nframe-like rectangle patches in the temporal order leads to\nbetter performance when the model is trained from scratch.\nHowever, ImageNet supervised pretrained model performs\nsignificantly better than the from-scratch model, which also\nconstrains the original AST to use square patches. In con-\ntrast, our proposed MSPM self-supervised pretraining sup-\nports any patch size and shape including a conventional\nframe. As discussed in Section 2, heuristically, square patch\nbased pretraining could capture correlation in frequency\nbands in addition to time frames, which is potentially useful\nwhen the input has a complex frequency structure (e.g., natu-\nral sounds). For clarity, we refer to the AST model that uses\nsquare patches and frame-like rectangle patches as patch-\nbased AST model and frame-based AST model, respec-\ntively. In this section, we compare patch-based and frame-\nbased AST models in both from-scratch setting and self-\nsupervised pretraining setting. Specifically, the two models\nhave exactly the same architecture except the patch split-\nting layer, for the patch-based AST model, we use 16 ×16\npatches as described in Section 2; for the frame-based AST\nmodel, instead of splitting the spectrogram into 16 ×16\npatches, we split the spectrogram into 128 ×2 patches in\nthe temporal order (128 is the number of frequency bins of\nthe spectrogram). Patches are split without overlap during\npretraining and are split with an overlap of 1 on the time\ndimension during fine-tuning. This makes a fair compari-\nson as the area of the patch is the same and the number of\npatches after splitting is similar. In the pretraining setting,\nboth models are pretrained using the method described in\nSection 2. The only pretraining setting difference is that we\ndo not cluster the masked frames for frame-based AST be-\ncause this would lower the pretext and downstream task per-\nformance, instead, we just random sample the masked frame\nfor frame-based AST pretraining. We test models pretrained\nwith 250 and 400 masked patches (frames) and show the re-\nsults in Table 4. Key findings are as follows:\nFirst, when trained from scratch, frame-based AST al-\nways performs better than patch-based AST (except ER),\nwhich is consistent with the finding in (Gong, Chung, and\nGlass 2021) and as expected because 1-D temporal struc-\nture is easier to learn than 2-D temporal-frequency structure.\nSecond, after MSPM self-supervised pretraining, frame-\nbased AST still outperforms patch-based AST on speech\ntasks (KS1, KS2, SID, and ER) but the advantage becomes\nmuch smaller. Patch-based AST performs better on audio\ntasks (AS and ESC). MSPM significantly improves the per-\nformance of both patch-based and frame-based AST, but the\nimprovement is noticeably larger for patch-based AST (ex-\nModel Task\nAS ESC KS2 KS1 SID ER\nFrame-Scratch 16.6 53.7 96.0 91.7 54.9 51.2\nPatch-Scratch 14.8 41.9 92.6 87.2 30.1 51.9\nSSAST-Frame-250 27.1 84.0 98.0 96.6 73.6 58.3\nSSAST-Patch-250 30.4 86.7 98.1 96.2 66.6 57.1\nSSAST-Frame-400 29.2 85.9 98.1 96.7 80.8 60.5\nSSAST-Patch-400 31.0 88.8 98.0 96.0 64.2 59.6\nFrame-Improvement 12.6 32.2 2.1 5.0 25.9 9.3\nPatch-Improvement 16.2 46.9 5.4 8.8 34.1 7.7\nTable 4: Comparison of frame and patch based AST models.\nModel Task\nKS1 SID ER\nAPC (Chung et al. 2019) 94.0 60.4 59.3\nWav2vec (Schneider et al. 2019) 96.2 56.6 59.8\nWav2vec 2.0 (Baevski et al. 2020)∗ 96.2 75.2 63.4\nHuBERT (Hsu et al. 2021)∗ 96.3 81.4 64.9\nSSAST-Patch (Librispeech Only) 95.6 60.8 58.3\nSSAST-Patch 96.0 64.3 59.6\nSSAST-Frame 96.7 80.8 60.5\nTable 5: Comparison of SSAST and existing speech self-\nsupervised pretraining frameworks (∗frozen setting results).\ncept ER), which verifies our hypothesis that square patch\nbased pretraining can be more effective, particularly for\ndata that has a complex frequency structure such as natu-\nral sounds. Our experiment also demonstrates that MPSM\nis patch shape agnostic, it also works well with frame-\nbased AST and makes frame-based SSAST a strong model\nfor speech tasks. In contrast, previous ImageNet pretraining\nonly supports square patches.\n3.9 Comparing with Existing Speech\nSelf-supervised Pretraining Frameworks\nFinally, we compare the performance of SSAST with ex-\nisting speech self-supervised pretraining frameworks. Since\nthese frameworks are designed for speech tasks and are\nonly pretrained on speech datasets, we only compare with\nthem on the speech benchmarks. Specifically, we compare\nthree SSAST models with previous models: 1) SSAST-\nPatch (Librispeech): Patch-based SSAST model pretrained\non only Librispeech (same pretraining data with previous\nspeech self-supervised models); 2) SSAST-Patch: Patch-\nbased SSAST model pretrained on both AudioSet and Lib-\nrispeech; and 3) SSAST-Frame SSAST model described in\nSection 3.8 that uses frame-like patches and is pretrained on\nboth AudioSet and Librispeech.\nComparing with APC and wav2vec 1.0 We first com-\npare SSAST models with autoregressive predictive coding\n10706\n(APC) (Chung et al. 2019), a generative pretraining frame-\nwork, and wav2vec 1.0 (Schneider et al. 2019), a discrimina-\ntive pretraining framework. We evaluate APC and wav2vec\n1.0 in both fine-tuned and frozen settings and report the best\nresult. As shown in Table 5, SSAST models match or out-\nperform APC and wav2vec 1.0 on all three benchmarks.\nComparing with wav2vec 2.0 and HuBERT We then\ncompare SSAST models with the state-of-the-art wav2vec\n2.0 (Baevski et al. 2020) and HuBERT (Hsu et al. 2021)\nmodels. Specifically, we compare the base model that is pre-\ntrained on Librispeech 960 dataset. Due to the complexity\nof finding optimal hyperparameters and the large compu-\ntation cost for fine-tuning these two models, we only re-\nport the results in the frozen setting. As shown in Table 5,\nfrozen wav2vec and HuBERT can already match or out-\nperform fine-tuned SSAST for speech tasks. Nevertheless,\nit is worth noting that although wav2vec 2.0 and HuBERT\nperform better, they are pre-trained with 64/32 GPUs and\nhence have larger batch sizes than our SSAST that is trained\nwith 4 GPUs. The computational resource difference could\ngreatly impact the performance, e.g., for HuBERT, using 8\nGPUs leads to 40% WER while 32 GPUs leads to below\n20% WER. With more computational resources and larger\nbatch size, SSAST potentially can achieve better results.\n4 Related Work\nPure Transformer Based ModelsSelf-attention models,\nespecially the Transformer (Vaswani et al. 2017), have been\nwidely used in natural language processing. Recently, pure\nTransformer models, e.g., Vision Transformer (Dosovitskiy\net al. 2021; Touvron et al. 2020; Yuan et al. 2021) and Audio\nSpectrogram Transformer (Gong, Chung, and Glass 2021),\nare found to outperform CNN based models for vision tasks\nand audio classification. Such models differ from CNN mod-\nels or CNN-Attention hybrid models in that they do not\ncontain non-degenerated convolutions (Chen, Xie, and He\n2021) and have less inductive bias such as spatial locality\nand translation equivariance. However, it is found that such\npure Transformer models require a lot of training data to\nperform well (Dosovitskiy et al. 2021).\nSelf-Supervised Learning In the vision domain, self-\nsupervised Vision Transformer has been studied in (Caron\net al. 2021; Chen, Xie, and He 2021; Atito, Awais, and Kit-\ntler 2021). In addition, patch based self-supervised frame-\nwork has been extensively studied in the vision domain, e.g.,\nin (Noroozi and Favaro 2016; Trinh, Luong, and Le 2019;\nBao, Dong, and Wei 2021). However, to the best of our\nknowledge, the self-supervised Audio Spectrogram Trans-\nformer and patch based self-supervised learning framework\nhas not been studied in the audio and speech domain. Pre-\nvious self-supervised learning frameworks in the speech do-\nmain are mainly based on CNN, RNN, or CNN-Transformer\nhybrid models with the pretext task of predicting past, cur-\nrent, or future frames (Chung et al. 2019; Oord, Li, and\nVinyals 2018; Liu et al. 2020; Schneider et al. 2019). In\ncontrast, the proposed MSPM framework allows the model\nto learn both the temporal and frequency structure of the\nspectrogram. Further, most previous research only focuses\non learning either a speech or audio representation, only a\nfew efforts (Saeed, Grangier, and Zeghidour 2021; Niizumi\net al. 2021) studied learning a general audio and speech rep-\nresentation. However, both efforts pretrain the model with\nonly AudioSet. In contrast, we explore pretraining the AST\nmodel with both AudioSet and Librispeech. Finally, we pre-\ntrain the model with joint discriminative and generative ob-\njectives, which is also novel in the audio and speech domain\nand only has been explored in (Pascual et al. 2019; Jiang\net al. 2020; Ravanelli et al. 2020).\n5 Conclusion\nThis paper aims to reduce the need for large amounts of la-\nbeled data for the AST self-attention based audio and speech\nclassification model by leveraging self-supervised learning.\nWe propose MSPM, a novel patch-based joint discriminative\nand generative pretraining framework. In order to make the\npretrained model generalize to both audio and speech tasks,\nwe pretrain AST using both AudioSet and Librispeech, and\nevaluate on six downstream benchmarks including audio\nevent classification, keyword spotting, speaker identifica-\ntion, and emotion recognition.\nWith extensive experiments, we observe the following\nkey findings. First, the proposed MSPM self-supervised pre-\ntraining framework significantly improves the performance\nof AST for all downstream tasks with an average improve-\nment of 60.9%. Our SSAST model can match or even out-\nperform previous supervised pretrained models and shows\nbetter generalization capability, indicating that the proposed\nMSPM can replace supervised pretraining that requires a\nlarge amount of labeled data. Second, we find that pretrain-\ning the model with both generative and discriminative ob-\njectives leads to a better performance than using a single ob-\njective, similarly, pretraining the model on both speech and\naudio datasets leads to better performance than using data\nfrom a single domain. Third, the flexibility of MSPM on\npatch shape allows us to explore frame-based AST. We find\nthat frame-based AST always outperforms patch-based AST\nin the from-scratch setting, but patch-based pretraining leads\nto a larger improvement from the random-initialized models.\nAfter MSPM pretraining, the patch-based AST wins on the\naudio tasks while the frame-based AST wins on the speech\ntasks. We plan to investigate the reason for this difference in\nour future work. Finally, we find MSPM allows us to scale\nup the AST model, with MSPM pretraining, larger AST al-\nways performs better. In contrast, in the from-scratch setting,\nscaling up the model may cause a performance drop. Nev-\nertheless, the current version of SSAST is pretrained with a\nsmall batch size due to computational resource limitations.\nIn the future, we plan to further investigate the scaling law\nof AST.\nAcknowledgments\nWe thank the anonymous reviewers for their insightful com-\nments and suggestions. This work is partly supported by Sig-\nnify.\n10707\nReferences\nAtito, S.; Awais, M.; and Kittler, J. 2021. Sit:\nSelf-supervised vision transformer. arXiv preprint\narXiv:2104.03602.\nBaevski, A.; Zhou, Y .; Mohamed, A.; and Auli, M. 2020.\nwav2vec 2.0: A Framework for Self-Supervised Learning of\nSpeech Representations. NeurIPS.\nBao, H.; Dong, L.; and Wei, F. 2021. BEiT: BERT\nPre-Training of Image Transformers. arXiv preprint\narXiv:2106.08254.\nBerg, A.; O’Connor, M.; and Cruz, M. T. 2021. Keyword\nTransformer: A Self-Attention Model for Keyword Spotting.\nIn Interspeech.\nBusso, C.; Bulut, M.; Lee, C.-C.; Kazemzadeh, A.; Mower,\nE.; Kim, S.; Chang, J. N.; Lee, S.; and Narayanan, S. S.\n2008. IEMOCAP: Interactive emotional dyadic motion cap-\nture database. Language resources and evaluation, 42(4):\n335–359.\nCaron, M.; Touvron, H.; Misra, I.; J ´egou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging proper-\nties in self-supervised vision transformers. arXiv preprint\narXiv:2104.14294.\nChen, X.; Xie, S.; and He, K. 2021. An empirical study of\ntraining self-supervised vision transformers. arXiv preprint\narXiv:2104.02057.\nChung, Y .-A.; Hsu, W.-N.; Tang, H.; and Glass, J. 2019.\nAn unsupervised autoregressive model for speech represen-\ntation learning. In Interspeech.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn image is worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR.\nGemmeke, J. F.; Ellis, D. P.; Freedman, D.; Jansen, A.;\nLawrence, W.; Moore, R. C.; Plakal, M.; and Ritter, M.\n2017. Audio Set: An ontology and human-labeled dataset\nfor audio events. In ICASSP.\nGong, Y .; Chung, Y .-A.; and Glass, J. 2021. AST: Audio\nSpectrogram Transformer. In Interspeech.\nHe, T.; Zhang, Z.; Zhang, H.; Zhang, Z.; Xie, J.; and Li, M.\n2019. Bag of tricks for image classification with convolu-\ntional neural networks. In CVPR.\nHsu, W.-N.; Tsai, Y .-H. H.; Bolte, B.; Salakhutdinov, R.; and\nMohamed, A. 2021. HuBERT: How much can a bad teacher\nbenefit ASR pre-training? In ICASSP.\nJiang, D.; Li, W.; Cao, M.; Zhang, R.; Zou, W.; Han, K.; and\nLi, X. 2020. Speech SIMCLR: Combining Contrastive and\nReconstruction Objective for Self-supervised Speech Repre-\nsentation Learning. arXiv preprint arXiv:2010.13991.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for\nstochastic optimization. In ICLR.\nLeCun, Y .; and Bengio, Y . 1995. Convolutional networks\nfor images, speech, and time series. The Handbook of Brain\nTheory and Neural Networks, 3361(10): 1995.\nLiu, A. T.; Yang, S.-w.; Chi, P.-H.; Hsu, P.-c.; and Lee, H.-\ny. 2020. Mockingjay: Unsupervised speech representation\nlearning with deep bidirectional transformer encoders. In\nICASSP.\nNagrani, A.; Chung, J. S.; Xie, W.; and Zisserman, A. 2020.\nV oxceleb: Large-scale speaker verification in the wild.Com-\nputer Speech and Language, 60: 101027.\nNiizumi, D.; Takeuchi, D.; Ohishi, Y .; Harada, N.; and\nKashino, K. 2021. BYOL for Audio: Self-Supervised\nLearning for General-Purpose Audio Representation. arXiv\npreprint arXiv:2103.06695.\nNoroozi, M.; and Favaro, P. 2016. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In ECCV.\nOord, A. v. d.; Li, Y .; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nPanayotov, V .; Chen, G.; Povey, D.; and Khudanpur, S. 2015.\nLibrispeech: an asr corpus based on public domain audio\nbooks. In ICASSP.\nPark, D. S.; Chan, W.; Zhang, Y .; Chiu, C.-C.; Zoph, B.;\nCubuk, E. D.; and Le, Q. V . 2019. SpecAugment: A simple\ndata augmentation method for automatic speech recognition.\nIn Interspeech.\nPascual, S.; Ravanelli, M.; Serra, J.; Bonafonte, A.; and Ben-\ngio, Y . 2019. Learning problem-agnostic speech represen-\ntations from multiple self-supervised tasks. arXiv preprint\narXiv:1904.03416.\nPiczak, K. J. 2015. ESC: Dataset for environmental sound\nclassification. In Multimedia.\nRavanelli, M.; Zhong, J.; Pascual, S.; Swietojanski, P.;\nMonteiro, J.; Trmal, J.; and Bengio, Y . 2020. Multi-task\nself-supervised learning for robust speech recognition. In\nICASSP.\nSaeed, A.; Grangier, D.; and Zeghidour, N. 2021. Con-\ntrastive learning of general-purpose audio representations.\nIn ICASSP.\nSchneider, S.; Baevski, A.; Collobert, R.; and Auli, M. 2019.\nwav2vec: Unsupervised pre-training for speech recognition.\narXiv preprint arXiv:1904.05862.\nTokozume, Y .; Ushiku, Y .; and Harada, T. 2018. Learning\nfrom between-class examples for deep sound recognition.\nIn ICLR.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2020. Training data-efficient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877.\nTrinh, T. H.; Luong, M.-T.; and Le, Q. V . 2019. Selfie: Self-\nsupervised pretraining for image embedding. arXiv preprint\narXiv:1906.02940.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS.\n10708\nWarden, P. 2018. Speech commands: A dataset for\nlimited-vocabulary speech recognition. arXiv preprint\narXiv:1804.03209.\nYang, S.-w.; Chi, P.-H.; Chuang, Y .-S.; Lai, C.-I. J.; Lakho-\ntia, K.; Lin, Y . Y .; Liu, A. T.; Shi, J.; Chang, X.; Lin, G.-T.;\net al. 2021. SUPERB: Speech processing Universal PERfor-\nmance Benchmark. arXiv preprint arXiv:2105.01051.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.; Tay,\nF. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Train-\ning vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986.\n10709",
  "topic": "Spectrogram",
  "concepts": [
    {
      "name": "Spectrogram",
      "score": 0.8033091425895691
    },
    {
      "name": "Computer science",
      "score": 0.7708988189697266
    },
    {
      "name": "Discriminative model",
      "score": 0.6586555242538452
    },
    {
      "name": "Speech recognition",
      "score": 0.628359317779541
    },
    {
      "name": "Transformer",
      "score": 0.6029489040374756
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5553461909294128
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4706639051437378
    },
    {
      "name": "Hidden Markov model",
      "score": 0.4281969666481018
    },
    {
      "name": "Deep learning",
      "score": 0.4168643355369568
    },
    {
      "name": "Machine learning",
      "score": 0.3713234066963196
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37086737155914307
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 219
}