{
  "title": "Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers",
  "url": "https://openalex.org/W4378771677",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2116853344",
      "name": "Chan-Young Chung",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2120560200",
      "name": "Jaejun Lee",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2086498169",
      "name": "Joyce Jiyoung Whang",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3203587881",
    "https://openalex.org/W102708294",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3163550690",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W3082429057",
    "https://openalex.org/W3154812445",
    "https://openalex.org/W4319453160",
    "https://openalex.org/W4382240030",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3156306687",
    "https://openalex.org/W3035021589",
    "https://openalex.org/W3100239257",
    "https://openalex.org/W3094537309",
    "https://openalex.org/W3035101093",
    "https://openalex.org/W3154561712",
    "https://openalex.org/W2914812111",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4224311351",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W6945344587",
    "https://openalex.org/W2980763157",
    "https://openalex.org/W4285613774",
    "https://openalex.org/W4379251496",
    "https://openalex.org/W2963870853",
    "https://openalex.org/W3012851129",
    "https://openalex.org/W3156178579",
    "https://openalex.org/W2963606508",
    "https://openalex.org/W4290948420",
    "https://openalex.org/W4246698901",
    "https://openalex.org/W2022166150",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2309189658",
    "https://openalex.org/W2964161331",
    "https://openalex.org/W2250184916",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3130909864",
    "https://openalex.org/W3173831770",
    "https://openalex.org/W2885483808",
    "https://openalex.org/W3155631545",
    "https://openalex.org/W2787882523",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W804133461",
    "https://openalex.org/W2531563875",
    "https://openalex.org/W3155001903",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W178169250",
    "https://openalex.org/W4297630050",
    "https://openalex.org/W2343431530",
    "https://openalex.org/W3012846531",
    "https://openalex.org/W2914263187",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2892181857"
  ],
  "abstract": "A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding them into the transformers, we reduce the computation cost of using transformers. Using HyNT, we can predict missing numeric values in addition to missing entities or relations in a hyper-relational knowledge graph. Experimental results show that HyNT significantly outperforms state-of-the-art methods on real-world datasets.",
  "full_text": "Representation Learning on Hyper-Relational and Numeric\nKnowledge Graphs with Transformers\nChanyoung Chungâˆ—\nSchool of Computing, KAIST\nDaejeon, Republic of Korea\nchanyoung.chung@kaist.ac.kr\nJaejun Leeâˆ—\nSchool of Computing, KAIST\nDaejeon, Republic of Korea\njjlee98@kaist.ac.kr\nJoyce Jiyoung Whangâ€ \nSchool of Computing, KAIST\nDaejeon, Republic of Korea\njjwhang@kaist.ac.kr\nABSTRACT\nIn a hyper-relational knowledge graph, a triplet can be associated\nwith a set of qualifiers, where a qualifier is composed of a relation\nand an entity, providing auxiliary information for the triplet. While\nexisting hyper-relational knowledge graph embedding methods as-\nsume that the entities are discrete objects, some information should\nbe represented using numeric values, e.g., (J.R.R., was born in, 1892).\nAlso, a triplet (J.R.R., educated at, Oxford Univ.) can be associated\nwith a qualifier such as (start time, 1911). In this paper, we propose\na unified framework named HyNT that learns representations of a\nhyper-relational knowledge graph containing numeric literals in\neither triplets or qualifiers. We define a context transformer and a\nprediction transformer to learn the representations based not only\non the correlations between a triplet and its qualifiers but also on\nthe numeric information. By learning compact representations of\ntriplets and qualifiers and feeding them into the transformers, we\nreduce the computation cost of using transformers. Using HyNT, we\ncan predict missing numeric values in addition to missing entities\nor relations in a hyper-relational knowledge graph. Experimental\nresults show that HyNT significantly outperforms state-of-the-art\nmethods on real-world datasets.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Semantic networks; Reason-\ning about belief and knowledge .\nKEYWORDS\nKnowledge Graph; Hyper-Relational Fact; Numeric Literals; Knowl-\nedge Graph Completion; Representation Learning; Transformer\nACM Reference Format:\nChanyoung Chung, Jaejun Lee, and Joyce Jiyoung Whang. 2023. Repre-\nsentation Learning on Hyper-Relational and Numeric Knowledge Graphs\nwith Transformers. In Proceedings of the 29th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining (KDD â€™23), August 6â€“10, 2023, Long\nBeach, CA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.\n1145/3580305.3599490\nâˆ—Authors in alphabetical order with equal contribution.\nâ€ Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0103-0/23/08. . . $15.00\nhttps://doi.org/10.1145/3580305.3599490\nstart time\npublication date\nstart time\nstudents count\nduration\nposition held\nposition\nheldcitizenship\nacademic degree\ncountry\ndirector\ndirector\nplace of\npublication\nmember of\nbox office\n2009.\n01.20.\n760,507,625 $\n4\n1997.\n12.19\n1945.\n10.24.\n1,990 2020\n194\nmin.\nBarack \nObama\nU.S.\nPresident\nUnited \nStates of\nAmerica\nJuris\nDoctor\nGeorge\nW. Bush\nJames\nCameron\nAvatar\nUnited\nNations\nHarvard\nLaw \nSchool\nTitanic\nDiscrete Entities Triplets/Qualifiers with only discrete entities\nNumeric Values Triplets/Qualifiers with numeric values\nFigure 1: A real-world hyper-relational knowledge graph\ncontaining numeric literals. This is a subgraph of HN-WK\nwhich is created based on Wikidata. Details are in Section 3.2.\n1 INTRODUCTION\nWhile most research on knowledge graphs assumes a classical\nform of a knowledge graph composed only of triplets [ 8, 15, 21,\n34], it has been recognized that the triplet format oversimplifies\ninformation that can be represented [35]. To enrich information in\na knowledge graph, a hyper-relational knowledge graph has been\nrecently studied [13, 47], where a triplet is extended to a hyper-\nrelational fact which is defined by a triplet and its qualifiers. Each\nqualifier is represented by a relation-entity pair and adds auxiliary\ninformation to a triplet. For example, a triplet (Barack Obama,\nacademic degree, Juris Doctor) can be associated with a qualifier\n(educated at, Harvard Law School)representing that Barack Obama\ngot his JD at Harvard Law School, as shown in Figure 1.\nDifferent hyper-relational knowledge graph embedding meth-\nods have been recently proposed, e.g., HINGE [35], NeuInfer [16],\nStarE [13], Hy-Transformer [51], and GRAN [47]. However, exist-\ning methods treat all entities as discrete objects [16â€“18, 29, 47] or\nremove numeric literals [13, 35, 51], even though some information\nshould be represented using numeric values. For example, Figure 1\nshows a subgraph of a real-world hyper-relational knowledge graph\nHN-WK (Hyper-relational, N umeric W iKidata) which will be de-\nscribed in Section 3.2. To represent that the number of students\nat Harvard Law School was 1,990 in 2020, we have (Harvard Law\nSchool, students count, 1,990) and its qualifier (point in time, 2020).\nWe propose a representation learning method that learns rep-\nresentations of a hyper-relational knowledge graph containing\narXiv:2305.18256v5  [cs.LG]  5 Oct 2024\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Chanyoung Chung, Jaejun Lee, and Joyce Jiyoung Whang\n0 25000 50000 75000 100000 125000 150000\nTraining Time (seconds)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Mean Reciprocal Rank (MRR)\nHyNT\nHN-WK\nNaLP\ntNaLP\nRAM\nHINGE\nNeuInfer\nStarE\nHy-Transformer\nGRAN\nHyNT\nFigure 2: Link prediction performance according to the train-\ning time on HN-WK. A higher MRR score indicates better\nperformance. HyNT performs better than all the other meth-\nods while requiring less training time.\ndiverse numeric literals. We name our method HyNT which stands\nfor Hyper-relational knowledge graph embedding with Numeric\nliterals using Transformers. Using well-known knowledge bases,\nWikidata [45], YAGO [38], and Freebase [6], we create three real-\nworld datasets that include various numeric literals in triplets and\nqualifiers. HyNT properly encodes and utilizes the numeric liter-\nals. By considering a loss that makes the numeric values be re-\ncovered using the latent representations, HyNT can predict not\nonly missing entities or relations but also missing numeric val-\nues. For example, given (Titanic, duration, ?), HyNT can predict\n? in Figure 1. Our datasets and implementations are available at\nhttps://github.com/bdi-lab/HyNT.\nWe define a context transformer and a prediction transformer\nto learn embedding vectors by incorporating not only the struc-\nture of a hyper-relational fact but also the information provided\nby the numeric literals. In particular, HyNT learns the representa-\ntions of triplets and qualifiers by aggregating the embeddings of\nentities and relations that constitute the corresponding triplets or\nqualifiers. By feeding the representations of triplets and qualifiers\ninto transformers, HyNT effectively captures the correlations be-\ntween a triplet and its qualifiers while reducing computation cost.\nExperimental results show that HyNT significantly outperforms\n12 different baseline methods in link prediction, numeric value\nprediction, and relation prediction tasks on real-world datasets.\nFigure 2 shows the Mean Reciprocal Rank (MRR) according\nto the training time of different methods on HN-WK. MRR is a\nstandard metric for link prediction; the higher, the better. Among\nthe eight baseline methods, the first three methods, NaLP [ 18],\ntNaLP [17], and RAM [29], utilize n-ary representations where a\nhyper-relational fact is represented as role-entity pairs. The rest\nfive methods are hyper-relational knowledge graph embedding\nmethods. We show the MRR score of each method until the method\nfinishes1 or it runs 36 hours. For a fair comparison, we run all meth-\nods on GeForce RTX 2080 Ti except RAM because RAM requires\nmore memory. We run RAM using RTX A6000. In Figure 2, we see\n1We determine each methodâ€™s best epoch on a validation set and run methods until\ntheir best epochs on a test set. In Figure 2, the MRRs are measured on the test set.\nthat HyNT substantially outperforms all other baseline methods in\nperformance while requiring less training time.\n2 RELATED WORK\nKnowledge Graph Embedding with Numeric Literals . For\nnormal knowledge graphs but not for hyper-relational knowledge\ngraphs, several methods incorporate numeric literals into knowl-\nedge graph embedding [5, 14, 20, 23, 24, 33, 41, 50]. For example,\nMT-KGNN [41] and TransEA [50] introduce an additional loss that\nregresses entity embeddings to their corresponding numeric at-\ntributes. KBLRN [14] collects patterns between numeric attributes,\nwhich is used to measure the plausibility of triplets. LiteralE [24]\ndirectly utilizes numeric literals to compute entity embeddings us-\ning gated recurrent units. However, these models can only handle\ntriplets and cannot appropriately handle hyper-relational facts.\nN-ary Relations & Hyper-Relational Facts . Some recent\nworks [11, 12, 17, 18, 28, 29, 49, 52] extend triplets using n-ary\nrepresentations where each fact is represented by multiple role-\nentity pairs. Among them, NaLP [18], tNaLP [17], and RAM [29]\nmeasure the plausibility of each n-ary fact by modeling the related-\nness between role-entity pairs. On the other hand, recent studies use\nhyper-relational knowledge graphs [2, 13, 16, 35, 47, 51], pointing\nout that n-ary representations cannot fully express the diverse in-\nformation in knowledge bases [35]. HINGE [35] and NeuInfer [16]\nconsider the validity of a triplet as well as its qualifiers to mea-\nsure the plausibility of each hyper-relational fact. Unlike HyNT,\nthe aforementioned methods assume that all entities in the n-ary\nrelational or hyper-relational facts are discrete.\nTransformer-based Knowledge Graph Embedding. Inspired\nby the great success of transformer-based frameworks [10, 43], var-\nious methods utilize transformers to solve tasks on knowledge\ngraphs [7, 46] and hyper-relational knowledge graphs [13, 47, 51].\nStarE [13] uses a GNN-based encoder, which reflects the structure\nof hyper-relational facts, along with a transformer-based decoder.\nHy-Transformer [51] replaces the GNN-based encoder in StarE with\na layer normalization. GRAN [47] treats each hyper-relational fact\nas a heterogeneous graph and uses it as an input of a transformer.\nDifferent from HyNT, these methods do not consider the case of\nhaving numeric literals in hyper-relational facts.\nExisting Benchmark Datasets . Most well-known benchmark\ndatasets for hyper-relational knowledge graphs are JF17K [48, 49],\nWD50K [13], WikiPeopleâˆ’[35], and WikiPeople [18]. When creating\nJF17K, WD50K, and WikiPeopleâˆ’, numeric literals were removed\nbecause the previous embedding methods could not adequately\nhandle them. On the other hand, the original version of WikiPeo-\nple includes some limited numeric literals, where WikiPeople was\ncreated by extracting facts involving entities of type human from\nWikidata dump [18]. Thus, the information in WikiPeople is re-\nstricted to humans, and all numeric literals are â€˜yearâ€™. To create\ndatasets containing more diverse numeric literals, we make HN-\nWK using the original Wikidata dump without the constraint of\nconsidering humans. We also create two more datasets usingYAGO\nand Freebase. While we focus on evaluating methods on datasets\ncontaining various numeric literals, we also provide experimental\nresults on WikiPeopleâˆ’and WD50K in Appendix D.\nRepresentation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\n3 HYPER-RELATIONAL KNOWLEDGE\nGRAPHS CONTAINING NUMERIC LITERALS\nWe introduce a formal definition of a hyper-relational and numeric\nknowledge graph and describe real-world datasets and the tasks.\n3.1 Definition of a Hyper-Relational and\nNumeric Knowledge Graph\nWe assume that an entity in a hyper-relational knowledge graph\ncan be either a discrete object or a numeric literal. If an entity\nis a discrete object, we call it a discrete entity; if an entity is a\nnumeric literal, we call it a numeric entity. When a hyper-relational\nknowledge graph includes numeric entities, we call it a Hyper-\nrelational and Numeric Knowledge Graph (HN-KG). When an entity\nis a numeric entity in HN-KG, the numeric value is accompanied\nby its unit, e.g., 80 kg or 80 years. Let VN âŠ‚R Ã—T denote a set\nof numeric entities where Tdenotes a set of units. We formally\ndefine an HN-KG as follows:\nDefinition 1 (Hyper-Relational and Numeric Knowledge\nGraph). A hyper-relational and numeric knowledge graph is defined\nby ğº = (V,R,E)where Vis a set of entities represented by VB\nVD âˆªVN, VD is a set of discrete entities, VN is a set of numeric\nentities, Ris a set of relations, and Eis a set of hyper-relational facts\ndefined by EâŠ‚E tri Ã—P(Equal)where Etri âŠ‚VÃ—RÃ—V is a set\nof primary triplets, Equal âŠ‚RÃ—V is a set of qualifiers, and P(S)\ndenotes the power set of S.\n3.2 Creating Real-World Datasets\nWe create threeHN-KG datasets: HN-WK, HN-YG, andHN-FB which\nare created based on Wikidata [45], YAGO [38], and Freebase [6],\nrespectively. Details about how to make these datasets are described\nin Appendix A. According to the standard Resource Description\nFramework (RDF), the numeric entities should only appear as a tail\nentity of a primary triplet or a qualifierâ€™s entity [36]. We classify\nthe numeric entities based on their relations in the primary triplet\nor qualifier they belong to and make the numeric entities with\nthe same relation have the same unit, e.g., 176.4 lb is converted\ninto 80 kg. We use the International System of Units (SI) in this\nprocess, e.g., the mass should be represented in kilograms. By using\nthe same unit per relation, the prediction task can become simpler.\nEven though a numeric entity consists of its value and unit in\ngeneral, we can interpret the numeric entities without their units\nbecause the semantics of the numeric entities are determined by\ntheir relations. For example, let us consider two triplets (Robbie\nKeane, weight, 80 kg) and (Canada, life expectancy, 80 years). Even\nthough the numeric values of the tail entities of these two triplets\nare both 80, we can distinguish them by looking at their relations,\nweight and life expectancy. Therefore, from this point, we drop the\nunits from VN for brevity, i.e., VN âŠ‚R.\n3.3 Link Prediction, Numeric Value Prediction\nand Relation Prediction Tasks\nOn an HN-KG, a link prediction task predicts a missing discrete\nentity in each hyper-relational fact, whereas a numeric value pre-\ndiction task predicts a missing numeric value. Also, a relation pre-\ndiction task predicts a missing relation.\nConsider ğº = (V,R,E)where E = {((â„,ğ‘Ÿ,ğ‘¡ ),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1) :\nâ„ âˆˆV ,ğ‘Ÿ âˆˆ R,ğ‘¡ âˆˆV ,ğ‘ğ‘– âˆˆ R,ğ‘£ğ‘– âˆˆV} and ğ‘˜ is the number of\nqualifiers in a hyper-relational fact. We define a general entity pre-\ndiction task on ğºas predicting a missing component ? in one of the\nfollowing forms: ((?,ğ‘Ÿ,ğ‘¡ ),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1), ((â„,ğ‘Ÿ, ?),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1), and\n((â„,ğ‘Ÿ,ğ‘¡ ),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1,ğ‘–â‰ ğ‘— âˆª{(ğ‘ğ‘—,?)}). If the missing component ? is\na discrete entity, we call it a link prediction task. If the missing com-\nponent ? is a numeric entity, we call it a numeric value prediction\ntask. On the other hand, we define a relation prediction task to be\nthe task of predicting a missing relation ? in ((â„,?,ğ‘¡),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1)\nor ((â„,ğ‘Ÿ,ğ‘¡ ),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1,ğ‘–â‰ ğ‘— âˆª{(?,ğ‘£ğ‘— )}).\n4 LEARNING ON HYPER-RELATIONAL AND\nNUMERIC KNOWLEDGE GRAPHS\nWe propose HyNT which learns representations of discrete entities,\nnumeric entities, and relations in an HN-KG.\n4.1 Representations of Triplets and Qualifiers\nGiven an HN-KG, ğº = (V,R,E), let us consider a hyper-relational\nfact represented by ((â„,ğ‘Ÿ,ğ‘¡ ),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1)where (â„,ğ‘Ÿ,ğ‘¡ )is a pri-\nmary triplet and {(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1 is a set of qualifiers. Let ğ‘‘ denote the\ndimension of an embedding vector. Each component of the hyper-\nrelational fact is represented as an embedding vector: let h âˆˆRğ‘‘\ndenote an embedding vector of â„, r âˆˆRğ‘‘ denote an embedding vec-\ntor of ğ‘Ÿ, t âˆˆRğ‘‘ denote an embedding vector of ğ‘¡, qğ‘– âˆˆRğ‘‘ denote an\nembedding vector of ğ‘ğ‘– , and vğ‘– âˆˆRğ‘‘ denote an embedding vector\nof ğ‘£ğ‘– . We assume all vectors are column vectors.\nFor the entities,â„, ğ‘¡, and ğ‘£ğ‘– , if the entities are discrete entities, we\ndirectly learn the corresponding embedding vectors,h, t, and vğ‘– . On\nthe other hand, if the entities are numeric entities, we utilize their\nnumeric values as well as relation-specific weights and bias vec-\ntors to represent the embedding vectors of those numeric entities.2\nSpecifically, given (â„,ğ‘Ÿ,ğ‘¡ )with a numeric entity ğ‘¡ âˆˆVN, we com-\npute its embedding vector t = ğ‘¡wğ‘Ÿ +bğ‘Ÿ where wğ‘Ÿ âˆˆRğ‘‘ is a weight\nvector of a relation ğ‘Ÿ and bğ‘Ÿ âˆˆRğ‘‘ is a bias vector of ğ‘Ÿ. Similarly,\nfor a qualifier (ğ‘ğ‘–,ğ‘£ğ‘– )with ğ‘£ğ‘– âˆˆVN, we compute vğ‘– = ğ‘£ğ‘– wğ‘ğ‘– +bğ‘ğ‘–\nwhere wğ‘ğ‘– âˆˆRğ‘‘ is a weight vector of a qualifierâ€™s relation ğ‘ğ‘– and\nbğ‘ğ‘– âˆˆRğ‘‘ is a bias vector of ğ‘ğ‘– .\nWe define an embedding vector of a primary triplet (â„,ğ‘Ÿ,ğ‘¡ )\nas xtri. Using a projection matrix ğ‘¾tri âˆˆ Rğ‘‘Ã—3ğ‘‘ , we compute\nxtri = ğ‘¾tri [h; r; t]where [u1; Â·Â·Â· ; uğ‘›]is a vertical concatenation\nof vectors u1,..., uğ‘›. We call this process a triplet encoding. Also,\nto compute an embedding vector of a qualifier (ğ‘ğ‘–,ğ‘£ğ‘– ), we compute\nxqualğ‘– = ğ‘¾qual [qğ‘– ; vğ‘– ]where ğ‘¾qual âˆˆRğ‘‘Ã—2ğ‘‘ is a projection matrix.\nWe call this process a qualifier encoding.\n4.2 Context Transformer\nWe define a context transformer to learn representations of the pri-\nmary triplets and the qualifiers by exchanging information among\nthem. Given a primary triplet (â„,ğ‘Ÿ,ğ‘¡ )and its qualifiers {(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1,\nthe context transformer learns the relative importance of each of\nthe qualifiers to the primary triplet and learns the representation of\nthe primary triplet by aggregating the qualifiersâ€™ representations,\n2As discussed in Section 3.2, the numeric entities can only be positioned at ğ‘¡ or ğ‘£ğ‘– .\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Chanyoung Chung, Jaejun Lee, and Joyce Jiyoung Whang\nweighing each of the aggregations using the relative importance.\nSimilarly, the representation of each qualifier is also learned by\nconsidering the relative importance of the primary triplet and the\nother qualifiers to the target qualifier. This process allows the rep-\nresentations of the primary triplet and their qualifiers to reflect the\ncontext by considering the correlations between them.\nThe input of the context transformer is:\nğ‘¿ (0)= [(xtri +ptri)âˆ¥(xqual1 +pqual)âˆ¥Â·Â·Â·âˆ¥( xqualğ‘˜ +pqual)]\n= [x(0)\ntri âˆ¥x(0)\nqual1\nâˆ¥Â·Â·Â·âˆ¥ x(0)\nqualğ‘˜\n]\n(1)\nwhere xtri is the embedding of a primary triplet andxqualğ‘– is the em-\nbedding of the ğ‘–-th qualifier, respectively, discussed in Section 4.1,\nptri âˆˆRğ‘‘ is a learnable vector for positional encoding of a primary\ntriplet for the context transformer, pqual âˆˆRğ‘‘ is another learnable\nvector for positional encoding of a qualifier for the context trans-\nformer, and [u1âˆ¥Â·Â·Â·âˆ¥ uğ‘›]is a horizontal concatenation of vectors\nu1,..., uğ‘›. By introducing ptri and pqual, we encode the informa-\ntion about whether the corresponding embedding vector indicates\na primary triplet or one of its qualifiers.\nIn the attention layer, we compute\neğ‘¿ (ğ‘™ )= ğ‘½ (ğ‘™ )ğ‘¿ (ğ‘™ )softmax\n \n(ğ‘¸(ğ‘™ )ğ‘¿ (ğ‘™ ))ğ‘‡ (ğ‘² (ğ‘™ )ğ‘¿ (ğ‘™ ))âˆš\nğ‘‘\n!\nwhere ğ‘¸(ğ‘™ ),ğ‘² (ğ‘™ ),ğ‘½ (ğ‘™ ) âˆˆRğ‘‘Ã—ğ‘‘ are projection matrices for query,\nkey, and value, respectively [43], in the context transformer with\nğ‘™ = 0,Â·Â·Â· ,ğ¿C âˆ’1 and ğ¿C is the number of layers in the context\ntransformer. We use the multi-head attention mechanism withğ‘›C\nheads where ğ‘›C is a hyperparameter [43, 44]. Then, we employ a\nresidual connection [19] and apply layer normalization [ 4]. The\nfollowing feedforward layer is computed by\nğ‘¿ (ğ‘™+1)= ğ‘¾ (ğ‘™ )\n2 ğœ\n\u0010\nğ‘¾ (ğ‘™ )\n1 eğ‘¿ (ğ‘™ )+b(ğ‘™ )\n1\n\u0011\n+b(ğ‘™ )\n2\nwhere ğœ(ğ‘¥) = max(0,ğ‘¥)is the ReLU function, ğ‘¾ (ğ‘™ )\n1 âˆˆ Rğ‘‘F Ã—ğ‘‘ ,\nğ‘¾ (ğ‘™ )\n2 âˆˆRğ‘‘Ã—ğ‘‘F , b(ğ‘™ )\n1 âˆˆRğ‘‘F , b(ğ‘™ )\n2 âˆˆRğ‘‘ , and ğ‘‘F is the hidden di-\nmension of the feedforward layer in the context transformer. Then,\nwe again employ a residual connection, followed by layer normal-\nization. By repeating the above process for ğ‘™ = 0,Â·Â·Â· ,ğ¿C âˆ’1, we\nget the final representations of a primary triplet and its qualifiers:\nğ‘¿ (ğ¿C )= [x(ğ¿C )\ntri âˆ¥x(ğ¿C )\nqual1\nâˆ¥Â·Â·Â·âˆ¥ x(ğ¿C )\nqualğ‘˜\n]for each hyper-relational fact.\n4.3 Prediction Transformer\nWe define a prediction transformer that learns representations used\nto predict a missing component in a primary triplet or predict a\nmissing component in a qualifier. To make a prediction on a primary\ntriplet (â„,ğ‘Ÿ,ğ‘¡ ), the input of the prediction transformer is defined by\nğ’ (0)= [(x(ğ¿C )\ntri +bptri)âˆ¥(h +bph)âˆ¥(r +bpr)âˆ¥(t +bpt)]\n= [z(0)\ntri âˆ¥h(0)âˆ¥r(0)âˆ¥t(0)]\n(2)\nwhere x(ğ¿C )\ntri is the representation of the primary triplet returned\nby the context transformer, h, r, and t are the embedding vectors\nof â„, ğ‘Ÿ, and ğ‘¡, respectively, and bptri, bph, bpr, bpt âˆˆRğ‘‘ are learnable\nvectors for positional encoding of the primary triplet, a head entity,\na relation, and a tail entity, respectively.\nTo make a prediction on a qualifier (ğ‘ğ‘–,ğ‘£ğ‘– ), the input of the\nprediction transformer is defined by\nğ’ (0)= [(x(ğ¿C )\nqualğ‘–\n+bpqual)âˆ¥(qğ‘– +bpq)âˆ¥(vğ‘– +bpv)]\n= [z(0)\nqualğ‘–\nâˆ¥qğ‘– (0)âˆ¥vğ‘– (0)]\n(3)\nwhere x(ğ¿C )\nqualğ‘–\nis the representation of the qualifier returned by the\ncontext transformer, qğ‘– and vğ‘– are the embedding vectors of ğ‘ğ‘– and\nğ‘£ğ‘– , respectively, and bpqual, bpq, bpv âˆˆRğ‘‘ are learnable vectors for\npositional encoding of a qualifier, a relation in a qualifier, and an\nentity in a qualifier, respectively.\nIn the attention layer, we compute\neğ’ (ğ‘™ )= bğ‘½ (ğ‘™ )ğ’ (ğ‘™ )softmax\n \n(bğ‘¸\n(ğ‘™ )\nğ’ (ğ‘™ ))ğ‘‡ (bğ‘² (ğ‘™ )ğ’ (ğ‘™ ))âˆš\nğ‘‘\n!\nwhere bğ‘¸\n(ğ‘™ )\n,bğ‘² (ğ‘™ ),bğ‘½ (ğ‘™ ) âˆˆRğ‘‘Ã—ğ‘‘ are projection matrices for query,\nkey, and value, respectively, in the prediction transformer with\nğ‘™ = 0,Â·Â·Â· ,ğ¿P âˆ’1, and ğ¿P is the number of layers in the predic-\ntion transformer. We use the multi-head attention mechanism with\nğ‘›P heads where ğ‘›P is a hyperparameter. After employing a resid-\nual connection and layer normalization, the feedforward layer is\ndefined by\nğ’ (ğ‘™+1)= bğ‘¾ (ğ‘™ )\n2 ğœ\n\u0010\nbğ‘¾ (ğ‘™ )\n1 eğ’ (ğ‘™ )+bb(ğ‘™ )\n1\n\u0011\n+bb(ğ‘™ )\n2\nwhere bğ‘¾ (ğ‘™ )\n1 âˆˆRbğ‘‘F Ã—ğ‘‘ , bğ‘¾ (ğ‘™ )\n2 âˆˆRğ‘‘Ã—bğ‘‘F , bb(ğ‘™ )\n1 âˆˆRbğ‘‘F , bb(ğ‘™ )\n2 âˆˆRğ‘‘ , and bğ‘‘F\nis the hidden dimension of the feedforward layer in the prediction\ntransformer. Then, we again apply the residual connection and\nlayer normalization.\nBy repeating the above process for ğ‘™ = 0,Â·Â·Â· ,ğ¿P âˆ’1, we get the\nrepresentation:\nğ’ (ğ¿P )= [z(ğ¿P )\ntri âˆ¥h(ğ¿P )âˆ¥r(ğ¿P )âˆ¥t(ğ¿P )]\nfor the case of prediction on a primary triplet, and\nğ’ (ğ¿P )= [z(ğ¿P )\nqualğ‘–\nâˆ¥qğ‘– (ğ¿P )âˆ¥vğ‘– (ğ¿P )]\nfor the case of prediction on a qualifier.\n4.4 Training of HyNT\nTo train HyNT, we adopt a masking strategy [ 10]; we introduce\nthree different types of masks depending on the prediction tasks.\n4.4.1 Discrete Entity Prediction Loss. We consider the task of pre-\ndicting a discrete entity in a hyper-relational fact. We introduce a\nspecial entity for a mask, denoted by ğ‘šent, and consider it as one\nof the discrete entities. This special entity ğ‘šent is associated with a\nlearnable embedding vector denoted by ment âˆˆRğ‘‘ . Given a hyper-\nrelational fact in a training set, we replace one of the discrete entities\nin the given fact with ğ‘šent and train the model so that the replaced\nentity can be recovered. Let m(ğ¿P )\nent âˆˆRğ‘‘ be the output representa-\ntion of the prediction transformer ofğ‘šent. Using a projection matrix\nğ‘¾ent âˆˆR|VD |Ã—ğ‘‘ and a bias vectorbent âˆˆR|VD |, we compute a prob-\nability distribution y âˆˆR|VD |by y = softmax(ğ‘¾entm(ğ¿P )\nent +bent)\nwhere the ğ‘—-th element ofy is the probability that the masked entity\nğ‘šent is the ğ‘—-th entity in VD. To compute the loss incurred by the\nmatching between the masked entity and the ground-truth entity,\nRepresentation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nğ’ent\nğ‘šent\nğ’ent\nğ‘šent\nğ’™tri\n(ğ¿C)\nTriplet\nEncoding\nQualifier\nEncoding\nQualifier\nEncoding\nContext\nTransformer Layerğ¿C Ã—\nPrediction\nTransformer Layer\nLinear\nSoftmax\nFinal Prediction\n(Softmax distribution)\nğ¿P Ã—\nğ‘‘ â†’ ğ’±D\nğ’‘qual\nà·ğ’‘tri à·ğ’‘h à·ğ’‘r à·ğ’‘t\nLinear\nğ‘Ÿ ğ‘¡ ğ‘1 ğ‘£1 ğ‘ğ‘˜ ğ‘£ğ‘˜\nğ’’1 ğ’—1 ğ’’ğ‘˜ ğ’—ğ‘˜ğ’“ ğ’•\nğ’™qual1ğ’™tri ğ’™qualğ‘˜\nğ’‘qualğ’‘tri\nğ’™qual1\n(ğ¿C) ğ’™qualğ‘˜\n(ğ¿C)\nğ’™tri\n(ğ¿C)\nğ‘¡ğ‘Ÿ\nğ’•ğ’“\nğ’›tri\n(ğ¿P) ğ’ent\nğ¿P ğ’“ ğ¿P ğ’• ğ¿P\nnumeric\n(a) Predicting a discrete head entity in a primary triplet.\nğ’rel\nğ‘šrel\nğ’™tri\n(ğ¿C)\nTriplet\nEncoding\nQualifier\nEncoding\nQualifier\nEncoding\nContext\nTransformer Layerğ¿C Ã—\nPrediction\nTransformer Layer\nLinear\nSoftmax\nFinal Prediction\n(Softmax distribution)\nğ¿P Ã—\nğ‘‘ â†’ â„›\nğ’‘qual\nà·ğ’‘tri à·ğ’‘r à·ğ’‘t\nLinear\nğ‘¡ ğ‘1 ğ‘£1 ğ‘ğ‘˜ ğ‘£ğ‘˜\nğ’’1 ğ’—1 ğ’’ğ‘˜ ğ’—ğ‘˜ğ’•\nğ’™qual1ğ’™tri ğ’™qualğ‘˜\nğ’‘qualğ’‘tri\nğ’™qual1\n(ğ¿C) ğ’™qualğ‘˜\n(ğ¿C)\nğ’™tri\n(ğ¿C)\nğ‘¡\nğ’•\nğ’›tri\n(ğ¿P)\nğ’• ğ¿P\nnumeric\nğ’‰\nâ„\nğ’rel\nğ‘šrel\nà·ğ’‘h\nâ„\nğ’‰\nğ’rel\nğ¿Pğ’‰ ğ¿P\n(b) Predicting a relation in a primary triplet.\nğ’’ğ‘˜\nğ¿P\nğ’™qualğ‘˜\n(ğ¿C)\nğ’™tri\n(ğ¿C)\nTriplet\nEncoder\nQualifier\nEncoder\nQualifier\nEncoder\nContext\nTransformer Layerğ¿C Ã—\nPrediction\nTransformer Layer\nLinear\nFinal Prediction\n(Real Value)\nğ¿P Ã—\nğ‘‘ â†’ 1\nğ’‘qual\nà·ğ’‘qual à·ğ’‘q à·ğ’‘v\nLinear\nâ„ ğ‘Ÿ ğ‘¡ ğ‘1 ğ‘£1 ğ‘ğ‘˜\nğ’‰ ğ’’1 ğ’—1 ğ’’ğ‘˜ğ’“ ğ’•\nğ’™qual1ğ’™tri ğ’™qualğ‘˜\nğ’‘qualğ’‘tri\nğ’™qual1\n(ğ¿C)\nğ’™qualğ‘˜\n(ğ¿C)\nğ‘ğ‘˜\nğ’›qualğ‘˜\n(ğ¿P)\nğ’num\n(ğ¿P)\nLinear\nğ’’ğ‘˜ğ’num\nğ‘šnum\nLinear\nğ’num\nğ‘šnum\nnumeric numeric numeric\n(c) Predicting a numeric value in a qualifier.\nFigure 3: Examples of the training procedure of HyNT. We re-\nplace a missing component with a mask and train the model\nto recover the missing component to the ground-truth one.\nwe use the cross entropy loss: Lent B âˆ’logğ‘¦ğ‘— where ğ‘— is the index\nof the masked entity and ğ‘¦ğ‘— is the ğ‘—-th element of y. In Figure 3(a),\nwe show the training procedure of the discrete entity prediction\ntask where we mask the head entity in a primary triplet and train\nthe model so that the masked entity can be recovered.\n4.4.2 Relation Prediction Loss. We consider the task of predicting\na relation. We define a special relation for a mask denoted byğ‘šrel\nwith its learnable embedding vector mrel âˆˆRğ‘‘ . Given a hyper-\nrelational fact in a training set, we replace one of the relations with\nğ‘šrel. Let m(ğ¿P )\nrel âˆˆRğ‘‘ be the output representation of the prediction\ntransformer of ğ‘šrel. Using ğ‘¾rel âˆˆR|R|Ã—ğ‘‘ and brel âˆˆR|R|, we\ncompute y = softmax(ğ‘¾relm(ğ¿P )\nrel +brel)where the ğ‘—-th element\nindicates the probability that the masked relation is theğ‘—-th relation\nin R. To compute the relation prediction loss, we compute Lrel B\nâˆ’logğ‘¦ğ‘— where ğ‘— is the index of the masked relation and ğ‘¦ğ‘— is the\nğ‘—-th element of y. In Figure 3(b), we show an example where we\nmask the relation in a primary triplet and train the model so that\nthe masked relation can be recovered.\n4.4.3 Numeric Value Prediction Loss. We consider the task of pre-\ndicting a numeric value. We define a special learnable parame-\nter ğ‘šnum âˆˆR for masking and replace one of the numeric enti-\nties with ğ‘šnum in a hyper-relational fact. Let m(ğ¿P )\nnum âˆˆRğ‘‘ be the\noutput representation of the prediction transformer of ğ‘šnum and\nğ‘Ÿğ‘š âˆˆR be the relation associated with the masked numeric value.\nUsing wğ‘Ÿğ‘š âˆˆRğ‘‘ and ğ‘ğ‘Ÿğ‘š âˆˆR, we predict the masked value by\nğ‘£pred = wğ‘Ÿğ‘š Â·m(ğ¿P )\nnum +ğ‘ğ‘Ÿğ‘š . The loss of this prediction is computed\nby Lnum B (ğ‘£gt âˆ’ğ‘£pred)2 where ğ‘£gt is the ground-truth masked\nnumeric value. In Figure 3(c), we show the numeric value prediction\ntask where we mask a numeric value of a qualifier and train the\nmodel so that the ground-truth numeric value can be recovered.\n4.4.4 Joint Loss & Implementation Details.We define the final loss\nfunction of HyNT by adding all three aforementioned losses with\nappropriate weights: LB Lent +ğœ†1 Â·Lrel +ğœ†2 Â·Lnum where ğœ†1\nand ğœ†2 are hyperparameters governing the relative importance of\nthe relation prediction loss and the numeric value prediction loss,\nrespectively. In our experiments, we set ğœ†1 = ğœ†2 = 1.\nWe implement HyNT using PyTorch [32]. In our implementation\nand the experiments, we fix the seeds for random number genera-\ntors for reproducibility. To prevent overfitting, we apply the label\nsmoothing [39] on Lent and Lrel with the label smoothing ratio ğœ–.\nWe also apply the dropout strategy [37] with the dropout rate ğ›¿ in\nboth the context transformer and the prediction transformer lay-\ners. We utilize the Adam Algorithm [22] and the cosine annealing\nlearning rate scheduler with restarts [30].\n4.5 Predictions using HyNT\nWe describe how HyNT performs the link prediction, relation pre-\ndiction, and numeric value prediction tasks.\n4.5.1 Link Prediction Using HyNT. Consider a link prediction task,\n((?,ğ‘Ÿ,ğ‘¡ ),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1). To predict the missing discrete entity ?, we\nreplace ? with ğ‘šent in the given hyper-relational fact and use it as\nthe input of our model. Using the output representation m(ğ¿P )\nent of\nğ‘šent from the prediction transformer, we calculate the probability\ndistribution y = softmax(ğ‘¾entm(ğ¿P )\nent +bent). We predict the miss-\ning entity ? to be the entity with the highest probability in y. As\ndiscussed in Section 3.3, the position of the missing entity ? can be\nchanged to a tail entity of a primary triplet or a qualifierâ€™s entity.\n4.5.2 Relation Prediction Using HyNT. Given a relation prediction\ntask, ((â„,?,ğ‘¡),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1), we replace ? with ğ‘šrel. Using the out-\nput representation m(ğ¿P )\nrel of ğ‘šrel from the prediction transformer,\nwe calculate y = softmax(ğ‘¾relm(ğ¿P )\nrel +brel)and predict ? to be the\nrelation with the highest probability in y. The position of ? can be\nchanged to a qualifierâ€™s relation.\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Chanyoung Chung, Jaejun Lee, and Joyce Jiyoung Whang\n4.5.3 Numeric Value Prediction UsingHyNT. In a hyper-relational\nfact ((â„,ğ‘Ÿ,ğ‘¡ ),{(ğ‘ğ‘–,ğ‘£ğ‘– )}ğ‘˜\nğ‘–=1,ğ‘–â‰ ğ‘— âˆª{(ğ‘ğ‘—,?)}), we replace ? with ğ‘šnum.\nUsing the output representationm(ğ¿P )\nnum of ğ‘šnum from the prediction\ntransformer, we predict the missing value by ğ‘£pred = wğ‘ğ‘— Â·m(ğ¿P )\nnum +\nğ‘ğ‘ğ‘— where ğ‘£pred is our prediction. Similarly, we can also predict a\nnumeric value in a primary triplet.\n5 EXPERIMENTAL RESULTS\nWe compare the performance of HyNT with other state-of-the-art\nmethods using real-world HN-KG datasets.\n5.1 Datasets, Baseline Methods, and the Settings\nWe use three real-world HN-KGs described in Section 3.2. Table 1\nshows the statistic of these datasets where |RD|is the number of\nrelations only having discrete entities, |RN|is the number of rela-\ntions involving numeric entities, |Etri_D|is the number of primary\ntriplets that consist of only discrete entities, |Etri_N|is the number\nof primary triplets that include numeric entities, |Equal_D|is the\nnumber of qualifiers containing discrete entities, and |Equal_N|is\nthe number of qualifiers containing numeric entities. We randomly\nsplit Einto training, validation, and test sets with a ratio of 8:1:1.\nWe also show the number of triplets associated with or without\nqualifiers, denoted by w/ qual. and w/o qual., respectively.\nWe compare the performance of HyNT with 12 baseline methods:\nTransEA [50], MT-KGNN [41], KBLN [14], LiteralE [24], NaLP [18],\ntNaLP [17], RAM [29], HINGE [35], NeuInfer [16], StarE [13], Hy-\nTransformer [51], and GRAN [47]. The first four methods can handle\nnumeric literals in knowledge graphs and cannot handle hyper-\nrelational facts. On the other hand, the last eight methods handle\nhyper-relational knowledge graphs and do not consider numeric lit-\nerals. Note that StarE, Hy-Transformer, and GRAN are transformer-\nbased methods. Among the baseline methods, NaLP, tNaLP, RAM,\nHINGE, NeuInfer, StarE, Hy-Transformer, and GRAN failed to pro-\ncess HN-FB due to scalability issues. Since the maximum number\nof qualifiers attached to a primary triplet is 358 in HN-FB, these\nmethods require much more memory than any machines that we\nhave. Thus, we create a smaller version,HN-FB-S, by restricting the\nmaximum number of qualifiers to five.3 Also, these eight baseline\nmethods cannot handle numeric literals. To feed our datasets into\nthese methods, we treat the numeric entities as discrete entities by\nfollowing how the authors of NaLP, tNaLP, RAM, NeuInfer, and\nGRAN run their methods on datasets containing numeric literals.\nWhen these methods predict missing numeric entities, e.g., (â„,ğ‘Ÿ, ?)\nand ? is a numeric entity, we narrow down the candidates for ? by\nfiltering out the entities that never appear as a tail entity or a quali-\nfierâ€™s entity of ğ‘Ÿ. For example, to solve (Robbie Keane, weight, ?),\nthe candidates can be80 kg and 75 kg as long as they have appeared\nas a tail entity or a qualifierâ€™s entity of weight in a training set; but\n120 years is filtered out and cannot be considered as a candidate\nfor ?. We set ğ‘‘ = 256 for all methods and all datasets except for\nRAM on HN-WK. For RAM, we setğ‘‘ = 64 on HN-WK due to an out-\nof-memory issue. Details about how to run the baseline methods\nare described in Appendix B.\n3We show the results of HyNT on HN-FB in Appendix C. Note that HyNT is the only\nmethod that can handle HN-FB among the methods considering hyper-relational facts.\nTable 1: Real-world HN-KG Datasets.\nHN-WK HN-YG HN-FB HN-FB-S\nV\n|VD| 13,655 12,439 14,284 5,510\n|VN| 79,600 24,508 62,056 21,824\n|V| 93,255 36,947 76,340 27,334\nR\n|RD| 200 31 278 161\n|RN| 157 26 83 47\n|R| 357 57 361 208\nE\n|E| 296,783 76,383 284,288 108,140\nw/ qual. 126,524 9,916 71,598 30,121\nw/o qual. 170,259 66,467 212,690 78,019\nEtri\n|Etri_D| 155,394 45,001 185,269 73,686\n|Etri_N| 112,787 31,382 74,017 25,701\n|Etri| 268,181 76,383 259,286 99,387\nEqual\n|Equal_D| 3,894 4 9,416 3,775\n|Equal_N| 9,218 8,008 2,639 500\n|Equal| 13,112 8,012 12,055 4,275\n5.2 Link Prediction Results\nWe show the performance of link prediction using standard metrics:\nMean Reciprocal Rank (MRR), Hit@10, Hit@3, and Hit@1; higher\nvalues indicate better performance [21]. Since TransEA, MT-KGNN,\nKBLN, and LiteralE cannot consider qualifiers, we first consider the\nlink prediction only on the primary triplets, i.e., we consider the case\nwhere the missing entity is only in a primary triplet. Table 2 shows\nthe link prediction results on the primary triplets, where the best\nresults are boldfaced and the second-best results are underlined. We\nsee that the transformer-based hyper-relational knowledge graph\nembedding methods, StarE, Hy-Transformer, and GRAN show bet-\nter performance than the other baselines. More importantly, HyNT\nsignificantly outperforms all the baseline methods in terms of all\nmetrics on HN-WK and HN-YG. On HN-FB-S, HyNT outperforms\nall baselines in terms of Hit@10 and Hit@3, shows comparable\nperformance to RAM in MRR, and lags behind the best-performing\nbaselines in Hit@1. When predicting a missing entity in a primary\ntriplet, the answer can be changed depending on its qualifiers. For\nexample, Table 3 shows examples of the predictions made by HyNT\non HN-WK where HyNT successfully predicts the answers.\nWe also consider the case where the missing entity can be placed\neither in a primary triplet or a qualifier. Table 4 shows the results in\nHN-WK and HN-FB-S. In HN-YG, all qualifiers in the test set contain\nonly numeric entities. Therefore, for HN-YG, the link prediction\nresults on all entities of the hyper-relational facts are identical to\nthose on the primary triplets. While StarE and Hy-Transformer\ncan handle the hyper-relational facts, their implementations are\nprovided only for the prediction on the primary triplets but not for\nthe qualifiers. Thus, we could not report the results of these methods.\nTable 4 shows that HyNT achieves the best performance among all\nthe methods in HN-WK in terms of all metrics, while HyNT shows\ncomparable performance to GRAN in HN-FB-S. The performance\ngap between HyNT and the best baseline, GRAN, is substantial in\nHN-WK. This is because HN-WK contains diverse numeric literals,\nand GRAN does not effectively use that information, whereas HyNT\ncan appropriately interpret and utilize numeric literals.\nRepresentation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nTable 2: Link Prediction Results on the Primary Triplets in HN-WK, HN-YG, and HN-FB-S. The best results are boldfaced and the\nsecond-best results are underlined. Our model, HyNT, significantly outperforms all baseline methods in terms of all metrics.\nHN-WK HN-YG HN-FB-S\nMRR Hit@10 Hit@3 Hit@1 MRR Hit@10 Hit@3 Hit@1 MRR Hit@10 Hit@3 Hit@1\nTransEA 0.1413 0.2921 0.1728 0.0613 0.1397 0.2707 0.1738 0.0660 0.2565 0.4647 0.2912 0.1554\nMT-KGNN 0.1448 0.2244 0.1566 0.1035 0.1377 0.2181 0.1474 0.0998 0.2509 0.4383 0.2758 0.1612\nKBLN 0.1500 0.2275 0.1620 0.1112 0.1540 0.2231 0.1690 0.1161 0.2406 0.4303 0.2586 0.1528\nLiteralE 0.1635 0.2563 0.1792 0.1182 0.1577 0.2459 0.1733 0.1133 0.2632 0.4612 0.2885 0.1701\nNaLP 0.1326 0.2303 0.1359 0.0771 0.0838 0.1529 0.0864 0.0489 0.3721 0.5602 0.4283 0.2726\ntNaLP 0.1419 0.2446 0.1460 0.0859 0.1074 0.1790 0.1163 0.0690 0.3410 0.5165 0.3899 0.2499\nRAM 0.1696 0.2986 0.1746 0.1032 0.1682 0.2641 0.1862 0.1185 0.5077 0.6269 0.5402 0.4432\nHINGE 0.1706 0.2880 0.1770 0.1051 0.1493 0.2463 0.1640 0.1003 0.4147 0.6262 0.4729 0.3060\nNeuInfer 0.1621 0.2651 0.1707 0.1052 0.1213 0.2042 0.1327 0.0774 0.2872 0.4725 0.3297 0.1943\nStarE 0.2177 0.3523 0.2268 0.1438 0.1826 0.2907 0.2042 0.1262 0.4707 0.6521 0.5254 0.3734\nHy-Transformer 0.2438 0.3463 0.2567 0.1812 0.1884 0.2968 0.2070 0.1335 0.4911 0.6577 0.5410 0.4012\nGRAN 0.2627 0.3761 0.2738 0.2029 0.1951 0.3137 0.2223 0.1319 0.5028 0.6570 0.5488 0.4203\nHyNT 0.3037 0.5082 0.3228 0.2084 0.2035 0.3147 0.2237 0.1474 0.5079 0.7037 0.5610 0.4063\nTable 3: Link Prediction Results of HyNT on HN-WK. The predictions made by HyNT are changed depending on the qualifiers.\nHyNT successfully predicts the missing entities in the primary triplets by considering their qualifiers.\nLink Prediction Problem Prediction\n((?, nominated for, Best Actor), {(for work, Moneyball), (subject of, 84th Academy Awards)}) Brad Pitt\n((?, nominated for, Best Actor), {(for work, Forrest Gump), (subject of, 67th Academy Awards)}) Tom Hanks\n((?, diplomatic relation, Nicaragua), {(start time, 1979-08-21)}) North Korea\n((?, diplomatic relation, Nicaragua), {(start time, 1985-12-07), (end time, 1990-11-09)}) China\n((London, country, ?), {(start time, 927), (end time, 1707-04-30)}) Kingdom of England\n((London, country, ?), {(start time, 1922-12-06)}) United Kingdom\n((Best Actress, winner, ?), {(for work, The Iron Lady), (point in time, 2011)}) Meryl Streep\n((Best Actress, winner, ?), {(for work, The Hours), (point in time, 2002)}) Nicole Kidman\nTable 4: Link Prediction Results on All Entities of Hyper-\nRelational Facts in HN-WK and HN-FB-S.\nMRR Hit@10 Hit@3 Hit@1\nHN-WK\nNaLP 0.1541 0.2560 0.1602 0.0965\ntNaLP 0.1616 0.2693 0.1685 0.1030\nRAM 0.1766 0.3079 0.1830 0.1092\nHINGE 0.1876 0.3074 0.1961 0.1209\nNeuInfer 0.1740 0.2808 0.1849 0.1153\nGRAN 0.2901 0.4020 0.3019 0.2310\nHyNT 0.3254 0.5261 0.3459 0.2314\nHN-FB-S\nNaLP 0.4578 0.6414 0.5160 0.3606\ntNaLP 0.4357 0.6069 0.4865 0.3460\nRAM 0.5788 0.6874 0.6085 0.5205\nHINGE 0.4809 0.6893 0.5412 0.3736\nNeuInfer 0.3414 0.5334 0.3836 0.2466\nGRAN 0.5873 0.7223 0.6286 0.5149\nHyNT 0.5796 0.7586 0.6323 0.4857\nRecall that the link prediction task is to predict discrete entities\nbut not numeric entities, according to our definition in Section 3.3.\nIn Table 2 and Table 4, all methods solve the same problems where\nthe missing entities are originally discrete entities. Even though we\nconsider the numeric literals as discrete entities in NaLP, tNaLP,\nRAM, HINGE, NeuInfer, StarE, Hy-Transformer, and GRAN, we\nexclude the predictions on numeric literals when measuring the\nlink prediction performance of these methods for a fair comparison.\nInstead, we use the predictions on numeric literals to measure\nthe numeric value prediction performance of the aforementioned\nmethods, which will be presented in Section 5.4.\n5.3 Relation Prediction Results\nWhile all baseline methods provide the implementation of link pre-\ndiction, relation prediction was only implemented in NaLP, tNaLP,\nHINGE, NeuInfer, and GRAN. We compare the relation prediction\nperformance of HyNT with these methods. Table 5 shows the re-\nsults of relation prediction on the primary triplets (Tri) and all\nrelations of the hyper-relational facts (All). In HN-WK and HN-YG,\nwe see that HyNT achieves the highest MRR, Hit@10, Hit@3, and\nHit@1. In HN-FB-S, both HyNT and GRAN achieve over 99% Hit@3,\nindicating that these methods almost always correctly predict a\nmissing relation within the top 3 predictions.\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Chanyoung Chung, Jaejun Lee, and Joyce Jiyoung Whang\nTable 5: Relation Prediction Results on the Primary Triplets (Tri) and All Relations of the Hyper-Relational Facts (All). Overall,\nHyNT outperforms the baseline methods in relation prediction.\nHN-WK HN-YG HN-FB-S\nMRR Hit@10 Hit@3 Hit@1 MRR Hit@10 Hit@3 Hit@1 MRR Hit@10 Hit@3 Hit@1\nTri\nNaLP 0.5682 0.6886 0.6014 0.5002 0.3419 0.5696 0.3926 0.2113 0.5539 0.6776 0.5889 0.4760\ntNaLP 0.5967 0.7542 0.6316 0.5164 0.4273 0.6840 0.4817 0.2957 0.5330 0.6873 0.5645 0.4528\nHINGE 0.8806 0.9700 0.9183 0.8295 0.8174 0.9538 0.8786 0.7400 0.9684 0.9964 0.9871 0.9483\nNeuInfer 0.7485 0.8929 0.7929 0.6734 0.6437 0.8965 0.7175 0.5162 0.7643 0.9427 0.8630 0.6523\nGRAN 0.9285 0.9899 0.9615 0.8898 0.8347 0.9383 0.8858 0.7697 0.9845 0.9947 0.9934 0.9755\nHyNT 0.9474 0.9905 0.9789 0.9145 0.8797 0.9851 0.9379 0.8135 0.9815 0.9994 0.9966 0.9669\nAll\nNaLP 0.7410 0.8245 0.7684 0.6927 0.4224 0.6222 0.4669 0.3078 0.7010 0.8306 0.7748 0.6084\ntNaLP 0.7625 0.8631 0.7912 0.7081 0.4946 0.7220 0.5441 0.3775 0.6545 0.8345 0.7471 0.5348\nHINGE 0.9278 0.9828 0.9527 0.8953 0.8397 0.9594 0.8935 0.7718 0.9675 0.9981 0.9924 0.9431\nNeuInfer 0.8320 0.9367 0.8800 0.7671 0.6803 0.9054 0.7479 0.5655 0.7980 0.9692 0.9098 0.6838\nGRAN 0.9599 0.9941 0.9784 0.9382 0.8548 0.9459 0.8998 0.7977 0.9918 0.9972 0.9965 0.9870\nHyNT 0.9706 0.9947 0.9881 0.9522 0.8944 0.9869 0.9455 0.8363 0.9902 0.9997 0.9982 0.9825\nTable 6: Numeric Value Prediction Results on the Primary\nTriplets (Tri) and All Numeric Values in the Hyper-Relational\nFacts (All) in terms of RMSE ( â†“).\nHN-WK HN-YG HN-FB-S\nTri All Tri All Tri All\nTransEA 0.0772 - 0.0778 - 0.1332 -\nMT-KGNN 0.1390 - 0.1203 - 0.0908 -\nKBLN 0.1550 - 0.1342 - 0.0890 -\nLiteralE 0.2104 - 0.1783 - 0.0801 -\nNaLP 0.2329 0.1681 0.1399 0.1375 0.1055 0.0894\ntNaLP 0.2312 0.1673 0.1185 0.1176 0.0929 0.0923\nRAM 0.1102 0.0820 0.0969 0.1132 0.0706 0.0627\nHINGE 0.2435 0.1752 0.1141 0.1123 0.1077 0.0939\nNeuInfer 0.2425 0.1744 0.1176 0.1396 0.1093 0.1004\nStarE 0.0832 - 0.0990 - 0.0670 -\nHy-Transformer 0.0761 - 0.0972 - 0.0656 -\nGRAN 0.2293 0.1667 0.1180 0.1247 0.0835 0.0773\nHyNT 0.0548 0.0405 0.0706 0.0694 0.0532 0.0499\n5.4 Numeric Value Prediction Results\nTable 6 shows the numeric value prediction results in terms of the\nRoot-Mean-Square Error (RMSE). The lower RMSE indicates better\nperformance. Since the scales of the numeric values vary depending\non the types of attributes, e.g., the running time of a TV episode\nvaries from 10 minutes to 360 minutes, whereas the number of days\nin a month varies from 28 days to 31 days; we apply the min-max\nnormalization for each attribute type to rescale all numeric values\nfrom 0 to 1. Since TransEA, MT-KGNN, KBLN, and LiteralE cannot\nhandle hyper-relational facts, we first measure the performance on\nthe primary triplets where the missing values are placed only in the\nprimary triplets (denoted by Tri). We also consider the case where\nthe missing numeric values can be either on a primary triplet or\na qualifier (denoted by All). Note that StarE and Hy-Transformer\ndo not provide the implementations of the prediction on the quali-\nfiers. In Table 6, we see that HyNT achieves the least RMSE on all\ndatasets in all settings. We further analyze the results by selecting\na few attribute types among the diverse numeric literals. Table 7\nTable 7: Numeric Value Prediction Results per Attribute Type\nin HN-WK in terms of RMSE. HyNT shows better perfor-\nmance than all other methods for each attribute type.\nPrimary Triplet Qualifier\nranking HDI fertility rate point in time\nFrequency 11,951 708 353 18,613\nMinimum value 1.0 0.190 0.827 1.0\nMaximum value 246.0 0.957 7.742 2285.2\nStandard deviation 49.2 0.166 1.594 35.8\nTransEA 19.3 0.049 0.279 -\nMT-KGNN 40.4 0.079 0.569 -\nKBLN 35.2 0.130 0.782 -\nLiteralE 47.7 0.168 0.900 -\nNaLP 68.2 0.074 0.760 94.4\ntNaLP 67.1 0.065 1.149 81.8\nRAM 22.7 0.066 1.472 24.9\nHINGE 70.5 0.077 1.363 91.8\nNeuInfer 71.6 0.067 0.967 93.9\nStarE 18.0 0.046 0.380 -\nHy-Transformer 16.0 0.032 0.508 -\nGRAN 67.7 0.057 0.758 88.5\nHyNT 12.4 0.023 0.222 21.0\nshows the numeric value prediction performance on â€˜rankingâ€™, â€˜Hu-\nman Development Index (HDI)â€™, â€˜fertility rateâ€™, and â€˜point in timeâ€™\nwhere we present the raw data without rescaling. For â€˜point in\ntimeâ€™, we convert the date into a real number, e.g., converting Janu-\nary 28th, 1922, into 1922 + 28/365. In Table 7, we also present the\nfrequency, the minimum, the maximum, and the standard deviation\nof each attribute type. We see that HyNT shows the best perfor-\nmance among all the methods. We also consider a numeric value\nprediction problem in the form of ((â„,ğ‘Ÿ, ?),{(point in time,ğ‘£1)}),\n((â„,ğ‘Ÿ, ?),{(point in time,ğ‘£2)}), Â·Â·Â· in HN-WK where ğ‘£1,ğ‘£2,Â·Â·Â· cor-\nrespond to different points in time. Let us focus on the missing\nnumeric values in the primary triplet and call them target values.\nBy visualizing the target values associated with different qualifiers\nindicating different points in time, we trace how the target values\nchange over time. In Figure 4, we show HyNTâ€™s predictions and the\nRepresentation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\n2000 2004 2008 2012\nPoint in time (year)\n1\n2\n3\n4\n5\n6\n7\n8Total fertility rate\n(Uruguay, total fertility rate, ?)\nGround Truth\nPredictions by HyNT\n(a) Uruguay\n2000 2004 2008 2012\nPoint in time (year)\n1\n2\n3\n4\n5\n6\n7\n8Total fertility rate\n(Ghana, total fertility rate, ?)\nGround Truth\nPredictions by HyNT (b) Ghana\nFigure 4: Comparison between HyNTâ€™s predictions and the\nground-truth values of the total fertility rates of Uruguay\nand Ghana over time.\n1995 2000 2005 2010\nPoint in time (year)\n0\n50\n100\n150\n200\n250Ranking\n(Sweden National Team, ranking, ?)\nGround Truth\nPredictions by HyNT\n(a) Sweden National Team\n1995 2000 2005 2010 2015\nPoint in time (year)\n0\n50\n100\n150\n200\n250Ranking\n(India National Team, ranking, ?)\nGround Truth\nPredictions by HyNT (b) India National Team\nFigure 5: Comparison between HyNTâ€™s predictions and the\nground-truth values of rankings of Sweden National Team\nand India National Team over time.\nground-truth values of the total fertility rate of Uruguay and Ghana\nover time. In Figure 5, we show the rankings of Sweden National\nTeam and India National Team over time. The dotted lines indicate\nthe minimum and the maximum values. We see that the predictions\nmade by HyNT are very close to the ground-truth numeric values.\n5.5 Ablation Studies of HyNT\nTable 8 shows the link prediction and numeric value prediction\nresults of HyNT for all entities and numeric values of the hyper-\nrelational facts in HN-WK where we treat the numeric values as\ndiscrete entities (denoted by Discrete), or we treat them as numeric\nentities (denoted by Numeric). As described in Section 5.1, a straight-\nforward way to handle numeric entities in a hyper-relational knowl-\nedge graph is to treat them as discrete entities like how we feed the\nnumeric entities into the baseline methods that are not designed\nfor numeric literals. In Table 8, we see that the way we handle\nnumeric values in HyNT is much more effective, leading to better\nperformance on both link prediction and numeric value prediction.\nTable 9 shows the ablation studies of HyNT on HN-WK. We\nanalyze MRR scores of link and relation predictions and RMSE of\nnumeric value prediction. We consider the case where the predic-\ntions are made only on the primary triplets (Tri) and the case where\nthe predictions are made on all components of the hyper-relational\nfacts (All). In Section 4.4, we introduce the masking strategy where\nwe mask a component in a hyper-relational fact. The first five rows\nin Table 9 indicate the cases where we do not mask some types\nTable 8: Link Prediction and Numeric Value Prediction Using\nDifferent Handling of Numeric Entities in HyNT on HN-WK.\nMRR (â†‘) Hit@10 ( â†‘) Hit@3 ( â†‘) Hit@1 ( â†‘) RMSE ( â†“)\nDiscrete 0.2478 0.3713 0.2621 0.1827 0.2267\nNumeric 0.3254 0.5259 0.3459 0.2314 0.0405\nTable 9: Link/Relation/Numeric Value Predictions on the Pri-\nmary Triplets (Tri) and All Components in Hyper-relational\nfacts (All) for Ablation Studies of HyNT on HN-WK.\nLink(MRRâ†‘) Relation(MRR â†‘) Value(RMSE â†“)\nTri All Tri All Tri All\nw/o masking VN, R, Equal 0.277 0.264 0.014 0.018 0.507 0.733\nw/o masking R, Equal 0.291 0.278 0.074 0.043 0.064 0.112\nw/o masking VN 0.291 0.312 0.939 0.966 0.429 0.786\nw/o masking R 0.259 0.272 0.015 0.010 0.059 0.042\nw/o masking Equal 0.256 0.244 0.936 0.514 0.064 0.329\nw/o prediction transformer 0.275 0.296 0.930 0.961 0.060 0.043\nw/ Hadamard encoding 0.258 0.255 0.939 0.964 0.064 0.048\nHyNT 0.304 0.325 0.947 0.971 0.055 0.040\nof components. We see that if we do not mask relations ( R), the\nrelation prediction performance significantly degrades. Similarly,\nif we do not mask numeric entities ( VN), the numeric value pre-\ndiction performance degrades. Also, if we do not mask qualifiers\n(Equal), â€˜Allâ€™ performances degrade because the model is not trained\nto perform predictions on the qualifiers. On the other hand, we also\nreplace a prediction transformer with a simple linear projection\nlayer (denoted by w/o prediction transformer). Lastly, we change the\ntriplet encoding and the qualifier encoding in Section 4.1 by replac-\ning them with the Hadamard product, i.e., we computextri = hâ—¦râ—¦t\nand xqualğ‘– = qğ‘– â—¦vğ‘– (denoted by w/ Hadamard encoding). While the\nlast two variations are not as critical as removing maskings, HyNT\nachieves the best performance when all pieces are together.\n6 CONCLUSION & FUTURE WORK\nWe propose HyNT which is a transformer-based representation\nlearning method for hyper-relational knowledge graphs having var-\nious numeric literals. One of the key ideas is to learn representations\nof a primary triplet and each qualifier by allowing them to exchange\ninformation with each other, where the relative importance is also\nlearned using an attention mechanism. HyNT considers link pre-\ndiction, numeric value prediction, and relation prediction tasks\nto compute embeddings of entities and relations, where entities\ncan be either discrete or numeric. Experiments show that HyNT\nsignificantly outperforms 12 different state-of-the-art methods.\nWe will extend HyNT to an inductive learning setting which\nassumes that some entities and relations can only appear at test\ntime [1, 26]. Also, we plan to consider incorporating text descrip-\ntions or images into HyNT. Additionally, we will explore how HyNT\ncan be utilized in various domains, such as conditional link predic-\ntion or triplet prediction [9] and applications requiring advanced\nknowledge representation [25].\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Chanyoung Chung, Jaejun Lee, and Joyce Jiyoung Whang\nACKNOWLEDGMENTS\nThis research was partly supported by NRF grants funded by MSIT\n(2022R1A2C4001594 and 2018R1A5A1059921). This work was also\nsupported by IITP grants funded by MSIT 2022-0-00369 (Develop-\nment of AI Technology to support Expert Decision-making that\ncan Explain the Reasons/Grounds for Judgment Results based on\nExpert Knowledge) and 2020-0-00153 (Penetration Security Testing\nof ML Model Vulnerabilities and Defense).\nREFERENCES\n[1] Mehdi Ali, Max Berrendorf, Mikhail Galkin, Veronika Thost, Tengfei Ma, Volker\nTresp, and Jens Lehmann. 2021. Improving Inductive Link Prediction Using\nHyper-relational Facts. In Proceedings of the 20th International Semantic Web\nConference. 74â€“92. https://doi.org/10.1007/978-3-030-88361-4_5\n[2] Dimitrios Alivanistos, Max Berrendorf, Michael Cochez, and Mikhail Galkin.\n2022. Query Embedding on Hyper-Relational Knowledge Graphs. In Proceedings\nof the 10th International Conference on Learning Representations .\n[3] SÃ¶ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,\nand Zachary Ives. 2007. DBpedia: A Nucleus for a Web of Open Data. In Proceed-\nings of the 6th International Semantic Web Conference and the 2nd Asian Semantic\nWeb Conference. 722â€“735. https://doi.org/10.1007/978-3-540-76298-0_52\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Everest Hinton. 2016. Layer Nor-\nmalization. (2016). https://doi.org/10.48550/arXiv.1607.06450 arXiv:1607.06450\n[5] Eda Bayram, Alberto Garcia-Duran, and Robert West. 2021. Node Attribute\nCompletion in Knowledge Graphs with Multi-Relational Propagation. In Pro-\nceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal\nProcessing. 3590â€“3594. https://doi.org/10.1109/ICASSP39728.2021.9414016\n[6] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.\n2008. Freebase: A Collaboratively Created Graph Database for Structuring Human\nKnowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on\nManagement of Data . 1247â€“1250. https://doi.org/10.1145/1376616.1376746\n[7] Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao, Ruofei Zhang, and Yangfeng\nJi. 2021. HittER: Hierarchical Transformers for Knowledge Graph Embeddings.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing. 10395â€“10407. https://doi.org/10.18653/v1/2021.emnlp-main.812\n[8] Chanyoung Chung and Joyce Jiyoung Whang. 2021. Knowledge Graph Embed-\nding via Metagraph Learning. In Proceedings of the 44th International ACM SIGIR\nConference on Research and Development in Information Retrieval . 2212â€“2216.\nhttps://doi.org/10.1145/3404835.3463072\n[9] Chanyoung Chung and Joyce Jiyoung Whang. 2023. Learning Representations of\nBi-Level Knowledge Graphs for Reasoning beyond Link Prediction. InProceedings\nof the 37th AAAI Conference on Artificial Intelligence . 4208â€“4216. https://doi.org/\n10.1609/aaai.v37i4.25538\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers) . 4171â€“4186. https://doi.org/10.18653/v1/N19-1423\n[11] Shimin Di, Quanming Yao, and Lei Chen. 2021. Searching to Sparsify Tensor\nDecomposition for N-ary Relational Data. In Proceedings of The Web Conference\n2021. 4043â€“4054. https://doi.org/10.1145/3442381.3449853\n[12] Bahare Fatemi, Perouz Taslakian, David Vazquez, and David Poole. 2020. Knowl-\nedge Hypergraphs: Prediction Beyond Binary Relations. In Proceedings of the\n29th International Joint Conference on Artificial Intelligence . 2191â€“2197. https:\n//doi.org/10.24963/ijcai.2020/303\n[13] Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens\nLehmann. 2020. Message Passing for Hyper-Relational Knowledge Graphs. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing. 7346â€“7359. https://doi.org/10.18653/v1/2020.emnlp-main.596\n[14] Alberto Garcia-Duran and Mathias Niepert. 2018. KBLRN: End-to-end Learning of\nKnoweldge Base Representations with Latent, Relational, and Numerical Features.\nIn Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence . 372â€“\n381.\n[15] Genet Asefa Gesese, Russa Biswas, Mehwish Alam, and Harald Sack. 2021. A\nSurvey on Knowledge Graph Embeddings with Literals: Which Model Links\nBetter Literal-ly? Semantic Web 12, 4 (2021), 617â€“647. https://doi.org/10.3233/SW-\n200404\n[16] Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, and Xueqi Cheng.\n2020. NeuInfer: Knowledge Inference on N-ary Facts. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics . 6141â€“6151.\nhttps://doi.org/10.18653/v1/2020.acl-main.546\n[17] Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, and Xueqi Cheng.\n2021. Link Prediction on N-ary Relational Data Based on Relatedness Evaluation.\nIEEE Transactions on Knowledge and Data Engineering (2021). https://doi.org/10.\n1109/TKDE.2021.3073483\n[18] Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. 2019. Link\nPrediction on N-ary Relational Data. In Proceedings of The Web Conference 2019 .\n583â€“593. https://doi.org/10.1145/3308558.3313414\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nLearning for Image Recognition. In Proceedings of the 2016 IEEE Conference on\nComputer Vision and Pattern Recognition . 770â€“778. https://doi.org/10.1109/CVPR.\n2016.90\n[20] Vinh Thinh Ho, Daria Stepanova, Dragan Milchevski, Jannik StrÃ¶tgen, and Ger-\nhard Weikum. 2022. Enhancing Knowledge Bases with Quantity Facts. In Pro-\nceedings of The Web Conference 2022 . 893â€“901. https://doi.org/10.1145/3485447.\n3511932\n[21] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. 2022.\nA Survey on Knowledge Graphs: Representation, Acquisition, and Applications.\nIEEE Transactions on Neural Networks and Learning Systems 33, 2 (2022), 494â€“514.\nhttps://doi.org/10.1109/TNNLS.2021.3070843\n[22] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-\nmization. In Proceedings of the 3rd International Conference on Learning Represen-\ntations.\n[23] Bhushan Kotnis and Alberto Garcia-Duran. 2019. Learning Numerical Attributes\nin Knowledge Bases. In Proceedings of the 1st Conference on Automated Knowledge\nBase Construction . https://doi.org/10.24432/C5Z59Q\n[24] Agustinus Kristiadi, Mohammad Asif Khan, Denis Lukovnikov, Jens Lehmann,\nand Asja Fischer. 2019. Incorporating Literals into Knowledge Graph Embeddings.\nIn Proceedings of the 18th International Semantic Web Conference . 347â€“363. https:\n//doi.org/10.1007/978-3-030-30793-6_20\n[25] Ji Ho Kwak, Jaejun Lee, Joyce Jiyoung Whang, and Sungho Jo. 2022. Semantic\nGrasping Via a Knowledge Graph of Robotic Manipulation: A Graph Represen-\ntation Learning Approach. IEEE Robotics and Automation Letters 7, 4 (2022),\n9397â€“9404. https://doi.org/10.1109/LRA.2022.3191194\n[26] Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang. 2023. InGram: Induc-\ntive Knowledge Graph Embedding via Relation Graphs. In Proceedings of the 40th\nInternational Conference on Machine Learning . 18796â€“18809.\n[27] Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio,\nand David S. Rosenblum. 2019. MMKG: Multi-modal Knowledge Graphs. In\nProceedings of the 16th International Semantic Web Conference . 459â€“474. https:\n//doi.org/10.1007/978-3-030-21348-0_30\n[28] Yu Liu, Quanming Yao, and Yong Li. 2020. Generalizing Tensor Decomposition\nfor N-ary Relational Knowledge Bases. In Proceedings of The Web Conference 2020 .\n1104â€“1114. https://doi.org/10.1145/3366423.3380188\n[29] Yu Liu, Quanming Yao, and Yong Li. 2021. Role-Aware Modeling for N-ary\nRelational Knowledge Bases. In Proceedings of The Web Conference 2021 . 2660â€“\n2671. https://doi.org/10.1145/3442381.3449874\n[30] Ilya Loshchilov and Frank Hutter. 2017. SGDR: Stochastic Gradient Descent with\nWarm Restarts. In Proceedings of the 5th International Conference on Learning\nRepresentations.\n[31] Farzaneh Mahdisoltani, Joanna Biega, and Fabian M. Suchanek. 2015. YAGO3: A\nKnowledge Base from Multilingual Wikipedias. In Proceedings of the 7th Biennial\nConference on Innovative Data Systems Research .\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-\nmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\nLibrary. In Proceedings of the 33rd Conference on Neural Information Processing\nSystems. 8026â€“8037.\n[33] Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. 2018. Embedding Multimodal\nRelational Data for Knowledge Base Completion. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing . 3208â€“3218.\nhttps://doi.org/10.18653/v1/D18-1359\n[34] Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Denny Zhou, Jure Leskovec,\nand Dale Schuurmans. 2022. SMORE: Knowledge Graph Completion and Multi-\nhop Reasoning in Massive Knowledge Graphs. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining . 1472â€“1482. https:\n//doi.org/10.1145/3534678.3539405\n[35] Paolo Rosso, Dingqi Yang, and Philippe Cudre-Mauroux. 2020. Beyond Triplets:\nHyper-Relational Knowledge Graph Embedding for Link Prediction. In Proceed-\nings of The Web Conference 2020 . 1885â€“1896. https://doi.org/10.1145/3366423.\n3380257\n[36] Guus Schreiber, VU University Amsterdam, Yves Raimond, and BBC. 2014. RDF\n1.1 Primer. Retrieved January 31, 2023 from https://www.w3.org/TR/2014/NOTE-\nrdf11-primer-20140624/\n[37] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting. Journal of Machine Learning Research 15, 56 (2014), 1929â€“1958.\n[38] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core\nof Semantic Knowledge. In Proceedings of the 16th International Conference on\nWorld Wide Web. 697â€“706. https://doi.org/10.1145/1242572.1242667\nRepresentation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\n[39] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew\nWojna. 2016. Rethinking the Inception Architecture for Computer Vision. In\nProceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition .\n2818â€“2826. https://doi.org/10.1109/CVPR.2016.308\n[40] Thomas Pellissier Tanon, Denny VrandeÄiÄ‡, Sebastian Schaffert, Thomas Steiner,\nand Lydia Pintscher. 2016. From Freebase to Wikidata: The Great Migration. In\nProceedings of the 25th International Conference on World Wide Web . 1419â€“1428.\nhttps://doi.org/10.1145/2872427.2874809\n[41] Yi Tay, Luu Anh Tuan, Minh C. Phan, and Siu Cheung Hui. 2017. Multi-Task\nNeural Network for Non-discrete Attribute Prediction in Knowledge Graphs.\nIn Proceedings of the 26th ACM International Conference on Information and\nKnowledge Management . 1029â€“1038. https://doi.org/10.1145/3132847.3132937\n[42] Kristina Toutanova and Danqi Chen. 2015. Observed versus Latent Features\nfor Knowledge Base and Text Inference. In Proceedings of the 3rd Workshop\non Continuous Vector Space Models and their Compositionality . 57â€“66. https:\n//doi.org/10.18653/v1/W15-4007\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Proceedings of the 31st Conference on Neural Information Processing\nSystems. 5998â€“6008.\n[44] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In Proceedings of the\n6th International Conference on Learning Representations .\n[45] Denny VrandeÄiÄ‡ and Markus KrÃ¶tzsch. 2014. Wikidata: A Free Collaborative\nKnowledgebase. Commun. ACM 57, 10 (2014), 78â€“85. https://doi.org/10.1145/\n2629489\n[46] Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying Wang, and Yi Chang. 2021.\nStructure-Augmented Text Representation Learning for Efficient Knowledge\nGraph Completion. In Proceedings of The Web Conference 2021 . 1737â€“1748. https:\n//doi.org/10.1145/3442381.3450043\n[47] Quan Wang, Haifeng Wang, Yajuan Lyu, and Yong Zhu. 2021. Link Prediction on\nN-ary Relational Facts: A Graph-based Approach. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021 . 396â€“407. https://doi.org/10.\n18653/v1/2021.findings-acl.35\n[48] Jianfeng Wen. 2016. The JF17K Dataset . Retrieved January 31, 2023 from\nhttps://www.site.uottawa.ca/~yymao/JF17K/\n[49] Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. 2016. On\nthe Representation and Embedding of Knowledge Bases beyond Binary Relations.\nIn Proceedings of the 25th International Joint Conference on Artificial Intelligence .\n1300â€“1307.\n[50] Yanrong Wu and Zhichun Wang. 2018. Knowledge Graph Embedding with Nu-\nmeric Attributes of Entities. In Proceedings of the 3rd Workshop on Representation\nLearning for NLP . 132â€“136. https://doi.org/10.18653/v1/W18-3017\n[51] Donghan Yu and Yiming Yang. 2022. Improving Hyper-Relational Knowl-\nedge Graph Completion. (2022). https://doi.org/10.48550/arXiv.2104.08167\narXiv:2104.08167\n[52] Richong Zhang, Junpeng Li, Jiajie Mei, and Yongyi Mao. 2018. Scalable Instance\nReconstruction in Knowledge Bases via Relatedness Affiliated Embedding. In\nProceedings of The Web Conference 2018 . 1185â€“1194. https://doi.org/10.1145/\n3178876.3186017\nKDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA Chanyoung Chung, Jaejun Lee, and Joyce Jiyoung Whang\nAPPENDIX\nA DETAILS ABOUT DATASETS\nWe create three real-world HN-KGs, named HN-WK, HN-YG, and\nHN-FB. Similar to [ 13, 27], we construct the datasets consisting\nof the entities in FB15K237 [42] and their related numeric values.\nFor HN-WK, we first extract all hyper-relational facts about the\nentities in Wikidata [45] that are mapped from FB15K237. Then,\nwe filter the triplets and qualifiers so that the remaining entities\nare either from FB15K237 or numeric values. For HN-YG, since\nthere is no direct mapping from Freebase [6] to YAGO [38], we\nfirst map FB15K237 entities to DBpedia [3], and map the converted\nentities to YAGO3 [31]. We extract the triplets whose head and\ntail entities match FB15K237 entities and also extract the numeric\nliterals of the extracted entities. Then, the qualifiers of the extracted\ntriplets are added if the entities of the qualifiers match theFB15K237\nentities or the entities in the qualifiers are numeric literals. For HN-\nFB, we first transform Compound Value Types (CVTs) inFreebase\ninto hyper-relational facts by following [ 40]. Similar to HN-WK,\nwe filter the triplets and qualifiers so that the remaining entities\nare either from FB15K237 or numeric literals. When we create our\ndatasets, we remove inverse and near-duplicate relations to prevent\ndata leakage problems by following [42].\nWe have inspected the minimum and the maximum values of\nnumeric entities per relation and found that the raw data include\nsome incorrect information. We have filtered out those incorrect\ntriplets or qualifiers. For example, the triplet (Glee, number of\nseasons, 91) was removed since Glee has a total of 6 seasons. In the\nprocess of removing the incorrect triplets, 0.04% (HN-WK), 0.52%\n(HN-YG), and 0.02% (HN-FB) of triplets were removed.\nB DETAILS ABOUT BASELINE METHODS\nFor the hyperparameters of the baseline methods, we follow the\nnotation from the original papers. For all models, we set the em-\nbedding dimension to 256 unless otherwise stated. We conduct our\nexperiments using GeForce RTX 2080 Ti, GeForce RTX 3090, or\nRTX A6000 by considering the dependencies of the implementa-\ntions of each method. We run NaLP, tNaLP, HINGE, NeuInfer, StarE,\nHy-Transformer, and GRAN using GeForce RTX 2080 Ti, and run\nRAM using GeForce RTX 3090 and RTX A6000, and run TransEA,\nMT-KGNN, KBLN, LiteralE, and HyNT using GeForce RTX 2080 Ti,\nGeForce RTX 3090, and RTX A6000.\nTransEA [ 50]. We re-implemented TransEA using PyTorch\nsince the original code has inconsistencies with the description\nin the paper: in the original code, the bias term is not computed\nper attribute. We perform a grid search on ğœ† = {0.1,0.5,1.0},\nğ›¾ = {2.5,5.0,7.5,10.0}, and ğ›¼ = {0.25,0.5,0.75}. We perform valida-\ntion every 50 epochs up to 500 epochs and select the best hyperpa-\nrameters using the validation set.\nMT-KGNN [41]. Since the official implementation of MT-KGNN\nis not provided, we use the implementation of MT-KGNN provided\nin [24]. We modified the implementation so that the regression loss\ndoes not take the entities without numeric literals into account, as\nstated in the original paper. We tune the model with the learning\nrate = {0.0001,0.001}, ğ‘‘ = {0.1,0.2}, and the label smoothing ra-\ntio = {0.0,0.1}for all datasets. We train the model for 100 epochs\nwhere we conduct validation every epoch and select the best epoch.\nKBLN [ 14]. In the original KBLRN [ 14], the module for rela-\ntional features requires extra logical rules. Thus, by following [24],\nwe use KBLN provided in [24] instead of KBLRN for fair compar-\nison. We try the learning rate = {0.0001,0.001,0.01}, the dropout\nrate = {0.1,0.2}and the label smoothing ratio = {0.0,0.1}for all\ndatasets. We first train the model for 100 epochs on a link prediction\ntask, where we perform validation every epoch and select the best\nepoch. Then, we additionally train a linear regression layer for a\nnumeric value prediction task by following [41]. For the training of\nthe linear regression layer, we try the learning rate= {0.0005,0.01}\nand the number of epochs = {100,500}for all datasets.\nLiteralE [24]. Similar to KBLN, we first train LiteralE for 100\nepochs on a link prediction task and then additionally train a linear\nregression layer for a numeric value prediction. We tune LiteralE\nwith the learning rate = {0.001,0.01,0.1}, the embedding dropout\nrate = {0.1,0.2}, the feature map dropout rate = {0.1,0.2}, and the\nprojection layer dropout rate = {0.2,0.3}. For the training of the\nlinear regression layer, we try the learning rate = {0.0005,0.01}\nand the number of epochs = {100,500}for all datasets.\nNaLP [ 18]. Since the official implementation of NaLP saves\nall permutations of qualifiers in the pre-processing step, it is not\nscalable enough to process our datasets; thus, we re-implemented\nNaLP. This also applies to tNaLP, HINGE, and NeuInfer since they\nshare the same implementation for the pre-processing step. On all\ndatasets, we try ğœ†= {0.00001,0.00005,0.0001}, ğ‘›ğ‘“ = {100,200}, and\nğ‘›gFCN = {800,1000,1200}. We perform validation every 500 epochs\nup to 5,000 epochs and select the best hyperparameters.\ntNaLP [17]. On all datasets, we tryğœ†= {0.00001,0.00005,0.0001},\nğ‘›ğ‘“ = {100,200}, ğ‘›gFCN = {800,1000,1200}, and ğ‘›tFCN = {100,200}.\nWe perform validation every 500 epochs up to 5,000 epochs.\nRAM [ 29]. Since the total embedding dimension of entities\nin RAM is ğ‘‘ Ã—ğ‘š, we use ğ‘‘ = 128 and ğ‘š = 2 for HN-YG and\nHN-FB-S. However, due to GPU memory constraints, we use ğ‘‘ =\n32 and ğ‘š = 2 for HN-WK, WikiPeopleâˆ’, and WD50K. We tune\nRAM with the learning rate = {0.001,0.002,0.003,0.005}, the decay\nrate = {0.99,0.995}, and the dropout rate = {0.0,0.2,0.4}for all\ndatasets. We run RAM with a maximum of 200 epochs, with an\nearly stopping strategy adopted. For HN-YG and WikiPeopleâˆ’, we\nperform validation every epoch with the validation patience of 10,\nwhile for HN-WK, WD50K, and HN-FB-S, we perform validation\nevery 5 epochs with the same validation patience.\nHINGE [ 35]. We try ğœ† = {0.00001,0.00005,0.0001}and ğ‘›ğ‘“ =\n{100,200,400}for all datasets. We perform validation every 100\nepochs up to 1,000 epochs.\nNeuInfer [ 16]. We try ğœ† = {0.00001,0.00005,0.0001}, ğ‘‘ =\n{800,1000,1200}, ğ‘›1 = {1,2}, ğ‘›2 = {1,2}, and ğ‘¤ = {0.1,0.3}for all\ndatasets. We perform validation every 500 epochs up to 5,000.\nStarE [13]. We tune StarE with the number of StarE layers =\n{1,2}, the number of transformer layers = {1,2}, the StarE dropout\nrate = {0.2,0.3}, and the transformer dropout rate = {0.1,0.2}for\nall datasets. We run StarE for 400 epochs and perform validation\nevery 5 epochs.\nRepresentation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers KDD â€™23, August 6â€“10, 2023, Long Beach, CA, USA\nHy-Transformer [51]. We tune Hy-Transformer with the num-\nber of transformer layers = {1,2}, the qualifier perturb probability\n= {0.0,0.5,1.0}, the transformer dropout rate = {0.1,0.2}, and the\nentity embedding matrix dropout rate = {0.2,0.3}for all datasets.\nWe run Hy-Transformer for 400 epochs and perform validation\nevery 10 epochs.\nGRAN [47]. We useğ‘ = 256 for HN-WK due to GPU memory\nconstraints. We try ğœŒ = {0.1,0.2,0.3}, ğœ–(ğ‘’)= {0.2,0.4,0.6,0.8}, and\nğœ–(ğ‘Ÿ )= {0.0,0.2,0.4}for all datasets. We perform validation every\n20 epochs up to 200 epochs.\nC EXPERIMENTAL RESULTS ON HN-FB\nAs explained in Section 5.1, among 12 baseline methods, all methods\nhandling hyper-relational facts (i.e., NaLP, tNaLP, RAM, HINGE,\nNeuInfer, StarE, Hy-Transformer, and GRAN) fail to processHN-FB\nsince they are not scalable enough to process this scale of data. The\nremaining four baseline methods (i.e., TransEA, MT-KGNN, KBLN,\nand LiteralE) are the methods dealing with numeric literals but\nnot hyper-relational facts. Note that HyNT is the only method that\ncan handle both numeric literals and hyper-relational facts and can\nprocess HN-FB.\nTable 10 shows the link prediction and numeric value prediction\nresults of HyNT on HN-FB. We report the prediction performance\non the primary triplets (Tri) and all entities of the hyper-relational\nfacts (All). The best results are boldfaced and the second-best results\nare underlined. We see that HyNT significantly outperforms all the\nbaseline methods in terms of all metrics. Also, Table 11 shows the\nresults of relation prediction on the primary triplets (Tri) and all re-\nlations of the hyper-relational facts (All) on HN-FB. Since TransEA,\nMT-KGNN, KBLN, and LiteralE do not provide implementations\nfor relation prediction, we could not report the relation prediction\nresults of these methods. Even though we cannot compare HyNT\nwith other methods onHN-FB, we see that HyNT shows around 97%\nHit@1 on HN-FB, indicating that HyNT correctly predicts missing\nrelations with a 97% chance.\nD EXPERIMENTAL RESULTS ON\nWIKIPEOPLEâˆ’AND WD50K\nIn Section 2, we discuss the well-known benchmark datasets for\nhyper-relational knowledge graphs, JF17K [48, 49], WD50K [13],\nWikiPeopleâˆ’[35], and WikiPeople [18]. Among these, the JF17K\ndataset has been considered problematic because it includes many\nredundant entities that cause data leakage problems. In Table 12,\nwe show link prediction performance on WikiPeopleâˆ’and WD50K\nwhich include hyper-relational facts containing only discrete en-\ntities but not numeric entities. While StarE, Hy-Transformer, and\nGRAN retrained their methods using both training and validation\nsets in their original papers, we trained all methods only using\nthe training set to be consistent with our other experiments. In\nTable 13, we show link prediction performance on WD50K after\nretraining using both training and validation sets. In both tables,\nwe see that HyNT shows better or comparable performance to the\nbest baseline methods depending on the metric and the dataset. We\nprovide the implementations, hyperparameters, and checkpoints\nof HyNT at https://github.com/bdi-lab/HyNT.\nTable 10: Link Prediction and Numeric Value Prediction Re-\nsults on HN-FB. Our model, HyNT, significantly outperforms\nall baseline methods in terms of all metrics.\nMRR (â†‘) Hit@10 ( â†‘) Hit@3 ( â†‘) Hit@1 ( â†‘) RMSE ( â†“)\nTri\nTransEA 0.2327 0.4793 0.2894 0.1084 0.0637\nMT-KGNN 0.2393 0.4270 0.2578 0.1540 0.2072\nKBLN 0.2343 0.4204 0.2523 0.1494 0.0735\nLiteralE 0.2602 0.4559 0.2872 0.1695 0.0678\nHyNT 0.4544 0.6571 0.5036 0.3520 0.0517\nAll HyNT 0.5022 0.7207 0.5561 0.3939 0.0558\nTable 11: Relation Prediction Results of HyNT on HN-FB.\nMRR Hit@10 Hit@3 Hit@1\nTri 0.9809 0.9990 0.9958 0.9662\nAll 0.9860 0.9987 0.9954 0.9766\nTable 12: Link Prediction on WikiPeopleâˆ’and WD50K ob-\ntained by training the models only using the training set.\nWikiPeopleâˆ’ WD50K\nMRR Hit@10 Hit@1 MRR Hit@10 Hit@1\nTri\nNaLP 0.3563 0.4994 0.2713 0.2303 0.3471 0.1697\ntNaLP 0.3577 0.4857 0.2876 0.2205 0.3308 0.1630\nRAM 0.4593 0.5837 0.3843 0.2756 0.3988 0.2104\nHINGE 0.3929 0.5467 0.3092 0.2636 0.4099 0.1870\nNeuInfer 0.3568 0.5326 0.2469 0.2202 0.3466 0.1543\nStarE 0.4579 0.6108 0.3640 0.3087 0.4515 0.2340\nHy-Transformer 0.4597 0.5942 0.3818 0.3041 0.4427 0.2309\nGRAN 0.4616 0.6097 0.3664 0.3299 0.4722 0.2551\nHyNT 0.4820 0.6020 0.4153 0.3332 0.4743 0.2594\nAll\nNaLP 0.3601 0.5034 0.2751 0.2508 0.3747 0.1866\ntNaLP 0.3607 0.4896 0.2900 0.2425 0.3597 0.1815\nRAM 0.4606 0.5850 0.3856 0.2955 0.4156 0.2316\nHINGE 0.3947 0.5488 0.3108 0.2771 0.4244 0.1996\nNeuInfer 0.3569 0.5324 0.2475 0.2254 0.3549 0.1581\nGRAN 0.4654 0.6133 0.3706 0.3605 0.5013 0.2863\nHyNT 0.4811 0.6025 0.4139 0.3599 0.4999 0.2867\nTable 13: Link Prediction on WD50K obtained by training the\nmodels using both training and validation sets. Baseline re-\nsults are either from their original papers or from StarE [ 13].\nWD50K\nMRR Hit@10 Hit@1\nTri\nNaLP 0.1770 0.2640 0.1310\nHINGE 0.2430 0.3770 0.1760\nStarE 0.3490 0.4960 0.2710\nHy-Transformer 0.3560 0.4980 0.2810\nHyNT 0.3568 0.5010 0.2811\nAll HyNT 0.3834 0.5267 0.3081",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.7184842824935913
    },
    {
      "name": "Transformer",
      "score": 0.717584490776062
    },
    {
      "name": "Statistical relational learning",
      "score": 0.6501438617706299
    },
    {
      "name": "Computer science",
      "score": 0.629463255405426
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5684710144996643
    },
    {
      "name": "Graph",
      "score": 0.48553702235221863
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43091684579849243
    },
    {
      "name": "Relational database",
      "score": 0.4304863512516022
    },
    {
      "name": "Knowledge graph",
      "score": 0.41659241914749146
    },
    {
      "name": "Natural language processing",
      "score": 0.38762378692626953
    },
    {
      "name": "Machine learning",
      "score": 0.3524450957775116
    },
    {
      "name": "Information retrieval",
      "score": 0.3168771266937256
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    }
  ]
}