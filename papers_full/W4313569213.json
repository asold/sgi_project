{
    "title": "STTF: An Efficient Transformer Model for Traffic Congestion Prediction",
    "url": "https://openalex.org/W4313569213",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2106523734",
            "name": "Xing Wang",
            "affiliations": [
                "Fujian Normal University",
                "Fujian Polytechnic of Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3081932948",
            "name": "Ruihao Zeng",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2008037254",
            "name": "Fumin Zou",
            "affiliations": [
                "Fujian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2149252551",
            "name": "Lyuchao Liao",
            "affiliations": [
                "Fujian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2427097389",
            "name": "Faliang Huang",
            "affiliations": [
                "Nanning Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2106523734",
            "name": "Xing Wang",
            "affiliations": [
                "Fujian Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A3081932948",
            "name": "Ruihao Zeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2008037254",
            "name": "Fumin Zou",
            "affiliations": [
                "Fujian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2149252551",
            "name": "Lyuchao Liao",
            "affiliations": [
                "Fujian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2427097389",
            "name": "Faliang Huang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3126787811",
        "https://openalex.org/W2276671495",
        "https://openalex.org/W2604472983",
        "https://openalex.org/W2613961954",
        "https://openalex.org/W2963051003",
        "https://openalex.org/W2962845481",
        "https://openalex.org/W2065297139",
        "https://openalex.org/W2018596395",
        "https://openalex.org/W2095828038",
        "https://openalex.org/W2940289431",
        "https://openalex.org/W2460563573",
        "https://openalex.org/W2964066133",
        "https://openalex.org/W2985096867",
        "https://openalex.org/W2921600633",
        "https://openalex.org/W2891280833",
        "https://openalex.org/W2806822299",
        "https://openalex.org/W2946953362",
        "https://openalex.org/W2891705404",
        "https://openalex.org/W2785818537",
        "https://openalex.org/W2942757402",
        "https://openalex.org/W2942812842",
        "https://openalex.org/W2990542783",
        "https://openalex.org/W2808871417",
        "https://openalex.org/W3159993297",
        "https://openalex.org/W3139729413",
        "https://openalex.org/W2955819484",
        "https://openalex.org/W2563738891",
        "https://openalex.org/W3094073818",
        "https://openalex.org/W3021810927",
        "https://openalex.org/W3003862857",
        "https://openalex.org/W2968259729",
        "https://openalex.org/W2963285479",
        "https://openalex.org/W3080911033",
        "https://openalex.org/W2795276038",
        "https://openalex.org/W3093761440",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3171884590",
        "https://openalex.org/W1888005072",
        "https://openalex.org/W2799900537",
        "https://openalex.org/W2094283130",
        "https://openalex.org/W6600175266",
        "https://openalex.org/W6600146492",
        "https://openalex.org/W2756203131",
        "https://openalex.org/W2965341826",
        "https://openalex.org/W3103427490",
        "https://openalex.org/W3103720336"
    ],
    "abstract": "Abstract With the rapid development of economy, the sharp increase in the number of urban cars and the backwardness of urban road construction lead to serious traffic congestion of urban roads. Many scholars have tried their best to solve this problem by predicting traffic congestion. Some traditional models such as linear models and nonlinear models have been proved to have a good prediction effect. However, with the increasing complexity of urban traffic network, these models can no longer meet the higher demand of congestion prediction without considering more complex comprehensive factors, such as the spatio-temporal correlation information between roads. In this paper, we propose a traffic congestion index and devise a new traffic congestion prediction model spatio-temporal transformer (STTF) based on transformer, a deep learning model. The model comprehensively considers the traffic speed of road segments, road network structure, the spatio-temporal correlation between road sections and so on. We embed temporal and spatial information into the model through the embedding layer for learning, and use the spatio-temporal attention module to mine the hidden spatio-temporal information within the data to improve the accuracy of traffic congestion prediction. Experimental results based on real-world datasets demonstrate that the proposed model significantly outperforms state-of-the-art approaches.",
    "full_text": "Vol.:(0123456789)1 3\nInternational Journal of Computational Intelligence Systems            (2023) 16:2  \nhttps://doi.org/10.1007/s44196-022-00177-3\nRESEARCH ARTICLE\nSTTF: An Efficient Transformer Model for Traffic Congestion Prediction\nXing Wang1,2 · Ruihao Zeng3  · Fumin Zou4 · Lyuchao Liao4 · Faliang Huang5\nReceived: 21 October 2022 / Accepted: 18 December 2022 \n© The Author(s) 2023\nAbstract\nWith the rapid development of economy, the sharp increase in the number of urban cars and the backwardness of urban \nroad construction lead to serious traffic congestion of urban roads. Many scholars have tried their best to solve this problem \nby predicting traffic congestion. Some traditional models such as linear models and nonlinear models have been proved to \nhave a good prediction effect. However, with the increasing complexity of urban traffic network, these models can no longer \nmeet the higher demand of congestion prediction without considering more complex comprehensive factors, such as the \nspatio-temporal correlation information between roads. In this paper, we propose a traffic congestion index and devise a new \ntraffic congestion prediction model spatio-temporal transformer (STTF) based on transformer, a deep learning model. The \nmodel comprehensively considers the traffic speed of road segments, road network structure, the spatio-temporal correlation \nbetween road sections and so on. We embed temporal and spatial information into the model through the embedding layer \nfor learning, and use the spatio-temporal attention module to mine the hidden spatio-temporal information within the data \nto improve the accuracy of traffic congestion prediction. Experimental results based on real-world datasets demonstrate that \nthe proposed model significantly outperforms state-of-the-art approaches.\nKeywords Traffic congestion prediction · Free-stream velocity · Road network structure · Spatio-temporal information · \nTransformer\nAbbreviations\nSTTF  Spatio-temporal transformer\nTCI  Traffic Congestion Index\nTCS  Traffic Congestion Score\nARIMA  Autoregressive integrated moving \naverage\nEMD  Empirical mode decomposition\nMM  Markov models\nHMM  Hidden Markov models\nKNN  K-nearest neighbor\nDBSCAN  Density-based spatial clustering of \napplications with noise\nSVM  Support vector machine\nAMSVM  Adaptive multi-kernel SVM\nKELM  Kernel extreme learning machine\nS-ELM-Cluster  Symmetric-extreme learning machine \ncluster\nDTM  Decision tree models\nRF  Random forest\nBN  Bayesian network\nCNN  Convolutional neural network\nPCNN  Convolution-based deep neural network \nmodeling periodic traffic data\nASA-RGCNN  Analogous self-attention-residual gated \nCNN\nSG-CNN  Road segment group-CNN\nSTFSA  Spatio-temporal feature selection \nalgorithm\nLSTM  Long short-term memory\nRNN  Recurrent neural network\nXing Wang and Ruihao Zeng contributed equally to this work.\n * Ruihao Zeng \n rzen0142@uni.sydney.edu.au\n1 College of Computer and Cyber Security, Fujian Normal \nUniversity, Fuzhou 350108, China\n2 Digital Fujian Institute of Big Data Security Technology, \nFuzhou 350108, China\n3 School of Civil Engineering, The University of Sydney, \nSydney 2008, Australia\n4 Fujian Key Laboratory of Automotive Electronic \nand Electrical Drive Technology, Fujian University \nof Technology, Fuzhou 350117, Fujian, China\n5 School of Computer and Information Engineering, Nanning \nNormal University, Nanning 530001, Guangxi, China\n International Journal of Computational Intelligence Systems            (2023) 16:2 \n1 3    2  Page 2 of 16\nPrePCT  Predictor for position congestion tensor\nGCN  Graph convolutional network\nNGSIM  Next Generation SIMulation\nAJB  Amended Jarque-Bera\nNLP  Natural language processing\nCV  Computer visual\nDCRNN  Diffusion convolutional RNN\nST-GCN  ST-graph convolution network\nMAE  Mean absolute error\nRMSE  Root mean squared error\nMAPE  Mean absolute percentage error\n1 Introduction\nIn the past decade, with the rapid growth of global popula-\ntion and the acceleration of urbanization, cities have become \nmore and more crowded, and urban road traffic is inevitably \nfacing the problem of traffic congestion. Traffic congestion \nnot only leads to inefficient transportation, but also increases \nthe time and money spent by travelers. The environmental \npollution problems are also aggravated by the increasing \nemissions of vehicles. Therefore, it is considered to be one \nof the important tasks for municipal management to solve \nthe traffic congestion problem efficiently.\nCurrent research on traffic congestion prediction can \nbe mainly divided into three directions: linear models \n[9–15], nonlinear models [16–22] and neural network mod-\nels [23– 38]. Among them, linear models usually consider \nthe traffic prediction values in a probability distribution \nmodel and make predictions by calculating the variation \npattern of the predicted values on the timeline, for exam-\nple, in literature [ 12, 15]. However, this type of models \ndoes not consider the spatio-temporal correlation between \nroads at all. The quantified road congestion is not a simple \ntime-flow prediction problem. Considering that the gen-\neral activity habits of most residents are regular, and roads \nmay show the same changes at different times or different \nroads may show the same changes at the same time, all \nthese potential relationships may help us to make better \ncongestion prediction. Nonlinear models are mainly based \non clustering and classification models, where researchers \nwork to simplify complex flow changes into several differ -\nent types of patterns and use them as a benchmark, such as \nin literature [16, 19]. But again, such type of models suffers \nfrom a lack of applicability. Considering the unsupervised \nnature of clustering, the optimal clustering criteria may \nalso be completely different in areas with very different \ntraffic conditions. Neural network models are widely used \nin congestion prediction because of their strong learning \nand in-depth mining ability for large-scale datasets, for \nexample, in literature [24, 30]. But since traffic flow and \nroad network structure are two completely different types \nof information, it is difficult for the neural network to learn \nboth features at the same time. Of course, some scholars \nhave tried, for example, in literature [49, 50], to integrate \nthe road network structure information into the graph net-\nwork and learn it at the same time. However, the predic-\ntion accuracy still needs to be further improved. The model \nalso needs to consider more critical impact factors, such \nas traffic flow, speed, running time, spatial and temporal \ncorrelation between road segments, etc.\nBased on the above problems in traffic congestion pre-\ndiction, we propose a new traffic congestion index with \nthe introduced free-stream velocity of the road segment to \nreflect the road capacity and devise our prediction model \nSpatio-temporal Transformer (STTF).\nAlthough there are many traffic data, such as traffic vol-\nume, vehicle speed and travel time that can reflect traffic con-\ngestion to some extent, the reason why we use free-stream \nvelocity instead of traffic volume is that there are large gaps \nbetween main roads and non-main roads on traffic conditions \nin cities (especially large and densely populated cities). For \nexample, traffic volume and speed are closely related to the \ngeographical location of the road and the capacity of the road. \nA high traffic volume may only mean that the road segment \nis busy and does not necessarily indicate congestion. A low \ntraffic volume may not necessarily indicate congestion if it is \nsurrounded by residences or schools which has complex road \nconditions or has speed limits. Only traffic volume or speed \ndoes not accurately reflect the congestion of the road. Thus, \nwe introduce the free-stream velocity to reflect the capacity \nof the road and then propose a new traffic congestion index.\nbesides, we deeply excavate the relationship among road \nnetwork structure, correlation between road segments and \nroad itself from spatio-temporal perspective. We take the \nconstruction of road network structure as the starting point \nand use the improved transformer to gradually retain the \nspatio-temporal information of roads.\nThe main contributions of this paper are summarized as \nfollows.\n• We propose a new traffic congestion index, which can \naccurately reflect the congestion degree of the road sec-\ntion according to the different traffic capacity and daily \ntraffic conditions of each road.\n• We devise an efficient STTF model for traffic conges-\ntion prediction based on the Transformer model, which \ncan learn both the spatio-temporal information and road \nnetwork structure information.\n• We introduce an embedding learning module to learn the \nspatial and temporal information of the road network. \nOn top of that, we encode and decode these two parts of \ninformation separately in the training phase to ensure that \nthe model can obtain the spatio-temporal relationship of \nthe data.\nInternational Journal of Computational Intelligence Systems            (2023) 16:2  \n1 3 Page 3 of 16     2 \n• In experiments with real-world datasets, our model has \nsuperior performance and accuracy compared with both \nclassical and state-of-the-art models.\nThe remainder of paper is organized as follows. Section  2 \nmainly introduces the state-of-the-art research on traffic \ncongestion prediction. Sections  3 and 4 mainly introduce \nthe notations used in this paper and present our proposed \nmodel. In Sect. 5, we verify the superiority of STTF model \nby experiments. Finally, this paper is concluded in Sect. 6.\n2  Related Work\nTraffic congestion prediction can usually be viewed as a \ncomplex time series prediction problem. Considering the \nrich variety of data in the traffic domain and referring to \nAkhtar et al. [1] for an overview of research in this area, we \ncan classify the research directions into direct and indirect \ntypes based on the type of data. Among them, the direct type \nof methods uses data that may affect traffic conditions, such \nas weather conditions [2] and emergencies [3], which often \ngive direct information about the traffic status and facilitate \ndrivers’ judgments. Some data that reflect the public state \ncan also directly reflect the congestion, such as the diver -\nsion structure of roads [4 ], public opinion reports [5 ], and \nelectricity consumption [6].\nThe indirect type of methods is the one that has been \nstudied by more scholars. These methods usually use some \nvehicle travel data, such as traffic flow, vehicle speed, etc. \nAlthough these data do not directly reflect the traffic conges-\ntion information, researchers use these basic data to quantify \nthe congestion as a parameter [7, 8], which is usually called \nTraffic Congestion Index (TCI) or Traffic Congestion Score \n(TCS) and forecast traffic congestion with TCI or TCS pre-\ndictions. We can simply classify this type of research into \nthree categories, which are linear models, nonlinear models, \nand neural network models.\n2.1  Linear Models\nLinear model-based approaches usually consider traffic \ndata to satisfy a particular distribution. Such approaches \ninclude traditional mathematical statistical models and \nstate-space models. Traditional statistical models were first \nused for traffic state prediction by Nicholson et al. [9 ] who \nused spectral analysis to find the interconnections of data in \nthe time dimension. Later, Yang et al. [10] used road occu-\npancy, He et al. [11] used speed performance index to mine \nroad congestion probability are similar reasoning. Besides, \nin recent years, Autoregressive Integrated Moving Aver -\nage (ARIMA) model is also widely used in the research. \nFor example, Alghamdi et al. [12] used ARIMA model to \nstudy the factors affecting traffic congestion and proposed \na short-term prediction model for non-Gaussian distributed \ndata. Wang et al. [13] combined ARIMA with Empirical \nMode Decomposition (EMD), based on which the hybrid \nframework has better short-term prediction than similar \nmethods. In addition, methods based on Markov Models \n(MM) and Hidden Markov Models (HMM) are also widely \nused. For example, Zaki et al. [14] used HMM to find a \nsuitable Neuro-Fuzzy prediction network for congestion at \na specific period, while Ali-Eldin et al. [15] used HMM to \nconstruct a two-dimensional space based on average speed \nand contrast and used it to capture the changing patterns of \ntraffic conditions.\nThis type of linear time-series-based prediction models \nusually utilizes only the temporal characteristics of the data \nand does not consider other additional information. So, it is \nonly suitable for road data with strong stability in the time \ndimension.\n2.2  Nonlinear Models\nWith the increasing randomness and volatility of modern \nurban traffic, it is difficult for simple linear models to meet \nthe requirements for congestion prediction. Therefore, \nresearchers have started to use non-linear models to tap into \ntraffic variations.\nOne of the main categories is the mining of traffic pat-\nterns from the perspective of historical data using cluster -\ning models led by K-Nearest Neighbor (KNN) and Den -\nsity-Based Spatial Clustering of Applications with Noise \n(DBSCAN). For example, Wen et al. [16] used DBSCAN to \nfind the spatio-temporal association rules of roads and per -\nformed classification simulation for different patterns of road \nlinks to improve the prediction accuracy. Support Vector \nMachine (SVM) is also widely used for congestion predic-\ntion due to its non-linear regression capability. For example, \nFeng et al. [17] proposed an Adaptive Multi-kernel SVM \n(AMSVM) using Gaussian kernel and polynomial kernel to \nexplore the stochasticity and spatio-temporal relationship \nof traffic flow. Xing et al. [18] proposed a Kernel Extreme \nLearning Machine (KELM) based on kernel function as the \nreplacement of hidden layer, while Ban et al. [19] proposed \nan efficient learning method based on Symmetric-Extreme \nLearning Machine Cluster (S-ELM-Cluster), which is able \nto transform large-scale data learning to different problems \non small-scale datasets. In addition, Decision Tree Models \n(DTM) [20], Random Forest (RF) [21] and Bayesian Net-\nwork (BN) [22] also have similar ideas to nonlinear methods.\n2.3  Neural Network Models\nWith the development of deep learning, neural network \nmodels [23] have achieved excellent results in more and \n International Journal of Computational Intelligence Systems            (2023) 16:2 \n1 3    2  Page 4 of 16\nmore prediction fields, no exception in traffic congestion \nprediction.\nAmong them, Convolutional Neural Network (CNN) \nhas been chosen by many researchers for its powerful fea-\nture extraction capability and adaptability to high-dimen-\nsional data. For example, Zhu et al. [24] used CNN to \ndetect complex traffic conditions in Bath City. Chen et al. \n[25] proposed a PCNN (Convolution-based deep Neural \nNetwork modeling Periodic traffic data) model capable of \ntransforming one-dimensional data into image-like data \nfor input. Zhang et al. [26] proposed an Analogous Self-\nAttention-Residual Gated CNN (ASA-RGCNN) model \ncombining gated structure and ASA structure to discover \nthe impact of data spatio-temporal characteristics on dif-\nferent levels of traffic flow. Meanwhile, CNNs are usually \ncombined with graph networks. SG-CNN (Road Segment \nGroup-CNN) proposed by Tu et al. [27] can mine the com-\nmon information between road segments, while the work \nof Zhang et al. [28] that used Spatio-Temporal Feature \nSelection Algorithm (STFSA) to extract spatio-temporal \ninformation and then handed over to CNN for learning, \nwhich had also been proved to have better prediction accu -\nracy. In addition, the Long Short-Term Memory (LSTM) \n[29] network adapted from Recurrent Neural Network \n(RNN) is often used as a benchmark method for prediction \nbecause of its temporal learning capability. For example, \nBai et al. [30] used LSTM to learn temporal features while \nusing Predictor for Position Congestion Tensor (PrePCT) \nand CNN for spatial features. Similar ideas are used in \nliterature [31– 34].\nBesides, some emerging models such as Graph Convo-\nlutional Network (GCN) [35], Next Generation SIMulation \n(NGSIM) [36], MetaNet [37], and Attention mechanism \n[38] are able to perform high accuracy congestion predic -\ntion. However, neural network models still have some urgent \nproblems, such as the loss of the ability to mine hidden \ninformation because of the depth of the network, the high \nconsumption of computational resources, and the inability to \nconsider the road network structure information completely.\n3  Notations\nIn this section, we focus on some of the basic parameters \nthat will be used in this paper.\n3.1  Traffic Congestion Index\nAccording to the description in Sect.  1, we introduce the \nvehicle free-stream velocity vfree-stream and propose a new \ntraffic congestion index TCI . we denote the current time \nperiod as t , and use /uni0302.s1vt to denote the average velocity of all \nsampled vehicles passing through one road segment or sen-\nsor in time period t. Then TCI is calculated as follows,\nwhere vfree-stream is defined as the speed of a vehicle passing \nthe road segment under ideal conditions (only one vehicle \non the road and no external factors are considered), i.e., the \nmaximum withstand speed of the road. By observing the \nvehicle speed distribution in the public dataset PeMS-Bay, \nBeijing and the private dataset FUZHOU we can simply \nassume that the vehicle speed variable v  conforms to the \nnormal distribution N = /parenleft.s1/u1D707,/u1D70E2 /parenright.s1 . Then the probability den-\nsity function f(v) of the speed is as follows.\nThen, if this assumption holds, the variables should satisfy \nthe more efficient amended Jarque-Bera (AJB) test [27], i.e.,\nwhere VS is velocity skewness, which is used to measure the \ndirection and degree of skewness of the velocity sample data \ndistribution. VS is represented by the third-order standard \nmatrix of the velocity variable v. The VK is velocity kurtosis, \nwhich is used to indicate the sharpness of the peak of the \nvelocity sample data distribution. VK is represented by the \nfourth-order standard matrix of the velocity variable v. The \ntwo variables are calculated as follows.\nIn the case of satisfying the AJB test we can express the free \nflow speed vfree-stream of vehicle in terms of the total overall \nexpectation, i.e.,\n3.2  Road Network Structure Graph\nWe first denote the selected road network as a weighted \ndirected graph G where G =( V, E, W)  . Among it, V is the set \nof nodes in the network and the number of vertices N = /uni007C.varV/uni007C.var . E \n(1)TCI t =\n/braceleft.s4\n1 − /uni0302.s1vt\nvfree-stream\n, /uni0302.s1vt ≤ vfree-stream\n0, otherwise\n,\n(2)f(v)= 1√\n2/u1D70B/u1D70E\n⋅ exp\n�\n−(v − /u1D707)2\n2/u1D70E2\n�\n.\n(3)AJB = (VS)2\nVar(VS) + (VK − E(VK ))2\nVar(VK ) ,\n(4)VS =E\n/bracketleft.s3/parenleft.s2v− /u1D707\n/u1D70E\n/parenright.s23 /bracketright.s3\n,\n(5)VK =E\n/bracketleft.s3/parenleft.s2v− /u1D707\n/u1D70E\n/parenright.s24 /bracketright.s3\n.\n(6)vfree-stream = /uni222B.dspv ⋅ f(v)dv .\nInternational Journal of Computational Intelligence Systems            (2023) 16:2  \n1 3 Page 5 of 16     2 \nis the set of link states in the network, and W is the set of \nweights between nodes, which can be regarded as a weight \nmatrix and W ∈ ℝN×N . Therefore, Wv i,v j\n denotes the link \nweights between nodes vi and vj . The exact calculation method \nwill be given later. In the traffic road network, each node rep-\nresents a specific road segment (or sensor), while the link \nbetween nodes indicates the connected relationship between \nroad segments (or sensors). The link weights represent the \ndegree of association between connected road segments (or \nsensors).\nThen the time variables are defined. We define historical \ntime steps as h  and future time steps as f . Then the TCI t in \nthe current time period t can be represented by the matrix X t \nand Xt ∈ ℝN  . Therefore, the TCI of the road network V in the \npast time period h is denoted as X = \n/parenleft.s1X t1 ,X t2 ,…,X th\n/parenright.s1\n and \nX ∈ ℝN×h , while the TCI of the future f time steps that need \nto be predicted can be written as Y = /parenleft.s1X th+1 ,X th+2 ,…,X th+f\n/parenright.s1\n \nand Y ∈ ℝN ×f . The ground truth of the TCI in the future f time \nsteps can be written as U = /parenleft.s1U th+1 ,U th+2 ,…,U th+f\n/parenright.s1\n.\nIn that case, we give a definition of the link weight coef-\nficient W so that it reflects the actual distance between two \ninterconnected road segments and the correlation between the \ntwo road segments. Then we have the following equation,\n(7)Wvi,vj\n=\n⎧\n⎪\n⎨\n⎪⎩\nexp\n�\n−\nr(vi,vj)\nd vi,vj ∕/u1D709\n�\n, exp\n�\n−\nr(vi,vj)\nd vi,vj ∕/u1D709\n�\n≥ /u1D700\n0, otherwise\n,\nwhere d vi,vj\n denotes the actual distance between the center \npoints of the two road segments. r/parenleft.s1v i, v j\n/parenright.s1\n denotes the Pearson \nCorrelation Coefficient of the traffic flow at nodes vi and vj . \nHere, /u1D709 is introduced as the adjustment factor to make d vi,vj\n \nand r/parenleft.s1v i, v j\n/parenright.s1\n comparable, which is taken as /u1D709= 1000 . /u1D700 is the \nthreshold used to control the degree of W diffusion, and here \n/u1D700= 0.05.\n4  STTF Model\nIn this section, we introduce the structure of the proposed \nSTTF model and the functions of each part.\n4.1  The STTF Model Structure\nThe Transformer model was first proposed by Vaswani et al. \n[39]. The Attention mechanism, encoder and decoder in the \nmodel together form the black box, which is the core struc-\nture of this model. The complex nature of its parallelized \ncomputation dictates that it is better than RNN in terms of \naccuracy and performance. Transformer has previously been \nwidely used in the Natural Language Processing (NLP) [40] \nand Computer Visual (CV) [41] fields. Lim et al. [42] also \nused it to mine the temporal dimensional features of time \nseries data, but studies using Transformer to mine spatio-\ntemporal patterns are less common.\nFig. 1  Structure of STTF model\n International Journal of Computational Intelligence Systems            (2023) 16:2 \n1 3    2  Page 6 of 16\nBased on the classical Transformer, we propose a new Spa-\ntio-Temporal Transformer (STTF) model. The complete struc-\nture is shown in Fig. 1. The Transformer framework mainly \nconsists of encoder, decoder and embedding module, which \ncontains the new given ST-Embedding layer (Spatial Embed \nblock & Temporal Embed block), the new given ST-Attention \nlayers (Spatial Att layer & Temporal Att layer) and other clas-\nsical structures. The input of the Transformer is the TCI data \nX at h time steps in the past. The output is the predicted TCI \ndata Y at f time steps in the future. Each module is set to out-\nput a D-dimensional vector to facilitate the connection of the \nmodules in each layer.\n4.2  ST‑Embedding Layer\nST-Embedding layer is the number 1 module in Fig. 1.\nSpatial Embed block Considering that the road network \nstructure graph G is a directed acyclic graph with weights, \nto transform it into variables that the Transformer can learn \nand retain the structural information, we need to transform \nthe network nodes into vector form represented in the vec-\ntor space. Here we use the LINE algorithm proposed by \nTang et al. [43]. The input structure graph G is vectorically \nrepresented and a feedforward neural network with GRLU \nactivation function is added after the output to transform it \ninto a D-dimensional vector. Then the final output is noted \nas sevi\n while sevi\n∈ ℝD  , vi ∈ V.\nTemporal Embed block Spatial Embed block provides \nstructural information of road data, then Temporal Embed \nblock is also needed to provide temporal feature information \nfor Transformer. Referring to the nonlinear method to learn \nthe distribution pattern of data in time dimension by histori-\ncal data, the historical data is also used here for embedding \nencoding. Considering the uniqueness of each time dimen-\nsion, one-hot encoding [44] is used here to encode the time \nin the past h steps. We encode the number of days in a week \ninto the vector space of ℝ7 and the time period t in a day into \nthe vector space of ℝt . Finally, the two encodings are trans-\nformed into the vector space of ℝ7+t by concatenation opera-\ntion, and a feedforward neural network with GRLU activa-\ntion function is also added to the output to transform it into \na D-dimensional vector. In this case, we can encode the \ntemporal features of the past h time steps and write the vec-\ntor of the neural network output as tetj\n while tetj\n∈ ℝD  , tj ∈ /braceleft.s1t1 ,t2 ,…,th\n/braceright.s1\n.\nAfter getting the feature information of temporal embed-\nding and spatial embedding respectively, we need to integrate \nthe two parameters of the same dimensions. Here we introduce \nthe new embedding coefficients stev i,tj\n , and we can get the fol-\nlowing embedding coefficients in tj steps of node vi.\nWe denote this operation as ⊙ . Then the ST-Embedding \nlayer structure diagram is shown below in Fig. 2.\n4.3  Encoder Architecture\nEncoder is the number 2 module in Fig.  1. A total of L \nencoders are included in STTF model. Each encoder con-\nsists of three consecutive layers: Spatial Att layer (number 3 \nmodule in Fig. 1), Temporal Att layer (number 4 module in \nFig. 1), and Feed Forward layer (where Spatial Att layer and \nTemporal Att layer together form the ST-Attention layer). \nThe first two attention structures have a skip-connection \nstructure used to skip inter-layer connections (indicated by \ndashed lines). To improve the generalization ability, each \nattention operation is employed the normalization and drop-\nout. The Feed Forward layer is mainly designed to integrate \nhigh-dimensional attention information and consists of two \nfully connected neural networks with ReLU activation func-\ntions. After feeding the feature vector sequence X to the \nfirst encoder, the ST-Embedding layer finally outputs the \nhidden representation vector of the encoder to the decoder’s \nattention layer after the L − 1 encoder’s attention operation.\nReferring to the design of the attention layer in the clas-\nsical Transformer structure [39], we propose a two-layer ST-\nAttention layer structure consisting of Spatial Att layer and \nTemporal Att layer. Each encoder and decoder has one ST-\nAttention layer, then we can note that in l-th ST-Attention \nlayer in encoder or decoder, the output of Spatial Att layer \nis sa (l)\nv i,tj\n , and the output of Temporal Att layer is ta (l)\nv i,tj\n. Then \nthere are l -th ST-Attention layer whose input is z(l−1)\nv i,tj\n and \noutput is z(l)\nv i,tj\n(8)stev i,tj\n= exp\n/parenleft.s2\n−\n/parenleft.s2\nse v i\n+ tetj\n/parenright.s2/parenright.s2\n.\nFig. 2  Structure of ST-Embedding layer\nInternational Journal of Computational Intelligence Systems            (2023) 16:2  \n1 3 Page 7 of 16     2 \nSpatial Att layer To fully consider the influence of each \nroad link on the specified road segment in the road network \nstructure, we calculate the effect of each node in the (l − 1)\n-th Spatial Att layer on the node vi in the l -th layer, i.e., \nassign different weights to each node at different time peri-\nods, which is shown in Fig. 3. Then the output hidden repre-\nsentation vector of this layer is calculated below,\nwhere /u1D6FCvi,v  denotes the normalized attention coefficient. \nNoting its pre-normalization state as srvi,v , which is directly \nused to represent the correlation coefficient between each \nnode v of upper layer and the given node vi of current layer. \nAccording to the classical Transformer structure [39], we \nchoose to use scaled dot-product approach to represent the \ncorrelation between the two nodes. Then we can obtain the \nfollowing equation,\nwhere [a , b] denotes the calculation of the inner product \nof a and b .d denotes the dimension of the vector after the \nconcatenation operation is performed. Thus, we normalize \nsrvi,v using the softmax function to obtain /u1D6FCvi,v.\nFinally, to improve the efficiency and expand the capacity \nof the network through parallel computation, we introduce \nthe multi-head attention mechanism [39]. We set the number \nof attention heads to Q , i.e., use different, learnable linear \n(9)sa (l)\nv i,tj\n=\n/uni2211.s1\nv ∈V\n/parenleft.s2\n/u1D6FCv i,v ⋅ z(l−1)\nv ,tj\n/parenright.s2\n,\n(10)\nsrvi,v =\n�\nConcat\n�\nz(l−1)\nvi,tj\n, stevi,tj\n�\n, Concat\n�\nz(l−1)\nv,tj\n, stev,tj\n��\n√\nd\n,\n(11)/u1D6FCvi,v =\nexp �srvi,v\n�\n∑\nv∈V exp �srvi,v\n�.\nprojections to project each parameter linearly Q times to the \ncorresponding dimension. The attention function of each \nprojection is computed in parallel, and the concatenation \noperation is performed after each computation. In that case, \nwe denote the projection operation as p . Then, p (x) is the \nlinear projection function, which is calculated below,\nwhere both m  and n denote learnable variable parameters. \np(h)\nm, n denotes the projection function with different param-\neters. Then these can be obtained that at the q-th projection,\nTemporal Att layer To fully explore the hidden temporal \npatterns in the historical data of the same road segment, \nTemporal Att layer is introduced in encoder, whose input is \nthe output sa (l)\nv i,tj\n of Spatial Att layer of the same ST-Attention \nlayer. We calculate the influence of past and future moment \nof node vi in each Temporal Att layer on the present moment, \nwhich is shown in Fig.  4. Using the same computational \nmodel and time vector as in the Spatial Att layer, the hidden \nrepresentation vector of the layer output is noted as tavi,tj\n , the \nattention coefficient in the layer is denoted by /u1D6FDtj ,t , and its \n(12)p(x)=sigmoid (mx + n),\n(13)/u1D6FC(q)\nvi,v =\nexp\n�\nsr(q)\nvi,v\n�\n∑\nv∈V exp\n�\nsr(q)\nvi,v\n�\n(14)\nsr(q)\nvi,v =\n/bracketleft.s2\np(q)\nm, n\n/parenleft.s2\nConcat\n/parenleft.s2\nz(l−1)\nvi,tj\n, stevi,tj\n/parenright.s2/parenright.s2\n,p(q)\nm, n\n/parenleft.s2\nConcat\n/parenleft.s2\nz(l−1)\nv,tj\n, stev,tj\n/parenright.s2/parenright.s2/bracketright.s2\n/uni221A.s1\nd\nq\n,\n(15)sa(l,q)\nvi,tj\n= Concat q\nQ=1\n/parenleft.s4\n/uni2211.s1\nv∈V\n/parenleft.s2\n/u1D6FC(q)\nvi,v ⋅ p(q)\nm,n ⋅ z(l−1)\nv,tj\n/parenright.s2/parenright.s4\n.\nFig. 3  Principle of spatial \nattention\n International Journal of Computational Intelligence Systems            (2023) 16:2 \n1 3    2  Page 8 of 16\nstate before normalization is denoted as trtj,t , which indicates \nthe impact of t time step on the current step tj of same road \nsegment. The hf tj\n is the set of all time steps before and after \nthe step tj (including the current step tj ). Finally, the multi-\nhead attention mechanism, p(h)\nm, n , is introduced to denote the \nprojection function with different parameters. Then we have \nthe following equations.\nThen these can be obtained that at the q-th linear projection,\n(16)trtj,t =\n�\nConcat\n�\nsa (l)\nvi,tj\n, stevi,tj\n�\n, Concat\n�\nz(l)\nv,tj\n, stev,tj\n��\n√\nd\n,\n(17)/u1D6FDtj,t =\nexp\n�\ntrtj,t\n�\n∑\nt∈hf tj\nexp\n�\ntrtj,t\n�,\n(18)ta(l)\nvi,tj\n=\n/uni2211.s1\nt∈hftj\n/parenleft.s2\n/u1D6FDtj,t ⋅ z(l−1)\nv,tj\n/parenright.s2\n.\n(19)\ntr(q)\ntj,t =\n/bracketleft.s2\np(q)\nm, n\n/parenleft.s2\nConcat\n/parenleft.s2\nsa(l)\nvi,tj\n, stevi,tj\n/parenright.s2/parenright.s2\n,p(q)\nm, n\n/parenleft.s2\nConcat\n/parenleft.s2\nsa(l)\nv,tj\n, stev,tj\n/parenright.s2/parenright.s2/bracketright.s2\n/uni221A.s1\nd\nq\n,\n(20)/u1D6FD(q)\ntj,t =\nexp\n�\ntr(q)\ntj,t\n�\n∑\nt∈hftj\nexp\n�\ntr(q)\ntj,t\n�,\nMasked-Temporal Att layer The Masked-Temporal Att \nlayer (number 6 module in Fig. 1), exists only in the decoder. \nThe only difference between it and the Temporal Att layer is \nthat it masks the influence of future time steps on the present \ntime step, thus limiting the attention of the decoder to the \nhistorical time steps, which is shown in Fig. 5. Therefore, by \ndefining Ttj\n as the set of all time steps before the step tj \n(including the current step tj ), we have the equation below.\n \n4.4  Decoder Architecture\nEncoder is the number 5 module in Fig.  1. A total of L \ndecoders are included in STTF model. The overall structure \nof each decoder is similar to that of the encoder, including \nan identical Spatial Att layer, an amended Masked-Temporal \nAtt layer, a classical E-D Att layer (Encoder-Decoder Atten-\ntion layer, number 7 module in Fig. 1) [39], and an identical \nFeed Forward layer. Among them, the E-D Att layer extracts \nfeature information using encoder and Masked-Temporal Att \nlayer’s encoding vectors. Each node’s embedding vector \nstev i,tf\n at future time steps and stevi,th\n at historical time steps \n(21)ta(l,q)\nvitj\n= Concat q\nQ=1\n⎛\n⎜\n⎜⎝\n�\nt∈hftj\n�\n/u1D6FD(q)\ntj,t ⋅ p(q)\nm,n ⋅ sa(l)\nvi,t\n�⎞\n⎟\n⎟⎠\n.\n(22)ta(l,q)\nvi,tj\n= Concat q\nQ=1\n⎛\n⎜\n⎜⎝\n�\nt∈Ttj\n�\n/u1D6FD(q)\ntj,t ⋅ p(q)\nm,n ⋅ sa(l)\nvi,t\n�⎞\n⎟\n⎟⎠\n.\nFig. 4  Principle of temporal \nattention\nInternational Journal of Computational Intelligence Systems            (2023) 16:2  \n1 3 Page 9 of 16     2 \ncorrespond to the key and value in the classical structure, \nrespectively. After decoder outputting the feature space vec-\ntor, the prediction sequence Y is finally outputted by linear \nlayer and normalization operation.\n5  Experiments\nTo test the practical effectiveness of our model, we con-\nduct experiments on two real-world large-scale datasets, \nrespectively.\n5.1  Datasets\nConsidering that FUZHOU is vehicle GPS data and PeMS-\nBay is sensor data, we first use the IVMM algorithm [45] to \ndo map matching for the vehicles data in FUZHOU. After \nthat, we count the speed data in both datasets in every 5, \n10, and 15 minutes and fill the missing data with 0 values \nas well as normalized the data in the way of Li et al. [46].\nFUZHOU This private traffic dataset is collected by \nDepartment of Transport of Fujian Province. The dataset \ncontains speed data for 2 months ranging from May 1 st  to \nJune 31 st  in 2018 , gathered from part of urban roads in \nFuzhou City, Fujian Province. The distribution of road sec-\ntions is shown in the following Fig. 6a.\nPeMS-Bay This public traffic dataset is collected by \nCalifornia Transportation Agencies (Cal-Trans) Performance \nMeasurement System (PeMS). The dataset contains speed \ndata for 6 months ranging from January 1 st  to May 31 st  in \n2017 from 325 sensor, gathered from highway in Bay Area, \nLos Angeles. The distribution of the sensors is shown in \nFig. 6b below. Among them, considering the complexity of \nurban road links, we consider that the information complex-\nity of FUZHOU dataset is higher than that of PeMS-Bay \ndataset.\nFig. 5  Principle of masked-\ntemporal attention\nFig. 6  Dataset descrip-\ntion (FUZHOU and PeMS-Bay)\n\n International Journal of Computational Intelligence Systems            (2023) 16:2 \n1 3    2  Page 10 of 16\n5.2  Experimental Configuration\nAccording to the method of Li et al. [46], we set a standard \ntime step of 5 minutes. Thus, the historical time periods \nh = 12 time steps and the future time periods f = 12 time \nsteps, i.e., both are one hour. For the use of optimizer, we \nchoose Adam-warmup optimizer [47] and set the initial \nlearning rate as 0.001, warmup step size and batch size as \n4000 and 20 , respectively.\nIn STTF model, there are three hyperparameters, namely, \nthe number of layers L of Encoder and Decoder, the number \nof attention heads Q in the multi-head attention mechanism, \nand the vector dimension D  of the output of each module. \nAfter several experiments and referring to the setting of \nthe classic transformer structure, we selected the hyperpa-\nrameter with the better performance, i.e., L = 4,Q = 8 , and \nD = 64 . In addition, we set the dropout rate to 0.3 and ini-\ntialize the parameters of the network using Xavier weight \ninitialization [48].\n5.3  Baselines and Measures\nWe select five benchmark models for comparative experi-\nments, including some basic models in the prediction prob-\nlem and some state-of-the-art deep learning models. These \nfive baselines are ARIMA [12], PrePCT [30], DCRNN (Dif-\nfusion Convolutional RNN) [46], ST-GCN (ST-Graph Con-\nvolution Network) [49], and Graph WaveNet [ 50]. Among \nthem, ARIMA is the representative work in the linear model, \nPrePCT and DCRNN are the state-of-the-art convolutional \nneural network models, and the remaining two models are \nthe state-of-the-art graph neural network models. Consider-\ning the different training mechanisms and the lack of labels, \na comparison with the non-linear model is not made here. \nThe codes of all the above models are publicly available by \nthe authors, so we can all experiment with our own datasets. \nIn our experiments, we measure the accuracy of the mod-\nels by three widely used metrics, namely, Mean Absolute \nError (MAE), Root Mean Squared Error (RMSE), and Mean \nAbsolute Percentage Error (MAPE). For a more visual com-\nparison of values, all MAE and RMSE values are artificially \nexpanded by a factor of 50.\n5.4  Experimental Results and Discussion\nThe main purpose of our experiments is to explore the pre-\ndiction accuracy, the generalization ability for different \nroad conditions, the robustness under different time inter -\nvals and time steps, and the computational efficiency of the \nmodel. Therefore, we design several experiments to test \nSTTF model by varying the time variables and road condi-\ntions. Moreover, the prediction time step indicates the time \nperiod of the model prediction results, the standard time step \ndenotes the time period used in model learning, and the time \ninterval indicates the time period of integrating data during \ndata processing.\nWe first test the prediction accuracy of the model under \ndifferent prediction time step. In Fuzhou dataset, complex \nroad network structure data can better verify the predic-\ntion ability of each model. Then all six models are made \nto predict the change value of TCI  every 30 min during the \nmain weekday period (June 4, 2018, Monday, 6:00–20:00). \nThe visualization results are shown in the Fig.  7 below, \nwhere ground truth is bolded. We can find that the STTF \nmodel has significantly stronger accuracy compared to the \nARIMA and PrePCT models, especially for peak values and \nmoments with large change rates that STTF is better fitted \nto the ground truth. To better compare quantitatively with \nthe remaining deep neural network models, we calculate \nthe MAE, RMSE, and MAPE values of the six models for \nthe given time periods in the FUZHOU dataset under dif-\nferent prediction ranges. The results are shown in Table  1. \nFrom the results we can see that ARIMA performs the worst \nunder the same prediction range because of its singularity of \ntemporal characteristics. The prediction ability of PrePCT \ndiffers more from its authors’ experimental results, prob-\nably because it is more suitable for road network prediction \nwith a smaller number of nodes. The better performance of \n6:00 8:00 10:00 12:00 14:001 6:00 18:002 0:00\nTime\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nTCI\nFig. 7  TCI prediction results\nInternational Journal of Computational Intelligence Systems            (2023) 16:2  \n1 3 Page 11 of 16     2 \ngraph-based deep learning models such as DCRNN illus-\ntrates that current deep learning methods are better than \nmost traditional linear methods, and that neural networks \nbased on graph structures are more likely to perform bet -\nter than traditional time series networks. The STTF model \noutperforms all benchmark models, which proves that our \nST-Attention layer can better mine hidden information and \nis more efficient compared to short-term serial prediction \nmethods. \nSecond, we need to consider the performance of the mod-\nels in road network structures with different levels of com-\nplexity, where the change of road complexity is reflected in \nthe difference of data collection locations. Therefore, we \ntest the prediction performance of TCI for each model in the \nselected time periods (March 6, 2017, Monday, 6:00–20:00) \nof the PeMS-Bay dataset in different prediction ranges. The \nresults are presented in Table 2. Combining Tables 1 and 2 \nwe can see that the STTF model outperforms most of the \nbenchmark models, and its predictions are more stable for \ncomplex road networks. It only loses to Graph WaveNet in \npredicting TCI values for 60 min. Such a situation may be \nexplained by the following. In the complex road networks, \neach road segment has more neighboring road segments. \nOne road segment may affect more road segments. More \nvalid information is credited when we consider the impact \nof all road segments on the specific one road segment in the \nstructure. Whereas in a relatively simple road structure, a \nroad segment may only affect some neighboring road seg-\nments. When we record its impact on all other road seg-\nments, more invalid information enters the network, which \neventually leads to different performance of the STTF model \nin the face of road networks of different complexity.\nIn addition, the traffic conditions in the same city may \nhave exceptions during both peak/off-peak hours and week-\ndays/weekends, we further validate the STTF model’s ability \nto cope with these exceptions.\nFirstly, we consider that traffic volumes tend to have \ndifferent patterns of variation at different times of the day, \ni.e., what we generally consider as morning peak, evening \npeak, and off-peak periods. This directly results in differ -\nent peaks and different rates of change of TCI  values for \neach time period. Therefore, we test the generalization \nTable 1  Performance of each model in FUZHOU database under different prediction time step (predictions are made every 30 minutes and the \noptimal values under the same time step are bolded)\nPrediction time \nsteps\nMetrics ARIMA PrePCT DCRNN ST-GCN Graph WaveNet STTF\nFUZHOU 15 min MAE (×50) 3.577 2.877 2.312 2.276 2.253 2.225\nRMSE (×50) 4.027 3.359 2.460 2.452 2.387 2.349\nMAPE 7.5% 6.1% 5.7% 5.9% 5.57% 5.52%\n30 min MAE (×50) 3.974 3.111 2.407 2.397 2.416 2.358\nRMSE (×50) 4.895 3.442 2.517 2.518 2.463 2.382\nMAPE 7.5% 6.6% 5.8% 6.2% 5.63% 5.52%\n60 min MAE (×50) 4.339 3.247 2.528 2.522 2.543 2.497\nRMSE (×50) 5.401 3.485 2.559 2.568 2.535 2.505\nMAPE 12.2% 10.2% 7.5% 8.4% 7.28% 7.24%\nTable 2  Performance of each model in PeMS-BAY database under different prediction time step (predictions are made every 30 minutes and the \noptimal values under the same time step are bolded)\nPrediction time \nsteps\nMetrics ARIMA PrePCT DCRNN ST-GCN Graph WaveNet STTF\nPeMS-Bay 15 min MAE (×50) 1.573 0.874 0.305 0.288 0.271 0.263\nRMSE (×50) 2.053 1.337 0.435 0.442 0.389 0.362\nMAPE 3.4% 3.2% 2.8% 2.9% 2.79% 2.71%\n30 min MAE (×50) 1.969 1.127 0.392 0.411 0.388 0.357\nRMSE (×50) 2.883 1.432 0.512 0.504 0.447 0.404\nMAPE 5.6% 4.7% 3.9% 4.1% 3.59% 3.51%\n60 min MAE (×50) 2.334 1.276 0.539 0.548 0.514 0.529\nRMSE (×50) 3.418 1.508 0.580 0.574 0.536 0.577\nMAPE 8.8% 7.1% 4.7% 5.5% 4.32% 4.39%\n International Journal of Computational Intelligence Systems            (2023) 16:2 \n1 3    2  Page 12 of 16\nability of the model for different time periods of the day. \nWith reference to the peak traffic periods, we divide the day \ninto three periods on average (04:00–12:00, 12:00–20:00, \n20:00–24:00–04:00). Considering that the traffic changes are \nmore significant during peak and off-peak periods in urban \nweekdays, the FUZHOU dataset (Monday, June 4, 2018) is \nchosen here for the experiments. Because graph-based deep \nlearning models have a notable predictive advantage, we \nonly use DCRNN, ST-GCN and Graph WaveNet to com-\npare with our STTF model. From the results in Table  3, it \nis obvious that the STTF model has the advantageous and \ncomprehensive performance, especially for the 30-minute \nprediction range.\nSecondly, we take the large differences in traffic patterns \nbetween weekdays and weekends into account. For example, \npeople wake up relatively later on weekends, so the morn-\ning peak is later and has a smaller peak. More people may \nTable 3  Performance of each model with different time intervals and same prediction time steps at different time periods of the day (predictions \nare made every 30 minutes and the optimal values within the same time period are bolded)\n Model - Time periods 15 min 30 min 60 min\nMAE (×50) RMSE (×50) MAPE MAE (×50) RMSE (×50) MAPE MAE (×50) RMSE (×50) MAPE\nDCRNN (4–12) 2.343 2.574 5.82% 2.532 2.595 5.78% 2.691 2.419 7.53%\nST-GCN (4–12) 2.471 2.422 5.93% 2.536 2.578 6.15% 2.617 2.483 8.56%\nGraph WaveNet (4–12) 2.294 2.573 5.56% 2.544 2.355 5.69% 2.692 2.352 7.35%\nSTTF (4–12) 2.134 2.154 5.50% 2.498 2.561 5.64% 2.547 2.533 7.11%\nDCRNN (12–20) 2.187 2.394 5.60% 2.393 2.413 5.62% 2.516 2.561 7.46%\nST-GCN (12–20) 2.166 2.377 5.88% 2.269 2.506 6.03% 2.422 2.496 8.30%\nGraph WaveNet (12–20) 2.170 2.318 5.52% 2.253 2.289 5.49% 2.506 2.362 7.16%\nSTTF (12–20) 2.223 2.166 5.51% 2.217 2.222 5.48% 2.322 2.356 7.07%\nDCRNN (20–4) 2.249 2.507 5.67% 2.355 2.536 5.85% 2.509 2.568 7.56%\nST-GCN (20–4) 2.287 2.402 5.97% 2.377 2.572 6.25% 2.555 2.578 8.47%\nGraph WaveNet (20–4) 2.265 2.322 5.52% 2.366 2.507 5.63% 2.506 2.491 7.36%\nSTTF (20–4) 2.188 2.377 5.51% 2.324 2.346 5.49% 2.445 2.536 7.25%\nTable 4  Performance of each model in weekdays and weekends with different time intervals and same prediction time steps (predictions are \nmade every 30 minutes and the optimal values in the same day are bolded)\nModel - Days 15 min 30 min 60 min\nMAE (×50) RMSE (×50) MAPE MAE (×50) RMSE (×50) MAPE MAE (×50) RMSE (×50) MAPE\nDCRNN (6.4) 2.221 2.365 5.61% 2.353 2.466 5.71% 2.477 2.504 7.42%\nST-GCN (6.4) 2.176 2.399 5.83% 2.318 2.398 6.10% 2.463 2.457 8.30%\nGraph WaveNet (6.4) 2.199 2.309 5.48% 2.322 2.351 5.58% 2.456 2.438 7.22%\nSTTF (6.4) 2.105 2.237 5.45% 2.276 2.292 5.42% 2.429 2.425 7.13%\nDCRNN (6.9) 2.372 2.448 5.75% 2.439 2.527 5.76% 2.555 2.53 7.48%\nST-GCN (6.9) 2.392 2.505 5.86% 2.462 2.534 6.27% 2.507 2.611 8.40%\nGraph WaveNet (6.9) 2.301 2.338 5.54% 2.513 2.464 5.59% 2.641 2.509 7.29%\nSTTF (6.9) 2.204 2.423 5.49% 2.313 2.413 5.53% 2.524 2.474 7.26%\nTable 5  Performance of STTF model with different time step\nStandard time steps Metrics STTF\nFUZHOU 5 min MAE (×50) 2.497\nRMSE (×50) 2.505\n10 min MAE (×50) 2.584\nRMSE (×50) 2.640\n15 min MAE (×50) 2.696\nRMSE (×50) 2.731\nPeMS-Bay 5 min MAE (×50) 0.529\nRMSE (×50) 0.577\n10 min MAE (×50) 0.603\nRMSE (×50) 0.689\n15 min MAE (×50) 0.672\nRMSE (×50) 0.737\nInternational Journal of Computational Intelligence Systems            (2023) 16:2  \n1 3 Page 13 of 16     2 \nhave time to go out and relax in the weekends’ evening, so \nthe roads may be more congested at night. Therefore, we re-\nselect weekday hours (June 4, 2018, Monday, 00:00–24:00) \nand weekend hours (June 9, 2018, Saturday, 00:00–24:00) \nfor the FUZHOU dataset to compare the generalization abil-\nity of the four graph-based learning models, respectively. \nFrom the results in Table 4, it is noticeable that the accuracy \nof all models for predicting weekend data is not significantly \ndifferent from that of weekdays, and the STTF model has a \nmarked predictive advantage.\nAlso, we need to consider the robustness of the model, \ni.e., to examine whether the model still maintains a com-\nparable accuracy when the standard time step changes. \nTherefore, for the specific time of the FUZHOU data-\nset (Monday, June 4, 2018, 6:00–20:00) and the specific \ntime of the PeMS-Bay dataset (Monday, March 6, 2017, \n6:00–20:00), we use different standard time steps (5 min, \n10 min, and 15 min) to predict the TCI  value using STTF \nmodel with a prediction range of 60 min and calculate \nMAE and RMSE, as shown in Table 5. Comparing Table 5 \nwith Tables  1 and 2 , it can be seen that the prediction \naccuracy of the STTF model decreases as the step length \nbecomes longer, although it still has a competitive predic -\ntion ability under the variation of the standard step length. \nThis is also due to its reduced amount of learning for \ntemporal information. In the case of longer standard step \n12345\nTime Step\n2\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8MAE\nFUZHOU-a\n123456\nTime Step\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nMAE\nPeMS-a\n123456\nTime Step\n2\n2.5\n3\n3.5\n4\n4.5\n5\n5.5\nMAE\nFUZHOU-b\n123456\nTime Step\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\nMAE\nPeMS-b\nFig. 8  Ablation experiments (display with MAE×50)\n International Journal of Computational Intelligence Systems            (2023) 16:2 \n1 3    2  Page 14 of 16\nlength, the prediction range remains the same, resulting \nin a reduction in the amount of input temporal informa-\ntion and thus a decrease in the model’s learning ability for \ntemporal feature information.\nConsidering the many improvements of STTF over the \nclassical Transformer structure, the ablation experiment is \nintroduced here to examine the contribution of each struc-\nture in the STTF model. We predict the 30-min TCI val-\nues for the FUZHOU and PeMS-Bay datasets at the spe-\ncific time given above, with a standard time step of 5 min. \nAmong them, experiment (a) removes Spatial Att layer (A), \nTemporal Att layer (B), Masked-Temporal Att layer (C) and \nE-D Att layer (D), respectively, and experiment (b) removes \nthe Spatial Att layer, Spatial Att layer & Temporal Att layer \n(F), Spatial Att layer & Temporal Att layer & Masked-\nTemporal Att layer (G), respectively. Finally, we calculate \nthe MAE of the predicted values for each time step. The \nresults are shown in Fig.  8. We can see that STTF consist-\nently outperforms the model with the remaining incomplete \nmodels, indicating the ability of the four modules to mine \nspatio-temporal information. In particular, it should be \nnoted that the missing Spatial Att layer module causes a \nparticularly significant decrease in accuracy, especially in \nthe long-term prediction range, which further indicates the \nlong-term impact of the spatial structure of road networks \non traffic congestion.\nFinally, we compare the training time and prediction time \nof all six models on the PeMS-Bay dataset for the same \ntime period (Monday, March 6, 2017, 6:00–20:00), the same \nstandard time step (standard step size of 5 min), and the \nsame prediction range (15 min), as shown in Table 6. As can \nbe seen from the results, in the training phase, since ARIMA \nis a simple linear operation, its training time is absolutely \nsuperior. The last five algorithms are neural network algo-\nrithms, so there is a significant increase in the training time. \nAmong them, ST-GCN is relatively efficient, but its predic-\ntion accuracy is far inferior to other graph neural networks. \nGraph WaveNet and STTF are the two models with the best \nand most similar overall performance. However, referring \nto the performance of the two models in Tables 1 and 2, we \ncan see that STTF performs better.\n6  Conclusion\nWe propose a new traffic congestion index and devise a \nSTTF model based on data spatio-temporal information for \npredicting congestion on the road network. Specifically, we \ndevise a new information embedding learning module that \ntransforms both road network structure information and tem-\nporal information into feature vectors that can be learned \nby the network. The embedding vectors are learned by a \nnew spatial attention module and a temporal attention mod-\nule with different learning directions. The model has better \nprediction accuracy and relatively high efficiency compared \nwith the state-of-the-art algorithm under real-world data.\nAcknowledgements The authors would like to thank the anonymous \nreviewers for providing helpful comments.\nAuthor Contributions RZ and XW completed the writing of the thesis, \nFZ conducted the guidance of the thesis, LL conducted part of the \nexperiment and grammar modification of the thesis, and FH conducted \npart of the experiment of the thesis.\nFunding This research was funded by the Natural Science Foundation \nof China (Grant No. 61962038), the Foreign Cooperation Project of \nFujian Provincial Department of Science and Technology (Grant No. \n2020I0014), and in part by the Guangxi Bagui Teams for Innovation \nand Research (Grant No. 201979).\nAvailability of Data and Materials The FUZHOU datasets analyzed \nduring the current study are not publicly available due to the confi-\ndentiality of private data of citizens. The PeMS-Bay datasets we used \nis publicly available at https:// github. com/ SANDAG/ PeMS- Datas ets.\nDeclarations \nConflict of Interest The authors declare that they have no known com-\npeting financial interests or personal relationships that could have ap-\npeared to influence the work reported in this paper.\nEthics Approval Not applicable.\nConsent to Participate Not applicable.\nConsent to Publication Not applicable.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nTable 6  Training and predicting efficiency of each model\nModal Training time (s/\nepoch)\nInference time (s)\nARIMA 3.94 28.84\nPrePCT 698.04 209.43\nDCRNN 834.89 137.93\nST-GCN 78.76 102.50\nGraph WaveNet 219.98 9.37\nSTTF 243.93 12.52\nInternational Journal of Computational Intelligence Systems            (2023) 16:2  \n1 3 Page 15 of 16     2 \nReferences\n 1. Akhtar, M., Moridpour, S.: A review of traffic congestion predic-\ntion using artificial intelligence. J. Adv. Transport. 2021, 8878011 \n(2021)\n 2. Lee, J., Hong, B., Lee, K., Jang, Y.J.: A prediction model of traf-\nfic congestion using weather data. In: 2015 IEEE International \nConference on Data Science and Data Intensive Systems. IEEE, \npp. 81–88 (2015)\n 3. Fouladgar, M., Parchami, M., Elmasri, R., Ghaderi, A.: Scalable \ndeep traffic flow neural networks for urban traffic congestion pre-\ndiction. In: 2017 International Joint Conference on Neural Net-\nworks (IJCNN). IEEE, pp. 2251–2258 (2017)\n 4. Jain, S., Jain, S.S., Jain, G.: Traffic congestion modelling based \non origin and destination. Procedia Eng. 187, 442–450 (2017)\n 5. Adetiloye, T., Awasthi, A.: Multimodal big data fusion for traffic \ncongestion prediction. In: Multimodal Analytics for Next-Gener-\nation Big Data Technologies and Applications. Springer, Cham, \npp. 319–335 (2019)\n 6. Zhang, P., Qian, Z.S.: User-centric interdependent urban sys-\ntems: using time-of-day electricity usage data to predict morning \nroadway congestion. Transport. Res. Part C: Emerg. Technol. 92, \n392–411 (2018)\n 7. Boarnet, M.G., Kim, E.J., Parkany, E.: Measuring traffic conges-\ntion. Transp. Res. Rec. 1634(1), 93–99 (1998)\n 8. Lee, J., Hong, B.: Congestion score computation of big traffic \ndata. In 2014 IEEE Fourth International Conference on Big Data \nand Cloud Computing. IEEE, pp. 189–196 (2014)\n 9. Nicholson, H., Swann, C.D.: The prediction of traffic flow vol-\numes based on spectral analysis. Transp. Res. 8 (6), 533–538 \n(1974)\n 10. Yang, X., Luo, S., Gao, K., Qiao, T., Chen, X.: Application of data \nscience technologies in intelligent prediction of traffic congestion. \nJ. Adv. Transp. 2019, 2915369 (2019)\n 11. He, F., Yan, X., Liu, Y., Ma, L.: A traffic congestion assessment \nmethod for urban road networks based on speed performance \nindex. Procedia Eng. 137, 425–433 (2016)\n 12. Alghamdi, T., Elgazzar, K., Bayoumi, M., Sharaf, T., Shah, S.: \nForecasting traffic congestion using ARIMA modeling. In 2019 \n15th International Wireless Communications & Mobile Comput-\ning Conference (IWCMC). IEEE, pp. 1227–1232 (2019)\n 13. Wang, H., Liu, L., Dong, S., Qian, Z., Wei, H.: A novel work zone \nshort-term vehicle-type specific traffic speed prediction model \nthrough the hybrid EMD-ARIMA framework. Transportmetrica \nB: Transp. Dyn. 4(3), 159–186 (2016)\n 14. Zaki, J.F., Ali-Eldin, A.M., Hussein, S.E., Saraya, S.F., Areed, \nF.F.: Time aware hybrid hidden Markov models for traffic conges-\ntion prediction. Int. J. Elect. Eng. Inform. 11(1), 1–17 (2019)\n 15. Zaki, J.F., Ali-Eldin, A., Hussein, S.E., Saraya, S.F., Areed, F.F.: \nTraffic congestion prediction based on Hidden Markov Models \nand contrast measure. Ain Shams Eng. J. 11(3), 535–551 (2020)\n 16. Wen, F., Zhang, G., Sun, L., Wang, X., Xu, X.: A hybrid temporal \nassociation rules mining method for traffic congestion prediction. \nComput. Ind. Eng. 130, 779–787 (2019)\n 17. Feng, X., Ling, X., Zheng, H., Chen, Z., Xu, Y.: Adaptive multi-\nkernel SVM with spatial-temporal correlation for short-term \ntraffic flow prediction. IEEE Trans. Intell. Transp. Syst. 20(6), \n2001–2013 (2018)\n 18. Xing, Y.M., Ban, X.J., Liu, R.: A short-term traffic flow predic-\ntion method based on kernel extreme learning machine. In: 2018 \nIEEE International Conference on Big Data and Smart Computing \n(BigComp). IEEE, pp. 533–536 (2018)\n 19. Xing, Y., Ban, X., Liu, X., Shen, Q.: Large-scale traffic conges-\ntion prediction based on the symmetric extreme learning machine \ncluster fast learning method. Symmetry 11(6), 730 (2019)\n 20. Alajali, W., Zhou, W., Wen, S., Wang, Y.: Intersection traffic pre-\ndiction using decision tree models. Symmetry 10(9), 386 (2018)\n 21. Liu, Y., Wu, H.: Prediction of road traffic congestion based on \nrandom forest. In: 2017 10th International Symposium on Com-\nputational Intelligence and Design (ISCID), Vol. 2. IEEE, pp. \n361–364 (2017)\n 22. Wang, S., Huang, W., Lo, H.K.: Traffic parameters estimation for \nsignalized intersections based on combined shockwave analysis \nand Bayesian Network. Transp. Res. Part C: Emerg. Technol. 104, \n22–37 (2019)\n 23. Sun, S., Chen, J., Sun, J.: Traffic congestion prediction based \non GPS trajectory data. Int. J. Distrib. Sens. Netw. 15(5), \n1550147719847440 (2019)\n 24. Zhu, L., Krishnan, R., Guo, F., Polak, J.W., Sivakumar, A.: Early \nidentification of recurrent congestion in heterogeneous urban traf-\nfic. In: 2019 IEEE Intelligent Transportation Systems Conference \n(ITSC). IEEE, pp. 4392–4397 (2019)\n 25. Chen, M., Yu, X., Liu, Y.: PCNN: Deep convolutional networks \nfor short-term traffic congestion prediction. IEEE Trans. Intell. \nTransp. Syst. 19(11), 3550–3559 (2018)\n 26. Zhang, Z., Jiao, X.: A deep network with analogous self-attention \nfor short-term traffic flow prediction. IET Intel. Transport Syst. \n15(7), 902–915 (2021)\n 27. Tu, Y., Lin, S., Qiao, J., Liu, B.: Deep traffic congestion predic-\ntion model based on road segment grouping. Appl. Intell. 51(11), \n8519–8541 (2021)\n 28. Zhang, W., Yu, Y., Qi, Y., Shu, F., Wang, Y.: Short-term traf-\nfic flow prediction based on Spatio-temporal analysis and CNN \ndeep learning. Transportmetrica A: Transp. Sci. 15(2), 1688–1711 \n(2019)\n 29. Chen, Y.Y., Lv, Y., Li, Z., Wang, F.Y.: Long short-term memory \nmodel for traffic congestion prediction with online open data. In: \n2016 IEEE 19th International Conference on Intelligent Transpor-\ntation Systems (ITSC). IEEE, pp. 132–137 (2016)\n 30. Bai, M., Lin, Y., Ma, M., Wang, P., Duan, L.: PrePCT: traffic con-\ngestion prediction in smart cities with relative position congestion \ntensor. Neurocomputing 444, 147–157 (2021)\n 31. Ranjan, N., Bhandari, S., Zhao, H.P., Kim, H., Khan, P.: City-wide \ntraffic congestion prediction based on CNN, LSTM and transpose \nCNN. IEEE Access 8, 81606–81620 (2020)\n 32. Bogaerts, T., Masegosa, A.D., Angarita-Zapata, J.S., Onieva, E., \nHellinckx, P.: A graph CNN-LSTM neural network for short and \nlong-term traffic forecasting based on trajectory data. Transp. Res. \nPart C Emerg. Technol. 112, 62–77 (2020)\n 33. Di, X., Xiao, Y., Zhu, C., Deng, Y., Zhao, Q., Rao, W.: Traffic \ncongestion prediction by spatiotemporal propagation patterns. In \n2019 20th IEEE International Conference on Mobile Data Man-\nagement (MDM). IEEE, pp. 298–303 (2019)\n 34. Huang, Z., Xia, J., Li, F., Li, Z., Li, Q.: A peak traffic congestion \nprediction method based on bus driving time. Entropy 21(7), 709 \n(2019)\n 35. Dai, R., Xu, S., Gu, Q., Ji, C., Liu, K.: Hybrid spatio-temporal \ngraph convolutional network: improving traffic prediction with \nnavigation data. In: Proceedings of the 26th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery & Data Mining, pp. \n3074–3082 (2020)\n 36. Elfar, A., Talebpour, A., Mahmassani, H.S.: Machine learning \napproach to short-term traffic congestion prediction in a connected \nenvironment. Transp. Res. Rec. 2672(45), 185–195 (2018)\n 37. Pan, Z., Zhang, W., Liang, Y., Zhang, W., Yu, Y., Zhang, J., \nZheng, Y.: Spatio-temporal meta learning for urban traffic pre-\ndiction. IEEE Trans. Knowl. Data Eng. (2020)\n 38. Park, C., Lee, C., Bahng, H., Tae, Y., Jin, S., Kim, K., Ko, S., \nChoo, J.: ST-GRAT: a novel Spatio-temporal graph attention net-\nworks for accurately forecasting dynamically changing road speed. \n International Journal of Computational Intelligence Systems            (2023) 16:2 \n1 3    2  Page 16 of 16\nIn: Proceedings of the 29th ACM International Conference on \nInformation & Knowledge Management, pp. 1215–1224 (2020)\n 39. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., \nGomez, A.N., Kaiser, Ł. Polosukhin, I.: Attention is all you need. \nAdv. Neural Inf. Process. Syst. 30, 5998–6008 (2017)\n 40. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, \nA., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J.: \nTransformers: state-of-the-art natural language processing. In Pro-\nceedings of the 2020 Conference on Empirical Methods in Natural \nLanguage Processing: System Demonstrations, pp. 38–45 (2020)\n 41. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, \nB.: Swin transformer: hierarchical vision transformer using shifted \nwindows. In: Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pp. 10012–10022 (2021)\n 42. Lim, B., Arık, S.Ö., Loeff, N., Pfister, T.: Temporal fusion trans-\nformers for interpretable multi-horizon time series forecasting. \nInt. J. Forecast. 37(4), 1748–1764 (2021)\n 43. Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., Mei, Q.: Line: \nlarge-scale information network embedding. In: Proceedings of \nthe 24th international conference on world wide web, pp. 1067–\n1077 (2015)\n 44. Rodrıguez, P., Bautista, M.A., Gonzalez, J., Escalera, S.: Beyond \none-hot encoding: lower dimensional target embedding. Image \nVis. Comput. 75, 21–31 (2018)\n 45. Yuan, J., Zheng, Y., Zhang, C., Xie, X., Sun, G.Z.: An interactive-\nvoting based map matching algorithm. In: 2010 Eleventh interna-\ntional conference on mobile data management. IEEE, pp. 43–52 \n(2010)\n 46. Li, Y., Yu, R., Shahabi, C., Liu, Y.: Diffusion convolutional recur-\nrent neural network: data-driven traffic forecasting. arXiv preprint \narXiv: 1707. 01926 (2017)\n 47. Ma, J., Yarats, D.: On the adequacy of untuned warmup for adap-\ntive optimization. arXiv preprint 7. arXiv: 1910. 04209 (2019)\n 48. Glorot, X., Bengio, Y.: Understanding the difficulty of training \ndeep feedforward neural networks. In: Proceedings of the thir -\nteenth international conference on artificial intelligence and statis-\ntics. JMLR Workshop and Conference Proceedings, pp. 249–256 \n(2010)\n 49. Yu, B., Yin, H., Zhu, Z.: Spatio-temporal graph convolutional \nnetworks: a deep learning framework for traffic forecasting. arXiv \npreprint arXiv: 1709. 04875 (2017)\n 50. Wu, Z., Pan, S., Long, G., Jiang, J. and Zhang, C.: Graph wavenet \nfor deep spatial-temporal graph modeling. (2019) arXiv preprint \narXiv: 1906. 00121\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}