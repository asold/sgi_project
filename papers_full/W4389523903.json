{
  "title": "Systematic Assessment of Factual Knowledge in Large Language Models",
  "url": "https://openalex.org/W4389523903",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3052483792",
      "name": "Linhao Luo",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2126165162",
      "name": "Trang Vu",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2314522249",
      "name": "Dinh Phung",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A4320557257",
      "name": "Reza Haf",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4387322659",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W2980345007",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W4386081161",
    "https://openalex.org/W2547620388",
    "https://openalex.org/W2741253951",
    "https://openalex.org/W3173480423",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2785896739",
    "https://openalex.org/W4384615762",
    "https://openalex.org/W4380136141",
    "https://openalex.org/W4287815536",
    "https://openalex.org/W4308288330",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4200025781",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W4385573367"
  ],
  "abstract": "Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13272–13286\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSystematic Assessment of Factual Knowledge in Large Language Models\nLinhao Luo Thuy-Trang Vu Dinh Phung Gholamreza Haffari\nDepartment of Data Science and AI\nFaculty of Information Technology, Monash University, Australia\n{linhao.luo,trang.vu1,dinh.phung,gholamreza.haffari}@monash.edu\nAbstract\nPrevious studies have relied on existing\nquestion-answering benchmarks to evaluate the\nknowledge stored in large language models\n(LLMs). However, this approach has limita-\ntions regarding factual knowledge coverage, as\nit mostly focuses on generic domains which\nmay overlap with the pretraining data. This\npaper proposes a framework to systematically\nassess the factual knowledge of LLMs by lever-\naging knowledge graphs (KGs). Our frame-\nwork automatically generates a set of questions\nand expected answers from the facts stored in a\ngiven KG, and then evaluates the accuracy of\nLLMs in answering these questions. We sys-\ntematically evaluate the state-of-the-art LLMs\nwith KGs in generic and specific domains. The\nexperiment shows that ChatGPT is consistently\nthe top performer across all domains. We also\nfind that LLMs performance depends on the in-\nstruction finetuning, domain and question com-\nplexity and is prone to adversarial context.1\n1 Introduction\nThe rise of Large Language Models (LLMs) has\ngreatly improved the capabilities of natural lan-\nguage processing (NLP). However, one primary\nconcern with these models is the potential for ex-\ntrinsic hallucinations where LLMs generate state-\nments that cannot be verified from the source (Levy\net al., 2021; Ji et al., 2023). This issue severely\nimpairs the trustworthiness of LLMs and is par-\nticularly concerning when relying on LLMs for\ndecision-making. Rigorous evaluation is necessary\nbefore deploying them in critical applications.\nOne evaluation approach is to use question-\nanswering datasets to assess the language and\nknowledge capabilities of LLMs. Recent research\nhas mainly focused on evaluation using existing\nbenchmarks (Bommasani et al., 2023; Bang et al.,\n1Code and data will be released at https://github.com/\nRManLuo/llm-facteval\nFigure 1: Our proposed assessment framework gener-\nates a diverse set of questions to evaluate factual knowl-\nedge in LLMs.\n2023; Guo et al., 2023). While these bench-\nmarks are valuable for comparison and measuring\nprogress in LLM research, they may not provide\nsufficient assessment for production. Benchmarks\nconstructed from public datasets can pose infor-\nmation leakage problems due to overlap with pre-\ntraining data. Furthermore, constructing domain-\nspecific benchmarks is costly, requiring domain\nexpertise and adequate knowledge coverage.\nThis paper proposes a systematic approach to\nassess factual knowledge in LLMs by generating a\ncomprehensive assessment suite from knowledge\ngraphs (KGs) and evaluating the correctness of\nLLMs’ responses. The question generation process\nis carefully designed to ensure coverage of facts,\nas well as diversity and validity of the questions\n(Figure 1). Using this framework, we evaluate mul-\ntiple models from three LLM families on factual\nquestions derived from four KGs, covering both\ngeneric and specialized domains. Specifically, our\ncontributions are:\n• We propose a novel framework to evaluate\nfactual knowledge in LLMs by systematically\ngenerating valid and diverse questions from\nKGs while also ensuring knowledge coverage.\n• We observe that LLMs may abstain from an-\n13272\nswering certain questions, prioritizing preci-\nsion by avoiding the provision of inaccurate or\nhallucinated answers. We propose to use the\nF1 metric to take the abstention into account\nand ensure fair comparison across models.\n• We show that LLMs performance depends on\nseveral factors such as instruction finetuning,\ndomains, and question complexity. Despite\nsharing the same parametric knowledge base,\nmodels finetuned with different instruction\ndatasets show varying performance levels. In\ngeneral-domain KGs, LLMs achieve the high-\nest score, but their performance declines in\nspecialized domains and is worse on questions\nhaving a wide range of potential answers.\n• We assess robustness of LLMs to the prompt-\ning context and find they are highly sensitive\nto irrelevant information and are susceptible\nto being misled by antifactual contexts.\n2 Systematic Assessment Framework\nThis section describes the question generation com-\nponent in our proposed assessment framework, fol-\nlowed by the answer prompting strategy to collect\nLLM’s response and the evaluation metric.\n2.1 Question Generation\nOur framework leverages the facts stored in a KG,\norganized into triplets, i.e., (subject, relation la-\nbel, object) , to automatically generate a set of\nknowledge-based questions and answers satisfying\nthree requirements: (i) validity: questions should\nhave unique or verifiable answers ; (ii) coverage:\nquestions should cover all explicit facts; and (iii)\ndiversity: questions should vary in format and diffi-\nculty.\nIn this paper, we assume the complete KG and\ngenerate valid questions by considering the object\nof a given triplet as the reference answer and gener-\nating questions with the subject and relation label.\nTo ensure the question coverage and diversity, we\nutilize all available triplets and employ two ques-\ntion generation methods from a predefined tem-\nplate (Petroni et al., 2019) or using ChatGPT (Ope-\nnAI, 2023). We consider three types of questions:\ntrue-false question (TFQ), multiple choice ques-\ntion (MCQ), and short-answer question (SAQ). In\naddition, each question type can be represented in\ndifferent formats: true/false question, fill-in-the-\nbank (FiB) question, and Wh- question (Figure 3\nin Appendix).\nTrue-false question (TFQ) Given a triplet, we\ncreate factual questions that ask the LLM to deter-\nmine whether a given statement is true or false. For\nexample, given the triplet (Barack Obama, born\nin, Hawaii), we can generate a true statement “The\nbirth place of Barack Obama is Hawaii. ”. For false\nstatement, we randomly replace the object with a\nwrong entity.\nMultiple choice questions (MCQ)The LLM is\npresented with a list of answer candidates (choices)\nand is required to select the correct one. The candi-\ndates consist of the object along with randomly se-\nlected incorrect entities. We consider two formats\nfor MCQ: fill-in-the-blank (FiB) by replacing the\nreference object in the true statement with [MASK]\ntoken and Wh-question (Aigo et al., 2021).\nShort-answer questions (SAQ) Instead of pro-\nviding answer candidates as in MCQ, we ask the\nLLM to predict the correct answer directly in SAQ.\nFor many-to-many relations, we consider all possi-\nble objects as potential correct answers and request\nthe LLMs to list all possible answers.\n2.2 Evaluation\nAnswer Prompting We carefully design\nprompts to describe the task and instruct the LLMs\nto provide concise answers. We also verify the\nrobustness and consistency of LLMs by injecting\ndifferent types of knowledge into the question,\nincluding (i) relevant knowledge, (ii) irrelevant\nknowledge which is correct but not related to the\nquestion, and (iii) anti-factual knowledge that\nprovides false or erroneous information. The\ninjected knowledge can come from the relation\ndescription or extra evidence information, which\nare available in several knowledge graphs.\nMetric Although we prompt LLMs to provide\nbrief and concise answers, evaluating the correct-\nness of the generated answer is not trivial. A small\npercentage of generated answers are long and con-\ntain explanations. Hence, the standard exact match\nmetric used in question-answering tasks is not a\nsuitable metric. Instead, we use a fuzzy match met-\nric that checks if the generated answer appears in\nthe reference answers and vice versa.\nMany LLMs employ several guardrails to avoid\nproviding inaccurate or hallucinated answers which\nreturn an abstained answer (e.g., “I am unable to an-\nswer the questions without more knowledge.”). We\n13273\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 76.98 77.43 60.94 63.82 50.63 5.13 2.47 19.57 44.62\nLLaMA-7B 1.23 1.46 7.20 0.76 0.27 2.93 3.07 0.15 2.13\nAlpaca 65.07 60.65 41.95 40.50 41.68 7.68 6.01 8.42 34.00\nVicuna 52.83 51.84 15.47 18.28 33.84 3.79 4.81 7.83 23.59\nT5-XL 23.87 6.79 5.77 3.96 8.23 1.63 1.69 1.33 6.66\nFLAN-T5-XL 75.01 79.75 51.59 50.72 51.66 11.57 11.42 9.99 42.71\nFLAN-Alpaca 54.83 53.08 46.59 47.58 48.59 9.89 8.06 10.93 34.94\nFLAN-Vicuna 63.73 63.80 46.80 46.76 48.69 2.74 2.89 10.83 35.78\n% abstention 0% 25% 50% 75% 100%\nTable 1: Precision of LLMs on different types of questions generated from Google-RE: true-false question (TFQ),\nmulti-choice question (MCQ) and short-answer questions (SAQ). SAQ and MCQ questions can be in fill-in-the-blank\n(FiB) or Wh-question (Wh) format. TPL and GPT3.5 denote whether the questions are generated by the template\nand GPT3.5, respectively. The shade of the background color shows the percentage of abstained responses. The\nbest performance of each question type is marked in bold.\ndefine precision as the accuracy of non-abstained\nanswers\nP = correct\ncorrect + incorrect (1)\nP∗ = P × (1 − A) (2)\nand recall as the percentage of accuracy of all ques-\ntions\nR = correct\ncorrect + incorrect + abstained (3)\nThe F1 score F1 = 2× P×R\nP+R is the main evaluation\nmetric to compare the performance of LLMs.\n3 Experiments\n3.1 Setup\nDatasets We use four KGs in LAMA (Petroni\net al., 2019) and BioLAMA (Sung et al., 2021)\nbenchmarks to generate factual questions, includ-\ning two general-domain KGs: Google-RE (Petroni\net al., 2019), T-REx (Elsahar et al., 2018), and two\ndomain-specific KGs: WikiBio (Sung et al., 2021)\nin biology domain and ULMS (Bodenreider, 2004)\nin the medical domain. Each relation in the KGs is\nassociated with a predefined template to construct a\nnatural sentence from a given triplet. Detail descrip-\ntions of the datasets and the predefined templates\nare reported in Appendix A.1.\nLarge Language Models We evaluate the knowl-\nedge captured in several LLMs coming from\nthree backbones: (i) ChatGPT2 (OpenAI, 2023);\n2We did not assess GPT4 due to its high cost.\n(ii) LLaMA family, including LLaMA-7B (Tou-\nvron et al., 2023), Alpaca (Taori et al., 2023)\nand Vicuna (Chiang et al., 2023) which are in-\nstruction finetuned from LLaMA-7B backbone; and\n(iii) T5 family, including T5-XL (Raffel et al.,\n2020), FLAN-T5 XL (Chung et al., 2022) and\ntwo FLAN-T5-XL-based models which are instruc-\ntion finetuned on Alpaca and Vicuna datasets, de-\nnoted as FLAN-Alpaca (Chia et al., 2023) and\nFLAN-Vicuna respectively. The details regarding\nprompting can be found in Appendix A.2.\nExperiment Settings We employ two question\ngeneration methods: (i) template-based (TPL)\nwhere the subject is plugged into the provided tem-\nplate and the object is the ground-truth answer;\nand (ii) LLM-based where we use GPT-3.5-turbo\nto generate the questions. The question generation\nprompt can be found in Appendix C. Given a triplet,\nwe generate the TFQ with the ratio of true and false\nquestions set to 1 : 1. For MCQ, we randomly se-\nlect three incorrect entities and combine them with\nthe correct entities as the choices.\n3.2 Results\nPrecision We report the precision of LLMs on\nquestion generated from Google-RE in Table 1. As\nexpected, LLMs perform best on TFQ and worst\non SAQ due to the increasing difficulty level. Sur-\nprisingly, almost all LLMs struggle with FiB ques-\ntions, often returning abstentions or the [MASK]\ntoken without making any predictions. While\nFiB questions are commonly used in masked lan-\nguage model evaluation, we find that Wh-questions,\nwhich are more natural and occur more frequently\n13274\nChatGPTAlpacaVicunaFlan-AFlan-V0\n25\n50\n75\n100F1\nTFQ-GPT3.5\nChatGPTAlpacaVicunaFlan-AFlan-V0\n25\n50\n75\n100 MCQ-Wh-GPT3.5\nnone relevance irrelevance antifactual\nChatGPTAlpacaVicunaFlan-AFlan-V0\n25\n50\n75\n100 SAQ-GPT3.5\nFigure 2: F1 score of LLMs on Google-RE with different context prompt: none, relevant, irrelevant and antifactual\ncontext (best seen in color). FLAN-A and FLAN-B denote FLAN-Alpaca and FLAN-Vicuna respectively.\nin the instruction set, are more suitable for evaluat-\ning conversational LLMs. Moreover, we observe\ncomparable performance between template-based\nand GPT3.5-based questions.\nOverall, ChatGPT achieves the best average pre-\ncision. However, it also has a high percentage of\nabstained answers across all question types. Both\nthe LLaMA-7B and T5-XL models perform worse\nthan random guessing in TFQ and MCQ, indicat-\ning a failure to follow instructions due to the lack\nof training on instruction finetuning datasets. Al-\nthough sharing the same parametric knowledge\nbase (LLaMA-7B), Alpaca consistently outperforms\nVicuna. On the other hand, further instruction fine-\ntuning the FLAN-T5-XL does not improve precision.\nF1 Measure Table 2 shows the average F1 score\nacross all question types for each KG. The detailed\nbreakdown of F1 scores for each question type can\nbe found in Appendix B. Overall, ChatGPT out-\nperforms other LLMs, and models from the T5\nfamily generally perform better than those from the\nLLaMA family. Among the models from the same\nfamily, those fine-tuned on the Alpaca instruction\nset have better performance. This contrasts with\nthe above observation whereFLAN-T5-XL is the top\nperformer in terms of precision in the T5 family.\nWith the high abstention rate, it can be seen that\nFLAN-T5-XL tends to abstain from uncertain ques-\ntions to achieve higher precision, which comes at\nthe expense of losing recall for correct answers.\nImpact of Domain As shown in Table 2, the F1\nscores on TREx (general domain) are higher than\nthose in specific domains (WikiBio and UMLS).\nAdditionally, the relatively stronger performance\non WikiBio over UMLS can be attributed to the\npretraining data overlap as it is derived from\nWikipedia. Interestingly, all LLMs perform poorly\non the Google-RE dataset, despite also being ex-\ntracted from the general domain (Wikipedia). We\nGoogle-RE TREx WikiBio UMLS\nChatGPT 35.77 74.00 62.74 48.99\nLLaMA-7B 2.07 8.49 1.26 1.35\nAlpaca 32.56 61.00 41.99 36.84\nVicuna 22.86 41.08 25.78 22.99\nT5-XL 6.65 11.31 9.51 15.35\nFLAN-T5-XL 30.62 57.14 35.82 30.33\nFLAN-Alpaca 34.89 58.41 36.13 35.39\nFLAN-Vicuna 32.69 54.60 36.60 34.91\nTable 2: Average F1 score of LLMs across question\ntypes on different KGs. The best score is in bold.\nspeculate that this discrepancy may be attributed to\nthe complexity of the answer range of the Google-\nRE questions such as date-of-birth, birth place, and\ndeath place which have a wide answer range.\nRobustness to Adversarial ContextWe inject\ndifferent contexts to the questions of Google-RE\nevaluation set and reported the results in Figure 2.\nOur observations reveal that the responses of LLMs\nare highly sensitive to the contexts. Incorporating\nrelevant context leads to significant performance\nimprovement across all LLMs. Conversely, LLMs\nare prone to be misled by antifactual context, de-\nspite explicitly instructed to base their answers on\nreal-world facts. LLMs performance also decrease\nwhen conditioned on irrelevant contexts. These\nfindings highlight the lack of robustness in LLMs\nagainst adversarial examples. Ideally, a robust\nLLM should perform comparable in the absence\nof context or with irrelevant context. This poses\na challenge in deploying LLMs to production, as\nthey may inadvertently reinforce misinformation\nprovided by users.\n4 Related Works\nLLM Evaluation Evaluation of the Large Lan-\nguage Model (LLM) has gained increasing interest\namong researchers (Bommasani et al., 2023; Bang\n13275\net al., 2023; Guo et al., 2023). For instance, Bang\net al. (2023) conducts a multitask, multilingual, and\nmultimodal evaluation for ChatGPT. Holistic Eval-\nuation of Language Models (HELM) (Bommasani\net al., 2023) selects a broad of datasets and bench-\nmarks to evaluate the ability of LLMs. However,\nprevious works mostly focus on human evaluation\nand using existing datasets and benchmarks (Guo\net al., 2023). This requires lots of human effort and\ncannot guarantee the knowledge coverage to assess\nknowledge in LLMs comprehensively.\nFactual Knowledge Evaluation for LLMsEval-\nuating the factual knowledge of LLMs can ensure\nthe model is providing reliable and trustworthy\ninformation to users. Knowledge Graphs (KGs),\nwhich capture vast amounts of facts, offer a reli-\nable source of factual knowledge for evaluation\n(Pan et al., 2023; Luo et al., 2023). LAMA (Petroni\net al., 2019) adopts pre-defined templates to con-\nvert the facts in KGs into cloze questions then uses\nLLMs to predict the answers. The prediction re-\nsults are used to evaluate the knowledge stored in\nLLMs. Similarly, BioLAMA (Sung et al., 2021)\nand MedLAMA (Meng et al., 2021) assess the fac-\ntual knowledge of LLMs in medical domains by us-\ning medical knowledge graphs. Alex et al. (Mallen\net al., 2022) selects unpopular facts from Wiki-\ndata knowledge graphs which have low-frequency\nclicked entities to investigate the ability of LLMs\nto retain less popular factual knowledge. By enu-\nmerating all available factual triplets in KGs, we\ncould ensure the evaluation coverage of the fac-\ntual knowledge. Nevertheless, exciting methods\nlack a systematic framework containing question\ngeneration and evaluation modules. They often\nuse pre-defined templates for question generation\nwhich cannot provide diverse questions to evaluate\nthe knowledge of instruction-tuning LLMs (Sun\net al., 2023).\nAutomatically Question Generation from KGs\nTo assess knowledge in instruction-tuning LLMs,\nwe need to evaluate whether they have such knowl-\nedge and whether they can accurately express their\nknowledge, i.e. instruct following ability and ro-\nbustness. Therefore, given the same factual knowl-\nedge, we need to generate diverse questions at dif-\nferent levels of difficulty. Early works that gen-\nerate questions from KGs either use sequence-to-\nsequence models or graph neural networks to con-\nvert the triplet into a natural language question\n(Seyler et al., 2017; Kumar et al., 2019; Indurthi\net al., 2017; Chen et al., 2023). Recently, many\nmethods harness the ability of LLMs to generate\nquestions from KGs (Guo et al., 2022; Axelsson\nand Skantze, 2023). In this way, they can generate\nquestions with different diversities and complexi-\nties. Although there are previous works that gener-\nate questions from knowledge graphs, to the best of\nour knowledge, none of them adopt the generated\nquestions for evaluating the factual knowledge in\nLLMs.\n5 Conclusion\nWe propose a systematic framework to evaluate\nfactual knowledge of LLMs with the diverse and\nwell-coverage questions generated from KG. The\nexperiment reveals several factors affecting LLMs’\nperformance and highlights their vulnerability to\nadversarial context. Our findings contribute to un-\nderstanding LLMs’ capabilities and limitation in\nhandling factual knowledge.\nLimitations\nThe limitation of our work includes\n• Assuming a completed knowledge graph. In\nour work, we access the knowledge of LLMs\nby using the facts in knowledge graphs. How-\never, knowledge graphs are often incomplete,\nwhich could contain lots of implicit facts.\nThus, it could be inadequate to evaluate the\nLLMs with the existing KGs. In the future,\nwe plan to incorporate the knowledge graph\ncompletion methods and present a more com-\nprehensive assessment framework.\n• Focusing only on triplet-based facts. We only\nassess the knowledge of LLMs by using the\nquestion generated from the single triplet,\nwhich ignores the complex knowledge rep-\nresented by the combination of triplets. To\nassess the completed knowledge, we need to\ndesign a framework that considers the reason-\ning ability of LLMs on knowledge graphs.\n• Evaluating the correctness of multiple answer\nquestions. For N-M relations, we have multi-\nple answers to a question. However, the LLMs\nmight not return all the answers. How to eval-\nuate the partially answered questions is still\nan open question for accessing the knowledge\nof LLMs.\n13276\nEthics Statement\nOur work aims to design a framework that can au-\ntomatically assess the factual knowledge stored in\nlarge language models. In this research, we con-\nducted experiments on publicly available datasets\nand implemented our approaches using commonly\naccepted techniques, giving utmost consideration\nto fairness and avoiding potential biases. We ac-\nknowledge the significance of transparency and\nhave furnished comprehensive elucidations regard-\ning our methodology and decision-making process.\nTo conclude, our research adheres to ethical guide-\nlines and poses no potential risks.\nAcknowledgments\nThis research is supported by the ARC Future Fel-\nlowship FT190100039. The authors are grateful\nto the anonymous reviewers for their helpful com-\nments.\nReferences\nKosuke Aigo, Takashi Tsunakawa, Masafumi Nishida,\nand Masafumi Nishimura. 2021. Question generation\nusing knowledge graphs with the t5 language model\nand masked self-attention. In 2021 IEEE 10th Global\nConference on Consumer Electronics (GCCE).\nAgnes Axelsson and Gabriel Skantze. 2023. Using\nlarge language models for zero-shot natural language\ngeneration from knowledge graphs. In Proceedings\nof the Workshop on Multimodal, Multilingual Natu-\nral Language Generation and Multilingual WebNLG\nChallenge (MM-NLG 2023), pages 39–54.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267–\nD270.\nRishi Bommasani, Percy Liang, and Tony Lee. 2023.\nHolistic evaluation of language models. Annals of\nthe New York Academy of Sciences.\nYu Chen, Lingfei Wu, and Mohammed J Zaki. 2023.\nToward subgraph-guided knowledge graph question\ngeneration with graph neural networks. IEEE Trans-\nactions on Neural Networks and Learning Systems.\nYew Ken Chia, Pengfei Hong, Lidong Bing, and Sou-\njanya Poria. 2023. Instructeval: Towards holistic\nevaluation of instruction-tuned large language mod-\nels. arXiv preprint arXiv:2306.04757.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-REx: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection. arXiv\npreprint arXiv:2301.07597.\nShasha Guo, Jing Zhang, Yanling Wang, Qianyi Zhang,\nCuiping Li, and Hong Chen. 2022. Dsm: Question\ngeneration over knowledge base via modeling diverse\nsubgraphs with meta-learner. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4194–4207.\nSathish Reddy Indurthi, Dinesh Raghu, Mitesh M\nKhapra, and Sachindra Joshi. 2017. Generating natu-\nral language question-answer pairs from a knowledge\ngraph using a rnn based question generation model.\nIn Proceedings of the 15th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Volume 1, Long Papers, pages 376–385.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nVishwajeet Kumar, Yuncheng Hua, Ganesh Ramakrish-\nnan, Guilin Qi, Lianli Gao, and Yuan-Fang Li. 2019.\nDifficulty-controllable multi-hop question generation\nfrom knowledge graphs. In The Semantic Web–ISWC\n2019: 18th International Semantic Web Conference,\nAuckland, New Zealand, October 26–30, 2019, Pro-\nceedings, Part I 18, pages 382–398. Springer.\nSharon Levy, Michael Saxon, and William Yang Wang.\n2021. Investigating memorization of conspiracy theo-\nries in text generation. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4718–4729, Online. Association for Computa-\ntional Linguistics.\n13277\nLinhao Luo, Yuan-Fang Li, Gholamreza Haffari, and\nShirui Pan. 2023. Reasoning on graphs: Faithful and\ninterpretable large language model reasoning. arXiv\npreprint arxiv:2310.01061.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Inves-\ntigating effectiveness and limitations of paramet-\nric and non-parametric memories. arXiv preprint\narXiv:2212.10511.\nZaiqiao Meng, Fangyu Liu, Ehsan Shareghi, Yixuan Su,\nCharlotte Collins, and Nigel Collier. 2021. Rewire-\nthen-probe: A contrastive recipe for probing biomedi-\ncal knowledge of pre-trained language models. arXiv\npreprint arXiv:2110.08173.\nOpenAI. 2023. Gpt-4 technical report.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-\napu Wang, and Xindong Wu. 2023. Unifying large\nlanguage models and knowledge graphs: A roadmap.\narXiv preprint arxiv:2306.08302.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In EMNLP 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nDominic Seyler, Mohamed Yahya, and Klaus Berberich.\n2017. Knowledge questions from knowledge graphs.\nIn Proceedings of the ACM SIGIR International Con-\nference on Theory of Information Retrieval , pages\n11–18.\nKai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and\nXin Luna Dong. 2023. Head-to-tail: How knowl-\nedgeable are large language models (llm)? aka will\nllms replace knowledge graphs? arXiv preprint\narXiv:2308.10168.\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can language\nmodels be biomedical knowledge bases? In EMNLP,\npages 4723–4734.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st international ACM SIGIR conference\non research & development in information retrieval,\npages 1097–1100.\n13278\nKGs Domain #Relations #Entities #Triplet\nGoogle-RE General 3 7,242 55,28\nT-Rex General 46 31,180 34,039\nWikiBio Biology 5 68,39 17,582\nULMS Medical 17 18,910 64,305\nTable 3: Dataset Statistics\nA Implementation and Experiment Detail\nA.1 Dataset\nWe evaluate LLMs with the knowledge derived\nfrom the following KGs\n• T-REx (Elsahar et al., 2018), a knowledge\ngraph extracted from Wikipedia. T-REx in-\ncludes a relation label, a description, and a\ntemplate (Table 20) for each relation which\ncan be used to generate cloze sentences.\n• Google-RE (Petroni et al., 2019) is a subset of\nknowledge graphs containing three relations:\nplace of birth, date of birth, and place of death.\nThe fact triplets associated with each relation\nare extracted from Wikipedia and aligned with\na short piece of support text. Table 19 shows\nthe predefined template for each relation in\nGoogle-RE.\n• Wikipedia Biography Dataset (WikiBio)\n(Sung et al., 2021) is a biology knowledge\ngraph that is constructed by extracting the\nbiology-related facts from Wikidata. Table 21\nshows the template for each relation in Wik-\niBio.\n• Unified Language Medical System (ULMS)\n(Bodenreider, 2004) is a medical knowledge\ngraph constructed by domain experts. It con-\ntains information about various medical con-\ncepts and their relationships. Table 22 shows\nthe template for each relation in UMLS.\nTable 3 reports the domains and data statistics.\nA.2 Implementations\nLarge Language Model We use the Hugging-\nFace implementation of LLaMA and the T5 family.\nThe inference process is run on a single RTX8000\nGPU with 48GB memory with Mixed-precision\n(FP16).\nLLM #params Model Implementation\nChatGPT - GPT-3.5-turbo\nLLaMA-7B 7B Touvron et al. (2023)\nAlpaca 7B Taori et al. (2023)\nVicuna 7B Chiang et al. (2023)\nT5-XL 3B t5-3b\nFLAN-T5-XL 3B google/flan-t5-xl\nFLAN-Alpaca 3B declare-lab/flan-alpaca-xl\nFLAN-Vicuna 3B lmsys/fastchat-t5-3b-v1.0\nTable 4: Large language model (LLM) description and\nstatistics.\nTriplet: (Barack Obama, born in, Hawaii)\nQuestion Generation Methods\nTemplate-based ChatGPT-based\nPrompt: Please answer the following questions based on facts.\nShort-answer questions (SAQ)\nWh- question: Where was Barack Obama born?\nFill-in-blank: The birth place of Barack Obama is [MASK].\nMultiple choice questions (MCQ)\nChoices: New York, Miami, Hawaii....\nPrompt: Please select the answers from the given choices.\nFill-in-blank: The birth place of Barack Obama is [MASK].\nWh- question: Where was Barack Obama born?\nTrue-false questions (TFQ)\nPrompt: Please predict whether this fact is correct or not?\nFalse statement: The birth place of Barack Obama is Miami.\nTrue staement: The birth place of Barack Obama is Hawaii.\nFigure 3: Our question generation process iterates\nthrough all fact triplets and creates multiple question\ntypes for each triplet.\nQuestion Generation Given a triplet, we gener-\nate the TFQ with the ratio of true and false ques-\ntions set to 1 : 1. For MCQ, we randomly select\nthree incorrect entities and combine them with the\ncorrect entities as the choices. Table 5 shows the\nnumber of generated questions for each KG. We\nalso illustrate the example of template-based and\nLLM-based questions in Figure 3.\nAbstained Answer Detection Assessing the ac-\ncuracy of answers generated by LLMs in free text\nformat presents a challenge in determining both\nthe correctness of the answer and whether the\nmodel chooses to abstain from answering. For\nTFQ, instead of treating it as a binary classification\nproblem, we instruct the model to respond with\n\"UNKNOWN\" when uncertain, effectively trans-\nforming it into a 3-class text classification task.\nFor MCQ and ASQ, we compile a curated list of\nphrases that indicate abstention, such as \"cannot\n13279\nQuestion Google-RE TREx WikiBio UMLS\nTFQ 11,056 68,078 35,164 128,610\nMCQ 5,528 34,039 17,582 64,305\nSAQ 5,506 32,454 7,391 35,958\nTable 5: Number of generated questions for each ques-\ntion type: true-false question (TFQ), multi-choice ques-\ntion (MCQ), short-answer question (SAQ).\nG_RE TREx WikiBio UMLS\nTFQ 90.79 92.14 90.75 90.07\nFiB 92.11 92.78 91.86 89.39\nTable 6: Similarity (BERT score) between template-\nbased and LLM-based questions, w.r.t two question for-\nmats: true/false question (TFQ) and fill-in-blank (FiB).\npredict\" or \"I am sorry,\" and check if any of these\nphrases appear in the output. If such phrases are de-\ntected, we consider the answer to be an abstained.\nAnswer Prompting Each relation in Google-RE\nKG comes with the corresponding paragraphs from\nwhich it is extracted. We treat this paragraph as\nrelevant context for a given triplet. We sample the\nparagraph from an unrelated triplet, i.e. not sharing\nsubjects or objects as irrelevant context. For the\nantifactual context, we replace the correct answer\nwith a randomly selected entity from KG.\nB Additional Results\nQuestion Analysis We first evaluate the valid-\nity of the LLM-based questions by calculating the\nsimilarity between the template-based questions in\nTable 6. Then, we report the diversity of LLM-\nbased questions in Table 7. Since the templates are\nwritten by humans, higher similarities indicate the\nhigher validity of the LLM-based question. From\nthe results in Table 6, we can find that LLM-based\nquestions are highly similar to template-based ques-\ntions across all datasets w.r.t two question formats:\ntrue/false question (TFQ) and fill-in-blank (FiB).\nThis verifies the good quality of the LLM-based\nquestions which can be further used to assess the\nfactual knowledge of LLMs.\nAlthough the accountability of the template, the\ntask of defining diverse templates can be quite bur-\ndensome. Due to the lack of templates for Wh-\nquestions, we evaluate the diversity of Wh- ques-\ntions generated by ChatGPT using the self-bleu\nscores (Zhu et al., 2018). The lower the scores,\nthe more diverse the questions. From the results\nG_RE TREx WikiBio UMLS\nTFQ TPL 95.18 98.37 99.97 99.70\nFiB TPL 74.12 79.33 87.53 87.53\nWh- GPT3.5 50.39 64.97 81.78 78.98\nTable 7: Diversity measure (self-bleu score) of template-\nbased and LLM-based questions.\nin Table 7, we can see that compared to the TFQ\nand FiB questions generated based on templates.\nThe Wh- questions generated by ChatGPT achieve\na higher diversity, which provides a more natural\nand clear instruction to assess the knowledge.\nF1 score F1 score on different question types are\nshown in Tables 8 to 11.\nPrecision Precision score on different KGs are\nshown in Tables 12 to 14. Similar to Google-RE,\nChatGPT are the top performer across all KG, fol-\nlowed by FLAN-T5-XL.\nRecall Recall score on different KGs are shown\nin Tables 15 to 18.\nAdversarial Context The F1 score of different\nLLMs under different types of context is shown\nin Figure 4.\nC Example Prompts\nQuestion Generation Prompt The question gen-\neration prompts for TFQ, FiB and Wh- question\ncan be found in Table 23, Table 24 and Table 25\nrespectively.\nAnswer Prompts Table 26 provides the prompt\ntemplate for different LLM families.LLaMA-7B and\nmodels finefuned on Alpaca dataset are prompted\nwith the same instruction format. On the other\nhand, Vicuna and FLAN-T5-XL employ different\ntemplates. The instructions also vary for different\nquestion types and formats, as shown in Table 27.\n13280\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 65.06 67.23 47.25 53.20 46.36 2.22 1.21 3.63 35.77\nLLaMA-7B 1.11 1.10 7.20 0.76 0.27 2.92 3.07 0.15 2.07\nAlpaca 60.99 53.66 41.95 40.50 41.68 7.30 6.01 8.39 32.56\nVicuna 51.51 50.57 15.17 18.09 33.53 3.34 4.77 5.90 22.86\nT5-XL 23.85 6.76 5.77 3.96 8.23 1.63 1.69 1.33 6.65\nFLAN-T5-XL 31.82 26.15 51.59 50.72 51.66 11.57 11.42 9.99 30.62\nFLAN-Alpaca54.82 53.06 46.59 47.58 48.59 9.47 8.06 10.93 34.89\nFLAN-Vicuna52.92 49.91 46.80 46.76 48.67 2.74 2.89 10.83 32.69\nTable 8: F1 on different question types generated from Google_RE KGs.\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 85.43 82.38 90.32 87.17 87.61 49.17 52.57 57.38 74.00\nLLaMA-7B 2.10 3.63 2.69 3.82 4.36 31.73 19.42 0.19 8.49\nAlpaca 66.95 67.77 67.05 65.77 73.50 45.36 44.32 57.27 61.00\nVicuna 56.52 55.87 29.62 27.46 46.74 34.54 29.27 48.58 41.08\nT5-XL 20.79 21.99 7.23 4.99 12.75 6.70 1.52 14.47 11.31\nFLAN-T5-XL 66.88 58.98 77.46 73.80 78.87 36.14 26.78 38.22 57.14\nFLAN-Alpaca69.75 62.45 72.71 70.44 75.39 37.33 30.96 48.22 58.41\nFLAN-Vicuna70.43 64.65 74.71 70.75 76.42 19.41 18.42 41.99 54.60\nTable 9: F1 on different question types generated from TREx KGs.\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 71.27 69.07 81.77 86.57 79.07 36.45 38.72 39.02 62.74\nLLaMA-7B 0.32 3.49 0.36 0.38 2.78 1.27 1.43 0.08 1.26\nAlpaca 59.28 58.75 38.77 57.24 45.35 7.41 34.74 34.40 41.99\nVicuna 51.15 52.55 13.02 15.45 13.54 4.42 21.20 34.90 25.78\nT5-XL 16.85 29.05 5.65 6.46 5.17 0.96 9.07 2.87 9.51\nFLAN-T5-XL 35.43 50.36 44.23 63.17 52.57 10.44 14.09 16.30 35.82\nFLAN-Alpaca49.75 51.73 25.44 58.68 44.46 15.15 22.16 21.67 36.13\nFLAN-Vicuna55.21 54.79 45.89 63.06 49.72 3.62 3.02 17.45 36.60\nTable 10: F1 on different question types generated from WikiBio KGs.\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 68.12 54.67 70.78 83.52 63.11 21.43 18.05 12.21 48.99\nLLaMA-7B 0.84 1.73 0.47 0.30 1.28 3.04 3.12 0.00 1.35\nAlpaca 55.21 49.60 33.66 63.32 35.87 12.71 36.55 7.80 36.84\nVicuna 51.36 50.00 12.16 21.06 11.72 7.79 19.56 10.26 22.99\nT5-XL 32.29 30.81 5.87 9.30 6.95 1.57 34.34 1.69 15.35\nFLAN-T5-XL 23.80 34.12 41.41 69.90 45.69 12.45 8.89 6.40 30.33\nFLAN-Alpaca50.81 50.31 34.63 63.48 42.47 8.91 26.70 5.82 35.39\nFLAN-Vicuna51.54 51.51 46.46 72.98 45.99 3.73 1.50 5.53 34.91\nTable 11: F1 on different question types generated from UMLS KG.\n13281\nTFQ MCQ ASQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 88.63 85.10 91.40 88.13 88.11 57.38 58.36 64.12 77.65\nLLaMA-7B 3.22 5.17 2.69 3.82 4.36 32.21 19.43 0.19 8.89\nAlpaca 71.89 71.52 67.05 65.80 73.50 45.75 44.39 57.31 62.15\nVicuna 57.85 57.18 29.83 27.71 46.97 34.77 29.48 51.02 41.85\nT5-XL 20.81 22.24 7.23 4.99 12.75 6.70 1.53 14.47 11.34\nFlan-T5-XL 85.89 79.69 77.46 73.80 78.87 36.64 27.49 38.22 62.26\nFlan-Alpaca 70.30 63.18 72.71 70.47 75.39 37.33 30.98 48.22 58.57\nFlan-Vicuna 77.45 69.81 74.71 70.76 76.42 19.68 18.43 42.01 56.16\n% invalid responses 0% 25% 50% 75% 100%\nTable 12: Precision on different question types in TREx KGs. The shade of background color shows the percentage\nof invalid responses.\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 75.75 72.29 81.83 86.65 79.09 39.54 40.4 41.27 64.60\nLLaMA-7B 0.34 4.03 0.36 0.38 2.78 1.49 1.43 0.08 1.36\nAlpaca 62.08 60.33 38.77 57.24 45.35 7.94 34.74 34.58 42.63\nVicuna 52.5 53.72 13.12 15.57 13.57 4.5 21.26 35.44 26.21\nT5 17.13 29.2 5.65 6.46 5.17 0.96 9.07 2.87 9.56\nFLAN-T5 74.01 61.07 44.23 63.17 52.57 10.44 14.61 16.3 42.05\nFLAN-Alpaca 52.11 51.91 25.44 58.68 44.47 15.15 22.16 21.67 36.45\nFLAN-Vicuna 59.65 55.38 45.89 63.06 49.72 5.18 3.91 17.45 37.53\n% abstention 0% 25% 50% 75% 100%\nTable 13: Precision on different question types generated from wikibio KG. The shade of background color shows\nthe percentage of abstained responses.\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 73.20 59.09 71.09 83.60 64.04 22.76 18.43 14.39 50.83\nLLaMA-7B 0.94 1.96 0.47 0.30 1.28 3.04 3.12 0.00 1.39\nAlpaca 57.71 53.54 33.67 63.34 35.88 12.72 36.58 7.86 37.66\nVicuna 52.52 51.07 12.29 21.23 11.78 7.84 19.62 10.51 23.36\nT5-XL 32.32 31.40 5.87 9.31 6.95 1.57 34.36 1.69 15.43\nFlan-T5-XL 69.66 55.00 41.44 69.94 45.73 12.45 8.89 6.40 38.69\nFlan-Alpaca 54.63 51.56 34.67 63.53 42.52 8.91 26.72 5.82 36.05\nFlan-Vicuna 62.52 53.10 46.49 73.01 46.02 3.73 1.50 5.53 36.49\n% abstention 0% 25% 50% 75% 100%\nTable 14: Precision on different question types generated from UMLS KG. The shade of background color shows\nthe percentage of abstained responses.\n13282\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 56.33 59.41 38.59 45.60 42.75 1.42 0.80 2.00 30.86\nLLaMA-7B 1.01 0.89 7.20 0.76 0.27 2.92 3.07 0.15 2.03\nAlpaca 57.38 48.11 41.95 40.50 41.68 6.96 6.01 8.35 31.37\nVicuna 50.26 49.36 14.87 17.91 33.21 2.98 4.74 4.74 22.26\nT5-XL 23.83 6.74 5.77 3.96 8.23 1.63 1.69 1.33 6.65\nFLAN-T5-XL 20.20 15.64 51.59 50.72 51.66 11.57 11.42 9.99 27.85\nFLAN-Alpaca54.82 53.04 46.58 47.58 48.59 9.08 8.06 10.93 34.84\nFLAN-Vicuna45.24 40.99 46.80 46.76 48.64 2.74 2.89 10.82 30.61\nTable 15: Recall on different question types generated from Google_RE KGs.\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 82.46 79.84 89.27 86.23 87.12 43.02 47.83 51.93 70.96\nLLaMA-7B 1.56 2.8 2.69 3.82 4.36 31.27 19.4 0.19 8.26\nAlpaca 62.63 64.4 67.04 65.74 73.5 44.97 44.25 57.23 59.97\nVicuna 55.25 54.61 29.42 27.21 46.5 34.31 29.06 46.37 40.34\nT5-XL 20.78 21.76 7.23 4.99 12.75 6.7 1.52 14.47 11.28\nFLAN-T5-XL 54.76 46.81 77.46 73.8 78.87 35.66 26.1 38.22 53.96\nFLAN-Alpaca 69.2 61.74 72.71 70.4 75.39 37.33 30.93 48.22 58.24\nFLAN-Vicuna64.58 60.2 74.71 70.74 76.42 19.14 18.42 41.97 53.27\nTable 16: Recall on different question types generated from TREx KGs.\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 67.28 66.13 81.72 86.49 79.04 33.82 37.18 37.00 61.08\nLLaMA-7B 0.30 3.07 0.36 0.38 2.78 1.11 1.43 0.08 1.19\nAlpaca 56.72 57.25 38.77 57.24 45.35 6.95 34.74 34.21 41.40\nVicuna 49.88 51.42 12.92 15.33 13.51 4.34 21.15 34.39 25.37\nT5-XL 28.89 5.65 6.46 5.17 0.96 9.07 2.87 9.46\nFLAN-T5-XL 23.29 42.84 44.23 63.17 52.57 10.44 13.61 16.30 33.31\nFLAN-Alpaca47.59 51.56 25.44 58.68 44.45 15.15 22.16 21.67 35.84\nFLAN-Vicuna51.38 54.21 45.89 63.06 49.72 2.78 2.46 17.45 35.87\nTable 17: Recall on different question types generated from wikibio KGs.\nTFQ MCQ SAQ\nModel TPL GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5FiB-TPL FiB-GPT3.5 Wh-GPT3.5A VG\nChatGPT 63.70 50.86 70.48 83.43 62.21 20.24 17.70 10.60 47.40\nLLaMA-7B 0.75 1.55 0.47 0.30 1.28 3.04 3.12 0.00 24.36\nAlpaca 52.91 46.19 33.65 63.30 35.85 12.71 36.52 7.74 18.71\nVicuna 50.24 48.98 12.04 20.90 11.37 7.75 19.50 10.02 29.35\nT5-XL 29.29 28.36 5.86 9.29 6.94 1.57 34.31 1.69 18.63\nFLAN-T5-XL 14.35 24.73 41.38 69.87 45.65 12.45 8.89 6.40 30.13\nFLAN-Alpaca47.49 49.11 34.59 63.43 42.43 8.91 26.67 5.82 31.39\nFLAN-Vicuna43.84 50.00 46.44 72.94 45.95 3.73 1.50 5.53 34.27\nTable 18: Recall on different question types generated from UMLS KGs.\n13283\nRelation Type Template\ndate_of_birth N-1 The birth date of [X] is [Y].\nplace_of_birth N-1 The birth place of [X] is [Y].\nplace_of_death N-1 The death place of [X] is [Y].\nTable 19: Examples of question generation template for Google_RE, where [X] denotes the subject, and [Y] denotes\nthe object.\nRelation Type Template\ncapital 1-1 The capital of [X] is [Y].\nmember of political party N-1 [X] is a member of the [Y] political party.\nshares border with N-M [X] shares border with [Y].\nTable 20: Examples of question generation template for Trex, where [X] denotes the subject, and [Y] denotes the\nobject.\nRelation Type Template\ndrug used for treatment N-M The standard treatment for patients with [X] is a drug such as [Y].\nmedical condition treated N-M [X] has effects on diseases such as [Y].\ntherapeutic area N-M [X] cures diseases such as [Y].\nTable 21: Examples of question generation template for WikiBio, where [X] denotes the subject, and [Y] denotes\nthe object.\nRelation Type Template\nmay_be_prevented_by N-M [X] treats [Y].\ngene_mapped_to_disease N-M [X] has a genetic association with [Y].\nmay_be_finding_of_disease N-M [X] has symptoms such as [Y].\nTable 22: Examples of question generation template for UMLS, where [X] denotes the subject, and [Y] denotes the\nobject.\nTRUE -FALSE QUESTION\nI have a triplet extracted from a knowledge graph. The triplet is organized as (Subject, Relation,\nObject), which describes the relation between object and relation. Can you help me to generate a\nnatural language sentence to describe this triplet as accurate as possible?\n{ triplet }\nTable 23: Question generation prompts for true-false question format.\nFILL -IN-BLANK QUESTION\nI have a triplet extracted from a knowledge graph. The triplet is organized as (Subject, Relation,\nObject), which describes the relation between object and relation. Can you help me to generate a\nnatural language sentence to describe this triplet as accurate as possible and replace Object with\n[MASK]?\n{ triplet }\nTable 24: Question generation prompt for fill-in-blank question format.\n13284\nWH- QUESTION\nI have a triplet extracted from a knowledge graph. The triplet is organized as (Subject, Relation,\nObject), which describes the relation between object and relation. Can you help me to generate a\nquestion based on this triplet that the object is the corresponding answer? Please return the question\nonly.\n{ triplet }\nTable 25: Question generation prompt for Wh- question format.\nChatGPT { context }\n{ intructions }\n{ question }\nLLaMA-7B, Alpaca,\nFLAN-Alpaca\nBelow is an instruction that describes a task, paired with an input that provides\nfurther context. Write a response that appropriately completes the request.\n### Instruction:\n{ context }\n{ intructions }\n### Input:\n{ question }\n### Response:\nVicuna,\nFLAN-Vicuna\n{ context }\n{ intructions }\nHUMAN:\n{ question }\nASSISTANT:\nT5-XL,Flan-T5-XL { context }\n{ intructions }\nQUESTION: { question }\nTable 26: Inference prompt format for different LLMs.\n13285\nChatGPTLLaMAAlpacaVicuna T5 Flan-T5Flan-AFlan-V0\n50\n100F1\nTFQ-Template\nChatGPT AlpacaVicuna T5 Flan-T5Flan-AFlan-V0\n50\n100 TFQ-GPT3.5\nnone relevance irrelevanceantifactual\nChatGPTLLaMAAlpacaVicuna T5 Flan-T5Flan-AFlan-V0\n50\n100F1\nMCQ-FiB-Template\nChatGPTLLaMAAlpacaVicuna T5 Flan-T5Flan-AFlan-V0\n50\n100 MCQ-GPT3.5-Template\nChatGPT AlpacaVicuna T5 Flan-T5Flan-AFlan-V0\n50\n100F1\nMCQ-Q-GPT3.5\nChatGPTLLaMAAlpacaVicuna T5 Flan-T5Flan-AFlan-V0\n50\n100 SAQ-FiB-Template\nChatGPTLLaMAAlpacaVicuna T5 Flan-T5Flan-AFlan-V0\n50\n100F1\nSAQ-FiB-GPT3.5\nChatGPTLLaMAAlpacaVicuna T5 Flan-T5Flan-AFlan-V0\n50\n100 SAQ-Q-GPT3.5\nFigure 4: F1 score of LLM on google_re with different context prompt: no context (none), relevant, irrelevant and\nantifactual context.\nTFQ The following sentence describes a real-world fact. Please predict whether this fact is correct\nor not? Please only return correct or incorrect. If don’t know, please answer UNKNOWN. If\ncorrect, answer CORRECT. If incorrect, answer INCORRECT.\nFiB Please predict the missing words to complete the following sentence based on the facts in the\nreal-world. The missing words are represented by [MASK]. Please return the missing words\nonly.\nMCQ Please answer the following questions based on the facts in the real-world. Please select the\nanswers from the given choices and return the answer only.\nChoices: { choices }\nSAQ Please answer the following questions based on the facts in the real-world. Please keep the\nanswer as simple as possible and return the possible answers as a list.\nTable 27: Instructions for different question type.\n13286",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.6701705455780029
    },
    {
      "name": "Computer science",
      "score": 0.6409462094306946
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5993898510932922
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5757448077201843
    },
    {
      "name": "Domain knowledge",
      "score": 0.4391661584377289
    },
    {
      "name": "Data science",
      "score": 0.39056506752967834
    },
    {
      "name": "Natural language processing",
      "score": 0.35173630714416504
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3426269590854645
    },
    {
      "name": "Programming language",
      "score": 0.08048474788665771
    },
    {
      "name": "Geography",
      "score": 0.06619545817375183
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    }
  ]
}