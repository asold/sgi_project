{
  "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains",
  "url": "https://openalex.org/W3176936914",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1998996302",
      "name": "Yao, Yunzhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3192209706",
      "name": "Huang, Shaohan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1496033914",
      "name": "Wang Wen-hui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097573093",
      "name": "Dong Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2389670735",
      "name": "Wei, Furu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W2766839578",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W3099008231",
    "https://openalex.org/W1623072288",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3092115807",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3101140821"
  ],
  "abstract": "Large pre-trained models have achieved great success in many natural language processing tasks. However, when they are applied in specific domains, these models suffer from domain shift and bring challenges in fine-tuning and online serving for latency and capacity constraints. In this paper, we present a general approach to developing small, fast and effective pre-trained models for specific domains. This is achieved by adapting the off-the-shelf general pre-trained models and performing task-agnostic knowledge distillation in target domains. Specifically, we propose domain-specific vocabulary expansion in the adaptation stage and employ corpus level occurrence probability to choose the size of incremental vocabulary automatically. Then we systematically explore different strategies to compress the large pre-trained models for specific domains. We conduct our experiments in the biomedical and computer science domain. The experimental results demonstrate that our approach achieves better performance over the BERT BASE model in domain-specific tasks while 3.3x smaller and 5.1x faster than BERT BASE. The code and pre-trained models are available at https://aka.ms/adalm.",
  "full_text": "Adapt-and-Distill: Developing Small, Fast and Effective\nPretrained Language Models for Domains\nYunzhi Yao†∗, Shaohan Huang‡, Wenhui Wang‡, Li Dong‡, Furu Wei‡\n†Shandong University, Jinan, China\n‡Microsoft Research, Beijing, China\nyyz@mail.sdu.edu.cn\n{shaohanh,wenhui.wang,lidong1,fuwei}@microsoft.com\nAbstract\nLarge pretrained models have achieved great\nsuccess in many natural language processing\ntasks. However, when they are applied in spe-\nciﬁc domains, these models suffer from do-\nmain shift and bring challenges in ﬁne-tuning\nand online serving for latency and capacity\nconstraints. In this paper, we present a gen-\neral approach to developing small, fast and\neffective pretrained models for speciﬁc do-\nmains. This is achieved by adapting the off-\nthe-shelf general pretrained models and per-\nforming task-agnostic knowledge distillation\nin target domains. Speciﬁcally, we propose\ndomain-speciﬁc vocabulary expansion in the\nadaptation stage and employ corpus level oc-\ncurrence probability to choose the size of in-\ncremental vocabulary automatically. Then we\nsystematically explore different strategies to\ncompress the large pretrained models for spe-\nciﬁc domains. We conduct our experiments\nin the biomedical and computer science do-\nmain. The experimental results demonstrate\nthat our approach achieves better performance\nover the BERTBASE model in domain-speciﬁc\ntasks while 3.3× smaller and 5.1× faster than\nBERTBASE. The code and pretrained models\nare available at https://aka.ms/adalm.\n1 Introduction\nPre-trained language models, such as GPT (Rad-\nford et al., 2018), BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019) and UniLM (Dong\net al., 2019) have achieved impressive success in\nmany natural language processing tasks. These\nmodels usually have hundreds of millions of param-\neters. They are pre-trained on a large corpus of gen-\neral domain and ﬁne-tuned on target domain tasks.\nHowever, it is not optimal to deploy these models\ndirectly to edge devices in speciﬁc domains. First,\nheavy model size and high latency makes it difﬁcult\n∗Contribution during internship at Microsoft Research.\n(b) Distill-then-Adapt(a) From scratch\nDomain Corpus\n(c) Adapt-then-Distill\n (d) Adapt-and-Distill\nFigure 1: The four alternatives when distilling BERT\ninto speciﬁc domains. All strategies are task-agnostic.\nto deploy on resource-limited edge devices such as\nmobile phone. Second, directly ﬁne-tuning a gen-\neral pre-trained model on a domain-speciﬁc task\nmay not be optimal when the target domain varies\nsubstantially from the general domain. Thirdly,\nmany specialized domains contain their own spe-\nciﬁc terms, which are not included in pre-trained\nlanguage model vocabulary.\nIn this paper, we introduce AdaLM, a framework\nthat aims to develop small, fast and effective pre-\ntrained language models for speciﬁc domains. To\naddress domain shift problem, recent studies (Lee\net al., 2020; Gururangan et al., 2020) conduct con-\ntinual pre-training to adapt a general domain pre-\ntrained model to speciﬁc domains. However, spe-\nciﬁc domains contain many common in-domain\nterms, which may be divided into bite-sized pieces\n(e.g., lymphoma is tokenized into [l, ##ym, ##ph,\n##oma]). Gu et al.(2020) mentions that domain-\nspeciﬁc vocabularies play a vital role in domain\nadaptation of pre-trained models. Speciﬁcally, we\npropose a domain-speciﬁc vocabulary expansion\nin the adaptation stage, which augments in-domain\nterms or subword units automatically given in-\ndomain text. Also, it is critical to decide the size of\nincremental vocabulary. Motivated by subword reg-\narXiv:2106.13474v2  [cs.CL]  29 Jun 2021\nularization (Kudo, 2018), AdaLM introduces a cor-\npus occurrence probability as a metric to optimize\nthe size of incremental vocabulary automatically.\nWe systematically explore different strategies\nto compress general BERT models to speciﬁc do-\nmains (Figure 1): (a) From scratch: pre-training\ndomain-speciﬁc small model from scratch with do-\nmain corpus; (b) Distill-then-Adapt: ﬁrst distill-\ning large model into small model, then adapting it\ninto a speciﬁc domain; (c) Adapt-then-Distill: ﬁrst\nadapting BERT into a speciﬁc domain, then distill-\ning model into small size; (d) Adapt-and-Distill:\nadapting both the large and small models, then dis-\ntilling with these two models initializing the teacher\nand student models respectively.\nWe conduct experiments in both biomedical and\ncomputer science domain and ﬁne-tune the domain-\nspeciﬁc small models on different downstream\ntasks. Experiments demonstrate that Adapt-and-\nDistill achieves state-of-the-art results for domain-\nspeciﬁc tasks. Speciﬁcally, the 6-layer model of\n384 hidden dimensions outperforms theBERTBASE\nmodel while 3.3 × smaller and 5.1 × faster than\nBERTBASE.\n2 Related Work\nDomain adaptation of pre-trained model\nMost previous work on the domain-adaptation\nof pre-trained models targets large models. Lee\net al. (2020) conduct continual pre-training to\nadapt the BERT model to the biomedical domain\nusing the PubMed abstracts and the PMC full text.\nGururangan et al. (2020) also employ continual\npre-training to adapt pre-trained models into\ndifferent domains including biomedical, computer\nscience and news. However, many specialized\ndomains contain their own speciﬁc words that\nare not included in pre-trained language model\nvocabulary. Gu et al.(2020) propose a biomedical\npre-trained model PubMedBERT, where the\nvocabulary was created from scratch and the\nmodel is pre-trained from scratch. Furthermore, in\nmany specialized domains, large enough corpora\nmay not be available to support pre-training\nfrom scratch. Zhang et al. (2020) and Tai et al.\n(2020) extend the open-domain vocabulary with\ntop frequent in-domain words to resolve this\nout-of-vocabulary issue. This approach ignores\ndomain-speciﬁc sub-word units (e.g., blasto-,\ngermin- in biomedical domain). These subword\nunits help generalize domain knowledge and avoid\nunseen words.\nTask-agnostic knowledge distillation In recent\nyears, tremendous progress has been made in\nmodel compression (Cheng et al., 2017). Knowl-\nedge distillation has proven to be a promising way\nto compress large models while maintaining ac-\ncuracy (Sanh et al., 2019; Jiao et al., 2020; Sun\net al., 2020; Wang et al., 2020). In this paper, we\nfocus on task-agnostic knowledge distillation ap-\nproaches, where a distilled small pre-trained model\ncan be directly ﬁne-tuned on downstream tasks.\nDistilBERT (Sanh et al., 2019) employs the soft\nlabel and embedding outputs to supervise the stu-\ndent. TinyBERT (Jiao et al., 2020) and Mobile-\nBERT (Sun et al., 2020) introduce self-attention\ndistributions and hidden states to train the student\nmodel. MiniLM (Wang et al., 2020) avoids restric-\ntions on the number of student layers and employs\nthe self-attention distributions and value relation\nof the teacher’s last transformer layer to supervise\nthe student model. Because this method is more\nﬂexible, we implement MiniLM to compress large\nmodels in this work. No previous work system-\natically explores different strategies to achieve an\neffective and efﬁcient smaller model in speciﬁc\ndomains.\n3 Methods\n3.1 Overview\nWe systematically explore different strategies to\nachieve an effective and efﬁcient small model in\nspeciﬁc domains. We summarize them into four\nstrategies: from scratch, distill-then-adapt, adapt-\nthen-distill and adapt-and-distill.\nPretrain-from-scratch Domain-speciﬁc\npretraining from scratch employs a random\ninitialization of a pretrained model and pretrains a\nsmall model directly on domain-speciﬁc corpus.\nIn this work, we conduct pretraining from scratch\non different vocabularies including BERT original\nvocabulary, from scratch vocabulary, and expanded\nvocabulary.\nDistill-then-adapt These approaches ﬁrst distill\nthe large general pretrained model which pretrained\non Wikipedia and BookCorpus. Then it continues\nthe pretraining process using a domain-speciﬁc cor-\npus. In this work, we ﬁrst distill the BERT model\ninto a small model using task-agnostic knowledge\ndistillation in MiniLM (Wang et al., 2020). Then\nwe initialize the small model with it and conduct\ncontinual training with both the BERT original vo-\ncabulary and the expanded vocabulary.\nAdapt-then-distill In this work, we select differ-\nent large models as teacher models such as BERT\nand large models with different vocabularies. We\nﬁrst adapt these models into domain-speciﬁc mod-\nels and then implement MiniLM to compress them\nto small models.\nAdapt-and-distill In the previous part, when do-\ning knowledge distill, we initialized the student\nmodel randomly. In order to get a better domain-\nspeciﬁc small model, we try to explore the impact\nof the initialization of the student model. In this\npart, we adapt large and small models into spe-\nciﬁc domains separately, then use these two models\nto initialize the teacher and student model respec-\ntively.\n3.2 Domain Adaptation\nAdaLM contains a simple yet effective domain\nadaptation framework for a pretrained language\nmodel. As shown in Figure 2, it takes a general pre-\ntrained language model, original vocabulary and a\ndomain speciﬁc corpus as input. Through vocabu-\nlary expansion and continual pretraining, AdaLM\nadapts general models into speciﬁc domains.\nThe core pipeline of domain adaptation consists\nof the three steps described below:\n1. Given original vocabulary and a domain-\nspeciﬁc corpus, the vocabulary expansion\nmodule aims to augment original vocabulary\nwith domain-speciﬁc subword units or terms.\nWe augment domain-speciﬁc vocabulary from\nthe target domain, while keeping the original\nBERT vocabulary unchanged. We describe\nthem in more detail in Section 3.3.\n2. Due to the size of the vocabulary having\nchanged, we cannot initialize our model with\nBERT directly. As illustrated in Figure 3, we\ninitialize the original embedding and Trans-\nformer encoder with weights from BERT (the\ngreen part in Figure 3). For incremental vocab-\nulary, we ﬁrst tokenize them into sub-words\nwith the original vocabulary and then use an\naverage pooling of their own sub-words em-\nbedding to initialize. As shown in Figure 3,\nthe word ‘lymphoma’ is not included in BERT\nvocabulary. We tokenize it into three sub-\nwords (lym, ##pho, ##ma). The embedding\nGeneral BERT Original V ocab Domain Corpus\nV ocabulary Expansion\nExpanded \nV ocab\nContinual Pre-training\nModel Initial Data Preprocessing\nFigure 2: The pipeline of domain adaptation. Here we\nadapt the BERT model into the biomedical domain with\nPubMed dataset.\nvector of ‘lymphoma’ is initialized by the av-\nerage embedding vector of ‘lym’, ‘##pho’and\n‘##ma’.\n3. After model initialization and data prepro-\ncessing, we continually pretrain our model\nwith domain-speciﬁc corpus using masked\nlanguage model loss. Following BERT, we\nrandomly replace 15% of tokens by a special\ntoken (e.g., [MASK]) and ask the language\nmodel to predict them in continual pretrain-\ning.\n3.3 Vocabulary Expansion\nV ocabulary expansion is the core module of\nAdaLM. It augments domain-speciﬁc terms or sub-\nword units to leverage domain knowledge. The\nsize of the incremental vocabulary is a vital param-\neter for vocabulary expansion. Considering that\nunigram language modeling (Kudo, 2018) aligns\nmore closely with morphology and avoids prob-\nlems stemming from BPE’s greedy construction\nprocedure, as proposed in (Bostrom and Durrett,\n2020), we followed Kudo (2018) and introduced a\ncorpus occurrence probability as a metric to opti-\nmize the size of incremental vocabulary automati-\nTransformer Encoder\nOriginal Embedding\n[30522 ×768]\nExpanded Embedding\n[29709 ×768]\nInitial from Original BERT\nrecently\n3728\n3728     30735     9178      2764      2856 \nlymphoma\n30735\ndeveloped\n2764\nentity\n9178\nquickly \n2856\nrecently lymphoma entity developed quickly\nFigure 3: Concatenate original embedding with ex-\npanded embedding.\ncally. We assume that each subword occurs inde-\npendently and we assign to each subword in the\ncorpus a probability equal to its frequency in the\ncorpus.\n∀i xi ∈V,\n∑\nxi∈V\np(xi) = 1, (1)\nwhere Vis a pre-determined vocabulary. The prob-\nability of a subword sequence x = (x1,...,x M )\ncan be computed by the product of the subword\nappearance probabilities p(xi). We convert it to\nlogarithmic form:\nP(x) =\nM∑\ni=1\nlog(p(xi)), (2)\nGiven a domain-speciﬁc corpus D, the occur-\nrence probability of corpus Dis formulated as:\nP(D) =\n|D|∑\nx\nlog(P(x)), (3)\nwhere x represents tokenized sentence in corpus\nD.\nWe sample 550k sentences from the PubMed cor-\npus and compute the occurrence probability P(D)\nwith different vocabulary sizes. The results are\nshown in Figure 4. We compare the occurrence\nprobability with BERT and PubMedBERT vocabu-\nlaries. We observe that P(D) reveals a logarithmic\ntrend with substantial increases at the beginning\nand little inﬂuence after vocabulary size of 70k in\nthe biomedical domain. The PubMedBERT vocab-\nulary performs similarly to the 40k size vocabulary.\nWe present the occurrence probability of different\nvocabulary sizes in Appendix A.\n-260\n-250\n-240\n-230\n-220\n-210\n-200\n30 40 50 60 70 80 90 100\nincre vocab\nPubMedBERT vocabBERT vocab\nFigure 4: The P(D) of different vocab sizes under\nbiomedical domain. We use the BERT’s vocabulary as\nthe 30k vocabulary without vocabulary expanding. The\nPubMedBERT vocabulary is also 30k.\nWe propose a simple method to decide the size of\nthe incremental vocabulary. Assume the probability\nat the time step i−1 is Pi−1(D) and at the time\nstep iis Pi(D). If the rise Pi(D)−Pi−1(D)\nPi−1(D) is lower\nthan a threshold δ, we regard the vocabulary size\nat the time step ias the ﬁnal size.\nAlgorithm 1: V ocabulary Expansion\nInput: Original vocabulary raw vocab,\ndomain corpora D, threshold δand\nvocabulary size step V∆\nOutput: vocabfinal\ntoken count←whitespace split from D;\nP0 ←computed from raw vocab;\nV0 ←|raw vocab|;\ndo\nvocabulary size Vi ←Vi−1 + V∆;\nsub count←split token to subwords;\nSort sub countby frequency;\nincr vocab←keep (Vi −V0) subwords;\nvocabi ←raw vocab+ incr vocab;\nPi ←computed from vocabi\nwhile Pi−Pi−1\nPi−1\n>δ;\nreturn vocabfinal ←vocabi ;\nWe expand the domain-speciﬁc vocabulary with\nthe process shown in Algorithm 1. We implement\nour vocabulary expansion algorithm referring to\nSubwordTextBuilder in tensor2tensor1. In experi-\nments, we set the threshold δas 1% and vocabulary\nsize step V∆ as 10k. Finally, we obtain the ex-\npanded vocabulary size of biomedical as 60k and\ncomputer science domain as 50k.\n1https://github.com/tensorﬂow/tensor2tensor\n4 Experiment Details\nWe conduct our experiments in two domains:\nbiomedical and computer science.\n4.1 Datasets\nDomain corpus: For the biomedical domain, we\ncollect a 16GB corpus from PubMed2 abstracts to\nadapt our model. We use the latest collection and\npre-process the corpora with the same process as\nPubMedBERT (we omit any abstracts with less\nthan 128 words to reduce noise.).\nFor the computer science domain, we use the\nabstracts text from the arXiv3 Dataset. We select\nabstracts in computer science categories, collecting\n300M entries for the corpus.\nFine-tuning tasks: For the biomedical domain,\nwe choose three tasks: named entity recognition\n(NER), evidence-based medical information ex-\ntraction (PICO), and relation extraction (RE). We\nperform entity-level F1 in NER task and word-\nlevel macro-F1 in the PICO task. The RE task\nuses the micro-F1 of positive classes evaluation.\nJNLPBA (Collier and Kim, 2004) NER dataset con-\ntains 6,892 disease mentions, which are mapped\nto 790 unique disease concepts with BIO tagging\n(Ramshaw and Marcus, 1995). EBM PICO (Nye\net al., 2018) datasets annotates text spans with\nfour tags: Participants, Intervention, Comparator\nand Outcome. ChemProt (Krallinger et al., 2017)\ndataset consists of ﬁve interactions between chem-\nical and protein entities. We list the statistics of\nthose tasks in Table 1.\nWe ﬁne-tune two downstream tasks in the com-\nputer science domain. They are both classiﬁcation\ntasks. The ACL-ARC (Jurgens et al., 2018) dataset\nmainly focuses on analyzing how scientiﬁc works\nframe their contributions through different types\nof citations. SCIERC (Luan et al., 2018) dataset\nincludes annotations for scientiﬁc entities, their re-\nlations, and coreference clusters. The statistics are\navailable in Table 2.\n4.2 Implementation\nWe use the uncased version of BERTBASE (12 lay-\ners, 768 hidden size) as the large model and the\nMiniLM (6 layers, 384 hidden size) as the small\nmodel.\n2https://pubmed.ncbi.nlm.nih.gov/\n3https://www.kaggle.com/Cornell-University/arxiv\n4https://microsoft.github.io/BLURB/\nDataset Train Dev Test\nJNLPBA 46,750 4551 8,662\nEBM PICO 339,167 85321 16,364\nChemProt 18,035 11268 15,745\nTable 1: Biomedical dataset used in our experiment.\nAll selected from BLURB4\nDataset Train Task Test Classes\nACL-ARC 1,688 114 139 6\nSCIERC 3,219 455 974 7\nTable 2: Computer science dataset used in our exper-\niment. We use the same train, development, and test\nsplits as Gururangan et al. (2020)\nTo adapt the large model, we set the batch size\nat 8192 and the training step at 30,000. The peak\nlearning rate was set to 6e-4. To adapt the small\nmodel, we set the batch size as 256 and the training\nstep as 200,000. The learning rate is set to 1e-4.\nThe maximum length of the input sequence was\n512 and the token masking probability was 15%\nfor both the large model and the small model.\nWe implement MiniLM to compress large mod-\nels and follow the setting of MiniLM, where the\nbatch size was set to 256 and peak learning rate as\n4e-4. We set the training step as 200,000.\nFor biomedical tasks, we follow the setting of\nPubMedBERT (Gu et al., 2020) to ﬁne-tune these\nthree tasks. For computer science tasks, we use\nthe same setting as Gururangan et al. (2020). The\nconcrete parameters are shown in Appendix B.\n5 Results\nThe results of the tasks are shown in the Table 3\nand 4. We structure our evaluation by stepping\nthrough each of our three ﬁndings:\n(1) Domain-speciﬁc vocabulary plays a signiﬁ-\ncant role in domain-speciﬁc tasks and expanding\nvocabulary with the general vocabulary is better\nthan just using domain-speciﬁc vocabulary.\nWe observe improved results via the expanded\nvocabulary with both the large and small models.\nFor large model, AdaLM achieves the best results\nunder each domain, where 77.74 on biomedical\ndomain tasks, beating BioBERT and PubMedBERT\nand 77.76 on the computer science domain tasks.\nFor small models, in the biomedical domain,\nwhether we train from scratch or distill-then-adapt\nwith small models, incremental vocabulary mod-\nConﬁg Type Model Teacher JNLPBA PICO Chemprot Average\nL=12;d=786 Large model\nBERT† - 78.63 72.34 71.86 74.28\nBioBERT† - 79.35 73.18 76.14 76.22\nPubMedBERT† - 80.06 73.38 77.24 76.89\nAdaLM♦ - 79.46 75.47 78.41 77.74\nL=6;d=384\nSmall model MiniLM - 77.44 71.69 68.08 72.40\nFrom scratch\nBERT vocab (a) - 77.89 72.97 70.21 73.69\nPubMed vocab (b) - 77.82 73.82 70.32 73.99\nAdaLM vocab (c) - 77.80 73.39 70.86 74.02\nDistill-then-Adapt\nBERT vocab (d) - 78.63 74.00 72.28 74.97\nPubMed vocab (e) - 78.36 73.91 71.33 74.53\nAdaLM vocab (f) - 78.77 74.23 72.29 75.10\nAdapt-then-Distill\nRandom initial (g) BERT 77.98 72.38 68.86 73.07\nRandom initial (h) PubMedBERT 78.78 74.20 70.89 74.62\nRandom initial (i) AdaLM♦ 78.98 74.78 71.51 75.09\nAdapt-and-Distill Model (f) initial (j) AdaLM♦ 79.04 74.91 72.06 75.34\nTable 3: Comparison between different strategies on biomedical tasks. The AdaLM ♦ means we just adapt the\nlarge model without distillation. Scores of the methods marked with† are taken from (Gu et al., 2020). Underlined\ndata marks the small models whose performances surpass the BERT model’s performance. Land dindicate the\nnumber of layers and the hidden dimension of the model.\nels always perform better than the general vocabu-\nlary or just the domain-speciﬁc vocabulary. (When\ndistill-then-adapt with the PubMed vocabulary, we\ninitialize the word embedding in the same way as\nmentioned in Section 3.2). In addition, with distill-\nthen-adapt, the model (f) (75.10) can surpass the\nBERT model (74.28).\nIn the computer science domain, distill-then-\nadapt models with incremental vocabulary also\nshow great performance. Model (d) achieves a\ncomparable result of 72.91 as BERT and outper-\nforms BERT in the ACL-ARC datasets with 65.93\n(+1.01 F1). We also observe that when training\nfrom scratch, the results of Model (b) with incre-\nmental vocabulary are lower (1.45 lower) than that\nof model (a). This may be because after vocabu-\nlary expansion, a from-scratch model needs to be\npretrained with more unlabeled data.\n(2) Continual pretraining on domain-speciﬁc\ntexts from general language models is better than\npretraining from scratch.\nGu et al. (2020) ﬁnds that for domains with abun-\ndant unlabeled texts, pretraining language models\nfrom scratch outperforms continual pretraining of\ngeneral-domain language models. However, in our\nexperiments, we ﬁnd that general-domains model\ncan help our model to learn the target domain better.\nIn the biomedical domain, we use MiniLM model\nto initialize the model (d), (e) and (f) in distill-then-\nadapt setting. No matter which vocabulary is used,\ncontinual pretraining on domain-speciﬁc texts from\ngeneral language models is better than pretraining\nfrom scratch. For AdaLM vocabulary, the model\n(f) gets 75.10, outperforming the model (c) trained\nfrom scratch with the same vocabulary by 1.08. On\nthe other hand, for domains that do not have enor-\nmous unlabeled texts such as the computer science\ndomain in our experiments, continual pretraining\nalso showed better results. With continual pretrain-\ning, model (d) achieves higher results exceeding\nboth model (b) (+5.66 F1) and model (c) (+0.47\nF1).\n(3) Adapt-and-Distill is the best strategy to de-\nvelop a task-agnostic domain-speciﬁc small pre-\ntrained model.\nIn the Adapt-then-Distill part, our ﬁndings sup-\nports evidence from previous observations (Wang\net al., 2020) that a better teacher model leads to a\nbetter student model. Using AdaLM which per-\nforms best among large models as the teacher\nmodel can yield good results: 75.09 in the biomed-\nical domain and 71.62 in the computer science,\nbetter than other domain-speciﬁc large models. Fur-\nthermore, we ﬁnd that a better student model for\ninitialization can also help to get a better small\nmodel. In the Adapt-and-Distill part, we adapt\nlarge and small models into speciﬁc domains sepa-\nrately and then compress the adapted large model\nas the teacher with the adapted small model as ini-\ntialization. In the biomedical domain, the model\n(j), initialized from model (i), achieves the best\nresult of 75.34 among the small models. It also\nConﬁg Type Model Teacher ACL-ARC SCIERC Average\nL=12;d=786 Large model BERT - 64.92 81.14 73.03\nAdaLM♦ - 73.61 81.91 77.76\nL=6;d=384\nSmall model MiniLM - 61.5 72.88 67.19\nFrom scratch BERT vocab (a) - 62.48 74.93 68.70\nAdaLM vocab (b) - 59.57 74.93 67.25\nDistill-then-Adapt BERT vocab (c) - 65.75 79.13 72.44\nAdaLM vocab (d) - 65.93 79.88 72.91\nAdapt-then-Distill Random initial (e) BERT 63.12 77.89 70.50\nRandom initial (f) AdaLM ♦ 66.21 77.04 71.62\nAdapt-and-Distill Model (d) initial (g) AdaLM ♦ 68.74 78.88 73.81\nTable 4: Comparison between different strategies on computer science tasks. The AdaLM ♦ is the adapted large\nmodel without compressing. We report averages across ﬁve random seeds. Data marked with underlines are the\nresults of small models which outperform the BERT model’s.Land dindicate the number of layers and the hidden\ndimension of the model.\noutperforms the BERT model (+1.06 F1). In the\ncomputer science domain, model (g), initialized by\nmodel (d), is the only small model that outperforms\nBERT (+0.78 F1).\n6 Analysis\n6.1 Inference Speed\nWe compare AdaLM’s parameters’ size and infer-\nence speed with the BERT model in the biomedical\ndomain in Table 5.\nType Model #Params Speedup\nLarge\nBERT 109M ×1.0\nPubMedBERT 109M ×1.0\nAdaLM vocab 132M ×1.07\nSmall BERT vocab 22M ×5.0\nAdaLM 34M ×5.1\nTable 5: Comparison of model’s parameter size and the\ninference speed. The inference speedup is computed\nby the classiﬁcation task ChemProt and evaluated on a\nsingle NVIDIA P100 GPU.\nFirst we can ﬁnd that the vocabulary expansion\nyields marginal improvements on the model’s in-\nference speed. We added about 20M parameters\nin the embedding weights in the large model us-\ning AdaLM vocabulary, but its inference speed\nis slightly faster than BERT and PubMedBERT.\nSince most domain-speciﬁc terms are shattered into\nfragmented subwords, the length of the token se-\nquence we get by using the incremental vocabulary\nis shorter than the length of the sequence got by the\noriginal vocabulary, which reduces the computation\nload. We list the change of the sequence length of\nthe downstream tasks in Appendix C. Meanwhile,\nin the embedding layers, the model just needs to\nmap the sub-words’ id to their dense representa-\ntions, which is little affected by the parameters’\nsize. The small model shows the same trend.\nIn addition, the small model AdaLM shows great\npotential. Compared with the 12-layer model of\n768 hidden dimensions, the 6-layer model of 384\nhidden dimensions is 3.3x smaller and 5.1x faster\nin the model efﬁciency, while performing similarly\nto or even better than BERTBASE.\n6.2 Impact of Training Time\nPre-training often demands lots of time. In this sec-\ntion, we examine the adapted model’s performance\nas a function of training time. Here we use the\nbiomedical domain since its unlabelled texts are\nabundant and compare the large domain-speciﬁc\nadapted model with BioBERT. For every 24 hrs of\ncontinual pre-training, we ﬁne-tuned the adapted\nmodel on the downstream tasks. For comparison,\nwe convert the training time of BioBERT to the\ntime it may take with the same computing resource\nof this work (16 V100 GPUs).\nWe list the results in Table 6, we denote the large\nadapted model as AdaLM in the table. AdaLM at 0\nhrs means that we ﬁne-tune the initialized model di-\nrectly without any continual pre-training. We ﬁnd\nthat BERT is slightly better than 0hr AdaLM and\nafter 24 hrs, AdaLM outperforms BioBERT, which\ndemonstrates that domain-speciﬁc vocabulary is\nvery critical for domain adaption of pre-trained\nmodel. Our experiments demonstrate promising re-\nModel Training Time Average\nAdaLM\n0 hrs 74.25\n24 hrs 76.80\n48 hrs 77.36\n72 hrs 77.74\nBERT 0 hrs 74.28\nBioBERT 120 hrs 76.22\nTable 6: Results with different pre-training time. In\nthe table, AdaLM is the adapted large model without\ncompressing.\nsults in the biomedical domain. Under constrained\ncomputation, AdaLM achieves better performance\ncompared to BioBERT. More details can be found\nin Appendix D\n6.3 Impact of Vocabulary Size\nTo understand the impact of the vocabulary size,\nwe conduct some experiments with different vocab-\nulary sizes in the biomedical domain. We select\nthe biomedical large AdaLM model and to reduce\nthe computation load, we set the batch size as 256\nand step as 250K in our ablation studies. We show\nperformance of the model with different sizes in\nTable 7.\n40k 50k 60k 70k 80k\nJNLPBA 78.84 79.02 78.91 78.94 79.01\nPICO 75.09 74.81 74.99 74.58 75.00\nChemProt 76.10 76.80 77.21 76.40 76.85\nAverage 76.67 76.87 77.03 76.64 76.95\nTable 7: The performance of different vocabulary sizes\nWe observe that the model of 60k achieves the\nbest results in our ablation studies. The result is\na bit surprising. Despite having a larger vocab-\nulary, the 70k and 80k model does not show a\nstronger performance. A possible explanation for\nthese results may be that a larger vocabulary set\nmay contain some more complicated but less fre-\nquent words, which cannot be learnt well through\ncontinual pre-training. For example, the word fer-\nrocytochrome exists in 70k and 80k vocabularies\nbut is split into (‘ferrocy’, ‘##tochrom’, ‘##e’) in\nthe 60k vocabulary. In our sampled data (about\n550k sentences), ‘ferrocytochrome’ appears less\nthan 100 times, while the subword ‘##tochrom’ ap-\npears more than 10k times and ‘ferrocy’ appears\nmore than 200 times. The representation of those\nrare words cannot be learnt well due to the sparsity\nproblem.\n6.4 Vocabulary Visualization\nThe main motivation for using an the expanded\nvocabulary set is to leverage domain knowledge\nbetter. Compared to PubMedBERT which just uses\nthe domain-speciﬁc vocabulary and initializes the\nmodel randomly, the keep of the general vocabulary\nand the general language model’s weights may help\nus make good use of the existing knowledge and\nword embedding.\nTo assess the importance of the expanded vo-\ncabulary, we compute the L2-distance of the em-\nbedding weights before and after pre-training in\nour AdaLM model in the biomedical domain in\nFigure 5.\nOriginal vocab Domain-specific vocab\nFigure 5: The L2-distance of the embedding layer. The\ndeeper the color, the farther the distance.\nWe observe that the domain-speciﬁc vocabulary\npart changes a lot during the pre-training time,\nwhich indicates that our model learns much infor-\nmation about these domain-speciﬁc terms. We also\nobserve that there is little change in many original\nsub-words’ embedding weights, which indicates\nthat many general vocabularies can be used directly\nin continual training.\n7 Conclusion\nIn this paper, we investigate several variations to\ncompress general BERT models to speciﬁc do-\nmains. Our experiments reveal that the best strat-\negy to obtain a task-agnostic domain-speciﬁc pre-\ntrained model is to adapt large and small models\ninto speciﬁc domains separately and then compress\nthe adapted large model with the adapted small\nmodel as initialization. We show that the adapted 6-\nlayer model of 384 hidden dimensions outperforms\nthe BERTBASE model while 3.3× smaller and 5.0×\nfaster than BERTBASE. Our ﬁndings suggest that\ndomain-speciﬁc vocabulary and general-domain\nlanguage model play vital roles in domain adapta-\ntion of a pretrained model. In the future, we will\ninvestigate more directions in domain adaptation,\nsuch as data selection and efﬁcient adaptation.\nReferences\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4617–4624, Online.\nAssociation for Computational Linguistics.\nYu Cheng, Duo Wang, Pan Zhou, and Tao Zhang.\n2017. A survey of model compression and accel-\neration for deep neural networks. arXiv preprint\narXiv:1710.09282.\nNigel Collier and Jin-Dong Kim. 2004. Introduc-\ntion to the bio-entity recognition task at JNLPBA.\nIn Proceedings of the International Joint Workshop\non Natural Language Processing in Biomedicine\nand its Applications (NLPBA/BioNLP), pages 73–78,\nGeneva, Switzerland. COLING.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 13042–13054.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedi-\ncal natural language processing. arXiv preprint\narXiv:2007.15779.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4163–4174, Online. Association for Computational\nLinguistics.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-\nFarland, and Dan Jurafsky. 2018. Measuring the evo-\nlution of a scientiﬁc ﬁeld through citation frames.\nTransactions of the Association for Computational\nLinguistics, 6:391–406.\nMartin Krallinger, Obdulia Rabal, Saber A\nAkhondi, Martın P ´erez P ´erez, Jes ´us Santamar ´ıa,\nGP Rodr ´ıguez, et al. 2017. Overview of the\nbiocreative vi chemical-protein interaction track.\nIn Proceedings of the sixth BioCreative challenge\nevaluation workshop, volume 1, pages 141–146.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiﬁcation of enti-\nties, relations, and coreference for scientiﬁc knowl-\nedge graph construction. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3219–3232, Brussels, Bel-\ngium. Association for Computational Linguistics.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei\nYang, Iain Marshall, Ani Nenkova, and Byron Wal-\nlace. 2018. A corpus with multi-level annotations\nof patients, interventions and outcomes to support\nlanguage processing for medical literature. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 197–207, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nLance Ramshaw and Mitch Marcus. 1995. Text chunk-\ning using transformation-based learning. In Third\nWorkshop on Very Large Corpora.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nWen Tai, HT Kung, Xin Luna Dong, Marcus Comiter,\nand Chang-Fu Kuo. 2020. exbert: Extending pre-\ntrained models with domain-speciﬁc vocabulary un-\nder constrained training resources. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings, pages\n1433–1439.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In Advances in Neural\nInformation Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nRong Zhang, Revanth Gangi Reddy, Md Arafat Sul-\ntan, Vittorio Castelli, Anthony Ferritto, Radu Flo-\nrian, Efsun Sarioglu Kayi, Salim Roukos, Avi Sil,\nand Todd Ward. 2020. Multi-stage pre-training for\nlow-resource domain adaptation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5461–\n5468, Online. Association for Computational Lin-\nguistics.\nA Occurrence probability of different\nvocabulary sizes\nVocabulary P(D)\nBERT -255.92\nPubMed -218.49\n40k vocab -220.06\n50k vocab -214.40\n60k vocab -211.88\n70k vocab -210.44\n80k vocab -209.57\n90k vocab -208.86\n100k vocab -208.42\nTable 8: The P(D) of different vocabulary under\nbiomedical domain.\nVocabulary P(D)\nBERT -211.14\n40k vocab -194.08\n50k vocab -192.56\n60k vocab -191.87\n70k vocab -191.45\n80k vocab -191.09\n90k vocab -190.76\n100k vocab -190.53\nTable 9: The P(D) of different vocabulary under com-\nputer science domain.\nB Fine-tuning hyperparameters for\ndownstream tasks\nHyperameter Assignment\nNER PICO RE\nBatch size 32 {16,32} 32\nLearning rate {1e-5,3e-5,5e-5}\nEpoch {30-40} {10,15} {40-50}\nDropout 0.1\nTable 10: Hyparameters we used to ﬁnetune on biomed-\nical tasks.\nC Sequence Length\nAfter the vocabulary expansion, the length of the\ntoken sequence may get shorter. We compute the\naverage sentence length of the downstream tasks.\nWe list the results in Table 12\nHyperameter Assignment\nACL-ARC SCIERC\nBatch size 16\nLearning rate 2e-5\nEpoch 20\nDropout 0.1\nTable 11: Hyparameters we used to ﬁnetune on com-\nputer science tasks.\nDataset Original Vocab Incr. Vocab\nChemProt 66 53\nEBM PICO 36 31\nJNLPBA 41 32\nACL-ARC 53 50\nSCIERC 45 42\nTable 12: The sequence length tokenized by the origi-\nnal vocabulary and expanded vocabulary.\nD Results of different training time\nWe list the biomedical tasks’ results of each pre-\ntraining time in the following table.\n0h 24h 48h 72h\nJNLPBA 77.56 79.14 79.11 79.46\nPICO 73.29 74.22 75.28 75.36\nChemProt 71.91 77.06 77.69 78.42\nAverage 74.25 76.80 77.36 77.74\nTable 13: The performance of models with different\npretraining time",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8622657060623169
    },
    {
      "name": "Vocabulary",
      "score": 0.6728144884109497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6232969164848328
    },
    {
      "name": "Language model",
      "score": 0.6181035041809082
    },
    {
      "name": "Domain adaptation",
      "score": 0.6168339252471924
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5494741797447205
    },
    {
      "name": "Machine learning",
      "score": 0.5420480966567993
    },
    {
      "name": "Task (project management)",
      "score": 0.48224005103111267
    },
    {
      "name": "Natural language processing",
      "score": 0.4696749150753021
    },
    {
      "name": "AKA",
      "score": 0.4585454761981964
    },
    {
      "name": "Code (set theory)",
      "score": 0.44136282801628113
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4195806086063385
    },
    {
      "name": "Programming language",
      "score": 0.06896069645881653
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Library science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}