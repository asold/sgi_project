{
    "title": "The implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA",
    "url": "https://openalex.org/W2765994921",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2357512762",
            "name": "Hao Yufeng",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Quigley, Steven",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2591922920",
        "https://openalex.org/W1528140509",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W1846874585",
        "https://openalex.org/W2588448445",
        "https://openalex.org/W2136922672",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2252143850",
        "https://openalex.org/W2150355110",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W1595187495"
    ],
    "abstract": "Recently, FPGA has been increasingly applied to problems such as speech recognition, machine learning, and cloud computation such as the Bing search engine used by Microsoft. This is due to FPGAs great parallel computation capacity as well as low power consumption compared to general purpose processors. However, these applications mainly focus on large scale FPGA clusters which have an extreme processing power for executing massive matrix or convolution operations but are unsuitable for portable or mobile applications. This paper describes research on single-FPGA platform to explore the applications of FPGAs in these fields. In this project, we design a Deep Recurrent Neural Network (DRNN) Language Model (LM) and implement a hardware accelerator with AXI Stream interface on a PYNQ board which is equipped with a XILINX ZYNQ SOC XC7Z020 1CLG400C. The PYNQ has not only abundant programmable logic resources but also a flexible embedded operation system, which makes it suitable to be applied in the natural language processing field. We design the DRNN language model with Python and Theano, train the model on a CPU platform, and deploy the model on a PYNQ board to validate the model with Jupyter notebook. Meanwhile, we design the hardware accelerator with Overlay, which is a kind of hardware library on PYNQ, and verify the acceleration effect on the PYNQ board. Finally, we have found that the DRNN language model can be deployed on the embedded system smoothly and the Overlay accelerator with AXI Stream interface performs at 20 GOPS processing throughput, which constitutes a 70.5X and 2.75X speed up compared to the work in Ref.30 and Ref.31 respectively.",
    "full_text": "The implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n1 \nAbstractâ€”Recently, FPGA has been increasingly applied to \nproblems such as speech recognition, machine learning, and cloud \ncomputation such as the Bing search  engine used by Microsof t. \nThis is due to FPGAsâ€™ great parallel computation capacity as well \nas low power consumption compared to general purpose \nprocessor s. However, these applications mainly focus on large \nscale FPGA clusters which have an extreme processing power for \nexecuting massive matrix or convo lution operation s but are \nunsuitable for portable or mobile applications . Th is paper \ndescribes research on single -FPGA platform to explore the \napplications of FPGAs in these fields. In this project, we design a \nDeep Recurrent Neural Network (DRNN) Language  Model (LM) \nand implement a hardware accelerator with AXI Stream interface \non a PYNQ board which is equipped with a XILINX ZYNQ SOC \nXC7Z020-1CLG400C. The PYNQ has not only abundant \nprogrammable logic resources but also a flexible embedded \noperation system,  which makes it suitable to be applied in the \nnatural language processing field. We design the DRNN language \nmodel with Python and Theano, train the model on a CPU \nplatform, and deploy the model on a PYNQ board to validate the \nmodel with Jupyter notebook. Meanwhile, we design the hardware \naccelerator with Overlay , which is a kind of hardware library on \nPYNQ, and verify the acceleration effect on  the PYNQ board. \nFinally, we have found that the DRNN language model can be \ndeployed on the embedded system smooth ly and the Overlay \naccelerator with AXI Stream interface performs at 20 GOPS \nprocessing throughput, which constitutes a 70.5X and 2.75X speed \nup compared to the work in Ref.[30] and Ref.[31] respectively. \nIndex Termsâ€”Deep Recurrent Neural Network (DRNN), \nlanguage model, overlay, PYNQ. \nI. INTRODUCTION \nn contemporary society, researchers have made a vast \nnumber of achievements in the Artificial Intelligence (AI) \nand Machine Learning (ML) fields. In particular , the \nsuccess of alpha Go has greatly raised confidence in machine \nfor processing the human-machine interaction field. However, \nin the Natural Language Processing field, it is still difficult to \nhave machines understand language. This problem seems to be \nthe significant obstacle in progress into the AI age. It has also \nbecome one of the most challenging topic s in the Natural \nLanguage Processing (NLP) field. As the core part of NLP, the \nLanguage Model (LM) play an important role in this processing. \nA successful LM not only improves recognition accuracy, but \nalso speed s up the decoding process. Furthermore, with \ntelecommunication and computer technology development, our \nsmart mobile devices have become increasingly powerful in \ncomputing capacity which has led to the mobile internet \nrevolution. This paper will seek to find a way to deploy a LM \ninto a mobile device and verify the results on a prototype \nmachine. \nA. Background \nNowadays, Natural Language Processing (NLP) has become \na large interdisciplinary research topic which includes \nlinguistics, probability, statistics, Analogue and Digital Signal \nProcessing, Neural Network s, Machine Learning, Parallel \nComputing, Embedded System s, Software and Hardware \nSystem Design, and Programming Languages. There are many \nscenarios such as Machine Translation, Intelligent Navigation, \nSpeech Recognition, and Voice Search, using the NLP \ntechniques. Especially, with the development of smart mobile \ndevices, the requirement for intelligent voice interaction in \nsmart devices becomes increasingly stronger.  \nLanguage Model (LM) as the core part of the NLP system, \nmaps the words or characters into sentences in the decoding \nstage, and eventually obtains a maximum probability sentence \nwhich generally is closer to human beingâ€™s speaking habits. In \nthe past several decades, researchers have used different kinds \nof theories and strategies to structure the Language Model and \ngenerate lots of useful and important technologies such as \nGaussian Mixture Model s (GMM), Hidden Markov Model s \n(HMM), and N-gram LM. Most of speech recognition system \nused GMMs as the probability model of character s or words. \nThis model is easy to estimate and train while it recognizes \ndifferent states very well. But the GMM actually is a shallow \nlayer network and it is inefficient to build a model for nonlinear \ndata set. In 2006, an effective method of training  the Deep \nBelief Network (DBN) [1] was presented by Geoffrey Hintonâ€™s \nteam, which solved the gradient vanishing problem in deep \nneural networks and paved the way to the application of Deep \nNeural Networks, which usually contain more than two hidden \nlayers. From then on, researchers started to introduce the Deep \nNeural Network into Language Model and Acoustic Model \nresearch. Dr. Li Dengâ€™s and Dong Yuâ€™s team from Microsoft \nwere the first researchers to develop a Deep Neural Network \nspeech processing system successfully  [2]. The speech \nrecognition system based on DNN ha s a different framework \ncompared to previous speech recognition technologies because \nDNN can generate high dimen sional features through multi -\nYufeng Hao                                                                      Steven Quigley \nDept. of Electronic, Electrical and                                    Dept. of Electronic, Electrical and  \nSystems Engineering                                                         Systems Engineering \nUniversity of Birmingham, Edgbaston,                                University of Birmingham, Edgbaston, \nBirmingham, B152TE, UK                                                       Birmingham, B152TE, UK \nE-mail: yxh663@student.bham.ac.uk                                  E-mail:S.F.QUIGLEY@bham.ac.uk \nThe implementation of a Deep Recurrent Neural \nNetwork Language Model on a Xilinx FPGA \nI \nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n2 \nlayers features extraction. The DNN model system reduces the \nerror rate by 30% compared to conventional model s in \ncontinuous speech recognition condition [2].  \nHowever, these DNN language or acoustic models have been \ndeveloped and applied on large machine s with powerful \ncomputation capacity, and usually require a Graphics \nProcessing Unit (GPU) to accelerate computation. However, \nembedded systems usually have limited computing units and \nprogramming capacity. It is challenging bu t interesting to \nimplement and deploy a DNN model in an embedded system; \nit is also meaningful because there are a vast number of mobile \ndevices with embedded system s. If an effective and efficient \nDNN model can be developed in a small embedded system, this \nwill be a promising application area for embedded systems and \nDNNs. Some researchers have launched DNN on Embedded \nSystem Projects. A TensorFlow -on-Raspberry-pi Project was \nissued by Sam Abrahams in GitHub.com [3]. In this project, he \nfocused on how to install TensorFlow, which is a very popular \nNeural Network framework programming tool, in an embedded \nsystem such as a Raspberry Pi. The installation is appropriate \nfor other embedded system based on the ARM architecture. \nAlso, Matthew Rubashkin, an enginee r from Silicon Valley \nData Science, ha s implemented a TensorFlow Image \nRecognition on a Raspberry Pi [4], which runs well and is able \nto recognize different pictures with a high accuracy. Moreover, \non GitHub a Xilinx research group published a Binary Neural \nNetwork (BNN) project on an FPGA [5], which convert s the \nfloating point weights and activations in conventional neural \nnetwork into binary values. The Neural Network model is \npurely implemented on an FPGA with High Level System tools. \nIn this way, the size of the model and the computation volume \ncan be reduced dramatically, reaching a level that can be run on \nembedded system. Also further research work done by  Eriko \nNurvitadhi, et al. from Accelerator Architecture Lab, Intel \nCorporation, realizes almost 50x acceleration performance \nimprovement over the baseline CPU  [6]. These works have \ngreatly promoted the application of DNN in Embedded Systems. \nB. Objective and Methodology \nInspired by the progression of NLP and the above Neural \nNetwork on Embedded System projects, we try to implement a \nDeep Recurrent Neural Network (DRNN) Language Model on \na PYNQ board, which is equipped with a ZYNQ-7020-\n1CLG400C and supports Python and Jupyter notebook \nprogramming. Moreo ver, considering that the Field \nProgrammable Gate Array (FPGA) SOC platform has plenty of \nparallel computation resources in its Programmable Logic (PL) \nside and the ability to execute flexible programs in  its \nProcessing System (PS) side with a 32-bit ARM core, we can \nutilize the Overlay which is a kind of FPGA hardware library  \nused in PYNQ board to create a hardware accelerator for the \nDRNN.  \nWe use Python to build the program for data processing and \nDRNN training and verification. Python is very popular in  \nscientific computation and data processing. It is supported well \nby most of deep learning frameworks such as TensorFlow, \nTheano, Caffe, and so on, and there are plenty of software \nlibraries in Python such as Numpy, Scipy, Scikit-learn, and so \non, which gr eatly facilitate data  sampling, analysis, and \nprocessing. \nFor terms of module training and verification, we chose the \nTheano Deep Learning Framework. We compare d several \nNeural Network framework programming tools such as \nTensorFlow, Theano, Torch7, Caffe. We f ound that Theano \nsupports Python and runs well under a 32 bits Linux Operation \nSystem (OS), whereas TensorFlow is powerful but it is more \nsuitable to run under a 64 bit OS because its SparseTensor has \nto be defined in a 64 bit data type. Torch7 and Caffe are both \npowerful and popular DNN programming framework s, but \nTorch7 does not support Python and Caffe is also a little large \nfor embedded system applications. \nII. PYNQ ARCHITECTURE AND FUNC TIONS \nPYNQ is the abbreviation of Python Productivity for ZYNQ \n[21]. From the hardware architecture perspective, the core chip \nof PYNQ is a Xilinx ZYNQ Chip, which is a FPGA SOC \nplatform combin ing Programmable Logic (PL) with \nProgrammable System (PS) to perform signal sampling, \nprocessing, and control. Form the software p erspectiv e, \nintegrated with the Python language and other programming \nlibraries, PYNQ makes it convenient to develop embedded \nsystems based on FPGA. The PYNQ board is shown in Fig.1. \n \nFig.1. PYNQ board  \nThe ZYNQ chipâ€™s internal architecture is shown in Fig. 2. \nThe SOC chip consists of a Dual-Core ARM cortex -A9, a \nprogrammable logic part, and other peripheral interfaces such \nas USB, HDMI, Ethernet, GPIO, and Mic in. The Vivado \nDesign Suite tool can be used to design and configure the PS, \nPL, and Overlay. An Overlay is a kind of hardware library \ninitiated by Xilinx, which establishes a connection between \nhardware and software through a Python API. With th is \nconnection, it is convenient to ex change data between the \nsoftware program and logic hardware, which will be helpful to \nbuild an accelerator for the algorithm operation. More \nspecifically, because Python is a popular language in natural \nlanguage processing, machine learning, and artificial  \nintelligence, the PYNQ board is suitable for performing similar \napplications in these fields. \n\nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n3 \n \nFig.2. ZYNQ chip internal architecture  [7] \nIII. RECURRENT NEURAL NETWORK AND LANGUAGE MODEL \nA. Recurrent Neural Network \nAn RNN is a kind of neural network  that has a memory \nfeature. For general neural networks, from input layer to hidden \nlayer and output layer, the neuron cells in the adjacent layer s \nare fully connected and those in the same layer have no \nconnection [8]. But things are different for the RNN. The \nhidden layer cells of an RNN look like a combination of general \nneural network cell s in a sequence loop, where one output of \none cell will be the input of the next one. That means there exist \nconnections between the neuron cells in the hidden layer of the \nRNN. T hese connections give the  RNN the capacity to \nremember previous information. The unit diagram of RNN is \nshown below. \n \nFig.3. RNN u nit perspective modified from [9 ] \nThe relationship between the symbols in the above figure \ncan be described by the following formulas: \nğ‘ ğ‘¡ = tanh(ğ‘ˆğ‘¥ğ‘¡âˆ’1 + ğ‘Šğ‘ ğ‘¡âˆ’1)                         (2 âˆ’ 1) \nğ‘œğ‘¡ = softmax(ğ‘‰ğ‘ ğ‘¡ )                                (2 âˆ’ 2) \nğ¸ğ‘¡ = âˆ’ğ‘œğ‘¡Ì‚  log ğ‘œğ‘¡                                    (2 âˆ’ 3) \nğ¸ = âˆ‘ ğ¸ğ‘¡\nğ‘‡\nğ‘¡\n                                         (2 âˆ’ 4) \nAs we can see from the above figure and formulas, there are \nhidden states labelled as ğ‘ ğ‘¡  (t=1, 2, Â· Â· Â·  t-1, t, t+1Â· Â· Â· ) in the \nunfolded RNN cell. The hidden state is the special unit that is \nused by the RNN. It is a nonlinear transformation unit as \nmentioned previously and can be obtained from formula (2-1). \nWhen t=0, ğ‘ ğ‘¡âˆ’1 will be all 0s. Through this operation, hidden \nstates extract all previous input data features and map them into \na higher dimension ; therefore the RNN can record the data \nfeatures extracted from previous input data [ 10]. This is how \nthe memory feature of an RNN arise.  \nThe other important feature of the RNN is processing of \nsequence data. The ğ‘¥ğ‘¡ (t=1, 2, Â· Â· Â·  t-1, t, t+1Â· Â· Â· ) in figure 2-3 \nrepresents the input vector arranged in time sequenc e. Each \ninput vector combined with the previous hidden state generates \nthe input for the next hidden state unit. This is the reason for the \nname â€œrecurrent neural network â€. Therefore the RNN is \nsuitable to process natural language and other time sequence \nprediction events [11]. In these events, the input data can be \nreceived as sequence data , e.g.  word by word in natural \nlanguage processing, or frame by frame in auto matic spoken \nlanguage recognition. \nThe cross entropy is often used to calculate the loss between \nthe predict ed value and the target value [ 12]. In supervised \nlearning, the loss function is used to measure the difference \nbetween prediction values and actual value. As the formula (2-\n3) shows, the cross entropy ğ¸ğ‘¡ reflects the loss in step t, and the \nformula (2-4) is the accumulation of loss in the whole sequence \nT [13]. This feature will make the back propagation more \ncomplicated compared to a conventional neural network, \nbecause every time step has to be considered when calculating \nthe gradient descent. Therefore the Back Propagation Through \nTime (BPTT) [14] algorithm is used to upgrade the weights of \nthe RNN. \nCompared to a conventional neural network, an RNN has \nshared weights for the same hidden layer. In the unfolded RNN \ncell, each step shares the same weights (W, V, U) ; the only \ndifference is that they have different inputs. The shared weights \nfeature reduces the number of weights needed, thus giving a  \nsaving when training a neural network model. \nFor solving the gradient vanishing and explosion problem in \na vanilla RNN [15], researchers initiated the Long Short Term \nMemory (LSTM) network [16], which is also a kind of RNN. \nBut there are some differences in the basic unit. The basic unit \nof the LSTM is shown in Fig.4 . Compared to the â€œvanillaâ€ \nneural network, we can see that there are more cells in the \nrecurrent modules. These cells serve different functions. \nBasically, there are three kinds of gates in the LSTM cell. They \nare the input gate, the forget gate, and the output gate. \n\nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n4 \nÄ‰t ot\nft it\nCt-1 Ct\nU\nW\n \nFig.4. LSTM basic unit modified from [22]  \nAs their names indicate, the input gate decides which kinds \nof information goes through into the next cell, the forget gate \ncontrols the information which should be forgotten, and the \noutput gate finally outputs the previous information to the next \ncell. The formulas for these symbols are listed as follows: \nğ‘“ğ‘¡ = Ïƒ(ğ‘Šğ‘“â„ğ‘¡âˆ’1,+ğ‘ˆğ‘“ğ‘¥ğ‘¡)  + ğ‘ğ‘“                   (2 âˆ’ 5) \nğ‘–ğ‘¡ = Ïƒ(ğ‘Šğ‘–â„ğ‘¡âˆ’1,+ğ‘ˆğ‘–ğ‘¥ğ‘¡)  + ğ‘ğ‘–                   (2 âˆ’ 6) \nğ¶ğ‘¡Ìƒ = tanh (ğ‘Šğ‘”â„ğ‘¡âˆ’1, +ğ‘ˆğ‘”ğ‘¥ğ‘¡) + ğ‘ğ‘”               (2 âˆ’ 7) \nğ¶ğ‘¡ = ğ‘“ğ‘¡ âˆ— ğ¶ğ‘¡âˆ’1 +  ğ‘–ğ‘¡  âˆ— ğ¶ğ‘¡Ìƒ                         (2 âˆ’ 8) \nğ‘œğ‘¡ = Ïƒ(ğ‘Šğ‘œâ„ğ‘¡âˆ’1,+ğ‘ˆğ‘œğ‘¥ğ‘¡)  + ğ‘ğ‘œ                  (2 âˆ’ 9) \nâ„ğ‘¡ = ğ‘œğ‘¡ âˆ— tanh (ğ¶ğ‘¡)                        (2âˆ’ 10) \nThe reason that LSTM can avoid gradient vanishing is the \nway it calculates the inner hidden states. In a conventional RNN, \nthe inner hidden states can be represented as in formula 2-1. \nAccording to the chain rule, the gradient of the hidden state in \nthe back propagation sequence can be calculated by the product \nof the gradient in every time step [17]. In this case, the product \nof the gradient will might eventually come to zero due to  \ncontaining many gradients whose values are far less than 1.  \nHowever, things are different in an LSTM. As we can see \nfrom the above formulas, the main difference of the LSTM from \nthe general RNN is in the way of updating the hidden stateğ¶ğ‘¡. \nIn the data flow path from the previous hidden state ğ¶ğ‘¡âˆ’1 to the \nnext hidden stateğ¶ğ‘¡, there is a multiply unit and an addition unit \nwhile there is only a multiply unit in the conventional RNN cell. \nThe addition unit mak es the back propagation gradient larger \nthan 0 at all times. \nB. Language Model \nLanguage Model (LM) which is involved in ma chine \ntranslation, voice search, speech tagging, and speech \nrecognition, is a fundamental topic in the Natural Language \nProcessing (NLP) field. The role of the LM is to predict the \nprobability of a sentence.  LM can be represented as the \nprobability of a wo rd sequence [ 18]. Its mathematical \nexpression is listed as follows. \nğ‘ƒ(ğ‘¤1,â€¦ , ğ‘¤ğ‘‡) = âˆ ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤1,â€¦ , ğ‘¤ğ‘–âˆ’1)              (3 âˆ’ 1) \nğ‘‡\nğ‘–=1\n \nğ‘ƒ(ğ‘¤1,â€¦ , ğ‘¤ğ‘‡)  represents the probability of  the word \nsequence (ğ‘¤1,â€¦ , ğ‘¤ğ‘‡). ğ‘¤ğ‘– is the ith  word in the sequence.  This \nformula means that the probability of a sentence is equal to the \nmultiplication of the conditional probability of each word in the \nsequence, and the conditional probability of each word is \ndetermined by all its previous words. This probability model is \ncalled an n-gram probability mode l (the probability of ğ‘¤ğ‘–  is \ndetermined by its n-1 previous words). Specifically, the more \nprevious words that are involved, the more accurate results w e \ncould expect. However, it needs a large memory to store these \nprobability weights. For example, if the vocabulary size is \n10000, the parametersâ€™ volume will be 10000 n . Meanwhile, we \nneed more training date if we use a larger n. Practically, n is set \nto 2 or 3 to give a balance between model performance and \nparameter volume. \nNeural Network Language Model  (NNLM) [19] was first \ninitiated by Bengio et al at 2003. The NNLM use s a neural \nnetwork to calculate the probability of sentences, pointing the \nway to natural language processing. Its system architecture is \nshown as fol lows. As can be seen from the diagram, there are \nthree parts: feature vectors input, nonlinear transform, and \nsoftmax output, which is the basic structure of the neural \nnetwork. \n \nFig.5. NNLM system diagram modified from [19]  \nIV. IMPLEMENTATION \nA. Software libraries and versions \nOn the PC side, the main software packages used include \nVivado Design Suite  16.1, Anaconda, Python, Theano, and \nNLTK. We used the Vivado tool to design the Overlay and RTL \ncode running on the PYNQ. We use Conda 4.3.22 on an X86 \nCPU platform as the software management environment \nbecause anaconda software provides an exc  ellent scheme to \n\nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n5 \nmanage software libraries. The Python version is 3.4.5, which \nis the same version with that in PYNQ board. The Theano \nversion is 0.8.2, which is used to construct mathematic \noperations. The NLTK version is 3.0.5, which is used to process \nnatural language and build the interface between the corpus \ndataset and the neural network in Python programs. \nOn the PYNQ side, the image version is \npynq_z1_image_2016_09_14, which includes Python and other \nsoftware libraries. The original Python version in the system  \nwas 2.7 . We might need to upgrade it to version 3.4.5. The \nJupyter notebook version is 4.0.4. \nB. PYNQ board configuration \nIn this section, we will briefly introduce how to boot the \nPYNQ board and install dependent software packages through \nthe Internet. The pynq_z1_image_2016_09_14 image file \nincludes only Python2.7, Jupyter notebook, and a few other \nsoftware packages. We need ed to install or upgrade  the \nnecessary packages by ourselves.  \nWe can download the image file from http://www.pynq.io, \nand then decompress it into a micro sd card larger than 8G. \nFinally, insert the micro sd card into sd card slot in the PYNQ \nboard, connect power to the board, and turn on the power switch.  \nIn practice, the best way to access to internet for PYNQ is to \nuse the wireless usb wifi dongle, which is a tiny wifi adaptor \nwith an usb interface. In order to use the wifi connector, we \nneed to download the usb wifi driver file from the PYNQ github: \nhttps://github.com/Xilinx/PYNQ, the usb wifi driver file name \nis Usb_Wifi.py. Copy the driver file into \n\\\\pynq\\xilinx\\pynq\\drivers on PYNQ board. The base.bit file, \nwhich contains PS and PL design, needed to be downloaded \nthrough Overlay interface, and then combined with the driver, \nthe usb wifi connector will work.  \nC. DRNN Language Model Design and Deployment \nIn this section, we will concretely describe the DRNN \nLanguage Model design, training, and deployment methods and \nprocedures. Because the training of the neural network is a time \nand computing resource consuming work, we use a CPU to train \nthe network, presenting and deploying the model on the PYNQ \nboard. \nThe DRNN LM program includes 3 principal parts: \npreprocessing the dataset, building the DRNN model, and \ntraining the model. The preprocessing dataset module generates \ntraining data from a corpus, converting the words into vectors \nwhich can be calculated by the computer. The DRNN model \nmodule defines the DRNN topology and algorithms. The \ntraining module combine s the above modules together to \nperform the training process and generates prediction r esults. \nThe DRNN LM program flow chart is listed in Fig.6. \nBefore go ing in to concrete module design, we will first \npresent the DRNN LM system architecture. As mentioned at \nsection 3.1, an RNN is suitable for processing of sequence data. \nWe illustrate the data processing in Fig.7. \n \nFig.6. DRNN LM Program Flow Chart  \nThe DRNN LM receives a word sequence input and \ngenerates a prediction sequence with the same length. There \nmay be a gap between the prediction sequence and the label \nsequence as the red marked words in Fig.7, which will result in \nan increase in the value of th e loss function. Then the DRNN \nweights will be adjusted according to the value of the loss. That \nis the meaning of the training of the neural network, reducing \nthe loss values and making the prediction close to the label. In \nthe training process, we donâ€™t need any pre-generated label files. \nWe just use the dataset in text form and generates the label \nsentences by the data pre-processing module. \ny1\ns1\nx1\ns0\ny2\ns2\nx2\ny3\ns3\nx3\ny4\ns4\nx4\ny5\ns5\nx5\ny6\ns6\nx6\ny7\ns7\nx7\nISOS have an appointment with my supervisor\ny8\ns8\nx8\nSoS: Start of the Sentence\nEoS: End of the Sentence\nI have an appointment with my supervisor EOSlabel:\nI have an apartment with my supervisor EOSprediction:\n \nFig.7. RNN LM system diagram  \nThe first step  is to translate the words in human natural \nlanguage into vectors which can be calculated by the computer. \nNeural network algorithms  use arithmetic operati ons such as \nmultiplication or addition. So to be suitable for neural network \nprocessing, t he computer or pr ocessor ha s to translate the \nsymbols into binary or other digital numbers. In this stage the \nNLTK [20] software package is used to process the natural \nlanguage dataset, converting the sentences into separated words. \nAfter the separated words  have been obta ined, they must be  \nCorpus data set\ndata_preprocess\nLSTM Model\nFunction\nSGD Algorithm\nepoch <=Nepoch\nsave model\nepoch > Nepoch\npredict results\nLoss\nFunction\ntraining\nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n6 \nconverted to digital numbers. The Python language has a list \nvariable that can store a word and its corresponding digital \nnumber. The words in the dataset are sorted according to  \nfrequency of occurrence.  Finally, based on the vocabulary \ndataset, we can get a vocabulary list where each word has a \nunique one-hot vector coding. \nThe next step is to build a LSTM Neural Network. Basically, \nthere are two stages in designing a neural network [23]: forward \nprocess and backward process. In the forward process, we need \nto define the neural network topology and neural cells. We use \n3 LSTM hidden layers neural network to perform our task. The \ntopology of 3 LSTM hidden layers neural network is shown as \nfollows. \nxt\n LSTM\n LSTM\n LSTM\nSt-1\n St-1\n St-1\nSt+1\n St+1\n St+1\n1 2 3\nInput\nlayer\nOutput\nlayer\n \nFig.8. 3 LSTM hidden layers neural network topology  \nThe forward process will transform the data from input to \noutput according to the formulas (2-5)â€”(2-10). The following \nexpressions in the below figure show the relation between the \nPython code and the corresponding formulas. \n f_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) + W[1].dot(s_t1_prev) + b[1])\ni_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) + W[0].dot(s_t1_prev) + b[0])\n  Ä‰ _t1 = T.tanh(U[3].dot(x_e) + W[3].dot(s_t1_prev) + b[3])\n  c_t1 = f_t1 * c_t1_prev + Ä‰_t1 * i_t1\n             o_t1 =T.nnet.hard_sigmoid(U[2].dot(x_e) + W[2].dot(s_t1_prev) + b[2])          \n          h_t1 = T.tanh(c_t1) *o_t1\n \nFig.9. LSTM forward process  \nBecause the hard_sigmoid function is faster than a standard \nsigmoid function, we use hard_sigmoid function to perform the \nsigmoid operation. The difference of hard_sigmoid and sigmoid \nfunction can be seen from the following figure. \n \nFig.10. the difference of hard_sigmoid and sigmoid function  \nThe final stage of the forward propagation is the output layer. \nWe use the softmax function in the output layer, which is a kind \nof normalization function that can convert a vector into a \nprobability distribution form, giving every element a \nprobability. This method is widely used in classification tasks \nin neural network s. In Theano, we can call softmax function \ndirectly. The definition of softmax is described as (5-1) shown \nand expression (5-2) shows the Theano implementation of the \nsoftmax function with Python. \nğœ(ğ‘§)ğ‘— = ğ‘’ğ‘§ğ‘—\nâˆ‘ ğ‘’ğ‘§ğ‘˜ğ¾\nğ‘˜=1\n        ğ‘— ğ‘–ğ‘› 1,â€¦ , ğ¾                      (4 âˆ’ 1) \nğ‘‡. ğ‘›ğ‘›ğ‘’ğ‘¡.ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘‰. ğ‘‘ğ‘œğ‘¡(ğ‘ ))                        (4 âˆ’ 2) \nNow we need a loss function to calculate the gap between the \nprediction and the actual values, which will be used to get the \ngradient of every weight in the back propagation. Because the \ncross entropy loss function is faster in training than the mean \nsquare error function, we use a cross entropy loss function \nwhose definition is shown as (5-3), and expression (5-4) show s \nthe Theano implementation of cross entropy with Python.  \nğ»(ğ‘. ğ‘) = âˆ’ âˆ‘ ğ‘(ğ‘¥) log(ğ‘(ğ‘¥))                      (4 âˆ’ 3)\nğ‘›\nğ‘–=1\n \nğ‘‡. ğ‘›ğ‘›ğ‘’ğ‘¡.ğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘ğ‘ğ‘™_ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘œ, ğ‘¦)          (4 âˆ’ 4) \nThe backward process of theh neural network is to calculate \nthe gradients of the neural network, using the Back Propagation \nThrough Time (BPTT) algorithm [24 ]. In practice, Theano \nsupports auto -differentiation of formulas. We can get the \ngradients through the grad function in Theano.  \nThe neural network needed to be trained for hundreds of \nepochs to make the prediction values close to the actual values. \nOne epoch includes one forward propagation and one backward \npropagation. We use the Stochastic Gradient Descent (SGD) \nalgorithm to update the weights of the neural network through \ncalculating the partial derivatives of the loss function  with \nrespect to weights and biases. \nIn terms of evaluating the language model, we need to check \nthe loss values and the perplexity of the language model.  The \ntwo indexes are simple and fast to calculate and can make the \nperformance of the model readily comprehensible  [25]. We \nhave introduced the cross -entropy loss function above. In this \nsection, we will mainly introduce perplexity.  Perplexity is a \nkind of intrinsic evaluation method for a language model [26]. \nIt can be obtained through taking the reciprocal of the \nprobability of a test sentence and then computing the Nth root \n[27]. If taking the logarithm, in practice, the perplexity can be \nexpressed as cross-entropy to the power of e. More specifically, \nthe perplexity means in a specific context, the number of words \nthat the langu age model might choose as the next word. \nTherefore, the smaller perplexity, the higher probability of a \nsentence, and the better model. In Python, it can be calculated \nas the following expression: \nğ‘›ğ‘. exp (ğ‘›ğ‘. ğ‘šğ‘’ğ‘ğ‘›(ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğ‘™ğ‘œğ‘ ğ‘ /ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ ))            (4 âˆ’ 5) \nIn Theano, the trained neural network model is saved in  a \n*.npz file, which saves several arrays into a single file in \nuncompressed format. The final size of the model is around \n17MB, which can be easily deployed into the PYNQ board. \nWhen we get the model of the LSTM LM neural network, we \ncan use it to predict results on the PYNQ. The model file needed \n\nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n7 \nto be copied to the PYNQ board file system, and loaded with \nthe following instructions in Jupyter notebook. Then we can use \nthe model on PYNQ board. \n \nFig.11. model load instructions in Jupyter notebook  \nD. DRNN Accelerator Design \nAn overlay is built in programmable logic on the ZYNQ \nsystem [28]. It is a kind of hardware library which has a Python \ninterface, making it convenient for the software programmer to \nuse overlay s as Python packages. Therefore, the overlay \nincludes a PL part and a Python interface part. We can design \nthe PL part and generate the corresponding tcl file with the \nVivado Design Suite tool. The tcl file for the PL design can be \nautomatically identified by the PYNQ system and used as an \ninterface connecting to Python methods [28]. In order to use the \noverlay, we need to copy the bit and tcl file s, which are \ngenerated by Vivado Design Suite  with the command: \nğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’_ğ‘ğ‘‘_ğ‘¡ğ‘ğ‘™, to the Python file system. \nVivado Design Suite 16.1 is used to create a new overlay \nthrough building a new block design and adding the new IP \nmodules. The PYNQ project in GitHub [29] has provided a base \ndesign which includes most of the peripheral interface overlay. \nTherefore, we donâ€™t need to build everything from scratch. We \nwill build the accelerator overlay based on this project. In order \nto reuse the base PYNQ project, we create a Vivado project \nin .\\vivado\\base, and import the tcl file and bitstream file. The \nTcl Console windows is in the bottom of th e Vivado Design \nSuite tool, and the command is â€œsource  â€¦/***.tclâ€. \nAfter importing the base PYNQ block design, we start to add \nthe accelerator overlay into it. The accelerator overlay needs an \nAXI Stream interface to communicate with the PS, because we \ntransfer data as batches, and the AXI Stream performs data \nbatch transferring very well. In Vivado Design Suite, an IP with \nAXI Stream interface can be created through going to \nTools/Create and Package IP option. Finally, we generate an \nAXI Stream interface IP, add it into the Vivado IP repository \nautomatically, and connect it to the PS part in the block design. \nThe block diagram of the accelerator overlay data path in block \ndesign is shown in Fig.12. \nBecause AXI Stream cannot directly map its data into \nmemory address on the PS si de, the AXI DMA modules are \nused to map the data from FIFO to the memory address, and \nvice versa. The acceleration procedure is: the accelerator IP \nreceives a batch of data from the PS, finishes the acceleration \noperations, and sends the results to the PS. \nThe DRNN accelerator system is implemented with Verilog \nHardware Language on the PL side of the ZYNQ SOC. In order \nto improve the parallel computation level, the accelerator core \nhas 5 parallel Processing Elements (PE). The accelerator system \narchitecture is illustrated as follows fig.13. \n \nARM\nPS\nAXI\nDMA\nDRNN \nAccelerator \nExterior\nDDR\nMemory\nZYNQ SOC\nUSB\nEthernet\nAXI Stream\nCentral \nInterconnect\nAXI\nDMA\nJupyter Notebook\nPL\nPS\n \nFig.12. accelerator overlay data path diagram  \nThe accelerator perform s matrix multiplication operations \nwhich are the most frequent operation in forward propagation \nof the neural network. We design a multiplication and addition \naccelerator which accelerates ğ‘Šğ‘“â„ğ‘¡âˆ’1,+ğ‘ˆğ‘“ğ‘¥ğ‘¡  operation in a \nlanguage model, where ğ‘Šğ‘“  âˆˆ ğ‘…50ğ‘‹50 , â„ğ‘¡âˆ’1  âˆˆ ğ‘…50ğ‘‹1 , ğ‘ˆğ‘“ âˆˆ\nğ‘…4000ğ‘‹50, ğ‘¥ğ‘¡ âˆˆ ğ‘…4000ğ‘‹1 (Vocabulary size = 4000, hidden layer \ncell = 50). There are five Process Elements (PE) implemented \nin one accelerator core unit. Each PE receives 50 data in one \nbatch and performs 50*10 times multiplication operations and \n50*10 times addition operations. \nPE\n PE\nAXI Stream\nPL\nX\n X\n X\n X\n X\n X\n X\n X\n X\n X\nW1Wi Ïµ R30X1\nW2\nW10\nUXiÏµ R30X1\n+\n +\n +\n +\n +\n +\n +\n +\n +\n +\nSLAVE_AXIS\nMASTER_AXIS\nX X X X X XA0A1A9\nPE\nDRNN Accelerator \nAXI Stream\nPE\n PE\n PE\n \nFig.13. multiplication and addition accelerator basic unit and hardware \nimplement \nThe whole accelerator will perform 50*10*5 times \nmultiplication operations and 50*10*5 times addition \noperations in one batch. The latency of the accelerator is around \n50*5ns. That means the number  of operations that the \naccelerator executes per second is around 20GOPS (measured \nin Operations per Second). Because all hidden layer hold the \nsame weights, the accelerator overlay can be reused by all of \nthem.  \nV. EXPERIMENT \nWe use Jupyter notebook and Vivado Design Suite to build a \njoint simulation system. Using overlay, a Python program can \n\nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n8 \ncontrol the internal DMA IP address to launch data transmission, \nand external data sends/receives to/from the accelerator through \nPython API and AXI interface. The joint simulat ion system \ndiagram is shown as follow. \nJupyter\nnotebook\nVivado\nHLS\n16.1\nHardware\nmanager\nPS\nZYNQ SOC\nPL\nJTAGPython API\n \nFig.14. joint simulation system framework  \nWe send consecutive numbers from 1 to 50 into the hardware \naccelerator, and get the results back to the program.  \nThe hardware calculation results by hardware are:  \nsum_hardware=[1275,2550,3825,5100,6375,7650,8925,10200,11475,12750,\n14025,15300,16575,17850,19125,20400,21675,22950,24225,\n25500,26775,28050,29325,30600,31875,33150,34425,35700,\n36975,38250,39525,40800,42075,43350,44625,45900,47175,\n48450,49725,51000,52275,53550,54825,56100,57375,58650,\n59925,61200,62475,63750]\n \nFig.15. hardware calculation results  \nIn order to verify the results of hardware implemention, we \ncalculate the multiplication and addition by using a computer \nfirst. The results by computer are as follows. \nfor j in range(1,51):\nsum =0\n                  for i in range(1,51):\n                          sum = i*j + sum\nsum =[\n1275,2550,3825,5100,6375,7650,8925,10200,11475,12750,14025,15300,16575,17850,\n19125,20400,21675,22950,24225,25500,26775,28050,29325,30600,31875,33150,34425\n35700,36975,38250,39525,40800,42075,43350,44625,45900,47175.48450.49725.51000\n52275,53550,54825,56100,57375,58650,59925,61200,62475,63750]\nOutput:\n \nFig.16. the multiplication and addition by using computer  \nWe use matplot library to display the above accumulation \nresults by hardware and software. It can be seen from the Fig.21, \nthe results by hardware match those by software very closely. \n \nFig.17. the accumulation results by hardware and software  \nVI. RESULTS \nA. Cross-Entropy Loss Function Results \nWe obtained the loss values of 245 epochs in the training \nprocessing and plot them in Ju pyter notebook with the \nmatplotlib package. As can be seen from the figure below, the \nloss values are gradually reducing with the increase of training \nepochs, which means the training algorithms works well and the \nprediction of the model will be close to t he label when the \ntraining process is finished. \n \nFig.18. the loss values of 245 epochs  \nB. Language Model Perplexity \nWe obtain the perplexity in every 100 training steps. The \nresult is shown in Fig. 23. The perplexity decreases from around \n3500 to about 100 after 1000 training steps, which means the \nlanguage model is reducing the scope of vocabulary pool when \nchoosing the next words to predict. That means the .prediction \nspeed and accura cy rate of the language model is increasing \nwith the progress of training. \n \nFig.19. perplexity of language model  \n\nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n9 \nC. DRNN Accelerator Performance \nThe FPGA r esource utilization is listed as follow. We use \nspeed optimization option when generating the multiplier IP \ncore, therefore each multiplier IP core consumes 4 DSPs. We \nhave 50 multipliers in our accelerator module. BRAM resource \nis used to restore the weights. LUT resource is consumed to \nregister and routing. The overall resource utilization is very \nwell. \nTable 1: FPGA resource utilization  \nResource  Utilization Available Utilization \nLUT 39851 53200 74.91% \nLUTRAM 3245 17400 18.65% \nFF 54488 106400 51.21% \nBRAM 75.50 140 53.93% \nDSP 206 220 93.64% \nIn order to evaluate the accelerator performance, we compare \nour implementation to previous related works. The comparison \ndimensions includes FPGA type, running frequency, precision, \nDSP utilization, neural network type, and acceleration \nperformance. The work in Ref. [30] implements a whole LSTM \ncell on FPGA and the acceleration performance is 0.2837GOPS. \nRef. [31] implements the NNLM on a larger FPGA which has \n2800 DSP resources overall. Compared to the two related works, \nour implementation has 70.5X and 2.75X speed up respectively. \nTable 2: Accelerator performance comparison to  related work \n Ref.[30] Ref.[31] Our \nimplement \nFPGA type  Zynq 7020 Virtex7-485t Zynq 7020 \nFrequency/M\nHz \n142 150 200 \nPrecision  Fixed \npoint(Q8.8) \nFloat-32 Fixed point  \nDSP \nutilization \n50 1176 206 \nNeural \nnetwork \nRNN-LSTM RNN-LSTM RNN-LSTM \nPerformance 0.2837GOPS(1\nX) \n7.26GFLOPS(25.6\nX) \n20 \nGOPS(70.5\nX) \nVII. CONCLUSION AND FUTURE WORK  \nThe paper implements a DRNN language model with Python \nand deploy s the trained model on PYNQ through Jupyter \nnotebook, and designs an DRNN hardware accelerator using an \noverlay. We use a high level programming languageâ€”Python - \nto design an FPGA SOC system, which greatly accelerates the \ndevelopment process and extends the range of FPGA \napplication. We build a hardware accelerator with AXI Stream \ninterface, which can interact with a software program through \noverlay form. The  acceleration performance has a great \nimprovement compared to previous related works.  \nMore importantly, we showed that a software and hardware \njoint design and simulation process can be useful in the neural \nnetwork field. The model trained through CPU or GPU can be \ndeployed into the FPGA SOC system to explore the application \non a mobile device. Meanwhile, the programmable logic on the \nFPGA can be used as a hardware accelerator to improve the \nperformance of the model.  The project is uploaded into \nhttps://github.com/hillhao/PYNQ-project . \n \nThere are several points that can be optimized or studied further. \nThe word-to-index part of the language model can be optimized \nto low density vector, which can greatly reduce the magnitude \nof dimensions of word vectors. The training methods, which \ncan shorten the training length, accelerate convergence, and \navoid over-fit, need to be studied further. In the hardware side, \nimplementing all the neural network parts on the FPGA SOC \nplatform would be beneficial because of its low power \nconsumption and powerful parallel processing features. \nREFERENCES \n[1]     G. E. Hinton, S. Osindero, Yee-Whye. The A  fast learning algorithm for \ndeep belief nets. Neural Computation, 2006  \n[2]      Dong Yu, Li Deng, et al. Analysis of Deep Learning: a practice on speech \nrecognition. 1st ed., Beijing: Publishing house of electronics industry, 2016, pp. \n246-247. \n[3]    S. Abrahams. â€œTensorflow on raspberry piâ€. [Online] Available:  https: // \ngith ub.com/ samjabra hams/tensorflow -on-raspberry -pi \n[4]  M. Bubashkin. â€œTensorFlow Image Recognition on a Raspberry Piâ€. \n[Online] Available: https://svds.com/tensorflow -image-recognition -raspberry -\npi/ \n[5]    Y. Umuroglu, N. J.Fraser, G. Gambardella. FINN: A Framework for Fast \nScalable Binarized Neural Network Interface. 2 5th International Symposium on \nField Programmable Gate Arrays, 2017  \n[6]    E. Nurvitadhi, el at.,Accelerating Binarized Neural Networks: Comparison \nof FPGA, CPU, GPU, and ASIC. International Conference on Field -\nProgrammable Technology  (FPT), 2016 \n[7]   â€œZynq-7000-Overviewâ€. [Online]  Available: https://  www.xilinx. com/s- \nupport/documentation/data_sheets/ds190 -Zynq-7000-Overview.pdf \n[8] â€œArtificial neural networkâ€. [Online] Available: https://en.wikipedi \na.org/wiki/Artificial_ne ural_network  \n[9]  David Kriesel. â€œA Brief Introduction to Neural Network â€. [Online] \nAvailable: http://www.dkriesel.com/_media/science/neuronalenetze -en-zeta2 -\n2col-dkrieselcom.pdf, 2009 \n[10]   P. Safari. Deep Learning For Sequential Pattern Recognition. 2013 \n[11]   I. Sutskever, O. Vinyals, Q.V. Le. Sequence to Sequence Learning with \nNeural Networks. arXiv:1409.3215, 2014  \n[12]    M.Nielson. â€œImproving the way neural network learnâ€. [Online] Availab  \nle: http://neuralnetworksanddeeplearning.com/chap3.html  \n[13]   Rumelhart D E, Hinton G E, Williams R J. Learning representations by \nback-propagating errors. Cognitive modeling, 1988  \n[14]   I.  Sutskever, â€œTraining Recurrent Neural Networks â€. [Online] Avail abl \ne:http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf , 2013 \n[15]    Bengio, Y., Simard, P., and Frasconi, P. Learning long -term dependenci  \n-es with gradient descent is difficult. Neural Networks. IEEE Transactions on, \n5(2):157â€“166, 1994  \n[16]   S. Hochreiter, J. Schmidhuber. Long Short -Term Memory. Neural Comp  \n-utation, 9(8):1735-1780, 1997  \n[17]   Werbos, P.J. Backpropagation through time: what it does and how to do \nit. Proc. IEEE, 78(10): 1550-1560, 1990  \n[18]   Tomas Mikolov, Kai Chen. Ef ficient estimation of word representation \nin vector space. arXiv:1301.3781, 2013  \n [19]    Bengio Y, Schwenk H, SenÃ© cal J S, et al. A neural probabilistic language \nmodel. Journal of Machine Learning Research, 2003, 3(6):1137 -1155. \n[20]  â€œNatural Language To olkitâ€. [Online] Available: http://www.nltk.org/  \n[21]  â€œPYNQâ€. [Online] Available: http://www.pynq.io/home.html  \n[22]   C. Olah. â€œUnderstanding LSTMâ€. [Online] Available: http://colah.githu \nb.io/posts/2015-08-Understanding -LSTMs/ \n[23]   Zeyu Zheng, Siyu Gu. TensorFlow: a practice on Google deep learning \nframework. Beijing, Publishing House of Electronics Industry. 2017  \n[24]   â€œBackPropagation Through Timeâ€. [Online] Available: http://www.cnb \nlogs.com/wacc/p/5 341670.html \n[25]    â€œIntrinsic Evaluation â€. [Online] A vailable: https://cs224d.stanford.edu/l  \necture_notes/notes2.pdf   \n[26]    N.A. Smith, Adversarial Evaluation for Models of Natural Language. \narXiv:1207.0245, 2012 \nThe implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA \n \n \n \n10 \n[27]    D.Gibbon, R. Moore, R.winski, Handbook of Standards and Resources \nfor Spoken Language Systems. Paperback ed. New York: Mouton de Gruy ter, \n1998, pp.99-102 \n[28] â€œOverlay Designâ€. [Online] Available: http://pynq.readthedocs.i o \n/en/latest/overlay_design_ methodology/overlay_design.html  \n[29]  â€œPYNQ open -source projectâ€. [Online] Available: https://github.com / \nXilinx/PYNQ \n[30]    Andre Xian Ming Chang, Berin Martini, et al. Recurrent Neural Network \nHardware Implementation on FPGA. in arXiv preprint arXiv:1511.05552, 2015  \n[31]   Yijin Guan, Zhihang Yuan, et al. FPGA -based Accelerator for Long \nShort-Term Memory Recurrent Neural Networks. Design Automation \nCoference (ASP-DAC), 2017. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n "
}