{
    "title": "Retrieval-Augmented Generation and LLM Agents for Biomimicry Design Solutions",
    "url": "https://openalex.org/W4398160786",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5094134748",
            "name": "Christopher Toukmaji",
            "affiliations": [
                "University of California, Irvine"
            ]
        },
        {
            "id": "https://openalex.org/A5026132342",
            "name": "Allison Tee",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5094134748",
            "name": "Christopher Toukmaji",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5026132342",
            "name": "Allison Tee",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4327810158",
        "https://openalex.org/W3036622557",
        "https://openalex.org/W4353113046",
        "https://openalex.org/W3186138538",
        "https://openalex.org/W1928559212",
        "https://openalex.org/W4313590997",
        "https://openalex.org/W4385570777",
        "https://openalex.org/W4318146754",
        "https://openalex.org/W3128859113",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W3155807546",
        "https://openalex.org/W2161237822",
        "https://openalex.org/W4309591663",
        "https://openalex.org/W2968609997",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4388626886"
    ],
    "abstract": "We present BIDARA, a Bio-Inspired Design And Research Assistant, to address the complexity of biomimicry -- the practice of designing modern-day engineering solutions inspired by biological phenomena. Large Language Models (LLMs) have been shown to act as sufficient general-purpose task solvers, but they often hallucinate and fail in regimes that require domain-specific and up-to-date knowledge. We integrate Retrieval-Augmented Generation (RAG) and Reasoning-and-Action agents to aid LLMs in avoiding hallucination and utilizing updated knowledge during generation of biomimetic design solutions. We find that incorporating RAG increases the feasibility of the design solutions in both prompting and agent settings, and we use these findings to guide our ongoing work. To the extent of our knowledge, this is the first work that integrates and evaluates Retrieval-Augmented Generation within LLM-generated biomimetic design solutions.",
    "full_text": "Retrieval-Augmented Generation and LLM Agents for Biomimicry Design\nSolutions\nChristopher Toukmaji1*†, Allison Tee2*†\n1University of California, Irvine\n2Stanford University\nctoukmaj@uci.edu, ateecup@stanford.edu\nAbstract\nWe present BIDARA, a Bio-Inspired Design And Research\nAssistant, to address the complexity of biomimicry – the\npractice of designing modern-day engineering solutions in-\nspired by biological phenomena. Large Language Mod-\nels (LLMs) have been shown to act as sufficient general-\npurpose task solvers, but they often hallucinate and fail in\nregimes that require domain-specific and up-to-date knowl-\nedge. We integrate Retrieval-Augmented Generation (RAG)\nand Reasoning-and-Action agents to aid LLMs in avoiding\nhallucination and utilizing updated knowledge during gener-\nation of biomimetic design solutions. We find that incorpo-\nrating RAG increases the feasibility of the design solutions in\nboth prompting and agent settings, and we use these findings\nto guide our ongoing work. To the extent of our knowledge,\nthis is the first work that integrates and evaluates Retrieval-\nAugmented Generation within LLM-generated biomimetic\ndesign solutions.\nIntroduction\nBiomimicry practitioners face design barriers such as hard-\nships trying to find plausible biological strategies for en-\ngineering problems, and the extensive time used identify-\ning design solutions from these biological strategies (Nagel\n2014; Chen et al. 2021b). As a result, we turn to LLMs to\nact as research assistants to streamline the literature review\nprocess and biological abstraction cycle.\nInstruction-tuned LLMs have been demonstrating an in-\ncrease in adoption as assistants. One such use case of LLMs\nis for researchers to formulate new ideas or automate the\nsearch of related work and relevant papers. In practice, how-\never, there are several shortcomings when using LLMs as\nresearch assistants. Namely, general-purpose LLMs are not\ntypically updated frequently, so the research works pro-\nvided may be outdated. Moreover, LLMs may hallucinate,\ncausing them to cite and reference non-existent work. To\ncombat both of these issues present when using LLMs,\nwe integrate Retrieval-Augmented Generation (RAG) with\na general-purpose LLM. RAG is the practice of retrieving\n*Work done during an internship at NASA Glenn Research\nCenter.\n†These authors contributed equally.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ndata outside an LLM, then prompting the LLM with the re-\ntrieved data as context under the pretext that the additional\ncontext will guide the model into a more accurate response.\nAnother solution to mitigate hallucination issues in LLMs\nis LLM agents. LLM agents wrap around LLMs and are\nprompted with a user query and instructions to select an ac-\ntion from a list of tools until the query has been answered.\nWe experiment with both RAG and LLM agents; the class of\nLLM agents we use are Reasoning-and-Action agents (Re-\nAct) (Yao et al. 2023), where a reasoning step and an action\nstep are intertwined.\nOur contributions are two-fold:\n1. We evaluate the quality of LLM-generated biomimetic\ndesign solutions in settings with and without Retrieval-\nAugmented Generation, and in settings with and without\nReAct Agents, to determine which setting to implement\nin BIDARA.\n2. We present our directions for ongoing work further ex-\npanding the RAG tool suite. This is sparked by both the\nresults of the quality of biomimetic design solutions, as\nwell as feedback from domain experts.\nBackground and Related Work\nBiomimicry With Natural Language Processing\nThe most similar work to ours is that of Zhu, Zhang, and Luo\n(2022). The work fine-tunes a base GPT-3 Davinci model\nwith a dataset of 221 biomimicry examples gathered from\nthe AskNature website, then the fine-tuned model is used\nto generate design concepts. We complement the limitations\nof the work by using a large academic knowledge base for\nretrieval, and we use a more robust and capable LLM.\nIn the broader NLP space, there is more work on knowl-\nedge extraction for biomimetic analogy linking. Shu (2010)\nutilizes keyword frequency and knowledge extraction from\na knowledge base to identify biomimetic analogies to design\nquestions. Chen et al. (2021a) presents an algorithm that can\nbe applied to generate keywords to search for relevant bio-\nlogical information. Chen et al. (2021b) presents a knowl-\nedge extraction method to identify biomimetic analogies.\nRetrieval-Augmented Generation and Agents\nRAG has largely been a successful strategy to minimize hal-\nlucination with LLMs (Shuster et al. 2021; Mallen et al.\nAAAI Spring Symposium Series (SSS-24)\n273\nFigure 1: System Diagram of Few-Shot Prompting LLM + RAG.\n2023; Komeili, Shuster, and Weston 2022). LLM agents,\nsuch as the ReAct Agents, wrap around pre-trained LLMs\nthat do not use the internet. LLM agents have been applied\nin tasks where LLMs tend to hallucinate, such as arithmetic,\nsource citation, or other tasks that would benefit from the\nuse of the Internet or an external tool. Some use-cases of\nagents are for program assistance (Gao et al. 2023), multi-\nmodal retrieval (Yang et al. 2023), and general-purpose API\nuse (Liang et al. 2023).\nExperimental Setup\nWe develop a test set of ten biomimicry design questions.\nEach question is evaluated in four experimental settings\nwhich are outlined below. The final solution is evaluated by\nhuman annotators in terms of feasibility and novelty. A sys-\ntem diagram of one of the experiment settings is given in\nFigure 1.\nExperiments\nWe experiment in the following settings:\n1. Zero-Shot Prompting LLM: The LLM is prompted\nwith a user query appended to a prompt with the task\ndescription as a baseline.\n2. Few-Shot Prompting LLM + RAG: The LLM is\nprompted twice in few-shot settings. The first prompt\ngenerates a search term which is queried to an academic\npaper database. The top-k papers are retrieved from the\ndatabase, and the abstracts of the papers are appended\nto the prompt. The second pass conditions on the same\ncontext as the first pass, in addition to the retrieved paper\nabstracts, to generate the rest of the answer.\n3. ReAct Agent:The LLM is given a list of descriptions of\ntools to use and decides which tools to use, following the\nzero-shot ReAcT framework (Yao et al. 2023).\n4. ReAct Agent + RAG:The LLM is given the same list of\ntools as the ReAct Agent, but RAG is added as a tool.\nModel\nWe use GPT-4 (OpenAI 2023b) with zero temperature as the\nbase LLM for all experiments as it has been trained on the\nlargest amount of data, so the model has stored a larger view\nof world understanding which would help with design solu-\ntions. Moreover, GPT-4 has a large number of parameters,\nso it is a robust model capable of zero-shot and few-shot\nlearning (Brown et al. 2020).\nRetrieval Augmentation Module\nWe use the Semantic Scholar Graph API (Kinney et al. 2023)\nwith keyword search as the RAG module for retrieving pa-\npers. Our motivation behind the use of the Semantic Scholar\nGraph API is that the API is updated with new papers fre-\nquently, and it enables access to the largest academic knowl-\nedge base at the time of writing - with over 200 million pa-\npers available.\nMetrics\nDesign solutions are evaluated by human evaluators on both\nfeasibility and novelty on a scale of 0 to 5. We use a similar\n274\na) Histogram of Feasibility Scores\n b) Histogram of Novelty Scores\nFigure 2: Experts’ Annotations of generated design solutions for the design questions in the test set.\nrubric as Zhu, Zhang, and Luo (2022) which spans from 1 to\n5. A feasibility score of 1 denotes that the generated design\nsolution makes no sense and is infeasible, while a feasibility\nscore of 5 denotes that the generated design solution makes\nperfect sense and is completely feasible. Similarly, a novelty\nscore of 1 denotes that the generated design solution already\nexists and is a common solution, while a novelty score of 5\ndenotes that the generated design solution is entirely novel.\nWe introduce a score of 0 in both feasibility and novelty\nmetrics, which is reserved for design solutions that do not\nanswer the design question. Our motivation behind the in-\ntroduction of this score is that an unrelated solution should\nbe scored lower than a related, but low-quality, solution.\nHuman Evaluation\nEach annotator is presented with a biomimicry design ques-\ntion and four outputs, one for each of the prompting meth-\nods. We report the average score and standard deviation\nacross the test dataset which contains ten biomimetic design\nquestions. For scalable evaluation, we present the generated\ndesign answer and the generated explanation of the answer\nto the annotator, leaving out the chained intermediate steps.\nTo avoid any bias towards preferring a design solution that\nrefers to a paper, we omit all intermediate steps except for\nthe final answer, and we manually remove any references to\ncited papers within the answer. Due to resource constraints,\nwe have two biomimicry practitioners as human annotators.\nMethods\nWe further describe the four settings used in experiments.\nZero-Shot Prompting LLM\nThe LLM is prompted to generate a design solution in zero-\nshot with a natural language description of the first three\nsteps of the Biomimicry Design Toolbox (Biomimicry In-\nstitute 2023): (1) Biologize, (2) Discover, (3) Abstract.\nFew-Shot Prompting LLM + RAG\nThe LLM is prompted to generate a design solution in a few-\nshot ReAct prompt with three examples of the following rea-\nsoning sequence: generating a search query, retrieving top-k\nabstracts, and using the first three steps of the Biomimicry\nDesign Toolbox - (1) Biologize, (2) Discover, (3) Abstract.\nThere are two LLM inference steps. The first pass prompts\nthe LLM to generate the Biologize step and a search query\nto pass to the RAG module. The RAG module searches the\nSemantic Scholar API and retrieves the top-k papers. The\ntitles, authors, URLs, and abstracts of each of the top-k pa-\npers are appended to the few-shot prompt and the output\nof the Biologize step. The LLM is then prompted again to\ncomplete the Discover and Abstract step. The prompt does\nnot include any additional instructions. We find that setting\nk = 2 is the maximum value of k that consistently fits the\npaper abstracts within the context window.\nReAct Agent\nWe utilize a zero-shot ReAct framework to develop a\nReasoning-and-Action Agent. The agent is given a natural\nlanguage description of the steps from the Biomimicry De-\nsign Toolbox - (1) Biologize, (2) Discover, (3) Abstract, (4)\nEmulate, (5) Evaluate - as agent tools. When one of the tools\nis selected, a natural language description of the toolbox step\nis passed to the LLM. To prevent the agent chain from ex-\nceeding the context window, we pass the initial zero-shot\nprompt and the most recent reasoning step at every time\nstep to the agent. The tools in this experiment are natural-\nlanguage prompts and do not make use of the Internet.\nReAct Agent + RAG\nWe utilize an identical setup as the ReAct Agent setting, but\nRAG is added as a tool. When the RAG tool is selected,\nthe RAG module searches the Semantic Scholar API with a\n275\nQuestion: How can we design a vehicle to travel from Sydney\nto USA in one hour?\nBiologize: The essential function we need to address is rapid long-\ndistance travel. In biological terms, we can ask, ”How does nature\ntravel long distances quickly?” Specifically, we can look at the\nPeregrine Falcon, the fastest bird in the world, which uses grav-\nity to its advantage during its hunting stoop (high-speed dive). So,\nour biologized question becomes, ”How does the Peregrine Falcon\nachieve high-speed travel?”\nAction: Search[Peregrine Falcon high-speed travel]\nShape Optimization of High-Speed Rail by Biomimetic by C. T.\nFoo, B. Omar, I. Taib.\nAbstract: The Southern Corridor High-Speed Rail (HSR) has the\npotential to connect Kuala Lumpur and Singapore with only 90\nminutes of travel time. The HSR needs to travel at minimum drag\nin order to promote ride comfort and environmental sustainability.\nEight combinations of train head bio-models are generated from a\nmorphological chart of three key concepts formed by biomimetic\nmethod, referring to the peregrine falcon, belted kingfisher, sail-\nfish and dolpine. The three key concepts are the type of train nose,\ncross-section shape, and train nose shape. The bio-models are\ndrawn by computer-aided drawing software and the air flow around\nthe bio-models are simulated in order to obtain the drag coeffi-\ncients in counterflow condition. The drag coefficient of each bio-\nmodels are compared and validated with published drag coefficient\nof Harmony CRH380A, which is 0.1971. The bio-models with\nlowest drag coefficient is the bio-model with sharp-streamlined\nnose and round-based crosssection, which is 0.1559... (cntd.)\nRussia’s Peregrine Falcon Takes Flight by K. Barrow.\nAbstract: Known in Russia as the Sapsan, or Peregrine Falcon,\nRussian Railway’s (RZD’s) first high-speed train is being launched\nthis month with service between Moscow and St. Petersburg.\nSiemens was awarded a contract for a fleet of eight 10-car Sap-\nsan trains in May 2006. Designed to operate in temperatures as\nlow as -40 degrees Celsius, the Sapsan has a wider body shell than\nits western European counterparts, very strict criteria ....(cntd.)\nDiscover: The Peregrine Falcon achieves high-speed travel by us-\ning a streamlined body shape to reduce drag. This allows it to reach\nspeeds of up to 240 miles per hour during its hunting stoop. Sim-\nilarly, high-speed trains are designed with a streamlined shape to\nreduce air resistance and increase speed. The Russian Sapsan train,\nnamed after the Peregrine Falcon, is designed to operate in extreme\ntemperatures and has a wider body shell than its western European\ncounterparts to reduce drag.\nAbstract: The essential feature that makes the Peregrine Falcon’s\nhigh-speed travel successful is its streamlined body shape that re-\nduces drag. In design terms, we can describe this strategy as fol-\nlows: ”A streamlined body shape is used to reduce air resistance\nand increase speed.”\nAnswer: A vehicle designed for rapid long-distance travel could\nmimic the streamlined body shape of the Peregrine Falcon to re-\nduce drag and increase speed. This could involve a design with a\nsharp, pointed nose and a smooth, rounded body. This design could\npotentially improve the speed and efficiency of long-distance travel\nby reducing air resistance.\nFigure 3: Full output from the Few-Shot Prompting LLM +\nRAG experiment and the answer presented to the annota-\ntors. Human annotators labeled this response with a feasi-\nbility score of 5 and a novelty score of 1.\nMethod Feasibility Novelty\nZero-Shot Prompting 3.45 ± 1.77 1.90 ± 1.55\nFew-Shot Prompting\n+ RAG\n3.70 ± 1.58 1.75 ± 1.41\nReAct Agent 2.45 ± 2.06 1.85 ± 1.53\nReAct Agent + RAG 3.50 ± 1.50 2.00 ± 1.34\nTable 1: Experts’ Annotations of generated design solutions\nfor the design questions in the test set.\nsearch query and retrieves the top-k papers. The titles, au-\nthors, URLs, and abstracts of each of the top-k papers are\npassed to the next reasoning step. We find that settingk = 2\nis the maximum value ofk that consistently fits the paper ab-\nstract and previous reasoning steps within the context win-\ndow. To prevent the agent chain from exceeding the context\nwindow, we pass the initial zero-shot prompt and the most\nrecent reasoning step at every time step to the agent. The\nRAG tool uses the Internet, but the other tools are natural-\nlanguage prompts.\nResults\nResults from the expert annotations of all four experimen-\ntal settings are displayed in Table 1 and Figure 2. There are\nseveral key takeaways after analysis of the distribution of the\nexpert annotations. First, integrating RAG increases the fea-\nsibility of the design solution in both prompting and agent\nsettings, but the novelty of the design solution varies. More-\nover, methods with RAG are less likely to be unrelated to\nthe design question. Lastly, prompting methods have higher\nfeasibility and novelty scores in most cases than the agent\ncounterparts. In turn, these results suggest that the Few-Shot\nPrompting LLM with RAG setting performs the best based\non the experts’ annotations and their feedback that the con-\nsideration of feasibility warrants a heightened emphasis. As\na result, we opt to use the prompting LLM with RAG setting\nwithin BIDARA.\nDirections and Motivation of Current Work\nFunction-Calling Methods\nThe approach for RAG and tool usage in BIDARA shifted\nto function-calling, which provides a suite of tools to aid the\nuser in further exploration of their topic. Function-calling\nis a feature supported by the OpenAI API (OpenAI 2023a).\nBIDARA has access to five different functions: paper\nretrieval, setting/querying research space, patent search, and\nimage generation. Paper retrieval, patent search, and image\ngeneration use a straightforward API call structure while\nthe research space functions store and access information in\nLlamaIndex nodes (Liu 2022).\n276\nFrameworks\nBIDARA with function-calling uses GPT-4 as the base\nLLM. In addition, we use a Google Search API to retrieve\npatents from Google Patents (SerpApi 2023). Images are\ngenerated by Dall-E 2, and the full text of papers is re-\ntrieved from Semantic Scholar and organized into a search-\nable database using LlamaIndex. The research space (paper\nQ&A) feature is based on code from S2QA (Rohatgi 2023),\na project made in collaboration with Semantic Scholar.\nMotivation and Testing\nBiomimicry domain experts were invited to interact with\nBIDARA and feedback via user interviews was collected\non their experiences with a preliminary few-shot version of\nBIDARA, such as liked and disliked features along with ar-\neas for improvement. This feedback was collected to allow\nexperts to input challenges relevant to their work and steer\nthe dialogue.\nThe accuracy and dependability of function-calling is\ntested during development. We ask targeted questions to-\nwards the research space, and paper Q&A outputs are man-\nually compared to the information provided in the papers\nthemselves to ensure accuracy.\nExpert Feedback\nOne expert, a bio-inspired design researcher, and professor,\nstates that she likes “the reminders about the process,” the\nprovided citations, and the ease of chatting. However, she\nnotes that the response time is slower than other AI sys-\ntems and that the suggested design strategies are limiting.\nShe suggested that the citations should become hyperlinks\nto papers, a feature later implemented using RAG. The em-\nphasis on fact-based generation is echoed by other experts.\nWe address this concern with RAG, gathering research and\npatent papers to enable more in-depth responses.\nAnother interviewee, a consultant and university fellow\nfor biomimicry, likes how BIDARA provides a detailed,\nstructured response in the Biologize step and how it points\nout if a question is unfeasible and suggests rephrasing the\nquestion (e.g. “How can we design a vehicle to travel from\nSydney to the U.S. in one hour?” → “How might we enable\nrapid, efficient, and sustainable transcontinental travel?”).\nHowever, she notices that sometimes, BIDARA misses im-\nportant factors such as listing impacts and leverage points.\nOther interviewees have mentioned that there are not\nenough visuals, a user experience issue that led to the addi-\ntion of generateImage(). Although capable of general\nvisualizations, BIDARA is unable to provide scientifically\naccurate design sketches, judged in comparison to expert di-\nagrams in the same context (Biomimicry Institute 2023). For\ninstance, when asking BIDARA to display an image of “a\njackrabbit using its large ears to cool off,” the assistant re-\nsponds with a picture of a jackrabbit and a short description\nof blood vessel expansion in the ears dissipating heat. When\nasked to provide a diagram of the facilitation of heat loss,\nBIDARA outputs a vague depiction of branching vessels.\nFigure 4: Function-calling retrieves patents in response to a\nuser query on harbor seal whiskers.\nFunction-Calling Results\nFrom tests performed during development, the function-\ncalling framework calls the function implied by a query to\na very high degree of accuracy. The paper Q&A responses\nalso align with the papers in the research space. When re-\nquesting specific papers using ambiguous references (”Ac-\ncording to Guo and Wang”, “In Cui et. al.”,...), specific in-\nformation from the correct papers is also reliably retrieved.\nConclusion\nThe development of BIDARA marks an advancement in the\nfield of AI tools for biomimicry. This study integrates Re-\nAct agents and RAG to address the key issues of hallucina-\ntion and outdated knowledge in LLMs, particularly when ap-\nplied as research assistants in biomimetic design. Our results\ndemonstrate that the inclusion of RAG enhances the feasibil-\nity of design solutions generated by LLMs in both prompting\nand agent settings. Notably, the Few-Shot Prompting LLM\n+ RAG emerged as the most feasible method, and Agent +\nRAG was the most novel. Moreover, the ongoing incorpo-\nration of a more diverse RAG toolkit provides an expanded,\ninteractive capability. This work not only introduces a for-\nmal study of biomimicry via LLMs but also furthers the in-\ntegration of more sophisticated AI techniques in the field.\nAcknowledgements\nThis work was conducted as part of the Periodic Table of\nLife (PeTaL) project at the NASA Glenn Research Center\n(Shyam et al. 2019). We extend our heartfelt gratitude to our\nmentors, Brandon Ruffridge and Herbert Schilling, for their\nguidance, expertise, and support throughout our work on this\nproject. Finally, we are grateful to the many biomimicry ex-\nperts for sharing their knowledge and providing critical feed-\nback on BIDARA.\n277\nReferences\nBiomimicry Institute. 2023. Abstract Design Strate-\ngies. https://toolbox.biomimicry.org/methods/abstract/. Ac-\ncessed: 2024-02-26.\nBiomimicry Institute. 2023. The Biomimicry Process -\nBiomimicry Toolbox — toolbox.biomimicry.org. https://\ntoolbox.biomimicry.org/methods/process/. [Accessed 18-\n09-2023].\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. arXiv:2005.14165.\nChen, C.; Li, Y .; Tao, Y .; Chen, J.; Liu, Q.; and Li, S. 2021a.\nA method to automatically push keywords for biological in-\nformation searching in bio-inspired design. Proceedings of\nthe Institution of Mechanical Engineers, Part C: Journal of\nMechanical Engineering Science, 235(1): 30–47.\nChen, C.; Tao, Y .; Li, Y .; Liu, Q.; Li, S.; and Tang, Z. 2021b.\nA structure-function knowledge extraction method for bio-\ninspired design. Computers in Industry, 127: 103402.\nGao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang,\nY .; Callan, J.; and Neubig, G. 2023. Pal: Program-aided\nlanguage models. In International Conference on Machine\nLearning, 10764–10799. PMLR.\nKinney, R. M.; Anastasiades, C.; Authur, R.; Beltagy,\nI.; Bragg, J.; Buraczynski, A.; Cachola, I.; Candra, S.;\nChandrasekhar, Y .; Cohan, A.; Crawford, M.; Downey,\nD.; Dunkelberger, J.; Etzioni, O.; Evans, R.; Feldman, S.;\nGorney, J.; Graham, D. W.; Hu, F.; Huff, R.; King, D.;\nKohlmeier, S.; Kuehl, B.; Langan, M.; Lin, D.; Liu, H.; Lo,\nK.; Lochner, J.; MacMillan, K.; Murray, T.; Newell, C.; Rao,\nS. R.; Rohatgi, S.; Sayre, P. L.; Shen, Z.; Singh, A.; Soldaini,\nL.; Subramanian, S.; Tanaka, A.; Wade, A. D.; Wagner,\nL. M.; Wang, L. L.; Wilhelm, C.; Wu, C.; Yang, J.; Zamar-\nron, A.; van Zuylen, M.; and Weld, D. S. 2023. The Seman-\ntic Scholar Open Data Platform. ArXiv, abs/2301.10140.\nKomeili, M.; Shuster, K.; and Weston, J. 2022. Internet-\nAugmented Dialogue Generation. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 8460–8478. Dublin,\nIreland: Association for Computational Linguistics.\nLiang, Y .; Wu, C.; Song, T.; Wu, W.; Xia, Y .; Liu, Y .;\nOu, Y .; Lu, S.; Ji, L.; Mao, S.; Wang, Y .; Shou, L.; Gong,\nM.; and Duan, N. 2023. TaskMatrix.AI: Completing Tasks\nby Connecting Foundation Models with Millions of APIs.\narXiv:2303.16434.\nLiu, J. 2022. LlamaIndex. https://github.com/jerryjliu/\nllama\nindex. Accessed: 2024-02-26.\nMallen, A.; Asai, A.; Zhong, V .; Das, R.; Khashabi, D.;\nand Hajishirzi, H. 2023. When Not to Trust Language\nModels: Investigating Effectiveness of Parametric and Non-\nParametric Memories. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 9802–9822. Toronto, Canada: As-\nsociation for Computational Linguistics.\nNagel, J. K. S. 2014. A Thesaurus for Bioinspired Engi-\nneering Design, 63–94. London: Springer London. ISBN\n978-1-4471-5248-4.\nOpenAI. 2023a. Function Calling - OpenAI Platform\nDocumentation. https://platform.openai.com/docs/guides/\nfunction-calling. Accessed: 2024-02-26.\nOpenAI. 2023b. GPT-4 Technical Report.\narXiv:2303.08774.\nRohatgi, S. 2023. S2QA: Question Answering on research\npapers from Semantic Scholar.\nSerpApi. 2023. Google Search Results in Python. https:\n//github.com/serpapi/google-search-results-python. Ac-\ncessed: 2024-02-26.\nShu, L. 2010. A Natural Language Approach to Biomimetic\nDesign. AI EDAM, 24: 507–519.\nShuster, K.; Poff, S.; Chen, M.; Kiela, D.; and Weston, J.\n2021. Retrieval Augmentation Reduces Hallucination in\nConversation. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, 3784–3803. Punta Cana,\nDominican Republic: Association for Computational Lin-\nguistics.\nShyam, V .; Friend, L.; Whiteaker, B.; Bense, N.; Dowdall,\nJ.; Boktor, B.; Johny, M.; Reyes, I.; Naser, A.; Sakhamuri,\nN.; Kravets, V .; Calvin, A.; Gabus, K.; Goodman, D.;\nSchilling, H.; Robinson, C.; Reid II, R. O.; and Unsworth,\nC. 2019. PeTaL (Periodic Table of Life) and Physiomimet-\nics. Designs, 3(3).\nYang, Z.; Li, L.; Wang, J.; Lin, K.; Azarnasab, E.; Ahmed,\nF.; Liu, Z.; Liu, C.; Zeng, M.; and Wang, L. 2023. MM-\nREACT: Prompting ChatGPT for Multimodal Reasoning\nand Action. arXiv:2303.11381.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK.; and Cao, Y . 2023. ReAct: Synergizing Reasoning and\nActing in Language Models. arXiv:2210.03629.\nZhu, Q.; Zhang, X.; and Luo, J. 2022. Biologically Inspired\nDesign Concept Generation Using Generative Pre-Trained\nTransformers. arXiv:2212.13196.\n278"
}