{
    "title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?",
    "url": "https://openalex.org/W3213881810",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Dong, Xinhsuai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2587105294",
            "name": "Tuan, Luu Anh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2037051402",
            "name": "Lin Min",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2479487170",
            "name": "Yan, Shuicheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2313469222",
            "name": "Zhang, Hanwang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2964153729",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2962818281",
        "https://openalex.org/W1682403713",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2014703436",
        "https://openalex.org/W2995607862",
        "https://openalex.org/W3118062200",
        "https://openalex.org/W2963389226",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963857521",
        "https://openalex.org/W2963501948",
        "https://openalex.org/W3106428938",
        "https://openalex.org/W2963207607",
        "https://openalex.org/W2152790380",
        "https://openalex.org/W2804613085",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970727289",
        "https://openalex.org/W2766108848",
        "https://openalex.org/W2949128310",
        "https://openalex.org/W2949759968",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2787708942",
        "https://openalex.org/W2963488798",
        "https://openalex.org/W2964420626",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2555897561",
        "https://openalex.org/W2799007037",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963126845",
        "https://openalex.org/W2913266441",
        "https://openalex.org/W2947415936",
        "https://openalex.org/W2963969878",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2950687935",
        "https://openalex.org/W3103340107"
    ],
    "abstract": "The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.",
    "full_text": "How Should Pre-Trained Language Models Be\nFine-Tuned Towards Adversarial Robustness?\nXinshuai Dong\nNanyang Technological University & Sea AI Lab\ndongxinshuai@outlook.com\nLuu Anh Tuan\nNanyang Technological University\nanhtuan.luu@ntu.edu.sg\nMin Lin\nSea AI Lab\nlinmin@sea.com\nShuicheng Yan\nSea AI Lab\nyansc@sea.com\nHanwang Zhang\nNanyang Technological University\nhanwangzhang@ntu.edu.sg\nAbstract\nThe ﬁne-tuning of pre-trained language models has a great success in many NLP\nﬁelds. Yet, it is strikingly vulnerable to adversarial examples,e.g., word substitution\nattacks using only synonyms can easily fool a BERT-based sentiment analysis\nmodel. In this paper, we demonstrate that adversarial training, the prevalent defense\ntechnique, does not directly ﬁt a conventional ﬁne-tuning scenario, because it\nsuffers severely from catastrophic forgetting: failing to retain the generic and robust\nlinguistic features that have already been captured by the pre-trained model. In\nthis light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial\nﬁne-tuning method from an information-theoretical perspective. In particular, RIFT\nencourages an objective model to retain the features learned from the pre-trained\nmodel throughout the entire ﬁne-tuning process, whereas a conventional one only\nuses the pre-trained weights for initialization. Experimental results show that RIFT\nconsistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment\nanalysis and natural language inference, under different attacks across various\npre-trained language models. 1.\n1 Introduction\nDeep models are well-known to be vulnerable to adversarial examples [64, 19, 50, 35]. For instance,\nﬁne-tuned models pre-trained on very large corpora can be easily fooled by word substitution attacks\nusing only synonyms [ 2, 58, 32, 12]. This has raised grand security challenges to modern NLP\nsystems, such as spam ﬁltering and malware detection, where pre-trained language models like BERT\n[11] are widely deployed.\nAttack algorithms [ 19, 7, 76, 40, 2] aim to maliciously generate adversarial examples to fool a\nvictim model, while adversarial defense aims at building robustness against them. Among the\ndefense methods, adversarial training [64, 19, 48, 45] is the most effective one [3]. It updates model\nparameters using perturbed adversarial samples generated on-the-ﬂy and yields consistently robust\nperformance even against the challenging adaptive attacks [3, 68].\nHowever, despite its effectiveness in training from scratch, adversarial training may not directly\nﬁt the current NLP paradigm, the ﬁne-tuning of pre-trained language models. First, ﬁne-tuning\nper sesuffers from catastrophic forgetting [ 46, 18, 34], i.e., the resultant model tends to over-ﬁt\nto a small ﬁne-tuning data set, which may deviate too far from the pre-trained knowledge [25, 81].\nSecond, adversarially ﬁne-tuned models are more likely to forget: adversarial samples are usually\n1Our code will be available at https://github.com/dongxinshuai/RIFT-NeurIPS2021.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2112.11668v1  [cs.CL]  22 Dec 2021\nFigure 1: An illustration of the overall objective of RIFT. Maximizing I(S; Y) encourages features\nof the objective model to be predictive of the class label, while maximizing I(S; T/divides.alt0Y) encourages\nlearning robust and generic linguistic information from the pre-trained model. (Random variable S\ndenotes extracted features of X by the objective model and T by the pre-trained language model)\nout-of-distribution [38, 63], thus they are generally inconsistent with the pre-trained model. As a\nresult, adversarial ﬁne-tuning fails to memorize all the robust and generic linguistic features already\nlearned during pre-training [65, 57], which are however very beneﬁcial for a robust objective model.\nAddressing forgetting is essential for achieving a more robust objective model. Conventional wisdom\nsuch as pre-trained weight decay [8, 10] and random mixout [37] mitigates forgetting by constraining\nlp distance between the two models’ parameters. Though effective to some extent, however, it is\nlimited because change in the model parameter space only serves as an imperfect proxy for that in the\nfunction space [5]. Besides, the extent to which an encoder fails to retain information also depends\non the input distribution. Therefore, a better way to encourage memorization is favorable.\nIn this paper, we follow an information-theoretical lens to look into the forgetting problem: we\nemploy mutual information to measure how well an objective model memorizes the useful features\ncaptured before. This motivates our novel adversarial ﬁne-tuning method, Robust Informative Fine-\nTuning (RIFT). In addition to ﬁtting a down-stream task as conventions, RIFT maximizes the mutual\ninformation between the output of an objective model and that of the pre-trained model conditioned\non the class label. It encourages an objective model to continuously retain useful information from\nthe pre-trained one throughout the whole ﬁne-tuning process, whereas the conventional one only\nmakes use of the pre-trained weights for initialization. We illustrate the overall objective of RIFT in\nFigure 1 and summarize the major contributions of this paper as follows:\n• To the best of our knowledge, we are the ﬁrst to investigate the intensiﬁed phenomenon of\ncatastrophic forgetting in the adversarial ﬁne-tuning of pre-trained language models.\n• To address the forgetting problem and achieve more robust models, we propose a novel approach\nnamed Robust Informative Fine-Tuning (RIFT) from an information-theoretical perspective. RIFT\nenables an objective model to retain robust and generic linguistic features throughout the whole\nﬁne-tuning process and thus enhance robustness against adversarial examples.\n• We empirically evaluate RIFT on two prevailing NLP tasks, sentiment analysis and natural language\ninference, where RIFT consistently outperforms the state-of-the-arts in terms of robustness, under\ndifferent attacks across various pre-trained language models.\n2 Methodology\n2.1 Notations and Problem Setting\nIn this paper, we focus on the text classiﬁcation task to introduce our method, while it can be easily\nextended to other NLP tasks. We suppose random variables X,Y ∼pD(x,y), where X represents\nthe textual input, Y represents the class label, pD is the data distribution, and x,y are the observed\nvalues. Our goal is to build a classiﬁer q(y/divides.alt0Fs(x)), where Fs(⋅), referred to as our objective model\n2\n(a) In adversarial ﬁne-tuning, the relative L2 dist-\nance continuously grows as the ﬁne-tuning proceeds.\n(b) Trading clean accuracy off against robustness (by\nincreasing β in Eq. 2) induces increased relative L2\ndistance at the last epoch.\nFigure 2: Relative L2 distance between the pre-trained model and the objective model in the parameter\nspace, under different ﬁne-tuning schemes on IMDB, with small learning rate 2e-5.\nin the rest of this paper, is a feature extractor ﬁne-tuned from the encoder of a pre-trained language\nmodel, e.g., Transformer [72], and q(⋅/divides.alt0⋅) is parameterized by using an MLP with softmaxed output.\nWe favor a classiﬁer that is robust against adversarial attacks [64, 2], i.e., maintaining high accuracy\neven given adversarial examples as inputs.\n2.2 Adversarial Fine-Tuning Suffers From Forgetting\nAdversarial training [64, 19, 45] is currently the most effective defense technique [3]. Its training\nobjective can be generally formulated as:\nmin[ E\nx,y∼pD\n[ max\nˆx∈B(x)\nL(x,ˆx,y)]], (1)\nwhere B(x) deﬁnes the allowed perturbation space around x, and L is a loss to encourage correct\npredictions both given vanilla samples and given perturbed examples. Such an objective function can\nalso be applied to the ﬁne-tuning of a pre-trained language model towards robustness, and we refer\nto it as adversarial ﬁne-tuning. The loss function in Eq. 1 can be speciﬁed following Miyato et al.\n[48], Zhang et al. [79] in a semi-supervised fashion as:\n−log q(y/divides.alt0Fs(x)) +βKL/parenleft.alt1q(⋅/divides.alt0Fs(x))/divides.alt0/divides.alt0q(⋅/divides.alt0Fs(ˆx))/parenright.alt1, (2)\nwhere the Kullback–Leibler divergence encourages invariant predictions betweenxand ˆx.\nDespite its effectiveness in training from scratch, such an adversarial objective function may not\ndirectly ﬁt the ﬁne-tuning scenario. To begin with, ﬁne-tuning itself suffers from catastrophic\nforgetting [46, 18, 34]. During ﬁne-tuning, an objective model Fs(⋅) tends to continuously deviate\naway from the pre-trained one to ﬁt a down-stream task [ 25, 81], and thus the useful information\nalready captured before is less utilized as the ﬁne-tuning proceeds. Further, adversarial ﬁne-tuning\nsuffers even more from the forgetting problem, the reasons of which are given as what follows.\n(i) Adversarial Fine-Tuning Tends to Forget:Adversarial ﬁne-tuning targets at tackling adversarial\nexamples, which are generally out of the manifold [38, 63] of the pre-training corpora. To additionally\nhandle them, an objective model would be ﬁne-tuned towards a solution that is far away from the\noptimization starting point, i.e., the pre-trained model. Figure 2 (b) empirically shows this point: by\nincreasing βin Eq. 2, we emphasize more on robustness instead of vanilla accuracy, and consequently,\nat the last epoch the distance between models also increases. Besides, adversarial ﬁne-tuning often\nentails more iterations to converge (several times of normal ﬁne-tuning), which further intensiﬁes the\nforgetting problem, as the objective model is continuously deviating away as shown in Figure 2 (a).\n(ii) Adversarial Fine-Tuning Needs to Memorize:Overﬁtting to training data is a dominant phe-\nnomenon throughout the adversarial training/ﬁne-tuning process [60]. To mitigate overﬁtting and\ngeneralize better, adversarial ﬁne-tuning should retain all the generalizable features already captured\nby the pre-trained model [11, 41]. In addition, adversarial ﬁne-tuning favors an objective model that\n3\nextracts features invariant to perturbations [70, 28] for robustness. As such, all those generalizable\nand robust linguistic information captured during pre-training [11, 65] are particularly beneﬁcial and\nshould be memorized. We empirically validates that encouraging memorization does improve both\ngeneralization and robustness in Sec. 3.5.\nTo address forgetting, previous methods such as pre-trained weight decay [8, 10] and random mixout\n[37] have shown their effectiveness in stabilizing ﬁne-tuning [ 81]. However, they focus only on\nthe parameter space, in which they encourage an object model to be similar to the pre-trained one.\nDistance in the parameter space can only approximately characterize the change in the function space,\nand fails to take the data distribution into consideration. A more natural way to capture the extent to\nwhich a model memorizes or forgets, should be using the mutual information between outputs of the\ntwo models, and we provide our solution as follows.\n2.3 Informative Fine-Tuning\nWe ﬁrst use an information-theoretical perspective to look into how a pre-trained model should be\nleveraged throughout the whole ﬁne-tuning process.\nWe deﬁne random variable T = Ft(X) as the feature of X extracted by the pre-trained language\nmodel Ft(⋅), and tas the observed value of T. Similarly, we deﬁne random variable S = Fs(X)\nas the feature of X extracted by our objective model Fs(⋅), and sas the observed value of S. We\nformulate an overall ﬁne-tuning objective as follows. The motivation is to train Fs(⋅) such that Sis\ncapable of predicting Y, as well as preserving the information from T.\nmax I(S; Y,T ), (3)\ni.e., maximizing the mutual information between (i) the feature extracted by the objective model\nand (ii) the class label plus the feature extracted by the pre-trained model. Since the pre-trained\nmodel Ft(⋅) is a ﬁxed deterministic function, the ﬁne-tuning objective in Eq. 3 in essense encourages\nFs(⋅) to output features that contain as much information as possible for predicting Y and T. This\nenables the objective model to learn from the pre-trained language model via T throughout the whole\nﬁne-tuning process, and thus helps address the forgetting problem.\nHowever, the objective deﬁned in Eq. 3 is generally hard to optimize directly. Therefore, we\ndecompose Eq. 3 into two terms as follows:\nI(S; Y,T ) =I(S; Y) +I(S; T/divides.alt0Y), (4)\nwhere I(S; Y) measures how well the output of our objective model can predict the label, and\nI(S; T/divides.alt0Y) measures when conditioned on the class label, how well the output features of the two\nmodels can predict each other. Visualization of each component can be seen in Figure 1 for more\nintuitive understandings. We next introduce how each term in the right-hand side of Eq. 4 can be\ntransformed into a tractable lower bound for optimization.\n(i) Maximizing I(S;Y): We treat q(y/divides.alt0s), which is the classiﬁcation layer that takes the features\nextracted by the objective model as input, as a variational distribution of p(y/divides.alt0s), and derive a\nvariational lower bound on I(S; Y) following variational inference [33] as follows:\nI(S; Y) =H(Y) −Ex,y∼pD[−log q(y/divides.alt0s)]+KL/parenleft.alt1p(⋅/divides.alt0s)/divides.alt0/divides.alt0q(⋅/divides.alt0s)/parenright.alt1(5)\n≥H(Y) −Ex,y∼pD[−log q(y/divides.alt0s)], (6)\nwhere H(Y) is a constant measuring the shannon entropy ofY, and EX,Y[−log q(y/divides.alt0s)]is essentially\nthe cross-entropy loss using q(y/divides.alt0s) for classiﬁcation. Then, the objective of maximizing I(S; Y) can\nbe achieved by minimizing EX,Y[−log q(y/divides.alt0s)]instead.\n(ii) Maximizing I(S;T| Y):The deﬁnition of the conditional mutual information I(S; T/divides.alt0Y) is as\nfollows:\nI(S; T/divides.alt0Y) =Ey∼pD(y)[I(S; T)/divides.alt0Y =y]=Ey∼pD(y)[Ex∼pD(x/divides.alt0y)[log p(s,t/divides.alt0y)\np(s/divides.alt0y)p(t/divides.alt0y)]]. (7)\nTo achieve a tractable objective for maximizing Eq. 7, we employ noise contrastive estimation [20, 49]\nand derive a lower bound −Linfo on the conditional mutual information. This can be summarized in\nthe following Lemma (the proof of which can be found in Appendix A.2):\n4\nFigure 3: Visualization of the learned geometry using 500 random samples from IMDB by t-SNE [71].\n▲ and /uni220Edenote samples from two different classes respectively. Different colors represent different\ndata point IDs (approximately due to limited color space). In (c) and (d), Fs(x) are projected to S1\nwith radius 1, while Ft(x) with radius 0.9. For each sub-image: (a) The geometry of Fs(x) before\nﬁne-tuning, represents the data manifold. (b) Maximizing I(S; Y) encourages separating the whole\ndata manifold into two class-speciﬁc data manifolds. (c) Maximizing I(S; T) by contrastive loss in\nessence encourages alignment between two circles and uniformity over the whole data manifold. (d)\nMaximizing I(S; T/divides.alt0Y) by contrastive loss in essence encourages alignment and uniformity inside\neach class-speciﬁc data manifold. Best view in color with zooming in.\nLemma 1. Given {xi,y}N\ni=1 that is sampled i.i.d. frompD(x/divides.alt0y), si = Fs(xi), and ti = Ft(xi),\nI(S; T/divides.alt0Y) is lower bounded by−Linfo = Ey∼pD(y)/bracketleft.alt1E{xi,y}N\ni=1\n[1\nN ∑N\ni=1 log efy(si,ti)\n∑N\nj=1 efy(si,tj) +log N]/bracketright.alt,\nand fy is a score function indexed byy.\nBy leveraging Lemma 1, Linfo can be computed using a batch of samples and then minimized for\nmaximizing I(S; T/divides.alt0Y). The score function fy is deﬁned as the inner product after non-linear\nprojections into a space of hyper-sphere following [9] as fy(a,b) = 1\nτ\n/uni27E8g1\ny(a),g2\ny(b)/uni27E9\n/parallel.alt1g1y(a)/parallel.alt12/parallel.alt1g2y(b)/parallel.alt12\n, where g1\ny and\ng2\ny are parameterized by using MLPs.\n2.4 Conditional Mutual Information Better Fits A Down-Stream Task\nAbove we have introduced the training objective of informative ﬁne-tuning and decomposed it into\nI(S; Y) and I(S; T/divides.alt0Y) for optimization. However, one may wonder why not maximize I(S; T)\ndirectly instead of I(S; T/divides.alt0Y). From an information-theoretical perspective, if we maximize I(S; Y)\nand I(S; T), the intersection of the three circles in Figure 1, i.e., the interaction information of them,\nis repeatedly optimized and might induce conﬂiction. To further elaborate this point, we look into\ncontrastive loss and give an explanation as follows.\nWe consider contrastive learning by decomposing it into the encouragement of alignment and unifor-\nmity following [74]. For example, maximizing I(S; T) =H(S) −H(S/divides.alt0T) is to decrease H(S/divides.alt0T)\nand increase H(S). Decreasing H(S/divides.alt0T) corresponds to encouraging alignment in contrastive learn-\ning, which aims to align si and ti in the sense of inner product when they are projected into a\nhyper-sphere. In the meanwhile, increasing H(S) corresponds to uniformity in contrastive learning,\nwhich encourages si and sj, where i≠j, to diffuse over the whole sub-space as uniformly as possible.\nIt can be seen in Figure 3 (c) that, in the space of S1, alignment is achieved in that points from two\ndifferent circles but with the same color are aligned, and uniformity is achieved in that all points\ndiffuse over the whole S1.\nHowever, when maximizing I(S; T), the uniformity in the contrastive loss is encouraged over the\nwhole data manifold. Diffusing uniformly over the whole data manifold can be against the objective\nof maximizing I(S; Y), which aims to separate the whole data manifold into class-speciﬁc parts as\nshown in Figure 3 (b). In contrast, maximizing I(S; T/divides.alt0Y) only encourages uniformity inside each\nclass-speciﬁc data manifold, which is complementary to maximizingI(S; Y), as show in Figure 3 (d).\nIn the meanwhile, the alignment is still enforced. More empirical support can be found in Sec. 3.4.\n5\nAlgorithm 1RIFT\nInput: dataset D, hyper-parameters of AdamW [43]\nOutput: the model parameters θand φ\n1: Initialize θusing the pre-trained model, and initialize φand ϕrandomly.\n2: repeat\n3: Sample y∼pD(y) and then {xi,y}N\ni=1 ∼pD(x/divides.alt0y)\n4: for every xi, yin the mini-batch {xi,y}N\ni=1 do\n5: Find ˆxi by solving Eq. 8;\n6: end for\n7: Compute the loss function deﬁned in Eq. 11 and update θ, φ, and ϕby gradients.\n8: until the training converges.\n2.5 Robust Informative Fine-Tuning\nIn this section, we are going to put the objective function of informative ﬁne-tuning into the adversarial\ncontext and introduce Robust Informative Fine-Tuning (RIFT).\nTo robustly train a model, the adversarial examples ˆxfor training should be formulated and generated\nﬁrst. As our end goal is to enhance robustness in down-stream tasks, the generation process of\nadversarial examples should focus on preventing a model from predicting the ground truth label.\nHowever, using label for generating adversarial examples in training often induces label leaking [36],\ni.e., the generated adversarial examples contains label information which is used as a trivial short-cut\nby a classiﬁer. As such, we follow [48, 79] to generate ˆxin a self-supervised fashion:\nˆx=arg max\nx′ ∈B(x)\nKL/parenleft.alt1q(⋅/divides.alt0Fs(x))/divides.alt0/divides.alt0q(⋅/divides.alt0Fs(x′))/parenright.alt1. (8)\nBy solving Eq. 8, ˆxis found to induce the most different prediction from that of a vanilla sample xin\nterms of KL, inside the attack space B(x).\nNext, we introduce how to robustly optimize each objective in the informative ﬁne-tuning by using ˆx.\n(i) Robustly Maximizing I(S;Y):As shown in Sec. 2.3, maximizing I(S; Y) can be achieved by\nminimizing a cross-entropy loss instead. To encourage adversarial robustness, this cross-entropy loss\ncan be upgraded to encourage both correct predictions on and invariant predictions between xand ˆx\n[48, 79]. We formulate such an objective function as follows:\nmin\nθ,φ\nLr-task, Lr-task = E\nx,y∼pD\n/bracketleft.alt1−log q(y/divides.alt0Fs(x)) +βKL/parenleft.alt1q(⋅/divides.alt0Fs(x))/divides.alt0/divides.alt0q(⋅/divides.alt0Fs(ˆx))/parenright.alt1/bracketright.alt, (9)\nwhere θdenotes the parameters of Fs(⋅) and φdenotes the parameters of q(⋅/divides.alt0⋅). Minimizing Lr-task in\nEq. 9 corresponds to adversarilly maximizing I(S; Y) for robust performance in a down-stream task.\nBy doing so, the adversarial example ˆxis generated by Eq. 8 ﬁrst and then both xand ˆxare used to\noptimize the model parameters θand φ.\n(ii) Robustly Maximizing I(S;T| Y):We aim to maximize the conditional mutual information\nI(S; T/divides.alt0Y), but under an adversarial distribution of input data. We formulate such a term as\nI( ˆS; T/divides.alt0Y), where random variable ˆS=Fs( ˆX), ˆX∼padv(ˆx/divides.alt0x,θ,φ, B), and sampling from padv, the\nadversarial distribution of input data, is to generate adversarial example ˆxby Eq. 8.\nTo optimize I( ˆS; T/divides.alt0Y), we propose −Lr-info as a lower bound on it, and formulate the objective to\nminimize Lr-info as follows (similar to −Linfo by using Lemma 1):\nmin\nθ,ϕ\nLr-info, Lr-info = E\ny∼pD(y)\n/bracketleft.alt1E\n{xi,y}N\ni=1∼pD(x/divides.alt0y)\n[ 1\nN\nN\n/summation.disp\ni=1\n−log efy(ˆsi,ti)\n∑N\nj=1 efy(ˆsi,tj) −log N]/bracketright.alt, (10)\nwhere ˆsi = Fs(ˆxi), ti = Ft(xi), and ϕdenotes the parameters of all the score functions fy. By\nEq. 10, we are able to encourage Fs(⋅) to retain information from Ft(⋅) in a robust fasion.\nNoted that, in Lr-info we do not use ˆxi to extract features from the pre-trained model. This follows the\nspirit of Knowledge Distillation [22] (though the setting is different in that a student is expected to\nperform identically to a teacher in knowledge distillation, while in ﬁne-tuning it does not): the data\nused to extract features of a teacher should be inside the domain for which the teacher is trained. A\n6\nTable 1: Accuracy(%) of different ﬁne-tuning methods under attacks on IMDB.\nMethod Model Genetic PWWS\nStandard BERT 38.1 ±2.5 40.7±1.1\nAdv-Base BERT 74.8 ±0.4 68.3±0.3\nAdv-PTWD BERT 73.9 ±0.4 69.1±0.7\nAdv-Mixout BERT 75.4 ±0.7 68.8±0.6\nRIFT BERT 77.2±0.8 70.1±0.5\n(a) Accuracy (%) based on BERT-base-uncased.\nMethod Model Genetic PWWS\nStandard RoBERTa 42.1 ±2.1 45.6±3.1\nAdv-Base RoBERTa 70.3 ±1.2 63.3±0.7\nAdv-PTWD RoBERTa 69.3 ±1.4 64.4±0.3\nAdv-Mixout RoBERTa 70.6 ±1.0 63.9±1.3\nRIFT RoBERTa 73.5±0.8 66.3±0.7\n(b) Accuracy (%) based on RoBERTa-base.\nTable 2: Accuracy(%) of different ﬁne-tuning methods under attacks on SNLI.\nMethod Model Genetic PWWS\nStandard BERT 40.1 ±0.7 19.4±0.4\nAdv-Base BERT 75.7 ±0.5 72.9±0.2\nAdv-PTWD BERT 75.2 ±1.0 72.6±0.5\nAdv-Mixout BERT 76.3 ±0.8 73.2±1.0\nRIFT BERT 77.5±0.9 74.3±1.1\n(a) Accuracy (%) based on BERT-base-uncased.\nMethod Model Genetic PWWS\nStandard RoBERTa 43.4 ±1.2 20.4±1.0\nAdv-Base RoBERTa 82.6 ±0.6 79.9±0.7\nAdv-PTWD RoBERTa 81.2 ±0.8 78.9±0.7\nAdv-Mixout RoBERTa 82.6 ±0.9 80.6±0.3\nRIFT RoBERTa 83.5±0.8 81.1±0.4\n(b) Accuracy (%) based on RoBERTa-base.\ndown-stream NLP task, e.g., sentiment analysis, has a data domain that is generally a sub-domain of\nthe pre-training corpora, while ˆxi can be signiﬁcantly off the pre-training data manifold.\nObjective Function of Robust Informative Fine-Tuning (RIFT):The objective function of RIFT\nis a combination of Lr-task and Lr-info, deﬁned as follows:\nmin\nθ,φ,ϕ\nLr-task +αLr-info, (11)\nwhere the hyper-parameter αcontrols to what extent we encourage an objective model to absorb\ninformation from the pre-trained one (ablation on αcan be seen in Sec. 3.5). We summarize the\nwhole training process in Algorithm 1 and include more implementation details in Appendix A.1.\n3 Experiments\n3.1 Experimental Setting\nTasks and Datasets:We evaluate the robust accuracy and compare our method with the state-of-the-\narts on: (i) Sentiment analysis using the IMDB dataset [44]. (ii) Natural language inference using\nthe SNLI dataset [6]. We mainly focus on robustness against adversarial word substitutions [2, 58],\nas such attacks preserve the syntactic and semantics very well [ 31, 78, 12], and are very hard to\ndetect even by humans. Under this attack setting, any word in the input sequence can be substituted\nby a semantically similar word of it (often its synonym). We evaluate robustness on 1000 random\nexamples from the testset of IMDB and SNLI respectively following [31, 12].\nModel Architectures:We examine our methods and compare with state-of-the-arts on the following\ntwo prevailing pre-trained language models: (i) BERT-base-uncased [11]. (ii) RoBERTa-base [41].\nAttack Algorithms: Two powerful attacks are employed: (i) Genetic [ 2] based on population\nalgorithm. Aligned with [31, 12], the population size and iterations are set as 60 and 40 respectively.\n(ii) PWWS [58] based on word saliency. We only attack hypothesis on SNLI aligned with [31, 12].\nSubstitution Set: We follow [31, 12] to use the substitution set from [ 2], and the same language\nmodel constraint is applied to Genetic attacks and not to PWWS attacks.\n3.2 Comparative Methods\n(i) Standard Fine-Tuning:The standard ﬁne-tuning process ﬁrst initializes the objective model by\nthe pre-trained weight, and then use cross-entropy loss to ﬁne-tune the whole model.\n7\n(a) Accuracy (%) under Genetic attacks.\n (b) Accuracy (%) under PWWS attacks.\nFigure 4: Tradeoff curve between robustness and vanilla accuracy of BERT-based model on IMDB.\nTable 3: Accuracy(%) of RIFT with maximizing I(S; T/divides.alt0Y) and I(S; T) respectively.\nMaximizing Model Genetic PWWS\nI(S; T/divides.alt0Y) BERT 77.2 70.1\nI(S; T) BERT 76.1 69.4\nI(S; T/divides.alt0Y) RoBERTa 73.5 66.3\nI(S; T) RoBERTa 72.0 65.3\n(a) Accuracy (%) under attacks on IMDB.\nMaximizing Model Genetic PWWS\nI(S; T/divides.alt0Y) BERT 77.5 74.3\nI(S; T) BERT 76.6 72.1\nI(S; T/divides.alt0Y) RoBERTa 83.5 81.1\nI(S; T) RoBERTa 82.5 79.4\n(b) Accuracy (%) under attacks on SNLI.\n(ii) Adversarial Fine-Tuning Baseline (Adv-Base):We employ the state-of-the-art defense against\nword substitutions, ASCC-Defense [12], as the adversarial ﬁne-tuning baseline. This method is not\ninitially proposed for pre-trained language models but can readily extend to perform adversarial\nﬁne-tuning. During ﬁne-tuning, adversarial example ˆx, which is a sequence of convex combinations,\nis generated ﬁrst and then both xand ˆxare used for optimization using objective deﬁned in Eq. 9.\n(iii) Adv + Pre-Trained Weight Decay (Adv-PTWD):Pre-trained weight decay [8, 10] penalizes\nλ/parallel.alt1Wobj −Wpre/parallel.alt12, and mitigates catastrophic forgetting [75, 37]. We combine it with the adversarial\nbaseline for comparisons and λis chosen as 0.01 on IMDB and 0.005 on SNLI for best robustness.\n(iv) Adv + Mixout (Adv-Mixout):Motivated by Dropout [62] and DropConnect [73], Mixout [37]\nis proposed to addresses catastrophic forgetting in ﬁne-tuning. At each iteration each parameter\nis replaced by its pre-trained counter-part with probability m. We combine it with the adversarial\nﬁne-tuning baseline for comparisons with our method and mis chosen as 0.6 for best robustness.\n(v) Robust Informative Fine-Tuning (RIFT):The proposed adversarial ﬁne-tuning method. It can\nbe deemed as the adversarial ﬁne-tuning baseline plus the Lr-info term. We set τ as 0.2 for all score\nfunctions fy. For best robust accuracy, αis chosen as 0.1 and 0.7 on IMDB and SNLI respectively.\nAblation study on αcan be seen in Sec. 3.5.\nFor fair comparisons, all compared adversarial ﬁne-tuning methods use the same β on a same\ndataset, i.e., β = 10 on IMDB and β = 5 on SNLI, both of which are chosen for the best robust\naccuracy. Early stopping [60] is used for all compared methods according to best robust accuracy\nMore implementation details and runtime analysis can be found in Appendix A.1 and A.3.\n3.3 Main Result\nIn this section we compare our method with state-of-the-arts by robustness under attacks. As shown\nin Tables 1 and 2., RIFT consistently achieves the best robust performance among state-of-the-arts in\nall datasets across different pre-trained language models under all attacks. For instance, on IMDB our\nmethod outperforms the RoBERTa-based runner-up method by2.9% under Genetic attacks and 1.9%\nunder PWWS attacks. On SNLI based on BERT, we surpass the runner-up method by1.2% under\nGenetic attacks and 1.1% under PWWS attacks. In addition, RIFT consistently improves robustness\nupon the adversarial ﬁne-tuning baseline, while the improvements by other methods are not stable.\nWe observe that on SNLI, we surpass the runner-up by a relatively small margin compared to IMDB.\nThis may relate to the dataset property: input from SNLI has a smaller attack space (on average 6.54\n8\nTable 4: Ablation study on the hyper-parameter α.\nParameter α Vanilla Genetic PWWS\n0.00 78.1 74.8 68.3\n0.05 78.4 76.5 69.2\n0.10 78.3 77.2 70.1\n0.30 78.3 76.2 69.5\n(a) Acc (%) of RIFT based on BERT on IMDB.\nParameter α Vanilla Genetic PWWS\n0.00 79.4 75.7 72.9\n0.50 80.0 76.9 73.8\n0.70 80.4 77.5 74.3\n1.00 80.6 77.3 73.2\n(b) Acc (%) of RIFT based on BERT on SNLI.\ncombinations of substitutions on SNLI compared to 6108 on IMDB), and thus smaller absolute space\nleft for improvement. For results of vanilla accuracy please refer to Appendix A.4.\n3.4 Conditional Mutual Information Does Fit a Down-Stream Task Better\nIn this section we empirically validate that maximizing I(S; T/divides.alt0Y) cooperates with a down-stream\ntask better than maximizing I(S; T) does. We plot two sets of results of our method with max-\nimizing I(S; T/divides.alt0Y) and I(S; T) respectively (using similar noise contrastive loss with all other\nhyper-parameters the same). As shown in Table 3, RIFT with maximizing I(S; T/divides.alt0Y) consistently\noutperforms that with maximizing I(S; T) in terms of robustness under all attacks on both IMDB\nand SNLI, which serves as an empirical validation of Sec. 2.4.\n3.5 Ablation Study and Tradeoff Curve Between Robustness and Vanilla Accuracy\nIn this section we conduct ablation study on α, which is the weight of Lr-info in Eq. 11. It aims\nto control to what extent the objective model absorbs information from the pre-trained one. As\nshown in Table 4, a good value of αimproves both vanilla accuracy and robust accuracy; e.g., on\nIMDB increasing αfrom 0 to 0.1 results in increased vanilla accuracy by 0.2% and increased robust\naccuracy under Genetic attacks by 2.4%. This demonstrates that RIFT does motivate the objective\nmodel to retain robust and generalizable features that are beneﬁcial to both robustness and vanilla\naccuracy. When αgoes too large, the objective of ﬁne-tuning would focus too much on preserving as\nmuch information from the pre-trained model as possible, and thus ignores the down-stream task.\nOne may wonder whether informative ﬁne-tuning itself can improve the clean accuracy upon normal\nﬁne-tuning. The answer is yes; e.g., on IMDB using RoBERTa, informative ﬁne-tuning improves\nabout 0.3% clean accuracy upon normal ﬁne-tuning baseline. The improvement is not very signiﬁcant\nas normal ﬁne-tuning targets at vanilla input only and thus suffers less from the forgetting problem.\nOne common problem in contrastive loss is that the complexity grows quadratically with N, but\nlarger N contributes to tighter bound on the mutual information [49, 53]. One potential improvement\nis to maintain a dictionary updated on-the-ﬂy like [21] and we will leave it for future exploration.\nWe ﬁnally show the trade-off curve between robustness and vanilla accuracy in Figure 4. As shown,\nRIFT outperforms the state-of-the-arts in terms of both robust accuracy and vanilla accuracy. It again\nvalidates that RIFT indeed helps the objective model capture robust and generalizable information to\nimprove both robustness and vanilla accuracy, rather than trivially trading off one against another.\n4 Related Work\nPre-Trained Language Models:Language modeling aims at estimating the probabilistic density of\nthe textual data distribution. Classicial language modeling methods vary from CBOW, Skip-Gram\n[47], to GloVe [ 51] and ELMo [ 52]. Recently, Transformer [ 72] has demonstrated its power in\nlanguage modeling, e.g., GPT [55] and BERT [11]. The representations learned by deep pre-trained\nlanguage models are shown to be universal and highly generalizable [ 77] [41], and ﬁne-tuning\npre-trained language models becomes a paradigm in many NLP tasks [25, 11].\nAdversarial Robustness: DNNs have achieved success in many ﬁelds, but they are susceptible\nto adversarial examples [64]. In NLP, attacks algorithms include char-level modiﬁcations [24, 13,\n4, 17, 15, 54], sequence-level manipulations [ 29, 59, 30, 82], and adversarial word substitutions\n[2, 40, 58, 32, 80, 78]. For defenses, adversarial training [64, 19, 45] generates adversarial examples\non-the-ﬂy and is currently the most effective. Miyato et al. [48] ﬁrst introduce adversarial training to\n9\nNLP using L2-ball to model perturbations, and later more geometry-aligned methods like ﬁrst-order\napproximation[13], axis-aligned bound [31, 27], and convex hull [12], become favorable.\nFine-Tuning and Catastrophic Forgetting:The ﬁne-tuning of pre-trained language models [25, 11]\ncan be very unstable [11], as during ﬁne-tuning the objective model can deviate too much from the\npre-trained one, and easily over-ﬁts to small ﬁne-tuning sets [25]. Such phenomenon is referred as\ncatastrophic forgetting [46, 18] in ﬁne-tuning [25, 37, 81]. Methods to address catastrophic forgetting\ninclude pre-trained weight decay [8, 10, 81], learning rate decreasing [25], and Mixout regularization\n[37]. These methods focus on parameter space to constrain the distance between two models while\nour method addresses forgetting following an information-theoretical perspective. In continuous\nlearning, method like rehearsal-based [42, 56, 39, 1] and regularization-based [34, 14, 26] also aims\nat the forgetting problem, but under a different setting: they focus on balanced performance on\ntasks with previous data, while in our setting the pre-training corpora are not available and language\nmodeling is not our concern.\nContrastive Learning and Mutual Information:Contrastive learning [ 49, 23, 61] has demon-\nstrated its power in learning self-supervised representations [ 9, 21]. The contrastive loss such as\nInfoNCE[49], InfoMax [23], is proposed as maximizing the mutual information of features in dif-\nferent views, and further discussed in [53, 67, 69, 66, 74]. This paper shares the same perspective\nof information theory, but looks into a different problem, addressing forgetting for ﬁne-tuning.\nAddressing forgetting motivates our objective of maximizing the mutual information between the\noutputs of two models, which is different from conventional contrastive learning, and cooperation\nwith ﬁne-tuning motivates the use of conditional mutual information, which is quite different also.\nFischer and Alemi [16] propose to restrict information for robustness in vision domain. Restricting\ninformation to some extent helps a model ignore spurious features, but may be at the cost of ignoring\nrobust features as well. In contrast, the proposed method aims to retain the generic and robust features\nthat are already captured by a pre-trained language model, and thus better ﬁts a ﬁne-tuning scenario.\n5 Discussion and Conclusion\nIn this paper, we proposed RIFT to ﬁne-tune a pre-trained language model towards robust down-stream\nperformance. RIFT addresses the forgetting problem from an information-theoretical perspective and\nconsistently outperforms state-of-the-arts in our experiments. We hope that this work can contribute\nto the NLP robustness in general and thus to more reliable NLP systems in real-life applications.\nAcknowledgements\nThis work is supported by the Singapore Ministry of Education (MOE) Academic Research Fund\n(AcRF) Tier 1 (S21/20) and Tier 2.\nReferences\n[1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection\nfor online continual learning. In NeurIPS, 2019.\n[2] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei\nChang. Generating natural language adversarial examples. In EMNLP, 2018.\n[3] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of\nsecurity: Circumventing defenses to adversarial examples. In ICML, 2018.\n[4] Yonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine\ntranslation. In ICLR, 2018.\n[5] Ari S Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in\nfunction space. In ICML, 2019.\n[6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\nannotated corpus for learning natural language inference. In EMNLP, 2015.\n10\n[7] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In\nSP, 2017.\n[8] Ciprian Chelba and Alex Acero. Adaptation of maximum entropy capitalizer: Little data can\nhelp a lot. Computer Speech & Language, 20(4):382–399, 2006.\n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020.\n[10] Hal Daumé III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL, 2019.\n[12] Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards robustness against natural\nlanguage word substitutions. In ICLR, 2021.\n[13] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotﬂip: White-box adversarial\nexamples for text classiﬁcation. In ACL, 2018.\n[14] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-\nguided continual learning with bayesian neural networks. In ICLR, 2020.\n[15] Steffen Eger, Gözde Gül ¸ Sahin, Andreas Rücklé, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar,\nKrishnkant Swarnkar, Edwin Simpson, and Iryna Gurevych. Text processing like humans do:\nVisually attacking and shielding nlp systems. In NAACL, 2019.\n[16] Ian Fischer and Alexander A Alemi. Ceb improves model robustness. Entropy, 22(10):1081,\n2020.\n[17] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial\ntext sequences to evade deep learning classiﬁers. In SPW. IEEE, 2018.\n[18] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical\ninvestigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint\narXiv:1312.6211, 2013.\n[19] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-\nial examples. In ICLR, 2015.\n[20] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation\nprinciple for unnormalized statistical models. In AISTATS, 2010.\n[21] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 9729–9738, 2020.\n[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2015.\n[23] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,\nAdam Trischler, and Yoshua Bengio. Learning deep representations by mutual information\nestimation and maximization. In ICLR, 2019.\n[24] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. Deceiving google’s\nperspective api built for detecting toxic comments. arXiv preprint arXiv:1702.08138, 2017.\n[25] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-\ntion. In ACL, 2018.\n[26] Xinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, and Hanwang Zhang. Distilling\ncausal effect of data in class-incremental learning. In CVPR, 2021.\n[27] Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal,\nKrishnamurthy Dvijotham, and Pushmeet Kohli. Achieving veriﬁed robustness to symbol\nsubstitutions via interval bound propagation. In EMNLP, 2019.\n11\n[28] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and\nAleksander Madry. Adversarial examples are not bugs, they are features. arXiv preprint\narXiv:1905.02175, 2019.\n[29] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example genera-\ntion with syntactically controlled paraphrase networks. In NAACL, 2018.\n[30] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.\nIn EMNLP, 2017.\n[31] Robin Jia, Aditi Raghunathan, Kerem Göksel, and Percy Liang. Certiﬁed robustness to adver-\nsarial word substitutions. In EMNLP, 2019.\n[32] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language\nattack on text classiﬁcation and entailment. AAAI, 2020.\n[33] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\n[34] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\nOvercoming catastrophic forgetting in neural networks. Proceedings of the national academy of\nsciences, 114(13):3521–3526, 2017.\n[35] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical\nworld, 2016.\n[36] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In\nICLR, 2017.\n[37] Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to\nﬁnetune large-scale pretrained language models. In ICLR, 2020.\n[38] Yingzhen Li, John Bradshaw, and Yash Sharma. Are generative classiﬁers more robust to\nadversarial attacks? In ICML, 2019.\n[39] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern\nanalysis and machine intelligence, 40(12):2935–2947, 2017.\n[40] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text\nclassiﬁcation can be fooled. In IJCAI, 2018.\n[41] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[42] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.\nIn NeurIPS, 2017.\n[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n[44] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In ACL, 2011.\n[45] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In ICLR, 2018.\n[46] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:\nThe sequential learning problem. In Psychology of learning and motivation. Elsevier, 1989.\n[47] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed\nrepresentations of words and phrases and their compositionality. In NeurIPS, 2013.\n[48] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-\nsupervised text classiﬁcation. In ICLR, 2017.\n12\n[49] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748, 2018.\n[50] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and\nAnanthram Swami. The limitations of deep learning in adversarial settings. In EuroS&P. IEEE,\n2016.\n[51] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for\nword representation. In EMNLP, pages 1532–1543, 2014.\n[52] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. In NAACL-HLT, 2018.\n[53] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational\nbounds of mutual information. In ICML, 2019.\n[54] Danish Pruthi, Bhuwan Dhingra, and Zachary C Lipton. Combating adversarial misspellings\nwith robust word recognition. In ACL, 2019.\n[55] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving lan-\nguage understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-\nassets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf, 2018.\n[56] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:\nIncremental classiﬁer and representation learning. In CVPR, 2017.\n[57] Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce,\nand Been Kim. Visualizing and measuring the geometry of bert. In NeurIPS, 2019.\n[58] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial\nexamples through probability weighted word saliency. In ACL, 2019.\n[59] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically equivalent adversarial\nrules for debugging nlp models. In ACL, 2018.\n[60] Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning. In\nICML, 2020.\n[61] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NeurIPS,\n2016.\n[62] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. The journal of machine\nlearning research, 15(1):1929–1958, 2014.\n[63] David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and\ngeneralization. In CVPR, 2019.\n[64] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-\nfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2013.\n[65] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert re-discovers the classical nlp pipeline. In\nACL, 2019.\n[66] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In\nICLR, 2020.\n[67] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What\nmakes for good views for contrastive learning. In NeurIPS, 2020.\n[68] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks\nto adversarial example defenses. In NeurIPS, 2020.\n[69] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On\nmutual information maximization for representation learning. In ICLR, 2020.\n13\n[70] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.\nRobustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.\n[71] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\n[72] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[73] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural\nnetworks using dropconnect. In ICML, 2013.\n[74] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through\nalignment and uniformity on the hypersphere. In ICML, 2020.\n[75] Georg Wiese, Dirk Weissenborn, and Mariana Neves. Neural domain adaptation for biomedical\nquestion answering. In CoNLL, 2017.\n[76] Eric Wong, Frank R Schmidt, and J Zico Kolter. Wasserstein adversarial examples via projected\nsinkhorn iterations. In ICML, 2019.\n[77] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS,\n2019.\n[78] Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong\nSun. Word-level textual adversarial attacking as combinatorial optimization. In ACL, 2020.\n[79] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I.\nJordan. Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.\n[80] Huangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li. Generating ﬂuent adversarial examples\nfor natural languages. In ACL, 2019.\n[81] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting\nfew-sample bert ﬁne-tuning. In ICLR, 2021.\n[82] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. In\nICLR, 2018.\nA Appendix\nA.1 Implementation Details\nTextual sequence processing.For consistent word numbers per input under word substitution attacks,\nwe seperate word-level tokens by space and punctuations, and then follow the original tokenizer\nof BERT/RoBERTa to tokenize the input sequence. The byte-level RoBERTa tokenizer is further\nmodiﬁed to output one token per word to ﬁt the setting of word substitution attacks. The maximum\nnumber of tokens including special tokens per input is set as 300 for IMDB, and 80 for SNLI.\nHyper-parameters and optimization details.We set αas 0.1 for IMDB and 0.7 for SNLI. τ is set\nas 0.2 for both IMDB and SNLI. Other hyper-parameters are set as the same among all compared\nmethods for fair comparisons. For both standard ﬁne-tuning and adversarial ﬁne-tuning, we run for\n20 epochs with batch size 32 for IMDB, and run for 20 epochs with batch size 120 for SNLI. Early\nstopping is used for all compared methods according to best robust accuracy. AdamW optimizer is\nemployed with learning rate of 0.00002. We do not apply weight decay on an objective model, and\nset weight decay rate as 0.0002 for task-speciﬁc layers.\nModel architectures.For both BERT and RoBERTa, the representation with respect to the sequence\nclassiﬁcation token of the last layer is employed as the output feature, which is later taken as the\ninput of the task-speciﬁc layers for predictions. The task-speciﬁc layer is a MLP that has two linear\nlayers with relu activation after the ﬁrst layer and softmax after the second one.\n14\nTable 5: Vanilla Accuracy(%) of different ﬁne-tuning methods on IMDB.\nMethod Model Vanilla Accuracy\nStandard BERT 93.1\nAdv-Base BERT 74.6\nAdv-PTWD BERT 76.6\nAdv-Mixout BERT 77.8\nRIFT BERT 78.3\n(a) Accuracy (%) based on BERT-base-uncased.\nMethod Model Vanilla Accuracy\nStandard RoBERTa 94.9\nAdv-Base RoBERTa 80.1\nAdv-PTWD RoBERTa 80.7\nAdv-Mixout RoBERTa 79.0\nRIFT RoBERTa 84.2\n(b) Accuracy (%) based on RoBERTa-base.\nTable 6: Vanilla Accuracy(%) of different ﬁne-tuning methods on SNLI.\nMethod Model Vanilla Accuracy\nStandard BERT 89.2\nAdv-Base BERT 79.4\nAdv-PTWD BERT 78.4\nAdv-Mixout BERT 79.3\nRIFT BERT 80.5\n(a) Accuracy (%) based on BERT-base-uncased.\nMethod Model Vanilla Accuracy\nStandard RoBERTa 91.3\nAdv-Base RoBERTa 87.1\nAdv-PTWD RoBERTa 85.9\nAdv-Mixout RoBERTa 87.1\nRIFT RoBERTa 87.9\n(b) Accuracy (%) based on RoBERTa-base.\nA.2 The Proof of Lemma 1\nThe loss Linfo is the categorical cross-entropy loss of identifying ti among {tj}N\nj=1, given si and\ny. Thus, the optimial efy(s,t) that minimizes Linfo is proportional to p(t/divides.alt0s,y)\np(t/divides.alt0y) (refer to [49] for more\ndetails). We then insert p(t/divides.alt0s,y)\np(t/divides.alt0y) into Linfo and get what follows:\nLinfo = E\ny∼pD(y)\n/bracketleft.alt1E\n{xi,y}N\ni=1\n[ 1\nN\nN\n/summation.disp\ni=1\n−log N\np(ti/divides.alt0si,y)\np(ti/divides.alt0y)\n∑N\nj=1\np(tj/divides.alt0si,y)\np(tj/divides.alt0y)\n]/bracketright.alt (12)\n= E\ny∼pD(y)\n/bracketleft.alt1E\n{xi,y}N\ni=1\n[ 1\nN\nN\n/summation.disp\ni=1\nlog 1\nN\np(ti/divides.alt0si,y)\np(ti/divides.alt0y) +∑N\nj≠i\np(tj/divides.alt0si,y)\np(tj/divides.alt0y)\np(ti/divides.alt0si,y)\np(ti/divides.alt0y)\n]/bracketright.alt (13)\n= E\ny∼pD(y)\n/bracketleft.alt1E\n{xi,y}N\ni=1\n[ 1\nN\nN\n/summation.disp\ni=1\nlog 1\nN(1 + p(ti/divides.alt0y)\np(ti/divides.alt0si,y)\nN\n/summation.disp\nj≠i\np(tj/divides.alt0si,y)\np(tj/divides.alt0y) )]/bracketright.alt(14)\n= E\ny∼pD(y)\n/bracketleft.alt1E\n{xi,y}N\ni=1\n[ 1\nN\nN\n/summation.disp\ni=1\nlog( 1\nN + N−1\nN E\nx∼pD(x/divides.alt0y)\n[p(tj/divides.alt0si,y)\np(tj/divides.alt0y) ] p(ti/divides.alt0y)\np(ti/divides.alt0si,y))]/bracketright.alt(15)\n= E\ny∼pD(y)\n/bracketleft.alt1E\n{xi,y}N\ni=1\n[ 1\nN\nN\n/summation.disp\ni=1\nlog( 1\nN + N−1\nN\np(ti/divides.alt0y)\np(ti/divides.alt0si,y))]/bracketright.alt (16)\n≥ E\ny∼pD(y)\n/bracketleft.alt1E\n{xi,y}N\ni=1\n[ 1\nN\nN\n/summation.disp\ni=1\nlog p(ti/divides.alt0y)\np(ti/divides.alt0si,y)]/bracketright.alt (17)\n= E\ny∼pD(y)\n/bracketleft.alt1E\nx∼pD(x/divides.alt0y)\n[−log p(t/divides.alt0s,y)\np(t/divides.alt0y) ]/bracketright.alt (18)\n=−I(S; T/divides.alt0Y). (19)\nEq. 16 to Eq. 17 is by Jensen’s inequality. As such, −Linfo is a lower bound on I(S; T/divides.alt0Y) and a\nlarger N makes the bound tighter. The speciﬁc design of the score function fy does not impact the\ncorrectness of Lemma 1: when −Linfo is maximized, −Linfo is still a lower bound on the mutual\ninformation term. However, if the capacity of fy is limited, the bound might be loose.\n15\nA.3 Runtime Analysis\nAll models are trained using the Nvidia A100 GPU and our implementation is based on PyTorch. As\nfor IMDB, it takes about 10 GPU hours to train a BERT or RoBERTa based model using RIFT. As\nfor SNLI, it takes about 40 GPU hours to train a BERT or RoBERTa based model using RIFT.\nA.4 Vanilla Accuracy\nwe here show the vanilla accuracy of each methods in Tabs. 5 and 6 as a supplement. As we can\nsee, RIFT surpasses all other adversarial ﬁne-tuning method in terms of vanilla accuracy. It again\nvalidates that RIFT does help retain the generalizable information learned before.\nA.5 License of Used Assets\nThe assets and the corresponding licenses are as follows. IMDB dataset [ 44]: Non-Commercial\nLicensing. SNLI dataset [6]: Creative Commons Attribution-ShareAlike 4.0 International License.\nGenetic attack [2]: MIT License. PWWS attack [58]: MIT License. Certiﬁed robustness [ 31]: MIT\nLicense. ASCC-defense [12]: MIT License.\n16"
}