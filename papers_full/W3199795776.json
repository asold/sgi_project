{
  "title": "Transformer-Unet: Raw Image Processing with Unet",
  "url": "https://openalex.org/W3199795776",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Sha, Youyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2017932134",
      "name": "Zhang Yonghong",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ji, Xuquan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2088483407",
      "name": "Hu Lei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962891704",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3034505102",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2161236525",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W3034562924",
    "https://openalex.org/W2136573752",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W1923115158",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3034954654",
    "https://openalex.org/W2963739249",
    "https://openalex.org/W2124592697",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2800507189",
    "https://openalex.org/W1745334888",
    "https://openalex.org/W3127751679"
  ],
  "abstract": "Medical image segmentation have drawn massive attention as it is important in biomedical image analysis. Good segmentation results can assist doctors with their judgement and further improve patients' experience. Among many available pipelines in medical image analysis, Unet is one of the most popular neural networks as it keeps raw features by adding concatenation between encoder and decoder, which makes it still widely used in industrial field. In the mean time, as a popular model which dominates natural language process tasks, transformer is now introduced to computer vision tasks and have seen promising results in object detection, image classification and semantic segmentation tasks. Therefore, the combination of transformer and Unet is supposed to be more efficient than both methods working individually. In this article, we propose Transformer-Unet by adding transformer modules in raw images instead of feature maps in Unet and test our network in CT82 datasets for Pancreas segmentation accordingly. We form an end-to-end network and gain segmentation results better than many previous Unet based algorithms in our experiment. We demonstrate our network and show our experimental results in this paper accordingly.",
  "full_text": "Transformer-Unet: Raw Image Processing with\nUnet\nYouyang Sha1 Yonghong Zhang1,2 Xuquan Ji1 Lei Hu1,2\n1Beijing Zoezen Robot Co Ltd 2Beihang University\nAbstract\nMedical image segmentation have drawn massive attention as it is\nimportant in biomedical image analysis. Good segmentation results can\nassist doctors with their judgement and further improve patients’ expe-\nrience. Among many available pipelines in medical image analysis, Unet\nis one of the most popular neural networks as it keeps raw features by\nadding concatenation between encoder and decoder, which makes it still\nwidely used in industrial ﬁeld. In the mean time, as a popular model\nwhich dominates natural language process tasks, transformer is now in-\ntroduced to computer vision tasks and have seen promising results in\nobject detection, image classiﬁcation and semantic segmentation tasks.\nTherefore, the combination of transformer and Unet is supposed to be\nmore eﬃcient than both methods working individually. In this article, we\npropose Transformer-Unet by adding transformer modules in raw images\ninstead of feature maps in Unet and test our network in CT82 datasets\nfor Pancreas segmentation accordingly. We form an end-to-end network\nand gain segmentation results better than many previous Unet based al-\ngorithms in our experiment. We demonstrate our network and show our\nexperimental results in this paper accordingly.\n1 Introduction\nConvolutional neural networks(CNN), as a popular backbone for many\ncomputer vision tasks, have reached state-of-the-art eﬃciency in many areas.\nFor segmentation tasks, many works based on CNN(Dai et al., 2015; Badri-\nnarayanan et al, 2017) surpass previous methods based on statistic models\nsuch as Markov Random Fields(MRFs) and Conditional Random Fields(CRFs).\nHowever, both CNN and statistic models have their own edges since CNN can\ncapture regional features with shared parameters and sliding boxes and statistic\nmodels are solid for global information as they establish connections between\nevery pixel pair. So the combination of CNN and statistic models are extremely\nuseful in segmentation as many previous works(Liu et al., 2018; Chen et al.,\n2018) have proven.\n1\narXiv:2109.08417v1  [eess.IV]  17 Sep 2021\nTransformer-Unet: Raw Image Processing with Unet\nTransformer was ﬁrst proposed by Vaswani et al.(2017) and it soon be-\ncame a popular model in natural language process(NLP) tasks as it can pay\nattention to the whole sequence of array without losing any useful information\nlike recurrent neural networks(RNN). Many works(Parmar et al., 2018; Carion\net al., 2020) have introduced transformer to computer vision tasks by dividing\npictures or feature maps into diﬀerent parts and forming sequences of array ac-\ncordingly. These methods naturally perform better in capturing global features\nas transformer can establish relations between diﬀerent arrays in a sequence.\nHowever, both models have their shortcomings as CNN is not sensetive to\nglobal features and transformer costs a lot of computational amount and cannot\ncapture regional features eﬃciently. Dosovitskiy et al.(2021) further mention\nthe importance of the size of datasets as transformer model is naturally hard\nto converge. Therefore, the combination of CNN and transformer is suited for\nmany segmentation tasks(Zheng et al, 2021) as models combining CNN and\ntransformer can pay attention to regional and global features.\nAs an alternative choice for statistical models, transformer can improve\nthe utilities of plain CNN. So we propose adding transformer layers in classical\nsegmentation network Unet(Ronneberger et al., 2015) and forms an end-to-end\nnetwork w.r.t. semantic segmentation. Although previous work have combined\ntransformer and Unet (Chen et al., 2021), we argue transformer can be a more\nstraightforward part in Unet structure since previous work process feature maps\nwith transformer instead of raw images. We argue that this may not be eﬃcient\nsince pixel-wise relations have been formed by CNN and capturing relations\nbetween diﬀerent patches can also be done by CNN with bigger kernel size in\nthis case, which means this method does not highlight the advantage of trans-\nformer directly. Because raw images are important in medical image analysis,\nwe propose processing inputs directly with transformer and decode its output\nstep-by-step following Zheng et al.(2021). We form our transformer layers like\nDosovitskiy et al.(2021) and follow the design of Unet(Ronneberger et al., 2015)\nby concatenating feature maps from transformer and CNN in decoders. We\nname our network TUnet(Transformer and Unet) which surpass Unet, Atten-\ntion Unet and TransUnet in CT82 datasets for Pancreas segmentation in our\nexperiment. Our model can run in a fast speed despite the massive size of\ntransformer model based on multi-layer perceptron(MLP) and it can be trained\neasily in a computer with modern GPUs.\n2 Related Work\nCNN are extremely successful in computer vision(CV) tasks since the pro-\nposal of AlexNet(Krizhevsky et al., 2014) and they dominate machine learn-\ning models in domains such as object detection(Redmon et al., 2016) and hu-\nman joint point detection(Cao et al., 2021). CNN models nowadays are very\ndeep(Simonyan and Zisserman, 2015; He et al., 2016), they usually contains\nmillions of trainable parameters and multiple layers. Therefore, CNN models\nare typically more eﬃcient than statistical models such as Markov Random\n2\nTransformer-Unet: Raw Image Processing with Unet\nFields(MRFs), resulting in better performance in many areas. The majority\nof state-of-the-art algorithms in CV use CNN as their backbone and some of\nthem(Zheng et al., 2015) also apply diﬀerent network designs such as recurrent\nneural network(RNN).\nSemantic Segmentationtask means signing labels to every pixel in an\nimage. It majorly relies on CNN after Long et al.(2015) proposed fully convo-\nlutional networks. Two major types of work have been done in segmentation\nresearch. The ﬁrst is proposing new methods for encoding and decoding(Chen\net al., 2016; Noh et al., 2016; Wang et al., 2017). These methods optimize fea-\nture capturing process in CNN and enable deep nerual networks understanding\nimages better. The second is combining CNN with statistical models(Liu et al.,\n2016; Chen et al., 2018). These methods modify outputs of CNN for better\npredictions like transfer learning. Semantic segmentation have gained amaz-\ning results with these eﬀorts and they are also helpful for object segmentation\ntasks(He et al., 2017).\nAttention modules were ﬁrst introduced in NLP tasks as an alternative\nchoice for RNN since self-attention can capture relationships between diﬀer-\nent parts of a sequence without losing information. Currently attention have\nbeen introduced in CV(Wang et al., 2017; Oktay et al., 2018) and it can ex-\ntract suitable information together with CNN. As a model based on attention,\nTransformer was ﬁrst proposed by Vaswani et al.(2017) as a major approach for\nmachine translation and it soon became a dominant model after the proposal of\nBERT(Devlin et al., 2018). Currently, it has been introduced as a new model\nfor image recognition(Dosovitskiy et al., 2021), semantic segmentation(Zheng\net al., 2021) and many other CV tasks(Parmar et al., 2018; Carion et al., 2020).\nBoth CNN and transformer have their own edges so recent works such as\nTransUnet(Chen et al., 2021) apply them together by building a CNN pipeline\nwith the combination of transformer modules. These methods make CNN mod-\nels more capable for global feature generation compared with plain CNN models.\nSome of transformer based neural network models have reached state-of-the-art\neﬃciency in some datasets such as ViT(Dosovitskiy et al., 2021). These works\nsuggest that transformer, together with its combination of CNN, has great po-\ntential in CV areas. Despite previous success of transformer and CNN, little\nmodels focus on how transformer can act in raw images since it is computa-\ntionally costly. Many transformer-CNN models(Carion et al., 2020; Chen et\nal., 2021) only apply transformer in a feature maps as they are smaller in size\nand this may reduce computational amount. However, we believe this limit the\nperformance of neural networks as relations between diﬀerent patches is more\nclear in raw images so we apply transformer in raw images directly. Our method\ngains positive results in our experiment.(see section 5.)\n3 Method\nFollowing Ronneberger et al.(2015), we ﬁrst make a typical Unet as our\nCNN structure with bilinear interpolation as our up sample method and max-\n3\nTransformer-Unet: Raw Image Processing with Unet\nFigure 1: Transformer and vision transformer, the positions of layer normaliza-\ntion layers are diﬀerent which makes ViT suits CV tasks better.\nimum pooling as our down sample method. For the convenience of implemen-\ntation, we design an almost symmetric network(see Section 5.2) which can be\neasily modiﬁed with attention modules (Oktay et al., 2018) and transformer\nmodules(Chen et al., 2021). In TUnet however, the encoder and decoder do not\nconnect directly which is explained beneath in this section.\nAs a model taking a sequence of arrays as input, transformer requires 1D\ndata for segmentation tasks. Therefore, with a C×H×W raw image, we ﬂatten\nit into arrays of C ×n2 dimensions where n×n is the size of image patches\nand HW\nn2 is the length of array sequences. We follow Dosovitskiy et al.(2021)\nby dividing a whole image into diﬀerent squared pieces and n is the length of\nsquare edges. To simplify the implementation process, in most cases we assume\nH = W and H,W can be divided evenly by n.\nWe form our transformer model like Dosovitskiy et al.(2021), which is\nnamed vision transformer(ViT) and is slightly diﬀerent from NLP transformer,\nas show in ﬁgure 1. ViT puts layer normalization before Multi-Head Attention\nand MLP as it ensures the input values are not too big to process. In addition\nto that, ViT keep the major design of Vaswani et al.(2017) such as Multi-Head\nSelf-Attention and MLP layers. Dosovitskiy et al.(2021) further add a learnable\narray for position embedding before feeding the whole sequence into transformer\nwhich is kept in our TUnet. We further modify ViT by replacing GELU with\n4\nTransformer-Unet: Raw Image Processing with Unet\nFigure 2: Our proposed network structure, part A is a transformer taking raw\nimage as input and part B is a classical Unet where decoder takes the output\nof transformer as its input.\nELU as our activation function in MLP layers of transformer as we observe that\nELU performs better in our experiment. ELU is seldomly used compared with\nRELU and GELU in transformer and it is deﬁned as:\nELU =\n{\nx,if x≥0\nαex −1,if x< 0 (1)\nwe argue that ELU is useful because negative values are as important as positive\nvalues in CT images. In our experiment, we set hyper parameter α to 1.\nWith methodology explained above, we form our transformer model with\nthe following equations:\nz0 = [x1E; x2E; ··· ; xnE] + Epos (2)\n5\nTransformer-Unet: Raw Image Processing with Unet\nz′\nl = MHA(LN(zl−1)) + zl−1 (3)\nzl = MLP(LN(z′\nl)) + z′\nl (4)\nwhere MHA represents Multi-Head Attention layers, LN stands for layer nor-\nmalization and x1,··· ,xn are image patches, l ∈{1,2,··· ,m}where m is the\nnumber of transformer layers. For raw image processing, we do the embedding\nprocess in ViT by applying a convolutional layer with kernel size 1 ×1 on the\nwhole image, as E in equation 2 indicates.\nTransformer is not as eﬃcient as CNN in capturing regional features so\nwe add an additional encoder following the design of Unet(Ronneberger et al.,\n2015) in TUnet. This encoder does not connect with decoder directly. Instead,\nit outputs feature maps with diﬀerent receptive ﬁelds and concatenate them\nwith feature maps in decoder, as demonstrate in ﬁgure 2. Our decoder takes\nthe output of transformer as its input, speciﬁcally, for transformer taking array\nsequences of size HW\nn2 ×Cn2, we reshape its output to size CHW\nn2 ×n×n and\nfeed it directly into decoder. By doing this, we ensure the input of decoder\ncontains information of diﬀerent image patches and is therefore better for our\nﬁnal prediction.\n4 Implementation\nSince we are processing raw images in TUnet, the sizes of original images\nand image patches are important because they determine the size of transformer\nmodel and its running speed. As we are choosing CT82 as our experimental\ndatasets in which high resolution CT slices of size 512 ×512 include, we choose\n16 ×16 as our image patches’ size and therefore construct a sequence of length\n1024. As a result, the input of decoder has a size of 1024 ×16 ×16 in our\nexperiment and we further reconstruct it into size 1 ×512 ×512 by bilinear\ninterpolation. We add concatenation part in our decoder following Ronneberger\net al.(2015) and build an encoder accordingly. To minimize our model while\nkeeping its eﬃciency, the numbers of attention heads and total layers in our\ntransformer module are 8 and 6.\n5 Experiment\n5.1 Loss\nTo evaluate our model by comparing with other algorithms, we pick the\nmost commonly used loss function in binary segmentation tasks, Binary Cross\nEntropy(BCE) loss, as our major optimization target. This loss function is\nsimple and it cannot reﬂect the relations between pixels in our ﬁnal predicted\nprobability maps, therefore, it can better demonstrate how our model connect\ndiﬀerent parts of pictures. Generally, BCE loss is deﬁned as:\nL= 1\nN\n∑\ni\n−(yi ·log(pi) + (1−yi) ·log(1 −pi)) (5)\n6\nTransformer-Unet: Raw Image Processing with Unet\nFigure 3: A is the output of ﬁrst layer in transformer while B is the output of\nlast layer. Transformer enables the network capturing global abstract features\nwith a few layers. The background is the original image.\nwhere N is the number of pixels, yi is the label of pixel i, pi is the prob-\nability that pixel i’s label is true in our ﬁnal prediction map. By deﬁnition is\nobviously that this function only calculates the loss of ﬁnal prediction pixel-by-\npixel instead of regionally.\n5.2 Setup\nPreviously, no work has processed raw biomedical images with transformer,\nso no pre-trained model is available for our CT82 datasets. Due to that, we re-\nplace the traditional pre-tranied and ﬁne-tuning pattern with end-to-end train-\ning in the whole model. Another reason is that for TUnet, the major feature\nextractor is transformer module, so we design our Unet encoder shallower than\ndecoder to decrease the size of our model and guarantee the performance of\ndecoder. This is also why we call our model ’almost symmetric’ in Section 3.\nThe same backbone is also used in Attention Unet and TransUnet.\nWe pick the developed self-adjusted optimizer AdamW(Loshchilov and\nHutter, 2019) with an initial learning rate of 10 −3. We train our model for\na total of 120 epochs and follow traditional stage training by decreasing our\nlearning rate by a half after 60 and 100 epochs. We also set the decay percent-\nage rate to 10−6 to avoid overﬁtting and train our model with a single NVIDIA\n3090 GPU(24GB graphic memory).\nThe original CT82 datasets contains 3D CT ﬁles and for models like trans-\nformer, it is better to train with single CT slice in segmentation tasks. This is\ndecided based on the following facts:\n7\nTransformer-Unet: Raw Image Processing with Unet\nnetwork mIOU Dice Score Pixel Accuracy Precision Recall\nUnet 0.8113 0.7689 0.9981 0.8249 0.7200\nAttn-Unet 0.8172 0.7777 0.9982 0.8346 0.7280\nTransUnet 0.7882 0.7330 0.9979 0.8379 0.6515\nTUnet 0.8301 0.7966 0.9983 0.8278 0.7676\nTable 1: Utilities of networks based on diﬀerent validation indexes\nnetwork Params Inference Time\nUnet 506.6MB 0.028s\nAttn-Unet 508.0MB 0.030s\nTransUnet 1.2GB 0.034s\nTUnet 548.6MB 0.041s\nTable 2: Size and speed of diﬀerent networks\n• The size of datasets is important for transformer(Dosovitskiy et al., 2021).\nBy processing CT slices instead of the whole CT series, we are able to\nenlarge the size of datasets.\n• Transformer cost a lot of graphic memory as it is based on MLP. So\ntransformer is more suitable for 2D images because it does not increase\nthe size of weight ﬁles massively.\nWe therefore process CT slices in our experiments and compare TUnet\nwith existing models Unet, Attention Unet and TransUnet. To make our model\nbetter process data, we divided the whole image matrix with 1024, which is the\napproximate maximum absolute value of all CT slices in the datasets.\n5.3 Results\nOur major evaluating method is multiple validation indexes including mIOU\nvalue and dice score of our ﬁnal prediction. CT82 datasets is seperated to 60/22\nfor training and testing. The lowest resolution is 16 ×16 in our model and this\nalso applies to Unet, Attention Unet and TransUnet.\nFor results demonstration, we set threshold to 0.8(i.e. a pixel with value\nlarger than 0.8 in the ﬁnal prediction map will be considered as a Pancreas\npoint) and consider not only the accuracy of Pancreas segmentation but also\nthe recognition of background when calculating mIOU and pixel accuracy values.\nFigure 3 shows a major advantage of transformer and this enables our model\ndoing feature capturing job globally and locally with a few transformer layers.\nTable 1 shows the performance of Unet and its variances including our network.\nWith deep Unet model as backbone, our model is able to surpass Unet and its\nrelated networks including the popular Attention Unet. Table 2 shows the size\n8\nTransformer-Unet: Raw Image Processing with Unet\nFigure 4: Visual results of diﬀerent networks. Our positive results show that\nTUnet can make more precisive predictions when targets are far seperated.\nResolution mIOU Dice Score Pixel Accuracy Precision Recall\n16 ×16 0.8301 0.7966 0.9983 0.8278 0.7676\n32 ×32 0.8044 0.7584 0.9980 0.8140 0.7099\n64 ×64 0.8008 0.7529 0.9980 0.8404 0.6818\nTable 3: Utilities of networks with various resolution based on diﬀerent valida-\ntion indexes\nand inference time of diﬀerent models and our model does not increase the size\nand inference speed by a lot.\nFigure 4 shows the visual results of diﬀerent networks, our TUnet is able to\nmake good segmentation results for long-distance pixel pairs because of trans-\nformer and is therefore better than other previous networks based on Unet.\n5.4 Model Variance\nIn our experiment we choose n= 16 as the size of image patches. However,\nmany other options are available which suggest 16 may not be the ideal value\nfor TUnet and we further perform experiments with n= 32,64 respectively.\nAnother important feature of TUnet is the deep and large Unet backbone.\nHowever, both Unet and Attention Unet are still useful in shallow models. Since\ndeep models are not as convenient as shallow models because they naturally\nrequire better hardware such as GPUs, we further try our model with shallower\nUnet as backbone. In shallower model, we reduce 1\n3 CNN layers in Unet and\ndecrease ﬁlter numbers to 1\n4 of deep model. The whole model is still end-to-end\nand we still train the raw model throughly.\nOur results of variant models are listed in table 3 and table 4. From table 3\n9\nTransformer-Unet: Raw Image Processing with Unet\nnetwork mIOU Dice Score Pixel Accuracy Precision Recall\nUnet 0.8011 0.7532 0.9980 0.8362 0.6853\nAttn-Unet 0.8112 0.7686 0.9981 0.8414 0.7075\nTransUnet 0.7894 0.7348 0.9979 0.8503 0.6470\nTUnet 0.8078 0.7634 0.9981 0.8639 0.6839\nTable 4: Utilities of networks with shallow Unet backbone based on diﬀerent\nvalidation indexes\nwe can observe that for TUnet, 16×16 is the optimal resolution for transformer\nand high resolution may weaken the eﬃciency for transformer since the length\nof array sequences is decreasing at the same time which is essential for self-\nattention layers in transformer. From table 4 we observe that TUnet shows no\nobvious advantage when using shallow Unet as backbone. Therefore the abstract\nfeatures extracted by transformer may require deeper models for decoding.\n6 Conclusion\nIn this article we propose a transformer and Unet based nerual network for\nmedical image analysis. The transformer process raw image directly instead of\nextracted feature maps. Our model is able to surpass other Unet based methods\nwhen applying deep backbones. However, this method does not improve Unet’s\neﬃciency a lot when using shallow models and we may apply more modiﬁcation\nin the future for utilities improvement.\nAcknowledgement\nThis work is supported by Natural Science Foundation of Beijing, China\n(No.L202010).\n10\nReferences\n[1] Badrinarayanan, Kendall, and Cipolla, 2017. SegNet: A Deep Convolu-\ntional Encoder-Decoder Architecture for Image Segmentation. In: IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 12,\npp. 2481-2495.\n[2] Cao et al., 2021. OpenPose: Realtime Multi-Person 2D Pose Estimation\nUsing Part Aﬃnity Fields. In: IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 43, no. 1, pp. 172-186.\n[3] Carion et al., 2020. End-to-End Object Detection with Transformers. arXiv\npreprint arXiv:2005.12872.\n[4] Chen et al., 2021, TransUNet: Transformers Make Strong Encoders for\nMedical Image Segmentation. arXiv preprint arXiv:2102.04306.\n[5] Chen et al., 2018. DeepLab: Semantic Image Segmentation with Deep Con-\nvolutional Nets, Atrous Convolution, and Fully Connected CRFs. In: IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 4,\npp. 834–848.\n[6] Chen et al., 2016. Attention to Scale: Scale-Aware Semantic Image Segmen-\ntation. In: IEEE Conference on Computer Vision and Pattern Recognition,\npp. 3640-3649.\n[7] Chen et al., 2015. Semantic Image Segmentation with Deep Convolutional\nNets and Fully Connected CRFs. In: International Conference on Learning\nRepresentations.\n[8] Dai, He, and Sun, 2015. Convolutional feature masking for joint object and\nstuﬀ segmentation. In: IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3992-4000.\n[9] Devlin et al., 2018. BERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding. In: North American Chapter of the Associa-\ntion for Computational Linguistics.\n[10] Dosovitskiy et al., 2021. An Image Is Worth 16 ×16 Words: Transformers\nFor Image Recognition At Scale. In: International Conference on Learning\nRepresentations.\n[11] He et al., 2017. Mask R-CNN. In: IEEE International Conference on Com-\nputer Vision, pp. 2980-2988.\n[12] He et al., 2016. Deep Residual Learning for Image Recognition. In: IEEE\nConference on Computer Vision and Pattern Recognition, pp. 770-778.\n[13] Loshchilov and Hutter, 2019. Decoupled Weight Decay Regularization. In:\nInternational Conference on Learning Representations.\nTransformer-Unet: Raw Image Processing with Unet\n[14] Kr¨ ahenb¨ uhl and Koltun, 2011. Eﬃcient Inference in Fully Connected CRFs\nwith Gaussian Edge Potentials. In: Conference and Workshop on Neural\nInformation Processing Systems.\n[15] Krizhevsky, Sutskever, and Hinton, 2014. ImageNet Classiﬁcation with\nDeep Convolutional Neural Networks. In: Conference and Workshop on\nNeural Information Processing Systems.\n[16] Liu et al., 2020. Severity-Aware Semantic Segmentation With Reinforced\nWasserstein Training. In: IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 12563-12572.\n[17] Liu et al., 2018. Deep Learning Markov Random Field for Semantic Seg-\nmentation. In: IEEE Transactions on Pattern Analysis and Machine Intel-\nligence, vol. 40, no. 8, pp. 1814-1828.\n[18] Long, Shelhamer, and Darrell, 2015. Fully convolutional networks for se-\nmantic segmentation. In IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3431-3440.\n[19] Manzke et al., 2010. Automatic Segmentation of Rotational X-Ray Images\nfor Anatomic Intra-Procedural Surface Generation in Atrial Fibrillation\nAblation Procedures. In: IEEE Transactions on Medical Imaging, vol. 29,\nno. 2, pp. 260-272.\n[20] Noh, Hong and, Han, 2015. Learning Deconvolution Network for Semantic\nSegmentation. In: IEEE International Conference on Computer Vision, pp.\n1520-1528.\n[21] Oktay et al., 2018. Attention U-Net: Learning Where to Look for the\nPancreas. In: Medical Imaging with Deep Learning.\n[22] Pan et al., 2020. Unsupervised Intra-Domain Adaptation for Semantic Seg-\nmentation Through Self-Supervision. In: IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 3763-3772.\n[23] Parmar et al., 2018. Image Transformer. In: International Conference on\nMachine Learning.\n[24] Redmon et al., 2016. You Only Look Once: Uniﬁed, Real-Time Object\nDetection. In: IEEE Conference on Computer Vision and Pattern Recog-\nnition, pp. 779-788.\n[25] Ronneberger, Fischer, and Brox, 2015. U-net: Convolutional networks for\nbiomedical image segmentation. In: International Conference on Medical\nImage Computing and Computer Assisted Intervention.\n[26] Simonyan and Zisserman, 2015. Very Deep Convolutional Networks for\nLarge-Scale Image Recognition. In International Conference on Learning\nRepresentations.\n12\nTransformer-Unet: Raw Image Processing with Unet\n[27] Szegedy et al., 2015. Going deeper with convolutions. In: IEEE Conference\non Computer Vision and Pattern Recognition, pp. 1-9.\n[28] Teichmann and Cipolla, 2015. Convolutional CRFs for Semantic Segmen-\ntation. arXiv preprint arXiv:1805.047777.\n[29] Vaswani et al., 2017. Attention is All You Need. In: Conference and Work-\nshop on Neural Information Processing Systems.\n[30] Wang et al., 2017. Residual Attention Network for Image Classiﬁcation.\nIn: IEEE Conference on Computer Vision and Pattern Recognition, pp.\n6450-6458.\n[31] Zhang et al., 2020. Transferring and Regularizing Prediction for Semantic\nSegmentation. In: IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 9618-9627.\n[32] Zhang, Brady, and Smith, 2001. Segmentation of brain MR images through\na hidden Markov random ﬁeld model and the expectation-maximization\nalgorithm. In: IEEE Transactions on Medical Imaging, vol. 20, no. 1, pp.\n45-57.\n[33] Zhao et al., 2017. Pyramid Scene Parsing Network. In: IEEE Conference\non Computer Vision and Pattern Recognition, pp. 6230-6239.\n[34] Zheng et al., 2021. Rethinking Semantic Segmentation from a Sequence-to-\nSequence Perspective with Transformers. arXiv preprint arXiv:2012.15840.\n[35] Zheng et al., 2015. Conditional Random Fields as Recurrent Neural Net-\nworks. In: IEEE International Conference on Computer Vision, pp. 1529-\n1537.\n[36] Zhou et al., 2018. UNet++: A Nested U-Net Architecture for Medical Im-\nage Segmentation. In: Deep Learning in Medical Image Analysis Workshop.\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7539207935333252
    },
    {
      "name": "Segmentation",
      "score": 0.6564414501190186
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6363897323608398
    },
    {
      "name": "Transformer",
      "score": 0.5590775012969971
    },
    {
      "name": "Image segmentation",
      "score": 0.5236687064170837
    },
    {
      "name": "Encoder",
      "score": 0.4981241226196289
    },
    {
      "name": "Computer vision",
      "score": 0.46055567264556885
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3382244110107422
    },
    {
      "name": "Engineering",
      "score": 0.12728086113929749
    },
    {
      "name": "Voltage",
      "score": 0.09636110067367554
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}