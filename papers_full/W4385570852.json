{
    "title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling",
    "url": "https://openalex.org/W4385570852",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5033014552",
            "name": "Yu Song",
            "affiliations": [
                "Université de Montréal"
            ]
        },
        {
            "id": "https://openalex.org/A5013678286",
            "name": "Santiago Miret",
            "affiliations": [
                "Intel (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5100691219",
            "name": "Bang Liu",
            "affiliations": [
                "Université de Montréal"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3032049803",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3172768590",
        "https://openalex.org/W3201869313",
        "https://openalex.org/W4281482733",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W3168090480",
        "https://openalex.org/W3208687975",
        "https://openalex.org/W4281476575",
        "https://openalex.org/W3025621567",
        "https://openalex.org/W4282935671",
        "https://openalex.org/W3094582681",
        "https://openalex.org/W3104578551",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W4287887163",
        "https://openalex.org/W2987972786",
        "https://openalex.org/W3029277393",
        "https://openalex.org/W4226293470",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W2966049804",
        "https://openalex.org/W2963842982",
        "https://openalex.org/W4248414713",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2984452801",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4229443452",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3105451204",
        "https://openalex.org/W2999645992",
        "https://openalex.org/W3034949140",
        "https://openalex.org/W3115677442",
        "https://openalex.org/W3127365350",
        "https://openalex.org/W3161820423",
        "https://openalex.org/W3099034701",
        "https://openalex.org/W2296024507",
        "https://openalex.org/W3175823016",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3154888152",
        "https://openalex.org/W4243782893",
        "https://openalex.org/W3134756482",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2971258845"
    ],
    "abstract": "We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks.Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on {pasted macro 'BENCHMARK'} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 3621–3639\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMatSci-NLP: Evaluating Scientific Language Models on Materials Science\nLanguage Tasks Using Text-to-Schema Modeling\nYu Song1,∗ Santiago Miret2,∗ Bang Liu1,†\n1University of Montreal / Mila - Quebec AI, 2Intel Labs\n{yu.song, bang.liu}@umontreal.ca\n{santiago.miret}@intel.com\nAbstract\nWe present MatSci-NLP, a natural language\nbenchmark for evaluating the performance of\nnatural language processing (NLP) models on\nmaterials science text. We construct the bench-\nmark from publicly available materials science\ntext data to encompass seven different NLP\ntasks, including conventional NLP tasks like\nnamed entity recognition and relation classifica-\ntion, as well as NLP tasks specific to materials\nscience, such as synthesis action retrieval which\nrelates to creating synthesis procedures for ma-\nterials. We study various BERT-based models\npretrained on different scientific text corpora on\nMatSci-NLP to understand the impact of pre-\ntraining strategies on understanding materials\nscience text. Given the scarcity of high-quality\nannotated data in the materials science domain,\nwe perform our fine-tuning experiments with\nlimited training data to encourage the general-\nize across MatSci-NLP tasks. Our experiments\nin this low-resource training setting show that\nlanguage models pretrained on scientific text\noutperform BERT trained on general text. Mat-\nBERT, a model pretrained specifically on mate-\nrials science journals, generally performs best\nfor most tasks. Moreover, we propose a uni-\nfied text-to-schema for multitask learning on\nMatSci-NLP and compare its performance with\ntraditional fine-tuning methods. In our analy-\nsis of different training methods, we find that\nour proposed text-to-schema methods inspired\nby question-answering consistently outperform\nsingle and multitask NLP fine-tuning methods.\nThe code and datasets are publicly available1.\n1 Introduction\nMaterials science comprises an interdisciplinary\nscientific field that studies the behavior, properties\nand applications of matter that make up materials\nsystems. As such, materials science often requires\n∗Equal contribution.\n† Corresponding author. Canada CIFAR AI Chair.\n1https://github.com/BangLab-UdeM-Mila/\nNLP4MatSci-ACL23\ndeep understanding of a diverse set of scientific dis-\nciplines to meaningfully further the state of the art.\nThis interdisciplinary nature, along with the great\ntechnological impact of materials advances and\ngrowing research work at the intersection of ma-\nchine learning and materials science (Miret et al.;\nPilania, 2021; Choudhary et al., 2022), makes the\nchallenge of developing and evaluating natural lan-\nguage processing (NLP) models on materials sci-\nence text both interesting and exacting.\nThe vast amount of materials science knowledge\nstored in textual format, such as journal articles,\npatents and technical reports, creates a tremendous\nopportunity to develop and build NLP tools to cre-\nate and understand advanced materials. These tools\ncould in turn enable faster discovery, synthesis and\ndeployment of new materials into a wide variety\nof application, including clean energy, sustainable\nmanufacturing and devices.\nUnderstanding, processing, and training lan-\nguage models for scientific text presents distinctive\nchallenges that have given rise to the creation of\nspecialized models and techniques that we review\nin Section 2. Additionally, evaluating models on\nscientific language understanding tasks, especially\nin materials science, often remains a laborious task\ngiven the shortness of high-quality annotated data\nand the lack of broad model benchmarks. As such,\nNLP research applied to materials science remains\nin the early stages with a plethora of ongoing re-\nsearch efforts focused on dataset creation, model\ntraining and domain specific applications.\nThe broader goal of this work is to enable the\ndevelopment of pertinent language models that can\nbe applied to further the discovery of new material\nsystems, and thereby get a better sense of how well\nlanguage models understand the properties and be-\nhavior of existing and new materials. As such, we\npropose MatSci-NLP, a benchmark of various NLP\ntasks spanning many applications in the materials\nscience domain described in Section 3. We utilize\n3621\nthis benchmark to analyze the performance of var-\nious BERT-based models for MatSci-NLP tasks\nunder distinct textual input schemas described in\nSection 4. Concretely, through this work we make\nthe following research contributions:\n• MatSci-NLP Benchmark: We construct the\nfirst broad benchmark for NLP in the materi-\nals science domain, spanning several different\nNLP tasks and materials applications. The\nbenchmark contents are described in Section 3\nwith a general summary and data sources pro-\nvided in Table 1. The processed datasets and\ncode will be released after acceptance of the\npaper for reproducibility.\n• Text-to-Schema Multitasking: We develop\na set of textual input schemas inspired by\nquestion-answering settings for fine-tuning\nlanguage models. We analyze the models’\nperformance on MatSci-NLP across those set-\ntings and conventional single and multitask\nfine-tuning methods. In conjunction with this\nanalysis, we propose a new Task-Schema in-\nput format for joint multitask training that in-\ncreases task performance for all fine-tuned\nlanguage models.\n• MatSci-NLP Analysis: We analyze the per-\nformance of various BERT-based models\npretrained on different scientific and non-\nscientific text corpora on the MatSci-NLP\nbenchmark. This analysis help us better un-\nderstand how different pretraining strategies\naffect downstream tasks and find that Mat-\nBERT (Walker et al., 2021), a BERT model\ntrained on materials science journals, gener-\nally performs best reinforcing the importance\nof curating high-quality pretraining corpora.\nWe centered our MatSci-MLP analysis on ex-\nploring the following questions:\nQ1 How does in-domain pretraining of language\nmodels affect the downstream performance on\nMatSci-NLP tasks? We investigate the per-\nformance of various models pretrained on dif-\nferent kinds of domain-specific text including\nmaterials science, general science and gen-\neral language (BERT (Devlin et al., 2018)).\nWe find that MatBERT generally performs\nbest and that language models pretrained on\ndiverse scientific texts outperform a general\nlanguage BERT. Interestingly, SciBERT (Belt-\nagy et al., 2019) often outperforms materials\nscience language models, such as MatSciB-\nERT (Gupta et al., 2022) and BatteryBERT\n(Huang and Cole, 2022).\nQ2 How do in-context data schema and multi-\ntasking affect the learning efficiency in low-\nresource training settings? We investigate\nhow several input schemas shown in Figure 1\nthat contain different kinds of information af-\nfect various domain-specific language mod-\nels and propose a new Task-Schema method.\nOur experiments show that our proposed Task-\nSchema method mostly performs best across\nall models and that question-answering in-\nspired schema outperform single task and mul-\ntitask fine-tuning settings.\n2 Background\nThe advent of powerful NLP models has enabled\nthe analysis and generation of text-based data\nacross a variety of domains. BERT (Devlin et al.,\n2018) was one of the first large-scale transformer-\nbased models to substantially advance the state-of-\nthe-art by training on large amounts of unlabeled\ntext data in a self-supervised way. The pretrain-\ning procedure was followed by task-specific fine-\ntuning, leading to impressive results on a variety of\nNLP task, such as named entity recognition (NER),\nquestion and answering (QA), and relation classifi-\ncation (Hakala and Pyysalo, 2019; Qu et al., 2019;\nWu and He, 2019). A significant collection of large\nlanguage models spanning millions to billions of\nparameters followed the success of BERT adopting\na similar approach of pretraining on vast corpora\nof text with task-specific fine-tuning to push the\nstate-of-the-art for in natural language processing\nand understanding (Raffel et al., 2020; Brown et al.,\n2020; Scao et al., 2022).\n2.1 Scientific Language Models\nThe success of large language models on gen-\neral text motivated the development of domain-\nspecific language models pretrained on custom text\ndata, including text in the scientific domain: SciB-\nERT (Beltagy et al., 2019), ScholarBERT (Hong\net al., 2022) and Galactica (Taylor et al., 2022)\nare pretrained on general corpus of scientific arti-\ncles; BioBERT (Lee et al., 2020), PubMedBERT\n(Gu et al., 2021), BioMegatron (Shin et al., 2020)\nand Sci-Five (Phan et al., 2021) are pretrained on\n3622\nvarious kinds of biomedical corpora; MatBERT\n(Walker et al., 2021), MatSciBERT (Gupta et al.,\n2022) are pretrained on materials science specific\ncorpora; and BatteryBERT (Huang and Cole, 2022)\nis pretrained on a corpus focused on batteries.\nConcurrently, several domain-specific NLP bench-\nmarks were established to assess language model\nperformance on domain-specific tasks, such as\nQASPER (Dasigi et al., 2021) and BLURB (Gu\net al., 2021) in the scientific domain, as well as\nPubMedQA (Jin et al., 2019), BioASQ (Balikas\net al., 2015), and Biomedical Language Under-\nstanding Evaluation (BLUE) (Peng et al., 2019)\nin the biomedical domain.\n2.2 NLP in Materials Science\nThe availability of openly accessible, high-quality\ncorpora of materials science text data remains\nhighly restricted in large part because data from\npeer-reviewed journals and scientific documents\nis usually subject to copyright restrictions, while\nopen-domain data is often only available in\ndifficult-to-process PDF formats (Olivetti et al.,\n2020; Kononova et al., 2021). Moreover, special-\nized scientific text, such as materials synthesis pro-\ncedures containing chemical formulas and reaction\nnotation, require advanced data mining techniques\nfor effective processing (Kuniyoshi et al., 2020;\nWang et al., 2022b). Given the specificity, com-\nplexity, and diversity of specialized language in\nscientific text, effective extraction and processing\nremain an active area of research with the goal\nof building relevant and sizeable text corpora for\npretraining scientific language models (Kononova\net al., 2021).\nNonetheless, materials science-specific language\nmodels, including MatBERT (Walker et al., 2021),\nMatSciBERT (Gupta et al., 2022), and Battery-\nBERT (Huang and Cole, 2022), have been trained\non custom-built pretraining dataset curated by dif-\nferent academic research groups. The pretrained\nmodels and some of the associated fine-tuning data\nhave been released to the public and have enabled\nfurther research, including this work.\nThe nature of NLP research in materials science\nto date has also been highly fragmented with many\nresearch works focusing on distinct tasks motivated\nby a given application or methodology. Common\nideas among many works include the prediction\nand construction of synthesis routes for a variety\nof materials (Mahbub et al., 2020; Karpovich et al.,\n2021; Kim et al., 2020), as well as the creation of\nnovel materials for a given application (Huang and\nCole, 2022; Georgescu et al., 2021; Jensen et al.,\n2021), both of which relate broader challenges in\nthe field of materials science.\n3 MatSci-NLP Benchmark\nThrough the creation of MatSci-NLP, we aim to\nbring together some of the fragmented data across\nmultiple research works for a wide-ranging ma-\nterials science NLP benchmark. As described in\nSection 2, the availability of sizeable, high-quality\nand diverse datasets remain a major obstacle in ap-\nplying modern NLP to advance materials science\nin meaningful ways. This is primarily driven by a\nhigh cost of data labeling and the heterogeneous\nnature of materials science. Given those challenges,\nwe created MatSci-NLP by unifying various pub-\nlicly available, high-quality, smaller-scale datasets\nto form a benchmark for fine-tuning and evaluating\nmodern NLP models for materials science appli-\ncations. MatSci-NLP consists of seven NLP tasks\nshown in Table 1, spanning a wide range of materi-\nals categories including fuel cells (Friedrich et al.,\n2020), glasses (Venugopal et al., 2021), inorganic\nmaterials (Weston et al., 2019; MatSciRE, 2022),\nsuperconductors (Yamaguchi et al., 2020), and syn-\nthesis procedures pertaining to various kinds of\nmaterials (Mysore et al., 2019; Wang et al., 2022a).\nSome tasks in MatSci-NLP had multiple source\ncomponents, meaning that the data was curated\nfrom multiple datasets (e.g. NER), while many\nwere obtained from a single source dataset.\nThe data in MatSci-NLP adheres to a standard\nJSON-based data format with each of the samples\ncontaining relevant text, task definitions, and an-\nnotations. These can in turn be refactored into\ndifferent input schemas, such as the ones shown\nin Figure 1 consisting of 1) Input: primary text\njointly with task descriptions and instructions, and\n2) Output: query and label, which we perform\nin our text-to-schema modeling described in Sec-\ntion 4. Next, we describe the tasks in MatSci-NLP\nin greater detail:\n• Named Entity Recognition (NER): The\nNER task requires models to extract summary-\nlevel information from materials science text\nand recognize entities including materials, de-\nscriptors, material properties, and applications\namongst others. The NER task predicts the\nbest entity type label for a given text span\n3623\nFigure 1: Example of different question-answering inspired textual input schemas (Task-Schema , Potential Choices,\nExample) applied on MatSci-NLP. The input of the language model includes the shared text (green) along with\nrelevant task details (blue for NER and orange for event extraction). The shared text can contain relevant information\nfor multiple tasks and be part of the language model input multiple times.\nTask Size\n(# Samples)\nMeta-Dataset\nComponents\nNamed Entity\nRecognition 112,191 4\nRelation\nClassification 25,674 3\nEvent Argument\nExtraction 6,566 2\nParagraph\nClassification 1,500 1\nSynthesis\nAction Retrieval 5,547 1\nSentence\nClassification 9,466 1\nSlot Filling 8,253 1\nTable 1: Collection of NLP tasks in the meta-dataset of\nthe MatSci-NLP Benchmark drawn from Weston et al.\n(2019); Friedrich et al. (2020); Mysore et al. (2019);\nYamaguchi et al. (2020); Venugopal et al. (2021); Wang\net al. (2022a); MatSciRE (2022).\nsi with a non-entity span containing a “null”\nlabel. MatSci-NLP contains NER task data\nadapted from Weston et al. (2019); Friedrich\net al. (2020); Mysore et al. (2019); Yamaguchi\net al. (2020).\n• Relation Classification: In the relation clas-\nsification task, the model predicts the most\nrelevant relation type for a given span pair\n(si, sj). MatSci-NLP contains relation classi-\nfication task data adapted from Mysore et al.\n(2019); Yamaguchi et al. (2020); MatSciRE\n(2022).\n• Event Argument Extraction: The event\nargument extraction task involves extracting\nevent arguments and relevant argument roles.\nAs there may be more than a single event for\na given text, we specify event triggers and\nrequire the language model to extract corre-\nsponding arguments and their roles. MatSci-\nNLP contains event argument extraction task\ndata adapted from Mysore et al. (2019); Yam-\naguchi et al. (2020).\n• Paragraph Classification: In the paragraph\nclassification task adapted from Venugopal\net al. (2021), the model determines whether a\ngiven paragraph pertains to glass science.\n• Synthesis Action Retrieval (SAR): SAR is\na materials science domain-specific task that\ndefines eight action terms that unambiguously\nidentify a type of synthesis action to describe\na synthesis procedure. MatSci-NLP adapts\nSAR data from Wang et al. (2022a) to ask\nlanguage models to classify word tokens into\npre-defined action categories.\n• Sentence Classification: In the sentence\n3624\nclassification task, models identify sentences\nthat describe relevant experimental facts based\non data adapted from Friedrich et al. (2020).\n• Slot Filling: In the slot-filling task, models\nextract slot fillers from particular sentences\nbased on a predefined set of semantically\nmeaningful entities. In the task data adapted\nfrom Friedrich et al. (2020), each sentence de-\nscribes a single experiment frame for which\nthe model predicts the slots in that frame.\nThe tasks contained in MatSci-NLP were se-\nlected based on publicly available, high-quality\nannotated materials science textual data, as well\nas their relevance to applying NLP tools to mate-\nrials science. Conventional NLP tasks (NER, Re-\nlation Classification, Event Argument Extraction,\nParagraph Classification, Sentence Classification)\nenable materials science researchers to better pro-\ncess and understand relevant textual data. Domain\nspecific tasks (SAR, Slot Filling) enable materials\nscience research to solve concrete challenges, such\nas finding materials synthesis procedures and real-\nworld experimental planning. In the future, we aim\nto augment to current set of tasks with additional\ndata and introduce novel tasks that address materi-\nals science specific challenges with NLP tools.\n4 Unified Text-to-Schema Language\nModeling\nAs shown in Figure 1, a given piece of text can in-\nclude multiple labels across different tasks. Given\nthis multitask nature of the MatSci-NLP bench-\nmark, we propose a new and unified Task-Schema\nmultitask modeling method illustrated in Figure 2\nthat covers all the tasks in the MatSci-NLP dataset.\nOur approach centers on a unified text-to-schema\nmodeling approach that can predict multiple tasks\nsimultaneously through a unified format. The\nunderlying language model architecture is made\nup of modular components, including a domain-\nspecific encoder model (e.g. MatBERT, MatSciB-\nERT, SciBERT), and a generic transformer-based\ndecoder, each of which can be easily exchanged\nwith different pretrained domain-specific NLP mod-\nels. We fine-tune these pretrained language models\nand the decoder with collected tasks in MatSci-\nNLP using the procedure described in Section 4.3.\nThe unified text-to-schema provides a more\nstructured format to training and evaluating lan-\nguage model outputs compared to seq2seq and text-\nto-text approaches (Raffel et al., 2020; Luong et al.,\n2015). This is particularly helpful for the tasks in\nMatSci-NLP given that many tasks can be refor-\nmulated as classification problems. NER and Slot\nFilling, for example, are classifications at the token-\nlevel, while event arguments extraction entails the\nclassification of roles of certain arguments. With-\nout a predefined schema, the model relies entirely\non unstructured natural language to provide the\nanswer in a seq2seq manner, which significantly\nincreases the complexity of the task and also makes\nit harder to evaluate performance. The structure\nimposed by text-to-schema method also simplifies\ncomplex tasks, such as event extraction, by en-\nabling the language model to leverage the structure\nof the schema to predict the correct answer. We\nutilize the structure of the schema in decoding and\nevaluating the output of the language models, as\ndescribed further in Section 4.3 in greater detail.\nMoreover, our unified text-to-schema approach\nalleviates error propagation commonly found in\nmultitask scenarios (Van Nguyen et al., 2022; Lu\net al., 2021), enables knowledge sharing across\nmultiple tasks and encourages the fine-tuned lan-\nguage model to generalize across a broader set of\ntext-based instruction scenarios. This is supported\nby our results shown in Section 5.2 showing text-\nto-schema outperforming conventional methods.\n4.1 Language Model Formulation\nThe general purpose of our model is to achieve\nmultitask learning by a mapping function ( f) be-\ntween input (x), output ( y), and schema ( s), i.e.,\nf(x, s) = y. Due to the multitasking nature of\nour setting, both inputs and outputs can originate\nfrom different tasks n, i.e. x = [xt1, xt2, ...xtn]\nand y = [ yt1, yt2, ...ytn], all of which fit un-\nder a common schema ( s). Given the presence\nof domain-specific materials science language,\nour model architecture includes a domain-specific\nBERT encoder and a transformer decoder. All\nBERT encoders and transformer decoders share\nthe same general architecture, which relies on a\nself-attention mechanism: Given an input sequence\nof length N, we compute a set of attention scores,\nA = softmax(QTK/(√dk)). Next, we compute\nthe weighted sum of the value vectors, O = AV ,\nwhere Q, K, and V are the query, key, and value\nmatrices, and dk is the dimensionality of the key\nvectors.\nAdditionally, the transformer based decoder dif-\n3625\nFigure 2: Unified text-to-schema method for MatSci-\nNLP text understanding applied across the seven tasks.\nThe language model includes a domain specific encoder,\nwhich can be exchanged in a modular manner, as well\nas a general language pretrained transformer decoder.\nfer from the domain specific encoder by: 1) Ap-\nplying masking based on the schema applied to\nensure that it does not attend to future positions\nin the output sequence. 2) Applying both self-\nattention and encoder-decoder attention to com-\npute attention scores that weigh the importance\nof different parts of the output sequence and in-\nput sequence. The output of the self-attention\nmechanism ( O1) and the output of the encoder-\ndecoder attention mechanism ( O2) are concate-\nnated and linearly transformed to obtain a new\nhidden state, H = tanh(Wo[O1; O2] + bo) with\nWo and bo being the weight and biases respec-\ntively. The model then applies a softmax to H\nto generate the next element in the output sequence\nP = softmax(WpH + bp) , where P is a proba-\nbility distribution over the output vocabulary.\n4.2 Text-To-Schema Modeling\nAs shown in Figure 1, our schema structures the\ntext data based on four general components: text,\ndescription, instruction options, and the predefined\nanswer schema.\n• Text specifies raw text from the literature that\nis given as input to the language model.\n• Description describes the task for a given text\naccording to a predefined schema containing\nthe task name and the task arguments.\n• Instruction Options contains the core expla-\nnation related to the task with emphasis on\nthree different types: 1) Potential choices of\nanswers; 2) Example of an input/output pair\ncorresponding to the task; 3) Task-Schema :\nour predefined answer schema illustrated in\nFigure 2.\n• Answer describes the correct label of each\ntask formatted as a predefined answer schema\nthat can be automatically generated based on\nthe data structure of the task.\n4.3 Language Decoding & Evaluation\nEvaluating the performance of the language model\non MatSci-NLP requires determining if the text\ngenerated by the decoder is valid and meaningful\nin the context of a given task. To ensure consis-\ntency in evaluation, we apply a constrained decod-\ning procedure consisting of two steps: 1) Filtering\nout invalid answers through the predefined answer\nschema shown in Figure 2 based on the structure\nof the model’s output; 2) Match the model’s predic-\ntion with the most similar valid class given by the\nannotation for the particular task. For example, if\nfor the NER task shown in Figure 1 the model’s pre-\ndicted token is “BaCl2 2H2O materials”, it will be\nmatched with the NER label of “material”, which\nis then used as the final prediction for computing\nlosses and evaluating performance. This approach\nessentially reformulates each task as a classification\nproblem where the classes are provided based on\nthe labels from the tasks in MatSci-NLP. We then\napply a cross-entropy loss for model fine-tuning\nbased on the matched label from the model output.\nThe matching procedure simplifies the language\nmodeling challenge by not requiring an exact match\nof the predicted tokens with the task labels. This in\nturns leads to a more comprehensible signal in the\nfine-tuning loss function.\n5 Evaluation and Results\nOur analysis focuses on the questions outlined in\nSection 1: 1) Studying the effectiveness of domain-\nspecific language models as encoders, and 2) An-\nalyzing the effect of different input schemas in\nresolving MatSci-NLP tasks. Concretely, we study\nthe performance of the language models and lan-\nguage schema in a low resource setting where we\n3626\nNLP Model Named Entity\nRecognition\nRelation\nClassification\nEvent Argument\nExtraction\nParagraph\nClassification\nSynthesis\nAction Retrieval\nSentence\nClassification\nSlot\nFilling\nOverall\n(All Tasks)\nMatSciBERT\n(Gupta et al., 2022)\n0.707±0.076\n0.470±0.092\n0.791±0.046\n0.507±0.073\n0.436±0.066\n0.251±0.075\n0.719±0.116\n0.623±0.183\n0.692±0.179\n0.484±0.254\n0.914±0.008\n0.660±0.079\n0.436±0.142\n0.194±0.062\n0.671±0.060\n0.456±0.042\nMatBERT\n(Walker et al., 2021)\n0.875±0.015\n0.630±0.047\n0.804±0.071\n0.513±0.138\n0.451±0.091\n0.288±0.066\n0.756±0.073\n0.691±0.188\n0.717±0.040\n0.549±0.091\n0.909±0.009\n0.614±0.134\n0.548±0.058\n0.273±0.051\n0.722±0.023\n0.517±0.041\nBatteryBERT\n(Huang and Cole, 2022)\n0.786±0.113\n0.472±0.150\n0.801±0.081\n0.466±0.111\n0.457±0.024\n0.277±0.034\n0.633±0.075\n0.610±0.046\n0.614±0.128\n0.419±0.149\n0.912±0.015\n0.684±0.095\n0.520±0.057\n0.224±0.073\n0.663±0.038\n0.456±0.048\nSciBERT\n(Beltagy et al., 2019)\n0.734±0.079\n0.497±0.091\n0.819±0.067\n0.545±0.119\n0.451±0.077\n0.276±0.080\n0.696±0.094\n0.546±0.243\n0.701±0.138\n0.516±0.217\n0.911±0.017\n0.617±0.143\n0.481±0.144\n0.224±0.010\n0.685±0.056\n0.460±0.044\nScholarBERT\n(Hong et al., 2022)\n0.168±0.067\n0.101±0.034\n0.428±0.148\n0.274±0.110\n0.489±0.083\n0.356±0.109\n0.663±0.032\n0.433±0.122\n0.322±0.260\n0.178±0.051\n0.906±0.007\n0.478±0.008\n0.296±0.085\n0.109±0.044\n0.468±0.028\n0.276±0.024\nBioBERT\n(Wada et al., 2020)\n0.715±0.031\n0.459±0.055\n0.797±0.092\n0.465±0.134\n0.488±0.036\n0.274±0.049\n0.675±0.144\n0.578±0.102\n0.647±0.140\n0.446±0.231\n0.915±0.021\n0.686±0.098\n0.452±0.114\n0.191±0.045\n0.670±0.061\n0.442±0.057\nBERT\n(Devlin et al., 2018)\n0.657±0.077\n0.461±0.058\n0.782±0.056\n0.494±0.061\n0.418±0.053\n0.225±0.091\n0.665±0.057\n0.532±0.194\n0.656±0.099\n0.515±0.067\n0.910±0.017\n0.633±0.133\n0.520±0.019\n0.257±0.022\n0.658±0.030\n0.439±0.021\nTable 2: Low-resource fine-tuning results applying unified Task-Schema setting for various BERT-based encoder\nmodels pretrained on different domain specific text data. For each model, the top line represents the micro-F1 score\nand the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence\ninterval of two standard deviations. We denote the best performing encoder model and those that outperform the\ngeneral language BERT according to the micro-f1 with orange shading with MatBERT and SciBERT performing\nbest on most tasks and ScholarBERT and general language BERT generally performing worst.\nperform fine-tuning on different pretrained BERT\nmodels with limited data from the MatSci-NLP\nbenchmark. This low-resource setting makes the\nlearning problem harder given that the model has\nto generalize on little amount of data. Moreover,\nthis setting approximates model training with very\nlimited annotated data, which is commonly found\nin materials science as discussed in Section 2. In\nour experiments, we split the data in MatSci-NLP\ninto 1% training subset and a 99% testing subset\nfor evaluation. None of the evaluated encoder mod-\nels were exposed to the fine-tuning data in advance\nof our experiments and therefore have to rely on\nthe knowledge acquired during their respective pre-\ntraining processes. We evaluate the results of our\nexperiments using micro-F1 and macro-F1 scores\nof the language model predictions on the test split\nof the MatSci-NLP that were not exposed during\nfine-tuning.\n5.1 How does in-domain pretraining of\nlanguage models affect the downstream\nperformance on MatSci-NLP tasks? (Q1)\nBased on the results shown in Table 2, we can\ngather the following insights:\nFirst, domain-specific pretraining affects model\nperformance. We perform fine-tuning on various\nmodels pretrained on domain-specific corpora in a\nlow-resource setting and observe that: i) MatBert,\nwhich was pretrained on textual data from materials\nscience journals, generally performs best for most\ntasks in the MatSci-NLP benchmark with SciBERT\ngenerally performing second best. The high perfor-\nmance of MatBERT suggests that materials science\nspecific pretraining does help the language models\nacquire relevant materials science knowledge. Yet,\nthe underperformance of MatSciBERT compared\nto MatBERT and SciBERT indicates that the cu-\nration of pretraining data does significantly affect\nperformance. ii) The importance of the pretraining\ncorpus is further reinforced by the difference in\nperformance between SciBERT and ScholarBERT,\nboth of which were trained on corpora of general\nscientific text, but show vastly different results. In\nfact, ScholarBERT underperforms all other models,\nincluding the general language BERT, for all tasks\nexcept event argument extraction where Scholar-\nBERT performs best compared to all other mod-\nels. iii) The fact that most scientific BERT models\noutperform BERT pretrained on general language\nsuggests that pretraining on high-quality scientific\ntext is beneficial for resolving tasks involving ma-\nterials science text and potentially scientific texts\nfrom other domains. This notion of enhanced per-\nformance on MatSci-NLP when pretraining on sci-\nentific text is further reinforced by the performance\nof BioBERT by Wada et al. (2020). BioBERT\noutperforms BERT on most tasks even though it\nwas trained on text from the biomedical domain\nthat has minor overlap with the materials science\ndomain. This strongly indicates that scientific lan-\nguage, regardless of the domain, has a significant\ndistribution shift from general language that is used\nto pretrain common language models.\n3627\nNLP Model Single Task Single Task Prompt MMOE No Explanations Potential Choices Examples Task-Schema\nMatSciBERT\n(Gupta et al., 2022)\n0.501±0.057\n0.320±0.078\n0.485±0.043\n0.238±0.017\n0.457±0.021\n0.228±0.038\n0.651±0.045\n0.438±0.052\n0.670±0.036\n0.435±0.061\n0.688±0.045\n0.463±0.040\n0.671±0.060\n0.456±0.042\nMatBERT\n(Walker et al., 2021)\n0.537±0.036\n0.330±0.063\n0.523±0.021\n0.267±0.014\n0.557±0.010\n0.301±0.006\n0.721±0.033\n0.514±0.045\n0.699±0.020\n0.478±0.032\n0.705±0.025\n0.470±0.029\n0.722±0.023\n0.517±0.041\nBatteryBERT\n(Huang and Cole, 2022)\n0.469±0.050\n0.288±0.055\n0.488±0.011\n0.241±0.009\n0.431±0.044\n0.200±0.022\n0.660±0.013\n0.450±0.031\n0.622±0.069\n0.423±0.039\n0.660±0.033\n0.416±0.054\n0.663±0.038\n0.456±0.048\nSciBERT\n(Beltagy et al., 2019)\n0.500±0.055\n0.300±0.080\n0.502±0.030\n0.248±0.015\n0.504±0.052\n0.275±0.031\n0.680±0.066\n0.458±0.060\n0.660±0.042\n0.435±0.061\n0.686±0.039\n0.460±0.042\n0.685±0.056\n0.460±0.044\nScholarBERT\n(Hong et al., 2022)\n0.472±0.137\n0.234±0.094\n0.429±0.258\n0.250±0.142\n0.367±0.075\n0.165±0.044\n0.461±0.016\n0.271±0.022\n0.513±0.041\n0.295±0.055\n0.467±0.019\n0.260±0.018\n0.468±0.028\n0.276±0.024\nBioBERT\n(Wada et al., 2020)\n0.487±0.059\n0.281±0.026\n0.488±0.032\n0.238±0.017\n0.360±0.007\n0.151±0.002\n0.663±0.044\n0.442±0.079\n0.587±0.022\n0.365±0.018\n0.632±0.040\n0.404±0.046\n0.670±0.061\n0.442±0.057\nBERT\n(Devlin et al., 2018)\n0.498±0.051\n0.266±0.044\n0.488±0.043\n0.239±0.011\n0.394±0.009\n0.166±0.008\n0.670±0.020\n0.440±0.052\n0.601±0.046\n0.382±0.039\n0.636±0.052\n0.394±0.051\n0.658±0.030\n0.439±0.021\nOverall\n(All Models)\n0.493±0.064\n0.288±0.063\n0.486±0.062\n0.246±0.032\n0.439±0.003\n0.212±0.022\n0.644±0.034\n0.430±0.049\n0.622±0.035\n0.402±0.049\n0.639±0.044\n0.410±0.043\n0.688±0.046\n0.435±0.039\nTable 3: Consolidated results among all MatSci-NLP tasks on different training settings for various BERT-based\nencoder models pretrained on different domain specific text data. For each model, the top line represents the\nmicro-F1 score and the bottom line represents the macro-F1 score. We report the mean across 5 experiments with\na confidence interval of two standard deviations. We highlight the performance of different schema according\nto heatmap ranging from best and worst. The concentration of red hues on right side indicates that the question-\nanswering inspiring schema generally outperform conventional fine-tuning method. Our proposed Task-Schema\ngenerally outperforms all other schemas across most enconder models.\nSecond, imbalanced datasets in MatSci-NLP\nskew performance metrics: We can see from Ta-\nble 2 that the micro-F1 scores are significantly\nhigher than the macro-f1 across all tasks. This\nindicates that the datasets used in the MatSci-NLP\nare consistently imbalanced, including in the binary\nclassification tasks, and thereby push the micro-F1\nhigher compared to the macro-F1 score. In the case\nof paragraph classification, for example, the num-\nber of positive examples is 492 compared with the\ntotal number of 1500 samples. As such, only mod-\nels with a micro-F1 score above 0.66 and macro-F1\nabove 0.5 can be considered to have semantically\nmeaningful understanding of the task. This is even\nmore pronounced for sentence classification where\nonly 876/9466 ≈10% corresponds to one label.\nAll models except ScholarBERT outperform a de-\nfault guess of the dominant class for cases. While\nimbalanced datasets may approximate some real-\nworld use cases of materials science text analysis,\nsuch as extracting specialized materials informa-\ntion, a highly imbalanced can be misguiding in\nevaluating model performance.\nTo alleviate the potentially negative effects of\nimbalanced data, we suggest three simple yet ef-\nfective methods: 1) Weighted loss functions: This\ninvolves weighting the loss function to give higher\nweights to minority classes. Focal loss (Lin et al.,\n2017), for example, is a loss function that dy-\nnamically modulates the loss based on the predic-\ntion confidence, with greater emphasis on more\ndifficult examples. As such, Focal loss handles\nclass imbalance well due to the additional atten-\ntion given to hard examples of the minority classes.\n2) Class-balanced samplers: Deep learning frame-\nworks, such as Pytorch, have class-balanced batch\nsamplers that can be used to oversample minority\nclasses within each batch during training, which\ncan help indirectly address class imbalance. 3)\nModel architecture tweaks: The model architec-\nture and its hyper-parameters can be adjusted to\nplace greater emphasis on minority classes. For\nexample, one can apply separate prediction heads\nfor minority classes or tweak L2 regularization and\ndropout to behave differently for minority and ma-\njority classes.\n5.2 How do in-context data schema and\nmultitasking affect the learning efficiency\nin low-resource training settings? (Q2)\nTo assess the efficacy of the proposed textual\nschemas shown in Figure 1, we evaluate four dif-\nferent QA-inspired schemas: 1) No Explanations -\nhere the model receives only the task description;\n2) Potential Choices - here the model receives the\nclass labels given by the task; 3) Examples - here\nthe model receives an example of a correct answer,\n4) Task-Schema - here the model receives our pro-\n3628\nposed textual schema. We compare the schemas to\nthree conventional fine-tuning methods: 1) Single\nTask - the traditional method to solve each task sep-\narately using the language model and a classifica-\ntion head; 2) Single Task Prompt - here we change\nthe format of the task to the same QA-format as\n“No Explanations”, but train each task separately;\n3) MMOE by Ma et al. (2018) uses multiple en-\ncoders to learn multiple hidden embeddings, which\nare then weighed by a task-specific gate unit and\naggregated to the final hidden embedding using a\nweighted sum for each task. Next, a task-specific\nclassification head outputs the label probability dis-\ntribution for each task.\nBased on the results shown in Table 3, we gather\nthe following insights:\nFirst, Text-to-Schema methods perform better\nfor all language models. Overall, the Task-Schema\nmethod we proposed performs best across all tasks\nin the MatSci-NLP benchmark. The question-\nanswering inspired schema (“No Explanations”,\n“Potential Choices”, “Examples”, “Task-Schema\n”) perform better than fine-tuning in a traditional\nsingle task setting, single task prompting, as well\nas fine-tuning using the MMOE multitask method.\nThis holds across all models for all the tasks in\nMatSci-NLP showing the efficacy of structured lan-\nguage modeling inspired by question-answering.\nSecond, schema design affects model perfor-\nmance. The results show that both the pretrained\nmodel and the input format affect performance.\nThis can be seen by the fact that while all scientific\nmodels outperform general language BERT using\nthe Task-Schema method, BERT outperforms some\nmodels, mainly ScholarBERT and BioBERT, in the\nother text-to-schema settings and the conventional\ntraining settings. Nevertheless, BERT underper-\nforms the stronger models (MatBERT, SciBERT,\nMatSciBERT) across all schema settings for all\ntasks in MatSci-NLP, further emphasizing the im-\nportance of domain-specific model pretraining for\nmaterials science language understanding.\n6 Conclusion and Future Works\nWe proposed MatSci-NLP, the first broad bench-\nmark on materials science language understand-\ning tasks constructed from publicly available data.\nWe further proposed text-to-schema multitask mod-\neling to improve the model performance in low-\nresource settings. Leveraging MatSci-NLP and\ntext-to-schema modeling, we performed an in-\ndepth analysis of the performance of various\nscientific language models and compare text-to-\nschema language modeling methods with other\ninput schemas, guided by (Q1) addressing the\npretrained models and (Q2) addressing the tex-\ntual schema. Overall, we found that the choice of\npretrained models matters significantly for down-\nstream performance on MatSci-NLP tasks and that\npretrained language models on scientific text of\nany kind often perform better than pretrained lan-\nguage models on general text. MatBERT gener-\nally performed best, highlighting the benefits of\npretraining with high-quality domain-specific lan-\nguage data. With regards to the textual schema\noutlined in (Q2), we found that significant improve-\nments can be made by improving textual schema\nshowcasing the potential of fine-tuning using struc-\ntured language modeling.\nThe proposed encoder-decoder architecture, as\nwell as the proposed multitask schema, could also\nbe useful for additional domains in NLP, includ-\ning both scientific and non-scientific domains. The\npotential for open-domain transferability of our\nmethod is due to: 1) Our multitask training method\nand associated schemas do not depend on any\ndomain-specific knowledge, allowing them to be\neasily transferred to other domains. 2) The en-\ncoder of our proposed model architecture can be\nexchanged in a modular manner, which enables\nour model structure to be applied across multiple\ndomains. 3) If the fine-tuning data is diverse across\na wide range of domains, our method is likely to\nlearn general language representations for open-\ndomain multitask problems. Future work could\nbuild upon this paper by applying the model and\nproposed schema to different scientific domains\nwhere fine-tuning data might be sparse, such as\nbiology, physics and chemistry. Moreover, future\nwork can build upon the proposed schema by sug-\ngesting novel ways of modeling domain-specific\nor general language that lead to improvements in\nunified multi-task learning.\n3629\nLimitations\nOne of the primary limitations of NLP modeling\nin materials science, including this work, is the\nlow quantity of available data as discussed in Sec-\ntion 2. This analysis is affected by this limitation\nas well given that our evaluations were performed\nin a low-data setting within a dataset that was al-\nready limited in size. We believe that future work\ncan improve upon this study by applying larger\ndatasets, both in the number of samples and in the\nscope of tasks, to similar problem settings. The\nsmall nature of the datasets applied in this study\nalso presents the danger that some of the models\nmay have memorized certain answers instead of\nachieving a broader understanding, which could be\nmitigated by enlarging the datasets and making the\ntasks more complex.\nMoreover, we did not study the generalization of\nNLP models beyond the materials science domain,\nincluding adjacent domains such as chemistry and\nphysics. This targeted focus was intentional but\nimposes limitations on whether the proposed tech-\nniques and insights we gained from our analysis are\ntransferable to other domains, including applying\nNLP models for scientific tasks outside of materials\nscience.\nAnother limitation of our study is the fact that we\nfocused on BERT-based models exclusively and did\nnot study autoregressive models, including large\nlanguage models with billions of parameters high-\nlighted in the introduction. The primary reason\nfor focusing on BERT-based models was the di-\nversity of available models trained on different sci-\nentific text corpora. Large autoregressive models,\non the other hand, are mostly trained on general\ntext corpora with some notable exceptions, such as\nGalactica (Taylor et al., 2022). We believe that fu-\nture work analyzing a greater diversity of language\nmodels, including large autoregressive models pre-\ntrained on different kinds of text, would signifi-\ncantly strengthen the understanding surrounding\nthe ability of NLP models to perform text-based\ntasks in materials science.\nWhile the results presented in this study indicate\nthat domain-specific pretraining can lead to notice-\nable advantages in downstream performance on\ntext-based materials science tasks, we would like\nto highlight the associated risks and costs of pre-\ntraining a larger set of customized language mod-\nels for different domains. The heavy financial and\nenvironmental costs associated with these pretrain-\ning procedures merit careful consideration of what\nconditions may warrant expensive pretraining and\nwhich ones may not. When possible, we encour-\nage future researchers to build upon existing large\nmodels to mitigate the pretraining costs.\nBroader Impacts and Ethics Statement\nOur MatSci-NLP benchmark can help promote the\nresearch on NLP for material science, an impor-\ntant and growing research field. We expect that\nthe experience we gained from the material sci-\nence domain can be transferred to other domains,\nsuch as biology, health, and chemistry. Our Text-\nto-Schema also helps with improving NLP tasks’\nperformance in low-resource situations, which is a\ncommon challenge in many fields.\nOur research does not raise major ethical con-\ncerns.\nAcknowlegments\nThis work is supported by the Mila internal funding\n- Program P2-V1: Industry Sponsored Academic\nLabs (project number: 10379), the Canada CIFAR\nAI Chair Program, and the Canada NSERC Discov-\nery Grant (RGPIN-2021-03115).\nReferences\nGeorgios Balikas, Anastasia Krithara, Ioannis Partalas,\nand George Paliouras. 2015. Bioasq: A challenge on\nlarge-scale biomedical semantic indexing and ques-\ntion answering. In International Workshop on Multi-\nmodal Retrieval in the Medical Domain, pages 26–39.\nSpringer.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. arXiv\npreprint arXiv:1903.10676.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nKamal Choudhary, Brian DeCost, Chi Chen, Anubhav\nJain, Francesca Tavazza, Ryan Cohn, Cheol Woo\nPark, Alok Choudhary, Ankit Agrawal, Simon JL\nBillinge, et al. 2022. Recent advances and applica-\ntions of deep learning methods in materials science.\nnpj Computational Materials, 8(1):1–26.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A Smith, and Matt Gardner. 2021. A dataset of\ninformation-seeking questions and answers anchored\nin research papers. arXiv preprint arXiv:2105.03011.\n3630\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAnnemarie Friedrich, Heike Adel, Federico Tomazic,\nJohannes Hingerl, Renou Benteau, Anika Marus-\ncyk, and Lukas Lange. 2020. The sofc-exp cor-\npus and neural approaches to information extrac-\ntion in the materials science domain. arXiv preprint\narXiv:2006.03039.\nAlexandru B Georgescu, Peiwen Ren, Aubrey R Toland,\nShengtong Zhang, Kyle D Miller, Daniel W Ap-\nley, Elsa A Olivetti, Nicholas Wagner, and James M\nRondinelli. 2021. Database, features, and machine\nlearning model to identify thermally driven metal–\ninsulator transition compounds. Chemistry of Mate-\nrials, 33(14):5591–5605.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nTanishq Gupta, Mohd Zaki, NM Krishnan, et al. 2022.\nMatscibert: A materials domain language model for\ntext mining and information extraction. npj Compu-\ntational Materials, 8(1):1–11.\nKai Hakala and Sampo Pyysalo. 2019. Biomedical\nnamed entity recognition with multilingual bert. In\nProceedings of the 5th workshop on BioNLP open\nshared tasks, pages 56–61.\nZhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon\nDuede, Carl Malamud, Roger Magoulas, Kyle Chard,\nand Ian Foster. 2022. Scholarbert: Bigger is not\nalways better. arXiv preprint arXiv:2205.11342.\nShu Huang and Jacqueline M Cole. 2022. Batterybert:\nA pretrained language model for battery database\nenhancement. Journal of Chemical Information and\nModeling.\nZach Jensen, Soonhyoung Kwon, Daniel Schwalbe-\nKoda, Cecilia Paris, Rafael Gómez-Bombarelli, Yuriy\nRomán-Leshkov, Avelino Corma, Manuel Moliner,\nand Elsa A Olivetti. 2021. Discovering relationships\nbetween osdas and zeolites through data mining and\ngenerative neural networks. ACS central science ,\n7(5):858–867.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nChristopher Karpovich, Zach Jensen, Vineeth Venu-\ngopal, and Elsa Olivetti. 2021. Inorganic synthesis re-\naction condition prediction with generative machine\nlearning. arXiv preprint arXiv:2112.09612.\nEdward Kim, Zach Jensen, Alexander van Grootel,\nKevin Huang, Matthew Staib, Sheshera Mysore,\nHaw-Shiuan Chang, Emma Strubell, Andrew Mc-\nCallum, Stefanie Jegelka, et al. 2020. Inorganic ma-\nterials synthesis planning with literature-trained neu-\nral networks. Journal of chemical information and\nmodeling, 60(3):1194–1201.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nOlga Kononova, Tanjin He, Haoyan Huo, Amalie Tre-\nwartha, Elsa A Olivetti, and Gerbrand Ceder. 2021.\nOpportunities and challenges of text mining in mate-\nrials research. Iscience, 24(3):102155.\nFusataka Kuniyoshi, Kohei Makino, Jun Ozawa, and\nMakoto Miwa. 2020. Annotating and extracting syn-\nthesis process of all-solid-state batteries from scien-\ntific literature. arXiv preprint arXiv:2002.07339.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\nand Piotr Dollár. 2017. Focal loss for dense object\ndetection. In Proceedings of the IEEE international\nconference on computer vision, pages 2980–2988.\nYaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong\nTang, Annan Li, Le Sun, Meng Liao, and Shaoyi\nChen. 2021. Text2event: Controllable sequence-to-\nstructure generation for end-to-end event extraction.\narXiv preprint arXiv:2106.09232.\nMinh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2015. Multi-task\nsequence to sequence learning. arXiv preprint\narXiv:1511.06114.\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan\nHong, and Ed H Chi. 2018. Modeling task relation-\nships in multi-task learning with multi-gate mixture-\nof-experts. In Proceedings of the 24th ACM SIGKDD\ninternational conference on knowledge discovery &\ndata mining, pages 1930–1939.\nRubayyat Mahbub, Kevin Huang, Zach Jensen,\nZachary D Hood, Jennifer LM Rupp, and Elsa A\nOlivetti. 2020. Text mining for processing conditions\nof solid-state battery electrolytes. Electrochemistry\nCommunications, 121:106860.\nMatSciRE. 2022. Material science relation extraction\n(matscire).\nSantiago Miret, Marta Skreta, Benjamin Sanchez-\nLengelin, Shyue Ping Ong, Zamyla Morgan-Chan,\nand Alan Aspuru-Guzik. Ai4mat - neurips 2022.\n3631\nSheshera Mysore, Zach Jensen, Edward Kim, Kevin\nHuang, Haw-Shiuan Chang, Emma Strubell, Jeffrey\nFlanigan, Andrew McCallum, and Elsa Olivetti. 2019.\nThe materials science procedural text corpus: Anno-\ntating materials synthesis procedures with shallow se-\nmantic structures. arXiv preprint arXiv:1905.06939.\nElsa A Olivetti, Jacqueline M Cole, Edward Kim, Olga\nKononova, Gerbrand Ceder, Thomas Yong-Jin Han,\nand Anna M Hiszpanski. 2020. Data-driven materials\nresearch enabled by natural language processing and\ninformation extraction. Applied Physics Reviews ,\n7(4):041317.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of bert and elmo on ten bench-\nmarking datasets. In Proceedings of the 2019 Work-\nshop on Biomedical Natural Language Processing\n(BioNLP 2019).\nLong N Phan, James T Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature. arXiv\npreprint arXiv:2106.03598.\nGhanshyam Pilania. 2021. Machine learning in ma-\nterials science: From explainable predictions to au-\ntonomous design. Computational Materials Science,\n193:110360.\nChen Qu, Liu Yang, Minghui Qiu, W Bruce Croft,\nYongfeng Zhang, and Mohit Iyyer. 2019. Bert with\nhistory answer embedding for conversational ques-\ntion answering. In Proceedings of the 42nd inter-\nnational ACM SIGIR conference on research and\ndevelopment in information retrieval , pages 1133–\n1136.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020. Biomegatron: Larger\nbiomedical domain language model. arXiv preprint\narXiv:2010.06060.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nMinh Van Nguyen, Bonan Min, Franck Dernoncourt,\nand Thien Nguyen. 2022. Joint extraction of entities,\nrelations, and events via modeling inter-instance and\ninter-label dependencies. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 4363–4374.\nVineeth Venugopal, Sourav Sahoo, Mohd Zaki, Man-\nish Agarwal, Nitya Nand Gosvami, and NM Anoop\nKrishnan. 2021. Looking through glass: Knowledge\ndiscovery from materials science literature using nat-\nural language processing. Patterns, 2(7):100290.\nShoya Wada, Toshihiro Takeda, Shiro Manabe, Shozo\nKonishi, Jun Kamohara, and Yasushi Matsumura.\n2020. A pre-training technique to localize medical\nbert and enhance biobert.\nNicholas Walker, Amalie Trewartha, Haoyan Huo,\nSanghoon Lee, Kevin Cruse, John Dagdelen, Alexan-\nder Dunn, Kristin Persson, Gerbrand Ceder, and\nAnubhav Jain. 2021. The impact of domain-specific\npre-training on named entity recognition tasks in ma-\nterials science. Available at SSRN 3950755.\nZheren Wang, Kevin Cruse, Yuxing Fei, Ann Chia, Yan\nZeng, Haoyan Huo, Tanjin He, Bowen Deng, Olga\nKononova, and Gerbrand Ceder. 2022a. Ulsa: Uni-\nfied language of synthesis actions for the representa-\ntion of inorganic synthesis protocols. Digital Discov-\nery.\nZheren Wang, Olga Kononova, Kevin Cruse, Tanjin He,\nHaoyan Huo, Yuxing Fei, Yan Zeng, Yingzhi Sun,\nZijian Cai, Wenhao Sun, et al. 2022b. Dataset of\nsolution-based inorganic materials synthesis proce-\ndures extracted from the scientific literature. Scien-\ntific Data, 9(1):1–11.\nLeigh Weston, Vahe Tshitoyan, John Dagdelen, Olga\nKononova, Amalie Trewartha, Kristin A Persson,\nGerbrand Ceder, and Anubhav Jain. 2019. Named\nentity recognition and normalization applied to large-\nscale information extraction from the materials sci-\nence literature. Journal of chemical information and\nmodeling, 59(9):3692–3702.\nShanchan Wu and Yifan He. 2019. Enriching pre-\ntrained language model with entity information for\nrelation classification. In Proceedings of the 28th\nACM international conference on information and\nknowledge management, pages 2361–2364.\nKyosuke Yamaguchi, Ryoji Asahi, and Yutaka Sasaki.\n2020. Sc-comics: a superconductivity corpus for ma-\nterials informatics. In Proceedings of The 12th Lan-\nguage Resources and Evaluation Conference, pages\n6753–6760.\n3632\nAppendix\nA Experimental Details\nWe performed fine-tuning experiments using a\nsingle GPU with a learning rate was 2e-5, the\nhidden size of the encoders being 768, except\nScholarBERT which is 1024, using the Adam\n(Kingma and Ba, 2014) optimizer for a max num-\nber of 20 training epochs with early stopping. All\nmodels are implemented with Python and PyTorch,\nand repeated five times to report the average\nperformance. The full set of hyperparameters is\navailable in our publicaly released code at https:\n//github.com/BangLab-UdeM-Mila/\nNLP4MatSci-ACL23.\nB Additional Text-to-Schema\nExperiments\nTo arrive at our data presented in Table 3, we con-\nducted experiments for all the language models\nacross all tasks in MatSci-NLP. The results for\nseven tasks in MatSci-NLP are shown in subse-\nquent tables:\n• Named Entity Recognition in Table 4.\n• Relation Classification in Table 5.\n• Event Argument Extraction in Table 6.\n• Paragraph Classification in Table 7.\n• Synthesis Action Retrieval in Table 8.\n• Sentence Classification in Table 9.\n• Slot Filling in Table 10.\nThe experimental results summarized in the afore-\nmentioned tables reinforce the conclusions in our\nanalysis of (Q2) in Section 5.2 with the text-to-\nschema based fine-tuning method generally out-\nperforming the conventional single and multitask\nmethods across all tasks and all language models.\n3633\nNLP Model Single Task Single Task Prompt MMOE No ExplanationsPotential\nChoices Examples Text2Schema\nMatSciBERT\n(Gupta et al., 2022)\n0.690±0.018\n0.403±0.029\n0.707±0.089\n0.445±0.071\n0.451±0.114\n0.188±0.065\n0.655±0.066\n0.410±0.051\n0.732±0.048\n0.480±0.087\n0.753±0.060\n0.505±0.066\n0.707±0.076\n0.470±0.092\nMatBERT\n(Walker et al., 2021)\n0.705±0.011\n0.469±0.037\n0.796±0.029\n0.558±0.044\n0.691±0.060\n0.400±0.070\n0.805±0.018\n0.574±0.061\n0.756±0.071\n0.524±0.088\n0.778±0.015\n0.547±0.039\n0.798±0.031\n0.569±0.055\nBatteryBERT\n(Huang and Cole, 2022)\n0.690±0.014\n0.464±0.018\n0.673±0.029\n0.407±0.045\n0.439±0.185\n0.168±0.110\n0.733±0.026\n0.483±0.049\n0.607±0.169\n0.369±0.140\n0.743±0.015\n0.497±0.015\n0.722±0.045\n0.470±0.043\nSciBERT\n(Beltagy et al., 2019)\n0.686±0.015\n0.464±0.035\n0.754±0.029\n0.493±0.063\n0.598±0.027\n0.298±0.048\n0.708±0.115\n0.465±0.115\n0.724±0.045\n0.471±0.069\n0.754±0.054\n0.509±0.064\n0.734±0.079\n0.497±0.091\nScholarBERT\n(Hong et al., 2022)\n0.206±0.350\n0.069±0.131\n0.179±0.088\n0.108±0.057\n0.109±0.142\n0.018±0.033\n0.134±0.036\n0.071±0.023\n0.263±0.109\n0.122±0.073\n0.168±0.044\n0.098±0.045\n0.168±0.067\n0.101±0.034\nBioBERT\n(Wada et al., 2020)\n0.665±0.018\n0.403±0.030\n0.708±0.119\n0.431±0.115\n0.204±0.114\n0.019±0.000\n0.723±0.075\n0.474±0.071\n0.455±0.114\n0.188±0.065\n0.725±0.024\n0.452±0.044\n0.715±0.031\n0.459±0.055\nBERT\n(Devlin et al., 2018)\n0.606±0.009\n0.304±0.024\n0.636±0.034\n0.382±0.041\n0.235±0.069\n0.055±0.040\n0.670±0.056\n0.441±0.060\n0.455±0.138\n0.267±0.089\n0.664±0.047\n0.418±0.046\n0.657±0.079\n0.416±0.058\nTable 4: Results of named entity recognition task among seven tasks on different schema settings for various BERT\nmodels pre-trained on different domain specific text data. For each model, the top line represents the micro-F1 score\nand the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence\ninterval of two standard deviations. We highlight the best performing method.\nNLP Model Single Task Single Task Prompt MMOE No ExplanationsPotential\nChoices Examples Text2Schema\nMatSciBERT\n(Gupta et al., 2022)\n0.671±0.083\n0.439±0.137\n0.545±0.102\n0.219±0.035\n0.490±0.139\n0.218±0.073\n0.747±0.128\n0.461±0.190\n0.800±0.058\n0.482±0.064\n0.818±0.137\n0.530±0.203\n0.791±0.046\n0.507±0.073\nMatBERT\n(Walker et al., 2021)\n0.714±0.023\n0.487±0.075\n0.644±0.050\n0.310±0.078\n0.591±0.267\n0.297±0.143\n0.871±0.020\n0.623±0.035\n0.804±0.071\n0.513±0.138\n0.848±0.045\n0.569±0.019\n0.875±0.015\n0.630±0.047\nBatteryBERT\n(Huang and Cole, 2022)\n0.594±0.085\n0.359±0.075\n0.592±0.084\n0.297±0.025\n0.423±0.097\n0.167±0.074\n0.823±0.073\n0.553±0.074\n0.801±0.081\n0.466±0.111\n0.854±0.029\n0.592±0.066\n0.786±0.113\n0.472±0.150\nSciBERT\n(Beltagy et al., 2019)\n0.699±0.105\n0.495±0.099\n0.585±0.125\n0.267±0.042\n0.643±0.088\n0.311±0.098\n0.799±0.139\n0.527±0.204\n0.783±0.085\n0.474±0.099\n0.814±0.125\n0.528±0.180\n0.819±0.067\n0.545±0.119\nScholarBERT\n(Hong et al., 2022)\n0.603±0.179\n0.178±0.186\n0.619±0.248\n0.384±0.154\n0.243±0.351\n0.078±0.139\n0.416±0.013\n0.334±0.006\n0.543±0.060\n0.252±0.062\n0.367±0.080\n0.236±0.119\n0.428±0.148\n0.274±0.110\nBioBERT\n(Wada et al., 2020)\n0.692±0.105\n0.458±0.087\n0.538±0.108\n0.243±0.029\n0.306±0.032\n0.079±0.017\n0.743±0.199\n0.442±0.215\n0.674±0.093\n0.323±0.092\n0.666±0.220\n0.324±0.118\n0.797±0.092\n0.465±0.134\nBERT\n(Devlin et al., 2018)\n0.564±0.130\n0.357±0.076\n0.626±0.103\n0.306±0.075\n0.368±0.112\n0.100±0.018\n0.792±0.056\n0.533±0.041\n0.696±0.046\n0.382±0.039\n0.636±0.094\n0.382±0.043\n0.782±0.056\n0.494±0.061\nTable 5: Results of relation classification task among seven tasks on different schema settings for various BERT\nmodels pre-trained on different domain specific text data. For each model, the top line represents the micro-F1 score\nand the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence\ninterval of two standard deviations. We highlight the best performing method.\n3634\nNLP Model Single Task Single Task Prompt MMOE No ExplanationsPotential\nChoices Examples Text2Schema\nMatSciBERT\n(Gupta et al., 2022)\n0.108±0.062\n0.041±0.020\n0.148±0.182\n0.050±0.071\n0.280±0.127\n0.122±0.063\n0.448±0.091\n0.251±0.075\n0.498±0.045\n0.310±0.036\n0.484±0.015\n0.292±0.052\n0.436±0.066\n0.251±0.075\nMatBERT\n(Walker et al., 2021)\n0.152±0.093\n0.029±0.021\n0.160±0.169\n0.033±0.033\n0.341±0.006\n0.174±0.027\n0.453±0.108\n0.274±0.087\n0.483±0.063\n0.298±0.037\n0.515±0.040\n0.288±0.064\n0.451±0.091\n0.288±0.066\nBatteryBERT\n(Huang and Cole, 2022)\n0.149±0.072\n0.030±0.039\n0.162±0.166\n0.036±0.029\n0.232±0.196\n0.104±0.088\n0.397±0.105\n0.233±0.086\n0.438±0.063\n0.298±0.037\n0.443±0.023\n0.250±0.068\n0.457±0.024\n0.277±0.034\nSciBERT\n(Beltagy et al., 2019)\n0.152±0.123\n0.041±0.068\n0.160±0.189\n0.033±0.032\n0.312±0.015\n0.159±0.024\n0.449±0.079\n0.259±0.072\n0.442±0.135\n0.264±0.103\n0.484±0.042\n0.287±0.075\n0.451±0.077\n0.276±0.080\nScholarBERT\n(Hong et al., 2022)\n0.349±0.102\n0.250±0.101\n0.444±0.091\n0.253±0.103\n0.262±0.062\n0.102±0.108\n0.454±0.094\n0.312±0.131\n0.454±0.095\n0.264±0.102\n0.431±0.081\n0.296±0.144\n0.489±0.083\n0.356±0.109\nBioBERT\n(Wada et al., 2020)\n0.119±0.080\n0.030±0.011\n0.160±0.170\n0.034±0.032\n0.054±0.000\n0.013±0.000\n0.489±0.058\n0.305±0.090\n0.491±0.027\n0.295±0.059\n0.473±0.034\n0.268±0.061\n0.488±0.036\n0.274±0.049\nBERT\n(Devlin et al., 2018)\n0.198±0.041\n0.042±0.055\n0.160±0.170\n0.033±0.033\n0.232±0.002\n0.049±0.008\n0.400±0.017\n0.194±0.025\n0.414±0.064\n0.214±0.092\n0.451±0.074\n0.265±0.104\n0.418±0.053\n0.225±0.091\nTable 6: Results of event argument extraction task among seven tasks on different schema settings for various\nBERT models pre-trained on different domain specific text data. For each model, the top line represents the micro-F1\nscore and the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence\ninterval of two standard deviations. We highlight the best performing method.\nNLP Model Single Task Single Task Prompt MMOE No ExplanationsPotential\nChoices Examples Text2Schema\nMatSciBERT\n(Gupta et al., 2022)\n0.685±0.074\n0.588±0.152\n0.673±0.003\n0.402±0.001\n0.607±0.277\n0.386±0.150\n0.706±0.013\n0.633±0.115\n0.694±0.041\n0.524±0.175\n0.686±0.158\n0.583±0.226\n0.719±0.116\n0.623±0.183\nMatBERT\n(Walker et al., 2021)\n0.753±0.031\n0.730±0.016\n0.671±0.002\n0.402±0.001\n0.673±0.001\n0.404±0.004\n0.727±0.089\n0.601±0.212\n0.776±0.059\n0.722±0.076\n0.649±0.039\n0.509±0.155\n0.756±0.073\n0.691±0.188\nBatteryBERT\n(Huang and Cole, 2022)\n0.663±0.088\n0.585±0.156\n0.672±0.001\n0.402±0.000\n0.672±0.002\n0.402±0.001\n0.621±0.160\n0.564±0.180\n0.626±0.113\n0.574±0.092\n0.672±0.031\n0.540±0.129\n0.633±0.075\n0.610±0.046\nSciBERT\n(Beltagy et al., 2019)\n0.690±0.074\n0.605±0.150\n0.673±0.002\n0.402±0.001\n0.568±0.289\n0.370±0.089\n0.703±0.041\n0.598±0.204\n0.711±0.076\n0.598±0.203\n0.662±0.169\n0.562±0.202\n0.696±0.094\n0.546±0.243\nScholarBERT\n(Hong et al., 2022)\n0.620±0.161\n0.386±0.150\n0.603±0.271\n0.371±0.122\n0.658±0.029\n0.407±0.010\n0.672±0.003\n0.482±0.001\n0.662±0.144\n0.534±0.260\n0.668±0.016\n0.405±0.007\n0.663±0.032\n0.433±0.122\nBioBERT\n(Wada et al., 2020)\n0.629±0.041\n0.507±0.033\n0.672±0.002\n0.402±0.001\n0.671±0.001\n0.401±0.001\n0.658±0.211\n0.588±0.258\n0.709±0.033\n0.651±0.081\n0.680±0.193\n0.622±0.226\n0.675±0.144\n0.578±0.102\nBERT\n(Devlin et al., 2018)\n0.709±0.090\n0.585±0.093\n0.672±0.001\n0.468±0.283\n0.672±0.003\n0.402±0.001\n0.685±0.050\n0.562±0.221\n0.727±0.102\n0.602±0.238\n0.629±0.291\n0.468±0.283\n0.665±0.057\n0.532±0.194\nTable 7: Results of paragraph classification task among seven tasks on different schema settings for various BERT\nmodels pre-trained on different domain specific text data. For each model, the top line represents the micro-F1 score\nand the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence\ninterval of two standard deviations. We highlight the best performing method.\n3635\nNLP Model Single Task Single Task Prompt MMOE No ExplanationsPotential\nChoices Examples Text2Schema\nMatSciBERT\n(Gupta et al., 2022)\n0.383±0.024\n0.082±0.009\n0.334±0.004\n0.063±0.001\n0.424±0.249\n0.169±0.096\n0.676±0.071\n0.505±0.094\n0.631±0.081\n0.445±0.153\n0.741±0.157\n0.549±0.179\n0.692±0.179\n0.484±0.254\nMatBERT\n(Walker et al., 2021)\n0.346±0.006\n0.067±0.004\n0.334±0.001\n0.063±0.000\n0.549±0.087\n0.300±0.045\n0.792±0.073\n0.653±0.184\n0.669±0.061\n0.497±0.086\n0.744±0.010\n0.557±0.082\n0.717±0.040\n0.549±0.091\nBatteryBERT\n(Huang and Cole, 2022)\n0.280±0.004\n0.118±0.041\n0.334±0.001\n0.063±0.000\n0.311±0.062\n0.073±0.028\n0.670±0.046\n0.496±0.117\n0.558±0.179\n0.358±0.149\n0.492±0.181\n0.282±0.184\n0.614±0.128\n0.419±0.149\nSciBERT\n(Beltagy et al., 2019)\n0.281±0.009\n0.052±0.027\n0.334±0.001\n0.063±0.001\n0.455±0.081\n0.207±0.095\n0.727±0.114\n0.564±0.137\n0.623±0.069\n0.456±0.135\n0.740±0.133\n0.533±0.160\n0.701±0.138\n0.516±0.217\nScholarBERT\n(Hong et al., 2022)\n0.437±0.104\n0.193±0.076\n0.489±0.105\n0.266±0.105\n0.330±0.007\n0.070±0.015\n0.389±0.001\n0.190±0.000\n0.492±0.165\n0.308±0.156\n0.389±0.001\n0.191±0.001\n0.322±0.260\n0.178±0.051\nBioBERT\n(Wada et al., 2020)\n0.300±0.015\n0.073±0.002\n0.324±0.001\n0.062±0.000\n0.334±0.062\n0.073±0.027\n0.662±0.060\n0.426±0.078\n0.561±0.128\n0.346±0.133\n0.545±0.157\n0.347±0.128\n0.647±0.140\n0.446±0.231\nBERT\n(Devlin et al., 2018)\n0.348±0.047\n0.091±0.020\n0.334±0.001\n0.063±0.000\n0.313±0.083\n0.073±0.037\n0.668±0.061\n0.495±0.058\n0.593±0.059\n0.424±0.086\n0.594±0.081\n0.371±0.103\n0.656±0.099\n0.515±0.067\nTable 8: Results of synthesis action retrieval task among seven tasks on different schema settings for various BERT\nmodels pre-trained on different domain specific text data. For each model, the top line represents the micro-F1 score\nand the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence\ninterval of two standard deviations. We highlight the best performing method.\nNLP Model Single Task Single Task Prompt MMOE No ExplanationsPotential\nChoices Examples Text2Schema\nMatSciBERT\n(Gupta et al., 2022)\n0.888±0.093\n0.602±0.151\n0.908±0.001\n0.476±0.001\n0.907±0.001\n0.493±0.069\n0.908±0.010\n0.601±0.159\n0.903±0.019\n0.573±0.135\n0.905±0.020\n0.616±0.150\n0.914±0.008\n0.660±0.079\nMatBERT\n(Walker et al., 2021)\n0.908±0.011\n0.441±0.038\n0.908±0.001\n0.476±0.001\n0.907±0.000\n0.476±0.000\n0.906±0.016\n0.645±0.025\n0.910±0.012\n0.561±0.135\n0.903±0.018\n0.600±0.089\n0.909±0.009\n0.614±0.134\nBatteryBERT\n(Huang and Cole, 2022)\n0.908±0.012\n0.452±0.045\n0.907±0.000\n0.475±0.001\n0.908±0.000\n0.476±0.000\n0.895±0.050\n0.679±0.080\n0.890±0.036\n0.685±0.074\n0.907±0.002\n0.519±0.144\n0.912±0.015\n0.684±0.095\nSciBERT\n(Beltagy et al., 2019)\n0.896±0.080\n0.421±0.159\n0.907±0.000\n0.469±0.004\n0.825±0.218\n0.535±0.079\n0.908±0.009\n0.586±0.166\n0.902±0.017\n0.596±0.161\n0.902±0.020\n0.623±0.130\n0.911±0.017\n0.617±0.143\nScholarBERT\n(Hong et al., 2022)\n0.805±0.020\n0.458±0.099\n0.839±0.268\n0.477±0.004\n0.908±0.001\n0.485±0.000\n0.908±0.000\n0.476±0.000\n0.900±0.019\n0.509±0.093\n0.907±0.001\n0.476±0.001\n0.906±0.007\n0.478±0.008\nBioBERT\n(Wada et al., 2020)\n0.908±0.001\n0.476±0.001\n0.907±0.001\n0.478±0.001\n0.907±0.001\n0.503±0.005\n0.910±0.012\n0.614±0.175\n0.899±0.047\n0.610±0.078\n0.908±0.015\n0.638±0.089\n0.915±0.021\n0.686±0.098\nBERT\n(Devlin et al., 2018)\n0.911±0.010\n0.475±0.036\n0.907±0.000\n0.476±0.000\n0.907±0.001\n0.476±0.000\n0.906±0.007\n0.549±0.086\n0.905±0.010\n0.581±0.153\n0.892±0.035\n0.563±0.136\n0.910±0.016\n0.633±0.133\nTable 9: Results of sentence classification task among seven tasks on different schema settings for various BERT\nmodels pre-trained on different domain specific text data. For each model, the top line represents the micro-F1 score\nand the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence\ninterval of two standard deviations. We highlight the best performing method.\n3636\nNLP Model Single Task Single Task Prompt MMOE No ExplanationsPotential\nChoices Examples Text2Schema\nMatSciBERT\n(Gupta et al., 2022)\n0.083±0.047\n0.087±0.045\n0.086±0.072\n0.010±0.011\n0.043±0.023\n0.016±0.005\n0.419±0.074\n0.182±0.043\n0.433±0.121\n0.169±0.069\n0.428±0.187\n0.169±0.075\n0.436±0.142\n0.194±0.062\nMatBERT\n(Walker et al., 2021)\n0.179±0.074\n0.087±0.030\n0.151±0.121\n0.024±0.022\n0.148±0.148\n0.057±0.067\n0.547±0.050\n0.276±0.047\n0.493±0.078\n0.230±0.067\n0.502±0.034\n0.221±0.011\n0.548±0.058\n0.273±0.051\nBatteryBERT\n(Huang and Cole, 2022)\n0.093±0.074\n0.009±0.012\n0.073±0.033\n0.008±0.011\n0.032±0.031\n0.008±0.009\n0.540±0.092\n0.270±0.108\n0.433±0.155\n0.211±0.056\n0.506±0.065\n0.236±0.072\n0.520±0.057\n0.262±0.073\nSciBERT\n(Beltagy et al., 2019)\n0.098±0.054\n0.020±0.021\n0.099±0.075\n0.013±0.018\n0.125±0.073\n0.047±0.016\n0.469±0.112\n0.207±0.066\n0.432±0.106\n0.183±0.061\n0.446±0.167\n0.179±0.071\n0.481±0.144\n0.224±0.010\nScholarBERT\n(Hong et al., 2022)\n0.286±0.042\n0.110±0.009\n0.289±0.044\n0.111±0.019\n0.063±0.007\n0.005±0.004\n0.323±0.058\n0.111±0.027\n0.276±0.080\n0.076±0.024\n0.338±0.053\n0.117±0.015\n0.296±0.085\n0.109±0.044\nBioBERT\n(Wada et al., 2020)\n0.096±0.171\n0.023±0.020\n0.094±0.118\n0.015±0.024\n0.042±0.024\n0.004±0.001\n0.517±0.031\n0.241±0.082\n0.319±0.059\n0.110±0.048\n0.424±0.145\n0.177±0.119\n0.452±0.114\n0.191±0.045\nBERT\n(Devlin et al., 2018)\n0.086±0.032\n0.011±0.005\n0.082±0.065\n0.012±0.018\n0.034±0.026\n0.005±0.006\n0.566±0.042\n0.306±0.073\n0.421±0.137\n0.204±0.078\n0.476±0.079\n0.225±0.066\n0.520±0.019\n0.257±0.022\nTable 10: Results of slot filling task among seven tasks on different schema settings for various BERT models\npre-trained on different domain specific text data. For each model, the top line represents the micro-F1 score and\nthe bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence interval\nof two standard deviations. We highlight the best performing method.\n3637\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nYes - in Section 7.\n□\u0013 A2. Did you discuss any potential risks of your work?\nYes - in Section 7.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nYes - in Section 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nYes - in Section 3.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nYes - in Section 3.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nYes - in Section 3.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nYes - in Section 3 and Section 7.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nYes - in Section 3.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nYes - in Section 3.\nC □\u0013 Did you run computational experiments?\nYes - in Section 5.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nYes - in Appendix Section A.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n3638\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nYes - in Appendix Section A.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nYes - in Section 5.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n3639"
}