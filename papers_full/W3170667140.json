{
  "title": "SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale Place Recognition",
  "url": "https://openalex.org/W3170667140",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2524573977",
      "name": "Zhao-Xin Fan",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2168546156",
      "name": "Zhenbo Song",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099822741",
      "name": "Hongyan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2134475493",
      "name": "Zhiwu Lu",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2103372213",
      "name": "Jun He",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2124748885",
      "name": "Xiaoyong Du",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2524573977",
      "name": "Zhao-Xin Fan",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2168546156",
      "name": "Zhenbo Song",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099822741",
      "name": "Hongyan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2134475493",
      "name": "Zhiwu Lu",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2103372213",
      "name": "Jun He",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2124748885",
      "name": "Xiaoyong Du",
      "affiliations": [
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2179042386",
    "https://openalex.org/W6796878617",
    "https://openalex.org/W2939201152",
    "https://openalex.org/W3108515319",
    "https://openalex.org/W2157315715",
    "https://openalex.org/W1989484209",
    "https://openalex.org/W2584853650",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W2064859577",
    "https://openalex.org/W3102159028",
    "https://openalex.org/W1565312575",
    "https://openalex.org/W2967952058",
    "https://openalex.org/W6730757611",
    "https://openalex.org/W3102327032",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2767621619",
    "https://openalex.org/W2890807499",
    "https://openalex.org/W3033110609",
    "https://openalex.org/W2798194297",
    "https://openalex.org/W3107495767",
    "https://openalex.org/W2940791172",
    "https://openalex.org/W2936435387",
    "https://openalex.org/W2558027072",
    "https://openalex.org/W3103648783",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2620629206",
    "https://openalex.org/W4394652702",
    "https://openalex.org/W2963125977",
    "https://openalex.org/W2117228865",
    "https://openalex.org/W2963708168",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2139137304",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W2981207549",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3168256178",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3173736705",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2986519585",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2980535048",
    "https://openalex.org/W2963588253",
    "https://openalex.org/W2737529802",
    "https://openalex.org/W3120082156",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3165205231",
    "https://openalex.org/W2964091144",
    "https://openalex.org/W1677409904",
    "https://openalex.org/W4300614726"
  ],
  "abstract": "Simultaneous Localization and Mapping (SLAM) and Autonomous Driving are becoming increasingly more important in recent years. Point cloud-based large scale place recognition is the spine of them. While many models have been proposed and have achieved acceptable performance by learning short-range local features, they always skip long-range contextual properties. Moreover, the model size also becomes a serious shackle for their wide applications. To overcome these challenges, we propose a super light-weight network model termed SVT-Net. On top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based Sparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer (CSVT) are proposed respectively to learn both short-range local features and long-range contextual features. Consisting of ASVT and CSVT, SVT-Net can achieve state-of-the-art performance in terms of both recognition accuracy and running speed with a super-light model size (0.9M parameters). Meanwhile, for the purpose of further boosting efficiency, we introduce two simplified versions, which also achieve state-of-the-art performance and further reduce the model size to 0.8M and 0.4M respectively.",
  "full_text": "SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large\nScale Place Recognition\nZhaoxin Fan1, Zhenbo Song3, Hongyan Liu4\u0003, Zhiwu Lu2, Jun He1\u0003 , and Xiaoyong Du1\n1 Key Laboratory of Data Engineering and Knowledge Engineering of MOE,\nSchool of Information, Renmin University of China, 100872, Beijing, China\n2 Gaoling School of Artiﬁcial Intelligence, Renmin University of China, 100872, Beijing, China\n3 School of Computer Science and Engineering, Nanjing University of Science and Technology, 210094, Nanjing, China\n4 Department of Management Science and Engineering, Tsinghua University, 100084, Beijing, China\n{fanzhaoxin, luzhiwu, hejun}@ruc.edu.cn, hyliu@tsinghua.edu.cn, songzb@njust.edu.cn\nAbstract\nSimultaneous Localization and Mapping (SLAM) and Au-\ntonomous Driving are becoming increasingly more important\nin recent years. Point cloud-based large scale place recogni-\ntion is the spine of them. While many models have been pro-\nposed and have achieved acceptable performance by learning\nshort-range local features, they always skip long-range con-\ntextual properties. Moreover, the model size also becomes a\nserious shackle for their wide applications. To overcome these\nchallenges, we propose a super light-weight network model\ntermed SVT-Net. On top of the highly efﬁcient 3D Sparse\nConvolution (SP-Conv), an Atom-based Sparse V oxel Trans-\nformer (ASVT) and a Cluster-based Sparse V oxel Trans-\nformer (CSVT) are proposed respectively to learn both short-\nrange local features and long-range contextual features. Con-\nsisting of ASVT and CSVT, SVT-Net can achieve state-of-\nthe-art performance in terms of both recognition accuracy\nand running speed with a super-light model size (0.9M pa-\nrameters). Meanwhile, for the purpose of further boosting\nefﬁciency, we introduce two simpliﬁed versions, which also\nachieve state-of-the-art performance and further reduce the\nmodel size to 0.8M and 0.4M respectively.\nIntroduction\nLarge scale place recognition is the spine of a wide range\nof applications like Simultaneous Localization and Map-\nping (SLAM) (Mur-Artal and Tard ´os 2017), Autonomous\nDriving (Levinson et al. 2011), Robot Navigation (Ravankar\net al. 2018), etc. Commonly, the place recognition result can\nbe used for loop-closure (Chen et al. 2020) in a SLAM sys-\ntem or for user location in a indoor vision positioning sys-\ntem, when GPS signal is not available. Fig. 1 (Top) illus-\ntrates a common pipeline of large place recognition. For a\nlarge scale region, a database of scenes (usually represented\nby point clouds or images ) tagged with UTM coordinates\nacquired from GPS/INS readings are constructed in advance.\nWhen a user traverses the same region, he/her may collect a\nquery scene from scratch. Then the most similar scene to the\nquery scene should be retrieved from the database to deter-\nmine where the location of the query scene is.\n\u0003Corresponding authors\nCopyright c\r 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nquery sample\ndatabase \nSubmap 1retrieval Submap 2 Submap 3\nFigure 1: (Top) Pipeline of point cloud based place recogni-\ntion. (Bottom) Model size and accuracy.\nA straight-forward idea for this task is to use images to\nlearn global descriptors for accurate and efﬁcient scene re-\ntrieval (Li, Snavely, and Huttenlocher 2010; Han et al. 2017;\nYu et al. 2019). However, images are sensitive to illumina-\ntion, weather change, diurnal variation, etc, making mod-\nels based on them unstable and unreliable. Besides, images\nare short of perceiving 3D scenes due to lack of depth in-\nformation. Recently, a line of point cloud based deep learn-\ning models (Uy and Lee 2018; Zhang and Xiao 2019; Sun\net al. 2020; Liu et al. 2019; Fan et al. 2020; Xia et al.\n2021; Komorowski 2021) for large scale place recognition\nhave been proposed. Since point clouds are invariant to illu-\nmination and weather changes, point cloud based methods\nare more robust than image based methods. Besides, since\npoint clouds contain richer 3D information, global descrip-\ntors learned from them are stronger in describing 3D scenes\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n551\nthan image descriptors and therefore they always achieve\nbetter performance.\nThough better, existing point cloud based methods still\nface three main challenges. 1) Most of existing methods\nlearn descriptors from point-wise point cloud encoders,\nwhich are sensitive to local noise. These local noise may\nstand for scene details and being important for some ﬁne\ngrained level tasks such as segmentation. However, they are\nuseless for place recognition but even become a burden for\nthe network to understand the scene. Therefore, they should\nbe regarded as noise and outliers. 2) We observe that most of\nprevious methods only consider how to better extract short-\nrange local features, while the equally important long-range\ncontextual properties have long been skipped. And we argue\nthat lacking awareness of long-range contextual properties,\npower of the learned descriptors would be greatly limited.\n3) Most of existing models are suffered from huge model\nsize, which stops their application in resource constrained\nportable devices. Considering the above issues, we claim\nthat designing a local noise-insensitive light-weight point\ncloud descriptor extraction model that can capture long-\nrange contextual features is necessary.\nIn this paper, we propose a novel super light-weight net-\nwork named SVT-Net for point cloud based large scale\nplace recognition. SVT-Net’s nework architecture is built\nupon the delicate light weight 3D Sparse Convolution (SP-\nConv) (Choy, Gwak, and Savarese 2019). The reason why\nwe choose SP-Conv lies in two aspects. First, the sparse\nvoxel representation require to voxelize point cloud, which\nreduces local noise but retains most of overall scene geome-\ntries. Therefore, it can liberate the model from understand-\ning useless scene details. Second, the SP-Conv is efﬁcient\nand fast. It only computes outputs for predeﬁned coordinates\nand saves them into a compact sparse tensor. In other words,\nit meets our requirements for building a light-weight model.\nHowever, simply stacking SP-Conv layers may cause\nneglect of long-range contextual properties. A direct way\nto solve this problem is introducing Vision Transformers\n(Dosovitskiy et al. 2020) for learning long-range contex-\ntual features. There indeed exists point cloud Transform-\ners (Guo et al. 2020) in literature, however, they are not\nsuitable for the point cloud based place recognition task. It\nis because all existing point cloud Transformers are point-\nwise modules and therefore not efﬁcient enough. Besides, as\nmentioned before, point-wise modules may suffer from lo-\ncal noise. Therefore, we propose two kinds of Sparse V oxel\nTransformers (SVTs) tailored for large scale place recogni-\ntion on top of SP-Conv layers named Atom-based Sparse\nV oxel Transformer (ASVT) and Cluster-based Sparse V oxel\nTransformer (CSVT) respectively. ASVT and CSVT implic-\nitly extract long-range contextual features from the sparse\nvoxel representation through two perspectives: attending on\ndifferent key atoms and clustering different key regions in\nthe feature space, thereby helping to obtain more discrimina-\ntive descriptors through interacting different atoms (to learn\ninter-atoms long-range features) and different clusters (to\nlearn inter-clusters long-range features) respectively. Since\nSP-Conv only conducts convolution operation on non-empty\nvoxels, it is computational efﬁcient and ﬂexible, so do the\ntwo SVTs built upon it. Thanks to the strong capabilities\nof the two SVTs, our proposed model can learn sufﬁciently\npowerful descriptors from an extremely shallow network ar-\nchitecture. And thanks to the shallow network architecture,\nmodel size of SVT-Net is very small as shown in Fig. 1 (Bot-\ntom).\nWe conduct extensive experiments on Oxford RobotCar\ndataset (Maddern et al. 2017) and three in-house datasets\n(Uy and Lee 2018) to verify the effectiveness and efﬁciency\nof SVT-Net. Results show that though light-weight, SVT-\nNet can achieve state-of-the-art performance in terms of\nboth accuracy and speed. What’s more, to further increase\nspeed and reduce model size, we introduce two simpliﬁed\nversion of SVT-Net: ASVT-Net and CSVT-Net, which also\nachieve state-of-the-art performances with further reduced\nmodel sizes of only 0.8M parameters and 0.4M parameters\nrespectively.\nOur main contributions are three folds. 1) We propose\na novel light-weight point cloud based place recognition\nmodel named SVT-Net as well as two simpliﬁed versions:\nASVT-Net and CSVT-Net, which all achieve state-of-the-art\nperformance in terms of both accuracy and speed with a ex-\ntremely small model size. 2) We propose Atom-based Sparse\nV oxel Transformer (ASVT) and Cluster-based Sparse V oxel\nTransformer (CSVT) for learning long-range contextual fea-\ntures hidden in point clouds. To the best of our knowledge,\nwe are the ﬁrst to propose Transformers for sparse voxel\nrepresentations. 3) We have conducted extensive quantita-\ntive and qualitative experiments to verify the effectiveness\nand efﬁciency of our proposed models and analysed what\nthe two proposed Transformers actually learn.\nRelated Work\nLarge Scale Place Recognition\nLarge scale place recognition plays an important role in\nSLAM and autonomous driving and has been interested in\nby many researchers for a long time. In early years, hand-\ncraft features (G ´alvez-L´opez and Tardos 2012; Fern ´andez-\nMoral et al. 2013; Johns and Yang 2011) or learned features\n(Arandjelovic et al. 2016; Yu et al. 2019; Hausler et al. 2021)\nextracted from images are used for place recognition. These\nmethods, though straight-forward, are suffered from vulner-\nability of features caused by images’ sensitivity towards il-\nlumination, weather change, diurnal variation, etc.\nCompared to image, point cloud is more insensitive to\nenvironmental changes, therefore it is a better alternative\nfor place recognition. PointNetVLAD (Uy and Lee 2018)\nadopts PointNet (Qi et al. 2017) and NetVLAD (Arand-\njelovic et al. 2016) to learn global point cloud descriptors\nfor this task. Then, a series of following works (Zhang and\nXiao 2019; Sun et al. 2020; Fan et al. 2020; Liu et al. 2019;\nXia et al. 2021; Komorowski 2021) are proposed. They use\ngraph networks, attentions and voxel representation to learn\npowerful global descriptors for this task respectively. How-\never, most of them are suffered from three aspects: ﬁrst, they\nfail to learn long-range contextual features of scenes from\npoint cloud; second, model size and efﬁciency are not con-\nsidered in their methods; third, they are sensitive to local\n552\nnoise. In our work, we design two light-weight but strong\nSparse V oxel Tranformers to tackle the above problems.\nVision Transformers\nTransformer (Vaswani et al. 2017) is one of the\nmost successful design for natural language processing\n(NLP)(Devlin et al. 2018; Hu, Shen, and Sun 2018; Yang\net al. 2019; Kim, Son, and Kim 2021; Chen, Fan, and Panda\n2021; Chen et al. 2018), the core of which is a self-attention\nmechanism to capture long-range contextual features. Re-\ncently, inspired by the great success of Transformer in NLP,\nresearchers begin to design Transformers tailored for com-\nputer vision tasks.\nTherefore, Vision Transformer (ViT) (Dosovitskiy et al.\n2020) is proposed recently. It adopts the idea of self-\nattention and divides images to 16x16 visual words. In this\nway, images can be processed like nature language. Then,\na variety of following works (Wu et al. 2020; Wang et al.\n2021; Liu et al. 2021; Jiang, Chang, and Wang 2021) are\nproposed based on it. However, all the above introduced vi-\nsion Transformers are designed for learning from images.\nTo boost the performance of point cloud based tasks, point-\nwise vision Transformers like (Zhao et al. 2020; Guo et al.\n2020) are proposed. Though tailored for point clouds, they\nare not suitable for the place recognition task. Because they\nare not light-weight enough and are suffered from small\nlocal noise in raw point clouds. In contrast, we propose\ntwo kinds of super-light Sparse V oxel Transformers to learn\nglobal features from scenes, which are less suffered from lo-\ncal noise and are much more efﬁcient. To our knowledge,\nthis is the ﬁrst work designs Sparse V oxel Transformers for\npoint clouds.\nMethodology\nProblem Deﬁnition\nLet Mr = {mi|i= 1;2; :::; M}be a database of pre-deﬁned\n3D submaps (represented as point clouds), and Q be a query\npoint cloud scan. The place recognition problem is deﬁned\nas retrieving a submap ms from Mr with the goal of ms is\nthe closest one to Q. To achieve accurate retrieving, a deep\nlearning model F(∗) that can embed all point clouds into\ndiscriminative global descriptors, e.g. Q → fq ∈Rd, is\nrequired so that a following KNNs algorithm can be used\nfor ﬁnding ms.\nTo meet the goal, we choose to use the sparse voxel rep-\nresentation of point cloud as input and choose 3D Sparse\nConvolution (SP-Conv) (Choy, Gwak, and Savarese 2019)\nas the basic unit to build the deep learning model. To em-\nploy SP-Conv, we ﬁrst voxelize all point clouds into sparse\nvoxel representations, e.g. Q → Qv ∈RL\u0002W\u0002H\u00021, where\nfor each voxel, 1 means that it is occupied by any points in\nQ, called non-empty voxel, and otherwise 0, called empty\nvoxel. SP-Conv operation is only conducted on non-empty\nvoxels. Hence, it is very efﬁcient and ﬂexible. Next, we will\nintroduce the two proposed Transformers: the Atom-based\nSparse V oxel Transformer (ASVT) and the Cluster-based\nSparse V oxel Transformer (CSVT) respectively. And then,\n…\nSP-Conv\n(Key)\nSP-Conv\n(Query)\nSP-Conv\n(Value)\nRearrange\n…\n…\n…\nTranspose\nN✕C\nN✕C\nN✕C\nSoftMax\nRearrange\nRearrange\nRearrangeSP-ConvN✕C\nN✕C\nN✕C\nN✕C\nN✕C\nFigure 2: Network architecture of ASVT.\nthe overall network architecture of SVT-Net as well as net-\nwork architectures of the two simpliﬁed versions (ASVT-\nNet and CSVT-Net) will be introduced in detail. The loss\nfunction will be presented ﬁnally.\nAtom-Based Sparse Voxel Transformer\nAs mentioned before, simply stacking SP-Conv layers may\ncause the loss of learning long-range contextual features.\nTo make up for this loss, we design the ﬁrst Transformer,\nASVT, which adopts the idea of self-attention to aggregate\ninformation from both nearby and far-away voxels to bet-\nter capture sparse voxel features. In ASVT, we deﬁne each\nindividual voxel as an atom. During processing, each atom\nshould be interacted with all other atoms according to the\nlearned per-atom contributions. By doing so, different key\natoms could be attended by other atoms so that both local\nrelationship of nearby atoms and long-range contextual re-\nlationship of far way atoms will be learned, i.e, inter-atoms\nlong-range contextual features are learned. Note that learn-\ning such kind of inter-atoms long-range contextual relation-\nship is very important for the model. For example, in a scene,\nassume there are two atoms that belong to different instances\nof the same category. If only SP-Conv is used, the ”same-\ncategory” information may be ignored due to the small re-\nceptive ﬁeld. While if A VST is added to learn such kind\nof information, the model can better encode what the scene\ndescribes. Hence the ﬁnal global descriptor would be more\npowerful. The architecture of ASVT is illustrated in Fig. 2.\nLet Xin ∈ RL\u0002W\u0002H\u0002C be the input sparse voxel\nfeatures learned by SP-Convs (SP-voxel features for sim-\nplicity). We ﬁrst learn the sparse voxel values (SP-values\nfor simplicity) Xv ∈ RL\u0002W\u0002H\u0002C, SP-queries Xq ∈\nRL\u0002W\u0002H\u0002Cr , and SP-keys Xk ∈RL\u0002W\u0002H\u0002Cr through\nthree different SP-Convs respectively:\nXv = SPConv (Xin)\nXq = SPConv (Xin)\nXk = SPConv (Xin)\n(1)\nwhere we often set Cr < Cto reduce computational cost in\nlater steps. That is to say, the dimension of SP-queries and\nSP-keys are reduced from C to Cr for efﬁciency. After that,\nSP-voxel features of SP-values (SP-queries/keys) are rear-\nranged to a tensor of N ×C (N ×Cr), where N is the num-\nber of non-empty voxels. The rearrange step is easy. Since\ncoordinates and features of non-empty voxels have been al-\nready stored as sparse tensors in SP-Conv’s output, we only\nneed to take out the feature tensor from its data structure for\n553\nrearrange. Note that N ≪ L×W ×H and N ≪ Np, where\nNp is point number of the raw point cloud, therefore, the SP-\nConv and the following matrix multiplications based on the\nfeature tensor are all very computational efﬁcient.\nThen, we use Xq and Xk to calculate the SP-attention\nmap S:\nS = softmax(Xq ·XT\nk ) (2)\nwhere S ∈ RN\u0002N encodes the contribution relationship\nof each atom with all the other atoms. In the following at-\ntending operation, these relationships will contribute to ag-\ngregating both short-range local information and long-range\ncontextual information by interacting atoms. The attending\noperation can be summarized as:\nXs = SPConv (S ·Xv) (3)\nwhere Xs ∈RN\u0002C is called atom-attended SP-voxel fea-\ntures. In Xs, features of each atom xi have adaptively ac-\ncepted contributions from all the other atoms according to\nthe implicit mode hidden in S. Thus meaningful contextual\nand semantic information can be represented in Xs to de-\nscribe the scene.\nFinally, we rearrange Xs back to sparse voxel representa-\ntions with a dimension of L×W ×H ×C and regard it as a\nresidual term. The ﬁnal ASVT feature is deﬁned as the sum\nof Xin and Xs:\nXasvt = Xin + Xs (4)\nCluster-Based Sparse Voxel Transformer\nAnother observation we ﬁnd is: in the sparse voxel repre-\nsentation, some atoms may share the same characteristics.\nFor example, atoms of a wall always form a plane like struc-\nture, while atoms of a ﬂower bed easily form a cylinder like\nstructure. This means that atoms can actually cluster into\ndifferent clusters according to their geometric or semantic\ncharacteristics, and the long-range contextual properties can\nalso be extracted from the perspective of interacting between\nthese clusters, i.e, learning inter-clusters long-range contex-\ntual features. Motivated by this intuition, we propose the\nsecond Transformer: CSVT. As shown in Fig. 3, CSVT con-\nsists of three component, a Tokenizer module, a Transformer\nmodule and a Projector module. They cooperatively learn\nhow to implicitly group atoms into characteristics-similar\nclusters and interact clusters for enhancing learned features.\nNext, we will introduce them in detail.\nThe Tokenizer module is used to transform the input SP-\nvoxel features into tokens, where each token represents\na cluster in the latent space. We again deﬁne Xin ∈\nRL\u0002W\u0002H\u0002C as the initial SP-voxel features. To achieve the\ngoals of the tokenizer, we ﬁrst use a SP-Conv operation fol-\nlowed by a rearrange operation to generate a grouping map\nXg ∈RN\u0002Lt :\nXg = softmax(RE (SPConv (Xin))) (5)\nwhere RE is the rearrange operation. Lt is the number of\ntokens we choose to generate and N is the number of non-\nempty voxels. Xg stores the probabilities of each voxel be-\nlonging to each token. Therefore, we can use Xg to capture\nrepresentations of tokens as grouping different tokens into\ndifferent clusters in an implicit way:\nT = XT\ng ·SPConv (Xin) (6)\nwhere T ∈ RLt\u0002C denotes representations of Lt tokens\nwith each of them being described by C features.\nThe Transformer module is then used to learn inter-\nclusters long-range contextual features through interacting\nthese tokens. First, we generate values, keys, and queries us-\ning shared convolutional kernel Conv1d:\nTv = Conv1d(T); Tq = Conv1d(T); Tk = Conv1d(T)\n(7)\nThen, tokens are interacted with each other through the fol-\nlowing attention operation:\nTs = T + Conv1d(softmax(Tq ·TT\nk ) ·Tv) (8)\nwhere Ts ∈ RLt\u0002C is the attended tokens. Through the\nTransformer module, relationship between different clusters\nare learned to characterize the distribution characteristics of\nthe scene with high quality. For example, the ﬁnal descrip-\ntor may memorize that there is a rectangular building in the\nscene stands 5 meters away from a cylindrical building, or\nremember that there is a spherical building stands behind a\nslender tree.\nThe Projector moduleis then used to project token features\nback to the sparse voxel representation. Speciﬁcally, we use\nTs to calculate a re-projection map Mp ∈RN\u0002Lt:\nTp = Conv1d(Ts) (9)\nMp = softmax(RE(SPConv (Xin)) ·TT\np ) (10)\nwhere Tp ∈RLt\u0002C. Then, the re-projection operation is\ndeﬁned as:\nXs = SPConv (Mp ·Tp) (11)\nAgain, we rearrangeXs back to sparse voxel representations\nwith a dimension ofL×W×H×C and regard it as a residual\nterm. The ﬁnal CSVT feature is deﬁned as:\nXcsvt = Xin + Xs (12)\nNote that, though both aim to learn long-range contex-\ntual features, roles and working mechanisms of ASVT and\nCSVT are different. The ASVT focus on learning relation-\nship between similar and dissimilar individual atoms and\nlearns inter-atoms long-range contextual features in a ﬁne-\ngrained level, while CSVT focus on learning relationship\nbetween different characteristics-similar clusters so that it\nlearns inter-clusters long-range contextual features in a rela-\ntive coarser level. They are complementary to each other.\nNetwork Architecture\nThe overall architecture of SVT-Net is built upon the above\nintroduced ASVT and CSVT as well as the light-weight SP-\nConv. Speciﬁcally, as shown in Fig. 4. The initial sparse\nvoxel representation is fed into the ﬁrst SP-Conv layer with\nan output dimension of 32 to learn initial SP-features. Then\ntwo SP-Res-Blocks (each consists of two SP-Convs with a\n554\n…SP-Conv\n& Rearrange\nSP-Conv\n& Rearrange\n(grouping)\n…\nSoftMax\n…\nN✕L N✕L\nN✕C Transpose\nTokenizer\n…\nL✕C …Conv1d\n(Query)\nConv1d\n(Key)\nConv1d\n(Value)\n…\n…\nTranspose\nSoftMax\n…\nTransformer\nL✕C\nL✕C\nL✕C\nL✕L L✕C\nSP-Conv\n& Rearrange …\nN✕C\n N✕C\nProjector\nTranspose\nSoftMax…\nN✕L\n…\nN✕C\nSP-Conv\n& Rearrange\nFigure 3: Network architecture of CSVT.\nskip connection) are used to enhance learned features and\nincrease the feature dimension to 64. Next, another SP-conv\nlayer is used to increase the feature dimension to the ﬁnal\ndescriptor’s dimensiond. After that, the SP-features are fed\ninto two branches for learning ASVT features and CSVT\nfeatures using the two proposed Sparse V oxel Transform-\ners(SVTs) respectively. Then, the learned ASVT features\nand CSVT features are fused by directly adding them to-\ngether. Finally, the ﬁnal global descriptor is calculated by us-\ning a GeM Pooling operation (Radenovi´c, Tolias, and Chum\n2018):\nf = [f1; ···; fk; ···; fd]; fk = 1\n|Xfinal;k |\nX\nx2Xfinal;k\n(xpk )\n1\npk\n(13)\nwhere f ∈d is the ﬁnal descriptor, Xfinal is Xcsvt +Xasvt,\nand pk is a learnable control parameter.\nOther details of the network architecture can be found in\nSupp. Thanks to the strong power of ASVT and CSVT, our\nproposed model SVT-Net can achieve superior performance\ncompared to previous methods, even though our network ar-\nchitecture is simpler and smaller (from another words, it is\nshallower). Note that ASVT and CSVT can also be indi-\nvidually utilized in different networks. Therefore, we pro-\npose two simpliﬁed versions of SVT-Net: ASVT-Net and\nCSVT-Net, by eliminating the ASVT module and CSVT\nmodule, respectively, to verify the effectiveness of the two\nmodules. According to experimental results, both ASVT-Net\nand CSVT-Net also achieve state-of-the-art performances\nbut further reduce the model size for a large margin.\nLoss Function\nIn view of its superior performance in (Komorowski 2021),\nwe adopt the following triplet loss to train our model:\nL(fi; fp\ni ; fn\ni ) =max{d(fi; fp\ni ) −d(fi; fn\ni ) +m; 0}(14)\nwhere fi is the descriptor of the query scan, fp\ni and fn\ni are\ndescriptors of positive sample and negative sample respec-\ntively, and m is a margin. d(x; y) means the Euclidean dis-\ntance between x and y. To build informative triplets, we use\nbatch-hard negative mining following (Komorowski 2021).\nAfter the network is trained, all point clouds are embed-\nded into descriptors using the model. And we use the KNNs\nalgorithm to ﬁnd ms in the database, which is the closest\none to the query scan Q.\nCSTV\nModule\nASTV\nModule\nVoxelize\nSP-Conv\nBlocks\n(Stride 2)\nGeM Pooling\nInput\nOutput\nFigure 4: Pipeline of SVT-Net. The circle-add symbol means\nelement-wise sum.\nExperiments\nDatasets and Metrics\nTo fairly compare with other methods, we use the bench-\nmark datasets proposed by (Uy and Lee 2018) to evalu-\nate our method, which are now recognized as the most in-\nﬂuential datasets for point cloud based place recognition.\nThe benchmark contains four datasets: one outdoor dataset\nnamed Oxford generated from Oxford RobotCar (Maddern\net al. 2017) and three in-house datasets: university sector\n(U.S.), residential area (R.A.) and business district (B.D.).\nThe benchmark contains 21711, 400, 320, 200 submaps for\ntraining and 3030, 80, 75, 200 submaps for testing for Ox-\nford., U.S., R.A. and B.D. respectively. Each point cloud\ncontains 4096 points, which is the common setting of point\ncloud based place recognition. We use average recall at top\n1% and average recall at top 1 as main metrics as previous\nmethods for a fair comparison.\nImplementation Details\nIn all experiments, we voxelize 3D point coordinates with\n0.01 quantization step. The voxelization and the following\nSP-Conv operation are performed by the MinkowskiEngine\nauto differentiation library (Choy, Gwak, and Savarese\n2019). The dimension of the ﬁnal descriptor is set to 256.\nThe number of tokens Lt is set to 8. Following previous\nwork, we train two versions of models: baseline model and\nreﬁned model. The baseline model is trained only using\nthe training set of Oxford dataset, and the reﬁned model is\ntrained by adding the training set of U.S. and R.A. (Note\nthat training set of B.D. is not added). Random jitter, random\ntranslation, random points removal and random erasing aug-\nmentation are adopted for data augmentation during train-\ning. All experiments are performed on a Tesla V100 GPU\nwith a memory of 32G. More details can be found in the\nSupp.\n555\nAverage recall at top-1% (%) Average recall at top-1 (%)\nMethod Oxford U.S. R.A. B.D. Oxford U.S. R.A. B.D.\nPointNetVLAD 80.3 72.6 60.3 65.3 - - - -\nPCAN 83.8 79.1 71.2 66.8 - - - -\nDAGC 87.5 83.5 75.7 71.2 - - - -\nSOE-Net 96.4 93.2 91.5 88.5 - - - -\nSR-Net 94.6 94.3 89.2 83.5 86.8 86.8 80.2 77.3\nLPD-Net 94.9 96.0 90.5 89.1 86.3 87.0 83.1 82.3\nMinkloc3D 97.9 95.0 91.2 88.5 93.0 86.7 80.4 81.5\nSVT-Net(Ours) 97.8 96.5 92.7 90.7 93.7 90.1 84.3 85.5\nASVT-Net(Ours) 98.0 96.1 92.0 88.4 93.9 87.9 83.3 82.3\nCSVT-Net(Ours) 97.7 95.5 92.3 89.5 93.1 88.3 82.7 83.3\nTable 1: Comparison with the state-of-the-art methods under the baseline setting.\nMain Results\nIn this section, we experimentally verify the effectiveness\nand efﬁciency of our method. Speciﬁcally, we ﬁrst compare\nour models with PointNetVLAD (Uy and Lee 2018), PCAN\n(Zhang and Xiao 2019), DAGC (Sun et al. 2020), SR-Net\n(Fan et al. 2020), LPD-Net (Liu et al. 2019), SOE-Net (Xia\net al. 2021) and Minkloc3D (Komorowski 2021) in terms of\nrecognition accuracy. Then, we compare the inference time\nand model size between our models with them. Finally, we\nqualitatively analyze what the two SVTs have learned.\nAccuracy: In Table 1, we show the results of all methods\non the baseline setting. It can be found that SVT-Net signif-\nicantly outperforms all state-of-the-art methods, especially\nfor the average recall at top 1 metric on U.S., R.A., and B.D.,\nwhere SVT-Net wins for 3.4%, 3.9%, 4% compared to Min-\nkloc3D respectively. Compared to SVT-Net, performances\nof ASVT-Net and CSVT-Net drop to some extent. How-\never, their performances still largely outperform the previ-\nous best model Minkloc3D. We contribute the accuracy gain\nto the two novel SVTs we design. Note that Minkloc3D is\nalso built upon SP-Conv and shares the same loss function\nas our model, while its performance is not as excellent as\nour models, which further conﬁrms the superiority of our\ntwo proposed SVTs. Speciﬁcally, our SVT-Net build light-\nweight sparse voxel transformers based on SPConv, while\nMinkloc3D simply stacks SPConv layers, which is the main\ndifference between Minkloc3D and our model, and therefore\nit is the two SVTs being the main force make our model per-\nform better. What’s more, SOE-Net also use self-attention in\nits network architecture to learn long range context depen-\ndencies, but our model outperforms SOE-Net. This demon-\nstrates that sparse voxel transformers are more effective than\npoint-wise transformers for large scale place recognition.\nWe also note the self-attention module in SOE-Net is inef-\nﬁcient especially when the number of points is large due to\ncomputing attention weights for each of the Np raw points.\nIn contrast, the novel ASVT and CSVT in our SVT-Net are\nbuilt for processing sparse voxels, which are much more ef-\nﬁcient because we only need to compute attention weights\nfor each of N (N ≪ Np) non-empty voxels. Recall curves\nof the baseline setting can be found inSupp. We also visual-\nize some top-k matching results in Fig. 5 to provide readers\nQuery sample Top 1 Top 2 Top 3\nFigure 5: Visualization of top 3 matching results.\nMethod Time Parameters\nPointNetVLAD - 19.8M\nPCAN - 20.4M\nLPD-Net - 19.8M\nMinkloc3D 12.16ms 1.1M\nSVT-Net(Ours) 12.97ms 0.9M\nASVT-Net(Ours) 11.04ms 0.4M\nCSVT-Net(Ours) 11.75ms 0.8M\nTable 2: Efﬁciency comparison.\nwith a comprehensive view to understand our place recogni-\ntion results.\nFor a comprehensive comparison, we also show the re-\nsults of all models at the reﬁned setting in Supp. We ﬁnd\nthat at the reﬁned setting, our models still signiﬁcantly out-\nperform all models except Minkloc3D. In fact, our models\nstill perform better than Minklo3D in most cases, although\nonly by a small margin. The difference between our three\nmodels becomes narrow. We attribute this to that all models\nhave reached the performance upper bound.\nModel size and speed: To verify the efﬁciency of our\nmethod, we compare our models with previous works in\nterms of model size and inference time in Table 2 and Fig.\n556\nOxford B.D.U.S. R.A.\nFigure 6: Visualization of what ASVT and CSVT have learned. First row: original point clouds. Second row: features learned\nby ASVT, ”same category” atoms are attended similarly, e.g, in Oxford, two walls of the same height share the same color.\nThird row: features learned by CSVT, atoms belong to the same geometric shape are clustered together and interacted with each\nother, e.g, in B.D., all the atoms in the same ﬂowerbed (colored in Crimson) form a cube and are clustered together.\nAverage r\necall at top-1% (%) Average r\necall at top-1 (%)\nMethod Oxford U .S.\nR.A. B.D. Oxford U .S.\nR.A. B.D.\nA: Lt=4, d=256, add 97.9 96.4 92.5 89.0 93.7 89.0 83.9 82.5\nB: Lt=6, d\n=256, add 98.0 96.2 92.3 90.1 93.8 88.3 83.7 84.4\nC: Lt=10, d\n=256, add 97.9 96.2 92.0 89.4 93.8 87.2 83.3 83.5\nD: Lt=8, d=128, add 97.8 95.2 92.0 89.0 93.3 88.9 81.9 82.5\nE: Lt=8, d\n=384, add 98.2 94.8 92.5 89 94.4 86.9 84.9 83.7\nF: Lt=8, d=512, add 98.0 97.3 92.1 88.2 93.9 90.1 84.0 82.7\nG: Lt=8,d =512, cat 97.5 93.4 85.8 84.7 92.7 81.9 73.9 77.1\nH: Lt=8, d=256, cat&\nspconv 96.5 89.8 84.5 82.4 89.5 78.2 71.2 74.0\nSVT\n-Net: Lt=8, d=256, add 97.8 96.5 92.7 90.7 93.7 90.1 84.3 85.5\nTable 3: Results of ablation study for our SVT-Net.\n1 respectively. For model size, it can be seen that SVT-Net\nand CSVT-Net save 18.2% and 27.3% parameters respec-\ntively compared to the existing smallest model Minkloc3D.\nAs for ASVT-Net, it even only has 36.4% parameters of\nMinkloc3D, which is a signiﬁcant reduction. And it is worth\nnoting that all of our three models outperform Minkloc3D\nfor a large margin in terms of accuracy at the baseline set-\nting. The ability of signiﬁcantly improving accuracy under\nthe condition of drastically reduced parameters further fully\ndemonstrates the superiority of our two SVTs. For speed,\ncompared to the current fastest model Minkloc3D, SVT-\nNet only add ignorable additional inference time. And both\nASVT-Net and CSVT-Net run faster than Minkloc3D. Ap-\nproximately, voxelization and SP-Conv blocks cost about\nhalf of the running time, while ASVT and CSVT cost the\nanother half. We ﬁnd the speed increase is not as signiﬁcant\nas the model size reduction, which is because that the in-\nherent Transformer operation requires multiple matrix mul-\ntiplications. Summing up the above results, we can conclude\nthat our models are good enough in terms of both model size\nand running speed. Note, compared to Minkloc3D, our net-\nwork architectures are much shallower, that’s why our mod-\nels are more light-weight than it. And since the main differ-\nence between our models with Minkloc3D is the two SVTs\nwe design, we can contribute all performance gains into the\nlearned long-range contextual features.\nWe believe that expect recognition accuracy, both storage\nefﬁciency and recognition accuracy are also signiﬁcant fac-\ntors to make solid and convincing comparisons. In this work,\nextensive results show that our model outperforms the SOTA\nin all the three aspects. Besides, we also ﬁnd our three ver-\nsion show different specialties towards the three different as-\npects, and so we can accordingly make different utilization\nchoice. Speciﬁcally, SVT-Net is larger than ASVT-Net and\nCSVT-Net, but performs better in most cases. Therefore, if\nthere is enough resource, we recommend to use SVT-Net for\nthe place recognition task. If you only have limited compu-\ntational resource and can’t ﬁne-tune the model on new sce-\nnarios, we recommend to use CSVT-Net because its gener-\nalization ability towards new scenarios is better than ASVT-\nNet. Otherwise, ASVT-Net is a better choice because it is\nthe fastest and the smallest one.\nWhat Transformers have learned: One may be interested\nin what ASVT and CSVT have learned that could make our\n557\nmodels so elegant. To explore this question, we show some\nvisualization results in Fig. 6. The ﬁrst row shows the origi-\nnal point clouds randomly selected from Oxford, U.S., R.A.\nand B.D respectively.\nIn the second row, we visualize the features of each non-\nempty voxel after ASVT using T-SNE (Van der Maaten and\nHinton 2008). Different colors represent different distribu-\ntion of these features in the feature space. It can be seen that\nby interacting each atom with all the others, the model in-\ndeed learns the relationship between atoms. Speciﬁcally, it\nis obvious that nearby atoms share the same color, which\nmeans they are attended similarly since they may belong to\nthe same object parts. And it can be seen that far away atoms\nin the 3D space sharing the same implicit mode have simi-\nlar colors, which means inter-atoms long-range features like\nrelationship between far way semantic similar atoms (e.g.,\nthe ”same-category” information) has been discovered by\nthe model. A typical example that can prove the above anal-\nysis is: in Oxford, two walls of the same height share the\nsame color, which means atoms of them are attend similarly.\nIn the third row, we visualize which token that each non-\nempty voxel belongs to. Different color represents different\ntokens. It can be seen that voxels belong to the same to-\nken always represent the same objects and share some com-\nmon geometric characteristics. For example, in B.D., all the\natoms in the same ﬂowerbed (colored in Crimson) form a\ncube and are clustered together. This observation means that\nvoxels indeed have been clustered together in the feature\nspace according to their geometric characteristics. And ob-\nviously, the interaction between clusters or tokens could en-\nhance model’s understanding towards the scene. The inter-\nclusters long-range context properties like the relative posi-\ntions between clusters would be encoded through such kind\nof interaction. In a word, the visualization results have con-\nﬁrmed our intuition of designing ASVT and CSVT and they\nhave all contributed to the performance improvement.\nAblation Study\nWe have veriﬁed the effectiveness and efﬁciency of ASVT\nand CSVT in the Main Results section. Next, we ex-\nperimentally study the effectiveness of other key designs.\nSpeciﬁcally, we study the impact of the number of tokensLt,\ndimension of descriptors d, Transformer feature fusion strat-\negy and training stability. We design experiments from A to\nH for this study. Table 3 shows the results. ”SVT-Net” in the\nlast row of Table 3 refers to the model we ﬁnally choose.\nImpact of number of tokens: The number of tokens (L t)\ndecides how many clusters we divide the scene into. We\nchange the value of Lt and compare the results in Table 3.\nComparing the experiment A, B, C and SVT-Net, we ﬁnd\nthat setting Lt as 8 is the best choice. When Lt is too small,\ninteraction between different geometric characteristic (hid-\nden in different clusters) would be limited. When Lt is too\nlarge, it is easy to cause over-ﬁtting.\nImpact of descriptor’s dimension: To a certain extent, the\ndimension d determines the descriptor’s capability of de-\nscribing a scene. From experiment D, E, F and SVT-Net in\nTable 3, we ﬁnd that overall larger dimension leads to bet-\nter performance. However, when it is larger than 256, the\naccuracy increase is minimal while the model size is signiﬁ-\ncantly increased to 1.8M and 3.0M ford = 384and d = 512\nrespectively. Therefore, for a better trade-off between accu-\nracy and model size, we choose d = 256in our implemen-\ntation.\nImpact of fusion strategy: In SVT-Net, we need to fuse fea-\ntures learned by ASVT and CSVT before aggregating voxel\nfeatures into a global descriptor. In experiment G, we inves-\ntigate the effectiveness of another fusion method, concatena-\ntion. In this way, the output dimension is 512. However, the\nperformance of concatenating the two features is not as good\nas simply adding them (the dimension is 256). Then, we sus-\npect if it is the higher dimension that causes the performance\ndrop. Therefore, in experiment H, we add an additional SP-\nConv layer after concatenation to make the dimension be\n256. Unfortunately, the model’s performance becomes even\nworse than before. Therefore, ﬁnally, we believe that direct\nadding together is the best way to fuse the features of the\ntwo SVTs.\nTraining stability: We notice that for each training time,\nthere are some small differences on the evaluation results.\nTo avoid conclusion bias, we train each model for multiple\ntimes and show the boxplot of each model in Supp, which\nreﬂects the training stability of each model. Considering the\ntrade off between accuracy, model size, and training stabil-\nity, we claim that SVT-Net is the best performed model.\nConclusions\nIn this paper, we proposed a super light-weight network for\nlarge scale place recognition named SVT-Net. In SVT-Net,\ntwo Sparse V oxel Transformers: Atom-based Sparse V oxel\nTransformer (ASVT) and Cluster-based Sparse V oxel Trans-\nformer (CSVT) are proposed to learn long-range contextual\nproperties. Extensive experiments have demonstrated that\nSVT-Net as well as its two simpliﬁed versions ASVT-Net\nand CSVT-Net can all achieve state-of-the-art performance\nwith an extremely light-weight network architecture.\nAcknowledgements\nThis work was supported in part by National Key Re-\nsearch and Development Program of China under Grant No.\n2020YFB2104101 and National Natural Science Foundation\nof China (NSFC) under Grant Nos. 62172421, 71771131,\nU1711262 and 62072459.\nReferences\nArandjelovic, R.; Gronat, P.; Torii, A.; Pajdla, T.; and Sivic,\nJ. 2016. NetVLAD: CNN architecture for weakly supervised\nplace recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 5297–5307.\nChen, C.-F.; Fan, Q.; and Panda, R. 2021. CrossViT: Cross-\nAttention Multi-Scale Vision Transformer for Image Classi-\nﬁcation. arXiv preprint arXiv:2103.14899.\nChen, X.; L ¨abe, T.; Milioto, A.; R ¨ohling, T.; Vysotska, O.;\nHaag, A.; Behley, J.; Stachniss, C.; and Fraunhofer, F. 2020.\n558\nOverlapNet: Loop closing for LiDAR-based SLAM. In\nProc. of Robotics: Science and Systems (RSS).\nChen, Y .; Kalantidis, Y .; Li, J.; Yan, S.; and Feng, J.\n2018. A2-Nets: Double Attention Networks. arXiv preprint\narXiv:1810.11579.\nChoy, C.; Gwak, J.; and Savarese, S. 2019. 4d spatio-\ntemporal convnets: Minkowski convolutional neural net-\nworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 3075–3084.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFan, Z.; Liu, H.; He, J.; Sun, Q.; and Du, X. 2020. SRNet:\nA 3D Scene Recognition Network using Static Graph and\nDense Semantic Fusion. Computer Graphics Forum, 39(7):\n301–311.\nFern´andez-Moral, E.; Mayol-Cuevas, W.; Arevalo, V .; and\nGonzalez-Jimenez, J. 2013. Fast place recognition with\nplane-based maps. In 2013 IEEE International Conference\non Robotics and Automation, 2719–2724. IEEE.\nG´alvez-L´opez, D.; and Tardos, J. D. 2012. Bags of binary\nwords for fast place recognition in image sequences. IEEE\nTransactions on Robotics, 28(5): 1188–1197.\nGuo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin, R. R.;\nand Hu, S.-M. 2020. PCT: Point Cloud Transformer. arXiv\npreprint arXiv:2012.09688.\nHan, F.; Yang, X.; Deng, Y .; Rentschler, M.; Yang, D.;\nand Zhang, H. 2017. SRAL: Shared representative appear-\nance learning for long-term visual place recognition. IEEE\nRobotics and Automation Letters, 2(2): 1172–1179.\nHausler, S.; Garg, S.; Xu, M.; Milford, M.; and Fischer,\nT. 2021. Patch-NetVLAD: Multi-Scale Fusion of Locally-\nGlobal Descriptors for Place Recognition. arXiv preprint\narXiv:2103.01486.\nHu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation\nnetworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 7132–7141.\nJiang, Y .; Chang, S.; and Wang, Z. 2021. Transgan: Two\ntransformers can make one strong gan. arXiv preprint\narXiv:2102.07074.\nJohns, E.; and Yang, G.-Z. 2011. From images to scenes:\nCompressing an image cluster into a single scene model\nfor place recognition. In 2011 International Conference on\nComputer Vision, 874–881. IEEE.\nKim, W.; Son, B.; and Kim, I. 2021. ViLT: Vision-and-\nLanguage Transformer Without Convolution or Region Su-\npervision. arXiv preprint arXiv:2102.03334.\nKomorowski, J. 2021. MinkLoc3D: Point Cloud Based\nLarge-Scale Place Recognition. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, 1790–1799.\nLevinson, J.; Askeland, J.; Becker, J.; Dolson, J.; Held, D.;\nKammel, S.; Kolter, J. Z.; Langer, D.; Pink, O.; Pratt, V .;\net al. 2011. Towards fully autonomous driving: Systems and\nalgorithms. In 2011 IEEE Intelligent Vehicles Symposium\n(IV), 163–168. IEEE.\nLi, Y .; Snavely, N.; and Huttenlocher, D. P. 2010. Location\nrecognition using prioritized feature matching. In European\nconference on computer vision, 791–804. Springer.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLiu, Z.; Zhou, S.; Suo, C.; Yin, P.; Chen, W.; Wang, H.; Li,\nH.; and Liu, Y .-H. 2019. Lpd-net: 3d point cloud learning\nfor large-scale place recognition and environment analysis.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2831–2840.\nMaddern, W.; Pascoe, G.; Linegar, C.; and Newman, P. 2017.\n1 year, 1000 km: The Oxford RobotCar dataset. The Inter-\nnational Journal of Robotics Research, 36(1): 3–15.\nMur-Artal, R.; and Tard´os, J. D. 2017. Orb-slam2: An open-\nsource slam system for monocular, stereo, and rgb-d cam-\neras. IEEE Transactions on Robotics, 33(5): 1255–1262.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017. Pointnet:\nDeep learning on point sets for 3d classiﬁcation and segmen-\ntation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 652–660.\nRadenovi´c, F.; Tolias, G.; and Chum, O. 2018. Fine-tuning\nCNN image retrieval with no human annotation.IEEE trans-\nactions on pattern analysis and machine intelligence, 41(7):\n1655–1668.\nRavankar, A.; Ravankar, A. A.; Kobayashi, Y .; Hoshino, Y .;\nand Peng, C.-C. 2018. Path smoothing techniques in robot\nnavigation: State-of-the-art, current and future challenges.\nSensors, 18(9): 3170.\nSun, Q.; Liu, H.; He, J.; Fan, Z.; and Du, X. 2020. Dagc: Em-\nploying dual attention and graph convolution for point cloud\nbased place recognition. In Proceedings of the 2020 Inter-\nnational Conference on Multimedia Retrieval, 224–232.\nUy, M. A.; and Lee, G. H. 2018. Pointnetvlad: Deep point\ncloud based retrieval for large-scale place recognition. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 4470–4479.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research, 9(11).\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. arXiv preprint arXiv:1706.03762.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122.\nWu, B.; Xu, C.; Dai, X.; Wan, A.; Zhang, P.; Tomizuka, M.;\nKeutzer, K.; and Vajda, P. 2020. Visual transformers: Token-\nbased image representation and processing for computer vi-\nsion. arXiv preprint arXiv:2006.03677.\n559\nXia, Y .; Xu, Y .; Li, S.; Wang, R.; Du, J.; Cremers, D.; and\nStilla, U. 2021. Soe-net: A self-attention and orientation en-\ncoding network for point cloud based place recognition. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 11348–11357.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR.; and Le, Q. V . 2019. Xlnet: Generalized autoregres-\nsive pretraining for language understanding. arXiv preprint\narXiv:1906.08237.\nYu, J.; Zhu, C.; Zhang, J.; Huang, Q.; and Tao, D. 2019. Spa-\ntial pyramid-enhanced NetVLAD with weighted triplet loss\nfor place recognition. IEEE transactions on neural networks\nand learning systems, 31(2): 661–674.\nZhang, W.; and Xiao, C. 2019. PCAN: 3D attention map\nlearning using contextual information for point cloud based\nretrieval. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 12436–12445.\nZhao, H.; Jiang, L.; Jia, J.; Torr, P.; and Koltun, V . 2020.\nPoint transformer. arXiv preprint arXiv:2012.09164.\n560",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6708624958992004
    },
    {
      "name": "Transformer",
      "score": 0.6552996039390564
    },
    {
      "name": "Voxel",
      "score": 0.5120294690132141
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5044482946395874
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.45624107122421265
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4002854526042938
    },
    {
      "name": "Engineering",
      "score": 0.13615795969963074
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 77
}