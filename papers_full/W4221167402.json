{
  "title": "ViewFormer: NeRF-Free Neural Rendering from Few Images Using Transformers",
  "url": "https://openalex.org/W4221167402",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5038140870",
      "name": "Jonáš Kulhánek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2769456950",
      "name": "Erik Derner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2307601301",
      "name": "Torsten Sattler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214801287",
      "name": "Robert Babuška",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2892865870",
    "https://openalex.org/W3203570626",
    "https://openalex.org/W6631448583",
    "https://openalex.org/W3134943881",
    "https://openalex.org/W3035182575",
    "https://openalex.org/W3141835154",
    "https://openalex.org/W2795645133",
    "https://openalex.org/W2798952332",
    "https://openalex.org/W2144990732",
    "https://openalex.org/W2898255154",
    "https://openalex.org/W2165981022",
    "https://openalex.org/W4225463530",
    "https://openalex.org/W3187654524",
    "https://openalex.org/W2990256309",
    "https://openalex.org/W3095782429",
    "https://openalex.org/W2983230029",
    "https://openalex.org/W2808492412",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W4214661523",
    "https://openalex.org/W2042819305",
    "https://openalex.org/W2901982540",
    "https://openalex.org/W3175047242",
    "https://openalex.org/W4255443132",
    "https://openalex.org/W3202995573",
    "https://openalex.org/W2963024893",
    "https://openalex.org/W2605111497",
    "https://openalex.org/W2200124539",
    "https://openalex.org/W2964175348",
    "https://openalex.org/W2891630339",
    "https://openalex.org/W1616969904",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3041380886",
    "https://openalex.org/W3177583232",
    "https://openalex.org/W2592936284",
    "https://openalex.org/W3109585842",
    "https://openalex.org/W2974053236",
    "https://openalex.org/W4221151978",
    "https://openalex.org/W6850455183",
    "https://openalex.org/W2752796333",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W3203806429",
    "https://openalex.org/W3196466825",
    "https://openalex.org/W3109244019",
    "https://openalex.org/W3167914619",
    "https://openalex.org/W4214610961",
    "https://openalex.org/W2981978060",
    "https://openalex.org/W2962705366",
    "https://openalex.org/W3176602998",
    "https://openalex.org/W2522940611",
    "https://openalex.org/W2922243907",
    "https://openalex.org/W3138197200",
    "https://openalex.org/W2065343501",
    "https://openalex.org/W1989476314",
    "https://openalex.org/W2067132795",
    "https://openalex.org/W4214628039",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3186630079",
    "https://openalex.org/W3112108866",
    "https://openalex.org/W3136656704",
    "https://openalex.org/W3176368002",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W2043732461",
    "https://openalex.org/W3091598621"
  ],
  "abstract": null,
  "full_text": "ViewFormer: NeRF-free Neural Rendering\nfrom Few Images Using Transformers\nJon´ aˇ s Kulh´ anek1,2 , Erik Derner1 , Torsten Sattler1 , and Robert\nBabuˇ ska1,3\n1 Czech Institute of Informatics, Robotics and Cybernetics,\nCzech Technical University in Prague\n2 Faculty of Electrical Engineering, Czech Technical University in Prague\n3 Cognitive Robotics, Faculty of 3mE, Delft University of Technology\nAbstract. Novel view synthesis is a long-standing problem. In this work,\nwe consider a variant of the problem where we are given only a few con-\ntext views sparsely covering a scene or an object. The goal is to predict\nnovel viewpoints in the scene, which requires learning priors. The current\nstate of the art is based on Neural Radiance Field (NeRF), and while\nachieving impressive results, the methods suffer from long training times\nas they require evaluating millions of 3D point samples via a neural net-\nwork for each image. We propose a 2D-only method that maps multiple\ncontext views and a query pose to a new image in a single pass of a\nneural network. Our model uses a two-stage architecture consisting of a\ncodebook and a transformer model. The codebook is used to embed indi-\nvidual images into a smaller latent space, and the transformer solves the\nview synthesis task in this more compact space. To train our model effi-\nciently, we introduce a novel branching attentionmechanism that allows\nus to use the same model not only for neural rendering but also for cam-\nera pose estimation. Experimental results on real-world scenes show that\nour approach is competitive compared to NeRF-based methods while not\nreasoning explicitly in 3D, and it is faster to train.\nKeywords: Novel view synthesis; Neural rendering; Localization\n1 Introduction\nImage-based novel view synthesis, i.e., rendering a 3D scene from a novel view-\npoint given a set of context views (images and camera poses), is a long-standing\nproblem in computer graphics with applications ranging from robotics (e.g. plan-\nning to grasp objects) to augmented and virtual reality ( e.g. interactive virtual\nmeetings). Recently, the field has gained a lot of popularity thanks to Neural\nRadiance Field (NeRF) methods [2, 44] that were successfully applied to the\nproblem and outperformed prior approaches. We distinguish between two vari-\nants of the view synthesis problem. The first variant renders a novel view from\nmultiple context images taken from similar viewpoints [44, 75]. Only a (very)\nsparse set of context images is provided in the second variant [56,80], i.e., larger\narXiv:2203.10157v2  [cs.CV]  21 Jul 2022\n2 J. Kulh´ anek et al.\ncontext images GT generated context images GT generated\nFig. 1. Our novel view synthesis method renders images of previously unseen objects\nbased on a few context images. It operates in 2D space without any explicit 3D rea-\nsoning (as opposed to NeRF-based approaches [56, 80]). The results are shown on the\nCO3D [56] (right) and InteriorNet [35] ( left) datasets rendered for unseen scenes\nviewpoint variations and missing observations need to be handled. The latter\ntask is much more difficult as it is necessary to learn suitable priors that can be\nused to predict unseen scene parts. This paper focuses on the second variant.\nRecently, generalizable NeRF-based approaches have been proposed to tackle\nthis problem by learning priors for a class of objects and scenes [56,80]. Instead\nof learning a radiance field for each scene, they use context views captured\nfrom the target scene to construct the radiance field on the fly by projecting\nthe image features from all context views into 3D. Highly optimized NeRF ap-\nproaches [23, 47, 55, 79] can be sped up by tuning or caching the radiance field\nrepresentation [47], although often requiring lots of images per scene. To the best\nof our knowledge, these techniques do not apply to generalizable NeRF-based\nmethods that do not learn a scene-specific radiance field, and take thousands\nof GPU-hours to train [56]. In contrast, 2D-only feed-forward networks can be\nhighly efficient. However, explicitly encoding 3D geometric principles in them\ncan be challenging. In our work, we thus pose the question: Is reasoning in\n3D necessary for high-quality novel view synthesis, or can a purely image-based\nmethod achieve a competitive performance?\nRecently, Rombachet al. [59] successfully tackled single-view novel view syn-\nthesis, where the model was able to predict novel views without explicit 3D\nreasoning. Inspired by these findings, we tackle the more complex problem of\nmulti-view novel view synthesis. To answer the question, we propose a method\nwith no explicit 3D reasoning able to predict novel views using multiple context\nimages in a forward pass of a neural network. We train our model on a large\ncollection of diverse scenes to enable the model to learn 3D priors implicitly.\nOur approach is able to render a view in a novel scene, unseen at training time,\nthree orders of magnitude faster than state-of-the-art (SoTA) NeRF-based ap-\nproaches [56], while also being ten times faster to train. Furthermore, we are able\nto train a single model to render multiple classes of scenes (see Fig. 1), whereas\nthe SoTA NeRF-based approaches typically train per-class models [56].\nViewFormer: NeRF-free Neural Rendering from Few Images 3\nOur model uses a two-stage architecture consisting of a Vector Quantized-\nVariational Autoencoder (VQ-VAE) codebook [49] and a transformer model. The\ncodebook model is used to embed individual images into a smaller latent space.\nThe transformer solves the novel view synthesis task in this latent space before\nthe image is recovered via a decoder. This enables the codebook to focus on\nfiner details in images while the transformer operates on shorter input sequences,\nreducing the quadratic memory complexity of its attention layer.\nFor training, we pass a sequence of views into the transformer and optimize\nit for all context sizes at the same time, effectively utilizing all images in the\ntraining batch, which is different from other methods [21, 22, 50, 53] that train\nonly one query view. Unlike autoregressive models [22,50,53], we do not decode\nimages token-by-token but all tokens are decoded at once which is both faster\nand mathematically exact (while autoregressive models rely on greedy strate-\ngies). Our approach can be considered a combination of autoregressive [51, 74]\nand masked [18] transformer models. With the standard attention mechanism,\nthe complexity would be quadratic in the number of views, because we would\nhave to stack different query views corresponding to different context sizes along\nthe batch dimension. Therefore, we propose a novel attention mechanism called\nbranching attention with constant overhead regardless of how many query views\nwe optimize. Our attention mechanism also allows us to optimize the same model\nfor the camera pose estimation task – predicting the query image’s camera pose\ngiven a set of context views. Since this task can be considered an “inverse” of\nthe novel view synthesis task [78], we consider the ability to perform both tasks\nvia the same model to be an intriguing property. Even though the localization\nresults are not yet competitive with state-of-the-art localization pipelines, we\nachieve a similar level of pose accuracy as comparable methods such as [1,65].\nIn summary, this paper makes the following contributions: 1) We propose\nan efficient novel view synthesis approach that does not use explicit 3D reason-\ning. Our two-stage method consisting of a codebook model and a transformer\nis competitive with state-of-the-art NeRF-based approaches while being more\nefficient to train. Compared to similar methods that do not use explicit 3D\nreasoning [15, 21, 71], our approach is not only evaluated on synthetic data but\nperforms well on real-world scenes.2) Our transformer model is a combination of\nan autoregressive and a masked transformer. We propose a novel attention mech-\nanism called branching attention that allows us to optimize for multiple context\nsizes at once with a constant memory overhead. 3) Thanks to the branching\nattention, our model can both render a novel view from a given pose and pre-\ndict the pose for a given image. 4) Our source code and pre-trained models are\npublicly available at https://github.com/jkulhanek/viewformer.\n2 Related work\nNovel view synthesis has a long history [12, 68]. Recently, deep learning\ntechniques have been applied with great success, enabling higher realism [16,26,\n42,57,58]. Some approaches use explicit reconstructed geometry to warp context\n4 J. Kulh´ anek et al.\nimages into the target view [16,26,57,58,70]. In our approach, we do not require\nany proxy geometry and only operate on 2D images.\nNeural Radiance Field methods [2,29,39,42,44,55,79] use neural networks to\nrepresent the continuous volumetric scene function. To render a view, for each\npixel in the image plane, they project a ray into 3D space and query the radi-\nance field in 3D points along each ray. The radiance field is trained for each scene\nseparately. Some methods generalize to new scenes by conditioning the contin-\nuous volumetric function on the context images [60, 69], which allows them to\nutilize trained priors and render views from scenes on which the model was not\ntrained, much like our approach. Other approaches remove the trainable contin-\nuous volumetric scene function altogether. Instead, they reproject the context\nimage’s features into the 3D space and apply the NeRF-based rendering pipeline\non top of this representation [27, 56, 73, 75, 80]. Similarly to these methods, our\napproach also utilizes few context views (less than 20), and it also generalizes to\nunseen objects. However, we do not use the continuous volumetric function nor\nthe reprojection into the 3D space. A different approach, IBRNet [75], learns\nto copy existing colors from context views, effectively interpolating the context\nviews. Unlike ours, it thus cannot be applied to the settings where the object is\nnot covered enough by the context views [27,56,73,80].\nA different line of work directly maps 2D context images to the 2D query\nimage using an end-to-end neural network [15, 21, 71]. GQN-based methods\n[15, 21, 71] apply a CNN to context images and camera poses and combine the\nresulting features. While some GQN methods [15,21] do not use any explicit 3D\nreasoning (same as our approach), Tobin et al. [71] uses an epipolar attention\nto aggregate the features from the context views. We optimize our model on all\ncontext images and fully utilize the training sequences, whereas GQN methods\noptimize only a single query view.\nA recent work by Rombach et al. [59] proposed an approach for novel view\nsynthesis without explicit 3D modeling. They used a codebook and a transformer\nmodel to map a single context view to a novel view from a different pose. Their\napproach is limited in its scope to mostly forward-facing scenes where it is easier\nto render the novel view given a single context view and the poses have to be\nclose to one another. It cannot be extended to more views due to the limit on the\nsequence size of the transformer model. In contrast, in our approach, we focus on\nusing multiple context views, which we tackle through the proposed branching\nattention. Furthermore, we can jointly train the same model for both the novel\nview synthesis and camera pose estimation and our decoding is faster because\nwe decode the output at once instead of autoregressive decoding.\nVisual localization. There is an enormous body of work tackling the problem\nof localization, where the goal is to output the camera pose given the camera\nimage. Structure-based approaches use correspondences between 2D pixel posi-\ntions and 3D scene coordinates for camera pose estimation [6,11,37,41,61,63,67].\nOur method does not explicitly reason in 3D space, and the camera pose is in-\nstead predicted by the network. Simple image retrieval (IR) approaches store\na database of all images with camera poses and for each query image they try\nViewFormer: NeRF-free Neural Rendering from Few Images 5\nto find the most similar images [9, 10, 17, 28, 64, 82] and use them to estimate\nthe pose of the query. IR methods can also be used to select relevant images for\naccurate pose estimation [4,28,61,82,83].\nPose regression methods train a convolutional neural network (CNN) to\nregress the camera pose of an input image. There are two categories: absolute\npose regression (APR) methods [5,8,14,30,32,36,45,65] and relative pose regres-\nsion (RPR) methods [1, 19, 34, 36, 43]. It was shown [64] that APR is often not\n(much) more accurate than IR. RPR methods do not train a CNN per scene or\na set of scenes, but instead, condition the CNN on a set of context views. While\nour approach performs relative pose regression, the main focus of our method\nis on the novel view synthesis. Some pose regression methods use novel view\nsynthesis methods [14, 45, 46, 48], however, they assume there is a method that\ngenerates images, whereas our method performs both the novel view synthesis\nand camera pose regression in a single model. Iterative refinement pose regres-\nsion methods [62, 78] start with an initial camera pose estimate and refine it\nby an iterative process, however, our approach generates novel views and the\ncamera pose estimates in a single forward pass.\n3 Method\nIn this work, we tackle the problem of image-based novel view synthesis – given\na set of context views, the algorithm has to generate the image it would most\nlikely observe from a query camera pose. We focus on the case where the number\nof context views is small, and the views sparsely cover the 3D scene. Thus,\nthe algorithm must hallucinate parts of the scene in a manner consistent with\nthe context views. Therefore, it is necessary to learn a prior over a class of\nscenes (e.g., all indoor environments) and use this prior for novel scenes. Besides\nrendering novel views, our model can also perform camera pose estimation, i.e.,\nthe “inverse” of the view synthesis task: given a set of context views and a query\nimage, the model outputs the camera pose from which the image was taken.\nOur framework consists of two components: a codebook model and a trans-\nformer model. The codebook is used to map images to a smaller discrete latent\nspace (code space), and back to the image space. In the code space, each image\nis represented by a sequence of tokens. For the novel view synthesis task, the\ntransformer is given a set of context views in the code space and the query cam-\nera pose, and it generates an image in the code space. The codebook then maps\nthe image tokens back to the image space. See Fig. 2 for an overview. For the\ncamera pose estimation task, the transformer is given the set of context views\nand the query image in the code space, and it generates the camera pose using a\nregression head attached to the output of the transformer corresponding to the\nquery image tokens.\nHaving the codebook and the transformer as separate components was in-\nspired by the recent work on image generation [22,53,59]. The main motivation\nis to decrease it sequence size, because the required memory grows quadrati-\n6 J. Kulh´ anek et al.\nsi\ncontext images\nxi Eθ\n(si)\n(ci)\nTransformer\nEθ\nEθ\ngenerated image\nDθ\ncontext query\n+\nFig. 2.Inference pipeline. The context images xi are encoded by the codebook’s en-\ncoder Eθ to the code representation si. We embed all tokens in si, and add the trans-\nformed camera pose ci. The transformer generates the image tokens which are decoded\nby the codebook’s decoder Dθ\ncally with it. It also allows us to separate image generation and view synthesis,\nenabling us to train the transformer more efficiently in a simpler space.\nCodebook model is a VQ-VAE [49,54], which is a variational autoencoder with\na categorical distribution over the latent space. The model consists of two parts:\nthe encoder Eθ and decoder Dθ. The encoder first reduces the dimension of the\ninput image from 128 ×128 pixels to 8 ×8 tokens by several strided convolution\nlayers. The convolutional part is followed by a quantization layer, which maps\nthe resulting feature map to a discrete space. The quantization layer stores nlat\nembedding vectors of the same dimension as the feature vectors returned by the\nconvolutional part of the encoder. It encodes each point of the feature map by\nreturning the index of the closest embedding vector. The output of the encoder\nat position (i, j) for image x is:\narg min\nk\n∥(f(enc)\nθ (x))i,j − W(emb)\nk ∥2 , (1)\nwhere W(emb) ∈ Rnlat×dlat is the embedding matrix with rows W(emb)\nk of length\ndlat and f(enc)\nθ is the convolutional part of the encoder. The decoder then per-\nforms an inverse operation by first encoding the indices back to the embedding\nvectors by using W(emb) followed by several convolutional layers combined with\nupscaling to increase the spatial dimension back to the original image size.\nSince the operation in Eq. (1) is not differentiable, we approximate the gradi-\nent with a straight-through estimator [3] and copy the gradients from the decoder\ninput to the encoder output. The final loss for the codebook is a weighted sum of\nthree parts: the pixel-wise mean absolute error (MAE) between the input image\nand the reconstructed image, the perceptual loss between the input and recon-\nstructed image [22], and the commitment loss [49, 54] Lc, which encourages the\noutput of the encoder to stay close to the chosen embedding vector to prevent\nit from fluctuating too frequently from one vector to another:\nLc = min\nk\n||f(enc)\nθ (x)i,j − sg(W(emb)\nk )||2\n2 , (2)\nViewFormer: NeRF-free Neural Rendering from Few Images 7\ns1,c1\n. . .\ns5,c5 s6,c6\n∅,c6\ns6,∅\ns7,c7\n∅,c7\ns7,∅\n. . .\nsn,cn\n∅,cn\nsn,∅\npure context output part\nimage gen.localization\nFig. 3. Branching attentionmechanism: the nodes represent parts of the processed\nsequence. Starting in any node and tracing the arrows backwards gives the sequence\nover which the attention is computed,e.g., node s7, ∅ attends to s1, c1, s2, c2, . . . ,s7, ∅.\nBlue and red nodes in the last transformer block are used in the loss computation\nwhere sg is the stop-gradient operation [49]. We use the exponential moving aver-\nage updates for the codebook [49]. See [49,54] for more details on the codebook,\nand the supp. mat. for the architecture details.\nTransformer. We first describe the case of image generation and extend the\napproach to camera pose estimation later. We optimize the transformer for mul-\ntiple context sizes and multiple query views in the batch at the same time. This\nhas two benefits: it will allow the trained model to handle different context sizes,\nand the model will fully utilize the training batch (multiple images will be targets\nin the loss function). Each training batch consists of a set of n views. Let (xi)n\ni=1\nbe the sequence of images under a random ordering and ( ci)n\ni=1 be the sequence\nof the associated camera poses. Let us also define the sequence of images trans-\nformed by the encoder Eθ parametrized by θ as si = Eθ(xi), i = 1, . . . , n. Note\nthat each si is itself a sequence of tokens. With this formulation, we generate\nthe next image in the sequence given all the previous views, effectively opti-\nmizing all different context sizes at once. Therefore, we model the probability\np(si|s<i, c≤i). Note that we do not optimize the first nmin views (called the pure\ncontext), because they usually do not provide enough information for the task.\nIn practice, we need to replace the tokens corresponding to each query view\nwith mask tokens to allow the transformer to decode them in a single forward\npass. For the image generation task, the tokens of the last image in the sequence\nare replaced with special mask tokensλ, and, for the localization task, the tokens\nof the last image do not include the camera pose (denoted as ∅). However, if\nwe replaced the tokens in the training batch, the next query image would not\nbe able to perceive the original tokens. Therefore, we have to process both the\noriginal and the masked tokens. For the i-th query image, we need the sequence\nof i − 1 context views ending with masked tokens at the i-th position. We can\nrepresent the sequences as a tree (see Fig. 3) where different endings branch\noff the shared trunk. By following a leaf node back to the root of the tree, we\nrecover the original sequence corresponding to the particular query view.\nFor localization, we train the model to output the camera pose ci given s≤i\nand c<i. For image generation, this leads to n − nmin sequences. We attach a\nregression head to the hidden representation of all tokens of the last image in\n8 J. Kulh´ anek et al.\nthe sequence. The query image tokens form the input, and we mask the camera\nposes by replacing the camera pose representation with a single trainable vector.\nBranching attention. In this section, we introduce the branching attention\nwhich computes attention over the tree shown in Fig. 3, and allows us to optimize\nthe transformer model for all context sizes and tasks very efficiently. Note that\nwe have to forward all tree nodes through all layers of the transformer. Therefore,\nthe memory and time complexity is proportional to the number of nodes in the\ntree and thus to the number of views and tasks.\nThe input to the branching attention is a sequence of triplets of keys, values,\nand queries:\n\u0000\n(K(i), Q(i), V(i))\n\u0001p\ni=0 for p = 2, because we train the model on two\ntasks. Each element in the sequence corresponds to a single row in Fig. 3 and\ni = 0 is the middle row. All K(i), Q(i), V (i) have the size (nk2)×dm where dm is\nthe dimensionality of the model and k is the size of the image in the latent space.\nThe output of the branching attention is a sequence\n\u0000\nR(i)\u0001p\ni=0. The case of R(0)\nis handled differently because it corresponds to the trunk shared for all tasks\nand context sizes. Let us define a lower triangular matrix M ∈ Rn×n, where\nmi,j = 1 if i ≤ j. We compute the causal block attention as:\nR(0) = (softmax(Q(0)(K(0))T ) ⊙ M ⊗ 1k2×k2\n)V (0) , (3)\nwhere ⊗ and ⊙ are the Kronecker and element-wise product, respectively, and\n1m×n is a matrix of ones. Eq. (3) is similar to normal masked attention [74] with\nthe only difference in the causal mask. In this case, we allow the model to attend\nto all previous images and all other vectors from the same image. For i >0 we\ncan compute R(i) as follows:\nD = Q(i)(K(0))T , (4)\nC =\n\n\nQ(i)\n1:k2 (K(i)\n1:k2 )T\n...\nQ(i)\n(n−1)·k2+1:n·k2 (K(i)\n(n−1)·k2+1:n·k2 )T\n\n , (5)\nS = softmax([D, C]) ⊙ [(M − I) ⊗ 1k2×k2\n), 1nk2×k2\n] , (6)\nS′ = S·,1:n·k2 , S′′ = S·,n·k2+1:(n+1)·k2 , (7)\nR(i) = S′V (0) +\n\n\nS′′\n1:k2 V (i)\n1:k2\n...\nS′′\nn·k2+1:(n+1)·k2 V (i)\nn·k2+1:(n+1)·k2\n\n . (8)\nMatrix D represents the unmasked raw attention scores between i-th queries\nand keys from all previous images. Matrix C contains the raw pairwise attention\nscores between i-th queries and i-th keys (the ending of each sequence). Then,\nthe softmax is computed to normalize the attention scores and the causal mask is\napplied to the result, yielding the attention matrix S, and the respective values\nare weighted by the computed scores. In particular, the scores contained in the\nlast k2 columns of the attention matrix are redistributed back to the associated\nViewFormer: NeRF-free Neural Rendering from Few Images 9\ni-th values. The result R(0) corresponds to the nodes in the middle row in Fig. 3,\nwhereas R(i), i >0 are the other nodes.\nTransformer input and training. To build the input for the transformer,\nwe first embed all image tokens into trainable vector embeddings of length dm.\nBefore passing camera poses to the network, we express all camera poses relative\nto the first context camera pose in the sequence. We represent camera poses\nby concatenating the 3D position with the normalized orientation quaternion (a\nunit quaternion with a positive real part). Finally, we transform the camera poses\nwith a trainable feed-forward neural network in order to increase the dimension\nto the same size as image token embeddings dm in order to be able to sum them.\nSimilarly to [51], we also add the positional embeddings by summing the\ninput sequence with a sequence of trainable vectors. However, our positional\nembeddings are shared for all images in the sequence, i.e., the i-th token of\nevery image will share the same positional embedding.\nThe output of the last transformer block is passed to an affine layer followed\nby a softmax layer, and it is trained using the cross-entropy loss to recover the\nlast k2 tokens ( sj,1, . . . , sj,k2 ). For the localization task, the output is passed\nthrough a two-layer feed-forward neural network, and it is trained using the\nmean square error to match the ground-truth camera pose of the last k2 tokens.\nNote that we compute the losses over position and orientation separately and\nadd them together without weighing.4 Since we attach the pose prediction head\nto the hidden representation of all tokens of the query image, we obtain multiple\npose estimates. During inference, we simply average them.\n4 Experiments\nTo answer the question of whether explicit 3D reasoning is really needed for novel\nview synthesis, we designed a series of experiments evaluating the proposed ap-\nproach. First, we evaluate the codebook, whose performance is the upper bound\non what we can achieve with the full pipeline. We next compare our method to\nGQN-based methods [14,21,71] that also do not use continuous volumetric scene\nrepresentations. We continue by evaluating our approach on other synthetic data.\nThen, we compare our approach to state-of-the-art NeRF-based approaches on\na real-world dataset. Finally, we show our model’s localization performance.\nWe evaluate our approach on both real and synthetic datasets: a) Shepard-\nMetzler-7-Parts (SM7) [21,66] is a synthetic dataset, where objects composed\nof 7 cubes of different colors are rotated in space. b) ShapeNet [13] is a syn-\nthetic dataset of simple objects. We use 128 × 128 pixel images rendered by [69]\ncontaining two categories: cars and chairs. c) InteriorNet [35] is a collection\nof interior environments designed by 1,100 professional designers. We used the\npublicly available part of the dataset (20k scenes with 20 images each). While\nthe dataset is synthetic, the renderings are similar to real-world environments.\nThe first 600 environments serve as our test set. d) Common Objects in\n4 We tried dynamic weighting as described in [31], but it performed worse.\n10 J. Kulh´ anek et al.\nGT generated\n(a) InteriorNet [35]\nGT generated\n(b) CO3D [56]\nGT not-finetuned finetuned\n(c) 7-Scenes [24]\nFig. 4. Codebook evaluationon multiple datasets comparing the ground truth (GT)\nwith the reconstructed image. For the 7-Scenes dataset, we compare the model fine-\ntuned and not-finetuned on the 7-Scenes dataset\ncontext images GT GQN STR-GQN Ours\nFig. 5.Results on the SM7 dataset. We compare against GQN [21] and STR-GQN [15]\n3D (CO3D) [56] is a real-world dataset containing 1.5 million images show-\ning almost 19k objects from 51 MS-COCO [38] categories ( e.g., apple, donut,\nvase, etc.). The capture of the dataset was crowd-sourced. e) 7-Scenes [24] is\na real-world dataset depicting 7 indoor scenes as captured by a Kinect RGB-D\ncamera. The dataset consists of 44 sequences of 500–1,000 frames each and it is\na standard benchmark for visual localization [1,8,32,34,43].\nCodebook evaluation. First, we evaluate the quality of our codebooks by\nmeasuring the quality of the images generated by the encoder-decoder architec-\nture without the transformer. We trained codebooks of size 1,024 using the same\nhyperparameters for all experiments using an architecture very similar to [22].\nThe training took roughly 480 GPU-hours. A detailed description of the model\nand the hyperparameters is given in supp. mat. as well as in the published code.\nExamples of reconstructed images are shown in Fig. 4. As can be seen, al-\nthough losing some details and image sharpness, the codebooks can recover the\noverall shape well. The results show that using the codebook leads to good re-\nsults, even though we use only 8×8 codes to represent an image. In some images,\nthere are noticeable artifacts. In our analysis, we pinpointed the perceptual loss\nViewFormer: NeRF-free Neural Rendering from Few Images 11\nGT generated GT generated\nFig. 6.Evaluation of our method on the InteriorNet dataset with the context size 19\nto be the cause, but removing the perceptual loss led to more blurry images.\nFurther analysis of the codebooks is included in the supp. mat.\nFull method evaluation. The transformer is trained using only the tokens\ngenerated by the codebook. Having verified that our codebooks work as intended,\nwe evaluate our complete approach in the context of image synthesis. The archi-\ntecture of our transformer model is based on GPT2 [51]. We give more details\non the architecture, the motivation, and the hyperparameters in the supp. mat.\nThe SM7 dataset was used to compare our approach to other methods that\nonly operate in 2D image space [15,21,71]. Our method achieved the best mean\nabsolute error (MAE) of 1.61, followed by E-GQN [71] with 2.14, STR-GQN [14]\nwith 3.11 and the original GQN [21] method with MAE 3.13. The results were\naveraged over 1,000 scenes (context size was 3) and computed on images with\nsize 64 × 64 pixels. A qualitative comparison is shown in Fig. 5.\nWe use the InteriorNet dataset because of its large size and realistic appear-\nance. The models pre-trained on it are also used in other experiments. Since each\nscene provides 20 images, we use 19 context views. Fig. 6 shows images generated\nby the model trained for both the localization and novel view synthesis tasks.\nShapeNet evaluation. We used the InteriorNet pre-trained model and we fine-\ntuned it on the ShapeNet dataset. We trained a single model for both categories\n(cars and chairs) using 3 context views. The training details and additional\nresults are given in supp. mat. We show the qualitative comparison with Pixel-\nNeRF [80] in Fig. 7. PixelNeRF trained a different model for each category.\nThe results show that our method achieves good visual quality overall, es-\npecially on the cars dataset. However, the geometry is slightly distorted on the\nchairs. Compared to PixelNeRF, it prefers to hallucinate a part of the scene\ninstead of rendering a blurry image. This can cause some neighboring views to\nhave a different color or shape in places where the scene is less covered by context\nviews. However, this problem can be reduced by simply adding the previously\ngenerated view to the set of context views. See the video in the supp. mat.\nCommon Objects in 3D. In order to show that we can transfer a model pre-\ntrained on synthetic data to real-world scenes, we evaluate our method on the\nCO3D dataset [56]. We compare our approach with NeRF-based methods using\n12 J. Kulh´ anek et al.\ncontext views GT PixelNeRF ViewFormer\nFig. 7. ShapeNetqualitative comparison with PixelNeRF [80] using 2 context views\nTable 1. Novel view synthesisresults on the CO3D dataset [56] on all categories\nand 10 categories from [56]. We compare ViewFormer with and without localization\n(‘no-loc’) trained on all categories (‘@ all cat.’) and 10 selected categories (‘@ 10 cat.’).\nWe show the PSNR and LPIPS for seen and unseen scenes (‘train’ and ‘test’) and test\nPSNR with varying context size. The best value is bold; the second is underlined\navg. test avg. train PSNR ↑ @ # ctx. size\nEC Method 3D PSNR ↑ LPIPS↓ PSNR↑ LPIPS↓ 9 7 5 3 1\nall categories\nViewFormer @ all cat. ✗ 15.3 0.23 15.6 0.22 16.1 15.9 15.5 15.1 13.7\nViewFormer no-loc @ all cat. ✗ 15.4 0.23 15.8 0.22 16.2 16.0 15.6 15.2 13.8\nNerFormer [56] ✗ 15.7 0.24 16.5 0.24 16.7 16.4 16.1 15.5 13.9\nSRN+WCE ✗ 14.2 0.27 16.3 0.25 14.4 14.3 14.3 14.2 13.5\nSRN+WCE+γ ✗ 13.7 0.28 17.1 0.25 14.0 13.8 13.9 13.7 13.2\nNeRF+WCE [27] ✗ 11.6 0.27 12.6 0.27 11.9 11.8 11.8 11.6 10.8\n10 categories\nViewFormer @ 10 cat. ✗ 15.6 0.25 16.6 0.23 16.5 16.3 15.8 15.3 14.0\nViewFormer no-loc @ 10 cat. ✗ 15.6 0.25 17.1 0.22 16.5 16.2 15.8 15.3 14.0\nViewFormer @ all cat. ✗ 16.0 0.25 16.4 0.24 17.0 16.7 16.3 15.7 14.3\nViewFormer no-loc @ all cat. ✗ 16.1 0.25 16.6 0.23 17.0 16.8 16.3 15.8 14.3\nNerFormer [56] ✓ 17.6 0.27 17.9 0.26 18.9 18.6 18.1 17.1 15.1\nSRN+WCE+γ ✓ 14.4 0.27 17.6 0.24 14.6 14.5 14.6 14.5 13.9\nSRN+WCE ✓ 14.6 0.27 16.6 0.26 14.9 14.8 14.8 14.6 13.9\nNeRF+WCE [27] ✓ 13.8 0.27 14.3 0.27 12.6 14.5 14.4 14.2 13.8\nIPC+WCE ✓ 13.5 0.37 14.1 0.36 13.8 13.8 13.7 13.6 12.6\nP3DMesh ✓ 12.4 0.26 17.2 0.23 12.6 12.5 12.5 12.5 12.1\nNV+WCE ✓ 11.6 0.35 12.3 0.34 11.7 11.6 11.6 11.6 11.3\nthe results reported in [56]. Unfortunately, we tried to train the PixelNeRF [80]\non the CO3D dataset, but were not able to obtain good results. Therefore we\nomit it from the comparison. While the baselines are trained separately per\ncategory, we train two transformer models: one on the 10 categories used for\nViewFormer: NeRF-free Neural Rendering from Few Images 13\nGT generated GT generated\nFig. 8.Evaluation of our method on the CO3D dataset [56] with the context size 9\nevaluation in [56] and one for all dataset categories. We fine-tune the model\ntrained on the InteriorNet dataset. The context size is 9. Additional details and\nhyperparameters are given in supp. mat.\nThe testing set of each category in the CO3D dataset is split into two subsets:\n‘train’ and ‘test’ containing unseen images of objects seen and unseen during\ntraining respectively. We use the evaluation procedure provided by Reizenstein\net al. [56]. It evaluates the model on 1,000 sequences from each category with\ncontext sizes 1, 3, 5, 7, 9. The PSNR) and the LPIPS distance [81] are reported.\nNote that the PSNR is calculated only on foreground pixels. For more details on\nthe evaluation procedure and the details of compared methods, please see [56].\nTab. 1 shows results of the evaluation on all CO3D categories and on the 10\ncategories used for evaluation in [56]. Our method is competitive even though it\ndoes not explicitly reason in 3D as other baselines, does not utilize object masks,\nand even though we trained a single model for all categories while other baselines\nare trained per category. Note that on the whole dataset, the top-performing\nmethod, NerFormer [56], was trained for about 8400 GPU-hours while training\nour codebook took 480 GPU-hours, training the transformer on InteriorNet took\n280 GPU-hours, and fine-tuning the transformer took 90 GPU-hours, giving a\ntotal of 850 GPU-hours. Also, note that rendering a single view takes 178 s for\nthe NerFormer and only 93 ms for our approach.\nThe results show that our model has a large capacity (it is able to learn all\ncategories while the baselines are only trained on a single category), and it ben-\nefits from more training data as can be seen when comparing models trained on\n10 and all categories. We also observe that models achieve a higher performance\non 10 categories than on all categories, suggesting that the categories selected by\nthe authors of the dataset are easier to learn or of higher quality. All our models\noutperform all baselines in terms of LPIPS, which indicates that the images can\nlook more realistic while possibly not matching the real images very precisely.\nFig. 1 and 8 show qualitative results. Our method is able to generalize well\nto unseen object instances, although it tends to lose some details. To answer the\noriginal question if explicit 3D reasoning is needed for novel view synthesis, based\non our results, we claim that even without explicit 3D reasoning, we can achieve\nsimilar results, especially when the data are noisy, e.g. a real-world dataset.\n14 J. Kulh´ anek et al.\nEvaluating localization accuracy on 7-Scenes. We compare the local-\nization part of our approach to methods from the literature on the 7-Scenes\ndataset [24]. Due to space constraints, here we only summarize the results of the\ncomparisons. Detailed results can be found in the supp. mat.\nOur approach performs similar to existing APR and RPR techniques that also\nuse only a single forward pass in a network [1,8,32,65], but worse than iterative\napproaches such as [19] or methods that use more densely spaced synthetic\nviews as additional input [45]. Note that these approaches that do not use 3D\nscene geometry are less accurate than state-of-the-art methods based on 2D-3D\ncorrespondences [7,61,63]. Overall, the results show that our approach achieves a\nsimilar level of pose accuracy as comparable methods. Furthermore, our approach\nis able to perform both localization and novel view synthesis in a simple forward\npass, while other methods can only be used for localization.\n5 Conclusions & future work\nThis paper presents a two-stage approach to novel view synthesis from a few\nsparsely distributed context images. We train our model on classes of similar 3D\nscenes to be able to generalize to a novel scene with only a handful of images\nas opposed to NeRF and similar methods that are trained per scene. The model\nconsists of a VQ-VAE codebook [49] and a transformer model. To efficiently\ntrain the transformer, we propose a novel branching attention module. Our ap-\nproach, ViewFormer, can render a view from a previously unseen scene in 93 ms\nwithout any explicit 3D reasoning and we train a single model to render mul-\ntiple categories of objects, whereas NeRF-based approaches train per-category\nmodels [56]. We show that our method is competitive with SoTA NeRF-based\napproaches especially on real-world data, even without any explicit 3D reasoning.\nThis is an intriguing result because it implies that either current NeRF-based\nmethods are not utilizing the 3D priors effectively or that a 2D-only model is\nable to learn it on its own without explicit 3D modeling. The experiments also\nshow that ViewFormer outperforms other 2D-only multi-view methods.\nOne limitation of our approach is the large amount of data needed, which we\ntackle through pre-training on a large synthetic dataset. Also, we need to fine-\ntune both the codebook and the transformer to achieve high-quality results on\nnew datasets, which could be resolved by utilizing a larger codebook trained on\nmore data. Using more tokens to represent images should increase the rendering\nquality and pose accuracy. We also want to experiment with a simpler architec-\nture with no codebook and larger scenes, possibly of outdoor environments.\nAcknowledgements. This work was supported by the European Regional De-\nvelopment Fund under projects IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15 003/\n0000468) and Robotics for Industry 4.0 (reg. no. CZ.02.1.01/0.0/0.0/15 003/\n0000470), the EU Horizon 2020 project RICAIP (grant agreement No 857306),\nthe Grant Agency of the Czech Technical University in Prague (grant no. SGS22/\n112/OHK3/2T/13), and the Ministry of Education, Youth and Sports of the\nCzech Republic through the e-INFRA CZ (ID:90140).\nViewFormer: NeRF-free Neural Rendering from Few Images 15\nReferences\n1. Balntas, V., Li, S., Prisacariu, V.: RelocNet: Continuous metric learning relocali-\nsation using neural nets. In: Proceedings of the European Conference on Computer\nVision (ECCV). pp. 751–767 (2018) 3, 5, 10, 14, 25, 27\n2. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-\nvasan, P.P.: Mip-NeRF: A multiscale representation for anti-aliasing neural radi-\nance fields. In: Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision. pp. 5855–5864 (2021) 1, 4\n3. Bengio, Y., L´ eonard, N., Courville, A.: Estimating or propagating gradi-\nents through stochastic neurons for conditional computation. arXiv preprint\narXiv:1308.3432 (2013) 6\n4. Bhayani, S., Sattler, T., Barath, D., Beliansky, P., Heikkil¨ a, J., Kukelova, Z.: Cali-\nbrated and partially calibrated semi-generalized homographies. In: Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 5\n5. Blanton, H., Greenwell, C., Workman, S., Jacobs, N.: Extending absolute pose\nregression to multiple scenes. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops. pp. 38–39 (2020) 5\n6. Brachmann, E., Rother, C.: Visual camera re-localization from RGB and RGB-D\nimages using DSAC. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence pp. 1–1 (2021) 4, 27\n7. Brachmann, E., Rother, C.: Visual camera re-localization from RGB and RGB-D\nimages using DSAC. IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence (2021) 14, 25\n8. Brahmbhatt, S., Gu, J., Kim, K., Hays, J., Kautz, J.: Geometry-aware learning of\nmaps for camera localization. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition. pp. 2616–2625 (2018) 5, 10, 14, 25, 27\n9. Camposeco, F., Cohen, A., Pollefeys, M., Sattler, T.: Hybrid camera pose estima-\ntion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. pp. 136–144 (2018) 5\n10. Cao, S., Snavely, N.: Graph-based discriminative learning for location recognition.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition. pp. 700–707 (2013) 5\n11. Cavallari, T., Golodetz, S., Lord, N.A., Valentin, J., Prisacariu, V.A., Di Stefano,\nL., Torr, P.H.S.: Real-time RGB-D camera pose estimation in novel scenes us-\ning a relocalisation cascade. IEEE transactions on pattern analysis and machine\nintelligence 42(10), 2465–2477 (2019) 4\n12. Chan, S., Shum, H.Y., Ng, K.T.: Image-based rendering and synthesis. IEEE Signal\nProcessing Magazine 24(6), 22–33 (2007) 3\n13. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,\nSavarese, S., Savva, M., Song, S., Su, H., et al.: ShapeNet: An information-rich\n3D model repository. arXiv preprint arXiv:1512.03012 (2015) 9\n14. Chen, S., Wang, Z., Prisacariu, V.: Direct-posenet: Absolute pose regression with\nphotometric consistency. arXiv preprint arXiv:2104.04073 (2021) 5, 9, 11, 33\n15. Chen, W.C., Hu, M.C., Chen, C.S.: STR-GQN: Scene representation and rendering\nfor unknown cameras based on spatial transformation routing. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 5966–5975\n(2021) 3, 4, 10, 11, 30, 33\n16. Choi, I., Gallo, O., Troccoli, A., Kim, M.H., Kautz, J.: Extreme view synthesis.\nIn: Proceedings of the IEEE/CVF International Conference on Computer Vision.\npp. 7781–7790 (2019) 3, 4\n16 J. Kulh´ anek et al.\n17. Derner, E., Gomez, C., Hernandez, A.C., Barber, R., Babuˇ ska, R.: Change de-\ntection using weighted features for image-based localization. Robotics and Au-\ntonomous Systems 135, 103676 (2021) 5\n18. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\npp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota\n(Jun 2019) 3, 28\n19. Ding, M., Wang, Z., Sun, J., Shi, J., Luo, P.: CamNet: Coarse-to-fine retrieval for\ncamera re-localization. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision. pp. 2871–2880 (2019) 5, 14, 25, 27\n20. Engilberge, M., Collins, E., Susstrunk, S.: Color representation in deep neural net-\nworks. In: Proceedings of the IEEE International Conference on Image Processing.\npp. 2786–2790 (2017) 25\n21. Eslami, S.A., Rezende, D.J., Besse, F., Viola, F., Morcos, A.S., Garnelo, M., Ruder-\nman, A., Rusu, A.A., Danihelka, I., Gregor, K., et al.: Neural scene representation\nand rendering. Science 360(6394), 1204–1210 (2018) 3, 4, 9, 10, 11, 30, 33, 34, 35\n22. Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image\nsynthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 12873–12883 (2021) 3, 5, 6, 10, 28, 34, 36\n23. Garbin, S.J., Kowalski, M., Johnson, M., Shotton, J., Valentin, J.: FastNeRF:\nHigh-fidelity neural rendering at 200FPS. In: Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision. pp. 14346–14355 (2021) 2\n24. Glocker, B., Izadi, S., Shotton, J., Criminisi, A.: Real-time RGB-D camera relocal-\nization. In: 2013 IEEE International Symposium on Mixed and Augmented Reality\n(ISMAR). pp. 173–179. IEEE (2013) 10, 14, 21, 25, 26, 34\n25. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016) 36\n26. Hedman, P., Philip, J., Price, T., Frahm, J.M., Drettakis, G., Brostow, G.: Deep\nblending for free-viewpoint image-based rendering. ACM Transactions on Graphics\n(TOG) 37(6), 1–15 (2018) 3, 4\n27. Henzler, P., Reizenstein, J., Labatut, P., Shapovalov, R., Ritschel, T., Vedaldi, A.,\nNovotny, D.: Unsupervised learning of 3D object categories from videos in the wild.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 4700–4709 (2021) 4, 12\n28. Irschara, A., Zach, C., Frahm, J.M., Bischof, H.: From structure-from-motion point\nclouds to fast location recognition. In: 2009 IEEE Conference on Computer Vision\nand Pattern Recognition. pp. 2599–2606. IEEE (2009) 5\n29. Jain, A., Tancik, M., Abbeel, P.: Putting nerf on a diet: Semantically consistent\nfew-shot view synthesis. In: Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision. pp. 5885–5894 (2021) 4\n30. Kendall, A., Cipolla, R.: Modelling uncertainty in deep learning for camera relo-\ncalization. In: 2016 IEEE International Conference on Robotics and Automation\n(ICRA). pp. 4762–4769. IEEE (2016) 5\n31. Kendall, A., Cipolla, R.: Geometric loss functions for camera pose regression with\ndeep learning. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. pp. 5974–5983 (2017) 9\nViewFormer: NeRF-free Neural Rendering from Few Images 17\n32. Kendall, A., Grimes, M., Cipolla, R.: PoseNet: A convolutional network for real-\ntime 6-DOF camera relocalization. In: Proceedings of the IEEE International Con-\nference on Computer Vision. pp. 2938–2946 (2015) 5, 10, 14, 25, 27\n33. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR\n(Poster) (2015) 34\n34. Laskar, Z., Melekhov, I., Kalia, S., Kannala, J.: Camera relocalization by comput-\ning pairwise relative poses using convolutional neural network. In: Proceedings of\nthe IEEE International Conference on Computer Vision Workshops. pp. 929–938\n(2017) 5, 10\n35. Li, W., Saeedi, S., McCormac, J., Clark, R., Tzoumanikas, D., Ye, Q., Huang,\nY., Tang, R., Leutenegger, S.: InteriorNet: Mega-scale multi-sensor photo-realistic\nindoor scenes dataset. In: British Machine Vision Conference (BMVC) (2018) 2,\n9, 10, 21, 22, 27, 28, 29, 30, 34\n36. Li, X., Ling, H.: TransCamP: Graph transformer for 6-DoF camera pose estimation.\nArXiv abs/2105.14065 (2021) 5\n37. Li, Y., Snavely, N., Huttenlocher, D.P., Fua, P.: Worldwide Pose Estimation Using\n3D Point Clouds. In: ECCV (2012) 4\n38. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar,\nP., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: European\nConference on Computer Vision. pp. 740–755. Springer (2014) 10\n39. Liu, L., Gu, J., Zaw Lin, K., Chua, T.S., Theobalt, C.: Neural sparse voxel fields.\nAdvances in Neural Information Processing Systems 33 (2020) 4\n40. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International\nConference on Learning Representations (2018) 35\n41. Lynen, S., Zeisl, B., Aiger, D., Bosse, M., Hesch, J., Pollefeys, M., Siegwart, R.,\nSattler, T.: Large-scale, real-time visual–inertial localization revisited. The Inter-\nnational Journal of Robotics Research 39(9), 1061–1084 (2020) 4\n42. Martin-Brualla, R., Radwan, N., Sajjadi, M.S., Barron, J.T., Dosovitskiy, A., Duck-\nworth, D.: NeRF in the wild: Neural radiance fields for unconstrained photo col-\nlections. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. pp. 7210–7219 (2021) 3, 4\n43. Melekhov, I., Ylioinas, J., Kannala, J., Rahtu, E.: Relative camera pose estimation\nusing convolutional neural networks. In: International Conference on Advanced\nConcepts for Intelligent Vision Systems. pp. 675–687. Springer (2017) 5, 10\n44. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,\nR.: NeRF: Representing scenes as neural radiance fields for view synthesis. In:\nEuropean Conference on Computer Vision. pp. 405–421. Springer (2020) 1, 4\n45. Moreau, A., Piasco, N., Tsishkou, D., Stanciulescu, B., de La Fortelle, A.: LENS:\nLocalization enhanced by NeRF synthesis. In: 5th Annual Conference on Robot\nLearning (2021) 5, 14, 25, 27\n46. Mueller, M.S., Sattler, T., Pollefeys, M., Jutzi, B.: Image-to-image translation for\nenhanced feature matching, image retrieval and visual localization. ISPRS Annals\nof the Photogrammetry, Remote Sensing and Spatial Information Sciences 4, 111–\n119 (2019) 5\n47. M¨ uller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives\nwith a multiresolution hash encoding. arXiv preprint arXiv:2201.05989 (2022) 2\n48. Ng, T., Lopez-Rodriguez, A., Balntas, V., Mikolajczyk, K.: Reassessing the Lim-\nitations of CNN Methods for Camera Pose Regression. arXiv:2108.07260 (2021)\n5\n18 J. Kulh´ anek et al.\n49. van den Oord, A., Vinyals, O., kavukcuoglu, k.: Neural discrete representation\nlearning. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vish-\nwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Sys-\ntems. vol. 30. Curran Associates, Inc. (2017) 3, 6, 7, 14\n50. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:\nImage transformer. In: International Conference on Machine Learning. pp. 4055–\n4064. PMLR (2018) 3, 28\n51. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nmodels are unsupervised multitask learners (2019) 3, 9, 11, 28, 35\n52. Ramachandran, P., Zoph, B., Le, Q.V.: Searching for activation functions. arXiv\npreprint arXiv:1710.05941 (2017) 36\n53. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,\nSutskever, I.: Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092\n(2021) 3, 5, 28, 34, 36\n54. Razavi, A., van den Oord, A., Vinyals, O.: Generating diverse high-fidelity images\nwith vq-vae-2. In: Wallach, H., Larochelle, H., Beygelzimer, A., d 'Alch´ e-Buc, F.,\nFox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems.\nvol. 32. Curran Associates, Inc. (2019) 6, 7\n55. Reiser, C., Peng, S., Liao, Y., Geiger, A.: KiloNeRF: Speeding up neural radiance\nfields with thousands of tiny MLPs. In: Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision. pp. 14335–14345 (2021) 2, 4\n56. Reizenstein, J., Shapovalov, R., Henzler, P., Sbordone, L., Labatut, P., Novotny,\nD.: Common objects in 3D: Large-scale learning and evaluation of real-life 3D cat-\negory reconstruction. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision. pp. 10901–10911 (2021) 1, 2, 4, 10, 11, 12, 13, 14, 21, 22, 23,\n34, 35\n57. Riegler, G., Koltun, V.: Free view synthesis. In: European Conference on Computer\nVision. pp. 623–640. Springer (2020) 3, 4\n58. Riegler, G., Koltun, V.: Stable view synthesis. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 12216–12225 (2021)\n3, 4\n59. Rombach, R., Esser, P., Ommer, B.: Geometry-free view synthesis: Transformers\nand no 3d priors. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 14356–14366 (2021) 2, 4, 5\n60. Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: PIFu:\nPixel-aligned implicit function for high-resolution clothed human digitization. In:\nProceedings of the IEEE/CVF International Conference on Computer Vision. pp.\n2304–2314 (2019) 4\n61. Sarlin, P.E., Cadena, C., Siegwart, R., Dymczyk, M.: From coarse to fine: Robust\nhierarchical localization at large scale. In: CVPR (2019) 4, 5, 14, 25, 27\n62. Sarlin, P.E., Unagar, A., Larsson, M., Germain, H., Toft, C., Larsson, V., Pollefeys,\nM., Lepetit, V., Hammarstrand, L., Kahl, F., et al.: Back to the feature: Learning\nrobust camera localization from pixels to pose. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 3247–3257 (2021) 5\n63. Sattler, T., Leibe, B., Kobbelt, L.: Efficient & effective prioritized matching for\nlarge-scale image-based localization. IEEE Transactions on Pattern Analysis and\nMachine Intelligence 39(9), 1744–1756 (2016) 4, 14, 25, 27\n64. Sattler, T., Zhou, Q., Pollefeys, M., Leal-Taixe, L.: Understanding the limitations\nof CNN-based absolute camera pose regression. In: Proceedings of the IEEE/CVF\nConference On computer Vision and Pattern Recognition. pp. 3302–3312 (2019)\n5, 25, 27\nViewFormer: NeRF-free Neural Rendering from Few Images 19\n65. Shavit, Y., Ferens, R., Keller, Y.: Learning multi-scene absolute pose regression\nwith transformers. arXiv preprint arXiv:2103.11468 (2021) 3, 5, 14, 25, 27\n66. Shepard, R.N., Metzler, J.: Mental rotation of three-dimensional objects. Science\n171(3972), 701–703 (1971) 9, 30, 34, 35\n67. Shotton, J., Glocker, B., Zach, C., Izadi, S., Criminisi, A., Fitzgibbon, A.: Scene\nCoordinate Regression Forests for Camera Relocalization in RGB-D Images. In:\nCVPR (2013) 4\n68. Shum, H., Kang, S.B.: Review of image-based rendering techniques. In: Visual\nCommunications and Image Processing 2000. vol. 4067, pp. 2–13. International\nSociety for Optics and Photonics (2000) 3\n69. Sitzmann, V., Zollh¨ ofer, M., Wetzstein, G.: Scene representation networks: Con-\ntinuous 3D-structure-aware neural scene representations. Advances in Neural In-\nformation Processing Systems 32 (2019) 4, 9, 29, 30\n70. Thies, J., Zollh¨ ofer, M., Theobalt, C., Stamminger, M., Nießner, M.: Image-guided\nneural object rendering. In: 8th International Conference on Learning Representa-\ntions. OpenReview. net (2020) 4\n71. Tobin, J., Zaremba, W., Abbeel, P.: Geometry-aware neural rendering. Advances\nin Neural Information Processing Systems 32, 11559–11569 (2019) 3, 4, 9, 11, 30,\n33\n72. Torii, A., Arandjelovic, R., Sivic, J., Okutomi, M., Pajdla, T.: 24/7 place recog-\nnition by view synthesis. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition. pp. 1808–1817 (2015) 25\n73. Trevithick, A., Yang, B.: GRF: Learning a general radiance field for 3d represen-\ntation and rendering. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision. pp. 15182–15192 (2021) 4\n74. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. pp. 5998–6008 (2017) 3, 8, 28\n75. Wang, Q., Wang, Z., Genova, K., Srinivasan, P.P., Zhou, H., Barron, J.T., Martin-\nBrualla, R., Snavely, N., Funkhouser, T.: IBRNET: Learning multi-view image-\nbased rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition. pp. 4690–4699 (2021) 1, 4\n76. Wang, Z., Bovik, A.C.: Mean squared error: Love it or leave it? a new look at signal\nfidelity measures. IEEE signal processing magazine 26(1), 98–117 (2009) 29\n77. Wu, Y., He, K.: Group normalization. In: Proceedings of the European conference\non computer vision (ECCV). pp. 3–19 (2018) 36\n78. Yen-Chen, L., Florence, P., Barron, J.T., Rodriguez, A., Isola, P., Lin, T.Y.: iNeRF:\nInverting neural radiance fields for pose estimation. In: IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) (2021) 3, 5\n79. Yu, A., Li, R., Tancik, M., Li, H., Ng, R., Kanazawa, A.: PlenOctrees for real-time\nrendering of neural radiance fields. In: Proceedings of the IEEE/CVF International\nConference on Computer Vision. pp. 5752–5761 (2021) 2, 4\n80. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelNeRF: Neural radiance fields from\none or few images. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 4578–4587 (2021) 1, 2, 4, 11, 12, 22, 29, 30,\n31, 32\n81. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable\neffectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. pp. 586–595 (2018) 13,\n29, 34\n20 J. Kulh´ anek et al.\n82. Zhang, W., Kosecka, J.: Image based localization in urban environments. In: Third\ninternational symposium on 3D data processing, visualization, and transmission\n(3DPVT’06). pp. 33–40. IEEE (2006) 5\n83. Zhou, Q., Sattler, T., Pollefeys, M., Leal-Taix´ e, L.: To learn or not to learn: Visual\nlocalization from essential matrices. In: 2020 IEEE International Conference on\nRobotics and Automation (ICRA). pp. 3319–3326 (2020) 5\nViewFormer: NeRF-free Neural Rendering\nfrom Few Images Using Transformers\n–\nSupplementary Material\nJon´ aˇ s Kulh´ anek1,2 , Erik Derner1 , Torsten Sattler1 , and Robert\nBabuˇ ska1,3\n1 Czech Institute of Informatics, Robotics and Cybernetics,\nCzech Technical University in Prague\n2 Faculty of Electrical Engineering, Czech Technical University in Prague\n3 Cognitive Robotics, Faculty of 3mE, Delft University of Technology\nhttps://jkulhanek.github.io/viewformer\nIn this supplementary material, we give more details on the results presented\nin the main paper and provide more details on the network architecture. First,\nin Sec. A, we present additional qualitative results on various datasets. We also\nshow examples of context views used to render the final view. The attached\nvideo is described in Sec. B. We include the camera pose estimation results on\nthe 7-Scenes dataset [24] in Sec. C, and we also show qualitative results of the\nnovel view synthesis task on the same dataset. In Sec. D, we present an ablation\nstudy. We also show how the performance increases with larger context sizes. In\nSections E and F, we include additional results on the ShapeNet dataset and\nthe Shepard-Metzler-Parts-7 (SM7) dataset, respectively. Quantitative results of\nthe codebook model are given in Sec. G. Finally, we give details on the training\nhyperparameters and architecture of the models in Sections H and I.\nA Qualitative results\nWe add qualitative results to the ones presented in the paper (see Fig. 1, 6,\nand 8 in the main paper). We show the context views together with the rendered\nimages on the InteriorNet [35], the Common Objects in 3D (CO3D) [56], and\nthe 7-Scenes [24] datasets. The generated images are displayed in Fig. 9, Fig. 10,\nand Fig. 12, respectively. We also show images generated with full context sizes\nin Fig. 11. It is important to note that all the visualizations, including the video,\nwere rendered on previously unseen scenes (objects).\nThe images rendered on the largest and most complex dataset – InteriorNet,\nalthough slightly blurry, resemble the ground truth (GT) images well. For the\n7-Scenes dataset, the trained model overfitted the data, and the quality of the\ngenerated images was not as good as on other datasets. Notice how the image\nrendered on CO3D is smoother than the ground truth image. In the case of the\nflower pot (Fig. 10), we can see that the model could not represent the particular\nshape and used a simpler shape instead. This is an intriguing property of the\n22 J. Kulh´ anek et al.\ncontext images GT generated\nFig. 9.Visualization of the model trained on the InteriorNet dataset [35]. We show the\nimages generated with context size 8 while the model was trained with context size 19\nmodel which in the case of incomplete information uses its large prior to achieve\nmore realistic renderings at the cost of being less similar to the real object.\nB Attached video\nWe attach a video file 4 showing the generated images on various datasets. The\nvideo contains the results generated on the ShapeNet, CO3D, InteriorNet, and\n7-Scenes datasets. On the ShapeNet dataset, we compare our model with Pixel-\nNeRF [80]. We render video sequences of rotating objects using the same three\ncontext views. For the CO3D dataset, we show video sequences of rotating ob-\njects using 9 context views. We also show how the model changes its predic-\ntion given more context views. Unfortunately, we cannot compare with Pixel-\nNeRF [80] because the method was not able to converge properly on the dataset\n(see Sec. 4 in the main paper). Also, we cannot compare with NerFormer [56]\nbecause the source code is not publicly available. Finally, we show the results on\nthe InteriorNet dataset as well as on all scenes from the 7-Scenes dataset.\nOne might expect that with the discrete codebook codes the learned repre-\nsentation would be quantized and an arbitrary pose could not be represented\nby the model. However, from the sequences generated on the ShapeNet dataset,\nwe can see that this problem does not occur and the model is able to capture\nthe motion, smoothly transitioning between the true poses. Therefore, although\nthe codes are discrete, they can represent a continuous range of objects’ orien-\ntations and positions. It is interesting to see that our approach is occasionally\n4 https://jkulhanek.com/viewformer/video.html\nViewFormer – Supplementary Material 23\nctx. images GT generated\nFig. 10.Visualization of the model trained on the CO3D dataset [56]. We show the\nimages generated with context sizes 1, 4, and 8 while the model was trained with\ncontext size 9\n24 J. Kulh´ anek et al.\nGT generated GT generated\nFig. 11.Images generated on the InteriorNet dataset ( left) with context size 19 and\nthe CO3D dataset ( right) with context size 9. For the CO3D evaluation, we used the\nmodel trained on all categories\nViewFormer – Supplementary Material 25\nTable 2. Camera pose estimation accuracy on the 7-Scenes dataset [24], reported as\nthe mean median position (in meters) and orientation (in degrees) errors over all scenes.\nWe report results with an InteriorNet pre-trained codebook (‘-in’) and a codebook\nfine-tuned on 7-Scenes (‘-7s’). We further compare a simple decoding scheme (random\ncontext views) with a variant that uses the top-10 most similar training images for\neach query view (‘top10’), identified via image retrieval\nAll Chess Fire Heads Office Pumpkin Kitchen Stairs\nMethod Pos/Ori Pos/Ori Pos/Ori Pos/Ori Pos/Ori Pos/Ori Pos/Ori Pos/Ori\nViewFormer-in 0 .24/10.49 0 .16/8.03 0 .24/11.35 0.17/13.23 0.25/10.33 0.23/8.20 0 .31/11.01 0.30/11.28\nViewFormer-in-top10 0 .19/7.82 0 .13/6.36 0 .22/10.27 0.17/10.85 0.17/6.42 0 .19/6.26 0 .21/6.62 0 .21/7.97\nViewFormer-7s 0 .23/8.46 0 .15/6.31 0 .23/10.03 0.19/12.68 0.23/7.69 0 .19/5.59 0 .27/7.75 0 .31/9.18\nViewFormer-7s-top10 0 .17/6.68 0 .12/4.85 0 .20/8.65 0 .17/10.41 0.15/5.11 0 .16/4.78 0 .18/5.01 0 .22/7.93\nOracle-top10 0 .21/10.01 0 .18/9.16 0 .27/10.37 0.12/11.44 0.22/8.33 0 .24/8.20 0 .26/9.72 0 .19/12.85\nPoseNet [32] 0 .44/10.4 0 .32/8.12 0 .47/14.4 0 .29/12.0 0 .48/7.68 0 .47/8.42 0 .59/8.64 0 .47/13.8\nMapNet [8] 0 .18/6.56 0 .09/3.24 0 .20/9.29 0 .12/8.45 0 .19/5.45 0 .19/3.96 0 .20/4.94 0 .27/10.57\nLENS [45] 0 .05/2.5 0 .04/2.0 0 .03/1.5 0 .02/1.5 0 .09/3.6 0 .08/3.1 0 .07/3.4 0 .03/2.2\nMS-Transformer [65] 0 .18/7.28 0 .11/4.66 0 .24/9.6 0 .14/12.19 0.17/5.66 0 .18/4.44 0 .17/5.94 0 .26/8.45\nRelocNet [1] 0 .21/6.72 0 .12/4.14 0 .26/10.4 0 .14/10.5 0 .18/5.32 0 .26/4.17 0 .23/5.0 0 .28/7.53\nCamNet [19] 0 .04/1.69 0 .04/1.73 0 .03/1.74 0 .05/1.98 0 .04/1.62 0 .04/1.64 0 .04/1.63 0 .04/1.51\nDenseVLAD [64,72] 0 .26/13.1 0 .21/12.5 0 .33/13.8 0 .15/14.9 0 .28/11.2 0 .31/11.3 0 .30/12.3 0 .25/15.8\nDenseVLAD+Int. [64] 0.24/11.7 0 .18/10.0 0 .33/12.4 0 .14/14.3 0 .25/10.1 0 .26/9.42 0 .27/11.1 0 .24/14.7\nDSAC* [7] 0 .03/1.36 0 .02/1.10 0 .02/1.24 0 .01/1.82 0 .03/1.15 0 .04/1.34 0 .04/1.68 0 .03/1.16\nhloc [61] 0 .03/1.09 0 .02/0.85 0 .02/0.94 0 .01/0.75 0 .03/0.92 0 .05/1.30 0 .04/1.40 0 .05/1.47\nActive Search [63] 0 .04/1.18 0 .03/0.87 0 .02/1.01 0 .01/0.82 0 .04/1.15 0 .07/1.69 0 .05/1.72 0 .04/1.01\nnot color consistent from frame to frame, e.g., see the police car at time 0:07.\nWe believe that the cause of this problem may stem from the codebook. It was\ntrained using a perceptual loss, which might be less sensitive to colors [20]. On\nthe InteriorNet dataset (time 3:02), look at the pictures on the wall. The model\nfirst generates a window in place of the pictures, and with more context views,\nit replaces the window with two pictures. This illustrates well how the model\nimproves its prediction given more context views.\nC 7-Scenes evaluation\nIn order to evaluate the performance of our approach on the task of camera\npose estimation, we present the results on a localization benchmark dataset –\n7-Scenes [24] (cf. Sec. 4 in the main paper). We trained two models – one with a\nfine-tuned codebook and the other one with the InteriorNet-trained codebook.\nFor all models, we used context size 19. We have evaluated the method on all\nviews from the test set of each of the 7 scenes and used the views from the\ntraining set as context images. Generated images can be seen in Fig. 12.\nFor localization, we have experimented with different strategies for obtaining\nthe context view required by our approach: by default, we simply randomly select\n19 training images as context for each test image. We further evaluate a variant\nthat uses the top-10 most similar images identified via image retrieval with\nDenseVLAD [72] descriptors (indicated as “-top10”). The remaining 9 context\nimages are randomly selected from the training images. We also experimented\nwith using the top-19 retrieved images but found this approach to work worse.\nWe attribute this to the fact that the images of the 7-Scenes datasets are taken\n26 J. Kulh´ anek et al.\nGT interiornet-cb 7scenes-cb\nFig. 12.Evaluation of the transformer model on the 7-Scenes dataset [24]. We display\nthe ground-truth image ( GT), the image generated using a codebook trained only on\nthe InteriorNet dataset ( interiornet-cb) and the image generated by a model with\ncodebook fine-tuned on the 7-Scenes ( 7scenes-cb). For the visualization the context\nsize was set to 19\nViewFormer – Supplementary Material 27\nin sequences and that there is little viewpoint variation between the top-19\nretrieved images.\nWe evaluate variants where the codebook is trained only on InteriorNet (in-\ndicated as “-in”) and where the codebook is fine-tuned on the training images of\n7-Scenes (“-7s“). As can be seen in Tab. 2, using a fine-tuned codebook improves\nperformance. Similarly, using the top-10 retrieved images leads to more accurate\ncamera poses. For evaluation, we follow the common practice and report the\nmedian position and orientation error per scene, as well as the mean median\nposition and mean median orientation error over all the scenes.\nTo better understand the performance of our approach, we compare it against\nan oracle. Given the top-10 retrieved images via DenseVLAD, the oracle selects\nthe retrieved image with the smallest position and the smallest orientation error.\nAs shown in Tab. 2, our approach outperforms the oracle on most scenes. This\nimplies that the model is able to interpolate the context views such that it\ngenerates a pose that is closer to the query than any other in the context.\nTab. 2 also includes comparison with various baselines. Absolute pose regres-\nsion techniques [8, 32, 45, 65] train a CNN to directly regress the camera pose\nfor a given input image. Our approach performs similarly well or better than\nthese baselines, with the exception of LENS [45], which uses additional training\ndata in the form of images rendered from novel viewpoints. Our approach also\ntypically outperforms the two image retrieval-based baselines (DenseVLAD and\nDenseVLAD + Int.) They were proposed in [64] as a form of sanity check for\nabsolute pose regression approaches.\nSimilar to our approach, relative pose regression approaches [1, 19] estimate\nthe pose of the test image wrt. a set of context views. These context views are\nobtained by finding the most similar training images using image retrieval. Our\napproach performs similarly well (and often better) as RelocNet [1], which also\nuses a single forward pass to regress relative poses (between pairs of images).\nCamNet [19] uses a more complicated pipeline consisting of coarse and fine\nrelative pose regression stages, which results in higher accuracy.\nStructure-based approaches use 2D-3D matches between pixels in a test im-\nage and 3D scene points [6, 61, 63]. These approaches currently represent the\nstate-of-the-art in terms of pose accuracy and are more accurate than pose\nregression-based techniques. In contrast to the other baselines, they store the\n3D structure of the scene. Overall, the results show that our approach achieves\na similar level of pose accuracy as comparable methods.\nD Ablation study\nWe compare our model with alternative architectures to validate the design\nchoices we made. We also demonstrate how the quality of predictions improves\nwith larger context sizes. The InteriorNet dataset [35] was used for all evaluations\nbecause of its large size. The context size was 19.\nDifferent model variants. We compare variants of our approach trained for\nonly one of the two tasks – image generation and localization – on the Interi-\n28 J. Kulh´ anek et al.\norNet dataset [35]. We also evaluate the importance of the proposed branching\nattention by training alternative language models (LMs) that do not use it. As\ndiscussed in Sec. 1 in the main paper, one way to train the transformer without\nthe branching attention is to have a purely autoregressive (causal) LM [51, 74].\nThese models were successfully applied to similar tasks [22,50,53]. We also train\nanother alternative – masked LMs – that benefits from the same inference speed\nas our method [18]. In particular, the following models are compared:\n– ViewFormer – our approach with both localization and image generation\nenabled.\n– ViewFormer no-loc – our approach without localization.\n– ViewFormer no-imagen – our approach without image generation.\n– Causal LM – the same transformer model with autoregressive decoding.\nInstead of decoding all tokens at once, we model the probability distribution\nover the next image token given all previous tokens [51,74].\n– Causal LM + masked loc. – causal LM with added localization. For the\nlocalization, we mask the poses of three random views from the training\nbatch and attach a regression head to the last token of each image.\n– Masked LM – the same transformer model with masked decoding (without\nthe branching attention). We randomly mask three views from the training\nsequence and train the model to recover it. Note that the model is optimized\nfor a single context size (previous variants optimized for all context sizes).\n– Masked LM + masked loc. – masked LM with added localization. For\nthe localization, we mask the poses of three random views from the training\nbatch and attach a regression head to all image tokens. The resulting poses\nare averaged in the same way as in ViewFormer.\nThe results (averaged over all test scenes) are shown in Tab. 3. We also\ninclude a qualitative comparison in Fig. 13. As can be seen, training without\nthe localization task improves image quality, whereas there is little difference in\nterms of pose accuracy between training with or without the generation.\nOur method outperforms both causal LM and masked LM in image gener-\nation performance and localization accuracy. Note that our decoding is much\nfaster compared to causal LM because we decode all tokens at once (see Sec-\ntion 1 in the main paper). For a causal LM, generating a single view takes 10 s\neven when using cache. Compare this to 93 ms for the ViewFormer. Compared\nto masked LM, our model has the same inference speed, but the added benefit\nof being optimized for all context sizes. Masked LM can be optimized for one\ncontext size only.\nIncreasing the context size. We show the effect of increasing the context size\non localization and image generation performance. The image generation perfor-\nmance (measured with PSNR) and the localization accuracy (median Euclidean\ndistance between the predicted camera position and the ground truth) are shown\nin Fig. 14. The results were computed on all scenes from the test set.\nWe can see that the performance of both novel view synthesis and camera\npose estimation increases with more context views. The change is most promi-\nnent in the first five views, but after that it keeps increasing as well.\nViewFormer – Supplementary Material 29\nGT ViewFormer ViewFormer\nno-loc\nCausal LM Causal LM\nmasked loc.\nMasked LM Masked LM\nmasked loc.\nFig. 13.Examples generated by alternative architectures described in Sec. D. The\nexamples were generated on the test set of the InteriorNet dataset using context size\n19.\nTable 3. Ablation study evaluated on the InteriorNet dataset [35]. See Sec. D for a\ndescription of the compared variants. We show the PSNR, the pixel-wise MAE, and\nthe LPIPS distance [81]. For localization, we show the median position error in meters\nand the median orientation error in degrees computed over all scenes.\nImage generation Localization\nMethod PSNR ↑ MAE↓ LPIPS↓ Pos/Ori↓\nViewFormer 18.53 23.35 0.33 0.19/4.22\nViewFormer no-loc 19.10 21.56 0.32 -\nViewFormer no-imagen - - - 0.19/4.34\nCausal LM 16.75 29.88 0.39 -\nCausal LM + masked loc. 16.67 30.22 0.39 0.22/6.24\nMasked LM 18.76 22.91 0.32 -\nMasked LM + masked loc. 14.51 42.89 0.51 0.32/29.65\nE ShapeNet evaluation\nIn this section, we give more details on the ShapeNet results from the main paper\n(Fig. 7). We include quantitative and additional qualitative results. We trained\nour model on ShapeNet dataset rendered by SRN [69]. The context size used for\ntraining was three. We compare ViewFormer with SRN [69] and PixelNeRF [80].\nWe show the PSNR and SSIM [76] averaged across color channels for both car\n30 J. Kulh´ anek et al.\n0 5 10 15\nContext size\n8\n10\n12\n14\n16\n18\nPSNR\n0 5 10 15\nContext size\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nPos. error\nFig. 14.This plot shows the effect of increasing the context size on the PSNR ( left)\nand the position error ( right) evaluated on the InteriorNet dataset [35]\nTable 4.ShapeNet results comparing ViewFormer with SRN [69] and PixelNeRF [80].\nWe show the results for both car and chair category with one or two context views\ncars 1 view cars 2 views chairs 1 view chairs 2 views\nMethod 3D PSNR ↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑\nViewFormer ✗ 19.03 0.83 20.09 0.85 14.74 0.79 17.20 0.84\nSRN [69] ✓ 22.25 0.89 24.84 0.92 22.89 0.89 24.48 0.92\nPixelNeRF [80] ✓ 23.72 0.91 26.20 0.94 23.17 0.90 25.66 0.94\nand chair categories with one or two context views. The results are presented in\nTab. 4. We also extend Fig. 7 from the paper with additional qualitative results\non cars and chairs in Fig. 15 and 16.\nFrom the results, we can see that our method performs worse than both\nSRN [69] and PixelNeRF [80] in terms of the quantitative results. This is ex-\npected because our method was designed for more views (more than 10) and\nwas evaluated using one or two views. However, compared to PixelNeRF our\nmethod is able to recover more detail, whereas PixelNeRF produces blurry out-\nput, especially on the car category. Based on the qualitative results, we argue\nthat although our approach has worse quantitative numbers, our results look\nmore realistic. A possible cause for this observation could be that blurring the\nedges of an object can hide the unprecise geometry rendered by the model and\nincrease PSNR. However, it loses fine detail in the images.\nF Shepard-Metzler-Parts-7 evaluation\nWe evaluated our model on the Shepard-Metzler-Parts-7 dataset [21,66] to com-\npare our approach to other methods that only operate in 2D [15,21,71]. For the\nViewFormer – Supplementary Material 31\ncontext views GT PixelNeRF ViewFormer\nFig. 15.Additional ShapeNet cars qualitative comparison with PixelNeRF [80] using\ntwo context views\n32 J. Kulh´ anek et al.\ncontext views GT PixelNeRF ViewFormer\nFig. 16. Additional ShapeNet chairs qualitative comparison with PixelNeRF [80]\nusing two context views\nViewFormer – Supplementary Material 33\ncontext images GT GQN STR-GQN ours\nFig. 17.Qualitative results on the SM7 dataset [21]. We compare against GQN [21]\nand STR-GQN [15]\nTable 5.Comparison with GQN-based methods [14, 21, 71] on the SM7 dataset. We\nshow the MAE, RMSE, and the position and orientation errors ( Pos, Ori)\nImage generation Localization\nMethod MAE ↓ RMSE↓ Pos/Ori↓\nViewFormer 1.61 7.02 0.21/3.48\nGQN [21] 3.13 9.97 -\nE-GQN [71] 2.14 5.63 -\nSTR-GQN [14] 3.11 10.56 -\nevaluation, we used the context size three. The additional qualitative results,\npresented in Fig. 17, extend Fig. 5 from the main paper. Unfortunately, in the\nqualitative analysis, we cannot compare with E-GQN [71] because the authors\ndid not make the generated images or models public.\nTab. 5 presents quantitative results (averaged over 1000 scenes). As our\nmethod uses images of sizes 128 × 128 pixels, we rescaled the images before\ntraining the codebook. For evaluation, we used the original image size 64 × 64\npixels of the dataset. We report the pixel-wise mean absolute error (MAE) and\nroot mean square error (RMSE). For reference, we also show the localization\naccuracy. The position error (Pos) is the median euclidean distance between the\npredicted positions and the ground-truth camera positions, and the orientation\nerror (Ori) is the median of the angular distances in degrees.\nAs can be seen, our method clearly outperforms the baselines in terms of the\nMAE. E-GQN performs best in terms of the RMSE as it is trained to optimize\nthis metric, whereas our method uses MAE and perceptual loss.\n34 J. Kulh´ anek et al.\nTable 6.Codebook evaluation on the SM7 [21, 66], InteriorNet [35], CO3D [56], and\n7-Scenes [24] datasets. We report the PSNR, MAE, and LPIPS metrics averaged over\n1000 sampled images. The codebooks were evaluated with image size 128 ×128, except\nfor ‘CO3D@400’, which was evaluated with image size 400 × 400 pixels\ndataset PSNR ↑ MAE↓ LPIPS↓\nSM7 36.96 1.06 0.0075\nInteriorNet 24.86 11.01 0.1966\nCO3D 25.14 5.70 0.0994\nCO3D@400 25.34 5.63 0.1670\n7-Scenes (fine-tuned) 19.29 17.51 0.2937\n7-Scenes 19.00 19.22 0.3621\nShapeNet-cars 23.50 5.46 0.0734\nShapeNet-chairs 27.43 2.75 0.0425\nG Codebook evaluation\nIn this section, we add more details on the codebook’s representation capabilities\n(see Fig. 4 in the main paper) by showing quantitative results. We evaluated the\ncodebook models on each dataset’s test set. We report the peak signal-to-noise\nratio (PSNR), mean absolute error computed in the RGB image space (MAE),\nand the LPIPS distance [81]. All codebooks were evaluated with image size\n128 × 128 pixels except for ‘CO3D@400’, which was evaluated with image size\n400 ×400 pixels to be comparable with [56]. The metrics are averaged over 1000\nrandomly sampled images. The results can be seen in Tab. 6.\nBefore training the final codebook, we experimented with different codebook\nmodels. We also trained the DALL·E codebook [53], which yielded slightly blurry\nimages even when we used a codebook of size 8192 (normally, we use a codebook\nof size 1024). We observed a similar outcome with our codebook when we did\nnot use the perceptual loss. We also tried to use a GAN loss for the codebook,\nas described in [22]. However, the generated images did not look geometrically\nconsistent.\nH Training details\nTo allow our results to be reproduced, we give the details on the architecture of\nour method as well as the training hyperparameters.\nAll our codebook models were trained using the same set of hyperparame-\nters.5 We trained codebooks of size 1024. The architecture is very similar to [22]\nand is summarized in Sec. I. We used the Adam optimizer [33] with learning\nrate6 1.584×10−3 to train for 200k steps (roughly 480 GPU-hours) with a batch\nsize of 352. For the CO3D dataset, we trained on the same 10 object categories as\n5 Except for the SM7 dataset, where we only fine-tuned an existing model.\n6 The learning rate was rescaled from prior experiments; 1 .6 × 10−3 would work too.\nViewFormer – Supplementary Material 35\nin [56] as well as on the full dataset. For the 7-Scenes dataset, due to not having\nenough images to train from scratch, we finetuned an InteriorNet pre-trained\nmodel. Therefore, we used only 20k batch updates with the same hyperparame-\nters.\nThe architecture of our transformer model is based on GPT2-base [51],\nand has 12 transformer blocks, 12 attention heads, and the hidden size is 768.\nThe model design was chosen based on its successes in other domains and because\nits size fits well on our hardware. We trained our transformer models using the\nAdamW optimizer [40]; we used the cosine schedule for the learning rate with a\n2k step linear warmup.\nFor the InteriorNet dataset, we used the mixed-precision training with\nlearning rate 8 × 10−5, batch size 40, and learning rate decay 0 .01. The context\nsize was 19, but we did not optimize the first four views. The weight of the\nlocalization loss term was 5. In all other experiments, the localization loss weight\nwas 1 unless stated otherwise.\nFor the Shepard-Metzler-7-Parts (SM7) [21, 66] dataset, we trained\nthe transformer for 120k steps with the context size 5, batch size 128, and the\nlearning rate 10−4 (cosine decay, warmup). Before passing camera poses into the\ntransformer, we normalized the positions by multiplying them by 0 .2. We also\ngradually increased the weight of the localization term from 0 to 1 using the\ncosine schedule.7\nFor the CO3D dataset, we fine-tuned the model trained on the InteriorNet\ndataset. For the 10 categories, we optimized the model for 40k gradient steps\nwith learning rate 10−4 (cosine decayed with a 2,000 step warmup), weight decay\n0.05, and batch size 80, employing mixed-precision training. The context size was\n9, and the batch size was 80. We scaled the camera positions by 0.05 in order for\nthe positions to have a similar range as the pre-trained model. We also trained\na model on all dataset categories using 100k gradient steps with the batch size\n40, without using mixed-precision training, and when using the localization, we\nfurther used gradient clipping with the norm 1 to improve stability.\nFor the 7-Scenes dataset, we used a single InteriorNet pre-trained model\nwhich we fine-tuned on all 7-Scenes scenes. Same as in the original model, the\ncontext size was 19, but we did not optimize the first four views. The transformer\nwas fine-tuned for 10k gradient steps with learning rate 10 −5 (cosine schedule,\nwarmup). We rescaled the positions by multiplying them by 5 to be in the same\nrange as InteriorNet.\nFinally, for the ShapeNet dataset , we fine-tuned InteriorNet pre-trained\nmodel as well. We trained a single model for both categories: cars and chairs with\nthe context size 3. We did not use mixed-precision training and the batch size\nwas 64. The transformer was fine-tuned for 100k gradient steps with learning rate\n10−4 (cosine schedule, warmup), weight decay was 0 .05, and we used gradient\nclipping with the norm 1.\n7 The schedule is not needed for the training to work and in newer experiments, we\nuse a constant instead.\n36 J. Kulh´ anek et al.\nTable 7. Codebook architecture details: the encoder ( top left), the decoder (right),\nand the residual block ( bottom left). For each layer, we list the number of output\nfeatures (Num. features) and their sizes (Out. size). We denote kernel size as ‘ks’, stride\nas ‘s’, and the number of groups as ‘g’. We use nearest neighbor for the Upsample 2D\nlayer. Note that the output of the residual block is added to its input as in ResNets [25].\nIf the number of input channels is not equal to the number of output channels, the\nresidual connection is implemented by applying an affine transformation to the input\nfeatures position-wise before summing them with the output of this block\nLayer type Num. features Out. size\nConv 2D (ks: 3) 128 128\nResBlock 128 128\nResBlock 128 128\nConv 2D (ks: 3, s: 2) 128 64\nResBlock 128 64\nResBlock 128 64\nConv 2D (ks: 3, s: 2) 128 32\nResBlock 256 32\nResBlock 256 32\nConv 2D (ks: 3, s: 2) 256 16\nResBlock 256 16\nAttention 2D 256 16\nResBlock 256 16\nAttention 2D 256 16\nConv 2D (ks: 3, s: 2) 256 8\nResBlock 512 8\nResBlock 512 8\nResBlock 512 8\nAttention 2D 512 8\nResBlock 512 8\nGroupNorm 2D [77] (g: 32) 512 8\nSwish [52] 512 8\nConv 2D (ks: 3) 256 8\nConv 2D (ks: 1) 256 8\n(a) Encoder\nLayer Num. features\nGroupNorm [77] (g: 32) in\nSwish [52] in\nConv 2D (ks: 3) out\nGroupNorm [77] (g: 32) out\nSwish [52] out\nConv 2D (ks: 3) out\n(b) ResBlock\nLayer type Num. features Out. size\nConv 2D (ks: 1) 256 8\nConv 2D (ks: 3) 512 8\nResBlock 512 8\nAttention 2D 512 8\nResBlock 512 8\nResBlock 512 8\nResBlock 512 8\nResBlock 512 8\nUpsample 2D 512 16\nConv 2D (ks: 3) 512 16\nResBlock 256 16\nAttention 2D 256 16\nResBlock 256 16\nAttention 2D 256 16\nResBlock 256 16\nAttention 2D 256 16\nUpsample 2D 256 32\nConv 2D (ks: 3) 256 32\nResBlock 256 32\nResBlock 256 32\nResBlock 256 32\nUpsample 2D 256 64\nConv 2D (ks: 3) 256 64\nResBlock 128 64\nResBlock 128 64\nResBlock 128 64\nUpsample 2D 128 128\nConv 2D (ks: 3) 128 128\nResBlock 128 128\nResBlock 128 128\nResBlock 128 128\nGroupNorm 2D [77] (g: 32) 128 128\nSwish [52] 128 128\nConv 2D (ks: 3) 128 3\n(c) Decoder\nI Codebook architecture\nIn Tab. 7 we give the details on the codebook architecture ( cf. Sec. 3 in the\nmain paper). The codebook model architecture was taken from [22] and mod-\nified slightly to downscale the images into two times smaller latent space. We\nhave chosen this architecture because it had shown promising results for image\ngeneration in combination with transformers [22]. The other architecture we had\nconsidered was DALL·E [53], but from our experiments, it performed worse.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7785356044769287
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6984246969223022
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.6611483693122864
    },
    {
      "name": "Transformer",
      "score": 0.5923537015914917
    },
    {
      "name": "Artificial neural network",
      "score": 0.5190673470497131
    },
    {
      "name": "Computer vision",
      "score": 0.5162939429283142
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 12
}