{
  "title": "Simple Fusion: Return of the Language Model",
  "url": "https://openalex.org/W2892095314",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2601790773",
      "name": "Felix Stahlberg",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A1965196160",
      "name": "James Cross",
      "affiliations": [
        "Menlo School",
        "Meta (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2130709233",
      "name": "Veselin Stoyanov",
      "affiliations": [
        "Menlo School",
        "Meta (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2252272516",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2765961751",
    "https://openalex.org/W2797913374",
    "https://openalex.org/W2561274697",
    "https://openalex.org/W2748679025",
    "https://openalex.org/W2552838200",
    "https://openalex.org/W1916559533",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2766182427",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2597891111",
    "https://openalex.org/W2798362442",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2756566411",
    "https://openalex.org/W2886540570",
    "https://openalex.org/W2903012348",
    "https://openalex.org/W2101456909",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W1915251500"
  ],
  "abstract": "Neural Machine Translation (NMT) typically leverages monolingual data in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training: We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the residual probability of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for fluency. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and cold fusion.",
  "full_text": "Proceedings of the Third Conference on Machine Translation (WMT), V olume 1: Research Papers, pages 204–211\nBelgium, Brussels, October 31 - Novermber 1, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/W18-64021\nSimple Fusion: Return of the Language Model\nFelix Stahlberg†∗and James Cross‡ and Veselin Stoyanov‡\n†Department of Engineering, University of Cambridge, UK\n‡Applied Machine Learning, Facebook, Menlo Park, CA, USA\nfs439@cam.ac.uk, jcross@fb.com, vesko.st@gmail.com\nAbstract\nNeural Machine Translation (NMT) typically\nleverages monolingual data in training through\nbacktranslation. We investigate an alterna-\ntive simple method to use monolingual data\nfor NMT training: We combine the scores of\na pre-trained and ﬁxed language model (LM)\nwith the scores of a translation model (TM)\nwhile the TM is trained from scratch. To\nachieve that, we train the translation model to\npredict the residual probability of the train-\ning data added to the prediction of the LM.\nThis enables the TM to focus its capacity on\nmodeling the source sentence since it can rely\non the LM for ﬂuency. We show that our\nmethod outperforms previous approaches to\nintegrate LMs into NMT while the architec-\nture is simpler as it does not require gating\nnetworks to balance TM and LM. We observe\ngains of between +0.24 and +2.36 BLEU on\nall four test sets (English-Turkish, Turkish-\nEnglish, Estonian-English, Xhosa-English) on\ntop of ensembles without LM. We compare\nour method with alternative ways to uti-\nlize monolingual data such as backtranslation,\nshallow fusion, and cold fusion.\n1 Introduction\nMachine translation (MT) relies on parallel train-\ning data, which is difﬁcult to acquire. In contrast,\nmonolingual data is abundant for most languages\nand domains. Traditional statistical machine trans-\nlation (SMT) effectively leverages monolingual\ndata using language models (LMs) (Brants et al.,\n2007). The combination of LM and TM in SMT\ncan be traced back to the noisy-channel model\nwhich applies the Bayes rule to decompose a\n0This work was done when the ﬁrst author was on an in-\nternship at Facebook.\ntranslation system (Brown et al., 1993):\nˆy = argmax\ny\nP(y|x)\n= argmax\ny\nPTM (x|y)PLM (y)\n(1)\nwhere x = (x1,...,x m) is the source sentence,\ny = ( y1,...,y n) is the target sentence, and\nPTM (·) and PLM (·) are translation model and lan-\nguage model probabilities.\nIn contrast, NMT (Sutskever et al., 2014; Bah-\ndanau et al., 2014) uses a discriminative model and\nlearns the distribution P(y|x) directly end-to-end.\nTherefore, the vanilla training regimen for NMT is\nnot amenable to integrating an LM or monoglin-\ngual data in a straightforward manner.\nAn early attempt to use LMs for NMT, also\nknown as shallow fusion, combines LM and\nNMT scores at inference time in a log-linear\nmodel (Gulcehre et al., 2015, 2017). In contrast,\nwe integrate the LM scores during NMT train-\ning. Our training procedure ﬁrst trains an LM on\na large monolingual corpus. We then hold the LM\nﬁxed and train the NMT system to optimize the\ncombined score of LM and NMT on the parallel\ntraining set. This allows the NMT model to fo-\ncus on modeling the source sentence, while the\nLM handles the generation based on the target-\nside history. Sriram et al. (2017) explored a simi-\nlar idea for speech recognition using a gating net-\nwork for controlling the relative contribution of\nthe LM. We show that our simpler architecture\nwithout an explicit control mechanism is effective\nfor machine translation. We observe gains of up to\nmore than 2 BLEU points from adding the LM to\nTM training. We also show that our method can\nbe combined with backtranslation (Sennrich et al.,\n2016a), yielding further gains over systems with-\nout LM.\n204\n2 Related Work\n2.1 Inference-time Combination\nShallow fusion (Gulcehre et al., 2015) integrates\nan LM by changing the decoding objective to:\nˆy = argmax\ny\nlog PTM(y|x) +λlog PLM(y). (2)\nPLM(·) is produced by an LSTM-based RNN-\nLM (Mikolov et al., 2010) which has been\ntrained on monolingual target language data.\nPTM(·) can be a typical encoder-decoder Seq2Seq\nmodel (Sutskever et al., 2014; Bahdanau et al.,\n2014; Luong et al., 2015a). λis a hyper-parameter\nwhich is tuned on the development set.\n2.2 Cold Fusion\nShallow fusion combines a ﬁxed TM with a ﬁxed\nLM at inference time. Sriram et al. (2017) pro-\nposed to keep the LM ﬁxed, but train a sequence\nto sequence (Seq2Seq) NMT model from scratch\nwhich includes the LM as a ﬁxed part of the net-\nwork. They argue that this approach allows the\nSeq2Seq network to use its model capacity for the\nconditioning on the source sequence since the lan-\nguage modeling aspect is already covered by the\nLM. Their cold fusionarchitecture includes a gat-\ning network which learns to regulate the contribu-\ntions of the LM at each time step. They demon-\nstrated superior performance of cold fusion on a\nspeech recognition task.\n2.3 Other Approaches\nGulcehre et al. (2015, 2017) suggest to combine a\npre-trained RNN-LM with a pre-trained NMT sys-\ntem using a controller network that dynamically\nadjusts the weights between RNN-LM and NMT\nat each time step ( deep fusion). Both deep fusion\nand n-best reranking with count-based LMs have\nbeen used in WMT evaluation systems (Jean et al.,\n2015; Wang et al., 2017). An important limitation\nof these approaches is that LM and TM are trained\nindependently.\nA second line of research augments the parallel\ntraining data with additional synthetic data from a\nmonolingual corpus in the target language. The\nsource sentences can be generated with a sepa-\nrate translation system (Schwenk, 2008; Sennrich\net al., 2016a) (backtranslation), or simply copied\nover from the target side (Currey et al., 2017).\nSince data augmentation methods rely on some\nbalance between real and synthetic data (Sennrich\net al., 2016a; Currey et al., 2017; Poncelas et al.,\n2018), they can often only use a small fraction of\nthe available monolingual data. A third class of\napproaches change the NMT training loss func-\ntion to incorporate monolingual data. For exam-\nple, Cheng et al. (2016); Tu et al. (2017) pro-\nposed to add autoencoder terms to the training\nobjective which capture how well a sentence can\nbe reconstructed from its translated representation.\nHowever, training with respect to the new loss is\noften computationally intensive and requires ap-\nproximations. Alternatively, multi-task learning\nhas been used to incorporate source-side (Zhang\nand Zong, 2016) and target-side (Domhan and\nHieber, 2017) monolingual data. Another way\nof utilizing monolingual data in both source and\ntarget language is to warm start Seq2Seq train-\ning from pre-trained encoder and decoder net-\nworks (Ramachandran et al., 2017; Skorokhodov\net al., 2018). We note that pre-training can be used\nin combination with our approach.\nAn extreme form of leveraging monolingual\ntraining data is unsupervised NMT (Lample et al.,\n2017; Artetxe et al., 2017) which removes the need\nfor parallel training data entirely. In this work, we\nassume to have access to some amount of parallel\ntraining data, but aim to improve the translation\nquality even further by using a language model.\n3 Translation Model Training under\nLanguage Model Predictions\nIn spirit of the cold fusion technique of Sriram\net al. (2017) we also keep the LM ﬁxed when train-\ning the translation network. However, we greatly\nsimplify the architecture by removing the need for\na gating network. We follow the usual left-to-right\nfactorization in NMT:\nP(y|x) =\nn∏\nt=1\nP(yt|yt−1\n1 ,x). (3)\nLet STM(yt|yt−1\n1 ,x) be the output of the TM\nprojection layer without softmax, i.e., what we\nwould normally call the logits. We investigate\ntwo different ways to parameterize P(yt|yt−1\n1 ,x)\nusing STM(yt|yt−1\n1 ,x) and a ﬁxed and pre-\ntrained language model PLM(·): P OST NORM and\nPRENORM .\nPOST NORM This variant is directly inspired by\nshallow fusion (Eq. 2) as we turn STM(yt|yt−1\n1 ,x)\n205\ninto a probability distribution using a softmax\nlayer, and sum its log-probabilities with the log-\nprobabilities of the LM, i.e. multiply their proba-\nbilities:\nP(yt|yt−1\n1 ,x) =softmax(STM(yt|yt−1\n1 ,x))\n·PLM(yt|yt−1\n1 ).\n(4)\nPRENORM Another option is to apply normal-\nization after combining the raw STM(yt|yt−1\n1 ,x)\nscores with the LM log-probability:\nP(yt|yt−1\n1 ,x) =softmax\n(\nSTM(yt|yt−1\n1 ,x)\n+ logPLM(yt|yt−1\n1 )\n)\n.\n(5)\n3.1 Theoretical Discussion of POST NORM\nand PRENORM\nNote that P(yt|yt−1\n1 ,x) might not represent a\nvalid probability distribution under the P OST-\nNORM criterion since, as component-wise prod-\nuct of two distributions, it is not guaranteed to\nsum to 1. A way to ﬁx this issue would be to\ncombine TM and LM probabilities in the proba-\nbility space rather than in the log space. However,\nwe have found that probability space combination\ndoes not work as well as POST NORM in our exper-\niments. We can describe STM(yt|yt−1\n1 ,x) under\nPOST NORM informally as the residual probability\nadded to the prediction of the LM.\nIt is interesting to investigate what signal is\nactually propagated into STM(yt|yt−1\n1 ,x) when\ntraining with the P RENORM strategy. We can\nrewrite P(yt|yt−1\n1 ,x) as:\nP(yt|yt−1\n1 ,x) =P(yt,yt−1\n1 |x)\nP(yt−1\n1 |x)\n=P(yt,x|yt−1\n1 )\nP(x|yt−1\n1 )\n=P(x|yt,yt−1\n1 )\nP(x|yt−1\n1 ) P(yt|yt−1\n1 ).\n(6)\nAlternatively, we can decomposeP(yt|yt−1\n1 ,x) as\nLanguage pair # Sentences\nTurkish-English (WMT) 207.7K\nEstonian-English (WMT) 2,178.0K\nXhosa-English (INTERNAL ) 739.2K\nTable 1: Parallel training data.\nLanguage # Sentences LM Perplexity\ndev test\nEnglish (WMT) 26.9M 91.16 87.77\nTurkish (WMT) 3.0M 59.19 70.46\nEnglish (INTERNAL ) 20.0M 105.28 108.19\nTable 2: Monolingual training data.\nfollows using Eq. 5:\nP(yt|yt−1\n1 ,x) =softmax\n(\nSTM(yt|yt−1\n1 ,x)\n+ logPLM(yt|yt−1\n1 )\n)\n∝exp\n(\nSTM(yt|yt−1\n1 ,x)\n+ logPLM(yt|yt−1\n1 )\n)\n= exp(STM(yt|yt−1\n1 ,x))\n·PLM(yt|yt−1\n1 ).\n(7)\nCombining Eq. 6 and Eq. 7 leads to:\nexp(STM(yt|yt−1\n1 ,x)) ∝ P(x|yt\n1)\nP(x|yt−1\n1 ) (8)\nThis means that STM(yt|yt−1\n1 ,x) under\nPRENORM is trained to predict how much\nmore likely the source sentence becomes when a\nparticular target token yt is revealed.\n4 Experimental Setup\nWe evaluate our method on a variety of pub-\nlicly available and proprietary data sets. For\nour Turkish-English (tr-en), English-Turkish (en-\ntr), and Estonian-English (et-en) experiments we\nuse all available parallel data from the WMT18\nevaluation campaign to train the translation mod-\nels. Our language models are trained on News\nCrawl 2017. We use news-test2017 as develop-\nment (“dev”) set andnews-test2018 as test set.\nAdditionally, we collected our own proprietary\ncorpus of public posts on Facebook. We refer to\nit as ‘I NTERNAL ’ data set. This corpus consists\nof monolingual English in-domain sentences and\nparallel data in Xhosa-English. Training set sizes\nare summarized in Tables 1 and 2.\nOur preprocessing consists of lower-casing, to-\nkenization, and subword-segmentation using joint\n206\nArchitecture Hyperparameters\nSource vocab size (BPE) 16,000\nTarget vocab size (BPE) 16,000\nEmbedding size (all) 256\nEncoder LSTM units 512\nEncoder layers 2\nDecoder LSTM units 512\nDecoder layers 2\nAttention type dot product\nTraining Settings\nOptimization Vanilla SGD\nLearning rate 0.5\nBatch size 32\nLabel smoothing ϵ 0.1\nCheckpoint averaging Last 10\nTable 3: Summary of NMT settings for all models.\nbyte pair encoding (Sennrich et al., 2016b) with\n16K merge operations. On Turkish, we addition-\nally remove diacritics from the text.\nOn WMT we use lower-cased Sacre-\nBLEU1 (Post, 2018) to be comparable with\nthe literature. 2 On our internal data we report\ntokenized BLEU scores.\nOur Seq2Seq models are encoder-decoder ar-\nchitectures (Sutskever et al., 2014; Bahdanau\net al., 2014) with dot-product attention (Luong\net al., 2015b) trained with our PyTorch Trans-\nlate library.3 Both decoder and encoder consist\nof two 512-dimensional LSTM layers and 256-\ndimensional embeddings. The ﬁrst encoder layer\nis bidirectional, the second one runs from right to\nleft. Our training and architecture hyperparame-\nters are summarized in Tab. 3. Our LSTM-based\nLMs have the same size and architecture as the de-\ncoder networks, but do not use attention and do not\ncondition on the source sentence. We run beam\nsearch with beam size of 6 in all our experiments.\nFor each setup we train ﬁve models using SGD\n(batch size of 32 sentences) with learning rate\ndecay and label smoothing, and either select the\nbest one (single system) or ensemble the four best\nmodels based on dev set BLEU score.\n5 Results\nTab. 4 compares our methods P RENORM and\nPOST NORM on the tested language pairs. Shal-\nlow fusion (Sec. 2.1) often leads to minor im-\nprovements over the baseline for both single sys-\ntems and ensembles. We also reimplemented the\n1SacreBLEU signature for tr-en test-2017:\nBLEU+c.lc+l.tr-en+#.1+s.exp+t.wmt17+tok.13a+v.1.2.10\n2For translation into Turkish we evaluate after diacritics\nremoval.\n3https://github.com/pytorch/translate\nEnglish-Turkish (WMT)\nMethod Single 4-Ensemble\ndev test dev test\nBaseline (no LM) 12.23 11.56 14.17 13.35\nShallow fusion 12.45 11.61 14.43 13.51\nCold fusion 12.39 11.54 14.20 13.23\nThis work: P RENORM 12.82 11.93 14.78 13.41\nThis work: P OST NORM 13.30 12.27 14.77 13.61\nTurkish-English (WMT)\nMethod Single 4-Ensemble\ndev test dev test\nBaseline (no LM) 16.14 16.60 18.01 18.67\nShallow fusion 16.11 16.70 18.01 18.67\nCold fusion 16.25 16.21 17.99 18.40\nThis work: P RENORM 15.88 16.39 17.95 18.40\nThis work: P OST NORM 16.59 17.03 18.38 19.17\nEstonian-English (WMT)\nMethod Single 4-Ensemble\ndev test dev test\nBaseline (no LM) 16.02 16.57 16.83 17.91\nShallow fusion 16.02 16.57 16.83 17.91\nCold fusion 15.40 15.99 16.48 17.79\nThis work: P RENORM 16.80 17.44 17.78 19.01\nThis work: P OST NORM 16.43 17.10 17.62 18.63\nXhosa-English (INTERNAL )\nMethod Single 4-Ensemble\ndev test dev test\nBaseline (no LM) 10.39 11.49 13.87 15.43\nShallow fusion 10.69 11.65 14.06 15.54\nCold fusion 10.72 11.29 13.66 15.13\nThis work: P RENORM 11.06 12.13 14.50 16.07\nThis work: P OST NORM 12.34 13.27 15.45 17.79\nTable 4: Comparison of our P RENORM and P OST-\nNORM combination strategies with shallow fu-\nsion (Gulcehre et al., 2015) and cold fusion (Sriram\net al., 2017) under an RNN-LM.\ncold fusion technique (Sec. 2.2) for comparison.\nFor our machine translation experiments we re-\nport mixed results with cold fusion, with per-\nformance ranging between 0.33 BLEU gain on\nXhosa-English and slight BLEU degradation in\nmost of our Turkish-English experiments.\nBoth of our methods, P RENORM and P OST-\nNORM yield signiﬁcant improvements in BLEU\nacross the board. We report more consistent gains\nwith P OST NORM than with P RENORM . All our\nPOST NORM systems outperform both shallow fu-\nsion and cold fusion on all language pairs, yielding\ntest set gains of up to +2.36 BLEU (Xhosa-English\nensembles).\n6 Discussion and Analysis\nBacktranslation A very popular technique to\nuse monolingual data for NMT is backtransla-\ntion (Sennrich et al., 2016a). Backtranslation\n207\nFigure 1: Performance using backtranslation on\nEnglish-Turkish. Synthetic sentences are mixed at a\nratio of 1:nwhere nis plotted on the x-axis.\nFigure 2: Convergence of NMT training with and with-\nout LM on English-Turkish.\nuses a reverse NMT system to translate mono-\nlingual target language sentences into the source\nlanguage, and adds the newly generated sentence\npairs to the training data. The amount of monolin-\ngual data which can be used for backtranslation is\nusually limited by the size of the parallel corpus\nas the translation quality suffers when the mix-\ning ratio between synthetic and real source sen-\ntences is too large (Poncelas et al., 2018). This\nis a severe limitation particularly for low-resource\nMT. Fig. 1 shows that both our baseline system\nwithout LM and our P OST NORM system beneﬁt\ngreatly from backtranslation up to a mixing ratio\nof 1:8, but degrade slightly if this ratio is exceeded.\nPOST NORM is signiﬁcantly better than the base-\nline even when using it in combination with back-\ntranslation.\nTraining convergence We have found that\ntraining converges faster under the P OST NORM\nloss. Fig. 2 plots the training curves of our sys-\nEnglish-Turkish (WMT, single system)\nMethod Dev set Test set\nFFN RNN FFN RNN\nBaseline (no LM) 12.23 11.56\nShallow fusion 12.25 12.45 11.53 11.61\nCold fusion 12.33 12.39 11.51 11.54\nThis work: P RENORM 12.76 12.82 11.82 11.93\nThis work: P OST NORM 12.65 13.30 11.79 12.27\nTable 5: Comparison between using a recurrent LM\n(RNN) and an n-gram based feedforward LM (FFN)\non English-Turkish.\nEnglish-Turkish (WMT), POST NORM strategy\nLM type Single 4-Ensemble\nFFN RNN dev test dev test\n12.23 11.56 14.17 13.35\n✓ 12.65 11.79 14.36 13.48\n✓ 13.30 12.27 14.77 13.61\n✓ ✓ 12.86 12.02 14.72 13.70\nTable 6: Combining an RNN-LM and a feedforward\nLM with the translation model using the P OST NORM\nstrategy.\ntems. The baseline (orange curve) reaches its max-\nimum of 19.39 BLEU after 28 training epochs.\nPOST NORM surpasses this BLEU score already\nafter 12 epochs.\nLanguage model type So far we have used re-\ncurrent neural network language models (Mikolov\net al., 2010, RNN-LM) with LSTM cells in all\nour experiments. We can also parameterize an\nn-gram language model with a feedforward neu-\nral network (Bengio et al., 2003, FFN-LM). In\norder to compare both language model types we\ntrained a 4-gram feedforward LM with two 512-\ndimensional hidden layers and 256-dimensional\nembeddings on Turkish monolingual data. Tab. 5\nshows that the PRENORM strategy works particu-\nlarly well for the n-gram LM. However, using an\nRNN-LM with the POST NORM strategy still gives\nthe best overall performance. Using both RNN\nand n-gram LM at the same time does not improve\ntranslation quality any further (Tab. 6).\nImpact on the TM distribution With the POST-\nNORM strategy, the TM still produces a distribu-\ntion over the target vocabulary as the scores are\nMethod Perplexity Average entropy\nBaseline (no LM) 23.46 3.19\nRNN-LM 59.19 4.66\nTM under POST NORM 113.69 1.82\nTable 7: Perplexity and average entropies of the dis-\ntributions generated by our systems on the English-\nTurkish dev set.\n208\nMethod BLEU Precisions BP\n1-gram 2-gram 3-gram 4-gram\nBaseline (no LM) 17.91 53.0 23.7 12.3 6.6 0.996\nThis work: P RENORM 19.01 54.0 24.9 13.4 7.4 1.000\nRelative improvement +6.14% +1.89% +5.06% +8.94% +12.12% –\nTable 8: BLEU n-gram precisions for Estonian-English.\nSource Eestis ja Hispaanias peeti kinni neli Kemerovo grupeeringu liiget\nReference Four members of the Kemerovo group arrested in Estonia and Spain\nBaseline (no LM) In Estonia and Spain, four kemerovo groups were held\nThis work (PRENORM ) Four Kemerovo group members were held in Estonia and Spain\nSource Ta tleb, et elab aastaid hiljem endiselt hirmus.\nReference He says that years later, he still lives in fear.\nBaseline (no LM) He says that, for years, he still lives in fear.\nThis work (PRENORM ) He says that many years later he still lives in fear.\nSource “Ma kardan,” tleb ta.\nReference “I’m afraid,” he says.\nBaseline (no LM) “I fear,” says he.\nThis work (PRENORM ) “I am afraid,” he says.\nTable 9: Translation samples from the Estonian-English test set.\nnormalized before the combination with the LM.\nThis raises a natural question: How different are\nthe distributions generated by a TM trained un-\nder POST NORM loss from the distributions of the\nbaseline system without LM? Tab. 7 gives some\ninsight to that question. As expected, the RNN-\nLM has higher perplexity than the baseline as it is\na weaker model of translation. The RNN-LM also\nhas a higher average entropy which indicates that\nthe LM distributions are smoother than those from\nthe baseline translation model. The TM trained\nunder P OST NORM loss has a much higher per-\nplexity which suggests that it strongly relies on the\nLM predictions and performs poorly when it is not\ncombined with it. However, the average entropy is\nmuch lower (1.82) than both other models, i.e. it\nproduces much sharper distributions.\nLanguage models improve ﬂuency A tradi-\ntional interpretation of the role of an LM in MT\nis that it is (also) responsible for the ﬂuency of\ntranslations (Koehn, 2009). Thus, we would ex-\npect more ﬂuent translations from our method than\nfrom a system without LM. Tab. 8 breaks down\nthe BLEU score of the baseline and the PRENORM\nensembles on Estonian-English inton-gram preci-\nsions. Most of the BLEU gains can be attributed to\nthe increase in precision of higher order n-grams,\nindicating improvements in ﬂuency. Tab. 9 shows\nsome examples where our PRENORM system pro-\nduces a more ﬂuent translation than the baseline.\nTraining set size We artiﬁcially reduced the size\nof the English-Turkish training set even further\nFigure 3: English-Turkish BLEU over training set size.\nto investigate how well our method performs in\nlow-resource settings (Fig. 3). Our P OST NORM\nstrategy outperforms the baseline regardless of the\nnumber of training sentences, but the gains are\nsmaller on very small training sets.\n7 Conclusion\nWe have presented a simple yet very effective\nmethod to use language models in NMT which in-\ncorporates the LM already into NMT training. We\nreported signiﬁcant and consistent gains from us-\ning our method in four language directions over\ntwo alternative ways to integrate LMs into NMT\n(shallow fusionand cold fusion) and showed that\nour approach works well even in combination with\nbacktranslation and on top of ensembles. Our\nmethod leads to faster training convergence and\nmore ﬂuent translations than a baseline system\nwithout LM.\n209\nReferences\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2017. Unsupervised neural ma-\nchine translation. arXiv preprint arXiv:1710.11041.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.\nOch, and Jeffrey Dean. 2007. Large language\nmodels in machine translation. In Proceedings\nof the 2007 Joint Conference on Empirical Meth-\nods in Natural Language Processing and Com-\nputational Natural Language Learning (EMNLP-\nCoNLL), pages 858–867, Prague, Czech Republic.\nAssociation for Computational Linguistics.\nPeter E. Brown, Stephen A. Della Pietra, Vincent\nJ. Della Pietra, and Robert L. Mercer. 1993. The\nmathematics of statistical machine translation: Pa-\nrameter estimation. Computational Linguistics,\n19(2).\nYong Cheng, Wei Xu, Zhongjun He, Wei He, Hua\nWu, Maosong Sun, and Yang Liu. 2016. Semi-\nsupervised learning for neural machine translation.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1965–1974. Association for\nComputational Linguistics.\nAnna Currey, Antonio Valerio Miceli Barone, and Ken-\nneth Heaﬁeld. 2017. Copied monolingual data im-\nproves low-resource neural machine translation. In\nProceedings of the Second Conference on Machine\nTranslation, pages 148–156. Association for Com-\nputational Linguistics.\nTobias Domhan and Felix Hieber. 2017. Using target-\nside monolingual data for neural machine translation\nthrough multi-task learning. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1500–1505. Associa-\ntion for Computational Linguistics.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv preprint arXiv:1503.03535.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, and Yoshua Bengio. 2017. On integrating\na language model into neural machine translation.\nComputer Speech & Language, 45:137 – 148.\nS´ebastien Jean, Orhan Firat, Kyunghyun Cho, Roland\nMemisevic, and Yoshua Bengio. 2015. Montreal\nneural machine translation systems for WMT’15. In\nProceedings of the Tenth Workshop on Statistical\nMachine Translation, pages 134–140. Association\nfor Computational Linguistics.\nPhilipp Koehn. 2009. Statistical machine translation.\nCambridge University Press.\nGuillaume Lample, Ludovic Denoyer, and\nMarc’Aurelio Ranzato. 2017. Unsupervised\nmachine translation using monolingual corpora\nonly. arXiv preprint arXiv:1711.00043.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015a. Effective approaches to attention-\nbased neural machine translation. In Proceedings of\nthe 2015 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1412–1421. Asso-\nciation for Computational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015b. Effective approaches to attention-\nbased neural machine translation. In Proceedings of\nthe 2015 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1412–1421. Asso-\nciation for Computational Linguistics.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nAlberto Poncelas, Dimitar Shterionov, Andy Way,\nGideon Maillette de Buy Wenniger, and Pey-\nman Passban. 2018. Investigating backtransla-\ntion in neural machine translation. arXiv preprint\narXiv:1804.06189.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. arXiv preprint arXiv:1804.08771.\nPrajit Ramachandran, Peter Liu, and Quoc Le. 2017.\nUnsupervised pretraining for sequence to sequence\nlearning. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 383–391. Association for Computational\nLinguistics.\nHolger Schwenk. 2008. Investigations on large-scale\nlightly-supervised training for statistical machine\ntranslation. In In IWSLT, pages 182–189.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725. Association for Computational Linguistics.\n210\nIvan Skorokhodov, Anton Rykachevskiy, Dmitry\nEmelyanenko, Sergey Slotin, and Anton Ponkratov.\n2018. Semi-supervised neural machine translation\nwith language models. In Proceedings of the AMTA\n2018 Workshop on Technologies for MT of Low Re-\nsource Languages (LoResMT 2018), pages 37–44.\nAssociation for Machine Translation in the Ameri-\ncas.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2017. Cold fusion: Training seq2seq\nmodels together with language models. arXiv\npreprint arXiv:1708.06426.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Proceedings of the 27th International\nConference on Neural Information Processing Sys-\ntems - Volume 2, NIPS’14, pages 3104–3112, Cam-\nbridge, MA, USA. MIT Press.\nZhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,\nand Hang Li. 2017. Neural machine translation with\nreconstruction.\nYuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun\nYang, Wei Chen, Muze Li, Lin Shi, Yanfeng Wang,\nand Hongtao Yang. 2017. Sogou neural machine\ntranslation systems for WMT17. In Proceedings\nof the Second Conference on Machine Translation,\npages 410–415. Association for Computational Lin-\nguistics.\nJiajun Zhang and Chengqing Zong. 2016. Exploit-\ning source-side monolingual data in neural machine\ntranslation. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 1535–1545. Association for Compu-\ntational Linguistics.\n211",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8359357714653015
    },
    {
      "name": "Fluency",
      "score": 0.6910907030105591
    },
    {
      "name": "Machine translation",
      "score": 0.595214307308197
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5934456586837769
    },
    {
      "name": "Sentence",
      "score": 0.5886027812957764
    },
    {
      "name": "Natural language processing",
      "score": 0.5663973093032837
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.47322145104408264
    },
    {
      "name": "Language model",
      "score": 0.4359643757343292
    },
    {
      "name": "Test data",
      "score": 0.4230020046234131
    },
    {
      "name": "Speech recognition",
      "score": 0.4167032241821289
    },
    {
      "name": "Linguistics",
      "score": 0.12617307901382446
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}