{
    "title": "Text Sentiment Analysis Based on Transformer and Augmentation",
    "url": "https://openalex.org/W4280503842",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2656981597",
            "name": "Xiaokang Gong",
            "affiliations": [
                "Soochow University",
                "Changshu Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2223770179",
            "name": "Wenhao Ying",
            "affiliations": [
                "Changshu Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2102009910",
            "name": "Shan Zhong",
            "affiliations": [
                "Changshu Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2126651258",
            "name": "Shengrong Gong",
            "affiliations": [
                "Changshu Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2656981597",
            "name": "Xiaokang Gong",
            "affiliations": [
                "Soochow University",
                "Changshu Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2223770179",
            "name": "Wenhao Ying",
            "affiliations": [
                "Changshu Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2102009910",
            "name": "Shan Zhong",
            "affiliations": [
                "Changshu Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2126651258",
            "name": "Shengrong Gong",
            "affiliations": [
                "Changshu Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2125592902",
        "https://openalex.org/W4230097545",
        "https://openalex.org/W3034368386",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W2213612645",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2892137778",
        "https://openalex.org/W2516941294",
        "https://openalex.org/W2984353870",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3021895194",
        "https://openalex.org/W3006989784",
        "https://openalex.org/W3138154797",
        "https://openalex.org/W2990138404"
    ],
    "abstract": "With the development of Internet technology, social media platforms have become an indispensable part of people’s lives, and social media have been integrated into people’s life, study, and work. On various forums, such as Taobao and Weibo, a large number of people’s footprints are left all the time. It is these chats, comments, and other remarks with people’s emotional evaluations that make up part of public opinion. Analysis of this network public opinion is conducive to maintaining the peaceful development of society. Therefore, sentiment analysis has become a hot research field and has made great strides as one of the hot topics in the field of natural language processing. Currently, the BERT model and its variants have achieved excellent results in the field of NLP. However, these models cannot be widely used due to huge demands on computing resources. Therefore, this paper proposes a model based on the transformer mechanism, which mainly includes two parts: knowledge distillation and text augmentation. The former is mainly used to reduce the number of parameters of the model, reducing the computational cost and training time of the model, and the latter is mainly used to expand the task text so that the model can achieve excellent results in the few-sample sentiment analysis task. Experiments show that our model achieves competitive results.",
    "full_text": "Frontiers in Psychology | www.frontiersin.org 1 May 2022 | Volume 13 | Article 906061\nORIGINAL RESEARCH\npublished: 13 May 2022\ndoi: 10.3389/fpsyg.2022.906061\nEdited by: \nYuanpeng Zhang,  \nNantong University, China\nReviewed by: \nJuxin Hu,  \nXuzhou University of Technology, \nChina\nQin Qin,  \nHenan Institute of Engineering, China\n*Correspondence: \nWenhao Ying  \nywh@cslg.edu.cn\nSpecialty section: \nThis article was submitted to  \nEmotion Science,  \na section of the journal  \nFrontiers in Psychology\nReceived: 28 March 2022\nAccepted: 25 April 2022\nPublished: 13 May 2022\nCitation:\nGong X, Ying W, Zhong S and \nGong S (2022) Text Sentiment \nAnalysis Based on Transformer and \nAugmentation.\nFront. Psychol. 13:906061.\ndoi: 10.3389/fpsyg.2022.906061\nText Sentiment Analysis Based on \nTransformer and Augmentation\nXiaokang Gong 1,2, Wenhao Ying 2*, Shan Zhong 2 and Shengrong Gong 2\n1 School of Computer Science and Technology, Soochow University, Suzhou, China, 2 School of Computer Science and \nEngineering, Changshu Institute of Technology, Suzhou, China\nWith the development of Internet technology, social media platforms have become an \nindispensable part of people’s lives, and social media have been integrated into people’s \nlife, study, and work. On various forums, such as Taobao and Weibo, a large number of \npeople’s footprints are left all the time. It is these chats, comments, and other remarks \nwith people’s emotional evaluations that make up part of public opinion. Analysis of this \nnetwork public opinion is conducive to maintaining the peaceful development of society. \nTherefore, sentiment analysis has become a hot research field and has made great strides \nas one of the hot topics in the field of natural language processing. Currently, the BERT \nmodel and its variants have achieved excellent results in the field of NLP . However, these \nmodels cannot be widely used due to huge demands on computing resources. Therefore, \nthis paper proposes a model based on the transformer mechanism, which mainly includes \ntwo parts: knowledge distillation and text augmentation. The former is mainly used to \nreduce the number of parameters of the model, reducing the computational cost and \ntraining time of the model, and the latter is mainly used to expand the task text so that \nthe model can achieve excellent results in the few-sample sentiment analysis task. \nExperiments show that our model achieves competitive results.\nKeywords: sentiment analysis, social media, transformer, knowledge distillation, text augmentation\nINTRODUCTION\nIn today’s era of extremely complex social networking, the number of Internet users in China \nhas increased dramatically. As of 2021, the number of domestic Internet users will reach \n989  million ( Cnnic, 2021 ). Such a huge scale that it involves a wide range of fields, such as \nshopping, games, communications, and video. In all these areas, there is a huge amount of \ndata and information left behind, and these data and information have formed a certain \ntendency of online opinion on the Internet, which is both positive, negative, and neutral. \nMany companies can recommend products based on users’ comments and preferences, thereby \nincreasing product sales; while some undesirable emotional messages can cause social unrest, \nsuch as donation scams, which can lead to heated debates among internet users. For the \ntextual information formed by some remarks on the Internet, if sentiment analysis can be carried \nout in a certain way, the chaotic information can be  extracted by certain means, and then \ndifferentiated according to different categories to form useful information for the society, which \ncan ultimately guide the normal development of society is a direction worthy of study ( Zhang \net  al., 2020 ).\nFrontiers in Psychology | www.frontiersin.org 2 May 2022 | Volume 13 | Article 906061\nGong et al. Text Sentiment Analysis\nTo solve the above problems, deep learning plays an important \nrole. Deep learning ( LeCun et al., 2015 ) is to learn the inherent \nlaws and representation levels of sample data, and the information \nobtained during these learning processes is of great help to \nthe interpretation of data, such as text, images, and sounds. \nIts ultimate goal is to enable machines to have the ability to \nanalyze and learn like humans, and to recognize data, such \nas words, images, and sounds. Deep learning is a complex \nmachine learning algorithm that far outperforms previous related \ntechnologies in speech and image recognition.\nDeep learning requires a large amount of labeled data to \nget excellent results, and the need for large amounts of labeled \ndata is one of its most fundamental shortcomings. When the \nlabeled data are limited, supervised deep learning models are \ntended to suffer from overfitting ( Hawkins, 2004 ). The strong \nreliance on labeled data limits the practical use of deep learning \nmodels, due to the need for a large amount of time and \nmoney to obtain enough labeled data. As a result, semi-supervised \nlearning (SSL; Van Engelen and Hoos, 2020) has received much \nattention, because it is one of the most promising paradigms \nof leveraging unlabeled data to address this weakness ( Chawla \nand Karakoulas, 2005 ).\nThe current NLP technology is witnessing a revolution in \npre-trained self-supervised models. These models often have \nhundreds of millions of parameters. Among these models, \nBERT ( Devlin et  al., 2018 ) is one of the most representatives. \nThese models show substantial improvement in results on \nprevious tasks. However, as one of the largest models in the \nhistory of natural language processing, the BERT model has \na large number of parameters, large size, and high latency. \nThese drawbacks prevent the model from running on devices \nwith limited computing power or taking too long to run. In \nsome real-time scenarios, the long response times would outweigh \nthe benefits in comparison to the degree of improvement in \naccuracy. Therefore, we  investigate reducing the number of \nparameters of the model while retaining its accuracy, thereby \nreducing the computational cost and response time required \nto run the model. This paper investigates how to reduce the \nparameters of the model. It is found that good results can \nbe achieved using knowledge distillation and text augmentation \nfor text tasks. Experimental results show that the accuracy of \ntext sentiment classification can be  improved with a small \nnumber of samples.\nThe contributions of this paper are summarized as follows:\n 1. We propose a text sentiment classification model based on \nthe Transformer mechanism, which combines knowledge \ndistillation and text augmentation methods to improve the \naccuracy of sentiment classification in the few-shot \nlabeling task.\n 2. The model reduces the number of parameters based on the \nTransformer mechanism by means of knowledge distillation \nand uses the method of text augmentation to solve the \nproblem of low accuracy for the few-sample task.\n 3. We conduct experiments on two public corpora and compare \nwith different state-of-the-art methods to verify the \nperformance of the model.\nRELATED WORK\nNeural Language Model for NLP\nSentiment analysis is the classification of emotions and attitudes \nin subjective texts, and the main methods are machine learning \nand deep learning. From the perspective of machine learning, \nthe text sentiment analysis method based on machine learning \n(Lin and Luo, 2020) needs to use a corpus to train a classification \nmodel. For example, Y an et  al. (2020) used the bag-of-words \nmodel to classify text data and considered Word2Vec to establish \na new word vector; in the ensemble algorithm Naive Bayes \nand SVM ( Tiwari and Mallick, 2016 ), both the precision and \nrecall rate were improved. From the perspective of deep learning, \nin recent years, deep learning technology has made great \nprogress in processing text information-related tasks. The neural \nnetwork structure has achieved remarkable results in text \nclassification. The neural network structure is used to construct \ndifferent neural network algorithms, such as CNN, bidirectional \nLSTM, text CNN, and so on. Chen (2015)  first proposed to \napply CNN to text orientation analysis, and obtained good \nresults; on the basis of Chen’s analysis, Conneau et  al. (2016)  \nproposed a VDCNN model by using a deep convolutional \nnetwork method. The subsequent rise of pre-trained models, \nwhich have led to major breakthroughs in the field of NLP , \nThe BERT model proposed by ( Vaswani et  al., 2017 ) is a \nbetter improvement in terms of evaluation indicators. Devlin \net al. (2018)  mainly introduced the practical value of the BERT \nmodel, and obtained good research results on 11 natural \nlanguage processing tasks.\nOne of the major breakthroughs in the history of natural \nlanguage processing is the attention mechanism. The attention \nmechanism is proposed to solve long-term dependency problems \nof models that use a single context vector compressing every \ninput from previous time steps. The attention mechanism \nmakes it possible to capture the global semantics in the texts. \nBecause the attention mechanism allows the model to take \nhidden states from multiple time steps as input and calculate \nthe importance of the input regarding the current time step. \nTransformer serving as the core architecture of the attention \nmechanism has been widely used for language models ( Han \net  al., 2021 ) to capture complex linguistic patterns. These \npre-trained language models work well on various NLP tasks; \nhowever, it is accepted that these pre-trained models rely on \nlarge-scale data training and have high requirements for the \nquality of the data set.\nPre-training and Fine-Tuning Framework\nNeural network methods generally start with random \ninitialization of model parameters and then train the model \nparameters using optimization algorithms, such as back \npropagation and gradient descent. Before the advent of \npre-training techniques, the application of neural network-based \ndeep learning in NLP faced the following problems: firstly, \ndeep learning models at this time were not complex enough, \nand simply stacking neural network layers did not bring more \nperformance gains; secondly, data-driven deep learning models \nGong et al. Text Sentiment Analysis\nFrontiers in Psychology | www.frontiersin.org 3 May 2022 | Volume 13 | Article 906061\nlacked large-scale annotated data, and manual annotation was \ntoo expensive to drive complex models. Therefore, pre-training \ntechniques based on knowledge augmentation, migration \nlearning, and multi-task learning are gradually being given \nmore attention by scholars.\nThe pre-training and fine-tuning framework have achieved \ngreat success in recent years, especially in the NLP field, and \nhas been applied to a series of NLP tasks. Howard and Ruder \n(2018) proposed to pre-train a language model on a large \ncorpus and fine-tune it on the target task. Such a model uses \nsome novel techniques like gradual unfreezing and slanted \ntriangular learning rates. Encouraged by the good performance \nof the pre-trained models, researchers get excellent performance \neven with small amounts of labeled data. Pre-trained models \nare often applied to different objectives, such as language \nmodeling and masked language modeling. With the increase \nin the training data size, the performance of pre-trained models \nis also improved ( Araci, 2019 ; Rezaeinia et  al., 2019 ).\nData Augmentation for Language Data\nData augmentation is a technique that can increase the size \nof a data set. Many researchers work on data augmentation, \nmainly in the field of computer vision, speech, etc. Compared \nto these, there is less research on data augmentation in the \nfield of text and there is no standard method yet. The commonly \nused method is to use a dictionary or thesaurus or database \nof synonyms to make word replacements. In a situation where \nthere is no dictionary, the other way is to use distributed \nword representation for finding similar words. A method \nbelonging to this is called synonym augmentation. In fact, the \nbest way to augment is to artificially change the wording of \nthe language, but the cost of this method is too expensive. \nTherefore, the most option in data augmentation for most \nresearch is to replace words or phrases with their synonyms. \nFor example, the most popular open-source lexical database \nfor the English language is WordNet ( Fellbaum, 2010). Another \nmethod is called semantic similarity augmentation, it uses \ndistributed word representation, namely, word embedding, one \ncan identify semantically similar words. This approach requires \neither pre-trained word embedding models for the language \nat hand or enough data from the target application to be  able \nto build the embedding model. Its advantage is that no additional \ndictionaries are required to find synonyms. And the last method \nis back translation, its process is translating words, phrases, \nor texts into another language, namely, forward translation, \nthen translating the results back into the original language, \nthis is called back translation ( Edunov et  al., 2018 ).\nMixup augmentation (Guo, 2020) creates new training examples \nby drawing samples from the original data and combining them \nconvexly, usually, the samples are two or even more, it combines \nthe data both in terms of the input and output. It takes pair \nof samples from the initial data set and sums both the input \nand output. The main idea of the mixup is a sample. Given \ntwo labeled data ( xyii, ) and ( xyjj,) , where x is the input \nvector and y is the one-hot encoded labels, the algorithm creates \ntraining samples by linear interpolations:\n ( )ˆ 1ijx xxll= +-  (1)\n ( )ˆ 1ijy yyll= +-  (2)\nWhere l Î []01, , and la al ll~ () =- ()Beta ,, ma x1  in \nwhich α is the hyper-parameter to control the distribution of \nl . It is best suited for learning models that use the cross-\nentropy loss and change the input. Augmentation by mixup \ncan be  done on text representation for text problems. as such \nwe can use mixup with bag-of-words models, word embeddings, \nand language models.\nKnowledge Distillation\nDeep learning has achieved incredible performance in numerous \nfields, including computer vision, speech recognition, natural \nlanguage processing, and more. However, most models are too \ncomputationally expensive to run on mobile or embedded \ndevices. Therefore, the model needs to be  compressed, and \nknowledge distillation (Gou et al., 2021 ) is one of the important \ntechniques in model compression. Knowledge distillation adopts \nthe Teacher–Student mode: the complex and large model is \nused as the teacher, the student model structure is relatively \nsimple, and the teacher is used to assist the training of the \nstudent model. The teacher has strong learning ability and \ncan transfer the knowledge it has learned to relatively weak \nlearning ability. The student model, in order to enhance the \ngeneralization ability of the student model. The complex and \ncumbersome but effective Teacher model is not online, it is \nsimply a mentor role, and the flexible and lightweight Student \nmodel is really deployed online for prediction tasks.\nVanilla Knowledge Distillation is simply the learning of \nlightweight student models from the soft targets output by \nthe teacher model. However, when the teacher model becomes \ndeeper, learning the soft targets alone is not enough. Therefore, \nwe  need to acquire not only the knowledge output from the \nteacher model, but also other knowledge that is implicit in \nthe teacher model, such as output feature knowledge, intermediate \nfeature knowledge, relational feature knowledge, and structural \nfeature knowledge. The four forms of knowledge distilled from \nthe student’s problem-solving perspective can be  compared to \nthe following: output knowledge provides the answer to a \nproblem, intermediate knowledge provides the process of solving \na problem, relational knowledge provides the method of solving \na problem, and structural knowledge provides the complete \nbody of knowledge.\nMATERIALS AND METHODS\nIn this section, we  have made improvements to the Mixup \nmethod. Mixup can be  interpreted in different ways. On the \none hand, the Mixup method can be  viewed as a data \naugmentation approach that creates new data based on the \noriginal training set. On the other hand, it enhances the \nGong et al. Text Sentiment Analysis\nFrontiers in Psychology | www.frontiersin.org 4 May 2022 | Volume 13 | Article 906061\nregularization of the model. Mixup was first designed for images \ntasks; thus, mixup was demonstrated to work well on continuous \nimage data; however, it is challenging to extend mixup from \nimage to text, since it is infeasible to compute the interpolation \nof discrete tokens. To overcome this challenge, we  propose a \nnovel method that expands on the original text and interpolates \nin the hidden space. Inspired by the excellent performance of \nthe pre-trained models like Bidirectional Encoder Representations \nfrom Transformers (BERT), we  use a multi-layer model to \nencode the given text to get the semantic representations, in \nthis process; we expand the original text and apply interpolations \nwithin hidden space as a data augment method for text. For \nan encoder with L layers, we  choose to mix up the hidden \nrepresentation at layer K.\nAs shown in Figure  1 , we  first use text augmentation \ntechniques to expand the labeled data size, such as EDA, \nback translation, and Word2vec-based (learned semantic \nsimilarity) augmentation, and then, we  compute the hidden \nrepresentation in the bottom layers and mixup the hidden \nrepresentation at layer m, and feed the interpolated hidden \nrepresentations to the upper layers. In mathematical \nexpressions, we  denote the layer m in the encoder network \nas mixupm .;q() , thus the hidden representation of the layer \nm can be  computed as hiddenm ixup hiddenmm m= () -1 ;q . For \nthe text samples X i  and its augmentation X j  define the \nlayer 0 as the embedding layer, for example, hW Xi\nEi0 =  \nand the hidden representation of the two samples from the \nlower layers and the mixup at layer k and forward passing \nto upper layers are defined as follows:\n \nhm ixup hl ml\ni\nl l\ni= () Î []-1 1;,q ,\n \n(3)\n ( )ˆ 1ij\nmm mh hhll= +-  (4)\n ( ) [ ]1;, 1ˆˆ ,l ll mixup l m Lhh q-= Î+\n \n(5)\n ( )( ), .; , ˆ,,ij LMATEXT X X mix hup mql =  (6)\nWe use an encoder model mixup .;q() , Equation (3)  \nrepresents the mixup method before the fusion layer m, where \nθ represents the remaining parameters in the model. Equation \n(4) expresses the calculation method in the mixed layer. \nEquations (5, 6) , respectively, express the operation above \nthe mixed layer and the whole mixing method. MATEXT \ncombines the original text and its simple augmentation text \nas input and interpolates textual semantic hidden \nrepresentations as a type of data augmentation. Compared \nto the mixup ( Zhang et al., 2017 ) defined in the augmentation \non text representation. In the experiment, we  sample the \nmix parameter l  from a Beta distribution for every batch \nto perform the interpolation:\n la a~ Beta ,()  (7)\n ll l=- ()ma x, 1  (8)\nIn the equation, a  is the hyper-parameter to control the \ndistribution of l . In MATEXT, different from the original \nmixup, we  share the label of the original text and its \naugmentation text.\nData Augmentation\nWe use Easy Data Augmentation (EDA) technique ( Wei and \nZou, 2019) which consists of four different text editing operations: \n(1) Synonym replacement: N-words are randomly selected from \nthe sentence and replaced with one of its synonyms chosen \nat random. (2) Random noise injection: N-words are randomly \nselected from the sentence and a single character of each word \nis replaced with a random alphabetical character. (3) Random \nswap: we  randomly choose two words in the sentence and \nswap their positions and repeat N times. (4) Random deletion: \nN-words from the sentence are randomly chosen and removed \nfrom the sentence.\nThe value N is depended on the length of each sentence. \nIn our experiment, the p is set 0.1 and calculated \npl en sentence´ () , where the number words in the sentence \nis used as a length of the sentence. Rounded up value of \npl en sentence´ ()  is used as the value of N.\nIn addition, Back translations are also a common data \naugmentation technique and can generate diverse paraphrases \nwhile preserving the semantics of the original sentences. And \nWord2vec is another robust augmentation method that uses a \nword embedding model trained on the public data set to find \nthe most similar words for a given input word, which is called \nWord2vec-based (learned semantic similarity) augmentation \n(Budhkar and Rudzicz, 2018 ). Table  1 shows some examples \nof text augmentation.\nFIGURE 1 |  Mixup augmentation text.\nGong et al. Text Sentiment Analysis\nFrontiers in Psychology | www.frontiersin.org 5 May 2022 | Volume 13 | Article 906061\nKnowledge Distillation\nThe main purpose of this chapter is to reduce the parameters \nof the current mainstream BERT model through the method \nof knowledge distillation. We  study the knowledge distillation \nunder a limited amount of labeled data and a large amount \nof unlabeled data. With sufficiently accurate teacher models \nand large amounts of distilled unlabeled data, using Bi-LSTM \nand RNN as encoders as student models can greatly reduce \nthe parameter space and perform on par with large pre-trained \nmodels. The distillation technique is mainly divided into two \nparts, first using a fine-tuned teacher model trained with labeled \ntext data to automatically label a large amount of unlabeled \ndata and then using the augmented data to train a student \nmodel with a supervised cross-entropy loss. In the second \npart, we used the logarithmic and internal representations from \nthe transformer mechanism to generate training student models \non unlabeled data, using different training schedules to optimize \ndifferent loss functions, and experiments proved that this partial \ndistillation can further regularize these models to improve \nperformance. The specific steps are shown in Figure  2.\nThe definition of the loss function is shown in following  \nformula:\n LL Lsoft hard=+ab  (9)\nThe soft loss function is for the teacher model to label the \nunlabeled data and then train the student model, while the \nhard loss function is for the student model to predict the \nlabel and take the cross-entropy of the labeled data, because \nthere may be  different probability distributions of different \ncategories in the training data. Balanced, a reasonable distribution \nof soft labels and hard labels, that is, the T value in the \nformula can improve the generalization ability of the model.\n \nLp qsoft\nj\nN\nj\nT\nj\nT=- ()å log\n \n(10)\n \np vT\nvT\ni\nT i\nk\nN\nk\n= ()\n()å\nexp/\nexp/  \n(11)\n \nq zT\nzT\ni\nT i\nk\nN\nk\n= ()\n()å\nexp/\nexp/  \n(12)\n \nLc qhard\nj\nN\njj=- ()å log 1\n \n(13)\n \nq z\nz\ni\ni\nj\nN\nj\n1 = ()\n()å\nexp\nexp\n \n(14)\nAmong them, T is the temperature coefficient, which is \nused to control the softening degree of the output probability. \nIt is easy to see that Equation (11) represents the class probability \nof the network output Softmax when T = 1. When T is positive \ninfinity, Hinton et al. (2015)  show in their paper that Equation \n(11) represents the logical unit of the network output at this \npoint. v i and z i are both generated by the neural network \nusing the softmax layer to generate class probabilities. The \nTABLE 1 |  Example of text augmentation.\nOperation Text\nOrigin The film has several strong performances.\nSynonym replacement The film has several solid performances.\nRandom noise injection The film has several strong ha performances.\nRandom swap Several film has the strong performances.\nRandom deletion Film has the strong performances.\nBack translation The film has several strong performances.\nWord2vec-based Another movies have not two strength showings.\nContextual-wordaug However, the animated film usually has several strong \nperformances.\nFIGURE 2 |  Knowledge distillation steps diagram.\nGong et al. Text Sentiment Analysis\nFrontiers in Psychology | www.frontiersin.org 6 May 2022 | Volume 13 | Article 906061\noutput layer converts the logarithm of z i calculated by each \nclass into probabilities q i and pi by comparing z i and other \nlogical probabilities. From Jiao et  al. (2019) , it is known from \nexperiments that knowledge distillation from the transformer \nto the non-transformer framework will limit the knowledge \nlearnable from the teacher model due to the incomparability \nof the parameters of the intermediate layers, due to the transfer \nof knowledge to smaller models, the effect is not good, and \nthe knowledge distillation transferred to the same transformer \nframework, due to the interoperability of the middle layer, \nthe parameters can be  used, and the effect is better than the \nformer because the reduction of the parameter amount will \nalso reduce the running time in the actual processing process \nand computing power costs.\nThe model structure we  designed is shown in Figure  3. \nThe input text data are expanded and then mixed in the \ntransformer layer before training and classification. In addition \nto the traditional modules, the model is mainly divided into \nfour parts, namely, the data augmentation part, the mixed \nlayer, the linear layer, and the stacked module.\nThe mixup and text augmentation modules have been shown \nin the two vignettes above, and the linear layer in Figure  3 \ncan also be  referred to as the bottleneck layer. We  have \nintroduced this linear layer into the model for two reasons: \non the one hand, this bottleneck layer allows for the scaling \nof the input and output sizes to 512, facilitating subsequent \nknowledge transfer and model training; on the other hand, \nthe choice of a linear layer over an activation function prevents \nthe non-linearity from corrupting too much information, while \nthe activation function may filter out much useful information.\nThe addition of the bottleneck structure to the transformer \nlayer causes the balance between the multi-head attention \nmechanism and the feedforward neural network to be broken, \nthat is, the feature mapping of the input and output through \nthe latter two will be  different through the bottleneck layer, \nwhich can cause many problems. The overall structure of \nthe multi-head attention mechanism allows the model to \njointly process information in different subspaces, while the \nfeedforward neural network increases the non-linearity of \nthe model. In the original BERT model, the ratio of the \nmulti-head attention mechanism to the feedforward neural \nnetwork is 1:2. However, with the addition of the bottleneck \nstructure, the input to the multi-head attention mechanism \nwill come from a wider feature map, that is, the size of the \nmodules between each other, while the input to the feedforward \nneural network will come from a narrower bottleneck, that \nis, the size of the modules. These two changes lead to the \ninclusion of more parameters in the multi-headed attention \nmechanism. To address this issue, stacked layers, that is, \nstacked feedforward neural networks, were introduced to \nrebalance the relative relationship between the multi-headed \nattention mechanism and the feedforward neural network. \nSize, and in this chapter, we  use four stacked feedforward \nneural networks for balancing.\nEXPERIMENT\nData Set Introduction\nThis chapter uses two data sets to analyze the model from \nmultiple perspectives. These data sets are AG News Corpus, \nThe Stanford Sentiment Treebank (SST) data set, and AG News \nCorpus with 496,835 items in four categories and more than \n2,000 News articles from a news source, the data set has title \nand description fields, and each category has 30,000 training \nsamples and 1,900 test samples, of which we  randomly select \n10,000 samples for training. The SST data set is a sentiment \nanalysis data set released by Stanford University. Its essence \nis a sentiment classification task, mainly for the sentiment \nclassification of movie reviews. Therefore, SST belongs to single-\nsentence text classification. We  selected SST-2 for the two \nclassification task.\nModel Settings\nIn order to compare the prediction effect of the model, \nwe  compared the results of multiple models on multiple \ntasks and compared the accuracy of multiple models on the \nclassification task, in which a BERT-based-uncased tagger \nwas used to tag the text, and use this model for text encoding. \nIn order to reduce the parameters of the original BERT \nmodel, we  designed the above model to change the effect \nof the student model by adjusting the ratio of the multi-\nhead attention mechanism and the feedforward neural network, \nas shown by the experiments in Mobilebert. The model \nperformance will peak when the ratio between the two is \n0.4 ~ 0.6, so we choose a ratio of 0.5. In addition, the maximum \nsentence length is 256. For sentences that exceed the length, \nFIGURE 3 |  Model structure diagram.\nGong et al. Text Sentiment Analysis\nFrontiers in Psychology | www.frontiersin.org 7 May 2022 | Volume 13 | Article 906061\nwe  keep the first 256 characters, the learning rate is set to \n5e-5, and the batch size is 32. Due to the difference in the \nnumber of training samples, we  set the saving step in the \ndifferent numbers of samples. When the number of trains \nis the maximum value, please refer to Table  2  for \ndetailed parameters.\nBaselines\nWe tried to compare a number of different classification \nmodels. The following are the model names and their \nbrief introductions.\nBERT\nThis model is one of the current mainstream models; we  use \nthe already trained BERT-based-uncased model and fine-tune \nit for classification tasks.\nALBERT\nThis model is a simplified version of the BERT model. It is \ntransformed on the basis of the BERT model and reduces a \nlot of parameters, which greatly improves the speed of model \ntraining and model prediction, and the effect of the model \nwill only be  a slight decrease ( Lan et  al., 2019 ).\nMobileBert\nThis model is a compressed version of the BERT model ( Sun \net  al., 2020 ). It uses knowledge distillation to improve based \non BERT. Its scale is reduced by 3–4 times and the speed is \nincreased by 4–5 times. Compared with the original knowledge \ndistillation model, the model prunes parameters while retaining \nmost of the accuracy.\nResult\nThis chapter conducts comparative experiments on three data \nsets, namely, the SST data set and the AG news corpus. Based \non these three data sets, experiments are conducted with \ndifferent data scales to test the effectiveness of the model; the \nresults are shown in Table  3.\nAs can be  seen from the results in the table, we  have cut \nthe data sets and cut different scales on the different corpus. \nIt can be  seen that when the amount of data is sufficient, \nBERT The results obtained by the model are the best, but \nsince they are all transformer-based models, the difference \nbetween the final results is not large. It is easy to see that \nthe method given in this paper can achieve better results in \nthe case of a small number of samples. For example, in the \ncase where the number of labeled labels in the AG News data \nset is less than 1,000, the method given in this paper has \nachieved relatively good results. The excellent results, especially \nwhen there are only 100 labeled data are greatly improved \ncompared to the other methods. When MobileBERT does not \nintroduce the mixing layer and the data enhancement layer, \nit can be  seen from the above table that the results achieved \nby the same type of data set and the same data scale are \nlower than those achieved by other models.\nFigures 4, 5 can clearly show that when the sentiment analysis \ndata set and the news data set are classified, especially when \nthe number of labeled data is less than 100, our method has \nan improvement of about 20% compared with the other methods. \nThis is due to the small amount of training data and the extreme \ndemands of each transformer-based model for training data, both \nof which lead to problems, such as overfitting of the trained \nmodel, resulting in poor results. The method proposed in this \npaper is when the data scale is small, the method of text expansion \nis provided, which increases the scale of training data to a certain \nextent and thus has better results than other models. With the \nincrease of data scale, the results of each model have a tendency \nto approach each other. When the data size is greater than 1,000, \nthe difference between the results of each model is small. Similar \nto the principle of a small amount of training data, when the \ntraining data reach a certain size, the difference between the \ntraining results of each model is not large. The advantage of \nthe method proposed in this paper is that our results are excellent \nwhen used in a small amount of labeled data, and also have \ngood results in large-scale data.\nTABLE 2 |  Experimental parameter table.\nHyperparameters Value\nThe maximum length 256\nbatch size 32\nLearning rate 5e-5\nepoch 5\nSave step 2,105\nTABLE 3 |  Performance [test accuracy(%)] comparison with baselines.\nData sets Model 100 500 1,000 5,000 All\nAG NEWS BERT 64.99 67.42 84.93 89.93 93.38\nALBERT 60.14 76.45 84.06 89.64 92.06\nMobileBERT 65.05 75.88 84.48 89.47 92.74\nOur Method 80.18 84.01 85.14 89.68 93.28\nModel 20 100 500 1,000 All\nSST BERT 50.92 69.72 73.27 83.03 92.94\nALBERT 48.81 63.87 81.19 83.14 92.42\nMobileBERT 50.91 55.28 81.65 84.17 91.28\nOur Method 78.21 82.35 84.63 85.21 90.03\nBold values indicate the best performance of different models on the same dataset.\nGong et al. Text Sentiment Analysis\nFrontiers in Psychology | www.frontiersin.org 8 May 2022 | Volume 13 | Article 906061\nCONCLUSION\nThe purpose of text sentiment analysis is to analyze and study \nthe large amount of comment information on the web and \nprovide a relevant basis for the authorities to rectify the online \nenvironment. To alleviate the reliance on annotated data, a \ntext sentiment classification method that combines data \naugmentation techniques and Transformer is proposed. Through \nexperiments on two benchmark text classification data sets, \nthe reliability and efficiency of text sentiment classification \nare fully weighed, and the reliability and efficiency of text \nsentiment classification are greatly improved, which also has \ngreat advantages compared with other state-of-the-art research. \nWe plan to explore the effect on tasks outside the experimental \ndata set, such as other real-world scenarios with limited \nlabeled data.\nDATA AVAILABILITY STATEMENT\nPublicly available data sets were analyzed in this study. These \ndata can be  found at: https://gluebenchmark.com/tasks.\nETHICS STATEMENT\nThe individual(s) provided their written informed consent for \nthe publication of any identifiable images or data presented \nin this article.\nAUTHOR CONTRIBUTIONS\nXG was responsible for designing the framework of the entire \nmanuscript, from topic selection to solution to experimental \nverification. All authors contributed to the article and approved \nthe submitted version.\nFUNDING\nThis work was supported by the Humanities and Social Sciences \nFoundation of the Ministry of Education under Grant \n18YJCZH229, and in part by the 13th Five-Y ear Plan Project \nof Educational Science in Jiangsu Province under Grant \nX-a/2018/10, and the National Natural Science Foundation of \nChina (61972059).\n \nREFERENCES\nAraci, D. (2019). Finbert: financial sentiment analysis with pre-trained language \nmodels. arXiv [Preprint]. doi: 10.48550/arXiv.1908.10063\nBudhkar, A., and Rudzicz, F . (2018). Augmenting word2vec with latent \nDirichlet allocation within a clinical application. arXiv [Preprint]. doi: \n10.48550/arXiv.1808.03967\nChawla, N. V ., and Karakoulas, G. (2005). Learning from labeled and unlabeled \ndata: an empirical study across techniques and domains. J. Artif. Intell. Res.  \n23, 331–366. doi: 10.1613/jair.1509\nChen, Y . (2015). Convolutional Neural Network for Sentence Classification. \nMaster's thesis. University of Waterloo\nCnnic (2021). The 45th Statistical Report on China’s Internet Development.\nConneau, A., Schwenk, H., Barrault, L., and Lecun, Y . (2016). Very deep \nconvolutional networks for text classification. arXiv [Preprint]. doi: 10.48550/\narXiv.1606.01781\nDevlin, J., Chang, M. W ., Lee, K., and Toutanova, K. (2018). Bert: pre-training \nof deep bidirectional transformers for language understanding. arXiv [Preprint]. \ndoi: 10.48550/arXiv.1810.04805\nEdunov, S., Ott, M., Auli, M., and Grangier, D. (2018). Understanding back-\ntranslation at scale. arXiv [Preprint]. doi: 10.48550/arXiv.1808.09381\nFellbaum, C. (2010). “WordNet, ” in Theory and Applications of Ontology: Computer \nApplications. eds. R. Poli, M. Healy and A. Kameas (Dordrecht: Springer), 231–243.\nGou, J., Yu, B., Maybank, S. J., and Tao, D. (2021). Knowledge distillation: a \nsurvey. Int. J. Comput. Vis. 129, 1789–1819. doi: 10.1007/s11263-021-01453-z\nGuo, H. (2020). “Nonlinear mixup: Out-of-Manifold Data Augmentation for \nText Classification. ” in Proceedings of the AAAI Conference on Artificial \nIntelligence; April 03, 2020 Vol. 34, 4044–4051.\nHan, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y . (2021). “Transformer \nin transformer, ” in Advances in Neural Information Processing Systems . \neds. M. Ranzato, A. Beygelzimer, Y . Dauphin, P . S. Liang and J. Wortman \nVaughan (MIT Press), 34.\nFIGURE 4 |  Performance [test accuracy (%)] on AG News.\n FIGURE 5 |  Performance [test accuracy (%)] on Stanford Sentiment \nTreebank (SST).\nGong et al. Text Sentiment Analysis\nFrontiers in Psychology | www.frontiersin.org 9 May 2022 | Volume 13 | Article 906061\nHawkins, D. M. (2004). The problem of overfitting. J. Chem. Inf. Comput. Sci.  \n44, 1–12. doi: 10.1021/ci0342472\nHinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a \nneural network. arXiv [Preprint]. doi: 10.48550/arXiv.1503.02531\nHoward, J., and Ruder, S. (2018). Universal language model fine-tuning for \ntext classification. arXiv [Preprint]. doi: 10.48550/arXiv.1801.06146\nJiao, X., Yin, Y ., Shang, L., Jiang, X., Chen, X., Li, L., et al. (2019). Tinybert: \nDistilling bert for natural language understanding. arXiv [Preprint]. doi: \n10.48550/arXiv.1909.10351\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P ., and Soricut, R. (2019). \nAlbert: A lite bert for self-supervised learning of language representations. \narXiv [Preprint]. doi: 10.48550/arXiv.1909.11942\nLeCun, Y ., Bengio, Y ., and Hinton, G. (2015). Deep learning. Nature 521, \n436–444. doi: 10.1038/nature14539\nLin, P ., and Luo, X. (2020). “ A Survey of Sentiment Analysis Based on Machine \nLearning. ” in CCF International Conference on Natural Language Processing \nand Chinese Computing . Springer, Cham. 372–387.\nRezaeinia, S. M., Rahmani, R., Ghodsi, A., and Veisi, H. (2019). Sentiment \nanalysis based on improved pre-trained word embeddings. Expert Syst. Appl. \n117, 139–147. doi: 10.1016/j.eswa.2018.08.044\nSun, Z., Yu, H., Song, X., Liu, R., Y ang, Y ., and Zhou, D. (2020). Mobilebert: \na compact task-agnostic bert for resource-limited devices. arXiv [Preprint]. \ndoi: 10.48550/arXiv.2004.02984\nTiwari, D., and Mallick, B. (2016). SVM and naïve bayes network traffic \nclassification using correlation information. Int. J. Comput. Appl.  147, 1–5. \ndoi: 10.5120/ijca2016911010\nVan Engelen, J. E., and Hoos, H. H. (2020). A survey on semi-supervised \nlearning. Mach. Learn.  109, 373–440. doi: 10.1007/s10994-019-05855-6\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \net al. (2017). Attention is all you  need. Adv. Neural Inf. Proces. Syst.  30:15. \ndoi: 10.48550/arXiv.1706.03762\nWei, J., and Zou, K. (2019). Eda: easy data augmentation techniques for boosting \nperformance on text classification tasks. arXiv [Preprint]. doi: 10.48550/\narXiv.1901.11196\nY an, D., Li, K., Gu, S., and Y ang, L. (2020). Network-based bag-of-words model \nfor text classification. IEEE Access 8, 82641–82652. doi: 10.1109/ACCESS.2020. \n2991074\nZhang, H., Cisse, M., Dauphin, Y . N., and Lopez-Paz, D. (2017). mixup: \nBeyond empirical risk minimization. arXiv [Preprint]. doi: 10.48550/arXiv.1710. \n09412\nZhang, L., Wei, J., and Boncella, R. J. (2020). Emotional communication \nanalysis of emergency microblog based on the evolution life cycle of \npublic opinion. Inf. Discov. Deliv.  48, 151–163. doi: 10.1108/\nIDD-10-2019-0074\nConflict of Interest:  The authors declare that the research was conducted in \nthe absence of any commercial or financial relationships that could be  construed \nas a potential conflict of interest.\nPublisher’s Note: All claims expressed in this article are solely those of the \nauthors and do not necessarily represent those of their affiliated organizations, \nor those of the publisher, the editors and the reviewers. Any product that may \nbe evaluated in this article, or claim that may be made by its manufacturer, is \nnot guaranteed or endorsed by the publisher.\nCopyright © 2022 Gong, Ying, Zhong and Gong. This is an open-access article \ndistributed under the terms of the Creative Commons Attribution License (CC BY). \nThe use, distribution or reproduction in other forums is permitted, provided the \noriginal author(s) and the copyright owner(s) are credited and that the original \npublication in this journal is cited, in accordance with accepted academic practice. \nNo use, distribution or reproduction is permitted which does not comply with \nthese terms."
}