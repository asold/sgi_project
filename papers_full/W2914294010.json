{
  "title": "Compression of recurrent neural networks for efficient language modeling",
  "url": "https://openalex.org/W2914294010",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2748294763",
      "name": "Artem M. Grachev",
      "affiliations": [
        "National Research University Higher School of Economics",
        "Samsung (Russia)"
      ]
    },
    {
      "id": "https://openalex.org/A2050689488",
      "name": "Dmitry I. Ignatov",
      "affiliations": [
        "National Research University Higher School of Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2150671800",
      "name": "Andrey V. Savchenko",
      "affiliations": [
        "National Research University Higher School of Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2748294763",
      "name": "Artem M. Grachev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2050689488",
      "name": "Dmitry I. Ignatov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150671800",
      "name": "Andrey V. Savchenko",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2795921906",
    "https://openalex.org/W6635726886",
    "https://openalex.org/W6640281019",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6684941089",
    "https://openalex.org/W6731079551",
    "https://openalex.org/W6640598943",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2094459996",
    "https://openalex.org/W6633532678",
    "https://openalex.org/W6743031765",
    "https://openalex.org/W6635446068",
    "https://openalex.org/W6638836233",
    "https://openalex.org/W6732814185",
    "https://openalex.org/W6738735913",
    "https://openalex.org/W6685537299",
    "https://openalex.org/W6682051436",
    "https://openalex.org/W6638060716",
    "https://openalex.org/W6730093588",
    "https://openalex.org/W6738495787",
    "https://openalex.org/W6745544366",
    "https://openalex.org/W6703611849",
    "https://openalex.org/W6698517984",
    "https://openalex.org/W6755261065",
    "https://openalex.org/W6601546654",
    "https://openalex.org/W6754543806",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W6631636882",
    "https://openalex.org/W6729239390",
    "https://openalex.org/W6726320248",
    "https://openalex.org/W1993482030",
    "https://openalex.org/W6650436118",
    "https://openalex.org/W6688167117",
    "https://openalex.org/W6720905350",
    "https://openalex.org/W6742632731",
    "https://openalex.org/W6745265922",
    "https://openalex.org/W6755868333",
    "https://openalex.org/W2951595529",
    "https://openalex.org/W2606429533",
    "https://openalex.org/W2559655401",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2175402905",
    "https://openalex.org/W2566563465",
    "https://openalex.org/W2963991999",
    "https://openalex.org/W2964258799",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2592980454",
    "https://openalex.org/W2152332944",
    "https://openalex.org/W2617837836",
    "https://openalex.org/W4297781745",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2559813832",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W1798945469",
    "https://openalex.org/W2963117513",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W1904228841",
    "https://openalex.org/W1899504021",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2963016848",
    "https://openalex.org/W2952533036",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W2739963392",
    "https://openalex.org/W2963042606",
    "https://openalex.org/W4247950230",
    "https://openalex.org/W2617991662",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2964293190",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W4251247712",
    "https://openalex.org/W3163547718",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2003800060",
    "https://openalex.org/W2582745083",
    "https://openalex.org/W1498990157",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2765932895",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W2896528354",
    "https://openalex.org/W4299527668",
    "https://openalex.org/W2607303097",
    "https://openalex.org/W4235832433",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2898992238",
    "https://openalex.org/W4365800080",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2312434537",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2619959423",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1826234144",
    "https://openalex.org/W2119144962",
    "https://openalex.org/W4365799834",
    "https://openalex.org/W2952088488",
    "https://openalex.org/W2747579829",
    "https://openalex.org/W2962832505"
  ],
  "abstract": null,
  "full_text": "arXiv:1902.02380v1  [cs.CL]  6 Feb 2019\nCompression of Recurrent Neural Networks for Eﬃcient\nLanguage Modeling\nArtem M. Grachev a,b, Dmitry I. Ignatov b, Andrey V. Savchenko c\naSamsung R&D Institute, Moscow, Russia\nbNational Research University Higher School of Economics, Moscow, Russia\ncNational Research University Higher School of Economics, Laboratory of Algorithms\nand Technologies for Network Analysis, Nizhny Novgorod, Russia\nAbstract\nRecurrent neural networks have proved to be an eﬀective metho d for sta-\ntistical language modeling. However, in practice their memory and ru n-time\ncomplexity are usually too large to be implemented in real-time oﬄine mob ile\napplications. In this paper we consider several compression techn iques for\nrecurrent neural networks including Long-Short Term Memory mo dels. We\nmake particular attention to the high-dimensional output problem c aused by\nthe very large vocabulary size. We focus on eﬀective compression m ethods\nin the context of their exploitation on devices: pruning, quantizatio n, and\nmatrix decomposition approaches (low-rank factorization and ten sor train\ndecomposition, in particular). For each model we investigate the tr ade-oﬀ\nbetween its size, suitability for fast inference and perplexity. We pr opose\na general pipeline for applying the most suitable methods to compres s re-\ncurrent neural networks for language modeling. It has been show n in the\nexperimental study with the Penn Treebank (PTB) dataset that t he most\neﬃcient results in terms of speed and compression-perplexity balan ce are\nobtained by matrix decomposition techniques.\nKeywords: Recurrent neural network compression, language modeling,\nmobile devices, low-rank factorization\nEmail address: grachev.art@gmail.com (Artem M. Grachev)\n1. Introduction\nWith the increasing popularity of neural networks, the question of their\nimplementation on mobile devices is actually emerging. Consider the lan-\nguage modeling problem [1, 2], in which it is required to develop a prob-\nabilistic mechanism for generating text (statistical language model) . This\nproblem appears in many practical applications, i.e., text sequence g ener-\nation, machine translation, speech recognition and text analysis. T he ﬁrst\nmethods to create a language model were based on the storing of a ll possi-\nble continuations for a given beginning sequence of words. Those me thods\nare the so-called n-gram models [3, 4, 5]. Unfortunately, it is known that\nthese models have common issues. For example, all the chains of the length\nn = 4 for a ﬁxed vocabulary V of size |V| = 10 , 000 naturally occupy several\ngigabytes and the taken memory grows exponentially with an increas e of n.\nThus, the chains of length n = 20 cannot be physically stored in the memory\nof modern computers.\nRecurrent neural networks (RNN) can solve this problem in certain as-\npects. It was shown that contemporary Long-Short Term Memor y (LSTM)\nor Gated Recurrent Units (GRU) models can take into account the lo ng-\nterm dependencies between words [6, 7]. Nowadays RNN-based mod els are\nimplemented in various practical tasks of natural language proces sing due to\ntheir ability to provide high accuracy and can be robustly trained by u sing\nthe well-established hyperparameters. Though conventional con volutional\nneural networks [8, 9] can be also used in these tasks, they are limit ed by\na ﬁxed-length context and cannot directly learn longer-term depe ndencies.\nThe modern transformer models [10, 11] based on attention mech anism are\nnot still well-studied due to their diﬃcult training process. Hence, in t his\npaper we decided to focus on modern RNN-based language models.\nUnfortunately, such language models still have large memory comple x-\nity. Hence, they are inappropriate for use in embedded systems, e .g., mobile\nphones, which usually have less computational resources for mode rn applica-\ntions (for example see in [12]) when compared to modern graphical pr ocessor\nunits (GPU). This problem is especially challenging for the language mod el-\ning task, in which the RNNs are characterized by very large dimension ality\nof the last fully connected layer [13] because this layer produces |V| ≫ 1\nposterior probabilities of all words from the vocabulary.\nThe main contribution of this paper is as follows. Firstly, we proposed\na pipeline to compress the recurrent neural networks for languag e modeling.\n2\nSuch networks can be roughly structured as architectures cont aining the in-\nput embedding level for continuous representation of words in the vector\nspace, recurrent cells and the output layer for prediction of the n ext word\nin a sequence. In the paper we compress the output layer and LSTM -layers\nseparately. We mainly focus on the matrix factorization compressio n tech-\nniques resulting in low-rank matrices as well as more complex approac hes like\nTensor-Train decomposition. Secondly, we made particular attent ion to the\nhigh-dimensional output problem caused by the very large vocabula ry size.\nThis problem is especially challenging in the language modeling tasks with\nword prediction. We presented the solution of this problem using low- rank\nand Tensor Train decomposition of the weight matrices in the last fully -\nconnected layer of the neural network. Thirdly, we implemented ou r models\nfor GPUs of a mobile phone in order to test the studied algorithms on r eal\ndevices. It was experimentally shown that compression techniques based on\nmatrix factorization suﬃciently speed up the inference in the langua ge model.\nThis article is an extended version of our conference paper [14]. In com-\nparison with our previous paper we: 1) formulated the methodology for\ncompression of language models; 2) presented an approach to solv ing the\nhigh-dimensional output problem; 3) signiﬁcantly extended the sur vey of re-\nlated works and references section; 4) provided plenty of new exp eriments\nfor compression of conventional baselines (i.e., those that were de scribed by\nZaremba et al [15]) including their several extensions; 5) measured inference\ntime for GPU computations on a real mobile device.\nThe paper is organized as follows. Section 2 overviews related works .\nIn Section 3, we give an overview of language modeling task and then f o-\ncus on respective RNN-based approaches to this task. Next, in Se ction 4,\nwe describe diﬀerent types of compression. In Subsection 4.1, we c onsider\nthe simplest known methods for neural networks compression like p runing\nand quantization. In Subsection 4.2, we consider compression of ne ural net-\nworks based on diﬀerent matrix factorization methods. Subsectio n 4.3 deals\nwith Tensor Train decomposition (TT-decomposition). In Subsectio n 4.4 we\npresent the general pipeline of the methodology to compress RNN- based lan-\nguage models and make them suitable for oﬄine usage in mobile devices.\nSection 5 describes our experimental results and important impleme ntation\ndetails. Finally, in Section 6, we summarize the results of our work and\ndiscuss future research.\n3\n2. Related works\nGenerally, there are several approaches to the neural network compres-\nsion. They can be roughly divided into two kinds, namely, the methods based\non sparse computations and the methods based on using diﬀerent p roperties\nof weights matrices, e.g., matrix factorization.\nThe ﬁrst kind of techniques include pruning and quantization and was\noriginally applied in computer vision. In one of the ﬁrst works on these\nmethods [16] it was shown that pruning makes it possible to remove a lo t of\nweights before doing quantization without loss of accuracy. It was veriﬁed\nfor such neural networks as LeNet, AlexNet, VGGNet that prunin g can re-\nmove 67% for convolutional layers and up to 90% for fully connected layers.\nMoreover, an even higher compression ratio can be achieved by com bining\npruning with mixed precision [17].\nThe pruning techniques have been justiﬁed in terms of variational d ropout.\nThe variational dropout was introduced in [18] as a method for auto matic\ntuning of a proper dropout rate, i.e. the probability that a given neu ron will\nbecome active. In [19, 20], the authors adapt this dropout techniq ues for the\nneural network compression. In their studies, the dropout rate is allowed to\nbe equal to one, which is equivalent to a complete elimination of this neu ron\nfrom a network. As a matter of fact, pruning and quantization are able to\nprovide a rather large reduction of the size of a trained network fo r the models\nstored on a hard disk. However, there are several issues when we try to use\nsuch models in the inference phase. They are caused by high comput ation\ntime of sparse computing with the prunned matrices. The example of one\npossible solution is the so-called structured pruning as for example in [20, 21],\nwhen a set of rows or columns is dropped in a certain layer matrix.\nAnother branch of compression methods include diﬀerent matrix de com-\nposition approaches by using either matrices of lower sizes or exploit ing the\nproperties of special matrices involved in a compression method. Fo r ex-\nample, the paper [22] proposes a new type of RNNs based on poten tially\nmore compact unitary matrices, called Unitary Evolution Recurrent Neural\nNetworks (better than RNN in copying memory and adding problems) . In\n[23], the authors applied the so-called FastFood transform for fully -connected\nand convolutional layers (up to 90% of compression in terms of the n umber\nof parameters stored). Diﬀerent matrix decomposition technique s can be\nalso related to this class of compression methods. These methods c an be as\nsimple as low-rank decomposition or more complex like Tensor Train (TT )\n4\ndecomposition [24, 25, 26, 27]. However, the TT-based approach h ave not\nbeen studied in language modeling task, where there are such issues as high-\ndimensional input and output data, and, as a consequence, more o ptions\nto conﬁgure TT decomposition. Thus, the second kind of methods a llows\ncompressing neural networks, get reduction in the model size and still have\nsuitable non-sparse matrices for multiplication.\nMost of described methods can be used for compression of RNNs. M eth-\nods based on matrix decomposition of RNNs were mainly applied in auto-\nmatic speech recognition [28, 29, 26]. For example, the usage of the Toeplitz-\nlike structured matrices in [28] gave up to 40% compression of RNN fo r\nvoice search task. Compression of embeddings layers is considered in the\npaper [30].\nOne of the most challenging problems, which inﬂuenced the large size o f\na language model, is the very-high dimensionality at the output layer c aused\nby the huge size of vocabulary. This problem is widely discussed in the\nliterature. For example, peculiarities of unknown and rare words pr esence\nare considered in [31]. Huge computation complexity and big size of the\nsoftmax layer is discussed in [13, 32]. Morin et al [32] develop the idea o f\nhierarchical softmax computation. They show that it is possible to o btain\nO(\n√\n|V|) parameters for this way of softmax computation. Bengio et al.[13]\npropose the method for speeding up softmax computation by samp ling of\nsubset of words from the available vocabulary on each iteration dur ing the\ntraining phase.\nThe pruning with the variational dropout technique was applied to co m-\npression of RNNs in natural language processing tasks [21, 33]. How ever, the\nresults in language modeling [21] are signiﬁcantly worth in terms of ac hieved\nperplexity when compared even with classical results of Zaremba et al [15].\nMoreover, the acute problem with high-dimensional output is comple tely ig-\nnored.\nHence, it seems that there is still no study which provides a methodo logy\nto compress RNNs in language modeling, which overcomes the diﬃcultie s\npeculiar to high-dimensional output layers and makes it possible to ac hieve\nboth low memory and run-time complexity in order to be implemented in\nreal-time oﬄine mobile applications.\n5\n3. RNNs in language modeling problem\nConsider the language modeling problem, in which it is required to es-\ntimate the probability of a sentence or sequence of words ( w1, . . . , w T ) in a\nlanguage L.\nP(w1, . . . , w T ) = P(w1, . . . , w T − 1)P(wT |w1, . . . , w T − 1) =\n=\nT∏\nt=1\nP(wt|w1, . . . , w t− 1) (1)\nThe use of such a model directly requires estimation of posterior pr oba-\nbility P(wt|w1, . . . , w t− 1). In general, this estimation has too much run-time\ncomplexity. Hence, a typical approach approximates it with the pro bability\nP(wt|wt− n, . . . , w t− 1) of the next word given a ﬁxed number n of previous\nwords. This naturally leads us to n-gram models [4, 5], in which a discrete\nprobability distribution P(wt|wt− n, . . . , w t− 1) is given by a table with ( n + 1)\ncolumns, which contains the count of phrases with ( n + 1) sequential words\nin a large text corpora. It was a common approach for language mod el-\ning until the middle of the 2000s. Unfortunately, such an approach requires\na very large memory to store the long term dependencies. Moreove r, the\nprobabilities of rare phrases are usually underestimated.\nThus, a new milestone in the domain had become the use of RNNs, which\nwere successfully implemented for language modeling in the papers [34 , 35,\n36]. Consider an RNN, where L is the number of recurrent layers, xt\nℓ is the\ninput of the layer ℓ at the moment t. Here t ∈ { 1, . . . , T }, ℓ ∈ { 1, . . . , L },\nand xt\n0 is the embedding vector. We can describe each layer as follows:\nzt\nℓ =Wℓxt\nℓ− 1 + Uℓxt− 1\nℓ + bl (2)\nxt\nℓ =σ(zt\nℓ), (3)\nwhere Wℓ and Vℓ are matrices of weights and σ is an activation function. The\noutput of the network uses the softmax activation:\nyt = softmax\n[\nWL+1xt\nL + bL+1\n]\n. (4)\nThen, we estimate the posterior probability in Eq. 1) as an output of such\nRNN:\nP(wt|w1, . . . , w t− 1) = yt. (5)\n6\nWhile the n-gram models even with not very large n require a lot of\nmemory space due to the combinatorial explosion, RNNs can learn re presen-\ntations of words and their sequences without memorizing directly all word\ncontexts.\nNowadays the mainly used variations of RNN are designed to solve the\nproblem of vanishing gradients, which usually appears in the training w ith\nlong sequences [2, 37]. The most popular implementations of the RNN s\nwhich do not suﬀer from this problem are LSTM [6] and GRU [7] networ ks.\nRNN-based approaches to the language modeling problem are eﬃcien t\nand widely adopted, but still require a lot of space. For example, eac h\nLSTM layer with the input dimensionality k and output size k involves eight\nmatrices of size k × k. Moreover, usually in language modeling applications,\none wants the model to use words (rather than characters) as t he fundamental\nunits as the input and the output. These peculiarities naturally lead u s\nto large sizes for both the input and output layers. The input layer is an\nembedding layer that maps every word from vocabulary V to a vector. The\noutput layer is an aﬃne transformation from a hidden representat ion to the\noutput space, for which then we apply the softmax function. The s ize of\nvocabulary, |V|, is of the order of thousands or even tens of thousand. Hence,\nthe matrices in the input and output layers contain |V|×k parameters. Thus,\nthe number of parameters in the whole network with L LSTM layers and the\ndimensionality of the input embeddings identical to the size of the hidd en\nstate k is given by\nntotal = 8 Lk2 + 2|V|k. (6)\nLet us analyze the contribution of each term in (6) in a realistic scena rio.\nFor the PTB dataset we have the vocabulary size |V| = 10 , 000. Consider an\nLSTM network with two hidden layers of k = 650 units in each layer. Each\nLSTM layer includes eight matrices of size 650 × 650, i.e. 650 × 650 × 8 × 2 =\n6.76M parameters . The output layer in this network have 650 × 10000 =\n6.5M parameters. Similar calculations for the LSTM with the size of hidde n\nlayers of 1500 give us 36M parameters and 15M parameters, respe ctively.\nThus, we can see that the output (softmax) layer can occupy up t o one third\nof the whole network. Note that the embedding of a network can oc cupy the\nsame memory size if we do not use techniques like “tied softmax” [38, 3 9].\nHence, in this paper we decided to address this problem by performin g several\nexperiments with the softmax layer using low-rank decomposition an d TT\ndecomposition to reduce its size.\n7\n4. Compression methods\n4.1. Pruning and quantization\nIn this subsection, we consider very simple though not the most eﬀe ctive\ntechniques to compress neural networks. Some of them were suc cessfully\napplied to audio processing [17] and image processing [40]. However, they\nare not yet well-studied in the language modeling task [14].\nPruning is a method for reducing the number of parameters of a neu ral\nnetwork by removing the weights, which are approximately equal to zero.\nIn Fig. 1 (top), one can notice that usually the majority of weight va lues is\nconcentrated near zero. It means that such weights do not prov ide a valuable\ncontribution to the ﬁnal output. Hence, we can remove from the n etwork all\nthe connections with the weights, which do not exceed a certain thr eshold\n(Fig. 1 (bottom)). After that, the network is ﬁne-tuned to learn the ﬁnal\nweights for the remaining sparse connections.\nQuantization is a method for reducing the size of a compressed neur al\nnetwork in memory. In this technique, each ﬂoating-point value of w eight is\npacked into, e.g., 8-bit integer representing the closest real numb er in one of\n256 equally-sized intervals within the whole range of the weight.\nPruning and quantization have common disadvantages since they do not\nsupport training from scratch. Moreover, their practical usage is quite labo-\nrious. The reason for such behaviour of pruning mostly lies in the ineﬃ ciency\nof sparse computing. In the case of quantization, the model is sto red in an\n8-bit representation, but 32-bits computations are still required . It means\nthat we do not obtain advantages using in-memory techniques at lea st until\nthe tensor processing unit (TPU) is not used, which is adapted for e ﬀective\n8- and 16-bits computations.\n4.2. Low-rank factorization\nLow-rank (LR) factorization [28] represents more powerful tec hniques to\ncompress the matrices of weights. A simple LR decomposition for RNN can\nbe implemented as follows:\nxt\nl = σ\n[\nW a\nℓ W b\nℓ xt\nℓ− 1 + Ua\nl Ub\nl xt− 1\nℓ + bl\n]\n(7)\n8\n−3 −2 −1 0 1 2 30\n0.5\n1\n1.5\n·105\nValue\nFrequency\n−3 −2 −1 0 1 2 3\n0\n2,000\n4,000\n6,000\n8,000\nValue\nFrequency\nFigure 1: An example of weights distribution before (top) and after pruning (bottom)\nThe paper [28] requires the following constraint: W b\nl = Ub\nℓ− 1. Thus, the RNN\nequation can be rewritten as follows:\nxt\nl = σ\n[\nW a\nl mt\nl− 1 + Ua\nl mt− 1\nl + bl\n]\n(8)\nmt\nl = Ub\nl xt\nl (9)\nyt = softmax\n[\nWL+1mt\nL + bL+1\n]\n(10)\nCompression of LSTM and GRU layers is implemented in a similar way\nbut with slightly more complex equations. First let us describe one laye r of\nLSTM:\n9\nit\nℓ = σ\n[\nW i\nl xt\nl− 1 + Ui\nl xt− 1\nl + bi\nl\n]\ninput gate (11)\nft\nℓ = σ\n[\nW f\nl xt\nl− 1 + Uf\nl xt− 1\nl + bf\nl\n]\nforget gate (12)\nct\nℓ = ft\nl ⊙ ct− 1\nl + it\nl tanh\n[\nW c\nl xt\nl− 1 + Uc\nl xt− 1\nl + bc\nl\n]\ncell state (13)\not\nℓ = σ\n[\nW o\nl xt\nℓ− 1 + Uo\nl xt− 1\nl + bo\nl\n]\noutput gate (14)\nxt\nℓ = ot\nℓ · tanh[ct\nl ], (15)\nAnd here are equations for the GRU layer:\nzt\nl = σ\n(\nW z\nl xt\nl− 1 + Uz\nl xt− 1\nl\n)\nupdate gate (16)\nrt\nl = σ\n(\nW r\nl xt\nl− 1 + Ur\nl xt− 1\n)\nreset gate (17)\n˜xt\nl = tanh\n(\nW h\nl xt\nl− 1 + Uh\nl\n(\nrt\nl · xt− 1\nl\n))\nproposal output (18)\nxl\nt =(1 − zt\nl ) ⊙ xt\nl− 1 + zt\nl ⊙ ˜xt\nl ﬁnal output (19)\nwhere ct\nℓ is the memory vector at the layer ℓ and time step t. The output of\nthe network is given by the same Eq. 4 as above.\nThe compressed LSTM is described by the same equations 11-15, bu t the\nsizes of matrices are changed. Here, similarly to the RNN case, we re quire\nexistence of a special matrix W p\nl such that W ib\nl = W fb\nl = W cb\nl = W ob\nl =\nUib\nl− 1 = Ufb\nl− 1 = Ucb\nl− 1 = Uob\nl− 1 = W p\nl and the output of the network is computed\nas follows:\nˆxl\nl = W p\nl xt\nl . (20)\nThe situation is more peculiar with the GRU cells. If we just reduce\nthe sizes of matrices W z\nl , U z\nl , W f\nl , U f\nl to k × r, we will end up with wrong\ndimensions in Eq. (18-19). That is why we reduce those matrices dow n\nto r × r, reduce W h\nl , U h\nl down to r × k and introduce the projection matrix\nW p\nl ∈ Rk× r after Eq. 18 so that Eq. 19 is applied in the same way by replacing\n˜xl\nt to:\nxlp\nt = W p\nl ˜xl\nt. (21)\nThe main advantage of the LR technique lies in potentially small sizes\nr × k and k × r of matrices W a\nl /Ub\nl and Ua\nl , respectively (in case of RNN).\nThose sizes are much less than the size k × k of the original weight matrices,\n10\nWl and Vl, if r ≪ k. With a reasonably small r we obtain the advantage\nboth in size and multiplication speed. The same considerations are valid for\nLSTM and GRU cells with 8 and 6 matrices, respectively.\n4.3. Tensor Train decomposition\nTaking into account the recent advances of TT decomposition in dee p\nlearning [24, 25], we have also decided to apply this technique to recur rent\nneural network compression for language modeling.\nThe TT decomposition was originally proposed as an alternative and mo re\neﬃcient form of tensor representation [41]. Let us describe how t his decompo-\nsition could be applied to neural networks. Consider, for example, t he weights\nmatrix W ∈ Rk× k of the RNN layer (2). One can arbitrarily choose such num-\nbers k1, . . . , k d so that k1 ×. . . ×kd = k×k, and reshape the weights matrix to\na tensor ⃗W ∈ Rk1× ...× kd . Here d is an order (degree) of a tensor, k1, . . . , k d are\nthe sizes of each dimension. Thus we can perform the TT-decompos ition of\nthe tensor ⃗W and obtain a set of matrices Gm[im] ∈ Rrm− 1× rm , im = 1 , . . . , k m,\nm = 1 , . . . , d and r0 = rd = 1 such that each of the tensor element can be\nrepresented as ⃗W (i1, i2, . . . , i d) = G1[i1]G2[i2] . . . G d[id]. Here r0, . . . r m are\nthe ranks of the decomposition. Such TT decomposition can be eﬃcie ntly\nimplemented with the TT-SVD algorithm described in [41]. In fact, each\nGm ∈ Rrm− 1× km× rm is a three-dimensional tensor with the second dimen-\nsion km corresponding to the dimension of the original tensor and two rank s\nrm− 1, rm, that in certain sense is a size of an internal representation for th is\ndimension. It is necessary to emphasize that even with the ﬁxed num ber for\ndimensions of reshaped tensors and their sizes we still have plenty o f variants\nto choose the ranks in the TT-decomposition.\nLet us denote these two operations of converting matrix W to ⃗W and\ndecomposing it on TT format as one operation TT( W ). Applying it to both\nthe matrices W and V from Eq. 2 we obtain TT-RNN layer in next form:\nzt\nℓ = σ(TT(Wl)xt\nℓ− 1 + TT(Ul)xt− 1\nℓ + bℓ). (22)\nSimilarly we can apply TT-decomposition to each matrix of LSTM layer\n(11)-(14) or the matrix of the output layer (4). Moreover, acco rding to [41],\nthe matrix-by-vector product and matrix sum can be eﬃciently imple mented\ndirectly in the TT format without the need to convert these matrice s to the\nTT.\n11\nThe TT compression can be achieved by choosing the internal ranks\nr1, . . . , r d− 1. Let R = max\nm=0,...,d\nrm, K = max\nm=0,...,d\nkm. Hence, the number of pa-\nrameters for the TT-decomposition is equal to NTT = ∑ rm− 1kmrm ≤ dR2K.\nIn fact, each factor in this product can be smaller an order of magn itude than\nthe original k.\nThis approach was successfully applied to compress fully connected neu-\nral networks [24], to develop convolutional TT layer [25] and to com press\nand improve RNNs [26, 27]. However, there are still no studies of the TT de-\ncomposition for language modeling and similar tasks with high-dimension al\noutputs at the softmax layer.\n4.4. Proposed pipeline for compressing RNN model\nTo sum it all up, we propose a general pipeline (Fig. 2) for the compre s-\nsion of RNNs in the language modeling problem. Here, ﬁrstly, the inter nal\nrecurrent layers of RNNs are compressed. Then we continue by co mpression\nof external embedding layers and the output one by using either co nventional\nmatrix factorization or TT decomposition. In addition, pruning and q uan-\ntization can be applied for the preliminary decomposed neural nets. The\nresulted compressed language model can be used directly on mobile d evices.\nWe developed an optimized version of inference in the LR-factorized RNNs\nusing the GPU of a modern mobile phone.\nFigure 2: Proposed pipeline of the RNN compression for eﬃcient langu age modeling\n12\n5. Experimental results\n5.1. Experimental setup\nWe choose the models from [15] as a reliable baseline. By default, ther e\nare Small, Medium, and Large LSTM models with the sizes of their hidden\nlayers 200, 650, and 1500, respectively, but we provide additional experi-\nments with diﬀerent hidden sizes and with diﬀerent types of cells like RN N\nand GRU. All experiments are produced with the PTB (Penn TreeBan k)\ndataset [34].\nWe compare all the models in terms of two quality metrics: perplexity\nand the number of parameters. The perplexity of language models is a\nconventional quality metric for language modeling. The value p of perplexity\nshows that the model is as confused on test data as if it had to choo se uni-\nformly and independently among p options for each word. In addition, we\ncharacterize the quality of the model by the average word prediction ac-\ncuracy, i.e., the probability to correctly predict the next word in a sequence ,\nwhich can be estimated as one divide by perplexity.\nFinally, we measure the average inference time using our own imple-\nmentation of inference in RNNs for mobile GPUs. We have performed t esting\non a real mobile device to compare the performance of compressed models. A\nmobile phone Samsung S7 Edge with GPU processor Mali-T880 was used in\nour experiments. The calculations are carried out after the “warm ing phase”\n(100 preliminary calculation loops) to achieve a maximum computationa l\nperformance of the mobile GPU. The inference time results are aver aged\nover 1000 runs for each model.\nWe implemented all the above-mentioned compression techniques. T he\npruning and quantization were tested for small LSTM model from [1 5]. In\naddition, we thoroughly studied how matrix factorization technique s perform\nfor LSTM layers of diﬀerent sizes as well as for nets based on such u nits as\nGRU and RNN. We have tried to hold compression ratio in a range x3-x5 . For\nexample, let us describe the sizes for one of the obtained decompos itions with\nthe LSTM 650-650 model. We start with the initial sizes for W ∈ R650× 650\nU ∈ R650× 650, and |V| = 10 , 000. The corresponding matrix for the embed-\nding is Wemb ∈ R10,000× 650 and the matrix for the output is Wout ∈ R10,000× 650.\nThe size of each weight matrix, W and U, is reduced down to 650 × 128 and\nthe sizes of the embedding and output matrices are down to 10 , 000 × 128\nand 128 × 10, 000, respectively. The value 128 is chosen as the most suitable\ndegree of 2 for eﬃcient device implementation. We have performed s everal\n13\nexperiments with other size options, but the above-mentioned con ﬁguration\nis the best in terms of compression-perplexity balance. The diagram s of the\noriginal and the LR-compressed models are shown in Fig. 3.\n(a)\n(b)\nFigure 3: Neural network architectures: (a) original LSTM 650-650, (b) LR-compressed\nmodel\nIn order to appropriately choose the hyperparameters of our mo dels and\nthe training procedure (learning rate, schedule, dropout rate, s izes of decom-\nposition, number of hidden neurons, etc.) and avoid expensive grid s earch in\nthe space of the parameters, we have followed the random search procedure\nin the respective hyperparameter space.\nIn the case of TT-decomposition under a ﬁxed decomposition schem e, we\nexamine diﬀerent values of internal rank and choose the value, whic h provides\nthe lowest perplexity. We set the basic conﬁguration of an LSTM-ne twork\nwith two 600-600 layers and four tensors for each matrix in a layer. The\nsize of layers is chosen as 600 by 600 instead of 650 by 650 due to bet ter\nfactorization for TT-decomposition with more divisors: 600 = 2 · 3 · 5 · 2 · 5 · 2\n14\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 950\n100\n200\n300\n400\n500\n600\nEpoch\nPerplexity\nTraining\nValidation\nFigure 4: Learning curves for training LR LSTM 500-500model\nversus 650 = 2 · 5 · 5 · 13. Then we perform a grid search through a diﬀerent\nnumber of dimensions and various tensor rank values.\nTo obtain the best perplexity, we perform two stages of training fo r the\nLR-compressed models. At ﬁrst, Adam optimizer is used. Then we sw itch to\nSGD (Stochastic Gradient Descent). An example of typical learning curves\nfor training of LR LSTM 500-500 model is given in Fig. 4. In the auto-\nmated tuning process we try to prevent the overﬁtting by using co nventional\nregularization techniques including stopping the training procedure when the\nvalidation perplexity starts to increase.\n5.2. Compression results\nThe main results of our research for diﬀerent compression models a re\nsummarized in Table 1.\nAs one can see, the use of pruning and quantization let us obtain suit able\ncompression level, sometimes even without quality loss (e.g., see quan tiza-\ntion). Unfortunately, those methods are not well-suited to decre ase the run-\nning time of inference on mobile devices. In fact, the diﬀerence in infe rence\ntime with the baseline LSTM 200-200 is not statistically signiﬁcant, when\nusing the McNemar’s test with signiﬁcance level 5%.\n15\nTable 1: Compression results on PTB dataset\nModel Size, Mb\nNo. of\nparam.,\nM\nTest\nperplex-\nity\nAvg.\nword pre-\ndiction\naccuracy\nInference\ntime, ms\nPTB\nBaselines\nLSTM 200-200 18.6 4.64 117.659 0.0085 9.63\nLSTM 300-300 29.8 7.45 91.95 0.0109 10.24\nLSTM 400-400 42.24 10.56 86.687 0.0115 12.4\nLSTM 500-500 56 14 84.778 0.0118 14.03\nLSTM 650-650 79.1 19.7 82.07 0.0122 16.13\nRNN 650-650 67.6 16.9 124.371 0.008 15.91\nGRU 650-650 72.28 18.07 92.86 0.0108 16.94\nLSTM 1500-1500 264.1 66.02 78.29 0.0128 45.47\nOurs\nLSTM 200-200\npruning output layer\n90% w/o additional\ntraining\n5.5 0.5 149.31 0.0067 9.56\nLSTM 200-200\npruning output layer\n90% with additional\ntraining\n5.5 0.5 121.123 0.0083 9.56\nLSTM 200-200\nquantization (1 byte\nper number)\n4.7 4.64 118.232 0.0085 9.61\nLR LSTM 200-200 3.712 0.928 136.115 0.0073 7.83\nLR LSTM 300-300 8.228 2.072 113.691 0.0088 8.39\nLR LSTM 400-400 13.12 3.28 106.623 0.0094 8.82\nLR LSTM 500-500 14.336 3.584 97.282 0.0103 8.95\nLR LSTM 650-650 16.8 4.2 92.885 0.0108 9.68\nLR RNN 650-650 35 8.75 134.111 0.0075 11.03\nLR GRU 650-650 12.76 3.19 111.06 0.009 8.74\nTT LSTM 600-600 50.4 12.6 168.639 0.0059 16.75\nLR LSTM 1500-1500 94.9 23.72 89.462 0.0112 15.70\nAn important feature of the LR decompositions is high eﬃciency of op -\nerations on mobile devices. The experiments have demonstrated th at our\nmodel LR LSTM 650-650 compressed with the LR-factorization is even\nsmaller than LSTM 200-200, though the perplexity of the latter model is\n16\nmuch worse and this diﬀerence in perplexity is statistically signiﬁcant. The\ninference time of our compressed model LR LSTM 650-650 on mobile\nphone remains approximately identical to the inference time in the sim ple\nLSTM 200-200. In all cases the diﬀerence in the model size and the infer-\nence time of compressed and base models are statistically signiﬁcant .\nThe best obtained result for the TT decomposition ( TT LSTM 600-\n600) is even worse than LSTM 200-200 both in terms of size and perplex-\nity. Hence, we can conclude that the LR-decomposition is the most s uitable\ntechnique to compress the recurrent cells, because it decreases the memory\nspace and inference time without large degradation in perplexity.\nTable 2: State-of-the-art models on PTB dataset\nModel Size, Mb No. of\nparam., M Test perplexity\nAvg. word\nprediction\naccuracy\nRNN-LDA+KN-5 cache [42] 36 9 92 0.0109\nLSTM 650-650 [15] 79.1 19.7 82.7 0.0121\nVariational LSTM (MC) [43] 80 20 78.6 0.0127\nCharCNN [9] 76 19 78.9 0.0127\nVariational RHN [44] 92 23 65.4 0.0153\nAWD-LSTM [45] 88 22 55.97 0.0179\nAWD-LSTM-MoS [46] 88 22 54.44 0.0184\nTrellisNet-MoS [47] 136 34 54.19 0.0185\nLSTM-SparseVD [33] 13.248 3.312 109.2 0.0092\nLSTM-SparseVD-VOC [33] 6.688 1.672 120.2 0.0083\nIn Table 2, the state-of-the-art perplexities for language modelin g prob-\nlem are assembled. In addition, we present in the last two rows of this table\nthe best known results (for the PTB dataset) of compressed RNN s using\nSparseVD method [33]. Here the number of parameters for the com pressed\nmodel from the paper [33] is computed in line with the remaining models a s\nfollows. We assume that all computation is performed in matrix form. There-\nfore, we take all non-zero words and all non-zero LSTM cells (even though\nthere are parameters containing zero). Then the number of para meters is\ncomputed according to Eq. 6.\nIt has been shown that despite their higher prediction quality, the n um-\n17\nber of parameters for the best methods is 3-6 times higher the size s of our\ncompressed models and these results are statistically signiﬁcant. M oreover,\nfrom a practical viewpoint, the average accuracy of the next wor d prediction\nis a more interpretable value. One can notice by comparison of Table 1 and\nTable 2, that these accuracies of our compressed models and the s tate-of-\nthe-art models are rather close to each other. It is important to e mphasize\nthat our approach (Fig. 2) makes it possible to obtain lower perplexit y and\nmodel sizes than the existing compression technique [33].\n5.3. Last layer decomposition\nIn the next experiments we have analyzed the eﬀectiveness of LR a nd TT\ndecomposition applied to the last (fully-connected) layer. We ﬁx the neural\nnetwork architecture and change only this layer. We perform a ran domized\nsearch for each network over the following parameters: internal size of de-\ncomposition, TT-ranks, starting learning rate, learning rate sche dule, and\ndropout rate. The best improvements in speed and memory consum ption\nachieved by compressing only the last high-dimensional layer are sho wn in\nTable 3.\nTable 3: LR and TT decomposition of the output layer\nModel Size, Mb\nNo. of\nparameters,\nM\nNo. of\noutput layer\nparameters,\nM\nTest per-\nplexity\nPTB\nBenchmarks\nLSTM 200-200 18.6 4.64 2.0 117.659\nLSTM 650-650 79.1 19.7 6.5 82.07\nLSTM 1500-1500 264.1 66.02 15.0 78.29\nLR for\nSoftmax layer\nLSTM 200-200 12.6 3.15 0.51 112.065\nLSTM 650-650 57.9 14.48 1.193 84.12\nLSTM 1500-1500 215.4 53.85 2.829 89.613\nTT for\nSoftmax layer\nLSTM 200-200 11.8 2.95 0.304 116.588\nLSTM 600-600 51.12 12.8 1.03 88.551\nLSTM 1500-1500 215.8 53.95 2.92 85.63\nHere, the decomposition of the last layer only reduces the model siz e\nin 1.2-1.5 times. Even though in general perplexity is increased, we ha ve\nsucceeded to decrease perplexity of the simplest LSTM 200-200model. One\n18\ncan notice that the TT-decomposition is rather promising in this part icular\ntask. However, the achieved quality of TT representation is still ve ry unstable\nand drastically depends on the learning parameters. Hence, usually we have\nto examine much more conﬁgurations to obtain admissible results.\n6. Conclusion\nIn this paper, we examined several methods of RNNs compression f or\nthe language modeling problem. Much attention was paid to the speciﬁ c\nproblem of high-dimensional output, which is especially crucial in langu age\nmodeling due to the very large vocabulary size. Such techniques as p runing,\nquantization, low-rank matrix factorization and Tensor Train deco mposition\nwere experimentally compared in terms of their accuracy (perplexit y), model\nsize and inference speed on the real mobile device. We tested them a cross\ndiﬀerent types of RNNs (LSTM/GRU/RNN) and diﬀerent model sizes (Ta-\nble 1).\nAs a result, we formulated the general methodology (Fig. 2) for co m-\npressing such types of models and make them suitable for implementa tion\nin oﬄine mobile applications. Our pipeline is suitable for any such net with\nLSTM cells and high-dimensional input and output matrices (Table 1, T a-\nble 3). At the moment of submission, this is one of the ﬁrst implementa tions\nof RNNs (moreover, compressed ones) for mobile GPUs.\nIt was shown that the main beneﬁt obtained from compression of RN Ns\nby means of LR matrix decomposition in comparison to pruning and qua nti-\nzation lies in the fact that we almost do not lose the speed of matrix mu lti-\nplication and the memory gain becomes almost equal to the operation s gain.\nIn contrast, nowadays, many methods works with sparse matrice s, which are\nable to provide memory gain, but fail with operations gain. Our exper imen-\ntal results on the mobile device conﬁrmed that the LR-compression of the\nmodel LR LSTM 650-650 is more eﬃcient in both memory and running-\ntime complexity of the inference.\nSince our approach is studied for recurrent neural nets, certain state-of-\nthe-art models (Table 2) that are based on alternative RNN modiﬁca tions\n(e.g., Trellis network, RHN, and AWD-LSTM-MOS) of classic architect ures\nhave not been tested along with our compression schemes due to th eir “frag-\nile” architecture with many separate hacks applied. Hence, as for t he prospec-\ntive venues of future research, we leave the implementation of com pression\nmethods for these complex models, many of which have major modiﬁc ations\n19\nin comparison to conventional LSTMs/GRUs and require specialized in di-\nvidual treatment.\nAcknowledgements.\nThe work of A.V. Savchenko and D.I. Ignatov was prepared within th e\nframework of the Basic Research Program at the National Resear ch Univer-\nsity Higher School of Economics (HSE) and supported within the fra mework\nof a subsidy by the Russian Academic Excellence Project ’5-100’. The au-\nthors have no conﬂicts of interest to declare.\nThe authors would like to thank Dmitriy Polubotko for his valuable help\nwith the experiments on mobile devices.\nReferences\nReferences\n[1] W. B. Croft, J. Laﬀerty, Language modeling for information ret rieval,\nSpringer Science & Business Media, 2003.\n[2] H. Deng, L. Zhang, X. Shu, Feature memory-based deep recur rent neural\nnetwork for language modeling, Appl. Soft Comput. 68 (2018) 432– 446.\n[3] F. Jelinek, R. L. Mercer, Interpolated estimation of Markov sou rce pa-\nrameters from sparse data, in: E. S. Gelsema, L. N. Kanal (Eds.), Pro-\nceedings, Workshop on Pattern Recognition in Practice, North Holla nd,\nAmsterdam, 1980, pp. 381–397.\n[4] R. Kneser, H. Ney, Improved backing-oﬀ for m-gram language m odel-\ning, in: 1995 International Conference on Acoustics, Speech, an d Signal\nProcessing, ICASSP ’95, Detroit, Michigan, USA, May 08-12, 1995, pp.\n181–184.\n[5] F. Jelinek, Statistical Methods for Speech Recognition, MIT Pre ss, 1997.\n[6] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neu ral Com-\nputation 9 (1997) 1735–1780.\n[7] K. Cho, B. van Merrienboer, D. Bahdanau, Y. Bengio, On the pro p-\nerties of neural machine translation: Encoder-decoder approac hes, in:\n20\nProceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Se -\nmantics and Structure in Statistical Translation, Doha, Qatar, 25 Octo-\nber 2014, pp. 103–111.\n[8] N. Pham, G. Kruszewski, G. Boleda, Convolutional neural netwo rk\nlanguage models, in: Proceedings of the 2016 Conference on Empiric al\nMethods in Natural Language Processing, EMNLP 2016, Austin, Te xas,\nUSA, November 1-4, 2016, pp. 1153–1162.\n[9] Y. Kim, Y. Jernite, D. Sontag, A. M. Rush, Character-aware ne ural\nlanguage models, in: Proceedings of the Thirtieth AAAI Conference on\nArtiﬁcial Intelligence, February 12-17, 2016, Phoenix, Arizona, U SA.,\npp. 2741–2749.\n[10] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: pre-training of\ndeep bidirectional transformers for language understanding, Co RR\nabs/1810.04805 (2018).\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A . N. Gomez,\nL. Kaiser, I. Polosukhin, Attention is all you need, in: Advances in Ne u-\nral Information Processing Systems 30: Annual Conference on N eural\nInformation Processing Systems 2017, 4-9 December 2017, Long Beach,\nCA, USA, pp. 6000–6010.\n[12] A. D. Cheok, J. Zhang, C. E. Siong, Eﬃcient mobile phone chinese\noptical character recognition systems by use of heuristic fuzzy r ules and\nbigram markov language models, Appl. Soft Comput. 8 (2008) 1005–\n1017.\n[13] Y. Bengio, J. Senecal, Quick training of probabilistic neural nets by\nimportance sampling, in: Proceedings of the Ninth International Wo rk-\nshop on Artiﬁcial Intelligence and Statistics, AISTATS 2003, Key We st,\nFlorida, USA, January 3-6, 2003.\n[14] A. M. Grachev, D. I. Ignatov, A. V. Savchenko, Neural netw orks com-\npression for language modeling, in: Pattern Recognition and Machine\nIntelligence - 7th International Conference, PReMI 2017, Kolkat a, India,\nDecember 5-8, 2017, Proceedings, pp. 351–357.\n[15] W. Zaremba, I. Sutskever, O. Vinyals, Recurrent neural net work regu-\nlarization, CoRR abs/1409.2329 (2014).\n21\n[16] S. Han, J. Pool, J. Tran, W. J. Dally, Learning both weights and c onnec-\ntions for eﬃcient neural network, in: Advances in Neural Informa tion\nProcessing Systems 28: Annual Conference on Neural Informat ion Pro-\ncessing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada,\npp. 1135–1143.\n[17] S. Han, H. Mao, W. J. Dally, Deep compression: Compressing dee p\nneural network with pruning, trained quantization and huﬀman cod ing,\nCoRR abs/1510.00149 (2015).\n[18] D. P. Kingma, T. Salimans, M. Welling, Variational dropout and the\nlocal reparameterization trick, in: C. Cortes, N. D. Lawrence, D. D.\nLee, M. Sugiyama, R. Garnett (Eds.), Advances in Neural Informa tion\nProcessing Systems 28, Curran Associates, Inc., 2015, pp. 2575 –2583.\n[19] D. Molchanov, A. Ashukha, D. P. Vetrov, Variational dropout sparsiﬁes\ndeep neural networks, in: Proceedings of the 34th Internationa l Confer-\nence on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-1 1\nAugust 2017, pp. 2498–2507.\n[20] K. Neklyudov, D. Molchanov, A. Ashukha, D. P. Vetrov, Struc tured\nbayesian pruning via log-normal multiplicative noise, in: Advances\nin Neural Information Processing Systems 30: Annual Conferenc e on\nNeural Information Processing Systems 2017, 4-9 December 201 7, Long\nBeach, CA, USA, pp. 6778–6787.\n[21] E. Lobacheva, N. Chirkova, D. Vetrov, Bayesian sparsiﬁcatio n of recur-\nrent neural networks, arXiv preprint arXiv:1708.00077 (2017).\n[22] M. Arjovsky, A. Shah, Y. Bengio, Unitary evolution recurrent neural\nnetworks, in: Proceedings of the 33nd International Conferenc e on Ma-\nchine Learning, ICML 2016, New York City, NY, USA, June 19-24, 20 16,\npp. 1120–1128.\n[23] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola, L. Son g,\nZ. Wang, Deep fried convnets, in: 2015 IEEE International Confe rence\non Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 20 15,\npp. 1476–1483.\n[24] A. Novikov, D. Podoprikhin, A. Osokin, D. P. Vetrov, Tensorizin g neu-\nral networks, in: Advances in Neural Information Processing Sys tems\n22\n28: Annual Conference on Neural Information Processing Syste ms 2015,\nDecember 7-12, 2015, Montreal, Quebec, Canada, pp. 442–450.\n[25] T. Garipov, D. Podoprikhin, A. Novikov, D. P. Vetrov, Ultimate\ntensorization: compressing convolutional and FC layers alike, CoRR\nabs/1611.03214 (2016).\n[26] A. Tjandra, S. Sakti, S. Nakamura, Compressing recurrent n eural net-\nwork with tensor train, in: 2017 International Joint Conference o n\nNeural Networks, IJCNN 2017, Anchorage, AK, USA, May 14-19, 2017,\npp. 4451–4458.\n[27] R. Yu, S. Zheng, A. Anandkumar, Y. Yue, Long-term forecas ting using\ntensor-train rnns, CoRR abs/1711.00073 (2017).\n[28] Z. Lu, V. Sindhwani, T. N. Sainath, Learning compact recurren t neural\nnetworks, in: 2016 IEEE International Conference on Acoustics , Speech\nand Signal Processing, ICASSP 2016, Shanghai, China, March 20-2 5,\n2016, pp. 5960–5964.\n[29] R. Prabhavalkar, O. Alsharif, A. Bruguier, I. McGraw, On the c ompres-\nsion of recurrent neural networks with an application to LVCSR aco ustic\nmodeling for embedded speech recognition, in: 2016 IEEE Internat ional\nConference on Acoustics, Speech and Signal Processing, ICASSP 2016,\nShanghai, China, March 20-25, 2016, pp. 5970–5974.\n[30] A. Acharya, R. Goel, A. Metallinou, I. S. Dhillon, Online embedding\ncompression for text classiﬁcation using low rank matrix factorizat ion,\nCoRR abs/1811.00641 (2018).\n[31] C ¸ . G¨ ul¸ cehre, S. Ahn, R. Nallapati, B. Zhou, Y. Bengio, Pointin g the\nunknown words, in: Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2016, August 7-12, 2 016,\nBerlin, Germany, Volume 1: Long Papers.\n[32] F. Morin, Y. Bengio, Hierarchical probabilistic neural network la nguage\nmodel, in: Proceedings of the Tenth International Workshop on Ar tiﬁ-\ncial Intelligence and Statistics, AISTATS 2005, Bridgetown, Barba dos,\nJanuary 6-8, 2005.\n23\n[33] N. Chirkova, E. Lobacheva, D. P. Vetrov, Bayesian compress ion for\nnatural language processing, in: Proceedings of the 2018 Confer ence on\nEmpirical Methods in Natural Language Processing, Brussels, Belg ium,\nOctober 31 - November 4, 2018, pp. 2910–2915.\n[34] T. Mikolov, M. Karaﬁ´ at, L. Burget, J. Cernock´ y, S. Khudan pur, Re-\ncurrent neural network based language model, in: INTERSPEECH\n2010, 11th Annual Conference of the International Speech Com munica-\ntion Association, Makuhari, Chiba, Japan, September 26-30, 2010 , pp.\n1045–1048.\n[35] Y. Bengio, R. Ducharme, P. Vincent, C. Janvin, A neural proba bilistic\nlanguage model, Journal of Machine Learning Research 3 (2003) 11 37–\n1155.\n[36] T. Mikolov, Statistical Language Models Based on Neural Netwo rks,\nPh.D. thesis, Brno University of Technology, 2012.\n[37] S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber, Gradie nt ﬂow\nin recurrent nets: the diﬃculty of learning long-term dependencies , S.\nC. Kremer and J. F. Kolen, eds. A Field Guide to Dynamical Recurrent\nNeural Networks (2001).\n[38] H. Inan, K. Khosravi, R. Socher, Tying word vectors and word classiﬁers:\nA loss framework for language modeling, CoRR abs/1611.01462 (201 6).\n[39] O. Press, L. Wolf, Using the output embedding to improve langua ge\nmodels, in: Proceedings of the 15th Conference of the European C hapter\nof the Association for Computational Linguistics, EACL 2017, Valen cia,\nSpain, April 3-7, 2017, Volume 2: Short Papers, pp. 157–163.\n[40] A. G. Rassadin, A. V. Savchenko, Deep neural networks perf ormance\noptimization in image recognition, Proceedings of the 3rd Internatio nal\nConference on Information Technologies and Nanotechnologies (I TNT)\n(2017).\n[41] I. V. Oseledets, Tensor-train decomposition, SIAM J. Scientiﬁ c Com-\nputing 33 (2011) 2295–2317.\n24\n[42] T. Mikolov, G. Zweig, Context dependent recurrent neural ne twork\nlanguage model, in: 2012 IEEE Spoken Language Technology Worksh op\n(SLT), Miami, FL, USA, December 2-5, 2012, pp. 234–239.\n[43] Y. Gal, Z. Ghahramani, A theoretically grounded application of dr opout\nin recurrent neural networks, in: Advances in Neural Informatio n Pro-\ncessing Systems 29: Annual Conference on Neural Information P rocess-\ning Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 10 19–\n1027.\n[44] J. G. Zilly, R. K. Srivastava, J. Koutn´ ık, J. Schmidhuber, Recu rrent\nhighway networks, in: Proceedings of the 34th International Con ference\non Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 Aug ust\n2017, pp. 4189–4198.\n[45] S. Merity, N. S. Keskar, R. Socher, Regularizing and optimizing L STM\nlanguage models, CoRR abs/1708.02182 (2017).\n[46] Z. Yang, Z. Dai, R. Salakhutdinov, W. W. Cohen, Breaking the so ftmax\nbottleneck: A high-rank RNN language model, CoRR abs/1711.03953\n(2017).\n[47] S. Bai, J. Z. Kolter, V. Koltun, Trellis networks for sequence mo deling,\nCoRR abs/1810.06682 (2018).\n25",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8642966151237488
    },
    {
      "name": "Perplexity",
      "score": 0.7980948686599731
    },
    {
      "name": "Language model",
      "score": 0.609917402267456
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5502679944038391
    },
    {
      "name": "Treebank",
      "score": 0.535800576210022
    },
    {
      "name": "Artificial neural network",
      "score": 0.46394988894462585
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4529125690460205
    },
    {
      "name": "Memory footprint",
      "score": 0.44084393978118896
    },
    {
      "name": "Pipeline (software)",
      "score": 0.4397995173931122
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Dependency (UML)",
      "score": 0.0
    }
  ]
}