{
    "title": "Fast Parallel Training of Neural Language Models",
    "url": "https://openalex.org/W2741884846",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A1983914940",
            "name": "Tong Xiao",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2496766346",
            "name": "Jingbo Zhu",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2163293007",
            "name": "Tongran Liu",
            "affiliations": [
                "Institute of Psychology, Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2122370144",
            "name": "Chunliang Zhang",
            "affiliations": [
                "Northeastern University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2120861206",
        "https://openalex.org/W1558797106",
        "https://openalex.org/W2138302120",
        "https://openalex.org/W36903255",
        "https://openalex.org/W2951781666",
        "https://openalex.org/W2131462252",
        "https://openalex.org/W2144839971",
        "https://openalex.org/W3015789953",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2166706236",
        "https://openalex.org/W2049469158",
        "https://openalex.org/W2127163275",
        "https://openalex.org/W2952033860",
        "https://openalex.org/W2113104171",
        "https://openalex.org/W2463033603",
        "https://openalex.org/W2152808281",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2963804082",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2949198759",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2407022425",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2138243089"
    ],
    "abstract": "Training neural language models (NLMs) is very time consuming and we need parallelization for system speedup. However, standard training methods have poor scalability across multiple devices (e.g., GPUs) due to the huge time cost required to transmit data for gradient sharing in the back-propagation process. In this paper we present a sampling-based approach to reducing data transmission for better scaling of NLMs. As a ''bonus'', the resulting model also improves the training speed on a single device. Our approach yields significant speed improvements on a recurrent neural network-based language model. On four NVIDIA GTX1080 GPUs, it achieves a speedup of 2.1+ times over the standard asynchronous stochastic gradient descent baseline, yet with no increase in perplexity. This is even 4.2 times faster than the naive single GPU counterpart.",
    "full_text": "Fast Parallel Training of Neural Language Models\nTong Xiaoy, Jingbo Zhuy, Tongran Liuz, Chunliang Zhangy\nyNiuTrans Lab., Northeastern University, Shenyang 110819, China\nzInstitute of Psychology (CAS), Beijing 100101, China\nfxiaotong,zhujingbo,zhangclg@mail.neu.edu.cn, liutr@psych.ac.cn\nAbstract\nTraining neural language models (NLMs) is very\ntime consuming and we need parallelization for\nsystem speedup. However, standard training meth-\nods have poor scalability across multiple devices\n(e.g., GPUs) due to the huge time cost required\nto transmit data for gradient sharing in the back-\npropagation process. In this paper we present a\nsampling-based approach to reducing data trans-\nmission for better scaling of NLMs. As a “bonus”,\nthe resulting model also improves the training\nspeed on a single device. Our approach yields sig-\nniﬁcant speed improvements on a recurrent neu-\nral network-based language model. On four N-\nVIDIA GTX1080 GPUs, it achieves a speedup of\n2.1+ times over the standard asynchronous stochas-\ntic gradient descent baseline, yet with no increase\nin perplexity. This is even 4.2 times faster than the\nnaive single GPU counterpart.\n1 Introduction\nNeural language models (NLMs) use continuous representa-\ntions of words to predict their probability distributions. Sev-\neral good models have been developed, showing very promis-\ning results in many natural language processing (NLP) tasks.\nThe simplest of these uses feedforward neural networks to\nlearn a distribution over the vocabulary given a number of\nhistory words [Bengio et al., 2003], whereas others resort to\nrecurrent neural networks (RNNs) for modeling sequences of\narbitrary length [Mikolov et al., 2010].\nAs in standard neural networks, a neural language mod-\nel consists of multiple layers of neurons (or neural units). It\nrepresents each word using a group of neurons, and the num-\nber of model parameters increases linearly as more words are\ninvolved. Thus, the parameter set is huge for non-toy models\ntrained on real world data though they have stronger ability\nfor prediction. E.g., for a language model with 50k vocab-\nulary size and 1k internal layer size, the parameter number\nis larger than 100 million. It is well known that it is slow to\ntrain such big models using stochastic gradient descent (S-\nGD). Several research groups have been aware of this and\ndesigned good methods to speed up the learning process of\nNLMs [Bengio and Sen´ecal, 2003; Morin and Bengio, 2005;\nMnih and Hinton, 2008; Mnih and Teh, 2012; Zoph et al.,\n2016].\nHowever, training time is still unsatisfactory on a single\ndevice due to its limited computation capability. The next\nobvious step is toward parallelized training, e.g., running the\nwork on a machine with two or more GPU cards. To do this, a\npopular way is data parallelism where different GPUs process\ndifferent minibatches of samples in parallel. Unfortunately, a\nnaive adaptation of existing NLMs in the high-latency paral-\nlelization scenario is inefﬁcient [Bengio and Sen ´ecal, 2008].\nThe reason is that we have to accumulate the gradient for each\nprocessor and then share the gradient for model update. The\nprocess of gradient sharing requires huge data transmission\nwhich is extremely slow between two GPUs or between a G-\nPU and a CPU. It is found to take up to 50% of the time of\nthe back-propagation process in a minibatch1.\nA solution to this problem is to minimize the data trans-\nmission time. This motivates us to develop a sampling-based\napproach to reducing data transmission for better scaling of\nNLM training. In our approach, we sample a small portion\nof data for transmission and share gradients on the selected\ndata. Beyond this, we apply the sampling method to gradient\ncomputation of a single minibatch for further system speedup.\nWe experiment with the proposed approach in a state-of-the-\nart RNN-based language model on three different sized tasks.\nExperimental results show that it yields a signiﬁcant scaling\nimprovement over the baseline. On four NVIDIA GTX1080\nGPUs, it achieves a 2.1x speedup over the standard asyn-\nchronous SGD method, yet with no increase in perplexity.\nThis is 4.2 times faster than the naive single GPU system.\n2 Background\n2.1 Neural Language Model\nNeural language models resemble the general architecture\nof neural network-based systems. They operate by creating\nconnections between neurons which are organized into dif-\nferent layers. A layer can be deﬁned as a procedure that\nmaps an input vector x = (x1;:::;x m)T to an output vector\ny= (y1;:::;y n)T, like this\ny = f(s) (1)\n1The result was obtained on a machine with an Intel X99 main-\nboard and 4 GPUs. The minibatch size was set to 64.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n4193\nembedding\nhidden\noutput\n1 0 0 0\n<s>\nrock\nembedding\nhidden\noutput\n0 0 1 00\nrock\nit\nembedding\nhidden\noutput\n0 0 0 1\nit\n</s>\none-hot\nrepresentation\ninput word\noutput\ndistribution\noutput word\nA Recurrent Neural Network-based Language Model\nembedding layer\ny= s\ns= We \u0001 x\nparameters: We\nhidden layer\ny= tanh(s)\ns= Wh \u0001 x+ bh\nparameters: Wh, bh\noutput layer\ny= softmax(s)\ns= Wo \u0001 x+ bo\nparameters: Wo, bo\nWo, bo Wh, bh We\nparameter server:\u0012new = \u0012\u0000 \u0011\u0001 @E\n@\u0012\nProcessor 2\non GPU2 (G2)\nProcessor 1\non GPU1 (G1)\nProcessor 3\non GPU3 (G3)\npush (P)\n@E\n@\u0012\nfetch (F)\n\u0012new\nF minibatch3 P\nF minibatch2 P\nF minibatch1 P\nUpdate\nG3 G2 G1\nSynchronous\nTraining\ntime line\nF minibatch3 P\nF minibatch2 P\nF minibatch1 P\nUpdate\nUpd.\nG3 G2 G1\nAsynchronous\nTraining\ntime line\nFigure 1:\nAn illustration of RNN-based language models and parallel training. tanh(\u0001 ) = hyperbolic tangent function. <s> and </s>\nrepresent the beginning and the end of a word sequence.\ns = W \u0001x+ b (2)\nwhere the model parameters are the n\u0002mweight matrix W\nand the n-dimensional bias termb. Wi;j represents the weight\nof the connection from the j-th neuron in the input to the i-\nth neuron in the output. f(\u0001) is an activation function that\ngenerally performs a non-linear transformation on s before\npassing it to the output.\nA neural language model consists of three types of layers\n\u000fInput layer. It receives the one-hot representation of a\nword and outputs its real-valued multi-dimensional rep-\nresentation (called word embedding).\n\u000fOutput layer. This layer produces the probability for\neach word in the vocabulary by using the softmax func-\ntion - f(si) = exp(si)=P\ni0 exp (si0)\n\u000fHidden layer. This layer is sandwiched between the\ninput and output layers. Its output is regarded as a high-\nlevel representation of the input sequence. For a deep\nrepresentation, several hidden layers can be stacked.\nDifferent topologies of these layers can lead to differen-\nt networks. A feedforward neural network-based language\nmodel takes a ﬁxed number of history words as input and\npredicts the next one. For variable-length sequences of word-\ns, the recurrent neural network-based language model applies\nthe same recurrent layer over the sequence of word embed-\ndings, and the representation of the entire sequence is encod-\ned in the output of the ﬁnal hidden layer. See Figure ?? (top)\nfor a running example of a RNN-based language model.\nThe inference process (or forward process) of an NLM is\nstraightforward. E.g., in RNN-based LMs, we can pass vari-\nous vectors through the network and obtain P(wkjw1:::wk\u00001)\nin the output layer corresponding to the k-th word. A popu-\nlar method of training RNN models is stochastic gradient de-\nscent. We run a backward process to calculate the gradients\nfor model parameters \u0012with respect to the error (denoted as\n@E\n@\u0012 ), namely back-propagation. Then we update the model\nby \u0012new = \u0012\u0000\u0011\u0001@E\n@\u0012 , where \u0011is the learning rate.\n2.2 Parallel Training\nFor faster training, a popular way is parallel SGD. In this\nwork we follow the framework presented in [Dean et al.,\n2012]. It distributes training across multiple model instances.\nEach model instance is assigned to a processor and is respon-\nsible for processing a minibatch of samples. The processors\ncommunicate with a centralized parameter server for model\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n4194\n0\nBB@\n1\n0\n1\n1\n0\n1\nCCA\n 1 0 0 0 0\n0 0 1 0 0\n0 0 0 1 0\n!\n0\nBB@\n:3 :1 :2\n0 :5 :2\n\u00001 :1 :3\n:7 :7 :2\n0 :3 :4\n1\nCCA\n :3 :1 :2\n\u00001 :1 :3\n:7 :7 :2\n!\n\u0002 =\nmarks t row selection matrix T original matrix W matrix W0\nfor transmission\n :3 :1 :2\n\u00001 :1 :3\n:7 :7 :2\n!\nmatrix W0\non the target GPU\ntransmit\nGPU1: GPU2:\nFigure 2: Sampling-based data transmission.\nupdate, which keeps the latest state of all parameters of the\nmodel. The parameter server can be sharded across sever-\nal devices or machines, where each shard stores and applies\nupdates to a portion of the model parameters.\nFor a processor, before processing the minibatch, it asks\nthe parameter server for a current copy of the model parame-\nters. After processing the minibatch, it pushes the gradients to\nthe server and sends an update message. See Figure ?? (bot-\ntom) for an illustration of the parallel training model used in\nthis work.\nIn general, there are two paradigms for parameter update.\nThe server can wait until all the processors ﬁnish their job-\ns, and then refresh the model parameters after receiving the\ngradients from the processors, i.e., the training runs in a syn-\nchronous manner. Alternatively, processors can run in an\nasynchronous way in which the server starts the update once\na processor ﬁnishes the upload of the gradient information\n(Figure ??). Theoretically, asynchronous parallel training can\nnot guarantee good convergency as its synchronous counter-\npart - but it is faster and shows good empirical results in many\nlarge-scale systems.\nAnother way for fast parallel training is to overlap com-\nmunication and computation. This can be done by using the\nmulti-way data transmission model of modern GPUs. Note\nthat the problem discussed in this paper is orthogonal to the\nmethod of overlapping data transmission and computation.\nIn the following we restrict ourselves to the model where no\nsuch overlaps are employed.\n3 Data Transmission via Sampling\nIn parallel training of NLMs, processing a minibatch re-\nquires two programs: 1) we ﬁrst employ the back-propagation\nmethod to obtain the gradients for the minibatch, and then 2)\nwe upload the gradients onto the parameter server and wait\nuntil the server ﬁnishes the update and sends back the new\nparameters. The second program takes little time if we run\nit on a single GPU with a high speed of on-chip data copy.\nBut it is not the case when we switch to the multi-GPU envi-\nronment. We observe that in a normal NLM setting, the data\ntransmission between two GPUs takes 50% of the time of the\nback-propagation process. This means that we roughly have\n50% inefﬁciency at best if we transmit the parameters with\nno other cost.\nA possible solution to the issue is that processors com-\nmunicate updates with the parameter server occasionally,\nrather than performing parameter update for every minibatch\n[Zhang et al., 2015]. Though it can reduce the total amount\nof the transmitted data, we ﬁnd that it is harmful to NLMs if\nwe delay the update of the model parameters. Because the\ngradient matrices of softmax layers and hidden layers are big\nand dense, we need to keep the update in a relatively frequent\nfashion so that the learning can have reasonable convergence.\nHere we present a method that balances the sending over\ntime. The idea is pretty simple - we only select a small (nec-\nessary) portion of the data for transmission. Let W be an\nn\u0002mmatrix we need to copy from a GPU to another 2, and\nW(i) be the i-th row of W. For rows of W, we generate an\nn-dimensional vector t = ( t1;:::;t n)T, where ti = 0 or 1.\nThen we transmit W(i) only if ti = 1 , and do not perform\ndata copy for W(i) if ti = 0 . Here we call tthe row-based\nmark for data transmission.\nMore formally, letn0be the number of the non-zero entries\nof t, and !i0 be the index of the i0-th non-zero entry of t.\nWe deﬁne an n0\u0002nmatrix T subject to Ti0;j = 1 if j = !i0,\notherwise Ti0;j = 0. In other words, Ti0;j = 1 means that tj is\nthe i0-th non-zero entry of t. Then, we deﬁne the transmitted\nmatrix W0to be:\nW0 = T0\u0002W (3)\nEq. (3) actually performs a transformation onW by select-\ning rows of W with a non-zero mark in t. To implement this\nway of data copy between processor and server, all we need\nis to transmit W0from the source device and unpack it to W\non the target device. Other parts of the training procedure can\nproceed as usual. See Figure 2 for an illustration of the data\nselection and transmission in our method.\nNote that the matrix multiplication in Eq. (3) is not heavy\nbecause W0 is very sparse. Also, transmitting W0 from a\nprocessor to the server can be implemented via sparse matrix\ntransmission. Here we choose a straightforward method that\npacks W0into a continuous block of data and unpacks it on\nthe server end. Then the server can do sparse matrix opera-\ntions after collecting the gradients from the processors. This\nprocess is basically the same as that of the baseline system\nand we can reuse the parameter fetching and pushing strate-\ngies.\nTo determine which rows of a matrix are selected, we\nchoose different methods for different layers. In the softmax\nlayer, each row of the weight matrix corresponds to a word.\n2The matrix keeps either the model parameters or the gradients.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n4195\nRather than random sampling, a better way is to keep updat-\ning the model on the most frequent words, which can lead to\nfast convergence. Also, the words in the minibatch can be\nseen as positive samples that we intend to learn from. Thus,\nwe design the following sets of words to make the vector t\n\u000fWe select the words in the minibatch, denoted as Vbase.\n\u000fWe select p[%] of the most frequent words in the vocab-\nulary, denoted as V\u000b.\n\u000fWe randomly selectq[%] of the words in the vocabulary,\ndenoted as V\f.\nThen, we generate the union set Vall = Vbase\nSV\u000b\nSV\f\nand deﬁne that ti = 1 only if wordi 2 Vall. The use of\nV\f introduces randomness into training and makes the model\nbehave more stable on the test data. For hidden layers, we\ngenerate tin a random manner, where we only use parameter\nqto control how often a row is selected to make t.\nAnother note on our method. For the embedding layer, the\ngradient is non-zero only for columns of the input words. In\nthis case we employ a column-based method that transmit-\ns the gradients or model parameters for the active columns\nof the words in the minibatch. Though column-based data\ntransmission is applicable to all other layers, using row-based\ntransmission has several beneﬁts (for softmax layers and hid-\nden layers). First, a row in W is for an output neuron of the\nlayer. Row selection is essentially a process that randomly\nomits the update of some neurons. This idea is similar to that\nof Dropout [Srivastava et al., 2014], which has been proven\nto improve the robustness of neural network-based models on\nthe unseen data. In addition, row-based copy is efﬁcient be-\ncause the data elements are stored in a block of contiguous\nmemory space3. This can ease its implementation in practical\nsystems.\n4 Gradient Computation via Sampling\nAs we only transmit gradients for some of the neurons in a\nlayer, it is natural to save more time by discarding the com-\nputing work of those neurons whose gradients are not sent to\nthe parameter server. This makes more sense for the softmax\nlayer whose weight matrix is huge. Generally, the big soft-\nmax layer is a bottleneck of the forward and backward pro-\ncesses for NLMs. Therefore, we apply the sampling method\nto gradient computation in the softmax layer.\nLet g = (g1;:::;g n)T be the gold-standard word probabil-\nity distribution for the softmax layer (gi = 1 if wordi is the\ncorrect answer), and E(g;y) be the error function given the\ngold standard gand the layer output y. Here we choose cross\nentropy as the error function. By using Eqs. (1) and (2), we\ncompute the gradient of the error with respect to Wi;j:\n@E(g;y)\n@Wi;j\n= ( yi \u0000gi) \u0001xj (4)\nEq. (4) implies a two-step procedure - for each word (with\nindex i), we ﬁrst calculate its output probability yi in the for-\nward phase, and then calculate @E(g;y)\n@Wi;j\nfor each (i;j) in the\n3We assume matrices are in a row-major order for storage.\nentry PTB FBIS Xinhua\ntraining (words) 952K 4.94M 110M\nvalidation (words) 76K 101K 115K\ntest (words) 85K 104K 118K\nvocabulary size 15K 30K 40K\nembedding size 512 512 1024\nhidden size 512 512 1024\nminibatch size 64 64 64\nTable 1: Data and model settings.\nbackward phase. Given the fact that we only transmit @E(g;y)\n@Wi;j\nfor wordi 2Vall, there is no need to compute the gradient for\nwordi =2Vall in the backward phase.\nAlso, we can obtain yi based on a sampled set of words in\nthe forward phase. Given the word indices for data transmis-\nsion Vall, we randomly select additional \u0016[%] of the words in\nthe vocabulary (V\r). Then, we compute yi over the union set\nof the two: Vf = Vall\nSV\r, like this\nyi =\n( exp(si)\nP\nwordi02Vf\nexp(si0) if wordi 2Vf\n0 otherwise\n(5)\nAn efﬁcient implementation of Eq. (5) is that we compute\nfsi jwordi 2Vfgand exclude fsi jwordi =2Vfgfrom\nproducing the output. It can beneﬁt from faster operations on\nsmaller matrices4. In this work we also apply this method to\nobtaining @E(g;y)\n@bi\nand @E(g;y)\n@xj\nfor further speedup.\nNote that the method presented here basically shares a\nsimilar idea as presented in [Bengio and Sen ´ecal, 2003;\n2008]. Unlike previous work, we employ two different sam-\nplers for the forward-backward process. We use a slightly\nlarger word set (Vf) to compute yi in the forward phase, and\na smaller word set (V all) to compute @E(g;y)\n@Wi;j\nin the back-\nward phase. It seems like a procedure that introduces some\ndummy words (i.e., words in V\r controlled by \u0016) into back-\npropagation. In this way, the probability can be distributed to\nthese dummy words. It is very similar to the way of generat-\ning noise for robust training[Hoaglin et al., 1983]. In general,\nsuch a method is very beneﬁcial to learning statistical models\nwith good generalization ability.\n5 Experiments\n5.1 Experimental setup\nWe experimented with the proposed approach in a recurren-\nt neural network-based language model. We chose asyn-\nchronous SGD for the training framework. We ran all ex-\nperiments on a machine with four NVIDIA GTX1080 GPUs.\nThe quality of language modeling was measured by the av-\nerage per-word log-scale probability (perplexity, or PPL for\nshort). Three different sized tasks were chosen for training\nand evaluation.\n4Instead, we can use sparse matrix operations, e.g, multiplica-\ntion on sparse matrices. But we found that it was faster to operate\nslimmed dense matrices.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n4196\nPTB FBIS Xinhua\nEntry Valid. Test Speed Valid. Test Speed Valid. Test Speed\n1 GPU baseline 85.2 90.8 20.4 45.2 50.7 11.0 48.3 55.7 4.48\n+ gradient comp. sampling 86.7 87.2 26.9 45.0 49.1 15.4 48.6 55.7 6.27\nbaseline 84.3 88.9 33.6 46.2 51.8 16.7 48.5 56.8 7.03\n2 GPUs + data trans. sampling 88.1 86.2 39.1 48.8 50.2 21.4 50.1 54.2 8.64\n+ gradient comp. sampling 86.4 87.3 56.1 47.7 50.0 29.0 49.4 55.1 11.6\nbaseline 87.0 87.4 40.8 45.5 51.0 20.9 47.6 56.0 8.46\n4 GPUs + data trans. sampling 86.9 87.0 65.4 48.0 49.8 36.3 47.9 56.3 15.1\n+ gradient comp. sampling 85.0 88.2 86.2 47.3 51.2 50.8 46.2 55.7 20.4\nTable 2: Perplexity and speed [k words/second] results of different systems.\nbaseline + data trans. sampling + grad. comp. sampling\n1 2 3 4\n2:0\n4:0\n6:0\n8:0\n10:0\n(a) PTB\n1 2 3 40:0\n2:0\n4:0\n6:0\n(b) FBIS\n1 2 3 40:0\n1:0\n2:0\n(c) Xinhua\nFigure\n3: Speed [10k words/second] against GPU number.\n\u000fPenn Treebank (PTB). It is the standard data set used in\nevaluating LMs. We used sections 00-20 as the training\ndata, sections 21-22 as the validation data, and sections\n23-24 as the test data.\n\u000fFBIS. We used the Chinese side of the FBIS corpus\n(LDC2003E14) for a medium sized task. We extract-\ned a 4,000 sentence data set for validation and a 4,027\nsentence data set for test. We left the rest for training.\n\u000fXinhua. For large-scale training, we generated a 4.5\nmillion sentence set from the Xinhua portion of the En-\nglish Gigaword (LDC2011T07). The validation and test\nsets (5,050 and 5,100 sentences) were from the same\nsource but with no overlap with the training data.\nFor the FBIS and Xinhua tasks, we removed sentences of\nmore than 50 words from the training data. All Chinese sen-\ntences in the FBIS data were word segmented using the tool\nprovided within NiuTrans [Xiao et al., 2012]. We used vo-\ncabularies of 15K, 30K and 40K most frequent words for the\nthree tasks. The out of vocabulary (OOV) words were marked\nwith a special UNK token. See Table 1 for a summary of the\ndata and model settings.\nFor RNN-based LM, we chose long short-term memory\n(LSTM) for the recurrent unit [Hochreiter and Schmidhuber,\n1997]. The weights in all the networks were initialized with a\nuniform distribution in [-0.1, 0.1]. The gradients were clipped\nso that their norm was bounded by 3.0. For all experiments,\ntraining was iterated for 20 epochs. We started with a learning\nrate of 0.7 and then halved the learning rate if the perplexity\nincreased on the validation set. In our sampling-based ap-\nproach, we set \u0016= 5% by default. For softmax layers, we set\np= 10% and q = 10%. For hidden layers, we set q = 90%,\nwhich could prevent the system from disturbing the training\nof the LSTM layer too much.\n5.2 Results\nWe ﬁrst compare the system speed by varying the number\nof GPUs. Figure 3 shows that the standard asynchronous S-\nGD method (baseline) has poor scaling due to the time cost\nof data transmission. The NLM can be scaled up better when\nsampling-based data transmission is employed. Also, the sys-\ntem runs faster when the sampling method is applied to gradi-\nent computation in the back-propagation process. It achieves\nover 1.3x speedups on a single GPU for all three of the tasks.\nUnder the 4-GPU setting, it achieves a speedup of 2.1x over\nthe asynchronous SGD baseline, and is even 4.2 times faster\nthan the naive 1-GPU counterpart. More interestingly, it is\nobserved that more speed improvements can be obtained on\nbigger models (as in Xinhua). This is because that sampling\nover a larger parameter set discards more data for transmis-\nsion and thus saves more time.\nThen we study the quality of various NLMs as measured by\nperplexity (Table 2). We see that the perplexity of sampling-\nbased models is comparable to that of the baseline, and the\nresult is consistent under different settings of GPU number.\nMoreover, we plot the learning curve on the validation set\nof the Xinhua task. Figure 4 shows that our sampling-based\nmethod has good convergence. The perplexity drops signif-\nicantly in the ﬁrst 5 epochs and tends to coverage in 8\u001810\nepochs.\nAs the percentage of time spent exchanging parameters de-\npends on the size of minibatch, it is worth a study on how\nour method performs for different sized minibatches. Figure\n5 shows that the speed improvement persists under differen-\nt settings of minibatch size. For smaller minibatches, larger\nspeed improvements are obtained because more time (in per-\ncentage) is spent on communicating gradients.\nAlso, we do sensitivity analysis on the hyper-parameters of\nour method (p, q, and \u0016). We ﬁnd that it helps if we sample\nless data in data transmission for the softmax layer. The sys-\ntem can run faster by using smaller p and q while it in turn\nresults in more training epochs to converge. The interesting\nobservation here is that using two-stage sampling in gradien-\nt computation is promising (see Section 4). Figure 6 shows\nthat setting \u0016around 0.03\u00180.05 leads to better convergence,\nwhich indicates that introducing some noise into training is\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n4197\n20 40 60\n50\n100\n150\ntraining time\n(hours)\nPerplexity\nbaseline (1\nGPUs)\nbaseline (4\nGPUs)\n+ sampling\n(4 GPUs)\nFigure 4: Perplexity against training time (Xinhua).\n4 16 64\n0\n5\n10\n15\n20\nminibatch size\nk w\nords/second\nbaseline (1\nGPU )\nbaseline (4 GPUs)\n+ sampling (4 GPUs)\nFigure 5: Speed against minibatch size (Xinhua).\ngood for generalizing the model on the unseen data.\nIn addition to vanilla SGD, the sampling-based method is\napplicable to other training and data transmission paradigms.\nFor a further improvement we apply the 1-bit method [Seide\net al., 2014] on top of our system. We ﬁnd that the 1-bit\nmethod can speed up our sampling-based system, but with a\ndecrease of language modeling quality in the default setting\nof sampling. We suspect that it is due to the loss of accuracy\nfor the model when we quantize a small portion of gradients.\nFor a comparable perplexity result, we enlarge both pand q\nto 25% when the 1-bit method is employed. The ﬁnal speed\nup is 2.6x over the baseline on 4 GPUs. It is 5.2 times faster\nthan the single GPU baseline.\n6 Related Work\nIt has been observed that increasing the scale of learning with\nrespect to training samples can drastically improve the per-\nformance of NLMs [J´ozefowicz et al., 2016]. In response,\nit is natural to scale up training algorithms through paral-\nlelization. In machine learning, many researchers have inves-\ntigated parallel training methods for statistical models [Mann\net al., 2009; McDonald et al., 2010; Zinkevich et al., 2010;\nDean et al., 2012]. Some of this research focuses on delayed\ngradient updates, which can reduce the communication be-\ntween machines [Langford et al., 2009; Agarwal and Duchi,\n2011]. Other groups work on problems with sparse gradients\nand develop faster asynchronous stochastic gradient descent\nin a lock-less manner [Niu et al., 2011]. These methods work\nwell for convex and/or sparse problems but have not been well\nstudied in the training of large-scale NLMs. We note that, de-\nspite signiﬁcant development effort, we were not able to have\ngood scaling for our NLM using existing methods; it was this\nexperience that led us to develop a model-speciﬁc method in\nthis paper.\nIn the context of language modeling, several research\n5 10 15 2040\n60\n80\n100\n120\ntraining time (hours)\nPerplexity\n\u0016= 0\n\u0016= 0:03\n\u0016= 0:05\n\u0016= 0:10\nFigure 6: Learning curves in different settings of \u0016(Xinhua).\ngroups have addressed the computational challenge in learn-\ning large vocabulary NLMs. They proposed good solution-\ns to the problem, including importance sampling [Bengio\nand Sen´ecal, 2003; 2008], noise contrastive estimation[Mnih\nand Teh, 2012 ], and hierarchical softmax [Morin and Ben-\ngio, 2005; Mnih and Hinton, 2008 ]. Most of these focus on\nthe scaling problem on a single processor or running small\nmodels in a multi-CPU environment. More recently, Zoh-\np et al. [2016] trained their 4-layer LSTM-based NLM by\nlaying each layer on a GPU. Though their model parallelis-\nm method works for multi-layer RNNs, it is only applicable\nto certain architecture where different parts of the model can\nbe processed in parallel. Another interesting study reported\nthat NLMs could be run on a GPU cluster [J´ozefowicz et al.,\n2016], but the details were missing. It is still rare to see in-\ndepth studies on scaling up the training of NLMs over modern\nGPUs through data parallelism.\nAnother related study is [Seide et al., 2014]. They used\nthe 1-bit method to ease data transmission by quantizing gra-\ndients. Actually, the 1-bit method and our sampling-based\nmethod are two research lines that are compatible to each oth-\ner. As is discussed in Section 4, they can work together for\na further speedup. Note that the idea of sampling is not new\nin language modeling. E.g., our method of sampling-based\ngradient computation and importance sampling [Bengio and\nSen´ecal, 2003] are two variants on a theme. Unlike previous\nwork, we develop a different sampling strategy and use two\nsamplers for the forward and backward processes.\n7 Conclusions\nWe have presented a sampling-based approach to scaling up\nthe training of NLMs over multiple devices. It reduces cost\ndata transmission and gradient computation in a distributed\ntraining environment. We have demonstrated that on 4 GPUs\nthe proposed approach can lead to a speedup of 2.1+ times\nover the standard asynchronous SGD, with no loss in lan-\nguage modeling quality. It is even 4.2 times faster than the\nsingle GPU baseline without sampling.\nAcknowledgments\nThis work was supported in part by the National Science\nFoundation of China (61672138 and 61432013) and the Fun-\ndamental Research Funds for the Central Universities. The\nauthors would like to thank anonymous reviewers, Fuxue Li,\nYaqian Han, Ambyer Han and Bojie Hu for their comments.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n4198\nReferences\n[Agarwal and Duchi, 2011] Alekh Agarwal and John C\nDuchi. Distributed delayed stochastic optimization. In\nProceedings of the 25th Annual Conference on Neural\nInformation Processing Systems (NIPS), pages 873–881,\nGranada, Spain, 2011.\n[Bengio and Sen´ecal, 2003] Yoshua Bengio and Jean-\nSbastien Sen ´ecal. Quick training of probabilistic neural\nnets by importance sampling. In Proceedings of AISTATS,\n2003.\n[Bengio and Sen´ecal, 2008] Yoshua Bengio and Jean-\nS´ebastien Sen ´ecal. Adaptive importance sampling to\naccelerate training of a neural probabilistic language mod-\nel. IEEE Transactions on Neural Network, 4:713–722,\n2008.\n[Bengio et al., 2003] Yoshua Bengio, R ´ejean Ducharme,\nPascal Vincent, and Christian Jauvin. A neural proba-\nbilistic language model. Journal of Machine Learning Re-\nsearch, 3:1137–1155, 2003.\n[Dean et al., 2012] Jeffrey Dean, Greg Corrado, Rajat Mon-\nga, Kai Chen, Matthieu Devin, Q Le, Mark Z Mao, Mar-\ncaurelio Ranzato, Andrew W Senior, Paul Tucker, et al.\nLarge scale distributed deep networks. Information pro-\ncessing systems, pages 1223–1231, 2012.\n[Hoaglin et al., 1983] David C. Hoaglin, Frederick\nMosteller, and John W. Tukey. Understanding Ro-\nbust and Exploratory Data Analysis. John Wiley,\n1983.\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and\nJ¨urgen Schmidhuber. Long short-term memory. Neural\nComputation, 9(8):1735–17808, 1997.\n[J´ozefowicz et al., 2016] Rafal J ´ozefowicz, Oriol Vinyals,\nMike Schuster, Noam Shazeer, and Yonghui Wu. Ex-\nploring the limits of language modeling. CoRR, ab-\ns/1602.02410, 2016.\n[Langford et al., 2009] John Langford, Alexander J Smola,\nand Martin Zinkevich. Slow learners are fast. In Proceed-\nings of the 23rd Annual Conference on Neural Information\nProcessing Systems (NIPS), pages 2331–2339, Vancouver,\nCanada, 2009.\n[Mann et al., 2009] Gideon Mann, Ryan T. McDonald,\nMehryar Mohri, and Dan Walker. Efﬁcient large-scale dis-\ntributed training of conditional maximum entropy models.\nIn Proceedings of the 23rd Annual Conference on Neu-\nral Information Processing Systems (NIPS), pages 1231–\n1239, Vancouver, Canada, 2009.\n[McDonald et al., 2010] Ryan McDonald, Keith Hall, and\nGideon Mann. Distributed training strategies for the struc-\ntured perceptron. In Proceedings of Human Language\nTechnologies: The 2010 Annual Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics, pages 456–464, Los Angeles, California, June\n2010.\n[Mikolov et al., 2010] Tom´a\u0014s Mikolov, Martin Karaﬁ ´at,\nLuk´a\u0014s Burget, \u0014Cernock´y Jan, and Sanjeev Khudanpur. Re-\ncurrent neural network based language model. In Pro-\nceedings of INTERSPEECH, pages 1045–1048, Makuhari,\nChiba, Japan, September 2010.\n[Mnih and Hinton, 2008] Andriy Mnih and Geoffrey Hinton.\nA scalable hierarchical distributed language model. In\nProceedings of NIPS, pages 1081–1088, Vancouver, Cana-\nda, December 2008.\n[Mnih and Teh, 2012] Andriy Mnih and Yee Whye Teh. A\nfast and simple algorithm for training neural probabilistic\nlanguage models. In Processings of the 29th Internation-\nal Conference on Machine Learning (ICML), pages 1751–\n1758, Edinburgh, Scotland, June 2012.\n[Morin and Bengio, 2005] Frederic Morin and Yoshua Ben-\ngio. Hierarchical probabilistic neural network language\nmodel. In Proceedings of AISTATS, pages 246–252, 2005.\n[Niu et al., 2011] Feng Niu, Benjaminm Recht, Christopher\nR´e, and Stephen J. Wright. Hogwild: A lock-free approach\nto parallelizing stochastic gradient descent. In Proceed-\nings of the 25th Annual Conference on Neural Information\nProcessing Systems (NIPS), Granada, Spain, 2011.\n[Seide et al., 2014] Frank Seide, Hao Fu, Jasha Droppo,\nGang Li, and Dong Yu. 1-bit stochastic gradient descent\nand its application to data-parallel distributed training of\nspeech dnns. In Proceedings of INTERSPEECH, pages\n1058–1062, Singapore, September 2014.\n[Srivastava et al., 2014] Nitish Srivastava, Geoffrey E Hin-\nton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. Journal of Machine Learning\nResearch, 15(1):1929–1958, 2014.\n[Xiao et al., 2012] Tong Xiao, Jingbo Zhu, Hao Zhang, and\nQiang Li. NiuTrans: An Open Source Toolkit for Phrase-\nbased and Syntax-based Machine Translation. InProceed-\nings of the 50th Annual Meeting of the Association for\nComputational Linguistic (ACL): System Demonstrations,\npages 19–24, Jeju Island, Korea, July 2012.\n[Zhang et al., 2015] Sixin Zhang, Anna Choromanska, and\nYann Lecun. Deep learning with elastic averaging sgd.\nIn Proceedings of the Twenty-ninth Annual Conference\non Neural Information Processing Systems (NIPS), pages\n685–693, Montr´eal, Canada, 2015.\n[Zinkevich et al., 2010] Martin A. Zinkevich, Alex Smola,\nMarkus Weimer, and Lihong Li. Parallelized stochastic\ngradient descent. In Proceedings of the 24th Annual Con-\nference on Neural Information Processing Sytems (NIPS),\npages 2595–2603, Vancouver, Canada, 2010.\n[Zoph et al., 2016] Barret Zoph, Ashish Vaswani, Jonathan\nMay, and Kevin Knight. Simple, fast noise-contrastive es-\ntimation for large rnn vocabularies. In Proceedings of the\n2016 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1217–1222, San Diego, Cali-\nfornia, June 2016.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n4199"
}