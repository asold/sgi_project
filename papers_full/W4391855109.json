{
  "title": "A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges",
  "url": "https://openalex.org/W4391855109",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4364289887",
      "name": "Mohaimenul Azam Khan Raiaan",
      "affiliations": [
        "United International University"
      ]
    },
    {
      "id": "https://openalex.org/A377584420",
      "name": "Md. Saddam Hossain Mukta",
      "affiliations": [
        "Lappeenranta-Lahti University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1949565934",
      "name": "Kaniz Fatema",
      "affiliations": [
        "Charles Darwin University"
      ]
    },
    {
      "id": "https://openalex.org/A4364289886",
      "name": "Nur Mohammad Fahad",
      "affiliations": [
        "United International University"
      ]
    },
    {
      "id": "https://openalex.org/A2466871828",
      "name": "Sadman Sakib",
      "affiliations": [
        "United International University"
      ]
    },
    {
      "id": "https://openalex.org/A5092939745",
      "name": "Most. Marufatul Jannat Mim",
      "affiliations": [
        "United International University"
      ]
    },
    {
      "id": "https://openalex.org/A2904262746",
      "name": "Jubaer Ahmad",
      "affiliations": [
        "United International University"
      ]
    },
    {
      "id": "https://openalex.org/A2160137718",
      "name": "Mohammed Eunus Ali",
      "affiliations": [
        "Bangladesh University of Engineering and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2228867553",
      "name": "Sami Azam",
      "affiliations": [
        "Charles Darwin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2165545766",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W4318069287",
    "https://openalex.org/W4367666119",
    "https://openalex.org/W6744845598",
    "https://openalex.org/W179875071",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3164670515",
    "https://openalex.org/W3158733382",
    "https://openalex.org/W4327545654",
    "https://openalex.org/W6851581138",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W6854422757",
    "https://openalex.org/W3002029438",
    "https://openalex.org/W3201107983",
    "https://openalex.org/W2070398553",
    "https://openalex.org/W187290754",
    "https://openalex.org/W4211202729",
    "https://openalex.org/W4283654020",
    "https://openalex.org/W2005874308",
    "https://openalex.org/W3213080957",
    "https://openalex.org/W6727690538",
    "https://openalex.org/W3187459523",
    "https://openalex.org/W3202773593",
    "https://openalex.org/W3130890978",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2994045732",
    "https://openalex.org/W3194005300",
    "https://openalex.org/W4320495408",
    "https://openalex.org/W4382583857",
    "https://openalex.org/W6767997687",
    "https://openalex.org/W4327519588",
    "https://openalex.org/W3146142859",
    "https://openalex.org/W2037450062",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W6638575559",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W6761628794",
    "https://openalex.org/W2137983211",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6767440493",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W4378619531",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4290737473",
    "https://openalex.org/W4385571791",
    "https://openalex.org/W4283794395",
    "https://openalex.org/W6803508786",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W4225808286",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6811129797",
    "https://openalex.org/W6846002521",
    "https://openalex.org/W6810296985",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W4386273386",
    "https://openalex.org/W6852584927",
    "https://openalex.org/W6810220367",
    "https://openalex.org/W6811340617",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W6794212170",
    "https://openalex.org/W6854475153",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3194782062",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W6787564935",
    "https://openalex.org/W6854506972",
    "https://openalex.org/W3164692279",
    "https://openalex.org/W4389523861",
    "https://openalex.org/W2964264308",
    "https://openalex.org/W3141961557",
    "https://openalex.org/W4387876242",
    "https://openalex.org/W6854135110",
    "https://openalex.org/W4213148310",
    "https://openalex.org/W6851960618",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W4383500999",
    "https://openalex.org/W6849941170",
    "https://openalex.org/W6773820404",
    "https://openalex.org/W6805239564",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W6810874553",
    "https://openalex.org/W6846868997",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W6853700134",
    "https://openalex.org/W4387860528",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W3096485810",
    "https://openalex.org/W3173751215",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4389524559",
    "https://openalex.org/W4385573917",
    "https://openalex.org/W3033777936",
    "https://openalex.org/W4353094257",
    "https://openalex.org/W4313015712",
    "https://openalex.org/W6850068309",
    "https://openalex.org/W6851437002",
    "https://openalex.org/W4385346108",
    "https://openalex.org/W4378783467",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W3164718925",
    "https://openalex.org/W4366989525",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4323050332",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W6847280464",
    "https://openalex.org/W4364378150",
    "https://openalex.org/W6853582642",
    "https://openalex.org/W4366392287",
    "https://openalex.org/W4225411436",
    "https://openalex.org/W6854474899",
    "https://openalex.org/W4366420437",
    "https://openalex.org/W4360620450",
    "https://openalex.org/W6846767490",
    "https://openalex.org/W3093671002",
    "https://openalex.org/W6771889691",
    "https://openalex.org/W3035498813",
    "https://openalex.org/W3090789254",
    "https://openalex.org/W4361984138",
    "https://openalex.org/W6855399105",
    "https://openalex.org/W4361988710",
    "https://openalex.org/W4388858458",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W6769263558",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W6721933647",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6857605197",
    "https://openalex.org/W4389524085",
    "https://openalex.org/W4385576054",
    "https://openalex.org/W6846168271",
    "https://openalex.org/W6853731461",
    "https://openalex.org/W4362498881",
    "https://openalex.org/W4387014587",
    "https://openalex.org/W2966255914",
    "https://openalex.org/W4383913712",
    "https://openalex.org/W4388335799",
    "https://openalex.org/W6855961868",
    "https://openalex.org/W4200112156",
    "https://openalex.org/W4307958490",
    "https://openalex.org/W4321610465",
    "https://openalex.org/W4380267320",
    "https://openalex.org/W6850260469",
    "https://openalex.org/W6846421259",
    "https://openalex.org/W3203279312",
    "https://openalex.org/W4366376874",
    "https://openalex.org/W6853244377",
    "https://openalex.org/W6855780230",
    "https://openalex.org/W6784577980",
    "https://openalex.org/W6846907077",
    "https://openalex.org/W4385852384",
    "https://openalex.org/W4221090794",
    "https://openalex.org/W6855109554",
    "https://openalex.org/W6852789761",
    "https://openalex.org/W6849677550",
    "https://openalex.org/W6857364563",
    "https://openalex.org/W6857089736",
    "https://openalex.org/W4301581299",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4379251582",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4387356327",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4387323731",
    "https://openalex.org/W2323385789",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4383046644",
    "https://openalex.org/W4404518472",
    "https://openalex.org/W4319165821",
    "https://openalex.org/W4383737134",
    "https://openalex.org/W4385681528",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4311408938",
    "https://openalex.org/W2075354211",
    "https://openalex.org/W4384389802",
    "https://openalex.org/W3112467147",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4287327402",
    "https://openalex.org/W4386917649",
    "https://openalex.org/W4400450746",
    "https://openalex.org/W4321854923",
    "https://openalex.org/W4385570090",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2982555889",
    "https://openalex.org/W4362679551",
    "https://openalex.org/W4382173247",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W4394994587",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4383473194",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3099750501",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4390659326",
    "https://openalex.org/W4327810667",
    "https://openalex.org/W4402364221",
    "https://openalex.org/W4311991135",
    "https://openalex.org/W4385890089",
    "https://openalex.org/W4386566677",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4402794779",
    "https://openalex.org/W4393160809",
    "https://openalex.org/W4396870811",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3197782487",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4381571703",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Large Language Models (LLMs) recently demonstrated extraordinary capability in various natural language processing (NLP) tasks including language translation, text generation, question answering, etc. Moreover, LLMs are new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a plethora of research on LLMs have been appeared within a short time, it is quite impossible to track all of these and get an overview of the current state of research in this area. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews LLMs, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different training methods that have been used to train them. The paper also demonstrates the datasets utilized in the studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the paper also explores open issues and challenges to deploy LLMs in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained architectures, applications, challenges, and future goals.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier\nA Review on Large Language Models:\nArchitectures, Applications,\nTaxonomies, Open Issues and\nChallenges\nMOHAIMENUL AZAM KHAN RAIAAN1, MD. SADDAM HOSSAIN MUKTA1, KANIZ FATEMA2,\nNUR MOHAMMAD FAHAD1, SADMAN SAKIB1, MOST. MARUFATUL JANNAT MIM1, JUBAER\nAHMAD1, MOHAMMED EUNUS ALI3, and, SAMI AZAM2\n1Department of Computer Science and Engineering, United International University, Dhaka 1212, Bangladesh\n2Faculty of Science and Technology, Charles Darwin University, Australia\n3Department of CSE, Bangladesh University of Engineering and Technology (BUET), Dhaka-1000, Bangladesh\nCorresponding author: Md. Saddam Hossain Mukta (e-mail: saddam@cse.uiu.ac.bd).\nABSTRACT Large Language Models (LLMs) recently demonstrated extraordinary capability, includ-\ning natural language processing (NLP), language translation, text generation, question answering, etc.\nMoreover, LLMs are a new and essential part of computerized language processing, having the ability to\nunderstand complex verbal patterns and generate coherent and appropriate replies for the situation. Though\nthis success of LLMs has prompted a substantial increase in research contributions, rapid growth has made\nit difficult to understand the overall impact of these improvements. Since a lot of new research on LLMs is\ncoming out quickly, it is getting tough to get an overview of all of them in a short note. Consequently, the\nresearch community would benefit from a short but thorough review of the recent changes in this area. This\narticle thoroughly overviews LLMs, including their history, architectures, transformers, resources, training\nmethods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts\nof LLMs with its traditional pipeline of the LLMs training phase. It then provides an overview of the\nexisting works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs,\nthe different resources of LLMs, and the different training methods that have been used to train them. It\nalso demonstrated the datasets utilized in the studies. After that, the paper discusses the wide range of\napplications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. It\nalso illustrates how LLMs create an impact on society and shape the future of AI and how they can be used\nto solve real-world problems. Then it also explores open issues and challenges to deploying LLMs in real-\nworld scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand\nthe evolution of LLMs, pre-trained architectures, applications, challenges, and future goals.\nINDEX TERMS Large Language Models, Natural Language Processing, Evolution, Transformer, Pre-\ntrained models, Taxonomy, Application\nI. INTRODUCTION\nLanguage is a remarkable tool for human expression and\ncommunication, one that begins to emerge in infancy and\nmakers throughout a lifetime [1], [2]. Nevertheless, machines\nare unable to possess the innate ability to understand and\nspeak in human language without the help of sophisticated\nartificial intelligence (AI) [3]. Therefore, a long-standing\nscientific challenge and aim has been to achieve human-\nlike reading, writing, and communication skills in machines\n[4]. However, advances in deep learning approaches, the\navailability of immense computer resources, and the avail-\nability of vast quantities of training data all contributed to the\nemergence of large language models (LLMs). It is a category\nof language models that utilizes neural networks containing\nbillions of parameters, trained on enormous quantities of\nunlabeled text data using a self-supervised learning approach\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nFIGURE 1. Pipeline of the LLMs training phase\n[5]. It is considered a huge step forward in natural language\nprocessing (NLP) and AI [6]. Frequently pre-trained on large\ncorpora from the web, these models may learn complicated\npatterns, language subtleties, and semantic linkages. Besides,\nthey have proved their ability in various language-related\ntasks, including text synthesis, translation, summarization,\nquestion-answering, and sentiment analysis, by leveraging\ndeep learning techniques and large datasets. Moreover, fine-\ntuning these models on specific downstream tasks has been\nquite promising, with state-of-the-art performance in several\nbenchmarks [7]. LLMs have their roots in the early devel-\nopment of language models and neural networks. Statistical\napproaches and n-gram models were used in earlier attempts\nto develop language models [8], but these models have\nshortcomings in expressing long-term interdependence and\ncontext in language. After that, researchers began to explore\nmore complex ways with the development of neural networks\nand the availability of larger datasets. The creation of the\nRecurrent Neural Network (RNN) [9], which allowed for the\nmodeling of sequential data, including language, was a cru-\ncial milestone. However, RNNs were limited in their efficacy\ndue to vanishing gradients and long-term dependencies. The\nsignificant advancement in LLMs systems occurred when the\ntransformer architecture was introduced in the seminal work\n[10]. The transformer model is built around the self-attention\nmechanism, enabling parallelization and efficient handling\nof long-range dependencies. Furthermore, it served as the\nbasis for models such as Google’s Bidirectional Encoder\nRepresentations from Transformers [11] and open AI’s Gen-\nerative Pre-trained Transformer (GPT) series, which excelled\nat various language tasks.\nThe pipeline of the basic LLMs architecture is shown in\nFigure 1. It receives text data from multiple sources and\nthen forwards it to the subsequent stage for preprocessing.\nIt then completes its training process by executing a series of\nstages, including random parameter initialization, numerical\ndata input, loss function calculation, parameter optimization,\nand iterative training. They offer text translation, text sum-\nmarization, sentiment analysis, and other services following\nthe training phase. Prior research has shown the potential of\nLLMs in many NLP tasks, including specialized applications\nin domains such as the medical and health sciences [12] and\npolitics [13]. Moreover, after inventing the most sophisti-\ncated GPT model [14], developing the state-of-the-art models\n(LLaMa and Bard [15]), and exploring their capabilities,\nsuch as Alpaca and GPTHuggingface [16], it has become a\ncrucial and impactful domain. As a result, a trustworthy as-\nsessment of current LLMs research is becoming increasingly\nimportant, and prior research has shown the potential and\nsuperiority of LLMs in NLP tasks. Despite this, only a few\nstudies have thoroughly reviewed their work’s latest LLMs\ndevelopments, possibilities, and limitations.\nBesides, researchers have presented various aspects of the\nLLMs domain in several studies [3], [17]–[19], but their work\nstill has several limitations. For example, there is a lack of\nintroduction to the core architecture and configurations of\nthe LLMs model, a lack of proper explanation of the taxon-\nomy of LLMs, differentiation based on ML, domain-specific\napplications, API applications, and descriptions of LLMs\ndatasets. Furthermore, the vast majority of LLMs review\npapers are not peer-reviewed works. The absence of these\nkey points in a review indicates that a thorough investigation\nis missing in the current literature. Due to the significant\nextent of the constraints, it is possible to mitigate them by\nthoroughly analyzing and successfully resolving them. Thus,\nthe motivation of this paper is to comprehensively explore the\ncurrent review papers, identify their limitations, and outline\nthe current state-of-the-art methods to address these vital\nchallenges. Therefore, our primary objective is to explore,\ncomprehend, and evaluate LLMs that encompass domains,\nevolution, classification, the structure of pre-trained mod-\nels, resources, and real-time applications. Additionally, our\ncomprehensive review discusses open issues and challenges\nassociated with LLMs, including security, ethical, privacy,\neconomic, and environmental considerations. In addition, we\npresent a set of guidelines to explore future research and\ndevelopment in the effective use of LLMs. We hope that this\nstudy will contribute to a better understanding and use of\nLLMs. The list of contributions to this paper is as follows:\n• Providing a complete overview of LLMs, including their\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nFIGURE 2. Section organization of the review\nevolution, classification, and transformer architecture.\nThe history of LLMs provides a brief account of the\nevaluation from its origins (1940) to the present (2023),\nas well as a taxonomy of LLMs based on pre-trained and\nAPI-based models and major LLMs structures.\n• Describing the comparison of different pre-trained\nmodel designs in LLMs, along with their own systems\nthat show how the model architectures are different.\n• Explaining the influence of ML models on LLMs,\ndemonstrating the significance of ML in various LLMs\ndomains.\n• Providing a brief overview of the data sets used in the\ntraining phase to differentiate between the models in\nexisting works.\n• Presenting a thorough explanation of the hardware im-\nplementation in terms of LLMs.\n• Defining insights into the potential of LLMs and their\nimpact on society and demonstrating bio-medical appli-\ncations in five practical domains, including bio-medical\nand healthcare, education, social media, business, and\nagriculture.\n• Investigating LLMs’s diverse set of open issues, chal-\nlenges, and future opportunities. This section focuses on\nidentifying key challenges and future opportunities that\ncan aid in advancing knowledge in this area.\nThe remaining sections of the paper are organized as\ndepicted in Figure 2. In Section II, the literature review is dis-\ncussed. Section III illustrates the history of LLMs; Section IV\ndemonstrates the Methodology; Section V explains the clear\nconcept of large language models; Section VI describes the\nresources of LLMs; Section VII demonstrates the domain-\nspecific applications of LLMs; and Section VIII explains the\nsocietal impact of LLMs, Indusrial significance of LLMs is\nhighlighted in Section IX, Section X discuss the open issues\nand challenges regarding this study, Section XI discuss about\nthe future prospects of LLMs, Section XII acknowledge the\nlimitation and Section XIII finally concludes the paper.\nII. LITERATURE REVIEW\nThe growing number of LLMs is an extraordinary develop-\nment in AI. In recent years, the prevalence of these models\nhas skyrocketed, and numerous studies have been conducted\nto investigate and evaluate their expanding capabilities. Re-\nsearchers from various fields have conducted exhaustive stud-\nies on the rise of LLMs, shedding light on their remarkable\nadvancements, diverse applications, and potential to revo-\nlutionize tasks from text generation and comprehension to\ndemonstrating reasoning skills. Collectively, these studies\ncontribute to our comprehension of LLMs’ significant role\nin shaping the landscape of AI-driven language processing\nand problem-solving.\nHuang et al., [17] presented a study on reasoning in large\nLanguage models that comprehensively summarizes the cur-\nrent state of LLMs reasoning capabilities. It examines various\naspects of reasoning in LLMs, such as techniques to enhance\nand extract reasoning abilities, methodologies and criteria\nfor assessing these abilities, insights from prior research,\nand suggestions for future directions. The primary concern\nis the extent to which LLMs can demonstrate reasoning\nskills. This paper aims to provide an in-depth and up-to-\ndate examination of this topic, fostering fruitful discussions\nand guiding future research in LLMs-based reasoning. In\nanother study, Zhao et al., [3] survey on LLMs illustrates a\ncomprehensive examination of the evolution and impact of\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nTABLE 1. Comparison between state-of-the-art research\nPapers LLMsLLMsModelLLMsAPI LLMsDataset\nDomainSpecificLLMsTaxonomyLLMsArchitectureLLMsConfigurations\nML BasedComparison(Domain Specific)\nPerformance ofLLMs Parameters andHardware SpecificationScope Key Findings Methodologyand Approach\nHuang et al., (2022)[17] ✓ X X X X X X X X X Reasoning inLLMs\nAims to provide a criticalanalysis of LLMs capabilities,methods for improving andevaluating reasoning,conclusions from earlierresearch, and future directions.\nReview and analysisof reasoning abilitiesin LLMs\nZhao et al., (2023)[3] ✓ X ✓ X ✓ X ✓ X X X Evolution andimpact of LLMs\nExplore the historical journeyof LLMs, including pre-trainedlanguage models (PLMs) ,discussed about LLMs’ uniquecapabilities, insights into LLMsdevelopment resources andhighlights significant contributionsof LLMs to AI and NLP researchareas.\nSurvey and analysisof LLMs evolutionand impact\nFan et al., (2023)[18] ✓ X X X X X X X X X Bibliometricreview of LLMsresearch\nPresent a comprehensive overviewof LLMs research from 2017 to 2023,tracking research trends, advancements,and provides insights into the dynamicnature of LLMs research, and impact invarious domains.\nBibliometric analysisof over 5,000 LLMspublications\nChang et al., (2023)[19] ✓ X ✓ X ✓ X X X X X Assessmentof LLMs\nInvestigate the methodologies employedin evaluating LLMs programs, with aspecific focus on the aspects of what,where and how to conduct evaluationsand identified the potential risks andthe future challenge also.\nSurvey and analysisof LLMs evaluationapproaches\nOURS ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Detailed reviewon LLMs\nOur research investigated the history,resources, architectural configuration,domain-specific analysis, ml-baseddifferentiation, broad level of openissues, challenges, and future scope oflarge language models.\nBroad review andanalysis of LLMsconsidering all thekey aspects\nLLMs in the field of artificial intelligence and natural lan-\nguage processing. It traces the historical journey from early\nlanguage models to the recent emergence of pre-trained lan-\nguage models (PLMs) with billions of parameters. Notably,\nthe paper discusses LLMs’ unique capabilities as they scale\nin size, including in-context learning. The authors highlight\nthe significant contributions of LLMs to the AI community\nand the launch of ChatGPT, a prominent AI chatbot powered\nby LLMs. The survey is structured around four key aspects\nof LLMs: pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Additionally, the paper provides insights\ninto available resources for LLMs development and identifies\nfurther research and development areas.\nA recent study by Fan et al., [18] conducted a bibliometric\nreview of LLMs research from 2017 to 2023, encompass-\ning over 5,000 publications. The study aims to provide re-\nsearchers, practitioners, and policymakers with an overview\nof the evolving landscape of LLMs research. It tracks re-\nsearch trends during the specified time period, including\nadvancements in fundamental algorithms, prominent NLP\ntasks, and applications in disciplines such as medicine, engi-\nneering, the social sciences, and the humanities. In addition\nto highlighting the dynamic and swiftly changing nature of\nLLMs research, the study offers insights into their current\nstatus, impact, and potential in the context of scientific and\ntechnological advancements. Another study by Chang et\nal., [19] focuses on the assessment of LMMs. Their research\nexamines the increasing prevalence of LLMs in academia\nand industry due to their exceptional performance in vari-\nous applications. It highlights the growing significance of\nevaluating LLMs at both the task and societal levels in\norder to comprehend potential risks. The paper thoroughly\nanalyzes LLMs evaluation methods, focusing on three critical\ndimensions: what to evaluate, where to evaluate, and how to\nevaluate. It includes tasks such as natural language process-\ning, reasoning, medical applications, ethics, and education.\nThe article examines evaluation methods and benchmarks for\nassessing LLMs performance, emphasizing successful and\nunsuccessful cases. It underlines future challenges in LLMs\nevaluation and emphasizes the significance of evaluating\nLLMs as a fundamental discipline to support the develop-\nment of more competent LLMs.\nTable 1 illustrates the comparison between different review\npapers based on some critical factors such as LLMs model,\nLLMs API, LLMs dataset, domain specific LLMs, ml-based\ncomparison of LLMs, taxonomy, LLMs architecture, LLMs\nperformance, LLMs harware specifications, LLMs configu-\nrations. Huang et al., [17] lack information on LLMs API,\nLLMs Dataset, Domain-Specific LLMs, Taxonomy, LLMs\nArchitecture, and LLMs Configurations. In contrast, Zhao\net al., [3] lack information on LLMs API, Domain-Specific\nLLMs, Taxonomy, LLMs Architecture, and LLMs Config-\nurations. Moreover, Fan et al., [18] and Chang et al., [19]\nlack information on LLMs API, Domain-Specific LLMs,\nTaxonomy, LLMs Architecture, and LLMs Configurations.\nOn the contrary, our research offers a considerably broader\nperspective on the LLMs context. In addition to incorporat-\ning every parameter specified in the table, we provide an\nelaborate account of the hardware implementation and LLMs\ndatasets. Previous research frequently focuses on particular\naspects of LLMs, including their historical development,\nbibliometric patterns, or assessment approaches. However,\nour investigation surpasses these limitations. A thorough\nexamination is conducted on each of these aspects, result-\ning in a comprehensive comprehension of the strengths and\nweaknesses of LLMs. Furthermore, our research is focused\non the crucial element of reasoning capabilities in LLMs,\nthereby providing a significant and groundbreaking addition\nto the body of knowledge in the field. By giving thorough\ninformation, such as descriptions of datasets and hardware\nimplementations, our paper stands out as an innovative re-\nsource for LLMs practitioners and researchers. Furthermore,\nwe briefly discuss open issues in LLMs research, such as eth-\nical and responsible AI, multimodal integration, energy effi-\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nciency, privacy and data protection, generalization and few-\nshot learning, and cross-lingual and low-resource settings.\nWe also highlight key challenges, including data complexity\nand scale, tokenization sensitivity, computational resource\ndemands, fine-tuning complexity, real-time responsiveness,\ncontextual constraints, bias and undesirable output, knowl-\nedge temporality, and evaluation complexity. Our review sug-\ngests future research directions to tackle these issues, making\nour review a pioneering resource for LLMs researchers and\npractitioners. Our extensive systematic review, which encom-\npasses a broad spectrum of subjects, makes a substantial\ncontribution to the field of LLMs research. Our study has\nsuccessfully resolved the constraints associated with current\ncutting-edge research in this domain.\nIII. HISTORY OF LARGE LANGUAGE MODELS\nLLMs refer to a category of AI models developed specif-\nically to comprehend and produce human language [20].\nLLMs have significantly transformed the field of AI and\nhave been implemented in diverse areas, including educa-\ntion, communication, content generation, article composi-\ntion, healthcare, research, entertainment, and information\ndissemination, among others [20], [21]. The origins of LLMs\ncan be attributed to the emergence and advancement of neural\nnetwork-based methodologies in the field of natural language\nprocessing (NLP) [21]. In order to process language, early\nNLP systems utilized rule-based techniques and statistical\nmodels. However, they frequently encountered difficulties\nin comprehending the textual context and producing cohe-\nsive and contextually pertinent discourse [22]. This section\nprovides a high-level overview of LLMs, including their\nbackground, development, training, and functioning. Figure 3\ndepicts the history of language models.\nIn the 1940s, Warren McCulloch and Walter Pitts intro-\nduced the world to the idea of artificial neural networks\n(ANNs) [23]. Following this, the 1950s and 1960s saw the\ndevelopment of the first language models [24]. These models\nincluded early neural networks as well as rule-based models.\nThe processing of language was facilitated by their utilization\nof precisely established linguistic rules and features [25].\nThese models experienced limitations in their abilities and\nencountered difficulties in managing the complexities of\ncomplicated language assignments. The models were pre-\ndominantly employed for tasks involving binary classifica-\ntion. However, their efficacy in dealing with the intricacies\ninherent in natural language was limited [25].\nStatistics-based models of language were created in the\n’80s and ’90s. These models belong to a category of models\nutilized in the field of NLP and machine learning (ML)\nwith the purpose of capturing and quantifying the statis-\ntical patterns and correlations within language data [22].\nThe models employed probabilistic techniques to assess the\nprobability of a sequence of words or phrases inside a spe-\ncific context. Statistical language models have significance\nin several applications, such as predictive text input, text\ngeneration, speech recognition, and spam detection, etc. They\nwere superior in terms of accuracy to early neural networks\nand rule-based models, as they were able to process large\namounts of data with ease [22]. Although statistical language\nmodels have been successful in many applications of NLP,\nthey still have limits when it comes to comprehending the\nsemantic relationships and context of language, and they have\ndifficulty dealing with long-range dependencies [26].\nDuring the mid-2000s, the field of NLP witnessed the\nintroduction of word embeddings, which were recognized\nas a notable breakthrough and subsequently acquired con-\nsiderable attention [27]. It refers to the process of repre-\nsenting words in a continuous vector space. The approach\ncaptures the semantic relationships among words by repre-\nsenting them in a vector space. This representation reduces\nthe computational cost by mapping the words to a lower-\ndimensional space. Word2Vec and GloVe are widely recog-\nnized word embedding models in the domain [28]. These\nmodels are mostly utilized for assessing word similarity and\nassisting in the clustering and representation of words within\nsemantic domains. Although not classified as LLMs, these\nembeddings have significantly contributed to the progress of\nnatural language comprehension and have set the path for the\ndevelopment of more complex models. Nevertheless, these\nmodels possess several limitations, such as their difficulty\nin effectively dealing with words that have many meanings\nor words that sound the same, as well as their inability to\ncomprehend contextual information [27].\nThe introduction of neural language models in the mid-\n2010s marked a significant advancement in large language\nmodeling [29]. These models employed deep learning ap-\nproaches to acquire knowledge of language patterns from\nextensive textual data and additionally utilized artificial neu-\nral networks to comprehend, produce, or forecast human\nlanguage. Furthermore, they have demonstrated exceptional\noutcomes in a wide range of language-related tasks. The ini-\ntial neural language model to be introduced was the recurrent\nneural network language model (RNNLM) in 2010 [30]. The\npurpose of its development was to capture the sequential\ndependencies present in textual data. The utilization of a\nhidden state allows for the retention and propagation of\ninformation from preceding words in a particular sequence.\nRNNLM has been employed in several applications such as\ntext production, speech recognition, machine translation, and\nlanguage modeling. The RNNLM demonstrated the capabil-\nity to effectively capture the contextual information of words,\nresulting in the generation of text that exhibits a higher\ndegree of naturalness compared to earlier models. Although\nthe RNNLM offers certain advantages, it is not without its\ndrawbacks. Some of these limitations include a limited short-\nterm memory capacity, extended training time requirements,\nand vulnerability to overfitting [31].\nIn the year 2015, Google unveiled the initial large neural\nlanguage model that employed deep learning methodologies.\nThe technology was referred to as the Google Neural Ma-\nchine Translation (GNMT) model [32]. The model under-\nwent training using huge quantities of multilingual textual\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nFIGURE 3. Brief history of language models.\ndata. This development signifies a notable progression in the\nfield of machine translation [33]. The model demonstrated\nexceptional performance on machine translation tasks, de-\nparting from traditional rule-based and statistical techniques\nin favor of neural network-based methodologies. When com-\npared to earlier language models, it was able to tackle\ncomplex natural language tasks with ease. The utilization of\nthis model resulted in enhanced translation accuracy and the\ngeneration of meaningful translations, while also mitigating\nerrors associated with intricate linguistic constructions [32].\nThe advancement of Language models persisted with the\nemergence of the Transformer model in the year 2017 [34].\nThe transformer model has had a significant impact on the\nfield of NLP and has played a crucial role in the development\nof language models such as Bidirectional Encoder Repre-\nsentations from Transformers (BERT) and Generative Pre-\ntrained Transformers (GPT) [35]. These models employ a\nself-attention mechanism that enables them to assess the rel-\native significance of individual words in a sentence, thereby\nencoding complex relationships within the text [35]. The\nprimary objective behind the development of the Transformer\nmodel was to overcome the inherent constraints observed in\nearlier models such as RNNs and Long Short-Term Memory\n(LSTM) networks. The Transformer models possess notable\nadvantages in comparison to other models due to their ability\nto capture longer-term dependencies in language and fa-\ncilitate concurrent training on many Graphical Processing\nUnits (GPUs) with a vast number of parameters, enabling\nthe construction of much larger models [36]. Parallelization\ncapabilities and scalability are further benefits that have\nresulted in notable progress across many NLP activities [34].\nThe introduction of BERT in 2018 by Google AI repre-\nsents a noteworthy advancement in the domain of NLP [18].\nThe underlying framework utilized in this study was the\ntransformer architecture. Before the introduction of BERT,\nthe preceding language model rooted in NLP had constraints\nin understanding contextual information due to its reliance on\nunidirectional language modeling. BERT was introduced by\nGoogle as a solution to address this particular constraint [37].\nThe employed methodology involved the utilization of deep\nbidirectional representations, which were conditioned on\nboth the left and right contexts across all layers [38]. The\npre-trained BERT model was able to undergo fine-tuning\nby incorporating an additional output layer, hence enabling\nits applicability to diverse tasks such as question answering\nand language inference. Due to the widespread adoption\nof BERT, several versions and subsequent models, such as\nRoBERTa, T5, and DistilBERT, have been developed to ef-\nfectively address diverse tasks across multiple domains [38].\nFollowing the advent of transformers, subsequent years\nsaw the development of scaling-up LLMs models through the\nexpansion of training data and parameter counts [21]. Ope-\nnAI significantly contributed to the development of LLMs in\n2018. During the same year, GPT, an additional transformer-\nbased architecture, was developed. Multiple iterations of\nthe GPT models, developed by OpenAI, underwent pre-\ntraining using extensive datasets comprising excerpts from\nthe Internet, novels, and various other textual sources [39].\nThe first version of the GPT model was referred to as GPT-\n1 [40]. The introduction of GPT-1 was a notable progression\nin the field of NLP. GPT-1 effectively produces words that\nare contextually appropriate, showcasing the transformative\ncapabilities of transformers in significantly advancing natu-\nral language processing tasks. This proficiency is attributed\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nto its extensive training on a vast number of parameters,\nspecifically 117 million. The model underwent a two-step\nprocedure consisting of unsupervised pre-training followed\nby supervised fine-tuning [21]. The initial iteration of GPT\ndid not attain the same level of popularity as BERT due to\nseveral inherent limitations [41]. These drawbacks include a\nrestricted context window, absence of bi-directionality, and\noccasional generation of biased content. Despite the inherent\nlimits of GPT-1, this model played a crucial role in paving\nthe way for later, more advanced models. As a result, it has\nsparked a new era of AI research and intensified competition\nin the development of LLMs.\nThe subsequent version of the GPT series, known as\nGPT-2, was designed with the purpose of addressing the\nlimitations observed in its predecessor, GPT-1 [41]. Similar\nto GPT-1, GPT-2 was developed utilizing the Transformer\narchitecture. In the year 2019, Alec Radford introduced GPT-\n2, a language model that was developed on a deep neural\nnetwork consisting of 1.5 billion parameters [42]. The GPT-\n2 model includes a transformer design, which incorporates\nself-attention processes to extract information from differ-\nent positions within the input sequence. Despite the high\ncomputing cost associated with training and executing the\nmodel, its substantial magnitude facilitates the comprehen-\nsion and generation of a wide range of linguistic subtleties\nand diversified outputs [41]. The GPT-2 model has played\na pivotal function in the advancement of LLMs and the\nexecution of NLP activities. The influence of GPT-2 has had a\nsignificant impact on successor models like GPT-3 and GPT-\n4, leading to additional advancements in the field of language\nprocessing and creation [43].\nIn 2019, NVIDIA produced Megatron-LM, which is an\nLLMs [44]. Similar to GPT, this model is built on the\ntransformer architecture. The model possesses a total of 8.3\nbillion parameters, a notably bigger quantity compared to\nthe parameter count of GPT-1 and GPT-2 [18]. The mag-\nnitude of this dimension facilitates the model’s capacity to\nacquire and produce intricate linguistic structures. Never-\ntheless, Megatron-LM has certain limitations, primarily due\nto its substantial dimensions, which necessitate substantial\ncomputational resources for both the training and inference\nprocesses [44].\nIn the year 2020, OpenAI introduced GPT-3 as the suc-\ncessor to GPT-2 [41]. GPT-3 was trained on an extensive\ncollection of textual data and demonstrated the ability to\ngenerate text that exhibited a high degree of coherence and\nnaturalness. Similar to GPT-1 and GPT-2, this model also\nutilizes the Transformer architecture [21]. The potential of\nLLMs for various NLP applications was exemplified by GPT-\n3. This particular LLMs was trained on a deep neural network\nwith an enormous 175 billion parameters, surpassing the size\nof any other LLMs available at that particular time [18].\nThe ability to produce natural language text of superior\nquality with less fine-tuning is facilitated by sophisticated\nmethodologies, including a more significant number of layers\nand a wider range of training data. One of the most essential\ncharacteristics of GPT-3 is its capacity to engage in few-\nshot and zero-shot learning, hence mitigating the necessity\nfor extensive data in order to generate natural language text\nof superior quality. The advent of GPT-3 has catapulted the\ndomain of natural language processing to new heights. [41]\nIn the year 2020, OpenAI introduced GPT-4, the subse-\nquent version of their language model, following the achieve-\nments of GPT-3 [21]. Similar to its predecessor, GPT-4 is\na transformer-based model. The system has the capability\nto analyze both textual and visual data to produce textual\noutputs [18]. The performance of the system was assessed\nusing a range of standardized professional and academic\nexaminations specifically intended for human test-takers.\nGPT-4 exhibited a level of performance comparable to that\nof humans on the majority of examinations. Significantly, it\nachieved a ranking inside the highest decile of participants on\na simulated iteration of the Uniform Bar Examination [45].\nGPT-4 has greater dimension and efficacy compared to its\npredecessor, GPT-3, as it possesses the capacity to generate\ntext that is even more comprehensive and exhibits a height-\nened level of naturalness [21].\nThe development of large language models presents addi-\ntional prospects for innovation, knowledge acquisition, and\nexperimentation across diverse domains such as healthcare,\neducation, research, etc. The utilization of AI and NLP in\nthese models has significantly transformed how we engage\nwith machine devices.\nIV. METHODOLOGY\nPreferred Reporting Items for Systematic Reviews and Meta-\nAnalyses (PRISMA) guide is crucial for drafting review pa-\npers as it assists systematic reviews in conducting transparent\nmeta-analyses, accurately reporting aims and conclusions of\nthe study, and ensuring the adequate reliability and rele-\nvance with the findings of the study [46]. Therefore, this\nreview work focuses on the adoption of PRISMA technique\nin analyzing the design, configurations, applications, and\nchallenges of LLMs.\nInitial Searching: The research materials employed in\nthis study have been acquired from recognized scientific\njournals and conferences from January 2020 to August 2023,\nconducted through the Google Scholar platform. A com-\nprehensive selection of scholarly research articles has been\nspecified, encompassing various reputable academic sources\nsuch as IEEE Xplore, ScienceDirect, ACM Digital Library,\nWiley Online Library, Springer Link, MDPI, and patents.\nInitially, 355 papers were selected based on their relevance\nto the topic and keyword. Table 2 describes the identification\ntechnique of the materials from various electronic sources.\nSearching Query and Keywords: Using the combination\nof the appropriate search queries and keywords enlisted in Ta-\nble 3 helps to perform a proper literature search. To conduct\na thorough search of the articles for our LLMs-based review\nwork, we encompass the following terms: \"LLMs AND\nmachine learning OR deep learning OR models,\" \"LLMs\nAND machine learning OR deep learning OR API,\" \"LLMs\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nFIGURE 4. PRISMA flow diagram of the review.\nAND machine learning OR deep learning OR Dataset,\" and\n\"LLMs\" AND machine learning OR deep learning OR tools.\"\nThese specific searching techniques help to extract the eligi-\nble and quality research papers.\nInclusion and Exclusion criteria set: To acquire the\nfinal research papers, PRISMA protocols and principles were\nadhered to formulate a standard set of Inclusion Criteria (IC)\nand Exclusion Criteria (EC). The inclusion criteria define the\nstandards of the paper that need to be included, while the\nexclusion criteria eliminate articles that do not meet the in-\nclusion scope. Thus, this manual screening process improves\nthe transparency of selection process. Table 4 presents the\ninclusion and exclusion criteria set for the proposed study.\nPRISMA Diagram: Figure 4 depicts the PRISMA flow\ndiagram utilized in selecting papers for the study. It also\nprovides the numbers of included and excluded papers for\nbetter understanding. The diagram begins by identifying\narticles from electronic databases using keywords, queries,\nresulting in 355 papers. After applying the screening method\nto exclude duplicated, low-quality, and irrelevant journal\npapers, the total number of papers for review is reduced to\n294. Following a thorough analysis of the titles and abstracts,\na total of 207 papers were selected. The final screening\nmethod involves the application of inclusion and exclusion\ncriteria. Following this process, a total of 135 papers were\nultimately selected for the final review. The process begins\nwith an extensive collection of papers and reduces to the final\nselection that meets the pre-defined selection criteria for the\nsystematic review.\nV. LARGE LANGUAGE MODELS\nLarge language models (LLMs) refer to a specific type of AI\nalgorithm that holds the capability to execute a diverse range\nof NLP operations. The most common tasks entail text gen-\nTABLE 2. Electronic database search\nElectronic\nDatabase\nType URL\nIEEE Xplore Digital Library https://ieeexplore.ieee.org/\nXplore/home.jsp (accessed on 18\nSeptember, 2023)\nSpringer Digital Library https://www.springer.com/gp (ac-\ncessed on 18 September, 2023)\nGoogle\nScholar\nSearch Engine https://scholar.google.com.au (ac-\ncessed on 18 September, 2023)\nScience\nDirect—\nElsevier\nDigital Library https://www.sciencedirect.com\n(accessed on 18 September, 2023)\nMDPI Digital Library https://www.mdpi.com (accessed\non 18 September, 2023)\nACM Digital Library https://www.researchgate.net (ac-\ncessed on 18 September, 2023)\nTABLE 3. Search queries used for the review paper.\nSearch Queries (SQ)\nSQ1 “LLMs” AND machine learning OR deep learning OR\nmodels\nSQ2 “LLMs” AND machine learning OR deep learning OR\nAPI\nSQ3 “LLMs” AND machine learning OR deep learning OR\nDataset\nSQ4 “LLMs” AND machine learning OR deep learning OR\ntools\nTABLE 4. Inclusion and exclusion criteria.\nList of Inclusion and Exclusion Criteria\nInclusion\nCriteria(IC)\nIC1 Should contain at least one of the keywords\nIC2 Must be included in one of the selected databases\nIC3 Published within the last three years (2020–2023)\nIC4 Publication in a journal, conference is required\nIC5 The research being examined should have a matching\ntitle, abstract, and full text\nExclusion\nCrite-\nria(EC)\nEC1 Redundant items\nEC2 Whole text of paper cannot be taken\nEC3 Purpose of the paper is not related to LLMs\nEC4 Non-english documents\neration, text analysis, translation, sentiment analysis, ques-\ntion answering, and other related functions. GPT-3, GPT-\n4, PaLM, and LaMDA are extensively used transformer-\nbased LLMs models trained on a large amount of textual\ndata. In terms of architectural properties, these models show\nvariations in size and depth. For example, GPT-3 generates\nparameters of 175 billion, distributed across 96 levels, while\nPaLM has an even larger parameter number of 540 billion,\norganized across 106 layers. All of these models have distinct\nconfigurations. The configurations of GPT-3 and PaLM differ\nin terms of their techniques for generating output. LLMs\nhave evaluated several datasets within Wikipedia, code repos-\nitories, books, question sets, and social media data. They\nhave demonstrated their ability to execute diverse activities\nsuccessfully. Consequently, LLMs have drawn significant\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nattention for their effective contribution in different domains,\nincluding education, healthcare, media marketing, and other\ncustomer services. A particular LLMs program has superior\nperformance in a specific domain compared to others, such\nas GPT-3, which has gained recognition for its proficiency in\ngenerating text styles, whereas LaMDA demonstrates supe-\nrior performance in providing accurate responses to factual\ninquiries. LLMs are an emerging technological innovation\nthat holds the potential to bring about transformative changes\nacross various sectors.\nA. BACKGROUND OF LARGE LANGUAGE MODELS\nIn this section, the necessary background to comprehend the\nessential aspects associated with LLMs are presented. Large\nLanguage Model research requires a comprehensive expla-\nnation of the crucial concept. Various vital aspects, such as\ntokenization, encoding technique, layer normalization, etc.,\nare encompassed in the following background section.\nTokenization: The primary emphasis is on tokenization,\na crucial preprocessing stage of LLMs that involves parsing\ntext into discrete parts referred to as tokens [47]. Characters,\nsubwords, symbols, or words may serve as tokens, contingent\nupon the language model’s dimensions and nature [48], [49].\nVarious tokenization algorithms are utilized in LLMs, such\nas WordPiece, UnigramLM, and Byte Pair Encoding (BPE).\nThis algorithm has distinct technique for tokenizing from the\ninput and then, applied for the specific tasks [48]–[50].\nAttention Mechanism: The attention mechanisms used in\nLLMs is a crucial topic hence it contributes in the improvi-\nsation of the architecture and performance. This mechanism\nhelps to figure out the representation of input sequences by\nforming links between various tokens. There are several at-\ntention mechanism available namely Self-Attention where all\nthe queries and values come from the same encoder-decoder\nblock. Then, Full Attention which is the naive understanding\nversion of self attention, and finally, when the output of\nencoder block is used as the query of immediate decoder\nblock, is called as cross attention mechanism. [10], [51].\nActivation Function: The activation functions play a\nvital role in the curve-fitting capacities of LLMs archi-\ntectures [52]. Several activation functions, such as ReLU,\nGeLU, and other GLU variations, are explored to determine\ntheir significance in current research on LLMs [53], [54].\nNormalization Layer: Layer normalization is essential\nfor achieving faster convergence in LLMs model and em-\nphasizes their effects on stability during training sessions.\nIt presents different approaches, such as LayerNorm, Deep-\nNorm, and RMSNorm. These layer normalization tech-\nniques offer distinct advantages and contribute to the regu-\nlarization of LLMs applications like GPT-3, BERT, T5, etc.,\nfacilitating effective training. [55].\nTraining Methods and Frameworks: LLMs training has\ndifferent distributed methodologies, including Data Paral-\nlelism, Pipeline Parallelism, Tensor Parallelism, Model Par-\nallelism, and Optimizer Parallelism [44], [56]. These tech-\nniques contribute to understanding the practical and expand-\nable training. Additionally, different libraries and frame-\nworks, including Transformers, DeepSpeed, PyTorch, Ten-\nsorFlow, MXNet, and MindSpore, are used frequently for\ntheir training and further implementation [56].\nData Preprocessing: The approaches used to preprocess\ndata focus on the significance of quality filtering, data dedu-\nplication and privacy reduction in preparing training data for\nLLMs. The filtering technique helps to reduce low quality\nand relevant data. Besides, it reduces the compute complexity\nby ignoring the useless pattern of the input. Duplicate sam-\nples are removed using deduplication technnique and it also\navoid the overfitting tendency of the model. Finally, privacy\nreduction ensures the security and compliance of data and\nupholds the preservance of the personal data.\nParameter Tuning: The researchers explore the many\nstages of adaptation for LLMs, starting from pre-training and\nprogressing to fine-tuning for subsequent tasks. These ap-\nproaches serve as a guide for customizing models to suit spe-\ncific applications. Several model adaptation and parameter-\nefficient tuning techniques, such as Prefix Tuning, Prompt\nTuning, and Adapter Tuning, provide strategies for achieving\neffective fine-tuning while minimizing resource usage [57]–\n[59].\nThis background part aims to provide a thorough under-\nstanding of the underlying concepts and approaches that\nform the basis of Language Models, which are constantly\ndeveloping.\nThe transformer is widely employed in most advanced\nLLMs as the basic structure because its architecture, scala-\nbility, and pretraining approach, which render it the optimal\nframework for constructing robust large language models.\nIn addition, the self-attention mechanism of transformers is\nsignificantly effective for capturing and representing long-\nrange relationships in language. Consequently, Transformer-\nbased LLMs have significantly improved the state-of-the-art\nachievement in NLP related tasks. In the section V-A1,a com-\nprehensive overview of transformer architectures, configura-\ntions are provided for building a high-scalable, optimized and\ncost-efficient LLMs. Figure 5 depicts the visualization of the\nLLMs background.\n1) What is Transformer?\nTransformer architecture is considered as the basic building\nblock of LLMs. It is intended for neural networks to effi-\nciently handle sequential data [10]. This architecture does not\nuse iteration methods. Instead, it employs an attentional ap-\nproach to determine global input-output dependencies. This\nresults in increased size and performance levels of the novel\nmodel, resulting in substantial parallelization and reduced\ntraining time in NLP. Furthermore, it can take input of vary-\ning lengths and can change its focus depending on the length\nof the sequence. As a result, it has become the go-to architec-\nture in many fields, often replacing sophisticated recurrent\nor convolutional neural networks with much more efficient\nstructure [60]. In this regard, it is particularly important for\nLLMs applications. Figure 6 illustrates the architecture of\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nFIGURE 5. Background of LLMs\nthe transformer model. Transformer architecture consists of\nseven main components. A demonstration of each component\nis shown below.\nInputs and Input Embeddings\nThe ML models utilize tokens, which are units of text\nlike words or sub words, as training data. However, these\nmodels process numbers. Tokenization begins this transla-\ntion process by dividing down input text into meaningful\ncomponents. A unique number identification is assigned\nto each token, connecting the linguistic information to the\nnumerical realm. This numerical format is known as \"input\nembeddings.\" These input embeddings are numerical repre-\nsentations of words, which ML models may subsequently\nprocess. These embeddings function similarly to a dictionary,\nassisting the model in understanding the meaning of words by\narranging them in a mathematical space where comparable\nphrases are situated close together. The model is trained to\ngenerate these embeddings so that vectors of the same size\nrepresent words with similar meanings. Figure 6A illustrates\nthe input and input embeddings.\nPositional Encoding\nThe sequence of words in a sentence frequently conveys\nimportant semantic information. The same set of words in\na different order can convey completely different meanings.\nIn this regard, understanding the word order in a sentence is\nessential in NLP to identify the correct utterance meaning. In\ngeneral, in terms of neural networks, they do not fundamen-\ntally perceive the order of inputs. To remedy the problem,\npositional encoding can be used to encode the position of\neach word in the input sequence as a collection of integers.\nThe transformer model uses integer and input embedding\nand positional encoding to help GPT understand sentence\nword order and provide grammatically accurate and semanti-\ncally appropriate output [61]. The positional encoding part is\nshown in Figure 6B.\nEncoder\nThe encoder is a crucial component of the neural network\nresponsible for processing the input text. Its primary function\nis to generate a series of hidden states that represent the input\ntext in a meaningful way [62]. Then, it uses a series of self-\nattention layers that are often referred to metaphorically as\n\"voodoo magic,\" emphasizing their complex and powerful\nability to capture relationships between different elements\nin the input. In the transformer, the encoder is used in\nmore than one layer. This section is depicted in Figure 6C\ncomprehensively.\nOutputs (shifted right)\nDuring the training process, the decoder in the transformer\nmodel learns to predict the next word in a sequence by\nanalyzing the preceding words. This is achieved through a\nmechanism known as autoregressive training. The decoder’s\nability to predict the next word is critical for generating\ncoherent and contextually relevant sequences. Additionally,\nthe GPT (GPT-3) is also trained on a massive amount of text\ndata, that helps it generate sense while writing something.\nBesides, several corpus including the Common Crawl web\ncorpus, the BooksCorpus dataset, and the English Wikipedia\nare also used during the common issue. Figure 6D highlights\nthe transformer’s outputs (shifted right) module.\nOutput Embeddings\nInput embeddings, which contain text and are not recog-\nnized by the model. Therefore, the output must be converted\nto a format known as \"output embedding.\" Similar to input\nembeddings, output embeddings undergo positional encod-\ning, enabling the model to understand the order of words\nin a sentence [63]. In machine learning, the loss function\nevaluates the difference between a model’s prediction and\nthe objective value. Loss functions are essential for complex\nGPT language models. The loss function modifies a portion\nof the model to increase accuracy by reducing the discrep-\nancy between predictions and targets. The change improves\nthe overall performance of the model. The loss function is\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nFIGURE 6. Architecture of a Transformer model\ncalculated during training, and the model parameters are\nmodified. In the inference process, the output text is created\nby mapping the predicted probability of each token in the\nmodel to the corresponding token in the vocabulary. The\noutput embedding part is illustrated in Figure 6E.\nDecoder\nThe decoder processes both positionally encoded input and\noutput embeddings. Positional encoding is crucial for the\nmodel to understand the sequential order of the tokens in both\nthe input and output sequences. The positional information\nhelps the decoder effectively capture the structure within\nthe sequences. The decoder has an attention mechanism that\nhelps to improve the output’s quality by leveraging contextual\ninformation received from the encoder. The primary function\nof the decoder is to create output sequences based on the\nencoded input sequences. It generates a sequence of tokens,\noften representing words or sub-words, as its output. The\ndependency between the encoder-decoder in a transformer is\nsignificant where the encoder processes the input sequence\nand based on this representation, the decoder provides the\ndesired output sequence. In addition, GPT is a decoder-\nonly transformer [64]. The decoder part of GPT uses a\nmasked self-attention mechanism which can process the in-\nput sequence without requiring encoder explicitly. Figure 6F\ndemonstrates the decoder component of a transformer.\nLinear Layer and Softmax\nThe linear layer is a fully connected neural network\nlayer that transforms the output embedding into a higher-\ndimensional space. This step is required to convert the output\nembedding into the original input space. This transformation\nenhances the expressiveness of the representation, allowing\nthe model to capture more complex patterns and relation-\nships in the data. Besides, the softmax function generates\na probability distribution for each output token in the de-\nveloped vocabulary, allowing us to generate probabilistic\noutput tokens [65]. Figure 6G shows the process by which\nthe features are propagated through a linear layer, followed\nby the activation of the accurate output probability using the\nsoftmax activation function.\nB. HARDWARE SPECIFICATIONS FOR LARGE\nLANGUAGE MODELS\nUnderstanding the computing resources and training dura-\ntions needed for various language models is crucial. It allows\nfor informed decision-making when choosing a model for\nspecific tasks. To choose a model that is appropriate for a\ngiven task, a clear understanding of the training times and\ncomputational resources is a must. Table 5 shows the hard-\nware specifications, parameters number, training duration\nand other configurations of individual LLMs model.\nGPT-3: GPT-3 uses Nvidia A100 GPUs to pre-train on a\nlarge 300 billion token set, generating around 175 billion\nparameters [66].It is not stated about the specific training du-\nration. Besides, it has context learning features, enables itself\nto understand the words reasoning, sentence, and language\nproperly.\nBert: Trained on an unspecified data scale, the Bert model\nhas a variable parameter count that depends on batch size and\nthe corresponding model’s hidden layer numbers and usually\nit is around 340 million. Nvidia A100 and V100 GPUs are\nused for training, and the length of the training depends on\nthe scale of the model’s parameters [67]. Contextual learning\nis incorporated in the model also.\nRoBERTa: RoBERTa, an improvised version of BERT has\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nTABLE 5. Hardware specifications for the LLMs model.\nModel’s Name Parameters Pre trained\nData Scale\nHardware\nSpecifications\nTraining\nDuration\nContext\nLearning\nGPT-3 [66] 175 Billion (B) 300B tokens Nvidia A100 GPU - Yes\nBERT [67] 340 Million (M) - Nvidia A100, V100 Depends on model\nparameter scale. Yes\nRoBERTa [68] 340 M - 6144 TPU v4 Nearly 2 weeks. Yes\nT5 [69] 11 B 1 Trillion (T) tokens 1024 TPU v3 - Yes\nPaLM [70] 540 B 780 B tokens 6144 TPU v4 120 Days Yes\nLaMDA [71] 137 B 768 B tokens 1024 TPU v3 57.7 Days Yes\nGLM-130B [72] 130 B 400 B tokens 1024 TPU v4 60 Days Yes\nGopher [73] 280B 300 B tokens 4096 TPU v3 920 Hours Yes\nJurassic-1 [74] 178 B 300 B tokens 800 GPU - Yes\nMT-NLG [75] 530 B 270 B tokens 4480 80G A100 - Yes\nLLaMA [76] 65 B 1.4 T tokens 2048 80G A100 21 Days Yes\nLLaMA 2 [77] 70 B 2 T tokens 2000 80G A100 25 Days Yes\nFalcon [78] 40 B 1.3 T tokens - - Yes\nChinchilla [79] 70 B 1.4 T tokens - - Yes\nOPT [80] 175 B 180 B tokens 992 80G A100 - Yes\nGalactica [81] 120 B 106 B tokens - - Yes\nBLOOM [56] 176 B 366 B tokens 384 80G A100 105 Days Yes\nPanGU-a [82] 207 B 1.1 TB 2048 Ascend 910 - Yes\na parameter count of 3ro million and conducts pre-training\non a specific amount of data. The training process completed\non 6144 TPU v4 units, running for around two weeks [68].\nThe model also employs context learning feature.\nT5: T5 uses 1024 TPU v3 units and has 11 billion pa-\nrameters. It has been pre-trained on 1 trillion tokens [69].\nThere is no information available on GPU training time. It\nalso enables the features of contextual learning and provides\nan accurate result.\nPaLM: PaLM produces a substantial number of parame-\nters, around 540 billion, and it goes pre-training on a large\ndataset of 780 billion tokens. This pre-training process is\ncarried out utilizing 6144 TPU v4 units [70]. The training\nperiod extends for 120 days, and the model also incorporates\ncontextual learning.\nLaMDA: LaMDA uses 1024 TPU v3 units during training\nand the model is pre-trained on 768 billion tokens which\ngenerates 137 billion parameters [71]. It has a long 57.7 days\nof training time.\nGLM-130B: GLM-130B model possesses an immense 130\nbillion parameters and has undergone pre-training on a huge\ndataset of 400 billion tokens. This training was conducted\nutilizing 1024 TPU v4 units and the training session lasts for\n60 days [72].\nGopher: Gopher is a language model that has been pre-\ntrained on 300 billion tokens and requires 4096 TPU v3 for\nthe experiment. It has a total of 280 billion parameters [73].\nThe GPU training period is precisely stated as 920 hours.\nFurthermore, the model integrates context learning to demon-\nstrate an effective outcome.\nJurassic-1 is a model with an impressive capacity of 178\nbillion parameters. It has been pre-trained on a massive\ndataset of 300 billion tokens, utilizing the computational\npower of 800 GPUs [74]. No information regarding the\nduration of GPU training is available.\nMT-NLG: MT-NLG has an impressive size of 530 billion\nparameters. It has been trained on a massive dataset of 270\nbillion tokens, utilizing 4480 80GB A100 GPUs [75]. No\ndata regarding the duration of GPU training is available. The\nmodel integrates context learning features.\nLLaMA: LLaMA isa language model with an enormous\ncapacity of 65 billion parameters. It has undergone pre-\ntraining on a massive dataset consisting of 1.4 trillion tokens.\nThis training process was carried out utilizing 2048 high-\nperformance 80GB A100 GPUs [76]. The training period is\nexplicitly set to 21 days.\nLLaMA 2: LLaMA 2 is equipped with 70 billion param-\neters and has performed pre-training on 2 trillion tokens,\nutilizing 2000 80GB A100 GPUs [77]. The training period is\nset to 25 days, and the model contains context-based learning.\nFalcon: Falcon, equipped with 40 billion parameters, un-\ndergoes pre-training on a large dataset of 1.3 trillion to-\nkens [78]. No details regarding the duration of GPU training\nand it also have the context learning features.\nChinchilla: Chinchilla is a language model that has 70\nbillion parameters and has been pre-trained on 1.4 trillion\ntokens [79]. There is no details regarding the duration of GPU\ntraining.\nOPT: OPT, equipped with 175 billion parameters, conducts\npre-training on 180 billion tokens utilizing 992 A100 GPUs\nwith a capacity of 80GB each [80]. No details regarding the\nduration of GPU training.\nGalactica: Galactica possesses 120 billion parameters and\nhas undergone pre-training using 106 billion tokens [81].\nDetails regarding the duration of GPU training are not given.\nBLOOM: BLOOM has a remarkable capacity of 176\nbillion parameters and has undergone pre-training on 366\nbillion tokens utilizing 384 80GB A100 GPUs [56]. The\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\ntraining period lasts for 105 days, and the model incorporates\ncontextual learning.\nPanGU-a: PanGU-a is a language model that has been pre-\ntrained on a massive amount of data, specifically 1.1 billion,\nemploying 2048 Ascend 910 processing units [82]. It has\nan impressive parameter count of 207 billion. No details\nregarding the duration of GPU training.\nThis comprehensive analysis helps to determine the hard-\nware specifications and the computational complexity of each\nmodel and thus researchers navigate the implementation of\nthis model precisely and improve the performance of their\nresearch outcomes.\nC. DEEP NEURAL NETWORK ARCHITECTURES OF\nLLMS\nA deep neural network is utilized in LLMs to understand and\ngenerate new content more accurately and efficiently. In this\nsection, we include a summary of various DNN architectures\nof different large language models with respect to various\nliterature studies as well as various real-time applications of\nlarge language models using various DNN models.\n1) Comparison Between State-of-The-Art studies\nA large language model is a dynamic model capable of\nperforming various tasks, such as creating coherent text and\nsummarizing text. This is possible because of its pre-training\non extensive text data. A defining feature of a language model\nis its ability to anticipate the subsequent word by analyzing\nthe preceding text. The deep neural network (DNN) frame-\nwork that is utilized in LLMs enhances this process to be\nmore akin to human-like understanding [3], [83]. LLMs use\ndifferent DNN models in their architecture to enhance task\nperformance.\nThe transformer architecture serves as the core component\nof all language models. GPT-1, the initial version of GPT\nemploys the Transformer decoder architecture [67]. In GPT-\n1 the decoder structure operates independently from the\nencoder, therefore eliminating the Multi-Head Attention and\nLayer Norm components that are linked to the encoder. The\npre-trained GPT model consists of 12 transformer blocks,\neach with a d(model) value of 768 and a total of 110 million\nparameters. GPT-2, the second version of GPT, employs\nthe transformer decoder architecture like GPT-1 [67]. GPT-\n2 employs 50,257 BPE tokens and ensures that the Masked\nMulti-Head component is preceded by the Layer Norm. In\nGPT-2, an additional Layer Norm is included subsequent\nto the last block. There are four pre-trained GPT-2 models\navailable, each with a unique quantity of decoder blocks. The\nlargest model, which has a d(model) value of 1600 and 48\nblocks, comprises a total of 1.5 billion model parameters.\nBERT employs the transformer encoder structure, in contrast\nto the Transformer decoder structure utilized by GPT-1 and\nGPT-2 [84]. Following the final encoder block is composed\nof two fully connected output layers separated by a Layer\nNorm component. The calculation of the likelihood of each\ntoken’s output depends on both the previous and next tokens,\nmaking BERT a bidirectional language model. The smaller\nvariant of BERT consists of 12 encoder blocks with a model\ndimension of 768 and a parameter count that is approximately\nequal to that of GPT. In contrast, the larger variant has 24\nencoder blocks with a model dimension of 1024 and 336\nmillion parameters [67].\nIn contrast to encoder-only models such as BERT and\ndecoder-only models like GPT-1 and GPT-2, T5 pre-trains\nwith generative span corruption and an encoder-decoder ar-\nchitecture [85]. T5 models have displayed state-of-the-art\nperformance on a wide variety of NLP tasks, like GLUE and\nSuperGLUE, and are able to expand up to hundreds of bil-\nlions of parameters. LLaMA normalizes the input for every\ntransformer sub-layer rather than the output [76]. To increase\nperformance, it employs the RMSNorm normalizing function\nand the SwiGLU activation function rather than the ReLU.\nSingle models are utilized by LaMDA to execute multiple\nduties. The model architecture is a decoder-only Transformer\nlanguage model. The Transformer is comprised of 64 layers,\na d(model) value of 8192, gated-GELU as the activation\nfunction, and relative attention the same as T5 LLMs [71].\nAlphaCode employs an encoder-decoder transformer archi-\ntecture in which input tokens are passed to the encoder,\nand one token is extracted from the decoder until an end-\nof-code token is generated [86]. When contrasting encoder-\ndecoder architectures with decoder-only architectures, the\nencoder-decoder architecture provides the advantage of en-\nabling bidirectional description representation and provides\nadditional flexibility by separating the encoder structure from\nthe decoder. It employs an asymmetric architecture with 1536\nencoder tokens but only 768 decoder tokens. It makes use of\nmulti-query attention to lower sampling costs. Cache update\ncosts and memory utilization are greatly reduced when all\nquery heads are used but only shared for key and value\nheads in each attention block. It employed a SentencePiece\ntokenizer for tokenization, trained on a combination of Code-\nContests and GitHub data, with a vocabulary size of 8,000\ntokens. Through the usage of DNNs, all of these LLMs have\ndemonstrated remarkable performance on various NLP tasks\nlike as language understanding and generation.\n2) Applications of LLMs using various DNN models\nPre-training Transformer models have led to the proposal\nof LLMs with impressive capacities in addressing a vari-\nety of NLP tasks, including question-answering, document\nsummarization, and language translation [3]. Due to their\nremarkable abilities in basic tasks of language processing\nand creation, they have completely transformed the fields\nof NLP and AI. Various DNN models have been employed\nin different industries, such as technology, healthcare, and\nretail to increase performance. DNNs have made substantial\nprogress in improving the capabilities of LLMs [88]. DNN\nmodels, such as convolutional neural networks (CNNs), re-\ncurrent neural networks (RNNs), generative adversarial net-\nworks (GANs), capsule networks (CapsNets), transformers,\nand BERT, have been extensively employed in diverse appli-\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nTABLE 6. Comparison of applications of LLMs using various DNN models\nStudy DNN model Application\nKoizumi et al., [87] Transformer (decoder) Assessd the use of a pre-trained large-scale language model\nin audio captioning.\nFan et al., [88] Transformer (encoder-decoder) Discuss the significance of Recommender Systems in web\napplications and e-commerce cite\nBai et al., [89] Non-Autoregressive attention\nbased encoder-decoder\nPropose a non-autoregressive speech recognition model\nnamed LASO (Listen Attentively and Spell Once)\nSun et al., [90] Decoder + SOCIALSENSE\n(belief-centered graph)\nForecast the impact of news releases and attempt to mitigate\npotential adverse consequences by automatically anticipating\nnews media responses\nDrossos et al., [91] RNN\nPropose a method for sound event detection which takes a\nsequence of audio frames as input and predicts the activities\nof sound events in each frame.\nChiu et al., [92] Transformer (encoder) Propose a method called TPBERT to improve the reranking\nof N-best hypotheses in automatic recognition of speech.\nElhafsi et al., [93] Encoder structure Propose a monitoring system for dealing with semantic\nabnormalities in robotic systems.\nShen et al., [94] Transformer (decoder) Propose a self-regulating edge AI system to autonomously\nplan, and adjust itself to fulfill the needs of users.\ncations of LLMs [95]. Numerous studies [87]–[94] suggest\nthat DNN models are utilized in several types of LLMs-based\napplications to increase task efficiency.\nKoizumi et al., [87] introduce an innovative method to\naddress the issue of insufficient training data in audio cap-\ntioning that utilizes a pre-trained extensive LLMs that uses\na deep neural network, as a decoder for generating cap-\ntions. The findings of the study demonstrate the effectiveness\nof the proposed methodology in utilizing LLMs for audio\ncaptioning. Significantly, the performance of this proposed\napproach outperforms the traditional approaches which are\ntrained from scratch.\nIn a recent study, Fan et al., [88] discuss the significance\nof Recommender Systems in web applications and the short-\ncomings of current DNN approaches in comprehending user\ndesires and integrating textual side information efficiently.\nThey discuss the capacity of LLMs to tackle the difficulties\nand also highlight the necessity for a systematic evaluation of\nrecommender systems.\nLASO (Listen Attentively and Spell Once) is an end-to-\nend non-autoregressive speech recognition model that was\ndeveloped in a recent study by Bai et al., [89] to improve the\nspeed of inference by simultaneously predicting all tokens.\nThe proposed model utilizes attention methods to combine\ndecoded speech information into hidden representations for\nevery token. Moreover, they suggest using cross-modal trans-\nfer learning to increase the performance of the speech-modal\nLASO model by utilizing a text-modal language model to\nalign the semantic meaning of tokens.\nIn another study, Sun et al., [90] provide a new method-\nology to predict the effect of news releases and try to\nminimize potential negative consequences by automatically\nforecasting responses in news media. By utilizing an LLMs\nwhich utilizes a deep neural network, their method creates\na belief-centered graph on an existing social network that\nutilizes graph-based propagation to analyze social dynamics.\nThe proposed framework depicts its efficiency in predicting\nresponses.\nDrossos et al., [91] present a technique in their study that\nenables a recurrent neural network (RNN) to acquire LLMs\nfor sound event detection. The proposed approach adjusts\nthe input of the RNN based on the activity of classes in the\npreceding time step. This proposed approach is evaluated on\nthree distinct datasets: the TUT-SED Synthetic 2016, TUT\nSound Events 2016, and TUT Sound Events 2017 datasets.\nIn a separate study, Chiu et al., [92] present an efficient\nmethod called TPBERT (based on BERT) for improving the\nreranking of N-best hypotheses in automatic recognition of\nspeech. This approach uses task-specific topic information\nto increase the BERT model’s ability to create accurate\nembeddings of the N-best hypotheses.\nIn another study, Elhafsi et al., [93] propose a moni-\ntoring methodology that utilizes large language models to\ntackle the issue of semantic irregularities in robotic systems.\nThe efficiency of LLMs-based monitoring in recognizing\nsemantic abnormalities and aligning with human thinking is\ndemonstrated through tests on autonomous driving.\nShen et al., [94] present a self-regulating edge artificial\nintelligence system that utilizes a deep neural network to\nautonomously plan, and adjust itself to fulfill the needs of\nusers. The proposed system uses a hierarchical design known\nas cloud-edge-client, where the primary language model is\nlocated in the cloud. By leveraging the robust capabilities\nof GPT in language comprehension, and code creation, in\nthis study, they introduce a methodology that effectively\nhandles edge AI models to meet users’ requirements while\nautomatically generating new codes for training new models\nthrough edge federated learning.\nTable 6 gives a brief overview of these DNN applications-\noriented studies of LLMs. These studies suggest that employ-\ning deep neural networks in language models increases the\nperformance of LLMs-based applications in several indus-\ntries. .\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nD. ARCHITECTURAL OVERVIEW OF LARGE\nLANGUAGE MODELS\nIn Table 7, a description and architecture of LLMs such as\nGPT-1, BERT, RoBERta, and T5 are presented. This table\nwill assist researchers in selecting the optimal model for a\nnatural language processing task. GPT-1, BERT base, and\nBERT large contain 12, 12, and 24 layers, correspondingly, in\nthe larger language model. RoBERta is an enhanced variant\nof BERT, while T5 is a decoder and encoder transformer.\nDiagram illustrating BERT’s input token processing, context-\naware embedding, and masked language modeling tasks,\nwhere the masked words are intended to predict the model.\nT5 demonstrates the sequential layers of the transformer\nmodel, including the feedforward neural network, and self-\nattention. It explains how information flows and structures\ntext. GPT-1 passes data input embedding and positional\nencoding through multiple transformer layers.\nE. COMPARISON BETWEEN CONFIGURATIONS OF\nLLMS\nTable 8 provides an extensive overview of various Large\nLanguage Models (LLMs), highlighting their configuration\ndetails and optimization settings. These LLMs have played\na crucial role in advancing natural language comprehension\nand generation tasks, making them a focal point in artificial\nintelligence and natural language processing. This analysis\ncompares and contrasts these LLMs based on critical param-\neters, including model size, learning rate, category, activa-\ntion function, batch size, bias, number of layers, optimizer,\nnumber of attention heads, hidden state size, dropout rate,\nand maximum training context length. GPT-4 stands out as\nthe most prominent model on display, with a staggering 1.8\ntrillion parameters. It is comparatively faster than the prior\nGPT versions and provide many advanced features. Besides,\nit has fast prompt response, generate more accurate output\nand it has reduced the biases presented in the model sub-\nstantially. GPT-1, despite being lesser with 125 million pa-\nrameters, demonstrates the significant development of LLMs\nover the years. An increased number of parameters in LLMs\nenhances the model’s ability to comprehend intricate patterns\nand produce text that is more contextually appropriate and\nreminiscent of human language. GPT3’s selection of a mod-\nest learning rate of 6 is notable, which highlights the sig-\nnificance of cautious hyperparameter selection. Models are\ncategorized as Causal decoder (CD), Autoregressive (AR),\nEncoder-decoder (ED), and Prefix decoder (PD) to illustrate\narchitectural diversity. Activation functions vary, influencing\nthe models’ expressive strength from GeLU in GPT-3 to\nSwiGLU in LLaMA and LLaMA-2. All versions of GPT\nemploy the GeLU as its activation function as it mitigates\nthe vanishing gradient problem and facilitates the generation\nof smoother gradients throughout the training process. The\nutilization of SwiGLU as the activation function is observed\nin models such as PaLM and LLaMA versions 1 and 2, as it\nhas gating mechanisms that enhance its ability to capture in-\ntricate correlations within the data. Models like BERT, OPT,\nand T5 use ReLU as the activation function. The Formula of\nthese activation functions are given below [7], [60]:\nReLU(x) = max(0, x) =f(x) =\n(\nx, if x ≥ 0\n0, if x <0 (1)\nGeLU(x) = 0.5x(tanh[\np\n2/π(x + 0.44715x3)]) (2)\nSwiGLU(x) =x.Sigmoid(βx).xV (3)\nBARD is recognized for its informative response. It fea-\ntures 24 attention heads and facilitates its contextually re-\nlated response. BERT size is identical to BARD of 340M.\nThe key advantage of BERT is understanding the context\nof words. It has effective training settings with a proper\nlearning rate, batch size, and a dropout value of 0.1, lever-\nages the convergence of the model, and contributes to the\nNLP-based tasks precisely. PanGU BLOOM, Galactica, and\nChinchilla are also LLMs but possess distinct configurations\nand challenges. Usually, PanGU is highly effective for the\nChinese language, whereas Galactica performs well with\nrepeated data. Chinchilla is a scaling strategy constrained by\ndata limitations and creates efficient resource allocation for\ntraining and generating output. Falcon and T5 are compact\ncompared to other LLMs, and both are transformer-based\nmodels. However, they have some unique differences, such\nas Falcon is a decoder-based model whereas T5 integrated\nboth encoder-decoders. Additionally, Falcon utilizes multi-\nhead query attention to increase the scalability of the model.\nLLaMA-2 is the updated version of LLaMA. It is an en-\nhanced fine-tuned version that exploits the hardware utiliza-\ntion for efficient training sessions. MT-NLG and PaLM have\nsubstantial parameter sizes of 530B and 540B, respectively.\nBoth of them also use the casual decoder technique. How-\never, they have some architectural differences, such as PaLM\nuses a SwiGLU activation function and adafactor optimizer.\nMoreover, it uses a higher learning rate and batch size of 1\n× 102 and 1000K. On the contrary, MT-NLG uses a lower\nlearning rate and batch size of 5 × 105 and 64K, respectively.\nGLM-130B and LaMDA are also effective LLMs, widely\nused for NLP-based tasks, including question answering, text\ngeneration, etc. Both of them use the Gated GLU (GeGLU)\nactivation function, a GLU variant. The following equation is\nused to express the GeGLU operation [100].\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c) (4)\nHowever, there are noticeable differences between GLM-\n130B and LaMDA in terms of their decoder mechanisms.\nGLM-130B employs a prefix decoder, whereas LaMDA\nadopts a casual decoder technique. In addition, the GLM-\n130B model employs a larger batch size compared to the\nLaMDA model. In addition, the presence or absence of\nbiased terms in models, such as Falcon, T5, LLaMA 1,2, and\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nTABLE 7. Architectural overview of different LLMs\nModel Description Architecture\nGPT-1 [67]\nTwelve-level decoder\ntransformer that uses\ntwelve masked\nself-focusing heads.\nBERT [11]\nBERT is a transformer\narchitecture. It has two model\nsizes. BERT base has 12 layers\nin encoder stack and BERT\nLarge has 24 layers in encoder\nstack.\nRoBERTa [68] Optimized version of\nBERT model.\nT5 [85]\nThe model consists of an encoder\nand a decoder transformer, which\nhas many layers.\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nTABLE 8. Various LLMs with configuration details and optimization settings (Here, LR = learning rate, CG = Category, AF = the activation function, bs = batch size,\nNL = the number of layers, NAH = the number of attention heads, SHS = the size of the hidden states, MCLDT = the maximum context length during training, CD =\ncausal decoder, ED = encoder-decoder, PD = prefix decoder, and AR = autoregressive)\nModel Size LR CG AF BS Bias NL Optimizer NAH SHS Dropout MCLDT\nGPT-4 [96] 1.8 T − CD GeLU − Yes 120 Adam 120-150 20000 − 32768\nGPT-3 [66] 175B 6 × 10−5 CD GeLU 32K-3200K Yes 96 Adam 96 12288 − 2048\nGPT-2 [97] 1.5B 1 × 10−4 AR GeLU 16K-64K Yes 48 Adam 24 1280 0.1 1024\nGPT-1 [67] 125M 1 × 10−4 AR GeLU 16K-64K Yes 12 Adam 12 768 0.1 512\nBARD [98] 340M − − ReLU 64K Yes 24 − 24 768 − 512\nBERT [67] 340M 1 × 10−5 − ReLU 16K-64K Yes 24 Adam 16 1024 0.1 512\nPanGU-α [82] 207B 2 × 10−5 CD GeLU − Yes 64 Adam 128 16384 − 1024\nBLOOM [56] 176B 6 × 10−5 CD GeLU 4000K Yes 70 Adam 112 14336 0 2048\nGalactica [99] 120B 7 × 10−6 CD GeLU 2000K No 96 AdamW 80 10240 0.1 2048\nOPT [80] 175B 1.2 × 10−4 CD ReLU 2000K Yes 96 AdamW 96 12288 0.1 2048\nChinchilla [79] 70B 1 × 10−4 CD − 1500K-3000K − 80 AdamW 64 8192 − −\nFalcon [78] 40B 1.85 × 10−4 CD GeLU 2000K No 60 AdamW 64 8192 − 2048\nT5 [69] 11B 1 × 10−2 ED ReLU 64K No 24 AdaFactor 128 1024 0.1 512\nLLaMA [76] 65B 1.5 × 10−4 CD SwiGLU 4000K No 80 AdamW 64 8192 − 2048\nLLaMA-2 [77] 70B 1.5 × 10−4 CD SwiGLU 4000K No 80 AdamW 64 8192 − 4096\nMT-NLG [75] 530B 5 × 10−5 CD − 64K-3750K − 105 Adam 128 20480 − 2048\nJurassic-1 [74] 178B 6 × 10−5 CD GeLU 32K-3200K Yes 76 − 96 13824 − 2048\nGopher [73] 280B 4 × 10−5 CD − 3000K-6000K − 80 Adam 128 16384 − 2048\nGLM-130B [72] 130B 8 × 10−5 PD GeGLU 400k-8250K Yes 70 AdamW 96 12288 0.1 2048\nLaMDA [71] 137B − CD GeGLU 256K − 64 − 128 8192 − −\nPaLM [70] 540B 1 × 10−2 CD SwiGLU 1000K-4000K No 118 Adafactor 48 18432 0.1 2048\nGalactica’s \"No,\" highlights the complexity of the choices\nmade. From 12 for GPT-1 to 118 for PaLM, the number\nof layers affects a model’s ability to capture intricate pat-\nterns. Optimizers are also diverse, with Adam, AdamW, and\nAdaFactor playing crucial roles. All GPT variants employ\nAdam as the optimizer, although models such as Galactica,\nOPT, and Falcon utilize AdamW as their optimizer. Both\nT5 and PaLM models utilize the Adafactor optimizer in\ntheir respective architectures. These variations highlight the\nsignificance of selecting models and configurations that are\ntailored to particular tasks, with performance, computational\nresources, and task requirements playing a central role.\nThe number of attention heads also exhibits variation\nacross different models. GPT-1 is equipped with a total\nof 12 attention heads, whilst GPT-4 boasts a much larger\nnumber of attention heads, ranging from 120 to 150 within its\nmodel. The additional number of attention heads in the LLMs\nenables the model to concurrently attend to several segments\nof the input sequence, hence expediting the model’s training\nprocess. In order to enhance the efficacy of the LLMs,\nresearchers employ diverse dimensions for the hidden states\nwithin their model. The larger dimensions of the hidden\nstate enable the capturing of complex patterns within the\ntext. Both GPT 4 and MT-NLG employ hidden state sizes\nof approximately 20,000, which is significantly greater in\ncomparison to the hidden state sizes of other LLMs included\nin the table. Certain LLMs models incorporate a dropout\nvalue of 0.1 to prevent overfitting issues, whereas others\ndo not employ any dropout value. The maximum context\nlength denotes the number of tokens that can be remembered\nby the model during training. Increasing the size of the\ncontext window boosts the model’s ability to grasp the distant\nrelationships between the texts. Consequently, the model is\nable to generate text outputs with a great coherence. Table 8\nreports that GPT-4 has the context length of 32768 which is\nthe maximum among all the LLMs. This substantial length\nnumber indicates the capability of GPT-4 to remember the\nmore extended token sequence during training. LLaMA-2\nobtained the second-highest context length of 4096. Most of\nthe models have a context length of 2048, meaning they can\nhandle a maximum of 2048 tokens simultaneously during\nthe text generation. A few compacted models, including\nBARD, BERT, and T5, possess a maximum context length\nof 512. This table presents a qualitative architectural com-\nparison among the most popular LLMs. It also provides\ncomprehensive knowledge about the configurations, strength\nof these models. These variations highlight the significance\nof selecting models for the particular tasks considering the\nperformance, computational resources.\nF. COMPARISON BETWEEN DATASETS OF LLMS\nDifferent LLMs utilized different datasets for the training\nphase, distinguishing the models from one another. A concise\noverview of the datasets is provided in this section. Moreover,\nit explicitly exhibits the diverse range of datasets used by the\nmodel since understanding of these datasets facilitates the\ndevelopment and training of the model and boost the per-\nformance. The datasets used to train various large language\nmodels (LLMs) and their compatibility with each model are\ndetailed in Table 9.\nTable 9 demonstrates that datasets have been divided into\nmultiple categories: webpages, conversation data, literature\nand news, scientific data, and code. This classification en-\nables us to comprehend the variety of data sources that con-\ntribute to LLMs training. C4, OpenWebText, and Wikipedia\nare examples of datasets that belong to the \"Webpages\"\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nTABLE 9. Dataset for large language models\nDataset→ Webpages ConversationData Books and News Scientific Data Code\nLLMs↓ C4 OpenWebTextWikipedia the Pile -StackExchangeBookCorpusGutenbergCC-Stories-RCC-NEWESREALNEWsthe Pile -ArXiv\nthe Pile -PubMedAbstracts BigQuery the Pile -GitHub\nT5 [69] ✓ ✓ ✓ X X X X X X X X X XFalcon [78] ✓ ✓ ✓ X X X X X X X X X XLLaMA [76] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓GPT-3 [66] ✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X X XGPT-4 [96] ✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X X XMT-NLG [75] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓Gopher [73] ✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X ✓ ✓Chinchilla [79] ✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X ✓ ✓GLaM [101] ✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X X XPaLM [70] ✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X ✓ ✓LaMDA [71] ✓ ✓ ✓ X X X X X X ✓ ✓ ✓ ✓Galactica [99] ✓ ✓ ✓ X X X X X X ✓ ✓ ✓ ✓GPT-NeoX [102]✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓CodeGen [103] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓AlphaCode [86] X X X X X X X X X X X ✓ ✓GPT-1 [86] X ✓ ✓ X ✓ X X X X X X ✓ ✓GPT-2 [86] X ✓ ✓ X ✓ X X X X X X X XBARD [86] ✓ ✓ ✓ X ✓ X X X X X X X XBERT [86] ✓ ✓ ✓ X ✓ X X X X X X X XPanGU- [86] ✓ ✓ ✓ X ✓ X X X X X X X XBLOOM [86] ✓ ✓ ✓ X X X X X X X X X XOPT [86] X ✓ ✓ X ✓ ✓ X X X X X X XGLM-130 [86] X ✓ ✓ X X X X ✓ ✓ X X ✓ ✓Size 800GB 38GB 21GB 800GB 5GB - 31GB 78GB 120GB 800GB 800GB - 800GB\nSource CommonCrawl(April 2019)RedditLinks(March 2023)Wikipedia(March 2023)Other(Dec 2020)Books(Dec 2015)Books(Dec 2021)CommonCrawl(Sep 2019) CommonCrawl(Feb 2019) CommonCrawl(April 2019)Other(Dec 2020)Other(Dec 2020)Codes(March 2023)Other(Dec 2020)\ncategory. At the same time, BookCorpus, Gutenberg, CC-\nStories-R, CC-NEWES, and REALNEWS are examples of\ndatasets that belong to the \"Books and News\" category. These\ncategories reflect the richness and diversity of text data used\nto train LLMs, including web content, novels, news articles,\nscientific literature, and code.\nFrom the ✓, it can be seen that LLaMA has been trained\non a wide range of data sources, with significant expo-\nsure to webpages (87%), conversation data (5%), books\nand news (2%), scientific data (3%), and code (5%). This\nmakes LLaMA a versatile model suitable for a wide array\nof natural language processing tasks that involve these data\ntypes. In contrast, platforms such as GPT-3 and AlphaCode\nhave restricted data exposure. GPT-1 and GPT-2 focus on\nwebpages and (70%) and books and news (30%) data to\ntrain the model. GPT-3 is proficient with web pages (84%),\nliterature, and news (16%) but requires additional instruction\nwith conversation data, scientific data, and code. This diverse\ndataset range enables the GPT models to generate more\ncontextual information across various domains. Specifically,\nthe Webpages, books, and news datasets help to comprehend\nformal and structured language. Consequently, GPT models\nachieve the capability of responding in a more informative\nand accurate way. AlphaCode, as its name suggests, is solely\nfocused on code (100%) and does not utilize any other data\nsources. This feature uniquely distinguish AlphaCode from\nother models and emphasize the significance of this model\nfor code-based tasks. Bard, Bert, and Pangu- A model ex-\nhibits identical traits, with each of them concentrating on the\nextensive textual data obtained from webpage contents and\nbooks for pretraining the models. Bloom and OPT primarily\nemphasize on evaluating data from books and websites, such\nas Wikipedia or other online sources. On the other hand,\nGLM-130 not only analyzes books and web data but also\nincorporates computer code data to provide further tech-\nnological benefits. LaMDA, Galactica and CodeGen model\nuse scientific data source for training which advances these\nmodels to adapt the scientific knowledge and terminology.\nHence, these model can lead to a more accurate responses\nin scientific domains. These findings highlight the signifi-\ncance of this table, which contributed to distinguishing the\nmodels based on their data source for training and task\naccomplishments. AlphaCode, GLM-130 is the model of\nchoice for code-related tasks, whereas LLaMA, Bert excels\nin diverse text data applications. Most of the LLMs such\nas T5, GPT models, Gopher, GLam, PaLM, BLOOM vastly\nutilize websource data which helps them to automate various\npractical tasks such as content creation, data analysis and\nvirtual chatbot for answering the question. On the contray,\nsome models such as Falcon, OPT different version of GPT\nmodels utilizes books and news data facilitates in education\nmaterial based application such as document summarization,\narticle writings. The models trained on scientific data have\nseveral use cases in scientific domain. In addition, Table 9\nprovides contextual information of the datasets to maintain\nthe transparency of the comparison and an effective guide\nto future model implementation. The \"Size\" and \"Source\"\ncolumns of the Table listed the additional information. The\nsize of datasets ranges from 5GB (BookCorpus) to a massive\n800GB (several datasets), indicating the sheer magnitude of\ndata required to train these LLMs. The source information\nreveals when and where the data were collected, which\nis essential for comprehending the training data’s temporal\nrelevance and potential biases. Table 9 provides a multitude\nof information regarding the datasets used to train LLMs and\nhow each model leverages these datasets. This information\nis invaluable for natural language processing researchers,\ndevelopers, and practitioners, as it enables them to make\ninformed decisions about which LLMs to use for specific\ntasks and casts light on the breadth and depth of data that\npowers these cutting-edge language models.\n18 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nG. PERFORMANCE ANALYSIS OF LLMS\nLarge language models are models that perform the major-\nity of natural language processing and are trained on vast\nquantities of data. Over time, numerous large languages in-\ncluding GPT-1 through GPT-4, Bing, ChatpGPT, and BERT\nhave developed in order to contribute jointly to industry and\nacademia. As a result of the scarcity of adequate data pertain-\ning to large language models, we have solely presented per-\nformance outcomes for diverse tasks of publicly accessible\nLLMs in Table 10. All GPT series, including GPT-1, GPT-2,\nGPT-3, GPT-3.5, and GPT-4, are evaluated using a variety of\nmetrics, including the Stanford Question Answering Dataset\n(SQuAD), Language Model Benchmark (LAMBADA), and\nGeneral Language Understanding Evaluation (GLUE), as\nshown in Table 10. GPT-1 obtains a score of 68.4 on the\nGLUE, while GPT-2, GPT-3, GPT-3.5, and GPT-4 attain\nscores of 84.6, 93.2, 93.5, and 94.4 respectively. GLUE re-\nsults indicate that GPT-4 outperforms prior versions of GPT.\nThe GPT-4 scores in SQuAD and LAMBDA are 93.6 and\n82.4, respectively. As shown in the table, GPT-4 outperforms\nits predecessors in both LAMBDA and SQuAD. As GPT-4\noutperforms its predecessors in all three benchmark metrics\nand exhibits robust performance, it can be concluded that\nGPT-4 is significantly more effective than its predecessors in\ntasks involving language comprehension and language mod-\neling. The VietNamese High School Graduation Examination\n(VNHSGE) English dataset was utilized to analyze various\nLLMs, including GPT-3.5, BingChat, and BARD. Based on\nthe accuracy presented in Table 10, it is evident that BingChat\nLLMs outperforms the other two models, achieving an accu-\nracy of 92.4% in this dataset. LLMs such as ChatGPT and\nBing were evaluated using the average intraclass correlation\ncoefficient (ICC) values. The ICC value for Bing was 0.975,\nwhereas for ChatGPT it was 0.858. The higher mean ICC\nvalue indicates that Bing exhibited robust performance and\nconsistency in comparison to ChatGPT. Table 10 depicts that,\nall of the LLMs mentioned in the table have been analyzed\nand tested on multiple performance metrics and datasets\nto validate the robustness and reliability of these language\nmodels.\nVI. RESOURCES OF LARGE LANGUAGE MODELS\nLarge Language Models (LLMs) have a wide range of po-\ntential applications and resources available for their develop-\nment, deployment, and utilization. In Figure 7, we present\nan LLMs taxonomy that categorizes Large Language Models\ninto two main branches: those based on pre-trained models\nand those based on APIs. This taxonomy allows for a com-\nprehensive exploration of these two distinct aspects of Large\nLanguage Models. Here are some key resources presented\nwhich are associated with LLMs:\nA. PRETRAINED MODELS\nPretrained language models play a pivotal role in natural\nlanguage processing due to their ability to encapsulate broad\nlanguage understanding and generation skills gleaned from\ndiverse text sources. They offer a substantial advantage by\nminimizing the computational resources and data required\nfor fine-tuning specific tasks. There are some of the most\ncommon pre-trained LLMs models, which have been de-\npicted in Table 11.\n1) Generative Pretrained Transformer (GPT)\nGenerative Pre-trained Transformer [66] is an influential\nbreakthrough in artificial intelligence, particularly in natural\nlanguage processing (NLP). Developed by OpenAI, GPT\nleverages the Transformer architecture and extensive pre-\ntraining on vast internet text data to achieve a deep under-\nstanding of human language. This generative model excels\nat tasks like text generation, translation, question answering,\nand more, making it a versatile tool across various NLP do-\nmains. GPT’s capacity to capture intricate language patterns\nand context, coupled with its iterative improvements, has\nprofoundly impacted academia and industry, revolutionizing\nthe landscape of language understanding and generation.\n2) BERT\nBERT [11], short for \"Bidirectional Encoder Representations\nfrom Transformers,\" is a language model with a distinctive\napproach. Unlike previous models, BERT is designed to pre-\ntrain deep bidirectional representations from unlabeled text\nby considering both left and right context in all layers. This\npre-trained BERT model can be fine-tuned with minimal\nadjustments to create cutting-edge models for various tasks\nlike question answering and language inference, eliminating\nthe need for extensive task-specific modifications. BERT is\nboth conceptually straightforward and remarkably effective,\nachieving state-of-the-art results on eleven different natural\nlanguage processing tasks. Notable accomplishments include\nraising the GLUE score to 80.5% (an impressive 7.7% ab-\nsolute improvement), boosting MultiNLI accuracy to 86.7%\n(a 4.6% absolute improvement), and significantly improving\nSQuAD v1.1 question answering Test F1 to 93.2 (a 1.5 point\nabsolute improvement) and SQuAD v2.0 Test F1 to 83.1 (a\nremarkable 5.1 point absolute improvement).\nIn our analysis, we have exclusively considered versions\nof BERT (Bidirectional Encoder Representations from Trans-\nformers) that are inherently Large Language Models (LLMs).\nSpecifically, we focused on variants of BERT that are pre-\ntrained on extensive text corpora and possess the characteris-\ntics of LLMs, enabling them to understand and generate natu-\nral language comprehensively. This deliberate choice ensures\nthat the models we have included in our study harness the full\nspectrum of language understanding and generation capabil-\nities, thereby aligning with the core objective of our research\nin exploring the impact and advancements of LLMs in the\nfield of natural language processing. Non-LLMs versions\nof BERT or those with significantly reduced model sizes\nwere excluded from our analysis to maintain consistency and\nrelevance in our investigation of the transformative potential\nof Large Language Models.\nVOLUME 4, 2016 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nTABLE 10. Accuracy of various LLMs on different datasets.\nLLMs Accuracy Task\nGPT-1 [67] 68.4%, 48.4% , and 82% Score of GPT-1 in standard NLP Modeling tasks GLUE,LAMBDA,\nand SQuAD 68.4,48.4 , and 82.0 respectively.\nGPT-2 [97] 84.6%, 60.1%, and 89.5% Score of GPT-2 in standard NLP Modeling tasks GLUE,LAMBDA,\nand SQuAD 84.6 ,60.1 , and 89.5 respectively.\nGPT-3 [66] 93.2%, 69.6%, and 92.4% Score of GPT-3 in standard NLP Modeling tasks GLUE,LAMBDA,\nand SQuAD 93.2,69.6, and 92.4 respectively.\nGPT-3.5 [104] 93.5%, 79.3%, and 92.4 Score of GPT-3.5 in standard NLP Modeling tasks GLUE,LAMBDA,\nand SQuAD 93.5 ,79.3 , and 92.4 respectively.\n79.20% GPT-3.5 is 79.2% performance on the VNHSGE English dataset.\nGPT-4 [96] 85.50% 3 shot accuracy on MMLU across languages (English) 85.5%.\n94.2%, 82.4%, and 93.6% Score of GPT-4 in standard NLP Modeling tasks GLUE,LAMBDA,\nand SQuAD 94.2 ,82.4 , and 93.6 respectively.\nChatGPT [105]\n71% and 68%\nA total of 167 SCORE and 112 Data-B questions were presented to\nthe ChatGPT interface. ChatGPT correctly answered 71% and 68%\nof multiple-choice SCORE and Data-B questions, respectively.\n75.1% (SD 3%) and 64.5% (SD 5%)\nThe 5-year average percentage of correct answers for ChatGPT\nwas 75.1% (SD 3%) for basic knowledge questions and 64.5%\n(SD 5%) for general questions.\n0.858 (95% CI: 0.777 to 0.91, p<0.0001)\nThe average intraclass correlation coefficient (ICC) values for ChatGPT\nwere 0.858 (95% CI: 0.777 to 0.91, p<0.0001) with a total of 77 cases\n(answering case vignettes in physiology).\nBingChat [106] 92.40% 92.4% performance on the VNHSGE English dataset.\nBard [98] 86% 86% performance on the VNHSGE English dataset.\nBing [107] 0.975 (95% CI: 0.961 to 0.984, p<0.0001)\nThe average intraclass correlation coefficient (ICC) values for Bing\nwere 0.975 (95% CI: 0.961 to 0.984, p<0.0001) with a total of 77 cases\n(answering case vignettes in physiology).\nBERT [67] Dev 86.6%, Test 86.3% BERT(large)’s performance on SW AG(Situations With Adversarial\nGenerations) where Dev 86.6 ,Test 86.3.\n82.1%, grammatical 60.5%, sentiment analysis 94.9%,\nsimilarity 86.5%, paraphrase 89.3%,\nquestion similarity 72.1%, contradiction 86.7%,\nanswerable 92.7%, and entail 70.1%\nBERT(large)’s performance on GLUE(General Language Understanding\nEvaluation) where 82.1, grammatical 60.5, sentiment analysis 94.9,\nsimilarity 86.5, paraphrase 89.3, question similarity 72.1 ,\ncontradiction 86.7/85.9, answerable 92.7, and entail 70.1.\nFIGURE 7. Taxonomy of LLMs\n3) RoBERTa\nRoBERTA [68] is a study that replicates the BERT pre-\ntraining approach outlined by Devlin et al., in 2019. In this\nstudy, we meticulously assess the influence of various critical\nhyperparameters and training data sizes. It’s worth noting\nthat BERT was initially trained with room for improvement,\nyet it can now perform on par with or even surpass the\nperformance of subsequent models that have been published.\nAs a result, RoBERTa achieves top-tier results in GLUE,\nRACE, and SQuAD evaluations. These outcomes underscore\n20 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nTABLE 11. Description of LLMs\nModel Name Description Key Features Training Data Fine-Tuning\nData\nFine-Tuning\nTasks\nApplications\nGPT (Generative\nPretrained Trans-\nformer) [66]\nTransformative LLMs\nby OpenAI for versa-\ntile NLP tasks.\nExtensive pre-\ntraining, deep\nlanguage under-\nstanding, iterative\nimprovements, impact\non academia/industry\nInternet text data Custom datasets Text generation,\ntranslation, QA,\nand more\nChatbots, content\ngeneration, NLP\ndomains\nBERT\n(Bidirectional\nEncoder\nRepresentations\nfrom\nTransformers)\n[11]\nGoogle AI’s NLP\nmodel excelling with\nbidirectional context\nlearning.\nDeep bidirectional\nrepresentations,\nconceptually\nstraightforward,\nminimal task-specific\nadjustments\nBookCorpus,\nWikipedia\nTask-specific\ndatasets\nVarious NLP\ntasks\nQuestion answer-\ning, language in-\nference\nRoBERTa [68] BERT-based\nmodel with refined\nhyperparameters.\nSignificance of design\ndecisions, publicly\navailable, top-tier\nNLP results\nBookCorpus,\nWikipedia\nTask-specific\ndatasets\nVarious NLP\ntasks\nBenchmark\nimprovements,\nresearch\nXLNet [108] Combines\nautoregressive\npretraining with\nbidirectional context\nlearning.\nBidirectional context\nlearning, versatile ap-\nproach\nInternet text data Task-specific\ndatasets\nDiverse NLP\ntasks\nResearch, appli-\ncations\nSpeech-XLNet\n[109]\nUnsupervised acous-\ntic model with robust\nregularization.\nRobust regularizer,\nimproved recognition\naccuracy\nSpeech datasets TIMIT, WSJ\ndatasets\nSpeech recogni-\ntion\nSpeech recogni-\ntion systems\nDialogXL [110] Improved dialogue\nhandling with dialog-\naware self-attention.\nEnhanced\nconversation\nmodeling,\noutperforms baselines\nInternet text data Dialogue datasets Dialogue under-\nstanding\nChatbots,\ncustomer support\nT5 (Text-to-\nText Transfer\nTransformer)\n[85]\nGoogle’s unified text-\nto-text NLP model.\nUnified framework,\nextensive pre-training,\nversatile tool\nInternet text data Task-specific\ndatasets\nText\nclassification,\ntranslation, and\nmore\nLanguage\ntranslation,\nsummarization\nBioGPT [111] Specialized biomedi-\ncal LLMs with state-\nof-the-art results.\nBiomedical literature\npretraining, excels in\nbiomedical tasks\nBiomedical liter-\nature\nBiomedical\ndatasets\nBiomedical text\nanalysis\nBiomedical text\nanalysis, research\nthe significance of design decisions that were previously\noverlooked and prompt inquiries into the origins of recently\nreported advancements. We have made our models and code\navailable for public use.\n4) XLNet\nXLNet [108] represents a versatile autoregressive pretraining\napproach that achieves bidirectional context learning by op-\ntimizing expected likelihood across all possible permutations\nof factorization orders. It addresses the constraints of BERT\nthrough its autoregressive design and incorporates insights\nfrom Transformer-XL, a leading autoregressive model. In\npractical experiments with consistent conditions, XLNet con-\nsistently surpasses BERT on 20 diverse tasks, frequently\nby a substantial margin. These tasks encompass question\nanswering, natural language inference, sentiment analysis,\nand document ranking, among others.\n5) Speech-XLNet\nSpeech-XLNet [109] is a method for training unsupervised\nacoustic models to learn speech representations using a Self-\nAttention Network (SAN) and subsequently fine-tuning it\nwithin the hybrid SAN/HMM framework. Our hypothesis is\nthat by rearranging the order of speech frames, the permuta-\ntion technique in Speech-XLNet acts as a robust regularizer,\nencouraging the SAN to make inferences by prioritizing\nglobal structures through its attention mechanisms. More-\nover, Speech-XLNet enables the model to explore bidirec-\ntional contexts, enhancing the effectiveness of speech repre-\nsentation learning. Experimental results on TIMIT and WSJ\ndatasets demonstrate that Speech-XLNet significantly en-\nhances the performance of the SAN/HMM system in terms of\nboth convergence speed and recognition accuracy compared\nto systems trained from randomly initialized weights. Our\nbest models achieve an impressive relative improvement of\n11.9% and 8.3% on the TIMIT and WSJ tasks, respectively.\nNotably, the top-performing system achieves a phone error\nrate (PER) of 13.3% on the TIMIT test set, which, to the best\nof our knowledge, is the lowest PER achieved by a single\nsystem.\n6) DialogXL\nDialogXL [110] introduces enhancements to tackle longer\nhistorical context and multi-party structures in dialogues.\nInitially, alterations are made to how XLNet manages re-\ncurrence, transitioning from segment-level to utterance-level,\nVOLUME 4, 2016 21\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nthereby improving its effectiveness in modeling conversa-\ntional data. Secondly, the integration of dialog-aware self-\nattention, as opposed to the standard self-attention in XLNet,\nenables capturing crucial dependencies within and between\nspeakers. While training the DialogXL, a comprehensive set\nof experiments is conducted on four ERC benchmarks, com-\nparing DialogXL with mainstream models. The experimental\nresults consistently demonstrate that DialogXL outperforms\nthe baseline models across all datasets.\n7) T5\nT5 (Text-to-Text Transfer Transformer) [85] is a ground-\nbreaking large language model developed by Google Re-\nsearch, revolutionizing natural language processing (NLP).\nT5’s innovation lies in framing all NLP tasks as text-to-\ntext tasks, simplifying the NLP pipeline and unifying various\ntasks under a single framework. Built upon the Transformer\narchitecture, T5 utilizes multi-head self-attention to capture\nintricate language relationships. Its extensive pre-training on\nvast text data, followed by fine-tuning on specific tasks,\nempowers T5 to excel in text classification, translation,\nsummarization, question answering, and more. With con-\nsistently state-of-the-art results across NLP benchmarks, T5\nhas reshaped the field, offering researchers and developers a\nversatile tool for comprehensive language understanding and\ngeneration tasks.\n8) BioGPT\nBioGPT [111] is a large-scale language model that was\nconstructed by the Allen Institute for AI (AI2) with the\nexplicit purpose of undertaking training on biomedical text.\nIt was trained on an extensive corpus of biomedical literature,\nincluding PubMed abstracts and full-text articles, and is\nbased on the GPT architecture. It has been demonstrated\nthat BioGPT outperforms alternative biomedical language\nmodels across a range of tasks, such as query answering,\nrelation extraction, and named entity recognition. The pre-\ntrained weights of the model are accessible to the public,\nenabling researchers to optimize it using their biomedical\ntext data. BioGPT has the capacity to substantially drive\nbiomedical research forward by facilitating the analysis of\nvast quantities of biomedical text data in a more precise and\nefficient manner [112], [113].\nIn summary, pre-trained LLMs are foundational in NLP,\nproviding a starting point for various applications without\nthe need for extensive training from scratch. They are widely\nused and have democratized access to advanced language un-\nderstanding and generation capabilities. However, responsi-\nble use and ethical considerations are essential when working\nwith these models to ensure fair and unbiased outcomes.\nB. API OF LLMS\nIn this section, we discuss the APIs of LLMs, which have\nbeen described in Table 12.\nOpen AI API: The API provided by OpenAI offers access\nto GPT models that may be utilized for a wide range of\ntext-related applications [120]. It facilitates many tasks such\nas coding, question and answer, analysis, and other related\nactivities. The available models encompass a spectrum of\noptions, spanning from gpt-4 to gpt-3.5-turbo, as well as\nmany legacy variants. The Chat Completions API facilitates\ninteractive dialogues by incorporating distinct roles such as\nuser, and assistance. The programming language provides\nsupport for function calling, which allows for the retrieval of\nstructured data. The OpenAI API provides developers with\nthe capability to leverage advanced modeling of languages\nfor a diverse range of applications.\nHugging Face: Hugging Face provides a complimentary\nInference API that facilitates the examination and assessment\nof more than 150,000 publicly available ML models [121].\nIt features predictive capabilities, and integration with more\nthan 20 open-source libraries, and facilitates fast change\nbetween models. The API facilitates a range of operations,\nincluding classification, image segmentation, text analysis,\nspeech recognition, and other related functionalities.\nGoogle Cloud API: The Cloud-based NLP API developed\nby Google provides support for a range of approaches, such\nas sentiment analysis, text analysis, entity recognition, and\nother text annotations [116]. The functionalities can be ac-\ncessed by developers through REST API calls utilizing either\nthe client libraries or their own custom libraries. Additionally,\nit offers moderation functionalities for the purpose of detect-\ning potentially sensitive content. Several API exists, and each\npossesses distinct features and functions.\nMicrosoft Azure Language APIs: These APIs support\nmany activities, including sentiment analysis, text summa-\nrization, and other related tasks [117]. Developers use REST-\nful endpoints to include Azure LLMs APIs. Microsoft pro-\nvides useful SDKs and code examples in other programming\nlanguages, including Python, Java, etc. to facilitate the uti-\nlization of these APIs.\nIBM Watson Natural Language:The IBM Watson API is a\nrobust tool for investigating and extracting valuable informa-\ntion from textual data. This API offers developers a variety\nof functionalities, encompassing sentiment analysis, emotion\nanalysis, and additional features [118]. Due to its provision of\nmultilingual support and a user-friendly API, this technology\nenables developers to effectively include sophisticated text\nanalytics into their programs.\nAmazon Comprehend API:The Amazon Comprehend API\nis a powerful NLP service provided by Amazon Web Ser-\nvices [119]. This tool evaluates textual data, allowing the\nresearchers to acquire significant knowledge, such as en-\ntity recognition, language detection, sentiment analysis, and\ntopic modeling. Due to its ability to accommodate many\nlanguages and simple integration, this tool displays adapt-\nability in addressing a range of use cases, including customer\nfeedback analysis and others. The utilization of this API can\nprove to be a significant resource for enterprises’ marketing\nto extract practical insights from unstructured textual data.\nFacebook AI’s Fairseq:The Fairseq framework developed\nby Facebook AI is a comprehensive tool for performing\n22 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nTABLE 12. Comparison of LLMs APIs\nAPI Name Provider Languages\nSupported\nAccess\nType\nApplication\nArea\nAdvantages Constraints\nOpenAI\nAPI [114]\nOpenAI Multiple\nlanguages\nAPI Key NLP, text\ngeneration,\nchatbots\nState-of-the-\nart models,\nversatility, GPT\narchitecture\nAPI rate con-\nstrain, cost con-\nsiderations\nHugging\nFace Trans-\nformers\n[115]\nHugging\nFace\nMultiple\nlanguages\nOpen\nSource\nNLP, model\nfine-tuning,\nresearch\nLarge model\nrepository,\nextensive\ncommunity\nsupport\nSelf-hosting\ncomplexity, no\nofficial support\nGoogle\nCloud AI-\nLanguage\n[116]\nGoogle\nCloud\nMultiple\nlanguages\nAPI Key Sentiment\nanalysis, entity\nrecognition,\ntranslation\nGoogle’s robust\ninfrastructure,\neasy integration\nCost may vary\nbased on usage\nMicrosoft\nAzure\nLanguage\n[117]\nMicrosoft\nAzure\nMultiple\nlanguages\nAPI Key Sentiment\nanalysis, entity\nrecognition,\nlanguage\nunderstanding\nIntegration\nwith Azure\nservices,\ncomprehensive\nAPIs\nPricing based\non usage\nIBM\nWatson\nNLU [118]\nIBM\nWatson\nMultiple\nlanguages\nAPI Key Sentiment\nanalysis,\nemotion\nanalysis,\nkeyword\nextraction\nIBM’s AI\nexpertise,\ncustomization\noptions\nCosts may add\nup for high us-\nage\nAmazon\nCompre-\nhend [119]\nAmazon\nAWS\nMultiple\nlanguages\nAPI Key Entity recogni-\ntion, sentiment\nanalysis, topic\nmodeling, doc-\nument classifi-\ncation\nIntegration\nwith AWS,\nscalability\nCosts may vary\nbased on usage\nFacebook\nAI’s\nFairseq\n[119]\nFacebook\nAI\nMultiple\nlanguages\nOpen\nSource\nNeural machine\ntranslation,\nlanguage\nmodeling,\nresearch,\ndevelopment\nResearch-\noriented,\nflexibility,\nopen-source.\nSelf-\nhosting and\nmaintenance\ncomplexity.\nsequence-to-sequence modeling, specifically designed for\nhandling LLMs [122]. Fairseq is a well-suited API for many\napplications related to analyzing and generating natural lan-\nguage. The platform provides support for advanced models\nsuch as BERT and RoBERTa, allowing researchers to per-\nform fine-tuning on these models according to specific needs.\nIn this study, we have provided a comprehensive overview\nof seven popular APIs in Table 12 that leverage the capa-\nbilities of LLMs for the purpose of NLP-based function-\nalities. However, the taxonomy revealed the presence of\nseveral other APIs that are associated with text analysis but\ndo not utilize LLMs. The aforementioned APIs, including\nTextBlob, TextRazor, Sapling AI, MonkeyLearn, and Aylien,\netc., utilize traditional machine learning, statistical methods,\nand rule-based natural NLP techniques instead of relying on\nextensive pre-trained LLMs. Since, the primary focus of this\nstudy has been on describing the tools that particularly utilize\nLLMs for the purpose of advanced text analysis, generation,\nand comprehension, we have refrained from discussing these\nAPIs in depth.\nVII. DOMAIN SPECIFIC APPLICATION\nSince there are several pre-trained models in LLMs, all\nof them are utilized by training or fine-tuned to perform\nwell-defined tasks maintained by their requirements in dif-\nferent fields. Numerous research studies have consistently\ncontributed by using LLMs model in diverse domains such\nas healthcare, finance, education, forecasting, and natural\nlanguage processing. The extensive experiments of differ-\nent LLMs models contribute to revolutionizing the use of\nAI across these diverse domains. This section demonstrates\nthe potential contribution of LLMs application in different\ndomains. Table 13 illustrates the major contribution of LLMs\nin the specific domain, as well as outline their prospective\nlimitations and future directions.\nBio-Medical and Healthcare: As previously stated, GPT\nhas several versions, ranging from GPT1 to GPT4. GPT3 is\nextremely useful in the healthcare industry since it can be\ntrained to support customer service with no effort. It can\nget all required information through a conversation rather\nthan an intake form, and many systems might be built to\nVOLUME 4, 2016 23\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nTABLE 13. Domain Specific Machine learning-based study comparison in LLMs\nDomain Author Major Contributions Limitations Future Research Direction\nMedical\nChen et al., [123]\n(2023)\nI. Assess the state-of-the-art performance\nof biomedical LLMs for the purpose of\nclassifying and reasoning tasks on clinical\ntext data. II. Emphasizes the vulnerability\nof LLMs performance in relation to prompts\nand addresses it.\nI. Data limitation due to privacy\nconcern of biomedical data.\nII. Did not evaluate the performance\nof the model in an out-of-domain task.\nI. To support this study’s findings,\nneed to experiment using real clinical\ndata.\nII. Optimize the models to make them\nmore robust and resource-efficient.\nHuang et al., [124]\n(2023)\nI. Investigates the possible utilization of LLMs,\nspecifically ChatGPT and its variety within the\ndomain of dentistry.\nII. Design a MultiModal LLMs system for\nclinical dentistry application and address\ncritical challenges to revolutionize\ndental diagnosis.\nI. Lack of data resulted in the\npost-training process, raising concerns\nabout the model’s reliability.\nII. The possibility of data breaches has\nno strict security method.\nIII. Requires intensive computational\ncost.\nI. Reducing operational costs by fine-tuning\nthe model and enhancing efficiency.\nII. Explore diverse medical data to provide\npersonalized dental care.\nSorin et al., [125]\n(2023)\nI. Evaluating the efficacy of ChatGPT-3.5 as\na supporting tool for facilitating clinical\ndecision-making in breast tumor cases.\nII. Outlines the implementation of a grading\nsystem for evaluating the responses generated\nby ChatGPT.\nI. Conducting the experiment with a small\nsample size leads to performance bias in\nthe model.\nII. Human errors in the grading system can\npotentially add biases to the system.\nI. More diverse sample of breast tumor cases\nto increase ChatGPT’s performance and\ngeneralizability.\nII. Introducing a multimodal approach to\nincrease the reliability of clinical\nrecommendations.\nThirunavukarasu et al.,\n[126] (2023)\nI. Focuses on the energy and environmental\nimpact of training LLMs models such as\nGPT-3 and GPT-4 and emphasize cost\nreduction to make them more accessible.\nII. Examines the utilization of LLMs models\nin the medical domain, specifically focusing\non medical education and medical research.\nI. Inaccuracies observed in the responses\nprovided to queries due to the lack of\nupdates on the training data.\nII. Lack of interpretability of LLMs model\nsince it is a black box, hence the concept was\nfrequently misunderstood.\nI. Emphasis on integrating more recent and\nup-to-date training data.\nII. Further investigation should strive to\nenhance the transparency and interpretability\nof LLMs.\nIII. Including the feasibility of implementing\nrandomized trials to evaluate the effects of\nLLMs on medical outcomes.\nKorngiebel et al., [127]\n(2021)\nI. Discuss the benefits and potential pitfalls of\nNLP technologies in eHealth.\nII. Discuss the benefits of using GPT in the\nmedical domain.\nI. Conversational AI like GPT-3 will not\nreplace human interaction in healthcare\nsoon, despite extensive development.\nII. Examines GPT’s applicability in a\ncertain medical domain.\nI. Analyze GPT’s impact on real-world\nhealthcare settings to assess its performance.\nII. Provide personalized healthcare by\nanalyzing a variety of medical data.\nAngelis et al., [128]\n(2023)\nI. examine LLMs’ ethical and practical issues,\nfocusing on medicinal use and public health.\nII. Discuss how ChatGPT can provide false or\nmisleading information.\nIII. Suggest the detectable-by-design technique\nto spot fake news or information.\nI. The addition of a detectable-by-design\nthe technique may slow LLMs development\nand AI business acceptance.\nII. Experimental data has been limited due\nto medical data privacy concerns.\nI. An experiment using real clinical data is\nneeded to support the findings.\nII. Further research should be conducted to\nspeed up the entire procedure.\nSallam et al., [129]\n(2023)\nI. Saves time in scientific research through\ncode delivery and literature review.\nII. Makes the publication process faster by\nproviding better research ideas and results.\nIII. Reduces potential costs and increases\nefficiency in healthcare delivery.\nIV . Enhances communication skills in\nhealthcare education through proper\nacademic mentoring.\nI. Copyright issues, bias based on the training\ndataset, plagiarism, over-detailed content, lack\nof scientific accuracy, limited updated\nknowledge, and lack of ability to critically\ndiscuss the results in using ChatGPT in scientific\nresearch.\nII. Unable to understand the complexity\nof biological systems, lack of emotional and\npersonal perspective,\ninaccurate content, bias, and transparency issues\nin healthcare practice.\nIII. Copyright issues, inaccurate references,\nlimited updated knowledge, and plagiarism in\nhealthcare education.\nI. Accountability, honesty, transparency, and\nintegrity must be considered in scientific\nresearch.\nII. To enhance healthcare and academics,\nChatGPT should uphold ethical principles.\nPotential dangers and other issues must also\nbe considered.\nIII. An AI editor and an AI reviewer in\nacademic writing to advance academic\nresearch, given the previous shortcomings\nof the editorial and peer review process.\nCascella et al., [130]\n(2023)\nI. Support of clinical practice\nII. Scientific writing\nI. Generates answers that sound plausible but\nmay be incorrect or meaningless and biased\nbased on trained data.\nI. Enhance the ability to answer medical\nquestions and provide the context for\nunderstanding complex relationships\nbetween various medical conditions\nand treatments.\nKung et al., [131]\n(2023)\nI. The investigation of AI within the context\nof medical education.\nII. Assessment of ChatGPT’s Performance in\nClinical Decision-making.\nIII. Explore the demands of AI in medical\neducation to standardize methods and readouts\nand quantify human-AI interactions\nI. The experiment is conducted on a small\ninput size.\nII. Human adjudication variability and bias.\nIII. The absence of real-life instructional\nscenarios.\nI. To evaluate the efficacy of ChatGpt in\nreal-world clinical practice by assessing\nits performance and impact.\nII. A comprehensive analysis of ChatGPT’s\neffectiveness in relation to subject taxonomy.\nGu et al., [132]\n(2021)\nI. Shows that domain-specific pretraining from\nscratch outperforms mixed-domain in\nbiomedical NLP.\nII. Formulate a new dataset using the\nBiomedical set of diverse tasks.\nI. Explore the applicability only in a fixed\nBiomedical Domain.\nII. Future modifications of the benchmark may\nbe required to reflect the effectiveness of the\nresearch.\nI. An Investigation and analysis into\npretraining strategies.\nII. The addition of Biomedical NLP tasks.\nIII. Exploring other domains for comparative\nanalysis.\nKraljevic et al., [133]\n(2022)\nI. Introduced a foresight application based on\nelectronic health records.\nII. Develop a multifunctional model.\nIII. Conduct experiments in different hospitals.\nI. Should include metrics, and comparative\nanalysis in real-world clinical scenarios to\nevaluate Foresight’s performance.\nII. Integrate enough security on health records\nto protect the privacy of the patients.\nI. Integrating input from healthcare specialists\nand consistently updating the model with the\nlatest medical data.\nII. Implement a real-life scenario to investigate\nthe clinical application of Foresight.\nTourism\nMich et al., [134]\n(2023)\nI. Highlights how ChatGPT is contributing to\nthe tourism sector by identifying new target\nmarkets, implementing the marketing strategy\ndesigns, and improving customer service.\nI. Transparency and accountability issues: the\ndataset is not updated, and can not see the logic\nof what is wrong and what is right.\nI. Applications should increase user trust and\nfact-checking.\n24 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nTABLE 13. (Continued) Domain Specific Machine learning-based study comparison in LLMs\nDomain Author Major Contributions Limitations Future Research Direction\nIndustry\nYu et al., [135]\n(2023)\nI. Examines how LLMs can use their superior\nknowledge and reasoning to predict financial\ntime series.\nII. Focuses on NASDAQ-100 stocks using\npublicly available historical stock price data.\nIII. To prove LLMs can solve problems\ncomprehensively, experiments are conducted.\nI. The study utilizes a small amount of data\nsamples.\nII. Data is collected from only one specific\ndomain.\nIII. Utilizing a small sample size during\nexperiments cause performance bias.\nI. SP500 and Russell 2000 stock indexes\nwill be added to the research.\nII. The research will use macro-economy\ntime series, stock trading volumes, and social\nnetwork data.\nIII. To improve reasoning, larger public models\nlike 30B will be refined.\nFrederico et al., [136]\n(2023)\nI. Discusses the uses and concerns with ChatGPT\nin supply chains.\nII.Provide supply chain specialists\nadvice about ChatGPT’s effects and\nusage.\nI.A limited amount of data is used in the\nexperiment.\nII. Did not assess the efficacy of ChatGPT\nin practical industrial settings.\nI. Analyze how ChatGPT can enhance the\nsupply chain efficiency.\nII. Discuss supply chain ChatGPT\nimplementation\nissues and success factors.\nGaming\nSobieszek et al., [137]\n(2022)\nI. Examines the efficacy of employing LLMs\nas a gaming tool.\nII. Assess the performance of GPT in the\ncontext of the Turing test.\nIII. Analyze the boundaries of LLMs.\nIV . Discuss the challenges these models\nencounter in accurately conveying\ninformation.\nI. They did not employ a well-curated set of\ntargeted questions.\nII. It may produce answers that are either\nerroneous or lack significance.\nI. Assess the performance of LLMs by\nadministering inquiries across diverse\ndomains.\nEducation\nAbramski et al., [43]\n(2023)\nI. Utilized network science and cognitive\npsychology to study biases toward math and\nSTEM across language models.\nII. Behavioral Forma Mentis Networks\n(BFMNs) are used to understand how LLMs\ncomprehend arithmetic, STEM, and similar\nconcepts.\nI. Commercial GPT systems can be tested\nby researchers but not replicated by everyone\ndue to their structure.\nII. The old interface or API system no longer\nallows public access to GPT-3.\nI. Putting a priority on integrating data from\ntraining that is up-to-date.\nII. Investigating several other fields for the\npurpose of comparative research.\nIII. More information from students at\ndifferent institutions will be gathered.\nKasneci et al., [20]\n(2023)\nI.Helps students develop critical thinking in\nreading and writing, provides practice problems\nand quizzes, helps improve research skills, and\nimproves various developmental skills.\nII.Provides guidance to teachers on how to improve\nstudent learning in each aspect\nof teaching and helps develop teaching materials.\nI. Helpful only for English-speaking people,\nbut also for people of other languages cannot\nenjoy the benefits.\nII.Consumes high energy and financial\ncost of maintenance.\nIII. Negative effect on critical thinking and\nproblem-solving skills of students and teachers.\nIV .Privacy and security risks to students’\npersonal and sensitive information.\nI. Creating an age-appropriate user interface\nthat maximizes the benefits and minimizes\nthe pitfalls of interaction with AI-based tools.\nII. To guarantee equity for all educational entities\ninterested in current technologies, government\norganizations may regulate financial obstacles to\naccessing, training, and maintaining large\nlanguage models.\nHadi et al., [138]\n(2023)\nI. Helps students save labor and time by assigning\nassignments and helps teachers automate the grading\nprocess, and provides detailed feedback to students,\nwhich reduces their workload.\nII.Aid decision-making, problem-solving and promote\nlearning in medical education.\nIII.Provides financial advice based on their queries to\nimprove customer service, and provides various steps\nbased on financial algorithms to reduce risk by\nanalyzing past market data.\nIV . Saves software engineers time and increases\noverall efficiency by providing code snippets,\nidentifying and generating test cases, etc.\nI. Bias, reasoning errors, counting errors,\ninformation hallucination, LLMs explainability.\nI. Improving the accuracy and performance\nof LLMs, addressing their limitations, and\nexploring new ways to utilize them.\nLo et al., [139]\n(2023)\nI. Helps students in learning and assessment and helps\nteachers in teaching preparation and assessment.\nI. Negative effect on critical thinking and\nproblem-solving skills of students and\nteachers.\nI. Training instructors on how to effectively\nuse ChatGPT and identify student intelligence.\nAlso, educate students about the uses and\nlimitations of ChatGPT.\nDwivedi et al., [140]\n(2023)\nI. Highlights the challenges, opportunities, and impacts of\nChatGPT in education, business, and society, as well as\ninvestigates important research questions asked of\nChatGPT across the education, business, and society sectors.\nI. The generated text is hard to understand\nand can’t answer questions correctly unless\nphrased a certain way, lacks updated information,\nand doesn’t automatically update the actual data.\nI. Teaching, learning, and scholarly research,\ndigital transformation organization and society,\nknowledge, transparency, and ethics to enhance\nChatGPT’s efficiency in all these areas.\nassist numerous patients at the same time [127]. Besides,\nclinics and hospitals are places to cure illness, but it is\nalso true that various contagious viruses are brought into\nthese places. Patients and healthcare providers can be better\nprotected from infection by replacing a human receptionist\nwith a robot. This becomes increasingly important during\nthe COVID-19 epidemic [141]. Since clinics and hospitals\noften see a high volume of patients on a daily basis, an\noptimum and lightweight system may submit several queries\nfor single patients to create acceptable output. Consequently,\nGPT models can also aid in cost reduction in the medical\nindustry. Furthermore, biomedical and clinical text mining\nhas always been an essential and major challenge due to\nthe complex nature of domain corpora and the continually\nexpanding number of documents. As a result, using the BERT\nmodels improves the performance of biomedical and clinical\ntext mining models [142]. Salam et al., [129] and Korngiebel\net al., [127] demonstrate the substantial advantages of Chat-\nGPT in the domains of healthcare, clinical research, and prac-\ntice, although simultaneously underscoring the imperative\nnecessity for proactive inspection and ethical transparency.\nSeveral studies [126], [130], [132], [133] investigations at\nexploring the prospective utilities and constraints of LLMs\nsuch as ChatGPT within the healthcare domain, namely in\nthe context of clinical practice, research, and public health.\nIn their study, Kung et al., [131] conducted an evaluation\nof ChatGPT’s performance on the United States Medical\nLicensing Examination (USMLE), and the outcomes indi-\ncate the potentiality of LLMs to support clinical decision-\nmaking and medical education. Sorin et al., [125] evaluated\nChatGPT-3.5 as a decision support for breast tumor boards\nwhere they compared the tumor board’s explanations, and\nsummaries with ChatGPT-3.5 and showed that ChatGPT-3.5\nand the tumor board had a high degree of decisional align-\nment. Huang et al., [124] (year) investigate the prospective\napplications of LLMs with a specific emphasis on ChatGPT,\nin the field of dentistry, mainly focusing on automated dental\ndiagnosis and highlighting the efficacy of LLMs in den-\nVOLUME 4, 2016 25\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\ntal diagnosis. Furthermore, the XLNet contributes to better\nclinical note representation by adding temporal information\nand a realistic prediction setup [143]. Furthermore, various\nLLMs models also assist this medical industry by making the\nprocedure easier than previously.\nEducation: Educators have long struggled with unequal\neducational resources to student demand across disciplines.\nOne of the significant challenges is a shortage of accessible\neducational resources for pupils to study outside of school.\nAlthough online instructional videos are helping to alleviate\nthe problem, society still hopes that AI will deliver indi-\nvidualized teaching services to satisfy the learning demands\nof each student and increase teaching efficiency. So, the\nLLMs models are very significant and have the potential to\nrevolutionize many facets of learning, teaching, and educa-\ntional research in the education sector [141]. So, the GPT\nmodel aids the students in converting the math word prob-\nlems into representative equations [144]. Kasenci et al., [20]\nhighlighted substantial impact of LLMs in education by fa-\ncilitating personalized learning, automating grading process,\nand accessibility of educational resources. Hadi et al., [138]\npresents a thorough analysis of LLMs, covering their his-\ntorical development, wide-ranging applications in domains\nsuch as medicine, engineering, education, and their potential\nimpact on the trajectory of AI. Lo et al., [139] and Dwivedi et.\nal. [140] investigate the prospective uses of ChatGpt within\nthe realm of education and identify the primary obstacles that\nhave arisen during its initial deployment. Besides, in terms of\nwriting authentic texts in distinct formats, including essays,\nsummaries, and articles, these models help to accomplish\nthis without any error. In contrast, the manual process may\nhave human errors in the documentation. In this case, the\nGPT model helps to address this problem. In addition, the\nXLNet Excel method also helps understand the texts and\ndocuments that can be employed in the education sector [39].\nFurthermore, other models significantly impact the education\nsystem, making it more engaging, accessible, and productive\nfor both students and teachers.\nSocial Media: The LLMs have revolutionized several as-\npects of the social media industry regarding content produc-\ntion, moderation, sentiment analysis, etc. There are some\ncrucial aspects of the LLMs in the social media sector in\nterms of writing content, generating images, classifying and\ngenerating text, and even full blogs and articles for social me-\ndia. Also, these models can perform named entity recognition\n(NER) and text classification [145], [146]. When the GPT,\nXLNet, BERT, etc., model aids the writer and content pro-\nducers in generating a consistent flow of excellent material. It\nalso provides content suggestions, and to create a safer online\nenvironment, these models are hired to assist in discovering\nand filtering out different dangerous and improper content.\nIn their study, Abramski et al., [43] utilized network science\nand the principles of cognitive psychology to evaluate biases\npresent in LLMs. Sobieszek et al., [137] presents a critical\nexamination of the stated semantic capabilities of GPT-3,\naiming to challenge the current view of its dismissal. More-\nover, it assists in determining public opinion on certain topics\nby analyzing public interest and demand.\nBusiness: In business, LLMs helps companies improve\ntheir decision-making processes, product manufacturing pro-\ncesses, operations, and customer interactions. Communicat-\ning with customers and providing 24/7 customer service\nby answering their queries, assisting them in their work,\nand providing advanced advice related to areas of interest\nto customers is crucial for business progress. Moreover, it\nis also important to analyze customer sentiment, market\ntrends, risk factors, and competitive intelligence [21]. In this\ncase, LLMs help to fulfill all their requirements within a\nshort period. The LLMs models, like GPT, XLNet, BERT,\netc., play a vital role in creating customer documents and\nproduct details and efficiently maintaining the entire business\nby saving time and reducing laborious tasks. Frederico et\nal., [136] presents an initial investigation into the potential\napplications and effects of ChatGPT in the domain of supply\nchain management. Their study provides significant insights\nfor professionals engaged in this domain. Mich et. al. [134]\npresent an initial investigation of potential hazards associated\nwith the implementation of ChatGPT in bussiness domain.\nYu et al., [135] presented an analysis of the capabilities\nof LLMs, specifically GPT-4, in the context of financial\nforecasting for a time series. Besides, their findings reveal\nthat the performance of LLMs outperforms other traditional\nmodels also.\nAgriculture: In agriculture, variations of GPT models,\nincluding GPT3, BERT, and XLNet models, play a signif-\nicant role [147]–[149]. They are able to analyze large data\nhubs of soil, crop, and weather data along with satellite im-\nagery. They can provide recommendations on plating times,\nirrigation, fertilizer application, and optimizing fields and\nresources. Farmers can obtain current updates and market\nrequirements, predict crop prices, anticipate natural disasters,\nand document farmers’ and crop details. Manual agriculture\nmanagement can be time-consuming and laborious, but these\nmodels can handle all the issues.\nVIII. IMPACT OF LARGE LANGUAGE MODELS ON\nSOCIETY\nLarge Language Models (LLMs) and similar AI technolo-\ngies have had a profound impact on society across various\ndomains. While these technologies offer many benefits, they\nalso raise important ethical, social, and economic considera-\ntions. Here’s an overview of the impact of LLMs on society:\n1. Advancements in Natural Language Processing\n(NLP): LLMs have significantly advanced the field of NLP,\nmaking it possible to automate and scale a wide range of\nlanguage-related tasks such as translation, summarization,\nsentiment analysis, and more. In recent years, Natural Lan-\nguage Processing (NLP) has witnessed significant advance-\nments, primarily driven by the emergence of Large Language\nModels (LLMs). These advancements, exemplified by mod-\nels such as BERT [11], RoBERTa [68], and XLNet [108],\nhave transformed the NLP landscape. Notably, LLMs have\n26 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\nFIGURE 8. Visual representation of impact on LLMs\nbeen fine-tuned for various specific NLP tasks, enabling\nremarkable performance improvements. Multilingual models\nlike mBERT [150] and cross-lingual models like XLM-R\n[151] have facilitated language understanding across diverse\nlinguistic contexts. Additionally, there has been a focus on\ncreating more efficient versions of LLMs such as DistilBERT\n[152] and ALBERT [153]. These developments have not only\nexpanded the applicability of NLP but have also raised ethical\nconsiderations, prompting research in bias mitigation [154]\nand responsible AI. LLMs have enabled breakthroughs in\napplications like conversational AI, few-shot and zero-shot\nlearning, and domain-specific NLP in fields like healthcare\nand finance. These advancements underscore the pivotal role\nof LLMs in advancing the capabilities of NLP and continue\nto shape the future of language understanding and generation.\n2. Automation and Efficiency: LLMs are used to auto-\nmate tasks that were previously time-consuming and labor-\nintensive, leading to increased efficiency in industries such\nas customer support, content generation, and data analysis.\nThe automation and efficiency of Large Language Mod-\nels (LLMs), driven by models like BERT and GPT, have\nrevolutionized industries and applications. These models\nhave automated intricate language-related tasks, from sen-\ntiment analysis to language translation, making them more\nefficient and accessible. LLMs, such as DialoGPT [155]\nand ChatGPT, have powered conversational AI, streamlining\ncustomer support and interactions. Moreover, they excel in\nfew-shot and zero-shot learning, as demonstrated by GPT-3\n[156], automating tasks with minimal examples. Multilingual\nLLMs like mBERT have automated language tasks across\nvarious languages, enhancing global accessibility. Efficiency\nhas further advanced through models like DistilBERT and\nALBERT, which maintain performance while reducing com-\nputational resources. These models can be fine-tuned for\nspecific domains, such as healthcare [157], making them in-\ndispensable in automating domain-specific tasks efficiently.\n3. Content Generation: LLMs are capable of generating\nhuman-like text, which has implications for content creation,\nincluding automated news articles, marketing materials, and\ncreative writing.\n4. Language Translation: LLMs have improved ma-\nchine translation systems, making communication across lan-\nguages more accessible and accurate.\n5. Virtual Assistants and Chatbots: LLMs power virtual\nassistants and chatbots, enhancing customer service and pro-\nviding round-the-clock support in various industries.\n6. Medical and Scientific Research:LLMs are used to an-\nalyze and summarize vast amounts of medical and scientific\nliterature, aiding researchers in finding relevant information\nquickly.\n7. Accessibility: LLMs have the potential to improve ac-\ncessibility by providing real-time translation and transcrip-\ntion services for individuals with hearing impairments or\nlanguage barriers.\n8. Personalization: LLMs enable personalized recommen-\ndations and content curation on platforms such as social\nmedia, e-commerce, and news websites.\n9. Creative Tools: LLMs are used as creative tools in\nvarious art forms, including generating poetry, music, and\nvisual art.\n10. Ethical Concerns: Bias and fairness issues in LLMs\nhave raised ethical concerns. LLMs may perpetuate or am-\nplify biases present in training data, leading to unfair or\ndiscriminatory outcomes.\n11. Misinformation and Disinformation: LLMs can gen-\nerate realistic-sounding fake text, raising concerns about the\nspread of misinformation and disinformation.\n12. Job Displacement: The automation capabilities of\nLLMs may lead to job displacement in certain industries, par-\nticularly in routine data-entry and content-generation roles.\n13. Data Privacy: The use of LLMs often involves pro-\ncessing large amounts of user-generated text data, which\nraises data privacy concerns, especially regarding sensitive\nor personal information.\n14. Economic Impact: The adoption of LLMs can disrupt\ntraditional business models and create economic shifts as\nindustries adapt to automation and AI technologies.\n15. Regulation and Accountability: Policymakers and\nregulators are grappling with the need to establish guidelines\nand regulations for the responsible use of LLMs, including\naddressing issues of bias, transparency, and accountability.\n16. Education and Skill Development: The rise of LLMs\nunderscores the importance of education and skill develop-\nment in AI and data science, as these technologies become\nincreasingly integral to various industries.\nThe impact of LLMs on society is multifaceted, and it is\nimportant to consider both the positive and negative conse-\nquences. As these technologies continue to evolve, stakehold-\ners, including governments, businesses, researchers, and the\ngeneral public, must work together to harness the benefits\nof LLMs while addressing their challenges and ethical im-\nplications. The visual representation of Figure 8 effectively\ndemonstrates the impact of LLMs, outlining their benefits on\nthe left and the adversarial impacts on the right side. The\nutilization of this figure will provide a distinct and easily\nVOLUME 4, 2016 27\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nunderstandable visual depiction of LLMs’ impact across\ndifferent domains.\nIX. INDUSTRIAL SIGNIFICANCE OF LARGE LANGUAGE\nMODELS\nLarge Language Models (LLMs) have gained substantial\npopularity in various industries, bringing about significant\ntransformations. Their industrial importance can be under-\nstood through several key facets:\n1. Enhancing Natural Language Processing (NLP) Ap-\nplications: LLMs have ushered in a revolution in NLP appli-\ncations [158] across sectors like customer service, chatbots,\nand sentiment analysis. They contribute to more precise\nand efficient interactions with users, leading to increased\ncustomer satisfaction and reduced response times.\n2. Enabling Data Analysis and Information Extraction:\nLLMs play a pivotal role in extracting valuable insights from\nunstructured text data [159]. This is particularly critical in\nfields like finance, market research [160], and healthcare,\nwhere deciphering market trends, sentiment in news, or med-\nical records holds paramount significance.\n3. Facilitating Translation Services: Industries heavily\nreliant on multilingual communication [161], such as e-\ncommerce, travel, and international business, benefit from\nLLMs that streamline automated translation. This not only\nsaves time but also resources, ensuring high-quality transla-\ntions across multiple languages.\n4. Empowering Content Generation:LLMs are harnessed\nfor content generation [162], which encompasses automated\narticle writing, social media posts [163], product descrip-\ntions, and more. This automation simplifies content creation\nprocesses and allows for scalable production of top-tier con-\ntent.\n5. Revolutionizing Healthcare: LLMs find applications in\nmedical record analysis [130], diagnosis assistance, and drug\ndiscovery. They empower healthcare professionals to access\nand comprehend extensive medical literature and patient\ndata, thereby enhancing healthcare decision-making.\n6. Revamping Education: The education sector [164]\nleverages LLMs for automated grading, ensuring prompt\nfeedback to students. These models also contribute to the\ndevelopment of intelligent tutoring systems and personalized\nlearning platforms.\n7. Aiding Legal Practices: Legal practitioners [165] bene-\nfit from LLMs for contract analysis, legal research, and docu-\nment review. These models assist in efficiently extracting per-\ntinent information and identifying potential legal concerns.\n8. Assisting Human Resources:LLMs support HR profes-\nsionals [166] in tasks like candidate screening, resume pars-\ning, and identifying potential job candidates. They streamline\ntime-consuming processes within the recruitment phase.\n9. Empowering Financial Services: In the realm of fi-\nnancial services [167], LLMs come into play for activities\nlike sentiment analysis of news articles, algorithmic trading,\nrisk assessment, and fraud detection. They are instrumental in\nmaking informed investment choices and managing financial\nrisks.\n10. Boosting E-commerce: LLMs enable personalized\nproduct recommendations [168], chatbots for customer sup-\nport, and efficient inventory management. These enhance-\nments result in enriched user experiences and heightened\nsales.\n11. Illuminating Customer Insights: LLMs analyze cus-\ntomer reviews [169], feedback, and social media data, fur-\nnishing businesses with insights into customer preferences,\nopinions, and sentiments. This invaluable information aids\ncompanies in customizing their products and services.\nAs LLMs continue to advance, their industrial importance\nis on a steady rise. They streamline operations, enhance\ndecision-making, and bolster efficiency across diverse do-\nmains, positioning them as a transformative technology in the\ncontemporary business landscape.\nX. OPEN ISSUES AND CHALLENGES\nThis section discusses critical analysis of open issues and\nchallenges of LLMs.\nA. OPEN ISSUES\nIn this section, we delve into the critical open issues sur-\nrounding LLMs. These concerns are at the vanguard of artifi-\ncial intelligence research and development. They emphasize\nthe need for ongoing research and innovation to resolve\nissues that have emerged alongside the rapid development of\nLLMs. Our discussion will cast light on the significance of\nthese unresolved issues, highlighting their impact on various\napplications and the AI landscape as a whole.\n• Issue 1: Ethical and Responsible AI\nThe question regarding how to ensure the ethical use\nof large language models remains unresolved. Filtering,\nmoderation, and accountability concerns regarding AI-\ngenerated content remain troublesome. Misinformation,\nhate speech, and biased content generated by LLMs\nnecessitate continuous research and development [170].\n• Issue 2: Multimodal Integration\nWhile LLMs are predominantly concerned with text,\nthere is a growing demand for multimodal models that\ncan comprehend and generate content that includes text,\nimages, and other media types [171]. Integrating multi-\nple modalities into a single model poses difficulties in\ndata acquisition, training, and evaluation.\n• Issue 3: Energy Efficiency\nThe environmental impact of training and deploying\nlarge language models is still an urgent concern [172].\nIt is essential to develop more energy-efficient training\nmethods, model architectures, and hardware solutions to\nreduce the carbon footprint of LLMs.\n• Issue 4: Security and Adversarial Attacks\nLLMs are vulnerable to adversarial assaults, where\nslight input modifications can lead to unexpected and\npotentially harmful outputs [173]. Improving model ro-\nbustness and security against such assaults is a crucial\n28 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\narea of study, particularly for cybersecurity and content\nmoderation applications.\n• Issue 5: Privacy and Data Protection\nAs LLMs become more competent, user privacy and\ndata protection concerns increase. Finding methods for\nusers to interact with these models without compromis-\ning their personal information is an ongoing challenge.\nThere is a need for research on privacy-preserving tech-\nniques and regulatory compliance [174].\n• Issue 6: Generalization and Few-Shot Learning\nLLMs excel when there is abundant data but struggle\nwith tasks requiring few examples or domain-specific\nknowledge. Improving their capacity to generalize and\nperform well with limited training data is a crucial area\nof research [175].\n• Issue 7: Cross-Lingual and Low-Resource Settings\nIt is an ongoing challenge to make LLMs more ac-\ncessible and effective in languages and regions with\nlimited resources and data [176]. Global applications\nrequire developing techniques for cross-lingual transfer\nlearning and low-resource language support.\nB. CHALLENGES\nLLMs have rapidly evolved from being non-existent to be-\ncoming a ubiquitous presence in the field of machine learning\nwithin just a few years. Their extraordinary ability to generate\ntext that resembles that of a human has garnered significant\nattention and applications in numerous fields. However, this\nmeteoric rise in prominence has also revealed many chal-\nlenges and concerns that must be addressed to realize the\npotentiality of these models fully. In this discussion, we will\nexamine ten of the most significant challenges pertaining to\nLLMs.\n• Challenge 1: Data Complexity and Scale\nIn the era of LLMs, the size and complexity of the\ndatasets on which they are trained is one of the most sig-\nnificant challenges. These models are typically trained\non enormous corpora of Internet-sourced text data.\nThese datasets are so extensive that it is nearly impos-\nsible to comprehend or investigate the totality of their\ninformation. This raises concerns regarding the quality\nand biases of the training data and the potential for the\nunintentional dissemination of detrimental or inaccurate\ninformation [177].\n• Challenge 2: Tokenization Sensitivity\nFor analysis, LLMs rely significantly on tokenization,\ndividing text into smaller units (tokens) [178]. Tokeniza-\ntion is essential for language processing and compre-\nhension but can also present challenges. For instance,\nthe meaning of a sentence can alter significantly based\non the choice of tokens or the ordering of words. This\nsensitivity to input phrasing can lead to unintended out-\ncomes when generating text, such as adversarial assaults\nand output variations based on minute input changes.\n• Challenge 3: Computational Resource Demands\nThe training of LLMs is a computationally intensive\nprocedure that requires substantial hardware and energy\nresources [179]. It is necessary to have access to su-\npercomputing clusters or specialized hardware in order\nto train large models, and the environmental impact\nof such resource-intensive training has raised concerns.\nSignificant energy consumption is associated with train-\ning LLMs at scale, contributing to the AI industry’s\noverall carbon footprint.\n• Challenge 4: Fine-Tuning Complexity\nWhile pre-training gives LLMs a broad comprehension\nof language, fine-tuning is required to adapt these mod-\nels to specific tasks [180]. Fine-tuning entails training\nthe model on a smaller dataset, frequently requiring\nhuman annotators to label examples. As it involves\nthe construction of task-specific datasets and extensive\nhuman intervention, this process can be both time-\nconsuming and costly.\n• Challenge 5: Real-Time Responsiveness\nThe remarkable training capabilities of LLMs come at\nthe expense of inference speed. Real-time response or\nprediction generation with these models can be slug-\ngish, limiting their applicability in applications such as\nchatbots or recommendation systems where low-latency\nresponses are crucial for user satisfaction.\n• Challenge 6: Contextual Constraints\nLLMs can only evaluate a limited number of preceding\ntokens when generating text due to their limited context\nwindow [181]. This limitation presents difficulties when\nworking with lengthy documents or having lengthy con-\nversations. Maintaining coherence and relevance over\nlengthy text sequences can be challenging because the\nmodel may neglect or lose track of pertinent informa-\ntion.\n• Challenge 7: Bias and Undesirable Output\nIn their output, LLMs can display biases or undesirable\ncharacteristics. This is due to the inherent biases in\nthe training data, which are assimilated by the model\nand reflected in its responses [182]. Such biases can\nmanifest as objectionable, discriminatory, or harmful\ncontent, making it imperative to address and mitigate\nthese concerns to ensure the responsible deployment of\nAI.\n• Challenge 8: Knowledge Temporality\nLLMs learn using historical data from the Internet, and\ntheir knowledge is restricted to what is available as of\na particular date. Consequently, they may lack access\nto the most recent information or events. This can be\nproblematic when users expect up-to-date responses or\nwhen the conversation involves recent events.\n• Challenge 9: Evaluation Complexity\nEvaluation of LLMs presents significant difficulties.\nMany extant evaluation metrics are insufficient to cap-\nture the nuances of model performance, which raises\nquestions about their efficacy. Additionally, these met-\nrics can be susceptible to manipulation or gaming,\nVOLUME 4, 2016 29\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nwhich may provide an inaccurate image of a model’s\ncapabilities. To assess LLMs’ actual performance and\nlimitations, robust and reliable evaluation methodolo-\ngies are required.\n• Challenge 10: Dynamic Evaluation Needs\nFrequently, evaluating LLMs entails comparing their\noutputs to static benchmarks or human-authored ground\ntruth. However, language is dynamic and evolves, and\npreset evaluation data may not adequately reflect a\nmodel’s adaptability to language and context change.\nThis difficulty underscores the need for evaluation\nframeworks that are more dynamic and continually up-\ndated.\nXI. FUTURE RESEARCH PROSPECTS ON LLMS\nIn the ever-evolving realm of Large Language Models\n(LLMs), several key research focuses and directions are\nemerging that promise to address and resolve the challenges\nand open issues discussed earlier. These endeavors will play\na pivotal role in harnessing the full potential of LLMs while\nensuring their responsible and ethical utilization in our dy-\nnamic AI landscape.\nEnhancing Bias Mitigation: Researchers are dedicated\nto refining training data to minimize bias, devising effective\ndebiasing techniques, and establishing guidelines for respon-\nsible AI development [183]. They are also focused on inte-\ngrating continuous monitoring and auditing mechanisms into\nAI pipelines, thereby conforming fairness and impartiality of\nthe system. This commitment to mitigating bias ensures that\nLLMs not only advance in capability but do so in a way that\nupholds ethical standards.\nEfficiency Optimization: A core concern driving research\nis the quest for more efficient training techniques. Re-\nsearchers are delving into innovative methods like federated\nlearning, which enables the distribution of training across\ndecentralized data sources [184]. They are also exploring\nknowledge distillation techniques for model compression and\nfinding ways to reduce the substantial computational and en-\nvironmental costs associated with LLMs. This optimization\npaves the way for more sustainable and resource-efficient AI\nmodels.\nDynamic Context Handling: LLMs are being endowed\nwith enhanced context management capabilities. This em-\npowers them to comprehend longer context windows and\nseamlessly handle extensive documents or conversations.\nSuch enhancements significantly expand their utility in vari-\nous applications and resolve previous limitations.\nContinuous Learning: To keep LLMs up-to-date, re-\nsearchers are focusing on developing techniques that enable\nthese models to adapt on evolving language and knowledge\nover time. This ensures that LLMs remain valuable and\naccurate sources of information and consistently overcoming\nchallenges of obsolescence.\nInterpretable AI: The research community is committed\nto making LLMs outputs more transparent and interpretable.\nThis fosters confidence and comprehension in AI decision-\nmaking processes, addressing the interpretability issues that\nhave been a concern [185].\nMultimodal LLMs: Researchers are pioneering the de-\nvelopment of LLMs that incorporate text, vision, and other\nmodalities [186]. These models can understand and generate\ntext from images, videos, and audio, creating new avenues\nfor AI applications and effectively addressing the need for\nmulti-sensory comprehension.\nHuman-AI Collaboration: Research on how humans and\nLLMs can collaborate effectively, with AI assisting and aug-\nmenting human tasks, is a crucial focal point. This collab-\noration bridges the gap between AI capabilities and human\nneeds, thereby resolving previous challenges and issues in\ndeployment.\nDynamic Evaluation Metrics and Relevant Bench-\nmarks: Researchers are working on dynamic evaluation\nmetrics that adapt to changing language and context, en-\nsuring that LLMs performance is accurately assessed [187].\nThis includes the development of relevant and up-to-date\nbenchmarks that address earlier shortcomings in assessing AI\ncapabilities.\nPersonalization and Customization: Techniques to cus-\ntomize LLMs interactions to individual user preferences and\nneeds are gaining prominence. This personalization boosts\nuser satisfaction and resolves issues related to one-size-fits-\nall AI interactions.\nEthical and Legal Frameworks: In response to evolving\nAI regulation, researchers are diligently developing ethical\nand legal regulatory frameworks. These frameworks serve\nas guiding principles for the responsible use of LLMs and\nensure compliance with data protection and privacy regula-\ntions, effectively addressing previous concerns about ethical\nAI deployment [188].\nThese forward-looking research directions stand as bea-\ncons of progress, poised to overcome challenges and open\nissues and ultimately lead to the maximization of LLMs\npotential while upholding the highest standards of account-\nability and ethics in our evolving AI landscape.\nXII. LIMITATIONS\nWhile conducting a thorough examination of LLMs, which\nincludes analyzing their application taxonomies, comparing\nconfigurations, and addressing concerns and obstacles, it is\nessential to recognize the existence of some limitations that\nshould be considered. A primary limitation of this study\nis the unavailability of review papers that directly relate\nto the topic of LLMs. Although we have made diligent\nattempts to address the available research thoroughly, the\nlimited quantity of papers in this field restricts our potential\nto perform broad comparisons and evaluations. However,\nwe have overcome the limitations of all the existing review\npapers and introduced many new aspects to give a compre-\nhensive overview of LLMs. While endeavoring to offer a\nbroad perspective on LLMs concepts, we recognize that this\nanalysis predominantly focuses on the ground-level concepts\nof LLMs configurations and applications. Limited resources,\n30 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\ntime, and page constraints affect the extensive exploration of\nindividual LLMs architectures. Although our goal is not to\noffer the comprehension of single LLMs but instead provide\nthe evolution of LLMs and its application around various\ndomains, however, readers looking for detailed analysis of\nspecific architectures and advanced topics are not thoroughly\ncovered. Furthermore, the impact of the LLMs across vari-\nous domains, including education, health, and economy, is\nhighlighted, but assessing the practical impacts of LLMs in\nmany domains can be complex and subjective, especially\nwhen considering their impact on social aspects.\nXIII. CONCLUSION\nThe field of LLMs has witnessed a remarkable evolution and\nexpansion, resulting in extraordinary capabilities in natural\nlanguage processing (NLP) and various applications in vari-\nous areas. Based on neural networks and the transformative\ntransformer architecture, these LLMs have revolutionized our\napproach to machine language comprehension and genera-\ntion. The thorough review of this research has provided an\ninsightful overview of LLMs, encompassing their historical\ndevelopment, architectural foundations, training methods,\nand vast advancement resources. It has also examined the\nvarious applications of LLMs in disciplines such as health-\ncare, education, social sciences, business, and agriculture,\ndemonstrating their potential to address real-world issues.\nIn addition, this review has delved into the societal effects\nof LLMs, discussing how they shape the future of AI and\ncan be utilized to address complex problems. However, it\nhas not shied away from addressing the pressing challenges\nand ethical considerations associated with deploying LLMs,\nincluding model biases, privacy concerns, and the need for\nenhanced robustness and controllability. As the field of LLMs\nresearch continues to evolve swiftly, this review is a valuable\nresource for practitioners, researchers, and experts seeking\na comprehensive understanding of LLMs’ past, present, and\nfuture. It emphasizes the significance of ongoing efforts to\nimprove the efficacy and dependability of LLMs, as well as\nthe need for ethical development and deployment practices.\nLLMs represent a pivotal advancement in AI and NLP, with\nthe potential to revolutionize a variety of domains and solve\ncomplex problems. This article provides a comprehensive\nfoundation for future research and development in Large\nLanguage Models’ dynamic and thrilling field.\nREFERENCES\n[1] S. Pinker, The language instinct: How the mind creates language. Pen-\nguin uK, 2003.\n[2] M. D. Hauser, N. Chomsky, and W. T. Fitch, “The faculty of language:\nwhat is it, who has it, and how did it evolve?” science, vol. 298, no. 5598,\npp. 1569–1579, 2002.\n[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\nJ. Zhang, Z. Dong et al., “A survey of large language models,” arXiv\npreprint arXiv:2303.18223, 2023.\n[4] I. Turing, “Computing machinery and intelligence-am turing,” Mind,\nvol. 59, no. 236, p. 433, 2007.\n[5] Y . Shen, L. Heacock, J. Elias, K. D. Hentel, B. Reig, G. Shih, and L. Moy,\n“Chatgpt and other large language models are double-edged swords,” p.\ne230163, 2023.\n[6] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[7] M. A. K. Raiaan, K. Fatema, I. U. Khan, S. Azam, M. R. ur Rashid,\nM. S. H. Mukta, M. Jonkman, and F. De Boer, “A lightweight robust\ndeep learning model gained high accuracy in classifying a wide range of\ndiabetic retinopathy images,” IEEE Access, 2023.\n[8] B. Ramabhadran, S. Khudanpur, and E. Arisoy, “Proceedings of the\nnaacl-hlt 2012 workshop: Will we ever really replace the n-gram model?\non the future of language modeling for hlt,” in Proceedings of the\nNAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram\nModel? On the Future of Language Modeling for HLT, 2012.\n[9] T. Mikolov, M. Karafiát, L. Burget, J. Cernock `y, and S. Khudanpur,\n“Recurrent neural network based language model.” in Interspeech, vol. 2,\nno. 3. Makuhari, 2010, pp. 1045–1048.\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[12] Y . Khare, V . Bagal, M. Mathew, A. Devi, U. D. Priyakumar, and C. Jawa-\nhar, “Mmbert: Multimodal bert pretraining for improved medical vqa,”\nin 2021 IEEE 18th International Symposium on Biomedical Imaging\n(ISBI). IEEE, 2021, pp. 1033–1036.\n[13] R. Liu, C. Jia, J. Wei, G. Xu, L. Wang, and S. V osoughi, “Mitigating\npolitical bias in language models through reinforced calibration,” in\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 35,\nno. 17, 2021, pp. 14 857–14 866.\n[14] K. Sanderson, “Gpt-4 is here: what scientists think,” Nature, vol. 615, no.\n7954, p. 773, 2023.\n[15] S. Pichai, “An important next step on our ai journey, feb 2023,” URL\nhttps://blog. google/technology/ai/bard-google-ai-search-updates, vol. 2,\nno. 9, 2023.\n[16] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin,\nP. Liang, and T. B. Hashimoto, “Alpaca: A strong, replicable instruction-\nfollowing model,” Stanford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6, p. 7,\n2023.\n[17] J. Huang and K. C.-C. Chang, “Towards reasoning in large language\nmodels: A survey,” arXiv preprint arXiv:2212.10403, 2022.\n[18] L. Fan, L. Li, Z. Ma, S. Lee, H. Yu, and L. Hemphill, “A bibliometric\nreview of large language models research from 2017 to 2023,” arXiv\npreprint arXiv:2304.02020, 2023.\n[19] Y . Chang, X. Wang, J. Wang, Y . Wu, K. Zhu, H. Chen, L. Yang, X. Yi,\nC. Wang, Y . Wang et al., “A survey on evaluation of large language\nmodels,” arXiv preprint arXiv:2307.03109, 2023.\n[20] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\nF. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier et al.,\n“Chatgpt for good? on opportunities and challenges of large language\nmodels for education,” Learning and individual differences, vol. 103, p.\n102274, 2023.\n[21] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. Shaikh,\nN. Akhtar, J. Wu, and S. Mirjalili, “A survey on large language models:\nApplications, challenges, limitations, and practical usage,” TechRxiv,\n2023.\n[22] B. Cronin, “Annual review of information science and technology,” 2004.\n[23] M. Kardum, “Rudolf carnap–the grandfather of artificial neural networks:\nThe influence of carnap’s philosophy on walter pitts,” Guide to Deep\nLearning Basics: Logical, Historical and Philosophical Perspectives, pp.\n55–66, 2020.\n[24] G. Leech, “Corpora and theories of linguistic performance,” Svartvik, J.\nDirections in Corpus Linguistics, pp. 105–22, 1992.\n[25] E. D. Liddy, “Natural language processing,” 2001.\n[26] B.-H. Juang and L. R. Rabiner, “Automatic speech recognition–a brief\nhistory of the technology development,” Georgia Institute of Technology.\nAtlanta Rutgers University and the University of California. Santa Bar-\nbara, vol. 1, p. 67, 2005.\n[27] D. S. Hain, R. Jurowetzki, T. Buchmann, and P. Wolf, “A text-embedding-\nbased approach to measuring patent-to-patent technological similarity,”\nTechnological Forecasting and Social Change, vol. 177, p. 121559, 2022.\n[28] G. Curto, M. F. Jojoa Acosta, F. Comim, and B. Garcia-Zapirain, “Are\nai systems biased against the poor? a machine learning analysis using\nword2vec and glove embeddings,” AI & society, pp. 1–16, 2022.\nVOLUME 4, 2016 31\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\n[29] P. Azunre, Transfer learning for natural language processing. Simon and\nSchuster, 2021.\n[30] Y . Shi, M. Larson, and C. M. Jonker, “Recurrent neural network lan-\nguage model adaptation with curriculum learning,” Computer Speech &\nLanguage, vol. 33, no. 1, pp. 136–154, 2015.\n[31] A. Kova ˇcevi´c and D. Ke ˇco, “Bidirectional lstm networks for abstractive\ntext summarization,” in Advanced Technologies, Systems, and Applica-\ntions VI: Proceedings of the International Symposium on Innovative and\nInterdisciplinary Applications of Advanced Technologies (IAT) 2021.\nSpringer, 2022, pp. 281–293.\n[32] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao, K. Macherey et al., “Google’s neural ma-\nchine translation system: Bridging the gap between human and machine\ntranslation,” arXiv preprint arXiv:1609.08144, 2016.\n[33] R. K. Yadav, S. Harwani, S. K. Maurya, and S. Kumar, “Intelligent chat-\nbot using gnmt, seq-2-seq techniques,” in 2021 International Conference\non Intelligent Technologies (CONIT). IEEE, 2021, pp. 1–5.\n[34] D. Luitse and W. Denkena, “The great transformer: Examining the role\nof large language models in the political economy of ai,” Big Data &\nSociety, vol. 8, no. 2, p. 20539517211047734, 2021.\n[35] M. O. Topal, A. Bas, and I. van Heerden, “Exploring transformers\nin natural language generation: Gpt, bert, and xlnet,” arXiv preprint\narXiv:2102.08036, 2021.\n[36] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz et al., “Transformers: State-of-the-art\nnatural language processing,” in Proceedings of the 2020 conference on\nempirical methods in natural language processing: system demonstra-\ntions, 2020, pp. 38–45.\n[37] C. Sur, “Rbn: enhancement in language attribute prediction using global\nrepresentation of natural language transfer learning technology like\ngoogle bert,” SN Applied Sciences, vol. 2, no. 1, p. 22, 2020.\n[38] J. J. Bird, A. Ekárt, and D. R. Faria, “Chatbot interaction with artificial\nintelligence: human data augmentation with t5 and language transformer\nensemble for text classification,” Journal of Ambient Intelligence and\nHumanized Computing, vol. 14, no. 4, pp. 3129–3144, 2023.\n[39] B. D. Lund and T. Wang, “Chatting about chatgpt: how may ai and gpt\nimpact academia and libraries?” Library Hi Tech News, vol. 40, no. 3,\npp. 26–29, 2023.\n[40] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improving\nlanguage understanding by generative pre-training,” 2018.\n[41] B. Ghojogh and A. Ghodsi, “Attention mechanism, transformers, bert,\nand gpt: tutorial and survey,” 2020.\n[42] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[43] K. Abramski, S. Citraro, L. Lombardi, G. Rossetti, and M. Stella,\n“Cognitive network science reveals bias in gpt-3, gpt-3.5 turbo, and gpt-4\nmirroring math anxiety in high-school students,” Big Data and Cognitive\nComputing, vol. 7, no. 3, p. 124, 2023.\n[44] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-\nzaro, “Megatron-lm: Training multi-billion parameter language models\nusing model parallelism,” arXiv preprint arXiv:1909.08053, 2019.\n[45] D. M. Katz, M. J. Bommarito, S. Gao, and P. Arredondo, “Gpt-4 passes\nthe bar exam,” Available at SSRN 4389233, 2023.\n[46] M. J. Page, J. E. McKenzie, P. M. Bossuyt, I. Boutron, T. C. Hoffmann,\nC. D. Mulrow, L. Shamseer, J. M. Tetzlaff, E. A. Akl, S. E. Brennan\net al., “The prisma 2020 statement: an updated guideline for reporting\nsystematic reviews,” International journal of surgery, vol. 88, p. 105906,\n2021.\n[47] J. J. Webster and C. Kit, “Tokenization as the initial phase in nlp,” in\nCOLING 1992 volume 4: The 14th international conference on compu-\ntational linguistics, 1992.\n[48] T. Kudo, “Subword regularization: Improving neural network trans-\nlation models with multiple subword candidates,” arXiv preprint\narXiv:1804.10959, 2018.\n[49] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of\nrare words with subword units,” arXiv preprint arXiv:1508.07909, 2015.\n[50] M. Schuster and K. Nakajima, “Japanese and korean voice search,”\nin 2012 IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP). IEEE, 2012, pp. 5149–5152.\n[51] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long\nsequences with sparse transformers,” arXiv preprint arXiv:1904.10509,\n2019.\n[52] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward\nnetworks are universal approximators,” Neural networks, vol. 2, no. 5,\npp. 359–366, 1989.\n[53] V . Nair and G. E. Hinton, “Rectified linear units improve restricted boltz-\nmann machines,” in Proceedings of the 27th international conference on\nmachine learning (ICML-10), 2010, pp. 807–814.\n[54] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv\npreprint arXiv:1606.08415, 2016.\n[55] B. Zhang and R. Sennrich, “Root mean square layer normalization,”\nAdvances in Neural Information Processing Systems, vol. 32, 2019.\n[56] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow, R. Castagné,\nA. S. Luccioni, F. Yvon, M. Gallé et al., “Bloom: A 176b-parameter open-\naccess multilingual language model,” arXiv preprint arXiv:2211.05100,\n2022.\n[57] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for\nparameter-efficient prompt tuning,” arXiv preprint arXiv:2104.08691,\n2021.\n[58] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for\ngeneration,” arXiv preprint arXiv:2101.00190, 2021.\n[59] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li,\nX. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-finetuned\nlanguage models,” arXiv preprint arXiv:2210.11416, 2022.\n[60] I. U. Khan, M. A. K. Raiaan, K. Fatema, S. Azam, R. u. Rashid, S. H.\nMukta, M. Jonkman, and F. De Boer, “A computer-aided diagnostic\nsystem to identify diabetic retinopathy, utilizing a modified compact con-\nvolutional transformer and low-resolution images to reduce computation\ntime,” Biomedicines, vol. 11, no. 6, p. 1566, 2023.\n[61] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural\nlanguage processing,” ACM Computing Surveys, vol. 55, no. 9, pp. 1–35,\n2023.\n[62] W. Ansar, S. Goswami, A. Chakrabarti, and B. Chakraborty, “A novel\nselective learning based transformer encoder architecture with enhanced\nword representation,” Applied Intelligence, vol. 53, no. 8, pp. 9424–9443,\n2023.\n[63] G. Dar, M. Geva, A. Gupta, and J. Berant, “Analyzing transformers in\nembedding space,” arXiv preprint arXiv:2209.02535, 2022.\n[64] D. Hazarika, M. Namazifar, and D. Hakkani-Tür, “Attention biasing and\ncontext augmentation for zero-shot control of encoder-decoder trans-\nformers for natural language generation,” in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 738–\n10 748.\n[65] J. Lu, J. Yao, J. Zhang, X. Zhu, H. Xu, W. Gao, C. Xu, T. Xiang,\nand L. Zhang, “Soft: Softmax-free transformer with linear complexity,”\nAdvances in Neural Information Processing Systems, vol. 34, pp. 21 297–\n21 309, 2021.\n[66] L. Floridi and M. Chiriatti, “Gpt-3: Its nature, scope, limits, and conse-\nquences,” Minds and Machines, vol. 30, pp. 681–694, 2020.\n[67] X. Zheng, C. Zhang, and P. C. Woodland, “Adapting gpt, gpt-2 and\nbert language models for speech recognition,” in 2021 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU). IEEE,\n2021, pp. 162–168.\n[68] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n[69] A. Roberts, C. Raffel, and N. Shazeer, “How much knowledge can\nyou pack into the parameters of a language model?” arXiv preprint\narXiv:2002.08910, 2020.\n[70] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., “Palm: Scaling\nlanguage modeling with pathways,” arXiv preprint arXiv:2204.02311,\n2022.\n[71] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\nCheng, A. Jin, T. Bos, L. Baker, Y . Du et al., “Lamda: Language models\nfor dialog applications,” arXiv preprint arXiv:2201.08239, 2022.\n[72] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\nW. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trained\nmodel,” arXiv preprint arXiv:2210.02414, 2022.\n[73] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\nmodels: Methods, analysis & insights from training gopher,” arXiv\npreprint arXiv:2112.11446, 2021.\n[74] O. Lieber, O. Sharir, B. Lenz, and Y . Shoham, “Jurassic-1: Technical\ndetails and evaluation,” White Paper. AI21 Labs, vol. 1, 2021.\n32 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\n[75] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper,\nZ. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti et al., “Using deep-\nspeed and megatron to train megatron-turing nlg 530b, a large-scale\ngenerative language model,” arXiv preprint arXiv:2201.11990, 2022.\n[76] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al.,\n“Llama: Open and efficient foundation language models,” arXiv preprint\narXiv:2302.13971, 2023.\n[77] T. T. Nguyen, C. Wilson, and J. Dalins, “Fine-tuning llama 2 large\nlanguage models for detecting online sexual predatory chats and abusive\ntexts,” arXiv preprint arXiv:2308.14683, 2023.\n[78] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobei-\ndli, B. Pannier, E. Almazrouei, and J. Launay, “The refinedweb dataset for\nfalcon llm: outperforming curated corpora with web data, and web data\nonly,” arXiv preprint arXiv:2306.01116, 2023.\n[79] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Ruther-\nford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark et al.,\n“Training compute-optimal large language models,” arXiv preprint\narXiv:2203.15556, 2022.\n[80] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\nM. Diab, X. Li, X. V . Lin et al., “Opt: Open pre-trained transformer\nlanguage models,” arXiv preprint arXiv:2205.01068, 2022.\n[81] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in Neural Information Processing Systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[82] W. Zeng, X. Ren, T. Su, H. Wang, Y . Liao, Z. Wang, X. Jiang, Z. Yang,\nK. Wang, X. Zhang et al., “Pangu: Large-scale autoregressive pretrained\nchinese language models with auto-parallel computation,” arXiv preprint\narXiv:2104.12369, 2021.\n[83] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman,\nN. Barnes, and A. Mian, “A comprehensive overview of large language\nmodels,” arXiv preprint arXiv:2307.06435, 2023.\n[84] G. Jawahar, B. Sagot, and D. Seddah, “What does bert learn about\nthe structure of language?” in ACL 2019-57th Annual Meeting of the\nAssociation for Computational Linguistics, 2019.\n[85] J. Ni, G. H. Ábrego, N. Constant, J. Ma, K. B. Hall, D. Cer, and Y . Yang,\n“Sentence-t5: Scalable sentence encoders from pre-trained text-to-text\nmodels,” arXiv preprint arXiv:2108.08877, 2021.\n[86] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al., “Competition-level\ncode generation with alphacode,” Science, vol. 378, no. 6624, pp. 1092–\n1097, 2022.\n[87] Y . Koizumi, Y . Ohishi, D. Niizumi, D. Takeuchi, and M. Yasuda, “Audio\ncaptioning using pre-trained large-scale language model guided by audio-\nbased similar caption retrieval,” arXiv preprint arXiv:2012.07331, 2020.\n[88] W. Fan, Z. Zhao, J. Li, Y . Liu, X. Mei, Y . Wang, J. Tang, and Q. Li,\n“Recommender systems in the era of large language models (llms),”\narXiv preprint arXiv:2307.02046, 2023.\n[89] Y . Bai, J. Yi, J. Tao, Z. Tian, Z. Wen, and S. Zhang, “Fast end-to-\nend speech recognition via non-autoregressive models and cross-modal\nknowledge transferring from bert,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 29, pp. 1897–1911, 2021.\n[90] C. Sun, J. Li, Y . R. Fung, H. P. Chan, T. Abdelzaher, C. Zhai, and\nH. Ji, “Decoding the silent majority: Inducing belief augmented social\ngraph with large language model for response forecasting,” arXiv preprint\narXiv:2310.13297, 2023.\n[91] K. Drossos, S. Gharib, P. Magron, and T. Virtanen, “Language modelling\nfor sound event detection with teacher forcing and scheduled sampling,”\narXiv preprint arXiv:1907.08506, 2019.\n[92] S.-H. Chiu and B. Chen, “Innovative bert-based reranking language mod-\nels for speech recognition,” in 2021 IEEE Spoken Language Technology\nWorkshop (SLT). IEEE, 2021, pp. 266–271.\n[93] A. Elhafsi, R. Sinha, C. Agia, E. Schmerling, I. A. Nesnas, and\nM. Pavone, “Semantic anomaly detection with large language models,”\nAutonomous Robots, pp. 1–21, 2023.\n[94] Y . Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, and K. B.\nLetaief, “Large language models empowered autonomous edge ai for\nconnected intelligence,” arXiv preprint arXiv:2307.02779, 2023.\n[95] H. Abdel-Jaber, D. Devassy, A. Al Salam, L. Hidaytallah, and M. El-\nAmir, “A review of deep learning algorithms and their applications in\nhealthcare,” Algorithms, vol. 15, no. 2, p. 71, 2022.\n[96] B. Peng, C. Li, P. He, M. Galley, and J. Gao, “Instruction tuning with\ngpt-4,” arXiv preprint arXiv:2304.03277, 2023.\n[97] J. Vig and Y . Belinkov, “Analyzing the structure of attention in a trans-\nformer language model,” arXiv preprint arXiv:1906.04284, 2019.\n[98] A. McGowan, Y . Gui, M. Dobbs, S. Shuster, M. Cotter, A. Selloni,\nM. Goodman, A. Srivastava, G. A. Cecchi, and C. M. Corcoran, “Chat-\ngpt and bard exhibit spontaneous citation fabrication during psychiatry\nliterature search,” Psychiatry Research, vol. 326, p. 115334, 2023.\n[99] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\nA. Poulton, V . Kerkez, and R. Stojnic, “Galactica: A large language\nmodel for science,” arXiv preprint arXiv:2211.09085, 2022.\n[100] N. Shazeer, “Glu variants improve transformer,” arXiv preprint\narXiv:2002.05202, 2020.\n[101] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\nY . Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of language\nmodels with mixture-of-experts,” in International Conference on Ma-\nchine Learning. PMLR, 2022, pp. 5547–5569.\n[102] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Gold-\ning, H. He, C. Leahy, K. McDonell, J. Phang et al., “Gpt-neox-\n20b: An open-source autoregressive language model,” arXiv preprint\narXiv:2204.06745, 2022.\n[103] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,\nand C. Xiong, “Codegen: An open large language model for code with\nmulti-turn program synthesis,” arXiv preprint arXiv:2203.13474, 2022.\n[104] T. Hagendorff, S. Fabi, and M. Kosinski, “Machine intuition: Uncov-\nering human-like intuitive decision-making in gpt-3.5,” arXiv preprint\narXiv:2212.05206, 2022.\n[105] P. P. Ray, “Chatgpt: A comprehensive review on background, applica-\ntions, key challenges, bias, ethics, limitations and future scope,” Internet\nof Things and Cyber-Physical Systems, 2023.\n[106] X.-Q. Dao, “Performance comparison of large language models on\nvnhsge english dataset: Openai chatgpt, microsoft bing chat, and google\nbard,” arXiv preprint arXiv:2307.02288, 2023.\n[107] D. Kelly, Y . Chen, S. E. Cornwell, N. S. Delellis, A. Mayhew, S. Onao-\nlapo, and V . L. Rubin, “Bing chat: The future of search engines?”\nProceedings of the Association for Information Science and Technology,\nvol. 60, no. 1, pp. 1007–1009, 2023.\n[108] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V .\nLe, “Xlnet: Generalized autoregressive pretraining for language under-\nstanding,” Advances in neural information processing systems, vol. 32,\n2019.\n[109] X. Song, G. Wang, Z. Wu, Y . Huang, D. Su, D. Yu, and H. Meng,\n“Speech-xlnet: Unsupervised acoustic model pretraining for self-\nattention networks,” arXiv preprint arXiv:1910.10387, 2019.\n[110] W. Shen, J. Chen, X. Quan, and Z. Xie, “Dialogxl: All-in-one xlnet\nfor multi-party conversation emotion recognition,” in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 35, no. 15, 2021, pp.\n13 789–13 797.\n[111] R. Luo, L. Sun, Y . Xia, T. Qin, S. Zhang, H. Poon, and T.-Y . Liu, “Biogpt:\ngenerative pre-trained transformer for biomedical text generation and\nmining,” Briefings in Bioinformatics, vol. 23, no. 6, p. bbac409, 2022.\n[112] D. Deutsch, J. Juraska, M. Finkelstein et al., “Training and meta-\nevaluating machine translation evaluation metrics at the paragraph level,”\narXiv preprint arXiv:2308.13506, 2023.\n[113] A. Ushio, F. Alva-Manchego, and J. Camacho-Collados, “Generative\nlanguage models for paragraph-level question generation,” arXiv preprint\narXiv:2210.03992, 2022.\n[114] openai, “openai,” 2023, accessed Sep 12, 2023. [Online]. Available:\nhttps://openai.com/blog/openai-api\n[115] huggingface, “huggingface,” 2023, accessed Sep 12, 2023. [Online].\nAvailable: https://huggingface.co/docs/transformers/index\n[116] Google Cloud. (2023) Cloud natural language. Accessed Sep 12, 2023.\n[Online]. Available: https://cloud.google.com/natural-language\n[117] azure, “azure,” 2023, accessed Sep 12, 2023. [Online]. Available:\nhttps://azure.microsoft.com/en-us/products/ai-services/ai-language\n[118] IBM. (2023) Ibm watson natural language understanding. Accessed\nSep 12, 2023. [Online]. Available: https://www.ibm.com/products/\nnatural-language-understanding\n[119] G. Satyanarayana, J. Bhuvana, and M. Balamurugan, “Sentimental anal-\nysis on voice using aws comprehend,” in 2020 International Conference\non Computer Communication and Informatics (ICCCI). IEEE, 2020,\npp. 1–4.\n[120] A. Kolides, A. Nawaz, A. Rathor, D. Beeman, M. Hashmi, S. Fatima,\nD. Berdik, M. Al-Ayyoub, and Y . Jararweh, “Artificial intelligence foun-\ndation and pre-trained models: Fundamentals, applications, opportuni-\nVOLUME 4, 2016 33\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nties, and social impacts,” Simulation Modelling Practice and Theory, vol.\n126, p. 102754, 2023.\n[121] S. M. Jain, “Hugging face,” in Introduction to Transformers for NLP:\nWith the Hugging Face Library and Models to Solve Problems. Springer,\n2022, pp. 51–67.\n[122] J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y . Wang, L. Yang,\nH. Huang, W. Ye, X. Geng et al., “On the robustness of chat-\ngpt: An adversarial and out-of-distribution perspective,” arXiv preprint\narXiv:2302.12095, 2023.\n[123] S. Chen, Y . Li, S. Lu, H. Van, H. J. Aerts, G. K. Savova, and D. S. Bitter-\nman, “Evaluation of chatgpt family of models for biomedical reasoning\nand classification,” arXiv preprint arXiv:2304.02496, 2023.\n[124] H. Huang, O. Zheng, D. Wang, J. Yin, Z. Wang, S. Ding, H. Yin, C. Xu,\nR. Yang, Q. Zheng et al., “Chatgpt for shaping the future of dentistry: the\npotential of multi-modal large language model,” International Journal of\nOral Science, vol. 15, no. 1, p. 29, 2023.\n[125] V . Sorin, E. Klang, M. Sklair-Levy, I. Cohen, D. B. Zippel, N. Balint La-\nhat, E. Konen, and Y . Barash, “Large language model (chatgpt) as a\nsupport tool for breast tumor board,” NPJ Breast Cancer, vol. 9, no. 1,\np. 44, 2023.\n[126] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F.\nTan, and D. S. W. Ting, “Large language models in medicine,” Nature\nmedicine, pp. 1–11, 2023.\n[127] D. M. Korngiebel and S. D. Mooney, “Considering the possibilities\nand pitfalls of generative pre-trained transformer 3 (gpt-3) in healthcare\ndelivery,” NPJ Digital Medicine, vol. 4, no. 1, p. 93, 2021.\n[128] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\nTozzi, and C. Rizzo, “Chatgpt and the rise of large language models:\nthe new ai-driven infodemic threat in public health,” Frontiers in Public\nHealth, vol. 11, p. 1166120, 2023.\n[129] M. Sallam, “Chatgpt utility in healthcare education, research, and prac-\ntice: systematic review on the promising perspectives and valid con-\ncerns,” in Healthcare, vol. 11, no. 6. MDPI, 2023, p. 887.\n[130] M. Cascella, J. Montomoli, V . Bellini, and E. Bignami, “Evaluating the\nfeasibility of chatgpt in healthcare: an analysis of multiple clinical and\nresearch scenarios,” Journal of Medical Systems, vol. 47, no. 1, p. 33,\n2023.\n[131] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon,\nC. Elepaño, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo\net al., “Performance of chatgpt on usmle: Potential for ai-assisted medical\neducation using large language models,” PLoS digital health, vol. 2, no. 2,\np. e0000198, 2023.\n[132] Y . Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann,\nJ. Gao, and H. Poon, “Domain-specific language model pretraining for\nbiomedical natural language processing,” ACM Transactions on Com-\nputing for Healthcare (HEALTH), vol. 3, no. 1, pp. 1–23, 2021.\n[133] Z. Kraljevic, D. Bean, A. Shek, R. Bendayan, H. Hemingway, and\nJ. Au, “Foresight-generative pretrained transformer (gpt) for modelling\nof patient timelines using ehrs.”\n[134] L. Mich and R. Garigliano, “Chatgpt for e-tourism: a technological\nperspective,” Information Technology & Tourism, pp. 1–12, 2023.\n[135] X. Yu, Z. Chen, Y . Ling, S. Dong, Z. Liu, and Y . Lu, “Temporal data\nmeets llm–explainable financial time series forecasting,” arXiv preprint\narXiv:2306.11025, 2023.\n[136] G. F. Frederico, “Chatgpt in supply chains: Initial evidence of applica-\ntions and potential research agenda,” Logistics, vol. 7, no. 2, p. 26, 2023.\n[137] A. Sobieszek and T. Price, “Playing games with ais: the limits of gpt-3\nand similar large language models,” Minds and Machines, vol. 32, no. 2,\npp. 341–364, 2022.\n[138] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\nN. Akhtar, J. Wu, S. Mirjalili et al., “Large language models: A com-\nprehensive survey of its applications, challenges, limitations, and future\nprospects,” 2023.\n[139] C. K. Lo, “What is the impact of chatgpt on education? a rapid review of\nthe literature,” Education Sciences, vol. 13, no. 4, p. 410, 2023.\n[140] Y . K. Dwivedi, N. Kshetri, L. Hughes, E. L. Slade, A. Jeyaraj, A. K.\nKar, A. M. Baabdullah, A. Koohang, V . Raghavan, M. Ahuja et al., ““so\nwhat if chatgpt wrote it?” multidisciplinary perspectives on opportunities,\nchallenges and implications of generative conversational ai for research,\npractice and policy,” International Journal of Information Management,\nvol. 71, p. 102642, 2023.\n[141] M. Zong and B. Krishnamachari, “A survey on gpt-3,” arXiv preprint\narXiv:2212.00857, 2022.\n[142] R. Zhu, X. Tu, and J. X. Huang, “Utilizing bert for biomedical and clinical\ntext mining,” in Data analytics in biomedical engineering and healthcare.\nElsevier, 2021, pp. 73–103.\n[143] K. Huang, A. Singh, S. Chen, E. T. Moseley, C.-Y . Deng, N. George,\nand C. Lindvall, “Clinical xlnet: modeling sequential clinical notes\nand predicting prolonged mechanical ventilation,” arXiv preprint\narXiv:1912.11975, 2019.\n[144] J. Zhang, L. Wang, R. K.-W. Lee, Y . Bin, Y . Wang, J. Shao, and E.-P. Lim,\n“Graph-to-tree learning for solving math word problems,” in Proceedings\nof the 58th Annual Meeting of the Association for Computational Lin-\nguistics, 2020, pp. 3928–3937.\n[145] X. Dai, S. Karimi, B. Hachey, and C. Paris, “Cost-effective selection of\npretraining data: A case study of pretraining bert on social media,” arXiv\npreprint arXiv:2010.01150, 2020.\n[146] S. Biswas, “The function of chat gpt in social media: According to chat\ngpt,” Available at SSRN 4405389, 2023.\n[147] R. Peng, K. Liu, P. Yang, Z. Yuan, and S. Li, “Embedding-based retrieval\nwith llm for effective agriculture information extracting from unstruc-\ntured data,” arXiv preprint arXiv:2308.03107, 2023.\n[148] S. Biswas, “Importance of chat gpt in agriculture: According to chat gpt,”\nAvailable at SSRN 4405391, 2023.\n[149] M. A. K. Raiaan, N. M. Fahad, S. Chowdhury, D. Sutradhar, S. S. Mihad,\nand M. M. Islam, “Iot-based object-detection system to safeguard en-\ndangered animals and bolster agricultural farm security,” Future Internet,\nvol. 15, no. 12, p. 372, 2023.\n[150] T. Pires, E. Schlinger, and D. Garrette, “How multilingual is multilingual\nbert?” arXiv preprint arXiv:1906.01502, 2019.\n[151] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov, “Un-\nsupervised cross-lingual representation learning at scale,” arXiv preprint\narXiv:1911.02116, 2019.\n[152] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,” arXiv preprint\narXiv:1910.01108, 2019.\n[153] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“Albert: A lite bert for self-supervised learning of language representa-\ntions,” arXiv preprint arXiv:1909.11942, 2019.\n[154] T. Bolukbasi, K.-W. Chang, J. Y . Zou, V . Saligrama, and A. T. Kalai,\n“Man is to computer programmer as woman is to homemaker? debiasing\nword embeddings,” Advances in neural information processing systems,\nvol. 29, 2016.\n[155] Y . Zhang, S. Sun, M. Galley, Y .-C. Chen, C. Brockett, X. Gao, J. Gao,\nJ. Liu, and B. Dolan, “Dialogpt: Large-scale generative pre-training for\nconversational response generation,” arXiv preprint arXiv:1911.00536,\n2019.\n[156] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” Advances in neural information processing sys-\ntems, vol. 33, pp. 1877–1901, 2020.\n[157] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert:\na pre-trained biomedical language representation model for biomedical\ntext mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020.\n[158] Y . Gat, N. Calderon, A. Feder, A. Chapanin, A. Sharma, and R. Reichart,\n“Faithful explanations of black-box nlp models using llm-generated\ncounterfactuals,” arXiv preprint arXiv:2310.00603, 2023.\n[159] M. Josifoski, M. Sakota, M. Peyrard, and R. West, “Exploiting asym-\nmetry for synthetic training data generation: Synthie and the case of\ninformation extraction,” arXiv preprint arXiv:2303.04132, 2023.\n[160] M. S. H. Mukta, J. Ahmad, M. A. K. Raiaan, S. Islam, S. Azam, M. E.\nAli, and M. Jonkman, “An investigation of the effectiveness of deepfake\nmodels and tools,” Journal of Sensor and Actuator Networks, vol. 12,\nno. 4, p. 61, 2023.\n[161] A. Awasthi, N. Gupta, B. Samanta, S. Dave, S. Sarawagi, and P. Taluk-\ndar, “Bootstrapping multilingual semantic parsers using large language\nmodels,” arXiv preprint arXiv:2210.07313, 2022.\n[162] P. Sridhar, A. Doyle, A. Agarwal, C. Bogart, J. Savelka, and M. Sakr,\n“Harnessing llms in curricular design: Using gpt-4 to support authoring\nof learning objectives,” arXiv preprint arXiv:2306.17459, 2023.\n[163] M. A. K. Raiaan, A. Al Mamun, M. A. Islam, M. E. Ali, and M. S. H.\nMukta, “Envy prediction from users’ photos using convolutional neural\nnetworks,” in 2023 International Conference on Computer, Electrical &\nCommunication Engineering (ICCECE). IEEE, 2023, pp. 1–7.\n34 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Survey on Large Language Models\n[164] E. Waisberg, J. Ong, M. Masalkhi, and A. G. Lee, “Large language model\n(llm)-driven chatbots for neuro-ophthalmic medical education,” Eye, pp.\n1–3, 2023.\n[165] W. Channell, “Making a difference: The role of the llm in policy formu-\nlation and reform,” in The Export of Legal Education. Routledge, 2016,\npp. 13–21.\n[166] P. Budhwar, S. Chowdhury, G. Wood, H. Aguinis, G. J. Bamber, J. R.\nBeltran, P. Boselie, F. Lee Cooke, S. Decker, A. DeNisi et al., “Human\nresource management in the age of generative artificial intelligence:\nPerspectives and research directions on chatgpt,” Human Resource Man-\nagement Journal, vol. 33, no. 3, pp. 606–659, 2023.\n[167] G. Fatouros, J. Soldatos, K. Kouroumali, G. Makridis, and D. Kyriazis,\n“Transforming sentiment analysis in the financial domain with chatgpt,”\nMachine Learning with Applications, vol. 14, p. 100508, 2023.\n[168] Y . Li, S. Ma, X. Wang, S. Huang, C. Jiang, H.-T. Zheng, P. Xie, F. Huang,\nand Y . Jiang, “Ecomgpt: Instruction-tuning large language model with\nchain-of-task tasks for e-commerce,” arXiv preprint arXiv:2308.06966,\n2023.\n[169] P. Weingart, T. Wambsganss, and M. Soellner, “A taxonomy for deriving\nbusiness insights from user-generated content,” 2023.\n[170] L. Zhu, X. Xu, Q. Lu, G. Governatori, and J. Whittle, “Ai and\nethics—operationalizing responsible ai,” Humanity Driven AI: Produc-\ntivity, Well-being, Sustainability and Partnership, pp. 15–33, 2022.\n[171] I. Molenaar, S. de Mooij, R. Azevedo, M. Bannertd, S. Järveläe, and\nD. Gaševi´cf, “Measuring self-regulated learning and the role of ai: Five\nyears of research using multimodal multichannel data,” Computers in\nHuman Behavior, p. 107540, 2022.\n[172] M. C. Rillig, M. Ågerstrand, M. Bi, K. A. Gould, and U. Sauerland,\n“Risks and benefits of large language models for the environment,”\nEnvironmental Science & Technology, vol. 57, no. 9, pp. 3464–3466,\n2023.\n[173] B. Liu, B. Xiao, X. Jiang, S. Cen, X. He, W. Dou et al., “Adversarial\nattacks on large language model-based system and mitigating strategies:\nA case study on chatgpt,” Security and Communication Networks, vol.\n2023, 2023.\n[174] Z. Sun, “A short survey of viewing large language models in legal aspect,”\narXiv preprint arXiv:2303.09136, 2023.\n[175] Y . Meng, M. Michalski, J. Huang, Y . Zhang, T. Abdelzaher, and J. Han,\n“Tuning language models as training data generators for augmentation-\nenhanced few-shot learning,” in International Conference on Machine\nLearning. PMLR, 2023, pp. 24 457–24 477.\n[176] S. Fincke, S. Agarwal, S. Miller, and E. Boschee, “Language model\npriming for cross-lingual event extraction,” in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 627–\n10 635.\n[177] N. M. Fahad, S. Sakib, M. A. K. Raiaan, and M. S. H. Mukta, “Skinnet-8:\nAn efficient cnn architecture for classifying skin cancer on an imbalanced\ndataset,” in 2023 International Conference on Electrical, Computer and\nCommunication Engineering (ECCE). IEEE, 2023, pp. 1–6.\n[178] N. Jain, K. Saifullah, Y . Wen, J. Kirchenbauer, M. Shu, A. Saha, M. Gold-\nblum, J. Geiping, and T. Goldstein, “Bring your own data! self-supervised\nevaluation for large language models,” arXiv preprint arXiv:2306.13651,\n2023.\n[179] X. Zhu, J. Li, Y . Liu, C. Ma, and W. Wang, “A survey on model com-\npression for large language models,” arXiv preprint arXiv:2308.07633,\n2023.\n[180] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained text-\nto-text transformer,” arXiv preprint arXiv:2010.11934, 2020.\n[181] N. Ratner, Y . Levine, Y . Belinkov, O. Ram, O. Abend, E. Karpas,\nA. Shashua, K. Leyton-Brown, and Y . Shoham, “Parallel context win-\ndows improve in-context learning of large language models,” arXiv\npreprint arXiv:2212.10947, 2022.\n[182] F. Motoki, V . P. Neto, and V . Rodrigues, “More human than human:\nMeasuring chatgpt political bias,” Public Choice, pp. 1–21, 2023.\n[183] K. Werder, B. Ramesh, and R. Zhang, “Establishing data provenance\nfor responsible artificial intelligence systems,” ACM Transactions on\nManagement Information Systems (TMIS), vol. 13, no. 2, pp. 1–23, 2022.\n[184] J. Jiang, X. Liu, and C. Fan, “Low-parameter federated learning with\nlarge language models,” arXiv preprint arXiv:2307.13896, 2023.\n[185] W. S. Saba, “Towards explainable and language-agnostic llms: Sym-\nbolic reverse engineering of language at scale,” arXiv preprint\narXiv:2306.00017, 2023.\n[186] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola, “Mul-\ntimodal chain-of-thought reasoning in language models,” arXiv preprint\narXiv:2302.00923, 2023.\n[187] Z. Liu, Y . Zhang, P. Li, Y . Liu, and D. Yang, “Dynamic llm-agent\nnetwork: An llm-agent collaboration framework with agent team opti-\nmization,” arXiv preprint arXiv:2310.02170, 2023.\n[188] U. Iqbal, T. Kohno, and F. Roesner, “Llm platform security: Applying\na systematic evaluation framework to openai’s chatgpt plugins,” arXiv\npreprint arXiv:2309.10254, 2023.\nMOHAIMENUL AZAM KHAN RAIAAN earned\nhis Bachelor of Science in Computer Science and\nEngineering from United International University\n(UIU) in 2023. Currently, he holds the position of\nResearch Assistant within the Computer Science\nand Engineering Department at UIU. His profes-\nsional pursuits are marked by active involvement\nin diverse research areas such as computer vision,\nhealth informatics, explainable artificial intelli-\ngence, and graph optimization. Notably, Raiaan\nhas made significant contributions to the field, as evidenced by his multiple\nresearch papers published in prestigious journals indexed by Scopus and\ncategorized under the Q1 ranking.\nMD. SADDAM HOSSAIN MUKTA received the\nPh.D. degree from the Data Science and Engi-\nneering Research Laboratory (Data Laboratory),\nBUET, in 2018. He is currently an Associate Pro-\nfessor and an Undergraduate Program Coordinator\nwith the Department of Computer Science and\nEngineering. He has a number of quality publi-\ncations in both national and international confer-\nences and journals. His research interests include\ndeep learning, machine learning, data mining, and\nsocial computing.\nKANIZ FATEMA has completed her bachelor’s\ndegree in Computer Science and Engineering\nfrom Daffodil International University, Dhaka,\nBangladesh. She is actively involved in research\nactivities, especially in Health informatics, Com-\nputer vision, Machine learning, Deep learning,\nand Artificial intelligence-based system. She is\ncurrently working as a Research Assistant (RA)\nat Charles Darwin University. She has published\nseveral research papers in journals (Scopus) and\ninternational conferences.\nVOLUME 4, 2016 35\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: A Review on Large Language Models\nNUR MOHAMMAD FAHAD holds a Bachelor’s\ndegree from the Department of Computer Science\nand Engineering at United International Univer-\nsity (UIU). During his undergraduate years, he\ncontributed to the academic community as an un-\ndergraduate teaching assistant at UIU’s Depart-\nment of Computer Science and Engineering in\nBangladesh. In addition to his teaching role, Fahad\nis deeply engaged in cutting-edge research across\nseveral domains, including computer vision, ma-\nchine learning, deep learning, health informatics, graph theory, and mental\nhealth modeling.\nSADMAN SAKIB acquired a bachelor’s degree\nin computer science and engineering from the\nDepartment of Computer Science and Engineering\nat United International University in 2023. He was\nalso a teaching assistant for undergraduate stu-\ndents in the Department of Computer Science and\nEngineering at United International University,\nBangladesh. Besides this, he is actively involved\nin machine learning, deep learning, artificial intel-\nligence, computer vision, and health informatics\nresearch.\nMOST. MARUFATUL JANNAT MIMis currently\npursuing a degree in the Computer Science and\nEngineering Department at United International\nUniversity (UIU). She is actively involved in re-\nsearch activities related to computer vision, deep\nlearning, graph theory, and human-computer inter-\naction. Her passion lies in pioneering innovative\nresearch in computer science. Apart from studies,\nshe is involved in co-curricular activities at the\nUIU APP Forum, where she currently serves as\npresident and demonstrates strong leadership by organizing various seminars\nand workshops for computer science students.\nJUBAER AHMAD received the B.Sc. degree in\ncomputer science and engineering from United In-\nternational University (UIU), Dhaka, Bangladesh,\nin 2022. He is currently working as a Research\nAssistant with the IAR project, United Interna-\ntional University, Bangladesh. His research inter-\nests include Computer Vision, NLP, Big Data, and\nDistributed Learning.\nMOHAMMED EUNUS ALI is a Professor with\nthe Department of CSE, Bangladesh University of\nEngineering and Technology (BUET). He is the\nGroup Leader of the Data Science and Engineer-\ning Research Laboratory (DataLab), CSE, BUET.\nHis research papers have published in top rank-\ning journals and conferences, such as the VLDB\nJournal, IEEE TRANSACTIONS ON KNOWL-\nEDGE AND DATA ENGINEERING, DMKD, In-\nformation Systems Journal, WWWJ, DKE, ICDE,\nCIKM, EDBT, PVLDB, and UbiComp. His research interests include\ndatabase systems and information management, including spatial databases,\npractical machine learning, and social media analytics. He served as a\nProgram Committee Member for many prestigious conferences, including\nSIGMOD, VLDB, AAAI, and SIGSPATIAL\nSAMI AZAM is a leading researcher and Professor\nat the Faculty of Science and Technology, Charles\nDarwin University, Australia. He is actively in-\nvolved in the research fields relating to Computer\nVision, Signal Processing, Artificial Intelligence\nand Biomedical Engineering. Dr. Azam has num-\nber of publications in peer reviewed journals and\ninternational conference proceedings.\n36 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3365742\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7990404367446899
    },
    {
      "name": "Open research",
      "score": 0.4150713384151459
    },
    {
      "name": "Data science",
      "score": 0.40544137358665466
    },
    {
      "name": "Natural language processing",
      "score": 0.320198655128479
    },
    {
      "name": "World Wide Web",
      "score": 0.26218676567077637
    }
  ]
}