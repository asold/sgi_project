{
  "title": "Sound Event Detection Transformer: An Event-based End-to-End Model for Sound Event Detection",
  "url": "https://openalex.org/W3204005682",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2745724006",
      "name": "Ye, Zhirong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1542467846",
      "name": "Wang Xiang-dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1928286344",
      "name": "Liu Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2741300598",
      "name": "Qian, Yueliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1956137351",
      "name": "Tao Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122136306",
      "name": "Yan Long",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2626222759",
      "name": "Ouchi Kazushige",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2959539607",
    "https://openalex.org/W3015469592",
    "https://openalex.org/W3015805515",
    "https://openalex.org/W2341393370",
    "https://openalex.org/W2771361008",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2798350598",
    "https://openalex.org/W3016195076",
    "https://openalex.org/W2145419885",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W2038484192",
    "https://openalex.org/W2977117446",
    "https://openalex.org/W2998304428",
    "https://openalex.org/W1991139021",
    "https://openalex.org/W2997266872",
    "https://openalex.org/W1844944916",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2591013610",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3017521796",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W2144506857",
    "https://openalex.org/W2963610932",
    "https://openalex.org/W3096762387",
    "https://openalex.org/W2980605556",
    "https://openalex.org/W1501987291",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2408239454",
    "https://openalex.org/W3122089402",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3006275583",
    "https://openalex.org/W3015594652",
    "https://openalex.org/W3015387077",
    "https://openalex.org/W2123843894",
    "https://openalex.org/W2799258971",
    "https://openalex.org/W2937959246",
    "https://openalex.org/W3112462324",
    "https://openalex.org/W2154318594",
    "https://openalex.org/W2963099423",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W2147917435",
    "https://openalex.org/W1650531274",
    "https://openalex.org/W2982468701"
  ],
  "abstract": "Sound event detection (SED) has gained increasing attention with its wide application in surveillance, video indexing, etc. Existing models in SED mainly generate frame-level prediction, converting it into a sequence multi-label classification problem. A critical issue with the frame-based model is that it pursues the best frame-level prediction rather than the best event-level prediction. Besides, it needs post-processing and cannot be trained in an end-to-end way. This paper firstly presents the one-dimensional Detection Transformer (1D-DETR), inspired by Detection Transformer for image object detection. Furthermore, given the characteristics of SED, the audio query branch and a one-to-many matching strategy for fine-tuning the model are added to 1D-DETR to form Sound Event Detection Transformer (SEDT). To our knowledge, SEDT is the first event-based and end-to-end SED model. Experiments are conducted on the URBAN-SED dataset and the DCASE2019 Task4 dataset, and both show that SEDT can achieve competitive performance.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nSound Event Detection Transformer:\nAn Event-based End-to-End Model for Sound Event\nDetection\nZhirong Ye1,2, Xiangdong Wang1,†, Hong Liu 1, Yueliang Qian1, Rui Tao3, Long Yan3, Kazushige Ouchi 3\nAbstract—Sound event detection (SED) has gained increasing\nattention with its wide application in surveillance, video index-\ning, etc. Existing models in SED mainly generate frame-level\nprediction, converting it into a sequence multi-label classiﬁcation\nproblem. A critical issue with the frame-based model is that it\npursues the best frame-level prediction rather than the best event-\nlevel prediction. Besides, it needs post-processing and cannot be\ntrained in an end-to-end way. This paper ﬁrstly presents the\none-dimensional Detection Transformer (1D-DETR), inspired by\nDetection Transformer for image object detection. Furthermore,\ngiven the characteristics of SED, the audio query branch and\na one-to-many matching strategy for ﬁne-tuning the model are\nadded to 1D-DETR to form Sound Event Detection Transformer\n(SEDT). To our knowledge, SEDT is the ﬁrst event-based and\nend-to-end SED model. Experiments are conducted on the\nURBAN-SED dataset and the DCASE2019 Task4 dataset, and\nboth show that SEDT can achieve competitive performance.\nIndex Terms —Sound event detection, machine learning,\nweakly-supervised learning, Transformer, end-to-end.\nI. I NTRODUCTION\nS\nOUND event detection (SED) is important for many\napplications, such as smart cities [1], healthcare [2],\nsurveillance [3], video indexing [4], and so on. It consists\nof two subtasks, one is to recognize the event in an audio\nclip, and the other is to locate its corresponding start and\nend boundaries. Sound events in real life tend to overlap with\neach other considerably. Recognizing such overlapping sound\nevents is referred to as polyphonic SED [5]. This is similar\nto the task of object detection in the ﬁeld of computer vision\nwhose goal is to locate and classify objects in an image where\ndifferent objects may also be occluded and overlapped. Sound\nevent detection and image object detection can be regarded\nas subtasks of object detection for one-dimensional and two-\ndimensional signals.\nFor SED, deep learning has shown initial success and\nbecame the mainstream. Existing work mainly employs convo-\nlutional neural networks (CNNs) [6] or convolutional recurrent\nneural networks (CRNNs) [7] to learn the representation for\neach frame, obtains the frame-level classiﬁcation probability\nby a classiﬁer, thresholds the probabilities to get frame-level\ndecisions, and then poses post-processing such as median\nﬁltering to smooth the outputs. Recently, considering the\n1Beijing Key Laboratory of Mobile Computing and Pervasive Device,\nInstitute of Computing Technology, Chinese Academy of Sciences, Beijing,\nChina\n2University of Chinese Academy of Sciences, Beijing, China\n3Toshiba China R&D Center, Beijing, China\nsuperiority of the Transformer architecture [8], Miyazaki et al.\n[9] propose to replace recurrent neural networks (RNNs) of the\nCRNN model with the encoder of Transformer or Conformer\n[10]. The above models transform sound event detection to a\nsequence-to-sequence multi-label classiﬁcation problem.\nHowever, a critical issue with the sequence-to-sequence\nclassiﬁcation scheme is the inconsistency between the goal\nof the model and the task, where the former pursues the best\nframe-level prediction while the latter aims at the best event-\nlevel prediction. Such inconsistency brings up two problems:\nFirst, the model is not optimal for the task, for its design\nand training are both for the best frame-level prediction.\nSpeciﬁcally, for model architecture, to extract local informa-\ntion required by frame-level prediction, only a small time-\naxis compression scale is allowed, resulting in the failure\nof the model to capture the global information describing\nthe event as a whole. For model training, the frame-level\nclassiﬁcation loss is adopted as the objective function. Hence\nthe model obtained after training is compliant with the frame-\nlevel rather than the event-level target. In short, the frame-\nbased model does not care about the correlation between\nframes, nor does it care about the event-level information,\nsuch as event duration, which counts for a great deal for event-\nlevel prediction. Second, for such frame-based models, post-\nprocessing is indispensable to aggregate frame-level prediction\ninto event-level prediction. Some work [9], [11] has greatly\nimproved the performance of the model by setting class-\nadaptive window sizes and thresholds. However, this will\nintroduce too many hyperparameters, resulting in difﬁculty in\ntraining and bringing the risk of over-ﬁtting. Besides, these\nhyperparameters are speciﬁc to the dataset and need to be\nmanually searched from scratch when applying to different\ndatasets, which greatly reduces the generalization ability of\nthe model. To resolve the inconsistency between the model\ntarget and the task, end-to-end learning has been proposed,\nwhich requires the model to output the ﬁnal results directly to\nensure that the model is optimized towards the task goal during\ntraining. This end-to-end philosophy has become a trend and\nachieves state-of-the-art (SOTA) results in many ﬁelds, such\nas speech recognition and machine translation, but not yet in\nSED. To construct an end-to-end SED model, an event-based\nprediction mechanism needs to be designed.\nIn image object detection, traditional models also only\ngive intermediate outputs, and apply the non-max suppression\n(NMS) [12] procedure to get the ﬁnal results. However, the\ncommonly adopted GreedyNMS cannot be easily parallelized\narXiv:2110.02011v3  [cs.SD]  12 Nov 2021\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nand has become a performance bottleneck [13]. To bypass the\ntime-consuming post-processing, the ﬁrst end-to-end object\ndetection model, Detection Transformer (DETR) [14], has\nbeen proposed and achieves competitive performance. DETR\nimplements an object-based prediction mechanism to directly\npredict the boundary and category label for each object\nwithout any post-processing. Learning from the object-based\nprediction mechanism of DETR, we built to our knowledge\nthe ﬁrst end-to-end model for SED which outputs event-\nlevel predictions directly. The event-based prediction schema\nmakes the model better adapted to the task. For the event-\nbased model, the time axis compression scale will be up to\nthe event duration to take both local and global information\ninto account to get better event representation. At the same\ntime, it is capable of learning various event characteristics\nsuch as duration, without introducing hyperparameters during\npost-processing which are obtained by manual search on the\ndevelopment dataset.\nIn this paper, we propose to our knowledge the ﬁrst event-\nbased and end-to-end SED model. Firstly, the one-dimensional\nDetection Transformer (1D-DETR) is presented for SED,\ninspired by Detection Transformer for image object detection.\nAnd then, to overcome the two shortcomings we observed in\nthe 1D-DETR model, namely, lacking category information\nand tending to judge events as the “empty” category due to\nthe one-to-one matching principle adopted during training,\nwe propose the audio query branch and the one-to-many\nmatching strategy to obtain a Sound Event Detection Trans-\nformer (SEDT) which does not need any post-processing and\nrealizes end-to-end sound event detection. Experiments on the\nURBAN-SED dataset and the DCASE2019 Task4 dataset both\nshow that SEDT achieves competitive performances compared\nwith SOTA models. The code will be available soon.\nII. R ELATED WORK\nA. Supervised SED\nAs mentioned above, polyphonic SED is often formulated\nas a sequence-to-sequence multi-label classiﬁcation problem\nand addressed by classifying each frame and then integrating\nthe frame-level results. The major challenge of polyphonic\nSED lies in the addictive nature of sound sources, which\nmakes it difﬁcult to ﬁnd robust features and classiﬁers. Earlier\napproaches use the widely applied Mel-Frequency Cepstral\nCoefﬁcients (MFCC) feature and conventional classiﬁers from\nspeech recognition. For example, T. Heitto et al. [15] use Gaus-\nsian Mixture Models (GMMs) to obtain context-dependent\nframe representation from MFCC and use Hidden Markov\nModels (HMMs) as the classiﬁer. However, due to the random\nco-occurrence patterns of sound events and noise interference,\nsuch models have poor robustness. To tackle this problem,\nthe spectrogram image feature (SIF) is utilized as input to\ntrain CNNs [6]. This CNN-based model shows excellent\nperformance and has been further optimized and adjusted by\nother researchers [16]. However, although the CNN model can\ncapture frame feature with great robustness from the spectro-\ngram, it fails to exploit longer temporal context information,\nwhich is essential for sequence related tasks. To address this\nlimitation, CNNs are combined with RNNs, which can model\nsequential information well, therefore generating CRNNs for\nSED [17]. There are also many follow-up researches on the\nCRNN architecture [18], [19]. Recently, with the great success\nof Transformer and its variants in other ﬁelds, Miyazaki et\nal. [20] propose to replace the RNNs with the encoder of\nTransformer [8] or Conformer [10].\nIn order to get event-level results, post-processing is applied\nto integrate the frame-level predictions to obtain the temporal\nboundaries, which is crucial to the detection results [21]. It\nmainly includes two steps: binarization and median ﬁltering.\nSome work suggests tuning the binarization thresholds [22]\nand window sizes [11] for different event categories on the\ndevelopment set to improve the model’s ability to detect\nevents of different durations. However, it only works when the\ndevelopment set and the test set have similar data distribution.\nWhat’s worse, searching for the best combination of these\nhyperparameters is quite time-consuming.\nB. SED with weakly-labeled and synthetic data\nDue to the high cost of annotating audios with temporal\nboundaries of events, it has become a trend to train SED\nmodels with weakly-labeled data and synthetic data.\nFor weakly supervised SED, which utilizes weakly-labeled\ndata that only indicates the presence of event categories in\nan audio clip, it is usually approached as a multiple instance\nlearning problem [23], where the frame-level representations\nare aggregated to obtain a clip-level representation, and then\naudio tags are predicted to compare with the ground-truth\nlabels [24]. Based on this architecture, many pooling methods\nhave been studied, such as max pooling, average pooling,\nattention pooling, and so on [25], [26]. However, whatever\nmethods are adopted to exploit the weak labels, there is always\na trade-off between frame-level (event boundary) and clip-level\n(audio tag) prediction, where the former expects more detailed\n(frame-level) information, corresponding to a smaller time\naxis compression scale, while the latter requires more global\ninformation, corresponding to a larger compression scale [27].\nTo liberate model from such trade-off, Lin et al. [28] propose\na teacher-student framework named Guided Learning for SED\nwith weakly-labeled and unlabeled data, where the teacher\nmodel focuses on audio tagging, while the student model\nfocuses on the prediction of event boundaries. In addition,\nHuang et al. [29] propose a multi-branch learning method to\nensure the encoder can capture more comprehensive feature\nﬁt for various subtasks.\nSynthetic data is another alternative to train SED models,\nwhich is generated by mixing up isolated events with back-\nground noise randomly. Synthetic data is often used jointly\nwith real data with strong or weak labels. Initially, synthetic\ndata is used equally to real data, which is obviously not a\ngood choice for there is a large domain gap between synthetic\ndata and real data [30]. To use synthetic data more effectively,\ndomain adaption is introduced into SED to compensate for\nthe inconsistency of their feature distribution. Adversarial\nlearning and metric learning are two major approaches utilized\nin existing domain adaptation models for SED [30]–[32].\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nDe-overlap\nEvent queriesClass objectClass objectNo objectClass objectTransformer  encoder......Transformer  decoderCNNFeatures1D positional encodingBackbone\nTF+\nEncoderDecoderSpectrogamClassTimestampFFNsOutputSignal processing\nFig. 1. Overview of 1D-DETR model.\nDifferent domain adaptation methods may focus on different\nstages, some aim at data collection condition adaptation [31],\nothers apply it during model training [30], [33].\nTo make soundscape synthesis and augmentation more\nconvenient, Scaper [34], an open source library, is developed.\nMany datasets are generated by it, such as the DESED syn-\nthetic soundscapes evaluation set [35] [36] and the URBAN-\nSED dataset.\nC. DETR for object detection\nDetection Transformer (DETR) [14] is the ﬁrst end-to-end\nmodel in the ﬁeld of image object detection, which adopts\nthe encoder-decoder model architecture. DETR exploits the\nglobal features of the image by the encoder, learning the\npossible objects in the image, then employs the decoder to\npredict the precise position and category. The principle and\nprocedure of DETR are more in line with the top-down\nhuman visual attention characteristics, that is, visual attention\nis usually guided by “goals” in mind, which has been explained\nby cognitive science in the Preferred Competition Theory\n[37]. Supported by cognitive principles and good performance,\nDETR has received great attention. Many DETR-related works\nhave been proposed, such as deformable DETR [38] and\nunsupervised learning methods for DETR [39]. It has also\nbeen applied to pedestrian detection where detection targets\nare highly overlapped, and achieves competitive performance,\ndemonstrating its great potential in crowded scenes [40].\nHowever, most work still focuses on image object detection.\nAs mentioned earlier, SED and object detection have much\nin common. Besides, reports are showing that human auditory\nattention also follows the top-down manner [41], which means\nthat the detection principle of DETR is still applicable to SED,\nwhile the frame-based model only learns locally.\nIII. T HE 1D-DETR MODEL\nConsidering the characteristics of one-dimensional signals,\nwe modify the designs for two-dimensional signals in DETR\nto obtain 1D-DETR. In this section, we introduce the model\narchitecture and training methods of 1D-DETR.\nA. Model Architecture\nAs depicted in Figure 1, 1D-DETR consists of three main\ncomponents: a backbone, an encoder-decoder transformer, and\nprediction feed-forward networks (FFNs). 1D-DETR repre-\nsents each sound event as a vector yi = ( ci,bi), where ci\nis the class label and bi = ( mi,li) denotes the temporal\nboundary containing normalized event center mi and duration\nli. Given the spectrogram of an audio clip, the backbone and\ntransformer encoder are utilized to extract its feature. The\ndecoder is adopted to generate event representations which are\nthen fed into prediction FFNs to obtain ﬁnal event detection\nresults. We describe each component in detail below.\n1) Backbone: In this paper, we use ResNet-50 [42] as\nthe backbone, which has achieved excellent performance on\naudio classiﬁcation [4], conﬁrming its efﬁciency in audio\nfeature extraction. For an audio clip, its corresponding Mel-\nspectrogram X ∈R1×T0×F0 will be transformed into a feature\nmap f ∈RC×T×F, where T0 and T denote the dimensions of\nthe time axis, F0 and F denote the dimensions of the frequency\naxis, and C denotes the number of channels. Then a 1 ×1\nconvolution is applied to reduce the channels to the number\nof transformer attention units d, resulting in a new feature map\nz0 ∈Rd×T×F.\n2) Positional encoding: 1D-DETR only needs to locate\non one axis, so unlike the original DETR, we adopt one-\ndimensional positional encoding for 1D-DETR, enforcing the\nmodel to focus on the time axis. The corresponding formula\ncan be expressed as:\nP(t,f,2i) = sin(t/100002i/d) (1)\nP(t,f,2i+1) = cos(t/100002i/d) (2)\nwhere t,f are the two-dimensional coordinates in a Mel-\nspectrogram, i is the dimension, and d is the number of\ntransformer attention units. Using these equations, we generate\nthe positional encoding P ∈Rd×T×F with the same shape as\nz0.\n3) Encoder: The encoder has a stack of E identical blocks\nwith two sub-layers: a multi-head self-attention mechanism\nand a feed-forward network. We ﬂatten z0 and P on the\ntime and frequency axis to get a d×TF feature map and\npositional encoding to make them meet the input requirements\nof the encoder. Unlike standard Transformer where positional\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nencoding is only added to the initial input of Transformer,\n1D-DETR performs the addition of positional encoding to the\ninput of each layer to enhance its positioning ability.\n4) Decoder: The decoder consists of M identical blocks\nwith multi-head self-attention layers, encoder-decoder atten-\ntion layers, and feed-forward layers. It is employed to generate\nevent representations. Conventional transformers are mainly\nused to process sequences with strong context dependence,\nsuch as text and speech. To capture dependence among out-\nputs, the prediction is made in an incremental token-by-token\nway with previous output as input. However, for the detection\ntask, the objects are almost independent of each other, so\nthe auto-regressive mechanism would be inappropriate. In 1D-\nDETR, we model detection as a set prediction problem by us-\ning N embeddings as input, where N is a ﬁxed hyperparameter\nand larger than the typical number of events in an audio. These\ninput embeddings are learned positional encodings, which we\ncall event queries , and will be added to the input of each\nattention layer. This scheme enables 1D-DETR to decode N\nevents in parallel.\n5) Prediction feed-forward networks (FFNs): Prediction\nFFNs are used to transform the event representations from the\ndecoder into timestamps and class labels. For the timestamp,\nwe represent it as a vector bi = ( mi,li), where mi and li\ndenote normalized event center position and event duration\nrespectively. We use a linear perceptron to obtain the times-\ntamps. For class labels, a simple linear projection layer with\na softmax function is used. For each audio, the number of\npredictions output by 1D-DETR is a ﬁxed number N which\nis usually larger than the number of ground truth events, hence,\nan additional “empty” class label ø is added to indicate that\ncertain detection results have detected “empty” events.\nB. Model Training\n1) Matching Method: A unique matching between pre-\ndicted and ground truth events is essential for loss compu-\ntation. In 1D-DETR, we follow the method adopted by DETR\n[14] and employ the Hungarian algorithm [43] to obtain an\noptimal bipartite matching. Let yi = (ci,bi) denote an event,\nwhere ci is the target class label and bi ∈[0,1]2 denotes the\ntimestamp. Then the matching process can be formulated as\nsearching for a permutation of N elements ˆσ∈SN to get the\nﬁnal matching relation:\nˆσ= arg min\nσ∈SN\nN∑\ni\nLmatch\n(\nyi,ˆyσ(i)\n)\n(3)\nwhere y is a set of the ground truth events yi with “empty”\nevents ø padded to expand its size to N, ˆy = {ˆyi}N\ni=1\nrepresents the set of N predictions, and Lmatch is the\ncorresponding pairwise matching cost, which is deﬁned as\nˆpσ(i) (ci) + Lloc\n(\nbi,bσ(i)\n)\nfor ci ̸= ø where ˆpσ(i) (ci) is the\nprobability of class ci and Lloc is the location loss which will\nbe deﬁned later.\n2) Loss Function: The loss function is the sum of location\nloss Lloc and classiﬁcation loss Lc:\nL= Lloc + Lc (4)\nTimestamp\n&\nClass\nFFNs De-overlap\nEvent queries\nAudio tag\nClass object\nClass object\nNo object\nClass object\nTransformer  decoder\nOutput\nSignal\nprocessing\n&\nBackbone\n&\nEncoder\nAudio query\nMulti-label \nclassifier\nFig. 2. Overview of the audio query branch of SEDT, drawn with the bold\nblack line, including Audio query and Multi-label classiﬁer.\nThe location loss is obtained by computing the L1 norm\nbetween the target and predicted location vector. In order to\nspeed up the convergence during training, IOU loss is further\napplied:\nLloc =\nN∑\ni\n1 {ci̸=∅}Lloc(bi,ˆbˆσ(i)) (5)\n=\nN∑\ni\n1 {ci̸=∅}\n(\nλIOULIOU(bi,ˆbˆσ(i)) + λL1 ∥bi −ˆbˆσ(i) ∥1\n)\nwhere λIOU,λL1 ∈R are hyperparameters, ˆσ is the optimal\nassignment given by the matching process, and N is the\nnumber of predictions. The classiﬁcation loss is the cross-\nentropy between the labels and the predictions:\nLc =\nN∑\ni=1\n−log ˆpˆσ(i)(ci) (6)\nThe prediction loss of each decoder block is calculated to assist\nthe model training in 1D-DETR. To sum up, the loss function\nof the entire network can be expressed as:\nLtotal =\nM∑\nm=1\nLm (7)\nwhere M represents the number of decoder blocks.\nIV. S OUND EVENT DETENTION TRANSFORMER\nWhen applying 1D-DETR to SED, we found many pre-\ndictions with accurate positions but wrong class labels. We\nconjecture the poor classiﬁcation performance of 1D-DETR\nis mainly due to two reasons: the insufﬁcient category infor-\nmation extracted by the model and the one-to-one matching\nprinciple adopted during training. To deal with these two\nproblems, on the basis of 1D-DETR, we propose an audio\nquery branch and one-to-many matching strategy, obtaining\nSound Event Detection Transformer (SEDT).\nA. Audio query branch\nAs mentioned in [14], DETR may wrongly classify a certain\nnumber of accurately positioned predictions as the “empty”\nclass. To optimize the performance of DETR, these predictions\nare overridden with the second-highest scoring classes in the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nwork of [14] . But it is not a good choice, because sometimes\nDETR does detect accurate “empty” events. We explored the\ncause of this problem and speculated that the poor classi-\nﬁcation performance may be due to the decreased category\ninformation extraction ability of the model caused by the\npositioning subtask and the introduction of the “empty” class.\nTo verify this conjecture, we compared the backbone network\ntrained within the 1D-DETR targeting SED with the same\nnetwork trained individually only targeting event classiﬁcation\n(audio tagging) by connecting both trained networks to a\nclassiﬁer and measuring the audio tagging performance of\nthem (the details will be described in Section V-C). We found\nthat the backbone trained individually performed better than\nthe backbone trained within 1D-DETR, indicating that 1D-\nDETR might lose a lot of category information in the backbone\nfeature extraction stage.\nBased on the above observation, we introduce an audio\nquery branch to 1D-DETR. On one hand, the added clip-level\nclassiﬁcation branch can enhance the category information in\nthe model. On the other hand, we employ the audio tag to\nassist event-level decision-making, which is more reasonable\ncompared with the method adopted in DETR which replaces\nthe “empty” class with the second-highest scoring class.\n1) Audio query: We hope the model can better recognize\nevents without reducing the positioning accuracy, thereby\nimproving the event-based metric. The key is to guarantee\nthere is enough category information during decoding. We\nintroduce an audio query, which is also a learned positional\nembedding, similarly to event queries. The audio query and\nevent queries are sent to the decoder together. We assume\nthe output corresponding to audio query aggregates the whole\nsequence information and feed it into a multi-label classiﬁer\nwhich is a fully connected layer with Sigmoid function to\nget the predicted weak label lTAG as illustrated in Figure 2.\nThere would be an extra audio tagging loss for SEDT, which\nis calculated as the binary cross-entropy (BCE) between lTAG\nand the weak label yTAG:\nLat = BCE(lTAG,yTAG) (8)\n2) Event-level classiﬁcation fusion: Compared with SED,\naudio tagging is a relatively simple task, a model for which can\nachieve high recognition accuracy. Thus we expect the tagging\nresults can further assist to improve SED performances, and\nwe propose three strategies to fuse the event-level and clip-\nlevel classiﬁcation results:\nStrategy 1: Delete the detected events with class labels\nwhich are predicted by audio tagging as inactive. As depicted\nin Figure 3 (b), for all event queries, the probabilities of class\n1 are set to 0, so that no predictions for class 1 will be retained\nin the ﬁnal output.\nStrategy 2: On the basis of strategy 1, ensure that there exist\ndetected events with class labels that are predicted as active\nby audio tagging. Speciﬁcally, ﬁnd the detected event with\nthe largest class probability for each class that is predicted\nas active by audio tagging and set its corresponding class\nprobability as the threshold if its original probability is smaller\nthan the threshold. In Figure 3 (c), Class 3 is predicted as\nactive by audio tagging. The largest probability of class 3 lies\n0.50.10.31 0.10.20.60.10.10.10.20.40.323Class_1Class_2Class_3Audio taggingEvent queries\n(a)\nNYN00.10.31 0.100.60.10.100.20.40.323Event queries\n(b)\nEvent queriesNYY00.10.31 0.100.60.10.100.20.50.323\n(c)\n0.50.10.31 0.10.20.60.10.10.10.20.50.323 YYYEvent queries\n(d)\nFig. 3. Illustration of three event-level classiﬁcation fusion strategies with\nthree event categories and three event queries. The audio tagging results\nare represented by rounded rectangles, where different borderline colors\nindicate different event categories and gray ﬁlling indicates the corresponding\ncategories are judged as inactive, while white ﬁlling as active. For event\nqueries, the illustration shows the probability distribution of all event classes\n(including the “empty” class). Figure (a) is the original output, (b)-(d) give\nthe probability distribution and the ﬁnal output under corresponding strategies,\nwhere “Y” means the predicted event is regarded as the ﬁnal output, while\n“N” means not.\nin event query 3 but is smaller than the threshold, which we\nset as 0.5. Hence, we modify it as 0.5 to ensure the occurrence\nof class 3 in predictions.\nStrategy 3: Ensure there are detected events with class\nlabels predicted as active by audio tagging, and the speciﬁc\nimplementation is the same as strategy 2. Compared with\nstrategy 2, strategy 3 will not set the probability of class 1\nas 0.\nLet Caudio tagging and CSED denote the active class set\npredicted by audio tagging and SED respectively, then the\nclass set after fusion can be correspondingly expressed as:\nStrategy 1: C = Caudio tagging\n⋂CSED\nStrategy 2: C = Caudio tagging\nStrategy 3: C = Caudio tagging\n⋃CSED\nB. One-to-many matching strategy\nAs mentioned earlier, the number of predictions N by\n1D-DETR is usually larger than the number of ground-truth\nevents, resulting in a considerable proportion of the predictions\nwith accurate boundaries being matched to “empty” events\nduring training, so the model is inclined to judge some\nevents as “empty” class during testing even though they locate\ncorrectly. The root of the above problem is that the Hungarian\nalgorithm only permits one-to-one matching. So a natural\nchoice is to relax the one-to-one restriction and allow one-\nto-many matching. In this section, we introduce the training\nprocess which adopts the one-to-many matching strategy, as\nshown in Algorithm 1.\nThe one-to-many matching strategy not only considers\nthe matching obtained by the Hungarian algorithm but also\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nAlgorithm 1 Model training with one-to-many matching\nRequire: η= the initial learning rate\nRequire: xk = the kth training sample\nRequire: Sθ(xk) = set of predictions of xk\nRequire: yk = set of ground-truth events of xk\nRequire: G= the number of ground-truth events in the audio\nRequire: N = the number of predictions for each input\nRequire: Lmatch = pair-wise matching loss function\nRequire: Lloc = location loss function\nRequire: L= loss function\nRequire: SampleαG\nN\n= randomly sample function at the ratio\nof αG\nN ;\nEnsure: θ\n1: for epoch= 1 →epoches do\n2: η←0.1\n⌊ epoch\nepochsdrop\n⌋\n×η\n3: for each batch B do\n4: ˆyk ←Sθ(xk)\n5: ˆσk = arg min\nσ∈SN\n∑N\ni Lmatch\n(\nyk\ni,ˆyk\nσ(i)\n)\n6: if epoch<epoches lr then ⊿ the learning stage\n7: loss← 1\n|B|\n∑\nk\n∑N\ni L(yk\ni,ˆyˆσk(i))\n8: else ⊿ the ﬁne-tuning stage\n9: Ik\nj ←arg miniLloc(yk\ni,ˆyk\nj)\n10: Dk\nj ←miniLloc(yk\ni,ˆyk\nj)\n11: Σk\nϵ ←\n{\nj|Dk\nj <ϵ,j ∈\n{\nˆσk(ø)\n}}\n12: ˆΣk\nϵ ←SampleαG\nN\n(Σk\nϵ)\n13: ˆΣk\nϵ(i) ←\n{\nj|Ik\nj == i,j ∈ˆΣk\nϵ\n}\n14: ˆΣk(i) ←ˆΣk\nϵ(i) ∪\n{\nˆσk(i)\n}\n15: loss← 1\n|B|\n∑\nk\n∑N\ni\n∑\nm∈ˆΣk(i) L(yk\ni,ˆyk\nm)\n16: end if\n17: update θ ⊿ update network parameters\n18: end for\n19: end for\nintroduces matching whose location cost is less than a preset\nvalue ϵ. The location cost of a prediction is deﬁned as the\nsmallest location loss between itself and all targets. In practice,\nthe following strategy is employed to obtain the one-to-many\nmatching pairs: First, the Hungarian algorithm is applied to\nget one-to-one matching ˆσ as DETR does. Then, the location\ncost of predictions that are matched with “empty” events in\nHungarian matching is calculated. After that, we can get a\nset Σϵ of predictions with location cost smaller than a preset\nvalue ϵ. Finally, predictions from Σϵ are randomly selected at\nthe ratio of αG\nN to obtain ˆΣϵ, where αis a hyperparameter, G\nis the number of ground-truth events and N is the number\nof predictions for each audio. The selected predictions are\nmatched with targets with which the location cost are obtained.\nUnder this strategy, a ground truth event may match multiple\npredictions, thereby reducing the “empty” class matching.\nIn [14], DETR learns different specialization for each query\nslot, that is, each slot focuses on targets of different sizes\nand positions. Unfortunately, we observe that the one-to-\nmany matching strategy can undermine this specialization. We\nspeculate this is because, under this strategy, some slots are\noptimized to detect the same target, leading to homogeneity.\nTo overcome this shortcoming, in our work, the training of\nSEDT is carried out in two stages. One adopts the one-to-\none matching strategy to learn the slot with specialization,\nreferred to as the learning stage. The other adopts the one-to-\nmany matching strategy for the alleviation of “empty” class\npredictions, referred to as the ﬁne-tuning stage.\nThe ﬁne-tuning stage makes one target corresponds to\nmultiple predictions, leading to a large overlapping between\npredictions. To remove redundant predictions, we perform de-\noverlapping on the output of SEDT. De-overlapping is only\nperformed for predictions with the same class label. Class\nprobability is applied as the retention metric, that is, only\nthe result with the highest class probability in the overlapped\npredictions is retained.\nC. weakly-supervised SEDT\nSEDT is trained by minimizing both clip-level and event-\nlevel loss function. However, weakly-labeled data only in-\ndicates the presence of event categories in a clip, which\nmeans the event-related modules of SEDT, that is, prediction\nFFNs for ﬁnal timestamp and class label output, will not\nbe optimized. To equip SEDT with the ability of weakly\nsupervised learning, we perform max pooling operation on the\nevent-level class probability vectors to obtain a clip-level class\nprobability vector. Then we calculate the binary cross-entropy\n(BCE) between this pooling probability vector pTAG and the\nweak label yTAG to obtain pooling audio tagging loss , which\ncan be formulated as\nLp\nat = BCE(pTAG,yTAG) (9)\nObviously, pooling audio tagging loss still fails to update\nthe linear perceptron for timestamp prediction. We assort to\nsynthetic data to train this module. In this way, we can train\nSEDT by real weakly-labeled data and synthetic data, which\nis a kind of weakly supervised learning.\nD. Loss function\n1) Loss for strongly-labeled data: For training audios with\nannotation of temporal boundaries of sound events, the SEDT\nis trained by minimizing the linear combination of audio tag-\nging loss and the basic loss of 1D-DETR, including location\nloss and classiﬁcation loss:\nLstrong = L1D-DETR + λatLat (10)\n=\nM∑\nm=1\n(Lm\nloc + Lm\nc ) + λatLat\nwhere λat is a hyperparameter.\n2) Loss for weakly-labeled data: The predicted weak labels\nby audio query and pooling operation allow the model to\nlearn the distribution characteristics of weakly-labeled audios.\nFor weakly-labeled data, only audio tagging loss and pooling\naudio tagging loss are computed:\nLweak = λatLat + λp\natLp\nat (11)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nWe can train a SEDT with real weakly-labeled data and syn-\nthetic strongly-labeled data. The corresponding loss function\ncan be formulated as\nL= Lsynthetic\nstrong + Lreal\nweak (12)\nWith the knowledge of real data learned by audio query and\npooling operation, it can be used to detect events in real audios.\nV. E XPERIMENTS\nA. Experimental setup\n1) Dataset: We perform experiments on the URBAN-\nSED and DCASE2019 Task4 datasets. URBAN-SED [44]\nis a dataset of synthesized audio clips generated by Scaper\n[34], and the data is with annotation of temporal boundaries\nof sound events (strongly-labeled). It contains 10 different\ncategories of sound events and is divided into a training set\n(6000 clips), a validation set (2000 clips), and a test set\n(2000 clips). The DCASE2019 Task4 dataset also has 10\nsound event categories and is divided into a training set and\na validation set. The training set consists of three subsets, i.\ne., a weakly-labeled dataset of real-world recordings (1578\nclips), a synthetic dataset with strong labels (2045 clips) and an\nunlabeled dataset of real-world recordings (14412 clips). The\nvalidation data (1168 clips) are real recordings with strong\nlabels. The DCASE2019 Task4 dataset is the same as the\nDCASE2021 Task4 dataset except for the synthetic subset.\nIn our experiment, the unlabeled subset will not be used. We\nextract 64-dimensional log-Mel spectrograms from all the data\nas input features.\n2) Model: For SEDT, ResNet-50 is adopted as the back-\nbone model, and Transformer with 3 encoder and 3 decoder\nlayers is employed. The number of event queries is set as\n10 for the URBAN-SED dataset and 20 for the DCASE2019\nTask 4 dataset. We use the following rules to name the\nSEDT models in our experiments: the sufﬁx “AQ” denotes\nmodels with audio query, “FT” denotes models after ﬁne-\ntuning with one-to-many matching, and “P m” denotes event-\nlevel classiﬁcation fusion strategy m is applied.\nAs for the baselines, we follow the model architecture in\n[20] to build the CRNN-based [7] and the Transformer-based\nmodel [20], which is referred to as “CTrans” in this paper.\nAnd on the DCASE2019 Task4 dataset, the synthetic data\nis used as strongly-label data as in [20]. For CRNN and\nCTrans, the frame-level predictions are smoothed by a median\nﬁlter with a ﬁxed window size of 0.45s for different event\ncategories. To demonstrate that SEDT can learn the duration\nof different event categories, we also give the results of CRNN\nand CTrans post-processed by adaptive median ﬁltering [11],\nthat is, the window size for each event category is optimized\non the validation dataset. We denote them as CRNN-CWin and\nCTrans-CWin, respectively. The class probability threshold is\nﬁxed as 0.5 to determine sound events for both SEDT and\nbaseline models.\n3) Training: The models are trained with a batch of 128\nclips with the AdamW optimizer [45]. In the learning stage,\nthe model is trained for 400 epochs with an initial learning rate\nof 10−4. Then the best model obtained in the learning stage is\nTABLE I\nPERFORMANCE ON URBAN-SED TEST SET\nModel Eb [%] Sb[%] At[%]\nCRNN [47] - 64.30 -\nCRNN 35.26 65 .75 74 .64\nCRNN-CWin 36.75 65 .74 74 .19\nCTrans 31.33 64 .51 74 .67\nCTrans-CWin 34.36 64 .73 74 .05\n1D-DETR 32.71 60 .64 70 .90\nSEDT-AQ-FT-P1 37.27 65.21 74 .37\nSEDT-AQ-FT-P2 36.61 65 .53 75 .12\nSEDT-AQ-FT-P3 36.36 65.77 75 .61\nﬁne-tuned for another 100 epochs, with initial learning rate of\n10−5. The hyperparameters of loss function are set as follow:\nλat = λp\nat = 0.25 for the DCASE2019 Task4 dataset, λat = 3\nfor the URBAN-SED dataset, and λIOU = 2,λL1 = 5 for both\ndatasets. As for the ﬁne-tuning stage, the setup of ϵ= 1,α = 1\nis used.\n4) Evaluation: To evaluate the performance of models, we\ncompute the event-based measure (a 200ms collar on onsets\nand a 200ms/20% of the events length collar on offsets) and\nthe segment-based measure (the length of a segment is 1s)\nby the sed eval package [46]. We also compute the clip-level\nmacro F1 score to measure the performance of models on\naudio tagging. For the SEDT models, the audio tagging results\nare given by the audio query branch.\nB. Results of 1D-DETR and SEDT\n1) Results on the URBAN-SED dataset: In the published\nwork [47], CRNN achieved SOTA results on the URBAN-\nSED dataset with a segment-based metric of 64.30%, while\nthe event-based metric and audio tagging results are not given.\nSo we train the DCASE2020 Task4 baseline model, a well-\ndesigned CRNN model, as one of our baselines. The CTrans\nmodel follows the architecture proposed by [20], and the\nhyperparameters of Transformer encoder are consistent with\nthat of SEDT.\nAs shown in Table I, where Eb, Sb and At denote Event-\nbased, Segment-based, and Audio tagging macro F1 respec-\ntively, the SEDT models obtain results competitive to the\nbaselines, which shows the effectiveness of our event-based\nend-to-end SED framework. SEDT-AQ-FT-P1 achieves the\nbest event-based F1 score of 37.27%, and SEDT-AQ-FT-P3\nachieves the best segment-based and audio tagging F1 score.\nAlthough class-adaptive median ﬁltering can bring beneﬁts,\nCRNN-CWin is still inferior to SEDT-AQ-FT-P1. The results\ndemonstrate that the proposed model can take both local and\nglobal information of the audio into account. The positioning\naccuracy required by event-based, segment-based, and audio-\nbased metrics is gradually reduced, and strategy 1, 2, and 3\nhave gradually relaxed requirements for outputs. Thus they\ncan cater to different metrics and achieve the best F1 score on\nthe corresponding metric.\nFrom table I, we can also observe that the CTrans model\nperforms worse than the CRNN model, but it gets more im-\nprovement from class-adaptive median ﬁltering, from 31.33%\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE II\nPERFORMANCE ON DCASE2019 T ASK 4 VALIDATION SET\nModel Eb [%] Sb[%] At[%]\nCRNN [20] 23.61 61 .00 -\nCRNN-CWin 28.59 61 .48 65 .13\nCTrans [20] 21.31 62 .44 -\nCTrans-CWin 30.72 63.91 68.70\nSEDT-AQ-P1 31.64 61.18 69 .11\nSEDT-AQ-P2 31.15 61 .03 69 .09\nSEDT-AQ-P3 29.52 59 .94 69 .32\nSEDT-AQ-FT-P1 29.29 62 .71 70.97\nSEDT-AQ-FT-P2 29.19 62 .68 70 .93\nSEDT-AQ-FT-P3 28.04 61 .52 70 .97\nto 34.36%, which is consistent with the experiment results\nin [20]. The CTrans-CWin does not outperform the CRNN-\nCWin model, which may be due to that the URBAN-SED\ndataset is a synthetic dataset, where the duration of different\nevent categories does not vary signiﬁcantly thus the class-\nadaptive median ﬁltering can not bring much extra event-\nrelated information.\n2) Results on the DCASE2019 Task4 dataset: On this\ndataset, SEDT can only learn to predict event boundaries from\nsynthetic data, while the real data can only be learned through\nthe audio tagging task. We measure the performance of SEDT\non the validation set, which is composed of real-recording\nclips. The results are given in Table II, where Eb, Sb, and At\ndenote Event-based, Segment-based, and Audio tagging macro\nF1 respectively.\nResults show that for the main evaluation metric in SED, i.e.\nevent-based metric, SEDT-AQ-P1 has a signiﬁcant advantage\nover the CRNN and CTrans. Compared with the baselines with\na ﬁxed threshold and the class-adaptive window size, SEDT-\nAQ-P1 still outperforms CRNN-CWin and CTrans-CWin by\n3.05% and 0.92% respectively. Since the test set is not public,\nwe optimize the window size and evaluate the model on\nthe same dataset, i.e. the validation set. Hence, the class-\nadaptive window size will bring much more improvement on\nthe DCASE2019 Task4 dataset than the URBAN-SED dataset.\nConsidering that SEDT does not exploit information from\nthe validation set, we argue that SEDT has greater potential\nthan the baselines. For audio tagging, the SEDT models also\noutperform the baselines. For the segment-based metric, the\nCTrans-Cwin achieves the best F1 score. This may be because\nthat the SEDT is trained with the objective of accurate event\ntemporal boundaries, tending to reduce predictions with longer\nduration than the ground-truth, while the frame-based models\nhave more relaxed localization requirment.\nTable II shows that after ﬁne-tuning, the event-based per-\nformance of the SEDT models have decreased slightly, which\nis contrary to the results on the URBAN-SED dataset. The\none-to-many matching used in ﬁne-tuning sacriﬁces the spe-\ncialization of event query slots to make the model ﬁt the\ntraining set better. When the distribution of the training set\nand the test set are the same (URBAN-SED), this strategy can\nwork, but when the two sets are quite different (the training\ndata of the DCASE2019 dataset contains synthetic data while\nthe data in the validation set are all real recordings), it may\ncause performance degradation instead. However, the audio\ntagging performance increases after ﬁne-tuning, and SEDT-\nAQ-FT-F1 performs best among all models. The segment-\nbased metric also improves due to the excellent performance of\naudio tagging. This shows that the audio tagging task is less\naffected by the difference between the training and the test\ndata and can beneﬁt from the one-to-many matching strategy.\nC. Ablations\nIn this paper, we modiﬁed the two-dimensional positional\nencoding of DETR into one-dimensional positional encoding,\nobtaining the 1D-DETR model suitable for one-dimensional\nsignals. And based on 1D-DETR, the audio query and ﬁne-\ntuning method based on the one-to-many matching strategy are\nadded to form the Sound Event Detection Transformer (SEDT)\nmodel. To verify the effectiveness of these modiﬁcations, we\nperform ablation experiments on the URBAN-SED dataset.\n1) Positional encoding: As shown in Table III, on all of\nthe three metrics, 1D-DETR with two-dimensional positional\nencoding, denoted as 1D-DETR (2D-pos), lags 1D-DETR with\none-dimensional positional encoding, denoted as 1D-DETR\n(1D-pos), by about 1%, verifying that the two-dimensional\npositional encoding brings redundant information, which mis-\nleads the model and worsens its detection performance.\n2) Audio query: We investigate the effects of audio query of\nthe SEDT model. The experimental results are shown in Table\nIII. SEDT-AQ outperforms 1D-DETR by 0.91% and 0.38%\nin event-based and segment-based macro F1 respectively. We\nguess that this improvement is due to that the audio query\nguides the model to learn more category information during\nthe feature extraction stage of the backbone, which will be sent\nto the decoder, thus improving its classiﬁcation performance.\nTo verify our conjecture, we ﬁx the parameters of the backbone\nof the trained 1D-DETR and the SEDT-AQ and use them\nas feature extractors to construct two audio tagging models\nby cascading classiﬁers after them respectively. The macro\nF1 score is computed to evaluate their performance on audio\ntagging. We also give the audio tagging results when the\nbackbone and the classiﬁer are trained jointly to evaluate the\nnetwork structure. The results are shown in Table IV. The\nmodel with SEDT-AQ backbone achieves an average macro F1\nscore of 75.44% and outperforms the model with 1D-DETR\nbackbone, suggesting that SEDT with audio query learns\nmore category information than 1D-DETR. But it still lags\nbehind the model with a trainable backbone by 1.80%. This\nis because the model needs to weigh between classiﬁcation\nand positioning.\nWe continue to compare different event-level classiﬁcation\nfusion strategies. We note that strategy 1 achieves the best\nP but the worst R on both event-based and segment-based\nmetrics, while on the contrary, strategy 3 achieves the worst\nP but the best R. This is reasonable because strategy 3 uses\nmore predictions as the ﬁnal output, will cover more correct\npredictions to improve R and introduce incorrect predictions\nto lower P. Strategy 2 comprehensively considered P and R,\nachieving the best event-based and segment-based macro F1\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE III\nRESULTS OF ABLATION EXPERIMENTS ON URBAN-SED TEST SET\nModel Event-based [%] Segment-based[%] Audio tagging[%]\nF1 P R F1 P R F1\n1D-DETR(2D-pos) 31.99 38 .39 27 .86 59 .63 71 .20 51 .99 69 .98\n1D-DETR(1D-pos) 32.71 38 .43 28 .86 60 .64 70 .89 53 .55 70 .90\nSEDT-AQ 33.62 39 .86 29 .57 61 .02 72 .36 53 .53 71 .17\nSEDT-AQ-P1 34.92 42 .64 30 .15 62 .08 75.65 53.49 71 .76\nSEDT-AQ-P2 35.00 40 .29 31 .46 64 .32 73 .72 57 .85 74 .67\nSEDT-AQ-P3 34.80 39 .16 31 .70 63 .94 71 .75 58 .27 74 .82\nSEDT-AQ-FT 36.72 41 .63 33 .30 65 .17 73 .19 59 .38 74 .75\nSEDT-AQ-FT-P1 37.27 43 .32 33.21 65 .21 74.82 58.46 74 .37\nSEDT-AQ-FT-P2 36.61 41 .36 33 .25 65 .53 72 .91 60 .13 75 .12\nSEDT-AQ-FT-P3 36.36 39 .95 33.79 65 .77 71.28 61.70 75 .61\n(a) Raw\n (b) Strategy 1\n (c) Strategy 2\n (d) Strategy 3\nFig. 4. The impact of ϵ and α on the performance of models with different strategies applied.\nTABLE IV\nTHE PERFORMANCE OF AUDIO TAGGING MODELS\nFeature extractor Audio tagging macro F1[%]\nBackbone (trainable) 77.24\n1D-DETR Backbone (ﬁxed) 74.12\nSEDT-AQ Backbone (ﬁxed) 75.44\nscore of 35.00% and 64.32% respectively. The results show\nit is feasible and effective to use audio tagging, a relatively\nsimple task, to assist detection.\n3) One-to-many matching strategy: We explore the impact\nof ϵand αon the performance of models with different fusion\nstrategies. As shown in Figure 4, we vary the location cost\nthreshold ϵfrom −2 to 6 in increments of 0.5, for the location\ncost of almost all predictions is in this range. As for α, we set\nit as 0.5,1,2, and at the same time give the results when all\npredictions with location cost within ϵare retained, denoted as\n“all”. Overall, we can intuitively ﬁnd that models withα= 0.5\nand α = 1 perform similarly, but the latter ﬂuctuates more\nnoticeably with the change of ϵ. Both these two curves show an\nupward trend before ϵ= 0.5, and then ﬂatten out. When α=\n2, the performance of the models drop signiﬁcantly, but they\nare still better than the models with all qualiﬁed predictions\nretained. The “ α= 2” and “all” curves are not sensitive to ϵ.\nThe ﬁgure suggests that it makes sense to tune α since\nthere is an apparent gap between models with a small α and\nmodels with “all”. However, ϵ has little effect on the models,\nespecially when it is greater than 0.5. ϵ cannot be set too\nsmall, otherwise, it will greatly reduce the performance of\nmodels. We argue this is because when ϵ is small, only a\nsmall proportion of the predictions will be utilized to form\nadditional matching, limiting the inﬂuence of ﬁne-tuning.\nWhen ϵ is large, the number of predictions that meet the\npositioning requirements increases. If all of them are added\nto the matching relationship during training, different event\nquery slots will be optimized towards the same targets. Then\nduring inference, these event query slots will tend to give\nsimilar detection results and miss some events, which will\nreduce the recall of the model. Actually, the setting of ϵ and\nα is seeking a balance between generalization ability and a\nbetter ﬁtting of the training dataset. Regarding the setting of\nthese two parameters, our suggestion is to apply a large ϵ(>0)\nwith a small α(<2). The optimal combination can be searched\nunder this principle.\nWith ϵ = 1,α = 1, We ﬁne-tune the best model obtained\nin the learning stage, i.e., SEDT-AQ-P2, by the one-to-many\nmatching method to get the ﬁnal model which is referred to\nas SEDT-AQ-FT-Pm, where m denotes that the event-level\nclassiﬁcation fusion strategy mis applied. As shown in Table\nIII, SEDT-AQ-FT-P1 is 2.27% higher than SEDT-AQ-P2 on\nthe event-based macro F1 and 0.89% higher on the segment-\nbased macro F1. To show the impact of one-to-many matching\non the model more intuitively, we visualize the distribution of\nthe predictions obtained by SEDT-AQ-FT-P2 and SEDT-AQ-\nP2 for the same audios in Figure 5. It can be seen that after\nﬁne-tuning, the distribution of boxes is more concentrated, that\nis, there will be several predictions targeting the same ground\ntruth event, leading to the reduction of results predicted as\n“empty” class. One-to-many matching also helps to predict\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nSEDT-AQ-P2SEDT-AQ-FT-P2Ground TruthExample (a) Example (b) Example (c) Example (d)12345678910123456789101234567891012345678910\nFig. 5. Visualization of predictions, where the horizontal axis represents time, and the vertical axis represents the index of predictions, numbered from 1 to\n10 from top to bottom. The boxes with the dashed border represent the predictions that are deleted in the de-overlapping phase.\nmore accurate boundaries, such as the predictions for the\nground truth event “dog bark” with index 1 in example (b).\nWhat’s more, it helps to generate correct predictions which\nare omitted by SEDT-AQ-P2, such as the predictions for the\nground truth event “street music” with index 1 in example (a)\nand the predictions for the ground truth event “dog bark” with\nindex 4 in example (c).\nAfter ﬁne-tuning, the rules of P and R over different\nstrategies described above remain, but the model with the\nbest F1 score has changed. For SEDT-AQ-P m, the model\nwith strategy 2 gets the highest event-based macro F1, while\nafter ﬁne-tuning, strategy 1 performs best. The reason may be\nthat the essence of ﬁne-tuning is to improve R by mitigating\npredictions that are labeled as “empty” class, while strategy\n1 focuses on improving P by deleting possible wrong predic-\ntions, their combination can complement each other to achieve\nthe best performance. However, for Strategy 2, it already takes\nboth R and P into account, after ﬁne-tuning, it pays too much\nattention to R, therefore lowering F1.\nVI. C ONCLUSIONS\nIn this paper, based on the characteristics of the one-\ndimensional signals, we present the one-dimensional Detection\nTransformer (1D-DETR) aiming at event-based SED, which is\ninspired by Detection Transformer for image object detection.\nWe further propose the audio query branch and one-to-many\nmatching strategy, obtaining Sound Event Detection Trans-\nformer (SEDT). To our knowledge, SEDT is the ﬁrst end-to-\nend SED model, which gets rid of the frame-based prediction\nmanner and generates event-level output directly. Experiments\non the URBAN-SED dataset and the DCASE2019 Task4\ndataset show that it can achieve competitive performance. For\nweakly supervised learning, SEDT relies on synthetic data to\nobtain localization ability. In the future, we will attempt to\ntrain SEDT to locate random cropped patches so that local-\nization ability can be obtained without any strongly-labeled\ndata, either real recordings or synthetic clips, realizing totally\nweakly supervised learning. The semi-supervised learning and\nunsupervised learning methods for SEDT are also our future\nresearch interests.\nREFERENCES\n[1] J. P. Bello, C. Silva, O. Nov, R. L. DuBois, A. Arora, J. Salamon,\nC. Mydlarz, and H. Doraiswamy, “Sonyc: A system for the monitor-\ning, analysis and mitigation of urban noise pollution,” arXiv preprint\narXiv:1805.00889, 2018.\n[2] S. Goetze, J. Schroder, S. Gerlach, D. Hollosi, J.-E. Appell, and\nF. Wallhoff, “Acoustic monitoring and localization for social care,”\nJournal of Computing Science and Engineering, vol. 6, no. 1, pp. 40–50,\n2012.\n[3] M. Crocco, M. Cristani, A. Trucco, and V . Murino, “Audio surveillance:\nA systematic review,” ACM Computing Surveys (CSUR) , vol. 48, no. 4,\npp. 1–46, 2016.\n[4] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen,\nR. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al. ,\n“Cnn architectures for large-scale audio classiﬁcation,” in 2017 ieee\ninternational conference on acoustics, speech and signal processing\n(icassp). IEEE, 2017, pp. 131–135.\n[5] E. Cakir, T. Heittola, H. Huttunen, and T. Virtanen, “Polyphonic sound\nevent detection using multi label deep neural networks,” in 2015\ninternational joint conference on neural networks (IJCNN) . IEEE,\n2015, pp. 1–7.\n[6] H. Zhang, I. McLoughlin, and Y . Song, “Robust sound event recognition\nusing convolutional neural networks,” in 2015 IEEE international con-\nference on acoustics, speech and signal processing (ICASSP) . IEEE,\n2015, pp. 559–563.\n[7] N. Turpault, R. Serizel, J. Salamon, and A. P. Shah, “Sound event\ndetection in domestic environments with weakly labeled data and\nsoundscape synthesis,” 2019.\n[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems , 2017, pp. 5998–6008.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\n[9] K. Miyazaki, T. Komatsu, T. Hayashi, S. Watanabe, T. Toda, and\nK. Takeda, “Convolution augmented transformer for semi-supervised\nsound event detection,” in Proc. Workshop Detection Classiﬁcation\nAcoust. Scenes Events (DCASE) , 2020, pp. 100–104.\n[10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y . Wu et al., “Conformer: Convolution-augmented\ntransformer for speech recognition,” arXiv preprint arXiv:2005.08100 ,\n2020.\n[11] L. Lin, X. Wang, H. Liu, and Y . Qian, “Guided learning convolution\nsystem for dcase 2019 task 4,” inProceedings of the Detection and Clas-\nsiﬁcation of Acoustic Scenes and Events 2019 Workshop (DCASE2019) ,\n2019, pp. 134–138.\n[12] A. Neubeck and L. Van Gool, “Efﬁcient non-maximum suppression,” in\n18th International Conference on Pattern Recognition (ICPR’06), vol. 3.\nIEEE, 2006, pp. 850–855.\n[13] L. Cai, B. Zhao, Z. Wang, J. Lin, C. S. Foo, M. S. Aly, and\nV . Chandrasekhar, “Maxpoolnms: getting rid of nms bottlenecks in two-\nstage object detectors,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 9356–9364.\n[14] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” arXiv\npreprint arXiv:2005.12872, 2020.\n[15] T. Heittola, A. Mesaros, A. Eronen, and T. Virtanen, “Context-dependent\nsound event detection,” EURASIP Journal on Audio, Speech, and Music\nProcessing, vol. 2013, no. 1, pp. 1–13, 2013.\n[16] H. Phan, L. Hertel, M. Maass, and A. Mertins, “Robust audio event\nrecognition with 1-max pooling convolutional neural networks,” arXiv\npreprint arXiv:1604.06338, 2016.\n[17] E. Cakır, G. Parascandolo, T. Heittola, H. Huttunen, and T. Virtanen,\n“Convolutional recurrent neural networks for polyphonic sound event\ndetection,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 25, no. 6, pp. 1291–1303, 2017.\n[18] Y . Li, M. Liu, K. Drossos, and T. Virtanen, “Sound event detection\nvia dilated convolutional recurrent neural networks,” in ICASSP 2020 -\n2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2020, pp. 286–290.\n[19] S. Adavanne, P. Pertil ¨a, and T. Virtanen, “Sound event detection using\nspatial features and convolutional recurrent neural network,” in 2017\nIEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2017, pp. 771–775.\n[20] K. Miyazaki, T. Komatsu, T. Hayashi, S. Watanabe, T. Toda, and\nK. Takeda, “Weakly-supervised sound event detection with self-\nattention,” in ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) . IEEE, 2020, pp.\n66–70.\n[21] L. Cances, P. Guyot, and T. Pellegrini, “Evaluation of post-processing al-\ngorithms for polyphonic sound event detection,” in2019 IEEE Workshop\non Applications of Signal Processing to Audio and Acoustics (WASPAA).\nIEEE, 2019, pp. 318–322.\n[22] Y . Hou and S. Li, “Semi-supervised sound event detection with convo-\nlutional recurrent neural network using weakly labelled data,” DCASE\nChallenge, Woking, Tech. Rep, 2018.\n[23] O. Maron and T. Lozano-P ´erez, “A framework for multiple-instance\nlearning,” in Proceedings of the 1997 Conference on Advances in Neural\nInformation Processing Systems 10 , ser. NIPS ’97. Cambridge, MA,\nUSA: MIT Press, 1998, p. 570–576.\n[24] B. Mcfee, J. Salamon, and J. P. Bello, “Adaptive pooling operators\nfor weakly labeled sound event detection,” IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) , 2018.\n[25] Y . Wang, J. Li, and F. Metze, “A comparison of ﬁve multiple instance\nlearning pooling functions for sound event detection with weak labeling,”\nin ICASSP 2019 - 2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2019.\n[26] L. Lin, X. Wang, H. Liu, and Y . Qian, “Specialized decision surface\nand disentangled feature for weakly-supervised polyphonic sound event\ndetection,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 28, pp. 1466–1478, 2020.\n[27] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for\nsemantic segmentation,” in 2015 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2015.\n[28] L. Lin, X. Wang, H. Liu, and Y . Qian, “Guided learning for weakly-\nlabeled semi-supervised sound event detection,” in ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2020, pp. 626–630.\n[29] Y . Huang, X. Wang, L. Lin, H. Liu, and Y . Qian, “Multi-branch learning\nfor weakly-labeled sound event detection,” in ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2020, pp. 641–645.\n[30] H.-W. Park, S. Yun, J. Eum, J. Cho, and K. Hwang, “Weakly labeled\nsound event detection using tri-training and adversarial learning,” ArXiv,\nvol. abs/1910.06790, 2019.\n[31] W. Wei, H. Zhu, E. Benetos, and Y . Wang, “A-crnn: A domain adaptation\nmodel for sound event detection,” in ICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2020, pp. 276–280.\n[32] Y . Huang, L. Lin, X. Wang, H. Liu, Y . Qian, M. Liu, and K. Ouchi,\n“Learning generic feature representation with synthetic data for weakly-\nsupervised sound event detection by inter-frame distance loss,” arXiv\npreprint arXiv:2011.00695, 2020.\n[33] S. Cornell, M. Olvera, M. Pariente, G. Pepe, E. Principi, L. Gabrielli,\nand S. Squartini, “Domain-Adversarial Training and Trainable Parallel\nFront-end for the DCASE 2020 Task 4 Sound Event Detection\nChallenge,” in DCASE 2020 - 5th Workshop on Detection and\nClassiﬁcation of Acoustic Scenes and Events , Virtual, Japan, Nov.\n2020. [Online]. Available: https://hal.inria.fr/hal-02962911\n[34] J. Salamon, D. MacConnell, M. Cartwright, P. Li, and J. P. Bello,\n“Scaper: A library for soundscape synthesis and augmentation,” in 2017\nIEEE Workshop on Applications of Signal Processing to Audio and\nAcoustics (WASPAA). IEEE, 2017, pp. 344–348.\n[35] R. Serizel, N. Turpault, H. Eghbal-Zadeh, and A. P. Shah, “Large-\nscale weakly labeled semi-supervised sound event detection in domestic\nenvironments,” 2018.\n[36] R. Serizel, N. Turpault, A. Shah, and J. Salamon, “Sound event detection\nin synthetic domestic environments,” in ICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2020.\n[37] D. M. Beck and S. Kastner, “Top-down and bottom-up mechanisms in\nbiasing competition in the human brain,”Vision research, vol. 49, no. 10,\npp. 1154–1165, 2009.\n[38] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:\nDeformable transformers for end-to-end object detection,”arXiv preprint\narXiv:2010.04159, 2020.\n[39] Z. Dai, B. Cai, Y . Lin, and J. Chen, “Up-detr: Unsupervised\npre-training for object detection with transformers,” arXiv preprint\narXiv:2011.09094, 2020.\n[40] M. Lin, C. Li, X. Bu, M. Sun, and Z. Deng, “Detr for pedestrian\ndetection,” 2020.\n[41] E. C. Cherry, “Some experiments on the recognition of speech, with one\nand with two ears,” J.acoust.soc.america, vol. 26, no. 5, 2005.\n[42] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016, pp. 770–778.\n[43] H. W. Kuhn, “The hungarian method for the assignment problem,” Naval\nresearch logistics quarterly, vol. 2, no. 1-2, pp. 83–97, 1955.\n[44] J. Salamon, C. Jacoby, and J. P. Bello, “A dataset and taxonomy for\nurban sound research,” in acm International Conference on Multimedia ,\n2014.\n[45] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\narXiv preprint arXiv:1711.05101 , 2017.\n[46] A. Mesaros, T. Heittola, and T. Virtanen, “Metrics for polyphonic sound\nevent detection,” Applied Sciences, vol. 6, no. 6, p. 162, 2016.\n[47] I. Mart ´ın-Morat´o, A. Mesaros, T. Heittola, T. Virtanen, M. Cobos, and\nF. J. Ferri, “Sound event envelope estimation in polyphonic mixtures,” in\nICASSP 2019-2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2019, pp. 935–939.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nZhirong Ye received the B.E. degree in Communication Engineering from\nWuhan University, Wuhan, China, in 2019. She is currently pursuing an\nM.E. degree in Computer Science and Technology in Institute of Computing\nTechnology, Chinese Academy of Sciences, Beijing, China. Her research\ninterest includes audio signal processing and machine learning.\nXiangdong Wang is an associate professor in Institute of Computing Tech-\nnology, Chinese Academy of Sciences, Beijing, China. He received Doctors\ndegree in Computer Science at Institute of Computing Technology, Chinese\nAcademy of Sciences, Beijing, China, in 2007. His research ﬁeld includes\nhuman computer interaction, speech recognition and audio processing.\nHong Liu is an associate professor in Institute of Computing Technology,\nChinese Academy of Sciences, Beijing, China. She received her Doctors\ndegree in Computer Science at Institute of Computing Technology, Chinese\nAcademy of Sciences, Beijing, China, in 2007. Her research ﬁeld includes\nhuman computer interaction, multimedia technology, and video processing.\nYueliang Qian is a professor in Institute of Computing Technology, Chinese\nAcademy of Sciences, Beijing, China. He received his Bachelors degree in\nComputer Science at Fudan University, Shanghai, China in 1983. His research\nﬁeld includes human computer interaction and pervasive computing.\nRui Tao is a researcher in Toshiba China R&D Center, Beijing, China.\nShe received Bachelors degree in Communication Engineering at Tianjin\nUniversity of Commerce, Tianjin, China, in 2017. She is currently pursuing\nan M.E degree at University of Chinese Academy of Sciences. Her research\nﬁeld includes human computer interaction and speech recognition.\nLong Yan is a researcher in Toshiba China R&D Center, Beijing, China.\nHe received Doctors degree in signal and information processing at Beijing\nUniversity of Posts and Telecommunications, Beijing, China, in 2005. His\nresearch ﬁeld includes human computer interaction, speech recognition and\naudio processing.\nKazushige Ouchi received B.E. and M.E. degrees in 1996 and 1998 from\nWaseda University, Tokyo, Japan and then afﬁliated with Toshiba Corporation.\nHe received Ph.D. from Waseda University in 2017. He used to be a chief\nresearch scientist at Toshiba Corporate R&D Center and is currently a vice\npresident of Toshiba (China) Co., Ltd., and a director of Toshiba China\nR&D Center. He is engaged in research on human interface using various AI\ntechnologies. He is a board member of the Academy of Human Informatics\nand is a senior member of the Information Society of Japan (IPSJ). He\nhas been awarded several prizes: IPSJ Nagao Special Research Award, IPSJ\nYamashita SIG Research Award, the Best Paper Award from Human Interface\nSociety, and so on.",
  "topic": "End-to-end principle",
  "concepts": [
    {
      "name": "End-to-end principle",
      "score": 0.6829168200492859
    },
    {
      "name": "Computer science",
      "score": 0.6585496068000793
    },
    {
      "name": "Transformer",
      "score": 0.6509295701980591
    },
    {
      "name": "Event (particle physics)",
      "score": 0.5161467790603638
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4812574088573456
    },
    {
      "name": "Object detection",
      "score": 0.4693965017795563
    },
    {
      "name": "Frame (networking)",
      "score": 0.4666896462440491
    },
    {
      "name": "Data mining",
      "score": 0.4014309048652649
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.374031662940979
    },
    {
      "name": "Speech recognition",
      "score": 0.3220231831073761
    },
    {
      "name": "Engineering",
      "score": 0.1552673578262329
    },
    {
      "name": "Telecommunications",
      "score": 0.12906178832054138
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}