{
  "title": "FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting",
  "url": "https://openalex.org/W3196995472",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1930211867",
      "name": "Liu Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2341874967",
      "name": "Deng, Hanming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353896415",
      "name": "Huang Yang-yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2336906064",
      "name": "Shi Xiaoyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4295680286",
      "name": "Lu, Lewei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1930196199",
      "name": "Sun, Wenxiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1832420164",
      "name": "Wang Xiao-gang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2345450207",
      "name": "Dai, Jifeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969345873",
      "name": "Li Hongsheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3104310079",
    "https://openalex.org/W2551763541",
    "https://openalex.org/W1975049209",
    "https://openalex.org/W2970400386",
    "https://openalex.org/W3123547918",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W2886787375",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2470139095",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W2986330308",
    "https://openalex.org/W2807633959",
    "https://openalex.org/W2295936755",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W3043547428",
    "https://openalex.org/W2069237980",
    "https://openalex.org/W2986433113",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2954284167",
    "https://openalex.org/W2890447039",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W2156235915",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2738588019",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094825490",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W1993120651",
    "https://openalex.org/W2798365772",
    "https://openalex.org/W2987614525",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2969109746",
    "https://openalex.org/W2886714066",
    "https://openalex.org/W2210871532",
    "https://openalex.org/W3135404760",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2988407809",
    "https://openalex.org/W1999360130"
  ],
  "abstract": "Transformer, as a strong and flexible architecture for modelling long-range relations, has been widely explored in vision tasks. However, when used in video inpainting that requires fine-grained representation, existed method still suffers from yielding blurry edges in detail due to the hard patch splitting. Here we aim to tackle this problem by proposing FuseFormer, a Transformer model designed for video inpainting via fine-grained feature fusion based on novel Soft Split and Soft Composition operations. The soft split divides feature map into many patches with given overlapping interval. On the contrary, the soft composition operates by stitching different patches into a whole feature map where pixels in overlapping regions are summed up. These two modules are first used in tokenization before Transformer layers and de-tokenization after Transformer layers, for effective mapping between tokens and features. Therefore, sub-patch level information interaction is enabled for more effective feature propagation between neighboring patches, resulting in synthesizing vivid content for hole regions in videos. Moreover, in FuseFormer, we elaborately insert the soft composition and soft split into the feed-forward network, enabling the 1D linear layers to have the capability of modelling 2D structure. And, the sub-patch level feature fusion ability is further enhanced. In both quantitative and qualitative evaluations, our proposed FuseFormer surpasses state-of-the-art methods. We also conduct detailed analysis to examine its superiority.",
  "full_text": "FuseFormer: Fusing Fine-Grained Information\nin Transformers for Video Inpainting\nRui Liu†* Hanming Deng‡* Yangyi Huang‡§* Xiaoyu Shi† Lewei Lu‡\nWenxiu Sun‡♯ Xiaogang Wang† Jifeng Dai‡ Hongsheng Li†#\n†CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong ‡SenseTime Research\n§Zhejiang University ♯Tetras.AI #School of CST, Xidian University\n{ruiliu@link, xiaoyushi@link, xgwang@ee, hsli@ee}.cuhk.edu.hk\n{denghanming, huangyangyi, luotto, daijifeng}@sensetime.com\nAbstract\nTransformer, as a strong and ﬂexible architecture for\nmodelling long-range relations, has been widely explored\nin vision tasks. However, when used in video inpainting\nthat requires ﬁne-grained representation, existed method\nstill suffers from yielding blurry edges in detail due to the\nhard patch splitting. Here we aim to tackle this problem\nby proposing FuseFormer, a Transformer model designed\nfor video inpainting via ﬁne-grained feature fusion based\non novel Soft Split and Soft Composition operations. The\nsoft split divides feature map into many patches with given\noverlapping interval. On the contrary, the soft composi-\ntion operates by stitching different patches into a whole fea-\nture map where pixels in overlapping regions are summed\nup. These two modules are ﬁrst used in tokenization be-\nfore Transformer layers and de-tokenization after Trans-\nformer layers, for effective mapping between tokens and\nfeatures. Therefore, sub-patch level information interaction\nis enabled for more effective feature propagation between\nneighboring patches, resulting in synthesizing vivid content\nfor hole regions in videos. Moreover, in FuseFormer, we\nelaborately insert the soft composition and soft split into\nthe feed-forward network, enabling the 1D linear layers to\nhave the capability of modelling 2D structure. And, the\nsub-patch level feature fusion ability is further enhanced.\nIn both quantitative and qualitative evaluations, our pro-\nposed FuseFormer surpasses state-of-the-art methods. We\nalso conduct detailed analysis to examine its superiority.\nCode and pretrained models are available at https://\ngithub.com/ruiliu-ai/FuseFormer.\n*The ﬁrst three authors contribute equally to this work.\nFigure 1. Illustration of different patch split/composition strategies\nfor Transformer model. The top row shows hard split/composition,\nbased on which the trained model generates rough inpainting re-\nsults. The bottom row shows soft split/composition, based on\nwhich the trained model generates smooth results due to interac-\ntion of features between neighbor patches. Double arrow indicates\nthe corresponding overlapped regions between adjacent patches.\n1. Introduction\nTransformer has recently gained increasing attention in\nvarious vision tasks such as classiﬁcation [8, 42], object de-\ntection [28, 47] and image generation [18, 16]. Interest-\ningly, Transformer is suitable to video inpainting, a vision\ntask that depends on the information propagation between\nﬂowing pixels across frames to ﬁll the spatiotemporal holes\nwith plausible and coherent content in a video clip.\nSpatial Temporal Transformer Net (STTN) [43] is the pi-\noneer work for investigating the use of Transformer in video\ninpainting. However, its multi-scale variant of self-attention\nintertwined with fully convolutional networks makes it hard\nto exploit rich experience from other Transformer models\ndue to large structural differences. On the other hand, re-\narXiv:2109.02974v1  [cs.CV]  7 Sep 2021\ncent Vision Transformer (ViT) [8] demonstrates the strong\ncapability of vanilla Transformer [34] in vision recognition\ntask. These motivate us to build a Video inpainting Base-\nline with vanilla Transformer (ViB-T), which differs from\nViT in 2 aspects: a) the tokens are embedded from patches\nof multiple frames instead of a single frame; b) a light\nconvolutional encoder and decoder before and after Trans-\nformer block is exploited to relieve the computational bur-\nden caused by high resolution frames. Experiment veriﬁes\nthat this simple baseline can reach competitive performance\nwith STTN [43] under similar computation cost.\nNevertheless, similar to all existing patch-based Trans-\nformer models [8, 42], the hard split operation used in ViB-\nT makes it unable to effectively encode sub-token (sub-\npatch) level representations. Since the attention score is\ncalculated between different tokens, there is no direct sub-\ntoken level feature interaction. For us, human beings, frag-\nmenting an image into many non-overlapping patches poses\na challenging task to composite them back into an origi-\nnal image with masked regions ﬁlled. This is the same for\ndeep learning systems: the lack of accurate sub-token level\nfeature interaction can lead to inconsistent content between\nneighboring patches. As shown in Fig.1, to accurately re-\nbuild the black circle on the canvas, every token correspond-\ning to an image patch has to understand not only the patch\nlevel information but also sub-patch level information. As\na result, in order to fully unleash the power of Transformers\nin video inpainting tasks, an improved patch splitting man-\nner and a better sub-token level feature fusion mechanism\nto maintain pixel level-feature accuracy is in demand.\nTo achieve this goal, we propose a Soft Split (SS) module\nas well as its corresponding Soft Composition (SC) module.\nBuilt upon the simple and straightforward ViB-T baseline\nmodel, we propose to softly split images into patches with\noverlapping regions and correspondingly, to softly compos-\nite these overlapped patches back to images. Speciﬁcally,\nin the soft split module, we exploit anunfold operation with\nkernel size greater than stride to softly split the input im-\nage into overlapping 2D patches and are ﬂattened as 1D\ntokens. On the contrary, in the soft composition module,\ntokens are reshaped to 2D patches maintaining their orig-\ninal sizes, and then each pixel is registered to its original\nspatial location according to the kernel size and stride used\nin soft split module. During this process, features of the\npixels located in the overlapping area are fused from multi-\nple overlapping neighboring patches’ corresponding areas,\nthus providing sub-token level feature fusion. We design a\nbaseline ViB-T model equipped with the Soft Split and Soft\nComposition modules as ViB-S where S stands for soft op-\nerations. And we ﬁnd that the ViB-S model easily surpasses\nthe state-of-the-art video inpainting model STTN [43] with\nminimum extra computation cost.\nFinally, we propose a Fusion Feed Forward Network\n(F3N) to replace the two-layer MLPs in the standard Trans-\nformer model, which is dubbed as FuseFormer, to fur-\nther improve its sub-token fusion ability for learning ﬁne-\ngrained feature, yet without extra parameters. In the F3N,\nbetween the two fully-connected layers, we reshape each\n1D token back to 2D patch with its original spatial shape\nand then softly composite them to be a whole image.\nThe overlapping features of pixel at overlapping regions\nwould sum up the corresponding value from all neighbor-\ning patches for further ﬁne-grained feature fusion. Then the\npatches are softly split and ﬂattened into 1Dvectors, which\nare fed to the second MLP. In this way, sub-token segment\ncorresponding to the same pixel location are matched and\nregistered without extra learnable parameters, and informa-\ntion of the same pixel location from different patches are\naggregated. Subsequently, our FuseFormer model consist-\ning of F3N even surpasses our strong baseline ViB-S by a\nsigniﬁcant margin, both qualitatively and quantitatively.\nBased on these novel designs, our proposed FuseFormer\nnetwork achieves effective and efﬁcient performance in\nvideo restoration and object removal. We testify the supe-\nriority of the proposed model to other state-of-the-art video\ninpainting approaches by thorough qualitative and quanti-\ntative comparisons. We further conduct ablation study to\nshow how each component of our model beneﬁts the in-\npainting performance.\nIn summary, our contributions are three-fold:\n1. We ﬁrst propose a simple yet strong Transformer base-\nline for video inpainting, and propose a soft split and\ncomposition method to boost its performance.\n2. Based on the proposed strong baseline and novel soft\noperations, we propose FuseFormer, a sub-token fu-\nsion enabled Transformer model with no extra param-\neters.\n3. Extensive experiments demonstrate the superiority of\nFuseFormer over state-of-the-art approaches in video\ninpainting, both qualitatively and quantitatively.\n2. Related work\nImage Inpainting. In traditional image inpainting, the tar-\nget holes are usually ﬁlled by sampling and pasting the\nknown textures and signiﬁcant progress has been made on\nthis type of image inpainting approach [2, 3, 6, 9, 10].\nPatchMatch [1] proposes to ﬁll the missing region by\nsearching the patches outside the hole based on the approx-\nimate nearest neighbor algorithm, which is ﬁnally served as\na commercial product.\nWith the rise of deep neural network [21, 13] and gen-\nerative adversarial network [12], some works investigated\non building an end-to-end deep neural network for image\ninpainting task with the auxiliary discriminator and adver-\nsarial loss [30, 17]. After that, DeepFill propose to use a\ncontextual attention for ﬁlling target holes by propagating\nthe feature outside the region [41]. Then Liu et al. and\nYu et al. apply partial convolution [25] and gated convolu-\ntion [40] to make vanilla convolution kernels aware of given\nmask guidance respectively, so as to complete free-form im-\nage inpainting.\nVideo Inpainting. Building upon patch-based image in-\npainting, Newson et al. extend PatchMatch algorithm [1] to\nvideo for further modelling the temporal dependencies and\naccelerating the process of patch matching [27]. Strobel et\nal. [33] introduce an accurate motion ﬁeld estimation for\ncapturing object movement. Huang et al. perform an alter-\nnate optimization on 3 steps including patch search, color\ncompletion and motion ﬁeld estimation and obtain success-\nful video completion performance [15].\nDeep learning also boosts the performance of video in-\npainting. Wang et al. proposes a groundbreaking deep neu-\nral network that combines 2D and 3D convolution seam-\nlessly for completing missing contents in video [35]. Kim\net al. propose a recurrent neural network to cumulatively\naggregate temporal features through traversing all video se-\nquences [19]. Xu et al. use existing ﬂow extraction tools to\nobtain robust optical ﬂow and then warp the regions from\nreference frames to ﬁll the hole in target frame [39]. Lee\net al. propose a copy-and-paste network that learns to copy\ncorresponding contents in reference frames and paste them\nto ﬁll the holes in the target frame [23]. Chang et al. de-\nvelop a learnable Gated Temporal Shift Module and adapt\ngated convolution[40] to a 3D version for performing free-\nform video inpainting [5, 4]. Zhang et al. adopts internal\nlearning to train one-size-ﬁts-all model for different given\nvideos [44]. Hu et al. propose a region proposal-based strat-\negy for picking up best inpainted result from many partic-\nipants [14]. Recently attention mechanisms are adopted to\nfurther promote both realism and temporal consistency via\ncapturing long-range correspondences in video sequences.\nTemporally-consistent appearance is implicitly learned and\npropagated to the target frame with a frame-level atten-\ntion [29] and dynamic long-term context aggregation mod-\nule [24].\nTransformers in Vision. Transformers are ﬁrstly proposed\nin 2017 [34] and gradually dominated natural language pro-\ncessing models [7, 32, 26]. A Transformer block basically\nconsists of a multi-head attention module for modelling\nlong-range correspondence of the input vector and a multi-\nlayer perceptron for fusing and reﬁning the feature repre-\nsentation. In computer vision, it has been adapted to various\ntasks such image classiﬁcation [8, 42], object detection and\nsegmentation [28, 47, 45, 11], image generation [18, 16],\nvideo segmentation [37], video captioning [46] and so on in\npast two years.\nAs far as our knowledge concerns, STTN [43] is the only\nwork for investigating the use of Transformer in video in-\npainting and propose to learn a deep generative Transformer\nmodel along spatial-temporal dimension. It roughly splits\nframes into non-overlapped patches with certain given patch\nsize and then feeds the obtained spatiotemporal patches into\na stack of Transformer encoder blocks for thorough spa-\ntiotemporal propagation. However, it suffers from captur-\ning local texture like edges and lines and modelling the\narbitrary pixel ﬂowing. In this work, we propose a novel\nTransformer-based video inpainting framework endorsed\nby 2 carefully-designed soft operations, which improve the\nperformance on both video restoration and object removal\nand make the inference much faster as well.\n3. Method\nIn this section we introduce our FuseFormer model for\nvideo inpainting. We start by proposing a simple Trans-\nformer baseline, named ViB-T (Video inpainting Baseline\nwith vanilla Transformer), then we introduce our novel de-\nsigns step by step by ﬁrst introducing our Soft Split (SS) and\nSoft Composition (SC) technique, which boost the perfor-\nmance of ViB-T. We term ViB-T with SS and SC as ViB-\nS. Finally, build upon ViB-S, we introduce FuseFormer, a\nﬁne-grained vision Transformer block whose regular feed\nforward network is replaced with fusion feed forward net-\nwork, and term the ﬁnal model as ViF (Video inpainting\nwith FuseFormer).\n3.1. Video inpainting Baseline with Transformer\nWe start by proposing a straightforward baseline model\nViB-T for directly deploying patch-based Transformer in\nvideo inpainting without complex modiﬁcations. It consists\nof three parts: a) a convolutional encoder and a correspond-\ning decoder; b) a stack of Transformer blocks between the\nencoder and decoder; and c) a pair of patch-to-token and\ntoken-to-patch module. The patch-to-token module locates\nbetween the convolutional encoder and the ﬁrst Transformer\nblock, and token-to-patch locates between the last Trans-\nformer block and the convolutional decoder. Different from\nSTTN [43], this baseline model’s Transformer block is the\nsame as standard Transformer [34] where there is neither\nthe scheme of multi-scale frames for different multi-head\nself-attention nor using 3 ×3 convolution to replace linear\nlayers in feed forward network. Patches are hard split from\nfeature map and linearly embedded to feature vectors with\nmuch lower channel dimension, which is more computa-\ntionally friendly for following processing.\nAs shown in Fig. 2, given corrupted video frames fi ∈\nRh×w×3,i ∈[0,t), it would work as follows:\nFirst, it encodes video frames with a CNN encoder,\nobtaining c channel convolutional feature maps of frames\nXi ∈Rh/4×w/4×c,i ∈[0,t), and each X is split into k×k\nFigure 2. Illustrations of our proposed FuseFormer. On the left is our proposed video inpainting pipeline with Transformers. On the right is\nour proposed FuseFormer block and Fusion Feed Forward Network (F3N). The tuple indicates the counting number of patch along spatial\ndimension.\nsmaller patches with stride s. Then all patches are linearly\nembedded into tokensZ ∈R(t·n)×d, where nis the number\nof tokens in one image and dis the token channel.\nSecond, Z is fed into standard Transformer blocks for\nspatial-temporal information propagation, resulting in re-\nﬁned tokens ˜Z ∈R(t·n)×d.\nThird, each reﬁned token ˜zi ∈Rd,i ∈[0,n ·t) from\n˜Z is linearly transformed to k ·k ·c channel vector and\nreshaped to patch shape k×k×c. All the resulting patches\nare registered back to its original frame’s location pixel by\npixel, obtaining feature maps ˜Xi ∈Rh/4×w/4×c,i ∈[0,t).\nThis re-composited feature map is of the same size as the\nfeature map input to the ﬁrst Transformer block.\nFinally, the re-composited feature maps ˜X are decoded\nwith a couple of deconvolution layers to output the in-\npainted video frames ˜fi ∈Rh×w×3,i ∈[0,t) with original\nsize.\nFor the baseline model ViB-T, we set kernel size equal\nto the stride in patch splitting. As a starting point, this sim-\nple model already has competitive performance with STTN\n[43] but with faster inference speed and fewer parameters\n(refer to appendix C).\nThe key of our proposed method is the sub-token level\nﬁne-grained feature fusion, which is realized by the newly-\nproposed Soft Split (SS) and Soft Composite (SC) process-\ning, it enables precise sub-token level fusion between neigh-\nboring patches. In the following section, we will ﬁrst intro-\nduce the SS and SC modules, based on which we introduce\nour proposed FuseFormer in section 3.3.\n3.2. Soft Split (SS) and Soft Composite (SC)\nDifferent from STTN [43] that roughly split frames into\npatches without overlapping region, here we propose to\nFigure 3. The illustration of Soft Split (SS) and Soft Composite\n(SC) module.\nsoftly split each frame into overlapped patches and then\nsoftly composite them back, by using an unfold and fold\noperator with patch size k being greater than patch stride\ns. When compositing patches back to its original spatial\nshape, we add up feature values at each overlapping spatial\nlocation of neighboring patches.\nSoft Split (SS). As shown in Fig. 3, it softly split each\nfeature map into overlapped patches of sizek×kwith stride\ns < k, and ﬂattened to a one-dimensional token, which is\nsimilar to the image spliting strategy in T2T-ViT [42]. The\nnumber of tokens is then\nn= ⌊h+ 2·p−k\ns + 1⌋×⌊ w+ 2·p−k\ns + 1⌋, (1)\nwhere pis the padding size.\nSoft Composite (SC). The SC operator composites the\nsoftly split npatches by their original spatial location and\nform a new feature map with the same hand was original\nfeature map size. However, due to the existence of overlap-\nping area, the SC operator sums up pixel values that over-\nlapped on the same spatial location, as shown in Fig. 3.\nThis design of soft split and composition lays founda-\ntion for our ﬁnal FuseFormer, as when softly compositing\npatches back to its original position after Transformer pro-\ncessing, the overlapped position aggregated a piece of in-\nformation from different tokens, contributing to smoother\npatch boundaries and enlarges its receptive ﬁeld by fus-\ning information from neighboring patches. As our exper-\niment shows, the baseline model equipped with these two\noperators, dubbed as ViB-S, have already surpassed the\nstate-of-the-art video inpainting performance reached by\nSTTN [43].\n3.3. FuseFormer\nA FuseFormer block is the same to standard Transformer\nblock except that feed forward network is replaced with our\nproposed Fusion Feed Forward Network (F3N). Given in-\nput patch tokens Zl at l-th stack where l ∈ [0,L), L is\nthe stacking number of FuseFormer blocks, a FuseFormer\nblock can be formulated as:\nZ′\nl = MSA(LN1(Zl−1)) +Zl, (2)\nZl+1 = F3N(LN2(Z′\nl)) +Z′\nl, (3)\nwhere the MSA and LN respectively denote standard multi-\nhead self-attention and layer normalization in Transform-\ners [34] and our key difference from other Transformers\nlies in the newly-proposed Fusion Feed Forward Network\n(F3N).\nFusion Feed Forward Network (F3N). F3N brings no\nextra parameter into the standard feed forward net and the\ndifference is that F3N inserts a SC and a SS operation be-\ntween the two layer of MLPs. For clear formulation, we let\nF′ = F3N(F) = F3N(LN2(Z′l)) where F,F′ ∈Rtn×d\nand the mapping functions are the same to Equ. 3. Let\nfi,f′\ni be the token vectors from F,F′ where i∈[0,t·n),\nso the F3N can be formulated as\npi = MLP1(fi), i ∈[0,t ·n) (4)\nAj = SC(pj,0,..., pj,n−1), j ∈[0,t) (5)\np′\nj,0,..., p′\nj,n−1 = SS(Aj), j ∈[0,t) (6)\nf′\ni = MLP2(p′\ni), i ∈[0,t ·n) (7)\nwhere MLP1 and MLP2 denote the vanilla multi-layer per-\nceptron. SC denotes soft composition for composing those\n1-D vectors pj,0,..., pj,n−1 to a 2-D feature map Aj and\nSS denotes the soft split for splitting Aj into 1-D vectors\np′\nj,0,..., p′\nj,n−1. Note that there is a feature fusion pro-\ncessing during the mapping p′\ni = SS(SC(pi)).\nBesides the introduction of soft composition and soft\nsplit module, there is another difference between F3N and\nFFN. In FFN, the input and output channel of MLP 1 and\nMLP2 are (4 ·d,d) and (d,4 ·d), respectively. On the\ncontrary, in F3N, we change the input and output chan-\nnel of the two MLPs to (d,k2 ·c′) and (k2 ·c′,d), where\nc′= 10·⌊4·d/(10·k2)⌋, which aims to ensure the interme-\ndiate feature vectors are able to be reshaped to feature 2-D\nmaps.\nFor each soft composition module in F3N, different pixel\nlocations may correspond to various number of overlapping\npatches, which leads to large variance on pixel value. Mean-\nwhile, the spatial location of the reshaped patch is actually\nmixed up after passing through the MLP 1. Therefore, we\nintroduce a normalization for Equ. 5. Let 1 ∈Rn×(k2·c′) be\nthe vectors where all elements’ value are 1, so the normal-\nized SC can be formulated as:\n˜Aj = SC(pj,0,..., pj,n−1)\nSC(1) ,j ∈[0,t) (8)\n3.4. Training Objective\nWe train our network by minimizing the following loss:\nL= λR ·LR + λadv ·Ladv, (9)\nwhere LR is the reconstruction loss for all pixels,Ladv is the\nadversarial loss from GAN [12], λR and λadv weigh the im-\nportance of different loss functions. For reconstruction loss,\nL1 loss is utilized for measuring the distance between syn-\nthesized video ˜Y and original one Y. It can be formulated\nas\nLR = ∥( ˜Y −Y)∥1 (10)\nIn addition, following [43], we also adopt a discrimina-\ntor D for assisting training the FuseFormer generator, in\norder to obtain a better synthesis realism and temporal con-\nsistency. This discriminator takes both real videos and syn-\nthesized ones as input and outputs a scalar ranging in [0,1]\nwhere 0 indicates fake and 1 indicates true. It is trained\ntoward the direction that all the synthesized videos could\nbe distinguished from real ones. The FuseFormer generator\nis trained towards an opposite direction where it generates\nvideos that can not be told byDanymore. The loss function\nfor Dis formulated as\nLD = EY [log D(Y] +E˜Y\n[\nlog (1−D( ˜Y))\n]\n(11)\nAnd the loss function for the FuseFormer generator is\nLadv = E˜Y\n[\nlog D( ˜Y)\n]\n(12)\nFigure 4. Qualitative results of our proposed ViB-S and ViF. Ref-\nerence denotes masked object found in the same video. Compared\nto STTN, with soft patch split/composition, our ViB-S can bet-\nter handle detail information. When replacing Transformer block\nin ViB-S with FuseFormer, ViF excels at recovering details and\nheavily occluded objects.\n4. Experiments\n4.1. Implementation details\nDataset. Following previous works [43, 23], we choose 2\nvideo object segmentation datasets for training and eval-\nuation. YouTube-VOS [38] contains 3,471, 474 and 508\nvideo clips in training, validation and test set, respectively.\nDAVIS [31], short for Densely Annotated Video Segmenta-\ntion, contains 150 video sequences in various scenes. Fol-\nlowing STTN [43], a test set including60 video clips is split\nfrom the whole dataset for fair comparison with other ap-\nproaches. We do not use this dataset for training.\nNetwork and training. We use 8 stacks of Transformer\n(FuseFormer) layers in our ViB-T, ViB-S and ViF models,\nwhose token dimension is 512. For ViF, the token is ex-\npanded to 1960 instead of 2048 for patch reshape compati-\nbility. Other network structures including the CNN encoder,\ndecoder and discriminator are the same as STTN [43], ex-\ncept that we insert several convolutional layers between en-\ncoder and the ﬁrst Transformer block to compensate for ag-\ngressive channel reduction in patch tokenization. Note that\ndifferent from STTN [43], we do not ﬁnetune our model\non DA VIS training set and the same checkpoint is used for\nevaluation on both YouTube-VOS test set and DA VIS test\nset. In all our ablations, we train our model with Adam op-\ntimizer [20] for 250k iterations. At each iteration, 5 random\nframes from one video is sampled on each GPU and8 GPU\nis utilized. The initial learning rate is0.01 and is reduced by\nfactor of 10 at 200k iteration. For our fair comparison with\nstate-of-the-art models, we train our best model for 500k it-\nerations, and the learning rate is reduced at 400k and 450k\niterations respectively.\nEvaluation metrics.First, we take Video-based Fr´echet In-\nModel Patch Size Overlap PSNR ↑ SSIM ↑\nSTTN [43] (5,9) ∗ no 30.67 0.9560\nViB-T (3,3) no 30.68 0.9569\nViB-T (5,5) no 30.56 0.9563\nViB-T (7,7) no 30.50 0.9559\nViB-S⊿ (7,7) yes 30.74 0.9577\nViB-S◁ (7,7) yes 30.99 0.9597\nViB-S (5,5) yes 30.91 0.9588\n(7,7) yes 31.02 0.9598\nViF† (7,7) yes 31.72 0.9654\nViF (7,7) yes 31.87 0.9662\nTable 1. Evaluation of our proposed SS, SC module and Fuse-\nFormer. All models except STTN use patch stride of 3. ViB-S ⊿\nand ViB-S◁ denotes using only SC or SS respectively. ViF †de-\nnote using F3N without normalizing in Equ.8 and ViF denote us-\ning F3N with normalizing. ∗: STTN uses multi-scale patch sizes\nand refer to [43] for more details.\nception Distance (VFID) as our metric for scoring the per-\nceptually visual quality by comparing with natural video se-\nquences [36, 43]. Lower value suggests better realism and\nvisually closer to natural videos. We also use a optical ﬂow-\nbased warping error Ewarp for measuring the temporal con-\nsistency [22]. Lower value indicates better temporal consis-\ntency. Finally, we use two popular metrics for measuring\nthe quality of reconstructed image compared with original\none: Structural SIMilarity (SSIM) and Peak Signal to Noise\nRatio (PSNR). The score is calculated frame by frame and\ntheir mean value is reported. Higher value of these two met-\nrics indicates better reconstruction quality.\n4.2. Ablations\nThe effectiveness of soft split and soft composition.In\nTab. 4.1 we show the performance under different patch\nsize used in soft split and soft composition operation on our\nbaseline model ViB-T and ViB-S. For ViB-T, we keep the\nstride the same as the patch size. For ViB-S and TiF, they\nshare the same stride 3 to ensure the same number of tokens\nfor each frames.\nFirst, by changing the patch size for ViB-T, we ﬁnd that\nViB-T with patch size3, a straight-forward variant of Trans-\nformer has already achieved competitive performance com-\npared to the state-of-the-art STTN [43], even without soft\nsplit and soft composition operations. For ViB-S and ViF,\nwhen patch size is larger than 3, SS and SC operations are\nincorporated to handle overlap area between patches. All\nlarger patches improves the performance for a signiﬁcant\nmargin, showing the effectiveness of overlapping patches.\nHere we further vary the patch size between SS and SC,\nlimiting the overlapping area to appear in either SS or SC\noperations. Apart from SS, the overlapped composition in\nSC can also improve the performance even without SS.\nFigure 5. Qualitative comparison with other methods.\nAccuracy\nYouTube-VOS DA VIS\nModels PSNR ↑ SSIM ↑ VFID ↓ Ewarp(×10−2) ↓ PSNR ↑ SSIM ↑ VFID ↓ Ewarp(×10−2) ↓\nVINet [19] 29.20 0.9434 0.072 0.1490 / - 28.96 0.9411 0.199 0.1785 / -\nDFVI [39] 29.16 0.9429 0.066 0.1509 / - 28.81 0.9404 0.187 0.1880 / 0.1608∗\nLGTSM [5] 29.74 0.9504 0.070 0.1859 / - 28.57 0.9409 0.170 0.2566 / 0.1640∗\nCAP [23] 31.58 0.9607 0.071 0.1470 / - 30.28 0.9521 0.182 0.1824 / 0.1533∗\nSTTN [43] 32.34 0.9655 0.053 0.1451 / 0.0884∗ 30.67 0.9560 0.149 0.1779 / 0.1449∗\nViB-S 32.47 0.9635 0.056 - / 0.0889∗ 31.50 0.9636 0.144 - / 0.1346∗\nViF 33.16 0.9673 0.051 - / 0.0875∗ 32.54 0.9700 0.138 - / 0.1336∗\nTable 2. Quantitative results of video completion on YouTube-VOS and DA VIS dataset.∗: our evaluation results following descriptions in\nSTTN [43], the numerical differences may result from different optical ﬂow models in the evaluation process.\nThe effectiveness of F3N in FuseFormer.As shown in\nTab.4.1, by replacing standard Transformer block with our\nproposed FuseFormer block in ViB-S, the performance is\nboosted signiﬁcantly, showing the effectiveness of sub-\ntoken level feature fusion. Moreover, with the proposed nor-\nmalizing technique in Equ.8, the performance has been fur-\nther improved. Compared to standard Transformer in video\ninpainting, FuseFormer has slightly fewer parameters and\nnegligible time cost but enabled the sub-token level ﬁne-\ngrain feature fusion.\nFig.4 further illustrates the qualitative results of VIB-S\nand ViF, demonstrating that their better performance comes\nfrom more detailed inpainting results, showing the effec-\ntiveness of sub-token level feature fusion.\nFigure 6. Image decoded from different layers of our trained ViF.\nIt shows that images are reﬁned in a coarse to ﬁne manner.\nFigure 7. Visualization of attention between patches cross multiple\nframes in object removal.\n4.3. Comparison with other methods\nQualitative comparison. In Fig.5 we show the qualitative\nresults of our model compared with state-of-the-art meth-\nods including CAP [23], LGTSM [5], and STTN [43] and\nour proposed FuseFormer synthesize the most realistic and\ntemporally-coherent videos.\nQuantitative comparison. In Tab.2 we show the perfor-\nmance comparison with state-of-the-art models on video\ncompletion, evaluated on both YouTubeVOS. Our ViF\nmodel outperforms all the state-of-the-art video inpainting\napproaches in video restoration by improving PSNR and\nSSIM by 3.3% and 0.7%, and it yields videos with best real-\nFigure 8. User study results. Percentage of ranking ﬁrst among 38\nviewers of 30 videos on video completion and object removal task.\nism and temporal coherence by reducing VFID and warping\nerror by 7.4% and 7.8%.\nUser study. We choose CAP [23] and STTN [43], two\nof the state-of-the-art video inpainting models as our base-\nlines for user study. 30 videos are randomly sampled from\nDA VIS [31] for object removal and video completion evalu-\nation. 38 volunteers has participated this user study. Videos\nprocessed by 3 models are presented at each time for vol-\nunteers to rank the inpainting quality. On our dedicated\nsoftware for this user study, volunteers can stop/replay any\nvideo until they make ﬁnal judgement. The percentage of\nﬁrst ranking model from each user on each video are shown\nin Fig.8, where for both object removal and video comple-\ntion we have the best performance.\nVisualizing inpainting process. Fig.6 demonstrates im-\nages decoded at different layer of ViF, showing the process\nof how the our model inpaints a video frame. We can see\nit starts with coarse context information and gradually re-\nﬁne features in deeper layers. In Fig.7, we further show\nthe detailed attention process between different multi-frame\npatches in an object removal task. We can see how our\nproposed model accurately ﬁnd reference patch and explore\nthe spatiotemporal information to inpaint the background as\nwell as the pillar.\n5. Conclusion\nIn this work we propose FuseFormer, a Transformer\nmodel designed for video inpainting via ﬁne-grained fea-\nture fusion. It aims at tackling the drawbacks of lacking\nﬁne-grained information in patch-based Transformer mod-\nels. The soft split divides feature map into many patches\nwith given overlapping interval while the soft composition\nstitches them back into a whole feature map where pixels\nin overlapping regions are summed up. FuseFormer elab-\norately builds soft composition and soft split into its feed-\nforward network for further enhancing sub-patch level fea-\nture fusion. Together with our strong Transformer baseline,\nour FuseFormer model achieve state-of-the-art performance\nin video restoration and object removal.\nAcknowledgement. This work is supported in part by the\nGeneral Research Fund through the Research Grants Coun-\ncil of Hong Kong under Grants (Nos. 14204021, 14208417,\n14207319, 14202217, 14203118, 14208619), in part by Re-\nsearch Impact Fund Grant No. R5001-18, in part by CUHK\nStrategic Fund.\nReferences\n[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and\nDan B Goldman. PatchMatch: A randomized corre-\nspondence algorithm for structural image editing. ACM\nTransactions on Graphics (Proc. SIGGRAPH), 2009.\n[2] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and\nColoma Ballester. Image inpainting. In Proceedings of\nthe 27th Annual Conference on Computer Graphics and\nInteractive Techniques, page 417C424, 2000.\n[3] M. Bertalmio, L. Vese, G. Sapiro, and S. Osher. Si-\nmultaneous structure and texture image inpainting. IEEE\nTransactions on Image Processing, page 882C889, 2003.\n[4] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston\nHsu. Free-form video inpainting with 3d gated convolution\nand temporal patchgan. In Proceedings of the International\nConference on Computer Vision (ICCV), 2019.\n[5] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston\nHsu. Learnable gated temporal shift module for deep video\ninpainting. In BMVC, 2019.\n[6] Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B\nGoldman, and Pradeep Sen. Image Melding: Com-\nbining inconsistent images using patch-based synthesis.\nACM Transactions on Graphics (TOG) (Proceedings of\nSIGGRAPH 2012), 2012.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding, 2018.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\nage is worth 16x16 words: Transformers for image recog-\nnition at scale. In International Conference on Learning\nRepresentations, 2021.\n[9] Alexei Efros and Thomas Leung. Texture synthesis by non-\nparametric sampling. In In International Conference on\nComputer Vision, pages 1033–1038, 1999.\n[10] Alexei A. Efros and William T. Freeman. Image quilt-\ning for texture synthesis and transfer. In Proceedings of\nSIGGRAPH, page 341C346.\n[11] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai,\nand Hongsheng Li. Fast convergence of DETR with spatially\nmodulated co-attention. CoRR, abs/2101.07448, 2021.\n[12] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. 2014.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. arXiv preprint\narXiv:1512.03385, 2015.\n[14] Yuan-Ting Hu, Heng Wang, Nicolas Ballas, Kristen Grau-\nman, and Alexander G. Schwing. Proposal-based video com-\npletion. In The Proceedings of the European Conference on\nComputer Vision (ECCV), 2020.\n[15] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Jo-\nhannes Kopf. Temporally coherent completion of dynamic\nvideo. ACM Trans. Graph., 2016.\n[16] Drew A Hudson and C. Lawrence Zitnick. Generative adver-\nsarial transformers. arXiv preprint:2103.01209, 2021.\n[17] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.\nGlobally and Locally Consistent Image Completion. ACM\nTransactions on Graphics (Proc. of SIGGRAPH), 36, 2017.\n[18] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo transformers can make one strong gan. arXiv preprint\narXiv:2102.07074, 2021.\n[19] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So\nKweon. Deep video inpainting. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\n2019.\n[20] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In 3rd International Conference on\nLearning Representations, ICLR, 2015.\n[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In Advances in Neural Information Processing\nSystems 25. 2012.\n[22] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,\nErsin Yumer, and Ming-Hsuan Yang. Learning blind video\ntemporal consistency. In European Conference on Computer\nVision, 2018.\n[23] Sungho Lee, Seoung Wug Oh, DaeYeun Won, and Seon Joo\nKim. Copy-and-paste networks for deep video inpainting.\nIn Proceedings of the IEEE International Conference on\nComputer Vision, 2019.\n[24] Ang Li, Shanshan Zhao, Xingjun Ma, Mingming Gong,\nJianzhong Qi, Rui Zhang, Dacheng Tao, and Ramamoha-\nnarao Kotagiri. Short-term and long-term context aggrega-\ntion network for video inpainting. In ECCV, 2020.\n[25] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for ir-\nregular holes using partial convolutions. In The European\nConference on Computer Vision (ECCV), 2018.\n[26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach, 2019.\n[27] Alasdair Newson, Andr ´es Almansa, Matthieu Fradet, Yann\nGousseau, and Patrick P ´erez. Video Inpainting of Complex\nScenes. SIAM Journal on Imaging Sciences, pages 1993–\n2019, 2014.\n[28] Gabriel Synnaeve Nicolas Usunier Alexander Kirillov\nSergey Zagoruyko Nicolas Carion, Francisco Massa. End-\nto-end object detection with transformers. In European\nConference on Computer Vision, 2020.\n[29] Seoung Wug Oh, Sungho Lee, Joon-Young Lee, and\nSeon Joo Kim. Onion-peel networks for deep video comple-\ntion. In Proceedings of the IEEE International Conference\non Computer Vision, 2019.\n[30] Deepak Pathak, Philipp Kr ¨ahenb¨uhl, Jeff Donahue, Trevor\nDarrell, and Alexei Efros. Context encoders: Feature\nlearning by inpainting. In Computer Vision and Pattern\nRecognition (CVPR), 2016.\n[31] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.\nGross, and A. Sorkine-Hornung. A benchmark dataset and\nevaluation methodology for video object segmentation. In\nComputer Vision and Pattern Recognition, 2016.\n[32] Alec Radford and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. 2018.\n[33] M. Strobel, Julia Diebold, and D. Cremers. Flow and color\ninpainting for video completion. In GCPR, 2014.\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. InAdvances in Neural\nInformation Processing Systems, pages 5998–6008, 2017.\n[35] Chuan Wang, Haibin Huang, Xiaoguang Han, and Jue Wang.\nVideo inpainting by jointly learning temporal structure and\nspatial details. In AAAI, 2019.\n[36] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,\nAndrew Tao, Jan Kautz, and Bryan Catanzaro. Video-\nto-video synthesis. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2018.\n[37] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua\nShen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-\nto-end video instance segmentation with transformers. In\nProc. IEEE Conf. Computer Vision and Pattern Recognition\n(CVPR), 2021.\n[38] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen\nLiang, Jianchao Yang, and Thomas Huang. Youtube-vos:\nA large-scale video object segmentation benchmark. arXiv:\n1809.03327, 2018.\n[39] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change\nLoy. Deep ﬂow-guided video inpainting. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, 2019.\n[40] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Free-form image inpainting with gated\nconvolution. arXiv preprint arXiv:1806.03589, 2018.\n[41] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5505–5514,\n2018.\n[42] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021.\n[43] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learn-\ning joint spatial-temporal transformations for video inpaint-\ning. In The Proceedings of the European Conference on\nComputer Vision (ECCV), 2020.\n[44] Haotian Zhang, Long Mai, Ning Xu, Zhaowen Wang, John\nCollomosse, and Hailin Jin. An internal learning approach to\nvideo inpainting. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2720–2729, 2019.\n[45] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng\nLi, and Hao Dong. End-to-end object detection with adaptive\nclustering transformer. CoRR, abs/2011.09315, 2020.\n[46] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher,\nand Caiming Xiong. End-to-end dense video captioning with\nmasked transformer. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 8739–\n8748, 2018.\n[47] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020.",
  "topic": "Image stitching",
  "concepts": [
    {
      "name": "Image stitching",
      "score": 0.8109866976737976
    },
    {
      "name": "Computer science",
      "score": 0.741488516330719
    },
    {
      "name": "Transformer",
      "score": 0.7340908646583557
    },
    {
      "name": "Inpainting",
      "score": 0.7007450461387634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6125152111053467
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5285229682922363
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44983938336372375
    },
    {
      "name": "Feature extraction",
      "score": 0.43781906366348267
    },
    {
      "name": "Computer vision",
      "score": 0.42565372586250305
    },
    {
      "name": "Pixel",
      "score": 0.41252878308296204
    },
    {
      "name": "Image (mathematics)",
      "score": 0.150253564119339
    },
    {
      "name": "Engineering",
      "score": 0.10209739208221436
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}