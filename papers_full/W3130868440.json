{
    "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
    "url": "https://openalex.org/W3130868440",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2097808136",
            "name": "Tao Lei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4295838474",
        "https://openalex.org/W2963655672",
        "https://openalex.org/W2983981554",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W3102129360",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3132672614",
        "https://openalex.org/W2964182247",
        "https://openalex.org/W4288024261",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W3010714856",
        "https://openalex.org/W2963386218",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W3101005014",
        "https://openalex.org/W2995154514",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3123615524",
        "https://openalex.org/W3174401451",
        "https://openalex.org/W3007773043",
        "https://openalex.org/W2970157301",
        "https://openalex.org/W2342173569",
        "https://openalex.org/W3131922516",
        "https://openalex.org/W4288265053",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W3106147182",
        "https://openalex.org/W2991324852",
        "https://openalex.org/W2955227499",
        "https://openalex.org/W2952494384",
        "https://openalex.org/W2963034893",
        "https://openalex.org/W2994689640",
        "https://openalex.org/W2553397501",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W4292356195",
        "https://openalex.org/W2975044525",
        "https://openalex.org/W2963641307",
        "https://openalex.org/W2617242334",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2474388053",
        "https://openalex.org/W2892090442",
        "https://openalex.org/W3214897310",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3196389743",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2519314406",
        "https://openalex.org/W2952436057",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W4287667694",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W4288289156",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W3093960091",
        "https://openalex.org/W3103334733",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W3123673616",
        "https://openalex.org/W2963174729",
        "https://openalex.org/W4287725215",
        "https://openalex.org/W3025165719",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W3034457371",
        "https://openalex.org/W2963938518",
        "https://openalex.org/W3037798801",
        "https://openalex.org/W2757047188",
        "https://openalex.org/W3172099915",
        "https://openalex.org/W2157331557"
    ],
    "abstract": "Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7633–7648\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n7633\nWhen Attention Meets Fast Recurrence:\nTraining Language Models with Reduced Compute\nTao Lei\nASAPP, Inc.\ntaoleics@gmail.com\nAbstract\nLarge language models have become increas-\ningly difﬁcult to train because of the grow-\ning computation time and cost. In this work,\nwe present SRU++, a highly-efﬁcient archi-\ntecture that combines fast recurrence and at-\ntention for sequence modeling. SRU++ ex-\nhibits strong modeling capacity and training\nefﬁciency. On standard language modeling\ntasks such as ENWIK 8, W IKI -103 and BIL -\nLION WORD datasets, our model obtains bet-\nter bits-per-character and perplexity while us-\ning 3x-10x less training cost compared to top-\nperforming Transformer models. For instance,\nour model achieves a state-of-the-art result on\nthe ENWIK 8 dataset using 1.6 days of train-\ning on an 8-GPU machine. We further demon-\nstrate that SRU++ requires minimal attention\nfor near state-of-the-art performance. Our re-\nsults suggest jointly leveraging fast recurrence\nwith little attention as a promising direction\nfor accelerating model training and inference.1\n1 Introduction\nMany recent advances in language modeling have\ncome from leveraging ever larger datasets and\nmodel architectures. As a result, the associated\ncomputation cost for developing such models have\ngrown enormously, requiring hundreds of GPU\nhours or days per experiment, and raising concerns\nabout the environmental sustainability of current\nresearch (Schwartz et al., 2020). As a conse-\nquence, it has become imperative to build compu-\ntationally efﬁcient models that retain top modeling\npower while reducing computational costs.\nThe Transformer architecture (Vaswani et al.,\n2017) was proposed to accelerate model training\nand has become the predominant architecture in\nNLP. Speciﬁcally, it is built entirely upon self-\nattention and avoids the use of recurrence to en-\nable strong parallelization. While this change has\n1Our code, experimental setup and models are available\nat https://github.com/asappresearch/sru.\nTable 1\nEﬀective GPU \nhour\nTransformer-XL SRU++ SRU++ (single \nattention)\n0 1.520\n4 1.407\n7 1.363\n11 1.324\n14 1.308\n18 1.284\n22 1.278\n25 1.260\n29 1.250\n32 1.240\n36 1.239\n40 1.226\n43 1.222\n47 1.214\n50 1.209\n54 1.201\n58 1.196\n61 1.197\n65 1.194\n68 1.190\n72 1.188\n76 1.181\n79 1.181\n83 1.175\n86 1.170\n90 1.174\n94 1.167\n97 1.166\n101 1.162\n104 1.158\n108 1.157\n112 1.155\n115 1.151\n119 1.150\n122 1.148\n126 1.144\n130 1.147\n133 1.142\n137 1.140\n140 1.139\n144 1.138\n148 1.133\n151 1.129\n155 1.134\n158 1.127\n162 1.130\n166 1.128\n169 1.122\n173 1.121\n176 1.125\n180 1.121\n184 1.120\n187 1.120\n191 1.118\n194 1.114\n198 1.112\n202 1.114\n205 1.113\n209 1.110\n212 1.111\n216 1.107\n220 1.109\n223 1.108\n227 1.108\n230 1.106\n234 1.104\n237 1.102\n241 1.104\n245 1.098\n248 1.102\n252 1.099\n255 1.099\n259 1.099\n263 1.100\n266 1.098\n270 1.100\n273 1.097\n277 1.096\n281 1.097\n284 1.095\n288 1.097\n291 1.097\n295 1.097\n299 1.095\n302 1.095\n306 1.095\n309 1.095\n313 1.093\n317 1.093\n320 1.094\n324 1.094\n327 1.093\nBits Per Character (BPC)\n1.0\n1.2\n1.3\n1.5\nEffective training hours\n0 90 180 270 360\nTransformer-XL\nSRU++\nSRU++ (single attention)\nBits Per Character (BPC)\n1.00\n1.17\n1.33\n1.50\nEffective training hours\n0 90 180 270 360\n1.09\n1.17\nTransformer-XL\nSRU++ (single attention)\n5.1x efﬁciency\n8.7x efﬁciency\nBits Per Character (BPC)\n1.00\n1.17\n1.33\n1.50\nEffective training hours\n0 90 180 270 360\nTransformer-XL\nSRU++ (single attention)\n5.1x efﬁciency\n8.7x efﬁciency\n\u0000 1\nFigure 1: Bits-per-character on ENWIK 8 dev set vs.\nGPU hours used for training. SRU++ obtains better\nBPC by using 1/8 of the resources. We compare with\nTransformer-XL as it is one of the strongest models on\nthe datasets tested. Models are trained with single pre-\ncision and comparable training settings.\nled to many empirical success and improved com-\nputational efﬁciency, we are interested in revisit-\ning the architectural question: Is attention all we\nneed for modeling?\nThe attention mechanism permits learning de-\npendencies between any parts of the input, mak-\ning it an extremely powerful neural component\nin many machine learning applications (Bahdanau\net al., 2015; Lin et al., 2017). We hypothesize\nthat this advantage can still be complemented with\nother computation that is directly designed for se-\nquential modeling. Indeed, several recent works\nhave studied and conﬁrmed the same hypothesis\nby leveraging recurrence in conjunction with at-\ntention. For example, Merity (2019) demonstrates\nthat single-headed attention LSTMs can produce\nresults competitive to Transformer models in lan-\nguage modeling. Other work have incorporated\nRNNs into Transformer, and obtain better re-\nsults in machine translation (Lei et al., 2018; Hao\net al., 2019) and language understanding bench-\nmarks (Huang et al., 2020). These results high-\nlight one possibility – we could build more efﬁ-\ncient models by combining attention and fast re-\ncurrent networks (Bradbury et al., 2017; Zhang\nand Sennrich, 2019).\n7634\nIn this work, we validate this idea and present\na self-attentive recurrent unit that achieves strong\ncomputational efﬁciency. Our work builds upon\nthe SRU (Lei et al., 2018), a highly paralleliz-\nable RNN implementation that has been shown ef-\nfective in language and speech applications (Park\net al., 2018; Kim et al., 2019; Hsu et al., 2020;\nShangguan et al., 2019). We incorporate atten-\ntion into the SRU by simply replacing the linear\ntransformation of input with a self-attention com-\nponent. The proposed architecture, called SRU++,\nenjoys enhanced modeling capacity and remains\nequally parallelizable. Figure 1 compares its per-\nformance with the Transformer-XL model (Dai\net al., 2019) on the ENWIK 8 dataset. SRU++\nachieves better results while using a fraction of the\ntraining resources needed by the baseline.\nWe evaluate SRU++ on standard language mod-\neling benchmarks including the ENWIK 8, W IKI -\n103 and BILLION WORD datasets. SRU++ consis-\ntently outperforms various Transformer models on\nthese datasets, delivering better or on par results\nwhile using 3x-10x less computation. Our model\ndo not use positional encoding, multi-head atten-\ntion and other techniques useful to Transformer\nmodels. Furthermore, we demonstrate that a cou-\nple of attention layers are sufﬁcient for SRU++\nto obtain near state-of-the-art performance. These\nchanges not only highlight the effectiveness of re-\ncurrence but also enable strong computation re-\nduction in training and inference. Finally, we\nalso showcase the effectiveness of SRU++ on the\nIWSLT’14 De →En translation task, and open\nsource our implementation in Pytorch to facilitate\nfuture research.\n2 Background: SRU\nWe ﬁrst describe the Simple Recurrent Unit (SRU)\nin this section. A single layer of SRU involves the\nfollowing computation:\nf[t] = σ(Wx[t] +v ⊙c[t-1] +b)\nr[t] = σ\n(\nW′x[t] +v′⊙c[t-1] +b′)\nc[t] = f[t] ⊙c[t-1] + (1−f[t]) ⊙(W′′x[t])\nh[t] = r[t] ⊙c[t] + (1−r[t]) ⊙x[t]\nwhere ⊙is the element-wise multiplication, W,\nW′ and W′′ are parameter matrices and v, v′,\nb and b′ are parameter vectors to be learnt dur-\ning training. The SRU architecture consists of\na light recurrence component which successively\ncomputes the hidden states c[t] by reading the in-\nput vector x[t] for each step t. The computation\nresembles other gated recurrent networks such as\nLSTM (Hochreiter and Schmidhuber, 1997) and\nGRU (Cho et al., 2014). Speciﬁcally, the state vec-\ntor c[t] is a weighted average between the previous\nstate c[t-1] and a linear transformation of the input\nW′′x[t]. The weighted aggregation is controlled\nby a forget gate f[t] which is a sigmoid function\nover the current input and hidden state. Once the\ninternal state c[t] is produced, SRU uses a high-\nway network to introduce a skip connection and\ncompute the ﬁnal output state h[t]. Similarly, the\ninformation ﬂow in the highway network is con-\ntrolled by a reset gate r[t].\nTwo important code-level optimizations are per-\nformed to enhance the parallelism and speed\nof SRU. First, given the input sequence X =\n{x[1],··· ,x[L]}where each x[t] ∈ Rd is a d-\ndimensional vector, SRU combines the three ma-\ntrix multiplications across all time steps as a sin-\ngle multiplication. This signiﬁcantly improves\nthe computation intensity (e.g. GPU utilization).\nSpeciﬁcally, the batched multiplication is a linear\nprojection of the input tensor X ∈RL×d:\nU⊤ =\n\n\nW\nW′\nW′′\n\nX⊤ , (1)\nwhere U ∈RL×3×d is the output tensor, Lis the\nsequence length and dis the hidden state size.\nThe second optimization performs all element-\nwise operations in an efﬁcient way. This involves\nf[t] =σ(U[t,0] +v ⊙c[t-1] +b) (2)\nr[t] =σ(U[t,1] +v′⊙c[t-1] +b′) (3)\nc[t] =f[t] ⊙c[t-1] + (1−f[t]) ⊙U[t,2] (4)\nh[t] =r[t] ⊙c[t] + (1−r[t]) ⊙x[t]. (5)\nSimilar to other built-in operations such as atten-\ntion and cuDNN LSTM (Appleyard et al., 2016),\nSRU implements all these operations as a single\nCUDA kernel to accelerate computation. Note that\neach dimension of the hidden vectors is indepen-\ndent once U is computed. The computation can\nrun in parallel across each hidden dimension (and\neach input sequence given a mini-batch of multi-\nple sequences).\n3 SRU++\nThe key modiﬁcation of SRU++ is to incorporate\nmore expressive non-linear operations into the re-\n7635\nElementwise recurrence\nMatMul\nMatMul\ninput x\noutput h, c\n2048\n512\n2048*3\nElementwise recurrence\nMatMul\ninput x\noutput h, c\n2048\n2048*3\nElementwise recurrence\nMatMul\nMatMul\nAttention\ninput x\noutput h, c\n2048\n512\n2048*3\n(a) SRU (b) SRU w/ projection trick (c) SRU++ w/ attention\nFigure 2: An illustration of SRU and SRU++ networks: (a) the original SRU, (b) the SRU variant with projection to\nreduce the number of parameters, experimented in Lei et al. (2018) and (c) SRU++ proposed in this work. Numbers\nindicate the dimension of intermediate inputs/outputs given hidden size d= 2048and attention size d′= 512.\ncurrent network. Note that the computation of U\n(Equation 1) is a linear transformation of the in-\nput sequence X. We can replace this linear trans-\nformation with self-attention operation to enhance\nmodeling capacity.\nSpeciﬁcally, given the input sequence repre-\nsented as a matrix X ∈RL×d, the attention com-\nponent computes the query, key and value repre-\nsentations using the following multiplications,\nQ = Wq X⊤\nK = Wk Q\nV = Wv Q\nwhere Wq ∈ Rd′×d, Wk,Wv ∈ Rd′×d′\nare\nmodel parameters. d′ is the attention dimension\nthat is typically much smaller than d. Note that\nthe keys K and values V are computed using Q\ninstead of X such that the weight matrices Wk\nand Wv are signiﬁcantly smaller. We also tested\nanother variant in which we ﬁrst project X′ =\nWX⊤into the lower dimensiond′, and then apply\nthree independent d′-by-d′matrix multiplications\nover X′to obtain the query, key and value repre-\nsentations. This variant achieves similar results.\nNext, we compute a weighted average output\nA ∈Rd′×L using the scaled dot-product attention\nintroduced in Vaswani et al. (2017),\nA⊤= softmax\n(Q⊤K√\nd′\n)\nV⊤.\nThe ﬁnal output U required by the elementwise re-\ncurrence is obtained by another linear projection,\nU⊤= Wo (Q + α·A) .\nwhere α∈R is a learned scalar andWo ∈R3d×d′\nis a parameter matrix. Q + α ·A is a residual\nconnection which improves gradient propagation\nand stabilizes training. We initialize αto zero and\nas a result,\nU⊤= Wo Q = (Wo Wq) X⊤\ninitially falls back to a linear transformation of\nthe input X skipping the attention transformation.\nIntuitively, skipping attention encourages leverag-\ning recurrence to capture sequential patterns dur-\ning early stage of training. As |α|grows, the at-\ntention mechanism can learn long-range depen-\ndencies for the model. In addition, WoWq can\nbe interpreted as applying a matrix factorization\ntrick with a small inner dimension d′< d, reduc-\ning the total number of parameters. Figure 2 (a)-\n(c) compares the differences of SRU, SRU with\nthis factorization trick (but without attention), and\nSRU++ proposed in this section.\nThe last modiﬁcation is adding layer normaliza-\ntion (Ba et al., 2016) to each SRU++ layer. In our\nimplementation, we apply normalization after the\nattention operation and before the matrix multipli-\ncation with Wo,\nU⊤= Wo layernorm(Q + α·A).\nThis implementation is post-layer normalization in\nwhich the normalization is added after the resid-\nual connection. Alternatively, pre-layer normal-\nization (Xiong et al., 2020) only applies to the non-\nlinear transformation. While pre-normalization\ntends to be less sensitive to different learning rates,\nwe use post-normalization for better results fol-\nlowing the observations in Liu et al. (2020b). We\nanalyze the effectiveness of layer normalization in\nAppendix A.2.\n7636\nModel Batch size B×M BPC ↓\nTrans-XL 24 ×512 1.06\nSRU++ 24 ×512 1.03\nSRU++ 16 ×768 1.02\nTable 1: Test BPC of SRU++ and Transformer-XL on\nENWIK 8 dataset. We train SRU++ using the same\nsetting as Transformer-XL base model. Numbers are\nsmaller the better. B is the number of sequence. M is\nthe unroll size (and additional context size).\n4 Experimental setup\nDatasets We evaluate our model on four stan-\ndard NLP benchmarks.\n•ENWIK 8 (Hutter, 2006) is a character-level\nlanguage modeling dataset consisting of\n100M tokens taken from Wikipedia. The vo-\ncabulary size of this dataset about 200. We\nuse the standard 90M/5M/5M splits as the\ntraining, dev and test sets, and report bits-per-\ncharacter (BPC) as the evaluation metric.\n•WIKI -103 (Merity et al., 2017) is a word-\nlevel language modeling dataset. The train-\ning data contains 100M tokens extracted from\nWikipedia articles. Following prior work, we\nuse a vocabulary of 260K tokens, and adap-\ntive embedding and softmax layers (Grave\net al., 2017; Baevski and Auli, 2019).\n•BILLION WORD (Chelba et al., 2013) is one\nof the largest language modeling datasets\ncontaining 768M tokens for training. Unlike\nWIKI -103 in which sentences in the same\narticle are treated as consecutive inputs to\nmodel long context, the sentences in BIL -\nLION WORD are randomly shufﬂed. Follow-\ning Baevski and Auli (2019), we use a vocab-\nulary of 800K tokens, adaptive embedding\nand softmax layers.\n•IWSLT’14 De →En is a low-resource ma-\nchine translation dataset consists of 170K\ntranslation pairs. We showcase SRU++ can\nbe applied to other tasks such as translation.\nWe follow the same setup of Lin et al. (2020)\nand other previous work. The dataset uses a\nshared vocabulary of 14K BPE tokens.\nModels All our language models are constructed\nwith a word embedding layer, multiple layers of\nModel Param BPC ↓ GPU hrs↓\nTrans-XL 41M 1.06 356\nSHA-LSTM 54M 1.07 28 †\nk= 1\n42M\n1.022 37 †\nk= 2 1.025 29 †\nk= 5 1.032 24 †\nk= 10 1.033 22 †\nNo attention 1.190 20 †\nTable 2: Results of SRU++ on ENWIK 8 by enabling\nattention every k layers. We adjust the hidden size so\nthe number of parameters are comparable. †indicates\nmixed precision training.\nSRU++ and an output linear layer followed by\nsoftmax operation. We use single-head attention\nin each layer and 10 SRU++ layers for all our mod-\nels. We use the same dropout probability for all\nlayers and tune this value according to the model\nsize and the results on the dev set. By default, we\nset the hidden dimension d : d′ = 4 : 1. We re-\nport additional analysis and tune this ratio for best\nresults in Section 5 and Appendix A.\nFor simplicity, SRU++ does not use recent tech-\nniques that are shown useful to Transformer such\nas multi-head attention, compressed memory (Rae\net al., 2020), relative position (Shaw et al., 2018;\nPress et al., 2021), nearest-neighbor interpola-\ntion (Khandelwal et al., 2020) and attention vari-\nants to handle very long context (Sukhbaatar et al.,\n2019a; Roy et al., 2021).\nWe compare with previous Transformer models\nthat incorporate one or several these techniques.\nHowever, we do not compare with results that\nuse additional data or dynamic evaluation (Graves,\n2013; Krause et al., 2018), for a fair comparison\nbetween all models.\nOptimization We use RAdam (Liu et al., 2020a)\nwith the default βvalues as our optimizer. RAdam\nis a variant of Adam optimizer (Kingma and Ba,\n2014) that is reported less sensitive to the choice\nof learning rate and warmup steps while achiev-\ning similar results at the end. We use a ﬁxed\nweight decay of 0.1 and an initial learning rate of\n0.0003 in our experiments. These values are se-\nlected based on ENWIK 8 dev set and used for other\ntasks. See Appendix A.3 for more details. We use\na cosine learning rate schedule following Dai et al.\n(2019). We do not change the initial learning rate\nunless otherwise speciﬁed. See Appendix B for\nthe detailed training conﬁguration of each model.\n7637\nTable 1\nEﬀective GPU \nhour\nTransformer-XL SRU++ SRU++ (k=10, \nmixed precision)\n0 1.52\n4 1.41\n7 1.36\n11 1.32\n14 1.31\n18 1.28\n22 1.28\n25 1.26\n29 1.25\n32 1.24\n36 1.24\n40 1.23\n43 1.22\n47 1.21\n50 1.21\n54 1.20\n58 1.20\n61 1.20\n65 1.19\n68 1.19\n72 1.19\n76 1.18\n79 1.18\n83 1.17\n86 1.17\n90 1.17\n94 1.17\n97 1.17\n101 1.16\n104 1.16\n108 1.16\n112 1.15\n115 1.15\n119 1.15\n122 1.15\n126 1.14\n130 1.15\n133 1.14\n137 1.14\n140 1.14\n144 1.14\n148 1.13\n151 1.13\n155 1.13\n158 1.13\n162 1.13\n166 1.13\n169 1.12\n173 1.12\n176 1.13\n180 1.12\n184 1.12\n187 1.12\n191 1.12\n194 1.11\n198 1.11\n202 1.11\n205 1.11\n209 1.11\n212 1.11\n216 1.11\n220 1.11\n223 1.11\n227 1.11\n230 1.11\n234 1.10\n237 1.10\n241 1.10\n245 1.10\n248 1.10\n252 1.10\n255 1.10\n259 1.10\n263 1.10\n266 1.10\n270 1.10\n273 1.10\n277 1.10\n281 1.10\n284 1.09\n288 1.10\n291 1.10\n295 1.10\n299 1.09\n302 1.09\n306 1.10\n309 1.09\n313 1.09\n317 1.09\n320 1.09\n324 1.09\n327 1.09\n331 1.09\n335 1.09\n338 1.09\n342 1.09\n345 1.09\n349 1.09\n353 1.09\n356 1.09\n0 1.47\n2 1.36\n4 1.31\n6 1.28\n8 1.26\n10 1.25\n12 1.23\n13 1.22\n15 1.22\n17 1.21\n19 1.20\n21 1.20\nBits Per Character (BPC)\n1.0\n1.2\n1.3\n1.5\n                              Effective training hours\n0 90 180 270 360\nTransformer-XL\nSRU++\nSRU++ (k=10, mixed precision)\n\u0000 1\nFigure 3: Dev BPC vs. total GPU hours used on EN-\nWIK 8 for each model. Using automatic mixed preci-\nsion (amp) and only one attention sub-layer achieves\n16x reduction. To compute the dev BPC, the maximum\nattention length is the same as the unroll sizeM during\ntraining.\nEach training batch contains B sequences (i.e.\nthe batch size) and M consecutive tokens for each\nsequence (i.e. the unroll size), which gives an ef-\nfective size of B×M tokens per batch. Follow-\ning standard practice, the previous training batch is\nprovided as additional context for attention, which\nresults in a maximum attention length of 2 ×M.\nFor ENWIK 8 and WIKI -103 datasets, the training\ndata is partitioned into Bchunks by concatenating\narticles and ignoring the boundaries between arti-\ncles. For BILLION WORD dataset, we follow Dai\net al. (2019) and concatenate sentences to create\nthe training batches. Sentences are randomly shuf-\nﬂed and separated by a special token <s> indicat-\ning sentence boundaries.\n5 Results\nDoes recurrence improve upon attention-only\nmodel? We ﬁrst conduct a comparison with the\nTransformer-XL model (Dai et al., 2019) on EN-\nWIK 8 dataset2. Their base model consists of 41M\nparameters and 12 Transformer layers. Follow-\ning the ofﬁcial instructions, we reproduced the re-\nported test BPC of 1.06 by training with 4 Nvidia\n2080 Ti GPUs. The training took about 4 days or\na total of 360 GPU hours equivalently.\nWe train a 10-layer SRU++ model with 42M\nparameters. For a fair comparison, we use the\nsame hyperparameter setting including the effec-\ntive batch size, attention context length, learning\nrate and the number of training iterations as the\nTransformer-XL base model. Notably, our base\nmodel can be trained using 2 GPUs due to less\nGPU memory usage. After training, we set the at-\n2https://github.com/kimiyoung/\ntransformer-xl/tree/master/pytorch\n42M model\nDev BPC Test BPC\n10 dev_bpc=1.072  test_bpc=1.053 dev_bpc=1.053  test_bpc=1.032 1.053 1.032\n9 dev_bpc=1.074  test_bpc=1.055 dev_bpc=1.056  test_bpc=1.034 1.056 1.034\n8 dev_bpc=1.073  test_bpc=1.053 dev_bpc=1.056  test_bpc=1.033 1.056 1.033\n7 dev_bpc=1.074  test_bpc=1.054 dev_bpc=1.057  test_bpc=1.035 1.057 1.035\n6 dev_bpc=1.074  test_bpc=1.055 dev_bpc=1.058  test_bpc=1.036 1.058 1.036\n5 dev_bpc=1.077  test_bpc=1.057 dev_bpc=1.061  test_bpc=1.040 1.061 1.040\n4 dev_bpc=1.079  test_bpc=1.059 dev_bpc=1.064  test_bpc=1.043 1.064 1.043\n3 dev_bpc=1.081  test_bpc=1.062 dev_bpc=1.067  test_bpc=1.048 1.067 1.048\n2 dev_bpc=1.085  test_bpc=1.068 dev_bpc=1.074  test_bpc=1.056 1.074 1.056\n1 dev_bpc=1.174  test_bpc=1.177 dev_bpc=1.173  test_bpc=1.176 1.173 1.176\n0 dev_bpc=1.176  test_bpc=1.184 dev_bpc=1.176  test_bpc=1.184\n9 dev_bpc=1.069  test_bpc=1.048 dev_bpc=1.050  test_bpc=1.026 1.050\n8 dev_bpc=1.070  test_bpc=1.048 dev_bpc=1.050  test_bpc=1.025 1.050\n7 dev_bpc=1.069  test_bpc=1.048 dev_bpc=1.049  test_bpc=1.026 1.049\n6 dev_bpc=1.070  test_bpc=1.049 dev_bpc=1.050  test_bpc=1.026 1.050\n5 dev_bpc=1.073  test_bpc=1.053 dev_bpc=1.053  test_bpc=1.032 1.053\n4 dev_bpc=1.073  test_bpc=1.053 dev_bpc=1.054  test_bpc=1.032 1.054\n3 dev_bpc=1.073  test_bpc=1.054 dev_bpc=1.054  test_bpc=1.033 1.054\n2 dev_bpc=1.075  test_bpc=1.055 dev_bpc=1.056  test_bpc=1.035 1.056\n1 2 3 4 5 6 7 8 9 10\n1.173\n1.074\n1.067\n1.064\n1.061\n1.058\n1.057\n1.056\n1.056\n1.053\nDev BPC\nWhich layer to put the 1 attention\n1\n1.04\n1.08\n1.12\n1.16\n1.2\n1 2 3 4 5 6 6 7 8 9 10\nDev BPC\nTest BPC\nLocation of the attention\n2 3 4 5 6 7 8 9\n1.056\n1.054\n1.054\n1.053\n1.050\n1.049\n1.050\n1.050\n 1\nFigure 4: Analyzing where to apply attention. We en-\nable only one attention layer (top ﬁgure) or two (bottom\nﬁgure) in the SRU++ model. For the latter, we always\napply attention in the last layer and move the location\nof the other. X-axis is the layer index. The layer closest\nto the input embedding layer has index 1.\ntention context length to 2048 for testing, similarly\nto the Transformer-XL baseline. Table 1 presents\nthe results. Our model achieves a test BPC of 1.03,\noutperforming the baseline by a large margin. This\nresult suggests that combining recurrence and at-\ntention can greatly outperform an attention-only\nmodel. We obtain a BPC of 1.02 by extending\nthe attention context length from 512 to 768, while\nkeeping the number of tokens per batch the same.\nHow much attention is needed?Merity (2019)\ndemonstrated that using a single attention layer\nwith LSTM retains most of the modeling capac-\nity compared to using multiple attention layers.\nWe conduct a similar analysis to understand how\nmuch attention is needed in SRU++. To do so, we\nonly enable attention every k layers. The layers\nwithout attention become the variant with dimen-\nsion projection illustrated in Figure 2 (b). Note\nthat k = 1 gives the default SRU++ model with\nattention in every layer, and k = 10 means only\nthe last layer has attention in a 10-layer model.\nTable 2 presents the results by varying k. Our\nbase model is the same 10-layer SRU++ model\nin Table 1. We see that using 50% less atten-\ntion ( k = 2 ) achieves almost no increase in\ntest BPC. Moreover, using only a single atten-\ntion module ( k = 10) leads to a marginal loss\nof 0.01 BPC but reduces the training time by\n40%. Our results still outperform Transformer-XL\nmodel and single-headed attention LSTM (Merity,\n2019) greatly by 0.03 BPC. Figure 3 showcases\nthe training efﬁciency of our model. SRU++ is\n7638\nModel Parameters ↓ Test BPC↓ GPU days↓\nLongformer 30L (Beltagy et al., 2020) 102M 0.99 104 †\nAll-attention network 36L (Sukhbaatar et al., 2019b) 114M 0.98 64\nTransformer-XL 24L (Dai et al., 2019) 277M 0.99 -\n◦ Compressive memory (Rae et al., 2020) - 0.97 -\nFeedback Transformer (Fan et al., 2020) 77M 0.96 -\nSRU++ Base 108M 0.97 6 †\n◦ only 2 attention layers (k= 5) 98M 0.98 4 †\nSRU++ Large 191M 0.96 12 †\n◦ d= 8d′ 195M 0.95 13 †\nTable 3: Comparison with top-performing models on ENWIK 8 dataset. We include the training cost (measured by\nthe number of GPUs used ×the number of days) if it is reported in the previous work. Our results are obtained\nusing an AWS p3dn instance with 8 V100 GPUs. The reported training time of all-attention network is based on\nV100 GPUs while the training time of Longformer is based on RTX8000 GPUs (which is about 90% speed of\nV100). †indicates mixed precision training.\nRatio Dimensions d, d′ Dev BPC↓\n4 3072 768 0.997\n6 3840 640 0.992\n8 4480 560 0.991\n10 5040 504 0.992\nTable 4: Dev BPC on ENWIK 8 by changing the ratio\nd: d′in the SRU++ model while ﬁxing the number of\nparameters to 108M.\n5x faster to reach the dev BPC obtained by the\nTransformer-XL model. Furthermore, using au-\ntomatic mixed precision training and a single at-\ntention layer (k = 10) achieves 16x reduction on\ntraining cost.\nWhere to use attention?Next, we analyze if the\nlocation of attention in SRU++ makes a non-trivial\ndifference. Figure 4 (top) compares the results by\nenabling attention in only one of the SRU++ lay-\ners. Applying attention in the ﬁrst bottom layer\nachieves signiﬁcantly worse result. We believe\nthis is due to the lack of positional information\nfor attention, since SRU++ does not use positional\nencoding. Enabling attention in subsequent layers\ngives much better and comparable results because\nrecurrence can encode positional information.\nMoreover, SRU++ consistently achieves worse\nresults by moving the attention to lower layer\ncloser to the input embedding. We also enable a\nsecond attention layer while ﬁxing the ﬁrst one\nin the 10th layer. The corresponding results are\nshown in Figure 4 (bottom). Similarly, SRU++\nachieves worse results if the attention is added to\none of the lower layers. In contrast, results are\ncomparable once the attention is placed in a high-\nenough layer. These observations suggest that the\nmodel should ﬁrst learn local features before atten-\ntion plays a most effective role at capturing long-\nrange dependencies. More analyses can be found\nin Appendix A.\nDoes the ratiod: d′matter? Transformer mod-\nels by default use a FFN dimension that is 4 times\nlarger than the attention dimension (Vaswani et al.,\n2017). We analyze the ratio of recurrence dimen-\nsion d to attention dimension d′ for SRU++. A\nsmall value of d′can reduce the amount of com-\nputation and the number of parameters used in at-\ntention layers but may limit the modeling capac-\nity. Table 4 compares the results of using different\nd : d′ratio given a similar amount of model pa-\nrameters. We ﬁx the model size to around 108M\nand use 10 SRU++ layers. Changing this ratio\nfrom 4 to a higher value gives better result. The\nbest dev result is obtained with a ratio of 8.\nGiven this observation, we report SRU++ result\nusing a default ratio of 4 as well as a ratio of 8\nin the subsequent result sections. This ensures we\nconduct a comparison that uses a setup similarly to\nthe default of Transformer models, but also show-\ncases stronger results SRU++ can achieve.\nENWIK 8 Table 3 compares our model with other\ntop-performing models on the ENWIK 8 dataset.\nWe train a base model with d = 3072and a large\nmodel with d = 4096using 400K training steps.\nThe unroll size and attention context length are set\nto 1024 during training and 3072 during evalua-\n7639\nModel Parameters ↓ Test PPL↓ GPU days↓\nAll-attention network 36L (Sukhbaatar et al., 2019b) 133M 20.6 -\nFeedback Transformer (Fan et al., 2020) 139M 18.2 214\nTransformer (Baevski and Auli, 2019) 247M 18.7 22 †\nTransformer-XL 18L (Dai et al., 2019) 257M 18.3 -\n◦ Compressive memory (Rae et al., 2020) - 17.1 -\nRouting Transformer (Roy et al., 2021) - 15.8 -\nkNN-LM (Khandelwal et al., 2020) - 15.8 -\nSRU++ Base 148M 18.3 8 †\nSRU++ Large 232M 17.4 14 †\n◦ d= 8d′ 234M 17.1 15 †\n◦ only 2 attention layers (k= 5) 225M 17.3 11 †\nTable 5: Comparison with top-performing models onWIKI -103 dataset. We include the training cost (measured by\nthe number of GPUs used ×the number of days) if it is reported in the previous work. The reported training costs\nare based on V100 GPUs. Our results are similarly obtained using an AWS p3dn instance with 8 V100 GPUs. †\nindicates mixed precision training.\nModel Param PPL ↓ Days ↓\nTransformer 331M 25.6 57 †\n25.2 147 †\n465M 23.9 192 †\nSRU++ 328M 25.1 36 †\nSRU++ (k= 5) 465M 23.5 63 †\nTable 6: Test perplexity and effective GPU days for\ntraining of SRU++ models and the Transformer models\nof Baevski and Auli (2019) onBILLION WORD dataset.\ntion. To compare the computation efﬁciency we\nreport the effective GPU days – the number of\nGPUs multiplied by the number of days needed\nto ﬁnish training. Our base model achieves bet-\nter BPC and uses a fraction of the training cost\nreported in previous work. Furthermore, our large\nmodels achieve a new state-of-the-art result on this\ndataset, reaching a test BPC of 0.96 whend= 4d′\nand 0.95 when d= 8d′.\nWIKI -103 Table 5 presents the result of SRU++\nmodels and other top results on the W IKI -103\ndataset. We train one base model with 148M pa-\nrameters and a few large models which contain\nabout 230M parameters. As shown in the table,\nour base model obtains a test perplexity of 18.3\nusing 8 GPU days of training, about 3x reduction\ncompared to the Transformer model in Baevski\nand Auli (2019) and over 10x reduction com-\npared to Feedback Transformer (Fan et al., 2020).\nAgain, changing the hidden size ratio to d = 8d′\nimproves the modeling capacity. Our big model\nModel Speed ↑ PPL↓\nkNNLM (Khandelwal et al.) 145 15.8\nTrans (Baevski and Auli) 2.5k 18.7\nTrans-XL (Dai et al.) 3.2k 18.3\nShortformer (Press et al.) 15k 18.2\nSRU++ Large 15k 17.1\nSRU++ Large (k= 5) 22k 17.3\nTable 7: Inference speed (tokens/second) on WIKI -103\ntest set. Results of baselines are taken from Press et al.\n(2021). We use a single V100 GPU, a batch size of 1\nand maximum attention length 2560 for consistency.\nachieves a test perplexity of 17.1. The required\ntraining cost remains signiﬁcantly lower.\nBILLION WORD We double our training itera-\ntions to 800K and use a learning rate of 0.0002 for\nthe BILLION WORD dataset. We train a base model\nusing d= 4096, d′= 1024and an effective batch\nsize of 65K tokens per gradient update. We also\ntrain a large model by increasing the hidden size\ndto 7616 and the batch size to 98K. In addition,\nwe use only 2 attention layers (k= 5) for the large\nmodel. Table 6 reports the test perplexity and asso-\nciated training cost. Our base and large model ob-\ntain a test perplexity of 25.1 and 23.5 respectively,\noutperforming the Transformer model of Baevski\nand Auli (2019) given similar model size. More-\nover, SRU++ achieves 3-4x training cost reduction\nand is trained using 8 GPUs. In comparison, the\nTransformer model uses 32 or 64 V100 GPUs.\n7640\nModel Param BLEU ↑ Hrs ↓\nTransformer 20.1M 35.9 ±0.1 10.5\nSRU++ 20.4M 36.3 ±0.2 8.5\nSRU++ (k= 2) 19.6M 36.1 ±0.1 7.5\nTable 8: Results on IWSLT’14 De →En test set. We\nuse a beam size of 5. BLEU scores and training time\nare averaged over 4 independent runs.\nInference speed Table 7 compares the infer-\nence speed of SRU++ with other top-performing\nmodels on WIKI -103 test set. We use a single\nV100 GPU for inference. Our large model runs\nat least 4.5x faster than all baseline models ex-\ncept Shortformer (Press et al., 2021). In addition,\nour model achieves 0.9-1.1 perplexity lower than\nShortformer and runs 50% faster when using 2 at-\ntention layers (k= 5).\nIWSLT Does SRU++ work well for other\ntasks? We study this question by evaluating\nSRU++ on the IWSLT’14 De →En translation\ntask. We use the open-sourced training and eval-\nuation code of Lin et al. (2020). The base model\nis an 8-layer Transformer model containing 20M\nparameters. We train SRU++ models using 6 lay-\ners and d = 1024, resulting in similar number\nof parameters. We use the original settings such\nas learning rate and batch size, except that we\nuse RAdam optimizer for consistency and increase\nthe number of training epochs to 50. Both archi-\ntectures achieve much higher BLEU scores given\nmore training epochs.3 Table 8 presents the test re-\nsults. Without additional hyperparameter tuning,\nSRU++ achieves 0.4 BLEU score higher and less\ntraining time compared to the Transformer model\ntuned in Lin et al. (2020).\nWhy does SRU++ reduce training cost in our\nexperiments? Several factors contribute to the\ncomputation reduction observed in our experi-\nments. First, combining attention and recurrence\ngives stronger modeling capacity. As shown in\nour experiments, SRU++ often achieves compara-\nble results using fewer layers and/or fewer param-\neters. The required computation are much lower\nfor shallower and smaller models.\nWe also observe higher training efﬁciency, re-\nquiring fewer training steps and smaller training\nbatch compared to several Transformer models.\n3Lin et al. (2020) reports a test BLEU of 35.2. We obtain\n35.9 for the same Transformer model by training longer.\nFor example, SRU++ uses a maximum effective\nbatch size of 98K tokens and 800K training steps\non the BILLION WORD dataset, while the Trans-\nformer model in comparison (Baevski and Auli,\n2019) uses 128K tokens and near 1000K steps.\nThe reduced batch size and gradient updates cut\ndown the training cost.\nFinally, model implementation is an important\nfactor for computation saving. Our implemen-\ntation is highly efﬁcient for two reasons. First,\nthe fast recurrence operation of SRU is a reusable\nmodule that is already optimized for speed (Lei\net al., 2018). Second, since recurrence encodes\npositional information, we can use simple single-\nhead attention and remove positional encoding.\nOn the contrary, advanced attention and po-\nsitional encoding mechanism can generate non-\ntrivial computation overhead. To see this, we mea-\nsure the running time of SRU++ and Transformer-\nXL using Pytorch Proﬁler. Figure 5 (a) shows\nthe average model forward time of a single batch.\nSRU++ runs 4-5x times faster compared to the\nTransformer-XL implementation. Figure 5 (b)\nbreaks down the computation and highlights the\nmost time-consuming operations in both models.\nThe matrix multiplications are one of the most\nexpensive operations for both models. Surpris-\ningly, many operations in the relative attention of\nTransformer-XL are computationally expensive.\nFor example, the relative attention requires shift-\ning the attention scores and adding up different at-\ntention score matrices. Both require a lot of time\nbut they are not needed in non-relative attention.\nIn addition, the last column shows the running\ntime of tensor transpose operators needed by batch\nmatrix-matrix multiplications in attention. Again,\nthe relative attention uses an order of magnitude\nmore time compared to the simple single-head at-\ntention used in our model implementation.4\n6 Related Work\nAccelerating common architectures for NLP has\nbecome an increasingly important research topic\nrecently (Tay et al., 2020; Sun et al., 2020; Lan\net al., 2020). Our work is closely related to two\nlines of research under this topic.\n4Note that this high latency of tensor transpose might be\ncaused by sub-optimal implementation choices such as a poor\narrangement of tensor axes in the open-sourced model. There\nis room for improvement. Nevertheless, relative attention and\npositional encoding are reported to be non-trivially slower in\nother works (Shaw et al., 2018; Tian et al., 2021).\n7641\nTable 1\nTable 2\n42M parameters 139M parameters\nSRU++ 69.8 223.3\nTransformer-XL 284.6 1175.6\n0\n300\n600\n900\n1,200\n41M parameters 139M parameters\n1175.6\n284.6\n223.3\n69.8\nSRU++\nTransformer-XL\nTable 3\nMatMul Mem concat Layernorm Recurrence Rel. position: \nscore shift\nRel. position: \nscore addition\nTranspose\nSRU++ 130.7 15.9 3.3 23.2 9.3\nTransformer-XL 160.4 5 15 64.6 269.1 255.3\n0\n75\n150\n225\n300\nMatMul\nMem concatLayernorm RecurrenceRel. position:\n\nscore shiftRel. position:\n\nscore addition\nTranspose\nSRU++\nTransformer-XL\n(a) (b)\n 1\nFigure 5: Proﬁling of SRU++ and Transformer-XL: (a) forward time (in milliseconds) of small and large models\nand (b) forward time used in various types of time-consuming operations. We use a single GPU for proﬁling to\navoid extra overhead such as data synchronization between GPUs. We use an unroll size / context lengthM = 512\nand 1024 respectively for small and large models. All models use a batch size B = 16for proﬁling.\nFirst, previous works have tackled the speed\nproblem of recurrent neural networks (RNNs)\nand have proposed various fast RNN implemen-\ntations (Diamos et al., 2016; Campos et al., 2018;\nZhang and Sennrich, 2019). Notably, the Quasi-\nRNN (Bradbury et al., 2017) and SRU (Lei et al.,\n2018) have invented highly-parallelizable recur-\nrence and combined them with convolutions or\nhighway networks respectively. The resulting ar-\nchitectures achieve equivalent parallelism as con-\nvolutional and attention models. This advance-\nment eliminates the need of avoiding recurrence\ncomputation to trade model training efﬁciency, a\ndesign choice made by the Transformer architec-\nture. Our model builds on top of SRU.\nSecond, several recent works have argued that\nusing attention alone is not the best architecture\nin terms of model expressiveness. For example,\nDong et al. (2021) demonstrate theoretically and\nempirically that using pure attention results in per-\nformance degeneration. Gulati et al. (2020) have\ncombined convolution and attention and obtained\nnew state-of-the-art results for speech recogni-\ntion. Moreover, RNNs have been incorporated\ninto Transformer architectures, resulting in im-\nproved results in machine translation and language\nunderstanding tasks (Lei et al., 2018; Huang et al.,\n2020). Our work is built upon a similar hypoth-\nesis that recurrence and attention are complemen-\ntary at sequence modeling. We demonstrate that\njointly leveraging fast recurrence and attention not\nonly achieves state-of-the-art modeling results but\nalso obtain signiﬁcant computation reduction.\nBeing orthogonal to our work, many recent\nworks improve the efﬁciency of Transformer mod-\nels by accelerating attention computation (Zaheer\net al., 2020; Katharopoulos et al., 2020; Vyas\net al., 2020; Peng et al., 2021). Examples include\nLongformer (Beltagy et al., 2020), Reformer (Ki-\ntaev et al., 2020), Linformer (Wang et al., 2020)\nand Routing Transformer (Roy et al., 2021). In\ncontrast, our work optimizes computational efﬁ-\nciency using recurrence combined with minimal\nattention and our model can incorporate these at-\ntention variants for additional speed improvement.\n7 Conclusion\nWe present a highly-efﬁcient architecture com-\nbining fast recurrence and attention, and evalu-\nate its effectiveness on various language modeling\ndatasets. We demonstrate fast RNNs with little at-\ntention not only achieve top results but also reduce\ntraining cost signiﬁcantly. Our work shares a dif-\nferent idea to accelerating attention, therefore pro-\nviding an orthogonal direction to advancing state-\nof-the-art model architecture. As future work, we\nbelieve the model can be improved using stronger\nattention or recurrent implementations, better nor-\nmalization or optimization techniques.\nAcknowledgement\nWe would like to thank ASAPP Inc. for mak-\ning this work possible. We thank Hugh Perkins,\nJoshua Shapiro, Sam Bowman, Danqi Chen and\nYu Zhang for providing invaluable feedback for\nthis work. Finally, we thank Jeremy Wohlwend,\nJing Pan, Prashant Sridhar and Kyu Han for help-\nful discussions, and ASAPP Language Technol-\nogy and Infra teams for the compute cluster setup\nfor our research experiments.\n7642\nReferences\nJeremy Appleyard, Tomas Kocisky, and Phil Blun-\nsom. 2016. Optimizing performance of recur-\nrent neural networks on gpus. arXiv preprint\narXiv:1604.01946.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. InInternational Con-\nference on Learning Representations (ICLR).\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document trans-\nformer. arXiv:2004.05150.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-Recurrent Neural Net-\nworks. In International Conference on Learning\nRepresentations (ICLR).\nAndrew Brock, Soham De, and Samuel L Smith. 2021.\nCharacterizing signal propagation to close the per-\nformance gap in unnormalized resnets. In Interna-\ntional Conference on Learning Representations.\nVíctor Campos, Brendan Jou, Xavier Giró i Nieto,\nJordi Torres, and Shih-Fu Chang. 2018. Skip rnn:\nLearning to skip state updates in recurrent neural\nnetworks. In International Conference on Learning\nRepresentations (ICLR).\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. Tech-\nnical report, Google.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics.\nGreg Diamos, Shubho Sengupta, Bryan Catanzaro,\nMike Chrzanowski, Adam Coates, Erich Elsen,\nJesse Engel, Awni Hannun, and Sanjeev Satheesh.\n2016. Persistent rnns: Stashing recurrent weights\non-chip. In Proceedings of The 33rd International\nConference on Machine Learning (ICML).\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas\nLoukas. 2021. Attention is not all you need: pure at-\ntention loses rank doubly exponentially with depth.\nIn Proceedings of the 38th International Conference\non Machine Learning (ICML).\nAngela Fan, Thibaut Lavril, Edouard Grave, Armand\nJoulin, and Sainbayar Sukhbaatar. 2020. Access-\ning higher-level representations in sequential trans-\nformers with feedback memory. arXiv preprint\narXiv:2002.09402.\nEdouard Grave, Armand Joulin, Moustapha Cissé,\nHervé Jégou, et al. 2017. Efﬁcient softmax approx-\nimation for gpus. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning (ICML).\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,\nZhengdong Zhang, Yonghui Wu, and Ruoming\nPang. 2020. Conformer: Convolution-augmented\ntransformer for speech recognition. In Proceedings\nof the 21st Annual Conference of the International\nSpeech (INTERSPEECH).\nJie Hao, Xing Wang, Baosong Yang, Longyue Wang,\nJinfeng Zhang, and Zhaopeng Tu. 2019. Modeling\nrecurrence for transformer. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation.\nElad Hoffer, Itay Hubara, and Daniel Soudry. 2017.\nTrain longer, generalize better: closing the gener-\nalization gap in large batch training of neural net-\nworks. In Advances in Neural Information Process-\ning Systems.\nYi-Te Hsu, Sarthak Garg, Yi-Hsiu Liao, and Ilya\nChatsviorkin. 2020. Efﬁcient inference for neural\nmachine translation. In Proceedings of SustaiNLP:\nWorkshop on Simple and Efﬁcient Natural Language\nProcessing.\nZhiheng Huang, Peng Xu, Davis Liang, Ajay Mishra,\nand Bing Xiang. 2020. Trans-blstm: Transformer\nwith bidirectional lstm for language understanding.\narXiv preprint arXiv:2003.07000.\nMarcus Hutter. 2006. The human knowledge compres-\nsion contest. http://prize.hutter1.net/.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nRNNs: Fast autoregressive transformers with linear\nattention. In Proceedings of the 37th International\nConference on Machine Learning (ICML).\n7643\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nYoung Jin Kim, Marcin Junczys-Dowmunt, Hany Has-\nsan, Alham Fikri Aji, Kenneth Heaﬁeld, Roman\nGrundkiewicz, and Nikolay Bogoychev. 2019. From\nresearch to production and back: Ludicrously fast\nneural machine translation. In Proceedings of the\n3rd Workshop on Neural Generation and Transla-\ntion.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR).\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2018. Dynamic evaluation of neural\nsequence models. In Proceedings of the 35th Inter-\nnational Conference on Machine Learning (ICML).\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations (ICLR).\nTao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav\nArtzi. 2018. Simple recurrent units for highly par-\nallelizable recurrence. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nAlexander Lin, Jeremy Wohlwend, Howard Chen, and\nTao Lei. 2020. Autoregressive knowledge distilla-\ntion through imitation learning. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nZhouhan Lin, Minwei Feng, Cícero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. In International Conference on Learn-\ning Representations (ICLR).\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2020a. On the variance of the adaptive learning rate\nand beyond. In International Conference on Learn-\ning Representations (ICLR).\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu\nChen, and Jiawei Han. 2020b. Understanding the\ndifﬁculty of training transformers. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nStephen Merity. 2019. Single headed attention rnn:\nStop thinking with your head. arXiv preprint\narXiv:1911.11423.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nJinhwan Park, Yoonho Boo, Iksoo Choi, Sungho Shin,\nand Wonyong Sung. 2018. Fully neural network\nbased speech recognition on mobile and embedded\ndevices. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In International Confer-\nence on Learning Representations (ICLR).\nOﬁr Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using\nshorter inputs. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learn-\ning Representations (ICLR).\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efﬁcient content-based\nsparse attention with routing transformers. Transac-\ntions of the Association for Computational Linguis-\ntics.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green AI. Communications of the\nACM.\nYuan Shangguan, Jian Li, Qiao Liang, Raziel Alvarez,\nand Ian McGraw. 2019. Optimizing speech recogni-\ntion for the edge. arXiv preprint arXiv:1909.12408.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nSheng Shen, Zhewei Yao, Amir Gholami, Michael Ma-\nhoney, and Kurt Keutzer. 2020. PowerNorm: Re-\nthinking batch normalization in transformers. In\nProceedings of the 37th International Conference on\nMachine Learning (ICML).\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019a. Adaptive at-\ntention span in transformers. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Herve Jegou, and Armand Joulin. 2019b.\nAugmenting self-attention with persistent memory.\narXiv preprint arXiv:1907.01470.\n7644\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobile-\nBERT: a compact task-agnostic BERT for resource-\nlimited devices. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732.\nRan Tian, Joshua Maynez, and Ankur P Parikh. 2021.\nShatter: An efﬁcient transformer encoder with\nsingle-headed self-attention and relative sequence\npartitioning. arXiv preprint arXiv:2108.13032.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nApoorv Vyas, Angelos Katharopoulos, and François\nFleuret. 2020. Fast transformers with clustered at-\ntention. In Advances in Neural Information Process-\ning Systems (NeurIPS).\nSinong Wang, Belinda Z Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tieyan Liu. 2020. On layer\nnormalization in the transformer architecture. In\nProceedings of the 37th International Conference on\nMachine Learning (ICML).\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang\nZhao, and Junyang Lin. 2019. Understanding and\nimproving layer normalization. In Advances in Neu-\nral Information Processing Systems (NeurIPS).\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems (NeurIPS).\nBiao Zhang and Rico Sennrich. 2019. A lightweight\nrecurrent network for sequence modeling. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics.\n7645\nA Additional results\nA.1 Detailed analysis of attention\nTable 10 presents a more comprehensive analysis\nof attention in SRU++ models. First, we change\nthe number of attention layers and their locations\nin the model. As shown in the top block of Ta-\nble 10, using attention in 50% of the layers leads\nto no (or negligible) loss in model performance.\nThis is consistent with the results in Table 2 using\na smaller model. Enabling attention in higher lay-\ners performs slightly better than evenly distribut-\ning attention from the bottom to top layers.\nWe also experiment with using more than one\nattention head in each of the attention layer, as\nshown in the middle block of the table. Unlike\nTransformer models however, we do not observe\na signiﬁcant improvement using multiple heads.\nWe hypothesize that the recurrence states can al-\nready carry different features or information that\nare present in different input positions, making re-\ndundant heads unnecessary.\nFinally, changing the ratio d : d′ from 4 to 8\ngives similar improvements regardless of using 2\nattention layers or 10 attention layers. This sug-\ngests that the amount of attention and the hid-\nden size ratio can be tuned independently for best\nmodel performance.\nA.2 The effectiveness of layer normalization\nIn our experiments, we have always used layer\nnormalization to stabilize training. However, we\nalso found layer normalization to achieve worse\ngeneralization for larger models that are more\nprone to over-ﬁtting. Figure 6 showcases our em-\npirical observation on the ENWIK 8 dataset. Us-\ning layer normalization achieves more rapid train-\ning progress and lower training loss, but results\nin higher dev loss in the case of training a 108M\nmodel. This generalization gap remains even if\nwe tune the dropout rate carefully. In addition,\nalthough using layer normalization in the smaller\nmodel with 41M parameters gives slightly better\ndev results, we still observe a larger generalization\ngap (indicated by the difference between training\nloss and dev loss) compared to the run without\nlayer normalization. Similar over-ﬁtting patterns\nare observed on Wiki-103 dataset, and also in pre-\nvious work (Xu et al., 2019).\nOn the other hand, turning off layer normaliza-\ntion can achieve better generalization but makes\ntraining sensitive to learning rate and parameter\ninitialization. For example, we have to use a\nsmaller learning rate of 0.00025 or lower to avoid\nsudden gradient explosion during training. These\nresults suggest possible future work by improv-\ning the normalization method (Shen et al., 2020;\nBrock et al., 2021).\nA.3 Tuning weight decay and learning rate\nWe ﬁnd that tuning the weight decay and learn-\ning rate critical to the success of training SRU++\nand achieving best results. Table 9 provides a sen-\nsitivity analysis by testing different learning rates\nand weight decay values. Increasing the weight\ndecay consistently gives better results for all learn-\ning rates tested. Tuning the learning rate is also\nneeded to reach the best result. The non-trivial\neffect of weight decay seems to be unique for\nSRU++.\nOn the other hand, the performance of SRU++\nremains robust once the appropriate weight decay\nand learning rate are set. As shown in previous\nresults and analyses, SRU++ achieves strong and\nrelatively stable results to various hidden sizes,\nnumber of attention layers and datasets. In partic-\nular, using the same weight decay value general-\nize well for all datasets (including language mod-\neling and translation tasks) and model conﬁgura-\ntions tested.\n0.10 0 .01 0 .00\n3 ×10−4 1.014 - -\n2 ×10−4 1.022 1 .035 1 .047\n1.5 ×10−4 1.030 1 .038 1 .040\nTable 9: Dev BPC of SRU++ given a learning rate ∈\n{1.5,2,3}×10−4 and a weight decay ∈{0.1,0.01,0}.\n‘-‘ means the training run diverged or got gradient ex-\nplosion.\nB Training details\nLanguage modeling We use the RAdam opti-\nmizer5 with the default hyperparameters β1 = 0.9\nand β2 = 0.999 for all our experiments. We use a\ncosine learning rate schedule with only 1 cycle for\nsimplicity. For faster training, we also leverage the\nnative automatic mixed precision (AMP) training\nand distributed data parallel (DDP) of Pytorch in\nall experiments, except those in Table 1 and Fig-\n5https://github.com/LiyuanLucasLiu/\nRAdam\n7646\nure 1 for a fair comparison with the Transformer-\nXL implementation.\nTable 11 shows the detailed training conﬁgura-\ntion of SRU++ models on ENWIK 8 dataset. Most\ntraining options are kept the same for all models.\nWe tune the dropout probability more carefully as\nwe found training is more prone to over-ﬁtting and\nunder-ﬁtting for this dataset. The large model is\ntrained with 2x batch size. As a result, we increase\nthe learning rate proportionally by a factor of\n√\n2\n(Hoffer et al., 2017), which results in a rounded\nlearning rate of 0.0004.\nTable 12 presents the detailed training conﬁg-\nuration on W IKI -103 dataset. Similarly we use\nd = 3072 and d = 4096 for the base and large\nmodel respectively for a hidden size ratio d: d′=\n4 : 1. Following (Baevski and Auli, 2019), we use\nan adaptive word embedding layer and an adap-\ntive softmax layer for our models, and we tie the\nweight matrices of the two layers. We keep the\ntotal number of parameters comparable when we\nuse a different hidden size ratio d: d′= 8 : 1.\nMachine translation We use the open-sourced\ncode from Lin et al. (2020) for the IWSLT’14\nDe→En translation task. The Transformer model\ntuned by the original work uses 8 layers for both\nthe encoder and decoder and a total of 20M pa-\nrameters. Most of the training conﬁguration re-\nmains the same as the original work 6, except for\na couple of changes. First, we use RAdam opti-\nmizer and the same β values for consistency with\nthe language model task. We use the same weight\ndecay value of 0.1 for SRU++. The Transformer\nmodel uses a weight decay of 0 that is tuned based\non dev set performance. Second, we increase the\nnumber of training epochs to 50 (or equivalently\n64K training steps) since all models achieve better\nBLEU scores by training longer. This ensures we\ncompare models when they reach the maximum\nperformance.\nOur SRU++ model uses a hidden sized= 1024,\nan attention size d′= 256and 6 layers for the en-\ncoder and decoder, resulting in a similar number\nof parameters as the Transformer model in com-\nparison. Let Xsrc be the output representation of\nthe SRU++ encoder. Each SRU++ decoder layer\nmake uses ofXsrc by simplying treating it as extra\nattention context. That is, the query, key and value\n6https://github.com/asappresearch/\nimitkd/blob/master/configs/iwslt/\nteacher.yaml\nrepresentations are computed by concatenating the\ninput of the current layer Xtgt with Xsrc,\nQ = [Qsrc,Qtgt]\n= Wq [Xsrc,Xtgt]⊤\nK = Wk Q\nV = Wv Q\nThe resulting representations Qtgt, K and V are\nused for the rest of the attention computation. The\nattention mask is set such that each target token\ncan only attend to all source tokens and preceding\ntarget tokens.\n7647\nLayers that has attention Num of heads d d ′ Model size Dev BPC\nAll layers 1 3072 768 108M 0.997\n6,7,8,9,10 102M 0.997\n2,4,6,8,10 102M 0.999\n8,9,10 3136 784 103M 1.000\n3,6,9 1.001\n5,10 1 3072 768 98M 1.002\n2 1.002\n10 1 97M 1.007\n2 1.006\nAll layers 1 3072 768 108M 0.997\n5,10 98M 1.002\nAll layers 4480 560 109M 0.991\n5,10 104M 0.997\nTable 10: Results of 10-layer SRU++ models by varying the attention setting. We report the dev BPC on the EN-\nWIK 8 dataset. The ﬁrst column indicates layers where the attention are located. Smaller index numbers represent\nlayers that are closer to the input of the model.\nw/o layernorm (train)\nWall time Step Value\n1611337071.5371500 1000 2.657274007797240\n1611337101.2236000 1200 2.529681444168090\n1611337219.7553800 2000 2.169663906097410\n1611337249.5076100 2200 2.107694625854490\n1611337308.776290 2600 2.004533529281620\n1611337338.5322400 2800 1.9634863138198900\n1611337368.2808800 3000 1.9250108003616300\n1611337427.8713000 3400 1.857570767402650\n1611337457.7218200 3600 1.8287327289581300\n1611337487.7397000 3800 1.8009421825408900\n1611337517.8699700 4000 1.7779759168624900\n1611337636.9908000 4800 1.6934561729431200\n1611337906.0310200 6600 1.5519849061965900\n1611337935.4139400 6800 1.540087342262270\n1611337995.2831700 7200 1.5166194438934300\n1611338084.6097400 7800 1.3757587671279900\n1611338192.2343200 8000 1.3422778844833400\n1611338251.5155600 8400 1.2865111827850300\n1611338311.247780 8800 1.246571660041810\n1611338341.3814700 9000 1.2302122116088900\n1611338401.182700 9400 1.2039235830307000\n1611338460.9054000 9800 1.1817585229873700\n1611338491.2045100 10000 1.1716248989105200\n1611338579.986360 10600 1.1438642740249600\n1611338609.7874100 10800 1.1355069875717200\n1611338640.2011400 11000 1.1273367404937700\n1611338669.9184400 11200 1.1194311380386400\n1611338699.8080100 11400 1.1118884086608900\n1611338789.5217400 12000 1.0913453102111800\n1611338819.2172100 12200 1.0847340822219800\n1611338878.6564300 12600 1.0736950635910000\n1611338938.634150 13000 1.0638755559921300\n1611338968.3398900 13200 1.0596907138824500\n1611338998.3212200 13400 1.0557032823562600\n1611339028.213880 13600 1.0519869327545200\n1611339117.329950 14200 1.041569709777830\n1611339147.1197000 14400 1.0384671688079800\n1611339177.1859300 14600 1.0354359149932900\n1611339206.8670300 14800 1.0324267148971600\n1611339266.3683800 15200 1.0270730257034300\n1611339356.9129400 15800 1.0193467140197800\n1611339584.4410800 16800 1.0082122087478600\n1611339614.040620 17000 1.0062239170074500\n1611339644.1845000 17200 1.0042963027954100\n1611339674.0886400 17400 1.0023585557937600\n1611339703.9688600 17600 1.0005073547363300\n1611339734.3312200 17800 0.9987223744392400\n1611339884.082410 18800 0.9902057647705080\n1611340122.1803100 20400 0.9785571694374080\n1611340211.9193500 21000 0.9747878909111020\n1611340241.847840 21200 0.9735962748527530\n1611340451.0443200 22600 0.965568482875824\n1611340540.6210800 23200 0.9623193740844730\n1611340600.2668500 23600 0.9603083729743960\n1611340630.008350 23800 0.9593459963798520\n1611340737.6202000 24000 0.9583930373191830\n1611340856.9190000 24800 0.9547959566116330\n1611340886.8481200 25000 0.9539097547531130\n1611340916.538310 25200 0.9529796838760380\n1611341006.068060 25800 0.9504586458206180\n1611341035.730980 26000 0.9496104121208190\n1611341125.1855200 26600 0.9472671151161190\n1611341244.7483000 27400 0.9442614912986760\n1611341304.3362200 27800 0.942823588848114\n1611341364.2134900 28200 0.9414118528366090\n1611341393.8960400 28400 0.9407221078872680\n1611341423.7701300 28600 0.9400051832199100\n1611341453.7358700 28800 0.9393438696861270\n1611341572.7540600 29600 0.9367689490318300\n1611341632.4866600 30000 0.9355084300041200\n1611341721.9245800 30600 0.9335870146751400\n1611341751.693580 30800 0.9329618811607360\n1611341781.3765400 31000 0.9323965907096860\n1611341840.70701 31400 0.9312952756881710\n1611341870.8656600 31600 0.9307621121406560\n1611341900.8361200 31800 0.9302324652671810\n1611342068.1920400 32400 0.9285956621170040\n1611342127.5092600 32800 0.9275936484336850\n1611342158.0955600 33000 0.9270856976509090\n1611342217.7881900 33400 0.9260081052780150\n1611342247.5522200 33600 0.9255082607269290\n1611342336.847130 34200 0.9240961670875550\n1611342485.909570 35200 0.921694815158844\n1611342515.6840200 35400 0.9212267398834230\n1611342545.5141900 35600 0.9207830429077150\n1611342604.88217 36000 0.919914722442627\n1611342634.8365800 36200 0.91945481300354\n1611342694.1477900 36600 0.9186022281646730\n1611342753.1361600 37000 0.9177930355072020\n1611342782.943090 37200 0.917381763458252\n1611342812.6031400 37400 0.916907548904419\n1611342842.197100 37600 0.9164507985115050\n1611342901.6008600 38000 0.9156600832939150\nw/o layernorm (dev)\nWall time Step Value\n1611338188.9598500 8000 1.0663686990737900\n1611339461.661960 16000 0.9641713500022890\n1611340734.3594600 24000 0.9330013990402220\n1611342004.957120 32000 0.9114237427711490\n1611343273.7114700 40000 0.8965566158294680\n1611344546.9411700 48000 0.8856385946273800\n1611345817.9004100 56000 0.8756989240646360\n1611347089.4916100 64000 0.8720692992210390\n1611348360.7054900 72000 0.8686217665672300\n1611349630.3297000 80000 0.8582323789596560\n1611350901.0565900 88000 0.8546746969223020\n1611352174.063930 96000 0.8520373702049260\n1611353446.0834700 104000 0.8436130285263060\n1611354715.4457800 112000 0.839104950428009\n1611355987.3004200 120000 0.8373277187347410\n1611357256.235530 128000 0.8320601582527160\n1611358527.3281100 136000 0.8271564245224000\n1611359798.74332 144000 0.8258444666862490\n1611361069.4354800 152000 0.823585569858551\n1611362340.716950 160000 0.817122220993042\n1611363611.8503900 168000 0.8170658946037290\n1611364885.2492400 176000 0.8112522959709170\n1611366157.4374100 184000 0.8087211847305300\n1611367430.2773400 192000 0.8067322969436650\n1611368697.6260400 200000 0.803813636302948\n1611369967.8688700 208000 0.7996566295623780\n1611371239.9054200 216000 0.7956663966178890\n1611372513.0585700 224000 0.7940474152565000\n1611373787.0576800 232000 0.7916958928108220\n1611375055.3947400 240000 0.7893047332763670\n1611376325.6399600 248000 0.7844022512435910\n1611377594.440950 256000 0.7830475568771360\n1611378864.5893200 264000 0.7798649072647100\n1611380133.4820300 272000 0.7771931886672970\n1611381403.405950 280000 0.775232195854187\n1611382675.2811500 288000 0.7718082070350650\n1611383943.104460 296000 0.7713218927383420\n1611385211.9628000 304000 0.767278254032135\n1611386480.939330 312000 0.7652620077133180\n1611387754.8526100 320000 0.7647847533226010\n1611389025.2982300 328000 0.7623053193092350\n1611390300.0603400 336000 0.7601140737533570\n1611391572.798170 344000 0.7588826417922970\n1611392841.4177200 352000 0.7573227286338810\n1611394111.5734000 360000 0.7567527890205380\n1611395381.0969100 368000 0.7558491230010990\n1611396652.4800000 376000 0.7554975748062130\n1611397924.6392000 384000 0.755299985408783\n1611399203.1538300 392000 0.755027711391449\n1611400486.0493300 400000 0.7549550533294680\nw/ layernorm (train)\nWall time Step Value\n1611336585.1795100 1000 2.119985580444340\n1611336616.170890 1200 2.014101505279540\n1611336740.0102600 2000 1.7642699480056800\n1611336771.302070 2200 1.7258100509643600\n1611336833.7248400 2600 1.6623579263687100\n1611336864.8088900 2800 1.6381422281265300\n1611336895.9084900 3000 1.6147537231445300\n1611336958.114630 3400 1.5731769800186200\n1611336989.4217000 3600 1.5555953979492200\n1611337020.5708900 3800 1.5379095077514600\n1611337051.9692100 4000 1.5242030620575000\n1611337177.0957400 4800 1.470809817314150\n1611337456.8190400 6600 1.3770489692688000\n1611337488.0621600 6800 1.3685448169708300\n1611337549.6526300 7200 1.351279854774480\n1611337642.715610 7800 1.245132565498350\n1611337756.2791600 8000 1.2252988815307600\n1611337818.7660500 8400 1.1928768157959000\n1611337881.578760 8800 1.1683428287506100\n1611337913.0581700 9000 1.157269835472110\n1611337975.783070 9400 1.1379694938659700\n1611338037.9804700 9800 1.120625615119930\n1611338069.0690400 10000 1.112343668937680\n1611338162.5638600 10600 1.0888702869415300\n1611338193.7687400 10800 1.0816880464553800\n1611338225.2150600 11000 1.0745174884796100\n1611338256.396260 11200 1.0675355195999100\n1611338287.5100400 11400 1.0608973503112800\n1611338380.4976600 12000 1.042535662651060\n1611338411.5279200 12200 1.036475419998170\n1611338472.8294700 12600 1.025826334953310\n1611338535.0480400 13000 1.0157215595245400\n1611338566.1592700 13200 1.0116504430770900\n1611338597.1102400 13400 1.0079761743545500\n1611338628.3485200 13600 1.004743218421940\n1611338722.0753100 14200 0.9962643980979920\n1611338753.3985300 14400 0.9937120079994200\n1611338784.6021100 14600 0.9912429451942440\n1611338815.953610 14800 0.9888066053390500\n1611338878.8850800 15200 0.9843021631240850\n1611338973.1256500 15800 0.9779269099235540\n1611339210.3758000 16800 0.9688680768013\n1611339241.5963800 17000 0.9672225117683410\n1611339272.972990 17200 0.9655944108963010\n1611339304.4491400 17400 0.9639755487442020\n1611339336.1232700 17600 0.9623585343360900\n1611339367.7075200 17800 0.9608253240585330\n1611339524.4569400 18800 0.9537055492401120\n1611339775.1552800 20400 0.9437184929847720\n1611339868.1317500 21000 0.9404662251472470\n1611339898.704870 21200 0.9393579959869390\n1611340115.1396300 22600 0.9323247075080870\n1611340207.9069300 23200 0.9294213652610780\n1611340269.5831200 23600 0.9275630712509160\n1611340300.221280 23800 0.9266935586929320\n1611340413.4230900 24000 0.9258607029914860\n1611340538.3761700 24800 0.9226695895195010\n1611340569.5487200 25000 0.921903133392334\n1611340600.8156200 25200 0.9210814833641050\n1611340694.286360 25800 0.9187629222869870\n1611340725.7628500 26000 0.9179810285568240\n1611340819.9386000 26600 0.91579270362854\n1611340944.4259200 27400 0.9129766225814820\n1611341006.6115000 27800 0.9116482138633730\n1611341069.1364000 28200 0.910325825214386\n1611341100.2758100 28400 0.9097062945365910\n1611341131.737250 28600 0.9090807437896730\n1611341163.048630 28800 0.9084513187408450\n1611341287.5370200 29600 0.9060854315757750\n1611341350.0134600 30000 0.9048634171485900\n1611341443.502870 30600 0.9030522704124450\n1611341474.5326900 30800 0.9024683237075810\n1611341505.3050900 31000 0.9018980264663700\n1611341567.2212900 31400 0.9007482528686520\n1611341598.4170500 31600 0.9002240300178530\n1611341629.9051900 31800 0.8996858596801760\n1611341805.8235500 32400 0.8980556130409240\n1611341868.2460200 32800 0.89703369140625\n1611341899.413220 33000 0.8965168595314030\n1611341962.3077700 33400 0.895495593547821\n1611341993.7472700 33600 0.8949951529502870\n1611342088.1351600 34200 0.8935126066207890\n1611342242.7875600 35200 0.891061007976532\n1611342273.2558900 35400 0.890608549118042\n1611342304.148320 35600 0.890160322189331\n1611342366.6841200 36000 0.8892725706100460\n1611342398.2851200 36200 0.8888065814971920\n1611342460.4211300 36600 0.8879335522651670\n1611342522.8538000 37000 0.887054443359375\n1611342553.8458700 37200 0.8866237998008730\n1611342585.2092600 37400 0.8861683011054990\n1611342616.0320800 37600 0.8856978416442870\n1611342678.5250100 38000 0.8848704695701600\nw/ layernorm (dev)\nWall time Step Value\n1611337751.4762900 8000 1.0214351415634200\n1611339081.8197300 16000 0.939243495464325\n1611340408.721070 24000 0.9097658395767210\n1611341738.663950 32000 0.8873761296272280\n1611343067.5646800 40000 0.8747373223304750\n1611344401.873110 48000 0.8627641797065740\n1611345732.8109500 56000 0.8536322712898250\n1611347048.0255700 64000 0.8486016392707830\n1611348367.725490 72000 0.8427955508232120\n1611349690.5925300 80000 0.837395966053009\n1611351013.3003600 88000 0.832574725151062\n1611352332.728960 96000 0.8289396166801450\n1611353660.6106600 104000 0.8239091634750370\n1611354988.0951900 112000 0.8162994384765630\n1611356315.117280 120000 0.8147459626197820\n1611357636.103280 128000 0.8125931620597840\n1611358964.5019200 136000 0.807649552822113\n1611360293.1279000 144000 0.8055095672607420\n1611361622.5774200 152000 0.8034166097640990\n1611362953.502950 160000 0.7980242967605590\n1611364284.5170000 168000 0.7979138493537900\n1611365617.0539000 176000 0.7920936942100530\n1611366946.1695700 184000 0.7901331186294560\n1611368279.902790 192000 0.7885696887969970\n1611369609.8335600 200000 0.7873808145523070\n1611370934.3665800 208000 0.7831627726554870\n1611372258.80488 216000 0.7794942855834960\n1611373589.312810 224000 0.7778018712997440\n1611374919.3646400 232000 0.7751069068908690\n1611376249.6181500 240000 0.7731921672821050\n1611377578.853580 248000 0.77020263671875\n1611378906.2698500 256000 0.768462598323822\n1611380236.9947000 264000 0.7674402594566350\n1611381565.013780 272000 0.7644534111022950\n1611382895.6540700 280000 0.7643362879753110\n1611384225.2520600 288000 0.7619809508323670\n1611385553.8424000 296000 0.7606557607650760\n1611386881.7432300 304000 0.7560462355613710\n1611388215.0718100 312000 0.7567671537399290\n1611389542.786990 320000 0.7553955316543580\n1611390870.9998500 328000 0.7532590627670290\n1611392203.4092700 336000 0.7519116997718810\n1611393528.784500 344000 0.7512940168380740\n1611394857.3429300 352000 0.7508137226104740\n1611396186.8103100 360000 0.750404417514801\n1611397515.6988500 368000 0.7495713233947750\n1611398845.7995100 376000 0.7493221759796140\n1611400176.2838000 384000 0.748978853225708\n1611401500.7605100 392000 0.7489020228385930\n1611402826.5938500 400000 0.74882972240448\nTable 1\nStep w/o layernorm \n(train)\nw/o layernorm (dev) w/ layernorm \n(train)\nw/ layernorm \n(dev)\n1000 2.657274007797240\n1200 2.529681444168090\n2000 2.169663906097410\n2200 2.107694625854490\n2600 2.004533529281620\n2800 1.9634863138198900\n3000 1.9250108003616300\n3400 1.857570767402650\n3600 1.8287327289581300\n3800 1.8009421825408900\n4000 1.7779759168624900\n4800 1.6934561729431200\n6600 1.5519849061965900\n6800 1.540087342262270\n7200 1.5166194438934300\n7800 1.3757587671279900\n8000 1.3422778844833400\n8400 1.2865111827850300\n8800 1.246571660041810\n9000 1.2302122116088900\n9400 1.2039235830307000\n9800 1.1817585229873700\n10000 1.1716248989105200\n10600 1.1438642740249600\n10800 1.1355069875717200\n11000 1.1273367404937700\n11200 1.1194311380386400\n11400 1.1118884086608900\n12000 1.0913453102111800\n12200 1.0847340822219800\n12600 1.0736950635910000\n13000 1.0638755559921300\n13200 1.0596907138824500\n13400 1.0557032823562600\n13600 1.0519869327545200\n14200 1.041569709777830\n14400 1.0384671688079800\n14600 1.0354359149932900\n14800 1.0324267148971600\n15200 1.0270730257034300\n15800 1.0193467140197800\n16800 1.0082122087478600\n17000 1.0062239170074500\n17200 1.0042963027954100\n17400 1.0023585557937600\n17600 1.0005073547363300\n17800 0.9987223744392400\n18800 0.9902057647705080\n20400 0.9785571694374080\n21000 0.9747878909111020\n21200 0.9735962748527530\n22600 0.965568482875824\n23200 0.9623193740844730\n23600 0.9603083729743960\n23800 0.9593459963798520\n24000 0.9583930373191830\n24800 0.9547959566116330\n25000 0.9539097547531130\n25200 0.9529796838760380\n25800 0.9504586458206180\n26000 0.9496104121208190\n26600 0.9472671151161190\n27400 0.9442614912986760\n27800 0.942823588848114\n28200 0.9414118528366090\n28400 0.9407221078872680\n28600 0.9400051832199100\n28800 0.9393438696861270\n29600 0.9367689490318300\n30000 0.9355084300041200\n30600 0.9335870146751400\n30800 0.9329618811607360\n31000 0.9323965907096860\n31400 0.9312952756881710\n31600 0.9307621121406560\n31800 0.9302324652671810\n32400 0.9285956621170040\n32800 0.9275936484336850\n33000 0.9270856976509090\n33400 0.9260081052780150\n33600 0.9255082607269290\n34200 0.9240961670875550\n35200 0.921694815158844\n35400 0.9212267398834230\n35600 0.9207830429077150\n36000 0.919914722442627\n36200 0.91945481300354\n36600 0.9186022281646730\n37000 0.9177930355072020\n37200 0.917381763458252\n37400 0.916907548904419\n37600 0.9164507985115050\n38000 0.9156600832939150\n0.69\n0.78\n0.87\n0.96\n0K 100K 200K 300K 400K\n41M parameters\n0.62\n0.70\n0.77\n0.85\n0K 100K 200K 300K 400K\nw/o layernorm (train)\nw/o layernorm (dev)\nw/ layernorm (train)\nw/ layernorm (dev)\n108M parameters\n 1\nFigure 6: Understanding the empirical effect of layer normalization. We show the training and dev loss of SRU++\nmodels using 41M parameters and 108M parameters on ENWIK 8 dataset. The model with layer normalization ﬁts\nthe training data better, but achieves worse generalization.\n7648\nBase model Base model Large model Large model\n(k= 5)\nAttention / unroll size - train 1024 1024 1024 1024\nAttention / unroll size - test 3072 3072 3072 3072\nBatch size ×Num of GPUs 4 ×8 4 ×8 8 ×8 8 ×8\nDropout 0.22 0.22 0.32 0.35\nGradient clipping 1.0 1.0 1.0 1.0\nHidden size ratio d: d′ 4 4 4 8\nHidden size d 3072 3072 4096 6016\nHidden size d′ 768 768 1024 752\nLearning rate 0.0003 0.0003 0.0004 0.0004\nLR warmup steps 16K 16K 16K 16K\nTraining steps 400K 400K 400K 400K\nWeight decay 0.1 0.1 0.1 0.1\nModel size 98M 108M 191M 195M\nDev BPC 1.002 0.997 0.985 0.974\nTest BPC 0.980 0.974 0.963 0.953\nTable 11: Training details of SRU++ models on ENWIK 8 dataset.\nBase model Large model Large model Large model\n(k= 5)\nAttention / unroll size - train 768 1024 1024 1024\nAttention / unroll size - test 2560 2560 2560 2560\nBatch size ×Num of GPUs 8 ×8 8 ×8 8 ×8 8 ×8\nDropout 0.15 0.2 0.2 0.2\nGradient clipping 1.0 1.0 1.0 1.0\nHidden size ratio d: d′ 4 4 8 8\nHidden size d 3072 4096 5952 5952\nHidden size d′ 768 1024 744 744\nLearning rate 0.0003 0.0003 0.0003 0.0003\nLR warmup steps 16K 16K 16K 16K\nTraining steps 400K 400K 400K 400K\nWeight decay 0.1 0.1 0.1 0.1\nModel size 148M 232M 225M 234M\nDev PPL 17.5 16.7 16.6 16.4\nTest PPL 18.3 17.4 17.3 17.1\nTable 12: Training details of SRU++ models on WIKI -103 dataset."
}