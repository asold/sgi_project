{
  "title": "CPTR: Full Transformer Network for Image Captioning",
  "url": "https://openalex.org/W3124149278",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1765042403",
      "name": "Liu Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2349167958",
      "name": "Chen, Sihan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302753952",
      "name": "Guo, Longteng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2172858303",
      "name": "Zhu, Xinxin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2015439515",
      "name": "Liu Jing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W639708223",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W2552161745",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2971310675",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2965697393",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2984138079"
  ],
  "abstract": "In this paper, we consider the image captioning task from a new sequence-to-sequence prediction perspective and propose CaPtion TransformeR (CPTR) which takes the sequentialized raw images as the input to Transformer. Compared to the \"CNN+Transformer\" design paradigm, our model can model global context at every encoder layer from the beginning and is totally convolution-free. Extensive experiments demonstrate the effectiveness of the proposed model and we surpass the conventional \"CNN+Transformer\" methods on the MSCOCO dataset. Besides, we provide detailed visualizations of the self-attention between patches in the encoder and the \"words-to-patches\" attention in the decoder thanks to the full Transformer architecture.",
  "full_text": "CPTR: FULL TRANSFORMER NETWORK FOR IMAGE CAPTIONING\nWei Liu*1,2, Sihan Chen*1,2, Longteng Guo1,2, Xinxin Zhu1, Jing Liu1,2\n1 National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences\n2 School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences\nABSTRACT\nIn this paper, we consider the image captioning task from\na new sequence-to-sequence prediction perspective and pro-\npose CaPtion TransformeR (CPTR) which takes the sequen-\ntialized raw images as the input to Transformer. Compared\nto the “CNN+Transformer” design paradigm, our model can\nmodel global context at every encoder layer from the begin-\nning and is totally convolution-free. Extensive experiments\ndemonstrate the effectiveness of the proposed model and we\nsurpass the conventional “CNN+Transformer” methods on\nthe MSCOCO dataset. Besides, we provide detailed visual-\nizations of the self-attention between patches in the encoder\nand the “words-to-patches” attention in the decoder thanks to\nthe full Transformer architecture.\nIndex Terms— image captioning, Transformer, sequence-\nto-sequence\n1. INTRODUCTION\nImage captioning is a challenging task which concerns about\ngenerating a natural language to describe the input image au-\ntomatically. Currently, most captioning algorithms follow an\nencoder-decoder architecture in which a decoder network is\nused to predict words according to the feature extracted by\nthe encoder network via attention mechanism. Inspired by\nthe great success of Transformer [1] in the natural language\nprocessing ﬁeld, recent captioning models tend to replace the\nRNN model with Transformer in the decoder part for its ca-\npacity of parallel training and excellent performance, how-\never, the encoder part always remains unchanged, i.e., uti-\nlizing a CNN model (e.g. ResNet [2]) pretrained on image\nclassiﬁcation task to extract spatial feature or a Faster-RCNN\n[3] pretrained on object detection task to extract bottom-up\n[4] feature.\nRecently, researches about applying Transformer to com-\nputer vision ﬁeld have attracted extensive attention. For\nexample, DETR [5] utilizes Transformer to decode detection\npredictions without prior knowledge such as region proposals\nand non-maximal suppression. ViT [6] ﬁrstly utilizes Trans-\nformer without any applications of convolution operation for\n* Wei Liu and Sihan Chen contribute equally to this paper.\nimage classiﬁcation and shows promising performance espe-\ncially when pretrained on very huge datasets (i.e., ImageNet-\n21K, JFT). After that, full Transformer methods for both\nhigh-level and low-level down-stream tasks emerge, such as\nSETR [7] for image semantic segmentation and IPT [8] for\nimage processing.\nInspired by the above works, we consider solving the im-\nage captioning task from a new sequence-to-sequence per-\nspective and propose CaPtion TransformeR (CPTR), a full\nTransformer network to replace the CNN in the encoder part\nwith Transformer encoder which is totally convolution-free.\nCompared to the conventional captiong models taking as in-\nput the feature extracted by CNN or object detector, we di-\nrectly sequentialize raw images as input. Speciﬁcally, we di-\nvide an image into small patches of ﬁxed size (e.g. 16 ×16),\nﬂatten each patch and reshape them into a 1D patch sequence.\nThe patch sequence passes through a patch embedding layer\nand a learnable positional embedding layer before being fed\ninto the Transformer encoder.\nCompared to the “CNN+Transformer” paradigm, CPTR\nis a more simple yet effective method that totally avoids con-\nvolution operation. Due to the local operator essence of con-\nvolution, the CNN encoder has limitation in global context\nmodeling which can only be fulﬁlled by enlarging receptive\nﬁeld gradually as the convolution layers go deeper. How-\never, encoder of CPTR can utilize long-range dependencies\namong the sequentialized patches from the very beginning via\nself-attention mechanism. During the generation of words,\nCPTR models “words-to-patches” attention in the cross atten-\ntion layer of decoder which is proved to be effective. We eval-\nuate our method on MSCOCO image captioning dataset and\nit outperforms both “CNN+RNN” and “CNN+Transformer”\ncaptioning models.\n2. FRAMEWORK\n2.1. Encoder\nAs depicted in Fig. 1, instead of using a pretrained CNN or\nFaster R-CNN model to extract spatial features or bottom-up\nfeatures like the previous methods, we choose to sequentialize\nthe input image and treat image captioning as a sequence-to-\nsequence prediction task. Concretely, we divide the original\narXiv:2101.10804v3  [cs.CV]  28 Jan 2021\nPosition embedding\n+\n<bos> a puppy rests ... next to a bicycle\nWord Embedding\n+\nPosition embedding\n...\nInput image\nFlatten\n & \nReshape\nDivide into patches\n...\na puppy rests ... next to a bicycle <eos>\nSelf-Attention\nAdd & Layer Norm\n FFN\nAdd & Layer Norm\nNe  ×\nMasked Self-Attention\n FFN\n Cross Attention\nAdd & Layer Norm\nAdd & Layer Norm\nAdd & Layer Norm\n+\nPatch Embedding\n × Nd \nLinear &Softmax\nFig. 1. The overall architecture of proposed CPTR model.\nimage into a sequence of image patches to adapt to the input\nform of Transformer.\nFirstly, we resize the input image into a ﬁxed resolution\nX ∈RH×W×3 (with 3 color channels), then divide the re-\nsized image into N patches, where N = H\nP ×W\nP and P is the\npatch size (P = 16in our experiment settings). After that, we\nﬂatten each patch and reshape them into a 1D patch sequence\nXp ∈RN×(P2·3). We use a linear embedding layer to map\nthe ﬂattened patch sequence to latent space and add a learn-\nable 1D position embedding to the patch features, then we get\nthe ﬁnal input to the Transformer encoder which is denoted as\nPa = [p1,...,p N].\nThe encoder of CPTR consists of Ne stacked identical\nlayers, each of which consists of a multi-head self-attention\n(MHA) sublayer followed by a positional feed-forward sub-\nlayer. MHA contains H parallel heads and each head hi cor-\nresponds to an independent scaled dot-product attention func-\ntion which allows the model to jointly attend to different sub-\nspaces. Then a linear transformation WO is used to aggregate\nthe attention results of different heads, the process can be for-\nmulated as follows:\nMHA(Q,K,V ) = Concat (h1,...,h H) WO (1)\nThe scaled dot-product attention is a particular attention pro-\nposed in Transformer model, which can be computed as fol-\nlows:\nAttention(Q,K,V ) = Softmax\n(QKT\n√dk\n)\nV (2)\nwhere Q ∈RNq×dk , K ∈RNk×dk and V ∈RNk×dv are the\nquery, key and value matrix, respectively.\nThe followed positional feed-forward sublayer is imple-\nmented as two linear layers with GELU activation function\nand dropout between them to further transform features. It\ncan be formulated as:\nFFN(x) = FC2(Dropout(GELU(FC1(x)))) (3)\nIn each sublayer, there exists a sublayer connection com-\nposed of a residual connection, followed by layer normaliza-\ntion.\nxout = LayerNorm(xin + Sublayer(xin))) (4)\nwhere xin, xout are the input and output of one sublayer\nrespectively and the sublayer can be attention layer or feed\nforward layer.\n2.2. Decoder\nIn the decoder side, we add sinusoid positional embedding to\nthe word embedding features and take both the addition re-\nsults and encoder output features as the input. The decoder\nconsists of Nd stacked identical layers with each layer con-\ntaining a masked multi-head self-attention sublayer followed\nby a multi-head cross attention sublayer and a positional feed-\nforward sublayer sequentially.\nThe output feature of the last decoder layer is utilized to\npredict next word via a linear layer whose output dimension\nequals to the vocabulary size. Given a ground truth sentence\ny∗\n1:T and the prediction y∗\nt of captioning model with parame-\nters θ, we minimize the following cross entropy loss:\nLXE(θ) =−\nT∑\nt=1\nlog\n(\npθ\n(\ny∗\nt |y∗\n1:t−1\n))\n(5)\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE CIDEr\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nCNN+RNN\nSCST [9] 78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.0\nLSTM-A [10] 78.7 93.7 62.7 86.7 47.6 76.5 35.6 65.2 27.0 35.4 56.4 70.5 116.0 118.0\nUp-Down [4] 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nRF-Net [11] 80.4 95.0 64.9 89.3 50.1 80.1 38.0 69.2 28.2 37.2 58.2 73.1 122.9 125.1\nGCN-LSTM [12] - - 65.5 89.3 50.8 80.3 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nSGAE [13] 81.0 95.3 65.6 89.5 50.7 80.4 38.5 69.7 28.2 37.2 58.6 73.6 123.8 126.5\nCNN+Transformer\nETA [14] 81.2 95.0 65.5 89.0 50.9 80.4 38.9 70.2 28.6 38.0 58.6 73.9 122.1 124.4\nCPTR 81.8 95.0 66.5 89.4 51.8 80.9 39.5 70.8 29.1 38.3 59.2 74.4 125.4 127.3\nTable 1. Performance comparisons on MSCOCO online test server. All models are ﬁnetuned with self-critical training. c5/c40\ndenotes the ofﬁcial test settings with 5/40 ground-truth captions.\nLike other captioning methods, we also ﬁnetune our model\nusing self-critical training [9].\n3. EXPERIMENTS\n3.1. Dataset and Implementation Details\nWe evaluate our proposed model on MS COCO [15] dataset\nwhich is the most commonly used benchmark for image cap-\ntioning. To be consistent with previous works, we use the\n“Karpathy splits” [16] which contains 113,287, 5,000 and\n5,000 images for training, validation and test, respectively.\nThe results are reported on both the Karpathy test split for\nofﬂine evaluation and MS COCO test server for online evalu-\nation.\nWe train our model in an end-to-end fashion with the en-\ncoder initialized by the pre-trained ViT model. The input im-\nages are resized to 384 ×384 resolution and the patch size\nis setting to 16. The encoder contains 12 layers and decoder\ncontains 4 layers. Feature dimension is 768, and the attention\nhead number is 12 for both encoder and decoder. The whole\nmodel is ﬁrst trained with cross-entropy loss for 9 epochs us-\ning an initial learning rate of 3 ×10−5 and decayed by 0.5 at\nthe last two epochs. After that, we ﬁnetune the model using\nself-critical training [9] for 4 epochs with an initial learning\nrate of 7.5 ×10−6 and decayed by 0.5 after 2 epochs. We use\nAdam optimizer and the batch size is 40. Beam search is used\nand the beam size is 3.\nWe use BLEU-1,2,3,4, METEOR, ROUGE and CIDEr\nscores [17] to evaluate our method which are denoted as B-\n1,2,3,4, M, R and C, respectively.\n3.2. Performance Comparison\nWe compare proposed CPTR to “CNN+RNN” models includ-\ning LSTM [18], SCST [9], LSTM-A [10], RFNet [11], Up-\nMethod B-1 B-2 B-3 B-4 M R C\nCNN+RNN\nLSTM [18] - - - 31.9 25.5 54.3 106.3\nSCST [9] - - - 34.2 26.7 55.7 114.0\nLSTM-A [10] 78.6 - - 35.5 27.3 56.8 118.3\nRFNet [11] 79.1 63.1 48.4 36.5 27.7 57.3 121.9\nUp-Down [4] 79.8 - - 36.3 27.7 56.9 120.1\nGCN-LSTM [12] 80.5 - - 38.2 28.5 58.3 127.6\nLBPF [19] 80.5 - - 38.3 28.5 58.4 127.6\nSGAE [13] 80.8 - - 38.4 28.4 58.6 127.8\nCNN+Transformer\nORT [20] 80.5 - - 38.6 28.7 58.4 128.3\nETA [14] 81.5 - - 39.3 28.8 58.9 126.6\nCPTR 81.7 66.6 52.2 40.0 29.1 59.4 129.4\nTable 2. Performance comparisons on COCO Karpathy test\nsplit. All models are ﬁnetuned with self-critical training.\nDown [4], GCN-LSTM [12], LBPF [19], SGAE [13] and\n“CNN+Transformer” models including ORT [20], ETA [14].\nThese methods mentioned above all use image features ex-\ntract by a CNN or object detector as inputs, while our model\ndirectly takes the raw image as input. Table 2 shows the\nperformance comparison results on the ofﬂine Karpathy test\nsplit, and CPTR achieves 129.4 Cider score which outper-\nforms both “CNN+RNN” and “CNN+Transformer” models.\nWe attribute the superiority of CPTR model over conventional\n“CNN+” architecture to the capacity of modeling global con-\ntext at all encoder layers. The online COCO test server eval-\nuation results shown in Table 1 also demonstrates the effec-\ntiveness of our CPTR model.\n3.3. Ablation Study\nWe conduct ablation studies from the following aspects: (a)\nDifferent pre-trained models to initialize the Transformer en-\nPretrained Model Res #Layer Dim B-4 M R C\nFrom scratch 224 4 768 16.5 17.3 42.1 45.5\nViT21K 224 4 768 33.5 27.4 55.8 110.6\nViT21K+2012 224 4 768 33.8 27.4 55.8 111.6\nViT21K+2012 224 1 768 33.8 27.5 56.1 111.2\nViT21K+2012 224 2 768 33.4 27.5 56.0 110.9\nViT21K+2012 224 6 768 33.7 27.5 55.9 110.9\nViT21K+2012 224 4 512 34.0 27.4 56.0 111.0\nViT21K+2012 384 4 768 34.9 28.2 56.9 116.5\nTable 3. Ablation studies on the cross-entropy training stage.\nRes: image resolution. #Layer: the number of decoder layers.\nDim: the feature dimension of decoder.\ncoder. (b) Different resolutions of input image. (c) The num-\nber of layers and feature dimension in the Transformer de-\ncoder. All experiments are conducted on the Karpathy valida-\ntion set and optimized by cross-entropy loss only.\nThe experiment results are shown in Table 3 from which\nwe can draw the following conclusions. Firstly, pretraining\nvitals for CPTR model. Compared to training from scratch,\nusing parameters of the ViT model pretrained on ImageNet-\n21K dataset to initialize CPTR encoder brings signiﬁcant per-\nformance gains. Besides, using the parameters of the ViT\nmodel ﬁnetuned on the ImageNet 2012 dataset to initialize the\nencoder further brings one point improvement on the CIDEr\nscore. Secondly, CPTR is little sensitive to the decoder hyper-\nparameter including the number of layers and feature dimen-\nsion, among which 4 layers, 768 dimensions shows the best\nperformance (111.6 Cider score). Regards to the input im-\nage resolution, we found that increasing it from 224 ×224\nto 384 ×384 while maintaining the patch size equals to 16\ncan bring huge performance gains (from 111.6 Cider score to\n116.5 Cider score). It is sensible for that the length of patch\nsequence increases from 196 to 576 due to the increasing in-\nput resolution, and can divide image more speciﬁcally and\nprovide more features to interact with each other via the en-\ncoder self-attention layer.\n3.4. Attention Visualization\nIn this section, we take one example image to show the\ncaption predicted by CPTR model and visualize both the self-\nattention weights of the patch sequences in the encoder and\n“words-to-patches” cross attention weights in the decoder.\nWith regards to the encoder self-attention, we choose an im-\nage patch to visualize its attention weights to all patches.\nAs shown in Fig. 2, in the shallow layers, both the local\nand global contexts are exploited by different attention heads\nthanks to the full Transformer design which can not be ful-\nﬁlled by the conventional CNN encoders. In the middle layer,\nmodel tends to pay attention to the primary object, i.e., “teddy\nbear” in the image. The last layer fully utilizes global context\nand pays attention to all objects in the image, i.e., “teddy\nbear”, “chair” and “laptop”.\nBesides, we visualize the “words-to-patches” attention\n1st layer\n12th layer\n6th layer\nFig. 2. Visualization of the predicted encoder self-attention\nweights of different layers and attention heads. The image at\nthe upper left corner is the raw image and the red point on\nit is the chosen query patch. The ﬁrst, second and third row\nare the attention weights visualization of the 1st, 6th, 12th en-\ncoder layer, respectively. The columns show different heads\nin given layers.\nFig. 3. Visualization of the attention weights computed by the\n“words-to-patches” cross attention in the last decoder layer.\n“A teddy bear sitting in a blue chair with a laptop” is the cap-\ntion generated by our model.\nweights in the decoder during the caption generation process.\nAs is shown in Fig. 3, CPTR model can correctly attend to\nappropriate image patches when predicting every word.\n4. CONCLUSIONS\nIn this paper, we rethink image captioning as a sequence-\nto-sequence prediction task and propose CPTR, a full Trans-\nformer model to replace the conventional “CNN+Transformer”\nprocedure. Our network is totally convolution-free and pos-\nsesses the capacity of modeling global context information at\nevery layer of the encoder from the beginning. Evaluation re-\nsults on the popular MS COCO dataset demonstrate the effec-\ntiveness of our method and we surpass “CNN+Transformer”\nnetworks. Detailed visualizations demonstrate that our model\ncan exploit long range dependencies from the beginning and\nthe decoder “words-to-patches” attention can precisely attend\nto the corresponding visual patches to predict words.\n5. REFERENCES\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin, “Attention is all you need,” in Ad-\nvances in neural information processing systems, 2017,\npp. 5998–6008.\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun, “Deep residual learning for image recognition,” in\nProceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[3] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun, “Faster r-cnn: Towards real-time object detection\nwith region proposal networks,” IEEE transactions on\npattern analysis and machine intelligence, vol. 39, no.\n6, pp. 1137–1149, 2016.\n[4] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang,\n“Bottom-up and top-down attention for image caption-\ning and visual question answering,” in Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 6077–6086.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko, “End-to-end object detection with trans-\nformers,” arXiv preprint arXiv:2005.12872, 2020.\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al., “An image is worth\n16x16 words: Transformers for image recognition at\nscale,” arXiv preprint arXiv:2010.11929, 2020.\n[7] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xia-\ntian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jian-\nfeng Feng, Tao Xiang, Philip HS Torr, et al., “Re-\nthinking semantic segmentation from a sequence-to-\nsequence perspective with transformers,”arXiv preprint\narXiv:2012.15840, 2020.\n[8] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu,\nYiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao, “Pre-trained image processing\ntransformer,” arXiv preprint arXiv:2012.00364, 2020.\n[9] Steven J Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel, “Self-critical se-\nquence training for image captioning,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017, pp. 7008–7024.\n[10] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and\nTao Mei, “Boosting image captioning with attributes,”\nin Proceedings of the IEEE International Conference on\nComputer Vision, 2017, pp. 4894–4902.\n[11] Lei Ke, Wenjie Pei, Ruiyu Li, Xiaoyong Shen, and Yu-\nWing Tai, “Reﬂective decoding network for image cap-\ntioning,” in Proceedings of the IEEE International Con-\nference on Computer Vision, 2019, pp. 8888–8897.\n[12] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei, “Ex-\nploring visual relationship for image captioning,” in\nProceedings of the European conference on computer\nvision (ECCV), 2018, pp. 684–699.\n[13] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei\nCai, “Auto-encoding scene graphs for image caption-\ning,” in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2019, pp. 10685–\n10694.\n[14] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang, “Entan-\ngled transformer for image captioning,” in Proceedings\nof the IEEE International Conference on Computer Vi-\nsion, 2019, pp. 8928–8937.\n[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC Lawrence Zitnick, “Microsoft coco: Common objects\nin context,” inEuropean conference on computer vision.\nSpringer, 2014, pp. 740–755.\n[16] Andrej Karpathy and Li Fei-Fei, “Deep visual-semantic\nalignments for generating image descriptions,” in Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 2015, pp. 3128–3137.\n[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Doll ´ar, and\nC Lawrence Zitnick, “Microsoft coco captions: Data\ncollection and evaluation server,” arXiv preprint\narXiv:1504.00325, 2015.\n[18] Oriol Vinyals, Alexander Toshev, Samy Bengio, and\nDumitru Erhan, “Show and tell: A neural image cap-\ntion generator,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2015, pp.\n3156–3164.\n[19] Yu Qin, Jiajun Du, Yonghua Zhang, and Hongtao Lu,\n“Look back and predict forward in image captioning,”\nin Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, 2019, pp. 8367–8375.\n[20] Simao Herdade, Armin Kappeler, Koﬁ Boakye, and\nJoao Soares, “Image captioning: Transforming objects\ninto words,” in Advances in Neural Information Pro-\ncessing Systems, 2019, pp. 11137–11147.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8402866125106812
    },
    {
      "name": "Closed captioning",
      "score": 0.8171387910842896
    },
    {
      "name": "Computer science",
      "score": 0.7379201650619507
    },
    {
      "name": "Encoder",
      "score": 0.5378879904747009
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48356959223747253
    },
    {
      "name": "Architecture",
      "score": 0.46228933334350586
    },
    {
      "name": "Speech recognition",
      "score": 0.42522549629211426
    },
    {
      "name": "Image (mathematics)",
      "score": 0.20654922723770142
    },
    {
      "name": "Engineering",
      "score": 0.13128486275672913
    },
    {
      "name": "Electrical engineering",
      "score": 0.12485232949256897
    },
    {
      "name": "Voltage",
      "score": 0.11690011620521545
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ]
}