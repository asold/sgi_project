{
  "title": "UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization",
  "url": "https://openalex.org/W4386272941",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2109530972",
      "name": "Hongxia Wang",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A3008056042",
      "name": "Mingshan Du",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2117309048",
      "name": "Hanqing Liu",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2104523023",
      "name": "Yang Zhou",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A1914422753",
      "name": "Qiang Zeng",
      "affiliations": [
        "Sichuan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205203579",
    "https://openalex.org/W3173161217",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W4298053201",
    "https://openalex.org/W4319978495",
    "https://openalex.org/W4313127140",
    "https://openalex.org/W4200630755",
    "https://openalex.org/W4312396403",
    "https://openalex.org/W3093077034",
    "https://openalex.org/W3179508259",
    "https://openalex.org/W4298635755",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W4304091726",
    "https://openalex.org/W3139060449",
    "https://openalex.org/W3178572954",
    "https://openalex.org/W4294435344",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W2962958939",
    "https://openalex.org/W3143320354",
    "https://openalex.org/W4298394377",
    "https://openalex.org/W3034196597",
    "https://openalex.org/W4304092233",
    "https://openalex.org/W3137384391",
    "https://openalex.org/W4312655706",
    "https://openalex.org/W3176444885",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2983918066",
    "https://openalex.org/W3201844719",
    "https://openalex.org/W4319336387",
    "https://openalex.org/W4312300737",
    "https://openalex.org/W2963720850",
    "https://openalex.org/W3201143670",
    "https://openalex.org/W4367316141",
    "https://openalex.org/W3094728142",
    "https://openalex.org/W3207845001",
    "https://openalex.org/W2982058372",
    "https://openalex.org/W4304080557",
    "https://openalex.org/W3128626728",
    "https://openalex.org/W3202003978",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W4285019302",
    "https://openalex.org/W2889986507",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W4312891570",
    "https://openalex.org/W4312508181",
    "https://openalex.org/W4304015019",
    "https://openalex.org/W4312598744",
    "https://openalex.org/W3190748826",
    "https://openalex.org/W3174508664",
    "https://openalex.org/W4214691743",
    "https://openalex.org/W3092879151",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4226246616",
    "https://openalex.org/W4287026043",
    "https://openalex.org/W4382318331",
    "https://openalex.org/W4288090950",
    "https://openalex.org/W2502312327",
    "https://openalex.org/W4287758545",
    "https://openalex.org/W4308608407",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4387195417",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3123394884"
  ],
  "abstract": "The emergence of artificial intelligence-generated content (AIGC) has raised\\nconcerns about the authenticity of multimedia content in various fields.\\nHowever, existing research for forgery content detection has focused mainly on\\nbinary classification tasks of complete videos, which has limited applicability\\nin industrial settings. To address this gap, we propose UMMAFormer, a novel\\nuniversal transformer framework for temporal forgery localization (TFL) that\\npredicts forgery segments with multimodal adaptation. Our approach introduces a\\nTemporal Feature Abnormal Attention (TFAA) module based on temporal feature\\nreconstruction to enhance the detection of temporal differences. We also design\\na Parallel Cross-Attention Feature Pyramid Network (PCA-FPN) to optimize the\\nFeature Pyramid Network (FPN) for subtle feature enhancement. To evaluate the\\nproposed method, we contribute a novel Temporal Video Inpainting Localization\\n(TVIL) dataset specifically tailored for video inpainting scenes. Our\\nexperiments show that our approach achieves state-of-the-art performance on\\nbenchmark datasets, including Lav-DF, TVIL, and Psynd, significantly\\noutperforming previous methods. The code and data are available at\\nhttps://github.com/ymhzyj/UMMAFormer/.\\n",
  "full_text": "UMMAFormer: A Universal Multimodal-adaptive Transformer\nFramework for Temporal Forgery Localization\nRui Zhang\nzhangrui1997@stu.scu.edu.cn\nSchool of Cyber Science and\nEngineering, Sichuan University\nChengdu, Sichuan, China\nHongxia Wangâˆ—\nhxwang@scu.edu.cn\nSchool of Cyber Science and\nEngineering, Sichuan University\nChengdu, Sichuan, China\nMingshan Du\n2022226245054@stu.scu.edu.cn\nSchool of Cyber Science and\nEngineering, Sichuan University\nChengdu, Sichuan, China\nHanqing Liu\nliuhanqing0520@stu.scu.edu.cn\nSchool of Cyber Science and\nEngineering, Sichuan University\nChengdu, Sichuan, China\nYang Zhou\nyzhoulv@stu.scu.edu.cn\nSchool of Cyber Science and\nEngineering, Sichuan University\nChengdu, Sichuan, China\nQiang Zeng\nzengqiang@stu.scu.edu.cn\nSchool of Cyber Science and\nEngineering, Sichuan University\nChengdu, Sichuan, China\nABSTRACT\nThe emergence of artificial intelligence-generated content (AIGC)\nhas raised concerns about the authenticity of multimedia con-\ntent in various fields. However, existing research for forgery con-\ntent detection has focused mainly on binary classification tasks\nof complete videos, which has limited applicability in industrial\nsettings. To address this gap, we propose UMMAFormer, a novel\nuniversal transformer framework for temporal forgery localization\n(TFL) that predicts forgery segments with multimodal adaptation.\nOur approach introduces a Temporal Feature Abnormal Atten-\ntion (TFAA) module based on temporal feature reconstruction to\nenhance the detection of temporal differences. We also design a\nParallel Cross-Attention Feature Pyramid Network (PCA-FPN) to\noptimize the Feature Pyramid Network (FPN) for subtle feature\nenhancement. To evaluate the proposed method, we contribute a\nnovel Temporal Video Inpainting Localization (TVIL) dataset specif-\nically tailored for video inpainting scenes. Our experiments show\nthat our approach achieves state-of-the-art performance on bench-\nmark datasets, including Lav-DF, TVIL, and Psynd, significantly\noutperforming previous methods. The code and data are available\nat https://github.com/ymhzyj/UMMAFormer/.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Computer vision; â€¢ Applied\ncomputing â†’Investigation techniques.\nKEYWORDS\ntemporal forgery localization, transformer, multimodal-adaptive\nâˆ—Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0108-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3581783.3613767\nACM Reference Format:\nRui Zhang, Hongxia Wang, Mingshan Du, Hanqing Liu, Yang Zhou, and Qiang\nZeng. 2023. UMMAFormer: A Universal Multimodal-adaptive Transformer\nFramework for Temporal Forgery Localization. In Proceedings of the 31st\nACM International Conference on Multimedia (MM â€™23), October 29-November\n3, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 11 pages. https:\n//doi.org/10.1145/3581783.3613767\n1 INTRODUCTION\nThe rapid development of advanced multimedia editing software\nenabled by artificial intelligence-generated content (AIGC) [ 1, 5,\n22, 30, 32, 43, 54, 56] has raised concerns about its potential misuse,\nsuch as manipulating public opinion and fabricating evidence. This\nhas led to a growing interest in developing methods for detecting\nmanipulated content in multimedia forensics, with a primary focus\non deepfake detection [13, 28, 60, 63, 65] in facial and audio media.\nDespite the promising results demonstrated by these methods in\na variety of benchmarks [14, 15, 17, 27, 46, 66], their mainstream\nadoption by the industry remains limited due to the constraint of\nbinary classification tasks. These methods are inadequate for iden-\ntifying the temporal boundaries of manipulations, which is crucial\nfor practical applications. Further research is necessary to develop\ntechniques that can accurately locate temporal boundaries of ma-\nnipulations in multimedia content and promote the responsible use\nof AIGC for the betterment of society.\nRecent studies [6, 11, 21, 58] have proposed a new task called\ntemporal forgery localization (TFL) to overcome the limitations of\nbinary classification in detecting manipulated content in multime-\ndia. TFL aims to locate the start and end timestamps of manipulated\nsegments, providing a wider range of application scenarios and\nhelping users better understand the results of forgery detection.\nTFL is similar to temporal action localization (TAL) [23, 45, 50] and\nfollows a similar process: pre-processing the video or audio data\nusing a pre-trained feature extractor, enhancing the representation\nof feature vectors with a designed neural network architecture, and\ndecoding the feature vectors using regression and classification\nheads to obtain the start and end times of each action segment and\ntheir corresponding categories. Note that this process may vary\ndepending on specific task requirements.\narXiv:2308.14395v1  [cs.MM]  28 Aug 2023\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Rui Zhang et al.\nInpainting Results\nTime\nInput Masks\nOriginal FramesModified Frames\nFigure 1: We show a collection of keyframes extracted from manipulated videos where a person has been removed, posing a\nserious threat to digital evidence integrity. The original person mask is displayed on the left with the corresponding video\ninpainting results underneath it, while the unmanipulated results are shown on the right with a green background. Of the 144\nframes in the video, only 11 involve person removal, and it only takes minor modifications to a small section of the video to\nachieve this. This manipulation technique has drawn significant attention in forensic video analysis because manipulated\nvideos may be presented as genuine evidence in legal proceedings and can be difficult to detect using classification-based\nmethods.\nTFL tasks present unique challenges compared to TAL. Firstly,\nreal-world scenarios often involve various modalities, including\naudio-only, visual-only, and audio-visual data, requiring separate\nmodels for manipulation detection and potentially delaying TFL\ntechnology development. Secondly, unmodified or real samples are\nessential in TFL, just like background samples, but they are often\nneglected in TAL. Thirdly, manipulation changes are usually more\nsubtle than action changes, with minor alterations like a single\nword or short pronunciation time making detection more challeng-\ning. Finally, the lack of available datasets is a significant bottleneck\nfor TFL. Most multimedia forgery datasets evaluate manipulation\nperformance over the entire video or audio. Only a few studies\nhave validated TFL performance, limited to a single dataset such\nas Lav-DF [6] for the visual domain and Psynd [58] for the audio\ndomain. Besides, available datasets for TFL [6, 21, 58] and deepfake\ndetection [17, 27, 66] primarily focus on facial manipulations and\nspeech forgeries, while AIGC still poses a threat in other scenarios.\nThis narrow scope limits the potential applications for forgery de-\ntection and TFL. For instance, video inpainting [32] techniques can\nremove specific objects from videos, leading to fabricated evidence,\nas shown in Figure 1. Based on the above observations, we propose\nthe following work.\nFor different modalities of multimedia, we propose a novel uni-\nversal multimodal-adaptive transformer framework for TFL called\nUMMAFormer. The framework aims to predict forgery segments\nand their corresponding start and end timestamps in untrimmed\nvideos or audios. Transformer-based models [19, 26, 31, 51] have\ndemonstrated excellent performance in various tasks and can adapt\nto different modality feature inputs. Therefore, we build a universal\nmultimodal adaptive framework based on the transformer block\nthat can be used for TFL tasks involving different modalities of\ndata.\nIn order to fully utilize real samples, we design a Temporal Fea-\nture Abnormal Attention (TFAA) module based on temporal feature\nreconstruction. Our motivation is based on the observation that\ncompared to TAL which relies on spatial content to recognize spe-\ncific types of actions, TFL relies more on temporal features that\nreflect the changes caused by spatial content manipulations. The un-\nderlying difference in feature distribution between manipulated and\nreal segments can be considered a universal feature of multimedia\nmanipulation that exists across any modality of input. By incor-\nporating TFAA, our method enhances the detection of temporal\ndifferences, leading to improved TFL performance across different\ninput feature modalities.\nFor analyzing short video clips with subtle variations, Feature\nPyramid Network (FPN) [ 34] is a commonly used solution that\neffectively enhances subtle features. We further optimize FPN by\nintroducing a parallel structure and proposing a Parallel Cross-\nAttention Feature Pyramid Network (PCA-FPN). PCA-FPN signifi-\ncantly improves the performance of small manipulated segments\nlocalization.\nTo advance research further, it is critical to create a new dataset\nfor a novel scenario and providing new evaluation benchmarks for\nadvancing research in TFL tasks. We introduce a novel temporal\nvideo inpainting localization dataset called TVIL for training and\nevaluation of TFL tasks. As per our knowledge, we are the first\nones to present a TFL dataset that is tailored for video inpainting\nscenes. Our dataset is built on the YouTube-VOS 2018 [55] dataset.\nWe employ XMEM [10] to annotate segmentation masks for all\nframes in the dataset, and then use four different video inpainting\nmodels [32, 37, 57, 61] to erase objects in random time periods. We\nacquire 4453 tampered videos with annotations, divided into train-\ning, validation, and testing sets according to the same proportions\nas the original dataset.\nWe conduct extensive experiments on three benchmark datasets,\nLav-DF [6], Psynd [58], and TVIL to evaluate the effectiveness of\nour proposed method. The results demonstrate that our approach\nachieves state-of-the-art performance on these datasets, outper-\nforming the previous best results by a significant margin.\nIn summary, our contributions are:\nâ€¢We introduce UMMAFormer, a novel universal transformer\nframework for multimedia temporal forgery localization that\ncan be applied to various modalities of input.\nâ€¢We propose a TFAA module that enables the model to focus\non temporal anomalies caused by spatial content tampering.\nUMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nâ€¢We design PCA-FPN, a parallel cross-attention feature pyra-\nmid network, to improve the recognition and localization of\nultrashort forgery segments.\nâ€¢We present TVIL, a novel temporal video inpainting local-\nization dataset, for research on TFL.\n2 RELATED WORK\n2.1 Image-Level Forgery Detection\nDetecting manipulated content, especially deepfake [30, 43], has\nbecome a critical task in multimedia forensics. Significant efforts\nhave been made to enhance image-level face forgery classifica-\ntion [7, 29, 44]. Early studies [41, 46] primarily relied on basic binary\nclassifiers built upon existing backbone networks, suitable only for\ndetecting low-quality generated images. With advancements in\ndeepfake techniques, several approaches have been proposed to\ncapture specific forgery traces. These approaches explore various\nfeatures, including noise features [9, 64], local texture characteris-\ntics [7, 63], and frequency domain anomalies [28, 44], to enhance\ndetection capabilities. Unfortunately, these approaches overlooked\nthe inclusion of temporal-level features, resulting in inconsistencies\nin discriminations for consecutive video frames due to variations in\nlighting, environmental factors, and other disturbances. As a con-\nsequence, they struggle to accurately differentiate genuine videos\nfrom forgeries and fail to identify temporal boundaries of the forg-\neries within the videos.\n2.2 Temporal-Level Forgery Detection\nTemporal-level forgery detection involves the classification of forgery\nat video or audio level and the TFL task, which is the main focus of\nthis paper. The availability of various datasets [15, 27, 66] has signif-\nicantly contributed to the advancement of temporal-level forgery\nclassification methods. Previous research has proposed different\napproaches to address this challenge. Hu et al.[ 25] presented a\ntwo-stream method, utilizing a temporal-level stream to extract\ntemporal correlation features and analyze deepfake videos. Han\net al.[20] introduced a two-stream network that uses temporal in-\nformation and learnable spatial rich model (SRM) filters to detect\nfake videos at the video level. Song et al.[47] utilized a symmetric\ntransformer to enhance discrimination consistency between frames\nfor video-level forgery classification. Additionally, Kwak et al.[28]\ndeveloped a frequency feature masking method to classify real and\nfake audio in noisy environments. However, existing temporal-level\nforgery classification approaches usually treat temporal multime-\ndia content as a cohesive entity, mainly focusing on distinguishing\nbetween real and manipulated content without verifying the au-\nthenticity of specific timestamps. To address this limitation and\nenhance the practical value of deepfake detection, the TFL task was\nintroduced. Some studies [6, 11, 21, 58] have focused on this task,\nbut there is still room for significant improvement.\n2.3 Temporal Action Localization\nThe goal of TAL is to localize the time intervals in a video when\nspecific actions take place. Existing methods [50] typically followed\na general paradigm of feature extraction, feature enhancement,\nand prediction with post-processing. During the feature extrac-\ntion stage, most TAL methods typically utilized pre-trained action\nrecognition networks [16, 24, 53] to extract visual or audio-visual\nfeatures. Given offline features, most algorithms mainly focus on\nenhancing features, by modeling action boundaries attention [8, 33]\nand relationships [3, 40, 59]. Some studies [36, 39, 48] also focused\non proposing new regression and classification heads to further\nenhance the localization performance of the model.\n3 METHODOLOGY\nIn this section, we introduce our universal multimodal-adaptive\ntransformer framework, which aims to localizing temporal forgery\nin sequential multimedia data with various modalities. We have\nconsidered three scenarios, including visual-only, audio-only, or\njoint audio-visual modalities. Of course, the proposed approach\ncan also be further extended to other types of tampered sequential\nmultimedia data.\nSequential Multimedia Datağ‘‹\nAudio-Visual Modality\nVisual-only Modality\nAudio-only Modality\nVisual\nBackbone\nAudio\nBackbone\nConv\nProjection\n(a) Pre-trained Feature Extractor\nF\n(b) Feature Enhancement Module\nSample-level\nClassiferğ‘“ğ‘†\nğ‘“ğ·\nğ‘“ğ¸\nğ‘\nğ‘ğ‘¡\nCRATrans Block\nTFAA\nPCA-FPN\nğ‘ƒ0\nğ‘ƒ1\nğ‘ƒ2\nğ‘ƒ3\nğ‘ƒ4Forgery Localization\nClassification Head\nPost-processing\n(c) Feature Decoder\nFeature Pyramid\nSequenceğ‘ƒ 0~ğ‘ƒ 4\nRegression Head\nTemporal\nğ‘¡1,ğ‘ ğ‘“ ğ‘¡1,ğ‘’ğ‘“\nğ‘ 1,ğ‘“\nğ‘¡2,ğ‘ ğ‘“ ğ‘¡2,ğ‘’ğ‘“\nğ‘ 2,ğ‘“\nğ¹\nğ¹crat\nFigure 2: Illustration of the proposed UMMAFormer, which\nconsists of three main components: (a) a pre-trained feature\nextractor that maps the sequential multimedia data ğ‘‹ to\nsequential features ğ¹, (b) a feature enhancement module that\nenhances the feature representation to multi-scale modified-\nsensitive features, and (c) a feature decoder that decodes the\nfeature to localize forgeries in the data.\n3.1 Overview\nOur objective is to detect forgeries in untrimmed sequential mul-\ntimedia data ğ‘‹ and locate the corresponding segments. Segments\ncan be represented as ğ‘† =\n\b\nğ‘¡ğ‘›,ğ‘ ğ‘“ ,ğ‘¡ğ‘›,ğ‘’ğ‘“ ,ğ‘ ğ‘›,ğ‘“\n\tğ‘ğ‘“\nğ‘›=1, where ğ‘ğ‘“ is the\nnumber of detected modified segments, ğ‘¡ğ‘›,ğ‘ ğ‘“ , ğ‘¡ğ‘›,ğ‘’ğ‘“ , and ğ‘ ğ‘›,ğ‘“ are the\nstart time, the end time, and the confidence score, respectively. To\nachieve this,ğ‘‹ is evenly split intoğ‘‡ segments {ğ‘¥ğ‘¡}ğ‘‡\nğ‘¡=1, and a feature\nsequence ğ¹ âˆˆRğ¶Ã—ğ‘‡ is obtained using TSN [53] and BYOL-A [42]\nas backbone networks for visual and audio data with concatenation\nof their features. Our proposed UMMAFormer framework, shown\nin Figure 2, consists of a pre-trained feature extractor, a feature en-\nhancement module based on a transformer-based network structure\ncomposed of the proposed TFAA module and PCA-FPN, and fea-\nture decoders for localization. We build on the ActionFormer [59]\nframework for our approach, with the feature decoding module\ndirectly utilized. Our proposed structure can also be extended to\nother TFL or TAL networks with similar processes.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Rui Zhang et al.\nPositional\nEncoding\nCRA\nLN\nFFN\nğ¹ğ‘£\nConv1d\nLeakyReLU\nInstanceNorm1d Ã—ğ¿\nÃ—ğ¿\nConv1d\nAvg-Pool\nLinear\nSoftmax\nğ¹\nDCAE\nğ¹ğ‘ ğ¹ğ‘˜\nğ¹ğ‘ğ‘’\nLN Positional\nEncoding\nğ¹\nğ¹ğ‘ğ‘’\nCRA\nFFN\nDCAE\nğ¹crat\nğ‘“ğ¸\nğ¹\nTransposeConv1d\nLeakyReLU\nInstanceNorm1dğ‘“ğ·\nğ‘ğ‘¡\nğ‘“ğ‘†\n(a) DCAE\n(b) CRA\nğ¹ğ‘˜\nâŠº\nğ¹ğ‘\nğ¹ğ‘£\nTFAA\nZ\nğ·Ã—ğ‘‡\nğ·Ã—ğ‘‡\nğ·Ã—ğ‘‡\nğ¹\nğ‘‡Ã—ğ‘‡\nğ¹ğ‘Ÿ\nSoftmax\nğ¹ğ‘’ğ‘\nğ¹ğ‘ğ‘Ÿğ‘\nğ·Ã—ğ‘‡\nFigure 3: Illustration of the proposed TFAA module.\n3.2 Temporal Feature Abnormal Attention\nTo adapt to modified data from different modalities and make full\nuse of real samples, we construct a Temporal Feature Abnormal\nAttention (TFAA) module built from reconstruction learning and\nCross-Reconstruction Attention Transformer (CRATrans) block.\nThe reconstruction learning can be used to determine abnormal\nstates of multi-sensor time-series signals[62]. We believe that tem-\nporal features from different modalities can also be viewed as a type\nof multi-source data. We try to use an encoder-decoder structure\nto learn the distribution of real samples during the training phase.\nDuring the inference phase, we use an attention mechanism to\nfocus on the abnormal segments generated by feature reconstruc-\ntion, which can adapt well to the features extracted from various\nmodality data. The proposed module is shown in Figure 3.\nReconstruction Learning. To be specific, given the encod-\ning feature sequence ğ¹, we first employ a Deep Convolutional\nAutoEncoder (DCAE) as illustrated in Figure 3(a) to learn robust\nrepresentations for real samples. The DCAE consists of a convo-\nlutional encoder ğ‘“ğ¸ and a de-convolutional decoder ğ‘“ğ·. The en-\ncoder is composed of ğ¿convolutional modules. Each convolution\nmodule contain a convolution layer followed by LeakyReLU, and\nInstance Normalization [49]. The low-dimensional representation\nğ‘ âˆˆRğ¶ğ‘§ Ã—ğ‘‡\n2ğ¿ and the reconsturcted features Ë†ğ¹ can be formulated as\nfollows:\n\u001ağ‘ = ğ‘“ğ¸ (ğ¹),\nË†ğ¹ = ğ‘“ğ· (ğ‘). (1)\nThe ğ‘“ğ¸ encodes the input features into low-dimensional through\nconvolution layers with a stride of 2. ğ‘ represents the latent distri-\nbution of real samples. The ğ‘“ğ· decodes the latent low-dimensional\nrepresentation to reconstruct the feature. The decoder is composed\nof transpose convolution layer, activation layer, and normalization\nlayer.\nDuring the training, we compute the distance between input\nfeatures and reconstructed features of unmodified samples in a\nmini-batch as:\nğ¿ğ‘Ÿğ‘’ğ‘ = 1\nğ‘ğ‘Ÿ\nÃğ‘ğ‘Ÿ\nğ‘– âˆ¥Ë†ğ¹ğ‘– âˆ’ğ¹ğ‘–âˆ¥1, (2)\nwhere ğ‘ğ‘Ÿ is the number of unmodified samples in a mini-batch, and\nâˆ¥Â·âˆ¥ 1 is the ğ‘™1-norm. To enhance the consistency of real samples\nin the low-dimensional embedding space, we utilize a sample-level\nclassifier, denoted as ğ‘“ğ‘†, to distinguish the category to which the\ncurrent feature sequence belongs - whether it is real or tampered.\nThe classifier ğ‘“ğ‘† extracts sample-level features from latent features\nğ‘ using average pooling and passes them through two fully con-\nnected layers to obtain the probability scoreğ‘ğ‘¡ for the sample being\ntampered. To address the issue of imbalanced data between real and\ntampered samples, we utilize the focal loss [35] as the loss function\nduring training. The sample-level focal loss is computed as follows:\nğ¿ğ‘ ğ‘ğ‘™ğ‘  = âˆ’ğ›¼(1 âˆ’ğ‘ğ‘¡)ğ›¾ log(ğ‘ğ‘¡), (3)\nwhere ğ›¼is weighting factor to balance positive and negative samples\nand ğ›¾ is the modulating factor to balance easy and hard samples.\nCross-Reconstruction Attention Transformer. Furthermore,\nmany existing anomaly detection algorithms for time series data\nuse reconstruction error to identify abnormal segments. These al-\ngorithms set a threshold and flag any segments with reconstruction\nerror above the threshold as anomalous. However, for our task, we\nneed to consider the difference in information carried by different\ntypes of samples, which can affect the difficulty of reconstruction\nand lead to larger errors in some real samples. Additionally, ma-\nnipulated segments can be very similar to real segments, resulting\nin small differences in reconstruction. Therefore, directly using\nreconstruction error to improve our algorithmâ€™s performance is\ndifficult.\nTo address above problem, we introduce a CRATrans module,\nas shown in Figure 3(b) . As mentioned in [59], transformer block\nwith self-attention module computes a weighted average of features\nby assigning weights proportional to the similarity score between\npairs of input features. In our case, our CRATrans block with Cross-\nReconstruction Attention (CRA) will compute similarity scores\nbetween pairs of original and reconstructed features in order to\nreplace simple reconstruction errors.\nIn detail, given the original featuresğ¹ âˆˆRğ¶Ã—ğ‘‡ and reconstructed\nfeatures Ë†ğ¹ âˆˆRğ¶Ã—ğ‘‡, we add positional encodings [51] at these fea-\ntures to make position-sensitive feature ğ¹ğ‘ğ‘’ and Ë†ğ¹ğ‘ğ‘’. We believe\nthat positional encodings help to enhance the attention to subtle\nchanges in temporal features. Then we transform them into a latent\nspace by using Layer Normalization (LN) [2] and learnable parame-\nter matrices\n\b\nğ‘Šğ‘,ğ‘Šğ‘˜\n\t\nâˆˆRğ·Ã—ğ¶, respectively. The query ğ¹ğ‘ and key\nğ¹ğ‘˜ are calculated by\nğ¹ğ‘ = ğ‘Šğ‘\n\u0000ğ¿ğ‘ \u0000ğ¹ğ‘ğ‘’\n\u0001\u0001 ,ğ¹ğ‘˜ = ğ‘Šğ‘˜\n\u0010\nğ¿ğ‘\n\u0010\nË†ğ¹ğ‘ğ‘’\n\u0011\u0011\n, (4)\nwhere\n\b\nğ¹ğ‘,ğ¹ğ‘˜\n\t\nâˆˆRğ·Ã—ğ‘‡. The original-reconstructed correlation\nmatrix ğ¹ğ‘Ÿ âˆˆRğ‘‡Ã—ğ‘‡is given by\nğ¹ğ‘Ÿ = ğ¹âŠ¤\nğ‘ ğ¹ğ‘˜, (5)\nwhich represents the similarity between the original features and\nthe reconstructed features in the temporal domain. A CRA matrix\nğ¹ğ‘ğ‘Ÿğ‘ is obtained by normalizing the correlation matrixğ¹ğ‘Ÿ, as follows:\nğ¹ğ‘ğ‘Ÿğ‘ = ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥\n\u0012 ğ¹ğ‘Ÿâˆš\nğ¶\n\u0013\n, (6)\nUMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nwhere ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ is performed row-wise, 1âˆš\nğ¶ is used as the scaling\nfactor. This approach can effectively avoid misjudgment or neglect\nof abnormalities between the reconstructed and original features\ndue to factors such as scale. Meanwhile, we project the feature\nğ¹ğ‘ğ‘’ to value ğ¹ğ‘£ âˆˆRğ·Ã—ğ‘‡ by using the LN and learnable parameter\nmatrix ğ‘Šğ‘£:\nğ¹ğ‘£ = ğ‘Šğ‘£\n\u0000ğ¿ğ‘ \u0000ğ¹ğ‘ğ‘’\n\u0001\u0001 . (7)\nIn next step, a dot-product is performed on ğ¹ğ‘ğ‘Ÿğ‘ and the fea-\nture ğ¹ğ‘£ to get the representation ğ¹ğ‘’ğ‘ enhanced by reconstruction\nanomaly attention. We formulate the function as\nğ¹ğ‘’ğ‘ = ğ¹ğ‘ğ‘Ÿğ‘ğ¹ğ‘£, (8)\nwhere ğ¹ğ‘’ğ‘ âˆˆRğ·Ã—ğ‘‡. Furthermore, we actually used a Multi-head\nCross-Reconstruction Attention(MCRA) for our model, where sev-\neral CRA operations are concatenated together in parallel.\nThe output features ğ¹ğ‘’ğ‘ are added to the original featureğ¹ğ‘ğ‘’ and\nare normalized by the LN layer. Finally, We employ a simple fully\nconnected feed-forward network (FFN) with a residual connection\nto product the output ğ¹ğ‘ğ‘Ÿğ‘ğ‘¡ of CRATrans block.\n3.3 Parallel Cross-Attention Feature Pyramid\nNetwork\nHigh-resolution feature maps are crucial for position-sensitive tasks,\nsuch as TFL, which involve numerous short video segments. A\nmulti-scale Transformer encoder was used in [59] to locate action\nsegments in video based on features maps of different resolutions.\nThis encoder utilizes a simple hierarchical multi-scale network, as\nshown in Figure 4(a). However, the limited representation capability\nof high-resolution feature maps for complex content poses a chal-\nlenge. To address this issue, [34] is commonly used to fuse features\nof different scales to improve the networkâ€™s temporal localization\nability. The scheme of FPN is shown in Figure 4(b). Despite its ef-\nfectiveness, the fusion process using a simple form of upsampling\nand downsampling followed by addition usually introduces noise\nto features of different levels, which may interfere with localization.\nThis effect is particularly pronounced for shorter segments, where\neven small localization deviations can cause a sharp change in the\ntemporal Intersection over Union (tIoU) between predicted and\ntrue values. For example, a segment of 0.5 seconds, when shifted\nby 0.1 seconds from its correct position, can result in a 20% de-\ncrease in tIoU, while for 2 seconds, the tIoU will only decrease by\n5%. Inspired by HRNet [52], we propose a Parallel Cross-Attention\nFeature Pyramid Network (PCA-FPN) to enhance high-resolution\nfeatures in such cases. The PCA-FPN is illustrated in Figure 4(c),\nand effectively addresses the problem of noise in feature fusion,\nimproving the localization performance of the network.\nThe PCA-FPN fuses features of different scales simultaneously\nthrough parallel and down-sampling branches, and improves their\ninteraction through a cross-attention (CA) mechanism, Specifically,\ngiven the input feature ğ¹ğ‘ğ‘Ÿğ‘ğ‘¡ from TFAA module we can encode it\nto obtain a high-resolution feature map, denoted asğ‘ƒğ‘–ğ‘›\n0 âˆˆRğ·ğ‘“ ğ‘ğ‘›Ã—ğ‘‡.\nSimilar to other methods, ğ‘ƒğ‘–ğ‘›\n0 is downsampled by an encode mod-\nule with a factor of 2 to obtain a medium-resolution feature map\nğ‘ƒğ‘–ğ‘›\n1 âˆˆRğ·ğ‘“ ğ‘ğ‘›Ã—ğ‘‡\n2 . Following [59], the encoder module is a multi-\nscale transformer unit. To further enhance the representation of\nthe high-resolution feature map ğ‘ƒğ‘–ğ‘›\n0 , we feed these two different\nresolution feature mapsğ‘ƒğ‘–ğ‘›\n0 and ğ‘ƒğ‘–ğ‘›\n1 into the CA module to enhance\ntheir features. The CA module is calculated as follows:\nğ‘ƒğ‘ğ‘™\n1 = ğ¶ğ´\n\u0010\nğ‘Šğ‘ğ‘\n\u0010\nğ¿ğ‘\n\u0010\nğ‘ƒğ‘–ğ‘›\n0\n\u0011\u0011\n,ğ‘Šğ‘ğ‘˜\n\u0010\nğ¿ğ‘\n\u0010\nğ‘ƒğ‘–ğ‘›\n1\n\u0011\u0011\n,ğ‘Šğ‘ğ‘£\n\u0010\nğ¿ğ‘\n\u0010\nğ‘ƒğ‘–ğ‘›\n1\n\u0011\u0011\u0011\n, (9)\nwhere\nğ¶ğ´(ğ‘„,ğ¾,ğ‘‰ )= ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥\n \nğ‘„âŠ¤ğ‘”(ğ¾)âˆšï¸ğ·ğ‘“ğ‘ğ‘›\n!\nğ‘”(ğ‘‰), (10)\n\b\nğ‘Šğ‘ğ‘,ğ‘Šğ‘ğ‘˜,ğ‘Šğ‘ğ‘£\n\t\nâˆˆRğ·ğ‘“ ğ‘ğ‘›Ã—ğ·ğ‘“ ğ‘ğ‘› are learnable parameter matrices,\nğ‘”(Â·)is temporal interpolation function that resamples the ğ¾ and\nğ‘‰, which are the inputs of CA to the same size as ğ‘„, ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ is\nperformed row-wise, 1âˆšğ·ğ‘“ ğ‘ğ‘›\nis used as the scaling factor and ğ‘ƒğ‘ğ‘™\n1 is\nthe first level parallel high-resolution feature. Subsequently, Subse-\nquently, ğ‘ƒğ‘–ğ‘›\n1 used as Query ğ‘„and ğ‘ƒğ‘ğ‘™\n1 used as Key ğ¾and Valueğ‘‰ to\nCA module. The output of the CA module is then passed to the multi-\nscale transformer unit for downsampling to obtain ğ‘ƒğ‘–ğ‘›\n2 . These pro-\ncesses preserve the feature of short segments in the high-resolution\nfeature map while enhancing the representation of features at dif-\nferent scales. By repeating these processes, we can obtain five levels\nof parallel multi-scale features\nn\nğ‘ƒğ‘ğ‘™\n4 ,ğ‘ƒğ‘–ğ‘›\n1 ,ğ‘ƒğ‘–ğ‘›\n2 ,ğ‘ƒğ‘–ğ‘›\n3 ,ğ‘ƒğ‘–ğ‘›\n4\no\n. Finally, we\nfuse the five levels of features from top to bottom, similar to FPN,\nto obtain the enhanced multi-scale features ğ‘ƒ = {ğ‘ƒ0,ğ‘ƒ1,ğ‘ƒ2,ğ‘ƒ3,ğ‘ƒ4}.\nğ‘ƒ0\nğ‘–ğ‘›\nğ‘ƒ1\nğ‘–ğ‘›\nğ‘ƒ2\nğ‘–ğ‘›\nğ‘ƒ3\nğ‘–ğ‘›\nğ‘ƒ4\nğ‘–ğ‘› Conv1d\nConv1d\nConv1d\nConv1d\nConv1d\nğ‘ƒ4\nğ‘ƒ3\nğ‘ƒ2\nğ‘ƒ1\nğ‘ƒ0\nğ¹crat\n(a) Hierarchical Network\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nğ‘ƒ4\nğ‘ƒ3\nğ‘ƒ2\nğ‘ƒ1\nğ‘ƒ0\nğ¹crat\nğ‘ƒ0ğ‘–ğ‘›\nğ‘ƒ1ğ‘–ğ‘›\nğ‘ƒ2\nğ‘–ğ‘›\nğ‘ƒ3\nğ‘–ğ‘›\nğ‘ƒ4\nğ‘–ğ‘›\nğ‘ƒ0\nğ‘šğ‘–ğ‘‘\nğ‘ƒ1\nğ‘šğ‘–ğ‘‘\nğ‘ƒ2\nğ‘šğ‘–ğ‘‘\nğ‘ƒ3\nğ‘šğ‘–ğ‘‘\nğ‘ƒ4\nğ‘šğ‘–ğ‘‘ (b) FPN\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nConv1d\nğ‘ƒ4\nğ‘ƒ3\nğ‘ƒ2\nğ‘ƒ1\nğ‘ƒ0\nğ¹crat\nCross-attention\nCross-attention\nCross-attention\nCross-attention\nCross-attention\nCross-attention\nCross-attention\nğ‘ƒ0\nğ‘–ğ‘›\nğ‘ƒ1ğ‘–ğ‘›\nğ‘ƒ2\nğ‘–ğ‘›\nğ‘ƒ3\nğ‘–ğ‘›\nğ‘ƒ4ğ‘–ğ‘›\nğ‘ƒ1\nğ‘ğ‘™\nğ‘ƒ0\nğ‘šğ‘–ğ‘‘\nğ‘ƒ1ğ‘šğ‘–ğ‘‘\nğ‘ƒ2\nğ‘šğ‘–ğ‘‘\nğ‘ƒ3ğ‘šğ‘–ğ‘‘\nğ‘ƒ4\nğ‘šğ‘–ğ‘‘\nğ‘ƒ2\nğ‘ğ‘™\nğ‘ƒ3\nğ‘ğ‘™\nğ‘ƒ4\nğ‘ğ‘™\nConv1d\n(c) PCA-FPN\nFigure 4: Comparison of feature pyramid networks design\nin the case of 5 levels.\n3.4 Training and Inference\nThe given feature pyramid ğ‘ƒ can be decoded into output ğ‘† =\b\nğ‘¡ğ‘›,ğ‘ ğ‘“ ,ğ‘¡ğ‘›,ğ‘’ğ‘“ ,ğ‘ ğ‘›,ğ‘“\n\tğ‘ğ‘“\nğ‘›=1 through classification and regression heads.\nThe final training loss for the overall model is:\nğ¿= ğ¿ğ‘ğ‘™ğ‘  +ğœ†ğ‘Ÿğ‘’ğ‘”ğ¿ğ‘Ÿğ‘’ğ‘” +ğœ†ğ‘Ÿğ‘’ğ‘ğ¿ğ‘Ÿğ‘’ğ‘ +ğœ†ğ‘ ğ‘ğ‘™ğ‘ ğ¿ğ‘ ğ‘ğ‘™ğ‘ , (11)\nwhere ğ¿ğ‘ğ‘™ğ‘  and ğ¿ğ‘Ÿğ‘’ğ‘” are losses for the classification head outputs\n\b\nğ‘ ğ‘›,ğ‘“\n\tğ‘ğ‘“\nğ‘›=1 and regression head outputs\n\b\nğ‘¡ğ‘›,ğ‘ ğ‘“ ,ğ‘¡ğ‘›,ğ‘’ğ‘“\n\tğ‘ğ‘“\nğ‘›=1, respec-\ntively. ğ¿ğ‘ğ‘™ğ‘  is binary classification loss, where the label of forgery\nsegments is set to 1 and the rest is set to 0. Other settings are di-\nrectly adopted from ActionFormer. The reconstruction loss ğ¿ğ‘Ÿğ‘’ğ‘\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Rui Zhang et al.\nand sample-level focal loss ğ¿ğ‘ ğ‘ğ‘™ğ‘  are mentioned in section 3.2. ğœ†ğ‘Ÿğ‘’ğ‘”,\nğœ†ğ‘Ÿğ‘’ğ‘ and ğœ†ğ‘ ğ‘ğ‘™ğ‘  are hyper-parameters used to balance the relation-\nship between the losses. By default, we set ğœ†ğ‘Ÿğ‘’ğ‘” = 2, ğœ†ğ‘Ÿğ‘’ğ‘ = 1, and\nğœ†ğ‘ ğ‘ğ‘™ğ‘  = 0.1.\nFor the inference stage, we applied Soft-NMS [4] to post-process\nthe results and remove a large number of redundant predictions.\n4 TEMPORAL VIDEO INPAINTING\nLOCALIZATION\nWith the rapid development of AIGC technology, highly deceptive\nvideo and audio content has been widely spread on the Internet,\nleading to potential harm caused by the spread of misleading infor-\nmation. While benchmarks for deepfake videos [15, 27, 46, 66] and\naudios [17] have emerged in recent years to address the forgery of\nfacial or speech content, these methods only cover a small portion\nof all forged content. There is a lack of relevant dataset research\nfor other harmful forgery methods. Therefore, we synthesized a\ndataset for locating video inpainting segments as a new benchmark\nfor TFL, namely TVIL. Our goal is to detect various types of inpaint-\ning forgery in sequential images or videos to defend against the\nspread of misinformation and bring new insights to the research\ncommunity.\nData Collection. The dataset is constructed based on YouTube-\nVOS 2018 [55], which contains over 4,000 online videos from YouTube.\nConsidering that YouTube is currently one of the most popular\nvideo platforms and also an important source for generating and\nspreading misleading information, we believe that generating a\nsynthesized dataset based on YouTube videos can effectively evalu-\nate the performance of TFL algorithms and prevent the spread of\nmisinformation.\nData Processing. YouTube-VOS 2018 is a semi-supervised video\nsemantic segmentation dataset that does not provide complete\nsegmentation masks required for video inpainting. Therefore, we\nutilized XMEM [10], a state-of-the-art video semantic segmentation\nalgorithm, to generate the segmentation masks. These generated\nmasks can be classified into two types: stationary masks and moving\nmasks [57], which are widely used in real-world scenarios. Station-\nary masks can be used for removing static objects, simulating the\nremoval of visible watermarks leading to copyright infringement,\nand so on. On the other hand, moving masks can be used for remov-\ning moving objects, simulating the removal of specific targets such\nas people in surveillance videos. This technology can potentially\nbe used to provide false evidence in certain situations. To better\nsimulate real-world scenarios, we randomly split the dataset into\nfive parts, where one part is used as the real sample set without any\nmanipulation. The remaining four parts are subjected to different\nvideo inpainting methods, namely STTN [ 57], FuseFormer [ 37],\nE2FGVI [32] and FGT [61], which randomly removed some frames\nof the target object. This process aimed to create more diverse\nand challenging samples to test the effectiveness of the proposed\nmethod in handling complex scenarios.\nDataset Statistics. We follow the original split in YouTube-\nVOS 2018, which consisted of 3,471 video clips for training, 474 for\nvalidation and 508 for testing. The average length of video clips is\nabout 140 frames, as shown in Figure 5. The training set consists of\n3340 forgery segments, the validation set consists of 451 forgery\nsegments and the test set consisists of 463 forgery segments. In our\ntask, video clips with a duration of less than 1 second are defined\nas short clips. Compared to the Lav-DF [6] dataset, where 89.26%\nof the manipulated clips are short, our dataset has a proportion of\n99.60%, making our dataset more challenging. The distribution of\nour dataset is illustrated in Figure 6. In Appendix A.1, we provide a\nfurther comparison between the TVIL dataset and other commonly\nused multimedia forensic datasets.\n50 100 150\nFrame Length\n0\n250\n500\n750\n1000\n1250Count\n(a) train set\n50 100 150\nFrame Length\n0\n50\n100\n150\n200Count (b) validation set\n50 100 150\nFrame Length\n0\n50\n100\n150\n200Count (c) test set\nFigure 5: Distribution of video lengths.\nFuseFormer\n27.4%\nSTTN\n23.1%\nFGT\n24.4%\nE2FGVI\n25.1%\n(a) Inpainting Methods\n0\n20.5%\n1\n63.4%\n2\n16.1% (b) No. of Fake Segments\n0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200\nForgery Segments Ratio\n0\n250\n500\n750\n1000\n1250\n1500\n1750\nCount (c) Fake Segments Ratio\nFigure 6: Distribution of the TVIL datasets. (a) The ratio of\ndifferent methods used in modified video segments. (b) The\nproportion of manipulated segments in the dataset. (c) The\nproportion of different manipulated clip lengths to the total\nlength of the video.\n5 EXPERIMENTS\n5.1 Experimental Setup.\nFor visual data, we use the two-stream TSN [ 53] network pre-\ntrained on ActivityNet dataset [23] to extract the two-stream visual\nfeatures. The optical flow is extracted by TV- ğ¿1 algorithm. The\nframe interval is set to 1. For audio data, we employ a pre-trained\nBYOL-A [42] pre-trained on AudioSet [18]. The dimension of the\nextracted video features is 4096, while that of the audio features is\n2048. The extracted features are interpolated to 768 in the temporal\ndimension.\nDatasets and Evaluation Metric. We evaluate our method on\nthree benchmark datasets, including Lav-DF [6] for multi-modal\ndata in face forgery scenarios, our proposed TVIL dataset for vi-\nsual modality in general scenes beyond faces, Psynd [58] for audio\nmodality data in speech scenarios. We follow the evaluation proto-\ncol in [6, 21] and report average precision (AP) and average recall\n(AR) as evaluation metrics, Following conventions, we set the tIoU\nthreshold values at {0.5,0.75,0.95}and set Average Number of pro-\nposals (AN) to {10,20,50,100}. In addition, for dataset Psynd, we\nalso provide tIoU to follow the protocol of dataset Psynd.\nUMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nBaseline and Comparison. We use ActionFormer [59] as our\nbaseline network and reproduced it based on the official code1 with\ndefault settings on our own datasets. We extend the advanced TAL\nnetwork, DCAN [8] and TAGS [39], on the TVIL dataset, represent-\ning research efforts focused on enhancement of boundary features\nand improvement of head location, respectively. Additionally, we\ncompare our algorithm with the state-of-the-art methods on each\ndataset to quantitatively evaluate the performance of our approach.\nImplementation Details. We follow ActionFormer with minor\nmodifications as follows. Our models are trained on a single RTX\n3090 GPU with initial learning rate of 0.001. The batch size for\nLav-DF is 32, for TVIL is 16 and for Psynd is 8.\n5.2 Results for Temporal Face Forgery\nLocalization\nWe report the AP and AR performance of our method and state-\nof-the-art methods on the Lav-DF Full Set in Table 1. For the full\nset, which includes three types of attacks (audio-only modified,\nvideo-only modified, and audio-video modified), most unimodal\nmodels [11, 36, 40] that focus only on visual information struggle\nto accurately locate the tampered segments. Although multimodal\nmodels [3, 6] perform well in terms of AP at tIoU 0.5, they com-\npletely fail for the more challenging AP at tIoU 0.95. The main\nreason for this is the lack of effective feature enhancement for short\nvideo segments. Short segments are extremely sensitive to the tIoU\nmetric. ActionFormer [59] network introduces a simple hierarchical\ntransformer-based network that effectively improves both AP and\nAR. Furthermore, our method further outperforms BA-TFD[6] by\n37.36% in terms of AP at tIoU 0.95 through the proposed PCA-FPN\nand TFAA with mutilmodal features. For visual-only feature as\ninputs, our significantly improves AP at tIoU 0.95 from 0.16% to\n25.68% compare with BA-TFD. In Appendix A.2, we further present\nthe experimental results for the Lav-DF subset. In Appendix A.3, we\nprovide the forgery classification results and additional evaluation\nmetrics for the Lav-DF Full Set.\nTable 1: Performance comparison on Lav-DF Full Set. Bold\nfaces correspond to the top performance.\nMethods Feature Full SetAP@0.5 AP@0.75 AP@0.95 AR@10 AR@20 AR@50 AR@100MDS [11] Visual 12.78 1.62 0.00 37.88 36.71 34.39 32.15AGT [40] Visual 17.85 9.42 0.11 43.15 34.23 24.59 16.71BMN [36] Visual 24.01 7.61 0.07 53.26 41.24 31.60 26.93BMN (I3D) [36] Visual10.56 1.66 0.00 48.49 44.39 37.13 31.55AVFusion [3] Visual+Audio65.38 23.89 0.11 62.98 59.26 54.80 52.11\nBA-TFD [6] Visual 58.55 28.60 0.16 62.49 58.77 53.86 50.29Visual+Audio76.90 38.50 0.25 66.90 64.08 60.77 58.42ActionFormer [59] Visual95.34 90.20 23.73 88.41 89.63 90.33 90.41\nOurs Visual 97.30 92.96 25.68 90.19 90.85 91.14 91.18Visual+Audio98.83 95.54 37.61 92.10 92.42 92.47 92.48\n5.3 Results for Temporal Video Inpainting\nLocalization\nExperimental results in Table 2 show that our method outperforms\nall compared TAL methods on both the AP and AR evaluations\non the proposed TVIL dataset. DCAN [8] is a boundary-enhanced\nalgorithm based on BMN [36] implementation. TAGS [39] is a based\n1https://github.com/happyharrycn/actionformer_release\non a novel localization head that does not include a regression task.\nIt is worth mentioning that due to the increase in in the number of\nshort video clips, the overall performance of ActionFormer is lower\nthan that of Lav-DF. Nevertheless, our method still achieved the\nbest performance, showing the superiority of our method.\nTable 2: Comparison between our method and other state-of-\nthe-art TAL methods on TVIL. Bold faces correspond to the\ntop performance.\nMethods AP@0.5 AP@0.75 AP@0.95 AR@10 AR@20 AR@50 AR@100\nTAGS [39] 18.40 12.68 0.09 24.41 25.05 25.56 25.56\nDCAN [8] 82.75 75.00 3.22 64.73 66.02 68.82 69.97\nActionFormer [59] 86.27 83.03 28.17 84.82 85.77 88.10 88.49\nOurs 88.68 84.70 62.43 87.09 88.21 90.43 91.16\n5.4 Results for Partial Synthetic Speech\nLocalization\nWe further evaluate UMMAFormer on partial synthetic speech local-\nization task to illustrate its superiority on different modal adaption.\nFollowing LFSS [58], we report tIoU on Psynd dataset. LFSS is the\nonly work so far focused on localizing voice cloning partially faked\nEnglish speech. Due to the absence of completely real samples in the\noriginal training set of Psynd, we randomly extracted 299 unaltered\naudio segments from the original dataset as the real training sam-\nples for our proposed method. To ensure fairness, we also selected\nthe best tIoU at different thresholds as the final result, as LFSS did.\nAs shown in Table 3, our algorithm achieves better performance\nunder most conditions, especially in the landline and cellular test\nsubsets. This means that our algorithm has good robustness even\nwhen the audio is further disturbed. It is worth mentioning that the\ntest special subset contains both completely fake and completely\nreal samples. Under this condition, LFSS based on simple binary\nclassification achieves good results and outperforms our method by\n1.11%. However, for other more challenging scenarios, such as local\npartial modified segments, especially under conditions where data\nare disturbed, our proposed method performs better. This further\ndemonstrates the value of research on TFL tasks.\nTable 3: Performance comparison on Psynd in terms of tIoU.\nBold faces correspond to the top performance.\nMethods test set special test set landline cellular\nLFSS [58] 98.58 99.35 80.29 80.94\nOurs 98.70 98.24 92.04 86.57\n5.5 Ablation Studies\nWe compare the contributions of different components of our method\nfor different modalities. Table 4 shows the comparison between the\nperformance of our proposed PCA-FPN and FPN. The experiments\nare conducted under three scenarios: visual-audio (Lav-DF Full\nSet), visual-only (TVIL), and audio-only (Psynd-Test). The baseline\nrefers to ActionFormer. We observed that FPN actually reduces\nmodel performance in audio tasks because it introduces noise dur-\ning the multi-scale fusion process. On the other hand, our proposed\nPCA-FPN greatly improves the localization accuracy of the model.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Rui Zhang et al.\nWe also find that PCA-FPN can be applied to localization tasks in\ndifferent modalities.\nTable 4: Comprasion with FPN. Bold faces correspond to the\ntop performance of each dataset.\nDataset Methods AP@0.5 AP@0.75 AP@0.95 AR@10 AR@20 AR@50 AR@100\nLav-DF Full SetBaseline 97.58 93.75 40.38 92.23 92.71 92.87 92.90Baseline+FPN98.84 95.6138.63 92.30 92.5992.65 92.66Baseline+PCA-FPN 98.72 95.5239.00 92.31 92.60 92.65 92.66\nTVIL Baseline 86.10 82.86 28.11 84.68 85.71 88.04 88.43Baseline+FPN 88.50 84.35 38.9585.9187.2689.63 90.09Baseline+PCA-FPN88.57 84.82 40.3785.5687.4489.53 89.78\nPsynd-TestBaseline100.00 100.0071.08 95.95 95.95 95.95 95.95Baseline+FPN 43.28 5.13 0.11 47.22 48.48 48.86 48.86Baseline+PCA-FPN100.0098.54 77.72 97.34 97.34 97.34 97.34\nTable 5 provides further evidence of the value of TFAA, which\nwas evaluated in the same three scenarios as mentioned earlier.\nThe results show that TFAA effectively improved the modelâ€™s per-\nformance in most scenarios, suggesting that it enhances the ap-\nplicability of different modality features. Additionally, our model\ndemonstrates the capability of universal modality-adaptation.\nTable 5: Ablation studies of the proposed TFAA modules. Bold\nfaces correspond to the top performance of each dataset.\nDataset Methods AP@0.5 AP@0.75 AP@0.95 AR@10 AR@20 AR@50 AR@100\nLav-DF Full SetBaseline 97.58 93.75 40.38 92.23 92.71 92.87 92.90Baseline+TFAA 97.57 93.7440.53 92.31 92.80 92.98 92.99Baseline+PCA-FPN+TFAA (ours)98.83 95.5437.61 92.10 92.42 92.47 92.48\nTVIL Baseline 86.10 82.86 28.11 84.68 85.71 88.04 88.43Baseline+TFAA 85.82 83.23 51.71 86.32 87.48 89.31 89.55Baseline+PCA-FPN+TFAA (ours)88.68 84.70 62.43 87.09 88.21 90.43 91.16\nPsynd-Test Baseline 100.00 100.0071.08 95.95 95.95 95.95 95.95Baseline+TFAA100.0098.41 76.23 97.09 97.09 97.09 97.09Baseline+PCA-FPN+TFAA (ours)100.00 100.00 79.87 97.60 97.60 97.60 97.60\nFigure 7 demonstrates the impact of different modules on tIoU.\nWe conducted tests on Psynd, varying the confidence scores from\n0.05 to 0.95, and compared the resulting tIoU values as well as their\naverages. A higher average value indicates a higher effectiveness\nof our predicted candidates. Both TFAA and PCA-FPN effectively\nimprove the modelâ€™s performance, and combining them results in\neven better performance.\nFigure 8 provides visual representations of two qualitative ex-\namples from Lav-DF and TVIL. As shown in Figure 8(a), the base-\nline method can locate the corresponding forged segments, but it\nexhibits a larger offset compared to our method. As depicted in\nFigure 8(b), due to the difficulty in locating forged segments that\nbelong to short segments, the baseline method failed to identify\nthem, whereas our method achieved significant detection results.\n6 CONCLUSIONS\nIn this paper, we propose a noval universal multimodal-adaptive\ntransformer framework for TFL, which fosters deeper investigations\nin multimedia content security and helps prevent the misuse of\nAIGC. To solve the challenges in the task, we propose a novel\nTFAA module and PCA-FPN to enhance the feature from sequential\nmultimedia data. We also provide a new dataset called TVIL for\nTFL in a novel scenario which has been released for academic use.\nThe experimental results show the effectiveness of the proposed\nframework. Especially concerning the LAV-DF dataset, compared to\nthe previous state-of-the-art method BA-TFD [6], our approach has\nshown significant performance improvements. Specifically, the AP\nhas increased from 76.90% to 98.83% at tIOU 0.5, and from 0.25% to\n0.2 0.4 0.6 0.8\nScore Thresholds\n70\n75\n80\n85\n90\n95\n100tIoU\nModels\nBaseline(86.68)\nBaseline+TFAA(93.78)\nBaseline+PCA-FPN(97.13)\nBaseline+PCA-FPN+TFAA(ours)(97.45)\n(a) test set\n0.2 0.4 0.6 0.8\nScore Thresholds\n50\n60\n70\n80\n90\n100tIoU\nModels\nBaseline(79.30)\nBaseline+TFAA(73.79)\nBaseline+PCA-FPN(83.02)\nBaseline+PCA-FPN+TFAA(ours)(84.08) (b) special test set\n0.2 0.4 0.6 0.8\nScore Thresholds\n60\n65\n70\n75\n80\n85\n90tIoU\nModels\nBaseline(80.58)\nBaseline+TFAA(83.92)\nBaseline+PCA-FPN(88.61)\nBaseline+PCA-FPN+TFAA(ours)(88.77)\n(c) landline\n0.2 0.4 0.6 0.8\nScore Thresholds\n55\n60\n65\n70\n75\n80\n85tIoU\nModels\nBaseline(75.65)\nBaseline+TFAA(77.55)\nBaseline+PCA-FPN(83.43)\nBaseline+PCA-FPN+TFAA(ours)(83.61) (d) cellular\nFigure 7: Ablation studies with respect to effect of score\nthresholds on tIoU. Score threshold values varies from 0.05\nto 0.95 with a step size of 0.05. We calculate the average tIoU\nover different thresholds.\nGT\nTFAA\nPCA-FPN\nBaseline\nOurs\n(a) Lav-DF\nGT\nTFAA\nPCA-FPN\nBaseline\nOurs\n(b) TVIL\nFigure 8: Qualitative examples of our proposed model ab-\nlation experiments. Red indicates fake segments and green\nindicates real segments.\n37.61% at tIOU 0.95. In the future, we will conduct further research\nto spatial localization on top of temporal localization to enhance\nthe practicality of the model.\nACKNOWLEDGMENTS\nThis work is supported by the National Natural Science Foundation\nof China (NSFC) under Grants 62272331 and 61972269, Sichuan\nScience and Technology Program under Grant 2022YFG0320.\nUMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nREFERENCES\n[1] Nuha Aldausari, Arcot Sowmya, Nadine Marcus, and Gelareh Mohammadi. 2023.\nVideo Generative Adversarial Networks: A Review. ACM Comput. Surv. 55, 2\n(2023), 30:1â€“30:25. https://doi.org/10.1145/3487891\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-\ntion. arXiv:1607.06450\n[3] Anurag Bagchi., Jazib Mahmood., Dolton Fernandes., and Ravi Kiran Sarvadevab-\nhatla. 2022. Hear Me out: Fusional Approaches for Audio Augmented Temporal\nAction Localization. In Proceedings of the 17th International Joint Conference\non Computer Vision, Imaging and Computer Graphics Theory and Applications .\nSciTePress, 144â€“154. https://doi.org/10.5220/0010832700003124\n[4] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S. Davis. 2017. Soft-\nNMS - Improving Object Detection with One Line of Code. In Proceedings of\nthe IEEE International Conference on Computer Vision . IEEE, 5562â€“5570. https:\n//doi.org/10.1109/ICCV.2017.593\n[5] Jiayin Cai, Changlin Li, Xin Tao, Chun Yuan, and Yu-Wing Tai. 2022. DeViT:\nDeformed Vision Transformers in Video Inpainting. In Proceedings of the 30th\nACM International Conference on Multimedia . ACM, 779â€“789. https://doi.org/10.\n1145/3503161.3548395\n[6] Zhixi Cai, Kalin Stefanov, Abhinav Dhall, and Munawar Hayat. 2022. Do You Re-\nally Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal\nMethod for Temporal Forgery Localization. In 2022 International Conference\non Digital Image Computing: Techniques and Applications (DICTA) . IEEE, 1â€“10.\nhttps://doi.org/10.1109/DICTA56598.2022.10034605\n[7] Junyi Cao, Chao Ma, Taiping Yao, Shen Chen, Shouhong Ding, and Xiaokang\nYang. 2022. End-to-End Reconstruction-Classification Learning for Face Forgery\nDetection. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. IEEE, 4103â€“4112. https://doi.org/10.1109/CVPR52688.2022.00408\n[8] Guo Chen, Yin-Dong Zheng, Limin Wang, and Tong Lu. 2022. DCAN: Improving\nTemporal Action Detection via Dual Context Aggregation. In Proceedings of the\nAAAI Conference on Artificial Intelligence . AAAI Press, 248â€“257.\n[9] Guo Chen, Yin-Dong Zheng, Limin Wang, and Tong Lu. 2023. Noise Based\nDeepfake Detection via Multi-Head Relative-Interaction. In Proceedings of the\nAAAI Conference on Artificial Intelligence . AAAI Press, 14548â€“14556.\n[10] Ho Kei Cheng and Alexander G. Schwing. 2022. XMem: Long-Term Video Object\nSegmentation with an Atkinson-Shiffrin Memory Model. In European Conference\non Computer Vision . Springer, 640â€“658. https://doi.org/10.1007/978-3-031-19815-\n1_37\n[11] Komal Chugh, Parul Gupta, Abhinav Dhall, and Ramanathan Subramanian. 2020.\nNot made for each other- Audio-Visual Dissonance-based Deepfake Detection\nand Localization. In Proceedings of the 28th ACM international conference on\nMultimedia. ACM, 439â€“447. https://doi.org/10.1145/3394171.3413700\n[12] Davide Coccomini, Nicola Messina, Claudio Gennaro, and Fabrizio Falchi. 2022.\nCombining EfficientNet and Vision Transformers for Video Deepfake Detec-\ntion. In Proceedings of International Conference on Image Analysis and Processing .\nSpringer, 219â€“229. https://doi.org/10.1007/978-3-031-06433-3_19\n[13] Jiacheng Deng, Terui Mao, Diqun Yan, Li Dong, and Mingyu Dong. 2022. Detec-\ntion of Synthetic Speech Based on Spectrum Defects. In Proceedings of the 1st\nInternational Workshop on Deepfake Detection for Audio Multimedia . ACM, 3â€“8.\nhttps://doi.org/10.1145/3552466.3556529\n[14] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin\nWang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge\n(DFDC) Dataset. arXiv:2006.07397\n[15] Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, and Cristian Can-\nton Ferrer. 2019. The Deepfake Detection Challenge (DFDC) Preview Dataset.\narXiv:1910.08854\n[16] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow-\nFast Networks for Video Recognition. In IEEE International Conference on Com-\nputer Vision . IEEE, 6201â€“6210. https://doi.org/10.1109/ICCV.2019.00630\n[17] Joel Frank and Lea SchÃ¶nherr. 2021. WaveFake: A Data Set to Facilitate Audio\nDeepfake Detection. In Proceedings of the Neural Information Processing Systems\nTrack on Datasets and Benchmarks , Vol. 1. Curran Associates, Inc.\n[18] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade\nLawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio\nSet: An ontology and human-labeled dataset for audio events. In IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing . IEEE, 776â€“780.\nhttps://doi.org/10.1109/ICASSP.2017.7952261\n[19] Jiwei Guo, Jiajia Tang, Weichen Dai, Yu Ding, and Wanzeng Kong. 2022. Dynam-\nically Adjust Word Representations Using Unaligned Multimodal Information.\nIn Proceedings of the 30th ACM international conference on Multimedia . ACM,\n3394â€“3402. https://doi.org/10.1145/3503161.3548137\n[20] Bing Han, Xiaoguang Han, Hua Zhang, Jingzhi Li, and Xiaochun Cao. 2021.\nFighting Fake News: Two Stream Network for Deepfake Detection via Learnable\nSRM. IEEE Trans. Biom. Behav. Identity Sci. 3, 3 (2021), 320â€“331. https://doi.org/\n10.1109/TBIOM.2021.3065735\n[21] Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu\nSheng, Jing Shao, and Ziwei Liu. 2021. ForgeryNet: A Versatile Benchmark\nfor Comprehensive Forgery Analysis. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . IEEE, 4360â€“4369. https://doi.org/10.\n1109/CVPR46437.2021.00434\n[22] Sindhu B. Hegde, K. R. Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri,\nand C. V. Jawahar. 2022. Lip-to-Speech Synthesis for Arbitrary Speakers in the\nWild. In Proceedings of the 30th ACM International Conference on Multimedia .\nACM, 6250â€“6258. https://doi.org/10.1145/3503161.3548081\n[23] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles.\n2015. ActivityNet: A large-scale video benchmark for human activity under-\nstanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. IEEE, 961â€“970. https://doi.org/10.1109/CVPR.2015.7298698\n[24] Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke, Aren\nJansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan\nSeybold, Malcolm Slaney, Ron J. Weiss, and Kevin W. Wilson. 2017. CNN archi-\ntectures for large-scale audio classification. In IEEE International Conference on\nAcoustics, Speech and Signal Processing . IEEE, 131â€“135. https://doi.org/10.1109/\nICASSP.2017.7952132\n[25] Juan Hu, Xin Liao, Wei Wang, and Zheng Qin. 2022. Detecting Compressed Deep-\nfake Videos in Social Networks Using Frame-Temporality Two-Stream Convolu-\ntional Network. IEEE Trans. Circuits Syst. Video Technol. 32, 3 (2022), 1089â€“1102.\nhttps://doi.org/10.1109/CVPRW.2017.229\n[26] Ronghang Hu and Amanpreet Singh. 2021. UniT: Multimodal Multitask Learning\nwith a Unified Transformer. InProceedings of the IEEE International Conference on\nComputer Vision. IEEE, 1419â€“1429. https://doi.org/10.1109/ICCV48922.2021.00147\n[27] Hasam Khalid, Shahroz Tariq, Minha Kim, and Simon Woo. 2021. FakeAVCeleb:\nA Novel Audio-Video Multimodal Deepfake Dataset. In Proceedings of the Neural\nInformation Processing Systems Track on Datasets and Benchmarks , Vol. 1. Curran\nAssociates, Inc.\n[28] Il-Youp Kwak, Sunmook Choi, Jonghoon Yang, Yerin Lee, Soyul Han, and Se-\nungsang Oh. 2022. Low-quality Fake Audio Detection through Frequency Feature\nMasking. In Proceedings of the 1st International Workshop on Deepfake Detection\nfor Audio Multimedia . ACM, 9â€“17. https://doi.org/10.1145/3552466.3556533\n[29] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and\nBaining Guo. 2020. Face X-Ray for More General Face Forgery Detection. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition .\nIEEE, 5000â€“5009. https://doi.org/10.1109/CVPR42600.2020.00505\n[30] Yudong Li, Xianxu Hou, Zhe Zhao, Linlin Shen, Xuefeng Yang, and Kimmo\nYan. 2022. Talk2Face: A Unified Sequence-based Framework for Diverse Face\nGeneration and Analysis Tasks. In Proceedings of the 30th ACM International\nConference on Multimedia . ACM, 4594â€“4604. https://doi.org/10.1145/3503161.\n3548205\n[31] Zekang Li, Zongjia Li, Jinchao Zhang, Yang Feng, and Jie Zhou. 2021. Bridging\nText and Video: A Universal Multimodal Transformer for Audio-Visual Scene-\nAware Dialog. IEEE ACM Trans. Audio Speech Lang. Process. 29 (2021), 2476â€“2483.\nhttps://doi.org/10.1109/TASLP.2021.3065823\n[32] Zhen Li, Chengze Lu, Jianhua Qin, Chun-Le Guo, and Ming-Ming Cheng. 2022.\nTowards An End-to-End Framework for Flow-Guided Video Inpainting. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition . IEEE,\n17541â€“17550. https://doi.org/10.1109/CVPR52688.2022.01704\n[33] Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang, Ying Tai, Chengjie\nWang, Jilin Li, Feiyue Huang, and Yanwei Fu. 2021. Learning Salient Boundary\nFeature for Anchor-free Temporal Action Localization. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition . IEEE, 3320â€“3329.\nhttps://doi.org/10.1109/CVPR46437.2021.00333\n[34] Tsung-Yi Lin, Piotr DollÃ¡r, Ross B. Girshick, Kaiming He, Bharath Hariharan,\nand Serge J. Belongie. 2017. Feature Pyramid Networks for Object Detection. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition .\nIEEE, 936â€“944. https://doi.org/10.1109/CVPR.2017.106\n[35] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr DollÃ¡r. 2017.\nFocal Loss for Dense Object Detection. In Proceedings of the IEEE International\nConference on Computer Vision . IEEE, 2999â€“3007. https://doi.org/10.1109/ICCV.\n2017.324\n[36] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. 2019. BMN: Boundary-\nMatching Network for Temporal Action Proposal Generation. In Proceedings of\nthe IEEE International Conference on Computer Vision . IEEE, 3888â€“3897. https:\n//doi.org/10.1109/ICCV.2019.00399\n[37] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun,\nXiaogang Wang, Jifeng Dai, and Hongsheng Li. 2021. FuseFormer: Fusing Fine-\nGrained Information in Transformers for Video Inpainting. In Proceedings of the\nIEEE International Conference on Computer Vision . IEEE, 14020â€“14029. https:\n//doi.org/10.1109/ICCV48922.2021.01378\n[38] Trisha Mittal, Ritwik Sinha, Viswanathan Swaminathan, John P. Collomosse,\nand Dinesh Manocha. 2023. Video Manipulations Beyond Faces: A Dataset with\nHuman-Machine Analysis. InIEEE/CVF Winter Conference on Applications of Com-\nputer Vision Workshops . IEEE, 643â€“652. https://doi.org/10.1109/WACVW58289.\n2023.00071\n[39] Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, and Tao Xiang. 2022. Proposal-Free\nTemporal Action Detection via Global Segmentation Mask Learning. InEuropean\nConference on Computer Vision . Springer, 645â€“662. https://doi.org/10.1007/978-\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Rui Zhang et al.\n3-031-20062-5_37\n[40] Megha Nawhal and Greg Mori. 2021. Activity Graph Transformer for Temporal\nAction Localization. arXiv:2101.08540\n[41] Huy H. Nguyen, Junichi Yamagishi, and Isao Echizen. 2019. Capsule-forensics:\nUsing Capsule Networks to Detect Forged Images and Videos. In IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing . IEEE, 2307â€“2311.\nhttps://doi.org/10.1109/ICASSP.2019.8682602\n[42] Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio\nKashino. 2021. BYOL for Audio: Self-Supervised Learning for General-Purpose\nAudio Representation. In International Joint Conference on Neural Networks . IEEE,\n1â€“8. https://doi.org/10.1109/IJCNN52387.2021.9534474\n[43] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda,\nChris UmÃ©, Mr. Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, Sheng Zhang,\nPingyu Wu, Bo Zhou, and Weiming Zhang. 2021. DeepFaceLab: Integrated,\nflexible and extensible face-swapping framework. https://doi.org/10.1016/j.\npatcog.2023.109628 arXiv:2005.05535\n[44] Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing Shao. 2020. Thinking\nin Frequency: Face Forgery Detection by Mining Frequency-Aware Clues. In\nEuropean Conference on Computer Vision . Springer, 86â€“103. https://doi.org/10.\n1007/978-3-030-58610-2_6\n[45] Yifan Ren, Xing Xu, Fumin Shen, Yazhou Yao, and Huimin Lu. 2021. CAA:\nCandidate-Aware Aggregation for Temporal Action Detection. InProceedings of\nthe 29th ACM international conference on Multimedia . ACM, 4930â€“4938. https:\n//doi.org/10.1145/3474085.3475616\n[46] Andreas RÃ¶ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies,\nand Matthias NieÃŸner. 2019. FaceForensics++: Learning to Detect Manipulated\nFacial Images. In IEEE International Conference on Computer Vision . IEEE, 1â€“11.\nhttps://doi.org/10.1109/ICCV.2019.00009\n[47] Luchuan Song, Xiaodan Li, Zheng Fang, Zhenchao Jin, Yuefeng Chen, and Chen-\nliang Xu. 2022. Face Forgery Detection via Symmetric Transformer. In Proceed-\nings of the 30th ACM International Conference on Multimedia . ACM, 4102â€“4111.\nhttps://doi.org/10.1145/3503161.3547806\n[48] Jing Tan, Jiaqi Tang, Limin Wang, and Gangshan Wu. 2021. Relaxed Transformer\nDecoders for Direct Action Proposal Generation. InIEEE International Conference\non Computer Vision . IEEE, 13506â€“13515. https://doi.org/10.1109/ICCV48922.2021.\n01327\n[49] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2017. Instance Normal-\nization: The Missing Ingredient for Fast Stylization. arXiv:1607.08022\n[50] Elahe Vahdani and Yingli Tian. 2023. Deep Learning-Based Action Detection in\nUntrimmed Videos: A Survey. IEEE Trans. Pattern Anal. Mach. Intell. 45, 4 (2023),\n4302â€“4320. https://doi.org/10.1109/TPAMI.2022.3193611\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems , Vol. 30. Curran\nAssociates, Inc., 5998â€“6008.\n[52] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao,\nDong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao.\n2021. Deep High-Resolution Representation Learning for Visual Recognition.\nIEEE Trans. Pattern Anal. Mach. Intell. 43, 10 (2021), 3349â€“3364. https://doi.org/\n10.1109/TPAMI.2020.2983686\n[53] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and\nLuc Van Gool. 2016. Temporal Segment Networks: Towards Good Practices for\nDeep Action Recognition. In European Conference on Computer Vision . Springer,\n20â€“36. https://doi.org/10.1007/978-3-319-46484-8_2\n[54] Yongqi Wang and Zhou Zhao. 2022. FastLTS: Non-Autoregressive End-to-End\nUnconstrained Lip-to-Speech Synthesis. In Proceedings of the 30th ACM Inter-\nnational Conference on Multimedia . ACM, 5678â€“5687. https://doi.org/10.1145/\n3503161.3548194\n[55] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang,\nBrian L. Price, Scott Cohen, and Thomas S. Huang. 2018. YouTube-VOS: Sequence-\nto-Sequence Video Object Segmentation. In European Conference on Computer\nVision. Springer, 603â€“619. https://doi.org/10.1007/978-3-030-01228-1_36\n[56] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao,\nWentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion Models: A\nComprehensive Survey of Methods and Applications. arXiv:2209.00796\n[57] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. 2020. Learning Joint Spatial-\nTemporal Transformations for Video Inpainting. In European Conference on Com-\nputer Vision . Springer, 528â€“543. https://doi.org/10.1007/978-3-030-58517-4_31\n[58] Bowen Zhang and Terence Sim. 2022. Localizing Fake Segments in Speech. In\n26th International Conference on Pattern Recognition . IEEE, 3224â€“3230. https:\n//doi.org/10.1109/ICPR56361.2022.9956134\n[59] Chen-Lin Zhang, Jianxin Wu, and Yin Li. 2022. ActionFormer: Localizing Mo-\nments of Actions with Transformers. In European Conference on Computer Vision .\nSpringer, 492â€“510. https://doi.org/10.1007/978-3-031-19772-7_29\n[60] Daichi Zhang, Fanzhao Lin, Yingying Hua, Pengju Wang, Dan Zeng, and Shiming\nGe. 2022. Deepfake Video Detection with Spatiotemporal Dropout Transformer.\nIn Proceedings of the 30th ACM International Conference on Multimedia . ACM,\n5833â€“5841. https://doi.org/10.1145/3503161.3547913\n[61] Kaidong Zhang, Jingjing Fu, and Dong Liu. 2022. Flow-Guided Transformer for\nVideo Inpainting. In European Conference on Computer Vision . Springer, 74â€“90.\nhttps://doi.org/10.1007/978-3-031-19797-0_5\n[62] Yuxin Zhang, Yiqiang Chen, Jindong Wang, and Zhiwen Pan. 2023. Unsupervised\nDeep Anomaly Detection for Multi-Sensor Time-Series Signals.IEEE Trans. Knowl.\nData Eng. 35, 2 (2023), 2118â€“2132. https://doi.org/10.1109/TKDE.2021.3102110\n[63] Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Tianyi Wei, Weiming Zhang, and\nNenghai Yu. 2021. Multi-Attentional Deepfake Detection. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition . IEEE, 2185â€“2194.\nhttps://doi.org/10.1109/CVPR46437.2021.00222\n[64] Peng Zhou, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2017. Two-Stream\nNeural Networks for Tampered Face Detection. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition Workshops . IEEE, 1831â€“1839.\nhttps://doi.org/10.1109/CVPRW.2017.229\n[65] Yipin Zhou and Ser-Nam Lim. 2021. Joint Audio-Visual Deepfake Detection. In\nIEEE International Conference on Computer Vision . IEEE, 14780â€“14789. https:\n//doi.org/10.1109/ICCV48922.2021.01453\n[66] Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. 2020.\nWildDeepfake: A Challenging Real-World Dataset for Deepfake Detection. In\nProceedings of the 28th ACM International Conference on Multimedia . ACM, 2382â€“\n2390. https://doi.org/10.1145/3394171.3413769\nUMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nA APPENDIX\nA.1 Comparison between Existing Audio and\nVideo Forensics Datasets\nWe present a comprehensive analysis of the recently popular datasets\nfor audio and video forensics. Table 6 summarizes the benchmark\ndatasets that have been used for research on detecting genera-\ntive content of audio and video, particularly in deepfake detection.\nWhile most of the existing datasets [66] focused on simple binary\nclassification tasks related to facial image manipulation, recent\nadvancements in deepfake detection technology resulted in the\nemergence of binary classification tasks for audio [17] and multi-\nmodal audio-visual data [27]. These datasets involve not only facial\nbut also audio information, and their emergence is indicative of the\nrapid development of deepfake detection technology. Lav-DF [6]\nand Psynd [58], two other emerging datasets for TFL, are manipu-\nlated based on semantic content, making the attacks on these types\nof videos more similar to real-world scenarios. However, research\non these datasets has remained limited to scenes related to human\nfaces and speeches, which is only a small part of the AIGC task.\nFurthermore, while there are emerging classification datasets [38]\nfor tampering detection beyond face, the quantity of these datasets\nis limited due to manual generation. To expand the scope of re-\nsearch, it is important to develop more diverse datasets for TFL\ntasks. Our datasets, similar to TFL subset of ForgeryNet [21] , uses a\nrandom approach to generate segments, which facilitates research\non TFL tasks beyond facial images and beyond binary classification\ntasks. Moreover, our generation process can be reproduced under\nlow-cost conditions. Based on our research, the dataset can further\nexpand to more diverse scenarios and promote TFL task research.\nTable 6: Quantitative comparison of TVIL to existing pop-\nular Video and Audio Forensics Datasets in recent 3 years.\nCls: Classification; SL: Spatial Localization; TFL: Temporal\nForgery Localization; V: Visual; A: Audio.\nDataset Year Tasks Modality Application Manipulated # Attacks #Real #FakeWildDeepfake [66] 2021 Cls V Face AIGC - 3,805 3,509FakeAVCeleb [27] 2021 Cls A+V Face AIGC 3 570 19,500ForgeryNet [21] 2021 SL/TFL/Cls V Face AIGC 5 99,630 121,617Lav-DF [6] 2022 TFL/Cls A+V Face AIGC 2 36,431 99,873WaveFake [17] 2021 Cls A Speech AIGC 6 18,100 104,885Psynd [58] 2022 TFL A Speech AIGC 1 30 2371VideoSham [38] 2023 Cls A+V Video Manipulation User Generated 40 413 413TVIL(Ours) 2023 TFL V Video Manipulation AIGC 4 914 3539\nA.2 More Experiments Results for LAV-DF\nSubset\nWe present the AP and AR performance of our method and state-\nof-the-art algorithms on the Lav-DF subset in Table 7. This subset\nexclusively contains manipulated videos with visual forgeries, ex-\ncluding those with audio-only modifications. The results show that\nthe single-modal algorithms in Table 7 outperform their counter-\nparts in Table 2, validating the efficacy of using visual information\nalone in this context. Notably, our method achieves state-of-the-art\nperformance with an AP@0.5 of98.83% using solely visual modality.\nDespite our methodâ€™s adaptability to different modalities, apply-\ning techniques like contrastive learning to analyze modal inconsis-\ntencies during multi-modal fusion posed challenges. Introducing\nthe audio modality in this subset, where authenticity is independent\nof audio, might lead to a decrease in overall performance. Feature\nfusion could potentially confuse critical features within this specific\nsubset, as evident from the performance drop of the multi-modal\nalgorithm AVFusion [3].\nNevertheless, our proposed method consistently outperforms\nother algorithms for both uni- and multi-modal inputs. Moreover,\nby incorporating audio features, our model achieves a substantial\n7.41% improvement in AP at tIoU 0.95, demonstrating the robustness\nand adaptability of our approach in multi-modal scenarios.\nTable 7: Performance comparison on Lav-DF Sub Set. Bold\nfaces correspond to the top performance.\nMethods Feature Sub SetAP@0.5 AP@0.75 AP@0.95 AR@10 AR@20 AR@50 AR@100MDS [11] Visual 23.43 3.48 00.00 58.53 56.68 53.16 49.67AGT [40] Visual 15.69 10.69 00.15 49.11 40.31 31.70 23.13BMN [36] Visual 32.32 11.38 00.14 59.69 48.17 39.01 34.17BMN (I3D) [36] Visual28.10 5.47 00.01 55.49 54.44 52.14 47.72AVFusion [3] Visual+Audio62.01 22.77 00.11 61.98 58.08 53.31 50.52\nBA-TFD [6] Visual 83.55 41.88 00.24 65.79 62.30 57.95 55.34Visual+Audio85.20 47.06 00.29 67.34 64.52 61.19 59.32ActionFormer [59] Visual98.06 94.43 27.25 91.30 92.04 92.27 92.28\nOurs Visual 98.83 95.95 30.11 92.32 92.65 92.74 92.75Visual+Audio98.54 94.30 37.52 91.61 91.97 92.06 92.06\nA.3 More Experiments Results for Video-level\nFace Forgery Classification\nWe also conducted a comparison between our method and previous\ndeepfake detection methods on the Lav-DF Full Set for the video-\nlevel forgery classification task. The evaluation metric used is the\nArea Under the Receiver Operating Characteristic Curve (AUC), and\nthe results are summarized in Table 8. In our approach, we utilize\nthe scores obtained from detected forgery timestamps as the classi-\nfication scores for the respective videos. As observed, frame-based\nalgorithms such as ğ¹3-Net [44] exhibit significant performance\ndegradation in classifying partially manipulated videos due to their\nlack of consideration of temporal factors, leading to substantial dis-\ncrepancies in discriminating between different frames. On the other\nhand, video-level algorithms such as EfficientViT [12] demonstrated\nrelatively effective recognition of deepfake videos with partially\nmanipulated segments, but are unable to provide corresponding\ntimestamps for the forgeries. In contrast, our method achieved the\nbest classification performance while also providing corresponding\nforgery timestamps. Additionally, our temporal forgery localization\nperformance significantly outperforms MDS [11] and BA-TFD [6].\nIt is worth noting that our model was not specifically designed for\nthe classification task, and further performance improvement could\nbe achieved by introducing a dedicated classification head.\nTable 8: Deepfake detection results on the Lav-DF dataset.\nBold faces correspond to the top performance\nMethods AUC\nğ¹3-Net [44] 52.0\nMDS [11] 82.8\nEfficientViT [12] 96.5\nBA-TFD [6] 99.0\nOurs 99.8",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8106777667999268
    },
    {
      "name": "Artificial intelligence",
      "score": 0.661320149898529
    },
    {
      "name": "Inpainting",
      "score": 0.6482185125350952
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.5918706655502319
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5706727504730225
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5680339336395264
    },
    {
      "name": "Feature extraction",
      "score": 0.5462890863418579
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45562151074409485
    },
    {
      "name": "Code (set theory)",
      "score": 0.4439966082572937
    },
    {
      "name": "Computer vision",
      "score": 0.3839697241783142
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2311471402645111
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24185976",
      "name": "Sichuan University",
      "country": "CN"
    }
  ]
}