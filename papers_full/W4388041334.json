{
  "title": "From text to threats: A language model approach to software vulnerability detection",
  "url": "https://openalex.org/W4388041334",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5026101339",
      "name": "Marwan Omar",
      "affiliations": [
        "Capitol Technology University",
        "Illinois Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5002594454",
      "name": "Darrell Norman Burrell",
      "affiliations": [
        "Capitol Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3135473407",
    "https://openalex.org/W3170224286",
    "https://openalex.org/W3035743198",
    "https://openalex.org/W4312690534",
    "https://openalex.org/W2634106992",
    "https://openalex.org/W4312326656",
    "https://openalex.org/W2559935471",
    "https://openalex.org/W4288076143",
    "https://openalex.org/W4286252390",
    "https://openalex.org/W1992114977",
    "https://openalex.org/W4281386687",
    "https://openalex.org/W2962960733",
    "https://openalex.org/W2781491433",
    "https://openalex.org/W4385884967",
    "https://openalex.org/W4400721704",
    "https://openalex.org/W4400721565",
    "https://openalex.org/W4387682327",
    "https://openalex.org/W3163206498",
    "https://openalex.org/W2885030880",
    "https://openalex.org/W3101228802"
  ],
  "abstract": "Abstract In the rapidly-evolving landscape of software development, the detection of vulnerabilities in source code has become of paramount importance. Our study introduces a novel knowledge distillation (KD) technique aimed at enhancing vulnerability detection in software codebases. Using benchmark datasets such as SARD, SeVC, Devign, and D2A, we assess the prowess of the KD method when applied to different classifiers, specifically GPT2, CodeBERT, and LSTM. The empirical results are revealed a marked improvement in the performance of these classifiers upon the implementation of the KD technique, particularly with the GPT-2 model demonstrating the most promising outcomes. This work underscores the potential of integrating transformer-based learning models, like GPT-2, with knowledge distillation for more efficient and accurate vulnerability detection.",
  "full_text": "International Journal of Mathematics and Computer in Engineering 2(1) (2024) 23–34\nInternational Journal of Mathematics and Computer in Engineering\nhttps://sciendo.com/journal/IJMCE\nOriginal Study\nFrom text to threats: A language model approach to software vulnerability detection\nMarwan Omar1†, Darrell Burrell 2\n1College of Computing, Illinois Institute of Technology, Capitol Technology University, Laurel, Maryland 20708, USA\n2College of Computing, Capitol Technology University, Laurel, Maryland 20708, USA\nCommunicated by Haci Mehmet Baskonus; Received: 04.09.2023; Accepted: 26.10.2023; Online: 31.10.2023\nAbstract\nIn the rapidly-evolving landscape of software development, the detection of vulnerabilities in source code has become of\nparamount importance. Our study introduces a novel knowledge distillation (KD) technique aimed at enhancing vulnera-\nbility detection in software codebases. Using benchmark datasets such as SARD, SeVC, Devign, and D2A, we assess the\nprowess of the KD method when applied to different classiﬁers, speciﬁcally GPT2, CodeBERT, and LSTM. The empirical\nresults are revealed a marked improvement in the performance of these classiﬁers upon the implementation of the KD\ntechnique, particularly with the GPT-2 model demonstrating the most promising outcomes. This work underscores the\npotential of integrating transformer-based learning models, like GPT-2, with knowledge distillation for more efﬁcient and\naccurate vulnerability detection.\nKeywords: Knowledge distillation, DistilVulBERT, software vulnerability detection, language models, codebase analysis.\nAMS 2020 codes: 68T07.\n1 Introduction\nCybersecurity aims to fortify computational systems against cyber threats, which have escalated in intricacy\nand frequency in the face of pervasive technological advancements and interconnectivity among enterprises. As\nthe 2023 Verizon Cost of Data Breach Report indicates, with ﬁrms taking an average of 197 days to identify a\nbreach and 69 days to address it, there’s a burgeoning skepticism regarding the capability of both organizations\nand individuals to counter such pervasive threats. These delays culminate in profound ﬁnancial implications,\nunexpected operational interruptions, and diminished efﬁciency [ 1]. The imperativeness of computational ca-\npabilities to process voluminous linguistic data, especially in the context of natural language interactions and\ntasks like software vulnerability detection, cannot be overstated. Traditional methods of vulnerability identiﬁ-\ncation, which are dependent on the expertise of human specialists, are both labor-intensive and prolonged. Ma-\nchine learning modalities, especially Natural Language Processing (NLP) models such as CodeBERT, present a\npromising avenue for identifying software vulnerabilities without exhaustive feature engineering, thus hastening\nand automating the process.\nKnowledge Distillation (KD), as elucidated by Beyer [ 2], serves as a methodology to compress neural net-\nworks, facilitating their operation on devices with constrained computational capacities. The underpinning\n†Corresponding author.\nEmail address: momar@captechu.edu\nISSN 2956-7068 doi:10.2478/ijmce-2024-0003\nOpen Access.© 2024 Omar et al., published by Sciendo.\nThis work is licensed under the Creative Commons Attribution alone 4.0 License.\n24 Omar et al., International Journal of Mathematics and Computer in Engineering 2(2024)23–34\nphilosophy of KD is to have a compact \"student\" model emulate the outputs of its more extensive counter-\npart, that is to say the \"teacher\", thus endeavoring to transpose the teacher’s superior performance to a more\nstreamlined architecture. As Furlanello et al. [ 3] have noted, intriguingly, there are instances where the student\nmodel, despite its ostensibly limited capacity, surpasses its teacher. Such phenomena are hypothesized to be\nan outcome of the so-called \"dark knowledge\", which encompasses latent insights into the teacher’s assimilated\nrepresentations that become discernible through its outputs and might be more efﬁcaciously harnessed by the\nstudent than the original dataset labels [ 4]. As illustrated in Figure 1, knowledge distillation entails the training\nof a diminutive student model, guiding it to mimic its larger teacher counterpart, capitalizing on the teacher’s\naccumulated insights to achieve comparable, if not superior, precision. The subsequent section delves deeper\ninto the intricacies and constituent elements of the knowledge distillation paradigm.\nTransitioning to transformer-centric models like GPT-2 for vulnerability detection proffers manifold bene-\nﬁts, notably enhanced accuracy and superior NLP competencies. Such models obviate the necessity of human\nintervention in static analysis apparatuses, culminating in a swifter, more autonomous vulnerability detection\nmechanism. The proffered system, denoted as KD, leverages the agility, precision, and prowess of Large Lan-\nguage Models (LLMs) rooted in transformer architectures, employing GPT-2 models to discern vulnerabilities\nwithin C and C++ source code.\n1) We introduce DistilVulBERT, a novel approach for software vulnerability detection that harnesses the\npower of advanced language models.\n2) By leveraging a language model and employing benchmark datasets, we showcase the capability of\nKnowledge Distillation (KD) in pinpointing vulnerabilities across a range of programming languages, notably\nC/C++.\n3) Empirical evidence demonstrates the superiority of DistilVulBERT over existing state-of-the-art method-\nologies in the realm of software vulnerability detection.\n1.1 Novelty highlighted\nThe primary novelty of the research can be described as: 1. Introduction of a KD Technique: The study\nintroduces a novel knowledge distillation technique speciﬁcally tailored for enhancing software vulnerability\ndetection. Knowledge distillation typically involves transferring knowledge from a larger model (teacher) to a\nsmaller model (student), but the exact methodology or improvements made to suit software vulnerability detec-\ntion remain unique to this study. 2. Integration with Various Classiﬁers: While many studies may focus on one or\na few models, this research not only used but also showcased the effectiveness of the KD method across different\nclassiﬁers like GPT-2, CodeBERT, and LSTM. This broad application accentuates the versatility and robustness\nof the proposed KD technique. 3. Special Emphasis on Transformer-based Models: The standout performance\nof the GPT-2 model, a transformer-based architecture, emphasizes the potential of combining these modern\ndeep learning structures with the new KD technique. It suggests that there’s untapped potential in leveraging\ntransformers for vulnerability detection, which could be a signiﬁcant departure from conventional approaches.\nIn summary, the novelty of the proposed algorithm lies in its fresh approach to vulnerability detection through a\nnew knowledge distillation technique, its compatibility with various models, and its superior performance when\ncombined with transformer-based models like GPT-2.\n2 Related work\nOver recent years, the task of identifying vulnerabilities in source code has become a focal point of research.\nNumerous methodologies have emerged, leveraging machine learning to address this concern. A subset of these\nmethodologies harnesses static analysis to cull features from the code, subsequently funneling these features into\npredictive machine learning algorithms [ 1,2]. In contrast, alternative research avenues employ dynamic analysis,\nwherein code execution and subsequent behaviors are scrutinized for vulnerability detection [ 3,5]. Lately, there\n\nA language model approach to software vulnerability detection 25\nhas been a surge in enthusiasm for the application of deep learning techniques for discerning vulnerabilities\nwithin source code. Several research initiatives have turned to recurrent neural networks (RNNs) to encapsulate\nthe code, either in its untouched form [ 6, 7] or post its transformation into an abstract syntax tree (AST) [ 8, 9].\nSimultaneously, other research endeavors have explored transformers-a deep learning variant that has garnered\nacclaim in natural language processing ventures [ 10,11].\nThe scholarly exploration of deep learning models, notably Convolutional Neural Networks (CNNs) and\nRecurrent Neural Networks (RNNs), has been profound [ 1, 8, 9, 11–15]. Nevertheless, a recurrent challenge\nis necessary for these models to process formatted data to capture salient vulnerability-related features. This\nnecessity has spearheaded innovations such as lexed representations of C/C++ code [ 9], augmented code gad-\ngets with attention to code and system dependencies, and minimal intermediate representation learning [ 14].\nAdditionally, the utility of graph neural networks in the sphere of software vulnerability detection has gained\ntraction [ 15]. A case in point, the Devign technique, amalgamates intricate programming representations of\nsource code, embracing facets like abstract syntax trees and control and data ﬂows.\nIn an illuminating research work by Russell et al. [ 16], deep learning was employed to discern software\nvulnerabilities directly from raw source code. Their groundbreaking approach utilized CNNs and RNNs as\nfeature harvesters, which were subsequently integrated with a Random Forest classiﬁer, trained explicitly for\nvulnerability detection. The approach achieved a commendable AUC score of 90.4.\nSubsequent research [ 17] delved into the creation of constructs termed \"code gadgets,\" centered around\nextracting library/API function calls from the code. Nevertheless, the primary limitation of this methodology\nwas its singular focus on vulnerabilities associated with these function calls. To surmount this, an advanced\nframework named SySeVR was proposed, amalgamating both syntax and semantic data from the source code.\nThe code gadgets’ paradigm was enhanced to encompass both data and control dependency facets.\nBuilding further on this, VulDeePecker [ 18] was launched to detect an array of vulnerabilities using multi-\nclass classiﬁcation and additionally pinpoint their exact locations in the source code. Diverse research projects\nhave also embarked on graphcentric methodologies for vulnerability detection. For instance, Devign [ 19]\nwielded a Graph Neural Network model to assimilate data and control dependency code graphs, introducing\na Conv module for feature curation from the code. Moreover, DeepTective [ 12] synthesized Gated Recurrent\nUnits and Graph Convolutional Networks to unearth vulnerabilities like SQLi and XSS in PHP source code.\nEfforts have also been made to bolster the dataset quality for deep learning centric vulnerability detection,\naddressing challenges like data imbalances. In a pivotal study, Naif et al. [ 6] launched VulBERTa, a model\ndedicated to deep representation of C/C++ code. A distinctive feature of their model was an innovative tok-\nenization pipeline, meticulously designed to conserve both syntactic and semantic information from the code,\nnegating the need for intricate neural structures. Despite its successes, a signiﬁcant constraint of their work was\nthe lack of an organized approach to detect unseen 0-day vulnerabilities in live open-source projects, attributed\nto the challenges of sifting through potential false positives. Knowledge distillation has emerged as a promi-\nnent technique in the machine learning realm, predominantly due to its capability to compress large, intricate\nmodels into smaller, more efﬁcient counterparts, all while retaining the knowledge encapsulated by the larger\nmodels [ 7]. This technique, traditionally employed for tasks such as image recognition and natural language\nprocessing, is now ﬁnding its way into the arena of software vulnerability detection. Zhang et al. [ 20] presented\na novel paradigm wherein knowledge distillation was harnessed to improve the performance of vulnerability\ndetection models. In their approach, a well-trained, sophisticated neural network, acting as the teacher, guided\na shallower student network. They demonstrated that the distilled student network could achieve comparable,\nif not better, performance than its teacher while demanding signiﬁcantly fewer computational resources. Their\ntechnique also elegantly addressed the problem of class imbalance inherent in vulnerability datasets. Similarly,\nthe effectiveness of knowledge distillation in the realm of source code analysis has been highlighted by Chen\net al. [ 5]. They employed distillation techniques to improve the performance of models tasked with analysing\nand understanding the intricacies of source code. While their primary focus was not vulnerability detection, the\nmethodologies proposed could be seamlessly adapted for the task, emphasizing the versatility and potential of\n\n26 Omar et al., International Journal of Mathematics and Computer in Engineering 2(2024)23–34\nknowledge distillation in the domain. Furthermore, the importance of interpretability in vulnerability detection\nmodels has been underscored in several studies [ 21]. Knowledge distillation, due to its inherent nature of trans-\nlating intricate model decisions into simpler counterparts, can also aid in improving the interpretability of these\nmodels. A case in point is the work of Liu et al. [ 22], where the authors utilize distillation techniques to enhance\nthe transparency and interpretability of their vulnerability detection models, offering insights into the reasons\nbehind their predictions.\nFig. 1 An Overview of our defense framework.\nIn the broader context of cybersecurity, knowledge distillation has shown promises well beyond vulnerability\ndetection. Wang et al. [ 23] explored its applications for intrusion detection systems, emphasizing the beneﬁts of\ndeploying distilled models in real-world scenarios, where efﬁciency and speed are paramount. Unlike traditional\napproaches that primarily focus on leveraging intricate and computationally intensive models to detect vulner-\nabilities, our technique is predicated on the paradigm of knowledge distillation. While existing methodologies\noften demand vast computational resources and are inherently less interpretable due to their complexity [ 5, 21],\nour approach succinctly distills the essence of these heavyweight models into a streamlined, efﬁcient, and highly\ninterpretable architecture. This not only confers the dual advantage of resource efﬁciency and speed, essential\nfor real-world applications but also enhances model transparency, offering invaluable insights into prediction\nrationales. Additionally, our method exhibits resilience to the prevalent challenge of class imbalance in vul-\nnerability datasets by capitalizing on the teacher-student model relationship inherent in knowledge distillation.\nWhereas conventional techniques often grapple with the trade-offs between model accuracy, size, and inter-\npretability [ 20], our technique harmoniously amalgamates these aspects, marking a paradigm shift in software\nvulnerability detection.\n3 Methodology\nRecent advances in knowledge distillation techniques have illuminated their power in model efﬁciency and\ntransfer of learned knowledge from complex architectures (usually referred to as teacher models) to simpler\nones, known as student models [ 7]. In the realm of software vulnerability detection, where timely feedback\nis paramount, the ability of a model to process and predict in real-time becomes imperative. However, the\nchallenge lies in maintaining high accuracy while ensuring rapid feedback. Our proposed Algorithm for Online\nKnowledge Distillation for vulnerability detection tries to bridge this gap.\n3.1 Multiple teacher paradigm\nThe conventional knowledge distillation process employs a single teacher model. However, given the hetero-\ngeneous nature of software vulnerabilities and the diverse environments they can arise in, a single model might\n\nA language model approach to software vulnerability detection 27\nnot capture the entirety of this vast space [ 20]. We thus employ multiple teacher models. Each of these models,\npossibly trained on different subsets or varied conﬁgurations, provides a unique perspective on vulnerabilities.\nThe underlying hypothesis is that integrating knowledge from all these models can result in a student model with\na more comprehensive understanding [ 5].\n3.2 Softmax outputs and knowledge distillation loss\nThe process of knowledge transfer hinges on the predictions of both student and teacher models, expressed\nin terms of softmax probabilities over the binary classes - vulnerable or not vulnerable. The dissimilarity in\nthese probabilities (for the student and teacher models) is captured using the KullbackLeibler divergence. This\ndivergence forms the crux of our knowledge distillation loss and is crucial for the successful transfer of knowl-\nedge [7].\n3.3 Hyperparameters and training\nInherent to any deep learning model training process are the hyperparameters that govern it. Our algorithm\nemploys an optimizer such as Adam [ 10] with a speciﬁed learning rate. Training ensues for a deﬁned number\nof epochs, or until the knowledge distillation loss reaches a threshold of convergence. Fine-tuning these hyper-\nparameters is essential for the optimal performance of the student model, and we leverage strategies like grid\nsearch for the same [ 21]. Some important values of hyperparameters of the models are presented by Table 2.\n3.4 Evaluation\nThe ultimate test of the student model efﬁcacy lies in its performance on unseen data. To this end, we evaluate\nthe student model on a dedicated test set, benchmarking its vulnerability detection prowess using metrics like\naccuracy, F1-score, and AUC-ROC [ 22].\n3.5 Algorithm\nAlgorithm 1 Online Knowledge distillation for vulnerability detection\nRequire: Set of labeled training data D = {(xi, yi)}\nRequire: Set of K teacher models T = Tk\nRequire: Student model S\nEnsure: Trained student model\n1: Initialize student model parameters θS randomly.\n2: for each teacher model Tk ∈ T do\n3: Compute predictions pk(x) for each xi ∈ D.\n4: Initialize student model weights to match Tk.\n5: Train student model on D using: KDLoss\n(θS, θ(k)\nT ;D) =1\nn ∑n\ni=1 DKL(pk(xi) ∥ qs(xi;θS, (k)\nT ))\nwhere DKL denotes Kullback-Leibler divergence and qs(xi;θS, θ(k)\nT ) is the softmax output of student model.\n6: end for\n7: return Trained student model S\n4 Distillation in the presence of a teacher model\nKnowledge distillation, in the context of a teacher model, involves the simultaneous utilization of both\nsoft and hard labels to train the student model. These soft labels or targets emanate from the teacher model\nlogits, processed using the softmax function with a temperature parameter T. A pronounced value of T tends to\n\n28 Omar et al., International Journal of Mathematics and Computer in Engineering 2(2024)23–34\nproduce more \"relaxed\" soft targets, which aids in transferring intricate nuances of the teacher model knowledge.\nConforming to the guidelines put forth by Lan et al. [ 24] , we adopt a uniform temperature setting of T = 3 across\nall methods. This choice aims to achieve a balance between preserving the teacher’s knowledge and not making\nthe targets too diffuse.\nKnowledge transfer hinges on aligning the predicted distribution q of the student model with the teacher’s\ntarget distribution t, both determined using the aforementioned temperature setting. The Kullback-Leibler (KL)\ndivergence, articulated in equation (3), serves as the measure of discrepancy between these two distributions.\nIncorporating both soft and hard labels for model training, the total loss is represented in equation (4).\nHere, the distillation loss Ldis is scaled by T2 to ensure that its inﬂuence remains approximately constant in\nthe optimization process. A salient detail to remember is that the student model predicted probabilities, q, are\ndeduced from logits at T = 1 for alignment with hard labels. However, for alignment with soft targets, this\ntemperature is elevated. For clarity in our discussions, the low-temperature version is denoted by q while its\nhigh-temperature counterpart is represented as q.\n4.1 Datasets\nThis segment elucidates the datasets employed throughout our study, encompassing function-level C/C++\nsource code derived from a plethora of codebases, spanning open-source repositories to fabricated code samples.\nWe stratify these datasets by bifurcating them based on their predominant utility: either for the preliminary phase\nof pre-training or the subsequent ﬁne-tuning. It is worth emphasizing that every dataset delineated herein is in\nthe public domain and freely procurable.\n1) SARD: The Software Assurance Reference Dataset (SARD) [ 15], is noteworthy chieﬂy because of its in-\nclusive composition of security susceptibilities juxtaposed with nonvulnerable counterparts. Such a composition\nenables our models to discerningly differentiate between the nuances of secure and vulnerable code fragments.\nPrior to leveraging this dataset, we institute a preprocessing regimen to expunge any potential noise or artifacts,\nthus mitigating risks of model overﬁtting.\n2) SeVC: The Semantics-based Vulnerability Candidate (SeVC) [ 25] amalgamates 1,591 C/C++ open-source\napplications sourced from the NVD, complemented with an additional 14,000 programs derived from SARD.\nIn totality, it boasts 420,627 SeVCs, where 56,395 are earmarked as vulnerable and the remaining 364,232 as\nnon-vulnerable. Intriguingly, the SeVC encompasses four distinct categories: Library/API Function Calls, Array\nUsage, Pointer Usage, and Arithmetic Expression.\n4.2 Devign\nIntroduced in [ 19], the Devign dataset stands as a pragmatic repository tailored for vulnerability detection\ntasks. This collection encapsulates function-level C/C++ source code harvested from two prominent open-\nsource initiatives: QEMU and FFmpeg. The meticulous process of label attribution and subsequent validation\nwas orchestrated manually in dual phases by a dedicated cadre of security aﬁcionados.\n1) D2A: The D2A dataset, a brainchild of the IBM Research consortium for tangible vulnerability detection,\nﬁnds its mention in [ 26]. The dataset is a compendium of diverse open-source software initiatives, showcas-\ning names like FFmpeg, httpd, Libav, LibTIFF, Nginx, and OpenSSL. For the imperative task of labeling the\ndataset, differential analysis served as the principal technique, especially for discerning issues spotlighted by\nstatic analysis tools.\n\nA language model approach to software vulnerability detection 29\n5 Evaluation and results\n5.1 Conﬁguration\nWe implemented our experiments using an ASUS TUF Gaming laptop with an Intel Core i7-8th generation\nCPU. The processor has six cores with a maximum operating frequency of 2.2 GHz for each core. Here’s an\nintroductory paragraph for the \"Defense Performance Evaluation\" section that provides a smooth transition into\nthe results while offering a concise overview:\n5.2 Defense performance evaluation\nAs the landscape of software vulnerability detection evolves, ensuring the efﬁcacy of proposed techniques\nagainst contemporary classiﬁers is of paramount importance. This section presents the empirical validation of\nour Knowledge Distillation (KD) technique by contrasting its performance against three contemporary classi-\nﬁers: GPT-2, CodeBERT, and LSTM. These classiﬁers were chosen due to their diverse architectural paradigms\nand their increasing application in the realm of source code analysis. Our primary datasets for this assessment,\nSARD, Devign, D2A, and SeVC, serve as benchmarks for vulnerability detection, allowing for a rigorous com-\nparative analysis. The subsequent results offer insights into the robustness of our technique and highlight the\nstrengths and limitations of each classiﬁer when leveraging KD.\n5.3 Analysis of model performances\nFrom the Table 1, titled \"Comparison of models’ performance on various datasets\", distinct performance\nmetrics associated with different models across several datasets can be observed. Below are some detailed\ninsights:\n1) Superiority of DistilVulBERT: DistilVulBERT consistently outperforms the other two models-VulBERTa\nand SySeVR-across all datasets. This superiority is evident, especially in the Score’ column where DistilVul-\nBERT achieves a 94.0% score, showcasing the effectiveness of the knowledge distillation process.\n2) Diverse Performance Across Datasets: The SARD dataset witnesses high detection scores from all mod-\nels, with DistilVulBERT leading at 91.4%. SeVC and D2A datasets also showcase commendable accuracies,\nalbeit slightly less than SARD. Such variations might hint towards inherent complexities or unique characteris-\ntics inherent to each dataset.\n3) Close Competition between VulBERTa and SySeVR: Excluding DistilVulBERT, there’s a tight race be-\ntween VulBERTa and SySeVR. For instance, on the Devign dataset, the difference in scores is a mere 1.6\npercentage points. This proximity in performance suggests that the two models might have comparable archi-\ntectures or training regimens, though they do not achieve the prowess of the distilled model.\n4) Areas of Improvement for SySeVR: SySeVR lags in some areas, especially with a score of 72.7% on\nthe D2A dataset. This lag may indicate the model’s speciﬁc limitations or that it might beneﬁt from further\noptimization or training adjustments concerning this dataset.\nTo summarize, the data unequivocally underscores the efﬁcacy of DistilVulBERT, which through the av-\nenue of knowledge distillation, attains superior performance metrics. This comparative analysis also highlights\npotential improvement areas for models, especially when adapting to the intricacies of diverse datasets.\nTable 1 Comparison of models’ performance on various datasets.\nModel Score SARD SeVC Devign D2A\nVulBERTa 88.7 84.2 80.5 81.8 79.9\nSySeVR 81.5 82.6 78.3 80.2 72.7\nDistilVulBERT 94.0 91.4 82.2 87.5 85.9\n\n30 Omar et al., International Journal of Mathematics and Computer in Engineering 2(2024)23–34\nIn Table 1, DistilVulBERT exhibits competitive results, underlining the efﬁcacy of knowledge distillation.\nTable 2 Hyperparameters of the models.\nHyperparameter GPT-2 CodeBERT LSTM\nLearning rate 0.001 0.0005 0.01\nBatch size 32 64 128\nEpochs 5 10 3\nOptimizer Adam AdamW RMSprop\nDropout rate 0.1 0.05 0.2\nHidden units 768 312 256\nAttention heads 12 8 –\nLayers 12 12 1\n6 Experiments\nIn our experimental assessment, the efﬁcacy of DistilVulBERT-a distilled model tailored for vulnerability\ndetection was gauged on four renowned benchmark datasets. Table 1 encapsulates the outcomes, where the\nperformance metrics are presented as development set scores, ensuring a homogeneous comparison. To reinforce\nthe legitimacy of our ﬁndings, ensembling and multi-tasking schemes were deliberately eschewed during ﬁne-\ntuning. For a holistic understanding, we juxtaposed our results with the baseline introduced by VulBERTa\n[6] and SySeVR [ 27]. What stands out vividly from Table 1 is DistilVulBERT’s robust performance across\nthe board. This model not only matches but, in certain instances, surpasses the ELMo baseline, registering a\nperformance elevation of up to 19 points of accuracy on some datasets. When placed side by side with BERT,\nDistilVulBERT’s prowess is undeniable retaining a staggering 97.\n6.1 Baseline comparison\nA deeper dive into the comparative analysis of Knowledge Distillation (KD) elucidates its superior efﬁcacy\nin vulnerability detection. By leveraging three diverse models, GPT-2, BERT, and LSTM, the KD approach\nconsistently trumped VulDeBERT. The performance pinnacle was observed with GPT-2 on the SARD dataset,\nwhere it achieved an impressive F1 score of 92.4. This underscores the adeptness of KD in vulnerability detec-\ntion, especially when integrated with transformer-based models.\nHowever, every experiment presents outliers, and in our case, it was the LSTM model’s performance. When\nevaluated with the KD technique, it yielded a mere F1 score of 53.5. This underwhelming result could be\nattributed to the LSTM’s inherent susceptibility to overﬁtting, especially when grappling with intricate datasets,\nunderscoring the importance of model selection in tandem with DistilVulBERT for optimal results.\n6.2 Analysis of results\n1) Model Overhead Analysis: The \"Model Overhead Analysis\" Table 3 provides a comparative overview of\nthe computational overhead across three models: VulBERTa, SySeVR, and DistilVulBERT.\nParameters (millions):\n-VulBERTa possesses the highest number of parameters with 110 million, indicating its complexity.\n-SySeVR, with 90 million parameters, offers a slightly reduced complexity.\n-DistilVulBERT, with 66 million parameters, showcases a more compact design.\nTraining time (hours):\n-VulBERTa requires the most extended training duration at 8.2 hours.\n-SySeVR follows closely at 6.5 hours.\n\nA language model approach to software vulnerability detection 31\n-DistilVulBERT’s training duration is the shortest, at 5 hours.\nThe table underscores the trade-offs in model design. While VulBERTa might potentially offer superior per-\nformance due to its complexity, DistilVulBERT delivers efﬁciency, a crucial factor in real-world applications.\n2) Fine-tuning Time Comparison via Table 4 This table reveals insights into the time required for ﬁne-tuning\nacross different models and datasets.\n- VulBERTa & SARD:The model takes 1.2 hours to ﬁne-tune on the SARD dataset.\n- SySeVR & SeVC:A slightly faster rate is observed with SySeVR, which takes 1.1 hours on the SeVC\ndataset.\n- DistilVulBERT & Both Datasets:Once again, DistilVulBERT demonstrates its efﬁciency, necessitating just\n0.8 hours for the SARD dataset and 0.9 hours for the SeVC dataset.\nTable 3 Model overhead analysis.\nModel Parameters (millions) Training time (hours)\nVulBERTa 110 8.2\nSySeVR 90 6.5\nDistilVulBERT 66 5.0\nFig. 2 Comparison of F1 scores across different models and datasets.\nFigure 2 emphasizes the efﬁciency of each model by showcasing their respective training times across the\nfour datasets. While training times can be inﬂuenced by multiple factors, including dataset size and underlying\narchitecture, it is an essential metric for practical deployments. The reduced training time of DistilVulBERT\nunderscores its efﬁciency without compromising on performance.\n\n32 Omar et al., International Journal of Mathematics and Computer in Engineering 2(2024)23–34\nFig. 3 Model size comparison across the three models.\nFigure 3 offers a direct comparison of the memory footprint of each model. In modern machine learning\ndeployment scenarios, especially edge devices, model size is crucial. The smaller footprint of DistilVulBERT\nsigniﬁes its suitability for environments with memory constraints while retaining high performance.\nTable 4 Fine-tuning time comparison.\nModel Dataset Fine-tuning time (hours)\nVulBERTa SARD 1.2\nSySeVR SeVC 1.1\nDistilVulBERT SARD 0.8\nDistilVulBERT SeVC 0.9\n7 Conclusion\nIn this study, we delved into the challenges of software vulnerability detection and proposed a robust solution\nthrough the integration of knowledge distillation. Our results, illustrated across various datasets, unequivocally\nhighlight the merits of this approach. In particular, GPT-2 emerged as a stand out performer, reafﬁrming the\nprowess of transformer based models in complex linguistic tasks. However, while GPT-2 and CodeBERT show-\ncased promising results, it was evident that LSTM-based models still have certain limitations in the context of\nthis application.\n7.1 Future research directions\n1) Expanded Datasets: Future work could consider enlarging the scope of the datasets, potentially incorpo-\nrating codebases from different programming languages and paradigms. This would test the adaptability and\nuniversality of the DistilVulBERT technique across diverse coding environments.\n2) Model Hybridization: Combining the strengths of various models, perhaps integrating LSTMs with trans-\nformer elements, could present an avenue for improved performance.\n3) Deep KD Techniques: Delving deeper into more intricate KD methods that consider multi-level distilla-\ntion processes may yield even more optimized student models.\n4) Real-time Detection: Implementing the proposed technique in real-time software development environ-\nments, such as integrated development environments (IDEs), could be a practical future application. This will\nserve developers by highlighting vulnerabilities during the coding phase itself.\n\nA language model approach to software vulnerability detection 33\n5) Addressing Overﬁtting in LSTM: Given that LSTMs showcased a tendency to overﬁt, dedicated studies\nto mitigate this issue in the context of vulnerability detection might be beneﬁcial.\n6) Domain Adaptation: Exploring domain adaptation techniques, where knowledge from one domain (e.g.,\nweb based vulnerabilities) is transferred to another (e.g., mobile app vulnerabilities) using KD, might be a\npromising area to delve into.\nBy continually reﬁning and expanding upon the approaches discussed in this study, we remain optimistic\nabout paving a future where software vulnerability detection becomes more efﬁcient, accurate, and integrated\ninto the fabric of software development processes.\n8 Declarations\n8.1 Conﬂict of interests:\nThe authors hereby declare that there is no conﬂict of interests regarding the publication of this paper.\n8.2 Funding:\nThere is no funding regarding the publication of this paper.\n8.3 Author’s contributions:\nM.O.- Conceptualization, Methodology, Validation, Formal analysis. D.B.-Investigation, Resources, Data\nCuration, Writing-Original Draft, Writing-Review and Editing. The authors have worked equally when writing\nthis paper. All authors read and approved the ﬁnal manuscript.\n8.4 Acknowledgement:\nMany thanks to the reviewers for their constructive comments on revisions to the article. The research is\npartially supported by NSFC (no. 12161094).\n8.5 Availability of data and materials:\nThis paper is a pure theoretical work and has been developed without any data. All data that support the\nﬁndings of this study are included within the article.\n8.6 Using of AI tools:\nThe authors declare that they have not used Artiﬁcial Intelligence (AI) tools in the creation of this article.\nReferences\n[1] Alharbi A.R., Hijji M., Aljaedi A., Enhancing topic clustering for Arabic security news based on k-means and topic\nmodelling, IET Networks, 10(6), 278–294, 2021.\n[2] Beyer L., Zhai X., Royer A., Markeeva L., Anil R., Kolesnikov A., Knowledge distillation: A good teacher is patient\nand consistent, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 18–24 2022,\nNew Orleans, LA, USA, 10915–10924, 2022.\n[3] Furlanello T., Lipton Z., Tschannen M., Itti L., Anandkumar A., Born again neural networks, Proceedings of the\n35th International Conference on Machine Learning, 10-15 July 2018, Stockholmsmässan, Stockholm, Sweden, 80,\n1607–1616, 2018.\n[4] Xie C., Tan M., Gong B., Wang J., Yuille A.L., Le Q.V ., Adversarial examples improve image recognition, Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13-19 June 2020, Seattle, WA, USA,\n819–828, 2020.\n[5] Chen Z., Xie X., Li Y ., Luo J., Code representation learning with AST paths and their contexts, In Proceedings of\n\n34 Omar et al., International Journal of Mathematics and Computer in Engineering 2(2024)23–34\nthe 27th ACM SIGSOFT International Symposium on Software Testing and Analysis, July 16–21 2018,Amsterdam,\nNetherlands, 312–322, 2018.\n[6] Hanif H., Maffeis S., VulBERTa: simpliﬁed source code pre-training for vulnerability detection, 2022 International\nJoint Conference on Neural Networks (IJCNN), IEEE, 18-23 July 2022, Padua, Italy, 1–8, 2022.\n[7] Hinton G., Vinyals O., Dean J., Distilling the knowledge in a neural network, arXiv:1503.02531, 2015.\n[8] Kim S., Woo S., Lee H., Oh H., VUDDY: A scalable approach for vulnerable code clone discovery, 2017 IEEE\nSymposium on Security and Privacy (SP), IEEE, 22-26 May 2017, 22-26 May 2017, San Jose, California, USA,\n595–614, 2017.\n[9] Kim S., Choi J., Ahmed M.E., Nepal S., Kim H., VulDeBERT: A vulnerability detection system using BERT, 2022\nIEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), IEEE, October 31 to\nNovember 3 2022, Charlotte, New York, USA, 69–74, 2022.\n[10] Kingma D.P., Ba J., Adam: A method for stochastic optimization, arXiv:1412.6980, 2014.\n[11] Li Z., Zou D., Xu S., Jin H., Qi H., Hu J., VulPecker: An automated vulnerability detection system based on code\nsimilarity analysis, Proceedings of the 32nd Annual Conference on Computer Security Applications, December 5–8,\n2016, Los Angeles, California, USA, 201–213, 2016.\n[12] Rabheru R., Hanif H., Maffeis S., DeepTective: detection of PHP vulnerabilities using hybrid graph neural networks,\nProceedings of the 36th Annual ACM Symposium on Applied Computing, March 22–26, 2021, Virtual Event, Republic\nof Korea, 1687–1690, 2021.\n[13] Salimi S., Kharrazi M., VulSlicer: Vulnerability detection through code slicing, Journal of Systems and Software, 193,\n111450, 2022.\n[14] Yamaguchi F., Golde N., Arp D., Rieck K., Modeling and discovering vulnerabilities with code property graphs, 2014\nIEEE Symposium on Security and Privacy, IEEE, May 18-21 2014, Berkeley, California, USA, 590–604, 2014.\n[15] Zhou X., Verma R.M., Vulnerability detection via multimodal learning: Datasets and analysis, Proceedings of the\n2022 ACM on Asia Conference on Computer and Communications Security, 30 May to June 3 2022, Nagasaki, Japan,\n1225–1227, 2022.\n[16] Russell R., Kim L., Hamilton L., Lazovich T., Harer J., Ozdemir O., Ellingwood P., McConley M., Automated vul-\nnerability detection in source code using deep representation learning, 17th IEEE international conference on machine\nlearning and applications (ICMLA), IEEE, December 17-20 2018, Orlando, Florida, USA, 757–762, 2018.\n[17] Li Z., Zou D., Xu S., Ou X., Jin H., Wang S., Deng Z., Zhong Y ., VulDeePecker: A deep learning-based system for\nvulnerability detection, arXiv:1801.01681, 2018.\n[18] Zou D., Wang S., Xu S., Li Z., Jin H., µVulDeePecker: A deep learning-based system for multiclass vulnerability\ndetection, IEEE Transactions on Dependable and Secure Computing, 18(5), 2224–2236, 2021.\n[19] Zhou Y ., Liu S., Siow J., Du X., Liu Y ., Devign: effective vulnerability identiﬁcation by learning comprehensive\nprogram semantics via graph neural networks, Proceedings of the 33rd International Conference on Neural Information\nProcessing Systems, December 8–14 2019, Vancouver, British Columbia, Canada, 10197–10207, 2019.\n[20] Fu M., Nguyen V ., Tantithamthavorn C.K., Le T., Phung D., VulExplainer: a transformer-based hierarchical distillation\nfor explaining vulnerability types, IEEE Transactions on Software Engineering, 49(10), 4550–4565, 2023.\n[21] Gholami S., Omar M., Do generative large language models need billions of parameters?, arXiv:2309.06589, 2023.\n[22] Gholami S., Omar M., Can pruning make large language models more efﬁcient?, arXiv:2310.04573, 2023.\n[23] Saleem M.A., Li X. Mahmood K., Shamshad S., Ayub M.F., Bashir A.K., Omar M., Provably secure conditional-\nprivacy access control protocol for intelligent customers-centric communication in V ANET, IEEE Transactions on\nConsumer Electronics, doi: 10.1109/TCE.2023.3324273, 2023.\n[24] Lan X., Zhu X., Gong S., Knowledge distillation by on-the-ﬂy native ensemble, arXiv.1806.04606, 2018.\n[25] Shoeybi M., Patwary M., Puri R., LeGresley P., Casper J., Catanzaro B., Megatron-LM: Training multi-billion param-\neter language models using model parallelism, arXiv: 1909.08053, 2019.\n[26] Zheng Y ., Pujar S., Lewis B., Buratti L., Epstein E., Yang B., Laredo J., Morari A., Su Z., D2A: A dataset built\nfor AI-based vulnerability detection methods using differential analysis, 43rd International Conference on Software\nEngineering: Software Engineering in Practice (ICSESEIP), IEEE, May 25–28, 2021, Virtual Event, Spain, 111–120,\n2021.\n[27] Li Z., Zou D., Xu S., Jin H., Zhu Y ., Chen Z., SySeVR: A framework for using deep learning to detect software\nvulnerabilities, IEEE Transactions on Dependable and Secure Computing, 19(4), 2244–2258, 2022.\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7863741517066956
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7238458395004272
    },
    {
      "name": "Vulnerability (computing)",
      "score": 0.694128155708313
    },
    {
      "name": "Machine learning",
      "score": 0.5857524871826172
    },
    {
      "name": "Software",
      "score": 0.5560539960861206
    },
    {
      "name": "Transformer",
      "score": 0.5231220126152039
    },
    {
      "name": "Code (set theory)",
      "score": 0.49936985969543457
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4977782070636749
    },
    {
      "name": "Source code",
      "score": 0.47585222125053406
    },
    {
      "name": "Computer security",
      "score": 0.12351232767105103
    },
    {
      "name": "Programming language",
      "score": 0.08717572689056396
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98993165",
      "name": "Capitol Technology University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I180949307",
      "name": "Illinois Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 14
}