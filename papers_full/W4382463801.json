{
    "title": "Improving Dynamic HDR Imaging with Fusion Transformer",
    "url": "https://openalex.org/W4382463801",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2331500333",
            "name": "Rufeng Chen",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A2404649762",
            "name": "Bolun Zheng",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A1551912349",
            "name": "Hua Zhang",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A2102344228",
            "name": "Quan Chen",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A2109892987",
            "name": "Chenggang Yan",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A3158996578",
            "name": "Gregory Slabaugh",
            "affiliations": [
                "Queen Mary University of London"
            ]
        },
        {
            "id": "https://openalex.org/A2176584952",
            "name": "Shanxin Yuan",
            "affiliations": [
                "Queen Mary University of London"
            ]
        },
        {
            "id": "https://openalex.org/A2331500333",
            "name": "Rufeng Chen",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A2404649762",
            "name": "Bolun Zheng",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A1551912349",
            "name": "Hua Zhang",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A2102344228",
            "name": "Quan Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109892987",
            "name": "Chenggang Yan",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A3158996578",
            "name": "Gregory Slabaugh",
            "affiliations": [
                "Hangzhou Dianzi University",
                "Queen Mary University of London"
            ]
        },
        {
            "id": "https://openalex.org/A2176584952",
            "name": "Shanxin Yuan",
            "affiliations": [
                "Queen Mary University of London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2104604834",
        "https://openalex.org/W4225619305",
        "https://openalex.org/W6679040706",
        "https://openalex.org/W2147743564",
        "https://openalex.org/W6736170873",
        "https://openalex.org/W2069281566",
        "https://openalex.org/W3173941784",
        "https://openalex.org/W2766497195",
        "https://openalex.org/W6746045642",
        "https://openalex.org/W6684905197",
        "https://openalex.org/W2001868068",
        "https://openalex.org/W6778981233",
        "https://openalex.org/W2110285966",
        "https://openalex.org/W2894939846",
        "https://openalex.org/W3016539359",
        "https://openalex.org/W6800689796",
        "https://openalex.org/W4320855503",
        "https://openalex.org/W4221163966",
        "https://openalex.org/W4226016551",
        "https://openalex.org/W3164958723",
        "https://openalex.org/W6840201148",
        "https://openalex.org/W2590560192",
        "https://openalex.org/W2794312178",
        "https://openalex.org/W1494666760",
        "https://openalex.org/W3040184427",
        "https://openalex.org/W6967218089",
        "https://openalex.org/W3106019615",
        "https://openalex.org/W2954591474",
        "https://openalex.org/W3216987090",
        "https://openalex.org/W1505723240",
        "https://openalex.org/W1995813543",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W6712417245",
        "https://openalex.org/W3199093552",
        "https://openalex.org/W2883566908",
        "https://openalex.org/W3207296465",
        "https://openalex.org/W6773743208",
        "https://openalex.org/W3033492948",
        "https://openalex.org/W6735986144",
        "https://openalex.org/W3210946101",
        "https://openalex.org/W4285200952",
        "https://openalex.org/W2897840030",
        "https://openalex.org/W4304015013",
        "https://openalex.org/W6776151872",
        "https://openalex.org/W3199549300",
        "https://openalex.org/W4385483056",
        "https://openalex.org/W2134726061",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4388462054",
        "https://openalex.org/W2769930525",
        "https://openalex.org/W3035047434",
        "https://openalex.org/W3034337578",
        "https://openalex.org/W2964076515",
        "https://openalex.org/W3173755231",
        "https://openalex.org/W2168014483",
        "https://openalex.org/W4382465644",
        "https://openalex.org/W3207918547",
        "https://openalex.org/W4226440503",
        "https://openalex.org/W3006487162",
        "https://openalex.org/W4283311225",
        "https://openalex.org/W2963728414",
        "https://openalex.org/W2054927225",
        "https://openalex.org/W2397465079",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W2127129827",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2601564443",
        "https://openalex.org/W4309117966",
        "https://openalex.org/W4287020683",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W2963882477",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2736611505",
        "https://openalex.org/W3201111672"
    ],
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets.",
    "full_text": "Improving Dynamic HDR Imaging with Fusion Transformer\nRufeng Chen1, Bolun Zheng1*, Hua Zhang1*, Quan Chen1,\nChenggang Yan1, Gregory Slabaugh2, Shanxin Yuan2\n1Hangzhou Dianzi University, Xiasha No.2 Street, Hangzhou, 310018, Zhejiang, China\n2Queen Mary University of London, London, UK\n{chenrufeng, blzheng, zhangh, chenquan, cgyan}@{hdu.edu.cn}\n{g.slabaugh, shanxin.yuan}@{qmul.ac.uk}\nAbstract\nReconstructing a High Dynamic Range (HDR) image from\nseveral Low Dynamic Range (LDR) images with different\nexposures is a challenging task, especially in the presence\nof camera and object motion. Though existing models us-\ning convolutional neural networks (CNNs) have made great\nprogress, challenges still exist, e.g., ghosting artifacts. Trans-\nformers, originating from the field of natural language pro-\ncessing, have shown success in computer vision tasks, due to\ntheir ability to address a large receptive field even within a\nsingle layer. In this paper, we propose a transformer model\nfor HDR imaging. Our pipeline includes three steps: align-\nment, fusion, and reconstruction. The key component is the\nHDR transformer module. Through experiments and ablation\nstudies, we demonstrate that our model outperforms the state-\nof-the-art by large margins on several popular public datasets.\nIntroduction\nDynamic range is used to define the ability of the camera\nto capture a range of brightness, usually between the low-\nest and highest values of the same image. Scenes with a\nlarge differences in lighting may pose a challenge to capture.\nIf the dynamic range is not large enough and the illumina-\ntion is too bright, an overexposed image will be produced;\nand if the scene is too dark, the image will appear under-\nexposed. Both over- and under-exposure will lead to loss of\ndetails in the image. While most sensors can record 8-bit,\n10-bit, or slightly higher depth images, those that can record\n16-bit depth images are too expensive to be widely used in\neveryday devices, and standard displays only support 8 bit\nprompting the need for HDR imaging (Meylan, Daly, and\nS¨usstrunk 2006; Dong et al. 2021).\nInitial work performing high dynamic range restoration\nusing a single LDR image (An, Ha, and Cho 2012; Aky ¨uz\net al. 2007; Banterle et al. 2007; Huo et al. 2014; Rempel\net al. 2007; Eilertsen et al. 2017; Endo, Kanamori, and Mi-\ntani 2017; Lee, An, and Kang 2018; Zheng et al. 2022b)\nshowed the dynamic range of the image can be extended,\nbut the under- or over-exposed regions are unrecoverable.\nTherefore researchers began to explore using multiple LDR\nimages at different exposures (e.g. short, medium, long).\n*Corresponding author: Bolun Zheng and Hua Zhang.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nGT and Our Result\nInput LDRs\nGT\nEV=0.0\nEV=2.0\nSen\n Kalantari\n GT\nOurs\nHDR-GAN\nHDRRNN\nDeepHDR\n AHDR\nPatches\nOur ResultEV=4.0\nFigure 1: Three LDR images with different exposures are\nlocated on the left side. The image in the middle is our result\nand ground-truth (GT). EV denotes exposure value, which is\ndetermined by the exposure time, ISO and f-number.\nThe task is to synthesize a single HDR image that preserves\nthe details of the scene using multiple LDR images (De-\nbevec and Malik 2008; Jacobs, Loscos, and Ward 2008;\nReinhard et al. 2010; Sen et al. 2012; Dai et al. 2021; Li\net al. 2022).\nHowever, capturing multiple low dynamic range images\nat different exposures poses some challenges. This typically\nrequires capturing multiple exposures at different times us-\ning a single sensor. However motion due to the camera or ob-\njects in the scene will result in misalignment between LDR\nimages. If unaddressed, this misalignment will result in ob-\nvious ghosting artifacts (Bogoni 2000; Li et al. 2020; Ma\net al. 2017; Zheng et al. 2019) in the merged HDR image.\nTo solve this problem, many recurrent networks and\nlightweight networks have been proposed, such as AHDR-\nNet, HDRRNN, Kalantari, NHDRRNet (Yan et al. 2019;\nPrabhakar, Agrawal, and Babu 2021; Kalantari, Ramamoor-\nthi et al. 2017; Yan et al. 2020). All of these models aim\nto build higher-performing architecture, and follow a simi-\nlar design of LDR CNN-based alignment and fusion to re-\nconstruct the HDR image. At present, the proposed methods\nare aimed at the alignment between images, the reconstruc-\ntion of HDR images, and the use of various model structures\nof recurrent neural networks through attention orientation,\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n340\nbut they cannot handle the task of LDR-to-HDR well as un-\nresolved motion results in ghosting artifacts, blurring, and\ncolor defects. Due to the specific nature of this task, using\ntransformer (Vaswani et al. 2017; Dosovitskiy et al. 2020),\nwhich has recently received much attention in computer vi-\nsion, can be difficult due to hardware and GPU memory lim-\nitations. However, traditional convolutional neural network\nthemselves have limitations in terms of receptive field.\nIn order to solve the above problems, this paper proposes a\nmulti-exposure LDR-to-HDR converter called HDR Fusion\nTransformer (HFT), which uses a transformer to capture\nlong-range context dependence while ensuring the support\nof hardware devices. Notably, it uses a “CNN+Transformer”\narchitecture. Specifically, HFT can be divided into three\nparts: a Shallow Feature Alignment (SFA), a Pyramid Fu-\nsion Module (PFM), and an Image Reconstruction Module\n(IRM). For SFA, more attention is paid to reducing the depth\nof the features of the middle layer and using Deformable\nConvolution (DCN) (Dai et al. 2017; Zhao et al. 2021) to\ncorrect the problem of alignment between images. In the\nPFM, a HDR Fusion Module (HFM) is used in three scales\nand HDR Transformer (HT) is used in the smallest scale,\nwhich simultaneously removes the decoder part of tradi-\ntional transformer to reduce GPU memory consumption by\nusing a multi-head attention mechanism. It is worth noting\nthat HT takes into account the global information of the im-\nage, and it has a much larger receptive field than conven-\ntional convolution and can extract more useful contextual\ninformation. Therefore, HFT can effectively repair the fused\ndefects through long-distance features after image fusion,\nmaking it more competitive. For image reconstruction, an\nnovel Channel Attention Dilated Block (CADB) is proposed\nas the basic feature extraction unit, which can adaptively ad-\njust the weight of each channel to eliminate the ghost caused\nby misalignment. The main contributions are as follows:\n• We propose a new Pyramid Fusion Module (PFM) with\nTransformer. The HDR Fusion Module (HFM) fuses the\nhigher scale features; while the smallest scale features are\nfused with Self-Attention Fusion (SAF) which includes a\nlightweight transformer. With this approach, the features\ncan be fused with less computation and according to the\nglobal information.\n• We propose a Channel Attention Dilated Block (CADB)\nto reduce ghosting artifacts.\n• We propose HDR Fusion Transformer (HFT), which can\nbetter learn non-local features for the HDR fusion.\nRelated Work\nCNN-Based HDR Models\nCNNs have been widely use for image restoration (Zhao\net al. 2021; Liu et al. 2020b,a; Isobe et al. 2020). Recently,\nmany CNN-based models have been proposed for HDR.\nFor example, Kalantari et al. (Kalantari, Ramamoorthi et al.\n2017) use an optical flow algorithm to compensate for mo-\ntion and merge the resulting images using a simple CNN.\nADNet (Liu et al. 2021) was proposed to align the dynamic\nframes with a deformable alignment module. Wu et al. (Wu\net al. 2018) use homographies to align the background mo-\ntion prior to fusion. Yanet al. use spatial attention to rule out\nmisaligned components and build a deep convolutional neu-\nral network to merge features. Prabhakar et al. (Prabhakar,\nAgrawal, and Babu 2021) propose a scalable CNN architec-\nture to efficiently handle the varying LDR inputs. In addi-\ntion, earlier work by Prabhakar et al. (Prabhakar et al. 2020)\nuses the optical flow of aligned images prior to fusion. Niuet\nal. (Niu et al. 2021) use a GAN to create images and video\nfrom the adjustable part of a data stream based on an event\ncamera. Zhang et al. (Zhang and Lalonde 2017) use a depth\nself coding architecture to regress linear and high dynamic\nrange panoramic images from nonlinear, saturated and low\ndynamic range panoramic images.\nVision Transformer\nTransformers, which started out in natural language pro-\ncessing, have made a major breakthrough in NLP, lead-\ning the computer vision community to consider their ap-\nplication. Transformer’s core idea is the multi-head self-\nattention mechanism, which can capture long-range infor-\nmation without the limitation of narrow receptive field in\ntraditional CNNs. The pioneering work of the Vision Trans-\nformer (ViT) demonstrated the potential of transformers to\nreplace traditional CNNs, by representing 2D image fea-\ntures into a one-dimensional sequence, which can be fed\ninto a Transformer. Transformers have been fully used in\nimage classification (Li et al. 2021; Touvron et al. 2021), tar-\nget detection (Carion et al. 2020; Wang et al. 2022), super-\nresolution (Lu et al. 2022; Liang et al. 2021; Yang et al.\n2020) and other tasks (Qu et al. 2022; Bai et al. 2022; Liu\net al. 2022b,a, 2023). However, in the LDR-to-HDR task,\ntransformers have not yet been applied due to the limita-\ntion of hardware devices and insufficient GPU memory. Our\ngoal was to develop an effective HDR Fusion Transformer\nfor LDR-to-HDR reconstruction.\nProposed Method\nAs shown in Figure 2, the HDR Fusion Transformer (HFT)\nis mainly composed of three parts: Shallow Feature Align-\nment (SFA), the Pyramid Fusion Module (PFM), and the Im-\nage Reconstruction Module (IRM). We define [L−1,L0,L1]\nand H as the input and output, whereL0 is the reference im-\nage, L−1,L0 and L1 are the supporting images. In SFA, the\nfeatures of the two supporting images are aligned with the\nfeatures of the reference image.\nFi = SFA(Lr, Li) (1)\nwhere Lr denotes the reference image, Li denotes the sup-\nporting image, SFA denotes the shallow feature alignment\nlayer. Fi is the extracted shallow aligned feature, which is\nthen used as the input to the PFM module.\nP = PFM(F−1, F0, F1) (2)\nwhere P denotes the fused features, which are are sent to the\nIRM for HDR image reconstruction.\nH = IRM(P) (3)\n341\nSAF\nSAF\n+\n+\nPFM\nC\nC\nIRM\nSFA\nSFA\nSFA\nA\nlignment\nFusion\nR\neconstruction\n+\n+\n+\nSupervised\nAdvanced Sobel \nLoss (ASL)\nHFM\nHFM\nHFM\n+\n+\nCADB\nn\nCADB\n1\nCADB\n2\nCADB\nn\nCADB\n1\nCADB\n2\nDCB\n1\nCAB\nDCB\nn\nC\nC\nDCB\n2\nCADB\n1x1 Conv\n3x3 Conv\nDilated \nConv\nDeformable\nConv \nPixel\n-\nShuffle\nAvg\n-\nPool\nConCat\nAdd\nMultiply\nC\nC\n+\n+\nSFA\nC\nC\nFigure 2: The architecture of the proposed HDR Fusion Transformer (HFT). Among them, SFA, PFM, IRM and CADB stand\nfor the Shallow Feature Alignment, Pyramid Fusion Module, Image Reconstruction Module and Channel Attention Dilated\nBlock, respectively.\nShallow Feature Alignment\nThe Shallow Feature Alignment (SFA) module aligns the\nfeatures of Lr and Li through deformable convolution (Liu\net al. 2021). Due to the motion between the reference image\nand supporting image, it is necessary to align the features to\nminimize ghosting effects.\ny (p0) =\nX\npn∈R\nw (pn) · x (p0 + pn) (4)\nwhere y is the result of the convolution, pn is the nth pixel\nand w is the convolutional kernel, x is the input feature and\nR = {(−1, −1), (−1, 0), ...,(0, 1), (1, 1)} denotes the sam-\npling area at a shifted pixel. However, the traditional con-\nvolutions as shown in Eq. 4 are limited by the size of re-\nceptive field, which struggles with longer range dependen-\ncies. Therefore, we add a learnable offset∆pn to learn more\ncomplete information. The convolution model with the off-\nset can be expressed as:\ny (p0) =\nX\npn∈R\nw (pn) · x (p0 + pn + ∆pn) (5)\nwhere ∆pn denotes the offset to be learned.\nAfter the initial alignment, the aligned features are con-\ncatenated with Lr. The concatenated features are sent to\nPFM.\nPyramid Fusion Module\nThe second difficulty in synthesizing an HDR image is to\ncombine the features of three different exposures. Therefore,\nwe propose a new pyramid model (PFM), which is more ca-\npable of high quality fusion than other models.\nIn HFT, patches surrounding the image can be used as a\nreference image, so that the real details of the current im-\nage block can be fused using the reference features around\nFigure 3: The architecture of the proposed modeule Self-\nAttention Fusion (SAF) in PFM, which is composed of the\nHDR Fusion Module and HDR Transformer, respectively.\nThe bottom in the figure is Multi-Head Self Attention.\nthe image. Due to the large size of the image, we adopt a\nmulti-scale fusion mechanism, which can effectively refer-\nence information around the image. In addition, although\ntraditional transformers such as ViT have been applied in\nthe field of computer vision, there is no transformer suitable\nfor HDR reconstruction because of the high GPU memory\nrequired. In this paper, an novel lightweight Self-Attention\nFusion (SAF) module based on transformer is proposed, and\nits effectiveness is proved.\nFigure 2 shows the architecture of the PFM. The input fea-\nture Fi is downsampled to a smaller scale, andFi is fused to\nthe feature through the HFM module. Between two different\nscales, the input features are down-sampled through average\npooling, the processed features (through HFM or SAF) up-\nsampled through bicubic interpolation. After up-sampling,\nthe feature of the smaller scale Si is added to the feature\nBi−1 of the larger scale, then the combined features are sent\nto deformable convolution. HFM and SAF are used for fu-\nsion in the first three scales and the last scale, respectively.\n342\nThe main purpose of SAF is to refine the fused feature\nby capturing the long-range features after image fusion, to\nachieve the best fusion effect. Due to the computational cost,\nthe transformer is only used in the coarsest scale. We im-\nprove the original transformer as follows: we 1) remove the\ndecoder part of the transformer, 2) simplify the encoder part,\nand 3) use only features at the bottleneck in the transformer\nand retain these features.\nHDR Fusion Module (HFM) The HFM module’s struc-\nture is shown in Figure 3. The main purpose of HFM is to\ninitially fuse the features of different exposed images.\nThe outputs of SFAs are taken as the input of HFM in the\nfirst scale. In HFM, F0 is concatenated with Fi (i = −1, 1)\nrespectively, and then the concatenated features are sent into\nthe convolution layer. We speculate that exposure is affected\nby ambient brightness and belongs to global information.\nTherefore, in order to capture the missing information of the\nreference image, we multiply Fi and the convolution fea-\ntures to Fi obtaining the refined feature Ft\ni . After the Ft\n−1\nand Ft\n1 are concatenate with F0, they enter the convolution\nlayer, and Ha is the output of HFM.\nFt\ni = Fi · Conv(Cat(Fi, F0))|i=−1,1 (6)\nHa = Conv(Cat(Ft\n−1, F0, Ft\n1)) (7)\nHDR Transformer (HT) ViT divides the two-\ndimensional image into several small patches and combines\nthem into a one-dimensional representation. This allows\nthe transformer to be applied to visual tasks, but has some\ndisadvantages such as a large demand for training data and\na large amount of calculation.\nInspired by ESRT (Lu et al. 2022), we apply an unfold\noperation to the feature map in HDR-Transformer, as shown\nin the upper right corner of Figure 3. This turns the original\ntwo-dimensional features into a one-dimensional sequence\nHo. Considering the shortcomings of VIT (Yu et al. 2022),\nwe focus on reducing the computational burden and com-\nplexity of the transformer in the visual domain. We directly\nmap the features into 1D features, which greatly reduces the\ncomputation and ensures that the hardware can support sub-\nsequent processing. And when used at the smallest scale, it\ncan ensure that the GPU memory can meet its needs. Then,\nthe method of up-sampling using a pixel-shuffle, residual\nstructure and deformable convolution is used to ensure that\nsufficient information can be retained, so that satisfactory\nfeature information can be obtained.\nThe traditional transformer requires heavy computation.\nFor the sake of simplicity and efficiency, we remove the\ndecoder part in our HT, instead only retaining the encoder\nstructure as shown in the bottom right of Figure 3. This plot\nshows the multi-head self-attention module, layer normal-\nization, and MLP. Although batch normalization has bene-\nficial effects when dealing with two-dimensional features,\nlayer normalization is preferred after the two-dimensional\nfeatures are compressed into one dimension.\nIn the HDR-Transformer, the 2D features are transformed\ninto 1D sequential features through the unfolding operation\nand become Hn. As shown in Figure 3,Hn go through layer\nnormalization and MLP layer to obtain three categories of\nQ, K, V , and each category was segmented into N pieces\nagain. which enters the multi-head self-attention module.\nThese serve as inputs for the scaled dot-product atten-\ntion module. The output Hm is multiplied by Q and K and\nthen multiplied by V . After passing through a concatena-\ntion, Hm are output of the multi-head self-attention module\nthrough the full connection layer. After residual operations\nare performed on Hn and Hm in the HT,then go through\nlayer normalization and MLP layer, and then perform resid-\nual operation with the previous residual result to obtain Hs.\nThe one-dimensional feature Hs output from HT is folded\nto the two-dimensional feature of the corresponding size im-\nage. The specific formula is as follows.\nQ, K, V= MLP(Norm(Unfold(Ha))) (8)\nSv = MLP(SoftMax(Q · KT ) · V ) + Hn (9)\nHs = Fold(MLP(Norm(Sv)) + Sv) (10)\nImage Reconstruction Module\nAfter the initial alignment of features, although the ghosting\nis greatly reduced, it is inevitable that there will be local mis-\nalignments that may result in residual ghosting. At the same\ntime, the high dynamic range image must be reconstructed.\nImage Reconstruction Module (IRM) is designed to address\nthese challenges.\nAs shown in Figure 2, the IRM is composed of several\nChannel Attention Dilated Block (CADB) modules whose\npurpose is to eliminate the small amount of ghosting caused\nby residual misalignment and reconstruct the HDR image.\nIn the process of training, HFT cannot effectively distin-\nguish the ghosting caused by feature misalignment from the\naligned part of the real scene. CADB can effectively reduce\nthe influence of the features learned from the ghosting in the\nmodel at the level of feature channel, so that it can eliminate\nthe ghosting that cannot be solved through alignment to the\ngreatest extent.\nThe CADB structure is shown in the bottom of Figure 2,\nwith Hs as the input. First, global pooling is carried out to\nobtain the weight of channels at each layer, and then feature\nextraction is performed. The extracted feature H is used as\nthe weight of channels at each layer and attached to Hs to\nachieve the effect of reducing the weight of virtual shadows\nand obtain ghost-free featureHw. The formula is as follows:\nHw = Conv(AvgPool(Hs)) ∗ Hs (11)\nThen, the features are sent into several dilated residual mod-\nules. Due to the small receptive field of the common convo-\nlution layer, some local patches of the HDR image require a\nlarge range of information for reconstruction. Therefore, di-\nlated convolution is used for concatenation after traditional\nconvolution is applied, and helps make full use of local and\nnon-local feature information. This effectively expands the\nreceptive field and better reconstructs the details of under-\nand over-exposed regions producing high quality results.\n343\nExperiments\nTraining Loss\nSince HDR images are usually displayed after tone mapping,\nit is more effective to train the network on a tone mapped im-\nage than directly in the HDR domain. Given the HDR image\nH in the HDR domain, we use the µ-law to compress H\nwithin the range of [0,1], with µ=5000.\nT (H) = log(1 + µH)\nlog(1 + H) (12)\nwhere µ is the parameter that defines the amount of com-\npression, and T (H) represents the tone mapped image.\nTo train our HFT, we adopt the L1 loss as the base loss\nfunction. However, as the L1 loss is a point-wise loss, it does\nnot capture edge information important particularly to min-\nimize ghosting in the HDR reconstruction. Following the\nnovel training strategy of CNN (Zheng et al. 2020, 2021,\n2022a), we adopt the Advanced Sobel Loss (ASL) and com-\nbine it with the L1 loss to formulate the loss function for to\nenhance the edge information, which can be expressed as:\nLoss( bZ, Z) = L1( bZ, Z) + 1\nN\nPi\nN ASLi\n\u0010\nbZ, Z\n\u0011\n(13)\nwhere bZ and Z represent the generated HDR image and the\nground truth (GT) image respectively, L1 represents the L1\nLoss, N represents the number of Sobel Loss kernels, and\nASL represents Advanced Sobel Loss function. Here four\nconvolution kernels are used to optimize edge information\non bZ and Z in four directions. The specific optimization pro-\ncess is as follows:\nASLi\n\u0010\nbZ, Z\n\u0011\n= L1(Ki( bZ), Ki(Z)) (14)\nwhere Ki denotes Sobel loss kernel (Vincent, Folorunso\net al. 2009; Gao et al. 2010).\nImplementation and Details\nDue to the multi-scale architecture, the model must be a mul-\ntiple of 16, so we zero-pad the image as necessary. A 64-\nchannel, 3×3 convolution kernel is used in the Conv layer.\nWe use an Adam optimizer, with the initial learning rate set\nto 10−4. LDR images and corresponding images were split\ninto patches of 128×128 size for training, however valida-\ntion and test images were full resolution. In order to avoid\nover-fitting in the training stage, patches were randomly ro-\ntated for data augmentation. During training, we measured\nthe validation set at using PSNR-µ. If the model perfor-\nmance was not improved after five epochs, the learning rate\nis halved. When the learning rate is less than10−6, the train-\ning ends. We implemented our HFT using Pytorch on single\nNVIDIA RTX3090 GPU.\nComparisons with Advanced HDR Models\nDatasets and Metrics Kalantari’s dataset is used as the\nbasic training data set. All models are trained on this\ndataset.1 In addition, we performed supplementary experi-\nments using the Prabhakar’s dataset (Prabhakar et al. 2019)\n1https://cseweb.ucsd.edu/ viscomp/projects/SIG17HDR/\nInput LDRs  Our Result  LDRs Patches\nGTOurs\nHDR-GANAHDR\n Sen\n DeepHDR\nSPD-MEF\n Kalantari\n HDRRNN\nFigure 4: Visual comparisons on the testing data from Kalan-\ntari’s dataset. We compare the zoomed-in local areas of the\nHDR images generated by our method with seven other\nmethods, namely Sen, SPD-MEF, Kalantari, DeepHDR,\nAHDR, HDRRNN and HDR-GAN.\nto prove the generalization of the proposed model. Tur-\nsun’s (Tursun et al. 2016) dataset are widely used in the re-\nlated studies, which is a dataset without ground true. The\nPSNR and SSIM of predicted HDR images were measured\nin the linear domain (-L) and HDR domain (-µ) for quanti-\ntative evaluation. We also used HDR-VDP-2 (Mantiuk et al.\n2011; Marnerides et al. 2018) as another indicator in the\ncomparison.\nWe compare our results with previous state-of-the-art\nmethods, including three patch-based methods Sen, Hu,\nSPD-MEF (Sen et al. 2012; Hu et al. 2013; Ma et al. 2017)\nand eight CNN based methods, Kalantari (Kalantari, Ra-\nmamoorthi et al. 2017), DeepHDR (Wu et al. 2018), AH-\nDRNet (Yan et al. 2019), HDR-GAN (Niu et al. 2021),\nPrabhakar (Prabhakar et al. 2020), HFNet (Xiong and Chen\n2021), HDRRNN (Prabhakar, Agrawal, and Babu 2021).\nWe generated Sen, SPD-MEF, DeepHDR, AHDRNet, HDR-\nRNN, HDR-GAN results and compared them, and we repli-\ncated them for all other methods (if open source was avail-\nable). For methods without publicly available code we did\nnot reproduce their results and instead use the results re-\nported in their papers. The HDR-VDP-2 score is only for\nreference because it may vary depending on the parame-\nters assessed and is not described in the details of the paper.\nQuantitative evaluation was calculated for five indicators.\nQuantitative evaluations: as shown in the Table 1, our\nHFT achieves excellent performance across all indicators,\nand reaches a new SOTA on PSNR-µ, PSNR-Land SSIM-µ.\nBased on Kalantari’s benchmark dataset, the performance of\nHFT is qualitatively compared with other models. As shown\nin Figure 4, our experimental results show that HFT is better\nthan other methods in reconstructing the dynamic range in-\ncluding darker and brighter regions, better preserving details\nand colors more realistically matching the ground truth.\nExperiments on Additional Datasets In addition to\nKalantari’s dataset, we also performed experiments on Prab-\n344\nMethods Publication Quantitative Results Computational Costs\nPSNR-µ PSNR-L SSIM-µ SSIM-L HDR-VDR-2 Params(M) Time(s)\nSen TOG 2012 40.95 38.31 0.982 0.972 56.72 - 73.41\nHu CVPR 2013 32.18 31.88 0.970 0.969 55.24 - 103.57\nSPD-MEF TIP 2017 43.34 40.77 0.986 0.986 61.84 - 13.29\nKalantari TOG 2017 42.74 41.22 0.988 0.985 60.51 - -\nDeepHDR ECCV 2018 41.65 40.86 0.986 0.986 61.21 13.57 0.28\nAHDRNet CVPR 2019 43.62 41.03 0.990 0.989 62.30 1.24 0.94\nPrabhakar ECCV 2020 43.08 41.68 - - 62.21 - -\nHDR-GAN TIP 2021 43.92 41.57 0.991 0.987 65.45 - -\nHFNet ACM MM 2021 44.28 41.48 - - 62.33 2.70 -\nHDRRNN TCI 2021 42.82 41.68 0.990 0.990 - - 0.47\nHFT - 44.45 42.14 0.992 0.988 66.32 4.19 0.10\nTable 1: Quantitative results of our HFT method compared with other advanced methods on Kalantari’s dataset.\nInput LDRs\n Our Result\n LDRs Patches\nAHDR\n Kalantari\n HDRRNN\n GT\nOurs\nHDR-GAN\nSPD-MEF\nFigure 5: Visual comparison on Prabhakar’s dataset.\nhakar’s dataset2. Quantitative results are shown in Table 2\nand qualitative results are shown in Figure 5 and Figure 6.\n‘Our Cross Result’ denotes the model is trained on Kalan-\ntari’s dataset. The results on Prabhakar’s dataset are similar\nto those on Kalantari’s dataset, with our method outperform-\ning others.\nAdditionally, we also provide the results of all compared\nresults on our hand-captured images in Figure 7. From the\nvisual comparison, only our method is able to produce a\nghosting-free result, while the ghosting effects more or less\nexist in results produced by the other compared methods.\nAblation Study\nWe verify the key parts of the HFT model, including (1) the\ninitial alignment is improved by using the SFA module com-\npared with concatenating the two LDR images directly; (2)\nthe use of the SAF module in the fourth scale of PFM in-\nstead of the original HFM module; and (3) CADB has the\nadvantage over ordinary DRDB in the IRM module. We also\n2https://val.cds.iisc.ac.in/HDR/ICCP19/, MIT License\nGT\nHDR-GAN\nOur Cross Result\nHDRRNN\n Ours\nKalantari\nSPD-MEF\n AHDR\nI n put LDRs Our Result LDRs Patc h es\nFigure 6: Visual comparison on Prabhakar’s dataset.\nexplore the number of CADB in IRM required, and the gap\nbetween the L1 and DSL in the loss function.\nAs shown in Table 3, we quantitatively compared the ef-\nfects of the three modules relative to the whole model, list-\ning eight cases respectively in Kalantrai’s and Prabhakar’s\ndataset. The SFA, HT, and CADB are all required to achieve\n345\nOurs\nKalantari\nHDRRNN\nSPD-MEFSen\nInput-3Input-2Input-1\nAHDR\nFigure 7: Visual comparison on our hand-captured image.\nMethods Quantitative Results\nPSNR-µ PSNR-L SSIM-µ SSIM-L\nKalantari 35.63 32.50 0.961 0.969\nDeepHDR 38.03 34.40 0.971 0.977\nAHDR 38.65 35.28 0.973 0.980\nSCHDR 36.08 32.74 0.959 0.967\nPrabhakar 38.30 34.98 0.970 0.978\nHDRRNN 39.03 36.38 0.975 0.983\nHFT 40.12 37.64 0.971 0.987\nTable 2: Quantitative comparison on Prabhakar’s dataset.\nthe best results. Figure 8 provides visual results of our ab-\nlation studies. We compared all combinations of the three\nmodules to objectively demonstrate the role of each module\nand prove its value.\nLoss Function Table 4 shows the results of using the\nKalantari’s dataset on several IRM modules using different\nloss functions (L1 or ASL). As shown in Table 4, we con-\nducted quantitative ablation experiments on the number of\nCADB in IRM and the loss function used. Under the condi-\ntion that other parameters remain unchanged, the ASL loss\nHFT\nHFT\nPFM+CADB\nPFM+CADB\nSFA+CADB\nSFA+CADB\nSFA+PFM\nSFA+PFM\nPFM\nPFM\nSFA\nSFA\nCADB\nCADB\n-\n-\nG T\nG T\nFigure 8: The comparison of seven ablation experiments\nwith GT, HFT’s result and all comparison methods in abla-\ntion experiments. The lower part of the image indicates the\nincluded module.\noutperforms the L1 loss, and the model achieves the best re-\nsults when the number of CADBs is 3. Qualitative results\nshown in Figure 9 demonstrate that compared to the L1 loss,\nthe ASL better preserves details and more faithfully recon-\nstructs colors.\nShallow Feature Alignment The main purpose of Shal-\nlow Feature Alignment module is to better align the moving\nimage to the reference image and reduce ghosting as much\nas possible in the initial stage. We perform an ablation study\nby removing the SFA and replacing it with an ordinary con-\ncatenation of the two input feature maps.\nHDR Transformer The purpose of using HDR Trans-\nformer at the smallest scale is to repair the defects of\nthe fused features with long-distance information. We con-\nducted an ablation study which removes the HT at the mini-\nmum scale.\nChannel Attention Dilated Block Our IRM module is\ncomposed of several CADBs, which reconstruct the HDR\nimage and eliminate ghosting. We performed a study to ex-\nplore the number of CADBs on the performance. As shown\nin Figure 10, we compare the changes of features when they\npass through CADB, and intuitively show that the CADB is\nsuccessful in eliminating ghosting artifacts which caused by\nmisalignment of features.\nConclusion\nIn this paper, we propose a combined CNN and Transformer\nmodel for end-to-end HDR generation. Input multi-exposure\nLDR images are combined to produce a high quality HDR\n346\nModules Kalantari’s Prabhakar’s\nSFA HT CADB PSNR-µ PSNR-L SSIM-µ SSIM-L PSNR-µ PSNR-L SSIM-µ SSIM-L\n- - - 44.06 41.83 0.991 0.987 39.03 36.60 0.970 0.981\n- ✓ - 44.19 41.90 0.991 0.988 39.45 37.31 0.971 0.983\n✓ - - 44.23 41.71 0.991 0.988 39.14 36.86 0.970 0.980\n- - ✓ 44.15 41.75 0.991 0.988 39.67 37.34 0.971 0.986\n- ✓ ✓ 44.15 41.69 0.991 0.987 40.07 37.63 0.971 0.986\n✓ - ✓ 44.04 41.76 0.991 0.988 39.81 37.33 0.971 0.987\n✓ ✓ - 44.26 41.70 0.991 0.987 39.53 37.48 0.971 0.984\n✓ ✓ ✓ 44.45 42.14 0.992 0.988 40.15 37.75 0.971 0.987\nTable 3: Ablation experiments on the Kalantari’s and Prabhakar’s dataset were performed to compare the effects of using three\nmodules (SFA, HT, and CADB).\nG T\n A SL\n L1Loss\nFigure 9: ASL loss compared to L1 loss with the ground\ntruth as reference. Obviously, the model with ASL preserves\nthe edge details and color better.\nLDR inputs\nFeature before\nCADB\nFeature after\nCADBFigure 10: The three columns pf images on the left are\nthe same position with different exposures in three different\ndatasets(Tursun et al. 2016; Prabhakar et al. 2019; Kalantari,\nRamamoorthi et al. 2017) respectively. The two columns of\nimages on the right are the features before and after CADB.\nLoss & N Quantitative Results\nPSNR-µ PSNR-L SSIM-µ SSIM-L\nASL & 3 44.45 42.14 0.992 0.988\nASL & 2 43.18 41.82 0.991 0.988\nASL & 1 44.35 41.94 0.992 0.988\nL1 & 3 43.58 41.38 0.990 0.987\nL1 & 2 44.04 41.60 0.991 0.987\nL1 & 1 42.10 40.64 0.990 0.986\nTable 4: Quantitative comparison of using two loss functions\n(ASL and L1) on model performance, where N denotes the\nnumber of CADB in IRM.\nimage with preserved details, colors and minimal ghost-\ning. To achieve this, we introduced a module for prelim-\ninary alignment (SFA). Then we propose PFM, in which\na transformer is used for the first time in the HDR prob-\nlem to learn the discontinuous information of features at\ndifferent scales. Finally, we proposed the IRM to recon-\nstruct the HDR features with minimal ghosting. Experiments\nshow that our approach has strong performance and pro-\nduces high quality HDR images. Source code for HFT is\navailable at https://github.com/Chenrf1121/HFT\nAcknowledgements\nThis work is supported by the National Nature Science\nFoundation of China (U21B2024, 62001146). This work\nis also supported by the Fundamental Research Funds\nfor the Provincial Universities of Zhejiang under Grants\nGK229909299001-009, and the National Nature Science\nFoundation of China 62271180.\nReferences\nAky¨uz, A. O.; Fleming, R.; Riecke, B. E.; Reinhard, E.; and\nB¨ulthoff, H. H. 2007. Do HDR displays support LDR content? A\npsychophysical evaluation. ACM Transactions on Graphics (TOG),\n26(3): 38–es.\n347\nAn, J.; Ha, S. J.; and Cho, N. I. 2012. Reduction of ghost effect\nin exposure fusion by detecting the ghost pixels in saturated and\nnon-saturated regions. In 2012 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 1101–1104.\nIEEE.\nBai, Y .; Yang, X.; Liu, X.; Jiang, J.; Wang, Y .; Ji, X.; and Gao, W.\n2022. Towards End-to-End Image Compression and Analysis with\nTransformers. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, 104–112.\nBanterle, F.; Ledda, P.; Debattista, K.; Chalmers, A.; and Bloj, M.\n2007. A framework for inverse tone mapping. The Visual Com-\nputer, 23(7): 467–478.\nBogoni, L. 2000. Extending dynamic range of monochrome and\ncolor images through fusion. In Proceedings 15th International\nConference on Pattern Recognition. ICPR-2000, volume 3, 7–12.\nIEEE.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.;\nand Zagoruyko, S. 2020. End-to-end object detection with trans-\nformers. In European conference on computer vision, 213–229.\nSpringer.\nDai, J.; Qi, H.; Xiong, Y .; Li, Y .; Zhang, G.; Hu, H.; and Wei, Y .\n2017. Deformable convolutional networks. In Proceedings of the\nIEEE international conference on computer vision, 764–773.\nDai, T.; Li, W.; Cao, X.; Liu, J.; Jia, X.; Leonardis, A.; Yan, Y .; and\nYuan, S. 2021. Wavelet-Based Network For High Dynamic Range\nImaging. arXiv preprint arXiv:2108.01434.\nDebevec, P. E.; and Malik, J. 2008. Recovering high dynamic\nrange radiance maps from photographs. In ACM SIGGRAPH 2008\nclasses, 1–10.\nDong, X.; Hu, X.; Li, W.; Wang, X.; and Wang, Y . 2021. MIEHDR\nCNN: Main Image Enhancement based Ghost-Free High Dynamic\nRange Imaging using Dual-Lens Systems. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 35, 1264–1272.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.;\nZhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold,\nG.; Gelly, S.; et al. 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929.\nEilertsen, G.; Kronander, J.; Denes, G.; Mantiuk, R. K.; and Unger,\nJ. 2017. HDR image reconstruction from a single exposure using\ndeep CNNs. ACM transactions on graphics (TOG), 36(6): 1–15.\nEndo, Y .; Kanamori, Y .; and Mitani, J. 2017. Deep reverse tone\nmapping. ACM Trans. Graph., 36(6): 177–1.\nGao, W.; Zhang, X.; Yang, L.; and Liu, H. 2010. An improved So-\nbel edge detection. In 2010 3rd International conference on com-\nputer science and information technology, volume 5, 67–71. IEEE.\nHu, J.; Gallo, O.; Pulli, K.; and Sun, X. 2013. HDR deghosting:\nHow to deal with saturation? In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 1163–1170.\nHuo, Y .; Yang, F.; Dong, L.; and Brost, V . 2014. Physiological\ninverse tone mapping based on retina response. The Visual Com-\nputer, 30(5): 507–517.\nIsobe, T.; Li, S.; Jia, X.; Yuan, S.; Slabaugh, G.; Xu, C.; Li, Y .-L.;\nWang, S.; and Tian, Q. 2020. Video super-resolution with temporal\ngroup attention. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 8008–8017.\nJacobs, K.; Loscos, C.; and Ward, G. 2008. Automatic high-\ndynamic range image generation for dynamic scenes. IEEE Com-\nputer Graphics and Applications, 28(2): 84–93.\nKalantari, N. K.; Ramamoorthi, R.; et al. 2017. Deep high dynamic\nrange imaging of dynamic scenes.ACM Trans. Graph., 36(4): 144–\n1.\nLee, S.; An, G. H.; and Kang, S.-J. 2018. Deep recursive hdri: In-\nverse tone mapping using generative adversarial networks. In pro-\nceedings of the European Conference on Computer Vision (ECCV),\n596–611.\nLi, H.; Ma, K.; Yong, H.; and Zhang, L. 2020. Fast multi-scale\nstructural patch decomposition for multi-exposure image fusion.\nIEEE Transactions on Image Processing, 29: 5805–5816.\nLi, W.; Xiao, S.; Dai, T.; Yuan, S.; Wang, T.; Li, C.; and Song,\nF. 2022. SJ-HDˆ 2R: Selective Joint High Dynamic Range\nand Denoising Imaging for Dynamic Scenes. arXiv preprint\narXiv:2206.09611.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L. 2021.\nLocalvit: Bringing locality to vision transformers. arXiv preprint\narXiv:2104.05707.\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and Timofte,\nR. 2021. Swinir: Image restoration using swin transformer. InPro-\nceedings of the IEEE/CVF International Conference on Computer\nVision, 1833–1844.\nLiu, L.; An, J.; Liu, J.; Yuan, S.; Chen, X.; Zhou, W.; Li, H.; Wang,\nY .; and Tian, Q. 2023. Low-Light Video Enhancement with Syn-\nthetic Event Guidance. In Proceedings of the AAAI Conference on\nArtificial Intelligence.\nLiu, L.; Liu, J.; Yuan, S.; Slabaugh, G.; Leonardis, A.; Zhou, W.;\nand Tian, Q. 2020a. Wavelet-based dual-branch network for image\ndemoir´eing. In European Conference on Computer Vision, 86–102.\nSpringer.\nLiu, L.; Xie, L.; Zhang, X.; Yuan, S.; Chen, X.; Zhou, W.; Li, H.;\nand Tian, Q. 2022a. TAPE: Task-Agnostic Prior Embedding for\nImage Restoration. In European Conference on Computer Vision.\nLiu, L.; Yuan, S.; Liu, J.; Bao, L.; Slabaugh, G.; and Tian, Q.\n2020b. Self-adaptively learning to demoir ´e from focused and de-\nfocused image pairs. Advances in Neural Information Processing\nSystems, 33: 22282–22292.\nLiu, L.; Yuan, S.; Liu, J.; Guo, X.; Yan, Y .; and Tian, Q.\n2022b. Siamtrans: zero-shot multi-frame image restoration with\npre-trained siamese transformers. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 36, 1747–1755.\nLiu, Z.; Lin, W.; Li, X.; Rao, Q.; Jiang, T.; Han, M.; Fan, H.; Sun,\nJ.; and Liu, S. 2021. ADNet: Attention-guided deformable con-\nvolutional network for high dynamic range imaging. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 463–470.\nLu, Z.; Li, J.; Liu, H.; Huang, C.; Zhang, L.; and Zeng, T. 2022.\nTransformer for single image super-resolution. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, 457–466.\nMa, K.; Li, H.; Yong, H.; Wang, Z.; Meng, D.; and Zhang, L. 2017.\nRobust multi-exposure image fusion: a structural patch decompo-\nsition approach. IEEE Transactions on Image Processing, 26(5):\n2519–2532.\nMantiuk, R.; Kim, K. J.; Rempel, A. G.; and Heidrich, W. 2011.\nHDR-VDP-2: A calibrated visual metric for visibility and qual-\nity predictions in all luminance conditions. ACM Transactions on\ngraphics (TOG), 30(4): 1–14.\nMarnerides, D.; Bashford-Rogers, T.; Hatchett, J.; and Debattista,\nK. 2018. Expandnet: A deep convolutional neural network for high\ndynamic range expansion from low dynamic range content. In\nComputer Graphics Forum, volume 37, 37–49. Wiley Online Li-\nbrary.\n348\nMeylan, L.; Daly, S.; and S¨usstrunk, S. 2006. The reproduction of\nspecular highlights on high dynamic range displays. In Color and\nImaging Conference, volume 2006, 333–338. Society for Imaging\nScience and Technology.\nNiu, Y .; Wu, J.; Liu, W.; Guo, W.; and Lau, R. W. 2021. Hdr-gan:\nHdr image reconstruction from multi-exposed ldr images with large\nmotions. IEEE Transactions on Image Processing, 30: 3885–3896.\nPrabhakar, K. R.; Agrawal, S.; and Babu, R. V . 2021. Self-gated\nmemory recurrent network for efficient scalable HDR deghosting.\nIEEE Transactions on Computational Imaging, 7: 1228–1239.\nPrabhakar, K. R.; Agrawal, S.; Singh, D. K.; Ashwath, B.; and\nBabu, R. V . 2020. Towards practical and efficient high-resolution\nHDR deghosting with CNN. InEuropean Conference on Computer\nVision, 497–513. Springer.\nPrabhakar, K. R.; Arora, R.; Swaminathan, A.; Singh, K. P.; and\nBabu, R. V . 2019. A fast, scalable, and reliable deghosting method\nfor extreme exposure fusion. In 2019 IEEE International Confer-\nence on Computational Photography (ICCP), 1–8. IEEE.\nQu, L.; Liu, S.; Wang, M.; and Song, Z. 2022. Transmef: A\ntransformer-based multi-exposure image fusion framework using\nself-supervised multi-task learning. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36, 2126–2134.\nReinhard, E.; Heidrich, W.; Debevec, P.; Pattanaik, S.; Ward, G.;\nand Myszkowski, K. 2010. High dynamic range imaging: acquisi-\ntion, display, and image-based lighting. Morgan Kaufmann.\nRempel, A. G.; Trentacoste, M.; Seetzen, H.; Young, H. D.; Hei-\ndrich, W.; Whitehead, L.; and Ward, G. 2007. Ldr2hdr: on-the-fly\nreverse tone mapping of legacy video and photographs.ACM trans-\nactions on graphics (TOG), 26(3): 39–es.\nSen, P.; Kalantari, N. K.; Yaesoubi, M.; Darabi, S.; Goldman, D. B.;\nand Shechtman, E. 2012. Robust patch-based hdr reconstruction of\ndynamic scenes. ACM Trans. Graph., 31(6): 203–1.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.;\nand J ´egou, H. 2021. Training data-efficient image transformers\n& distillation through attention. In International Conference on\nMachine Learning, 10347–10357. PMLR.\nTursun, O. T.; Aky¨uz, A. O.; Erdem, A.; and Erdem, E. 2016. An\nobjective deghosting quality metric for HDR images. In Computer\nGraphics Forum, volume 35, 139–152. Wiley Online Library.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. Advances in neural information processing systems,\n30.\nVincent, O. R.; Folorunso, O.; et al. 2009. A descriptive algorithm\nfor sobel image edge detection. In Proceedings of informing sci-\nence & IT education conference (InSITE), volume 40, 97–107.\nWang, Y .; Zhang, X.; Yang, T.; and Sun, J. 2022. Anchor detr:\nQuery design for transformer-based detector. InProceedings of the\nAAAI Conference on Artificial Intelligence, volume 36, 2567–2575.\nWu, S.; Xu, J.; Tai, Y .-W.; and Tang, C.-K. 2018. Deep high\ndynamic range imaging with large foreground motions. In Pro-\nceedings of the European Conference on Computer Vision (ECCV),\n117–132.\nXiong, P.; and Chen, Y . 2021. Hierarchical Fusion for Practical\nGhost-free High Dynamic Range Imaging. In Proceedings of the\n29th ACM International Conference on Multimedia, 4025–4033.\nYan, Q.; Gong, D.; Shi, Q.; Hengel, A. v. d.; Shen, C.; Reid, I.; and\nZhang, Y . 2019. Attention-guided network for ghost-free high dy-\nnamic range imaging. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 1751–1760.\nYan, Q.; Zhang, L.; Liu, Y .; Zhu, Y .; Sun, J.; Shi, Q.; and Zhang, Y .\n2020. Deep HDR imaging via a non-local network. IEEE Trans-\nactions on Image Processing, 29: 4308–4322.\nYang, F.; Yang, H.; Fu, J.; Lu, H.; and Guo, B. 2020. Learning tex-\nture transformer network for image super-resolution. In Proceed-\nings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 5791–5800.\nYu, F.; Huang, K.; Wang, M.; Cheng, Y .; Chu, W.; and Cui, L. 2022.\nWidth & Depth Pruning for Vision Transformers. In AAAI Confer-\nence on Artificial Intelligence (AAAI), volume 2022.\nZhang, J.; and Lalonde, J.-F. 2017. Learning high dynamic range\nfrom outdoor panoramas. In Proceedings of the IEEE International\nConference on Computer Vision, 4519–4528.\nZhao, H.; Zheng, B.; Yuan, S.; Zhang, H.; Yan, C.; Li, L.; and\nSlabaugh, G. 2021. CBREN: Convolutional Neural Networks for\nConstant Bit Rate Video Quality Enhancement.IEEE Transactions\non Circuits and Systems for Video Technology.\nZheng, B.; Chen, Q.; Yuan, S.; Zhou, X.; Zhang, H.; Zhang, J.; Yan,\nC.; and Slabaugh, G. 2022a. Constrained Predictive Filters for Sin-\ngle Image Bokeh Rendering. IEEE Transactions on Computational\nImaging, 8: 346–357.\nZheng, B.; Chen, Y .; Tian, X.; Zhou, F.; and Liu, X. 2019. Im-\nplicit dual-domain convolutional network for robust color image\ncompression artifact reduction. IEEE Transactions on Circuits and\nSystems for Video Technology, 30(11): 3982–3994.\nZheng, B.; Pan, X.; Zhang, H.; Zhou, X.; Slabaugh, G.; Yan, C.;\nand Yuan, S. 2022b. DomainPlus: Cross-Transform Domain Learn-\ning towards High Dynamic Range Imaging. In Proceedings of the\n30th ACM International Conference on Multimedia, 1–10.\nZheng, B.; Yuan, S.; Slabaugh, G.; and Leonardis, A. 2020. Image\ndemoireing with learnable bandpass filters. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 3636–3645.\nZheng, B.; Yuan, S.; Yan, C.; Tian, X.; Zhang, J.; Sun, Y .; Liu, L.;\nLeonardis, A.; and Slabaugh, G. 2021. Learning frequency domain\npriors for image demoireing. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence.\n349"
}