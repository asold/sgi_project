{
    "title": "Using a Language Model to Generate Music in its Symbolic Domain while Controlling its Perceived Emotion",
    "url": "https://openalex.org/W4378648062",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4378655603",
            "name": "Naomi Imasato",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2168796573",
            "name": "Kazuki Miyazawa",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2141221199",
            "name": "Caitlin Duncan",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2103006713",
            "name": "Takayuki Nagai",
            "affiliations": [
                "Osaka University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6793801364",
        "https://openalex.org/W6781553146",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W6749351710",
        "https://openalex.org/W6755406732",
        "https://openalex.org/W3092879656",
        "https://openalex.org/W6841982715",
        "https://openalex.org/W6776218486",
        "https://openalex.org/W6843330092",
        "https://openalex.org/W6849105126",
        "https://openalex.org/W2078040942",
        "https://openalex.org/W6771522293",
        "https://openalex.org/W6768028577",
        "https://openalex.org/W3031000691",
        "https://openalex.org/W2963104691",
        "https://openalex.org/W4221148975",
        "https://openalex.org/W6790978476",
        "https://openalex.org/W4206232983",
        "https://openalex.org/W6765775151",
        "https://openalex.org/W2898827701",
        "https://openalex.org/W2129069237",
        "https://openalex.org/W2149628368",
        "https://openalex.org/W6810940779",
        "https://openalex.org/W6779823529",
        "https://openalex.org/W6810311916",
        "https://openalex.org/W6755182157",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W6839643428",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W6809885388",
        "https://openalex.org/W6838639034",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W4306679606",
        "https://openalex.org/W3094138986",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6713717318",
        "https://openalex.org/W2046255764",
        "https://openalex.org/W6770160433",
        "https://openalex.org/W6799448330",
        "https://openalex.org/W3036167779",
        "https://openalex.org/W4281485151",
        "https://openalex.org/W3047453285",
        "https://openalex.org/W4318351475",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4226125322",
        "https://openalex.org/W4283388932",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W2792210438",
        "https://openalex.org/W4224035735",
        "https://openalex.org/W4381786045",
        "https://openalex.org/W2997195635",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4226275767",
        "https://openalex.org/W2405190343",
        "https://openalex.org/W4293575120",
        "https://openalex.org/W3152733922",
        "https://openalex.org/W4286859154",
        "https://openalex.org/W4287802874",
        "https://openalex.org/W2959300817",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2950547518",
        "https://openalex.org/W3134753491",
        "https://openalex.org/W2973049837"
    ],
    "abstract": "This work proposes a transformer-based model capable of generating music in its symbolic domain, in a controllable fashion. The ultimate goal of this is to build a system with which people can compose music collaboratively with a computer. Using an NLP model as a base (GPT-2), we take advantage of the similarities across symbolic music representation and written language to build a model capable of conditionally predicting musical sequences. Controllability is achieved without explicit programming for it, and does not require extensive retraining of the model. A study with 939 participants was performed to evaluate this controllability. The results of this suggest the proposed method is indeed effective and can be used to control the generation of music in its symbolic domain. The method itself is flexible to any desired &#x201C;control&#x201D;, but this work focuses specifically on the emotion conveyed when one listens to a piece of music.",
    "full_text": "Received 16 May 2023, accepted 23 May 2023, date of publication 29 May 2023, date of current version 2 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3280603\nUsing a Language Model to Generate Music in Its\nSymbolic Domain While Controlling\nIts Perceived Emotion\nNAOMI IMASATO\n 1, KAZUKI MIYAZAWA\n 1, CAITLIN DUNCAN1,\nAND TAKAYUKI NAGAI\n 1,2, (Member, IEEE)\n1Graduate School of Engineering Science, Osaka University, Osaka 560-8531, Japan\n2Artificial Intelligence Exploration Research Center, The University of Electro-Communications, Tokyo 182-8585, Japan\nCorresponding author: Naomi Imasato (c.imasato@rlg.sys.es.osaka-u.ac.jp)\nThis work was supported in part by the Grant-in-Aid for Scientific Research on Innovative Areas under Grant 20H05565\nand Grant 22H04865, and in part by the Japan Science and Technology Agency (JST) Moonshot Research and Development\nunder Grant JPMJPS2011.\nThis work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and\nprotocols was granted by the Research Ethics Committee, Graduate School of Engineering Science, Osaka University.\nABSTRACT This work proposes a transformer-based model capable of generating music in its symbolic\ndomain, in a controllable fashion. The ultimate goal of this is to build a system with which people can\ncompose music collaboratively with a computer. Using an NLP model as a base (GPT-2), we take advantage\nof the similarities across symbolic music representation and written language to build a model capable of\nconditionally predicting musical sequences. Controllability is achieved without explicit programming for\nit, and does not require extensive retraining of the model. A study with 939 participants was performed to\nevaluate this controllability. The results of this suggest the proposed method is indeed effective and can be\nused to control the generation of music in its symbolic domain. The method itself is flexible to any desired\n‘‘control’’, but this work focuses specifically on the emotion conveyed when one listens to a piece of music.\nINDEX TERMS AI music composition, controlled music generation, deep learning, language model,\nautoregressive model.\nI. INTRODUCTION\nArt creation is one of many activities that human intelligence\nis capable of engaging in. As creators, we find ways of\ncommunicating our thoughts in many sensitive or provoking\nways using a multitude of tools and mediums, as seen in\npoetry, paintings and music, for example. As artists create,\nit is not uncommon to see them seek inspiration in their\nsurroundings, day-to-day life, or even in the creations of\nothers. Said inspirations may build up over time, or serve as\nspontaneous triggers for an artistic creation. The purpose of\nthis work is to help us achieve a creative framework where\nartists can collaboratively work with computers in order to\nproduce art. Specifically, an artist (human) can inform the\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Wai-Keung Fung\n.\ncomputer of their intentions, and the computer will provide\nsuitable suggestions that may help in the creative process.\nIn this work, we focus on using a mood or a sentiment as\nthis intent.\nAlthough interpretations of art are subject to one’s past\nexperiences and cultural background (among many other\naspects), there are certain features that are present in differ-\nent pieces of art that can be almost universally associated\nwith a specific concept. As music is often referred to as\na universal language, where the communication does not\nnecessarily rely on language (lyrics), multiple listeners may\nreact similarly towards a musical piece from the sounds alone.\nAs a language, music communicates via its acoustic features,\nhow the presence or absence of sounds are laid across time,\nand the nuances in how they are played. Analogously, with\nboth written and spoken languages, we put words together to\n52412\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nform sentences to convey our messages, which can change\ndepending on our choice of words and how we arrange, spell\nor enunciate them.\nWith the latest advances made in the field of Natural Lan-\nguage Processing (NLP), namely in the task of text predic-\ntion, we are witnessing the rise of language models capable\nof generating texts that are completely coherent. Rarely do\nthey break grammatical rules, almost as if the text they pro-\nduced was carefully crafted by a person. Not only are these\nmodels capable of constructing well-written text, but some\nresearchers have recently investigated methods to control its\ncontents and nuance so the models can fulfill more specific\ntasks.\nIn this work, we exploit the similarities between symbolic\nmusic and written language to train OpenAI’s GPT-2 to\npredict and generate musical sequences and use the method\nderived from [1] to manipulate the output in generation time\nto be more fitting to an emotion of interest. Here, we use\nonly conveyed emotion in a piece of music as our element\nfor music generation control. However our method is capable\nof any type of control, provided there is enough training data\navailable to support it. On top of that, adding or changing\nthe elements of sampling control does not require extensive\nand time-consuming training steps. In the following sections,\nwe will discuss our motivation, detail the proposed method,\nshare some of the obtained results and, finally, describe the\nexperiment performed to obtain subjective evaluations from\ndifferent people, allowing us to draw some comparisons\nbetween different sampling methods. Lastly, we will refer to\nsome points that may have interfered with the outputs and the\nperformance of our method, as well as some possible future\nwork.\nII. RELATED WORKS\nA. MUSIC EMOTION CLASSIFICATION\nManually classifying and labeling data can be quite the\ntedious job, so automated solutions for these problems were\nmore than welcomed in the industry. Data classification has\nbeen one of the primary tasks tackled by AI researchers since\nits early ages [2], and it still is one of the most common\napplications in this field.\nAccurately classifying emotions and other high-level char-\nacteristics from pieces of music is undoubtedly one of the\nmain objectives in the field of Music Information Retrieval\n(MIR), but admittedly, it also is a non-trivial matter. Not\nonly are these developments essential for data labelling and\nanalysis, but they also play an important role in the field of\nmusic generation, including this work. Even though we use\na simple design for its discriminator (classification) model,\nhigh-level features such as emotions might be too complex\nfor simple and shallow models to infer, especially because\nof the subjectivity that lies in the act of perceiving emotions\nfrom pieces of music [3].\nSome approaches to this problem are based around design-\ning rules based on acoustic features and music theory, in both\nraw audio and symbolic domains [4], [5]. Focusing on the\nsymbolic domain, NLP-based approaches such as LSTM-\nderived models [5], [6] are significantly more popular. As the\ndatasets that have emotional annotations are limited, more\nrecent works favor the use of fine-tuned large language mod-\nels for emotion classification [7], [8], where the LLM is\npre-trained with a more comprehensive dataset, and then it\nis fine-tuned to perform the classification task with fewer\nsamples of the dataset.\nOur main target in this work is to propose a music gen-\neration that is fitting for co-creative systems where humans\ncan easily use machines as a tool for music creation, and\ntherefore, this work is not focused on the classification task\nin itself, even though a classification model plays a big part\nin our proposed method.\nB. VERSATILITY OF LANGUAGE MODELS\nAs we write a piece of text, we select the most appropriate\nwords and put them together so we can communicate our\nmessage to the reader. Analogously, songwriters carefully\nselect chords, beats, notes and melodies that can be arranged\nappropriately to convey a specific emotion, feeling or story to\nthe listener. In both reading and writing, we also hold different\nlevels of expectations towards the use of correct grammar and\northography depending on the author, who the text is targeted\nat, and what the intention of the text is. To a degree, we hold\nsimilar expectations when listening to music from a specific\nartist or genre. In both contexts, writers may also break these\n‘‘expectations’’ (or ‘‘conventions’’, rather) to cause an impact\nor to better express their story (the so-called poetic license).\nProvided the closeness between language and symbolic\nmusic, many researchers in both fields have approached\nthe tasks of text prediction/generation and music compo-\nsition by using similar methods. Specifically for the cur-\nrent study, we used sequence models (autoregressive mod-\nels). Amidst the many types of architectures used to tackle\nsequence learning, one that quickly rose in popularity among\nNLP researchers was the Transformer architecture [9], an\nencoder-decoder structure that solely relies on attention, tak-\ning away the necessity of recurrent networks in sequence\nmodeling. Among the most recent breakthroughs in NLP,\nlarge-scale transformer-based pre-trained models, such as\nOpenAI’s GPT, GPT-2 and GPT-3 [10], [11], have been under\nthe spotlight for being able to produce text that is comparable\nto that written by a person. These models were trained on\na massive amount of text sourced from the web, where the\nstrings of characters are represented by numerical sequences\n(token embeddings) and the task at hand is to predict the next\ntoken given an input sequence. The NLP community soon\nstarted taking advantage of these pre-trained models to solve\na multitude of more specialized language-related problems,\nby fine-tuning them with a few more additional steps of\ntraining, as they already held the capability of generating\ncoherent and cohesive text.\nVOLUME 11, 2023 52413\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nGiven the huge success of transformer-based models with\nlearning text sequences, their usage was then extended\ntowards different modalities, such as with images, videos and\nmusic [12], [13], [14]. Specifically, with the symbolic rep-\nresentation of music, transformer-based models performed\noutstandingly well in maintaining coherence along the gen-\neration, particularly with long-term structures. As shown in\nMusic Transformer[14], where the authors present the upper\nhand transformer-based structures have over an LSTM-based\nRNN architecture (Performance RNN [15]). In addition, this\napproach to the problem of music generation takes away the\nneed to carefully craft the ‘‘rules’’ in music theory in order\nto obtain musically pleasing compositions, while still main-\ntaining enough room for ‘‘creative freedom’’, as there is to\ntext generation. Music Transformershed light on another pos-\nsibility to tackle AI-based music composition, but it mostly\nfocused on completing unfinished musical phrases.\nC. CONTROLLING TEXT GENERATION\nTypically, when generating text, these models are given an\ninput sentence which the user wishes to predict the contin-\nuation for (represented by a sequence of the corresponding\ntokens). However, a given piece of text can be completed in\nmany different ways that can hold a wide range of connota-\ntions. For example, completing the phrase ‘‘this experience\nwas’’ with either ‘‘amazing’’ or ‘‘traumatic’’ is possible, but\nthey both express completely opposing views. Later works\nadjusted the transformer-based models to then accept the\nfeature of control over different attributes, like length, mean-\ning/content and more [16], [17]. The main issue when using\nany of the aforementioned methods is that, if we wish to\ncontrol an attribute that is not yet accounted for in the models\nor if we desire to add a new category for text generation\ncontrol, the entire model has to be trained once more with all\nof the data (accounting for the desired modifications as well).\nThis is time-consuming and sometimes altogether unfeasible\ndepending on the resources available.\nLater research then proposed using different methods for\ntext generation control that do not demand re-training the\nlarge-scale NLP models, one of which is Keyword2Text(K2T)\n[18]. K2T uses keywords as hard and soft constraints for\ntext generation. This means that when given a list of words,\nthese may or may not be included in the generated piece of\ntext, but they dictate the desired meaning or context of the\ngenerated text. This feat is achieved with the help of the fact\nthat the words are embedded in a space where proximity in\nplacement generally implies proximity in meaning or con-\ntext. In other words, if the generative process is conditioned\nusing the word ‘‘delicious’’, terms like ‘‘tasty’’, ‘‘yummy’’\nor ‘‘delicious’’ itself might be sampled. Another work that\ntackles this issue is coined Plug and Play Language Models\n(PPLM) [1], where the method also involves the use of a\npool of keywords (referred to as ‘‘bag of words’’) to guide\nthe text generation, but it also accomplishes said task by\nusing an auxiliary model. This simple discriminator model\nhelps steer the text generation. As opposed to spending days\ntraining the large-scale language model itself, we only need\nto train a small model (which can be accomplished in a few\nhours or less) to employ control over the contents of the text\nbeing generated. Another interesting aspect of PPLM is the\n‘‘stackability’’ of different labels for control, which allows\nhighly specific and sometimes ‘‘creative’’ and unexpected\noutputs.\nD. ART CREATION WITH AI\nArt creation using AI is a broad field that spans many dif-\nferent mediums. The concept of ‘‘art’’ is often immediately\nassociated with the visual arts, with strong imagery of art\ngalleries filled with paintings and sculptures. This can also\nbe said about AI-generated art: among the different existing\nmethods, image generation seems to be the sub-field that\nhas been developing the most in the past few years. Ope-\nnAI’s DALL-E [19] is an autoregressive model (like GPT-\n3) that generates images from text prompts. The model can\n‘‘create’’ images from virtually any text prompt, no mat-\nter how ordinary or absurd that prompt may seem. More\nrecent developments in ‘‘text-to-image’’ autoregressive mod-\nels include DeepMind’s Perceiver AR [20] and Google’s\nParti [21]. Along with DALL-E, OpenAI released CLIP [22],\na text-image alignment scoring system that proved to be quite\nrobust in image captioning and image classification. Still,\nwithin the field of image generation, an existing category\nof generative models gained a lot of attention soon after the\nrise of DALL-E: Diffusion Models [23], [24], [25]. These\nmodels’ sampling processes are based on a repeated applica-\ntion of noise functions, to obtain a noisy representation of a\ndata sample, and then the repeated application of denoising\nfunctions to reconstruct said data sample. It did not take\nlong until researchers coupled diffusion models with different\nguiding methods to bring new ‘‘text-to-image’’ models that\ncan sample high-quality and realistic images from simple text\nprompts, such as seen in GLIDE [26], Imagen [27], DALL-E\n2 [28] and Stable Diffusion [29].\nArguably, the field of music and sound generation is still\nlagging behind the wave of developments we see in image\ngeneration systems. Especially within the domain of raw\naudio generation, most systems are limited to short snippets\nof sounds, as the longer in duration an audio file is, the more\ncomplex the problem of reconstructing it becomes.\nOpenAI’s Jukebox [30] was able to achieve considerably\ngood results with a system that was primarily designed with\nVariational Autoencoders (V AEs) and Transformers. The sys-\ntem can generate music by outputting raw audio files that\ncontain both instruments and vocals. The authors also made it\npossible for users to exert some control over the generation,\nby adding control signals to the data in training. However,\ndespite achieving impressive results, Jukebox’s audio out-\nputs are not completely clear of artifacts and cannot com-\n52414 VOLUME 11, 2023\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\npete against the high-quality counterparts in the field of\nimage generation. A more recent development in the field\nof acoustic generation can be seen in Google Research’s\nMuLan [31] and MusicLM [32], where both systems are\ncapable of generating audio from text descriptions. In the for-\nmer, the authors used two separate transformer-based models\nto perform music (audio) and text embedding separately,\nbut used contrastive learning to create a shared embedding\nspace. In the latter, the authors extended a previous work,\nAudioLM [33], to accept text conditioning. AudioLM bases\nitself on language models, using an autoregressive approach\nfor audio generation. To incorporate the text conditioning\naspect into AudioLM to develop MusicLM, the authors relied\non the shared embedding space in MuLan, resulting in a sys-\ntem capable of generating high-quality audio files (including\nmusic) from any text prompt.\nE. SYMBOLIC MUSIC GENERATION AND AI\nA computer is often not recognized as the ‘‘artist’’ but easily\naccepted as a ‘‘tool’’ for the creation process. The most inter-\nactive and possibly natural way a computer can contribute\nto art creation is by helping people with suggestions as if\nit assumed the position of an auxiliary author. That is only\none possible use for the task of co-creation when people and\nmachines join their ‘‘efforts’’ to make something that can be\nenjoyed by other people.\nMagenta’s MusicV AE [34] approaches this problem by\nintroducing a ‘‘musical palette’’. As the name suggests, here\nthe authors use a V AE to learn and depict a space of melodies,\nwhich are all organized in a way that one can make sense\nof. One interesting application for this approach is the explo-\nration of this latent space and the interpolation between\ntwo or more short melodies. Despite the intuitiveness of the\nspace exploration and the potential for interactive systems\nthat allow human-machine collaborations, the melodies rep-\nresented in the latent space are very short in length, and the\ncreative process might take some time since the exploration\nitself can bring too many possibilities to complete a piece of\nmusic. This type of approach might be more fitting to those\nwho already have a vague idea of what they wish to create,\nwhich might require previous experiences or prior knowledge\nin music composition. Novices may find it fun to play with\ntools that use this ‘‘musical palette’’, but the vastness of the\nlatent space can be quite overwhelming for beginners.\nThe models and pipeline proposed in Music SketchNet [35]\nshows another system that enables co-creation. The work uses\na three-step system where each step is fulfilled by a different\nmodel (in other words, there is a total of 3 models involved in\nthis pipeline). The first model is coined SketchV AE, which\nis a V AE that is trained to take a melody and transpose it\nto music melody tokens, from which both pitch and rhythm\ntokens are extracted. This first step is designed to find the\nappropriate representation of music. The second step uses\nthe model they call SketchInpainter, which is responsible\nfor an initial prediction. In this step, the gap between a past\ncontext and a future context (both given by a user) is filled by\npredicting the ‘‘missing pieces’’ in their latent representation.\nLastly, they use the SketchConnector, which modifies the\ninitial prediction to introduce the user’s controls (both in\ndesired pitches and rhythmic sketch). Despite being a rather\norganized and easy-to-understand pipeline, we need to keep\nin mind that this system involves three different models that\nmight require re-training if we wish to adjust something in it.\nIt also requires some prior knowledge from its users, as the\nperson who wishes to compose using SketchNet should know\nwhich pitches and what types of rhythms should be used to\nexpress a certain emotion or concept.\nWe mentioned Music Transformer[14] earlier in this arti-\ncle, which showed that transformer-based architectures not\nonly are fit to predict text but also fit to conduct (symbolic)\nmusic prediction. But that alone is not sufficient to give\nits user control over the outputs. A closely related work\nis [36] where the authors propose a more expressive repre-\nsentation of the MIDI-like events (coined REMI), resulting\nin a more rhythm-aware transformer-based model. Another\nnotable contribution in this field is OpenAI’s MuseNet [37],\na ‘‘GPT-2-like’’transformer-based model that was able to\nachieve polyphonic music composition in the symbolic\ndomain. MuseNet did also allow some degree of control over\nthe generative process, as the user can select a known com-\nposer as a reference for the style, as well as the instruments\nused in the piece. But these control elements were introduced\nas prepended ‘‘composer’’ and ‘‘instrumentation’’ tokens in\nthe sequences, meaning that introducing a new composer or\ninstrument to the vocabulary can be demanding in time and\nresources as it requires additional training of the large-scale\nmodel. A similar method was adopted in the work of [5],\nwhere emotional annotations were used to train the model.\nThe authors made an interactive demo 1 available, where users\ncan quickly generate music according to a given emotional\nquality. Using the demo (examples available in a SoundCloud\nplaylist2), we were able to see that the model is quite robust\nand reliable in generating piano pieces under emotional con-\nditioning, where the outputs were quite ‘‘expressive’’ and\n‘‘musically pleasing’’. However, like MuseNet, this method\ninvolves training a large model with the musical sequences\nprepended with their corresponding emotional condition and\ntherefore requires more time and resources if we wish to add\nor change the control attributes.\nSimilarly to the work presented here, Bardo Composer\n[7] introduces the ability to control the music generation\nin its symbolic representation, but specifically for tabletop\nrole-playing games. The authors also used an NLP-based\napproach. The method focuses on generating music for story-\ntelling, where the emotional information is detected from text,\n1Emopia’s interactive demo (replicate.com/annahung31/emopia).\n2SoundCloud playlist (on.soundcloud.com/TWdC1) containing samples\ngenerated using Emopia’s demo.\nVOLUME 11, 2023 52415\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nobtained through speech recognition. The authors pre-trained\na large language model (LLM) with sequences of musi-\ncal symbols (extracted from MIDI files) and proposed a\nsearch method (coined Stochastic Bi-Objective Beam Search,\nor SBBS) where they not only evaluate the quality of possible\noutputs, but also take into consideration their ‘‘emotional\nvalue’’ (using auxiliary emotional classifiers, also derived\nfrom NLP models). Both Bardo Composer and this work\nshow ways to introduce control in music generation in sam-\npling time, but while the former uses a search algorithm,\nwe choose to make small adjustments to the NLP model itself\nin sampling time to achieve control. Despite the similarities,\nwe believe that our method presents some advantages over\nBardo Composer, as the later relies on beam search. If the\nuser desires more diversity in their piece, SBBS should have\nits beam size increased, making the generative process more\ncostly, especially in a real-time (or near real-time) context.\nBardo Composer also relies on some prior context to generate\nfrom (which is taken from the dataset), meaning that the\nmethod does not necessarily generate music from scratch,\nwhereas our approach allows the user to generate both from\nscratch and from a given context.\nLater, the authors proposed a sampling method based on\nAlphaZero’s tree search approach (Monte Carlo Tree Search,\nor MCTS) [8]. This method also uses an LM trained on\nmusic sequences and a model trained to classify pieces of\nmusic among different emotional values. Additionally, it uses\na discriminator model trained to distinguish ‘‘real’’ music\nfrom ‘‘fabricated’’ music (human-made music vs generated\nmusic). This approach, MCTS, is somewhat similar to our\nwork in the sense that it shifts the probability distribution for\nthe next token to be sampled with the help of discriminator\nmodels, but instead of directly sampling the next token from\nthe updated distribution, it performs a tree search to further\nmanipulate the distribution and then sample the next token.\nThe exploration of the tree search is rather costly, making\nthe search algorithm quite time-consuming and not suitable\nfor real-time (or near real-time) applications. We generated\npieces using the pre-trained models provided by the authors,\nas well as the hyperparameters and settings described in the\nproject’sGitHub page.3 Under these circumstances, it took us\nup to 4.5 hours to complete the generation of a single piece,\nwhere the pieces had 27 to 48 seconds of duration. From the\nfew samples we generated, we were able to observe that they\nwere able to maintain coherence and were ‘‘musically pleas-\ning’’, but they did not provide much ‘‘novelty’’, sounding\nquite repetitive in most cases. We are sharing the generations\nwe obtained using MCTS in a SoundCloud playlist 4 and we\nencourage the readers to listen to them to compare the outputs\nfrom the different methods.\nIn a slightly different approach, FIGARO [38] also tackled\nthe problem of controlled music generation in its symbolic\n3MCTS GitHub page (github.com/lucasnfe/puct-music-emotion).\n4SoundCloud playlist (on.soundcloud.com/c1aNm) containing samples\ngenerated using MCTS.\ndomain by relying on transformer-based structures, but it\nintroduces human interpretable and learned descriptions to\nguide the generative process. The former is determined by a\nhand-crafted algorithm, where features such as time signa-\nture, instruments, chords and note density are some of the\nquantities determining the descriptions. The latter is imple-\nmented in the form of a VQ-V AE (Vector-Quantized V AE)\nmodel, where partitions of the music pieces are organized\nin a latent space and the latent codes are used to represent\nthem as the description. This approach lets the user have\nfine-grained control over the generative process, but the use\nof these models does not offer the same flexibility seen in\nlarge pre-trained models.\nIn this work, we wish to introduce a model that can help its\nusers compose music by using a large symbolic music model,\nand can offer the option of control to make it more viable to\nget meaningful and useful suggestions from the model. The\nlarge ‘‘music model’’ is based on GPT-2, and the controllabil-\nity aspect can be easily designed, implemented and trained,\nas the control is achieved with the help of a small, lightweight\ndiscriminator model. This approach opens the possibility for\nusers to share a hypothetical global large ‘‘music model’’\n(like GPT-2 itself is for text) and personalize the controls\nwith their own data in order to achieve highly specific results,\nor perhaps share different pre-trained discriminators among\nthemselves as a community, etc. The intent of this work\nis to help people expand their possibilities as creators and\nhelp make the act of working with machines less frustrating.\nAdditionally, the large ‘‘music model’’ used in our work does\nnot require any additional training if we wish to adjust or add\na different control, making it less cumbersome to customize\nand contribute to.\nIII. METHOD\nWhen it comes to co-creation, where both human and\nmachine work collaboratively to create a piece of art, the\nmachine is expected to help the person with fitting and useful\nsuggestions, whilst preserving the user’s sense of authorship\nand freedom to create. A study conducted with novice musi-\ncians [39] suggested that, although AI-powered tools can\nindeed be helpful in music composition (especially when the\nuser does not have enough prior knowledge in music theory),\nthe use of an AI-powered tool to compose music becomes\nless overwhelming and easier to handle when there are clear\n‘‘high-level’’ controls that users can readily get a hold of.\nAs demonstrated by previous studies, NLP models can be\nsuccessfully used to generate music [14], [40]. Here, we con-\nsider the possibility of extending this hypothesis beyond the\ngeneration itself and employ control over symbolic music\nsampling by using a method derived from NLP. The work\npresented here is based on PPLM [1], a method used to\ncontrol text generation in sampling time. As PPLM employs\na discriminator-based control, where the discriminator is\ndesigned and built by ourselves, this method is fitting to\nbuild a ‘‘high-level’’ control that has the potential to be user-\nfriendly.\n52416 VOLUME 11, 2023\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nA. DATA\nThe symbolic representation of music is commonly digitized\nin the form of MIDI files. From these, we can extract plenty\nof information on a specific piece of music, such as the\ninstruments used, the tempo, and the notes played at each\ntime step of a piece. One great advantage of using MIDI files\nfor music representation is that it is already described in a\ndiscrete and finite collection of events.\nOur data was pre-processed based on the encoding method\nperformed in the Music Autobot project [41]. However,\nas they use different tokens for note events, duration\nevents, and silence events (separators) the decoding\nprocess can only be successfully performed if the string of\ntokens is sampled in a specific order of types of token. The\nmethod proposed here employs control over the generative\nprocess in sampling time by making targeted adjustments\nin the LM’s weights, which can make it harder for the\nmodel to follow that very specific and strict configuration of\ntokens. To combat that, we chose to adapt the vocabulary to\nhave tokens that each represent a pair of one note token\nand one duration token, resulting in a vocabulary with\n20790 tokens in total.\nFrom the MIDI representation, we adopted 128 differ-\nent notes, with the addition of the lack of sound (separa-\ntor/pause). Each one of these 129 symbols was then paired\nwith 161 different length symbols. This process gives us a\ntotal of 20769 tokens with different duration and sound (or\nlack thereof) representations. On top of these, we dedicated\nthe first 11 tokens to special events, such as the [BOS]\nand [EOS] tokens (beginning and end of sequence tokens).\nAnd to the end of the vocabulary, we added 10 tokens that\nrepresent different tempo annotations.\nThe datasets involved in this work are:\n• MAESTRO: a dataset comprised of piano performances\nof classical music [42];\n• VGMIDI: a dataset that contains MIDI files of video-\ngame soundtrack, annotated on whether they sound pos-\nitive or negative and whether they are high or low in\narousal [6];\n• EMOPIA: a dataset built (mostly) on piano covers of pop\nmusic found online annotated with their corresponding\nemotion with respect to Russell’s model of affect [5].\nWe chose to use different datasets containing a range of\nmusic genres in order to expose the language model to as\nmany sequences as possible. By providing a richer variety of\nmusic, we expected the model to be familiarized with, and\nproduce a wider variety of outputs. However, it is important\nto emphasize that despite using three different datasets with\ndifferent genres of music, the data involved in this study is not\nin the slightest comparable to the comprehensive collection of\ntext originally used to train any of the previously discussed\nlanguage models. This means it is highly likely that there\nare sequences and combinations of tokens that are considered\n‘‘music’’ that are not expressed in any of the samples in our\nTABLE 1. The total amount of data samples found in each dataset, as well\nas the quantity of samples for each class (if any). VGMIDI is partially\nlabelled, which explains the mismatching total amount of samples.\ndatasets and, therefore, our model has not been exposed to\nand is not ‘‘aware’’ of these. The number of samples used\nfor training per dataset (as well as the distribution of the\n‘‘emotional’’ classes across them) can be seen in table 1.\nThe dataset MAESTRO was split according to the dis-\ntribution provided by the authors: about 75% for training,\n14% for testing (used for evaluation in our case) and 11%\nfor validation (not used in our training process). VGMIDI’s\nunlabelled portion of the dataset was entirely dedicated to\nthe training portion, whereas the labelled portion was split\ninto training (80%) and evaluation (20%). EMOPIA was also\nsplit into two parts, one for training (about 90%) and another\nfor evaluation (about 10%). Both VGMIDI (labelled) and\nEMOPIA were manually split to keep the distribution of\nlabels as even as possible across the two splits to train the\nemotional discriminator model.\nB. LANGUAGE MODEL\nThe base language model used here is GPT-2 [10], a\ntransformer-decoder based model developed by OpenAI\nand made available under Hugging Face’s transformers\nlibrary [43]. Transformer architecture is an attention-based\narchitecture, which was proposed by [9] as a way of perform-\ning machine translation. It was introduced at a time when the\nmost common approach to sequence-related problems was to\nuse recurrent neural networks (RNNs). It was quickly adopted\nas a standard in the NLP community as it handled long-range\ndependencies better than other architectures available at the\ntime. Earlier, we pointed out that GPT-2 (as a large-scale\nlanguage model) was trained on an extensive collection of\ntext data sourced from the web. To turn GPT-2 from a lan-\nguage model to a ‘‘music model’’, we trained it over the data\ndescribed previously to predict sequences of music tokens\n(pre-processed MIDI data). By the end of the training process,\nour modified version of GPT-2 was capable of both generat-\ning a piece of music from scratch and completing unfinished\nsequences of music. The top half of figure 1 illustrates the\nprocess of training GPT-2 and the bottom half displays an\nexample of how it is used to then sample pieces of music once\nit is done training. In this case, the GPT-2 model consisted\nof a stack of 6 transformer decoder blocks, with the hidden\ndimension of 768 and 12 attention heads. The model was\ntrained on sequences with 1024 tokens of length, where the\nsequences that exceeded said length were cropped to fit the\nmaximum length, and shorter sequences were padded to reach\nVOLUME 11, 2023 52417\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nFIGURE 1. We pre-processed the MIDI files into sequences ofmusic\ntokens and fed them to GPT-2 during training. Once our ‘‘music model’’ is\ndone training, we can use it to generate sequences ofmusic tokens, that\nare then decoded back into MIDI files.\n1024 tokens. The model was trained using Hugging Face’s\nTrainer API, where we set the epochs to 500, warm-up steps\nto 25000, training batch size and evaluation batch size were\nset to 16 and 32 respectively, where the entries in the dataset\nwere randomly shuffled, and we set half-precision to True.\nAs we did not set up an optimizer ourselves, the optimizer\nused was the default AdamW (Adam algorithm with weight\ndecay fix as described in [44]), with its default configurations.\nC. DISCRIMINATOR AND PPLM\nAs we stated previously, the generation control in our work\nis performed using one of the approaches presented in\nPPLM [1], where a discriminator model is used to steer the\ngenerative process towards a ‘‘label’’ of interest. To better\nillustrate in the context of the original PPLM (in NLP),\nconsider a discriminator model that was trained to classify\ndifferent pieces of text between ‘‘positive’’, ‘‘negative’’ and\n‘‘neutral’’. This discriminator is then used to adjust GPT-2 to\nmake it more likely to sample words that are more closely\nrelated to the label chosen as a condition for generation. For\nexample, if the current context given to GPT-2 to generate\nfrom is the phrase ‘‘this chicken tastes’’, the model may\nsample ‘‘bad’’ next (yielding a negative message), but by\nperforming the adjustments in GPT-2 with the help of the dis-\ncriminator model, we can make it sample ‘‘good’’ or ‘‘okay’’\nnext, depending on our choice for the control (positive and\nneutral, respectively). What the procedure described above\ndoes is change the ‘‘perception’’ of the model in regards\nto the context. It is as if the model was being placed in a\nposition where it was given additional context prior, where\nthis additional context is closely related to the sentiment we\nwish it to generate with.\nThe aforementioned ‘‘adjustments’’ are made by, first, per-\nforming a forward pass through the language model, and then\nthe discriminator model is used to compute the probability of\nthe output belonging to the desired ‘‘class’’. Then a backward\npass is performed from the gradients in the discriminator to\nFIGURE 2. Illustration from PPLM’s original paper that shows in a\nsimplified manner the steps involved when performing control over text\ngeneration.\nFIGURE 3. Simplified illustration of the controlled generation using PPLM.\nthe language model, updating the cached key-value pairs in\nthe previous time step t. This process is repeated m times for\na gradual adjustment towards the desired attribute. Lastly, the\nnext element in the sequence is sampled using the updated\nlanguage model (see figure 2). As alluded in [1], the adjust-\nments performed in the past key-value pairs can be thought of\nas ‘‘gradual reinterpretations of the past — that guide future\ngeneration in the desired direction’’.\nThe gradual adjustments involve a range of hyperparam-\neters that can significantly change the outputs from the\nmodel. Some of the hyperparameters (that were more heavily\ntweaked in our process) include the amount m of iterations\nthat the process described above was repeated for, as well as\nthe step size α in each iteration that controls the ‘‘strength’’ of\nthe adjustments made; λKL , a scalar used in the minimization\nof the KL divergence between the output distribution from\nthe original model and the updated model; and a scalar γgm\nused in the sampling process, where it dictates how much\nthe updated distribution should affect the original distribu-\ntion (keeping both the modified distribution and the original\ndistribution from the LM tied).\nThe discriminator was slightly adapted from the original\ncode provided in [1] as it was proven to perform well with\ncontrolled text sampling. The discriminator has only a single\nlayer that has the a vector with the same dimensionality as\nthe hidden states as input. It applies a linear transformation\nto output a vector, with the length equal to the amount of\nclasses used for the sampling manipulation (4, in the case\nof the quadrants). Another change we made to the original\ndiscriminator model is that, instead of it outputting a one-hot\n52418 VOLUME 11, 2023\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nFIGURE 4. Russell’s circumplex model of affect. Indicated by the dots are\nsome of the emotions that are commonly associated to their respective\ncoordinates.\nvector indicating the predominant class, we chose to work\nwith the probabilities of a piece belonging to each class and\noptimized our model with respect to the MSE between the\ndesired and obtained probabilities. We trained a discriminator\nusing the EMOPIA dataset [5]. In this dataset each entry is\nannotated with its corresponding quadrant in Russell’s model\nof affect [45] (Figure 4). This means that when presented\nwith a sequence of ‘‘musical tokens’’, the discriminator will\nclassify it among four labels: high arousal and high valence\n(Q1), high arousal and low valence (Q2), low arousal and low\nvalence (Q3) and low arousal and high valence (Q4). Just as\ndone in the NLP example described above, this discriminator\nmodel is then used to make adjustments within ‘‘music GPT-\n2’’ to then sample a sequence of tokens that are deemed\nto be more closely related to the label of choice (in figure\n3, we illustrate how the method is used to make GPT-2\ngenerate a piece of music that is high in arousal and valence,\nthat is, it should belong to Q1). The discriminator model\nwas trained for 300 epochs, with a learning rate of 5e-5.\nWe set the batch size to 8 where the entries were randomly\nshuffled.\nD. GENERATIVE PROCESS\nEven though transformers are known for being able to main-\ntain coherence in symbolic music generation over time [14],\nthe pieces generated for this study were limited to 256 tokens\nin length, as they were intended to be used in our survey\n(described in detail in a later section). As the sampling is not\nperformed in the audio space, the generated pieces varied in\ntime duration, but all of them remained under one minute of\nduration.\nIn this section, we will be explaining the generative process\nwe designed to obtain our samples.\n1) SAMPLING PROCESS\nIn the interest of obtaining more dynamic and ‘‘interesting’’\ngenerations, as well as fully exploiting the generative capa-\nbilities of this method, we chose to vary the hyperparameters\nalong different parts of the generated sequence. To achieve\nthat, we set a range of possible values for each one of the\nhyperparameters available in the PPLM sampling step and\nexplored along the possible combinations of values.\nAs we previously mentioned, every generation shared here\nwas limited to 256 tokens in length. They were sampled\nin intervals of 64 tokens at a time. For each one of these\nintervals, we generated a set of sequences, each obtained\nwith a different setting of hyperparameters. Once we have\nthe outputs from all of the predetermined range of values for\nthe hyperparameters, we selected one of them to be included\nin our final composition. This process is repeated until the\nfinal output reaches the desired length or the end-of-sequence\ntoken is sampled.\n2) AUTONOMOUS × COLLABORATIVE GENERATION\nHere we take into consideration two different sampling meth-\nods: autonomous generationand human-assisted genera-\ntion. In the former, the generations are selected according to\na scoring system that is based on musical metrics, referenced\nfrom mgeval [46]. In the latter, this selection is performed\nwith human assistance, meaning that the user would take\ninto consideration the computed scores for each of the gen-\nerations, but would make the decision based on their own\npersonal preference.\n3) SCORING SYSTEM\nThe scoring system used in the sampling method was derived\nfrom the work in mgeval, in which we measure pitch-based\nand rhythm-based features from MIDI excerpts in order to\nget a grasp of what the music sounds like, without having to\nlisten to it [46]. As the metrics alone are not as obviously\nunderstandable as simply listening to the pieces of music\n(albeit less time-consuming and less demanding in effort),\nwe extracted the same metrics from EMOPIA as [5]. The met-\nrics we used directly from the mgeval are: pitch count, pitch\nrange, average pitch interval and average inter-onset-interval.\nTo account for the pitch class and note length histograms\nand the note count per bar, we chose to take the average and\nstandard deviation from them and take those as the metrics\nfor scoring instead. On top of these metrics derived from\nmgeval, we added duration to encourage longer composi-\ntions, the proportion of the duration of all of the rest/silence\nsymbols in the piece used in respect to the total length, to\nreward pieces with an adequate balance between notes and\nsilence in the pieces, and maximum amount of notes used in\na bar, to penalize pieces that occasionally had parts that were\ntoo busy.\nFor each one of the emotional annotations used in\nEMOPIA (Q1, Q2, Q3 and Q4), we extracted the features\nof interest from all pieces of music under a given class and\nVOLUME 11, 2023 52419\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nput together a ‘‘profile’’ by taking the average from each\nfeature (as well as the standard deviation for some of them).\nThese metrics were taken for every incremental interval of\n64 tokens, that were used as references for the corresponding\nstep in the generative process, i.e. when deciding on the gen-\nerated sequence for the tokens in the 65 th position to the 128 th\nposition under the condition of the class Q1, for example,\nwe take the ‘‘profile’’ obtained from the first 128 tokens of\nall of the music labelled as Q1 as a reference to select the\nchunk of generated tokens that will potentially fit best in that\ncontext and condition.\nAs we described in the previous section, the generative pro-\ncess is split into smaller steps where we explore a grid of dif-\nferent values for the hyperparameters to obtain more dynamic\nand interesting pieces of music. At each step, we compute the\nmetrics for all of the obtained generations and compare them\nto those taken as reference by computing a sort of ‘‘weighted\nsquared error’’ from one array of metrics to another. As dif-\nferent metrics have different significance to different classes,\nwe also assigned different weights to each one of the metrics.\nAs an additional safety measure, we also fed the generations\ninto our discriminator model and took into account their\nprobability of belonging to the class of interest (that is, the\ncondition used for the sampling). At the end of each step,\nwe have a list of the generations ranked from the closest to the\nfarthest from our desired reference metrics. To better illustrate\nour scoring system, consider that we are generating music\nusing Q1 as control, and the ‘‘profile’’ used as reference is\nas follows:\nmref =\n[\nmref\n1 , mref\n2 , . . . ,mref\nn\n]\n, probref\nQ1 = 1.0\nwhere mref\ni , i = 1, . . . ,n are the n metrics used to build the\nprofile and prob ref\nQ1 pertains to the probability of a piece being\nlabeled as Q1, here assumed to be 1.0 since this is our target\nprobability for the generative process. By taking one of the\nobtained generations, we extract the following metrics:\nmgen =\n[\nmgen\n1 , mgen\n2 , . . . ,mgen\nn\n]\n, probgen\nQ1\nwhere mgen\ni , i = 1, . . . ,n are the metrics from the generated\npiece and prob gen\nQ1 is the probability of it belonging to Q1\naccording to our discriminator model. To score the genera-\ntions obtained under the control of ‘‘Q1’’, we set a specific\narray of weights for each one of the metrics:\nw = [w1, w2, . . . ,wn] , wprob\nNote that for these results, the weights were set arbitrarily\nwith no particular logic, other than the user’s preference and\nintuition. The values were decided according to the user’s\nunderstanding of which metrics should be prioritized over the\nothers, and they were empirically adjusted to taste.\nAnd the score is then obtained as:\ns = wprob\n(\nprobgen\nQ1 − probref\nQ1\n)2\n+ smetrics\nsmetrics =\nn∑\ni=1\nwi\n(\nmgen\ni − mref\ni\nmref\ni\n)2\nFIGURE 5. An example of how the autonomous generation is conducted\nusing our scoring system. Here, out of all of pre-determined values for the\nhyperparameters, the third configuration (HP3) was the one to yield the\n‘‘best’’ result, which is then adopted as our output. In this case, the\ngenerated sequences have 64 tokens of length, if the target length for the\ngeneration is longer than 64 tokens, the output is then fed back into our\nsystem and this procedure is repeated until the 128th token has been\nsampled, or until the target length is reached, at which point the same\nranking procedure is conducted.\nAs our scores are based on ‘‘distances’’ (errors), we rank\nthe generations from lowest score to highest, the lowest being\nthe ‘‘most fitting’’ generation out of that particular batch,\nunder the Q1 condition. Refer to figure 5 for an illustrated\nscheme of how the autonomous sampling is performed.\nIV. RESULTS\nEvaluating music is a complex task, especially when it comes\nto assessing its quality and what type of emotion it conveys,\nsince both of these features rely on an individual’s perception\nand judgement. Here, we attempt to evaluate our method’s\neffectiveness in a more objective approach, and to further\nsupport our assessment, we conducted a study where we col-\nlected subjective evaluations on some of our computationally\ngenerated pieces of music. The pieces used for this study can\nbe found in our SoundCloud playlist. 5\nA. OBJECTIVE EVALUATION\nOur objective evaluation is based on an analysis of some of\nthe musical features derived from mgeval and the classi-\nfication performed by our own discriminator model. These\nmeasurements are not definitively telling of the class each\npiece belongs to, nor are they decisively reliable to draw\nconclusions on the quality of the AI-generated pieces, but\nthey provide some insight into them without having to listen\nto them individually.\nWe used our method to generate sequences of 64 tokens\nand selected those that were labelled accordingly to the class\nused to obtain them. These were then used as the base for\nthe generation of longer sequences using different methods.\nWe generated musical pieces using our method autonomously\n(‘‘our method (autonomous)’’, or simply, ‘‘Autonomous’’),\nour method with human participation (‘‘our method (col-\nlab.)’’, or simply ‘‘Collaborative’’) and with the ‘‘music GPT-\n5SoundCloud playlist (soundcloud.com/user-394289908/sets/our-\nsamples/s-m9wlpdnenCV) containing the samples obtained through our\nmethod.\n52420 VOLUME 11, 2023\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\n2’’ only (simply ‘‘GPT-2’’). Lastly, the generations were fed\ninto a pair of separate discriminator models trained solely\non VGMIDI, one model for arousal (low or high arousal),\nand another model for valence (negative or positive), and we\nverified whether the method was able to achieve control of\nthe generation or not. Note that the generation using ‘‘music\nGPT-2’’ only is controlled via the context (prompt) used to\ninitiate the generation. The results are as shown in table 2,\nwhere we present the proportion of generated pieces that were\nlabelled as their intended class, per class and per method.\nWe emphasize that the VGMIDI-based discriminator mod-\nels are not to be completely trusted, as the VGMIDI dataset\nin itself is quite limited, and therefore the accuracy of said\ndiscriminators is most likely far from ideal. But these mea-\nsurements gives us a reference for a perspective, more specif-\nically seen from the context of video-game soundtracks,\nwhich tend to be quite clear in conveying emotions to listen-\ners/players. Based on these results, we can see that the method\nthat performed better in terms of achieving control over the\ngeneration was the autonomous approach to our proposed\nmethod. On top of that, we can see that despite using a prompt\nsequence to generate from ‘‘music GPT-2’’, it was not enough\nto maintain control over the generation by itself. Overall, all\nmethods seemed to score lower when used to generate pieces\nmeant to have low arousaland negative valence. This could\nbe an issue associated to our discriminator models limitations.\nAdditionally, as VGMIDI has an unbalanced distribution of\nsamples with low/high arousal and negative/positive valence\n(see table 1), the performance of the discriminator models for\nlow arousal and negative valence might have been compro-\nmised.\nDisregarding the low accuracy scores seen in low arousal\nand negative valence in table 2, our methods were mostly able\nto maintain the desired emotional quality along the generative\nprocess. It is possible that our collaborative approach did not\nachieve as high a score as the autonomous approach due to\nthe stylistic choices made by the person interacting with our\nsystem, meaning that along the generative process the user\ndid believe that the generated segments did belong to the\nintended class. Because the user relies mostly on their own\npersonal perception of conveyed emotion, there is a chance\nthat the user’s perception did not align appropriately with our\ndiscriminator models. On top of that, the user was inclined\nto select the sequences that made the generation sound more\nlike what they envisioned, which is one of the main intended\nuses of our method.\nB. SUBJECTIVE EVALUATION\nAs we know, the emotion attributed to a particular piece of\nart is highly subjective. Although each individual’s percep-\ntion is formed in association with social aspects (allowing\nindividuals to have similarities to each other, and mutual\nunderstandings in their own interpretations), one’s personal\nexperiences can make two individuals’ interpretations and\nfeelings towards the same subject diverge completely.\nTABLE 2. Proportions of the ‘‘accurately generated’’ sequences using\ndifferent methods. We generated the continuation to prompt sequences\nof a given class, expecting that the generation would continue to follow\nsaid class. We then fed the final generations to the VGMIDI-based\ndiscriminator models to verify which labels were attributed to them.\nTo help verify whether the proposed method was\nable to achieve its intended purpose, of generating\nmusic under the condition of a desired ‘‘emotional\nvalue’’, we collected people’s opinions and thoughts on\nemotions they perceived in pieces of music generated\nwith AI.\nWe designed four different surveys, each of which con-\nsisted of three different sections. The first section was the\nsame for each survey, and gave the participants a short intro-\nduction to the study. They were informed that the musical\npieces presented were all generated with the help of AI.\nIt also gave them a brief explanation of Russell’s model of\naffect, as this was essential to answer the following questions.\nAdditionally, we tested their understanding, by providing a\ndescription of a specific quadrant, and asking the participant\nto tell us which quadrant in the model that was. If the partic-\nipant answered the question incorrectly, they would be met\nwith a warning, which informed them of the correct answer\nand advised them to review the explanation given previously\nbefore proceeding with the survey. As we suspected that\nsubjectivity could greatly impact the outcome of our survey,\nwe also gave the participants four different samples from the\ndataset and asked them to indicate where they would place\neach one of these tracks on the model of affect. This was\ndone using an illustration of the model of affect, as seen in\nfigure 6. For clarity, in this specific section of the survey, the\nparticipants had been informed that the tracks were composed\nby people, and we also emphasized that there were no right\nor wrong answers and that we were instead interested in col-\nlecting their own personal opinions. Each sample was from a\ndifferent quadrant (which participants were not informed of),\nand we designed this step to verify whether the participant’s\nperception of emotion in a piece of music was close to that\nannotated in the dataset used in this work (EMOPIA) or not\n(we refer to this step as ‘‘emotional profiling’’).\nThe second portion was comprised of questions about\nthe participant’s perceptions of six different computationally\ngenerated pieces of music. The six tracks were different\nacross the four surveys, and every group of six tracks had\ntwo tracks that were generated using musical GPT-2 (here\nlabeled as ‘‘GPT-2’’), two tracks that were autonomously\ngenerated using our method (labeled as ‘‘autonomous’’),\nand two tracks that were generated with our method with\nhuman intervention (labeled as ‘‘collaborative’’). Lastly, the\nparticipants were asked to answer a simple demographic\nquestionnaire.\nVOLUME 11, 2023 52421\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nFIGURE 6. Illustration of a squared version of Russell’s circumplex model\nof affect. This figure was presented in a clickable interface to our\nparticipants, where each participant clicked on where they would place\nthe perceived emotion when listening to one of our pieces of music. The\nnumbers denote which quadrant each area corresponds to.\nThe subjective assessment (in the second part of the survey)\nwas performed in three different categories: emotionality,\nwhich refers to the emotion a participant perceived when\nthey listened to the given piece; creativity, which refers to\nthe participant’s opinion of the creativity of a piece; and\nmusicality, which refers to whether the participant thinks\nthe given piece sounds like music or not. Likert scales were\nused for the questions on creativity and musicality. To rate\nthe given piece in emotionality, each participant was again\npresented with figure 6 in a clickable interface, where they\nindicated where they would place the perceived emotion in\nthe illustrated Valence-Arousal space. To rate the creativity\nof a piece, we asked the participant ‘‘Based on your own view\nof what creativity is, how creative do you think this piece is?’’,\nwhere the options for answers were: 1: ‘‘Not creative at all’’,\n2: ‘‘A little creative’’, 3: ‘‘Moderately creative’’, 4: ‘‘Very\ncreative’’ and 5: ‘‘Extremely creative’’. To rate the musicality\nof a piece, we asked the participant ‘‘Does this sound like\nmusic to you?’’, where the options for answers were: 1: ‘‘No,\nnot at all’’, 2: ‘‘Yes, a little’’, 3: ‘‘Yes, quite a bit’’ and 4: ‘‘Yes,\ncompletely’’.\nThis study was conducted over a period of two weeks.\nIt was built using Survey Monkey 6 and distributed through\nthe platform Prolific. 7 We collected answers from 989 partic-\nipants, but here we will be sharing the results obtained taking\ninto consideration only the answers gathered from partici-\npants who did not fail the ‘‘understanding-check’’ question\n(939 participants).\nOut of the 939 participants, we obtained a relatively even\nnumber of female (461) and male (478) participants and no\n6www.surveymonkey.com\n7www.prolific.co\nTABLE 3. Amount of participants that declared to belong in each age\ngroup.\nother genders were reported. The majority of our participants\nfell into the younger age groups, with around 80% of them\nbelow 35 years of age (see table 3 for a more detailed sum-\nmary).\nAt the beginning of our survey, we instructed participants\nto listen to each piece in its entirety before answering any\nquestions about it. The answers that did not meet a mini-\nmum time requirement (i.e. the length of the piece) were\ndisputed, and if the participant did not respond adequately\nto the dispute, they were rejected (and therefore not included\nin our pool of results). All participants that had their answers\naccepted were monetarily compensated.\nIn the figure 7 below, we show the answers for both\ncreativity (a) and musicality (b) grouped by the generative\nmethod (‘‘GPT-2’’, ‘‘Autonomous’’ and ‘‘Collaborative’’,\nas we described above). For every group of generated music,\nwe show the proportion of the answers for every option in our\nrating system.\nIn figure 7, we can observe that the ratings given to the\npieces in group ‘‘GPT-2’’ were in general lower than those\ngiven to both ‘‘Autonomous’’ and ‘‘Collaborative’’ in both\ncreativity and musicality. Interestingly, we can see that in\nterms of creativity, the ‘‘Autonomous’’ approach seemed to\ngenerate pieces that had a bigger impact on our partici-\npants, whereas the ‘‘Collaborative’’ method showed better\nresults in terms of musicality. This can be a reflection of\nthe novelty × quality in creativity, where the ‘‘Autonomous’’\napproach results in less conventional pieces of music, that are\nmusic-like to some degree. Adversely, the ‘‘Collaborative’’\napproach results in more music-like pieces, but that may\nnot offer much novelty for the choices made by the person\ninvolved in the generative process.\nA Kruskal-Wallis test was used to compare the results of\nthe three methods for creativity and musicality. The results\nof this test when comparing the medians of the perceived\ncreativity for each group gave a test statistic of H =100.6 with\na p-value < 0.001. The results for the perceived musicality\nfor each group gave a test statistic of H = 137.5 with a p-\nvalue < 0.001. This suggested the differences in the results\nbetween the methods were significant, for both creativity and\nmusicality.\nIn figure 8, we can see what ‘‘emotional values’’ the par-\nticipants associated the pieces of music with. These heatmaps\nshow the distribution of the clicks received from the par-\nticipants when asked to rate the ‘‘emotion’’ they perceived\nfrom our generated pieces of music. Each row of graphs\nis representative of a different generative method (from top\nto bottom: ‘‘GPT-2’’, ‘‘Autonomous’’ and ‘‘Collaborative’’).\n52422 VOLUME 11, 2023\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nFIGURE 7. Creativity and musicality ratings according to our user study.\nWithin each method, we grouped the collected answers by the\nintended quadrant when generating each piece. For ‘‘GPT-\n2’’ that means that the model was given a primer sequence\nthat had been classified as belonging to one of the quadrants\nby our discriminator model, whereas in the other methods\nthe model was explicitly asked to generate music pieces that\nbelonged to one of the quadrants, as described in the previous\nsection. Additionally, we present table 4 listing how many\nof the clicks provided by the participants matched with the\nintended quadrant, and how many of them did not match (as\nwell as the proportion that the matching clicks represented).\nWe grouped the clicks collected from the participants\naccording to the intended quadrant and the generative method\nused to obtain the pieces used in our user study. We then\nfurther analyzed them according to how ‘‘strong’’ the effect of\neach method was along the arousal and valence axes, i.e. how\nfar from the origin the clicks were in the [−0.5, 0.5] range.\nOneway-ANOV A’s were conducted to test if there were\nsignificant differences between the results for different\nmethods. These showed significant differences in all cases\n(p<0.001) except for the valence value for pieces which were\nintended to be in quadrant 4 (i.e. intended to be emotionally\npositive) which was not statistically significant (p=0.89).\nPost-hoc t-tests were performed to further investigate these\ndifferences.\nAccording to these the following results were statis-\ntically significant (p value<0.05). In the arousal axis,\nour autonomous method seemed to perform better (more\nstrongly) for the quadrants ‘‘Q1’’, ‘‘Q2’’ and ‘‘Q3’’. For\n‘‘Q4’’, the method that seemed to perform best in the arousal\naxis was our collaborative method. In the valence axis, the\nresults were less consistent, where the method that performed\nbest when generating pieces in ‘‘Q1’’ was our autonomous\nmethod, while the method that showed better results in ‘‘Q2’’\nwas our collaborative method, and for ‘‘Q3’’, GPT-2 seemed\nto perform better. There were no significant differences\nobserved in the valence axis in ‘‘Q4’’.\nFrom the graphs on the top row of figure 8, we can see that\nthe pieces generated using ‘‘GPT-2’’ were partially placed\nin the intended quadrant. Those that were generated using\nprimer sequences in the first and third quadrants (‘‘Q1’’ and\nTABLE 4. Amount of answers that did or did not match the intended\nquadrant. The autonomous approach to our method had the highest\nrating for matching clicks.\n‘‘Q3’’, respectively) seemed to properly convey the intended\nemotion. However, those that were generated using primers\nin the second and fourth quadrants (‘‘Q2’’ and ‘‘Q4’’, respec-\ntively) received a considerably low amount of clicks on the\ncorresponding quadrant. The pieces generated from a ‘‘Q2’’\nprimer received mostly ‘‘Q1’’ ratings, and despite ‘‘Q2’’\nreceiving the second highest amount of clicks, it was signifi-\ncantly lower compared to the number of clicks in ‘‘Q1’’. Fur-\nthermore, the pieces that were generated from ‘‘Q4’’ primers\nseemed to do worse, as the quadrant with the least amount of\nclicks was ‘‘Q4’’, and ‘‘Q1’’ and ‘‘Q3’’ were the quadrants\nthat received most of the clicks from the participants. Now,\nlooking into the middle and bottom rows of graphs, we can\nobserve that the situation with the pieces generated to belong\nin ‘‘Q1’’ and ‘‘Q3’’ does not change for most part, keeping a\nsignificant amount of votes within the intended quadrant. And\nalthough the highest concentration of clicks does not sit on the\ncorresponding quadrant for the ‘‘Q2’’ generations, there was\na significant shift of clicks to the intended quadrant compared\nto those seen in the top row, especially in the ‘‘Collaborative’’\ncase. The ‘‘Q4’’ generations had the least amount of ratings\nplaced in the appropriate quadrant under the ‘‘Autonomous’’\ncase, which seems to get considerably better under ‘‘Collab-\norative’’. Despite the fourth quadrant not holding the highest\nconcentration of clicks, it had a significantly higher amount\nof votes within and around the appropriate quadrant when\ngenerated with ‘‘Collaborative’’.\nOne possible reason for ‘‘music GPT-2’’ not to perform as\nwell as both of our methods (more specifically when gener-\nating them from prompts that were labeled as Q2 and Q4)\ncan be related to the prompts themselves. All the generations\nwere obtained from prompts of 64 tokens in length, which\nmay have been too short or ambiguous for ‘‘music GPT-2’’\nto infer the next tokens as intended. This further validates our\nVOLUME 11, 2023 52423\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nFIGURE 8. Heatmaps obtained from the points collected in the emotionality rating. The heatmaps on the right column show which quadrant\nin the model had the most clicks, whereas the heatmaps on the left show more details on the overall placement of the clicks. Each row\nshows the distribution of the clicks in the Valence-Arousal space for a different generative method, that is, we compiled all of the ratings\ngiven to all pieces generated by one of the three methods, separated them according to the intended ‘‘emotion’’ (quadrant) and obtained\nthe heatmaps from the points for each quadrant.\n52424 VOLUME 11, 2023\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nmethod, as a user may not always have prompts long enough\nto direct the generation according to their wish.\nDespite both of our methods exceeding ‘‘music GPT-2’’ in\nour user study, neither of them were able to achieve striking\nresults, more specifically when generating for ‘‘Q2’’ and\n‘‘Q4’’. In fact, the class that either of our methods seemed\nto perform best in generating for was ‘‘Q1’’, where the\nresponses received from our participants were visibly more\nfavorable. Despite the distribution of samples in the datasets\nacross the different classes not being necessarily even, where\nwe can see that ‘‘Q2’’ and ‘‘Q4’’ were the classes with most\nsamples present (see table 1), the discriminator models we\ntrained would tend to perform well in one quadrant in spe-\ncific, where in most cases the quadrant with the best accuracy\nwas ‘‘Q1’’. The discriminator model used to generate the\npieces used in the study in particular had a significantly supe-\nrior accuracy when labeling pieces tagged as ‘‘Q1’’. We fur-\nther discuss what could have compromised the discriminator\nmodel’s performance in the next section, but we can argue\nthat due to the plain simplicity in it’s architecture our method\nwas not able to perform as well in other classes as it did in\n‘‘Q1’’.\nV. DISCUSSION\nThe use of AI as a tool for art creation has become more\npopular as the latest developments in this field prove to hold\nmore and more potential. An ongoing discussion on AI’s\nparticipation in art creation is whether it contributes to the\nprocess positively or not. Another issue lies in the target\naudience, whether the latest research does or does not reach\nthe intended group of users and how they react to it. In this\nsection, we lay out some of the issues in the usability of the\nproposed method, and several obstacles that we faced during\nthe implementation and experimentation.\nA. REPRESENTATION OF OUR DATA\nWe described how the data used in this work was processed\nbefore being fed to the model (the ‘‘tokenization’’). As the\nfocus of this work was mainly on the implementation of\ncontrol over the sampling steps, we decided to do a sim-\nple ‘‘translation’’ process between the MIDI data and the\nsequences of tokens used to train our model. If we were to\ntrace parallels to text representation, we could say that it\nis equivalent to assigning a number for every character and\nfeeding the string of numbers to the models to learn. Using\ndifferent methods to encode sequences of notes and other\nMIDI metadata (such as velocity) may improve the model’s\nperformance with a more meaningful representation, possibly\nmaking the task of classifying the musical sequences easier,\nand therefore improving our system as a whole.\nB. DATA AVAILABILITY AND ITS EFFECTS\nAs AI-generated art (music, in this case) is still in its first\nsteps into becoming more accessible to (and understood by)\nthe general public, more questions and concerns are raised\nconcerning the concept of authorship and how licensing\napplies. One major limitation seen in this field is regarding\nthe availability of data used for the training of these models,\nespecially with music, where access to the pieces is a quite\ndelicate matter. It is out of the scope of our work, however, it is\nimportant to emphasize that when sourcing data for training\nmodels, as researchers, it is ideal to look for diverse and\nethically sourced material. As ‘‘text-to-image’’ models have\nbecome more accessible to the general public (the so-called\n‘‘real people’’), we are met with an ever-growing crowd of\nartists that are worried that their own pieces have been used\nin training without their knowledge. As of the writing of this\nwork, there is much to be talked about in AI ethics, especially\nin the creative space.\nThere is still much to be explored within the field of AI-\ngenerated music and there is much to improve in terms of data\nand usability. Given the limitations we currently face in the\ncontext of data accessibility, methods that can easily adapt\nto an ever-growing pool of data tends to be preferred over\nthose that require much more time and effort in order to bring\nin additional information. In the case of the work presented\nhere, if we were to consider the hypothetical occasion that a\nlarge-scale music model is made public (quite like GPT-2 was\nto the NLP community), this method can then be easily used\nby most people with what and however much data is available\nto them.\nAdditionally, large-scale models, GPT-2 in this case, are\nknown to perform better with a larger volume of data in\ntraining, so that the model is made ‘‘aware’’ of as many\npossible combinations that there may be for the sequence of\ntokens. That means that the base language model used for this\nwork is not likely to be in its best shape for music generation,\nas there definitely are combinations of notes that were not\naccounted for in our current pool of data.\nAnother facet that is unavoidably impacted by the lack of\ndata is the discriminator model used to control the sampling\nprocess. The more accurate it is, the better it is to control\nthe generative process, as the probabilities obtained from our\ndiscriminator directly impact the adjustments that are made\nover the PPLM steps. Since the difference between the target\nand the discriminator’s probabilities are backpropagated to\n‘‘music GPT-2’’ to achieve a more desirable output, it is fair\nto assume that our method would be able to perform better at\ncontrolling said generations if the discriminator model can\nmore accurately determine the classes of our generations.\nHowever, in this context, where we wish to achieve a ‘‘music\nequivalent’’ for GPT-2, the system requires a richer and more\nvaried dataset to properly train both ‘‘music GPT-2’’ and\nthe discriminator(s) model(s), meaning that the problem of\nobtaining an accurate ‘‘general’’ discriminator model is sig-\nnificantly harder. A future work that can derive from our work\ncomes from extending the datasets used here like the authors\nin [8] did, and verify how the controllability is affected with a\npossibly better performing discriminator model. On the other\nhand, as the discriminator model is completely separate from\n‘‘music GPT-2’’ by design, we can still use this framework\nwith highly specialized discriminator models that can achieve\nVOLUME 11, 2023 52425\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\nbetter accuracy with a limited portion of the data. However,\nin cases like this, the user has to be aware that diversity in the\noutputs will most likely be compromised.\nC. USABILITY OF OUR METHOD\nThe intent of the development of this method is to contribute\nto the field of AI-assisted compositional systems, where\nwhether the user knows the basics of music theory or not, they\nshould be able to obtain AI-generated results that are more\nlikely to fit their intentions and creative layout. However,\ndespite our method allowing users to create their own control\nsystem with very simple discriminator models and their own\ndata, it still is quite far from being accessible to the general\npublic (like the latest ‘‘text-to-image’’ models are).\nOne of the biggest issues we faced, specifically with our\n‘‘Autonomous’’ method, is that due to the scoring system used\nto select the most fitting generated sequence, the sampling\nsteps take a long time to run. Although the time needed\nto autonomously generate a piece is directly connected to\nthe width of the grid of hyperparameter values used for\nPPLM, we took less than 30 minutes to generate pieces with\n512 tokens in length (averaging about a minute of duration),\nwhere the grid of hyperparameters had 16 different combi-\nnations that were searched for every 64 tokens (8 × 16 =\n128 searches in total). In an ideal situation, a hypothetical\nuser should be able to load (or train) the discriminator model\nand set the parameters in sampling according to their will.\nSampling a sequence with a fixed set of hyperparameters does\nnot take much time, being applicable in near real-time cir-\ncumstances (about 2.5 minutes for a sequence of 512 tokens).\nHowever, the effect of each parameter is not immediately\nobvious and requires some time to adapt to. As future work,\nit would be interesting to study the hyperparameter space to\nverify whether explanations can be given to each parameter or\nnot, even across different discriminators. That would enable\nthe implementation of a near real-time system where the\nuser would be given a set of sliders where each one would\nbe assigned a more abstract (and less technical) description,\nmaking the system fitting for novices as well. Once the user\ndetermines the desired length of their generated sequence and\nthe values for the parameters indicated in the sliders, this\nmethod can output the sequence of MIDI events in a few\nseconds.\nAs a future work, we can develop a system where most of\nthe hyperparameters have their values frozen, allowing only\nthe ‘‘step size’’ to be adjusted by the user, as it determines\nhow ‘‘strong’’ the effects of the discriminator in PPLM will\nbe on the base music model.\nD. CONTROL GRANULARITY\nAs much ‘‘customization’’ the hyperparameter adjustment\ncan offer, even in the ideal context where every adjustment\nhas its effects explained to the user, there is only so much\ngranularity our model can offer when we are generating\npieces with respect to the quadrants in Russell’s model of\naffect. As the main motivation behind this work was to\ndevelop a method that can help people compose music, espe-\ncially novices, we focused our studies around the problem\nof generating music according to an emotional value, which\nrequires some level of abstraction.\nKnowing that this method was indeed able to achieve con-\ntrol over music generation in the symbolic domain, one way\nthat finer granularity of control can potentially be achieved is\nby training a set of coarser discriminator models to be used in\ncombination with different (and adjustable) hyperparameter\nsettings. For example, instead of having a single discrimina-\ntor that tells different ‘‘emotional quadrants’’ apart, we can\nuse one discriminator that outputs the probability of a piece\nbeing positive or negative (valence axis) and another one\nthat outputs the probability of it being high in energy or\nlow in energy(arousal axis). That could possibly allow us to\ngradually adjust the generation to a more specific region in\nthe Russell’s model of affect.\nAnother contribution that could potentially improve the\nuser experience in our case is adding discriminator models\nthat label/quantify more technical aspects of the musical\npieces. On top of adding another level of granularity to the\ncontrol input, this could also help the user learn what each\nacoustic or rhythmic feature is and better understand how they\naffect the perceived emotion in the final output.\nVI. CONCLUSION\nIn this work, we showed that it is possible to achieve a\nhuman-in-the-loop creative system for music composition,\nusing models and mechanisms that were previously used in\ntext generation. The involvement of a person in the gen-\nerative process calls for a more controlled method for the\ngeneration and the controls given to the user must be clear\nto them. As great of a tool AI-based generative models are\nfor some tasks when there is human participation in the act\nof ‘‘creating’’ something, they expect some level of agency\nover the process. That said, to improve the user experience\nin human×machine collaborative work, methods that allow\ncontrol over the contents being generated in a more abstract\nlevel are more likely to be picked up over those that do not\noffer that option.\nTo evaluate the efficiency of this method to manipulate\nmusic generation on its own, we designed it to rely on human\nusers as little as possible, while still experimenting a bit\nwith the collaborative framework. Realistically speaking, the\nmethod we currently use is not ideal for ‘‘real life’’ usageas\nthe process of generating multiple sequences with different\nvalues of hyperparameters and ranking based on the dataset\nused for training is time-consuming and, frankly, quite lim-\nited. An ideal design for this method (so it can be used to its\nfull potential) would let its users to manipulate the hyperpa-\nrameters themselves, so the generation would not have to be\nran multiple times unnecessarily. To make the hyperparame-\nters accessible to the users, they must understand what each\none of them does to the final product, meaning that there is\na need for explanations for each variable. By making these\nvariables understandable and meaningful to the users, the\n52426 VOLUME 11, 2023\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\ncomputation time needed for the generation will drastically\ndrop, making the implementation of a real-time application\nattainable, which consequently improves the user experience.\nREFERENCES\n[1] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski,\nand R. Liu, ‘‘Plug and play language models: A simple approach to\ncontrolled text generation,’’ Dec. 2019, arXiv:1912.02164.\n[2] S. R. Chettri, R. F. Cromp, and M. Birmingham, ‘‘Design of neu-\nral networks for classification of remotely sensed imagery,’’ Telematics\nInformat., vol. 9, nos. 3–4, pp. 145–156, Jun. 1992. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0736585305800322\n[3] L.-L. Balkwill and W. F. Thompson, ‘‘A cross-cultural investigation of\nthe perception of emotion in music: Psychophysical and cultural cues,’’\nMusic Perception, vol. 17, no. 1, pp. 43–64, Oct. 1999. [Online]. Available:\nhttp://www.jstor.org/stable/40285811\n[4] Y . Song, S. Dixon, and M. T. Pearce, ‘‘Evaluation of musical features for\nemotion classification,’’ in Proc. ISMIR, 2012, pp. 523–528.\n[5] H.-T. Hung, J. Ching, S. Doh, N. Kim, J. Nam, and Y .-H. Yang, ‘‘EMOPIA:\nA multi-modal pop piano dataset for emotion recognition and emotion-\nbased music generation,’’ Aug. 2021, arXiv:2108.01374.\n[6] L. N. Ferreira and J. Whitehead, ‘‘Learning to generate music with senti-\nment,’’ Mar. 2021, arXiv:2103.06125.\n[7] L. Ferreira, L. Lelis, and J. Whitehead, ‘‘Computer-generated music for\ntabletop role-playing games,’’ in Proc. AAAI Conf. Artif. Intell. Interact.\nDigit. Entertainment, Oct. 2020, vol. 16, no. 1, pp. 59–65.\n[8] L. N. Ferreira, L. Mou, J. Whitehead, and L. Lelis, ‘‘Controlling perceived\nemotion in symbolic music generation with Monte Carlo tree search,’’\nin Proc. AAAI Conf. Artif. Intell. Interact. Digit. Entertainment, 2022,\npp. 163–170.\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 1–11.\n[10] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, ‘‘Improv-\ning language understanding with unsupervised learning,’’ OpenAI,\nTech. Rep., 2018. [Online]. Available: https://openai.com/research/\nlanguage-unsupervised\n[11] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n‘‘Language models are unsupervised multitask learners,’’ OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[12] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,\n‘‘Generative pretraining from pixels,’’ in Proc. 37th Int. Conf. Mach.\nLearn., H. D. III and A. Singh, Eds., vol. 119, Jul. 2020, pp. 1691–1703.\n[Online]. Available: https://proceedings.mlr.press/v119/chen20s.html\n[13] W. Yan, Y . Zhang, P. Abbeel, and A. Srinivas, ‘‘VideoGPT: Video genera-\ntion using VQ-V AE and transformers,’’ Apr. 2021, arXiv:2104.10157.\n[14] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Simon,\nC. Hawthorne, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck,\n‘‘Music transformer,’’ Sep. 2018, arXiv:1809.04281.\n[15] I. Simon and S. Oore. (2017). Performance RNN: Generating\nMusic With Expressive Timing and Dynamics. [Online]. Available:\nhttps://magenta.tensorflow.org/performance-rnn\n[16] Y . Kikuchi, G. Neubig, R. Sasano, H. Takamura, and M. Okumura,\n‘‘Controlling output length in neural encoder–decoders,’’ in Proc. Conf.\nEmpirical Methods Natural Lang. Process., Austin, Texas: Association\nfor Computational Linguistics, 2016, pp. 1328–1338. [Online]. Available:\nhttps://aclanthology.org/D16-1140\n[17] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher,\n‘‘CTRL: A conditional transformer language model for controllable gen-\neration,’’ Sep. 2019, arXiv:1909.05858.\n[18] D. Pascual, B. Egressy, C. Meister, R. Cotterell, and R. Wattenhofer,\n‘‘A plug-and-play method for controlled text generation,’’ Sep. 2021,\narXiv:2109.09707.\n[19] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen,\nand I. Sutskever, ‘‘Zero-shot text-to-image generation,’’ Feb. 2021,\narXiv:2102.12092.\n[20] C. Hawthorne, A. Jaegle, C. Cangea, S. Borgeaud, C. Nash,\nM. Malinowski, S. Dieleman, O. Vinyals, M. Botvinick, I. Simon,\nH. Sheahan, N. Zeghidour, J.-B. Alayrac, J. Carreira, and J. Engel,\n‘‘General-purpose, long-context autoregressive modeling with perceiver\nAR,’’ Feb. 2022, arXiv:2202.07765.\n[21] J. Yu, Y . Xu, J. Y . Koh, T. Luong, G. Baid, Z. Wang, V . Vasudevan, A. Ku,\nY . Yang, B. K. Ayan, B. Hutchinson, W. Han, Z. Parekh, X. Li, H. Zhang,\nJ. Baldridge, and Y . Wu, ‘‘Scaling autoregressive models for content-rich\ntext-to-image generation,’’ Jun. 2022, arXiv:2206.10789.\n[22] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,\n‘‘Learning transferable visual models from natural language supervision,’’\nFeb. 2021, arXiv:2103.00020.\n[23] J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli,\n‘‘Deep unsupervised learning using nonequilibrium thermodynamics,’’\n2015, arXiv:1503.03585.\n[24] Y . Song and S. Ermon, ‘‘Generative modeling by estimating gradients of\nthe data distribution,’’ 2019, arXiv:1907.05600.\n[25] J. Ho, A. Jain, and P. Abbeel, ‘‘Denoising diffusion probabilistic models,’’\n2020, arXiv:2006.11239.\n[26] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew,\nI. Sutskever, and M. Chen, ‘‘GLIDE: Towards photorealistic image\ngeneration and editing with text-guided diffusion models,’’ Dec. 2021,\narXiv:2112.10741.\n[27] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton,\nS. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes,\nT. Salimans, J. Ho, D. J. Fleet, and M. Norouzi, ‘‘Photorealistic text-to-\nimage diffusion models with deep language understanding,’’ May 2022,\narXiv:2205.11487.\n[28] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, ‘‘Hierar-\nchical text-conditional image generation with CLIP latents,’’ Apr. 2022,\narXiv:2204.06125.\n[29] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n‘‘High-resolution image synthesis with latent diffusion models,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 10674–10685.\n[30] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever,\n‘‘Jukebox: A generative model for music,’’ Apr. 2020, arXiv:2005.00341.\n[31] Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y . Li, and D. P. W. Ellis, ‘‘MuLan:\nA joint embedding of music audio and natural language,’’ in Proc. 23rd Int.\nSoc. Music Inf. Retr. Conf. (ISMIR), 2022, pp. 1–8.\n[32] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon,\nQ. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, M. Sharifi,\nN. Zeghidour, and C. Frank, ‘‘MusicLM: Generating music from text,’’\n2023, arXiv:2301.11325.\n[33] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi,\nO. Teboul, D. Grangier, M. Tagliasacchi, and N. Zeghidour, ‘‘Audi-\noLM: A language modeling approach to audio generation,’’ 2022,\narXiv:2209.03143.\n[34] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and D. Eck, ‘‘A hierarchical\nlatent vector model for learning long-term structure in music,’’ Mar. 2018,\narXiv:1803.05428.\n[35] K. Chen, C.-I. Wang, T. Berg-Kirkpatrick, and S. Dubnov, ‘‘Music Sketch-\nNet: Controllable music generation via factorized representations of pitch\nand rhythm,’’ Aug. 2020, arXiv:2008.01291.\n[36] Y .-S. Huang and Y .-H. Yang, ‘‘Pop music transformer: Beat-based mod-\neling and generation of expressive pop piano compositions,’’ Feb. 2020,\narXiv:2002.00212.\n[37] C. Payne. (2019). Musenet, 2019. [Online]. Available: https://openai.com/\nblog/musenet\n[38] D. V on Rütte, L. Biggio, Y . Kilcher, and T. Hofmann, ‘‘FIGARO: Gen-\nerating symbolic music with fine-grained artistic control,’’ Jan. 2022,\narXiv:2201.10936.\n[39] R. Louie, A. Coenen, C. Z. Huang, M. Terry, and C. J. Cai, ‘‘Novice-\nAI music co-creation via AI-steering tools for deep generative mod-\nels,’’ in Proc. CHI Conf. Hum. Factors Comput. Syst.New York, NY ,\nUSA: Association for Computing Machinery, Apr. 2020, pp. 1–13, doi:\n10.1145/3313831.3376739.\n[40] C. Payne. (Apr. 2019). Musenet. [Online]. Available: https://openai.com/\nblog/musenet/\n[41] A. Shaw. (Aug. 2019). Creating a Pop Music Generator With the Trans-\nformer. [Online]. Available: https://towardsdatascience.com/creating-a-\npop-music-generator-with-the-transformer-5867511b382a\n[42] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A. Huang,\nS. Dieleman, E. Elsen, J. Engel, and D. Eck, ‘‘Enabling factorized\npiano music modeling and generation with the MAESTRO dataset,’’ in\nProc. Int. Conf. Learn. Represent., 2019, pp. 1–12. [Online]. Available:\nhttps://openreview.net/forum?id=r1lYRjC9F7\nVOLUME 11, 2023 52427\nN. Imasato et al.: Using a Language Model to Generate Music in Its Symbolic Domain\n[43] T. Wolf, ‘‘Transformers: State-of-the-art natural language processing,’’\nin Proc. Conf. Empirical Methods Natural Lang. Process., Syst. Demon-\nstrations, 2020, pp. 38–45. [Online]. Available: https://aclanthology.org/\n2020.emnlp-demos.6\n[44] I. Loshchilov and F. Hutter, ‘‘Decoupled weight decay regularization,’’\n2019, arXiv:1711.05101.\n[45] J. A. Russell, ‘‘A circumplex model of affect.,’’ J. Personality Social\nPsychol., vol. 39, no. 6, pp. 1161–1178, Dec. 1980.\n[46] L.-C. Yang and A. Lerch, ‘‘On the evaluation of generative models in\nmusic,’’Neural Comput. Appl., vol. 32, no. 9, pp. 4773–4784, May 2020.\nNAOMI IMASATOwas born in São Paulo, Brazil,\nin 1994. She received the B.S. degree in applied\nand computer mathematics and in systems and\ncontrol from University of São Paulo, São Paulo,\nin 2018, and the M.S. degree in engineering from\nOsaka University, Osaka, Japan, in 2021, where\nshe is currently pursuing the Ph.D. degree.\nFrom 2012 to 2014, she was a Research and\nDevelopment Technician with TOTVS, São Paulo.\nIn 2018, she joined a year-long internship program\nwith B3, São Paulo.\nKAZUKI MIYAZAWA received the M.E. degree\nin engineering from The University of Electro-\nCommunications, Tokyo, Japan, in 2019. He is\ncurrently pursuing the Ph.D. degree with the\nDepartment of System Innovation, Graduate\nSchool of Engineering Science, Osaka University,\nOsaka, Japan. He was a JSPS Research Fellowship\nfor Young Scientists, from 2019 to 2022. His\ncurrent research interests include multimodal data\nintegration, robot learning, concept formation,\nnatural language processing, and reinforcement learning.\nCAITLIN DUNCAN received the B.Sc. (Hons.)\nand Ph.D. degrees in computer science from\nthe University of Canterbury, Christchurch,\nNew Zealand, in 2013 and 2019, respectively. She\nreceived a JSPS Postdoctoral Fellowship, in 2021.\nShe is currently a Postdoctoral Fellow with the\nGraduate School of Engineering Science, Osaka\nUniversity, Osaka, Japan. She was a Lecturer and\na Teaching Assistant, from 2019 to 2020, and\na member of the Computer Science Education\nResearch Group, University of Canterbury, from 2014 to 2020. She was a\nFinalist for Young IT Professional of the Year, in the New Zealand Excellence\nin IT Awards, in 2018, for her contributions to computing education in\nschools, and computer science inclusivity initiatives.\nTAKAYUKI NAGAI(Member, IEEE) received the\nB.E., M.E., and Ph.D. degrees from the Depart-\nment of Electrical Engineering, Keio University,\nin 1993, 1995, and 1997, respectively. Since 1998,\nhe has been with The University of Electro-\nCommunications, and since 2018, he has been a\nProfessor with the Graduate School of Engineering\nScience, Osaka University. From 2002 to 2003,\nhe was a Visiting Scholar with the Department\nof Electrical Computer Engineering, University of\nCalifornia at San Diego, San Diego. He was also a specially-appointed\nProfessor with UEC AIX, a Visiting Researcher with the Tamagawa Uni-\nversity Brain Science Institute, and a Visiting Researcher with AIST AIRC.\nHis research interests include intelligent robotics, cognitive developmental\nrobotics, and robot learning. He aims at realizing flexible and general intel-\nligence, such as human by combining AI and robot technologies. He has\nreceived the IROS Best Paper Award Finalist, the Advanced Robotics Best\nPaper Award, and the JSAI Best Paper Award.\n52428 VOLUME 11, 2023"
}