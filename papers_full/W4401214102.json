{
  "title": "Comparing Large Language Models and Grammatical Evolution for Code Generation",
  "url": "https://openalex.org/W4401214102",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3110681111",
      "name": "Leonardo Lucio Custode",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": null,
      "name": "Chiara Camilla Migliore Rambaldi",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A1957840925",
      "name": "Marco Roveri",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A161487858",
      "name": "Giovanni Iacca",
      "affiliations": [
        "University of Trento"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2900121847",
    "https://openalex.org/W3183121011",
    "https://openalex.org/W4389285870",
    "https://openalex.org/W4387494819",
    "https://openalex.org/W3173383004",
    "https://openalex.org/W4281669782",
    "https://openalex.org/W4366735548",
    "https://openalex.org/W4285734663",
    "https://openalex.org/W2954383399",
    "https://openalex.org/W130232500",
    "https://openalex.org/W243424403"
  ],
  "abstract": "Code generation is one of the most valuable applications of AI, as it allows for automated programming and \"self-building\" programs. Both Large Language Models (LLMs) and evolutionary methods, such as Genetic Programming (GP) and Grammatical Evolution (GE), are known to be capable of performing code generation with reasonable performance. However, to the best of our knowledge, little work has been done so far on a systematic comparison between the two approaches. Most importantly, the only studies that conducted such comparisons used benchmarks from the GP community, which, in our opinion, may have provided possibly GP-biased results. In this work, we perform a comparison of LLMs and evolutionary methods, in particular GE, using instead a well-known benchmark originating from the LLM community. Our results show that, in this scenario, LLMs can solve significantly more tasks than GE, indicating that GE struggles to match the performance of LLMs on code generation tasks that have different properties from those commonly used in the GP community.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7597615718841553
    },
    {
      "name": "Grammatical evolution",
      "score": 0.7148056030273438
    },
    {
      "name": "Code generation",
      "score": 0.5933352708816528
    },
    {
      "name": "Programming language",
      "score": 0.5015408992767334
    },
    {
      "name": "Natural language processing",
      "score": 0.47141382098197937
    },
    {
      "name": "Code (set theory)",
      "score": 0.45977601408958435
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41311177611351013
    },
    {
      "name": "Linguistics",
      "score": 0.325980544090271
    },
    {
      "name": "Genetic programming",
      "score": 0.08960235118865967
    },
    {
      "name": "Philosophy",
      "score": 0.0590081512928009
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I193223587",
      "name": "University of Trento",
      "country": "IT"
    }
  ]
}