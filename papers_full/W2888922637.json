{
    "title": "Targeted Syntactic Evaluation of Language Models",
    "url": "https://openalex.org/W2888922637",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2700572840",
            "name": "Rebecca Marvin",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A817205692",
            "name": "Tal Linzen",
            "affiliations": [
                "Johns Hopkins University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2190736972",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2124059530",
        "https://openalex.org/W2122236285",
        "https://openalex.org/W2473344385",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2885588803",
        "https://openalex.org/W4298392964",
        "https://openalex.org/W2151222319",
        "https://openalex.org/W2251614820",
        "https://openalex.org/W1528941926",
        "https://openalex.org/W2159636675",
        "https://openalex.org/W2625014264",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2963457723",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2466736553",
        "https://openalex.org/W2415203208",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W4238252461",
        "https://openalex.org/W2118170533",
        "https://openalex.org/W2531882892",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W2592205058",
        "https://openalex.org/W2088177607",
        "https://openalex.org/W2728192282",
        "https://openalex.org/W2600110521",
        "https://openalex.org/W2137002739",
        "https://openalex.org/W2129914903",
        "https://openalex.org/W2157889740",
        "https://openalex.org/W2951279274",
        "https://openalex.org/W3104379732",
        "https://openalex.org/W2160073299",
        "https://openalex.org/W2963025830",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963015836",
        "https://openalex.org/W4300427683",
        "https://openalex.org/W2962911926",
        "https://openalex.org/W2123893795",
        "https://openalex.org/W1818785862",
        "https://openalex.org/W2798727047",
        "https://openalex.org/W2963748792",
        "https://openalex.org/W2111780752"
    ],
    "abstract": "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM’s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
    "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n1192\nTargeted Syntactic Evaluation of Language Models\nRebecca Marvin\nDepartment of Computer Science\nJohns Hopkins University\nbecky@jhu.edu\nTal Linzen\nDepartment of Cognitive Science\nJohns Hopkins University\ntal.linzen@jhu.edu\nAbstract\nWe present a dataset for evaluating the gram-\nmaticality of the predictions of a language\nmodel. We automatically construct a large\nnumber of minimally different pairs of En-\nglish sentences, each consisting of a gram-\nmatical and an ungrammatical sentence. The\nsentence pairs represent different variations of\nstructure-sensitive phenomena: subject-verb\nagreement, reﬂexive anaphora and negative\npolarity items. We expect a language model\nto assign a higher probability to the grammati-\ncal sentence than the ungrammatical one. In an\nexperiment using this data set, an LSTM lan-\nguage model performed poorly on many of the\nconstructions. Multi-task training with a syn-\ntactic objective (CCG supertagging) improved\nthe LSTM’s accuracy, but a large gap remained\nbetween its performance and the accuracy of\nhuman participants recruited online. This sug-\ngests that there is considerable room for im-\nprovement over LSTMs in capturing syntax in\na language model.\n1 Introduction\nA language model (LM) deﬁnes a probability dis-\ntribution over sequences of words. Recent techno-\nlogical advances have led to an explosion of neural\nnetwork-based LM architectures. The most pop-\nular ones are based on recurrent neural networks\n(RNNs) (Elman, 1990; Mikolov et al., 2010),\nin particular Long Short-Term Memory networks\n(LSTMs) (Hochreiter and Schmidhuber, 1997).\nWhile a large number of alternative architectures\nhave been proposed in the past few years, LSTMs\nare still highly competitive (Melis et al., 2018).\nLanguage models are typically evaluated using\nperplexity: it is considered desirable for an LM\nto assign a high probability to held-out data from\nthe same corpus as the training data. This mea-\nsure conﬂates multiple sources of success (or fail-\nure) in predicting the next word: common collo-\ncations, semantics, pragmatics, syntax, and so on.\nThe quality of the syntactic predictions made by\nthe LM is arguably particularly difﬁcult to mea-\nsure using perplexity: since most sentences are\ngrammatically simple and most words can be pre-\ndicted from their local context, perplexity rewards\nLMs primarily for collocational and semantic pre-\ndictions.\nWe propose to supplement perplexity with a\nmetric that assesses whether the probability dis-\ntribution deﬁned by the model conforms to the\ngrammar of the language. Following previous\nwork (Lau et al., 2017; Linzen et al., 2016; Gu-\nlordava et al., 2018), we suggest that given two\nsentences that differ minimally from each other,\none of which is grammatical and the other which is\nnot, it is desirable for the model to assign a higher\nprobability to the grammatical one.\nThe value of this approach can be illustrated\nwith a recent study by Tran et al. (2018), where a\nstandard LSTM language model was compared to\nan attention-only LM without recurrence (Vaswani\net al., 2017). Although the attention-only model\nhad somewhat better perplexity on the valida-\ntion set, when the models were tested speciﬁcally\non challenging subject-verb agreement dependen-\ncies, the attention-only model made three times\nas many errors as the LSTM. In other words, the\nLSTM learned more robust syntactic representa-\ntions, but this advantage was not reﬂected in its av-\nerage perplexity on the corpus, since syntactically\nchallenging sentences are relatively infrequent.\nPrevious work on targeted syntactic evaluation\nof language models has identiﬁed syntactically\nchallenging sentences in corpora (Linzen et al.,\n2016; Gulordava et al., 2018). While evaluation\non naturally occurring examples is appealing, this\napproach has its limitations (see Section 2). In\nparticular, syntactically challenging examples are\nsparsely represented in a corpus, their identiﬁca-\n1193\ntion requires a clean parsed corpus, and naturally\noccurring sentences are difﬁcult to control for con-\nfounds. We contrast the naturalistic approach with\na constructed dataset, which allows us to exam-\nine a much larger range of speciﬁc grammatical\nphenomena than has been possible before. We\nuse templates to automatically create our test sen-\ntences, making it possible to generate a large test\nset while maintaining experimental control over\nour materials as well as a balanced number of ex-\namples of each phenomenon.\nWe test three LMs on the data set we develop:\nan n-gram baseline, an RNN LM trained on an\nunannotated corpus, and an RNN LM trained on a\nmultitask objective: language modeling and Com-\nbinatory Categorial Grammar (CCG) supertagging\n(Bangalore and Joshi, 1999). We also conduct\na human experiment using the same materials.\nThe n-gram baseline largely performed at chance,\nsuggesting that good performance on the task re-\nquires syntactic representations. The RNN LMs\nperformed well on simple cases, but struggled on\nmore complex ones. Multi-task training with a su-\npervised syntactic objective improved the perfor-\nmance of the RNN, but it was still much weaker\nthan humans. This suggests that our data set is\nchallenging, especially when explicit syntactic su-\npervision is not available, and can therefore moti-\nvate richer language modeling architectures.\n2 Overview of the approach\n2.1 Grammaticality and LM probability\nHow should grammaticality be captured in the\nprobability distribution deﬁned by an LM? The\nmost extreme position would be that a language\nmodel should assign a probability of zero to un-\ngrammatical sentences. For most applications,\nsome degree of error tolerance is desirable, and\nit is not practical to assign a sentence a proba-\nbility of exactly zero. 1 Following Linzen et al.\n(2016) and Gulordava et al. (2018), our desider-\natum for the language model is more modest: if\ntwo closely matched sentence differ only in their\ngrammaticality, the probability of the grammati-\ncal sentence should be higher than the probability\nof the ungrammatical one. For example, the fol-\nlowing minimal pair illustrates the fact that third-\n1Nor is it possible to have a thresholdϵ such that all gram-\nmatical sentences have probability higher than ϵ and all un-\ngrammatical sentences have probability lower than ϵ, for the\nsimple reason that there is an inﬁnite number of grammatical\nsentences (Lau et al., 2017).\nperson present English verbs agree with the num-\nber of their subject:\n(1) Simple agreement:\na. The author laughs .\nb. *The author laugh.\nWe expect the probability of (1a) to be higher than\nthe probability of (1b). Previous work has simpli-\nﬁed this setting further by comparing the proba-\nbility that the LM assigns to a single word that is\nthe locus of ungrammaticality. In (1), for exam-\nple, the LM would be fed the ﬁrst two words of\nthe sentence, and would be considered successful\non the task if it predicts P(laughs) > P(laugh).\nThe prediction setting is only applicable when\nthe locus of ungrammaticality is a single word,\nrather than, say, the interaction between two\nwords; moreover, the information needed to make\nthe grammaticality decision needs to be available\nin the left context of the locus of grammaticality.\nThese conditions do not always hold. Negative po-\nlarity items (NPIs), for example, are words like\nany and ever that can only be used in the scope\nof negation.2 The grammaticality of placing a par-\nticular quantiﬁer in the beginning of the sentences\nin (2) depends on whether the sentence contains\nan NPI later on:\n(2) Simple NPI:\na. No students have ever lived here.\nb. *Moststudents have ever lived here.\nIt would not be possible to compare these two sen-\ntences using the prediction task. In the current pa-\nper, we use the more general setting and compare\nthe probability of the two complete sentences.\n2.2 Data set construction\nPrevious work has used syntactically complex sen-\ntences identiﬁed from a parsed corpus. This ap-\nproach has several limitations. If the corpus is\nautomatically parsed, the risk of a parse error in-\ncreases with the complexity of the construction\n(Bender et al., 2011). If the test set is restricted\nto sentences with gold parses, it can be difﬁcult\nor impossible to ﬁnd a sufﬁcient number of exam-\nples of syntactically challenging cases. Moreover,\nusing naturally occurring sentences can introduce\n2In practice, the conditions that govern the distribution of\nNPIs are much more complicated, but this ﬁrst approxima-\ntion will sufﬁce for the present purposes. For a review, see\nGiannakidou (2011).\n1194\nconfounds that may complicate the interpretation\nof the experiments (Ettinger et al., 2018).\nTo circumvent these issues, we use templates\nto automatically construct a large number of En-\nglish sentence pairs ( ∼350,000). Our data set in-\ncludes three phenomena that linguists consider to\nbe sensitive to hierarchical syntactic structure (Ev-\neraert et al., 2015; Xiang et al., 2009): subject-\nverb agreement (described in detail in Sections 4.1\nand 4.2), reﬂexive anaphora (Section 4.3) and neg-\native polarity items (Section 4.4).\nThe templates can be described using non-\nrecursive context-free grammars. We specify the\npreterminal symbols that make up a syntactic con-\nstruction and have different terminal symbols that\nthose preterminals could be mapped to. For ex-\nample, the template for the simple agreement con-\nstruction illustrated in (1) consists of the following\nrules:\n(3) a. Simple agreement →D MS MV\nb. D →the\nc. MS →{author, pilot, . . .}\nd. MV →{laughs, smiles, . . .}\nWe generate all possible combinations of the ter-\nminals. The Supplementary Materials provide a\nfull description of all our templates.3\nWhile these examples are somewhat artiﬁcial,\nour goal is to isolate the syntactic capabilities of\nthe model; it is in fact beneﬁcial to minimize the\nsemantic or collocational cues that can be used\nto identify the grammatical sentence. Gulordava\net al. took this approach further and constructed\n“colorless green ideas” test cases by substituting\nrandom content words into sentences from a cor-\npus. We take a more moderate position and avoid\ncombinations that are very implausible or violate\nselectional restrictions (e.g.,the apple laughs). We\ndo this by having separate templates for animate\nand inanimate subjects and verbs so that the re-\nsulting sentences are always reasonably plausible.\n3 Related work\nTargeted evaluation: LM evaluation data sets\nusing challenging prediction tasks have been pro-\nposed in the context of semantics and discourse\ncomprehension (Zweig and Burges, 2011; Paperno\net al., 2016). Evaluation sets consisting of chal-\n3The code, the data set and the Supplementary\nMaterials can be found at https://github.com/\nBeckyMarvin/LM_syneval.\nlenging syntactic constructions have been con-\nstructed for parser evaluation (Rimell et al., 2009;\nNivre et al., 2010; Bender et al., 2011), and mini-\nmal pair approaches have been proposed for eval-\nuating image captioning (Shekhar et al., 2017) and\nmachine translation systems (Sennrich, 2017), but\nno data sets exist that target a range of syntactic\nconstructions for language model evaluation.\nAcceptability judgments: Lau et al. (2017)\ncompared the ability of different LMs to pre-\ndict graded human acceptability judgments. The\nforced-choice approach used in the current pa-\nper has been shown to be effective in human\nacceptability judgment experiments (Sprouse and\nAlmeida, 2017). In some early work, neural net-\nworks were trained explicitly to predict acceptabil-\nity judgments (Lawrence et al., 1996; Allen and\nSeidenberg, 1999); Post (2011) likewise trained a\nclassiﬁer on top of a parser to predict grammatical-\nity. Warstadt et al. (2018) use a transfer learning\napproach, where an unsupervised model is ﬁne-\ntuned on acceptability prediction. Our work dif-\nfers from those studies in that we do not advocate\nproviding any explicit grammaticality signal to the\nLM at any point (“no negative evidence”).\nSyntax in LMs: There have been several pro-\nposals over the years to incorporate explicit syn-\ntax into LMs to overcome the inability of n-gram\nLMs to model long-distance dependencies (Juraf-\nsky et al., 1995; Roark, 2001; Pauls and Klein,\n2012). While RNN language models can in prin-\nciple model longer dependencies (Mikolov et al.,\n2010; Linzen et al., 2016), in practice it can still be\nbeneﬁcial to inject syntax into the model. This can\nbe done by combining it with a supervised parser\n(Dyer et al., 2016) or other multi-task learning ob-\njectives (Enguehard et al., 2017). Our work is or-\nthogonal to this area of research, but can be seen\nas providing a potential opportunity to underscore\nthe advantage of such syntax-infused models.\n4 Data set composition\nThis section describes all of the types of sentence\npairs included in our data set, which include exam-\nples of subject-verb agreement (Sections 4.1 and\n4.2), reﬂexive anaphoras (Section 4.3) and nega-\ntive polarity items (Section 4.4).\n1195\n4.1 Subject-verb agreement\nDetermining the correct number of the verb is triv-\nial in examples such as (1) above, in which the\nsentence only contains a single noun. By contrast,\nin cases where there are multiple nouns in the sen-\ntence, identifying which of them is the subject of a\ngiven verb requires understanding the structure of\nthe sentence. In particular, the relevant subject is\nnot necessarily the ﬁrst noun of the sentence:\n(4) Agreement in a sentential complement:\na. The bankers knew the ofﬁcer smiles .\nb. *The bankers knew the ofﬁcer smile.\nHere the verb smiles needs to agree with the em-\nbedded subject ofﬁcer rather than the main clause\nsubject bankers. The subject is also not necessar-\nily the most recent noun before the verb: when\nthe subject is modiﬁed by a phrase, a distracting\nnoun (“attractor”) often intervenes in the linear or-\nder of the sentence between the head of the subject\nand the verb. Two examples of such modiﬁers are\nprepositional phrases and relative clauses (RCs):\n(5) Agreement across a prepositional phrase:\na. The farmer near the parents smiles .\nb. *The farmer near the parents smile.\n(6) Agreement across a subject relative clause:\na. The ofﬁcers that love the skater smile .\nb. *The ofﬁcers that love the skater smiles.\nWe include all four possible conﬁgurations of\nnoun number for each type of minimal pair;\nfor (5), these would be:4\n(7) a. The farmer near the parent smiles/*smile.\nb. The farmer near the parents smiles/*smile.\nc. The farmers near the parent smile/*smiles.\nd. The farmers near the parents\nsmile/*smiles.\nSentences where the two nouns conﬂict in num-\nber are expected to be more challenging, but in-\nterpretable errors may certainly occur even when\nthey do not. For example, the model may use\nthe heuristic that sentences with multiple nouns\nare likely to have a plural verb (a heuristic that\n4The slash notation indicates the word that differs be-\ntween the grammatical and ungrammatical sentence; for ex-\nample, in (7a), the full sentence pair would be:\n(i) a. The farmer near the parent smiles.\nb. *The farmer near the parent smile.\nwould be effective for coordination); alternatively,\nit might prefer singular verbs to plural ones regard-\nless of whether the subject is singular or plural,\nsimply because the singular form of the verb is\nmore frequent.\nNext, in verb phrase (VP) coordination, both of\nthe verbs need to agree with the subject:\n(8) Short VP coordination:\na. The senator smiles and laughs .\nb. *The senator smiles and laugh.\nWe had both singular and plural subjects. The\nnumber of the verb immediately adjacent to the\nsubject was always grammatical. This problem\ncan in principle be solved with a trigram model\n(smiles and laughs is likely to be a more frequent\ntrigram than smiles and laugh); to address this po-\ntential concern, we also included a coordination\ncondition with a longer dependency:\n(9) Long VP coordination:\nThe manager writes in a journal every day and\nlikes/*like to watch television shows.\n4.2 Agreement and object relative clauses\nWe go into greater depth in object relative clauses,\nwhich most clearly require a hierarchical represen-\ntation. In (10) and (11), the model needs to be\nable to distinguish the embedded subject (parents)\nfrom the main clause subject (farmer) when mak-\ning its predictions:\n(10) Agreement across an object relative clause:\na. The farmer that the parents love swims .\nb. *The farmer that the parents love swim.\n(11) Agreement in an object relative clause:\na. The farmer that the parents love swims.\nb. *The farmer that the parents loves swims.\nIn keeping with the minimal pair approach, we\nnever introduce two agreement errors at the same\ntime: either the embedded verb or the main verb is\nincorrectly inﬂected, but not both.\nWe include a number of variations on the pat-\ntern in (11). First, we delete the relativizer that,\nwith the hypothesis that the absence of an overt\ncue to structure will make the task more difﬁcult:\n(12) The farmer the parents love/*loves swims.\nIn another condition, we replace the main sub-\nject with an inanimate noun and keep the embed-\n1196\nded subject animate. We base this manipulation\non human experimental work showing that sim-\nilar nouns (for example, two animate nouns) are\nmore likely to cause confusion during comprehen-\nsion than dissimilar nouns, such as an animate and\nan inanimate noun (Van Dyke, 2007):\n(13) The movies that the author likes are/*is good.\nFor a complete list of all the types of minimal pairs\nwe include, see the Supplementary Materials.\n4.3 Reﬂexive anaphora\nA reﬂexive pronoun such as himself needs to have\nan antecedent from which it derives its interpreta-\ntion. The pronoun needs to agree in number (and\ngender) with its antecedent:\n(14) Simple reﬂexive:\na. The senators embarrassed themselves.\nb. *The senators embarrassed herself.\nThere are structural conditions on the nouns to\nwhich a reﬂexive pronoun can be bound. One of\nthese conditions requires the antecedent to be in\nthe same clause as the reﬂexive pronoun. For ex-\nample, (15b) cannot refer to a context in whichthe\npilot embarrassed the bankers:\n(15) Reﬂexive in a sentential complement:\na. The bankers thought the pilot embar-\nrassed himself.\nb. *The bankers thought the pilot embar-\nrassed themselves.\nLikewise, in the following minimal pair, sentence\n(16b) is ungrammatical, because the reﬂexive pro-\nnoun themselves, which is part of the main clause,\ncannot be bound to the noun phrase the architects,\nwhich is inside an embedded clause:\n(16) Reﬂexive across an object relative clause:\na. The manager that the architects like\ndoubted himself.\nb. *The manager that the architects like\ndoubted themselves.\n4.4 Negative polarity items\nNegative polarity items, introduced in example (2)\nabove, are words that (to a ﬁrst approximation)\nneed to occur in the context of negation. Crucially\nfor the purposes of the present work, the scope\nof negation is structurally deﬁned. In particular\nthe negative noun phrase needs to c-command the\nNPI: the syntactic non-terminal node that domi-\nnates the negative noun phrase must also domi-\nnate the NPI. This is the case in (17a), but not\nin (17b), where the negative noun phrase is too\ndeep in the tree to c-command the NPIever (Xiang\net al., 2009; Everaert et al., 2015).\n(17) NPI across a relative clause:\na. No authors that the security guards like\nhave ever been famous.\nb. *The authors that no security guards like\nhave ever been famous.\nAll of the nouns and verbs in the NPI cases were\nplural. As in some of the agreement cases, we in-\ncluded a variant of (17) in which the subject was\ninanimate.\n5 Experimental setup\nTo show how our challenge set can be used to eval-\nuate the syntactic performance of LMs, we trained\nthree LMs with increasing levels of syntactic so-\nphistication. All of the LMs were trained on a\n90 million word subset of Wikipedia (Gulordava\net al., 2018). Our n-gram LM and LSTM LM do\nnot require annotated data. The third model is also\nan LSTM LM, but it requires syntactically anno-\ntated data (CCG supertags).\nN-gram model: We trained a 5-gram model on\nthe same 90M word corpus using the SRILM\ntoolkit (Stolcke, 2002) which backs off to smaller\nn-grams using Kneser-Ney smoothing.\nSingle-task RNN: The RNN LM had two layers\nof 650 LSTM units, a batch size of 128, a dropout\nrate of 0.2, and a learning rate of 20.0, and was\ntrained for 40 epochs (following the hyperparam-\neters of Gulordava et al. 2018).\nMulti-task RNN: In multi-task learning, the\nsystem is trained to optimize an objective func-\ntion that combines the objective functions of sev-\neral tasks. We combine language modeling with\nCCG supertagging, a task that predicts for each\nword in the sentence its CCG supertag (Bangalore\nand Joshi, 1999; Lewis et al., 2016). We sim-\nply sum the two objective functions with equal\nweights (Enguehard et al., 2017). Early stopping\nin this model is based on the combined loss on\nlanguage modeling and supertagging. Supertags\nprovide a large amount of syntactic information\n1197\nabout the word; the sequence of supertags of a\nsentence strongly constrains the possible parses of\nthe sentence. We use supertagging as a “scaffold”\ntask (Swayamdipta et al., 2017): our goal is not to\nproduce a competitive supertagger, but to induce\nbetter syntactic representations, which would then\nlead to improved language modeling. We used\nCCG-Bank (Hockenmaier and Steedman, 2007) as\nour CCG corpus.\nHuman evaluation: We designed a human ex-\nperiment on Amazon Mechanical Turk that mir-\nrored the task that was given to the LMs: both ver-\nsions of a minimal pair were shown on the screen\nat the same time, and participants were asked to\njudge which one of them was more acceptable (for\ndetails, see the Supplementary Materials). We em-\nphasize that we do not see human performance on\ncomplex syntactic dependencies as setting an up-\nper bound on the performance that we should ex-\npect from an LM. There is a rich literature showing\nthat humans make mistakes such as subject-verb\nagreement errors; in fact, most of the phenomena\nwe test were inspired by work in psycholinguistics\nthat studies these errors (Bock and Miller, 1991;\nPhillips et al., 2011). At the same time, while we\ndo not see a reason not to aspire for 100% accu-\nracy, we are interested in comparing LM and hu-\nman errors: if the errors are similar, the two sys-\ntems may be using similar representations.\n6 Results\nLocal agreement: The overall accuracy per\ncondition can be seen in Table 1. The n-gram\nLM’s accuracy was only 79% for simple agree-\nment and agreement in a sentential complement,\nboth of which can be solved entirely using local\ncontext. This is because not all subject and verb\ncombinations in our materials appeared verbatim\nin the 90M word training corpus; for those combi-\nnations, the model fell back on unigram probabili-\nties, which in this context amounts to selecting the\nmore frequent form of the verb.\nBoth RNNs performed much better than the\nn-gram model on the simple agreement case\n(single-task: 94%; multi-task: 100%), reﬂecting\nthese models’ ability to generalize beyond the spe-\nciﬁc bigrams that occurred in the corpus. Ac-\ncuracy on agreement in a sentential complement\nwas also very high (single-task: 99%; multi-task:\n93%). This indicates that the RNNs do not rely on\nthe heuristic whereby the ﬁrst noun of the sentence\nis likely to be its subject. They did slightly worse\nbut still very well on short VP coordination (both\n90%); this dependency is also local, albeit across\nthe word and.\nNon-local agreement: The accuracy of the\nn-gram model on non-local dependencies (long\nVP coordination and agreement across a phrase\nor a clause) was very close to 50%. This sug-\ngests that local collocational information is not\nuseful in these conditions. The single-task RNN\nalso performed much more poorly on these con-\nditions than on the local agreement conditions,\nthough for the most part its accuracy was better\nthan chance. Humans did worse on these depen-\ndencies as well, but their accuracy did not drop as\nsharply as the RNNs’ (human accuracies ranged\nfrom 82% to 88%). In most of these cases, multi-\ntask learning was very helpful; for example, accu-\nracy in long VP coordination increased from 61%\nto 81%. Still, both RNNs performed poorly on\nagreement across an object RC, especially with-\nout that, whereas humans performed comparably\non all non-local dependencies.\nAgreement inside an object RC: This case is\nparticularly interesting, because this dependency\nis purely local (see (11)), and the interference is\nfrom the distant sentence-initial noun. Although\nthis conﬁguration is similar to the sentential com-\nplement case, performance was worse both in\nRNNs and humans. However, RNNs performed\nbetter than humans, at least when the sentence in-\ncluded the overt relativizerthat. This suggests that\ninterference is sensitive to proximity in RNNs but\nto syntactic status in humans — humans appear to\nbe confusing the main clause subject and the em-\nbedded subject (Wagers et al., 2009).\nReﬂexive anaphora: The RNNs’ performance\nwas signiﬁcantly worse on simple reﬂexives (83%)\nthan on simple agreement (94%), and did not dif-\nfer between the single-task and multi-task mod-\nels. By contrast, human performance did not dif-\nfer between subject-verb agreement and reﬂexive\nanaphoras. The surprisingly poor performance for\nthis adjacent dependency seems to be due to an\nasymmetry in accuracy betweenhimself and them-\nselves on the one hand (100% accuracy in the\nmulti-task RNN) and herself on the other hand\n(49% accuracy).5 Accuracy was very low for all\n5This may be because himself and themselves are signiﬁ-\ncantly more frequent than herself, and consequently the num-\n1198\nRNN Multitask n-gram Humans # sents\nSUBJECT-VERB AGREEMENT:\nSimple 0.94 1.00 0.79 0.96 280\nIn a sentential complement 0.99 0.93 0.79 0.93 3360\nShort VP coordination 0.90 0.90 0.51 0.94 1680\nLong VP coordination 0.61 0.81 0.50 0.82 800\nAcross a prepositional phrase 0.57 0.69 0.50 0.85 44800\nAcross a subject relative clause 0.56 0.74 0.50 0.88 22400\nAcross an object relative clause 0.50 0.57 0.50 0.85 44800\nAcross an object relative (nothat) 0.52 0.52 0.50 0.82 44800\nIn an object relative clause 0.84 0.89 0.50 0.78 44800\nIn an object relative (nothat) 0.71 0.81 0.50 0.79 44800\nREFLEXIVE ANAPHORA:\nSimple 0.83 0.86 0.50 0.96 560\nIn a sentential complement 0.86 0.83 0.50 0.91 6720\nAcross a relative clause 0.55 0.56 0.50 0.87 44800\nNEGATIVE POLARITY ITEMS:\nSimple 0.40 0.48 0.06 0.98 792\nAcross a relative clause 0.41 0.73 0.60 0.81 31680\nTable 1: Overall accuracies for the LSTMs, n-gram model and humans on each test case.\npronouns in the structurally complex case in which\nthe dependency was across a relative clause (55%\ncompared to 87% in humans).\nNPIs: The dependency in simple NPIs spans\nonly four words, so the n-gram model could in\nprinciple capture it. In practice, the n-gram model\nsystematically selected the wrong answer, sug-\ngesting that it backed off to comparing the bi-\ngrams no students and most students, the ﬁrst of\nwhich is presumably less frequent. Surprisingly,\nthe n-gram model’s accuracy was higher than 50%\non NPIs across a relative clause, a dependency that\nspans more than ﬁve words. In this case, the bi-\ngrams that the and the chef (for example) happen\nto be more frequent than the that no and no chef.\nThis difference was apparently strong enough to\nmake up for the low-frequency bigram at the start\nof the sentence.\nThe RNNs did poorly on this task. The accu-\nracy of the single-task model was around 40%.\nThe multi-task did somewhat better on the simple\nNPIs (48%) and much better on the NPIs across a\nrelative clause (73%). At the same time, an exam-\nination of the plot of log probability of each word\nin a sentence (Figure A.1 in the Supplementary\nMaterials) suggests that the single-task RNN is in\nber representation learned for herself was not robust. An-\nother possibility is that gender bias reduces the probability\nof an anaphoric relation between herself and words such as\nsurgeon (Rudinger et al., 2018).\nfact able to differentiate between the grammatical\nand ungrammatical sentences when it reaches the\nNPI, but this difference does not offset the overall\nprobability advantage of the ungrammatical sen-\ntence (which is likely due to non-grammatical col-\nlocational factors). In any case, the fact that the\nn-gram baseline did not perform at chance sug-\ngests that there are non-syntactic cues to this task,\ncomplicating the interpretation of the performance\nof other LMs.\nPerplexity: The perplexity of the n-gram model\non the Wikipedia test data was 157.5, much higher\nthan the perplexity of the single-task RNN (78.65)\nand the multi-task RNN (61.10). In other words,\nperplexity tracked accuracy on our syntactic data\nset – an unsatisfying outcome given our goal of\ndissociating perplexity and our syntactic evalua-\ntion method, but an expected one given that each\nmodel was conditioned on richer information than\nthe previous one. In previous work, perplexity and\nsyntactic judgment accuracy have been found to\nbe partly dissociable (Kuncoro et al., 2018; Tran\net al., 2018).\nLexical variation and frequency: There was\nconsiderable lexical variation in the results; we\nhave mentioned the surprising asymmetry be-\ntween himself and herself above. As another\ncase study, we examine variation in the results\nof the simple agreement condition in the single-\n1199\nMain Embedded Single-task Multi-task Humans Example sentence\nsubject subject\nAcross an objective relative clause:\nSingular Singular 0.83 0.77 0.96 The author that the minister likes laughs/*laugh.\nSingular Plural 0.51 0.30 0.90 The author that the ministers like laughs/*laugh.\nPlural Singular 0.18 0.53 0.77 The authors that the minister likes laugh/*laughs.\nPlural Plural 0.50 0.73 0.80 The authors that the ministers like laugh/*laughs.\nWithin an objective relative clause:\nSingular Singular 0.73 0.92 0.94 The author that the minister likes/*like laughs.\nSingular Plural 0.91 0.81 0.72 The author that the ministers like/*likes laugh.\nPlural Singular 0.81 0.97 0.73 The authors that the minister likes/*like laugh.\nPlural Plural 0.87 0.84 0.76 The authors that the ministers like/*likes laugh.\nTable 2: Accuracy within and across an object relative clause (only in the cases in which the main subject was\nanimate and the relativizer that was present). The subject that the verb is expected to agree with is underlined.\ntask RNN. Accuracy varied by verb, ranging from\nis and are, which had 100% accuracy, to swims,\nwhere accuracy was only 60% (recall that average\naccuracy was 94%). This may be a frequency ef-\nfect: either the LM is learning less robust number\nrepresentations for infrequent verbs, or the tail of\nthe distribution over the vocabulary is more frag-\nile during word prediction. Pauls and Klein (2012)\npropose normalizing for unigram frequency when\nderiving acceptability judgments from an LM. Our\npreliminary experiments with this method did not\nsigniﬁcantly improve overall performance; regard-\nless of the effectiveness of this method, such cor-\nrections should arguably not be necessary in an\nLM that adequately captures grammaticality.\n7 Case study: agreement and object\nrelative clauses\nThe overall results in Table 1 were averaged over\nall of the possible number conﬁgurations within\neach condition. In this section, we take a closer\nlook at agreement in sentences with an object RC\n(see Table 2). This kind of ﬁner-grained analy-\nsis helps explain the cases in which the LMs are\nfailing, and might reveal some of the patterns or\nheuristics the LMs are using.\nPerformance in agreement across an object RC\nwas poor. Both RNNs made attraction errors: they\noften preferred the verb that agreed in number with\nthe irrelevant embedded subject to the verb that\nagreed with the correct main subject. The multi-\ntask RNN showed greater symmetry between the\nsimpler singular/singular and plural/plural cases,\nwhereas the single-task RNN performed poorly\neven in these cases, often preferring a singular\nverb when both subjects were plural. This default\npreference for singular verbs matches the behavior\nof younger children (Franck et al., 2004).\nPerformance in agreement within an object RC\nwas better; still, the single-task RNN made the\nmost errors when both subjects were singular, per-\nhaps due to a heuristic in which a sentence with\nmultiple subjects is likely to have a plural verb (as\nin coordination sentences). By contrast, the multi-\ntask model seemed to have a general bias towards\nsingular subjects in this condition. Incidentally,\nthe human results with object RCs were also unex-\npected: while attraction errors when the two sub-\njects differ in number are to be expected (Wagers\net al., 2009), our participants made a sizable num-\nber of errors even when both subjects were plural.\nDespite the generally poor performance in ob-\nject RCs, Figures A.2 and A.3 in the Supple-\nmentary Materials show that the single-task RNN\nis typically assigning a higher probability to the\ngrammatical word of a minimal pair than to the\nungrammatical word.\n8 Discussion\nWe have described a template-based data set for\ntargeted syntactic evaluation of language models.\nThe data set consists of pairs of sentences that are\nmatched except for their grammaticality; we con-\nsider a language model to capture the relevant as-\npects of the grammar of the language if it assigns\na higher probability to the grammatical sentence\nthan to the ungrammatical one.\nAn RNN language model performed very well\non local subject-verb agreement dependencies,\nsigniﬁcantly outperforming an n-gram baseline.\n1200\nThis suggests that the task is a viable evalua-\ntion strategy. Even on simple cases, however,\nthe RNN’s accuracy was sensitive to the partic-\nular lexical items that occurred in the sentence;\nthis would not be expected if its syntactic repre-\nsentations were fully abstract. The RNN’s per-\nformance degraded markedly on non-local depen-\ndencies, approaching chance levels on agreement\nacross an object relative clause. Multi-task train-\ning with a syntactic objective (CCG supertagging)\nmitigated this drop in performance for some but\nnot all of the dependencies we tested. We con-\njecture that the beneﬁts of the inductive bias con-\nferred by multi-task learning will be ampliﬁed\nwhen the amount of training data is limited.\nOur results contrast with the results of Gulor-\ndava et al. (2018), who obtained a prediction accu-\nracy of 81% on English sentences from their test\ncorpus and 74% on constructed sentences modeled\nafter sentences from the corpus. It is likely that our\nsentences are more syntactically challenging than\nthe ones they were able to ﬁnd in the relatively\nsmall manually annotated treebank they used.\nOne limitation of our approach is that it is not\nalways clear what constitutes a minimal grammati-\ncality contrast. In the subject-verb agreement case,\nthe contrast was clear: the two present-tense forms\nof the verb, e.g., laugh vs. laughs. Our NPI ma-\nnipulations, on the other hand, were less success-\nful: the members of the contrasts differed not only\nin their syntactic structure but also in low-level\nn-gram probabilities, making the performance on\nthis particular contrast harder to interpret.\nWe emphasize that the goal of this article was\nnot to advocate for LSTMs in particular as an ef-\nfective architecture for modeling syntax; indeed,\nour results show that LSTM language models are\nfar from matching naive annotators’ performance\non this task, let alone performing at 100% accu-\nracy. We hope that our data set, and future ex-\ntensions to other phenomena and languages, will\nmake it possible to measure progress in syntactic\nlanguage modeling and will lead to better under-\nstanding of the syntactic generalizations captured\nby language models.\n9 Acknowledgments\nWe would like to thank Ming Xiang for sharing\nmaterials from human experiments that inspired\nmany of our test cases. We also thank Brian Roark\nand the JHU Computational Psycholinguistics lab\nfor discussion, and Brian Leonard for help con-\nducting the human experiment.\nReferences\nJoseph Allen and Mark S. Seidenberg. 1999. The\nemergence of grammaticality in connectionist net-\nworks. In Brian MacWhinney, editor, Emergentist\napproaches to language: Proceedings of the 28th\nCarnegie symposium on cognition , pages 115–151.\nMahwah, NJ: Lawrence Erlbaum Associates.\nSrinivas Bangalore and Aravind K. Joshi. 1999. Su-\npertagging: An approach to almost parsing. Com-\nputational Linguistics, 25(2):237–265.\nEmily M. Bender, Dan Flickinger, Stephan Oepen, and\nYi Zhang. 2011. Parser evaluation over local and\nnon-local deep dependencies in a large corpus. In\nProceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing , pages\n397–408. Association for Computational Linguis-\ntics.\nKathryn Bock and Carol A. Miller. 1991. Broken\nagreement. Cognitive Psychology, 23(1):45–93.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural net-\nwork grammars. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 199–209. Association for\nComputational Linguistics.\nJeffrey L. Elman. 1990. Finding structure in time.\nCognitive Science, 14(2):179–211.\n´Emile Enguehard, Yoav Goldberg, and Tal Linzen.\n2017. Exploring the syntactic abilities of RNNs\nwith multi-task learning. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) , pages 3–14. Association\nfor Computational Linguistics.\nAllyson Ettinger, Ahmed Elgohary, Colin Phillips, and\nPhilip Resnik. 2018. Assessing composition in sen-\ntence vector representations. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 1790–1801. Association for Com-\nputational Linguistics.\nMartin B. H. Everaert, Marinus A. C. Huybregts, Noam\nChomsky, Robert C. Berwick, and Johan J. Bolhuis.\n2015. Structures, not strings: Linguistics as part\nof the cognitive sciences. Trends in Cognitive Sci-\nences, 19(12):729–743.\nJulie Franck, Stephany Cronel-Ohayon, Laurence\nChillier, Ulrich H. Frauenfelder, Cornelia Hamann,\nLuigi Rizzi, and Pascal Zesiger. 2004. Normal and\npathological development of subject–verb agree-\nment in speech production: A study on French chil-\ndren. Journal of Neurolinguistics, 17(2-3):147–180.\n1201\nAnastasia Giannakidou. 2011. Negative and positive\npolarity items: Variation, licensing, and composi-\ntionality. In Semantics: An international hand-\nbook of natural language meaning , pages 1660–\n1712. Berlin: Mouton de Gruyter.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205. Associ-\nation for Computational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nJulia Hockenmaier and Mark Steedman. 2007. CCG-\nbank: A corpus of CCG derivations and dependency\nstructures extracted from the Penn Treebank. Com-\nputational Linguistics, 33(3):355–396.\nDaniel Jurafsky, Chuck Wooters, Jonathan Segal, An-\ndreas Stolcke, Eric Fosler, G Tajchaman, and Nel-\nson Morgan. 1995. Using a stochastic context-free\ngrammar as a language model for speech recogni-\ntion. In Proceedings of ICASSP , volume 1, pages\n189–192. IEEE.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 1426–1436. Association for\nComputational Linguistics.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, (5):1202–1247.\nSteve Lawrence, Lee C. Giles, and Santliway Fong.\n1996. Can recurrent neural networks learn natural\nlanguage grammars? In IEEE International Con-\nference on Neural Networks, volume 4, pages 1853–\n1858.\nMike Lewis, Kenton Lee, and Luke Zettlemoyer. 2016.\nLSTM CCG parsing. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 221–231. Associa-\ntion for Computational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In Proceedings of the International Confer-\nence on Learning Representations.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Pro-\nceedings of the 11th Annual Conference of the In-\nternational Speech Communication Association (IN-\nTERSPEECH 2010) , pages 1045–1048, Makuhari,\nChiba, Japan.\nJoakim Nivre, Laura Rimell, Ryan McDonald, and\nCarlos G ´omez Rodr´ıguez. 2010. Evaluation of de-\npendency parsers on unbounded dependencies. In\nProceedings of the 23rd International Conference\non Computational Linguistics (Coling 2010) , pages\n833–841. Coling 2010 Organizing Committee.\nDenis Paperno, Germ ´an Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fernandez. 2016. The LAMBADA dataset:\nWord prediction requiring a broad discourse context.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1525–1534. Association for\nComputational Linguistics.\nAdam Pauls and Dan Klein. 2012. Large-scale syn-\ntactic language modeling with treelets. In Proceed-\nings of the 50th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 959–968. Association for Computa-\ntional Linguistics.\nColin Phillips, Matthew W. Wagers, and Ellen F. Lau.\n2011. Grammatical illusions and selective fallibility\nin real-time language comprehension. In Jeffrey T.\nRunner, editor,Experiments at the Interfaces, Syntax\nand Semantics 37 , pages 153–186. Bingley, U.K.:\nEmerald.\nMatt Post. 2011. Judging grammaticality with tree sub-\nstitution grammar derivations. In Proceedings of the\n49th Annual Meeting of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 217–222. Association for Computational\nLinguistics.\nLaura Rimell, Stephen Clark, and Mark Steedman.\n2009. Unbounded dependency recovery for parser\nevaluation. In Proceedings of the 2009 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 813–821. Association for Computa-\ntional Linguistics.\nBrian Roark. 2001. Probabilistic top-down parsing\nand language modeling. Computational Linguistics,\n27(2):249–276.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\n1202\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 8–14. Association for Computational Linguis-\ntics.\nRico Sennrich. 2017. How grammatical is character-\nlevel neural machine translation? Assessing MT\nquality with contrastive translation pairs. In Pro-\nceedings of the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Volume 2, Short Papers , pages 376–382.\nAssociation for Computational Linguistics.\nRavi Shekhar, Sandro Pezzelle, Yauhen Klimovich,\nAur´elie Herbelot, Moin Nabi, Enver Sangineto, and\nRaffaella Bernardi. 2017. FOIL it! Find one mis-\nmatch between image and language caption. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 255–265. Association for Computa-\ntional Linguistics.\nJon Sprouse and Diogo Almeida. 2017. Design sensi-\ntivity and statistical power in acceptability judgment\nexperiments. Glossa, 2(1):14.\nAndreas Stolcke. 2002. SRILM-an extensible lan-\nguage modeling toolkit. In Proccedings of the Sev-\nenth International Conference on Spoken Language\nProcessing, pages 901–904.\nSwabha Swayamdipta, Sam Thomson, Chris Dyer, and\nNoah A. Smith. 2017. Frame-semantic parsing with\nsoftmax-margin segmental RNNs and a syntactic\nscaffold. arXiv preprint arXiv:1706.09528.\nKe Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe importance of being recurrent for modeling hi-\nerarchical structure. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nJulie A. Van Dyke. 2007. Interference effects from\ngrammatically unavailable constituents during sen-\ntence processing. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition, 33(2):407–\n430.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008.\nMatthew W. Wagers, Ellen F. Lau, and Colin Phillips.\n2009. Agreement attraction in comprehension: Rep-\nresentations and processes. Journal of Memory and\nLanguage, 61(2):206–237.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471.\nMing Xiang, Brian Dillon, and Colin Phillips. 2009.\nIllusory licensing effects across dependency types:\nERP evidence. Brain and Language, 108(1):40–55.\nGeoffrey Zweig and Christopher J. C. Burges. 2011.\nThe Microsoft Research sentence completion chal-\nlenge. Technical report, Microsoft."
}