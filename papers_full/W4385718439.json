{
  "title": "Comparative Analysis of a Large Language Model and Machine Learning Method for Prediction of Hospitalization from Nurse Triage Notes: Implications for Machine Learning-based Resource Management",
  "url": "https://openalex.org/W4385718439",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5048858029",
      "name": "Dhavalkumar D. Patel",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5021533090",
      "name": "Prem Timsina",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5048518806",
      "name": "Larisa Gorenstein",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A5030539003",
      "name": "Benjamin S. Glicksberg",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5014053962",
      "name": "Ganesh Kumar Raut",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5054199533",
      "name": "Satya Narayan Cheetirala",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5068996233",
      "name": "Fábio Santana",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5027160697",
      "name": "Jules Tamegue",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5014803750",
      "name": "Arash Kia",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5079194583",
      "name": "Eyal Zimlichman",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A5089697111",
      "name": "Matthew A. Levin",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5070462882",
      "name": "Robert Freeman",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5039899830",
      "name": "Eyal Klang",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University",
        "Mount Sinai Health System"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2029598996",
    "https://openalex.org/W2046660755",
    "https://openalex.org/W1978719621",
    "https://openalex.org/W2953019443",
    "https://openalex.org/W4309691163",
    "https://openalex.org/W2177870565",
    "https://openalex.org/W2979690308",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3192540284",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3197237731",
    "https://openalex.org/W3138953784",
    "https://openalex.org/W4311344045",
    "https://openalex.org/W4362679551",
    "https://openalex.org/W4281606156",
    "https://openalex.org/W4280608694",
    "https://openalex.org/W3209254806",
    "https://openalex.org/W2746791326",
    "https://openalex.org/W2929110666",
    "https://openalex.org/W3121257277"
  ],
  "abstract": "Abstract Predicting hospitalization from nurse triage notes has significant implications in health informatics. To this end, we compared the performance of the deep-learning transformer-based model, bio-clinical-BERT, with a bag-of-words logistic regression model incorporating term frequency-inverse document frequency (BOW-LR-tf-idf). A retrospective analysis was conducted using data from 1,391,988 Emergency Department patients at the Mount Sinai Health System spanning 2017-2022. The models were trained on four hospitals’ data and externally validated on a fifth. Bio-clinical-BERT achieved higher AUCs (0.82, 0.84, and 0.85) compared to BOW-LR-tf-idf (0.81, 0.83, and 0.84) across training sets of 10,000, 100,000, and ∼1,000,000 patients respectively. Notably, both models proved effective at utilizing triage notes for prediction, despite the modest performance gap. Importantly, our findings suggest that simpler machine learning models like BOW-LR-tf-idf could serve adequately in resource-limited settings. Given the potential implications for patient care and hospital resource management, further exploration of alternative models and techniques is warranted to enhance predictive performance in this critical domain.",
  "full_text": " \n \n  \n \n \nComparative Analysis of a Large Language Model and Machine Learning Method for Prediction of \nHospitalization from Nurse Triage Notes: Implications for Machine Learning -based Resource \nManagement \n \nDhavalkumar Patel1, Prem Timsina1, Larisa Gorenstein2, Benjamin S Glicksberg1, Ganesh Raut1, \nSatya Narayan Cheetirala1,Fabio Santana1, Jules Tamegue1, Arash Kia1, Eyal Zimlichman2,3, \nMatthew A. Levin1, Robert Freeman1, Eyal Klang1,3,4 \nOriginal Article \n \n1 Mount Sinai Health System \n2 Hospital Management, Sheba Medical Center, Affiliated to Tel-Aviv \nUniversity \n3 ARC Innovation Center, Sheba Medical Center, Affiliated to Tel-Aviv \nUniversity \n4 Division of Diagnostic Imaging, Sheba Medical Center, Affiliated to Tel-Aviv \nUniversity \n* Correspondence: Dhaval.Patel@mountsinai.org; Eyal.Klang@mountsinai.org  \nAbstract: \nPredicting hospitalization from nurse triage notes has significant implications in \nhealth informatics. To this end, we compared the performance of the deep -learning \ntransformer-based model, bio-clinical-BERT, with a bag-of-words logistic regression \nmodel incorporating term frequency-inverse document frequency (BOW-LR-tf-idf).  \nA retrospective analysis was conducted using data from 1,391,988 Emergency \nDepartment patients at the Mount Sinai Health System spanning 2017 -2022. The \nmodels were trained on four hospitals' data and externally validated on a fifth. Bio -\nclinical-BERT achieved higher AUCs (0.82, 0.84, and 0.85) compared to BOW -LR-tf-\nidf (0.81, 0.83, and 0.84) across training sets of 10,000, 100,000, and ~1,000,000 patients \nrespectively. Notably, both models proved effective at utilizing triage notes for \nprediction, despite the modest performance gap. Importantly, our findings suggest \nthat simpler machine learning models like BOW-LR-tf-idf could serve adequately in \nresource-limited settings. Given the potential implications for patient care and \nhospital resource management, furt her exploration of alternative models and \ntechniques is warranted to enhance predictive performance in this critical domain. \n \nKeywords: Bio-clinical-BERT, Term frequency-inverse document frequency (TF-\nIDF), Health informatics, Patient care, Hospital resource management \n \n \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \n \n \n1. Introduction \n \nEfficient and effective patient triage within the emergency department (ED) plays \na pivotal role in enhancing treatment outcomes and optimizing care delivery [1,2,3]. \nThis crucial process involves rapidly identifying patients who require immediate \nhospitalization upon their arrival . Making the right call can be crucial for patients’ \nprognosis. One of the pivotal resources for making these critical predictions are nurse \ntriage notes, which provide a wealth of in -depth information about the patient's \ncondition at presentation [4,5]. In the field of healthcare, machine learning has opened \nup new avenues for potential improvement in such complex classification tasks, \nthereby augmenting clinical decision -making processes [6,7]. The recent \ndevelopments in deep lea rning and natural language processing (NLP) techniques \nhave further broadened this potential, bringing forth a whole new realm of \npossibilities for enhancing medical decision-making capabilities. \n \nAmong these advanced technological algorithms is the Bidire ctional Encoder \nRepresentations from Transformers (BERT) model . This model has emerged as a \npowerful tool in the sphere of NLP [8]. Its excellent performance in numerous NLP \ntasks [9] has inspired the development of more specialized versions tailored to \nparticular fields, such as the Bio -Clinical-BERT, which was specifically designed to \ncater to the biomedical field [10]. The focus of this study is to delve into the potential \nof a fine-tuned Bio-Clinical-BERT model and compare it against a simpler, robust, and \nmore traditional approach, namely the Bag of Words (BOW) Logistic Regression (LR) \ncomplemented by the term frequency -inverse document frequency (Tf -Idf) method. \nThe primary objective of our research is to gauge the efficacy of these two methods in \npredicting hospital admissions using nurse triage notes. \n \nWhile it's true that Bio-Clinical-BERT could potentially offer improved accuracy \nin its predictions, it should be noted that it also requires a substantial investment in \nterms of computational resources. It necessitates the use of specialized hardware and \ndemands a certain level of software expertise to operate effectively. On the other hand, \nthe LR model paired with the Tf -Idf method, is more resource -efficient and enjoys \nwide acceptance in the field of text classification due to its simplicity and effectiveness. \n \nConsidering these aspects, we have formulated a hypothesis for our study. We \nhypothesize that the Bio -Clinical-BERT model may surpass the performance of the \nBOW LR model combined with the Tf -Idf approach in the task of predicting triage \noutcomes. However, we also speculate that the incremental gains in performance \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \nmight not necessarily justify the additional demands imposed by the large deep -\nlearning model in terms of computational resources and technical know-how. To test \nthis hypothesis, we have undertaken an extensive study using over one million nurse \ntriage notes collected from a large health system, subjecting both models to the same \ndata for a fair comparison. \n \nThe fundamental contribution of this paper is a comprehensive comparison \nbetween these two distinct techniques for predicting hospital admission. Our \ncomparison not only looks at the accuracy of these models, but also weighs the trade-\noffs between predictive accuracy and computational efficiency, a consideration that is \noften overlooked but is of prime importance in real -world settings  – when \nimplementing models. Our aim is to equip healthcare practitioners, researchers, and \ndecision-makers with insights th at could potentially aid in enhancing hospital \nresource management and improve the quality of patient care. \n \n2. Methods \n2.1. Data Sources and Study Design \nFor the construction and testing of our models, we utilized an extensive dataset \nfrom the Mount Sinai  Health System (MSHS) . This is a diverse healthcare provider \nbased in New York City. In this study, the dataset included Emergency Department \n(ED) records spanning a five -year period from 2017 to 2022. This dataset was \nmeticulously gathered from five different MSHS hospitals, covering a broad range of \npopulation groups and diverse urban health settings. \n \nThese five participating hospitals provided a rich source of data for our study, \nrepresenting different communities in New York City. The hospitals include Mount \nSinai Hospital (MSH), a healthcare institution located in East Harlem, Manhattan; \nMount Sinai Morningside (MSM), situated in Morningside Heights, Manhattan; \nMount Sinai West (MSW), operating in Midtown West, Manhattan; Mount Sinai \nBrooklyn (MSB), a community-focused health facility located in Midwood, Brooklyn; \nand Mount Sinai Queens (MSQ) based in Astoria, Queens. The dataset used for our \nstudy was compiled using the Epic Electronic Health Records (EHR) software, a tool \nthat aids in efficient data collection, management, and analysis. The dataset was made \navailable by the diligent work of the MSH Clinical Data Science team. \n \n2.2. Model Development and Evaluation \nIn the development and testing of our models, we leveraged data from fou r \nhospitals for training, validation, and hyperparameter tuning processes. We elected to \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \nuse a distinct dataset from Mount Sinai Queens (MSQ) for external testing to ensure \nour model's generalizability. \n \nThe internal training and validation cohort underwen t a rigorous procedure \ninvolving five-fold cross-validation. Each fold contained 10,000 records, which were \nused for hyperparameter tuning. For the external dataset, we experimented with \ntraining sets of varying sizes: 10,000, 100,000, and roughly 1,000,00 0 patients, which \nrepresent the complete four-hospital cohort. Subsequently, testing was carried out on \n20% of these cohorts’ sizes, taken from the MSQ hospital cohort. \n \nOur study involved two prominent models: bio-clinical-BERT and bag-of-words \n(BOW) logi stic regression models, utilizing term frequency -inverse document \nfrequency (tf -idf) features. These models were employed to predict hospitalization \noutcomes from nurse triage notes. For bio -clinical-BERT, we adhered to text \npreprocessing and tokenization guidelines as outlined on the Huggingface.com \nwebsite [21]. Further details on hyperparameter selection are elucidated in section 3.2 \nof the results. For BOW LR Tf -Idf, we followed similar methodology outlined in our \nprevious publication [11], covering bot h text preprocessing and hyperparameter \nselection processes. \n \nBERT: BERT, or Bidirectional Encoder Representations from Transformers, is a \nmodel designed for natural language processing tasks. It learns from the context of \nboth preceding and following words, making it \"bidirectional\". This feature sets BERT \napart, as it allows for a better understanding of language semantics. This model is pre-\ntrained on large corpora and can be fine-tuned for specific tasks. \n \nBag of Words (BOW): The Bag of Wo rds model is a simple technique in natural \nlanguage processing. It represents text data by counting the frequency of each word, \ndisregarding the order in which they appear. Each unique word forms a feature, and \nthe frequency of the word represents the value of that feature. However, this method \ncan overlook context and semantics due to its simplicity. \n \nTF-IDF: TF-IDF stands for Term Frequency -Inverse Document Frequency. It's a \nnumerical statistic that reflects how important a word is to a document in a collection. \nIt is a combination of two metrics: Term Frequency, which is the number of times a \nword appears in a document, and Inverse Document Frequency, which diminishes the \nweight of common words and amplifies the weight of rare words across the entire \ndataset. This helps in reducing the impact of frequently used words and highlights \nmore meaningful terms. \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \n \n2.3. Study Population \nThe demographic for this study included adult patients aged 18 years and above. \nThese were patients who made ED visits within the specified five-year period of 2017-\n2022 across the five participating MSHS hospitals. \n2.4. Outcome Definition \nThe primary outcome for our study was to ascertain our models' effectiveness in \npredicting hospitalization. This prediction was based on two main types of data: \ntabular Electronic Health Records (EHR) and nurse triage notes. \n2.5. Model Evaluation and Comparison \nTo rigorously assess the performance of our models, we utilized various metrics \nsuch as Area Under the Receiver Operating Characterist ic curve (AUC), sensitivity, \nspecificity, and precision. These metrics allowed us to thoroughly evaluate the bio -\nclinical-BERT [10] and BOW logistic regression models with tf -idf features, and \ncompare their capabilities in predicting hospitalization from nurse triage notes. \n2.7. Ethical Considerations \nThis study, being retrospective in nature, was reviewed and approved by an \nethical institutional review board (IRB) committee from MSHS. The IRB committee \ndeemed that due to the retrospective nature of the study, the requirement for informed \nconsent was waived. \n2.8. Statistical Analysis \nOur statistical analyses were conducted using Python (Version 3.9.12). We \npresented continuous variables as median (IQR) and categorical variables as \npercentages for better interpretability. To identify words linked to hospital admission \nwithin nurse triage notes, we calculated the Odds Ratio (OR) and Mutual Information \n(MI) [11]. Statistical tests such as the χ2 test and Student’s t -test were employed for \ncomparing categorical and continuous variables, respectively. A p -value of less than \n0.05 was considered statistically significant. For evaluating our models, Receiver \nOperating Characteristic (ROC) curves were plotted, and metrics including AUC, \nsensitivity (recall), specificity, and positive predictive value (precision) were derived, \nwith Youden's index used to determine optimal cut-off values. \n \n2.9 Technical Architecture \nThe technical experiments involved in this study were conducted within a \ncontrolled hospital infrastructu re that used an On -Premises Centos Linux \nenvironment in conjunction with Azure Cloud infrastructure. For the BOW TF -IDF \nexperiments, we elected to use the Centos Linux OS. In contrast, the BERT experiment \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \nwas conducted using a Standard_NC6 GPU instance on Azure Cloud. This instance \ncame with 1 16GB GPU, 6 vCPU, and incurred a cost of approximately $80 during the \ntraining phase. Figure 1  offers a detailed depiction of the fundamental technical \narchitecture employed for training the BERT and LR -TFID models, across multiple \npatient datasets. \n \nFigure 1  illustrates the process flow of multiple patient datasets passing \nthrough two different models with GPU and non-GPU instances. \n \n \n3. Results \n \n3.1. Patient Population and Data \nOur study incorporated data from 1,745,199 patients drawn from the Mount \nSinai Health System. Upon exclusion of patients below 18, we had 1,391,988 \nparticipants in the study. These patients visited the ED between 2017 and 2022. \nTable 1 presents a summary of the patient characteristics. \n \nTable 1: Demographic distribution in the research cohorts (Abbreviations: IQR – \ninterquartile range, MSQ – Mount Sinai Queens) \n \n  \n4 hospitals \nMedian \n(IQR) \nMSQ \nMedian \n(IQR) \nP value \nTotal \nnumber of \npatients \n1,391,988 (includes MSH, MSM, MSW, \nMSB and MSQ Facilities) \n        \nAge \n48.0 \n45.0 (30.0 – \n75.0) P<0.001 (32.0 – \n75.0) \nSex Female: \n52.8% \nFemale: \n50.1% P<0.001 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \nMale: Male:  \n49.9% 47.20% \nRace:  \nP<0.001 \nBlack 382,898 \n(34.50%) \n45,696 \n(16.22%) \nWhite 265,457 \n(23.92%) \n77,622 \n(27.56%) \nOther 461,917 \n(41.58%) \n158,398 \n(56.22%) \n \nThe median number of words per triage note was 19.0 (IQR 12.0 – 31.0). Top ten \nwords associated with the highest MI score regarding hospital admission are \noutlined in Table 2. \n \nTable 2: Odds Ratios and Mutual Information values for words linked to admission to \nhospital wards, sorted by highest Mutual Information values. Abbreviations: OR – Odds \nRatio, MI – Mutual Information. \n \n \nWord OR for \nAdmission \nMI for \nAdmission p value \nSent 3.6 16.4 <0.001 \nPt 1.6 15.8 <0.001 \nPer 2.3 15 <0.001 \nOf 1.3 12.7 <0.001 \n     Home 2.2 11.5 <0.001 \n     EMS 2.2 10.8 <0.001 \n     \nWeakness 3.6 10.8 <0.001 \nChest 1.4 8.9 <0.001 \nSOB 2.1 8.8 <0.001 \nBIBA 2.1 7.9 <0.001 \n \n3.2. Hyperparameter Tuning Results \nA comprehensive hyperparameter tuning process was performed. The best \nhyperparameters were identified for each model based on their performance \nduring the five-fold cross-validation on the training validation set. The results \nof the BERT hyperparameter tuning process can be found in Table 3. \n \nTable 3: Hyperparameter tuning in the internal training/validation cohorts using five-\nfold experiments (Abbreviations: BS: Batch Size, LR: Learning Rate, ML: Max Length) \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \n \n   LR:2e-5  \nEpoch: 3    LR:3e-5 \nEpoch: 3    LR:5e-5 \nEpoch: 3  \n BS: 64 BS:128 BS: 128 \nML:128  \nBS:256 \nML: 64 BS: 64 BS:128 BS:128 \nML:128  \nBS:256 \nML:64 BS: 64 BS:128 BS:128 \nML:128 \nBS:256 \nML:64 \nBERT  \n0.78±SD \n \n0.80±SD \n \n0.80±SD \n \n0.79±SD \n \n0.79±SD \n \n0.79±SD \n \n0.78±SD \n \n0.78±SD \n \n0.79±SD \n \n0.80±SD \n \n0.79±SD \n \n0.79±SD \n \n \n3.3. Model Performance  \nAfter training the bio-clinical-BERT and LR-tf-idf models on the four hospitals' \ndata, we evaluated their performance on the held -out test data from Mount \nSinai Queens (MSQ). The area under the receiver operating characteristic \n(AUC) values were calculated for each model. Th e bio -clinical-BERT model \nachieved AUCs of 0.82, 0.84, 0.85 while the LR -tf-idf model had AUCs of 0.81, \n0.83, 0.84 for training on 10,000, 100,000, and ~1,000,000 patients.  \nFigure 1 shows the ROC and AUC comparisons between the two models. The \nbio-clinical-BERT model consistently outperformed the LR -tf-idf model in \nterms of AUC across the different training set sizes (10,000, 100,000, and \n~1,000,000 patients), albeit by a small margin. \n \nFigure 1: receiver operating characteristic curves (ROC) of the two models tested on \nincreasing training sample sizes. \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \n \nIn addition to the AUC comparisons, we also calculated other performance \nmetrics, such as sensitivity, specificity, and precision, for both models (Table 4 \nand Table 5). The probability cut -off values for these metrics were calculated \nusing Youden's index. These results further demonstrated the superior \nperformance of the bio-clinical-BERT model compared to the LR-tf-idf model. \n \n \n \n \nTable 4: metrics results for the training/testing (external) cohort for bio-clinical-BERT \n(Abbreviations: AUC – Area under the curve) \n \nTraining \nData Size AUC_Score Sensitivity Specificity Precision F1_Score \n10000 0.82 0.76 0.74 0.36 0.49 \n100000 0.84 0.74 0.77 0.39 0.51 \n1000000 0.85 0.39 0.96 0.67 0.50 \n \n \nTable 5: metrics results for the training/testing (external) cohort for tf-idf-LR \n(Abbreviations: AUC – area under the curve) \n \nTraining \nData Size AUC_Score Sensitivity Specificity Precision F1_Score \n10000 0.81 0.66 0.80 0.40 0.50 \n100000 0.83 0.75 0.74 0.37 0.50 \n1000000 0.84 0.71 0.80 0.42 0.53 \n \n \n4. Discussion \nIn this study, we sought to compare the performance of two predictive models, \nbio-clinical-BERT and LR -tf-idf, in predicting hospitalizations based on nurse \ntriage notes. The findings of our study suggest that while bio -clinical-BERT \ndoes marginally outperform LR -tf-idf in this predictive task, the difference in \ntheir performance is relatively minor. \n \nSuch results echo the findings of previous studies in the field, which have often \nfound BERT -based models to have a slight edge over traditional machine \nlearning methods like LR -tf-idf in various natural language processing tasks \n[12, 13]. However, it's essential to note that the marginal difference observed in \nour study suggests that, given certa in limitations such as constraints on \nhardware, software expertise, or budget, hospitals might lean towards simpler \nmachine learning methods. The rationale behind such a choice would lie in the \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \nease of implementing these simpler methods, as well as their r elatively less \ndemanding computational requirements. \n \nThe comparison of different models in the biomedical domain has been the \nfocus of numerous previous studies. For instance, Chen et al. conducted a \nrigorous assessment of transformer -based ChatGPT models  in tasks like \nreasoning and classification [14]. Their study found that fine -tuning remained \nthe most effective approach for two central NLP tasks. However, it's interesting \nto note that the basic Bag -of-Words model managed to deliver comparable \nresults to the more complex Language Model prompting. It should be noted \nthat the creation of effective prompts required a substantial resource \ninvestment. \n \nIn another study, Xavier et al. compared three different model types for a \nmulticlass text classification ta sk, which involved the assignment of protocols \nfor abdominal imaging CT scans [15]. These models spanned a range from \nconventional machine learning and deep learning to automated machine \nlearning builder workflows. While the automated machine learning buil der \nboasted the best performance with an F1 score of 0.85 on an unbalanced dataset, \nthe Tree Ensemble machine learning algorithm was superior on a balanced \ndataset, delivering an F1 score of 0.80. \n \nA further study delved into the evaluation of Machine Learning multiclass \nclassification algorithms' performance in classifying proximal humeral \nfractures using radiology text data [16]. Several statistical ML algorithms \nperformed reasonably, with a BERT model showcasing the best accuracy of \n61%. In another relevant study conducted by Ji et al., various models pretrained \nwith BERT were compared for medical code assignment based on clinical notes. \nInterestingly, it was found that simpler artificial neural networks could \nsometimes outperform BERT in cert ain scenarios [17]. This study, among \nothers, offers further support to our recommendation for hospitals with limited \nresources to consider simpler, less resource-demanding methods for achieving \ncomparable predictive performance. \n \nIn the specific task of p redicting hospitalization, both methods in our study \neffectively leveraged the rich information found within nurse triage notes. This \nfinding aligns with those from other studies [18, 19, 20]. For instance, a study \nby Zhang et al. that evaluated logistic regression and neural network modeling \napproaches in predicting hospital admission or transfer after initial ED triage \npresentation found that the patient's free text data regarding referral improved \noverall predictive accuracy [18]. Similarly, Raita et al. utilized machine learning \nmodels to predict ED outcomes and demonstrated superior performance in \npredicting hospitalization [19]. \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \nThe results of our study carry practical implications for healthcare \norganizations. The ability to predict hospitalization f rom nurse triage notes \ncould lead to significant improvements in patient care by facilitating efficient \nresource allocation, optimizing bed management, and improving patient flow. \nThe choice between the use of bio-clinical-BERT and simpler methods, such as \nLR-tf-idf, should be influenced by the specific context of the organization, \nincluding factors such as available computational resources, software expertise, \nand desired model performance. \n \nOur study is not without limitations. For instance, the data used  for our study \nis specific to MSHS hospitals, which might not be representative of other \nhealthcare systems, potentially limiting the generalizability of our findings.  \nDespite using multi-site data, representing the diverse NY city population, and \nan external validation site for our final analysis, we acknowledge the need for \nfurther studies with more diverse datasets. We also recognize that we did not \nexplore the potential of combining both methods or other potential techniques \nthat could enhance these mod els' performance. Moreover, the field of NLP is \nadvancing fast, with new large language models constantly evolving. \n \nFuture research could focus on the exploration of BERT models that are pre -\ntrained and trained from scratch on a site's entire textual data . Although such \nan approach may demand significant resources and be computationally \nintensive, it might yield better performance by capturing the unique \ncharacteristics and language patterns of a specific healthcare setting. The \nexploration of other pre -trained language models or more advanced natural \nlanguage processing techniques could also pave the way for the development \nof more effective hospitalization prediction methods based on nurse triage \nnotes. \n \nIn conclusion, our study demonstrates that while bi o-clinical-BERT does \nmarginally outperform LR-tf-idf in predicting hospitalization from nurse triage \nnotes, the difference is small enough to suggest that simpler methods might be \nviable for hospitals with limited resources. More research is needed to identify \nalternative methods that can enhance these models' performance in predicting \nhospitalization, ultimately improving patient care and hospital resource \nmanagement. \n \nThrough a rigorous investigation of bio -clinical-BERT and LR -tf-idf models' \nperformance, our study contributes to the growing body of literature in the field \nof natural language processing and machine learning in healthcare. It \nemphasizes the importance of considering the trade -offs between model \ncomplexity and performance when deploying pred ictive tools in clinical \nsettings, highlighting that sometimes, simpler methods can prove almost as \neffective as more complex ones. \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \n4. References \n[1] Chalfin, D.B.; Trzeciak, S.; Likourezos, A.; Baumann, B.M.; Dellinger, R.P. Impact of \ndelayed transfer of critically ill patients from the emergency department to the intensive \ncare unit. Crit. Care Med. 2007, 35, 1477–1483. [Google Scholar] [CrossRef] \n[2] Rabin, E.; Kocher, K.; McClelland, M.; Pines, J.; Hwang, U.; Ra thlev, N.; Asplin, B.; \nTrueger, N.S. Solutions to emergency department “boarding” and crowding are \nunderused and may need to be legislated.  Health Aff.  2012, 31, 1757 –1766. [ Google \nScholar] [CrossRef] \n[3] Forero, R.; McCarthy, S.; Hillman, K. Access block and emergency department \novercrowding. Crit. Care 2011, 15, 216. [Google Scholar] [CrossRef][Green Version] \n[4] Sterling, N.W.; Patzer, R.E.; Di, M.; Schrager, J.D. Prediction of emergency \ndepartment patient disposition based on natural language processing of triage notes. Int. \nJ. Med. Inform. 2019, 129, 184–188. [Google Scholar] [CrossRef] \n[5] Patel, D., Cheetirala, S. N., Raut, G., Tamegue, J., Kia, A., Glicksberg, B., Freeman, R., \nLevin, M. A., Timsina, P., & Klang, E. (2022). Predicting Adult Hospital Admission from \nEmergency Department Using Machine Learning: An Inclusive Grad ient Boosting \nModel. Journal of Clinical Medicine, 11(23), 6888. doi: 10.3390/jcm11236888. \n[6] Deo, R.C. Machine Learning in Medicine.  Circulation 2015, 132, 1920–1930. [Google \nScholar] [CrossRef][Green Version] \n[7] Klang, E.; Barash, Y.; Soffer, S.; Bechler, S.; Resheff, Y.S.; Granot, T.; Shahar, M.; Klug, \nM.; Guralnik, G.; Zimlichman, E.; et al. Promoting head CT exams in the emergency \ndepartment triage using a machine learni ng model. Neuroradiology 2020, 62, 153–160. \n[Google Scholar] [CrossRef] \n[8] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep \nBidirectional Transform ers for Language Understanding. arXiv preprint \narXiv:1810.04805. \n[9] Soffer, S., Glicksberg, B. S., Zimlichman, E., & Klang, E. (2022). BERT for the \nProcessing of Radiological Reports: An Attention -based Natural Language Processing \nAlgorithm. Academic Radiology, 29(4), 634 -635. doi: 10.1016/j.acra.2021.03.036. PMID: \n34362663. \n[10] Alsentzer, E., Murphy, J. R., Boag, W., Weng, W. H., Jin, D ., Naumann, T., & \nMcDermott, M. B. A. (2019). Publicly Available Clinical BERT Embeddings. arXiv \npreprint arXiv:1904.03323 [cs.CL]. Retrieved from https://arxiv.org/abs/1904.03323. \n[11] Klang, E.; Levin, M.A.; Soffer, S.; Zebrowski, A.; Glicksberg, B.S.; C arr, B.G.; \nMcgreevy, J.; Reich, D.L.; Freeman, R. A Simple Free -Text-like Method for Extracting \nSemi-Structured Data from Electronic Health Records: Exemplified in Prediction of In -\nHospital Mortality.  Big Data Cogn. Comput.  2021, 5, 40. \nhttps://doi.org/10.3390/bdcc5030040 \n[12] Khan, Junaed Younus, et al. \"A benchmark study of machine learning models for \nonline fake news detection.\" Machine Learning with Applications 4 (2021): 100032. \n[13] Yenkikar A, Babu CN. AirBERT: A fine -tuned language representation mo del for \nairlines tweet sentiment analysis. Intelligent Decision Technologies.(Preprint):1-7. \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint \n \n[14] Chen, Shan, et al. \"Evaluation of ChatGPT Family of Models for Biomedical \nReasoning and Classification.\" arXiv preprint arXiv:2304.02496 (2023). \n[15] Xavier B A, Chen PH. Natural Language Processing for Imaging Protocol \nAssignment: Machine Learning for Multiclass Classification of Abdominal CT Protocols \nUsing Indication Text Data. J Digit Imaging. 2022 Oct;35(5):1120 -1130. doi: \n10.1007/s10278-022-00633-8. Epub 2022 Jun 2. PMID: 35654878; PMCID: PMC9582109. \n[16] Dipnall JF, Lu J, Gabbe BJ, Cosic F, Edwards E, Page R, Du L. Comparison of state -\nof-the-art machine and deep learning algorithms to classify proximal humeral fractures \nusing radiology text. Eur J Radiol. 2022 Aug;153:110366. doi: 10.1016/j.ejrad.2022.110366. \nEpub 2022 May 20. PMID: 35623313. \n[17] Ji S, Hölttä M, Marttinen  P. Does the magic of BERT apply to medical code \nassignment? A quantitative study. Comput Biol Med. 2021 Dec;139:104998. doi: \n10.1016/j.compbiomed.2021.104998. Epub 2021 Oct 30. PMID: 34739971. \n[18] Zhang X, Kim J, Patzer RE, Pitts SR, Patzer A, Schrager JD. Prediction of Emergency \nDepartment Hospital Admission Based on Natural Language Processing and Neural \nNetworks. Methods Inf Med. 2017 Oct 26;56(5):377 -389. doi: 10.3414/ME17 -01-0024. \nEpub 2017 Aug 16. PMID: 28816338. \n[19] Raita Y, Goto T, Faridi MK, Brown DFM, Camargo CA Jr, Hasegawa K. Emergency \ndepartment triage prediction of clinical outcomes using machine learning models. Crit \nCare. 2019 Feb 22;23(1):64. doi: 10.1186/s13054 -019-2351-7. PMID: 30795786; PMCID: \nPMC6387562 \n[20] Klang, E., Kummer , B.R., Dangayach, N.S., Zhong, A., Kia, M.A., Timsina, P., \nCossentino, I., Costa, A.B., Levin, M.A. and Oermann, E.K., 2021. Predicting adult \nneuroscience intensive care unit admission from emergency department triage using a \nretrospective, tabular-free text machine learning approach. Scientific reports, 11(1), pp.1-\n12. PMID: 33446890 PMCID: PMC7809037 DOI: 10.1038/s41598-021-80985-3. \n[21] https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT \n \n \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted August 10, 2023. ; https://doi.org/10.1101/2023.08.07.23293699doi: medRxiv preprint ",
  "topic": "Triage",
  "concepts": [
    {
      "name": "Triage",
      "score": 0.8832854628562927
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5870477557182312
    },
    {
      "name": "Machine learning",
      "score": 0.582383394241333
    },
    {
      "name": "Logistic regression",
      "score": 0.5336450338363647
    },
    {
      "name": "Computer science",
      "score": 0.529710054397583
    },
    {
      "name": "Health informatics",
      "score": 0.5013928413391113
    },
    {
      "name": "Health records",
      "score": 0.4998917579650879
    },
    {
      "name": "Emergency department",
      "score": 0.4829532504081726
    },
    {
      "name": "Health care",
      "score": 0.456925630569458
    },
    {
      "name": "Informatics",
      "score": 0.4554038941860199
    },
    {
      "name": "Medical emergency",
      "score": 0.3353661596775055
    },
    {
      "name": "Medicine",
      "score": 0.3273712992668152
    },
    {
      "name": "Nursing",
      "score": 0.22147318720817566
    },
    {
      "name": "Engineering",
      "score": 0.16551852226257324
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Public health",
      "score": 0.0
    }
  ]
}