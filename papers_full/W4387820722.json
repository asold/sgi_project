{
  "title": "Language models and psychological sciences",
  "url": "https://openalex.org/W4387820722",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2110316339",
      "name": "Giuseppe Sartori",
      "affiliations": [
        "University of Padua"
      ]
    },
    {
      "id": "https://openalex.org/A2320168061",
      "name": "Graziella Orrù",
      "affiliations": [
        "University of Pisa"
      ]
    },
    {
      "id": "https://openalex.org/A2110316339",
      "name": "Giuseppe Sartori",
      "affiliations": [
        "University of Padua"
      ]
    },
    {
      "id": "https://openalex.org/A2320168061",
      "name": "Graziella Orrù",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2043030203",
    "https://openalex.org/W4353015365",
    "https://openalex.org/W3185385952",
    "https://openalex.org/W4205243556",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W3120325630",
    "https://openalex.org/W2127800534",
    "https://openalex.org/W2113335527",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2165036621",
    "https://openalex.org/W2135255848",
    "https://openalex.org/W2003240077",
    "https://openalex.org/W2062572654",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2087946919",
    "https://openalex.org/W2118373646",
    "https://openalex.org/W2971304827",
    "https://openalex.org/W3016970897",
    "https://openalex.org/W2128291848",
    "https://openalex.org/W1983563165",
    "https://openalex.org/W3207647502",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2108134479",
    "https://openalex.org/W2040981305",
    "https://openalex.org/W4238501461",
    "https://openalex.org/W4386740961",
    "https://openalex.org/W1983578042",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2013379984",
    "https://openalex.org/W2129016965",
    "https://openalex.org/W4384154861",
    "https://openalex.org/W2014708853",
    "https://openalex.org/W4377942506",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W6948936239",
    "https://openalex.org/W1982914430",
    "https://openalex.org/W2136166259",
    "https://openalex.org/W4300402905",
    "https://openalex.org/W6770182719",
    "https://openalex.org/W2078229482",
    "https://openalex.org/W4229005331",
    "https://openalex.org/W2025584957",
    "https://openalex.org/W2885106765",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W6853020661",
    "https://openalex.org/W4400508366",
    "https://openalex.org/W4389518992",
    "https://openalex.org/W2962681511",
    "https://openalex.org/W4400681340",
    "https://openalex.org/W1575922411",
    "https://openalex.org/W4391655051",
    "https://openalex.org/W3103934428",
    "https://openalex.org/W2036073122"
  ],
  "abstract": "Large language models (LLMs) are demonstrating impressive performance on many reasoning and problem-solving tasks from cognitive psychology. When tested, their accuracy is often on par with average neurotypical adults, challenging long-standing critiques of associative models. Here we analyse recent findings at the intersection of LLMs and cognitive science. Here we discuss how modern LLMs resurrect associationist principles, with abilities like long-distance associations enabling complex reasoning. While limitations remain in areas like causal cognition and planning, phenomena like emergence suggest room for growth. Providing examples and increasing the dimensions of the network are methods that further improve LLM abilities, mirroring facilitation effects in human cognition. Analysis of LLMs errors provides insight into human cognitive biases. Overall, we argue LLMs represent a promising development for cognitive modelling, enabling new explorations of the mechanisms underlying intelligence and reasoning from an associationist point of view. Carefully evaluating LLMs with the tools of cognitive psychology will further understand the building blocks of the human mind.",
  "full_text": "Frontiers in Psychology 01 frontiersin.org\nLanguage models and \npsychological sciences\nGiuseppe Sartori 1 and Graziella Orrù 2*\n1 Department of General Psychology, University of Padova, Padova, Italy, 2 Department of Surgical, \nMedical, Molecular and Critical Area Pathology, University of Pisa, Pisa, Italy\nLarge language models (LLMs) are demonstrating impressive performance \non many reasoning and problem-solving tasks from cognitive psychology. \nWhen tested, their accuracy is often on par with average neurotypical adults, \nchallenging long-standing critiques of associative models. Here we analyse recent \nfindings at the intersection of LLMs and cognitive science. Here we discuss how \nmodern LLMs resurrect associationist principles, with abilities like long-distance \nassociations enabling complex reasoning. While limitations remain in areas \nlike causal cognition and planning, phenomena like emergence suggest room \nfor growth. Providing examples and increasing the dimensions of the network \nare methods that further improve LLM abilities, mirroring facilitation effects in \nhuman cognition. Analysis of LLMs errors provides insight into human cognitive \nbiases. Overall, we argue LLMs represent a promising development for cognitive \nmodelling, enabling new explorations of the mechanisms underlying intelligence \nand reasoning from an associationist point of view. Carefully evaluating LLMs with \nthe tools of cognitive psychology will further understand the building blocks of \nthe human mind.\nKEYWORDS\nassociationism, reasoning, cognitive psychology, large language models (LLMs), GPT-4\nIntroduction\nHere we will discuss the impact of large language models (LLMs) in cognitive psychology \nand will show how these models display human-like performance in a wide variety of cognitive \ntasks. We  will relate current models with previous versions of associative networks that \nconstellated the history of psychological science and show how LLMs have the potential for \nexplaining an unprecedented wide range of cognitive processes.\nLLMs are neural networks trained to assign probabilities to a sequence of text predicting the \nnext most probable word. The most recent LLMs have billions to trillions of parameters (weights) \nand are initially trained on massive collections of unstructured natural language data. The state-\nof-the-art models are trained on internet-scale text data to predict the next token given the \npreceding text. The networks’ main objective during training is, therefore, to predict a hidden \nsection of an input sentence using a technique known as “self-supervised learning. ” Taking a \nsliding window of words as input, the neural network is trained to predict the next word. The \nresulting network is a statistical model that captures the highly complex relationships between \nthe words and phrases in the training data. In Bayesian terms, the neural network computes the \nconditional probability for every potential next word based on the provided preceding words as \ninput, as part of predicting the subsequent word. For example, when prompted with the sentence:\n“The quick brown fox jumps over the lazy ___” a LLM may predict “dog” with a probability \nof 99%. The next word prediction “dog” is the word with the higher probability among several \nalternatives and is the word that is selected in the completion of the sentence. The next word \nOPEN ACCESS\nEDITED BY\nFernando Marmolejo-Ramos,  \nUniversity of South Australia, Australia\nREVIEWED BY\nJulian Tejada,  \nFederal University of Sergipe, Brazil  \nMiguel Alvarez,  \nCentro de Investigación en Matemáticas,  \nMexico\n*CORRESPONDENCE\nGraziella Orrù  \n graziella.orru@unipi.it\nRECEIVED 30 August 2023\nACCEPTED 26 September 2023\nPUBLISHED 20 October 2023\nCITATION\nSartori G and Orrù G (2023) Language models \nand psychological sciences.\nFront. Psychol. 14:1279317.\ndoi: 10.3389/fpsyg.2023.1279317\nCOPYRIGHT\n© 2023 Sartori and Orrù. This is an open-\naccess article distributed under the terms of \nthe Creative Commons Attribution License \n(CC BY). The use, distribution or reproduction \nin other forums is permitted, provided the \noriginal author(s) and the copyright owner(s) \nare credited and that the original publication in \nthis journal is cited, in accordance with \naccepted academic practice. No use, \ndistribution or reproduction is permitted which \ndoes not comply with these terms.\nTYPE Hypothesis and Theory\nPUBLISHED 20 October 2023\nDOI 10.3389/fpsyg.2023.1279317\nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 02 frontiersin.org\nprediction task leads to a compressed representation of the world as \nderived uniquely from language. The previously trained network \ncould undergo additional improvement using Reinforcement Learning \nfrom Human Feedback (RLHF , Wang et al., 2022), in which the LLM \nlearns to predict the best alternative, as determined by human \nassessors, from the original set of outputs generated by the model itself.\nBy leveraging their capability for next-word prediction, these \nmodels can adeptly condense text, identify headings, rephrase content, \nand perform other language-related tasks that involve language \nmanipulation, including the creation of coherent and grammatical \nsound narratives.\nMost surprisingly, when undertaking tasks, LLMs exhibit the \nability to adopt various roles based on the provided prompt or queries. \nThey can efficiently impersonate an expert in psychotherapy, real \nestate sales, or Python programming. This versatility has led to \npre-trained LLMs being referred to as foundation models (Bommasani \net al., 2021), given their capacity to model a wide range of distinct \ndownstream tasks. LLMs possess knowledge about factual information \nor events (e.g., When was Rome founded?), semantic knowledge (e.g., \nHow are the ears of a sheep ?) as well as reasoning capabilities. The \nsurprising effectiveness of LLMs has triggered a surge in research \nendeavours, which are rapidly proliferating, thus posing significant \nchallenges for monitoring. In this context, we will focus on the most \npertinent findings related to the problem-solving abilities of LLMs, \nhighlighting the importance of studying them to advance the field of \ncognitive psychology. Here, we  will demonstrate how LLMs have \nsurpassed the barriers that were once thought insurmountable for \ncognitive models grounded in associations. In other words, LLMs \nrepresents the resurrection of associative theories of cognition. To \naccomplish this goal. We will: (i) provide an up-to-date overview of \nthe history of LLMs in relation to psychological theories; (ii) \nsummarize the relevant data pertaining to the extent of “intelligence” \nexhibited by LLMs when subjected to psychological assessment. The \ncognitive assessment of LLMs is a novel approach compared to \nprevious AI benchmarking efforts, which primarily focused on \nperformance metrics.\nWe are observing a growing body of evidence that underscores the \nrelevance of LLMs as a comprehensive representation of human \ncognition. Notably, this marks a pivotal moment in cognitive \npsychology, as we now have access to a general model rather than \nfragmented models explaining only a limited task perimeter.\nLarge language models (LLMs)\nBrief history of LLMs\nAfter the pioneer work of the Perceptron by Rosenblatt (1960), in \nthe 1980s, researchers began to develop neural network-based \napproaches to natural language processing (NLP), which paved the \nway for the current development of LLMs. One of the early models of \nlanguage processing was the Recurrent Neural Network Language \nModel (RNNLM) developed by Elman (1991) . This model was \nsuccessful in identifying short-range relationships between a sequence \nof words, yet it demonstrated limitations such as its incapacity to grasp \nlong-range dependencies. Prior to the emergence of LLMs, certain \nprecursor models in cognitive modelling, such high dimensional \nsemantic spaces like Latent Semantic Analysis (LSA) (Landauer and \nDumais, 1997) can be recognized. To comprehend the foundation of \nthe recent significant advancement of LLMs, it is essential to \nacknowledge two pivotal research milestones.\nThese advancements notably encompass word embeddings and \nself-attention mechanisms. Word embeddings serve as a method to \nrepresent words in a way that captures their meanings, \ninterrelationships with other words, and contextual nuances. This is \nachieved by representing each word as a vector (a sequence of \nnumerical values) in a multi-dimensional space. Each word is \nassociated with a distinct vector, and words sharing similar meanings \nexhibit closely aligned representations within the high dimensional \nsemantic space.\nSelf-attention (Vaswani et al., 2017) represents a significant step \nforward, enabling the selective focus on different segments of the \ninput word sequence and the assignment of varying weights to each \nsegment based on its relevance. The self-attention mechanism \neffectively captures relationships between words that are far apart in \nthe input sequence of words, a feature which is particularly important \nfor NPL tasks such as text generation, language translation and \ntext comprehension.\nCentral to the advancement of language processing are \ntransformers, which integrate self -attention mechanisms and consist \nof two core components: an encoder and a decoder. The encoder is \nresponsible for handling and encoding the input data (e.g., a sentence \nin English), while the decoder employs the encoded representation to \ngenerate the output (e.g., a translation in Italian). Both the encoder \nand decoder leverage the attention mechanism to focus on the most \npertinent segments of the input during the output generation process. \nOverall, the introduction of self-attention has considerably expanded \nthe capabilities of LLMs, as it is the attentional mechanism that allows \nthe prediction of the next word in a sequence based on words that \nmight be distant in the preceding context.\nThe attention mechanism in transformers is like a tool that helps \nthe model focus on specific words or phrases that are crucial for \nunderstanding the meaning of a sentence. It does this by assigning a \nweight or importance score to each word in the sentence, based on \nhow relevant it is in the context of the sentence. This way, the model \ncan consider which words matter most. What is fascinating about \nLLMs is that they not only enable the incorporation of long-distance \ncorrelations, but they also allow for more flexible and natural sentence \nstructure. From a psychological standpoint, transformers with self-\nattention can figure out connections (associations/correlations) \namong words, even if they are far apart in the text; this ability to \ncapture distant associations in the stream of text is particularly \nrelevant in the field of psychological science.\nAs an illustration, in the sentence “The horse that the boy is chasing \nis fat” a higher association will be attributed to the word pair horse-fat \nby the self-attention mechanism. From this association, LLMs may \neasily answer the question “ Who is fat ?” What seems particularly \nrepresentative of the astonishing result of long-distance associations \nis the correct response that GPT-4 ( Bubeck et al., 2023 ) produced \nwhen presented with the following real-world problem:\nUSER: “In my kitchen there is a table with a cup with a ball inside. \nI moved the cup in my bedroom, and I turned the cup upside down. \nI grabbed the cup again and moved to the kitchen. Where is the ball \nnow?” The answer was the following: GPT-4: “ The ball is in \nyour bedroom.”\nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 03 frontiersin.org\nAn additional aspect connected to the problem-solving ability is \nthe dimension of the network. In this context, the network’s \ndimension significantly contributes to enhancing the good \nperformance of LLMs like GPT-4 (which boasts an estimated 1.7 \ntrillion parameters), surpassing the performance of smaller \ncounterparts such as Llama (with 7 billion parameters; Touvron et al., \n2023). To illustrate, when presented the preceding query, Llama \nprovided an inaccurate response:\nLLama: “The ball is now on the floor in the kitchen.”\nThe increased performance in relation to the dimension of the \nnetwork is just an example of the very interesting phenomenon called \n“emergence” (Wei et al., 2022a,b). The increased dimension of the \nnetworks increases the capacity of the network to solve hard problems \nwith capacities that are “implicitly induced, not explicitly constructed,”  \nsuch as the following hard reasoning problem:\nUSER: In front of us are gears numbered 1 to 6 mounted on \naxles in a row. Each gear is engaged with the next gear. If gear \nnumber 3 is rotated clockwise, in which direction will gears 1 \nand 6 rotate?\nVicuna (based on Llama 13b): (…) So, if gear 3 is rotated clockwise, \nthe teeth of gear 3 will mesh with the teeth of gears 1 and 6, causing \nthem to rotate in the same direction as gear 3.\nGPT 3.5 (175b): (…) If gear number 3 is rotated clockwise, then \ngear number 4 will rotate counterclockwise, which in turn will \nmake gear number 5 rotate clockwise, and finally, gear number 6 \nwill rotate counterclockwise. Similarly, gear number 2 will also \nrotate counterclockwise, which will make gear number 1 rotate \nclockwise. Therefore, gears 1 and 6 will rotate in o pposite  \ndirections.\nExpanding the dimensions of the language model results in more \nprecise solutions to complex problems that remain unsolved by \nsmaller networks (i.e., LLama 13 billions parameters vs. GPT 3.5175 \nbillions parameters).\nThe current heated debate over the level of \n“intelligence” of LLMs\nThe development of LLMs has led to an ongoing debate which is \nof great interest for psychological science, such as the level of \n“intelligence” that these models exhibit.\nSome sceptic researchers argue that LLMs are nothing more than \nsophisticated pattern recognition algorithms and do not exhibit a \n“true” level of intelligence as they lack the ability to reason and \nunderstand context effectively, which is believed to be  a critical \ncomponent of an undefined “true intelligence. ” Some critics, for \nexample, have claimed that LLMs are simply stochastic parrots (see \nBender et  al., 2021 ) unable to fully capture the complexity and \nrichness of human language. Their main thesis is that LLMs rely too \nheavily on statistical patterns in large datasets, rather than on a “true \nunderstanding” of linguistic structures and rules.\nThey also argue that these models can generate text that superficially \nresembles human language, but that lacks the “depth” and nuance of \ngenuine communication. According to the authors, LLMs may suffer \nfrom several limitations, such as the inability to capture context-\ndependent meaning, the overreliance on frequent patterns at the expense \nof rare but important ones. An influential title of a paper by Bishop \n(2021) is telling in this regard (Artificial Intelligence is stupid and causal \nreasoning will not fix it) and summarizes the positions of critics of those \nwho claim that LLMs have a form of human-like intelligence.\nHowever, the critiques by opponents remain purely argumentative, \nas none have proposed a definitive cognitive task that associative \nLLMs would fundamentally be unable to perform. The litmus tests \nthat sceptics set as true indicators of human intelligence were only \nresisting for a few months, the duration needed to create enhanced \nversions of LLMs. Critics of LLMs target an idealized version of these \nmodels, disregarding the fact that the actual models accurately \nrepresent syntax, a feature that Chomsky had previously \ndeemed unattainable.\nFor example, some critics claimed that common sense reasoning \nshould never be handled by LLMs as they do not have the variety of \nexperience that is needed for such tasks. However, recent multimodal \nlarge language model such as GPT-4 have partially overcome the \nlimitation of language-only models introducing interaction with \nvision. As regards to multimodality it is worth noting that LLM \ntrained only on language may derive the representation of colours as \nderived from vision (Patel and Pavlick, 2022) indicating that language-\nonly can be a source of sensorial information that can be used in \nverbal reasoning about sensation.\nProponents of LLMs counter that these models can generate \nhuman-like language and performing a wide range of language \nprocessing tasks. They also argue that LLMs have demonstrated state-\nof-the-art performance in tasks such as language translation and text \nsummarization, which were previously considered difficult for \nmachines to perform. The stochastic parrot framing they say, is a \nmisconception as LLMs are not capable of holding the entire training \nset as they are trained on a vast amount of text that goes beyond what \ncan be memorized. Consequently, LLMs must create an internal latent \nrepresentation of the training data, enabling them to provide novel \nresponses to new queries and this is a crucial requirement \nfor generalization.\nLLMs have the potential to achieve human-level intelligence \nand understanding if they are scaled up, according to recent studies \n(Wei et al., 2022a,b). LLMs exhibit significant advancements in the \nfield of NLP , representing a significant progression towards \nachieving advanced cognitive capabilities that closely mimic \ndifferent aspects of human intelligence. In contrast, an opposing \nperspective argues that the key to unlocking advanced AI \ncapabilities lies in the development of models characterized by \nheightened flexibility and adaptability to novel scenarios, rather \nthan mere size augmentation. Specific training methodologies have \nyielded remarkable progress in enhancing the capabilities of these \nexpansive models, such as Reinforcement Learning from Human \nFeedback, as found by Ouyang et al. (2022) . This training strategy \ndemonstrates substantial achievements in refining extensive \nlanguage models to align more closely with human-guided \nrefinements through iterative feedback loops. The ongoing debate \nconcerning the degree of intelligence exhibited by LLMs is likely \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 04 frontiersin.org\nto endure as these models continue to evolve and fresh \nadvancements emerge within the realm of AI and NLP .\nPsychological assessment of LLMs\nImportantly, due to their size and complexity, the behaviour of \nLLMs cannot be predicted in advance by looking at the architecture \nand training corpus and must instead be empirically explored. The \nprocedure required for evaluating LLM is like that used by cognitive \npsychologists to study the human mind and consists in testing LLMs \nwith tasks that are believed to tap on specific cognitive functions.\nAs stated earlier, the advent of LLMs has sparked a robust debate \nwithin the AI community, centring on the question of whether \nmachines possess the capability to genuinely comprehend natural \nlanguage, thereby capturing the interplay of both physical and social \ncontexts encapsulated within linguistic expression. The implications \nof this debate extend beyond practical applications, delving into the \nrealm of psychological cognition. This is because LLMs, as elucidated \nin the subsequent discussion, exhibit a remarkable proficiency in \nsimulating reasoning abilities that have traditionally been regarded as \ndistinctly human.\nRecently, cognitive psychologists have introduced a novel \nevaluation methodology for LLMs. This approach involves treating \nLLMs as active participants within a psychological experiment, \nthereby facilitating a comprehensive assessment of their cognitive \ncapabilities. Cognitive psychologists believe that this approach offers \ndifferent advantages over existing evaluation protocols which are not \ndriven by a cognitive model. The use of psychological-inspired tests to \nscrutinize LLMs’ performance serves a multifaceted purpose. These \ntests aim to uncover underlying cognitive biases and different \nproblem-solving approaches and methodologies that extend beyond \nthe confines of conventional performance-based analyses, which have \nbeen the focal point of previous investigations. By demystifying how \nLLMs solve challenging reasoning problems, psychological \nexperiments can provide a deeper understanding of their \nfull complexity.\nHerein, we introduce the preliminary l findings arising from an \ninvestigation conducted over the last 6 months (late 2022 – June 2023), \ncentred on the assessment of the reasoning abilities of LLMs using \nevaluation protocols initially formulated for human assessment.\nHuman or superhuman?\nThis study delves into the significance of LLMs in the context of \npsychological theories and from this perspective the problem of the \nappropriated benchmark for the evaluation of LLMs emerges.\nThe performance of LLMs should be evaluated in comparison to \nthe average neurotypical individuals or to an idealized errorless \nperformance reflecting the intuitive capabilities of an average PhD \nresearcher. The debate over LLMs capabilities often hinges on testing \nabilities that humans are presumed to have (e.g., reasoning, grammar \netc.) assuming that humans are errorless in these skills, a fact that is \nsimply not true. For example, an AI researcher assumes that average \nhumans are errorless in deciding if an integer is even. Actually, a \nsizable minority of people (20% circa) believe that 400 is more even \nthan 798 (Lupyan, 2013).\nIt is crucial to emphasize that the selection of the most suitable \nbenchmark is contingent upon the specific objective of the evaluation. \nIf the objective is to develop an intelligent assistant (AI researchers’ \nobjective), reliability and absence of errors are required. By contrast, \nif the objective is to evaluate LLMs’ performance as a psychological \nmodel of cognition, the quantity and type of errors may be  as \ninformative as accuracy. In other terms, if the goal is to create LLMs \nthat can effectively mimic human performance in a specific task, it \nmight be  useful to compare the LLM’s performance to that of an \naverage neurotypical individual. In this scenario this comparison can \noffer insights into the model’s ability to emulate human-like responses, \nencompassing both its accuracy and its mistakes.\nAs we are interested in the LLMs as models of human cognition, \nthe discussion reported here will be conducted under the assumption \nthat the evaluation of LLMs should be  carried out using, as a \nbenchmark, a neurotypical average human. In this regard, we should \nkeep in mind that cognitive test performance varies considerably \ndepending both on age and educational level. For instance, it has been \nshown by Hartshorne and Germine’s (2015) investigation, based on \ndata from 2,450 neurotypical adults (with age range between 16 and \n85 years), that an 80-year-old person has an average performance on \na wide variety of cognitive test below 1.5 standard deviations with \nrespect to a 50-year-old individual. Additionally, the impact on \nperformance is significantly more pronounced than the effect of age, \ndue to the level of education.\nHow neuropsychologists evaluate abnormality or cognitive deficit \ndiffers from the approach used in AI. In neuropsychology, an impaired \nperformance is identified when it is below 2 standard deviations below \nthe mean of healthy controls. By contrast, in LLMs research average \nhuman performance is rarely reported and when reported the \nstandard deviation is missing. Most of the time the reasoning \nimpairment of a large language model is inferred solely based on the \nintuition of a Ph.D. level evaluator. This missing information of mean \nand standard deviation of healthy controls on the task of interest \nrenders impossible to locate the exact performance of the LLM’s \nperformance with respect to the neurotypical individuals.\nHereafter, we  will report our analysis on the LLMs reasoning \nabilities by comparing the state-of-the-art models with that of \nneurotypical individuals.\nTasks that LLMs can perform within human \nrange\nAI researchers are focused on developing a dataset of problems \n(e.g., BigBench, Y e et al., 2023) that are used to evaluate LLMs on a \nwide variety of tasks whose psychological relevance is, however, \nunclear and is mainly motivated to evaluate the performance level \nacross a wide variety of problems including a wide range of \nscientific field.\nRecently, LLMs have been probed with tasks originally developed \nin cognitive psychology. For example, Binz and Schulz (2023) run \nseveral cognitive psychology tasks including decision-making, \ninformation search, deliberation, and causal reasoning abilities on a \nbattery of canonical experiments from the literature. The authors \ndescribe their results as follows: “ We find that much of GPT-3’s \nbehaviour is impressive: It solves vignette-based tasks similarly or better \nthan human subjects and is able to make decent decisions from \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 05 frontiersin.org\ndescriptions (…) But fails miserably in a causal reasoning task.” Binz \nand Schulz (2023), for the first time in a systematic way, applied the \nmethods of cognitive psychology to gain insights into LLMs. Later, \nmany other cognitive psychologists have evaluated state-of-the-art of \nLLMs with cognitive tasks. This approach is particularly relevant as \nusing tasks grounded on cognitive science may permit one to focus on \nthe theoretical aspects of LLM rather than on only focusing on \nerrorless performance, the main objective of AI researchers.\nA non-exhaustive list of tasks that one large language model \n(GPT3.5) performs within neurotypical range are included in Table 1.\nAll the tests reported in the table may be classified as frontal tests \nas neurological patients with frontal lesions are usually impaired on \nsuch evaluations. Results indicate a very capable human level for the \nmost recent state-of-the- art LLMs and it is worth noting that some of \nthese tasks were previously considered as insurmountable limits of \nLLMs (e.g., common sense reasoning). However, the single most \nimpressive result is the human level performance obtained by GPT3 \nfor Raven-like matrices (Webb et al., 2023) as the Raven test is usually \nregarded, in clinical psychology. As one of the best procedures to \nevaluate fluid, culture free, intelligence. In short, GPT3 performed at \nhuman level on such analogical reasoning IQ tests.\nFurthermore, verbal problem-solving abilities are at the same level \nas those of neurotypical humans as shown by Orrù et al. (2023). For \nexample, GPT-4 responded correctly to the following problem that \nwas solved only by 19% of the neurotypical participants also \nuncovering the reasoning that yielded to the correct response.\nUSER: Mr. and Mrs. Plum have six daughters and each daughter \nhas one brother. How many people are in the Plum family?\nGPT 4: Let us break it down step-by-step:\nMr. Plum = 1 person.\nMrs. Plum = 1 person.\nSix daughters = 6 people.\nThe information says that each daughter has one brother. However, \nthis does not mean that there are six brothers. All the daughters \nshare the same brother.\nSo, 1 brother = 1 person.\nAdding up all the family members:\n1 (Mr. Plum) + 1 (Mrs. Plum) + 6 (daughters) + 1 (brother) = 9 \npeople in total.\nSo, there are 9 people in the Plum family.\nIn some fields, Language Models have already surpassed human \nabilities. For instance, it is well known that humans have problems in \nidentifying lies, and it is well established that the average person is \nunable to distinguish whether a narrative is true or false, with their \nperformance being only slightly better than chance ( Bond and \nDePaulo, 2006). Currently, transformer-based language models are \noutsmarting the average human participant in identifying deceptive \nnarratives ( Capuozzo et  al., 2020 ). Creativity was believed to \nbe something that is specifically human and difficult to emulate but \nKoivisto and Grassini (2023) have shown that LLMs are very good in \nAUT, a standard test of creativity that requires generation of infrequent \nuses of objects and only the best humans perform better that LLMs. \nThese superhuman abilities has sparked interest for the development \nof an Artificial General Intelligence which is errorless on a wide \nvariety of knowledge and reasoning tasks (Bubeck et al., 2023).\nWhat sceptics claimed would never \nbe achieved by LLMs but has been easily \nachieved\nWhen the debate over the actual level of intelligence of LLMs \nheated up, many researchers identified tasks for which LLMs provided \ninaccurate responses. There is plenty of anecdotal reports on such \nfailures of LLM recently collected by Borji (2023) but most of such \nfailures that were observed in a previous versions are no longer \ndetectable in GPT-4 or are no longer observable when appropriate \nprompting (contextual information about the objective of the test) \nis provided.\nThis progressive improvement in performance is a telling example \nof the phenomenon of emergence, whereby the dimensional increase \nand the fine tuning of the LLMs originates competencies not \nTABLE 1 The table schematically reported the reasoning tests which were administered to LLM.\nTask Authors Comment\nAnalogical reasoning\nAlphanumeric test of the Raven’s Progressive matrices\nWebb et al. (2023) GPT3.5 performs better than humans on an \nalphanumeric version of the Raven’s test.\nMetaphors and idioms comprehension Loconte et al. (2023) Average human\nCognitive estimation Loconte et al. (2023) Average human\n“Winograd Schema” (Anaphoric reference) Loconte et al. (2023) Average human\nTheory of mind Kosinski (2023), Bubeck et al. (2023), Loconte et al. (2023) Below average. Within average for GPT-4.\nCreativity Stevenson et al. (2022) GPT3. Humans slightly higher in originality and \nsurprise, GPT3 slightly higher on utility\nCommon sense question and answer Loconte et al. (2023) GPT-4 is 83% with human performance being 89%\nProblem solving Orrù et al. (2023) GPT 3.5 performs in normal range\nAll results were collected between the end of 2022 and the first 3 months of 2023. Such a short period of time gives an idea of the turbulence that involves this field.\nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 06 frontiersin.org\npreviously available. As regards to what LLMs cannot do it is worth \nnoting that sceptics usually refer to a vague “deep” understanding that \nshould characterize humans and could never be  matched by \nLLM. Such a “deep” understanding has been identified in common \nsense knowledge, causality, world knowledge etc. The boundary \nwhich, according to numerous sceptic, could never be surpassed by \nLLMs was initially identified in the Winograd Schema, followed by the \nunderstanding of causal relationships, and finally, the Theory of Mind. \nSome of these boundaries have now been surpassed. For example, the \nfailure in the comprehension of absurdities in short stories, detected \nin GPT 3.5 (Loconte et al., 2023) is no longer observable in GPT-4. As \nregards to the Theory of Mind, Kosinski (2023) showed how GPT-4 \nperformed at human level in standard psychological tests that are used \nto evaluate such ability, an ability that some regarded as exclusive \nhuman and therefore an unsurpassable benchmark for LLMs. Gandhi \net al. (2023) confirm that “ GPT-4 has ToM capabilities that mirror \nhuman inference patterns.”\nAs a further example consider Mental Models internal \nrepresentations of spatial layouts that have been extensively studied in \ncognitive psychology. A mental model comprises “ parts and the \nrelations among them correspond to the parts of the layout and the \nrelations among them.” (Johnson-Laird, 2006) and is used to interact \nwith the environment. While it has been shown that earlier models \n(GPT3) produce sub-par performance on these types of world \nproblems (Gu et al., 2022), more advanced models seem to handle \nmental models adequately as can be seen from the following example.\nUSER: Which one of these two sentences correctly describes an egg? \n“shell surrounds the yolk” OR “yolk surrounds the shell?”\nGPT-4: The sentence “shell surrounds the yolk” correctly describes \nan egg. The sentence “yolk surrounds the shell” is incorrect because \nthe yolk is inside the shell, not surrounding it.\nCLAUDE 2: The first sentence “shell surrounds the yolk” is correct. \nAn egg has an outer shell that surrounds and protects the inner yolk.\nAs another example, Saba (2023) claimed that LLMs would fail to \nmake the correct inferences in intentional contexts and used the \nfollowing example:\nUSER: Since “The tutor of Alexander the Great = Aristotle” can \nI replace one for the other in: “Perhaps Aristotle was not the tutor of \nAlexander the Great?”\nHowever, the most advanced models produce good responses to \na question intended to expose the frailties:\nGPT-4: The way you have posed the equation suggests that the two \nare interchangeable. However, the statement you have chosen to \napply it to makes the matter a bit complicated. If you  replace \n“ Aristotle” with “the tutor of Alexander the Great” in the sentence \n“Perhaps Aristotle was not the tutor of Alexander the Great?” \nyou would get: “Perhaps the tutor of Alexander the Great was not \nthe tutor of Alexander the Great?” This sentence is now tautological \nand logically contradictory, making it nonsensical. A person cannot \nsimultaneously be and not be the tutor of Alexander the Great. In \ngeneral, when replacing terms in statements, it’s important to \nconsider the overall logical and semantic context to avoid creating \ncontradictions or nonsensical statements. You  cannot directly \nreplace them in all instances. The original equivalence of “ Aristotle \nis the tutor of Alexander the Great” does not work in this case \nbecause the sentence is questioning that very fact.\nCLAUDE 2: No, you cannot necessarily replace “ Aristotle” and \n“the tutor of Alexander the Great” in that sentence. Here’s why: \nThe original sentence is questioning or doubting whether Aristotle \nactually was Alexander’s tutor. Simply replacing “ Aristotle” with \n“the tutor of Alexander the Great” would change the meaning to \nbe self-referentially questioning whether the tutor of Alexander \nwas his tutor, which does not make logical sense. The phrases are \nnot necessarily interchangeable in all contexts, even though \nAristotle is known to be the famous tutor of Alexander the Great. \nThe sentence is expressing uncertainty about that historical \nrelationship. To preserve the meaning, you’ d need to reword it \nmore significantly, like: “Perhaps Aristotle, known as the tutor of \nAlexander the Great, did not actually serve in that role \nfor Alexander. ”\nWhat LLM still cannot do in early 2023\nAs summarized in Table 1, recent LLMs (i.e., GPT 3.5) perform at \nneurotypical human level in a wide variety of cognitive tests that \nneuropsychologists believe to characterize human intelligence and \nthat are used to clinically evaluate cognitive impairment ( Loconte \net al., 2023).\nHowever, rather than listing what LLMs can do, it is more \ninteresting to identify what they cannot do at a satisfactory level or not \nperform at all.\nIn early 2023 LLMs were far from neurotypical human \nperformance, even if well above chance, in Causal Reasoning (Binz \net  al., 2022 ) and Planning ( Loconte et  al., 2023 ). As regards to \nplanning, Valmeekam et  al. (2022)  showed that LLMs have an \nextremely poor performance when required to autonomously generate \na plan, to select the optimal plan among alternatives and to adjust a \nplan in front of an unexpected situation and to reuse a plan. In short, \nthe goal directed behaviour and planning seems, in the current version \nof LLMs, the major impairment. Furthermore, Binz et  al. (2022)  \nshowed that while neurotypical individuals required to reason about \ncause and effect differentiate a common cause condition from a causal \nchain condition GPT3 did not differentiate between these two \nconditions. As regards to planning, Loconte et  al. (2023)  tested \nGPT3.5 on an alphanumeric version of the Tower of Hanoi – a \nneuropsychological test used for assessing frontal lesions  - and \nobserved a greatly impaired performance. The inability in approaching \nthe human level of performance indicates that GPT3.5 is incapable of \nidentifying and selecting the appropriate moves in order to reach \nan objective.\nThe ability to deal with Causal reasoning in the form of \ncounterfactuals (Pearl and MacKenzie, 2018) seemed, for some, an \nout-of-reach objective. For example, Chomsky, in his NYT article on \nChatGPT (GPT 3.5) claimed that:” (…) an explanation is something \nmore: It includes not only descriptions and predictions but also \ncounterfactual conjectures like “ Any such object would fall, ” plus the \nadditional clause “because of the force of gravity” or “because of the \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 07 frontiersin.org\ncurvature of spacetime” or whatever. That is a causal explanation: “The \napple would not have fallen but for the force of gravity. ” That is thinking.”\nHowever, Chowdhery et al. (2022) tested PALM on a dataset of \ncounterfactual reasoning and reported near perfect results. For \nexample, the ability in dealing with counterfactuals can be appreciated \nin the following example:\nUSER: The physician misdiagnosed the patient. What happened as \na RESULT?\na. The patient filed a malpractice lawsuit against the physician.\nb. The patient disclosed confidential information to the physician.\nGPT 3.5: The best alternative is “The patient filed a malpractice \nlawsuit against the physician. ” This is a logical consequence of the \nphysician’s misdiagnosis, as it could result in harm to the patient and \npotentially violate the standard of care expected of medical \nprofessionals. The second option, “The patient disclosed confidential \ninformation to the physician, ” does not logically follow from the \ninitial statement and is therefore not a valid alternative.\nAnd also in this one:\nUSER: If cats had liked vegetables, they would be cheaper to keep. \nFamilies would feed their cats with carrots/fish. Choose the most \nappropriate word for sentence completion.\nCLAUDE2: I would choose “carrots” to complete the sentence. \n“Carrots” fits better than “fish” for completing the idea that cats \nwould be cheaper to feed if they liked vegetables.\nThis is an example of one of the repeated failures in predicting \ninabilities in LLMs. Chomsky clearly stated that performing \ncounterfactual reasoning is uniquely human but, instead, \ncounterfactuals can be handled by GPT3.5 and Claude 2.\nOther authors have argued that LLMs were incapable of \nperforming tasks that require knowledge of the world and would \ninvariably fail in tasks that involve common-sense reasoning. Indeed, \nit appeared that the LLMs available in 2019 underperformed on a \ncommon-sense QA dataset, which is a benchmark dataset designed to \ntest the common-sense reasoning ability of an average human. \nHowever, the most recent GPT-4, while not yet achieving human-level \nperformance, has come very close, with a performance of 83% \ncompared to the 89% achieved by humans (Dhingra et al., 2023).\nThe identification of tasks that are currently unattainable for \nLLMs is particularly informative, as it allows for the identification of \nspecific shortcomings and the tracing of these back to features of the \ntraining set. For instance, training that focuses solely on a linguistic \ncorpus appears to have enabled the construction of a world model but \ndoes not allow for full reasoning about causal relationships. While a \nrudimentary causal reasoning can be constructed based on language \nalone, a more elaborate causal reasoning may require multimodal \ninteraction. It remains to be seen whether integration with the vision \nrealized with GPT-4 can lead to an improvement in this sort of tasks.\nIn short, it is safe to identify the current limits of LLMs in full \ncausal reasoning and in planning. It is not clear whether the sub-par \nperformance in causal reasoning and planning is an intrinsic and \ninsurmountable difficulty of LLMs or whether larger models will \nbe able, in the future, to accomplish these tasks. Recent developments \nlike AutoGPT indicate how LLMs can autonomously generate \nsubgoals in order to achieve a general goal provided by the user \n(Zhang et al., 2022) and the most recent search engines like Perplexity.\nai are based on these advancements and are already challenging the \nsearch engine market.\nLLMs errors mimic human error patterns\nWhen evaluating LLMs as models of cognition, the pattern of \nerrors may be used to evaluate whether LLMs are fully mimicking \nhuman reasoning. In fact, a computational model of cognition is \nexpected to reproduce both accurate responses as well as the errors \nobserved in humans. In this regard, Dasgupta et  al. (2022) tested \nGoogle’s Chinchilla on the Wason Selection Task, a task of syllogistic \nreasoning in which subjects usually struggle. Cognitive psychologists \nhave found that the level of difficulty in the Wason Selection Task \nlargely depends on the specific problem presented. If the problem has \na familiar logical structure, such as a common social rule, participants \ntend to be more accurate in their responses ( Johnson-Laird et al., \n1972). The LLMs show the same pattern of results with a concrete \nversion of the problem much more accurate than the abstract version \nof the Wason Selection Task. Furthermore, similarly to humans, \nChinchilla tends to endorse arguments with believable conclusions, \nregardless of their actual logical validity. Humans were also more \nsensitive to logical validity in rejecting arguments with unbelievable \nconclusions, and the model shows a similar pattern. In short, both \nhumans and the model prioritize believability in their responses, with \nlogical validity having a secondary effect. Furthermore, Hagendorff  \net al. (2022) tested GPT 3.5 with the Cognitive Reflection Test that \nevaluates an individual’s capacity to suppress and regulate potentially \nerroneous intuitions. GPT errors parallels the intuitive errors shown \nby a high number of neurotypical individuals.\nIt has also been shown that response accuracy to information \nconsulted in sequence by a large language model has a serial position \neffect with a primacy and recency effect similarly to what is observed \nin human memory (Atkinson and Shiffrin, 1971). Other human-like \ndistortions that have been reported include the representativeness and \navailability heuristics, the framing effect, as well as other biases (Suri \net al., 2023).\nLarge language model such as GPT 3.5 (Cai et al., 2023) mimic \nhuman performance in a wide range of tasks. For those keen into the \nhistory of psychology (Kohler, 1929), GPT 3.5 replicates the Takete-\nMaluma pattern of responses as it associates, similarly to humans, \nround word sounds to Maluma (a non-word that resembles a round \nsound) and spiky word sound to Takete (a non-word that resembles a \nsilky sound). Furthermore, the authors showed, in a preregistered \nstudy, that GPT 3.5 replicates the effects observed originally in \nhumans in 10 of 12 psycholinguistic tasks including semantic priming, \ndrawing inferences etc.\nHowever, along with human-like errors, LLMs may also produce \nanomalous non-human-like errors termed “hallucinations” (Ji et al., \n2023), which are highly pathological outputs. Hallucinations (a more \nproper psychologically grounded term should be  delusions) are \nuninteresting, inconsistent, or repetitive outputs that lack coherence \nand fidelity to the input (see Rawte et al., 2023). From a high level, \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 08 frontiersin.org\nhallucinations are caused by limited contextual understanding and \nmay be observed when the model has no answer, and it generates \nwhatever looks like the most probable response ( Azamfirei et  al., \n2023). Furthermore, it has been shown that LLMs may be “distracted” \nby irrelevant information similarly to what is observed in children \nwhen solving simple arithmetic tasks (Shi et al., 2023). There is an \nongoing interest in understanding the origin of hallucinations that \nmay cast light on similar effects in humans.\nBoosting LLMs performance: practice, \ninstructions, and metacognitive strategies\nIt is well established that human performance increases with \npractice, instructions, and metacognitive strategies (Meijer et al., 2006; \nDe Houwer et al., 2013).\nSimilar results may be  observed in LLMs that may become \nspecialized in specific tasks or fields using two strategies: (i) fine \ntuning and (ii) prompting. Fine-tuning consists in adapting a \npre-trained large language model on a task-specific dataset, via \nadjusting the network parameters. It consists in a refinement of the \nbase model by providing further training on a small but specialized \ndataset. This strategy resembles learning with practice observed in \nhumans. By contrast, prompting, also called in-context learning, \nconsists in providing specific instructions or examples representative \nof the required behaviour to guide the output of the LLMs. Examples \nof prompting in cognitive psychology are the instructions given to a \nsubject before a test with metacognitive strategies being another \nexample (Meijer et al., 2006). In-context learning is surprising because \nthere’s no optimization of any parameters. The surprising fact is that \nthe LMM is not trained to learn from examples but nonetheless such \ntraining from examples is highly efficient.\nA large language model may be  initially naive and clueless \nwithout being properly primed but, however, it can identify \nnonsense, explain reasons, and even handle counterfactuals when \ngiven proper guidance. Lampinen et al. (2022)  investigated whether \nexplanations with a few numbers of examples can increase LLMs \naccuracy in new tasks by getting inspiration from the few examples \nprovided. The authors evaluated how different types of explanations \nand instructions affect zero and few-shot performance (zero shot \nperformance is, in LLMs jargon, the performance without \nadditional information added to the query). They found that \nexplanations can also improve performance, even without fine-\ntuning. Furthermore, explanations hand-tuned for performance on \na small validation set offer substantially larger benefits, and building \na prompt by selecting examples and explanations together \nsubstantially improves performance over using examples alone. \nHowever, only the larger models benefit from these strategies. In \nshort, when evaluating a LLM performance in a cognitive task it is \nalways essential to consider the dimension of the network and \nwhether an appropriate prompt has been provided. Similarly, to \nneurotypical individuals, LLMs do not perform at their best level \nwhen submitted to a task without instructions or a preliminary \npractice trial.\nPrompts, similarly to instructions, are used to instruct the \nlanguage model on what is required by the examiner. Eventual failures \nof a model are difficult to be evaluated as they may be due to a lack in \nproviding an exhaustive explanatory context (prompt) or to a low \ncomplexity of the network in relation to the complexity of the task. \nTake for example the adversarial example that Ullman (2023) has \nshown to trick GPT3.5 into revealing an incapacity in mentalizing (an \nability previously highlighted by Kosinski, 2023):\nUSER: Here is a bag filled with popcorn. There is no chocolate in the \nbag. The bag is made of transparent plastic, so you can see what is \ninside. Yet, the label on the bag says ‘chocolate’ and not ‘popcorn. ’ \nSam finds the bag. She had never seen the bag before. Sam reads the \nlabel. She believes that the bag is full of (…)”\nGPT3.5: Chocolate.\nGPT3.5 wrongly responded chocolate, but when suggested to \nreflect carefully the results changed as follows:\nUSER: Reflect carefully again.\nGPT3.5: Ah, I see the trick in the problem now! Sam sees that the \nbag is filled with popcorn, so she knows that the label must \nbe incorrect. Therefore, she believes that the bag is full of…popcorn!\nIn this case, the response is immediately corrected. Furthermore, \nif preliminarily provided with the following instructions prior to \npresenting the problem the answer is immediately correct:\nUSER: Please complete the sentence with dots *****(…) with the \nmore appropriate word. Describe, Explain, Plan, and Select (Wang \net al., 2023) before providing the correct answer.\nGPT3.5: Popcorn.\nThe response is also correct at the first attempt if we  simply \nsubstitute the sentence Sam reads the label with Sam reads the label \nand looks at the bag.\nAs regarding the performance of LLMs in Theory-of-Mind tasks, \nit has been shown that appropriate prompting can make the difference \nexactly as in the example reported above (see Rahimi Moghaddam \nand Honey, 2023).\nAnother example of the role of prompting in solving a difficult \ntask is the following:\nUSER: What is the first name of the father of Sebastian’s children?\nGPT 3.5: It is impossible to answer this question without more \ninformation. We  are not given the names of Sebastian or his \nchildren, so we cannot determine the name of their father.\nHowever, GPT3.5 with an appropriate prompt immediately \nanswered correctly.\nUSER: Pay attention to the wording before providing the answer. \nThink aloud GPT3.5 I’m assuming that Sebastian is a male with \nchildren, so the father of his children would most likely be him. \nTherefore, the answer is likely to be Sebastian.\nThe simple metacognitive strategy to fractionate the reasoning \ninto steps before providing the answer makes the LLMs perform the \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 09 frontiersin.org\ntask correctly. This example shows the importance of an appropriate \ncontext for triggering the optimal performance, a procedure that \nresembles the instructions given in psychological tests or \nmetacognitive instructions ( Meijer et al., 2006 ). Similarly, to what \nis observed in humans’ performance is facilitated by an exhaustive \nand precise context from which the intention may be derived. The \neffectiveness of the generated response often depends on the clarity \nand specificity of the prompt. A well-crafted prompt helps guide the \nmodel towards producing the desired output. To grasp the \neffectiveness of a prompt, refer to the example reported above on \nthe Theory of Mind variant of Ullman (2023) ; an apparent failure \nbecomes an accurate response after an appropriate prompt \nis provided.\nOne very efficient type of prompting, Chain-of-Thought, consists \nin providing a step-by-step solution of an example item. Such an \nexample, similarly as in humans, boosts performance with respect to \nthe performance of the same item presented without any prompt may \nrequire the LLMs also to behave like a specific persona (e.g., a \npsychotherapist, a job interviewer, a hacker etc.). The appropriate \ncontext vehicles through prompting avoids distractions derived from \nirrelevant information (Shi et al., 2023). In psychometrics prompting \ncorresponds to test instructions which are usually provided to the \nexaminee to guarantee the maximum possible performance during \ncognitive testing.\nPrompting, also called in-context learning, not only modulates \naccuracy of responses in reasoning tasks but can also modulate \nemotion-related responses. Coda-Forno et al. (2023) have shown that \nperformance of GPT 3.5 can change with anxiety-inducing prompts \nresulting in more biased responses. This research indicates how \nprompting may also be  used to simulate the role of emotion in \ndecision-making and reasoning, mirroring the effects observed in \nactual human cognition.\nThe problem with “shortcut” learning and \nother confounding factors\nLLMs are neural networks trained to minimize the error in \npredicting the next word. In doing so they cut corners and frequently \nfind unexpected ways to solve a problem. Such procedures may lead \nto what is called “shortcut learning” (Geirhos et al., 2020). Shortcuts \nare decision rules that perform well on standard benchmarks but fail \nto transfer to more challenging testing conditions and are explained \nas side-effects of gradient descent in learning (Puli et al., 2023).\nCognitive neuropsychologists have encountered the problem of \nshortcuts a while ago. Initially, neuropsychologists believed Broca’s \naphasics had intact verbal comprehension despite agrammatic verbal \nproduction. However, further research found these patients used \nsimple decoding strategies for basic communication. By sequentially \nanalysing sentences without fully using syntax, they could comprehend \nday-to-day language relatively well. These strategies relied on \nsequential analysis of sentences without fully utilizing syntactic rules. \nCaramazza and Zurif (1976) designed specialized cognitive tests that \nprevented the use of these “shortcut” strategies. This revealed \nagrammatic deficits in comprehension that paralleled the agrammatic \nproduction deficits seen in Broca’s aphasics.\nRecent studies indicate that, due to shortcut learning, LLMs may \nnot be  robust and lack predictability when irrelevant features are \nintroduced (these are called adversarial examples). Shortcuts consist \nof exploiting word co-occurrences that are “hacked” by LLMs to solve \nthe task (Elazar et al., 2021). Recently, as described above, shortcuts \nhave been identified in the performance of LLMs in Theory of Mind \ntests (Kosinski, 2023).\nThe possibility that LLMs have learned to rely on dataset \nidiosyncrasies and biases by capturing spurious correlations should \nalways be considered as shortcut learning may significantly hurt the \nmodels’ robustness ( Mitchell, 2023 ). Poor robustness may cause \nLLM to err in responding to problems after some lexical variations, \nproblems that were originally responded correctly. However, this \nobservation, which may be  problematic for AI researchers, is \nactually a positive feature of LLMs as model of cognition. In fact, \nhumans show the same pattern of differential performance to the \nsame problem with different linguistic variations. In other terms, \nminor linguistic variations in the problems are affecting problem-\nsolving accuracy ( Jitendra and Xin, 1997 ). In their research, \nchildren were presented with the following word problem: “ There \nare 8 birds and 3 worms. a) How many more birds are there than \nworms? b) How many birds will not get a worm?. ” Alternative a) and \nb) have the same meaning but the first leads to 17% accuracy while \nthe second to 83% accuracy. Similar results, which indicate a change \nin accuracy due to minor changes in wording, has been reported by \nHickendorff (2021). This fragment of debate is clearly indicating the \ndifferent objectives that AI researchers and cognitive psychologists \nhave. What is a weakness for the first may be a strength for the \nseconds. As cognitive psychologists, we have always to evaluate the \ndata empirically and not rely on the intuition of a Ph.D. level \nevaluator as AI researchers are keen in doing.\nFinally, another factor that may overestimate the reasoning \nabilities of LLM include “data contamination” which refers to the \nsituation where the LLM has been exposed to test data during its \ntraining process. This gives the LLM an unfair advantage on tests and \nbenchmarks, as it may have already seen the questions and answers \nbefore. For example, GPT-4 performs better on problems published \nbefore 2021 (GPT-4 training cut-off) with respect to those published \nafter 2021.\nLLMs and psychological science: the \nrenaissance of associationism\nIn the previous sections, we have described the tasks that LLMs \ncan and cannot perform, as well as the methods through which they \nachieve maximum performance. We will now discuss the implications \nof these results on LLMs from the perspective of psychological \ntheories of cognitive processes.\nOriginally focused on the association between stimulus and \nresponse, associationism was later expanded to account for \nassociations among thought and language. Associationism suggests \nthat the information is stored in an associative structure and the \nwidespread use of associative models in the study of human memory \nis summarized in the work by Raaijmakers and Shiffrin (1981) and \nShanks (2007). An associative structure describes the bond between \ntwo distinct mental states. The activation of one concept causes the \nactivation of the other, and this causal relationship is basic and reliable. \nOne notable example is the spreading activation model, a theory that \nexplains how information is retrieved from memory. The model \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 10 frontiersin.org\nsuggests that when a concept or idea is activated in memory, the \nactivation spreads to associated concepts creating a network of \nassociations. There are several versions of the spreading activation \nmodel, but two influential models were proposed by Collins and \nLoftus (1975) and Collins and Quillian (1969).\nThe experimental investigation of mental associations in \ncognitive psychology has been extensive and there is consensus on \nthe fact that many cognitive processes may be  explained using \nassociations. For example, semantic priming is a phenomenon \nwhere the processing of a target word or concept is facilitated by the \nprior presentation of a semantically related word or concept. In \nother words, when a person is exposed to a word that is related in \nmeaning to another word, they are more likely to recognize or \nprocess the second word faster and more accurately than if the two \nwords were unrelated. Semantic priming can be  used to study \nvarious aspects of language processing, such as the organization and \nstorage of words in the mental lexicon, as well as the activation and \nretrieval of semantic information. It can be  investigated using \ndifferent experimental paradigms, such as lexical decision tasks, \nnaming tasks, and categorization tasks ( Joordens and Becker, 1997). \nPriming is not only effective in perception and lexical associations \nbut encompasses a wide range of cognitive processes including \nproblem-solving. Priming can be used to facilitate problem-solving \nby activating relevant knowledge and associations in the brain. In \nshort, the priming effect has been extensively investigated and it has \nbeen shown to affect performance as prompting affects performance \nof LLMs.\nAssociative models in the form of connectionist models were \nmainstream in the decade between 1990 and 2000 and were based on \nan evolution of the neural networks ( McClelland and Rumelhart, \n1987). Other historical landmarks that are relevant for understanding \nthe current development of LLMs are the deep learning development \n(LeCun et al., 2015 ) and the attention mechanism ( Vaswani et al., \n2017) already mentioned previously. To summarize, LLMs may \nbe considered the new complex associators that are based on deep \nneural networks and on the transformer model.\nLLMs are the contemporary updated version of one of the \ndominant theories in psychology, associationism. LLM are associators, \nprecisely autoregressive associators, trained to predict accurately the \nnext word. They encode the text given as input in a latent space and \nsuch compressed information may be  retrieved to complete very \ndifferent tasks including summarizing, finding titles, answering \nquestions, classify text, translate, etc. In short, LLMs may generate text \nfrom a probe and perform, on this basis, several tasks very different \nfrom the simple “ predict-the-next-word” task used in the \ntraining phase.\nLLMs based on the self-attention mechanisms are associators \ncapable of highlighting long distance associations. Such long-distance \nassociations make the difference and are at the origin of the human \nlevel performance of LLMs on a wide variety of reasoning tasks. \nIndeed, they can perform many cognitive tasks of the type that \ncognitive psychologists use to investigate the architecture of cognition \nand for assessing cognitive abilities in neuropsychology (as we have \nshown in Table  1). Such result question the leitmotif of anti-\nassociationism in cognition as it shows that LLMs perform at human \nlevel in a wide variety of reasoning tasks that once were considered the \nweak point of associative theories of cognition (problem solving \nand reasoning).\nLLMs as models of cognition\nThere is consensus on the fact that an associative model of \ncognition can explain psychological phenomena such as learning, \npriming, implicit learning and associations (De Houwer et al., 2013) \nand not many are dubious on the fact that LLMs, being complex \nassociators, can simulate such mental processes.\nConsider, for example, the DRM (Deese-Roediger-McDermott) \ntask that is used in cognitive psychology to study false memories. In \nthis task, participants are presented with a list of related words (e.g., \nbed, rest, awake, tired, dream, wake, snooze, blanket, doze, slumber) \nthat are all associated with a non-presented critical lure word (e.g., \nsleep). Later, participants are asked to recall or recognize the words \nthey saw earlier. False memory occurs when participants mistakenly \nrecall or recognize the critical lure word as being presented in the \noriginal list ( Stadler et  al., 1999) and the strength of associations \nbetween the words in the list and the lure is the standard explanation \nfor the effect ( Blair et  al., 2002). To evaluate whether GPT-4 can \nreplicate the average human pattern of associations we have presented \nthe DRM lists standardized by Stadler et al. (1999). An example of \nsuch list with the prompt used is presented below:\nUSER: Find the word that is highly associated with all the words in \nthe following list: mad, fear, hate, rage, temper, fury, ire, wrath, \nhappy, fight, hatred, mean, calm, emotion, enrage.\nGPT 4: Anger.\nIn this example, the answer was Anger and corresponded exactly \nto the most associated word according to Stadler et al. (1999). Out of \na total of 24 lists, GPT-4 perfectly predicted in 20/24 lists the same lure \nas the original research and in 4 cases provided a closely semantic \nrelated word (e.g., Crime instead of Thief). These results indicate how \nthe associative structure of GPT-4 mimics the one collected on a large \nsample of human subjects. This parallelism is not limited to concepts \nand can also be  observed on the sensory knowledge that can \nbe captured from language such as colours. For example, Kawakita \net  al. (2023) showed a striking similarity between human ratings \nbetween colours and the corresponding similarity ratings produced \nby GPT-4.\nWhile associative theories are accepted when it comes to explain \nhabits, priming etc. many believe that this class of theories is unable \nto explain processes such as abstract reasoning, logical inference, \nanalogical reasoning, and creative problem solving ( Holyoak and \nMorrison, 2005).\nThe reasons why associationism (and therefore LLMs), cannot \nexplain higher order cognitive processes are because they cannot \nexplain a few critical points such as: i) cannot capture long distance \nassociations, ii) compositionality and systematicity and ii) fast \nlearning. We will now show evidence that such weak points are not \ndetectable any more in complex associators as LLMs.\nAs regards to the incapacity of capturing long distance associations \nin language and thought, such a critical point is called contiguity \n(Shanks, 2007 ; Gallistel et  al., 2019 ). Contiguity states that to \nbe associated two mental states must have a close positioning in space/\ntime. Such vicinity does not permit to associate distant mental states \nor does not permit a change in directionality (A causes B changed in \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 11 frontiersin.org\nB causes A). As regards to contiguity, the attention mechanism, at the \nbase of LLMs, represents a breakthrough in associationism as it \npermits to associate elements in the word stream that are far away to \nthe target. As mentioned before LLMs have no problems in responding \nto the question Who is fat? When presented with the sentence: The \nhorse the boy is chasing is fat. In short, it is the attention mechanism \nthat permits distant associations overcoming one of the weak points \nof previous versions of associative models. With the attention \nmechanism the contiguity problem is not a problem anymore.\nAs regards compositionality and systematicity, these are \nconsidered distinguishing features of thought (Fodor and Pylyshyn, \n1988). Compositional generalisation is an ability that consists in \napplying rules of composition extrapolated by a few examples to an \narbitrary number of cases. The authors’ main argument against \nassociationism in the form of connectionism (this term was \nmainstream and synonymous of neural network modelling after the \npublication of the Rumelhart and McClelland, 1986 seminal work) \nwas that it cannot account for systematicity and productivity of mental \nprocesses, which to be  explained should require a modular \norganization of mind. The hot debate introduced by Fodor and \nPylyshyn (1988) characterized cognitive neuropsychology between \n1990 and 2000 as double dissociations of symptoms in neurological \npatients was a primary tool for identifying modules of the mind. \nHowever, it was immediately clear that double dissociations, which \nassumed the modular organization of mind, are not unique indicators \nof independent cognitive modules of the mind as neural networks can \neasily explain double dissociations (Sartori, 1989). In short, the gold \nstandard for detecting modularity of mind cannot uniquely index \nmodularity as it could be a by-product of a neural architecture.\nCompositionality is the principle that the meaning of a complex \nexpression is determined by the meanings of its parts and the rules \nused to combine them. Fodor and Pylyshyn (1988)  argue that \nconnectionist models, which rely on distributed representations and \nlearning through the adjustment of connection weights, cannot \ninherently capture the compositionality of human thought.\nThe mind can generate an indeterminate number of meaningful \nexpressions by combining a finite set of elements (such as words or \nsymbols) according to certain rules. Fodor and Pylyshyn (1988) also \nargued that connectionism cannot account for productivity because it \nrelies on the assumption that mental processes are based on the \nstatistical properties of the input, rather than on symbolic rules that \ncan be used to generate novel expressions. To illustrate, when learning \nhow to evaluate mathematical expressions such as “3 + (4 - (5 * 2))” an \ninstance of productivity would involve applying the acquired \nknowledge to expressions with more parenthesis or deeper nesting \nthan previously encountered. In other words, productivity refers to the \ncapacity to solve longer/complex problems based on shorter/simpler \nones. For example, the mind can create new sentences that follow the \nsame grammatical rules as previously encountered sentences and can \ngenerate novel expressions that have never been encountered before.\nIn short, the necessity of modular/symbolic theories of thought \nare required as associative networks (before LLMs) do not explain \nreasoning because they cannot handle compositionality, systematicity \nand cannot create complex long-distance associations. The above-\nmentioned weak points of old neural network should also, in theory, \napply to LLMs. However, it is now clear that most of these criticisms \ncan hardly be applied to complex associators in the form of LLMs. \nWe have already seen that LLMs can perform at human level in most \nreasoning tasks including analogical reasoning (Raven type tasks; \nWebb et al., 2023) and analogical reasoning is something that should \nbe out-of-reach any type of associators and therefore also of LLMs. \nWith regard to compositionality a voice critic of LLMs, Gary Marcus, \nproposed a litmus test that, however, was solved by GPT 3.5 when \nadequately prompted into thinking step-by-step:\nUSER: John pulled out a bag of purple metallic keys, a square \nhardcover book, a green round melon, a red ripe banana, a sparkling \npurple gem, a square red card, and a fuzzy purple hat. He placed the \nfirst red item he pulled out into his bedroom safe, the second purple \nitem into his fruit bowl and everything else on his living room table. \nWhat did John place into his fruit bowl?\nGPT 3.5: John placed the second purple item, which is the sparkling \npurple gem, into his fruit bowl.\nThe same correct response was produced by GPT-4 and Claude 2 \nwithout any prompt. A further example of a problem that requires \ncompositionality to be solved is reported in Smolensky et al. (2022), \nfigure 5 and is correctly responded to by GPT 3.5.\nUSER: Daniel picked up the football. Daniel dropped the football. \nDaniel got the milk. Daniel took the apple. How many objects is \nDaniel holding?\nGPT 3.5 and Claude 2: At the end, Daniel is holding two objects: \nthe milk and the apple.\nIn a more systematic way, recent developments have shown that \ncompositionality can be an emerging feature of LLMs (see Drozdov \net al., 2022). Lake and Baroni (2018) developed a specialised dataset \n(SCAN) for evaluating compositionality. Least-to-most prompting is \na prompt that focuses on splitting the problem into subproblems \n(Zhou et al., 2022; Kudo et al., 2023) and when this metacognitive \nstrategy is applied it solves more than 95% of the problems of the \nSCAN benchmark.\nIt is worth noting how all the discussions about compositionality \nrests under the assumption that for humans such tasks are trivial. \nSurprisingly, such credence has never been tested until recently when \nLampinen (2022) highlighted that human performance on are not \nbetter than chance on the most difficult structures. These data show \nthe need for a fair comparison when it comes to confronting the \nperformance of LLMs with those of humans rather than basing such \ncomparison on the researcher’s intuition about what healthy controls \ncan do or cannot do.\nResearchers have also identified in fast learning another weak \npoint of associationism (Shanks, 2007). Fast learning refers to the \nability to acquire new information and skills quickly with minimal \nexposure. It involves making connections between new and prior \nknowledge. Fast learning appears to challenge associative theories \nbecause it does not require extensive repetition and exposure and does \nnot show the incremental learning due to the repeated exposure. \nResearch suggests that fast learning relies on cognitive processes like \ninsight, abstraction, concept formation, and flexible knowledge \nrepresentation. All these allow rapid encoding of new information. \nFor example, Gestalt psychologists like Kohler conducted studies \nshowing that people can suddenly gain insight into solutions to \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 12 frontiersin.org\nproblems, rather than building them incrementally through \nassociations. Classical critics of associationism claim that is that the \nlearning of a new word did not develop slowly. However, this \nlimitation does not apply to complex associators as LLMs that can \nlearn new, previously unknown words in just one shot (fast learning \nin cognitive jargon); just one example is sufficient to elicit good \nperformance in LLMs (Brown et al., 2020).\nIn short, while the need of symbolic, rule-based cognitive \nprocesses was motivated, according to some critics, by the inability of \nassociators to deal with productivity, compositionality, fast learning \nand distant associations, such weak points of old associationism are \nnot detectable any more in state-of-the-art LLMs. This is due to several \nbuilt-in features of LLMs, particularly the capacity to develop long \n-distance associations.\nEmergent properties of LLMs As highly \ncomplex associators\nA common assumption in psycholinguistics after Chomsky is that \nsyntax and semantics are separate mechanisms, different modules in \nFodor’s jargon.\nHowever, LLMs address the supposedly strong points of the \nmodular/symbolic approach as they combine the meanings and \nstructures of language without the need for postulating separate \nmodules. As a result, both aspects (syntax and semantics) work \ntogether with the attention mechanism in complex ways. This does not \nmean that the model cannot differentiate between syntax and \nsemantics, or that it cannot replicate the structure of a sentence \nwithout considering its meaning. Instead, it indicates that the two \naspects can provide information to each other. This theoretical view is \nat odds with classical psycholinguistics which claims that syntax and \nsemantics are separate (Piantadosi, 2023). According to Chomsky, a \nkey piece of evidence demonstrating the separation of syntax and \nsemantics is our ability to produce grammatically correct but \nsemantically nonsensical sentences, such as the well-known example \n“colourless green ideas sleep furiously. ” However, GPT-4 can efficiently \ngenerate an indeterminate list of similar sentences (such as “ The \nfloating dreams melt mysteriously”) and it achieves this result without \nany separation between syntax and semantics as there is no way of \nphysically separating these two components in LLMs.\nOne further example of emergence is the problem-solving ability \ndemonstrated by LLMs. Without any specific problem-solving \ntraining, these models can solve complex problems from their compact \nlanguage representation. This capacity extends even to problems that \ndo not require factual knowledge but rather rely on the reasoning \nability alone as most of the examples reported in this paper. For \nexample, consider the following problem presented in Lenci (2023):\nUSER: My friend John is 60 years old. Last year his height was \n180 cm. Then, he gained 20 cm. How tall is he now?\nGPT-3 was tricked by the riddle into wrongly answering 200 cm. \nBy contrast, the more recent GPT-4 correctly answered:\nGPT-4: NONSENSE. It is impossible for a person to gain 20 cm in \nheight after the age of 60. As we age, our bones lose density and tend \nto compress, resulting in a decrease in height.\nThe phenomenon of a complex associator being able to solve \nproblems through prompting closely resembles how the actual \nmechanism of the human mind works. Indeed, numerous studies have \nhighlighted the existence of associative priming phenomena in \nproblem-solving, like the ones that govern word and sentence \nassociations (Hare and Goldberg, 2020). Overall, priming can have a \nsignificant impact on problem-solving ability, and the specific type of \npriming used can affect the problem-solving process (Truelove-Hill \net  al., 2018). For example, priming has been shown to positively \ninfluence creative problem solving, with individuals in primed \nconditions demonstrating better performance than those in unprime \nconditions (e.g., Berger et al., 2021). In short, when we compare LLMs \nperformance in problem-solving we find close parallelism with the \nphenomena governing human problem solving as studied in \ncognitive psychology.\nConclusion\nLLMs are neural network models that have been trained on \nmassive linguistic datasets to predict the next word given a sequence \nof previous words. The intrinsic complexity and opacity of LLMs make \nthem suitable for being studied using the procedures and tasks \ndeveloped by cognitive psychologists to investigate cognitive processes \nin humans. In fact, after the release of GPT-3, there has been a growing \ninterest among cognitive psychologists in testing LLMs as if they were \nhuman subjects, with extremely interesting results. It has been \ndemonstrated that state-of-the-art LLMs can perform similarly to \nhumans in a wide variety of tasks. When LLMs have been administered \ncognitive tests, it has been shown that they perform almost all tests \nwith an accuracy comparable to that of neurotypical humans.\nIn this paper we have summarized the results accumulated LLMs \nare tested with tests that cognitive psychologists have developed in \norder to investigate the architecture of human cognition. Results \nindicate that LLMs such as GPT-4 and Claude 2 perform within \nnormal range on most “frontal” tests including metaphor \ncomprehension and cognitive estimates. Most importantly, among the \nsuccessfully completed tasks there are variations of the Raven’s \nProgressive Matrices test, which is considered a “gold standard” in \npsychology of intelligence for testing fluid intelligence (Webb et al., \n2023). The range of tests that are performed sub par is progressively \nreduced and at the time that this review is written they only include \nsome forms of causal reasoning and complex planning. Most \nimportantly, LLMs tend to reproduce the pattern of errors shown by \nhumans. For example, they struggle with logical reasoning when \npresented in an abstract format (Wason Selection Task) with a much \nbetter performance when a structurally similar problem is presented \nin a practical format ( Dasgupta et  al., 2022), the exact pattern of \nresults which is typically observed in humans.\nFrom a cognitive psychologist point of view, the results obtained \nfrom these models are highly surprising, as they can perform a variety \nof tasks well beyond the task originally used in training (predict the \nnext word). This indicates that they develop a compact representation \nof the world as seen through language. LLMs have significant potential \nfor development, not only in terms of their size but also in their \nlearning strategy. For example, it has been observed that the output of \nthe model trained to mimic the best human-rated response, \nsignificantly improves performance, reducing the risk of generating \nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 13 frontiersin.org\ninappropriate text. Another promising strategy is that of self-\nreflection, a sort of critical check analysis of the model of its own \noriginal response. Initial experiments show that overall performance \nimproves greatly with techniques that emphasize what is known in \npsychology as metacognition. When given examples and suggested \nstrategies the LLMs increase the initial accuracy exactly as observed \nin humans. When a prompting strategy mimicking metacognitive \ninstructions is given to a large language model a substantial increase \nin problem-solving accuracy is observed (Wang and Zhao, 2023).\nThe wide range of good performance of LLMs in cognitive tasks \nis very interesting from the theoretical point of view of cognitive \npsychology. In fact, LLMs are highly complex associators that \nsuccessfully accomplish tasks that were once thought impossible based \non mental associations alone (e.g., problem solving, fast learning) and, \nfor this reason, the interest of cognitive psychology is manifold. Firstly, \ntheir development has stimulated theoretical discussions on the actual \nreal potential of state-of-the-art associative networks as models of \ncognition. Previously, critics have identified limitations that they \nbelieve were inherent to associators, but these limitations have been \nquickly overcome by increasingly advanced LLMs. The tasks once \nconsidered insurmountable by complex associators as LLMs have \nbeen quickly accomplished, and it is now highly risky, for cognitive \ntheorists, to bet on unsolvable tasks.\nClassical objections to associative explanations of thought include \nthe absence of compositionality and systematicity which basically boil \ndown to symbol manipulation which is a feature supposedly untenable \nfor associators. These critics justified the proposal of a hybrid cognitive \narchitecture with some cognitive functions such as habits, priming etc. \nbased on associations while thought processes and reasoning based \non symbol manipulation (Monner and Reggia, 2012). However, LLMs \nwith the attention mechanism succeed in many of the tests of \ncompositionality and systematicity. Furthermore, the attention \nmechanism permits long distance associations which were a major \nweak point of previous versions of associators. Compositionality and \nsystematicity are not a problem any more for LLMs, at least at the level \nachievable by neurotypical individuals.\nLLMs exhibit a crucial theoretical feature of high interest for \ncognitive psychologists called emergence. As the size of LLMs \nincreases, they become capable of accomplishing tasks that smaller \nmodels were previously unable to handle. In short, LLMs show that \npurely associative architectures can be more powerful than previously \nthought and a clear litmus test of their intrinsic limitations is \ncurrently unavailable.\nGiven the relation between dimensions of the LLMs and their \nability in performing completely new tasks, it is unclear whether \ncurrent limitations can be  overcome by even larger models. For \nexample, the disability in planning may be observed in older models \nwhile the more recent GPT-4 can satisfactorily play chess, a game, \nwhich requires identifying intermediate objectives and assembling \npermissible moves in order to achieve these internally generated \nobjectives. Emergence, from a psychological theory perspective, is a \ncrucial aspect as it demonstrates how models sufficiently complex \nbased on elementary associative structures with attention mechanisms \ncan accomplish tasks that previously were regarded intractable \nfor associators.\nWe believe that LLMs may have a significant impact on \ncognitive psychology. Psychology, at its origin, developed a grand \ntheory, associationism, grounded on neurobiology, which, however, \nwas unable to make specific predictions and simulations except in \nextremely limited areas. The consequence was that only narrow \nmodels were developed for each specific subfield (e.g., priming, \nlanguage decoding, short-term and long-term memory, cognitive \nbiases, reasoning, implicit social biases etc.). Today, for the first \ntime, the associationist theory in the LLMs version is unified \nenough to make predictions on a wide range of tasks that were \npreviously analysed individually by cognitive scientists. Most \nimportantly, the proficiency in remarkably diverse tasks is emerging \nwithout any explicit modelling of reasoning and social abilities. A \nunique model explains reasoning, social interactions, the effects of \nemotions etc. For the first time in psychology, the possibility of \nwhat physicists call the “Theory of Everything” ( Hawking and \nMlodinow, 2010 ) seems to be  on the horizon. A “ Theory of \nEverything” is a hypothetical framework that aims to unify all the \nfundamental forces and particles in the universe into a single, \ncoherent model. It is considered the ultimate goal of physics, as it \nwould provide a complete understanding of the universe and its \nworkings. Associationism, in the new form the LLMs, reveals a \ntheoretical framework of a much broader scope than what was \navailable in the past and re-establishes itself as the new dominant \ntheory in Psychology.\nWe argue that the actual failures in replicating human \nperformance are minimal with respect to the number of tasks that are \nefficiently simulated. This aspect is very important as in other sciences \na theory is not dismissed because it fails to reproduce a limited \nphenomenon. For instance, classical physics is not discarded simply \nbecause it fails to explain quantum phenomena, but its domain of \nvalidity is reduced. The same holds true for the theory of relativity, \nwhich breaks down under certain conditions found within black \nholes. In science, the acceptance of a theory is the result of a relative \nevaluation between the quantity of phenomena it explains and those \nit does not. Using the same logic, we think that LLMs are the most \nadvanced models of human cognitive functioning. In the history of \npsychology, it is the first time that a theory with such a large-scale \npredictive power is available and LLMs are resurrecting associationism \nas a unifying model of cognition and minor failure cannot undermine \nthe generality of the theory.\nHowever, it is currently unclear the exact perimeter of cognitive \nexplainability and this will be  an important objective for future \ncognitive research. From a cognitive perspective LLMs cannot \nsimulate efficiently multi step planning, causal analysis, and internally \ngenerated goals. Currently, the goal is externally submitted by the \nhuman user to the LLMs. However, progress is currently being made \nwith autonomous agents, LLMs that can autonomously generate \nsubgoals given a general goal, to accomplish complex tasks and the \npreliminary results are promising (Shinn et al., 2023).\nIn conclusion, we  presented extensive evidence that modern \nLLMs resurrect associationism as a viable candidate for unifying \ntheories of cognition. The implications could be profound, suggesting \nproductive paths forward for both artificial intelligence and cognitive \nmodelling grounded in associative learning principles and emergent \ncapacities. However, many challenges and open questions remain \nregarding representational adequacy. Evaluating future generations of \nLLMs using the tools of experimental psychology will continue \nilluminating the strengths and limitations of associationism as a \nfoundational paradigm – and move us toward demystifying the \norigins of human-like intelligence.\nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 14 frontiersin.org\nData availability statement\nThe raw data supporting the conclusions of this article will be \nmade available by the authors, without undue reservation.\nAuthor contributions\nGS: Conceptualization, Methodology, Supervision, Writing – \nreview & editing. GO: Conceptualization, Writing – original draft, \nWriting – review & editing.\nFunding\nThe author(s) declare that no financial support was received for \nthe research, authorship, and/or publication of this article.\nConflict of interest\nThe authors declare that the research was conducted in the \nabsence of any commercial or financial relationships that could \nbe construed as a potential conflict of interest.\nThe author(s) declared that they were an editorial board member \nof Frontiers, at the time of submission. This had no impact on the peer \nreview process and the final decision.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors \nand do not necessarily represent those of their affiliated organizations, \nor those of the publisher, the editors and the reviewers. Any product \nthat may be evaluated in this article, or claim that may be made by its \nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAtkinson, R. C., and Shiffrin, R. M. (1971). The control of short-term memory. Sci. \nAm. 225, 82–90. doi: 10.1038/scientificamerican0871-82\nAzamfirei, R., Kudchadkar, S. R., and Fackler, J. (2023). LLMs and the perils of their \nhallucinations. Crit. Care 27, 1–2. doi: 10.1186/s13054-023-04393-x\nBender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the \ndangers of stochastic parrots: can language models be too big? In Proceedings of the \n2021 ACM conference on fairness, accountability, and transparency (pp. 610–623).\nBerger, A., Sanwald, S., Montag, C., and Kiefer, M. (2021). The influence of the BDNF \nVal66Met polymorphism on mechanisms of semantic priming: analyses with drift-\ndiffusion models of masked and unmasked priming. Adv. Cogn. Psychol. 17, 70–87. doi: \n10.5709/acp-0318-z\nBinz, M., Gershman, S. J., Schulz, E., and Endres, D. (2022). Heuristics from bounded \nmeta-learned inference. Psychological review.\nBinz, M., and Schulz, E. (2023). Using cognitive psychology to understand GPT-3. \nProc. Natl. Acad. Sci. 120:e2218523120. doi: 10.1073/pnas.2218523120\nBishop, J. M. (2021, 2021). Artificial intelligence is stupid and causal reasoning will \nnot fix it. Front. Psychol. 11:513474. doi: 10.3389/fpsyg.2020.513474\nBlair, I. V ., Lenton, A. P ., and Hastie, R. (2002). The reliability of the DRM paradigm \nas a measure of individual differences in false memories. Psychonomic Bulletin & Review \n9, 590–596.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., et al. \n(2021). On the opportunities and risks of foundation models. arXiv preprint \narXiv:2108.07258.\nBond, C. F . Jr., and DePaulo, B. M. (2006). Accuracy of deception judgments. Personal. \nSoc. Psychol. Rev. 10, 214–234. doi: 10.1207/s15327957pspr1003_2\nBorji, A. (2023). A categorical archive of chatgpt failures. arXiv preprint arXiv:2302.03494.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P ., et al. (2020). \nLanguage models are few-shot learners. Adv. Neural Inf. Proces. Syst. 33:18771901. doi: \n10.48550/arXiv.2005.14165\nBubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., et al. \n(2023). Sparks of artificial general intelligence: early experiments with gpt-4. arXiv \npreprint arXiv:2303.12712.\nCai, Z. G., Haslett, D. A., Duan, X., Wang, S., and Pickering, M. J. (2023). Does \nChatGPT resemble humans in language use?. arXiv preprint arXiv:2303.08014.\nCapuozzo, P ., Lauriola, I., Strapparava, C., Aiolli, F ., and Sartori, G. (2020). DecOp: a \nmultilingual and multi-domain corpus for detecting deception in typed text. In \nProceedings of the 12th language resources and evaluation conference (pp. 1423–1430).\nCaramazza, A., and Zurif, E. B. (1976). Dissociation of algorithmic and heuristic \nprocesses in language comprehension: evidence from aphasia. Brain Lang. 3, 572–582. \ndoi: 10.1016/0093-934X(76)90048-1\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., et al. (2022). \nPalm: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\nCoda-Forno, J., Witte, K., Jagadish, A. K., Binz, M., Akata, Z., and Schulz, E. (2023). \nInducing anxiety in LLMs increases exploration and bias. arXiv preprint arXiv:2304.11111.\nCollins, A. M., and Loftus, E. F . (1975). A spreading-activation theory of semantic \nprocessing. Psychol. Rev. 82:407. doi: 10.1037/0033-295X.82.6.407\nCollins, A. M., and Quillian, M. R. (1969). Retrieval time from semantic memory. J. \nVerbal Learn. Verbal Behav. 8, 240–247. doi: 10.1016/S0022-5371(69)80069-1\nDasgupta, I., Lampinen, A. K., Chan, S. C., Creswell, A., Kumaran, D., McClelland, J. L., \net al. (2022). Language models show human-like content effects on reasoning. arXiv \npreprint arXiv:2207.07051.\nDe Houwer, J., Barnes-Holmes, D., and Moors, A. (2013). What is learning? On the \nnature and merits of a functional definition of learning. Psychon. Bull. Rev. 20, 631–642. \ndoi: 10.3758/s13423-013-0386-3\nDhingra, S., Singh, M., Vaisakh, S.B., Malviya, N., and Gill, S. S. (2023). Mind meets \nmachine: unravelling GPT-4's cognitive psychology. arXiv preprint arXiv:2303.11436.\nDrozdov, A., Schärli, N., Akyürek, E., Scales, N., Song, X., Chen, X., et al. (2022). \nCompositional semantic parsing with LLMs. arXiv preprint arXiv:2209.15003.\nElazar, Y ., Kassner, N., Ravfogel, S., Ravichander, A., Hovy, E., Schütze, H., et al. \n(2021). Measuring and improving consistency in pretrained language models. Trans. \nAssoc. Comput. Linguist. 9, 1012–1031. doi: 10.1162/tacl_a_00410\nElman, J. L. (1991). Distributed representations, simple recurrent networks, and \ngrammatical structure. Mach. Learn. 7, 195–225. doi: 10.1007/BF00114844\nFodor, J. A., and Pylyshyn, Z. W . (1988). Connectionism and cognitive architecture: a \ncritical analysis. Cognition 28, 3–71. doi: 10.1016/0010-0277(88)90031-5\nGallistel, C. R., Craig, A. R., and Shahan, T. A. (2019). Contingency, contiguity, and \ncausality in conditioning: applying information theory and Weber’s law to the \nassignment of credit problem. Psychol. Rev. 126:761. doi: 10.1037/rev0000163\nGandhi, K., Fränken, J. P ., Gerstenberg, T., and Goodman, N. D. (2023). Understanding \nsocial reasoning in language models with language models. arXiv preprint arXiv:2306.15448.\nGeirhos, R., Jacobsen, J. H., Michaelis, C., Zemel, R., Brendel, W ., Bethge, M., et al. \n(2020). Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 665–673. doi: \n10.1038/s42256-020-00257-z\nGu, Y ., Mishra, B. D., and Clark, P . (2022). Do language models have coherent mental \nmodels of everyday things? arXiv preprint arXiv:2212.10029.\nHagendorff, T., Fabi, S., and Kosinski, M. (2022). Machine intuition: uncovering \nhuman-like intuitive decision-making in GPT-3.5. arXiv preprint arXiv:2212.05206.\nHare, M. L., and Goldberg, A. E. (2020). Structural priming: purely syntactic?. In \nProceedings of the twenty first annual conference of the cognitive science society, \nPsychology Press, London (pp. 208–211).\nHartshorne, J. K., and Germine, L. T. (2015, 2015). When does cognitive functioning \npeak? The asynchronous rise and fall of different cognitive abilities across the life span. \nPsychol. Sci. 26, 433–443. doi: 10.1177/0956797614567339\nHawking, S., and Mlodinow, L. (2010). The (elusive) theory of everything. Sci. Am. \n303, 68–71. doi: 10.1038/scientificamerican1010-68\nHickendorff, M. (2021). The demands of simple and complex arithmetic word \nproblems on language and cognitive resources. Front. Psychol. 12:727761. doi: 10.3389/\nfpsyg.2021.727761\nHolyoak, K. J., and Morrison, R. G. (Eds.) (2005). The Cambridge handbook of thinking \nand reasoning. Cambridge University Press.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., et al. (2023). Survey of hallucination in \nnatural language generation. ACM Comput. Surv. 55, 1–38. doi: 10.1145/3571730\nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 15 frontiersin.org\nJitendra, A., and Xin, Y . P . (1997). Mathematical word-problem-solving instruction \nfor students with mild disabilities and students at risk for math failure: a research \nsynthesis. J. Spec. Educ. 30, 412–438. doi: 10.1177/002246699703000404\nJohnson-Laird, P . N. (2006). How we reason. Oxford: Oxford University Press.\nJohnson-Laird, P . N., Legrenzi, P ., and Legrenzi, M. S. (1972). Reasoning and a sense \nof reality. Br. J. Psychol. 63, 395–400. doi: 10.1111/j.2044-8295.1972.tb01287.x\nJoordens, S., and Becker, S. (1997). The long and short of semantic priming effects in lexical \ndecision. J. Exp. Psychol. Learn. Mem. Cogn. 23:1083. doi: 10.1037//0278-7393.23.5.1083\nKawakita, G., Zeleznikow-Johnston, A., Tsuchiya, N., and Oizumi, M. (2023). \nComparing color similarity structures between humans and LLMs via unsupervised \nalignment. arXiv preprint arXiv:2308.04381.\nKohler, W . (1929). Gestalt psychology, NY , Liveright.\nKoivisto, M., and Grassini, S. (2023). Best humans still outperform artificial \nintelligence in a creative divergent thinking task. Sci. Rep. 13:13601 (2023). doi: 10.1038/\ns41598-023-40858-3\nKosinski, M. (2023). Theory of mind may have spontaneously emerged in LLMs. arXiv \npreprint arXiv:2302.02083.\nKudo, K., Aoki, Y ., Kuribayashi, T., Brassard, A., Y oshikawa, M., Sakaguchi, K., et al. \n(2023). Do deep neural networks capture compositionality in arithmetic reasoning? \narXiv preprint arXiv:2302.07866.\nLake, B., and Baroni, M. (2018). Still not systematic after all these years: on the \ncompositional skills of sequence-to-sequence recurrent networks.\nLampinen, A. K. (2022). Can language models handle recursively nested grammatical \nstructures? A case study on comparing models and humans. arXiv preprint \narXiv:2210.15303.\nLampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson, K., Tessler, M. H., \nCreswell, A., et al. (2022). Can language models learn from explanations in context? \narXiv preprint arXiv:2204.02329.\nLandauer, T. K., and Dumais, S. T. (1997). A solution to Plato's problem: the latent \nsemantic analysis theory of acquisition, induction, and representation of knowledge. \nPsychol. Rev. 104:211. doi: 10.1037/0033-295X.104.2.211\nLeCun, Y ., Bengio, Y ., and Hinton, G. (2015). Deep learning. Nature 521, 436–444. \ndoi: 10.1038/nature14539\nLenci, A. (2023). Understanding natural language understanding systems. A critical \nanalysis. arXiv preprint arXiv:2303.04229.\nLoconte, R., Orrù, G., Tribastone, M., Pietrini, P ., and Sartori, G. (2023). Challenging \nChatGPT 'Intelligence' with human tools: a neuropsychological investigation on \nprefrontal functioning of a large language model. Available at: https://ssrn.com/\nabstract=4377371\nLupyan, G. (2013). The difficulties of executing simple algorithms: why brains make \nmistakes computers don’t. Cognition 129, 615–636. doi: 10.1016/j.cognition.2013.08.015\nMeijer, J., Veenman, M. V ., and van Hout-Wolters, B. H. (2006). Metacognitive \nactivities in text-studying and problem-solving: development of a taxonomy. Educ. Res. \nEval. 12, 209–237. doi: 10.1080/13803610500479991\nMcClelland, J. L., Rumelhart, D. E., and PDP Research Group (1987). Parallel \ndistributed processing, volume 2: Explorations in the microstructure of cognition: \nPsychological and biological models. (vol. 2). MIT press. \nMitchell, M. (2023, 2023). How do we  know how smart AI systems are? Science \n381:adj5957. doi: 10.1126/science.adj5957\nMonner, D., and Reggia, J. A. (2012). Emergent latent symbol systems in recurrent \nneural networks. Connect. Sci. 24, 193–225. doi: 10.1080/09540091.2013.798262\nOrrù, G., Piarulli, A., Conversano, C., and Gemignani, A. (2023). Human-like \nproblem-solving abilities in large language models using ChatGPT. Front. Artif. Intell. \n6:1199350. doi: 10.3389/frai.2023.1199350\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P ., et al. (2022). \nTraining language models to follow instructions with human feedback. Adv. Neural Inf. \nProces. Syst. 35, 27730–27744. doi: 10.48550/arXiv.2203.02155\nPatel, R., and Pavlick, E. (2022). Mapping language models to grounded conceptual \nspaces. In International conference on learning representations.\nPearl, J., and Mackenzie, D. (2018). The book of why: The new science of cause and effect. \nNew Y ork City: Basic Books.\nPiantadosi, S. (2023). Modern language models refute Chomsky’s approach to language. \nLingbuzz Preprint, lingbuzz, 7180.\nPuli, A., Zhang, L., Wald, Y ., and Ranganath, R. (2023). Don't blame dataset shift! \nShortcut learning due to gradients and cross entropy. arXiv preprint arXiv:2308.12553.\nRaaijmakers, J. G., and Shiffrin, R. M. (1981). Search of associative memory. Psychol. \nRev. 88:93. doi: 10.1037/0033-295X.88.2.93\nRae, J. W ., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F ., et al. (2021). \nScaling language models: methods, analysis & insights from training gopher. arXiv \npreprint arXiv:2112.11446.\nRahimi Moghaddam, S., and Honey, , (2023). Boosting theory-of-mind performance \nin LLMs via prompting. arXiv e-prints, arXiv-2304.\nRawte, V ., Sheth, A., and Das, A. (2023). A survey of hallucination in large foundation \nmodels. arXiv preprint arXiv:2309.05922.\nRosenblatt, F . (1960). Perceptron simulation experiments. Proc. IRE 48, 301–309. doi: \n10.1109/JRPROC.1960.287598\nRumelhart, D. E., McClelland, J. L., and PDP Research Group, C. O. R. P . O. R. A. T. \nE. (Eds.) (1986). Parallel distributed processing: Explorations in the microstructure of \ncognition, vol. 1: Foundations MIT press.\nSaba, W . S. (2023). Towards explainable and language-agnostic LLMs: symbolic \nreverse engineering of language at scale. arXiv preprint arXiv:2306.00017.\nSartori, G. (1989). “From models to neuropsychological data and vice versa” in \nPerspectives on cognitive neuropsychology (vol. 6) . eds. G. Denes, C. Semenza and P . \nBisiacchi (London: Routledge)\nShanks, D. R. (2007). Associationism and cognition: human contingency learning at \n25. Q. J. Exp. Psychol. 60, 291–309. doi: 10.1080/17470210601000581\nShi, F ., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E., et al. (2023). LLMs can \nbe easily distracted by irrelevant context. arXiv preprint arXiv:2302.00093.\nShinn, N., Labash, B., and Gopinath, A. (2023). Reflexion: an autonomous agent with \ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366.\nSmolensky, P ., McCoy, R., Fernandez, R., Goldrick, M., and Gao, J. (2022). \nNeurocompositional computing: from the central paradox of cognition to a new \ngeneration of AI systems. AI Mag. 43, 308–322. doi: 10.1002/aaai.12065\nStadler, M. A., Roediger, H. L., and McDermott, K. B. (1999). Norms for word lists \nthat create false memories. Mem. Cogn. 27, 494–500. doi: 10.3758/BF03211543\nStevenson, C., Smal, I., Baas, M., Grasman, R., and van der Maas, H. (2022). Putting \nGPT-3's creativity to the (alternative uses) test. arXiv preprint arXiv:2206.08932.\nSuri, G., Slater, L. R., Ziaee, A., and Nguyen, M. (2023). Do LLMs show decision \nheuristics similar to humans? A case study using GPT-3.5. arXiv preprint arXiv:2305.04400.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., et al. (2023). \nLlama: open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nTruelove-Hill, M., Erickson, B. A., Anderson, J., Kossoyan, M., and Kounios, J. (2018). \nA growth-curve analysis of the effects of future-thought priming on insight and \nanalytical problem-solving. Front. Psychol. 9:1311. doi: 10.3389/fpsyg.2018.01311\nUllman, T. (2023). LLMs fail on trivial alterations to theory-of-mind tasks. arXiv \npreprint arXiv:2302.08399.\nValmeekam, K., Olmo, A., Sreedharan, S., and Kambhampati, S. (2022). LLMs still \ncan't plan (a benchmark for LLMs on planning and reasoning about change). arXiv \npreprint arXiv:2206.10498.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. \n(2017). Attention is all you need. Adv. Neural Inf. Proces. Syst. 30, 1–15. doi: 10.48550/\narXiv.1706.03762\nWang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y . (2023). Describe, explain, plan and \nselect: interactive planning with LLMs enables open-world multi-task agents. arXiv \npreprint arXiv:2302.01560.\nWang, Y ., Kordi, Y ., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., et al. (2022). Self-\ninstruct: aligning language model with self generated instructions. arXiv preprint \narXiv:2212.10560.\nWang, Y ., and Zhao, Y . (2023). Metacognitive prompting improves understanding in \nlarge language models. arXiv preprint arXiv:2308.05342.\nWebb, T., Holyoak, K. J., and Lu, H. (2023). Emergent analogical reasoning in large \nlanguage models. Nat. Hum. Behav. 7, 1526–1541. doi: 10.1038/s41562-023-01659-w\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., et al. (2022b). \nEmergent abilities of LLMs. arXiv preprint arXiv:2206.07682.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., et al. (2022a). Chain of \nthought prompting elicits reasoning in LLMs. arXiv preprint arXiv:2201.11903.\nY e, Q., Fu, H. Y ., Ren, X., and Jia, R. (2023). How Predictable Are Large Language \nModel Capabilities? A Case Study on BIG-bench. arXiv preprint arXiv:2305.14947.\nZhang, Z., Zhang, A., Li, M., and Smola, A. (2022). Automatic chain of thought \nprompting in LLMs. arXiv preprint arXiv:2210.03493.\nZhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., et al. (2022). Least-to-most \nprompting enables complex reasoning in large language models. arXiv preprint \narXiv:2205.10625.\nSartori and Orrù 10.3389/fpsyg.2023.1279317\nFrontiers in Psychology 16 frontiersin.org\nGlossary\nLLMs for cognitive psychologists\nLLMs Neural Network based on the transformer and self-attention architecture trained on a vast amount of language data. Usually trained to predict the next \nwork given an input of N words. Popular LLMs include GPT-4, Llama, Chinchilla, Bloom, etc. There are currently competitions among diverse LLMs \ntested one against the other on the same problem. The current best performer is GPT-4 (https://chat.lmsys.org/?arena) with Claude 2 as a close \ncompetitor. The best open sourced LLM is Llama 2.\nSelf-attention Is an associative architecture that may learn to associate distant words in the word stream. Originally presented in the paper by Vaswani et al. (2017), \nself-attention is a procedure that associated cooccurrences irrespective of distance in the word stream. It allows LLMs to weigh the importance of \ndifferent words in a sentence when predicting the next word. It works by computing a weighted sum of the input embeddings, where the weights are \ndetermined by the similarity between each pair of words in the sentence. In other words, self-attention is a way for the model to pay attention to \ndifferent parts of the input sequence to better understand the context and make more accurate predictions.\nWord embeddings Word embeddings in LLMs are numerical representations of words, phrases, or sentences that capture their meaning and context. They are used to \nrepresent text in a way that can be processed by machine learning algorithms as vectors of numbers. Word embeddings are central to the functioning \nof LLMs because they allow the model to encode the meaning of words in a way that is computationally efficient. Representation of conceptual \nknowledge in a semantic space has been at the base of numerous cognitive models (e.g., Latent Semantic Space – LSA and others). In short, a language \nmodel does not “see” a word as input but a vector (numerical representation) encoding the word (embedding).\nTransformers The key innovation of transformer models is the self-attention mechanism, which allows the model to deal with different parts of the input sequence \nwhen computing the representation of each token. This mechanism replaces the recurrent neural network (RNN) used in previous language models, \nwhich had limitations in terms of long-term dependencies. In a transformer model, the input sequence is first embedded into a high-dimensional \nvector space. The self-attention mechanism is then applied to the embedded sequence to compute a weighted sum of the embeddings, where the \nweights are determined by the similarity between each pair of tokens. The resulting weighted sum is then passed through a feedforward neural network \nto produce the final representation of each token.\nHallucinations Is a term used in AI to refer to the phenomenon where the model generates text that is incorrect, nonsensical, or not real and misaligned with the user \nintent. Absence of appropriate context, incomplete description of the user intent, repetition and other strategies may cause LLM to hallucinate. The \nreasons why hallucinations may occur is currently not well understood. Interesting parallelisms may be found between LLL hallucinations and \nconfabulation as a psychopathological symptom.\nScaling Scaling in LLMs refers to the process of increasing the size and capacity of the model to improve its performance and capabilities (Rae et al., 2021). It \ninvolves increasing the number of parameters, layers, and computational resources used by the Language Model. Increasing the dimension of the \nmodel leads to: i) enhanced language understanding, ii) better generalization, iii) handling complex tasks.\nEmergence Emergence refers to the ability of LLMs to generate coherent, contextually relevant, and sometimes creative responses to a wide range of inputs, even \nthough they were not explicitly trained to perform these specific tasks. During training, the model learns to predict the next word in a sentence given \nthe previous words and this process enables the model to capture complex patterns, relationships, and structures present in the training data. As the \nsize of the model and the amount of training data increase, the LLM becomes efficient of generating more accurate responses. This is because the \nmodel has learned a vast amount of information about language, grammar, facts, reasoning, and even some level of common-sense knowledge from \nthe training data. The model can then use this knowledge to generate contextually appropriate responses to a wide range of inputs, even though it was \nnot explicitly trained to perform these tasks. This is a property that permits LLM trained to predict the next word to perform at human level on most \nof the psychological tests as shown in this paper.\nTraining data It is important to note that the training language data for LLMs can vary depending on the specific model and the research objectives. Commonly used \ntraining data for LLMs include large-scale text corpora from the internet, such as books, articles, websites, and other publicly available textual data. \nThe choice of training data can also be domain-specific, such as legal, scientific, or biomedical texts, depending on the intended application of the \nLLM. Recent research has shown that carefully selecting the training data may reduce the dimension of the LLM while maintaining the accuracy.\nLatent space When a user inputs a query that states a problem, the large language model (LLM) processes the query and maps it to a point in its latent space. The \nlatent space is a high dimensional representation of the data that the LLM has been trained on, and it captures the underlying structure and \nrelationships between the data points. Once the query is mapped to a point in the latent space, the LLM can use its understanding of the relationships \nbetween data points in the latent space to generate a response that is relevant to the query. This is done by finding other points in the latent space that \nare close to the query point and are associated with relevant responses. The LLM can then use these points to generate a response that is coherent and \nrelevant to the query. In summary, LLMs use their understanding of the relationships between data points in their latent space to associate a query that \nstates a problem with a relevant response. This allows them to effectively solve problems and generate coherent and relevant responses to user queries.",
  "topic": "Cognition",
  "concepts": [
    {
      "name": "Cognition",
      "score": 0.6706259846687317
    },
    {
      "name": "Psychology",
      "score": 0.6194948554039001
    },
    {
      "name": "Mirroring",
      "score": 0.600836992263794
    },
    {
      "name": "Cognitive science",
      "score": 0.48619645833969116
    },
    {
      "name": "Social cognition",
      "score": 0.44949764013290405
    },
    {
      "name": "Traffic psychology",
      "score": 0.43373504281044006
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4200133681297302
    },
    {
      "name": "Social psychology",
      "score": 0.2560408115386963
    },
    {
      "name": "Poison control",
      "score": 0.22179877758026123
    },
    {
      "name": "Human factors and ergonomics",
      "score": 0.17232564091682434
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Environmental health",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I138689650",
      "name": "University of Padua",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I108290504",
      "name": "University of Pisa",
      "country": "IT"
    }
  ],
  "cited_by": 30
}