{
    "title": "RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder",
    "url": "https://openalex.org/W3097055324",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2127600677",
            "name": "Chi, Cheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4209449461",
            "name": "Wei, Fangyun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117015939",
            "name": "Hu Han",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2570343428",
        "https://openalex.org/W2963381188",
        "https://openalex.org/W3038815964",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2938756873",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W2951548327",
        "https://openalex.org/W2935837427",
        "https://openalex.org/W2793812837",
        "https://openalex.org/W2925359305",
        "https://openalex.org/W3102701618",
        "https://openalex.org/W3042930119",
        "https://openalex.org/W2951649698",
        "https://openalex.org/W2950477723",
        "https://openalex.org/W2966926453",
        "https://openalex.org/W2941472577",
        "https://openalex.org/W3106250896",
        "https://openalex.org/W2996990703",
        "https://openalex.org/W2964093967",
        "https://openalex.org/W3035395201",
        "https://openalex.org/W3012573144",
        "https://openalex.org/W2950141105",
        "https://openalex.org/W2599765304",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W3012618108",
        "https://openalex.org/W3040430115",
        "https://openalex.org/W3035396860",
        "https://openalex.org/W2407521645",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W3035473155",
        "https://openalex.org/W2914868659",
        "https://openalex.org/W2936404177",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2796347433",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2908765305"
    ],
    "abstract": "Existing object detection frameworks are usually built on a single format of object/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet and Faster R-CNN, center points in FCOS and RepPoints, and corner points in CornerNet. While these different representations usually drive the frameworks to perform well in different aspects, e.g., better classification or finer localization, it is in general difficult to combine these representations in a single framework to make good use of each strength, due to the heterogeneous or non-grid feature extraction by different representations. This paper presents an attention-based decoder module similar as that in Transformer~\\cite{vaswani2017attention} to bridge other representations into a typical object detector built on a single representation format, in an end-to-end fashion. The other representations act as a set of \\emph{key} instances to strengthen the main \\emph{query} representation features in the vanilla detectors. Novel techniques are proposed towards efficient computation of the decoder module, including a \\emph{key sampling} approach and a \\emph{shared location embedding} approach. The proposed module is named \\emph{bridging visual representations} (BVR). It can perform in-place and we demonstrate its broad effectiveness in bridging other representations into prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS, where about $1.5\\sim3.0$ AP improvements are achieved. In particular, we improve a state-of-the-art framework with a strong backbone by about $2.0$ AP, reaching $52.7$ AP on COCO test-dev. The resulting network is named RelationNet++. The code will be available at https://github.com/microsoft/RelationNet2.",
    "full_text": "RelationNet++: Bridging Visual Representations for\nObject Detection via Transformer Decoder\nCheng Chi∗\nInstitute of Automation, CAS\nchicheng15@mails.ucas.ac.cn\nFangyun Wei\nMicrosoft Research Asia\nfawe@microsoft.com\nHan Hu\nMicrosoft Research Asia\nhanhu@microsoft.com\nAbstract\nExisting object detection frameworks are usually built on a single format of ob-\nject/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet and\nFaster R-CNN, center points in FCOS and RepPoints, and corner points in Corner-\nNet. While these different representations usually drive the frameworks to perform\nwell in different aspects, e.g., better classiﬁcation or ﬁner localization, it is in gen-\neral difﬁcult to combine these representations in a single framework to make good\nuse of each strength, due to the heterogeneous or non-grid feature extraction by\ndifferent representations. This paper presents an attention-based decoder module\nsimilar as that in Transformer [31] to bridge other representations into a typical\nobject detector built on a single representation format, in an end-to-end fashion.\nThe other representations act as a set of key instances to strengthen the main query\nrepresentation features in the vanilla detectors. Novel techniques are proposed\ntowards efﬁcient computation of the decoder module, including a key sampling\napproach and a shared location embeddingapproach. The proposed module is\nnamed bridging visual representations(BVR). It can perform in-place and we\ndemonstrate its broad effectiveness in bridging other representations into prevalent\nobject detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS,\nwhere about 1.5 ∼3.0 AP improvements are achieved. In particular, we improve a\nstate-of-the-art framework with a strong backbone by about 2.0 AP, reaching 52.7\nAP on COCO test-dev. The resulting network is named RelationNet++. The code\nwill be available at https://github.com/microsoft/RelationNet2.\n1 Introduction\nObject detection is a vital problem in computer vision that many visual applications build on. While\nthere have been numerous approaches towards solving this problem, they usually leverage a single\nvisual representation format. For example, most object detection frameworks [9, 8, 24, 18] utilize\nthe rectangle box to represent object hypotheses in all intermediate stages. Recently, there have also\nbeen some frameworks adopting points to represent an object hypothesis, e.g., center point in Center-\nNet [38] and FCOS [29], point set in RepPoints [35, 36, 3] and PSN [34]. In contrast to representing\nwhole objects, some keypoint-based methods, e.g., CornerNet [15], leverage part representations of\ncorner points to compose an object. In general, different representation methods usually steer the\ndetectors to perform well in different aspects. For example, the bounding box representation is better\naligned with annotation formats for object detection. The center representation avoids the need for an\nanchoring design and is usually friendly to small objects. The corner representation is usually more\naccurate for ﬁner localization.\nIt is natural to raise a question: could we combine these representations into a single framework to\nmake good use of each strength?Noticing that different representations and their feature extractions\n∗The work is done when Cheng Chi is an intern at Microsoft Research Asia.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2010.15831v1  [cs.CV]  29 Oct 2020\nAnchor\nDetection Result\nCenter\nLeft Top\nRight Bottom\nAttention\nAttention\nAttention\nPerson\n(a) Bridge representations.\nAnchor Bounding Box Proposal Bounding Box Object Center Corners (b) Typical object/part representations.\nFigure 1: (a) An illustration of bridging various representations, speciﬁcally leveraging corner/center\nrepresentations to enhance the anchor box features. (b) Object/part representations used in object\ndetection (geometric description and feature extraction). The red dashed box denotes ground-truth.\nare usually heterogeneous, combination is difﬁcult. To address this issue, we present an attention\nbased decoder modulesimilar as that in Transformer [31], which can effectively model dependency\nbetween heterogeneous features. The main representations in an object detector are set as the query\ninput, and other visual representations act as the auxiliary keys to enhance the query features by\ncertain interactions, where both appearance and geometry relationships are considered.\nIn general, all feature map points can act as corner/center key instances, which are usually too\nmany for practical attention computation. In addition, the pairwise geometry term is computation\nand memory consuming. To address these issues, two novel techniques are proposed, including a\nkey samplingapproach and a shared location embeddingapproach for efﬁcient computation of the\ngeometry term. The proposed module is named bridging visual representations(BVR).\nFigure 1a illustrates the application of this module to bridge center and corner representations into an\nanchor-based object detector. The center and corner representations act as key instances to enhance\nthe anchor box features, and the enhanced features are then used for category classiﬁcation and\nbounding box regression to produce the detection results. The module can work in-place. Compared\nwith the original object detector, the main change is that the input features for classiﬁcation and\nregression are replaced by the enhanced features, and thus the strengthened detector largely maintains\nits convenience in use.\nThe proposed BVR module is general. It is applied to various prevalent object detection frame-\nworks, including RetinaNet, Faster R-CNN, FCOS and ATSS. Extensive experiments on the COCO\ndataset [19] show that the BVR module substantially improves these various detectors by 1.5 ∼3.0\nAP. In particular, we improve a strong ATSS detector by about2.0 AP with small overhead, reaching\n52.7 AP on COCO test-dev. The resulting network is named RelationNet++, which strengthens the\nrelation modeling in [12] from bbox-to-bbox to across heterogeneous object/part representations.\nThe main contributions of this work are summarized as:\n• A general module, named BVR, to bridge various heterogeneous visual representations and\ncombine the strengths of each. The proposed module can be applied in-place and does not\nbreak the overall inference process by the main representations.\n• Novel techniques to make the proposed bridging module efﬁcient, including a key sampling\napproach and a shared location embeddingapproach.\n• Broad effectiveness of the proposed module for four prevalent object detectors: RetinaNet,\nFaster R-CNN, FCOS and ATSS.\n2 A Representation View for Object Detection\n2.1 Object / Part Representations\nObject detection aims to ﬁnd all objects in a scene with their location described by rectangle\nbounding boxes. To discriminate object bounding boxes from background and to categorize objects,\nintermediate geometric object/part candidates with associated features are required. We refer to the\njoint geometric descriptionand feature extractionas the representation, where typical representations\nused in object detection are illustrated in Figure 1b and summarized below.\nObject bounding box representation Object detection uses bounding boxes as the ﬁnal output.\nProbably because of this, bounding box is now the most prevalent representation. Geometrically, a\n2\nObject Center Detection\n(c) FCOS\n(a) Faster R-CNN\nAnchor Proposal Detection\n Anchor\n Detection\n(b) RetinaNet\nCorner Points Grouping\n(d) CornerNet\nFigure 2: Representation ﬂows for several typical detection frameworks.\nbounding box can be described by a 4-d vector, either as center-size (xc, yc, w, h) or as opposing\ncorners (xtl, ytl, xbr, ybr). Besides the ﬁnal output, this representation is also commonly used as initial\nand intermediate object representations, such as anchors [24, 20, 22, 23, 18] and proposals [9, 4, 17,\n11]. For bounding box representations, features are usually extracted by pooling operators within\nthe bounding box area on an image feature map. Common pooling operators include RoIPool [8],\nRoIAlign [11], and Deformable RoIPool [5, 40]. There are also simpliﬁed feature extraction methods,\ne.g., the box center features are usually employed in the anchor box representation [24, 18].\nObject center representation The 4-d vector space of a bounding box representation is at a scale\nof O(H2 ×W2) for an image with resolution H ×W, which is too large to fully process. To\nreduce the representation space, some recent frameworks [29, 35, 38, 14, 32] use the center point\nas a simpliﬁed representation. Geometrically, a center point is described by a 2-d vector (xc, yc), in\nwhich the hypothesis space is of the scale O(H ×W), which is much more tractable. For a center\npoint representation, the image feature on the center point is usually employed as the object feature.\nCorner representation A bounding box can be determined by two points, e.g., a top-left corner and\na bottom-right corner. Some approaches [30, 15, 16, 7, 21, 39, 26] ﬁrst detect these individual points\nand then compose bounding boxes from them. We refer to these representation methods as corner\nrepresentation. The image feature at the corner location can be employed as the part feature.\nSummary and comparison Different representation approaches usually have strengths in different\naspects. For example, object based representations (bounding box and center) are better in category\nclassiﬁcation while worse in object localization than part based representations (corners). Object\nbased representations are also more friendly for end-to-end learning because they do not require\na post-processing step to compose objects from corners as in part-based representation methods.\nComparing different object-based representations, while the bounding box representation enables\nmore sophisticated feature extraction and multiple-stage processing, the center representation is\nattractive due to the simpliﬁed system design.\n2.2 Object Detection Frameworks in a Representation View\nObject detection methods can be seen as evolving intermediate object/part representations until the\nﬁnal bounding box outputs. The representation ﬂows largely shape different object detectors. Several\nmajor categorization of object detectors are based on such representation ﬂow, such as top-down\n(object-based representation) vs bottom-up (part-based representation), anchor-based (bounding\nbox based) vs anchor-free (center point based), and single-stage (one-time representation ﬂow) vs\nmultiple-stage (multiple-time representation ﬂow). Figure 2 shows the representation ﬂows of several\ntypical object detection frameworks, as detailed below.\nFaster R-CNN [24] employs bounding boxes as its intermediate object representations in all stages.\nAt the beginning, multiple anchor boxes at each feature map position are hypothesized to coarsely\ncover the 4-d bounding box space in an image, i.e., 3 anchor boxes with different aspect ratios.\nThe image feature vector at the center point is extracted to represent each anchor box, which is\nthen used for foreground/background classiﬁcation and localization reﬁnement. After anchor box\nselection and localization reﬁnement, the object representation is evolved to a set of proposal boxes,\nwhere the object features are usually extracted by an RoIAlign operator within each box area. The\nﬁnal bounding box outputs are obtained by localization reﬁnement, through a small network on the\nproposal features.\nRetinaNet [18] is a one-stage object detector, which also employs bounding boxes as its intermediate\nrepresentation. Due to its one-stage nature, it usually requires denser anchor hypotheses, i.e.,9 anchor\nboxes at each feature map position. The ﬁnal bounding box outputs are also obtained by applying a\nlocalization reﬁnement head network.\n3\nFCOS [29] is also a one-stage object detector but uses object center points as its intermediate object\nrepresentation. It directly regresses the four sides from the center points to form the ﬁnal bounding\nbox outputs. There are concurrent works, such as [38, 14, 35]. Although center points can be seen as\na degenerated geometric representation from bounding boxes, these center point based methods show\ncompetitive or even better performance on benchmarks.\nCornerNet [15] is built on the intermediate part representation of corners, in contrast to the above\nframeworks where object representations are employed. The predicted corners (top-left and bottom-\nright) are grouped according to their embedding similarity, to compose the ﬁnal bounding box outputs.\nThe detectors based on corner representation usually reveal better object localization than those based\non an object-level representation.\n3 Bridging Visual Representations\nFor the typical frameworks in Section 2.2, mainly one kind of representation approach is employed.\nWhile they have strengths in some aspects, they may also fall short in other ways. However, it is in\ngeneral difﬁcult to combine them in a single framework, due to the heterogeneous or non-grid feature\nextraction by different representations. In this section, we will ﬁrst present a general method to bridge\ndifferent representations. Then we demonstrate its applications to various frameworks, including\nRetinaNet [18], Faster R-CNN [24], FCOS [29] and ATSS [37].\n3.1 A General Attention Module to Bridge Visual Representations\nWithout loss of generality, for an object detector, the representation it leverages is referred to as the\nmaster representation, and the general module aims to bridge other representations to enhance this\nmaster representation. Such other representations are referred to as auxiliary ones.\nInspired by the success of the decoder module for neural machine translation where an attention block\nis employed to bridge information between different languages, e.g., Transformer [31], we adapt this\nmechanism to bridge different visual representations. Speciﬁcally, the master representation acts as\nthe query input, and the auxiliary representations act as the key input. The attention module outputs\nstrengthened features for the master representation (queries), which have bridged the information\nfrom auxiliary representations (keys). We use a general attention formulation as:\nf′q\ni = fq\ni +\n∑\nj\nS\n(\nfq\ni , fk\nj , gq\ni , gk\nj\n)\n·Tv(fk\nj ), (1)\nwhere fq\ni , f′q\ni , gq\ni are the input feature, output feature, and geometric vector for a query instance\ni; fk\nj , gk\nj are the input feature and geometric vector for a key instance j; Tv(·) is a linear value\ntransformation function; S(·) is a similarity function between i and j, instantiated as [12]:\nS\n(\nfq\ni , fk\nj , gq\ni , gk\nj\n)\n= softmaxj\n(\nSA(fq\ni , fk\nj ) + SG(gq\ni , gk\nj )\n)\n, (2)\nwhere SA(fq\ni , fk\nj ) denotes the appearance similarity computed by a scaled dot product between query\nand key features [31, 12], and SG(gq\ni , gk\nj ) denotes a geometric term computed by applying a small\nnetwork on the relative locations between i and j, i.e., cosine/sine location embedding [31, 12] plus\na 2-layer MLP. In the case of different dimensionality between thequery geometric vector and key\ngeometric vector (4-d bounding box vs. 2-d point), we ﬁrst extract a 2-d point from the bounding box,\ni.e., center or corner, such that the two representations are homogeneous in geometry description for\nsubtraction operations. The same as in [31, 12], multi-head attention is employed, which performs\nsubstantially better than using single-head attention. We use an attention head number of 8 by default.\nThe above module is named bridging visual representations(BVR), which takes query and key repre-\nsentations of any dimension as input and generates strengthened features for the query considering\nboth their appearance and geometric relationships. The module can be easily plugged into prevalent\ndetectors as described in Section 3.2 and 3.3.\n3.2 BVR for RetinaNet\nWe take RetinaNet as an example to showcase how we apply the BVR module to an existing object\ndetector. As mentioned in Section 2.2, RetinaNet adopts anchor bounding boxes as its master\nrepresentation, where 9 bounding boxes are anchored at each feature map location. Totally, there\nare 9 ×H ×W bounding box instances for a feature map of H ×W resolution. BVR takes the\n4\nBackbone\n+\nFPN\nCls\nHead\nPoint \nHead\nCls\nAttention\nTop-k\nCenter Keys\nCls\nReg\nHead\nCorner\nCenter\nCls\nFeat\nReg\nAttention\nReg\nFeat\nCorner Keys\nReg\nBVR\n(a) Apply BVR to RetinaNet.\nShared Location\nEmbedding\nAppearanceGeometry\nLeft Top\nRight Bottom\nCenter\nSim\nSim\nSim\nAnchor (b) Attention-based feature enhancement.\nFigure 3: Applying BVR into an object detector and an illustration of the attention computation.\nC ×9×H ×W feature map (C is the feature map channel) asquery input, and generates strengthened\nquery features of the same dimension.\nWe use two kinds of key (auxiliary) representations to strengthen the query (master) features. One is\nthe object center, and the other is the corners. As shown in Figure 3a, the center/corner points are\npredicted by applying a small point head network on the output feature map of the backbone. Then\na small set of key points are selected from all predictions, and are fed into the attention modules to\nstrengthen the classiﬁcation and regression feature, respectively. In the following, we provide details\nof these modules and the crucial designs.\nAuxiliary (key) representation learning The point head network consists of two shared 3 ×3 conv\nlayers, followed by two independent sub-networks (a 3 ×3 conv layer and a sigmoid layer) to predict\nthe scores and sub-pixel offsets for center and corner prediction, respectively [15]. The score indicates\nthe probability of a center/corner point locating at the feature map bin. The sub-pixel offset ∆x, ∆y\ndenotes the displacement between its precise location and the top-left (integer coordinate) of each\nfeature bin, which accounts for the resolution loss by down-sampling of feature maps.\nIn learning, for the object detection frameworks with an FPN structure, we assign all ground-truth\ncenter/corner points to all feature levels. We ﬁnd it performs better than the common practice where\nobjects are assigned to a particular level [ 17, 18, 29, 15, 35], probably because it speeds up the\nlearning of center/corner representations due to more positive samples employed in each level. The\nfocal loss [18] and smooth L1 loss are employed for the center/corner score and sub-pixel offset\nlearning, with loss weights of 0.05 and 0.2, respectively.\nKey selection We use corner points to demonstrate the processing of auxiliary representation\nselection since the principle is same for center point representation. We treat each feature map\nposition as an object corner candidate. If all candidates are employed in the key set, the computation\ncost of BVR module is unaffordable. In addition, too many background candidates may suppress\nreal corners. To address these issues, we propose a top-k (k = 50 by default) key selection strategy.\nConcretely, a 3 ×3 MaxPool operator with stride 1 is performed on the corner score map, and the\ntop-k corner candidates are selected according to their corner-ness scores. For an FPN backbone, we\nselect the top-k keys from all pyramidal levels, and the key set is shared by all levels. This shared key\nset outperforms that of independent key set for different levels, as shown in Table 1.\nShared relative location embedding The computation and memory complexities for direct com-\nputation of the geometry term are O(time) = ( d0 + d0d1 + d1G)KHW and O(memory) =\n(2 + d0 + d1 + G)KHW , respectively, where d0, d1, G, Kare the cosine/sine embedding di-\nmension, inner dimension of the MLP network, head number of the multi-head attention module\nand the number of selected key instances, respectively. As shown in Table 3, the default setting\n(d0 = 512, d1 = 512, G= 8, K= 50) is time-consuming and space-consuming.\nNoting that the range of relative locations are limited, i.e., [−H + 1, H−1] ×[−W + 1, W−1],\nwe apply the cosine/sine embedding and the 2-layer MLP network on a ﬁxed 2-d relative location\nmap to produce a G-channel geometric map, and then compute the geometric terms for a key/query\npair by bilinear sampling on this geometric map. To further reduce the computation, we use a\n2-d relative location map with the unit length U larger than 1, e.g., U = 4, where each location\nbin indicates a length of U in the original image. In our implementation, we adopt U = 1\n2 S\n(S indicates the stride of the pyramid level) and a location map of 400 ×400 resolution, which\naccounts for a [−100S, 100S) ×[−100S, 100S) range on the original image for a pyramid level of\nstride S. Figure 3b gives an illustration. The computation and memory complexities are reduced to\n5\nBackbone\n+\nFPN\nCls \nHead\nReg \nHead\nPoint \nHead\nCls \nFeat\nCls \nAttention\nTop-k\nReg \nAttention\nCenter \nCorner\nReg \nFeat\nCenter Keys\nCorner Keys\nCls\nReg\nRoI \nAlign\nRoI\nAlign\nFeat\nBVR\n(a) Apply BVR to faster R-CNN.\n14×14×256\n×4\n14×14×256 28×28×256\n28×28×2\n28×28×2\nHeatmap\nOffset\n(b) Point head for center (corner) pre-\ndiction in faster R-CNN.\nFigure 4: Design of applying BVR to faster R-CNN.\nO(time) = (d0+d0d1+d1G)·4002+GKHW and O(memory) = (2+d0+d1+G)·4002+GKHW ,\nrespectively, which are signiﬁcantly smaller than direct computation, as shown in Table 3.\nSeparate BVR modules for classiﬁcation and regression Object center representations can pro-\nvide rich context for object categorization, while the corner representations can facilitate localization.\nTherefore, we apply separate BVR modules to enhance classiﬁcation and regression features respec-\ntively, as shown in Figure3a. Such separate design is beneﬁcial, as demonstrated in Table 5.\n3.3 BVR for Other Frameworks\nThe BVR module is general, and can be applied to other object detection frameworks.\nATSS [37] applies several techniques from anchor-free detectors to improve the anchor-based detec-\ntors, e.g. RetinaNet. The BVR used for RetinaNet can be directly applied.\nFCOS [29] is an anchor-free detector which utilizes center point as object representation. Since\nthere is no corner information in this representation, we always use the center point location and\nthe corresponding feature to represent the query instance in our BVR module. Other settings are\nmaintained the same as those for RetinaNet.\nFaster R-CNN [24] is a two-stage detector which employs bounding boxes as the inter-mediate\nobject representations in all stages. We adopt BVR to enhance the features of bounding box proposals,\nthe diagram is shown in Figure 4a. In each of the proposals, RoIAlign feature is used to predict\ncenter and corner representations. Figure 4b shows the network structure of point (center/corner)\nhead, which is similar with mask head in mask R-CNN [11]. The selection of keys is same with the\nprocess in RetinaNet, which is stated in Section 3.2. We use the features interpolated from the point\nhead as the key features, center and corner features are also employed to enhance classiﬁcation and\nregression, respectively. Since the number of the querys is much smaller than that in RetinaNet, we\ndirectly compute the geometry term other than using the shared geometric map.\n3.4 Relation to Other Attention Mechanisms in Object Detection\nNon-Local Networks (NL) [33] and RelationNet [12] are two pioneer works utilizing attention\nmodules to enhance detection performance. However, they are both designed to enhance instances of\na single representation format: non-local networks [33] use self-attention to enhance a pixel feature\nby fusing in other pixels’ features; RelationNet [12] enhance a bounding box feature by fusing in\nother bounding box features.\nIn contrast, BVR aims to bridge representations in different forms to combine the strengths of each.\nIn addition to this conceptual difference, there are also new techniques in the modeling aspect. For\nexample, techniques are proposed to enable homogeneous difference/similarity computation between\nheterogeneous representations, i.e., 4-d bounding box vs 2-d corner/center points. Also, there are\nnew techniques proposed to effectively model relationship between different kinds of representations\nas well as to speed-up computation, such as key representation selection, and the shared relative\nlocation embedding approach. The proposed BVR is actually complementary to these pioneer works,\nas shown in Table 7 and 8.\nLearning Region Features (LRF) [10] and DeTr [1] use an attention module to compute the\nfeatures of object proposals [10] or querys [1] from image features. BVR shares similar formulation\nas them, but has a different aim to bridge different forms of object representations.\n6\nTable 1: Ablation on key selection approaches\n#keys share AP AP 50 AP75\n- - 35.6 55.5 39.0\n20 \u0017 36.1 54.9 39.6\n50 \u0017 37.0 55.8 40.6\n20 \u0013 37.7 56.5 41.4\n50 \u0013 38.5 57.0 42.3\n100 \u0013 38.3 56.9 42.0\n200 \u0013 38.2 56.7 41.9\nTable 2: Ablation of sub-pixel corner/centers\nCLS (ct.) REG (cn.) AP AP 50 AP75 AP90\n- - 35.6 55.5 39.0 9.3\ninteger integer 37.0 55.6 40.8 11.0\ninteger sub-pixel 38.0 56.1 41.7 12.5\nsub-pixel integer 37.2 56.7 41.2 10.4\nsub-pixel sub-pixel 38.5 57.0 42.3 12.6\nTable 3: Effect of shared relative location embedding\ngeometry memory FLOPs AP AP 50 AP75\nbaseline 2933M 239G 35.6 55.5 39.0\nappearance only 3345M 264G 37.4 56.7 40.4\nnon-shared 9035M 468G 38.3 57.2 41.7(+5690M) (+204G)\nshared 3479M 266G 38.5 57.0 42.3(+134M) (+2G)\nTable 4: Comparison of different unit length\nand size of the shared location map\nunit length size AP AP 50 AP75\n[2, 4, 8, 16, 32] 400 × 400 38.2 56.7 41.8\n[4, 8, 16, 32, 64] 400 × 400 38.5 57.0 42.3\n[8, 16, 32, 64, 128] 400× 400 38.4 56.8 42.2\n[4, 8, 16, 32, 64] 800 × 800 38.3 56.9 42.1\n[4, 8, 16, 32, 64] 200 × 200 38.1 56.7 41.8\n4 Experiments\nWe ﬁrst ablate each component of the proposed BVR module using a RetinaNet base detector in\nSection 4.1. Then we show beneﬁts of BVR applied to four representative detectors, including\ntwo-stage (i.e., faster R-CNN), one-stage (i.e., RetinaNet and ATSS) and anchor-free (i.e., FCOS)\ndetectors. Finally, we compare our approach with the state-of-the-art methods.\nOur experiments are all implemented on the MMDetection v1.1.0 codebase [ 2]. All experiments\nare performed on MS COCO dataset[ 19]. A union of 80k train images and a 35k subset of val\nimages are used for training. Most ablation experiments are studied on a subset of 5k unused val\nimages (denoted as minival). Unless otherwise stated, all the training and inference details keep the\nsame as the default settings in MMDetection, i.e., initializing the backbone using the ImageNet [25]\npretrained model, resizing the input images to keep their shorter side being 800 and their longer side\nless than or equal to 1333, optimizing the whole network via the SGD algorithm with 0.9 momentum,\n0.0001 weight decay, setting the initial learning rate as 0.02 with the 0.1 decrease at epoch 8 and 11.\nIn the large model experiments in Table 10 and 12, we train 20 epochs and decrease the learning\nrate at epoch 16 and 19. Multi-scale training is also adopted in large model experiments, for each\nmini-batch, the shorter side is randomly selected from a range of [400, 1200].\n4.1 Method Analysis using RetinaNet\nOur ablation study is built on a RetinaNet detector using ResNet-50, which achieves 35.6 AP on\nCOCO minival (1×settings). Components in the BVR module are ablated using this base detector.\nKey selection As shown in Table 1, compared with independent keys across feature levels, sharing\nkeys can bring +1.6 and +1.5 AP gains for 20 and 50 keys, respectively. Using 50 keys achieves\nthe best accuracy, probably because that too few keys cannot sufﬁciently cover the representative\nkeypoints, while too large number of keys include many low-quality candidates.\nOn the whole, the BVR enhanced RetinaNet signiﬁcantly outperforms the original RetinaNet by 2.9\nAP, demonstrating the great beneﬁt of bridging other representations.\nSub-pixel corner/center Table 2 shows the beneﬁts of using sub-pixel representations for centers\nand corners. While sub-pixel representation beneﬁts both classiﬁcation and regression, it is more\ncritical for the localization task.\nShared relative location embedding As shown in Table 3, compared with direct computation of\nposition embedding [12], the proposed shared location embedding approach saves 42×memory cost\n(+134M vs +5690M) and saves 102×FLOPs (+2G vs +204G) in the geometry term computation,\nwhile achieves slightly better performance (38.5 AP vs 38.3 AP).\n7\nTable 5: Effect of different representations (‘ct.’:\ncenter, ‘cn.’: corner) for classiﬁcation and regression\nCLS REG AP AP 50 AP75 AP90\nnone none 35.6 55.5 39.0 9.3\nnone ct. 36.4 54.7 38.9 10.1\nnone cn. 37.5 54.6 40.3 12.2\nct. none 37.3 56.6 39.9 10.5\ncn. none 36.2 55.1 38.4 9.8\nct. cn. 38.5 57.0 42.3 12.6\nTable 6: Ablation of appearance and geometry\nterms\nappearance geometry AP AP 50 AP75 AP90\n\u0017 \u0017 35.6 55.5 39.0 9.3\n\u0013 \u0017 37.4 56.7 41.3 10.7\n\u0017 \u0013 37.6 55.8 41.5 12.0\n\u0013 \u0013 38.5 57.0 42.3 12.6\nTable 7: Compatibility with the non-local module\n(NL) [33]\nmethod AP AP 50 AP75\nRetinaNet 35.6 55.5 39.0\nRetinaNet + NL 37.0 57.0 39.3\nRetinaNet + BVR 38.5 57.0 42.3\nRetinaNet + NL + BVR 39.4 58.2 42.5\nTable 8: Compatibility with the object relation\nmodule (ORM) [12]. ResNet-50-FPN is used\nmethod AP AP 50 AP75\nfaster R-CNN 37.4 58.1 40.4\nfaster R-CNN + ORM 38.4 59.0 41.3\nfaster R-CNN + BVR 39.3 59.5 43.1\nfaster R-CNN + ORM + BVR 40.4 60.6 44.0\nAblation study of the unit length and the size of the shared location map in Table 4 indicates stable\nperformance. We adopt a unit length of [4, 8, 16, 32, 64] and map size of 400 ×400 by default.\nSeparate BVR modules for classiﬁcation and regressionTable 5 ablates the effect of using separate\nBVR modules for classiﬁcation and regression, indicating the center representation is a more suitable\nauxiliary for classiﬁcation and the corner representation is a more suitable auxiliary for regression.\nEffect of appearance and geometry terms Table 6 ablates the effect of appearance and geometry\nterms. Using the two terms together outperforms that using the appearance term alone by 1.1 AP and\noutperforms that using the geometry term alone by 0.9 AP. In general, the geometry term beneﬁts\nmore at larger IoU criteria, and less at lower IoU criteria.\nCompare with multi-task learning Only including an auxiliary point head without using it can\nboost the RetinaNet baseline by 0.8 AP (from 35.6 to 36.4). Noting the BVR brings a 2.9 AP\nimprovement (from 35.6 to 38.5) under the same settings, the major improvements are not due to\nmulti-task learning.\nComplexity analysis Table 9 shows the ﬂops analysis. The input images are resized to 800 ×1333.\nThe proposed BVR module introduces about 3% more parameters (39M vs 38M) and about 10%\nmore computations (266G vs 239G) than the original RetinaNet. We also conduct RetinaNet with\nheavier head network to have similar parameters and computations as our approach. By adding\none more layer, the accuracy slightly drops to 35.2, probably due to the increasing difﬁculty in\noptimization. We introduce a GN layer after every head conv layer to alleviate it, and one additional\nconv layer improves the accuracy by 0.3 AP. These results indicate that the improvements by BVR\nare mostly not due to more parameters and computation.\nThe real inference speed of different models using a V100 GPU (fp32 mode is used) are shown in\nTable 11. By using a ResNet-50 backbone, the BVR module usually takes less than10% overhead. By\nusing a larger ResNeXt-101-DCN backbone, the BVR module usually takes less than 3% overhead.\nTable 9: Complexity analysis\nmethod #conv #ch. FLOP param AP\nRetinaNet 4 256 239G 38M 35.6\nRetinaNet (deep) 5 256 265G 39M 35.2\nRetinaNet (wide) 4 288 267G 39M 35.6\nRetinaNet+BVR 4 256 266G 39M 38.5\nRetinaNet+GN 4 256 239G 38M 36.5\nRetinaNet (deep)+GN 5 256 265G 39M 36.8\nRetinaNet (wide)+GN 4 288 267G 39M 36.5\nRetinaNet+GN+BVR 4 256 266G 39M 39.2\nTable 10: BVR for four representative detectors\nusing a ResNeXt-64x4d-101-DCN backbone\nmethod AP AP 50 AP75\nRetinaNet 42.9 63.4 46.9\nRetinaNet + BVR 44.7 (+1.8) 64.9 49.0\nfaster R-CNN 45.0 66.2 48.8\nfaster R-CNN + BVR 46.5 (+1.5) 67.4 50.5\nFCOS 46.1 65.0 49.6\nFCOS + BVR 47.6 (+1.5) 66.2 51.4\nATSS 48.3 67.1 52.6\nATSS + BVR 50.3 (+2.0) 69.0 55.0\n8\nTable 11: Time cost of the BVR module.\nmethod backbone FPS FPS (+BVR)\nFaster R-CNN ResNet-50/ResNeXt-101-DCN 21.3/7.5 19.5/7.3\nRetinaNet ResNet-50/ResNeXt-101-DCN 18.9/7.0 17.4/6.8\nFCOS ResNet-50/ResNeXt-101-DCN 22.7/7.4 20.7/7.2\nATSS ResNet-50/ResNeXt-101-DCN 19.6/7.1 17.9/6.9\nTable 12: Results on MS COCO test-dev set, ‘∗’ denotes the multi-scale testing\nmethod backbone AP AP 50 AP75 APS APM APL\nDCN v2* [40] ResNet-101-DCN 46.0 67.9 50.8 27.8 49.1 59.5\nSNIPER* [27] ResNet-101 46.5 67.5 52.2 30.0 49.4 58.4\nRepPoints* [35] ResNet-101-DCN 46.5 67.4 50.9 30.3 49.7 57.1\nMAL* [13] ResNeXt-101 47.0 66.1 51.2 30.2 50.1 58.9\nCentripetalNet* [6] Hourglass-104 48.0 65.1 51.8 29.0 50.4 59.9\nATSS* [37] ResNeXt-64x4d-101-DCN 50.7 68.9 56.3 33.2 52.9 62.4\nTSD* [28] SENet154-DCN 51.2 71.9 56.0 33.8 54.8 64.2\nRelationNet++ (our) ResNeXt-64x4d-101-DCN 50.3 69.0 55.0 32.8 55.0 65.8\nRelationNet++ (our)* ResNeXt-64x4d-101-DCN 52.7 70.4 58.3 35.8 55.3 64.7\n4.2 BVR is Complementary to Other Attention Mechanisms\nThe BVR module acts differently compared to the pioneer works of the non-local module [33] and\nthe relation module [12] which also model dependencies between representations. While the BVR\nmodule models relationships between different kinds of representations, the latter modules model\nrelationships within the same kinds of representations (pixels [ 33] and proposal boxes [ 12]). To\ncompare with the object relation module (ORM) [ 12], we ﬁrst apply BVR to enhance RoIAlign\nfeatures with corner/center representations, the process of which is same as Figure 4a. Then the\nenhanced features are utilized to perform object relation between proposals. Different from [12], keys\nare sampled to make the module more efﬁcient. Table 8 shows that the BVR module and the relation\nmodule are mostly complementary. On the basis of faster R-CNN baseline, ORM can obtain +1.0\nAP improvement, while our BVR improves AP by 1.9. Applying our BVR on the basis of the ORM\ncontinually improves AP by 2.0. Table 7 and 8 show that the BVR modules is mostly complementary\nwith non-local and object relation module.\n4.3 Generally Applicable to Representative Detectors\nWe apply the proposed BVR to four representative frameworks, i.e., RetinaNet [ 18], Faster R-\nCNN [24, 17], FCOS [29] and ATSS [37], as shown in Table 10. The ResNeXt-64x4d-101-DCN\nbackbone, multi-scale and longer training (20 epochs) are adopted to test whether our approach\neffects on strong baselines. The BVR module improve these strong detectors by 1.5 ∼2.0 AP.\n4.4 Comparison with State-of-the-Arts\nWe build our detector by applying the BVR module on a strong detector of ATSS, which achieves\n50.7 AP on COCO test-dev using multi-scale testing based on the ResNeXt-64x4d-101-DCN\nbackbone. Our approach improves it by 2.0 AP, reaching 52.7 AP. Table 12 shows the comparison\nwith state-of-the-arts methods.\n5 Conclusion\nIn this paper, we present a new module, BVR, which bridge various other visual representations by an\nattention mechanism like that in Transformer [31] to enhance the main representations in a detector.\nThe BVR module can be applied plug-in for an existing detector, and proves broad effectiveness for\nprevalent object detection frameworks, i.e. RetinaNet, faster R-CNN, FCOS and ATSS, where about\n1.5 ∼3.0 AP improvements are achieved. We reach 52.7 AP on COCO test-dev by improving a\nstrong ATSS detector. The resulting network is named RelationNet++, which advances the relation\nmodeling in [12] from bbox-to-bbox to across heterogeneous object/part representations.\n9\nBroader Impact\nThis work aims for better object detection algorithms. Any object oriented visual applications may\nbeneﬁt from this work, as object detection is usually an indispensable component for them. There\nmay be unpredictable failures, similar as most other detectors. The consequences of failures by this\nalgorithm are determined on the down-stream applications, and please do not use it for scenarios\nwhere failures will lead to serious consequences. The method is data driven, and the performance\nmay be affected by the biases in the data. So please also be careful about the data collection process\nwhen using it.\nReferences\n[1] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020).\nEnd-to-end object detection with transformers. In ECCV.\n[2] Chen, K., Wang, J., Pang, J., Cao, Y ., Xiong, Y ., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., Zhang,\nZ., Cheng, D., Zhu, C., Cheng, T., Zhao, Q., Li, B., Lu, X., Zhu, R., Wu, Y ., Dai, J., Wang, J., Shi,\nJ., Ouyang, W., Loy, C. C., and Lin, D. (2019). MMDetection: Open mmlab detection toolbox\nand benchmark. arXiv:1906.07155.\n[3] Chen, Y ., Zhang, Z., Cao, Y ., Wang, L., Lin, S., and Hu, H. (2020). Reppoints v2: Veriﬁcation\nmeets regression for object detection. arXiv:2007.08508.\n[4] Dai, J., Li, Y ., He, K., and Sun, J. (2016). R-fcn: Object detection via region-based fully\nconvolutional networks. In NeurIPS.\n[5] Dai, J., Qi, H., Xiong, Y ., Li, Y ., Zhang, G., Hu, H., and Wei, Y . (2017). Deformable convolutional\nnetworks. In ICCV.\n[6] Dong, Z., Li, G., Liao, Y ., Wang, F., Ren, P., and Qian, C. (2020). Centripetalnet: Pursuing\nhigh-quality keypoint pairs for object detection. In CVPR.\n[7] Duan, K., Bai, S., Xie, L., Qi, H., Huang, Q., and Tian, Q. (2019). Centernet: Object detection\nwith keypoint triplets. In ICCV.\n[8] Girshick, R. (2015). Fast r-cnn. In ICCV.\n[9] Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). Rich feature hierarchies for accurate\nobject detection and semantic segmentation. In CVPR.\n[10] Gu, J., Hu, H., Wang, L., Wei, Y ., and Dai, J. (2018). Learning region features for object\ndetection. In ECCV.\n[11] He, K., Gkioxari, G., Dollár, P., and Girshick, R. (2017). Mask r-cnn. In ICCV.\n[12] Hu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y . (2018). Relation networks for object detection.\nIn CVPR.\n[13] Ke, W., Zhang, T., Huang, Z., Ye, Q., Liu, J., and Huang, D. (2020). Multiple anchor learning\nfor visual object detection. In CVPR.\n[14] Kong, T., Sun, F., Liu, H., Jiang, Y ., and Shi, J. (2019). Foveabox: Beyond anchor-based object\ndetector. arXiv:1904.03797.\n[15] Law, H. and Deng, J. (2018). Cornernet: Detecting objects as paired keypoints. In ECCV.\n[16] Law, H., Teng, Y ., Russakovsky, O., and Deng, J. (2019). Cornernet-lite: Efﬁcient keypoint\nbased object detection. arXiv:1904.08900.\n[17] Lin, T.-Y ., Dollár, P., Girshick, R., He, K., Hariharan, B., and Belongie, S. (2017a). Feature\npyramid networks for object detection. In ICCV.\n[18] Lin, T.-Y ., Goyal, P., Girshick, R., He, K., and Dollár, P. (2017b). Focal loss for dense object\ndetection. In ICCV.\n10\n[19] Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick,\nC. L. (2014). Microsoft coco: Common objects in context. In ECCV.\n[20] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y ., and Berg, A. C. (2016). Ssd:\nSingle shot multibox detector. In ECCV.\n[21] Lu, X., Li, B., Yue, Y ., Li, Q., and Yan, J. (2019). Grid R-CNN. In CVPR.\n[22] Redmon, J. and Farhadi, A. (2017). Yolo9000: better, faster, stronger. In CVPR.\n[23] Redmon, J. and Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv:1804.02767.\n[24] Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster r-cnn: Towards real-time object\ndetection with region proposal networks. In NeurIPS.\n[25] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A.,\nKhosla, A., Bernstein, M. S., Berg, A. C., and Li, F. (2015). Imagenet large scale visual recognition\nchallenge. IJCV.\n[26] Samet, N., Hicsonmez, S., and Akbas, E. (2020). Houghnet: Integrating near and long-range\nevidence for bottom-up object detection. In ECCV.\n[27] Singh, B., Najibi, M., and Davis, L. S. (2018). Sniper: Efﬁcient multi-scale training. In\nNeurIPS.\n[28] Song, G., Liu, Y ., and Wang, X. (2020). Revisiting the sibling head in object detector. InCVPR.\n[29] Tian, Z., Shen, C., Chen, H., and He, T. (2019). FCOS: fully convolutional one-stage object\ndetection. In ICCV.\n[30] Tychsen-Smith, L. and Petersson, L. (2017). Denet: Scalable real-time object detection with\ndirected sparse sampling. In ICCV.\n[31] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and\nPolosukhin, I. (2017). Attention is all you need. In NeurIPS.\n[32] Wang, J., Chen, K., Yang, S., Loy, C. C., and Lin, D. (2019). Region proposal by guided\nanchoring. In CVPR.\n[33] Wang, X., Girshick, R. B., Gupta, A., and He, K. (2018). Non-local neural networks. In CVPR.\n[34] Wei, F., Sun, X., Li, H., Wang, J., and Lin, S. (2020). Point-set anchors for object detection,\ninstance segmentation and pose estimation. In ECCV.\n[35] Yang, Z., Liu, S., Hu, H., Wang, L., and Lin, S. (2019). Reppoints: Point set representation for\nobject detection. In ICCV.\n[36] Yang, Z., Xu, Y ., Xue, H., Zhang, Z., Urtasun, R., Wang, L., Lin, S., and Hu, H. (2020). Dense\nreppoints: Representing visual objects with dense point sets. In ECCV.\n[37] Zhang, S., Chi, C., Yao, Y ., Lei, Z., and Li, S. Z. (2020). Bridging the gap between anchor-based\nand anchor-free detection via adaptive training sample selection. In CVPR.\n[38] Zhou, X., Wang, D., and Krähenbühl, P. (2019a). Objects as points. arXiv:1904.07850.\n[39] Zhou, X., Zhuo, J., and Krähenbühl, P. (2019b). Bottom-up object detection by grouping\nextreme and center points. In CVPR.\n[40] Zhu, X., Hu, H., Lin, S., and Dai, J. (2019). Deformable convnets v2: More deformable, better\nresults. In CVPR.\n11"
}