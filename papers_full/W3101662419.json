{
  "title": "The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",
  "url": "https://openalex.org/W3101662419",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2888964716",
      "name": "Ian Tenney",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2751901600",
      "name": "James Wexler",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2250490688",
      "name": "Jasmijn Bastings",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2174406895",
      "name": "Tolga Bölükbaşı",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2948422585",
      "name": "Andy Coenen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2580156231",
      "name": "Sebastian Gehrmann",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3048618226",
      "name": "Ellen Jiang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2593197224",
      "name": "Mahima Pushkarna",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2958965362",
      "name": "Carey Radebaugh",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2011782780",
      "name": "Emily Reif",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2111895186",
      "name": "Ann Yuan",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3038035611",
    "https://openalex.org/W2145065594",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2553981914",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2793978524",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2970863760",
    "https://openalex.org/W4245255589",
    "https://openalex.org/W2963123635",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2956281901",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W2088911157",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W2886614482",
    "https://openalex.org/W2973631113",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2949858875",
    "https://openalex.org/W2969670093",
    "https://openalex.org/W2038276547",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2945295328",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3006437051",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W113837456",
    "https://openalex.org/W3037116584",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2752194699",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2143927888"
  ],
  "abstract": "Ian Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, Ann Yuan. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.",
  "full_text": "Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 107–118\nNovember 16-20, 2020.c⃝2020 Association for Computational Linguistics\n107\nThe Language Interpretability Tool:\nExtensible, Interactive Visualizations and Analysis for NLP Models\nIan Tenney,∗ James Wexler,∗ Jasmijn Bastings, Tolga Bolukbasi,\nAndy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna,\nCarey Radebaugh, Emily Reif, Ann Yuan\nGoogle Research\n{iftenney,jwexler}@google.com\nAbstract\nWe present the Language Interpretability Tool\n(LIT), an open-source platform for visualiza-\ntion and understanding of NLP models. We\nfocus on core questions about model behav-\nior: Why did my model make this predic-\ntion? When does it perform poorly? What\nhappens under a controlled change in the in-\nput? LIT integrates local explanations, ag-\ngregate analysis, and counterfactual genera-\ntion into a streamlined, browser-based inter-\nface to enable rapid exploration and error anal-\nysis. We include case studies for a diverse set\nof workﬂows, including exploring counterfac-\ntuals for sentiment analysis, measuring gen-\nder bias in coreference systems, and explor-\ning local behavior in text generation. LIT sup-\nports a wide range of models—including clas-\nsiﬁcation, seq2seq, and structured prediction—\nand is highly extensible through a declara-\ntive, framework-agnostic API. LIT is under ac-\ntive development, with code and full documen-\ntation available at https://github.com/\npair-code/lit.1\n1 Introduction\nAdvances in modeling have brought unprecedented\nperformance on many NLP tasks (e.g. Wang et al.,\n2019), but many questions remain about the be-\nhavior of these models under domain shift (Blitzer\nand Pereira, 2007) and adversarial settings (Jia and\nLiang, 2017), and for their tendencies to behave\naccording to social biases (Bolukbasi et al., 2016;\nCaliskan et al., 2017) or shallow heuristics (e.g.\nMcCoy et al., 2019; Poliak et al., 2018). For any\nnew model, one might want to know: What kind\nof examples does my model perform poorly on?\nWhy did my model make this prediction? And\ncritically, does my model behave consistently if\n∗ Equal contribution.\n1A video walkthrough is available at https://youtu.\nbe/j0OfBWFUqIE.\nI change things like textual style, verb tense, or\npronoun gender? Despite the recent explosion of\nwork on model understanding and evaluation (e.g.\nBelinkov et al., 2020; Linzen et al., 2019; Ribeiro\net al., 2020), there is no “silver bullet” for analy-\nsis. Practitioners must often experiment with many\ntechniques, looking at local explanations, aggregate\nmetrics, and counterfactual variations of the input\nto build a full understanding of model behavior.\nExisting tools can assist with this process, but\nmany come with limitations: ofﬂine tools such as\nTFMA (Mewald, 2019) can provide only aggre-\ngate metrics, interactive frontends (e.g. Wallace\net al., 2019) may focus on single-datapoint expla-\nnation, and more integrated tools (e.g. Wexler et al.,\n2020; Mothilal et al., 2020; Strobelt et al., 2018)\noften work with only a narrow range of models.\nSwitching between tools or adapting a new method\nfrom research code can take days of work, distract-\ning from the real task of error analysis. An ideal\nworkﬂow would be seamless and interactive: users\nshould see the data, what the model does with it,\nand why, so they can quickly test hypotheses and\nbuild understanding.\nWith this in mind, we introduce the Language\nInterpretability Tool (LIT), a toolkit and browser-\nbased user interface (UI) for NLP model un-\nderstanding. LIT supports local explanations—\nincluding salience maps, attention, and rich vi-\nsualizations of model predictions—as well as ag-\ngregate analysis—including metrics, embedding\nspaces, and ﬂexible slicing—and allows users to\nseamlessly hop between them to test local hypothe-\nses and validate them over a dataset. LIT provides\nﬁrst-class support for counterfactual generation:\nnew datapoints can be added on the ﬂy, and their\neffect on the model visualized immediately. Side-\nby-side comparison allows for two models, or two\ndatapoints, to be visualized simultaneously.\nWe recognize that research workﬂows are con-\n108\nFigure 1: The LIT UI, showing a ﬁne-tuned BERT (Devlin et al., 2019) model on the Stanford Sentiment Treebank\n(Socher et al., 2013) development set. The top half shows a selection toolbar, and, left-to-right: the embedding\nprojector, the data table, and the datapoint editor. Tabs present different modules in the bottom half; the view above\nshows classiﬁer predictions, an attention visualization, and a confusion matrix.\nstantly evolving, and designed LIT along the fol-\nlowing principles:\n• Flexible: Support a wide range of NLP tasks,\nincluding classiﬁcation, seq2seq, language mod-\neling, and structured prediction.\n• Extensible: Designed for experimentation, and\ncan be reconﬁgured and extended for novel work-\nﬂows.\n• Modular: Components are self-contained,\nportable, and simple to implement.\n• Framework agnostic: Works with any model\nthat can run from Python —including Tensor-\nFlow (Abadi et al., 2015), PyTorch (Paszke et al.,\n2019), or remote models on a server.\n• Easy to use:Low barrier to entry, with only a\nsmall amount of code needed to add models and\ndata (Section 4.3), and an easy path to access\nsophisticated functionality.\n2 User Interface and Functionality\nLIT has a browser-based UI comprised of modules\n(Figure 1) which contain controls and visualiza-\ntions for speciﬁc tasks (Table 1). At the most basic\nlevel, LIT works as a simple demo server: one can\nenter text, press a button, and see the model’s pre-\ndictions. But by loading an evaluation set, allowing\ndynamic datapoint generation, and an array of in-\nteractive visualizations, metrics, and modules that\nrespond to user input, LIT supports a much richer\nset of user journeys:\nJ1 - Explore the dataset.Users can interactively\nexplore datasets using different criteria across mod-\nules like the data table and the embeddings module\n(similar to Smilkov et al. (2016)), in which a PCA\nor UMAP (McInnes et al., 2018) projection can be\nrotated, zoomed, and panned to explore clusters\nand global structures (Figure 1-top left).\nJ2 - Find interesting datapoints. Users can\nidentify interesting datapoints for analysis, cycle\nthrough them, and save selections for future use.\nFor example, users can select off-diagonal groups\nfrom a confusion matrix, examine outlying clusters\nin embedding space, or select a range based on\nscalar values (Figure 4 (a)).\nJ3 - Explain local behavior. Users can deep-\ndive into model behavior on selected individual\ndatapoints using a variety of modules depending\non the model task and type. For instance, users\ncan compare explanations from salience maps, in-\ncluding local gradients (Li et al., 2016) and LIME\n(Ribeiro et al., 2016), or look for patterns in atten-\ntion heads (Figure 1-bottom).\n109\nModule Description\nAttention Displays an attention visualization for each layer and head.\nConfusion Matrix A customizable confusion matrix for single model or multi-model comparison.\nCounterfactual Generator Creates counterfactuals for selected datapoint(s) using a variety of techniques.\nData Table A tabular view of the data, with sorting, searching, and ﬁltering support.\nDatapoint Editor Editable details of a selected datapoint.\nEmbeddings Visualizes dataset by layer-wise embeddings, projected down to 3 dimensions.\nMetrics Table Displays metrics such as accuracy or BLEU score, on the whole dataset and slices.\nPredictions Displays model predictions, including classiﬁcation, text generation, language model\nprobabilities, and a graph visualization for structured prediction tasks.\nSalience Maps Shows heatmaps for token-based feature attribution for a selected datapoint using tech-\nniques like local gradients and LIME.\nScalar Plot Displays a jitter plot organizing datapoints by model output scores, metrics or other\nscalar values.\nTable 1: Built-in modules in the Language Interpretability Tool.\nJ4 - Generate new datapoints.Users can create\nnew datapoints based on datapoints of interest ei-\nther manually through edits, or with a range of auto-\nmatic counterfactual generators, such as backtrans-\nlation (Bannard and Callison-Burch, 2005), nearest-\nneighbor retrieval (Andoni and Indyk, 2006), word\nsubstitutions (“great → terrible”), or adversarial\nattacks like HotFlip (Ebrahimi et al., 2018) (Fig-\nure A.1). Datapoint provenance is tracked to facili-\ntate easy comparison.\nJ5 - Compare side-by-side. Users can interac-\ntively compare two or more models on the same\ndata, or a single model on two datapoints simul-\ntaneously. Visualizations automatically “replicate”\nfor a side-by-side view.\nJ6 - Compute metrics. LIT calculates and dis-\nplays metrics for the whole dataset, the current\nselection, as well as on manual or automatically-\ngenerated slices (Figure 3 (c)) to easily ﬁnd patterns\nin model performance.\nLIT’s interface allows these user journeys to\nbe explored interactively. Selecting a dataset and\nmodel(s) will automatically show compatible mod-\nules in a multi-pane layout (Figure 1). A tabbed\nbottom panel groups modules by workﬂow and\nfunctionality, while the top panel shows persistent\nmodules for dataset exploration.\nThese modules respond dynamically to user in-\nteractions. If a selection is made in the embedding\nprojector, for example, the metrics table will re-\nspond automatically and compute scores on the se-\nlected datapoints. Global controls make it easy to\npage through examples, enter a comparison mode,\nor save the selection as a named “slice”. In this way,\nthe user can quickly explore multiple workﬂows\nusing different combinations of modules.\nA brief video demonstration of the LIT UI is\navailable at https://youtu.be/j0OfBWFUqIE.\n3 Case Studies\nSentiment analysis. How well does a sentiment\nclassiﬁer handle negation? We load the develop-\nment set of the Stanford Sentiment Treebank (SST;\nSocher et al., 2013), and use the search function\nin LIT’s data table (J1, J2) to ﬁnd the 56 data-\npoints containing the word “not”. Looking at the\nMetrics Table (J6), we ﬁnd that surprisingly, our\nBERT model (Devlin et al., 2019) gets 100% of\nthese correct! But we might want to know if this\nis truly robust. With LIT, we can select individ-\nual datapoints and look for explanations (J3). For\nexample, take the negative review, “It’snot the ulti-\nmate depression-era gangster movie.”. As shown\nin Figure 2, salience maps suggest that “not” and\n“ultimate” are important to the prediction.\nWe can verify this by creating modiﬁed inputs,\nusing LIT’s datapoint editor(J4). Removing “not”\ngets a strongly positive prediction from “ It’s the\nultimate depression-era gangster movie. ”, while\nreplacing “ultimate” to get “ It’s not the worst\ndepression-era gangster movie.” elicits a mildly\npositive score from our model.\nGender bias in coreference.Does a system en-\ncode gendered associations, which might lead to\nincorrect predictions? We load a coreference model\n110\nFigure 2: Salience maps on “ It’s not the ultimate\ndepression-era gangster movie.”, suggesting that “not”\nand “ultimate” are important to the model’s prediction.\nFigure 3: Exploring a coreference model on the Wino-\ngender dataset.\ntrained on OntoNotes (Hovy et al., 2006), and load\nthe Winogender (Rudinger et al., 2018) dataset into\nLIT for evaluation. Each Winogender example has\na pronoun and two candidate referents, one a occu-\npation term like (“technician”) and one an “other\nparticipant” (like “customer”). Our model predicts\ncoreference probabilities for each candidate. We\ncan explore the model’s sensitivity to pronouns by\ncomparing two examples side-by-side (see Figure 3\n(a).) We can see how commonly the model makes\nsimilar errors by paging through the dataset (J1), or\nby selecting speciﬁc slices of interest. For example,\nwe can use the scalar plot module ( J2) (Figure 3\n(b)) to select datapoints where the occupation term\nis associated with a high proportion of male or\nfemale workers, according to the U.S. Bureau of\nFigure 4: Investigating a local generation error, from\nselection of an interesting example to ﬁnding relevant\ntraining datapoints that led to an error.\nLabor Statistics (BLS; Caliskan et al., 2017).\nIn the Metrics Table (J6), we can slice this se-\nlection by pronoun type and by the true referent.\nOn the set of male-dominated occupations (< 25%\nfemale by BLS), we see the model performs well\nwhen the ground-truth agrees with the stereotype -\ne.g. when the answer is the occupation term, male\npronouns are correctly resolved 83% of the time,\ncompared to female pronouns only 37.5% of the\ntime (Figure 3 (c)).\nDebugging text generation. Does the training\ndata explain a particular error in text generation?\nWe analyze a T5 (Raffel et al., 2019) model on\nthe CNN-DM summarization task (Hermann et al.,\n2015), and loosely follow the steps of Strobelt et al.\n(2018). LIT’s scalar plot module(J2) allows us to\nlook at per-example ROUGE scores, and quickly\nselect an example with middling performance (Fig-\nure 4 (a)). We ﬁnd the generated text (Figure 4\n(b)) contains an erroneous constituent: “ alastair\ncook was replaced as captain by former captain\n...”. We can dig deeper, using LIT’s language mod-\neling module (Figure 4 (c)) to see that the token\n“by” is predicted with high probability (28.7%).\nTo ﬁnd out how T5 arrived at this prediction, we\nutilize the “similarity searcher” component through\nthe counterfactual generator tab (Figure 4 (d)).\nThis performs a fast approximate nearest-neighbor\nlookup (Andoni and Indyk, 2006) from a pre-built\n111\nindex over the training corpus, using embeddings\nfrom the T5 decoder. With one click, we can re-\ntrieve 25 nearest neighbors and add them to the LIT\nUI for inspection (as in Figure A.1). We see that\nthe words “captain” and “former” appear 34 and 16\ntimes in these examples–along with 3 occurrences\nof “replaced by” (Figure 4 (e))–suggesting a strong\nprior toward our erroneous phrase.\n4 System design and components\nThe LIT UI is written in TypeScript, and commu-\nnicates with a Python backend that hosts models,\ndatasets, counterfactual generators, and other inter-\npretation components. LIT is agnostic to model-\ning frameworks; data is exchanged using NumPy\narrays and JSON, and components are integrated\nthrough a declarative “spec” system (Section 4.4)\nthat minimizes cross-dependencies and encourages\nmodularity. A more detailed design schematic is\ngiven in the Appendix, Figure A.2.\n4.1 Frontend\nThe browser-based UI is a single-page web app,\nbuilt with lit-element2 and MobX3. A shared frame-\nwork of “service” objects tracks interaction state,\nsuch as the active model, dataset, and selection, and\ncoordinates a set of otherwise-independent mod-\nules which provide controls and visualizations.\n4.2 Backend\nThe Python backend serves models, data, and in-\nterpretation components. The server is stateless,\nbut includes a caching layer for model predictions,\nwhich frees components from needing to store inter-\nmediate results and allows interactive use of large\nmodels like BERT (Devlin et al., 2019) and GPT-2\n(Radford et al., 2019). Component types include:\n• Models which implement a predict()func-\ntion, input spec(), and output spec().\n• Datasets which load data from any source and\nexpose an .examplesﬁeld and a spec().\n• Interpreters are called on a model and a set of\ndatapoints, and return output—such as a salience\nmap—that may also depend on the model’s pre-\ndictions.\n• Generators are interpreters that return new input\ndatapoints from source datapoints.\n2https://lit-element.polymer-project.\norg/. Naming is coincidental; the Language Interpretability\nTool is not related to the lit-html or lit-element projects.\n3https://mobx.js.org/\n• Metrics are interpreters which return aggregate\nscores for a list of inputs.\nThese components are designed to be self-\ncontained and interact through minimalist APIs,\nwith most exposing only one or two methods plus\nspec information. They communicate through stan-\ndard Python and NumPy types, making LIT com-\npatible with most common modeling frameworks,\nincluding TensorFlow (Abadi et al., 2015) and Py-\nTorch (Paszke et al., 2019). Components are also\nportable, and can easily be used in a notebook or\nstandalone script. For example:\ndataset = SSTData(...)\nmodel = SentimentModel(...)\nlime = lime_explainer.LIME()\nlime.run([dataset.examples[0]],\nmodel, dataset)\nwill run the LIME (Ribeiro et al., 2016) component\nand return a list of tokens and their importance to\nthe model prediction.\n4.3 Running with your own model\nLIT is built as a Python library, and its typical use is\nto create a short demo.pyscript that loads models\nand data and passes them to the lit.Server\nclass:\nmodels = {'foo': FooModel(...),\n'bar': BarModel(...)}\ndatasets = {'baz': BazDataset(...)}\nserver = lit.Server(models, datasets)\nserver.serve()\nA full example script is included in the Appendix\n(Figure A.3). The same server can host several\nmodels and datasets for side-by-side comparison,\nand can also interact with remotely-hosted models.\n4.4 Extensibility: the spec() system\nNLP models come in many shapes, with inputs\nthat may involve multiple text segments, additional\ncategorical features, scalars, and more, and output\nmodalities that include classiﬁcation, regression,\ntext generation, and span labeling. Models may\nhave multiple heads of different types, and may\nalso return additional values like gradients, embed-\ndings, or attention maps. Rather than enumerate all\nvariations, LIT describes each model and dataset\nwith an extensible system of semantic types.\nFor example, a dataset class for textual entail-\nment (Dagan et al., 2006; Bowman et al., 2015)\nmight have spec(), describing available ﬁelds:\n112\n• premise: TextSegment()\n• hypothesis: TextSegment()\n• label: MulticlassLabel(vocab=...)\nA model for the same task would have an\ninput spec()to describe required inputs:\n• premise: TextSegment()\n• hypothesis: TextSegment()\nAs well as an output spec() to describe its\npredictions:\n• probas: MulticlassPreds(\nvocab=..., parent=\"label\")\nOther LIT components can read this spec, and\ninfer how to operate on the data. For example, the\nMulticlassMetricscomponent searches for\nMulticlassPredsﬁelds (which contain prob-\nabilities), uses the vocabannotation to decode to\nstring labels, and evaluates these against the input\nﬁeld described by parent. Frontend modules can\ndetect these ﬁelds, and automatically display: for\nexample, the embedding projector will appear if\nEmbeddingsare available.\nNew types can be easily deﬁned: a\nSpanLabels class might represent the out-\nput of a named entity recognition model, and\ncustom components can be added to interpret it.\n5 Related Work\nA number of tools exist for interactive analysis of\ntrained ML models. Many are general-purpose,\nsuch as the What-If Tool (Wexler et al., 2020), Cap-\ntum (Kokhlikyan et al., 2019), Manifold (Zhang\net al., 2018), or InterpretML (Nori et al., 2019),\nwhile others focus on speciﬁc applications like fair-\nness, including FairVis (Cabrera et al., 2019) and\nFairSight (Ahn and Lin, 2019). And some pro-\nvide rich support for counterfactual analysis, either\nwithin-dataset (What-If Tool) or dynamically gen-\nerated as in DiCE (Mothilal et al., 2020).\nFor NLP, a number of tools exist for speciﬁc\nmodel classes, such as RNNs (Strobelt et al., 2017),\nTransformers (Hoover et al., 2020; Vig and Be-\nlinkov, 2019), or text generation (Strobelt et al.,\n2018). More generally, AllenNLP Interpret (Wal-\nlace et al., 2019) introduces a modular framework\nfor interpretability components, focused on single-\ndatapoint explanations and integrated tightly with\nthe AllenNLP (Gardner et al., 2017) framework.\nWhile many components exist in other tools,\nLIT aims to integrate local explanations, aggre-\ngate analysis, and counterfactual generation into a\nsingle tool. In this, it is most similar to Errudite\n(Wu et al., 2019), which provides an integrated UI\nfor NLP error analysis, including a custom DSL\nfor text transformations and the ability to evaluate\nover a corpus. However, LIT is explicitly designed\nfor ﬂexibility: we support a broad range of work-\nﬂows and provide a modular design for extension\nwith new tasks, visualizations, and generation tech-\nniques.\nLimitations LIT is an evaluation tool, and as\nsuch is not directly useful for training-time mon-\nitoring. As LIT is built to be interactive, it does\nnot scale to large datasets as well as ofﬂine tools\nsuch as TFMA (Mewald, 2019). (Currently, the\nLIT UI can handle about 10,000 examples at once.)\nBecause LIT is framework-agnostic, it does not\nhave the deep model integration of tools such as\nAllenNLP Interpret (Wallace et al., 2019) or Cap-\ntum (Kokhlikyan et al., 2019). This makes many\nthings simpler and more portable, but also requires\nmore code for techniques like integrated gradients\n(Sundararajan et al., 2017) that need to directly\nmanipulate parts of the model.\n6 Conclusion and Roadmap\nLIT provides an integrated UI and a suite of com-\nponents for visualizing and exploring the behav-\nior of NLP models. It enables interactive analysis\nboth at the single-datapoint level and over a whole\ndataset, with ﬁrst-class support for counterfactual\ngeneration and evaluation. LIT supports a diverse\nrange of workﬂows, from explaining individual pre-\ndictions to disaggregated analysis to probing for\nbias through counterfactuals. LIT also supports a\nrange of model types and techniques out of the box,\nand is designed for extensibility through simple,\nframework-agnostic APIs.\nLIT is under active development by a small team.\nPlanned upcoming additions include new counter-\nfactual generation plug-ins, additional metrics and\nvisualizations for sequence and structured output\ntypes, and a greater ability to customize the UI for\ndifferent applications.\nLIT is open-source under an Apache 2.0 license,\nand we welcome contributions from the community\nat https://github.com/pair-code/lit.\n113\nAcknowledgments\nWe thank Slav Petrov, Martin Wattenberg, Fer-\nnanda Viegas, Kellie Webster, Emily Pitler, Dipan-\njan Das, Leslie Lai, Kristen Olson, and other mem-\nbers of PAIR and the Language team at Google\nResearch for many productive discussions during\ndevelopment. We also thank our anonymous re-\nviewers for their helpful feedback, and Pere Lluis,\nLuke Gessler, and Kevin Robinson for their contri-\nbutions to the open-source code.\nReferences\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Ian Goodfellow, Andrew Harp, Geoffrey\nIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-\nicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-\nberg, Dandelion Man´e, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,\nPaul Tucker, Vincent Vanhoucke, Vijay Vasudevan,\nFernanda Vi´egas, Oriol Vinyals, Pete Warden, Mar-\ntin Wattenberg, Martin Wicke, Yuan Yu, and Xiao-\nqiang Zheng. 2015. TensorFlow: Large-scale ma-\nchine learning on heterogeneous systems. Software\navailable from tensorﬂow.org.\nYongsu Ahn and Yu-Ru Lin. 2019. Fairsight: Visual\nanalytics for fairness in decision making. IEEE\nTransactions on Visualization and Computer Graph-\nics, page 1–1.\nAlexandr Andoni and Piotr Indyk. 2006. Near-optimal\nhashing algorithms for approximate nearest neigh-\nbor in high dimensions. In 2006 47th annual\nIEEE symposium on foundations of computer sci-\nence (FOCS’06), pages 459–468. IEEE.\nColin Bannard and Chris Callison-Burch. 2005. Para-\nphrasing with bilingual parallel corpora. In Proceed-\nings of the 43rd Annual Meeting of the Association\nfor Computational Linguistics (ACL’05), pages 597–\n604, Ann Arbor, Michigan. Association for Compu-\ntational Linguistics.\nYonatan Belinkov, Sebastian Gehrmann, and Ellie\nPavlick. 2020. Interpretability and analysis in neural\nNLP. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics: Tu-\ntorial Abstracts, pages 1–5, Online. Association for\nComputational Linguistics.\nJohn Blitzer and Fernando Pereira. 2007. Domain\nadaptation of natural language processing systems.\nUniversity of Pennsylvania, pages 1–106.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Ad-\nvances in Neural Information Processing Systems\n29, pages 4349–4357.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\n´Angel Alexander Cabrera, Will Epperson, Fred\nHohman, Minsuk Kahng, Jamie Morgenstern, and\nDuen Horng Chau. 2019. Fairvis: Visual analytics\nfor discovering intersectional bias in machine learn-\ning. In 2019 IEEE Conference on Visual Analytics\nScience and Technology (VAST), pages 46–56. IEEE.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges. Eval-\nuating Predictive Uncertainty, Visual Object Classi-\nﬁcation, and Recognising Tectual Entailment, pages\n177–190, Berlin, Heidelberg. Springer Berlin Hei-\ndelberg.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing\nDou. 2018. HotFlip: White-box adversarial exam-\nples for text classiﬁcation. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n31–36, Melbourne, Australia. Association for Com-\nputational Linguistics.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2017. AllenNLP: A deep semantic natural language\nprocessing platform.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural Informa-\ntion Processing Systems 28, pages 1693–1701.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian\nGehrmann. 2020. exBERT: A visual analysis tool to\nexplore learned representations in Transformer mod-\nels. In Proceedings of the 58th Annual Meeting of\n114\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations, pages 187–196, Online. Asso-\nciation for Computational Linguistics.\nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006. Ontonotes:\nThe 90% solution. In Proceedings of the Human\nLanguage Technology Conference of the NAACL,\nCompanion Volume: Short Papers , NAACL-Short\n’06, pages 57–60, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2021–2031, Copenhagen, Denmark. Association for\nComputational Linguistics.\nNarine Kokhlikyan, Vivek Miglani, Miguel Mar-\ntin, Edward Wang, Jonathan Reynolds, Alexan-\nder Melnikov, Natalia Lunova, and Orion Reblitz-\nRichardson. 2019. Pytorch captum. https://\ngithub.com/pytorch/captum.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 681–691, San Diego, California. As-\nsociation for Computational Linguistics.\nTal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and\nDieuwke Hupkes, editors. 2019. Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP . Association\nfor Computational Linguistics, Florence, Italy.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nLeland McInnes, John Healy, and James Melville.\n2018. Umap: Uniform manifold approximation and\nprojection for dimension reduction. arXiv preprint\narXiv:1802.03426.\nClemens Mewald. 2019. Introducing tensorﬂow model\nanalysis: Scaleable, sliced, and full-pass metrics.\nhttps://blog.tensorflow.org/2018/03/\nintroducing-tensorflow-model-analysis.\nhtml.\nRamaravind K Mothilal, Amit Sharma, and Chenhao\nTan. 2020. Explaining machine learning classiﬁers\nthrough diverse counterfactual explanations. In Pro-\nceedings of the 2020 Conference on Fairness, Ac-\ncountability, and Transparency, pages 607–617.\nHarsha Nori, Samuel Jenkins, Paul Koch, and Rich\nCaruana. 2019. InterpretML: A uniﬁed framework\nfor machine learning interpretability. arXiv preprint\narXiv:1909.09223.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language in-\nference. In Proceedings of the Seventh Joint Con-\nference on Lexical and Computational Semantics ,\npages 180–191, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multi-\ntask learners. https://blog.openai.com/\nbetter-language-models.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv e-prints.\nMarco Ribeiro, Sameer Singh, and Carlos Guestrin.\n2016. “why should I trust you?”: Explaining the pre-\ndictions of any classiﬁer. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Demon-\nstrations, pages 97–101, San Diego, California. As-\nsociation for Computational Linguistics.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 8–14, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nDaniel Smilkov, Nikhil Thorat, Charles Nicholson,\nEmily Reif, Fernanda B Vi´egas, and Martin Watten-\nberg. 2016. Embedding projector: Interactive visu-\nalization and interpretation of embeddings. In NIPS\n115\n2016 Workshop on Interpretable Machine Learning\nin Complex Systems.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nHendrik Strobelt, Sebastian Gehrmann, Michael\nBehrisch, Adam Perer, Hanspeter Pﬁster, and\nAlexander M Rush. 2018. Seq2seq-vis: A visual\ndebugging tool for sequence-to-sequence models.\nIEEE transactions on visualization and computer\ngraphics, 25(1):353–363.\nHendrik Strobelt, Sebastian Gehrmann, Hanspeter Pﬁs-\nter, and Alexander M Rush. 2017. LSTMvis: A tool\nfor visual analysis of hidden state dynamics in recur-\nrent neural networks. IEEE transactions on visual-\nization and computer graphics, 24(1):667–676.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Pro-\nceedings of the 34th International Conference on\nMachine Learning , volume 70, pages 3319–3328.\nPMLR.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63–76, Florence, Italy. As-\nsociation for Computational Linguistics.\nEric Wallace, Jens Tuyls, Junlin Wang, Sanjay Sub-\nramanian, Matt Gardner, and Sameer Singh. 2019.\nAllenNLP interpret: A framework for explaining\npredictions of NLP models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP): System Demonstrations , pages\n7–12, Hong Kong, China. Association for Compu-\ntational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nJ. Wexler, M. Pushkarna, T. Bolukbasi, M. Wattenberg,\nF. Vi´egas, and J. Wilson. 2020. The what-if tool: In-\nteractive probing of machine learning models. IEEE\nTransactions on Visualization and Computer Graph-\nics, 26(1):56–65.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer,\nand Daniel Weld. 2019. Errudite: Scalable, repro-\nducible, and testable error analysis. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 747–763, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nJiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, and\nDavid Ebert. 2018. Manifold: A model-agnostic\nframework for interpretation and diagnosis of ma-\nchine learning models. IEEE Transactions on Visu-\nalization and Computer Graphics, PP:1–1.\n116\nA Appendices\nFigure A.1: The counterfactual generator module, showing a set of generated datapoints in the staging area. The\nlabels can be maually edited before adding these to the dataset. In this example, the counterfactuals were created\nusing the word replacer, replacing the word “great” with “terrible” in each passage.\nFigure A.2: Overview of LIT system architecture. The backend manages models, datasets, metrics, generators,\nand interpretation components, as well as a caching layer to speed up interactive use. The frontend is a TypeScript\nsingle-page app consisting of independent modules (webcomponents built with lit-element) which interact with\nshared “services” that manage interaction state. The backend can be extended by passing components to the\nlit.Server class in the demo script (Section 4.3 and Figure A.3), while the frontend can be extended by\nimporting new components in a single ﬁle, layout.ts, which both lists available modules and speciﬁes their\nposition in the UI (Figure 1).\n117\nNLI_LABELS = ['entailment', 'neutral', 'contradiction']\nclass MultiNLIData(lit.Dataset):\n\"\"\"Loader for MultiNLI dataset.\"\"\"\ndef __init__(self, path):\n# Read the eval set from a .tsv file\ndf = pandas.read_csv(path, sep='\\t')\n# Store as a list of dicts, conforming to self.spec()\nself._examples = [{\n'premise': row['sentence1'],\n'hypothesis': row['sentence2'],\n'label': row['gold_label'],\n'genre': row['genre'],\n} for _, row in df.iterrows()]\ndef spec(self):\nreturn {\n'premise': lit_types.TextSegment(),\n'hypothesis': lit_types.TextSegment(),\n'label': lit_types.Label(vocab=NLI_LABELS),\n# We can include additional fields, which don't have to be used by the model.\n'genre': lit_types.Label(),\n}\nclass MyNLIModel(lit.Model):\n\"\"\"Wrapper for a Natural Language Inference model.\"\"\"\ndef __init__(self, model_path, **kw):\n# Load the model into memory so we're ready for interactive use.\nself._model = _load_my_model(model_path, **kw)\n##\n# LIT API implementations\ndef predict(self, inputs: List[Input]) -> Iterable[Preds]:\n\"\"\"Predict on a single minibatch of examples.\"\"\"\nexamples = [self._model.convert_dict_input(d) for d in inputs] # any custom preprocessing\nreturn self._model.predict_examples(examples) # returns a dict for each input\ndef input_spec(self):\n\"\"\"Describe the inputs to the model.\"\"\"\nreturn {\n'premise': lit_types.TextSegment(),\n'hypothesis': lit_types.TextSegment(),\n}\ndef output_spec(self):\n\"\"\"Describe the model outputs.\"\"\"\nreturn {\n# The 'parent' keyword tells LIT where to look for gold labels when computing metrics.\n'probas': lit_types.MulticlassPreds(vocab=NLI_LABELS, parent='label'),\n# This model returns two different embeddings, but you can easily add more.\n'output_embs': lit_types.Embeddings(),\n'mean_word_embs': lit_types.Embeddings(),\n# In LIT, we treat tokens as another model output. There can be more than one,\n# and the 'align' field describes which input segment they correspond to.\n'premise_tokens': lit_types.Tokens(align='premise'),\n'hypothesis_tokens': lit_types.Tokens(align='hypothesis'),\n# Gradients are also returned by the model; 'align' here references a Tokens field.\n'premise_grad': lit_types.TokenGradients(align='premise_tokens'),\n'hypothesis_grad': lit_types.TokenGradients(align='hypothesis_tokens'),\n# Similarly, attention references a token field, but here we want the model's full \"internal\"\n# tokenization, which might be something like: [START] foo bar baz [SEP] spam eggs [END]\n'tokens': lit_types.Tokens(),\n'attention_layer0': lit_types.AttentionHeads(align=['tokens', 'tokens']),\n'attention_layer1': lit_types.AttentionHeads(align=['tokens', 'tokens']),\n'attention_layer2': lit_types.AttentionHeads(align=['tokens', 'tokens']),\n# ...and so on. Since the spec is just a dictionary of dataclasses, you can populate it\n# in a loop if you have many similar fields.\n}\ndef main(_):\ndatasets = {\n'mnli_matched': MultiNLIData('/path/to/dev_matched.tsv'),\n'mnli_mismatched': MultiNLIData('/path/to/dev_mismatched.tsv'),\n}\nmodels = {\n'model_foo': MyNLIModel('/path/to/model/foo/files'),\n'model_bar': MyNLIModel('/path/to/model/bar/files'),\n}\nlit_demo = lit.Server(models, datasets, port=4321)\nlit_demo.serve()\nif __name__ == '__main__':\nmain()\nFigure A.3: Example demo script to run LIT with two NLI models and the MultiNLI (Williams et al., 2018)\ndevelopment sets. The actual model can be implemented in TensorFlow, PyTorch, C++, a REST API, or anything\nthat can be wrapped in a Python class: to work with LIT, users needs only to deﬁne the spec ﬁelds and implement\na predict()function which returns a dict of NumPy arrays for each input datapoint. The dataset loader is even\nsimpler; a complete implementation is given above to read from a TSV ﬁle, but libraries like TensorFlow Datasets\ncan also be used.\n118\nFigure A.4: Full UI screenshot, showing a BERT (Devlin et al., 2019) model on a sample from the “matched”\nsplit of the MultiNLI (Williams et al., 2018) development set. The embedding projector (top left) shows three\nclusters, corresponding to the output layer of the model, and colored by the true label. On the bottom, the metrics\ntable shows accuracy scores faceted by genre, and a confusion matrix shows the model predictions against the gold\nlabels.\n(a)\n(b)\nFigure A.5: Confusion matrix (a) and side-by-side comparison of predictions and salience maps (b) on two sen-\ntiment classiﬁers. In model comparison mode, the confusion matrix can compare two models, and clicking an\noff-diagonal cell with select examples where the two models make different predictions. In (b) we see one such\nexample, where the model in the second row (“sst 1”) predicts incorrectly, even though gradient-based salience\nshow both models focusing on the same tokens.",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.8764297962188721
    },
    {
      "name": "Computer science",
      "score": 0.7579089999198914
    },
    {
      "name": "Natural language processing",
      "score": 0.6510398983955383
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6099634766578674
    },
    {
      "name": "Data visualization",
      "score": 0.41848593950271606
    },
    {
      "name": "Visualization",
      "score": 0.40168941020965576
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}