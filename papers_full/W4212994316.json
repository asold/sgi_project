{
  "title": "BERT-Kgly: A Bidirectional Encoder Representations From Transformers (BERT)-Based Model for Predicting Lysine Glycation Site for Homo sapiens",
  "url": "https://openalex.org/W4212994316",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2108821508",
      "name": "Yinbo Liu",
      "affiliations": [
        "Anhui Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2098400666",
      "name": "Liu Yufeng",
      "affiliations": [
        "Anhui Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A4208647805",
      "name": "Gang‐Ao Wang",
      "affiliations": [
        "Anhui Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2944315920",
      "name": "Yinchu Cheng",
      "affiliations": [
        "Anhui Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2100661385",
      "name": "Shoudong Bi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097401853",
      "name": "Xiaolei Zhu",
      "affiliations": [
        "Anhui Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2108821508",
      "name": "Yinbo Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098400666",
      "name": "Liu Yufeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208647805",
      "name": "Gang‐Ao Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2944315920",
      "name": "Yinchu Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097401853",
      "name": "Xiaolei Zhu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2033274423",
    "https://openalex.org/W2959019559",
    "https://openalex.org/W2920013165",
    "https://openalex.org/W3167718630",
    "https://openalex.org/W3199970975",
    "https://openalex.org/W2912369228",
    "https://openalex.org/W2894836881",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W3095636085",
    "https://openalex.org/W2107815233",
    "https://openalex.org/W3162632405",
    "https://openalex.org/W2122111042",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1678356000",
    "https://openalex.org/W2919709896",
    "https://openalex.org/W150772031",
    "https://openalex.org/W2143210482",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2043274355",
    "https://openalex.org/W2097606916",
    "https://openalex.org/W2801931817",
    "https://openalex.org/W2153456067",
    "https://openalex.org/W2764095180",
    "https://openalex.org/W3094508205",
    "https://openalex.org/W3111718248",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W3129125493",
    "https://openalex.org/W6712546213",
    "https://openalex.org/W2050566492",
    "https://openalex.org/W2116062594",
    "https://openalex.org/W3094567318",
    "https://openalex.org/W3207838037",
    "https://openalex.org/W3032450815",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W2911904997",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2048356293",
    "https://openalex.org/W2009540433",
    "https://openalex.org/W2107361477",
    "https://openalex.org/W1994051814",
    "https://openalex.org/W2396245719",
    "https://openalex.org/W2141818629",
    "https://openalex.org/W2156909104",
    "https://openalex.org/W6606138569",
    "https://openalex.org/W2920338801",
    "https://openalex.org/W2610582292",
    "https://openalex.org/W2555891876",
    "https://openalex.org/W2558091742",
    "https://openalex.org/W3141691617",
    "https://openalex.org/W2996852829",
    "https://openalex.org/W2905744876",
    "https://openalex.org/W6696884364",
    "https://openalex.org/W3164453494",
    "https://openalex.org/W2888353590",
    "https://openalex.org/W2765093153",
    "https://openalex.org/W2981093945",
    "https://openalex.org/W2293023260",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W2951433247"
  ],
  "abstract": "As one of the most important posttranslational modifications (PTMs), protein lysine glycation changes the characteristics of the proteins and leads to the dysfunction of the proteins, which may cause diseases. Accurately detecting the glycation sites is of great benefit for understanding the biological function and potential mechanism of glycation in the treatment of diseases. However, experimental methods are expensive and time-consuming for lysine glycation site identification. Instead, computational methods, with their higher efficiency and lower cost, could be an important supplement to the experimental methods. In this study, we proposed a novel predictor, BERT-Kgly, for protein lysine glycation site prediction, which was developed by extracting embedding features of protein segments from pretrained Bidirectional Encoder Representations from Transformers (BERT) models. Three pretrained BERT models were explored to get the embeddings with optimal representability, and three downstream deep networks were employed to build our models. Our results showed that the model based on embeddings extracted from the BERT model pretrained on 556,603 protein sequences of UniProt outperforms other models. In addition, an independent test set was used to evaluate and compare our model with other existing methods, which indicated that our model was superior to other existing models.",
  "full_text": "BERT-Kgly: A Bidirectional Encoder\nRepresentations From Transformers\n(BERT)-Based Model for Predicting\nLysine Glycation Site forHomo\nsapiens\nYinbo Liu†, Yufeng Liu†, Gang-Ao Wang, Yinchu Cheng, Shoudong Bi* and Xiaolei Zhu*\nSchool of Sciences, Anhui Agricultural University, Hefei, China\nAs one of the most important posttranslational modiﬁcations (PTMs), protein lysine glycation\nchanges the characteristics of the proteins and leads to the dysfunction of the proteins,\nwhich may cause diseases. Accurately detecting the glycation sites is of great beneﬁtf o r\nunderstanding the biological function and potential mechanism of glycation in the treatment\nof diseases. However, experimental methods are expensive and time-consuming for lysine\nglycation site identiﬁcation. Instead, computational methods, with their higher efﬁciency and\nlower cost, could be an important supplement to the experimental methods. In this study, we\nproposed a novel predictor, BERT-Kgly, for protein lysine glycation site prediction, which\nwas developed by extracting embedding features of protein segments from pretrained\nBidirectional Encoder Representations from Transformers (BERT) models. Three pretrained\nBERT models were explored to get the embeddings with optimal representability, and three\ndownstream deep networks were employed to build our models. Our results showed that\nthe model based on embeddings extracted from the BERT model pretrained on 556,603\nprotein sequences of UniProt outperforms other models. In addition, an independent test set\nwas used to evaluate and compare our model with other existing methods, which indicated\nthat our model was superior to other existing models.\nKeywords: protein lysine glycation, BERT, biological sequence, natural language processing, posttranslational\nmodiﬁcation (PTM), embedding\n1 INTRODUCTION\nAs one of the most important posttranslational modiﬁcations (PTMs) of proteins, glycation is a two-\nstep non-enzymatic reaction that is different from glycosylation, which is an enzyme-dependent\nreaction (Stitt 2001). Advanced glycation end products (AGEs) generated in the reaction are involved\nin different human diseases (Vlassara et al., 1994; Ling et al., 1998; Ahmed et al., 2005), such as\ndiabetes, Alzheimer’s disease, and Parkinson’s disease. The identiﬁcation of glycation sites in\nproteins would be of great beneﬁt for the understanding of the biological function of protein\nglycation and treatment of the related diseases. In addition to human metabolism, protein glycation\nis also an unavoidable part of plant metabolism and proteotoxicity (Rabbani et al., 2020).\nDifferent methods have been developed for detecting lysine glycation (Kgly) sites. Wet experiment\nm e t h o d ss u c ha sm a s ss p e c t r o m e t r y(Thornalley et al., 2003) and electrochemical chip (Khan and Park\n2020) have been used to identify lysine glycation sites. However, wet experiment methods are both cost-\nEdited by:\nZhibin Lv,\nSichuan university, China\nReviewed by:\nLijun Dou,\nShenzhen Polytechnic, China\nJiangning Song,\nMonash University, Australia\n*Correspondence:\nShoudong Bi\nbishoudong@163.com\nXiaolei Zhu\nxlzhu_mdl@hotmail.com\n†These authors have contributed\nequally to this work.\nSpecialty section:\nThis article was submitted to\nProtein Bioinformatics,\na section of the journal\nFrontiers in Bioinformatics\nReceived: 13 December 2021\nAccepted: 20 January 2022\nPublished: 18 February 2022\nCitation:\nLiu Y, Liu Y, Wang G-A, Cheng Y, Bi S\nand Zhu X (2022) BERT-Kgly: A\nBidirectional Encoder Representations\nFrom Transformers (BERT)-Based\nModel for Predicting Lysine Glycation\nSite for Homo sapiens.\nFront. Bioinform. 2:834153.\ndoi: 10.3389/fbinf.2022.834153\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341531\nORIGINAL RESEARCH\npublished: 18 February 2022\ndoi: 10.3389/fbinf.2022.834153\nand time-consuming. Alternatively, several in silico methods\n(Johansen et al., 2006; Liu et al., 2015; Ju et al., 2017; Xu H.\net al., 2017; Zhao et al., 2017; Islam et al., 2018; Chen K. et al.,\n2019; Yu et al., 2019; Khanum et al., 2020; Yang et al., 2021; Yao et al.,\n2021) have been developed to predict the Kgly sites efﬁciently. In a\npioneer work, Johansen et al. proposed a predictor, GlyNN, built by\nneural networks based on a dataset with 89 Kgly sites and 126 non-\nKgly sites of 20 proteins (Johansen et al., 2006). Later, Liu et al.\ndeveloped a model, PreGly, by using support vector machine (SVM)\nfor detecting Kgly sites (Liu et al., 2015). They used the same dataset\nas Johansen et al.’s study and generated three kinds of sequence\nfeatures that were selected by using the maximum relevance\nminimum redundancy (mRMR) and the incremental feature\nselection (IFS) methods. Based on the larger training dataset, Xu\net al. built a Kgly site prediction model, Gly-PseAAC, based on\nsequence order information and position-speciﬁca m i n oa c i d\npropensity (Xu Y. et al., 2017). By using the same dataset as Xu\net al.’s study, Ju et al. constructed a model, BPB_GlySite, to predict\nglycation sites by using a single feature of bi-proﬁle Bayes (BPB) (Ju\net al., 2017). By using Xu et al.’s dataset as a training dataset, Zhao\net al. built a model, Glypre, based on fused multiple featuresvia using\na two-step feature selection method (Zhao et al., 2017). In addition,\nthey used another two datasets to test the generalization of their\nmodel. Benchmarked on Xu et al.’s dataset and other two datasets,\nIslam et al. proposed a method, iProtGly-SS, to predict Kgly by\nsearching the optimal feature subset from sequential features,\nphysicochemical properties, and structural features using an\nincremental group-based feature selection algorithm (Islam et al.,\n2018). Based on predicted structural properties of residues, Reddy\net al. developed a model, GlyStruct, based on the SVM (Reddy et al.,\n2019). Leveraging Xu et al.’s dataset as training dataset, Yao et al.\ndeveloped a model, ABC-Gly, by selecting the optimal feature subset\nwith a two-step feature selection method by combining the Fisher\nscore and an improved binary artiﬁcial bee colony algorithm (Yao\net al., 2021). All the previous methods were built on the dataset with\nless than 500 Kgly sites; however, four other methods, PredGly (Yu\net al., 2019), Gly-LysPred (Khanum et al., 2020), MUscADEL (Chen\nZ et al., 2019), and MultiLyGAN (Yang et al., 2021), which were built\non datasets with more than 1,000 Kgly sites. For building PredGly,\nYu et al. (2019)collected Kgly sites from PLMD (Xu H. et al., 2017)\nand used CD-HIT (Huang et al., 2010) to remove the redundancy for\nprotein sequences and peptide segments, with a cutoff of 30%. The\ndataset contains 3,969 non-redundant Kgly sites and 82,270 non-\nKgly sites. Based on the dataset, they built their model by selecting an\noptimal feature subset via XGBoost (Chen and Guestrin 2016). By\ncollecting Kgly sites from UniProt (https://www.uniprot.org/),\nKhanum et al. obtained their dataset with 1,287 Kgly sites and\n1,300 non-Kgly sites by using CD-HIT to remove the redundancy,\nwith a cutoff of 60%, and then built their model by using random\nforest (Khanum et al., 2020). Both MUscADEL (Chen K et al., 2019)\nand MultiLyGAN (Yang et al., 2021) were developed to predict\nmultiple lysine modiﬁcation sites. For MUscADEL, Chen et al.\ncollected Kgly sites for bothHomo sapiens and Mus musculus\nfrom the PhosphoSitePlus database (Hornbeck et al., 2015), and\nthen removed the redundancy of protein sequences by using CD-\nHIT (Huang et al., 2010), with a cutoff of 30%. Based on the dataset\nwith 3,209 Kgly sites, they built their model by using a deep learning\nalgorithm. In MultiLyGAN (Yang et al., 2021), Yang et al. collected\nlysine modiﬁcation sites from the CPLM2.0 database (Liu et al.,\n2014), and after removing redundancy by using CD-HIT at the\nsegment level with a cutoff of 40%, they obtained 1,454 Kgly sites.\nTheir model is a multiple-label model built with data augmentation\nby conditional Wasserstein generative adversarial networks. The\ndetails of all these tools are summarized inSupplementary Table S1.\nAlthough considerable progress has been made for\ndifferentiating Kgly sites and non-Kgly sites, the performance of\nthese methods is still not satisfactory. One possible reason is the\nlimitation of the representability of the features used before. The\npowerful representability of the Bidirectional Encoder\nRepresentations from Transformers (BERT) (Devlin et al., 2019)\nmodel has been demonstrated in the currentﬁeld of natural language\nprocessing (NLP). By considering the biological sequences as\nsentences, their representability has been explored in a variety of\nworks. Rives et al. pretrained protein language BERT models based\non 250 million protein sequences (Rives et al., 2021) and explored\nthe representations of these models, and their results demonstrated\nthat the information of protein structure and function was encoded\nin representations of these models. Rao et al. pretrained protein\nlanguage BERT models based on 31 million protein sequences (Rao\net al., 2019). Zhang et al. pretrained protein language BERT models\nbased on 556,603 protein sequences (Zhang et al., 2021). The\nembeddings extracted by the BERT pretrained models have been\nused as features for classiﬁcation in bioinformatics. With the\nembeddings, Le et al. have developed a model to predict\nenhancers (Le et al., 2021), Qiao et al. have developed a model to\npredict Kcr sites (Qiao et al., 2021). Thus, the embeddings of\npretrained BERT models may be helpful for building a more\neffective model for Kgly sites prediction.\nIn this study, we proposed a computational approach called\nBERT-Kgly to improve the predictive performance of lysine\nglycation sites. Considering peptide segments as sentences, the\nembeddings were extracted from three different pretrained BERT\nmodels which were fed to the downstream classiﬁers for Kgly site\nprediction. In addition, several traditional features were also\nextracted, and their performance was compared with the\nembeddings of BERT. Furthermore, the built model with\nembeddings of BERT was compared with the existing methods\nusing an independent test set. Empirical studies showed that our\nmodel, BERT-Kgly, outperforms other methods, with an area\nunder the receiver operating characteristic curve (AUROC) of\n0.69. The workﬂow of BERT-Kgly is shown inFigure 1.\n2 METHODS AND MATERIALS\n2.1 Data Sets\nIn this study, we used the same dataset as that collected by Yu\net al. (Yu et al., 2019), for their dataset is the largest forHomo\nsapiens, as shown inSupplementary Table S1. The dataset was\ncollected from the PLMD database (http://plmd. biocuckoo. org/)\n(Xu Y. et al., 2017). The redundancy of the dataset was removed\nby a two-step process on the protein level and segment level with\nCD-HIT ( Huang et al., 2010 ) by using a cutoff of 30%,\nrespectively. Overall, the dataset contains 3,969 positive and\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341532\nLiu et al. Kgly Prediction with BERT\n82,270 negative samples, and about 90% of positive samples\n3,573) and an equal number of negative samples were selected\nrandomly for training. For the independent test set, Yu et al.\nselected 200 positive and 200 negative samples from the\nremaining datasets. In Yu et al.’s dataset, each sample contains\n31 residues with the lysine in the middle. The protein segments\nwith different lengths can be used to build our model. In previous\nworks, (Yu et al., 2019and Zhao et al., 2017). have demonstrated\nthat the segments with 15 downstream and upstream residues\nshowed the best performance. All data and codes are available at\nhttps://github.com/yinboliu-git/Gly-ML-BERT-DL.\n2.2 Feature Extraction\n2.2.1 Embeddings of BERT Pretrained Models\nWe used three different BERT pretrained models to encode\nthe peptide segments in our datasets, which are the initial\nnatural language BERT-Base model released by Google\nResearch ( Devlin et al., 2019 ), Zhang et al.’sB E R Tm o d e l\n(Zhang et al., 2021) which was pretrained on 556,603 protein\nsequences from UniProt (n amed as BERT-prot), and the\nTAPE model ( Rao et al., 2019 )w h i c hi sb a s e do n\n31 million protein domains from Pfam. These models\nencode a 768-dimensional vector corresponding to each\nresidue of the peptide segments.\nThe Bidirectional Encoder Representations from\nTransformers (BERT) model was developed by Devlin et al.\n(Devlin et al., 2019), which has achieved new state-of-the-art\nresults on 11 natural language processing (NLP) tasks. The\narchitecture of BERT is a multilayered bidirectional\nTransformer encoder, which jointly conditions on both left\nand right context using the attention mechanism in all\nencoder layers and processes all words in the sentence in\nparallel. The network structures of all the encoder layers are\nthe same, which mainly consisted of two sublayers: the multi-\nhead self-attention layer and the feed-forward neural network\nlayer. In addition, a residual connection is added on each of the\nsublayer; thus, the output of each sublayer is LayerNorm (x +\nSublayer(x)). When a sentence is inputted into the BERT model,\neach word was encoded by three embeddings: token embeddings,\nsegment embeddings, and position embeddings. Then, we can\nFIGURE 1 |Flowchart for building our model.\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341533\nLiu et al. Kgly Prediction with BERT\nobtain context-dependent features from different encoder layers\nof the model.\nFor comparison, we also calculated six types of traditional\nsequence-based features as follows.\n2.2.2 Amino Acid Composition\nAs a classic sequence coding feature, amino acid composition\n(AAC) has been used extensively for PTM sites prediction (Xu\net al., 2016; Yu et al., 2019; Zhang et al., 2019; Basith et al., 2021).\nIt counts the occurrence frequency of each of the 20 natural\namino acids and one complementary amino acid “O” in the\npeptide segments.\n2.2.3 K-Spaced Amino Acid Pair Composition\nK-spaced amino acid pair composition or composition of\nk-spaced amino acid pairs (CKSAAP) is another sequence\nencoding scheme that has been employed to predict various\nPTMs (Chen et al., 2008; Fu et al., 2019; Wu et al., 2019; Lv\net al., 2020; Chen et al., 2021). This method mainly calculates the\nfrequency of different pairs of amino acids separated by k-length\npeptides. If we usedA\n1X{k}A2 to represent k-spaced amino acid\npairs, bothA1 and A2 can be the 21 types of amino acids, so there\nare 441 types of k-spaced amino acid pairs. Each of them can be\ncalculated as follows:\nf (A\n1X{k}A2) /equals N(A1X{k}A2)/(L − k + 1),\nwhere L represents the length of the segment and\nN(A1X{k}A2) is the occurrence frequency ofA1X{k}A2.\n2.2.4 Position Weight Amino Acid Composition\nPosition weight amino acid composition (PWAA), which isﬁrst\nproposed by Shi et al. (Shi et al., 2012), is used to extract the\nsequence order information of amino acid residues around target\nresidues. For each of the 20 types of residues, the feature can be\ncalculated by using the following equation:\nPWAA(i) /equals\n1\nL(L + 1) ∑\nL\nj/equals− L\nxi,j(j +\n⏐⏐⏐⏐j\n⏐⏐⏐⏐\nL),\nwhere i denotes one of the 20 types of residues, L represents the\nnumber of upstream or downstream residues, andxi,j describes if\nthe type of the residue on positionj of the peptide segment is the\nsame as i, if true then its value is 1, otherwise 0.\n2.2.5 Dipeptide Bi-Proﬁle Bayes (DBPB)\nThe bi-proﬁle Bayes feature proposed by Shao et al. (Shao et al.,\n2009) is used to represent the occurrence probability of each type\nof residues on each position of the positive peptide segments and\nnegative peptide segments, respectively. Thus, the dipeptide bi-\nproﬁle Bayes (DBPB) feature is used to represent the occurrence\nprobability of each type of dipeptides on each position of the\npositive peptide segments and negative peptide segments,\nrespectively. These probabilities were ﬁrst calculated based on\nthe data used for training which were then assigned to the peptide\nused for testing. Note that the data used for validation could not\nbe used for calculating the probabilities in the cross-\nvalidation stage.\n2.2.6 Encoding Based on Grouped Weight (EBGW)\nFor calculating this kind of feature, the 20 types of residues were\nﬁrst classiﬁed into four different groups according to the charge\nand hydrophobicity properties. Then, the four groups were\nfurther divided into three categories. For each category, the 20\ntypes of residues were divided into two classes, so that a binary\nrepresentation can be obtained for a residue according to which\nclass it belongs to. Thus, a peptide segment with length L can be\nrepresented as a binary vector with the same length. Totally, we\nobtained three binary vectors for each peptide segment. Each\nvector was then divided into J sub-vectors increasing in length,\nthe feature for each sub-vector were calculated as follows: X(j) =\nsum (sub-vector(j))/length (sub-vector(j)). In all, we obtained 3 J-\ndimension feature vectors for each peptide segment. J was set as 5\naccording to previous studies (Shi et al., 2012; Yu et al., 2019).\n2.2.7 K-Nearest Neighbor (KNN) Feature\nThe k-nearest neighbor feature counts the positive samples\npercentage of the k nearest samples in the training dataset to\nthe query sample. For peptide segment samples, the distance\nbetween two different samples is represented by sequence\nsimilarity which is calculated as follows:\nDist(S\n1, S2) /equals 1 − ∑L\ni/equals 1Sim(S1(i), S2(i))\nL ,\nwhere L is the length of the peptide segments andS1(i) and S2(i)\nrepresent the ith residues of the two segments S1 and S2,\nrespectively. The similarity between S1(i) and S2(i) is\ncomputed as follows:\nSim(m, n) /equals B(m, n) − min(B)\nmax(B) − min(B),\nwhere B represents the BLOSUM62 substitution matrix\n(Henikoff and Henikoff 1992 ) and max(B) and min(B)\nrepresent the largest and smallest values of the matrix,\nrespectively. Given k = 2,4,8,16,32, we generated 5D feature\nvectors for a given peptide segment.\n2.3 Machine Learning and Deep Learning\nAlgorithms\n2.3.1 SVM\nA support vector machine (SVM) is one of the most popular\nlearning algorithms which has been used extensively in\nbioinformatics (Chen Z. et al., 2019; Zhu et al., 2019; Chen\net al., 2020). SVM was ﬁrst proposed by Vapnik (1995), the\nmain idea of which is to determine a hyperplane to maximize the\nmargin between different classes. In this study, the sklearn\npackage for Python 3 (https://www.python.org) was used to\nbuild the SVM classiﬁers.\n2.3.2 Random Forest\nRandom forest (RF) (Breiman 2001) is an ensemble learning\nalgorithm by using a decision tree as a base learner. Based on\ndifferent training sets which were sampled from the original\ntraining dataset and different feature subsets which were\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341534\nLiu et al. Kgly Prediction with BERT\nrandomly selected from the original feature set, multiple decision\ntrees were built. The class of a test sample is determined based on\nthe voting result of all the base decision trees. In this work, the\nsklearn package for Python 3 (https://www.python.org) was used\nto build the RF classiﬁer.\n2.3.3 XGBoost\nXGBoost (Chen and Guestrin 2016) is an also ensemble learning\nalgorithm using a decision tree as a base learner. Based on the\ngradient boosting decision tree (GBDT) (Friedman 2001), the\nregularization term was added which effectively prevents the\nproblem of overﬁtting. The algorithm not only inherits the\ngood performance of the original boosting algorithm but also\nshows the advantage to process sparse data and high dimensional\ndata. In this study, the xgboost package for Python 3 is used to\nbuild the XGBoost classiﬁer.\n2.3.4 KNN\nThe K-nearest neighbor classiﬁcation rule wasﬁrst proposed by\nCover et al. (Cover and Hart 1967), in which the new sample was\nclassiﬁed based on its nearest set of previously classiﬁed samples.\nThe algorithm does not depend on any special distribution of the\nsamples, which has been a ubiquitous classiﬁcation tool with good\nscalability.\n2.3.5 CNN\nAs a famous deep network, the convolutional neural network\n(CNN) (Krizhevsky et al., 2012), was originally used in theﬁeld of\ncomputer vision which has been used extensively in many other\nﬁelds. CNN is composed of a convolutional layer and a pooling\nlayer. In this study, our network includes an input layer, a 1-\ndimensional convolutional layer with 64ﬁlters, aﬂatten layer, a\ndropout layer, a dense layer with 32 nodes, and an output layer.\nThe Adam algorithm was selected as the optimizer, and the cross-\nentropy loss formula was selected as the loss function.\n2.3.6 BiLSTM\nThe long short-term memory (LSTM) network (Hochreiter and\nSchmidhuber 1997) is a variant of a recurrent neural network\n(Schuster and Paliwal 1997). By combining forward LSTM and\nbackward LSTM, a bidirectional long short-term memory\n(BiLSTM) network ( Zhang et al., 2015 ) was proposed to\nmodel the context information and effectively capture\nbidirectional semantic dependencies in natural language\nprocessing (NLP). In this study, the architecture of our\nnetwork is composed of an input layer, a BiLSTM layer with\n128 hidden units, aﬂatten layer, a dense layer with 32 nodes, a\ndropout layer, and an output layer. The Adam algorithm was\nselected as the optimizer, and the cross-entropy loss formula was\nselected as the loss function.\n2.3.7 CNN + BiLSTM\nIn addition, we also designed a network that combined CNN\nand BiLSTM. Speci ﬁcally, the network contains an input\nlayer, a 1D CNN layer with 64 ﬁlters, a BiLSM layer wit\n128 hidden units, aﬂatten layer, a dense layer of 32 nodes, a\ndropout layer, and an output layer. The Adam algorithm was\nselected as the optimizer, and the cross-entropy loss formula\nw a ss e l e c t e da st h el o s sf u n c t i o n .\n2.4 Model Evaluation Parameters\nGenerally, we used the area under the receiver operating\ncharacteristic (ROC) curve as our main metric to evaluate the\nmodels. The ROC curve can evaluate the prediction performance\nof the proposed method in the whole decision value range, and\nthe area under the curve (AUROC) is often used to quantify the\nperformance of the model. In addition, we also calculated 5 other\nmetrics which are sensitivity (SEN), speciﬁcity (SPE), precision\n(PRE), accuracy (ACC), and Matthews correlation coefﬁcient\n(MCC). The ﬁve metrics are deﬁned as follows:\nSEN /equals\nTP\nTP + FN ,\nSPE /equals TN\nTN + FP ,\nPRE /equals TP\nTP + FP ,\nACC /equals TP + TN\nTP + FP + TN + FN , and\nMCC /equals TPpTN − FPpFN/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext /radicaltpext\n(TP + FP)p(TN + FN)p(TN + FP)p(TN + FN)\n√ ,\nwhere TP (true positive) means the number of predicted Kgly\nsites are actual Kgly sites, FP (false positive) means the number of\npredicted Kgly sites are actual non-Kgly sites, TN (true negative)\nmeans the number of predicted non-Kgly sites are actual non-\nKgly sites, and FN (false negative) means the predicted non-Kgly\nsites are actual Kgly sites.\n3 RESULTS\n3.1 Sequence Discrepancy Between\nPositive and Negative Samples in the\nBenchmark Dataset\nBased on the hypothesis that the sequence patterns of positive\nsamples are different from that of the negative sample, we are\nable to develop machine learning methods to discriminate\nKgly sites from non-Kgly sites. The overall pattern discrepancy\ncould be visualized by Two Sample Logo (Vacic et al., 2006).\nThe distribution and preference of theﬂanking residues of the\ncentral lysine were analyzed.Figure 2 shows that amino acids\nG, V, M, and A are enriched in positive samples, which are all\nuncharged residues. On the contrary, the amino acids K, R, and\nE are depleted in negative samples, which are all charged\nresidues. In addition, most of the depleted amino acids E of\nthe negative samples are on the left of central lysine sites at\npositions −11, −6, −5, −4, −3, and −1. On the other hand,\nresidues R and K of the negative samples are depleted on the\nright of central lysine sites at positions +1, +2, +3, +4, and +5.\nAlthough there is a difference in the distribution and\npreference between positive and negative samples, the\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341535\nLiu et al. Kgly Prediction with BERT\noverall enriched or depleted ratio for a speci ﬁcs e q u e n t i a l\nposition is less than 7.5%. Thus, the context information\nmay be helpful to build a classiﬁcation model.\n3.2 Model Performance Based on\nEmbeddings of Different Pretrained BERT\nModels\n3.2.1 Model Performance Based on the Embedding of\nToken “CLS”\nFrom pretrained BERT models, the token“CLS” is often used for\ndownstream classiﬁcation tasks, so we extracted the embeddings of the\ntoken “CLS” of different segments to build our models. Three deep\nnetworks were used to build our models including 1D CNN, BiLSTM,\nand 1D CNN + BiLSTM. The grid search has been used to optimize\nthe hyperparameters such as batch_size, learning rate, and epochs, for\nwhich the ranges are shown inSupplementary Table S2.T h u s ,w e\nobtained the optimal models for different networks and different\nembeddings (Supplementary Tables S3, S4, S5). Table 1shows that\nthe performance of the models based on the embeddings extracted\nfrom BERT-prot is generally better than the embeddings extracted\nfrom BERT-base and TAPE according to values of AUROC. The\nROC curves can be found in the supplemental materials\n(Supplementary Figure S1). In addition, the performance of the\nmodels based on the embeddings extracted from BERT-Base is better\nthan that of TAPE. Note that BERT-Base is a pretrained natural\nlanguage model, BERT-prot is a pretrained protein language model\nbased on about 560,000 sequences, and TAPE is a pretrained protein\nlanguage model based on about 31 million sequences.\nFurthermore, based on the embeddings of BERT-Base and\nBERT-prot, Table 1 also shows that the models with 1D CNN\noutperform the models with BiLSTM and 1D CNN + BiLSTM.\nBut for the embeddings of TAPE, the model with BiLSTM\noutperforms the other two networks. Overall, with the 1D\nCNN network, the model based on the embeddings of BERT-\nprot achieved the best performance.\n3.2.2 Model Performance Based on the Embedding of\nToken “K”\nIn this work, the middle residue of all the peptide segments is K\n(lysine), so we explored if we could use the embeddings of the\nmiddle Ks to build our models. Table 2 shows that the\nperformance of the models based on the embeddings extracted\nfrom BERT-Base and BERT-prot is similar according to the\nvalues of AUROC, and the performance of the model based\non the embeddings extracted from TAPE is inferior to that of the\nother two. Moreover, based on the embeddings of BERT-Base and\nBERT-prot, Table 2shows that the models with 1D CNN again\noutperform the models with BiLSTM and 1D CNN + BiLSTM.\nBut for the embeddings of TAPE, the model with BiLSTM\noutperforms the other two networks. Overall, with the 1D\nCNN network, the model based on the embeddings of BERT-\nBase achieved the best performance.\n3.2.3 Model Performance Based on the Average\nEmbeddings of the Peptide Segment\nThe average embedding of the tokens of the whole sentence\ncan also be used for downstream classiﬁc a t i o nt a s k s .I nt h i s\nstudy, the average embedding of the 31 tokens in a peptide\nsegment was extracted to build our models.Table 3shows that\nthe whole proﬁle of the results is similar to that of the results\nshown in Table 2. The model based on the combination of\nBERT-prot and 1D CNN network achieved the best\nperformance.\nAll in all, three different types of embeddings extracted from\nthree different pretrained models were fed to three different deep\nnetworks. It turned out that the representability of the\nembeddings extracted from BERT-prot is better than that of\nBERT-base and TAPE in this study. Moreover, the model based\non 1D CNN shows the best performance.\n3.3 Model Performance Based on\nHandcrafted Feature With Machine\nLearning Algorithms\nTo demonstrate the effectiveness of the embeddings of pretrained\nmodels, we also calculated six kinds of handcrafted features\n(HCFs) which were then used to build models based on four\nmachine learning algorithms, namely, XGBoost, random forest,\nSVM, and KNN. The hyperparameters of these algorithms were\nalso optimized (Supplementary Table S6,S7). The performance\nof these models is shown inTable 4. Overall, the models based on\nFIGURE 2 |Overall sequence pattern discrepancy between positive and native samples illustrated by Two Sample Logo.\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341536\nLiu et al. Kgly Prediction with BERT\nAAC and CKSAAP were superior to the models based on the\nother four kinds of features according to the values of AUROCs.\nThe ROC curves can be found in supplemental materials\n(Supplementary Figure S2). For all six kinds of features, the\nmodels based on XGBoost show the best or near-best\nperformance. The best model was obtained by combining\nAAC and XGBoost, for which the AUROC is 0.633.\nThe best model based on HCFs was compared with the best\nmodel based on embeddings of pretrained BERT. Figure 3\nshows the best model based on embeddings of BERT-prot,\nwhich showed higher AUROC than the best model based\non HCFs.\n3.4 Comparing With Other Existing Methods\non the Independent Test Set\nAn independent test set was used to evaluate the generalization\nof our model, which is obtained from Yu et al.’sw o r k(Yu et al.,\n2019). In addition, the dataset has also been used to test other\nfour models including GlyNN, Gly-PseAAC, BPB-GlySite, and\nPredGly. Although about 11 models have been developed\n(Supplementary Table S1 ) for predicting Kgly sites, only\nthe four models mentioned previously are available and\nwork well. In addition, the model PredGly has been built\nwith features including KNN encoding whose over ﬁtting\nTABLE 1 |Cross-validation performance of models based on embeddings of token“CLS” extracted from different pretrained BERT models.\nDeep networks Sen Spe Pre MCC ACC AUROC\nBERT-Base 1D CNN 0.668 0.440 0.548 0.117 0.554 0.581\nBiLSTM 0.602 0.485 0.547 0.096 0.544 0.571\n1D CNN + BiLSTM 0.640 0.454 0.544 0.105 0.547 0.572\nBERT-prot 1D CNN 0.569 0.616 0.603 0.191 0.592 0.643\nBiLSTM 0.604 0.576 0.594 0.188 0.590 0.638\n1D CNN + BiLSTM 0.588 0.592 0.604 0.191 0.590 0.639\nBERT-TAPE 1D CNN 0.308 0.685 0.505 -0.007 0.497 0.485\nBiLSTM 0.469 0.541 0.507 0.012 0.505 0.505\n1D CNN + BiLSTM 0.409 0.588 0.498 -0.003 0.498 0.498\nBold values means the highest values of that column in the tables.\nTABLE 2 |Cross-validation performance of models based on embeddings of the central“K” extracted from different pretrained BERT models.\nDeep networks Sen Spe Pre MCC ACC AUROC\nBERT-base 1D CNN 0.698 0.464 0.580 0.185 0.581 0.634\nBiLSTM 0.587 0.596 0.600 0.188 0.591 0.626\n1D CNN + BiLSTM 0.551 0.616 0.599 0.174 0.584 0.628\nBERT-prot 1D CNN 0.610 0.580 0.593 0.191 0.595 0.632\nBiLSTM 0.583 0.575 0.586 0.164 0.579 0.618\n1D CNN + BiLSTM 0.682 0.483 0.573 0.174 0.582 0.630\nBERT-TAPE 1D CNN 0.536 0.478 0.505 0.013 0.507 0.509\nBiLSTM 0.510 0.530 0.521 0.040 0.520 0.517\n1D CNN + BiLSTM 0.473 0.549 0.512 0.022 0.511 0.514\nBold values means the highest values of that column in the tables.\nTABLE 3 |Cross-validation performance of models based on embeddings of the central“average” extracted from different pretrained BERT models.\nDeep networks Sen Spe Pre MCC ACC AUROC\nBERT-base 1D CNN 0.526 0.605 0.589 0.144 0.566 0.606\nBiLSTM 0.699 0.423 0.556 0.144 0.561 0.600\n1D CNN + BiLSTM 0.610 0.510 0.568 0.138 0.560 0.597\nBERT-prot 1D CNN 0.595 0.595 0.598 0.192 0.595 0.640\nBiLSTM 0.613 0.584 0.598 0.199 0.599 0.636\n1D CNN + BiLSTM 0.703 0.465 0.583 0.194 0.584 0.639\nBERT-TAPE 1D CNN 0.517 0.491 0.501 0.008 0.504 0.503\nBiLSTM 0.476 0.514 0.491 -0.008 0.495 0.496\n1D CNN + BiLSTM 0.369 0.633 0.540 -0.026 0.501 0.501\nBold values means the highest values of that column in the tables.\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341537\nLiu et al. Kgly Prediction with BERT\nnature has been demonstrated in Basith et al.’sw o r k(Basith\net al., 2021 ). Furthermore, the performance could not be\nrecovered when we retrained the model. Thus, we only\ncompared our model with GlyNN, Gly-PseAAC, and BPB-\nGlySite. As shown inFigure 4, the AUROC, MCC, ACC, and\nSPE of our model are 0.69, 0.23, 0.61, and 0.73, respectively,\nwhich are substantially higher than those of GlyNN, Gly-\nPseAAc, and BPB-GlySite. Our results indicate that our\nmodel is better than other existing predictors, which implies\nthat the features extracted from NLP pretrained models could\nbe useful for predicting protein posttranslational\nmodiﬁcation sites.\n3.5 Web Implementation\nFor the easy use of our model, we deployed a web server at http://\nbert-kgly.zhulab.org.cn/. The users can carry out the prediction as\nfollows:\nFirst, the input of the server can be protein sequences in\ntext or a FASTA ﬁle that contains the query protein\nsequences. Then, by clicking the “submit ” button, a unique\ntask ID would be assigned to the job. To obtain the results, the\nusers can provide their email addresses on the webpage.\nWhen the job was done, the results would be sent to the\nusers by email.\nTABLE 4 |Cross-validation performance of models based on handcrafted\nfeatures.\nHCF Classi ﬁer Sen Spe Pre MCC ACC AUROC\nAAC KNN 0.587 0.514 0.547 0.102 0.551 0.571\nRF 0.664 0.526 0.584 0.192 0.595 0.631\nSVM 0.530 0.586 0.562 0.116 0.558 0.590\nXGBoost 0.638 0.551 0.587 0.191 0.595 0.633\nDBPB KNN 0.529 0.528 0.529 0.057 0.528 0.537\nRF 0.551 0.537 0.544 0.088 0.544 0.558\nSVM 0.522 0.575 0.551 0.097 0.548 0.570\nXGBoost 0.547 0.545 0.546 0.092 0.546 0.567\nEBGW KNN 0.537 0.498 0.517 0.035 0.517 0.527\nRF 0.707 0.386 0.535 0.099 0.547 0.564\nSVM 0.565 0.535 0.549 0.100 0.550 0.569\nXGBoost 0.688 0.413 0.540 0.106 0.551 0.568\nKNN KNN 0.418 0.617 0.522 0.036 0.518 0.523\nRF 0.710 0.368 0.529 0.084 0.539 0.555\nSVM 0.566 0.510 0.537 0.077 0.538 0.555\nXGBoost 0.685 0.396 0.531 0.085 0.541 0.554\nCKSAAP KNN 0.591 0.505 0.544 0.096 0.548 0.566\nRF 0.639 0.533 0.578 0.173 0.586 0.626\nSVM 0.554 0.625 0.596 0.180 0.590 0.629\nXGBoost 0.607 0.568 0.585 0.176 0.588 0.629\nPWAA KNN 0.517 0.517 0.517 0.033 0.517 0.527\nRF 0.483 0.629 0.565 0.113 0.556 0.584\nSVM 0.213 0.803 0.210 0.020 0.508 0.504\nXGBoost 0.534 0.603 0.574 0.138 0.569 0.593\nBold values means the highest values of that column in the tables.\nFIGURE 3 |Comparison of performance between the best model based on HCFs and the embeddings extracted from pretrained BERT model.\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341538\nLiu et al. Kgly Prediction with BERT\n4 DISCUSSIONS\nIn this study, the embeddings of three different pretrained BERT\nmodels were extracted to build our models. Our results indicated\nthat the embeddings obtained from BERT-prot which is based on\n556,603 protein sequences are more effective than the\nembeddings extracted from the other two BERT models,\nalthough the other two BERT models were pretrained on\nlarger datasets. Generally speaking, the model parameters and\nthe size of the dataset for pretraining are positively related to the\nrepresentability of the embeddings. Another factor, the domain-\nspeciﬁc data have also been reported to be proportionally related\nto the representability. In our study, one possible reason is that\nthe dataset obtained from UniProt (Swiss-Prot) may be more\nspeciﬁc than the dataset obtained from Pfam because the data\nfrom Swiss-Prot are from manually curated protein sequences.\nTo inspect the effectiveness of our 1D-CNN network, we\ncompared the features extractedfrom BERT-prot and the features\ntransformed by the 1D-CNN network. We used t-SNE to project the\nfeatures into the two-dimensional space (Figure 5). For the features\nextracted from BERT-prot for token“CLS,” although there are some\nclusters for positive or negative samples, overall, the positive and\nnegative samples are tangled together (Figure 5A). However, for the\nfeatures transformed by 1D CNN,Figure 5Bshows that negative\nsamples (green points) are concentrated at the bottom left, while\npositive samples (blue points) are concentrated at the top right. Thus,\nwe demonstrated that the informative feature representation from\ninput sequences can be learned by the pretrained BERT model and\nthe downstream 1D CNN network.\nConsidering the information of all residues of the protein\nsegments, we have built the model based on the embeddings of all\nresidues of the whole segment, and the corresponding cross-\nvalidation AUROC is 0.646, which is similar to the model based\non the embedding of“CLS” (0.643). Additional results showed\nthat the model based on the embeddings of all residues of the\nwhole sequences had worse generalization on the independent\ntest set with an AUROC of 0.624, which is smaller than that of our\nmodel based on the embeddings of“CLS.”\nTo investigate the complementarity between BERT embeddings\nand HCFs, we combined the embeddings of BERT with the AAC\nfeature, which is the best handcrafted feature in this study. The two\nfeatures were concatenated as the input of our deep networks. The\ncross-validation results showed that the corresponding AUROC is\n0.6427, which is similar to the highest value (0.643) based only on the\nembeddings of BERT.\nOur model was built and evaluated on balanced datasets;\nhowever, the reality is that the negative samples are more than\npositive samples. So, we have constructed an imbalanced\nindependent test to evaluate our model, which contains 200\npositive samples and 1,000 negative samples. We used our\nmodel to do prediction on the imbalanced test set and obtained\nan AUROC of 0.64 for the imbalanced test set. The imbalanced\nFIGURE 4 |Performance of our model and other existing models based on independent test set.\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 8341539\nLiu et al. Kgly Prediction with BERT\ndataset was also tested on other models. Based on the predictive\nresults of Gly-PseAAC and BPB-Glysite, the AUROCs for the two\nmodels were calculated which are 0.53 and 0.51, respectively. The\nresults indicated that our model was superior to the two models on\nthe imbalanced test set. Note that the results predicted by the web\nserver of GlyNN could not be displayed normally.\nMany studies (Bao et al., 2019a; Bao et al., 2019b; Bao et al.\n2021) have been conducted to predict the modiﬁcations of lysines.\nOur results indicated that the embeddings extracted from BERT\ncould be effective features for building the models.\n5 SUMMARY\nIn this study, we developed a new method, BERT-Kgly, to predict Kgly\nsites of proteins by extracting features from a pretrained protein\nlanguage BERT model. Recently, NLP pretrained models have been\ntransferred to analyze and tackle sequence information of biological\nmacromolecules. Different pretrained protein language BERT models\nhave been built based on different sizes of protein sequences. We\nadopted two protein language BERT models and one natural language\nBERT model to extract features from peptide segments. Our results\ndemonstrate the features extracted from BERT-prot are more\ninformative than the other two BERT models. Three different\ndownstream deep networks wereused to build our models; it\nturned out that the model based on 1D CNN was superior to the\nmodels based on other two networks. Our model was compared with\nthe models built on HCFs and traditional machine learning\nalgorithms, which indicated that our BERT-Kgly model\noutperformed these models. Thus, we demonstrate the\neffectiveness of features extracted from the pretrained protein\nBERT model and the downstream deep learning networks. In\ncomparison to the independent test set, we also showed that our\nmodel was superior to other state-of-the-art models.\nDATA AVAILABILITY STATEMENT\nPublicly available datasets were analyzed in this study. These data can\nbe found here: https://github.com/yinboliu-git/Gly-ML-BERT-DL.\nAUTHOR CONTRIBUTIONS\nX Za n dS Bc o n c e i v e dt h es t u d y .X Za n dY i Ld e s i g n e dt h ee x p e r i m e n t s .\nYiL and YuL performed the experiments. YiL, YuL, GW, and YC\nanalyzed the data. YiL and XZ wrote the article. All authors have read\nand agreed to the published version of the manuscript.\nFUNDING\nThis work was supported in part by the National Natural Science\nFoundation of China (Grant number: 21403002).\nSUPPLEMENTARY MATERIAL\nThe Supplementary Material for this article can be found online at:\nhttps://www.frontiersin.org/arti cles/10.3389/fbinf.2022.834153/\nfull#supplementary-material\nFIGURE 5 |t-SNE illustration of the embeddings extracted from BERT and the features transformed by 1D CNN.(A) Embeddings extracted from BERT-prot.(B)\nFeatures transformed by 1D CNN.\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 83415310\nLiu et al. Kgly Prediction with BERT\nREFERENCES\nAhmed, N., Babaei-Jadidi, R., Howell, S. K., Beisswenger, P. J., and Thornalley, P. J.\n(2005). Degradation Products of Proteins Damaged by Glycation, Oxidation\nand Nitration in Clinical Type 1 Diabetes.Diabetologia. 48, 1590– 1603. doi:10.\n1007/s00125-005-1810-7\nBao, W., Yang, B., Bao, R., and Chen, Y. (2019a). LipoFNT: Lipoylation Sites\nIdentiﬁcation with Flexible Neural Tree.Complexity. 2019, 1– 9. doi:10.1155/\n2019/1603867\nBao, W., Yang, B., Huang, D.-S., Wang, D., Liu, Q., Chen, Y.-H., et al. (2019b).\nIMKPse: Identiﬁcation of Protein Malonylation Sites by the Key Features into\nGeneral PseAAC. IEEE Access. 7, 54073 – 54083. doi:10.1109/access.2019.\n2900275\nBao, W., Yang, B., and Chen, B. (2021). 2-hydr_Ensemble: Lysine 2-\nhydroxyisobutyrylation Identiﬁcation with Ensemble Method.Chemometrics\nIntell. Lab. Syst.215, 104351. doi:10.1016/j.chemolab.2021.104351\nBasith, S., Lee, G., and Manavalan, B. (2021). STALLION: a Stacking-Based\nEnsemble Learning Framework for Prokaryotic Lysine Acetylation Site\nPrediction. Brief Bioinform. 23 (1), doi:10.1093/bib/bbab376\nBreiman, L. (2001). Random Forests . The Netherlands: Kluwer Academic\nPublishers.\nChen, K., Wei, Z., Zhang, Q., Wu, X., Rong, R., Lu, Z., et al. (2019). WHISTLE: a\nHigh-Accuracy Map of the Human N6-Methyladenosine (m6A)\nEpitranscriptome Predicted Using a Machine Learning Approach. Nucleic\nAcids Res. 47, 47e41. Epub 2019/04/18. doi:10.1093/nar/gkz074\nChen, Z., Liu, X., Li, F., Li, C., Marquez-Lago, T., Leier, A., et al. (2019). Large-scale\nComparative Assessment of Computational Predictors for Lysine post-\ntranslational Modi ﬁcation Sites. Brief Bioinform. 20, 2267 – 2290. doi:10.\n1093/bib/bby089\nChen, T., and Guestrin, C. (2016).“XGBoost: A Scalable Tree Boosting System,” in\nProceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining.\nChen, X., Xiong, Y., Liu, Y., Chen, Y., Bi, S., and Zhu, X. (2020). m5CPred-SVM: a\nNovel Method for Predicting m5C Sites of RNA.BMC Bioinformatics. 21,\n21489. Epub 2020/11/01. doi:10.1186/s12859-020-03828-4\nChen, Y. Z., Tang, Y. R., Sheng, Z. Y., and Zhang, Z. (2008). Prediction of Mucin-\ntype O-Glycosylation Sites in Mammalian Proteins Using the Composition of\nK-Spaced Amino Acid Pairs.BMC Bioinformatics. 9, 101. doi:10.1186/1471-\n2105-9-101\nChen, Y. Z., Wang, Z. Z., Wang, Y., Ying, G., Chen, Z., and Song, J. (2021). nhKcr: a\nNew Bioinformatics Tool for Predicting Crotonylation Sites on Human\nNonhistone Proteins Based on Deep Learning.Brief. Bioinform. 5, 22. Epub\n2021/05/19. doi:10.1093/bib/bbab146\nCover, T., and Hart, P. (1967). Nearest Neighbor Pattern Classiﬁcation. IEEE\nTrans. Inform. Theor.13, 21– 27. doi:10.1109/tit.1967.1053964\nDevlin, J., Chang, M. W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding.Hum. Lang.\nTech. 1, 4171– 4186. doi:10.18653/v1/N19-1423\nFriedman, J. H. (2001). Greedy Function Approximation: A Gradient Boosting\nMachine. Ann. Stat. Oct29, 1189– 1232. doi:10.1214/aos/1013203451\nF u ,H . ,Y a n g ,Y . ,W a n g ,X . ,W a n g ,H . ,a n dX u ,Y .( 2 0 1 9 ) .D e e p U b i :aD e e p\nLearning Framework for Prediction of Ubiquitination Sites in Proteins.\nBMC Bioinformatics.\n20, 2086. Epub 2019/02/20. doi:10.1186/s12859-\n019-2677-9\nHenikoff, S., and Henikoff, J. G. (1992). Amino Acid Substitution Matrices from\nProtein Blocks.Proc. Natl. Acad. Sci. U S A.89, 10915– 10919. doi:10.1073/pnas.\n89.22.10915\nHochreiter, S., and Schmidhuber, J. (1997). Long Short-Term Memory.Neural\nComput. 9, 1735– 1780. doi:10.1162/neco.1997.9.8.1735\nHornbeck, P. V., Zhang, B., Murray, B., Kornhauser, J. M., Latham, V., and\nSkrzypek, E. (2015). PhosphoSitePlus, 2014: Mutations, PTMs and\nRecalibrations. Nucleic Acids Res. 43, D512– D520. Epub 2014/12/18. doi:10.\n1093/nar/gku1267\nHuang, Y., Niu, B., Gao, Y., Fu, L., and Li, W. (2010). CD-HIT Suite: a Web Server\nfor Clustering and Comparing Biological Sequences. Bioinformatics. 26,\n680– 682. doi:10.1093/bioinformatics/btq003\nIslam, M. M., Saha, S., Rahman, M. M., Shatabda, S., Farid, D. M., and Dehzangi, A.\n(2018). iProtGly-SS: Identifying Protein Glycation Sites Using Sequence and\nStructure Based Features.Proteins. 86, 777– 789. doi:10.1002/prot.25511\nJohansen, M. B., Kiemer, L., and Brunak, S. (2006). Analysis and Prediction of\nMammalian Protein Glycation.Glycobiology. 16, 844– 853. doi:10.1093/glycob/\ncwl009\nJu, Z., Sun, J., Li, Y., and Wang, L. (2017). Predicting Lysine Glycation Sites Using\nBi-proﬁle Bayes Feature Extraction.Comput. Biol. Chem.71, 98– 103. doi:10.\n1016/j.compbiolchem.2017.10.004\nKhan, Z. A., and Park, S. (2020). An Electrochemical Chip to MonitorIn Vitro\nGlycation of Proteins and Screening of Antiglycation Potential of Drugs.\nPharmaceutics. 12, 12. doi:10.3390/pharmaceutics12111011\nKhanum, S., Ashraf, M. A., Karim, A., Shoaib, B., and Alswaitti, M. (2020). Gly-\nLysPred: Identiﬁcation of Lysine Glycation Sites in Protein Using Position\nRelative Features and Statistical Moments via Chou’s 5 Step Rule.Computers\nmaterials and Continua66. doi:10.32604/cmc.2020.013646\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet Classiﬁcation with\nDeep Convolutional Neural Networks. Adv. Neural Inf. Process. Syst. 25,\n1097– 1105. doi:10.1145/3065386\nLe, N. Q. K., Ho, Q. T., Nguyen, T. T., and Ou, Y. Y. (2021). A Transformer\nArchitecture Based on BERT and 2D Convolutional Neural Network to Identify\nDNA Enhancers from Sequence Information.Brief Bioinform 22 (5), Epub\n2021/02/05. doi:10.1093/bib/bbab005\nLing, X., Sakashita, N., Takeya, M., Nagai, R., Horiuchi, S., and Takahashi, K.\n(1998). Immunohistochemical Distribution and Subcellular Localization of\nThree Distinct Speciﬁc Molecular Structures of Advanced Glycation End\nProducts in Human Tissues.Lab. Invest. 78, 1591– 1606.\nLiu, Y., Gu, W., Zhang, W., and Wang, J. (2015). Predict and Analyze Protein\nGlycation Sites with the mRMR and IFS Methods.Biomed. Res. Int. 2015,\n561547. doi:10.1155/2015/561547\nLiu, Z., Wang, Y., Gao, T., Pan, Z., Cheng, H., Yang, Q., et al. (2014). CPLM: a\nDatabase of Protein Lysine Modiﬁcations. Nucleic Acids Res.42, D531\n– D536.\ndoi:10.1093/nar/gkt1093\nL v ,H . ,D a o ,F .Y . ,G u a n ,Z .X . ,Y a n g ,H . ,L i ,Y .W . ,a n dL i n ,H .( 2 0 2 0 ) .D e e p -\nKcr: Accurate Detection of Lysine Crotonylation Sites Using Deep\nLearning Method. Brief Bioinform . Epub 2020/10/26. doi:10.1093/bib/\nbbaa255\nQiao, Y., Zhu, X., and Gong, H. (2021). BERT-kcr: Prediction of Lysine\nCrotonylation Sites by a Transfer Learning Method with Pre-trained BERT\nModels. Bioinformatics 38 (3), 648 – 654. Epub 2021/10/14. doi:10.1093/\nbioinformatics/btab712\nRabbani, N., Al-Motawa, M., and Thornalley, P. J. (2020). Protein Glycation in\nPlants-An Under-Researched Field with Much Still to Discover.Int. J. Mol. Sci.\n21, 21. Epub 2020/06/04. doi:10.3390/ijms21113942\nRao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, X., Canny, J., et al. (2019).\nEvaluating Protein Transfer Learning with TAPE.Adv. Neural Inf. Process. Syst.\n32, 9689– 9701.\nReddy, H. M., Sharma, A., Dehzangi, A., Shigemizu, D., Chandra, A. A., and\nTsunoda, T. (2019). GlyStruct: Glycation Prediction Using Structural Properties\nof Amino Acid Residues.BMC Bioinformatics 19, 19547. Epub 2019/02/06.\ndoi:10.1186/s12859-018-2547-x\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., et al. (2021). Biological\nStructure and Function Emerge from Scaling Unsupervised Learning to 250\nMillion Protein Sequences.Proc. Natl. Acad. Sci. U S A.118, 118. Epub 2021/04/\n21. doi:10.1073/pnas.2016239118\nSchuster, M., and Paliwal, K. K. (1997). Bidirectional Recurrent Neural Networks.\nIEEE Trans. Signal. Process.45, 2673– 2681. doi:10.1109/78.650093\nShao, J., Xu, D., Tsai, S. N., Wang, Y., and Ngai, S. M. (2009). Computational\nIdentiﬁcation of Protein Methylation Sites through Bi-proﬁle Bayes Feature\nExtraction. PLoS One. 4, e4920. doi:10.1371/journal.pone.0004920\nShi, S. P., Qiu, J. D., Sun, X. Y., Suo, S. B., Huang, S. Y., and Liang, R. P. (2012).\nPLMLA: Prediction of Lysine Methylation and Lysine Acetylation by\nCombining Multiple Features. Mol. Biosyst. 8, 1520 – 1527. doi:10.1039/\nc2mb05502c\nStitt, A. W. (2001). Advanced Glycation: an Important Pathological Event in\nDiabetic and Age Related Ocular Disease. Br. J. Ophthalmol. 85, 746– 753.\ndoi:10.1136/bjo.85.6.746\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 83415311\nLiu et al. Kgly Prediction with BERT\nThornalley, P. J., Battah, S., Ahmed, N., Karachalias, N., Agalou, S., Babaei-Jadidi,\nR., et al. (2003). Quantitative Screening of Advanced Glycation Endproducts in\nCellular and Extracellular Proteins by Tandem Mass Spectrometry.Biochem. J.\n375, 581– 592. doi:10.1042/BJ20030763\nVacic, V., Iakoucheva, L. M., and Radivojac, P. (2006). Two Sample Logo: a\nGraphical Representation of the Differences between Two Sets of Sequence\nAlignments. Bioinformatics. 22, 1536– 1537. Epub 2006/04/25. doi:10.1093/\nbioinformatics/btl151\nVapnik, V. N. (1995).The Nature of Statistical Learning Theory. Berlin: Springer.\nVlassara, H., Bucala, R., and Striker, L. (1994). Pathogenic Effects of Advanced\nGlycosylation: Biochemical, Biologic, and Clinical Implications for Diabetes\nand Aging. Lab. Invest. 70, 138– 151.\nWu, M., Yang, Y., Wang, H., and Xu, Y. (2019). A Deep Learning Method to More\nAccurately Recall Known Lysine Acetylation Sites.BMC Bioinformatics.20, 49.\nEpub 2019/01/25. doi:10.1186/s12859-019-2632-9\nXu, H., Zhou, J., Lin, S., Deng, W., Zhang, Y., and Xue, Y. (2017). PLMD: An\nUpdated Data Resource of Protein Lysine Modiﬁcations. J. Genet. Genomics.44,\n243– 250. doi:10.1016/j.jgg.2017.03.007\nXu, Y., Li, L., Ding, J., Wu, L. Y., Mai, G., and Zhou, F. (2017). Gly-PseAAC:\nIdentifying Protein Lysine Glycation through Sequences.Gene. 602, 1– 7. doi:10.\n1016/j.gene.2016.11.021\nXu, Y., Ding, Y. X., Ding, J., Wu, L. Y., and Xue, Y. (2016). Mal-Lys: Prediction of\nLysine Malonylation Sites in Proteins Integrated Sequence-Based Features with\nmRMR Feature Selection.Sci. Rep.6 (6), 38318. Epub 2016/12/03. doi:10.1038/\nsrep38318\nYang, Y., Wang, H., Li, W., Wang, X., Wei, S., Liu, Y., et al. (2021). Prediction and\nAnalysis of Multiple Protein Lysine Modiﬁed Sites Based on Conditional\nWasserstein Generative Adversarial Networks.BMC Bioinformatics Mar.31,\n22. doi:10.1186/s12859-021-04101-y\nYao, Y., Zhao, X., Ning, Q., and Zhou, J. (2021). ABC-gly: Identifying Protein\nLysine Glycation Sites with Artiﬁcial Bee Colony Algorithm.Cp. 18, 18– 26.\ndoi:10.2174/1570164617666191227120136\nYu, J., Shi, S., Zhang, F., Chen, G., and Cao, M. (2019). PredGly: Predicting Lysine\nGlycation Sites for Homo sapiens Based on XGboost Feature Optimization.\nBioinformatics 35, 2749– 2756. doi:10.1093/bioinformatics/bty1043\nZhang, S., Zheng, D., Hu, X., and Yang, M. (2015). Bidirectional Long Short-Term\nMemory Networks for Relation Classiﬁcation. Proceedings of the 29th Paciﬁc\nAsia Conference on Language, Information and Computation.1 ,7 3– 78.\nZhang, Y., Lin, J., Zhao, L., Zeng, X., and Liu, X. (2021). A Novel Antibacterial\nPeptide Recognition Algorithm Based on BERT.Brief. Bioinform.5, 22. Epub\n2021/05/27. doi:10.1093/bib/bbab200\nZ h a n g ,Y . ,X i e ,R . ,W a n g ,J . ,L e i e r ,A . ,M a r q u e z - L a g o ,T .T . ,A k u t s u ,T . ,e ta l .( 2 0 1 9 ) .\nComputational Analysis and Prediction of Lysine Malonylation Sites by Exploiting\nInformative Features in an Integrative Machine-Learning Framework. Brief\nBioinform.20 (20), 2185– 2199. Epub 2018/10/24. doi:10.1093/bib/bby079\nZhao, X., Zhao, X., Bao, L., Zhang, Y., Dai, J., and Yin, M. (2017). Glypre: In Silico\nPrediction of Protein Glycation Sites by Fusing Multiple Features and Support\nVector Machine. Molecules.\n22, 15. doi:10.3390/molecules22111891\nZhu, X., He, J., Zhao, S., Tao, W., Xiong, Y., and Bi, S. (2019). A Comprehensive\nComparison and Analysis of Computational Predictors for RNA N6-\nMethyladenosine Sites of Saccharomyces cerevisiae. Brief. Funct. Genomics\n18 (18), 367– 376. Epub 2019/10/15. doi:10.1093/bfgp/elz018\nConﬂict of Interest:The authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be construed as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors, and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2022 Liu, Liu, Wang, Cheng, Bi and Zhu. This is an open-access article\ndistributed under the terms of the Creative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other forums is permitted, provided the\noriginal author(s) and the copyright owner(s) are credited and that the original\npublication in this journal is cited, in accordance with accepted academic practice.\nNo use, distribution or reproduction is permitted which does not comply with\nthese terms.\nFrontiers in Bioinformatics | www.frontiersin.org February 2022 | Volume 2 | Article 83415312\nLiu et al. Kgly Prediction with BERT",
  "topic": "Glycation",
  "concepts": [
    {
      "name": "Glycation",
      "score": 0.8199270963668823
    },
    {
      "name": "Lysine",
      "score": 0.6639187932014465
    },
    {
      "name": "Computer science",
      "score": 0.6151303052902222
    },
    {
      "name": "Encoder",
      "score": 0.4857642948627472
    },
    {
      "name": "Transformer",
      "score": 0.4748387932777405
    },
    {
      "name": "Computational biology",
      "score": 0.46955257654190063
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4680722951889038
    },
    {
      "name": "UniProt",
      "score": 0.46345019340515137
    },
    {
      "name": "Embedding",
      "score": 0.46046966314315796
    },
    {
      "name": "Machine learning",
      "score": 0.36495304107666016
    },
    {
      "name": "Chemistry",
      "score": 0.3227235674858093
    },
    {
      "name": "Biochemistry",
      "score": 0.25811174511909485
    },
    {
      "name": "Biology",
      "score": 0.23021897673606873
    },
    {
      "name": "Amino acid",
      "score": 0.19500970840454102
    },
    {
      "name": "Engineering",
      "score": 0.07745766639709473
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Receptor",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I140221134",
      "name": "Anhui Agricultural University",
      "country": "CN"
    }
  ],
  "cited_by": 23
}