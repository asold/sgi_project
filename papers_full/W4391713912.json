{
  "title": "The current state of artificial intelligence generative language models is more creative than humans on divergent thinking tasks",
  "url": "https://openalex.org/W4391713912",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Kent F. Hubert",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A4320887767",
      "name": "Kim N. Awa",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A1683421101",
      "name": "Darya L. Zabelina",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": null,
      "name": "Kent F. Hubert",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A4320887767",
      "name": "Kim N. Awa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1683421101",
      "name": "Darya L. Zabelina",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4412463333",
    "https://openalex.org/W4383617221",
    "https://openalex.org/W2038443646",
    "https://openalex.org/W2019602329",
    "https://openalex.org/W2030992761",
    "https://openalex.org/W4386170829",
    "https://openalex.org/W1985871962",
    "https://openalex.org/W3194730353",
    "https://openalex.org/W2747680751",
    "https://openalex.org/W2999089077",
    "https://openalex.org/W2113096445",
    "https://openalex.org/W3044750169",
    "https://openalex.org/W4320886395",
    "https://openalex.org/W4310398761",
    "https://openalex.org/W2143283816",
    "https://openalex.org/W4383058345",
    "https://openalex.org/W4286249763",
    "https://openalex.org/W3142878626",
    "https://openalex.org/W4224979739",
    "https://openalex.org/W2683902778",
    "https://openalex.org/W4323835264",
    "https://openalex.org/W4379015771",
    "https://openalex.org/W4328052330",
    "https://openalex.org/W4205941964",
    "https://openalex.org/W3044960932",
    "https://openalex.org/W6609429226",
    "https://openalex.org/W4360103907",
    "https://openalex.org/W4386740961",
    "https://openalex.org/W4386052615",
    "https://openalex.org/W3166509103",
    "https://openalex.org/W4386791285",
    "https://openalex.org/W4361298663",
    "https://openalex.org/W2105378264",
    "https://openalex.org/W4200130072",
    "https://openalex.org/W2902569624",
    "https://openalex.org/W4288704790",
    "https://openalex.org/W2134655035",
    "https://openalex.org/W2419575990",
    "https://openalex.org/W4207050572",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4385580856",
    "https://openalex.org/W2171960331",
    "https://openalex.org/W2006850907",
    "https://openalex.org/W2086438570",
    "https://openalex.org/W4412162123",
    "https://openalex.org/W2166269878",
    "https://openalex.org/W2516394738",
    "https://openalex.org/W2467285316",
    "https://openalex.org/W2100291730",
    "https://openalex.org/W4387453425",
    "https://openalex.org/W3179455893",
    "https://openalex.org/W2979392537",
    "https://openalex.org/W2318160544"
  ],
  "abstract": "Abstract The emergence of publicly accessible artificial intelligence (AI) large language models such as ChatGPT has given rise to global conversations on the implications of AI capabilities. Emergent research on AI has challenged the assumption that creative potential is a uniquely human trait thus, there seems to be a disconnect between human perception versus what AI is objectively capable of creating. Here, we aimed to assess the creative potential of humans in comparison to AI. In the present study, human participants (N = 151) and GPT-4 provided responses for the Alternative Uses Task, Consequences Task, and Divergent Associations Task. We found that AI was robustly more creative along each divergent thinking measurement in comparison to the human counterparts. Specifically, when controlling for fluency of responses, AI was more original and elaborate. The present findings suggest that the current state of AI language models demonstrate higher creative potential than human respondents.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports\nThe current state of artificial \nintelligence generative language \nmodels is more creative \nthan humans on divergent thinking \ntasks\nKent F. Hubert 1,2*, Kim N. Awa 1,2 & Darya L. Zabelina 1\nThe emergence of publicly accessible artificial intelligence (AI) large language models such as ChatGPT \nhas given rise to global conversations on the implications of AI capabilities. Emergent research on AI \nhas challenged the assumption that creative potential is a uniquely human trait thus, there seems to \nbe a disconnect between human perception versus what AI is objectively capable of creating. Here, \nwe aimed to assess the creative potential of humans in comparison to AI. In the present study, human \nparticipants (N = 151) and GPT-4 provided responses for the Alternative Uses Task, Consequences Task, \nand Divergent Associations Task. We found that AI was robustly more creative along each divergent \nthinking measurement in comparison to the human counterparts. Specifically, when controlling for \nfluency of responses, AI was more original and elaborate. The present findings suggest that the current \nstate of AI language models demonstrate higher creative potential than human respondents.\nThe emergence of ChatGPT—a natural language processing (NLP) model developed by  OpenAI1 to the general \npublic has garnered global conversation on the utility of artificial intelligence (AI). OpenAI’s Generative Pre-\ntrained Transformer (GPT) is a type of machine learning that specializes in pattern recognition and prediction \nand has been further trained using Reinforcement Learning from Human Feedback (RLHF) so that ChatGPT \nresponses would be indistinguishable from human responses. Recently,  OpenAI1 has advertised the new model \n(GPT-4) as “more creative” particularly “on creative and technical writing tasks” in comparison to previous \nversions, although there are arguably semantic limitations such as nonsensical answers or the possibilities of \nincorrect information  generation2. Given the accessibility of AI models in the current climate, research across \na variety of domains has started to emerge, thus contributing to our growing understanding of the possibilities \nand potential limitations of AI.\nCreativity as a phenomenological construct is not immune to the effects of AI. For example, researchers have \nbegun to assess AI models to determine appropriate design  solutions3 and logical  reasoning4. These assessments \nfocus on convergent thinking, i.e., determining one optimal solution to a pre-defined  problem5. Traditionally, \nconvergent thinking assumes an optimal single solution path and can be assessed through traditional intelligence \nmeasures or synthesis tasks. Although convergent thinking emphasizes single optimal solutions, this does not \nnegate the potential for original or non-obvious solutions. However, convergent thinking tasks by design typically \ndo not allow for flexible or out-of-the-box thinking. In contrast, divergent thinking involves generating multiple \ncreative solutions to a problem which allows for the flexibility to determine multiple creative  solutions6. Creativity \nresearchers commonly focus on divergent creativity (in comparison to convergent creativity), given the associa-\ntive mechanisms that allude to people’s ability to generate creative solutions (i.e., creative potential). Specifically, \ndivergent thinking is considered an indicator of a person’s creative potential, but this does not guarantee creative \n achievement7. Instead, creative potential can be indicative on future capability, rather than an immediate trait \nthat determines if someone is creative. Accordingly, a person’s creative potential has been captured via divergent \nthinking tasks such as the Alternative Uses Task [AUT 6,7] or the Consequences Task  [CT8,9]. Divergent thinking \ntasks can be evaluated along three dimensions: fluency (number of responses), originality (response novelty), and \nOPEN\n1Department of Psychological Sciences, University of Arkansas, Fayetteville, AR 72701, USA.  2These authors \ncontributed equally: Kent F. Hubert and Kim N. Awa. *email: khubert@uark.edu\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\nelaboration (length/detail of response). Responses in each category are given scores (i.e., according to each task) \nand used to assess individual differences in divergent creativity, or in other words, a person’s creative potential.\nGiven the emergence of OpenAI’s GPT-4 as a large language model, research has begun to empirically assess \nthe creative potential of artificial intelligence language models through divergent thinking tasks. On one hand, \nsome researchers argue that the human cognitive mechanisms present during creative tasks are not present in \nAI, and thus the creative potential of artificial intelligence can only reflect artificial  creativity10. On the other \nhand, computational creativity suggests parallel networks that reflect the mechanisms of how humans go through \niterative, deliberative, and generative creative processes which aid in the ability to determine creative  solutions11. \nAlthough these aspects have been shown to aid in creative solutions, humans can experience idea fixedness, which \ncan act as a roadblock to other creative solutions. Machines, however, will not experience this phenomenon in \na metacognitive way due to computationally trained models that streamline a machine’s direct responses to a \n prompt12–14. Instead, a machine’s fixedness may perhaps reflect the training data of the model which could be \nargued is a computational consideration, rather than a creative one.\nFurthermore, computational researchers have posed increasing debate on the creative capabilities of artificial \nintelligence  models15 by asking questions such as: How are machines capable of determining what is creative? \nAt present, AI’s inability to explicitly determine why or if something is creative is then compensated through \nhuman-assistance. For example, human intervention is necessary for inputting appropriate and relevant data \nto train the model and shape outputs to become more linguistically  natural16,17. This computational limitation \nsuggests that AI is not capable of divergent creativity due to the lack of metacognitive processes (i.e., evaluation, \ntask motivation) because AI could not generate creative ideas or reiterate on existing ideas without the interven-\ntion (i.e., input) of a human  user10. Similarly, emotions have been seen as an integral part of creativity such that \nemotions help dictate states of flow or mind-wandering that aid in creative  processes18. However, AI may not \nnecessarily need to rely on metacognitive or affective processes to generate novel  ideas19 due to the computational \nframework. Thus, inner processes that contribute to human creativity may be a philosophical argument within \nartificial creativity  models20.\nAs briefly reviewed, the creative capabilities of artificial intelligence, thus far, have scientifically and philo-\nsophically varied [e.g.,10,20]. Researchers posit humanistic and computational considerations of the creative poten-\ntial of AI, however, the accessibility of tools to artificially generate products or ideas have given researchers the \nopportunity to evaluate public perception. For instance, people think more highly of generated artworks if they \nwere told the artworks were created by humans but not  AI21,22. The expectancy that AI generated products or ideas \nare less creative or hold less aesthetic value than human-created artworks appear to depend on implicit anti-AI \n biases22–24, as AI has been found to be indistinguishable from human-created  products25–27. People’s inability \nto distinguish between human and AI-created products supports the feasibility of AI having creative potential.\nIndeed, AI has been found to generate novel connections in  music28,  science26,  medicine29, and visual  art30 to \nname a few. In assessments of divergent thinking, humans outperformed AI on the Alternative Uses  Task31, but \nit is noteworthy that the authors propose a possible rise in AI capabilities given future progress of large language \nmodels. In fact, recent studies have found that AI divergent creativity matched that of humans using a later ver-\nsion of GPT-432,33. Researchers have continued to demonstrate that the current state of LLM’s frequently score \nwithin the top 1% of human responses on standard divergent thinking tasks such as the Alternative Uses  Task32–34. \nAdditional studies utilizing other divergent thinking tasks have also reported findings that paint a more nuanced \npicture. For example, when scores were compared between humans and GPT-4 on a Divergent Associations \nTask  (DAT35), the researcher found that GPT-4 was more creative than human  counterparts36. Recent research \non OpenAI’s text-to-image platform DALL▪E has reported similar  findings37 and suggests that OpenAI models \ncould match or even outperform humans in combinational creativity tasks. Given the research on AI creativ-\nity thus far, OpenAI’s advertorial claims that GPT-4 is “more creative” may hold more merit than anticipated.\nCurrent research\nThus far, the novelty of OpenAI’s ChatGPT has posed more questions that have yet to be examined. Although \ncreativity has considered to be a uniquely human  trait38, the emergence of OpenAI’s generative models suggests a \npossible shift in how people may approach tasks that require “out of the box” thinking. Thus, the current research \naims to examine how divergent creativity (i.e., fluency, originality, elaboration) may differ between humans \nand AI on verbal divergent thinking tasks. To our knowledge, this is one of the first studies to comprehensively \nexamine the verbal responses across a battery of the most common divergent thinking tasks (i.e., Alternative Uses \nTask, Consequences Task, and Divergent Associations Task) with novel methodology by matching the fluency of \nideas between human subjects and ChatGPT. We anticipate that AI may demonstrate higher creative potential \nin comparison to humans, though given the recency of AI-centered creativity research, our primary research \nquestions serve as exploratory in nature.\nMethods\nParticipants\nHuman participation\nHuman participants (N = 151) were recruited via Prolific online data collection platform in exchange for mon-\netary compensation of $8.00. Participants were limited to having a reported approval rating above 97%, were \nproficient English speakers, and were born/resided in the USA. Average total response time for completing the \nsurvey was 34.66 min. A statistical sensitivity analysis indicated that we had sufficient power to detect small effects \nwith the present sample size (f 2 = 0.06, 1 − β = 0.80). The present study was performed in accordance with the \nDeclaration of Helsinki and was approved by the Institutional Review Board for Human Subjects Research at the \n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\nUniversity of Arkansas. All participants provided informed consent prior to the start of the study. All statistical \nanalyses were conducted in R  studio39. See Table 1 for participant demographics.\nAI participation\nArtificial participants were operationalized as ChatGPT’s instancing feature. Each ChatGPT session was con-\nsidered an independent interaction between the user and GPT interface. Here, we prompted separate instances \nper creativity measure (as detailed below) which resulted in artificial participation sessions. For example, we \nused a single session instance to feed each prompt and aggregated each prompt response into a data file. In total, \nwe collected 151 instances which represent AI’s participation for a balanced sample. For two of the creativity \nmeasures (Alternative Uses Task and Consequences Task), which are the only timed tasks, fluency was matched \n1:1 such that the number of responses for both groups is equal on these timed tasks. Fluency scores of each \nhuman respondent were first calculated to match 1:1 for each GPT-4 instance for the Alternative Uses Task and \nConsequences Task (detailed below). Only valid responses were retained. For example, human participant #52 \nhad a total fluency score of 6, thus GPT-4 instance #52 was instructed to provide 6 responses.\nCreativity measures\nAlternative uses task\nThe Alternate Uses Task (AUT 6) was used to test divergent thinking. In this task, participants were presented with \na common object (‘fork’ and ‘rope’) and were asked to generate as many creative uses as possible for these objects. \nResponses were scored for fluency (i.e., number of responses), originality (i.e., uniqueness of responses), and \nelaboration (i.e., number of words per valid response). Participants were given 3 min to generate their responses \nfor each item. Following prior  research40, instructions for human respondents on the AUT were:\nFor this task, you’ll be asked to come up with as many original and creative uses for [item] as you can. The goal \nis to come up with creative ideas, which are ideas that strike people as clever, unusual, interesting, uncommon, \nhumorous, innovative, or different.\nYour ideas don’t have to be practical or realistic; they can be silly or strange, even, so long as they are CREA-\nTIVE uses rather than ordinary uses.\nYou can enter as many ideas as you like. The task will take 3 minutes. You can type in as many ideas as you \nlike until then, but creative quality is more important than quantity. It’s better to have a few really good ideas \nthan a lot of uncreative ones. List as many ORIGINAL and CREATIVE uses for a [item].\nBecause the goal was to control for fluency, we excluded prompt parameters such as ’ quantity’ from the GPT-4 \ninstructions. Similarly, GPT does not need timing parameters in comparison to humans because we denoted the \nspecific number of responses required. See below for instructions used per GPT instance:\nFor this task, you’ll be asked to come up with as original and creative uses for [item] as you can. The goal is \nto come up with creative ideas, which are ideas that strike people as clever, unusual, interesting, uncommon, \nhumorous, innovative, or different.\nYour ideas don’t have to be practical or realistic; they can be silly or strange, even, so long as they are CREA-\nTIVE uses rather than ordinary uses. List [insert fluency number] ORIGINAL and CREATIVE uses for a \n[item].\nTable 1.  Demographics of human sample (N = 151).\nM (SD) or n (%)\nAge 41.21 (12.18)\nGender\n Female 58 (38%)\n Male 93 (62%)\nEthnicity\n White or European American 102 (68%)\n Black or African American 21 (14%)\n Asian or Asian American 11 (7.1%)\n Hispanic or Latinx 7 (5%)\n Multiracial 10 (7%)\nEducation\n Less than high school 3 (2%)\n High school graduate 26 (17%)\n Some college 28 (19%)\n 2 year degree 12 (8%)\n 4 year degree 62 (41%)\n Professional degree 18 (12%)\n Doctorate 2 (1%)\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\nConsequences task\nThe Consequences Task  (CT8,9) is part of the verbal section of the Torrance Test of Creative Thinking (TTCT) \nthat provides prompts to hypothetical scenarios (i.e., what would happen if humans no longer needed to sleep?). \nSimilar to the AUT, people respond to as many consequences to the prompt as they can within a given timeframe. \nResponses were scored for fluency (i.e., number of responses), originality (i.e., uniqueness of responses), and \nelaboration (i.e., number of words per valid response). General task instructions for human respondents were:\nIn this task, a statement will appear on the screen. The statement might be something like \"imagine gravity \nceases to exist\". For 3 minutes, try and think of any and all consequences that might result from the statement. \nPlease be as creative as you like. The goal is to come up with creative ideas, which are ideas that strike people \nas clever, unusual, interesting, uncommon, humorous, innovative, or different.\nYour responses will be scored based on originality and quality. Remember, it is important to try to keep think-\ning of responses and to type them in for the entire time for the prompt.\nREMINDER: In this task, a statement will appear on the screen. The statement might be something like \n\"imagine gravity ceases to exist\". For 3 minutes, try and think of any and all consequences that might result \nfrom the statement. Do this as many times as you can in 3 min.\nThe screen will automatically change when the time is completed. Remember, it is important to try to keep \nthinking of responses and to type them in for the entire time for the prompt.\nParticipants were given two prompts shown independently: “Imagine humans no longer needed sleep, ” and \n“Imagine humans walked with their hands. ” The two CT prompts have been extensively used in research on \ndivergent  thinking41–43. Similar to the AUT, fluency and timing parameters were excluded from the GPT instruc-\ntions on the CT:\nIn this task, a statement will appear on the screen. The statement might be something like \"imagine grav-\nity ceases to exist\". Please be as creative as you like. The goal is to come up with creative ideas, which are \nideas that strike people as clever, unusual, interesting, uncommon, humorous, innovative, or different. Your \nresponses will be scored based on originality and quality.\nTry and think of any and all consequences that might result from the statement. [Insert scenario]. What \nproblems might this create? List [insert fluency number] CREATIVE consequences.\nDivergent associations task\nThe Divergent Association Task  (DAT35) is a task of divergent and verbal semantic creative ability. This task asks \nparticipants to come up with 10 nouns as different from each other as possible. These nouns must not be proper \nnouns or any type of technical term. Pairwise comparisons of semantic distance between the 10 nouns are cal -\nculated using cosine distance. The average distance scores between all pairwise comparisons are then multiplied \nby 100 that results in a final DAT score (https:// osf. io/ bm5fd/). High scores indicate longer distances (i.e., words \nare not similar). Task instructions for both human participants and GPT-4 were:\nPlease enter 10 words that are as different from each other as possible, in all meanings and uses of the words. \nThe rules: Only single words in English. Only nouns (e.g., things, objects, concepts). No proper nouns (e.g., \nno specific people or places). No specialized vocabulary (e.g., no technical terms). Think of the words on your \nown (e.g., do not just look at objects in your surroundings).\nThere were no time constraints for this task. The average human response time was 126.19 s (SD = 90.62) and \nthe average DAT score was 76.95 (SD = 6.13). We scored all appropriate words that participants gave. Participants \nwith fewer than 7 responses were excluded from data analysis (n = 2). Instructions were identical for the GPT-4 \nto the human instructions.\nProcedure\nHuman participants’ responses were collected online via Qualtrics. The entire study took on average 34 min \n(SD = 13.64). The order of the creativity tasks was counterbalanced. The online study used two attention checks \nrandomly presented throughout the study. Each attention check allowed one additional attempt. Participants \nwho failed two attention checks were removed from all analyses (N = 2). After providing their responses to each \ntask, participants answered demographics questions.\nGPT-4 procedural responses were generated through human-assistance facilitated by the first author, who \nprovided each prompt in the following order: AUT, CT, and DAT. We did not have to account for typical human-\ncentered confounds such as feelings of  fatigue44,45 and order  biases44 as these states are not relevant confounds \nin AI, thus the order of tasks was not counterbalanced.\nResearch disclosure statement\nAll variables, measurements, and exclusions for this article’s target research question have been reported in the \nmethods section.\nResults\nCreativity scoring\nBoth human and GPT-4 responses were cleaned to remove any instances that were incomplete or inappropriate \nat two stages: First, human responses that did not follow instructions from the task or were not understandable \nas a use (AUT; 0.96% removed) or a consequence (CT; 4.83%) were removed. Only valid human responses were \nused in matching for GPT fluency; Second, inappropriate or incomplete GPT responses for the AUT (< 0.001% \n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\nremoved) and CT (< 0.001% removed) were removed. Despite matching for fluency, only valid responses in both \ngroups were used in subsequent analyses.\nTraditional scoring methods of divergent thinking tasks have required human ratings of products or ideas \nand are assumed to be normative tasks (i.e., consensus will eventually be met with more raters). Here, we used \nthe Open Creativity Scoring tool  [OCS46] to automate scoring of semantic distance objectively by capturing \nthe originality of ideas by assigning scores of the remoteness (uniqueness) of responses. Unlike human scoring \nwhich requires multiple factors of consideration (e.g., fatigue, biases, time,  cost47) which could result in potential \nconfounds, automated scoring tools such as OCS circumvent the human-centered issues and has been found to \nrobustly correlate with human  ratings46.\nOpen Creativity Scoring tool  (OCS46) was used to score both the AUT and CT tasks. Specifically, the seman-\ntic distance scoring  tool17 was used, which applies the GLoVe 840B text-mining  model48 to assess originality of \nresponses by representing a prompt and response as vectors in semantic space and calculates the cosine of the \nangle between the vectors. The OCS tool also scores for elaboration by using the stoplist  method46. The prompts \nfor the AUT were “rope” and “fork” and the prompts for the CT were “humans no sleep” and “humans walked \nhands. ”\nPreliminary results\nDescriptive statistics for all tasks are reported in Tables  2 and 3. Fluency descriptive statistics are reported in \nTable 2. Semantic distance descriptive statistics are reported in Table 3.\nTable 2.  Descriptive statistics of fluency for alternative uses task, consequences task, and divergent \nassociations task responses for human and GPT-4 samples. Skewness and kurtosis of DAT fluency was \nexpected due to the task requiring 10 responses. Only valid and legible DAT responses were retained between \nboth groups. AUT  Alternative Uses Task, CT Consequences Task, DAT Divergent Associations Task.\nPrompt M (SD) Median Skew Kurtosis\nHuman\n Fork (AUT) 6.82 (3.67) 6 1.79 4.67\n Rope (AUT) 7.06 (3.92) 6 1.07 1.17\n No more sleep (CT) 5.98 (3.09) 5 1.45 3.48\n Walk on hands (CT) 5.44 (3.30) 5 2.73 15.20\n DAT 9.72 (0.62) 10 − 2.73 8.18\nGPT-4\n Fork (AUT) 6.87 (3.66) 6 1.80 4.69\n Rope (AUT) 7.13 (3.95) 6 1.03 1.01\n No more sleep (CT) 5.72 (3.03) 5 1.39 3.28\n Walk on hands (CT) 5.27 (3.26) 5 2.87 16.60\n DAT 9.97 (0.18) 10 − 5.25 25.93\nTable 3.  Descriptive statistics of originality using semantic distance for alternative uses task, consequences \ntask, and divergent associations task responses for human and GPT-4 samples. AUT  Alternative Uses Task, CT \nConsequences Task, DAT Divergent Associations Task.\nPrompt M (SD) Median Skew Kurtosis\nHuman\n Fork (AUT) 0.79 (0.04) 0.79 − 0.35 0.50\n Rope (AUT) 0.68 (0.06) 0.68 0.03 0.03\n No more sleep (CT) 0.67 (0.05) 0.67 0.18 − 0.28\n Walk on hands (CT) 0.67 (0.06) 0.67 − 0.58 1.27\n DAT 76.95 (6.13) 77.58 − 0.85 1.5\nGPT-4\n Fork (AUT) 0.84 (0.02) 0.84 − 0.14 − 0.48\n Rope (AUT) 0.79 (0.02) 0.80 − 0.59 1.00\n No more sleep (CT) 0.71 (0.02) 0.71 0.05 0.34\n Walk on hands (CT) 0.73 (0.01) 0.73 − 0.13 0.61\n DAT 84.56 (3.05) 84.79 − 0.29 − 0.48\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\nPrimary results\nAlternative uses task\nAs expected, an independent sample t -test revealed no significant differences in total fluency due to control-\nling for fluency (as detailed above) between humans ( M = 6.94, SD = 3.80) and GPT-4 (M  = 7.01, SD = 3.81), \nt(602) = 0.21, 95% CI [− 0.54, 0.67], p = 0.83.\nTo assess originality of responses via semantic distance scores, we conducted a 2 (group: human, GPT-\n4) X 2 (prompt: ‘fork, rope) analysis of variance. The model revealed significant main effects of group (F (1, \n600) = 622.10, p < 0.001, η2 = 0.51) and prompt (F(1, 600) = 584.50, p < 0.001, η2 = 0.49) on originality of responses. \nAdditionally, there were significant interaction effects between group and prompt, F(1, 600) = 113.80, p < 0.001, \nη2 = 0.16. Particularly, both samples had higher originality scores for the prompt ‘fork’ in comparison to ‘rope, ’ but \nGPT-4 scored higher in originality, regardless of prompt. Tukey’s HSD post hoc analysis showed that all pairwise \ncomparisons were significantly different (p  < 0.001) aside from the human ‘fork’ and GPT-4 ‘rope’ originality \n(p = 0.989). Overall, GPT-4 was more successful at coming up with divergent responses given the same number \nof opportunities to generate answers compared to the human counterpart and showed higher originality but \nonly for specific prompts (Fig. 1).\nNext, we compared elaboration scores between humans and GPT-4. Fluency scores differ from elaboration in \nthe sense that fluency accounts for each coherent response whereas elaboration quantifies the number of words \nper valid response. For example, a person could respond “you could use a fork to knit or as a hair comb. ” In this \nexample, the fluency would be 2 (knitting instrument and comb), but the elaboration would be 12 (number of \nwords used in the response). The results of an independent t-test revealed that elaboration was significantly \nhigher for GPT-4 (M = 15.45, SD = 6.74) in comparison to humans (M = 3.38, SD = 2.91), t(602) = 28.57, 95% CI \n[11.24, 12.90], p < 0.001.\nConsequences task\nAs expected, an independent t-test revealed no significant differences in total fluency between humans (M = 5.71, \nSD = 3.20) and GPT-4 (M = 5.50, SD = 3.15), t(621) = 0.82, 95% CI [− 0.29, 0.71], p = 0.41.\nTo assess originality of responses via semantic distance scores, we conducted a 2 (group: human, GPT) X \n2 (prompt: ‘no more sleep, ’ ‘walk on hands’) analysis of variance. The model revealed significant main effects \nof group ( F(1, 619) = 622.10, p < 0.001, η2 = 0.51) and prompt (F (1, 619) = 584.50, p < 0.001, η2 = 0.49) on the \noriginality of responses. Additionally, there were significant interaction effects between group and prompt, F(1, \n619) = 113.80, p < 0.001, η2 = 0.16. Particularly, originality was marginally higher for the prompt ‘walk on hands’ \nin the GPT sample, although there were no significant differences in originality in the human sample between \nthe two prompts. Tukey’s HSD post hoc analysis showed that all pairwise comparisons were significantly different \n(p < 0.001) aside from the human responses for both prompts ( p = 0.607). Overall, GPT-4 was more successful \nat coming up with more divergent responses given the same number of opportunities compared to the human \ncounterparts, and also showed higher originality dependent on prompt type (Fig. 2).\nNext, we calculated the difference in elaboration between humans and GPT-4. The results of an independent \nI-test revealed that elaboration was significantly higher in the GPT-4 sample (M = 38.69, SD = 15.60) than in the \nhuman sample (M = 5.45, SD = 4.04), t(621) = − 36.04, 95% CI [− 35.04, − 31.45], p < 0.001.\nDivergent associations task\nWe assessed the qualitative aspect of the words generated in the DAT between both humans and GPT through \nword occurrence. Namely, the frequency of single-occurrence (non-repeating words within groups) and unique \noccurrence (words only occurring once between groups).\nFigure 1.  Analysis of variance of originality on the alternative uses task.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\nHumans had a higher number of single-occurrence words (n = 523) that accounted for 69.92% within the \ntotal group response in comparison to GPT’s number of single-occurrence words (n = 152) that accounted for \n47.95% within the total group response (Table  4). In total, there was 9.11% (n = 97) of overlapping responses \nbetween both groups. Exclusively unique words that only occurred in the human responses accounted for 87.03% \n(n = 651) in comparison to unique GPT responses which accounted for 69.40% (n = 220).\nA chi-square test of independence was performed to examine the relationship between groups (GPT vs \nhuman) and word type (single occurrence vs unique occurrence). The relationship between these variables was \nnot significant, χ2 (1, N = 302) = 1.56, p = 0.211. This suggests that uniqueness and occurrences of words may not \nhave necessarily aided either group in originality, but rather aided in word complexity.\nDifferences in semantic distance scores were calculated between human and GPT-4 DAT responses. An \nindependent sample t-test revealed that GPT responses (M = 84.56, SD = 3.05) had higher semantic distances in \ncomparison to human responses (M  = 76.95, SD = 6.13), t(300) = 13.65, 95% CI [6.51, 8.71], p  < 0.001. Despite \nhuman participants having a broader range of unique responses, the fluency uniqueness did not appear to \nadvantage semantic distance scores when comparing groups.\nFigure 2.  Analysis of variance of originality on the consequences task.\nTable 4.  Top 20 most frequent words on the divergent association task in human and GPT-4 samples.\nHuman GPT-4\nWord Frequency Word Frequency\nDog 28 Elephant 98\nCar 25 Symphony 55\nBook 25 Microscope 51\nCloud 22 Quasar 44\nTree 21 Freedom 44\nComputer 20 Dream 43\nWater 16 Democracy 43\nChair 16 Love 40\nCat 16 Volcano 39\nMoon 13 Quantum 39\nTable 12 Philosophy 31\nSky 12 Microbe 27\nOcean 12 Galaxy 27\nMountain 12 Desert 26\nGrass 12 Compass 22\nElephant 11 Microchip 19\nPaper 10 Ocean 16\nFlower 10 Justice 15\nFire 10 Harmony 15\nShoe 9 Dolphin 15\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\nDiscussion\nThe present study offers novel evidence on the current state of large language models (i.e., GPT-4) and the capa-\nbilities of divergent creative output in comparison to human participants. Overall, GPT-4 was more original and \nelaborate than humans on each of the divergent thinking tasks, even when controlling for fluency of responses. In \nother words, GPT-4 demonstrated higher creative potential across an entire battery of divergent thinking tasks \n(i.e., Alternative Uses Task, Consequences Task, and Divergent Associations Task).\nNotably, no other study has comprehensively assessed multiple dimensions of the most frequently used diver-\ngent thinking tasks and AI. However, studies have begun to examine differences in divergent creativity between \nhumans and AI, particularly after the public emergence of OpenAI’s ChatGPT, with findings showing that AI’s \ncreative potential scores within the top 1% of human responses in terms of  originality32–34. While there has been \nan influx in research examining the creativity of generative language models, to date only one previous study \nshowed that humans outperformed GPT on the AUT (GPT-331), while another study reported that later versions \nof GPT (GPT-4 showed similar, albeit slightly less, creative potential in comparison to  humans32). Similarly, one \nprevious study demonstrated that generative models were improved in GPT 4 compared to GPT 3.5, particularly \nin terms of fluency, but interestingly, not in terms of  elaboration49 which suggests that the creative potential of \nthese LLM’s are improving, particularly the ability to generate original ideas. Indeed, only one other study thus \nfar has reported similar results that GPT outperformed humans on the  DAT36, but the DAT is only one aspect \nof divergent thinking. Instead, the novelty of the present findings provides a foundation for future research to \ncontinue to examine multiple dimensions of divergent thinking and artificial intelligence.\nWhile the present results suggest that the current state of AI models outperform humans on divergent think-\ning tasks by a significant margin, there are methodological considerations that could have contributed to the \npresent results. To comprehensively examine creativity requires not only an assessment of originality, but also of \nthe usefulness and appropriateness of an idea or  product50. Traditionally, this has proven difficult to standard-\nize in comparison to assessing originality given the multifaceted dimensions that contribute to assessments of \nappropriateness such as accounting for sociocultural and historical contexts. Semantic distance scores do not \ntake into consideration the aforementioned variables; instead, the scores reflect the relative distance between \nseemingly related (or unrelated) ideas. In this instance, GPT-4’s answers yielded higher originality than human \ncounterparts, but the feasibility or appropriateness of an idea could be vastly inferior to that of humans. Thus, \nwe need to consider that the results reflect only a single aspect of divergent thinking, rather than a generalization \nthat AI is indeed more creative across the board. Future research on AI and creativity needs to not only account \nfor the traditional measurements of creativity (i.e., fluency, elaboration, originality) but also for the usefulness \nand appropriateness of the ideas.\nInterestingly, GPT-4 used a higher frequency of repeated words in comparison to human respondents. \nAlthough the breadth of vocabulary used by human responses was much more flexible, this did not necessarily \nresult in higher semantic distance scores. Flexibility, or number of categories of responses, has also been found \nto be smaller (i.e., more similar categories of words were generated) for AI in comparison to  humans34. In other \nwords, like our present results, humans came up with a wider range of responses, however, this did not indicate \nincreased originality. These findings highlight the consideration that flexible thinking may be the strong point \nin human-centered divergent thinking.\nMore so, the complexity of words chosen by AI, albeit more concentrated in occurrence, could have more \nrobustly contributed to the originality effects. For example, only AI used words that are non-tangible items (i.e., \nfreedom, philosophy) whereas humans may have experienced a fixedness on generating ideas that are appropriate \nand observable. The differences between generated lists (incorporating tangible and non-tangible word) could \ninflate originality to be biased toward AI.\nSimilarly, we need to critically consider the uniqueness of words generated in DAT responses. There was a \nmarginal overlap of responses between the human and the AI samples (9.11%), but humans responded with a \nhigher number of single-occurrence words. Despite these differences, AI still had a higher semantic distance \nscore. Prior research shows that in human respondent’s originality increases over  time51. This increase is seen \nas an expansion of activation in an individual’s semantic network, which leads to more original  responses52. \nHuman responses on these DT tasks tend to follow a diminishing returns curve before reaching a plateau for an \nindividual’s more original  responses53. The higher levels of elaboration and semantic distance in AI responses \nsuggests that the LLM processing possibly does not need this ramp-up time as seen in human responses, there-\nfore LLM’s can respond with their highest level of original responses when prompted. Whereas humans may \nfixate on more obvious responses at first, this algorithmic trait could then serve as an aid in overcoming ideation \nfixedness in humans.\nIt is important to note that the measures used in this study are all measures of creative potential, but involve-\nment in creative activities or achievements is another aspect of measuring a person’s creativity. Creative potential \nis not a guarantee for creative achievement; instead, we need to consider creative potential as an indicator of a \nperson’s creative  capabilities7. Here, AI was more original thus indicating higher creative potential, but this metric \nmay more appropriately reflect the advancement of the algorithms these models were trained on in conjunction \nwith human input. In other words, AI, unlike humans, does not have agency, thus AI creative potentials are \ndependent on the assistance of a human user to elicit responses. Therefore, the creative potential of AI is in a \nconstant state of stagnation unless prompted.\nMoreover, researchers have examined the interplay between creative potential and real-world creative \n achievements54,55 but this approach assumes human level creativity and is not able to account for artificial intelli-\ngence. AI can generate creative ideas, but it cannot be assumed that this potential would translate to achievement. \nThe creative potential of AI is limited by the (lack of) autonomy of what the algorithms can create (i.e., creative \npotential) without the intervention of human assistance. Thus, future research should consider the conceptual \n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\nimplications of current measurements of creativity as implicated in applications in real-world settings and how \ngeneralizability at the intersection of potential and achievement may be a human-centric consideration.\nThe prevalence and accessibility of the internet has drastically shaped the way in which humans interact with \nlanguage processing systems and search engines. LLM’s such as GPT-4 are now not an exception in ubiquity. \nSearching for information has multiple channels which were not previously available, and with these functions \ncome an array of strategies to best find the desired information. Research has shown that younger people are \nbetter and more efficient in their search strategies online to find the information they  want56, which suggests \nthat exposure to search platforms acts as a practice in efficiency. Similar to interactions with GPT-4 and other \nAI platforms, humans may gradually navigate how to best utilize LLM’s. For information seeking tools like \nGPT-4, the creative potential has shown clear progression in capabilities, albeit there are still limitations such as \nresponse appropriateness and AI’s ability to generate idiosyncratic associations. Generative AI has demonstrated \nrobustness in creative potential but has also shown weaknesses (i.e., less flexible thinking) that could then be \nsupplemented by human assistance. Moving forward, future possibilities of AI acting as a tool of inspiration, as \nan aid in a person’s creative process, or to overcome fixedness is promising.\nData availability\nAll data associated with the present study is available at https:// osf. io/ xv6kh/.\nReceived: 14 October 2023; Accepted: 30 January 2024\nReferences\n 1. OpenAI. ChatGPT: Optimizing Language Models for Dialogue. (2023). https:// openai. com/ blog/ chatg pt/. Accessed July 2023.\n 2. Rahaman, M. S., Ahsan, M. T., Anjum, N., Terano, H. J. R. & Rahman, M. M. From ChatGPT-3 to GPT-4: A significant advance-\nment in ai-driven NLP tools. J. Eng. Emerg. Technol. 2(1), 1–11. https:// doi. org/ 10. 52631/ jeet. v2i1. 188 (2023).\n 3. Lee, Y . H., & Lin, T. H. (2023). The feasibility study of AI image generator as shape convergent thinking tool. in International \nConference on Human-Computer Interaction (pp. 575–589). https:// doi. org/ 10. 1007/ 978-3- 031- 35891-3_ 36.\n 4. Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., & Zhang, Y . (2023). Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv: \n2304. 03439.\n 5. Cropley, A. In praise of convergent thinking. Creat. Res. J. 18(3), 391–404. https:// doi. org/ 10. 1207/ s1532 6934c rj1803_ 13 (2006).\n 6. Guilford, J. P . The Nature of Human Intelligence (McGraw-Hill, 1967).\n 7. Runco, M. A. & Acar, S. Divergent thinking as an indicator of creative potential. Creat. Res. J. 24(1), 66–75. https:// doi. org/ 10. \n1080/ 10400 419. 2012. 652929 (2012).\n 8. Torrance, E. P . The Torrance Tests of Creative Thinking: Norms-Technical Manual (Personal Press, 1974).\n 9. Wilson, R. C., Guilford, J. P ., Christensen, P . R. & Lewis, D. J. A factor-analytic study of creative-thinking abilities. Psychometrika \n19(4), 297–311. https:// doi. org/ 10. 1007/ bf022 89230 (1954).\n 10. Runco, M. A. AI can only produce artificial creativity. J. Creat. 33(3), 100063. https:// doi. org/ 10. 1016/j. yjoc. 2023. 100063 (2023).\n 11. Finke, R. A. Imagery, creativity, and emergent structure. Conscious. Cogn. 5(3), 381–393. https:// doi. org/ 10. 1006/ ccog. 1996. 0024 \n(1996).\n 12. Sarker, I. H. Deep learning: A comprehensive overview on techniques, taxonomy, applications and research directions. SN Comput. \nSci. 2(6), 420. https:// doi. org/ 10. 1007/ s42979- 021- 00815-1 (2021).\n 13. Khurana, D., Koli, A., Khatter, K. & Singh, S. Natural language processing: State of the art, current trends and challenges. Multimed. \nTools Appl. 82(3), 3713–3744. https:// doi. org/ 10. 1007/ s11042- 022- 13428-4 (2022).\n 14. Zhou, M., Duan, N., Liu, S. & Shum, H.-Y . Progress in neural NLP: Modeling, learning, and reasoning. Engineering 6(3), 275–290. \nhttps:// doi. org/ 10. 1016/j. eng. 2019. 12. 014 (2020).\n 15. Cardoso, A., Veale, T. & Wiggins, G. A. Converging on the divergent: The history (and future) of the international joint workshops \nin computational creativity. AI Mag. 30(3), 15. https:// doi. org/ 10. 1609/ aimag. v30i3. 2252 (2009).\n 16. Lambert, N., Castricato, L., von Werra, L., & Havrilla A. Illustrating Reinforcement Learning from Human Feedback (RLHF). Hug-\nging Face. (2022). https:// huggi ngface. co/ blog/ rlhf.\n 17. Dumas, D., Organisciak, P . & Doherty, M. Measuring divergent thinking originality with human raters and text-mining models: \nA psychometric comparison of methods. Psychol. Aesthet. Creat. Arts 15(4), 645–663. https:// doi. org/ 10. 1037/ aca00 00319 (2021).\n 18. Kane, S. et al. Attention, affect, and creativity, from mindfulness to mind-wandering. In The Cambridge Handbook of Creativity \nand Emotions (eds Ivcevic, Z. et al.) 130–148 (Cambridge University Press, 2023). https:// doi. org/ 10. 1017/ 97810 09031 240. 010.\n 19. Chatterjee, A. Art in an age of artificial intelligence. Front. Psychol. 13, 1024449. https:// doi. org/ 10. 3389/ fpsyg. 2022. 10244 49 (2022).\n 20. Boden, M. A. Computer models of creativity. AI Mag. 30(3), 23–23. https:// doi. org/ 10. 1609/ aimag. v30i3. 2254 (2009).\n 21. Bellaiche, L. et al. Humans versus AI: Whether and why we prefer human-created compared to AI-created artwork. Cogn. Res. \nPrinc. Implic. 8(1), 1–22. https:// doi. org/ 10. 1186/ s41235- 023- 00499-6 (2023).\n 22. Chiarella, S. et al. Investigating the negative bias towards artificial intelligence: Effects of prior assignment of AI-authorship on the \naesthetic appreciation of abstract paintings. Comput. Hum. Behav. 137, 107406. https:// doi. org/ 10. 1016/j. chb. 2022. 107406 (2022).\n 23. Fortuna, P . & Modliński, A. A(I)rtist or counterfeiter? Artificial intelligence as (D) evaluating factor on the art market. J. Arts \nManag. Law Soc. 51(3), 188–201. https:// doi. org/ 10. 1080/ 10632 921. 2021. 18870 32 (2021).\n 24. Liu, Y ., Mittal, A., Y ang, D., & Bruckman, A. (2022). Will AI console me when I lose my pet? Understanding perceptions of AI-\nmediated email writing. in Conference on Human Factors in Computing Systems. https:// doi. org/ 10. 1145/ 34911 02. 35177 31\n 25. Chamberlain, R., Mullin, C., Scheerlinck, B. & Wagemans, J. Putting the art in artificial: Aesthetic responses to computer-generated \nart. Psychol. Aesthet. Creat. Arts 12(2), 177–192. https:// doi. org/ 10. 1037/ aca00 00136 (2018).\n 26. Gao, C. A. et al. Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output \ndetector, plagiarism detector, and blinded human reviewers. Biorxiv https:// doi. org/ 10. 1016/j. patter. 2023. 100706 (2023).\n 27. Samo, A. & Highhouse, S. Artificial intelligence and art: Identifying the aesthetic judgment factors that distinguish human- and \nmachine-generated artwork. Psychol. Aesthet. Creat. Arts. https:// doi. org/ 10. 1037/ aca00 00570 (2023).\n 28. Yin, Z., Reuben, F ., Stepney, S. & Collins, T. Deep learning’s shallow gains: A comparative evaluation of algorithms for automatic \nmusic generation. Mach. Learn. 112(5), 1785–1822. https:// doi. org/ 10. 1007/ s10994- 023- 06309-w (2023).\n 29. Kumar, Y ., Koul, A., Singla, R. & Ijaz, M. F . Artificial intelligence in disease diagnosis: A systematic literature review, synthesizing \nframework and future research agenda. J. Ambient Intell. Hum. Comput. https:// doi. org/ 10. 1007/ s12652- 021- 03612-z (2022).\n 30. Anantrasirichai, N. & Bull, D. Artificial intelligence in the creative industries: A review. Artif. Intell. Rev. https:// doi. org/ 10. 1007/ \ns10462- 021- 10039-7 (2022).\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:3440  | https://doi.org/10.1038/s41598-024-53303-w\nwww.nature.com/scientificreports/\n 31. Stevenson, C., Smal, I., Baas, M., Grasman, R., & van der Maas, H. Putting GPT-3’s Creativity to the (Alternative Uses) Test. (2022). \narXiv: 2206. 08932.\n 32. Haase, J. & Hanel, P . H. (2023). Artificial Muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity. \nhttps:// doi. org/ 10. 48550/ arXiv. 2303. 12003\n 33. Koivisto, M. & Grassini, S. Best humans still outperform artificial intelligence in a creative divergent thinking task. Sci. Rep. 13, \n13601. https:// doi. org/ 10. 1038/ s41598- 023- 40858-3 (2023).\n 34. Guzik, E. E., Byrge, C. & Gilde, C. The originality of machines: AI takes the torrance test. J. Creat. 33(3), 100065. https:// doi. org/ \n10. 1016/j. yjoc. 2023. 100065 (2023).\n 35. Olson, J. A., Nahas, J., Chmoulevitch, D., Cropper, S. J. & Webb, M. E. Naming unrelated words predicts creativity. Proc. Natl. Acad. \nSci. 118, 25. https:// doi. org/ 10. 1073/ pnas. 20223 40118 (2021).\n 36. Cropley, D. Is artificial intelligence more creative than humans?: ChatGPT and the divergent association task. Learn. Lett. 2, 13–13. \nhttps:// doi. org/ 10. 59453/ ll. v2. 13 (2023).\n 37. Chen, L., Sun, L. & Han, J. A comparison study of human and machine-generated creativity. J. Comput. Inf. Sci. Eng. 23(5), 051012. \nhttps:// doi. org/ 10. 1115/1. 40622 32 (2023).\n 38. Sawyer, R. K. Explaining Creativity: The Science of Human Innovation (Oxford University Press, 2012).\n 39. R Core Team. R: A Language and Environment for Statistical Computing (Version 4.1.0) [Computer Software]. (2021). http:// www.R- \nproje ct. org.\n 40. Nusbaum, E. C., Silvia, P . J. & Beaty, R. E. Ready, set, create: What instructing people to “be creative” reveals about the meaning \nand mechanisms of divergent thinking. Psychol. Aesthet. Creat. Arts 8(4), 423. https:// doi. org/ 10. 1037/ a0036 549 (2014).\n 41. Acar, S. et al. Applying automated originality scoring to the verbal form of Torrance tests of creative thinking. Gift. Child Q. 67(1), \n3–17. https:// doi. org/ 10. 1177/ 00169 86221 10618 74 (2021).\n 42. Hass, R. W . & Beaty, R. E. Use or consequences: Probing the cognitive difference between two measures of divergent thinking. \nFront. Psychol. 9, 2327. https:// doi. org/ 10. 3389/ fpsyg. 2018. 02327 (2018).\n 43. Urban, M. & Urban, K. Orientation toward intrinsic motivation mediates the relationship between metacognition and creativity. \nJ. Creat. Behav. 57(1), 6–16. https:// doi. org/ 10. 1002/ jocb. 558 (2023).\n 44. Day, B. et al. Ordering effects and choice set awareness in repeat-response stated preference studies. J. Environ. Econ. Manag. 63(1), \n73–91. https:// doi. org/ 10. 1016/j. jeem. 2011. 09. 001 (2012).\n 45. Igorov, M., Predoiu, R., Predoiu, A. & Igorov, A. Creativity, resistance to mental fatigue and coping strategies in junior women \nhandball players. Eur. Proc. Soc. Behav. Sci. https:// doi. org/ 10. 15405/ epsbs. 2016. 06. 39 (2016).\n 46. Organisciak, P . & Dumas, D. Open Creativity Scoring [Computer Software]. (University of Denver, 2020). https:// opens coring. du. \nedu/.\n 47. Beaty, R. E., Johnson, D. R., Zeitlen, D. C. & Forthmann, B. Semantic distance and the alternate uses task: Recommendations for \nreliable automated assessment of originality. Creat. Res. J. 34(3), 245–260. https:// doi. org/ 10. 1080/ 10400 419. 2022. 20257 20 (2022).\n 48. Pennington, J., Socher, R. & Manning, C. Glove: Global vectors for word representation. in Proceedings of the 2014 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP), 1532–1543 (2014).\n 49. Vinchon, F ., Gironnay, V ., & Lubart, T. The Creative AI-Land: Exploring new forms of creativity. In Review. (2023).\n 50. Runco, M. A. & Jaeger, G. J. The standard definition of creativity. Creat. Res. J.  24(1), 92–96. https:// doi. org/ 10. 1080/ 10400 419. \n2012. 650092 (2012).\n 51. Beaty, R. E. & Silvia, P . J. Why do ideas get more creative across time? An executive interpretation of the serial order effect in \ndivergent thinking tasks. Psychol. Aesthet. Creat. Arts 6(4), 309–319. https:// doi. org/ 10. 1037/ a0029 171 (2012).\n 52. Mednick, S. The associative basis of the creative process. Psychol. Rev. 69(3), 220–232. https:// doi. org/ 10. 1037/ h0048 850 (1962).\n 53. Hubert K. F ., Finch A., Zabelina D. (2023). Diminishing Creative Returns: Predicting Optimal Creative Performance via Individual \nDifferences in Executive Functioning.\n 54. Carson, S. H., Peterson, J. B. & Higgins, D. M. Reliability, validity, and factor structure of the creative achievement questionnaire. \nCreat. Res. J. 17(1), 37–50. https:// doi. org/ 10. 1207/ s1532 6934c rj1701_4 (2005).\n 55. Jauk, E., Benedek, M. & Neubauer, A. C. The road to creative achievement: A latent variable model of ability and personality \npredictors. Pers. Individ. Diff. https:// doi. org/ 10. 1016/j. paid. 2013. 07. 129 (2014).\n 56. Chevalier, A., Dommes, A. & Marquié, J.-C. Strategy and accuracy during information search on the web: Effects of age and \ncomplexity of the search questions. Comput. Hum. Behav. 53, 305–315. https:// doi. org/ 10. 1016/j. chb. 2015. 07. 017 (2015).\nAuthor contributions\nD.L.Z., K.F .H., and K.N.A. contributed to the conceptualization and methodology. K.F .H. and K.N.A. contributed \nto formal analysis and investigation. K.F .H. prepared all figures. K.N.A. prepared all tables. D.L.Z., K.F .H., and \nK.N.A. contributed to writing and revision.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to K.F .H.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024",
  "topic": "Fluency",
  "concepts": [
    {
      "name": "Fluency",
      "score": 0.6647171378135681
    },
    {
      "name": "Task (project management)",
      "score": 0.6405857801437378
    },
    {
      "name": "Trait",
      "score": 0.627275824546814
    },
    {
      "name": "Perception",
      "score": 0.5921373963356018
    },
    {
      "name": "Computer science",
      "score": 0.5073410868644714
    },
    {
      "name": "Generative grammar",
      "score": 0.49928951263427734
    },
    {
      "name": "Cognitive science",
      "score": 0.4988882541656494
    },
    {
      "name": "Human language",
      "score": 0.4820340871810913
    },
    {
      "name": "Human intelligence",
      "score": 0.4389643967151642
    },
    {
      "name": "Cognitive psychology",
      "score": 0.437623530626297
    },
    {
      "name": "Creativity",
      "score": 0.4154165983200073
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41028064489364624
    },
    {
      "name": "Psychology",
      "score": 0.403738796710968
    },
    {
      "name": "Linguistics",
      "score": 0.18336132168769836
    },
    {
      "name": "Social psychology",
      "score": 0.1645282506942749
    },
    {
      "name": "Mathematics education",
      "score": 0.07852736115455627
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78715868",
      "name": "University of Arkansas at Fayetteville",
      "country": "US"
    }
  ]
}