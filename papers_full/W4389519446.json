{
  "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
  "url": "https://openalex.org/W4389519446",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5005339090",
      "name": "Seungone Kim",
      "affiliations": [
        "Naver (South Korea)",
        "University of Washington"
      ]
    },
    {
      "id": null,
      "name": "Se Joo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109115500",
      "name": "Doyoung Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3092799944",
      "name": "Joel Jang",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2150567914",
      "name": "Seonghyeon Ye",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2888032091",
      "name": "Jamin Shin",
      "affiliations": [
        "Naver (South Korea)",
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2252216270",
      "name": "Minjoon Seo",
      "affiliations": [
        "Naver (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891012317",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2996848635",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W4389519535",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W4389518968",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W3027453785",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4311728219",
    "https://openalex.org/W4303648904",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W4303649108",
    "https://openalex.org/W4389519396",
    "https://openalex.org/W4386566852",
    "https://openalex.org/W3175591618",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4322760437",
    "https://openalex.org/W4386655779",
    "https://openalex.org/W4280652569",
    "https://openalex.org/W4385572426",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4226408727",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3197499505",
    "https://openalex.org/W2963643701",
    "https://openalex.org/W3189216228",
    "https://openalex.org/W4319653585",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4318719086",
    "https://openalex.org/W4385223296",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2945720633",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W4281557543",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W4307927299",
    "https://openalex.org/W4310629099",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W3032232719"
  ],
  "abstract": "Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12685–12708\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nThe COT COLLECTION : Improving Zero-shot and Few-shot Learning of\nLanguage Models via Chain-of-Thought Fine-Tuning\nSeungone Kim1,2∗† Se June Joo1∗† Doyoung Kim1 Joel Jang1,3\nSeonghyeon Ye1 Jamin Shin2† Minjoon Seo1†\nKAIST AI1 NA VER AI Lab2 University of Washington3\n{seungone, sejune, minjoon}@kaist.ac.kr jayshin.nlp@gmail.com\nAbstract\nLanguage models (LMs) with less than 100B\nparameters are known to perform poorly on\nchain-of-thought (CoT) reasoning in contrast\nto large LMs when solving unseen tasks. In this\nwork, we aim to equip smaller LMs with the\nstep-by-step reasoning capability by instruction\ntuning with CoT rationales. In order to achieve\nthis goal, we first introduce a new instruction-\ntuning dataset called the COT C OLLECTION ,\nwhich augments the existing Flan Collection\n(including only 9 CoT tasks) with additional\n1.84 million rationales across 1,060 tasks. We\nshow that CoT fine-tuning Flan-T5 (3B & 11B)\nwith COT COLLECTION enables smaller LMs\nto have better CoT capabilities on unseen tasks.\nOn the BIG-Bench-Hard (BBH) benchmark,\nwe report an average improvement of +4.34%\n(Flan-T5 3B) and +2.60% (Flan-T5 11B), in\nterms of zero-shot task accuracy. Furthermore,\nwe show that instruction tuning withCOT COL-\nLECTION allows LMs to possess stronger few-\nshot learning capabilities on 4 domain-specific\ntasks, resulting in an improvement of +2.24%\n(Flan-T5 3B) and +2.37% (Flan-T5 11B), even\noutperforming ChatGPT utilizing demonstra-\ntions until the max length by a +13.98% mar-\ngin. Our code, the COT C OLLECTION data,\nand model checkpoints are publicly available 1.\n1 Introduction\nLanguage models (LMs) pre-trained on massive\ntext corpora can adapt to downstream tasks in both\nzero-shot and few-shot learning settings by incorpo-\nrating task instructions and demonstrations (Brown\net al., 2020; Wei et al., 2021; Sanh et al., 2021;\nMishra et al., 2022; Wang et al., 2022b; Iyer et al.,\n2022; Liu et al., 2022b; Chung et al., 2022; Long-\npre et al., 2023; Ye et al., 2023). One approach that\n∗denotes equal contribution. Work was done while Seun-\ngone was interning at NA VER AI Lab.\n†Corresponding authors\n1https://github.com/kaistAI/\nCoT-Collection\nhas been particularly effective in enabling LMs to\nexcel at a multitude of tasks is Chain-of-Thought\n(CoT) prompting, making LMs generate a rationale\nto derive its final prediction in a sequential man-\nner (Wei et al., 2022b; Kojima et al., 2022; Zhou\net al., 2022; Zhang et al., 2022; Yao et al., 2023).\nWhile CoT prompting works effectively for large\nLMs with more than 100 billion parameters, it does\nnot necessarily confer the same benefits to smaller\nLMs (Tay et al., 2022; Suzgun et al., 2022; Wei\net al., 2022a; Chung et al., 2022). The require-\nment of a large number of parameters consequently\nresults in significant computational cost and acces-\nsibility issues (Kaplan et al., 2020; Min et al., 2022;\nLiu et al., 2022b; Mhlanga, 2023; Li et al., 2023).\nRecent work has focused on empowering rela-\ntively smaller LMs to effectively solve novel tasks\nas well, primarily through fine-tuning with ratio-\nnales (denoted as CoT fine-tuning) and applying\nCoT prompting on a single target task (Shridhar\net al., 2022; Ho et al., 2022; Fu et al., 2023). How-\never, solving a single task does not adequately ad-\ndress the issue of generalization to a broad range of\nunseen tasks. While Chung et al. (2022) leverage 9\npublicly available CoT tasks during instruction tun-\ning to solve multiple unseen tasks, the imbalanced\nratio compared to 1,827 tasks used for direct fine-\ntuning results in poor CoT results across smaller\nLMs (Longpre et al., 2023). In general, the com-\nmunity still lacks a comprehensive strategy to fully\nleverage CoT prompting to solve multiple unseen\nnovel tasks in the context of smaller LMs.\nTo bridge this gap, we present the COT C OL-\nLECTION , an instruction tuning dataset that aug-\nments 1.84 million rationales from the FLAN Col-\nlection (Longpre et al., 2023) across 1,060 tasks.\nWe fine-tune Flan-T5 (3B & 11B) usingCOT COL-\nLECTION and denote the resulting model as CoT-\nT5. We perform extensive comparisons of CoT-T5\nand Flan-T5 under two main scenarios: (1) zero-\nshot learning and (2) few-shot learning.\n12685\nIn the zero-shot learning setting, CoT-T5 (3B &\n11B) outperforms Flan-T5 (3B & 11B) by +4.34%\nand +2.60% on average accuracy across 27 datasets\nfrom the Big Bench Hard (BBH) benchmark (Suz-\ngun et al., 2022) when evaluated with CoT prompt-\ning. During ablation experiments, we show that\nCoT fine-tuning T0 (3B) (Sanh et al., 2021) on a\nsubset of the CoT Collection, specifically 163 train-\ning tasks used in T0, shows a performance increase\nof +8.65% on average accuracy across 11 datasets\nfrom the P3 Evaluation benchmark. Moreover, we\ntranslate 80K instances of COT COLLECTION into\n5 different languages (French, Japanese, Korean,\nRussian, Chinese) and observe that CoT fine-tuning\nmT0 (3B) (Muennighoff et al., 2022) on each lan-\nguage results in 2x ∼10x performance improve-\nment on average accuracy across all 5 languages\nfrom the MGSM benchmark (Shi et al., 2022).\nIn the few-shot learning setting, where LMs must\nadapt to new tasks with a minimal number of in-\nstances, CoT-T5 (3B & 11B) exhibits a +2.24%\nand +2.37% improvement on average compared\nto using Flan-T5 (3B & 11B) as the base model\non 4 different domain-specific tasks2. Moreover, it\ndemonstrates +13.98% and +8.11% improvement\nover ChatGPT (OpenAI, 2022) and Claude (An-\nthropic, 2023) that leverages ICL with demonstra-\ntions up to the maximum input length.\nOur contributions are summarized as follows:\n• We introduce COT C OLLECTION , a new in-\nstruction dataset that includes 1.84 million ra-\ntionales across 1,060 tasks that could be used\nfor applying CoT fine-tuning to LMs.\n• With COT COLLECTION , we fine-tune Flan-\nT5, denoted as CoT-T5, which shows a non-\ntrivial boost in zero-shot and few-shot learn-\ning capabilities with CoT Prompting.\n• For ablations, we show that CoT fine-tuning\ncould improve the CoT capabilities of LMs\nin low-compute settings by using a subset\nof COT C OLLECTION and training on (1)\nsmaller number of tasks (T0 setting; 163 tasks)\nand (2) smaller amount of instances in 5 dif-\nferent languages (French, Japanese, Korean,\nRussian, Chinese; 80K instances).\n2We assess the efficacy of our approach on 4 domain-\nspecific datasets, two each from legal and medical fields,\nnamely including LEDGAR (Tuggener et al., 2020), Case\nHold (Zheng et al., 2021), MedNLI (Romanov and Shivade,\n2018), and PubMedQA (Jin et al., 2019). Each dataset is\nrepresented by 64 randomly chosen instances.\n2 Related Works\n2.1 Chain-of-Thought (CoT) Prompting\nWei et al. (2022b) propose Chain of Thought (CoT)\nPrompting, a technique that triggers the model to\ngenerate a rationale before the answer. By gener-\nating a rationale, large LMs show improved rea-\nsoning abilities when solving challenging tasks.\nKojima et al. (2022) show that by appending the\nphrase ‘Let’s think step by step’, large LMs could\nperform CoT prompting in a zero-shot setting. Dif-\nferent work propose variants of CoT prompting\nsuch as automatically composing CoT demonstra-\ntions (Zhang et al., 2022) and performing a fine-\ngrained search through multiple rationale candi-\ndates with a tree search algorithm (Yao et al., 2023).\nWhile large LMs could solve novel tasks with CoT\nPrompting, Chung et al. (2022) and Longpre et al.\n(2023) show that this effectiveness does not neces-\nsarily hold for smaller LMs. In this work, we aim\nto equip smaller LMs with the same capabilities by\ninstruction tuning on large amount of rationales.\n2.2 Improving Zero-shot Generalization\nPrevious work show that instruction tuning enables\ngeneralization to multiple unseen tasks (Wei et al.,\n2021; Sanh et al., 2021; Aribandi et al., 2021;\nOuyang et al., 2022; Wang et al., 2022b; Xu et al.,\n2022). Different work propose to improve instruc-\ntion tuning by enabling cross-lingual generaliza-\ntion (Muennighoff et al., 2022), improving label\ngeneralization capability (Ye et al., 2022), and train-\ning modular, expert LMs (Jang et al., 2023). Mean-\nwhile, a line of work shows that CoT fine-tuning\ncould improve the reasoning abilities of LMs on a\nsingle-seen task (Zelikman et al., 2022; Shridhar\net al., 2022; Ho et al., 2022; Fu et al., 2023). As a\nfollow-up study, we CoT fine-tune on 1,060 instruc-\ntion tasks and observe a significant improvement in\nterms of zero-shot generalization on multiple tasks.\n2.3 Improving Few-Shot Learning\nFor adapting LMs to new tasks with a few instances,\nrecent work propose advanced parameter efficient\nfine-tuning (PEFT) methods, where a small num-\nber of trainable parameters are added (Hu et al.,\n2021; Lester et al., 2021; Liu et al., 2021, 2022b;\nAsai et al., 2022; Liu et al., 2022c). In this work,\nwe show that a simple recipe of (1) applying\nLoRA (Hu et al., 2021) to a LM capable of per-\nforming CoT reasoning and (2) CoT fine-tuning on\na target task results in strong few-shot performance.\n12686\nScale\nFL AN - T5 ExQA\nFL AN - T5 Arithmetic\nFL AN - T5 MCQA\nFL AN - T5 NLI\nCo T Collection\n(1060 T asks,  1 . 84M instances)\nPr e viously a v ailable Co T r ationale data\n(9 T asks,  35K instances)\nFigure 1: An illustration of the overall task group and dataset source of where we obtained the instances to augment\nthe rationales in COT COLLECTION . Compared to the 9 datasets that provide publicly available rationales (included\nwithin ‘Flan-T5 ExQA’, ‘Flan-T5 Arithmetic’, ‘Flan-T5 MCQA’, ‘Flan-T5 NLI’ from the red box), we generate\n∼51.29 times more rationales (1.84 million rationales) and ∼117.78 times more task variants (1,060 tasks).\n3 The C OT COLLECTION\nDespite its effectiveness for CoT fine-tuning, ratio-\nnale data still remains scarce. To the best of our\nknowledge, recent work mostly rely on 9 publicly\navailable NLP datasets3 for fine-tuning with ratio-\nnales (Zelikman et al., 2022; Shridhar et al., 2022;\nChung et al., 2022; Ho et al., 2022; Longpre et al.,\n2023; Fu et al., 2023). This is due to the difficulty\nin gathering human-authored rationales (Kim et al.,\n2023). To this end, we create COT COLLECTION ,\nan instruction-tuning dataset that includes 1.84 mil-\nlion rationales augmented across 1,060 tasks4. In\nthis section, we explain the datasets we select to\naugment into rationales and how we perform the\noverall augmentation process.\nBroad Overview Given an input X = [I, z]\ncomposed of an instruction I, and an instance z\nalong with the answer y, we obtain a rationale r\nby applying in-context learning (ICL) with a large\nLM. Note that this differs from previous works\nwhich focused on generating new instances z using\nlarge LMs (West et al., 2022; Liu et al., 2022a;\nKim et al., 2022; Honovich et al., 2022; Wang\n3The 9 available datasets are QASC (Khot et al., 2020),\nAQuA (Amini et al., 2019), GSM8K (Cobbe et al., 2021),\nQED (Lamm et al., 2021), StrategyQA (Geva et al., 2021),\nSenseMaking (Wang et al., 2019), CREAK (Onoe et al., 2021),\ne-SNLI (Camburu et al., 2018), ECQA (Aggarwal et al., 2021).\n4Following Sanh et al. (2021), we use the notion of ‘task’\nreferring to each prompt applied to a dataset.\net al., 2022a; Taori et al., 2023; Chiang et al., 2023)\nwhile we extend it to generating new rationales r.\nSource Dataset Selection As a source dataset\nto extract rationales, we choose the Flan Collec-\ntion (Longpre et al., 2023), consisting of 1,836 di-\nverse NLP tasks from P3 (Sanh et al., 2021), Super-\nNaturalInstructions (Wang et al., 2022b), Flan (Wei\net al., 2021), and some additional dialogue & code\ndatasets. We choose 1,060 tasks, narrowing our\nfocus following the criteria as follows:\n• Generation tasks with long outputs are ex-\ncluded since the total token length of append-\ning r and y exceeds the maximum output to-\nken length (512 tokens) during training.\n• Datasets that are not publicly available such\nas DeepMind Coding Contents and Dr Re-\npair (Yasunaga and Liang, 2020) are excluded.\n• Datasets where the input and output do not\ncorrespond to each other in the huggingface\ndatasets (Lhoest et al., 2021) are excluded.\n• When a dataset appears in common across\ndifferent sources, we prioritize using the task\nfrom P3 first, followed by SNI, and Flan.\n• During preliminary experiments, we find that\nfor tasks such as sentiment analysis, sentence\n12687\ncompletion, coreference resolution, and word\ndisambiguation, rationales generated by large\nLMs are very short and uninformative. We\nexclude these tasks to prevent negative trans-\nfer during multitask learning (Aribandi et al.,\n2021; Jang et al., 2023).\nCreating Demonstrations for ICL We first\ncreate prompts to apply in-context learning (ICL)\nwith large LMs for augmenting the instances\nin the selected tasks with rationales. Preparing\ndemonstrations Dt for each task t is the most\nstraightforward, but it becomes infeasible to\nprepare demonstrations for each task as the number\nof tasks gets larger. Instead, we assign each task t\nto Tk, a family of tasks that shares a similar task\nformat such as multiple choice QA, closed book\nQA, and dialogue generation. Each family of tasks\nshare DTk , which consists of 6 ∼8 demonstrations.\nThese 6 ∼8 demonstrations for each task group\nTk is manually created by 3 of the authors in this\npaper. Specifically, given 136 instances sampled\nfrom Flan Collection, two annotators are assigned\nto write a rationale, and the other third annotator\nconducts an A/B testing between the two options.\nWe manually create DTk across k = 26 task\ngroups. We include the prompts for all of the\ndifferent task groups in Appendix D.\nRationale Augmentation We use the OpenAI\nCodex5 to augment rationales. Formally, given\n(Xt\ni , yt\ni), the ith instance of a task t, the goal is\nto generate corresponding rationale rt\ni. Note that\nduring preliminary experiments, we found that or-\ndering the label in front of the rationale within the\ndemonstration DTk was crucial to generate good\nquality rationales. We conjecture this is because\nordering the label in front of the rationale loosens\nthe need for the large LM to solve the underly-\ning task and only focus on generating a rationale.\nHowever, we also found that in some tasks such\nas arithmetic reasoning, large LMs fail to generate\ngood-quality rationales. To mitigate this issue, we\napply filtering to the augmented rationales. We\nprovide the criteria used for the filtering phase and\n5The use of Codex was largely due to limited academic\nbudget (OpenAI supported Codex with no cost for researchers\nup to June 2023). Moreover, other LLM services such as\nBard (Google, 2023) and Claude (Anthropic, 2023) were not\nsupported during the period of COT COLLECTION augmen-\ntation. To address the concern of reproducibility, analysis on\nquality of rationales from Codex, Bard and Claude is included\nin Appendix A,\n[Example 1]\n[Example 2]\n[Instruction and Question]\n[Answer]\n[Rationale]\nSkylar had stopped breathing but Lee [...]\nbeg the doctors to try again\nThe context of the situation is that Skylar has stopped\nbreathing and Lee is holding [...]\nThe answer is to beg the doctors to try again.\nGiven the context: Lee want to do what next?\n[Instruction and Question]\n[Answer]\n[Answer]\n[Rationale]\n[Rationale]\nDo you think the right answer to the question\n[Example 9]\n[Instruction and Question]\nI am testing my students’ logic. [...]\nthrough the grass because (A) more friction, (b) less friction.\nA\nNo\nAlcoholic fermentation is a process that [...] will not produce\n[...]\nenough energy to power. Therefore, the answer is No.\n“what can run alcoholic fermentation of [...]?”\nFigure 2: MCQA Prompt used to augment rationales\nfrom P3 dataset. Through ICL, the large LM generates\na rationale that is conditioned on the ground-truth label.\nthe filtered cases at Appendix B. Also, we include\nanalysis of the diversity and quality of COT COL-\nLECTION compared to the existing 9 CoT tasks and\nhuman-authored rationales in Appendix A.\n4 Experiments\nFor our main experiments, we use Flan-T5 (Chung\net al., 2022) as our base model, and obtain CoT-\nT5 by CoT fine-tuning on the COT COLLECTION .\nFormally, given Xt\ni , the goal of CoT fine-tuning is\nto sequentially generate the rationale rt\ni and answer\nyt\ni. To indicate that rt\ni should be generated before\nyt\ni, the trigger phrase ‘Let’s think step by step’ is\nadded during both training and evaluation. We\nmostly follow the details for training and evaluation\nfrom Chung et al. (2022), and provide additional\ndetails in Appendix C. In this section, we show\nhow training on COT COLLECTION enhances zero-\nshot generalization capabilities (Section 4.2) and\nfew-shot adaptation capabilities (Section 4.3).\n4.1 Evaluation\nWe evaluate under two different evaluation meth-\nods: Direct Evaluationand CoT Evaluation. For\nDirect Evaluation on classification tasks, we fol-\nlow previous works using verbalizers, choosing the\noption with the highest probability through com-\nparison of logit values (Schick and Schütze, 2021;\n12688\nSanh et al., 2021; Ye et al., 2022; Jang et al., 2023),\nand measure the accuracy. For generation tasks,\nwe directly compare the LM’s prediction with the\nanswer and measure the EM score.\nWhen evaluating with CoT Evaluation, smaller\nLMs including Flan-T5 often do not generate any\nrationales even with the trigger phrase ‘Let’s think\nstep by step’. Therefore, we adopt a hard constraint\nof requiring the LM to generate rt\ni with at least\na minimum length of 8 tokens. In classification\ntasks, we divide into two steps where the LM first\ngenerates rt\ni, and then verbalizers are applied with\na indicator phrase ‘[ANSWER]’ inserted between\nrt\ni and the possible options. For generation tasks,\nwe extract the output coming after the indicator\nphrase. Accuracy metric is used for classification\ntasks while EM metric is used for generation tasks.\n4.2 Zero-shot Generalization\nIn this subsection, we show how training withCOT\nCOLLECTION could effectively improve the LM’s\nability to solve unseen tasks. We have three differ-\nence experimental set-ups, testing different aspects:\nSetup #1: training on the entire 1060 tasks in COT\nCOLLECTION and evaluating the reasoning capabil-\nities of LMs with the Bigbench Hard (BBH) bench-\nmark (Suzgun et al., 2022), Setup #2: training only\non 163 tasks that T0 (Sanh et al., 2021) used for\ntraining (a subset of the COT COLLECTION ), and\nevaluating the linguistic capabilities of LMs with\nthe P3 evaluation benchmark (Sanh et al., 2021),\nand Setup #3: training with a translated, subset ver-\nsion of COT COLLECTION for each five different\nlanguages and evaluating how LMs could perform\nCoT reasoning in multilingual settings using the\nMGSM benchmark (Shi et al., 2022).\nSetup #1: CoT Fine-tuning with 1060 CoT Tasks\nWe first perform experiments with our main model,\nCoT-T5, by training Flan-T5 on the entire COT\nCOLLECTION and evaluate on the BBH bench-\nmark (Suzgun et al., 2022). In addition to eval-\nuating Flan-T5, we compare the performances\nof different baselines such as (1) T5-LM (Raffel\net al., 2020): the original base model of Flan-T5,\n(2) T0 (Sanh et al., 2021): an instruction-tuned\nLM trained with P3 instruction dataset, (3) Tk-\nInstruct (Wang et al., 2022b): an instruction-tuned\nLM trained with SNI instruction dataset, and (4)\nGPT-3 (Brown et al., 2020): a pre-trained LLM\nwith 175B parameters. For ablation purposes, we\nalso train T5-LM with COT C OLLECTION (de-\nMethod CoT Direct Total Avg\nT5-LM-3B 26.68 26.96 26.82\nT0-3B 26.64 27.45 27.05\nTK-INSTRUCT -3B 29.86 29.90 29.88\nTK-INSTRUCT -11B 33.60 30.71 32.16\nT0-11B 31.83 33.57 32.70\nFLAN -T5-3B 34.06 37.14 35.60\nGPT-3 (175B) 38.30 33.60 38.30\nFLAN -T5-11B 38.57 40.99 39.78\nT5-3B + C OT FT 37.95 35.52 36.74\nCOT-T5-3B 38.40 36.18 37.29\nT5-11B + C OT FT 40.02 38.76 39.54\nCOT-T5-11B 42.20 42.56 42.38\nTable 1: Evaluation performance on all the 27 unseen datasets\nfrom BBH benchmark, including generation tasks. All eval-\nuations are held in a zero-shot setting. The best comparable\nperformances are bolded and second best underlined.\nnoted as ‘T5 + CoT FT’). Note that FLAN Col-\nlection includes 15 million instances, hence ∼8\ntimes larger compared to our COT COLLECTION .\nThe results on BBH benchmark are shown across\nTable 1 and Table 2. In Table 1, CoT-T5 (3B &\n11B) achieves a +4.34% and +2.60% improvement\nover Flan-T5 (3B & 11B) with CoT Evaluation.\nSurprisingly, while CoT-T5-3B CoT performance\nimproves +4.34% with the cost of 0.96% degra-\ndation in Direct Evalution, CoT-T5-11B’s Direct\nEvaluation performance even improves, resulting\nin a +2.57% total average improvement. Since\nCOT C OLLECTION only includes instances aug-\nmented with rationales, these results show that CoT\nfine-tuning could improve the LM’s capabilities re-\ngardless of the evaluation method. Also, T5-3B\n+ C OT FT and T5-11B + C OT FT outperforms\nFLAN -T5-3B and FLAN -T5-11B by a +1.45%\nand +3.89% margin, respectively, when evaluated\nwith CoT evaluation. Moreover, T5-3B + C OT\nFINE -TUNING outperforms ∼4 times larger mod-\nels such as T0-11B and Tk-Instruct-11B in both\nDirect and CoT Evaluation. The overall results in-\ndicate that (1) CoT fine-tuning on a diverse number\nof tasks enables smaller LMs to outperform larger\nLMs and (2) training with FLAN Collection and\nCoT Collection provides complementary improve-\nments to LMs under different evaluation methods;\nCoT-T5 obtains good results across both evaluation\nmethods by training on both datasets.\nIn Table 2, CoT-T5-11B obtains same or better\nresults on 15 out of 23 tasks when evaluated with\nDirect evaluation, and 17 out of 23 tasks when\nevaluated with CoT Evaluation compared to Flan-\n12689\nTask\nC O T-T5-11B FLAN-T5-11B V ICUNA -13B C HAT GPT C ODEX GPT-4\nCoT Direct CoT Direct Direct Direct Direct Direct\nB OOLEAN E XPRESSIONS 65.6 59.2 51.6 56.8 40.8 82.8 88.4 77.6\nC AUSAL J UDGMENT 60.4 60.2 58.3 61.0 42.2 57.2 63.6 59.9\nD ATE U NDERSTANDING 52.0 51.0 46.8 54.8 10.0 42.8 63.6 74.8\nD ISAMBIGUATION QA 63.4 68.2 63.2 67.2 18.4 57.2 67.2 69.2\nF ORMAL F ALLACIES 51.2 55.2 54.4 55.2 47.2 53.6 52.4 64.4\nG EOMETRIC S HAPES 22.0 10.4 12.4 21.2 3.6 25.6 32.0 40.8\nH YPERBATON 65.2 64.2 55.2 70.8 44.0 69.2 60.4 62.8\nL OGICAL D EDUCTION (5) 48.2 54.4 51.2 53.6 4.8 38.8 32.4 66.8\nL OGICAL D EDUCTION (7) 52.4 60.6 57.6 60.0 1.2 39.6 26.0 66.0\nL OGICAL D EDUCTION (3) 55.4 75.0 66.4 74.4 16.8 60.4 52.8 94.0\nM OVIE R ECOMMENDATION 44.6 52.8 32.4 36.4 43.4 55.4 84.8 79.5\nN AVIGATE 59.0 60.0 60.8 61.6 46.4 55.6 50.4 68.8\nP ENGUINS IN A T ABLE 39.1 41.8 41.8 41.8 15.1 45.9 66.4 76.7\nR EASONING C OLORED O BJ . 32.6 33.2 22.8 23.2 12.0 47.6 67.6 84.8\nR UIN N AMES 42.8 41.6 31.6 34.4 15.7 56.0 75.2 89.1\nS ALIENT T RANS E RR . 43.8 49.2 35.6 49.2 2.0 40.8 62.0 62.4\nS NARKS 67.7 66.2 59.5 70.2 28.1 59.0 61.2 87.6\nS PORTS U NDERSTANDING 64.8 66.4 56.0 60.0 48.4 79.6 72.8 84.4\nT EMPORAL S EQUENCES 27.4 28.8 24.4 28.8 16.0 35.6 77.6 98.0\nT RACKING S HUFF O BJ . (5) 20.0 13.2 19.6 15.2 9.2 18.4 20.4 25.2\nT RACKING S HUFF O BJ . (7) 18.4 9.6 13.2 12.0 5.6 15.2 14.4 25.2\nT RACKING S HUFF O BJ . (3) 41.8 31.2 28.8 24.4 23.2 31.6 37.6 42.4\nW EB OF L IES 57.0 51.6 52.8 50.0 41.2 56.0 51.6 49.6\nA VERAGE 47.60 48.00 43.32 47.05 23.30 48.90 52.80 67.40\nTable 2: Evaluation performance on 23 unseen classification datasets from BBH benchmark. Scores of Vicuna, ChatGPT, Codex\n(teacher model of COT-T5), GPT-4 are obtained from Chung et al. (2022) and Mukherjee et al. (2023). Evaluations are held in a\nzero-shot setting. The best comparable performances are bolded and second best underlined among the open-sourced LMs.\nMethod Natural Language Inference Sentence Completion Coreference Resolut. WSDTotal AvgRTE CB AN. R1 AN. R2 AN. R3 COPA Hellasw. StoryC. Winogr. WSC WiC\nT5-3B (Raffel et al., 2020) 53.03 34.34 32.89 33.76 33.82 54.88 27.00 48.16 50.64 54.09 50.3042.99T0-3B (SANH ET AL., 2021) 60.61 48.81 35.10 33.27 33.52 75.13 27.18 84.91 50.91 65.00 51.2751.43ROE-3B (JANG ET AL., 2023) 64.01 43.57 35.49 34.64 31.22 79.25 34.60 86.33 61.6062.21 52.9753.48KIC-770M (PAN ET AL., 2022) 74.00 67.90 36.30 35.00 37.60 85.30 29.60 94.40 55.30 65.4052.40 57.56FLIPPED-3B (YE ET AL., 2022) 71.05 57.74 39.99 37.05 37.73 89.88 41.6495.88 58.56 58.37 50.42 58.03GPT-3 (175B) (BROWN ET AL., 2020) 63.50 46.40 34.60 35.40 34.5091.00 78.9083.20 70.20 65.4045.92 59.00T0-11B (SANH ET AL., 2021) 80.8370.1243.56 38.6841.2690.02 33.58 92.40 59.94 61.45 56.5860.76\nT5-3B + COT FT - EVAL W/DIRECT69.96 58.69 37.58 36.00 37.44 84.59 40.92 90.47 55.40 64.33 51.5356.99T0-3B + COT FT - EVAL W/DIRECT80.7965.00 39.49 35.13 38.58 88.27 41.04 92.13 56.4065.9653.60 59.67T5-3B + COT FT - EVAL W/ COT 80.61 69.17 40.24 36.67 40.13 90.10 41.08 93.00 56.47 55.1056.7359.94T0-3B + COT FT - EVAL W/ COT 80.25 72.6241.7137.2241.8990.8839.50 94.47 57.47 50.58 54.27 60.08\nTable 3: Evaluation performance on 11 different unseen P3 dataset (Sanh et al., 2021) categorized into 4 task categories. We\nreport the direct performance of the baselines since they were not CoT fine-tuned on instruction data. The best comparable\nperformances are bolded and second best underlined. We exclude Flan-T5 and CoT-T5 since they were trained on the unseen\ntasks (tasks from FLAN and SNI overlap with the P3 Eval datasets), breaking unseen task assumption.\nT5-11B. Interestingly, Vicuna (Chiang et al., 2023),\na LM trained on long-form dialogues between users\nand GPT models, perform much worse compared\nto both CoT-T5 and Flan-T5. We conjecture that\ntraining on instruction datasets from existing aca-\ndemic benchmarks consisting CoT Collection and\nFlan Collection is more effective in enabling LMs\nto solve reasoning tasks compared to chat LMs.\nSetup #2: CoT Fine-tuning with 163 CoT Tasks\n(T0 Setup) To examine whether the effect of\nCoT fine-tuning is dependent on large number of\ntasks and instances, we use the P3 training subset\nfrom the COT C OLLECTION consisted of 644K\ninstances from 163 tasks, and apply CoT fine-\ntuning to T0 (3B) (Sanh et al., 2021) and T5-LM\n(3B) (Raffel et al., 2020). Note that T0 is trained\nwith 12M instances, hence ∼18.63 times larger.\nThen, we evaluate on the P3 evaluation benchmark\nwhich consists of 11 different NLP datasets. In\naddition to the baselines from the previous section\n(T5-LM, T0, and GPT-3), we also include LMs\nthat are trained on the same T0 setup for compari-\nson such as, (1) RoE (Jang et al., 2023): a modu-\nlar expert LM that retrieves different expert mod-\nels depending on the unseen task, (2) KiC (Pan\net al., 2022): a retrieval-augmented model that\n12690\nis instruction-tuned to retrieve knowledge from a\nKB memory, and (3) Flipped (Ye et al., 2022): an\ninstruction-tuned model that is trained to generate\nthe instruction in order to resolve the LM over-\nfitting to the output label as baseline models.\nThe results are shown in Table 3. Surprisingly,\nT5-3B + C OT FT outperforms T0-3B by a +8.24%\nmargin when evaluated with CoT Evaluation, while\nusing ∼18.63 times less instances. This supports\nthat CoT fine-tuning is data efficient, being effec-\ntive even with less number of instances and tasks.\nMoreover, T0-3B + C OT FT improves T0-3B by\n+8.65% on average accuracy. When compared with\nT0-11B with ∼4 times more number of parameters,\nit achieves better performance at sentence comple-\ntion, and word sense disambiguation (WSD) tasks,\nand obtains similar performances at natural lan-\nguage inference and coreference resolution tasks.\nSetup #3: Multilingual Adaptation with CoT\nFine-tuning In previous work, Shi et al. (2022)\nproposed MGSM, a multilingual reasoning bench-\nmark composed of 10 different languages. In this\nsubsection, we conduct a toy experiment to exam-\nine whether CoT fine-tuning could enable LMs to\nreason step-by-step in multilingual settings as well,\nusing a subset of 5 languages (Korean, Russian,\nFrench, Chineses, Japanese) from MGSM.\nIn Table 4, current smaller LMs can be divided\ninto three categories: (1) Flan-T5, a LM that is CoT\nfine-tuned with mostly English instruction data, (2)\nMT5 (Xue et al., 2021), a LM pretrained on diverse\nlanguages, but isn’t instruction tuned or CoT fine-\ntuned, (3) MT0 (Muennighoff et al., 2022), a LM\nthat is instruction-tuned on diverse languages, but\nisn’t CoT fine-tuned. In relatively underrepresented\nlanguages such as Korean, Japanese, and Chinese,\nall three LMs get close to zero accuracy.\nA natural question arises whether training a mul-\ntilingual LM that could reason step-by-step on dif-\nferent languages is viable. As a preliminary re-\nsearch, we examine whether CoT Fine-tuning on a\nsingle language with a small amount of CoT data\ncould enable LMs to avoid achieving near zero\nscore such as Korean, Chinese and Japanese sub-\nsets of MGSM. Since there is no publicly available\nmultilingual instruction dataset, we translate 60K\n∼80K instances from COT COLLECTION for each\n5 languages using ChatGPT (OpenAI, 2022), and\nCoT fine-tune mT5 and mT0 on each of them.\nThe results are shown in Table 4. Across all\nthe 5 different languages, CoT fine-tuning brings\nMethod ko ru fr zh ja\nFLAN -T5-3B 0.0 2.8 7.2 0.0 0.0\nFLAN -T5-11B 0.0 5.2 13.2 0.0 0.0\nMT5-3.7B 0.0 1.2 2.0 0.8 0.8\nMT0-3.7B 0.0 4.8 7.2 1.6 2.4\nGPT-3 (175B) 0.0 4.4 10.8 6.8 0.8\nMT5-3.7B + C OT FT 3.2 6.8 9.6 6.0 7.6\nMT0-3.7B + C OT FT 7.6 10.4 15.6 11.2 11.0\nTable 4: Evaluation performance on MGSM benchmark (Shi\net al., 2022) across 5 languages (Korean, Russian, French,\nChinese, Japanese, respectively). All evaluations are held in a\nzero-shot setting with CoT Evaluation except GPT-3 using a\n6-Shot prompt for ICL. The best comparable performances are\nbolded and second best underlined. Note that ‘MT5-3.7B +\nCOT FT ’ and ‘MT0-3.7B + C OT FT ’ are trained on a single\nlanguage instead of multiple languages as mT5 and mT0.\nabout non-trivial gains in performance. Even for\nrelatively low-resource languages such as Korean\nJapanese, and Chinese, CoT fine-tuning on the spe-\ncific language allows the underlying LM to per-\nform mathematical reasoning in the target language,\nwhich are considered very difficult (Shi et al.,\n2022). Considering that only a very small num-\nber of instances were used for language-specific\nadaptation (60k-80k), CoT fine-tuning shows po-\ntential for efficient language adaptation.\nHowever, it is noteworthy that we limited our\nsetting to training/evaluating on a single target lan-\nguage, without exploring the cross-lingual transfer\nof CoT capabilities among varied languages. The\nchief objective of this experimentation was to ascer-\ntain if introducing a minimal volume of CoT data\ncould facilitate effective adaptation to the target\nlanguage, specifically when addressing reasoning\nchallenges. Up to date, no hypothesis has sug-\ngested that training with CoT in various languages\ncould enable cross-lingual transfer of CoT abilities\namong different languages. We identify this as a\npromising avenue for future exploration.\n4.3 Few-shot Generalization\nIn this subsection, we show how CoT-T5 performs\nin a few-shot adaptation setting where a limited\nnumber of instances from the target task can be\nused for training, which is sometimes more likely\nin real-world scenarios.\nDataset Setup We choose 4 domain-specific\ndatasets from legal and medical domains includ-\ning LEDGAR (Tuggener et al., 2020), Case\nHold (Zheng et al., 2021), MedNLI (Romanov and\nShivade, 2018), and PubMedQA (Jin et al., 2019).\nTo simulate a few-shot setting, we randomly sam-\nple 64 instances from the train split of each dataset.\n12691\nMethod #Train Param Ledgar Case Hold MedNLI PubmedQA Total Avg\nFlan-T5-3B + Full FT. 2.8B 52.60 61.40 66.82 66.28 61.78\nFlan-T5-3B + Full CoT FT. 2.8B 53.60 58.80 65.89 65.89 61.05\nCoT-T5-3B + Full CoT FT. (Ours) 2.8B 51.90 60.60 67.16 68.12 61.95\nFlan-T5-3B + LoRA FT. 2.35M 53.20 58.80 61.60 67.18 60.19\nFlan-T5-3B + LoRA CoT FT. 2.35M 51.20 61.60 62.59 66.06 60.36\nCoT-T5-3B + LoRA CoT FT. (Ours) 2.35M 54.80 63.60 68.00 69.66 64.02\nFlan-T5-11B + LoRA FT. 4.72M 55.30 64.90 75.91 70.25 66.59\nFlan-T5-11B + LoRA CoT FT. 4.72M 52.10 65.50 71.63 71.60 65.21\nCoT-T5-11B + LoRA CoT FT. (Ours) 4.72M 56.10 68.30 78.02 73.42 68.96\nClaude (Anthropic, 2023) + ICL 0 55.70 57.20 75.94 54.58 60.85\nClaude (Anthropic, 2023) + CoT PT. 0 34.80 43.60 76.51 52.06 51.74\nChatGPT (OpenAI, 2022) + ICL 0 51.70 32.10 70.53 65.59 54.98\nChatGPT (OpenAI, 2022) + CoT PT. 0 51.00 18.90 63.71 25.22 39.70\nTable 5: Evaluation performance on 4 domain-specific datasets. FT. denotes Fine-tuning, COT FT. denotes CoT\nfine-tuning, and COT PT. denotes CoT Prompting. The best comparable performances are bolded and second best\nunderlined. For a few-shot adaptation, we use 64 randomly sampled instances from each dataset.\nWe report the average accuracy across 3 runs with\ndifferent random seeds. We augment rationales\nfor the 64 training instances using the procedure\ndescribed in Section 3 for the rationale augmenta-\ntion phase, utilizing the MCQA prompt from P3\ndataset. In an applied setting, practitioners could\nobtain rationales written by human experts.\nTraining Setup We compare Flan-T5 & CoT-T5,\nacross 3B and 11B scale and explore 4 different ap-\nproaches for few-shot adaptation: (1) regular fine-\ntuning, (2) CoT fine-tuning, (3) LoRA fine-tuning,\nand (4) LoRA CoT fine-tuning. When applying\nLora, we use a rank of 4 and train for 1K steps fol-\nlowing Liu et al. (2022b). This results in training\n2.35M parameters for 3B scale models and 4.72M\nparameters for 11B scale models. Also, we include\nClaude (Anthropic, 2023) and ChatGPT (OpenAI,\n2022) as ICL baselines by appending demonstra-\ntions up to maximum context length6. Specifically,\nFor CoT prompting, the demonstrations are sam-\npled among 64 augmented rationales are used.\nEffect of LoRA The experimental results are\nshown in Table 5. Overall, CoT fine-tuning CoT-\nT5 integrated with LoRA obtains the best results\noverall. Surprisingly for Flan-T5, applying full\nfine-tuning obtains better performance compared\nto its counterpart using LoRA fine-tuning. How-\never, when using CoT-T5, LoRA achieves higher\nperformance compared to full fine-tuning. We con-\njecture this to be the case because introducing only\n6Full context length was 4k tokens for ChatGPT and 9k\ntokens for Claude.\na few parameters enables CoT-T5 to maintain the\nCoT ability acquired during CoT fine-tuning.\nFine-tuning vs. CoT Fine-tuning While CoT\nfine-tuning obtains similar or lower performance\ncompared to regular fine-tuning in Flan-T5, CoT-\nT5 achieves higher performance with CoT fine-\ntuning compared to Flan-T5 regular fine-tuning.\nThis results in CoT-T5 in combination with CoT\nfine-tuning showing the best performance in few-\nshot adaptation setting.\nFine-tuning vs. ICL Lastly, fine-tuning meth-\nods obtain overall better results compared to ICL\nmethods utilizing much larger, proprietary LLMs.\nWe conjecture this to be the case due to the long\ninput length of legal and medical datasets, making\nappending all available demonstrations (64) impos-\nsible. While increasing the context length could\nserve as a temporary solution, it would still mean\nthat the inference time will increase quadratically\nin proportion to the input length, which makes ICL\ncomputationally expensive.\n5 Analysis of of CoT Fine-tuning\nIn this section, we conduct experiments to address\nthe following two research questions:\n• For practitioners, is it more effective to aug-\nment CoT rationales across diverse tasks or\nmore instances with a fixed number of tasks?\n• During CoT fine-tuning, does the LM main-\ntain its performance on in-domain tasks with-\nout any catastrophic forgetting?\n12692\nBBH Peromance (CoT Evaluation)\n9 CoT Data\n(180K instances)\nCoT Collection\n(10K instances)\nCoT Collection\n(100K instances)\nCoT Collection\n(1.84M instances)\n40.33\n42.04\n44.76\n47.60\nBBH Performance (CoT Eval)\nbased on Number of T asks / Instances\nFigure 3: Scaling plot of increasing the number of in-\nstances within the COT COLLECTION compared to us-\ning the existing 9 CoT datasets. Even with less number\nof instances, maintaining a wider range of tasks is cru-\ncial to improve the CoT abilities of an underlying LLM.\n5.1 Scaling the number of tasks & instances\nIn our main experiments, we used a large number\nof instances (1.84M) across a large number of tasks\n(1,060) to apply CoT fine-tuning. A natural ques-\ntion arises: “Is it more effective to increase the\nnumber of tasks or the number of instances?” To\naddress this question, we conduct an experiment\nof randomly sampling a small number of instances\nwithin the COT COLLECTION and comparing the\nBBH performance with (1) a baseline that is only\nCoT fine-tuned with the existing 9 CoT tasks and\n(2) COT-T5 that fully utilizes all the 1.84M in-\nstances. Specifically, we sample 10K, 100K in-\nstances within the COT COLLECTION and for the 9\nCoT tasks, we fully use all the 180K instances. As\nCOT-T5, we use Flan-T5 as our base model and\nuse the same training configuration and evaluation\nsetting (CoT Eval) during our experiments.\nThe results are shown in Figure 3, where sur-\nprisingly, only using 10K instances across 1,060\ntasks obtains better performance compared to us-\ning 180K instances across 9 tasks. This shows that\nmaintaining a wide range of tasks is more crucial\ncompared to increasing the number of instances.\n5.2 In-domain Task Accuracy of CoT-T5\nIt is well known that LMs that are fine-tuned on a\nwide range of tasks suffer from catastrophic forget-\nting (Chen et al., 2020; Jang et al., 2021, 2023),\na phenomenon where an LM improves its per-\nformance on newly learned tasks while the per-\nformance on previously learned tasks diminishes.\nANLI-R1 ANLI-R3 RTE WinograndeANLI-R2\nIn-domain T ask Accuracy (CoT Evaluation)\nIn-domain T ask Accuracy (CoT Eval)\nFigure 4: In-domain task accuracy with CoT evaluation.\nCoT Fine-tuning with the COT COLLECTION also im-\nproves accuracy on in-domain tasks as well.\nWhile COT-T5 uses the same tasks as its base\nmodel (Flan-T5), we also check whether CoT fine-\ntuning on a wide range of tasks could possibly harm\nperformance. For this purpose, we use the test set\nof 5 tasks within the COT COLLECTION , namely\nANLI-R1, ANLI-R2, ANLI-R3, RTE, and Wino-\ngrande. Note that this differs with the Setup #2 in\nthe main experiments in that we use different base\nmodels (T0 vs Flan-T5), and the tasks are already\nused for CoT fine-tuning.\nResults are shown in Figure 4, where COT-T5\nconsistently improves in-domain accuracy on the\nlearned tasks as well. However, we conjecture that\nthis is because we used the exact same task that\nFlan-T5 used to CoT fine-tuned COT-T5. Adding\nadditional tasks that were not used to train Flan-T5\nand COT-T5 could show different results, and we\nleave additional exploration of catastrophic forget-\nting during CoT fine-tuning to future work.\n6 Conclusion\nIn this work, we show that augmenting rationales\nfrom an instruction tuning data using LLMs (Open\nAI Codex), and CoT fine-tuning could improve\nthe reasoning capabilities of smaller LMs. Specif-\nically, we construct COT C OLLECTION , a large-\nscale instruction-tuning dataset with 1.84M CoT\nrationales extracted across 1,060 NLP tasks. With\nour dataset, we CoT fine-tune Flan-T5 and obtain\nCoT-T5, which shows better zero-shot generaliza-\ntion performance and serves as a better base model\nwhen training with few number of instances. We\nhope COT COLLECTION could be beneficial in the\ndevelopment of future strategies for advancing the\ncapabilities of LMs with CoT fine-tuning.\n12693\nAcknowledgments\nThis work was partly supported by Institute of In-\nformation & communications Technology Planning\n& Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (No.2022-0-00264, Compre-\nhensive Video Understanding and Generation with\nKnowledge-based Deep Logic Neural Network,\n50%; No.2021-0-02068, Artificial Intelligence In-\nnovation Hub, 20%) and KAIST-NA VER Hyper-\ncreative AI Center.\nLimitations\nRecently, there has been a lot of focus on distill-\ning the ability to engage in dialogues with long-\nform outputs in the context of instruction follow-\ning (Taori et al., 2023; Chiang et al., 2023). Since\nour model COT-T5 is not trained to engage in di-\nalogues with long-form responses from LLMs, it\ndoes not necessarily possess the ability to be ap-\nplied in chat applications. In contrast, our work\nfocuses on improving the zero-shot and few-shot\ncapabilities by training on academic benchmarks\n(COT COLLECTION , Flan Collection), where LMs\ntrained with chat data lack on. Utilizing both long-\nform chat data from LLMs along with instruction\ndata from academic tasks has been addressed in\nfuture work (Wang et al., 2023). Moreover, vari-\nous applications have been introduced by using the\nFEEDBACK COLLECTION to train advanced chat\nmodels 7.\nAlso, since COT-T5 uses Flan-T5 as a base\nmodel, it doesn’t have the ability to perform step-\nby-step reasoning in diverse languages. Exploring\nhow to efficiently and effectively train on CoT data\nfrom multiple languages is also a promising and\nimportant line of future work. While Shi et al.\n(2022) has shown that large LMs with more than\n100B parameters have the ability to write CoT in\ndifferent languages, our results show that smaller\nLMs show nearly zero accuracy when solving math\nproblems in different languages. While CoT fine-\ntuning somehow shows slight improvement, a more\ncomprehensive strategy of integrating the ability to\nwrite CoT in diverse language would hold crucial.\nIn terms of reproducibility, it is extremely con-\ncerning that proprietary LLMs shut down such as\nthe example of the Codex, the LLM we used for ra-\ntionale augmentation. We provide additional analy-\nsis on how different LLMs could be used for this\n7https://huggingface.co/aiplanet/\neffi-13b\nprocess in Appendix A. Also, there is room of im-\nprovement regarding the quality of our dataset by\nusing more powerful LLMs such as GPT-4 and bet-\nter prompting techniques such as Tree of Thoughts\n(ToT) (Yao et al., 2023). This was examined by\nlater work in Mukherjee et al. (2023) which used\nGPT-4 to augment 5 million rationales and Yue et al.\n(2023) which mixed Chain-of-Thoughts and Pro-\ngram of Thoughts (PoT) during fine-tuning. Using\nrationales extracted using Tree of Thoughts (Yao\net al., 2023) could also be explored in future work.\nReferences\nShourya Aggarwal, Divyanshu Mandowara, Vishwajeet\nAgrawal, Dinesh Khandelwal, Parag Singla, and Di-\nnesh Garg. 2021. Explanations for commonsenseqa:\nNew dataset and models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3050–3065.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. Mathqa: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2357–2367.\nAnthropic. 2023. Claude. https:\n//www.anthropic.com/index/\nintroducing-claude.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-\nglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo\nNi, et al. 2021. Ext5: Towards extreme multi-\ntask scaling for transfer learning. arXiv preprint\narXiv:2111.10952.\nAkari Asai, Mohammadreza Salehi, Matthew E Peters,\nand Hannaneh Hajishirzi. 2022. Attentional mix-\ntures of soft prompt tuning for parameter-efficient\nmulti-task knowledge sharing. arXiv preprint\narXiv:2205.11961.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language expla-\nnations. Advances in Neural Information Processing\nSystems, 31.\n12694\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870–7881.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\nTushar Khot. 2023. Specializing smaller language\nmodels towards multi-step reasoning. arXiv preprint\narXiv:2301.12726.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nOlga Golovneva, Moya Chen, Spencer Poff, Martin\nCorredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,\nand Asli Celikyilmaz. 2022. Roscoe: A suite of\nmetrics for scoring step-by-step reasoning. arXiv\npreprint arXiv:2212.07919.\nGoogle. 2023. Bard. https://\nblog.google/technology/ai/\nbard-google-ai-search-updates/ .\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\nLarge language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor. arXiv\npreprint arXiv:2212.09689.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nJoel Jang, Seungone Kim, Seonghyeon Ye, Doyoung\nKim, Lajanugen Logeswaran, Moontae Lee, Kyung-\njae Lee, and Minjoon Seo. 2023. Exploring the bene-\nfits of training expert language models over instruc-\ntion tuning. arXiv preprint arXiv:2302.03202.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun Kim, Stanley Jungkyu\nChoi, and Minjoon Seo. 2021. Towards contin-\nual knowledge learning of language models. arXiv\npreprint arXiv:2110.03215.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2567–2577.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020. Qasc: A\ndataset for question answering via sentence compo-\nsition. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 8082–8090.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu,\nYoungjae Yu, Pei Zhou, Ronan Le Bras, Malihe\nAlikhani, Gunhee Kim, Maarten Sap, et al. 2022.\nSoda: Million-scale dialogue distillation with so-\ncial commonsense contextualization. arXiv preprint\narXiv:2212.10465.\nSeungone Kim, Se June Joo, Yul Jang, Hyungjoo Chae,\nand Jinyoung Yeo. 2023. Cotever: Chain of thought\nprompting annotation toolkit for explanation verifica-\ntion. arXiv preprint arXiv:2303.03628.\nNikita Kitaev, Steven Cao, and Dan Klein. 2019. Multi-\nlingual constituency parsing with self-attention and\npre-training. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3499–3505.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2676–2686.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\n12695\nMatthew Lamm, Jennimaria Palomaki, Chris Alberti,\nDaniel Andor, Eunsol Choi, Livio Baldini Soares,\nand Michael Collins. 2021. Qed: A framework\nand dataset for explanations in question answering.\nTransactions of the Association for computational\nLinguistics, 9:790–806.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, et al. 2021. Datasets: A commu-\nnity library for natural language processing. arXiv\npreprint arXiv:2109.02846.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and\nYangqiu Song. 2023. Multi-step jailbreaking privacy\nattacks on chatgpt. arXiv preprint arXiv:2304.05197.\nAlisa Liu, Swabha Swayamdipta, Noah A Smith, and\nYejin Choi. 2022a. Wanli: Worker and ai collabora-\ntion for natural language inference dataset creation.\narXiv preprint arXiv:2201.05955.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin A Raf-\nfel. 2022b. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Ad-\nvances in Neural Information Processing Systems ,\n35:1950–1965.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022c. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nDavid Mhlanga. 2023. Open ai in education, the re-\nsponsible and ethical use of chatgpt towards lifelong\nlearning. Education, the Responsible and Ethical\nUse of ChatGPT Towards Lifelong Learning (Febru-\nary 11, 2023).\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. Metaicl: Learning to learn\nin context. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2791–2809.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-\nhar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. 2023. Orca: Progressive learning from\ncomplex explanation traces of gpt-4. arXiv preprint\narXiv:2306.02707.\nYasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and\nGreg Durrett. 2021. Creak: A dataset for com-\nmonsense reasoning over entity knowledge. arXiv\npreprint arXiv:2109.01653.\nOpenAI. 2022. ChatGPT. https://openai.com/\nblog/chatgpt.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nXiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu,\nDong Yu, and Jianshu Chen. 2022. Knowledge-in-\ncontext: Towards knowledgeable semi-parametric\nlanguage models. arXiv preprint arXiv:2210.16433.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from natural language inference in the clini-\ncal domain. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1586–1596.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269.\n12696\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush V osoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.\nLanguage models are multilingual chain-of-thought\nreasoners. arXiv preprint arXiv:2210.03057.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2022. Distilling multi-step reasoning ca-\npabilities of large language models into smaller mod-\nels via semantic decompositions. arXiv preprint\narXiv:2212.00193.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv\npreprint arXiv:2210.09261.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022. Unify-\ning language learning paradigms. arXiv preprint\narXiv:2205.05131.\nDon Tuggener, Pius V on Däniken, Thomas Peetz, and\nMark Cieliebak. 2020. Ledgar: a large-scale multi-\nlabel corpus for text classification of legal provisions\nin contracts. In Proceedings of the Twelfth Language\nResources and Evaluation Conference, pages 1235–\n1241.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan\nLi, and Tian Gao. 2019. Does it make sense? and\nwhy? a pilot study for sense making and explana-\ntion. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4020–4026.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022a. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith,\nIz Beltagy, et al. 2023. How far can camels go?\nexploring the state of instruction tuning on open re-\nsources. arXiv preprint arXiv:2306.04751.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al.\n2022b. Super-naturalinstructions: Generalization\nvia declarative instructions on 1600+ nlp tasks. URL\nhttps://arxiv. org/abs/2204.07705.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Yi Tay, and Quoc V Le. 2022a. In-\nverse scaling can become u-shaped. arXiv preprint\narXiv:2211.02011.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4602–4625.\nHaike Xu, Zongyu Lin, Jing Zhou, Yanan Zheng,\nand Zhilin Yang. 2022. A universal discrimina-\ntor for zero-shot generalization. arXiv preprint\narXiv:2211.08099.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nMichihiro Yasunaga and Percy Liang. 2020. Graph-\nbased, self-supervised program repair from diagnos-\ntic feedback. In International Conference on Ma-\nchine Learning, pages 10799–10808. PMLR.\nSeonghyeon Ye, Hyeonbin Hwang, Sohee Yang,\nHyeongu Yun, Yireun Kim, and Minjoon Seo. 2023.\nIn-context instruction learning. arXiv preprint\narXiv:2302.14691.\nSeonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo\nShin, and Minjoon Seo. 2022. Guess the instruction!\nmaking language models stronger zero-shot learners.\narXiv preprint arXiv:2210.02969.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen-\nhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\n2023. Mammoth: Building math generalist models\nthrough hybrid instruction tuning. arXiv preprint\narXiv:2309.05653.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. Star: Bootstrapping reasoning with reason-\ning. arXiv preprint arXiv:2203.14465.\n12697\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models. arXiv preprint\narXiv:2210.03493.\nLucia Zheng, Neel Guha, Brandon R Anderson, Peter\nHenderson, and Daniel E Ho. 2021. When does pre-\ntraining help? assessing self-supervised learning for\nlaw and the casehold dataset of 53,000+ legal hold-\nings. In Proceedings of the eighteenth international\nconference on artificial intelligence and law, pages\n159–168.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\n12698\nA Analysis of COT COLLECTION\nNon-cherry picked rationales within COT C OL-\nLECTION are shown in Table 8. We perform\nan analysis regarding the quality, diversity, and\nreproducibility of rationale within the COT\nCOLLECTION .\nDiversity of Rationales To take a look into the\ndiversity of COT COLLECTION , we use Berkeley\nNeural Parser (Kitaev and Klein, 2018; Kitaev\net al., 2019) and parse rationales. More specifically,\nthe verb which is closest to the root of the parse\ntree along the noun object is extracted. We\ncompare this with the rationales from the 9 CoT\ndatasets used in Chung et al. (2022). As shown in\nFigure 5, COT COLLECTION have diverse textual\nformats included compared to the 9 existing CoT\ndatasets that have a high proportion assigned to\n‘answer question’ and ‘consider following’.\nQuality of Rationales To ensure the quality of\nCOT COLLECTION , we use ROSCOE (Golovneva\net al., 2022), a suite of metrics designed to\nevaluate rationales under different criteria within\nsemantic alignment, semantic similarity, logical\ninference, language coherence. We compare with\nhuman-authored rationales obtained during Prompt\nCreation in Section 3. The 13 ROSCOE scores\nare shown in Table 6. The results show that COT\nCOLLECTION include CoT rationales that are faith-\nful, less repetitive, informative, and logical even\nwhen compared to human-authored rationales. Yet,\nwe find that machine-generated rationales tend to\nhave higher perplexity, leading to lower language\ncoherence scores. We conjecture this is because in-\ncluding diverse textual formats leads may result in\nrelatively higher perplexity (Holtzman et al., 2019).\nIs COT COLLECTION Reproducible? One\ncould doubt whether COT COLLECTION is repro-\nducible due to the usage of OpenAI model in the\nprocess of CoT rationale augmentation 8. In this\nsection, we test different LLMs to generate 150\nrationales randomly sampled from COT COLLEC -\nTION , and compare the ROSCOE score (Golovneva\net al., 2022) in order to assess the quality. We use\nBard (Google, 2023), Claude (Anthropic, 2023),\nfor comparing with OpenAI Codex. The compar-\nison of quality is shown in Figure 6. The results\n8Moreover, OpenAI announced to stop its support on\nCodex model starting from June, 2023.\nMetrics Human CoT Collection\nSemantic\nAlignment\nfaithfulness 0.8836 0.8914\nfaithfulness_ww 0.8756 0.8793\nrepetition_word 0.9376 0.9419\ninformativeness_step0.9519 0.9521\nSemantic\nSimilarity\ninformativeness_chain0.2295 0.2797\nrepetition_sent 0.2453 0.2910\nLogical\nInference\ndiscourse_representation0.4855 0.4687\ncoherence_step_vs_step0.7763 0.7813\nLanguage\nCoherence\nperplexity_step 0.0198 0.0122\nperplexity_chain0.0475 0.0255\nperplexity_step_max0.0144 0.0088\ngrammar_step 0.8883 0.8721\ngrammar_step_max0.8013 0.7724\nTable 6: Comparison of the quality between human-authored\nrationales and machine-generated rationales. 13 label-free\nmetrics from ROSCOE (Golovneva et al., 2022) is used.\nshow that different LLMs are able to produce high\nquality rationales in terms of semantic alignment\nand language coherence.\nB Filtering C OT COLLECTION\nFiltering After generating multiple rationales,\nwe filter to ensure high quality. We apply the fol-\nlowing criteria to filter instances:\n• We exclude rationales that do not include the\nground truth answer when splitted by white\nspaces. While a rationale that doesn’t include\nthe answer isn’t necessarily a bad rationale,\nwe found it is effective to exclude inconsistent\nones.\n• We exclude CoT rationales that exceed the\nmaximum output length, where we constrain\nthe sum of r and y to be shorter than 512\ntokens.\n• We exclude rationales that are identical to pre-\nviously augmented ones during our process.\n• We exclude rationales that include repetitive\nsentences within the context.\nWe further include the filtered instances in\nTable 9.\nAlso, we found that in many cases, Codex de-\ngenerates and starts writing code after the rationale.\nTo prevent inclusion of code snippets, we apply ad-\nditional filtering based on trigger tokens that abun-\ndantly appear in the start of the code. The list of\ntrigger tokens are as follows:\n12699\nCODE_FILTER = [\n\"\\n‘\\n\\n’’’\", \"\\n‘\\n\", \"\\n’’\\n\",\n\"\\n’’’\\n\", \"\\n‘‘‘\", \"\\n\\n \\n’\",\n\"\\n\\n \\n‘\", \"\\n\\n \\nimport\",\n\"\\n‘;\\n\\n\", \"\\\"\\n\\n\",\n\"[examp\", \"[Examp\", \"\\n‘;\\n\\n\",\n\"’’’\\n\", \"\\n‘\\n\", \"\\n\\n‘‘\",\n\"\\n‘‘\", \"\\\"\\\"\\\"\\n\", \"\\n\\n\\t\",\n\"\\n#\", \"\\\";\\n\", \"\\\"\\n\\t\",\n\"print(\", \"\\n \", \"\\\"\\n’’’\",\n\"’’’\\nimport\", \"\\\"\\n\\n\\t\",\n\"\\n\\n \", \"\\n\\t\\t\", \"\\t \",\n\"\\\"\\n }\\n\", \"\\n\\n ####\",\n\"\\n\\n \\t‘)\\n}\", \"\\n</block>\\n\\n\",\n\"\\n\\n */\\n\",\n\"\\\"\\n\\n \\n \\t‘;\", \"\\n\\n \\t\",\n\"\\\"\\n\\n \\t */\", \"\\\";\\n\\n }\",\n\"\\n\\n \\t\\t\", \"\\\"\\n‘,\\n\\t}\",\n\"]]\\n\\t\", \"\\\"\\n\\n‘\", \"\\\"\\n’’’\\n\\n\",\n\"\\n\\n OR\", \"\\n \\n\"\n]\nC Training and Evaluation Details of\nCoT-T5\nParams Model Batch size LR Optimizer\n3B CoT-T5-3B 64 5e-5 AdamW\n11B CoT-T5-11B 8 1e-4 Adafactor\nTable 7: Hyperparameters used for fine-tuning CoT-T5.\nWe mostly follow the fine-tuning details of\nChung et al. (2022) to train CoT-T5. The hyper-\nparameters used for training CoT-T5 are shown in\nTable 7. We find 3B and 11B sized LMs converge\nwell using different optimizers. While CoT-T5-3B\ntends to converge well using AdamW, CoT-T5-11B\nis well optimized using Adafactor. For both sizes,\nwe train with 1 epoch using COT C OLLECTION\nwhich takes 1 day (3B) and 7 days (11B) when 8 A\n100 (80GB) GPUs are used. For both settings, we\nuse a gradient accumulation step of 8.\nFor sampling training instances, we sample\ninstances from Flan Collection (Longpre et al.,\n2023) by using the proportion of 23.94%(FLAN),\n30.85%(P3), 7.89%(Existing 9 CoT datasets),\n25.47%(SNI) and 11.85%(other dialogue & code\ndatasets). This is done by sampling 400 instances\n(FLAN), 300 instances (P3), 150 instances (SNI),\n4000 instances (Existing 9 CoT datasets), and 300\ninstances (other dialogue & code datasets), respec-\ntively. We generate 5 rationales per instance and\nthen apply filtering, leading to the final set of COT\nCOLLECTION , which is consisted of 1.84 million\ninstances and rationales across 1,060 tasks.\nDuring evaluation, we found that using nucleus\nsampling (Holtzman et al., 2019) with p =0.8 and\nno_repeat_n_gram =3 was very effective in gener-\nating good-quality rationales.\n12700\nCo T Collection (Ours)\npr o vide\ncontain\nmak e\nde s cr i b e\nask\ne x\np r ess\nf o\nllo w\nha v\ne\ni n c l u de\ngiv\ne\nansw e\nr\nu se\ncon\nv e y\nm en t i o n\nt a k e\ns tat e\ndi scus s\ni n di ca t e\nn ee d\ns h o w\ninf ormation\ncont e xt\nd et\na il\nd e ﬁn\ni t io\nn\ninf ormation\nlanguage\nw or d\ncon\nt en\nt\nsense\nclaim\nde bu t\ns t a t e ment\ns i t u a t ion\ne v e n t\ns t u dy\nm a n\nq u e s t io n\nu s e r\nsy\ns t e m\nt h i n g\no p in ion\nemot\ni on\ngr\natit\nude\nfr ustr\na tion\n stru\nctur e\nnar\nr at\niv e\npr e\nm\ni se\nt h\ne me\nn u m\nb e r\nv a\nlue\nl i n e\nqu\nes\ntion\ne rr\nor\nse\nnt e\nn c e\nin\nf o r m\na tio\nn\np h r ase\nin\nf o\nr ma\ntion\n=\na nsw e\nr\nst\nat\ne\nq u est\ni on\npr o mp\nt\ninf\no r mat\ni o n\ns tr at egy\n slur\nw or d\nst er eo\ntype\n e v e nt\nemotion\n f eel\ning\nname\n man\nc h a rac t e r\ndat e\ncar\nplace\n po i nt\nr esponsibility\nf act\nn e e d\ns t e r eo ty p e\npurpose\nn e e d\ns t udy\nt opic\n wa y\nf o cus\nl o cation\nr eas o n\nho pe\n=\na pp ointm en t\nc l ot he\ns ent ence\nempat h y\nhat e\nhes itati on\nho p e\nansw erconsider\nha v e\ntak e\nﬁnd\nuse\nr equi\nr e ma\nk e\np r o\nd uc\ne\npl\na y\nconta\ni n\ng et\neat\ngiv e\nw e a r\nbu y\nmea n\ncause\nn e ed\nsel l\nquestion\nphone\nf ollo wing\nsyst em\nc h ild\nc y c le\nor g a n\nplace\nmi n u t e\nh o u r\npi ct ur e\nnumber\nti me\na mo\nu n t\ncost\n e ner gy\ner o\nm\none\na c id\ns e nse\n ener gy\na p pl\ne\nlot\nt ea\nm\nf o od\n ph y\nl um\n par\nt\np\neo\np\nl e\ne g g\nz y got e\n spo\nr e\n c ak\ne\ng a me\n r ole\n g ui\ntar\ns o n g\nd na\ni n stru\nc t ion\nge\nne\n sa\nlt\nr em\naind\ne r\ncomp\nou\nn d\nene\nr gy\nf ood\ncookie\n f ood\npound\n cak\ne\nbir t\nh\nr em\na inder\nt ot a l\nsa t isf a ction\nshir t\n coat\ncos tu me\nhat\nt otal\nbook\nbo x\npound\nf eeling\nt hing\nem o tion\np os i t ion\ncancer\n l o t\ns w ell ing\ntr ou bl e\ng a llon\ni nch\nmone y\nwa t e r\nb o x\ni t e m\nb rac ele t\nc a ndy\nFlan- T5  Co T Data\nFigure 5: The top 20 common root verbs (inner circle) and their top 4 noun objects (outer circle) within the rationales\nof the 9 CoT tasks used in Chung et al. (2022) (left side) and our COT COLLECTION with 1,060 tasks (right side).\nfaithfulness\nfaithfulness_ww\nrepetition_word\ninformativeness_step\ninformativeness_chain\nrepetition_sent\ndiscourse_representation\ncoherence_step_vs_step\nperplexity_step\nperplexity_chain\nperplexity_step_max\ngrammar_step\ngrammar_step_max\n0.0000\n0.2500\n0.5000\n0.7500\n1.0000\nROSCOE_SA ROSCOE_SS ROSCOE_LI ROSCOE_LC\nCodex (Ours) Bard Claude\nFigure 6: Comparison of the quality of augmenting CoT rationales using different LLMs by reference-free ROSCOE\nscores. Semantic Alignment(SA), Semantic Similarity(SS), Logical Inference(LI), Coherence(LC).\n12701\nQuestion & Instruction Answer Rationale\nRead the following context and choose the best option to answer the question.\nContext: From the triple loop she executed after five minutes, he suspected thatshe had managed to sneak off for flying more often than during the summersand wondered just how diligent George was about practicing on his own time.\nQuestion: What is a triple loop ?\nOptions:A. None of the above choices .B. 3 round trips on a plane route .C. 3 neckties worn on flightsD. A feature pulled off by making 3 loops in the air.\nD\nA triple loop is a maneuver in which an aircraft makes three loops.Based on the context, it can be inferred thata triple loop involves making 3 loops in the air.Since this is what A means, D should be chosen as the correct answer.\nRead the passage below and choose the right answerto the following question (choices are more or less ):\nOn a hot day, an endotherm may give off more heatby increasing blood flow to the surface of the body.\nAfter Sarah finishes her walk, she notices thather blood is not really pumping that hard.Sarah can know that since it’s a cool day,she’s giving off what kind of heat compared to usual more or less?\nless\nThe given passage states that an endotherm gives off more heatby increasing blood flow to the surface of its body.As such, it can be inferred that decreased blood flowcorresponds with less heat being given off.Since Sarah notices that her blood is not really pumpinghard on a cool day, she is giving off less heat compared to usual.\nRead this and answer the question.Pubertal development also affects circulatory and respiratory systems asan adolescents’ heart and lungs increase in both size and capacity.These changes lead to increased strength and tolerance for exercise.Sex differences are apparent as males tend to develop\"larger hearts and lungs, higher systolic blood pressure,a lower resting heart rate,a greater capacity for carrying oxygen to the blood,a greater power for neutralizingthe chemical products of muscular exercise,higher blood hemoglobin and more red blood cells¨.\nWhich sex tends to develop larger hearts and lungs?\nmales The last sentence of the paragraph states thatmales tend to develop larger hearts and lungs.\nIn this task, you are given a string of characters.You need to remove duplicate characters from the stringif any, and return the resulting string.\n\"nXnGFzHFYudOiHyWLZXQfpw\"\nnXGFzHYudOiyWLZQfpwThere are no duplicate characters in the given string,so the resulting string will be the same as the input.Therefore, the output will be \"nXGFzHYudOiyWLZQfpw\".\nIn this task you will be given a list of numbersand you should remove all duplicates in the list.If every number is repeated in the list an empty list should be returned.Your list should be numbers inside brackets, just like the given list.\n{3, 5, 1, 0, 3}\n{5, 1, 0}\nThe given list is {3, 5, 1, 0, 3}.We can see that ’3’ appears in the list twice.Therefore, we need to keep only one of the ’3’s and remove the other one.The resulting list is {5, 1, 0}.\nHow to keep your phone cool in the carKeep your phone on a vent mount withair conditioning on if you need to use it.If you need to use your phone while you’re in your car,purchase a mount that attaches to one of the dashboard air vents.Turn on the air conditioner so it keeps your phone cold.\nCan we draw the following conclusion?keep your phone and tablet cool while mounted on anair conditioner vent if you need to use them while in the car.\nOPTIONS:- Yes- It’s impossible to say- No\nIt’s impossible to say\nWhile the passage does mention thatyour phone should be mounted on an air vent,it doesn’t specify whether this is referring to a car or home AC.Additionally, we don’t know from the contextif other devices such as tablets can also be kept cool using this method.\nSo, \"It’s impossible to say\"\nGive me a possible correct answer to the question\"who nominated stephen g. breyer?\" Bill Clinton\nStephen G. Breyer was a nominee for the Supreme Courtby Bill Clinton and was confirmed on July 29, 1994.Although he wasn’t nominated in 1998 or 1999,this is still a valid answerbecause it happened within 5 years of those dates before.\nWrite down the solution for this math problem:Solve 291∗c−264∗c=189 for c. 7 STEP 1. 291∗c−264∗c=189STEP 2. 27∗c=189STEP 3. c=7\nTable 8: Example of rationales within COT COLLECTION including tasks such as Multiple choice QA (MCQA), Ex-\ntractive QA (ExQA), Closed-book QA (CBQA), Formal Logic, Natural Language Inference (NLI), and Arithmetic.\n12702\nQuestion Answer CoT Rationale\nYou are given a math word problemand you are supposed to only use subtraction on the numbersembedded in the text to answer the following questionand then only report the final numerical answer.\nContext: Jake has 8 fewer peaches and 10 more apples than Steven.Steven has 11 apples and 18 peaches.\nQuestion: How many more peaches than apples does Steven have?\n7\nTo solve this problem,we need to find the number of apples and peaches that Steven has.From the text, we know that Jake has 8 fewer peachesthan Steven and 10 more apples than Steven.We also know that Steven has11 apples and 18 peaches.Using these clues we can write two equations:Steven’s peaches = Jake’s Peaches + 8Steven’s Apples = Jake’s Apples - 10We can then add these two equations together:(Steven’s Peaches)+(Stevens’ Apples) = (Jake’s Peaches+8) + (Jake’s Apples-10)(18+11)=(Jake ’s Peaches +8)+ (Jakes’Apples-10)29= Jakes’peach+Jakes’applees-2a=29-2=27b=(18+11)-ac=b/2nd=c*2\nWhat is the solution?Solve 690*c + 14495 = -4825 for c.-28\n690*c + 14495 = -4825Subtracting 4825 from both sides:690*c + 9669 = 0Dividing both sides by 690:c=-14.02391304348\nTable 9: Two representative rationale examples that are filtered with our filtering criteria. It demonstrates that not\nincluding the ground truth token within the rationale could indicate that the LLM failed to generate a consistent\nrationale, often leading to degeneration. This phenomenon is mostly found on math datasets.\n12703\nD Prompts used for Rationale\nAugmentation\nAmong the 26 demonstrations used for each fam-\nily of tasks when augmenting CoT rationales, we\ninclude 4 examples, other demonstrations for task\nfamilies can be seen at this link.\n[Example 1]\n[Instruction and Question]\nSkylar had stopped breathing but Lee held Skylar’s arms anyway, in vain. Given the\ncontext: Lee want to do what next?\nPossible answers: beg the doctors to try again, get her up and walking home, bring Skylar\nback\n[Options]\n− beg the doctors to try again\n− get her up and walking home\n− bring Skylar back\n[Answer]\nbeg the doctors to try again\n[Rationale]\nThe context of the situation is that Skylar has stopped breathing and Lee is holding\nSkylar’s arms. This suggests that Skylar has received medical attention and is no longer\nalive.\nBased on this, Lee would want to beg the doctors to try again, as this would be the best\nway to potentially save Skylar’s life.\nThe options of getting Skylar up and walking home or bringing Skylar back do not align\nwith the context, as they would not address the immediate medical emergency at hand.\nSo the answer is to beg the doctors to try again.\n[Example 2]\n[Instruction and Question]\nDo you think the right answer to the question \"what can run on alcoholic fermentation of\nthe glucose in corn or other plants?\" is \"bicycles\"?\n[Options]\n− No\n− Yes\n[Answer]\nNo\n[Rationale]\nAlcoholic fermentation is a process that occurs in living organisms, most commonly yeast,\nto convert glucose into energy. This process does not produce enough energy to power\nsomething like a bicycle.\nSo the answer is No.\n[Example 3]\n[Instruction and Question]\nEventually , the cyst can get so large that part of the wall weakens and fluid begins to leak\nout ; that is , the cyst ruptures . It is similar to popping a water balloon . When this\nhappens , the fluid may irritate the pelvic lining and cause pain , often severe . Most of the\ntime the pain begins to gradually subside over the course of the next few days .\nWhat type of doctor might the person have had to see ?\n− A paediatrician − A heart surgeon − a gynecologist − None of the above choices\n[Options]\n− A paediatrician\n− A heart surgeon\n− a gynecologist\n− None of the above choices\n[Answer]\na gynecologist\n[Rationale]\nThe context mentions a cyst in the pelvic region and the symptoms of the cyst rupturing,\nincluding pain and fluid leakage. This suggests that the issue is related to the reproductive\nor urinary system. Given that the issue is related to the reproductive or urinary system, it is\nlikely that the person would have had to see a specialist in these areas.\nOut of the given options, \"a gynecologist\" is the most likely specialist that the person\nwould have had to see, as gynecologists specialize in female reproductive health,\nincluding issues related to the uterus, ovaries, and other reproductive organs.\nIn conclusion, based on the given context, the most likely answer is a gynecologist.\n[Example 4]\n[Instruction and Question]\nProcess:\n− Get some seeds\n− Pick a spot to plant them\n− Dig a hole in the dirt\n− Put the seed in the hole\n− Pour some water on the seed and hole\n− Cover up the hole\n− Press down on it\n− Spray some plant food on it.\nQuestion:\nsuppose if no water is available happens, how will it affect vegetables?\n− A: more\n− B: less\n[Options]\n− A\n− B\n[Answer]\nB\n[Rationale]\nThe process of planting seeds requires water, which is essential for the growth of plants.\nWater helps transport nutrients, regulate temperature, and absorb minerals from the soil. If\nno water is available, the seed will not germinate and the plant will not grow properly,\nleading to fewer vegetables. Hence, the absence of water will result in \"less\" vegetables,\nmaking the answer \"B\".\n[Example 5]\n[Instruction and Question]\nRead the following conversation and answer the question.\nM: I’m worried about my sister, Lucy.\nW: Why is that?\nM: She thinks she’s too fat.\nW: Is she?\nM: No, but she keeps skipping meals. Then, she only eats chips and drinks cola.\nW: I used to do that. It’s called binge. It was no fun!\nM: Why did you stop doing it?\nW: Well, my doctor told me to eat when I’m hungry. She said, \"Eat till you’re full or\nyou’ll eat too much later.\" She said a lot of girls ruin their health this way.\nM: Did she say what to eat?\nW: She said, \"Eat fruit, vegetables, meats and grains. Have regular meals and snacks. Get\nexercise, too.\"\nQuestion: What’s the relationship between the speakers?\n− Doctor and patient.\n− Friends.\n− Brother and sister.\n[Options]\n− Doctor and patient\n− Friends\n− Brother and sister\n[Answer]\nFriends\n[Rationale]\nThe conversation between two people, a man(M) and a woman(W), is discussing the\nman’s sister who has a negative view of her own body and is skipping meals. The woman\nshares that she used to engage in similar behavior, and explains that her doctor advised her\nto eat a balanced diet and exercise.\nAmong the three options,\n− First, since the woman is sharing her experience of what the doctor told her, it is\nunlikely she would be a medical professional. So the answer couldn’t be doctor and\npatient.\n− Second, given that the man is talking about his sister, the woman is unlikely to be the\nman’s sister.\n− Lastly, because the conversation is informal and the speakers are discussing a personal\nmatter it is likely that the relationship between the speakers is that of friends.\nTherefore, the answer is Friends.\n[Example 6]\n[Instruction and Question]\nQuestion: The best place for the troublemaker to sneak a smoke while he was in where,\nwas in the bathroom?\nChoices:\n− playground\n− school\n− at hotel\n− theater\n− convenience store\nThe rationale to choose \"\"school\"\" as the answer is that:\n[Options]\nNot Given\n[Answer]\nsmokers traditionally use school bathrooms\n12704\n[Rationale]\nHere’s the rationale: \"smokers traditionally use school bathrooms\".\n[Example 7]\n[Instruction and Question]\nSo I was looking at quotes for our tshirts for ice hockey , and I came across one that sums\nup how I ’ m feeling right now . When you talk about an injury and the kind of depression\nyou go through , it ’s not just because you ’re out of shape and you can’t go out and play .\nYou ’re missing a part of you .\nWhy is the narrator so down in the dumps ?\nPick the best answer from the following options:\nA. They are unable to play ice curling .\nB. They are hurt .\nC. They are unable to play ice skating .\nD. They are unable to play in the yard .\n[Options]\n− A\n− B\n− C\n− D\n[Answer]\nB\n[Rationale]\nThe quote in the question mentions the narrator’s feelings about an injury and the\ndepression that follows. The narrator says that it’s not just about being out of shape and\nunable to play, but rather they feel like they are missing a part of themselves. This implies\nthat the reason for their depression is linked to their physical inability to participate in a\nsport or activity they enjoy. Based on the context, it can be inferred that the narrator is\nupset because they are hurt and unable to play ice hockey.\nSo, the answer is B.\n[Example 8]\n[Instruction and Question]\nQ: What does the ubiquitous generator use to generate currents?\nChoices:\n− inertia − water − gravity − magnetism\n[Options]\n− inertia\n− water\n− gravity\n− magnetism\n[Answer]\nmagnetism\n[Rationale]\nA generator is a device that converts mechanical energy into electrical energy. This is\ndone by using a magnetic field to induce a current in the generator’s windings. This\ncurrent is then sent to an external circuit, where it can be used to power electrical devices.\nThe generator uses magnetism to generate currents, so the answer is magnetism.\n[Example 9]\n[Instruction and Question]\nPrompts 1: Demonstration used for tasks in family of\nMCQA (T0)\n[Example 1]\n[Instruction and Question]\nSkylar had stopped breathing but Lee held Skylar’s arms anyway, in vain. Given the\ncontext: Lee want to do what next?\nPossible answers: beg the doctors to try again, get her up and walking home, bring Skylar\nback\n[Options]\n− beg the doctors to try again\n− get her up and walking home\n− bring Skylar back\n[Answer]\nbeg the doctors to try again\n[Rationale]\nThe context of the situation is that Skylar has stopped breathing and Lee is holding\nSkylar’s arms. This suggests that Skylar has received medical attention and is no longer\nalive.\nBased on this, Lee would want to beg the doctors to try again, as this would be the best\nway to potentially save Skylar’s life.\nThe options of getting Skylar up and walking home or bringing Skylar back do not align\nwith the context, as they would not address the immediate medical emergency at hand.\nSo the answer is to beg the doctors to try again.\n[Example 2]\n[Instruction and Question]\nDo you think the right answer to the question \"what can run on alcoholic fermentation of\nthe glucose in corn or other plants?\" is \"bicycles\"?\n[Options]\n− No\n− Yes\n[Answer]\nNo\n[Rationale]\nAlcoholic fermentation is a process that occurs in living organisms, most commonly yeast,\nto convert glucose into energy. This process does not produce enough energy to power\nsomething like a bicycle.\nSo the answer is No.\n[Example 3]\n[Instruction and Question]\nEventually , the cyst can get so large that part of the wall weakens and fluid begins to leak\nout ; that is , the cyst ruptures . It is similar to popping a water balloon . When this\nhappens , the fluid may irritate the pelvic lining and cause pain , often severe . Most of the\ntime the pain begins to gradually subside over the course of the next few days .\nWhat type of doctor might the person have had to see ?\n− A paediatrician − A heart surgeon − a gynecologist − None of the above choices\n[Options]\n− A paediatrician\n− A heart surgeon\n− a gynecologist\n− None of the above choices\n[Answer]\na gynecologist\n[Rationale]\nThe context mentions a cyst in the pelvic region and the symptoms of the cyst rupturing,\nincluding pain and fluid leakage. This suggests that the issue is related to the reproductive\nor urinary system. Given that the issue is related to the reproductive or urinary system, it is\nlikely that the person would have had to see a specialist in these areas.\nOut of the given options, \"a gynecologist\" is the most likely specialist that the person\nwould have had to see, as gynecologists specialize in female reproductive health,\nincluding issues related to the uterus, ovaries, and other reproductive organs.\nIn conclusion, based on the given context, the most likely answer is a gynecologist.\n[Example 4]\n[Instruction and Question]\nProcess:\n− Get some seeds\n− Pick a spot to plant them\n− Dig a hole in the dirt\n− Put the seed in the hole\n− Pour some water on the seed and hole\n− Cover up the hole\n− Press down on it\n− Spray some plant food on it.\nQuestion:\nsuppose if no water is available happens, how will it affect vegetables?\n− A: more\n− B: less\n[Options]\n− A\n− B\n[Answer]\nB\n[Rationale]\nThe process of planting seeds requires water, which is essential for the growth of plants.\nWater helps transport nutrients, regulate temperature, and absorb minerals from the soil. If\nno water is available, the seed will not germinate and the plant will not grow properly,\nleading to fewer vegetables. Hence, the absence of water will result in \"less\" vegetables,\nmaking the answer \"B\".\n[Example 5]\n[Instruction and Question]\nRead the following conversation and answer the question.\nM: I’m worried about my sister, Lucy.\nW: Why is that?\nM: She thinks she’s too fat.\nW: Is she?\nM: No, but she keeps skipping meals. Then, she only eats chips and drinks cola.\n12705\nW: I used to do that. It’s called binge. It was no fun!\nM: Why did you stop doing it?\nW: Well, my doctor told me to eat when I’m hungry. She said, \"Eat till you’re full or\nyou’ll eat too much later.\" She said a lot of girls ruin their health this way.\nM: Did she say what to eat?\nW: She said, \"Eat fruit, vegetables, meats and grains. Have regular meals and snacks. Get\nexercise, too.\"\nQuestion: What’s the relationship between the speakers?\n− Doctor and patient.\n− Friends.\n− Brother and sister.\n[Options]\n− Doctor and patient\n− Friends\n− Brother and sister\n[Answer]\nFriends\n[Rationale]\nThe conversation between two people, a man(M) and a woman(W), is discussing the\nman’s sister who has a negative view of her own body and is skipping meals. The woman\nshares that she used to engage in similar behavior, and explains that her doctor advised her\nto eat a balanced diet and exercise.\nAmong the three options,\n− First, since the woman is sharing her experience of what the doctor told her, it is\nunlikely she would be a medical professional. So the answer couldn’t be doctor and\npatient.\n− Second, given that the man is talking about his sister, the woman is unlikely to be the\nman’s sister.\n− Lastly, because the conversation is informal and the speakers are discussing a personal\nmatter it is likely that the relationship between the speakers is that of friends.\nTherefore, the answer is Friends.\n[Example 6]\n[Instruction and Question]\nQuestion: The best place for the troublemaker to sneak a smoke while he was in where,\nwas in the bathroom?\nChoices:\n− playground\n− school\n− at hotel\n− theater\n− convenience store\nThe rationale to choose \"\"school\"\" as the answer is that:\n[Options]\nNot Given\n[Answer]\nsmokers traditionally use school bathrooms\n[Rationale]\nHere’s the rationale: \"smokers traditionally use school bathrooms\".\n[Example 7]\n[Instruction and Question]\nSo I was looking at quotes for our tshirts for ice hockey , and I came across one that sums\nup how I ’ m feeling right now . When you talk about an injury and the kind of depression\nyou go through , it ’s not just because you ’re out of shape and you can’t go out and play .\nYou ’re missing a part of you .\nWhy is the narrator so down in the dumps ?\nPick the best answer from the following options:\nA. They are unable to play ice curling .\nB. They are hurt .\nC. They are unable to play ice skating .\nD. They are unable to play in the yard .\n[Options]\n− A\n− B\n− C\n− D\n[Answer]\nB\n[Rationale]\nThe quote in the question mentions the narrator’s feelings about an injury and the\ndepression that follows. The narrator says that it’s not just about being out of shape and\nunable to play, but rather they feel like they are missing a part of themselves. This implies\nthat the reason for their depression is linked to their physical inability to participate in a\nsport or activity they enjoy. Based on the context, it can be inferred that the narrator is\nupset because they are hurt and unable to play ice hockey.\nSo, the answer is B.\n[Example 8]\n[Instruction and Question]\nQ: What does the ubiquitous generator use to generate currents?\nChoices:\n− inertia − water − gravity − magnetism\n[Options]\n− inertia\n− water\n− gravity\n− magnetism\n[Answer]\nmagnetism\n[Rationale]\nA generator is a device that converts mechanical energy into electrical energy. This is\ndone by using a magnetic field to induce a current in the generator’s windings. This\ncurrent is then sent to an external circuit, where it can be used to power electrical devices.\nThe generator uses magnetism to generate currents, so the answer is magnetism.\n[Example 9]\n[Instruction and Question]\nPrompts 2: Demonstration used for tasks in family of\nCommonsense (SNI)\n[Example 1]\n[Instruction and Question]\nProblem: Context: Anorexia in males accounted for approximately six percent of cases\nseen in an eating disorder clinic.\\nAnorexia becomes common in males.\\nOPTIONS:\\n−\nyes\\n− no\\nA:\n[Answer]\nno\n[Rationale]\nThe information given only pertains to cases seen in this particular eating disorder clinic\nand may not be representative of the entire population. To determine whether anorexia is\nbecoming common in males, we would need to look at data from a larger sample size or\npopulation, as well as data from multiple sources to verify the trend. So the answer is \"no\".\n[Example 2]\n[Instruction and Question]\nGenerate a context and a hypothesis.\\n\\nAnswer:\n[Answer]\nHe also liked swimming and cycling. He said that he wrote stories, though he had to admit\nthat he had never got further than the first two pages. Willie meanwhile not only remained\nsilent during these conversations but picked his berries slowly so that they might forget\nthat he was there but he reckoned without Zach.\\n\\nHypothesis: Willie was there\n[Rationale]\nHere’s a context and an example for natural language inference task:\\nHe also liked\nswimming and cycling. He said that he wrote stories, though he had to admit that he had\nnever got further than the first two pages. Willie meanwhile not only remained silent\nduring these conversations but picked his berries slowly so that they might forget that he\nwas there but he reckoned without Zach.\\n\\nHypothesis: Willie was there\n[Example 3]\n[Instruction and Question]\nQ: Brock was a popular English writer and director.\\nJeremy Brock MBE (born 1959) is\nan English writer and director whose works include the screenplays \"Mrs Brown\",\n\"Driving Lessons\", \"The Last King of Scotland\", \"Charlotte Gray\", and \"The Eagle\".\nBrock has also written two plays for the Hampstead downstairs theatre.\\nOPTIONS:\\n−\nYes\\n− It’s impossible to say\\n− No\\nA:\n[Answer]\nIt’s impossible to say\n[Rationale]\nWhile the information provided in the options about Jeremy Brock suggests that he is\nindeed an English writer and director with a number of successful works under his belt,\nthere is not enough information to determine whether he is actually \"popular\" or not. The\nterm \"popular\" is subjective and can vary depending on different factors, such as time\nperiod, genre, and audience. So the answer is \"It’s impossible to say\".\n[Example 4]\n[Instruction and Question]\nBiff Burger (Best In Fast Food) was a burger fast food franchise that originated in\nClearwater, Florida. Early success in the late 1950s and early 1960s led the franchise to\nspread to many parts of the Southeastern United States. However, financial woes in the\n1970s sent the chain on a decline and most of its restaurants out of business by the 1980s.\n12706\nToday, two Biff Burger restaurants remain.\\nBiff Burger came about in Clearwater\nFlorida. OPTIONS:\\n− Yes\\n− It’s impossible to say\\n− No\\nA:\n[Answer]\nYes\n[Rationale]\nTo validate the claim \"Biff Burger came about in Clearwater Florida\", we should find a\nsupporting evidence within the context. We could see that \"Biff Burger (Best In Fast\nFood) was a burger fast food franchise that originated in Clearwater, Florida.\" supports the\nclaim we are trying to validate. So the answer is \"Yes\".\n[Example 5]\n[Instruction and Question]\ninput hypothesis: In the decade before the 1950s, the United States FBI began to maintain\na public list of the people it regarded as the Ten Most Wanted Fugitives\\nContext: In the\n1950s, the United States FBI began to maintain a public list of the people it regarded as\nthe Ten Most Wanted Fugitives. Following is a brief review of FBI people and events that\nplace the 1950s decade in context, and then an historical list of individual fugitives whose\nnames first appeared on the 10 Most Wanted list during the decade of the 1950s, under\nFBI Director J. Edgar Hoover.\\nOPTIONS:\\n− Yes\\n− It’s impossible to say\\n− No\\ntrue\nor false:\n[Answer]\nNo\n[Rationale]\nThe context provided contradicts this hypothesis by stating that the FBI began to maintain\nthis public list in the 1950s, under the direction of J. Edgar Hoover. The context goes on\nto list individuals who were first placed on the 10 Most Wanted list during the 1950s,\nfurther supporting the notion that the list was not in existence prior to that decade. So the\nanswer is \"No\".\n[Example 6]\n[Instruction and Question]\nQ: If The plaintiffs claim Penrose’s design is distinguished by its aperiodicity (the pattern\nalmost but never quite repeats itself) and its five−fold symmetry (a trait that at the time\nwas thought not to exist in nature but has since been identified in certain crystal\nformations)., can we say that \"The design has no identified traits that would distinguish\nit.\"?\\nOPTIONS:\\n− yes\\n− it is not possible to tell\\n− no\\n\\nA:\n[Answer]\nno\n[Rationale]\nThe plaintiffs are stating that the design has unique characteristics that set it apart, which\ndirectly contradicts the statement that the design has no identified traits that would\ndistinguish it. Therefore, the answer is \"no\".\n[Example 7]\n[Instruction and Question]\nDoes \"The first docking of Soviet spacecraft was finally realised in January 1969 by the\nSoyuz 4 and Soyuz 5 missions.\" contain the correct answer to \"When was the first\nsuccessful docking of a two man space crew?\"\\nOPTIONS:\\n− yes\\n− no\\n\\nA:\n[Answer]\nyes\n[Rationale]\nThe passage talks about the first successful docking of a two−man space crew, as both\nSoyuz 4 and Soyuz 5 had two cosmonauts on board at the time of their docking.\nTherefore, the statement contains the correct answer to the question. So, the answer is\n\"yes\".\n[Example 8]\n[Instruction and Question]\ninput question: Is it true that \"I shall have to guard The foxes.\" if \"The foxes are getting in\nat night and attacking the chickens. I shall have to guard them.\" is true? OPTIONS:\\n−\nno\\n− yes\\noutput answer:\n[Answer]\nno\n[Rationale]\nThe statement \"I shall have to guard The foxes\" does not accurately reflect the situation\ndescribed in the second sentence \"I shall have to guard them\". In this context, \"them\"\nrefers to the foxes, so the correct statement would be \"I shall have to guard them\", not \"I\nshall have to guard the foxes\". So, the answer is \"no\".\n[Example 9]\n[Instruction and Question]\nQuestion:\\nPremise: A man in a blue shirt and orange shorts looks on at a small child\nwearing a yellow shirt and blue shorts with trees comprising the backdrop.\\n\\nHypothesis:\nThere are a lot of trees behind the small child.\\nIs the hypothesis true?\\nOPTIONS:\\n−\nyes\\n− it is not possible to tell\\n− no\\n\\nAnswer:\\n\n[Answer]\nit is not possible to tell\n[Rationale]\nWhile it is mentioned that trees make up the backdrop, it is not specified whether there are\na lot of trees or not. Additionally, the focus of the premise is on the man and child and not\non the trees in the background. Hence, it is not possible to determine whether the\nhypothesis is true or not.\n[Example 10]\n[Instruction and Question]\nPrompts 3: Demonstration used for tasks in family of\nNLI (FLAN)\n[Example 1]\n[Instruction and Question]\nThere are 15 trees in the grove. Grove workers will plant trees in the grove today. After\nthey are done, there will be 21 trees. How many trees did the grove workers plant today?\n[Answer]\n6\n[Rationale]\nWe start with 15 trees. Later we have 21 trees. The difference must be the number of trees\nthey planted. So, they must have planted 21 − 15 = 6 trees.\n[Example 2]\n[Instruction and Question]\nIf there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the\nparking lot?\n[Answer]\n5\n[Rationale]\nThere are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars.\n[Example 3]\n[Instruction and Question]\nLeah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they\nhave left in total?\n[Answer]\n39\n[Rationale]\nLeah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 +\n42 = 74 chocolates. 35 have been eaten. So in total they still have 74 − 35 = 39 chocolates.\n[Example 4]\n[Instruction and Question]\nJason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How\nmany lollipops did Jason give to Denny?\n[Answer]\n8\n[Rationale]\nJason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny.\nThe number of lollipops he has given to Denny must have been 20 − 12 = 8 lollipops.\n[Example 5]\n[Instruction and Question]\nShawn has five toys. For Christmas, he got two toys each from his mom and dad. How\nmany toys does he have now?\n[Answer]\n9\n[Rationale]\nHe has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more\nfrom dad, so in total he has 7 + 2 = 9 toys.\n[Example 6]\n[Instruction and Question]\nThere were nine computers in the server room. Five more computers were installed each\nday, from monday to thursday. How many computers are now in the server room?\n[Answer]\n29\n[Rationale]\nThere are 4 days from monday to thursday. 5 computers were added each day. That means\nin total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so\n12707\nnow there are 9 + 20 = 29 computers.\n[Example 7]\n[Instruction and Question]\nMichael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2\nmore. How many golf balls did he have at the end of wednesday?\n[Answer]\n33\n[Rationale]\nMichael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 − 23 = 35\nballs. On Wednesday he lost 2 more so now he has 35 − 2 = 33 balls.\n[Example 8]\n[Instruction and Question]\nOlivia has $23. She bought five bagels for $3 each. How much money does she have left?\n[Answer]\n8\n[Rationale]\nShe bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She\nhad $23 in beginning, so now she has $23 − $15 = $8.\n[Example 9]\n[Instruction and Question]\nPrompts 4: Demonstration used for tasks in family of\nArithmetic (FLAN)\n12708",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7587144374847412
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5935554504394531
    },
    {
      "name": "Task (project management)",
      "score": 0.5430173277854919
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36439114809036255
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}