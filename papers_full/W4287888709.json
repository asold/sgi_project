{
    "title": "Clinical Flair: A Pre-Trained Language Model for Spanish Clinical Natural Language Processing",
    "url": "https://openalex.org/W4287888709",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5090639270",
            "name": "Matías Rojas",
            "affiliations": [
                "University of Chile"
            ]
        },
        {
            "id": "https://openalex.org/A5027733513",
            "name": "Jocelyn Dunstan",
            "affiliations": [
                null,
                "University of Chile"
            ]
        },
        {
            "id": "https://openalex.org/A5084525042",
            "name": "Fabián Villena",
            "affiliations": [
                "University of Chile"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963716420",
        "https://openalex.org/W3080321165",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W4205772143",
        "https://openalex.org/W2964242047",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2985294119",
        "https://openalex.org/W4226027641",
        "https://openalex.org/W3096590546",
        "https://openalex.org/W2880875857",
        "https://openalex.org/W4285116174",
        "https://openalex.org/W4287818038",
        "https://openalex.org/W2946119234",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3105240624",
        "https://openalex.org/W2047477415",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2970771982"
    ],
    "abstract": "Word embeddings have been widely used in Natural Language Processing (NLP) tasks. Although these representations can capture the semantic information of words, they cannot learn the sequence-level semantics. This problem can be handled using contextual word embeddings derived from pre-trained language models, which have contributed to significant improvements in several NLP tasks. Further improvements are achieved when pre-training these models on domain-specific corpora. In this paper, we introduce Clinical Flair, a domain-specific language model trained on Spanish clinical narratives. To validate the quality of the contextual representations retrieved from our model, we tested them on four named entity recognition datasets belonging to the clinical and biomedical domains. Our experiments confirm that incorporating domain-specific embeddings into classical sequence labeling architectures improves model performance dramatically compared to general-domain embeddings, demonstrating the importance of having these resources available.",
    "full_text": "Proceedings of the 4th Clinical Natural Language Processing Workshop, pages 87 - 92\nJuly 14, 2022 ©2022 Association for Computational Linguistics\nClinical Flair: A Pre-Trained Language Model for Spanish Clinical Natural\nLanguage Processing\nMatías Rojas1,3, Jocelyn Dunstan2,3,4, and Fabián Villena1,3\n1Department of Computer Sciences, University of Chile.\n2Initiative for Data & Artificial Intelligence, University of Chile.\n3Center for Mathematical Modeling - CNRS IRL 2807, University of Chile.\n4Millenium Institute for Intelligent Healthcare Engineering, ANID, Chile.\nmatias.rojas.g@ug.uchile.cl\n{jdunstan, fabian.villena}@uchile.cl\nAbstract\nWord embeddings have been widely used in\nNatural Language Processing (NLP) tasks. Al-\nthough these representations can capture the\nsemantic information of words, they cannot\nlearn the sequence-level semantics. This prob-\nlem can be handled using contextual word em-\nbeddings derived from pre-trained language\nmodels, which have contributed to significant\nimprovements in several NLP tasks. Fur-\nther improvements are achieved when pre-\ntraining these models on domain-specific cor-\npora. In this paper, we introduce Clinical Flair,\na domain-specific language model trained on\nSpanish clinical narratives. To validate the qual-\nity of the contextual representations retrieved\nfrom our model, we tested them on four named\nentity recognition datasets belonging to the clin-\nical and biomedical domains. Our experiments\nconfirm that incorporating domain-specific em-\nbeddings into classical sequence labeling archi-\ntectures improves model performance dramati-\ncally compared to general-domain embeddings,\ndemonstrating the importance of having these\nresources available.\n1 Introduction\nWord embeddings are dense, semantically meaning-\nful vector representations of a word. This method\nhas proven to be a fundamental building block\nwhen constructing neural network-based architec-\ntures. However, the main drawback of using these\nembeddings is that they provide only a single repre-\nsentation of a given word across many documents.\nThis is not optimal in practice, as the representation\ndepends on the sentence in which the word appears.\nContextual word embeddings address this problem\nby capturing syntactic and semantic information at\nthe sentence level to represent words according to\ntheir context.\nContextualized embeddings are commonly re-\ntrieved from language models trained on giant text\ncorpora. These models are usually composed of se-\nquential or attention neural networks, which allows\nobtaining sentence-level semantics. This method\nhas contributed to major advances in several NLP\ntasks such as named entity recognition, text classifi-\ncation, and relation extraction. Classic examples of\ncontextual representation models are Flair (Akbik\net al., 2018), ELMo (Peters et al., 2018), and BERT\n(Devlin et al., 2019).\nRegarding specific domains such as clinical and\nbiomedical, there are widely used models for the\nEnglish language, such as BioBERT (Lee et al.,\n2020), BioELMo (Jin et al., 2019), and the PubMed\nversion of Flair. These studies have shown that\nincorporating domain-specific contextual word em-\nbeddings contributes to a significant improvement\nin the performance of the models. However, al-\nthough unstructured clinical texts are abundant in\nSpanish, there is still a significant lack of language\nmodels. Most of the domain-specific contextual\nrepresentation models available for Spanish focus\non data obtained from scientific articles and not\nfrom texts written in a more realistic context.\nTo fill this gap, we trained and publicly released\nClinical Flair1, a character-level language model\ntrained on a corpus with real diagnoses in Span-\nish. To measure the potential impact of using these\nrepresentations, we provide an empirical study of\nthe effects of using language models trained on\ndomain-specific against general-domain corpora.\nWe evaluated the effectiveness of the proposed em-\nbeddings on four named entity recognition datasets\nbelonging to the clinical and biomedical domain in\nSpanish. The results suggest that the embeddings\nobtained from our model contribute to achieving a\nbetter model performance compared to the general-\ndomain contextualized embeddings by a wide mar-\ngin.\n1https://github.com/plncmm/\nspanish-clinical-flair\n87\n2 Related Work\nLanguage models allow us to generate high-quality\nrepresentations of words based on their surround-\ning context, better known as contextual word em-\nbeddings. These models are usually trained with\nlarge corpora, either general-domain or domain-\nspecific. Most of the available models have been\ntrained with English resources, where the most pop-\nular ones are BERT (Devlin et al., 2019), ELMo\n(Peters et al., 2018), GPT-2 (Radford et al., 2019),\nand Flair (Akbik et al., 2018).\nAs pointed out in Lee et al. (2020), building\ndomain-specific language models allows to im-\nprove models performance compared to general-\ndomain language models. In relation to biomedi-\ncal information retrieval (IR) tasks in English, the\nmost well-known architectures are BioBERT (Lee\net al., 2020), Clinical BERT (Alsentzer et al., 2019),\nSciBERT (Beltagy et al., 2019), Pubmed BERT\n(Gu et al., 2022), BioELMo (Jin et al., 2019) and\nPubmed Flair.\nRegarding the clinical domain in Spanish, we\nfound the models Biomedical Roberta (Carrino\net al., 2022) and SciELO Flair (Akhtyamova et al.,\n2020). In the first case, the main difference with\nour model is that Biomedical Roberta was trained\non a corpus formed by several biomedical and clin-\nical corpora, while we only used clinical narratives.\nIn the case of SciELO Flair, a point of differentia-\ntion is that they used data obtained from medical\npublications, whereas our data comes from primary\ncare diagnoses. Moreover, they only tested their\nmodel on the PharmaCoNER corpus, created from\nthe same data source they trained SciELO Flair. In\ncontrast, we tested the effectiveness of our model\nusing four clinical and biomedical datasets.\n3 Methods\nThis section describes the clinical dataset used to\ntrain our language model, the details of the training\nprocess, and, finally, the task and datasets used in\nour experiments.\n3.1 Clinical Flair\nFlair (Akbik et al., 2018) is a character-level lan-\nguage model, which represents words as sequences\nof characters contextualized by the surrounded text.\nFlair authors created a method to obtain contextual-\nized representations by retrieving the internal states\nof a bidirectional character-level LSTM. Specifi-\ncally, the embedding is created by concatenating\nthe output of the hidden state after the last charac-\nter and before the first character of the word. This\nprocess allows obtaining the word context in the\nsentence in both directions.\nWe decided to use Flair instead of BERT because\nthe character-level language model is beneficial for\nhandling misspelled and out-of-vocabulary words,\nwhich are abundant in clinical and biomedical texts.\nThis is because BERT is limited to a predefined vo-\ncabulary used to perform the tokenization. When\na word is outside the vocabulary, the BERT model\ncombines the embeddings of its subwords to com-\npute the final representation, which may decrease\nthe quality of the embeddings. This does not occur\nin the case of Flair, where each word has an em-\nbedding independent of its subword embeddings.\nTo create our clinical version of Flair, we used\nas a starting point the existing language models es-\nforward and es-backward. These models trained on\na large corpus obtained from the Spanish Wikipedia\nare freely available in the Flair framework (Akbik\net al., 2019). To incorporate key information from\nthe clinical context, we fine-tuned these models on\nthe Chilean Waiting List corpus (Báez et al., 2020),\nwhich is a clinical corpus created from real diag-\nnoses from the Chilean public healthcare system.\nThe Chilean Waiting List corpus consists of\n5, 157, 902 free-text diagnostic suspicions compris-\ning 14, 057, 401 sentences and 68, 541, 727 tokens.\nAlthough the general purpose of this dataset was to\nbe a new resource for named entity recognition,\nit has also been used to obtain static word em-\nbeddings from the clinical domain (Villena et al.,\n2021b). These representations have boosted the\nmodel’s performance in several clinical NLP tasks\nsuch as tumor encoding (Villena et al., 2021a) and\nnamed entity recognition (Báez et al., 2022).\nWe did not perform any pre-processing of the\ndata for training our language model. The cor-\npus was divided into 60% for training, 20% for\nvalidation, and 20% for testing. According to the\nsuggestions of Flair authors, we set the maximum\nsentence length to 250, the mini-batches to 100\nsentences, the maximum training epochs to 1, 000,\nand the learning rate to 20. The experiments were\nperformed with a Tesla V100 GPU and 192 GB\nRAM. After one week of training, we reached a\nfinal perplexity value of 1.61 and 1.63 for our es-\nclinical-forward and es-clinical-backward models,\nrespectively.\n88\nCANTEMIST PharmaCoNER Clinical Trials NUBes\nTrain Test Dev Train Test Dev Train Test Dev Train Test Dev\nTokens 442,097 240,326 396,457 210,778 104,201 100,147 208,188 68,994 69,319 255,897 51,233 35,416\nSentences 19,397 11,168 18,165 8,177 3 ,976 3 ,790 12,555 4,506 4,550 13,802 2,762 1,840\nAvg sentence length22.8 21 .5 21 .8 25.8 26 .2 26 .4 16.6 15 .3 15 .3 18.5 18 .6 19 ,2\nEntities 6,347 3 ,596 5 ,948 3,821 1 ,876 1 ,926 24,224 7,717 8,258 17,122 3,548 2,293\nAvg entity length2.4 2 .3 2 .3 1.4 1 .4 1 .4 2.0 2 .0 2 .0 2.6 2 .6 2 .6\nTable 1: Statistics of the NER datasets used in our experiments.\n3.2 Datasets\nTo evaluate the quality of our contextual repre-\nsentations, we used the Named Entity Recogni-\ntion (NER) task, which seeks to identify spans of\ntext expressing references to predefined categories.\nSpecifically, we performed our experiments on four\nNER corpora belonging to the clinical and biomed-\nical domains. The statistics for each corpus are\nshown in Table 1.\n• CANTEMIST2 (Miranda-Escalada et al.,\n2020): An open annotated corpus that com-\nprises 1, 301 oncologic clinical case reports\nwritten in Spanish and manually annotated\nby clinical experts with mentions of tumor\nmorphology. It contains a total of 48, 730 sen-\ntences and 15, 891 entity mentions.\n• PharmaCoNER3 (Gonzalez-Agirre et al.,\n2019): Biomedical corpus created for recog-\nnizing chemical and protein entities. It con-\nsists of 1, 000 clinical cases with 7, 623 entity\nmentions, corresponding to four entity types.\n• Clinical Trials4 (Campillos-Llanos et al.,\n2021): It consists of 1, 200 texts collected\nfrom 500 abstracts of journal articles about\nclinical trials and 700 announcements of trial\nprotocols. It comprises a total of 40, 199 en-\ntity mentions, which belong to a subset of\nsemantic groups from the Unified Medical\nLanguage System (UMLS).\n• NUBes5 (Lima Lopez et al., 2020): Biomed-\nical corpus obtained from anonymized health\nrecords annotated with negation and uncer-\ntainty. It consists of 18, 404 sentences, includ-\ning 22, 963 mentions of negation and uncer-\ntainty.\n2https://zenodo.org/record/3978041\n3https://zenodo.org/record/4270158\n4http://www.lllf.uam.es/ESP/\nnlpmedterm_en\n5https://github.com/Vicomtech/\nNUBes-negation-uncertainty-biomedical-corpus\nParameter Value\nmax epochs 150\noptimizer SGD\nbatch size 32\ninitial learning rate 0.1\nword dropout 0.05\nBiLSTM layers 1\nBiLSTM hidden size 256\nTable 2: Hyperparameters used in our experiments.\n3.3 NER Model\nTo solve the NER task, we used the LSTM-CRF\napproach proposed by Lample et al. (2016), which\nis one of the most widely used architectures for\nsequence labeling tasks. The model consists of\nthree main modules: the embedding layer, the en-\ncoding layer with a BiLSTM, and the classification\nlayer, where the most likely sequence of labels is\nobtained using the CRF algorithm. Our contextu-\nalized embeddings were incorporated in the first\nlayer, replacing traditional representations such as\nword and character-level embeddings.\nTo compare the performance of our language\nmodel, we used two baselines: the Spanish\nFlair model trained on the general domain using\nWikipedia articles and the SciELO Flair model,\nwhich was trained over a subset of SciELO text.\nIn addition, it is worth mentioning that some of\nthe datasets had nested entities, i.e., entities con-\ntained within other entity mentions (Finkel and\nManning, 2009). Since traditional sequence label-\ning architectures cannot address this problem, we\nfollowed the simplifications made in previous work,\nkeeping only the outermost entities in each nesting.\n3.4 Settings\nTo select the best hyperparameters, we performed\nthe random search strategy, which selects the best\nvalues by exhaustively testing different combina-\ntions of hyperparameters over a range of values.\nWe measured the performance using the validation\npartition to establish the best combination.\n89\nSpanish Flair SciELO Flair Clinical Flair\nDataset P R F1 P R F1 P R F1\nCANTEMIST0.827 (0.002) 0.842 (0.003) 0.834 (0.001)0.850 (0.001) 0.864 (0.001) 0.857 (0.001)0.857 (0.004) 0.867 (0.001)0.862 (0.002)PharmaCoNER0.876 (0.002) 0.849 (0.001) 0.862 (0.001)0.905 (0.001) 0.889 (0.002)0.897 (0.001)0.901 (0.001) 0.875 (0.002) 0.888 (0.001)Clinical Trials0.809 (0.003) 0.815 (0.001) 0.812 (0.001)0.814 (0.005) 0.832 (0.001) 0.823 (0.002)0.836 (0.002) 0.834 (0.003)0.835 (0.001)NUBes 0.887 (0.002) 0.901 (0.003) 0.894 (0.001)0.888 (0.002) 0.905 (0.001) 0.896 (0.001)0.905 (0.002) 0.897 (0.001)0.901 (0.001)\nTable 3: Overall results on four clinical and biomedical NER datasets. Data shown are mean (SD).\nIn Table 2, we list the main hyperparameters\nused throughout our experiments, which were the\nones that gave us the best results in most of the\ndatasets. We trained the NER models using the\nSGD optimizer to a maximum of 150 epochs, with\nmini-batches of size 32 and a learning rate of 0.1.\nTo control overfitting, we used the early stopping\nstrategy and a dropout regularization of 0.05 after\nthe embedding layer.\nPerformance was evaluated using precision, re-\ncall, and micro F1-score, which is the standard met-\nric used in NER. This metric is strict since an entity\nis considered correct when both entity types and\nboundaries are predicted correctly. Three rounds\nof evaluation were computed using different seeds,\nreporting the mean and standard deviation. All the\nexperiments were performed using the Flair frame-\nwork, and the source code is available to reproduce\nour experiments6.\n4 Results\nTable 3 shows the overall performance of the NER\nmodel comparing contextualized embeddings re-\ntrieved from our Clinical Flair model, Spanish\nFlair, and SciELO Flair. We can see that across\nall datasets, the performance of our model is su-\nperior to the model trained on a general domain,\ndemonstrating the importance of incorporating con-\ntextualized embeddings trained on domain-specific\ncorpora.\nOn the other hand, although we did not train our\nmodel on biomedical corpora, we observe that it is\nalso beneficial for solving NER on those datasets.\nAlthough we did not outperform the SciELO Flair\nmodel in PharmaCoNER, we obtained competi-\ntive results. However, as mentioned in their paper,\nthey selected a subset of SciELO texts to train the\nlanguage model in line with the PharmaCoNER\ncorpus. Therefore, we expected that their results\nwould be superior.\nCompared with Spanish Flair, the major differ-\nence occurs in CANTEMIST, reaching an average\n6https://github.com/plncmm/\nclinical-flair\ndifference of +0.028, while the slightest difference\nis observed in NUBes with +0.007 according to\nthe F1 measure. One possible reason for the similar\nperformance between our model and Spanish Flair\nin NUBes is that, although the dataset belongs to\nthe biomedical domain, the task aims to identify\nentities associated with negations and uncertain-\nties; therefore, the target labels are general-domain\nand distant from the original corpus on which we\ntrained our model.\nFinally, and as expected, in both corpora belong-\ning to the clinical domain CANTEMIST and Clin-\nical Trials, our model outperforms both Spanish\nFlair and SciELO Flair. In the case of Clinical Tri-\nals, we reached an average difference of +0.023\nand +0.012 compared to both models, respectively,\nwhile in the case of CANTEMIST, we obtained\nimprovements of +0.028 and +0.005 according to\nthe F1 measure.\n5 Conclusions and Future Work\nDespite the growing interest of the NLP research\ncommunity in contextualized embeddings, there\nis still a lack of language models for the Spanish\nlanguage, a gap that increases even more concern-\ning domain-specific texts. To address this issue,\nthis paper introduced Clinical Flair, a character-\nlevel language model for clinical NLP in Spanish.\nSpecifically, we used a general-domain language\nmodel as a starting point and then fine-tuned it on\nChilean clinical narratives. Our experimental re-\nsults on four clinical and biomedical NER datasets\nshow that incorporating our domain-specific em-\nbeddings outperforms by a wide margin the results\nobtained with general-domain embeddings, demon-\nstrating the importance of having these resources\navailable for languages not as widely explored.\nFuture work includes extending our study to\nother NLP tasks and using different combinations\nof embeddings, such as concatenating Word2vec\nor character-level embeddings. In addition, to pro-\nvide a variety of contextual representation models\nfor clinical texts, we are training a clinical ver-\nsion of BERT in Spanish. Although preliminary\n90\nresults have been inferior to those obtained with\nour Clinical Flair model, we expect to collect a\nlarger clinical corpus to improve performance.\nAcknowledgements\nThis work was funded by ANID Chile: Basal Funds\nfor Center of Excellence FB210005 (CMM), Mil-\nlennium Science Initiative Program ICN2021_004\n(iHealth), and Fondecyt grant 11201250. This re-\nsearch was partially supported by the supercom-\nputing infrastructure of the NLHPC (ECM-02) and\nthe Patagón supercomputer of Universidad Austral\nde Chile (FONDEQUIP EQM180042). We also\nacknowledge the help received from Kinan Martin\nand the reviewers.\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-\nart NLP. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54–59, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018.\nContextual string embeddings for sequence label-\ning. In Proceedings of the 27th International Con-\nference on Computational Linguistics, pages 1638–\n1649, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nLiliya Akhtyamova, Paloma Martínez, Karin Verspoor,\nand John Cardiff. 2020. Testing contextualized word\nembeddings to improve ner in spanish clinical case\nnarratives. IEEE Access, 8:164717–164726.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nPablo Báez, Felipe Bravo-Marquez, Jocelyn Dunstan,\nMatías Rojas, and Fabián Villena. 2022. Automatic\nextraction of nested entities in clinical referrals in\nspanish. ACM Trans. Comput. Healthcare, 3(3).\nPablo Báez, Fabián Villena, Matías Rojas, Manuel\nDurán, and Jocelyn Dunstan. 2020. The Chilean\nwaiting list corpus: a new resource for clinical named\nentity recognition in Spanish. In Proceedings of the\n3rd Clinical Natural Language Processing Workshop,\npages 291–300, Online. Association for Computa-\ntional Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nLeonardo Campillos-Llanos, Ana Valverde-Mateos,\nAdrián Capllonch-Carrión, and Antonio Moreno-\nSandoval. 2021. A clinical trials corpus annotated\nwith umls entities to enhance the access to evidence-\nbased medicine. BMC Medical Informatics and De-\ncision Making, 21.\nCasimiro Pio Carrino, Joan Llop, Marc Pàmies, Asier\nGutiérrez-Fandiño, Jordi Armengol-Estapé, Joaquín\nSilveira-Ocampo, Alfonso Valencia, Aitor Gonzalez-\nAgirre, and Marta Villegas. 2022. Pretrained biomed-\nical language models for clinical NLP in Spanish.\nIn Proceedings of the 21st Workshop on Biomedi-\ncal Language Processing, pages 193–199, Dublin,\nIreland. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJenny Rose Finkel and Christopher D. Manning. 2009.\nNested named entity recognition. In Proceedings of\nthe 2009 Conference on Empirical Methods in Natu-\nral Language Processing, pages 141–150, Singapore.\nAssociation for Computational Linguistics.\nAitor Gonzalez-Agirre, Montserrat Marimon, Ander In-\ntxaurrondo, Obdulia Rabal, Marta Villegas, and Mar-\ntin Krallinger. 2019. PharmaCoNER: Pharmacologi-\ncal substances, compounds and proteins named entity\nrecognition track. In Proceedings of The 5th Work-\nshop on BioNLP Open Shared Tasks, pages 1–10,\nHong Kong, China. Association for Computational\nLinguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2022. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare, 3(1):1–23.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embeddings\nfrom language models. In Proceedings of the 3rd\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP, pages 82–89, Minneapolis, USA. As-\nsociation for Computational Linguistics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\n91\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: A pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nSalvador Lima Lopez, Naiara Perez, Montse Cuadros,\nand German Rigau. 2020. NUBes: A corpus of nega-\ntion and uncertainty in Spanish clinical texts. In\nProceedings of the 12th Language Resources and\nEvaluation Conference, pages 5772–5781, Marseille,\nFrance. European Language Resources Association.\nA Miranda-Escalada, E Farré, and M Krallinger. 2020.\nNamed entity recognition, concept normalization\nand clinical coding: Overview of the cantemist\ntrack for cancer text mining in spanish, corpus,\nguidelines, methods and results. In Proceedings of\nthe Iberian Languages Evaluation Forum (IberLEF\n2020), CEUR Workshop Proceedings.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nFabián Villena, Pablo Báez, Sergio Peñafiel, Matías\nRojas, Inti Paredes, and Jocelyn Dunstan. 2021a. Au-\ntomatic support system for tumor coding in pathology\nreports in spanish. SSRN Electronic Journal.\nFabian Villena, Jorge Perez, Rene Lagos, and Jocelyn\nDunstan. 2021b. Supporting the classification of\npatients in public hospitals in chile by designing,\ndeploying and validating a system based on natural\nlanguage processing. BMC Medical Informatics and\nDecision Making, 21(1):1–11.\n92"
}