{
  "title": "Compressing Transformers: Features Are Low-Rank, but Weights Are Not!",
  "url": "https://openalex.org/W4382318758",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108890553",
      "name": "Hao Yu",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2100460527",
      "name": "Jianxin Wu",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2108890553",
      "name": "Hao Yu",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2100460527",
      "name": "Jianxin Wu",
      "affiliations": [
        "Nanjing University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2894175714",
    "https://openalex.org/W2990735210",
    "https://openalex.org/W2772955562",
    "https://openalex.org/W3212072588",
    "https://openalex.org/W2993466051",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6750968397",
    "https://openalex.org/W4283809320",
    "https://openalex.org/W6701947533",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2929164058",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W3116594510",
    "https://openalex.org/W6748641889",
    "https://openalex.org/W6637551013",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6799730821",
    "https://openalex.org/W4221143977",
    "https://openalex.org/W7000196434",
    "https://openalex.org/W6788817589",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W6745136726",
    "https://openalex.org/W6743440100",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4297801368",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W4312439321",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3034342078",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3190449293",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2964118293",
    "https://openalex.org/W4287391433",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4298422451"
  ],
  "abstract": "Transformer and its variants achieve excellent results in various computer vision and natural language processing tasks, but high computational costs and reliance on large training datasets restrict their deployment in resource-constrained settings. Low-rank approximation of model weights has been effective in compressing CNN models, but its application to transformers has been less explored and is less effective. Existing methods require the complete dataset to fine-tune compressed models, which are both time-consuming and data-hungry. This paper reveals that the features (i.e., activations) are low-rank, but model weights are surprisingly not low-rank. Hence, AAFM is proposed, which adaptively determines the compressed model structure and locally compresses each linear layer's output features rather than the model weights. A second stage, GFM, optimizes the entire compressed network holistically. Both AAFM and GFM only use few training samples without labels, that is, they are few-shot, unsupervised, fast and effective. For example, with only 2K images without labels, 33% of the parameters are removed in DeiT-B with 18.8% relative throughput increase, but only a 0.23% accuracy loss for ImageNet recognition. The proposed methods are successfully applied to the language modeling task in NLP, too. Besides, the few-shot compressed models generalize well in downstream tasks.",
  "full_text": "Compressing Transformers: Features Are Low-Rank, but Weights Are Not!\nHao Yu, Jianxin Wu*\nState Key Laboratory for Novel Software Technology, Nanjing University, China\nyuh@lamda.nju.edu.cn, wujx2001@nju.edu.cn\nAbstract\nTransformer and its variants achieve excellent results in var-\nious computer vision and natural language processing tasks,\nbut high computational costs and reliance on large training\ndatasets restrict their deployment in resource-constrained set-\ntings. Low-rank approximation of model weights has been\neffective in compressing CNN models, but its application\nto transformers has been less explored and is less effec-\ntive. Existing methods require the complete dataset to fine-\ntune compressed models, which are both time-consuming and\ndata-hungry. This paper reveals that the features (i.e., activa-\ntions) are low-rank, but model weights are surprisingly not\nlow-rank. Hence, AAFM is proposed, which adaptively de-\ntermines the compressed model structure and locally com-\npresses each linear layer’s output features rather than the\nmodel weights. A second stage, GFM, optimizes the entire\ncompressed network holistically. Both AAFM and GFM only\nuse few training samples without labels, that is, they are few-\nshot, unsupervised, fast and effective. For example, with only\n2K images without labels, 33% of the parameters are removed\nin DeiT-B with 18.8% relative throughput increase, but only a\n0.23% accuracy loss for ImageNet recognition. The proposed\nmethods are successfully applied to the language modeling\ntask in NLP, too. Besides, the few-shot compressed models\ngeneralize well in downstream tasks.\nIntroduction\nThe transformer architecture (Vaswani et al. 2017) has been\nwidely used in the natural language processing (NLP) area\nover the past years. Inspired by its excellent performance\nin NLP, transformer-based models have established numer-\nous new records in various computer vision (CV) tasks, such\nas image classification (Dosovitskiy et al. 2021) and ob-\nject detection (Liu et al. 2021). Despite these progresses,\nmost of these transformer-based structures suffer from large\nmodel sizes, huge run-time memory consumption and high\ncomputational costs, which prohibit the model deployment\nto resource-constrained platforms. Therefore, compressing\ntransformer-based models has attracted immense interests in\nrecent years.\nLow-rank approximation is a useful technique to strike a\nbalance between model accuracy and model size. Some pre-\n*J. Wu is the corresponding author.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nvious NLP research efforts (Noach and Goldberg 2020; Hsu\net al. 2022) focus on exploring the factorization of model\nweights. These compression methods have been very suc-\ncessful in reducing model size and to speedup inference.\nBut, the accuracy drops dramatically, hence fine-tuning for\nmany epochs using the entire training set is needed to partly\nrecover from the accuracy loss. Nevertheless, to protect data\nprivacy and/or achieve rapid deployment, in many scenarios\nthere may be only the original model and a small number\nof samples available under a tight compression time bud-\nget. Fine-tuning a deep learning model with limited data\nwill easily lead to overfitting, which invalidates these exist-\ning methods. Furthermore, these methods simply compress\nthe large transformer model itself, lacking the exploration of\nhow the compressed models perform on downstream tasks.\nLastly, automatically determining the compression ratio of\neach layer is one of the main difficulties in low-rank decom-\nposition, but efforts in this aspect remain scarce.\nHence, we believe that in order to successfully perform\nlow-rank approximation on transformers, we need an effec-\ntive approach which can be easily applied in both NLP and\nCV , which only requiresfew samples and short compression\ntime, adaptively determines the compression model struc-\nture and generalizes well to downstream tasks.\nTo fulfill these goals, our key finding is that although in\nlinear layers of transformers the weight matrices are almost\nfull rank (i.e., not suitable for decomposition), the features\n(i.e., activations) are generally low-rank. Therefore, we pro-\npose a novel Atomic Feature Mimicking (AFM) approach to\nreplace traditional weight approximation methods locally.\nBesides, since different layers have different compression\nsensitivity, we design an adaptive search method to deter-\nmine the compressed model architecture, namely Adaptive\nAFM (AAFM), for low-rank decomposition. The approxi-\nmation will accumulate errors along with the depth incre-\nment, so we propose Global Feature Mimicking (GFM) to\nfine-tune the compressed models with only a small number\nof unlabeled samples. Our contributions are:\n• We propose a novel and effective framework for low-\nrank approximation of transformers, with the key find-\ning and novelty being mimicking the features rather than\nthe weights. Extensive experiments demonstrate that our\nmethods can compress both vanilla transformers and its\nvariants in CV and NLP. With fewer parameters and\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n11007\nhigher throughput, our compressed models achieve com-\nparable accuracy to the original models.\n• Our framework is few-shot, unsupervised and swift.\nGiven a pre-set compression target and a small number\nof unlabeled samples, our methods canquickly determine\nthe sub-model architecture and fine-tune the compressed\nmodel. No extra hyper-parameters are introduced.\n• The compressed model can be generalized into various\ndownstream tasks. Experimental results show that even\nunder the few-shot settings, our methods achieve excel-\nlent generalization of the compressed model.\nRelated Works\nWe first briefly review some closely related works.\nTransformers\nThe transformer (Vaswani et al. 2017) utilizes the multi-\nhead self-attention (MHSA) mechanism to handle long-\nrange dependencies between pairs of input tokens in NLP\ntasks. Language models constructed with transformers have\nachieved widespread success in NLP tasks such as transla-\ntion (Vaswani et al. 2017), language modeling (Baevski and\nAuli 2018) and question answering (Kenton and Toutanova\n2019). Transformers have recently been introduced into the\nCV field, too. ViT (Dosovitskiy et al. 2021) first demon-\nstrated that with sufficient training data (e.g., JFT-300M),\nstandard transformers can achieve state-of-the-art accuracy\nin image classification tasks. DeiT (Touvron et al. 2021)\nfurther explores existing data augmentation and regulariza-\ntion strategies. With the same architecture as ViT, DeiT is\nalso effective when using the smaller ImageNet-1K dataset.\nSwin Transformer (Liu et al. 2021) applies a shifted win-\ndowing scheme. In this paper, we will show that our feature-\nmimicking approach can handle standard transformers and\ntheir variants in both CV and NLP.\nLow-Rank Approximation for Transformers\nTransformer models tend to have a large number of param-\neters and are computationally intensive. To reduce model\nsize and speed up inference, a natural idea is to factorize\none weight matrix into two or more smaller matrices. A\ncommon technique for low-rank factorization is Singular\nValue Decomposition (SVD) (Golub and Van Loan 2013),\nwhich can be applied to any linear layer. Noach and Gold-\nberg (2020) first decomposed each weight matrix by SVD.\nThen, they used knowledge distillation to refine the weights.\nDrone (Chen et al. 2021) minimizes the approximation er-\nror of the input representations layer-by-layer by exploiting\nthe full dataset. Later we show that it is a better idea to ap-\nproximate the output vector at low rank. FWSVD (Hsu et al.\n2022) introduces Fisher information to measure the impor-\ntance of parameters. But it requires a large amount of labeled\ndata to fine-tune the compressed model, and the weight ma-\ntrices are not necessarily low rank—in fact they are often\nfull rank instead. We perform low-rank decomposition on\nthe output feature maps rather than the weights, which re-\nquires only a small number of unlabeled samples. To the best\nof our knowledge, although weight matrix decomposition is\nwidespread, this is the first attempt to decompose transform-\ners’ features under the few-shot unsupervised settings.\nFeature Mimicking\nKnowledge Distillation (KD) (Hinton, Vinyals, and Dean\n2015) is a popular method to train student networks with\nthe help of high-capability teacher networks. The typical\nKD approach attempts to refine the softmax outputs, and\nthe feature mimicking distillation method was first proposed\nby FitNets (Romero et al. 2014). Wang, Ge, and Wu (2021)\ndemonstrate that it is more beneficial to make students only\nmimic the teacher’s features in the penultimate layer. Fea-\nture mimicking is also often used for model compression.\nFSKD (Li et al. 2020) adds a 1×1 convolutional layer after\neach layer and mimics the one-layer feature maps by solv-\ning a least-square problem. CD (Bai et al. 2020) optimizes\nmodel layer-by-layer by cross distillation between the com-\npressed and the original networks. MiR (Wang et al. 2022)\nis a global distillation method that only mimics one layer’s\nfeature map. Our framework combines their ideas, i.e., first,\nwe mimic the single-layer feature maps and then distill the\nglobal features. Numerous experiments demonstrate the ef-\nfectiveness of our approach.\nThe Proposed Methods\nWe describe our framework in this section, starting from the\npreliminaries. Then, we present our low-rank decomposi-\ntion method, Atomic Feature Mimicking (AFM). To auto-\nmatically determine the structure of the compressed model,\nwe propose Adaptive Atomic Feature Mimicking (AAFM),\na simple but effective low-rank approximation framework.\nFinally, we apply Global Feature Mimicking (GFM) to min-\nimize the output features’ difference. Throughout the com-\npression process, our framework requires only a small num-\nber of unlabeled samples.\nPreliminaries\nWe first introduce the classical low-rank decomposition\nmethod. Suppose we have a fully-connected (FC) layer,\nwhose input is a matrix x ∈ Rn×c and output is another\nmatrix y ∈ Rm×c. The relationship between them is simple:\ny = W x+ b , (1)\nwhere W ∈ Rm×n and b ∈ Rm. A standard way to acceler-\nate this computation is to perform low-rank approximation\nof W, i.e.,\ny ≈ W2(W1x + b1) +b2 , (2)\nwhere W1 ∈ Rk×n, b1 ∈ Rk and W2 ∈ Rm×k, b2 ∈ Rm.\nThe solution of Eq. 2 can be obtained by applying SVD on\nW. We can decompose W into\nW = USV T , (3)\nwhere U ∈ Rm×m and V ∈ Rn×n are orthonormal matri-\nces. S ∈ Rm×n is a diagonal rectangular matrix containing\nsingular values in the decreasing order. If we only use the\nlargest k terms of the singular values, the resulting matrix is\nan optimal approximation of W with a lower rank k < n:\nW ≈ U′V ′T , (4)\n11008\nwhere U′ ∈ Rm×k and V ′ ∈ Rk×n are the rank-k approxi-\nmation matrices by taking U′ = US\n1\n2\nk and V ′ = V S\n1\n2\nk , and\nS\n1\n2\nk is a diagonal matrix formed by the square-roots of the\ncorresponding top k singular values in S. Then, the original\nEq. 1 can be approximated as:\ny = W x+ b ≈ U′V ′T x + b. (5)\nAfter this low-rank approximation, the number of param-\neters in this linear layer decreases from O(mn) to O((m +\nn)k). However, in many models, particularly transformer\nand its variants, the weight W is nearly full rank (cf. Fig-\nure 1). In other words, if we separate one FC into two by\nSVD, we have to either choose a small k but endure large\naccuracy drop, or use a large k but increase the model size.\nAtomic Feature Mimicking (AFM)\nNow we propose Atomic Feature Mimicking (AFM). In-\nstead of decomposing the model weights, our AFM aims to\nfactorize the output features. Since we only mimic the fea-\nture within one single layer and do not involve any other\nlayers, it is atomic.\nSpecifically, we follow the notation described in Wu\n(2020). Let us treat the output feature in Rm×c as c instan-\ntiations of the random feature vector y (each in Rm), and\ncompute the covariance matrix:\nCov(y) =E\n\u0002\nyyT \u0003\n− E[y]E[y]T , (6)\nwhere E[·] is the expectation operator. SinceCov(y) is posi-\ntive semi-definite, its eigendecomposition (i.e., the principal\ncomponent analysis or PCA) is\nCov(y) =USU T . (7)\nWe only keep the top k eigenvalues and extract the first k\ncolumns of U ∈ Rm×m into Uk ∈ Rm×k and UkUT\nk ≈ I,\nhence\ny − E[y] ≈ UkUT\nk (y − E[y]) , or, (8)\ny ≈ UkUT\nk y + E[y] − UkUT\nk E[y] . (9)\nThat is, one linear layer can be transformed into two:\ny ≈ UkUT\nk (W x+ b) +E[y] − UkUT\nk E[y], (10)\n= Uk(UT\nk W x+ UT\nk b) +E[y] − UkUT\nk E[y], (11)\nwhere the first FC layer has weightsUT\nk W ∈ Rk×n and bias\nUT\nk b ∈ Rk, and the second one has weightsUk ∈ Rm×k and\nbias E[y] − UkUT\nk E[y] ∈ Rm.\nAlgorithm 1 presents the details of AFM. In order to cal-\nculate the covariance matrix, we randomly select a small\nnumber of samples from the training dataset to establish\na proxy dataset D. In only one forward execution, we can\ngather the output features for computing the covariance ma-\ntrices in all FC layers and decompose all FCs. Note that we\nadaptively updateE[yyT ] and E[y] in a streaming fashion in-\nstead of storing all output features. As our experiments will\nshow later, with only few samples we can compute weights\nwith good generalization and will not significantly decrease\nthe model accuracy.\nAlgorithm 1 Atomic Feature Mimicking\nInput: The original model M with weights W and bias b in\nthe i-th layer, the proxy dataset D and a pre-set rank k.\nOutput: Two compressed FC layers with weights W1 and\nW2, and biases b1 and b2.\n1: for each sample x in D do\n2: Forward propagate M(x) to obtain the output feature\ny in the i-th layer and update E[yyT ] and E[y].\n3: end for\n4: Calculate the eigenvectors U based on Eq. 6 and Eq. 7.\n5: Extract the first k columns of U into Uk, and obtain\nW1 = UT\nk W, b1 = UT\nk b, W2 = Uk, and b2 =\nE[y] − UkUT\nk E[y].\n6: return (W1, b1), (W2, b2)\nFigure 1 explains why we should approximate the fea-\ntures rather than the weights. We send the entire ImageNet-\n1K (Deng et al. 2009) validation set into DeiT-B. DeiT\nmodels contain two components, the attention layer and the\nFeed-Forward Network (FFN). Each component contains\ntwo FC layers, which we refer to as QKV and PROJ in the\nattention layer, plus FC1 and FC2 in the FFN. In particu-\nlar, we collect the input and output features of the QKV and\nFC1 layers in every block. Then we compute the covariance\nmatrix of these features separately and calculate the eigen-\nvalues. We also decompose the weights of QKV and FC1 by\nSVD. Figure 1 shows the percentages of eigen or singular\nvectors we need to keep in order to reach 90% of the energy\nfor them. As we can see, when 90% of the energy is re-\ntained, the dimensionality required for the output features is\nless than the input features and model weights, which shows\nthat output features are more likely low-rank (i.e., decompo-\nsition friendly) but the model weights are not.\nAdaptive Atomic Feature Mimicking (AAFM)\nAs aforementioned, one great challenge in low-rank decom-\nposition is to accurately determine the ranks k retained by\ndifferent layers. We propose Adaptive AFM (AAFM) to\novercome this difficulty. The basic idea behind AAFM is\nto keep higher rank or even not compress those more sen-\nsitive layers, while adopting a more aggressive compression\nstrategy for those less sensitive ones.\nTo measure a layer’s sensitivity score, we apply the proxy\ndataset D and extract the output logits of the original model\nM. Then we evaluate the performance change before/after\napplying AFM in a single layer. To maximize the GPU uti-\nlization and to reduce the search overhead, we fix rank k to\nbe a multiple of 32. Then, we compute the sensitivity score\nfor each layer separately, as the KL-Divergence between two\nmodels with and without AFM, i.e.,\nSi(k) =\nX\nx∈D\nDKL(M(x, w)∥M(x, wi(k))) , (12)\nwhere Si(k) measures how sensitive the i-th layer is when\nthe rank is k, and wi(k) refers to compress model weights\nin the i-th layer with rank k. The larger the score Si(k), the\nmore sensitive this layer is.\n11009\n/g19\n/g19/g17/g20\n/g19/g17/g21\n/g19/g17/g22\n/g19/g17/g23\n/g19/g17/g24\n/g19/g17/g25\n/g19/g17/g26\n/g19/g17/g27\n/g19/g17/g28\n/g20/g21/g22/g23/g24/g25/g26/g27/g28 /g20 /g19 /g20 /g20 /g20 /g21\n/g53/g68/g87/g76/g82\n/g37/g79/g82/g70/g78/g3/g44/g81/g71/g72/g91\n/g44/g81/g83/g88/g87\n/g50/g88/g87/g83/g88/g87\n/g58/g72/g76/g74/g75/g87\n/g19\n/g19/g17/g20\n/g19/g17/g21\n/g19/g17/g22\n/g19/g17/g23\n/g19/g17/g24\n/g19/g17/g25\n/g19/g17/g26\n/g19/g17/g27\n/g19/g17/g28\n/g20/g21/g22/g23/g24/g25/g26/g27/g28 /g20 /g19 /g20 /g20 /g20 /g21\n/g53/g68/g87/g76/g82\n/g37/g79/g82/g70/g78/g3/g44/g81/g71/g72/g91\n/g44/g81/g83/g88/g87\n/g50/g88/g87/g83/g88/g87\n/g58/g72/g76/g74/g75/g87\nFigure 1: Ratio of dimensions kept in each QKV (left) and FC1 (right) layer in DeiT-B when 90% energy is retained. The output\nfeatures (shown in red) have the lowest proportion compared to the input features (shown in blue) and the weights (shown in\ngray). The x-axis is the block index. DeiT-B has 12 blocks, and every block contains a QKV and an FC1 layer.\nAfter obtaining sensitivity scores with different rank k,\ngiven a target model sizePtar, we minimize the sum of sen-\nsitivity scores of all layers, i.e.,\nmin\n{ki}l\ni=1\nS =\nlX\ni=1\nSi(ki) (13)\ns.t.\nlX\ni=1\nPi(ki) ≤ Ptar , (14)\nwhere Pi(ki) is the number of parameters for the i-th layer\nwith rank ki, and l is the total number of linear layers. Our\nAAFM can generate multiple sub-networks from a well-\ntrained large model when a set of different targets Ptar are\ngiven, which is versatile.\nAs ki are integers, this is an integer programming problem\nand we approximately solve it by proposing a simple greedy\nalgorithm. It is worth noting that we have made a simplify-\ning assumption: the sensitivity of a layer is independent of\nthe ranks chosen for other layers. Our greedy algorithm is\nnot guaranteed to find the best possible configuration. How-\never, finding the precise global minimum from all potential\nrank configurations is very time-consuming, and our simpli-\nfied greedy approximation can greatly speedup the search\nprocess. Later, we will demonstrate that the adaptive config-\nuration we find via the greedy algorithm achieves excellent\nresults in experiments for different model architecture and\nin diverse tasks.\nGlobal Feature Mimicking (GFM)\nAlthough the AAFM reconstruction error is small in one\nlayer, they accumulate as more linear layers are approxi-\nmated. Hence, following MiR (Wang et al. 2022), our final\nstep is to use Global Feature Mimicking (GFM) to correct\nthem after applying AAFM.\nGFM is very simple: with the few-shot examples (i.e., the\nproxy dataset D), we mimic the output feature in the penulti-\nmate layer (before the GAP layer). We use the mean squared\nerror to measure the distance and the optimization is\nLMSE\n\u0000\nfL\nc , fL\no\n\u0001\n, (15)\nwhere fc and fo are the features of the compressed network\nand the original network, respectively, and L is the index of\nthe layer whose features are mimicked. In ViTs, fL is the\noutput feature map after the final LayerNorm layer; in the\nlanguage modeling task, it is the feature map after the final\nblock and before the adaptive softmax layer. Note that GFM\ndoes not involve the classification FC layer. We will empiri-\ncally show that even in the few-shot setting, fine-tuning the\ncompressed network by GFM will not lead to overfitting,\nand can be very helpful in boosting the accuracy.\nIn summary, our framework first uses AAFM to determine\nthe sub-model structure, and applies Algorithm 1 (AFM) to\ndecompose it, and finally uses GFM to fine-tune the com-\npressed model. Our framework only requires the unlabeled\nproxy dataset D, which is unsupervised. Because the proxy\ndataset D contains only a small number of samples, our ap-\nproach is few-shot and fast. We only need a pre-defined com-\npression target to determine the model architecture and pa-\nrameters adaptively, so no additional hyper-parameters are\nintroduced.\nExperiments\nWe now evaluate our methods. We first compress DeiT and\nSwin Transformers on ImageNet-1K classification. In addi-\ntion, more results on downstream small-scale classification\nand object detection datasets will be presented. We also eval-\nuate our framework on a language modeling task. We will\ndemonstrate that our approaches attain equivalent accuracies\nwith significantly fewer parameters on these tasks. Finally,\nwe end this section with several analyses. All the experi-\nments were conducted with PyTorch.\nDatasets and Metrics\nClassification. The ImageNet-1K (Deng et al. 2009) dataset\nconsists of 1.28 million training and 50K validation images.\nThose images have various spatial resolutions and come\nfrom 1K different categories. ImageNet-1K is usually used\nas the benchmark for model compression.\nBesides ImageNet-1K, we also evaluate our compressed\nmodels on several small-scale datasets.\nObjection Detection & Segmentation. We evaluate ob-\nject detection & segmentation performance on the MS\nCOCO2017 (Lin et al. 2014) dataset. MS COCO2017 con-\n11010\ntains 80 categories with 118K training and 5K validation im-\nages, respectively. We use mean Average Precision (mAP) to\nmeasure the accuracy.\nLanguage Modeling. We also evaluate our approach in\nthe WikiText-103 (Merity et al. 2017) dataset. WikiText-103\nis composed of shuffled Wikipedia articles where the con-\ntext carries across sentences. The training data of WikiText-\n103 comprises about 100M tokens with 28K articles and a\nvocabulary of around 260K. The test data contains 245K\ntokens with 4358 sentences. We use perplexity to measure\nthe performance of models. A lower perplexity indicates the\nprobability distribution is good at predicting the sample.\nCompressing DeiT & Swin\nAs mentioned before, our method needs a proxy dataset D\nto calculate the compressed weights, so we first sample 2K\nimages from the ImageNet-1k training dataset to form it.\nThen, since larger models tend to have more parameter re-\ndundancy, we test the performances of AAFM and GFM on\nDeiT-B and Swin-B & L.\nImplementation details. First, we set three different\ncompression levels when applying AAFM. In particular, 1/5,\n1/4, or 1/3 of the parameters of Swin-B & Swin-L were to\nbe removed, while DeiT-B’s model size was reduced by 1/5,\n1/3, or 2/5, respectively. We only compressed the four FC\nlayers in the blocks and used eight NVIDIA 3090 GPUs to\ncalculate the sensitivity scores. The whole AAFM process\ntook 0.6 hours when compressing DeiT-B. As a baseline or\ncomparison method for AAFM, we also performed SVD on\nthe original transformer model, i.e., we retain half of the sin-\ngular values for each FC layer in the transformer’s blocks.\nThen we fine-tuned the sub-models with GFM on the\nproxy dataset. When fine-tuning DeiT-B, we initialized the\nlearning rate as 8e-5 and used a mini-batch size of 512.\nWhen we fine-tuned Swin-B & Swin-L, we set the learn-\ning rate and mini-batch size as 3e-5 and 256, respectively.\nIn the above experiments, we used the AdamW (Loshchilov\nand Hutter 2018) optimizer and the cosine decay sched-\nule (Loshchilov and Hutter 2017). The sub-models were\nfine-tuned with 1000 epochs and the weight decay was 0.01.\nSince the proxy dataset D is small, fine-tuning 1000 epochs\nis still very fast. Random horizontal flipping, color jitter-\ning, Mixup (Zhang et al. 2018) and CutMix (Yun et al.\n2019) were applied as data augmentations. Particularly, Yu\net al. (2021) found that strong regularization has a negative\ninfluence on model performance in the later training period,\nand we also noticed a similar observation during the GFM\nprocess. Hence, in all of our experiments, we removed ran-\ndom erasing (Zhong et al. 2020), Rand-Augment (Cubuk\net al. 2020) and layer dropout (Huang et al. 2016). When\nfine-tuning the DeiT-B with 40% parameters removed, the\nentire GFM process consumed 0.6 hours. Therefore, our\nAAFM and GFM feature mimicking framework can finish\nfairly quickly.\nResults. Table 1 shows the results of compressing DeiT-\nB and Swin-B & Swin-L. We tested model accuracy on the\nImageNet-1K validation dataset. During testing, the shorter\nside was resized as 256 by bilinear interpolation and then we\ncropped the 224×224 image patch in the center. The accu-\nModel Throughput #Param.(M) Acc. (%)\nDeiT-B 619.46 86.57 81.85\n+SVD 741.07 (+19.6%) 58.27 (-33%) 77.21\n+GFM 80.36\n+AAFM 682.23 (+10.1%) 69.25 (-20%) 81.76\n+GFM 81.83\n+AAFM 735.97 (+18.8%) 58.26 (-33%) 81.21\n+GFM 81.62\n+AAFM 771.07 (+24.5%) 51.95 (-40%) 80.33\n+GFM 81.28\nSwin-B 458.86 88.10 83.47\n+SVD 489.95 (+6.8%) 60.20 (-33%) 74.30\n+GFM 81.13\n+AAFM 471.64 (+2.8%) 70.50 (-20%) 82.89\n+GFM 83.19\n+AAFM 477.71 (+4.1%) 66.09 (-25%) 82.41\n+GFM 83.00\n+AAFM 489.46 (+6.7%) 60.20 (-33%) 81.15\n+GFM 82.68\nSwin-L 257.40 196.87 86.25\n+SVD 288.82 (+12.2%) 134.09 (-33%) 82.02\n+GFM 84.52\n+AAFM 275.14 (+6.9%) 157.52 (-20%) 85.94\n+GFM 86.01\n+AAFM 282.58 (+9.8%) 147.67 (-25%) 85.73\n+GFM 85.83\n+AAFM 292.04 (+13.5%) 134.09 (-33%) 85.04\n+GFM 85.44\nTable 1: Top-1 accuracy (%) of performing low-rank approx-\nimation on DeiT-B and Swin-B & Swin-L.\nracy of the last epoch is reported. We also list the throughput\nin a 3090 GPU with a fixed 512 mini-batch size.\nComparison between the original models and our com-\npressed models proves the effectiveness of our framework.\nWe obtained a 24.5% throughput speedup and only a 0.57%\naccuracy reduction when we removed 40% of the parame-\nters in DeiT-B, while we lost only 0.02% accuracy when we\nremoved 20% of the parameters. When compressing Swin-\nB, we gained a 6.7% increase in throughput and a 33% de-\ncrease in parameters with dropping 0.79% accuracy. It is\nworth noting that our AAFM performance far exceeds that\nof traditional SVD methods, for example, when compress-\ning DeiT-B and Swin-B by removing 33% of the parameters,\nour AAFM is 4 percentage points (81.21% vs. 77.21%) and\n6.85 percentage points (81.15% vs. 74.30%) higher than that\nof SVD, respectively. Furthermore, Swin-L is pre-trained on\nthe large-scale ImageNet-21K dataset, and our method has\nbeen demonstrated to be effective in it, too.\nTransferring Ability\nTo further validate the effectiveness of our methods, we in-\nvestigated the compressed models’ transferring ability in\nseveral downstream tasks. We first investigated Swin-B’s\nmAP on MS COCO2017 detection and segmentation. Then\nwe tested the accuracy of the compressed DeiT-B in small-\nscale classification datasets.\nImplementation details. We used both the original Swin-\n11011\nBackbone Tasks AP AP 50 AP75 APS APM APL\nSwin-B Detection 52.0 70.8 56.4 35.0 55.6 67.4\nOurs 51.9 70.5 56.4 35.5 55.8 67.0\nSwin-B Segmentation 45.0 68.3 48.8 28.5 48.6 60.6\nOurs 44.7 67.9 48.5 28.8 48.4 59.7\nTable 2: mAP of Swin-B and our compressed model on the MS COCO2017 validation dataset.\nModels DeiT-B Ours\n#Param.(M) 86.57 69.25 58.26 51.95\nCIFAR-100 90.99 90.67 90.37 90.17\nCUB-200 85.88 85.07 85.38 84.85\nCars 90.45 91.18 90.66 90.72\nAircraft 79.87 80.92 81.19 80.80\nPets 94.74 94.22 93.98 93.95\nFlowers 97.77 97.45 97.30 97.02\niNaturalist-2019 77.39 77.56 76.70 77.13\nTable 3: Accuracy (%) of DeiT-B and our compressed mod-\nels on different small-scale classification datasets.\nB and our compressed Swin-B with 33% parameters re-\nmoved as backbones of Cascade Mask R-CNN (Cai and\nVasconcelos 2018). We followed the training settings of the\noriginal Swin Transformer paper and set the training sched-\nule as 3x (36 epochs).\nWhen fine-tuning the compressed DeiT-B, we adopted\nmini-batch size 1024 and learning rate 1e-4. We applied\nthe AdamW optimizer, cosine learning rate schedule and\nthe CutMix augmentation strategy. CutMix’s ratio was set\nto 0.5. We trained models with 100 epochs.\nResults. Table 2 shows the detection and segmentation\nresults on MS-COCO2017. The compressed Swin-B model\nachieved similar mAPs compared to the original model. For\nexample, after removing 33% of the backbone parameters,\nour model achieved 51.9 and 44.7 mAPs on object detec-\ntion and segmentation tasks, which is on par with that of the\noriginal Swin-B.\nTable 3 shows the classification results. The compressed\nmodels always achieved similar accuracy to DeiT-B on all\n7 datasets, indicating that our method maintains the gener-\nalization ability of the original model. On the Cars, Aircraft\nand iNaturalist-2019 datasets, our compressed model with\nfewer parameters even performed better than the original\nDeiT-B model.\nCompressing Transformer for NLP Tasks\nWe also compressed the standard transformer with adap-\ntive input representations (Baevski and Auli 2018) on the\nWikiText-103 dataset. Our methods achieved comparable\nperplexity with the original model.\nImplementation details. We implemented our methods\nbased on fairseq (Ott et al. 2019). The original transformer\nmodel follows the architectural choice described in Baevski\nand Auli (2018), which includes 16 decoder blocks and sinu-\nsoidal position embeddings in the input layer. Each MHSA\nmodule has 8 heads and adaptive input representations have\nDataset Throughput #Param.(M) Perplexity\nWikiText-103 3137.3 246.9 18.66\n+SVD 3303.3 (+5.3%) 196.6 (-20%) 29.76\n+GFM 20.24\n+AAFM 3252.3 (+3.7%) 209.9 (-15%) 20.23\n+GFM 19.07\n+AAFM 3293.2 (+5.0%) 196.6 (-20%) 22.34\n+GFM 19.46\n+AAFM 3356.4 (+7.0%) 185.2 (-25%) 26.20\n+GFM 20.05\nTable 4: Results of compressing the standard transformer\nwith adaptive inputs in language modeling.\nthree bands of size 20K, 40K and 200K. The embedding\nlayer and FFN’s hidden-state have dimensions of 1024 and\n4096, respectively. We sampled 4K sentences to form the\nproxy dataset D. Then, we reduced the parameters by 15%,\n20% and 25%.\nDuring the GFM process, we removed layer dropout and\ntrained on 8 GPUs. We limited the number of tokens per\nGPU to a maximum threshold 1536, which means each GPU\nprocesses 1536 tokens using the same model parameters. We\naccumulated gradient updates over 8 batches before commit-\nting a parameter update following Ott et al. (2018). We set\nthe batch size as 32 and trained 1000 updates. The AdamW\noptimizer was used and the weight decay was 0.05. The\nlearning rate was linearly warmed up from 1e-7 to 3e-5\nfor 30 steps and then annealed using a cosine learning rate\nschedule. We renormalized gradients if their norm exceeds\n0.1. In particular, we fixed the adaptive input and softmax\nlayer during fine-tuning.\nResults. Table 4 shows the results on WikiText-103. Dur-\ning testing, we denoted the size of context window as 2560.\nSimilar to previous experiments, our algorithms obtained re-\nsults that are comparable to those of the original model. Es-\npecially, AAFM decreased the perplexity by 7.42 compared\nto SVD when removing 20% parameters.\nAnalyses\nTo explore the impact of different modules of our method,\nwe performed three analyses in this section.\nThe influence of AAFM. We first explore the influence\nof our adaptive structure searching approach. In particular,\nwe take Swin-B and the transformer trained in WikiText-103\nas examples, and we compress these two models by 33%\nand 20% of parameters, respectively. We design two exper-\niments. The first employs AFM while retaining half of the\ndimensions for each FC layer in model blocks, while the sec-\n11012\nModel Adaptive Top-1 Acc. (%) Perplexity\nOriginal Model 83.47 18.66\n+AFM % 78.16 22.55\n+GFM 81.61 20.21\n+Adaptive SVD \" 80.24 122.41\n+GFM 82.49 20.23\nTable 5: Top-1 accuracy (%) and perplexity of exploring the\nimpact of adaptive structure searching method on Swin-B\nand transformer.\nond one exploits the model structure searched by AAFM but\nusing SVD to initialize sub-model. We refer to the second\nmethod as adaptive SVD. For a fair comparison, we adopt\nthe same training strategies as above.\nTable 5 summarizes the results. We discovered that our\nAFM outperformed SVD but was inferior to AAFM. For ex-\nample, on Swin-B AFM achieved 78.16% accuracy, which is\nhigher than SVD’s 74.30% accuracy but lower than AAFM’s\n81.15% accuracy (cf. Table 1). Furthermore, the model\nstructures we searched have good generalization. For exam-\nple, our adaptive SVD obtained 80.24% accuracy on the Im-\nageNet validation dataset, which is better than the typical\nSVD low-rank approximation method (77.21%, cf. Table 1).\nKnowledge distillation. We then compare several differ-\nent distillation strategies when fine-tuning sub-models. We\ncontinue to use the Swin-B and transformer sub-models with\n33 and 25 percent parameter removal, respectively. Besides\nGFM, we consider the following distillation approaches:\n• Soft distillation. q is the teacher’s output after softmax.\np is the output of the compressed sub-model and y is the\ntrue label. The soft distillation objective is:\nLCE (p, y) +αLKL(p, q). (16)\n• Soft distillation without labels. The loss function is:\nLKL(p, q). (17)\n• Hard distillation. Let yt = arg max(q) be the hard deci-\nsion of the teacher. The loss function of hard-label distil-\nlation is:\nLCE (p, y) +αLCE (p, yt) . (18)\n• Hard distillation without labels. The loss function is:\nLCE (p, yt) . (19)\n• GFM with labels. We add label information into GFM,\ni.e.,\nLCE (p, y) +αLMSE\n\u0000\nfL\nc , fL\no\n\u0001\n. (20)\nWe set the α as 1.0 and illustrate the results in Table 6.\nWe can conclude that under the few-shot settings, including\nlabel information when training the compressed model can\neasily lead to overfitting, while our GFM still obtains the\nbest results when fine-tuning the compressed model.\nThe proxy dataset size. We further research on the influ-\nence of the number of training samples in the proxy dataset\nD. Let us take Swin-B with 33% parameters removed as\nan example. We set the size of dataset D to 1K, 2K, 5K,\nDistillation Top 1 Acc. (%) Perplexity\nSoft w/ label 82.34 24.81\nw/o label 82.38 20.53\nHard w/ label 81.05 323.95\nw/o label 82.08 1.2 × 105\nGFM w/ label 78.81 20.10\nw/o label 82.68 20.05\nTable 6: Results on the ImageNet-1K and WikiText-103 val-\nidation datasets with different distillation strategies.\nSizes 1K 2K 5K 10K 100K 1.28M\nAAFM 81.21 81.15 81.20 81.31 81.30 81.22\nGFM 82.38 82.68 82.75 82.88 82.97 82.99\nTable 7: Top-1 accuracy (%) on the ImageNet-1K validation\nset with different number of training samples in the proxy\ndataset D.\n10K, 100K and the whole training dataset respectively. We\napplied AAFM and GFM sequentially on different proxy\ndatasets. In particular, we trained 10 and 100 epochs when\nD contains the full training samples and 100K samples, re-\nspectively, and trained 1000 epochs in other cases.\nThe results are showed in Table 7. The 1.28M in the ta-\nble refers to the entire dataset. We can conclude the final\naccuracy increases as the sample size grows, but the bene-\nfit is very limited. When applying the full training dataset,\nthe AAFM and final accuracy is only 0.07% (81.22% vs.\n81.15%) and 0.31% (82.99% vs. 82.68%) higher than us-\ning 2K samples each, respectively. This indicates that few\nunlabeled samples are sufficient for our feature-mimicking\nalgorithms. Therefore, we sample 2K images as the proxy\ndataset for faster speed and better simplicity.\nDiscussions and Conclusions\nIn this paper, we presented a novel framework for low-rank\napproximation of transformers. We built our framework after\nrevealing that the features are low-rank but model weights\nare not, which worked well in both CV and MLP. Extensive\nexperiments confirmed the efficacy of our framework. We\ncan quickly reduce the model size with small drop in model\naccuracy. In addition, our approach requires only a small\nnumber of unlabeled samples and effectively preserves the\noriginal model’s generalization capability.\nWe discover that because low-rank decomposition divides\na linear layer into two layers, it does not improve throughput\nsignificantly. Therefore, applying low-rank decomposition\nin a reasonable way to speed up inference is an intriguing\nfuture direction. Besides this, we randomly select samples\nto form the proxy dataset, so we will continue to explore\nthe effects of data distribution in the proxy dataset. Further-\nmore, our method can theoretically be applied to various\ndeep learning models, such as CNNs, so we will continue\nto extend our method to these models in the future.\n11013\nAcknowledgments\nThis research was partly supported by the National Natu-\nral Science Foundation of China under Grant 62276123 and\nGrant 61921006.\nReferences\nBaevski, A.; and Auli, M. 2018. Adaptive Input Representa-\ntions for Neural Language Modeling. In International Con-\nference on Learning Representations (ICLR).\nBai, H.; Wu, J.; King, I.; and Lyu, M. 2020. Few shot net-\nwork compression via cross distillation. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, 3203–3210.\nCai, Z.; and Vasconcelos, N. 2018. Cascade R-cnn: Delv-\ning into high quality object detection. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n6154–6162.\nChen, P.; Yu, H.-F.; Dhillon, I.; and Hsieh, C.-J. 2021.\nDrone: Data-aware low-rank compression for large nlp mod-\nels. In Advances in Neural Information Processing Systems,\nvolume 34, 29321–29334.\nCubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V . 2020. Ran-\ndAugment: Practical automated data augmentation with a re-\nduced search space. In The IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 702–703.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. ImageNet: A large-scale hierarchical image\ndatabase. In The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 248–255.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations (ICLR).\nGolub, G. H.; and Van Loan, C. F. 2013. Matrix computa-\ntions. Johns Hopkins University Press.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. arXiv:1503.02531.\nHsu, Y .-C.; Hua, T.; Chang, S.; Lou, Q.; Shen, Y .; and Jin,\nH. 2022. Language model compression with weighted low-\nrank factorization. In International Conference on Learning\nRepresentations (ICLR).\nHuang, G.; Sun, Y .; Liu, Z.; Sedra, D.; and Weinberger,\nK. Q. 2016. Deep Networks with Stochastic Depth. In The\nEuropean Conference on Computer Vision (ECCV), volume\n9908 of LNCS, 646–661. Springer.\nKenton, J. D. M.-W. C.; and Toutanova, L. K. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Lan-\nguage Understanding. In Proceedings of NAACL-HLT,\n4171–4186.\nLi, T.; Li, J.; Liu, Z.; and Zhang, C. 2020. Few sample\nknowledge distillation for efficient network compression. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 14639–14647.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. InThe European Con-\nference on Computer Vision (ECCV), volume 8693 ofLNCS,\n740–755. Springer.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 10012–10022.\nLoshchilov, I.; and Hutter, F. 2017. Sgdr: Stochastic gradient\ndescent with warm restarts. In International Conference on\nLearning Representations (ICLR).\nLoshchilov, I.; and Hutter, F. 2018. Decoupled Weight De-\ncay Regularization. In International Conference on Learn-\ning Representations (ICLR).\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.\nPointer Sentinel Mixture Models. In International Confer-\nence on Learning Representations (ICLR).\nNoach, M. B.; and Goldberg, Y . 2020. Compressing pre-\ntrained language models by matrix decomposition. In Pro-\nceedings of the 1st Conference of the Asia-Pacific Chapter of\nthe Association for Computational Linguistics and the 10th\nInternational Joint Conference on Natural Language Pro-\ncessing, 884–889.\nOtt, M.; Auli, M.; Grangier, D.; and Ranzato, M. A. 2018.\nAnalyzing uncertainty in neural machine translation. In\nInternational Conference on Machine Learning (ICML) ,\n3956–3965.\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;\nGrangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In Proceedings of NAACL-\nHLT 2019: Demonstrations.\nRomero, A.; Ballas, N.; Kahou, S. E.; Chassang, A.; Gatta,\nC.; and Bengio, Y . 2014. Fitnets: Hints for thin deep nets.\nIn International Conference on Learning Representations\n(ICLR).\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and Jegou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning (ICML), 10347–10357.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Advances in Neural Information\nProcessing Systems, volume 30, 5998–6008.\nWang, G.-H.; Ge, Y .; and Wu, J. 2021. Distilling knowl-\nedge by mimicking features. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence.\nWang, H.; Liu, J.; Ma, X.; Yong, Y .; Chai, Z.; and Wu, J.\n2022. Compressing Models With Few Samples: Mimicking\nThen Replacing. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n701–710.\nWu, J. 2020. Essentials of Pattern Recognition: An Accessi-\nble Approach. Cambridge University Press.\n11014\nYu, H.; Wang, H.; and Wu, J. 2021. Mixup without hesita-\ntion. In International Conference on Image and Graphics,\n143–154. Springer.\nYun, S.; Han, D.; Oh, S. J.; Chun, S.; Choe, J.; and Yoo,\nY . 2019. CutMix: Regularization Strategy to Train Strong\nClassifiers with Localizable Features. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 6023–6032.\nZhang, H.; Cisse, M.; Dauphin, Y . N.; and Lopez-Paz,\nD. 2018. Mixup: Beyond Empirical Risk Minimization.\nIn International Conference on Learning Representations\n(ICLR).\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom Erasing Data Augmentation. In Proceedings of the\nAAAI Conference on Artificial Intelligence, 13001–13008.\n11015",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8148705959320068
    },
    {
      "name": "Transformer",
      "score": 0.8058809041976929
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5755057334899902
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5556575059890747
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4385412037372589
    },
    {
      "name": "Software deployment",
      "score": 0.4150896370410919
    },
    {
      "name": "Machine learning",
      "score": 0.4036855399608612
    },
    {
      "name": "Mathematics",
      "score": 0.08912578225135803
    },
    {
      "name": "Voltage",
      "score": 0.06438344717025757
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I881766915",
      "name": "Nanjing University",
      "country": "CN"
    }
  ]
}