{
  "title": "Main–Sub Transformer With Spectral–Spatial Separable Convolution for Hyperspectral Image Classification",
  "url": "https://openalex.org/W4389722735",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2276920266",
      "name": "Jingpeng Gao",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2071259497",
      "name": "Xiang-Yu Ji",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2116780756",
      "name": "Geng Chen",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2896741995",
      "name": "Ruitong Guo",
      "affiliations": [
        "Harbin Engineering University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4319990441",
    "https://openalex.org/W2962770389",
    "https://openalex.org/W2039409148",
    "https://openalex.org/W3202024712",
    "https://openalex.org/W2952956606",
    "https://openalex.org/W3126418353",
    "https://openalex.org/W4322503277",
    "https://openalex.org/W4387310135",
    "https://openalex.org/W2087263574",
    "https://openalex.org/W2894165434",
    "https://openalex.org/W4312513332",
    "https://openalex.org/W2136251662",
    "https://openalex.org/W2761818166",
    "https://openalex.org/W2154240401",
    "https://openalex.org/W1964541653",
    "https://openalex.org/W2743255627",
    "https://openalex.org/W2882394250",
    "https://openalex.org/W2583142700",
    "https://openalex.org/W4320339642",
    "https://openalex.org/W2947104191",
    "https://openalex.org/W3046027728",
    "https://openalex.org/W2029316659",
    "https://openalex.org/W2090424610",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2323929895",
    "https://openalex.org/W1521436688",
    "https://openalex.org/W2314785379",
    "https://openalex.org/W2500751094",
    "https://openalex.org/W2914331134",
    "https://openalex.org/W4240485910",
    "https://openalex.org/W4224882649",
    "https://openalex.org/W4281633835",
    "https://openalex.org/W4324144346",
    "https://openalex.org/W4292257555",
    "https://openalex.org/W4220844146",
    "https://openalex.org/W4205615939",
    "https://openalex.org/W2777427437",
    "https://openalex.org/W4303980712",
    "https://openalex.org/W3214821343",
    "https://openalex.org/W4366504084",
    "https://openalex.org/W4321600311",
    "https://openalex.org/W4386634608",
    "https://openalex.org/W4210794570",
    "https://openalex.org/W4319069095",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3023351371",
    "https://openalex.org/W4205102943",
    "https://openalex.org/W3133055443",
    "https://openalex.org/W4387385752",
    "https://openalex.org/W4291727297",
    "https://openalex.org/W4382203304",
    "https://openalex.org/W4377001528",
    "https://openalex.org/W2971432438",
    "https://openalex.org/W4386766977",
    "https://openalex.org/W4285303509",
    "https://openalex.org/W3128776197",
    "https://openalex.org/W4385627363",
    "https://openalex.org/W2811355488",
    "https://openalex.org/W3004877455",
    "https://openalex.org/W4205342161",
    "https://openalex.org/W4291652931",
    "https://openalex.org/W4313627848",
    "https://openalex.org/W4386025622",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3049655825",
    "https://openalex.org/W2904698365",
    "https://openalex.org/W2768211636",
    "https://openalex.org/W3135396862",
    "https://openalex.org/W3122774149",
    "https://openalex.org/W3098388691"
  ],
  "abstract": "Due to their spatial and spectral information, hyperspectral images are frequently used in various scientific and industrial fields. Recent developments in hyperspectral image classification have revolved around the use of convolutional neural networks and transformers, which are capable of modeling local and global data. However, most of the backbone networks of existing methods are based on 3-D convolution, which has high complexity in network structure. Moreover, local information and global information are extracted through different modules, and the coupling relationship between the two types of information is weak. To address the above-mentioned issues, we propose a method named main&#x2013;sub transformer network with spectral&#x2013;spatial separable convolution method (MST-SSSNet), which includes two key modules: the spectral&#x2013;spatial separable convolution (SSSC) module and the main&#x2013;sub transformer encoder (MST) module. The SSSC module uses the proposed spectral&#x2013;spatial separable convolution, reducing network parameters and efficiently extracting local features. The MST module adds the designed subtransformer in front of the conventional transformer encoder (main transformer). It assists the main-transformer encoder to establish global correlation by learning local information. The WHU-Hi dataset can be used as a benchmark dataset for precise crop classification and hyperspectral image classification research. MST-SSSNet is shown to deliver better classification performance than current state-of-the-art methods on the datasets.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024 2747\nMain–Sub Transformer With Spectral–Spatial\nSeparable Convolution for Hyperspectral\nImage Classiﬁcation\nJingpeng Gao , Member, IEEE , Xiangyu Ji , Graduate Student Member, IEEE ,G e n gC h e n , and Ruitong Guo\nAbstract—Due to their spatial and spectral information, hyper-\nspectral images are frequently used in various scientiﬁc and indus-\ntrial ﬁelds. Recent developments in hyperspectral image classiﬁca-\ntion have revolved around the use of convolutional neural networks\nand transformers, which are capable of modeling local and global\ndata. However, most of the backbone networks of existing methods\nare based on 3-D convolution, which has high complexity in network\nstructure. Moreover, local information and global information are\nextracted through different modules, and the coupling relation-\nship between the two types of information is weak. To address\nthe above-mentioned issues, we propose a method named main–\nsub transformer network with spectral–spatial separable convo-\nlution method (MST-SSSNet), which includes two key modules:\nthe spectral–spatial separable convolution (SSSC) module and the\nmain–sub transformer encoder (MST) module. The SSSC module\nuses the proposed spectral–spatial separable convolution, reducing\nnetwork parameters and efﬁciently extracting local features. The\nMST module adds the designed subtransformer in front of the\nconventional transformer encoder (main transformer). It assists\nthe main-transformer encoder to establish global correlation by\nlearning local information. The WHU-Hi dataset can be used as a\nbenchmark dataset for precise crop classiﬁcation and hyperspec-\ntral image classiﬁcation research. MST-SSSNet is shown to de-\nliver better classiﬁcation performance than current state-of-the-art\nmethods on the datasets.\nIndex Terms —Convolution neural networks (CNNs),\nhyperspectral image (HSI) classiﬁcation, main–sub transformer\nencoder (MST), spectral–spatial separable convolution (SSSC).\nI. I NTRODUCTION\nH\nYPERSPECTRAL images (HSI) contain rich information\n[1], which has two spatial dimensions and one spectral\ndimension. Compared with ordinary RGB images, it has richer\nspectral information [2] and can be used in precision agricul-\nture [3], modern medical detection [4], military security [5],\nand other ﬁelds [6], [7], [8]. The process of HSI classiﬁcation\nencompasses the identiﬁcation of each pixel within a scene and\nManuscript received 22 October 2023; revised 24 November 2023, 5 De-\ncember 2023, and 7 December 2023; accepted 11 December 2023. Date of\npublication 14 December 2023; date of current version 10 January 2024. This\nwork was supported by the Basic Research Project Group Project under Grant\nKY10800220035. (Corresponding author: Xiangyu Ji.)\nThe authors are with the College of Information and Communication Engi-\nneering, Harbin Engineering University, Harbin 150001, China (e-mail: gao-\njingpeng@hrbeu.edu.cn; jixiangyu@hrbeu.edu.cn; chengengic@hrbeu.edu.cn;\n983402243@hrbeu.edu.cn).\nThe code will be downloaded to https://github.com/fengqinshou/MST-\nSSSNet.\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3342983\nits categorization into predeﬁned classes [9]. HSI classiﬁcation is\na basic technology in HSI processing and has been a hot research\ntopic in the remote sensing ﬁeld [10], [11], [12].\nTraditional HSI classiﬁcation techniques typically leverage\nthe abundant spectral features present in HSI, including support\nvector machine (SVM) [7], [14], k-nearest neighbor method\n[15], [16], and other methods [17], [18], [19].I n [20], a method\nconsidered spatial information for HSI classiﬁcation. The re-\nsearch by Sun et al. [21] introduced a multiscale spectral–spatial\nkernel approach, employing adjacent superpixels. This method\ntakes into account both the spectral and spatial information\nof the data, further improving the classiﬁcation performance.\nAlthough the above-mentioned methods achieved good results at\nthe time, most of them relied on manually designed features and\nwere shallow models that could not extract high-level features\nof HSI images. This limits the improvement of classiﬁcation\nperformance.\nThe emergence of deep learning techniques has brought con-\nsiderable focus to the utilization of deep models in the realm\nof HSI classiﬁcation [22]. Stacked autoencoder and deep belief\nnetworks were used as traditional depth models in HSI clas-\nsiﬁcation [23], [24]. Nevertheless, their approach involved the\nutilization of numerous fully connected layers, each containing\na substantial volume of trainable weights. The labeling cost of\nHSI is high, and insufﬁcient data to train the model means that\nit often leads to serious overﬁtting problems.\nHowever, convolutional neural networks (CNNs) based meth-\nods have the disadvantage of weak modeling ability of global\nfeature dependencies, which hinders further improvement of\nclassiﬁcation performance.\nIn recent times, CNNs have become a focal point of attention\nin numerous disciplines, primarily due to their outstanding abil-\nity to model local features [25], [26], [27], and have also been\nexplored a lot in HSI classiﬁcation. 1D-CNN [28],2 D - C N N\n[29], and 3D-CNN [30] were all explored in HSI classiﬁcation.\nRoy et al. [31] combined 3-D convolutional layers and 2-D\nconvolutional layers to enhance spatial feature extraction and\nproposed HybridSN. However, CNN-based methods have the\ndisadvantage of weak modeling ability of global feature de-\npendencies, which hinders further improvement of classiﬁcation\nperformance.\nBeyond the scope of CNNs, researchers have delved into nu-\nmerous alternative network architectures for HSI classiﬁcation.\nMou et al. [32] ﬁrst used RNN for HSI classiﬁcation. Graph\n© 2023 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see\nhttps://creativecommons.org/licenses/by-nc-nd/4.0/\n2748 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nconvolutional networks have been widely explored in HSI clas-\nsiﬁcation [33], [34], [35]. Zhu et al. [36] designed a multiscale\nlong and short graph convolution, which makes full use of texture\nstructures of different sizes and captures local and global spectral\ninformation at the same time. In addition, capsule networks [37],\n[38] and generative adversarial networks (GANs) [39], [40] have\nalso been explored in HSI classiﬁcation but they all have their\nown shortcomings, which can be summarized as follows. RNN\nmay experience problems of vanishing gradient and exploding\ngradient. The risk of overﬁtting in GCN is heightened by their\nsubstantial parameter count. The training cycle of a capsule\nnetwork is long and there is a problem of capsule congestion.\nThere is a problem of pattern collapse in GANs.\nIn the most recent developments, the vision transformer (ViT)\n[41] has attained remarkable success within the realm of com-\nputer vision. Hong et al. [42] proposed spectralFormer, which\nlearns spectral features from shallow to deep through spectral\ngrouping embedding and cross-layer adaptive fusion. However,\nit puts too much emphasis on the learning and modeling of\nspectral sequences, ignoring the spatial characteristics. Huang\net al. [43] proposed a supervised contrastive spectral–spatial\nmasked transformer (SC-SS-MTr). However, transformers have\nalways had the disadvantage of weak local modeling capabili-\nties. CNN has been widely studied for its local modeling ability\nto solve this problem [44], [45]. Sun et al. [46] introduced a\ntransformer with feature tokenization using the CNN feature\nextraction module. Roy et al. [47] designed a morphFormer\nnetwork with morphological operations.\nWhile the above-mentioned methods-based transformers and\nCNNs have been extensively explored in the HSI classiﬁcation,\nthere are still several shortcomings. The transformer has high\nrequirements for training data and computing resources [48] and\nthe combination with CNN makes this problem more serious.\nThis not only makes the model easy to overﬁt but also makes\nthe algorithm difﬁcult to deploy on the HSI classiﬁcation de-\nvice, limiting the application of the algorithm. Moreover, local\ninformation and global information are extracted by different\nmodules, respectively, and the coupling relationship between the\ntwo types of information is weak. Speciﬁcally, in previous meth-\nods, the global correlations established through the transformer\nwere mostly directly based on feature maps extracted from local\ninformation by CNN. Not introducing local information to guide\nthe establishment of global correlations may lead to incorrect\ncategory information being introduced into the model. Such an\noutcome could cause imprecise extraction of features, thereby\nconstraining the potential improvement in classiﬁcation perfor-\nmance. In response to the issues outlined above, we present\na solution termed main–sub transformer with spectral–spatial\nseparable convolution (MST-SSSNet) designed speciﬁcally for\nHSI classiﬁcation. It enables a more effective utilization of\nboth local and global spectral–spatial information, effectively\nintegrating these two types of information. Our method includes\ntwo key modules: spectral–spatial separable convolution (SSSC)\nmodule and main–sub transformer encoder (MST) module. We\nuse the SSSC module for feature extraction to capture shallow\nspectral–spatial features and use the MST module to learn local\ninformation to establish the correlation of global information.\nFor the SSSC module, the previous methods were mostly\nbased on 3-D convolution design feature extraction modules,\nwhich have the advantage of extracting spectral–spatial features.\nHowever, this will result in modules having higher algorithm\ncomplexity. Some models also use 1-D and 2-D convolutions to\nextract spectral and spatial features, respectively, which reduce\nthe requirement for computing resources. However, it cannot\ncapture the potential correlation between spectral and spatial\ninformation.\nWe innovatively propose SSSC to build a local feature ex-\ntraction module. The idea of this convolution is to extract\nspatial–spectral features through two orthogonal 2-D convolu-\ntions instead of the 3-D convolution commonly used in pre-\nvious methods. The two spatial dimensions and the spectral\ndimension form two orthogonal planes, respectively. Two 2-D\nconvolutions perform convolution operations through these two\nplanes respectively to replace the 3-D convolution. Therefore,\nspectral–spatial feature information can be effectively extracted\nwith fewer parameters.\nFor the MST module, unlike previous transformer-based\nmethods, the main–sub transformer proposed is not a simple\ntwo-layer or double-branch structure, it is designed as a two-step\nglobal feature construction. The ﬁrst step is to learn local features\nthrough the subtransformer, and the second step is to introduce\nthe learned local features into the main transformer to jointly\nestablish global information with the original data. The pivotal\ncontributions of this study can be outlined as follows.\n1) We propose a method named MST-SSSNet for HSI clas-\nsiﬁcation, which uses the designed SSSC to effectively\nextract spectral–spatial features and then uses local infor-\nmation to model global information.\n2) We design a lightweight SSSC module that replaces the\n3-D convolution with our SSSC. We innovatively use\nan orthogonal structured 2-D convolution instead of 3-D\nconvolution to achieve efﬁcient feature extraction.\n3) We design an MST module including a subtransformer\nand a main transformer. Unlike cascaded or dual-branch\nstructures, the designed MST module assists the main\ntransformer in establishing global information by dividing\nthe image into different scales and learning local informa-\ntion through the subtransformer.\nII. R ELATED WORK\nIn recent years, deep learning has developed rapidly. Com-\npared with traditional hand-designed feature extractors, deep\nlearning methods usually have stronger feature modeling capa-\nbilities and robustness [22]. Therefore, HSI classiﬁcation meth-\nods based on deep learning have been widely explored. In this\nsection, we will introduce the HSI classiﬁcation method based\non CNN, the HSI classiﬁcation method based on transformer,\nand other deep learning methods.\nA. Method Based on CNN\nCNNs have been explored a lot in HSI classiﬁcation [31],\n[49], [50], [51].H ue ta l . [28] involved the effective extraction\nof spectral features through the use of 1D-CNN. Zhao and Du\nGAO et al.: MAIN–SUB TRANSFORMER WITH SSSC FOR HSI CLASSIFICATION 2749\n[29] used the principal component analysis (PCA) to reduce the\ndimensionality of HSI data and then extracted spatial features\nthrough 2D-CNN. In [30], spectral and spatial features were\nextracted concurrently using a 3D-CNN. Furthermore, regular-\nization techniques were applied to enhance the system’s gen-\neralization prowess. In [49], a method combining 3-D and 2-D\nis proposed, which aims to extract spectral spatial features and\nuse them for classiﬁcation. Roy et al. [31] combined 3-D con-\nvolutional layers of different scales to extract features and used\n2-D convolutional layers to enhance spatial feature extraction. It\nachieved good results. In [50], a global framework classiﬁcation\npattern has been designed to alleviate the problem of difﬁcult ex-\ntraction of global information caused by traditional patch-based\ndataset partitioning. Yu et al. [51] introduced the feedback atten-\ntion mechanism proposed in this article into CNN networks and\ndesigned a dense spatial–spectral CNN structure for HSI classi-\nﬁcation, called FADCNN, which improved classiﬁcation perfor-\nmance. In addition to the above-mentioned work, some methods\nconsider introducing CNN into other backbone networks to\nenhance classiﬁcation performance [33], [34], [35], [59], [46].\nThese work with better performance than separate networks.\nB. Method Based on Transformer\nTransformers have received widespread attention for their\nexcellent long-distance modeling capabilities. It also demon-\nstrates excellent potential in HSI classiﬁcation [52], [53], [54],\n[55].H ee ta l . [56] used transformers and proposed BERT to\nHSI classiﬁcation. Hong et al. [42] used transformer encoders\nand designed a cross-layer fusion structure to create spectral-\nFormer. Cao et al. [57] proposed a transformer-based MAE using\ncontrastive learning, attempting to combine these two methods\nto further improve performance. Huang et al. [43] designed\na spectral–spatial masked transformer (SS-MTr) and through\ncontrastive and supervised learning proposed an SC-SS-MTr.\nSome other methods have been used to improve transformers.\nYu et al. [58] designed a lightweight classiﬁcation network using\nan image-level framework and CNN and transformer. In [44],\na multiscale network was designed for more accurate feature\nextraction, and the transformer was improved for HSI image\nclassiﬁcation. Huang et al. [44] introduced active learning into\nthe transformer and designed a learning strategy combining\nsuperpixel segmentation while improving the transformer using\nOutlook attention. However, a transformer has the disadvantage\nof poor local modeling ability. Some works have solved this\nproblem by combining CNN with a transformer, which has been\nproven to be effective. He et al. [59] used a VGG-like network\nto extract the spatial features of HSI data and then modeled\nthe spectral information through a transformer. Sun et al. [46]\nused a CNN network consisting of 3-D and 2-D convolutional\nlayers for feature extraction and learned advanced semantic\ninformation through a transformer. Roy et al. [47] combined the\nattention mechanism with morphological operations to improve\nthe interaction and proposed morphFormer. Yang et al. [60]\ndesigned a multilevel feature fusion network for class prediction\nusing interactive CNN and transformer. This network can extract\ncategory features of different perception ﬁelds and depths for\nbetter prediction.\nC. Method Based on Other Networks\n1) Method Based on RNN: Zhang et al. [61] scanned HSI into\na sequence of pixels and each pixel and its spectral information\nis a step of the model. In [62], the RNN model was simpliﬁed and\ndesigned into an efﬁcient model that can be extended. Zhou et al.\n[63] studied the effectiveness of multiple scanning strategies to\ngenerate features. This strategy is proven to have signiﬁcant\nimprovements in RNNs.\n2) Method Based on GCN: In [64], locality preserving low-\npass graph convolutional embedding autoencoder is proposed,\nand self-training clustering mechanism and joint optimization\nloss are introduced to achieve mutual beneﬁt. Ding et al. [33]\ncombined multiple ﬁlters through deﬁned degree scales to better\nprocess HSI information. Zhang et al. [65] proposed an adap-\ntive receptive ﬁeld graph neural framework that can alleviate\nexcessive smoothness in the model and reduce computational\ncomplexity.\n3) Method Based on GAN: Zhang et al. [39] designed a\nsemisupervised HSI data framework based on one-dimensional\nGAN for HSI classiﬁcation. Sun et al. [40] proposed an auxiliary\nclassiﬁer based on the gradient penalty Wasserstein GAN (AC-\nWGAN-GP). Feng et al. [66] introduced contrastive learning\ninto GAN and designed a pair of coarse-grained GAN networks.\nIII. P ROPOSED METHOD\nA. Overall Framework of the Proposed Method\nFig. 1 illustrates the overall framework of our MST-SSSNet\nmethod. In this section, we introduce the MST-SSSNet method\nfrom three steps: preprocessing, spectral–spatial separable con-\nvolutional module, and main–sub transformer module.\nIn the preprocessing part, the original HSI data are processed\nby PCA to reduce the redundancy existing in the data. This step\nis crucial to improving the overall algorithm speed. The data\nafter dimension reduction need to be block extracted to generate\na dataset for model learning, and the dataset is divided into a\ntraining set and a test set.\nThe data of the training set are used as the input of the SSSC\nmodule, which is a two-branch structure designed by the SSSC,\nand ﬁnally uses a 2-D convolution to enhance the extraction of\nspatial features, and ﬁnally outputs the feature map.\nThe feature map ﬁrst generates a patch token through trans-\nformation. Then the patch token generates subtokens and main\ntokens, respectively. The main tokens are the input of the main\ntransformer and the subtokens are the input of the subtrans-\nformer. The subtoken learns local detail features through the\nsubtransformer and then fuses them with the main token to help\nthe main token establish global features in the main transformer.\nFinally, the classiﬁcation structure is obtained through a linear\nlayer and softmax function.\nB. Preprocessing\nHSIs typically comprise a substantial number of spectral\nbands, and there is high information redundancy between adja-\ncent bands. These issues increase the complexity of computation\nand storage and may lead to overﬁtting issues. As an effective\nmethod for dimensionality reduction, PCA streamlines data\n2750 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nFig. 1. Overall framework of the proposed MST-SSSNet.\ncomplexity and minimizes redundancy among spectral bands. It\nchanges the feature spatial of pixels to better distinguish spectral\nsignals of different substances. Thus, PCA is applied in the\nhandling of HSI data. The HSI data, having undergone PCA, are\nrecorded as IPCA ∈ RH×W×D. H and W denote the spatial size,\nand C represents the center pixel to extract a patch P ∈ RS×S×D\nfrom IPCA. S is the size of the patch. The true label is ascertained\nbased on the label assigned to the central pixel. Edge pixels\nneed a padding operation before being extracted, with a ﬁlling\nwidth of (S −1)/2. After extracting all patches, we separate the\nremaining samples into a training set and a test set.\nC. Spectral–Spatial Separable Convolution Module\nTo address the above issues, inspired by spatial separable\nconvolution[67], we designed SSSC to replace 3-D convolution.\nRicher spectral–spatial features are extracted with equivalent\nparameter quantities.\nFig. 2 illustrates the comparison of details between 3-D\nconvolution and SSSC. X and Y denote spatial dimensions. L\nrepresents the spectral dimension. The input cube data are used\n2-D convolution on the Y–L plane for convolution. It is called\nthe Y–L convolution. Then convolution is performed on the X–L\nplane. It is called the X–L convolution. The above is used to\nreplace the 3-D convolution. The convolution of data with a size\nof 7 ×7 ×7 is taken as an example. The 7 ×7 ×7 sized blue\nblock represents the original data, and the 3 ×3 sized blocks\nof different colors represent Y–L convolution kernels and X–L\nconvolution kernels. We calculate the parameter quantities for\n3-D convolution and SSSC. Selecting a convolution kernel with a\nsize of 3 ×3 ×3 for 3-D convolution requires 175×27 = 4725\nmultiplication operations. SSSC selects kernel sizes 3 ×1 ×3\nand 3 ×3 ×1. It takes 175×9+75 ×9 = 2250times. The\nprocedure signiﬁcantly diminishes the parameter count, leading\nto enhanced computational efﬁciency. Compared to networks\ndesigned based on 3-D convolution, we by using the SSSC\nFig. 2. Comparison of details between 3-D convolution and SSSC.\ncan design deeper networks, thereby enhancing the ability to\ncapture features. It should be noted that the number of times we\nuse a pair of SSSCs to extract features in the frequency band\ndimension is two. Within the context of parameter reduction,\nwe have simultaneously reﬁned our capacity for spectral feature\nextraction. Consequently, employing SSSC enables the extrac-\ntion of profoundly efﬁcient feature representations, enriching\nthe process of network learning To elaborate further, in SSSC,\nthe calculated values of the jth feature cube of X–L convolution\nand Y–L convolution in the ith layer at the spatial position can\nbe given as\nvxyz\nij =Φ\n(\nbij +\n∑\nm\nXi−1∑\np=0\nLi−1∑\nr=0\nwpr\nijmv(x+p)y(z+r)\n(i−1)m\n)\n(1)\nvxyz\nij =Φ\n(\nbij +\n∑\nm\nYi−1∑\nq=0\nLi−1∑\nr=0\nwqr\nijmvx(y+q)(z+r)\n(i−1)m\n)\n(2)\nGAO et al.: MAIN–SUB TRANSFORMER WITH SSSC FOR HSI CLASSIFICATION 2751\nFig. 3. Illustration of the SSSC module.\nwhere Φ(·)denotes the activation function and bij represents the\nbias. m is the feature map in the (i −1)th layer that is connected\nto the jth feature map. Yi,X i,L i, respectively, represent the\nheight, width, and channel number of the SSSC kernel. Li stands\nfor spectral dimension. In the mth feature cube, the parameters\nwpr\nijm and wqr\nijm correspond to the weights linked with the\nposition (p,q,r).\nThe SSSC module is graphically presented in Fig. 3.T h e\nactivation function will result in spatial asymmetry in SSSC.\nThis asymmetry will affect the network’s preference for learning\nfeatures. Therefore, we design a dual-branch network to elim-\ninate this preference. The SSSC module processes data in two\nstages. In the ﬁrst stage, two pairs of SSSCs in different orders\nperform feature extraction on the input patches. The ﬁrst-stage\nfeature mapping formula can be described as follows:\nXP1 = RELU(BN(ConvYL(RELU(BN(ConvXL(X))))\n(3)\nXP2 =R E L U ( B N ( C o n vXL(RELU(BN(ConvYL (X))))\n(4)\nwhere X denotes the input. XP1 and XP2 represent the feature\nmaps.RELU(·) is the activation function, BN(·) denotes the\nBatchNorm operation.ConvXL(·)and ConvYL (·)represent the\ndifferent direction SSSCs. In the second stage, by reshaping and\nfusing the feature maps from the two branches, a new feature\nmap is derived. Then the feature map undergoes standard 2-D\nconvolution operations, enhancing the network’s spatial feature\nextraction ability. The formula for the second stage is given as\nfollows:\nXP3 = RELU(BN(Conv2D(reshape(XP1)\n⊕reshape(XP2)))) (5)\nwhere the SSSC module output is deﬁned as XP3 ∈ Rm×n×d.m\nis the height and nrepresents the weight. The number of channels\nis represented as d. ⊕ represents the concatenate function. The\nextracted features will provide a good feature representation for\nsubsequent module processing.\nD. Token Generator and Main–Sub Transformer Module\n1) Token Generator: SSSC module has extracted good local\nspectral–spatial features XP3 ∈ Ra×a×d, the spatial size is rep-\nresented as a, and d signiﬁes the number of channels in the given\ncontext. The feature token is deﬁned as XT ∈ Rw×d, where w\nrepresents the token number. It can be obtained by the following\nFig. 4. Details for a transformer encoder and MSA. (a) Transformer encoder.\n(b) MSA.\nformula:\nXﬂat = Flatten(XP3) (6)\nXT = softmax(WaXﬂat)TXﬂat (7)\nwhere Wa represents a learnable weight matrix. The feature map\nis projected into tokens through the above-mentioned steps [45]\nfor subsequent operations.\n2) Main–Sub Transformer Module: In Fig. 4(a), both the\nmain-transformer encoder and the subtransformer encoder share\nan identical architecture. This conﬁguration includes two nor-\nmalization layers (LN): a multihead self-attention (MSA) block\nand a multilayer perceptron (MLP) layer. Residual skip connec-\ntions are applied prior to the MSA block and the MLP layer.\nThe speciﬁcs of the MSA are illustrated in Fig. 4(b). The entire\nprocess can be mathematically expressed as follows:\nSA = Attention(Q, K, V) =\n(QKT\n√dK\n)\nV (8)\nMSA(Q, K, V) = Concat(SA1, SA2,..., SAh)W (9)\nwhere the matrices Q, K, and V are learnable weight matri-\nces. SA represents self-attention. The incorporation of SA in\nthe MSA block effectively captures correlations among feature\nsequences. Within the MSA block, multiple groups of weight\nmatrices are employed to map Q, K, and V , employing a uniform\noperation process to calculate multihead attention values. Sub-\nsequently, the results from each head attention are concatenated.\n“h” denotes the number of heads, and W signiﬁes the parameter\nmatrix. The MLP comprises two fully connected layers sepa-\nrated by a nonlinear activation function known as Gaussian error\nlinear units. In order to establish the correlation between local\nand global information, we designed a main–sub transformer.\nWhen people understand a sentence, they ﬁrst understand the\nmeaning of the words or phrases; second, they establish the\ncontext of the entire sentence through their meanings, thereby\nunderstanding the entire sentence. This idea also applies to\nthe model’s understanding of HSI. When establishing global\ncorrelation, irrelevant information is often introduced, which\naffects the model’s feature learning. Entering local information\nto assist in the establishment of global correlation will greatly\nimprove this problem. Speciﬁcally, we divide the input patch\ntokens into more ﬁne-scale subtokens. Its detailed features are\nthen learned through a subtransformer. Unlike general dual-\nscale or multiscale methods, we do not cascade two or more\ntransformers at different scales. Instead, the detailed features\nlearned by the subtransformer are integrated into the main token.\n2752 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nFig. 5. Illustration of the main–sub transformer module.\nFig. 6. LongKou dataset. (a) False-color composite image. (b) Ground truth\nmap.\nFinally, the main transformer is used to learn global correlations\nwhile retaining the detailed features. The details of the main–sub\ntransformer module are shown in Fig. 5.\nEach token as input represents XT =[ T1, T2,..., Tw].W e\ndivide the token into n subtokens by reshaping the data form.\nAfter adding trainable position coding, it is expressed as X′T =\n[T11, T12,..., T1n, T21,..., Twn]+PE sub. A subtransformer\nencoder is used to learn local details. Then the subtoken is\nconverted into the same data form Xt =[ T′1, T′1,..., T′w] as\nthe token. After processing a block consisting of two normal-\nization layers and a linear layer, it is added to the patch token.\nFinally, it is concatenated with a learnable class token Tcls\n0 to\nget the main token. The comprehensive process outlined above\nis encapsulated by the following equations:\nXt = Norm(Linear(Norm(X′\nT))) (10)\nXm = Xt +XT (11)\nXmain = Concat(Tcls\n0 ,X m). (12)\nThe main token is the input of the main-transformer encoder.\nFinally, we only use Tcls\n0 as the input to the linear layer for\nthe classiﬁcation. Via the linear layer, the probability of the\ninput belonging to a speciﬁc class will get through the softmax\nfunction. The label corresponding to the highest probability\nsigniﬁes the class of the given sample.\nIV . E XPERIMENT AND ANALYSIS\nThis section introduces the WHU-Hi dataset used in the exper-\niments, including LongKou, HanChuan, and HongHu [68], [69].\nFurthermore, we present the experimental settings, including\nevaluation indicators, conﬁguration, and parameter analysis.\nThen, the ablation experiment is introduced. Finally, we show\nand analyze the results.\nFig. 7. HanChuan dataset. (a) False-color composite image. (b) Ground truth\nmap.\nFig. 8. HongHu dataset. (a) False-color composite image. (b) Ground truth\nmap.\nA. Data Description\nTo evaluate the effectiveness of our MST-SSSNet, rigorous\nexperiments were executed utilizing the WHU-Hi dataset, a\nUA V-borne HSI resource provided by the RSIDEA research\ngroup at Wuhan University. This dataset is publicly available and\ncan be downloaded from the RSIDEA team’s homepage. This\ndataset encompasses three hyperspectral datasets: WHU-Hi-\nLongKou, WHU-Hi-HanChuan, and WHU-Hi-HongHu. The\nFalse-color composite image and ground truth maps of the three\ndatasets are shown in Figs. 6–8.\nThe LongKou dataset was procured through an 8-mm focal\nlength headwall nanohyperspectral imaging sensor mounted on\na DJI Matrice 600 Pro, conducting aerial surveys over Longkou\nTown in China in the year 2018. Within this study region, there\nexist nine speciﬁc classes, comprising six diverse crop species.\nThe dataset consists of 550 × 400 pixels and ranges from 400\nto 1000 nm, including 270 bands.\nThe HanChuan dataset was acquired utilizing a 17-mm focal\nlength headwall nanohyperspectral imaging sensor installed on\na Leica Aibot X6, conducting surveys over HanChuan, China,\nin the year 2016. There are 16 classes including 7 crop species:\nwatermelon, water spinach, greens, strawberry, cowpea, greens,\nand sorghum. The dataset consists of 1217 ×303 pixels ranging\nfrom 400 to 1000 nm and includes 274 bands. Notably, because\nthe time to collect the HanChuan dataset was in the afternoon,\nthere are many areas that are shadow-covered in the image.\nThe HongHu dataset was captured using a 17-mm focal length\nheadwall nano-hyperspectral imaging sensor mounted on a DJI\nMatrice 600 Pro, conducting surveys over HongHu City in China\nin the year 2017. The experimental area comprises a diverse\nagricultural landscape featuring 22 distinct classes, with varying\ncultivars of the same crop cultivated within the region. The size\nGAO et al.: MAIN–SUB TRANSFORMER WITH SSSC FOR HSI CLASSIFICATION 2753\nFig. 9. Impact between different SSSC kernel sizes and patch sizes for the OA. (a) LongKou. (b) HanChuan. (c) HongHu.\nof the image is 940 × 475 pixels and ranges from 400 to 1000\nnm, comprising 270 bands.\nB. Experimental Setting\n1) Evaluation Indicators: In our experimental framework,\nwe adopted overall accuracy (OA), average accuracy (AA), and\nKappa coefﬁcient (K) as assessment metrics [70]. OA represents\nthe comprehensive accuracy across all samples, while AA sig-\nniﬁes the mean accuracy computed for each individual class.\nKappa coefﬁcient gauges the alignment between the classiﬁca-\ntion outcomes and the true underlying classes. The mathematical\nexpressions for these metrics are detailed as follows:\nOA =\n(\n1\nn\n∑\nk\n(True positive+True Negative\nTotal number of pixels\n))\n(13)\nRecall = (True positive+False negative) (14)\nAA =\n∑ n\ni=1 Recalli\nn ×100% (15)\nK = N ∑ n\ni=1 xii−∑ n\ni=1 (xi+ ×x+i)\nN2 −∑ n\ni=1 (xi+ ×x+i) . (16)\n2) Conﬁguration: The veriﬁcation experiments for the pro-\nposed methodology were conducted within the PyTorch com-\nputational framework, using an 11th Gen Intel(R) Core(TM)\ni9-11900K processor clocked at 3.50 GHz, and an NVIDIA\nGeForce RTX 3090Ti 24-GB GPU server. The optimization\nprocess is initiated with the Adam optimizer, and the learning\nrate is set to 0.001. During batch training, each batch was\nconﬁgured to contain 64. The original spectral band number\nfor the LongKou and HongHu datasets is 270, while the original\nband number for the HanChuan dataset is 274. The number of\nbands after PCA operation is set to 30. The size of the input image\nis the same as the size of the original dataset. The LongKou\nconsists of 550 × 400 pixels. The HanChuan consists of 1217\n× 303 pixels. The HongHu consists of 1217 × 303 pixels. A\ntotal of 100 training epochs were applied to each dataset. Ten\nexperiments were rigorously carried out for every method, and\nthe most optimal result among them was meticulously selected\nas the deﬁnitive outcome.\n3) Patch Size and SSSC Kernel Size Analysis: We analyzed\nthe impact of input patch size and SSSC kernel size on classi-\nﬁcation accuracy. In Fig. 9 the impact of patch size and kernel\nsize on classiﬁcation accuracy is shown.\nIn general, the accuracy of classiﬁcation tends to diminish\nas the kernel size increases. In particular, we choose [2], [7]\nall kernel sizes instead of only odd ones. The result shows that\nodd kernel sizes have no obvious advantages over even kernel\nsizes. The reason for this may be that the feature has captured\nthe central feature because of the two orthogonal convolutions in\nthe spatial dimension. For the channel dimension, the centermost\nchannel has the same importance as the other channels, so there\nis no obvious difference between odd and even convolution\nkernels. The best kernel size of the three datasets is 2.\nFor patch size, the accuracy of classiﬁcation exhibits an\ninitial rise, followed by a subsequent decline as the patch size\nincreases. The patch sizes with the size of [9], [23] are tested in\nexperiments. In the local range, it accords with the characteristics\nof convex function. For three datasets, when small patch size\nand large kernel size are combined, it usually leads to a serious\ndecrease in accuracy. That is because too small a patch and too\nlarge a kernel can result in too few convolutions on a patch and\ncannot accurately extract features.\nFor the LongKou dataset, classiﬁcation accuracy is not sensi-\ntive to changes in two parameters. The optimal patch size is 15.\nIn HanChuan and HongHu, the inﬂuence of the two parameters\non classiﬁcation accuracy exhibits similarity. The optimal patch\nsize is 21. The experimental results indicate that selecting the\nappropriate patch size and kernel size can better extract fea-\ntures. Following an evaluation of the performance across the\nthree datasets, a patch size of 21 was chosen as the optimal\nselection.\nC. Ablation Studies\nTo fully validate the effectiveness of our method, ablation\nexperiments with different component combinations were con-\nducted on the HongHu dataset. Five combinations were consid-\nered. We assess the effect of distinct components on the over-\narching model through their impact on classiﬁcation accuracy.\nThe results of all experiments are meticulously documented in\n2754 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nTABLE I\nABLATION EXPERIMENT OF THE PROPOSED METHOD ON THE HONGHU DATASET\nTable I, with the most superior classiﬁcation outcome emphati-\ncally presented in bold typeface.\nThe effectiveness experiment of the MST module is per-\nformed by replacing it with a linear layer and ordinary trans-\nformer encoder (TE). About the linear layer, the feature map\noutput by the SSSC module is reshaped and connected to the\nlinear layer. In the linear layer, in_features is set to 1600 (the\nsame size as the reshapped feature map), and out_features is set\nto 22 (the same number of categories as the dataset). Among\nthem, the combination of the SSSC module and linear layer has\nthe worst effect because the features lack the establishment of\nlong-range correlation. The MST module is superior to ordinary\ntransformer encoders, especially in terms of signiﬁcantly im-\nproving AA. This indicates that the MST module can reduce the\nintroduction of irrelevant information by establishing long-range\ncorrelation through local information. Thereby the classiﬁcation\naccuracy is improved.\nTo validate the efﬁcacy of the SSSC module, two distinct sets\nof experiments were crafted. One set of experiments replaced the\nSSSC module with 3-D convolutional layers. The depth of the\n3-D convolutional layer is 1, the number of convolutional kernels\nis 8, and the size is 3 ×3 ×3. The other set of experiments\nonly used the MST module after PCA operation. The MST\nmodule operating in isolation demonstrates inferior classiﬁca-\ntion accuracy attributed to its absence of local feature extraction\ncapabilities. The combination of the SSSC module and the MST\nmodule is better than the combination of the 3-D convolution\nand MST module in classiﬁcation performance and calculation\ntime, which shows that the proposed spectral–spatial separable\nvolume of the product can extract richer features with fewer\nparameters. In summary, the analysis of comprehensive experi-\nmental results further conﬁrms the effectiveness of our model.\nD. Analysis of Results\nIn this section, three datasets will be selected for the ex-\nperiment. The training samples within each dataset have been\nallocated at an average of 100 samples per class.\nFor the LongKou dataset, we randomly selected 900 samples\nas the training set. In the case of the HanChuan dataset, 1600\nsamples were meticulously chosen to form the training set. For\nthe HongHu dataset, 2200 sample training sets were selected.\nThe samples not chosen for training are speciﬁcally allocated as\ntest sets to evaluate the model’s efﬁcacy.\n1) Comparison Methods: To validate the proposed method-\nology, a selection of representative baseline methods as well\nas the most advanced backbone methods have been chosen.\nWe divide them into nontransformer methods (such as SVM\n[13],1 D C o n v[28],2 D C o n v[29],3 D C o n v[30], and HybridSN\n[31]) and transformer-based methods (such as ViT [38], spec-\ntralFormer [40], SSFTT [45], and SC-SS-MTr [41]). The intro-\nduction to these methods is as follows.\nSVM is one of the ﬁrst machine-learning methods used in HSI\nclassiﬁcation. It represents the classic machine learning method.\nThe 1-D-CNN is a method including a 1-D convolutional layer,\nmaximum pooling layer, and fully connected layer. The 2-D-\nCNN is a method that includes three convolutional layers and\ntwo fully connected layers. HybridSN consists of a convolutional\npart, a fully 2-D convolutional layer, and two fully connected lay-\ners. The 3-D-CNN is a 3-D convolution-based method, includ-\ning three 3-D connected parts. The convolutional part includes\nthree 3-D convolutional layers of different scales and one 2-D\nconvolutional layer. The linear layer part consists of two linear\nlayers. MST-SSSNet surpasses current state-of-the-art methods\nacross three datasets based on three comprehensive evaluation\nindicators. The best performer among CNN-based approaches\nis HyBridSN. Thanks to the design of the convolutional kernel\nsize in the network, HybridSN pays attention to more abundant\nspectral features and achieves better results compared with other\nCNN-class methods. This shows that extracting rich spectral\nfeatures can better distinguish different classes. In addition to\nthe proposed methods, SSFTT and SC-SS-MTr perform better\nthan other transformer-based methods. SSFTT beneﬁts from\nthe excellent basic architecture combined with CNN and trans-\nformer and extracts local and global features at the same time to\nachieve better classiﬁcation performance. SC-SS-MTr enhances\nthe performance of the transformer and achieves competitive\nclassiﬁcation results through supervised learning, discriminant\nfeature learning, and mask. The proposed MST-SSSNet uses the\ndesigned SSSC to extract spectral–spatial features with fewer\nparameters so that the network can be designed deeper and the\nlocal information can be extracted more fully. Therefore, it has\nthe best classiﬁcation results. For the LongKou dataset, the OA,\nAA, and Kappa coefﬁcients of our method reached 98.97%,\n96.16%, and 98.64%, respectively. On this dataset, HybridSN\nand most transformer-based methods have achieved competitive\nclassiﬁcation results. We posit that this can be attributed to the\nlimited number of classes in the LongKou and the concentrated\ndistribution of these categories, leading to a reduced level of clas-\nsiﬁcation complexity. Datasets have low requirements for model\nclassiﬁcation ability. However, our method still achieves the best\nclassiﬁcation performance. For HanChuan, the proposed method\nreached 97.15%, 93.02%, and 96.67% in three evaluation indi-\ncators, and the best classiﬁcation accuracy was achieved in 12\nout of 16 classes. Compared with other methods, OA, AA, and\nKappa coefﬁcients of the best-performing methods increased\nby at least 0.89%, 4.51%, and 2.22%. For the HongHu dataset,\nthe OA, AA, and kappa coefﬁcients of the proposed method\nGAO et al.: MAIN–SUB TRANSFORMER WITH SSSC FOR HSI CLASSIFICATION 2755\nTABLE II\nCOMPARATIVEEXPERIMENTAL RESULTS ON THE LONGKOU DATASET\nreach 97.84%, 97.27%, and 94.51% in OA, AA, and Kappa\ncoefﬁcients, respectively, and the best classiﬁcation accuracy\nis achieved in 12 out of 22 classes. Compared with the best\nperformance of other methods, OA, AA, and Kappa coefﬁcients\ncan be improved by at least 1.08%, 2.11%, and 1.27%.\nIn HanChuan and HongHu, the accuracy of only two and one\nclasses of our method is lower than 90% respectively, while the\naccuracy of the other classes is relatively stable. HanChuan’s\nclass13 is a difﬁcult class to classify. Although the classiﬁcation\naccuracy of our method is lower than 90% (88.32%), it is still the\nhighest accuracy in this class. Other methods have more than ﬁve\nclasses with a classiﬁcation accuracy of less than 90%. This is\nbecause when establishing global correlation, combining local\nfeatures can reduce the inﬂuence of irrelevant information so\nthat the proposed method is less affected by class features and\nhas good robustness.\nBased on the performance of the three datasets, the proposed\nmethods have achieved the best classiﬁcation performance. This\nproves that the global correlation with strong coupling with\nlocal information is closer to the core features of the class. And\nthe SSSC module can extract excellent local detail features to\nprovide a good feature representation for subsequent modules.\nThe proposed method can effectively improve the classiﬁcation\nperformance of the model by local information to establish\nglobal correlation.\nVit used a transformer for image processing for the ﬁrst\ntime and achieved excellent results. It proves that the excellent\nperformance of the transformer is not limited to the ﬁeld of NLP.\nSpectralFormer is speciﬁcally designed to emphasize the extrac-\ntion of spectral information. Remarkably, it achieves exceptional\nclassiﬁcation results without the utilization of convolutional or\nrecurrent units. The improvement of SSFTT on the tokenizer\nfully explores the ability of the transformer to handle HSIs.\nIts feature extraction module consists of a 3-D convolutional\nlayer and a 2-D convolutional layer. After being tokenized, it is\nconnected to the transformer encoder and achieves excellent re-\nsults in image classiﬁcation. SC-SS-MTr used a spectral–spatial\nmasked transformer, which gains competitive classiﬁcation re-\nsults. It uses supervised learning and contrastive learning for\nbetter generalization. The parameters and experimental details\nof all the above methods are set in accordance with the reference\npapers.\n2) Quantitative Results and Analysis: Tables II–IV list\nthe comparison of various classiﬁcation indicators of var-\nious methods under three datasets. The best outcomes are\nhighlighted in bold black for clear visibility. The out-\ncomes unequivocally demonstrate the superior performance of\nour MST-SSSNet, surpassing current state-of-the-art methods\nacross three datasets based on three comprehensive evaluation\nindicators.\n3) Visual Evaluation: To visually discern the performance\ndisparity between the proposed method and other models, the\nclassiﬁcation results are plotted as visual classiﬁcation maps.\nThe visual classiﬁcation maps of the three datasets of LongKou,\nHanChuan, and HongHu are shown in Figs. 10–12.\nFor the LongKou dataset, this is relatively easy to distinguish\ndataset, except for 1D-CNN and 2D-CNN, which failed to\nmake use of the spatial or spectral features of HSI, resulting in\nunsatisfactory classiﬁcation results; other models have achieved\ngood results. However, for some of the difﬁcult areas, such as\nthe left side of the map where the blue extreme meets the top\nand bottom of the other classes, most methods make a mistake in\ndemarcating the boundaries. Even if the classiﬁcation accuracy\nis similar, such errors are often more serious. In addition, we\ncircle an area with a red border to enlarge it for display. In this\narea, the classiﬁcation, especially the boundary recognition, is\ndifﬁcult due to the variety of classes. In addition, other methods\nmake it easy to divide some gray areas into blue classes, because\nthe gray samples whose map coordinates are connected with\nblue affect the learning of the gray class. The proposed method\nsuccessfully classiﬁes the gray part correctly, and the recognition\nof the class boundary is more accurate.\nFor the HanChuan dataset, the classes of this dataset are\ninterleaved, making classiﬁcation difﬁcult. The classiﬁcation\nresults of other methods obviously contain more pronounced\nnoise, such as the region near the purple class in the upper\nright corner. Compared with other methods, the proposed MST-\nSSSNet gives smoother classiﬁcation results. We have circled\n2756 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nTABLE III\nCOMPARATIVEEXPERIMENTAL RESULTS ON HANCHUAN DATASET\nTABLE IV\nCOMPARATIVEEXPERIMENTAL RESULTS ON THE HONGHU DATASET\nGAO et al.: MAIN–SUB TRANSFORMER WITH SSSC FOR HSI CLASSIFICATION 2757\nFig. 10. Visualization of the experimental results based on the LongKou dataset. (a) Ground truth. (b) 1D-CNN. (c) 2D-CNN. (d) 3D-CNN. (e) HybridSN.\n(f) ViT. (g) SpectralFormer. (h) SSFTT. (i) SC-SS-MTr. (j) MST-SSSNet(ours).\nFig. 11. Visualization of the experimental results based on the HanChuan dataset. (a) Ground truth. (b) 1D-CNN. (c) 2D-CNN. (d) 3D-CNN. (e) HybridSN.\n(f) ViT. (g) SpectralFormer. (h) SSFTT. (i) SC-SS-MTr. (j) MST-SSSNet(ours).\n2758 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nFig. 12. Visualization of the experimental results based on the HongHu dataset. (a) Ground truth. (b) 1D-CNN. (c) 2D-CNN. (d) 3D-CNN. (e) HybridSN.\n(f) ViT. (g) SpectralFormer. (h) SSFTT. (i) SC-SS-MTr; (j) MST-SSSNet(ours).\nan area where our method is more accurate in distinguishing\nbetween the boundaries of the semicircle and the interior. In\naddition, almost all methods misidentify the gray corner above\nthe semicircle. It proves that our method is more reliable in\ndistinguishing details.\nFor the HongHu dataset, this dataset has the characteristics\nof the previous two datasets. There are large areas of the same\nclass, and there are areas where multiple classes gather. At the\nsame time, this dataset has the largest number of samples. Since\nthe partitioning of the training set is random, such a partitioning\nmethod is more challenging for the model. In order to pursue\nthe improvement of classiﬁcation accuracy, the model is easier\nto overﬁt a large number of classes. Other methods are more\naccurate for large areas of light green in the middle, but the area\nbelow obviously achieves poor results. The classiﬁcation results\nare also circled in an area where it is clear that the proposed\nmethod gets results closer to the ground truth map and has better\nantioverﬁtting ability for most classes.\nThe analysis of visual classiﬁcation results from three datasets\ndemonstrates that the proposed method exhibits relatively stable\nclassiﬁcation performance in areas with simple and complex\ncategories. However, for boundary areas that are difﬁcult to\ndistinguish but often very important, the proposed method has\nachieved signiﬁcant advantages over other methods. This stems\nfrom the SSSC module overcoming the drawback of most feature\nextraction modules focusing too much on spatial or spectral\nfeatures, as well as the design of the MST module to establish\nglobal correlations based on local features. Therefore, the best\nclassiﬁcation results were achieved.\n4) Inﬂuence of Sample Number: In addition, we selected\n20%, 40%, 60%, 80%, and 100% of the original training samples\nfrom three datasets to compare 2D-CNN, 3D-CNN, HybridSN,\nGAO et al.: MAIN–SUB TRANSFORMER WITH SSSC FOR HSI CLASSIFICATION 2759\nTABLE V\nTRAIN TIME AND TEST TIME OF DIFFERENT METHODS IN THE LONGKOU DATASET\nFig. 13. OA results on the HongHu dataset with a varying number of training\nsamples.\nSSFTT, and SC-SS-MTr with our method. The classiﬁcation\noutcomes are depicted in Fig. 13. It is evident that as the utilized\ntraining samples increase, the overall classiﬁcation performance,\nrepresented by OA, exhibits a gradual improvement. Among\nthem, the proposed MST-SSSNet method performs best in clas-\nsiﬁcation performance when the training set proportion is about\n30% to 100%, and its performance is equivalent to that of SSFTT\nand SC-SS-MTr methods when the training set proportion is\nbelow 30%, indicating that the effectiveness of our method\nendures even in situations characterized by inadequate sample\nsizes.\n5) Time Cost Comparison: The train and test time among\n2D-CNN, 3D-CNN, HybridSN, SSFTT, SC-SS-MTr, and our\nmethod on the LongKou dataset is given in Table V. The SSFTT\nmodel has the fastest calculation speed, and the proposed method\nand 2D-CNN are slightly higher than SSFTT, although SSFTT\nadopts a 3-D convolution and transformer structure that con-\nsumes computational resources. However, due to limited ﬁtting\nability, a large patch size is not required. In the original text,\nthe patch size is set to 13, which is much smaller than other\nmethods including the proposed method. This resulted in the\nmethod achieving the best training and testing time. However,\na too small patch size limits the improvement of classiﬁca-\ntion performance and prevents learning more global category\ninformation. The proposed SSSC can reduce parameters and,\nthus, reduce training time. However, the design of the two trans-\nformers in the proposed method, especially the subtransformer,\nrequires multiple calculations of attention and residual connec-\ntions, which increases the calculation time. After considering\nthe calculation time and classiﬁcation accuracy, it is worthwhile\nto increase computational resources. HyBridSN and 3D-CNN\nboth have high parameter count due to their multilayer 3-D\nconvolution, resulting in longer calculation time. SC-SS-MTr\nhas a heavy computational burden due to its two stages and the\nuse of masked transformers. Overall, the proposed MST-SSSNet\nachieves better classiﬁcation performance with minimal com-\nputing resources. Thus, it has excellent classiﬁcation efﬁciency.\nV. D ISCUSSION\nComputational complexity is an important indicator for eval-\nuating models. In this section, we discussed the computational\ncomplexity of the proposed module.\nWe analyzed the computational complexity of the proposed\nSSSC and the MST module. For the SSSC module, the biggest\ndifference from other feature extraction modules is the use of the\nproposed SSSC. Let us assume that the total number of pixels\nin the input 3-D feature map is P, K is the convolutional kernel\nsize, and Cl represents the number of output channels in the lth\nlayer. The algorithm complexity of SSSC can be expressed as\nO\n(\nP ·K2 ·2·\nl−1∑\ni=0\nCi ·Ci+1\n)\n. (17)\nThe complexity of most existing algorithms using 3-D con-\nvolution is\nO\n(\nP ·K3 ·\nl−1∑\ni=0\nCi ·Ci+1\n)\n. (18)\nIt can be seen that the proposed SSSC outperforms 3-D\nconvolution in terms of algorithm complexity.\nFor the MST module, the main computational resource is\nconsumed in the computation of multihead attention. If C repre-\nsents the number of channels, Nq and Nk represent the number\nof elements in query and key, respectively. The computational\ncomplexity of MSA can be expressed as\nO\n(\nNqC2 +vC2 +NqNkC\n)\n. (19)\nLet the token generator generate n tokens of length d. Each\npatch token is divided into s2 subtokens. The computational\ncomplexity of MSA in the MST module can further be expressed\nas\nO\n(\nn2d +(sn)2d/n\n)\n= O\n(\n5n2d\n)\n. (20)\nTherefore, the subtransformer consumes the main computing\nresources. For the entire MST, the computational complexity\ndepends on the number and length of tokens generated.\n2760 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nVI. C ONCLUSION\nThis article proposes an MST-SSSNet HSI classiﬁcation\nmethod, which achieves excellent classiﬁcation performance\nthrough the SSSC module and MST module. The SSSC module\nis a lightweight feature extraction module constructed from the\nproposed SSSC, which can accurately and effectively extract\nlocal spectral–spatial features. The MST module uses subtrans-\nformers to learn local details to help the main transformer\nestablish global correlation, thereby enhancing the coupling\nbetween local information and global information. On three\nrepresentative datasets, we compared state-of-the-art methods.\nBased on the results of experiments, our MST-SSSNet has the\nbest classiﬁcation performance compared with other methods.\nIn the future, the next step is to study MST-SSSNet-based\nunsupervised classiﬁcation methods in response to the scarcity\nof hyperspectral samples. And we hope to introduce the physical\nfeatures of spectral bands and prior knowledge of HSI into the\nmodel to improve classiﬁcation performance. In addition, we\nwill conduct more in-depth research on the problem that PCA\nmay not fully utilize HSI information.\nACKNOWLEDGMENT\nThe authors would like to thank Y . Zhong’s team at Wuhan\nUniversity, China, for collecting the Wuhan HSI dataset, as\nwell as the researchers who shared their source codes in the\ncommunity.\nREFERENCE\n[1] M. Wang et al., “Tensor decompositions for hyperspectral data\nprocessing in remote sensing: A comprehensive review,” IEEE\nGeosci. Remote Sens. Mag. , vol. 11, no. 1, pp. 26–72, Mar. 2023,\ndoi: 10.1109/MGRS.2022.3227063.\n[2] N. Audebert, B. L. Saux, and S. Lefevre, “Deep learning for\nclassiﬁcation of hyperspectral data: A comparative review,” IEEE\nGeosci. Remote Sens. Mag. , vol. 7, no. 2, pp. 159–173, Jun. 2019,\ndoi: 10.1109/MGRS.2019.2912563.\n[3] C. M. Gevaert, J. Suomalainen, J. Tang, and L. Kooistra, “Generation of\nspectral–temporal response surfaces by combining multispectral satellite\nand hyperspectral UA V imagery for precision agriculture applications,”\nIEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. , vol. 8, no. 6,\npp. 3140–3146, Jun. 2015, doi: 10.1109/JSTARS.2015.2406339.\n[4] Q. Hao et al., “Fusing multiple deep models for in vivo human\nbrain hyperspectral image classiﬁcation to identify glioblastoma tu-\nmor,” IEEE Trans. Instrum. Meas. , vol. 70, 2021, Art. no. 4007314,\ndoi: 10.1109/TIM.2021.3117634.\n[5] M. Shimoni, R. Haelterman, and C. Perneel, “Hyperspectral imaging for\nmilitary and security applications: Combining myriad processing and\nsensing techniques,” IEEE Geosci. Remote Sens. Mag. , vol. 7, no. 2,\npp. 101–117, Jun. 2019, doi: 10.1109/MGRS.2019.2902525.\n[6] S. Feng, S. Tang, C. Zhao, and Y . Cui, “A hyperspectral anomaly detection\nmethod based on low-rank and sparse decomposition with density peak\nguided collaborative representation,” IEEE Trans. Geosci. Remote Sens. ,\nvol. 60, 2022, Art. no. 5501513, doi: 10.1109/TGRS.2021.3054736.\n[7] J. Wang, Z. Li, J. Yang, S. Liu, J. Zhang, and S. Li, “A multilevel spatial and\nspectral feature extraction network for marine oil spill monitoring using\nairborne hyperspectral image,” Remote Sens. , vol. 15, no. 5, Feb. 2023,\nArt. no. 1302, doi: 10.3390/rs15051302.\n[8] A. Hamedianfar, K. Laakso, M. Middleton, T. Törmänen, J. Köykkä, and\nJ. Torppa, “Leveraging high-resolution long-wave infrared hyperspectral\nlaboratory imaging data for mineral identiﬁcation using machine learn-\ning methods,” Remote Sens. , vol. 15, no. 19, Oct. 2023, Art. no. 4806,\ndoi: 10.3390/rs15194806.\n[9] J. Atli Benediktsson and P. Ghamisi, Spectral-Spatial Classiﬁcation of\nHyperspectral Remote Sensing Images . Norwood, MA, USA: Artech\nHouse, 2015.\n[10] J. M. Bioucas-Dias, A. Plaza, G. Camps-Valls, P. Scheunders, N.\nNasrabadi, and J. Chanussot, “Hyperspectral remote sensing data analysis\nand future challenges,” IEEE Geosci. Remote Sens. Mag. , vol. 1, no. 2,\npp. 6–36, Jun. 2013, doi: 10.1109/MGRS.2013.2244672.\n[11] P. Ghamisi et al., “New frontiers in spectral-spatial hyperspectral image\nclassiﬁcation: The latest advances based on mathematical morphology,\nMarkov random ﬁelds, segmentation, sparse representation, and deep\nlearning,” IEEE Geosci. Remote Sens. Mag. , vol. 6, no. 3, pp. 10–43,\nSep. 2018, doi: 10.1109/MGRS.2018.2854840.\n[12] C. Zhao, B. Qin, S. Feng, W. Zhu, L. Zhang, and J. Ren, “An un-\nsupervised domain adaptation method towards multi-level features and\ndecision boundaries for cross-scene hyperspectral image classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens. , vol. 60, 2022, Art. no. 5546216,\ndoi: 10.1109/TGRS.2022.3230378.\n[13] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral re-\nmote sensing images with support vector machines,” IEEE Trans.\nGeosci. Remote Sens. , vol. 42, no. 8, pp. 1778–1790, Aug. 2004,\ndoi: 10.1109/TGRS.2004.831865.\n[14] Q. Ye et al., “L1-norm distance minimization-based fast ro-\nbust twin support vector plane clustering,” IEEE Trans. Neural\nNetw. Learn. Syst. , vol. 29, no. 9, pp. 4494–4503, Sep. 2018,\ndoi: 10.1109/TNNLS.2017.2749428.\n[15] L. Samaniego, A. Bardossy, and K. Schulz, “Supervised classiﬁcation\nof remotely sensed imagery using a modiﬁed k-NN technique,” IEEE\nTrans. Geosci. Remote Sens. , vol. 46, no. 7, pp. 2112–2125, Jul. 2008,\ndoi: 10.1109/TGRS.2008.916629.\n[16] W. Li, Q. Du, F. Zhang, and W. Hu, “Collaborative-representation-\nbased nearest neighbor classiﬁer for hyperspectral imagery,” IEEE\nGeosci. Remote Sens. Lett. , vol. 12, no. 2, pp. 389–393, Feb. 2015,\ndoi: 10.1109/LGRS.2014.2343956.\n[17] Y . Gu, J. Chanussot, X. Jia, and J. A. Benediktsson, “Multiple ker-\nnel learning for hyperspectral image classiﬁcation: A review,” IEEE\nTrans. Geosci. Remote Sens. , vol. 55, no. 11, pp. 6547–6565, Nov. 2017,\ndoi: 10.1109/TGRS.2017.2729882.\n[18] Y . E. SahIn, S. Arisoy, and K. Kayabol, “Anomaly detection with\nBayesian Gauss background model in hyperspectral images,” in\nProc. 26th Signal Process. Commun. Appl. Conf. , 2018, pp. 1–4,\ndoi: 10.1109/SIU.2018.8404293.\n[19] W. Liu, J. E. Fowler, and C. Zhao, “Spatial logistic regression\nfor support-vector classiﬁcation of hyperspectral imagery,” IEEE\nGeosci. Remote Sens. Lett. , vol. 14, no. 3, pp. 439–443, Mar. 2017,\ndoi: 10.1109/LGRS.2017.2648515.\n[20] M. Fauvel, J. A. Benediktsson, J. Chanussot, and J. R. Sveinsson, “Spectral\nand spatial classiﬁcation of hyperspectral data using SVMs and mor-\nphological proﬁles,” IEEE Trans. Geosci. Remote Sens. , vol. 46, no. 11,\npp. 3804–3814, Nov. 2008, doi: 10.1109/TGRS.2008.922034.\n[21] L. Sun, C. Ma, Y . Chen, H. J. Shim, Z. Wu, and B. Jeon, “Adjacent\nsuperpixel-based multiscale spatial-spectral kernel for hyperspectral clas-\nsiﬁcation,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. , vol. 12,\nno. 6, pp. 1905–1919, Jun. 2019, doi: 10.1109/JSTARS.2019.2915588.\n[22] B. Rasti et al., “Feature extraction for hyperspectral imagery: The\nevolution from shallow to deep: Overview and toolbox,” IEEE\nGeosci. Remote Sens. Mag. , vol. 8, no. 4, pp. 60–88, Dec. 2020,\ndoi: 10.1109/MGRS.2020.2979764.\n[23] Y . Chen, Z. Lin, X. Zhao, G. Wang, and Y . Gu, “Deep learning-based clas-\nsiﬁcation of hyperspectral data,” IEEE J. Sel. Topics Appl. Earth Observ.\nRemote Sens. , vol. 7, no. 6, pp. 2094–2107, Jun. 2014, doi: 10.1109/JS-\nTARS.2014.2329330.\n[24] Y . Chen, X. Zhao, and X. Jia, “Spectral–spatial classiﬁcation of hyper-\nspectral data based on deep belief network,” IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens. , vol. 8, no. 6, pp. 2381–2392, Jun. 2015,\ndoi: 10.1109/JSTARS.2015.2388577.\n[25] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-\nresolution using deep convolutional networks,” IEEE Trans. Pat-\ntern Anal. Mach. Intell. , vol. 38, no. 2, pp. 295–307, Feb. 2016,\ndoi: 10.1109/TPAMI.2015.2439281.\n[26] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:\nUniﬁed, real-time object detection,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2016, pp. 779–788, doi: 10.1109/CVPR.2016.91.\n[27] M. Anthimopoulos, S. Christodoulidis, L. Ebner, A. Christe, and S.\nMougiakakou, “Lung pattern classiﬁcation for interstitial lung diseases us-\ning a deep convolutional neural network,”I E E ET r a n s .M e d .I m a g ., vol. 35,\nno. 5, pp. 1207–1216, May 2016, doi: 10.1109/TMI.2016.2535865.\n[28] W. Hu, Y . Huang, L. Wei, F. Zhang, and H. Li, “Deep convolutional neural\nnetworks for hyperspectral image classiﬁcation,” J. Sensors , vol. 2015,\npp. 1–12, 2015.\nGAO et al.: MAIN–SUB TRANSFORMER WITH SSSC FOR HSI CLASSIFICATION 2761\n[29] W. Zhao and S. Du, “Spectral–spatial feature extraction for hyperspec-\ntral image classiﬁcation: A dimension reduction and deep learning ap-\nproach,”IEEE Trans. Geosci. Remote Sens. , vol. 54, no. 8, pp. 4544–4554,\nAug. 2016, doi: 10.1109/TGRS.2016.2543748.\n[30] Y . Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi, “Deep feature extrac-\ntion and classiﬁcation of hyperspectral images based on convolutional\nneural networks,” IEEE Trans. Geosci. Remote Sens. , vol. 54, no. 10,\npp. 6232–6251, Oct. 2016, doi: 10.1109/TGRS.2016.2584107.\n[31] S. K. Roy, G. Krishna, S. R. Dubey, and B. B. Chaudhuri, “HybridSN:\nExploring 3-D–2-D CNN feature hierarchy for hyperspectral image clas-\nsiﬁcation,” IEEE Geosci. Remote Sens. Lett. , vol. 17, no. 2, pp. 277–281,\nFeb. 2020, doi: 10.1109/LGRS.2019.2918719.\n[32] L. Mou, P. Ghamisi, and X. X. Zhu, “Deep recurrent neu-\nral networks for hyperspectral image classiﬁcation,” IEEE Trans.\nGeosci. Remote Sens. , vol. 55, no. 7, pp. 3639–3655, Jul. 2017,\ndoi: 10.1109/TGRS.2016.2636241.\n[33] Y . Ding et al., “AF2GNN: Graph convolution with adaptive ﬁlters and\naggregator fusion for hyperspectral image classiﬁcation,”Inf. Sci., vol. 602,\npp. 201–219, 2022.\n[34] Y . Ding et al., “Multi-feature fusion: Graph neural network and CNN com-\nbining for hyperspectral image classiﬁcation,” Neurocomputing, vol. 501,\npp. 246–257, 2022.\n[35] Y . Ding et al., “Multi-scale receptive ﬁelds: Graph attention neural network\nfor hyperspectral image classiﬁcation,” Expert Syst. Appl. , vol. 223, 2023,\nArt. no. 119858.\n[36] W. Zhu, C. Zhao, S. Feng, and B. Qin, “Multiscale short and long\nrange graph convolutional network for hyperspectral image classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens. , vol. 60, 2022, Art. no. 5535815,\ndoi: 10.1109/TGRS.2022.3199467.\n[37] R. Lei et al., “Multiscale feature aggregation capsule neural network for\nhyperspectral remote sensing image classiﬁcation,” Remote Sens., vol. 14,\nno. 7, Mar. 2022, Art. no. 1652, doi: 10.3390/rs14071652.\n[38] M. E. Paoletti, S. Moreno-Álvarez, and J. M. Haut, “Multiple attention-\nguided capsule networks for hyperspectral image classiﬁcation,” IEEE\nTrans. Geosci. Remote Sens. , vol. 60, 2022, Art. no. 5520420,\ndoi: 10.1109/TGRS.2021.3135506.\n[39] Y . Zhang, D. Hu, Y . Wang, and X. Yu, “Semisupervised hyper-\nspectral image classiﬁcation based on generative adversarial net-\nworks,” IEEE Geosci. Remote Sens. Lett. , vol. 15, no. 2, pp. 212–216,\nFeb. 2018.\n[40] C. Sun, X. Zhang, H. Meng, X. Cao, and J. Zhang, “AC-WGAN-GP:\nGenerating labeled samples for improving hyperspectral image classi-\nﬁcation with small-samples,” Remote Sens. , vol. 14, no. 19, Oct. 2022,\nArt. no. 4910, doi: 10.3390/rs14194910.\n[41] A. Dosovitskiy et al., “An image is worth 16 ×16 words: Transformers\nfor image recognition at scale,” in Proc. Int. Conf. Learn. Representions\n(ICLR), 2021, pp. 1–21.\n[42] D. Hong et al., “Spectral former: Rethinking hyperspectral image classi-\nﬁcation with transformers,” IEEE Trans. Geosci. Remote Sens. , vol. 60,\n2022, Art. no. 5518615, doi: 10.1109/TGRS.2021.3130716.\n[43] L. Huang, Y . Chen, and X. He, “Spectral–spatial masked transformer with\nsupervised and contrastive learning for hyperspectral image classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens. , vol. 61, 2023, Art. no. 5508718,\ndoi: 10.1109/TGRS.2023.3264235.\n[44] X. Huang, Y . Zhou, X. Yang, X. Zhu, and K. Wang, “SS-TMNet:\nSpatial–spectral transformer network with multi-scale convolution for\nhyperspectral image classiﬁcation,”Remote Sens., vol. 15, no. 5, Feb. 2023,\nArt. no. 1206, doi: 10.3390/rs15051206.\n[45] X. Qiao, S. K. Roy, and W. Huang, “Multiscale neighborhood atten-\ntion transformer with optimized spatial pattern for hyperspectral im-\nage classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 61, 2023,\nArt. no. 5523815, doi: 10.1109/TGRS.2023.3314550.\n[46] L. Sun, G. Zhao, Y . Zheng, and Z. Wu, “Spectral–spatial fea-\nture tokenization transformer for hyperspectral image classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens. , vol. 60, 2022, Art. no. 5522214,\ndoi: 10.1109/TGRS.2022.3144158.\n[47] S. K. Roy, A. Deria, C. Shah, J. M. Haut, Q. Du, and A. Plaza,\n“Spectral–spatial morphological attention transformer for hyperspectral\nimage classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 61, 2023,\nArt. no. 5503615, doi: 10.1109/TGRS.2023.3242346.\n[48] K. Han et al., “A survey on vision transformer,” IEEE Trans. Pat-\ntern Anal. Mach. Intell. , vol. 45, no. 1, pp. 87–110, Jan. 2023,\ndoi: 10.1109/TPAMI.2022.3152247.\n[49] C. Yu, R. Han, M. Song, C. Liu, and C.-I. Chang, “A simpliﬁed 2D-3D\nCNN architecture for hyperspectral image classiﬁcation based on spatial–\nspectral fusion,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. ,\nvol. 13, pp. 2485–2501, 2020, doi: 10.1109/JSTARS.2020.2983224.\n[50] H. Yu, H. Zhang, Y . Liu, K. Zheng, Z. Xu, and C. Xiao, “Dual-channel\nconvolution network with image-based global learning framework for\nhyperspectral image classiﬁcation,” IEEE Geosci. Remote Sens. Lett. ,\nvol. 19, 2022, Art. no. 6005705, doi: 10.1109/LGRS.2021.3139358.\n[51] C. Yu, R. Han, M. Song, C. Liu, and C. I. Chang, “Feedback\nattention-based dense CNN for hyperspectral image classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens. , vol. 60, 2022, Art. no. 5501916,\ndoi: 10.1109/TGRS.2021.3058549.\n[52] S. Hao, Y . Xia, and Y . Ye, “Generative adversarial network with trans-\nformer for hyperspectral image classiﬁcation,” IEEE Geosci. Remote Sens.\nLett., vol. 20, 2023, Art. no. 5510205, doi: 10.1109/LGRS.2023.3322139.\n[53] J. Bai et al., “Hyperspectral image classiﬁcation based on multibranch at-\ntention transformer networks,” IEEE Trans. Geosci. Remote Sens. , vol. 60,\n2022, Art. no. 5535317, doi: 10.1109/TGRS.2022.3196661.\n[54] C. Zhao et al., “Hyperspectral image classiﬁcation with multi-attention\ntransformer and adaptive superpixel segmentation-based active learn-\ning,” IEEE Trans. Image Process. , vol. 32, pp. 3606–3621, 2023,\ndoi: 10.1109/TIP.2023.3287738.\n[55] W. Zhou, S.-I. Kamata, H. Wang, and X. Xue, “Multiscanning-\nbased RNN–Transformer for hyperspectral image classiﬁcation,” IEEE\nTrans. Geosci. Remote Sens. , vol. 61, 2023, Art. no. 5512319,\ndoi: 10.1109/TGRS.2023.3277014.\n[56] J. He, L. Zhao, H. Yang, M. Zhang, and W. Li, “HSI-BERT: Hyperspec-\ntral image classiﬁcation using the bidirectional encoder representation\nfrom transformers,” IEEE Trans. Geosci. Remote Sens. , vol. 58, no. 1,\npp. 165–178, Jan. 2020, doi: 10.1109/TGRS.2019.2934760.\n[57] X. Cao, H. Lin, S. Guo, T. Xiong, and L. Jiao, “Transformer-based masked\nautoencoder with contrastive loss for hyperspectral image classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens. , vol. 61, 2023, Art. no. 5524312,\ndoi: 10.1109/TGRS.2023.3315678.\n[58] H. Yu, Z. Xu, K. Zheng, D. Hong, H. Yang, and M. Song, “MSTNet:\nA multilevel spectral–spatial transformer network for hyperspectral im-\nage classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 60, 2022,\nArt. no. 5532513, doi: 10.1109/TGRS.2022.3186400.\n[59] X. He, Y . Chen, and Z. Lin, “Spatial-spectral transformer for hyperspectral\nimage classiﬁcation,” Remote Sens., vol. 13, no. 3, Jan. 2021, Art. no. 498,\ndoi: 10.3390/rs13030498.\n[60] H. Yang, H. Yu, K. Zheng, J. Hu, T. Tao, and Q. Zhang, “Hyperspectral\nimage classiﬁcation based on interactive transformer and CNN with mul-\ntilevel feature fusion network,” IEEE Geosci. Remote Sens. Lett. , vol. 20,\n2023, Art. no. 5507905, doi: 10.1109/LGRS.2023.3303008.\n[61] X. Zhang, Y . Sun, K. Jiang, C. Li, L. Jiao, and H. Zhou, “Spatial se-\nquential recurrent neural network for hyperspectral image classiﬁcation,”\nIEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. , vol. 11, no. 11,\npp. 4141–4155, Nov. 2018.\n[62] M. E. Paoletti, J. M. Haut, J. Plaza, and A. Plaza, “Scalable recurrent\nneural network for hyperspectral image classiﬁcation,” J. Supercomputing,\nvol. 76, no. 11, pp. 8866–8882, Feb. 2020.\n[63] W. Zhou, S. I. Kamata, Z. Luo, and H. Wang, “Multiscanning\nstrategy based recurrent neural network for hyperspectral image\nclassiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 60, 2022,\nArt. no. 5521018.\n[64] Y . Ding et al., “Self-supervised locality preserving low-pass graph convo-\nlutional embedding for large-scale hyperspectral image clustering,” IEEE\nTrans. Geosci. Remote Sens. , vol. 60, 2022, Art. no. 5536016.\n[65] Z. Zhang et al., “Multireceptive ﬁeld: An adaptive path aggregation graph\nneural framework for hyperspectral image classiﬁcation,” Expert Syst.\nAppl., vol. 217, Art. no. 119508, 2023.\n[66] J. Feng, Z. Gao, R. Shang, X. Zhang, and L. Jiao, “Multi-complementary\ngenerative adversarial networks with contrastive learning for hyperspectral\nimage classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 61, 2023,\nArt. no. 5520018, doi: 10.1109/TGRS.2023.3304836.\n[67] S. Christian, V . Vincent, I. Sergey, S. Jonathon, and W. Zbigniew,\n“Rethinking the inception architecture for computer vision,” 2015,\narXiv:1512.00567v3.\n[68] Y . Zhong, X. Hu, C. Luo, X. Wang, J. Zhao, and L. Zhang, “WHU-Hi:\nUA V-borne hyperspectral with high spatial resolution (H2) benchmark\ndatasets and classiﬁer for precise crop identiﬁcation based on deep convo-\nlutional neural network with CRF,” Remote Sens. Environ., vol. 250, 2020,\nArt. no. 112012.\n[69] Y . Zhong et al., “Mini-UA V-borne hyperspectral remote sensing: From\nobservation and processing to applications,” IEEE Geosci. Remote Sens.\nMag., vol. 6, no. 4, pp. 46–62, Dec. 2018.\n[70] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Ex-\ntinction proﬁles fusion for hyperspectral images classiﬁcation,” IEEE\nTrans. Geosci. Remote Sens. , vol. 56, no. 3, pp. 1803–1815, Mar. 2018,\ndoi: 10.1109/TGRS.2017.2768479.\n2762 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nJingpeng Gao (Member, IEEE) received the B.S.,\nM.S., and Ph.D. degrees in electrical information\nengineering from Harbin Engineering University,\nHarbin, China, in 2002, 2007, and 2014, respectively.\nSince 2002, he has been with Harbin Engineering\nUniversity, and became a Lecturer in 2007, and a\nMaster Tutor in 2015. From 2015 to 2017, he was with\nthe State Key Laboratory of Computational Mathe-\nmatical and Experimental Physics, Beijing Institute\nof Space Long March Vehicle, as a Postdoctoral\nResearcher. His research interests include machine\nlearning, radar target recognition, and hyperspectral image processing.\nXiangyu Ji (Graduate Student Member, IEEE) was\nborn in 2001. He received the B.S. degree in electronic\nengineering from Xidian University, Xi’an, China, in\n2023. He is currently working toward the M.S. degree\nin electronic information with the College of In-\nformation and Communication Engineering, Harbin\nEngineering University, Harbin, China.\nHis research interests include deep learning and\nhyperspectral image processing.\nGeng Chen received the B.S. degree in electronic\nand information engineering from the Shandong Uni-\nversity of Science and Technology, Qingdao, China,\nin 2022. He is currently working toward the M.S.\ndegree in electronic and information engineering with\nHarbin Engineering University, Harbin, China.\nHis main research interests include machine learn-\ning and hyperspectral image processing.\nRuitong Guo was born in 2000. She received the\nB.S. degree in electronic engineering from Northeast\nPetroleum University, DaQing, China, in 2022. She\nis currently working toward the M.S. degree in infor-\nmation and communication engineering with Harbin\nEngineering University, Harbin, China.\nHer research interests include remote sensing im-\nage processing and machine learning.",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.858776867389679
    },
    {
      "name": "Computer science",
      "score": 0.6075524091720581
    },
    {
      "name": "Separable space",
      "score": 0.5590599179267883
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.5510823130607605
    },
    {
      "name": "Artificial intelligence",
      "score": 0.536811351776123
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5163766145706177
    },
    {
      "name": "Contextual image classification",
      "score": 0.44352346658706665
    },
    {
      "name": "Remote sensing",
      "score": 0.38869598507881165
    },
    {
      "name": "Computer vision",
      "score": 0.3484102189540863
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2838841676712036
    },
    {
      "name": "Mathematics",
      "score": 0.2332211434841156
    },
    {
      "name": "Geology",
      "score": 0.12997254729270935
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151727225",
      "name": "Harbin Engineering University",
      "country": "CN"
    }
  ]
}