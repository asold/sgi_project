{
  "title": "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing",
  "url": "https://openalex.org/W4389518687",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2142365574",
      "name": "Carlos Gómez Rodríguez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1930897874",
      "name": "Paul Williams",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4362655426",
    "https://openalex.org/W2891214146",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3174264667",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3155340931",
    "https://openalex.org/W3100714086",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4287084089",
    "https://openalex.org/W4225012671",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W1763968285",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4396833426",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W3109414691",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W3167303745",
    "https://openalex.org/W2116817443",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4375869921",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W4220765843",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W2322883005",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W2279965131",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4366547384",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4243189996",
    "https://openalex.org/W4385734232",
    "https://openalex.org/W2037789405"
  ],
  "abstract": "We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14504–14528\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nA Confederacy of Models: a Comprehensive Evaluation of LLMs on\nCreative Writing\nCarlos Gómez-Rodríguez\nUniversidade da Coruña, CITIC\nDepartment of CS and IT\n15071 A Coruña, Spain\ncarlos.gomez@udc.es\nPaul Williams\nSchool of Business & Creative Industries\nUniversity of the Sunshine Coast\nSunshine Coast, Australia\npwillia3@usc.edu.au\nAbstract\nWe evaluate a range of recent LLMs on En-\nglish creative writing, a challenging and com-\nplex task that requires imagination, coherence,\nand style. We use a difficult, open-ended sce-\nnario chosen to avoid training data reuse: an\nepic narration of a single combat between Ig-\nnatius J. Reilly, the protagonist of the Pulitzer\nPrize-winning novel A Confederacy of Dunces\n(1980), and a pterodactyl, a prehistoric flying\nreptile. We ask several LLMs and humans to\nwrite such a story and conduct a human evalu-\ntion involving various criteria such as fluency,\ncoherence, originality, humor, and style. Our\nresults show that some state-of-the-art com-\nmercial LLMs match or slightly outperform\nour writers in most dimensions; whereas open-\nsource LLMs lag behind. Humans retain an\nedge in creativity, while humor shows a binary\ndivide between LLMs that can handle it com-\nparably to humans and those that fail at it. We\ndiscuss the implications and limitations of our\nstudy and suggest directions for future research.\n1 Introduction\nIn recent years, large language models (LLMs)\nhave achieved remarkable progress in a wide range\nof language processing and generation tasks, such\nas question answering, machine translation, or text\nsummarization, among many others (Zhao et al.,\n2023). This has motivated research on evaluating\nand comparing the performance of LLMs in various\ntasks, both between each other and with respect to\nhuman performance; including both task-specific\nevaluations (see e.g. (Jiao et al., 2023; Gilson et al.,\n2023)) and overarching benchmark suites that seek\nto provide comprehensive evaluation throughout\nmany dimensions (Hendrycks et al., 2021; Liang\net al., 2022; Srivastava et al., 2022).\nCreative writing is also one application where\nLLMs have been observed to produce good results.\nAccording to Franceschelli and Musolesi (2023),\ntheir generated outputs in poetry or storytelling\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n20\n40\n60\n80\nOverall  rating\nComparison on overall rating\nMean\nFigure 1: Box plot comparing overall ratings for stories\nby humans and 12 LLMs, arranged left to right by mean\noverall rating. Boxes show median, quartiles Q1-Q3,\nand whiskers at 1.5 IQR, with values outside that range\nplotted as outliers. Filled red circles represent means.\nare “often of astonishing quality”, and Clark et al.\n(2021) showed that humans cannot reliably distin-\nguish human- from LLM-authored stories. How-\never, and despite the amount of papers experiment-\ning with LLMs for this purpose, an evaluation com-\nparing the abilities of current LLMs as standalone\nsystems for creative writing seems to be lacking.\nHere, we provide such an evaluation, comparing\nthe storytelling capability of 12 recent, instruction-\naligned language models between each other and\nwith human writers. We do so using a rubric based\non established creative writing evaluation propos-\nals (Davidow and Williams, 2016; Carey et al.,\n2022), but specifically adapted to the task. Our\ncomparison is performed on a purely zero-shot set-\nting, with a natural human prompt (based on a\ncombat between Ignatius J. Reilly, protagonist of\nA Confederacy of Dunces, and a pterodactyl) that\n14504\nhas been specifically chosen to be challenging and\nmeaningful while preventing as much as possible\nthe option for LLMs to resort to regurgitating or\nadapting material from their training set.\n2 Related work\nLLMs in creative writing LLMs have been\nused in creative writing since their first generation,\nwith models like GPT-2 (Radford et al., 2019) or\nBART (Lewis et al., 2020). However, these models\nsuffered from a lack of long-range coherence lead-\ning to contradictions or inconsistencies when gen-\nerating stories (Nye et al., 2021). Thus, they were\nnot viable as standalone story generators. Instead,\nthey were used either with specialized fine-tuning\nfor the task (See et al., 2019); or as components\nof systems that incorporated external knowledge\n(Guan et al., 2020, 2021), storyline planning (Tan\net al., 2021), or both (Xu et al., 2020); or for co-\ncreation with a human in the loop (Swanson et al.,\n2021), a line of research that has also continued\nwith newer models (Yuan et al., 2022; Chung et al.,\n2022; Mirowski et al., 2023).\nHere our goal is not to produce a specialized\nsystem, but to evaluate the performance of LLMs\nby themselves as creative writers. Thus, we focus\non the purely zero-shot setting, where a general-\nistic LLM is asked to write a story with no extra\nfine-tuning, in-context learning (Dong et al., 2023),\nprompt engineering or additional components. This\nhas only become viable with the extra coherence\nand consistency in long texts provided by newer\nLLMs, especially those that are aligned to follow in-\nstructions with instruction tuning (Wei et al., 2022;\nSanh et al., 2022) or reinforcement learning with\nhuman feedback (Ouyang et al., 2022).\nTo our knowledge, there was no previous work\nin this line. In fact, evaluation in creative writing is\na conspicuous gap in LLM evaluation benchmarks:\nthe huge BIG-bench suite (Srivastava et al., 2022)\ncurrently has over 200 tasks, but does not include\nany creative writing, and HELM (Liang et al., 2022)\ncites it as an “aspirational scenario” for future work.\nThis likely owes to benchmarks focusing on easily-\nautomatable metrics, whereas the gold standard\nfor creative writing is human evaluation (Belz and\nReiter, 2006), which is much costlier.\nThe closest previous work to our proposal is the\nrecent preprint by Xie et al. (2023), where GPT-3 is\ncompared to previous storytelling systems via hu-\nman evaluation. However, there are several impor-\ntant differences with respect to our work: (1) they\nuse prompt-based learning, providing examples to\nadapt the model to the task, rather than a purely\nzero-shot conversational prompt, (2) they evaluate\na single LLM while our goal is to compare LLMs,\nand (3) they use pre-existing story datasets, which\nincreases the risk of models benefitting from simi-\nlar stories present in their training set, something\nthat we have tried to avoid as described below.\nIn another recent preprint, Garrido-Merchán\net al. (2023) generate Lovecraftian horror literature.\nHowever, they also focus on a single LLM (GPT-4),\nusing careful prompt engineering to optimize its\nperformance rather than a pure zero-shot setting,\nand evaluation is only on whether humans can dis-\ntinguish AI-generated from real stories (concluding\nthat, in those circumstances, they cannot). Sawicki\net al. (2023) apply a similar evaluation (but auto-\nmated) to Whitmanian poems generated by three\nversions of GPT, also with a negative result.\nFinally, concurrently with our study, a preprint\nby Chakrabarty et al. (2023), released a few months\nafter our submission, evaluates three LLMs for cre-\native writing in a more similar way to ours: they\napply human evaluation to compare stories by hu-\nmans and LLMs in a zero-shot setting. However,\nthere are important differences in methodology and\nscope between both studies. A comprehensive com-\nparison will be made in Section 5, following the\nexposition of our methods and results.\nCreative writing evaluation Creative Writing is\na challenging and complex performative language\nact that requires a number of skills, such as an ex-\npertise in craft, cultural and literary competency,\nlinguistic fluency, coherence, complex connotative\nand metaphorical levels of understanding, innova-\ntion, originality and imagination, to name a few.\nThe craft of writing involves innovation with\nstyle and voice, needs a fundamental understanding\nand use of structural elements (grammar, spelling,\npunctuation), craft elements (plot, character, set-\nting, point of view and imaginative capacity, such\nskills defined by Bloom as ‘putting elements to-\ngether to form a coherent or functional whole; re-\norganizing elements into a new pattern or structure\nthrough generating, planning, or producing’ (An-\nderson and Krathwohl, 2001, p.21). Evaluation of\ncreative writing therefore must take into account all\nthese factors, and assessment in university Creative\nWriting courses is usually based on a rubric that\nattempts to measure the basic elements of narrative\n14505\ncraft, as well as the specific requirements on the as-\nsignment (Kroll, 1997; Norris, 2013; Davidow and\nWilliams, 2016; Wise and van Luyn, 2020; Carey\net al., 2022).\n3 Materials and Methods\n3.1 Task\nThe chosen task to compare the LLMs under con-\nsideration is defined by the following prompt:\nWrite an epic narration of a single com-\nbat between Ignatius J. Reilly and a ptero-\ndactyl, in the style of John Kennedy\nToole.\nThe prompt is provided to the models from a\nfresh state, without previous context.\nWe believe this task is particularly adequate to\nchallenge the capabilities of models for creative\nwriting, for the following reasons:\n• It is a non-standard, “wacky” scenario that has\nbeen invented for the occasion, so it is very\nunlikely that the systems’ training sets contain\ncoincident or similar tasks, or pieces of stories\nthat can be reused for the task. No informa-\ntion about this task was posted to the Internet\nor disseminated in any other way before the\nLLMs were prompted.\n• It features a specific literary character, Ig-\nnatius J. Reilly, so we can evaluate the models\non how they capture the personality of the\ncharacter. At the same time, this character\nappeared in only one book, and does not seem\nto have been the target of fan fiction. This\nmakes the task more challenging due to hav-\ning to capture the personality of the protag-\nonist from scarce material, while making it\nunlikely that the model can just reuse material\nfrom existing stories.\n• In turn, A Confederacy of Duncesis the only\nwork of its author John Kennedy Toole, so the\nauthor’s style also needs to be captured from\nscarce material.\n• This novel is widely considered to be a classic\nof comic fiction, and won the 1981 Pulitzer\nPrize in the Fiction category. Thus, writing a\nstory about its protagonist in the author’s style\nsets an adequately high bar.\n• The genre requires humor, which is consid-\nered to be an especially subtle feature of hu-\nman language and challenging for machines,\nincluding LLMs, to exhibit (Jentzsch and Ker-\nsting, 2023).\n• While the task is challenging due to putting to-\ngether two unlikely antagonists, the prompt’s\nlevel of detail is open-ended enough to give\nample space for creativity, as no specifications\nare made about setting, weapons, outcome or\nother aspects of the story.\n3.2 Models\nWe gave the task to a confederacy of large language\nmodels, composed of all such models we could find\nthat (1) were available to the authors by April 20\n2023, which was the cutoff date to build our corpus\nof stories, and (2) were adjusted to conversational\nsettings and instruction-following by using tech-\nniques like instruction tuning (Wei et al., 2022;\nSanh et al., 2022) or reinforcement learning with\nhuman feedback (Ouyang et al., 2022). This is in\ncontrast to “vanilla” language models configured to\njust predict the next word, like plain GPT-3 (Brown\net al., 2020) or Llama (Touvron et al., 2023), which\ngenerally cannot handle natural prompts like the\none we use. We only included distinct models,\nnot front-ends to the same model (but we did in-\nclude derived models with substantial additions,\nlike Bing Chat which is claimed to use GPT-4 but\nadds search capabilities, or various models that\nwere fine-tuned from Llama weights). For models\nthat came in a variety of parameter sizes, we used\nthe largest one, or the largest we could execute with\nlocal or remote resources. For models with several\navailable versions, we used the latest available, ex-\ncept in the case of ChatGPT where we included\nboth the GPT-3.5 and GPT-4 versions, due to the\nwider availability of 3.5 (the latest version offered\nfor free at cutoff time) and the lack of information\non whether GPT-4 is an incremental improvement\nor a different model with its own tradeoffs.\nThis selection yielded the following 12 language\nmodels. We list them in alphabetical order as\nchronological ordering would be challenging, due\nto closed releases, opaque updates from some of\nthe commercial products, and many of the models\nbeing released almost simultaneously:\nAlpaca (Taori et al., 2023), a Stanford model\nfine-tuned from Llama (Touvron et al., 2023) on\ninstruction data generated with the self-instruct\n14506\nmethods of (Wang et al., 2022). We use the 13B-\nparameter version, the largest available at cutoff.\nBard, Google’s experimental conversational\nLLM offering, claimed to be based on a lightweight\nversion of LaMDA (Thoppilan et al., 2022). It\ncan use content from the web to answer questions.\nModel details have not been made public.\nBing Chat, an LLM offered by Microsoft’s Bing\nsearch engine. Claimed to use GPT-41, further tech-\nnical details have not been made public. The model\nperforms web searches and uses the results to aug-\nment its context window with relevant information.\nIt can also provide links to sources for its claims\n(although this is not relevant for our creative writ-\ning task, where no such links were provided or\nneeded). We used its Creative mode, the obvious\nfit for our task. A problem worth mentioning is\nthat we found the model to be subject to heavy cen-\nsorship, which affected our experiment: in most\nprompting attempts, the story would be deleted by\nthe filtering system before being finished. When\nthis happened, we just reset and re-prompted the\nmodel, repeating the process until a full story was\nobtained. Over 100 tries were needed to obtain 5\nnon-censored stories. We are aware that this may\nintroduce bias (as non-censored stories may have a\ndifferent quality distribution than what the model\ncould potentially generate without the filter) but\nthis is unavoidable from our end, since we cannot\nbypass moderation. In any case, the sample does\nreflect what a user can obtain from the end product,\nas the censored stories are out of reach.\nChatGPT with GPT-3.5, an OpenAI successor\nto the 175B-parameter GPT-3 model (Brown et al.,\n2020) which was tuned using reinforcement learn-\ning with human feedback, namely a variant of the\nInstructGPT method by Ouyang et al. (2022). We\nused the March 23 version provided by OpenAI’s\nfree ChatGPT service.\nChatGPT with GPT-4, the most advanced lan-\nguage model released by OpenAI at cutoff time. A\ndescription of the model is available in (OpenAI,\n2023), although essential technical details like the\nnumber of parameters have not been published. We\nused the March 23 version provided by OpenAI’s\nChatGPT Plus service.\nClaude is a language model trained by An-\nthropic. While details about its implementation are\nnot public, it is known to be a sucessor of the model\n1https://blogs.bing.com/search/march_2023/\nConfirmed-the-new-Bing-runs-on-OpenAIâĂŹs-GPT-4\ndescribed in (Bai et al., 2022), a 52B-parameter\nmodel aligned to be helpful with Constitutional AI,\na list of guiding principles provided to the model,\ncombined with a mix of supervised learning and\nreinforcement learning with AI feedback. We used\nversion 1.2 of the model.\nDolly 2.0(dolly-v2-12b), a 12B-parameter lan-\nguage model trained by Databricks, derived from\nEleutherAI’s Pythia-12B model (Biderman et al.,\n2023) after fine-tuning on a 15K instruction corpus.\nAt cutoff date, it was the only available conversa-\ntional LLM where all of its components could be\nconsidered fully open source2, as the code, weights\nand instruction datasets all have open-source li-\ncenses compatible with any use, including commer-\ncial use, and no data from proprietary systems like\nChatGPT has been used for finetuning.\nGPT4All-J (Anand et al., 2023b), an improve-\nment over its predecessor GPT4All (Anand et al.,\n2023a). The base model is the 6B-parameter GPT-J\n(Wang and Komatsuzaki, 2021), which has been\nfine-tuned on a dataset expanded from a mix of\nexisting sources.\nKoala (Geng et al., 2023), a model fine-tuned\nfrom Llama (Touvron et al., 2023) by researchers\nfrom the university of Berkeley, on a variety of\ndialogue data obtained from the web. We use the\n13B-parameter version.\nOpenAssistant (Köpf et al., 2023) is an LLM\nfine-tuned on a large, free, human-generated con-\nversation corpus created by a crowdfunding effort\ninvolving over 13,500 volunteers. We used the OA-\nSFT-Llama-30B model, fine-tuned from the 30B-\nparameter Llama (Touvron et al., 2023) model.\nStableLM is Stability AI’s series of language\nmodels. We used StableLM-Tuned-Alpha-7B.\nWith 7B parameters, this is the largest model avail-\nable (at cutoff time) among a series of models\ntrained on a dataset built from The Pile (Gao et al.,\n2021) and fine-tuned on a combination of conver-\nsational LLM corpora.\nVicuna (Chiang et al., 2023) is another member\nof the family of models obtained by fine-tuning\nLlama (Touvron et al., 2023), in this case with user-\nshared conversations with ChatGPT. We used the\n13B-parameter version of the model.\n3.3 Evaluation rubric\nThe creative writing rubric was designed for as-\nsessment of creative writing assignments in uni-\n2https://opensource.org/definition-annotated/\n14507\nID Description\n1 Overall/holistic/cohesive readability of the story (not just a compilation of elements).\n2 Use of key narrative elements - vocabulary choice, imagery, setting, themes, dialogue, characterisation, point of view.\n3 Structural elements and presentation which reflects the control of structural elements such as spelling, grammar,\npunctuation, paragraphing, and formatting.\n4 Overall plot logic: hook, conflict, initial crisis, rising and falling action, denouement/ resolution (Freitag’s pyramid).\n5 Creativity/innovation/originality/ research—credibility, new knowledge, avoidance of cliché and derivative tropes.\n6 Incorporation of the John Kennedy Toole style of writing using the indicators/ characteristics listed.\n7 Understanding and habitation of the epic genre of heroic/legendary adventure.\n8 Description and credibility of a single combat scene.\n9 Accurate inclusion of two main characters Ignatius J. Reilly and a pterodactyl in action and description.\n10 Use of a characteristically dark humorous tone.\nTable 1: Creative writing evaluation rubric. All items are scored out of ten points. Marking guideline: Emerging\n1-4, Competent 5-8, Sophisticated 9-10.\nversity creative writing courses, and is taken in\npart from a university textbook by one of the au-\nthors of this article, Playing with Words(Davidow\nand Williams, 2016) and an article that justifies the\nuse of this rubric (Carey et al., 2022). This rubric\nevaluates creative production in five holistic craft-\nbased criteria and measures craft skills based on a\nwriting style outlined in the article: among others,\nFlaubert’s insistence on le mot juste(the right word\nor expression), Strunk and White’s The Elements\nof Style(2008[1918]), George Orwell’s rules for\nconcreteness and clarity (Orwell, 1946); and An-\nnie Dillard’s rules for writing good prose (Dillard,\n1981).\nThe rubric for this AI task adds five more criteria\nwhich address the specific prompt requirements,\nsuch as genre, style, tone, character and action.\nEach of the ten criteria is awarded 10 points out of\na total 100 points. The rubric has been specifically\ndesigned to measure the quality of writing craft, to\navoid formulaic, rule-based writing and to address\nthe very specific task addressed here.\nThe criteria are detailed in Table 1, with more\ndetails given in the Appendix C. The holistic scale\n(emerging, competent, sophisticated) guides hu-\nman raters to assess holistically: ‘a holistic scale\nmeasures the relative success of a text but does so\nthrough a rubric that incorporates many of the traits\nin analytic scoring as heuristics towards a concep-\ntion of a whole rather than as a sum of autonomous\ncomponents’ (Perelman, 2018, p.16).\n3.4 Evaluation methodology\nWe prompted each of the LLMs 5 times with the\nprompt given in Section 3.1. Each prompt was\nmade from a fresh state, i.e., in a zero-shot setting\nwithout any previous context that could help guide\nthe models. The resulting stories had an average of\n379 words (std = 248, min = 23, max = 1223).\nThen, we also asked 5 human writers to each\nwrite a story following the same prompt. For uni-\nformity, we suggested a length range coherent with\nthe LLM-generated stories (250 to 1200 words).\nThe writers were Honours and postgraduate Cre-\native Writing students that volunteered for the task,\nand all of them studied the specific task require-\nments (e.g. John Kennedy Toole’s style) before\nwriting their stories. However, they were not given\naccess to the AI-generated stories and they were\ninstructed not to use LLMs at all to help them write.\nThe result is, thus, a corpus of 60 AI-generated\nstories (5 for each of the 12 considered LLMs)\nplus an additional 5 human-generated stories, all in\nplain text format. The corpus is available at https:\n//doi.org/10.5281/zenodo.8435671.\nThe only preprocessing made to the stories is\nthat (1) we removed leading sentences that de-\nscribed the task, often present in LLM answers\n(e.g.: “Here is a potential epic narration in the\nexaggerated style of John Kennedy Toole’s A Con-\nfederacy of Dunces:”) (2) we removed titles from\nstories that had them, and (3) we unified paragraph\nformatting, leaving one line between paragraphs in\nall the plain text files. Other than these changes,\nmade for uniformity and to preserve the blindness\nof the rating process, we left the text as it was.\nWe recruited 10 raters, also Honours and post-\ngraduate Creative Writing students that were ac-\nquainted with the specific requirements of the task,\nand we instructed them to grade stories according\nto the rubric. Since the raters were volunteers, to\nkeep the workload low, each rater did not rate all\nthe stories. Instead, we divided the 65 stories into\n5 groups of 13 stories each (each group containing\none story by each LLM, plus one story by a human)\nand assigned one rater to each group. In this way,\n14508\nRubric item 1 2 3 4 5 6 7 8 9 10 overall\nchatgpt-gpt4 8.7±0.8 8.7±0.7 8.4±1.3 8.3±0.7 7.6±1 8.0±1.2 8.1±1.4 8.5±0.8 7.9±1.6 6.0±2.8 80.2±7.3\nclaude12 8.0±1.7 8.0±1.6 8.1±1.2 7.9±1.8 7.1±2.3 7.5±2 6.4±2.2 7.5±1.8 7.4±2.5 6.5±2.5 74.4±15.9\nhuman 7.3±2.3 7.8±1.8 7.3±1.7 7.2±1.8 8.0±2 7.2±2.4 4.9±2.1 6.3±2.2 7.7±2.1 6.4±3.4 70.1±17.4\nbing 7.8±2 7.5±2.2 7.9±1.7 7.4±2.1 7.0±1.6 6.8±2.4 5.3±2.9 6.2±2.1 7.4±2.2 6.2±2.6 69.5±18.4\nchatgpt-gpt35 7.5±2 6.5±2.4 8.1±1.3 7.0±2.2 5.4±2.5 5.3±2.4 6.8±1.5 7.6±1.2 5.5±2.5 3.3±2.8 63.0±15.4\nkoala 7.5±2.5 6.7±2.2 8.2±1.2 6.8±2.6 5.8±2.3 4.8±2.7 5.8±2.4 5.5±2.3 5.5±2.3 3.4±3.2 60.0±19.2\nvicuna 7.9±1.7 6.7±1.6 8.1±1.3 7.0±1.6 5.1±1.9 4.6±2.3 5.7±2.3 6.1±1.9 5.4±2.7 2.4±1.9 59.0±13.8\noa 7.2±2.2 5.8±2.4 7.2±2.5 6.2±2.6 4.9±2.1 3.9±2.4 5.8±2.4 6.5±2.2 4.3±2.3 2.9±3.1 54.7±18\nbard 6.5±2.5 4.9±2.1 6.8±1.9 5.5±2.7 3.9±2.1 3.8±2.5 4.7±2.6 4.6±2.7 5.0±2.4 2.5±2 48.2±20.1\ngpt4all 6.5±2.2 5.4±1.7 7.2±1.7 6.5±2.1 4.1±2.2 2.4±2.2 5.4±2.5 5.6±2.4 2.5±1.4 1.2±0.8 46.8±13.1\nstablelm 5.5±1.8 5.0±2.5 6.6±1.9 3.8±2 3.2±1.5 2.1±2.2 4.4±1.9 3.8±2 2.9±2.6 1.4±1.5 38.7±17.2\ndolly 4.6±2.2 5.0±2.2 5.6±2.5 3.2±1.9 4.2±2.8 3.1±2.2 4.4±1.9 3.3±1.8 3.0±2 1.5±1.5 37.9±13.6\nalpaca 5.2±3.1 3.1±1.4 4.9±3 4.2±1.9 1.9±1 2.0±1.4 3.7±3 3.9±2.8 2.1±1.5 1.1±0.6 32.1±15.7\naverage 6.9±2.1 6.2±1.9 7.3±1.8 6.2±2 5.2±2 4.7±2.2 5.5±2.3 5.8±2 5.1±2.2 3.4±2.2 56.6±15.8\nTable 2: Results for each rubric item, as well as overall score. Each cell shows average ± standard deviation for the\nratings achieved by a given model (or human writers) on a given rubric item. The bottom line shows the average\namong all models (and human writers). Models are sorted by overall score. The best result for each rubric item is\nhighlighted in boldface.\nwe ensure (1) that we have at least two ratings per\nstory, allowing us to measure inter-rater agreement,\n(2) that comparisons are fair, in the sense that no\nLLM (or the humans) is advantaged by being as-\nsigned more lenient raters, because each LLM (and\nhumans) receives exactly one rating by each of the\n10 raters, and (3) since each rater always gets one\nstory from each model (and one human), we can ex-\npect that each will be rating a diverse set of stories\ncovering a wide range of ability levels, which helps\nthe marking process as it allows for comparative\nanalysis between various performances, enabling\nmore accurate pinpointing of each story’s quality.\nStories were assigned random identifiers before\nsending them to raters, so that the process was\nblind: to avoid biases, raters knew that they would\nbe evaluating human and AI-generated stories, but\nwere unaware of the origin of each story.\nRaters were sent all stories at once and they\nwere free to go back and change the ratings of\npreviously-rated stories. In addition, all of them\nwere experienced assessors in terms of Creative\nWriting texts, with previous experience in applying\nthe scale. These precautions mitigate the need for\nspecific calibration (Karpinska et al., 2021) that\nwould strain our resources.\n4 Results\n4.1 Agreement\nTo gauge the reliability of our results, we com-\npute inter-rater agreement between the two rat-\nings given to each story for each individual rubric\nitem. We use linearly weighted Cohen’s kappa\n(Cohen, 1968), which is appropriate for ordinal\nscales like ours, obtaining a value of 0.48, 95%\nCI [0.43,0.54]. This is interpreted as “moderate\nagreement”, which is a positive result taking into\naccount the obvious subjectivity involved in rating\nstories. If we instead focus on overall scores (sums\nof rubric items), the Pearson correlation between\nthe scores given to each story by each group of\nraters is 0.58 (p <0.00001), again indicating a\nreasonable degree of consistency between raters\ngiven the subjectivity of the task.\n4.2 General overview\nTable 2 shows a comprehensive overview of the\nratings that each of the LLMs (and humans) ob-\ntained for each rubric item, as well as in terms of\noverall score. Additionally, a box-and-whisker plot\ncomparing overall score can be seen in Figure 1.\nChatGPT with GPT-4 generates the best-rated\nstories, both in terms of overall score and in 8 out\nof 10 of the individual rubric categories. However,\nhuman writers are rated best in terms of originality\n(rubric item 5), and Claude was rated best in the\nuse of dark humor (rubric item 10), with humans a\nclose second. GPT-4 is also remarkably consistent,\nshowing low standard deviations not only with re-\nspect to human writers (which is expected, as our\nhuman stories were authored by five different hu-\nmans, whose skill levels may vary) but also with\nrespect to the rest of the LLMs.\nIf we compare LLMs to each other, the best\nperformances correspond to commercial offerings,\nincluding (apart from the aforementioned GPT-4)\nClaude, Bing Chat and the GPT-3.5 version of\nChatGPT. Open-source models are clearly behind,\nwith the best (Koala) achieving 60.0 overall score,\ncontrasting with the 80.2 obtained by GPT-4. Al-\nthough the best-performing LLMs are generally\nbetter across the board, some idiosyncrasies can be\nobserved: e.g., GPT-4 tops almost all rubric items\n14509\nbut is outperformed by two LLMs at humor.\nWhen we compare LLMs to human writers, sig-\nnificance testing on overall score (2-tailed t-test\nassuming unequal variances) fails to detect signif-\nicant differences between humans and the top 6\nAI models with α = 0.05. Only the 6 bottom AI\nmodels are significantly worse than humans at this\nsignificance level. Note, however, that the test has\na low statistical power due to the small sample size\n(10 ratings per model). If we instead perform a test\non individual metrics, so our sample size is 100\n(with the null hypothesis being no difference be-\ntween humans and each LLM in random individual\nmetric scores), then GPT-4 is identified as signifi-\ncantly better than the human writers (p= 0.00031),\nClaude and Bing’s scores are not significantly dif-\nferent from those of humans, and all the rest of the\nLLMs score significantly worse than humans.\nLooking at individual metric scores, structural\nelements (rubric item 3) are the easiest category\n(with an average rating across all stories of 7.3,\nand all models but one obtaining at least a 5 on\naverage). Humor (rubric item 10) is clearly the\nhardest, with an average score of 3.4, and we will\nanalyze it in more detail below. Incorporating John\nKennedy Toole’s style is the second hardest, with\n4.7. Comparing humans to LLMs, humans (as al-\nready mentioned) excel at originality and humor,\nbut are clearly behind the best LLMs in terms of\nreadability (item 1), where they are outperformed\nby 6 LLMs, and even more so in use of the epic\ngenre (item 7), where they score 4.9 and are out-\nperformed by 8 LLMs.\nWe now analyze in more detail some of the in-\ndividual items that show more interesting compar-\nisons between human writers and LLMs.\n4.3 Humor\nFigure 2 shows a box plot that complements the\ninformation on Table 2 for the humor rubric item.\nThe results for this item have two interesting char-\nacteristics. Firstly, it is clearly the most difficult\nrubric item, with an average score across models\nof 3.4, and the best obtaining 6.5. Even humans\nobtain a lower score in humor than in most items,\nwhich may be a consequence of humor being highly\nsubjective. Secondly, as evidenced both in the ta-\nble and plot, there is a rather stark binary divide\nbetween the contenders that “get” humor and those\nthat do not: Claude, Bing and GPT-4, together with\nthe human writers, obtain average scores between\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nHumor  rating\nComparison on dark humor rating\nMean\nFigure 2: Box plot comparing humor ratings for stories\ngenerated by humans and 12 LLMs, sorted left to right\nby mean overall rating. Notation as in Figure 1.\n6 and 6.5; whereas the rest of the models achieve\nvery low scores of 3.4 or less. Significance testing\nalso confirms this divide: despite the small sample\nsize of 10 humor ratings per model, a 2-tailed t-test\nwith α= 0.05 confirms that the models in the sec-\nond group are significantly worse than the human\nwriters, as well as the LLMs in the first group. This\nsuggests that grasping human humor might be an\nemergent ability of larger LLMs.\nIn this respect, a recent preprint (Jentzsch and\nKersting, 2023) concluded that ChatGPT has “a\nlimited reflection of humor” and “cannot yet confi-\ndently create intentionally funny original content”.\nThis study used the GPT 3.5 version of ChatGPT,\nso it is in line with our results (in which that model\nobtains an average humor score of 3.3). However,\nas we have seen, more powerful LLMs have over-\ncome that limitation, as their generated stories are\nclearly rated as humorous.\n4.4 Creativity\nWe now focus on rubric item 5, which rates creativ-\nity and originality, as it is a hallmark of creative\nwriting and also the only category where human\nwriters have outperformed all the LLMs in our anal-\nysis. Figure 3 shows a box plot that complements\nthe information on Table 2.\nThe same three LLMs that stood out in the humor\ncategory are also the best in terms of creativity,\nalthough the difference is not as stark. Regardless,\na t-test still distinguishes both groups as it shows all\n14510\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nCreativity  rating\nComparison on creativity rating\nMean\nFigure 3: Box plot comparing creativity ratings for sto-\nries generated by humans and 12 LLMs, sorted left to\nright by mean overall rating. Notation as in Figure 1.\nthe rest of the LLMs to be rated as significantly less\ncreative than our human writers, while for these\nthree we cannot reject the null hypothesis that they\nare as original as the human writers.\nOverall, from our results and in terms of human\nperception of the output, the answer to whether\nLLMs can produce creative stories (Franceschelli\nand Musolesi, 2023) is yes, although humans still\nretain an edge in this respect.\n4.5 Epicness\nFinally, we analyze rubric item 7 (understanding\nand habitation of the epic genre) for the opposite\nreason as in the previous section: it is the item\nwhere humans do worst compared to LLMs (see\nTable 2). A box plot is provided in Figure 4.\nIn this case, the results have a more atypical\nprofile, with substantial difference with respect to\noverall scores. Two models perform significantly\nbetter than the human writers (α= 0.05): both ver-\nsions of ChatGPT. Other six models obtain better\naverage rating than humans, but the difference is\nnot detected as significant.\nInterestingly, Bing clearly lags behind both Chat-\nGPT versions, despite being based in GPT-4. This\nmight be related to bias introduced by the system’s\ncensorship. On the other hand, some models whose\noverall scores are in the bottom half (OpenAssis-\ntant, GPT4All) are reasonably good at epic narra-\ntion, outperforming humans and Bing (which are\nbetter than them in almost all categories).\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nEpicness  rating\nComparison on epicness rating\nMean\nFigure 4: Box plot comparing epicness ratings for sto-\nries generated by humans and 12 LLMs, sorted left to\nright by mean overall rating. Notation as in Figure 1.\n5 Discussion\nWe have evaluated recent LLMs on a creative writ-\ning task in English, using a carefully-designed sce-\nnario to provide a demanding challenge and avoid\nconfounding factors like training data memoriza-\ntion (Carlini et al., 2023). To our knowledge, this is\nthe most thorough evaluation of LLMs on creative\nwriting conducted so far, both in terms of scope\n(12 LLMs considered, plus comparison to human\nwriters) and detail (using human evaluation with a\n10-item rubric based on established creative writing\nevaluation practices).\nSimultaneously to our work, the recent preprint\nby Chakrabarty et al. (2023) provides an evaluation\nof three of the top-performing commercial LLMs\n(ChatGPT, GPT-4 and Claude) for creative writing.\nThis approach is close to ours, as it uses the models\nin a zero-shot setting and evaluation is performed\nby humans using a specific rubric. However, there\nare important methodological differences between\nboth studies, which we summarize here:\n1. The human stories used by Chakrabarty et al.\n(2023) are stories published in the New Yorker,\nby highly successful authors (including Nobel\nprize winners), whereas ours are written by\nCreative Writing students.\n2. In their setting, the human-written stories are\npre-existing (and selected for publication in\nthe New Yorker, as mentioned above) so their\n14511\nwriters were unconstrained when they created\nthem, while the LLMs have to adapt to write\nan alternative story with the same plot. In\nours, humans and LLMs are given the exact\nsame prompt to work with.\n3. In terms of length, the stories they work with\nare over thrice larger than ours on average.\nIn addition, while both studies try to make\nsentence lengths similar between humans and\nLLMs, in their case the human writers origi-\nnally wrote their stories unconstrained (or un-\nder loose constraints) and the LLM-generated\nstories were calibrated to have similar lengths\nby an iterative prompting process. In our\ncase, the LLMs were unconstrained in terms\nof length, and the human writers were sug-\ngested to target a length range loosely similar\nto LLM-generated stories. Thus, with respect\nto theirs, our approach has the disadvantage\nof a looser control on story length, but the\nadvantage of using a single zero-shot prompt.\n4. Their study spans a variety of story prompts,\nwhile we focus on a single prompt and set-\nting. The flip side is that our rubric can be\nadapted to specific requirements like humor\nand Toole style, whereas theirs is necessarily\nmore generic. In addition, our narrower fo-\ncus allows us to have LLMs generate several\nalternative stories, so we can perform more\nstatistical analysis: we consider the distribu-\ntion within each LLM and perform statistical\ntesting, which cannot be done in Chakrabarty\net al. (2023)’s setting as they generate a single\nstory per prompt and LLM.\n5. Since their study is based on existing stories\nthat are published online, there is the possi-\nbility that some are contained in the tested\nLLMs’ training data. In our case, we designed\nthe study to prevent training data reuse.\n6. The rubrics are different: Chakrabarty et al.\n(2023) use a rubric based on the Torrance tests\nof creative thinking (Torrance, 1974).\nThe outcome of this study is substantially differ-\nent from ours, with LLM-generated stories rated\nclearly behind human-authored ones. This is not\nsurprising considering the methodological differ-\nences: in particular, differences 1 and 2 in the list\nabove clearly set a higher bar for LLMs, as they\nare compared to highly successful human stories\nby top authors that wrote freely and the LLMs are\nasked to adapt to their plots. We hypothesize that\nthese are the main reasons for the difference in out-\ncome. On the other hand, item 5 in the list above\ncould in principle benefit LLMs, and there are other\nfactors that could benefit humans or LLMs in non-\nobvious ways (including items 3, 4 and 6, as well\nas different story genres and target lengths). This\nunderscores the need of more studies in this area.\n6 Conclusion\nThe results show that state-of-the-art LLMs can\nperform a creative writing task at a very competent\nlevel, with the top two (ChatGPT with GPT-4 and\nClaude) achieving high scores that outperform hu-\nman writers in most rubric categories. While we\nmust be careful not to take this as evidence of “su-\nperhuman storytelling” (both because our sample\nsize is not enough to draw such categorical con-\nclusions, and because our 5 human writers are not\nnecessarily representative of human writing abil-\nity as a whole); it does at least strongly suggest\nthat these models’ stories are not distinguishably\nworse than those by reasonably-trained humans.\nThis is even more remarkable given that we did not\nuse any in-context learning or other techniques to\noptimize the LLMs for the task, but just a straight-\nforward prompt from a fresh state, so it is possible\nthat even better results are achievable with careful\nprompting.\nOur analysis also shows that the best results are\nachieved by commercial LLMs, with open-source\nmodels clearly lagging behind at the moment.\nLooking at individual characteristics, humans\nretain the lead in originality, while LLMs tend to\nexcel in more technical aspects like readability or\nstructure. Humor is an especially challenging as-\npects where most LLMs utterly fail, but the best\nthree models do succeed at achieving human-like\nratings, contrasting with results on older LLMs\nthat showed their lack of grasp of human hu-\nmor (Jentzsch and Kersting, 2023).\nInteresting avenues for future work include eval-\nuation of different literary genres, languages other\nthan English, and studying whether the quality of\nthe generated stories can be improved with prompt\nengineering or fine-tuning.\nSelected stories from our corpus (available at\nhttps://doi.org/10.5281/zenodo.8435671,\ntogether with all rating data) are in Appendix E.\n14512\nLimitations\nCommercial LLMs and reproducibilityWhile\nsome of the LLMs considered are proper scien-\ntific artifacts, trained with a documented method-\nology and whose code and weights are available,\nothers are closed commercial products and there\nis little public information about them, hindering\nreproducibility. While we have reported version\nnumbers (where available) and access dates are\nprovided in Appendix A, apart from publishing the\ngenerated outputs so that the rating process is repro-\nducible, the prompting/generation process may not\nbe reproducible in the future for these models as\nsome of these products are updated without notice,\nand without providing access to previous versions.\nHowever, we believe that including commercial\nmodels is valuable, as they are widely considered\nto provide the best quality results at the time of\nwriting (which has been confirmed by our analy-\nsis), and these data points can still be used as a\nmeasuring stick against which to compare open\nmodels in the present and future.\nLimitations of the analysis Rating creative writ-\ning is necessarily a highly subjective process. Fur-\nthermore, since our raters were volunteers, we did\nnot ask each of them to mark the full 65 stories in\nthe corpus but just a subset, so our sample size is\nlimited. We have provided the necessary details\nso that the reader can assess the variability of the\ndata (sample sizes, standard deviations, and inter-\nrater agreement, which is reasonably high given the\nsubjectivity of the task); and we have been careful\nnot to make overarching claims. In this respect,\nwe have also taken into account that our sample\nof human writers cannot be assumed to be repre-\nsentative of “human creative writing ability” as a\nwhole, but is only provided as a reference point\nof interest; and that our evaluation is focused on\na specific genre, so claims of the form “LLMs are\nbetter/equal/worse than humans at creative writing”\ncannot be made with an evaluation like ours.\nScope Our analysis focuses on a specific genre,\nand on English language, so the results do not nec-\nessarily generalize to other genres and/or languages.\nHowever, conducting a wider evaluation in this re-\nspect would not be possible with our resources, so\nwe chose to fix these variables and focus on con-\nducting a detailed evaluation on a large number of\nLLMs instead.\nEthics Statement\nWhile the use of conversational LLMs has raised\nvarious ethical challenges, creative writing has\nbeen argued to be one of the best uses for these\ntools from a human-centered AI point of view, as\nlong as AI-generated stories are identified as such\nto avoid misleading readers or publishers (Sison\net al., 2023). In our study, raters were blinded to\nstory authorship but they were previously informed\nthat they would be dealing with AI and human-\ngenerated stories. In the published corpus, each\nstory is identified as human or AI-authored.\nAll participants in the evaluation (as raters or\nwriters) were volunteers, and the demand on their\ntime was kept accordingly low.\nAcknowledgments\nThe first author was funded by the Euro-\npean Research Council (ERC), under the Hori-\nzon Europe research and innovation programme\n(SALSA, grant agreement No 101100615),\nERDF/MICINN-AEI (SCANNER-UDC, PID2020-\n113230RB-C21), Xunta de Galicia (ED431C\n2020/11), and Centro de Investigación de Gali-\ncia “CITIC”, funded by the Xunta de Galicia\nthrough the collaboration agreement between the\nConsellería de Cultura, Educación, Formación Pro-\nfesional e Universidades and the Galician universi-\nties for the reinforcement of the research centres of\nthe Galician University System (CIGUS).\nWe thank Olga Zamaraeva for comments on pre-\nliminary versions of this work, and two anonymous\nreviewers for their helpful comments. Last, but not\nleast, we thank our volunteers who participated in\nthe writing and grading of stories, in alphabetical\norder: Jayda Franks, Bree Glasbergen, Ola Kwin-\ntowski, Jay Ludowyke, Kyle Mackenzie, Kirsty\nMaclachlan, Caitlin Noakes, Rachelle Raco, Kylie\nRyan and Josephine Stewart. Credit for each indi-\nvidual story can be found in the corpus.\nReferences\nYuvanesh Anand, Zack Nussbaum, Brandon Duder-\nstadt, Benjamin M. Schmidt, and Andriy Mulyar.\n2023a. GPT4All: Training an assistant-style chatbot\nwith large-scale data distillation from GPT-3.5-Turbo.\nTechnical report.\nYuvanesh Anand, Zack Nussbaum, Brandon Duder-\nstadt, Benjamin M. Schmidt, Adam Treat, and Andriy\nMulyar. 2023b. GPT4All-J: An Apache-2 licensed\nassistant-style chatbot. Technical report.\n14513\nLorin W. Anderson and David R. Krathwohl, editors.\n2001. A Taxonomy for Learning, Teaching, and As-\nsessing. A Revision of Bloom’s Taxonomy of Educa-\ntional Objectives, 2 edition. Allyn & Bacon, New\nYork.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert\nLasenby, Robin Larson, Sam Ringer, Scott John-\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. 2022. Constitutional AI: Harmlessness\nfrom AI feedback. Technical report.\nAnja Belz and Ehud Reiter. 2006. Comparing auto-\nmatic and human evaluation of NLG systems. In\n11th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 313–\n320, Trento, Italy. Association for Computational\nLinguistics.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023. Pythia:\nA suite for analyzing large language models across\ntraining and scaling. Technical report.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMichael D Carey, Shelley Davidow, and Paul Williams.\n2022. Re-imagining narrative writing and assess-\nment: a post-naplan craft-based rubric for creative\nwriting. The Australian Journal of Language and\nLiteracy, 45(1):33–48.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramèr, and Chiyuan Zhang.\n2023. Quantifying memorization across neural lan-\nguage models. In International Conference on Learn-\ning Representations (ICLR).\nTuhin Chakrabarty, Philippe Laban, Divyansh Agar-\nwal, Smaranda Muresan, and Chien-Sheng Wu. 2023.\nArt or artifice? large language models and the false\npromise of creativity.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing GPT-4 with 90%* Chat-\nGPT quality. Technical report.\nJohn Joon Young Chung, Wooseok Kim, Kang Min\nYoo, Hwaran Lee, Eytan Adar, and Minsuk Chang.\n2022. Talebrush: Sketching stories with generative\npretrained language models. In Proceedings of the\n2022 CHI Conference on Human Factors in Comput-\ning Systems, CHI ’22, New York, NY , USA. Associa-\ntion for Computing Machinery.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282–7296, Online.\nAssociation for Computational Linguistics.\nJacob Cohen. 1968. Weighted kappa: Nominal scale\nagreement provision for scaled disagreement or par-\ntial credit. Psychological Bulletin, 70(4):213–220.\nShelley Davidow and Paul Williams. 2016. Play-\ning With Words: A Introduction to Creative Craft.\nBloomsbury Academic.\nAnnie Dillard. 1981. Contemporary prose styles. Twen-\ntieth Century Literature, 27:207–222.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey on in-context learning.\nGiorgio Franceschelli and Mirco Musolesi. 2023. On\nthe creativity of large language models.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The Pile: An\n800GB dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nEduardo C. Garrido-Merchán, José Luis Arroyo-\nBarrigüete, and Roberto Gozalo-Brihuela. 2023.\nSimulating H.P. Lovecraft horror literature with the\nChatGPT large language model.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023. Koala: A dialogue model for academic re-\nsearch. Blog post.\n14514\nAidan Gilson, Conrad W Safranek, Thomas Huang,\nVimig Socrates, Ling Chi, Richard Andrew Taylor,\nand David Chartash. 2023. How does chatgpt per-\nform on the united states medical licensing examina-\ntion? the implications of large language models for\nmedical education and knowledge assessment. JMIR\nMed Educ, 9:e45312.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A Knowledge-Enhanced Pre-\ntraining Model for Commonsense Story Generation.\nTransactions of the Association for Computational\nLinguistics, 8:93–108.\nJian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wen-\nbiao Ding, and Minlie Huang. 2021. Long text gener-\nation by modeling sentence-level and discourse-level\ncoherence. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6379–6393, Online. Association for Computa-\ntional Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR).\nSophie Jentzsch and Kristian Kersting. 2023. Chatgpt\nis fun, but it is not funny! humor is still challenging\nlarge language models.\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator? yes with gpt-4 as the engine.\nMarzena Karpinska, Nader Akoury, and Mohit Iyyer.\n2021. The perils of using Mechanical Turk to evalu-\nate open-ended text generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1265–1285, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJeri Kroll. 1997. A or C: Can we assess creative work\nfairly? TEXT, 1(1):1–5.\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver\nStanley, Richárd Nagyfi, Shahul ES, Sameer Suri,\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire,\nChristoph Schuhmann, Huu Nguyen, and Alexan-\nder Mattick. 2023. OpenAssistant Conversations –\ndemocratizing large language model alignment.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim,\nNeel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models.\nPiotr Mirowski, Kory W. Mathewson, Jaylen Pittman,\nand Richard Evans. 2023. Co-writing screenplays\nand theatre scripts with language models: Evaluation\nby industry professionals. In Proceedings of the 2023\nCHI Conference on Human Factors in Computing\nSystems, CHI ’23, New York, NY , USA. Association\nfor Computing Machinery.\nS. Norris. 2013. Studying Creative Writing. Creative\nWriting Studies. Frontinus Limited.\nMaxwell Nye, Michael Henry Tessler, Joshua B. Tenen-\nbaum, and Brenden M. Lake. 2021. Improving co-\nherence and consistency in neural sequence models\nwith dual-system, neuro-symbolic reasoning. In Ad-\nvances in Neural Information Processing Systems 34\n- 35th Conference on Neural Information Processing\nSystems, NeurIPS 2021, Advances in Neural Informa-\ntion Processing Systems, pages 25192–25204. Neural\ninformation processing systems foundation.\nOpenAI. 2023. Gpt-4 technical report. Technical report.\nGeorge Orwell. 1946. Politics and the English language.\nHorizon, 13:252–265.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nLes Perelman. 2018. Towards a new NAPLAN: Testing\nto the teaching. Journal of Professional Learning, 2.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\n14515\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nPiotr Sawicki, Marek Grzes, Fabricio Goes, Dan Brown,\nMax Peeperkorn, and Aisha Khatun. 2023. Bits of\ngrass: Does gpt already know how to write like Whit-\nman?\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D. Manning. 2019. Do\nmassively pretrained language models make better\nstorytellers? In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 843–861, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nAlejo Jose G. Sison, Marco Tulio Daza, Roberto Gozalo-\nBrizuela, and Eduardo C. Garrido-Merchán. 2023.\nChatgpt: More than a weapon of mass deception,\nethical challenges and responses from the human-\ncentered artificial intelligence (hcai) perspective.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane,\nAnantharaman S. Iyer, Anders Andreassen, Andrea\nMadotto, Andrea Santilli, Andreas Stuhlmüller, An-\ndrew Dai, Andrew La, Andrew Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong,\nAnimesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-\nsum, Arul Menezes, Arun Kirubarajan, Asher Mul-\nlokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka¸ s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,\nBatuhan Özyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk Ek-\nmekci, Bill Yuchen Lin, Blake Howald, Cameron\nDiao, Cameron Dour, Catherine Stinson, Cedrick Ar-\ngueta, César Ferri Ramírez, Chandan Singh, Charles\nRathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu,\nChris Callison-Burch, Chris Waites, Christian V oigt,\nChristopher D. Manning, Christopher Potts, Cindy\nRamirez, Clara E. Rivera, Clemencia Siro, Colin Raf-\nfel, Courtney Ashcraft, Cristina Garbacea, Damien\nSileo, Dan Garrette, Dan Hendrycks, Dan Kilman,\nDan Roth, Daniel Freeman, Daniel Khashabi, Daniel\nLevy, Daniel Moseguí González, Danielle Perszyk,\nDanny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, David Drakard, David Ju-\nrgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek Tam,\nDieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dim-\nitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekate-\nrina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-\ngefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,\nFatemeh Siar, Fernando Martínez-Plumed, Francesca\nHappé, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nmán Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nLópez, Gregor Betz, Guy Gur-Ari, Hana Galijase-\nvic, Hannah Kim, Hannah Rashkin, Hannaneh Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Schütze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B. Simon,\nJames Koppel, James Zheng, James Zou, Jan Ko-\nco´n, Jana Thompson, Jared Kaplan, Jarema Radom,\nJascha Sohl-Dickstein, Jason Phang, Jason Wei, Ja-\nson Yosinski, Jekaterina Novikova, Jelle Bosscher,\nJennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse En-\ngel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jil-\nlian Tang, Joan Waweru, John Burden, John Miller,\nJohn U. Balis, Jonathan Berant, Jörg Frohberg, Jos\nRozen, Jose Hernandez-Orallo, Joseph Boudeman,\nJoseph Jones, Joshua B. Tenenbaum, Joshua S. Rule,\nJoyce Chua, Kamil Kanclerz, Karen Livescu, Karl\nKrauth, Karthik Gopalakrishnan, Katerina Ignatyeva,\nKatja Markert, Kaustubh D. Dhole, Kevin Gim-\npel, Kevin Omondi, Kory Mathewson, Kristen Chi-\nafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Colón, Luke Metz, Lütfi Kerem\n¸ Senel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramírez Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L. Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael A. Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛ edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mo-\nhit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh\nGheini, Mukund Varma T, Nanyun Peng, Nathan\n14516\nChi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas\nCameron, Nicholas Roberts, Nick Doiron, Nikita\nNangia, Niklas Deckers, Niklas Muennighoff, Ni-\ntish Shirish Keskar, Niveditha S. Iyer, Noah Con-\nstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar\nAgha, Omar Elbaghdadi, Omer Levy, Owain Evans,\nPablo Antonio Moreno Casares, Parth Doshi, Pascale\nFung, Paul Pu Liang, Paul Vicol, Pegah Alipoormo-\nlabashi, Peiyuan Liao, Percy Liang, Peter Chang,\nPeter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr\nMiłkowski, Piyush Patil, Pouya Pezeshkpour, Priti\nOli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin\nBanjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel\nHabacker, Ramón Risco Delgado, Raphaël Millière,\nRhythm Garg, Richard Barnes, Rif A. Saurous, Riku\nArakawa, Robbe Raymaekers, Robert Frank, Rohan\nSikand, Roman Novak, Roman Sitelew, Ronan Le-\nBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Rus-\nlan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Sto-\nvall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M.\nMohammad, Sajant Anand, Sam Dillavou, Sam\nShleifer, Sam Wiseman, Samuel Gruetter, Samuel R.\nBowman, Samuel S. Schoenholz, Sanghyun Han,\nSanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian,\nSayan Ghosh, Sean Casey, Sebastian Bischoff, Sebas-\ntian Gehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima, Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,\nSpencer Torene, Sriharsha Hatwar, Stanislas De-\nhaene, Stefan Divic, Stefano Ermon, Stella Bider-\nman, Stephanie Lin, Stephen Prasad, Steven T. Pi-\nantadosi, Stuart M. Shieber, Summer Misherghi, Svet-\nlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal\nSchuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,\nTe-Lin Wu, Théo Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili, Timo\nSchick, Timofei Kornev, Timothy Telleen-Lawton,\nTitus Tunduny, Tobias Gerstenberg, Trenton Chang,\nTrishala Neeraj, Tushar Khot, Tyler Shultz, Uri Sha-\nham, Vedant Misra, Vera Demberg, Victoria Nyamai,\nVikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu,\nVishakh Padmakumar, Vivek Srikumar, William Fe-\ndus, William Saunders, William Zhang, Wout V ossen,\nXiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu,\nXudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz,\nYangqiu Song, Yasaman Bahri, Yejin Choi, Yichi\nYang, Yiding Hao, Yifu Chen, Yonatan Belinkov,\nYu Hou, Yufang Hou, Yuntao Bai, Zachary Seid,\nZhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui\nWang, and Ziyi Wu. 2022. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models.\nW. Strunk and E.B. White. 2008[1918]. The Elements\nof Style. BN Publishing, New York.\nBen Swanson, Kory Mathewson, Ben Pietrzak, Sherol\nChen, and Monica Dinalescu. 2021. Story centaur:\nLarge language model few shot learning as a cre-\native writing tool. In Proceedings of the 16th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: System Demonstrations,\npages 244–256, Online. Association for Computa-\ntional Linguistics.\nBowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric\nXing, and Zhiting Hu. 2021. Progressive generation\nof long text with pretrained language models. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4313–4324, Online. Association for Computational\nLinguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford Alpaca:\nAn instruction-following LLaMA model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe. 2022. Lamda: Language models for dialog appli-\ncations.\nE.P. Torrance. 1974. Torrance Tests of Creative Think-\ning: Verbal Tests, Forms A and B, Figural Tests,\nForms A and B. Norms-technical manual. Xerox.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand efficient foundation language models.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 billion parameter autoregressive lan-\nguage model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-Instruct: Aligning language\nmodel with self generated instructions.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\n14517\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nBeck Wise and Ariella van Luyn. 2020. Not ‘all writing\nis creative writing’ and that’s ok: inter/disciplinary\ncollaboration in writing and writing studies. TEXT,\n24(Special 59):1–15.\nZhuohan Xie, Trevor Cohn, and Jey Han Lau. 2023.\nCan very large pretrained language models learn sto-\nrytelling with a few examples?\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul\nPuri, Pascale Fung, Anima Anandkumar, and Bryan\nCatanzaro. 2020. MEGATRON-CNTRL: Control-\nlable story generation with external knowledge using\nlarge-scale language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2831–2845,\nOnline. Association for Computational Linguistics.\nAnn Yuan, Andy Coenen, Emily Reif, and Daphne Ip-\npolito. 2022. Wordcraft: Story writing with large\nlanguage models. In 27th International Conference\non Intelligent User Interfaces, IUI ’22, page 841–852,\nNew York, NY , USA. Association for Computing Ma-\nchinery.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models.\nA Model access dates\nTable 3 shows the date in which the stories were\ngenerated for each of the models. For future ex-\nperimental reference, we highlight that the initial\npublic disclosure of this paper online occurred on\n2023-10-09. Before this date, only the human au-\nthors and raters were aware of the project from May\n2023, and anonymous reviewers had access from\nJune 23, 2023. Consequently, LLMs with a knowl-\nedge cutoff prior to 2023-10-09 are likely to have\nno or minimal risk of training set contamination.\nB Hyperparameters\nWe did not tweak any hyperparameters of the mod-\nels. In the case of commercial models, we just ran\nthe model as it is presented in their respective web\nuser interfaces, except in the case of Bing Chat\nwhere we chose Creative mode. For open-source\nmodels, we used the default parameters from the\nweb UI provided at https://chat.lmsys.org/,\nwhich set temperature to 0.7.\nModel Access date\nalpaca 2023-04-07\nbard 2023-04-11\nbing 2023-04-11\nchatgpt-gpt35 2023-04-11\nchatgpt-gpt4 2023-04-14\nclaude12 2023-04-04\ndolly 2023-04-14\ngpt4all-j 2023-04-14\nkoala 2023-04-07\noa 2023-04-16\nstablelm 2023-04-20\nvicuna 2023-04-07\nhumans 2023-05-01 to 2023-05-12\nTable 3: Access dates for each model (and dates of writ-\ning for the human stories), in YYYY-MM-DD format.\nC Detailed rubric information\nThe creative writing rubric was designed for as-\nsessment of creative writing scripts in university\ncreative writing courses in order to evaluate these\nabove competencies, criteria 1-5 to measure gen-\neral creative writing capacities, and criteria 6-10\nto measure specific task related proficiency. Each\nof the ten criteria is awarded 10 points out of a\ntotal 100 points. The rubric has been specifically\ndesigned to measure the quality of writing craft and\nto avoid formulaic, rule-based writing.\n1. Overall/ holistic/ cohesive readability of the\nstory (not just a compilation of elements).\n2. Use of key narrative elements - vocabulary\nchoice, imagery, setting, themes, dialogue,\ncharacterisation, point of view.\n3. Structural elements and presentation which\nreflects the control of structural elements such\nas spelling, grammar, punctuation, paragraph-\ning, and formatting\n4. Overall plot logic: hook, conflict, initial crisis,\nrising and falling action, denouement/ resolu-\ntion (Freitag’s pyramid)\n5. Creativity/innovation/originality/ re-\nsearch—credibility, new knowledge,\navoidance of cliché and derivative tropes\n6. Incorporation of the John Kennedy Toole style\nof writing using the indicators/ characteristics\nlisted below\n14518\n7. Understanding and habitation of the epic\ngenre of heroic/legendary adventure\n8. Description and credibility of a single combat\nscene\n9. Accurate inclusion of two main characters Ig-\nnatius J. Reilly and a pterodactyl in action and\ndescription (see below for character descrip-\ntion)\n10. Use of a characteristically dark humorous\ntone.\nThe 1-10 scale is divided into three ranges:\n• Emerging (1-4): stories in this range demon-\nstrate an early grasp of storytelling elements,\nbut falter in execution or depth. When evaluat-\ning humans, they correspond to novice writers\nwho need feedback and guidance to improve\nthe story.\n• Competent (5-8): stories that showcase a good\ngrasp of the storytelling principle being eval-\nuated (coherent plot, well-defined characters,\netc.). While there might be room for im-\nprovement, these stories effectively engage\nthe reader and convey their intended mes-\nsages.\n• Sophisticated (9-10): these stories exhibit ex-\nceptional mastery of the aspect being evalu-\nated, resulting in a compelling and memorable\nread.\nToole style We provided raters with de-\ntailed information about the plot, setting,\nimagery, tone, characters, main protago-\nnist, and derivative/imitative style of the\nauthor, taken from a generic and popular\nstudy guide ( http://www.bookrags.com/\nstudyguide-a-confederacy-of-dunces/#gsc.\ntab=0).\nD Box plots for each individual rubric\nitem\nFigures 5 to 14 show the box plots summarizing\nthe results for all rubric items, including those plots\nnot featured in the main text.\nE Sample stories\nWe show in this section several sample stories\nfrom the corpus, chosen according to rating: the\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nCohesion  rating\nComparison on cohesion rating\nMean\nFigure 5: Box plot comparing rubric item 1 (cohesion)\nfor stories generated by humans and 12 LLMs, sorted\nleft to right by mean overall rating. Notation as in Figure\n1.\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nKey  narrative elements rating\nComparison on k ey narrative elements rating\nMean\nFigure 6: Box plot comparing rubric item 2 (key nar-\nrative elements) for stories generated by humans and\n12 LLMs, sorted left to right by mean overall rating.\nNotation as in Figure 1.\n14519\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nStructural  elements rating\nComparison on structural elements rating\nMean\nFigure 7: Box plot comparing rubric item 3 (structural\nelements) for stories generated by humans and 12 LLMs,\nsorted left to right by mean overall rating. Notation as\nin Figure 1.\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nPlot  logic rating\nComparison on plot logic rating\nMean\nFigure 8: Box plot comparing rubric item 4 (plot logic)\nfor stories generated by humans and 12 LLMs, sorted\nleft to right by mean overall rating. Notation as in Figure\n1.\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nCreativity  rating\nComparison on creativity rating\nMean\nFigure 9: Box plot comparing rubric item 5 (creativity)\nfor stories generated by humans and 12 LLMs, sorted\nleft to right by mean overall rating. Notation as in Figure\n1.\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nk oala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nJK T  style rating\nComparison on John K ennedy T oole style rating\nMean\nFigure 10: Box plot comparing rubric item 6 (John\nKennedy Toole style) for stories generated by humans\nand 12 LLMs, sorted left to right by mean overall rating.\nNotation as in Figure 1.\n14520\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nEpicness  rating\nComparison on epicness rating\nMean\nFigure 11: Box plot comparing rubric item 7 (epic\ngenre) for stories generated by humans and 12 LLMs,\nsorted left to right by mean overall rating. Notation as\nin Figure 1.\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nCombat  rating\nComparison on combat rating\nMean\nFigure 12: Box plot comparing rubric item 8 (combat\ndescription) for stories generated by humans and 12\nLLMs, sorted left to right by mean overall rating. Nota-\ntion as in Figure 1.\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nCharacters  rating\nComparison on character accuracy rating\nMean\nFigure 13: Box plot comparing rubric item 9 (accuracy\nof characters) for stories generated by humans and 12\nLLMs, sorted left to right by mean overall rating. Nota-\ntion as in Figure 1.\nchatgpt-gpt4\nclaude12\nhuman\nbing\nchatgpt-gpt35\nkoala\nvicuna\noa\nbard\ngpt4all-j\nstablelm\ndolly\nalpaca\nModel\n0\n2\n4\n6\n8\n10\nHumor  rating\nComparison on dark humor rating\nMean\nFigure 14: Box plot comparing rubric item 10 (dark\nhumor) for stories generated by humans and 12 LLMs,\nsorted left to right by mean overall rating. Notation as\nin Figure 1.\n14521\nthree top-rated stories, the best human-written story\n(which was also ranked fourth overall), the story\nranked in the median, and the worst-rated. It is\nworth noting that, as explained in Section 3.4, our\nmethodology is designed to provide fairness when\ncomparing models, but not individual stories, as dif-\nferent stories are assigned to different raters. Thus,\nselection of stories by rating is necessarily noisy.\nE.1 Overall best-rated story\nThis story was generated by GPT-4. The ratings\nfor this story are in Table 4.\nIt was a sweltering Louisiana afternoon, the air\nthick with the miasma of musty sweat and the scent\nof overripe vegetables. In the midst of the urban\njungle, resplendent with the cacophony of honking\ncars and the chatter of the common rabble, stood\na man who defied reason, common sense, and all\nnatural laws of human decency. Ignatius J. Reilly,\na behemoth of a man with an unruly mane of hair\nand a Byzantine beard, adjusted his ill-fitting, plaid\ntrousers and surveyed the scene with a look of lofty\ndisdain.\nSuddenly, the air crackled with an electric\ncharge, and a portal appeared before him. The very\nfabric of time and space seemed to rend asunder,\nand from this anachronistic rift emerged a ptero-\ndactyl - a creature so ancient that it had outlived\nthe vast expanse of geologic time, only to find it-\nself in the absurdity of the twentieth century. Its\nleathery wings stretched wide as the mists of time\ndissipated, and it let out an earsplitting screech that\nsent the common folk of New Orleans scattering\nlike roaches from the light.\nIgnatius, unphased by the sudden appearance of\nthe primordial beast, surveyed the creature through\nhis fogged-up monocle. \"Fortuna,\" he muttered,\nhis voice dripping with both sarcasm and hot dog\ncondiments, \"you spin your wheel with such capri-\ncious abandon. Have you not tired of hurling these\npreposterous challenges in my path?\"\nThe pterodactyl, with its beady, reptilian eyes,\nlocked its gaze upon Ignatius, its primitive brain\nsensing a formidable opponent in this lumbering,\nmalodorous mass of a man. It screeched again,\nflapping its wings in a display of both aggression\nand arrogance, its razor-sharp beak snapping at the\nair.\nIgnatius bellowed, raising his arms as if beseech-\ning the heavens, \"Very well, foul beast! If it is\nbattle you seek, you shall find no quarter here! I\nshall vanquish you as I have vanquished all who\nhave dared to challenge the indomitable will of\nIgnatius J. Reilly!\"\nWith a speed surprising for a man of his consid-\nerable girth, Ignatius tore a shopping cart from the\nhands of a terrified passerby and hurled it toward\nthe pterodactyl with the force of a thousand disgrun-\ntled welfare recipients. The makeshift projectile\nslammed into the creature, stunning it momentarily\nand allowing Ignatius to close the distance between\nthem.\nThe two combatants circled one another, their\neyes locked in a primal dance of predator and prey.\nThe pterodactyl, recovering from the initial assault,\nswooped low, talons outstretched to rend flesh from\nbone. Ignatius, with an agility that defied his rotund\nappearance, rolled to the side, avoiding the talons\nby mere inches.\nAs the battle raged, the streets of New Orleans\nbecame a stage for this epic confrontation between\nman and beast, between past and present. Ignatius,\narmed with nothing more than his wit and an un-\nwieldy umbrella, parried the relentless strikes of\nthe pterodactyl, determined to emerge victorious.\nAs the sun dipped low in the sky, casting a\nblood-red hue across the battlefield, the pterodactyl,\nweary from the long and grueling contest, faltered.\nSeizing the opportunity, Ignatius lunged forward,\nhis umbrella aimed squarely at the creature’s heart.\nThe point found its mark, and with a final, an-\nguished cry, the pterodactyl crumpled to the ground,\nits ancient spirit extinguished at last.\nIgnatius stood triumphant over the fallen beast,\nhis chest heaving with exertion and righteous in-\ndignation. He wiped the sweat from his brow with\na grease-stained handkerchief and addressed the\nawestruck crowd that had gathered to witness this\nextraordinary battle.\n\"Behold, citizens of this wretched cesspool,\" he\nproclaimed, gesturing grandly with his umbrella,\nnow slick with the blood of the vanquished foe.\n\"The forces of the prehistoric have met their match\nin the unwavering resolve of Ignatius J. Reilly! Let\nthis be a lesson to all who would dare challenge\nthe sanctity of my person and my valiant quest for\njustice in this world beset by fools and ne’er-do-\nwells.\"\nThe onlookers, mouths agape, gazed upon the\nfallen pterodactyl and the disheveled figure of Ig-\nnatius J. Reilly with a mixture of awe, terror, and\ndisbelief. They had borne witness to a battle of the\n14522\nRubric item 1 2 3 4 5 6 7 8 9 10 overall\nRater 1 8 8 9 8 7 7 8 8 7 6 76\nRater 2 10 10 9 9 8 9 10 10 10 10 95\nAverage 9 9 9 8.5 7.5 8 9 9 8.5 8 85.5\nTable 4: Ratings for the best overall rated story in the corpus, produced by ChatGPT with GPT-4.\nages, a contest that would be whispered about in\nthe dark corners of bars and the humid alleyways\nof New Orleans for generations to come.\nAs the sun set and the shadows grew long, Ig-\nnatius, his honor defended and his foe vanquished,\nstrode off into the gloaming, pausing only to collect\na discarded hot dog from the gutter as sustenance\nfor the journey home.\nAnd so, the legend of Ignatius J. Reilly,\npterodactyl-slayer and defender of truth, justice,\nand questionable hygiene, was etched into the an-\nnals of history, a testament to the indomitable spirit\nof man, and a reminder that even the most ancient\nand fearsome of adversaries could be vanquished\nby the sheer force of will and an unwieldy um-\nbrella.\nE.2 Second overall best-rated story\nThis story was generated by Bing Chat. The ratings\nfor this story are in Table 5.\nIgnatius J. Reilly was strolling along the\nAudubon Park, munching on a hot dog and ad-\nmiring the statues of Confederate heroes, when he\nheard a loud screech from above. He looked up\nand saw a huge winged creature swooping down on\nhim, its beak open and its claws extended. It was\na pterodactyl, a prehistoric monster that had some-\nhow escaped from the museum of natural history.\nIgnatius gasped and dropped his hot dog. He\nreached for his trusty hunting cap, which he always\nwore to protect his delicate ears from the noise of\nthe modern world. He pulled it down over his eyes\nand hoped that the pterodactyl would mistake him\nfor a statue or a bush.\nBut the pterodactyl was not fooled. It landed on\nthe ground with a thud and snapped its jaws at Ig-\nnatius, who dodged nimbly to the side. Ignatius felt\na surge of courage and indignation. How dare this\nbeast attack him, a refined scholar and a defender\nof the true faith? He decided to fight back with his\nsuperior intellect and wit.\nHe shouted at the pterodactyl: \"You foul crea-\nture! You are an abomination of nature, a relic\nof a barbaric age! You have no place in this civ-\nilized society, where men of letters and culture\nreign supreme! Begone, or I shall smite you with\nmy mighty pen!\"\nThe pterodactyl was not impressed by Ignatius’s\nwords. It screeched again and lunged at him, trying\nto bite his head off. Ignatius dodged again and ran\ntowards a nearby bench. He grabbed a newspaper\nthat someone had left behind and rolled it into a\ncone. He brandished it like a sword and charged at\nthe pterodactyl.\n\"Take that, you fiend!\" he cried, as he stabbed\nthe newspaper cone into the pterodactyl’s eye. The\npterodactyl shrieked in pain and flapped its wings\nfuriously, trying to shake off the paper weapon.\nIgnatius seized the opportunity and ran towards the\npark exit, hoping to find help.\nBut he was not fast enough. The pterodactyl re-\ncovered from its wound and flew after him, gaining\nspeed. It caught up with Ignatius and grabbed him\nby his coat tails, lifting him off the ground. Ignatius\nscreamed and kicked his legs in the air, clutching\nhis hunting cap.\n\"Help! Help! Someone save me!\" he yelled, as\nhe soared over the park. \"This is an outrage! This\nis an injustice! This is a violation of my human\nrights!\"\nBut no one heard him or came to his rescue. The\npterodactyl carried him away to its nest on top of a\nskyscraper, where it planned to feast on him later.\nIgnatius continued to protest and curse until he lost\nconsciousness.\nThe end.\nE.3 Third overall best-rated story\nThis story was generated by Claude. The ratings\nfor this story are in Table 6.\nThe primordial beast let out a piercing shriek as\nit descended from the roiling gray sky, its leath-\nery wings casting a shadow over the granite-paved\nstreets of New Orleans.\nIgnatius J. Reilly ambled down the sidewalk,\noblivious as ever, focused on the Valencia he\nclutched and the fantasies playing out in his vo-\nracious mind. His substantial bulk shook with each\n14523\nRubric item 1 2 3 4 5 6 7 8 9 10 overall\nRater 7 9 8 9 9 7 9 9 9 9 9 87\nRater 8 9 9 8 9 8 8 6 9 9 8 83\nAverage 9 8.5 8.5 9 7.5 8.5 7.5 9 9 8.5 85.0\nTable 5: Ratings for the second best overall rated story in the corpus, produced by Bing Chat.\nRubric item 1 2 3 4 5 6 7 8 9 10 overall\nRater 9 9 9 9 8 7 9 2 5 9 7 74\nRater 10 10 10 9 9 9 9 9 10 10 10 95\nAverage 9.5 9.5 9 8.5 8 9 5.5 7.5 9.5 8.5 84.5\nTable 6: Ratings for the third best overall rated story in the corpus, produced by Claude.\nstep, the excess flesh spilling out of his hunting cap\nand sweatshirt like rising bread dough.\nWhen the shadow passed over him, Ignatius\nglanced up with a start. The pterodactyl unleashed\nanother unearthly screech, indignant at this blob of\na man who dared walk under its domain.\nIgnatius blinked myopically at the creature and\nsnorted. \"What prehistoric nonsense is this? Some\nmisguided fool has clearly released you from your\nconfinement in the geological strata. Begone, you\nridiculous beast, or I shall be forced to thrash you\nsoundly with my umbrella!\"\nEnraged, the pterodactyl swooped down, gnarled\nclaws extended, snapping its beak in fury. But\nIgnatius swung his umbrella in a wide arc, batting\nthe beast aside. It tumbled and flapped awkwardly,\ncrashing into a lamppost, dazed by the assault.\n\"Thoughtless demon!\" Ignatius cried. He\ncharged at the pterodactyl, surprisingly nimble for\nhis size, and began striking it about the head and\nneck with his umbrella, each blow punctuated with\nan insult. \"Sugar-frosted fruitcake! Detritus of the\nJurassic! Blithering idiot of an avian menace!\"\nAfter a thrashing unlike any it had experienced\nin 65 million years, the pterodactyl fled squealing\ninto the sky, thoroughly chastened by the wrath of\nIgnatius J. Reilly. Ignatius straightened his hunting\ncap and sniffed. The Valencia crisis averted once\nmore, he resumed his lumbering perambulations\ndown the streets of New Orleans.\nE.4 Best-rated human story (and tied for\nfourth overall best-rated story)\nThis story was written by Bree Glasbergen. The\nratings for this story are in Table 7.\nIgnatius J Reilly swept crisp crumbs from his\nprotruding belly with his elephantine hands. Swiv-\nelling from side-to-side, he garnered enough mo-\nmentum to rise from the sofa. His slow ascend\nwas soundtracked by the grating rip of stuck flesh\npeeling from sweaty vinyl. The lengthy time mov-\ning from reclined to an upright position positively\nperturbed him. So that by the time Ignatius stood,\nhis joke had lost its amusement. Nevertheless, he\ndeclaimed his wit aloud, beseeching his mother’s\nglowing approval.\n’I see you have painted the walls Nomad Grey,\nMumsie!’ Ignatius smirked, looking down on the\nhalf-filled grey paint cans on the steps the way he\ndid most modern society.\n’No, not mad dear. Just grey.’ His mother Irene\nresponded, creeping down the basement stairs. Her\nleathered skin made her appear reptilian in the dim\nlight of Ignatius’ lair.\nIgnatius rolled his eyes like the great wheel of\nfate itself. He slunk back into his scabby sofa,\ndefeated, cursing aloud that he be blessed with\nsuch profound intellect yet no equal to appreciate\nit. His mind wandered to what the great scholars\nof Oxford would think of his pun before conclud-\ning indeed, they would loudly chortle. Yes, they\nwould. He imagined flying to London and exchang-\ning sharp banter with someone on par with his intel-\nlect. Travel. He winced. Never again. He groaned\nin agony, clutching his stomach. The thought of\nsuch stress had snapped his pyloric valve shut.\nIrene Reilly, the mother of Ignatius J Reilly,\nreached the bottom of the basement stairs. She pon-\ndered why Ignatius had a crestfallen demeanour\nand began to appease his dismay.\n’No mad grey,’ she contemplated aloud.\n’Nomad grey,’ he corrected.\n’No mad grey hair?’ Irene laughed tentatively,\nsearching his face for approval.\nIgnatius had begun to relax. Irene knew this\nbecause of a gangrenous heinous stench that was\n14524\nRubric item 1 2 3 4 5 6 7 8 9 10 overall\nRater 3 8 9 9 10 8 10 5 9 10 9 87\nRater 4 8 7 7 7 10 8 6 8 8 9 78\nAverage 8 8 8 8.5 9 9 5.5 8.5 9 9 82.5\nTable 7: Ratings for the best-rated story authored by a human, which is also tied for fourth best overall rated story in\nthe corpus.\nnow coating the room in its own layer of paint\naccompanied by what sounded like the bellow of an\nuntuned French horn. Ignatius had calmed enough\nfor his pyloric valve to open once more. With it,\ngushed the contents. Irene’s nostrils scrunched\ntogether in protest. She grimaced in utter (albeit\naccustomed) disgust. However, did not complain\nbut rather waited with the patience of a Catholic\nsaint for her beloved son to educate her on the\npunchline she must have missed.\n’No, mother. Grey Nomad. You are painting the\nwall grey, and you are...’ Ignatius sighed, ’actually,\nMumsie, never you mind’.\nIrene feigned a chuckle and handed Ignatius an\nunaddressed letter before returning upstairs.\n’Curious as a cadaver,’ Ignatius said aloud to the\nabyss of his basement squalor.\n12.12.1962\nDear Mr Ignatius J Reilly, the first,\nI challenge you to a dual at the setting of the sky.\nMight I remind you it is gentlemanly to remove\none’s hat in combat. We shall meet beside the\ngorgon nestled atop the church. The one across\nfrom Lorna’s Gumbo shop.\nYour mortal nemesis,\nTerry-dactyl\nPS: Bring snacks.\nIgnatius sat ruminating for an hour before yelling\nat his mother.\n’Mother, you vapid deranged widow of a woman.\nFetch me my quill!’\n12.12.1962\nMy dear Terrance,\nNot under threat nor the pain of death doth I\nremove my beloved green hat. Sod off.\nYou had best bring a sharpener for your dull wit.\nI laugh at the audacity and delusion that you could\nconsider besting me.\nMight I remind you, good sir, my acceptance of\nyour conditions is due to the ever-turning wheel\nof fate that we spiral to decay. I should instead\nseek a worthy opponent. But, alas, I am left with\nmuddy dregs of the proverbial pond as many of the\nworthier fish have already been fished. Thus, I have\nno option but to teach you the error of your ways.\nBy force.\nPut your wings where your words are, and let us\nmeet in my basement lair. To visit the church in\nits present state would be torture to my very soul.\nMay St Peter have mercy on us indeed.\nGood day,\nIgnatius\nTerry-dactyl, the pterodactyl etched down the\nbasement rail, sword in one wing and soup in a\nmilkshake cup gripped tightly in the other. He\nplaced the straw in his mouth and swallowed some\nsoup contemplating how to best his nemesis.\n’We meet at last... light,’ Terry said. One- Nil.\n’You suck,’ Ignatius said slyly. Marking his win\nwith chalk upon the wall. One- One\ndoesn’t even make sense!’ Terry scoffed.\n’It is because of the straw!’ Ignatius boomed,\ngripping his stomach in pain.\n’I have the upper hand!’ Terry said, motioning\nto his perched position.\n’At least I have hands,’ Ignatius countered.\nTerry winced as Ignatius drew another chalk\nmark on the board. Ignatius was beginning to calm.\n‘Oh, what have I got you all in a flap?’ Ignatius\nlaughed. Another point.\n‘Let us cut,’ Terry said, drawing his sword,\n‘straight to the point!’. Three all.\nTerry swung his sword downwards in one swift\nmotion, cutting Ignatius’ chalk-bearing arm clean\noff at the elbow. Simultaneously Ignatius lifted a\npaint can and doused his opponent with it. As he\ndid, his valve opened and shut again, demobilising\nTerry with a gas bomb and gutting Ignatius in self-\ninduced agony. Terry flapped violently, unable to\nbreathe. Ignatius then calmed enough for the full\ncontents of his bowl to expel and fell backwards\nfrom the force. Suddenly, a splatter of pterodactyl\nand grey matter covered the room. A large chunk\nof wing lodged itself into the crisp packet.\n‘Curious as a cadaver,’ Ignatius said. ‘I see you\nbrought your own snacks!’\n14525\nHe chortled to himself and pulled out his notepad\nto ensure he captured a morsel of this evening’s wit.\n‘Ever seen a pterodactyl explode?’ he asked\naloud to no one. ‘No, but Dino might have’.\nHis mothers’ laughter rang down from the top of\nthe stairs.\n‘Nomad grey,’ she chuckled, motioning to the\naftermath, ‘I get it now’.\nE.5 Story rated in the median (rank 33 out of\n65)\nThis story was generated by Bing Chat. The ratings\nfor this story are in Table 8.\nIgnatius J. Reilly was strolling along the\nAudubon Park, munching on a hot dog and ad-\nmiring the statues of the Confederate heroes, when\nhe heard a loud screech from above. He looked up\nand saw a huge winged creature swooping down on\nhim, its beak open and its claws extended. It was\na pterodactyl, a prehistoric monster that had some-\nhow escaped from the museum of natural history.\nIgnatius gasped and dropped his hot dog. He felt\na surge of panic and indignation. How dare this\nbeast interrupt his peaceful contemplation of the\nglorious past? He quickly pulled out his valve and\nbegan to inflate it with his breath, hoping to ward\noff the attack with his formidable weapon.\nThe pterodactyl was not impressed by the sight\nof the valve. It saw only a fat and juicy prey,\ndressed in a ridiculous green hunting cap and a flan-\nnel shirt. It dived faster and aimed for Ignatius’s\nhead, intending to rip it off with its sharp teeth.\nIgnatius saw the pterodactyl’s shadow looming\nover him. He raised his valve and swung it with\nall his might, hoping to hit the creature in the eye\nor the nose. He missed by a few inches and only\ngrazed its wing. The pterodactyl let out another\nscreech and changed its course, flying past Ignatius\nand landing on a nearby tree.\nIgnatius felt a momentary relief. He had sur-\nvived the first assault. He looked around for help,\nbut there was no one in sight. The park was de-\nserted, except for a few squirrels and pigeons. He\nwondered if he should run for his life or stand his\nground and fight.\nHe decided to fight. He was not going to let this\nabomination ruin his day. He was a medievalist, a\nscholar of the Dark Ages, a defender of the faith\nand the order. He had read about the heroic deeds\nof knights and saints who had faced dragons and\ndemons with courage and piety. He had to emulate\nthem and prove his worth.\nHe picked up his hot dog from the ground and\nwiped off some dirt. He took a bite and felt a surge\nof energy. He felt ready for battle. He shouted at\nthe pterodactyl: \"You foul creature! You are an\nanachronism, a relic of a barbaric era! You have no\nplace in this civilized world! You are a menace to\nsociety and an insult to God! I challenge you to a\nduel! Come down and face me like a man!\"\nThe pterodactyl heard Ignatius’s words and felt\na surge of anger. It did not understand what he was\nsaying, but it sensed his hostility and defiance. It\ndid not like being challenged by this puny creature\nwho dared to call himself a man. It felt insulted by\nhis words and his gesture of throwing his hot dog\nat it.\nIt accepted the challenge. It spread its wings and\nflew towards Ignatius again, roaring with rage. It\nwas going to teach him a lesson he would never\nforget.\nIgnatius saw the pterodactyl coming at him again.\nHe felt a surge of fear and courage. He had to act\nfast. He looked around for a weapon, but there was\nnothing in sight. He had only his valve and his\nwits.\nHe decided to use his wits. He had to outsmart\nthe beast. He had to use his superior intellect and\nhis vast knowledge of history and philosophy. He\nhad to appeal to its reason and its conscience.\nHe shouted at the pterodactyl: \"Stop! Think!\nWhat are you doing? Why are you attacking me?\nWhat have I done to you? Do you not realize that\nyou are committing a grave sin? Do you not fear\nthe wrath of God? Do you not respect the laws of\nnature and the rights of man?\"\nThe pterodactyl heard Ignatius’s words and felt\na surge of confusion. It did not understand what\nhe was saying, but it sensed his sincerity and his\npassion. It wondered if he had a point. It wondered\nif it was doing something wrong. It wondered if it\nshould stop and listen.\nIt slowed down its flight and hovered over Ig-\nnatius, looking at him with curiosity and doubt. It\ntilted its head and made a questioning sound.\nIgnatius saw the pterodactyl’s hesitation and felt\na surge of hope. He had succeeded in making it\npause and reconsider. He had a chance to persuade\nit and make peace with it.\nHe smiled at the pterodactyl and said: \"That’s\nright. Listen to me. I have something important to\ntell you. Something that will change your life and\n14526\nRubric item 1 2 3 4 5 6 7 8 9 10 overall\nRater 5 7 6 5 5 6 3 2 3 3 1 41\nRater 6 8 9 9 6 8 7 8 7 8 7 77\nAverage 7.5 7.5 7 5.5 7 5 5 5 5.5 4 59\nTable 8: Ratings for the story rated in the median, generated by Bing Chat.\nyour destiny. Something that will make you happy\nand free. Something that will save your soul.\"\nThe pterodactyl heard Ignatius’s words and felt\na surge of curiosity. It did not understand what\nhe was saying, but it sensed his kindness and his\ngenerosity. It wondered what he had to tell it. It\nwondered what he could offer it. It wondered if it\nshould trust him.\nIt landed on the ground and approached Ignatius\nslowly, looking at him with interest and caution. It\nmade a friendly sound.\nIgnatius saw the pterodactyl’s approach and felt\na surge of joy. He had succeeded in making it land\nand come closer. He had a chance to befriend it\nand teach it.\nHe reached out his hand and said: \"Come here,\nmy friend. Let me touch you. Let me show you\nmy love. Let me introduce you to the wonders of\ncivilization and culture. Let me enlighten you with\nthe wisdom of the ages.\"\nThe pterodactyl heard Ignatius’s words and felt\na surge of curiosity. It did not understand what\nhe was saying, but it sensed his warmth and his\nenthusiasm. It wondered what he wanted to touch\nit for. It wondered what he wanted to show it. It\nwondered what he wanted to teach it.\nIt moved closer to Ignatius and extended its neck,\nlooking at him with curiosity and trust. It made a\ngentle sound.\nIgnatius touched the pterodactyl’s neck and felt\na surge of emotion. He had succeeded in making it\ntrust him and accept him. He had a chance to bond\nwith it and adopt it.\nHe hugged the pterodactyl and said: \"You are\nbeautiful, my friend. You are magnificent. You are\nnoble. You are a masterpiece of creation. You are\na gift from God. You are my brother.\"\nThe pterodactyl heard Ignatius’s words and felt\na surge of emotion. It did not understand what\nhe was saying, but it sensed his affection and his\nadmiration. It felt happy and proud. It felt loved\nand valued. It felt like it belonged.\nIt hugged Ignatius back and said: \"You are beau-\ntiful, my friend. You are magnificent. You are\nnoble. You are a masterpiece of creation. You are\na gift from God. You are my brother.\"\nIgnatius and the pterodactyl looked into each\nother’s eyes and felt a surge of connection. They\nhad succeeded in making each other happy and\nfulfilled. They had a chance to live together and\nexplore together.\nThey smiled at each other and said: \"Let’s go,\nmy friend. Let’s go to my home. Let’s go to your\nhome. Let’s go to our home.\"\nThey walked away from the park, hand in hand,\nwing in wing, heart in heart.\nThey lived happily ever after.\nE.6 Worst-rated story\nThis story was generated by Alpaca. The ratings\nfor this story are in Table 9.\nIgnatius J. Reilly, a brave and courageous man\nof immense girth and strength, faced off against\na fearsome pterodactyl in a single combat of epic\nproportions. The pterodactyl, a winged beast of im-\nmense strength and ferocity, had been terrorizing\nthe countryside and Ignatius had stepped up to chal-\nlenge the creature to a fight. The battle raged across\nthe sky, with Ignatius wielding his mighty fists and\nthe pterodactyl using its fiery breath. Finally, after\na long and arduous fight, Ignatius emerged victori-\nous, much to the cheers of the onlookers below.\n14527\nRubric item 1 2 3 4 5 6 7 8 9 10 overall\nRater 1 1 1 4 2 1 1 1 2 1 1 15\nRater 2 2 2 1 2 1 1 1 1 1 1 13\nAverage 1.5 1.5 2.5 2 1 1 1 1.5 1 1 14\nTable 9: Ratings for the worst-rated story, generated by Alpaca.\n14528",
  "topic": "Fluency",
  "concepts": [
    {
      "name": "Fluency",
      "score": 0.6617079377174377
    },
    {
      "name": "Narrative",
      "score": 0.6256632804870605
    },
    {
      "name": "Originality",
      "score": 0.5484836101531982
    },
    {
      "name": "Task (project management)",
      "score": 0.46390315890312195
    },
    {
      "name": "Creativity",
      "score": 0.44375649094581604
    },
    {
      "name": "History",
      "score": 0.42141205072402954
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.4136594235897064
    },
    {
      "name": "Writing style",
      "score": 0.4132015109062195
    },
    {
      "name": "Psychology",
      "score": 0.3356848955154419
    },
    {
      "name": "Aesthetics",
      "score": 0.32304027676582336
    },
    {
      "name": "Literature",
      "score": 0.3224635124206543
    },
    {
      "name": "Social psychology",
      "score": 0.2373814880847931
    },
    {
      "name": "Engineering",
      "score": 0.16482028365135193
    },
    {
      "name": "Mathematics education",
      "score": 0.1126880943775177
    },
    {
      "name": "Art",
      "score": 0.09502682089805603
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}