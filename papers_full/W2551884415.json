{
  "title": "Coherent Dialogue with Attention-Based Language Models",
  "url": "https://openalex.org/W2551884415",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2202636965",
      "name": "Hongyuan Mei",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2070810387",
      "name": "Mohit Bansal",
      "affiliations": [
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A2102070473",
      "name": "Matthew Walter",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2070810387",
      "name": "Mohit Bansal",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of North Carolina Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A2102070473",
      "name": "Matthew Walter",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W194577561",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W6816560326",
    "https://openalex.org/W2518570122",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W1958706068",
    "https://openalex.org/W1933065844",
    "https://openalex.org/W2204900930",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W6682137061",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W6600426076",
    "https://openalex.org/W2159640018",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W4213009331",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2227303133",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2165599843",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1447066968",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1525482321",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W2335122196",
    "https://openalex.org/W10957333",
    "https://openalex.org/W2411447339",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W2952013107",
    "https://openalex.org/W2311783643",
    "https://openalex.org/W2962905474",
    "https://openalex.org/W2291723583",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2951527505",
    "https://openalex.org/W2950738719",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "We model coherent conversation continuation via RNN-based dialogue models equipped with a dynamic attention mechanism. Our attention-RNN language model dynamically increases the scope of attention on the history as the conversation continues, as opposed to standard attention (or alignment) models with a fixed input scope in a sequence-to-sequence model. This allows each generated word to be associated with the most relevant words in its corresponding conversation history. We evaluate the model on two popular dialogue datasets, the open-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot dataset, and achieve significant improvements over the state-of-the-art and baselines on several metrics, including complementary diversity-based metrics, human evaluation, and qualitative visualizations. We also show that a vanilla RNN with dynamic attention outperforms more complex memory models (e.g., LSTM and GRU) by allowing for flexible, long-distance memory. We promote further coherence via topic modeling-based reranking.",
  "full_text": "Coherent Dialogue with Attention-Based Language Models\nHongyuan Mei\nJohns Hopkins University\nhmei@cs.jhu.edu\nMohit Bansal\nUNC Chapel Hill\nmbansal@cs.unc.edu\nMatthew R. Walter\nTTI-Chicago\nmwalter@ttic.edu\nAbstract\nWe model coherent conversation continuation via RNN-\nbased dialogue models equipped with a dynamic attention\nmechanism. Our attention-RNN language model dynami-\ncally increases the scope of attention on the history as the\nconversation continues, as opposed to standard attention (or\nalignment) models with a ﬁxed input scope in a sequence-to-\nsequence model. This allows each generated word to be asso-\nciated with the most relevant words in its corresponding con-\nversation history. We evaluate the model on two popular dia-\nlogue datasets, the open-domain MovieTriples dataset and the\nclosed-domain Ubuntu Troubleshootdataset, and achieve sig-\nniﬁcant improvements over the state-of-the-art and baselines\non several metrics, including complementary diversity-based\nmetrics, human evaluation, and qualitative visualizations. We\nalso show that a vanilla RNN with dynamic attention outper-\nforms more complex memory models (e.g., LSTM and GRU)\nby allowing for ﬂexible, long-distance memory. We promote\nfurther coherence via topic modeling-based reranking.\nIntroduction\nAutomatic conversational models (Winograd 1971), also\nknown as dialogue systems, are of great importance to a\nlarge variety of applications, ranging from open-domain en-\ntertaining chatbots to goal-oriented technical support agents.\nAn increasing amount of research has recently been done\nto build purely data-driven dialogue systems that learn from\nlarge corpora of human-to-human conversations, without us-\ning hand-crafted rules or templates. While most work in\nthis area formulates dialogue modeling in a sequence-to-\nsequence framework (similar to machine translation) (Rit-\nter, Cherry, and Dolan 2011; Shang, Lu, and Li 2015;\nVinyals and Le 2015; Sordoni et al. 2015; Li et al. 2016a;\nDuˇsek and Jurˇc´ıˇcek 2016), some more recent work (Serban\net al. 2016; Luan, Ji, and Ostendorf 2016) instead trains a\nlanguage model over the entire dialogue as one single se-\nquence. In our work, we empirically demonstrate that a\nlanguage model is better suited to dialogue modeling, as\nit learns how the conversation evolves as information pro-\ngresses. Sequence-to-sequence models, on the other hand,\nlearn only how the most recent dialogue response is gener-\nated. Such models are better suited to converting the same\nCopyright c⃝ 2017, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ninformation from one modality to another, e.g., in machine\ntranslation and image captioning.\nWe improve the coherence of such neural dialogue lan-\nguage models by developing a generative dynamic attention\nmechanism that allows each generated word to choose which\nrelated words it wants to align to in the increasing conver-\nsation history (including the previous words in the response\nbeing generated). Neural attention (or alignment) has proven\nvery successful for various sequence-to-sequence tasks by\nassociating salient items in the source sequence with the\ngenerated item in the target sequence (Mnih et al. 2014;\nBahdanau, Cho, and Bengio 2015; Xu et al. 2015; Mei,\nBansal, and Walter 2016a; Parikh et al. 2016). However,\nsuch attention models are limited to a ﬁxed scope of his-\ntory, corresponding to the input source sequence. In con-\ntrast, we introduce a dynamic attention mechanism to a re-\ncurrent neural network (RNN) language model in which the\nscope of attention increases as the recurrence operation pro-\ngresses from the start through the end of the conversation.\nThe dynamic attention model promotes coherence of the\ngenerated dialogue responses (continuations) by favoring\nthe generation of words that have syntactic or semantic asso-\nciations with salient words in the conversation history. Our\nsimple model shows signiﬁcant improvements over state-of-\nthe-art models and baselines on several metrics (including\ncomplementary diversity-based metrics, human evaluation,\nand qualitative visualizations) for the open-domain Movi-\neTriples and closed-domain Ubuntu Troubleshoot datasets.\nOur vanilla RNN model with dynamic attention outperforms\nmore complex memory models (e.g., LSTM and GRU) by\nallowing for long-distance and ﬂexible memory. We also\npresent several visualizations to intuitively understand what\nthe attention model is learning. Finally, we also explore a\ncomplementary LDA-based method to re-rank the outputs\nof the soft alignment-based coherence method, further im-\nproving performance on the evaluation benchmarks.\n1\nRelated Work\nA great deal of attention has been paid to developing data-\ndriven methods for natural language dialogue generation.\n1Arxiv version with appendices: https://arxiv.org/abs/1611.\n06997\nProceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)\n3252\n\u0001\u0002\u0003\u0004\u0002\u0003\n(a) RNN seq2seq (encoder-decoder) model (b) RNN language model\n\u0001\u0002\u0002\u0003\u0004\u0002\u0005\u0006\u0004\n\u0007\b\b\n(c) Attention seq2seq (encoder-decoder) model (d) Attention language model\nFigure 1: Comparing RNN language models to RNN sequence-to-sequence model, with and without attention.\nConventional statistical approaches tend to rely extensively\non hand-crafted rules and templates, require interaction with\nhumans or simulated users to optimize parameters, or pro-\nduce conversation responses in an information retrieval fash-\nion. Such properties prevent training on the large human\nconversational corpora that are becoming increasingly avail-\nable, or fail to produce novel natural language responses.\nRitter, Cherry, and Dolan (2011) formulate dialogue re-\nsponse generation as a statistical phrase-based machine\ntranslation problem, which requires no explicit hand-crafted\nrules. The recent success of RNNs in statistical machine\ntranslation (Sutskever, Vinyals, and Lee 2014; Bahdanau,\nCho, and Bengio 2015) has inspired the application of such\nmodels to the ﬁeld of dialogue modeling. Vinyals and Le\n(2015) and Shang, Lu, and Li (2015) employ an RNN to gen-\nerate responses in human-to-human conversations by treat-\ning the conversation history as one single temporally ordered\nsequence. In such models, the distant relevant context in the\nhistory is difﬁcult to recall. Some efforts have been made\nto overcome this limitation. Sordoni et al. (2015) separately\nencode the most recent message and all the previous context\nusing a bag-of-words representation, which is decoded using\nan RNN. This approach equates the distance of each word\nin the generated output to all the words in the conversation\nhistory, but loses the temporal information of the history.\nSerban et al. (2016) design a hierarchical model that stacks\nan utterance-level RNN on a token-level RNN, where the\nutterance-level RNN reduces the number of computational\nsteps between utterances. Wen et al. (2015) and Wen et al.\n(2016) improve spoken dialog systems via multi-domain and\nsemantically conditioned neural networks on dialog act rep-\nresentations and explicit slot-value formulations.\nOur work explores the ability of recurrent neural network\nlanguage models (Bengio et al. 2003; Mikolov 2010) to in-\nterpret and generate natural language conversations while\nstill maintaining a relatively simple architecture. We show\nthat a language model approach outperforms the sequence-\nto-sequence model at dialogue modeling. Recently, Tran,\nBisazza, and Monz (2016) demonstrated that the neural at-\ntention mechanism can improve the effectiveness of a neural\nlanguage model. We propose an attention-based neural lan-\nguage model for dialogue modeling that learns how a con-\nversation evolves as a whole, rather than only how the most\nrecent response is generated, and that also reduces the num-\nber of computations between the current recurrence step and\nthe distant relevant context in the conversation history.\nThe attention mechanism in our model has the additional\nbeneﬁt of favoring words that have semantic association\nwith salient words in the conversation history, which pro-\nmotes the coherence of the topics in the continued dialogue.\nThis is important when conversation participants inherently\nwant to maintain the topic of the discussion. Some past\nstudies have equated coherence with propositional consis-\ntency (Goldberg 1983), while others see it as a summary\nimpression (Sanders 1983). Our work falls in the cate-\ngory of viewing coherence as topic continuity (Crow 1983;\nSigman 1983). Similar objectives, i.e., generating dia-\nlogue responses with certain properties, have been addressed\nrecently, such as promoting response diversity (Li et al.\n2016a), enhancing personal consistency (Li et al. 2016b),\nand improving speciﬁcity (Yao et al. 2016). Concurrent with\nthis work, Luan, Ji, and Ostendorf (2016) improve topic\nconsistency by feeding into the model the learned LDA-\nbased topic representations. We show that the simple atten-\ntion neural language model signiﬁcantly outperforms such\na design. Furthermore, we suggest an LDA-based re-ranker\ncomplementary to soft neural attention that further promotes\ntopic coherence.\n3253\nThe Model\nRNN Seq2Seq and Language Models\nRecurrent neural networks have been successfully\nused both in sequence-to-sequence models (RNN-\nSeq2Seq, Fig. 1a) (Sutskever, Vinyals, and Lee 2014) and in\nlanguage models (RNN-LM, Fig. 1b) (Bengio et al. 2003;\nMikolov 2010). We ﬁrst discuss language models for\ndialogue, which is the primary focus of our work, then\nbrieﬂy introduce the sequence-to-sequence model, and\nlastly discuss the use of attention methods in both models.\nThe RNN-LM models a sentence as a sequence of tokens\n{w\n0,w1,...,w T }with a recurrence function\nht = f(ht−1,wt−1) (1)\nand an output (softmax) function\nP(wt = vj|w0:t−1)= expg(ht,vj)∑\ni expg(ht,vi), (2)\nwhere the recurrent hidden state ht ∈ Rd encodes all the\ntokens up to t−1 and is used to compute the probability of\ngenerating vj ∈ V as the next token from the vocabulary V.\nThe functions f and g are typically deﬁned as\nf(ht−1,wt−1)=t a n h (Hht−1 +PEwt−1 ) (3a)\ng(ht,vj)= O⊤\nvj ht, (3b)\nwhere H ∈ Rd×d is the recurrence matrix, Ewt−1 is a\ncolumn of word embedding matrix E ∈ Rde×V that corre-\nsponds to wt−1, P ∈ Rd×de projects word embedding into\nthe space of the same dimension d as the hidden units, and\nO ∈ Rd×V is the output word embedding matrix with col-\numn vector Ovj corresponding to vj.\nWe train the RNN-LM, i.e, estimate the parameters H,\nP, E and O, by maximizing the log-likelihood on a set of\nnatural language training sentences of size N\nℓ= 1\nN\nN∑\nn=1\nTn∑\nt=0\nlogP(wt|w0:t−1) (4)\nSince the entire architecture is differentiable, the objective\ncan be optimized by back-propagation.\nWhen dialogue is formulated as a sequence-to-sequence\ntask, the RNN-Seq2Seq model can be used in order to pre-\ndict a target sequence w\nT\n0:L = {wT\n0 ,wT\n1 ,...,w T\nL } given an\ninput source sequence wS\n0:M = {wS\n0 ,wS\n1 ,...,w S\nM }. In such\nsettings, an encoder RNN represents the input as a sequence\nof hidden states h\nS\n0:M = {hS\n0 ,hS\n1\n,...,h S\nM }, and a separate\ndecoder RNN then predicts the target sequence token-by-\ntoken given the encoder hidden states hS\n0:M .\nAttention in RNN-Seq2Seq Models\nThere are several ways by which to integrate the sequence of\nhidden states hS\n0:M in the decoder RNN. An attention mech-\nanism (Fig. 1c) has proven to be particularly effective for\nvarious related tasks in machine translation, image caption\nsynthesis, and language understanding (Mnih et al. 2014;\nBahdanau, Cho, and Bengio 2015; Xu et al. 2015; Mei,\nBansal, and Walter 2016a).\nThe attention module takes as input the encoder hidden\nstate sequence h\nS\n0:M and the decoder hidden state hT\nl−1 at\neach step l−1, and returns a context vector zl computed as\na weighted average of encoder hidden states hS\n0:M\nβlm = b⊤ tanh(WhT\nl−1 +UhS\nm\n) (5a)\nαlm = exp(βlm)/\nM∑\nm=0\nexp(βlm) (5b)\nzl =\nM∑\nm=0\nαlmhS\nm\n, (5c)\nwhere parameters W ∈ Rd×d, U ∈ Rd×d, and b ∈ Rd are\njointly learned with the other model parameters. The context\nvector zl is then used as an extra input to the decoder RNN\nat step l together with wT\n0:l−1 to predict the next token wT\nl .\nAttention in RNN-LM\nWe develop an attention-RNN language model (A-RNN-\nLM) as illustrated in Figure 1d, and describe how it can be\nused in the context of dialogue modeling. We then describe\nits advantages compared to the use of attention in sequence-\nto-sequence models.\nAs with the RNN-LM, the model ﬁrst encodes the in-\nput into a sequence of hidden states up to word t − 1\n(Eqn. 1). Given a representation of tokens up to t − 1\n{r\n0,r1,...,r t−1} (which we deﬁne shortly), the atten-\ntion module computes the context vector zt at step t as a\nweighted average of r0:t−1\nβti = b⊤ tanh(Wht−1 +Uri) (6a)\nαti = exp(βti)/\nt−1∑\ni=0\nexp(βti) (6b)\nzt =\nt−1∑\ni=0\nαtiri (6c)\nWe then use the context vector zt together with the hidden\nstate ht to predict the output at time t\ng(ht,zt,vj)= O⊤\nvj (Ohht +Ozzt) (7a)\nP(wt = vj|w0:t−1)= expg(ht,zt,vj)∑\ni expg(ht,zt,vi), (7b)\nwhere Oh ∈ Rd×d and Oz ∈ Rd×dz project ht and zt, re-\nspectively, into the same space of dimension d.\nThere are multiple beneﬁts of using an attention-RNN\nlanguage model for dialogue, which are empirically sup-\nported by our experimental results. First, a complete dia-\nlogue is usually composed of multiple turns. A language\nmodel over the entire dialogue is expected to better learn\nhow a conversation evolves as a whole, unlike a sequence-\nto-sequence model, which only learns how the most recent\nresponse is generated and is better suited to translation-\nstyle tasks that transform the same information from one\nmodality to another. Second, compared to LSTM models,\nan attention-based RNN-LM also allows for gapped con-\ntext and a ﬂexible combination of conversation history for\n3254\nevery individual generated token, while maintaining low\nmodel complexity. Third, attention models yield inter-\npretable results—we visualize the learned attention weights,\nshowing how attention chooses the salient words from the\ndialogue history that are important for generating each new\nword. Such a visualization is typically harder for the hidden\nstates and gates of conventional LSTM and RNN language\nmodels.\nWith an attention mechanism, there are multiple options\nfor deﬁning the token representation r\n0:t−1. The original\nattention model introduced by Bahdanau, Cho, and Ben-\ngio (2015) uses the hidden units h\n0:t−1 as the token rep-\nresentations r0:t−1. Recent work (Mei, Bansal, and Walter\n2016a; 2016b) has demonstrated that performance can be\nimproved by using multiple abstractions of the input, e.g.,\nr\ni =( E⊤\nwi ,h⊤\ni )⊤ , which is what we use in this work.\nLDA-based Re-Ranking\nWhile the trained attention-RNN dialogue model generates\na natural language continuation of a conversation while\nmaintaining topic concentration by token association, some\ndialogue-level topic-supervision can help to encourage gen-\nerations that are more topic-aware. Such supervision is\nnot commonly available, and we use unsupervised meth-\nods to learn document-level latent topics. We employ the\nlearned topic model to select the best continuation based on\ndocument-level topic-matching.\nWe choose Latent Dirichlet Allocation (LDA) (Blei, Ng,\nand Jordan 2003; Blei and Lafferty 2009) due to its demon-\nstrated ability to learn a distribution over latent topics given\na collection of documents. This generative model assumes\ndocuments {w\n0:Tn }N\nn=1 arise from K topics, each of which\nis deﬁned as a distribution over a ﬁxed vocabulary of terms,\nwhich forms a graphical structureL that can be learned from\nthe training data. The topic representation ˆθ of a (possi-\nbly unseen) dialogue w0:T can then be estimated with the\nlearned topic structure L as ˆθ(w0:T )= L(w0:T ).\nGiven a set of generated continuations {cm}M\nm=1 for each\nunseen dialogue w0:T , the topic representations of the di-\nalogue and its continuations are ˆθ(w0:T )= L(w0:T ) and\nˆθ(cm)=L( cm), respectively. We employ a matching score\nSm = S\n(ˆθ(w0:T ),ˆθ(cm)\n)\nto compute the similarity between\nˆθ(w0:T ) and each ˆθ(cm). In the end, a weighted score is\ncomputed as ¯Sm = λSm +(1− λ)ℓ(cm|w0:T ), where λ ∈\n[0,1]and ℓ(cm|w0:T )is the conditional log-likelihood of the\ncontinuation cm. The hyper-parameters K and λ are tuned\non a development set.\nConcurrent with our work, Luan, Ji, and Ostendorf (2016)\nuse learned topic representations ˆθ of the given conversation\nas an extra feature in a language model to enhance the topic\ncoherence of the generation. As we show in the Results sec-\ntion, our model signiﬁcantly outperforms this approach.\nExperimental Setup\nDataset We train and evaluate the models on two large\nnatural language dialogue datasets, MovieTriples (pre-\nprocessed by Serban et al. (2016)) and Ubuntu Troubleshoot\n(pre-processed by Luan, Ji, and Ostendorf (2016)). The di-\nalogue within each of these datasets consists of a sequence\nof utterances (turns), each of which is a sequence of tokens\n(words).\n2 The arxiv version’s appendix provides the statis-\ntics for these two datasets.\nEvaluation Metrics For the sake of comparison, we\nclosely follow previous work and adopt several standard\n(and complementary) evaluation metrics: perplexity (PPL),\nword error rate (WER), recall@N, BLEU, and diversity-\nbased Distinct-1. We provide further discussion of the var-\nious metrics and their advantages in the arxiv version’s ap-\npendix. On the MovieTriples dataset, we use PPL and WER,\nas is done in previous work. Following Serban et al. (2016),\nwe adopt two versions for each metric: i) PPL as the word-\nlevel perplexity over the entire dialogue conversation; ii)\nPPL@L as the word-level perplexity over the last utterance\nof the conversation; iii) WER; and iv) WER@L (deﬁned\nsimilarly).\nOn the Ubuntu dataset, we follow previous work and use\nPPL and recall@N. Recall@N (Manning et al. 2008) eval-\nuates a model by measuring how often the model ranks the\ncorrect dialogue continuation within top-N given 10 candi-\ndates. Additionally, we also employ the BLEU score (Pap-\nineni et al. 2001) to evaluate the quality of the generations\nproduced by the models. Following Luan, Ji, and Osten-\ndorf (2016), we perform model selection using PPL on the\ndevelopment set, and perform the evaluation on the test set\nusing the other metrics. We also present evaluation using\nthe Distinct-1 metric (proposed by Li et al. (2016a)) to mea-\nsure the ability of the A-RNN to promote diversity in the\ngenerations, because typical neural dialogue models gener-\nate generic, safe responses (technically appropriate but not\ninformative, e.g., “I dont know”). Finally, we also present a\npreliminary human evaluation.\nTraining Details For the MovieTriples dataset, we follow\nthe same procedure as Serban et al. (2016) and ﬁrst pre-\ntrain on the largeQ-A SubTitledataset (Ameixa et al. 2014),\nwhich contains 5.5M question-answer pairs from which we\nrandomly sample 20000 pairs as the held-out set, and then\nﬁne-tune on the target MovieTriples dataset. We perform\nearly-stopping according to the PPL score on the held-out\nset. We train the models for both the MovieTriples and\nUbuntu Troubleshootdatasets using Adam (Kingma and Ba\n2015) for optimization in RNN back-propagation. The arxiv\nversion’s appendix provides additional training details, in-\ncluding the hyperparameter settings.\nResults and Analysis\nPrimary Dialogue Modeling Results\nIn this section, we compare the performance on several met-\nrics of our attention-based RNN-LM with RNN baselines\nand state-of-the-art models on the two benchmark datasets.\n2Following Luan, Ji, and Ostendorf (2016), we randomly sam-\nple nine utterances as negative examples of the last utterance for\neach conversation in Ubuntu Troubleshootfor the development set.\n3255\nFigure 2: A visualization of attention on the (a) MovieTriples and (b) Ubuntu Troubleshootingdatasets, showing which words\nin the conversation history are being aligned to, for each generated response word. Shaded intervals indicate the strength with\nwhich the corresponding words in the conversation history and response are attend to when generating the bolded word in the\nresponse. We show this for two generated words in the same response (left and right column).\nTable 1: Results on the MovieTriples test set. The HRED\nresults are from Serban et al. (2016).\nModel PPL PPL@L WER WER@L\nRNN 27.09 26.67 64.10% 64.07%\nHRED 26.81 26.31 63.93% 63.91%\nA-RNN 25.52 23.46 61.58% 60.15%\nTable 1 reports PPL and WER results on the MovieTriples\ntest set, while Table 2 compares different models onUbuntu\nTroubleshoot in terms of PPL on the development set and\nrecall@N (N =1 and 2) on the test set (following what\nprevious work reports). In the tables, RNN is the plain\nvanilla RNN language model (RNN-LM), as deﬁned in\nThe Model section, and LSTM is an LSTM-RNN language\nmodel, i.e., an RNN-LM with LSTM memory units. A-\nRNN refers to our main model as deﬁned in the Atten-\ntion in RNN-LM section. HRED in Table 1 is the hier-\narchical neural dialogue model proposed by Serban et al.\n(2016).\n3 LDA-CONV in Table 2 is proposed by Luan, Ji,\nand Ostendorf (2016), which integrates learned LDA-topic-\nproportions into an LSTM language model in order to pro-\nmote topic-concentration in the generations. Both tables\ndemonstrate that the attention-RNN-LM (A-RNN) model\nachieves the best results reported to-date on these datasets\nin terms all evaluation metrics. It improves the ability of\nan RNN-LM to model continuous dialogue conversations,\nwhile keeping the model architecture simple.\nWe also evaluate the effectiveness of the RNN-LM and\nRNN-Seq2Seq models on both theMovieTriples and Ubuntu\n3We compare to their best-performing model version, that\nadopts bidirectional gated-unit RNN (GRU).\nTable 2: Ubuntu Troubleshoot PPL and recall@N, with\nLSTM and LDA-CONV results from Luan et al. (2016).\nModel PPL recall@1 recall@2\nRNN 56.16 11% 22%\nLSTM 54.93 12% 22%\nLDA-CONV 51.13 13% 24%\nA-RNN 45.38 17% 30%\nTable 3: RNN-LM vs. RNN-Seq2Seq\nPPL MovieTriples Ubuntu Troubleshoot\nRNN-Seq2Seq 35.10 104.61\nRNN-LM 27.81 56.16\nTroubleshoot development sets. As shown in Table 3, the\nRNN language model yields lower perplexity than the RNN\nsequence-to-sequence model on both datasets. Hence, we\npresent all primary results on our primary A-RNN attention-\nbased RNN language model.\n4\nGeneration Diversity Results\nNext, we investigate the ability of the A-RNN to promote\ndiversity in the generations, compared to that of the vanilla\nRNN using the Distinct-1 metric proposed by Li et al.\n(2016a). Distinct-1 is computed as the number of distinct\nunigrams in the generation scaled by the total number of\ngenerated tokens. Table 5 shows that our attention-based\nRNN language model (A-RNN) yields much more diversity\n4Experiments also demonstrate signiﬁcant improvements for\nthe Attention-RNN-LM over the Attention-RNN-Seq2Seq.\n3256\nTable 4: BLEU score on Ubuntu Troubleshoot\nModel dev BLEU test BLEU\nRNN 0.1846 0.1692\nA-RNN 0.2702 0.3713\nT-A-RNN 0.2908 0.3128\nA-RNN-RR 0.4696 0.4279\nT-A-RNN-RR 0.4895 0.3971\nHistory: A: person , but this time i got the money .\nR: if you give it to me , i might forget i found you .\nA-RNN: A:i’l lg i v ei tt oy o u.\nRNN: A: i don’tk n o w.\nReference: A: i don’th a v ei twith me . tell person .\n(a) MovieTriples\nHistory:\nA: i have really choppy streaming video in 9.10, any way to ﬁx that?\nA: any way to ﬁx choppy streaming video in 9.10 ?\nR: what kind of video\nA-RNN: A: what video card do you have ?\nRNN: A: what are you trying to do ?\nReference: A: what cpu do you have\n(b) Ubuntu Troubleshoot\nFigure 3: Examples of responses generated by the A-RNN.\nin its generations as compared to the vanilla RNN baseline.\nTopic Coherence Results\nNext, we investigate the ability of the different mod-\nels to promote topic coherence in the generations in\nterms of BLEU score. In addition to the RNN and A-\nRNN models, we consider T-A-RNN, a method that in-\ncorporates LDA-based topic information into an A-RNN\nmodel, following the approach of Luan, Ji, and Os-\ntendorf (2016). We also evaluate our LDA-based re-\nranker, A-RNN-RR, which re-ranks according to the score\n¯S\nm = λSm +(1− λ)ℓ(cm|w0:T ), where we compute the\nlog-likelihood ℓ(cm|w0:T ) based upon a trained A-RNN-M\nmodel and validate the weightλon the development set. We\nalso consider a method that combines the T-A-RNN model\nwith an LDA-based re-ranker (T-A-RNN-RR).5 Table 4 re-\nports the resulting BLEU scores for each of these methods\non the development and test sets from the Ubuntu Trou-\nbleshoot dataset. We make the following observations based\nupon these results: (1) The A-RNN performs substantially\nbetter than the RNN with regards to BLEU; (2) using our\nLDA-based re-ranker further improves the performance by\na signiﬁcant amount (A-RNN v.s. A-RNN-RR); (3) as op-\nposed to our LDA-based re-ranker, adopting the LDA de-\nsign of Luan, Ji, and Ostendorf (2016) only yields marginal\nimprovements on the development set, but does not general-\n5Since Luan, Ji, and Ostendorf (2016) do not publish BLEU\nscores or implementations of their models, we can not compare\nwith LDA-CONV on BLEU. Instead, we demonstrate the effect of\nadding the key component of LDA-CONV on top of the A-RNN.\nTable 5: Generation Diversity Results: A-RNN vs. RNN\nDistinct-1 MovieTriples Ubuntu Troubleshoot\nRNN 0.0004 0.0007\nA-RNN 0.0028 0.0104\nize well to the test set (A-RNN v.s. T-A-RNN and A-RNN-\nRR v.s. T-A-RNN-RR). Also, our LDA re-ranker results in\nsubstantial improvements even on top of their topic-based\nmodel (T-A-RNN v.s. T-A-RNN-RR).\nPreliminary Human Evaluation\nIn addition to multiple automatic metrics, we also report a\npreliminary human evaluation. On each dataset, we manu-\nally evaluate the generations of both the A-RNN and RNN\nmodels on 100 examples randomly sampled from the test\nset. For each example, we randomly shufﬂe the two response\ngenerations, anonymize the model identity, and ask a human\nannotator to choose which response generation is more topi-\ncally coherent based on the conversation history. As Table 6\nshows, the A-RNN model wins substantially more often than\nthe RNN model.\nTable 6: Human Evaluaton: A-RNN vs. RNN\nMovieTriples Ubuntu Troubleshoot\nNot distinguishable 48% 74%\nRNN wins 6% 5%\nA-RNN wins 46% 21%\nQualitative Analysis\nNext, we qualitatively evaluate the effectiveness of our A-\nRNN model through visualizations of the attention and out-\nputs on both datasets. Figure 2 provides a visualization of\nthe attention for a subset of the words in the generation for\nthe two datasets. The last line in both Figure 2a and Figure\n2b presents the generated response and we highlight in bold\ntwo output words (one on the left and one on the right) for\ntwo time steps. For each highlighted generated word, we vi-\nsualize the attention weights for words in the conversation\nhistory (i.e., words in the preceding turns and those previ-\nously generated in the output response), where darker shades\nindicate larger attention weights. As the ﬁgure indicates,\nthe attention mechanism helps learn a better RNN language\nmodel that promotes topic coherence, by learning to asso-\nciate the currently-generated word with informative context\nwords in the conversation history. As shown in Figure 3a,\nthe A-RNN generates meaningful and topically coherent re-\nsponses on the MovieTriples dataset. In comparison, the\nvanilla RNN tends to produce generic answers, such as “i\ndon’t know”. Similarly, the A-RNN follows up with useful\nquestions on the Ubuntu Troubleshootdataset (Fig. 3b).\nConclusion\nWe present an attention-RNN dialogue language model that\nincreases the scope of attention continuously as the conver-\n3257\nsation progresses (which distinguishes it from standard at-\ntention with ﬁxed scope in a sequence-to-sequence models)\nto promote topic coherence, such that each generated word\ncan be associated with its most related words in the conver-\nsation history. We evaluate this simple model on two large\ndialogue datasets (MovieTriples and Ubuntu Troubleshoot),\nand achieve the best results reported to-date on multiple\ndialogue metrics (including complementary diversity-based\nmetrics), performing better than gate-based RNN memory\nmodels. We also promote topic concentration by adopting\nLDA-based reranking, further improving performance.\nAcknowledgments\nWe thank Iulian Serban, Yi Luan, and the anonymous re-\nviewers for sharing their datasets and for their helpful dis-\ncussion. We thank NVIDIA Corporation for donating GPUs\nused in this research.\nReferences\nAmeixa, D.; Coheur, L.; Fialho, P.; and Quaresma, P. 2014. Luke,\nI am your father: dealing with out-of-domain requests by using\nmovies subtitles. In Intelligent Virtual Agents, 13–21. Springer.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural machine\ntranslation by jointly learning to align and translate. In ICLR.\nBengio, Y .; Ducharme, R.; Vincent, P.; and Jauvin, C. 2003. A\nneural probabilistic language model. Journal of Machine Learning\nResearch.\nBlei, D. M., and Lafferty, J. D. 2009. Topic models. Text mining:\nClassiﬁcation, clustering, and applications10(71):34.\nBlei, D. M.; Ng, A. Y .; and Jordan, M. I. 2003. Latent Dirichlet\nallocation. the Journal of machine Learning research.\nCrow, B. 1983. Topic shifts in couples’ conversations. Conversa-\ntional coherence: F orm, structure, and strategy136–156.\nDuˇsek, O., and Jurˇc´ıˇcek, F. 2016. A context-aware natural language\ngenerator for dialogue systems. In Proceedings of SIGDIAL.\nGoldberg, J. 1983. A move towards describing conversational co-\nherence. Conversational coherence: F orm, structure, and strategy\n25–45.\nKingma, D., and Ba, J. 2015. Adam: A method for stochastic\noptimization. In Proceedings of the International Conference on\nLearning Representations (ICLR).\nLi, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan, B. 2016a.\nA diversity-promoting objective function for neural conversation\nmodels. In NAACL.\nLi, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan, B. 2016b.\nA persona-based neural conversation model. arXiv preprint\narXiv:1603.06155.\nLuan, Y .; Ji, Y .; and Ostendorf, M. 2016. LSTM based conversa-\ntion models. arXiv preprint arXiv:1603.09457.\nManning, C. D.; Raghavan, P.; Sch ¨utze, H.; et al. 2008. Intro-\nduction to information retrieval, volume 1. Cambridge university\npress Cambridge.\nMei, H.; Bansal, M.; and Walter, M. R. 2016a. Listen, attend,\nand walk: Neural mapping of navigational instructions to action\nsequences. In Proceedings of the National Conference on Artiﬁcial\nIntelligence (AAAI).\nMei, H.; Bansal, M.; and Walter, M. R. 2016b. What to talk about\nand how? Selective generation using LSTMs with coarse-to-ﬁne\nalignment. In Proceedings of the Conference of the North American\nChapter of the Association for Computational Linguistics Human\nLanguage Technologies (NAACL HLT).\nMikolov, T. 2010. Recurrent neural network based language\nmodel. In Proceedings of Interspeech.\nMnih, V .; Hees, N.; Graves, A.; and Kavukcuoglu, K. 2014. Recur-\nrent models of visual attention. In Advances in Neural Information\nProcessing Systems (NIPS).\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2001. BLEU:\na method for automatic evaluation of machine translation. In Pro-\nceedings of the Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), 311–318.\nParikh, A. P.; T ¨ackstr¨om, O.; Das, D.; and Uszkoreit, J. 2016.\nA decomposable attention model for natural language inference.\narXiv preprint arXiv:1606.01933.\nRitter, A.; Cherry, C.; and Dolan, W. B. 2011. Data-driven response\ngeneration\nin social media. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nSanders, R. 1983. Tools for cohering discourse and their strate-\ngic utilization. Conversational coherence: F orm, structure, and\nstrategy 67–80.\nSerban, I. V .; Sordoni, A.; Bengio, Y .; Courville, A.; and Pineau,\nJ. 2016. Building end-to-end dialogue systems using generative\nhierarchical neural networks. In Proceedings of the National Con-\nference on Artiﬁcial Intelligence (AAAI).\nShang, L.; Lu, Z.; and Li, H. 2015. Neural responding machine\nfor short-text conversation. In Proceedings of the Annual Meeting\nof the Association for Computational Linguistics (ACL).\nSigman, S. 1983. Some multiple constraints placed on conver-\nsational topics. Conversational coherence: F orm, structure, and\nstrategy.\nSordoni, A.; Galley, M.; Auli, M.; Brockett, C.; Ji, Y .; Mitchell,\nM.; Nie, J.-Y .; Gao, J.; and Dolan, B. 2015. A neural network ap-\nproach to context-sensitive generation of conversational responses.\nIn Proceedings of the Conference of the North American Chapter\nof the Association for Computational Linguistics (NAACL).\nSutskever, I.; Vinyals, O.; and Lee, Q. V . 2014. Sequence to se-\nquence learning with neural networks. In Advances in Neural In-\nformation Processing Systems (NIPS).\nTran, K.; Bisazza, A.; and Monz, C. 2016. Recurrent memory net-\nwork for language modeling. In Proceedings of the Conference of\nthe North American Chapter of the Association for Computational\nLinguistics Human Language Technologies (NAACL HLT).\nVinyals, O., and Le, Q. 2015. A neural conversational model. In\nICML Deep Learning Workshop.\nWen, T.-H.; Ga ˇsi´c, M.; Mrk ˇsi´c, N.; Su, P.-H.; Vandyke, D.; and\nYoung, S. 2015. Semantically conditioned LSTM-based natural\nlanguage generation for spoken dialogue systems. In EMNLP.\nWen, T.-H.; Ga ˇsi´c, M.; Mrk ˇsi´c, N.; M. Rojas-Barahona, L.; Su,\nP.-H.; Vandyke, D.; and Young, S. 2016. Multi-domain neural net-\nwork language generation for spoken dialogue systems. InNAACL.\nWinograd, T. 1971. Procedures as a representation for data in a\ncomputer program for understanding natural language. Technical\nreport, DTIC Document.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhutdinov,\nR.; Zemel, R.; and Bengio, Y . 2015. Show, attend and tell: Neural\nimage caption generation with visual attention. In ICML.\nYao, K.; Peng, B.; Zweig, G.; and Wong, K.-F. 2016. An atten-\ntional neural conversation model with improved speciﬁcity. arXiv\npreprint arXiv:1606.01292.\n3258",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.833663821220398
    },
    {
      "name": "Conversation",
      "score": 0.7843080759048462
    },
    {
      "name": "Scope (computer science)",
      "score": 0.6940935254096985
    },
    {
      "name": "Language model",
      "score": 0.6830415725708008
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5677652955055237
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.5548273324966431
    },
    {
      "name": "Troubleshooting",
      "score": 0.5424813032150269
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5125886797904968
    },
    {
      "name": "Continuation",
      "score": 0.4879691004753113
    },
    {
      "name": "Natural language processing",
      "score": 0.48315805196762085
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4792875647544861
    },
    {
      "name": "Word (group theory)",
      "score": 0.45325595140457153
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.4185999035835266
    },
    {
      "name": "Machine learning",
      "score": 0.3215307593345642
    },
    {
      "name": "Linguistics",
      "score": 0.16739580035209656
    },
    {
      "name": "Programming language",
      "score": 0.06781801581382751
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I114027177",
      "name": "University of North Carolina at Chapel Hill",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I160992636",
      "name": "Toyota Technological Institute at Chicago",
      "country": "US"
    }
  ],
  "cited_by": 64
}