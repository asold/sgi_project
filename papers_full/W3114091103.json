{
    "title": "HinglishNLP at SemEval-2020 Task 9: Fine-tuned Language Models for Hinglish Sentiment Detection",
    "url": "https://openalex.org/W3114091103",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5004581154",
            "name": "Meghana Bhange",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5087186042",
            "name": "Nirant Kasliwal",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2952436057",
        "https://openalex.org/W2553397501",
        "https://openalex.org/W2740885753",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2154359981",
        "https://openalex.org/W2560970694",
        "https://openalex.org/W2784121710",
        "https://openalex.org/W2963918774",
        "https://openalex.org/W1946642269",
        "https://openalex.org/W2123402141",
        "https://openalex.org/W2794557536",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3115081393",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2948210185"
    ],
    "abstract": "Sentiment analysis for code-mixed social media text continues to be an under-explored area. This work adds two common approaches: fine-tuning large transformer models and sample efficient methods like ULMFiT. Prior work demonstrates the efficacy of classical ML methods for polarity detection. Fine-tuned general-purpose language representation models, such as those of the BERT family are benchmarked along with classical machine learning and ensemble methods. We show that NB-SVM beats RoBERTa by 6.2% (relative) F1. The best performing model is a majority-vote ensemble which achieves an F1 of 0.707. The leaderboard submission was made under the codalab username nirantk, with F1 of 0.689.",
    "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 934–939\nBarcelona, Spain (Online), December 12, 2020.\n934\nHinglishNLP: Fine-tuned Language Models for Hinglish Sentiment\nDetection\nMeghana Bhange\nVerloop.io\nBengaluru\nmeghana@verloop.io\nNirant Kasliwal\nVerloop.io\nBengaluru\nhi@nirantk.com\nAbstract\nSentiment analysis for code-mixed social media text continues to be an under-explored area. This\nwork adds two common approaches: ﬁne-tuning large transformer models and sample efﬁcient\nmethods like ULMFiT (Howard and Ruder, 2018). Prior work demonstrates the efﬁcacy of\nclassical ML methods for polarity detection. Fine-tuned general-purpose language representation\nmodels, such as those of the BERT family are benchmarked along with classical machine learning\nand ensemble methods. We show that NB-SVM beats RoBERTa by 6.2% (relative) F1. The best\nperforming model is a majority-vote ensemble which achieves an F1 of 0.707. The leaderboard\nsubmission was made under the codalab username nirantk, with F1 of 0.689.\n1 Introduction\nCode-mixing or code-switching refers to the use of two or more languages or speech variants together\n(Contini-Morava, 1995). This is commonly observed in informal conversations, especially those on social\nmedia, e.g. Twitter (Rudra et al., 2016) (Rijhwani et al., 2017). While a small body of work does exist on\ncode-mixing detection, this task focuses on polarity detection (Sentiment Analysis). We demonstrate the\nimpressive performance of transfer learning for the task of sentiment detection in code-mixed context. We\ndiscuss limitations of existing deep learning pre-trained models which are trained on ”monolingual” text –\nwhich has sentences in only one language.\nIn this work, we demonstrate how it is beneﬁcial to ﬁne-tune the language model (LM) for a code-mixed\nsetting like Hinglish, when Hindi is written in the Roman script. The labelled data for the sentiment\nclassiﬁer is from the task paper — (Patwa et al., 2020). It consists of a total of 17000 tweets. The\nsentiment labels are positive, negative, or neutral, and the code-mixed languages are English-Hindi. The\ntrain data after the split has 14k tweets. The validation and test data contain 3k tweets each.\n2 Data\nWe used two primary datasets. The data used for ﬁne-tuning LM is from Twitter stream. The tweet stream\ndata was used which contains ∼1.9 million Hinglish tweets. We manually sampled 3k tweets to verify\nwhat fraction of them are Hinglish. This dataset consists of ∼86.5% Hinglish tweets. The 17k tweet\nSentimix data (Patwa et al., 2020) was used to further ﬁne-tune the sentiment classiﬁer. The Sentimix\ndata contains 14,000 tagged tweets marked as positive, negative and neutral for training, and 3,000 for\ndevelopment and testing each.\n2.1 Mining and Filtering Twitter Corpus\nPre-training LM require large datasets. In order to enable this, we gathered ∼1.9 Million tweets from the\nTwitter 1% sample stream for the entire year of 2018. We curated a seed dictionary of Hinglish words and\ntheir spelling variants. Next, we calculate the Jaccard Index (Jaccard, 1912) between our seed dictionary\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n935\nand every tweet. For values above 0.6, we mark the tweet as “Hinglish tweet”. This threshold value 0.6\nwas selected empirically, by evaluating Jaccard values for 200 tweets.\nNext, every token which is present in the tweet, but missing in our dictionary is marked as a Candidate\ntoken Ct. We remove duplicates to get our set of unique candidate tokens C. For every unique Ct in C,\nwe manually review and add to our dictionary. A known limitation of this iterative-expansion dictionary\nbased approach is that it starts out with a bias for smaller tweets with fewer total tokens. Hence, we\nrepeated this exercise in batches of 10,000 tweets each – till we saw two batches of full 280 character\ntweets. We marked these tweets as “highly likely” to be Hinglish. These were roughly 160,000 tweets.\nA secondary split of roughly 380,000 tweets was marked as “possibly” Hinglish. Both of these were\nprimarily used for training or ﬁne-tuning the LM backbone.\nThe ∼1.9M tweets are composed of 3 different splits, with differing Hinglish percentage. The ﬁrst split\nof 162K tweets is enriched to 86.5% Hinglish, with 13% tweets being empty, or non-Hinglish in other\nways. The second split of 384K tweets is enriched to 89.6%. The remaining ˜1.4M tweets are expected to\nhave 83% Hinglish tweets. We randomly pulled 1K tweets from each of these splits to get these estimates.\nThe estimate is hence, prone to sampling biases/errors. To re-iterate, this added dump is neither tagged\nwith polarity nor pure Hinglish.\n2.2 Release\nThe dataset is released on Github.1 Since Twitter discourages releasing the text directly, tweet ids are\nshared. This leaves the user to pull the speciﬁc tweets using Twitter’s Developer API.\n2.3 Cleaning and Pre-processing\nWe de-duplicated the 1.9M tweets corpus using a string equality check. We also de-duplicated tweets using\nthe meta-information in the JSON when a “retweeted” text is included twice. The text was pre-processed\nbefore data were introduced to the model. The pre-processing included removal of both external links and\nshortened twitter links. The “@” was replaced with with “mention”. Similarly, “#” was replaced with\nthe word “hashtag”. Emojis were converted to text equivalent using the emoji package (Taehoon Kim\nand Kevin Wurster, 2019). During this stage, both the datasets (SemEval and Twitter Large Supervised\nDataset) are pre-processed with identical code, both during training/ﬁne-tuning and inference.\n3 Experiments\n3.1 Training and Fine-tuning\nThe Language Models (LMs) were ﬁne-tuned on the entire Twitter Stream Sample. We used held out\nabout 10% for measuring LM perplexity. Classiﬁers were trained using 14000 tweets from the 17000\ntweets in the SemEval training corpus. The linear layers were ﬁne-tuned on the SemEval training corpus\nfor 3 epochs for all experiments. The ﬁne-tuning parameters for the BERT-family sentiment classiﬁers are\nreferenced in Table 1. For Attention dropout and hidden dropout, the parameters were empirically chosen\nusing random grid-search with a range of 0.1 to 0.9. The range considered for Adam Epsilon was 1e-8 to\n9e-8 with 1e-8 granularity. Learning rates varied from 1e-7 to 1e-4. These parameters were combined\nwith two learning rate schedulers, a linear learning rate scheduler and a cosine learning rate scheduler. The\ntraining for models which took place in two steps: First, the pre-trained language model was ﬁne-tuned\nusing the 1.9M tweets. Second, this ﬁne-tuned deep LM was used as an encoder for training the polarity\nclassiﬁer using the 14K tagged tweets from Sentimix.\n3.2 Evaluation\nDuring the competition, we used multiple methods of evaluation and different train-test splits. In this\nwork, the test dataset refers to the ofﬁcially released test set of 3,000 tweets. The performance numbers\nhave been updated to reﬂect the same. We chose to ignore the validation set from SemEval for evaluation\nbecause most of our LMs had consistently very high performance of 0.95 F1 or more on the set. The F1\n1https://github.com/NirantK/Hinglish\n936\nParameter BERT Hinglish RoBERTa DistilBERT\nMultilingual Fine-tuned BERT\nAttention Dropout Probability 0.4 0.4 0.1 0.6\nHidden Dropout Probability 0.3 0.3 0.1 0.6\nAdam Epsilon 3e-8 1e-8 5e-8 1e-8\nWarmup Steps 100 100 0 100\nMaximum Learning Rate 5e-7 5e-7 4e-5 3e-5\nLearning Rate Scheduler linear linear linear cosine\nTable 1: Fine-tuning Parameters for BERT Family Classiﬁers\nscore used for internal evaluation is macro-F1 while the leaderboard submission uses weighted-F1. The\nF1 scores that are shown in the result table are from internal evaluation and thus are macro-f1.\n4 Modeling Approaches\n4.1 NB-SVM\nNBSVM is the approach proposed by Wang and Manning (2012), which performs well on text classiﬁca-\ntion in tasks like sentiment classiﬁcation. It takes a linear model such as SVM (or logistic regression)\nand incorporates the possibilities of Bayesian by replacing terms with Naive Bayes log-count ratios. The\nNBSVM implementation was borrowed as is from zaxcie (2018). The motivation for using NBSVM is\nthat they are comparatively faster to train as opposed to deep learning models. We chose C = 4, the\ninverse regularization parameter.\n4.2 ULMFiT: Universal Language Model Fine-tuning for Text Classiﬁcation\nWe used AWD-QRNN (Bradbury et al., 2016) instead of AWD-LSTM (Howard and Ruder, 2018) for\npre-training and ﬁne-tuning. We used Sentence Piece(Kudo and Richardson, 2018) for text tokenisation.\nThe intent was to capture sub-word level features. V ocabulary Size of the sentencepiece tokenizer was\n8000 and was trained on 540k tweets to save compute time. For the ULMFiT-QRNN model batch size\nof 1024 was used while training both the LM and classiﬁer (linear) layers. AWD-LSTM gives an F1\n0.48 on the test set where as AWD-QRNN performs with an F1 of 0.650. The hypothesis which could\nexplain this is that tweet length, which is typically less than 140 characters, is too short for LSTM is learn\na meaningful pattern.\n4.3 BERT Multilingual\nBERT-base-multilingual-cased(Devlin et al., 2018), without any ﬁne-tuning of LM on Hinglish data,\nwas used to train the sentiment classiﬁer. It is trained on cased text in the top 104 languages with the\nlargest Wikipedia corpora. The linear layers were trained/ﬁne-tuned without updating the frozen backbone\nfor 3 epochs.\n4.4 Hinglish Fine-tuned BERT\nThe base model for ﬁne-tuning BERT LM on Hinglish data was BERT-base-multilingual-cased (Devlin et\nal., 2018). Both the backbone and linear layers of the LM were ﬁne-tuned. This was on a pre-processed\nTwitter Stream Sample (described in the previous section) over 26,000 iterations. It was trained for a total\nof 4 epochs. Training batch size was four and vocab size 119,547. The perplexity of the ﬁne-tuned LM\nwas 8.2. The trained BERT tokenizer and model were utilized for ﬁne-tuning classiﬁer.\n4.5 RoBERTa\nRoBERTa (Liu et al., 2019) is a robustly optimized BERT pre-training approach. It is trained over longer\nsequences and removes the next sentence prediction task from BERT pre-training. The base model for\nﬁne-tuning the LM-backbone for RoBERTa on Hinglish data was RoBERTa-base. The LM was ﬁne-tuned\n937\nFigure 1: Once pre-processed, the data is used for predicting results which are then passed to the ensemble\ndescribed in section 4.7.\nModel Accuracy Precision Recall F1 LM-Perplexity\nMajority Vote (Ensemble) 0.704 0.709 0.707 0.707 –\nDistilBERT-Base-cased 0.685 0.690 0.685 0.687 6.51\nLogistic Regression Funnel (Ensemble) 0.678 0.678 0.696 0.684 –\nBERT-Base-Multilingual-Cased 0.680 0.692 0.678 0.677 8.2\nNB-SVM (Ensemble) 0.667 0.666 0.685 0.673 –\nULMFit A WD-QRNN 0.645 0.646 0.660 0.650 21.0\nRoBERTa-base 0.630 0.629 0.644 0.635 7.54\nTable 2: Results on sentiment classiﬁcation where the F1 is performances of the model on test-data\nprovided by Sentimix. The models with LM-backbones are provided with the perplexity of the ﬁne-tuned\nLM where as the ones without are denoted by NA.\non a pre-processed unsupervised twitter dataset over 25,000 iterations. It was trained for a total of 3\nepochs. Training batch size was four and vocab size 50265. The perplexity of this model was 7.54.\n4.6 DistilBERT\nDistilBERT (Sanh et al., 2019) uses the technique of knowledge distillation to improve the performance\nof BERT and create a smaller distilled version of the model. The LM-backbone was ﬁne-tuned on a\npre-processed unsupervised twitter dataset over 49,000 iterations. It was trained for a total of 6 epochs.\nTraining batch size was four and vocab size 28996. The perplexity of the ﬁne-tuned LM-backbone for\ndistilBERT was 6.51 and the base model used for ﬁne-tuning the LM was distilbert-base-cased.\n4.7 Ensemble\nFor the ﬁnal submissions, three variations of BERTs and two variations of DistilBERT were used. These\nwere the top 5 selected based on their validation accuracy. For the ensemble, Weighted Majority V oting,\nby using the prediction conﬁdence (0 to 1 scale) as the weight; The ensemble methodology and its usage\nin our case is described in Figure 1.\n5 Results\nThe result for the experiments are summarized in Table 2. Out of all the techniques used on test-data,\nWeighted majority vote ensemble with LR funneling gained a signiﬁcant edge when it comes to F1 score.\nTraditional machine learning models like NB-SVM show a comparative performance.\n6 Future Work\nThere are three main incremental directions of improvements: data, methods, adopting techniques from\ntext classiﬁcation. For instance, initial tweet data had a lot of truncated tweets, using tweet ids to get an\n938\nentire tweet would enrich our inputs. The training data can also be augmented in a wide variety of ways\nsuch as using vector similarity (Ma, 2019).\nWe can investigate other methods which might help in understanding missed case. Sentence embeddings\nfor Hinglish, similar to InferSent (Conneau et al., 2017) or Universal Sentence Encoding (Cer et al.,\n2018) may be promising, in addition to Skip Thought or other sentence vectorisation methods, as well as\nexploring the performance of models which do not focus on transfer learning like R-CNN, and LSTMs.\nLastly, a wide variety of deep learning tricks and methods could be used, such as label smoothing\n(M¨uller et al., 2019), which can help in generalising better beyond the small training sample.\n7 Conclusion\nWe demonstrate that ensembles of classical Machine Learning models, even NB-SVM exhibit competitive\nperformance and can in fact be better than some Transformer baselines. It is still worthwhile to implement\nsimple classical baselines. Additionally, we hope that the released dataset and models 2 will encourage\nreaders to investigate this further.\nReferences\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. 2016. Quasi-recurrent neural networks.\nCoRR, abs/1611.01576.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario\nGuajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal\nsentence encoder. CoRR, abs/1803.11175.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc Barrault, and Antoine Bordes. 2017. Supervised learning\nof universal sentence representations from natural language inference data. ArXiv, abs/1705.02364.\nEllen Contini-Morava. 1995. Duelling languages: Grammatical structure in codeswitching. Journal of Linguistic\nAnthropology, 5(2):246–247.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirec-\ntional transformers for language understanding. CoRR, abs/1810.04805.\nJeremy Howard and Sebastian Ruder. 2018. Fine-tuned language models for text classiﬁcation. CoRR,\nabs/1801.06146.\nPaul Jaccard. 1912. The distribution of the ﬂora in the alpine zone.1. New Phytologist, 11(2):37–50.\nTaku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. CoRR, abs/1808.06226.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692.\nEdward Ma. 2019. Nlp augmentation. https://github.com/makcedward/nlpaug.\nRafael M ¨uller, Simon Kornblith, and Geoffrey E. Hinton. 2019. When does label smoothing help? CoRR,\nabs/1906.02629.\nParth Patwa, Gustavo Aguilar, Sudipta Kar, Suraj Pandey, Srinivas PYKL, Bj¨orn Gamb¨ack, Tanmoy Chakraborty,\nThamar Solorio, and Amitava Das. 2020. Semeval-2020 task 9: Overview of sentiment analysis of code-mixed\ntweets. In Proceedings of the 14th International Workshop on Semantic Evaluation (SemEval-2020), Barcelona,\nSpain, December. Association for Computational Linguistics.\nShruti Rijhwani, Royal Sequiera, Monojit Choudhury, Kalika Bali, and Chandra Shekhar Maddila. 2017. Estimat-\ning code-switching on twitter with a novel generalized word-level language detection technique. InProceedings\nof the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n1971–1982, Vancouver, Canada, July. Association for Computational Linguistics.\n2https://github.com/NirantK/Hinglish\n939\nKoustav Rudra, Shruti Rijhwani, Raﬁya Begum, Kalika Bali, Monojit Choudhury, and Niloy Ganguly. 2016.\nUnderstanding language preference for expression of opinion and sentiment: What do Hindi-English speakers\ndo on twitter? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,\npages 1131–1141, Austin, Texas, November. Association for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. ArXiv, abs/1910.01108.\nTaehoon Kim and Kevin Wurster. 2019. emoji.\nSida Wang and Christopher D. Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic clas-\nsiﬁcation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short\nPapers - Volume 2, ACL ’12, page 90–94, USA. Association for Computational Linguistics.\nzaxcie. 2018. Nb-svm: Strong linear baseline."
}