{
  "title": "An Investigation of Suitability of Pre-Trained Language Models for Dialogue Generation – Avoiding Discrepancies",
  "url": "https://openalex.org/W3176740717",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5040786145",
      "name": "Yan Zeng",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A5018977183",
      "name": "Jian‐Yun Nie",
      "affiliations": [
        "Université de Montréal"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W3040352674",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3035451444",
    "https://openalex.org/W4287749601",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2972916088",
    "https://openalex.org/W2593751037",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2997892440",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W4288624561",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W3170012708",
    "https://openalex.org/W3023786569",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W3034337319",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3038115231",
    "https://openalex.org/W2053154970",
    "https://openalex.org/W2913443447",
    "https://openalex.org/W3103616906",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2965617855",
    "https://openalex.org/W3000779003"
  ],
  "abstract": "Pre-trained language models have been widely used in response generation for open-domain dialogue.These approaches are built within 4 frameworks: Transformer-ED, Transformer-Dec, Transformer-MLM and Transformer-AR.In this study, we experimentally compare them using both large and small-scale data.This reveals that decoder-only architecture is better than stacked encoder-decoder, and both leftto-right and bi-directional attention have their own advantages.We further define two concepts of model discrepancy, which provides a new explanation to the model performance.As discrepancies may hinder performance, we propose two solutions to reduce them, which successfully improve the model performance.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4481–4494\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4481\nAn Investigation of Suitability of Pre-Trained Language Models for\nDialogue Generation – Avoiding Discrepancies\nYan Zeng\nDIRO, Universit´e de Montr´eal\nyan.zeng@umontreal.ca\nJian-Yun Nie\nDIRO, Universit´e de Montr´eal\nnie@iro.umontreal.ca\nAbstract\nPre-trained language models have been widely\nused in response generation for open-domain\ndialogue. These approaches are built within\n4 frameworks: Transformer-ED, Transformer-\nDec, Transformer-MLM and Transformer-AR.\nIn this study, we experimentally compare them\nusing both large and small-scale data. This re-\nveals that decoder-only architecture is better\nthan stacked encoder-decoder, and both left-\nto-right and bi-directional attention have their\nown advantages. We further deﬁne two con-\ncepts of model discrepancy, which provides\na new explanation to the model performance.\nAs discrepancies may hinder performance, we\npropose two solutions to reduce them, which\nsuccessfully improve the model performance.\n1 Introduction\nIt has been shown (Wolf et al., 2019) that lever-\naging a pre-trained Language Model (LM) based\non transformer can achieve excellent performance\nfor dialogue generation. Different approaches\nhave been proposed recently, which can be catego-\nrized into 4 frameworks: Transformer-ED(Zheng\net al., 2019), an encoder-decoder Transformer,\nTransformer-Dec (Wolf et al., 2019; Lin et al.,\n2020), Transformer-MLM (Dong et al., 2019) and\nTransformer-AR (Bao et al., 2019; Shuster et al.,\n2019). The latter three all utilize a decoder-only\narchitecture. Besides, Trans-Dec uses left-to-right\nattention for both source and target side, while\nTrans-MLM and Trans-AR employ bi-directional\nattention on the source side to encode dialogue his-\ntory. Due to this difference, Trans-Dec only utilizes\nleft-to-right pre-trained models, e.g. GPT-2 (Rad-\nford et al., 2019), while Trans-MLM/AR are based\non the pre-trained models applying bi-directional\nattention (on the source side), e.g. BERT (Devlin\net al., 2018). The difference between Trans-MLM\nand Trans-AR is that Trans-MLM uses masked\nlanguage modeling while Trans-AR uses auto-\nregressive objective.\nRecent studies have explored pre-training dia-\nlogue models using large-scale Reddit/Twitter data\n(Adiwardana et al., 2020; Roller et al., 2020). It is\nthen straightforward to ﬁne-tune the models for a\nspeciﬁc dialogue task. However, in practice, there\nmay not always be enough data for pre-training. In\nsome cases, we still need to exploit a pre-trained\nLM. For example, some studies do further pre-\ntraining for dialogue based on a pre-trained LM\n(Zhang et al., 2019; Dong et al., 2019; Bao et al.,\n2019; Shuster et al., 2019), and some studies that\ndo multi-task learning (e.g. on dialogue and ques-\ntion answering) can only ﬁne-tune based on a pre-\ntrained LM (Lin et al., 2020; Zeng and Nie, 2021).\nThen, a critical question is how to best exploit a\npre-trained LM for dialogue generation. On this\nquestion, we have contradictory beliefs in the liter-\nature: some researchers believe that Trans-Dec is\nappropriate because it uses a left-to-right language\nmodel that corresponds well to the dialogue gen-\neration task (Zhang et al., 2019; Lin et al., 2020),\nwhile some others (Dong et al., 2019; Bao et al.,\n2019) show that Trans-MLM/AR ﬁne-tuning BERT\ncan also achieve state-of-the-art performance.\nIn this study, we aim to address the above ques-\ntion. To do it, we ﬁrst compare the 4 frameworks\nwith the same setting on 3 datasets, each with large\nand small scale training data. Our results on large-\nscale datasets show that Trans-ED that applies the\nstacked encoder-decoder architecture does not pro-\nduce competitive results against the others that use\na decoder-only architecture. Trans-Dec/AR gener-\nate the most appropriate responses. However, ac-\ncording to automatic metrics, Trans-Dec generates\nmost diverse responses while Trans-AR produce re-\nsponses most similar to the ground-truth. This may\nbe due to the fact that uni-directional attention does\nnot have constraint from the right side context and\n4482\nthus is more ﬂexible, while bi-directional attention\non source side can better model dialogue context.\nIn contrast, the results on small-scale datasets re-\nveal an important aspect, namely, the discrepancies\nthat may occur between the pre-training and the\nﬁne-tuning processes. We then try to explain the\nperformances of the 4 frameworks with respect to\nthe discrepancies.\nThe concept of model discrepancy has been\nbrieﬂy mentioned in Yang et al. (2019) to mean\nthat the model has been trained in a way, but used\nin a different way. However, the problem has not\nbeen investigated in depth. In this work, we go\nfurther in this direction and deﬁne two discrepan-\ncies: pretrain-ﬁnetune discrepancy which means\nthe differences in architecture and loss function be-\ntween pre-training and ﬁne-tuning, and ﬁnetune-\ngeneration discrepancy which means that the way\nthe model is used in generation (inference/test) is\ndifferent from the way it has been trained. For\nthe 4 frameworks, except Trans-Dec, they all have\nsome pretrain-ﬁnetune discrepancies. For example,\nTrans-AR relies on BERT pre-trained using bidi-\nrectional attention, but has to limit it to left-to-right\nattention on the target side during ﬁne-tuning. Only\nTrans-MLM has ﬁnetune-generation discrepancy\nbecause of MLM objective: during training, the\nmodel input has random masks, while in the gener-\nation process, the input does not contain masks.\nDiscrepancies might affect the model perfor-\nmance since models with such discrepancies cannot\nbest exploit the pre-trained model or employ the\nﬁne-tuned model. Our experiments on small-scale\ndatasets show that the performance of Trans-AR\nthat have larger pretrain-ﬁnetune discrepancy drops\nmore sharply than Trans-MLM. Trans-Dec/MLM\nthat have small pretrain-ﬁnetune discrepancy have\nclear advantage over other frameworks accord-\ning to human evaluation. It becomes clear that\ndiscrepancies hinder the performance of a dia-\nlogue model. To alleviate the problems, we pro-\npose 2 approaches to respectively reduce pretrain-\nﬁnetune and ﬁnetune-generation discrepancies of\nTrans-MLM, aiming at improving its performance.\nOur experiments show that both methods bring\nsome improvement. In particular, by eliminating\nﬁnetune-generation discrepancy of Trans-MLM,\nour approach signiﬁcantly outperforms previous\nmethods in most automatic metrics, and achieves\ncomparable performance to Trans-Dec in human\nevaluation that uses much larger dataset for pre-\ntraining. These results conﬁrm that discrepancies\nare indeed an important factor that inﬂuences the\neffectiveness of leveraging a pre-trained LM for a\nsequence-to-sequence task, and should be allevi-\nated.\nThe contributions in this work are as follows:\n• We compare the four commonly used frame-\nworks that utilize pre-trained language mod-\nels for open-domain dialogue generation on 3\npublic datasets each in large and small scale.\nand we analyze each framework based on the\nexperimental results.\n• We introduce the concept of pretrain-ﬁnetune\ndiscrepancy and ﬁnetune-generation discrep-\nancy, and we examine the discrepancies of\neach framework.\n• We propose two methods to reduce discrep-\nancies1, yielding improved performance. It\nis the ﬁrst investigation that shows explicitly\nthe phenomenon of model discrepancy and its\nimpact on performance.\n2 Pre-training Based Frameworks\nWe start with a brief description of the 4 frame-\nworks for dialogue generation based on pre-trained\nmodels. More details are provided in Appendix A.\nWe examine the pretrain-ﬁnetune discrepancy of\neach framework. Figure 1 and Table 1 provide an\noverview.\n2.1 Trans-ED\nTrans-ED discussed in this paper is an encoder-\ndecoder architecture used by ConvAI2 (Dinan et al.,\n2019) champion 2. The decoder of Trans-ED is\nstacked upon the encoder outputs, while in other\ndecoder-only frameworks, all hidden states of the\nsource side are utilized in the decoding part. The\nframework shares the encoder and the decoder and\ninitializes the parameters with GPT (Radford et al.,\n2018). In this case, the pretrain-ﬁnetune discrep-\nancy comes from the bi-directional attention in\nthe encoder since GPT is a left-to-right language\nmodel. This framework is not commonly used for\nﬁne-tuning on a dialogue task. In practice, more\nefﬁcient variants of Trans-ED are recently used for\nextremely large-scale dialogue pre-training from\n1The code is available at: https://github.com/\nzengyan-97/Transformer-MLM-DiffFree\n2https://github.com/atselousov/\ntransformer_chatbot\n4483\nFigure 1: Architectures of 4 pre-training based Transformers for dialogue generation.\nTrans-ED Trans-Dec Trans-MLM Trans-AR\nPre-trained LM GPT GPT-2 BERT BERT\nArchitecture encoder-decoder decoder-only decoder-only decoder-only\nSource Side Attn. bi-directional left-to-right bi-directional bi-directional\nTarget Side Attn. left-to-right left-to-right left-to-right left-to-right\nObjective auto-regressive auto-regressive MLM auto-regressive\nTable 1: Key characteristics of the 4 pre-training based Transformers. Characteristics in red are inconsistent\nbetween pre-training and ﬁne-tuning.\nscratch. For example, Adiwardana et al. (2020)\nutilizes Evolved Transformer to prune redundant\nconnections, and Roller et al. (2020) employs only\n2 encoder layers and 24 decoder layers of standard\nTransformer (Vaswani et al., 2017).\n2.2 Trans-Dec\nTrans-Dec is a left-to-right decoder-only architec-\nture, and it utilizes GPT-2 (Radford et al., 2019).\nThus, there is no pretrain-ﬁnetune discrepancy in\nterms of architecture and loss function. This frame-\nwork is widely applied for ﬁne-tuning on a dialogue\ntask. However, it encodes dialogue history using\nonly left-to-right attention, which limits the scope\nof context, resulting in a partial context modeling.\n2.3 Trans-MLM and AR\nThese two frameworks have an identical decoder-\nonly architecture that employs different self-\nattention masks for the source and target side: they\nuse bi-directional attention on the source side to en-\ncode dialogue history and left-to-right attention on\nthe target side. The only difference between them\nis the objective function: Trans-MLM masks some\ntokens at the target side and tries to predict them,\nwhile Trans-AR uses auto-regressive objective that\ntries to predict the next tokens successively. BERT\nis often exploited by the two frameworks, which\nis a bi-directional architecture using MLM as the\npre-training objective. Thus, the pretrain-ﬁnetune\ndiscrepancy of Trans-MLM/AR comes from the\nleft-to-right attention on the target side. Addition-\nally, Trans-AR applies the auto-regressive objec-\ntive, which is different from the MLM used in the\npre-training.\n2.4 Applications of the Frameworks\nThe four frameworks we described have been\nwidely applied to dialogue generation. For per-\nsonalized response generation, Wolf et al. (2019)\nuses Trans-Dec and Zheng et al. (2019) utilizes\nTrans-ED. Lin et al. (2019) uses Trans-Dec for\nempathetic response generation. Zeng and Nie\n(2021) proposes a multi-task learning approach\nbased on Trans-MLM for conditioned dialogue\ngeneration. Meanwhile, some studies propose to\nfurther pre-train the model using large-scale dia-\nlogue data based on a pre-trained language model:\nZhang et al. (2019) trains Trans-Dec on 147M Red-\ndit data based on GPT-2, Dong et al. (2019) trains\nTrans-MLM on natural language understanding and\ngeneration datasets based on BERT, Shuster et al.\n(2019) trains Trans-AR on large-scale Reddit data\nand then jointly trains on 12 dialogue sub-tasks\nbased on BERT, and Bao et al. (2019) trains a vari-\nant of Trans-AR on large-scale Reddit and Twitter\ndata based on BERT. Some recent studies have in-\ncreased the model size to billions of parameters and\nutilize even more training data, e.g. Reddit, to train\na conversational model from scratch (Adiwardana\net al., 2020; Roller et al., 2020; Bao et al., 2020b).\nIn general, these studies show that all the 4\nframeworks can produce good results, and increas-\ning the model size and training data is an effective\nmethod to further improve performance. However,\nbehind the success story, the question of suitability\n4484\nTwitter Ubuntu Reddit\nTrain Set 2M 1.5M 3M\nValid Set 60K 30K 80K\nTest Set 20K 20K 20K\nTable 2: Key characteristics of the three public datasets.\nFor each dataset, we also evaluate model performance\nusing 100K training data and the same test set.\nof a framework is masked. To investigate this ques-\ntion, we do not follow the current trend to increase\nthe model size and training data. Instead, we are\ninterested in the behaviors of different frameworks\non the same datasets and to understand the reasons.\n3 Experiments\n3.1 Datasets\nWe use all the three large-scale unlabeled dialogue\ndatasets in Shuster et al. (2019). Some important\ncharacteristics of the datasets are summarized in\nTable 2. We are interested in the behaviors of the\nmodels in two cases: 1) further pre-training on\nlarge dialogue data based on a pre-trained LM; and\n2) ﬁne-tuning on a small dialogue corpus based on\na pre-trained LM. Our large datasets contain a few\nmillion samples, and the small datasets consist of\n100K samples3. Although the datasets are smaller\nthan those used in several previous studies, we\nbelieve that a comparison of different models on\nthe same data, and the contrast between large and\nsmall datasets, can reveal interesting trends, which\nwe will explain with respect to discrepancies.\nSpeciﬁcally, we choose the following 3 datasets:\nTwitter Dialogue Corpus4 is collected from Twit-\nter consisting of 2.6M (message, response) pairs.\nWe ﬁltered out samples with history length longer\nthan 72 words (to limit the computation) or shorter\nthan 6 words (not enough information). Sam-\nples whose response is longer than 36 words or\nshorter than 6 words are also removed. As a re-\nsult, 2M samples are kept. Reddit Conversational\nCorpus 5(Dziri et al., 2019) is a 3-turn conversa-\ntional dataset collected from 95 selected subreddits.\nUbuntu Dialogue Corpus V2.0 6 (Lowe et al.,\n2017) contains two-person conversations extracted\n3Labeled datasets such as persona (Zhang et al., 2018) and\nemotion (Rashkin et al., 2019) are usually in similar scale.\n4https://github.com/Marsan-Ma-zz/chat_\ncorpus\n5https://github.com/nouhadziri/THRED\n6https://github.com/rkadlec/\nubuntu-ranking-dataset-creator\nfrom the Ubuntu chat logs of technical support for\nvarious Ubuntu-related problems.\n3.2 Implementation Details\nWe use open-source implementations for all four\nframeworks. Only minor adaptations (e.g. for\ndata loading) have been made. The pre-trained\nlanguage models used by these frameworks in pre-\nvious studies have comparable number of param-\neters (∼110M), while the pre-training data are in\ndifferent scales: Trans-ED < Trans-MLM/AR <\nTrans-Dec. We assume that the difference is trivial\nwhen there are millions of dialogue data. In this\nstudy, we use the same data for all the frameworks.\nMore implementation details of each framework\nand the full comparison among pre-trained LM are\ngiven in Appendix C.\nWe also equip all frameworks with an identical\ndecoding script7 to avoid extra factor affecting the\ngeneration quality, which uses beam search with\nbeam size of 4, prevents duplicated uni-grams, and\nsets minimum response length that encourages di-\nverse generation as in Roller et al. (2020). The\nminimum response length is set to make the av-\nerage length of generated responses match with\nthe average target length of the dataset. Genera-\ntion results are evaluated after applying an identical\nword tokenization method. With two P100 GPU\ndevices, the maximum input length is set to 128,\nand we ﬁne-tune all models for 6 epochs and ap-\nply early stopping based on the performance on\nvalidation set. Our methods (PF-free and FG-free,\nwhich will be described in Section 4.1) do not add\nparameters or increase runtime in comparison with\nTrans-MLM.\n3.3 Evaluation\nAutomatic Metrics We compare the similarity\nbetween generated responses and ground-truth re-\nsponses using8: BLEU (Papineni et al., 2002) eval-\nuating how many n-grams (n=1,2,3) overlapped;\nCIDEr (Vedantam et al., 2015) utilizing TF-IDF\nweighting for each n-gram. Besides, we evaluate re-\nsponse diversity using Distinct (denoted Dist) (Li\net al., 2016) that indicates the proportion of unique\nn-grams (n=1,2) in the entire set of generated re-\nsponses.\n7https://github.com/microsoft/unilm/\n8We use an open-source evaluation tool: https://\ngithub.com/Maluuba/nlg-eval\n4485\nModel BLEU-1 BLEU-2 BLEU-3 CIDEr Dist-1 Dist-2 avgLen\nSEQ2SEQ-MMI 10.872 (**) 4.555 (**) 2.259 (/) 0.119 (/) 0.008 (**) 0.028 (**) 10.6\nTrans-ED 15.319 (**) 4.877 (**) 2.037 (**) 0.097 (**) 0.014 (**) 0.063 (**) 19.0\nTrans-Dec 14.363 (**) 4.861 (**) 2.120 (*) 0.101 (**) 0.031 (**) 0.178 (/) 19.9\nTrans-MLM 13.749 (**) 4.253 (**) 1.715 (**) 0.061 (**) 0.018 (**) 0.106 (**) 29.3\nTrans-AR 15.694 5.221 2.272 0.119 0.029 0.164 18.9\nFG-free 15.659 (/) 5.176 (/) 2.200 (/) 0.112 (/) 0.027 (**) 0.147 (*) 18.7\nTrans-ED 14.813 (**) 4.249 (**) 1.330 (**) 0.066(**) 0.001 (**) 0.004 (**) 18.4\nTrans-Dec 13.805 (**) 4.407 (**) 1.787 (**) 0.092(*) 0.033 (**) 0.195 (**) 20.2\nTrans-MLM 15.487(**) 4.766(**) 1.814(**) 0.092 (*) 0.016(**) 0.080(**) 19.7\nTrans-AR 15.213 (**) 4.700 (**) 1.767 (**) 0.090(**) 0.019(**) 0.091(**) 18.8\nPF-free 15.880 (*) 4.970 (*) 1.868 (*) 0.093 (*) 0.022 (**) 0.114 (*) 15.7\nFG-free 16.395 5.218 2.043 0.101 0.026 0.129 16.2\nPF&FG-free 15.714 (*) 4.916 (*) 1.780 (**) 0.093 (*) 0.020 (**) 0.111 (*) 18.4\nTable 3: Evaluation results on large-scale (upper half) and small-scale (lower half) Twitter dataset. PF-free denotes\nthe method with reduced pretrain-ﬁnetune discrepancy of Trans-MLM. FG-free denotes the method that eliminates\nﬁnetune-generation discrepancy of Trans-MLM. Two-sided t-test compares each method with the one without ()\nsign, which is usually the best performer. Scores are denoted with * ( p <0.05) or ** (p <0.01) for statistically\nsigniﬁcant differences, and / for insigniﬁcant differences.\nModel BLEU-1 BLEU-2 BLEU-3 CIDEr Dist-1 Dist-2 avgLen\nSEQ2SEQ-MMI 12.056(**) 5.512(**) 2.841(**) 0.142(**) 0.005(**) 0.024(**) 9.8\nHRED-MMI 13.518(**) 4.564(**) 1.947(**) 0.060(**) 0.001(**) 0.003(**) 13.6\nTrans-ED 19.295(/) 6.712(**) 2.986(*) 0.125(**) 0.010(**) 0.069(**) 16.8\nTrans-Dec 18.974(*) 6.911(/) 3.022(*) 0.130(*) 0.018(**) 0.134(**) 18.0\nTrans-MLM 17.574(**) 5.884(**) 2.552(**) 0.096(**) 0.012(**) 0.097(**) 25.5\nTrans-AR 20.103 7.270 3.339 0.143 0.017 0.127 16.8\nFG-free 19.774 (/) 7.045 (/) 3.213 (/) 0.139 (/) 0.016 (*) 0.115 (/) 17.7\nTrans-ED 14.195(**) 4.533(**) 1.756(**) 0.074(**) 0.003(**) 0.012(**) 16.3\nTrans-Dec 17.944(**) 6.360(*) 2.727(*) 0.121(/) 0.018(**) 0.143(**) 18.3\nTrans-MLM 18.338(*) 6.018(**) 2.480(**) 0.108(**) 0.011(**) 0.066(**) 17.0\nTrans-AR 19.005 (*) 6.431 (/) 2.733 (*) 0.114(*) 0.012(**) 0.078(**) 17.4\nPF-free 19.116 (*) 6.356 (*) 2.684 (*) 0.118 (/) 0.012 (**) 0.086 (*) 16.7\nFG-free 18.884 6.530 2.869 0.125 0.014 0.095 17.3\nPF&FG-free 19.024 (*) 6.448 (/) 2.740 (*) 0.118 (/) 0.012 (**) 0.087 (*) 17.1\nTable 4: Evaluation results on large-scale (upper half) and small-scale (lower half) Ubuntu dataset.\nHuman Evaluation Furthermore, we ask human\nevaluators to rate a response in {0, 1, 2}. 2 repre-\nsents a coherent and informative response. Details\nare given in Appendix D. We also do a pair-wise\nevaluation to compare two models and indicate\nwhich one is better. To reduce time cost, we only\nperform human evaluations on Twitter and Reddit\ndatasets that are closer to daily dialogue. However,\nduring evaluation, we observe that ∼65% Reddit\ndata are professional discussions that are difﬁcult to\nunderstand. The percentage is ∼30% for Twitter\ndata. These test samples are discarded, and at the\nend the test set for each dataset consists of 200 ran-\ndom samples. The inter-rater annotation agreement\nin Cohen’s kappa (Cohen, 1960) is0.44 and 0.42\nfor Twitter and Reddit, which indicates moderate\nagreement.\nIn addition to the 4 frameworks, we also in-\nclude two general RNN-based baseline frameworks\n– SEQ2SEQ-MMI (Li et al., 2016) and HRED-MMI\n(Serban et al., 2016) to show how pre-trained mod-\nels perform against them.\n3.4 Architecture Analysis\nWe ﬁrst examine architecture appropriateness on\nthe large-scale data setting, since when data are lim-\nited pretrain-ﬁnetune discrepancy and the size of\npre-training data may strongly inﬂuence the results.\nAppendix E shows some generation samples. Our\nglobal observation is that Trans-Dec and Trans-AR\nare the best choice for large-scale data setting, e.g.\nfurther dialogue pre-training based on a pre-trained\nLM.\nLeft-to-Right Only vs. Bi-Direction on the\nSource Human evaluation results in response ap-\npropriateness (Table 6 and 7) show that Trans-Dec\nand Trans-AR generate most appropriate responses.\nAccording to automatic metrics, Trans-AR apply-\ning bi-directional attention on the source side ob-\ntains the highest BLEU and CIDEr scores on all\n4486\nModel BLEU-1 BLEU-2 BLEU-3 CIDEr Dist-1 Dist-2 avgLen\nSEQ2SEQ-MMI 15.550(**) 6.814(**) 3.321(**) 0.168(**) 0.011(**) 0.036(**) 11.2\nHRED-MMI 13.278(**) 3.845(**) 1.398(**) 0.047(**) 0.001(**) 0.003(**) 13.8\nTrans-ED 17.946(/) 6.626(**) 3.213(**) 0.165(**) 0.039(**) 0.203(**) 18.8\nTrans-Dec 17.581(**) 6.790(*) 3.372(*) 0.180(**) 0.043(/) 0.248(**) 18.2\nTrans-MLM 18.672(**) 7.115(**) 3.484(/) 0.177(**) 0.041(**) 0.215(**) 16.8\nTrans-AR 18.849 7.245 3.662 0.192 0.044 0.235 16.8\nFG-free 18.741 (/) 7.134 (**) 3.504 (*) 0.184 (*) 0.042 (**) 0.225 (**) 17.0\nTrans-ED 17.337(**) 5.366(**) 1.967(**) 0.073(**) 0.001(**) 0.003(**) 17.1\nTrans-Dec 17.460(**) 6.586(**) 3.161(*) 0.172(/) 0.045(/) 0.254(**) 17.7\nTrans-MLM 19.193 (/) 6.877 (/) 3.175(*) 0.152(**) 0.029(**) 0.128(**) 15.0\nTrans-AR 18.749(/) 6.746(/) 3.119(*) 0.153(**) 0.031(**) 0.141(**) 16.2\nPF-free 18.466 (/) 6.688 (*) 3.075 (*) 0.169 (*) 0.038 (/) 0.180 (*) 14.1\nFG-free 18.610 6.937 3.302 0.175 0.040 0.191 14.1\nPF&FG-free 19.302 (*) 6.923 (/) 3.073 (*) 0.159 (**) 0.034 (*) 0.164 (**) 15.3\nTable 5: Evaluation results on large-scale (upper half) and small-scale (lower half) Reddit dataset.\nModel Score (M) Score (K)\nSEQ2SEQ-MMI 0.39 -\nTrans-ED 0.53 0.11\nTrans-Dec 1.02 0.77\nTrans-MLM 0.88 0.58\nTrans-AR 0.99 0.47\nPF-free - 0.52\nFG-free 0.91 0.78\nPF&FG-free - 0.72\nTrans-Dec (M) FG-free (K)\nSEQ2SEQ-MMI (11%, 48%) -\nTrans-ED (14%, 46%) (4%, 47%)\nTrans-Dec / (24%, 29%)\nTrans-MLM (24%, 34%) (18%, 31%)\nTrans-AR (27%, 32%) (17%, 34%)\nPF-free - (18%, 38%)\nFG-free (28%, 32%) /\nPF&FG-free - (23%, 29%)\nTable 6: Human evaluation including pair-wise eval-\nuation (lower half) for generated response quality for\nmillion-scale (M) Twitter dataset and its 100K training\nsubset (K). Pair-wise comparisons show the wining per-\ncentages of the two parties.\nthree million-scale datasets. We believe that bi-\ndirectional attention helps the model to better en-\ncode the dialogue history. In contrast, Trans-Dec\nis able to generate the most diverse responses. We\nattribute it to the left-to-right attention that intro-\nduces less constraints than bidirectional attention,\nthus has a higher ﬂexibility for generation.\nTrans-MLM vs. AR With large data, Trans-\nAR substantially outperforms Trans-MLM in terms\nof both automatic and human evaluation. When\neliminating the ﬁnetune-generation discrepancy of\nTrans-MLM, i.e. FG-free (we will introduce in Sec-\ntion 4.2), the performance is improved while still\nhaving a small gap especially in automatic metrics\nto Trans-AR. This may be because MLM objective\nonly masks a certain percentage of tokens (40%)\nModel Score (M) Score (K)\nSEQ2SEQ-MMI 0.12 -\nTrans-ED 0.33 0.10\nTrans-Dec 0.58 0.43\nTrans-MLM 0.48 0.38\nTrans-AR 0.64 0.31\nPF-free - 0.28\nFG-free 0.68 0.40\nPF&FG-free - 0.33\nFG-free (M) Trans-Dec (K)\nSEQ2SEQ-MMI (5%, 40%) -\nTrans-ED (11%, 33%) (2%, 28%)\nTrans-Dec (25%, 32%) /\nTrans-MLM (18%, 29%) (15%, 19%)\nTrans-AR (18%, 23%) (15%, 23%)\nPF-free - (15%, 24%)\nFG-free / (23%, 24%)\nPF&FG-free - (16%, 24%)\nTable 7: Human evaluation on Reddit dataset.\nwhile AR objective predicts all tokens on the target\nside for training. Thus, the AR objective is more\ntraining-efﬁcient. Similar observation about the\nefﬁciency of MLM has been reported in Clark et al.\n(2020). However, when training data are limited,\nwe will show that it is better to use MLM objective\nwhich has smaller pretrain-ﬁnetune discrepancy.\nTrans-ED vs. Decoder-Only With large dia-\nlogue data, we assume the size of pre-training data\nand pretrain-ﬁnetune discrepancy only have small\ninﬂuence on performance. However, even compar-\ning with Trans-MLM(FG-free)/AR, Trans-ED gen-\nerates much less diverse or appropriate responses.\nWe also observe lower speed for convergence when\ntraining the model 9. We believe that the result is\nmore or less due to the main difference in archi-\ntecture: an explicit encoder in Trans-ED might be\n9Similar observation has been reported in: https:\n//github.com/atselousov/transformer_\nchatbot/issues/15\n4487\nredundant (Liu et al., 2018).\n3.5 Discrepancy Impact\nIn section 2, we have discussed the pretrain-\nﬁnetune discrepancy of each framework. When\na large training dataset is available, the impact of\npretrain-ﬁnetune discrepancy is less severe since\nthe model can be gradually adapted to the given\ntask. However, if the training data are limited, the\ndiscrepancy problems may surface. Evaluation re-\nsults, especially in human evaluation, show that the\nperformance is more reduced with small data if the\nframework has larger discrepancy. For example,\nby comparing Trans-MLM (FG-free) and Trans-\nAR, the latter having additional pretrain-ﬁnetune\ndiscrepancy due to its auto-regressive objective,\nwe see that the performance of Trans-AR drops\nmore when trained on a small dataset. Trans-MLM\n(FG-free) and Trans-Dec that have small pretrain-\nﬁnetune discrepancy have clear advantage over\nother frameworks according to human evaluation.\nThese results suggest that with a small dataset\none should reduce pretrain-ﬁnetune discrepancy to\nbest exploit pre-trained LM. In the next section,\nwe propose 2 methods to reduce pretrain-ﬁnetune\ndiscrepancy and ﬁnetune-generation discrepancy\nof Trans-MLM.\n4 Discrepancy-Free Trans-MLM\n4.1 Pretrain-Finetune Discrepancy\nThe discrepancy of Trans-MLM comes from the\nleft-to-right attention on the target side that has not\nbeen pre-trained in BERT. Therefore, this discrep-\nancy cannot be eliminated during ﬁne-tuning for\na generation task. However, we can alleviate the\ndiscrepancy by using bi-directional attention also\non the target side. Speciﬁcally, at inference time,\nto generate a new token denoted as gt, [MASK] is\nfed into t-th position, denoted as gt-M. Previously\ngenerated tokens g<t could be viewed as a special\ntype of dialogue history, and thus we can apply\nbi-directional attention on it.\nHowever, in this case, the corresponding train-\ning process will have efﬁciency problems – only\none token can be masked in each training sam-\nple; otherwise, there will be conﬂict for the self-\nattention mask (Appendix B). This would lead to\nmuch lower training efﬁciency: the loss on valida-\ntion set only decreases slightly to 5.39 from 6.27\nafter four epochs, while Trans-MLM masking 40%\nof the target tokens can reduce it to 4.35. To avoid\nFigure 2: The generation process of PF-free at 4 differ-\nent time steps. Bi-attention interval is 3 in the graph.\nthis situation, we cannot always update previous\nhidden states using bi-directional attention in gen-\neration. Therefore, we explore to set a time-step\ninterval for bi-directional attention on the target\nside – within the interval we apply left-to-right\nattention and at the end of an interval we apply bi-\ndirectional attention. The corresponding training\nmethod allows us to mask multiple target tokens at\nthe same time to guarantee training efﬁciency.\nFigure 2 illustrates the generation process of our\nmethod with interval of 3. Before time step 3,\nleft-to-right attention is used (e.g. t=2). At time\nstep 3, bidirectional attention is allowed. Then\nleft-to-right attention is used (e.g. t=5) before the\nend of next interval cycle (t=6). Accordingly, the\ntraining process is: given a target response, we ﬁrst\nrandomly select among all (3 in the ﬁgure because\nt=3 and t=5 are the same pattern) possible attention\npatterns (e.g. the case of t=3 or t=5 in Figure 2,\nwhere we apply bi-directional attention only on\ny0,1,2); then in the part of left-to-right attention,\nwe randomly mask several tokens. We can mask\nmultiple tokens because this part applies left-to-\nright attention and the masks at other positions will\nnot inﬂuence the prediction on a given mask. We\ncall this method PF-free, which means that the\npretrain-ﬁnetune discrepancy is reduced.\n4.2 Finetune-Generation Discrepancy\nA model having ﬁnetune-generation discrepancy\nmeans the way that it is used in generation (in-\nference/test) is different from the way it has been\ntrained. Only Trans-MLM has ﬁnetune-generation\ndiscrepancy because of its MLM objective as\nshown in Figure 3: during training, there is a\nmasked token, y1-M, before y2-M, while in in-\nference there is not a masked token before when\ngenerating the token for g2-M.\n4488\nFigure 3: The training process of vanilla Trans-MLM\nand FG-free. We only plot the attention connection at\nthe second position.\nTo deal with the problem, we propose that at\ntraining time, rather than replacing the tokens with\n[MASK] as in vanilla MLM, we keep all origi-\nnal input tokens unchanged and prepend [MASK]\ntokens in the input sequence as illustrated. The\nprepended [MASK] token uses the same position\nembedding of the corresponding token. Then, ev-\nery position after y1-M attends to y1 instead of the\n[MASK] token, and thus the ﬁnetune-generation\ndiscrepancy of MLM is eliminated. We call the\nmodiﬁed model FG-free. A similar method has\nalso been explored in (Bao et al., 2020a), where\nthey introduced an extra pseudo mask in addition to\n[MASK] and prepend it before the original token in\norder to handle factorization steps of their partially\nauto-regressive language model.\n4.3 Experimental Results\nThe results with PF-free, FG-free and PF&FG-free\nmodels on small-scale datasets are reported in pre-\nvious tables together with other models. We can\nsee that each of the proposed methods brings some\nimprovement. PF-free improves most automatic\nmetrics over Trans-MLM, but the response appro-\npriateness in human evaluation is not improved.\nWe observe that PF-free could generate some re-\nsponses that lack ﬂuency, which also inﬂuences\nPF&FG-free (Appendix E). In general, our explo-\nration shows that the left-to-right attention on the\ntarget side is necessary for a generative task.\nWe examine our FG-free method on both large\nand small-scale data. It always brings statistically\nsigniﬁcant improvement over Trans-MLM in all au-\ntomatic metrics, and generates more appropriate re-\nsponses. On small-scale datasets, it outperforms all\nother frameworks in similarity metrics and achieve\ncomparable performance in response appropriate-\nness to Trans-Dec that has leveraged much more\npre-training data.\nThis set of experimental results conﬁrm the use-\nfulness of reducing discrepancies in the model.\nThis demonstrates that model discrepancies are in-\ndeed important problems we need to address when\na pre-trained LM is used for dialogue generation,\nand the problems have been under-explored.\nConclusion\nIn this paper, we examined the 4 frameworks for\nopen-domain dialogue based on pre-trained mod-\nels. We compared their performances on several\ndatasets with the same setting. The comparison\nrevealed that Trans-Dec and Trans-AR are both\ngood choices when large-scale data are available,\ne.g. further dialogue pre-training. When data are\nlimited, e.g. ﬁne-tuning on small dialogue tasks,\nTrans-Dec is the most appropriate.\nFurthermore, we deﬁned the concept of pretrain-\nﬁnetune and ﬁnetune-generation discrepancy, and\nexamined the 4 frameworks with respect to these\nconcepts. We have shown that the performances\nof the 4 frameworks can be largely explained by\ntheir respective discrepancies, which hinder their\nperformances. This becomes more clear when the\ndataset is small.\nTo further show that reducing the discrepancies\ncan improve the performance, we designed PF-free\nand FG-free correction methods to reduce the dis-\ncrepancies on Trans-MLM, and tested the corrected\nTrans-MLM models on the datasets. Our results\nconﬁrmed that once discrepancies are eliminated,\nTrans-MLM can produce better results.\nThis study is the ﬁrst investigation on the widely\nused 4 frameworks based on pre-trained LM in\nterms of architectural appropriateness and discrep-\nancies. We believe that this question is important\nto understand how a pre-trained model can be used\nin dialogue generation. It deserves more investiga-\ntions in the future.\nAcknowledgments\nThis research work is partly supported by an\nNSERC discovery grant.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977.\n4489\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang,\nNan Yang, Xiaodong Liu, Yu Wang, Songhao\nPiao, Jianfeng Gao, Ming Zhou, et al. 2020a.\nUnilmv2: Pseudo-masked language models for uni-\nﬁed language model pre-training. arXiv preprint\narXiv:2002.12804.\nSiqi Bao, Huang He, Fan Wang, and Hua Wu.\n2019. Plato: Pre-trained dialogue generation\nmodel with discrete latent variable. arXiv preprint\narXiv:1910.07931.\nSiqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng\nWang, Wenquan Wu, Zhen Guo, Zhibin Liu, and\nXinchao Xu. 2020b. Plato-2: Towards building an\nopen-domain chatbot via curriculum learning. arXiv\npreprint arXiv:2006.16779.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nJacob Cohen. 1960. A coefﬁcient of agreement for\nnominal scales. Educational and psychological mea-\nsurement, 20(1):37–46.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\nAlexander Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\nLowe, et al. 2019. The second conversational\nintelligence challenge (convai2). arXiv preprint\narXiv:1902.00098.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, pages 13042–13054.\nNouha Dziri, Ehsan Kamalloo, Kory Mathewson, and\nOsmar R Zaiane. 2019. Augmenting neural re-\nsponse generation with context-aware topical atten-\ntion. In Proceedings of the First Workshop on NLP\nfor Conversational AI, pages 18–31.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting objec-\ntive function for neural conversation models. InPro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n110–119.\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2020. Exploring versatile generative language\nmodel via parameter-efﬁcient transfer learning.\narXiv preprint arXiv:2004.03829.\nZhaojiang Lin, Peng Xu, Genta Indra Winata, Zi-\nhan Liu, and Pascale Fung. 2019. Caire: An\nend-to-end empathetic chatbot. arXiv preprint\narXiv:1907.12108.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. 2018. Generating wikipedia by\nsummarizing long sequences. arXiv preprint\narXiv:1801.10198.\nRyan Thomas Lowe, Nissan Pow, Iulian Vlad Serban,\nLaurent Charlin, Chia-Wei Liu, and Joelle Pineau.\n2017. Training end-to-end dialogue systems with\nthe ubuntu dialogue corpus. Dialogue & Discourse,\n8(1):31–65.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric M Smith, et al. 2020. Recipes\nfor building an open-domain chatbot. arXiv preprint\narXiv:2004.13637.\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hier-\narchical neural network models. In Thirtieth AAAI\nConference on Artiﬁcial Intelligence.\nKurt Shuster, Da Ju, Stephen Roller, Emily Dinan,\nY-Lan Boureau, and Jason Weston. 2019. The\ndialogue dodecathlon: Open-domain knowledge\nand image grounded conversational agents. arXiv\npreprint arXiv:1911.03768.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\n4490\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nYan Zeng and Jian-Yun Nie. 2021. A simple and efﬁ-\ncient multi-task learning approach for conditioned\ndialogue generation. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 4927–4939.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2204–\n2213.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nYinhe Zheng, Rongsheng Zhang, Xiaoxi Mao, and\nMinlie Huang. 2019. A pre-training based personal-\nized dialogue generation model with persona-sparse\ndata. arXiv preprint arXiv:1911.04700.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19–\n27.\n4491\nFigure 4: i-th Transformer Block and two M settings\nrepresented in two ways. Shaded areas are blocked.\nA Multi-Layer Transformer\nIn this section, we provide some background knowl-\nedge on Transformer. The four frameworks we dis-\ncussed all consist of 12 Transformer blocks. Figure\n4 (a) shows a general architecture of a Transformer\nlayer, where the most important component is the\nmasked multi-head self-attention. The setting of\nattention masks is the largest difference between\nTrans-Dec and Trans-AR, and it is also the most\ncritical part to implement our PF-free and FG-free\nmethods.\nThe input representation H0 ∈Rn×dh , where\nn is the input length and dh = 768 is the hid-\nden dimension, is the sum of token embedding,\nposition embedding, and type embedding at each\nposition. Then, H0 is encoded into hidden rep-\nresentations of i-th layer Hi = [hi\n1, ...,hi\nn] by:\nHi = Transi(Hi−1), i ∈[1, L], where Transi\ndenotes the i-th Transformer Block as shown in\nFigure 4 (a). The core component of a transformer\nblock is the masked multi-head attention, whose\noutputs are Ci = [ci\n1, ...,ci\nn] that are computed via\nCi = Concat(head1, ...,headh), with\nheadj = softmax(\nQjKT\nj√dk\n+ M)Vj (1)\nwhere Qj, Kj, Vj ∈Rn×dk are obtained by trans-\nforming Hi−1 ∈Rn×dh using WQ\nj , WK\nj , WV\nj ∈\nRdh×dk respectively. M ∈ Rn×n is the self-\nattention mask matrix that determines whether\na position can attend to other positions. Mij ∈\n{0, −∞}. In particular, Mij = 0allows the i-th\nposition to attend to j-th position and Mij = −∞\nprevents from it. Figure 4 (b&c) shows two M\nsettings that are applied by Trans-MLM/AR and\nTrans-Dec respectively.\nFigure 5: Self-attention mask, M, conﬂicts – (a) if\npredicting y1, y2 and y3-M are ”future” and forbid-\nden to be accessed by y1-M; (b) if predicting y3, y1-\nM accesses to y2 and y3-M, which causes conﬂicts to\nM(a); (c) if forbidding y1-M to access to y2 and y3-M\nin M(b), there will still be (indirect) information leak\nas indicated in red arrows ( y2 and y3-M →y0 →y1-\nM). Masking two positions thus causes conﬂicts. Our\nPF-free method aims to overcome this problem.\nB Illustration of Attention Conﬂict\nIf applying bi-directional attention at each gener-\nation step, only one token at the target side could\nbe masked for each training sample; otherwise\nthere will be attention conﬂicts, i.e. different self-\nattention mask matrices are required for different\nmasked tokens, while only one mask matrix can\nbe provided per training sample. In Figure 5, we\nprovide an illustration of the mask conﬂict problem.\nWe assume y1 and y3 are masked and need to be\npredicted at the same time. We see in the ﬁgure that\ntwo different masks are required for predicting y1\nand y3, which cannot be done in a single training\nstep, making it impossible to mask more than one\ntoken in each step.\nC Implementation Details\nFor the 4 frameworks, we used open-source im-\nplementations. Only some minor adaptations to\nour data and task are made (e.g. re-wrote the data\nloader to load our experimental datasets, and mod-\niﬁed the training objective by keeping only the\nresponse reconstruction loss). For response genera-\ntion, we equipped all frameworks with an identical\ndecoding script 10. We did not modify other parts,\nand used the default settings for hyper-parameters,\ne.g. optimizer and learning rate. Some genera-\ntion examples are given in Appendix E. Although\nsome models (e.g. Trans-ED) produced poor per-\n10https://github.com/microsoft/unilm/\n4492\nModel Pre-trained LM Data\nTrans-ED GPT (Radford et al., 2018) BooksCorpus\nTrans-Dec GPT-2 small (Radford et al., 2019) WebText\nTrans-MLM/AR BERT base (Devlin et al., 2018) BooksCorpus, English Wikipedia\nTable 8: The text data used for language model pre-training.\nformance on small datasets, all model can generate\nsome coherent and ﬂuent responses with large scale\ntraining data, which is consistent with the perfor-\nmances reported in previous papers.\nLanguage Models The pre-trained language\nmodels used by these frameworks have comparable\nnumber of parameters as listed in Table 9, while the\npre-training data are in different scales as described\nin Table 8. BooksCorpus (Zhu et al., 2015) (800M\nwords) contains over 7,000 unique unpublished\nbooks from a variety of genres. English Wikipedia\n(2,500M words) consists of the text passages of\nWikipedia extracted by Devlin et al. (2018). Web-\nText crawled by Radford et al. (2019) contains 8M\ndiverse documents for a total of 40 GB of text.\nTrans-ED We use the implementation of Con-\nvAI2 champion 11. The model was for persona-\nconditioned dialogue generation. The framework\nis based on GPT architecture and uses GPT for\nparameter initialization. However they only pro-\nvide a model checkpoint that has been ﬁne-tuned\non large-scale dialogue data including Reddit. To\nexamine the ability of utilizing pre-trained LM, we\ndid not use this checkpoint but initialize the model\nwith GPT parameters 12. We also did not apply\npost-processing to the generation results (to be con-\nsistent with other experiments).\nTrans-Dec We use the released code of Wolf\net al. (2019) 13 that uses GPT-2 small by default.\nThe model was for persona-conditioned dialogue\ngeneration.\nTrans-MLM/AR These two models are imple-\nmented based on Dong et al. (2019) 14 that applies\nmulti-task learning on language understanding and\ngeneration tasks. We use BERT (base, uncased)\n11https://github.com/atselousov/\ntransformer_chatbot\n12https://github.com/openai/\nfinetune-transformer-lm/tree/master/\nmodel\n13https://github.com/huggingface/\npytorch-openai-transformer-lm\n14https://github.com/microsoft/unilm/\ntree/master/unilm-v1\nModel Params Runtime\nSEQ2SEQ-MMI 66M 50\nHRED-MMI 58M 25\nTrans-ED 117M 180\nTrans-Dec 117M 290\nTrans-MLM 110M 140\nTrans-AR 110M 140\nPF&FG-free 110M 140\nTable 9: The number of parameters of each tested ap-\nproach and the average runtime (minutes) for every mil-\nlion training samples. The runtime is tested using a\n1080Ti GPU device, and the batch size is set to take\nall of the GPU memories. Notice that the runtime will\nbe inﬂuenced by code implementation in addition to\nmodel structure.\nfor parameter initialization, and ﬁne-tune it on dia-\nlogue datasets. PF-free and FG-free are also imple-\nmented based on the code. We set the bi-directional\nattention interval of PF-free to 5. Since the average\nlength of ground-truth responses in the datasets is\n∼15, This setting is generally appropriate.\nD Human Evaluation Details\nSome existing studies considered response ﬂuency,\ncoherence, and informativeness. We make the man-\nual evaluation simpler and ask the human evalua-\ntors to rate a response in {0, 1, 2}. A score of 0\nrepresents an unacceptable response, which might\nhave ﬂaw in ﬂuency and logic or be incoherent.\nSpecial cases are for example completely coping\nfrom the dialogue history as the output, and a bland\nresponse such as “i do not know what you are talk-\ning about , but it ’s a good point .”. A score of 1\nrepresents an acceptable response, but it is generic\nor not perfectly coherent to the dialogue history. 2\nrepresents a coherent and informative response.\nE Generation Examples\n4493\nDialogue History one week before election day , early voting is nearly twice as high as <num>\nGold Response i hope it ’s<person> out in full force .\nSEQ2SEQ-MMI i am not voting for a third party candidate . (1)\nTrans-ED i am not sure what you are talking about , but it ’s a good thing that <person> is going to win . (1)\nTrans-Dec that ’s not true . early voting in nyc is times higher than the national average (2)\nTrans-MLM it ’s not even close to <num> % of the vote . i am sure you are right , but there is a huge difference\nbetween early voting and <person> voter suppression in ca (1)\nTrans-AR it ’s not that high , but i am sure there will be a lot of voter fraud . (2)\nFG-free it ’s not even close to <num> yet , but i am sure the polls will show up . (2)\nTrans-ED (K) i am not sure what you are talking about . it ’s the only one who is going to do with this (0)\nTrans-Dec (K) it ’s not that early voting is a bad thing , but it shouldnot be the case . (0)\nTrans-MLM (K) it ’s a little too late . i am not sure if that is the case , but we will see what happens next (2)\nTrans-AR (K) it ’s not even close to <num> % of voter turnout . i am sure you are right about that (0)\nPF-free (K) <num> % of people voted for <person> , so i am going to vote (2)\nFG-free (K) it ’s not even close to <num> % of voter turnout , but i am sure there are more . (2)\nPF&FG-free (K) it ’s not even <num> % of the vote . i am going to be a voter (2)\nTable 10: Responses generated by baselines and our methods trained on the Twitter dataset(million-scale and\n100K). Human evaluation scores are given at the end of each generated reply.\nDialogue History i think about this man every day\nGold Response it is so hypnotic . this man was found dead post-election\nSEQ2SEQ-MMI i do not know what you are talking about . (0)\nTrans-ED i think he ’s the only one who has a lot of work to do with it . (1)\nTrans-Dec i think about this man every day too . he ’s my favorite person in the world (2)\nTrans-MLM he ’s such a good guy . i love him so much, but it doesnot make sense to me that this is the only way\nwe can be together in our lives (2)\nTrans-AR i think about this man every day too . he ’s so good at it ! (2)\nFG-free he ’s such a good guy . i am so happy for him and his family (2)\nTrans-ED (K) i am not sure what you are talking about , but it ’s a good idea . (0)\nTrans-Dec (K) i think about this man every day . he ’s my hero and i love him (2)\nTrans-MLM (K) i think he ’s a great actor , but no one cares about him . lol (2)\nTrans-AR (K) i think he ’s a good guy , but not sure what you are talking about . (0)\nPF-free (K) i think he ’snitch . it makes me happy to watch him (0)\nFG-free (K) he ’s a great guy . i am so happy to have him back (2)\nPF&FG-free (K) i think he is little silly , but you are a fool . (2)\nTable 11: Responses generated by baselines and our methods trained on the Twitter dataset(million-scale and\n100K). Human evaluation scores are given at the end of each generated reply.\nDialogue History how old are you guys ? you did not have mechanical pencils ? [SEP] <url> young are you ?\ni was in school all the way up to a few years ago and most of us still used non - mechanical ones .\nGold Response i started using mechanical pencil in elementary school about 15 years ago .\nSEQ2SEQ-MMI i do not know what you are talking about . (0)\nTrans-ED how old are you ? i was in school all the way up to a few years ago . (0)\nTrans-Dec i was in school all the way up to a few years ago and most of us still used non - mechanical ones . (0)\nTrans-MLM i was in school for a few years , and most of us still use mechanical pencils . it ’s not the same age\nas you (2)\nTrans-AR i was in high school when we used to use mechanical pencils . (2)\nFG-free how old are you ? i was in high school and still use mechanical pencils . (2)\nTrans-ED (K) i do not know what you are talking about , but it ’s just a joke . (0)\nTrans-Dec (K) i was in school all the way up to a few years ago and most of us still used non - mechanical ones . (0)\nTrans-MLM (K) i have been in school for a few years , but not really . (0)\nTrans-AR (K) how old are you ? i have never been in school for a few years . (0)\nPF-free(K) how old are you ? i am not really a mechanical engineer . ! (1)\nFG-free (K) how old are you ? i have never had a mechanical pencil . (2)\nPF&FG-free (K) how old are you ? i have never had a lot of time in school (2)\nTable 12: Responses generated by baselines and our methods trained on the Reddit dataset(million-scale and\n100K). Human evaluation scores are given at the end of each generated reply.\n4494\nDialogue History i have read all of the books , which character do you not like ? [SEP] it ’s gon na be the dornish\nand greyjoys . and probably brienne . you will see .\nGold Response nah , they have only just started a game of thrones so it must be one of the starks , dan , or tyrion .\nSEQ2SEQ-MMI i do not know why you are being downvoted . (0)\nTrans-ED i have read all of the books , and it ’s gon na be dornish . (2)\nTrans-Dec i have not read the books yet , but i will deﬁnitely check them out . (2)\nTrans-MLM i have not read the dornish and greyjoys , but you will see them all as well . (1)\nTrans-AR brienne is not a ﬁctional character , she ’s an actual person . (2)\nFG-free i have read all of the dornish and greyjoys . (1)\nTrans-ED (K) i am not sure what you are talking about , but it ’s just a joke . (0)\nTrans-Dec (K) i do n’t know , i have read all of the dornish books . (1)\nTrans-MLM (K) i do not think it ’s a good idea to say that . (0)\nTrans-AR (K) i do not think it ’s gon na be the best . (0)\nPF-free (K) i am not sure what you are talking about here ’s a link . (0)\nFG-free (K) i do not think it ’s gon na be . (1)\nPF&FG-free (K) i do not know , but it ’s a lot of people ? (1)\nTable 13: Responses generated by baselines and our methods trained on the Reddit dataset(million-scale and\n100K). Human evaluation scores are given at the end of each generated reply.\nDialogue History is that easy to do ? [SEP] usually the ubuntu installer will set it up perfectly for you\nGold Response and then i would select at start-up ?\nSEQ2SEQ-MMI how do i install it ?\nHRED-MMI i am not sure what you mean by that\nTrans-ED is there a way to do it without installing the ubuntu installer ? i am not sure .\nTrans-Dec ok , i will try that . thanks for the help ! :-) it ’s working now\nTrans-MLM is there a way to do it with the live cd ? i am not sure if that ’s what you are looking for , but how\ndoes one install ubuntu on\nTrans-AR ok , i will give it a try . thank you very much for your help !\nFG-free is there a way to do it from the command line ? i am not sure what you mean\nTrans-ED (K) i am not sure what you mean by that , but it ’s just a little bit more speciﬁc\nTrans-Dec (K) ok , i will try that . thanks for your help ! :-) <url>\nTrans-MLM (K) i am not sure what you mean by that , but it ’s a good idea .\nTrans-AR (K) i am not sure what you are talking about , but it ’s possible that the installer will be able to boot from cd\nPF-free (K) i am not sure what to do ? it ’ s just a ﬁle in the repos\nFG-free (K) i do not think it ’ s a good idea to install ubuntu on my laptop\nPF&FG-free (K) i am not sure what you mean by setting it up perfectly . is there a way to do that ?\nTable 14: Responses generated by baselines and our methods trained on the Ubuntu dataset(million-scale and\n100K).\nDialogue History usually if on the same machine , <url> [SEP] : i am getting error . phpmyadmin not in this server\nGold Response you installed it from the software center , or with aptitude or apt-get , or some other way ?\nSEQ2SEQ-MMI what is the error you are getting ?\nHRED-MMI i have no idea what you are talking about\nTrans-ED i am not sure what you are trying to do , but it ’s a good idea .\nTrans-Dec did you install phpmyadmin from the repos ? if so , it should be in <path>\nTrans-MLM i am not sure what you are trying to do , but it should be in <path> . try sudo dpkg -\nreconﬁgure phpmyadmin\nTrans-AR i am not sure what you are trying to do , but it ’s possible .\nFG-free i am not sure what you are trying to do , but it ’ s usually in <path>\nTrans-ED (K) i am not sure what you mean by ” phpmyadmin <path>\nTrans-Dec (K) i am not sure what phpmyadmin is , but it ’s probably in <path>\nTrans-MLM (K) i am not sure what you mean by ” phpmyadmin - > <path> . <url>\nTrans-AR (K) i do not know what phpmyadmin is , but it ’s in the repos\nPF-free (K) it ’ s not in this server . i am trying to get phpmyadmin working\nFG-free (K) phpmyadmin is not in the same machine , it ’ s just a server .\nPF&FG-free (K) i am not sure if it ’s on the same machine , you can use phpmyadmin\nTable 15: Responses generated by baselines and our methods trained on the Ubuntu dataset(million-scale and\n100K).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7303664684295654
    },
    {
      "name": "Natural language processing",
      "score": 0.5622473955154419
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5065909624099731
    },
    {
      "name": "Language model",
      "score": 0.43358445167541504
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    }
  ]
}