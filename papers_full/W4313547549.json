{
    "title": "Few-shot training LLMs for project-specific code-summarization",
    "url": "https://openalex.org/W4313547549",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2182408631",
            "name": "Toufique Ahmed",
            "affiliations": [
                "University of California, Davis"
            ]
        },
        {
            "id": "https://openalex.org/A1481757371",
            "name": "Premkumar Devanbu",
            "affiliations": [
                "University of California, Davis"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3170092793",
        "https://openalex.org/W4284688961",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W3086007799",
        "https://openalex.org/W2740130862",
        "https://openalex.org/W3146720657",
        "https://openalex.org/W2560647685",
        "https://openalex.org/W3195727321",
        "https://openalex.org/W2082160726",
        "https://openalex.org/W2165747537",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W2741561716"
    ],
    "abstract": "Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.",
    "full_text": null
}