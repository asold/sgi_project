{
  "title": "Few-shot training LLMs for project-specific code-summarization",
  "url": "https://openalex.org/W4313547549",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2182408631",
      "name": "Toufique Ahmed",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A1481757371",
      "name": "Premkumar Devanbu",
      "affiliations": [
        "University of California, Davis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3170092793",
    "https://openalex.org/W4284688961",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3086007799",
    "https://openalex.org/W2740130862",
    "https://openalex.org/W3146720657",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3195727321",
    "https://openalex.org/W2082160726",
    "https://openalex.org/W2165747537",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W2741561716"
  ],
  "abstract": "Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.",
  "full_text": null,
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.7557607293128967
    },
    {
      "name": "Computer science",
      "score": 0.6932241320610046
    },
    {
      "name": "Identifier",
      "score": 0.5208990573883057
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.4640440344810486
    },
    {
      "name": "Terminology",
      "score": 0.45913225412368774
    },
    {
      "name": "Natural language processing",
      "score": 0.4287590980529785
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4279259145259857
    },
    {
      "name": "Data science",
      "score": 0.3563661575317383
    },
    {
      "name": "Linguistics",
      "score": 0.20767554640769958
    },
    {
      "name": "Programming language",
      "score": 0.1878097951412201
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84218800",
      "name": "University of California, Davis",
      "country": "US"
    }
  ]
}