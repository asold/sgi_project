{
  "title": "Large Language Models for Text Classification: From Zero-Shot Learning to Instruction-Tuning",
  "url": "https://openalex.org/W4386165096",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2141077685",
      "name": "Youngjin Chae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1947197505",
      "name": "Thomas Davidson",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6787300336",
    "https://openalex.org/W3033317208",
    "https://openalex.org/W3091998006",
    "https://openalex.org/W4296154596",
    "https://openalex.org/W2437771934",
    "https://openalex.org/W3034824379",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W4224295605",
    "https://openalex.org/W4310576793",
    "https://openalex.org/W6678277124",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W6759193872",
    "https://openalex.org/W4378771373",
    "https://openalex.org/W2947838315",
    "https://openalex.org/W2595653137",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4289525316",
    "https://openalex.org/W6717396518",
    "https://openalex.org/W3047084620",
    "https://openalex.org/W4376653723",
    "https://openalex.org/W2761527807",
    "https://openalex.org/W6749630143",
    "https://openalex.org/W7043604163",
    "https://openalex.org/W2413287953",
    "https://openalex.org/W2182643880",
    "https://openalex.org/W4376603774",
    "https://openalex.org/W3208294020",
    "https://openalex.org/W3197961390",
    "https://openalex.org/W3137930360",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W3103365499",
    "https://openalex.org/W3032046549",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3199400376",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3020745670",
    "https://openalex.org/W6785559428",
    "https://openalex.org/W6718565325",
    "https://openalex.org/W6762520361",
    "https://openalex.org/W1551219252",
    "https://openalex.org/W2769563773",
    "https://openalex.org/W2805117413",
    "https://openalex.org/W6674691379",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W4212939653",
    "https://openalex.org/W3159574466",
    "https://openalex.org/W3120042896",
    "https://openalex.org/W6763859671",
    "https://openalex.org/W3101900272",
    "https://openalex.org/W2098420572",
    "https://openalex.org/W4366277658",
    "https://openalex.org/W3041808347",
    "https://openalex.org/W6849792168",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4306322105",
    "https://openalex.org/W4294299598",
    "https://openalex.org/W4283688409",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6844383832",
    "https://openalex.org/W2963806437",
    "https://openalex.org/W4296928857",
    "https://openalex.org/W6767858076",
    "https://openalex.org/W2419499132",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W4318263917",
    "https://openalex.org/W4303685983",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2970200208",
    "https://openalex.org/W4240102194",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4383912341",
    "https://openalex.org/W2972735048",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4244432426",
    "https://openalex.org/W4368754639",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4311510002",
    "https://openalex.org/W4385564993",
    "https://openalex.org/W4322575383",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3102743123",
    "https://openalex.org/W4300170376",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W4252250696",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3175487198",
    "https://openalex.org/W2964325543",
    "https://openalex.org/W4385571220",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2097726431",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4313440285",
    "https://openalex.org/W2963811339",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "Advances in large language models (LLMs) have transformed the field of natural language processing and have enormous potential for social scientific analysis. We explore the application of LLMs to supervised text classification. As a case study, we consider stance detection and examine variation in predictive accuracy across different architectures, training regimes, and task specifications. We compare ten models ranging in size from 86 million to 1.7 trillion parameters and four distinct training regimes: prompt-based zero-shot learning; few-shot learning; fine-tuning; and instruction-tuning. The largest models generally offer the best predictive performance, but fine-tuning smaller models is a competitive solution due to their relatively high accuracy and low cost. For complex prediction tasks, instruction-tuned open-weights models can perform well, rivaling state-of-the-art commercial models. We provide recommendations for the use of LLMs for text classification in sociological research and discuss the limitations and challenges related to the use of these technologies.",
  "full_text": "LARGE LANGUAGE MODELS FOR TEXT CLASSIFICATION :\nFROM ZERO -SHOT LEARNING TO INSTRUCTION -TUNING\nYoungjin Chae\nDepartment of Sociology\nRutgers University\nNew Brunswick, NJ\nyj.chae@rutgers.edu\nThomas Davidson\nDepartment of Sociology\nRutgers University\nNew Brunswick, NJ\nthomas.davidson@rutgers.edu\nForthcoming at Sociological Methods & Research. Please cite the published version.\nABSTRACT\nAdvances in large language models (LLMs) have transformed the field of natural\nlanguage processing and have enormous potential for social scientific analysis. We\nexplore the application of LLMs to supervised text classification. As a case study,\nwe consider stance detection and examine variation in predictive accuracy across\ndifferent architectures, training regimes, and task specifications. We compare ten\nmodels ranging in size from tens of millions to hundreds of billions of parameters\nand four distinct training regimes: prompt-based zero-shot learning and few-shot\nlearning, fine-tuning using more training data, and instruction-tuning that combines\nprompting and training data. The largest models generally offer the best predictive\nperformance even with little or no training examples, but fine-tuning smaller models\nis a competitive solution due to their relatively high accuracy and low cost. For\ncomplex prediction tasks, instruction-tuned open-weights models can perform well,\nrivaling state-of-the-art commercial models. We provide recommendations for the\nuse of LLMs for text classification in sociological research and discuss the limitations\nand challenges related to the use of these technologies.\nKeywords large language models · supervised text classification · natural language processing ·\nstance detection · computational social science\n1 Introduction\nThe development of large language models (LLMs) has led to significant breakthroughs in many\nareas of natural language processing (NLP), and interest in artificial intelligence (AI) among\nacademic researchers and the wider public has grown dramatically since the release of OpenAI’s\nLLM chatbot ChatGPT in late 2022. These technological innovations represent a paradigm shift in\nmachine learning, as these technologies have evolved from being “narrow specialists” trained for\nspecific tasks to “competent generalists” that can perform many tasks with little or no additional\ntraining (Radford et al., 2019). The text-to-text interface makes it relatively simple to use these\nLarge language models for text classification\ntechnologies (Raffel et al., 2020) and enhancements that improve their capacity to follow instructions\n(Wei et al., 2022) enable LLMs to serve as a foundation for diverse downstream tasks (Bommasani\net al., 2022). LLMs and other generative AI promise to transform many areas of social scientific\nmethodology and research from text analysis and agent-based modeling to literature review and\nhypothesis generation (Grossmann et al., 2023; Bail, 2024; Davidson, 2024).\nIn this article, we evaluate the use of LLMs as a methodology for supervised text classification,\na branch of machine learning focused on classifying texts using a predefined schema. We situate\nLLMs in the context of earlier developments in language modeling and provide a thorough overview\nof the recent developments in the field. Building on work showing that even relatively small language\nmodels can achieve impressive performance with limited training (Do et al., 2022; Wankmüller,\n2022; Bonikowski et al., 2022; Ziems et al., 2024), we examine how the predictive accuracy of\nLLMs compares to conventional machine learning techniques and varies across different model\narchitectures and learning regimes. We also demonstrate how the generative capabilities of the\nlatest models can be leveraged to analyze complex, structured data in new ways. We use the results\nfrom these experiments to inform practical recommendations for the use of these techniques in\nsociological research.\nAs an empirical case, we use LLMs for stance detection, an application of supervised text classifica-\ntion to measure attitudes, opinions, and beliefs in observational text data (Somasundaran and Wiebe,\n2010; Sobhani et al., 2015; Mohammad et al., 2017). Sociologists have previously used sentiment\nanalysis for such tasks (Shor et al., 2015; Flores, 2017; Felmlee et al., 2020), but sentiment is a\nnoisy proxy for these constructs, and stance detection can help researchers to obtain more reliable\nmeasurements that can be tailored to specific tasks (Bestvater and Monroe, 2022). As a case study,\nwe focus on stances expressed on social media toward the two leading candidates in the 2016 US\npresidential election. We analyze three different stance detection datasets consisting of a popular\nbenchmark dataset from Twitter (Mohammad et al., 2016) and two new annotated Facebook datasets.\nThese analyses allow us to triangulate the performance of LLMs across related tasks and to illustrate\nmultiple approaches to text classification.\nWe begin by evaluating three distinct learning regimes that can be adopted when using LLMs for\ntext classification. First, by leveraging the capacity of models to generalize based on large amounts\nof text consumed during pre-training, the most advanced LLMs can perform text classification\nwithout any additional training relying solely on written instructions known as prompts (Radford\net al., 2019). We evaluate the extent to which LLMs can detect stances based on instructions alone\n2\nLarge language models for text classification\nand assess how their performance varies depending on the prompt length and detail. Second, we\nconsider whether labeled examples input alongside prompts result in improved performance (Brown\net al., 2020) and conduct experiments to test their sensitivity to different examples. Finally, LLMs\ncan be fine-tuned for specific tasks using larger sets of annotated training data, an approach more\nakin to conventional supervised learning. We conduct a series of evaluations to understand how\npredictive performance varies based on the model architecture and the amount of training data.\nAcross these experiments, we test ten different LLMs, ranging from relatively small models like\nBERT (Devlin et al., 2019) to OpenAI’s state-of-the-art GPT-4o (OpenAI, 2023).\nTo highlight the capacity of LLMs to handle complex, structured data, we formulate a novel approach\nto modeling comment-reply threads. This analysis uses instruction-tuning, which combines the\nstrengths of prompting and fine-tuning by training models with instructions and paired inputs and\noutputs (Wei et al., 2022). The task involves not only predicting the stance of a comment but also\nthe stances of any associated replies. This is a more challenging prediction problem, as there are\nmultiple predictions for each thread, the number of which varies across threads. We show how the\ncombination of structured data formats and detailed prompts can be used to classify comment threads\nusing zero-shot learning and use instruction-tuning to further improve the predictive performance\non this task. This analysis demonstrates the latest generative models can be used to study opinion\ndynamics in more realistic, conversational contexts and outlines a framework for using LLMs for\ntasks involving more complex data and coding schemes.\nWith sufficient training, most of the models evaluated substantially outperform conventional machine\nlearning approaches, but there is considerable variability in performance across architectures and\nlearning regimes. Prompt-based learning regimes work best when using the largest, most powerful\nmodels, but predictive performance is sensitive to the composition of the prompt and training\nexamples. The largest model evaluated, GPT-4o, outperforms fine-tuned models at predicting the\nstance of Facebook comments without any task-specific training. When fine-tuning models using\nmore training data, the largest models also achieve the best performance, but far smaller models\ncan perform almost as accurately as their larger counterparts when fine-tuned on more annotated\ntraining data. The results from our thread-prediction task show how LLMs trained to predict\nmultiple stances in comment-reply threads can achieve strong performance despite the difficulty\nof the task. Moreover, we find that cutting-edge open-weights models are becoming considerably\nmore powerful, as Llama3-70B shows comparable performance to GPT-4o when predicting threads\nusing prompts alone and exhibits further improvement after instruction-tuning.\n3\nLarge language models for text classification\nBased on these results, we develop recommendations for social scientists interested in using LLMs\nfor text classification. We discuss how the selection of models and learning regimes will vary\ndepending on factors including the number of documents to be classified, the amount of annotated\ndata available, and available computing resources. We also consider the limitations and challenges\nrelated to the use of these models, discussing issues including interpretability, reliability, bias,\nreproducibility, and privacy, as they pertain to our analyses and other social scientific applications,\nand discuss the trade-offs between proprietary models and open-weights alternatives in the context\nof these considerations. Overall, we anticipate that these experiments and recommendations will\nserve as a useful foundation for social scientists interested in using LLMs for text classification and\nrelated tasks.\n2 Background\n2.1 Language models and transfer learning\nA language model is a type of computational model that learns probabilistic representations of\nlanguage from text corpora. The main objective of a language model is to predict the next word or\nset of words in a sequence, although there are many variations of this basic task. For example, what\nis the most likely word to end the sentence: “The cat caught a ...”? Each word in a vocabulary can\nbe assigned a probability of appearing. In this case, a good model should assign a high probability\nto the word “mouse” and a much lower probability to words like “excavator” or “whale.” In a\nbi-gram model, the previous word in a sequence is used to predict the next word. More formally, a\nbi-gram language model is used to obtain P(wi|wi−1), the probability word wi is used given the\nprevious word wi−1. Given a corpus of text, these probabilities can be calculated by counting the\nnumber of times a given pair of words co-occurs sequentially and normalizing by the frequency\nof wi−1 (Martin and Jurafsky, 2024). N-gram language models extend this logic by using the\nprevious n words in a sequence to predict the next word. Considering the example above, we should\nexpect to make more accurate predictions if the model has more context (e.g., “cat caught a” versus\n“caught a” or “a”). The origins of language modeling date back to the early 20 th century, when\nAndrey Markov used transition probabilities to model character sequences in literature, and the\nmathematical properties of these models were further developed by Claude Shannon (1948), who\ndrew on statistical mechanics to study uncertainty in n-gram models (see Martin and Jurafsky, 2024;\nLi, 2022; Manning, 2022 for further historical background). By the 1990s, computer scientists had\ndeveloped techniques to compute probabilistic language models from larger corpora of text with\n4\nLarge language models for text classification\napplications to tasks including speech recognition, translation, and spelling correction (Bahl et al.,\n1983; Brown et al., 1990, 1992). However, these models become intractable as n increases and are\nsensitive to the composition of the training corpus, making it difficult to generalize to new data.\nDespite these limitations, n-gram language models can have useful applications for social scientific\nanalyses of texts (Danescu-Niculescu-Mizil et al., 2013; Jensen et al., 2022).\nMore sophisticated language models based on neural network architectures that learn patterns\nthrough multiple interconnected layers of parameters were proposed in the early 2000s (Bengio et al.,\n2003), but computational constraints made these approaches challenging to operationalize beyond\nrelatively simple tasks. By the early 2010s, the availability of large corpora of online text and the\ndevelopment of neural networks optimized to run on graphics processing units (GPUs)—specialized\nprocessors originally designed for rendering graphics in video games—made it possible to train\nmore powerful language models (Manning, 2022). Notably, Mikolov et al. (2013a,b) proposed\nthe Word2vec architecture, using word prediction tasks to train neural networks as probabilistic\nlanguage models. The language model itself was largely incidental, receiving less attention than\nrich semantic associations contained in internal weights, or embeddings, learned during training.\nThese embeddings represent words as dense numeric vectors and have opened up new possibilities\nfor the study of language and culture (Kozlowski et al., 2019; Stoltz and Taylor, 2021; Rodriguez\nand Spirling, 2022).\nNeural language models can be adapted to perform other types of natural language processing\ntasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) via a\nprocess known as transfer learning. A neural language model is first pre-trained using variants of\nthe next-word prediction task, enabling the model to “learn” a general representation of language\nfrom a corpus. This pre-trained model can be modified to perform new tasks such as translation or\nquestion-answering by using a process known as fine-tuning. This works similarly to conventional\nsupervised machine learning techniques by using training data to learn internal weights. The main\ndifference is that rather than learning parameters from scratch, the existing weights in the pre-trained\nmodel are updated as a model continues to train on a new dataset. The technique was pioneered\nin computer vision, as models trained on diverse sets of images could be adapted to detect new,\npreviously unseen imagery by making use of the generic representations they have already learned\n(Yosinski et al., 2014) (see Zhang and Pan, 2019 for a sociological application). Transfer learning\nrepresents an important breakthrough in machine learning insofar as it alleviates the need to train a\ncustom model for every new task.\n5\nLarge language models for text classification\n2.2 Large language models\nThe latest generation of neural language models are known as large language models, or LLMs,\ndue to their size and the amount of text they are trained on. This label encompasses a wide variety\nof models, which we describe in further detail below. These models have grown in size following\nevidence of “scaling laws,” as they show improvement in performance on both language modeling\nand downstream tasks as they get larger (Kaplan et al., 2020; Brown et al., 2020; Raffel et al., 2020).\nTo scale effectively, models require concurrent increases in both the number of internal parameters\nand the amount of training data used (Hoffmann et al., 2022). The series of GPT models from\nOpenAI illustrates this trajectory: The original GPT, released in 2018, had 117 million parameters\nand was trained on BookCorpus, a collection of 7,000 books (Radford et al., 2018); a year later,\nGPT-2 was released, with 1.5 billion parameters, trained on over 8 million web pages (Radford\net al., 2019); its successor GPT-3—a version of which was released to the public as ChatGPT—was\nscaled up to 175 billion parameters and was trained on much of the open internet, including the\nCommon Crawl dataset, digitized books, and all of Wikipedia (Brown et al., 2020); the exact details\nof the latest model, GPT-4, have not been disclosed (OpenAI, 2023), but it was trained on texts,\nimages, and audio and is rumored to contain 1.7 trillion parameters. The largest models excel\nat transfer learning, and the capacity to generalize well to many new tasks has led them to be\ntermed “foundation models” (Bommasani et al., 2022). For example, GPT-4 not only achieved\nstate-of-the-art performance on a variety of machine learning benchmarks but outscored the majority\nof people on standardized tests, including the Uniform Bar Exam, Advanced Placement Biology,\nand the Verbal GRE (OpenAI, 2023).\nWhile neural language models can have a variety of different formats (e.g., Mikolov et al., 2013a;\nHoward and Ruder, 2018; Jiang et al., 2024), the most popular frameworks are based on an\narchitecture known as the transformer. Proposed by Vaswani et al. (2017), the transformer consists\nof two key components, anencoder and a decoder, which were developed in earlier work in machine\ntranslation (Cho et al., 2014). The encoder takes in an input sequence of tokens x = x1, ..., xn,\nwhere each token may represent a word or a smaller linguistic unit, like a sequence of characters,\nand converts them into a sequence of continuous valued vectors, or embeddings, h = h1, ..., hn.\nThe decoder then uses the continuous representation from the encoder to generate an output text.\nIt works by mapping the embeddings from the encoder onto a conditional probability distribution\nof the next tokens and autoregressively generating the output tokens one by one from left to right.\nAt each step, the previously generated tokens are recursively input into the decoder, providing\n6\nLarge language models for text classification\ncontext to predict the next token. Through this process, the model outputs a sequence of text tokens\ny = (y1, ..., ym), such as a translation from the original text into another language. The main\ninnovation in the transformer model is the use of a self-attention mechanism, which enables the\nmodel to compute a weight for each token in the sequence based on its relevance to every other\ntoken. Martin and Jurafsky (2024, 214) describe the self-attention mechanism as “a way to build\ncontextual representations of a word’s meaning that integrate information from surrounding words,\nhelping the model learn how words relate to each other over large spans of text.” This mechanism\nenables language models to capture long-range dependencies in texts, addressing the tractability\nissues that constrained n-gram language models. Transformer models typically consist of stacks\nof encoder and decoder blocks to progressively refine the representations and ultimately generate\nthe final output (see Wankmüller, 2022 and Martin and Jurafsky, 2024 for further detail). The\nself-attention operation can also be parallelized during training (Vaswani et al., 2017), making it\nfeasible to train on much larger corpora of text than earlier approaches, although this comes at the\ncost of increased memory complexity and necessitates the use of large amounts of computing power.\nThere are many different variants of the transformer architecture. Broadly, these architectures can\nbe grouped into three categories, but all make use of the attention mechanism. Encoder-only models\nsuch as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019)—\none of the most widely used transformer models1—only use the encoder portion of the transformer.\nBERT is pre-trained using masked language modeling, where 15% of the input tokens are hidden, or\nmasked, and must be predicted by the model. It uses bi-directional attention, parsing inputs in both\ndirections, from left-to-right and right-to-left, using the surrounding context to predict the masked\ntokens.2 In contrast to word2vec, which assigns each word a single vector representation, this\napproach yields contextualized embeddings that adapt to surrounding text, capturing richer semantic\ninformation and improving performance across a variety of NLP tasks (Smith, 2020). However,\nthe masked language modeling pre-training task is inadequate for text generation. Decoder-only\nmodels, like those in OpenAI’s GPT family, are pre-trained using next token prediction tasks, more\nakin to earlier language models (Radford et al., 2018). Decoders are sometimes referred to as\n“causal” language models because masking is used to prevent the model from attending to later\ntokens in the input sequence,3 such that the model cannot “see into the future” (Raffel et al., 2020,\n17). When trained using large amounts of data, decoder-only models can generate high-quality\ntexts. Finally, encoder-decoder systems such as Meta’s BART and Google’s FLAN-T5 retain a\nstructure that more closely resembles the original transformer (Lewis et al., 2019; Raffel et al., 2020),\n7\nLarge language models for text classification\nbased on the assumption that both modules are beneficial for various downstream tasks. Below we\ncompare models with these different architectures to assess how they differ in performance on text\nclassification tasks.\n2.2.1 From zero-shot learning to instruction-tuning\nPerhaps the most notable feature of the most recent generation of LLMs is the capacity to interface\nwith models using text. Almost any natural language processing problem can now be posed by\nproviding a text as an input and generating a new text as an output (Radford et al., 2019; Raffel\net al., 2020). For example, a model might take a sentence as input and yield a translation as its\noutput. The text-to-text interface represents a fundamental shift from classical approaches to natural\nlanguage processing, where the analyst manually converts texts into numeric vectors, often referred\nto as features, which are then input into a model. This involves decisions regarding how to tokenize\nthe texts, such as whether to use words, n-grams, or character n-grams, and the type of vector\nrepresentations, which vary from simple counts to embeddings derived from language models. 4\nOther features, such as non-textual information like follower counts on social media, can also be\nappended to the vector representation. Depending on the task, machine learning algorithms are\nthen used to learn weights that map features onto labels, or, in unsupervised learning, to inductively\nsummarize the data (Martin and Jurafsky, 2024; Evans and Aceves, 2016). In contrast, raw texts\ncan be directly input to text-to-text LLMs, and tokenization and vectorization happen automatically,\nas the inputs are converted into the required format. 5 Other contextual information can also be\ninput alongside the texts, as we demonstrate in our final set of analyses. The decoder module then\ngenerates an output sequence based on the conditional probability distribution of next words given\nthe input.\nTo control the way models convert inputs into outputs, written instructions known as prompts\ncan be included alongside the inputs. Extending the potential of transfer learning, this affordance\npresents a more radical possibility: LLMs can generalize to new tasks without any additional\ntask-specific training, known as zero-shot learning (Radford et al., 2019) For example, one could\ninput a newspaper article and ask the model to determine whether it uses a certain kind of framing.\nThe model might produce a reasonable answer, despite never having been explicitly trained to\nperform the task, because it has been pre-trained on vast amounts of text, including newspaper\narticles and other relevant material, allowing it to interpret the input text and how it relates to\nthe prompt. Unlike earlier approaches to transfer learning, these techniques do not require that a\n8\nLarge language models for text classification\nmodel’s weights be updated using training data, and a single model can be prompted to perform\nmany different tasks. A nascent field known as prompt engineering examines how variations in\nwording, structure, and formatting of prompts can influence performance. For example, asking a\nmodel to write a short explanation for its reasoning, known as chain-of-thought prompting, can\nresult in better accuracy (Wei et al., 2023a) and simply including the sentence “Let’s think step\nby step” at the end of the prompt can improve zero-shot performance (Kojima et al., 2024). One\ncan also provide models with more guidance by including labeled examples along with the prompt,\nknown as few-shot or in-context learning (Brown et al., 2020). The additional information contained\nin the examples can help to improve the quality of the output, particularly for difficult tasks where\nthe prompt alone is insufficient. For instance, one could append some newspaper articles that have\nalready been coded to the prompt to help guide the model to identify the framing. When performing\nin-context learning, models use both the semantic information from the prompt and labels and the\ninput-label mappings from the examples to make predictions (Wei et al., 2023b). Taken together,\nprompt-based learning regimes represent a paradigm shift in the way that we interact with language\nmodels, opening up new possibilities for social scientific inquiry.\nA shortcoming of prompting is that there is no guarantee that models will follow instructions or\ngenerate the desired outputs. In some sense, it is happenstance that the most likely tokens generated\nby the model often correspond to the outputs we are interested in. To enhance the capacity of LLMs\nto follow instructions, pre-trained models can be fine-tuned using large quantities of instruction\nand output pairs (Wei et al., 2022), known as instruction-tuning. This technique can improve\nperformance on zero-shot and few-shot learning tasks and yield better quality text generations.\nInstruction-tuning the largest models can result in additional performance gains (Chung et al., 2022),\nfurther demonstrating the importance of scaling. LLMs can automatically improve their capacity to\nfollow instructions via a process known as reinforcement learning with human feedback (RLHF)\n(Ziegler et al., 2020), which helps to align the outputs with user expectations and is central to the\nconversational capabilities of chatbots like ChatGPT. To perform RLHF, human raters select the\nbest responses to queries from a set of possible candidates, and this information is used to train the\nmodel to optimize its generations to be more consistent with human preferences (Ouyang et al.,\n2022). Instruction-tuning can also improve efficiency by using outputs from larger “teacher” models\nas instructions to tune smaller models (Wang et al., 2023). For example, Taori et al. (2023) used 52k\noutputs from a 175B parameter GPT-3 model to fine-tune Llama 7B. Their model, dubbed Alpaca,\nis an order of magnitude smaller but generates texts that are qualitatively similar to those from the\n9\nLarge language models for text classification\nteacher model. LLMs that have undergone these additional instruction-tuning techniques can serve\nas a strong foundation for subsequent tasks (Bommasani et al., 2022), and, as we explore below,\ninstruction-tuning can be used to further adapt them for social scientific research (Wei et al., 2022).\n2.2.2 New methodological challenges\nLLMs have created many new opportunities for methodological advancement, but these technologi-\ncal innovations also raise several new challenges. Due to the cost of training LLMs (Strubell et al.,\n2019)—training the latest models can run into hundreds of millions of dollars considering hardware\nand electricity costs—researchers increasingly rely upon models trained by third parties. Some of\nthese models, like BLOOM,6 are open-source, meaning that information on the training data, under-\nlying code, and the models itself are public. Due to the resource investments needed to train these\nmodels, however, most have been developed by a handful of technology companies. Rather than\nopen-sourcing their models, some companies have released open-weights models that anyone can\nuse while keeping other proprietary details, such as the code and training data, private. Other models,\nparticularly the largest state-of-the-art models, including Google’s Gemini, Anthropic’s Claude,\nand OpenAI’s GPT models, are only accessible using paid application programming interfaces\n(APIs), and the use of these closed models in academic research raises concerns about transparency,\nreproducibility, and privacy (Spirling, 2023; Palmer et al., 2024). Others argue that LLMs will\nreproduce stereotypes and biases due to training on vast amounts of unvetted data (Bender et al.,\n2021; Weidinger et al., 2022). Several recent papers consider how these factors affect the use of\nLLMs in social scientific research (Grossmann et al., 2023; Bail, 2024; Davidson, 2024). Following\nour analyses, we return to these issues as they pertain to text classification and provide guidance to\naid researchers in addressing various obstacles to using LLMs in sociological research.\n2.3 Text classification using large language models\nUnlike other machine learning algorithms, language models are not strictly supervised or unsuper-\nvised and can be used as a foundation for many downstream applications (Bommasani et al., 2022).\nWe focus on the use of LLMs for text classification, an application of supervised machine learning\nused to categorize texts into pre-determined classes. This task differs from unsupervised approaches\nsuch as topic modeling, which are used to inductively summarize and group texts (Evans and Aceves,\n2016; Nelson, 2017; Molina and Garip, 2019). Text classification is widely used in computer science\nfor tasks such as spam filtering (Méndez et al., 2006), sentiment analysis (Pang and Lee, 2008), and\n10\nLarge language models for text classification\nhate speech detection (Davidson et al., 2017). Over the past decade, the technique has been adopted\nby sociologists for a variety of tasks, including the analysis of activism on Twitter (Hanna, 2013),\ncoverage of inequality in newspaper articles (Nelson et al., 2018), and workplace feedback (Nelson\net al., 2023). The capacity to perform new tasks with little or no additional training makes LLMs\nparticularly promising for empirical research (Do et al., 2022). This lowers the bar for entry by\nreducing the costs associated with data annotation, making supervised text classification a more\nviable option for social scientists. Several studies show how transformer-based language models\noutperform conventional machine learning algorithms at various classification tasks, including the\ndetection of emotional language (Widmann and Wich, 2022), nationalist, populist, and authoritarian\nrhetoric (Bonikowski et al., 2022), and discussion of policy in news articles (Do et al., 2022).\nMost existing work in the social sciences focuses on fine-tuning encoder-only models, but some\nrecent scholarship applies zero- and few-shot learning, building upon studies by AI labs that\ndemonstrate impressive performance of these techniques across a range of NLP tasks (e.g. Radford\net al., 2019; Brown et al., 2020; Raffel et al., 2020). These studies have mixed findings regarding the\nefficacy of the latest innovations in language modeling. Wankmüller (2022) conducts experiments\nusing variants of BERT, which can be adapted to perform rudimentary zero-shot learning, but\nfinds they fare poorly at predicting political sentiments and toxicity compared to fine-tuned models.\nAn analysis across fifteen psychological constructs found that zero-shot GPT-3 and GPT-4 often\nperformed equally or better than fine-tuned models, and the GPT-4 performed particularly well\nacross multiple languages (Rathje et al., 2023). The most comprehensive analysis compares the\nperformance of zero-shot FLAN and GPT models with a baseline encoder model, RoBERTa, which\nshares the BERT architecture but is trained on more data (Liu et al., 2019). Across twenty different\nprediction tasks, including emotion, misinformation, and ideology detection, Ziems et al. (2024)\nshow that the larger generative models rarely outperform fine-tuned RoBERTa. Models with more\nparameters tend to perform better than smaller variants, consistent with other work (Raffel et al.,\n2020; Brown et al., 2020). However, performance also varies by architecture: encoder-decoder\nFLAN models achieve the highest scores on nine tasks, while larger decoder-only GPT models\nperform best on the remainder. They also conduct few-shot learning using the FLAN models but\nsee little evidence of improvement. Beyond size and architecture, the authors also point to the\nimportance of subsequent training, showing that the variant of GPT-3 that had undergone RLHF\noutperformed the pre-trained version by an average of 3.5 points (based on F1 scores described\n11\nLarge language models for text classification\nbelow). Taken together, this study suggests that fine-tuning may still be a superior strategy to zero-\nand few-shot learning.\nWe build upon existing studies in several ways. In the following sections, we compare four different\napproaches to text classification using LLMs, ranging from zero-shot and few-shot learning to\nfine-tuning and instruction-tuning. Across these learning regimes, we evaluate how common\nmethodological decisions—including prompt construction, few-shot example selection, and the\nsize of the training data—affect predictive accuracy. We also consider a broader range of models\nthan other studies, testing ten LLMs that vary in size, architecture, and openness. Our goal is to\nconsider how these factors impact predictive accuracy to identify the most effective approaches and\nto illuminate the trade-offs faced by researchers using these models.\n2.4 Stance detection in social media posts\nWe use stance detection as an application of supervised machine learning. The goal of stance\ndetection is to identify attitudes, beliefs, or opinions expressed toward a target, such as a person,\ninstitution, or policy. The methodology was developed by computer scientists interested in studying\nonline debates (Somasundaran and Wiebe, 2010; Sobhani et al., 2015; Mohammad et al., 2017) and\nhas become a popular approach for studying social media discourse (see Aldayel and Magdy, 2021\nand Küçük and Can, 2020 for reviews). Prior work demonstrates that neural network architectures\ncan achieve competitive performance on stance detection benchmarks (Augenstein et al., 2016)\nand recent research finds that LLMs achieve reasonable accuracy in zero- and few-shot settings\n(Allaway and McKeown, 2020; Burnham, 2024; Ziems et al., 2024). We build upon these studies\nby considering how variation across models and learning regimes impacts the performance of\nstance detection classifiers and by considering how the generative capabilities of LLMs enable more\nsophisticated approaches to measuring stance in online discussions.\nTo perform stance detection, documents are annotated with labels like “Support/Oppose” or “Fa-\nvor/Against,” typically with a category, such as “Neither” or “Neutral” that captures ambivalent\nor irrelevant texts (Somasundaran and Wiebe, 2010; Mohammad et al., 2016). Often, stance de-\ntection also involves the identification of the target towards which the stance is directed, known as\n“multi-target” stance detection (Sobhani et al., 2017). Many readers will be more familiar with a\nrelated technique known as sentiment analysis, which is used to categorize the valence or tone of a\ntext (e.g., “Positive,” “Negative,” or “Neutral”) and has gained popularity among social scientists.\nSociologists have used sentiment as a proxy for attitudes towards groups like immigrants (Flores,\n12\nLarge language models for text classification\n2017), women (Shor et al., 2015), and ethnic and racial minorities (Felmlee et al., 2020; V oyer\net al., 2022). Following Bestvater and Monroe (2022), we contend that stance detection would have\nbeen a more suitable measurement strategy in these studies and similar applications. While stance\nand sentiment are often correlated (e.g., statements of support often use more positive language)\n(Mohammad et al., 2017), it is common to observe mismatches. As Bestvater and Monroe (2022,\n19) note, “political opinions are typically complex and multidimensional enough that it is trivial to\nexpress them either negatively or positively.” Using sentiment as a proxy for stance can result in\nsubstantial measurement error, particularly when the correlation between sentiment and stance is\nweak.7 We searched six generalist or methodology journals in sociology and found twenty articles\nthat mentioned sentiment analysis and several applications of the technique, but no mention of\nstance detection.8 We suspect the neglect of stance detection in sociology stems from the fact that it\nhas historically been difficult to implement because it requires the development of domain-specific\nannotated corpora and familiarity with supervised machine learning (Sen et al., 2020; Bestvater\nand Monroe, 2022), whereas sentiment analysis can be performed using off-the-shelf lexicons or\nclassifiers.9 We anticipate that stance detection will be a valuable methodology in many areas of\nsociological research since scholars are often interested in measuring attitudes, beliefs, and opinions\nexpressed in texts.10\n3 Data\nAs a case study, we consider stances expressed in online political debates. Specifically, we use three\ndatasets containing social media posts annotated for the stance towards the two leading candidates\nin the 2016 US Presidential election. This is an ideal case to explore because the election was a\nclassic example of what political scientist John Zaller (1992) calls a “two-message issue,” where\nmost people tend to pick a particular side or, in this case, a candidate. By comparing two different\nsocial media platforms, we can assess how LLMs perform across different contexts and approaches\nto stance detection.\n3.1 Twitter dataset\nThe first dataset consists of 1,691 tweets annotated for their stance towards Donald Trump or Hillary\nClinton, derived from SemEval, a widely-used benchmark dataset (Mohammad et al., 2016).11 Each\ntweet corresponds to a single target, Trump or Clinton, and is labeled with one of three stances:\n“Favor,” “Against,” or “None.” Each stance annotation represents the majority label from eight or\n13\nLarge language models for text classification\nmore crowd workers. The target distribution is imbalanced, with 707 tweets mentioning Trump\nand 984 mentioning Clinton (see Table S1 in the Supplemental Materials). We hold out 339 tweets\n(20% in total) for testing, balanced evenly across the two candidates. There is also an imbalance\nwith respect to the stances: tweets mentioning Trump tend to favor his candidacy, whereas those\nmentioning Clinton tend to be against her. Across both targets, tweets with the Against label occur\nmore than twice as often as those annotated as Favor. A potential issue with this dataset is that it\nmay have been indirectly or directly incorporated into LLMs during pre-training, either through\nweb pages scraped from the internet or through its use in instruction-tuning. This kind of data\n“contamination” undermines the principle that test data should be unseen during training. However,\nevidence suggests such contamination does not necessarily lead to perfect memorization, as models\noften generalize rather than memorize verbatim (Brown et al., 2020).\n3.2 Facebook comments\nThe second dataset is a random sample of 2400 top-level comments (i.e., not replies to other\ncomments) written on the Facebook pages of Donald Trump and Hillary Clinton from June 1,\n2016, until the election on November 8. This is a novel dataset, so there is no risk that the data\nhave already been seen during pre-training. 12 Each comment was annotated by a single author\nwith expertise in the topic. Following Do et al. (2022), we consider this to be a realistic setting\nfor many applied researchers who may not have the resources to annotate large datasets, which\noften requires training research assistants or the use of crowdsourcing platforms. Moreover, prior\nwork demonstrates that larger corpora with a single annotation per example can be more efficient\nthan using multiple annotations per example (Barberá et al., 2020). Unlike the SemEval dataset,\nwhich restricts each tweet to a single target, each comment was annotated for its stance towards\nTrump and Clinton. For consistency, we use a three-category annotation scheme similar to the\nprevious task: “Favor,” “Against,” and “Neutral/None.” 13 The stance labels are only used if a\ncomment expresses a stance toward a target, meaning that we do not infer that an expression of\nsupport for Clinton implies opposition to Trump and vice versa. This process yields a tuple for\neach comment, such that a comment like “I’m never voting for Trump!” would be labeled {Against\n[Trump], Neutral/None [Clinton]}. There are 1200 comments from each Facebook page, but the\ndistribution of the annotations is imbalanced in similar ways to the Twitter dataset (see Table S2 in\nthe Supplemental Material). Comments favoring Trump outnumber those against him more than\ntwo-to-one, whereas the inverse is true for Clinton. Only a small fraction of comments, 7%, express\n14\nLarge language models for text classification\na stance towards both candidates. The most frequent class is comments with no stance towards\neither candidate. We use 2000 comments for training and reserve 400 for testing.\n3.3 Facebook comment-reply threads\nOur third set of analyses extends the stance detection task to longer comment-reply threads. To\nunderstand political debates on social media, we cannot simply consider individual tweets or\ncomments but must investigate the threaded structure of online conversations (Backstrom et al.,\n2013; Berry and Taylor, 2017; Shugars and Beauchamp, 2019). In theory, threads furnish additional\ncontext that should help the model predict the stance expressed in comments, and it makes little\nsense to analyze replies abstracted from the conversational context (Murakami and Raymond, 2010;\nWalker et al., 2012; Sridhar et al., 2015). Extending insights from multitask learning (Caruana, 1997),\nwe expect that models will predict stances expressed in commentsand replies more accurately when\nthey are situated in a thread rather than taken out of context. However, it is difficult to integrate this\nkind of structured information into conventional classification approaches. Generally, most models\nusing thread data make inferences about entire threads, such as whether the conversation devolves\ninto personal attacks (Zhang et al., 2018) or whether a comment promotes discussion (Naab et al.,\n2023). This typically involves the construction of hand-picked features to characterize the thread\nand its constituent texts that can be input alongside vectorized representations of texts. Graph-based\nmethods have been proposed for inferring stances of posts in reply threads, but these techniques also\nrely on hand-crafted features that are tedious to scale (Zubiaga et al., 2016). In contrast, text-to-text\nLLMs enable us to use the entire thread as input and for the model to automatically parse the\ninformation about each text. We can also request variable-length responses, such that models can\nreturn different predictions depending on the length of the thread. Our goal is to jointly classify\ncomments and replies from different social media users using a single model.\nWe draw these threads from the same corpus as the previous Facebook task. While some comments\ncan spawn extended discussions, the modal comment receives zero replies, and the modal number of\nreplies among those with at least one reply is only two. Despite their sparsity at the comment level,\n20.8% of the texts in the corpus are replies, so a substantial amount of discourse is missed if we only\nconsider top-level comments. Each thread includes the page name, the original post text, and the\ntext of any comments and replies, as well as pseudonyms for their authors.14 To avoid annotating\nexcessively long threads, we restrict our analyses to the first five replies (0.56% of comments in the\ncorpus have more than five replies). We construct balanced training (N = 1200threads) and test\n15\nLarge language models for text classification\n(N = 300) datasets by randomly sampling threads from the corpus. In each case, there are equal\nnumbers of threads with zero through five replies (including longer threads that are truncated to the\nfirst five replies). For each thread, we use the same annotation scheme as the Facebook task, labeling\neach comment and reply with a stance toward both Trump and Clinton. We use Support/Oppose\nrather than Favor/Against as the stance labels, as we consider these labels to be clearer but chose to\nretain the terminology used in the SemEval task (Mohammad et al., 2016) for the previous analyses.\nAn initial balanced set of threads ( N = 36) was annotated by one of the authors and a graduate\nstudent with expertise in American politics. The annotators were instructed to take the entire thread\ninto account when making decisions about the stance in each text. As such, the interpretation of a\ncomment could be influenced by both the original post and any replies, and any replies could be\nunderstood in the context of the entire thread. In this initial evaluation, 90% of the stance labels\nwere consistent across the two annotators. Nearly all disagreements related to whether a stance\nwas present or not rather than the valence of the stance (e.g., one annotator rated a comment as\nSupport and another Neutral rather than one Support and another Oppose). After discussing the\ndiscrepancies, the graduate student then annotated the remaining threads. These evaluation data\nwere also used for prompt development to avoid overfitting the prompt to the training data during\ndevelopment.\nEach thread is represented using JavaScript Object Notation (JSON), a common data format that\nallows us to represent threads in a nested structure. This format is well-suited for LLMs since they\nare often trained on large amounts of code and associated data structures (Ziems et al., 2024).15 The\nfollowing example shows a comment with three replies on a post from Hillary Clinton’s Facebook\npage:\nJSON thread example:\n{\"post\": {\n\"page\": \"hillaryclinton\",\n\"text\": \"Equality is on the ballot.\\nJustice is on the ballot.\\nOur progress is on the\nballot. IWillVote.com\"\n},\n\"comment\": {\n\"author\": \"Ian\",\n\"text\": \"Sorry but it’s time to step down!!! Nothing you say or do anymore is believable.\"},\n\"replies\": [\n16\nLarge language models for text classification\n{\"reply_id\": 1,\n\"author\": \"Kristie\",\n\"text\": \"Go Hillary!\"},\n{\"reply_id\": 2,\n\"author\": \"Arman\",\n\"text\": \"Kristie Obama just cancelled all his upcoming Hillary campaign events ! Whatever\ninformation the FBI has found must be completely devastating for Clinton. So devastating,\nthat President Obama can no longer even be seen as supporting her candidacy! This FBI\nannouncement has \\\"criminality\\\" written all over it.\"},\n{\"reply_id\": 3,\n\"author\": \"Sabrina\",\n\"text\": \"She needs to step down\"}]\n}\nIn this example, a commenter responded to Clinton’s post, expressing a negative stance by asking\nher to “step down” and questioning her honesty. Another Facebook user, Kristie (all names are\npseudonyms), replied to this comment with a short supportive message, “Go Hillary!”. The reply is\nfollowed by a longer response, tagging Kristie and sharing a message insinuating that Clinton is a\ncriminal. Finally, a fourth person enters the conversation, reiterating the claim from the original\ncomment. The text below shows the labels corresponding to this thread, which are also stored in\nJSON format:\nJSON labels:\n{\"comment\": {\n\"stanceTrump\": \"Neither\",\n\"stanceClinton\": \"Oppose\"},\n\"replies\": [\n{\"reply_id\": 1,\n\"stanceTrump\": \"Neither\",\n\"stanceClinton\": \"Support\"},\n{\"reply_id\": 2,\n\"stanceTrump\": \"Neither\",\n\"stanceClinton\": \"Oppose\"},\n{\"reply_id\": 3,\n\"stanceTrump\": \"Neither\",\n\"stanceClinton\": \"Oppose\"}]}\n17\nLarge language models for text classification\nThread prediction is a much more demanding prediction problem than predicting the stance for a\nsingle comment or tweet. Threads with one comment and five replies have 96 unique combinations\nof labels and, for any given input, there are P6\nn=1 9n ≈ 589k unique label permutations, assuming\nthat a model must output not only the stance labels but the appropriate quantity of predictions.\nThis task enables us to assess the capacity of LLMs to handle more complex data structures than\ntraditional ML models and the extent to which they can analyze language use in a more realistic,\nconversational context.\n4 Models\nWe compare ten language models of varying architecture, size, and openness to examine which\nfactors have the greatest impact on the predictive performance. Broadly speaking, these models\ncan be categorized into three groups: small, encoder-only models that specialize in encoding\nsemantic information; medium to large decoder-only and encoder-decoder models that are capable\nof autoregressive text generation; and large proprietary, decoder-only models that can only be run\nusing third-party APIs. Table 1 gives an overview of each of the models, including the architecture,\nnumber of parameters, computing system, and whether or not they are open-weights.\nThe encoder-only models are all based on the BERT architecture (Devlin et al., 2019), which\nhas been used in several recent sociological analyses (Ren and Bloemraad, 2022; Bonikowski\net al., 2022; Le Mens et al., 2023). BERT is a bi-directional encoder model trained on a masked\nlanguage modeling task, where the objective is to predict one or more masked words in an input. For\nexample, “[MASK] cat [MASK] a mouse.” We use the smallestbert-base-uncased version.16\nBERT is relatively modest in scale compared to more recent language models, with twelve layers\nof transformer modules that total 110 million parameters. We also evaluate two extensions of\nthe architecture. SentenceBERT (SBERT) adds a pooling operation to output vectors (Reimers\nand Gurevych, 2019) to produce fixed-size document-level embeddings. DeBERTa (Decoding-\nenhanced BERT with disentangled attention) incorporates two techniques that improve the attention\nmechanism and the encoding process. We use versions of SBERT and DeBERTa that are comparable\nto BERT in size, namely all-mpnet-base-v2 and deberta-v3-base, and are the most recent\nversions at the time of our analysis. All of these models are small enough to be used on a high-end\npersonal computer (see Section C of the Supplemental Materials for further information).\nThe second medium-to-large family includes three different architectures, each with billions of\nparameters. These models are not only far larger than the encoder-only models but are also equipped\n18\nLarge language models for text classification\nModel Architecture Parameters System Open-weights\nBERT Encoder 110M Personal computer ✓\nSBERT Encoder 109M Personal computer ✓\nDeBERTa Encoder 86M Personal computer ✓\nFLAN-T5 XXL Encoder-Decoder 11B HPC cluster ✓\nMistral-7B Decoder 7B HPC cluster ✓\nLlama3-8B Decoder 8B HPC cluster ✓\nLlama3-70B Decoder 70B HPC cluster ✓\nGPT-3 Ada Decoder 350M API ✗\nGPT-3 Davinci Decoder 175B API ✗\nGPT-4o Decoder 1.7T API ✗\nTable 1: Model comparisons\nwith the autoregressive decoder component that allows for text generation and can be further\nfine-tuned to follow instructions in the prompts, making them ideal candidates for zero- and few-\nshot learning. FLAN-T5, developed by Google, uses the Text-to-Text Transfer Transformer (T5)\narchitecture where both the input and the output are natural language (Raffel et al., 2020). FLAN-\nT5 was created by instruction-tuning T5 on over one thousand distinct NLP tasks to improve its\ngeneralizability and instruction-following, including the use of chain-of-thought prompting (Chung\net al., 2022). We use the largest variant, flan-t5-xxl, which achieved strong performance on a\nstance detection task derived from our Twitter dataset (Ziems et al., 2024). Next, Mistral, developed\nby French company Mistral AI, is a decoder-only architecture intended to balance high performance\nand efficiency (Jiang et al., 2023). We use Mistral’s 7B parameter model, Mistral-7B-v0.3,\nreleased in May 2024. Finally, the Llama family of models developed by Meta have achieved\ncompetitive performance by training on more data without necessarily creating a larger model\n(Touvron et al., 2023). We use the two third-generation Llama models released in April 2024,\nLlama-3-8B, comparable in size to the other models in this category, and Llama-3-70B, the\nlargest open-weights model evaluated. Both were trained on 15 trillion tokens of text from public\nsources, although the exact training data are not disclosed. These models are also decoder-only and\nincorporate modifications to the attention mechanism and embeddings that enhance performance.\nWe used a high-performance computing (HPC) cluster with advanced GPUs to run these models\n(see Section C of the Supplemental Materials).\nThe final, large proprietary model family consists of several GPT variants released by OpenAI. GPT-\n3 is the third iteration of LLM developed by OpenAI and demonstrates strong performance in zero-\nand few-shot settings (Brown et al., 2020). We comparetext-ada-001 and text-davinci-003—\nhereafter Ada and Davinci—released in November 2022. 17 Ada, the smallest variant, has 350\n19\nLarge language models for text classification\nmillion parameters and was trained on 40GB of text data (making it more comparable in size to\nthe encoder-only models described above). Davinci has 175 billion parameters and was trained on\n45TB of text. The version we use has undergone additional refinement using RLHF to improve its\nperformance (Ouyang et al., 2022) and shows better performance on social science classification\ntasks compared to the standard pre-trained version (Ziems et al., 2024). A variant of this model was\nreleased as ChatGPT in late 2022. We compare these third-generation GPT models with GPT-4o,\none of the most powerful language models at the time of writing, released in 2024. The technical\ndetails have not been made public, but it is a version of GPT-4 that was trained on text, images, and\naudio and is claimed to have 1.7 trillion parameters (Google’s largest Gemini model is rumored\nto be similar in scale).18 We were unable to fine-tune GPT-4o, so it is only used for the zero- and\nfew-shot analyses.19 We use the OpenAI API to interact with the models running on their servers.\nEach query costs a small fee, and the larger, more computationally intensive versions are generally\nmore expensive.20\n4.1 Baselines\nPrior work finds that transformer-based models perform favorably compared with conventional\nmachine learning algorithms across a range of tasks relevant to social scientists (Widmann and\nWich, 2022; Bonikowski et al., 2022; Wankmüller, 2022). To assess how the LLMs evaluated here\nfare relative to earlier techniques, we calculate baseline scores for a subset of the specifications\nusing four different approaches, varying the feature representation (bag-of-words vs. embeddings)\nand learning algorithm (support vector machine (SVM) vs. convolutional neural network (CNN)).\nThese baseline models are described in further detail in the Supplemental Material (Section B). We\nalso compare the results against a random baseline. Our final thread-prediction task is structured\nas a text-generation task and cannot be performed using classical algorithms. Instead, we use the\nbest-performing LLM from our Facebook comment analyses to predict stances for each comment\nand reply separately.\n5 Experiments\n5.1 Zero-shot and few-shot learning\nWe perform zero-shot learning for all three tasks. In each case, we provide a prompt and a test\nexample as input and use the model output as a prediction. For the tweet and comment prediction\n20\nLarge language models for text classification\ntasks, we also implement few-shot learning. Since each tweet only includes a single target, we\ngive one example for each target for the few-shot learning task (two-shot), whereas we use a single\nexample for the Facebook comment task (one-shot). While more examples could theoretically be\nused—as many as fit into the context window, which defines the maximum number of tokens that\ncan be input into a model at once (Brown et al., 2020)—we restrict our focus to these simple cases\nto examine whether a modest amount of data can help to improve upon zero-shot learning.\nEncoder-only models (BERT, SBERT, and DeBERTa) are not designed to be prompted using text\ninputs and thus cannot perform zero- or few-shot learning like the decoder models. Table 2 describes\nhow each model is used in our analyses. However, it is possible to use fine-tuning to further adapt\nthese models to perform zero-shot classification (Yin et al., 2019; Wankmüller, 2022; Laurer et al.,\n2024). To explore this possibility, we use DeBERTa-v3-base-mnli-fever-anli, a version of\nDeBERTa fine-tuned for zero-shot classification (Laurer et al., 2024). The model is fine-tuned on\n764k pairs of sentences from three datasets designed for Natural Language Inference (NLI). The\ngoal of NLI is to measure relationships between pairs of sentences, known as the “hypothesis” and\nthe “premise.” In this case, we can consider the input text as the premise and the stance label as the\nhypothesis (Yin et al., 2019; Burnham, 2024). For example, the premise “Hillary will be a great\npresident” entails the hypothesis “Favors Clinton”. The model works by comparing the embeddings\nof the input text and stance labels, computing a similarity score to select the most likely label\n(see Laurer et al. (2024) for a more in-depth explanation). This procedure yields class predictions\nwithout additional training, even though the model lacks the generative decoder module.\nThe other models are all text-to-text and can process full prompts and other input examples. Where\npossible, we use variants of each model that have been instruction-tuned and optimized for conversa-\ntion via RLHF and related techniques. Specifically, the models are Mistral-7B-Instruct-v0.3,\nMeta-Llama-3-8B-Instruct, and Meta-Llama-3-70B-Instruct. We use the standard version\nof FLAN-T5 XXL, flan-t5-xxl, which is designed to follow instructions. Regarding the OpenAI\nmodels, GPT-3 Ada has not undergone any instruction-tuning, whereas GPT-3 Davinci and GPT-4o\nhave been tuned for instruction-following via RLHF.\n5.1.1 Prompt engineering\nFor zero- and few-shot learning, it is necessary to provide information to constrain the model to\nproduce the desired output. Relatively simple prompts can work for easy examples, but to achieve\nbetter performance, we must include more information on the task and the format of the output\n21\nLarge language models for text classification\n(Brown et al., 2020). To examine the impact of prompt engineering, we test three prompts for each\ntask that vary in length and the amount of information provided, discussed in further detail below.\nThis allows us to assess trade-offs between prompt complexity, predictive accuracy, and economic\ncosts (longer prompts consume more tokens). The best-performing prompts from these experiments\nare used in the relevant one- and few-shot prediction tasks.\nFor the few-shot models, we also evaluate the sensitivity to the examples included with the prompt\nas there is evidence that performance can vary depending on the choice of examples (Zhao et al.,\n2021). Moreover, prior work on stance detection finds that some test examples, particularly those\nthat do not explicitly mention a target, are considerably harder to predict (Sen et al., 2020; Burnham,\n2024). It is thus plausible that some examples will be more helpful than others when distinguishing\nbetween classes in few-shot settings. For each task, we conduct 100 replications using different\nexamples from the training data. In each case, we take a random sample without replacement from\nthe training data (stratified by the target for the two-shot task to ensure one example corresponding\nto each target is shown).21 Each example is concatenated to the prompt and used to predict the\nlabels for all texts in the test dataset. These experiments enable us to measure how the predictive\nperformance of few-shot learning varies according to the example(s) used.\nModel Zero-shot Few-shot Fine-tuned Instruction-tuned\nBERT ✓\nSBERT ✓\nDeBERTa ✓ ✓\nFLAN-T5 XXL ✓ ✓ ✓\nMistral-7B ✓ ✓ ✓\nLlama3-8B ✓ ✓ ✓ ✓\nLlama3-70B ✓ ✓ ✓ ✓\nGPT-3 Ada ✓ ✓ ✓\nGPT-3 Davinci ✓ ✓ ✓\nGPT-4o ✓ ✓\nTable 2: Learning regimes by model\nThe table lists the learning regimes used for models across all tasks. We perform zero-shot and few-shot learning for the Twitter and\nFacebook tasks and zero-shot learning for the Facebook comment-reply task. Fine-tuning is conducted on the Twitter and Facebook\ndatasets for text classification. Instruction-tuning is conducted on the Facebook comment-reply dataset for text generation, using a\nsubset of the decoder-only models that perform well in the other tasks.\n5.2 Fine-tuning\nWe evaluate how the performance of fine-tuning varies depending on the amount of data used. For\nthe Twitter task, we compare models fine-tuned on 10, 100, and all training examples. We repeat the\nsame for the Facebook comment task, additionally comparing models trained on 1000 comments\n22\nLarge language models for text classification\nand the full dataset, consisting of 2000 comments, enabling us to assess the relationship between\ntraining data size and performance for larger annotated samples.\nThe encoder-only models are fine-tuned by adding a layer known as a classification head to each\nneural network. This layer has randomly initialized weights, where each label is represented by a\nnumeric parameter. The classification head consists of a fully connected layer that outputs a logit\nfor each label and a softmax activation function that converts the logits into probabilities that sum\nto one. As the training examples are passed through the network, both the existing parameters and\nthose in the classification head are updated through backpropagation to minimize the cross-entropy\nbetween the predicted and true labels. Once trained, the test data are input into the fine-tuned\nmodels, which then output predicted probabilities for each class.\nFor the larger open-weights encoder-decoder and decoder-only models, the process differs in two\nways. First, the language modeling head of each model, the final part of the decoder component\nthat generates output tokens, is replaced entirely by the classification head. Fine-tuning thus\ncurtails the generative capacity of these models as they are adapted to generate the stance labels.\nSecond, due to the size of these models, it was necessary to use a technique known as QLoRA to\nperform this fine-tuning efficiently (Dettmers et al., 2023). In short, QLoRA enables us to update\na low-rank approximation of the weight matrix and to store the numeric values in a compressed,\nquantized format, substantially reducing memory consumption and compute time (see Section C in\nthe Supplemental Materials for further discussion). Fine-tuning of GPT-3 Ada and Davinci was\nperformed using the OpenAI API.22 OpenAI does not reveal exactly how its fine-tuning system\nworks, but the process appears to be similar, as fine-tuned GPT models appear to be constrained to\nproduce the tokens corresponding to the class labels and lose their generative capabilities.\nEach model also has various hyperparameters that can be modified during fine-tuning. Systematically\ntuning these parameters would be computationally expensive (and financially, in the case of the\nGPT models), so we instead tried to maintain similar settings across all models and generally used\ndefaults or those reported in previous literature. Further technical information on fine-tuning and\nhyperparameters is detailed in Section C of the Supplemental Materials.\n5.3 Instruction-tuning\nWe use instruction-tuning (Wei et al., 2022) for the thread-prediction task. This generative approach\nallows us to incorporate a dynamic label structure rather than specifying the fixed number of labels\n23\nLarge language models for text classification\nin advance, as is required for standard fine-tuning (if there are k classes, then the classification head\nis a k-dimensional vector). Classification tasks can be framed as a variant of instruction-tuning\nwhere inputs are the texts to be classified, a set of instructions, and the labels. In our case, we give\nthe instructions in a format known as a system prompt, which shapes the behavior of the model,\nand we continue to train on all Facebook thread JSONs and corresponding stance JSONs. When\ninput with a new thread from the test set, the instruction-tuned model should generate a JSON\nobject containing the correct stances for each comment and reply. As the Facebook threads in our\ndata have zero to five replies, the model should be able to identify the number of replies in each\nthread and the correct stances, generating output text with appropriate headings like the numeric\nidentifier for each reply. We discuss the prompt in more detail below. To perform these analyses,\nwe instruction-tune the two Llama3 models, which have already undergone instruction-tuning and\nRLHF (Touvron et al., 2023). This process is implemented using the Supervised Fine-Tuning trainer\nfrom the Python library trl. We compare the performance of instruction-tuned models to a baseline\nmodel predicting comments and replies in isolation and zero-shot learning using the JSON inputs.\n5.4 Evaluation metrics\nAll models are scored by calculating predictive performance on the held-out test data.23 The F1-\nscore is used in machine learning to measure the predictive performance of each classifier. It is the\nharmonic mean of precision and recall. For each class, precision is the number of true positives\ndivided by the number of true positives and false positives, andrecall is the number of true positives\ndivided by the number of true positives and false negatives. Precision captures how accurately a\nclassifier detects a particular class, whereas recall measures how many relevant examples were\ndetected. Ideally, we want to achieve high precision and recall, although there are cases where it\nmay be preferable to optimize for one over the other (e.g., Jensen et al., 2022, 46). The F1 score for\nclass k is defined as\nF1k = 2 precisionk · recallk\nprecisionk + recallk\nWe use a weighted F1 score to measure the performance across classes, F1w. This is calculated\nby taking a weighted average over each class, where K is the number of classes and Ntestk is the\nnumber of examples in the test set belonging to each class, and Ntest is the size of the test set:\n24\nLarge language models for text classification\nF1w =\nKX\nk=1\nF1k · Ntestk\nNtest\nFor example, F1Clinton is a weighted average of the F1 scores across three classes, “For,” “Against,”\nand “Neither/None.” We calculate several different versions of each score depending on the task.\nFor Twitter, we score models separately for target and stance prediction. For Facebook, we calculate\nthe accuracy for each target. We mostly focus on the strictest performance metrics, which we term\nF1joint, which count a prediction as correct when it exactly matches the original annotation. For the\nTwitter task, this implies that both the target and stance are correctly predicted. In this case K = 6\nbecause there are two possible targets and three stances for each target. For the Facebook task, the\nstances must be correct for both targets, thus K = 9, since there are three stance scores for each\ntarget and nine possible combinations of stance and target.24 Given the large label space for the\nthread-prediction task, we calculate aggregate scores at the comment/reply level.\nSince the test datasets are relatively small, it is possible that the scores on the held-out data do not\nadequately account for variation that would occur if the models were used on larger corpora of\nout-of-sample data. To account for this uncertainty, we calculate bootstrap confidence intervals\nfor each metric (Efron and Tibshirani, 1986). These are obtained by drawing N predictions with\nreplacement, where N is the size of the test data, and using these to calculate performance scores.\nThe procedure is repeated 10,000 times, and the results are aggregated to obtain 95% percentile\nconfidence intervals. These intervals capture variation due to the composition of the test data,\naccounting for uncertainty in the application of models to new data, although they do not capture\nother sources of uncertainty, such as the composition of the training data or stochastic variation in\nthe models, which is much more computationally expensive to evaluate.\n6 Results\n6.1 Prompt engineering and zero-shot learning\nWe created three different prompts for each task to assess the relationship between prompts and\npredictive performance. We begin with a simple prompt (Minimal) listing the basic information\nneeded to perform the task: the stance options and the format of the answer (see Table 3 for\nthe full prompts). This information is critical to ensuring consistent outputs that map onto the\nlabel schema. If this works, we assume that the model uses existing semantic information from\n25\nLarge language models for text classification\npre-training to interpret what target and stance mean and the relationship between these concepts\nand the input text. These prompts are extended by adding a short sentence describing the task more\nnaturally (Sentence), which could be helpful for models that have been instruction-tuned in natural\nlanguage. The final pair of prompts is more informative, describing how the statements refer to\npoliticians and represent attitudes (Context). This allows us to assess the extent to which additional\ncontext enhances the accuracy of the model. Ceteris paribus, we anticipate that more context should\nimprove performance because it includes more information about the task to the model. At the same\ntime, it is important to note that longer prompts also consume more tokens, making them more\ncomputationally expensive to use.\nPrompt Type Tokens †\nTwitter task\nReturn the TARGET [Trump/Clinton] and STANCE\n[Favor/Against/None]. Answer: {TARGET, STANCE}\nMinimal 29\nThis statement may express a STANCE about a TARGET.\nReturn the TARGET [Trump/Clinton] and STANCE\n[Favor/Against/None]. Answer: {TARGET, STANCE}\nSentence 43\nThis statement contains a TARGET and a STANCE. The target\nis a politician and the stance represents the attitude\nexpressed about them. The target options are Trump or\nClinton and stance options are Favor, Against or None.\nProvide the answer in the following format: {TARGET,\nSTANCE}\nContext 60\nFacebook task\nReturn the STANCE [Favor/Against/None] for Trump and\nClinton. Answer: {Trump: STANCE, Clinton: STANCE}\nMinimal 30\nThis statement may express a STANCE towards Trump,\nClinton, or both. Return the STANCE [Favor/Against/None]\nfor Trump and Clinton. Answer: {Trump: STANCE,\nClinton: STANCE}\nSentence 45\nThis statement may express a STANCE towards two\npoliticians, Trump and Clinton. Stance represents\nthe attitude expressed towards them. The stance options\nare Favor, Against or None. Provide the answer in the\nfollowing format: {Trump: STANCE, Clinton: STANCE}\nContext 54\nTable 3: Prompt variations\n† Token consumption for GPT-3 models calculated via OpenAI’s tokenizer:https://platform.openai.com/tokenizer\nThe results are shown in Figure 1, which displays the target-specific and joint F1 scores for each\nprompt evaluated on the relevant test data. Overall, the more informative prompts tended to achieve\nthe highest joint F1 score across both tasks. This pattern is clearest in the Facebook data, where\nthe two longer prompts significantly outperform the minimal example. The fact that the second\nprompt performs better, despite lacking any information about key terminology, suggests that more\n26\nLarge language models for text classification\ncomplete sentences can be helpful. Looking at the target-specific scores, the results show that the\npredictive performance for each candidate improves across the two prompts, particularly for the\nstance toward Trump. The results from Twitter are noisier, as the simple prompt seems to work\nbetter than the intermediate one, but the final prompt still performs better. Breaking the performance\ndown by candidate shows that this is due to an increase in performance for Clinton and a small\ndecline for Trump. In general, the labels in the Twitter dataset appear to be harder to predict, an\nissue we discuss further below. While we do not attempt to identify the best prompt exhaustively,\nthese results demonstrate substantial variation across prompts, underscoring the importance of\ncareful prompt selection for maximizing predictive accuracy.\nTwitter Facebook\nMinimal Sentence Context Minimal Sentence Context\n0.4\n0.5\n0.6\n0.7\n0.8\nPrompt\nF1\nClinton Trump Joint\nFigure 1: Zero-shot F1 scores by prompt variations\nThis figure shows the test set F1 scores for zero-shot learning with GPT-3 Davinci using the three different prompts for each task,\nwith Twitter on the left and Facebook on the right. Separate F1 scores for each candidate are shown, as well as the joint F1 score.\nThe error bars are 95% bootstrap confidence intervals.\n6.2 Example selection for few-shot learning\nThe one- and two-shot analyses were performed using the most detailed prompts from the preceding\nanalysis. Figure 2 shows the results of our experiments analyzing how performance varies as a\nfunction of the examples provided along with the prompts. Across both datasets, there is substantial\n27\nLarge language models for text classification\nTW Clinton 2−shot TW Trump 2−shot\nFB Clinton 1−shot FB Trump 1−shot\n0.3 0.4 0.5 0.6 0.7 0.8 0.3 0.4 0.5 0.6 0.7 0.8\n0\n10\n20\n0\n10\n20\nF1 score\nCount\n(a) F1 score distributions by target. The top row shows the\nresults for the Facebook task, and the bottom row for the\nTwitter task. Dashed lines denote the mean F1 score across all\nfew-shot examples.\n0.50\n0.60\n0.70F1 Trump\n0.40\n0.50F1 Trump\nFB 1−shot TW 2−shot\n0.60 0.70 0.80\nF1 Clinton\n0.50 0.60 0.70\nF1 Clinton\n(b) Correlations between target-specific F1 scores. The left\nfigure shows the results for the Facebook task, and the right\nfigure shows the Twitter task. Shaded regions indicate when\nmodels perform better for Trump (light) or Clinton (dark). The\ntrend line is indicated in black.\nFigure 2: Example variation and predictive performance.\nThese figures show the variation in predictive performance across different examples used in 1-shot or 2-shot learning. Panel (a)\nshows the distribution of F1 scores, and panel (b) shows the correlations between the target-specific F1 scores for each dataset.\nvariability in predictive performance on the test data, highlighting sensitivity to the examples. The\nF1 scores for the Twitter task range from 0.33 to 0.69, and the scores for Facebook range from\n0.50 to 0.84. In general, the models are more accurate at predicting the stance towards Clinton\nthan Trump, perhaps because the stances expressed toward Clinton are more consistently negative\nand are thus easier to detect. For the one-shot Facebook model, we observe a positive correlation\nbetween the F1 scores for each target. This suggests that the best examples help improve predictions\nfor both candidates. The correlation between the target-specific F1 scores is much weaker for the\nTwitter model, possibly due to the additional randomness induced by the two-shot setting since\nexample texts were sampled independently for each target. The shaded regions further show how\nalmost all models (all of those used on the Twitter test data) performed more accurately for Clinton\nthan Trump. Overall, these results highlight how LLM classifiers using few-shot learning can be\nhighly sensitive to the choice of examples, with evidence of wide variation in the out-of-sample F1\nscores.\nTo better understand how the characteristics of the example texts impact performance, we estimated\na series of regression models that use the stances in the examples to predict the target-specific F1\n28\nLarge language models for text classification\nscores, controlling for word count to account for the effect of example length. The full regression\nresults are reported in the Supplemental Material in Tables S3 and S4 (Section D). We find that\ncomments that favor one candidate over the other are associated with statistically significant\nincreases in performance in the Facebook predictions: Comments supporting Clinton are associated\nwith a 0.05 increase in the Clinton F1 score and a 0.07 increase in the Trump score compared\nto those with no stance towards Clinton; comments favoring Trump are associated with a 0.03\nincrease in the Trump F1 score. The R-squared statistics increase when the interaction between\nthe stances toward the two candidates is added, and the stance interactions in the model predicting\nthe accuracy for Trump are statistically significant. These findings suggest that the combination\nof stances expressed in the examples plays a role in performance. The patterns in the Twitter\npredictions are less conclusive but suggest that longer tweets help to improve the Clinton predictions\nand that tweets opposing Trump negatively impact the Trump predictions. These results show how\nthe qualities of the examples can impact the predictive performance of few-shot learning models. To\naccount for this variability, we use the modal prediction across all examples as our final prediction\nin subsequent analyses using few-shot learning, similar to how a random forest model combines\npredictions across trees to improve robustness.\n6.3 Stance detection across models and learning regimes\nFigure 3 shows the main results for the tweet and comment prediction tasks. The points are the joint\nF1 scores, representing the accuracy of a model at predicting the target and stance of tweets and the\nstances for both targets in comments, along with 95% bootstrap confidence intervals. Tables S5\nand S6 in the Supplemental Materials contain the F1 scores for all models, as well as the precision\nand recall metrics. In general, we observe significant variation in predictive performance across\nthe different models and learning regimes. Starting with the zero-shot and few-shot models, the\nlargest decoder-only models tend to perform best (Llama3-70B, GPT-3 Davinci, and GPT-4o) at\npredicting the stance and target for tweets. Indeed, zero-shot GPT-4o achieves the highest precision\nfor both stance and target prediction of any of the models tested (Table S5). The encoder-only\nDeBERTa model fine-tuned for NLI performs comparably to the smaller Llama3-8B model at\nzero-shot learning, but both tend to underperform the best conventional baseline (both the CNN and\nSVM with bag-of-words features obtained a joint F1 score of 0.48). At the bottom of the figure,\nwe find that the smaller GPT-3 Ada and the encoder-decoder FLAN-T5 XXL perform poorly for\nthese prompt-based learning regimes, with lower accuracy than random guessing. Inspection of\n29\nLarge language models for text classification\nFacebook\nTwitter\nZero−shot Few−shot 10 100 1000 2000\nZero−shot Few−shot 10 100 1352\n0.00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.75\nLearning regime\nF1\nModel\nBERT\nSBERT\nDeBERTa\nMistral−7B\nLlama3−8B\nFlan−T5 XXL\nLlama3−70B\nGPT−3 Ada\nGPT−3 Davinci\nGPT−4o\nGPT−3 Davinci\nGPT−4o\nFigure 3: Predictive performance by learning regime and model\nThe top panel shows the scores for Twitter, where the F1 score is the joint accuracy across both stance and target on the held-out test\ndata. The bottom panel shows the scores for Facebook, where the F1 score represents the joint accuracy of the stance predictions for\nboth targets on the held-out test data. The best model for each outcome is labeled in the figure. The dashed horizontal line in each\npanel represents the F1 score of the best-performing baseline model, and the dotted horizontal line represents a random baseline.\nError bars are 95% bootstrap confidence intervals.\n30\nLarge language models for text classification\nthe predictions shows that these models generally failed to return valid output, sometimes yielding\nonly one portion of the answer or irrelevant text sequences, as illustrated by the examples displayed\nin Table 4. Turning to the Facebook task, GPT-4o outperforms the other models at both zero-shot\nand few-shot learning. Like the Twitter task, GPT-3 and the Llama3 models also outperform the\nbaseline using zero-shot learning, whereas GPT-3 Ada and FLAN-T5 XXL also show worse-than-\nrandom performance. Across both tasks, adding the examples alongside the prompt results in\nimprovement for some models but a decline in others. One-shot GPT-4o performs better than all of\nthe fine-tuned models at predicting stances in Facebook comments, with a joint F1 score of 0.84,\nwhereas Llama3-8B and GPT-3 Davinci substantially decline in accuracy on the Facebook task\nwhen few-shot learning is used. As such, it is not apparent that few-shot is superior to zero-shot\nlearning using the prompt alone, at least when only one or two examples are included.\nThe models fine-tuned on 10 examples perform uniformly poorly across both tasks, far below the\nsimple baselines, and, at best, marginally better than random guessing. This further highlights the\nrelative strength of prompt-based strategies, which can perform significantly better with little or no\ntraining data. Focusing on the Twitter task, most models still fare poorly when using 100 examples.\nThe two GPT-3 models show the greatest upticks in performance, but only the larger Davinci\nmodel outperforms the conventional baseline. DeBERTa and FLAN-T5 XXL also show significant\nimprovement, but their performance is still poor. All three encoder-only models perform comparably\nto random guessing when predicting targets and stances in tweets. Once fine-tuned on the entire\ndataset (N=1352) all but one model significantly outperforms the baseline. The largest fine-tunable\nmodel, GPT-3 Davinci, has the best overall performance on the task (F1 = 0.69). However, the\nsmaller GPT-3 Ada and Llama-3 8B also perform reasonably well, along with the other fine-tunable\nmodels, except for FLAN-T5 XXL. All predictions consist of valid target-stance pairs, as illustrated\nin Table 4 since fine-tuning constrains the output of the models to valid labels.\nThe patterns are similar for the Facebook task, with GPT-3 Davinci performing best when fine-tuned\n(F1 = 0.83), equaling the zero-shot performance of GPT-4o (Table S6 shows that the model was\nmarginally worse at predicting the stances towards Trump). Ada and the encoder-only models\nall show improvement with more fine-tuning and achieve higher scores than most of the other\ndecoder-only models. The two Llama models perform similarly, while Mistral-7B and FLAN-T5\nXXL show less improvement. In general, fine-tuning on larger amounts of data tends to substantially\nimprove performance across both tasks. There are, of course, some exceptions. Llama3 70B shows\nstrong performance on the prompt-based task but scores slightly worse on both tasks than its smaller\n31\nLarge language models for text classification\nTwitter Zero-shot Fine-tuned\nText Label\nTarget, Stance FLAN-T5 XXL Llama3-70B GPT-3 Ada FLAN-T5 XXL Llama3-70B GPT-3 Ada\nHey Hillary Clinton...How’s things\nworking out on #Servergate? Bury\nenough evidence yet? #tcot\nClinton, Against , A C, A , C, A C, A C, A\nDonald Trump is a joke. His a simple-\nminded idiot, and I have nothing else to\nsay about him. He can go F**K him\nself.\nClinton, None , A T, A , F T, A T, A T, A\nWatching what Donald Trump said\nabout Mexicans was shocking! Let’s not\ngive this appalling man a platform.\nTrump, Against , A T, A , T, A T, A T, A\n@GeraldoRivera @realDonaldTrump\nDon’t apologize for the truth. People are\ntoo sensitive. #MakeAmericaGreatA-\ngain\nTrump, Favor , F T, F , C, A T, F T, F\nFacebook Zero-shot Fine-tuned\nText Label\nTrump, Clinton FLAN-T5 XXL Llama3-70B GPT-3 Ada FLAN-T5 XXL Llama3-70B GPT-3 Ada\nI’m with her...as President Obama says\n\"the most qualified individual ever to run\nfor the office.\" Our next President!\nNone, Favor F, F A, F F, F, N N, F N, F\nFairy tales are more then true not be-\ncause they tell us dragons exist,but be-\ncause they tell us dragons can be beaten.\nNone, None N, N, N A, N, N N, A N, N\nI agree Trump is Bad. But YOU are bad\ntoo. I’m NOT picking between the lesser\nof two evils. Trump WILL beat you!\nOnly Bernie CAN defeat Him. #notwith-\nher #bernieorbust\nAgainst, Against A, A A, A F, N, F N, A F, A\n\"Tweeting\" at 3 and 4 o’clock in the\nmorning is the sign of a sick and de-\nranged individual. He brags about his\nbeautiful wife, why in the hell isn’t he\nin bed with her?????????\nAgainst, None A, A, N F, N, N A, N A, N\nTable 4: Example texts and predictions from zero-shot and fine-tuned models\nThis table shows the predictions for four different examples in the test data for each task. For each example, the predictions from\nthree models are shown: FLAN-T5 XXL, Llama3-70B, and GPT-3 Ada. Predictions are shown for the zero-shot and fine-tuned\nversions of each model, where the latter was trained on the entire training dataset. The Label column shows the true label for each\ntext. The values underneath each model are the text output. Missing values indicate each model failed to generate the labels in the\ncorrect format. To conserve space, the labels have been abbreviated as follows: C=Clinton, T=Trump, F=Favor, A=Against, N=None.\nsibling when fine-tuned. Its accuracy when fine-tuned on the full dataset is comparable to its\nfew-shot performance. FLAN-T5 XXL shows consistent improvement as it is fine-tuned but still\nperforms significantly worse than other models. Finally, the comparison between models fine-tuned\non 1000 and all 2000 comments suggests evidence of diminishing returns to additional training\n32\nLarge language models for text classification\ndata, as there are marginal improvements in the F1 scores despite a doubling in the size of the\ntraining data, consistent with earlier work (Miller et al., 2019; Do et al., 2022; Wankmüller, 2022).\nIn general, the fine-tuning results indicate roughly linear increases in accuracy as the number of\ntraining examples increases on a logarithmic scale.\nThe scores for the Facebook task are generally higher than those on Twitter, particularly for the\nzero-shot and few-shot models. This is despite the fact that Facebook comment prediction is a\nmore complex task, with nine possible outcomes compared to six for the Twitter task and many\ntweets explicitly mentioning one of the two candidates, often making it easy to infer the target\n(GPT-3 Davinci achieves F1 = 0.89 for the target prediction component). We expect that these\ndiscrepancies are due to the composition of the two datasets. The Facebook comments are generally\nlonger, providing more relevant information to the classifier. More critically, a closer inspection\nof the Twitter data reveals some data quality issues. For example, the second tweet in Table 4\nis labeled as mentioning Clinton but not expressing a stance when the text clearly contains anti-\nTrump sentiment. All three fine-tuned models predicted what appears to be the correct label but\nare considered incorrect according to the labeled data. We identified other similar examples with\ninaccurate labels when inspecting the data. As such, the Twitter task scores may be an underestimate\nof the predictive accuracy of these models due to flaws in the evaluation metrics. Additionally,\nthe fact that the Twitter predictions were worse—despite the training data being available online\nand thus potentially seen when these models were pre-trained—adds further evidence that data\ncontamination does not lead to perfect memorization (Brown et al., 2020). Having established how\ndifferent models and learning regimes perform on stance detection tasks, we now turn to a more\ncomplex scenario involving stance prediction in comment-reply threads.\n6.4 Stance prediction in Facebook comment-reply threads\nEach thread consists of a post by either Donald Trump or Hillary Clinton, followed by a comment,\nand between zero and five replies. The task is to predict the stance towards the two candidates in\nthe comment and every reply in each thread. The following prompt was constructed to describe the\ntask and the format of the output:\nSystem prompt:\nYou will receive JSON inputs representing discussion threads from social media during\nthe 2016 US Presidential election. Each thread includes a post, one comment about the\n33\nLarge language models for text classification\npost, and up to five replies to the comment. Your task is to identify the stance\nexpressed towards two politicians, Donald Trump and Hillary Clinton, in the comment and\neach reply. Each text may express a stance towards one, both, or none of the politicians.\nYou will always provide a stance towards each politician separately.\nStance Options:\nSupport: Positive attitude towards the politician.\nOppose: Negative attitude towards the politician.\nNeither: No clear stance or irrelevant content.\nInstructions:\n- Identify the stance for Trump and Clinton in the comment and each reply using the\nstance options provided.\n- Always provide a stance even if the content is offensive or ambiguous.\n- There will be between zero and five replies to each comment. If there are fewer\nthan five replies, provide stances for the available replies only.\nOutput Format:\nStrictly follow this JSON format. Replace the STANCE placeholder with the actual stance.\nDo not add any other tokens:\n{ \"comment\": {\n\"stanceTrump\": \"STANCE\",\n\"stanceClinton\": \"STANCE\"},\n\"replies\": [ {\"reply_id\": 1,\n\"stanceTrump\": \"STANCE\",\n\"stanceClinton\": \"STANCE\"},\n{\"reply_id\": 2,\n\"stanceTrump\": \"STANCE\",\n\"stanceClinton\": \"STANCE\"},\n...]}\nThe prompt contains several components. It begins with a paragraph describing the task, including\nthe nature of the inputs and the general context. Next, the stance labels and short explanations are\nexplained. Additional instructions are then listed in bullet points, including an emphasis that the\nmodel should return output even for “offensive or ambiguous” content since prompt engineering\nexperiments using the evaluation dataset found that models sometimes refused to analyze texts\n34\nLarge language models for text classification\nincluding offensive language. The instructions also specify how to handle threads of different\nlengths. Finally, a truncated JSON is included to show exactly how the output should be formatted.\nWe also included instructions not to return any other tokens. This is important because instruction-\ntuned models often include other text to make the responses more conversational (e.g., “I am happy\nto help with your request, ...”). While there is some redundancy in the prompt, we erred on the\nside of providing more detail and were satisfied that the prompt steered the model to produce the\nappropriately formatted output through tests on the evaluation data. Of course, further prompt\nengineering may result in additional improvement.\n0.5\n0.6\n0.7\n0.8\nBaseline: Zero−shot text Zero−shot JSON Instruction−tuned JSON\nLearning regime\nF1\nGPT−4o Llama3−8B Llama3−70B\nFigure 4: Aggregate thread-prediction F1 scores by learning regime and model\nThe figure shows the aggregate F1 scores, representing the joint accuracy across both targets for all comments and replies in\nthe held-out test data, for GPT-4o, Llama3-8B, and Llama3-70B. The baseline model uses the text only and the same prompt as\nthe Facebook comment prediction task. The zero-shot JSON models use the full thread JSON and prompt. These models are\ninstruction-tuned on the JSON data. Error bars are 95% bootstrap confidence intervals.\nFor the zero-shot models, we input this prompt along with each thread JSON in the test data and\nstore the generated outputs. The instruction-tuning is performed by using the prompt as a system\nprompt and fine-tuning using the pairs of JSON inputs and outputs in the training data. The final\nmodel is then used to generate the outputs for each thread in the test data. As a baseline, we use\nGPT-4o and the prompt used in the previous task to classify each comment and reply individually\nbased on the text alone. Across all three approaches, we calculate joint F1 scores over all comments\nand replies in the test data. The results for all three sets of models are shown in Figure 4.\nThe baseline model performs considerably worse than in the previous stance detection task (F1 0.64\nversus 0.84). This discrepancy is likely attributable to the fact that the comments and replies in\n35\nLarge language models for text classification\neach thread were annotated in context, such that information in other texts was used to infer the\nstances but was not available to the classifier, whereas the previous dataset was annotated only using\nthe text alone. When the new prompt and full JSON inputs are used, GPT-4o shows substantial\nimprovement, demonstrating the importance of the information in the thread. The smaller Llama3-\n8B model performs poorly at the zero-shot task, with lower accuracy than the baseline, indicating\nthat it cannot sufficiently parse the prompt and thread. The larger version, Llama3-70B, on the\nother hand, achieves slightly better performance than OpenAI’s state-of-the-art model, although\nthe difference is not statistically significant. When instruction-tuned on 1200 annotated threads,\nboth Llama3 models show significant improvement. The joint F1 score from Llama3-8B jumps\nfrom 0.48 to 0.75, marginally higher than the zero-shot scores of the larger models. This shows\nthat even the smaller models can achieve competitive performance after undergoing task-specific\ninstruction-tuning. Llama3-70B achieves significantly better performance than all other models,\nwith a joint F1 score of 0.82, only slightly below the 0.84 obtained by the best comment-level model\nin the earlier Facebook task, despite the increased complexity of the task. When the scores are\nsplit by candidate, we see the instruction-tuned model performs remarkably well for the individual\npredictions, achieving F1 scores of 0.89 and 0.90 for Clinton and Trump, respectively (see Table S7\nin the Supplemental Materials). This demonstrates how the generative capacity of these models can\nbe leveraged to perform complex, structured text classification tasks with high accuracy.\nComment Reply 1 Reply 2 Reply 3 Reply 4 Reply 5\n0 1 2 3 4 5 1 2 3 4 5 2 3 4 5 3 4 5 4 5 5\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nNumber of replies\nF1\nFigure 5: F1 scores by thread position and length\nThe left-hand panel shows the held-out F1 scores for top-level comments, and each of the subsequent panels shows held-out scores\nfor sequentially numbered replies. Scores within each panel are grouped based on the number of replies in the thread. Error bars are\n95% bootstrap confidence intervals.\n36\nLarge language models for text classification\nFigure 5 shows the joint F1 score broken down by the thread position and length. Due to the\nrelatively small sample sizes in the test data (each position-length combination is observed at least\nfifty times), few differences are statistically significant, but there are some suggestive patterns. The\nfirst panel shows the performance across comments by the thread length. Comments with two\nreplies appear to be more difficult to predict than others, but the differences are relatively small,\nwith F1 scores ranging from 0.82 to 0.9. There is a clearer pattern when considering the initial\nreplies. The stance expressed in replies that do not have any follow-up replies is the easiest text to\npredict, with an F1 score of 0.94, but the stance in the initial reply becomes harder to predict as\nthe length of the thread increases. Besides the initial replies, the accuracy ranges between 0.74 and\n0.87 across all thread lengths and reply positions. For the second and third replies, the stance is\npredicted more accurately when at least one additional reply is included in the thread. These results\nthus demonstrate how some parts of the thread can be more challenging to predict than others.\nNonetheless, the instruction-tuned model can predict the stance of both comments and replies with\nmuch higher accuracy than a baseline model using text alone and achieves accuracy comparable to\nmodels fine-tuned to predict stance in comments alone.\n7 Discussion\n7.1 Findings\nLarge language models are a groundbreaking methodological innovation that can make significant\nadvances to the sociological study of texts. We examined the performance of LLMs for text\nclassification, a form of supervised machine learning used to automatically classify texts into\nschema defined by the analyst. Our results show that LLMs outperform conventional supervised\nlearning algorithms by a considerable margin and can identify stances with high accuracy. By\nexperimenting with four different learning regimes, we demonstrate the different ways in which\nthese models can be deployed.\nWhen provided with a prompt containing detailed instructions, the largest, most powerful model\ntested—OpenAI’s GPT-4o—can perform zero-shot text classification with high accuracy. Compara-\nble performance was only achieved by fine-tuning its predecessor, GPT-3 Davinci, on thousands of\nlabeled examples. This demonstrates how the most powerful LLMs can be adapted to new tasks\nwithout any task-specific training, representing a paradigm shift in the use of machine learning.\nWhile the closed, state-of-the-art models perform best, smaller, instruction-tuned open-weights\n37\nLarge language models for text classification\nalso show promising zero-shot performance. Both the 8B and 70B parameter variants of Llama3\noutperform the conventional baseline when predicting the stance of Facebook comments. Moreover,\nwe only tested a handful of prompts, so better performance may have been obtained by using newly\ndeveloped prompting techniques (Wei et al., 2023a; Kojima et al., 2024) and leveraging LLMs to\nassist with prompt engineering (Zhou et al., 2022; Khattab et al., 2023; Yuksekgonul et al., 2024).\nOur evaluations of few-shot learning are less conclusive. Depending on the model, the addition\nof one or two examples along with the prompt can sometimes improve performance but can also\nresult in significant declines. Nonetheless, we only considered minimal cases, and it is plausible\nthat utilizing the full context windows would lead to substantial improvements (Brown et al., 2020),\nparticularly for the larger models that can accommodate more examples. Overall, our experiments\nshow that predictive performance is sensitive to both the wording of the prompt and the examples\nprovided.\nRegarding fine-tuning, models trained on only ten examples tend to perform poorly, further high-\nlighting how prompting can be more effective than simply providing task-specific data. A small\nnumber of examples is thus insufficient to enable meaningful updates to the classification heads.\nOpenAI’s models improve markedly with as few as one hundred fine-tuning examples, and after\nfine-tuning on one thousand or more texts, the smaller models begin to approach the performance\nof the most powerful LLMs. Indeed, all of the billion parameter scale models were rivaled by the\nsmaller models. Moreover, the largest open-weights models showed less improvement when fine-\ntuned: FLAN-T5 XXL models performed worse than other models on both tasks, and Llama3-70B\nwas equaled or bettered by its 8B sibling. Fine-tuning smaller encoder models can be much more\nefficient than adapting larger generative models.\nThe thread-prediction task demonstrates how the generative capability of LLMs can be leveraged\nto perform much more complex classification tasks than were previously possible (Wei et al.,\n2022). The largest models, GPT-4o and Llama3-70B, performed reasonably well at generating\nthe JSON responses in a zero-shot setting, outperforming the baseline model by a considerable\nmargin. The smaller Llama3-8B model fared relatively poorly at zero-shot thread classification\nbut matched the zero-shot performance of these larger models after undergoing instruction-tuning.\nThe larger 70B parameter variant showed even more improvement, demonstrating the returns to\nscale when instruction-tuning LLMs (Chung et al., 2022). When evaluated at the comment and\nreply level, this model often achieved comparable performance to the previous comment-level task,\ndespite the significant additional complexities involved in jointly predicting the stances of multiple\n38\nLarge language models for text classification\ncomments and replies. Contrasting with previous work finding that LLMs can struggle with longer\ndocuments and conversational data (Ziems et al., 2024), our results show that structured inputs\nand instruction-tuning can enable these models to process complex sequences of information. This\nhighlights how LLMs can be leveraged to perform difficult, multifaceted kinds of text classification\ntasks with high accuracy, opening up new possibilities for computational social science.\nStance detection is a particularly useful form of text classification for sociologists and other social\nscientists since we often want to use texts to infer attitudes, opinions, and beliefs (Bestvater and\nMonroe, 2022). Our results have the most direct relevance to research in political sociology since\nour case study focuses on the stances expressed about candidates during an election campaign,\nbut these techniques can easily be extended to other domains. LLMs can be adapted for any text\nclassification task, and the capacity to perform zero- and few-shot learning makes it simple to\nexperiment with the capabilities of these models across a range of different tasks (Davidson, 2024).\nNonetheless, it is important to note that there may be substantial variation in performance depending\non factors such as the type of inputs, the complexity of the labeling scheme, and the degree to which\nthe labeling involves either expert knowledge or subjective decision-making (Ziems et al., 2024).\nWhile we are confident that LLMs will work effectively for many types of text classification and\nother NLP tasks, our findings do not imply that they will be the most effective methodology for all\ntasks.\n7.2 Recommendations\nWe use the results of our experiments to develop a set of recommendations to assist researchers\nin selecting the appropriate strategy for using LLMs for text classification tasks. Our main rec-\nommendations are summarized in Figure 6, which can help with selecting the appropriate model\narchitecture and learning regime given the parameters of a classification problem.\nThe social media posts used in our initial experiments are mostly short, although some Facebook\ncomments extend to several paragraphs. If the research necessitates analyzing long documents\nconsisting of multiple pages, such as newspaper articles, academic papers, or legal filings, then it\nwill be necessary to use models with longer context windows. The version of GPT-3 used here could\nhandle just over 2k tokens (approximately 1500 words), so it would not be sufficient for processing\nlong texts. More recent models can accommodate considerably more text. For example, GPT-4o has\na context window of 128k tokens and Google’s largest Gemini 1.5 Pro model can handle up to 2M\ntokens of input at once.25 These models can thus process long documents and handle long-range\n39\nLarge language models for text classification\nDocument/\ninstruction\nlength\nUse decoder\nmodels with\nlonger context\nwindow\nNumber of\ndocuments\nZero or\nfew-shot\nlearning using\nlarge decoder\nmodels\nNumber of\nannotated\nexamples\nZero or\nfew-shot\nlearning using\nlarge decoder\nmodels\nCompute\nbudget/resources\nFine-tune\nencoder\nmodels\nExperiment\nwith multiple\nlearning\nregimes using\nlarge decoder\nmodels\nLong\nShort\n<1000\n>1000\nLow\nHigh\nRestricted\nFlexible\nFigure 6: Selecting a suitable approach to using LLMs for text classification\ndependencies like recognizing how repeated mentions of a character throughout a novel correspond\nto the same entity. Longer context windows can also be advantageous for few-shot learning since\nmore training examples can be included along with the prompt and may be necessary to handle\ndetailed guidelines for complex instruction-tuning tasks. Moreover, longer documents can also be\nmore difficult to classify (Ziems et al., 2024), so using larger, more powerful models may also be\ndesirable. Longer documents can, of course, often be broken down into shorter segments. In some\ncases, sentences or paragraphs may be more theoretically meaningful units than an entire document\n(Barberá et al., 2020; Bonikowski et al., 2022). Ultimately, the unit of analysis should depend on\nthe research question and theoretical considerations.\nThe next consideration is the number of documents to be classified. Whereas conventional supervised\nlearning typically requires relatively large sets of annotated training data, LLMs can perform text\n40\nLarge language models for text classification\nclassification on relatively small datasets with far fewer annotated examples. If the sample is\nsmall, then zero- or few-shot learning is likely to be the preferred solution, as it will be relatively\ninexpensive to use an advanced model. Of course, one could also hand-code the data, but we\nexpect these techniques will increasingly complement qualitative approaches (e.g., to validate or\nextend hand-coding) (Ibrahim and V oyer, 2024). We recommend experimenting with different\nprompts and examples to optimize these approaches, as our experiments demonstrate that both\ncan have substantial impacts on both predictive performance and cost. Maximizing diversity by\nselecting a range of examples with a variation in both texts and labels may be a promising strategy.\nIf the number of documents is large—we use 1000 as a cut-off, but the number is arbitrary—the\nstrategy will depend on the amount of annotated data available. If only a small number of annotated\nexamples are available, we also recommend using zero or few-shot learning, which can achieve\nstrong performance with little or no annotated data. If zero/few-shot models perform poorly or are\ntoo expensive to use, then it will be necessary to annotate more data to achieve a satisfactory result.\nIn some cases, it is possible to fine-tune a state-of-the-art model with a modest amount of annotated\ndata (Do et al., 2022). Our experiments show that GPT-3 Davinci performs reasonably well on both\ntasks when fine-tuned using only 100 labeled documents. The use of LLMs to augment annotations\nto help produce larger training datasets is also a promising avenue for making the development of\ntraining data corpora faster and cheaper (Gilardi et al., 2023; Heseltine and Clemm von Hohenberg,\n2024; Ziems et al., 2024). Regardless of the approach used, it is critical to annotate a sample of\ndocuments for testing and validating the performance of any classification model.\nIf the goal is to classify a large sample of documents and more plentiful labeled data are available\nor can be created then the choices are only constrained by compute budgets and infrastructure. In\nmany settings, the goal of supervised text classification is to apply a trained model to a much larger\nunlabelled corpus. A realistic task might involve classifying a million or more comments on a social\nmedia platform. While the costs of using advanced models have decreased as the technologies have\nbecome more efficient, this can still be expensive when using commercial systems. As things stand,\nusing commercial models at scale could easily cost hundreds, even thousands, of dollars for the\nkinds of classification problems that arise in computational social science. 26 The open-weights\nalternatives can be considerably cheaper, particularly if hardware is readily available. We used two\nuniversity-owned Nvidia A100 GPUs to fine-tune Llama3-70B, which retailed for $12,995 at the\ntime of writing, but similar hardware can be rented via cloud computing providers for a few dollars\nper hour.27 As compute costs continue to decline, we expect open-weights models will be a more\n41\nLarge language models for text classification\neconomical solution for large-scale classification tasks. If advanced computing infrastructure or\nfunding to pay for APIs and cloud computing are unavailable, the smaller encoder-only models that\ncan be fine-tuned on a personal computer will be preferable for text classification tasks. Indeed, we\nexpect many researchers will prefer to sacrifice some accuracy to use a smaller model at a fraction of\nthe cost of a larger model. However, it will not be feasible to perform complex text generation tasks\nlike instruction-tuning at scale without larger decoder models and more sophisticated infrastructure.\nNotwithstanding resource constraints, we encourage experimentation with larger generative models\nto identify the optimal approach for a specific task. Our results show that prompt-based learning\nstrategies can be highly effective when using the largest models and that fine-tuning and instruction-\ntuning may lead to even better performance. For relatively simple prediction tasks, fine-tuning large\nmodels may offer some improvement compared to smaller encoder-only models or prompt-based\nstrategies. However, both prompting-based learning and fine-tuning struggle for difficult prediction\ntasks that are commonly encountered in social scientific research (Ziems et al., 2024). Instruction-\ntuning (Wei et al., 2022) combines the instructional capabilities of prompt-based approaches with\nthe customizability of fine-tuning, making it a powerful technique that could make supervised\nlearning more applicable to a range of different areas of sociological research. We expect that\ninstruction-tuning will be particularly helpful in cases where the number of labels is large, the\ncoding scheme is complex, multiple labels are needed, or the inputs are highly structured. But\ntasks can be challenging for a variety of reasons and this list is not exhaustive. While the largest\ndecoder models are best suited for this task (Chung et al., 2022), our results show that competitive\nperformance does not necessarily require the use of commercial models since open-weights models\ncan perform capably when instruction-tuned.\nDue to the complexities involved in working with LLMs, the choice of model will also depend on\nthe technical expertise of the research team. The use of the more advanced open-weights models\nrequires technical knowledge, including familiarity with Python and relevant packages like PyTorch\nand TensorFlow, command line programming, and the ability to work with hardware like HPCs and\nGPUs. The Hugging Face libraries make it relatively straightforward to interact with open-weight\nmodels with some basic Python experience, but there is a steeper learning curve concerning the\ninfrastructure. Social scientists who do not have such training may find it easier to adopt API-based\nsolutions that require minimal programming to operate. However, we expect that this gap will lessen\nas more interactive solutions and more advanced open-source models emerge.\n42\nLarge language models for text classification\nOptimizing predictive accuracy is the end goal in much of the research in computer science (Molina\nand Garip, 2019), but the development of an accurate classifier is typically only an intermediate\ngoal in social scientific research, where the objective is to use the model for measurement (Grimmer\net al., 2022). Even when we are satisfied that a model performs accurately, it is critical to recognize\nthat models cannot perfectly identify stances or any other attributes in texts. Indeed, human\nraters can be inaccurate and inconsistent at detecting stances (Joseph et al., 2021). Recent work\nhighlights how biases in the classification process can introduce biases in downstream tasks like\nregression. We recommend using bias-correction procedures when using output from classifiers\nas either dependent or independent variables. These procedures work by adjusting the estimates\nusing annotated data to correct for biases in the predictions from the classifier (Egami et al., 2023,\n2024; TeBlunthuis et al., 2024). The combination of high-performance LLM classifiers and careful\nstatistical adjustment procedures will allow social scientists to leverage machine learning classifiers\nto make valid measurements across a range of different domains.\n7.3 Limitations and challenges\nBeyond these technical and practical considerations, several limitations and challenges must be\nconsidered when using LLMs. We consider four issues and their implications for text classification:\n(1) interpretability, transparency, and reliability; (2) bias and bias mitigation; (3) reproducibility; and\n(4) privacy and data leakage. We discuss each issue in turn but point interested readers to several\nrecent works that address these topics more comprehensively in the context of social scientific\nresearch (see Bail, 2024; Ziems et al., 2024; Davidson, 2024) and related work that considers\nthese and other challenges associated with these emerging technologies (see Weidinger et al., 2022;\nBommasani et al., 2022).\n7.3.1 Interpretability, transparency, and reliability\nMachine learning models are often considered “black boxes” (Pasquale, 2015) that defy the kinds of\ninterpretation social scientists typically seek from statistical models (Mullainathan and Spiess, 2017;\nHofman et al., 2017; Davidson, 2019). Not only does the scale and complexity of LLMs make such\ninterpretation even more challenging, but the lack of transparency regarding both open-weight and\nclosed-source commercial models further complicates such efforts. We often do not know what texts\nwere used for pre-training or the additional information used for RLHF and instruction-tuning. As\nsuch, it is exceedingly difficult to explain why an input generates a given output, both with respect to\n43\nLarge language models for text classification\nthe weights in the model and the data used to train it. We thus caution researchers against using these\ntools in domains where interpretability or explainability are important (Rudin, 2019). Reliability is\nalso a related concern when using LLMs, as there is no guarantee that the outputs will be accurate.\nLLMs, particularly the larger models that have been instruction-tuned, are prone to generating\nplausible sounding but incorrect responses, often referred to as “hallucinations.” In our experiments,\nwe found that smaller models would often generate responses outside of the permitted answers\nwhen used for zero- or few-shot learning, showing how prompts cannot always constrain the outputs\nto the required labels. While researchers must be aware of these limitations, we think these issues\npose a greater threat to tasks where explanation is critical and the reliability of the outputs is harder\nto measure. Since the objective of classification tasks is prediction, interpretability and transparency\nare not necessarily required, and the quality of the predictive model should always be evaluated\nusing out-of-sample validation, so reliability issues can readily be identified and measured.\n7.3.2 Bias and bias mitigation\nBias in machine learning systems is well-documented, as systems can learn and reproduce stereo-\ntypical associations and patterns from the data they are trained on (O’Neil, 2016; Noble, 2018).\nFor example, hate speech detection models can be biased against African-Americans by dispro-\nportionately flagging their speech as hateful and abusive due to biased annotations and skewed\ntraining examples (Sap et al., 2019; Davidson et al., 2019). The fact that LLMs are trained using\nenormous swathes of the internet amplifies these risks (Bender et al., 2021; Bommasani et al., 2022;\nWeidinger et al., 2022). These problems extend to multimodal models trained on text and images,\nwhich often generate stereotypical and offensive depictions of marginalized groups (Bianchi et al.,\n2023). Bias can have serious implications for the quality of downstream tasks when LLMs are used\nas foundational models. An audit of several BERT-based classifiers showed evidence of bias against\nmembers of marginalized groups and people with stigmatized conditions or identities (Mei et al.,\n2023). Another study found that GPT-3 returned violent stereotypes casting Muslims as terrorists\nwhen completing innocuous prompts (Abid et al., 2021). Other work documents how political\nbiases learned from training data can affect the performance for hate speech and misinformation\ndetection (Feng et al., 2023).\nVarious strategies have been used to address bias and other problem behaviors in LLMs, from\nremoving offensive material from training data (Raffel et al., 2020) to instructing models to refuse\ncertain requests (OpenAI, 2023). While these efforts may make these models fairer, they can also\n44\nLarge language models for text classification\nbe detrimental to sociological inquiry, where biases and stereotypical associations are the objects of\ninterest (Argyle et al., 2023; Bail, 2024; Davidson, 2024). Researchers seeking to classify content\nthat covers sensitive topics may thus face challenges, particularly when using the most powerful\ninstruction-tuned models that have been adapted to avoid outputs related to various social issues and\ncan often refuse completely innocuous queries (Röttger et al., 2024). In our evaluations, we found\nthat zero- and few-shot classifiers sometimes refused to evaluate inputs containing contentious\npolitical claims. In some cases, it is possible to address this through prompting or instruction-tuning.\nIn our thread-prediction task, we provided specific directions in the prompt to process offensive\ncontent and did not encounter any refusals. But in other cases these moderation procedures cannot\nbe circumvented. For example, we attempted to fine-tune GPT-4o but received a message that the\noperation was blocked by the moderation system because the training data “contains too many\nexamples that violate OpenAI’s usage policies”, like due to the political content and the presence of\nsome offensive language in the texts. In this case, the company’s mitigation efforts prevented what\nwe consider to be a legitimate use of these tools.\nEfforts to better document training data and models will make it easier for researchers to understand\nthe strengths and weaknesses of particular models (Mitchell et al., 2019; Gebru et al., 2021), but\nsuch documentation gives little insight into how models generalize to specific tasks of interest to\nsocial scientists. We encourage practitioners to evaluate models to identify biases and scrutinize how\nthey could affect performance. A straightforward approach to measuring bias in text classification\nis to use templates to evaluate responses under different inputs (Dixon et al., 2018; Röttger et al.,\n2021). We also encourage researchers to compare multiple models to see how they fare on the same\ntask and to conduct audits to assess predictive differences across demographic subgroups or other\nrelevant variables to assess whether model performance varies in ways that could bias downstream\nanalyses (e.g., Buolamwini and Gebru, 2018). If mitigation efforts impede legitimate research,\nwe recommend the use of open-weights models that can be more readily modified, although\ninstruction-tuned models can still sometimes refuse legitimate requests (Röttger et al., 2024).\n7.3.3 Reproducibility\nDue to the stochastic nature of neural network models and estimation procedures, it can be difficult\nto obtain reproducible results (Liu and Salganik, 2019), and this issue is magnified when considering\nthe scale of LLMs. An identical query may yield multiple different results. Some decoder models\nhave a “temperature” parameter that can be used to minimize the stochastic variability of the results,\n45\nLarge language models for text classification\nwhich is desirable for text classification (see Section C of the Supplemental Materials for further\ndiscussion), but there is no guarantee that the results will always be identical. Moreover, commercial\nmodels can be modified or deprecated without notice, making it impossible for other researchers\nto reproduce results (Spirling, 2023). This is not merely a hypothetical. OpenAI withdrew a code-\ngeneration model used in hundreds of computer science publications from its API, later making it\navailable upon request following backlash from researchers (Davidson, 2024). This also impacts\nour analysis because the two versions of GPT-3 used in our experiments have since been replaced\nby more advanced models. If reproducibility is important, we concur with Spirling (2023) that\nresearchers use open-weight models that can be stored on disk and re-used rather than relying on\ncommercial APIs that are subject to change.\n7.3.4 Privacy and data leakage\nWhen using commercial models, the terms of service often mean that data input by users can be\nabsorbed into the training data and used to improve these models, raising concerns that sensitive data\nare retained and could theoretically be extracted from the model (Weidinger et al., 2022). In some\njurisdictions, the use of commercial models may fall afoul of data protection policies. For example,\nscholars in Europe have noted that this would amount to data sharing that violates the European\nUnion’s General Data Protection Regulation (Hacker et al., 2023). If performing classification using\nsensitive data, such as texts containing personally identifying information, we recommend using\nopen-weights models on secure servers to minimize the risk of data leakage. Relatedly, data input\ninto commercial models may also be used for additional training and instruction-tuning. This means\nthat future versions of the model could perform well simply because they have already seen the test\ndata, violating a core tenet of machine learning evaluation. In our case, it is unclear whether the data\nwe input into GPT-3 was used to train GPT-4, potentially compromising our Facebook comment test\ndata. While our Twitter results suggest this risk is low since the models do not perform noticeably\nbetter on this dataset even though it is available online, if this is a concern, it may be worthwhile\ncreating a small set of new evaluation data to verify out-of-sample performance.\n8 Conclusion\nLarge language models (LLMs) are poised to revolutionize computational sociology. The release of\nChatGPT has accelerated their adoption for text analysis and other tasks (Grossmann et al., 2023;\nBail, 2024; Ziems et al., 2024; Davidson, 2024). As the capabilities of these tools continue to\n46\nLarge language models for text classification\nadvance, their potential applications will multiply. Our analyses provide several key insights that\nwe anticipate will remain relevant despite the rapid pace of technological development. Consistent\nwith prior work (Wankmüller, 2022; Do et al., 2022; Bonikowski et al., 2022; Ziems et al., 2024),\nwe find that early generations of encoder models achieve competitive performance when fine-tuned\nto perform classification tasks. However, large decoder-only models consistently achieve the best\naccuracy with little or no additional training, demonstrating their potential to make supervised text\nclassification. The capacity to perform accurate text classification using prompts and minimal data\nmakes the technique far more accessible to social scientists. We encourage others to experiment\nwith prompt engineering and learning regimes to identify the most effective ways to use LLMs for\ndifferent applications. Contrary to previous work that finds conversational data difficult to analyze\n(Ziems et al., 2024), our thread-prediction experiments demonstrate how instruction-tuning can be\nused to perform complex, structured forms of text classification. This technique offers a powerful\nway to transform rich coding schemes and theoretical frameworks into prompts and instructions\nthat can help to adapt LLMs for social scientific inquiry. Overall, LLMs constitute an accurate,\naccessible, and adaptable toolkit for text classification, far surpassing the capabilities of conventional\nmachine learning techniques. These tools can be extended to other NLP tasks, such as question-\nanswering, named entity recognition, and summarization, while multimodal training (Radford et al.,\n2021; OpenAI, 2023) opens the door to analyses of images and audiovisual data. This article is a\nstep toward understanding how to integrate LLMs and other generative artificial intelligence into\nsociological research, offering a foundation for future work to harness their transformative potential.\nNotes\n1The original model was released by Google in late 2018, and the paper has over 100,000\ncitations on Google Scholar.\n2BERT was also trained using a next sentence prediction task, where the model was given a\npartial input sentence and had to select the correct second part of the sentence from several options\n(Devlin et al., 2019), but subsequent work found this training contributed little to the model’s\nperformance (Liu et al., 2019).\n3Formally, the model masks inputs wi+1 : wm when generating the next token yi corresponding\nto input token wi, where m is the number of input tokens.\n4Some LLMs can be used similarly to earlier word embedding models, as texts can be converted\ninto embeddings, which can be input as features in standard classifiers (Rodriguez and Spirling,\n2022; Bonikowski et al., 2022).\n5This does not preclude the analyst from conducting preliminary data cleaning and manipulation\nbefore the texts are input into the model.\n6https://huggingface.co/bigscience/bloom\n47\nLarge language models for text classification\n7Sociologists using sentiment analysis are not unaware of its limitations. Some have taken steps\nto improve measurement quality. For example, Flores (2017) weights sentiment keywords by their\nproximity to references to immigrants to help ensure that the keywords correspond to the target\nof interest. Nonetheless, a model trained to measure attitudes towards immigrants would enable a\nmore direct measure of the quantity of interest.\n8A keyword search for “stance detection” and “sentiment analysis” in American Journal of\nSociology, American Sociological Review, Social Forces, Socius, Sociological Methods & Research,\nSociological Methodology was performed in July 2024. Stance detection did not appear in any\njournals, but sentiment analysis was mentioned in twenty articles, at least once in every journal\nqueried.\n9Sentiment can also be expressed in domain-specific ways, so generic tools are not always\nreliable ways to measure sentiment (Hamilton et al., 2016).\n10Stances inferred from social media are not necessarily equivalent to those obtained from surveys,\nnor can such approaches replace traditional opinion polls. Comparisons between human annotation\nof social media posts and survey responses by the same users show variation in the correspondence\nbetween the two sources (Joseph et al., 2021) due to differences in measurement and temporal\nresolution. Thus, stance detection represents a reliable way of measuring the qualities of digital\ncommunication and other texts but is not a substitute for survey research.\n11The full dataset and further information is available here: https://alt.qcri.org/\nsemeval2016/task6/\n12The only exception to this is GPT-4o. OpenAI has a policy that it no longer trains models\nusing data passed through the API as of March 2023 (seehttps://platform.openai.com/docs/\nmodels/how-we-use-your-data ), but some of our experiments were run in January 2023. As\nsuch, it is plausible that some data used in our GPT-3 analyses could have been used to train GPT-4o.\nNonetheless, the test labels were never revealed to the model.\n13We do not distinguish between Neutral and None since our main interest is in whether an\nexpression supports or opposes a candidate. An alternative approach is to use a two-stage classifier,\nfirst predicting whether or not a target is mentioned and then predicting the stance. However, this is\nmore cumbersome and costly to implement as it requires training and evaluating multiple classifiers.\n14It is common for Facebook users to “tag” their friends in their comments and replies, meaning\nthat the user’s name is included in the text. The corpus was anonymized by replacing all names with\npseudonyms, ensuring that the names are constant within the thread (e.g., User A writes a comment,\nUser B replies, then User A responds). Regular expressions were then used to identify any tags in\nthe text and replace these with the corresponding pseudonyms.\n15There is no guarantee that a language model will return output in the required format. However,\nsince completing this analysis, OpenAI has released a format called Structured Outputs that can help\nto ensure that the output is valid JSON, see https://platform.openai.com/docs/guides/\nstructured-outputs.\n16The models listed in monospace font correspond to either open-weights models available on\nHugging Face, which can be downloaded athttps://huggingface.co/models, or closed models\nin OpenAI’s API.\n17These models have been deprecated since the completion of our analyses. Ada has been\nreplaced by babbage-002 and Davinci by gpt-3.5-turbo-instruct.\n18This number has been referenced in leaked materials and is cited in Ziems et al. (2024).\n19Fine-tuning GPT-4o was enabled in OpenAI’s API in July 2024, but the new moderation\npolicies blocked us from fine-tuning models on either dataset since the training data violated the\nusage policies. We contacted OpenAI about this problem but were unable to get a resolution. Many\n48\nLarge language models for text classification\nothers have reported facing similar issues in a discussion thread we initiated: https://community.\nopenai.com/t/fine-tuning-blocked-by-moderation-system/878133 .\n20At the time of our experiments with GPT-3 in early 2023, for every 1000 tokens of input—\nequivalent to approximately 750 words—Ada cost $0.0004 and Davinci cost $0.02, but the fees\nhave since decreased due to improved efficiency. The largest model, GPT-4o, cost $0.0025 per 1000\ntokens of input at the time of writing (July 2024) when data were input into the API in batches.\n21We truncate any long Facebook comments to avoid excessive token consumption for GPT-3\nAda and GPT-3 Davinci. The mean comment length is 272 characters with a standard deviation of\n604. We thus set the maximum allowed length to 876 characters (272 + 604), resulting in a total of\n5 examples out of 100 being truncated.\n22The endpoint we used for original experiments has been deprecated and replaced with a\nnew API that is capable of chat-styled instruction-tuning. See https://openai.com/index/\ngpt-3-5-turbo-fine-tuning-and-api-updates/ .\n23We do not perform cross-validation on the training data because it is computationally, and\nsometimes financially, costly when fine-tuning LLMs since k-fold cross-validation requires k\nseparate fine-tuning and prediction runs.\n24In practice, we never observe comments that favor both candidates, so there are only eight\ncombinations in the training and test data.\n25https://deepmind.google/technologies/gemini/pro/\n26Assuming a corpus of 1M documents, each consisting of 100 words, GPT-3 consumes approxi-\nmately 1000 tokens for every 7.5 documents (∼100 tokens for 75 words), putting the total token\nconsumption at approximately 133.3M. Based on OpenAI’s cheapest GPT-4o pricing ($2.5 per 1M\ntokens), the text alone would cost around $333.25 to input. If used for zero- or few-shot learning,\nthe prompts and examples would also add a considerable number of additional tokens. Outputs\nare charged at a higher rate ($7.5 per 1M tokens). In the simple Twitter example above, the output\nwith the format TARGET, STANCE uses five tokens. Thus, a million generations will consume 5M\ntokens, costing an additional $37.50 and bringing the total cost to $370.75. Text generation tasks\ncan consume substantially more tokens. For example, in thread-prediction analysis, it takes up to\n178 tokens to generate the output for the longest threads, consuming 36 times as many tokens as the\nsimple prediction task. Given that researchers may experiment with multiple models and learning\nregimes, including analyses for sensitivity to prompts and examples, the true training cost may end\nup being multiples of the cost to fine-tune and deploy the final model.\n27For example, Lambda Labs charges $2.58 plus tax for the usage of two A100 GPUs (July 2024),\nsee https://lambdalabs.com/service/gpu-cloud.\nAcknowledgments. We thank Daniel Karell, Laura Nelson, Eunkyung Song, and Josh Zhang for\ntheir comments and suggestions on an earlier version of the manuscript, as well as the anonymous\nreviewers and the editor Brandon Stewart for their extremely helpful feedback. This work was\npresented in the Text as Data session at the 2023 American Sociological Association Annual\nMeeting in Philadelphia and the AI and Social Sciences Seminar at the Institut Polytechnique de\nParis. We thank Marina Rivera Ramos for research assistance, the Office of Advanced Research\nComputing at Rutgers University for providing access to the Amarel high-performance computing\ncluster, and OpenAI for GPT-4o credits through the Researcher Access Program.\nAuthor’s Note. Data and code to replicate our analyses are available online at https://github.\ncom/yjin-chae/LLMs-for-text-classification .\n49\nLarge language models for text classification\nReferences\nAbid, A., Farooqi, M., and Zou, J. (2021). Persistent Anti-Muslim Bias in Large Language Models.\nIn Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 298–306,\nVirtual Event USA. ACM.\nAldayel, A. and Magdy, W. (2021). Stance detection on social media: State of the art and trends.\nInformation Processing & Management, 58(4):102597.\nAllaway, E. and McKeown, K. (2020). Zero-Shot Stance Detection: A Dataset and Model us-\ning Generalized Topic Representations. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 8913–8931, Online. Association for\nComputational Linguistics.\nArgyle, L. P., Busby, E. C., Fulda, N., Rytting, C., and Wingate, D. (2023). Out of One, Many:\nUsing Language Models to Simulate Human Samples. Political Analysis.\nAugenstein, I., Rocktäschel, T., Vlachos, A., and Bontcheva, K. (2016). Stance Detection with\nBidirectional Conditional Encoding. InProceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 876–885, Austin, Texas. Association for Computational\nLinguistics.\nBackstrom, L., Kleinberg, J., Lee, L., and Danescu-Niculescu-Mizil, C. (2013). Characterizing and\ncurating conversation threads: expansion, focus, volume, re-entry. In Proceedings of the sixth\nACM international conference on Web search and data mining, pages 13–22. ACM.\nBahl, L. R., Jelinek, F., and Mercer, R. L. (1983). A Maximum Likelihood Approach to Continuous\nSpeech Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-\n5(2):179–190.\nBail, C. A. (2024). Can Generative AI improve social science? Proceedings of the National\nAcademy of Sciences, 121(21):e2314021121. Publisher: Proceedings of the National Academy of\nSciences.\nBarberá, P., Boydstun, A. E., Linn, S., McMahon, R., and Nagler, J. (2020). Automated Text\nClassification of News Articles: A Practical Guide. Political Analysis, pages 1–24.\nBender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the Dangers of\nStochastic Parrots: Can Language Models Be Too Big? In FAccT ’21: Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, pages 610–623, Canada. ACM.\nBengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A Neural Probabilistic Language\nModel. Journal of Machine Learning Research, 3:1137–1155.\nBerry, G. and Taylor, S. J. (2017). Discussion Quality Diffuses in the Digital Public Square.\nIn Proceedings of the 26th International Conference on World Wide Web - WWW ’17 , pages\n1371–1380, Perth, Australia. ACM Press.\nBestvater, S. E. and Monroe, B. L. (2022). Sentiment is Not Stance: Target-Aware Opinion\nClassification for Political Text Analysis. Political Analysis, pages 1–22.\nBianchi, F., Kalluri, P., Durmus, E., Ladhak, F., Cheng, M., Nozza, D., Hashimoto, T., Jurafsky,\nD., Zou, J., and Caliskan, A. (2023). Easily Accessible Text-to-Image Generation Amplifies\nDemographic Stereotypes at Large Scale. In Proceedings of the 2023 ACM Conference on\nFairness, Accountability, and Transparency, FAccT ’23, pages 1493–1504, New York, NY , USA.\nAssociation for Computing Machinery.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S.,\nBohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji,\nN., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E.,\nErmon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K.,\nGoodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong,\nJ., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G.,\nKhani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F.,\n50\nLarge language models for text classification\nLee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D.,\nMirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman,\nB., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park,\nJ. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani,\nY ., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K.,\nTamkin, A., Taori, R., Thomas, A. W., Tramèr, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu,\nY ., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y .,\nZheng, L., Zhou, K., and Liang, P. (2022). On the Opportunities and Risks of Foundation Models.\narXiv:2108.07258 [cs].\nBonikowski, B., Luo, Y ., and Stuhler, O. (2022). Politics as Usual? Measuring Populism, National-\nism, and Authoritarianism in U.S. Presidential Campaigns (1952–2020) with Neural Language\nModels. Sociological Methods & Research, 51(4):1721–1787.\nBrown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V . J., Jelinek, F., Lafferty, J. D., Mercer,\nR. L., and Roossin, P. S. (1990). A Statistical Approach to Machine Translation. Computational\nLinguistics, 16(2):79–85.\nBrown, P. F., deSouza, P. V ., Mercer, R. L., Della Pietra, V . J., and Lai, J. C. (1992). Class-Based\nn-gram Models of Natural Language. Computational Linguistics, 18(4):14.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R.,\nRamesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,\nS., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.\n(2020). Language Models are Few-Shot Learners. In Advances in Neural Information Processing\nSystems, volume 33, pages 1877–1901.\nBuolamwini, J. and Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in\nCommercial Gender Classification. In Proceedings of Machine Learning Research, volume 81,\npages 1–15.\nBurnham, M. (2024). Stance detection: a practical guide to classifying political beliefs in text.\nPolitical Science Research and Methods, pages 1–18.\nCaruana, R. (1997). Multitask Learning. Machine Learning, 28(1):41–75.\nCho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio,\nY . (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine\nTranslation. In Moschitti, A., Pang, B., and Daelemans, W., editors, Proceedings of the 2014\nConference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734,\nDoha, Qatar. Association for Computational Linguistics.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, Y ., Wang, X., Dehghani, M.,\nBrahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros,\nA., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V ., Huang, Y ., Dai,\nA., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V ., and Wei, J.\n(2022). Scaling instruction-finetuned language models.\nDai, A. M. and Le, Q. V . (2015). Semi-supervised Sequence Learning. In Advances in Neural\nInformation Processing Systems, volume 28. Curran Associates, Inc.\nDanescu-Niculescu-Mizil, C., West, R., Jurafsky, D., Leskovec, J., and Potts, C. (2013). No country\nfor old members: User lifecycle and linguistic change in online communities. In Proceedings of\nthe 22nd international conference on World Wide Web, pages 307–318. ACM.\nDavidson, T. (2019). Black-Box Models and Sociological Explanations: Predicting High School\nGrade Point Average Using Neural Networks. Socius: Sociological Research for a Dynamic\nWorld, 5:237802311881770.\nDavidson, T. (2024). Start Generating: Harnessing Generative Artificial Intelligence for Sociological\nResearch. Socius: Sociological Research for a Dynamic World, 10. Publisher: SAGE Publications.\n51\nLarge language models for text classification\nDavidson, T., Bhattacharya, D., and Weber, I. (2019). Racial Bias in Hate Speech and Abusive\nLanguage Detection Datasets. In Proceedings of the Third Workshop on Abusive Language Online,\npages 25–35, Florence, Italy. ACL.\nDavidson, T., Warmsley, D., Macy, M., and Weber, I. (2017). Automated Hate Speech Detection\nand the Problem of Offensive Language. In Proceedings of the 11th International Conference on\nWeb and Social Media (ICWSM), pages 512–515.\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023). Qlora: Efficient finetuning of\nquantized llms.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT 2019,\npages 4171–4186. ACL.\nDixon, L., Li, J., Sorensen, J., Thain, N., and Vasserman, L. (2018). Measuring and Mitigating\nUnintended Bias in Text Classification. In Proceedings of the 2018 Conference on AI, Ethics, and\nSociety, pages 67–73. ACM Press.\nDo, S., Ollion, , and Shen, R. (2022). The Augmented Social Scientist: Using Sequential Transfer\nLearning to Annotate Millions of Texts with Human-Level Accuracy. Sociological Methods &\nResearch, page 00491241221134526. Publisher: SAGE Publications Inc.\nEfron, B. and Tibshirani, R. (1986). Bootstrap methods for standard errors, confidence intervals,\nand other measures of statistical accuracy. Statistical Science, 1(1):54–75.\nEgami, N., Hinck, M., Stewart, B., and Wei, H. (2023). Using Imperfect Surrogates for Downstream\nInference: Design-based Supervised Learning for Social Science Applications of Large Language\nModels. Advances in Neural Information Processing Systems, 36:68589–68601.\nEgami, N., Hinck, M., Stewart, B. M., and Wei, H. (2024). Using Large Language Model Annota-\ntions for the Social Sciences: A General Framework of Using Predicted Variables in Downstream\nAnalyses.\nEvans, J. A. and Aceves, P. (2016). Machine Translation: Mining Text for Social Theory. Annual\nReview of Sociology, 42(1):21–50.\nFelmlee, D., DellaPosta, D., Rodis, P. d. C. I., and Matthews, S. A. (2020). Can Social Media\nAnti-abuse Policies Work? A Quasi-experimental Study of Online Sexist and Racist Slurs. Socius:\nSociological Research for a Dynamic World, 6:237802312094871.\nFeng, S., Park, C. Y ., Liu, Y ., and Tsvetkov, Y . (2023). From Pretraining Data to Language Models\nto Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics, volume\nV olume 1: Long Papers, pages 11737–11762.\nFlores, R. D. (2017). Do anti-immigrant laws shape public sentiment? A study of Arizona’s SB\n1070 using Twitter data. American Journal of Sociology, 123(2):333–384.\nGebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., and Crawford, K.\n(2021). Datasheets for datasets. Communications of the ACM, 64(12):86–92.\nGilardi, F., Alizadeh, M., and Kubli, M. (2023). ChatGPT outperforms crowd workers for text-\nannotation tasks. Proceedings of the National Academy of Sciences , 120(30):e2305016120.\nPublisher: Proceedings of the National Academy of Sciences.\nGray, R. and Neuhoff, D. (1998). Quantization. IEEE Transactions on Information Theory ,\n44(6):2325–2383. Conference Name: IEEE Transactions on Information Theory.\nGrimmer, J., Roberts, M. E., and Stewart, B. M. (2022). Text as data: A new framework for machine\nlearning and the social sciences. Princeton University Press.\nGrossmann, I., Feinberg, M., Parker, D. C., Christakis, N. A., Tetlock, P. E., and Cunningham, W. A.\n(2023). AI and the transformation of social science research. Science, 380(6650):1108–1109.\nPublisher: American Association for the Advancement of Science.\n52\nLarge language models for text classification\nHacker, P., Engel, A., and Mauer, M. (2023). Regulating ChatGPT and other Large Generative\nAI Models. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and\nTransparency, FAccT ’23, pages 1112–1123, New York, NY , USA. Association for Computing\nMachinery.\nHamilton, W. L., Clark, K., Leskovec, J., and Jurafsky, D. (2016). Inducing Domain-Specific\nSentiment Lexicons from Unlabeled Corpora. In Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages 595–605. Association for Computational\nLinguistics.\nHanna, A. (2013). Computer-aided content analysis of digitally enabled movements. Mobilization:\nAn International Quarterly, 18(4):367–388.\nHeseltine, M. and Clemm von Hohenberg, B. (2024). Large language models as a substitute for\nhuman experts in annotating political text. Research & Politics, 11(1):20531680241236239.\nPublisher: SAGE Publications Ltd.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D.,\nHendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche,\nG., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J., and Sifre,\nL. (2022). An empirical analysis of compute-optimal large language model training. In Koyejo,\nS., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural\nInformation Processing Systems, volume 35, pages 30016–30030. Curran Associates, Inc.\nHofman, J. M., Sharma, A., and Watts, D. J. (2017). Prediction and explanation in social systems.\nScience, 355(6324):486–488.\nHoward, J. and Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. In\nGurevych, I. and Miyao, Y ., editors,Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nHu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. (2021).\nLoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs].\nIbrahim, E. I. and V oyer, A. (2024). The Augmented Qualitative Researcher: Using Generative AI\nin Qualitative Text Analysis.\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D.\n(2018). Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only\nInference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 2704–2713.\nJensen, J. L., Karell, D., Tanigawa-Lau, C., Habash, N., Oudah, M., and Fairus Shofia Fani,\nD. (2022). Language Models in Sociological Research: An Application to Classifying Large\nAdministrative Data and Measuring Religiosity. Sociological Methodology, 52(1):30–52.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand,\nF., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L.,\nLavril, T., Wang, T., Lacroix, T., and Sayed, W. E. (2023). Mistral 7b.\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas,\nD. d. l., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L.,\nLachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril,\nT., Wang, T., Lacroix, T., and Sayed, W. E. (2024). Mixtral of Experts. arXiv:2401.04088 [cs].\nJoseph, K., Shugars, S., Gallagher, R., Green, J., Quintana Mathé, A., An, Z., and Lazer, D. (2021).\n(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npages 312–324, Online and Punta Cana, Dominican Republic. Association for Computational\nLinguistics.\n53\nLarge language models for text classification\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A.,\nWu, J., and Amodei, D. (2020). Scaling Laws for Neural Language Models. arXiv:2001.08361\n[cs, stat].\nKhattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., Vardhamanan, S., Haq, S.,\nSharma, A., Joshi, T. T., Moazam, H., Miller, H., Zaharia, M., and Potts, C. (2023). DSPy:\nCompiling Declarative Language Model Calls into Self-Improving Pipelines. arXiv:2310.03714\n[cs].\nKim, Y . (2014). Convolutional Neural Networks for Sentence Classification. In Proceedings of\nthe 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n1746–1751, Doha, Qatar. Association for Computational Linguistics.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa, Y . (2024). Large language models are\nzero-shot reasoners. In Proceedings of the 36th International Conference on Neural Information\nProcessing Systems, NIPS ’22, pages 22199–22213, Red Hook, NY , USA. Curran Associates Inc.\nKozlowski, A. C., Taddy, M., and Evans, J. A. (2019). The Geometry of Culture: Analyz-\ning the Meanings of Class through Word Embeddings. American Sociological Review, page\n000312241987713.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolu-\ntional neural networks. In Advances in neural information processing systems, pages 1097–1105.\nKüçük, D. and Can, F. (2020). Stance Detection: A Survey. ACM Comput. Surv., 53(1):12:1–12:37.\nLaurer, M., Atteveldt, W. v., Casas, A., and Welbers, K. (2024). Less Annotating, More Classifying:\nAddressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning\nand BERT-NLI. Political Analysis, 32(1):84–100. Publisher: Cambridge University Press.\nLe Mens, G., Kovács, B., Hannan, M., and Pros, G. (2023). Using Machine Learning to Uncover\nthe Semantics of Concepts: How Well Do Typicality Measures Extracted from a BERT Text\nClassifier Match Human Judgments of Genre Typicality? Sociological Science, 10:82–117.\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V ., and\nZettlemoyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension.\nLi, H. (2022). Language models: past, present, and future. Communications of the ACM, 65(7):56–\n63.\nLiu, D. M. and Salganik, M. J. (2019). Successes and Struggles with Computational Reproducibility:\nLessons from the Fragile Families Challenge. Socius: Sociological Research for a Dynamic\nWorld, 5:1–21.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer,\nL., and Stoyanov, V . (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach.\narXiv:1907.11692 [cs]. arXiv: 1907.11692.\nMangrulkar, S., Gugger, S., Debut, L., Belkada, Y ., Paul, S., and Bossan, B. (2022). Peft: State-of-\nthe-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft.\nManning, C. D. (2022). Human Language Understanding & Reasoning. Daedalus, 151(2):127–138.\nMartin, J. H. and Jurafsky, D. (2024). Speech and Language Processing: An Introduction to Natural\nLanguage Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, Upper\nSaddle River, NJ, 3rd edition.\nMei, K., Fereidooni, S., and Caliskan, A. (2023). Bias Against 93 Stigmatized Groups in Masked\nLanguage Models and Downstream Sentiment Classification Tasks. In Proceedings of the 2023\nACM Conference on Fairness, Accountability, and Transparency, FAccT ’23, pages 1699–1710,\nNew York, NY , USA. Association for Computing Machinery.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of word representa-\ntions in vector space. arXiv preprint arXiv:1301.3781.\n54\nLarge language models for text classification\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013b). Distributed represen-\ntations of words and phrases and their compositionality. In Advances in Neural Information\nProcessing Systems, pages 3111–3119.\nMiller, B., Linder, F., and Mebane, Jr., W. R. (2019). Active Learning Approaches for Labeling Text:\nReview and Assessment of the Performance of Active Learning Approaches. Political Analysis.\nMitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D.,\nand Gebru, T. (2019). Model Cards for Model Reporting. In Proceedings of the Conference on\nFairness, Accountability, and Transparency, pages 220–229. arXiv:1810.03993 [cs].\nMohammad, S., Kiritchenko, S., Sobhani, P., Zhu, X., and Cherry, C. (2016). SemEval-2016 Task\n6: Detecting Stance in Tweets. In Proceedings of the 10th International Workshop on Semantic\nEvaluation (SemEval-2016), pages 31–41, San Diego, California. Association for Computational\nLinguistics.\nMohammad, S. M., Sobhani, P., and Kiritchenko, S. (2017). Stance and Sentiment in Tweets. ACM\nTrans. Internet Technol., 17(3):26:1–26:23.\nMolina, M. and Garip, F. (2019). Machine Learning for Sociology. Annual Review of Sociology,\n45:27–45.\nMullainathan, S. and Spiess, J. (2017). Machine Learning: An Applied Econometric Approach.\nJournal of Economic Perspectives, 31(2):87–106.\nMurakami, A. and Raymond, R. (2010). Support or Oppose? Classifying Positions in Online Debates\nfrom Reply Activities and Opinion Expressions. In Huang, C.-R. and Jurafsky, D., editors, Coling\n2010: Posters, pages 869–875, Beijing, China. Coling 2010 Organizing Committee.\nMéndez, J. R., Iglesias, E. L., Fdez-Riverola, F., Díaz, F., and Corchado, J. M. (2006). Tokenising,\nStemming and Stopword Removal on Anti-spam Filtering Domain. In Marín, R., Onaindía, E.,\nBugarín, A., and Santos, J., editors, Current Topics in Artificial Intelligence, Lecture Notes in\nComputer Science, pages 449–458, Berlin, Heidelberg. Springer.\nNaab, T. K., Ruess, H.-S., and Küchler, C. (2023). The influence of the deliberative quality of\nuser comments on the number and quality of their reply comments. New Media & Society, page\n14614448231172168. Publisher: SAGE Publications.\nNelson, L. K. (2017). Computational Grounded Theory: A Methodological Framework.Sociological\nMethods & Research, page 004912411772970.\nNelson, L. K., Brewer, A., Mueller, A. S., O’Connor, D. M., Dayal, A., and Arora, V . M. (2023).\nTaking the Time: The Implications of Workplace Assessment for Organizational Gender Inequality.\nAmerican Sociological Review, 88(4):627–655.\nNelson, L. K., Burk, D., Knudsen, M., and McCall, L. (2018). The Future of Coding: A Comparison\nof Hand-Coding and Three Types of Computer-Assisted Text Analysis Methods. Sociological\nMethods & Research, page 004912411876911.\nNoble, S. U. (2018). Algorithms of Oppression: How Search Engines Reinforce Racism . NYU\nPress, New York, NY .\nO’Neil, C. (2016). Weapons of math destruction: How big data increases inequality and threatens\ndemocracy. Broadway Books.\nOpenAI (2023). GPT-4 Technical Report. Technical report.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,\nS., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,\nWelinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow\ninstructions with human feedback. arXiv:2203.02155 [cs].\nPalmer, A., Smith, N. A., and Spirling, A. (2024). Using proprietary language models in academic\nresearch requires explicit justification. Nature Computational Science , 4(1):2–3. Publisher:\nNature Publishing Group.\n55\nLarge language models for text classification\nPang, B. and Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends in\nInformation Retrieval, 2(1-2):1–135.\nPasquale, F. (2015). The black box society. Harvard University Press, Cambridge, MA.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,\nN., and Antiga, L. (2019). Pytorch: An imperative style, high-performance deep learning library.\nAdvances in neural information processing systems, 32.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., Blondel, M.,\nPrettenhofer, P., Weiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,\nM., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of\nMachine Learning Research, 12:2825–2830.\nPennington, J., Socher, R., and Manning, C. (2014). Glove: Global Vectors for Word Representation.\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,\nMishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning Transferable Visual Models\nFrom Natural Language Supervision. OpenAI, page 47.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving Language Under-\nstanding by Generative Pre-Training.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models\nare Unsupervised Multitask Learners. Technical report, OpenAI.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu,\nP. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\narXiv:1910.10683 [cs, stat].\nRathje, S., Mirea, D.-M., Sucholutsky, I., Marjieh, R., Robertson, C., and Bavel, J. J. V . (2023).\nGPT is an effective tool for multilingual psychological text analysis. Publisher: OSF.\nReimers, N. and Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-\nnetworks.\nRen, C. and Bloemraad, I. (2022). New Methods and the Study of Vulnerable Groups: Using\nMachine Learning to Identify Immigrant-Oriented Nonprofit Organizations. Socius: Sociological\nResearch for a Dynamic World, 8:237802312210769.\nRodriguez, P. L. and Spirling, A. (2022). Word Embeddings: What Works, What Doesn’t, and How\nto Tell the Difference for Applied Research. The Journal of Politics, 84(1):101–115.\nRudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and\nuse interpretable models instead. Nature Machine Intelligence, 1(5):206–215.\nRöttger, P., Kirk, H., Vidgen, B., Attanasio, G., Bianchi, F., and Hovy, D. (2024). XSTest: A Test\nSuite for Identifying Exaggerated Safety Behaviours in Large Language Models. In Duh, K.,\nGomez, H., and Bethard, S., editors, Proceedings of the 2024 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 5377–5400, Mexico City, Mexico. Association for Computational\nLinguistics.\nRöttger, P., Vidgen, B., Nguyen, D., Waseem, Z., Margetts, H., and Pierrehumbert, J. (2021).\nHateCheck: Functional Tests for Hate Speech Detection Models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing, pages 41–58. ACL.\nSap, M., Card, D., Gabriel, S., Choi, Y ., and Smith, N. A. (2019). The Risk of Racial Bias\nin Hate Speech Detection. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1668–1678. ACL.\n56\nLarge language models for text classification\nSen, I., Flöck, F., and Wagner, C. (2020). On the Reliability and Validity of Detecting Approval of\nPolitical Actors in Tweets. In Proceedings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 1413–1426, Online. Association for Computational\nLinguistics.\nShannon, C. E. (1948). A mathematical theory of communication. The Bell System Technical\nJournal, 27(3):379–423. Publisher: Nokia Bell Labs.\nShor, E., Van De Rijt, A., Miltsov, A., Kulkarni, V ., and Skiena, S. (2015). A Paper Ceiling:\nExplaining the Persistent Underrepresentation of Women in Printed News.American Sociological\nReview, 80(5):960–984.\nShugars, S. and Beauchamp, N. (2019). Why Keep Arguing? Predicting Engagement in Political\nConversations Online. Sage Open, 9(1):2158244019828850. Publisher: SAGE Publications.\nSmith, N. A. (2020). Contextual word representations: putting words into computers. Communica-\ntions of the ACM, 63(6):66–74.\nSobhani, P., Inkpen, D., and Matwin, S. (2015). From argumentation mining to stance classification.\nIn Proceedings of the 2nd Workshop on Argumentation Mining, pages 67–77.\nSobhani, P., Inkpen, D., and Zhu, X. (2017). A Dataset for Multi-Target Stance Detection. In\nProceedings of the 15th Conference of the European Chapter of the Association for Computa-\ntional Linguistics: Volume 2, Short Papers , pages 551–557, Valencia, Spain. Association for\nComputational Linguistics.\nSomasundaran, S. and Wiebe, J. (2010). Recognizing Stances in Ideological On-Line Debates. In\nProceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and\nGeneration of Emotion in Text, pages 116–124.\nSpirling, A. (2023). Why open-source generative AI models are an ethical way forward for science.\nNature, 616(7957):413–413.\nSridhar, D., Foulds, J., Huang, B., Getoor, L., and Walker, M. (2015). Joint Models of Disagreement\nand Stance in Online Debate. In Proceedings of the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 116–125, Beijing, China. Association for Computational\nLinguistics.\nStoltz, D. S. and Taylor, M. A. (2021). Cultural cartography with word embeddings. Poetics, page\n101567.\nStrubell, E., Ganesh, A., and McCallum, A. (2019). Energy and Policy Considerations for Deep\nLearning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Association for Computational Linguistics.\nSun, C., Qiu, X., Xu, Y ., and Huang, X. (2020). How to fine-tune bert for text classification?\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B.\n(2023). Alpaca: A strong, replicable instruction-following model. Stanford Center for Research\non Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7.\nTeBlunthuis, N., Hase, V ., and Chan, C.-H. (2024). Misclassification in Automated Content Analysis\nCauses Bias in Regression. Can We Fix It? Yes We Can! Communication Methods and Measures,\n0(0):1–22. Publisher: Routledge _eprint: https://doi.org/10.1080/19312458.2023.2293713.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,\nN., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. (2023). LLaMA:\nOpen and Efficient Foundation Language Models. arXiv:2302.13971 [cs].\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, , and\nPolosukhin, I. (2017). Attention is All you Need. In NIPS, page 11, Long Beach, CA, USA.\nV oyer, A., Kline, Z. D., Danton, M., and V olkova, T. (2022). From Strange to Normal: Compu-\ntational Approaches to Examining Immigrant Incorporation Through Shifts in the Mainstream.\nSociological Methods & Research, 51(4):1540–1579. Publisher: SAGE Publications Inc.\n57\nLarge language models for text classification\nWalker, M., Anand, P., Abbott, R., and Grant, R. (2012). Stance Classification using Dialogic\nProperties of Persuasion. In Fosler-Lussier, E., Riloff, E., and Bangalore, S., editors, Proceedings\nof the 2012 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 592–596, Montréal, Canada. Association for\nComputational Linguistics.\nWang, Y ., Kordi, Y ., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. (2023).\nSelf-Instruct: Aligning Language Models with Self-Generated Instructions. In Rogers, A., Boyd-\nGraber, J., and Okazaki, N., editors, Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 13484–13508, Toronto, Canada.\nAssociation for Computational Linguistics.\nWankmüller, S. (2022). Introduction to Neural Transfer Learning With Transformers for Social\nScience Text Analysis. Sociological Methods & Research, page 00491241221134527. Publisher:\nSAGE Publications Inc.\nWei, J., Bosma, M., Zhao, V ., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V .\n(2022). Finetuned Language Models are Zero-Shot Learners. In International Conference on\nLearning Representations.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and\nZhou, D. (2023a). Chain-of-Thought Prompting Elicits Reasoning in Large Language Mod-\nels. arXiv:2201.11903 [cs].\nWei, J., Wei, J., Tay, Y ., Tran, D., Webson, A., Lu, Y ., Chen, X., Liu, H., Huang, D., Zhou, D., and\nMa, T. (2023b). Larger language models do in-context learning differently. arXiv:2303.03846\n[cs].\nWeidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.-S., Mellor, J., Glaese, A., Cheng, M.,\nBalle, B., Kasirzadeh, A., Biles, C., Brown, S., Kenton, Z., Hawkins, W., Stepleton, T., Birhane,\nA., Hendricks, L. A., Rimell, L., Isaac, W., Haas, J., Legassick, S., Irving, G., and Gabriel, I.\n(2022). Taxonomy of Risks posed by Language Models. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency, pages 214–229, Seoul Republic of Korea. ACM.\nWidmann, T. and Wich, M. (2022). Creating and Comparing Dictionary, Word Embedding, and\nTransformer-Based Models to Measure Discrete Emotions in German Political Text. Political\nAnalysis, pages 1–16.\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf,\nR., and Funtowicz, M. (2020). Transformers: State-of-the-art natural language processing. In\nProceedings of the 2020 conference on empirical methods in natural language processing: system\ndemonstrations, pages 38–45.\nYin, W., Hay, J., and Roth, D. (2019). Benchmarking Zero-shot Text Classification: Datasets,\nEvaluation and Entailment Approach. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pages 3912–3921, Hong Kong, China. Association for\nComputational Linguistics.\nYosinski, J., Clune, J., Bengio, Y ., and Lipson, H. (2014). How transferable are features in deep\nneural networks? In Proceedings of the 27th International Conference on Neural Information\nProcessing Systems - Volume 2, NIPS’14, pages 3320–3328, Cambridge, MA, USA. MIT Press.\nYuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Huang, Z., Guestrin, C., and Zou, J. (2024).\nTextGrad: Automatic \"Differentiation\" via Text. arXiv:2406.07496 [cs].\nZaller, J. R. (1992). The nature and origins of mass opinion. Cambridge University Press.\nZhang, H. and Pan, J. (2019). CASM: A Deep-Learning Approach for Identifying Collective Action\nEvents with Text and Image Data from Social Media. Sociological Methodology, 49(1):1–57.\nZhang, J., Chang, J., Danescu-Niculescu-Mizil, C., Dixon, L., Hua, Y ., Taraborelli, D., and\nThain, N. (2018). Conversations Gone Awry: Detecting Early Signs of Conversational Failure.\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n58\nLarge language models for text classification\n(Volume 1: Long Papers), pages 1350–1361, Melbourne, Australia. Association for Computational\nLinguistics.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. (2021). Calibrate before use: Improving\nfew-shot performance of language models. In Meila, M. and Zhang, T., editors, Proceedings of\nthe 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine\nLearning Research, pages 12697–12706. PMLR.\nZhou, Y ., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2022). Large\nLanguage Models are Human-Level Prompt Engineers. In The Eleventh International Conference\non Learning Representations.\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and\nIrving, G. (2020). Fine-Tuning Language Models from Human Preferences.\nZiems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., and Yang, D. (2024). Can Large Language\nModels Transform Computational Social Science? Computational Linguistics, pages 1–55.\nZubiaga, A., Kochkina, E., Liakata, M., Procter, R., and Lukasik, M. (2016). Stance Classification\nin Rumours as a Sequential Task Exploiting the Tree Structure of Social Media Conversations.\nIn Matsumoto, Y . and Prasad, R., editors,Proceedings of COLING 2016, the 26th International\nConference on Computational Linguistics: Technical Papers, pages 2438–2448, Osaka, Japan.\nThe COLING 2016 Organizing Committee.\n59\nLarge language models for text classification\nA Appendix\nA.1 Stance distributions\nTrump Clinton Sum\nFavor 148 163 311\nAgainst 299 565 864\nNone 260 256 516\nSum 707 984 1691\nTable A1: Twitter: Stance distribution by target\nClinton: Favor Against None Sum\nTrump: Favor 0 110 469 579\nAgainst 46 15 177 238\nNone 230 562 791 1583\nSum 276 687 1437 2400\nTable A2: Facebook: Stance distribution across targets\nA.2 Baseline models\nThe first baseline model is a support vector machine (SVM) classifier using bag-of-words (BoW)\nfeatures. The texts were preprocessed by performing tokenization, stopword removal, and stemming.\nThe resulting tokens were then used to construct term frequency-inverse document frequency\n(TF-IDF) weighted vectors representing each n-gram, where n ranges from 1 to 3. The purpose\nof TF-IDF weighting is to give higher weight to less frequent terms that should theoretically be\nbetter for discriminating between documents (Martin and Jurafsky, 2024). To reduce sparsity, we\nremove tokens that occur fewer than ten times in each training corpus, occur in more than 80% of\ndocuments, and rank outside the top 5000 most frequent tokens. We use the default SVM classifier\nfrom the Python package scikit-learn (Pedregosa et al., 2011).\nTo address the potential limitations of sparse BoW features, we implement a second baseline using\nembedding representations. We use pre-trained embeddings, specifically, GloVe (Global Vectors for\nWord Representation) embeddings trained on 2 billion tweets (Pennington et al., 2014). Pre-trained\nGloVe embeddings perform well on many NLP tasks (Rodriguez and Spirling, 2022), and these\nembeddings trained on Twitter data should better capture the specificities of language use on social\nmedia. We use the same preprocessing steps as above, except for stemming, which would prevent\nwords from being matched to the corresponding embedding. Each remaining word is represented as\n60\nLarge language models for text classification\na 200-dimensional real-valued vector, and documents are embedded in this vector space by taking\nthe average of the embeddings for each word. These document embeddings are then used as inputs\nin the same SVM classifier as above.\nTo assess whether more richly parameterized classifiers work better, we repeat baselines 1 and\n2 using deep neural networks. Specifically, we train two convolutional neural networks (CNNs)\nusing the Python package PyTorch (Paszke et al., 2019). Convolutional neural networks were\ninitially proposed for image classification (Krizhevsky et al., 2012) but have demonstrated strong\nperformance on text classification tasks (Kim, 2014). Each model contains 14 layers, including\nconvolutional, MaxPooling, and fully-connected layers. The ReLU activation function is used to\nconstrain outputs to non-negative values. The data are passed through Dropout layers to reduce\noverfitting by randomly setting a proportion of weights to zero. These models with BoW and GloVe\nare trained for 20 epochs with gradient updates after every batch of 16 documents.\nWe estimate all four models for each dataset using all training data, thus providing a baseline to\ncompare against the full fine-tuned models. The results for the Twitter and Facebook comment\ntasks are reported at the bottom of Table A5 and Table A6, respectively. In general, we find that the\nmodels with BoW features outperform the embedding representations when evaluated using the\nstrictest joint F1 score, despite the additional information encoded in the latter from pre-training.\nThis suggests that the TF-IDF weighted n-gram representations better capture key features related\nto the stance detection task. Regarding the model selection, both models achieve comparable\nperformance using the BoW features for the Twitter task, but the SVM performs better on the\nFacebook comments.\nA.3 Hyperparameters, fine-tuning specifications, and compute infrastructure\nOne challenge in comparing model performance across different families of models is to set up as\nconsistent hyperparameters and fine-tuning environments as possible. Since the size and architecture\nof the models differ significantly, we had to apply slightly different fine-tuning setups. For the\nencoder-only models, we modify two key parameters. Batch size controls how many training\nexamples are used for each gradient update. Although a batch size of 16 or 32 is common when\nfine-tuning BERT, we used a batch size of 4 to make the fine-tuning environment comparable to the\nother models. The learning rate controls how much the model updates its parameters each iteration.\nWe used a learning rate of 2e-5 that Sun et al. (2020) suggests is necessary to help the BERT model\nconverge during fine-tuning.\n61\nLarge language models for text classification\nFor the other models, we applied a batch size of 1 and performed gradient updates at every fourth\nbatch, averaging loss functions from 4 consecutive batches, such that our effective batch size is\n4 while using an actual batch size of 1. Without this technique, larger models like Llama3-70B\nconsume more GPU memory than consumer-level GPUs can offer. For memory efficiency, we\nalso deployed QLoRA (Dettmers et al., 2023), an approach that combines two separate techniques:\n4-bit parameter quantization and Parameter-Efficient Fine-Tuning (PEFT) (Mangrulkar et al., 2022)\nwith Low Rank Adapter (LoRA) (Hu et al., 2021). Quantization refers to techniques to cast model\nparameters, usually in float32 or bfloat16, to a less precise data type such as int4 (Gray and Neuhoff,\n1998). In practice, this means that weights are converted from decimals to integers. This approach\nsignificantly reduces memory usage and speeds up inference with minimal impact on accuracy\n(Jacob et al., 2018). PEFT is based on the well-known algebraic insight underlying techniques\nlike singular value decomposition that large matrices can be approximated by low-rank matrices\nwithout much information loss. LLMs can thus be fine-tuned by updating a low-rank approximation\nof the parameter matrices that has far fewer parameters than the original model (typically less\nthan 1 percent). For our fine-tuning experiments, we used hyperparameters recommended in the\nQLoRA paper (Dettmers et al., 2023): a learning rate of 2e-4, a dropout of 0.05 (0.1 for Llama3-\n70B), alpha = 16, and gamma = 64. Due to constraints on hardware availability, we were unable\nto systematically explore how variation in these parameters impacted performance, although we\nran ad-hoc experiments with Llama3-8B that showed quantization had no significant impact on\npredictive performance.\nEach GPT model has a “temperature” hyperparameter controlling the degree of stochastic variation\nin the output. The default temperature of 0.7 is suitable for generative applications because it\nensures that the output varies, even with identical inputs. Since this is a classification task, we set\nthe temperature to zero so the model returns the highest probability sequence of tokens, avoiding\nrandom variation in the predicted classes. 1 Other hyperparameters are set to defaults used by\nOpenAI.\nOur experiments were implemented using a MacBook Pro laptop with an M1 Max 32-core Graphical\nProcessing Unit (GPU) and 64 gigabytes (GB) of RAM for the first, encoder-only model family, and\nNVIDIA RTX 3090 (24GB) and A100 (40GB) GPUs on the Amarel cluster at Rutgers University\n1This practice is advised by OpenAI. See example code provided for fine-tuning a classi-\nfier: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_\nclassification.ipynb\n62\nLarge language models for text classification\nfor the second, medium to large encoder-decoder and decoder-only model family. Fine-tuning\nLlama3-70B, the largest among the open-weights models in this study, was distributed across\ntwo A100 GPUs and took approximately 10 hours. Except for the GPT experiments, which were\ncompleted using the OpenAI API, all other analyses were completed using the Hugging Face\nlibraries in Python (Wolf et al., 2020). Our experiments with GPT-3 models were self-funded\n(costing approximately $1500) and our GPT-4o analyses were funded by credits provided through\nOpenAI’s Researcher Access Program (costing less than $100 in credits since we did not perform\nfine-tuning).\nA.4 Few-shot regression models\nC1 C2 C3 T1 T2 T3\nwordcount 0.00 0.00 0.00 0.00 0.00 0.00\n(0.00) (0.00) (0.00) (0.00) (0.00) (0.00)\nTrumpAgainst -0.01 0.00 0.01 0.04\n(0.02) (0.03) (0.02) (0.03)\nTrumpFavor 0.02 0.04 0.01 0.03*\n(0.02) (0.02) (0.01) (0.02)\nClintonAgainst 0.02 0.03 0.01 0.03\n(0.02) (0.02) (0.01) (0.02)\nClintonFavor 0.03 0.05* 0.04* 0.07**\n(0.02) (0.02) (0.02) (0.02)\nTrumpAgainst × ClintonAgainst 0.03 0.04\n(0.08) (0.06)\nTrumpFavor × ClintonAgainst -0.05 -0.08*\n(0.04) (0.03)\nTrumpAgainst × ClintonFavor -0.08 -0.12*\n(0.06) (0.05)\nIntercept 0.71*** 0.69*** 0.68*** 0.63*** 0.61*** 0.61***\n(0.01) (0.01) (0.01) (0.01) (0.01) (0.01)\nR2 0.000 0.048 0.082 0.007 0.060 0.170\nR2 Adj. -0.010 -0.003 0.002 -0.004 0.009 0.097\n* p < 0.05, ** p < 0.01, *** p < 0.001. N=100.\nTable A3: Regression models predicting target-specific F1 score for Facebook one-shot experi-\nments\nThis table shows the regression coefficients from six models predicting target-specific F1 scores using the length of an exemplar, the\nstances in the exemplar towards both targets, and the stance interactions. The reference category of the target-specific stances is\nNeutral/None.\n63\nLarge language models for text classification\nC1 C2 C3 T1 T2 T3\nwordcountTrump 0.00 0.00 0.00 0.00 0.00 0.00\n(0.00) (0.00) (0.00) (0.00) (0.00) (0.00)\nwordcountClinton 0.00* 0.00* 0.00** 0.00 0.00 0.00\n(0.00) (0.00) (0.00) (0.00) (0.00) (0.00)\nTrumpAgainst 0.00 0.02 -0.02** -0.01\n(0.01) (0.02) (0.01) (0.02)\nTrumpFavor 0.00 0.01 -0.01 0.00\n(0.01) (0.02) (0.01) (0.02)\nClintonAgainst 0.00 0.01 -0.01 0.00\n(0.01) (0.02) (0.01) (0.02)\nClintonFavor 0.00 0.02 0.01 0.04\n(0.01) (0.02) (0.01) (0.02)\nTrumpAgainst × ClintonAgainst -0.02 -0.01\n(0.02) (0.02)\nTrumpFavor × ClintonAgainst 0.00 0.01\n(0.02) (0.03)\nTrumpAgainst × ClintonFavor -0.03 -0.03\n(0.03) (0.03)\nTrumpFavor × ClintonFavor -0.05 -0.03\n(0.03) (0.03)\nIntercept 0.63*** 0.63*** 0.62*** 0.40*** 0.42*** 0.41***\n(0.02) (0.02) (0.02) (0.02) (0.02) (0.02)\nR2 0.072 0.079 0.133 0.018 0.143 0.174\nR2 Adj. 0.053 0.019 0.035 -0.002 0.087 0.081\n* p < 0.05, ** p < 0.01, *** p < 0.001. N=100.\nTable A4: Regression models predicting target-specific F1 score for Twitter two-shot experi-\nments\nThis table shows the regression coefficients from six models predicting target-specific F1 scores using the length of target-specific\nexemplars, the stance of the exemplar towards each target, and the stance interactions. The reference category of the target-specific\nstances is Neutral/None.\n64\nLarge language models for text classification\nA.5 Full results\nModel Type F1T F1S F1J RT RS RJ PT PS PJ\nBERT 10 0.46 0.24 0.14 0.5 0.29 0.21 0.6 0.31 0.15\n100 0.43 0.36 0.18 0.58 0.53 0.35 0.34 0.28 0.12\nAll 0.85 0.65 0.6 0.85 0.65 0.6 0.85 0.65 0.61\nSBERT 10 0.5 0.39 0.21 0.59 0.5 0.34 0.57 0.36 0.28\n100 0.43 0.36 0.18 0.58 0.53 0.35 0.34 0.28 0.12\nAll 0.87 0.72 0.65 0.87 0.72 0.65 0.87 0.72 0.66\nDeBERTa 0 0.59 0.6 0.44 0.63 0.6 0.43 0.62 0.68 0.54\n10 0.25 0.36 0.05 0.42 0.53 0.18 0.18 0.28 0.03\n100 0.43 0.36 0.18 0.58 0.53 0.35 0.34 0.28 0.12\nAll 0.87 0.66 0.57 0.87 0.67 0.6 0.87 0.67 0.61\nFlan-T5 XXL 0 0 0.57 0 0 0.61 0 0 0.63 0\n2 0.01 0.58 0.01 0 0.63 0 0.21 0.65 0.04\n10 0.25 0.07 0.01 0.42 0.21 0.08 0.18 0.04 0.01\n100 0.43 0.36 0.18 0.58 0.53 0.35 0.34 0.28 0.12\nAll 0.77 0.53 0.41 0.78 0.61 0.49 0.79 0.5 0.38\nMistral-7B 0 0.76 0.63 0.48 0.7 0.64 0.48 0.85 0.66 0.55\n2 0.77 0.62 0.52 0.72 0.63 0.52 0.84 0.64 0.58\n10 0.45 0.28 0.15 0.49 0.29 0.18 0.59 0.35 0.23\n100 0.47 0.48 0.26 0.57 0.5 0.33 0.52 0.48 0.27\nAll 0.82 0.64 0.56 0.82 0.64 0.56 0.82 0.64 0.57\nLlama3-8B 0 0.69 0.59 0.43 0.68 0.62 0.44 0.83 0.67 0.54\n2 0.76 0.63 0.49 0.71 0.67 0.51 0.87 0.73 0.61\n10 0.58 0.45 0.25 0.59 0.45 0.3 0.58 0.44 0.23\n100 0.44 0.55 0.28 0.59 0.58 0.37 0.76 0.57 0.32\nAll 0.84 0.75 0.67 0.84 0.75 0.66 0.85 0.76 0.68\nLlama3-70B 0 0.77 0.64 0.52 0.7 0.66 0.53 0.86 0.68 0.56\n2 0.78 0.68 0.54 0.73 0.7 0.56 0.84 0.71 0.59\n10 0.56 0.37 0.22 0.56 0.4 0.27 0.57 0.36 0.22\n100 0.44 0.5 0.25 0.58 0.55 0.35 0.55 0.54 0.39\nAll 0.81 0.73 0.62 0.81 0.73 0.61 0.81 0.73 0.62\nGPT-3 Ada 0 0.26 0.04 0 0.23 0.02 0 0.73 0.46 0\n2 0.06 0.11 0.03 0.04 0.06 0.02 0.72 0.34 0.38\n10 0.81 0.44 0.3 0.81 0.5 0.41 0.83 0.43 0.25\n100 0.82 0.58 0.46 0.82 0.59 0.48 0.82 0.58 0.47\nAll 0.88 0.73 0.65 0.88 0.73 0.65 0.88 0.73 0.66\nGPT-3 Davinci 0 0.75 0.7 0.49 0.64 0.7 0.48 0.89 0.72 0.58\n2 0.77 0.68 0.54 0.66 0.68 0.5 0.92 0.7 0.66\n10 0.78 0.43 0.3 0.76 0.47 0.35 0.8 0.45 0.38\n100 0.84 0.69 0.61 0.84 0.69 0.61 0.85 0.69 0.63\nAll 0.89 0.76 0.69 0.88 0.76 0.69 0.89 0.76 0.7\nGPT-4o 0 0.75 0.72 0.53 0.63 0.72 0.48 0.94 0.78 0.62\n2 0.76 0.73 0.53 0.64 0.73 0.49 0.94 0.79 0.63\nSVM + BoW All 0.80 0.56 0.48 0.81 0.57 0.49 0.81 0.56 0.49\nSVM + GloVe All 0.71 0.48 0.34 0.72 0.53 0.40 0.72 0.48 0.33\nCNN + BoW All 0.78 0.59 0.48 0.78 0.59 0.48 0.80 0.59 0.49\nCNN + GloVe All 0.66 0.53 0.38 0.66 0.54 0.38 0.69 0.54 0.42\nTable A5: Twitter results.\nThis table shows the held-out scores for each model and learning regime. The scores are split into groups: F1 scores for Target (T),\nStance (S), and the joint score (J); Recall scores; and Precision scores. The best model by each metric is shown in bold.\n65\nLarge language models for text classification\nModel Type F1C F1T F1J RC RT RJ PC PT PJ\nBERT 10 0.38 0.47 0.14 0.4 0.47 0.16 0.41 0.49 0.16\n100 0.42 0.52 0.22 0.44 0.66 0.31 0.54 0.43 0.2\n1000 0.83 0.82 0.7 0.84 0.82 0.72 0.83 0.81 0.69\nAll 0.85 0.84 0.73 0.86 0.84 0.74 0.85 0.85 0.73\nSBERT 10 0.44 0.52 0.15 0.58 0.66 0.31 0.35 0.68 0.1\n100 0.35 0.52 0.26 0.42 0.66 0.36 0.65 0.43 0.32\n1000 0.84 0.83 0.72 0.84 0.84 0.74 0.84 0.84 0.72\nAll 0.85 0.84 0.74 0.86 0.84 0.76 0.86 0.85 0.74\nDeBERTa 0 0.44 0.68 0.32 0.43 0.67 0.3 0.69 0.73 0.61\n10 0.44 0.49 0.15 0.59 0.57 0.28 0.35 0.43 0.1\n100 0.53 0.52 0.29 0.56 0.66 0.4 0.58 0.43 0.24\n1000 0.82 0.86 0.71 0.82 0.86 0.73 0.82 0.87 0.71\nAll 0.85 0.86 0.74 0.85 0.86 0.75 0.85 0.87 0.74\nFlan-T5 XXL 0 0.2 0.26 0.02 0.23 0.34 0.04 0.18 0.81 0.02\n1 0.24 0.33 0.02 0.32 0.38 0.05 0.19 0.82 0.02\n10 0.44 0.52 0.15 0.59 0.66 0.32 0.35 0.43 0.1\n100 0.37 0.52 0.22 0.39 0.66 0.3 0.44 0.43 0.19\n1000 0.62 0.69 0.46 0.69 0.73 0.52 0.64 0.65 0.46\nAll 0.75 0.75 0.58 0.78 0.79 0.64 0.77 0.71 0.56\nMistral-7B 0 0.67 0.54 0.39 0.66 0.51 0.35 0.7 0.82 0.71\n1 0.67 0.59 0.42 0.66 0.56 0.38 0.71 0.77 0.67\n10 0.2 0.45 0.08 0.23 0.43 0.09 0.39 0.51 0.13\n100 0.5 0.56 0.26 0.49 0.6 0.29 0.55 0.57 0.24\n1000 0.73 0.73 0.56 0.73 0.73 0.56 0.74 0.73 0.57\nAll 0.77 0.77 0.61 0.77 0.77 0.61 0.77 0.77 0.61\nLlama3-8B 0 0.78 0.7 0.59 0.77 0.66 0.54 0.81 0.83 0.75\n1 0.78 0.53 0.45 0.77 0.5 0.41 0.81 0.86 0.77\n10 0.45 0.4 0.12 0.45 0.41 0.16 0.46 0.44 0.15\n100 0.52 0.54 0.3 0.5 0.62 0.34 0.6 0.55 0.28\n1000 0.79 0.8 0.66 0.79 0.81 0.67 0.79 0.8 0.67\nAll 0.83 0.83 0.69 0.83 0.84 0.7 0.83 0.83 0.69\nLlama3-70B 0 0.79 0.78 0.66 0.78 0.76 0.64 0.82 0.85 0.76\n1 0.82 0.81 0.69 0.81 0.8 0.68 0.84 0.87 0.79\n10 0.47 0.53 0.15 0.54 0.63 0.27 0.48 0.48 0.11\n100 0.51 0.52 0.35 0.5 0.63 0.4 0.61 0.45 0.33\n1000 0.79 0.8 0.64 0.79 0.81 0.65 0.79 0.8 0.64\nAll 0.82 0.84 0.69 0.82 0.84 0.7 0.82 0.84 0.7\nGPT-3 Ada 0 0.05 0.11 0 0.04 0.24 0 0.09 0.08 0\n1 0.05 0.07 0 0.03 0.07 0 0.46 0.43 0\n10 0.46 0.52 0.18 0.58 0.66 0.33 0.38 0.43 0.12\n100 0.71 0.73 0.55 0.74 0.73 0.57 0.74 0.73 0.56\n1000 0.84 0.82 0.71 0.84 0.82 0.71 0.84 0.82 0.72\nAll 0.86 0.82 0.73 0.86 0.82 0.73 0.87 0.82 0.74\nGPT-3 Davinci 0 0.72 0.77 0.61 0.72 0.76 0.56 0.79 0.84 0.73\n1 0.73 0.64 0.51 0.73 0.62 0.45 0.8 0.83 0.74\n10 0.45 0.51 0.15 0.58 0.64 0.29 0.37 0.43 0.1\n100 0.8 0.84 0.67 0.81 0.84 0.69 0.81 0.84 0.66\n1000 0.88 0.89 0.8 0.88 0.89 0.8 0.89 0.89 0.8\nAll 0.91 0.9 0.83 0.91 0.9 0.83 0.91 0.9 0.83\nGPT-4o 0 0.91 0.9 0.83 0.91 0.91 0.84 0.91 0.91 0.85\n1 0.91 0.91 0.84 0.91 0.91 0.84 0.91 0.91 0.85\nSVM + BoW All 0.71 0.76 0.54 0.72 0.78 0.55 0.71 0.75 0.55\nSVM + GloVe All 0.60 0.64 0.39 0.65 0.69 0.43 0.58 0.61 0.38\nCNN + BoW All 0.67 0.67 0.46 0.70 0.67 0.49 0.68 0.69 0.48\nCNN + GloVe All 0.57 0.61 0.38 0.65 0.62 0.44 0.68 0.67 0.47\nTable A6: Facebook results.\nThis table shows the held-out scores for each model and learning regime. The scores are split into three groups: F1 scores for Clinton\n(C), Trump (T), and the joint score (J); Recall scores; and Precision scores. The best model by each metric is shown in bold.\n66\nLarge language models for text classification\nLength Position Model F1C F1T F1J RC RT RJ PC PT PJ\nAll All Llama3-8B zero-shot 0.74 0.60 0.48 0.73 0.61 0.47 0.79 0.71 0.59\nLlama3-8B instruction-tuned 0.86 0.86 0.75 0.86 0.85 0.75 0.87 0.86 0.77\nLlama3-70B zero-shot 0.85 0.86 0.74 0.85 0.86 0.74 0.85 0.86 0.74\nLlama3-70B instruction-tuned 0.89 0.90 0.82 0.89 0.90 0.82 0.90 0.90 0.83\nGPT-4o zero-shot 0.85 0.85 0.73 0.85 0.84 0.73 0.87 0.85 0.76\nGPT-4o baseline 0.83 0.77 0.64 0.83 0.76 0.66 0.84 0.77 0.70\n0 0 Llama3-8B zero-shot 0.81 0.58 0.45 0.80 0.62 0.46 0.86 0.75 0.55\nLlama3-8B instruction-tuned 0.96 0.88 0.84 0.96 0.88 0.84 0.96 0.90 0.87\nLlama3-70B zero-shot 0.88 0.82 0.76 0.88 0.82 0.74 0.89 0.83 0.80\nLlama3-70B instruction-tuned 0.96 0.92 0.89 0.96 0.92 0.90 0.97 0.92 0.92\nGPT-4o zero-shot 0.83 0.69 0.58 0.82 0.70 0.54 0.89 0.71 0.74\nGPT-4o baseline 0.76 0.61 0.42 0.76 0.60 0.42 0.81 0.62 0.60\n1 All Llama3-8B zero-shot 0.81 0.65 0.56 0.81 0.64 0.57 0.83 0.73 0.65\nLlama3-8B instruction-tuned 0.92 0.87 0.83 0.92 0.87 0.83 0.93 0.87 0.83\nLlama3-70B zero-shot 0.85 0.86 0.75 0.85 0.86 0.75 0.85 0.87 0.76\nLlama3-70B instruction-tuned 0.95 0.92 0.90 0.95 0.92 0.90 0.95 0.92 0.91\nGPT-4o zero-shot 0.88 0.85 0.77 0.88 0.85 0.77 0.89 0.87 0.81\nGPT-4o baseline 0.78 0.79 0.61 0.78 0.78 0.63 0.78 0.81 0.65\n0 Llama3-8B zero-shot 0.81 0.66 0.60 0.82 0.66 0.62 0.85 0.75 0.72\nLlama3-8B instruction-tuned 0.92 0.84 0.82 0.92 0.84 0.82 0.93 0.85 0.85\nLlama3-70B zero-shot 0.92 0.86 0.83 0.92 0.86 0.82 0.93 0.88 0.85\nLlama3-70B instruction-tuned 0.96 0.86 0.86 0.96 0.86 0.86 0.96 0.86 0.87\nGPT-4o zero-shot 0.88 0.88 0.81 0.88 0.88 0.80 0.90 0.90 0.87\nGPT-4o baseline 0.77 0.78 0.57 0.78 0.78 0.62 0.78 0.83 0.54\n1 Llama3-8B zero-shot 0.80 0.63 0.53 0.80 0.62 0.52 0.82 0.71 0.65\nLlama3-8B instruction-tuned 0.92 0.90 0.84 0.92 0.90 0.84 0.94 0.90 0.86\nLlama3-70B zero-shot 0.77 0.86 0.66 0.78 0.86 0.68 0.78 0.86 0.66\nLlama3-70B instruction-tuned 0.94 0.98 0.94 0.94 0.98 0.94 0.95 0.98 0.95\nGPT-4o zero-shot 0.88 0.82 0.73 0.88 0.82 0.74 0.88 0.84 0.76\nGPT-4o baseline 0.78 0.77 0.63 0.78 0.78 0.64 0.78 0.79 0.66\n2 All Llama3-8B zero-shot 0.72 0.63 0.45 0.70 0.63 0.43 0.80 0.75 0.66\nLlama3-8B instruction-tuned 0.81 0.87 0.73 0.81 0.87 0.73 0.83 0.88 0.76\nLlama3-70B zero-shot 0.87 0.88 0.79 0.86 0.88 0.78 0.88 0.88 0.81\nLlama3-70B instruction-tuned 0.85 0.91 0.79 0.85 0.91 0.79 0.87 0.91 0.82\nGPT-4o zero-shot 0.87 0.89 0.78 0.87 0.89 0.77 0.87 0.89 0.80\nGPT-4o baseline 0.84 0.80 0.67 0.84 0.79 0.67 0.85 0.80 0.74\n0 Llama3-8B zero-shot 0.82 0.58 0.52 0.82 0.58 0.52 0.84 0.69 0.68\nLlama3-8B instruction-tuned 0.82 0.84 0.72 0.82 0.84 0.72 0.82 0.84 0.73\nLlama3-70B zero-shot 0.86 0.82 0.73 0.86 0.82 0.74 0.86 0.83 0.79\nLlama3-70B instruction-tuned 0.88 0.90 0.82 0.88 0.90 0.82 0.88 0.90 0.83\nGPT-4o zero-shot 0.88 0.92 0.80 0.88 0.92 0.82 0.88 0.92 0.85\nGPT-4o baseline 0.90 0.84 0.76 0.90 0.84 0.76 0.90 0.85 0.82\n1 Llama3-8B zero-shot 0.71 0.68 0.43 0.68 0.68 0.42 0.77 0.80 0.70\nLlama3-8B instruction-tuned 0.77 0.84 0.72 0.74 0.84 0.68 0.82 0.85 0.78\nLlama3-70B zero-shot 0.83 0.92 0.77 0.82 0.92 0.76 0.85 0.92 0.81\nLlama3-70B instruction-tuned 0.84 0.94 0.83 0.82 0.94 0.80 0.87 0.94 0.88\nGPT-4o zero-shot 0.83 0.88 0.74 0.82 0.88 0.72 0.83 0.88 0.80\nGPT-4o baseline 0.81 0.76 0.60 0.80 0.76 0.60 0.81 0.76 0.73\n2 Llama3-8B zero-shot 0.66 0.62 0.42 0.60 0.62 0.36 0.84 0.78 0.76\nLlama3-8B instruction-tuned 0.87 0.92 0.79 0.86 0.92 0.78 0.90 0.94 0.86\nLlama3-70B zero-shot 0.91 0.90 0.85 0.90 0.90 0.84 0.93 0.91 0.89\nLlama3-70B instruction-tuned 0.85 0.88 0.74 0.84 0.88 0.74 0.89 0.90 0.82\nGPT-4o zero-shot 0.90 0.86 0.77 0.90 0.86 0.78 0.92 0.89 0.84\nGPT-4o baseline 0.83 0.79 0.66 0.82 0.78 0.66 0.85 0.81 0.74\n3 All Llama3-8B zero-shot 0.70 0.55 0.44 0.70 0.56 0.44 0.76 0.65 0.55\nLlama3-8B instruction-tuned 0.87 0.88 0.77 0.87 0.88 0.78 0.89 0.88 0.79\nLlama3-70B zero-shot 0.87 0.88 0.78 0.87 0.88 0.78 0.88 0.88 0.80\nLlama3-70B instruction-tuned 0.90 0.90 0.83 0.90 0.90 0.83 0.90 0.91 0.84\n67\nLarge language models for text classification\n(continued)\nLength Position Model F1C F1T F1J RC RT RJ PC PT PJ\nGPT-4o zero-shot 0.87 0.88 0.78 0.87 0.88 0.78 0.89 0.88 0.80\nGPT-4o baseline 0.86 0.77 0.68 0.86 0.77 0.69 0.86 0.77 0.71\n0 Llama3-8B zero-shot 0.68 0.66 0.54 0.68 0.68 0.52 0.73 0.78 0.69\nLlama3-8B instruction-tuned 0.88 0.90 0.84 0.88 0.90 0.84 0.90 0.90 0.86\nLlama3-70B zero-shot 0.82 0.92 0.79 0.82 0.92 0.78 0.86 0.92 0.84\nLlama3-70B instruction-tuned 0.90 0.96 0.88 0.90 0.96 0.88 0.91 0.96 0.91\nGPT-4o zero-shot 0.88 0.88 0.81 0.88 0.88 0.80 0.90 0.89 0.84\nGPT-4o baseline 0.86 0.82 0.72 0.86 0.82 0.74 0.87 0.83 0.75\n1 Llama3-8B zero-shot 0.76 0.54 0.43 0.76 0.54 0.46 0.77 0.57 0.53\nLlama3-8B instruction-tuned 0.88 0.88 0.78 0.88 0.88 0.78 0.89 0.88 0.81\nLlama3-70B zero-shot 0.94 0.84 0.81 0.94 0.84 0.82 0.94 0.85 0.82\nLlama3-70B instruction-tuned 0.90 0.88 0.81 0.90 0.88 0.82 0.90 0.88 0.81\nGPT-4o zero-shot 0.86 0.90 0.77 0.86 0.90 0.76 0.88 0.91 0.79\nGPT-4o baseline 0.87 0.78 0.69 0.88 0.78 0.72 0.90 0.79 0.72\n2 Llama3-8B zero-shot 0.69 0.51 0.41 0.68 0.50 0.36 0.80 0.67 0.62\nLlama3-8B instruction-tuned 0.87 0.80 0.71 0.86 0.80 0.70 0.90 0.82 0.75\nLlama3-70B zero-shot 0.90 0.88 0.80 0.90 0.88 0.80 0.91 0.89 0.81\nLlama3-70B instruction-tuned 0.92 0.88 0.84 0.92 0.88 0.84 0.94 0.89 0.87\nGPT-4o zero-shot 0.88 0.88 0.80 0.88 0.88 0.78 0.91 0.89 0.86\nGPT-4o baseline 0.88 0.69 0.64 0.88 0.68 0.64 0.88 0.70 0.66\n3 Llama3-8B zero-shot 0.69 0.51 0.41 0.66 0.52 0.40 0.82 0.67 0.61\nLlama3-8B instruction-tuned 0.87 0.92 0.79 0.86 0.92 0.78 0.88 0.92 0.81\nLlama3-70B zero-shot 0.84 0.88 0.75 0.82 0.88 0.74 0.87 0.88 0.80\nLlama3-70B instruction-tuned 0.87 0.90 0.79 0.86 0.90 0.78 0.90 0.90 0.82\nGPT-4o zero-shot 0.87 0.86 0.76 0.86 0.86 0.76 0.90 0.87 0.80\nGPT-4o baseline 0.83 0.80 0.65 0.82 0.80 0.66 0.86 0.83 0.74\n4 All Llama3-8B zero-shot 0.75 0.54 0.46 0.74 0.54 0.45 0.79 0.70 0.61\nLlama3-8B instruction-tuned 0.89 0.81 0.74 0.89 0.80 0.73 0.89 0.83 0.76\nLlama3-70B zero-shot 0.85 0.82 0.71 0.85 0.82 0.71 0.85 0.83 0.71\nLlama3-70B instruction-tuned 0.92 0.87 0.81 0.92 0.87 0.81 0.92 0.88 0.82\nGPT-4o zero-shot 0.89 0.79 0.70 0.88 0.79 0.70 0.89 0.82 0.75\nGPT-4o baseline 0.85 0.77 0.65 0.85 0.76 0.66 0.85 0.78 0.73\n0 Llama3-8B zero-shot 0.82 0.66 0.59 0.82 0.66 0.58 0.85 0.71 0.68\nLlama3-8B instruction-tuned 0.90 0.86 0.78 0.90 0.86 0.78 0.91 0.88 0.81\nLlama3-70B zero-shot 0.88 0.94 0.82 0.88 0.94 0.82 0.88 0.95 0.85\nLlama3-70B instruction-tuned 0.92 0.94 0.86 0.92 0.94 0.86 0.93 0.95 0.89\nGPT-4o zero-shot 0.90 0.83 0.74 0.90 0.82 0.74 0.91 0.88 0.83\nGPT-4o baseline 0.90 0.79 0.69 0.90 0.80 0.72 0.91 0.84 0.81\n1 Llama3-8B zero-shot 0.76 0.58 0.45 0.76 0.58 0.46 0.77 0.63 0.49\nLlama3-8B instruction-tuned 0.86 0.84 0.72 0.86 0.84 0.72 0.86 0.84 0.73\nLlama3-70B zero-shot 0.79 0.78 0.63 0.80 0.78 0.62 0.81 0.78 0.67\nLlama3-70B instruction-tuned 0.86 0.86 0.75 0.86 0.86 0.74 0.86 0.87 0.79\nGPT-4o zero-shot 0.80 0.78 0.61 0.80 0.78 0.60 0.80 0.79 0.63\nGPT-4o baseline 0.77 0.78 0.66 0.78 0.78 0.66 0.78 0.79 0.73\n2 Llama3-8B zero-shot 0.73 0.42 0.40 0.72 0.42 0.36 0.79 0.68 0.60\nLlama3-8B instruction-tuned 0.86 0.72 0.68 0.86 0.70 0.64 0.86 0.77 0.73\nLlama3-70B zero-shot 0.82 0.69 0.60 0.82 0.68 0.58 0.83 0.73 0.63\nLlama3-70B instruction-tuned 0.98 0.83 0.81 0.98 0.82 0.80 0.98 0.85 0.86\nGPT-4o zero-shot 0.92 0.72 0.69 0.92 0.70 0.64 0.94 0.80 0.79\nGPT-4o baseline 0.86 0.75 0.66 0.86 0.74 0.66 0.87 0.76 0.68\n3 Llama3-8B zero-shot 0.72 0.51 0.47 0.70 0.52 0.44 0.80 0.74 0.68\nLlama3-8B instruction-tuned 0.88 0.85 0.80 0.88 0.84 0.78 0.90 0.87 0.84\nLlama3-70B zero-shot 0.86 0.84 0.75 0.86 0.84 0.74 0.87 0.85 0.76\nLlama3-70B instruction-tuned 0.92 0.90 0.87 0.92 0.90 0.86 0.93 0.91 0.89\nGPT-4o zero-shot 0.89 0.82 0.74 0.88 0.82 0.72 0.90 0.85 0.82\nGPT-4o baseline 0.81 0.82 0.62 0.80 0.82 0.64 0.85 0.83 0.68\n4 Llama3-8B zero-shot 0.75 0.57 0.47 0.68 0.54 0.40 0.86 0.86 0.83\n68\nLarge language models for text classification\n(continued)\nLength Position Model F1C F1T F1J RC RT RJ PC PT PJ\nLlama3-8B instruction-tuned 0.95 0.80 0.77 0.94 0.78 0.74 0.96 0.86 0.84\nLlama3-70B zero-shot 0.91 0.85 0.79 0.90 0.84 0.78 0.92 0.91 0.85\nLlama3-70B instruction-tuned 0.93 0.83 0.79 0.92 0.82 0.78 0.95 0.84 0.81\nGPT-4o zero-shot 0.92 0.83 0.80 0.92 0.82 0.78 0.93 0.90 0.88\nGPT-4o baseline 0.91 0.70 0.65 0.90 0.68 0.64 0.92 0.74 0.76\n5 All Llama3-8B zero-shot 0.73 0.67 0.51 0.72 0.66 0.51 0.78 0.74 0.60\nLlama3-8B instruction-tuned 0.82 0.87 0.73 0.81 0.87 0.72 0.85 0.87 0.77\nLlama3-70B zero-shot 0.83 0.87 0.71 0.82 0.87 0.71 0.83 0.87 0.73\nLlama3-70B instruction-tuned 0.86 0.91 0.79 0.86 0.91 0.79 0.88 0.91 0.81\nGPT-4o zero-shot 0.81 0.87 0.72 0.81 0.87 0.72 0.82 0.87 0.74\nGPT-4o baseline 0.83 0.77 0.65 0.83 0.77 0.66 0.83 0.78 0.69\n0 Llama3-8B zero-shot 0.75 0.70 0.54 0.74 0.70 0.56 0.80 0.77 0.62\nLlama3-8B instruction-tuned 0.88 0.94 0.85 0.88 0.94 0.84 0.88 0.95 0.86\nLlama3-70B zero-shot 0.88 0.92 0.81 0.88 0.92 0.80 0.89 0.93 0.84\nLlama3-70B instruction-tuned 0.90 0.98 0.91 0.90 0.98 0.90 0.90 0.98 0.93\nGPT-4o zero-shot 0.90 0.90 0.83 0.90 0.90 0.82 0.90 0.92 0.85\nGPT-4o baseline 0.90 0.82 0.74 0.90 0.82 0.74 0.90 0.83 0.79\n1 Llama3-8B zero-shot 0.75 0.69 0.53 0.74 0.68 0.54 0.78 0.74 0.57\nLlama3-8B instruction-tuned 0.72 0.88 0.66 0.68 0.88 0.62 0.84 0.88 0.80\nLlama3-70B zero-shot 0.79 0.84 0.68 0.78 0.84 0.68 0.80 0.87 0.79\nLlama3-70B instruction-tuned 0.81 0.89 0.75 0.78 0.90 0.72 0.86 0.90 0.83\nGPT-4o zero-shot 0.79 0.89 0.71 0.76 0.90 0.68 0.84 0.91 0.82\nGPT-4o baseline 0.80 0.73 0.63 0.80 0.76 0.66 0.80 0.80 0.70\n2 Llama3-8B zero-shot 0.69 0.69 0.49 0.68 0.68 0.48 0.74 0.76 0.57\nLlama3-8B instruction-tuned 0.81 0.79 0.65 0.80 0.78 0.64 0.84 0.81 0.70\nLlama3-70B zero-shot 0.77 0.86 0.63 0.76 0.86 0.62 0.79 0.88 0.70\nLlama3-70B instruction-tuned 0.89 0.86 0.75 0.88 0.86 0.74 0.91 0.87 0.81\nGPT-4o zero-shot 0.78 0.84 0.66 0.78 0.84 0.66 0.80 0.86 0.69\nGPT-4o baseline 0.78 0.80 0.66 0.78 0.80 0.66 0.78 0.81 0.67\n3 Llama3-8B zero-shot 0.76 0.70 0.55 0.76 0.68 0.54 0.78 0.78 0.63\nLlama3-8B instruction-tuned 0.86 0.90 0.77 0.86 0.90 0.76 0.87 0.91 0.79\nLlama3-70B zero-shot 0.90 0.84 0.76 0.90 0.84 0.76 0.90 0.84 0.78\nLlama3-70B instruction-tuned 0.88 0.94 0.82 0.88 0.94 0.82 0.90 0.94 0.85\nGPT-4o zero-shot 0.86 0.82 0.76 0.86 0.82 0.76 0.86 0.83 0.79\nGPT-4o baseline 0.86 0.70 0.58 0.86 0.70 0.62 0.87 0.71 0.57\n4 Llama3-8B zero-shot 0.68 0.64 0.43 0.66 0.64 0.44 0.79 0.73 0.55\nLlama3-8B instruction-tuned 0.85 0.84 0.75 0.84 0.84 0.74 0.89 0.84 0.81\nLlama3-70B zero-shot 0.80 0.86 0.65 0.80 0.86 0.66 0.84 0.87 0.73\nLlama3-70B instruction-tuned 0.85 0.86 0.74 0.84 0.86 0.74 0.88 0.86 0.81\nGPT-4o zero-shot 0.75 0.88 0.68 0.74 0.88 0.66 0.81 0.89 0.77\nGPT-4o baseline 0.81 0.72 0.56 0.80 0.72 0.58 0.84 0.74 0.61\n5 Llama3-8B zero-shot 0.78 0.60 0.51 0.76 0.60 0.48 0.84 0.71 0.65\nLlama3-8B instruction-tuned 0.83 0.86 0.76 0.82 0.86 0.74 0.88 0.87 0.81\nLlama3-70B zero-shot 0.82 0.88 0.74 0.82 0.88 0.74 0.82 0.88 0.76\nLlama3-70B instruction-tuned 0.88 0.90 0.80 0.88 0.90 0.80 0.89 0.90 0.81\nGPT-4o zero-shot 0.80 0.86 0.71 0.80 0.86 0.72 0.81 0.87 0.74\nGPT-4o baseline 0.83 0.82 0.71 0.84 0.82 0.72 0.83 0.83 0.73\nTable A7: Thread prediction scores.\nThis table contains the F1, precision, and recall scores for the thread-prediction models. Length and position denote the number\nof replies of the thread and the position of a particular text (where 0 equals the original comment). The top set of rows shows the\nresults overall across all comments and replies and the remaining rows show the performance for each thread length, starting with\nthe aggregate score across all texts, followed by scores for texts in a specific position. The Model column lists the models used.\nThe remaining columns are split into three groups: F1 scores for Clinton (C), Trump (T), and the joint score (J); Recall scores; and\nPrecision scores. The best-performing model across all lengths and positions is in bold.\n69",
  "topic": "Zero (linguistics)",
  "concepts": [
    {
      "name": "Zero (linguistics)",
      "score": 0.7904578447341919
    },
    {
      "name": "Computer science",
      "score": 0.6176837682723999
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5871074199676514
    },
    {
      "name": "Natural language processing",
      "score": 0.5128424763679504
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48250675201416016
    },
    {
      "name": "One shot",
      "score": 0.47862452268600464
    },
    {
      "name": "Linguistics",
      "score": 0.30705133080482483
    },
    {
      "name": "Engineering",
      "score": 0.10276469588279724
    },
    {
      "name": "Chemistry",
      "score": 0.07600170373916626
    },
    {
      "name": "Philosophy",
      "score": 0.06566122174263
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}