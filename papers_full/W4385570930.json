{
  "title": "Transformer Language Models Handle Word Frequency in Prediction Head",
  "url": "https://openalex.org/W4385570930",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5103679720",
      "name": "Goro Kobayashi",
      "affiliations": [
        "RIKEN",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5062574320",
      "name": "Tatsuki Kuribayashi",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5025689297",
      "name": "Sho Yokoi",
      "affiliations": [
        "RIKEN",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5101815181",
      "name": "Kentaro Inui",
      "affiliations": [
        "RIKEN",
        "Tohoku University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1533970595",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3035207248",
    "https://openalex.org/W2890560993",
    "https://openalex.org/W4317897818",
    "https://openalex.org/W3200704197",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3099143320",
    "https://openalex.org/W2119191234",
    "https://openalex.org/W4294443024",
    "https://openalex.org/W3200520312",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3152884768",
    "https://openalex.org/W3122855191",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W3204643324",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W3121309507",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2587690726",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W2963096510"
  ],
  "abstract": "Prediction head is a crucial component of Transformer language models.Despite its direct impact on prediction, this component has often been overlooked in analyzing Transformers.In this study, we investigate the inner workings of the prediction head, specifically focusing on bias parameters.Our experiments with BERT and GPT-2 models reveal that the biases in their word prediction heads play a significant role in the models’ ability to reflect word frequency in a corpus, aligning with the logit adjustment method commonly used in long-tailed learning. We also quantify the effect of controlling the biases in practical auto-regressive text generation scenarios;under a particular setting, more diverse text can be generated without compromising text quality.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 4523–4535\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nTransformer Language Models Handle Word Frequency\nin Prediction Head\nGoro Kobayashi1,3 Tatsuki Kuribayashi2,1 Sho Yokoi1,3 Kentaro Inui1,3\n1 Tohoku University 2 MBZUAI 3 RIKEN\ngoro.koba@dc.tohoku.ac.jp tatsuki.kuribayashi@mbzuai.ac.ae\n{yokoi, kentaro.inui}@tohoku.ac.jp\nAbstract\nPrediction head is a crucial component of Trans-\nformer language models. Despite its direct im-\npact on prediction, this component has often\nbeen overlooked in analyzing Transformers. In\nthis study, we investigate the inner workings\nof the prediction head, specifically focusing on\nbias parameters. Our experiments with BERT\nand GPT-2 models reveal that the biases in their\nword prediction heads play a significant role in\nthe models’ ability to reflect word frequency\nin a corpus, aligning with the logit adjustment\nmethod commonly used in long-tailed learning.\nWe also quantify the effect of controlling the\nbiases in practical auto-regressive text genera-\ntion scenarios; under a particular setting, more\ndiverse text can be generated without compro-\nmising text quality.\n/gtbhttps://github.com/gorokoba560/\ntransformer-lm-word-freq-bias\n1 Introduction\nTransformer language models (TLMs) (Devlin\net al., 2019; Radford et al., 2019) are now funda-\nmental to natural language processing (NLP) tech-\nniques, including text generation. Owing to this\nsuccess, extensive research has been conducted to\nanalyze their inner workings (Rogers et al., 2020;\nGeva et al., 2021).\nIn this study, we shed light on the operation of\nthe prediction head, the last block of the TLMs.\nDespite its direct impact on TLMs’ output, its char-\nacteristics have been overlooked in previous analy-\nses. Our experiments with BERT and GPT-2 reveal\nthat a particular bias parameter in the predic-\ntion head adjusts the model’s output toward\nword frequency in a corpus . Particularly, the\nbias increases the prediction probability for high-\nfrequency words and vice versa (Figure 1).\nWe further explore this phenomenon from sev-\neral perspectives. First, we analyze the geometric\ncharacteristics of this phenomenon, which show\nCorpus word frequency Corpus word frequency\nPrediction probability\nOriginal\nw/o\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ U/n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U ʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ\nʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ /n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U/n.pc/i.pc/f.pc/o.pc/r.pc/m.pcʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ ʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nͨωοτϫʔΫͰ͋Δʢਤ΍\nΛ\n৽͍ͯ͘͠ɽ\nʹ͸༧ଌϔου͕͋Γɼ͜Ε͕\nΛड\nମత\nԽʢ\nΈ૚Ͱ\nӨ͢\n͢Δɽ͜\n͠ɼ Ͱ\n਺ Λ\nशՄೳ\n͢Δɽ\nʹରԠ͢Δ\nͷ༧ଌ\n͢Δ\nʣɿ\n)\n⊙ + /u1D483LN\nΛड͚औ\n཰෼෍\n͢Δɿ\n͜͜Ͱɼ ͓Αͼ ͸ͦΕͧΕཁૉͰͷฏ\n͠ɼ ͸ཁૉੵΛද͢ɽ·ͨɼ\nशՄೳͳॏΈύ\nश\nՄೳͳόΠΞεύϥϝʔλΛද͢ɽҎ্ͷΑ͏ʹɼ\n௨ͯ͠όΠΞε\nͭɽ\nʹ͋Δ\nճ\n͢ΔόΠΞ\nʹରͯ͠৐\n༻͢ΔॏΈύϥϝʔλͱҧ͍ɼόΠΞεύϥ\n༻͢Δɽ\nͰ͸ϕΫτϧ͸ԣϕΫτϧͱ͍ͯ͠Δɽ\nݧ࣮\n༧ଌʹ\nʹ͍ͭͯ෼ੳ͢Δɽ અͰ͸༧ଌϔο\nशίʔύεͰͷ\nͳͬͯ\n͍Δ͜ͱΛ໌Β͔ʹ͢Δɽߦ\n͍\n͔ΊΔɽ\nϞσϧΈͷ ͱ Λ\nର৅ͱͨ͠ɽ ͸ ʢ ૚ʣ ɾʢ ૚ʣͷ\nछྨɼ ͸ ʢ ૚ʣ ɾ ʢ ૚ʣ ɾ\nʢ ૚ʣ ɾʢ ૚ʣͷ छྨΛ༻͍ͨɽ\nྻ\n༧ଌͤͨ͞ɽ\nσʔλ Ϟσϧ΁ೖྗ͢ΔςΩετͱͯ͠\nशίʔύεͰ͋Δ ͷςετ\nྻΛ༻͍ͨ ʣલ\nशͷઃఆʹै͍ɼ ʣͷτʔΫϯΛ ʹ\n͏\nྻ͔Βϥ\nྻͷΈΛ༻͍ͨɽ\nίʔύεස౓͸ɼ ͓\nग़\nͨ͠ ʣɽ\nڹ\nόΠΞ\nΛ༩͍͑ͯΔ͔ௐ΂Δɽ\nͯ͠ग़ྗ͞\n༧\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\n͍ͯ͠\nΔɽ\nΛऔͬͯ\nϓϩοτͯ͠Δ͜ͱͷઆ໌ͨ͠ͱ͜\n཰͕Լ͕Γɼ௿ස\nʣެ\n։͞Ε͍ͯΔ Λ༻͍ͨɽ\nྻͷ ͕બ͹Εɼͦͷ͏ͪ\n͞ΕΔɽ\nशίʔύε͸ ͱ ɼ\nशίʔύε͸ Ͱ͋ΔɽͦΕͧΕ\n։͞Ε\nग़ͨ͠ɽ\nFigure 1: Changes in word prediction probabilities due\nto the removal of bias bLN from BERT base (left) and\nGPT-2 small (right).\nthat word frequency is encoded in a specific direc-\ntion in the output embedding space. Second, we\nanalyze the behavioral impact of controlling their\nfrequency biases on text generation. The results\ndemonstrate that the model’s text generation can be\nmade more diverse while maintaining the fluency\nby adequately decaying the bias parameters, sug-\ngesting that models can more or less isolate word\nfrequency knowledge from other text generation\nability. Third, we discuss the potential connec-\ntion between our findings and the logit adjustment\nmethod that is typically used in the machine learn-\ning field to address the class imbalance problem.\n2 Background: Prediction Head\nTLMs have a stack of Transformer layers on top\nof the embedding layer. These components update\nhidden token representations (Figure 2). Predic-\ntion head, which is our target of analysis, is the\nlast, top-most component in TLMs. The prediction\nhead in a TLM computes the prediction probabili-\nties for all vocabulary Vbased on the hidden state\nin the last Transformer layer.\nFormally, the prediction head receives, for each\ntoken, the last Transformer layer’s hidden statex∈\nRd. The prediction head computes the probability\ndistribution p∈R|V|of the next word as follows,\n4523\nBERT GPT-2\nAttention Mechanism\nLayer Normalization\nLayer Normalization\nEmbedding Layer\nFully Connected Layer\nLayer Normalization\nProjection onto \nAdding\n!\t×\n\tTransformer \nLayer\nPrediction\nHead\nAttention Mechanism\nLayer Normalization\nLayer Normalization\nEmbedding Layer\nLayer Normalization\nProjection onto\n(w/  )\n (w/  )\n(w/  )\nFeed-forward Network\nFeed-forward Network\nFigure 2: Architecture overview of BERT and GPT-2.\nin the case of GPT-2:\np= softmax\n(\nLN(x)Wemb\n)\n(1)\nLN(x) := x−m(x)\ns(x) ⊙γ+ bLN ∈Rd, (2)\nwhere Wemb ∈Rd×|V| denotes the word embed-\nding matrix, m(x) and s(x) denote the element-\nwise mean and standard deviation, respectively,\nand ⊙denotes the element-wise product. γ and\nbLN ∈Rd are learnable parameters.\nFor BERT, there is an additional fully connected\nlayer (FC). The prediction head computes the prob-\nability distribution pfor the hidden state xthat\ncorresponds to the [MASK] token as follows:\np= softmax\n(\nLN(x′)Wemb + blast\n)\n(3)\nx′= GELU\n(\nxWFC + bFC\n)\n∈Rd, (4)\nwhere WFC ∈Rd×d denotes the learnable weight\nmatrix, and bFC ∈Rd and blast ∈R|V|denote the\nlearnable bias parameters. GELU (Hendrycks and\nGimpel, 2016) is the activation function.\nBoth prediction heads contain the bias bLN, and\nthe BERT head additionally contains the biases\nbFC and blast. As the first step in analyzing the\nprediction head, we focus on these three biases\nbecause they can easily be mapped to the output\nspace. Drawing on the existing findings about the\nfrequency-related workings of several components\nin the Transformer (V oita et al., 2019; Kobayashi\net al., 2020), we analyze the model behavior with\nrespect to word frequency.\n3 Experiments\nFirst, we show that the bias parameters are related\nto word frequency. Next, we analyze their proper-\nties from two perspectives: (i) geometric character-\nistics and (ii) text generation.\nModel: We used BERT (cased) (Devlin et al.,\n2019) in two different sizes (base and large) and\nGPT-2 (Radford et al., 2019) in four different sizes\n(small, medium, large, and xl).\nData: We used 5,000 sequences from the test set\nof the GPT-2 pre-trainng corpus, OpenWebText\nCorpus (Gokaslan and Cohen, 2019) 1. Each se-\nquence was fed into BERT after some tokens were\nreplaced with [MASK]2, and fed into GPT-2 as it\nwas. Further, word frequencies were calculated\nfrom the corpus used for training each of BERT\nand GPT-2.3\n3.1 Impact of biases on prediction distribution\nWe compared the models’ word prediction with\nand without each bias. Specifically, we once ob-\ntained word prediction distributions ˆp∈R|V|from\na model for each time step across the test data.\nThe average of these distributions are referred to as\nmodel’s word prediction distributionhenceforth.\nBias adjusts the model’s prediction distribution\ncloser to the corpus frequency distribution:\nFigure 1 shows changes in the model’s word pre-\ndiction distribution before and after the bias bLN\nis removed.4 The removal of bLN increases the\nprobability of the model predicting low-frequency\nwords (right side of the figures) and vice versa,\nwhich results in a word prediction distribution that\napproaches a flat (UNIFORM in the figure). In other\nwords, the bias bLN adjusts the models’ word pre-\ndiction distribution to be closer to the corpus word\nfrequency distribution ( UNIGRAM in the figure).\nThis finding can be generalized across all model\nsizes (Appendix A).\nTo quantify the above effect, we calculated the\nKullback–Leibler (KL) divergence between the\nmodel’s word prediction distribution and the cor-\npus word frequency distribution (UNIGRAM ). Note\nthat a higher value indicates that the model’s predic-\ntion distribution has more discrepancy with that in\n1webtext.test.jsonl published in https://github.\ncom/openai/gpt-2-output-dataset was used.\n2Following Devlin et al. (2019), 15% of tokens were re-\nplaced with [MASK] 80% of the time.\n3BERT was trained on Wikipedia and BooksCorpus (Zhu\net al., 2015), and GPT-2 was trained on OpenWebText Corpus.\nWe reproduced them using Datasets (Lhoest et al., 2021).\n4We created bins to divide the corpus word frequencies\ninto constant intervals and plotted each bin’s geometric mean\nand standard deviation for the word prediction probabilities.\n4524\nModel Original w/o bLN w/o bFC w/o blast\nBERT base 0.20 0.39 0.22 0.23\nlarge 0.21 0.39 0.23 0.23\nGPT-2\nsmall 0.14 0.83 - -\nmedium 0.14 0.34 - -\nlarge 0.14 0.17 - -\nxl 0.14 0.17 - -\nTable 1: KL divergence between the model’s word pre-\ndiction distribution and the corpus word frequency dis-\ntribution. A larger value means that the distributions\nare more divergent. bFC and bbias are contained only in\nBERT.\nthe pretraining corpus. Table 1 shows that remov-\ning bLN always results in a higher value, which\nindicates that bLN indeed adjusts the prediction\ndistribution to be closer to the corpus frequency\ndistribution. The biases bFC and blast in BERT\nalso exert a similar effect, but it is weaker than\nthat of bLN; we focus on bLN in the following. We\nalso observe that larger models have less change of\nfrequency biases due to bLN.\n3.2 Geometric observations\nWe observed the geometric properties of the bias\nbLN and the output embedding space of the TLMs.\nWord frequency is encoded in the bias vector’s\ndirection in the output embedding space:\nThe observation that the bias vector shifts predic-\ntions according to word frequency suggests that\nword frequency is encoded in the output embed-\nding space Wemb, and the bias vector bLN is a\ngood projection to extract this frequency informa-\ntion. In fact, the inner product of bLN and each\nword embedding wi in the embedding layer corre-\nlates well with the word frequency5 (Figure 3).\nFurthermore, we observed that removing the bias\ndirection (≈frequency direction) from the embed-\nding matrix Wemb improved the isotropy (unifor-\nmity in direction, e.g., Ethayarajh, 2019) in the out-\nput embedding space. Formally, we removed the\nbias direction using wi ←wi −⟨wi, bLN\n∥bLN∥⟩bLN\n∥bLN∥;\nthen, the average value 1\nn2\n∑\ni\n∑\nj cos(wi,wj) de-\ncreased from 0.15 to 0.09 in BERT base. This\nobservation shows that the anisotropy in the out-\nput space is, more or less, caused by the frequency\ndirection.\nWe further observed that hidden states htoken\nbefore bLN was added were almost orthogonal\n5Spearman’s ρwas 0.78 on GPT-2 small.\nCorpus frequency of word\nInner productsmall. hbLN, wii\nwi\nFigure 3: Relationship between the corpus word fre-\nquency and inner product of bLN and each output word\nembedding wi in GPT-2 small.\nto bLN (≈word frequency direction); in partic-\nular, Etoken |cos (htoken, bLN)|= 0.08 ≪1.0 in\nBERT-base. This corroborates that the frequency\nbias injected in the prediction head indeed does\nnot exist in the hidden states before the prediction\nhead.\nWord frequency encoded on the bias vector is\nshifted via fine-tuning:\nWe also inspected whether the model’s word pre-\ndiction distribution is shifted to that in the target\ndomain during fine-tuning to enhance the general-\nity of our observation. Specifically, we fine-tuned\nGPT-2 small on a dataset consisting of abstracts\nfrom papers in the machine learning field6, whose\nword frequency distribution is different from the\npretraining data. After fine-tuning, the inner prod-\nuct of bLN and each word embeddingwi correlated\nmore with the additional fine-tuning corpus after\nfine-tuning (the Spearman’s ρchanged from 0.38\nto 0.62) and slightly less with the pre-training cor-\npus (the Spearman’s ρchanged from 0.78 to 0.73).\nThis suggests that frequency information captured\nby the bias bLN is updated during fine-tuning.\n3.3 Impact of bias on text generation\nWe next demonstrate that controlling the bias bLN\ncan lead to more diverse text generation without\nsignificant harm to the quality of the text. We hope\nthat quantifying the effect of such control using\nmetrics for the evaluation of text generation (e.g.,\nn-gram diversity) will enhance the connection be-\ntween the language generation field and the field of\nprobing/interpreting LMs’ internals.\n6CShorten/ML-ArXiv-Papers published in https://\nhuggingface.co/datasets/CShorten/ML-ArXiv-Papers\non Datasets (Lhoest et al., 2021) was used.\n4525\nProcedure: We adjusted bLN during text gener-\nation by GPT-2, and we then evaluated the gen-\nerated text. Specifically, we introduced an adjust-\nment coefficient λ∈[0,1] and replaced bLN with\nλbLN. We report the evaluation scores by varying\nλ. The results generated with the top-p sampling\nstrategy (Fan et al., 2018) are reported in this sec-\ntion. The results for other decoding settings are in\nAppendix A; we found similar results for the top-p\nand top-k sampling but found degradation with the\nvanilla sampling setting. The details of the settings\nare described in Appendix B.\nEvaluation methods: Text generated by each\nmodel was evaluated from two perspectives: di-\nversity and quality. For the diversity evaluation,\nDistinct-n Dn (Li et al., 2016) and N-gram diver-\nsity D(Meister et al., 2022) were used. These mea-\nsures of n-gram overlap in generated texts were\ncalculated as follows:\nDn(texts) := # Unique n-grams in texts\n# n-grams in texts (5)\nD(texts) := 1\n4\n4∑\nn=1\nDn(texts). (6)\nFor the quality evaluation, MAUVE (Pillutla et al.,\n2021) and Perplexity ( PPL) were used. MAUVE\nevaluates how similar a given text generation model\nis to humans by comparing human-written texts and\nmodel-generated texts according to the difference\nin their distributions in a sentence embedding space.\nPPL evaluates how well models can predict words\nin human-written texts.\nResults: Table 2 shows the results. Weaken-\ning the bias bLN (λ < 1) increased the diver-\nsity of the generated text but decreased the PPL\nscore, exhibiting a general trade-off between them.\nNevertheless, for the larger models, GPT-2 large\n(λ = 0.5) and xl ( λ = 0.7), there was a sweet\nspot, where the diversity and the MAUVE score\nimproved with little decrease in PPL. This obser-\nvation can be interpreted as follows. The larger\nmodels were able to predict the context-dependent\nprobability of low-frequency words as precisely as\nthat of high-frequency words, so promoting low-\nfrequency words with those models improved the\ndiversity while maintaining the quality of the text.\nThe smaller models were equally accurate in pre-\ndicting the probability of high-frequency words but\ntended to be inaccurate for low-frequency words,\nso promoting low-frequency words degraded the\nModel λ Diversity ↑ Quality\nD1 D2 D MAUVE ↑ PPL ↓\nsmall\n1 0.04 0.32 0.49 0.85 19.4\n0.6 0.06 0.61 0.59 0.18 24.1\n0 0.04 0.36 0.32 0.01 65.9\nmed.\n1 0.05 0.35 0.51 0.90 14.6\n0.9 0.05 0.39 0.54 0.90 14.8\n0.2 0.07 0.63 0.60 0.14 18.8\n0 0.08 0.60 0.55 0.06 21.3\nlarge\n1 0.04 0.30 0.47 0.90 12.7\n0.5 0.04 0.36 0.50 0.91 12.9\n0 0.04 0.42 0.54 0.86 13.6\nxl\n1 0.04 0.30 0.47 0.90 11.4\n0.7 0.04 0.34 0.49 0.92 11.5\n0 0.04 0.41 0.53 0.86 12.1\nTable 2: Evaluation results for GPT-2 (top-p sampling)\nwhile bias bLN was controlled with λ. Results for λ=\n0,1, and other notable values are listed.\nquality of the text. This interpretation is also consis-\ntent with the class imbalance problem, which will\nbe discussed in Section 4.1. From the application\nperspective, this observation also suggests that the\nlexical diversity in text generation can be improved\nsimply by modifying particular parameters in the\nprediction head.\nWe also show several samples of the generated\ntexts (Appendix C). We generally observed that\noverly decreasing λincurs (i) more proper nouns,\n(ii) more repetitions of the same words or similar\nphrases, and (iii) the generation of ungrammatical\ntext, especially for the small models. This may also\nbe related to the suppression of the punctuation and\nend-of-sequence token, which are highly frequent.\n4 Discussion\n4.1 Connection with logit adjustment methods\nWe revealed that adding the bias bLN (which\nwas performed immediately before the logit was\ncomputed) encourages TLMs to generate high-\nfrequency words, and de-biasing promotes diver-\nsity. This can also be seen as analogous to logit ad-\njustment, which is a common technique for address-\ning the class imbalance problem, where the label\n(the word in text generation) frequency distribution\nis long-tailed (Provost, 2000; Zhou and Liu, 2006;\nCollell et al., 2016; Menon et al., 2021). In particu-\nlar, Menon et al. (2021) proposed to minimize the\nbalanced error (i.e., an average of per-class errors)\nby directly adding the label frequency distribution\nto logits during training but not during inference.\n4526\nOne can find an analogy between the modification\nof bLN and their method: (i) adding the frequency-\nshifting bias bLN corresponds to the operation of\nadding the class-frequency-based margins to the\nlogits; (ii) promoting low-frequency words by re-\nmoving bLN during inference corresponds to the\nway logit adjustment encourages low-class predic-\ntion. In other words, interestingly, TLMs seem to\nimplicitly learn something similar to balanced error\nminimization without being explicitly designed to\ndo so (e.g., loss function).\n4.2 Connection with a technique to initialize\nbias parameter with class frequency\nIn training neural classification models, using class\nfrequency to initialize the last bias to be added\nto the logit is a well-known and efficient tech-\nnique (Karpathy, 2019). Therefore, our observation\nthat the bias vector at the prediction head (i.e., the\nlast block) encodes word frequency might seem\nsomewhat obvious. However, our experimental re-\nsults showed peculiar trends that might be stemmed\nfrom the inductive bias of TLMs. First, although\nthe initialization technique implies the relationship\nbetween the last bias blast and the corpus word fre-\nquency, we found that the bias bLN ∈Rd, which\nis further away from the output and less expressive\nthan blast ∈R|V|, plays the role in encoding the\nfrequency in BERT (Table 1). For GPT-2, not even\nblast exists. Second, even bLN plays a weak role\nin encoding the frequency in larger models (Ta-\nble 1). These findings suggest that neural models\ndynamically determines the role of each internal\nmodule according to various factors such as param-\neter size and architecture. When and under what\nconditions the short vector bLN strongly encodes\nthe frequency is an interesting question and left to\nfuture research.\n5 Related work\nTransformer layers (e.g., attention patterns) have\nbeen the major focus of TLM analysis (Clark et al.,\n2019; Mareˇcek and Rosa, 2019; Kobayashi et al.,\n2021; Dai et al., 2022). The first embedding layer,\nespecially positional encoding, has also been stud-\nied (Wang et al., 2021; Kiyono et al., 2021). This\nstudy sheds light on the prediction head, the last\nblock of a TLM, and provides new insights into the\nworking mechanisms of TLMs.\nNotably, previous studies have reported that\nwords having a similar frequency are clustered in\nthe embedding spaces of various deep NLP mod-\nels (Mu and Viswanath, 2018; Gong et al., 2018;\nProvilkov et al., 2020; Liang et al., 2021); our ob-\nservation agrees with theirs. In addition to this, we\nnewly discovered that a particular bias parameter\nin the TLM prediction head corresponds to “word\nfrequency direction” in the word embedding space.\n6 Conclusions\nIn this study, we explored the workings of bias\nparameters in the prediction head of TLMs. Our\nexperiments with BERT and GPT-2 showed that the\nbiases adjust the model’s prediction with respect\nto word frequency. We further explored this phe-\nnomenon and provided the following insights: (i)\nword frequency is encoded in a specific direction\n(the bias direction) in the output embedding space,\n(ii) properly controlling the bias’s effect can en-\ncourage more diverse language generation without\ncompromising quality, and (iii) TLMs are implic-\nitly trained to be potentially consistent with the\nlogit adjustment method. In future work, we will\nanalyze larger TLMs, e.g., Open Pre-trained Trans-\nformers (Zhang et al., 2022). Further, we will ana-\nlyze the weight parameters in the prediction head\nin addition to the bias parameters.\nLimitations\nThere are mainly two limitations in this study.\nFirst, we still do not consider components other\nthan the bias parameters in the prediction head.\nFor example, the weight parameters of the pre-\ndiction head, i.e., γ and WFC, can also affect a\nmodel’s prediction. Second, our findings do not\ncover the Transformer language models other than\nBERT (base and large) and GPT-2 (small, medium,\nlarge, and xl). Consistent findings were obtained\nfor the two main architectures (i.e., encoder-based\nmasked, and decoder-based causal language mod-\nels) and for various model sizes, although future\nresearch is needed to show whether the findings\ncan be generalized to RoBERTa (Liu et al., 2019),\nOpen Pre-trained Transformer Language Models\n(OPT, Zhang et al., 2022), and other variants.\nConsidering Transformer encoder-decoder mod-\nels, such as neural machine translation models and\nT5 (Raffel et al., 2020), would also be an interest-\ning future direction.\n4527\nEthics Statement\nThis paper sheds light on the workings of the pre-\ndiction head of the fundamental models in NLP. In\nrecent years, unintended biases (e.g., gender bias)\nin neural network models have been problematic.\nThis paper may help in this direction by encourag-\ning researchers to analyze the prediction head as\nwell as Transformer layers.\nAcknowledgements\nWe would like to thank the members of the To-\nhoku NLP Group for their insightful comments,\nparticularly Shiki Sato for his valuable sugges-\ntions regarding experimental settings. This work\nwas supported by JSPS KAKENHI Grant Num-\nber JP22J21492, JP22H05106; JST CREST Grant\nNumber JPMJCR20D2, Japan; and JST ACT-X\nGrant Number JPMJAX200S, Japan.\nReferences\nSteven Bird and Edward Loper. 2004. NLTK: The Nat-\nural Language Toolkit. In Proceedings of the ACL\nInteractive Poster and Demonstration Sessions, pages\n214–217.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What Does BERT\nLook At? An Analysis of BERT’s Attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286.\nGuillem Collell, Drazen Prelec, and Kaustubh Patil.\n2016. Reviving Threshold-Moving: a Simple Plug-in\nBagging Ensemble for Binary and Multiclass Imbal-\nanced Data. arXiv preprint 1606.08698v3.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 8493–8502.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), pages 4171–4186.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 55–65.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical Neural Story Generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (ACL), pages 889–898.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer Feed-Forward Layers Are\nKey-Value Memories. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5484–5495.\nAaron Gokaslan and Vanya Cohen. 2019. OpenWeb-\nText Corpus.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei\nWang, and Tie-Yan Liu. 2018. FRAGE: Frequency-\nAgnostic Word Representation. In Advances in Neu-\nral Information Processing Systems 31 (NeurIPS) ,\npages 1341–1352.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian Error Linear Units (GELUs). arXiv preprint\n1606.08415v4.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The Curious Case of Neural Text\nDegeneration. In 8th International Conference on\nLearning Representations (ICLR).\nAndrej Karpathy. 2019. A Recipe for Training Neural\nNetworks. Andrej Karpathy blog.\nShun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Ken-\ntaro Inui. 2021. SHAPE: Shifted Absolute Position\nEmbedding for Transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 3309–3321.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is Not Only a Weight:\nAnalyzing Transformers with Vector Norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057–7075.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2021. Incorporating Residual and Nor-\nmalization Layers into Analysis of Masked Language\nModels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4547–4568.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A Community Library for Natural Lan-\nguage Processing. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\n4528\nProcessing (EMNLP): System Demonstrations, pages\n175–184.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A Diversity-Promoting Objec-\ntive Function for Neural Conversation Models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT), pages 110–119.\nYuxin Liang, Rui Cao, Jie Zheng, Jie Ren, and Ling\nGao. 2021. Learning to Remove: Towards Isotropic\nPre-trained BERT Embedding. In Artificial Neural\nNetworks and Machine Learning (ICANN) , pages\n448–459.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint, cs.CL/1907.11692v1.\nDavid Mare ˇcek and Rudolf Rosa. 2019. From\nBalustrades to Pierre Vinken: Looking for Syntax\nin Transformer Self-Attentions. In Proceedings of\nthe 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n263–275.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan\nCotterell. 2022. Locally Typical Sampling. arXiv\npreprint 2202.00666v4.\nAditya Krishna Menon, Sadeep Jayasumana,\nAnkit Singh Rawat, Himanshu Jain, Andreas\nVeit, and Sanjiv Kumar. 2021. Long-tail learning via\nlogit adjustment. In 9th International Conference on\nLearning Representations (ICLR).\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\nTop: Simple and Effective Postprocessing for Word\nRepresentations. In 6th International Conference on\nLearning Representations (ICLR).\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. MAUVE: Measuring the Gap Be-\ntween Neural Text and Human Text using Divergence\nFrontiers. In Advances in Neural Information Pro-\ncessing Systems 34 (NeurIPS).\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. BPE-Dropout: Simple and Effective Subword\nRegularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 1882–1892.\nFoster Provost. 2000. Machine Learning from Imbal-\nanced Data Sets 101. In Proceedings of the AAAI\n2000 Workshop on Imbalanced Data Sets, pages 1–3.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Tech-\nnical report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research\n(JMLR), 21(140):1–67.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What We Know\nAbout How BERT Works. Transactions of the Asso-\nciation for Computational Linguistics (TACL), 8:842–\n866.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nBottom-up Evolution of Representations in the Trans-\nformer: A Study with Machine Translation and Lan-\nguage Modeling Objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406.\nBenyou Wang, Lifeng Shang, Christina Lioma, Xin\nJiang, Hao Yang, Qun Liu, and Jakob Grue Simon-\nsen. 2021. On Position Embeddings in BERT. In\n9th International Conference on Learning Represen-\ntations (ICLR).\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: Open\nPre-trained Transformer Language Models. arXiv\npreprint 2205.01068v4.\nZhi-Hua Zhou and Xu-Ying Liu. 2006. Training Cost-\nSensitive Neural Networks with Methods Addressing\nthe Class Imbalance Problem. IEEE Transactions on\nKnowledge and Data Engineering, 18(1):63–77.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning Books and Movies: Towards\nStory-Like Visual Explanations by Watching Movies\nand Reading Books. In The IEEE International Con-\nference on Computer Vision (ICCV).\nA Experimental results in other settings\nIn Section 3.1, we presented the changes in the\nword prediction distribution before and after re-\nmoving the bias bLN of BERT base and GPT-2\nsmall in Figure 1. The results of the other models\nare shown in Figures 4 to 7.\nIn Section 3.2, we showed that the inner prod-\nuct of bLN and each output word embedding wi\ncorrelated well with the word frequency for GPT-2\nsmall (Figure 3). The results for the other mod-\nels are shown in Figures 8 to 12. The Spearman’s\ncorrelation coefficient is listed in Table 3.\n4529\nIn Section 3.3, we showed the effect of control-\nling the bias bLN on GPT-2’s text generation with\na top-p sampling strategy. We also conducted ex-\nperiments with other sampling strategies: top-k\nsampling (Holtzman et al., 2020) and vanilla sam-\npling. The results of these two sampling strategies\nare listed in Tables 4 and 5. We found that the\nresults of top-k sampling were similar to those of\ntop-p sampling; for the larger models, GPT-2 large\n(λ= 0.3) and xl (λ= 0.5), there also was a sweet\nspot, where diversity and MAUVE improved with\nlittle decrease in PPL. In contrast, with vanilla sam-\npling, both MAUVE and PPL decreased consistently\nand quickly.\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ U/n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U ʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ\nʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ /n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U/n.pc/i.pc/f.pc/o.pc/r.pc/m.pcʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ ʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nCorpus word frequency\nOriginal BERT\nw/o\nͨωοτϫʔΫͰ͋Δʢਤ΍\nΛ\n৽͍ͯ͘͠ɽ\nʹ͸༧ଌϔου͕͋Γɼ͜Ε͕\nΛड\nମత\nԽʢ\nΈ૚Ͱ\nӨ͢\n͢Δɽ͜\n͠ɼ Ͱ\n਺ Λ\nशՄೳ\n͢Δɽ\nʹରԠ͢Δ\nͷ༧ଌ\n͢Δ\nʣɿ\n)\n⊙ + /u1D483LN\nΛड͚औ\n཰෼෍\n͢Δɿ\n͜͜Ͱɼ ͓Αͼ ͸ͦΕͧΕཁૉͰͷฏ\n͠ɼ ͸ཁૉੵΛද͢ɽ·ͨɼ\nशՄೳͳॏΈύ\nश\nՄೳͳόΠΞεύϥϝʔλΛද͢ɽҎ্ͷΑ͏ʹɼ\n௨ͯ͠όΠΞε\nͭɽ\nʹ͋Δ\nճ\n͢ΔόΠΞ\nʹରͯ͠৐\n༻͢ΔॏΈύϥϝʔλͱҧ͍ɼόΠΞεύϥ\n༻͢Δɽ\nͰ͸ϕΫτϧ͸ԣϕΫτϧͱ͍ͯ͠Δɽ\nݧ࣮\n༧ଌʹ\nʹ͍ͭͯ෼ੳ͢Δɽ અͰ͸༧ଌϔο\nशίʔύεͰͷ\nͳͬͯ\n͍Δ͜ͱΛ໌Β͔ʹ͢Δɽߦ\n͍\n͔ΊΔɽ\nϞσϧΈͷ ͱ Λ\nର৅ͱͨ͠ɽ ͸ ʢ ૚ʣ ɾʢ ૚ʣͷ\nछྨɼ ͸ ʢ ૚ʣ ɾ ʢ ૚ʣ ɾ\nʢ ૚ʣ ɾʢ ૚ʣͷ छྨΛ༻͍ͨɽ\nྻ\n༧ଌͤͨ͞ɽ\nσʔλ Ϟσϧ΁ೖྗ͢ΔςΩετͱͯ͠\nशίʔύεͰ͋Δ ͷςετ\nྻΛ༻͍ͨ ʣલ\nशͷઃఆʹै͍ɼ ʣͷτʔΫϯΛ ʹ\n͏\nྻ͔Βϥ\nྻͷΈΛ༻͍ͨɽ\nίʔύεස౓͸ɼ ͓\nग़\nͨ͠ ʣɽ\nڹ\nόΠΞ\nΛ༩͍͑ͯΔ͔ௐ΂Δɽ\nͯ͠ग़ྗ͞\n༧\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\n͍ͯ͠\nΔɽ\nΛऔͬͯ\nϓϩοτͯ͠Δ͜ͱͷઆ໌ͨ͠ͱ͜\n཰͕Լ͕Γɼ௿ස\nʣެ\n։͞Ε͍ͯΔ Λ༻͍ͨɽ\nྻͷ ͕બ͹Εɼͦͷ͏ͪ\n͞ΕΔɽ\nशίʔύε͸ ͱ ɼ\nशίʔύε͸ Ͱ͋ΔɽͦΕͧΕ\n։͞Ε\nग़ͨ͠ɽ\nPrediction probability\nFigure 4: Changes in word prediction probabilities due\nto bias bLN removal on BERT large.\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ U/n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U ʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ ʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ /n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U/n.pc/i.pc/f.pc/o.pc/r.pc/m.pcʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ ʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nCorpus word frequency\nOriginal GPT-2\nw/o\nͨωοτϫʔΫͰ͋Δʢਤ΍\nΛ\n৽͍ͯ͘͠ɽ\nʹ͸༧ଌϔου͕͋Γɼ͜Ε͕\nΛड\nମత\nԽʢ\nΈ૚Ͱ\nӨ͢\n͢Δɽ͜\n͠ɼ Ͱ\n਺ Λ\nशՄೳ\n͢Δɽ\nʹରԠ͢Δ\nͷ༧ଌ\n͢Δ\nʣɿ\n)\n⊙ + /u1D483LN\nΛड͚औ\n཰෼෍\n͢Δɿ\n͜͜Ͱɼ ͓Αͼ ͸ͦΕͧΕཁૉͰͷฏ\n͠ɼ ͸ཁૉੵΛද͢ɽ·ͨɼ\nशՄೳͳॏΈύ\nश\nՄೳͳόΠΞεύϥϝʔλΛද͢ɽҎ্ͷΑ͏ʹɼ\n௨ͯ͠όΠΞε\nͭɽ\nʹ͋Δ\nճ\n͢ΔόΠΞ\nʹରͯ͠৐\n༻͢ΔॏΈύϥϝʔλͱҧ͍ɼόΠΞεύϥ\n༻͢Δɽ\nͰ͸ϕΫτϧ͸ԣϕΫτϧͱ͍ͯ͠Δɽ\nݧ࣮\n༧ଌʹ\nʹ͍ͭͯ෼ੳ͢Δɽ અͰ͸༧ଌϔο\nशίʔύεͰͷ\nͳͬͯ\n͍Δ͜ͱΛ໌Β͔ʹ͢Δɽߦ\n͍\n͔ΊΔɽ\nϞσϧΈͷ ͱ Λ\nର৅ͱͨ͠ɽ ͸ ʢ ૚ʣ ɾʢ ૚ʣͷ\nछྨɼ ͸ ʢ ૚ʣ ɾ ʢ ૚ʣ ɾ\nʢ ૚ʣ ɾʢ ૚ʣͷ छྨΛ༻͍ͨɽ\nྻ\n༧ଌͤͨ͞ɽ\nσʔλ Ϟσϧ΁ೖྗ͢ΔςΩετͱͯ͠\nशίʔύεͰ͋Δ ͷςετ\nྻΛ༻͍ͨ ʣલ\nशͷઃఆʹै͍ɼ ʣͷτʔΫϯΛ ʹ\n͏\nྻ͔Βϥ\nྻͷΈΛ༻͍ͨɽ\nίʔύεස౓͸ɼ ͓\nग़\nͨ͠ ʣɽ\nڹ\nόΠΞ\nΛ༩͍͑ͯΔ͔ௐ΂Δɽ\nͯ͠ग़ྗ͞\n༧\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\n͍ͯ͠\nΔɽ\nΛऔͬͯ\nϓϩοτͯ͠Δ͜ͱͷઆ໌ͨ͠ͱ͜\n཰͕Լ͕Γɼ௿ස\nʣެ\n։͞Ε͍ͯΔ Λ༻͍ͨɽ\nྻͷ ͕બ͹Εɼͦͷ͏ͪ\n͞ΕΔɽ\nशίʔύε͸ ͱ ɼ\nशίʔύε͸ Ͱ͋ΔɽͦΕͧΕ\n։͞Ε\nग़ͨ͠ɽ\nPrediction probability\nFigure 5: Changes in word prediction probabilities due\nto bias bLN removal on GPT-2 medium.\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ U/n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U ʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ ʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ /n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U/n.pc/i.pc/f.pc/o.pc/r.pc/m.pcʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ\nʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nCorpus word frequency\nOriginal GPT-2\nw/o\nͨωοτϫʔΫͰ͋Δʢਤ΍\nΛ\n৽͍ͯ͘͠ɽ\nʹ͸༧ଌϔου͕͋Γɼ͜Ε͕\nΛड\nମత\nԽʢ\nΈ૚Ͱ\nӨ͢\n͢Δɽ͜\n͠ɼ Ͱ\n਺ Λ\nशՄೳ\n͢Δɽ\nʹରԠ͢Δ\nͷ༧ଌ\n͢Δ\nʣɿ\n)\n⊙ + /u1D483LN\nΛड͚औ\n཰෼෍\n͢Δɿ\n͜͜Ͱɼ ͓Αͼ ͸ͦΕͧΕཁૉͰͷฏ\n͠ɼ ͸ཁૉੵΛද͢ɽ·ͨɼ\nशՄೳͳॏΈύ\nश\nՄೳͳόΠΞεύϥϝʔλΛද͢ɽҎ্ͷΑ͏ʹɼ\n௨ͯ͠όΠΞε\nͭɽ\nʹ͋Δ\nճ\n͢ΔόΠΞ\nʹରͯ͠৐\n༻͢ΔॏΈύϥϝʔλͱҧ͍ɼόΠΞεύϥ\n༻͢Δɽ\nͰ͸ϕΫτϧ͸ԣϕΫτϧͱ͍ͯ͠Δɽ\nݧ࣮\n༧ଌʹ\nʹ͍ͭͯ෼ੳ͢Δɽ અͰ͸༧ଌϔο\nशίʔύεͰͷ\nͳͬͯ\n͍Δ͜ͱΛ໌Β͔ʹ͢Δɽߦ\n͍\n͔ΊΔɽ\nϞσϧΈͷ ͱ Λ\nର৅ͱͨ͠ɽ ͸ ʢ ૚ʣ ɾʢ ૚ʣͷ\nछྨɼ ͸ ʢ ૚ʣ ɾ ʢ ૚ʣ ɾ\nʢ ૚ʣ ɾʢ ૚ʣͷ छྨΛ༻͍ͨɽ\nྻ\n༧ଌͤͨ͞ɽ\nσʔλ Ϟσϧ΁ೖྗ͢ΔςΩετͱͯ͠\nशίʔύεͰ͋Δ ͷςετ\nྻΛ༻͍ͨ ʣલ\nशͷઃఆʹै͍ɼ ʣͷτʔΫϯΛ ʹ\n͏\nྻ͔Βϥ\nྻͷΈΛ༻͍ͨɽ\nίʔύεස౓͸ɼ ͓\nग़\nͨ͠ ʣɽ\nڹ\nόΠΞ\nΛ༩͍͑ͯΔ͔ௐ΂Δɽ\nͯ͠ग़ྗ͞\n༧\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\n͍ͯ͠\nΔɽ\nΛऔͬͯ\nϓϩοτͯ͠Δ͜ͱͷઆ໌ͨ͠ͱ͜\n཰͕Լ͕Γɼ௿ස\nʣެ\n։͞Ε͍ͯΔ Λ༻͍ͨɽ\nྻͷ ͕બ͹Εɼͦͷ͏ͪ\n͞ΕΔɽ\nशίʔύε͸ ͱ ɼ\nशίʔύε͸ Ͱ͋ΔɽͦΕͧΕ\n։͞Ε\nग़ͨ͠ɽ\nPrediction probability\nFigure 6: Changes in word prediction probabilities due\nto bias bLN removal on GPT-2 large.\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ U/n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U ʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ ʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nදͷϞσϧ\n༧ଌ෼෍ͷ μΠόʔδΣϯεɽ஋͕খ͍͞΄Ͳ\n͍ͯ͠Δ͜ͱΛද͢ɽ\nͷ μΠόʔδΣϯε\nͳ͠\nڹ\nόΠΞε\n֤\n༧ଌ෼෍Λ௨ৗ\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\nࣔ\n͍ͯ͠Δɽ\nΛ\nऔͬͯϓϩοτͯ͠Δ͜ͱͷઆ໌͠\n཰͕Լ͕Γɼ\nՌ\n͍ͮͨɽͭ·ΓɼόΠΞε\nΛ༧ଌ͠\nස౓෼\nͳ͍ͬͯΔ͜ͱ͕Θ͔ͬͨɽ\nՌ͕ಘΒ\nΕͨʢਤ\nʣɽ\nόΠΞεʹΑΔස౓ิਖ਼Λఆ\n༧ଌ\nස౓෼\n෍ʢਤͷ /n.pc/i.pc/g.pc/r.pc/a.pc/m.pc,U/n.pc/i.pc/f.pc/o.pc/r.pc/m.pcʣͷ μΠόʔδΣϯ\nεΛଌͬͨʢද\nʣ ɽͲͷϞσϧͰ΋όΠΞε Λ\n௨\nΘΕ͍ͯΔ͜ͱ͕Θ͔ͬͨɽͨͩ͠ɼϞσϧ\n͸ऑ·͍ͬͯͨɽ\nʹ͓͚Δ ͓Αͼ ΋ಉ༷ͷස౓ิਖ਼\n͸খ͔ͬͨ͞ɽ\n͏ɽ\nԿྻ\nࢼ\nਤߦ\nՌɽ\nΈ ʹ฿͍ɼόΠΞεύϥϝʔλ Λ௚઀ɼຒ\nኮ਺ ͷ\nग़͢Δ͜ͱ\nମతʹ͸\n௨Γɼ\nͱͷ಺ੵ͕େ͖͘ͳΔΑ͏ͳόΠΞ\nೝ͞Ε\nͨʢਤ\nϞσϧ͸ɼ΂͖෼෍ʹै\n͢ΔॲཧΛ༧ଌϔουʹ͋Δఔ౓೚ͤɼ\n͍ੈքͰҙ\nͳ͍ͬͯ\n͞ΕΔɽ\nڹ\n༧ଌΛ୯\n͚ͮΔΑ͏ิਖ਼͍ͯ͠Δ͜ͱ͕෼͔ͬ\n͢Δ͜ͱ\nΛվળͰ͖ΔՄೳੑΛ\n͢ɽ\nखॱ͠ͳ͕Β ʹςΩ\n۩\n਺ Λಋೖ͠ɼ༧ଌϔου\n͑Δɽ ΛมԽͤ͞Δ͜ͱ\n͠ͳ͕ΒςΩετΛੜ੒\nͷσίʔσΟϯάख๏ʹ͸\nɼ ʢʣαϯϓϦϯάɼ\nʢ ʣ αϯϓϦϯάɼ ʢʣ αϯϓϦϯάͷ\nઃఆ͸෇࿥\n͢ɽ\nධՁવ͞ͱଟ༷ੑ\nવ͞ͷධՁʹ͸\n͕ੜ੒ͨ͠\nςΩετͱϞσϧ͕ੜ੒ͨ͠ςΩετΛड͚औΓɼ\nCorpus word frequency\nPrediction probability\nOriginal GPT-2\nw/o\nͨωοτϫʔΫͰ͋Δʢਤ΍\nΛ\n৽͍ͯ͘͠ɽ\nʹ͸༧ଌϔου͕͋Γɼ͜Ε͕\nΛड\nମత\nԽʢ\nΈ૚Ͱ\nӨ͢\n͢Δɽ͜\n͠ɼ Ͱ\n਺ Λ\nशՄೳ\n͢Δɽ\nʹରԠ͢Δ\nͷ༧ଌ\n͢Δ\nʣɿ\n)\n⊙ + /u1D483LN\nΛड͚औ\n཰෼෍\n͢Δɿ\n͜͜Ͱɼ ͓Αͼ ͸ͦΕͧΕཁૉͰͷฏ\n͠ɼ ͸ཁૉੵΛද͢ɽ·ͨɼ\nशՄೳͳॏΈύ\nश\nՄೳͳόΠΞεύϥϝʔλΛද͢ɽҎ্ͷΑ͏ʹɼ\n௨ͯ͠όΠΞε\nͭɽ\nʹ͋Δ\nճ\n͢ΔόΠΞ\nʹରͯ͠৐\n༻͢ΔॏΈύϥϝʔλͱҧ͍ɼόΠΞεύϥ\n༻͢Δɽ\nͰ͸ϕΫτϧ͸ԣϕΫτϧͱ͍ͯ͠Δɽ\nݧ࣮\n༧ଌʹ\nʹ͍ͭͯ෼ੳ͢Δɽ અͰ͸༧ଌϔο\nशίʔύεͰͷ\nͳͬͯ\n͍Δ͜ͱΛ໌Β͔ʹ͢Δɽߦ\n͍\n͔ΊΔɽ\nϞσϧΈͷ ͱ Λ\nର৅ͱͨ͠ɽ ͸ ʢ ૚ʣ ɾʢ ૚ʣͷ\nछྨɼ ͸ ʢ ૚ʣ ɾ ʢ ૚ʣ ɾ\nʢ ૚ʣ ɾʢ ૚ʣͷ छྨΛ༻͍ͨɽ\nྻ\n༧ଌͤͨ͞ɽ\nσʔλ Ϟσϧ΁ೖྗ͢ΔςΩετͱͯ͠\nशίʔύεͰ͋Δ ͷςετ\nྻΛ༻͍ͨ ʣલ\nशͷઃఆʹै͍ɼ ʣͷτʔΫϯΛ ʹ\n͏\nྻ͔Βϥ\nྻͷΈΛ༻͍ͨɽ\nίʔύεස౓͸ɼ ͓\nग़\nͨ͠ ʣɽ\nڹ\nόΠΞ\nΛ༩͍͑ͯΔ͔ௐ΂Δɽ\nͯ͠ग़ྗ͞\n༧\n͢Δɽ\nՌࡍ\n༧ଌ෼෍ͷมԽΛਤ\n͸\nޠ\n͍ͯ͠\nΔɽ\nΛऔͬͯ\nϓϩοτͯ͠Δ͜ͱͷઆ໌ͨ͠ͱ͜\n཰͕Լ͕Γɼ௿ස\nʣެ\n։͞Ε͍ͯΔ Λ༻͍ͨɽ\nྻͷ ͕બ͹Εɼͦͷ͏ͪ\n͞ΕΔɽ\nशίʔύε͸ ͱ ɼ\nशίʔύε͸ Ͱ͋ΔɽͦΕͧΕ\n։͞Ε\nग़ͨ͠ɽ\nFigure 7: Changes in word prediction probabilities due\nto bias bLN removal on GPT-2 xl.\nCorpus frequency of word\nInner productsmall. hbLN, wii\nwi\nFigure 8: Relationship between the corpus word fre-\nquency and the inner product of bLN and each output\nword embedding wi in BERT base.\nCorpus frequency of word\nInner productsmall. hbLN, wii\nwi\nFigure 9: Relationship between the corpus word fre-\nquency and the inner product of bLN and each output\nword embedding wi in BERT large.\nCorpus frequency of word wi\nInner productsmall. hbLN, wii\nFigure 10: Relationship between the corpus word fre-\nquency and the inner product of bLN and each output\nword embedding wi in GPT-2 medium.\n4530\nCorpus frequency of word\nInner productsmall. hbLN, wii\nwi\nFigure 11: Relationship between the corpus word fre-\nquency and the inner product of bLN and each output\nword embedding wi in GPT-2 large.\nCorpus frequency of word\nInner productsmall. hbLN, wii\nwi\nFigure 12: Relationship between the corpus word fre-\nquency and the inner product of bLN and each output\nword embedding wi in GPT-2 xl.\nModel Spearman’s ρ\nBERT base 0.84\nlarge 0.74\nGPT-2\nsmall 0.78\nmedium 0.43\nlarge 0.61\nxl 0.70\nTable 3: The Spearman’s correlation coefficient between\nthe corpus word frequency and inner product ofbLN and\neach output word embedding wi.\nModel λ Diversity ↑ Quality\nD1 D2 D MAUVE ↑ PPL ↓\nsmall\n1 0.03 0.23 0.42 0.78 19.4\n0.9 0.03 0.27 0.45 0.82 19.8\n0.7 0.03 0.34 0.48 0.72 22.0\n0 0.02 0.12 0.13 0.01 65.9\nmed.\n1 0.03 0.27 0.46 0.89 14.6\n0.3 0.03 0.38 0.50 0.64 17.8\n0 0.03 0.33 0.46 0.22 21.3\nlarge\n1 0.03 0.26 0.44 0.89 12.7\n0.3 0.03 0.32 0.48 0.90 13.1\n0 0.03 0.34 0.50 0.87 13.6\nxl\n1 0.03 0.28 0.45 0.92 11.4\n0.5 0.03 0.32 0.48 0.92 11.6\n0 0.03 0.36 0.50 0.89 12.1\nTable 4: Evaluation results for GPT-2 (top-k sampling)\nwhile bias bLN was controlled with λ.\nModel λ Diversity ↑ Quality\nD1 D2 D MAUVE ↑ PPL ↓\nsmall\n1 0.07 0.49 0.59 0.50 19.4\n0.5 0.14 0.88 0.73 0.02 27.0\n0 0.12 0.71 0.61 0.01 65.9\nmed.\n1 0.09 0.56 0.63 0.33 14.6\n0.2 0.19 0.86 0.74 0.03 18.8\n0 0.21 0.86 0.74 0.02 21.3\nlarge\n1 0.06 0.44 0.56 0.77 12.7\n0.5 0.08 0.55 0.61 0.53 12.9\n0 0.11 0.69 0.67 0.22 13.6\nxl\n1 0.06 0.43 0.56 0.82 11.4\n0.5 0.08 0.54 0.61 0.61 11.6\n0 0.11 0.68 0.67 0.24 12.1\nTable 5: Evaluation results for GPT-2 (vanilla sampling)\nwhile bias bLN was controlled with λ.\n4531\nB Detailed experimental settings\nTo observe the TLM word prediction distribution\n(the main experiments in Section 3.1 and the mea-\nsurement of PPL in Section 3.3), we let BERT pre-\ndict words corresponding to [MASK] tokens, and\nwe let GPT-2 predict the second and subsequent\nwords in each sequence. If the length of an input se-\nquence was greater than the maximum input length\nkof the model, only the first kwords were used.\nTo evaluate the TLM text generation (Sec-\ntion 3.3), the first 10 words of each sequence\nwere fed into to GPT-2, and subsequent words\nwere generated until the length of the sequence\nreached 1,024 words or the end-of-sequence token\nwas generated. For GPT-2 small and medium, we\nvaried λ in increments of 0.1 to control the bias\nbLN. For GPT-2 large and xl, we first checked\nthe results for 100 samples and obtained the val-\nues with some kind of trends; we then varied λin\n{0,0.3,0.5,0.7,1.0}for the entire dataset, includ-\ning the values.\nWe experimented with three decoding strategies:\nvanilla sampling, top-k sampling, and top-p sam-\npling. In the top-k sampling, k was set to 50. In\nthe top-p sampling, pwas set to 0.9. Furthermore,\nbefore we evaluated the model-generated texts with\nthe N-gram based diversity metrics, we applied the\nword tokenizer provided by NLTK (Bird and Loper,\n2004).\nC Examples of generated text\nTable 6 shows examples of text generated by GPT-2\nsmall and large while controlling the bias bLN with\nλ.\n4532\nModel λ Generated text\nsmall\n1\nThere has been one product that I’ve wanted for a while — that is baseball’s fountain and. I wanted to try to\nget another product to make it as polished and simple to use and even easier to push the right buttons. Have\nyou played with some of the furniture brands of the past? Do you think the new smart building is going to...\n0.6\nThere has been one product that I’ve wanted for awhile: Asus ZenUI Keyboard Replacement Kit FAQ. I\npurchased this replacement keyboard replacement kit prior to 2014 when Asus shipped its ZenUI\n...\nBIOS Reset Warranty Long warranty EUR 3500 EUR 4550 EUR 470 EUR 520 EUR 590 EUR 560 EUR...\n0\nThere has been one product that I’ve wanted for awhile got released that alters baseball’s bench press. I\nmention Alejandro Nazarovski prior thus preferring Julian Whitaker however altering Alejandro\n...\ncombined with dumbbell movements :::::::combined::::with :::::::negatives::::ratios::::::::Improved :::::athlete::::::mobility::::::::Decreased\n:::::fatigue ::::::Diseases::::::Whilst :::::::adjusting :::lifts::::::::::::Underestimating:::::injury:::::::Potential::::::::Extensions:::::::::Suspension:::::Period...\nlarge\n1\nThe Atlanta Falcons have started the 2015 season 4-0. (Photo: Winslow Townson/ Associated Press) The\nFalcons’ longest streak of consecutive seasons with a winning record started on the same day in Week 11 that\nMike Shanahan and the Falcons experienced one of their most compelling victories of the season...\n0.5\nThe Atlanta Falcons have started the 2015 season 4-1, including a triumph over the New Orleans Saints at\nMercedes-Benz Stadium. Look at what this team could be capable of as the season progresses. It has the\ngoods, the direction, the talent to make a run at becoming a legitimate Super Bowl contender. More...\n0\nThe Atlanta Falcons have started the 2015 season 4-0, including a win over the Minnesota Vikings last\nSunday night. It’s been a perfect start to 2014 as well. Looking ahead, what’s the road ahead?\nWeek 1 @ Tampa Bay Buccaneers ...\nTable 6: Examples of text generated by GPT-2 small and large with top-p sampling while biasbLN was controlled\nwith λ. Proper nouns are in bold, repetitions of similar phrases are straight underlined, and ungrammatical passages\nare highlighted with ::::wavy:::::::::underlines. Note that the first 10 words are given to the model as context.\n4533\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section after Conclusions (Section 5).\n□\u0013 A2. Did you discuss any potential risks of your work?\nWe discussed the generalizability of our ﬁndings in Limitations section after Conclusions (Section 5).\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Introduction (Section 1) summarize our main claims.\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nWe used DeepL and Langsmith. DeepL is a machine translation tool, and Langsmith is a rephrasing\ntool trained on scientiﬁc text. We used them only to reﬁne the English of our submission. Neither\nservice is to copy the work of others nor make novel ideas or claims.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3. We used published pre-trained models and corpus.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe have already checked their licenses and the artifacts are enough popular not to need to discuss\ntheir license in the paper.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nIt is obvious that all the artifacts we used were created in the context of the research and we also\nused them for the research purpose.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSince we used the dataset for evaluating models’ workings, discussing about its speciﬁc content is\nnot so essential for our paper. In addition, the dataset we used is the training corpus of the models.\nIt is natural to use as is.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAll the artifacts we used are enough popular not to need to provide documentation.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4534\nC □\u0013 Did you run computational experiments?\nSection 3.\n□\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSince the purpose of our study is to analyze models’ inner workings, the details of computational\nenvironment are not necessary.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3 and Appendix B.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n4535",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7545676231384277
    },
    {
      "name": "Computer science",
      "score": 0.7420849204063416
    },
    {
      "name": "Language model",
      "score": 0.5716425776481628
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5213324427604675
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.4972729980945587
    },
    {
      "name": "Word (group theory)",
      "score": 0.46649977564811707
    },
    {
      "name": "Natural language processing",
      "score": 0.4553471505641937
    },
    {
      "name": "Speech recognition",
      "score": 0.4065607786178589
    },
    {
      "name": "Engineering",
      "score": 0.13181832432746887
    },
    {
      "name": "Linguistics",
      "score": 0.10024350881576538
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210110652",
      "name": "RIKEN",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ]
}