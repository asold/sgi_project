{
  "title": "A versatile CRISPR/Cas9 system off-target prediction tool using language model",
  "url": "https://openalex.org/W4411080326",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2541845690",
      "name": "Weian Du",
      "affiliations": [
        "Foshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2097656237",
      "name": "Liang Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5100031484",
      "name": "Kaichuan Diao",
      "affiliations": [
        "Shenzhen Nanshan Center for Chronic Disease Control"
      ]
    },
    {
      "id": "https://openalex.org/A2157715057",
      "name": "Yangyang Zheng",
      "affiliations": [
        "Ji Hua Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2538335500",
      "name": "Qianyong Yang",
      "affiliations": [
        "Jiujiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2120566560",
      "name": "Zhenzhen Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2439766046",
      "name": "Xiangxing Zhu",
      "affiliations": [
        "Foshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2124330778",
      "name": "Dongsheng Tang",
      "affiliations": [
        "Foshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2541845690",
      "name": "Weian Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097656237",
      "name": "Liang Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5100031484",
      "name": "Kaichuan Diao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157715057",
      "name": "Yangyang Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2538335500",
      "name": "Qianyong Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120566560",
      "name": "Zhenzhen Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2439766046",
      "name": "Xiangxing Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124330778",
      "name": "Dongsheng Tang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2910047861",
    "https://openalex.org/W2763042654",
    "https://openalex.org/W2895383514",
    "https://openalex.org/W2929656677",
    "https://openalex.org/W2099538751",
    "https://openalex.org/W2465403665",
    "https://openalex.org/W2408339903",
    "https://openalex.org/W2807714507",
    "https://openalex.org/W4307386237",
    "https://openalex.org/W2173737475",
    "https://openalex.org/W4313947391",
    "https://openalex.org/W3017057562",
    "https://openalex.org/W2085993152",
    "https://openalex.org/W2611661371",
    "https://openalex.org/W2900885456",
    "https://openalex.org/W2606138761",
    "https://openalex.org/W2003797386",
    "https://openalex.org/W2782551527",
    "https://openalex.org/W3111011311",
    "https://openalex.org/W2115603949",
    "https://openalex.org/W2945567049",
    "https://openalex.org/W2970173547",
    "https://openalex.org/W2150853746",
    "https://openalex.org/W2751498160",
    "https://openalex.org/W2898360779",
    "https://openalex.org/W2810756255",
    "https://openalex.org/W3026606344",
    "https://openalex.org/W3097637439",
    "https://openalex.org/W2611134270",
    "https://openalex.org/W4362665266",
    "https://openalex.org/W3035751258",
    "https://openalex.org/W2336346625",
    "https://openalex.org/W2900054591",
    "https://openalex.org/W2051255304",
    "https://openalex.org/W1997281938",
    "https://openalex.org/W4285088294",
    "https://openalex.org/W4404595545",
    "https://openalex.org/W3095979265",
    "https://openalex.org/W4366085048",
    "https://openalex.org/W3093632917",
    "https://openalex.org/W4366551165",
    "https://openalex.org/W2982510384",
    "https://openalex.org/W4205143015",
    "https://openalex.org/W4226172825",
    "https://openalex.org/W4382603228",
    "https://openalex.org/W4390488734",
    "https://openalex.org/W2891210592",
    "https://openalex.org/W3092629496",
    "https://openalex.org/W4210558208",
    "https://openalex.org/W4316507374",
    "https://openalex.org/W2973312556",
    "https://openalex.org/W3194399644",
    "https://openalex.org/W3182913057",
    "https://openalex.org/W2970098032",
    "https://openalex.org/W2902555008",
    "https://openalex.org/W6920915683",
    "https://openalex.org/W6930937058",
    "https://openalex.org/W2803557679",
    "https://openalex.org/W2224398381",
    "https://openalex.org/W2726053863",
    "https://openalex.org/W2070614858",
    "https://openalex.org/W2188223075"
  ],
  "abstract": null,
  "full_text": "communicationsbiology Article\nA Nature Portfolio journal\nhttps://doi.org/10.1038/s42003-025-08275-6\nA versatile CRISPR/Cas9 system off-\ntarget prediction tool using\nlanguage model\nCheck for updates\nWeian Du1, Liang Zhao 2 ,K a i c h u a nD i a o3,Y a n g y a n gZ h e n g4, Qianyong Yang5, Zhenzhen Zhu2,\nXiangxing Zhu 1,6 &D o n g s h e n gT a n g1,6\nGenome editing with the CRISPR/Cas9 system has revolutionized life and medical sciences,\nparticularly in treating monogenic genetic diseases by enabling long-term therapeutic effects from a\nsingle intervention. However, the CRISPR/Cas9 system can tolerate mismatches and DNA/RNA\nbulges at target sites, leading to unintended off-target effects that pose challenges for gene-editing\ntherapy development. Existing high-throughput detection and in silico prediction methods are often\nlimited to speciﬁcally designed single guide RNAs (sgRNAs) and perform poorly on unseen\nsequences. To address these limitations, we introduce CCLMoff, a deep learning framework for off-\ntarget prediction that incorporates a pretrained RNA language model from RNAcentral. CCLMoff\ncaptures mutual sequence information between sgRNAs and target sites and is trained on a\ncomprehensive, updated dataset. This approach enables accurate off-target identiﬁcation and strong\ngeneralization across diverse NGS-based detection datasets. Model interpretation reveals the\nbiological importance of the seed region, underscoring CCLMoff’s analytical capabilities. The\ndevelopment of CCLMoff lays the foundation for a comprehensive, end-to-end sgRNA design\nplatform, enhancing both the precision and efﬁciency of CRISPR/Cas9-based therapeutics. CCLMoff\nis a versatile tool and is publicly available atgithub.com/duwa2/CCLMoff.\nThe CRISPR/Cas9 systems have been used to investigate target genes in\ngenome modiﬁcation1, transcription2 and splicing3, and have been applied\nin various research settings to investigate and treat multiple genetic\ndiseases\n4,5, infectious diseases6, immunological diseases7, and cancers8.\nAmong the exciting advances, the translational use of CRISPR/Cas9 system\nin monogenic human genetic diseases has the potential to provide long-term\ntherapy after a single treatment9. However, extensive studies have also\ndemonstrated that multiple mismatches as well as DNA/RNA bulges can be\ntolerated, resulting in the cleavage of unintended genomic sites, termed off-\ntargets\n10. The potential off-target effect ofthe CRISPR/Cas9 system can lead\nto inadvertent gene-editing outcomes and become a bottleneck in the\ndevelopment of gene therapy\n9.\nGenerally, the off-target effect is hard to discover as a result of the\nextremely low editing rate. Several experimental approaches have been\ndeveloped to detect the off-target activity of CRISPR/Cas9 system. To\nprovide clarity, the experimental detection techniques are divided into three\nmajor categories: (i) detection of Cas9 binding, such as Extru-seq11, SELEX\nand its derivatives12; (ii) detection of Cas9-induced Double Strand Breaks\n(DSBs), such as in vitro techniques, Digenome-seq13,C I R C L E - s e q14 and\nin vivo approaches DISCOVER-seq15; (iii) detection of repair products\narising from Cas9-induced DSBs including IDLV16 and GUIDE-seq17.\nHowever, while various experimental detection approaches can validate the\ndeﬁned sgRNA off-target effects, theyfail to provide prior knowledge for\nsgRNA design. Computational methods address this limitation by utilizing\nthe comprehensive datasets generated by these NGS-based approaches to\nconstruct predictive models, which efﬁciently forecast sgRNA off-target\neffects and offer valuable guidance for sgRNA design\n18.\nRecently, a variety of in silico tools for off-target prediction have been\nproposed. Based on their underlying principles, these methods can be\ncategorized into four major groups\n19: (i) The alignment-based approach was\n1Gene Editing Technology Center of Guangdong Province, School of Medicine, Foshan University, Foshan, Guangdong, China.2Shenzhen Health Development\nResearch and Data Management Center, Shenzhen, Guangdong, China.3Shenzhen Center for Chronic Disease Control, Shenzhen, Guangdong, China.\n4Guangdong Homy Genetics Ltd, Foshan, Guangdong, China.5Jiujiang Key Laboratory of Rare Disease Research, Jiujiang University, Jiujiang, Jiangxi, China.\n6Guangdong Provincial Key Laboratory of Animal Molecular Design and Precise Breeding, Foshan University, Foshan, Guangdong, China.\ne-mail: zhaoj93@mail2.sysu.edu.cn; zhu_xiangxing@126.com; tangdsh@163.com\nCommunications Biology|           (2025) 8:882 1\n1234567890():,;\n1234567890():,;\nthe ﬁrst computational method to introduce mismatch pattern into off-\ntarget prediction, such as Cas-OFFinder20, CHOPCHOP21 and GT-Scan22.\nThese approaches employed different alignment methods to improve\ngenome-wide scanning efﬁciency. (ii) Formula-based methods such as\nCCTop\n23 and MIT24 assigned the different mismatch weights of PAM-distal\nregion and PAM-proximal region to aggregate the contribution of mis-\nmatch in different positions. (iii ) Energy-based methods, including\nCRISPRoff\n25, present an approximate binding energy model for the Cas9-\ngRNA-DNA chimeric complex. (iv) Learning-based methods, such as\nDeepCRISPR\n26 and CRISPR-Net27, can automatically extract the sequence\ninformation from training dataset to determine the genomic pattern of the\noff-target site. The deep learning-based methods exhibit superior perfor-\nmance and now serve as the state-of-the-art model in the off-target effect\nprediction\n28. However, existing deep learning-based models are often\ntrained on limited datasets containing a small number of sgRNAs and NGS-\nbased off-target detection data, which restricts their generalization ability\nand conﬁnes their applicability to speciﬁc detection approaches.\nTo address these limitations, we proposed a deep learning framework,\nnamely CCLMoff, which incorporatesthe RNA language model to extract\nthe sequence information and the genomic contexts. Besides, we compiled a\ncomprehensive dataset with 13 genome-wide off-target detection technol-\nogies, forcing CCLMoff to learn the general off-target pattern. Thus,\nCCLMoff demonstrated superior performance over the state-of-the-art\nmodel in various scenarios. The thorough evaluation showed that CCLMoff\naccurately identiﬁed off-target sites and displayed strong cross-dataset\ngeneralization ability. The model interpretation analysis indicated that\nCCLMoff successfully captured the seed region for off-target prediction. The\ndevelopment of CCLMoff paves the way for the establishment of a versatile\nand end-to-end in silico sgRNA design platform.\nMethods\nData source\nIn order to guide the construction of a universal and versatile model for off-\ntarget prediction, we ﬁrst curated a comprehensive off-target dataset\nencompassing a wide range of validated sgRNAs and diverse off-target\ndetection methods. We speciﬁcally excluded targeted site detection tech-\nniques such as targeted PCR, and focused on the genome-wide deep\nsequencing-based off-target detection approaches to ensure the model’s\ncapability to detect off-target siteso nag e n o m e - w i d es c a l e .I nt o t a l ,w e\nintegrated 13 genome-wide deep sequencing techniques from 21 publica-\ntions, categorized into three groups based on their detection methods: DNA\nbinding detection methods (Extru-seq\n11,S I T E - s e q29); the DSB detection\nmethods (CIRCLE-seq 14,D I S C O V E R - s e q15, DISCOVER-seq +30,\nCHANGE-seq31,B L E S S32); the repair product detection methods (GUIDE-\nseq17, Digenome-seq13,D I G - s e q33,I D L V34,H T G T S35 and SURRO-seq36).\nHowever, these studies only released validated off-target sites corresponding\nto the tested sgRNAs. During the model training process, negative off-target\nsites need to be externally constructed.\nTo generate an appropriate negative dataset, Cas-OFFinder20 was\nemployed for the negative sample construction, imposing constraints on the\nnumber of mismatches and bulges to ensure a representative distribution\nbetween off-target sites and mismatchcandidates. The negative dataset was\ndivided into two major categories based on whether the corresponding\npositive off-target sites contained bulges. As only recent studies\n27 account for\nbulge information, many earlier studies do not incorporate it. To ensure a\nfair comparison, we constructed two distinct negative datasets. For positive\nsamples with bulge information, Cas-OFFinder was conﬁgured with para-\nmeters allowing up to 6 mismatches and 1 bulge. For positive samples\nwithout bulge information, Cas-OFFinder was set to consider only up to six\nmismatches between the sgRNA and the target sites. When constructing\nnegative samples using Cas-OFFinder, CCLMoff was designed to identify\noff-target sites from mismatch candidates, effectively reducing the sampling\nspace and providing challenging samples to enhance the model’s ability to\ndistinguish off-target sites.\nModel construction\nTo address the off-target predictionproblem, which has two components:\nsgRNA and target site, we adopted a question-answering framework,\nwherein we formulated the problem as follows (Fig.1). The input to this\nframework consisted of two separate parts: the sgRNA sequence, which\nserved as the question stem, and the target site candidate, which acted as the\nanswer. The target site, being a DNA sequence, was transformed into\npseudo-RNA by substituting thymine (T) with uracil (U) when using a\nlanguage model pretrained on RNA. The primary objective was to ascertain\nFig. 1 | Overview of the pipeline for CCLMoff.The high-throughput off-target data\nis encoded as the sgRNA-target site pair and concatenated by a predeﬁned token\n[SEP]. The sgRNA-target site pairs go through the Input Embedding layer and feed\ninto 12 Transformer Blocks initialized by RNAcentral. The [CLS] of theﬁnal hidden\nlayer is employed for classiﬁcation using the multilayer perceptron to predict the\nsgRNA-target site pair score.\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 2\nwhether the sgRNA sequence could interact with the candidate pseudo-\nRNA sequence and result into off-target effect. Building upon this\nhypothesis, we developed a transformer-based language model to fulﬁll this\nclassiﬁcation task. Initially, we tokenized the sgRNA sequence and candi-\ndate pseudo-RNA sequence at the nucleotide level, using Input Embedding\nblock. To indicate their discontinuity, we introduced a special token [SEP] as\na delimiter to separate them. Subsequently, the input embeddings of the\nsgRNA and the pseudo-RNA candidate were input into an encoder, which is\ncomposed of 12 transformer blocks. These transformer blocks with a multi-\nhead attention module enabled effective information processing and con-\ntextual feature extraction between sgRNA and the target site.\nFor the off-target classiﬁcation task, we utilized theﬁnal hidden layer\nstate of the transformer encoder. Speciﬁcally, the [CLS] token from theﬁnal\nhidden layer was employed as the inputfor a Multilayer Perceptron (MLP),\nwhich was tasked with predicting the sgRNA-target site pair score. This\nMLP layer generated a score representing the likelihoodthat the candidate\npseudo-RNA sequence is an off-target site for the sgRNA. The scoring\nmechanism evaluated the compatibility and binding afﬁnity between the\nsgRNA and the candidate pseudo-RNA sequence. The encoder, consisting\nof 12 transformer blocks, was initialized using the RNA-FM model\n37,w h i c h\nhad been pretrained on 23 million RNA sequences from RNAcentral38.T h i s\npre-training approach ensured that the encoder had a robust understanding\nof RNA sequences, enhancing its abilityto capture relevant features during\nthe subsequent off-target prediction task.\nTo evaluate the impact of epigenetic information on model perfor-\nmance, we incorporated epigeneticdata obtained from DeepCRISPR. A\nconvolutional neural network (CNN)was used to encode four epigenetic\nchannels: CTCF binding information, H3K4me3 histone modiﬁcation,\nchromatin accessibility, and DNA methylation derived from reduced\nrepresentation bisulﬁte sequencing (RRBS). The resulting representation\nvector was then concatenated with theoutput of the language model and fed\ninto the MLP layer. This enhanced model is referred to as CCLMoff-Epi. In\naddition, we introduced a model without the pretrained language model,\nreferred to as CCLMoff-Vanilla, whichwas trained from scratch on the off-\ntarget dataset. This setup was designed to evaluate the impact of the pre-\ntrained language model RNA-FM on model performance.\nTraining process. To utilize the robust feature extraction ability from the\nRNA-FM foundation model, we set a small learning rate for the para-\nmeter of 12 Transformer Blocks. The CCLMoff is trained using a stan-\ndard binary cross-entropy (BCE) loss deﬁned as follows:\nBCE ¼/C0\n1\nN\nXN\ni¼0\nyi /C1 log ^yi\n/C0/C1\nþ 1 /C0 yi\n/C0/C1\n/C1 log 1 /C0 ^yi\n/C0/C1\nwhereyi represents the true label (off-target or not) for thei-th sample, and\n^yi denotes the predicted probabilitythat the candidate pseudo-RNA is an\noff-target for the corresponding sgRNA. This loss function was used to guide\nthe optimization of the model, ensuring that it effectively distinguished\nbetween off-target and non-off-target sequences. We employed the AdamW\noptimizer with a learning rate of 5 × 10\n−4 for the 12 Transformer encoder\nblockers parameters and 1 × 10−3 for the parameters of the MLP. A learning\nrate warm-up strategy was applied during theﬁrst 5 epochs. CCLMoff was\ntrained for 10 epochs using 8 NVIDIA A100 80G GPUs, with a batch size of\n128, and the total training time amounted to ~3 h. The deep learning\nbaseline models, including CRIS PR-Net, LSTM, CCLMoff-Epi and\nCCLMoff-Vanilla were trained using the consistent hyperparameters.\nDue to the dataset’s high level of imbalance, the bootstrapping sampling\nstrategy was applied to ensure an equal number of positive and negative\ns a m p l e si ne a c ht r a i n i n gb a t c h .\nInterpretation analysis\nAttention scores serve as a fundamental mechanism in attention-based\nmodels, particularly in language models, by dynamically determining the\nimportance of different elements within an input sequence. This mechanism\nallows the model to focus selectively on relevant parts of the data. By\nassigning varying weights to different tokens, attention scores enable the\nmodel to capture contextual relationships, long-range dependencies, and\nnuanced interactions between sgRNA and target site. This selective\nweighting not only enhances the model’s predictive capabilities but also\nprovides a window into its decision-making process, making it a corner-\nstone for both performance and interoperability. Intuitively, these scores\nreﬂect the contribution of each nucleotide to the model’s prediction, with\nhigher attention scores indicating a greater signiﬁcance of the nucleotide’s\nposition and composition in the sequence. The attention mechanism allows\nthe model to focus on speciﬁc parts of the input, helping it prioritize key\nfeatures relevant to off-target prediction. The attention score for each\nnucleotide is calculated as follows:\nAttention Score¼\nQ × klﬃﬃﬃ\nd\np\nwhereQ is the query vector, andkl is thelth column ofK. Note that the score\nfunctions presented in this section can be more efﬁciently calculated in\nmatrix form usingK instead of each column separately.\nStatistics and reproducibility\nAll computational experiments were performed with clearly deﬁned\ntraining and testing procedures. Formodel training, we used a benchmark\ndataset composed of 418 sgRNAs and 82,699 validated off-target sites, along\nwith 9,521,638 negative samples generated via Cas-OFFinder. To address\nthe data imbalance (positive-to-negative ratios ranging from 1:26 to 1:4189),\nwe applied a bootstrapping sampling strategy that ensured an equal number\nof positive and negative samples in each training batch. Model performance\nwas evaluated using standard metrics including balanced accuracy, F1-\nscore, area under the Receiver Operating Characteristic curve (AUROC),\nand area under the precision–recall curve (AUPRC). Five-fold cross-vali-\ndation was performed to assess model robustness, and all reported results\nrepresent the mean and standard deviation acrossﬁve repeated runs with\ndifferent random seeds. For statistical comparisons, two-sided Student’s t\ntests were conducted to assess performance differences between CCLMoff\nand baseline models, withP values <0.05 considered signiﬁcant. All com-\nparisons were carried out using identical training settings, data partitions,\nand evaluation metrics to ensurefairness and reproducibility.\nReporting summary\nFurther information on research designis available in the Nature Portfolio\nReporting Summary linked to this article.\nResults\nBenchmark dataset construction\nIn this study, we aimed to create a comprehensive off-target dataset to\nfacilitate the development of a universal model for off-target prediction. To\nensure the dataset’s suitability for genome-wide off-target detection, we\nexcluded targeted site detection methods, such as targeted PCR, and con-\ncentrated on genome-wide and deep sequencing-based off-target detection\napproaches. Thus, we incorporated 13 genome-wide deep sequencing\ntechniques, including GUIDE-seq 17,C I R C L E - s e q14,S I T E - s e q29,\nDISCOVER-seq15,D I S C O V E R - s e q+30, CHANGE-seq31, Digenome-seq13,\nDIG-seq33, HTGTS35,I D L V34,B L E S S32,E x t r u - s e q11, and SURRO-seq36 to\nconstruct a comprehensive off-target dataset categorized into three groups\nbased on the underlying sequencing mechanisms. A bunch of in silico model\nstudies have curated datasets for off-target prediction, yet these datasets face\nsigniﬁcant limitations, such as covering a small number of sgRNAs or\nemploying inconsistent criteria for constructing negative samples. For\ninstance, the dataset utilized by CRISPR-Net\n27 contained only 145 sgRNAs\nand relied onﬁve deep sequencing-based off-target detection methods. For\nnegative sample construction, CRISPR-Net employed Cas-OFFinder with a\nparameter setting of 6 mismatches and 1 bulge. Similarly, CrisprDNT39\ncompiled datasets from multiplestudies, comprising 149 sgRNAs,ﬁve deep\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 3\nsequencing approaches, and one PCR-based method, which is a targeted off-\ntarget detection technique. CrisprDNT also utilized Cas-OFFinder but with\ndifferent parameters, permitting six mismatches without bulge information.\nThis highlights the urgent need for researchers to establish a standardized\nbenchmark dataset to unify and address the off-target site prediction\nchallenge.\nFor the benchmark dataset, we collected the positive off-target sites\nfrom the 21 studies, including both the original publications and their\napplication studies (Table1). These studies only provided validated off-\ntarget sites for the tested sgRNAs, requiring the negative off-target sites to be\nexternally constructed during the model training process. For the con-\nstruction of the negative dataset, wecategorized the data into two groups\nbased on whether the positive off-target sites contained bulge information.\nFor positive samples with bulge information, we set the Cas-OFFinder\nparameters to allow six mismatches and one bulge. For positive samples\nwithout bulge information, Cas-OFFinder was conﬁgured to consider up to\nsix mismatches between the sgRNA and target sites.\nIn developing a universal model, ourdataset includes two species and\nfour reference genomes. Furthermore, uncanonical-length sgRNAs were\nincorporated to enhance the dataset’s breadth and applicability. Overall, we\nconstructed a benchmark dataset integrating 13 off-target detection tech-\nnologies from 21 studies, making it the most comprehensive dataset to date,\nwith 418 sgRNAs and 82,699 validated off-target sites. The negative dataset,\ngenerated using Cas-OFFinder with constrained parameters, consisted of a\ntotal of 9,521,638 negative samples. Given the highly imbalanced nature of\nthe dataset, with imbalance ratios ranging from 26:1 to 4189:1, addressing\nthis imbalance was a critical focusduring the model construction and\ntraining process. In comparison to the recently published database\nCrisprSQL\n40, which contains cleavage data from only 144 guide RNAs and\n25,632 guide-target pairs, our dataset is signiﬁcantly more comprehensive.\nMoreover, CrisprSQL only includes positive target sites and lacks negative\nsample information, making it insufﬁcient for training predictive models.\nFurthermore, Sherkatghanad et al.41 provide a comprehensive review of\nCRISPR/Cas-related computational challenges, including a table summar-\nizing six off-target studies. This covers datasets such as GUIDE-seq and\nCHANGE-seq, which represent only a small fraction of the datasets\nincorporated into our benchmark. Consequently, our benchmark dataset\noffers a more complete resource for off-target prediction, addressing both\npositive and negative off-target sites to support future model development.\nLanguage model improve the off-target prediction\nTo evaluate the capability of the pretrained language model framework, we\nadopted a rigorous training process identical to that of the baseline models.\nBoth CCLMoff and the baseline model were trained on the same dataset to\nfacilitate a direct performance comparison and to assess the effectiveness of\nCCLMoff framework relative to the baseline model. Thus, we conducted a\nthorough evaluation of CCLMoff on the CIRCLE-seq dataset and found that\nit exhibited superior performance in identifying off-target sites compared to\nSOTA models, including CRISPR-Net, LSTM and CCTop. Besides, we also\nincorporated two variants of CCLMoff called CCLMoff-Epi and CCLMoff-\nVanilla, to investigate the impact of incorporating epigenetic information\nand pretraining process on massive RNA sequence datasets (Fig.2). For a\nfair comparison, we only included the models capable of accounting for\nbulge information in this section. The results (Table2) demonstrated the\nsuperior performance of the pretrained language model across various\nmetrics, including balanced accuracy, F1-score, AUROC and AUPRC.\nCCLMoff outperformed the state-of-the-art models, achieving a balanced\naccuracy of 0.998 (± 0.001), an F1-score of 0.409 (± 0.003), an AUROC of\nTable 1 | Summary of the comprehensive dataset construction with technique categorization based on the off-target detection\nmechanism\nID Technique Category Total Pos + Neg Target site Imbal ratioPosþNeg\nPos sgRNA Genome Bulge sgRNA length Ref.\n1 CIRCLE-seq DSB detection 1,683,395 5796 290 11 hg19 Yes 19, 20 14\n2 GUIDE-seq Repair Product Detection 1,599,541 414 3864 10 hg19 Yes 20 17\n3 GUIDE-seq Repair Product Detection 195,186 414 471 10 hg19 No 20 17\n4 DISCOVER-seq DSB Detection 104,971 31 3386 4 hg19 No 20 15\n5 DISCOVER-seq + DSB Detection 105,038 98 1072 4 hg19 No 20 30\n6 DISCOVER-seq DSB Detection 26,757 49 546 1 mm10 No 20 15\n7 DISCOVER-seq + DSB Detection 26,805 98 274 1 mm10 No 20 30\n8 CHANGE-seq DSB detection 1,880,364 71,254 26 110 hg19 No 20 31\n9 GUIDE-seq Repair Product Detection 804,809 1702 473 58 hg19 No 20 31\n10 Extru-seq DNA Binding 205,375 94 2185 5 hg19 No 19 11\n11 Extru-seq DNA Binding 137,936 56 2463 2 mm10 No 19 11\n12 SURRO-seq Repair Product Detection 1,263,524 863 1464 110 hg19 No 20 36\n13 SITE-seq DNA Binding 102,691 89 1154 8 hg38 No 20 54\n14 Digenome-seq Repair Product Detection 25,796 162 159 2 hg19 Yes 20 13\n15 Digenome-seq Repair Product Detection 195,031 258 756 10 hg19 No 20 13\n16 DIG-seq Repair Product Detection 91,297 141 647 8 hg38 No 20 33\n17 GUIDE-seq Repair Product Detection 385,759 426 906 31 mm9 No 20, 21 58\n18 GUIDE-seq Repair Product Detection 84,703 61 1389 7 hg19 Yes 20 59\n19 GUIDE-seq Repair Product Detection 162,136 272 596 9 hg19 No 20 18\n20 GUIDE-seq Repair Product Detection 115,465 203 569 5 hg19 No 20 60\n21 HTGTS Repair Product Detection 49,178 87 565 3 hg19 No 20 35\n22 IDLV Repair Product Detection 54,459 13 4189 2 hg19 No 20 34\n23 BLESS DSB Detection 73,807 31 2381 2 hg19 No 20 61\n24 BLESS DSB Detection 113,291 53 2138 3 hg19 No 19, 20 61\n25 BLESS DSB Detection 34,324 34 1010 2 mm9 No 19, 20 62\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 4\n0.985 (± 0.001), and an AUPRC of 0.524 (± 0.004). Notably, we observed\nthat the simpliﬁed LSTM version of CRISPR-Netoutperformed the original\nversion (CRISPR-Net) in terms of balanced accuracy, AUROC, and\nAUPRC. This suggests that the RNN framework may be more effective in\nprocessing raw sequence data. The CCLMoff-Epi obtained a balanced\naccuracy of 0.998 (± 0.001), an F1-score of 0.429 (± 0.005), an AUROC of\n0.989 (± 0.001), and an AUPRC of 0.513 (± 0.004). However, the addition of\nfour channels of epigenetic information, including DNase, CTCF,\nH 3 K 4 m e 3 ,a n dR R B S ,d i dn o tr e s u l ti ns i g n iﬁcant improvement. We\nhypothesize that the pretrained language model inherently captures\nepigenetic and genomic context information during training, rendering the\nextra epigenetic channels unnecessaryfor further enhancing off-target\nprediction performance. Notably, CCLMoff-Vanilla, trained from scratch\nwithout a pretraining process on a large-scale dataset, still achieved con-\nsiderable performance, which we attribute to the strength of its transformer-\nbased framework. The detailed AUROC and AUPRCﬁgures for each\nsgRNA were shown in Supplementary Figs. S1 and S2. At test further\nconﬁrmed that CCLMoff signiﬁcantly outperformed CRISPR-Net, LSTM,\nand CCTop across all metrics. Theseﬁndings indicate that the language\nmodel framework effectively captures mutual information between the\nsgRNA and target site, resulting in superior performance in off-target\nprediction.\nLanguage model exhibits robust cross-dataset generalization\nability\nTo comprehensively evaluate the generalization ability of the pretrained\nlanguage model in CCLMoff, we trained the model on a speciﬁc dataset and\ne v a l u a t e di to na ne x t e r n a ld a t a s e tf r o ma different experimental category.\nSpeciﬁcally, we trained CCLMoff on the CIRCLE-seq dataset and validated\nits performance on GUIDE-seq, which utilizes a distinct sequencing\nmechanism. The results demonstrated that CCLMoff exhibits robust cross-\ndataset generalization, signiﬁcantly outperforming existing state-of-the-art\nmodels such as AttenToCrispr\n42,C R I S P R - N e t27,a n dL S T M .N o t a b l y ,\nCCLMoff achieved an AUROC of 0.996 and an AUPRC of 0.520 on the\nGUIDE-seq dataset— substantially higher than the AUPRC of 0.210\nattained by CRISPR-Net.\nTo further contextualize theseﬁndings, we compared CCLMoff with\nseveral recent and high-performing methods. Theﬁrst is CRISPR-IP\n43,a\ngraph-based model that incorporatesmismatch position encoding. Trained\non CIRCLE-seq and tested on GUIDE-seq, CRISPR-IP achieved an\nAUROC of 0.945 and an AUPRC of 0.337. The second is MOFF\n44, a random\nforest model that integrates chromatin accessibility and sequence features,\nwhich achieved an AUROC of 0.876 and an AUPRC of 0.282. The third is\nCRISPR-DNT\n39, a dinucleotide-enhanced neural network model that\nobtained an AUROC of 0.977 and an AUPRC of 0.381 under the same\nevaluation setting. While these recent models demonstrate competitive\nperformance, CCLMoff consistently outperforms them across evaluation\nmetrics. We further benchmarked CCLMoff against two recent language\nmodel-based approaches to evaluate the impact of pretraining modality.\nFig. 2 | Cross-dataset evaluation on the GUIDE-seq dataset.The models were\ntrained on the CIRCLE-seq dataset and externally validated on GUIDE-seq, which\nuses a different experimental platform. Left: ROC curves showing the AUROC\nperformance of all models. Right: Precision–recall curves showing AUPRC per-\nformance, which is particularly important in imbalanced datasets. The RNA-\npretrained model CCLMoff achieved the best performance (AUROC = 0.996,\nAUPRC = 0.520), signiﬁcantly outperforming baseline models including LSTM,\nCRISPR-Net, and AttenToCrispr. In addition, we evaluated three recent language\nmodel-based approaches: CCLMoff-Hyena (DNA-pretrained), CRISPR-BERT\n(task-speciﬁc pretraining), and CRISPR-DNT (Transformer-based). Both CRISPR-\nBERT and CCLMoff-Hyena demonstrated lower AUPRC than CCLMoff, high-\nlighting the advantage of RNA-speciﬁc foundation model pretraining in capturing\nsgRNA–DNA interactions.\nTable 2 | Cross-validation results on the CIRCLE-seq dataset\nModel Bal Acc F1-score AUROC AUPRC\nCCLMoff 0.998 ± 0.001 0.409 ± 0.003 0.985 ± 0.001 0.524 ± 0.004\nLSTM 0.843 ± 0.002 0.052 ± 0.001 0.926 ± 0.001 0.479 ± 0.003\nCRISPR-\nNet\n0.806 ± 0.002 0.083 ± 0.001 0.915 ± 0.003 0.462 ± 0.007\nCCTop 0.887 ± 0.004 0.003 ± 0.001 0.711 ± 0.004 0.008 ± 0.001\nCCLMoff-\nEpi\n0.998 ± 0.001 0.429 ± 0.005 0.989 ± 0.001 0.513 ± 0.004\nCCLMoff-\nVan\n0.836 ± 0.001 0.053 ± 0.001 0.901 ± 0.003 0.422 ± 0.005\nCCLMoff\nv.s.\nLSTM 6 × 10\n−12 3×1 0−12 9×1 0−9 4×1 0−11\nCRISPR-\nNet\n1×1 0−9 8×1 0−7 1×1 0−5 7×1 0−6\nCCTop 1 × 10 −8 1×1 0−23 1×1 0−13 1×1 0−18\nCCLMoff-\nEpi\n0.82 0.53 0.09 0.12\nCCLMoff-\nVan\n1×1 0−4 2×1 0−13 6×1 0−4 5×1 0−6\nBal AccBalanced Accuracy, used for evaluating imbalanced datasets,CCLMoff-Van CCLMoff-\nVanilla.\nA t test was conducted to assess the statistical signiﬁcance of performance differences between\nCCLMoff and baseline models.\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 5\nThe ﬁrst is CCLMoff-Hyena, a variant of our model that incorporates the\ngenomic DNA foundation model HyenaDNA45.T h i sm o d e lw a st r a i n e do n\nCIRCLE-seq and tested on GUIDE-seq, achieving an AUROC of 0.904 and\nan AUPRC of 0.180. The second is CRISPR-BERT\n46, a task-speciﬁcB E R T\nmodel trained from scratch on sgRNA-target site pairs without large-scale\nDNA or RNA pretraining. Under the same protocol, CRISPR-BERT\nachieved an AUROC of 0.856 and an AUPRC of 0.150. These results\nunderscore the effectiveness of RNA-speciﬁc pretraining for modeling\nFig. 3 | The model performance on external validation.On DIG-seq dataset,\nCCLMoff achieved superior performance (AUROC=0.985 and AUPRC=0.720)\nthan the SOTA model, indicating that CCLMoff can successfully capture off-target\npattern revealed by DIG-seq. In DISCOVER-seq and DISCOVER-seq+ dataset,\nCCLMoff exhibited superior performance in AUPRC (AUPRC=0.661) and con-\nsiderable performance in AUROC (AUROC=0.944), indicating that CCLMoff have\nsufﬁcient capacity in recalling the potential off-target sites. In GUIDE-seq dataset,\nCCLMoff exhibited limited performance (AUPRC=0.810, AUROC=0.279), due to\nthe baseline model was directly trained on the dataset of GUIDE-seq, indicating that\nthe existing model intend to be an approach-speciﬁc model instead of general off-\ntarget site prediction model.\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 6\nCRISPR-Cas9 off-target effects. Compared to CRISPR-BERT, which suffers\nfrom limited generalization due to its narrow training domain, and\nCCLMoff-Hyena, which is pretrained on genomic DNA rather than RNA,\nCCLMoff leverages large-scale RNA-based pretraining to better capture the\nbiological properties of sgRNA and its interactions with DNA target sites.\nThis is particularly important given that sgRNA is an RNA molecule, and\nmodeling it in the correct modality appears to be beneﬁcial for downstream\nprediction tasks. Theseﬁndings demonstrate that CCLMoff captures shared\noff-target sequence patterns across datasets and technologies, enabling it to\ngeneralize across cell types, sequencing protocols, and experimental con-\nditions. Furthermore, these results highlight the potential of pretrained\nlanguage models in developing scalable and robust tools for genome editing,\nand lay a strong foundation for future extensions that integrate diverse data\nsources to further enhance off-target prediction.\nCCLMoff can accurately predict off-target sites\nThe pretrained language model’s robust generalization ability prompted us\nto train the full version of CCLMoff on a comprehensive dataset to learn the\ngeneral patterns of off-target effectoccurrence. We employed a leave-one-\ndataset-out evaluation strategy to assess the model’s performance on the\nlatest datasets: DIG-seq, GUIDE-seq, DISCOVER-seq, and DISCOVER-\nseq+. As these datasets are bulge-free, we included additional mismatch-\nonly models, such as AttenToCrispr\n42,C N N _ s t d47,a n dC R I S P R o f f48 for\nperformance comparison (Fig.3). These models were evaluated using the\nsame parameters as speciﬁed in their original papers and implementations\navailable on GitHub (ref“Code availability”).\nCCLMoff achieved outstanding performance across all evaluated\ndatasets. In the DIG-seq dataset, CCLMoff demonstrated superior results,\nwith an AUROC of 0.985 and an AUPRCof 0.720, outperforming state-of-\nthe-art models and showcasing its ability to capture off-target patterns\nrevealed by DIG-seq. In the DISCOVER-seq and DISCOVER-seq+ data-\nsets, CCLMoff also exhibited superior performance in terms of AUPRC\n(0.665) and solid performance in AUROC (0.944), demonstrating its\neffectiveness in recalling potential off-target sites. However, in the GUIDE-\nseq dataset, CCLMoff showed more considerable performance (AUPRC =\n0.810, AUROC = 0.279), while the state-of-the-art models (e.g., CRISPR-\nNet and CNNSTD) used for comparison were directly trained on the\nGUIDE-seq dataset. CCLMoff used the leave-one-dataset-out strategy and\nwas unable to assess GUIDE-seq dataset during the training process. This\nsuggests that these baseline models may be more approach-speciﬁcr a t h e r\nthan providing general off-target site prediction capabilities. Overall, these\nresults highlight that CCLMoff trained on the comprehensive dataset can\naccurately predict off-target sites and capture general off-target occurrence\npatterns, making it a valuable tool for general off-target prediction across\ndiverse datasets and technologies.\nCCLMoff achieved considerable performance on uncanonical\nlength sgRNA\nOne of the notable advantages of the language model is its ability to handle\nvariable-length inputs. This feature is particularly valuable in the context of\nsgRNA design, where several sgRNAswith non-canonical lengths are\nengineered to optimize cutting sites and on-target efﬁciency. Despite the\nneed for such sgRNAs with uncanonical length, no existing in silico model\ncan predict off-target effects for these uncanonical length sgRNAs. To\naddress this gap, we trained CCLMoff on a dataset with sgRNAs of length 20\nand evaluated its performance on datasets with sgRNAs of lengths 19 and\n21. CCLMoff achieved an AUROC of 0.8123, demonstrating a considerable\ncapability for off-target prediction (Fig.4). This result reveals that CCLMoff,\nwhen trained on a 20nt sgRNA dataset, can also effectively depict the off-\ntarget landscape for sgRNAs of different lengths. Furthermore, the full\nversion of CCLMoff, which will incorporate a comprehensive dataset\nencompassing various sgRNA lengths,is expected to offer even more robust\ngeneralization capabilit i e s .B yt r a i n i n go ns u c hd i v e r s ed a t a ,C C L M o f fa i m s\nto develop a more universal model for off-target predictions, capable of\naccurately predicting off-target effects across a wider range of sgRNA\nlengths. This advancement will signiﬁcantly enhance theﬂexibility and\napplicability of the model in genomic research, ensuring that it can be\neffectively used for designing sgRNAs with non-canonical lengths without\nsacriﬁcing prediction accuracy. Thus, CCLMoff not only addresses current\nlimitations but also sets the stage for more versatile and reliable off-target\nprediction tools in the future.\nCCLMoff reveals the PAM-near region motif for off-target\nprediction\nThe attention map (Fig.5) derived from CCLMoff reveals that the model\nplaces greater emphasis on the PAM-proximal region (positions 16–20),\nw h i c ha l i g n sw i t ht h es e e dr e g i o ni d e n t iﬁed in previous studies26.T h i s\nconsistency with established research highlights the accuracy and reliability\nof CCLMoff in capturing critical elements of the off-target occurrence\nmechanism. Interestingly, a similar pattern is observed on the target site,\nwith a slight positional shift to positions 14–18. Theseﬁndings suggest that\nCCLMoff effectively captures the off-target occurrence mechanism,\ndemonstrating its capability to identify and emphasize key genomic regions\ninvolved in off-target effects. The model’s alignment with known biological\nmechanisms further validates its effectiveness and potential for accurate off-\ntarget prediction in CRISPR applications.\nDiscussion\nThe development of genome editing technologies has opened new avenues\nfor disease treatment, but the potential off-target effects, particularly in the\nwidely used CRISPR/Cas9 system, remain a signiﬁcant concern. Current\noff-target detection methods rely on high-throughput sequencing, which is\nboth expensive and time-consuming, and are often limited in providing\nprior knowledge for effective sgRNA design. To overcome these limitations,\nseveral in silico models have been developed for off-target detection; how-\never, many models still struggle with generalization ability. In this study, we\npresent a deep learning framework forCRISPR/Cas9 off-target prediction,\nnamely CCLMoff. Built on the most comprehensive dataset and pretrained\nlanguage models to date, CCLMoff employs a two-step cascade strategy: off-\ntarget searching and off-target scoring. Theﬁrst step identiﬁes as many\nmismatch-based off-target candidates as possible, while the second step\nscores these candidates to determine which mismatches are tolerable for the\nCRISPR/Cas9 system. CCLMoff outperforms state-of-the-art models across\nvarious scenarios, including both cross-validation and external validation,\ndemonstrating its ability to accuratelyidentify off-target sites and capture\nFig. 4 | The CCLMoff performance on uncanonical length sgRNA (len= 19, 21).\nDespite being trained solely on canonical 20- nt sgRNAs, CCLMoff achieved an\nAUROC of 0.81 on this unseen dataset, highlighting its strong generalization ability.\nThis result underscores the advantage of the underlying language model in handling\nvariable-length inputs, which is crucial for real-world sgRNA design where non-\ncanonical lengths are frequently employed to optimize CRISPR targeting efﬁciency.\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 7\ngeneral off-target occurrence patterns. The current version of CCLMoff is a\nsequence-only model, making it user-friendly. Besides, CCLMoff achieved a\ncomparable performance with the CCLMoff-Epi with the extra channel of\nfour epigenetic information. Thus, CCLMoff lays the groundwork for\noptimizing sgRNA design and has thepotential to accelerate advancements\nin genome editing technologies for therapeutic applications.\nIn this study, we demonstrate that increasing the amount of data\nenhances the model’s ability to extract genomic patterns captured by the\nCas9 protein. To this end, we compileda comprehensive dataset utilizing\nvarious deep sequencing methods from multiple sources. Although we\nassembled the most extensive dataset available for constructing the\nCCLMoff model, it still falls short of fully leveraging the potential of a\nlanguage model. To further improve the model’s capabilities, efforts such as\nincorporating off-target datasets from other Cas proteins, like Cas12 and\nCas13, could reveal similar off-target patterns that would enhance model\ndevelopment. In addition, integrating on-target efﬁciency datasets could\nprovide valuable sgRNA-related information, enabling more effective fea-\nture extraction. Overall, the incorporation of diverse datasets related to the\nCRISPR/Cas system into the language model could lay the foundation for a\nversatile and robust model for the CRISPR/Cas system, capable of sup-\nporting a broad range of genome editing applications.\nExisting in silico models primarily focus on identifying the contribu-\ntion of mismatches to off-target effects, relying solely on sequence infor-\nmation for model construction. However, the CRISPR/Cas9 system involves\na protein-nucleic acid interaction,with two key components: the sgRNA\nand the target sites. Recent research\n49 has shown that the secondary struc-\nture of sgRNA plays a crucial role in enhancing CRISPR/Cas9 cleavage\nefﬁciency by introducing a structural lock into the hairpin structure. This\nﬁnding suggests that incorporatingstructural information into model\nconstruction could further capture the true interaction dynamics of the\nsgRNA-target site duplex for off-target prediction, potentially improving\nmodel performance and providing deeper insights into the mechanisms of\noff-target occurrences. Moreover, several studies\n26,40 have highlighted the\nimportance of epigenetic modiﬁcations and chromatin status at the target\nsite in predicting off-target effects. DeepCRISPR, for instance, employs\nadditional channels to encode epigenetic information, including DNase,\nCTCF, H3K4me3, and RRBS. Similarly, CrisprSQL integrates epigenetic\ndata such as CTCF, DNase, H3K4me3, RRBS, and DRIP, providing cell line-\nspeciﬁc annotations that enhance the precision of off-target effect predic-\ntion. In addition, energy features generated by computational models can be\nincorporated as extra inputs to represent the mutual information between\nthe sgRNA and the target site\n50. By incorporating structural information,\nepigenetic annotations, and energy features, future models can provide a\nmore comprehensive and explicit framework for off-target prediction,\nimproving both performance and understanding of the underlying\nmechanisms.\nThe current off-target prediction methods have been primarily utilized\nfor two key purposes. Theﬁrst is to evaluate the activity levels of a speciﬁc\nsgRNA on off-target regions, where CCLMoff has demonstrated strong\nperformance. The second purpose is to assess the off-target effects of\ndesigned sgRNAs in advance. Previous models, such as Elevation-\nAggregate\n18 and CRISPR-Net-Aggregate27, follow an aggregate strategy,\nwhere the potential sgRNA-target site pairs are summed into an overall off-\ntarget score for a designed sgRNA. In the future, CCLMoff could introduce a\nCCLMoff-Aggregate version that adopts a similar approach, enabling more\neffective evaluation of proposed sgRNAs. Predicting on-target efﬁciency is\nan important aspect of the CRISPR/Cas9 system, and recent studies have\nshown that deep learning frameworks such as DeepHF\n51 and CRISPRon52\nhave great potential for on-target efﬁciency prediction. However, these\nmodels have not yet utilized language models for this purpose. With the\nemergence of high-throughput sequencing-based on-target efﬁciency\ndetermination approaches, comprehensive on-target efﬁciency datasets can\nbe built to enhance in silico sgRNA efﬁciency prediction. The current ver-\nsion of CCLMoff is equipped with a pretrained language model on a\ncomprehensive sgRNA-target site pair dataset, which suggests that the\nmodel can also be applied to on-target efﬁciency prediction through transfer\nlearning. Fine-tuning a CRISPR-based language model in this way may\nsigniﬁcantly improve sgRNA on-target efﬁciency prediction. Moreover, the\nlanguage model has demonstrated its capability in multi-modality appli-\ncations, which means that it can directlyincorporate structural information\nabout sgRNA, such as secondary structure and chromatin status at the\ncleavage site.\nRecent studies have demonstrated that the editing outcome of a speciﬁc\nsgRNA is reproducible, suggesting that the editing outcome of a speciﬁc\nsgRNA can be predicted. Several models have been proposed for cleavage\noutcome prediction, such as CROTON\n53,L i n d e l54, and Forecast55,a l lo f\nwhich have shown promising performance. In future versions of CCLMoff,\nwe plan to include a function for cleavage outcome prediction to provide a\nmore comprehensive evaluation of sgRNA performance. To achieve this\ngoal, we plan to integrate CCLMoff into the existing sgRNA design platform\nFig. 5 | Model interpretation analysis for CCLMoff.The attention map of multi-head attention (layer=0, head=9 and layer=0, head=12) extracted from CCLMoff reveal\nthat CCLMoff lay higher weight in the position 16–20 on sgRNA and position 14–18 on off-target site.\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 8\nto provide a comprehensive evaluation of sgRNA on on-target efﬁciency,\noff-target effect, and cleavage outcome. By incorporating such a prediction\ntool, the researchers can better evaluate the potential outcomes of a\nspeciﬁc sgRNA and design the most appropriate sgRNA for their\nresearch purposes. Moreover, we aim to develop a user-friendly interface\nfor the platform, which enables users to input the target gene information\nand obtain the optimal sgRNA sequences based on the comprehensive\nevaluation. With such a platform, researchers can save a signiﬁcant amount\nof time and resources and accelerate their research in the ﬁeld of\ngenome editing.\nData availability\nThe comprehensive curated dataset is available on Figshare56 with the\nhttps://doi.org/10.6084/m9.ﬁgshare.27080566.v2. All other data supporting\nthe ﬁndings of this study are available from the corresponding author upon\nreasonable request.\nCode availability\nThe source code for CCLMoff is available athttps://github.com/duwa2/\nCCLMoffand Zenodo57. The model of AttenToCrispr is available athttps://\ngithub.com/qiaoliuhub/AttnToCrispr. The CNN_std is available athttps://\ngithub.com/MichaelLinn/off_target_prediction. CRISPR-Net is available at\nhttps://github.com/JasonLinjc/CRISPR-Net. CRISPRoff is available at\nhttps://github.com/RTH-tools/crisproff. CCTop is available at https://\ncctop.cos.uni-heidelberg.de/.\nReceived: 21 September 2024; Accepted: 22 May 2025;\nReferences\n1. Oakes, B. L. et al. CRISPR-Cas9 circular permutants as\nprogrammable scaffolds for genome modiﬁcation. Cell 176, 254– 267\n(2019).\n2. Abudayyeh, O. O. et al. RNA targeting with CRISPR– Cas13. Nature\n550, 280– 284 (2017).\n3. Yuan, J. et al. Genetic modulation of RNA splicing with a CRISPR-\nguided cytidine deaminase.Mol. Cell72, 380– 394 (2018).\n4. Papasavva, P., Kleanthous, M. & Lederer, C. W. Rare opportunities:\nCRISPR/Cas-based therapy development for rare genetic diseases.\nMol. Diagn. Ther.23, 201– 222 (2019).\n5. Ousterout, D. G. et al. Reading frame correction by targeted genome\nediting restores dystrophin expression in cells from Duchenne\nmuscular dystrophy patients.Mol. Ther.21, 1718– 1726 (2013).\n6. Kennedy, E. M. & Cullen, B. R. Gene editing: a new tool for viral\ndisease. Annu. Rev. Med.68, 401– 411 (2017).\n7. Xiong, X., Chen, M., Lim, W. A., Zhao, D. & Qi, L. S. CRISPR/Cas9 for\nhuman genome engineering and disease research.Annu. Rev.\nGenomics Hum. Genet.17, 131– 154 (2016).\n8. Huang, C.-H., Lee, K.-C. & Doudna, J. A. Applications of CRISPR-Cas\nenzymes in cancer therapeutics and detection.Trends Cancer4,\n499– 512 (2018).\n9. Chavez, M., Chen, X., Finn, P. B. & Qi, L. S. Advances in CRISPR\ntherapeutics. Nat. Rev. Nephrol.19,9 – 22 (2023).\n10. Zhang, X.-H., Tee, L. Y., Wang, X.-G., Huang, Q.-S. & Yang, S.-H. Off-\ntarget effects in CRISPR/Cas9-mediated genome engineering.Mol.\nTher. Nucleic Acids4, e264 (2015).\n11. Kwon, J. et al. Extru-seq: a method for predicting genome-wide Cas9\noff-target sites with advantages of both cell-based and in vitro\napproaches. Genome Biol.24,1 – 20 (2023).\n12. Zhang, L. et al. Systematic in vitro proﬁling of off-target afﬁnity,\ncleavage and efﬁciency for CRISPR enzymes.Nucleic Acids Res.48,\n5037– 5053 (2020).\n13. Kim, D. et al. Digenome-seq: genome-wide proﬁling of CRISPR-Cas9\noff-target effects in human cells.Nat. Methods12\n, 237– 243 (2015).\n14. Tsai, S. Q. et al. Circle-seq: a highly sensitive in vitro screen for\ngenome-wide CRISPR– Cas9 nuclease off-targets.Nat. Methods14,\n607– 614 (2017).\n15. Wienert, B. et al. Unbiased detection of CRISPR off-targets in vivo\nusing discover-seq.Science 364, 286– 289 (2019).\n16. Ortinski, P. I., O’Donovan, B., Dong, X. & Kantor, B. Integrase-deﬁcient\nlentiviral vector as an all-in-one platform for highly efﬁcient CRISPR/\nCas9-mediated gene editing.Mol. Ther. Methods Clin. Dev.5,\n153– 164 (2017).\n17. Tsai, S. Q. et al. Guide-seq enables genome-wide proﬁling of off-\ntarget cleavage by CRISPR-Cas nucleases.Nat. Biotechnol.33,\n187– 197 (2015).\n18. Listgarten, J. et al. Prediction of off-target activities for the end-to-end\ndesign of CRISPR guide RNAs.Nat. Biomed. Eng.2,3 8– 47 (2018).\n19. Bao, X. R., Pan, Y., Lee, C. M., Davis, T. H. & Bao, G. Tools for\nexperimental and computational analyses of off-target editing by\nprogrammable nucleases.Nat. Protoc.16,1 0– 26 (2021).\n20. Bae, S., Park, J. & Kim, J.-S. Cas-ofﬁnder: a fast and versatile\nalgorithm that searches for potential off-target sites of Cas9 RNA-\nguided endonucleases.Bioinformatics 30, 1473– 1475 (2014).\n21. Labun, K. et al. Chopchop v3: expanding the CRISPR web toolbox\nbeyond genome editing.Nucleic Acids Res.47, W171– W174 (2019).\n22. Bradford, J. & Perrin, D. A benchmark of computational CRISPR-Cas9\nguide design methods.PLoS Comput. Biol.15, e1007274 (2019).\n23. Dobson, L., Reményi, I. & Tusnády, G. E. Cctop: a consensus\nconstrained topology prediction web server.Nucleic Acids Res.43,\nW408– W412 (2015).\n24. Cao, Q. et al. Crispr-focus: a web server for designing focused crispr\nscreening experiments.PLoS ONE12, e0184281 (2017).\n25. Alkan, F., Wenzel, A., Anthon, C., Havgaard, J. H. & Gorodkin, J.\nCRISPR-Cas9 off-targeting assessment with nucleic acid duplex\nenergy parameters.Genome Biol.19,1 – 13 (2018).\n26. Chuai, G. et al. Deepcrispr: optimized CRISPR guide RNA design by\ndeep learning.Genome Biol.19,1 – 18 (2018).\n27. Lin, J., Zhang, Z., Zhang, S., Chen, J. & Wong, K.-C. Crispr-net: a\nrecurrent convolutional network quantiﬁes CRISPR off-target\nactivities with mismatches and indels.Adv. Sci.7, 1903562 (2020).\n28. Yan, J. et al. Benchmarking and integrating genome-wide CRISPR off-\ntarget detection and prediction.Nucleic Acids Res.48, 11370–11379\n(2020).\n29. Cameron, P. et al. Mapping the genomic landscape of CRISPR– Cas9\ncleavage. Nat. Methods14, 600– 606 (2017).\n30. Zou, R. S. et al. Improving the sensitivity of in vivo CRISPR off-target\ndetection with discover-seq+. Nat. Methods20, 706– 713 (2023).\n31. Lazzarotto, C. R. et al. Change-seq reveals genetic and epigenetic\neffects on CRISPR– Cas9 genome-wide activity.Nat. Biotechnol.38,\n1317– 1327 (2020).\n32. Tsai, S. Q. & Joung, J. K. Deﬁning and improving the genome-wide\nspeciﬁcities of CRISPR– Cas9 nucleases.Nat. Rev. Genet.17,\n300– 312 (2016).\n33. Kim, D. & Kim, J.-S. Dig-seq: a genome-wide CRISPR off-target\nproﬁling method using chromatin DNA.Genome Res.28, 1894– 1900\n(2018).\n34. Wang, X. et al. Unbiased detection of off-target cleavage by CRISPR-\nCas9 and talens using integrase-defective lentiviral vectors.Nat.\nBiotechnol. 33, 175– 178 (2015).\n35. Frock, R. L. et al. Genome-wide detection of DNA double-stranded\nbreaks induced by engineered nucleases.Nat. Biotechnol.33,\n179– 186 (2015).\n36. Pan, X. et al. Massively targeted evaluation of therapeutic CRISPR off-\ntargets in cells.Nat. Commun.13, 4049 (2022).\n37. Shen, T. et al. Accurate RNA 3D structure prediction using a language\nmodel-based deep learning approach.Nature Methods. pp. 1– 12\n(Nature Publishing Group US New York, 2024).\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 9\n38. Consortium, T. R. Rnacentral 2021: secondary structure integration,\nimproved sequence search and new member databases.Nucleic\nAcids Res.49, D212– D220 (2021).\n39. Guan, Z. & Jiang, Z. Transformer-based anti-noise models for\nCRISPR-Cas9 off-target activities prediction.Brief. Bioinforma.24,\nbbad127 (2023).\n40. Störtz, F. & Minary, P. crisprsql: a novel database platform for\nCRISPR/Cas off-target cleavage assays.Nucleic Acids Res.49,\nD855– D861 (2021).\n41. Sherkatghanad, Z., Abdar, M., Charlier, J. & Makarenkov, V. Using\ntraditional machine learning and deep learning methods for on-and\noff-target prediction in CRISPR/Cas9: a review.Brief. Bioinforma.24,\nbbad131 (2023).\n42. Liu, Q., He, D. & Xie, L. Prediction of off-target speciﬁcity and cell-\nspeciﬁc ﬁtness of CRISPR-Cas system using attention boosted deep\nlearning and network-based gene feature.PLoS Comput. Biol.15,\ne1007480 (2019).\n43. Zhang, Z.-R. & Jiang, Z.-R. Effective use of sequence information to\npredict CRISPR-Cas9 off-target.Comput. Struct. Biotechnol. J.20,\n650– 661 (2022).\n44. Fu, R. et al. Systematic decomposition of sequence determinants\ngoverning CRISPR/Cas9 speciﬁcity. Nat. Commun.13, 474 (2022).\n45. Nguyen, E. et al. Hyenadna: Long-range genomic sequence modeling\nat single nucleotide resolution.Adv. Neural Inf. Process. Syst.36,\n43177– 43201 (2023).\n46. Luo, Y., Chen, Y., Xie, H., Zhu, W. & Zhang, G. Interpretable CRISPR/\nCas9 off-target activities with mismatches and indels prediction using\nbert. Comput. Biol. Med.169, 107932 (2024).\n47. Lin, J. & Wong, K.-C. Off-target predictions in CRISPR-Cas9 gene\nediting using deep learning.Bioinformatics 34, i656– i663 (2018).\n48. Carlson-Stevermer, J. et al. Crisproff enables spatio-temporal control\nof CRISPR editing.Nat. Commun.11, 5041 (2020).\n49. Riesenberg, S., Helmbrecht, N., Kanis, P., Maricic, T. & Pääbo, S.\nImproved grna secondary structures allow editing of target sites\nresistant to CRISPR-Cas9 cleavage.Nat. Commun.13, 489 (2022).\n50. Mathis, N. et al. Predicting prime editing efﬁciency and product purity\nby deep learning.Nat. Biotechnol.41, 1151– 1159 (2023).\n51. Wang, D. et al. Optimized CRISPR guide RNA design for two high-\nﬁdelity Cas9 variants by deep learning.Nat. Commun.10\n, 4284 (2019).\n52. Xiang, X. et al. Enhancing CRISPR-Cas9 gRNA efﬁciency prediction\nby data integration and deep learning.Nat. Commun.12, 3238 (2021).\n53. Li, V. R., Zhang, Z. & Troyanskaya, O. G. Croton: an automated and\nvariant-aware deep learning framework for predicting CRISPR/Cas9\nediting outcomes.Bioinformatics 37, i342– i348 (2021).\n54. Chen, W. et al. Massively parallel proﬁling and predictive modeling of\nthe outcomes of CRISPR/Cas9-mediated double-strand break repair.\nNucleic Acids Res.47, 7989– 8003 (2019).\n55. Allen, F. et al. Predicting the mutations generated by repair of Cas9-\ninduced double-strand breaks.Nat. Biotechnol.37,6 4– 72 (2019).\n56. Du, W. et al. CCLMoff: A CRISPR/Cas9 System Off-target Prediction\nTool Using Language Model.https://doi.org/10.6084/m9.ﬁgshare.\n27080566.v2 (2024).\n57. Du, W. et al. CCLMoff: a CRISPR/Cas9 system off-target prediction\ntool using language model.Zenodo https://doi.org/10.5281/zenodo.\n15385508 (2025).\n58. Anderson, K. R. et al. CRISPR off-target analysis in genetically\nengineered rats and mice.Nat. Methods15, 512– 514 (2018).\n59. Kleinstiver, B. P. et al. High-ﬁdelity CRISPR– Cas9 nucleases with no\ndetectable genome-wide off-target effects.Nature 529, 490– 495\n(2016).\n60. Chen, J. S. et al. Enhanced proofreading governs CRISPR– Cas9\ntargeting accuracy.Nature 550, 407– 410 (2017).\n61. Ran, F. A. et al. In vivo genome editing usingStaphylococcus aureus\nCas9. Nature 520, 186– 191 (2015).\n62. Slaymaker, I. M. et al. Rationally engineered Cas9 nucleases with\nimproved speciﬁcity. Science 351,8 4– 88 (2016).\nAcknowledgements\nThis work was jointly supported by the National Key R&D Plan of China\n(2021YFA0805901), the National Natural Science Foundation of China\n(82070199), the Guangdong Basic and Applied Basic Research Fund\n(2021A1515220078), the Guangdong Provincial R&D Project in Key Areas\n(2022B0202110002), and the Special Project in Key Fields of Guangdong\nProvincial Universities (2021ZDZX2050).\nAuthor contributions\nL.Z., X.Z. and D.T. conceived the project. W.D. and L.Z. designed the\nmethodology and conducted the experiments. L.Z., X.Z.and D.T. supervised\nthe study. W.D., Y.Z., K.D., Z.Z. and Q.Y. drafted the manuscript. All authors\nreviewed and approved theﬁnal version of the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s42003-025-08275-6.\nCorrespondenceand requests for materials should be addressed to\nLiang Zhao, Xiangxing Zhu or Dongsheng Tang.\nPeer review informationCommunications Biologythanks the anonymous\nreviewers for their contribution to the peer review of this work. Primary\nHandling Editors: Aylin Bircan.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s42003-025-08275-6 Article\nCommunications Biology|           (2025) 8:882 10",
  "topic": "CRISPR",
  "concepts": [
    {
      "name": "CRISPR",
      "score": 0.907586932182312
    },
    {
      "name": "Computer science",
      "score": 0.6220093965530396
    },
    {
      "name": "Cas9",
      "score": 0.5285277366638184
    },
    {
      "name": "Genome editing",
      "score": 0.4899863302707672
    },
    {
      "name": "Computational biology",
      "score": 0.4546757638454437
    },
    {
      "name": "Programming language",
      "score": 0.41495001316070557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34507423639297485
    },
    {
      "name": "Biology",
      "score": 0.2656536400318146
    },
    {
      "name": "Genetics",
      "score": 0.14110296964645386
    },
    {
      "name": "Gene",
      "score": 0.08918514847755432
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I14894300",
      "name": "Foshan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210164269",
      "name": "Shenzhen Nanshan Center for Chronic Disease Control",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I134626604",
      "name": "Jiujiang University",
      "country": "CN"
    }
  ]
}