{
  "title": "Using Large Language Models to Generate JUnit Tests: An Empirical Study",
  "url": "https://openalex.org/W4367694420",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4229152795",
      "name": "Siddiq, Mohammed Latif",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4367935032",
      "name": "Santos, Joanna C. S.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4367935033",
      "name": "Tanvir, Ridwanul Hasan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4367935034",
      "name": "Ulfat, Noshin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4367935035",
      "name": "Rifat, Fahmid Al",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4367935036",
      "name": "Lopes, Vinicius Carvalho",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4315815628",
    "https://openalex.org/W4287668913",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W2623752963",
    "https://openalex.org/W2105896632",
    "https://openalex.org/W3106077701",
    "https://openalex.org/W4367672983",
    "https://openalex.org/W1538216099",
    "https://openalex.org/W4236387090",
    "https://openalex.org/W4388483640",
    "https://openalex.org/W1964730672",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2611190712",
    "https://openalex.org/W2107709519",
    "https://openalex.org/W4366204357",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W2997945246",
    "https://openalex.org/W4384302749",
    "https://openalex.org/W2963935794",
    "https://openalex.org/W2014309790",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2068964333",
    "https://openalex.org/W4382653968",
    "https://openalex.org/W4221154060",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W2294635146",
    "https://openalex.org/W4223598146",
    "https://openalex.org/W2888078871",
    "https://openalex.org/W3162689995",
    "https://openalex.org/W4385270094",
    "https://openalex.org/W2133414060",
    "https://openalex.org/W2807453842",
    "https://openalex.org/W2146511370",
    "https://openalex.org/W2035894543",
    "https://openalex.org/W2056514427",
    "https://openalex.org/W3210416950",
    "https://openalex.org/W4288347730",
    "https://openalex.org/W4384304865",
    "https://openalex.org/W4389544169",
    "https://openalex.org/W4320854935",
    "https://openalex.org/W4307479317",
    "https://openalex.org/W2147002252",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2150593711",
    "https://openalex.org/W3021206621",
    "https://openalex.org/W4286750487",
    "https://openalex.org/W2969467935",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W4387559769",
    "https://openalex.org/W4281669078",
    "https://openalex.org/W4281611258",
    "https://openalex.org/W2037237472",
    "https://openalex.org/W2119415615",
    "https://openalex.org/W2144978341"
  ],
  "abstract": "A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.",
  "full_text": "Using Large Language Models to Generate JUnit Tests: An\nEmpirical Study\nMohammed Latif Siddiq\nmsiddiq3@nd.edu\nUniversity of Notre Dame\nNotre Dame, IN, USA\nJoanna C. S. Santos\njoannacss@nd.edu\nUniversity of Notre Dame\nNotre Dame, IN, USA\nRidwanul Hasan Tanvir\nrpt5409@psu.edu\nPennsylvania State University\nUniversity Park, PA, USA\nNoshin Ulfat\nnoshin.ulfat@iqvia.com\nIQVIA Inc.\nDhaka, Bangladesh\nFahmid Al Rifat\nfahmid@cse.uiu.ac.bd\nUnited International University\nDhaka, Bangladesh\nVinÃ­cius Carvalho Lopes\nvlopes@nd.edu\nUniversity of Notre Dame\nNotre Dame, IN, USA\nABSTRACT\nA code generation model generates code by taking a prompt from\na code comment, existing code, or a combination of both. Although\ncode generation models (e.g., GitHub Copilot) are increasingly being\nadopted in practice, it is unclear whether they can successfully be\nused for unit test generation without fine-tuning for a strongly\ntyped language like Java. To fill this gap, we investigated how well\nthree models (Codex, GPT-3.5-Turbo, and StarCoder) can generate\nunit tests. We used two benchmarks (HumanEval and Evosuite\nSF110) to investigate the effect of context generation on the unit test\ngeneration process. We evaluated the models based on compilation\nrates, test correctness, test coverage, and test smells. We found that\nthe Codex model achieved above 80% coverage for the HumanEval\ndataset, but no model had more than 2% coverage for the EvoSuite\nSF110 benchmark. The generated tests also suffered from test smells,\nsuch as Duplicated Asserts and Empty Tests.\nCCS CONCEPTS\nâ€¢ Software and its engineering â†’Software testing and de-\nbugging; â€¢ Computing methodologies â†’Instance-based learn-\ning.\nKEYWORDS\ntest generation, unit testing, large language models, test smells,\njunit\nACM Reference Format:\nMohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir,\nNoshin Ulfat, Fahmid Al Rifat, and VinÃ­cius Carvalho Lopes. 2024. Using\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy\nÂ© 2024 ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\nhttps://doi.org/XXXXXXX.XXXXXXX\nLarge Language Models to Generate JUnit Tests: An Empirical Study. InPro-\nceedings of The 28th International Conference on Evaluation and Assessment\nin Software Engineering (EASE 2024). ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1 INTRODUCTION\nUnit testing [8] is a software engineering activity in which indi-\nvidual units of code are tested in isolation. This is an important\nactivity because it helps developers identify and fix defects early\non in the development process and understand how the various\nunits of code in a software system fit together and work as a co-\nhesive whole. Despite its importance, in practice, developers face\ndifficulties when writing unit tests [17, 36, 55, 65]. This leads to a\nnegative effect: developers may not write tests for their code. In\nfact, a prior study [26] showed that out of 82,447 studied GitHub\nprojects, only 17% of them contained test files.\nSince implementing test cases to achieve good code coverage is a\ntime-consuming and error-prone task, prior works [60, 66] devel-\noped techniques to automatically generate unit tests. Although au-\ntomatically generated unit tests help increase code coverage [6, 58],\nthey are still not frequently used in practice [23].\nWith the advances of large language models (LLMs), LLM-based\ncode generation tools ( e.g., GitHub Copilot) are increasingly be-\ncoming part of day-to-day software development. A survey of 500\nUS-based developers showed that 92% of them are using LLM-based\ncoding assistants both for work and personal use [61]. Part of this\nfast widespread adoption is that LLMs automate repetitive tasks so\nthat they can focus on higher-level, challenging tasks [71]. With the\nincreasing popularity of code generation LLMs, prior works investi-\ngated the correctness of the generated code [18], their quality [62],\nsecurity [50] and whether they can be used for API learning tasks\n[31], and code complexity prediction [63]. However, it is currently\nunclear the effectiveness of using prompt-based pre-trained code\ngeneration models to generate unit tests for strongly typed lan-\nguages such as Java. In fact, prior works [4, 14] have shown that\nLLMs perform better for weakly typed languages (e.g., Python and\nJavaScript) but not as well for strongly typed languages. This is\narXiv:2305.00418v4  [cs.SE]  9 Mar 2024\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and VinÃ­cius Carvalho Lopes\npartially due to the limited training sets availability and the fact\nthat strongly typed languages have strict type-checking that can\nprevent the code from even compiling.\nIn light of this research gap, we conducted an empirical study using\nthree LLMs (Codex [16], GPT-3.5-Turbo [1] and StarCoder [39]) to\ngenerate JUnit5 tests for classes in the HumanEval datasetâ€™s Java\nversion [5] and 47 open-source projects from the SF110 dataset [20].\nIn our study, we investigate how well LLMs can generate JUnit tests\n(RQ1) and how different context styles (e.g., only using the method\nunder test, the presence/ absence of JavaDocsetc.) provided as input\nto an LLM can influence its performance (RQ2). We examined the\ngenerated tests with respect to their compilation rates , correctness,\ncode coverage, and quality (in terms of test smells). While concurrent\nworks [38, 57] studied the usefulness of LLM as a helper for search-\nbased test generation techniques on weakly typed languages (i.e.,\nPython and JavaScript), our work investigates whether LLMs can\nbe used off-the-shelf to generated unit tests for a strongly-typed\nlanguage like Java. Moreover, we examine these generated tests in\nterms not only of their correctness, but also their quality, as well as\nthe effectiveness of different context styles .\nThe contributions of our work are: 1 A systematic study of three\nLLMs for zero-shot unit test generation for 194 classes from 47\nopen-source projects in the SF110 dataset [21] and 160 classes from\nthe HumanEval dataset [5]. 2 An investigation of the quality of\nthe produced unit tests by studying the prevalence of test smells in\nthe generated unit tests. 3 A comparison of how different context\nstyles affect the performance of LLMs in generating tests. 4 A\ndiscussion about the implication of using code generation models\nfor unit test generation in a Test Driven Development (TDD) envi-\nronment. All the scripts used to gather the data and spreadsheets\ncompiling all the results are available on Zenodo 1.\n2 BACKGROUND\n2.1 Unit Tests & Test Smells\nThe goal of unit testing is to validate that each program unit is\nworking as intended and meets its requirements [37]. A unit refers\nto a piece of code that can be isolated and examined independently\n(e.g., functions/methods, classes, etc.). Just like production code,\nunit tests need to be not only correct but also satisfy other quality\nattributes, such as maintainability and readability [26].\nUnit test smells (henceforth â€œtest smellsâ€) are indicators of poten-\ntial problems, inefficiencies, or bad programming/design practices\nin a unit test suite [ 28, 29, 49, 51, 67]. There are many test smell\ntypes, ranging from tests that are too slow/fragile to tests that are\ntoo complex or too tightly coupled to implementing the code under\ntest [45]. For example, the Java code in Listing 1 has a unit test\nfor a method from the LargestDivisor class. It checks whether the\nMethod Under Test (MUT) returns the largest divisor of a number.\nAlthough this test is correct, there is no explanation for the expected\noutputs passed to the assertions, which is a case of theMagic Num-\nber Test smell [45]. It also has multiple assertions in the same test\nmethod, an example of Assertion Roulette smell [67].\n1https://doi.org/10.5281/zenodo.10530787\nLargestDivisorTest.java\n1 public class LargestDivisorTest {\n2 @Test\n3 void testLargestDivisor() {\n4 assertEquals(5, LargestDivisor.largestDivisor(15));\n5 assertEquals(1, LargestDivisor.largestDivisor(3));\n6 }\n7 }\nListing 1: Example of Unit Test and Unit Test Smell\n2.2 Code Generation\nLarge Language Models (LLMs) are advanced AI systems capable\nof understanding and generating human-like text. They can bed\nused to answer questions, create content, and even engage in con-\nversation. Code LLMs (henceforth simply â€œLLMsâ€) are a specialized\ntype of LLMs trained on source code to help with code-related\ntasks, e.g., code completion [34, 35, 64], search [19], and summa-\nrization [25]. They are designed to generate source code from a\ngiven prompt [3], such as a text written in natural language, pseu-\ndocode, code comments etc. These techniques may also take into\naccount the surroundingcontext when generating the code, such as\nfile/variable names, other files in the software system, etc.\n3 METHODOLOGY\nIn this work, we answer two research questions.\nRQ1 How well can LLMs generate JUnit tests?\nWe used GPT-3.5-Turbo, StarCoder, and Codex to generate unit tests\nfor competitive programming assignments from the Java version\nof the HumanEval dataset [5] as well as 47 open-source projects\nfrom the EvoSuite SF110 benchmark dataset[20]. We measured the\nLLMsâ€™ performance by computing the testâ€™s branch/line coverage,\ncorrectness, and quality (in terms of test smells). We also compared\nthe performance of these models with Evosuite [ 20], an existing\nstate-of-the-art approach.\nRQ2 How do different code elements in a context influence the\nperformance of LLMs in generating JUnit tests?\nWhen developers use LLMs to generate JUnit tests, they create a\nprompt (e.g., â€œWrite a JUnit test to verify that login(req) returns\n... â€) and the method (unit) under test becomes the context for that\nprompt. Since the unit under test (context) can include several code\nelements, we investigate how these different elements affect the gen-\nerated tests. To answer RQ2, we conducted a controlled experiment\nin which we created 3 different scenarios for the HumanEval [5, 16],\nand 4 scenarios for 47 open-source projects from the EvoSuite SF110\ndataset[20]. Each scenario contains a different set of code elements.\nThen, we use Codex, GPT-3.5-Turbo, and StarCoder to generate JU-\nnit tests for each scenario. We measured their performance in terms\nof compilation rates, code coverage, the total number of correct\nunit tests, and the incidence of test smells.\n3.1 Answering RQ1\nWe followed a three-step systematic process to investigate how well\nLLMs can generate unit tests: 1 we collected 160 Java classes from\nthe multilingual HumanEval dataset [5] and 194 Java classes\nfrom 47 projects in the Evosuite SF110 benchmark dataset [13,\n21]; 2 we generated JUnit5 tests using three LLMs;3 we computed\nthe compilation rates, correctness, number of smells, as well as the\nUsing Large Language Models to Generate JUnit Tests: An Empirical Study EASE 2024, 18â€“21 June, 2024, Salerno, Italy\nline/branch coverage for the generated tests and compared with\nEvosuite v1.2.0, which is a state-of-the-art unit test generation\ntool [20]. In this paper, methods are our units under test.\n3.1.1 Data Collection. We use themultilingual HumanEval dataset[5]\nbecause it has been widely used in prior works [24, 47, 62] to eval-\nuate code LLMs. Similarly, we use the SF110 dataset because it is a\npopular benchmark for unit test generation [22].\nGreatestCommonDivisor.java\n1 class GreatestCommonDivisor {\n2 /**\n3 * Return the greatest common divisor of two integers a and b.\n4 * > greatestCommonDivisor(3, 5)\n5 * 1\n6 */\n7 public static int greatestCommonDivisor(int a, int b) {\n8 if (a == 0) return b;\n9 return greatestCommonDivisor(b % a, a);\n10 }\n11 }\nListing 2: Sample from the extended HumanEval [5]\nâ€“ The multilingual HumanEval dataset [5] contains 160 prompts\ndescribing programming problems for Java and other programming\nlanguages crafted from the original Python-based HumanEval [16].\nHowever, this multilingual version does not provide a solution for\neach prompt. Thus, we wrote the solution for each problem and\ntested our implementation using the provided test cases. Listing 2\nshows a sample taken from this dataset, where the prompt is in\nlines 1â€“7 and the solution is in lines 8â€“11.\nâ€“ The SF110 dataset, which is an Evosuite benchmark consisting\nof 111 open-source Java projects retrieved from SourceForge. This\nbenchmark contains 23,886 classes, over 800,000 bytecode-level\nbranches, and 6.6 million lines of code [22].\nClass and Method Under Test Selection . Each class in the multilin-\ngual HumanEval [5] has one public static method and may also\ncontain private â€œhelperâ€ methods to aid the solution implemen-\ntation. In this study, all the public static methods are selected as\nmethods under test (MUTs).\nFor the SF110 benchmark, we first retrieved only the classes that\nare public and not abstract. We then discarded test classes ( i.e.,\nplaced on a src/test folder, or that contains the keyword â€œTestâ€ in its\nname). Next, we identified testable methods by applying inclusion\nand exclusion criteria. The exclusion criteria are applied to the non-\nstatic methods that (E1) have a name starting with â€œgetâ€ and takes\nno parameters, or (E2) have a name starting with â€œisâ€, takes no\nparameter and returns a boolean value, or (E3) have a name start-\ning with â€œsetâ€, or (E4) override the ones from java.lang.Object\n(i.e., toString(), hashCode(), etc.). The exclusion criteria E1â€“E3 are\nmeant to disregard â€œgetterâ€ and â€œsetterâ€ methods. The inclusion\ncriteria are that the method has(I1) a public visibility, (I2) a return\nvalue, and (I3) a good JavaDoc. A good JavaDoc is one that (i)\nhas a description or has a non-empty @return tag, and (ii) all the\nmethodâ€™s parameters have an associated description with @param\ntag. After this step, we obtained a total of 30,916 methods under\ntest (MUTs) from 2,951 classes. Subsequently, we disregard projects\nbased on the number of retrieved testable methods (MUTs). We kept\nprojects with at least one testable method (i.e., first quartile) and at\nmost 31 testable methods (i.e., third quartile), obtaining a total of\n53 projects. This filtering aimed to remove projects with too little\nor too many MUTs, which would exceed the limit of the number of\ntokens that the models can generate. We then removed 6 of these\nprojects in which we could not compile their source code, obtaining\n47 projects and a total of 411 MUTs from 194 classes.\n3.1.2 Unit Test Generation. We used Codex, GPT-3.5-Turbo, and\nStarCoder to generate JUnit tests. Codex is a 12 billion parameters\nLLM [16] descendant of the GPT-3 model [11] which powers GitHub\nCopilot. In this study, we usedcode-davinci-002, the most powerful\ncodex model version of Codex. GPT-3.5-turbo is the model that\npowers the ChatGPT chatbot. It allows multi-turn conversation,\nand it can be instructed to generate code [1]. StarCoder is a 15.5\nbillion parameter open-source code generation model with 8,000\ncontext length and has infilling capabilities. In this work, we used\nthe base model from the StarCoder code LLM series.\nTo generate the JUnit tests, we performed a two-step process:\n1 Context and Prompt Creation : We created aunit test prompt\n(henceforth â€œpromptâ€), which instructs the LLM to generate 10 test\ncases for a specific method, and a context, which encompasses the\nwhole code from the methodâ€™s declaring class as well as import\nstatements to core elements from the JUnit5 API. Listing 3 illustrates\nthe structure of a prompt and context, in which lines 1-9 and lines\n10-20 are part of the context and prompt, respectively. The context\nstarts with a comment indicating the classâ€™ file name followed by\nits full code (i.e., its package declaration, imports, fields, methods,\netc.). Similarly, the prompt starts with a comment indicating the\nexpected file name of the generated unit test. Since a class can have\nmore than one testable method, we generated one unit test file\nfor each testable method in a class and appended a suffix to avoid\nduplicated test file names. A suffix is a number that starts from zero.\nAfter this code comment, the prompt includes the same package\ndeclaration and import statements from the class. It also has import\nstatements to the @Test annotation and the assert* methods (e.g.,\nassertTrue(...)) from JUnit5. Subsequently, the prompt contains\nthe test classâ€™ JavaDoc that specifies the MUT, and how many test\ncases to generate. The prompt ends with the test class declaration\nfollowed by a new line (\\n), which will trigger the LLM to generate\ncode to complete the test class declaration.\nclassNameSuffixTest.java\n1 // ${className}.java\n2 ${packageDeclaration}\n3 ${importedPackages}\n4 class ${className}{\n5 /* ... code before the method under test ... */\n6 public ${methodSignature}{ /* ... method implementation ... */ }\n7 /* ... code after the method under test ... */\n8 }\n9\n10 // ${className}${suffix}Test.java\n11 ${packageDeclaration}\n12 ${importedPackages}\n13 import org.junit.jupiter.api.Test;\n14 import static org.junit.jupiter.api.Assertions.*;\n15\n16 /**\n17 * Test class of {@link ${className}}.\n18 * It contains ${numberTests} unit test cases for the\n19 * {@link ${className}#${methodSignature}} method.\n20 */\n21 class ${className}${suffix}Test {\nListing 3: Prompt template for RQ1\n2 Test Generation: Although all used LLMs can generate code,\nthey have technical differences in terms of number of tokens they\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and VinÃ­cius Carvalho Lopes\ncan handle. Thus, we took slightly different steps to generate tests\nwith these LLMs. We used the OpenAI API to generate tests using\nthe Codex model. Codex can take up to 8,000 tokens as input and\ngenerate up to 4,000 tokens. Thus, we configured this model in two\nways: one to generate up to 2,000 tokens and another to generate up\nto 4,000 tokens. We will call each of them Codex (2K) and Codex\n(4K), respectively. For both cases, we set the modelâ€™s temperature\nas zero in order to produce more deterministic and reproducible\noutput motivated by previous studies [ 15, 53, 56]. The rest of its\ninference parameters are set to their default values.\nGPT-3.5-Turbois also accessible via the OpenAI API. It can take up\nto 4,096 tokens as input and generate up to 2,048 tokens. We asked\nthis LLM to generate up to 2,000 tokens and dedicated the rest (2,096)\nto be used as input. Its temperature is also set to zero and the other\nparameters are set to their defaults. Moreover, we set thesystem\nroleâ€™s content to â€œYou are a coding assistant. You generate only source\ncode. â€and the user roleâ€™s content to the context and prompt. Then,\nthe assistant role outputs the generated test. For StarCoder, we\nused the StarCoderBase model available on HuggingFace library2.\nIt has an 8,000 tokens context window combining the input prompt\ntokens and the output tokens. We limit the output token to 2,000\ntokens to align the experiment with the other two models. We also\nkeep the same inference parameters as the Codex model.\n3.1.3 Data Analysis and Evaluation. We compiled all the unit tests\ntogether with their respective production code and required li-\nbraries. As we compiled the code and obtained compilation errors,\nwe observed that several of these errors were caused by simple\nsyntax problems that could be automatically fixed through heuris-\ntics. Specifically, we noticed that LLMs may (i) generate an extra\ntest class that is incomplete, (ii) include natural language expla-\nnations before and/or after the code, (iii) repeat the class under\ntest and/or the prompt, (iv) change the package declaration or (v)\nremove the package declaration, (vi) generate integer constants\nhigher than Integer.MAX_VALUE, (vii) generate incomplete unit tests\nafter it reaches its token size limit. Thus, we developed 7 heuristics\n(H1â€“H7) to automatically fix these errors :\nH1 It removes any code found after any of the following patterns:\n\"</code>\", \"\\n\\n// {CUT_classname}\", and \"\\n```\\n\\n##\".\nH2 It keeps code snippets within backticks (i.e., ``` code ```) and\nremoves any text before and after the backticks.\nH3 It removes the original prompt from the generated unit test.\nH4 It finds the package declaration in the unit test and renames it\nto the package of the CUT.\nH5 It adds the package declaration if it is missing.\nH6 It replaces large integer constants by Integer.parseInt(n).\nH7 It fixes incomplete code by iteratively deleting lines (from bot-\ntom to top) and adding 1-2 curly brackets. At each iteration, it\nremoves the last line and adds one curly bracket. If the syntax\ncheck fails, it adds two curly brackets and checks the syntax\nagain. If it fails, it proceeds to the next iteration by removing\nthe next line (bottom to top). The heuristic stops if the syntax\ncheck passes or it finds the class declaration (i.e., â€œclass ABCâ€),\nwhichever condition occurs first.\n2https://huggingface.co\nMetrics. We ran each generated unit test with JaCoCo [2] to com-\npute the line coverage , branch coverage and test correctness\nmetrics. Branch Coverage [33] measures how many branches are\ncovered by a test,i.e., ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘£ğ‘–ğ‘ ğ‘–ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘Ÿğ‘ğ‘›ğ‘â„ğ‘’ğ‘ \nğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘ğ‘Ÿğ‘ğ‘›ğ‘â„ğ‘’ğ‘  Ã—100. Line Cover-\nage measures how many lines were executed by the unit test out of\nthe total number of lines [32], i.e., ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘’ğ‘¥ğ‘’ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘‘ ğ‘™ğ‘–ğ‘›ğ‘’ğ‘ \nğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘™ğ‘–ğ‘›ğ‘’ğ‘  Ã—100.\nTest Correctness measures how effectively an LLM generates cor-\nrect input/output pairs. We assume that the code under test is\nimplemented correctly. The reasoning behind this assumption is\ntwofold: the HumanEval dataset contains common problems with\nwell-known correct solutions, and the SF110 projects are mature\nopen-source projects. Given this assumption, a failing test case\nis considered to be incorrect. Thus, we compute the number of\ngenerated unit tests that did not fail.\nWe ran the tests using a timeout of 2 and 10 minutes for the Hu-\nmanEval and the SF110 datasets, respectively, because we observed\ngenerated tests with infinite loops. Moreover, we analyzed the qual-\nity of the unit test from the perspective of the test smells. To this\nend, we used TsDetect, a state-of-the-art tool that detects 20 test\nsmell types [51, 52]. Due to space constraints, we provide a list of\nthe test smells detectable by TsDetect with their descriptions in\nour replication package.\n3.2 RQ2: Code Elements in a Context\nTo investigate how different code elements in a context influence\nthe generated unit test, we first created three scenarios for the\nHumanEval dataset and four for the Evosuite Benchmark.\nHumanEval Scenarios: Recall that each MUT in this dataset has\na JavaDoc describing the methodâ€™s expected behavior and examples\nof input-output pairs (see Listing 1). Thus, we created one scenario\n(S1) that does not contain any JavaDoc (e.g., the JavaDoc from lines\n2-6 within Listing 2 is removed from the CUT). The second scenario\n(S2) has the JavaDoc but it does not include input/output examples,\nonly the methodâ€™s behavior description (e.g., Listing 2 will not have\nlines 4-5). The last scenario (S3) does not include the MUTâ€™s imple-\nmentation, only its signature (e.g., Listing 2 will not have lines 8-10).\nS1 and S2 demonstrate the effect of changing JavaDoc elements.\nTest-Driven Development (TDD) [8] inspires scenario S3, where\ntest cases are written before the code implementation.\nSF110 Scenarios: Unlike HumanEval, the classes from SF110 do\nnot necessarily include input/output pairs. Thus, we created scenar-\nios slightly different than before. Scenario S1 removes (i) any code\nwithin the class before and after the method under test as well as (ii)\nthe classâ€™ JavaDoc. Scenario S2 is the same as S1, but including the\nJavaDoc for the method under test. Scenario S3 is the same as S2,\nexcept that there is no method implementation for the MUT (only\nits signature). Scenario S4 mimics S3, but it also includes all the\nfields and the signatures for the other methods/constructors in the\nMUTâ€™s declaring class. Scenarios S1 and S2 demonstrate the effect\nof having or not having code documentation (JavaDoc). S3 verifies\nthe usefulness of LLMs for TDD whereas S4 is used to understand\nhow code elements in a class are helpful for test generation.\nUsing Large Language Models to Generate JUnit Tests: An Empirical Study EASE 2024, 18â€“21 June, 2024, Salerno, Italy\nAfter creating each of the scenarios above, we generated unit tests\nusing the same models and following the steps outlined in Sec-\ntion 3.1. Then, we used JUnit5, JaCoCo, and TsDetect to measure\ntest coverage, correctness, and quality. Similar to RQ1, we also\ncompared the results to Evosuite [20].\n4 RQ1 RESULTS\nWe analyze the generated tests according to their: (i) compilation\nstatus; (ii) correctness; (iii) coverage; and (iv) quality.\n4.1 Compilation Status\nTable 1 reports the percentage of generated unit tests that are\ncompilable before and after applying the heuristic-based fixes\ndescribed in Section 3.1.3. The number of unit tests and test methods\nfor each model and dataset is shown in the last two columns of\nTable 1. We obtained a total 2,536 test methods ( i.e., a method\nwith an @Test annotation) scattered across 572 compilable Java\ntest files for HumanEval and 2,022 test methods within 600 test\nfiles for SF110. For comparison, we also ran Evosuite [ 20] (with\ndefault configuration parameters) to generate unit tests for each\nof the MUTs. Moreover, in the case of HumanEval, we manually\ncreated a JUnit5 test for each input/output pair provided in each\nprompt.\nHumanEval Results. On the one hand, we found that less than\nhalf of the unit tests generated by Codex (2K), Codex (4K), and\nGPT-3.5-Turbo are compilable for the classes in HumanEval. On\nthe other hand, 70% of StarCoderâ€™s generated unit tests compiled.\nUpon applying heuristic-based fixes, the compilation rates have\nincreased an average of 41%. The biggest increase was observed for\nthe Codex (2K) model; its compilation rate increased from 37.5% to\n100%. StarCoder was the LLM that the heuristics were the least able\nto improve; it only increased the compilation rate by 6.9%.\nSF110 Results. For the SF110 dataset, the compilation rates are lower\nthan the ones observed for HumanEval. Between 2.7% and 12.7%\nof the generated unit tests for the SF110 dataset are compilable\nacross all the studied LLMs. StarCoder was the LLM that generated\nthe highest amount of compilable tests ( 12.7%), whereas Codex\n(2K) and Codex (4K) had the lowest compilation rate ( 2.7% and\n3.4%, respectively). Similar to HumanEval, the heuristic-based fixes\nwere able to increase the compilation rates by 81%, on average.\nCodex was the model with the highest increase; the compilation\nrates increased from less than 5% to over 99%. StarCoder was the\nmodel that least benefited with our heuristics; its compilation rate\nincreased by only 57.2%.\nTable 1: Compilation status of the generated unit tests\nLLM % Compilable % Compilable after fix #Test Methods #Test Classes\nHumanEval\nGPT-3.5-Turbo 43.1% 81.3% 1,117 130\nStarCoder 70.0% 76.9% 948 123\nCodex (2K) 37.5% 100% 697 160\nCodex (4K) 44.4% 99.4% 774 159\nEvosuite 100% NA 928 160\nManual 100% NA 1,303 160\nSF110\nGPT-3.5-Turbo 9.7% 85.9% 194 87\nStarCoder 12.7% 69.8% 1,663 368\nCodex (2K) 2.7% 74.5% 1,406 222\nCodex (4K) 3.4% 83.5% 1,039 152\nEvosuite 100% NA 12,362 1,618\nCompilation error root causes. The unit tests that were not fixable\nthrough heuristics were those that contained semantic errors that\nfailed the compilation. To observe the most common root causes of\ncompilation errors, we collected all the compilation errors and clus-\ntered them using K-means [42]. We used the silhouette method [54]\nto find the number of clusters K (ğ¾ = 48). After inspecting these\n48 clusters and making manual adjustments to clusters to fix im-\nprecise clustering, we found that the top 3 compilation errors for\nHumanEval were caused by unknown symbols (i.e., the com-\npiler cannot find the symbol), incompatible conversion from\njava.util.List<T> to java.util.List<X>, and incompatible con-\nversion from int[] to java.util.List<Integer>. Unknown sym-\nbols accounted for more than 62% of the compilation errors. Sev-\neral of these unknown symbols were caused by invoking non-\nexistent methods or instantiating non-existent classes. For example,\nStarCoder produced several test cases that invoked the method\njava.util.List.of(int,int,int,...), which does not exist. For the\nSF110 dataset, the top 3 compilation errors were unknown sym-\nbols, class is abstract; cannot be instantiated , and no suitable\nconstructor found .\n4.2 Test Correctness\nWe executed each test that were compilable after our automated\nfix. We considered a unit test to be correct if it had a success rate\nof 100% (i.e., all of its test methods passed) whereas a somewhat\ncorrect unit test is one that had at least one passing test method.\nAs explained in Section 3.1.3, the reasoning behind these metrics is\nthat the HumanEval has a canonical solution which is the correct\nimplementation for the problem. Thus, a correct test must not fail\n(or else the input/output generated does not match the benchmarkâ€™s\nproblem). Similarly, as the SF110 benchmark is a popular bench-\nmark for automatic test generation containing mature open-source\nprojects, they have a higher probability that they are functionally\ncorrect. Both metrics are reported in Table 2.\nHumanEval Results. StarCoder generated the highest amount of\ncorrect unit tests (â‰ˆ81%). Although GPT-3.5-Turbo only produced\n52% correct unit tests, it was the model that generated the highest\namount of tests that have atat least one passing test method (92.3%).\nWe also found that increasing Codexâ€™s token size did not yield\nhigher correctness rates. Moreover, between 52% to 81% of gener-\nated tests were correct whereas81%-92% of the tests hadat least one\npassing test case. From these results, we can infer that although all\nthe models could not produce correct tests, they can still be useful\nin generating at least a few viable input/output pairs.\nTable 2: Correct tests percentage for HumanEval and SF110\nGPT-3.5-Turbo StarCoder Codex (2K) Codex (4K)\nHE\n% Correct 52.3% 81.3% 77.5% 76.7%\n% Somewhat Correct 92.3% 81.3% 87.5% 87.4%\nSF110\n% Correct 6.9% 51.9% 46.5% 41.1%\n% Somewhat Correct 16.1% 58.6% 62.7% 53.7%\nSF110 Results. The correctness rates achieved by the LLMs are\nrather low. Less than 52% of the produced tests are correct for\nall models. Even when considering the unit tests that produced at\nleast one passing test case (somewhat correct ), only up to 63% fulfill\nthis criterion. The best-performing model for the SF110 dataset was\nStarCoder, which produced 51.9% correct tests. Codex (2K) was the\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and VinÃ­cius Carvalho Lopes\nbest performing LLM for generating unit tests that haveat least one\npassing test case.\n4.3 Test Coverage\nHumanEval Results. Table 3 shows the line and branch coverage for\nthe HumanEval dataset, computed considering all the Java classes\nin the dataset. The LLMs achieved line coverage ranging from\n67% to 87.7% and branch coverage ranging from 69.3% to 92.8%.\nCodex (4K) exhibited the highest line and branch coverage of87.7%\nand 92.8%, respectively. However, the coverage of the unit tests\ngenerated by LLMs are below the coverage reported by the manual\ntests and those generated by Evosuite. In fact, Evosuite, which relies\non an evolutionary algorithm to generate JUnit tests, has a higher\nline and branch coverage than the manually written tests.\nTable 3: Line and branch coverage\nMetric GPT-3.5-Turbo StarCoder Codex-2K Codex-4K Evosuite Manual\nHumanEval\nLine\nCoverage 69.1% 67.0% 87.4% 87.7% 96.1% 88.5%\nBranch\nCoverage 76.5% 69.3% 92.1% 92.8% 94.3% 93.0%\nSF110\nLine\nCoverage 1.3% 1.1% 1.9% 1.2% 27.5% â€“\nBranch\nCoverage 1.6% 0.5% 1.1% 0.7% 20.2% â€“\nSF110 Results. The test coverage for SF110 is worse when com-\npared to HumanEval (they were less than 2% for all models). Codex\n(2K) was the best performing one in terms of line coverage (1.9%),\nwhereas GPT-3.5-Turbo had the highest branch coverage ( 1.6%).\nYet, these coverages are â‰ˆ11-19Ã—lower than the coverage achieved\nby Evosuiteâ€™s tests.\n4.4 Test Smells\nHumanEval Results. Table 4 shows that the LLMs produced the fol-\nlowing smells3: Assertion Roulette (AR)[67], Conditional Logic Test\n(CLT) [45], Empty Test (EM) [51], Exception Handling (EH) [51],\nEager Test (EA)[67], Lazy Test (LT)[67], Duplicate Assert (DA)[51],\nUnknown Test (UT) [51], , and Magic Number Test (MNT) [45]. We\nfound that Magic Number Test (MNT) and Lazy Test (LT) are the\ntwo most reoccurring test smell types acrossall the approaches, i.e.,\nin the unit tests generated by the LLMs and Evosuite as well as the\nones created manually. The MNT smell occurs when the unit test\nhard-codes a value in an assertion without a comment explaining\nit, whereas the LT smell arises when multiple test methods invoke\nthe same production code.\nTable 4: Test smells distribution for the HumanEval dataset.\nTest Smell Codex (2K) Codex (4K) StarCoder GPT-3.5-Turbo Evosuite Manual\nAR 61.3% 59.7% 51.3% 23.8% 15.0% 0.0%\nCLT 0.0% 0.0% 0.0% 1.5% 0.0% 0.0%\nEM 1.9% 1.3% 3.8% 0.8% 0.0% 0.0%\nEH 0.0% 0.0% 0.0% 0.0% 100.0% 100.0%\nEA 60.6% 59.1% 48.8% 23.8% 16.3% 0.0%\nLT 39.4% 41.5% 51.3% 86.2% 99.4% 100.0%\nDA 15.6% 14.5% 10.6% 3.1% 0.6% 0.0%\nUT 10.0% 5.7% 6.3% 0.8% 0.0% 0.0%\nMNT 100.0% 100.0% 100% 100.0% 100.0% 100.0%\n3We hide Default Test, General Fixture , Mystery Guest , Verbose Test, Resource Optimism ,\nDependent Test, and other test smell types supported by TsDetect because they did\nnot occur in any of the listed approaches\nWhereas Codex, StarCoder, and GPT-3.5-Turbo did not produce\nunit tests with the Exception Handling (EH) smell, this smell type\nwas frequent in all manually created tests and those generated by\nEvosuite. We also found that Assertion Roulette (AR) is a common\nsmell produced by LLMs (frequency between 23.8% â€“ 61.3%) and\nthat also occurred in Evosuite in 15% of its generated tests. This\nsmell occurs when the same test method invokes an assert state-\nment to check for different input/output pairs and does not include\nan error message for each of these asserts. Similarly, the LLMs and\nEvosuite also produced unit tests with the Eager Test smell (EA),\nin which a single test method invokes different methods from the\nproduction class, as well as the Duplicate Assert smell (DA) (caused\nby multiple assertions for the same input/output pair).\nSF110 Results. The smells detected for the SF110 tests are listed in\nTable 5. Similar to HumanEval, Magic Number Test (MNT), Asser-\ntion Roulette (AR), and Eager Tests (EA) are frequently occurring\nsmells in the unit tests generated by the LLMs and Evosuite. The\nLLMs generated other types of smells that were not observed for\nthe HumanEval dataset, namely Constructor Initialization (CI) [51],\nMistery Guest (MG)[67], Redundant Print (RP)[51], Redundant As-\nsertion (RA) [51], Sensitive Equality (SE)[67], Ignored Test (IT)[51],\nand Resource Optimism (RO) [51].\nWhile LLMs produced tests that had Empty Tests (EM), Redundant\nPrint (RP), Redundant Assertion (RA), and Constructor Initializa-\ntion (CI) smells, Evosuite did not generate any unit test with these\nsmell types. We also observed that StarCoder generated (proportion-\nally) more samples than the other models (96.7% of its generated\ntests had at least one test smell).\nTable 5: Test smells distribution for the SF110 dataset (RQ1).\nTest Smell GPT-3.5-Turbo StarCoder Codex (2K) Codex (4K) Evosuite\nAR 4.6% 35.1% 14.4% 17.1% 35.0%\nCLT 2.3% 2.4% 0.5% 1.3% 0.0%\nCI 0.0% 4.9% 0.0% 0.7% 0.1%\nEM 0.0% 3.8% 7.2% 1.3% 0.0%\nEH 2.3% 18.2% 20.7% 19.1% 91.2%\nMG 0.0% 3.5% 2.7% 3.3% 3.0%\nRP 0.0% 10.6% 4.5% 5.9% 0.0%\nRA 0.0% 0.3% 0.9% 0.7% 0.0%\nSE 0.0% 1.9% 0.9% 1.3% 13.7%\nEA 12.6% 39.7% 28.4% 31.6% 39.6%\nLT 21.8% 33.4% 60.8% 60.5% 46.4%\nDA 1.1% 11.7% 1.4% 2.0% 1.5%\nUT 0.0% 21.2% 21.2% 10.5% 22.9%\nIT 0.0% 0.3% 0.0% 0.0% 0.0%\nRO 0.0% 4.6% 2.7% 3.9% 2.7%\nMNT 21.8% 95.4% 93.2% 96.1% 91.2%\n5 RQ2 RESULTS\nSimilar to RQ1, we investigated how code elements in a context\ninfluence the generated unit tests with respect to theircompilation\nstatus, correctness, coverage, and quality.\n5.1 Compilation Status\nFig. 1 shows the compilation rates for the HumanEval and SF110\ndatasets across the different scenarios and LLMs.HumanEval Results. Scenario 3 (S3) increased the original (S0) com-\npilation rates for Codex (2K and 4K) from 37.5%, and 44.4% to\n53.8% and 53.1%, respectively. Although scenario 3 increased the\noriginal compilation rates (blue bars in Fig. 1), these tests have sim-\nilar heuristic-based fix rates. In the case of StarCoder, the original\nprompt (S0) was the best in generating compilable code. GPT-3.5-\nTurbo, on the other hand, experienced a sharp decrease from 43.1%\nUsing Large Language Models to Generate JUnit Tests: An Empirical Study EASE 2024, 18â€“21 June, 2024, Salerno, Italy\n43.1%28.8%51.9%2.5%70.0%60.0%52.5%45.6%37.5%40.0%40.6%53.8%44.4%52.5%49.4%53.1%\n81.3%96.9%95.0%73.8%76.9%76.9%76.9%48.1%100.0%96.9%98.8%96.3%99.4%98.1%99.4%96.9%\nS0S1S2S3S0S1S2S3S0S1S2S3S0S1S2S3GPT-3.5-TurboStarCoderCodex (2K)Codex (4K)\nRQ2: COMPILATIONRATESFORHUMANEVAL\n% Compilable% Compilable (after heuristic-based fix)\n9.7%25.1%27.7%22.4%17.8%12.7%10.9%14.4%8.5%4.4%3.2%24.6%21.9%18.0%5.1%4.9%30.2%32.6%25.5%6.8%\n85.9%75.9%76.9%75.7%76.4%69.8%99.3%99.0%98.8%89.5%99.8%97.6%98.8%98.3%100.0%99.8%95.9%97.3%97.1%99.5%\nS0S1S2S3S4S0S1S2S3S4S0S1S2S3S4S0S1S2S3S4GPT-3.5-TurboStarCoderCodex (2K)Codex (4K)\nRQ2: COMPILATIONRATESFORSF110\n% Compilable% Compilable (after heuristic-based fix)\nFigure 1: Compilation rates for HumanEval and SF110\nto 2.5% for S3. Upon further inspection, we found that scenario 3\ntriggered GPT-3.5-Turbo 3.5 to include the original class under test\nin its entirety, followed by the unit test. This resulted in two pack-\nage declarations on the produced output; one was placed in the very\nfirst line (corresponding to the CUTâ€™s package), and the other was\nplaced after the CUT for the unit testâ€™s package. These duplicated\npackage declarations lead to compilation errors. These issues were\nlater fixed by applying the heuristic H3. For the GPT-3.5-Turbo\nmodel, the best-performing context scenario was S2, in which the\nprompt does not include sample input/output pairs.\nSF110 Results. S2 increased the original (S0) compilation rates for\nGPT-3.5-Turbo, StarCoder, and Codex (4K). However, scenario 1\n(S1) was the best performer for Codex (2K), while scenario 2 (S2)\nwas the second-best performer. What these results show is that\nit is beneficial to include a minimal context, which contains only\nthe MUTâ€™s implementation when generating test cases. The ben-\nefit seems twofold: (1) it can increase the compilation rate of the\ngenerated code snippets, and (2) it consumes less input tokens, as\nother methods from the class under test are removed.\n5.2 Test Correctness\nFig. 2 shows the percentage of unit tests generated by the LLMs\nthat are correct for the HumanEval and SF110 datasets. The best\nperforming scenarios for an LLM are highlighted in green.\n52.3%19.4%16.7%50.0%81.3%28.9%25.0%77.9%77.5%43.9%35.4%71.4%76.7%45.2%37.1%69.0%\nS0S1S2S3S0S1S2S3S0S1S2S3S0S1S2S3GPT-3.5-TurboStarCoderCodex (2K)Codex (4K)\nRQ2: % CORRECTTESTSGENERATEDFORHUMANEVAL\n6.9%3.8%6.1%5.4%3.4%51.9%47.6%47.4%43.5%36.1%46.5%45.5%46.7%49.5%37.9%41.1%42.4%44.1%50.7%37.0%\nS0S1S2S3S4S0S1S2S3S4S0S1S2S3S4S0S1S2S3S4GPT-3.5-TurboStarCoderCodex (2K)Codex (4K)\nRQ2: % CORRECTTESTSGENERATEDFORSF110\nFigure 2: Correctness rates\nHumanEval Results. The original context (S0) is the one that leads\nto the highest amount of correct tests for the HumanEval dataset.\nAmong all scenarios, scenario 3 S3) had a similar correctness rate\ncompared to the original prompt used in RQ1 for GPT-3.5-Turbo\nand Codex (2K, 4K). It is important to highlight that whereas GPT-\n3.5-Turbo only had 73.8% compilable tests in scenario 3 (compared\nto 81.3% tests from the original prompt) it still had a similar correct-\nness rate. Yet, the original prompt is the one that has the highest\ncorrectness rates. Recall that scenario 3 ( S3) is the one in which\nthe implementation of the method under test is not included in the\nprompt. These results show that LLMs can still generate unit tests\neven if the implementation is not provided. Such a scenario can be\nuseful in TDD; where developers write tests before the production\ncode.\nâ€¢Effects on including input/output examples on the prompt.\nThe HumanEval dataset has input/output examples in its problem\ndescription (see Listing 2). Thus, for this dataset, we also investi-\ngated to what extent LLMs are able to generate unique input/output\npairs that are not included in the original problem description and\nhow these are related to the test correctness rates observed. We\nmanually inspected each generated test to compute the total num-\nber of unique input/output pairs generated. For each unique\ninput/output pair, we compared with the ones provided in the\nproblemâ€™s description in order to compute the total number of\ninput-output pairs that are from the problem description and\nthe total number of input-output pairs that are not in the\nproblem description.\n101111119131599101312101117128S0S1S2S3S0S1S2S3S0S1S2S3S0S1S2S3GPT-3.5-TurboStarCoderCodex (2K)Codex (4K)HE\nAVERAGENUMBEROFUNIQUEINPUT/OUTPUTPAIRSPERPROMPTS\n3991367146246021014921683552901491189053637516132589650\n9361571298559411231816531190124951248115994991267\nS0S1S2S3S0S1S2S3S0S1S2S3S0S1S2S3GPT-3.5-TurboStarCoderCodex (2K)Codex (4K)\nDISTRIBUTIONOFUNIQUEINPUT/OUTPUTPAIRSPERSCENARIOANDLLMSFORHUMANEVAL\n# unique input/output pairs from the prompt\nFigure 3: Left: Average number of unique input/outputs per\nprompt for each LLM and the original dataset (HumanEval -\nHE). Right: Total number of unique input/outputs that are\nand are not from the problemâ€™s description.\nFig. 3 (left) shows the average number of unique input/output pairs\nfor each LLM and scenario combination compared to the prob-\nlem description in the HumanEval dataset. Each problem in the\nHumanEval dataset provides an average of 8 input/output pair ex-\namples, whereas the LLMs provide more than that, as the prompts\nexplicitly request 10 test cases for each problem description. We\nobserved that the scenarios S1 and S2, which do not include input-\noutput pairs in the prompt, has a higher average of a number of\nunique input-output pairs.\nFig. 3 (right) shows how many of the generated input/output pairs\nby the LLMs are from the problemâ€™s description and how many are\nnot. We found that the scenarios S1 and S2 generated more input-\noutput pairs that are not from the original description, whereas the\nscenarios S0 and S3 are repeating the test cases from the prompt.\nThat is, the models are behaving like â€œparrotsâ€ [ 9] by using the\nsame input/output in the prompt and just formatting it as a test\ncase without generating new examples. When contrasting with the\ncorrectness rates observed in Fig. 2 we see that scenarios S1 and\nS2 were consistently lower for all LLMs. These results show that\nalthough scenarios S1 and S2 generated more input-output exam-\nples, those were not necessarily correct. The prompts that included\nexamples of input-outputs had higher correctness rates.\nSF110 Results. While the original prompt (S0) achieved the highest\ncorrectness rate for GPT-3.5-Turbo (6.9%) and StarCoder (51.9%), the\nother LLMs observed a correctness increase when using the context\nfrom scenario 3 (S3). Codex (4K) experienced the highest increase\n(from 37.9% to 50.7%) for S3. This scenario (S3) has a context that\nonly includes the MUTâ€™s Javadoc and signature and removes other\nmethods from the class where the MUT is declared.\n5.3 Test Coverage\nFig. 4 shows the line and branch coverage for each scenario.\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and VinÃ­cius Carvalho Lopes\n69.1%75.8%83.9%60.8%67.0%58.1%60.6%36.3%87.4%82.1%81.7%81.4%87.7%83.6%81.7%81.9%96.1%88.5%\n76.5%83.6%89.3%66.6%69.3%56.5%58.1%43.0%92.1%85.0%81.2%86.1%92.8%86.0%80.9%86.8%94.3%93.0%\nS0S1S2S3S0S1S2S3S0S1S2S3S0S1S2S3EVSMNLGPT-3.5-TurboStarCoderCodex (2K)Codex (4K)\nRQ2: TESTCOVERAGEFORHUMANEVAL\nLine CoverageBranch Coverage\n1.3%0.5%0.6%0.4%0.4%1.1%1.5%1.4%1.3%2.1%1.9%2.5%2.4%1.6%2.5%1.2%2.4%2.5%1.6%2.1%27.5%\n1.6%0.4%0.5%0.2%0.4%0.5%0.7%0.6%0.6%0.8%1.1%1.0%1.0%0.7%1.0%0.7%1.0%1.0%0.7%0.9%20.2%\nS0S1S2S3S4S0S1S2S3S4S0S1S2S3S4S0S1S2S3S4S0GPT-3.5-TurboStarCoderCodex (2K)Codex (4K)EVS\nRQ2: TESTCOVERAGEFORSF110\nLine Coverage (CUT classes only)Branch Coverage (CUT classes only)\nFigure 4: Line and Branch Coverage across different datasets,\nscenarios, and LLMs (EVS = Evosuite; MNL = Manual).\nHumanEval Results. For Codex, scenario 1 is the one that had the\nhighest line coverage among the different scenarios in these models.\nGPT-3.5-Turbo and StarCoder, on the other hand, had scenario 2\nas the one with the highest line coverage. With respect to branch\ncoverage, we found that scenario 3 was the best performing one for\nCodex, and scenario 2 is the best one for GPT-3.5-Turbo and Star-\nCoder. None of the scenarios for Codex (2K and 4K) and StarCoder\noutperformed the line/branch coverage of the original prompts nor\nthe coverage achieved by the manual and Evosuiteâ€™s tests.\nSF110 Results. Among all scenarios, scenario 1 ( S1) and scenario\n2 (S2) had a slightly higher line coverage when compared to the\noriginal prompt (S0) used in RQ1 for Codex (2K) and Codex (4K),\nrespectively. For StarCoder the scenario 4 had a higher line coverage\nthan the original one. The original context of GPT-3.5-Turbo, on\nthe other hand, had the highest observed line coverage. In the case\nof branch coverage, scenario 1 (S1) had slightly higher coverage for\nCodex (4K), whereas scenario 4 (S4) was the best one for StarCoder.\nHowever, these increases are still much lower than Evosuiteâ€™s test\ncoverage, which achieved â‰ˆ27% line and branch coverage.\n5.4 Test Smells\nHumanEval Results. Table 6 shows the distribution of smells for\ndifferent scenarios and LLMs. The cells highlighted in green are\nthose in which the percentage is lower than the original context,\nwhereas those highlighted in red have a higher percentage than\nthe original context. In terms of smell types, all scenarios have the\nsame smell types that occurred in the original prompts (see Table 4).\nWe also observe that, overall, the scenarios tended to decrease the\nincidence of generated smells. When comparing each scenario to\none another, there is no clear outperformer across all the LLMs.\nYet, Scenario 3 for GPT-3.5-Turbo had higher percentages than the\noriginal context, on average. Although the average increases are\nnot significant (0.6% and 0.2% for these LLMs, respectively).\nTable 6: Test smells distribution for the HumanEval dataset.\nGPT-3.5-Turbo StarCoder Codex (2K) Codex (4K)\nS1 S2 S3 S1 S2 S3 S1 S2 S3 S1 S2 S3\nAR 7.1% 11.8% 30.5% 36.9% 36.3% 48.1% 16.8% 38.6% 61.0% 16.6% 40.3% 63.2%\nCLT 6.5% 3.3% 0.8% 0.0% 0.6% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0%\nEM 0.0% 0.7% 3.4% 1.9% 8.1% 3.8% 4.5% 3.2% 1.9% 1.3% 1.3% 1.9%\nEA 7.1% 10.5% 26.3% 28.8% 30.0% 48.1% 15.5% 37.3% 56.5% 15.3% 38.4% 58.1%\nLT 85.2% 92.8% 82.2% 61.9% 63.8% 53.1% 84.5% 60.8% 44.2% 84.7% 60.4% 42.6%\nDA 1.3% 0.0% 1.7% 8.1% 11.3% 11.3% 0.6% 8.2% 11.0% 1.9% 6.9% 11.6%\nUT 0.0% 0.7% 3.4% 11.3% 13.8% 6.3% 13.5% 16.5% 2.6% 5.1% 8.2% 2.6%\nMNT 89.7% 98.7% 100% 99.4% 99.4% 100% 100% 100% 100% 100% 100% 100%\nSF110 Results. As shown Table 7, there is not any scenario that con-\nsistently outperforms the other. However, we can observe that sce-\nnario 2 for GPT-3.5-Turbo produces more test smells than the other\nscenarios, as we can see from the cells highlighted in red.\n6 DISCUSSION\nâ€“ LLMs vs. Evosuite: Across all the studied dimensions, LLMs per-\nformed worse than Evosuite. One reason is that LLMs do not always\nTable 7: Test smells distribution for the SF110 dataset (RQ2).\nCodex (2K) Codex (4K) StarCoder GPT-3.5-Turbo\nS1 S2 S3 S4 S1 S2 S3 S4 S1 S2 S3 S4 S1 S2 S3 S4\nAR 17.3% 12.8% 12.4% 7.8% 17.5% 13.5% 13.6% 8.3% 23.0% 23.5% 21.4% 27.1% 6.6% 7.8% 4.4% 12.1%\nCLT 0.0% 0.5% 0.0% 0.7% 0.0% 0.0% 0.0% 0.8% 1.4% 1.6% 1.4% 1.1% 0.5% 1.7% 1.1% 3.5%\nCI 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.2% 0.0% 0.0% 1.1% 0.0% 0.0% 0.0% 0.0%\nEM 8.2% 5.1% 24.8% 5.9% 7.7% 5.0% 21.6% 5.4% 1.4% 1.6% 2.9% 2.9% 0.0% 0.0% 1.1% 2.1%\nEH 14.3% 19.5% 15.3% 24.5% 15.5% 18.5% 14.1% 25.7% 17.2% 22.5% 25.3% 21.5% 2.2% 3.3% 2.7% 5.0%\nMG 2.0% 1.5% 1.0% 2.6% 1.0% 1.5% 1.5% 2.5% 2.2% 2.7% 2.4% 2.7% 1.6% 1.1% 1.1% 3.5%\nRP 2.0% 2.1% 4.0% 3.0% 1.5% 2.5% 4.0% 2.9% 6.8% 16.5% 14.1% 10.7% 0.0% 0.0% 0.0% 0.7%\nRA 1.0% 0.5% 1.0% 1.5% 0.5% 0.5% 1.0% 1.2% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.5% 0.7%\nSE 1.0% 0.0% 1.5% 1.5% 1.0% 0.5% 1.0% 1.2% 0.6% 0.2% 0.4% 0.9% 0.5% 0.6% 1.1% 2.1%\nEA 16.8% 14.4% 11.4% 20.8% 17.0% 13.0% 11.6% 25.3% 24.6% 28.7% 20.8% 35.1% 7.7% 8.3% 6.6% 15.6%\nLT 31.6% 44.1% 32.7% 55.8% 33.0% 46.0% 35.2% 57.7% 30.1% 26.0% 27.1% 32.4% 14.2% 16.7% 13.7% 22.0%\nDA 6.1% 1.5% 1.5% 1.9% 5.2% 2.5% 2.0% 2.5% 6.4% 4.5% 5.1% 7.2% 2.2% 1.7% 0.5% 2.8%\nUT 14.8% 12.3% 30.7% 17.8% 12.9% 10.5% 24.1% 16.6% 17.8% 16.7% 19.4% 20.6% 0.0% 0.0% 1.6% 2.1%\nRO 1.5% 1.5% 2.0% 2.2% 1.0% 1.5% 2.5% 2.9% 3.6% 3.3% 3.7% 4.0% 1.6% 1.1% 1.1% 2.8%\nMNT 98.5% 98.5% 98.0% 91.8% 97.9% 97.5% 98.5% 95.0% 91.2% 96.9% 99.0% 96.4% 18.6% 21.1% 18.0% 29.1%\nproduce compilable unit tests (Table 1). For example, while Evosuite\nproduced one unit test for each of the 160 classes under test, GPT-\n3.5-Turbo only produced 130 compilable (i.e., executable) unit tests.\nAnother reason is that LLMs do not seem to pay attention to the\ncurrent MUTâ€™s implementation. A piece of evidence for this is that\nscenario 3 (which does not include the MUTâ€™s implementation) has\nbetter compilation rates than the rest. However, we also observed\nthat GPT-3.5-Turbo generated test cases for â€œstress-testingâ€, e.g.,\nusing Integer.MAX_VALUE to test for the MUTâ€™s behavior in the face\nof exceptionally large inputs.\nâ€“ Codex and StarCoder perform better than GPT-3.5-Turbo.\nThis can be explained by the fact that Codex and StarCoder are\nLLMs fine-tuned for code-related tasks in contrast to GPT-3.5-Turbo,\nwhich is tailored to dialogues (natural language).\nâ€“ LLMs often â€œhallucinateâ€ inexistent types, methods, etc.\nFor both datasets, the most common compilation error was due to\nmissing symbols. For instance, Codex generated inputs whose type\nwere Tuple, Pair, Triple, Quad, and Quint, which are non-existent\nin Javaâ€™s built-in class types.\nâ€“ Synergy between LLMs and TDD . Although LLMs did not\nachieve coverages or compilation rates comparable to Evosuite,\nthey can still be useful as a starting point for TDD. As we showed\nin our RQ2, LLMs can generate tests based on the MUTâ€™s JavaDoc.\nHowever, given the low correctness rates of LLMs, developers would\nstill need to adjust the generated tests manually.\nGiven these findings, we observe a need for future research to\nfocus on helping LLMs in reason over data types and path feasi-\nbility, as well as exploring the combination of SBST and LLMs for\nTDD. Furthermore, a recent study [71] surveyed 2,000 developers\nand analyzed anonymous user data, showing that GitHub Copilot\nmakes developers more productive because the generated code\ncan automate repetitive tasks. Thus, our findings provide some\ninitial evidence that practitioners following a TDD approach could\nbenefit from LLM-generated tests as a means to speed up their\ntesting. Although further user studies would be needed to verify\nthis hypothesis.\n6.1 Threats to Validity\nCreating canonical solutions for the Java samples in the HumanEval\ndataset [5] introduced an internal validity threat. To mitigate it,\nwe extensively vetted our solution with a test set provided by the\nUsing Large Language Models to Generate JUnit Tests: An Empirical Study EASE 2024, 18â€“21 June, 2024, Salerno, Italy\ndataset. Another validity threat relates to the use of the SF110\nbenchmark [20], JaCoCo [2] for calculating coverage results and\nTsDetect [52] to find test smells. In this case, our analyses depend\non the representativeness of the SF110 dataset (construct validity\nthreat) and the accuracy of these tools. However, the SF110 dataset\nis commonly used to benchmark automated test generation tools\n[12, 20, 59] and JaCoCo and TsDetect are state-of-the-art tools [10,\n68].\n7 RELATED WORK\nPrevious works have focused on creating source code that can do a\nspecific task automatically (code generation). The deductive synthe-\nsis approach [27, 44], in which the task specification is transformed\ninto constraints, and the program is extracted after demonstrating\nthe satisfaction of the constraints, is one of the foundations of pro-\ngram synthesis [30]. Recurrent networks were used by Yin et al.\n[70] to map text to abstract syntax trees, which were subsequently\ncoded using attention. A variety of large language learning mod-\nels have been made public to generate code (e.g., CodeBert [ 19],\nCodeGen [47] and CodeT5 [69]) after being refined on enormous\ncode datasets. Later, GitHub Copilot developed an improved auto-\ncomplete mechanism using the upgraded version of Codex [ 16],\nwhich can help to solve fundamental algorithmic problems [ 18].\nOur work focuses not on code generation but on how a publicly\navailable code generation tool can be used for specialized tasks\nlike unit test generation without fine-tuning ( i.e., zero-shot test\ngeneration).\nShamshiri et al. [60] proposed a search-based approach that au-\ntomatically generates tests that can reveal functionality changes,\ngiven two program versions. On the other hand, Tufano et al. [66]\nproposed an approach that aims to generate unit test cases by learn-\ning from real-world focal methods and developer-written test cases.\nPacheco et al. [48] presented a technique that improves random test\ngeneration by incorporating feedback obtained from executing test\ninputs as they are created for generating unit tests. Lu et al. [43]\nworked on testing autonomous driving systems with reinforcement\nlearning. Lima et [ 41] surveyed the practitioners on software test-\ning and refactoring. In our work, we focus on zero-shot unit test\ngeneration using different contexts in order to measure the LLMâ€™s\nability to generate compilable, correct, and smell-free tests.\nSchÃ¤fer et al. [57] used Codex [16] to automatically generate unit\ntests using an adaptive approach. They used 25 npm packages to\nevaluate their tool, TESTPILOT. However, they evaluated their\nmodel only on statement coverage. They did not provide insight\ninto the quality of the generated test cases and the choice of us-\ning a specific prompt structure. Lemieux et al. [38] combined the\nSearch-based software testing (SBST) technique with the LLM ap-\nproach. It explored whether Codex can be used to help SBSTâ€™s\nexploration. Nashid et al. [46] aimed to devise an effective prompt\nto help large language models with different code-related tasks,\ni.e., program repair and test assertion generation. Their approach\nprovided examples of the same task and asked the LLM to gener-\nate code for similar tasks. Li et al. [40] used ChatGPT [1] to find\nfailure-inducing tests with differential prompting. BareiÃŸ et al. [7]\nperformed a systematic study to evaluate how a pre-trained lan-\nguage model of code, Codex, works with code mutation, test oracle\ngeneration from natural language documentation, and test case\ngeneration using few-shot prompting like Nashid et al. [46]. How-\never, the benchmark has only 32 classes, so the findings may not be\ngeneralized. This work provides direction toward using examples\nof usage or similar tasks as a context. However, in a real case, there\nmay not be any example of using the method and class that can be\nused in the prompt, and creating an example of a similar task needs\nhuman involvement. Our work focused on different contexts taken\nfrom the code base. We evaluated the quality of the generated unit\ntests not only on coverage and correctness but also based on the\npresence of test smells.\n8 CONCLUSION\nWe studied the capability of three code generation LLMs for unit test\ngeneration. We conducted experiments with different contexts in\nthe prompt and compared the results based on compilation rate, test\ncorrectness, coverage, and test smells. These models have a close\nperformance with the state-of-the-art test generation tool for the\nHumanEval dataset, but their performance is poor for open-source\nprojects from Evosuite based on coverage. Though our developed\nheuristics can improve the compilation rate, several generated tests\nwere not compilable. Moreover, they heavily suffer from test smells\nlike Assertion Roulette and Magic Number Test. In future work,\nwe will explore how to enhance LLMs to understand language se-\nmantics better in order to increase test correctness and compilation\nrates.\nREFERENCES\n[1] 2023. Chat completions. Accessed Mar 25, 2023. https://platform.openai.com/\ndocs/guides/chat\n[2] 2023. JaCoCo - Java Code Coverage Library. https://www.jacoco.org/jacoco/\ntrunk/index.html [Online; accessed 30. Mar. 2023].\n[3] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton. 2018. A survey of machine\nlearning for big code and naturalness. ACM Computing Surveys (CSUR) 51, 4\n(2018), 1â€“37.\n[4] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, ..., and\nBing Xiang. 2022. Multi-lingual Evaluation of Code Generation Models. (2022).\n[5] B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, et al. 2022. Multi-lingual Evaluation\nof Code Generation Models.\n[6] A. Bacchelli, P. Ciancarini, and D. Rossi. 2008. On the effectiveness of manual\nand automatic unit test generation. In 2008 The Third Intâ€™l Conf. on Software\nEngineering Advances . IEEE, 252â€“257.\n[7] P. BareiÃŸ, B. Souza, M. dâ€™Amorim, and M. Pradel. 2022. Code generation tools\n(almost) for free? a study of few-shot, pre-trained language models on code.arXiv\npreprint arXiv:2206.01335 (2022).\n[8] K. Beck. 2003. Test-driven development: by example . Addison-Wesley Professional.\n[9] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models be too big?. In Procâ€™d. of the\n2021 ACM Conf. on fairness, accountability, and transparency . 610â€“623.\n[10] I. Bilal, I. Al-Taharwa, S. Rami, I. M. Alkhawaldeh, and N. Ghatasheh. 2021.\nJaCoCo-Coverage Based Statistical Approach for Ranking and Selecting Key\nClasses in Object-Oriented Software. J. Eng. Sci. Technol 16 (2021), 3358â€“3386.\n[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, et al. 2020. Language Models are Few-\nShot Learners. InAdvances in Neural Information Processing Systems , H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nInc., 1877â€“1901.\n[12] D. Bruce, H. D. MenÃ©ndez, and D. Clark. 2019. Dorylus: An ant colony based\ntool for automated test case generation. In Search-Based Software Engineering:\n11th Intâ€™l Symposium, SSBSE 2019, Tallinn, Estonia, August 31â€“September 1, 2019,\nProcâ€™d. 11. Springer, 171â€“180.\n[13] J. Campos, A. Arcuri, G. Fraser, and R. Abreu. 2014. Continuous test generation:\nEnhancing continuous integration with automated test generation. In Procâ€™d. of\nthe 29th ACM/IEEE interâ€™l Conf. on Automated software engineering . 55â€“66.\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and VinÃ­cius Carvalho Lopes\n[14] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-\nCostin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson,\nMolly Q Feldman, et al. 2023. MultiPL-E: a scalable and polyglot approach to\nbenchmarking neural code generation. IEEE Transactions on Software Engineering\n(2023).\n[15] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and W. Chen. 2022. Codet:\nCode generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).\n[16] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, et al. 2021. Evaluating\nLarge Language Models Trained on Code. arXiv:2107.03374 [cs.LG]\n[17] Ermira Daka, JosÃ© Campos, G. Fraser, Jonathan Dorn, and Westley Weimer. 2015.\nModeling readability to improve unit tests. Proceedings of the 2015 10th Joint\nMeeting on Foundations of Software Engineering (2015). https://doi.org/10.1145/\n2786805.2786838\n[18] A. M. Dakhel, V. Majdinasab, A. Nikanjam, F. Khomh, M. C. Desmarais, Z. Ming,\net al. 2022. GitHub Copilot AI pair programmer: Asset or Liability?arXiv preprint\narXiv:2206.15331 (2022).\n[19] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D.\nJiang, and M. Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and\nNatural Languages. In Findings of the Association for Computational Linguistics:\nEMNLP 2020 . Association for Computational Linguistics, Online, 1536â€“1547.\n[20] G. Fraser and A. Arcuri. 2011. EvoSuite: Automatic Test Suite Generation for\nObject-Oriented Software. In Procâ€™d. of the 19th ACM SIGSOFT Symposium and the\n13th European Conf. on Foundations of Software Engineering (Szeged, Hungary)\n(ESEC/FSE â€™11) . Association for Computing Machinery, New York, NY, USA,\n416â€“419.\n[21] G. Fraser and A. Arcuri. 2012. Sound Empirical Evidence in Software Testing.\nIn 34th Intâ€™l Conf. on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich,\nSwitzerland. IEEE, 178â€“188.\n[22] G. Fraser and A. Arcuri. 2014. A Large Scale Evaluation of Automated Unit\nTest Generation Using EvoSuite. ACM Transactions on Software Engineering and\nMethodology (TOSEM) 24, 2 (2014), 8.\n[23] Gordon Fraser, Matt Staats, Phil McMinn, Andrea Arcuri, and Frank Padberg. 2015.\nDoes automated unit test generation really help software testers? a controlled\nempirical study. ACM Transactions on Software Engineering and Methodology\n(TOSEM) 24, 4 (2015), 1â€“49.\n[24] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W. t. Yih, L.\nZettlemoyer, and M. Lewis. 2022. InCoder: A Generative Model for Code Infilling\nand Synthesis. CoRR abs/2204.05999 (2022).\n[25] Y. Gao and C. Lyu. 2022. M2TS: Multi-Scale Multi-Modal Approach Based on\nTransformer for Source Code Summarization. arXiv preprint arXiv:2203.09707\n(2022).\n[26] D. Gonzalez, J. C. S. Santos, A. Popovich, M. Mirakhorli, and M. Nagappan. 2017.\nA large-scale study on the usage of testing patterns that address maintainability\nattributes: patterns for ease of modification, diagnoses, and comprehension. In\n2017 IEEE/ACM 14th Intâ€™l Conf. on Mining Software Repositories (MSR) . IEEE,\n391â€“401.\n[27] C. Green. 1969. Application of Theorem Proving to Problem Solving. In Procâ€™d.\nof the 1st Intâ€™l Joint Conf. on Artificial Intelligence (Washington, DC) (IJCAIâ€™69).\nMorgan Kaufmann Publishers Inc., San Francisco, CA, USA, 219â€“239.\n[28] M. Greiler, A. Zaidman, A. van Deursen, and M.-A. Storey. 2013. Strategies for\nAvoiding Text Fixture Smells during Software Evolution. In Procâ€™d. of the 10th\nWorking Conf. on Mining Software Repositories (San Francisco, CA, USA) (MSR\nâ€™13). IEEE Press, 387â€“396.\n[29] E. M. Guerra and C. T. Fernandes. 2007. Refactoring Test Code Safely. In Intâ€™l\nConf. on Software Engineering Advances (ICSEA 2007) . 44â€“44.\n[30] S. Gulwani, O. Polozov, R. Singh, et al. 2017. Program synthesis. Foundations and\nTrendsÂ® in Programming Languages 4, 1-2 (2017), 1â€“119.\n[31] M. A. Hadi, I. N. B. Yusuf, F. Thung, K. G. Luong, J. Lingxiao, F. H. Fard, and D.\nLo. 2022. On the Effectiveness of Pretrained Models for API Learning. In Procâ€™d.\nof the 30th IEEE/ACM Intâ€™l Conf. on Program Comprehension (Virtual Event) (ICPC\nâ€™22). ACM, New York, NY, USA, 309â€“320.\n[32] M. Hilton, J. Bell, and D. Marinov. 2018. A Large-Scale Study of Test Coverage\nEvolution. In Procâ€™d. of the 33rd ACM/IEEE Intâ€™l Conf. on Automated Software\nEngineering (Montpellier, France) (ASE 2018) . ACM, New York, NY, USA, 53â€“63.\n[33] M. IvankoviÄ‡, G. PetroviÄ‡, R. Just, and G. Fraser. 2019. Code Coverage at Google.\nIn Procâ€™d. of the 2019 27th ACM Joint Meeting on European Software Engineering\nConf. and Symposium on the Foundations of Software Engineering (Tallinn, Estonia)\n(ESEC/FSE 2019) . ACM, New York, NY, USA, 955â€“963.\n[34] M. Izadi, R. Gismondi, and G. Gousios. 2022. CodeFill: Multi-token Code Com-\npletion by Jointly Learning from Structure and Naming Sequences. In 44th Intâ€™l\nConference on Software Engineering (ICSE) .\n[35] S. Kim, J. Zhao, Y. Tian, and S. Chandra. 2021. Code prediction by feeding trees\nto transformers. In 2021 IEEE/ACM 43rd Intâ€™l Conf. on Software Engineering (ICSE) .\nIEEE, 150â€“162.\n[36] P S Kochhar, F Thung, N Nagappan, T Zimmermann, and D Lo. 2015. Understand-\ning the test automation culture of app developers. In 2015 IEEE 8th International\nConference on Software Testing, Verification and Validation (ICST) . IEEE, 1â€“10.\n[37] T. Koomen and M. Pol. 1999. Test Process Improvement: A Practical Step-by-Step\nGuide to Structured Testing . Addison-Wesley Longman Publishing Co., Inc., USA.\n[38] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen. 2023. CODAMOSA: Escaping\nCoverage Plateaus in Test Generation with Pre-trained Large Language Models.\nIn 45th Intâ€™l Conf. on Software Engineering, ser. ICSE .\n[39] R. Li, L. Ben allal, Y. Zi, N. Muennighoff, D. Kocetkov, ..., and H. de Vries. 2023.\nStarCoder: may the source be with you! Transactions on Machine Learning\nResearch (2023). Reproducibility Certification.\n[40] T.-O. Li, W. Zong, Y. Wang, H. Tian, Y. Wang, S.-C. Cheung, and J. Kramer. 2023.\nNuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with\nDifferential Prompting. In 2023 38th IEEE/ACM Intâ€™l Conf. on Automated Software\nEngineering (ASE). IEEE, 14â€“26.\n[41] D. L. Lima, R. De Souza Santos, G. P. Garcia, S. S. Da Silva, C. FranÃ§a, and\nL. F. Capretz. 2023. Software Testing and Code Refactoring: A Survey with\nPractitioners. In 2023 IEEE Intâ€™l Conf. on Software Maintenance and Evolution\n(ICSME). 500â€“507.\n[42] S. P. Lloyd. 1982. Least squares quantization in PCM. IEEE Trans. Inf. Theory 28\n(1982), 129â€“136.\n[43] C Lu, T Yue, M Zhang, and S Ali. 2023. DeepQTest: Testing Autonomous Driving\nSystems with Reinforcement Learning and Real-world Weather Data. ACM\nTransactions on Software Engineering and Methodology (2023).\n[44] Z. Manna and R. J. Waldinger. 1971. Toward Automatic Program Synthesis.\nCommun. ACM 14, 3 (mar 1971), 151â€“165.\n[45] G. Meszaros, S. M. Smith, and J. Andrea. 2003. The Test Automation Manifesto.\nIn Extreme Programming and Agile Methods - XP/Agile Universe 2003 , F. Maurer\nand D. Wells (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 73â€“81.\n[46] N. Nashid, M. Sintaha, and A. Mesbah. 2023. Retrieval-Based Prompt Selection\nfor Code-Related Few-Shot Learning. ICSE23 (2023).\n[47] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C.\nXiong. 2022. A Conversational Paradigm for Program Synthesis. arXiv preprint\n(2022).\n[48] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball. 2007. Feedback-directed random\ntest generation. In 29th Intâ€™l Conf. on Software Engineering (ICSEâ€™07) . IEEE, 75â€“84.\n[49] F. Palomba, D. Di Nucci, A. Panichella, R. Oliveto, and A. De Lucia. 2016. On the\nDiffusion of Test Smells in Automatically Generated Test Code: An Empirical\nStudy. In 2016 IEEE/ACM 9th Intâ€™l Workshop on Search-Based Software Testing\n(SBST). 5â€“14.\n[50] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri. 2022. Asleep at the\nKeyboard? Assessing the Security of GitHub Copilotâ€™s Code Contributions. In\n2022 2022 IEEE Symposium on Security and Privacy (SP) (SP) . IEEE Computer\nSociety, Los Alamitos, CA, USA, 980â€“994.\n[51] A. Peruma, K. Almalki, C. D. Newman, M. Wiem Mkaouer, A. Ouni, and F.\nPalomba. 2019. On the Distribution of Test Smells in Open Source Android\nApplications: An Exploratory Study. In Procâ€™d. of the 29th Annual Intâ€™l Conf. on\nComputer Science and Software Engineering (Toronto, Ontario, Canada) (CASCON\nâ€™19). IBM Corp., USA, 193â€“202.\n[52] A. Peruma, K. Almalki, C. D. Newman, M. W. Mkaouer, A. Ouni, and F. Palomba.\n2020. TsDetect: An Open Source Test Smells Detection Tool. InProcâ€™d. of the 28th\nACM Joint Meeting on European Software Engineering Conf. and Symposium on\nthe Foundations of Software Engineering (Virtual Event, USA) (ESEC/FSE 2020) .\nAssociation for Computing Machinery, New York, NY, USA, 1650â€“1654.\n[53] J. A. Prenner, H. Babii, and R. Robbes. 2022. Can OpenAIâ€™s Codex Fix Bugs?: An\nevaluation on QuixBugs. In 2022 IEEE/ACM Intâ€™l Workshop on Automated Program\nRepair (APR). 69â€“75.\n[54] P. J. Rousseeuw. 1987. Silhouettes: A graphical aid to the interpretation and\nvalidation of cluster analysis. J. Comput. Appl. Math. 20 (1987), 53â€“65.\n[55] P. Runeson. 2006. A survey of unit testing practices. IEEE software 23, 4 (2006),\n22â€“29.\n[56] J. Savelka, A. Agarwal, C. Bogart, Y. Song, and M. Sakr. 2023. Can Generative Pre-\nTrained Transformers (GPT) Pass Assessments in Higher Education Programming\nCourses?. In Procâ€™d. of the 2023 Conf. on Innovation and Technology in Computer\nScience Education V. 1 (Turku, Finland) (ITiCSE 2023) . ACM, New York, NY, USA,\n117â€“123.\n[57] M. SchÃ¤fer, S. Nadi, A. Eghbali, and F. Tip. 2023. Adaptive Test Generation Using\na Large Language Model. arXiv preprint arXiv:2302.06527 (2023).\n[58] D. Serra, G. Grano, F. Palomba, F. Ferrucci, H. C. Gall, and A. Bacchelli. 2019. On\nthe effectiveness of manual and automatic unit test generation: ten years later.\nIn 2019 IEEE/ACM 16th Intâ€™l Conf. on Mining Software Repositories (MSR) . IEEE,\n121â€“125.\n[59] M. M. D. Shahabi, S. P. Badiei, S. E. Beheshtian, R. Akbari, and S. M. R. Moosavi.\n2017. On the performance of EvoPSO: A PSO based algorithm for test data\ngeneration in EvoSuite. In 2017 2nd Conf. on Swarm Intelligence and Evolutionary\nComputation (CSIEC) . IEEE, 129â€“134.\n[60] S. Shamshiri, J. M. Rojas, J. P. Galeotti, N. Walkinshaw, and G. Fraser. 2018. How\nDo Automatically Generated Unit Tests Influence Software Maintenance?. In\n2018 IEEE 11th Intâ€™l Conf. on Software Testing, Verification and Validation (ICST) .\n250â€“261.\nUsing Large Language Models to Generate JUnit Tests: An Empirical Study EASE 2024, 18â€“21 June, 2024, Salerno, Italy\n[61] Inbal Shani. 2023. Survey reveals AIâ€™s impact on the developer experience |The\nGitHub Blog. GitHub Blog (June 2023). https://github.blog/2023-06-13-survey-\nreveals-ais-impact-on-the-developer-experience/#methodology\n[62] M. L. Siddiq, S. H. Majumder, M. R. Mim, S. Jajodia, and J. C. S. Santos. 2022.\nAn Empirical Study of Code Smells in Transformer-based Code Generation\nTechniques. In 2022 IEEE 22nd Intâ€™l Working Conf. on Source Code Analysis and\nManipulation (SCAM) . 71â€“82.\n[63] M. L. Siddiq, A. Samee, S. R. Azgor, M. A. Haider, S. I. Sawraz, and J. C. S. Santos.\n2023. Zero-shot Prompting for Code Complexity Prediction Using GitHub Copilot.\nIn 2023 The 2nd Intl. Workshop on NL-based Software Engineering .\n[64] A. Svyatkovskiy, S. Lee, A. Hadjitofi, M. Riechert, J. Franco, and M. Allamanis.\n2021. Fast and memory-efficient neural code completion. In 2021 IEEE/ACM 18th\nIntâ€™l Conf. on Mining Software Repositories (MSR) . IEEE, 329â€“340.\n[65] Dave A. Thomas and A. Hunt. 2002. Mock Objects. IEEE Softw. 19 (2002), 22â€“24.\nhttps://doi.org/10.1109/MS.2002.1003449\n[66] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and N. Sundaresan. 2020.\nUnit test case generation with transformers and focal context. arXiv preprint\narXiv:2009.05617 (2020).\n[67] A. van Deursen, L. Moonen, A. van den Bergh, and G. Kok. 2001. Refactoring Test\nCode. In Procâ€™d. 2nd Intâ€™l Conf. on Extreme Programming and Flexible Processes in\nSoftware Engineering (XP2001) , M. Marchesi and G. Succi (Eds.).\n[68] T. VirgÃ­nio, L. Martins, R. Santana, A. Cruz, L. Rocha, H. Costa, and I. Machado.\n2021. On the test smells detection: an empirical study on the JNose test accuracy.\nJournal of Software Engineering Research and Development 9 (2021), 8â€“1.\n[69] Y. Wang, W. Wang, S. Joty, and S. C.H. Hoi. 2021. CodeT5: Identifier-aware Unified\nPre-trained Encoder-Decoder Models for Code Understanding and Generation.\nIn Procâ€™d. of the 2021 Conf. on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Online and Punta Cana, Dominican\nRepublic, 8696â€“8708.\n[70] P. Yin and G. Neubig. 2017. A Syntactic Neural Model for General-Purpose\nCode Generation. In Procâ€™d. of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) . Association for Computational\nLinguistics, Vancouver, Canada, 440â€“450.\n[71] A. Ziegler, E. Kalliamvakou, X. A. Li, A. Rice, D. Rifkin, S. Simister, G. Sittampalam,\nand E. Aftandilian. 2022. Productivity Assessment of Neural Code Completion.\nIn Procâ€™d. of the 6th ACM SIGPLAN Intâ€™l Symposium on Machine Programming\n(San Diego, CA, USA) (MAPS 2022) . ACM, New York, NY, USA, 21â€“29.",
  "topic": "Unit testing",
  "concepts": [
    {
      "name": "Unit testing",
      "score": 0.8311636447906494
    },
    {
      "name": "Computer science",
      "score": 0.7787784337997437
    },
    {
      "name": "Code coverage",
      "score": 0.7478348612785339
    },
    {
      "name": "Correctness",
      "score": 0.6838544607162476
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6046888828277588
    },
    {
      "name": "Code (set theory)",
      "score": 0.5905845761299133
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5828861594200134
    },
    {
      "name": "Programming language",
      "score": 0.572256863117218
    },
    {
      "name": "Java",
      "score": 0.5187185406684875
    },
    {
      "name": "Code generation",
      "score": 0.47121232748031616
    },
    {
      "name": "Test (biology)",
      "score": 0.4583297669887543
    },
    {
      "name": "Test case",
      "score": 0.44536370038986206
    },
    {
      "name": "Operating system",
      "score": 0.16568100452423096
    },
    {
      "name": "Machine learning",
      "score": 0.13813841342926025
    },
    {
      "name": "Software",
      "score": 0.11633774638175964
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Regression analysis",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63169043",
      "name": "United International University",
      "country": "BD"
    },
    {
      "id": "https://openalex.org/I130769515",
      "name": "Pennsylvania State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I107639228",
      "name": "University of Notre Dame",
      "country": "US"
    }
  ]
}