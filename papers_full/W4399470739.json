{
  "title": "Training Compute-Optimal Protein Language Models",
  "url": "https://openalex.org/W4399470739",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2758817447",
      "name": "Xingyi Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2037222934",
      "name": "Bo Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2119656127",
      "name": "Pan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123886867",
      "name": "Jing Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098032575",
      "name": "Jie Tang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2105801009",
      "name": "Le Song",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2758817447",
      "name": "Xingyi Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2037222934",
      "name": "Bo Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119656127",
      "name": "Pan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123886867",
      "name": "Jing Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098032575",
      "name": "Jie Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105801009",
      "name": "Le Song",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4315705838",
    "https://openalex.org/W3183713734",
    "https://openalex.org/W4288804596",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4390690464",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3132890345",
    "https://openalex.org/W4383550741",
    "https://openalex.org/W2550969987",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W4226153346",
    "https://openalex.org/W4281758439",
    "https://openalex.org/W4296032638",
    "https://openalex.org/W4320722432",
    "https://openalex.org/W4288369446",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W3212854871",
    "https://openalex.org/W4317374308",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W2021235830",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W4385255463",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W4281481109",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4387113741",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3010746544",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4387356039",
    "https://openalex.org/W4387559790",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W2169267759",
    "https://openalex.org/W2903697572",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W3200987621",
    "https://openalex.org/W4281790889",
    "https://openalex.org/W2984894304",
    "https://openalex.org/W4378505278",
    "https://openalex.org/W3176037658",
    "https://openalex.org/W4392351837",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W4389472984",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W2051210555",
    "https://openalex.org/W3170943566",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4313430583",
    "https://openalex.org/W2098914548",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W3199241049",
    "https://openalex.org/W4304697829",
    "https://openalex.org/W4307079190",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4210840673",
    "https://openalex.org/W4312197262",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W4394698525",
    "https://openalex.org/W2997234557",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W4392271030",
    "https://openalex.org/W4319071809"
  ],
  "abstract": "Abstract We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited. Most models are trained with extensive compute resources until performance gains plateau, focusing primarily on increasing model sizes rather than optimizing the efficient compute frontier that balances performance and compute budgets. Our investigation is grounded in a massive dataset consisting of 939 million protein sequences. We trained over 300 models ranging from 3.5 million to 10.7 billion parameters on 5 to 200 billion unique tokens, to investigate the relations between model sizes, training token numbers, and objectives. First, we observed the effect of diminishing returns for the Causal Language Model (CLM) and that of overfitting for the Masked Language Model (MLM) when repeating the commonly used Uniref database. To address this, we included metagenomic protein sequences in the training set to increase the diversity and avoid the plateau or overfitting effects. Second, we obtained the scaling laws of CLM and MLM on Transformer, tailored to the specific characteristics of protein sequence data. Third, we observe a transfer scaling phenomenon from CLM to MLM, further demonstrating the effectiveness of transfer through scaling behaviors based on estimated Effectively Transferred Tokens. Finally, to validate our scaling laws, we compare the large-scale versions of ESM-2 and PROGEN2 on downstream tasks, encompassing evaluations of protein generation as well as structure- and function-related tasks, all within less or equivalent pre-training compute budgets.",
  "full_text": null,
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.9085736274719238
    },
    {
      "name": "Computer science",
      "score": 0.6794759631156921
    },
    {
      "name": "Language model",
      "score": 0.6177366971969604
    },
    {
      "name": "Scaling",
      "score": 0.5930770039558411
    },
    {
      "name": "Security token",
      "score": 0.5738768577575684
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4986717700958252
    },
    {
      "name": "Training set",
      "score": 0.42762336134910583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40027308464050293
    },
    {
      "name": "Machine learning",
      "score": 0.3743700087070465
    },
    {
      "name": "Mathematics",
      "score": 0.13573157787322998
    },
    {
      "name": "Artificial neural network",
      "score": 0.10306364297866821
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}