{
    "title": "Multi-source transformer with combined losses for automatic post editing",
    "url": "https://openalex.org/W2902409908",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2902209665",
            "name": "Amirhossein Tebbifakhr",
            "affiliations": [
                "University of Trento",
                "Fondazione Bruno Kessler"
            ]
        },
        {
            "id": "https://openalex.org/A3045597939",
            "name": "Ruchit Agrawal",
            "affiliations": [
                "University of Trento",
                "Fondazione Bruno Kessler"
            ]
        },
        {
            "id": "https://openalex.org/A2060730313",
            "name": "Matteo Negri",
            "affiliations": [
                "Fondazione Bruno Kessler"
            ]
        },
        {
            "id": "https://openalex.org/A3137935677",
            "name": "Marco Turchi",
            "affiliations": [
                "Fondazione Bruno Kessler"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2791510479",
        "https://openalex.org/W2575583396",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W2964034111",
        "https://openalex.org/W2760656271",
        "https://openalex.org/W2963261349",
        "https://openalex.org/W2963463964",
        "https://openalex.org/W2143177362",
        "https://openalex.org/W2962678612",
        "https://openalex.org/W2758010889",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2149327368",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2016856586",
        "https://openalex.org/W2176263492",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W2903193068",
        "https://openalex.org/W4245882962",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2758074402",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2251994258",
        "https://openalex.org/W2963816901",
        "https://openalex.org/W2758520323"
    ],
    "abstract": "Recent approaches to the Automatic Post-editing (APE) of Machine Translation (MT) have shown that best results are obtained by neural multi-source models that correct the raw MT output by also considering information from the corresponding source sentence. To this aim, we present for the first time a neural multi-source APE model based on the Transformer architecture. Moreover, we employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics used for the task. These are the main features of our submissions to the WMT 2018 APE shared task, where we participated both in the PBSMT subtask (i.e. the correction of MT outputs from a phrase-based system) and in the NMT subtask (i.e. the correction of neural outputs). In the first subtask, our system improves over the baseline up to -5.3 TER and +8.23 BLEU points ranking second out of 11 submitted runs. In the second one, characterized by the higher quality of the initial translations, we report lower but statistically significant gains (up to -0.38 TER and +0.8 BLEU), ranking first out of 10 submissions.",
    "full_text": "Proceedings of the Third Conference on Machine Translation (WMT), V olume 2: Shared Task Papers, pages 846–852\nBelgium, Brussels, October 31 - Novermber 1, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/W18-64098\nMulti-source Transformer with Combined Losses\nfor Automatic Post-Editing\nAmirhossein Tebbifakhr1,2, Ruchit Agrawal1,2, Matteo Negri1, Marco Turchi1\n1 Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento - Italy\n2 University of Trento, Italy\n{atebbifakhr,ragrawal,negri,turchi}@fbk.eu\nAbstract\nRecent approaches to the Automatic Post-\nediting (APE) of Machine Translation (MT)\nhave shown that best results are obtained by\nneural multi-source models that correct the\nraw MT output by also considering informa-\ntion from the corresponding source sentence.\nTo this aim, we present for the ﬁrst time a\nneural multi-source APE model based on the\nTransformer architecture. Moreover, we em-\nploy sequence-level loss functions in order to\navoid exposure bias during training and to be\nconsistent with the automatic evaluation met-\nrics used for the task. These are the main fea-\ntures of our submissions to the WMT 2018\nAPE shared task, where we participated both\nin the PBSMT subtask (i.e. the correction of\nMT outputs from a phrase-based system) and\nin the NMT subtask (i.e. the correction of neu-\nral outputs). In the ﬁrst subtask, our system\nimproves over the baseline up to -5.3 TER and\n+8.23 BLEU points ranking second out of 11\nsubmitted runs. In the second one, character-\nized by the higher quality of the initial transla-\ntions, we report lower but statistically signiﬁ-\ncant gains (up to -0.38 TER and +0.8 BLEU),\nranking ﬁrst out of 10 submissions.\n1 Introduction\nThe purpose of Automatic Post-Editing (APE) is\nto correct the raw output of a Machine Transla-\ntion system by learning from human corrections.\nSince the inner workings of MT engines are often\nnot accessible (e.g. by users relying on Google\nTranslate), hence impossible to modify and im-\nprove, APE becomes a solution to enhance the\nquality of the translated segments. Good solutions\nto the problem have high potential in the transla-\ntion industry, where better translation means lower\ncosts for human revision and where the adapta-\ntions of third-party, general-purpose systems to\nnew projects is a major need.\nIn the last few years, the APE shared tasks\nat WMT (Bojar et al., 2015, 2016, 2017) have\nrenewed the interests in this topic and boosted\nthe technology around it. Moving from the\nphrase-based approaches used in the ﬁrst edi-\ntions of the task (Chatterjee et al., 2015), last\nyear the multi-source neural models (Chatterjee\net al., 2017; Junczys-Dowmunt and Grundkiewicz,\n2017; Hokamp, 2017) have shown their capability\nto signiﬁcantly improve the output of a PBSMT\nsystem. These APE systems shared several fea-\ntures and implementation choices, namely: 1) an\nRNN-based architecture, 2) the use of large arti-\nﬁcial corpora for training, 3) model ensembling\ntechniques, 4) parameter optimization based on\nMaximum Likelihood Estimation (MLE) and 5)\nvocabulary reduction using the Byte Pair Encod-\ning (BPE) technique. Although they achieve good\nperformance and impressive translation quality\nimprovements, some of these techniques are not\noptimal for the actual deployment of APE tech-\nnology in the translation industry. The main rea-\nsons are the long time required for model train-\ning and the high maintenance costs of complex\narchitectures that combine multiple models. To\nmake APE solutions usable and useful for the in-\ndustrial market, our submissions focus on the de-\nvelopment of an end-to-end system that does not\nrequire multiple models and external components\n(e.g. hypothesis re-ranker), but leverages a fast to\ntrain architecture, effective pre-processing meth-\nods and task-speciﬁc losses to boost performance.\nOur main contributions are:\n•We adapt the Transformer (Vaswani et al.,\n2017) to the APE problem, so that multi-\nple encoders can be exploited to leverage in-\nformation both from the MT output to be\ncorrected and from the corresponding source\nsentence (multi-source encoding).\n846\n•We explore different strategies for combining\ntoken and sentence level losses.\n•We apply ad hoc pre-processing for the\nGerman language by re-implementing the\npipeline used by the best system at the\nWMT‘17 Translation task (Huck et al.,\n2017).\n•In addition to the artiﬁcial data released\nby (Junczys-Dowmunt and Grundkiewicz,\n2016), we make extensive use of a synthetic\ncorpus of 7.2M English-German triplets (Ne-\ngri et al., 2018), which was provided by the\norganizers as additional training material.\nWe participated in both the APE‘18 subtasks\nwith positive results. In the PBSMT subtask our\ntop run improves the baseline up to -5.3 TER and\n+8.23 BLEU points (ranking second out of 11 sub-\nmissions) while, in the NMT subtask, it achieves a\n-0.38 TER and +0.8 BLEU improvement (ranking\nﬁrst out of 10 submissions).\n2 Multi-source Transformer Network\nThe Transformer network (Vaswani et al., 2017),\nlike most of the sequence-to-sequence models,\nfollows an encoder-decoder architecture. It\nuses stacked layers for the encoder and the de-\ncoder. The encoder layers consist of a multi-head\nself-attention, followed by a position-wise feed-\nforward network. The decoder layers have an ex-\ntra multi-head encoder-decoder attention after the\nmulti-head self-attention sub-layer. Also, a soft-\nmax normalization is applied to the output of the\nlast layer in the decoder to generate a probabil-\nity distribution over the target vocabulary. Since\nthere is no recurrence in this architecture, a posi-\ntional encoding is added to both the source and the\ntarget word embeddings in order to empower the\nmodel to capture the position of the words. More\nformally, the positional encoding is deﬁned as fol-\nlows:\nPE(pos,2i) = sin(pos/100002i/dmodel )\nPE(pos,2i+ 1) = cos(pos/100002i/dmodel )\nwhere posis the position of the word in the sen-\ntence, iis the dimension of the vector, and dmodel\nis the dimensionality of the word embeddings.\nThe attention is a mapping from a query ( Q), a\nkey (K), and a value ( V) to an output vector. In\nTransformer, the attention is based on dot-product\nattention which is deﬁned as follow:\nAttention(Q,K,V ) = softmax(QKT /\n√\ndk)V\nwhere dk is added as a scaling factor for improv-\ning the numerical stability, which is equal to the\ndimensionality of the key matrix. The multi-head\nattention receives h different representations of\n(Q,K,V ), which makes it possible to learn dif-\nferent relationships between information coming\nfrom different positions simultaneously. It is com-\nputed as follows:\nMH(Q,K,V ) = Concat(head1,...,head h)WO\nwhere his number of heads and WO is a param-\neter matrix with hdv*dmodel dimension. In Trans-\nformer, the multi-head attention is used in two dif-\nferent ways: encoder-decoder and self-attention.\nIn the self-attention, in both the encoder and the\ndecoder, the Q, K, and V matrices are coming\nfrom the previous layer, while in the encoder-\ndecoder attention, Q matrix comes from the pre-\nvious layer, and the K and V matrices come from\nthe encoder.\nIn order to encode the source sentence in addi-\ntion to the MT output, we employ the multi-source\nmethod by Zoph and Knight (2016). Our model\nconsists of two encoders, one for the source sen-\ntence and one for the MT output. The outputs of\nthese two encoders are concatenated and passed as\nthe key in the attention. This helps to have a better\nrepresentation, leading to a more effective atten-\ntion at decoding time.\n3 Sequence-Level Loss Function\nFor training the model, most of the approaches in\nsequence-to-sequence modeling try to maximize\nthe likelihood over the training data. In this sce-\nnario, the loss function is a token-level loss deﬁned\nas:\nLMLE = −\nN∑\nn=1\np(yn|y<n,x)\nwhere p(yn|y<n,x) is the probability of generat-\ning the target word in the n-th position. Ranzato\net al. (2015), however, indicate two drawbacks for\nMaximum Likelihood Estimation (MLE). First,\nduring training, the previous words passed to the\ndecoder are always chosen from the ground-truth.\nHowever, the fact that at test time the previous\n847\nwords are chosen from the model distribution, re-\nsults in a bias called exposure bias. Such bias\nmakes the model unable to recover from the er-\nrors made in the decoding step, which easily have\na cumulative catastrophic effect. Second, using\nMLE as loss function, the model is optimized\nto maximize the probability of the training data,\nwhile the performance of the model is evaluated\nby the sequence-level evaluation metrics (TER and\nBLEU in the case of the APE task). In order\nto overcome the mentioned drawbacks, follow-\ning Minimum Risk Training (MRT) introduced by\nShen et al. (2016), we use a risk function which is\ndeﬁned as:\nRMRT =\n∑\ny∈S(x)\nP(y|x)∑\ny′∈S(x) P(y′|x)∆(y)\nwhere S(x) is a set of sampled hypotheses from\nthe model for the input sentence x, P(y|x) is\nthe probability of the sampled hypothesis, and\n∆(y) is a cost value for generating the sample\ny, e.g. ∆(y) = −BLEU(y). Following Sen-\nnrich et al. (2016), we employ negative smoothed\nsentence-level BLEU (Papineni et al., 2002; Chen\nand Cherry, 2014) for computing the cost func-\ntion.1\n4 Data Pre-processing\nIn order to reduce the vocabulary size, on the Ger-\nman MT output and post-edits we apply our re-\nimplementation of the word segmentation method\nintroduced by Huck et al. (2017). It consists of\nthree different steps:\n1. Sufﬁxes are separated from the word stems\nby using a modiﬁed version of snowball\nstemming, which separates and keeps the suf-\nﬁxes instead of stripping them;\n2. The output of the previous step is passed to\nthe empirical compound splitter described in\n(Koehn and Knight, 2003), which is run with\nthe same parameters reported in (Huck et al.,\n2017);\n3. The output of the previous step is segmented\nwith Byte Pair Encoding (BPE) (Sennrich\net al., 2016).\n1Although TER (Snover et al., 2006) is the primary evalu-\nation metric for the task, we opted for BLEU since, according\nto (Shen et al., 2016), optimizing with this metric gives better\nresults also when evaluation is done with TER.\nFor the English source sentences, we only use BPE\nto reduce the vocabulary size.\n5 Experimental Setting\n5.1 Data\nTo train our models, we used both the in-domain\ndata released by APE the task organizers and the\nsynthetic data provided as additional training ma-\nterial.\nIn-domain Data. In-domain data consist of\nEnglish-German (SRC, MT, PE) triplets in which\nthe MT element (a German translation of the En-\nglish SRC sentence) has been generated by “black-\nbox” MT systems: a phrase-based one for the PB-\nSMT subtask and a neural one for the NMT sub-\ntask. In both cases, the post-edit element (PE)\nis a correction of the target made by professional\npost-editors. The PBSMT training set, which is\nthe largest one, comprises 28K triplets. The NMT\ntraining set, is smaller in size and contains 13K\ninstances. From the two training corpora, we ex-\ntracted 1K triplets to be used as development set to\ncompare the performance of different models dur-\ning training.\nSynthetic data. Since building neural APE\nmodels heavily relies on the availability of large\ntraining data, we took advantage of the following\ntwo corpora:\n•the eSCAPE corpus (Negri et al., 2018),\nwhich contains 7.2M English-German\ntriplets for each MT paradigm (i.e. 7.2M\nphrase-based and neural translations of the\nsame source sentences). It has been gener-\nated from a parallel English-German corpus,\nby taking the target sentences as artiﬁcial\npost-edits and the machine-translated source\nsentences as MT elements of each triplet.\n•The artiﬁcial corpus provided by Junczys-\nDowmunt and Grundkiewicz (2016), which\ncontains 4.0M English-German triplets gen-\nerated by applying a round-trip translation\nprotocol to German monolingual data.\nBefore applying the pre-processing described in\nSection 4 to the eSCAPE data, we performed the\nfollowing two cleaning steps:\n1. We removed the triplets in which the length\nratio between the source sentence and the\n848\npost-edit is too different from the average in\nthe corpus;\n2. We run a language identiﬁer 2 in order to re-\nmove the triplets having a non-English source\nsentence or a non-German post-edit.\nThe application of these two cleaning steps re-\nsulted in the removal of approximately 600K in-\nstances from the eSCAPE corpus.\n5.2 Evaluation Metrics\nIn order to evaluate our models, we use the two au-\ntomatic evaluation metrics: i) TER which is com-\nputed based on edit distance (Snover et al., 2006)\nand ii) BLEU which is the geometric mean of n-\ngram precisions multiplied to the brevity penalty\n(Papineni et al., 2002).\n5.3 Hyperparameters\nWe set the number of merging rules to 32K for\napplying BPE in the pre-processing steps. We\nemploy OpenNMT-tf toolkit (Klein et al., 2017)\nfor our implementation, by using 512 dimensions\nfor word embeddings, 4 layers for both the en-\ncoders and the decoder with 512 units, and feed-\nfroward dimension of 1,024. In order to avoid\nover-ﬁtting, we use attention and residual dropout\nby setting the dropout probability to 0.1, along\nwith the label-smoothing with parameter equal to\n0.1. For training using MLE, we use the Adam op-\ntimizer (Kingma and Ba, 2014) with batch size of\n8,192 tokens, learning rate of 2.0 and the warm-\nup strategy introduced by (Vaswani et al., 2017)\nwith the warm-up steps equals to 8,000. For train-\ning using MRT, we use stochastic gradient descent\noptimizer with the batch size of 4,096 tokens. We\nalso employ the beam search with beam width of\n5 to sample hypotheses from the model.\n6 Results\nFor both the subtasks, we train six different mod-\nels. The performance of these models on the PB-\nSMT and NMT development sets is reported in Ta-\nbles 1 and 2.\nGeneric. First, we train a model using the union\nof the (out-domain) synthetic datasets. As ex-\npected, the performance of this model in both sub-\ntasks is lower than the baseline. We only train this\n2For this purpose we used the language detector\navailable at: https://github.com/optimaize/\nlanguage-detector.\nmodel as initial generic model in order to ﬁne-tune\nit using the in-domain data.\nMLE. Using MLE, we ﬁne-tune the generic\nmodel on the corresponding in-domain data for\neach subtask. For the PBSMT subtask, this model\nachieves a -6.73 TER and +9.94 BLEU improve-\nment over the baseline. The gain is much lower for\nthe NMT subtask (-0.33 TER and +0.85 BLEU),\nconﬁrming that, together with the availability of\nless training data, the quality of the underlying\nNMT system has left little space for improvement.\nMRT. We continue the training by using MRT\nin two ways: i) by adding the reference to the\nset of hypotheses sampled from the model and\nii) without adding the reference. In contrast with\nShen et al. (2016), who suggest to add the refer-\nence to the sampled set of hypotheses, we found\nthat adding the reference is harmful. Actually, by\nadding the reference to the sample, the other hy-\npotheses are considered as poor alternatives, since\nthey have a lower BLEU score. Nevertheless,\nthese samples usually have good quality and a con-\nsiderable overlap with the reference. Therefore,\nupdating the model in the direction of decreas-\ning the probability of these hypotheses is does not\nseem a promising direction.\nMRT + MLE. In order to avoid this problem\nand take advantage of the reference, we re-run the\nprevious learning step using the linear combina-\ntion of the two loss functions. Formally, we use\nthe following loss function:\nLcomb = αLMLE + (1 −α)RMRT\nwhere αis set to 0.5 to give equal importance to\nthe two components 3 The results show that com-\nbining the two loss functions makes the model able\nto learn also from the reference without ignoring\nthe contribution of the other hypotheses.\nOur model outperforms the best performing\nsystem at the last round of the shared task (Chat-\nterjee et al., 2017), with improvements up to -1.27\nTER and +1.23 BLEU on the PBSMT develop-\nment set. Although we are using more out-of-\ndomain data, it is interesting to note that these\nscores are obtained with a much simpler architec-\nture, which does not require to ensemble nmod-\nels and to train a re-ranker. Since using only\n3We leave for future work the empirical estimation of op-\ntimal values for α.\n849\nModel Reference TER BLEU\nGeneric - 25.00 61.69\nMLE - 18.08 72.86\nMRT Yes 18.02 72.91\nMRT No 18.44 73.05\nMLE + MRT Yes 17.99 72.99\nMLE + MRT No 17.95 73.121\nTable 1: Results of the multi-source Transformer with\nspeciﬁc losses on the PBSMT outputs. The perfor-\nmance of the MT baseline are 24.81 TER and 62.92\nBLEU. Superscript 1 denotes that improvement over\nMLE is statistically signiﬁcant.\nModel Reference TER BLEU\nGeneric - 17.35 72.55\nMLE - 14.75 77.61\nMRT Yes 14.81 77.57\nMRT No 14.78 77.74\nMRT + MLE Yes 14.75 77.68\nMRT + MLE No 14.68 77.68\nTable 2: Results of the multi-source Transformer with\nspeciﬁc losses on the NMT outputs. The performance\nof the MT baseline are 15.08 TER and 76.76 BLEU.\nMRT produced a better BLEU score in NMT sub-\ntask, we submitted the best model using only MRT\nwithout reference as our Primary submission, and\nthe best model using MRT+MLE as our Con-\ntrastive submission.\n7 Test Set Results\nThe performance of the primary and contrastive\nAPE systems on the test set for both the subtasks\nis reported in Table 3. Apart from a minimal varia-\ntion in the TER scores for the PBSMT subtask, the\nresults conﬁrm what previously seen on the devel-\nopment set. Our APE systems are able to signiﬁ-\ncantly improve the quality of the PBSMT outputs\nby achieving a gain of -5.62 TER and +8.23 BLEU\npoints.\nWhen post-editing the output of a NMT sys-\ntem, the gains are smaller (-0.38 TER and +0.8\nBLEU). This is somehow expected since the role\nof an APE system is to ﬁx MT errors: in pres-\nence of higher quality translations (from PBSMT\nto NMT: -7.4 TER and +12.54 BLEU) there are\nless errors to correct and the chance to apply un-\nnecessary changes is higher. Apart from that, our\nAPE systems are able to improve the NMT outputs\nshowing that, even in this challenging condition,\nTask System TER BLEU\nPBSMT\nBaseline 24.24 62.99\nPrimary 18.94 71.22\nContrastive 18.62 71.04\nNMT\nBaseline 16.84 74.73\nPrimary 16.46 75.53\nContrastive 16.55 75.38\nTable 3: Submissions at the WMT APE shared task.\nAPE is useful.\n8 Conclusion\nWe presented the FBK’s submissions to the APE\nshared task at WMT 2018. Our models extend a\nTransformer-based architecture by: 1) leveraging\nmulti-source inputs consisting in the source and\nMT texts and 2) taking advantage of combined to-\nken and task-speciﬁc losses. Moreover, an ad hoc\ntext pre-processing for the German language and\nmore artiﬁcial data are exploited to help the train-\ning of the model. The resulting systems show large\ngains in performance when post-editing the PB-\nSMT translations (our top-submission ranks sec-\nond in this subtask), while minimal improvements\nare obtained when correcting the NMT outputs\n(still, our top-run ranks ﬁrst in this subtask). These\ndifferences in performance strongly depend on the\ninitial quality of the MT outputs that signiﬁcantly\nchanges from the PBSMT to the NMT system.\nIt is worth to remark that our implementation\nchoices were mainly driven by the needs of a trans-\nlation market in which simple solutions that are\neasy to maintain are always preferable to complex\narchitectures. In this direction, our APE systems\nconsist of a single network that can be trained in an\nend-to-end fashion, without recourse to ensembles\nof multiple models or the concatenation of com-\nponents (e.g. hypothesis re-ranker) that have to be\ntrained independently.\nAcknowledgments\nWe gratefully acknowledge the support of\nNVIDIA Corporation with the donation of the Ti-\ntan Xp GPU used for this research.\nReferences\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\n850\nLogacheva, Christof Monz, Matteo Negri, Matt\nPost, Raphael Rubino, Lucia Specia, and Marco\nTurchi. 2017. Findings of the 2017 conference\non machine translation (wmt17). In Proceedings\nof the Second Conference on Machine Translation,\nVolume 2: Shared Task Papers, pages 169–214,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck,\nAntonio Jimeno Yepes, Philipp Koehn, Varvara\nLogacheva, Christof Monz, Matteo Negri, Aure-\nlie Neveol, Mariana Neves, Martin Popel, Matt\nPost, Raphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the 2016 conference\non machine translation. In Proceedings of the First\nConference on Machine Translation, pages 131–\n198, Berlin, Germany. Association for Computa-\ntional Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nBarry Haddow, Matthias Huck, Chris Hokamp,\nPhilipp Koehn, Varvara Logacheva, Christof Monz,\nMatteo Negri, Matt Post, Carolina Scarton, Lucia\nSpecia, and Marco Turchi. 2015. Findings of the\n2015 workshop on statistical machine translation.\nIn Proceedings of the Tenth Workshop on Statistical\nMachine Translation, pages 1–46, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nRajen Chatterjee, M. Amin Farajian, Matteo Negri,\nMarco Turchi, Ankit Srivastava, and Santanu Pal.\n2017. Multi-source neural automatic post-editing:\nFbk’s participation in the wmt 2017 ape shared task.\nIn Proceedings of the Second Conference on Ma-\nchine Translation, pages 630–638. Association for\nComputational Linguistics.\nRajen Chatterjee, Marco Turchi, and Matteo Negri.\n2015. The fbk participation in the wmt15 automatic\npost-editing shared task. In Proceedings of the Tenth\nWorkshop on Statistical Machine Translation, pages\n210–215, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nBoxing Chen and Colin Cherry. 2014. A systematic\ncomparison of smoothing techniques for sentence-\nlevel bleu. In Proceedings of the Ninth Workshop on\nStatistical Machine Translation, pages 362–367.\nChris Hokamp. 2017. Ensembling factored neural ma-\nchine translation models for automatic post-editing\nand quality estimation. In Proceedings of the Sec-\nond Conference on Machine Translation, Volume 2:\nShared Task Papers, pages 647–654, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nMatthias Huck, Fabienne Braune, and Alexander\nFraser. 2017. Lmu munich’s neural machine trans-\nlation systems for news articles and health informa-\ntion texts. In Proceedings of the Second Conference\non Machine Translation, Volume 2: Shared Task Pa-\npers, pages 315–322, Copenhagen, Denmark. Asso-\nciation for Computational Linguistics.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2016. Log-linear combinations of monolingual and\nbilingual neural machine translation models for au-\ntomatic post-editing. In Proceedings of the First\nConference on Machine Translation: Volume 2,\nShared Task Papers, pages 751–758. Association for\nComputational Linguistics.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2017. The amu-uedin submission to the wmt 2017\nshared task on automatic post-editing. In Proceed-\nings of the Second Conference on Machine Trans-\nlation, Volume 2: Shared Task Papers, pages 639–\n646, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nD. P. Kingma and J. Ba. 2014. Adam: A Method for\nStochastic Optimization. ArXiv e-prints.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean\nSenellart, and Alexander Rush. 2017. Opennmt:\nOpen-source toolkit for neural machine translation.\nIn Proceedings of ACL 2017, System Demonstra-\ntions, pages 67–72. Association for Computational\nLinguistics.\nPhilipp Koehn and Kevin Knight. 2003. Empirical\nmethods for compound splitting. In Proceedings\nof the Tenth Conference on European Chapter of\nthe Association for Computational Linguistics - Vol-\nume 1, EACL ’03, pages 187–193, Stroudsburg, PA,\nUSA. Association for Computational Linguistics.\nMatteo Negri, Marco Turchi, Rajen Chatterjee, and\nNicola Bertoldi. 2018. eSCAPE: a Large-scale\nSynthetic Corpus for Automatic Post-Editing. In\nProceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting on Association for Com-\nputational Linguistics, ACL ’02, pages 311–318,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nM. Ranzato, S. Chopra, M. Auli, and W. Zaremba.\n2015. Sequence Level Training with Recurrent Neu-\nral Networks. ArXiv e-prints.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\n851\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua\nWu, Maosong Sun, and Yang Liu. 2016. Minimum\nrisk training for neural machine translation. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1683–1692. Association for Compu-\ntational Linguistics.\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A Study of\nTranslation Edit Rate with Targeted Human Anno-\ntation. In Proceedings of Association for Machine\nTranslation in the Americas, pages 223–231, Cam-\nbridge, Massachusetts, USA.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran As-\nsociates, Inc.\nBarret Zoph and Kevin Knight. 2016. Multi-\nsource neural translation. arXiv preprint\narXiv:1601.00710.\n852"
}