{
  "title": "Large Language Models for Multilingual Slavic Named Entity Linking",
  "url": "https://openalex.org/W4386566975",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3118135833",
      "name": "Rinalds Viksna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A316444232",
      "name": "Inguna Skadiņa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2740065075",
      "name": "Daiga Deksne",
      "affiliations": [
        "University of Latvia"
      ]
    },
    {
      "id": "https://openalex.org/A102454172",
      "name": "Roberts Rozis",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3049407327",
    "https://openalex.org/W3156550404",
    "https://openalex.org/W2973071945",
    "https://openalex.org/W1970961429",
    "https://openalex.org/W3155012289",
    "https://openalex.org/W4295887562",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W3039695075",
    "https://openalex.org/W1964189668",
    "https://openalex.org/W1574911124",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2973121920",
    "https://openalex.org/W2592451013",
    "https://openalex.org/W2740074898",
    "https://openalex.org/W3156797688",
    "https://openalex.org/W3105303309",
    "https://openalex.org/W2972710875",
    "https://openalex.org/W3101260801",
    "https://openalex.org/W3155417043",
    "https://openalex.org/W99937103",
    "https://openalex.org/W3125670107",
    "https://openalex.org/W2252276314",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2739922228",
    "https://openalex.org/W4253336001",
    "https://openalex.org/W4241096217"
  ],
  "abstract": "This paper describes our submission for the 4th Shared Task on SlavNER on three Slavic languages - Czech, Polish and Russian. We use pre-trained multilingual XLM-R Language Model (Conneau et al., 2020) and fine-tune it for three Slavic languages using datasets provided by organizers. Our multilingual NER model achieves 0.896 F-score on all corpora, with the best result for Czech (0.914) and the worst for Russian (0.880). Our cross-language entity linking module achieves F-score of 0.669 in the official SlavNER 2023 evaluation.",
  "full_text": "Proceedings of the 9th Workshop on Slavic Natural Language Processing 2023 (SlavicNLP 2023), pages 172–178\nMay 6, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models for Multilingual Slavic Named Entity Linking\nRinalds V¯ıksna1,2 and Inguna Skadin, a1,2 and Daiga Deksne1,2 and Roberts Rozis1\n1 Tilde, Vien¯ıbas gatve 75a, Riga, Latvia\n2 Faculty of Computing, University of Latvia, Rain, a bulv. 29, Riga, Latvia\n{Firstname.Lastname}@tilde.lv\nAbstract\nThis paper describes our submission for the\n4th Shared Task on SlavNER on three Slavic\nlanguages - Czech, Polish, and Russian. We\nuse pre-trained multilingual XLM-R Language\nModel and fine-tune it for three Slavic lan-\nguages using datasets provided by organizers.\nOur multilingual NER model achieves a 0.896\nF-score on all corpora, with the best result\nfor Czech (0.914) and the worst for Russian\n(0.880). Our cross-language entity linking mod-\nule achieves an F-score of 0.669 in the official\nSlavNER 2023 evaluation.\n1 Introduction\nThe 4th edition of Shared Task address three Slavic\nlanguages: Czech, Polish, and Russian, and five\ntypes of named entities (persons, locations, organi-\nzations, events, and products). All languages are\nhighly inflective and have a rather free word order.\nThus named entity normalization task faces an ad-\nditional challenge in the case of the normalization\nof multi-word expressions (MWE).\nIn our submission, we continue experiments with\nXLM-R Language Model (Conneau et al., 2020)\nwhich has demonstrated the best result in previ-\nous shared task (Ferreira et al., 2021). We also\nelaborate on the normalization step for MWEs by\napplying syntax-based noun phrase normalization\ntool to reach higher accuracy in named entity (NE)\nnormalization and linking tasks. Finally, we also\nimprove entity linking by better algorithms for link-\ning entity variants on a document level using string\nsimilarity, proximity, and type attributes.\nThe paper is organized as follows. We start with\nan overview of the data preparation step (Section 3)\nand the overall architecture of the system (Section\n4). Then, we present each step in our workflow -\nmention detection, entity normalization, and entity\nlinking. We conclude the paper with a subset of\nresults and a discussion (Section 8).\n2 Related Work\nThe shared task on Slavic multilingual named entity\nrecognition, normalization, and linking (SlavNER)\nhas been organized since 2017 (Piskorski et al.,\n2017). Only two systems were submitted for\nthe First SlavNER. The best result for NER was\nachieved for Polish (F-score of 66.6), while for\ncross-lingual entity matching only 9 F1 points were\nreached (Mayfield et al., 2017). Authors of this\nsystem annotated parallel English-target language\ndatasets using an English NER and projected an-\nnotations to the target language. A target language\ntagger was then trained using inferred datasets.\nSeven teams submitted systems to the 2 nd\nSlavNER (Piskorski et al., 2019). The three best\nsystems (RIS (Arkhipov et al., 2019), CogComp\n(Tsygankova et al., 2019) and IIUWR.PL (Pisko-\nrski et al., 2019)) used BERT for the NER task.\nThe best model, CogComp, yields an F-measure of\n91% according to the shared task organizers. The\ncross-lingual entity linking results also have im-\nproved significantly: the best-performing model,\nIIUWR.PL yields the F-measure of 45%.\nSix teams submitted their systems to the 3 rd\nSlavNER (Piskorski et al., 2021). Overall NER\ntask results were lower when compared to the\n2nd SlavNER. The best system, Priberam (Fer-\nreira et al., 2021), achieved F-measure of 85.7%\nfor the relaxed partial evaluation. Priberam used\nXLM-R Large model, a character-level embedding\nmodel, and a biaffine classifier for NER task. For\ncross-lingual entity linking, the best-performing\nmodel, TLD (V¯ıksna and Skadina, 2021), achieved\nan F-measure of 50.4% using LaBSE (Feng et al.,\n2022) embeddings to align entities according to\npre-defined thresholds.\n3 Data Preparation\nThe data provided by the SlavNER task organiz-\ners contains annotations for five classes of entities:\n172\nevent (EVT), location (LOC), person (PER), or-\nganization (ORG), and product (PRO). For NER\nsystem training we convert data into a conll2003-\nlike format. We do not use the data from the\nBSNLP2017 shared task (Piskorski et al., 2017), as\nit has 4 named entity classes which are inconsistent\nwith the rest of SlavNER data (Prelevikj and Zitnik,\n2021), and has shown to hurt the performance of\nNER models for this task (Ferreira et al., 2021). In\naddition to the dataset provided by the SlavNER\ntask organizers(Piskorski et al., 2019, 2021), we\nuse the following datasets in our experiments:\nKPWr (Oleksy et al., 2019) contains Polish texts\nlabeled using 82 classes of entities, which we map\nto the 5 classes used in the SlavNER task.\nNKJP (Przepiórkowski, 2012) is National Cor-\npus of Polish, tagged with fine-grained NEs. We\nuse entity types PER (’forename’, ’surname’), LOC\n(placeName, geogName), and ORG (orgName).\npoleval2018 (Ogrodniczuk and Łukasz\nKobyli´nski, 2018) is POLEV AL 2018 NER task\ngold dataset, labeled using the same guidelines as\nNKJP.\nFiNER (Ruokolainen et al., 2019) is a Finnish\ndataset that contains the same NE types as\nSlavNER, thus useful to train NER for EVT and\nPRO classes.\nCNEC (Ševˇcíková et al., 2014) Czech Named\nEntity Corpus 2.0 is labeled according to a two-\nlevel hierarchy of 46 named entities. It was mapped\nto the corresponding 4 classes of the SlavNER task:\nORG, PER, LOC, and PRO.\nFactRU (Starostin et al., 2016) is a Russian\ndataset, labeled with 4 classes of entities (Org, Lo-\ncOrg, Location, and Person), which can be mapped\nto 3 classes of the SlavNER task: ORG, LOC, PER.\nconll2002 (Tjong Kim Sang, 2002) is a Spanish\nNER dataset labeled with PER, LOC, ORG and\nMISC classes.\nconll2003 (Tjong Kim Sang and De Meulder,\n2003) is an English NER dataset labeled using PER,\nLOC, ORG and MISC classes.\n4 Architecture and systems\nThe architecture of our solution is modular: the\nmodules roughly correspond to the data processing\nsteps necessary to reach the objectives of different\nSlavNER Shared Tasks: mention detection, lemma-\ntization, and linking (Figure 1).\nWe submitted five systems to the SlavNER task.\nTable 1 provides an overview of our systems. In\nFigure 1: Overall System Architecture\nthe following sections, we provide more details of\nour solutions.\nNER Linker\n1 XLM-R Base Ensemble C\n2 XLM-R Large C\n3 XLM-R Large D and C\n4 XLM-R Large plus KPWr data D and C\n5 XLM-R Base additionaly\npre-trained plus KPWr data D and C\nTable 1: System overview (C-corpus level, D- document\nlevel)\n5 Two Approaches to Mention Detection:\ntraditional and ensemble\nWe consider the Named Entity Mention Detection\nand Classification task as the NER task. We use\nthe Flair library (Schweter and Akbik, 2020) to per-\nform NER. Flair library allows fine-tuning a Trans-\nformer (Vaswani et al., 2017) model with custom\ndata. Multilingual XLM-R has demonstrated the\nbest result in previous shared task (Ferreira et al.,\n2021) and is used as a basis for our NER models.\nThe XLM-R is available in XLM-R Base (L= 12, H\n= 768, A = 12, 270M params) and XLM-R Large(L\n= 24, H = 1024, A = 16, 550M params) variants.\nWe use XLM-R Large model fine-tuned on the\ndataset provided by the Shared Task organizers as\na NER model for our System-2 and System-3.\nFor System-4 we fine-tune a XLM-R Large\nmodel on the dataset given by the Shared Task\n173\norganizers combined together with KPWR-NER\ndataset (Marci´nczuk, 2020).\nAlthough multiple NER datasets for Czech, Pol-\nish, and Russian are available, most of them could\nnot be directly used due to differences in tagsets.\nHowever, even if the set of labeled classes is in-\ncompatible with the SlavNER labeling schema, it\nis still possible to use this data for training a NER\nsystem to recognize a single class that has compati-\nble labeling. This is done by keeping only a single\nlabel in a dataset and deleting all other labels.\nUsing single-label datasets, we train a NER sys-\ntem by combing SlavNER dataset with this dataset\nand evaluate against the SlavNER test split. If a sys-\ntem achieves better results than the baseline system\ntrained on SlavNER data, we consider this dataset\nas compatible with SlavNER and select to train\nthe final NER model for a given label. Datasets\nused to fine-tune each single-label NER model are\nsummarised in Table 2.\nModel Datasets used for training\nEVT SlavNER, KPWr\nLOC SlavNER, CNEC, KPWr, conll2002,\nconll2003, FactRu, finer, NKJP\nORG SlavNER, CNEC, KPWr,\nFactRu, NKJP, Poleval\nPER SlavNER, Poleval,\nPRO SlavNER, CNEC, KPWr, finer\nTable 2: Datasets used to train single-label models\nDue to performance and time restrictions, the\nXLM-R Base model is used to fine-tune ensemble\nmodels. During the evaluation, all five NER models\nare run sequentially. The overlapping labels are\nresolved, first by selecting the longest labeled entity\nand then, if there is an exact overlap, by selecting\nthe highest score returned by NER. This ensemble\napproach is used by our System-1.\nSince the XLM-R models were created more\nthan two years ago, and thus outdated with respect\nto current events, we crawled 2.6 GB of the latest\nCzech, Polish, and Russian news articles 1 to per-\nform additional pretraining of XLM-R base model.\nDue to the time restrictions, additional pretraining\nwas done using huggingface/transformers example\nscript2 with batch size 512, for 7000 steps. This\n1News were collected from the news portals:\nwww.idnes.cz, niezalezna.pl and censor.net Russian\nsection\n2https://github.com/huggingface/transformers/blob/main/\nexamples/pytorch/language-modeling/run_mlm.py\nadditionally pre-trained XLM-R model was fine-\ntuned using the SlavNER dataset and the KPWr\ndataset for NER of System-5.\n6 Entity Normalization\nWe use several strategies for entity normalization.\nIn case of the Czech language we apply a simple\nword-level lemmatization strategy. We use Stanza\n(Qi et al., 2020) Czech language lemmatizer for\nthis task.\nFor entity normalization in Polish and Russian,\nwe use a language-specific noun phrase generator.\nIt allows us to transform the noun phrase into the\ncorresponding base form taking into account the\ngrammar rules of the specific language.\nThe normalization workflow includes several\nsteps: tokenization, morphological analysis, syn-\ntactic parsing, morphological transfer, and mor-\nphological synthesis of the base form. Morpho-\nlogical analysis and synthesis are performed with\nhelp of a language-specific finite state transducer\n(FST). This FST solution was initially developed\nfor the Latvian language (Deksne, 2013) and re-\ncently extended to many other European languages\n- Lithuanian, Polish, Finnish, Swedish, Spanish,\nFrench, German, and English. For the syntactic\nparsing Cocke-Younger-Kasami (CYK) algorithm\n(Younger, 1967) is employed by adapting the cor-\nresponding Latvian tool (Deksne et al., 2014).\nWhen analysing output of the normalisation tool,\nwe identified several reasons for errors:\n• A word in a phrase is unrecognized acronym.\n• In the case of homographs, if a word has some\nidentical singular and plural forms, the nor-\nmalisation tool preserves the number of orig-\ninal phrases (singular or plural). As result in\nsome cases the number of the base form of a\nparticular NE is singular instead of plural or\nvice versa.\n• For the multi-word expressions, the normali-\nsation tool can create several base forms that\ncomply with syntactical rules. As there is no\ndisambiguation component that would take\ninto account the semantics of the particular\nphrase, the first result from the result list is\nassumed as the correct one.\n7 Entity Linking\nThe goal of the entity linking task is to associate\nentity mentions found in a text with corresponding\n174\nentries in a Knowledge Base (KB) (Zheng et al.,\n2010). Traditional entity linking pipeline consists\nof three steps: mention detection, candidate selec-\ntion, and disambiguation (Balog, 2018).\nThe mention detection step is described in the\nSection 5. Due to the small expected size of our\ncross-lingual knowledge base (the actual maximum\nnumber of KB entries produced in this Shared task\nby our systems was 939), we skip the candidate\nselection step. Instead, a simple consistency check\nis applied to filter out mentions which do not have\nthe same type (Khosla and Rose, 2020). As result,\nthe candidates are all entries in the Knowledge Base\nwhich have the same type as the entity mention,\nwhich we are attempting to link.\nThe candidate selection and disambiguation usu-\nally include a three-step process of candidate gen-\neration, candidate ranking, and unlinkable mention\nprediction (Shen et al., 2015). In our submission,\nthe candidates are ranked using a mention-ranking\nmodel (Rahman and Ng, 2009) to decide whether\nan active mention is co-referent with a candidate\nantecedent. We follow the algorithm proposed\nby (V¯ıksna and Skadina, 2021): at first, we use\nLaBSE to obtain entity mention embeddings and\nthen we apply cosine similarity to calculate the\nsimilarity between obtained embeddings and those\nin the Knowledge Base. The similarity threshold\nfor early stopping is set at 0.95 - if the similarity\nis above the threshold, the process links the en-\ntity mention to the candidate mention and returns\nthe candidate mention ID. If none of the candidate\nmentions has a similarity score above 0.6, the entity\nmention is considered not found in the Knowledge\nBase and is added as a new entry to the Knowledge\nBase. For entities with similarity scores between\n0.6 and 0.95, the candidate with the highest score\nis selected for linking.\nUsually, at the beginning of the text entities are\nintroduced (named) carefully, e.g. with a full name\n(and acronym), while later in a text, when it is clear\nfrom the context what they refer to, entities are of-\nten used in the shortened form (Rychlikowski et al.,\n2021). For such cases, we introduce an additional\nlinking step at the document level: for each entity\nmention, we check whether its name is part of an-\nother entity, e.g., encountering the name \"Asia\", it\ncould be matched as part of \"Asia Bibi\". We per-\nform this step before attempting to link an entity to\nthe Knowledge Base.\nWe also check for organization and person name\nabbreviations and translations. At first, we identify\nentities that are surrounded by brackets (optionally,\nquoted). Then, if the entity immediately preceding\nit belongs to the same type, both entities are linked\ntogether as aliases.\n8 Results\nTable 3 summarizes the performance of our five\nsystems. The best results in the entity recognition\ntask have been achieved by System-3. System-\n3 does not use any additional datasets for NER\ntraining. However, the overall results differ very\nlittle, and may not be statistically significant.\nNER Norm Link Link\ncross- document\nlang\nSystem-1 0.890 0.587 0.644 0.716\nSystem-2 0.896 0.595 0.668 0.712\nSystem-3 0.896 0.595 0.669 0.755\nSystem-4 0.885 0.584 0.668 0.727\nSystem-5 0.881 0.587 0.666 0.702\nTable 3: NER (Recognition, relaxed partial matching),\nnormalization and linking (cross-language level and doc-\nument level) results of Tilde systems, F scores\nSystem-4 shows noticeable improvement for\nEVT detection (Table 4), which could be explained\nby additional XLM-R pretraining on recent news\ndata. The performance of System-5, which was\nfine-tuned using additional KPWr data, is very\npoor in PER class. Our hypothesis is that the\nannotation guidelines for PER class differ signifi-\ncantly between KPWr and SlavNER datasets. This\ndrawback is addressed by our ensemble System-1,\nwhich, despite being fine-tuned with XLM-R Base,\nachieves an overall F-score of 0.89, and for the\nLOC class shows better performance than System-\n3 (achieving an F-score of 0.944).\nS1 S2, S3 S4 S5\nAll 0.890 0.896 0.885 0.881\nPER 0.969 0.971 0.930 0.906\nLOC 0.944 0.934 0.932 0.938\nORG 0.843 0.848 0.854 0.853\nPRO 0.689 0.823 0.761 0.796\nEVT 0.273 0.300 0.375 0.267\nTable 4: Entity recognition results evaluated on\nSlavNER test data (Relaxed partial matching, All 5\nsystems: S1 = System-1, S2 = System-2, ...), F scores\n175\nThe ensemble system shows good overall per-\nformance but performs poorly on PRO class. Al-\nthough the separate NER systems, fine-tuned to\ndetect PRO entities on CNEC, KPWr, and FiNER\ndata, performed better than the baseline on our test\nsetup, the final system, trained on the combined\ndataset, did not generalize well.\nThe NER results vary slightly between lan-\nguages (Table 5), with better scores for languages\nusing Latin script.\nRecall Precision F score\ncs (all) 0.885 0.945 0.914\nru (all) 0.878 0.884 0.880\npl (all) 0.869 0.932 0.899\nTable 5: System-3 entity recognition results evaluated\non SlavNER test data (Relaxed partial matching) by\nlanguage\nAll our systems use the same normalization tool,\ntherefore any differences in normalization results\nbetween our systems depend on the previous entity\nrecognition step. The normalization results for our\nbest-performing System-3 are summarized in Ta-\nble 6. The normalization tool demonstrates good\nresults for the Russian language (F-score 0.70),\nwhile for Polish (F-score 0.54) results are similar\nto Stanza, used for Czech language normalization.\nRecall Precision F score\nPER 0.488 0.496 0.492\nLOC 0.731 0.746 0.739\nORG 0.298 0.393 0.339\nPRO 0.459 0.436 0.447\nEVT 0.011 0.045 0.018\nAll corpora 0.566 0.627 0.595\ncs (all) 0.561 0.522 0.541\nru (all) 0.692 0.716 0.704\npl (all) 0.474 0.623 0.539\nTable 6: Entity normalization results evaluated on\nSlavNER test data (System-3)\nThe best results in entity linking task (in all\ntasks - document level, single- and cross-language)\nachieved System-3. Evaluation results for this sys-\ntem are summarized in Table 7. Since this task\ndepends on mention detection task, results for the\nEVT class are poor. Our entity linking system is\nbased on embeddings and in the case of organi-\nzations, it often fails to separate similar yet com-\npletely different organizations, e.g. our model con-\nsiders ORG-Gazprom and ORG-Gazprombank as\nthe same entity. When the output of System-2 and\nSystem-3 is compared (Table 3), we can see that\ndocument-level linking improves entity linking per-\nformance on the document level significantly (F-\nscores 0.712 and 0.755), while on the cross-lingual\nlevel its effects are negligible.\nRecall Precision F score\nPER 0.713 0.764 0.738\nLOC 0.813 0.787 0.800\nORG 0.422 0.416 0.419\nPRO 0.428 0.615 0.505\nEVT 0.102 0.241 0.144\nAll 0.660 0.677 0.669\nTable 7: Entity linking results evaluated on SlavNER\ntest data (Cross-language level, System-3)\n9 Conclusions\nIn this paper, we presented a modular architec-\nture for the Recognition, Normalization, Classi-\nfication, and Cross-lingual linking of Named Enti-\nties in Slavic Languages. Each module (NER, nor-\nmalization tool, and NE linker) is self-contained\nand could be improved independently from oth-\ners. While none of the systems fine-tuned on addi-\ntional datasets surpassed the XLM-R Large system\nfine-tuned on SlavNER data, the ensemble system\nseems promising and could be retrained again us-\ning the XLM-R Large model instead of XLM-R\nBase in order to obtain better results.\nLimitations\nOur best-performing systems use very large lan-\nguage models or are ensemble systems, resources\nrequired to train and run such systems are consid-\nerable.\nEntity linking module performs an embedding\ncomparison with all entities of a matching type\nfound in the knowledge base. While the KB is\nsmall such an approach works fast, however, as\nthe knowledge base grows, each additional entity\nadds to the search time. For large knowledge bases,\nsome form of candidate selection method would be\nnecessary.\nAcknowlegements\nThis research has been supported by the Euro-\npean Regional Development Fund within the re-\n176\nsearch project “AI Assistant for Multilingual Meet-\ning Management” No. 1.1.1.1/ 19/A/082.\nReferences\nMikhail Arkhipov, Maria Trofimova, Yuri Kuratov, and\nAlexey Sorokin. 2019. Tuning multilingual trans-\nformers for language-specific named entity recogni-\ntion. In Proceedings of the 7th Workshop on Balto-\nSlavic Natural Language Processing, pages 89–93,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nKrisztian Balog. 2018. Entity Linking, pages 147–188.\nSpringer International Publishing, Cham.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nDaiga Deksne. 2013. Finite state morphology tool for\nLatvian. In Proceedings of the 11th International\nConference on Finite State Methods and Natural Lan-\nguage Processing, pages 49–53, St Andrews, Scot-\nland. Association for Computational Linguistics.\nDaiga Deksne, Inguna Skadina, and Raivis Skadins.\n2014. Extended CFG formalism for grammar\nchecker and parser development. In Computational\nLinguistics and Intelligent Text Processing - 15th In-\nternational Conference, CICLing 2014, Kathmandu,\nNepal, April 6-12, 2014, Proceedings, Part I, volume\n8403 of Lecture Notes in Computer Science, pages\n237–249. Springer.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n878–891, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nPedro Ferreira, Ruben Cardoso, and Afonso Mendes.\n2021. Priberam labs at the 3rd shared task on\nSlavNER. In Proceedings of the 8th Workshop on\nBalto-Slavic Natural Language Processing, pages 86–\n92, Kiyv, Ukraine. Association for Computational\nLinguistics.\nSopan Khosla and Carolyn Rose. 2020. Using type in-\nformation to improve entity coreference resolution.\nIn Proceedings of the First Workshop on Computa-\ntional Approaches to Discourse, pages 20–31, Online.\nAssociation for Computational Linguistics.\nMichał Marci´nczuk. 2020. KPWr n82 NER model (on\npolish RoBERTa base). CLARIN-PL digital reposi-\ntory.\nJames Mayfield, Paul McNamee, and Cash Costello.\n2017. Language-independent named entity analy-\nsis using parallel projection and rule-based disam-\nbiguation. In Proceedings of the 6th Workshop on\nBalto-Slavic Natural Language Processing, pages 92–\n96, Valencia, Spain. Association for Computational\nLinguistics.\nMaciej Ogrodniczuk and Łukasz Kobyli ´nski, editors.\n2018. Proceedings of the PolEval 2018 Workshop.\nInstitute of Computer Science, Polish Academy of\nSciences, Warsaw, Poland.\nMarcin Oleksy, Michał Marci ´nczuk, Marek Maziarz,\nTomasz Berna´s, Jan Wieczorek, Agnieszka Turek,\nDominika Fikus, Michał Wolski, Marek Pustowaruk,\nJan Koco´n, and Paweł K˛ edzia. 2019. Polish corpus of\nwrocław university of technology 1.3. CLARIN-PL\ndigital repository.\nJakub Piskorski, Bogdan Babych, Zara Kancheva, Olga\nKanishcheva, Maria Lebedeva, Michał Marci´nczuk,\nPreslav Nakov, Petya Osenova, Lidia Pivovarova,\nSenja Pollak, Pavel P ˇribáˇn, Ivaylo Radev, Marko\nRobnik-Sikonja, Vasyl Starko, Josef Steinberger, and\nRoman Yangarber. 2021. Slav-NER: the 3rd cross-\nlingual challenge on recognition, normalization, clas-\nsification, and linking of named entities across Slavic\nlanguages. In Proceedings of the 8th Workshop on\nBalto-Slavic Natural Language Processing , pages\n122–133, Kiyv, Ukraine. Association for Computa-\ntional Linguistics.\nJakub Piskorski, Laska Laskova, Michał Marci ´nczuk,\nLidia Pivovarova, Pavel P ˇribáˇn, Josef Steinberger,\nand Roman Yangarber. 2019. The second cross-\nlingual challenge on recognition, normalization, clas-\nsification, and linking of named entities across Slavic\nlanguages. In Proceedings of the 7th Workshop on\nBalto-Slavic Natural Language Processing, pages 63–\n74, Florence, Italy. Association for Computational\nLinguistics.\nJakub Piskorski, Lidia Pivovarova, Jan Šnajder, Josef\nSteinberger, and Roman Yangarber. 2017. The first\ncross-lingual challenge on recognition, normaliza-\ntion, and matching of named entities in Slavic lan-\nguages. In Proceedings of the 6th Workshop on Balto-\nSlavic Natural Language Processing, pages 76–85,\nValencia, Spain. Association for Computational Lin-\nguistics.\nMarko Prelevikj and Slavko Zitnik. 2021. Multilingual\nnamed entity recognition and matching using BERT\nand dedupe for Slavic languages. In Proceedings of\nthe 8th Workshop on Balto-Slavic Natural Language\nProcessing, pages 80–85, Kiyv, Ukraine. Association\nfor Computational Linguistics.\nAdam Przepiórkowski. 2012. Narodowy korpus j˛ ezyka\npolskiego. Naukowe PWN.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A Python\nnatural language processing toolkit for many human\n177\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations.\nAltaf Rahman and Vincent Ng. 2009. Supervised mod-\nels for coreference resolution. In Proceedings of\nthe 2009 Conference on Empirical Methods in Natu-\nral Language Processing, pages 968–977, Singapore.\nAssociation for Computational Linguistics.\nTeemu Ruokolainen, Pekka Kauppinen, Miikka Silfver-\nberg, and Krister Lindén. 2019. A finnish news\ncorpus for named entity recognition. Language Re-\nsources and Evaluation, pages 1–26.\nPaweł Rychlikowski, Bartłomiej Najdecki, Adrian Lan-\ncucki, and Adam Kaczmarek. 2021. Named entity\nrecognition and linking augmented with large-scale\nstructured data. In Proceedings of the 8th Workshop\non Balto-Slavic Natural Language Processing, pages\n115–121, Kiyv, Ukraine. Association for Computa-\ntional Linguistics.\nStefan Schweter and Alan Akbik. 2020. FLERT:\nDocument-level features for named entity recogni-\ntion.\nMagda Ševˇcíková, Zdenˇek Žabokrtský, Jana Straková,\nand Milan Straka. 2014. Czech named entity corpus\n2.0. LINDAT/CLARIAH-CZ digital library at the\nInstitute of Formal and Applied Linguistics (ÚFAL),\nFaculty of Mathematics and Physics, Charles Univer-\nsity.\nW. Shen, J. Wang, and J. Han. 2015. Entity linking\nwith a knowledge base: Issues, techniques, and so-\nlutions. IEEE Transactions on Knowledge and Data\nEngineering, 27(2):443–460.\nAnatoli Starostin, Victor Bocharov, Svetlana Alex-\neeva, A. A. Bodrova, Alexander Chuchunkov,\nSh. Sh. Dzhumaev, Irina Efimenko, D V Granovsky,\nVladimir F. Khoroshevsky, Irina V . Krylova, Maria\nNikolaeva, Ivan Smurov, and Svetlana Toldova. 2016.\nFactrueval 2016: Evaluation of named entity recog-\nnition and fact extraction systems for russian. In\nComputational Linguistics and Intellectual Technolo-\ngies: Proceedings of the International Conference\n“Dialogue 2016”.\nErik F. Tjong Kim Sang. 2002. Introduction to the\nCoNLL-2002 shared task: Language-independent\nnamed entity recognition. In COLING-02: The 6th\nConference on Natural Language Learning 2002\n(CoNLL-2002).\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nTatiana Tsygankova, Stephen Mayhew, and Dan Roth.\n2019. BSNLP2019 shared task submission: Multi-\nsource neural NER transfer. In Proceedings of the\n7th Workshop on Balto-Slavic Natural Language Pro-\ncessing, pages 75–82, Florence, Italy. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nRinalds V¯ıksna and Inguna Skadina. 2021. Multilingual\nSlavic named entity recognition. In Proceedings of\nthe 8th Workshop on Balto-Slavic Natural Language\nProcessing, pages 93–97, Kiyv, Ukraine. Association\nfor Computational Linguistics.\nDaniel H Younger. 1967. Recognition and parsing of\ncontext-free languages in time n3. Information and\ncontrol, 10(2):189–208.\nZhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-\naoyan Zhu. 2010. Learning to link entities with\nknowledge base. In Human Language Technologies:\nThe 2010 Annual Conference of the North American\nChapter of the Association for Computational Lin-\nguistics, pages 483–491, Los Angeles, California.\nAssociation for Computational Linguistics.\n178",
  "topic": "Czech",
  "concepts": [
    {
      "name": "Czech",
      "score": 0.9543502330780029
    },
    {
      "name": "Slavic languages",
      "score": 0.9291563034057617
    },
    {
      "name": "Computer science",
      "score": 0.8058254718780518
    },
    {
      "name": "Task (project management)",
      "score": 0.6267272233963013
    },
    {
      "name": "Natural language processing",
      "score": 0.6160250902175903
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4999353885650635
    },
    {
      "name": "Named-entity recognition",
      "score": 0.47087204456329346
    },
    {
      "name": "Language model",
      "score": 0.448578417301178
    },
    {
      "name": "Linguistics",
      "score": 0.4397675693035126
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91123046",
      "name": "University of Latvia",
      "country": "LV"
    },
    {
      "id": "https://openalex.org/I4210146923",
      "name": "Tilde (Latvia)",
      "country": "LV"
    }
  ]
}