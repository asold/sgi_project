{
    "title": "Graph-Aware Transformer: Is Attention All Graphs Need?",
    "url": "https://openalex.org/W3035658562",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5032371858",
            "name": "Sanghyun Yoo",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100600315",
            "name": "Young Seok Kim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5101774667",
            "name": "Kang Hyun Lee",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5040816297",
            "name": "Kuhwan Jeong",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5110428431",
            "name": "Junhwi Choi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5014777103",
            "name": "Hoshik Lee",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5101084417",
            "name": "Young Sang Choi",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2786016794",
        "https://openalex.org/W2879390606",
        "https://openalex.org/W2997736261",
        "https://openalex.org/W2672952450",
        "https://openalex.org/W2884430236",
        "https://openalex.org/W2769423117",
        "https://openalex.org/W3103817024",
        "https://openalex.org/W2114704115",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2946466283",
        "https://openalex.org/W3100848837",
        "https://openalex.org/W2624407581",
        "https://openalex.org/W2970066309",
        "https://openalex.org/W2962711740",
        "https://openalex.org/W2751808960",
        "https://openalex.org/W2964108670",
        "https://openalex.org/W2996451395",
        "https://openalex.org/W2997494090",
        "https://openalex.org/W3003446182",
        "https://openalex.org/W2794980531",
        "https://openalex.org/W3007488165",
        "https://openalex.org/W2903262661",
        "https://openalex.org/W2952254971",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W2963396480",
        "https://openalex.org/W3009582169",
        "https://openalex.org/W2795108883",
        "https://openalex.org/W2753921001",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970709315",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2554952599",
        "https://openalex.org/W2981104283",
        "https://openalex.org/W2950898568",
        "https://openalex.org/W3103092523",
        "https://openalex.org/W2979750740",
        "https://openalex.org/W2996604169",
        "https://openalex.org/W1501856433",
        "https://openalex.org/W2786722833",
        "https://openalex.org/W2890187992",
        "https://openalex.org/W2952637862",
        "https://openalex.org/W2951101948",
        "https://openalex.org/W2901739041"
    ],
    "abstract": "Graphs are the natural data structure to represent relational and structural information in many domains. To cover the broad range of graph-data applications including graph classification as well as graph generation, it is desirable to have a general and flexible model consisting of an encoder and a decoder that can handle graph data. Although the representative encoder-decoder model, Transformer, shows superior performance in various tasks especially of natural language processing, it is not immediately available for graphs due to their non-sequential characteristics. To tackle this incompatibility, we propose GRaph-Aware Transformer (GRAT), the first Transformer-based model which can encode and decode whole graphs in end-to-end fashion. GRAT is featured with a self-attention mechanism adaptive to the edge information and an auto-regressive decoding mechanism based on the two-path approach consisting of sub-graph encoding path and node-and-edge generation path for each decoding step. We empirically evaluated GRAT on multiple setups including encoder-based tasks such as molecule property predictions on QM9 datasets and encoder-decoder-based tasks such as molecule graph generation in the organic molecule synthesis domain. GRAT has shown very promising results including state-of-the-art performance on 4 regression tasks in QM9 benchmark.",
    "full_text": "Graph-Aware Transformer: Is Attention All Graphs\nNeed?\nSanghyun Yoo Young-Seok Kim Kang Hyun Lee Kuhwan Jeong\nJunhwi Choi Hoshik Lee Young Sang Choi\nSamsung Advanced Institute of Technology\n{sam.yoo,ys24.kim,kh1216.lee,kuhwan.jeong,junhwi.choi,hoshik.lee,macho}\n@samsung.com\nAbstract\nGraphs are the natural data structure to represent relational and structural informa-\ntion in many domains. To cover the broad range of graph-data applications includ-\ning graph classiﬁcation as well as graph generation, it is desirable to have a general\nand ﬂexible model consisting of an encoder and a decoder that can handle graph\ndata. Although the representative encoder-decoder model, Transformer, shows\nsuperior performance in various tasks especially of natural language processing, it\nis not immediately available for graphs due to their non-sequential characteristics.\nTo tackle this incompatibility, we propose GRaph-Aware Transformer (GRAT),\nthe ﬁrst Transformer-based model which can encode and decode whole graphs in\nend-to-end fashion. GRAT is featured with a self-attention mechanism adaptive\nto the edge information and an auto-regressive decoding mechanism based on the\ntwo-path approach consisting of sub-graph encoding path and node-and-edge gen-\neration path for each decoding step. We empirically evaluated GRAT on multiple\nsetups including encoder-based tasks such as molecule property predictions on\nQM9 datasets and encoder-decoder-based tasks such as molecule graph generation\nin the organic molecule synthesis domain. GRAT has shown very promising results\nincluding state-of-the-art performance on 4 regression tasks in QM9 benchmark.\n1 Introduction\nThe ability to deal with graphs has many applications [1]: designing efﬁcient chip architectures [2],\npredicting missing information in knowledge bases [ 3], rating items in social recommendation\nsystems [4], predicting molecular properties and generating new molecular structures, which could\nexpedite the discovery process of new drugs and materials [5].\nThe crux of the ability to deal with graph data for such applications lies in two parts. The ﬁrst part\nis how to create a representation which can effectively capture the relationship between nodes and\nedges for a given graph. The second part is how to generate a new graph, i.e., a set of nodes and\nedges, satisfying certain requirements. Generally, the representation for a given graph is created by\nan encoder, and the new graph generation is conducted by a decoder. Therefore, in order to cover\nthe needs of the aforementioned broad application domains, it is desirable to have a general model\nconsisting of the encoder and the decoder, instead of having application-speciﬁc models.\nTransformer [6] has shown state-of-the-art performance in many applications such as automatic\nspeech recognition, neural machine translation, named entity recognition, sentiment analysis, question\nanswering, visual question answering, and more. These applications deal with different kinds of\nmodalities from voice wave and text to image and even multi-modal data. There are many aspects of\nPreprint. Under review.\narXiv:2006.05213v1  [cs.LG]  9 Jun 2020\nTransformer, which resulted in such superior performance. Among them, the multi-head self-attention\nmechanism is arguably the most important one. Considering the power of Transformer and its general\napplicability to many types of applications dealing with different kinds of modalities, it is a very\nnatural step to apply Transformer to graph-data applications and evaluate its effectiveness. Moreover,\nsince Transformer consists of the encoder and the decoder, it can serve as a general model which may\ncover the broad range of the graph-data applications.\nThe luxury of Transformer, however, is not immediately available for the graph data due to the\nnon-sequential characteristic. To tackle this incompatibility, we consider nodes as a sequence of\ntokens, one token for each node, where the order of the tokens could be imposed depending on the\napplications’ necessity. Edges are reﬂected when the self-attention weights are computed, since the\nattention weights are used to decide how much information should be taken from the other tokens\n(i.e., nodes). With this manipulation, graph data can go through Transformer without losing any\nmeaningful information. Moreover, in order to generate a new graph through the decoder, we add\nan ability to generate a new node with its associated edges to the nodes generated so far for each\ndecoding step in an auto-regressive manner.\nOur paper’s main contributions are as follows:\n• We propose a Transformer-based graph neural network, called Graph-Aware Transformer\n(GRAT). To the best of our knowledge, GRAT is the ﬁrst Transformer-based model which\ncan take graphs as input and generate whole graphs as output in end-to-end fashion. Also,\nsince we do not impose any application or domain speciﬁc assumptions on GRAT, it is very\ngeneral and ﬂexible enough to be applicable to any graph-data applications.\n• With GRAT, we study the effectiveness of applying Transformer to graphs in multiple setups.\nFirst, we apply GRAT to graph property prediction tasks, where only the encoder part is\nused to see whether the modiﬁed self-attention mechanism is able to effectively capture the\ngraph structure. Second, GRAT is employed for a task of generating new graphs for input\ngraphs, where, of course, both encoder and decoder parts are used together.\n• Our proposed GRAT model has shown very promising results including state-of-the-art\nperformance on graph property prediction tasks. Based on the results, we would like to\nargue that a Transformer-based model, i.e., a general encoder-decoder model armed with\nmulti-head self-attention, is a compelling and viable option for graph-data applications.\n2 Transformer\nSince our architecture is based on Transformer [6], we brieﬂy introduce it ﬁrst. It has achieved state-\nof-the-art performance for various sequence-to-sequence tasks such as machine translation, which\ncould be attributed to the general encoder-decoder architecture armed with the attention mechanism.\nOne of the most important features of Transformer is the attention function calledScaled Dot-Product\nAttention, which maps a query and a set of key-value pairs to output as shown below:\nAttention(Q,K,V ) = softmax\n(QKT\n√dk\n)\nV (1)\nwhere Q, K, and V are matrices of the same size, and dk is the dimension of the keys.\nTransformer, however, is not immediately available for graph applications in the following reasons.\nFirst, even if we can easily map nodes of a graph to a sequence of tokens, Transformer does not\nprovide a way to explicitly take the relational information (i.e., edge information) between nodes.\nAlthough the attention mechanism is designed to ﬁgure out the implicit relationship between input\ntokens (i.e., nodes), it may not fully utilize the explicitly given edge information of the input graph.\nSecond, the decoder of Transformer assumes every token highly depends on the very previous token,\nwhich is realized by inputting the very previous token as a query when predicting the next token.\nHowever, this assumption may not be valid for graphs since it could be better to take into account the\noverall structure of the generated sub-graph in each decoding step. We describe how GRAT tackles\nthese problems in the next section.\n2\n3 Graph-Aware Transformer\n3.1 Graph Encoding\nWe carve out nodes and edges of a given graph to ﬁt into Transformer model. Nodes are considered\nas tokens, one token for each node, where the order of the tokens could be imposed depending on\nthe applications’ necessity. Edges are reﬂected when the self-attention weights are computed. More\nspeciﬁcally, we allow Transformer to learn the importance of each connection between nodes by\nreﬂecting edge information into the scaled dot-product attention.\nMaziarka et al. [7] have recently proposed the similar idea which exploits the distance between nodes\nand the adjacency matrix as well as the original attention values. However, they just use the weighted\nsum of these three attention values, where the weights are hyper parameters which are handcrafted\nand may vary for each application.\nTo make our model learn the importance of each connection solely from the data, we use feature-wise\ntransformation [8]. In this method, the attention values are transformed with the scaling factor γand\nthe biasing factor β, which are generated based on the edge type as the conditioning information.\nThe attention values, Attention(Q,K,V ), are calculated as follows:\n(γij,βij) = fa(eij) (2)\nAttention(Q,K,V ) = softmax\n(Γ ⊙(QKT ) + B√dk\n)\nV (3)\nwhere eij indicates the edge type (in one-hot representation) between nodes i and j, and fa is\nmulti-layer perceptrons (MLPs). Γ and Bare the matrices whose elements at (i,j) position are γij\nand βij, respectively. The operator ⊙indicates the element-wise multiplication.\nNote that the way that we reﬂect the edge type into the attention mechanism is ﬂexible enough to\nincorporate any additional per-edge features as input to fa. For example, when the distance between\nnodes iand jis available as an additional edge feature, the concatenation of the distance and edge\ntype eij can be used as input to fa. Also, depending on applications’ characteristics, it might be\nbetter to prevent a node from attending on the non-neighbor nodes. This could be achieved by setting\ncorresponding logit values to the negative inﬁnity ( −∞) before the softmax function so that the\nattention values are set to zeros. Lastly, when the property of permutation invariance is required, we\nmay simply remove the positional encoding after the input embedding [6].\n3.2 Auto-regressive Graph Decoding\nGRAT’s decoder generates a node and its associated edges for each decoding step in an auto-\nregressive manner. The decoding process at time step i consists of two paths: 1) encoding the\nsub-graph generated by step i-1 and 2) generating a new node and its associated edges to the existing\nones. This is the point mainly different from Transformer decoder, since it does not distinguish these\ntwo paths. The rationale behind this idea is that generating a new node should not highly depend\non the immediate previous node. Instead, due to the relational nature of the graph, it should take\ninto account all the previously generated nodes based on their relationship, i.e, edge information. To\nreﬂect this rationale into the decoding step explicitly, we avoid injecting the immediate previous node\nas input token to the next step directly by introducing the two-path decoding step.\nFor clarity, we explain the detailed behavior of the two-path decoding step with an example illustrated\nin Figure 1. We omit the encoder and the stacked self-attention layers for brevity. In the example,\nthe decoder should generate a graph consisting of 4 nodes with associated edges, where a, b, c, and\ndare node labels, and the line types of the edges represent edge types. Suppose that the model has\nalready generated the nodes, a, b, and cwith their edges until time step t=2. Now, according to the\ntwo-path decoding step, the ﬁrst path (i.e., the encoding path) encodes the sub-graph and outputs the\nnodes’ representation. Since the representations for aand b, h0 and h1, are already made at t=1 and\nt=2 and can be reused, only the representation h2 for node cis computed at this time by reﬂecting\nthe representations of all neighbor nodes, only bhere though, through the attention mechanism as\ndescribed in Section 3.1.\nNext, the second path (i.e., node-and-edge generation path) inputs a special token <G> and outputs\nits node representation h′\n3 taking into account all previously generated nodes through the attention\n3\nFigure 1: An example of the two-path decoding steps (left), the corresponding edge matrix (top-right),\nand the masking matrix (bottom-right)\nagain1, and then generates a label dof a new node. Plus, the associated edges are generated, where\nthe order of the edge type values in the ﬁgure corresponds to the order of their node generations.\nSince the node dis connected to the node aonly, the predicted value representing the edge type\nbetween node dand ais one, and the others are zeros, each of which means no edge. The decoding\nprocess ends if the predicted label is another special token <EOG>.\nMore formally, if the token <G> is injected at time stepi, i.e., on the generation path, the representation\nfor a new node, h′\ni is generated. Then, the label for this i-th node, li, and the type of the edge between\nthe i-th and j-th node, eij (where i>j ), can be computed as:\nli = arg max(fl(h′\ni)) (4)\neij = arg max(fe(fp(h′\ni),fp(hj))) (5)\nwhere hj is the representation of the j-th node, fl and fe are MLPs, and (., .) denotes concatenation.\nWe use an additional MLP,fp, to reduce the dimension of the representation of each node.\nThe edge matrix used in this example is shown as the top-right matrix in Figure 1. There are two\nspecial edge types, virtual edge (v) and self-edge (s). The virtual edge v, corresponding to the purple\nline in the ﬁgure, represents the edge type unknown yet and is used to attend on existing nodes when\ngenerating a new node. The self-edge s(the orange line) indicates the attention to the node itself,\nwhich should be distinguished from other edge types. Also, values at the white cells in the matrix\nrepresent edge types to be predicted as the decoding proceeds while values at the gray cells are simply\ngiven at the beginning of the decoding.\nThe nodes to be generated after the current time step and the special <G> tokens before the current\nstep should not be visible from each decoding step’s perspective. To avoid such invalid reference,\nwe use a masking matrix as shown in the bottom-right matrix in Figure 1. The upper triangle in this\nmatrix corresponds to the future context as in Vaswani et al. [6], and the remaining dark cells are for\nblocking the reference to the previous <G> tokens. Thanks to this masking matrix, all the decoding\nsteps happen in parallel during the training time even though we have explained the decoding steps as\na sequential procedure one node at a step.\n3.3 Pretraining Strategies\nSince GRAT resembles Transformer, we can leverage the BERT-style pretraining tasks that enable\nTransformer architecture to achieve state-of-the-art in many NLP problems [9]. The ﬁrst task used\n1Note that the node representations generated on the encoding path and the generation path at time step iare\ndenoted as hi−1 and h′\ni, respectively. Also, note that hi−1 is reused until the end of the decoding, whereas h′\ni is\nused only for the current step and discarded.\n4\nby BERT is the masked LM by which the model is trained to predict the masked words. We tweak\nthis task to mask node labels and their associated edge types for a given graph such that GRAT\nis trained to predict not only the node labels but also the edge types. The second task is the next\nsentence prediction task targeted for coming up with a sentence-level representation in contrast to\nthe word-level representation in the masked LM task. We also tweak this task to come up with a\ngraph-level representation such that GRAT is trained to predict graph-level properties (if available)\nfor a given graph. To do so, we introduce another special token <CLS> in front of the input node\nsequence of the encoder. The token’s ﬁnal hidden vector generated by the encoder is fed into a\nfully-connected layer, and the subsequent output represents a predicted graph-level property. This\ntype of graph pretraining strategy is inspired by the one used in [10]. Both node-level and graph-level\npretraining tasks help GRAT have an ability to ﬁgure out the relational information between nodes\nand edges in two different levels.\n4 Experimental Results\nTo examine the effectiveness of applying Transformer for graph data and its generality and ﬂexibility,\nwe empirically evaluate GRAT on multiple setups including encoder-based tasks such as graph\nproperty predictions and encoder-decoder-based tasks such as graph-to-graph translations.\n4.1 Property Prediction\nFor graph property predictions, we use the QM9 benchmark [ 11]. QM9 is a dataset that provides\ngeometric, energetic, electronic and thermodynamic properties of roughly 130K molecules with up to\nnine heavy atoms, yielding 12 regression tasks. All molecules are modeled using density functional\ntheory. Each task should predict a property for a given molecule, i.e., a graph consisting of atoms and\nbonds, i.e., nodes and edges, respectively. We randomly chose 10000 samples for validation, 10000\nsamples for testing, and used the rest for training as in [12]. There are two modes to train models for\nthe tasks: a multi-task model, i.e., one model for all 12 regression tasks, and 12 single-task models,\ni.e., one model for each regression task.\nFor the benchmark, we use the encoder of GRAT as our proposed model and compare with two other\nstate-of-the-art models, MPNN [12] and DimeNet [13]. MPNN is a variant of models belonging to\nMessage Passing Neural Network framework, where all such models are characterized as a common\ncomposition, i.e., a message function, an update function, and a readout function. Further details\nare described in Section 5. DimeNet also belongs to this framework, but it takes into account\nmolecule-speciﬁc information such as angles formed between neighbor nodes.\nTable 1: MAE of 12 regression tasks in QM9\nMode Multi-task Single-task\nTarget Unit DimeNet GRAT MPNN DimeNet GRAT\nmu D 0.0775 0.04859 0.03 0.0286 0.03898\nalpha a3\n0 0.0649 0.10258 0.092 0.0469 0.07219\nHOMO eV 0.0451 0.02648 0.04257 0.0278 0.02228\nLUMO eV 0.0411 0.02783 0.03741 0.0197 0.02053\ngap eV 0.0592 0.03864 0.0688 0.0348 0.035\nR2 a2\n0 0.345 2.68846 0.18 0.331 0.76681\nZPVE eV 0.00287 0.007 0.001524 0.00129 0.00208\nU0 eV 0.0129 0.05864 0.01935 0.00802 0.0705\nU eV 0.013 0.05859 0.01935 0.00789 0.02825\nH eV 0.013 0.06014 0.01677 0.00811 0.02549\nG eV 0.0139 0.05769 0.01892 0.00898 0.02505\nCv cal/molK 0.0309 0.05512 0.04 0.0249 0.03261\nstdMAE % 1.92 1.62 1.7 1.05 1.19\nlogMAE - -5.07 -4.39 -5.09 -5.57 -4.92\n5\nThe mean absolute errors (MAE) of 12 regression tasks in QM9 are shown in Table 1. Numbers in\nbold font represent the state-of-the-art performance for those tasks. For the multi-task mode, GRAT\nshows state-of-the-art performance on 4 out of 12 tasks. Notably, in terms of the mean standardized\nMAE (stdMAE) [ 13], GRAT achieves new state-of-the-art performance across 12 tasks, overall.\nThe corresponding results of MPNN are not available. For the single-task mode, DimeNet shows\nstate-of-the-art performance on 10 out of 12 tasks.\nThe effect of pre-training The ﬁrst pre-training curriculum consists of two steps: pre-training\nwith QM9 dataset followed by ﬁne-tuning with QM9 dataset. The second one consists of three steps:\npre-training with GDB-17 dataset [14], then the same two steps of the ﬁrst curriculum. The GDB-17\ndataset consists of 50M molecules, but, for our pre-training, we remove molecules including any\natom that does not appear in QM9 dataset, which resulted in around 40M molecules. The graph-level\ntasks for GDB-17 dataset should predict 11 molecule descriptors2 for a given molecule. Since those\ndescriptors are different from the properties of the 12 tasks in QM9 and can be simply obtained by\nRDKit [15], we clarify that the number of train data points with respect to the ground truth remains\nthe same.\nThe results of the pre-trainings are shown in Table 2, where the models pre-trained with the ﬁrst\ncurriculum and with the second curriculum are denoted as p-GRAT and pp-GRAT, respectively. For\nthe single-task mode, pp-GRAT achieves new state-of-the-art performance on 4 out of 12 tasks. Also,\nin terms of the stdMAE, pp-GRAT obtains new state-of-the-art performance across 12 tasks, overall.\nThese results are not achievable without the pre-trainings. In addition, the underlined numbers under\np-GRAT model represent superior performance than DimeNet even though they are inferior than\npp-GRAT. Lastly, by comparing the results of GRAT with p-GRAT and pp-GRAT, we can see the\npositive effect of the pre-trainings very clearly throughout all 12 tasks.\n4.2 Chemical Reaction Outcomes Prediction\nFor evaluating the ability of the graph-to-graph translation, we apply GRAT to predict the products\nof organic chemical reactions given their reactants and reagents using USPTO dataset [16] which is\nﬁltered and split by Jin et al. [17]. Although many graph-based solutions have been proposed for the\ntask [17–19], GRAT is the ﬁrst work that tries to tackle such a task as a graph-to-graph translation\nin end-to-end fashion, where each data point consists of reactants and reagents as input graphs and\nproducts as output graphs.\nTable 3 shows the performance of GRAT and other compared models. MT [ 20] showing the best\nperformance represents the input and output chemical compounds in SMILES and takes the sequence-\nTable 2: MAE of 12 regression tasks in QM9 - the effect of pre-training\nMode Multi-task Single-task\nTarget DimeNet p-GRAT pp-GRAT MPNN DimeNet p-GRAT pp-GRAT\nmu 0.0775 0.03986 0.03461 0.03 0.0286 0.03022 0.02807\nalpha 0.0649 0.09815 0.08359 0.092 0.0469 0.05839 0.05451\nHOMO 0.0451 0.02476 0.02175 0.04257 0.0278 0.02032 0.01908\nLUMO 0.0411 0.02695 0.02231 0.03741 0.0197 0.01844 0.01771\ngap 0.0592 0.0347 0.03164 0.0688 0.0348 0.03134 0.0297\nR2 0.345 2.12725 1.86826 0.18 0.331 0.53468 0.63276\nZPVE 0.00287 0.00419 0.00534 0.001524 0.00129 0.00189 0.002\nU0 0.0129 0.0433 0.04443 0.01935 0.00802 0.02629 0.02199\nU 0.013 0.04419 0.05684 0.01935 0.00789 0.02795 0.02511\nH 0.013 0.04551 0.05639 0.01677 0.00811 0.02734 0.02115\nG 0.0139 0.04289 0.05692 0.01892 0.00898 0.0268 0.01855\nCv 0.0309 0.0453 0.05403 0.04 0.0249 0.02929 0.02931\nstdMAE 1.92 1.4 1.32 1.7 1.05 1.01 0.95\nlogMAE -5.07 -4.60 -4.57 -5.09 -.5.57 -5.11 -5.18\n2See appendix A for more details of the 11 molecule descriptors.\n6\nTable 3: Comparison of Top-1 Accuracy Obtained by the Different Single-Model Methods on the\nCurrent Benchmark\nModels S2S [21] WLDN [17] GTPN [18] WLDN5 [19] MT [20] GRAT\nAccuracy 80.3 79.6 82.4 85.6 88.8 (90.43) 88.25\nFigure 2: An example of a chemical reaction (left) and visualization of attention values (right)\nto-sequence approach with Transformer. Note that we only take atom labels for nodes and bond\ntypes and distances for edges from the graph structure, hence there is loss of information during\nthe transformation of SMILES into the graph structure. We expect that the performance could be\nimproved if GRAT considers the other chemical information carried in SMILES such as formal\ncharges and valences. Adding more complete molecule information to GRAT is remained as future\nwork.\nGRAT has a merit of providing an easier way to explain the reaction mechanism than the SMILES-\nbased model by using the graph structure and the attention values. For example, we can visualize\nwhich parts of the reactants are combined to produce a product by pinpointing the participating atoms\nas shown in Figure 2, where two reactants produce a product. (We omit the reagent information for\nbrevity.) The node 2 is combined with 23 in the reactant side (numbers in red) and becomes 10 in\nthe product side (numbers in blue). In the attention matrix (the right side of the ﬁgure), we can see\nthat at the generation step for the node 10 in the product side (along the x-axis), the attention values\ncorresponding to the node 2, 22, and 23 in the reactant side (along the y-axis) are higher/brighter than\nothers.\n5 Related Work\nGraph neural networks (GNNs) are designed for dealing with graph data structures. As abstracted\nby Gilmer et al. [12], many GNN models can be described by a common framework that consists\nof a message, update, and readout function. For each node, the message function generates a\nrepresentation, i.e., message by aggregating the features of the node, its neighbor nodes, and the\nassociated edges. The update function updates the node representation using the message iteratively.\nThe readout function generates a representation for the whole graph by aggregating all the node\nrepresentations. Many variants of GNN models have been proposed in terms of the aggregation\nmethod for the message function. The convolution mechanism of CNN was used as an aggregation\nmethod for generating the node representation [ 22, 23]. GraphSage [ 24] uniformly sampled a\nﬁxed-size set of neighbors within khops to keep the computational cost ﬁxed and examined three\ndifferent aggregation methods called mean aggregator, LSTM aggregator, and pooling aggregator.\nAttention-based aggregation was also proposed [25]. For the update function, the iterative update\n3From the model that used the training data augmentation and averaged checkpoints.\n7\nfunction was proposed by early GNNs [ 26, 27], and GRU was employed for such mechanism by\nGated Graph Neural Network (GG-NN) [ 28]. Graph Isomorphism Network (GIN) [ 29] used the\nsimple message and update function to capture the graph structure and showed good performance\nin graph classiﬁcation tasks. Recently, there have been many efforts to incorporate Transformer\nin GNNs. Most of them take the encoder part of Transformer to use the power of its multi-head\nself-attention mechanism in aggregations and updates [7, 30–34].\nAlthough such variants of GNNs have been applied to a variety of domains, the applications can\nbe grouped into two categories: 1) classiﬁcation or regression tasks, and 2) graph generation tasks.\nThe tasks of the ﬁrst category contain document classiﬁcation based on citation networks [23, 24],\ntrafﬁc forecast [35, 36], link prediction in recommendation systems [37, 38], learning dynamics of\nphysical systems [39], molecular properties and protein interfaces prediction [12, 13, 40], cancer type\nclassiﬁcation and polypharmacy’s side-effect prediction in biomedical domains [41, 42], reasoning\nabout knowledge graphs [3, 43], classiﬁcation and segmentation of images, videos and 3D meshes\n[44, 45], classiﬁcation of regions in images [46], text generation [31], and sketch recognition [33].\nFor the graph generation tasks, graphs can be generated in two different approaches: 1) non-auto-\nregressive approach and 2) auto-regressive approach. As an non-auto-regressive approach, Grover\net al. [47] proposed an iterative decoding method over a single ﬁxed set of nodes based on a GNN-\nbased encoder. In addition, variational autoencoder (V AE) has been used for the various types of\ngraph generation [48–51] and has shown meaningful results, especially for molecule generation tasks,\nwhere detailed attributes of nodes and edges are required to be generated as well. Such non-auto-\nregressive approach, however, has not shown effectiveness for large-scale graph generation. As an\nauto-regressive approach, GraphRNN model [52] generates new nodes through the graph-level RNN\nwhile generating the edges for each newly generated nodes through the edge-level RNN. Moreover,\nGraph Recurrent Attention Network (GRAN) [53] enables the block-wise generation of nodes and\nassociated edges where the block size is controllable, which enables large-scale graph generation.\nHowever, GRAN has not shown its capability of predicting possible detailed attributes associated\nwith nodes and edges (such as node labels, edge types, etc.) other than just predicting the existence\nof the nodes and edges.\nGRAT is the general-purpose, graph-to-graph model that can be applied to not only generation but\nalso classiﬁcation or regression of graph data by enabling the self-attention mechanism to be aware of\nedge information. To the best of our knowledge, GRAT is the ﬁrst Transformer-based general-purpose,\ngraph-to-graph model. Jin et al. [54] is closely related to our work in terms of the graph-to-graph\ntranslation model in auto-regressive manner. However, their coverage of applications can be limited\nsince their generation is nothing but growing the given input graph through subsequent attachments.\nLastly, Graph2Graph Transformer [55] only uses the encoder part of Transformer, i.e., the multi-head\nself-attention is not fully utilized in its own decoder called Transition Classiﬁer. Also, it may not\ngenerate the entire graph, instead, it can only generate an edge for a given two nodes.\n6 Conclusion\nIn this work, we presented Graph-Aware Transformer (GRAT), a general and ﬂexible encoder-\ndecoder model for graph applications. By fully utilizing the power of multi-head self-attention from\nTransformer, GRAT outperformed the existing state-of-the-art models in the graph property prediction\ntasks. Also, GRAT applied the graph-to-graph translation approach to the task of predicting resulting\nproducts in organic chemical reactions for the ﬁrst time and showed the competitive performance\nover existing methods. In the future, we plan to apply GRAT to tasks in other domains rather than\nthe chemistry, such as physics or knowledge graphs, to prove its generality. Alongside that, we\nalso would like to ﬁnd a way to incorporate domain-speciﬁc (i.e., chemistry or materials science)\nknowledge into the model without detriment to the general Transformer architecture.\nReferences\n[1] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural\nnetworks: A review of methods and applications. CoRR, abs/1812.08434, 2018. URL http://arxiv.\norg/abs/1812.08434.\n[2] Guo Zhang, Hao He, and Dina Katabi. Circuit-GNN: Graph neural networks for distributed circuit\ndesign. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International\n8\nConference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pages\n7364–7373, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.\npress/v97/zhang19e.html.\n[3] Takuo Hamaguchi, Hidekazu Oiwa, Masashi Shimbo, and Yuji Matsumoto. Knowledge transfer for\nout-of-knowledge-base entities: A graph neural network approach. In Proceedings of the Twenty-Sixth\nInternational Joint Conference on Artiﬁcial Intelligence (IJCAI-17), 2017.\n[4] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for\nsocial recommendation. In The World Wide Web Conference, WWW ’19, page 417–426, New York, NY ,\nUSA, 2019. Association for Computing Machinery. ISBN 9781450366748. doi: 10.1145/3308558.3313488.\nURL https://doi.org/10.1145/3308558.3313488.\n[5] Keith T. Butler, Daniel W. Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. Machine learning\nfor molecular and materials science. Nature, 559:547–555, 2018.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in Neural Information Processing Systems 31, 2017.\n[7] Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrz˛ ebski.\nMolecule attention transformer. arXiv preprint arXiv:2002.08264, 2020.\n[8] V . Dumoulin, E. Perez, N. Schucher, F. Strub, H.d. Vries, A. Courville, and Y . Bengio. Feature-wise transfor-\nmations. Distill, 2018. doi: 10.23915/distill.00011. https://distill.pub/2018/feature-wise-transformations.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/\nN19-1423.\n[10] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec.\nStrategies for pre-training graph neural networks. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2020.\n[11] Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu,\nKarl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chem. Sci., 9:\n513–530, 2018. doi: 10.1039/C7SC02664A. URL http://dx.doi.org/10.1039/C7SC02664A.\n[12] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message\npassing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning,\npages 1263–1272. JMLR. org, 2017.\n[13] Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecular\ngraphs. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.\n[14] Lars Ruddigkeit, Ruud van Deursen, Lorenz C. Blum, and Jean-Louis Reymond. Enumeration of 166\nbillion organic small molecules in the chemical universe database gdb-17.Journal of Chemical Information\nand Modeling, 52(11):2864–2875, 2012. doi: 10.1021/ci300415d. URL https://doi.org/10.1021/\nci300415d. PMID: 23088335.\n[15] Greg Landrum. Rdkit: Open-source cheminformatics. URL http://www.rdkit.org.\n[16] D. M. Lowe. Patent reaction extraction: downloads; https://bitbucket.org/dan2097/patent-reaction-\nextraction/downloads. 2014.\n[17] W. Jin, C. Coley, R. Barzilay, and T. Jaakkola. Predicting organic reaction outcomes with weisfeiler-lehman\nnetwork. In Advances in Neural Information Processing Systems 31, 2017.\n[18] K. Do, T. Tran, and S. Venkatesh. Graph transformation policy network for chemical reaction prediction.\nIn Proceedings of the 25th ACM SigKDD International Conference on Knowledge Discovery & Data\nMining, pages 750–760, 2019.\n[19] C.W. Coley, W. Jin, L. Rogers, T.F. Jamison, T.S. Jaakkola, W.H. Green, R. Barzilay, and K.F. Jensen. A\ngraph-convolutional neural network model for the prediction of chemical reactivity. Chemical Science, 10:\n370–377, 2019.\n9\n[20] P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C.A. Hunter, C. Bekas, and A.A. Lee. Molecular transformer:\nA model for uncertainty-calibrated chemical reaction prediction. ACS Central Science, 5:1572–1583, 2019.\n[21] P. Schwaller, T. Gaudin, D. Lanyi, C. Bekas, and T. Laino. Found in translation: Predicting outcomes of\ncomplex organic chemistry reactions using neural sequence-to-sequence models. Chemical Science, 9:\n6091–6098, 2018.\n[22] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán\nAspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints.\nIn Advances in neural information processing systems, pages 2224–2232, 2015.\n[23] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In\nProceedings of the International Conference on Learning Representations (ICLR), 2017.\n[24] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\nAdvances in neural information processing systems, pages 1024–1034, 2017.\n[25] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.\nGraph attention networks. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2018.\n[26] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.\nIn Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pages\n729–734. IEEE, 2005.\n[27] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The\ngraph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.\n[28] Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow. Gated graph sequence neural networks.\nIn Proceedings of ICLR’16, April 2016. URL https://www.microsoft.com/en-us/research/\npublication/gated-graph-sequence-neural-networks/ .\n[29] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2019.\n[30] Edward Choi, Zhen Xu, Yujia Li, Michael W Dusenberry, Gerardo Flores, Emily Xue, and Andrew M\nDai. Learning the graphical structure of electronic health records with graph convolutional transformer. In\nAssociation for the Advancement of Artiﬁcial Intelligence, 2020.\n[31] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Hannaneh Hajishirzi. Text generation\nfrom knowledge graphs with graph transformers. In NAACL, 2019.\n[32] Tianming Wang, Xiaojun Wan, and Hanqi Jin. Amr-to-text generation with graph transformer.Transactions\nof the Association for Computational Linguistics, 8:19–33, 2020.\n[33] Peng Xu, Chaitanya K Joshi, and Xavier Bresson. Multi-graph transformer for free-hand sketch recognition.\narXiv preprint arXiv:1912.11258, 2019.\n[34] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer\nnetworks. In Advances in Neural Information Processing Systems, pages 11960–11970, 2019.\n[35] Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. Trafﬁc graph convolutional recurrent\nneural network: A deep learning framework for network-scale trafﬁc learning and forecasting. IEEE\nTransactions on Intelligent Transportation Systems, 2019.\n[36] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-\ndriven trafﬁc forecasting. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2018.\n[37] Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix completion. arXiv\npreprint arXiv:1706.02263, 2017.\n[38] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph\nconvolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 974–983, 2018.\n[39] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for\nlearning about objects, relations and physics. In Advances in neural information processing systems, pages\n4502–4510, 2016.\n10\n[40] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph\nconvolutional networks. In Advances in neural information processing systems, pages 6530–6539, 2017.\n[41] Sungmin Rhee, Seokjun Seo, and Sun Kim. Hybrid approach of relation network and localized graph\nconvolutional ﬁltering for breast cancer subtype classiﬁcation. In Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artiﬁcial Intelligence (IJCAI-18), 2018.\n[42] Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects with graph\nconvolutional networks. Bioinformatics, 34(13):i457–i466, 2018.\n[43] Zhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph alignment via\ngraph convolutional networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 349–357, 2018.\n[44] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 7794–7803, 2018.\n[45] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.\nDynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 38(5):1–12,\n2019.\n[46] Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. Iterative visual reasoning beyond convolutions.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7239–7248,\n2018.\n[47] Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs. arXiv\npreprint arXiv:1803.10459, 2018.\n[48] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308,\n2016.\n[49] Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via regularizing\nvariational autoencoders. In Advances in Neural Information Processing Systems, pages 7113–7124, 2018.\n[50] Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational\nautoencoders. In International Conference on Artiﬁcial Neural Networks, pages 412–422. Springer, 2018.\n[51] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular\ngraph generation. arXiv preprint arXiv:1802.04364, 2018.\n[52] Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. Graphrnn: Generating\nrealistic graphs with deep auto-regressive models. arXiv preprint arXiv:1802.08773, 2018.\n[53] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun,\nand Richard Zemel. Efﬁcient graph generation with graph recurrent attention networks. In Advances in\nNeural Information Processing Systems, pages 4257–4267, 2019.\n[54] W Jin, R Barzilay, and T Jaakkola. Hierarchical graph-to-graph translation for molecules. arXiv preprint\narXiv:1907.11223, 2019.\n[55] Alireza Mohammadshahi and James Henderson. Graph-to-graph transformer for transition-based depen-\ndency parsing. arXiv preprint arXiv:1911.03561, 2019.\n[56] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,\nGeoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\nLevenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,\nJonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay\nVasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,\nand Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL\nhttps://www.tensorflow.org/. Software available from tensorﬂow.org.\n[57] Oleksii Kuchaiev, Boris Ginsburg, Igor Gitman, Vitaly Lavrukhin, Jason Li, Huyen Nguyen, Carl Case,\nand Paulius Micikevicius. Mixed-precision training for nlp and speech recognition with openseq2seq.\narXiv preprint arXiv:1805.10387, 2018.\n[58] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[59] Inc. Daylight Chemical Information Systems. Smiles - a simpliﬁed chemical language. URL https:\n//www.daylight.com/dayhtml/doc/theory/theory.smiles.html.\n11\nAppendix\nA Additional Details for Section 4.1\nOur model has 32 self-attention layers with 32 heads, node representations of size 256, and feed-forward layers\nwith inner dimension 1024. As described in Section 3.3, the graph-level embedding resulted from the <CLS>\ntoken is fed into a fully connected layer with the hidden size 512 which generates 12 outputs corresponding to\nthe properties of the 12 regression tasks. fa consists of hidden states of size 32 with tanhactivation function\nand ﬁnal outputs of size 64 (γand βfor each layer).\nThe code is implemented in Tensorﬂow [56], and the self-attention is based on NVIDIA OpenSeq2Seq [57]. We\nused one GPU (Nvidia v100). The batch size is 50, and all models were trained using SGD with the ADAM\noptimizer [58].\nWe add the atom group number, the atom weight, and the formal charge for each atom as an additional input\nnode/atom features. Note that those are commonly used atom features in the chemical domain tasks, and are\neasily obtained by RDKit [15].\nIn addtion, the 11 molecule descriptors (obtained by RDKit) used for the pre-training are listed below:\n• Number of atoms\n• Topological polar surface area (TPSA)\n• logP\n• Molecular mass\n• Molecular weight\n• Number of valence electrons\n• Number of aromatic rings\n• Number of saturated rings\n• Number of aliphatic rings\n• Balaban’s J index (BalabanJ)\n• Bertz CT (BertzCT)\nB Additional Details for Section 4.2\nSince chemical compounds in the USPTO dataset are represented in Simpliﬁed Molecular Input Line Entry\nSystem (SMILES) [59], which is a sequence of characters designed to describe the chemical structure, we\nconvert the input compounds to input graph structures using RDKit [15] and convert the output graph structures\nback to SMILES once ﬁnishing the prediction to compare the predicted graph with the ground truth in SMILES.\nSince the tokens in GRAT do not need to contain the edge information, the vocabulary size is the same as\nthe number of elements in the periodic table plus a few special tokens such as <G> and <EOG>. This makes\nour model enable to maintain small parameters (about 8M) despite of many layers. (The model proposed\nby Schwaller et al. [20] has 12M parameters.) We add an additional special token, one of <REACTANT>,\n<REAGENT>, and <PRODUCT>, at the start of each graph as a delimiter to explicitly distinguish each graph\nfrom others as is done in [20]. These special tokens are also connected to all other atom nodes by introducing an\nadditional virtual edge type. Since SMILES imposes a canonical order on participating atoms in the compound,\nwe impose the same order on the atoms/nodes in the corresponding graph through the positional encoding for\nthe graph translation task.\nBoth of the encoder and the decoder have 24 self-attention layers with 8 heads, node representations of size 128,\nand feed-forward layers with inner dimension 256. fa for the encoder and the decoder each consists of hidden\nstates of size 32 with tanhactivation function and ﬁnal outputs of size 48 (γand βfor each layer). We assume\nthat there is a \"no bond\" edge type for the encoder because we want to utilize the distance information between\nevery pair of nodes. However, for the decoder, we do not use such \"no bond\" edge by making attention values\ncorresponding to \"no bond\" zero so that two disconnected nodes cannot attend on each other as described in\nSection 3.1.\nAs in Section A, the code is implemented in Tensorﬂow and based on NVIDIA OpenSeq2Seq. We used 8 GPUs\n(Nvidia p40), and the total batch size is 128. All models were trained using SGD with the ADAM optimizer [58].\nDuring decoding, the beam size is set to 8.\n12"
}