{
  "title": "FTRANS: Energy-Efficient Acceleration of Transformers using FPGA",
  "url": "https://openalex.org/W3043034704",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1623828274",
      "name": "Li Bingbing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753377104",
      "name": "Pandey Santosh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2381544141",
      "name": "Fang, Haowen",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lyv, Yanjun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978330610",
      "name": "Li Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2752357668",
      "name": "Chen, Jieyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4228065639",
      "name": "Xie, Mimi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353305043",
      "name": "Wan, Lipeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2076313181",
      "name": "Liu Hang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744413473",
      "name": "Ding, Caiwen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3103093779",
    "https://openalex.org/W3104393472",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2078492923",
    "https://openalex.org/W1489355995",
    "https://openalex.org/W2889736774",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2949847915",
    "https://openalex.org/W584173323",
    "https://openalex.org/W2962934478",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2962820060",
    "https://openalex.org/W2964199361"
  ],
  "abstract": "In natural language processing (NLP), the \"Transformer\" architecture was proposed as the first transduction model replying entirely on self-attention mechanisms without using sequence-aligned recurrent neural networks (RNNs) or convolution, and it achieved significant improvements for sequence to sequence tasks. The introduced intensive computation and storage of these pre-trained language representations has impeded their popularity into computation and memory-constrained devices. The field-programmable gate array (FPGA) is widely used to accelerate deep learning algorithms for its high parallelism and low latency. However, the trained models are still too large to accommodate to an FPGA fabric. In this paper, we propose an efficient acceleration framework, Ftrans, for transformer-based large scale language representations. Our framework includes enhanced block-circulant matrix (BCM)-based weight representation to enable model compression on large-scale language representations at the algorithm level with few accuracy degradation, and an acceleration design at the architecture level. Experimental results show that our proposed framework significantly reduces the model size of NLP models by up to 16 times. Our FPGA design achieves 27.07x and 81x improvement in performance and energy efficiency compared to CPU, and up to 8.80x improvement in energy efficiency compared to GPU.",
  "full_text": "FTRANS: Energy-Efficient Acceleration of Transformers using\nFPGA\nBingbing Li1, Santosh Pandey2, Haowen Fang3, Yanjun Lyv1, Ji Li4, Jieyang Chen5, Mimi Xie6,\nLipeng Wan5, Hang Liu2 and Caiwen Ding1\n1University of Connecticut 2Stevens Institute of Technology 3Syracuse University\n4Microsoft Corporation 5Oak Ridge National Laboratory 6University of Texas at San Antonio\n1{bingbing.li, lyu.yanjun, caiwen.ding}@uconn.edu 2{spande1, Hang.liu}@stevens.edu 3hfang02@syr.edu\n4changzhouliji@gmail.com 5{chenj3, wanl}@ornl.gov 6mimi.xie@utsa.edu\nABSTRACT\nIn natural language processing (NLP), the “Transformer\" architec-\nture was proposed as the first transduction model replying entirely\non self-attention mechanisms without using sequence-aligned re-\ncurrent neural networks (RNNs) or convolution, and it achieved\nsignificant improvements for sequence to sequence tasks. The in-\ntroduced intensive computation and storage of these pre-trained\nlanguage representations has impeded their popularity into compu-\ntation and memory constrained devices. The field-programmable\ngate array (FPGA) is widely used to accelerate deep learning algo-\nrithms for its high parallelism and low latency. However, the trained\nmodels are still too large to accommodate to an FPGA fabric. In\nthis paper, we propose an efficient acceleration framework,Ftrans,\nfor transformer-based large scale language representations. Our\nframework includes enhanced block-circulant matrix (BCM)-based\nweight representation to enable model compression on large-scale\nlanguage representations at the algorithm level with few accuracy\ndegradation, and an acceleration design at the architecture level. Ex-\nperimental results show that our proposed framework significantly\nreduce the model size of NLP models by up to 16 times. Our FPGA\ndesign achieves 27.07×and 81 ×improvement in performance and\nenergy efficiency compared to CPU, and up to 8.80×improvement\nin energy efficiency compared to GPU.\nACM Reference Format:\nBingbing Li1, Santosh Pandey2, Haowen Fang3, Yanjun Lyv1, Ji Li4, Jieyang\nChen5, Mimi Xie6, Lipeng Wan5, Hang Liu2 and Caiwen Ding1. 2020. FTRANS:\nEnergy-Efficient Acceleration of Transformers using FPGA. In ACM/IEEE\nInternational Symposium on Low Power Electronics and Design (ISLPED ’20),\nAugust 10–12, 2020, Boston, MA, USA. ACM, New York, NY, USA, 7 pages.\nhttps://doi.org/10.1145/3370748.3406567\n1 INTRODUCTION\nRNN and its variant Long Short-Term Memory (LSTM) unit [6] and\nGated Recurrent unit (GRU) [3] used to dominate in sequence mod-\neling, language modeling and machine translation, etc. However,\nthey in general lack efficiency in transmitting global information,\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nISLPED ’20, August 10–12, 2020, Boston, MA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7053-0/20/08. . . $15.00\nhttps://doi.org/10.1145/3370748.3406567\ndue to the bottleneck in the memory (hidden state) and compli-\ncated bypassing logic (additive and derivative branches) where long\nrange information is passed. In addition, the inherently sequential\nnature precludes parallelization within training examples through\nbackpropagation, which is critical at longer sequence lengths [9].\nTo overcome the shortcomings in RNNs, the “Transformer\" ar-\nchitecture was proposed as the first transduction model replying\nentirely on self-attention mechanisms without using sequence-\naligned RNNs or convolution. It achieved notable improvements\nfor sequence to sequence tasks [18]. The breakthroughs and devel-\nopments of new models have accelerated at an unprecedented pace\nsince the attention mechanisms have become the mainstream in\nNLP domain with the invention of Transformer. Many transformer-\nbased NLP language models like BERT [4] and RoBERTa [10] intro-\nduced pretraining procedures to the transformer architecture and\nachieved record-breaking results on major NLP tasks, including\nquestion answering, sentiment analysis, and language inference.\nNevertheless, the introduced intensive computation and power\nfootprint of these pre-trained language representations has impeded\ntheir popularity into computation and energy constrained as edge\ndevices. Moreover, despite of the rapid advancement achieved by\nthe recent transformer-based NLP models, there is a serious lack of\nstudies on compressing these models for embedded and internet-\nof-things (IoT) devices.\nIn this paper, we propose an energy-efficient acceleration frame-\nwork, Ftrans, for transformer-based large scale language repre-\nsentations using FPGA. Ftrans is comprised of an enhanced BCM-\nbased method enabling model compression on language represen-\ntations at the algorithm level, and an acceleration design at the\narchitecture level. Our contributions are summarized as follows:\n•Enhanced BCM-based model compression for Transformer.\nWe address the accuracy degradation caused by traditional BCM\ncompression, and propose an enhanced BCM-based compression\nto reduce the footprint of weights in Transformer. With small\naccuracy loss, Ftrans achieves up to 16 times compression ratio.\n•Holistic optimization for Transformers on FPGA. Given\nthe large size and complex data flow of transformer-based mod-\nels, even with model compression, we still need to schedule\nthe computation resources carefully to optimize latency and\nthroughput. We propose a two stage optimization approach to\nmitigate the resource constraints and achieve high throughput.\n•Low hardware footprint and low power (energy) consump-\ntion. We propose an FPGA architecture design to support the\nmodel compression technique and we develop a design automa-\ntion and optimization technique. Overall, the proposed Ftrans\nachieves the lowest hardware cost and energy consumption in\narXiv:2007.08563v1  [cs.DC]  16 Jul 2020\nISLPED ’20, August 10–12, 2020, Boston, MA, USA Bingbing Li et al.\nimplementing Transformer and RoBERTa compared to CPU and\nGPU references.\nExperimental results show that our proposed framework signifi-\ncantly reduce the size of NLP models by up to 16 times. Our FPGA\ndesign achieves 27.07×and 81 ×improvement in performance and\nenergy efficiency compared to CPU. The power consumption of\nGPU is up to 5.01×compared to that of FPGA, and we achieve up\nto 8.80×improvement in energy efficiency compared to GPU.\n2 RELATED WORK\nAttention mechanisms have become an integral part of compelling\nsequence modeling and transduction models in various tasks [9]. Ev-\nidence of NLP community moving towards attention-based models\ncan be found by more attention-based neural networks developed\nby companies like Amazon [8], Facebook [16], and Salesforce [2].\nThe novel approach of Transformer is the first model to eliminate\nrecurrence completely with self-attention to handle the dependen-\ncies between input and output. BERT [4] and RoBERTa [10] extend\nTransformer’s capacity from a sequence to sequence model to a\ngeneral language model by introducing the pretraining procedure,\nand achieved state-of-the-art results on major NLP benchmarks.\nAlthough RNNs and Convolutional Neural Networks (CNNs) are\nbeing replaced by Transformer-based models in NLP community,\nthere are only a few works that accelerate Transformers and fo-\ncus on reducing the energy and power footprint, e.g., a case study\nof Transformer is presented in [1] using one of the cutting-edge\nFPGA boards. However, it is noteworthy that [1] targets at a special-\nized FPGA architecture, which employs High Bandwidth Memory\n(HBM) technology. Unlike the conventional FPGA, HBM is pack-\naged directly within the FPGA fabric to alleviate the on chip mem-\nory constraint. However, work [1] did not adopt model compression\ntechnique, and used the sequence length of 8 and 16, which are\ntoo short and not favorable in practise. The model details such as\nnumber of encoder/decoders, hidden size are also not listed.\n3 TRANSFORMER WORKLOAD ANALYSIS\nThe “Transformer\" architecture is the heart for all state-of-the-\nart large scale language models. It has an encoder-decoder struc-\nture [18] as shown in Figure 1. The encoder maps a sequence of the\ninput symbols x = (x1; x2; x3; ... ; xn)to a sequence of continuous\nrepresentations z = (z1; z2; z3; ... ; zn). Given x, the decoder then\nproduces an output sequence y = (y1;y2;y3; ... ;ym)of symbols one\nelement per time step. For the next time step, the model takes the\npreviously generated symbols as additional input when generating\nthe next. The Transformer follows this overall architecture using\nstacked self-attention and fully-connected (FC) layers for both the\nencoder and decoder, shown in Figure 1.\nInputs\nembedding\nPositional \nencoding Encoder *N\nDecoder *N\nOutputs\nembedding\nFC \nlayers\nMulti-head\nattention\nAdd & \nNorm\nAdd & \nNorm\nFC \nlayers\nMasked\nmulti-head\nattention\nAdd & \nNorm\nAdd & \nNorm\nMulti-head\nattention\nAdd & \nNorm\n+\n+ Linear\nOutput\nprobabilities\nInputs\nOutputs\nSoftmax\nPositional \nencoding\nFigure 1: Model structure of Transformer.\nEncoder: The encoder consists of a stack of N identical layers.\nEach layer has two sub-layers. The first is a multi-head self-attention\nmechanism, and the second is a FC feed-forward network. There is\na residual connection around each of the two sub-layers, followed\nby layer normalization.\nDecoder: The decoder contains of a stack of N identical layers.\nWithin each layer, there are three sub-layers, where the third sub-\nlayer is the same as the encoder. The inserted second sub-layer\nperforms multi-head attention over the output of encoder stack. The\nfirst-sublayer utilizes masked multi-head attention, to ensure that\npredictions for position i only depends on its previous positions.\n3.1 Attention\nThe attention function can be described as mapping a queryq and a\nset of keys k and values v pairs to an output o as shown in Figure 2\n(a), named scaled dot-product attention, or single head attention.\n3.1.1 Single Head Attention. In this paper, we select dot-product\nattention as the attention function since it is much faster and more\nspace-efficient [18]. The input consists of queries and keys of di-\nmension dk , and values of dimension dv . We denote\np\ndk is the\nscaling factor for dot-product attention. We compute the dot prod-\nucts of the query with all keys, divide each by\np\ndk , and apply a\nsoftmax function to obtain the weights on the values. The attention\nfunction on q, k, and v can be computed simultaneously by con-\ncatenated into matrix Q, K, and V, respectively. Accordingly, the\noutput matrix Oatt is:\nOatt = Attention (Q, K, V)= sof tmax(QKT\np\ndk\n) (1)\n3.2 Multi-head Attention\nMulti single-head attention are then concatenated as multi-head\nattention, as shown in Figure 2 (b). MultiHead (Q, K, V) = Concat\n(Head1, ··· , Headh)×WO , where the Head is defined as:\nHeadi = Attention (QWQ\ni , KWK\ni , VWV\ni ) (2)\nwhere the projections are parameter matrices WQ\ni ∈Rdmodel ×dk ,\nWK\ni ∈Rdmodel ×dk , and WV\ni ∈Rdmodel ×dv . Multi-head attention\nenables the model to jointly attend to information from different\nrepresentation subspaces at different positions [18].\nIn this work, we implement a shallow Transformer and a large\nscale Transformer, i.e., RoBERTa. The shallow Transformer hash\n= 2 parallel attention layers with 4 attention heads and RoBERTa\n(base configuration) has 12 layers with 12 heads. For each head\nwe use dk = dv = dmodel /h = 200 and 768 for Transformer and\nRoBERTa, respectively.\nMatMul MatMul\n(a) Scaled dot-product attention\n{\n(b) Multi-head attention\nScale\nh\nMask\nConcat\nQ\nK\nV\nSoftmax\nLinear\nLinear\nScaled \ndot-product\nattention\nLinear\nLinear\nQ\nK\nV\nFigure 2: : (a) Scaled Dot-Product Attention. (b) Multi-Head\nAttention.\nFTRANS: Energy-Efficient Acceleration of Transformers using FPGA ISLPED ’20, August 10–12, 2020, Boston, MA, USA\n4 TRANSFORMER COMPRESSION USING\nENHANCED BLOCK-CIRCULANT MATRIX\nThe introduced intensive computation and weight storage of large\npre-trained language representations have brought challenges in\nhardware implementation. Therefore, model compression is a natu-\nral method to mitigate the these challenges.\n4.1 Enhanced BCM-based Transformer\nCirCNN [5] and C-LSTM [19] have adopted BCM for model com-\npression on small to medium scale datasets in image classification\nand speech recognition, respectively, and achieved significant im-\nprovement in terms of performance and energy efficiency compared\nto the prior arts. Using this method, we can reduce weight storage\nby replacing the original weight matrix with one or multiple blocks\nof circulant matrices, where each row/column is the cyclic reformu-\nlation of the others. We useb to represent the row/column size of\neach circulant matrix (or block size, FFT size). Suppose the shape of\na weight matrix in Transformer (e.g.,WQ\ni , WK\ni , WV\ni )is W ∈Rm×n,\nthere will be f ×д blocks after partitioning W, where f = m ÷b\nand д = n ÷b. Then W = [Wij ], i ∈{1 . . . f }, j ∈{1 . . .д}.\nThe input x is also partitioned as x = [xT\n1 , xT\n2 , . . . ,xTд ]T . In\neach BCM, only the first column/row is needed for storage and\ncomputation, and is termed the index vector , pij . The theoretical\nfoundation is derived in [ 20], which demonstrates the universal\napproximation property and the error bounds of BCM-based neural\nnetworks are as efficient as general neural networks.\nPrior works [5, 19] have not investigated large-scale language\nrepresentations. To further maintain the prediction accuracy, we\nuse an enhanced BCM-based model compression. We modify the\nformulation of the index vector as follows:\npij =\n\n1\nb\nÍb\nj=1 W1j\n1\nb\nÍb\nj=1 W2j\n. . .\n1\nb\nÍb\nj=1 Wbj\n\n(3)\nwhere Wij is a circulant matrix. We observe that in this way, we\ncan better preserve the parameter information and maintain the\noverall prediction accuracy. The main reason is that prior works\ntake the first column/row as the index vector , missing the effective\nrepresentations for other rows/columns.\nBased on the circulant convolution theorem [14, 17], instead of\ndirectly performing the matrix-vector multiplication, we could use\nthe fast Fourier transform (FFT)-based multiplication method, and\nit is equivalent to matrix-vector multiplication. The calculation\nof a BCM-based matrix-vector multiplication Wij xj is: Wij xj =\npij ⊛xj = IFFT\u0000FFT(pij )◦FFT(xj )\u0001, where ‘⊛ ’ represents circular\nconvolution, and ◦is element-wise multiplication. Therefore, the\ncomputational complexity is reduced from O(b2) to O(b logb).\n5 ARCHITECTURE\nFPGA is widely used to accelerate deep learning models for its\nhigh parallelism and low latency. As large amount of transformer\nparameters exceed the on-chip memory or block RAM (BRAM)\ncapacity on FPGA fabric, even with model compression technique,\nthe full model cannot be stored on chip. To address the challenge,\nwe partition a model into embedding layer and encoder/decoder\nstacks. The embedding layer contributes 30.89% of parameters. Es-\nsentially it is a look-up table which transforms discrete tokens into\ncontinuous space, the computation is less than that of encoder and\ndecoder. Therefore, our basic idea is to off-load embedding layer\nto off-chip memory, thus it is possible to deploy the most compu-\ntational intensive part, i.e. the encoder and decoder stack on chip,\navoiding frequently access off-chip weights, hence to accelerate\ncomputation. Second, to mitigate the I/O constrain, we developed\nthe inter-layer coarse grained pipelining, intra-layer fine grained\npipeling, and computation scheduling.\n5.1 Overall Hardware Architecture\nAs shown in Figure 3, the proposed hardware architecture consists\nof computation units for encode/decoder computation, on-chip\nmemory banks, a transformer controller, and an off-chip memory\n(DDR) and DDR controller. The transformer controller communi-\ncates with the host and controls all the modules in FPGA. The host\nPC loads the inputs (i.e., sentence pairs ) to the FPGA for inference\nthrough PCIE. On the FPGA part, given the tokenized sentences,\nthe embedding look up module accesses DDR to fetch embeddings.\nNext, the embeddings will be fed into the pipelined encoder/decoder\nstacks to perform inference.\nThe computing units consist of multi-head attention, scaled dot\nproduct attention, point wise feed forward layer, linear, and ad-\nd/norm. The transformer controller orchestrates the computing\nflow and data flow of inputs from PCIEs, BRAMs and computing\nunits on the FPGA fabric. Since the encoder and decoder share same\ntype of operations, so we first decompose them into different com-\nputing primitives, including matrix multiplication of different sizes,\nvectorized exponentials etc. The multi-head attention, linear, and\nadd/norm modules are reconfigured to form as encoder or decoder\nunder the transformer control logic. We have two pipeline strate-\ngies. For shallow networks, the entire network can be straightfor-\nwardly implemented, i.e. all layers can be implemented by dedicated\nFPGA resources. For the state-of-the-art designs such as BERT and\nRoBERTa, there are multiple encoders/decoders, hence the resource\nsuch as DSPs may not enough. In such cases, reuse of certain PE or\nentire encoder/decoder module are necessary.\n5.2 Multi-Head Attention Design\nMulti-head attention includes multi- processing elements (named\nPE) banks, for matrix multiplication), buffers (K buf, Q buf, and\nV buf), a normalization module (Norm), a masking function for\nmasked multi-head attention, and a softmax module as described\nin Equation (2) and shown in Fig. 4.\nPCIe\nMemory Bank\nDDR\nController DDR\nHost\nCPU\nHost\nMemory\nTransformer\nControl Logic\nHost FPGA\nEncoder Stack\nEmbedding\nLookup\nDecoder Stack\nMemory Bank\nBuffer\nFigure 3: The overall hardware architecture on FPGA.\nISLPED ’20, August 10–12, 2020, Boston, MA, USA Bingbing Li et al.\nK Buf\nFPGA Fabric\nHead Cntl\n0\nPE BankBRAM\n    K\nQ Buf\nV Buf\nBRAM\n    Q\nBRAM\n    V\nh\nPE Bank\nPE Bank\nPE Bank\nPE Bank\nSoftmax\nNorm\nK Buf\nFPGA Fabric\nHead Cntl\n0\nPE BankBRAM\n    K\nQ Buf\nV Buf\nBRAM\n    Q\nBRAM\n    V\n2\nPE Bank\nPE Bank\nPE Bank\nPE Bank\nSoftmax\nNorm\nK Buf\nWeights\nFPGA Fabric\nHead Cntl\n0\nInput\n...\n...\nPE BankDDR BRAM\n    K\nQ Buf\nV Buf\nBRAM\n    Q\nBRAM\n    V\nPCIE\n1\nPE Bank\nPE Bank\nPE Bank\nPE Bank\nSoftmax\nNorm\nFigure 4: Multi-head attention (Head 1, ··· , Headh) design.\nThe input are fetched from DDR and fed into encoder pipeline,\nthen multiplied with a set of query matrixQ and key matrixK stored\non BRAMs. The intermediate resultsQWQ and KWK are then prop-\nagated to the buffers (i.e., K buffer and Q buffer to store KWK , and\nQWQ , respectively). Next, we compute the matrix multiplication\nof the values stored in the K buffer and W buffer. The product will\nbe loaded to the normalization (Norm) module, i.e.,product√\ndk\n. After\nthe softmax module, the results will be propagated to a PE bank\nto perform matrix multiplication with the matrix stored in V Buf,\ni.e., VWV . Each head will have a local controller to orchestrate the\ncomputing flow and data flow of PE banks and buffers. The local\ncontroller will also enable the multiplexer for masked multi-head\nattention with a goal of masking the future tokens in a sequence\n(setting to 0), to prevent the current output predictions from being\nable to see later into the sentence. To support masked multi-head\nattention, the local controller controls multiplexer to set future\ntokens to 0, such that current output predictions are not able to\nsee later sequences. Decoder has one more multi-head attention,\nthus takes longer time to compute than encoder. In the case of\ndecoder module has to be reused, to prevent encoder pipeline stall,\na buffer is placed between the encoder and decoder stacks. The\nbuffer also stores the output from the last encoder to support the\nresidue connection.\n5.3 PE Design and Softmax Module\nWe develop three different configurable PEs, which as PE-A, PE-B,\nand PE-FFT/IFFT. For the BCM-based matrix-vector multiplication\nin FC layers, we use FFT/IFFT-based processing elements (PE); for\nother layers, we use matrix-vector multiplication, i.e., PE-A and\nPE-B for different matrix sizes.\n5.3.1 Matrix-vector multiplication-based PE. The major part of the\nPE-A or PE-B is a multiplier for matrix multiplication of different\nsizes. It also consists of two accumulators, dividers and exponential\nFFT\nBRAM FFT(W) \nMAC\nPE\nRegister bank\nFFT\n(FFT twiddle factors)\n(b) Softmax module(a) PE design\n Buf\nAccExp(-x)\nDIV\nFigure 5: FFT/IFFT-based PE and softmax module design.\nunits to support scaling and softmax required by multi-head atten-\ntion. The output of multipliers are fed into divider or accumulator\nas stream, hence scaling and softmax layer can be overlapped with\nmatrix multiplication.\n5.3.2 FFT/IFFT-based PE . Figure 5 shows the design of FFT/IFFT-\nbased PE and softmax, including a FFT/IFFT kernel, an accumulator,\nand an adder. The accumulator is an adder tree with N inputs (the\nsize is chosen the same as the FFT/IFFT kernel size). We select\nRadix-2 Cooley Tukey algorithm [7] for FFT implementation.\n5.3.3 Softmax Module. Figure 5 (b) shows the implementation of\nthe softmax function softmax(x)i = exp (xi )Í\nj exp (xj )). The exponential\nfunction exp (xi )or exp (xj )is expensive in resource consumption\nfor FPGAs. We adopt piece-wise linear functions to estimate their\noutputs, in order to simultaneously reduce the resource consump-\ntion and maintain the accuracy. A buffer is used to store exp (xi )\nand an accumulator is used to compute the summation of exp (xj ).\nNext, we perform the division and generate the softmax results.\n6 DESIGN AUTOMATION & OPTIMIZATION\nWe developed a workflow to prototype and explore the hardware\narchitecture. First, we generate a data dependency graph based on\ntrained models to illustrate the computation flow. The operators\nin graph are scheduled to compose the pipeline under the design\nconstraints, to achieve maximum throughput. At last, a code gener-\nator receives the scheduling results and generates the final C/C++\nimplementation, which can be fed into the commercial HLS tool\nfor synthesis. Our target synthesis backend is Xilinx SDx.\nThe major computationally intensive operations are shown in\nFigure 6. Other operations such as division and softmax consume\nmuch less time, and can be merged/overlapped with these major\noperations. The computation in different layers can be decomposed\ninto common computing elements, i.e., PEs. The layers in same\ncolor can be performed by same PE, however, with unbalanced\noperations. For example, the time consumed by the KWK , QWQ\nand VWV is roughly 4 times of computation required by the n\nheads. To improve the utilization of pipeline, it is desirable to let\nInput\nVWV\nHead \n1\nHead \nn\nHead \n2\nAttn 1\nAttn n\nAttn 2\nsplit\nFC\n Add 1\n FFT-\nIFFT 1\n Add 2\nFFT-\nIFFT 2\nInput\nInput\n1 2 3 4 5 6 7 80\nQWQ\n split\nKWK\n split\nFigure 6: Data flow of major operations.\nMM-A 1\nMM-A 2\nHead\nHead\nHead\nHead\nHead\nHead\nHead\nHead\nAtt\nAtt\nAtt\nAtt\nAtt\nAtt\nAtt\nAtt\nFC\nAdd\nFFT-IFFT\n FFT-IFFT\nAdd\nWk * k\nWq * q\nWv * v\nMM-A 3\nMM-A 4\nMM-B 1\nMM-B 2\nMM-B 3\nMM-B 4\nAdder\nFFT-IFFT\nResource\n Stage 1\n Stage 2\n Stage 3\n Stage 4\n Stage 5\n Stage 6\n Stage 7\n Stage 8\nAdder\nFigure 7: Fine grained operation scheduling\nFTRANS: Energy-Efficient Acceleration of Transformers using FPGA ISLPED ’20, August 10–12, 2020, Boston, MA, USA\neach layers consumes roughly the same time. This can be achieved\nby allocating more resources to the slowest layer. We adopt a two-\nstage optimization flow. In first stage, we find a resource scheme\nthat can minimize the maximum time required by layers. In second\nstage, under such resource constrains, we optimize the scheduling\nof an individual encoder/decoder.\nThe optimization starts from a basic implementation of an indi-\nvidual encoder/decoder, i.e. no parallization nor resource reusing,\nsuch that we can obtain an estimation of resource consumption,\nnumber of operations and execution time of each layer, through-\nput obtained by unit number of resources. Then we will examine\nhow much resource can be allocated to each encoder/decoder to\nminimize the execution time of the slowest layer:\nminimize max(T1, T2, ..., Tn),\nsubject to RF [i]≥ M\nÕ\nj\nRj [i]+ Rmisc [i] (4)\nwhere i ∈(0, ..., 3), j ∈n, n is the number of layers, M is the to-\ntal number of encoder/decoder, RF = [RF F, RLUT , RDSP , RBRAM ]\nis on-chip resource constraints for look-up table (LUT), flip-flop\n(FF), digital signal processing unit (DSP), and BRAM, respectively.\nTj is the time required by the j-th layer. Rj is resource utiliza-\ntion of the j-th layer, which is also represented as a vector: Rj =\n[Rj\nF F, Rj\nLUT , Rj\nDSP , Rj\nBRAM ]. Rmisc is the resource utilization of\nmodules except encoder/decoder module, such as DDR controller,\nPCIE controller, etc.Tj can be given as:\nTj = ⌈N i\nop /(Fj ·Kj )⌉, j ∈n (5)\nwhere N iop is the number of operations required by the j-th layer.\nKj is resource allocation factor of the j-th layer. Fj is the through-\nput of non-optimized design, which can be obtained empirically.\nTherefore, the throughput is:\nThrou дhput = f req/(n ·max(T1, T2, ..., Tj )) (6)\nIt finds the slowest layer, allocates more resources, then up-\ndates the resource consumption and execution time. If resource\nconstraints are satisfied, we repeat this procedure until no more\nspeedup. Then the algorithm will examine the fastest layer. If it\ntakes significantly less time than the slowest layer, it is possible to\nallocate less resources for that layer, hence more resources can be as-\nsigned to the slowest layer. After this procedure, we obtain resource\nconstraints, e.g. the No. of different PEs of an encoder and decoder.\nUnder resource constraints, each layer may not have dedicated com-\nputation resource, hence matrix multipliers, adders, etc. have to be\nshared. Therefore, the computation has to be carefully scheduled to\nminimize the computation latency. The encoder/decoder can be rep-\nresented as a Directed Acyclic Graph (DAG) –G(V , E), whereV is a\nset of vertices representing different computation, edges E indicate\nthe dependency. The available computation units such as PEs and\nadders are represented by a setOp = {PE −A1, PE −A2, ..., Adder }.\nThe algorithm used for operation scheduling takes G and Op as\ninput, is shown in Algorithm 1.\n7 EVALUATION\n7.1 Training of Transformer-Based Language\nRepresentation\nIn this section, we apply both enhanced BCM-based model compres-\nsion on the linear layers, and adopt 16 fixed-point data representa-\ntion for all the weights. We evaluate the accuracy impact with two\nAlgorithm 1: Pseudo-code for operation scheduling\nInput: Dependency graph G(V, E), available PEs Op = {P E_A1, P E_A2, . . .,Adder }\nOutput: C and designated P Efor each Layer\nQ = T OPO_SORT (G(V, E))\\\\Topological sort to obtain priority queue of all layers\nP = Q[0]\\\\List of layers to be scheduled\nE = ∅\\\\List of layers being executed\nS = ∅\\\\The final schedule result\nsta дe = 0\nwhile Q , ∅∧E , ∅do\nfor layer ∈Q do\nif available P E∃Op for layer then\nQ .pop ()\nOp .remo ve(P E)\nE.push _back ((layer , P E))\nfor V ∈N EIGH BOR(layer )do\nQ .push _back (V )\nend\nend\nsta дe+ =1\nfor layer , P E∈E do\nif IS _F I N ISH ED(layer )== T ruethen\nE.pop ()\nS .push _back ((layer , sta дe, P E))\nOp .push _back (P E)\nend\nend\nreturn S\nrepresentative Transformer structures, i.e., a shallow Transformer\nwith both encoder and decoder, and a pretrained deep Transformer\narchitecture - RoBERTa (base configuration) which only has en-\ncoder [10]. The shallow Transformer is evaluated in a language\nmodeling task, which is an unsupervised sequence-to-sequence\nproblem that requires the decoder part. On the other hand, we run\na RoBERTa on a sentiment classification task that is a supervised\nclassification problem without the requirement for decoder block.\nThe software is implemented in PyTorch deep learning framework\n[15] and FairSeq sequence modeling toolkit [13]. Table 1 summa-\nrizes the key parameters of the shallow Transformer and RoBERTa\nmodels in the experiments.\nTable 1: Key parameters of Shallow Transformer and\nRoBERTa\nModel TransformerTransformerHiddenAttentionTotal\nConfiguration Structure Layers Size Heads Params\nShallow Transformerencoder-decoder2 200 4 6M\nRoBERTa (base config.)encoder only 12 768 12 125M\n7.1.1 Finetuned RoBERTa for Sentiment Classification. We evaluate\nthe proposed model compression method for finetuned RoBERTa [10]\non IMDB movie review sentiment classification [11] to shed some\nlight on training trial reductions. Starting from the saved state of\npretrained models in work [ 10], we finetune the model until it\nreaches to its best validation accuracy at 95.7%. To maintain over-\nall accuracy, we compress partial layers. The process suppresses\nrandomness by using a deterministic seed. Thus the accuracy dif-\nference between the original RoBERTa and compressed version is\nsorely contributed by the compression techniques.\n7.1.2 Shallow Transformer. Language modeling task takes a se-\nquence of words as input and determines how likely that sequence\nis the actual human language. We consider the popular WikiText-2\ndataset [12] in this experiment, which contains 2M training tokens\nwith a vocabulary size of 33k. A shallow Transformer model with 4\nattention heads and 200 hidden dimension is established.\nThe baseline and model compression results of shallow Trans-\nformer and RoBERTa on WikiText-2 and IMDB review are shown\nISLPED ’20, August 10–12, 2020, Boston, MA, USA Bingbing Li et al.\nTable 2: Comparison among different model configurations\nID Network BlockWikiText-2ACC loss ACC loss\nType Size (ACC) %with BCM (%)with BCM & Quant. (%)\n1 Shallow Transformer− 91.3 − −\n2 Shallow Transformer4 90.7 0.6 0\n3 Shallow Transformer8 90.7 0.6 0.6\n4 Shallow Transformer16 90.0 1.3 0.6\nID Network Block IMDB ACC loss ACC loss\nType Size (ACC)% with BCM (%)with BCM & Quant. (%)\n4 RoBERTa (base)− 95.7 − −\n5 RoBERTa (base)4 91.5 4.2 4.3\n6 RoBERTa (base)8 91.4 4.3 4.3\nTable 3: Comparison among different model configurations\nShallow Transformer\nBatch SizeDSP FF LUT Latency (ms)Power (W)Throughput (FPS)\n1 5647 3040122689332.94 22.45 680.91\n4 5647 30429626936111.59 22.52 690.50\n8 5647 30582026975322.90 22.66 698.72\n16 5647 30617627044945.54 22.73 702.54\nRoBERTa (base)\nBatch SizeDSP FF LUT Latency (ms)Power (W)Throughput (FPS)\n1 6531 50661245107310.61 25.06 94.25\n4 6531 50693645154540.33 25.13 99.13\n8 6531 50848845200579.03 25.89 101.23\n16 6531 508916452661157.18 25.96 101.79\nin Table 2, respectively. We compress the models using enhanced\nBCM-based method with block size of 4 or 8. From Table 2, we\nobserve that for the shallow Transformer, thers is no accuracy loss\nwith block size of 4 and only 0.6% accuracy loss with block size of\n8. The RoBERTa, on the other hand, incurs 4.2% and 4.3% accuracy\ndrop after model compression using 4 and 8 block size, respectively1.\nWe also observe that changing from 32-bit floating point to 16-bit\nfixed point will not cause accuracy loss. The comparable accuracy\nbetween the original model and the weight compressed version\ndemonstrates the effectiveness of the proposed model compression\nmethod.\n7.2 Performance and Energy Efficiency\n7.2.1 Experimental Platform. The Xilinx Virtex UltraScale+ VCU118\nboard, comprising 345.9Mb BRAM, 6,840 DSPs, 2,586K logic cells\n(LUT), and two 4GB DDR5 memory, is connected to the host ma-\nchine through PCIE Gen3 ×8 I/O Interface. The host machine\nadopted in our experiments is a server configured with multiple\nIntel Core i7-8700 processors. We use Xilinx SDX 2017.1 as the com-\nmercial high-level synthesis backend to synthesize the high-level\n(C/C++) based designs on the selected FPGAs.\n7.2.2 Experimental Results of Transformer and RoBERTa. We imple-\nment the compressed model to FPGA to evaluate the performance\nand energy efficiency. For different batch sizes, we obtain the paral-\nlelism per stage for the 7 stages in encoder/decoders of Transformer\nand RoBERTa based on Algorithm 1 as shown in Table 3, respec-\ntively. We report the resource utilization on FPGA including DSP,\nLUT, and FF. The latency (ms), throughput (frame/sequence per\nsecond) and power consumption (W) are also reported. Our results\nshows that there is a trade-off between latency and power con-\nsumption. For both Transformer and RoBERTa, we can achieve the\nbest trade-off (the lowest ratio of Latency/Power) when the batch\n1The accuracy drop on RoBERTa is slightly higher because its parameters are carefully\npretrained on the Giga byte dataset (160GB of text) using a masked language model [10]\nand more sensitive to compression.\nsize is 8 since the latency will be significantly increased and the\nthroughput will not be increased when we use larger batch size.\n7.2.3 Cross-platform comparison. We compare the performance\n(throughput) and energy efficiency among CPU, GPU, and FPGA\nusing same model and same benchmark (IMDB), as shown in Ta-\nble 4. We also validate our method on embedded low-power devices,\nimplement our pruned model on Jetson TX2, an embedded AI com-\nputing device. ItâĂŹs built by a 256-core NVIDIA Pascal-family\nGPU and the memory is 8 GB with 59.7 GB/s bandwidth. Our FPGA\ndesign achieves 27.07×and 81×improvement in throughput and\nenergy efficiency compared to CPU. For GPU TRX5000, the power\nconsumption is 5.01×compared to that of FPGA, and Our FPGA\ndesign achieves 8.80×improvement in energy efficiency and 1.77×\nthroughput improvement compared to GPU. For embedded GPU\nJason TX2, our FPGA design achieves 2.44×improvement in energy\nefficiency.\nTable 4: The performance and energy efficiency comparison\namong CPU, GPU, FPGA using RoBERTa\nCPU GPU FPGA Jetson TX2\ni7-8700KRTX5000VCU118Embedded GPU\nThroughput (FPS) 3.76 57.46 101.79 9.75\nPower (W) 80 126 25.13 5.86\nEnergy efficiency (FPS/W)0.05 0.46 4.05 1.66\n8 CONCLUSION\nIn this paper, we propose an energy-efficient acceleration frame-\nwork for transformer-based large scale language representations.\nOur framework includes an enhanced BCM-based method to enable\nmodel compression on large-scale language representations at the\nalgorithm level, and an acceleration design at the architecture level.\nWe propose an FPGA architecture design to support the model\ncompression technique and we develop a design automation and\noptimization technique to explore the parallelism and achieve high\nthroughput and performance. Experimental results show that our\nproposed framework significantly reduces the size of NLP models\nwith small accuracy loss on Transformer. Our FPGA-based imple-\nmentation significantly outperforms CPU and GPU in terms of\nenergy efficiency.\nREFERENCES\n[1] 2019. Supercharge Your AI and Database Applications with Xilinx’s HBM-Enabled\nUltraScale+ Devices Featuring Samsung HBM2. Xilinx white paper, WP508 (v1.1.2)\n(2019).\n[2] James Bradbury and Richard Socher. 2017. Towards Neural Machine Translation\nwith Latent Tree Attention. EMNLP 2017 (2017), 12.\n[3] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Ben-\ngio. 2014. On the properties of neural machine translation: Encoder-decoder\napproaches. arXiv preprint arXiv:1409.1259 (2014).\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[5] Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang,\nXuehai Qian, Yu Bai, Geng Yuan, et al. 2017. Circnn: accelerating and compressing\ndeep neural networks using block-circulant weight matrices. InProceedings of the\n50th Annual IEEE/ACM International Symposium on Microarchitecture . 395–408.\n[6] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory.Neural\ncomputation 9, 8 (1997), 1735–1780.\n[7] S Lennart Johnsson and Robert L Krawitz. 1992. Cooley-tukey FFT on the\nconnection machine. Parallel Comput. 18, 11 (1992), 1201–1221.\n[8] Joo-Kyung Kim and Young-Bum Kim. 2018. Supervised Domain Enablement\nAttention for Personalized Domain Classification. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing . 894–899.\nFTRANS: Energy-Efficient Acceleration of Transformers using FPGA ISLPED ’20, August 10–12, 2020, Boston, MA, USA\n[9] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. 2017. Structured\nattention networks. arXiv preprint arXiv:1702.00887 (2017).\n[10] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[11] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and\nChristopher Potts. 2011. Learning word vectors for sentiment analysis. In ACL.\nAssociation for Computational Linguistics, 142–150.\n[12] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. [n. d.].\nPointer Sentinel Mixture Models. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April 24-26, 2017 .\n[13] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,\nDavid Grangier, and Michael Auli. 2019. fairseq: A Fast, Extensible Toolkit for\nSequence Modeling. In Proceedings of NAACL-HLT 2019: Demonstrations .\n[14] Victor Pan. 2012. Structured matrices and polynomials: unified superfast algorithms .\nSpringer Science & Business Media.\n[15] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,\nZachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.\n2017. Automatic differentiation in PyTorch. (2017).\n[16] Peng Shi, Jinfeng Rao, and Jimmy Lin. 2018. Simple Attention-Based Rep-\nresentation Learning for Ranking Short Social Media Posts. arXiv preprint\narXiv:1811.01013 (2018).\n[17] Julius Orion Smith. 2007. Mathematics of the discrete Fourier transform (DFT):\nwith audio applications . Julius Smith.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998–6008.\n[19] Shuo Wang, Zhe Li, Caiwen Ding, Bo Yuan, Qinru Qiu, Yanzhi Wang, and Yun\nLiang. 2018. C-LSTM: Enabling Efficient LSTM Using Structured Compression\nTechniques on FPGAs. In FPGA’18.\n[20] Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, and Bo Yuan. 2017. Theo-\nretical Properties for Neural Networks with Weight Matrices of Low Displacement\nRank. In International Conference on Machine Learning . 4082–4090.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7892130613327026
    },
    {
      "name": "Field-programmable gate array",
      "score": 0.6765795946121216
    },
    {
      "name": "Transformer",
      "score": 0.6328112483024597
    },
    {
      "name": "Computation",
      "score": 0.571996808052063
    },
    {
      "name": "Efficient energy use",
      "score": 0.5502915978431702
    },
    {
      "name": "Parallel computing",
      "score": 0.4802951514720917
    },
    {
      "name": "Recurrent neural network",
      "score": 0.44882848858833313
    },
    {
      "name": "Hardware acceleration",
      "score": 0.4479486644268036
    },
    {
      "name": "Gate array",
      "score": 0.4254026412963867
    },
    {
      "name": "Computer engineering",
      "score": 0.422211229801178
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3549647927284241
    },
    {
      "name": "Artificial neural network",
      "score": 0.3142683804035187
    },
    {
      "name": "Algorithm",
      "score": 0.2647992968559265
    },
    {
      "name": "Computer hardware",
      "score": 0.23621001839637756
    },
    {
      "name": "Voltage",
      "score": 0.13184690475463867
    },
    {
      "name": "Engineering",
      "score": 0.10014143586158752
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}