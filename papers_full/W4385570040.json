{
    "title": "Don’t Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",
    "url": "https://openalex.org/W4385570040",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2117761656",
            "name": "Yu Gu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101262251",
            "name": "Xiang Deng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2107570501",
            "name": "Yu Su",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2016589492",
        "https://openalex.org/W2890431379",
        "https://openalex.org/W3104616515",
        "https://openalex.org/W2745934983",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W3034758614",
        "https://openalex.org/W4287659415",
        "https://openalex.org/W4285069854",
        "https://openalex.org/W4226053975",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3170656151",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2252136820",
        "https://openalex.org/W4214700710",
        "https://openalex.org/W4372270366",
        "https://openalex.org/W3156333129",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W4307011114",
        "https://openalex.org/W2593652440",
        "https://openalex.org/W4302010387",
        "https://openalex.org/W3156012351",
        "https://openalex.org/W4385573529",
        "https://openalex.org/W4385572953",
        "https://openalex.org/W4308827461",
        "https://openalex.org/W2997937415",
        "https://openalex.org/W4310998175",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W4224267275",
        "https://openalex.org/W4295683125",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W4310428050",
        "https://openalex.org/W4385574135",
        "https://openalex.org/W4304699955",
        "https://openalex.org/W2094728533",
        "https://openalex.org/W4287024925",
        "https://openalex.org/W3034835156",
        "https://openalex.org/W2251079237",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3034273250",
        "https://openalex.org/W3156366114",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2566402689",
        "https://openalex.org/W4320003957",
        "https://openalex.org/W4297161808",
        "https://openalex.org/W3214600982",
        "https://openalex.org/W3105388824",
        "https://openalex.org/W2546950329",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W4284676027",
        "https://openalex.org/W4287079536",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4288601872",
        "https://openalex.org/W3199258975",
        "https://openalex.org/W4312436794",
        "https://openalex.org/W2511149293",
        "https://openalex.org/W4309088836"
    ],
    "abstract": "A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains.Pangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4928–4949\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDon’t Generate, Discriminate:\nA Proposal for Grounding Language Models to Real-World Environments\nYu Gu\nThe Ohio State University\ngu.826@osu.edu\nXiang Deng\nThe Ohio State University\ndeng.595@osu.edu\nYu Su\nThe Ohio State University\nsu.809@osu.edu\nAbstract\nA key missing capacity of current language\nmodels (LMs) is grounding to real-world en-\nvironments. Most existing work for grounded\nlanguage understanding uses LMs to directly\ngenerate plans that can be executed in the en-\nvironment to achieve the desired effects. It\nthereby casts the burden of ensuring grammati-\ncality, faithfulness, and controllability all on the\nLMs. We propose Pangu, a generic framework\nfor grounded language understanding that capi-\ntalizes on the discriminative ability of LMs in-\nstead of their generative ability. Pangu consists\nof a symbolic agent and a neural LM working\nin a concerted fashion: The agent explores the\nenvironment to incrementally construct valid\nplans, and the LM evaluates the plausibility of\nthe candidate plans to guide the search process.\nA case study on the challenging problem of\nknowledge base question answering (KBQA),\nwhich features a massive environment, demon-\nstrates the remarkable effectiveness and flexi-\nbility of Pangu: A BERT-base LM is sufficient\nfor setting a new record on standard KBQA\ndatasets, and larger LMs further bring substan-\ntial gains. Pangu also enables, for the first\ntime, effective few-shot in-context learning for\nKBQA with large LMs such as Codex.1\n1 Introduction\nLanguage models (LMs) such as BERT (Devlin\net al., 2019), GPT-3 (Brown et al., 2020), and\nCodex (Chen et al., 2021a) have demonstrated an\nextraordinary capacity in understanding and gener-\nating both natural language (Minaee et al., 2021;\nLiang et al., 2022) and generic programs ( e.g.,\nPython) (Li et al., 2022; Jain et al., 2022; Austin\net al., 2021). The recent release of ChatGPT and\nsimilar large LMs is elevating this paradigm to\na new level. It seems to point us towards a future\nwhere natural language serves as a universal device,\npowered by LMs, for automated problem solving\nand interacting with the (computing) world.\n1The Pangu library: OSU-NLP-Group/Pangu.\nUtterance \nDatabases\n Knowledge \nBases\nTablesWeb Pages AppsPhysical \nWorld\nEnvironments \nPlans\nGrammatical \nFaithful \nSymbolic \nNeural Scores\nControllable \nFigure 1: A schematic illustration of the proposed frame-\nwork, Pangu, where a symbolic agent interacts with the\ntarget environment to propose candidate plans, and a\nneural LM evaluates the plausibility of each plan. The\nagent searches the environment to incrementally con-\nstruct the plans, and the LM guides the search process.\nHowever, a key missing piece in realizing this\nfuture is the connection between LMs and real-\nworld environments, including both digital envi-\nronments (e.g., databases, knowledge bases, Excel\nspreadsheets, software, websites, among others)\nand physical environments ( e.g., instruction fol-\nlowing robots (Shridhar et al., 2020; Ahn et al.,\n2022)). Such environments are where many real\nproblems lie. For example, a biologist may need\nto find all the species of a certain butterfly genus\nand their geographic distribution from a biology\nknowledge base, a local grocery store owner may\nwant to visualize the historical sales of different\nitem categories in Excel to decide what and how\nmuch to restock before the holiday season, and a\nphysician may need to find patients with specific\nconditions in a large database of electronic medi-\ncal records to inform the current diagnosis. How\ncan LMs enable solving all these problems, which\ninvolve seeking information or taking actions in a\nspecific environment, with natural language?\nEach environment is a unique context for in-\nterpreting natural language requests from users.\nGrounding, i.e., linking of (natural language) con-\n4928\ncepts to contexts (Chandu et al., 2021), therefore\nbecomes the fundamental problem. More precisely,\nwe need to produce a plan that can be executed in\nan environment to achieve the desired effects of\nthe corresponding language request. When a plan\nis described in a formal language ( e.g., SQL for\nrelational databases (Yu et al., 2018) or APIs for\nweb services (Su et al., 2017; Andreas et al., 2020)),\nit is also called a program. The unique challenge\nof such grounded language understanding prob-\nlems stems from 1) the vast heterogeneity of envi-\nronments and their planning languages (e.g., SQL,\nGraphQL/REST APIs, λ-calculus, and robot plan-\nning languages), and 2) the vast, oftentimes infinite,\nnumber of possible instantiations (or states) of each\nenvironment. Some environments can also be dy-\nnamic, e.g., a database that is constantly updated\nor a physical environment with moving objects.\nMost existing methods for grounded language\nunderstanding follow the popular sequence-to-\nsequence framework (Sutskever et al., 2014; Cho\net al., 2014) and generate the plans/programs in an\nautoregressive fashion (Xie et al., 2022; Ye et al.,\n2022; Wang et al., 2021; Song et al., 2022a). A\ncore thesis of this paper is that directly generating\nplans may not be the optimal way of using LMs\nfor grounded language understanding. It requires\nLMs to have intimate knowledge about each spe-\ncific planning language and environment, neither\nof which may be part of an LM’s pre-training, to\nensure the grammaticality and faithfulness of the\ngenerated plans.2 The infinite and dynamic environ-\nment states also reduce the potential effectiveness\nof pre-training for improving faithfulness, even if\none manages to do so. Furthermore, autoregressive\ngeneration with a neural LM lacks fine-grainedcon-\ntrol over planning; it is cumbersome, though not\nimpossible, to factor preferences, business logic,\nand other values and constraints into the plan gener-\nation process. A focus of recent work is to alleviate\n(some of) these limitations by augmenting autore-\ngressive generation with environment-specific pre-\ntraining (Yu et al., 2021; Deng et al., 2021) or con-\nstrained decoding (Scholak et al., 2021; Shin et al.,\n2021; Gu and Su, 2022). However, the fundamental\nchallenges still largely remain.\nMathematically, an LM is simply a joint distribu-\n2We generalize the definition of faithfulness to mean plans\nthat conform to the specifics of an environment such that it\ncan be successfully executed and achieve non-trivial results,\ne.g., a SQL query that is executable in a specific database and\nyields a non-empty result set.\ntion p(x1,x2,...,x n) that factors as a product of\nconditional distributions ∏n\ni=1 p(xi|x1,...,x i−1).\nExisting work leverages the conditional distribu-\ntion formulation to generate the plan. It thereby\ncasts the burden of ensuring grammaticality, faith-\nfulness, and controllability all on the LM. The main\nproposal of this paper is to disentangle LMs from\nthese responsibilities and let LMs be what they orig-\ninally are—a model that assigns a probability to a\nsequence of tokens. In other words, we advocate\nfor using the joint distribution formulation of LMs\nto evaluate the plausibility of (utterance, candidate\nplan) pairs instead of directly generating the plan.\nTo this end, we propose Pangu, a generic frame-\nwork for grounded language understanding that\ncapitalizes on the discriminative ability of LMs in-\nstead of their generative ability (Figure 1).3 Pangu\nconsists of a symbolic agent and a neural LM work-\ning in a concerted fashion. The symbolic agent ex-\nplores the environment to propose candidate plans,\nwhich are guaranteed by design to be both gram-\nmatical and faithful. For most real-world environ-\nments, due to the size of the search space or par-\ntial observability, it is necessary for the agent to\nsearch in the environment and incrementally ex-\ntend or refine the plans. The LM plays a key role\nin this search process—it evaluates the candidate\n(partial) plans at each search step and guides the\nagent towards promising search directions; it also\ndetermines when the search ends. Finally, it is also\neasier to control the search process of a symbolic\nagent than the generation process of a neural LM.\nAs a case study, we instantiate the proposed\nframework for complex question answering over\nknowledge bases (KBQA). KBQA provides an\nideal testbed for grounded language understanding\nbecause of its massive environment—direct gener-\nation with LMs often fails dramatically (Gu et al.,\n2021). We show that simply using BERT-base\nwith Pangu is sufficient for setting a new record\non standard KBQA datasets, and larger LMs fur-\nther bring substantial gains. Pangu also enables,\nfor the first time, few-shot KBQA by prompting\nlarge language models (e.g., Codex): Using only 10\nlabeled examples, it outperforms all prior methods\non GRAPH Q (Su et al., 2016). It provides unprece-\ndented uniformity for using LMs—one can easily\nplug encoder-only LMs, encoder-decoder LMs, or\ndecoder-only LMs into Pangu, through either fine-\n3Pangu is a primordial being in Chinese mythology who\nseparated heaven and earth. We name our framework after that\nfor its separating the realm of the neural and the symbolic.\n4929\ntuning or in-context learning. These results high-\nlight the remarkable effectiveness and flexibility of\nPangu and validate the proposal of using LMs for\ndiscrimination instead of generation.\n2 Related Work\n2.1 Generation for Grounded Language\nUnderstanding\nThe Seq2Seq framework (Sutskever et al., 2014;\nBahdanau et al., 2015) has been the de facto choice\nfor grounded language understanding, where the\nLM directly generates a plan given an input utter-\nance. However, the lack of grounding during pre-\ntraining makes generating valid plans from LMs\nchallenging. Recent studies endeavor to alleviate\nthis issue via input augmentation or constrained de-\ncoding. For input augmentation, the environment\n(or some relevant portion of it) is fed to the LM’s\nencoder together with the utterance (Hwang et al.,\n2019; Wang et al., 2020; Xie et al., 2022). Such\nmethods rely on the LM to understand the interplay\nbetween the language requests and the environment\nand correctly factor that into plan generation. They\ntherefore require substantial training data to learn\nand also provide no guarantee for grammaticality\nor faithfulness. In contrast, constrained decoding\nmethods regulate the decoder’s behavior to guar-\nantee grammaticality (Scholak et al., 2021; Shu\net al., 2022) or even faithfulness (Liang et al., 2017;\nGu and Su, 2022). However, such uses still cast\nthe burden of generating valid plans on the LM\nitself; controlling the generation process of an LM\ncan be difficult and specific to each planning lan-\nguage and/or environment. In our proposal, the LM\nis only used to discriminate valid plans proposed\nby an agent through a controllable search process.\nMore detailed comparison is presented in §5.3.\n2.2 Few-Shot Grounded Language\nUnderstanding with LLMs\nLarge language models (LLMs) (Brown et al.,\n2020; Chen et al., 2021a) have demonstrated strong\nfew-shot learning capabilities in various tasks, from\nwriting programs to query structured and unstruc-\ntured data (Austin et al., 2021; Rajkumar et al.,\n2022; Cheng et al., 2022), interacting with online\nwebsites (Gur et al., 2022; Nakano et al., 2021), to\ngenerating procedural plans and guiding embodied\nagents in virtual environments (Singh et al., 2022;\nAhn et al., 2022; Shah et al., 2022; Song et al.,\n2022b). Most existing work still capitalizes on the\ngenerative ability of LLMs. A common strategy\nto encourage an LLM to produce valid plans is to\ndirectly describe the environment in the LLM’s\ncontext (i.e., input augmentation), which is difficult\nfor complex environments like KBs. A concurrent\nwork of ours (Li et al., 2023b) asks the LLM to\ndirectly generate a proxy plan from the input ques-\ntion without the environment description, which\nis then used to retrieve a valid plan from a set of\ncandidate plans. However, this design is tailored\nspecifically to the KB query language and is lim-\nited to generating plans with at most two hops due\nto the combinatorial explosion in their candidate\nenumeration. In contrast, Pangu shields the LLM\nfrom the complexity of the environment and lets\nthe LLM focus on evaluating the plausibility of\ncandidate plans proposed by an agent. One inter-\nesting related work is Ahn et al. (2022), where an\nLLM is used to score atomic action (skill) propos-\nals, which are guaranteed to conform to affordance\nconstraints, from an embodied agent. Pangu shares\na similar spirit of using LMs for discrimination, but\nwe support more complex plans through a search\nprocess in the environment guided by an LM.\n2.3 Bottom-Up Semantic Parsing\nOur instantiation of Pangu on KBQA is closely con-\nnected to bottom-up semantic parsing, particularly\nSmBoP (Rubin and Berant, 2021), a text-to-SQL\nmodel that iteratively constructs a complex plan\nfrom a set of subplans. Pangu similarly constructs\na complex plan incrementally from smaller sub-\nplans, but it makes the following main departures.\nFirst, SmBoP requires all ingredients (i.e., column\nheaders, table names, and DB values) at the begin-\nning of parsing. This assumption does not generally\nhold for more complex or partially observable envi-\nronments, where ingredients need to be discovered\nthrough search. In our method, only topic entities\nare needed as the initial plan, which can be read-\nily obtained using an entity linker (Li et al., 2020).\nSecond, our scoring function is based on a straight-\nforward application of LMs, while SmBoP uses a\nmore intricate architecture with extra parameters.\nAlso related is an array of earlier KBQA methods\nthat adopt an enumerate-and-rank approach (Yih\net al., 2015; Gu et al., 2021; Ye et al., 2022). Be-\ncause they try to enumerate all candidate plans up\nfront, the maximum plan complexity is bound to\nbe small. Our adaptive search process allows for\nflexible construction of more complex plans.\n4930\nKnowledge Base\nWhat is the latest released computer emulator\ndeveloped in Java?\nt = 1 t = 2\nt = 3 t = 4\nInput utterance:\nEnvironment:\nTarget plan:\n(a)\n(b)\n Utterance: What is the latest released computer emulator developed in java?Candidate Plan: (AND ComputerEmulator (JOIN LanguagesUsed Java))\n[CLS]  Utterance [SEP] Candidate Plan [SEP] UtteranceCandidate PlanScore:\nScore:\n1a.  (JOIN ParentLanguage Java)\n1b.  (JOIN Influenced Java)\n1c.  (JOIN LanguagesUsed Java)\n        : {Java}\n3a.  (COUNT 2a)\n3b.  (ARGMIN 2a LatestReleaseDate)\n3c.  (ARGMAX 2a LatestReleaseDate)\n2a.  (AND ComputerEmulator 1c)\n2b.  (AND ComputerSoftware 1c)\n2c.  (JOIN ReadBy 1c)\n4a.  (JOIN UsesSoftware 3c)\n4b.  (JOIN WrittenBy 3c)\n4c.  (COUNT 3c)\nBeam Size = 1\n(ARGMAX (AND ComputerEmulator \n             (JOIN LanguagesUsed Java)) \n        LatestReleaseDate)\n      : {1c}\n      : {3c}      : {2c}\nFigure 2: (a) An illustration of how an agent collaborates with an LM to incrementally produce a complex target\nplan over a KB using beam search (beam size = 1in this example). At each step, the agent extends the current\nplans based on the environment to produce new candidate plans. An LM then scores the candidate plans and returns\nthe top-ranked ones. The search process terminates when there is no candidate plan that scores higher than the\ncurrent best plan (e.g., 4a-care all worse than 3c). (b) Using different LMs (left: BERT, right: Codex) to evaluate\nthe plausibility of plan 2a. It resembles using LMs for semantic matching between the utterance and the plan.\n3 Approach\nAn overview of the Pangu framework is presented\nin Algorithm 1. An overarching assumption of\nPangu is that a complex plan can be incrementally\nconstructed by an agent through its exploration in\nan environment. Such an agent can be a robot doing\nhousehold tasks in a physical environment (Shrid-\nhar et al., 2020), or a virtual agent that orchestrates\nAPI calls of different web services (Andreas et al.,\n2020) or traverses a database/KB (Yu et al., 2018;\nGu et al., 2022). Starting from a set of initial plans\nP0 (may be empty), at each step, the agent inter-\nacts with the environment Eto extend the current\nplans into a new set of candidate plans (line 4).\nThe candidate plans are guaranteed to be valid (i.e.,\nboth grammatical and faithful). An LM then scores\nthe candidate plans, and the top K(the beam size)\nplans are retained for further exploration in the next\nstep (line 5). The same procedure loops until a ter-\nmination check is passed (line 6); the best plan is\nthen returned.\nPangu mainly shines in that a symbolic agent\nexplores the environment to propose valid plans\nand shields the LM from having to handle the large\nsearch space for valid plan generation. Instead,\nthe LM only focuses on evaluating the plausibility\nof the proposed plans. An LM can be easily fine-\ntuned to excel at this assignment, or, in the case of\nLLMs such as Codex, they come with such ability\nout of the box, which enables few-shot in-context\nAlgorithm 1: PANGU\n1 Input: utterance q, initial plans P0, environment E\n2 t ←1;\n3 while True do\n// Agent proposes plans\n4 Ct ←Candidate-Plans(Pt−1, E)\n// LM scores and prunes plans\n5 Pt ←Top-K(q, Ct)\n6 if Check-Termination() = True then\n7 return top-scored plan\n8 t ←t + 1\nlearning. Pangu is a generic framework and can\npotentially accommodate many grounded language\nunderstanding tasks by instantiating the various\nfunctions in Algorithm 1 accordingly. Next, we\ndiscuss our instantiation on KBQA. More discus-\nsion on Pangu’s applicability to other tasks, with\npreliminary results, can be found in Appendix A.\n3.1 KBQA: Preliminaries\nWithout loss of generality, we use KBs as our\ntarget environment and the KBQA task as a con-\ncrete example for ease of discussion. It is an ideal\ntestbed because of the massive environment pro-\nvided by modern KBs (e.g., FREEBASE (Bollacker\net al., 2008) contains 45 million entities and 3 bil-\nlion facts for over 100 domains), which makes\ngrounding particularly challenging. Given a KB\nK⊂E×R× (E∪L∪C ), where Cis a set of\nclasses, Ea set of entities, La set of literals and\n4931\nRa set of binary relations, the task of KBQA is to\nfind a set of answer entities to an input utterance in\nthe KB. KBQA is typically modeled as semantic\nparsing (Gu et al., 2022), where the utterance is\nmapped to an executable program/plan in a certain\nformal language (e.g., SPARQL, λ-calculus, or S-\nexpression) whose denotation is the answer. We\nuse S-expressions (Gu et al., 2021) for its compact-\nness. An example is shown in Figure 2.\n3.2 Candidate Plan Enumeration\nTo handle the large search space, the agent casts\nthe task as a step-wise decision-making problem.\nA plan for KBQA can be decomposed into a nested\nsequence of subplans (Gu and Su, 2022) (Figure 2).\nThe length of a plan is defined as the number of\natomic subplans it contains.\nFor KBQA, P0 can be a set of entity proposals\n(e.g., { Java}) obtained using off-the-shelf entity\nlinkers (Li et al., 2020). At step t, the agent con-\nsiders Pt−1, the length t−1 plans, and decides\nhow to further extend them into Ct, the valid plans\nof length t, based on the environment. This often\ninvolves executing the current plans in the environ-\nment. Consider the example in Figure 2 at t= 1,\nthe agent finds all the relations connected to Java\nand enumerates all the length-1 valid plans. The\nLM scores the candidate plans and prunes all but\nthe top-ranked plan because beam size is 1. At\nt = 2, the agent executes plan 1cto get its deno-\ntation (i.e., a set of entities) in the KB, based on\nwhich the agent further discovers the relations and\nclasses (e.g., ComputerEmulator, ComputerSoftware,\nand ReadBy) connected to those entities to form\nvalid length-2 plans. All the plans produced in this\nprocess are guaranteed to be valid. See Appendix B\nfor a more detailed discussion of this process.\n3.3 LM-Based Scoring\nAfter the agent enumerates a set of candidate plans,\nan LM assists with its decision making by evalu-\nating the plausibility of each candidate plan. The\ninterface for evaluating a plan using LMs resem-\nbles using LMs for semantic matching: Given a\npair of (u: utterance, c∈Ct : candidate plan), an\nLM acts as a scoring function: s(u,c) →R, which\nindicates to what extent the candidate plan matches\nthe intent of the utterance. The plausibility of a\ncandidate oftentimes can be indicated by simple\nlinguistic cues, e.g., ComputerEmulator in 2amight\nbe a strong indicator (Figure 2(a)).\nWe follow the common practice of using LMs\nfor semantic matching. For encoder-only LMs like\nBERT, we directly get a score from the representa-\ntion of the [CLS] token (Figure 2(b)). For encoder-\ndecoder LMs like T5 (Raffel et al., 2020), we fol-\nlow Zhuang et al. (2022) to feed both the utterance\nand the candidate plan to the encoder and let the\ndecoder decode only for one step. The decoding\nprobability over an token that is unused during pre-\ntraining is then repurposed as a proxy for match-\ning score.4 For decoder-only LMs like Codex, we\nmodel the score as the probability of generating\nthe candidate plan conditioned on the utterance,\ni.e., P(c|u). Intuitively, a good scoring function s\nshould respect the following partial order:\ns(u,c1) >s(u,c2), ∀c1 ∈Gt and ∀c2 ∈Gt−1,\ns(u,c1) >s(u,c2), ∀c1 ∈Gt and ∀c2 ∈Ct\\Gt,\ns(u,c′) >s(u,ci), ∀ci ̸= c′\nwhere Gt is the set of gold (sub-)plans at step t\n(i.e., length-tsubplans of the target plan), Ct\\Gt is\nthe set of length-tcandidate plans except the gold\n(sub-)plans, and c′is the target plan.\nIn other words, a gold subplan should be scored\nhigher than (1) any negative (i.e., not gold) plans at\nthe same step (e.g., 2ashould be scored higher than\n2c), because they contain information irrelevant to\nu, and (2) any gold sub-plans of length < t(e.g.,\n2ashould be scored higher than 1c) because they\nare less complete. In addition, c′should be scored\nhigher than any other plan.\n3.4 Termination Check\nAssuming the LM can assign reasonable scores to\ncandidate plans following the above partial order,\nwe can naturally define the condition for termina-\ntion in Algorithm 1: It terminates if the highest\nscore of candidate plans at step tis lower than the\nhighest score of candidate plans at stept−1, which,\nideally, should indicate no reachable candidate plan\nof length ≥tis better than the plans at step t−1,\nand thus the search process terminates.\n3.5 Learning\nWe discuss the learning procedure for both fine-\ntuning LMs ( e.g., BERT and T5) and in-context\nlearning with LLMs ( e.g., Codex). For both set-\ntings, we use pairs of utterances and gold plans for\nsupervision.\n4We use <extra_id_23> as the proxy token for T5.\n4932\nFine-tuning. Given a gold plan of length T, we\nfirst derive its gold sub-plansGt of each step t≤T\n(e.g., 1cfor step 1 and 2afor step 2 in Figure 2).\nFine-tuning proceeds with beam search similar to\nthe test-time behavior, but with bottom-up teacher\nforcing (Williams and Zipser, 1989; Rubin and\nBerant, 2021), i.e., the gold plans of the current\nstep should always be inserted into the beam. At\neach step of beam search, we get the probability of\neach candidate plan c∈Ct with softmax over the\nscores: p(c) =softmax{s(u,c)}c∈Ct∪Gt−1 . Gt−1\nis also included here to encourage LMs to explicitly\nlearn the partial order by minimizing the loss:\n−1\nZ\nT+1∑\nt=1\n∑\nc∈Ct\nˆp(c)log p(c)\nwhere Zis the total number of summed items, and\nˆp(c) equals 1 if c∈Gt and 0 elsewise. Note that,\nfor the T + 1step, we let GT+1 = GT . This\nadditional step aims to enforce the third condition\nin the partial order. Our objective is essentially\na listwise learning-to-rank objective based on the\ncross entropy (Cao et al., 2007).\nIn-Context Learning. We directly use pairs of ut-\nterances and gold plans as in-context demonstra-\ntions to the LLM, with a simple task instruction in\nthe prompt: “Please translate the following ques-\ntions to Lisp-like programs. ”The LLM is therefore\nexpected to capture the desired partial order by\nobserving the in-context examples. For concrete\nexamples of prompts, please refer to Appendix F.\nWhen scoring using LLMs, we normalize the like-\nlihood w.r.t. the number of tokens in the plan to\nhandle plans of varying lengths.\n4 Experimental Setup\n4.1 Datasets\nWe experiment with three KBQA datasets of differ-\nent scale and nature (statistics in Table C.3).\nGRAIL QA (Gu et al., 2021) is a large-scale dataset\nthat evaluates three levels of generalization, namely,\ni.i.d., compositional (novel compositions of seen\nconstructs), and zero-shot (totally novel domains).\nIt also features diverse questions of different com-\nplexity (e.g., programs may involve up to 4 rela-\ntions) and aggregation functions ( e.g., compara-\ntives, superlatives, and counting).\nGRAPH Q (Su et al., 2016) is a moderate-scale\ndataset. Due to the small size of its training set\nand the non-i.i.d. setting, GRAPH Q is particularly\nchallenging. In our experiments, we use the pro-\ncessed version by Gu and Su (2022), which maps\nthe original dataset from FREEBASE 2013-07 to\nFREEBASE 2015-08-09.\nWEBQSP (Yih et al., 2016) is a moderate-scale\ndataset with questions from Google query logs. It\nmainly tests i.i.d. generalization on simple ques-\ntions. It is a clean subset of WEBQ (Berant et al.,\n2013) with program annotations.\nThe gold programs for all three datasets are pro-\nvided in S-expressions (Gu and Su, 2022), which\ncan be determinstically converted into SPARQL\nqueries to get final execution results.\n4.2 Baselines\nWe mainly compare Pangu with state-of-the-\nart baselines that use LMs as a generative\nmodel, including ArcaneQA (Gu and Su, 2022),\nTIARA (Shu et al., 2022), DecAF (Yu et al., 2022),\nand RnG-KBQA (Ye et al., 2022). Constrained\ndecoding (i.e., ArcaneQA and TIARA) and input\naugmentation (i.e., TIARA, DecAF) are used to en-\nhance plan generation. Also, the last three models\nuse a combination of LMs for multiple purposes\n(i.e., retrieval/ranking/decoding). In addition, we\nalso compare with UnifiedSKG (Xie et al., 2022).\nUnifiedSKG assumes a set of schema items are\nprovided as input, where the gold schema items\nare always included and the number of negative\nschema items is restricted to 20 for GRAIL QA. It\nis thus a less fair comparison for other methods,\nbut we include it anyway because it is a represen-\ntative way of autoregressive plan generation using\nan LLM. Compared with the baselines, Pangu re-\nquires no extra parameter, no modification to the\nLM, and no need to combine multiple LMs. Pangu\nprovides unprecedented uniformity of using LMs\nof different nature. More details on baselines can\nbe found in Appendix C.2.\n4.3 Implementation Details\nFor the fine-tuning experiments, we experiment\nwith BERT-base, T5-base, T5-large, and T5-3B,\nand use the full training set of each dataset for fine-\ntuning. For the in-context learning experiments,\nwe experiment with Codex.5 We randomly sample\n10/100/1,000 training examples from each dataset\nand use that as the pool for dynamic retrieval. Dur-\ning inference, for each test example, we retrieve\n5We opt for Codex because it is free, but small-scale exper-\niments also show competitive performance from ChatGPT.\n4933\nOverall I.I.D. Compositional Zero-shot Dev Overall\nModel EM F1 EM F1 EM F1 EM F1 EM F1\nQGG (Lan and Jiang, 2020) − 36.7 − 40.5 − 33.0 − 36.6 − −\nBERT+Ranking (Gu et al., 2021) 50.6 58.0 59.9 67.0 45.5 53.9 48.6 55.7 − −\nReTraCk (Chen et al., 2021b) 58.1 65.3 84.4 87.5 61.5 70.9 44.6 52.5 − −\nRnG-KBQA (Ye et al., 2022) 68.8 74.4 86.2 89.0 63.8 71.2 63.0 69.2 71.4 76.8\nArcaneQA (Gu and Su, 2022) 63.8 73.7 85.6 88.9 65.8 75.3 52.9 66.0 69.5 76.9\nUni-Parser (Liu et al., 2022) 69.5 74.6 85.5 88.5 65.1 71.1 64.0 69.8 70.8 76.5\nTIARA (Shu et al., 2022) 73.0 78.5 87.8 90.6 69.2 76.5 68.0 73.9 75.3 81.9\nDecAF (Yu et al., 2022) 68.4 78.7 84.8 89.9 73.4 81.8 58.6 72.3 − 81.4\nUnifiedSKG w/ T5-3B (Xie et al., 2022)− − − − − − − − 70.1∗ −\nPangu (this work)\nw/ BERT-base 73.7 79.9 82.6 87.1 74.9 81.2 69.1 76.1 75.0 82.1\nw/ T5-base 73.6 79.9 84.7 88.8 73.1 80.1 68.6 75.8 76.0 82.8\nw/ T5-large 74.8 81.4 82.5 87.3 75.2 82.2 71.0 78.4 75.8 83.3\nw/ T5-3B 75.4 81.7 84.4 88.8 74.6 81.5 71.6 78.5 75.8 83.4\nw/ Codex (10-shot) 48.9 56.3 51.8 58.1 43.3 51.2 50.1 57.8 − −\nw/ Codex (100-shot) 53.3 62.7 54.7 62.9 54.5 63.7 52.3 62.2 − −\nw/ Codex (1000-shot) 56.4 65.0 67.5 73.7 58.2 64.9 50.7 61.1 − −\n(a) GRAIL QA\nModel F1\nUDEPLAMBDA (Reddy et al., 2017) 17.7♯\nPARA4QA (Dong et al., 2017) 20.4 ♯\nSPARQA (Sun et al., 2020) 21.5 ♯\nBERT+Ranking (Gu et al., 2021) 27.0\nArcaneQA (Gu and Su, 2022) 34.3\nPangu (this work)\nw/ BERT-base 52.0\nw/ T5-base 53.3\nw/ T5-large 55.6\nw/ T5-3B 62.2\nw/ Codex (10-shot) 42.8\nw/ Codex (100-shot) 43.3\nw/ Codex (1000-shot) 44.3\n(b) GRAPH Q\nModel F1\nQGG (Lan and Jiang, 2020) 74.0\nReTraCk (Chen et al., 2021b) 71.0\nCBR (Das et al., 2021) 72.8\nProgram Transfer (Cao et al., 2022) 76.5∗\nRnG-KBQA (Ye et al., 2022) 75.6\nArcaneQA (Gu and Su, 2022) 75.6\nUni-Parser (Liu et al., 2022) 75.8\nTIARA (Shu et al., 2022) 76.7\nDecAF (Yu et al., 2022) 78.8\nPangu (this work)\nw/ BERT-base 77.9\nw/ T5-base 77.3\nw/ T5-large 78.9\nw/ T5-3B 79.6\nw/ Codex (10-shot) 45.9\nw/ Codex (100-shot) 54.5\nw/ Codex (1000-shot) 68.3\n(c) WEBQSP\nTable 1: Overall results. Pangu achieves a new state of the art on all three datasets and shows great flexibility\nin accommodating LMs of different nature. Also, for the first time, Pangu enables effective few-shot in-context\nlearning for KBQA with Codex. ∗using oracle entity linking. ♯ results on the original GRAPH Q 2013-07, otherwise\nit uses the version from Gu and Su (2022), which is a slightly smaller subset. All baselines after 2020 are trained\nusing gold programs in S-expressions.\n10 in-context examples from the pool using BM25-\nbased utterance similarity. We use entity linking re-\nsults from off-the-shelf entity linkers. More details\non implementations can be found in Appendix C.3.\n5 Results\n5.1 Main Results\nFine-tuning results. The main results are shown\nin Table 1. Using a BERT-base LM, Pangu al-\nready achieves a new state of the art on GRAIL QA\nand GRAPH Q, and only trails behind DecAF on\nWEBQSP , which uses a 3B-parameter LM. On\nGRAPH Q, Pangu with BERT-base dramatically\nimproves the state-of-the-art F1 from 31.8% to\n48.2%. These are strong evidence for Pangu be-\ning a better protocol for using LMs for grounded\nlanguage understanding. Pangu’s strong generaliz-\nability with limited training data is also confirmed\nby its performance on the zero-shot generaliza-\ntion of GRAIL QA. Our method also shows an un-\nprecedented uniformity in accommodating differ-\nent LMs (encoder-only, encoder-decoder, decoder-\nonly, through both fine-tuning and in-context learn-\ning) and a reliable return from model size—using\nincreasingly larger LMs yields monotonically im-\nproved results across the board, with T5-3B set-\nting the new state of the art on all datasets. One\ninteresting observation is that Pangu slightly under-\nperforms on the i.i.d. subset of GRAIL QA. It turns\nout that, because the discriminative task is much\n4934\nQuestion I “neil leslie diamond composed what tv song?\"\nPangu (AND tv.tv_song (JOIN music.composition.composer m.015_30))(/enc-33)\nArcaneQA (AND music.recording (JOINmusic.recording.song (JOIN music.composition.composer m.015_30)))(/enc-37)\nArcaneQA△ (JOIN music.composition.composer m.015_30) (JOINmusic.recording.song #0) (AND music.recording\n#1)\nQuestion II “which software falls into both continuous integration and build automation genres?\"\nPangu (AND computer.software (AND (JOIN computer.software.software_genre m.05vvqy) (JOIN com-\nputer.software.software_genre m.0h2vrf)))(/enc-33)\nArcaneQA (ANDcomputer.software (JOIN computer.software.software_genre m.05vvqy))(/enc-37)\nArcaneQA△ (JOIN computer.software.software_genre m.05vvqy) (ANDcomputer.software #0)\nTable 2: Two representative examples that Pangu succeeds while ArcaneQA fails, both w/ BERT-base.△denotes\nthe original order of the decoder’s output. The first incorrect token predicted by ArcaneQA is marked in red.\neasier for LMs to learn than the generative task,\nPangu converges very fast (at most two epochs)\nand gets fewer training steps for overfitting the i.i.d.\nsetting, in exchange for better non-i.i.d. general-\nization. The strong performance on WEBQSP , an\ni.i.d. dataset, further supports this observation, be-\ncause now Pangu can more sufficiently fit the i.i.d.\ntraining data.\nIn-context learning results. For the first time, we\nshow the feasibility of effective few-shot KBQA\nwith LLMs. On GRAIL QA, Pangu with Codex\nachieves an overall F1 of 56.3% with only 10 train-\ning examples. Though there is still a gap to the\nfull-data fine-tuning results, it is still impressive,\nespecially considering the massive meaning space\nof the KB. On GRAPH Q, Pangu with Codex even\noutperforms ArcaneQA using 10 training exam-\nples. This further confirms that Pangu is particu-\nlarly strong in generalizing to new environments\nwith limited training data. On WEBQSP , Pangu\ntrails behind fine-tuning methods when only us-\ning 10 training examples; however, increasing the\nsize of the pool for retrieval can significantly boost\nthe performance, which is expected given WE-\nBQSP ’s i.i.d. nature. While for non-i.i.d. datasets\nlike GRAIL QA and GRAPH Q, the gain from more\ntraining examples is marginal.\nFine-grained performance decomposition by\nquestion complexity can be found in Appendix D,\nwhich show that Pangu works well across questions\nof different complexity.\n5.2 Sample Efficiency Analysis\nIntuitively, by using LMs for discrimination instead\nof generation, the task becomes easier for LMs and\nthus improves their sample efficiency. Our sam-\nple efficiency experiments in Figure 3 confirm this\nhypothesis. We downsample GRAIL QA’s training\ndata and randomly sample 1, 10, 100, and 1,000\n1-shot 10-shot 100-shot 1000-shot Full\n0\n15\n30\n45\n60\n75\n90\nArcaneQA (BERT-base)\nPangu (BERT-base)\nUnifiedSKG (T5-base)\nPangu (T5-base)\nUnifiedSKG (Codex)\nPangu (Codex)\nFigure 3: Sample efficiency results. We conduct three\nruns with different training examples and show the mean\nEM; shaded areas denote max/min.\ntraining examples and report the results on 500\nrandom dev examples. We compare Pangu with Ar-\ncaneQA and UnifiedSKG using the same LMs. We\nuse oracle entity linking to have a more direct com-\nparison with UnifiedSKG (though UnifiedSKG still\nhas an unfair advantage as previously mentioned).\nIn addition, we also include Pangu with Codex and\nuse the downsampled training set as the pool for\nretrieval. First, we observe that, when both using\nT5-base, UnifiedSKG significantly underperforms\nPangu. The main reason is that most predicted\nplans by UnifiedSKG are invalid in the low-data\nregime. ArcaneQA uses constrained decoding to\nalleviate this issue, but still consistently underper-\nforms Pangu when both using BERT-base. For\nin-context learning using Codex, Pangu achieves\nan EM of over 50% with only one training instance.\nIt consistently outperforms all fine-tuning models\nunder low-data settings (i.e., less than 1,000 train-\ning examples). Compared with UnifiedSKG, Pangu\nshows both stronger performance and better robust-\nness against different training data selections.\n5.3 Pangu vs. Constrained Decoding\nTo better understand Pangu’s advantage over\ngeneration-based methods, we compare Pangu with\n4935\n0.0 0.5 1.0\nDensity\nPangu (BERT-base)Pangu (BERT-base)\n0.0 0.5 1.0\nArcaneQAArcaneQA\nProbability\nseen unseen\nFigure 4: Distribution of the probabilities assigned to\npredicted programs that are seen and unseen during\ntraining. We use kernal density smoothing for better\nvisualization, so the x-axis goes over 1.0.\nArcaneQA. ArcaneQA is the only open-source\nbaseline that uses constrained decoding to enforce\nthe validity of predicted plans. There are two main\nreasons for Pangu’s superiority. First, though con-\nstrained decoding can also help ensure plan va-\nlidity, the autoregressive decoder operates with\ntoken-level local normalization and thus lacks a\nglobal view. As a result, local failures may break\nits predictions. For example, a wrong local predic-\ntion (e.g., function name) by ArcaneQA leads to\ncatastrophic errors (Table 2). By evaluating candi-\ndate plans instead of candidate tokens, Pangu has\na more global view and is less likely to make such\nlocal errors. Second, Pangu is less susceptible to\noverfitting and thus achieves better performance in\nnon-i.i.d. settings. Pangu does not learn to generate\na plan; instead, it learns to evaluate the plausibility\nof utterance-plan pairs. Such knowledge is more\ntransferable. An interesting observation is shown\nin Figure 4, where Pangu’s output probability dis-\ntributions are consistent across programs seen and\nunseen in training. For ArcaneQA, however, there\nis a drastic shift from seen to unseen. This is also\nconsistent with prior findings that autoregressive\nmodels tend to overfit seen structures during train-\ning by Bogin et al. (2022). It makes non-i.i.d. gen-\neralization more difficult.\nWe also conduct an error analysis in Appendix E,\nwhich sheds some light on future improvements.\n6 Conclusions\nIn this paper, we proposed to capitalize on the dis-\ncriminative ability of language models (LMs) in-\nstead of their generative ability for grounded lan-\nguage understanding. Building on this proposal,\nwe proposed a generic framework, Pangu, which\nconsists of a symbolic agent and a neural LM work-\ning in a concerted fashion and creates a better sep-\naration between the realm of the neural and the\nsymbolic. This work opens the door for developing\nversatile and sample-efficient grounded language\nunderstanding systems that fully capitalize on the\nlanguage understanding ability of LMs while avoid-\ning their limitations. It also sheds light on develop-\ning better neuro-symbolic systems in general.\nLimitations\nDespite the strong performance of Pangu, we iden-\ntify several limitations that call for further improve-\nment. The first major limitation lies in efficiency.\nBecause Pangu requires an LM to iteratively score\ncandidate plans, it is resource-consuming in terms\nof both time and computing. Compared with Ar-\ncaneQA, which efficiently handles complex ques-\ntions in KBQA, Pangu is about twice as slow for\nboth training and inference and consumes about\ntwice as much GPU memory when using the same\nLM. Concretely, to predict a plan of L tokens,\ngeneration-based methods involve using an LM\nto do L forward passes. For Pangu, the number\nof forward passes is proportional to the number of\ncandidate plans, which can range widely. In the fu-\nture, algorithms with complexity better than O(N),\nN being the number of candidate plans, are desired\nto find the top-Kcandidates. That being said, we\nwould like to note that both ArcaneQA and Pangu\nare more efficient than most existing methods due\nto their efficient dynamic search design. For ex-\nample, Pangu is 8 times faster than RnG-KBQA,\naccording to the numbers reported in Gu and Su\n(2022). Nonetheless, we list efficiency as a limi-\ntation because there is clear potential for further\nimprovement.\nSecond, though Pangu has shown some promis-\ning results with Codex, the true potential of en-\nabling few-shot grounded language understanding\nwith Pangu has yet to be fully realized. We only\nexperiment with a straightforward scoring function\nand have not experimented with different prompt\ndesigns systematically. In the future, we plan to\ntry different prompt designs, retrievers, and scor-\ning functions, including using latest techniques like\nchain-of-thought prompting (Wei et al., 2022).\nThird, though orthogonal to the general frame-\nwork of our proposal, in our current instantiation,\nwe assume gold plans for training. However, gold\nplans can be expensive to collect for some envi-\nronments. Exploring fine-tuning LMs with weak\nsupervision can be an interesting direction. In ad-\ndition to proposing candidate plans to the LM, the\n4936\nagent may also respond to the LM with rewards\nbased on its decisions (Liang et al., 2017).\nFinally, one important merit of Pangu, control-\nlability, is under-explored in this paper, because it\nis not very necessary for KBQA. While for tasks\nlike text-to-SQL parsing, controllability could be a\nhighly desirable property. Intruders may manipu-\nlate text-to-SQL models to launch database attacks\nvia SQL injection (Peng et al., 2022). With Pangu,\nwe can easily get rid of malicious SQL operations\nin candidate enumeration. However, for generation-\nbased methods, such controls are hard to achieve\nduring generation because the decoding process\ncan be shortsighted—it is difficult to tell whether\nthe current predicted token would lead to a mali-\ncious operation several steps later. We leave explo-\nration on Pangu’s controllability to future work.\nEthics Statement\nLLMs are no longer just a laboratory curiosity;\nthey are being used in real-world systems to in-\nteract with real-world environments (both digital\nand physical). To ensure successful deployment of\nLLMs in these scenarios, it is essential to improve\ntheir controllability, as failure to do so could lead to\ncatastrophic results. In digital environments, such\nas databases, unexpected behavior could lead to\nsafety issues with a company’s data and property.\nIn physical environments, it could even put human\nlife at risk. Pangu is proposed to provide better\ncontrollability for LLMs when being depolyed to\ninteract with different environments. Specifically,\nsafety considerations can be explicitly incorporated\ninto the agent’s candidate proposal (the symbolic\npart of Pangu) for enhanced security (i.e., harmful\nactions are directly excluded from the candidates\npool).\nAcknowledgements\nThe authors would like to thank Percy Liang, Ji-\nawei Han, Jonathan Berant, Huan Sun, and other\ncolleagues from the OSU NLP group for their valu-\nable feedback. The authors would also like to\nthank Shijie Chen and Chan Hee Song for proof-of-\nconcept implementation of Pangu on other tasks,\nYiheng Shu for sharing their entity linking re-\nsults, and Tianbao Xie for clarifications on Uni-\nfiedSKG. This research was supported in part by\nARL W911NF2220144, NSF OAC 2112606, and\nOhio Supercomputer Center (Center, 1987).\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex\nHerzog, et al. 2022. Do as i can, not as i say:\nGrounding language in robotic affordances. CoRR,\nabs/2204.01691.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei V orobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell I. Nye,\nMaarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V . Le,\nand Charles Sutton. 2021. Program synthesis with\nlarge language models. CoRR, abs/2108.07732.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Linguis-\ntics.\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg,\nand Yoav Artzi. 2022. A persistent spatial semantic\nrepresentation for high-level natural language instruc-\ntion execution. In Conference on Robot Learning ,\npages 706–717. PMLR.\nBen Bogin, Shivanshu Gupta, and Jonathan Berant.\n2022. Unobserved local structures make composi-\ntional generalization hard. CoRR, abs/2201.05899.\nKurt D. Bollacker, Colin Evans, Praveen K. Paritosh,\nTim Sturge, and Jamie Taylor. 2008. Freebase: a\ncollaboratively created graph database for structuring\nhuman knowledge. In Proceedings of the ACM SIG-\nMOD International Conference on Management of\nData, SIGMOD 2008, Vancouver, BC, Canada, June\n10-12, 2008, pages 1247–1250. ACM.\n4937\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nShulin Cao, Jiaxin Shi, Zijun Yao, Xin Lv, Jifan Yu, Lei\nHou, Juanzi Li, Zhiyuan Liu, and Jinghui Xiao. 2022.\nProgram transfer for answering complex questions\nover knowledge bases. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8128–\n8140, Dublin, Ireland. Association for Computational\nLinguistics.\nZhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and\nHang Li. 2007. Learning to rank: from pairwise ap-\nproach to listwise approach. In Machine Learning,\nProceedings of the Twenty-Fourth International Con-\nference (ICML 2007), Corvallis, Oregon, USA, June\n20-24, 2007, volume 227 of ACM International Con-\nference Proceeding Series, pages 129–136. ACM.\nOhio Supercomputer Center. 1987. Ohio supercomputer\ncenter.\nKhyathi Raghavi Chandu, Yonatan Bisk, and Alan W\nBlack. 2021. Grounding ‘grounding’ in NLP. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 4283–4305, On-\nline. Association for Computational Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nShuang Chen, Qian Liu, Zhiwei Yu, Chin-Yew Lin,\nJian-Guang Lou, and Feng Jiang. 2021b. ReTraCk:\nA flexible and efficient framework for knowledge\nbase question answering. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Con-\nference on Natural Language Processing: System\nDemonstrations, pages 325–336, Online. Association\nfor Computational Linguistics.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2022. Binding\nlanguage models in symbolic languages. CoRR,\nabs/2210.02875.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings\nof the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nE. F. Codd. 1970. A relational model of data for large\nshared data banks. Commun. ACM, 13(6):377–387.\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya God-\nbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros\nPolymenakos, and Andrew McCallum. 2021. Case-\nbased reasoning for natural language queries over\nknowledge bases. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 9594–9611, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nXiang Deng, Ahmed Hassan Awadallah, Christopher\nMeek, Oleksandr Polozov, Huan Sun, and Matthew\nRichardson. 2021. Structure-grounded pretraining\nfor text-to-SQL. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1337–1350, Online. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLi Dong, Jonathan Mallinson, Siva Reddy, and Mirella\nLapata. 2017. Learning to paraphrase for question an-\nswering. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 875–886, Copenhagen, Denmark. Association\nfor Computational Linguistics.\n4938\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nYu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler,\nPercy Liang, Xifeng Yan, and Yu Su. 2021. Beyond\nI.I.D.: three levels of generalization for question an-\nswering on knowledge bases. In WWW ’21: The Web\nConference 2021, Virtual Event / Ljubljana, Slovenia,\nApril 19-23, 2021, pages 3477–3488. ACM / IW3C2.\nYu Gu, Vardaan Pahuja, Gong Cheng, and Yu Su. 2022.\nKnowledge base question answering: A semantic\nparsing perspective. In 4th Conference on Automated\nKnowledge Base Construction.\nYu Gu and Yu Su. 2022. ArcaneQA: Dynamic program\ninduction and contextualized encoding for knowl-\nedge base question answering. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 1718–1731, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nIzzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Saf-\ndari, Austin Huang, Aakanksha Chowdhery, Sharan\nNarang, Noah Fiedel, and Aleksandra Faust. 2022.\nUnderstanding HTML with large language models.\nCoRR, abs/2210.03945.\nWonseok Hwang, Jinyeung Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\nCoRR, abs/1902.01069.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nNaman Jain, Skanda Vaidyanath, Arun Shankar Iyer,\nNagarajan Natarajan, Suresh Parthasarathy, Sriram K.\nRajamani, and Rahul Sharma. 2022. Jigsaw: Large\nlanguage models meet program synthesis. In 44th\nIEEE/ACM 44th International Conference on Soft-\nware Engineering, ICSE 2022, Pittsburgh, PA, USA,\nMay 25-27, 2022, pages 1219–1231. ACM.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nYunshi Lan and Jing Jiang. 2020. Query graph gen-\neration for answering multi-hop complex questions\nfrom knowledge bases. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 969–974, Online. Association for\nComputational Linguistics.\nBelinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar\nMehdad, and Wen-tau Yih. 2020. Efficient one-pass\nend-to-end entity linking for questions. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6433–6441, Online. Association for Computational\nLinguistics.\nHaoyang Li, Jing Zhang, Cuiping Li, and Hong Chen.\n2023a. Decoupling the skeleton parsing and schema\nlinking for text-to-sql. CoRR, abs/2302.05965.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. VisualBERT: A\nsimple and performant baseline for vision and lan-\nguage. CoRR, abs/1908.03557.\nTianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su,\nand Wenhu Chen. 2023b. Few-shot in-context learn-\ning for knowledge base question answering. CoRR,\nabs/2305.01750.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022. Competition-level code generation with alpha-\ncode. Science, 378(6624):1092–1097.\nChen Liang, Jonathan Berant, Quoc Le, Kenneth D. For-\nbus, and Ni Lao. 2017. Neural symbolic machines:\nLearning semantic parsers on Freebase with weak\nsupervision. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 23–33, Vancouver,\nCanada. Association for Computational Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Ladhak,\nFrieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert\nYüksekgönül, Mirac Suzgun, Nathan Kim, Neel\nGuha, Niladri S. Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models. CoRR, abs/2211.09110.\n4939\nYe Liu, Semih Yavuz, Rui Meng, Dragomir Radev,\nCaiming Xiong, and Yingbo Zhou. 2022. Uni-\nParser: Unified semantic parser for question an-\nswering on knowledge base and database. CoRR,\nabs/2211.05165.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. ViLBERT: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks.\nIn Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada, pages 13–23.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-\njes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.\n2021. Deep learning–based text classification: A\ncomprehensive review. ACM Comput. Surv., 54(3).\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff\nWu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William\nSaunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight,\nBenjamin Chess, and John Schulman. 2021. We-\nbGPT: Browser-assisted question-answering with hu-\nman feedback. CoRR, abs/2112.09332.\nAlexander Pashevich, Cordelia Schmid, and Chen Sun.\n2021. Episodic transformer for vision-and-language\nnavigation. In 2021 IEEE/CVF International Confer-\nence on Computer Vision, ICCV 2021, Montreal, QC,\nCanada, October 10-17, 2021, pages 15922–15932.\nIEEE.\nXutan Peng, Yipeng Zhang, Jingfeng Yang, and Mark\nStevenson. 2022. On the security vulnerabilities of\ntext-to-sql models. CoRR, abs/2211.15363.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-sql capabilities\nof large language models. CoRR, abs/2204.00498.\nSiva Reddy, Oscar Täckström, Slav Petrov, Mark Steed-\nman, and Mirella Lapata. 2017. Universal semantic\nparsing. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 89–101, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nOhad Rubin and Jonathan Berant. 2021. SmBoP: Semi-\nautoregressive bottom-up semantic parsing. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n311–324, Online. Association for Computational Lin-\nguistics.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9895–9901, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nDhruv Shah, Bła ˙zej Osi ´nski, brian ichter, and Sergey\nLevine. 2022. LM-Nav: Robotic navigation with\nlarge pre-trained models of language, vision, and\naction. In 6th Annual Conference on Robot Learning.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7699–7715, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. ALFRED: A\nbenchmark for interpreting grounded instructions for\neveryday tasks. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020 , pages\n10737–10746. IEEE.\nYiheng Shu, Zhiwei Yu, Yuhan Li, Börje Karlsson,\nTingting Ma, Yuzhong Qu, and Chin-Yew Lin. 2022.\nTIARA: Multi-grained retrieval for robust question\nanswering over large knowledge base. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 8108–8121,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,\nJesse Thomason, and Animesh Garg. 2022. Prog-\nprompt: Generating situated robot task plans using\nlarge language models. CoRR, abs/2209.11302.\nChan Hee Song, Jihyung Kil, Tai-Yu Pan, Brian M\nSadler, Wei-Lun Chao, and Yu Su. 2022a. One step at\na time: Long-horizon vision-and-language navigation\nwith milestones. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 15482–15491.\nChan Hee Song, Jiaman Wu, Clayton Washington,\nBrian M Sadler, Wei-Lun Chao, and Yu Su. 2022b.\nLLM-Planner: Few-shot grounded planning for em-\nbodied agents with large language models. CoRR,\nabs/2212.04088.\nYu Su, Ahmed Hassan Awadallah, Madian Khabsa,\nPatrick Pantel, Michael Gamon, and Mark J. Encar-\nnación. 2017. Building natural language interfaces to\nweb apis. In Proceedings of the 2017 ACM on Con-\nference on Information and Knowledge Management,\n4940\nCIKM 2017, Singapore, November 06 - 10, 2017 ,\npages 177–186. ACM.\nYu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa,\nIzzeddin Gür, Zenghui Yan, and Xifeng Yan. 2016.\nOn generating characteristic-rich question sets for\nQA evaluation. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 562–572, Austin, Texas. Associa-\ntion for Computational Linguistics.\nYawei Sun, Lingling Zhang, Gong Cheng, and Yuzhong\nQu. 2020. SPARQA: skeleton-based semantic pars-\ning for complex questions over knowledge bases. In\nThe Thirty-Fourth AAAI Conference on Artificial In-\ntelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020 , pages\n8952–8959. AAAI Press.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Sys-\ntems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Mon-\ntreal, Quebec, Canada, pages 3104–3112.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. RAT-SQL:\nRelation-aware schema encoding and linking for text-\nto-SQL parsers. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7567–7578, Online. Association for\nComputational Linguistics.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nRonald J. Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural Comput., 1(2):270–280.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir R. Radev, Caim-\ning Xiong, Lingpeng Kong, Rui Zhang, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2022. Unified-\nSKG: Unifying and multi-tasking structured knowl-\nedge grounding with text-to-text language models.\nCoRR, abs/2201.05966.\nXi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou,\nand Caiming Xiong. 2022. RNG-KBQA: Generation\naugmented iterative ranking for knowledge base ques-\ntion answering. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6032–6043,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jian-\nfeng Gao. 2015. Semantic parsing via staged query\ngraph generation: Question answering with knowl-\nedge base. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1321–1331, Beijing, China. Association for\nComputational Linguistics.\nWen-tau Yih, Matthew Richardson, Chris Meek, Ming-\nWei Chang, and Jina Suh. 2016. The value of se-\nmantic parse labeling for knowledge base question\nanswering. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers) , pages 201–206, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nDonghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu,\nAlexander Hanbo Li, Jun Wang, Yiqun Hu, William\nWang, Zhiguo Wang, and Bing Xiang. 2022. De-\ncAF: Joint decoding of answers and logical forms for\nquestion answering over knowledge bases. CoRR,\nabs/2210.00063.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang,\nYi Chern Tan, Xinyi Yang, Dragomir R. Radev,\nRichard Socher, and Caiming Xiong. 2021. GraPPa:\nGrammar-augmented pre-training for table semantic\nparsing. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nHonglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui,\nJi Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and\nMichael Bendersky. 2022. RankT5: Fine-tuning\nT5 for text ranking with ranking losses. CoRR,\nabs/2210.10634.\n4941\nAppendices\nIn this supplementary material, we provide further\ndetails as follows:\n• Appendix A: Broader Applicability of Pangu\n• Appendix B: Candidate Enumeration\n• Appendix C: Experimental Setup\n• Appendix D: Decomposition by Question\nComplexity\n• Appendix E: Error Analysis\n• Appendix F: Examples of Prompts\nA Broader Applicability of Pangu\nAlgorithm 1 describes a generic framework for\ngrounded language understanding, but the concrete\nimplementation for the functions in Algorithm 1\nmay vary for different tasks. We have shown a rep-\nresentative instantiation for KBQA. In this section,\nwe briefly discuss the possible instantiation for two\nother tasks of different nature. In addition, we also\npresent some preliminary results we have obtained\nto demonstrate the feasibility of Pangu.\nA.1 Text-to-SQL Parsing\nPossible Instantiation. Similar to KBQA, Text-\nto-SQL parsing also aims to map a natural language\nutterance into a program that can be executed over\na relational database (instead of a KB). When the\ndatabase schema is reasonably small (generally true\nfor existing datasets like Spider (Yu et al., 2018)),\nwe can define P0 as the set of all schema items (i.e.,\ncolumn headers and table names) plus the set of cell\nvalues mentioned in the utterance, which should be\nstraightforward to identify (e.g., with string match-\ning). In this way, the agent can construct candidate\nprograms similarly to Rubin and Berant (2021). 6\nThe termination check for text-to-SQL parsing can\nalso be implemented similarly.\nPreliminary Results. We test Pangu on Spider, a\npopular benchmark on Text-to-SQL parsing, fol-\nlowing the aforementioned instantiation. Due to the\nsmaller environment of relational databases (com-\npared with KBs), schema linking performance on\n6One necessary step is to convert a SQL query into an\nalgebra tree (Codd, 1970), similar to what is done by Rubin\nand Berant (2021). In this way, the agent can more easily\nenumerate the candidate programs in a bottom-up manner.\nWhen the database schema is too large, we may define P0\nonly as the set of mentioned cell values, then the process of\ncandidate enumeration will resemble KBQA (i.e., cell values\ncan be treated as entities in the KB.)\nSpider is already at 99% (Li et al., 2023a). We\ntherefore start the search assuming ingredients have\nbeen identified (i.e., the initial plan). With the same\nlanguage model being used (i.e., CodeBERT (Feng\net al., 2020)), we achieved 70.6% Exact-Match on\nSpider dev, comparable to SmBop’s71.7% (Rubin\nand Berant, 2021), a strong bottom-up parser base-\nline, when using the same LM. Note that these are\nonly preliminary results to demonstrate the feasi-\nbility of applying Pangu to other tasks. There is\nstill a large room to improve by, e.g., optimizing\nthe search process or using stronger LMs.\nA.2 Interacting with Real-World\nEnvironments\nPossible Instantiation. Pangu can also be used\nfor guiding bots that interact with real-world envi-\nronments, both online websites (Gur et al., 2022;\nNakano et al., 2021) and physical environments\nthrough embodied agents (Shridhar et al., 2020).\nGiven a complex task to be accomplished in the\nenvironment, an agent may decompose it into a\nsequence of subplans (e.g., making a cup of coffee\nentails first finding a cup then picking up the cup,\netc.; Song et al. (2022b)), and combine it with all\nexecutable actions in the environment to enumerate\nthe candidate plans and select the best action with\nan LM. One difference in these cases is that real-\nworld environments often contain information from\nmultiple modalities, thus requiring multi-modal lan-\nguage models (Li et al., 2019; Lu et al., 2019) that\nare capable of jointly handling textual, visual, and\nother modalities.\nMore concretely, let us consider the task of em-\nbodied instruction following, on the popular AL-\nFRED dataset (Shridhar et al., 2020). We use LMs\nas high-level planners for the embodied agent. For\nexample, for a command like “make me a cup of\ncoffee\", a high-level plan like [Navigate cup, PickUp\ncup, Navigate coffee_maker, . . .] is first generated.\nThe agent is equipped with an object detector and\na low-level planner to execute the high-level plan\nfrom the LM (e.g., navigating to a cup is a classic\nobject localization problem handled by the low-\nlevel planner). At each search step, the agent gen-\nerates a candidate (high-level action, object) pair\nfor each object observed in the environment as pos-\nsible extensions of the current plan. The LM then\nscores the candidate expansions similar to KBQA;\nthe best one is executed by the low-level planner.\nPreliminary Results. We use the object detector\n4942\nComposition Rule Signature Comments\nJOIN R×(E∪E′)→E′ a single hop along an edge\nAND (T∪E′)×E′→E′ intersection of two sets\nARGMAX/ARGMIN(T∪E′)×R→E′ superlative aggregations\nLT/LE/GT/GE R×E→E′ < /≤/ > /≥\nCOUNT E′→N set cardinality\nTable B.1: Functions in KBQA. We follow the defini-\ntions in (Gu and Su, 2022). R: relation, T: type, E:\nentity, E′: a set of entities, N: integer.\nand low-level planner from HLSM (Blukis et al.,\n2022), and use GPT-3.5 text-davinci-003 as the\nLM with in-context learning using only100 labeled\nexamples.7 We achieved 10% overall success rate\nand 25% goal completion on ALFRED’s unseen\ndev, already outperforming recent baselines (Pa-\nshevich et al., 2021) trained with full data ( 21K+\nexamples).\nB Candidate Enumeration\nOur candidate enumeration for KBQA strictly\nfollows the definition of functions in Table B.1.\nSpecifically, given a set of current plans Pt, to con-\nstruct the candidate set Ct+1, for each plan pi in Pt,\nthe agent executes it and gets types and relations\nthat are reachable from the denotation of the plan.\nFor each type t, the agent enumerates (AND t pi) as\na candidate. For each relation r, the agent enumer-\nates (JOIN r pi) as a candidate. If the denotation of\npi is a numerical value, then four similar candidates\nwith comparatives are also included (LT/LE/GT/GE r\npi). In addition, candidate plans with superlatives\ncan be enumerated as (ARGMAX/ARGMIN pi r). Also,\n(COUNT pi) can always be included to Ct+1. Af-\nter checking each pi independently, the agent then\nchecks each pair of plans pi and pj from Pt, if the\nexecution of pi and pj has an overlap, then (AND pi\npj) is also included as a candidate plan. The can-\ndidate enumeration process is totally transparent\nto the LM and can be easily controlled based on\ndifferent needs.\nC Experimental Setup\nC.1 Datasets Statistics\nAll three datasets provide gold program annota-\ntions. For consistency, we use the converted S-\nexpressions representation provided by Gu and Su\n(2022) in our experiments. Concrete statistics of\ndifferent datasets are shown in Table C.3.\n7Codex was deprecated on March 23, 2023, so we run our\nexperiments with GPT-3.5 here.\nC.2 More Details on Baselines\nDifferent LMs and decoding strategies are used in\nthe baseline models.\nArcaneQA (Gu and Su, 2022) is an encoder-\ndecoder model built on top of a BERT encoder.\nIt leverages constrained decoding and incremen-\ntally synthesizes a sequence of subprograms, where\nthe constraints come from both the grammar and\nthe execution of existing subprograms, to enforce\ngrammaticality and faithfulness.\nTIARA (Shu et al., 2022) first uses BERT to re-\ntrieve a set of schema items, which are further used\nas the input, together with the question, to T5 for\nplan generation. They also apply constrained de-\ncoding but only for grammaticality.\nDecAF (Yu et al., 2022) similarly retrieves a rele-\nvant subgraph from the KB using DPR (Karpukhin\net al., 2020), and then input the retrieved items to\nFiD (Izacard and Grave, 2021), a T5 model fine-\ntuned for question answering.\nRnG-KBQA (Ye et al., 2022) first uses BERT to\nrank a set of enumerated candidate programs (up\nto a limited complexity), and then uses T5 to edit\nthe top programs into more complex programs.\nUnifiedSKG (Xie et al., 2022) also retrieves a sub-\ngraph from the KB as input to T5. The setting of\nUnifiedSKG is different from other baselines. It\nassumes the gold schema items are always included\nin the retrieved subgraph and restricts the number\nof negative schema items in the subgraph (i.e., at\nmost 20 schema items for GRAIL QA). It is thus\na less fair comparison for other methods, but we\ninclude it anyway because it is a representative way\nof autoregressive plan generation using a large LM.\nA summary of the baselines can be found in Ta-\nble C.2.\nC.3 Implementation Details\nFor GRAIL QA we use the entity linking results\nfrom TIARA. For WEBQSP , we get that from\nELQ (Li et al., 2020), which is also used by our\nbaseline models. For GRAPH Q, get that from Ar-\ncaneQA. The entity proposals for the input utter-\nance form the initial plans (P0) for our search pro-\ncess. We use beam size 5 for all of our fine-tuning\nexperiments. We run our experiments with T5-3B\nusing a single NVIDIA A100 80GB card, while\nfor all other fine-tuning experiments, we run them\nusing 4×NVIDIA A6000 48GB cards.\nFor our experiments with Codex, we use a beam\nsize of 2 and a max number of candidates of 1,000\n4943\nModel LMs Grounding Strategy Guarantees\nArcaneQA(Gu and Su, 2022) BERT-base Constrained Decoding Grammatical+Faithful\nRnG-KBQA(Ye et al., 2022) BERT-base + T5-base Input Augmentation N/A\nTIARA(Shu et al., 2022) BERT-base + T5-base Input Augmentation + Constrained Decoding Grammatical\nDecAF(Yu et al., 2022) DPR + FiD-3B Input Augmentation N/A\nUnifiedSKG(Xie et al., 2022) T5-base(/large/3B) Input Augmentation N/A\nTable C.2: A brief summary of main baseline models.\nDataset Training Dev Test\nGRAIL QA 44,337 6,763 13,231\nGRAPH Q 2,381 − 2,395\nW EB QSP 3,098 − 1,639\nTable C.3: Statistics of KBQA datasets.\nfor speed concerns, which to some extent sacrifices\nthe performance. As the first endeavor towards\nenabling few-shot KBQA with LLMs, we did not\ntune the hyper-parameters very hard. The only\nthing we tuned is the scoring function. We tune\nthe scoring function using 10-shot training data\nfrom GRAIL QA with cross-validation. If we di-\nrectly use P(c|u) as our scoring function s(u,c)\nin Section 3.3, Codex tends to favor programs with\nrepeated relations. As a result, we add a penalizing\nfactor to P(c|u), and define s(u,c) as P(c|u)×ηn,\nwhere η∈[0,1] is a hyper-parameter, and nis the\nmaximal occurrences of a relation in a program.\nWe set η= 0.7 based on cross-validation using the\n10 training examples.\nFinally, a small percentage of questions (around\n5%) in GRAPH Q and GRAIL QA do not have a\ntopic entity (e.g., “who is the heaviest film direc-\ntor?\" from GRAIL QA, whose target program is\n(ARGMAX film.director people.person.weight_kg)). For\nthese questions, we use the answer types ( e.g.,\nfilm.director) predicted in Gu and Su (2022) as our\ninitial state P0.\nD Decomposition by Question Complexity\nWe present a fine-grained analysis of Pangu with\nT5-3B and Codex (100-shot) on questions of dif-\nferent complexity, measured by the number of re-\nlations in the gold program, in Table D.4. For\nGRAIL QA, we report the performance on its dev\nset because the test set is hidden. Pangu performs\ncompetitively across all complexity. Note that there\nare only two questions in GRAIL QA’s dev set with\n4 relations, so the results on that may not be in-\ndicative. On GRAPH Q, Pangu significantly outper-\nforms ArcaneQA. The F1 of Pangu with T5-3B is\n# of relations 1 2 3 4\nRnG-KBQA 79.2 74.8 44.4 100.0\nArcaneQA 80.9 71.1 37.7 100.0\nTIARA 85.6 75.8 48.5 83.3\nPangu w/ T5-3B 87.0 78.4 48.1 83.3\nPangu w/ Codex (100-shot) 73.9 43.4 33.0 16.7\n(a) GRAIL QA\n# of relations 1 2 3\nArcaneQA 48.2 19.3 9.6\nPangu w/ T5-3B 72.3 55.5 27.8\nPangu w/ Codex (100-shot) 52.2 36.1 17.5\n(b) GRAPH Q\nTable D.4: F1 decomposition by program complexity\non GRAIL QA’s dev set and GRAPH Q’s test set.\nalmost three times higher than ArcaneQA on ques-\ntions with 2 and 3 relations. Interestingly, Pangu\nwith Codex also outperforms ArcaneQA consider-\nably on questions with 2 and 3 relations. These\nfindings suggest the superiority of Pangu in gener-\nalizing to more complex programs.\nE Error Analysis\nWe analyze 200 incorrect predictions (i.e., EM=0)\nrandomly sampled from GRAIL QA’s dev set for\nour best model (i.e., T5-3B). The major errors are\ndue to unidentified topic entities during entity link-\ning (62%).8 Also, Pangu tends to include unre-\nlated entities provided by the entity linker into\nthe final programs (6.5% of the errors), this is be-\ncause Pangu is fine-tuned with gold entities only,\nand thus does not learn to handle unrelated enti-\nties. In addition, wrong termination check corre-\nsponds to 12.5% of the errors, indicating a venue\nfor better enforcing the partial order to Pangu.\nApart from these errors, 10.5% of the mistakes are\ndue to ambiguous annotations or annotation errors\nin GRAIL QA. The remaining error types include\nwrong comparators, answer types, and relations\n(particularly relations involve a subtle direction\nlike cvg.computer_game_engine.predecessor_engine).\n8The recall of entity linking on GRAIL QA is 88.6% (Shu\net al., 2022)\n4944\nIn addition, for in-context learning with Codex\n(100-shot), we also randomly sample 200 wrong\npredictions from GRAIL QA’s dev set. In addi-\ntion to 22% errors caused by missing entities, the\nmost common errors ( 25.5%) are due to wrong\nschema items. Distinguishing gold schema items\nfrom confusing ones is challenging for in-context\nlearning. Also, missing constraints ( 16.5%) and\nmissing relations (10%) are another two major er-\nror types, because we use a small batch size ( i.e.,\n2) for Codex and the model tends to prefer short\nprograms. These two error types are also related\nto wrong termination check. Finally, there are 12%\nwrong functions. The error types of Pangu w/\nCodex are very different from Pangu w/ T5-3B.\nThis is because for a complex task like KBQA,\nthe performance of in-context learning with Pangu\nstill largely lags behind fine-tuning. Particularly,\nfine-tuning methods directly learn the partial or-\nder among programs during training, while Codex\nneeds to implicitly infer a partial order by itself,\nwhich is not directly shown in the demonstrations.\nAs a result, Pangu w/ Codex makes more trivial\nmistakes that fine-tuning methods can easily avoid.\nMore advanced in-context learning techniques to\nclose this gap remains to be explored.\nF Examples of Prompts\nWe show two examples of prompts with 10 in-\ncontext samples retrieved from the 100 training\ndata pool in Figure F.1 and Figure F.2 for two dif-\nferent questions from GRAIL QA’s dev set. Our\nprompt design is very straightforward. More ad-\nvanced prompting techniques for Pangu remains to\nbe explored.\n4945\n### Please translate the following questions to Lisp-like query language. # which automotive designer designed astonmartin db7 zagato? (AND automotive.designer(JOIN automotive.designer.automobiles_designedastonmartin db7 zagato)) # d-series machines was designed by which computer designer? (AND computer.computer_designer(JOIN computer.computer_designer.computers_designedd-series machines)) # who designed both visual basic .netand j#? (AND computer.programming_language_designer(AND (JOIN computer.programming_language_designer.languages_designedvisual basic .net) (JOIN computer.programming_language_designer.languages_designedj#))) # which architect designed katherineatkinshouse by polk? (AND architecture.architect(JOIN architecture.architect.structures_designedkatherineatkinshouse by polk)) # what is the name of the author who wrote it is an open question whether any behavior based on fear of eternal punishment can be regarded as ethical or should be regarded as merely cowardly.? (AND film.director(JOIN media_common.quotation.author_invit is an open question whether any behavior based on fear of eternal punishment can be regarded as ethical or should be regarded as merely cowardly.)) # who was the manufacturer of kosmos3m? (AND spaceflight.rocket_manufacturer(JOIN spaceflight.rocket_manufacturer.rockets_manufacturedkosmos3m)) # who is the endorser of coke products? (AND business.product_endorser(JOIN business.product_endorsement.endorser_inv(JOIN business.product_endorsement.productcoke))) # what short story has a character who also is in doing clarence a bit of good? (AND book.short_story(JOIN book.short_story.characters(JOIN book.book_character.appears_in_storiesdoing clarence a bit of good))) # who was the director of the episode katejackson/delbertmcclinton? (AND tv.tv_director(JOIN tv.tv_director.episodes_directedkatejackson/delbertmcclinton)) # what is the identity of the football player who appeared 23 times internationally? (AND soccer.football_player(JOIN soccer.football_player.total_international_appearances23)) # what is the role of opera designer gig who designed the telephone / the medium? \nFigure F.1: Example prompt (i) for question “what is the role of opera designer gig who designed the telephone /\nthe medium?\"\n4946\n### Please translate the following questions to Lisp-like query language. # homegrown is a recurring segment on what tv program? (AND tv.tv_program(JOIN tv.tv_program.recurring_segmentshomegrown)) # on 07/01/1970, which warship v1.1 was hit? (AND user.patrick.default_domain.warship_v1_1 (JOIN user.patrick.default_domain.warship_v1_1.struck 07/01/1970)) # what is the isbnof the edition with scottfisher on its book cover? (AND book.isbn(JOIN book.book_edition.isbn_inv(JOIN book.illustrator.book_edition_covers_invscottfisher))) # which musical artist stopped being active as musical artist on 1985-06? (AND music.artist(JOIN music.artist.active_end1985-06)) # the honorary degree recipient that was born most recently is named what? (ARGMAX education.honorary_degree_recipientpeople.person.date_of_birth) # the medical trials conducted on safety and effectiveness of giving indinavir plus stavudine plus lamivudine to hiv-infected children are under the authority of who? (AND medicine.medical_trial_health_authority(JOIN medicine.medical_trial_health_authority.medical_trialssafety and effectiveness of giving indinavir plus stavudine plus lamivudine to hiv-infected children)) # bataan1 and bataan2 is what aircraft model? (AND aviation.aircraft_model(JOIN aviation.aircraft_model.aircraftbataan1 and bataan2)) # what ingredient is in frenchcuisine? (AND food.ingredient(JOIN food.ingredient.cuisinefrenchcuisine)) # south kentschool and redfieldcollege fall under what category of school? (AND education.school_category(AND (JOIN education.educational_institution.school_type_invsouth kentschool) (JOIN education.school_category.schools_of_this_kindredfieldcollege))) # chiangkai shekcollege and sacred heart high school (roseville, michigan) are in what category of school? (AND education.school_category(AND (JOIN education.educational_institution.school_type_invchiangkai shekcollege) ( JOIN education.school_category.schools_of_this_kindsacred heart high school (roseville, michigan)))) # semaphore railway line is on the rail network named what? \nFigure F.2: Example prompt (ii) for question “semaphore railway line is on the rail network named what?\"\n4947\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nThe section after conclusion, following the instruction in the Latex ﬁle.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4.1; Section C.1\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.1; Section C.1\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 4.1; Section C.1\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe used datasets are all widely used for this task and there is no report of identifying or offensive\ncontent as far as we know\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4.1; Section C.1\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection C.1\nC □\u0013 Did you run computational experiments?\nSection 5.1; Section 5.2; Section D\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.3; Section C.3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4948\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection C.3\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5.1; Section 5.2\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4.3; Section C.3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4949"
}