{
  "title": "An Analysis of the Ability of Statistical Language Models to Capture the Structural Properties of Language",
  "url": "https://openalex.org/W2564413773",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A5083561499",
      "name": "Aneiss Ghodsi",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5072072153",
      "name": "John DeNero",
      "affiliations": [
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2171928131"
  ],
  "abstract": "We investigate the characteristics and quantifiable predispositions of both n-gram and recurrent neural language models in the framework of language generation.In modern applications, neural models have been widely adopted, as they have empirically provided better results.However, there is a lack of deep analysis of the models and how they relate to real language and its structural properties.We attempt to perform such an investigation by analyzing corpora generated by sampling from the models.The results are compared to each other and to the results of the same analysis applied to the training corpus.We carried out these experiments on varieties of Kneser-Ney smoothed n-gram models and basic recurrent neural language models.Our results reveal a number of distinctive characteristics of each model, and offer insights into their behavior.Our general approach also provides a framework in which to perform further analysis of language models.",
  "full_text": "Proceedings of The 9th International Natural Language Generation conference, pages 227–231,\nEdinburgh, UK, September 5-8 2016.c⃝2016 Association for Computational Linguistics\nAn Analysis of the Ability of Statistical Language Models to Capture the\nStructural Properties of Language\nAneiss Ghodsi and John DeNero\nComputer Science Division\nUniversity of California, Berkeley\n{aneiss, denero}@berkeley.edu\nAbstract\nWe investigate the characteristics and quan-\ntiﬁable predispositions of both n-gram and re-\ncurrent neural language models in the frame-\nwork of language generation. In modern ap-\nplications, neural models have been widely\nadopted, as they have empirically provided\nbetter results. However, there is a lack of deep\nanalysis of the models and how they relate\nto real language and its structural properties.\nWe attempt to perform such an investigation\nby analyzing corpora generated by sampling\nfrom the models. The results are compared to\neach other and to the results of the same anal-\nysis applied to the training corpus. We carried\nout these experiments on varieties of Kneser-\nNey smoothed n-gram models and basic recur-\nrent neural language models. Our results re-\nveal a number of distinctive characteristics of\neach model, and offer insights into their be-\nhavior. Our general approach also provides a\nframework in which to perform further analy-\nsis of language models.\n1 Introduction\nStatistical language modelling is critical to natural\nlanguage processing and many generation systems.\nIn recent years use has shifted from the previously\nprevalent n-gram model to the recurrent neural net-\nwork paradigm that now dominates in most applica-\ntions. Researchers have long sought to ﬁnd the best\nlanguage modeling solutions for particular applica-\ntions, but it is important to understand the behav-\nior of language models in a more generalizable way.\nThis is advantageous both in developing language\nmodels and in applying them practically. Whether\nin tasks where statistical models are used to directly\ngenerate language or in cases where the model is\nused for ranking for surface realization, the statis-\ntical predispositions of the language model will be\nreﬂected in the results. In this paper we compare\nthe behavior of n-gram models and Recurrent Neural\nNetwork Language Models (RNNLMs) with regard\nto properties of their generated language.\nWe use the SRILM toolkit for training and gen-\nerating from n-gram models (Stolcke and others,\n2002). Our n-gram model is a modiﬁed Kneser-\nNey back-off interpolative model, unless otherwise\nstated (Chen and Goodman, 1999). We use Tomas\nMikolov’s implementation of an RNNLM, avail-\nable at rnnlm.org (Mikolov et al., 2010). This\nmodel has a single hidden recurrent layer, and three\ndeﬁning parameters: class size, hidden layer size,\nand backpropagation through time (BPTT) steps.\nClasses are used to factor the vocabulary mappings\nto improve performance, by predicting a distribution\nover classes of words and then over words in a class\n(Mikolov et al., 2011). BPTT steps determine how\nmany times the recurrent layer of the network is un-\nwrapped for training. Unless otherwise mentioned\nall neural models have class of 100 and use four\nBPTT steps. We use the Penn Tree Bank (PTB),\nconstructed from articles from the Wall Street Jour-\nnal, as our primary training corpus, with the stan-\ndard training split of 42068 sentences (Marcus et\nal., 1993). Correspondingly, our generated language\ncorpora also contain 42068 sentences. Novel sen-\ntences are easily sampled from trained language\nmodels by prompting with a start of sentence token,\n227\nFigure 1:Sentence Length Distributions\nsampling from the predicted distribution, using the\nresult as context, and repeating until an end of sen-\ntence token is encountered.\nWe select three primary metrics with which to\nevaluate the various resulting corpora. The ﬁrst is\nthe distribution of sentence lengths. Sentence length\nis compared visually and through the sum of error as\ncompared to the length distribution from the training\ncorpus. The second metric is word frequency. Word\nfrequency is analyzed by ﬁtting a Zipﬁan distribu-\ntion (Kingsley, 1932), and comparing between the\ndistributions for each model. Third is pronoun fre-\nquency relative to distance from the start of a sen-\ntence. This was selected as a metric due to the fact\nthat one-word pronouns are a small class fairly eas-\nily identiﬁable regardless of context (though there\nare a few that can be other parts of speech), partly\navoiding the ambiguities and challenges that follow\nfrom part of speech taggers. This is especially useful\nin a corpus with a restricted vocabulary resulting in\nthe replacement of uncommon tokens with a single\ntoken, such as the PTB, and with generated language\nthat is not always semantically sound. These experi-\nments were repeated multiple times with small vari-\nations, ensuring the key patterns in the results were\nnot a product of chance.\nThrough these three metrics we seek to develop\nsome insights into the behavior of standard stochas-\ntic models in language generation.\n2 Sentence Lengths\nThe natural expectation is that a recurrent neural\nmodel, with its superior ability to ‘remember’ com-\nCorpus Sum of Error\nTrigram 27736\n5-gram 29694\nNeural Hidden 100 19237\nNeural Hidden 500 14132\nTable 1:Sum of errors for sentence lengths, including normal-\nized over total sentences.\nplex context, would vastly outperform even fairly\nhigh order n-gram models in modeling sentence\nlength. While in training errors are only propagated\nas far back as truncated backpropagation is executed\n(the BPTT steps hyperparameter), the power of the\nrecurrent layer seems to exceed its apparent depth\nduring training, taking advantage of the ability of re-\ncurrent memory to retain subtle contextual informa-\ntion. As seen in Figure 1, even the four BPTT step\nmodel performs fairly well. Contrastingly, n-gram\nmodels perform very poorly. Table 1 notes the sum\nof the absolute errors across the full range of mod-\nels. N-gram models exhibit no improvement with\nincreasing order. In neural production, however, we\nsee substantial improvements with increasing net-\nwork complexity; speciﬁcally, with an increase in\nthe size of the hidden layer and the number of BPTT\nsteps. However, the neural models tested here are\nunable to replicate the precise shape of the distribu-\ntion. All models overestimate the incidence of very\nlong sentences.\n3 Vocabulary Distribution\nZipf’s Law states that, for N unique words and s as\nthe deﬁning parameter, the frequency of a word with\nrank k is given by the following (Kingsley, 1932):\nf(k; s, N) = 1/ks\n∑N\nn=1(1/ns)\nThere are two aspects of evaluation for word fre-\nquencies: First, the difference between the Zipf\nparameters of distributions ﬁtted to various text\nsources; second, the error on the data set to which a\nZipﬁan distribution is ﬁtted, indicating how closely\nthe data follows a distribution known to match natu-\nral language production.\nAs shown in table 2, n-gram smoothing tech-\nniques have a signiﬁcant effect on the accuracy of\nthe generated Zipf distribution. As an n-gram model\napproaches being a simple unigram model, it should\n228\nCorpus s LL\nReal 0.99193 -104598\nUnigram 0-Discount 0.99293 -104416\nTrigram 0-Discounts 0.98348 -103967\nTrigram Discounts 0.97921 -104049\nTrigram Back-Off Only 0.93515 -102532\nNeural Hidden 100 0.98707 -104332\nNeural Hidden 500 0.99735 -104655\nTable 2:Zipf ﬁt parameters s with Log-Likelihood.\nFigure 2:Zipf Distributions\napproach the same distribution as real language, due\nto the fact that a unigram model behaves like direct\nsampling of words from the training corpus. Thus\nit is intuitive that the interpolated models, in which\nunigram information always inﬂuences generation,\nperforms better than a simple Kneser-Ney back-off\nmodel. Critically, on any conﬁguration, non-zero\ndiscounting seems to worsen the distribution. As\ndiscounting is a method by which probability is held\nout to distribute amongst less likely or unseen se-\nquences or tokens, it is reasonable that it would af-\nfect the distribution. Figure 2 shows the distributions\nfrom a selection of models on a log-log scale, with\nthe trigram model with non-zero discounts (D) and\nwith zero discounts (ND).\n4 Pronoun Frequency with Depth\nFinally, we observe the probability of encountering\na pronoun at an index according to the following ex-\npression:\n∑\ns∈sentences s[i] ∈pronouns∑\ns∈sentences len(s) ≥i + 1\nWe ﬁnd that there is a spike in the probability\nFigure 3:Pronoun Probability with Position\nof encountering a pronoun as the ﬁrst word in a\nsentence, to approximately 0.15, an intuitive result\ngiven the prevalence of pronouns as sentence sub-\njects. All models captured this fairly well. More in-\nterestingly, the probability of generating or observ-\ning a pronoun decreases with depth into a sentence.\nThis phenomenon is clearly observable in the train-\ning set, with a fairly linear slope, which we cal-\nculate to be approximately −6.9 ×10−4 when re-\nstricted to the ﬁrst twenty indices, excluding zero,\ndue to the low number of samples at further posi-\ntions in the sentence causing noise to dominate. In\norder to verify this result, the slope was calculated\nby sampling 20 subsets of sentences and averaging\nthe slope across subsets. A comparable slope exists\neven when the domain is restricted to a set of sen-\ntences all of the same length (for example fourteen\nword sentences). This means the phenomenon is not\nan artifact resulting from the distribution of sentence\nlengths and a relationship between pronoun occur-\nrences and sentence endings.\nNeither class of model does particularly well at\ncapturing this property, as can been seen in Figure\n3. N-gram models were able to effectively capture\nthe pronoun probability at the ﬁrst word, as expected\ngiven the model should more or less reproduce the\nﬁrst-word distribution of the training data. They also\nappear to reﬂect the probabilities at the next sev-\neral indices, but as with sentence length, they fail\nat any signiﬁcant sentence depth regardless of n-\ngram order. The distribution in the n-gram generated\nlanguage becomes approximately uniform. Neural\nmodels seem to capture some negative slope in the\n229\nﬁrst ten to twenty words, but with depressed overall\nprobabilities, and a loss of the pattern after a certain\ndepth. Figure 3 also shows that increasing RNNLM\ncomplexity, whether in class, hidden size, or number\nof BPTT steps, does little to change the performance\nof the model in this metric.\nThis is concerning regarding the ability of this\nform of RNNLM to capture certain complex struc-\ntural patterns, and indicates that the structure is in-\nherently limited. It may be that a model with a\nLong-Short Term Memory unit (LSTM) as the re-\ncurrent component could perform better, with its\nsuperior ability to capture longer term contextual\ndependencies (Hochreiter and Schmidhuber, 1997).\nIndeed, LSTMs have become highly popular in\nmany sequential learning tasks. However, given that\nthese same basic RNNLMs performed well in the\nposition-dependent sentence length metric, this re-\nsult is disappointing.\n5 Future Work\nThere are a number of clear steps to expand on\nthis line of research, including experimenting with\na greater variety of language models. In particular,\na recurrent model with a Long Short-Term Memory\nunit (LSTM) might improve on the weaknesses of\nthe simple RNNLM demonstrated here.\nAdditionally, further diversiﬁcation of data sets is\nimportant to learning about patterns as they differ\nor remain consistent across sources. For example,\npreliminary analysis of the more stylistically diverse\nBrown corpus (Francis, 1964) indicates that the pro-\nnoun trend observed in the PTB may not be present\nin other domains, at least not as clearly. Addition-\nally to proﬁling models on speciﬁc text genres, the\nexperiments must be recreated on a far more sizeable\ndataset, such as the Wikipedia text corpus.\nFinally, the introduction of new metrics to the lan-\nguage model analysis could add further value. Auto-\nmatic tagging and parsing systems are likely to suf-\nfer from signiﬁcant inaccuracy on the often ﬂawed\ntext produced by stochastic models; however, the\nresults from applying such systems could prove in-\nformative about language model quality, as a model\nis not effectively capturing structural and semantic\nproperties of language if parsing and tagging results\nstatistics are not comparable to those of real lan-\nguage. Statistical analysis of parsing results would\nhelp expand the quantitative portrait of a language\nmodel.\n6 Conclusion\nOur work characterizes some key structural proper-\nties of language generated from two common sta-\ntistical models. The results presented here ver-\nify many of the expectations regarding the behav-\nior of n-gram and RNN techniques, and also intro-\nduce some new observations. RNNs have a struc-\ntural capacity largely missing from n-gram models,\nwhich is particularly apparent in sentence length dis-\ntributions. The recurrent model used here, however,\nstruggled in reproducing the more complex pattern\nrepresented by the pronoun distribution over posi-\ntion. The results of the Zipﬁan distribution analysis\nindicate that neural networks with reasonable com-\nplexity are capable of approaching the correct vo-\ncabulary distribution, and competing favorably with\nthe most vocabulary-optimized n-gram models. We\nfound some interesting phenomena where smooth-\ning, especially with high order n-gram models, ﬂat-\ntened the Zipf distribution. At the very least we see\nthat basic RNNLMs exhibit no real weaknesses next\nto n-gram models, beyond training time.\nOverall, the methods we present here comprise an\napproach to language model analysis that is more in-\ndependent from speciﬁc applications than previous\nreviews of language model performance. By select-\ning structural properties of language that are mea-\nsurable and ideally equally valid on real and sam-\npled language, it is possible to characterize language\nmodels and examine their learning capacities and\npredispositions in generation and ranking. Future\navenues of investigation in line with this paradigm\ncan provide more detailed portraits and serve as\nguidance both in the selection of models for applica-\ntions and for further developments in statistical lan-\nguage modeling.\nAcknowledgments\nWe would like to thank the members of the NLP\ngroup at the University of California, Berkeley for\ntheir contributions to discussions, as well as the two\nanonymous reviewers of this paper for their sugges-\ntions.\n230\nReferences\nStanley F Chen and Joshua Goodman. 1999. An empir-\nical study of smoothing techniques for language mod-\neling. Computer Speech & Language, 13(4):359–393.\nWinthrop Nelson Francis. 1964. A standard sample of\npresent-day english for use with digital computers.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nZipf George Kingsley. 1932. Selective studies and the\nprinciple of relative frequency in language.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-\nrice Santorini. 1993. Building a large annotated cor-\npus of english: The penn treebank. Computational lin-\nguistics, 19(2):313–330.\nTomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In INTER-\nSPEECH, volume 2, page 3.\nTom´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2011. Extensions\nof recurrent neural network language model. In 2011\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 5528–5531.\nIEEE.\nAndreas Stolcke et al. 2002. Srilm-an extensible lan-\nguage modeling toolkit. In INTERSPEECH, volume\n2002, page 2002.\n231",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.819212019443512
    },
    {
      "name": "Language model",
      "score": 0.7872382998466492
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6307355165481567
    },
    {
      "name": "Natural language processing",
      "score": 0.5657755732536316
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.494759738445282
    },
    {
      "name": "Language understanding",
      "score": 0.47453486919403076
    },
    {
      "name": "Cache language model",
      "score": 0.45091044902801514
    },
    {
      "name": "Universal Networking Language",
      "score": 0.2811715602874756
    },
    {
      "name": "Natural language",
      "score": 0.210818350315094
    },
    {
      "name": "Comprehension approach",
      "score": 0.15563997626304626
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ]
}