{
    "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
    "url": "https://openalex.org/W3166441238",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2205106504",
            "name": "Andrew Yates",
            "affiliations": [
                "Max Planck Institute for Informatics"
            ]
        },
        {
            "id": "https://openalex.org/A2304906377",
            "name": "Rodrigo Nogueira",
            "affiliations": [
                "University of Waterloo"
            ]
        },
        {
            "id": "https://openalex.org/A2163619555",
            "name": "Jimmy Lin",
            "affiliations": [
                "University of Waterloo"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3168875417",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2995289474",
        "https://openalex.org/W2962779279",
        "https://openalex.org/W3004654429",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W2909544278",
        "https://openalex.org/W3034212969",
        "https://openalex.org/W2945127593",
        "https://openalex.org/W3092302355",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W2982596739",
        "https://openalex.org/W3134665270",
        "https://openalex.org/W4287645694",
        "https://openalex.org/W2462891382",
        "https://openalex.org/W3105107530",
        "https://openalex.org/W3038572442",
        "https://openalex.org/W3022373106",
        "https://openalex.org/W4300427681",
        "https://openalex.org/W3092952717",
        "https://openalex.org/W4287711528",
        "https://openalex.org/W3100107515",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3174783074",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3044812140",
        "https://openalex.org/W2260194779",
        "https://openalex.org/W2136189984",
        "https://openalex.org/W3099384026",
        "https://openalex.org/W2940927814",
        "https://openalex.org/W3102378333",
        "https://openalex.org/W3064953855",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2982096936",
        "https://openalex.org/W3102286003",
        "https://openalex.org/W3118668786",
        "https://openalex.org/W3012594078",
        "https://openalex.org/W3035183674",
        "https://openalex.org/W3023238803",
        "https://openalex.org/W3034439313",
        "https://openalex.org/W3089973398",
        "https://openalex.org/W2971209824",
        "https://openalex.org/W3021397474",
        "https://openalex.org/W3093955333",
        "https://openalex.org/W2611029872",
        "https://openalex.org/W3104657626",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2897754576",
        "https://openalex.org/W2937036051",
        "https://openalex.org/W4378513414",
        "https://openalex.org/W3006057906",
        "https://openalex.org/W3037128914",
        "https://openalex.org/W3092683697"
    ],
    "abstract": "Andrew Yates, Rodrigo Nogueira, Jimmy Lin. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials. 2021.",
    "full_text": "Proceedings of NAACL-HLT 2021: Tutorials, pages 1–4\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n1\nPretrained Transformers for Text Ranking: BERT and Beyond\nAndrew Yates,1 Rodrigo Nogueira,2 and Jimmy Lin2\n1 Max Planck Institute for Informatics\n2 David R. Cheriton School of Computer Science, University of Waterloo\n1 Overview\nThe goal of text ranking is to generate an ordered\nlist of texts retrieved from a corpus in response to\na query for a particular task. Although the most\ncommon formulation of text ranking is search, in-\nstances of the task can also be found in many text\nprocessing applications. This tutorial provides an\noverview of text ranking with neural network ar-\nchitectures known as transformers, of which BERT\n(Bidirectional Encoder Representations from Trans-\nformers) (Devlin et al., 2019) is the best-known ex-\nample. These models produce high quality results\nacross many domains, tasks, and settings.\nThis tutorial, which is based on the preprint (Lin\net al., 2020a) of a forthcoming book to be published\nby Morgan and & Claypool under the Synthesis\nLectures on Human Language Technologies series,\nprovides an overview of existing work as a single\npoint of entry for practitioners who wish to deploy\ntransformers for text ranking in real-world appli-\ncations and researchers who wish to pursue work\nin this area. We cover a wide range of techniques,\ngrouped into two categories: transformer models\nthat perform reranking in multi-stage ranking ar-\nchitectures and learned dense representations that\nperform ranking directly.\n2 Multi-Stage Ranking Architectures\nThe most straightforward application of transform-\ners to text ranking is to convert the task into a text\nclassiﬁcation problem, and then sort the texts to\nbe ranked based on the probability that each item\nbelongs to the relevant class. The ﬁrst application\nof BERT to text ranking, by Nogueira and Cho\n(2019), used BERT in exactly this manner. Thisrel-\nevance classiﬁcation approach is usually deployed\nin a module that reranks candidate texts from an\ninitial keyword search engine.\nOne key limitation of BERT is its inability\nto handle long input sequences and hence dif-\nﬁculty in ranking texts beyond a certain length\n(e.g., “full-length” documents such as news arti-\ncles). This limitation is addressed by a number of\nmodels (Nogueira and Cho, 2019; Akkalyoncu Yil-\nmaz et al., 2019; Dai and Callan, 2019b; MacA-\nvaney et al., 2019; Wu et al., 2020; Li et al., 2020),\nand a simple retrieve-then-rerank approach can\nbe elaborated into a multi-stage architecture with\nreranker pipelines (Nogueira et al., 2019a; Mat-\nsubara et al., 2020; Soldaini and Moschitti, 2020)\nthat balance effectiveness and efﬁciency. On top\nof multi-stage ranking architectures, researchers\nhave proposed additional innovations, including\nquery expansion (Zheng et al., 2020), document\nexpansion (Nogueira et al., 2019b; Nogueira and\nLin, 2019) and term importance prediction (Dai\nand Callan, 2019a, 2020).\nA natural question that arises is, “What’s be-\nyond BERT?” We describe efforts to build rank-\ning models that are faster (i.e., lower inference la-\ntency), that are better (i.e., higher ranking effective-\nness), or that manifest interesting tradeoffs between\neffectiveness and efﬁciency. These include rank-\ning models that leverage BERT variants (Li et al.,\n2020), exploit knowledge distillation to train more\ncompact student models (Gao et al., 2020a), and\nother transformer architectures, including ground-\nup redesign efforts (Hofstätter et al., 2020b; Mitra\net al., 2020) and adapting pretrained sequence-to-\nsequence models (Nogueira et al., 2020; dos Santos\net al., 2020). These discussions set up a natural tran-\nsition to ranking based on dense representations,\nthe other main category of approaches we cover.\n3 Learned Dense Representations\nArguably, the single biggest beneﬁt brought about\nby modern deep learning techniques to text rank-\ning is the move away from sparse signals, mostly\n2\nlimited to exact matches, to dense representations\nthat are able to capture semantic matches to bet-\nter model relevance. The potential of continuous\ndense representations for natural language analysis\nwas ﬁrst demonstrated nearly a decade ago with\nword embeddings on word analogy tasks (Mikolov\net al., 2013). As soon as researchers tried to build\nrepresentations for any larger spans of text: phrases,\nsentences, paragraphs, and documents, the same\nissues that arise in text ranking come into focus. In\nfact, ranking with dense representations predates\nBERT by many years (Huang et al., 2013; De Boom\net al., 1999; Mitra et al., 2016; Henderson et al.,\n2017; Wu et al., 2018; Zamani et al., 2018).\nIn the context of transformers, the general setup\nof ranking with dense representations involves\nlearning transformer-based encoders that convert\nqueries and texts into dense, ﬁxed-size vectors. In\nthe simplest approach, ranking becomes the prob-\nlem of approximate nearest neighbor (ANN) search\nbased on some simple metric such as cosine sim-\nilarity (Lee et al., 2019; Xiong et al., 2020; Lu\net al., 2020; Reimers and Gurevych, 2019; MacA-\nvaney et al., 2020; Gao et al., 2020b; Karpukhin\net al., 2020; Zhan et al., 2020; Qu et al., 2020;\nHofstätter et al., 2020a; Lin et al., 2020b). How-\never, recognizing that accurate ranking cannot be\ncaptured via simple metrics, researchers have ex-\nplored using more complex machinery to compare\ndense representations (Humeau et al., 2020; Khat-\ntab and Zaharia, 2020). Here, as with multi-stage\nranking architectures, limitations on text length\nand effectiveness–efﬁciency tradeoffs are impor-\ntant considerations. It becomes increasingly difﬁ-\ncult to accurately capture the semantics of longer\ntexts with ﬁxed-sized representations, and increas-\ningly complex comparison architectures increase\nlatency and may necessitate reranking designs.\n4 Looking Ahead\nLearned dense representations complement sparse\n(bag-of-words) term-based representations central\nto keyword search techniques that have domi-\nnated the landscape for more than half a cen-\ntury. Together, hybrid multi-stage approaches (e.g.,\ncombining both ranking and reranking) present a\npromising future direction.\nDespite the excitement in directly ranking with\ndense learned representations, we anticipate that\nreranking transformers will remain important in the\nfuture. For one, results from dense retrieval can\nusually be reranked to achieve even higher effec-\ntiveness. At a high level, there are three current ap-\nproaches: apply existing transformer models with\nminimal modiﬁcations, adapt existing transformer\nmodels, perhaps adding additional architectural el-\nements, and redesign transformer-based architec-\ntures from scratch. Which approach will prove to\nbe most effective? The jury’s still out.\nRelated, in NLP we see that the GPT fam-\nily (Brown et al., 2020) continues to push the fron-\ntier of larger models, more compute, and more\ndata. For text ranking, is the simple answer to build\nbigger models? Probably not, since ranking has\nimportant differences with many traditional NLP\ntasks. But if not, what are the evolving roles of zero-\nshot learning, distant supervision, transfer learning,\ndomain adaptation, data augmentation, and task-\nspeciﬁc ﬁne-tuning? This remains an interesting\nopen research question.\nWhile there are aspects of text ranking with\npretrained transformers that are well understood,\nmany promising directions await further explo-\nration. Looking ahead, we anticipate many more\nexciting developments!\n5 Presenter Bios\nAndrew Yatesis a Senior Researcher at the Max\nPlanck Institute for Informatics, where he heads\na research group working in areas of information\nretrieval and natural language processing. Yates re-\nceived his Ph.D. in Computer Science from George-\ntown University in 2016.\nRodrigo Nogueirais a post-doctoral researcher at\nthe University of Waterloo, an adjunct professor at\nUNICAMP, Brazil, and a senior research scientist\nat NeuralMind, Brazil. Nogueira received his Ph.D.\nfrom New York University in 2019.\nJimmy Linholds the David R. Cheriton Chair in\nthe David R. Cheriton School of Computer Science\nat the University of Waterloo. Prior to 2015, he was\na faculty at the University of Maryland, College\nPark. Lin received his Ph.D. in Electrical Engineer-\ning and Computer Science from the Massachusetts\nInstitute of Technology in 2004.\nAcknowledgments\nThis work was supported in part by the Canada\nFirst Research Excellence Fund and the Natural Sci-\nences and Engineering Research Council (NSERC)\nof Canada.\n3\nReferences\nZeynep Akkalyoncu Yilmaz, Wei Yang, Haotian\nZhang, and Jimmy Lin. 2019. Cross-domain mod-\neling of sentence-level evidence for document re-\ntrieval. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3490–3496, Hong Kong, China.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. arXiv:2005.14165.\nZhuyun Dai and Jamie Callan. 2019a. Context-aware\nsentence/passage term importance estimation for\nﬁrst stage retrieval. arXiv:1910.10687.\nZhuyun Dai and Jamie Callan. 2019b. Deeper text un-\nderstanding for IR with contextual neural language\nmodeling. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval (SIGIR 2019) , pages\n985–988, Paris, France.\nZhuyun Dai and Jamie Callan. 2020. Context-aware\ndocument term weighting for ad-hoc search. In Pro-\nceedings of The Web Conference 2020 , WWW ’20,\npage 1897–1907.\nCedric De Boom, Steven Van Canneyt, Thomas De-\nmeester, and Bart Dhoedt. 1999. Representation\nlearning for very short texts using weighted word\nembedding aggregation. Pattern Recognition Let-\nters, 80(C):150–156.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota.\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2020a. Un-\nderstanding bert rankers under distillation. In Pro-\nceedings of the 2020 ACM SIGIR International Con-\nference on Theory of Information Retrieval (ICTIR\n2020), pages 149–152.\nLuyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan.\n2020b. Complementing lexical retrieval with seman-\ntic residual embedding. arXiv:2004.13969.\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun\nhsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Efﬁ-\ncient natural language response suggestion for Smart\nReply. arXiv:1705.00652.\nSebastian Hofstätter, Sophia Althammer, Michael\nSchröder, Mete Sertkan, and Allan Hanbury.\n2020a. Improving efﬁcient neural ranking mod-\nels with cross-architecture knowledge distillation.\narXiv:2010.02666.\nSebastian Hofstätter, Markus Zlabinger, and Allan\nHanbury. 2020b. Interpretable & time-budget-\nconstrained contextualization for re-ranking. In Pro-\nceedings of the 24th European Conference on Arti-\nﬁcial Intelligence (ECAI 2020) , Santiago de Com-\npostela, Spain.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In Proceedings of 22nd Interna-\ntional Conference on Information and Knowledge\nManagement (CIKM 2013) , pages 2333–2338, San\nFrancisco, California.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accu-\nrate multi-sentence scoring. In Proceedings of the\n8th International Conference on Learning Represen-\ntations (ICLR 2020).\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781.\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Ef-\nﬁcient and effective passage search via contextual-\nized late interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval\n(SIGIR 2020), pages 39–48.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6086–6096, Florence,\nItaly.\nCanjia Li, Andrew Yates, Sean MacAvaney, Ben\nHe, and Yingfei Sun. 2020. PARADE: Passage\nrepresentation aggregation for document reranking.\narXiv:2008.09093.\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates.\n2020a. Pretrained transformers for text ranking:\nBERT and beyond. arXiv:2010.06467.\n4\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n2020b. Distilling dense representations for ranking\nusing tightly-coupled teachers. arXiv:2010.11386.\nWenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twin-\nBERT: Distilling knowledge to twin-structured bert\nmodels for efﬁcient retrieval. arXiv:2002.06275.\nSean MacAvaney, Franco Maria Nardini, Raffaele\nPerego, Nicola Tonellotto, Nazli Goharian, and\nOphir Frieder. 2020. Expansion via prediction of\nimportance with contextualization. In Proceedings\nof the 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval\n(SIGIR 2020), page 1573–1576.\nSean MacAvaney, Andrew Yates, Arman Cohan, and\nNazli Goharian. 2019. CEDR: Contextualized em-\nbeddings for document ranking. In Proceedings of\nthe 42nd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval\n(SIGIR 2019), pages 1101–1104, Paris, France.\nYoshitomo Matsubara, Thuy Vu, and Alessandro Mos-\nchitti. 2020. Reranking for efﬁcient transformer-\nbased answer selection. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR\n2020), page 1577–1580.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26 (NIPS 2013) , pages 3111–3119, Lake\nTahoe, California.\nBhaskar Mitra, Sebastian Hofstatter, Hamed Zamani,\nand Nick Craswell. 2020. Conformer-kernel with\nquery term independence for document retrieval.\narXiv:2007.10434.\nBhaskar Mitra, Eric Nalisnick, Nick Craswell, and\nRich Caruana. 2016. A dual embedding space\nmodel for document ranking. arXiv:1602.01137v1.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with BERT. arXiv:1901.04085.\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and\nJimmy Lin. 2020. Document ranking with a pre-\ntrained sequence-to-sequence model. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 708–718.\nRodrigo Nogueira and Jimmy Lin. 2019. From\ndoc2query to docTTTTTquery.\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\nJimmy Lin. 2019a. Multi-stage document ranking\nwith BERT. In arXiv:1910.14424.\nRodrigo Nogueira, Wei Yang, Jimmy Lin, and\nKyunghyun Cho. 2019b. Document expansion by\nquery prediction. In arXiv:1904.08375.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu,\nRuiyang Ren, Xin Zhao, Daxiang Dong, Hua\nWu, and Haifeng Wang. 2020. RocketQA:\nAn optimized training approach to dense pas-\nsage retrieval for open-domain question answering.\narXiv:2010.08191.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China.\nCicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nal-\nlapati, Zhiheng Huang, and Bing Xiang. 2020. Be-\nyond [CLS] through ranking by generation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 1722–1727.\nLuca Soldaini and Alessandro Moschitti. 2020. The\ncascade transformer: an application for efﬁcient an-\nswer sentence selection. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 5697–5708.\nLedell Wu, Adam Fisch, Sumit Chopra, Keith\nAdams, Antoine Bordes, and Jason Weston. 2018.\nStarSpace: Embed all the things! In Proceedings\nof the Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence (AAAI 2018).\nZhijing Wu, Jiaxin Mao, Yiqun Liu, Jingtao Zhan,\nYukun Zheng, Min Zhang, and Shaoping Ma. 2020.\nLeveraging passage-level cumulative gain for docu-\nment ranking. In Proceedings of The Web Confer-\nence 2020, WWW ’20, page 2421–2431.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020. Approximate nearest neighbor neg-\native contrastive learning for dense text retrieval.\narXiv:2007.00808.\nHamed Zamani, Mostafa Dehghani, W. Bruce Croft,\nErik Learned-Miller, and Jaap Kamps. 2018. From\nneural re-ranking to neural ranking: Learning a\nsparse representation for inverted indexing. In Pro-\nceedings of the 27th ACM International Conference\non Information and Knowledge Management (CIKM\n2018), pages 497–506, Torino, Italy.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang,\nand Shaoping Ma. 2020. RepBERT: Contex-\ntualized text embeddings for ﬁrst-stage retrieval.\narXiv:2006.15498.\nZhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun,\nand Andrew Yates. 2020. BERT-QE: Contextual-\nized Query Expansion for Document Re-ranking. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4718–4728."
}