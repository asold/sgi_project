{
    "title": "Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters",
    "url": "https://openalex.org/W4389519841",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5090257103",
            "name": "Manikanta Loya",
            "affiliations": [
                "University of California, Irvine"
            ]
        },
        {
            "id": "https://openalex.org/A5030568818",
            "name": "Divya Sinha",
            "affiliations": [
                "University of California, Irvine"
            ]
        },
        {
            "id": "https://openalex.org/A5050104206",
            "name": "Richard Futrell",
            "affiliations": [
                "University of California, Irvine"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W41554520",
        "https://openalex.org/W4379539325",
        "https://openalex.org/W4385573636",
        "https://openalex.org/W4376652803",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W4324098937",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4283263983",
        "https://openalex.org/W4378532197",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2921890305",
        "https://openalex.org/W4385681252",
        "https://openalex.org/W4379933644",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4362597819",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4363624465",
        "https://openalex.org/W2054698589",
        "https://openalex.org/W4385573216",
        "https://openalex.org/W4385572162",
        "https://openalex.org/W4377136808"
    ],
    "abstract": "The advancement of Large Language Models (LLMs) has led to their widespread\\nuse across a broad spectrum of tasks including decision making. Prior studies\\nhave compared the decision making abilities of LLMs with those of humans from a\\npsychological perspective. However, these studies have not always properly\\naccounted for the sensitivity of LLMs' behavior to hyperparameters and\\nvariations in the prompt. In this study, we examine LLMs' performance on the\\nHorizon decision making task studied by Binz and Schulz (2023) analyzing how\\nLLMs respond to variations in prompts and hyperparameters. By experimenting on\\nthree OpenAI language models possessing different capabilities, we observe that\\nthe decision making abilities fluctuate based on the input prompts and\\ntemperature settings. Contrary to previous findings language models display a\\nhuman-like exploration exploitation tradeoff after simple adjustments to the\\nprompt.\\n",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3711–3716\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nExploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights\nfrom Prompt Variation and Hyperparameters\nManikanta Loya∗\nmanikanl@uci.edu\nDivya Anand Sinha∗\ndasinha@uci.edu\nUniversity of California, Irvine\nRichard Futrell\nrfutrell@uci.edu\nAbstract\nThe advancement of Large Language Mod-\nels (LLMs) has led to their widespread use\nacross a broad spectrum of tasks, including\ndecision-making. Prior studies have compared\nthe decision-making abilities of LLMs with\nthose of humans from a psychological perspec-\ntive. However, these studies have not always\nproperly accounted for the sensitivity of LLMs’\nbehavior to hyperparameters and variations in\nthe prompt. In this study, we examine LLMs’\nperformance on the Horizon decision-making\ntask studied by Binz and Schulz (2023), analyz-\ning how LLMs respond to variations in prompts\nand hyperparameters. By experimenting on\nthree OpenAI language models possessing dif-\nferent capabilities, we observe that the decision-\nmaking abilities fluctuate based on the input\nprompts and temperature settings. Contrary to\nprevious findings, language models display a\nhuman-like exploration–exploitation tradeoff\nafter simple adjustments to the prompt. 1\n1 Introduction\nThe recent success of large language models\n(LLMs) at a variety of tasks has led to curios-\nity about their cognitive abilities and characteris-\ntics. As LLMs are increasingly integrated in daily\nlife both as conversation partners and economic\ndecision-makers (Munir et al., 2023; Chaturvedi\net al., 2023; Yang et al., 2023), such studies are\nnecessary for understanding the limits and char-\nacteristics of such agents. An understanding of\nLLMs at a psychological level may also provide\nstrategies for improved prompting and training. To\nthis end, a number of researchers have recently\nadopted methods from cognitive psychology and\nbehavioral economics to evaluate language models\nin the same way that humans have been evaluated\n(e.g. Linzen et al., 2016; Miotto et al., 2022; Phelps\nand Russell, 2023, among many others).\n*Equal Contribution\n1Code is available at the following github link.\nHowever, such work has not always paid due at-\ntention to the fact that LLM responses can be highly\nvariable and sensitive to the details of the prompt\nused and to hyperparameters such as temperature.\nLimited interactions with LLMs—such as interac-\ntions using only one prompt—can be misleading\n(Bowman, 2023). In this work, we follow up on\nthe behavioral experiments conducted by Binz and\nSchulz (2023), who studied LLMs’ decision mak-\ning using analogues of a number of well-known\nhuman experimental paradigms, finding strong di-\nvergences from human behavior. However, the\nexperiments in the previous work used only one\nprompt per task, and did not study the effects of\nhyperparameters. We adopt the same task as the\nprevious work, but systematically vary prompts and\ntemperature.\nOur aims are both substantive—we seek to find\nwhether, with basic changes to the prompt, mod-\nels show human-like behavior in these decision\nmaking tasks—and methodological: we wish to\nemphasize that psychological LLM research must\nconsider variability as a function of prompt and\nhyperparameters.\n2 Background and Related Work\nAs mechanistic understanding and control of LLMs\nremains complex, researchers have increasingly\nadopted methods from human behavioral sciences\nfor characterizing LLMs’ behavior: in the same\nway that the human brain is largely a black box that\nmust be probed using experimental methods and\nconstructs, LLMs may be studied in the same way.\nIn addition to studies that have used the meth-\nods of cognitive psychology to understand LLMs’\nreasoning and grammatical abilities (e.g., Linzen\net al., 2016; Futrell et al., 2019; Cai et al., 2023), re-\nsearchers have increasingly adapted methods from\npsychometrics (Miotto et al., 2022; Bodroza et al.,\n2023; Abramski et al., 2023), which seek to charac-\nterize LLMs in terms of personality variables such\n3711\nFigure 1: Original Horizon 6 task prompt (Binz and\nSchulz, 2023).\nas agreeableness and conscientiousness, and meth-\nods from behavioral economics (Cartwright, 2018;\nPhelps and Russell, 2023; Horton, 2023), which\ncharacterize LLMs’ decision-making in terms of\npreferences for risk and reward.\nPrior research on prompting techniques (Wei\net al., 2022; Wang et al., 2023) has shown that sub-\ntle modifications in input prompts can lead to varied\noutcomes in reasoning tasks (Cobbe et al., 2021).\nSrivastava et al. (2022) revealed that Large Lan-\nguage Models (LLMs) are notably susceptible to\nthe precise wording of natural language questions,\nespecially when presented in a multiple-choice set-\nting. In a recent study, Ouyang et al. (2023) empha-\nsized the influence of temperature adjustments on\nLLM’s performance in code generation tasks. Un-\nlike previous studies, our research delves into the\nsensitivity of LLMs concerning economic decision-\nmaking abilities.\nOur work is a focused followup on Binz and\nSchulz (2023), investigating the sensitivity of one\nof their results to changes in prompt and hyper-\nparameters. Binz and Schulz (2023) evaluated\non decision-making, information search, delibera-\ntion, and causal reasoning in text-davinci-002\n(Brown et al., 2020) by presenting it with prompts\nsuch as the one shown in Figure 1. We follow up on\nthe tasks from the information search area, instanti-\nated in the Horizon task, described in the following\nsection. In this task, humans show a characteristic\ntrade-off of exploration and exploitation (Wilson\net al., 2014), favoring exploration in early trials and\nexploitation later, whereas Binz and Schulz (2023)\nfind that LLMs do not.\nThe results of Binz and Schulz (2023), however,\nare based on single prompt and setting, limiting\nthe generality of their results. Furthermore, ob-\nserving the Horizon task prompt (and the others\nused throughout the paper), it does not follow what\nare now regarded as best practices for such tasks,\nfor example the use of Chain-of-Thought (CoT)\nprompting (Wei et al., 2022)—the original prompt\nforces the LLM to choose a machine in the next\ntoken generated, without deliberation. Below, we\ninvestigate the behavior of LLMs on this task under\nsystematic variations of temperature and prompt.\n3 Horizon Task Experiments\nThe Horizon Task as shown in Binz and Schulz\n(2023) is a special case of the Multi-Armed bandit\n(MAB) setting. MAB problems (Sutton and Barto,\n2018, Ch. 2) are one of the common problems in\nthe area of Reinforcement Learning. This game\ninvolves an agent interacting with a slot machine\npossessing k arms. Each arm the agent pulls has a\nreward associated with it defined by an underlying\nprobability distribution. This game is played over\nmultiple episodes with the goal of maximizing the\naccrued rewards.\nOne approach involves persistently selecting the\narm that has delivered the maximum amount of re-\nwards in the past. An alternative strategy involves\nthorough exploration of all arms to discern their\nrespective underlying probability distributions, fol-\nlowed by the selection of the arm with the highest\npotential for reward. While this is feasible, every\nturn spent in discerning the underlying distribution,\ndiverts from the primary goal of reward maximiza-\ntion. The former and latter strategies are known as\nexploitation and exploration respectively, and the\nexploration–exploitation dilemma is a fundamen-\ntal concept in decision-making that arises in many\ndomains.\nTo explore the extent to which humans use these\nstrategies, Wilson et al. (2014) reports experiments\nwhere participants were asked to play the Horizon\nTask. This task consists of a set of two-armed ban-\ndit problems, where participants are presented with\ntwo options, each associated with noisy rewards.\nThe task comprises either five or ten trials, and\nin each trial, participants must select one option,\nreceiving corresponding reward feedback. In the\ninitial four trials of the task, participants have only\none option and are provided with the corresponding\nreward feedback. These forced-choice trials create\ntwo distinct information conditions: “unequal in-\nformation” and “equal information.” In the unequal\ninformation condition, one option is played three\ntimes, while the other option is played only once.\nIn the equal information condition, both options are\n3712\n1 2 3 4 5 6\nTrials\n1\n2\n3\n4\n5\n6\n7Mean regret\nrandom\n(a) text-davinci-002\n1 2 3 4 5 6\nTrials\n1\n2\n3\n4\n5\n6\n7Mean regret\nrandom (b) text-davinci-003\n1 2 3 4 5 6\nTrials\n1\n2\n3\n4\n5\n6\n7Mean regret\nrandom Horizon 1\nHorizon 6\nHumans\nt = 0.0\nt = 0.5\nt = 1.0 (c) gpt-3.5-turbo\nFigure 2: Mean regret obtained in the Horizon (multi-trial multi-armed bandit) task by humans and LLMs with\nvarying temperature, using the prompt from . The solid black line indicates human performance; others are LLMs.\nError bars show the standard error of the mean.\nQ: Which machine do you choose? At the end, summarize your output as “So, \nI chose Machine [machine name]”. \nA: Let’s think step by step\nCoT\nQ: Which machine do you choose? \nA: Thinking step by step I chose Machine\nQuasi-CoT\nThe following hints will help you make good decision: \n1. In each round you choose either Machine F or Machine J and receive reward \nfrom that machine. \n2. You must choose the machine with highest average of delivered dollars. \n3. Average of 1, 2, 3 is calculated first by computing sum of all observations \nwhich is 1 + 2 + 3 = 6 and then dividing it by number of observations which is \n6/3 = 2. \n4. Out of x and y, if x - y is positive integer then x is higher else y is higher. \nYour goal is to maximize the sum of received dollars within one additional round. \nQ: Which machine do you choose? \nA: Let’s think step by step\nThe following hints will help you make good \ndecision: \n1. In each round you choose either Machine F or \nMachine J and receive reward from that machine. \n2. If you have more than 4 rounds left, you choose \nthe machine with lesser number of past \nobservations. \nYour goal is to maximize the sum of received dollars \nwithin one additional round. \nQ: Which machine do you choose? \nA: Let’s think step by step\nCoT-exploit CoT-explore\nFigure 3: Modifications in prompt for the Horizon task. Horizon 1 prompt is shown. In case of CoT, CoT-Exploit &\nCoT-Explore we explicit ask the model to summarize its choice at the end by appending the entire prompt with\n\"Answer the following question and summarize your choice at the end as ‘Machine:[machine_name]’.\" at the\nbeginning.\nplayed twice. The five-trial setting is denoted Hori-\nzon 1, indicating that participants make decisions\nonly once, while the ten-trial setting is referred to\nas Horizon 6, as participants make decisions over\nsix rounds.\nBinz and Schulz (2023) applied this experimen-\ntal design to language models using the prompt in\nFigure 1, and we follow their experimental setup\nexactly except for variations to the prompt and\nhyperparameters. The performance of LLMs is\nassessed by measuring the mean regret across mul-\ntiple runs. The regret is defined as the difference\nbetween the optimal reward, which corresponds to\nthe machine with the higher reward, and the actual\nreward obtained from the selection process. Hu-\nman behavior favors exploitation in Horizon 1, but\na gradual shift from exploration to exploitation in\nHorizon 6.\n3.1 Varying Temperature\nThe impact of various temperature settings\n(0.0,0.5,1.0) on all three OpenAI models 2\ntested is illustrated in Figure 2. It is clear that\nthe behavior of each model, as indicated by the\nmean regret line, differs according to the temper-\nature. For Horizon 1, the lowest regret is ob-\ntained for temperature zero across all three models.\nFurther, unlike text-davinci-002 and as shown\nin Binz and Schulz (2023), the mean regret is\n2https://platform.openai.com/docs/models/overview\n3713\n1 2 3 4 5 6\nTrials\n0\n1\n2\n3\n4\n5\n6\n7Mean regret\nrandom\n(a) text-davinci-0023\n1 2 3 4 5 6\nTrials\n0\n1\n2\n3\n4\n5\n6\n7Mean regret\nrandom (b) text-davinci-003\n1 2 3 4 5 6\nTrials\n0\n1\n2\n3\n4\n5\n6\n7Mean regret\nrandom Horizon 1\nHorizon 6\nHumans\nWithout CoT\nQuasi CoT\nCoT (c) gpt-3.5-turbo\nFigure 4: Mean regret obtained by humans and LLMs on the Horizon task, varying prompt. ‘Quasi-CoT’ means a\nprompt of the form ‘Thinking step-by-step, I choose Machine . . . ’ which does not enable true chain-of-thought\nreasoning. The temperatures for GPT-2, GPT-3, and GPT-3.5 are 1.0, 0.5, and 1.0 respectively. These temperatures\nshow the greatest learning effect (negative slope) in the Horizon 6 task.\nlower than humans for both text-davinci-003\nand gpt-3.5-turbo. In the case of Horizon 6,\nthere is a notable rise in the inital mean regret,\nsuggesting that higher temperatures result in subop-\ntimal decision-making. However, increasing tem-\nperature demonstrates a more pronounced learning\neffect, as evidenced by a greater negative slope.\n3.2 Varying Prompt\nTo encourage deliberation during decision-making,\nwe incorporate variations in the input prompt.\nSpecifically, we explore two different variants of\nthe Chain of Thought (CoT) prompting technique\n(Wei et al., 2022)—CoT and Quasi-CoT. In Fig-\nure 3, we illustrate the modifications made to the\noriginal prompt. The variant referred to as Quasi-\nCoT utilizes the prompt “Thinking step by step\nI choose Machine”, which forces the machine to\nmake a decision before fully processing its reason-\ning. On the other hand, the CoT variant makes a\ndecision only after fully processing its reasoning.\nThe Quasi-CoT condition allows us to disentangle\nthe effects of true step-by-step reasoning in CoT\nfrom the effects of prompting the LLM to think\ncarefully.\nThe alteration in the behavior of LLMs due\nto changes in the input prompt is depicted in\nFigure 4. Across all models, CoT demonstrates\nlower-regret compared to both Quasi-CoT and\nthe original prompt, whereas Quasi-CoT performs\nworse than original prompt. Furthermore, even in\n3Experiments with text-davinci-002 using CoT\nprompt failed due to its inability to summarize its choice\nat the end.\ntext-davinci-002, we find that altered prompts\nyield the human-like negative slope, indicating an\nexploration–exploitation trade-off.\n3.3 CoT Prompting with Hints\nTo overcome the identified limitations in LLMs,\nsuch as their inaccuracies in computing averages\n(Razeghi et al., 2022; Imani et al., 2023) and sub-\noptimal exploration capabilities, we introduce ad-\nditional hints within the input prompt to guide\nthe decision-making process. Specifically, we de-\nsigned two prompts, namely CoT-Exploit and CoT-\nExplore, which aim to facilitate explicit exploita-\ntion and exploration. The hints associated with\nthese prompts are shown in Figure 3.\nIn the CoT-Exploit prompt, we instruct the\nmodel to base its decisions on the average of\nobserved experiences and equip it with the required\nmathematical calculations to make a decision.\nLikewise, in the CoT-Explore approach, we\nexplicitly direct the model to select a machine with\nlower frequency among the observed experiences.\nThe performance of gpt-3.5-turbo, using var-\nious CoT prompting variants, is compared in Fig-\nure 5. As anticipated, CoT-Exploit outperforms\nCoT-Explore, displaying a consistent decrease in\nslope. However, CoT-Explore performs signifi-\ncantly worse than random decision-making. CoT-\nExplore primary concentrates on getting more in-\nformation about each machine rather than overall\nrewards.\n3714\n1 2 3 4 5 6\nTrials\n0\n2\n4\n6\n8\n10Mean regret\nrandom\nHorizon 1\nHorizon 6\nHumans\nQuasi CoT\nCoT\nCoT-Exploit\nCoT-Explore\nFigure 5: gpt-3.5-turbo’s behavior under different\nvariants of CoT prompts at temperature 1.0.\n3.4 Discussion\nThrough our experiments, we have discovered that\nthe decision-making capabilities of LLMs are in-\nfluenced by both the prompts used and the tem-\nperature settings, more so by the choice of prompt\nrather than the temperature. This highlights the\nimportance of varying prompts to elicit the desired\nbehavior from LLMs during decision-making tasks,\nand that studies which have used only one kind of\nprompt are potentially misleading.\nIntriguingly, we observed that the model\ngpt-3.5-turbo with the Quasi-CoT prompt (Fig-\nure 5) exhibits the closest resemblance to human\nbehavior. This prompt alerts the model to the need\nfor reasoning, but does not give it the space to actu-\nally perform any reasoning. The similarity of the\nQuasi-CoT result to humans suggests that humans\nmay also struggle to fully process the associated\ninformation and reasoning.\nFurthermore, by providing hints to guide the\ndecision-making process, we have observed that su-\nperhuman performance can be achieved, as demon-\nstrated by the CoT-Exploit variant (Figure 5). This\nresult suggests that language model behavior in\nthese tasks is potentially highly controllable.\n4 Conclusion\nWe have demonstrated that the psychological be-\nhavior of LLMs, as previously explored by Binz\nand Schulz (2023), is highly sensitive to the way\nthese LLMs are queried. The non-human-like be-\nhavior observed by Binz and Schulz (2023) van-\nishes under simple variations of prompt, and super-\nhuman performance in terms of minimizing regret\nis easily achievable. Going forward, we urge care-\nful consideration in the LLM psychology literature\nof the fact that model behavior can diverge under\ndifferent settings.\n5 Limitations\nWe have presented a focused extension of one of\nthe studies from Binz and Schulz (2023), demon-\nstrating sensitivity to prompt and hyperparameters\nwhich was overlooked in the previous work. How-\never, our work is limited in that (1) we have only\nexamined one of the tasks from Binz and Schulz\n(2023), (2) we have only presented a few variations\nof temperature and prompt, and (3) we have only\nexperimented with some of the models available to\nus as of June 2023, selecting high-profile closed-\nsource models over open-source models. Never-\ntheless, we believe that our overarching point that\nLLM psychology needs to take into account hy-\nperparameters, prompts, and variability remains\nvalid.\nEthics Statement\nThis work involves psychological studies of LLMs\nin economic decision making contexts. If LLMs\nare really deployed as economic decision makers,\nthen ethical issues could result from biases and\nlimitations of the models. We urge caution in such\napplications.\nReferences\nKatherine Abramski, Salvatore Citraro, Luigi Lombardi,\nGiulio Rossetti, and Massimo Stella. 2023. Cognitive\nnetwork science reveals bias in gpt-3, chatgpt, and\ngpt-4 mirroring math anxiety in high-school students.\narXiv preprint arXiv:2305.18320.\nMarcel Binz and Eric Schulz. 2023. Using cognitive\npsychology to understand gpt-3. Proceedings of the\nNational Academy of Sciences, 120(6):e2218523120.\nBojana Bodroza, Bojana M Dinic, and Ljubisa Bojic.\n2023. Personality testing of gpt-3: Limited temporal\nreliability, but highlighted social desirability of gpt-\n3’s personality instruments results. arXiv preprint\narXiv:2306.04308.\nSamuel R Bowman. 2023. Eight things to know\nabout large language models. arXiv preprint\narXiv:2304.00612.\n3715\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nZhenguang G Cai, David A Haslett, Xufeng Duan,\nShuqi Wang, and Martin J Pickering. 2023. Does\nChatGPT resemble humans in language use? arXiv\npreprint arXiv:2303.08014.\nEdward Cartwright. 2018. Behavioral Economics.\nRoutledge.\nRijul Chaturvedi, Sanjeev Verma, Ronnie Das, and Yo-\ngesh K Dwivedi. 2023. Social companionship with\nartificial intelligence: Recent trends and future av-\nenues. Technological Forecasting and Social Change,\n193:122634.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic subjects:\nRepresentations of syntactic state. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 32–42, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJohn J Horton. 2023. Large language models as sim-\nulated economic agents: What can we learn from\nhomo silicus? arXiv preprint arXiv:2301.07543.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–\n42, Toronto, Canada. Association for Computational\nLinguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nMarilù Miotto, Nicola Rossberg, and Bennett Klein-\nberg. 2022. Who is gpt-3? an exploration of per-\nsonality, values and demographics. arXiv preprint\narXiv:2209.14338.\nIqbal Munir et al. 2023. Artificial intelligence chatgpt\nin medicine. can it be the friend you are looking for?\nJournal of Bangladesh Medical Association of North\nAmerica (BMANA) BMANA Journal, pages 01–04.\nShuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng\nWang. 2023. Llm is like a box of chocolates: the non-\ndeterminism of chatgpt in code generation.\nSteve Phelps and Yvan I Russell. 2023. Investigating\nemergent goal-like behaviour in large language mod-\nels using experimental economics. arXiv preprint\narXiv:2305.07970.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot numerical reasoning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022, pages 840–854, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\nment Learning: An Introduction. MIT press.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2023. Self-consistency improves chain\nof thought reasoning in language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nRobert C Wilson, Andra Geana, John M White, Elliot A\nLudvig, and Jonathan D Cohen. 2014. Humans use\ndirected and random exploration to solve the explore–\nexploit dilemma. Journal of Experimental Psychol-\nogy: General, 143(6):2074.\nHui Yang, Sifu Yue, and Yunzhong He. 2023. Auto-gpt\nfor online decision making: Benchmarks and addi-\ntional opinions. arXiv preprint arXiv:2306.02224.\n3716"
}