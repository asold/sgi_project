{
  "title": "Benchmarking for Public Health Surveillance tasks on Social Media with a Domain-Specific Pretrained Language Model",
  "url": "https://openalex.org/W4285168837",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2963399842",
      "name": "Usman Naseem",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2213446847",
      "name": "Byoung Chan Lee",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2155621213",
      "name": "Matloob Khushi",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2098955200",
      "name": "Jinman Kim",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2123734628",
      "name": "Adam Dunn",
      "affiliations": [
        "University of Sydney"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2953902152",
    "https://openalex.org/W3200855856",
    "https://openalex.org/W2999823319",
    "https://openalex.org/W3162081707",
    "https://openalex.org/W4301100562",
    "https://openalex.org/W2987392802",
    "https://openalex.org/W3116085234",
    "https://openalex.org/W4324392584",
    "https://openalex.org/W3090413818",
    "https://openalex.org/W2067495470",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4287813888",
    "https://openalex.org/W2113631586",
    "https://openalex.org/W2251420382",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4224308224",
    "https://openalex.org/W3058043232",
    "https://openalex.org/W3209409148",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2720850745",
    "https://openalex.org/W3081791287",
    "https://openalex.org/W1981148113",
    "https://openalex.org/W2953413710",
    "https://openalex.org/W3012932209",
    "https://openalex.org/W2891270000",
    "https://openalex.org/W2970205254",
    "https://openalex.org/W3199422761",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4224326626",
    "https://openalex.org/W2511501696",
    "https://openalex.org/W3101757358",
    "https://openalex.org/W4211258374",
    "https://openalex.org/W2964054038",
    "https://openalex.org/W3175170601",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W3097301690",
    "https://openalex.org/W1961375973",
    "https://openalex.org/W3179963059",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3127056209",
    "https://openalex.org/W3098649723"
  ],
  "abstract": "A user-generated text on social media enables health workers to keep track of information, identify possible outbreaks, forecast disease trends, monitor emergency cases, and ascertain disease awareness and response to official health correspondence. This exchange of health information on social media has been regarded as an attempt to enhance public health surveillance (PHS). Despite its potential, the technology is still in its early stages and is not ready for widespread application. Advancements in pretrained language models (PLMs) have facilitated the development of several domain-specific PLMs and a variety of downstream applications. However, there are no PLMs for social media tasks involving PHS. We present and release PHS-BERT, a transformer-based PLM, to identify tasks related to public health surveillance on social media. We compared and benchmarked the performance of PHS-BERT on 25 datasets from different social medial platforms related to 7 different PHS tasks. Compared with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT achieved state-of-the-art performance on all 25 tested datasets, showing that our PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT available, we aim to facilitate the community to reduce the computational cost and introduce new baselines for future works across various PHS-related tasks.",
  "full_text": "Proceedings of NLP Power! The First Workshop on Efﬁcient Benchmarking in NLP, pages 22 - 31\nMay 26, 2022 ©2022 Association for Computational Linguistics\nBenchmarking for Public Health Surveillance tasks on Social Media with a\nDomain-Specific Pretrained Language Model\nUsman Naseem1, Byoung Chan Lee2, Matloob Khushi1, Jinman Kim1, Adam G. Dunn3\n1School of Computer Science, University of Sydney, Australia\n2School of Medicine, University of Sydney, Australia\n3School of Medical Sciences, University of Sydney, Australia\nblee9781@uni.sydney.edu.au\n{usman.naseem,matloob.khushi,jinman.kim,adam.dunn}@sydney.edu.au\nAbstract\nA user-generated text on social media enables\nhealth workers to keep track of information,\nidentify possible outbreaks, forecast disease\ntrends, monitor emergency cases, and ascer-\ntain disease awareness and response to official\nhealth correspondence. This exchange of health\ninformation on social media has been regarded\nas an attempt to enhance public health surveil-\nlance (PHS). Despite its potential, the technol-\nogy is still in its early stages and is not ready\nfor widespread application. Advancements in\npretrained language models (PLMs) have fa-\ncilitated the development of several domain-\nspecific PLMs and a variety of downstream ap-\nplications. However, there are no PLMs for so-\ncial media tasks involving PHS. We present and\nrelease PHS-BERT, a transformer-based PLM,\nto identify tasks related to public health surveil-\nlance on social media. We compared and bench-\nmarked the performance of PHS-BERT on 25\ndatasets from different social medial platforms\nrelated to 7 different PHS tasks. Compared\nwith existing PLMs that are mainly evaluated\non limited tasks, PHS-BERT achieved state-of-\nthe-art performance on all 25 tested datasets,\nshowing that our PLM is robust and general-\nizable in the common PHS tasks. By making\nPHS-BERT available1, we aim to facilitate the\ncommunity to reduce the computational cost\nand introduce new baselines for future works\nacross various PHS-related tasks.\n1 Introduction\nPublic health surveillance (PHS) is defined by the\nWorld Health Organization2 as the ongoing, sys-\ntematic collection, assessment, and understanding\nof health-related required information for the plan-\nning, implementation, and assessment of health-\ncare (Aiello et al., 2020). PHS aims to design and\n1https://huggingface.co/publichealthsurveillance/PHS-\nBERT\n2https://www.euro.who.int/en/health-topics/Health-\nsystems/public-health-services\nassist interventions; it acts as a primary warning\nsystem in health emergencies (epidemics, i.e., acute\nevents), it reports and records public health inter-\nventions (i.e., monitoring health), and it observes\nand explains the epidemiology of health issues, al-\nlowing for the prioritization of necessary details for\nhealth policy formulation (i.e., targeting chronic\nevents). Traditional PHS systems are often lim-\nited by the time required to collect data, restricting\nthe quick or even instantaneous identification of\noutbreaks (Hope et al., 2006).\nSocial media is growingly being used for pub-\nlic health purposes and can disseminate disease\nrisks and interventions and promote wellness and\nhealthcare policy. Social media data provides an\nabundant source of timely data that can be used\nfor various public health applications, including\nsurveillance, sentiment analysis, health commu-\nnication, and analyzing the history of a disease,\ninjury, or promote health. Systematic reviews of\nstudies that examine personal health experiences\nshared online reveal the breadth of application do-\nmains, which include infectious diseases and out-\nbreaks (Charles-Smith et al., 2015), illicit drug\nuse (Kazemi et al., 2017), and pharmacovigilance\nsupport (Golder et al., 2015). These applied health\nstudies are motivated by their potential in support-\ning PHS, augmenting adverse event reporting, and\nas the basis of public health interventions (Dunn\net al., 2018).\nThe use of deep learning in natural language\nprocessing (NLP) has advanced the development\nof pretrained language models (PLMs) that can\nbe used for a wide range of tasks in PHS. How-\never, directly applying the state-of-the-art (SOTA)\nPLMs such as Bidirectional Encoder Representa-\ntions from Transformers (BERT) (Devlin et al.,\n2019), and its variants (Liu et al., 2019; Lan et al.,\n2019; Sanh et al., 2019; Naseem et al., 2021c)\nthat are trained on general domain corpus (e.g.,\nBookcorpus, Wikipedia, etc.) may yield poor per-\n22\nformances on domain-specific tasks. To address\nthis limitation, several domain-specific PLMs have\nbeen presented. Some of the well-known in the\nbiomedical field include the following: biomedical\nBERT (BioBERT) (Lee et al., 2019) and biomedi-\ncal A Lite BERT (BioALBERT) (Naseem et al.,\n2020, 2021a). Recently, other domain-specific\nLMs such as BERTweet (Nguyen et al., 2020) for\n3 downstream tasks, i.e., part-of-speech tagging,\nnamed-entity-recognition, and text classification\nand COVID Twitter BERT (CT-BERT) (Müller\net al., 2020) for 5 text classification tasks have been\ntrained on datasets from Twitter.\nDespite the number of PLMs that have been re-\nleased, none have been produced specifically for\nPHS from online text. Furthermore, all these LMs\nwere evaluated with the selected dataset, and there-\nfore their generalizability is unproven. To bench-\nmark and fill the gap, we present PHS-BERT, a\nnew domain-specific contextual PLM trained and\nfine-tuned to achieve benchmark performance on\nvarious PHS tasks on social media. PHS-BERT is\ntrained on a health-related corpus collected from\nuser-generated content. Our work is the first large-\nscale study to train, release and test a domain-\nspecific PLM for PHS tasks on social media. We\ndemonstrated that PHS-BERT outperforms other\nSOTA PLMs on 25 datasets from different social\nmedia platforms related to 7 different PHS tasks,\nshowing that PHS is robust and generalizable.\n2 Related Work\n2.1 Pretrained Language Models\nTransformer-based PLMs such as BERT (Devlin\net al., 2019) and its variants (Liu et al., 2019; Lan\net al., 2019) have altered the landscape of research\nin NLP domain. These PLMs are trained on a\nhuge corpus but may not provide a good represen-\ntation of specific domains (Müller et al., 2020).\nTo improve the performance in domain-specific\ntasks, various domain-specific PLMs have been\npresented. Some of the famous in the biomedi-\ncal domain are BioBERT (Lee et al., 2019) and\nBioALBERT (Naseem et al., 2020). Recently, for\ntasks on social media-specific, other PLMs such as\nBERTweet (Nguyen et al., 2020), COVID Twitter\nBERT (CT-BERT) (Müller et al., 2020) have been\ntrained on datasets from Twitter. For various down-\nstream tasks, these domain-specific PLMs were\ndemonstrated to be effective alternatives for PLMs\ntrained on a general corpus for a variety of down-\nstream tasks (Müller et al., 2020). The assumption\nis that the LMs trained on the user-generated text\non Twitter can handle the short and unstructured\ntext in tweets. Despite this progress, their gener-\nalizability is unproven, and there is no PLM for\npublic health surveillance using social media.\n2.2 NLP for Public Health Surveillance\nThe use of social media in conjunction with ad-\nvances in NLP for PHS tasks is a growing area of\nstudy (Paul and Dredze, 2017). NLP can assist\nresearchers in the surveillance of mental disorders,\nsuch as identifying depression diagnosis, assess-\ning suicide risk and stress identification, vaccine\nhesitancy and refusal, identifying common health-\nrelated misconceptions, sentiment analysis, and\nthe health-related behaviors they support (Naseem\net al., 2022a,b).\nRao et al. (2020) presented a hierarchical method\nthat used BERT with attention-based BiGRU and\nachieved competitive performance for depression\ndetection. For vaccine-related sentiment classifi-\ncation, Zhang et al. (2020) classified tweet-level\nHPV vaccine sentiment using three transfer learn-\ning techniques (ELMo, GPT, and BERT) and found\nthat a finely tuned BERT produced the best results.\nBiddle et al. (2020) presented a method (BiLSTM-\nSenti) that leveraged contextual word embeddings\n(BERT) with word-level sentiment to improve per-\nformance. Naseem et al. (2021b) presented a model\nthat uses domain-specific LM and captures com-\nmonsense knowledge into a context-aware bidi-\nrectional gated recurrent network. Sawhney et al.\n(2021) presented an ordinal hierarchical attention\nmodel for Suicide Risk Assessment where text em-\nbeddings obtained by Longformer were fed to BiL-\nSTM with attention and ordinal loss as an objective\nfunction. However, there is no PLM trained on\nhealth-related text collected from social media that\ndirectly benefit the applications related to PHS.\n3 Method\nPHS-BERT has the same architecture as BERT.\nFig. 1 illustrates an overview of pretraining, fine-\ntuning, and datasets used in this study. We describe\nBERT and then the pretraining and fine-tuning pro-\ncess employed in PHS-BERT.\n3.1 BERT\nPHS-BERT has the same architecture as BERT.\nBERT was trained on 2 tasks: mask language mod-\n23\nFigure 1: An overview of pretraining, fine-tuning, and the various tasks and datasets used in PHS benchmarking\neling (MLM) (15% of tokens were masked and\nnext sentence prediction (NSP) (Given the first\nsentence, BERT was trained to predict whether\na selected next sentence was likely or not). BERT\nis pretrained on Wikipedia and BooksCorpus and\nneeds task-specific fine-tuning. Pretrained BERT\nmodels include BERTBase (12 layers, 12 atten-\ntion heads, and 110 million parameters), as well\nas BERTLarge (24 layers, 16 attention heads, and\n340 million parameters).\n3.2 Pretraining of PHS-BERT\nWe followed the standard pretraining protocols of\nBERT and initialized PHS-BERT with weights\nfrom BERT during the training phase instead of\ntraining from scratch and used the uncased version\nof the BERT model.\nPHS-BERT is the first domain-specific LM for\ntasks related to PHS and is trained on a corpus\nof health-related tweets that were crawled via the\nTwitter API. Focusing on the tasks related to PHS,\nkeywords used to collect pretraining corpus are set\nto disease, symptom, vaccine, and mental health-\nrelated words in English. Pre-processing methods\nsimilar to those used in previous works (Müller\net al., 2020; Nguyen et al., 2020) were employed\nprior to training. Retweet tags were deleted from\nthe raw corpus, and URLs and usernames were re-\nplaced with HTTP-URL and @USER, respectively.\nAdditionally, the Python emoji3 library was used to\nreplace all emoticons with their associated mean-\nings. The HuggingFace 4, an open-source python\nlibrary, was used to segment tweets. Each sequence\nof BERT LM inputs is converted to 50,265 vocab-\n3https://pypi.org/project/emoji/\n4https://huggingface.co/\nulary tokens. Twitter posts are restricted to 200\ncharacters, and during the training and evaluation\nphase, we used a batch size of 8. Distributed train-\ning was performed on a TPU v3-8.\n3.3 Fine-tuning for downstream tasks\nWe applied the pretrained PHS-BERT in the bi-\nnary and multi-class classification of different PHS\ntasks such as stress, suicide, depression, anorexia,\nhealth mention classification, vaccine, and covid\nrelated misinformation and sentiment analysis. We\nfine-tuned the PLMs in downstream tasks. Specifi-\ncally, we used the ktrain library (Maiya, 2020)\nto fine-tune each model independently for each\ndataset. We used the embedding of the special to-\nken [CLS] of the last hidden layer as the final\nfeature of the input text. We adopted the multilayer\nperceptron (MLP) with the hyperbolic tangent acti-\nvation function and used Adam optimizer (Kingma\nand Ba, 2014). The models are trained with a one\ncycle policy (Smith, 2017) at a maximum learning\nrate of 2e-05 with momentum cycled between 0.85\nand 0.95.\n4 Experimental Analysis\n4.1 Tasks and Datasets\nWe evaluated and benchmarked the performance of\nPHS-BERT on 7 different PHS classification tasks\n(e.g., stress, suicidal ideation, depression, health\nmention, vaccine, covid related sentiment analy-\nsis, and other health-related tasks) collected from\npopular social platforms (e.g., Reddit and Twitter).\nWe used 25 datasets (see Table 1) crawled from\nsocial media platforms (e.g., Reddit and Twitter).\nWe relied on the datasets that are widely used in the\ncommunity and described each of these tasks and\n24\nTable 1: Statistics of the datasets used. We used the Stratified 5-Folds cross-validation (CV) strategy for train/test\nsplit if original datasets do not have an official train/test split.\nTask (Classification) Dataset Platform # of Samples # of Classes Training Strategy Used\nSuicide R-SSD (Cao et al., 2019) Reddit 500 Users 5 Stratified 5-Folds CV\nStress Dreaddit (Turcan and McKeown, 2019) Reddit 3553 Posts 2 Official Split\nSAD (Mauriello et al., 2021) SMS-like 6850 SMS 2 Official Split\nHealth Mention\nPHM (Karisani and Agichtein, 2018) Twitter 4635 Posts 4 Stratified 5-Folds CV\nPHM (Karisani and Agichtein, 2018) Twitter 4635 Posts 2 Stratified 5-Folds CV\nHMC2019 (Biddle et al., 2020) Twitter 15393 Posts 3 Stratified 5-Folds CV\nRHMD (Naseem et al., 2022b) Reddit 3553 Posts 4 Stratified 5-Folds CV\nVaccine Sentiment VS1 (Dunn et al., 2020) Twitter 9261 Posts 3 Stratified 5-Folds CV\nVS2 (Müller and Salathé, 2019) Twitter 18522 Posts 3 Stratified 5-Folds CV\nCOVID Related\nCovid Lies (Hossain et al., 2020) Twitter 3204 Posts 3 Stratified 5-Folds CV\nCovid Category (Müller et al., 2020) Twitter 4328 Posts 2 Stratified 5-Folds CV\nCOVIDSentiA (Naseem et al., 2021d) Twitter 30000 Posts 3 Stratified 5-Folds CV\nCOVIDSentiB (Naseem et al., 2021d) Twitter 30000 Posts 3 Stratified 5-Folds CV\nCOVIDSentiC (Naseem et al., 2021d) Twitter 30000 Posts 3 Stratified 5-Folds CV\nDepression\neRISK T3 (Losada and Crestani, 2016) Reddit 190 Users 4 Stratified 5-Folds CV\nDepression_Reddit_1 (Naseem et al., 2022a) Reddit 3553 Posts 4 Stratified 5-Folds CV\neRISK19 T1 (Losada and Crestani, 2016) Reddit 2810 Users 2 Official Split\nDepression_Reddit_2 (Pirina and Çöltekin, 2018) Reddit 1841 Posts 2 Stratified 5-Folds CV\nDepression_Twitter_1 Twitter 1793 Posts 3 Stratified 5-Folds CV\nDepression_Twitter_2 Twitter 10314 Posts 2 Stratified 5-Folds CV\nOther Health related\nPubHealth (Kotonya and Toni, 2020) News Websites 12251 Posts 4 Official Split\nAbortion (Mohammad et al., 2016) Twitter 933 Posts 3 Official Split\nAmazon Health (He and McAuley, 2016) Amazon 2003 Posts 4 Official Split\nSMM4H T1 (Weissenbacher et al., 2018) Twitter 14954 Posts 2 Official Split\nSMM4H T2 (Weissenbacher et al., 2018) Twitter 13498 Posts 3 Official Split\nHRT (Paul and Dredze, 2012) Twitter 2754 Posts 4 Stratified 5-Folds CV\ndatasets. Below we briefly discussed each task and\ndataset used in our study (appendix A for details).\n1. Suicide: The widespread use of social media\nfor expressing personal thoughts and emotions\nmakes it a valuable resource for assessing sui-\ncide risk on social media. We used the fol-\nlowing dataset to evaluate the performance of\nour model. We used R-SSD (Cao et al., 2019)\ndataset to evaluate the performance of our model\non suicide risk detection.\n2. Stress: It is desirable to detect stress early in\norder to address the growing problem of stress.\nTo evaluate stress detection using social media,\nwe evaluated PHS-BERT on the Dreaddit (Tur-\ncan and McKeown, 2019) and SAD (Mauriello\net al., 2021) datasets.\n3. Health mention: In social media platforms,\npeople often use disease or symptom terms in\nways other than to describe their health. In\ndata-driven PHS, the health mention classifi-\ncation task aims to identify posts where users\ndiscuss health conditions rather than using dis-\nease and symptom terms for other reasons.\nWe used PHM (Karisani and Agichtein, 2018),\nHMC2019 (Biddle et al., 2020) and RHMD 5\nhealth mention-related datasets.\n5https://github.com/usmaann/RHMD-Health-Mention-\nDataset\n4. Vaccine sentiment: Vaccines are a critical com-\nponent of public health. On the other hand, vac-\ncine hesitancy and refusal can result in clusters\nof low vaccination coverage, diminishing the\neffectiveness of vaccination programs. Identi-\nfying vaccine-related concerns on social media\nmakes it possible to determine emerging risks\nto vaccine acceptance. We used VS1 (Dunn\net al., 2020) and VS2 (Müller and Salathé, 2019)\nvaccine-related Twitter datasets to show the ef-\nfectiveness of our model.\n5. COVID related: Due to the ongoing pandemic,\nthere is a higher need for tools to identify\nCOVID-19-related misinformation and senti-\nment on social media. Misinformation can have\na negative impact on public opinion and endan-\nger the lives of millions of people if precau-\ntions are not taken. We used COVID Lies (Hos-\nsain et al., 2020), Covid category (Müller et al.,\n2020), and COVIDSenti (Naseem et al., 2021d)6\ndatasets to test our model.\n6. Depression: User-generated text on social\nmedia has been actively explored for its fea-\nsibility in the early identification of depres-\nsion. We used following eRisk T3 (Losada and\nCrestani, 2016), eRisk T1 (Losada and Crestani,\n2016), Depression_Reddit_1 (Naseem et al.,\n6we used 3 subsets (COVIDSentiA, COVIDSentiB and\nCOVIDSentiC)\n25\n2022a)7, Depression_Reddit_2 (Pirina and Çöl-\ntekin, 2018), Depression_Twitter_18, and De-\npression_Twitter_29 depression-related datasets\nin our experiments.\n7. Other health related tasks: We also evalu-\nated the performance of our PHS-BERT on\nother health-related 6 datasets. We used PUB-\nHEALTH (Kotonya and Toni, 2020), Abor-\ntion (Mohammad et al., 2016) 10, Amazon\nHealth dataset (He and McAuley, 2016),\nSMM4H T1 (Weissenbacher et al., 2018),\nSMM4H T2 (Weissenbacher et al., 2018) and\nHRT (Paul and Dredze, 2012).\n4.2 Evaluation Metric\nTo evaluate the performance, we used F1-score and\nthe relative improvement in marginal performance\n(∆MP ) used in a previous similar study (Müller\net al., 2020).\n4.3 Baselines\nWe evaluated the performance of PHS-BERT with\nvarious SOTA existing PLMs in different domains.\nWe compared the performance with BERT (Devlin\net al., 2019), ALBERT (Lan et al., 2019), and Dis-\ntilBERT (Sanh et al., 2019) pretrained with general\ncorpus, BioBERT (Lee et al., 2019) pretrained in\nthe biomedical domain, CT-BERT (Müller et al.,\n2020) and BERTweet (Nguyen et al., 2020) pre-\ntrained on covid related tweets and MentalBERT (Ji\net al., 2021) pretrained on corpus from Reddit from\nmental health-related subreddits.\n4.4 Results\nTable 2 summarizes the results of the presented\nPHS-BERT in comparison to the baselines. We ob-\nserve that the performance of PHS-BERT is higher\nthan SOTA PLMs on all tested tasks and datasets.\nBelow we discuss the performance comparison\nof PHS-BERT with BERT and the results of the\nsecond-best PLM.\nSuicide Ideation Task: We observed that the\nmarginal increases in performance of PHS-BERT\nis 18.45% when compared to BERT and 12.79%\nwhen compared to second best results.\n7https://github.com/usmaann/Depression_Severity_Dataset\n8https://github.com/AshwanthRamji/Depression-\nSentiment-Analysis-with-Twitter-Data\n9https://github.com/viritaromero/Detecting-Depression-\nin-Tweets\n10The SemEval 2016 stance detection task has 5 target\ndomains. We used the legalization of abortion.\nStress Detection Task: We showed that PHS-\nBERT achieved higher performance than the best\nbaseline on both datasets. The average marginal\nincrease in performance of PHS-BERT is 3.80%\ncompared to BERT and 2% when compared to\nsecond-best results.\nHealth Mention Task: PHS-BERT outperformed\nall the baselines on all health mention classification\ndatasets. The average marginal increase in perfor-\nmance of PHS-BERT is 3.34% compared to BERT\nand 1.76% when compared to second-best results.\nDepression Detection Task:We demonstrated that\nPHS-BERT outperformed all the baselines on all 6\ndepression datasets to identify depression on social\nmedia. We observed that the average marginal\nincrease in performance of PHS-BERT is 6.03%\ncompared to BERT and 2.76% when compared to\nsecond-best results.\nVaccine Sentiment Task: For the vaccine sen-\ntiment task, PHS-BERT achieved higher perfor-\nmance compared to all baselines on both datasets.\nResults showed that the average marginal increase\nin performance of PHS-BERT is 7.70% than BERT\nand 0.34% compared to second-best results.\nCOVID Related Task: PHS-BERT outperformed\nall baselines on all 5 datasets for COVID-related\ntasks. On average, the marginal increase in perfor-\nmance is 11.82% compared to BERT and 4.471%\ncompared to the second-best results.\nOther Health Related Task: We showed that\nPHS-BERT outperformed all the baselines on all 6\ndatasets to identify other health-related tasks on so-\ncial media. We observed that the average marginal\nincrease in performance of PHS-BERT is 11.82%\ncompared to BERT and 4.71% when compared to\nsecond-best results.\n4.5 Discussion\nWe demonstrated the effectiveness of our domain-\nspecific PLM on a downstream classification task\nrelated to PHS. Compared to previous SOTA\nPLMs, PHS-BERT improved the performance on\nall datasets (7 tasks). Our experimental results\nshowed that BERT, a PLM trained in the general\ndomain, gets competitive results on downstream\nclassification tasks. However, for domain-specific\ntasks, general domain PLMs (BERT, ALBERT, dis-\ntilBERT) might need more training on relevant cor-\npora to achieve better performance on the domain-\nspecific downstream classification task. Further, we\nobserved that using a domain-specific PLM trained\n26\nTable 2: Comparison of PHS-BERT (Ours) v/s SOTA PLMs. Best results (F1-score) are represented in bold, whereas\nsecond-best results are underlined. ∆MPBERT and ∆MPSB represent the marginal increase in performance\ncompared to the BERT and the second-best PLM (under-lined).\nSuicide Ideation Task\nDataset BERT ALBERT distilBERT CT-BERT BioBERT BERTweet MentalBERT Ours ∆MPBERT ∆MPSB\nR-SSD 25.72 23.07 26.96 18.67 23.51 24.82 17.35 30.28 18.45↑ 12.79↑\nStress Detection Task\nDreaddit 78.55 79.43 78.22 81.46 78.34 80.03 80.89 82.89 5.60↑ 1.78↑\nSAD 92.66 91.11 91.47 91.11 93.92 94.17 93.23 94.75 2.28↑ 0.62↑\nAverage 85.61 85.27 84.85 86.29 86.13 87.10 87.06 88.82 3.80↑ 2.00↑\nHealth Mention Task\nPHM (Multi-class) 86.21 80.05 85.06 82.02 82.22 85.59 87.76 89.38 3.72↑ 1.87↑\nPHM (Binary) 91.89 90.53 90.64 92.17 89.62 92.12 92.29 93.27 1.52↑ 1.07↑\nHMC2019 88.99 87.22 88.01 90.82 86.27 90.65 90.17 91.71 3.09↑ 0.99↑\nRHMD 74.20 69.02 73.22 72.87 72.25 74.66 75.28 77.16 5.48↑ 2.53 ↑\nAverage 85.07 81.71 84.23 84.47 82.59 85.76 86.38 87.38 3.34↑ 1.76 ↑\nDepression Detection Task\neRisk T3 64.56 64.78 67.33 63.17 64.86 63.56 67.75 68.98 6.95↑ 1.84↑\nDepression_Reddit_1 22.39 21.09 21.95 24.21 24.00 20.84 21.95 28.75 29.73↑ 19.56↑\neRisk T1 93.72 93.79 93.34 86.74 91.73 91.92 94.30 94.52 0.86↑ 0.24↑\nDepression_Reddit_2 91.33 90.72 91.01 68.16 90.53 91.75 92.70 93.36 2.25↑ 0.72↑\nDepression_Twitter_1 64.17 51.70 66.71 57.11 64.12 64.24 72.95 76.18 19.01↑ 4.49↑\nDepression_Twitter_2 96.99 96.79 96.70 96.96 96.59 96.87 97.09 97.12 0.14↑ 0.03↑\nAverage 72.19 69.81 72.84 66.06 71.97 71.53 74.46 76.49 6.03↑ 2.76↑\nVaccine Sentiment Task\nVS1 74.14 70.00 73.95 79.92 73.30 76.81 71.56 79.96 7.96↑ 0.05↑\nVS2 76.60 74.82 75.91 81.73 76.77 79.10 77.65 82.24 7.46↑ 0.63↑\nAverage 75.37 72.41 74.93 80.84 75.04 77.96 74.61 81.10 7.70↑ 0.34↑\nCOVID Related Task\nCovid Lies 92.96 91.53 92.14 92.24 93.79 91.07 94.60 95.35 2.60↑ 0.80↑\nCOVID Category 93.98 93.94 94.35 95.29 93.72 93.45 94.97 95.83 1.99↑ 0.57↑\nCOVIDSentiA 90.90 90.81 90.90 78.96 90.41 66.30 91.55 93.97 3.41↑ 2.67↑\nCOVIDSentiB 91.31 89.88 91.06 86.85 91.02 89.46 92.06 93.44 2.36↑ 1.52↑\nCOVIDSentiC 91.24 83.72 90.77 84.83 90.55 61.78 91.66 93.11 2.03↑ 1.60↑\nAverage 92.08 89.98 91.84 87.63 91.90 80.41 92.97 94.34 2.48↑ 1.49↑\nOther Health Related Task\nPubHealth 60.30 61.43 60.77 63.97 58.85 60.57 57.30 64.77 7.54↑ 1.27↑\nAbortion 58.79 58.59 68.09 70.39 62.53 62.82 63.03 72.31 23.40↑ 2.77↑\nAmazon Health 63.45 63.18 62.30 54.84 60.27 65.50 65.57 68.09 7.43↑ 3.90↑\nSMM4H T1 33.33 33.86 35.80 45.50 39.45 45.87 39.81 46.49 40.71↑ 1.38↑\nSMM4H T2 75.54 72.76 75.12 79.19 73.43 80.20 77.54 80.34 6.44↑ 0.18↑\nHRT 78.67 76.97 78.35 80.90 76.13 80.48 80.46 81.12 3.15↑ 0.28↑\nAverage 61.68 61.13 63.41 65.80 61.78 65.91 63.95 68.85 11.82↑ 4.71↑\non biomedical corpora (BioBERT) is less effective\nthan pretraining on the target domain. We also ob-\nserved that using CT-BERT, BERTweet, and Men-\ntalBERT, which are trained on social media-based\ntext, performs better compared to PLMs trained\nin the general and biomedical domain. These re-\nsults also demonstrated the effectiveness of train-\ning in a target domain. In particular, CT-BERT\nhas the second-best performance on 9 datasets, and\nMentalBERT has the second-best performance on\n13 datasets. The results of domain-specific PLMs\ndemonstrated that continued pretraining in the rele-\nvant domain improves performance on downstream\ntasks.\n5 Conclusion\nWe present PHS-BERT, a domain-specific PLM\ntrained on health-related social media data. Our\nresults demonstrate that using domain-specific cor-\npora to train general domain LMs improves per-\nformance on PHS tasks. On all 25 datasets related\nto 7 different PHS tasks, PHS-BERT outperforms\nprevious state-of-the-art PLMs. We expect that the\nPHS-BERT PLM will benefit the development of\nnew applications based on PHS NLP tasks.\nEthics and Societal Impact\nEthics: No additional ethics approval was sought\nfor the analysis of data in this study because data\nwere drawn from already published studies.\nSocietal Impact: We train and release a PLM to ac-\ncelerate the automatic identification of tasks related\nto PHS on social media. Our work aims to develop\na new computational method for screening users\nin need of early intervention and is not intended to\nuse in clinical settings or as a diagnostic tool.\nReproducibility: For reproducibility and future\nworks, PHS-BERT is publicly released and\nis available at https://huggingface.\nco/publichealthsurveillance/\nPHS-BERT.\n27\nReferences\nAllison E Aiello, Audrey Renson, and Paul N Zivich.\n2020. Social media–and internet-based disease\nsurveillance for public health. Annual review of pub-\nlic health, 41:101–118.\nAaron T Beck, Calvin H Ward, Mock Mendelson,\nJeremiah Mock, and John Erbaugh. 1961. An inven-\ntory for measuring depression. Archives of general\npsychiatry, 4(6):561–571.\nRhys Biddle, Aditya Joshi, Shaowu Liu, Cecile Paris,\nand Guandong Xu. 2020. Leveraging sentiment dis-\ntributions to distinguish figurative from literal health\nreports on twitter. In Proceedings of The Web Con-\nference 2020, pages 1217–1227.\nLei Cao, Huijun Zhang, Ling Feng, Zihan Wei, Xin\nWang, Ningyun Li, and Xiaohao He. 2019. La-\ntent suicide risk detection on microblog via suicide-\noriented word embeddings and layered attention.\narXiv preprint arXiv:1910.12038.\nLauren E Charles-Smith, Tera L Reynolds, Mark A\nCameron, Mike Conway, Eric HY Lau, Jennifer M\nOlsen, Julie A Pavlin, Mika Shigematsu, Laura C\nStreichert, Katie J Suda, et al. 2015. Using social me-\ndia for actionable disease surveillance and outbreak\nmanagement: a systematic literature review. PloS\none, 10(10):e0139701.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAdam G Dunn, Kenneth D Mandl, and Enrico Coiera.\n2018. Social media interventions for precision public\nhealth: promises and risks. NPJ digital medicine,\n1(1):1–4.\nAdam G Dunn, Didi Surian, Jason Dalmazzo, Dana\nRezazadegan, Maryke Steffens, Amalie Dyda, Julie\nLeask, Enrico Coiera, Aditi Dey, and Kenneth D\nMandl. 2020. Limited role of bots in spreading\nvaccine-critical information among active twitter\nusers in the united states: 2017–2019. American\nJournal of Public Health, 110(S3):S319–S325.\nSu Golder, Gill Norman, and Yoon K Loke. 2015. Sys-\ntematic review on the prevalence, frequency and com-\nparative value of adverse events data in social media.\nBritish journal of clinical pharmacology, 80(4):878.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative filtering. In proceedings of\nthe 25th international conference on world wide web,\npages 507–517.\nKirsty Hope, David N Durrheim, Edouard Tursan\nd’Espaignet, and Craig Dalton. 2006. Syndromic\nsurveillance: is it a useful tool for local outbreak\ndetection?\nTamanna Hossain, Robert L Logan IV , Arjuna Ugarte,\nYoshitomo Matsubara, Sean Young, and Sameer\nSingh. 2020. Covidlies: Detecting covid-19 misin-\nformation on social media. In Proceedings of the 1st\nWorkshop on NLP for COVID-19 (Part 2) at EMNLP\n2020.\nShaoxiong Ji, Tianlin Zhang, Luna Ansari, Jie Fu,\nPrayag Tiwari, and Erik Cambria. 2021. Mentalbert:\nPublicly available pretrained language models for\nmental healthcare. arXiv preprint arXiv:2110.15621.\nPayam Karisani and Eugene Agichtein. 2018. Did you\nreally just have a heart attack? towards robust detec-\ntion of personal health mentions in social media. In\nProceedings of the 2018 World Wide Web Conference,\npages 137–146.\nDonna M Kazemi, Brian Borsari, Maureen J Levine,\nand Beau Dooley. 2017. Systematic review of surveil-\nlance by social media platforms for illicit drug use.\nJournal of Public Health, 39(4):763–776.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nNeema Kotonya and Francesca Toni. 2020. Explain-\nable automated fact-checking for public health claims.\nCoRR, abs/2010.09926.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nDavid E Losada and Fabio Crestani. 2016. A test col-\nlection for research on depression and language use.\nIn International Conference of the Cross-Language\nEvaluation Forum for European Languages, pages\n28–39. Springer.\nArun S. Maiya. 2020. ktrain: A low-code li-\nbrary for augmented machine learning. arXiv,\narXiv:2004.10703 [cs.LG].\nMatthew Louis Mauriello, Thierry Lincoln, Grace Hon,\nDorien Simon, Dan Jurafsky, and Pablo Paredes.\n2021. Sad: A stress annotated dataset for recog-\nnizing everyday stressors in sms-like conversational\n28\nsystems. In Extended Abstracts of the 2021 CHI Con-\nference on Human Factors in Computing Systems,\npages 1–7.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemEval-2016 task 6: Detecting stance in tweets.\nIn Proceedings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016), pages 31–\n41, San Diego, California. Association for Computa-\ntional Linguistics.\nDanielle L Mowery, Craig Bryan, and Mike Conway.\n2015. Towards developing an annotation scheme\nfor depressive disorder symptoms: A preliminary\nstudy using twitter data. In Proceedings of the 2nd\nWorkshop on Computational Linguistics and Clini-\ncal Psychology: From Linguistic Signal to Clinical\nReality, pages 89–98.\nMartin Müller, Marcel Salathé, and Per E Kummervold.\n2020. Covid-twitter-bert: A natural language pro-\ncessing model to analyse covid-19 content on twitter.\narXiv preprint arXiv:2005.07503.\nMartin M Müller and Marcel Salathé. 2019. Crowd-\nbreaks: Tracking health trends using public social\nmedia data and crowdsourcing. Frontiers in public\nhealth, 7:81.\nUsman Naseem, Adam G Dunn, Matloob Khushi, and\nJinman Kim. 2021a. Benchmarking for biomedical\nnatural language processing tasks with a domain spe-\ncific albert. arXiv preprint arXiv:2107.04374.\nUsman Naseem, Adam G. Dunn, Jinman Kim, and Mat-\nloob Khushi. 2022a. Early identification of depres-\nsion severity levels on reddit using ordinal classifi-\ncation. In Proceedings of the Web Conference 2022,\npages 1–10.\nUsman Naseem, Matloob Khushi, Jinman Kim, and\nAdam G Dunn. 2021b. Classifying vaccine sentiment\ntweets by modelling domain-specific representation\nand commonsense knowledge into context-aware at-\ntentive gru. arXiv preprint arXiv:2106.09589.\nUsman Naseem, Matloob Khushi, Vinay Reddy, Sak-\nthivel Rajendran, Imran Razzak, and Jinman Kim.\n2020. Bioalbert: A simple and effective pre-trained\nlanguage model for biomedical named entity recog-\nnition. arXiv preprint arXiv:2009.09223.\nUsman Naseem, Jinman Kim, Matloob Khushi, and\nAdam G. Dunn. 2022b. Identification of disease or\nsymptom terms in reddit to improve health mention\nclassification. In Proceedings of the Web Conference\n2022, pages 11–19.\nUsman Naseem, Imran Razzak, Shah Khalid Khan, and\nMukesh Prasad. 2021c. A comprehensive survey\non word representation models: From classical to\nstate-of-the-art word representation language models.\nTransactions on Asian and Low-Resource Language\nInformation Processing, 20(5):1–35.\nUsman Naseem, Imran Razzak, Matloob Khushi, Pe-\nter W Eklund, and Jinman Kim. 2021d. Covidsenti:\nA large-scale benchmark twitter data set for covid-19\nsentiment analysis. IEEE Transactions on Computa-\ntional Social Systems, 8(4):1003–1015.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. Bertweet: A pre-trained language model for\nenglish tweets. arXiv preprint arXiv:2005.10200.\nMichael J Paul and Mark Dredze. 2012. A model for\nmining public health topics from twitter. Health,\n11(16-16):1.\nMichael J Paul and Mark Dredze. 2017. Social monitor-\ning for public health. Synthesis Lectures on Informa-\ntion Concepts, Retrieval, and Services, 9(5):1–183.\nInna Pirina and Ça ˘grı Çöltekin. 2018. Identifying de-\npression on reddit: The effect of training data. In\nProceedings of the 2018 EMNLP Workshop SMM4H:\nThe 3rd Social Media Mining for Health Applications\nWorkshop & Shared Task, pages 9–12.\nGuozheng Rao, Chengxia Peng, Li Zhang, Xin Wang,\nand Zhiyong Feng. 2020. A knowledge enhanced\nensemble learning model for mental disorder detec-\ntion on social media. In International Conference on\nKnowledge Science, Engineering and Management,\npages 181–192. Springer.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRamit Sawhney, Harshit Joshi, Saumya Gandhi, and Ra-\njiv Ratn Shah. 2021. Towards ordinal suicide ideation\ndetection on social media. In Proceedings of the 14th\nACM International Conference on Web Search and\nData Mining, pages 22–30.\nLeslie N Smith. 2017. Cyclical learning rates for train-\ning neural networks. In 2017 IEEE winter conference\non applications of computer vision (WACV), pages\n464–472. IEEE.\nElsbeth Turcan and Kathleen McKeown. 2019. Dread-\ndit: A reddit dataset for stress analysis in social me-\ndia. arXiv preprint arXiv:1911.00133.\nDavy Weissenbacher, Abeed Sarker, Michael J. Paul,\nand Graciela Gonzalez-Hernandez. 2018. Overview\nof the third social media mining for health (SMM4H)\nshared tasks at EMNLP 2018. In Proceedings of the\n2018 EMNLP Workshop SMM4H: The 3rd Social\nMedia Mining for Health Applications Workshop &\nShared Task, pages 13–16, Brussels, Belgium. Asso-\nciation for Computational Linguistics.\nLi Zhang, Haimeng Fan, Chengxia Peng, Guozheng\nRao, and Qing Cong. 2020. Sentiment analysis meth-\nods for hpv vaccines related tweets based on transfer\nlearning. In Healthcare, volume 8, page 307. Multi-\ndisciplinary Digital Publishing Institute.\n29\nA Dataset description\n1. Depression: We used 6 depression-related\ndatasets in our experiments.\n• eRisk T3: We used eRISK, a publicly avail-\nable dataset, released by (Losada and Crestani,\n2016) and labeled across 4 depression sever-\nity levels using Beck’s Depression Inven-\ntory (Beck et al., 1961) criteria to detect the ex-\nistence of depression and identify its severity\nlevel in social media posts. eRISK was later\nused in the CLEF’s eRISK challenge Task 311\non early identification of depression in social\nmedia. Since in each years’ challenge author\nreleased a small number of user’s data (rang-\ning from 70-90 users data), we combined and\nused the data of the last 3 years, which is\nequivalent to 190 Reddit users, labeled across\n4 depression severity levels.\n• Depression_Reddit_1: We used new Red-\ndit depression data released by Naseem et al.\n(2022a). This dataset consists of 3,553 Reddit\nposts to identify the depression severity on\nsocial media. Annotators manually labeled\ndata into 4 depression severity levels i.,e., (i)\nminimal depression; (ii) mild depression, (iii)\nmoderate depression; and (iv) severe depres-\nsion using Depressive Disorder Annotation\nscheme (Mowery et al., 2015).\n• eRisk T1: The third depression data is from\neRisk shared task 1 (Losada and Crestani,\n2016), which is a public competition for de-\ntecting early risk in health-related areas. The\neRisk data consists of posts from 2,810 users,\nwith 1,370 expressing depression and 1,440\nas a control group without depression.\n• Depression_Reddit_2: The fourth depression\ndataset used is released by Pirina and Çöl-\ntekin (Pirina and Çöltekin, 2018). The au-\nthors used Reddit to collect additional social\ndata, which they combined with previously\ncollected data to identify depression.\n• Depression_Twitter_1: Our fifth depression\ndataset is a publicly availabl 12. This data is\ncollected from Twitter and labeled into 3 la-\nbels (e.g., Positive, Negative, and Neutral) for\ndepression sentiment analysis.\n• Depression_Twitter_2: Our sixth depression\n11https://erisk.irlab.org/2021/index.html\n12https://github.com/AshwanthRamji/Depression-\nSentiment-Analysis-with-Twitter-Data\ndataset is a public dataset 13, collected from\nTwitter and labeled into 2 labels (e.g., Positive\nand Negative) for depression detection.\n2. Health Mention: We used 3 health mention-\nrelated datasets in our experiments.\n• PHM: Karisani and Agichtein (2018) con-\nstructed and released the PHM dataset consist-\ning of 7,192 English tweets across 6 diseases\nand symptoms. They used the Twitter API to\nretrieve the data using the colloquial disease\nnames as search keywords. They manually an-\nnotated the tweets and categorized them into\n4 labels. In addition to 4 labels, similar to\nKarisani and Agichtein (2018) we also used\nbinary labels for health mention classification.\n• HMC2019: HMC2019 is presented by Biddle\net al. (2020) by extending the PHM dataset\nto include 19,558 tweets and included labels\nrelated to figurative mentions, and included 4\nmore different disease or symptom terms (10\nin total) for health mention classification.\n• RHMD: We also used Reddit health mention\ndataset (RHMD)(Naseem et al., 2022b) for\nHMC task. RHMD consists of 10K+ Red-\ndit posts manually annotated with 4 labels\n(personal health mention, non-personal health\nmention, figurative health mention, hyper-\nbolic health mention). In our study, we used\n3 label versions of data released by authors\nwhere they merged figurative health mention\nand hyperbolic health mention into 1 class.\n3. Suicide: We used the following dataset to eval-\nuate the performance of our model on suicide\nrisk detection.\n• R-SSD: For suicide ideation, we used a\ndataset released by Cao et al. (2019), which\ncontains 500 individuals’ Reddit postings cat-\negorized into 5 increasing suicide risk classes\nfrom 9 mental health and suicide-related sub-\nreddits.\n4. Stress: To evaluate stress detection using social\nmedia, we evaluated PHS-BERT on the follow-\ning datasets.\n• Dreaddit: For stress detection, we used\nDreaddit (Turcan and McKeown, 2019)\ncollected from 5 different Reddit forums.\n13https://github.com/viritaromero/Detecting-Depression-\nin-Tweets\n30\nDreaddit consists of 3,553 posts and fo-\ncuses on three major stressful topics: inter-\npersonal conflict, mental illness, and finan-\ncial need. Posts in Dreaddit are collected\nfrom 10 subreddits, including some mental\nhealth domains such as anxiety and PTSD.\n• SAD: The SAD (Mauriello et al., 2021)\ndataset, which contains 6,850 SMS-like\nsentences, is used to recognize everyday\nstressors. The SAD dataset is derived from\nstress management articles, chatbot-based\nconversation systems, crowdsourcing, and\nweb crawling. Some of the more specific\nstressors are work-related issues like fa-\ntigue or physical pain, financial difficulties\nlike debt or anxiety, school-related deci-\nsions like final projects or group projects,\nand interpersonal relationships like friend-\nships and family relationships.\n5. Vaccine sentiment: We used two vaccine-\nrelated Twitter datasets to show the effectiveness\nof our model.\n• VS1: Our first dataset consists of tweets about\nvaccine dissemination on Twitter from Jan-\nuary 12, 2017, to December 3, 2019. Dunn\net al. (2020) crawled and labeled this data.\nThe total tweets count is 9,212, with 6,683\npositive, 1,084 negatives, and 1,445 neural\ntweets.\n• VS2: The second dataset 14 includes tweets\nabout measles and vaccinations obtained via\nthe Twitter Streaming API between July 2018\nand January 2019 and provided by Müller and\nSalathé (2019). The total number of tweets is\n18,503, with 8,965 pro-vaccine tweets, 1,976\nanti-vaccine tweets, and 7,562 neutral tweets.\n6. COVID: We used 5 covid related datasets to\ntest our model.\n• COVID Lies: Hossain et al. (2020) released\nCOVIDLIES, a dataset (6761 tweets) anno-\ntated by experts with known COVID-19 mis-\nconceptions and tweets that agree, disagree,\nor express no stance.\n• Covid category: Covid category dataset is re-\nleased by Müller et al. (2020). Amazon Turk\nannotators were asked to classify a given tweet\n14https://github.com/digitalepidemiologylab/crowdbreaks-\npaper\ntext as personal narrative or news. Crowd-\nbreaks was used to perform the annotation.\n• COVIDSenti: We used a newly released\nlarge-scale sentiment dataset, COVIDSenti,\nwhich contains 90,000 COVID-19-related\ntweets obtained during the pandemic’s early\nstages, from February to March 2020. The\ntweets are labeled into positive, negative, and\nneutral sentiment classes. In our experiments,\nwe used 3 subsets (COVIDSentiA, COVID-\nSentiB and COVIDSentiC) released by au-\nthors (Naseem et al., 2021d).\n7. Other health related tasks: We used PUB-\nHEALTH (Kotonya and Toni, 2020), a dataset\nfor automated fact-checking of public health\nclaims that are explainable. PUBHEALTH is\nlabeled with its factuality (true, false, unproven,\nmixture). (ii) Abortion: In SemEval 2016 stance\ndetection task (Mohammad et al., 2016), 5 tar-\nget domains are given: legalization of abortion,\natheism, climate change, feminism, and Hillary\nClinton. We used the legalization of abortion in\nour experiments. (iii) Amazon Health dataset:\nThe Amazon Health dataset (He and McAuley,\n2016) contains reviews of Amazon healthcare\nproducts and has 4 classes i.e., strongly posi-\ntive, positive, negative, and strongly negative.\n(iv) SMM4H T1: We used Social Media Min-\ning for Health (SMM4H) Shared Task 1 rec-\nognizing whether a tweet is reporting an ad-\nverse drug reaction (Weissenbacher et al., 2018).\n(v) SMM4H T2: Drug Intake Classification\n(SMM4H Task 2) (Weissenbacher et al., 2018)\nwhere participants were given tweets manually\ncategorized as definite intake, possible intake,\nor no intake. (vi) HRT: Health related tweets\n(HRT) (Paul and Dredze, 2012) were collected\nusing Twitter and manually annotated using Me-\nchanical Turk as related or unrelated to health.\nHealth-related tweets were further labeled as\nsick (the text implied that the user was suffer-\ning from an acute illness, such as a cold or the\nflu) or health (the text made general comments\nabout the user’s or the other’s health, such as\nchronic health conditions, lifestyle, or diet) and\nunrelated tweets were further labeled as unre-\nlated (texts that were not about a specific per-\nson’s health, such as news and updates about the\nswine flu or advertisements for diet pills) and\nnon-English.\n31",
  "topic": "Social media",
  "concepts": [
    {
      "name": "Social media",
      "score": 0.7926753759384155
    },
    {
      "name": "Computer science",
      "score": 0.7274607419967651
    },
    {
      "name": "Benchmarking",
      "score": 0.7244657278060913
    },
    {
      "name": "Language model",
      "score": 0.518684983253479
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5079532265663147
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4615625739097595
    },
    {
      "name": "Public health",
      "score": 0.4190130829811096
    },
    {
      "name": "Transformer",
      "score": 0.4144134521484375
    },
    {
      "name": "Data science",
      "score": 0.3891429603099823
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36354824900627136
    },
    {
      "name": "Machine learning",
      "score": 0.3431757688522339
    },
    {
      "name": "World Wide Web",
      "score": 0.22500592470169067
    },
    {
      "name": "Business",
      "score": 0.151638001203537
    },
    {
      "name": "Engineering",
      "score": 0.14995864033699036
    },
    {
      "name": "Medicine",
      "score": 0.13825723528862
    },
    {
      "name": "Nursing",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    }
  ]
}