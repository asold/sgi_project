{
  "title": "Language Modeling through Long-Term Memory Network",
  "url": "https://openalex.org/W2940057731",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2755324845",
      "name": "Anupiya Nugaliyadde",
      "affiliations": [
        "Murdoch University"
      ]
    },
    {
      "id": "https://openalex.org/A179541017",
      "name": "Ferdous Sohel",
      "affiliations": [
        "Murdoch University"
      ]
    },
    {
      "id": "https://openalex.org/A2148757765",
      "name": "Kok Wai Wong",
      "affiliations": [
        "Murdoch University"
      ]
    },
    {
      "id": "https://openalex.org/A1994681933",
      "name": "Hong Xie",
      "affiliations": [
        "Murdoch University"
      ]
    },
    {
      "id": "https://openalex.org/A2755324845",
      "name": "Anupiya Nugaliyadde",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A179541017",
      "name": "Ferdous Sohel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2148757765",
      "name": "Kok Wai Wong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1994681933",
      "name": "Hong Xie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6734807902",
    "https://openalex.org/W2116261113",
    "https://openalex.org/W6764815766",
    "https://openalex.org/W6684821475",
    "https://openalex.org/W6638318767",
    "https://openalex.org/W6631399359",
    "https://openalex.org/W2753993436",
    "https://openalex.org/W6697250464",
    "https://openalex.org/W6638545294",
    "https://openalex.org/W6631636882",
    "https://openalex.org/W6677548962",
    "https://openalex.org/W2229162816",
    "https://openalex.org/W2734653669",
    "https://openalex.org/W2028140375",
    "https://openalex.org/W2735758254",
    "https://openalex.org/W2751148689",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W6732742072",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2765238425",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2884001105",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W2106564373",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6679844565",
    "https://openalex.org/W6623517193",
    "https://openalex.org/W6696934422",
    "https://openalex.org/W1525783482",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2952752214",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2964091467",
    "https://openalex.org/W854541894",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W2118776487",
    "https://openalex.org/W2594978815",
    "https://openalex.org/W2341587966",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2742947407",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2295360187",
    "https://openalex.org/W2964308564"
  ],
  "abstract": "Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTM), and Memory Networks which contain memory are popularly used to learn patterns in sequential data. Sequential data has long sequences that hold relationships. RNN can handle long sequences but suffers from the vanishing and exploding gradient problems. While LSTM and other memory networks address this problem, they are not capable of handling long sequences (50 or more data points long sequence patterns). Language modelling requiring learning from longer sequences are affected by the need for more information in memory. This paper introduces Long Term Memory network (LTM), which can tackle the exploding and vanishing gradient problems and handles long sequences without forgetting. LTM is designed to scale data in the memory and gives a higher weight to the input in the sequence. LTM avoid overfitting by scaling the cell state after achieving the optimal results. The LTM is tested on Penn treebank dataset, and Text8 dataset and LTM achieves test perplexities of 83 and 82 respectively. 650 LTM cells achieved a test perplexity of 67 for Penn treebank, and 600 cells achieved a test perplexity of 77 for Text8. LTM achieves state of the art results by only using ten hidden LTM cells for both datasets.",
  "full_text": "Language Modeling through Long Term Memory\nNetwork\n*The article was accepted to IJCNN 2019\nAnupiya Nugaliyadde\nCollege of Science, Health, Engineering and Education\nMurdoch University\nMurdoch, Australia\na.nugaliyadde@murdoch.edu.au\nKok Wai Wong\nCollege of Science, Health, Engineering and Education\nMurdoch University\nMurdoch, Australia\nk.wong@murdoch.edu.au\nFerdous Sohel\nCollege of Science, Health, Engineering and Education\nMurdoch University\nMurdoch, Australia\nf.sohel@murdoch.edu.au\nHong Xie\nCollege of Science, Health, Engineering and Education\nMurdoch University\nMurdoch, Australia\nh.xie@murdoch.edu.au\nAbstract—Recurrent Neural Networks (RNN), Long Short-\nTerm Memory Networks (LSTM), and Memory Networks which\ncontain memory are popularly used to learn patterns in se-\nquential data. Sequential data has long sequences that hold\nrelationships. RNN can handle long sequences but suffers from\nthe vanishing and exploding gradient problems. While LSTM\nand other memory networks address this problem, they are not\ncapable of handling long sequences (50 or more data points long\nsequence patterns). Language modelling requiring learning from\nlonger sequences are affected by the need for more information\nin memory. This paper introduces Long Term Memory network\n(LTM), which can tackle the exploding and vanishing gradient\nproblems and handles long sequences without forgetting. LTM is\ndesigned to scale data in the memory and gives a higher weight\nto the input in the sequence. LTM avoid overﬁtting by scaling the\ncell state after achieving the optimal results. The LTM is tested\non Penn treebank dataset, and Text8 dataset and LTM achieves\ntest perplexities of 83 and 82 respectively. 650 LTM cells achieved\na test perplexity of 67 for Penn treebank, and 600 cells achieved\na test perplexity of 77 for Text8. LTM achieves state of the art\nresults by only using ten hidden LTM cells for both datasets.\nIndex Terms—Long-Term Memory Network, Language Mod-\neling, Long Term Dependencies\nI. I NTRODUCTION\nNatural language understanding requires processing sequen-\ntial data. Natural language is time-dependent, and past infor-\nmation can inﬂuence the current and future output. Therefore,\nmodels which are capable of processing sequential data are re-\nquired. Memory determines the models capability of recalling\nfrom past information. Sequential deep learning models have\nshown to achieve state-of-the-art results in natural languages\n2019 IEEE. Personal use of this material is permitted. Permission from\nIEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.\nunderstanding tasks such as question answering [1], machine\ntranslation [2][3], and language modelling [4][5][6].\nThe memory networks have a recurrent behaviour which\nuse outputs to inﬂuence the current output [5][7][8]. With\nthe increase in sequence length, the effect on the current\ninput is reduced, and after a certain number of steps the\neffect on the current input becomes invisible. In order to\nunderstand a language, the model is required to learn from past\nknowledge. Relevant information to understand language is\nspread throughout the sequence. Therefore, long-term memory\nis required for natural language understanding [4][5].\nRecurrent Neural Network (RNN)s are capable of handling\nlong sequences but suffer from the exploding and vanishing\ngradient descent [9][4]. In order to overcome the issue Long\nShort-Term Memory Networks (LSTM)s [7], Simple Recur-\nrent Network [4] and Memory Network [8] clip the gradient.\nThese models still suffer from the problem of vanishing\ngradient when the sequences are long. The gradients of non-\nlinear functions are close to zero, and the gradient is back\npropagated through time while multiplied. When the eigen-\nvalues are small, the gradient will converge to zero rapidly.\nTherefore, these models are capable of only handling short-\nterm dependencies.\nLSTM, GRU and SRN proposed by Mikolov [4] use gates\nto control the vanishing gradient problem. These gates control\nthe vanishing gradient problem. The gates control the memory\nsequence and prevents the overﬂow of data. The forget gate in\nthe LSTM is a crucial element which forgets the past sequence\n[10]. The gates control or forget the previous sequences which\ninﬂuence the current input. Therefore, these memory networks\ndo not handle long term sequences.\nHolding longer sequences in memory is important in prop-\nerly understanding a language and it is also necessary in\narXiv:1904.08936v1  [cs.CL]  18 Apr 2019\nmany long term dependency tasks[11]. In order to remember\nlong sequences as well as to prevent the learning model from\nsuffering from the vanishing gradient problem, Long Term\nMemory Network (LTM) is introduced in this paper. LTM\ndoes not forget the past sequences. LTM incorporates the past\noutputs and current inputs. LTM generalises the past sequences\nand gives a higher emphasis on the new inputs in order\nto support natural language understanding. LTM was tested\nfor long-term memory dependency based language modelling\ntasks. LTM is tested on Penn Treebank and Text8 datasets and\nit outperformed the current state-of-the-art memory network\nmodels.\nII. B ACKGROUND\nLong term memory dependencies require learning from\npatterns. Memory networks are used in order to learn long\nterm dependencies [1]. Memory networks including RNN\nand LSTM are used for many natural language tasks such\nas question answering, speech to text, language modelling\nand time series analysis [1][12][13][14][15].These memory\nnetworks have shown to achieve state-of-the-art results in\nbenchmark datasets. However, RNN, LSTM and other memory\nnetworks perform differently from each other, and each has its\nown merits.\nRNN is capable of handling inﬁnite continuous sequence\n[16][17]. It takes an input and passes the value continuously.\nThe output is looped back and combined with the input [18].\nThe long term dependencies learning fails due to exploding\nand vanishing gradient problem [19]. This is due to the direct\ninﬂuence of the past information to the current input xt (1).\nThe internal state St for a current input of RNN can be deﬁned\nas:\nS = activation(Uxt + Wst−1 ) (1)\nwhere activation can be any activation function (e.g. tanh,\nRelu), U the weight for the current input, W being the weight\nfor the past input state St−1 [16]. Therefore, the overall output\nwould be affected by the past outputs. When Wst−1 is added\nto the weight of the current input Uxt, the past state St−1\ndirectly affects current state St as shown in (1).\nLSTM was introduced in order to handle the vanishing and\nexploding gradient problem [7]. The forget gate was later\nadded to the original LSTM. This is capable of preventing\nthe internal state from growing indeﬁnitely and handling the\nnetwork break [10]. The forget gate resets the cell state when\nthe it decides on forgetting the past sequence. The cell state\nholds the past inputs with the network or resets the cell state to\nforget past information held in the network. LSTM has shown\nto be a stable model that is not affected by the vanishing and\nexploding gradient problem [19]. However, the LSTM is only\ncapable of handling short-term dependencies [17].\nTraditional memory networks (RNN and LSTM) have\nshown to handle natural language understanding tasks [20].\nRNN is capable of handling continuous data streams which\nare entered into the network as in speech recognition [21] and\nlanguage modelling [4][22][6]. LSTM has shown to perform\nmore complex tasks such as question answering [1][14]. The\ntraditional memory networks and speciﬁed memory networks\n(Dynamic Memory Network [23] and Reinforced Memory\nNetwork [1]) beneﬁt learning from longer dependencies in\norder to understand language. Longer dependencies are cap-\ntured by adding more hidden layers. The hidden layers would\nalso contribute towards the vanishing and exploding gradient.\nTherefore, forgetting the past sequences is one main approach\nused in memory networks [5][10]. This affects on long-term\ndependency.\nThe vanishing and exploding gradient is one of the most\nproblematic issues in memory networks through backpropaga-\ntion [24]. Memory network trained by deriving the gradients\nof the network weights using backpropagation and chain rule.\nConsider a long sequence which has more than 30 words as\nthe input, “ I was born in France. I moved to UK when I was\n5 years old ... I speak ﬂuent French ”. Using language models\nthe last word of the paragraph “ French” requires learning\nthrough a long dependency from the ﬁrst word France. Passing\nthe paragraph through an RNN can cause the vanishing and\nexploding gradient problem [18]. This problem occurs while\nthe RNN is training. Gradients from the deeper layers have\nto go through matrix multiplications using Chain Rule, and if\nthe previous layers have small values, it declines exponentially\n[18]. These gradient values are insigniﬁcant to the model to\nlearn from; this is vanishing gradient problem. If the gradient\nis large, it gets larger and explodes which negatively affects\nthe models training; this is the exploding gradient problem.\nClipping the gradients which places a predeﬁned threshold\nvalue which changes the gradient length and attempts to\ncontrol the vanishing and exploding gradient problem of RNN\n[18]. Gradient clipping affects the convergence of the gradient.\nLSTM and other memory networks avoid vanishing and ex-\nploding gradient by using gates which controls the passing the\npast outputs to the current input [25]. Clipping also requires\na target to be deﬁned at every time step which increases the\ncomplexity [18]. Memory networks including LSTM forget the\npast outputs which the network deems irrelevant. Attention-\nbased memory networks [26] avoid vanishing and exploding\ngradient by focusing on only a few factors which are relevant\nto the tasks. These methods used to avoid the vanishing and\nexploding gradient prevents prolonging the memory of the\nnetwork. According to the example, either the sequence is\nlong, or the model does not identify relevancy in “ France”,\nit is removed from the memory. Model not knowing “ France”\nwould directly inﬂuence the model in predicting the last word\n“French”.\nLong-term memory network should have the capability of\nholding all the past sequences and not be affected by the\nvanishing or exploding gradient.\nIII. P ROPOSED METHODOLOGY FOR LONG TERM\nMEMORY\nThe proposed model has two main objectives: 1) to handle\nlonger sequences; and 2) to overcome the vanishing gradient.\nThe proposed LTM is structured such that it is capable of\nholding and generalizing old sequences (Fig. 1) and give an\nemphasis on the recent information. Fig. 1. shows a single\ncell LTM which holds long-term memory which generalises\nthe past sequences.\nFig. 1. Long-Term Memory cell, the arrows show the data ﬂow from within\nthe cell. . indicates the dot product between the two vectors and + indicates\nthe sum of the two vectors.\nRetaining longer memory sequences is a crucial requirement\nin natural language understanding since the past sequences\naffect the current inputs [27]. Furthermore, LTM gives an\nemphasis weight to the current input. The LTM holds three\nstates:\n(a) input state: handles the current input to pass on to the\noutput\n(b) cell state: carries the past information through each step\nto the other step.\n(c) output state: handles the current output and passes the\noutput to the cell state.\nThe LTMs functionality relies on the gate structure within\nit. The LTM cell contains four gates with the ﬁrst three\ngates impact on the inputs and the last gate controlling and\ngeneralising the cell state. However, the LTMs cell state does\nnot reset itself similar to LSTMs forget gates function [10].\nTherefore, LTM is capable of holding longer sequences in\nmemory. The following sections provide the detail of the\narchitecture.\nA. Input state\nThe input is combined with the previous output and passed\non to the Sigmoid 1 as shown in (2). Equation (2), σindicates\nthe sigmoid functions and W1 is the weight for the gate.\nThe Lt1 is the by-product which generates an effect on the\nLTM cell which depends on the current input and the previous\noutput.\nLt1 = σ(W1(ht−1 + input)) (2)\nSimilarly, (3) shows a similar functionality with different\nweight W2 which gives a higher impact on the current input\nalthough scaled through the sigmoid functions (Sigmoid 1 and\nSigmoid 2). These two equations (2) and (3) support long-\nterm memory by emphasising on the current input and adds\non to the past input. W2 is the weight for the gate represented\nby (3).\nLt2 = σ(W2(ht−1 + input)) (3)\nIn order to emphasise the current input to effect on the\noutput Lt1 and Lt2 are passed through a dot operation to create\nL′\nt. L′\nt is created as showin in (4).\nL′\nt = Lt1.Lt2 (4)\nL′\nt ampliﬁes the effect of the current input and past output.\nL′\nt is passed on to the cell state, which would be carried along\nto the future sequences. L′\nt ampliﬁes the current inputs effect\non the output.\nB. Cell state\nCell state similar to LSTM’s cell state [7] carries forward the\npast outputs to the present cell. Natural language understand-\ning requires both past output and current inputs. The current\ninput is emphasised over the past outputs. Therefore, L′\nt has a\nhigher value combining the current input which is passed on\nto the cell state as shown in (5). Therefore, the output would\nhave a higher effect on the current input. As shown in (5), C′\nt,\nthe current cell state combines the current input L′\nt and the\npast output Ct−1.\nC′\nt = L′\nt + Ct−1 (5)\nThe ﬁnal cell state Ct as shown in (6) is calculated using the\nC′\nt and passing through the Sigmoid 4. Through this, the LTM\nscales the cell state Ct. The cell state carries on a scaled value\nto the ﬁnal output state. W4 is the weight for the (6).\nCt = σ(W4C′\nt) (6)\nC. Output state\nEquation (7) shows the direct inﬂuence on the output of a\ngiven LTM cell. Lt3 directly inﬂuences the output by passing\nthe current input. W3 is the weight for the (7).\nLt3 = σ(W3(ht−1 + input)) (7)\nThe cell state Ct and the Lt3 are joined together and combined\nthrough the dot operation. The Ct and the Lt3 create the ﬁnal\noutput ht. Equation (8) shows the ﬁnal output creation. ht\nhas a higher impact through the current input as well as the\npast outputs. Therefore, the impact from both the past and the\ncurrent input are combined as shown in (8).\nht = Ct.Lt3 (8)\nThe output ht and Ct, is passed on to the next time step,\nwhich is shown in (Fig. 3). LTM is used as a cell, and the cell\npasses the Ct and ht. This also shows how the cells passes\nthe past outputs on and combine with the current inputs.\nIV. E XPERIMENTATION\nIn order to demonstrate the long-term dependency learn-\ning, LTM is tested on language modelling. Three types of\nexperiments are conducted to evaluate the LTM using Penn\ntreebank dataset and Text8 dataset. Penn treebank dataset\ncontains 2499 stories of Wall Street Journal. These stories\nare in raw text format. Text8 dataset contains over 240000\nWikipedia articles. Articles from both datasets contain long\nFig. 2. Long-Term Memory cells connected. The ﬁgure also illustrates the\ndata passed on in the cell state and how the output is passed on from one\nLTM cell to another.\nrelationship dependencies between words. LTM is evaluated\non the two datasets against the current state of the art models,\nand ﬁnally, LTM is evaluated against itself by changing the\nnumber of cells to ﬁnd the best cell size which generates the\nbest results.\nLTM was ﬁrst evaluated on Pennbank dataset [28]. Similar\nto Mikolov et al. model [29], it consists of pre-processing the\ndata and the training size of 930K tokens, validating the size\nof 74K tokens and testing size of 82K tokens. The dataset has\na vocabulary of 10K words. In order to match with the current\nstate-of-the-art model experiments, 300 LTM cells are used.\nSecond dataset Text8 [4] has 44K vocabulary from\nWikipedia. The dataset has 15.3m training tokens, 848K\nvalidation tokens and 855K test tokens. The settings are similar\nto [30]. Words which occur ten times or lower are placed as an\nunknown token. 500 LTM cells are used in the experiments.\nIn order to evaluate the model on its performance, the cell\nnumber is gradually increased and tested for both Pennbank\ndataset and Text8. The experiment conditions are the same as\nthe above experiments except for the number of layers. All the\nlearning models on the Penn Treebank dataset follow similar\n[29] and experiments on Text8 follows [4] this includes the\ninputs with the hyper-parameters.\nV. R ESULTS\nLTMs long term memory is tested on Penn Treebank dataset\nand Text8 dataset. The results are validated using perplexity\nshown in (9). Perplexity is the inverse probability of the\ntest set, normalized by the number of words. The lower the\nperplexity the better the model.\nPerplexity(W) =\nN\n√\nN∏\ni=1\n1\nP(wi|w1...wi−1) (9)\nThe ﬁrst experiment was based on the Penn treebank dataset.\nResults are shown in Table I. LTM is tested against the tra-\nditional memory and recurrent networks and the current state\nof the art models (Delta-RNN). RNN which had the lowest\nperformance over the tested models with 300 hidden layers\nachieved a test perplexity of 129. This demonstrates that RNN\nis not capable of handling long-term dependencies. Although\nLSTM has outperformed the RNN, ultra-speciﬁc models which\nhandle long-term memory outperforms the generalised models\non long-term memory. LTM achieves a test perplexity 83 with\n300 units, which is 20 points above the current state of the art\nresults. Furthermore, LTM achieves the state of the art results\nat ten hidden layers (Table III).\nLTM was also tested with the Text8 dataset with 500 hidden\nlayers. The LTM was compared against the traditional memory\nnetworks and the current state of the art models (MemNet)\n(Table II). LTM has outperformed all the state of the art model\nby only using ten hidden units (Table III). The ultra-speciﬁed\nlong-term dependency based memory networks have shown to\noutperform the generic memory networks.\nLTM was tested on Text8 and Penn treebank dataset by\nincreasing its hidden layers in order to identify the best per-\nforming number of hidden layers. Table III shows validation\nand testing perplexity for Text8 and Penn Treebank while\nincreasing the hidden layers. Table III also shows that LTM\nachieves the state of the art results with only ten hidden layers,\nin which other networks require 300 hidden layers or more to\nachieve state of the art results. The results are further improved\nby increasing the number of hidden layers. LTM achieved the\nbest results for the Penn treebank dataset with 650 hidden\nlayers. Furthermore, LTM achieved its best results for Text8\nwith 600 hidden layers.\nTABLE I\nPENN TREEBANK VALIDATE AND TEST PERPLEXITY . PERPL =\nPERPLEXITY\nModels Penn Treebank\n# hidden layers Validate Perpl. Test Perpl.\nRNN 300 133 129\nLSTM[4] 300 123 119\nSCRN[4] 300 120 115\nDelta-RNN 300 - 102.7\nLTM 300 85 83\nTABLE II\nTEXT8 VALIDATE AND TEST PERPLEXITY. P ERPL = PERPLEXITY\nModels Text8\n# hidden layers Validate Perpl. Test Perpl.\nRNN 500 - 184\nLSTM[4] 500 122 154\nSCRN[4] 500 161 161\nMemNet[13] 500 118 147\nLTM 500 85 82\nVI. D ISCUSSION\nThe structure of the LTM, as shown in Fig.1. is designed\nin order to hold the inputs passed through the LTM cell\nand scale the output. The use of the sigmoid functions is a\ncrucial aspect of maintaining a scaled output. Equation 6 is\nused to create the cell state and the output. The use of the\nsigmoid function in equation 6 scales the cell state in order\nto prevent exploding or vanishing gradient problem. Since the\ncell state is scaled and passed on from one-time stamp to\nthe other time stamp the cell state value would not explode\nor vanish preventing the vanishing or exploding gradient.\nVanishing and exploding gradient is the main reason for a\nTABLE III\nINCREASING THE NUMBER OF HIDDEN LAYERS THE\nVALIDATION AND TEST PERPLEXITY FOR TEXT 8 AND PENN\nTREEBANK DATASETS. P ERPL = PERPLEXITY\n# hidden Text8 Penn Treebank\nlayers Train Perpl. Test Perpl. Train Perpl. Test Perpl.\n10 103 100 100 99\n50 101 99 98 97\n100 99 98 95 92\n150 97 95 90 89\n200 95 93 88 86\n250 93 90 87 85\n300 90 89 85 83\n350 89 87 82 80\n400 87 86 79 78\n450 86 84 77 76\n500 85 82 74 72\n550 81 80 72 70\n600 79 77 69 67\n650 79 77 68 67\n700 79 77 68 67\nmemory network to forget or underperform. In order to prevent\nexploding or vanishing, gradient LSTM introduced the forget\ngate [7]. Using the forget gate the LSTM can handle longer\nsequences and forget the sequence when irreverent sequences\nare presented to the LSTM. However, the past sequences\nalthough not substantially relevant have an effect in long-term\nnatural language understanding tasks. LSTM has a downfall in\nlong-term memory. LTM scales the outputs and holds it in the\nmemory. Therefore, even the long dependencies would affect\nthe ﬁnal output of the LTM.\nLTM gives a high impact on the new inputs (4). LTM\ncombines Lt1 and Lt2 in order to pass a higher impact from\nthe current input to the output as shown in (4). Therefore, the\nLTM gives a higher priority to the new inputs, which is more\nrelevant to the current output. Equation 8 shows the effect on\nthe ﬁnal output which combines both the processed input and\nthe cell state, which carries the past sequential information.\nLanguage modelling is one evaluation method to analyse\nthe long-term dependencies of LTM. The Penn treebank and\nText8 datasets require longer learning capabilities. Language\nmodelling requires a clear understanding of the entire text,\nrather than a window of text. Holding an entire article in\norder to predict and understand text is easier for the model.\nLTM through scaling holds all the information passed through\nthe LTM. Therefore, LTM is capable of understanding a clear\npicture of the entire article. Attention-based memory networks\n[9] identify the most relevant information and the network\npredicts based on the information the attention has capture.\nAttention-based memory networks are capable of handling\nshorter sequences. It failed to hold long sequence. The at-\ntention diverts when given longer sequences. LTM does not\nfocus on memory and holds all past inputs. Unlike attention\nbased networks would forget the most irreverent information\nwhich might be relevant later on the sequence, LTM would\nhold all the information passed through the model.\nTable I and Table II compare the LTM with other state-of-\nthe-art models and traditional memory networks. This shows\nthat LTM is capable of handling longer sequences and pro-\nduces state of the art results. LTM’s longer memory plays a\ncrucial role in language modelling tasks. Table III shows that\nincreasing LTM cells would further enhance the results and\nproduce lower perplexity score. LTM has shown to hold longer\nsequences and be unaffected by vanishing and exploding\ngradient.\nSimilar to LSTM, LTM avoids vanishing or exploding gra-\ndient decent using gates. LTM uses gates to enhance the input\npassed to the network. LTM handles long-term dependencies\nby the use sigmoid functions to scale the new inputs and\ncarry on the past outputs at the gates. LTM handles long\nsequences through the scaling. The example of “ I was born\nin France. I moved to UK when I was 5 years old ... I\nspeak ﬂuent French” predicting “French” is attainable since the\nmodel holds the entire sequence. Holding the entire sequence\nin the memory supports the model to predict the last word\n“French”. LTM carries forward the entire sequence allowing\nthe models to use the entirety of the sequence to predict the\nﬁnal word, which holds the most important factor that requires\nthe model to predict the last word. LTM is capable of handling\nvanishing and exploding gradient as well as handling long-\nterm dependencies.\nFig. 3. General cell of a Long Short Term Memory network. The ﬁgure\nillustrates a general Long Short Term Memory Network cell taken from the\ntime time stamp inputt.\nFig. 3 shows the LSTM cell which holds three gates (forget\ngate, input gate and output gate). LSTM holds a combination\nof sigmoid and tanh activation fucntions, while LTM relies\nonly on sigmoid. Comparing Fig. 1 with Fig. 3 indicates the\ncore difference between LSTM and LTM. LTM uses general-\nization through the sigmoid activation functions hold a longer\nsequence without forgetting the past information. However,\nLSTM forgets longer sequences through the forget gates in\norder to maintain the networks stability. LSTM sacriﬁes long\nterm dependencies for network stability.\nVII. C ONCLUSION\nThis paper presents a long-term memory network which is\ncapable of handling long-term dependencies. LTM is capable\nof handling long sequences without being affected by van-\nishing or exploding gradient. LTM has shown to outperform\ntraditional LSTM and RNN as well as the memory speciﬁc\nnetworks in language modelling. LTM was tested on both Penn\ntreebank and Text8 dataset in which LTM has outperformed all\nstate of the art memory networks using minimal hidden units.\nIncreasing the number of hidden units have shown that the\nLTM does not get affected by the vanishing and exploding\ngradient. Adding more hidden unit the LTM has achieved\nlower perplexity scores and stabilised.\nACKNOWLEDGMENT\nThis work was partially supported by a Murdoch University\ninternal grant on the high-power computer.\nREFERENCES\n[1] A. Nugaliyadde, K. W. Wong, F. Sohel, and H. Xie,\n“Reinforced memory network for question answering,”\nin International Conference on Neural Information Pro-\ncessing. Springer, 2017, pp. 482–490.\n[2] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine\ntranslation by jointly learning to align and translate,”\narXiv preprint arXiv:1409.0473 , 2014.\n[3] Z. Yang, W. Chen, F. Wang, and B. Xu, “Multi-sense\nbased neural machine translation,” in Neural Networks\n(IJCNN), 2017 International Joint Conference on. IEEE,\n2017, pp. 3491–3497.\n[4] T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and\nM. Ranzato, “Learning longer memory in recurrent neu-\nral networks,” arXiv preprint arXiv:1412.7753 , 2014.\n[5] A. G. Ororbia II, T. Mikolov, and D. Reitter, “Learn-\ning simpler language models with the differential state\nframework,” Neural computation , vol. 29, no. 12, pp.\n3327–3352, 2017.\n[6] M. D. Singh and M. Lee, “Temporal hierarchies in\nmultilayer gated recurrent neural networks for language\nmodels,” in Neural Networks (IJCNN), 2017 Interna-\ntional Joint Conference on. IEEE, 2017, pp. 2152–2157.\n[7] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural computation , vol. 9, no. 8, pp. 1735–\n1780, 1997.\n[8] J. Weston, S. Chopra, and A. Bordes, “Memory net-\nworks,” arXiv preprint arXiv:1410.3916 , 2014.\n[9] Y . Bengio, P. Simard, and P. Frasconi, “Learning long-\nterm dependencies with gradient descent is difﬁcult,”\nIEEE transactions on neural networks , vol. 5, no. 2, pp.\n157–166, 1994.\n[10] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning\nto forget: Continual prediction with lstm,” 1999.\n[11] A. Nugaliyadde, K. W. Wong, F. Sohel, and H. Xie,\n“Enhancing semantic word representations by em-\nbedding deeper word relationships,” arXiv preprint\narXiv:1901.07176, 2019.\n[12] A. Graves, G. Wayne, and I. Danihelka, “Neural turing\nmachines,” arXiv preprint arXiv:1410.5401 , 2014.\n[13] S. Sukhbaatar, J. Weston, R. Fergus et al. , “End-to-end\nmemory networks,” in Advances in neural information\nprocessing systems, 2015, pp. 2440–2448.\n[14] J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. van\nMerri¨enboer, A. Joulin, and T. Mikolov, “Towards ai-\ncomplete question answering: A set of prerequisite toy\ntasks,” arXiv preprint arXiv:1502.05698 , 2015.\n[15] S. Boukoros, A. Nugaliyadde, A. Marnerides, C. Vassi-\nlakis, P. Koutsakis, and K. W. Wong, “Modeling server\nworkloads for campus email trafﬁc using recurrent neural\nnetworks,” in International Conference on Neural Infor-\nmation Processing. Springer, 2017, pp. 57–66.\n[16] R. Pascanu and Y . Bengio, “Learning to deal with long-\nterm dependencies,” Neural Computation , vol. 9, pp.\n1735–1780, 1986.\n[17] H. Salehinejad, “Learning over long time lags,” arXiv\npreprint arXiv:1602.04335, 2016.\n[18] R. Pascanu, T. Mikolov, and Y . Bengio, “On the difﬁculty\nof training recurrent neural networks,” in International\nConference on Machine Learning, 2013, pp. 1310–1318.\n[19] S. Hochreiter, Y . Bengio, P. Frasconi, J. Schmidhuber\net al., “Gradient ﬂow in recurrent nets: the difﬁculty of\nlearning long-term dependencies,” 2001.\n[20] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent\ntrends in deep learning based natural language process-\ning,” ieee Computational intelligenCe magazine , vol. 13,\nno. 3, pp. 55–75, 2018.\n[21] S.-H. Chen, S.-H. Hwang, and Y .-R. Wang, “An rnn-\nbased prosodic information synthesizer for mandarin\ntext-to-speech,” IEEE transactions on speech and audio\nprocessing, vol. 6, no. 3, pp. 226–239, 1998.\n[22] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and\nS. Khudanpur, “Recurrent neural network based language\nmodel,” in Eleventh Annual Conference of the Interna-\ntional Speech Communication Association , 2010.\n[23] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury,\nI. Gulrajani, V . Zhong, R. Paulus, and R. Socher, “Ask\nme anything: Dynamic memory networks for natural\nlanguage processing,” in International Conference on\nMachine Learning, 2016, pp. 1378–1387.\n[24] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,”\nnature, vol. 521, no. 7553, p. 436, 2015.\n[25] H. Sak, A. Senior, and F. Beaufays, “Long short-term\nmemory recurrent neural network architectures for large\nscale acoustic modeling,” in Fifteenth annual conference\nof the international speech communication association ,\n2014.\n[26] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and\nY . Bengio, “Attention-based models for speech recog-\nnition,” in Advances in neural information processing\nsystems, 2015, pp. 577–585.\n[27] E. Cambria and B. White, “Jumping nlp curves: A\nreview of natural language processing research,” IEEE\nComputational intelligence magazine , vol. 9, no. 2, pp.\n48–57, 2014.\n[28] A. Taylor, M. Marcus, and B. Santorini, “The penn\ntreebank: an overview,” in Treebanks. Springer, 2003,\npp. 5–22.\n[29] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock`y, and\nS. Khudanpur, “Extensions of recurrent neural network\nlanguage model,” in Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2011 IEEE International Conference\non Acoustics, Speech and Signal Processing . IEEE,\n2011, pp. 5528–5531.\n[30] Z. Xie, S. I. Wang, J. Li, D. L ´evy, A. Nie, D. Ju-\nrafsky, and A. Y . Ng, “Data noising as smoothing\nin neural network language models,” arXiv preprint\narXiv:1703.02573, 2017.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9750248193740845
    },
    {
      "name": "Treebank",
      "score": 0.956527590751648
    },
    {
      "name": "Overfitting",
      "score": 0.8213323354721069
    },
    {
      "name": "Computer science",
      "score": 0.7567939758300781
    },
    {
      "name": "Forgetting",
      "score": 0.7340900897979736
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6868689060211182
    },
    {
      "name": "Language model",
      "score": 0.5787539482116699
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5541050434112549
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5512564182281494
    },
    {
      "name": "Deep learning",
      "score": 0.44779515266418457
    },
    {
      "name": "Term (time)",
      "score": 0.4454951584339142
    },
    {
      "name": "Artificial neural network",
      "score": 0.41362008452415466
    },
    {
      "name": "Machine learning",
      "score": 0.3465084731578827
    },
    {
      "name": "Natural language processing",
      "score": 0.3200724422931671
    },
    {
      "name": "Dependency (UML)",
      "score": 0.07071971893310547
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I176790772",
      "name": "Murdoch University",
      "country": "AU"
    }
  ],
  "cited_by": 8
}