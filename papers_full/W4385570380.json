{
  "title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models",
  "url": "https://openalex.org/W4385570380",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2744127780",
      "name": "Li, Siheng",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2065589776",
      "name": "Yang Cheng",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2291070915",
      "name": "Yin, Yichun",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2184035887",
      "name": "Zhu Xin-yu",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A4299705207",
      "name": "Cheng, Zesen",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2379035984",
      "name": "Shang, Lifeng",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2030371270",
      "name": "Jiang Xin",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A1952241286",
      "name": "LIU Qun",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2228976576",
      "name": "Yang, Yujiu",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3172205429",
    "https://openalex.org/W4280579537",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3093233911",
    "https://openalex.org/W3207604419",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1814042870",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W4296413526",
    "https://openalex.org/W3212362289",
    "https://openalex.org/W4221166832",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4327656064",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2888302696",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W4281652360",
    "https://openalex.org/W4389009366",
    "https://openalex.org/W4226099034"
  ],
  "abstract": "Information-seeking conversation, which aims to help users gather information\\nthrough conversation, has achieved great progress in recent years. However, the\\nresearch is still stymied by the scarcity of training data. To alleviate this\\nproblem, we propose AutoConv for synthetic conversation generation, which takes\\nadvantage of the few-shot learning ability and generation capacity of large\\nlanguage models (LLM). Specifically, we formulate the conversation generation\\nproblem as a language modeling task, then finetune an LLM with a few human\\nconversations to capture the characteristics of the information-seeking process\\nand use it for generating synthetic conversations with high quality.\\nExperimental results on two frequently-used datasets verify that AutoConv has\\nsubstantial improvements over strong baselines and alleviates the dependence on\\nhuman annotation. In addition, we also provide several analysis studies to\\npromote future research.\\n",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1751–1762\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAutoConv: Automatically Generating Information-seeking\nConversations with Large Language Models\nSiheng Li1†∗, Cheng Yang1†, Yichun Yin2, Xinyu Zhu1, Zesen Cheng3\nLifeng Shang2, Xin Jiang2, Qun Liu2, Yujiu Yang1‡\n1Shenzhen International Graduate School, Tsinghua University\n2Huawei Noah’s Ark Lab, 3Peking University\n{lisiheng21, yangc21}@mails.tsinghua.edu.cn\n{yinyichun, shang.lifeng, jiang.xin, qun.liu}@huawei.com\nyang.yujiu@sz.tsinghua.edu.cn\nAbstract\nInformation-seeking conversation, which aims\nto help users gather information through con-\nversation, has achieved great progress in recent\nyears. However, the research is still stymied\nby the scarcity of training data. To alleviate\nthis problem, we propose AutoConv for syn-\nthetic conversation generation, which takes ad-\nvantage of the few-shot learning ability and\ngeneration capacity of large language models\n(LLM). Specifically, we formulate the conver-\nsation generation problem as a language mod-\neling task, then finetune an LLM with a few\nhuman conversations to capture the character-\nistics of the information-seeking process and\nuse it for generating synthetic conversations\nwith high quality. Experimental results on two\nfrequently-used datasets verify that AutoConv\nhas substantial improvements over strong base-\nlines and alleviates the dependence on human\nannotation. In addition, we also provide several\nanalysis studies to promote future research.\n1 Introduction\nIn information-seeking conversations, users repeat-\nedly ask questions based on their interests, and the\ndialogue system provides answers to fulfill their in-\nformation needs (Stede and Schlangen, 2004; Choi\net al., 2018; Reddy et al., 2019). This scenario\nis important for addressing real-world open-ended\nquestions, which requires discussions to explore in\ndepth (Dai et al., 2022), e.g., How to learn more ef-\nficiently? Though great progress has been achieved\nin recent years, most existing researches depend on\nabundant human annotation, which can be highly\ncostly and limited in knowledge coverage.\nA promising way to alleviate this problem\nis data augmentation (Chen et al., 2021). Tra-\nditional methods, including token-level manip-\nulation (Kobayashi, 2018; Wei and Zou, 2019)\n∗ ∗This work is done when Siheng Li is an intern at Huawei\nNoah’s Ark Lab.\n††Equal contribution.\n‡‡Corresponding author.\nMethod DG Data Needs\nEDA (Wei and Zou, 2019) ✗ -\nBack-Translation (Sennrich et al., 2016) ✗ -\nSeemSeek (Kim et al., 2022) ✔ Large\nDialog Inpainting (Dai et al., 2022) ✔ Large\nAutoConv (Ours) ✔ Few\nTable 1: The differences between AutoConv and others.\nDG represents whether the augmentation is document\ngrounded, and Data Needs denotes the scale of human\nconversations used for augmentation.\nand sentence-level paraphrasing (Sennrich et al.,\n2016), improve the linguistic diversity of training\ndata. However, they cannot create conversations\ngrounded on new documents, which are indispens-\nable for dealing with out-of-domain scenarios. An-\nother line of research focuses on simulation-based\nmethods (Wu et al., 2021; Kim et al., 2022). Specif-\nically, they can iteratively generate conversations\ngrounded on new documents based on a span ex-\ntractor and an utterance generator. Nevertheless,\nboth the training of the extractor and the generator\nstill require abundant human dialogues. Besides\nthe above ways, Dai et al. (2022) propose Dialog\nInpainting, which creates information-seeking dia-\nlogues by inserting utterances between neighboring\nsentences in documents. One potential risk is the\ngap between the structure of documents and that of\nconversations. Documents are tighter, while real-\nworld conversations are more open-ended.\nTo alleviate the above issues, we propose\na simple yet effective method AutoConv for\nAutomatically generating information-seeking\nConversations, which takes advantage of the few-\nshot learning ability and generation capacity of\nlarge language models (LLM) (Brown et al., 2020).\nSpecifically, we formulate conversation generation\nas a language modeling task and utilize an LLM\nfor generating synthetic conversations grounded on\nexternal documents. Surprisingly, finetuning with\na few human dialogues can help LLM capture the\ncharacteristics of the information-seeking process\n1751\nDoc LargeLanguageModelUsr-1Sys-1Usr-2 …Sys-2SamplingGreedy\nFigure 1: The generation process of AutoConv. We\nuse nucleus sampling for generating user questions and\ngreedy search for generating system answers.\n(e.g., grounding, question answering) and gener-\nate high-quality synthetic conversations. Then, we\ncan train a small task model with these dialogues.\nThe differences between AutoConv and others are\nshown in Table 1.\nWe conduct comprehensive experiments on two\nfrequently-used datasets QuAC (Choi et al., 2018)\nand CoQA (Reddy et al., 2019) in the low-resource\nsetting, where only dozens of human dialogues are\navailable. The results show that AutoConv has\nsubstantial improvements over several strong base-\nlines. When scaling up the synthetic dialogues,\nAutoConv has the improvement of up to 5.06 F1\ngain compared with directly finetuning, and thus\nlargely reduces the labor force for annotation. In\naddition, we find that the small task model trained\nwith synthetic dialogues can even surpass finetuned\nLLM with only 1.7% parameters. Moreover, we\nalso investigate the impact of decoding strategy and\nscaling laws for AutoConv.\n2 Method\n2.1 Task Formulation\nOur goal is automatically generating information-\nseeking conversations. Specifically, each conversa-\ntion is grounded on a document d and consists of a\nseries of user questions and system answers.\n2.2 Conversation Generation\nTraining. We formulate conversation generation\nas a language modeling task and finetune1 an LLM\nwith a few human dialogues (e.g., 50 from QuAC\n(Choi et al., 2018)) to capture the characteristics of\ninformation-seeking conversations (e.g., grounding,\nquestion answering). The objective is the negative\nlog-likelihood of each utterance:\nL= −\nT∑\nt=1\nL∑\nl=1\nlog P(ut\nl|ut\n<l, h<t, d),\n1In our preliminary experiments, we try to prompt LLM\nwithout training. However, we find that the performance is\npoor and LLM fails to generate conversations grounded on the\ndocuments, similar to the observation in Zheng et al. (2022).\nwhere u represents a user question or a system\nanswer, h is the dialogue history, L and T are the\nnumber of tokens and turns respectively.\nGenerating. Based on the finetuned LLM, we\ncan generate synthetic dialogues with unlabeled\ndocuments, as in Figure 1. In information-seeking\nscenarios, user questions are typically open-ended.\nThus we choose nucleus sampling (Holtzman et al.,\n2020) for generating user questions, which has\nshown great performance in various open-ended\ngeneration tasks (Su et al., 2022). However, when\napplying a sampling decoding strategy for system\nanswer generation, we find it results in the “hal-\nlucination” problem (Shuster et al., 2021), where\nthe generation is plausible but factually incorrect\nbased on the document. To this end, we utilize\ngreedy search for answer generation. Neural lan-\nguage models often generate the same sentences\nrepetitively (Xu et al., 2022). To alleviate this prob-\nlem, we first compute the diversity score of each\nsynthetic dialogue as in Su et al. (2022), which\nconsiders the repetition at different n-gram levels.\nThen, we filter out dialogues based on this score.\nAfter that, a two-stage training strategy is\nadopted (Xie et al., 2020b) for training a small\ntask model. Specifically, we first pre-train it on\nthe synthetic dialogues, then finetune it on the hu-\nman dialogues used for finetuning the LLM. More\ntraining details are given in Appendix B.\n3 Experiments\nWe conduct experiments on QuAC (Choi et al.,\n2018) and CoQA (Reddy et al., 2019), more details\nabout them are shown in Appendix A.\n3.1 Implementation\nWe focus on the low-resource setting, where hu-\nman dialogues are scarce. To simulate this setting,\nwe randomly sample a few human dialogues from\nthe training set of QuAC or CoQA, and use them\nfor finetuning the LLM. We use OPT-13B (Zhang\net al., 2022) as the LLM and UnifiedQA-V2-base\n(222M) (Khashabi et al., 2022) as the small task\nmodel. All data augmentation methods use the\nsame training strategy and small task model. More\nimplementation details are shown in Appendix B.\n3.2 Comparison with Baselines\nWe compare AutoConv with a series of baselines,\nand the details of them are given in Appendix C. As\n1752\nMethod QuAC CoQA\nF1 EM F1 EM\nPrompting\nGPT-3 Zero-shot (Brown et al., 2020) 41.5 - 81.5 -\nGPT-3 Few-shot (Brown et al., 2020) 44.3 - 85.0 -\nData Augmentation (50 Human Dialogues)\nFinetuning 46.57 ±1.29 30.68±1.25 70.41±0.46 60.43±0.56\nBack-Translation (Sennrich et al., 2016) 47.92 ±0.49 28.26±1.39 67.59±2.73 56.34±3.41\nEDA (Wei and Zou, 2019) 46.04 ±1.28 28.88±2.20 58.89±2.08 47.64±2.14\nUtterance Manipulation (Chen and Yang, 2021) 48.83 ±0.63 33.91±0.73 68.69±0.85 58.30±1.21\nDialog Inpainting (Dai et al., 2022) 48.33 ±1.24 32.23±1.55 70.25±0.93 59.83±0.98\nAutoConv 50.48±0.94 34.12±0.93 73.87±0.85 63.78±1.01\nHuman Annotation 53.24 ±0.28 36.85±0.35 76.02±0.71 65.92±1.01\nData Augmentation (100 Human Dialogues)\nFinetuning 48.98 ±1.16 31.98±1.09 72.78±0.69 62.41±0.85\nBack-Translation (Sennrich et al., 2016) 48.41 ±0.96 28.10±2.51 69.18±2.82 57.72±3.28\nEDA (Wei and Zou, 2019) 46.86 ±0.61 29.14±1.71 60.61±4.23 49.24±4.74\nUtterance Manipulation (Chen and Yang, 2021) 49.07 ±1.06 31.77±1.86 69.23±0.21 59.15±0.74\nDialog Inpainting (Dai et al., 2022) 49.48 ±0.34 33.29±0.98 72.15±0.74 61.80±0.99\nAutoConv 51.21±1.02 34.65±1.00 74.84±0.24 64.36±0.46\nHuman Annotation 54.22 ±0.90 37.42±2.06 76.35±0.51 65.71±0.55\nTable 2: Comparison with baselines. All experiments are performed 4 runs with different random seeds. Finetuning\nmeans directly training with only human dialogues. All data augmentation methods use the same human dialogues\nand the same number of synthetic dialogues for the sake of fairness (5 times the number of human dialogues).\nHuman annotation represents replacing the synthetic dialogues with the same number of human dialogues.\nshown in Table 2, AutoConv achieves better perfor-\nmance than GPT-3 prompting on QuAC with only\n0.13% parameters and 50 human dialogues, but is\nless competitive on CoQA. We conjecture the rea-\nson stems from the intrinsic difference between the\ntwo datasets. CoQA contains more factoid ques-\ntions, and the answers are named entities or short\nnoun phrases like those in SQuAD (Rajpurkar et al.,\n2016). By training on large-scale text corpus from\na web forum, GPT-3 might implicitly learn the\nformat and structure of question answering (Sanh\net al., 2022), and thus gets excellent performance\non CoQA. On the other side, QuAC has more open-\nended and exploratory questions as in natural con-\nversations, and 86% questions are contextual (Choi\net al., 2018). Therefore, it brings more difficul-\nties for GPT-3 inference with few demonstrations,\nwhile our method learns better from both human\ndialogues and synthetic dialogues.\nCompared with data augmentation methods, Au-\ntoConv achieves the best performance on both\ndatasets and mitigates the gap between synthetic\ndialogues and human upper bounds. We find\nthat the token-level augmentation method EDA\nand the sentence-level augmentation method Back-\nTranslation even hurt the performance, which is\n0 1000 2000 4000 10000 20000\nThe number of synthetic dialogues.\n45\n48\n50\n52\n54\n56\n58\n60F1 Score\nFinetuning (50)\nFinetuning (100)\nFinetuning (200)\nFinetuning (500)\nFinetuning (1000)\nFinetuning (2000)\nAutoConv (50)\nAutoConv (100)\nAutoConv (200)\nAutoConv (500)\nFigure 2: The results of scaling up human dialogues\nand synthetic dialogues on QuAC. The number in the\nparentheses represents the number of human dialogues.\nsimilar to the observation in Chen et al. (2021).\nOne possible reason is that they bring too much\nnoise. Dialog Inpainting (Dai et al., 2022) gets\nordinary performance, and the reason possibly de-\nrives from the gap between the structure of natural\nconversations and that of the documents used for\nconstructing synthetic dialogues.\n3.3 Scaling up Human Dialogues and\nSynthetic Dialogues\nIn this part, we further analyze the performance of\nAutoConv when scaling up the human dialogues\nand synthetic dialogues. As shown in Figure 2, the\n1753\nModel #Params #FLOPs F1 (50) F1 (200)\nFinetuning (LLM) 12.9B 7049.3B 53.53 54.85\nFinetuning (STM) 222M 60.2B 47.97 50.38\nAutoConv(STM) 222M 60.2B 52.40 55.44\nTable 3: Comparison results on QuAC. Finetuning\nmeans training with only human dialogues. AutoConv\nuses the same human dialogues and 20K synthetic dia-\nlogues. LLM is large language model and STM is small\ntask model. The number in the parentheses represents\nthe number of human dialogues.\nperformance boosts when more human dialogues\nor synthetic dialogues are used. With 50 human\ndialogues, AutoConv outperforms the results of\nfinetuning with 200 human dialogues. With 500 hu-\nman dialogues, AutoConv gets competitive perfor-\nmance compared with finetuning with 2000 human\ndialogues. These results verify the high quality of\nsynthetic dialogues, and our AutoConv can largely\nalleviate the labor force for annotation.\n3.4 Comparison with Finetuned Large\nLanguage Model\nAutoConv is a kind of symbolic knowledge distilla-\ntion (West et al., 2022), where the finetuned large\nlanguage model (LLM) transfers its knowledge to\nthe small task model (STM) by generating synthetic\ndialogues for the training of STM. Here, we further\ninvestigate the effectiveness of AutoConv from the\naspect of knowledge distillation. As shown in Ta-\nble 3, finetuned LLM has substantial improvements\nover finetuned STM. However, it brings large mem-\nory and computation cost. On the other side, our\nAutoConv not only keeps the efficiency of STM,\nbut also boosts the performance. Surprisingly, Au-\ntoConv even outperforms its teacher model in the\n200 human dialogues setting. Similar observations\nare found in West et al. (2022); Ye et al. (2022),\nwhile they focus on different tasks. We leave the\nanalysis of this novel observation for future work.\n3.5 Impact of Decoding Strategy\nDuring our preliminary experiments, we find that\nthe decoding strategy is important for system an-\nswer generation. More precisely, we evaluate the\nanswer generation performance of LLM with dif-\nferent decoding strategies on QuAC, and the results\nare shown in Table 4. Though nucleus sampling\n(Holtzman et al., 2020) has shown great perfor-\nmance in various generation tasks (Su et al., 2022),\nit performs less competitively than maximization-\nDecoding Strategy F1 Exact Match\nNucleus Sampling ( p = 0.8) 50.77 32.63\nNucleus Sampling ( p = 0.9) 49.88 31.57\nGreedy Search 53.53 36.38\nBeam Search ( b = 4) 54.43 38.64\nBeam Search (b = 8) 54.43 38.70\nTable 4: The results of LLM with different decoding\nstrategies for answer generation on QuAC, 50 human\ndialogues are used for finetuning the LLM.\n0 350M 1.3B 2.7B 6.7B 13B\nModel Size (# Parameters)\n44\n46\n48\n50\n52F1 Score\n1K Synthetic Dialogues\n4K Synthetic Dialogues\n10K Synthetic Dialogues\nNo Synthetic Dialogues\nFigure 3: The results of AutoConv with different LLM\non QuAC. We use different scale of OPT (Zhang et al.,\n2022) as the LLM. All models are trained with 50 hu-\nman dialogues for fairness and synthetic dialogues are\ngenerated with the corresponding LLM.\nbased decoding strategies for answer generation.\nCompared with beam search, greedy search shows\ncompetitive performance and is more efficient.\nThus we use greedy search by default in this paper.\n3.6 Scaling Laws\nWe further analyze how the benefit of AutoConv is\naffected by the scale of LLM. As shown in Figure\n3, the performance gets better with a larger model\nacross a various number of synthetic dialogues. In\naddition, when the LM is small (350M) and with\nlimited generation ability, the synthetic dialogues\ncan even hurt the performance when the available\nhuman dialogues are scarce. Due to the limitation\nof computational resources, we limit our investiga-\ntion to 13B parameters and leave larger models for\nfuture work.\n3.7 Case Study\nIn Table 5, we present an example of our synthetic\nconversation for the case study. The original doc-\nument describes the singer Ciara’s second studio\nalbum and her acting debut. The conversation con-\nsists of seven user questions and seven system an-\nswers, covering the title and sales of the album, the\nduration of the tour, etc. As we can see from this\n1754\nTitle\n2006-2007: Ciara: The Evolution and acting debut\nDocument\nOn December 5, 2006, Ciara released her second\nstudio album, Ciara: The Evolution. According to\nthe singer, the title of the album is \\\"about so much\nmore than just my personal growth - it’s about the\nevolution of music, the evolution of dance, the\nevolution of fashion \\\" The source of the album ´s\ncreativity such as the sound and edge comes from\nCiara in general. Ciara: The Evolution became\nCiara´s first and only number one album on the U.S.\nBillboard 200, and her second number one on the\nTop R&B/Hip-Hop Albums charts with sales of\n338,000 in the first week . . .\nDialogue\nUsr: What was the evolution?\nSys: On December 5, 2006, Ciara released her sec-\nond studio album, Ciara: The Evolution.\nUsr: What was the name of the album?\nSys: The title of the album is \\\"about so much more\nthan just my personal growth - it’s about the evolu-\ntion of music, the evolution of dance, the evolution\nof fashion.\\\"\nUsr: Did the album do well?\nSys: The album became Ciara’s first number one\nalbum on the U.S. Billboard 200 and second on\nthe Top R&B/Hip-Hop Albums charts with sales\nof more than 338,000.\nUsr: What awards did she win if any?\nSys: The album went on to be certified platinum\nby the RIAA in the United States, within only five\nweeks of its release, and has sold 1.3 million copies\naccording to Nielsen SoundScan.\nUsr: What else did she accomplish during that\ntime?\nSys: In support of the album, Ciara went on her\nfirst headlining tour in October 2006.\nUsr: How long was the tour?\nSys: The tour went to seventeen different clubs in\ncities throughout the United States.\nUsr: Did she have success with any songs?\nSys: The lead single, \\\"Get Up\\\", which fea-\ntures Chamillionaire, reached number seven in the\nUnited States and gained a platinum accreditation.\nTable 5: An example of the synthetic conversation gener-\nated by AutoConv, the LLM is finetuned with 50 human\ndialogues from QuAC (Choi et al., 2018).\nexample, the user questions are diverse (e.g. what,\nhow, did, etc.) and the conversation is informative\nand conversational. For example, when the system\nmentions “tour” (the fifth system utterance), the\nuser follows by asking “How long was the tour?”.\n3.8 Error Analysis\nTo further analyze the limitation of our method, we\nconduct an error analysis by manually investigating\n50 synthetic conversations generated by AutoConv,\nwhich is finetuned with 50 human conversations\nfrom QuAC (Choi et al., 2018). Particularly, we\nfind that only 5% generated questions are not suit-\nable (e.g., misspelled names). The reason stems\nfrom the open-ended characteristic of natural con-\nversation that many kinds of user questions are\npossible under the same context. However, nearly\n40% of system answers are not perfect, and we sum-\nmarize the wrong answers into four major classes:\n(1) Irrelevant: 75% of them are totally irrelevant\nto user questions. (2) Related but not Accurate:\n14% of them contain related knowledge from the\ngrounded documents, but the answers are not accu-\nrate. Take an example in Table 5, the second user\nquestion asks for the name of the album, which is\nCiara: The Evolution according to the document.\nWhile the LLM generates the interpretation of the\nalbum name by mistake. (3) Missing: 4% of them\nbelong to the missing error that the system answers\nare “No Answer”, while the questions actually can\nbe answered based on the documents. (4) Halluci-\nnation: 3% of them mention hallucination knowl-\nedge, which cannot be found in the documents. In\naddition, we also notice that AutoConv is more\nlikely to generate wrong answers when grounding\non longer and more complex documents.\n4 Conclusion\nIn this paper, we propose a simple yet effective\nmethod, AutoConv, which formulates the conver-\nsation generation problem as a language modeling\ntask. Then, based on a large language model and a\nfew human dialogues, AutoConv can generate syn-\nthetic dialogues with high quality. Experimental\nresults on both QuAC and CoQA verify the effec-\ntiveness of AutoConv, which alleviates the human\nefforts for annotation largely. Furthermore, we also\nprovide case study and error analysis to prompt\nfuture research.\n1755\nLimitations\nIn this paper, we propose a method named Au-\ntoConv, which means automatically generating\ninformation-seeking conversations with large lan-\nguage models (LLM). Though it has achieved great\nperformance on both QuAC (Choi et al., 2018) and\nCoQA (Reddy et al., 2019), there are still some\nlimitations that should be noticed.\nLimitation of LLM. In our experiments, we use\nOPT-13B (Zhang et al., 2022) as the LLM for gen-\nerating synthetic conversations due to the limited\ncomputational resources. Larger models should be\nconsidered to further understand the potential abil-\nity of AutoConv, e.g., GPT-3 (Brown et al., 2020),\nOPT-175B (Zhang et al., 2022), BLOOM-176B\n(Scao et al., 2022), and GLM-130B (Zeng et al.,\n2022) etc.\nLimitation of Implementation. As mentioned\nin Section 2.2 and Appendix B, our method needs\nto finetune LLM and generate massive synthetic\nconversations based on the finetuned LLM, which\nhas a high cost for implementation.\nLimitation of Synthetic Dialogues.As shown in\nTable 2 and Section 3.8, there is still a gap between\nour synthetic dialogues and human dialogues. It\nis important to improve the quality of synthetic\ndialogues so that we can further alleviate the de-\npendence on human annotation.\nEthics Statement\nAutoConv is based on large language models\n(LLM), while LLM has some potential risks, e.g.,\nsocial bias (Liang et al., 2021), offensive content\n(Ganguli et al., 2022) etc. Fortunately, we fine-\ntune the LLM to capture the characteristics of the\ninformation-seeking process, and the generated\nconversations are mostly grounded on the provided\ndocuments (take an example in Table 5). Therefore,\nour method alleviates the potential risks of directly\nusing LLM. According to our manual check in error\nanalysis (Section 3.8), we do not find any harmful\ncontent in the synthetic conversations. In addition,\nwe also encourage considering more safety meth-\nods (Xu et al., 2020; Sun et al., 2022) to guarantee\nthe quality of synthetic conversations.\nAcknowledgements\nThis work was partly supported by the National\nKey Research and Development Program of China\n(No. 2020YFB1708200) , the \"Graph Neural Net-\nwork Project\" of Ping An Technology (Shenzhen)\nCo., Ltd. and AMiner.Shenzhen SciBrain fund.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nJiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, and\nDiyi Yang. 2021. An empirical survey of data aug-\nmentation for limited data learning in NLP. CoRR,\nabs/2106.07499.\nJiaao Chen and Diyi Yang. 2021. Simple conversa-\ntional data augmentation for semi-supervised abstrac-\ntive dialogue summarization. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 6605–6616. Association for Computa-\ntional Linguistics.\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-\ntau Yih, Yejin Choi, Percy Liang, and Luke Zettle-\nmoyer. 2018. Quac: Question answering in context.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, Brus-\nsels, Belgium, October 31 - November 4, 2018, pages\n2174–2184. Association for Computational Linguis-\ntics.\nZhuyun Dai, Arun Tejasvi Chaganty, Vincent Y . Zhao,\nAida Amini, Qazi Mamunur Rashid, Mike Green, and\nKelvin Guu. 2022. Dialog inpainting: Turning docu-\nments into dialogs. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research , pages 4558–4586.\nPMLR.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda\nAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,\nEthan Perez, Nicholas Schiefer, Kamal Ndousse,\nAndy Jones, Sam Bowman, Anna Chen, Tom Con-\nerly, Nova DasSarma, Dawn Drain, Nelson Elhage,\nSheer El Showk, Stanislav Fort, Zac Hatfield-Dodds,\nTom Henighan, Danny Hernandez, Tristan Hume,\nJosh Jacobson, Scott Johnston, Shauna Kravec,\nCatherine Olsson, Sam Ringer, Eli Tran-Johnson,\n1756\nDario Amodei, Tom Brown, Nicholas Joseph, Sam\nMcCandlish, Chris Olah, Jared Kaplan, and Jack\nClark. 2022. Red teaming language models to re-\nduce harms: Methods, scaling behaviors, and lessons\nlearned. CoRR, abs/2209.07858.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nDaniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-\njishirzi. 2022. Unifiedqa-v2: Stronger general-\nization via broader cross-format training. CoRR,\nabs/2202.12359.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\naries with a single QA system. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, Online Event, 16-20 November 2020, volume\nEMNLP 2020 of Findings of ACL, pages 1896–1907.\nAssociation for Computational Linguistics.\nGangwoo Kim, Sungdong Kim, Kang Min Yoo, and\nJaewoo Kang. 2022. Towards more realistic gener-\nation of information-seeking conversations. CoRR,\nabs/2205.12609.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nSosuke Kobayashi. 2018. Contextual augmentation:\nData augmentation by words with paradigmatic rela-\ntions. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 2 (Short Papers), pages 452–\n457. Association for Computational Linguistics.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and\nRuslan Salakhutdinov. 2021. Towards understanding\nand mitigating social biases in language models. In\nProceedings of the 38th International Conference\non Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 ofProceedings of Machine\nLearning Research, pages 6565–6576. PMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nThe Association for Computational Linguistics.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In KDD ’20: The 26th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Virtual Event, CA, USA, August\n23-27, 2020, pages 3505–3506. ACM.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. Coqa: A conversational question answering\nchallenge. Trans. Assoc. Comput. Linguistics, 7:249–\n266.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, Jonathan Tow, Alexander M. Rush,\nStella Biderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Benoît Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-\ntor Sanh, Hugo Laurençon, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, and et al. 2022. BLOOM:\nA 176b-parameter open-access multilingual language\nmodel. CoRR, abs/2211.05100.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers. The Association\nfor Computer Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\n1757\nof the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 3784–\n3803. Association for Computational Linguistics.\nManfred Stede and David Schlangen. 2004.\nInformation-seeking chat: Dialogues driven\nby topic-structure. In Proceedings of Catalog (the\n8th workshop on the semantics and pragmatics of\ndialogue; SemDial04). Citeseer.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama,\nLingpeng Kong, and Nigel Collier. 2022. A con-\ntrastive framework for neural text generation. CoRR,\nabs/2202.06417.\nHao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng,\nChujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan\nZhu, and Minlie Huang. 2022. On the safety of con-\nversational models: Taxonomy, dataset, and bench-\nmark. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 3906–3923. Association for Com-\nputational Linguistics.\nJason W. Wei and Kai Zou. 2019. EDA: easy data\naugmentation techniques for boosting performance\non text classification tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 6381–6387. Association for\nComputational Linguistics.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D.\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, NAACL 2022, Seattle,\nWA, United States, July 10-15, 2022 , pages 4602–\n4625. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing: System Demon-\nstrations, EMNLP 2020 - Demos, Online, November\n16-20, 2020, pages 38–45. Association for Computa-\ntional Linguistics.\nQingyang Wu, Song Feng, Derek Chen, Sachindra Joshi,\nLuis A. Lastras, and Zhou Yu. 2021. DG2: data\naugmentation through document grounded dialogue\ngeneration. CoRR, abs/2112.08342.\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong,\nand Quoc Le. 2020a. Unsupervised data augmenta-\ntion for consistency training. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nQizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and\nQuoc V . Le. 2020b. Self-training with noisy student\nimproves imagenet classification. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, CVPR 2020, Seattle, WA, USA, June 13-19,\n2020, pages 10684–10695. Computer Vision Founda-\ntion / IEEE.\nJin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang\nLi, and Jian Li. 2022. Learning to break the loop:\nAnalyzing and mitigating repetitions for neural text\ngeneration. CoRR, abs/2206.02369.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason\nWeston, and Emily Dinan. 2020. Recipes for safety\nin open-domain chatbots. CoRR, abs/2010.07079.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022. Zerogen: Efficient zero-shot learning via\ndataset generation. CoRR, abs/2202.07922.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan\nMa, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng\nZhang, Yuxiao Dong, and Jie Tang. 2022. GLM-\n130B: an open bilingual pre-trained model. CoRR,\nabs/2210.02414.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nChujie Zheng, Sahand Sabour, Jiaxin Wen, and Minlie\nHuang. 2022. Augesc: Large-scale data augmen-\ntation for emotional support conversation with pre-\ntrained language models. CoRR, abs/2202.13047.\n1758\nA Datasets\nQuAC. QuAC (Choi et al., 2018) is a leading con-\nversational question answering dataset, consists of\n14K information-seeking dialogues. Different from\nthe factoid questions in most existing QA datasets,\nthe questions in QuAC are more open-ended and\nexploratory. In addition, 86% of questions are con-\ntextual, and the model needs to understand the di-\nalogue context to resolve coreference. As the test\nset is only available in the QuAC challenge 2, we\nevaluate the performance on the development set.\nCoQA. CoQA (Reddy et al., 2019) consists of\n127K conversational QA pairs across seven do-\nmains. Different from QuAC, CoQA focus more\non factoid questions, and the answers are mostly\nnamed entities or short phrases as in SQuAD (Ra-\njpurkar et al., 2016). The test set of CoQA is only\navailable in the CoQA challenge 3, therefore we\nevaluate the performance on the development set.\nB Implementation Details\nGeneral Setting. All experiments are based on\nTransformers4 (Wolf et al., 2020), DeepSpeed 5\n(Rasley et al., 2020) and Pytorch Lightning6. We\nuse UnifiedQA-V2-base7 (Khashabi et al., 2020,\n2022) as the small task model, which is based on T5\narchitecture with 222M parameters and pre-trained\non many QA tasks (the tasks in our experiments\nare not included in). The training of the small task\nmodel follows the original paper (Khashabi et al.,\n2020) in a Text-to-Text framework (Raffel et al.,\n2020). The input is Dialogue History \\n Document\nand the output is System Answer.\nFor the training hyperparameters, we set the\nlearning rate as 3e −4, batch size as 32, and\nuse Adam optimizer (Kingma and Ba, 2015) with\nwarmup learning rate schedule, the warmup ratio\nis 0.1. When comparing with baseline methods\nas in Section 3.2, all methods use the same small\ntask model, the same two-stage training strategy\n(Xie et al., 2020b; Chen and Yang, 2021), the same\nhuman dialogues and the same number of synthetic\ndialogues for fairness (5 times the number of hu-\n2https://quac.ai/\n3https://stanfordnlp.github.io/coqa/\n4https://huggingface.co/docs/transformers/\nindex\n5https://github.com/microsoft/DeepSpeed\n6https://github.com/Lightning-AI/lightning\n7https://huggingface.co/allenai/\nunifiedqa-v2-t5-base-1363200\nman dialogues). For the 50 human dialogues set-\nting, we train each model for 1K gradient steps in\nthe pre-training stage and 200 gradient steps in the\nfintuning stage. For the 100 human dialogues set-\nting, the steps are 2K and 400 respectively. When\nscaling up the number of synthetic dialogues as in\nSection 3.3 and Section 3.6, the numbers of pre-\ntraining steps scale up, which are 2K, 4K, 8K, 20K\nand 40K for 1K, 2K, 4K, 10K and 20K synthetic\ndialogues respectively, and the finetuning steps are\n200, 400, 800 and 2K for 50, 100, 200 and 500\nhuman dialogues respectively. For all experiments,\nwe randomly sample 20% dialogues as the valida-\ntion set, and others as the training set. The model\nis validated every epoch, and we choose the check-\npoint with the best F1 score on the validation set\nfor evaluation.\nOurs. We use OPT-13B8 (Zhang et al., 2022) as\nthe LLM for generating synthetic dialogues, which\nis a decoder-only pre-trained language model with\n13B parameters. The learning rate and batch size\nare set as 1e-5 and 32. Adam optimizer (Kingma\nand Ba, 2015) with warmup learning rate schedule\nis utilized for optimization and the warmup ratio is\n0.1. The max training steps of LLM are 200, 400,\n800 and 2K for 50, 100, 200 and 500 human dia-\nlogues respectively. According to the performance\nof AutoConv on the validation set of human dia-\nlogues, we find that training LLM for 4 epochs is\nthe most suitable. We randomly sample 5K docu-\nments from the training sets of QuAC and CoQA,\nand generate 8 synthetic dialogues for each docu-\nment. The number of turn is set as 14 for QuAC\nand 30 for CoQA. Then, we filter a quarter of the\nsynthetic dialogues based on the diversity score of\neach dialogue as in Su et al. (2022), which takes\ninto account the repetition at different n-gram lev-\nels. It takes around 5 hours for training LLM and\n18 hours for generating synthetic dialogues with 8\nTesla V100 32GB GPUs.\nEvaluation. To evaluate the quality of synthetic\nconversations, we evaluate the conversational ques-\ntion answering performance of the small task\nmodel, which is trained on both synthetic conversa-\ntions and a few human conversations. The metrics\nare Exact Match and word-level F1 as in Choi et al.\n(2018).\n8https://huggingface.co/facebook/opt-13b\n1759\nC Baselines\nPrompting. Prompting is a promising method\nfor many NLP tasks. It aims to elicit the ability of\nlarge language models learned from pre-training\nwith text demonstrations (e.g., task instruction and\nfew-shot examples etc). In Table 2, we report the\nresults from Brown et al. (2020).\nFinetuning. Train the small task model with only\nhuman annotations.\nEDA. Easy Data Augmentation (EDA) is a sim-\nple but effective method for text classification (Wei\nand Zou, 2019). Given an input text, including\nboth the knowledge paragraph and dialogue history\nin our experiments, four operations are applied to\ncreate new examples, including synonym replace-\nment, random insertion, random swap and random\ndeletion. We use their open source code9 for imple-\nmentation.\nBack-Translation. Back-Translation is one of\nthe most popular augmentation method for NLP\ntasks (Sennrich et al., 2016; Xie et al., 2020a).\nSpecifically, we first translate the input text to a tar-\nget language, then translate it back to the source lan-\nguage, thus we can get a paraphrased example. To\nget various augmentations for each sample, we use\nfive target languages, including Chinese, French,\nGerman, Arabic, and Korean. Huawei Translate10\nis used for the translation process.\nUtterance Manipulation. Chen and Yang (2021)\npropose utterance-level manipulation to perturb the\ndiscourse relations in the conversation. Two simple\noperations are used: (1) random swapping, which\nrandomly swaps two utterances to mess up the logic\nchain of the conversation, and (2) random deletion,\nwhich means randomly deleting an utterance to im-\nprove the discourse diversity. We randomly select\none operation for each augmentation.\nDialog Inpainting. The state-of-the-art data aug-\nmentation method for conversational question an-\nswering. Given a document, they iteratively insert\ngenerated utterances between the consecutive sen-\ntences in the document, then the utterances and\nsentences can form an informative conversation\n(Dai et al., 2022). We randomly sample generated\n9https://github.com/jasonwei20/eda_nlp\n10https://www.huaweicloud.com/product/nlpmt.\nhtml\ndialogues from their open source data11.\n11https://github.com/google-research/\ndialog-inpainting\n1760\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthics Statement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nAppendix A and Appendix B\n□\u0013 B1. Did you cite the creators of artifacts you used?\nAppendix A\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nEthics Statement and Appendix A\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAppendix A\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix A and Appendix B\nC □\u0013 Did you run computational experiments?\nSection 3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1761\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix B\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n1762",
  "topic": "Conversation",
  "concepts": [
    {
      "name": "Conversation",
      "score": 0.9494211673736572
    },
    {
      "name": "Computer science",
      "score": 0.8035281896591187
    },
    {
      "name": "Scarcity",
      "score": 0.5890599489212036
    },
    {
      "name": "Task (project management)",
      "score": 0.5781514048576355
    },
    {
      "name": "Process (computing)",
      "score": 0.5565534830093384
    },
    {
      "name": "Language model",
      "score": 0.49214762449264526
    },
    {
      "name": "Annotation",
      "score": 0.46279335021972656
    },
    {
      "name": "Information seeking",
      "score": 0.4580070674419403
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4527301490306854
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44115710258483887
    },
    {
      "name": "Natural language processing",
      "score": 0.39585787057876587
    },
    {
      "name": "Data science",
      "score": 0.3204490542411804
    },
    {
      "name": "Information retrieval",
      "score": 0.24010145664215088
    },
    {
      "name": "Linguistics",
      "score": 0.13027030229568481
    },
    {
      "name": "Engineering",
      "score": 0.07173943519592285
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3131625388",
      "name": "University Town of Shenzhen",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ]
}