{
  "title": "Selecting relevant text subsets from web-data for building topic specific language models",
  "url": "https://openalex.org/W2057589672",
  "year": 2006,
  "authors": [
    {
      "id": "https://openalex.org/A2008850676",
      "name": "Abhinav Sethy",
      "affiliations": [
        "University of Southern California",
        "Engineering Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2118032982",
      "name": "Panayiotis G. Georgiou",
      "affiliations": [
        "University of Southern California",
        "Engineering Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2632368822",
      "name": "Shrikanth Narayanan",
      "affiliations": [
        "University of Southern California",
        "Engineering Systems (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W161909597",
    "https://openalex.org/W2134510195",
    "https://openalex.org/W2000577984",
    "https://openalex.org/W2113632140",
    "https://openalex.org/W2164666834",
    "https://openalex.org/W2097089247",
    "https://openalex.org/W2136504847",
    "https://openalex.org/W2105830342",
    "https://openalex.org/W2047295649"
  ],
  "abstract": "In this paper we present a scheme to select relevant subsets of sentences from a large generic corpus such as text acquired from the web.A relative entropy (R.E) based criterion is used to incrementally select sentences whose distribution matches the domain of interest.Experimental results show that by using the proposed subset selection scheme we can get significant performance improvement in both Word Error Rate (WER) and Perplexity (PPL) over the models built from the entire web-corpus by using just 10% of the data.In addition incremental data selection enables us to achieve significant reduction in the vocabulary size as well as number of n-grams in the adapted language model.To demonstrate the gains from our method we provide a comparative analysis with a number of methods proposed in recent language modeling literature for cleaning up text.",
  "full_text": null,
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8985799551010132
    },
    {
      "name": "Computer science",
      "score": 0.8039817810058594
    },
    {
      "name": "Language model",
      "score": 0.6565978527069092
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6147520542144775
    },
    {
      "name": "Vocabulary",
      "score": 0.5739918351173401
    },
    {
      "name": "Natural language processing",
      "score": 0.547760546207428
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5133547186851501
    },
    {
      "name": "Word error rate",
      "score": 0.4936746060848236
    },
    {
      "name": "Word (group theory)",
      "score": 0.4911520183086395
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.4539667069911957
    },
    {
      "name": "Model selection",
      "score": 0.4155643582344055
    },
    {
      "name": "Mathematics",
      "score": 0.09804192185401917
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}