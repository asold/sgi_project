{
  "title": "Topically Driven Neural Language Model",
  "url": "https://openalex.org/W2608962050",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2095936123",
      "name": "Jey Han Lau",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2096893938",
      "name": "Timothy Baldwin",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2188741563",
      "name": "Trevor Cohn",
      "affiliations": [
        "University of Melbourne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2097606805",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2171343266",
    "https://openalex.org/W2166354010",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2147946282",
    "https://openalex.org/W2159426623",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2464598814",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W1916589227",
    "https://openalex.org/W2100002341",
    "https://openalex.org/W2098062695",
    "https://openalex.org/W2251582277",
    "https://openalex.org/W2295103979",
    "https://openalex.org/W2130339025",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2157006255",
    "https://openalex.org/W2197590357",
    "https://openalex.org/W2250533720",
    "https://openalex.org/W2112971401",
    "https://openalex.org/W2172140247",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2342395274",
    "https://openalex.org/W2144100511",
    "https://openalex.org/W3037881859",
    "https://openalex.org/W2952230511"
  ],
  "abstract": "Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.",
  "full_text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 355–365\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1033\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 355–365\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1033\nTopically Driven Neural Language Model\nJey Han Lau1,2 Timothy Baldwin2 Trevor Cohn2\n1 IBM Research\n2 School of Computing and Information Systems,\nThe University of Melbourne\njeyhan.lau@gmail.com, tb@ldwin.net, t.cohn@unimelb.edu.au\nAbstract\nLanguage models are typically applied at\nthe sentence level, without access to the\nbroader document context. We present a\nneural language model that incorporates\ndocument context in the form of a topic\nmodel-like architecture, thus providing\na succinct representation of the broader\ndocument context outside of the current\nsentence. Experiments over a range of\ndatasets demonstrate that our model out-\nperforms a pure sentence-based model in\nterms of language model perplexity, and\nleads to topics that are potentially more co-\nherent than those produced by a standard\nLDA topic model. Our model also has the\nability to generate related sentences for a\ntopic, providing another way to interpret\ntopics.\n1 Introduction\nTopic models provide a powerful tool for extract-\ning the macro-level content structure of a docu-\nment collection in the form of the latent topics\n(usually in the form of multinomial distributions\nover terms), with a plethora of applications in NLP\n(Hall et al., 2008; Newman et al., 2010a; Wang\nand McCallum, 2006). A myriad of variants of\nthe classical LDA method (Blei et al., 2003) have\nbeen proposed, including recent work on neural\ntopic models (Cao et al., 2015; Wan et al., 2012;\nLarochelle and Lauly, 2012; Hinton and Salakhut-\ndinov, 2009).\nSeparately, language models have long been a\nfoundational component of any NLP task involv-\ning generation or textual normalisation of a noisy\ninput (including speech, OCR and the processing\nof social media text). The primary purpose of a\nlanguage model is to predict the probability of a\nspan of text, traditionally at the sentence level, un-\nder the assumption that sentences are independent\nof one another, although recent work has started\nusing broader local context such as the preceding\nsentences (Wang and Cho, 2016; Ji et al., 2016).\nIn this paper, we combine the beneﬁts of a\ntopic model and language model in proposing\na topically-driven language model, whereby we\njointly learn topics and word sequence informa-\ntion. This allows us to both sensitise the predic-\ntions of the language model to the larger docu-\nment narrative using topics, and to generate topics\nwhich are better sensitised to local context and are\nhence more coherent and interpretable.\nOur model has two components: a language\nmodel and a topic model. We implement both\ncomponents using neural networks, and train them\njointly by treating each component as a sub-task\nin a multi-task learning setting. We show that our\nmodel is superior to other language models that\nleverage additional context, and that the generated\ntopics are potentially more coherent than LDA\ntopics. The architecture of the model provides\nan extra dimensionality of topic interpretability,\nin supporting the generation of sentences from a\ntopic (or mix of topics). It is also highly ﬂex-\nible, in its ability to be supervised and incor-\nporate side information, which we show to fur-\nther improve language model performance. An\nopen source implementation of our model is avail-\nable at: https://github.com/jhlau/\ntopically-driven-language-model.\n2 Related Work\nGrifﬁths et al. (2004) propose a model that learns\ntopics and word dependencies using a Bayesian\nframework. Word generation is driven by either\nLDA or an HMM. For LDA, a word is generated\nbased on a sampled topic in the document. For the\n355\nDocument context\n(n x e)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTopic input A\n(k x a)\nTopic output B\n(k x b)\nSoftmax\nConvolutional Max-over-time\npooling\nFully connected with softmax output\nAttention distribution p\nTopic model output\nDocument-topic \nrepresentation s\nneural\nLanguage model output\nModern\nnetwork\napproach\nnetwork\nLanguage model\nTopic model\n \nx\ng\nlstm\nlstm\nlstm\nNeural\nNetworks\nare\na\ncomputational\napproach\nwhich\nis\nbased\non Document vector d\nFigure 1: Architecture of tdlm. Scope of the models are denoted by dotted lines: blue line denotes the\nscope of the topic model, red the language model.\nHMM, a word is conditioned on previous words.\nA key difference over our model is that their lan-\nguage model is driven by an HMM, which uses a\nﬁxed window and is therefore unable to track long-\nrange dependencies.\nCao et al. (2015) relate the topic model view\nof documents and words — documents having\na multinomial distribution over topics and top-\nics having a multinomial distributional over words\n— from a neural network perspective by embed-\nding these relationships in differentiable functions.\nWith that, the model lost the stochasticity and\nBayesian inference of LDA but gained non-linear\ncomplex representations. The authors further pro-\npose extensions to the model to do supervised\nlearning where document labels are given.\nWang and Cho (2016) and Ji et al. (2016) re-\nlax the sentence independence assumption in lan-\nguage modelling, and use preceeding sentences as\nadditional context. By treating words in preceed-\ning sentences as a bag of words, Wang and Cho\n(2016) use an attentional mechanism to focus on\nthese words when predicting the next word. The\nauthors show that the incorporation of additional\ncontext helps language models.\n3 Architecture\nThe architecture of the proposed topically-driven\nlanguage model (henceforth “tdlm”) is illustrated\nin Figure 1. There are two components intdlm: a\nlanguage model and a topic model. The language\nmodel is designed to capture word relations in sen-\ntences, while the topic model learns topical infor-\nmation in documents. The topic model works like\nan auto-encoder, where it is given the document\nwords as input and optimised to predict them.\nThe topic model takes in word embeddings of\na document and generates a document vector us-\ning a convolutional network. Given the document\nvector, we associate it with the topics via an atten-\ntion scheme to compute a weighted mean of topic\nvectors, which is then used to predict a word in the\ndocument.\nThe language model is a standard LSTM lan-\nguage model (Hochreiter and Schmidhuber, 1997;\nMikolov et al., 2010), but it incorporates the\nweighted topic vector generated by the topic\nmodel to predict succeeding words.\n356\nMarrying the language and topic models allows\nthe language model to be topically driven, i.e. it\nmodels not just word contexts but also the doc-\nument context where the sentence occurs, in the\nform of topics.\n3.1 Topic Model Component\nLet xi ∈Re be the e-dimensional word vector for\nthe i-th word in the document. A document of n\nwords is represented as a concatenation of its word\nvectors:\nx1:n = x1 ⊕x2 ⊕...⊕xn\nwhere ⊕denotes the concatenation operator. We\nuse a number of convolutional ﬁlters to process the\nword vectors, but for clarity we will explain the\nnetwork with one ﬁlter.\nLet wv ∈Reh be a convolutional ﬁlter which\nwe apply to a window ofhwords to generate a fea-\nture. A feature ci for a window of words xi:i+h−1\nis given as follows:\nci = I(w⊺\nvxi:i+h−1 + bv)\nwhere bv is a bias term and I is the identity func-\ntion.1 A feature map c is a collection of features\ncomputed from all windows of words:\nc = [c1,c2,...,c n−h+1]\nwhere c ∈Rn−h+1. To capture the most salient\nfeatures in c, we apply a max-over-time pool-\ning operation (Collobert et al., 2011), yielding a\nscalar:\nd= max\ni\nci\nIn the case where we use a ﬁlters, we have\nd ∈Ra, and this constitutes the vector represen-\ntation of the document generated by the convolu-\ntional and max-over-time pooling network.\nThe topic vectors are stored in two lookup tables\nA ∈Rk×a (input vector) and B ∈Rk×b (output\nvector), where kis the number of topics, andaand\nbare the dimensions of the topic vectors.\nTo align the document vector d with the topics,\nwe compute an attention vector which is used to\n1A non-linear function is typically used here, but prelimi-\nnary experiments suggest that the identity function works best\nfor tdlm.\ncompute a document-topic representation:2\np = softmax(Ad) (1)\ns = B⊺p (2)\nwhere p ∈ Rk and s ∈ Rb. Intuitively, s is a\nweighted mean of topic vectors, with the weight-\ning given by the attentionp. This is inspired by the\ngenerative process of LDA, whereby documents\nare deﬁned as having a multinomial distribution\nover topics.\nFinally s is connected to a dense layer with soft-\nmax output to predict each word in the document,\nwhere each word is generated independently as a\nunigram bag-of-words, and the model is optimised\nusing categorical cross-entropy loss. In practice,\nto improve efﬁciency we compute loss for pre-\ndicting a sequence of m1 words in the document,\nwhere m1 is a hyper-parameter.\n3.2 Language Model Component\nThe language model is implemented using LSTM\nunits (Hochreiter and Schmidhuber, 1997):\nit = σ(Wivt + Uiht−1 + bi)\nft = σ(Wf vt + Uf ht−1 + bf )\not = σ(Wovt + Uoht−1 + bi)\nˆct = tanh(Wcvt + Ucht−1 + bc)\nct = ft ⊙ct−1 + it ⊙ˆct\nht = ot ⊙tanh(ct)\nwhere ⊙denotes element-wise product; it, ft, ot\nare the input, forget and output activations respec-\ntively at time step t; and vt, ht and ct are the in-\nput word embedding, LSTM hidden state, and cell\nstate, respectively. Hereinafter W, U and b are\nused to refer to the model parameters.\nTraditionally, a language model operates at the\nsentence level, predicting the next word given its\nhistory of words in the sentence. The language\nmodel of tdlm incorporates topical information\nby assimilating the document-topic representation\n(s) with the hidden output of the LSTM ( ht) at\neach time step t. To prevent tdlm from memoris-\ning the next word via the topic model network, we\nexclude the current sentence from the document\ncontext.\n2The attention mechanism was inspired by memory net-\nworks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar\net al., 2015; Tran et al., 2016). We explored various attention\nstyles (including traditional schemes which use one vector for\na topic), but found this approach to work best.\n357\nWe use a gating unit similar to a GRU (Cho\net al., 2014; Chung et al., 2014) to allow tdlm\nto learn the degree of inﬂuence of topical informa-\ntion on the language model:\nzt = σ(Wzs + Uzht + bz)\nrt = σ(Wrs + Urht + br)\nˆht = tanh(Whs + Uh(rt ⊙ht) +bh)\nh′\nt = (1−zt) ⊙ht + zt ⊙ˆht\n(3)\nwhere zt and rt are the update and reset gate acti-\nvations respectively at timestept. The new hidden\nstate h′\nt is connected to a dense layer with linear\ntransformation and softmax output to predict the\nnext word, and the model is optimised using stan-\ndard categorical cross-entropy loss.\n3.3 Training and Regularisation\ntdlm is trained using minibatches and SGD.3 For\nthe language model, a minibatch consists of a\nbatch of sentences, while for the topic model it is\na batch of documents (each predicting a sequence\nof m1 words).\nWe treat the language and topic models as sub-\ntasks in a multi-task learning setting, and train\nthem jointly using categorical cross-entropy loss.\nMost parameters in the topic model are shared by\nthe language model, as illustrated by their scopes\n(dotted lines) in Figure 1.\nHyper-parameters of tdlm are detailed in Ta-\nble 1. Word embeddings for the topic model and\nlanguage model components are not shared, al-\nthough their dimensions are the same ( e).4 For\nm1, m2 and m3, sequences/documents shorter\nthan these thresholds are padded. Sentences\nlonger than m2 are broken into multiple se-\nquences, and documents longer than m3 are trun-\ncated. Optimal hyper-parameter settings are tuned\nusing the development set; the presented values\nare used for experiments in Sections 4 and 5.\nTo regularise tdlm, we use dropout regularisa-\ntion (Srivastava et al., 2014). We apply dropout to\nd and s in the topic model, and to the input word\nembedding and hidden output of the LSTM in the\nlanguage model (Pham et al., 2013; Zaremba et al.,\n2014).\n4 Language Model Evaluation\nWe use standard language model perplexity as the\nevaluation metric. In terms of dataset, we use doc-\n3We use Adam as the optimiser (Kingma and Ba, 2014).\n4Word embeddings are updated during training.\nument collections from 3 sources: APNEWS , IMDB\nand BNC . APNEWS is a collection of Associated\nPress5 news articles from 2009 to 2016. IMDB is\na set of movie reviews collected by Maas et al.\n(2011). BNC is the written portion of the British\nNational Corpus (BNC Consortium, 2007), which\ncontains excerpts from journals, books, letters, es-\nsays, memoranda, news and other types of text.\nFor APNEWS and BNC , we randomly sub-sample a\nset of documents for our experiments.\nFor preprocessing, we tokenise words and sen-\ntences using Stanford CoreNLP (Klein and Man-\nning, 2003). We lowercase all word tokens, ﬁlter\nword types that occur less than 10 times, and ex-\nclude the top 0.1% most frequent word types.6 We\nadditionally remove stopwords for the topic model\ndocument context. 7 All datasets are partitioned\ninto training, development and test sets; prepro-\ncessed dataset statistics are presented in Table 2.\nWe tune hyper-parameters of tdlm based on\ndevelopment set language model perplexity. In\ngeneral, we ﬁnd that optimal settings are fairly ro-\nbust across collections, with the exception of m3,\nas document length is collection dependent; opti-\nmal hyper-parameter values are given in Table 1.\nIn terms of LSTM size, we explore 2 settings: a\nsmall model with 1 LSTM layer and 600 hidden\nunits, and a large model with 2 layers and 900\nhidden units.8 For the topic number, we experi-\nment with 50, 100 and 150 topics. Word embed-\ndings are pre-trained 300-dimension word2vec\nGoogle News vectors.9\nFor comparison, we compare tdlm with:10\nvanilla-lstm: A standard LSTM language\nmodel, using the same tdlm hyper-parameters\nwhere applicable. This is the baseline model.\nlclm: A larger context language model that\nincorporates context from preceding sentences\n(Wang and Cho, 2016), by treating the preced-\ning sentence as a bag of words, and using an\n5https://www.ap.org/en-gb/.\n6For the topic model, we remove word tokens that corre-\nspond to these ﬁltered word types; for the language model we\nrepresent them as ⟨unk⟩ tokens (as for unseen words in test).\n7We use Mallet’s stopword list: https://github.\ncom/mimno/Mallet/tree/master/stoplists.\n8Multi-layer LSTMs are vanilla stacked LSTMs without\nskip connections (Gers and Schmidhuber, 2000) or depth-\ngating (Yao et al., 2015).\n9https://code.google.com/archive/p/\nword2vec/.\n10Note that all models use the same pre-trained\nword2vec vectors.\n358\nHyper- Value Descriptionparameter\nm1 3 Output sequence length for topic model\nm2 30 Sequence length for language model\nm3 300,150,500 Maximum document length\nnbatch 64 Minibatch size\nnlayer 1,2 Number of LSTM layers\nnhidden 600,900 LSTM hidden size\nnepoch 10 Number of training epochs\nk 100,150,200 Number of topics\ne 300 Word embedding size\nh 2 Convolutional ﬁlter width\na 20 Topic input vector size or number of features for convolutional ﬁlter\nb 50 Topic output vector size\nl 0.001 Learning rate of optimiser\np1 0.4 Topic model dropout keep probability\np2 0.6 Language model dropout keep probability\nTable 1: tdlm hyper-parameters; we experiment with 2 LSTM settings and 3 topic numbers, and m3\nvaries across the three domains (APNEWS , IMDB , and BNC ).\nCollection Training Development Test\n#Docs #Tokens #Docs #Tokens #Docs #Tokens\nAPNEWS 50K 15M 2K 0.6M 2K 0.6M\nIMDB 75K 20M 12.5K 0.3M 12.5K 0.3M\nBNC 15K 18M 1K 1M 1K 1M\nTable 2: Preprocessed dataset statistics.\nattentional mechanism when predicting the next\nword. An additional hyper-parameter in lclm is\nthe number of preceeding sentences to incorpo-\nrate, which we tune based on a development set\n(to 4 sentences in each case). All other hyper-\nparameters (such as nbatch, e, nepoch, k2) are the\nsame as tdlm.\nlstm+lda: A standard LSTM language model\nthat incorporates LDA topic information. We ﬁrst\ntrain an LDA model (Blei et al., 2003; Grifﬁths\nand Steyvers, 2004) to learn 50/100/150 topics for\nAPNEWS , IMDB and BNC .11 For a document, the\nLSTM incorporates the LDA topic distribution (q)\nby concatenating it with the output hidden state\n(ht) to predict the next word (i.e. h′\nt = ht ⊕q).\nThat is, it incorporates topical information into the\nlanguage model, but unlike tdlm the language\nmodel and topic model are trained separately.\nWe present language model perplexity perfor-\nmance in Table 3. All models outperform the base-\nline vanilla-lstm, with tdlm performing the\n11Based on Gibbs sampling; α= 0.1, β = 0.01.\nbest across all collections. lclm is competitive\nover the BNC , although the superiority oftdlm for\nthe other collections is substantial. lstm+lda\nperforms relatively well over APNEWS and IMDB ,\nbut very poorly over BNC .\nThe strong performance of tdlm over lclm\nsuggests that compressing document context into\ntopics beneﬁts language modelling more than us-\ning extra context words directly.12 Overall, our re-\nsults show that topical information can help lan-\nguage modelling and that joint inference of topic\nand language model produces the best results.\n5 Topic Model Evaluation\nWe saw that tdlm performs well as a language\nmodel, but it is also a topic model, and like LDA it\nproduces: (1) a probability distribution over topics\nfor each document (Equation (1)); and (2) a prob-\nability distribution over word types for each topic.\n12The context size of lclm (4 sentences) is technically\nsmaller than tdlm (full document), however, note that in-\ncreasing the context size does not beneﬁt lclm, as the con-\ntext size of 4 gives the best performance.\n359\nDomain LSTM Size vanilla- lclm lstm+lda tdlm\nlstm 50 100 150 50 100 150\nAPNEWS small 64.13 54.18 57.05 55.52 54.83 53.00 52.75 52.65\nlarge 58.89 50.63 52.72 50.75 50.17 48.96 48.97 48.21\nIMDB small 72.14 67.78 69.58 69.64 69.62 63.67 63.45 63.82\nlarge 66.47 67.86 63.48 63.04 62.78 58.99 59.04 58.59\nBNC small 102.89 87.47 96.42 96.50 96.38 87.42 85.99 86.43\nlarge 94.23 80.68 88.42 87.77 87.28 82.62 81.83 80.58\nTable 3: Language model perplexity performance of all models overAPNEWS , IMDB and BNC . Boldface\nindicates best performance in each row.\nlda ntm tdlm-small tdlm-large0.05\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n(a) APNEWS\nlda ntm tdlm-small tdlm-large0.05\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35 (b) IMDB\nlda ntm tdlm-small tdlm-large0.05\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35 (c) BNC\nFigure 2: Boxplots of topic coherence of all models; number of topics = 100.\nRecall that s is a weighted mean of topic vec-\ntors for a document (Equation (2)). Generating\nthe vocabulary distribution for a particular topic is\ntherefore trivial: we can do so by treatings as hav-\ning maximum weight (1.0) for the topic of interest,\nand no weight (0.0) for all other topics. LetBt de-\nnote the topic output vector for the t-th topic. To\ngenerate the multinomial distribution over word\ntypes for the t-th topic, we replace s with Bt be-\nfore computing the softmax over the vocabulary.\nTopic models are traditionally evaluated using\nmodel perplexity. There are various ways to es-\ntimate test perplexity (Wallach et al., 2009), but\nChang et al. (2009) show that perplexity does not\ncorrelate with the coherence of the generated top-\nics. Newman et al. (2010b); Mimno et al. (2011);\nAletras and Stevenson (2013) propose automatic\napproaches to computing topic coherence, and Lau\net al. (2014) summarises these methods to under-\nstand their differences. We propose using auto-\nmatic topic coherence as a means to evaluate the\ntopic model aspect of tdlm.\nFollowing Lau et al. (2014), we compute\ntopic coherence using normalised PMI (“NPMI”)\nscores. Given the top- n words of a topic, co-\nherence is computed based on the sum of pair-\nwise NPMI scores between topic words, where the\nword probabilities used in the NPMI calculation\nare based on co-occurrence statistics mined from\nEnglish Wikipedia with a sliding window (New-\nman et al., 2010b; Lau et al., 2014).13\nBased on the ﬁndings of Lau and Baldwin\n(2016), we average topic coherence over the top-\n5/10/15/20 topic words. To aggregate topic coher-\nence scores for a model, we calculate the mean\ncoherence over topics.\nIn terms of datasets, we use the same document\ncollections (APNEWS , IMDB and BNC ) as the lan-\nguage model experiments (Section 4). We use the\nsame hyper-parameter settings for tdlm and do\nnot tune them.\nFor comparison, we use the following topic\nmodels:\nlda: We use a LDA model as a baseline topic\nmodel. We use the same LDA models as were\nused to learn topic distributions for lstm+lda\n(Section 4).\n13We use this toolkit to compute topic coher-\nence: https://github.com/jhlau/topic_\ninterpretability.\n360\nTopic No. System Coherence\nAPNEWS IMDB BNC\n50\nlda .125 .084 .106\nntm .075 .064 .081\ntdlm-small .149 .104 .102\ntdlm-large .130 .088 .095\n100\nlda .136 .092 .119\nntm .085 .071 .070\ntdlm-small .152 .087 .106\ntdlm-large .142 .097 .101\n150\nlda .134 .094 .119\nntm .078 .075 .072\ntdlm-small .147 .085 .100\ntdlm-large .145 .091 .104\nTable 4: Mean topic coherence of all models over\nAPNEWS , IMDB and BNC . Boldface indicates the\nbest performance for each dataset and topic set-\nting.\nntm: ntm is a neural topic model proposed by\nCao et al. (2015). The document-topic and topic-\nword multinomials are expressed from a neu-\nral network perspective using differentiable func-\ntions. Model hyper-parameters are tuned using de-\nvelopment loss.\nTopic model performance is presented in Ta-\nble 4. There are two models of tdlm\n(tdlm-small and tdlm-large), which spec-\nify the size of its LSTM model (1 layer +600\nhidden vs. 2 layers +900 hidden; see Section 4).\ntdlm achieves encouraging results: it has the\nbest performance over APNEWS , and is compet-\nitive over IMDB . lda, however, produces more\ncoherent topics over BNC . Interestingly, coher-\nence appears to increase as the topic number in-\ncreases for lda, but the trend is less pronounced\nfor tdlm. ntm performs the worst of the 3 topic\nmodels, and manual inspection reveals that topics\nare in general not very interpretable. Overall, the\nresults suggest that tdlm topics are competitive:\nat best they are more coherent than lda topics,\nand at worst they are as good as lda topics.\nTo better understand the spread of coherence\nscores and impact of outliers, we present box plots\nfor all models (number of topics= 100) over the 3\ndomains in Figure 2. Across all domains, ntm has\npoor performance and larger spread of scores. The\ndifference between lda and tdlm is small (tdlm\n> lda in APNEWS , but lda < tdlm in BNC ),\nwhich is consistent with our previous observation\nthat tdlm topics are competitive withlda topics.\nPartition #Docs #Tokens\nTraining 9314 2.6M\nDevelopment 2000 0.5M\nTest 7532 1.7M\nTable 5: 20NEWS preprocessed statistics.\n6 Extensions\nOne strength of tdlm is its ﬂexibility, owing to\nit taking the form of a neural network. To show-\ncase this ﬂexibility, we explore two simple ex-\ntensions of tdlm, where we: (1) build a super-\nvised model using document labels (Section 6.1);\nand (2) incorporate additional document metadata\n(Section 6.2).\n6.1 Supervised Model\nIn datasets where document labels are known, su-\npervised topic model extensions are designed to\nleverage the additional information to improve\nmodelling quality. The supervised setting also has\nan additional advantage in that model evaluation\nis simpler, since models can be quantitatively as-\nsessed via classiﬁcation accuracy.\nTo incorporate supervised document labels, we\ntreat document classiﬁcation as another sub-task\nin tdlm. Given a document and its label, we feed\nthe document through the topic model network to\ngenerate the document-topic representation s, and\nconnect it to another dense layer with softmax out-\nput to generate the probability distribution over\nclasses.\nDuring training, we have additional minibatches\nfor the documents. We start the document classiﬁ-\ncation training after the topic and language models\nhave completed training in each epoch.\nWe use 20NEWS in this experiment, which is a\npopular dataset for text classiﬁcation. 20 NEWS is\na collection of forum-like messages from 20 news-\ngroups categories. We use the “bydate” version\nof the dataset, where the train and test partition is\nseparated by a speciﬁc date. We sample 2K doc-\numents from the training set to create the devel-\nopment set. For preprocessing we tokenise words\nand sentence using Stanford CoreNLP (Klein and\nManning, 2003), and lowercase all words. As\nwith previous experiments (Section 4) we addi-\ntionally ﬁlter low/high frequency word types and\nstopwords. Preprocessed dataset statistics are pre-\nsented in Table 5.\nFor comparison, we use the same two topic\n361\nTopic No. System Accuracy\n50\nlda .567\nntm .649\ntdlm .606\n100\nlda .581\nntm .639\ntdlm .602\n150\nlda .597\nntm .628\ntdlm .601\nTable 6: 20 NEWS classiﬁcation accuracy. All\nmodels are supervised extensions of the original\nmodels. Boldface indicates the best performance\nfor each topic setting.\nTopic No. Metadata Coherence Perplexity\n50 No .128 52.45\nYes .131 51.80\n100 No .142 52.14\nYes .139 51.76\n150 No .135 52.25\nYes .143 51.58\nTable 7: Topic coherence and language model per-\nplexity by incorporating classiﬁcation tags on AP-\nNEWS . Boldface indicates optimal coherence and\nperplexity performance for each topic setting.\nmodels as in Section 5: ntm and lda. Both\nntm and lda have natural supervised extensions\n(Cao et al., 2015; McAuliffe and Blei, 2008) for\nincorporating document labels. For this task, we\ntune the model hyper-parameters based on devel-\nopment accuracy.14 Classiﬁcation accuracy for all\nmodels is presented in Table 6. We present tdlm\nresults using only the small setting of LSTM (1\nlayer + 600 hidden), as we found there is little\ngain when using a larger LSTM.\nntm performs very strongly, outperforming\nboth lda and tdlm by a substantial margin.\nComparing lda and tdlm, tdlm achieves bet-\nter performance, especially when there is a smaller\nnumber of topics. Upon inspection of the topics\nwe found that ntm topics are much less coherent\nthan those of lda and tdlm, consistent with our\nobservations from Section 5.\n14Most hyper-parameter values for tdlm are similar to\nthose used in the language and topic model experiments; the\nonly exceptions are: a = 80 , b = 100 , nepoch = 20 ,\nm3 = 150. The increase in parameters is unsurprising, as the\nadditional supervision provides more constraint to the model.\nFigure 3: Scatter plots of tag embeddings\n(model=150 topics)\n6.2 Incorporating Document Metadata\nIn APNEWS , each news article contains addi-\ntional document metadata, including subject clas-\nsiﬁcation tags, such as “General News”, “Acci-\ndents and Disasters”, and “Military and Defense”.\nWe present an extension to incorporate document\nmetadata in tdlm to demonstrate its ﬂexibility in\nintegrating this additional information.\nAs some of the documents in our original AP-\nNEWS sample were missing tags, we re-sampled\na set of APNEWS articles of the same size as our\noriginal, all of which have tags. In total, approxi-\nmately 1500 unique tags can be found among the\ntraining articles.\nTo incorporate these tags, we represent each\nof them as a learnable vector and concatenate it\nwith the document vector before computing the\nattention distribution. Let zi ∈ Rf denote the\nf-dimension vector for the i-th tag. For the j-th\ndocument, we sum up all tags associated with it:\ne =\nntags∑\ni=1\nI(i,j)zi\nwhere ntags is the total number of unique tags, and\nfunction I(i,j) returns 1 is thei-th tag is in thej-th\ndocument or 0 otherwise. We computed as before\n(Section 3.1), and concatenate it with the summed\ntag vector: d′= d ⊕e.\nWe train two versions of tdlm on the new AP-\nNEWS dataset: (1) the vanilla version that ignores\nthe tag information; and (2) the extended version\nwhich incorporates tag information. 15 We exper-\n15Model hyper-parameters are the same as the ones used in\nthe language (Section 4) and topic model (Section 5) experi-\nments.\n362\nTopic Generated Sentences\nprotesters suspect gunman\nofﬁcers occupy gun arrests\nsuspects shooting ofﬁcer\n•police say a suspect in the shooting was shot in the chest and later shot and killed by a police ofﬁcer .\n•a police ofﬁcer shot her in the chest and the man was killed .\n•police have said four men have been killed in a shooting in suburban london .\nﬁlm awards actress comedy\nmusic actor album show\nnominations movie\n•it ’s like it ’s not fair to keep a star in a light , ” he says .\n•but james , a four-time star , is just a⟨unk⟩.\n•a⟨unk⟩adaptation of the movie ” the dark knight rises ” won best picture and he was nominated for best\ndrama for best director of ”⟨unk⟩, ” which will be presented sunday night .\nstorm snow weather inches\nﬂooding rain service\nwinds tornado forecasters\n•temperatures are forecast to remain above freezing enough to reach a tropical storm or heaviest temperatures .\n•snowfall totals were one of the busiest in the country .\n•forecasters say tornado irene ’s strong winds could ease visibility and funnel clouds of snow from snow\nmonday to the mountains .\nvirus nile ﬂu vaccine\ndisease outbreak infected\nsymptoms cough tested\n•he says the disease was transmitted by an infected person .\n•⟨unk⟩says the man ’s symptoms are spread away from the heat .\n•meanwhile in the⟨unk⟩, the virus has been common in the mojave desert .\nTable 8: Generated sentences for APNEWS topics.\nimented with a few values for the tag vector size\n(f) and ﬁnd that a small value works well; in the\nfollowing experiments we use f = 5. We evalu-\nate the models based on language model perplex-\nity and topic model coherence, and present the re-\nsults in Table 7.16\nIn terms of language model perplexity, we see\na consistent improvement over different topic set-\ntings, suggesting that the incorporation of tags\nimproves modelling. In terms of topic coher-\nence, there is a small but encouraging improve-\nment (with one exception).\nTo investigate whether the vectors learnt for\nthese tags are meaningful, we plot the top-14 most\nfrequent tags in Figure 3.17 The plot seems reason-\nable: there are a few related tags that are close to\neach other, e.g. “State government” and “Govern-\nment and politics”; “Crime” and “Violent Crime”;\nand “Social issues” and “Social affairs”.\n7 Discussion\nTopics generated by topic models are typically in-\nterpreted by way of their top- N highest probabil-\nity words. In tdlm, we can additionally generate\nsentences related to the topic, providing another\nway to understand the topics. To do this, we can\nconstrain the topic vector for the language model\nto be the topic output vector of a particular topic\n(Equation (3)).\nWe present 4 topics from a APNEWS model\n(k = 100 ; LSTM size = “large”) and 3 ran-\ndomly generated sentences conditioned on each\n16As the vanilla tdlm is trained on the new APNEWS\ndataset, the numbers are slightly different to those in Tables 3\nand 4.\n17The 5-dimensional vectors are compressed using PCA.\ntopic in Table 8.18 The generated sentences high-\nlight the content of the topics, providing another\ninterpretable aspect for the topics. These results\nalso reinforce that the language model is driven by\ntopics.\n8 Conclusion\nWe propose tdlm, a topically driven neural lan-\nguage model. tdlm has two components: a lan-\nguage model and a topic model, which are jointly\ntrained using a neural network. We demonstrate\nthat tdlm outperforms a state-of-the-art language\nmodel that incorporates larger context, and that\nits topics are potentially more coherent than LDA\ntopics. We additionally propose simple extensions\nof tdlm to incorporate information such as docu-\nment labels and metadata, and achieved encourag-\ning results.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments and valuable suggestions. This\nwork was funded in part by the Australian Re-\nsearch Council.\nReferences\nNikos Aletras and Mark Stevenson. 2013. Evaluat-\ning topic coherence using distributional semantics.\nIn Proceedings of the Tenth International Workshop\non Computational Semantics (IWCS-10) . Potsdam,\nGermany, pages 13–22.\n18Words are sampled with temperature= 0.75. Generation\nis terminated when a special end symbol is generated or when\nsentence length is greater than 40 words.\n363\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent Dirichlet allocation. Journal of Ma-\nchine Learning Research 3:993–1022.\nBNC Consortium. 2007. The British National Corpus,\nversion 3 (BNC XML Edition). Distributed by Ox-\nford University Computing Services on behalf of the\nBNC Consortium. http://www.natcorp.ox.ac.uk/.\nZiqiang Cao, Sujian Li, Yang Liu, Wenjie Li, and Heng\nJi. 2015. A novel neural topic model and its su-\npervised extension. In Proceedings of the 29th An-\nnual Conference on Artiﬁcial Intelligence (AAAI-\n15). Austin, Texas, pages 2210–2216.\nJonathan Chang, Sean Gerrish, Chong Wang, Jordan L.\nBoyd-Graber, and David M. Blei. 2009. Reading\ntea leaves: How humans interpret topic models. In\nAdvances in Neural Information Processing Systems\n21 (NIPS-09). Vancouver, Canada, pages 288–296.\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statisti-\ncal Translation. Doha, Qatar, pages 103–111.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence mod-\neling. In NIPS Deep Learning and Representation\nLearning Workshop. Montreal, Canada, pages 103–\n111.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research\n12:2493–2537.\nFelix A. Gers and J ¨urgen Schmidhuber. 2000. Recur-\nrent nets that time and count. In Proceedings of the\nInternational Joint Conference on Neural Networks\n(IJCNN’2000). Como, Italy, pages 198–194.\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines. CoRR abs/1410.5401.\nThomas L. Grifﬁths and Mark Steyvers. 2004. Find-\ning scientiﬁc topics. Proceedings of the National\nAcademy of Sciences 101:5228–5235.\nThomas L. Grifﬁths, Mark Steyvers, David M. Blei,\nand Joshua B. Tenenbaum. 2004. Integrating topics\nand syntax. In Advances in Neural Information Pro-\ncessing Systems 17 (NIPS-05) . Vancouver, Canada,\npages 537–544.\nDavid Hall, Daniel Jurafsky, and Christopher D. Man-\nning. 2008. Studying the history of ideas using topic\nmodels. In Proceedings of the 2008 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2008). Honolulu, USA, pages 363–371.\nGeoffrey E. Hinton and Ruslan R. Salakhutdinov. 2009.\nReplicated softmax: an undirected topic model. In\nAdvances in Neural Information Processing Systems\n21 (NIPS-09) . Vancouver, Canada, pages 1607–\n1614.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation 9:1735–\n1780.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2016. Document context lan-\nguage models. In Proceedings of ICLR-16 Work-\nshop, 2016. Toulon, France.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR\nabs/1412.6980.\nDan Klein and Christopher D. Manning. 2003. Accu-\nrate unlexicalized parsing. In Proceedings of the\n41st Annual Meeting of the Association for Com-\nputational Linguistics (ACL 2003). Sapporo, Japan,\npages 423–430.\nHugo Larochelle and Stanislas Lauly. 2012. A neu-\nral autoregressive topic model. In Advances in Neu-\nral Information Processing Systems 25. pages 2708–\n2716.\nJey Han Lau and Timothy Baldwin. 2016. The sensitiv-\nity of topic coherence evaluation to topic cardinality.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics — Human Language Technologies\n(NAACL HLT 2016). San Diego, USA, pages 483–\n487.\nJey Han Lau, David Newman, and Timothy Baldwin.\n2014. Machine reading tea leaves: Automatically\nevaluating topic coherence and topic model quality.\nIn Proceedings of the 14th Conference of the EACL\n(EACL 2014). Gothenburg, Sweden, pages 530–539.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (ACL HLT 2011). Portland,\nOregon, USA, pages 142–150.\nJon D. McAuliffe and David M. Blei. 2008. Super-\nvised topic models. In Advances in Neural Informa-\ntion Processing Systems 20 (NIPS-08) . Vancouver,\nCanada, pages 121–128.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Pro-\nceedings of the 11th Annual Conference of the In-\nternational Speech Communication Association (IN-\nTERSPEECH 2010). Makuhari, Japan, pages 1045–\n1048.\n364\nDavid Mimno, Hanna Wallach, Edmund Talley,\nMiriam Leenders, and Andrew McCallum. 2011.\nOptimizing semantic coherence in topic models. In\nProceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2011). Edinburgh, UK, pages 262–272.\nDavid Newman, Timothy Baldwin, Lawrence Cave-\ndon, Sarvnaz Karimi, David Martinez, and Justin\nZobel. 2010a. Visualizing document collections and\nsearch results using topic mapping. Journal of Web\nSemantics 8(2–3):169–175.\nDavid Newman, Jey Han Lau, Karl Grieser, and Tim-\nothy Baldwin. 2010b. Automatic evaluation of\ntopic coherence. In Proceedings of Human Lan-\nguage Technologies: The 11th Annual Conference\nof the North American Chapter of the Association\nfor Computational Linguistics (NAACL HLT 2010) .\nLos Angeles, USA, pages 100–108.\nVu Pham, Christopher Kermorvant, and J ´erˆome\nLouradour. 2013. Dropout improves recurrent neu-\nral networks for handwriting recognition. CoRR\nabs/1312.4569.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch 15:1929–1958.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston,\nand Rob Fergus. 2015. End-to-end memory net-\nworks. In Advances in Neural Information Process-\ning Systems 28 (NIPS-15). Montreal, Canada, pages\n2440–2448.\nKe Tran, Arianna Bisazza, and Christof Monz. 2016.\nRecurrent memory networks for language modeling.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics — Human Language Technologies\n(NAACL HLT 2016) . San Diego, California, pages\n321–331.\nHanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,\nand David Mimno. 2009. Evaluation methods for\ntopic models. In Proceedings of the 26th Interna-\ntional Conference on Machine Learning (ICML-09).\nMontreal, Canada, pages 1105–1112.\nLi Wan, Leo Zhu, and Rob Fergus. 2012. A hybrid neu-\nral network-latent topic model. In Proceedings of\nthe Fifteenth International Conference on Artiﬁcial\nIntelligence and Statistics (AISTATS-12). La Palma,\nCanary Islands, pages 1287–1294.\nTian Wang and Kyunghyun Cho. 2016. Larger-\ncontext language modelling with recurrent neural\nnetwork. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2016). Berlin, Germany, pages 1319–1329.\nXuerui Wang and Andrew McCallum. 2006. Topics\nover time: a non-Markov continuous-time model of\ntopical trends. In Proceedings of the 12th ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining . Philadelphia, USA,\npages 424–433.\nJason Weston, Sumit Chopra, and Antoine Bordes.\n2014. Memory networks. CoRR abs/1410.3916.\nKaisheng Yao, Trevor Cohn, Katerina Vylomova,\nKevin Duh, and Chris Dyer. 2015. Depth-gated\nLSTM. CoRR abs/1508.03790.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\nCoRR abs/1409.2329.\n365",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9688707590103149
    },
    {
      "name": "Computer science",
      "score": 0.8249013423919678
    },
    {
      "name": "Language model",
      "score": 0.7937338352203369
    },
    {
      "name": "Sentence",
      "score": 0.7595683336257935
    },
    {
      "name": "Natural language processing",
      "score": 0.7133607864379883
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6688166856765747
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5999237895011902
    },
    {
      "name": "Representation (politics)",
      "score": 0.49466750025749207
    },
    {
      "name": "Context model",
      "score": 0.4114988148212433
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    }
  ]
}