{
  "title": "Performance of Large Language Models (ChatGPT, Bing Search, and Google Bard) in Solving Case Vignettes in Physiology",
  "url": "https://openalex.org/W4385900159",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5034041256",
      "name": "Anup Kumar D Dhanvijay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3118565030",
      "name": "Mohammed Jaffer Pinjar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2883517138",
      "name": "Nitin Dhokane ,",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2007445343",
      "name": "Smita R Sorte",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099091421",
      "name": "Amita Kumari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2556512418",
      "name": "Himel Mondal",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4366989525",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4382020836",
    "https://openalex.org/W3025380454",
    "https://openalex.org/W4376866715",
    "https://openalex.org/W4322723456",
    "https://openalex.org/W4380423243",
    "https://openalex.org/W4376114558",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W2981296841",
    "https://openalex.org/W4321459182",
    "https://openalex.org/W4323980051",
    "https://openalex.org/W4327715333",
    "https://openalex.org/W4362510886",
    "https://openalex.org/W4384078169",
    "https://openalex.org/W4312211961"
  ],
  "abstract": null,
  "full_text": "Review began\n 07/27/2023 \nReview ended\n 08/02/2023 \nPublished\n 08/04/2023\n© Copyright \n2023\nDhanvijay et al. This is an open access\narticle distributed under the terms of the\nCreative Commons Attribution License CC-\nBY 4.0., which permits unrestricted use,\ndistribution, and reproduction in any\nmedium, provided the original author and\nsource are credited.\nPerformance of Large Language Models\n(ChatGPT, Bing Search, and Google Bard) in\nSolving Case Vignettes in Physiology\nAnup Kumar D. Dhanvijay \n \n, \nMohammed Jaffer Pinjar \n \n, \nNitin Dhokane \n \n, \nSmita R. Sorte \n \n, \nAmita Kumari \n, \nHimel Mondal \n1.\n Physiology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND \n2.\n Physiology, Government Medical\nCollege, Sindhudurg, Oros, IND \n3.\n Physiology, All India Institute of Medical Sciences, Nagpur, Nagpur, IND\nCorresponding author: \nHimel Mondal, \nhimelmkcg@gmail.com\nAbstract\nBackground\nLarge language models (LLMs) have emerged as powerful tools capable of processing and generating human-\nlike text. These LLMs, such as ChatGPT (OpenAI Incorporated, Mission District, San Francisco, United\nStates), Google Bard (Alphabet Inc., CA, US), and Microsoft Bing (Microsoft Corporation, WA, US), have been\napplied across various domains, demonstrating their potential to assist in solving complex tasks and\nimproving information accessibility. However, their application in solving case vignettes in physiology has\nnot been explored. This study aimed to assess the performance of three LLMs, namely, ChatGPT (3.5; free\nresearch version), Google Bard (Experiment), and Microsoft Bing (precise), in answering cases vignettes in\nPhysiology.\nMethods\nThis cross-sectional study was conducted in July 2023. A total of 77 case vignettes in physiology were\nprepared by two physiologists and were validated by two other content experts. These cases were presented\nto each LLM, and their responses were collected. Two physiologists independently rated the answers\nprovided by the LLMs based on their accuracy. The ratings were measured on a scale from 0 to 4 according to\nthe structure of the observed learning outcome (pre-structural = 0, uni-structural = 1, multi-structural = 2,\nrelational = 3, extended-abstract). The scores among the LLMs were compared by Friedman’s test and inter-\nobserver agreement was checked by the intraclass correlation coefficient (ICC).\nResults\nThe overall scores for ChatGPT, Bing, and Bard in the study, with a total of 77 cases, were found to be\n3.19±0.3, 2.15±0.6, and 2.91±0.5, respectively, p<0.0001. Hence, ChatGPT 3.5 (free version) obtained the\nhighest score, Bing (Precise) had the lowest score, and Bard (Experiment) fell in between the two in terms of\nperformance. The average ICC values for ChatGPT, Bing, and Bard were 0.858 (95% CI: 0.777 to 0.91,\np<0.0001), 0.975 (95% CI: 0.961 to 0.984, p<0.0001), and 0.964 (95% CI: 0.944 to 0.977, p<0.0001),\nrespectively.\nConclusion\nChatGPT outperformed Bard and Bing in answering case vignettes in physiology. Hence, students and\nteachers may think about choosing LLMs for their educational purposes accordingly for case-based learning\nin physiology. Further exploration of their capabilities is needed for adopting those in medical education\nand support for clinical decision-making.\nCategories:\n Medical Education, Other\nKeywords:\n problem-based learning, self-directed learning (sdl), medical education, case vignette, physiology, bing,\nbard, chatgpt, language model, artificial intelligence\nIntroduction\nWith rapid advancements in natural language processing, large language models (LLMs) have emerged as\npowerful tools capable of processing and generating human-like text. These LLMs, such as ChatGPT (OpenAI\nIncorporated, Mission District, San Francisco, United States), Google Bard (Alphabet Inc., CA, US), and\nMicrosoft Bing (Microsoft Corporation, WA, US), have been applied across various domains, demonstrating\ntheir potential to assist in solving complex tasks and improving information accessibility \n[1]\n. In the context\nof medical education and practice, the ability of LLMs to provide accurate and contextually relevant\nresponses holds significant promise \n[2]\n. There are various domains of medical education like solving higher-\norder problems, generating questions, and preparing content for PowerPoint slides where LLMs can help\nteachers and students \n[3]\n. However, their application in solving case vignettes in physiology remains\n1\n1\n2\n3\n1\n1\n \n Open Access Original\nArticle\n \nDOI:\n 10.7759/cureus.42972\nHow to cite this article\nDhanvijay A D, Pinjar M, Dhokane N, et al. (August 04, 2023) Performance of Large Language Models (ChatGPT, Bing Search, and Google Bard)\nin Solving Case Vignettes in Physiology . Cureus 15(8): e42972. \nDOI 10.7759/cureus.42972\nrelatively unexplored.\nThe background of this study is rooted in the growing interest in leveraging LLMs to enhance medical\neducation and support clinical decision-making. Traditional medical teaching methodologies, including\ncase-based learning, have long been utilized to facilitate critical thinking and problem-solving skills in\nmedical students \n[4]\n. The integration of LLMs into this educational framework offers an opportunity to\nexplore novel approaches to learning and problem-solving in the medical field.\nThe potential implications of this study are twofold. Firstly, it seeks to shed light on the performance of\nthree prominent LLMs, namely, ChatGPT-3.5 (free research version), Google Bard, and Microsoft Bing, in\nanswering case vignettes in the domain of physiology. Understanding how these LLMs perform in this\nspecific context can inform educators and practitioners about their efficacy as supplementary tools in\nmedical education \n[5]\n. Secondly, the study's findings could have broader implications for the integration of\nLLMs into medical practice. If LLMs demonstrate proficiency in accurately answering case vignettes, they\nmay be employed in various medical settings to support clinical decision-making, provide quick access to\nrelevant medical information, and potentially reduce the workload of healthcare professionals \n[6]\n.\nNevertheless, it is essential to critically assess the reliability and limitations of LLMs, as their responses can\nbe influenced by the data they were trained on, leading to potential biases or inaccuracies.\nMaterials And Methods\nType and settings\nThis was a cross-sectional observational study involving data audit sourced from three public domain LLMs.\nThe study was conducted as a comparative analysis of LLMs in solving cases vignettes in the domain of\nphysiology. The research was carried out in an academic setting, involving the collaboration of two\nphysiologists as raters. The three LLMs under investigation were ChatGPT, Bard, and Bing.\nPreparation of case vignettes\nA total of 77 case vignettes were carefully curated to encompass a diverse range of physiological and\npathophysiological scenarios. However, the questions were set for the level an undergraduate student with\nknowledge of physiology can answer it. An example is shown in Figure \n1\n.\nFIGURE\n 1: An example of a case vignette and related questions\nEach case vignette was designed to present challenging medical situations, requiring critical thinking and\nexpertise in physiology for accurate and contextually appropriate responses. Two physiologists created the\ncase vignettes which were reviewed and validated by another two experts to ensure their relevance and\naccuracy.\nData collection method\nFor data collection, the selected case vignettes were presented individually to each of the three LLMs -\nChatGPT 3.5 (free research version), Google Bard (Alphabet Inc., CA, US), and Bing Chat (Precise search)\n(Microsoft Corporation, WA, US). The LLMs were given access to the case vignettes and asked to provide\nwritten responses to each scenario with a prompt - \"Read the case vignette and answer the questions\". These\nresponses were then compiled and stored for further analysis.\nRating of answers\nTwo experienced physiologists independently rated the responses generated by the LLMs based on accuracy\n2023 Dhanvijay et al. Cureus 15(8): e42972. DOI 10.7759/cureus.42972\n2\n of \n7\nand appropriateness. The ratings were performed on a numerical scale from 0 to 4, with higher scores\nindicating more accurate and contextually relevant answers. This rating was according to the structure of\nobserved learning outcome and scored as pre-structural = 0, uni-structural = 1, multi-structural = 2,\nrelational = 3, extended-abstract = 4. The study process in a flow chart is shown in Figure \n2\n.\nFIGURE\n 2: Brief study process flow chart\nStatistical analysis\nThe obtained ratings from the two physiologists for each LLM's responses were subjected to statistical\nanalysis. Mean and standard deviation were calculated for each LLM's scores to assess their overall\nperformance in solving the cases vignettes. Friedman’s test was applied to compare the variances among the\nscores. Inter-observer agreement was checked by the intraclass correlation coefficient (ICC). We used\nIBM SPSS Statistics for Windows, Version 20 (Released 2011; IBM Corp., Armonk, New York, United\nStates) for conducting statistical tests, and a p-value <0.05 was considered statistically significant.\nEthics\nThe study adhered to ethical guidelines for research. There were no human research participants in this\nstudy. The data of case vignettes were fictitious and any resemblance with any subject or patient is\ncoincidental. Hence, this study does not need ethical clearance according to the Indian Council of Medical\nResearch (ICMR)’s comprehensive ethics guidelines for conducting research involving human subjects.\nResults\nA total of 77 cases in physiology were analyzed by two physiologists. The scores obtained by ChatGPT, Bing,\nand Bard were 3.17±0.31, 2.14±0.6, and 2.92±0.49, respectively as rated by the first rater as shown in Figure\n3\n. There was a statistically significant (p<0.0001) difference among the scores with the highest score\nobtained by ChatGPT and the lowest by Bing.\n2023 Dhanvijay et al. Cureus 15(8): e42972. DOI 10.7759/cureus.42972\n3\n of \n7\nFIGURE\n 3: Scores of ChatGPT, Bing, and Bard in solving cases in\nphysiology as rated by the first rater\nA similar pattern is seen in the score rated by the second rater as shown in Figure \n4\n. The scores were\n3.23±0.34, 2.16±0.61, and 2.89±0.53, respectively, p<0.0001.\nFIGURE\n 4: Scores of ChatGPT, Bing, and Bard in solving cases in\nphysiology as rated by the second rater\nWe calculated the average scores of two raters and system-wise scores, and overall scores are shown in Table\n1\n. In the majority of the physiological systems, there was a difference in scores among the LLMs with an\noverall performance highest for ChatGPT and lowest for Bing.\n2023 Dhanvijay et al. Cureus 15(8): e42972. DOI 10.7759/cureus.42972\n4\n of \n7\nDomain in physiology\nChatGPT\nBing\nBard\np-value\nGeneral (n=2)\n3.25±0.35\n2.5±0.35\n2.63±0.53\n0.67\nNerve-muscle (n=2)\n3.5±0\n2.75±0.35\n3.5±0\n0.33\nCentral nervous system (n=8)\n3.22±0.24\n2.53±0.3\n3.16±0.31\n0.0007\nCardiovascular (n=11)\n3.09±0.35\n2.25±0.56\n2.82±0.55\n0.0005\nBlood and  immunity (n=10)\n3.28±0.29\n2.08±0.79\n2.88±0.46\n<0.0001\nGastrointestinal (n=9)\n3.25±0.25\n2.58±0.39\n2.91±0.31\n0.0007\nRenal (n=7)\n3.14±0.56\n1.64±0.48\n2.89±0.19\n0.0009\nTemperature (n=2)\n3.5±0\n1.5±0.71\n3.25±0.35\n0.33\nRespiratory (n=7)\n3.14±0.24\n1.57±0.35\n2.79±0.39\n0.0003\nSpecial sense (n=5)\n3.2±0.27\n2±0.35\n3.1±0.22\n0.12\nEndocrine (n=5)\n3.1±0.22\n1.9±0.22\n3±0\n0.12\nReproductive (n=9)\n3.17±0.25\n2.26±0.65\n2.61±1.02\n0.015\nOverall (n=77)\n3.19±0.3\n2.15±0.6\n2.91±0.5\n<0.0001\nTABLE\n 1: Domain-wise scores of ChatGPT, Bing, and Bard in solving physiology cases\nThe average ICC values for ChatGPT, Bing, and Bard were 0.858 (95% CI: 0.777 to 0.91, p<0.0001), 0.975 (95%\nCI: 0.961 to 0.984, p<0.0001), and 0.964 (95% CI: 0.944 to 0.977, p<0.0001), respectively. These ICC values\nindicate a stronger inter-rater agreement level of agreement between the raters for each language model's\nperformance.\nDiscussion\nWe found that ChatGPT consistently achieved the highest scores among the three LLMs, while Bing\nconsistently obtained the lowest scores. This observation was confirmed by both the first and second raters.\nFurthermore, an average of the scores given by the two raters followed a consistent pattern, where ChatGPT\noutperformed Bing and Bard across the majority of physiological systems.\nA study by Rahsepar et al. showed that ChatGPT exhibited higher accuracy compared to Google Bard in\nanswering common lung cancer questions \n[7]\n. In contrast, a study by Raimondi showed different results.\nThey found that, in the Royal College of Ophthalmologists fellowship exams, Bing Chat performed the best\namong the three AI systems, while ChatGPT had the lowest accuracy \n[8]\n. A study by Ali found that ChatGPT\nperforms better than Bard in answering higher-order knowledge questions in neurosurgery oral board\npreparation questions \n[9]\n. Hence, the performance may vary according to various domains of the medical\nfield.\nSeveral potential underlying reasons may contribute to these performance differences. One plausible factor\ncould be the varying capabilities and design of each language model. We presume that ChatGPT might have\nundergone more advanced training algorithms, received higher-quality training data, or been fine-tuned\nmore effectively for physiology-related tasks. Additionally, ChatGPT's contextual understanding and\ncoherence in generating responses may have played a role in earning higher scores. Moreover, the expertise\nand potential biases of the raters themselves could have influenced the evaluations. It is also essential to\nconsider the model's handling of uncertainty and consistency in responses as contributing factors. Further\nresearch and investigation are required to comprehensively understand the nuanced reasons behind the\nobserved performance disparities among the language models \n[10]\n.\nThe use of LLMs in medical education has shown considerable promise in transforming traditional learning\nmethodologies. LLMs, such as ChatGPT, have demonstrated their ability to process vast amounts of medical\nliterature and provide contextually relevant information, making them valuable resources for both educators\nand students \n[11-14]\n. By leveraging LLMs, medical educators can offer interactive and dynamic learning\nexperiences, allowing students to access up-to-date medical information, review complex concepts, and\nengage in problem-solving scenarios. Moreover, LLMs can enhance the efficiency of knowledge retrieval,\nproviding quick answers to medical queries and supporting evidence-based decision-making. Integrating\nLLMs into medical education can foster self-directed learning, critical thinking, and analytical skills,\n2023 Dhanvijay et al. Cureus 15(8): e42972. DOI 10.7759/cureus.42972\n5\n of \n7\nempowering the next generation of healthcare professionals with cutting-edge resources and fostering\ncontinuous professional development in the rapidly evolving medical field \n[15]\n. However, careful\nconsideration of the limitations and potential biases of LLMs is essential to ensure their responsible and\nethical use, as well as to complement their role with hands-on, practical training and mentorship in clinical\nsettings \n[16]\n.\nThe study has some limitations. The study focused on a specific set of LLMs, namely, ChatGPT, Bard, and\nBing, which may not represent the entire spectrum of LLMs available. Including a broader range of LLMs\ncould provide a more comprehensive understanding of their capabilities in this context. Additionally, the\nstudy's assessment of LLM performance was based on responses to pre-defined case vignettes, limiting the\nexploration of their adaptability to a wider variety of clinical scenarios. There may be chances of bias due to\nits training data. Moreover, the study relied on the ratings of two physiologists, which, although valuable,\nmay still introduce subjectivity and inter-rater variability in the evaluation process. Furthermore, the study's\nfindings may not be generalizable to other medical specialties, as the efficacy of LLMs could vary depending\non the complexity and domain-specific nature of different medical fields. Lastly, the study's design did not\nexplore the potential biases or limitations of LLMs in their responses, which could be crucial in real-world\napplications.\nConclusions\nThis study provides valuable insights into the application of LLMs in solving physiological case vignettes in\nmedical education. The findings demonstrate the potential of LLMs, particularly ChatGPT, in offering\naccurate and contextually relevant responses to complex medical scenarios in physiology. Further research\nshould explore a wider range of LLMs, examine adaptability to diverse clinical scenarios, and address\npotential biases in LLM responses.\nAdditional Information\nDisclosures\nHuman subjects:\n All authors have confirmed that this study did not involve human participants or tissue.\nAnimal subjects:\n All authors have confirmed that this study did not involve animal subjects or tissue.\nConflicts of interest:\n In compliance with the ICMJE uniform disclosure form, all authors declare the\nfollowing: \nPayment/services info:\n All authors have declared that no financial support was received from\nany organization for the submitted work. \nFinancial relationships:\n All authors have declared that they have\nno financial relationships at present or within the previous three years with any organizations that might\nhave an interest in the submitted work. \nOther relationships:\n All authors have declared that there are no\nother relationships or activities that could appear to have influenced the submitted work.\nAcknowledgements\nThe corresponding author would like to thank Sarika Mondal and Ahana Aarshi for their sacrifice of family\ntime during the analysis, interpretation, visualization, drafting, and handling of this manuscript in the\njournal management system. We acknowledge ChatGPT, the language model developed by OpenAI, which\nhas been of invaluable assistance in the grammar and language editing of this document.\nReferences\n1\n. \nDe Angelis L, Baglivo F, Arzilli G, Privitera GP, Ferragina P, Tozzi AE, Rizzo C: \nChatGPT and the rise of large\nlanguage models: the new AI-driven infodemic threat in public health\n. Front Public Health. 2023,\n11:1166120. \n10.3389/fpubh.2023.1166120\n2\n. \nSallam M: \nChatGPT utility in healthcare education, research, and practice: systematic review on the\npromising perspectives and valid concerns\n. Healthcare (Basel). 2023, 11:887. \n10.3390/healthcare11060887\n3\n. \nAgarwal M, Sharma P, Goswami A: \nAnalysing the applicability of ChatGPT, Bard, and Bing to generate\nreasoning-based multiple-choice questions in medical physiology\n. Cureus. 2023, 15:e40977.\n10.7759/cureus.40977\n4\n. \nKaur G, Rehncy J, Kahal KS, Singh J, Sharma V, Matreja PS, Grewal H: \nCase-based learning as an effective\ntool in teaching pharmacology to undergraduate medical students in a large group setting\n. J Med Educ\nCurric Dev. 2020, 7:\n10.1177/2382120520920640\n5\n. \nAbd-Alrazaq A, AlSaad R, Alhuwail D, et al.: \nLarge language models in medical education: opportunities,\nchallenges, and future directions\n. JMIR Med Educ. 2023, 9:e48291. \n10.2196/48291\n6\n. \nAhn S: \nThe impending impacts of large language models on medical education\n. Korean J Med Educ. 2023,\n35:103-7. \n10.3946/kjme.2023.253\n7\n. \nRahsepar AA, Tavakoli N, Kim GH, Hassani C, Abtin F, Bedayat A: \nHow AI responds to common lung cancer\nquestions: ChatGPT vs Google Bard\n. Radiology. 2023, 307:e230922. \n10.1148/radiol.230922\n8\n. \nRaimondi R, Tzoumas N, Salisbury T, Di Simplicio S, Romano MR: \nComparative analysis of large language\nmodels in the Royal College of Ophthalmologists fellowship exams\n. Eye (Lond). 2023, \n10.1038/s41433-023-\n02563-3\n9\n. \nAli R, Tang OY, Connolly ID, et al.: \nPerformance of ChatGPT, GPT-4, and Google Bard on a Neurosurgery\nOral Boards Preparation Question Bank\n. Neurosurgery. 2023, \n10.1227/neu.0000000000002551\n10\n. \nParanjape K, Schinkel M, Nannan Panday R, Car J, Nanayakkara P: \nIntroducing artificial intelligence training\nin medical education\n. JMIR Med Educ. 2019, 5:e16048. \n10.2196/16048\n2023 Dhanvijay et al. Cureus 15(8): e42972. DOI 10.7759/cureus.42972\n6\n of \n7\n11\n. \nSinha RK, Deb Roy A, Kumar N, Mondal H: \nApplicability of ChatGPT in assisting to solve higher order\nproblems in pathology\n. Cureus. 2023, 15:e35237. \n10.7759/cureus.35237\n12\n. \nDas D, Kumar N, Longjam LA, Sinha R, Deb Roy A, Mondal H, Gupta P: \nAssessing the capability of ChatGPT\nin answering first- and second-order knowledge questions on microbiology as per competency-based\nmedical education curriculum\n. Cureus. 2023, 15:e36034. \n10.7759/cureus.36034\n13\n. \nJuhi A, Pipil N, Santra S, Mondal S, Behera JK, Mondal H: \nThe capability of ChatGPT in predicting and\nexplaining common drug-drug interactions\n. Cureus. 2023, 15:e36272. \n10.7759/cureus.36272\n14\n. \nGhosh A, Bir A: \nEvaluating ChatGPT's ability to solve higher-order questions on the competency-based\nmedical education curriculum in medical biochemistry\n. Cureus. 2023, 15:e37023. \n10.7759/cureus.37023\n15\n. \nMondal H, Marndi G, Behera JK, Mondal S: \nChatGPT for teachers: Practical examples for utilizing artificial\nintelligence for educational purposes\n. Indian J Vasc Endovasc Surg. 2023,\n16\n. \nGudis DA, McCoul ED, Marino MJ, Patel ZM: \nAvoiding bias in artificial intelligence\n. Int Forum Allergy\nRhinol. 2023, 13:193-5. \n10.1002/alr.23129\n2023 Dhanvijay et al. Cureus 15(8): e42972. DOI 10.7759/cureus.42972\n7\n of \n7",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.5212845802307129
    },
    {
      "name": "Microsoft excel",
      "score": 0.5006198883056641
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.424111008644104
    },
    {
      "name": "Human physiology",
      "score": 0.4189176559448242
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2524303197860718
    },
    {
      "name": "Computer science",
      "score": 0.18141385912895203
    },
    {
      "name": "Internal medicine",
      "score": 0.10188627243041992
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4396570500",
      "name": "All India Institute of Medical Sciences, Deoghar",
      "country": null
    },
    {
      "id": "https://openalex.org/I4401200305",
      "name": "All India Institute of Medical Sciences, Nagpur",
      "country": null
    }
  ]
}