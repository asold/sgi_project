{
  "title": "Limits of Detecting Text Generated by Large-Scale Language Models",
  "url": "https://openalex.org/W3004715608",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5065423139",
      "name": "Lav R. Varshney",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5028709336",
      "name": "Nitish Shirish Keskar",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5059955534",
      "name": "Richard Socher",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2518143623",
    "https://openalex.org/W6735867283",
    "https://openalex.org/W2128741289",
    "https://openalex.org/W2063042357",
    "https://openalex.org/W1998123258",
    "https://openalex.org/W2913980229",
    "https://openalex.org/W6767101625",
    "https://openalex.org/W6767305871",
    "https://openalex.org/W6763373685",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W6764055196",
    "https://openalex.org/W6767184454",
    "https://openalex.org/W6770248041",
    "https://openalex.org/W6762100110",
    "https://openalex.org/W2169128714",
    "https://openalex.org/W2146854872",
    "https://openalex.org/W2091794796",
    "https://openalex.org/W2099111195",
    "https://openalex.org/W2072716814",
    "https://openalex.org/W2144679335",
    "https://openalex.org/W2065937235",
    "https://openalex.org/W2082660934",
    "https://openalex.org/W6768028577",
    "https://openalex.org/W2079145130",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2963466651",
    "https://openalex.org/W6758779242",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W6761551260",
    "https://openalex.org/W2159381752",
    "https://openalex.org/W2971775690",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2988675894",
    "https://openalex.org/W2944508037",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2969263964",
    "https://openalex.org/W3021153741",
    "https://openalex.org/W2971671202",
    "https://openalex.org/W2948975009",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2948869406",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3008170323",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2605133364"
  ],
  "abstract": "Some consider large-scale language models that can generate long and coherent pieces of text as dangerous, since they may be used in misinformation campaigns. Here we formulate large-scale language model output detection as a hypothesis testing problem to classify text as genuine or generated. We show that error exponents for particular language models are bounded in terms of their perplexity, a standard measure of language generation performance. Under the assumption that human language is stationary and ergodic, the formulation is extended from considering specific language models to considering maximum likelihood language models, among the class of k-order Markov approximations; error probabilities are characterized. Some discussion of incorporating semantic side information is also given.",
  "full_text": "arXiv:2002.03438v1  [cs.CL]  9 Feb 2020\nLimits of Detecting T ext Generated by Large-Scale\nLanguage Models\nLav R. V arshney , Nitish Shirish Keskar, and Richard Socher\n{lvarshney , nkeskar, rsocher }@salesforce.com\nSalesforce Research, Palo Alto, CA, USA\nAbstract—Some consider large-scale language models that can\ngenerate long and coherent pieces of text as dangerous, sinc e they\nmay be used in misinformation campaigns. Here we formulate\nlarge-scale language model output detection as a hypothesi s\ntesting problem to classify text as genuine or generated. W e\nshow that error exponents for particular language models ar e\nbounded in terms of their perplexity, a standard measure of\nlanguage generation performance. Under the assumption tha t\nhuman language is stationary and ergodic, the formulation i s ex-\ntended from considering speciﬁc language models to conside ring\nmaximum likelihood language models, among the class of k-order\nMarkov approximations; error probabilities are character ized.\nSome discussion of incorporating semantic side informatio n is\nalso given.\nI. I N T RO D U CT IO N\nBuilding on a long history of language generation models\nthat are based on statistical knowledge that people have\n[1]–[6], large-scale, neural network-based language mode ls\n(LMs) that write paragraph-length text with the coherence o f\nhuman writing have emerged [7]–[9]. Such models have raised\nconcerns about misuse in generating fake news, misleading\nreviews, and hate speech [9]–[13]. The alarming consequenc es\nof such machine-generated misinformation present an urgen t\nneed to discern fake content from genuine, as it is becoming\nmore and more difﬁcult for people to do so without cog-\nnitive support tools [14]. Several recent studies have used\nsupervised learning to develop classiﬁers for this task [9] ,\n[10], [15]–[17] and interpreted their properties. Here we t ake\ninspiration from our recent work on information-theoretic\nlimits for detecting audiovisual deepfakes generated by GA Ns\n[18] to develop information-theoretic limits for detectin g the\noutputs of language models. In particular, we build on the\ninformation-theoretic study of authentication [19] to use a\nformal hypothesis testing framework for detecting the outp uts\nof language models.\nIn establishing fundamental limits of detection, we consid er\ntwo settings. First, we characterize the error exponent for a\nparticular language model in terms of standard performance\nmetrics such as cross-entropy and perplexity. As far as we\nknow , these informational performance metrics had not pre-\nviously emerged from a formal operational theorem. Second,\nwe consider not just a setting with a speciﬁc language model\nwith given performance metrics, but rather consider a unive rsal\nsetting where we take a generic view of language models as\nempirical maximum likelihood k-order Markov approximations\nof stationary, ergodic random processes. Results on estima tion\nof such random processes are revisited in the context of the\nerror probability, using a conjectured extension of the rev erse\nPinsker inequality. In closing, we discuss how the semantic s of\ngenerated text may be a form of side information in detection .\nII. P RO BL E M FO RM U L AT IO N A N D BA S ICS\nA. Language Models and their P erformance Metrics\nConsider a language L like English, which has tokens drawn\nfrom a ﬁnite alphabet A; tokens can be letters, words, or\nother such symbols. A language model assigns probabilities\nto sequences of tokens (a1, a2, . . . , a m) so the more likely a\nsequence is in L, the greater its probability. Language models\ndiscussed in Sec. I estimate this probability Q as a product of\neach token’s probability q given its preceding tokens:\nQ(a1,...,an) = q(a1)q(a2|a1)q(a3|a2, a1) · · · q(am|am− 1, . . . , a 1)\n=\nm∏\ni=1\nq(ai|ai− 1, . . . , a 1),\na Markov structure. Some language models including BER T\n[20] consider tokens on both sides in assessing probabiliti es\n(cloze task) and are Markov random ﬁeld models rather than\nMarkov language models [21]. Using training data, a languag e\nmodel aims to learn a distribution Q that is close to the\nempirical distribution P of the language L.\nBasic language models can be extended to be conditional\nlanguage models so as to allow control of style. In certain\nmodels, such as CTRL, one can generate text only from a\nconditioning variable without the need for an initial promp t\n[8]. An example of text generated from the conditioning text :\nhttps://www.cnn.com/2007/09/20/us-president-meets-british-pm\nis the following:\nLONDON, England (CNN) -- U.S. President George W. Bush met with British Prime\nMinister Tony Blair on Monday to discuss the war in Iraq, according to a statement\nfrom Blair’s office.\\n\\nThe meeting was held at 10 Downing Street and lasted about\nan hour.\\n\\n\"The two leaders discussed Iraq and other international issues of mutual\nconcern,\" said Blair.\\n\\nBush, who is scheduled to meet Wednesday with Russian\nPresident Vladimir Putin, will also visit Germany for talks later this week.\\n\\nIn\nhis statement, Blair said, \"We agreed that we should continue our efforts together\nto bring peace and stability to Iraq. We both reaffirmed our commitment to working\nclosely together, as well as to continuing to work constructively toward achieving\nlasting security and prosperity throughout the Middle East region.\"\\n\\nBush’s trip\ncomes after he visited Britain last week where he spoke out against terrorism while\nvisiting Buckingham Palace.\\n\\nHe has been criticized by some lawmakers over what\nthey say are insufficient military resources being devoted to fighting terrorism.\nNotwithstanding their limitations [22], [23], the standar d\nperformance metrics used for assessing language models are\nthe cross-entropy and the perplexity, which quantify how close\nQ is to P . As far as we know , these performance measures\nhave been proposed through the intuitive notion that small\nvalues of these quantities seem to correspond, empirically ,\nto higher-quality generated text as judged by people. Withi n\nthe common task framework [11], there are leaderboards 1 that\nassess the perplexity of language models over standard data sets\nsuch as WikiT ext-103 [24].\nThe cross-entropy of Q with respect to P is deﬁned as:\nH(P, Q) = −EP [log Q],\nwhich simpliﬁes, using standard information-theoretic id enti-\nties, to:\nH(P, Q) = H(P ) + DKL(P ||Q),\nwhere H(·) with one argument is the Shannon entropy and\nDKL(·||·) is the Kullback-Leibler divergence (relative en-\ntropy). For a given language L being modeled, the ﬁrst term\nH(P ) can be thought of as ﬁxed [25]. The second term\nDKL(P ||Q) can be interpreted as the excess information rate\nneeded to represent a language using a mismatched probabili ty\ndistribution [26].\nPerplexity is also a measure of uncertainty in predicting th e\nnext letter and is simply deﬁned as:\nPPL(P, Q) = eH(P,Q)\n= eH(P ) · eDKL(P ||Q).\nwhen entropies are measured in nats, rather than bits.\nFor a given language, we can consider the ratio of perplexity\nvalues or the difference of cross-entropy values of two mode ls\nQ1 and Q2 as a language-independent notion of performance\ngap:\nPPL(P, Q1)/PPL(P, Q2) = e[DKL(P ||Q1)− DKL(P ||Q2)]\n= e[H(P,Q1)− H(P,Q2)].\nB. Hypothesis T est and General Error Bounds\nRecall that the distribution of authentic text is denoted P\nand the distribution of text generated by the language model\nis Q. Suppose we have access to n tokens of generated text\nfrom the language model, which we call Y1, Y2, Y3, . . . , Y n.\nW e can then formalize a hypothesis test as:\nH0 := Y ∼ P (authentic)\nH1 := Y ∼ Q (LM generated)\nIf we assume the observed tokens are i.i.d., that only makes t he\nhypothesis test easier than the non-i.i.d. case seen in real istic\ntext samples, and therefore its performance acts as a bound.\nThere are general characterizations of error probability o f\nhypothesis tests as follows [27]. For the Neyman-Pearson\nformulation of ﬁxing the false alarm probability at ǫ and\n1 https://paperswithcode.com/sota/language-modelling-on-wikitext-103\nmaximizing the true detection probability, it is known that the\nerror probability satisﬁes:\nβn\nǫ\n.\n= exp( −nDKL(P ||Q))\nfor n i.i.d. samples, where\n.\n= indicates exponential equality.\nThus the error exponent is just the divergence DKL(P ||Q)).\nFor more general settings (including ergodic settings), th e\nerror exponent is given by the asymptotic Kullback-Leibler\ndivergence rate, deﬁned as the almost-sure limit of:\n1\nn log Pn\nQn\n(y1, . . . , y n), as n → ∞ ,\nif the limit exists, where Pn and Qn are the null and alternate\njoint densities of (Y1, . . . , Y n), respectively, see further details\nin [28], [29].\nWhen considering Bayesian error rather than Neyman-\nPearson error, for i.i.d. samples, we have the following upp er\nbound:\nP (n)\ne ≤ exp(−nC(P, Q))\nwhere C(·, ·) is Chernoff information. Here we will focus on\nthe Neyman-Pearson formulation rather than the Bayesian on e.\nIII. L IM IT S TH E O RE M S\nWith the preparation of Sec. II-B, we can now establish\nstatistical limits for detection of LM-generated texts. W e ﬁrst\nconsider a given language model, and then introduce a generi c\nmodel of language models.\nA. Given Language Model\nSuppose we are given a speciﬁc language model such as\nGPT -2 [7], GROVER [9], or CTRL [8], and it is characterized\nin terms of estimates of either cross-entropy H(P, Q) or\nperplexity PPL(P, Q).\nW e can see directly that the Neyman-Pearson error of\ndetection in the case of i.i.d. tokens is:\nβn\nǫ\n.\n= exp( −nDKL(P ||Q))\n= exp( −n(H(P, Q) − H(P ))),\nand similar results hold for ergodic observations.\nSince we think of H(P ) as a constant, we observe that\nthe error exponent for the decision problem is precisely an\nafﬁne shift of the cross-entropy. Outputs from models that a re\nbetter in the sense of cross-entropy or perplexity are harde r to\ndistinguish from authentic text.\nThus we see that intuitive measures of generative text qual-\nity match a formal operational measure of indistinguishabi lity\nthat comes from the hypothesis testing limit.\nB. Optimal Language Model\nNow rather than considering a particular language model,\nwe consider bounding the error probability in detection of t he\noutputs of an empirical maximum likelihood (ML) language\nmodel. W e speciﬁcally consider the empirical ML model\namong the class of models that are k-order Markov approxi-\nmations of language L, which is simply the empirical plug-in\nestimate.\nManning and Sch ¨ utze argue that, even though not quite\ncorrect, language text can be modeled as stationary, ergodi c\nrandom processes [30], an assumption that we follow . More-\nover, given the diversity of language production, we assume\nthis stationary ergodic random process with ﬁnite alphabet A\ndenoted X = {Xi, −∞ < i < ∞} is non-null in the sense\nthat always P (x− 1\n− m) > 0 and\npm = inf\nm≥ 1\nmin\na∈A ,x− 1\n− m∈A m\nP (a|x− 1\n− m) > 0.\nThis is sometimes called the smoothing requirement.\nW e further introduce an additional property of random\nprocesses that we assume for language L. W e deﬁne the\ncontinuity rate of the process X as:\nγ(k)\n= sup\nm≥ k\nmax\na∈A\nmax\nx− 1\n− m,y− 1\n− m∈A m:x− 1\n− m=y− 1\n− m\n|P (a|x− 1\n− m) − P (a|y− 1\n− m)|.\nW e further let γ = ∑ ∞\nk=1 γ(k),\nα = 1\n∏ ∞\nj=1\n(1 − γ(j)),\nand\nβ(k) = 1 − (1 − |A|γ(k))k\nkγ(k) ∏ ∞\nj=1(1 − |A|γ(j))2 .\nIf γ < ∞, then the process has summable continuity rate .\nThese speciﬁc technical notions of smoothing and continuit y\nare taken from the literature on estimation of stationary,\nergodic random processes [31].\nAs such, the hypothesis test we aim to consider here is\nbetween a non-null, stationary, ergodic process with summa ble\ncontinuity rate (genuine language) and its empirical k-order\nMarkov approximation based on training data (language mode l\noutput). W e think of the setting where the language model is\ntrained on data with many tokens, a sequence of very long\nlength m. For example, the CTRL language model was trained\nusing 140 GB of text [8].\nW e think of the Markov order k as a large value and\nso the family of empirical k-order Markov approximations\nencompasses the class of neural language models like GPT -2\nand CTRL, which are a fortiori Markov in structure. Empirical\nperplexity comparisons show that LSTM and similar neural\nlanguage models have Markov order as small as k = 13 [32].\nThe appropriate Markov order for large-scale neural langua ge\nmodels has not been investigated empirically, but is though t\nto scale with the neural network size.\nNow we aim to bound the error exponent in hypothesis\ntesting, by ﬁrst drawing on a bound for the Ornstein ¯d-distance\nbetween a stationary, ergodic process and its Markov approx i-\nmation, due to Csiszar and T alata [31]. Then we aim to relate\nthe Ornstein ¯d-distance to the Kullback-Leibler divergence\n(from error exponent expressions), using a generalization of\nthe so-called reverse Pinsker inequality [33], [34].\nBefore proceeding, let us formalize a few measures. Let the\nper-letter Hamming distance between two strings xm\n1 and ym\n1\nbe dm(xm\n1 , ym\n1 ). Then the Ornstein ¯d-distance between two\nrandom sequences Xm\n1 and Y m\n1 with distributions PX and\nPY is deﬁned as:\n¯d(Xm\n1 , Y m\n1 ) = min\nP\nEPdm( ˜Xm\n1 , ˜Y m\n1 ),\nwhere the minimization is over all joint distributions whos e\nmarginals equal PX and PY .\nLet Nm(ak\n1 ) be the number of occurrences of the string\nak\n1 in the sample Xm\n1 . Then the empirical k-order Markov\napproximation of a random process X based on the sample\nXm\n1 is the stationary Markov chain of order k whose transition\nprobabilities are the following empirical conditional pro babil-\nities:\nˆPm(a|ak\n1 ) = Nm(ak\n1 a)\nNm− 1(ak\n1 ), a ∈ A and ak\n1 ∈ A k.\nW e refer to this empirical approximation as ˆX[k]m\n1 .\nAlthough they give more reﬁned ﬁnitary versions, let us\nrestate Csisz ´ ar and T alata’s asymptotic result on estimat ing\nMarkov approximations of stationary, ergodic processes fr om\ndata. The asymptotics are in the size of the training set, m →\n∞, and we let the Markov order scale logarithmically with m.\nTheorem 1 ( [31]): Let X be a non-null stationary ergodic\nprocess with summable continuity rate. Then for any ν > 0,\nthe empirical (ν log m)-order Markov approximation ˆX satis-\nﬁes:\n¯d(Xm\n1 , ˆX[ν log m]m\n1 ) ≤ β(ν log m)\np2\nm\nγ(ν log m) + 1\nm1/2− µ\neventually almost surely as m → ∞ if ν < µ\n|log pm|.\nNow we consider Kullback-Leibler divergence. Just as\nMarton had extended Pinsker’s inequality between variatio nal\ndistance and Kullback-Leibler divergence to an inequality be-\ntween Ornstein’s ¯d-distance and Kullback-Leibler divergence\n[35], [36] as given in Theorem 2 below , is it possible to make\na similar conversion for the reverse Pinsker inequality whe n\nthere is a common ﬁnite alphabet A?\nTheorem 2 ( [36]): Let X be a stationary random process\nfrom a discrete alphabet A. Then for any other random process\nY deﬁned on the same alphabet A,\n¯d(Xm\n1 , Y m\n1 ) ≤ (u + 1)\n√\n1\n2m D(Xm\n1 ∥Y m\n1 )\nfor a computable constant u.\nW e conjecture that one can indeed convert the reverse\nPinsker inequality [33]:\nD(P ∥Q) ≤ log e\nQmin\n|P − Q|2\nfor two probability distributions P and Q deﬁned on a com-\nmon ﬁnite alphabet A, where Qmin = min a∈A Q(a). That is,\nwe make the following conjecture.\nConjecture 1: Let X be a stationary random process from\na ﬁnite alphabet A. Then for any other random process Y\ndeﬁned on the same alphabet A,\nD(Xm\n1 ∥Y m\n1 ) ≤ ˜K ¯d(Xm\n1 , Y m\n1 )2\nfor some constant ˜K.\nIf this generalized reverse Pinsker inequality holds, it im -\nplies the following further bound on the Kullback-Leibler\ndivergence and therefore the error exponent of the detectio n\nproblem for the empirical maximum likelihood Markov lan-\nguage model.\nConjecture 2: Let X be a non-null stationary ergodic\nprocess with summable continuity rate deﬁned on the ﬁnite\nalphabet A. Then for any ν > 0, the empirical (ν log m)-order\nMarkov approximation ˆX satisﬁes:\nD(Xm\n1 ∥ ˆX[ν log m]m\n1 )\n≤ ˆK\n{ β(ν log m)\np2\nm\nγ(ν log m) + 1\nm1/2− µ\n} 2\neventually almost surely as m → ∞ if ν < µ\n|log pm|, for some\nconstant ˆK.\nUnder the conjecture, we have a precise asymptotic characte r-\nization of the error exponent in deciding between genuine te xt\nand text generated from the empirical maximum likelihood\nlanguage model, expressed in terms of basic parameters of\nthe language, and of the training data set.\nIV . D IS CU S S IO N\nMotivated by the problem of detecting machine-generated\nmisinformation text that may have deleterious societal con -\nsequences, we have developed a formal hypothesis testing\nframework and established limits on the error exponents.\nFor the case of speciﬁc language models such as GPT -2 or\nCTRL, we provide a precise operational interpretation for\nthe perplexity and cross-entropy. For any future large-sca le\nlanguage model, we also conjecture a precise upper bound on\nthe error exponent.\nIt has been said that “in AI circles, identifying fake media\nhas long received less attention, funding and institutiona l\nbacking than creating it: Why sniff out other peoples fantas y\ncreations when you can design your own? ‘There’s no money\nto be made out of detecting these things, ’ [Nasir] Memon said ”\n[37]. Here we have tried to demonstrate that there are, at lea st,\ninteresting research questions on the detection side, whic h may\nalso inform practice.\nAs we had considered previously in the context of deepfake\nimages [18], it is also of interest to understand how error pr ob-\nability in detection parameterizes the dynamics of informa tion\nspreading processes in social networks, e.g. in determinin g\nepidemic thresholds.\nMany practical fake news detection algorithms use a kind\nof semantic side information, such as whether the generated\ntext is factually correct, in addition to its statistical pr operties.\nAlthough statistical side information would be straightfo rward\nto incorporate in the hypothesis testing framework, it rema ins\nto understand how to cast such semantic knowledge in a\nstatistical decision theory framework.\nACK N OW L E D G M E N T\nDiscussions with Bryan McCann, Kathy Baxter, and Miles\nBrundage are appreciated.\nRE F E RE N CE S\n[1] C. E. Shannon, “The redundancy of English, ” in Transactions of the\nSeventh Conference on Cybernetics , Mar . 1950, pp. 123–158.\n[2] ——, “Prediction and entropy of printed English, ” Bell System T echnical\nJournal, vol. 30, no. 1, pp. 50–64, Jan. 1951.\n[3] A. Chapanis, “The reconstruction of abbreviated printe d messages, ”\nJournal of Experimental Psychology , vol. 48, no. 6, pp. 496–510, Dec.\n1954.\n[4] D. Jamison and K. Jamison, “ A note on the entropy of partia l-known\nlanguages, ” Information and Control , vol. 12, no. 2, pp. 164–167, Feb.\n1968.\n[5] N. S. Tzannes, R. V . Spencer, and A. J. Kaplan, “On estimat ing the\nentropy of random ﬁelds, ” Information and Control , vol. 16, no. 1, pp.\n1–6, Mar . 1970.\n[6] T . M. Cover and R. C. King, “ A convergent gambling estimat e of the\nentropy of English, ” IEEE Transactions on Information Theory , vol. IT -\n24, no. 4, pp. 413–421, Jul. 1978.\n[7] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Suts kever,\n“Language models are unsupervised multitask learners, ” 20 19.\n[8] N. S. Keskar, B. McCann, L. R. V arshney , C. Xiong, and R. So cher,\n“CTRL: A conditional transformer language model for contro llable\ngeneration, ” Sep. 2019, arXiv:1909.05858 [cs.CL].\n[9] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F . Roes-\nner, and Y . Choi, “Defending against neural fake news, ” May 2 019,\narXiv:1905.12616 [cs.CL].\n[10] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herber t-V oss, J. Wu,\nA. Radford, G. Krueger, J. W . Kim, S. Kreps, M. McCain, A. Newh ouse,\nJ. Blazakis, K. McGufﬁe, and J. W ang, “Release strategies an d the social\nimpacts of language models, ” Nov . 2019, arXiv:1908.09203v 2 [cs.CL].\n[11] L. R. V arshney , N. S. Keskar, and R. Socher, “Pretrained AI models:\nPerformativity , mobility , and change, ” Sep. 2019, arXiv:1 909.03290\n[cs.CY].\n[12] J. Bullock and M. Luengo-Oroz, “ Automated speech gener ation from\nUN general assembly statements: Mapping risks in AI generat ed texts, ”\nJun. 2019, arXiv:1906.01946 [cs.CL].\n[13] A. Mitchell, J. Gottfried, G. Stocking, M. W alker, and S . Fedeli, Many\nAmericans Say Made-Up News Is a Critical Problem That Needs T o Be\nFixed. Pew Research Center, Jun. 2019.\n[14] S. Gehrmann, H. Strobelt, and A. Rush, “GL TR: Statistic al detection\nand visualization of generated text, ” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics ( ACL), Jul.\n2019, pp. 111–116.\n[15] A. Bakhtin, S. Gross, M. Ott, Y . Deng, M. Ranzato, and A. S zlam, “Real\nor fake? learning to discriminate machine from human genera ted text, ”\nJul. 2019, arXiv:1906.03351 [cs.LG].\n[16] T . Schuster, R. Schuster, D. J. Shah, and R. Barzilay , “ A re we safe yet?\nthe limitations of distributional features for fake news de tection, ” Aug.\n2019, arXiv:1908.09805 [cs.CL].\n[17] D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Ec k, “Human and\nautomatic detection of generated text, ” Nov . 2019, arXiv:1 911.00650\n[cs.CL].\n[18] S. Agarwal and L. R. V arshney , “Limits of deepfake detec tion: A\nrobust estimation viewpoint, ” in Proceedings of Synthetic Realities: Deep\nLearning for Detecting AudioV isual F akes W orkshop at ICML 2019, Jun.\n2019.\n[19] U. M. Maurer, “ Authentication theory and hypothesis te sting, ” IEEE\nTransactions on Information Theory , vol. 46, no. 4, pp. 1350–1356, Jul.\n2000.\n[20] J. Devlin, M.-W . Chang, K. Lee, and K. T outanova, “BER T: Pre-\ntraining of deep bidirectional transformers for language u nderstanding, ”\nin Proceedings of the 2019 Conference of the North American Cha pter\nof the Association for Computational Linguistics (NAACL) , Jun. 2019,\npp. 4171–4186.\n[21] A. W ang and K. Cho, “BER T has a mouth, and it must speak: BE R T as\na Markov random ﬁeld language model, ” Feb. 2019, arXiv:1902 .04094\n[cs.CL].\n[22] T . B. Hashimoto, H. Zhang, and P . Liang, “Unifying human and\nstatistical evaluation for natural language generation, ” in Proceedings of\nthe 2019 Conference of the North American Chapter of the Asso ciation\nfor Computational Linguistics (NAACL) , Jun. 2019, pp. 1689–1701.\n[23] A. Holtzman, J. Buys, M. Forbes, and Y . Choi, “The curiou s case of\nneural text degeneration, ” Apr . 2019, arXiv:1904.09751 [c s.CL].\n[24] S. Merity , C. Xiong, J. Bradbury , and R. Socher, “Pointe r sentinel\nmixture models, ” Sep. 2016, arXiv:1609.07843 [cs.CL].\n[25] C. Coup´ e, Y . M. Oh, D. Dediu, and F . Pellegrino, “Differ ent languages,\nsimilar encoding efﬁciency: Comparable information rates across the\nhuman communicative niche, ” Science Advances , vol. 5, no. 9, p.\neaaw2594, Sep. 2019.\n[26] E. N. Gilbert, “Codes based on inaccurate source probab ilities, ” IEEE\nTransactions on Information Theory , vol. IT -17, no. 3, pp. 304–314,\nMay 1971.\n[27] T . M. Cover and J. A. Thomas, Elements of Information Theory . New\nY ork: John Wiley & Sons, 1991.\n[28] Y . Sung, L. T ong, and H. V . Poor, “Neyman-Pearson detect ion of Gauss-\nMarkov signals in noise: Closed-form error exponent and pro perties, ”\nIEEE Transactions on Information Theory , vol. 52, no. 4, pp. 1354–\n1365, Apr . 2006.\n[29] H. Luschgy , A. L. Rukhin, and I. V ajda, “ Adaptive tests f or stochastic\nprocesses in the ergodic case, ” Stochastic Processes and their Applica-\ntions, vol. 45, no. 1, pp. 45–59, Mar . 1993.\n[30] C. D. Manning and H. Sch ¨ utze, F oundations of Statistical Natural\nLanguage Processing. Cambridge, MA, USA: MIT Press, 1999.\n[31] I. Csisz´ ar and Z. T alata, “On rate of convergence of sta tistical estimation\nof stationary ergodic processes, ” IEEE Transactions on Information\nTheory, vol. 56, no. 8, pp. 3637–3641, Aug. 2010.\n[32] C. Chelba, M. Norouzi, and S. Bengio, “ N-gram language modeling us-\ning recurrent neural network estimation, ” Mar . 2017, arXiv :1703.10724\n[cs.CL].\n[33] I. Sason and S. V erd ´ u, “ f-divergence inequalities, ” IEEE Transactions\non Information Theory , vol. 62, no. 11, pp. 5973–6006, Nov . 2016.\n[34] O. Binette, “ A note on reverse Pinsker inequalities, ” IEEE Transactions\non Information Theory , vol. 65, no. 7, pp. 4094–4096, Jul. 2019.\n[35] K. Marton, “Bounding ¯d-distance by informational divergence: a method\nto prove measure concentration, ” Annals of Probability , vol. 24, no. 2,\npp. 857–866, Apr . 1996.\n[36] ——, “Measure concentration for a class of random proces ses, ” Proba-\nbility Theory and Related Fields , vol. 110, pp. 427–439, Mar . 1998.\n[37] D. Harwell, “T op AI researchers race to detect ‘deepfak e’ videos: ‘We\nare outgunned’, ” The W ashington P ost, Jun. 2019.",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.8067046999931335
    },
    {
      "name": "Perplexity",
      "score": 0.780855655670166
    },
    {
      "name": "Computer science",
      "score": 0.7001293301582336
    },
    {
      "name": "Ergodic theory",
      "score": 0.6027716994285583
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5774844288825989
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5043331384658813
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.46335721015930176
    },
    {
      "name": "Natural language processing",
      "score": 0.4307847023010254
    },
    {
      "name": "Mathematics",
      "score": 0.2035299837589264
    },
    {
      "name": "Data mining",
      "score": 0.14902731776237488
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    }
  ],
  "cited_by": 1
}