{
  "title": "TransMed: Transformers Advance Multi-modal Medical Image Classification",
  "url": "https://openalex.org/W3134904735",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2224422689",
      "name": "Dai Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2380843783",
      "name": "Gao Yifan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963820951",
    "https://openalex.org/W2970546341",
    "https://openalex.org/W2608641923",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W3104897464",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2975244163",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3004967630",
    "https://openalex.org/W2441649867",
    "https://openalex.org/W2963046541",
    "https://openalex.org/W2132971773",
    "https://openalex.org/W3012412627",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963730812",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2158869829",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2963276418",
    "https://openalex.org/W2301358467",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2133059825",
    "https://openalex.org/W3018151807"
  ],
  "abstract": "Over the past decade, convolutional neural networks (CNN) have shown very competitive performance in medical image analysis tasks, such as disease classification, tumor segmentation, and lesion detection. CNN has great advantages in extracting local features of images. However, due to the locality of convolution operation, it can not deal with long-range relationships well. Recently, transformers have been applied to computer vision and achieved remarkable success in large-scale datasets. Compared with natural images, multi-modal medical images have explicit and important long-range dependencies, and effective multi-modal fusion strategies can greatly improve the performance of deep models. This prompts us to study transformer-based structures and apply them to multi-modal medical images. Existing transformer-based network architectures require large-scale datasets to achieve better performance. However, medical imaging datasets are relatively small, which makes it difficult to apply pure transformers to medical image analysis. Therefore, we propose TransMed for multi-modal medical image classification. TransMed combines the advantages of CNN and transformer to efficiently extract low-level features of images and establish long-range dependencies between modalities. We evaluated our model for the challenging problem of preoperative diagnosis of parotid gland tumors, and the experimental results show the advantages of our proposed method. We argue that the combination of CNN and transformer has tremendous potential in a large number of medical image analysis tasks. To our best knowledge, this is the first work to apply transformers to medical image classification.",
  "full_text": "Preprint.\nDigital Object Identiﬁer 10.1109/ACCESS.2017.DOI\nTransMed: Transformers Advance\nMulti-modal Medical Image Classiﬁcation\nYin Dai1,2, Yifan Gao1.\n1College of Medicine and Biological Information Engineering, Northeastern University, China\n2Engineering Center on Medical Imaging and Intelligent Analysis, Ministry Education, Northeastern University, Shenyang 110169, China\nCorresponding author: Yin Dai (e-mail: daiyin@bmie.neu.edu.cn).\nABSTRACT Over the past decade, convolutional neural networks (CNN) have shown very competitive\nperformance in medical image analysis tasks, such as disease classiﬁcation, tumor segmentation, and lesion\ndetection. CNN has great advantages in extracting local features of images. However, due to the locality of\nconvolution operation, it can not deal with long-range relationships well. Recently, transformers have been\napplied to computer vision and achieved remarkable success in large-scale datasets. Compared with natural\nimages, multi-modal medical images have explicit and important long-range dependencies, and effective\nmulti-modal fusion strategies can greatly improve the performance of deep models. This prompts us to\nstudy transformer-based structures and apply them to multi-modal medical images. Existing transformer-\nbased network architectures require large-scale datasets to achieve better performance. However, medical\nimaging datasets are relatively small, which makes it difﬁcult to apply pure transformers to medical\nimage analysis. Therefore, we propose TransMed for multi-modal medical image classiﬁcation. TransMed\ncombines the advantages of CNN and transformer to efﬁciently extract low-level features of images and\nestablish long-range dependencies between modalities. We evaluated our model for the challenging problem\nof preoperative diagnosis of parotid gland tumors, and the experimental results show the advantages of our\nproposed method. We argue that the combination of CNN and transformer has tremendous potential in\na large number of medical image analysis tasks. To our best knowledge, this is the ﬁrst work to apply\ntransformers to medical image classiﬁcation.\nINDEX TERMS Deep Learning, Medical Image Analysis, Transformer, Multi-modal.\nI. INTRODUCTION\nTransformers were ﬁrst applied in the ﬁeld of natural lan-\nguage processing (NLP) [1]. It is a deep neural network\nmainly based on the self-attention mechanism to extract\nintrinsic features. Because of its powerful representation ca-\npabilities, researchers hope to ﬁnd a way to apply transform-\ners to computer vision tasks. Compared with text, images\ninvolve larger size, noise, and redundant modalities, so it is\nconsidered more difﬁcult to use transformers on these tasks.\nRecently, transformers have made a breakthrough in com-\nputer vision. A large number of transformer-based methods\nhave been proposed for computer vision tasks, such as DETR\n[2] for object detection, SETR [3] for semantic segmentation,\nViT [4] and DeiT [5] for image recognition.\nTransformers have achieved success in natural images,\nbut it has received little attention in medical image analy-\nsis, especially in multi-modal medical image fusion. Multi-\nmodal images are widely used in medical image analysis\nto achieve lesion segmentation or disease classiﬁcation. The\nexisting medical image multi-modal fusion based on deep\nlearning can be divided into three categories: input-level\nfusion, feature-level fusion, and decision-level fusion [6].\nInput-level fusion strategy fuses multi-modal images into the\ndeep network by multi-channel, learns fusion feature repre-\nsentation, and then trains the network. Input-level fusion can\nretain the original image information to the maximum extent\nand learn the image features. Feature-level fusion strategy\ntrains a single deep network by taking the image of each\nmodality as a single input. Each representation is fused in the\nnetwork layer, and the ﬁnal result is fed to the decision layer\nto obtain the ﬁnal result. Feature-level fusion network can\neffectively capture the information of different modalities of\nthe same patient. Decision-level fusion integrates the output\nof each network to obtain the ﬁnal result. Decision-level\nfusion network aims to learn more abundant information\nfrom different modalities independently.\nVOLUME 4, 2016 1\narXiv:2103.05940v1  [cs.CV]  10 Mar 2021\nY . Daiet al.: TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nHowever, they all have shortcomings in varying degrees.\nThe input-level fusion strategy is difﬁcult to establish the\ninternal relationship between different modalities of the same\npatient, which leads to the degradation of the model per-\nformance. Each modality of the feature-level network corre-\nsponds to a neural network, which brings huge computational\ncosts, especially in the case of a large number of modali-\nties. The output of each modality of decision-level fusion\nis independent of each other, so the model cannot establish\nthe internal relationship between different modalities of the\nsame patient. In addition, like decision-level fusion strategy,\ndecision-level fusion strategy is also computationally inten-\nsive.\nTherefore, there is an urgent need to combine the three\nfusion strategies efﬁciently. A good multi-modal fusion strat-\negy should achieve as much interaction between different\nmodalities as possible with low computational complexity.\nCompared with CNN, transformers can effectively mine\nlong-range relationships between sequences. The existing\ncomputer vision models based on transformer mainly deal\nwith 2D natural images, such as ImageNet [6] and other\nlarge-scale datasets. The method of constructing sequences in\n2D images is to cut the images into a series of patches. This\nkind of sequence construction method implicitly shows long-\nrange dependencies, which is not very intuitive, so it may be\ndifﬁcult to bring signiﬁcant performance improvement.\nOn the contrary, there are more explicit sequences in med-\nical images, which contain important long-range dependency\nand semantic information, as shown in Fig.1. Due to the\nsimilarity of human organs, most visual representations are\norderly in medical images. Destruction of these sequences\nwill signiﬁcantly reduce the performance of the model. It\ncan be considered that compared with natural images, the\nsequence relationship of medical images (such as modality,\nslice, patch) holds more abundant information. In practice,\ndoctors will synthesize the pathological information of each\nmodality to make the diagnosis. However, the existing multi-\nmodal fusion methods are too simple to consider the correla-\ntion of these sequences, and lack of modeling for these long-\nrange dependencies. The transformer is an elegant, efﬁcient,\nand powerful encoder for processing sequence relations,\nwhich is the motivation for us to propose the multi-modal\nmedical image classiﬁcation method based on transformers.\nIn this work, we present the ﬁrst study to explore the\ntremendous potential of transformers in the context of multi-\nmodal medical image classiﬁcation. The proposed method\nis inspired by the property that the transformer is effective\nin extracting the relationship between sequences. However,\ndue to the small scale of medical image datasets and the\nlack of sufﬁcient information to establish the relationship\nbetween low-level semantic features, the performance of pure\ntransformer networks based on ViT and DeiT is not satisfac-\ntory in multi-modal medical image classiﬁcation. Therefore,\nwe propose TransMed, which combines the advantages of\nCNN and transformer to capture low-level features and cross-\nmodality high-level connections. TransMed ﬁrst processes\nFIGURE 1. Compared with natural images, multi-modal medical images have\nmore informative sequences.\nthe multi-modal images as sequences and sends them to\nCNN, then uses transformers to learn the relationship be-\ntween the sequences and make predictions. Since the trans-\nformer effectively models the global features of multi-modal\nimages, TransMed outperforms the existing multi-modal fu-\nsion methods in terms of parameters, operation speed, and\naccuracy. A large number of experiments have proved the\neffectiveness of our method.\nIn summary, we make the following three contributions:\n1) We apply transformers to medical image classiﬁcation\nfor the ﬁrst time, and greatly improve the accuracy and\nefﬁciency of deep models.\n2) We propose a novel multi-modal image fusion strategy\nin this work, which can be leveraged to capture mutual\ninformation from images of different modalities in a\nmore efﬁcient way.\n3) Experimental evaluations demonstrate that the pro-\nposed method achieves the most advanced perfor-\nmance in the classiﬁcation of the parotid gland tumors.\nThe rest of this paper is organized as follows. Section\nII presents some closely related works. The pipeline of our\nproposed method is in Section III. Section IV introduces the\nexperimental results and details. Finally, we summarize our\nwork in Section V .\nII. RELATED WORK\nA. MULTI-MODAL MEDICAL IMAGE ANALYSIS\nMulti-modal medical analysis is one of the most fundamental\nand challenging parts of medical image analysis. It is proved\nthat a reasonable fusion of different modalities has been a\npotential means to enhance Deep networks [6]. Multi-modal\nfusion can capture more abundant pathological information\nand improve the quality of diagnosis.\n[8]–[10] mainly used the input-level fusion, which is\nthe most common fusion method in multi-modal medical\nimage analysis. Some other papers have shown the potential\nof feature-level fusion in medical image processing. Hyper\nDenseNet built dual deep networks for different modalities\n2 VOLUME 4, 2016\nY . Daiet al.: TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nFIGURE 2. Overview of TransMed, which is composed of CNN branch and transformer branch.\nof Magnetic resonance imaging (MRI) and linked features\nacross these streams [11]. [12] fused ﬁnal features from\nmodality-speciﬁc paths to make ﬁnal decisions. MMFNet\nused speciﬁc encoders to capture modality-speciﬁc features\nand designs a decoder with a complex structure to fuse these\nfeatures [13]. Different from the ﬁrst two techniques, [14],\n[15] applied decision-level fusion technology to improve\nperformance. [15] set three modality-speciﬁc encoders to\ncapture low-level features and a decoder to fuse low-level and\nhigh-level features, then the results of each branch were fused\nto generate the ﬁnal result. [14] designed a gate network to\ndynamically combine each decision and make a prediction.\nBesides, some studies have evaluated multiple fusion\nmethods at the same time. [16] used feature-level fusion and\ndecision-level fusion in their work. [17] designed three kinds\nof fusion networks, and gets better performance than a single\nmodality. These fusion methods improve the performance of\nthe model to a certain extent, but there are some shortcom-\nings, such as poor scalability, large computational complex-\nity, and difﬁculty in establishing long-range connections.\nB. TRANSFORMERS\nTransformers were ﬁrst proposed for machine translation\nand achieved satisfactory results in a large number of NLP\ntasks. Then, the transformer structures were introduced into\nthe ﬁeld of computer vision, and some modiﬁcations were\nmade according to the speciﬁc tasks. The results show the\npotential of transformers to surpass pure CNN. Some work\nuses the framework of CNN and transformer [2], [18], while\nothers directly use pure transformers to replace CNN [2], [4],\n[5], [19]. These methods have shown encouraging results in\ncomputer vision tasks, but their direct applications in multi-\nmodal medical images are not effective and require a lot\nof computing resources. As far as we know, TransMed is\nthe ﬁrst multi-modal medical image classiﬁcation framework\nbased on transformers, which provides a novel multi-modal\nimage fusion strategy.\nIII. METHODS\nThe most common method of multi-modal medical image\nclassiﬁcation is to train CNN directly (such as Resnet [20]).\nFirstly, the image is encoded as a high-level feature represen-\ntation, and then its features or decisions are fused. Different\nfrom the existing methods, our method uses transformers\nto introduce the self-attention mechanism into the multi-\nmodal fusion strategy. We will ﬁrst introduce how to directly\napply transformers to aggregate feature representations from\ndecomposed image patches in Section 3.A. Then, the overall\nframework of TransMed will be described in detail in Section\n3.B.\nA. TRANSFORMERS AGGREGATE MULTI-MODAL\nFEATURES\nIn this work, we follow the original DeiT implementation\nas much as possible. The advantage of this intentionally\nsimple setting is to reduce the impact of other tricks on the\nperformance of the model and intuitively show the beneﬁts\nof transformers. In addition, we can use the extensible DeiT\nmodel and its pre-trained weights almost immediately.\nVOLUME 4, 2016 3\nY . Daiet al.: TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nFIGURE 3. Overview of self-attention, matmul means matrix product of two\narrays.\nThe important components of the transformer including\nself-attention (SA), multi-head self-attention (MSA), and\nmulti-layer perception (MLP). The input of transformers\nincludes a variety of embeddings and tokens. Slightly dif-\nferent from DeiT, we remove the linear projection layer and\ndistillation token. We will describe each of these components\nin this section.\n1) Self-Attention\nSA is an attention mechanism, which uses other parts of\nthe same sample to predict the rest of the data sample. In\ncomputer vision, it is a little similar to non-local networks\n[21]. SA has many forms, and the common transformer relies\non the form of scaled dot-product shown in Figure 3. In\nthe SA layer, the input vector is ﬁrst transformed into three\ndifferent vectors: query matrix Q, key matrix K, and value\nmatrix V , the output is the weighted sum of the value vectors.\nThe weight assigned to each value is determined by the dot\nproduct of the query and the corresponding key. The attention\nfunction between different input vectors is calculated as\nfollows:\nAttention(Q, K, V) =Softmax (QKT\n√dk\n) ·V (1)\nWhere dk is the dimension of key vector k. √dk provides\nan appropriate normalization to make the gradient more\nstable.\n2) Multi-head Self-Attention\nMSA is the core component of the transformer. As shown\nin Figure 4, The difference from SA is that the multi-head\nmechanism splits the input into many small parts, then cal-\nculates the scaled dot-product of each input in parallel, and\nsplices all the attention outputs to get the ﬁnal result. The\nformula of MSA can be written as follows:\nheadi = Attention(QWQ\ni , KWK\ni , V WV\ni ) (2)\nMSA (Q, K, V) =Concat(headi, ..., headi)WO (3)\nFIGURE 4. An illustration of our multi-head self-attention component, concat\nmeans concatenate representations.\nWhere the projections WQ\ni , WK\ni , WV\ni and WO are train-\nable parameter matrices, h is the number of transformer lay-\ners. The advantage of MSA is that it allows the model to learn\nsequence and location information in different representation\nsubspaces.\n3) Multi-Layer Perceptron\nIn this paper, a MLP is added on top of the MSA layer. The\nMLP is composed of linear layers separated by a GeLU [22]\nactivation. Both MSA and MLP have skip-connections like\nresidual networks and with a layer normalization. Therefore,\nit is assumed that the representation of thet−1 layer is xt−1,\nLN represents the linear normalization, and the output of the\nt layer can be written as follows:\nˆxt = MSA (LN(xt−1)) +xt−1 (4)\nxt = MLP (LN( ˆxt)) + ˆxt (5)\n4) Embeddings and Tokens\nThe input layer contains ﬁve embeddings and tokens, which\nare patch embedding, position embedding, class embedding,\npatch token, and class token.\nPatch embedding is the representation of each patch’s\noutput from CNN, and class embedding is a trainable vector.\nTo encode the spatial information and location information\nof a patch into patch tokens, we use position embeddings and\npatch embeddings to preserve the information. Class embed-\nding does not have patch embedding that can be added, so\nclass token and class embedding are equivalent. Suppose the\ninput is x, the trainable vector isWc, the position embedding\nis xpo, patch tokens xpt and class token xct can be expressed\nas follows:\nxpt = Conv(x) +xpo (6)\nxct = Wc (7)\nThe class token is attached to patch tokens before the input\nlayer of transformers, passes through the transformer layer,\n4 VOLUME 4, 2016\nY . Daiet al.: TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nand then outputs from the fully connected layer to predict the\nclass.\nB. TRANSMED\nThe structure of TransMed is shown in Figure 2. Instead of\nusing pure transformers as the encoder, TransMed adopts a\nhybrid model including CNN and transformer, in which CNN\nis used as a low-level feature extractor to generate the patch\nembedding.\nGiven a multi-modal image x ∈ RB×M×C×D×H×W ,\nwhere spatial resolution is H ×W, the depth is D, the\nnumber of channels is C, the number of modalities is M,\nand the batch size is B. Before sending it to the CNN en-\ncoder, it is necessary to construct the sequence. Firstly, three\nadjacent 2D slices of a multi-modal image are superimposed\nto construct three-channel images. Then, according to [4],\neach image will be divided into K ×K. The larger K value\nmeans that the size of each patch is smaller. We will evaluate\nthe impact of different K values on the performance of the\nmodel in Section 4.E. Finally, the image is encoded into a\n(1\n3 BMCDK 2, 3, H\nK , W\nK ) patch.\nAfter the image sequence is constructed, it is input into\nthe 2D CNN. The last fully connected layer of 2D CNN\nis replaced by a linear projection layer to map the fea-\ntures of the vector patch to the potential embedding space.\n2D CNN extracts low-level features from the image se-\nquence and encodes them preliminarily. The output shape is\n(B, 1\n3 MCDK 2, P), in which the size of P is set to adapt to\nthe input size of the transformer.\nIV. RESULTS\nA. DATASET AND PREPROCESSING\n1) DATASET\nWe use a dataset collected in cooperative hospitals to evaluate\nthe performance of our proposed method under multi-modal\nimages. This dataset included two modalities of MRI (T1 and\nT2) of 344 patients, and the ground truth labels are obtained\nfrom biopsies.\nThe incidence of malignant tumors in parotid gland tumors\nis about 20% [23]. Correct preoperative diagnosis of these tu-\nmors is essential for proper surgical planning. Among them,\nimaging examination plays an important role in determining\nthe nature of parotid gland masses. MRI is considered to\nbe the preferred imaging method for preoperative diagnosis\nof parotid tumors [24]. MRI can provide information about\nthe exact location of the lesion, the relationship with the\nsurrounding structure, and can assess the spread of nerves\nand bone invasion. However, it is reported that parotid gland\ntumors show considerable overlap in imaging features (such\nas tumor margins, homogeneity, and signal intensity), so it is\ndifﬁcult for doctors to identify the mass.\nAccording to common clinical classiﬁcations, we divide\nparotid gland tumors into ﬁve categories: Pleomorphic Ade-\nnoma (PA), Warthin Tumor (WT), Malignant Tumor (MT),\nBasal Cell Adenoma (BCA), and Other Benign Lesions\n(OBL) [25].\n2) PREPROCESSING\nFirst, perform OTSU [26] to extract the foreground area in\nthe original image. Then the images of different modalities\nof the same patient are registered to improve the consistency\nof the foreground area. Then resample each image to (18,\n448, 448). Therefore, 344 images are ﬁnally included, each of\nwhich is a stack of 3D images of MRI T1 and T2, and the size\nis (36, 448, 448). Data augmentation uses random ﬂipping\nand random noise. Random ﬂipping performs ﬂipping of the\nimage with 50 % probability. Random noise adds Gaussian\nnoise with a mean value of 0 and a variance of 0.1 to the\nimage.\nB. EXPERIMENTAL SETTINGS AND EVALUATION\nCRITERIA\nThe patients were randomly divided into the training group (n\n= 275) and independent test group (n = 69) according to the\nratio of 4:1, and then the training group was used to optimize\nthe model parameters. We set SGD as the optimizer with\na learning rate equal to 10−3 and momentum equal to 0.7.\nThe maximum training round is set to 100. Our experiments\nwere performed on NVIDIA 3080 GPU (with 10GB GPU\nmemory). The code is implemented using Pytorch [27] and\nTorchIO [28]. To eliminate accidental factors, each model\nis subjected to 10 independent experiments, and each ex-\nperiment is randomly divided into the training group and\nthe test group. Besides, other experimental parameters keep\nconsistent during training.\nThe evaluation criteria for each model are the overall\naccuracy rate ACC(i) and the precision rate of each category\nP(i), as deﬁned in the following:\nACC = Tc\nT (8)\nWhere T is the total number of samples and Tc is the total\nnumber of samples with the correct prediction.\nP(i) = Tic\nTic + Tif\n(9)\nWhere Tic is the total number of samples correctly pre-\ndicted as class i, and Tif is the total number of samples\nthat are wrongly predicted as class i. P(i) can describe the\nstability and robustness of the model in small datasets.\nC. BASELINE METHODS\nThe input-level fusion strategy can be easily implemented\nusing mainstream 2D CNN and 3D CNN, so the selected\nnetwork includes Resnet34, Resnet152, 3D Resnet34, P3D\n[29], and C3D [30]. In feature-level fusion experiments, we\nused two common feature-level fusion methods [11], [12].\nSince these two papers focus on segmentation tasks, we\nmodify the network structure to adapt to the classiﬁcation\ntasks. The deep network used in the decision-level fusion\nexperiments is the same as the input-level strategy.\nVOLUME 4, 2016 5\nY . Daiet al.: TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nTABLE 1. Comparison on the parotid gland tumors dataset (average accuracy % and precision % for each disease. IF , FF and DF represent input-level fusion,\nfeature-level fusion and decision-level fusion, respectively.).\nMethod Dim Backbone Fusion Params TFlops Acc PA WT MT BCA OBL\nP3D 3D P3D IF 67M 0.31 76.1±5.5 59.9±23.1 84.3±5.3 69.7±19.0 71.4±7.3 78.0±14.0\nC3D 3D ConvNet IF 28M 1.36 71.0±4.1 68.3±38.9 81.3±15.4 67.8±10.0 71.4±7.5 84.5±12.4\nResnet34 2D Resnet34 IF 22M 0.02 69.9±4.0 81.0±11.0 77.6±7.4 61.4±10.8 53.8±15.5 68.1±7.9\nResnet152 2D Resnet152 IF 58M 0.05 69.0±3.5 50.5±18.0 74.1±10.1 64.3±20.0 62.9±8.9 75.2±11.4\n3D Resnet34 3D 3D Resnet34 IF 64M 1.08 73.3±5.1 69.2±16.2 86.8±5.2 75.3±18.1 68.7±13.8 65.7±6.4\n[11] 3D ConvNet FF 45M 3.47 74.2±2.9 76.0±24.8 86.2±10.0 72.9±16.3 75.2±24.6 80.0±15.2\n[12] 3D 3D Resnet34 FF 130M 1.35 73.3±2.4 46.2±13.2 78.4±7.9 70.2±15.5 69.8±15.8 79.0±10.7\n3D Resnet34 3D 3D Resnet34 DF 128M 1.28 72.1±3.5 64.7±14.4 81.5±9.3 66.8±8.4 69.6±8.9 72.1±14.9\nP3D 3D P3D DF 136M 0.42 74.8±4.6 50.5±20.0 85.1±4.4 70.5±20.2 69.5±8.7 73.4±14.3\nC3D 3D ConvNet DF 57M 1.45 71.0±3.3 58.3±33.3 70.7±9.3 74.0±8.5 78.9±20.0 73.2±6.7\nResnet34 2D Resnet34 DF 45M 0.03 71.3±4.5 72.7±21.7 75.3±8.1 72.5±10.3 60.9±16.8 70.3±9.7\nResnet152 2D Resnet152 DF 116M 0.06 72.2±5.5 63.5±18.3 75.6±10.4 73.4±18.7 83.2±16.3 69.6±11.5\nTransMed-T 2D Renset18+DeiT-T —— 17M 0.09 87.0±2.6 80.1±13.8 87.3±3.0 90.7±5.1 82.5±15.3 93.6±3.3\nTransMed-S 2D Resnet34+DeiT-S —— 43M 0.19 88.9±3.0 90.1±12.2 89.2±6.8 92.0±4.4 82.9±9.3 88.3±6.1\nTransMed-B 2D Resnet50+DeiT-B —— 110M 0.22 87.4±2.1 86.2±15.2 88.4±3.8 88.2±7.0 84.8±13.8 92.2±8.0\nTransMed-L 2D Resnet152+DeiT-B —— 145M 0.58 86.6±3.4 85.4±12.9 88.7±6.1 90.4±5.3 75.9±8.0 91.8±7.5\nD. RESULTS\nTable. 1 reports the performance of our proposed models, in\nwhich four variants are provided: the tiny version (TransMed-\nT) use ResNet18 and DeiT-Tiny (DeiT-T) as backbones\nfor CNN branch and transformer branch, respectively; the\nsmall version (TransMed-S) use ResNet34 and DeiT-Small\n(DeiT-S) as backbone; the base version (TransMed-B) use\nResNet50 and DeiT-Base (DeiT-B) as backbone; the large\nversion (TransMed-L) use ResNet152 and DeiT-B.\nTransMed consistently outperforms previous multi-modal\nfusion strategies by a large margin. TransFuse-S achieves on\naverage about 12.8 % improvement in terms of the average\naccuracy with respect to the P3D while the larger version\nTransMed-B and TransMed-L slightly suffer from overﬁtting\non the dataset. Table 1 also compares the number of parame-\nters and computational costs between our proposed models\nand previous methods. TransMed achieves state-of-the-art\nperformance with much fewer parameters and computational\ncosts. TransMed is highly efﬁcient as it models the long-\nrange relationship between modalities very well. We expect\nthat our method can inspire further exploration of multi-\nmodal medical image fusion in future work.\nE. ABLATION EXPERIMENTS\nTo demonstrate the effect of transformers in TransMed, we\nconducted ablation experiments. For TransMed, changing\nthe backbone from TransMed-T to TransMed-S results in\n1.9% improvement in average accuracy, at the expense of\na much larger computational cost. Therefore, considering\nthe computation cost, all experimental comparisons in this\npaper are conducted with TransMed-T to demonstrate the\neffectiveness of TransMed.\nIn the experiment, TransMed’s CNN and transformers\nwere removed respectively, and all other conditions remained\nunchanged. The results are shown in Table 2. The results in-\ndicate that the transformer greatly improves the ability of the\ndeep model to explore the relationship between modalities\nwith little increase of parameters and computation. However,\nthe performance of the pure transformer structure is poor due\nto the small dataset.\nWe also explored the impact of different patch sizes on\nperformance in image serialization by changing K values\nrespectively while other conditions remain unchanged. The\nresults are shown in Table 3. The experimental results show\nthat the performance is poor when the K value is large. The\npossible reason is that too small image patches destroy the\nsemantic information of the image.\nV. CONCLUSION\nThe transformer is a powerful deep neural network structure\nfor processing sequences in NLP, but it has received little\nattention in medical image analysis. In this paper, we propose\nTransMed, which is a novel design of multi-modal medical\nimage classiﬁcation based on transformers. TransMed has\nachieved very competitive results in challenging parotid tu-\nmor classiﬁcation. TransMed is easy to implement and has a\nﬂexible structure, which can be extended to multiple medical\nimage modalities with low resource cost.\nThese preliminary results are encouraging, but there are\nstill many challenges. One is to apply TransMed to other\nmedical image analysis tasks, such as tumor segmentation\nand lesion detection. Another challenge is to use the pure\ntransformer structure. Pure transformer structure has been\nsuccessful in large-scale natural image datasets. However,\n6 VOLUME 4, 2016\nY . Daiet al.: TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nTABLE 2. Ablation study on the effectiveness of CNN branch and transformer branch.\nModel Params TFlops Acc PA WT MT BCA OBL\nTransMed-T 17M 0.09 87.0±2.6 80.1±13.8 87.3±3.0 90.7±5.1 82.5±15.3 93.6±3.3\nw/o transformer 12M 0.01 71.3±2.5 71.6±17.8 78.9±3.9 73.9±12.5 66.9±13.1 69.9±13.3\nw/o CNN 5M 0.07 51.3±5.9 20.0±18.7 61.5±13.5 37.0±5.7 41.7±40.1 51.0±16.4\nTABLE 3. Ablation study on different patch sizes.\nModel K Acc PA WT MT BCA OBL\nTransMed-T 1 86.8±2.3 83.1±12.4 90.1±3.9 88.7±10.1 78.3±13.7 95.3±5.9\nTransMed-T 2 87.0±2.6 80.1±13.8 87.3±3.0 90.7±5.1 82.5±15.3 93.6±3.3\nTransMed-T 4 86.4±3.3 86.3±17.2 87.0±6.5 89.5±3.6 75.5±8.3 92.1±7.2\nTransMed-T 8 80.0±5.8 81.2±17.4 81.1±5.3 88.5±4.2 63.9±12.1 88.2±13.6\nTransMed-T 16 65.2±5.8 49.0±32.2 72.1±7.7 70.4±7.9 62.3±23.0 64.1±14.5\nour preliminary experiments show that there is still a big\ngap between the pure transformer and typical CNN in small\nmedical image datasets. We expect future work to further\nimprove TransMed.\nREFERENCES\n[1] Vaswani A et al., “ Attention is All you Need,” in Proc. Conf. Neural Inf.\nProcess. Syst. (NeurIPS), 2017, pp 1-12.\n[2] Carion et al., “ End-to-end object detection with transformers,” in Proc.\nEur. Conf. Comput. Vis. (ECCV), 2020, pp. 213-229.\n[3] S. Zheng et al., “ Rethinking Semantic Segmentation from a Sequence-\nto-Sequence Perspective with Transformers,” ArXiv, [Online]. Available:\nhttp://arxiv.org/abs/2012.15840.\n[4] A. Dosovitskiy et al., “ An Image is Worth 16x16 Words: Trans-\nformers for Image Recognition at Scale,” ArXiv, [Online]. Available:\nhttp://arxiv.org/abs/2010.11929.\n[5] H. Touvron et al., “ Training data-efﬁcient image transformers\n& distillation through attention,” ArXiv, [Online]. Available:\nhttp://arxiv.org/abs/2012.12877.\n[6] T. Zhou, S. Ruan, and S. Canu, “A review: Deep learning for medical\nimage segmentation using multi-modality fusion,” in Array., vol. 3–4, p.\n100004, Sep. 2019, doi: 10.1016/j.array.2019.100004.\n[7] J. Deng et al., “ ImageNet: A large-scale hierarchical image database,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2009, pp.\n248–255, doi: 10.1109/CVPR.2009.5206848.\n[8] A. Myronenko et al., “3D MRI Brain Tumor Segmentation Using Autoen-\ncoder Regularization,” in Proc. Int. Conf. Med. Image Comput. Comput.-\nAssist. Intervent. Workshop(MICCAI), 2019, vol. 11384, pp. 311–320, doi:\n10.1007/978-3-030-11726-9.\n[9] K. Kamnitsas et al., “ Efﬁcient multi-scale 3D CNN with fully connected\nCRF for accurate brain lesion segmentation,” inMed. Image Anal., vol. 36,\npp. 61–78, Feb. 2017, doi: 10.1016/j.media.2016.10.004.\n[10] F. Isensee, P. Kickingereder, W. Wick, M. Bendszus, and K. H. Maier-\nHein, “Brain Tumor Segmentation and Radiomics Survival Prediction:\nContribution to the BRATS 2017 Challenge,” in Proc. Int. Conf. Med.\nImage Comput. Comput.-Assist. Intervent. Workshop(MICCAI), 2018, pp.\n287–297, doi: 10.1007/978-3-319-75238-9.\n[11] J. Dolz, K. Gopinath, J. Yuan, H. Lombaert, C. Desrosiers, and I. Ben\nAyed, “HyperDense-Net: A Hyper-Densely Connected CNN for Multi-\nModal Image Segmentation,” in IEEE Trans. Med. Imaging , vol. 38, no.\n5, pp. 1116–1126, May 2019, doi: 10.1109/TMI.2018.2878669\n[12] D. Nie, L. Wang, Y . Gao, and D. Shen, “Fully convolutional networks for\nmulti-modality isointense infant brain image segmentation,” in2016 IEEE\n13th International Symposium on Biomedical Imaging (ISBI) , Apr. 2016,\npp. 1342–1345, doi: 10.1109/ISBI.2016.7493515.\n[13] H. Chen et al., “ MMFNet: A multi-modality MRI fusion network for\nsegmentation of nasopharyngeal carcinoma,” in Neurocomputing., 2020,\nvol. 394, pp. 27-40, doi: https://doi.org/10.1016/j.neucom.2020.02.002\n[14] Y . Shachor, H. Greenspan, and J. Goldberger, “A mixture of views network\nwith applications to multi-view medical imaging,” inNeurocomputing, vol.\n374, pp. 1–9, Jan. 2020, doi: 10.1016/j.neucom.2019.09.027.\n[15] K.-L. Tseng, Y .-L. Lin, W. Hsu, and C.-Y . Huang, “Joint Sequence Learn-\ning and Cross-Modality Convolution for 3D Biomedical Segmentation,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp.\n3739–3746, doi: 10.1109/CVPR.2017.398.\n[16] A. A. A. Setio et al., “ MMFNet: A multi-modality MRI fusion net-\nwork for segmentation of nasopharyngeal carcinoma,” in IEEE Trans.\nMed. Imaging. , vol. 35, no. 5, pp. 1160–1169, May 2016, doi:\n10.1109/TMI.2016.2536809.\n[17] Z. Guo, X. Li, H. Huang, N. Guo, and Q. Li, “Medical image seg-\nmentation based on multi-modal convolutional neural network: Study\non image fusion schemes,” in 2018 IEEE 15th International Sympo-\nsium on Biomedical Imaging (ISBI) , Apr. 2018, pp. 903–907, doi:\n10.1109/ISBI.2018.8363717.\n[18] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR:\nDeformable Transformers for End-to-End Object Detection,” in Arxiv,\n[Online]. Available: http://arxiv.org/abs/2010.04159.\n[19] Chen M et al., “Generative pretraining from pixels,” inInt. Conf. on Mach.\nLearn. (ICML), 2020, pp. 1691-1703.\n[20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image\nRecognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nLas Vegas, NV , USA, Jun. 2016, pp. 770–778.\n[21] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-Local Neural Net-\nworks,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018,\npp. 7794-7803.\n[22] D. Hendrycks and K. Gimpel, “Gaussian Error Linear Units (GELUs),” in\nArxiv, [Online]. Available: http://arxiv.org/abs/1606.08415.\n[23] R. A. Lima et al., “Clinical Prognostic Factors in Malignant Parotid Gland\nTumors,” in Otolaryngol. Neck Surg., vol. 133, no. 5, pp. 702–708, Nov.\n2005.\n[24] V . Q. Joe and P. L. Westesson., “Tumors of the parotid gland: MR imaging\ncharacteristics of various histologic types,” inAm. J. Roentgenol., vol. 163,\nno. 2, pp. 433–438, Aug. 1994.\n[25] J.-S. Jiang et al., “ Added value of susceptibility-weighted imaging to\ndiffusion-weighted imaging in the characterization of parotid gland tu-\nmors,” in Eur. Arch. Otorhinolaryngol., ol. 277, no. 10, pp. 2839–2846,\nOct. 2020.\n[26] Otsu N., “A threshold selection method from gray-level histograms,” in\nIEEE Trans. Systems, Man, and Cybernetics. , vol. 9, no. 1, pp. 62-66,\n1979.\n[27] A. Paszke et al., “ PyTorch: An Imperative Style, High-\nPerformance Deep Learning Library,” ArXiv, [Online]. Available:\nhttp://arxiv.org/abs/1912.01703.\n[28] F. Pérez-García, R. Sparks, and S. Ourselin, “TorchIO: a Python library\nfor efﬁcient loading, preprocessing, augmentation and patch-based sam-\npling of medical images in deep learning,” in Arxiv, [Online]. Available:\nhttp://arxiv.org/abs/2003.04696.\nVOLUME 4, 2016 7\nY . Daiet al.: TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\n[29] Z. Qiu, T. Yao, and T. Mei, “Learning Spatio-Temporal Representation\nWith Pseudo-3D Residual Networks,” in Proc. IEEE Inter. Conf. Comput.\nVis. (ICCV), 2017, pp. 5533-5541.\n[30] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learn-\ning Spatiotemporal Features with 3D Convolutional Networks,” in Proc.\nIEEE Inter. Conf. Comput. Vis. (ICCV) , Santiago, Chile, Dec. 2015, pp.\n4489–4497.\n8 VOLUME 4, 2016",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7405182719230652
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6670879125595093
    },
    {
      "name": "Transformer",
      "score": 0.6519244313240051
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6160647869110107
    },
    {
      "name": "Locality",
      "score": 0.5381044149398804
    },
    {
      "name": "Modal",
      "score": 0.46328750252723694
    },
    {
      "name": "Medical imaging",
      "score": 0.4622015953063965
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4473626911640167
    },
    {
      "name": "Segmentation",
      "score": 0.42655253410339355
    },
    {
      "name": "Machine learning",
      "score": 0.32537591457366943
    },
    {
      "name": "Engineering",
      "score": 0.1118110716342926
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I87182695",
      "name": "Universidad del Noreste",
      "country": "MX"
    }
  ]
}