{
    "title": "Efficient Dynamic WFST Decoding for Personalized Language Models",
    "url": "https://openalex.org/W2981436362",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100361857",
            "name": "Jun Liu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5050766674",
            "name": "Jiedan Zhu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5011745485",
            "name": "Vishal Kathuria",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5047400593",
            "name": "Fuchun Peng",
            "affiliations": [
                "Meta (Israel)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1895481600",
        "https://openalex.org/W2406494896",
        "https://openalex.org/W1604697534",
        "https://openalex.org/W2402365080",
        "https://openalex.org/W2407340526",
        "https://openalex.org/W1582482241",
        "https://openalex.org/W143425341",
        "https://openalex.org/W205749991",
        "https://openalex.org/W2293829681"
    ],
    "abstract": "We propose a two-layer cache mechanism to speed up dynamic WFST decoding with personalized language models. The first layer is a public cache that stores most of the static part of the graph. This is shared globally among all users. A second layer is a private cache that caches the graph that represents the personalized language model, which is only shared by the utterances from a particular user. We also propose two simple yet effective pre-initialization methods, one based on breadth-first search, and another based on a data-driven exploration of decoder states using previous utterances. Experiments with a calling speech recognition task using a personalized contact list demonstrate that the proposed public cache reduces decoding time by factor of three compared to decoding without pre-initialization. Using the private cache provides additional efficiency gains, reducing the decoding time by a factor of five.",
    "full_text": "Efﬁcient Dynamic WFST Decoding for Personalized Language Models\nJun Liu, Jiedan Zhu, Vishal Kathuria, Fuchun Peng\nFacebook AI, Menlo Park, CA, USA\n{junliu, jiedan, vishal, fuchunpeng}@fb.com\nAbstract\nWe propose a two-layer cache mechanism to speed up dynamic\nWFST decoding with personalized language models. The ﬁrst\nlayer is a public cache that stores most of the static part of\nthe graph. This is shared globally among all users. A sec-\nond layer is a private cache that caches the graph that repre-\nsents the personalized language model, which is only shared\nby the utterances from a particular user. We also propose two\nsimple yet effective pre-initialization methods, one based on\nbreadth-ﬁrst search, and another based on a data-driven explo-\nration of decoder states using previous utterances. Experiments\nwith a calling speech recognition task using a personalized con-\ntact list demonstrate that the proposed public cache reduces de-\ncoding time by factor of three compared to decoding without\npre-initialization. Using the private cache provides additional\nefﬁciency gains, reducing the decoding time by a factor of ﬁve.\nIndex Terms: Speech Recognition, WFST, Pre-Composition,\nPersonalized Language Model\n1. Introduction\nSpeech input is now a common feature for smart devices. In\nmany cases, the user’s query involves entities such as a name\nfrom a contact list, a location, or a music title. Recognizing en-\ntities is particularly challenging for speech recognition because\nmany entities are infrequent or out of the main vocabulary of\nthe system. One way to improve performance is such cases is\nthrough the use of a personal language model (LM) which con-\ntains the expected user-speciﬁc entities. Because each user can\nhave their own personalized LM, it is vital that the speech de-\ncoder be able to efﬁciently load the model on the ﬂy, so it can\nbe used in decoding, without any noticeable increase in latency.\nMany state-of-the-art speech recognition decoders are\nbased on the weighted ﬁnite state transducer (WFST)\nparadigm [1, 2]. A conventional WFST decoder searches a\nstatically composed HCLG graph, where H is the graph that\ntranslates HMM states to CD phones, C translates CD phones\nto graphemes, Ltranslates graphemes to words and Gis graph\nthat represents the language model. Using a statically composed\ngraph has two limitations. First, it is both compute and mem-\nory intensive when the vocabulary and LM are large. Second,\nthe static graph approach makes it hard to handle personalized\nlanguage models [3]. Many common tasks a user may want to\nperform with a voice assistant such as making phone calls, mes-\nsaging to a speciﬁc contact or playing favorite music require\na personalized language model. A dynamic WFST decoder is\nbetter suited for such cases. As denoted in Eq (1), in a dynamic\nWFST decoder,HCL is composed and optimized ofﬂine, while\nGis composed on the ﬂy with lazy (on-demand) composition,\ndenoted as ◦.\nT = HCL ◦G (1)\nTo handle dynamic entities, a class LMGc is normally used\nas background Gand a personalized LM Gp is replaced on-the-\nﬂy, before applying lazy composition.\nT = HCL ◦Replace(Gc,Gp) (2)\nSince the non-terminal states are composed on-the-ﬂy, it\nmeans the states of recognition FST will also contain personal-\nized information that cannot be used by other users or service\nthreads.\nIn previous work, a method was proposed to do a pre-\ninitialized composition for a non-class LM [4]. However, it the\ndynamic part is still expanded on-the-ﬂy. In this work, we pro-\npose two improvements in order to best leverage class language\nmodels. First, we use simpler methods for pre-initialization\nwhich do not need to pre-generate decoder state statistics. Sec-\nond, we propose a two-layer pre-initialization mechanism that\nalso avoids performing dynamic expansion on per user basis.\nIn the two-layer pre-initialization method, we make use of a\nclass LM with class tag. We build a personalized FST that con-\ntains the members of the class for each user. Using the FST re-\nplacement algorithm, we obtain a personalized language trans-\nducer [5]. We perform a pre-composition for all FST states\nwhose transitions do not contain class tags. By doing so, the\nactual on-demand composition is only required for the states\nin personalized FST. For a multi-threaded service, the pre-\ncomposed FST can be shared by all threads, since it does not\ncontain personalized FST states (non-terminals). The personal-\nized part will be shared for all utterances from the same user,\nwhich will take full advantage of memory usage.\nUnlike the previous pre-initialization approach that is based\non calculating the state statistics [4], our simpliﬁed pre-\ninitialization methods do not rely on pre-calculated state fre-\nquencies. Instead, we directly expand the graph with breadth-\nﬁrst search or through a data-driven approach where a small\nnumbers of utterances are processed by the decoder ofﬂine. We\nfound that both methods are effective, but the data-driven ap-\nproach outperforms the breadth ﬁrst search algorithm. Both\nmethods can be combined to achieve the best performance.\nThrough a series of experiments on a speech recognition task\nfor the calling domain, we found that pre-initialization on the\npublic graph speeds up the decoding time by a factor of three.\nFuthermore, sharing the private graph further reduces decoding\ntime and results in factor of ﬁve improvement in efﬁciency.\n2. Architecture and Algorithm\nThe general composition algorithm is well-explained in [6, 7]\nand a pre-composition algorithm with a non-class LM is de-\nscribed in [4]. Here we will only present our new algorithm\nfocusing on how to pre-compose the graph while avoiding non-\nterminal states. In this work, we use the same mathematical\nnotation as [1].\narXiv:1910.10670v1  [cs.CL]  23 Oct 2019\n2.1. Two-layer cached FST during decoding\nA WFST can be written as\nT = (A,B,Q,I,F,E,λ,ρ ) (3)\nwhere A, Bare ﬁnite label sets for input and output. Qis the\nﬁnite state set. I ⊆Qis the initial state set, F ⊆Qis ﬁnal\nstate set. E ⊆Q×(A∪{ ϵ}) ×(B∪{ ϵ}) ×K ×Q is a\nset of transitional mapping between states in Qwith weighted\ninput/output label pair, where K is a semiring (K,⊕,⊗,0,1).\nThe composition of two weighted FSTs is deﬁned as\n(T1 ◦T2)(x,y) =\n⨁\nz∈B\nT1(x,z) ⊗T2(z,y) (4)\nwhere B = B1 ∩A2 is the intersection of output label set\nof T1 and input label set of T2. For a,b,c ̸= ϵ, two tran-\nsitions (q1,a,b,w 1,q′\n1) in T1 and (q2,b,c,w 2,q′\n2), the com-\nposed transition will be ((q1,q2),a,c,w 1\n⨂w2,(q′\n1,q′\n2)).\nFor two FSTs T1, T2 over semiring K,\nT1 = HCL (5)\nT2 = Replace(Gc,Gp),p ∈C (6)\nis the class language model transducer obtained by replacing\nthe class labels in generic root FST Gc with class FSTs Gp\nfor different classes, where Cdenotes the set of all supported\nclasses.\nThe calculation for composition is very slow for LM with\nlarge vocabulary size. Naive on-the-ﬂy composition is very\ntime-consuming. In [4], the authors proposed a pre-initialized\ncomposition algorithm, which does a partial composition based\non the state frequency. This one-time cost calculation can do\nsome composition in advance. During decoding search, the FST\nwill skip the composition of pre-initialized states. However, ex-\ntending this algorithm to class LMs is non-trivial in practice.\nFor a class LM, the non-terminal states cannot be composed\nduring pre-initialization since we need a pre-initialization that\nis applicable to all users, which means we need to apply some\nrestrictions to prevent composition of the personalized part.\nWe deﬁne TP as a partial composed FST structure for T =\nT1 ◦T2, where P ⊆Qis the set of pre-composed states. In real\ntime decoding, the on-the-ﬂy composition will be performed on\ntop of the pre-initialized TP , which is similar to previous work\n[4]. In a production environment, multiple threads will share\nthe same pre-composed FST TP structure, while each thread\nwill own a private FST structure.\nT = TP ∪TD (7)\nwhere TD is the dynamic cache built on top of TP . TD may\nneed to copy some states from TP if we need to update infor-\nmation for those states in TP .\nIn order to support this mechanism, we use a two-layered\ncached FST for decoding. The ﬁrst layer is public cache which\nrepresents TP . It is a static cache created by pre-initialization.\nThe second layer is the private cache, which is owned by a par-\nticular user and constructed on-the-ﬂy. Figure 1 shows the ar-\nchitecture of our two-layer FST. The solid box denotes the static\ngraph and the dashed ones show the dynamic graph. Personal-\nized states will appear only in TD.\nThe static public cache stores the most frequent states,\nwhich greatly reduces the run time factor (RTF) of online de-\ncoding. Since TD has a smaller size than a fully dynamic graph,\nHCL GC GP\nTP TD\nFigure 1: Architecture of two layer cached FST.TP is the static\npublic cache built in pre-initialization.TD is the private cache\nfor dynamic composition in decoding time. The lifetime ofTD\nvaries based on the length of dialog section.\nthe marginal memory efﬁciency for multi-threaded service will\nbe better.\nFurthermore, the private cache will not be freed after decod-\ning a single utterance. The lifetime of a private cache actually\ncan last for the entire dialog section for a speciﬁc user. The\nprivate cache keeps updating during the dialog session, making\nprocessing the subsequent utterances faster as more states are\ncomposed and stored in TD. With this accumulated dynamic\ncache, a longer dialog can expect a better RTF in theory. In\ngeneral, the static public cache serves all threads, while the pri-\nvate cache boosts the performance within a dialog session. The\nprivate cache will be freed at the end of the dialog.\n2.2. Pre-composition algorithm for class language models\nBased on the algorithm described in [4], we allow the states\n(q1,q2) such that q2 = (qc,qp),qc ∈Qc,qp = 0 to be pre-\ncomposed, where qc and qp denote states in Gc and Gp, respec-\ntively. States in Gc with a class label transition will be ignored\nduring pre-composition.\nP = ∪{(q1,(qc,0))},∀e∈E(qc) :o(e) /∈C (8)\nBy applying this restriction, the states in the pre-composed\nrecognition FST TP will not contain any personalized states,\nand thus, can be shared by all users and threads.\nNote that care must taken to account for the special case\nwhen the initial states could have transitions with a class label.\nIn this case, the entire graph is blocked (Figure 2(a)), so we need\nto add an extra ϵtransition before class label in the root FST,\nwhich will guarantee all the initial states are composed (Fig-\nure 2(b)). In the pre-composition stage, we don’t need the actual\nclass FSTs for each class, so Gp is simply a placeholder FST\nwhich only contains a placeholder word ⟨temp⟩. This means\nall the transitions following the placeholder transition may be\nblocked if there is no other path that skips over the placeholder\ntransition. In practice, for a large LM graph with a large vo-\ncabulary, the connectivity is usually very high, once the initial\nstates are guaranteed to be composed.\nThis pre-composition algorithm can be applied with looka-\nhead ﬁlter [8]. We implemented this algorithm using OpenFst\nframework [5], which supports such a lookahead ﬁlter in both\nthe pre-composition and decoding stages. In our implementa-\ntion, the decoding FST has a two-layered cache and state table.\nThe state table is necessary since the add-on composition during\ndecoding must be based on the same state map.\n2.3. Pre-composition methods\nIn general, we can pre-compose all the states of the decod-\ning FST that are applied to all users, i.e. those unrelated\n0\n1call:call\n2@contact:@contact\n@contact:@contact\n(a)\n0\n1call:call\n3\n<eps>:<eps> 2\n@contact:@contact\n@contact:@contact\n(b)\nFigure 2: Class language model FST with contact tags. (a) Con-\nventional LM with @contact. (b) LM with additional<eps>\nbetween start state 0 and @contact. This guarantees the start\nstate is pre-composed.\nto the personalized language model. However, this full set\npre-composition could be very slow and memory consuming.\nIn fact, most of the states are rarely composed during real\ndata trafﬁc, and therefore, performing partial pre-composition\nis sufﬁcient. Here we propose two simple methods for pre-\ncomposition.\n2.3.1. Distance based method\nNaive breath-ﬁrst-search (BFS) is the most obvious way to per-\nform pre-composition. We iterate over all states within a spe-\nciﬁc distance from the start state of decoding FST. It generalizes\nto a full set pre-composition when the search depth is large.\n2.3.2. Data-driven warm-up\nOur goal is to pre-compose the most frequently encountered\nstates. However, if some frequent states are far from the start\nstate, they may not be identiﬁed by naive BFS. In this case, it\nis very time and memory consuming to increase the depth of\nthe BFS. Moreover, if we simply use a ofﬂine corpus of utter-\nances to analyze the frequency of all states, some highly fre-\nquent states could be blocked by less frequent states. Thus, the\neasiest way is to do pre-composition using real utterances.\nThe decoding FST can be expanded while decoding utter-\nances. We utilize a special decoder in the warm-up stage. This\nwarm-up decoder will apply the same restriction discussed in\nthe previous section. We use an empty contact FST in the warm-\nup stage to avoid expanding any personalization-related states.\nThis data driven pre-composition will expand most frequent\nstates which are visited during warm-up decoding, especially\nfor some speciﬁc patterns.\n2.4. Out-Of-Vocabulary recognition\nHandling out-of-vocabulary (OOV) words in speech recogni-\ntion is very important especially for contact name recognition.\nWe replace the normal class (contact) FST with a mono-phone\nFST by adding monophone words in the lexicon [3, 9, 10]. By\nusing s monophone FST, we avoid the necessity of adding new\nwords into lexicon on-the-ﬂy, which signiﬁcantly simpliﬁes the\nsystem. We use silence phone ”SIL” to represent the word\nboundary. These monophone words will not be applied with\nsilence phone in lexicon since they are not real words.\nIn Figure 3, the contact name is represented as monophone\nwords using IPA phone set. SIL is added after each name in\ncontact FST. Names with the same pronunciation also need to\nbe handled using disambiguation symbols. In practice, because\nof accent and pronunciation variability, we have found that mul-\ntiple pronunciations of OOV names are required in the person-\nalized class FST.\n0\n1d͡ʒ:Jun\n5\nl:Liu\n2uː:<eps>\n6iː:<eps>\n3\nn:<eps>\n4SIL:<eps>\nuː:<eps>\nFigure 3: Monophone contact FST. The monophone will be\ntreated as word in the lexicon without a word boundary, so there\nis an additional silence phone after each name.\n3. Experiments\nWe performed a series of experiments on different data sets\nin order to evaluate the impact on real-time factor (RTF) and\nword error rate (WER) of the proposed approach. In theory, the\npre-composition algorithm will not change the WER, since the\nsearch algorithm does not change.\n3.1. Experimental Setup\nIn these experiments, speech recognition was performed using\na hybrid LSTM-HMM framework. The acoustic model is an\nLSTM that consumes 40-dimensional log ﬁlterbank coefﬁcients\nas the input and generates the posterior probabilities of 8000\ntied context-dependent states as the output. The LM is a pruned\n4-gram model trained using various semantic patterns that in-\nclude a class label as well as a general purpose text corpus. The\nLM contains @contact as an entity word, which will be re-\nplaced by the personalized contact FST. After pruning, the LM\nhas 26 million n-grams.\nThe personalized class FST (contact FST) only contains\nmonophone words. Determinization and minimization are ap-\nplied to the contact FST with disambiguation symbols. The\ndisambiguation symbols are removed after graph optimization.\nThe decoding experiments are performed on a server with 110\nGB memory and 24 processors.\nExperiments are performed on two data sets. The ﬁrst con-\ntains 7,500 utterances from the calling domain from Facebook\nemployees. This includes commands like “Please call Jun Liu\nnow”. The second consists of approximately 10,000 utterances\nfrom other common domains, such as weather, time, and mu-\nsic. Note that we include the contact FST for both calling and\nnon-calling utterances, as we do not assume knowledge of the\nuser’s intent a priori. Each user has a contact FST containing\n500 contacts on average. We keep up to ﬁve pronunciations for\neach name, generated by a grapheme-to-phoneme model.\nWe experiment with both the naive BFS and the proposed\ndata-driven pre-composition methods. For the data-driven ap-\nproach, we randomly picked 500 utterances from the evaluation\ndata set as warm up utterances. We use an empty contact FST to\nbe replaced into the root LM to avoid personalized states during\nwarm-up decoding. In order to evaluate the beneﬁt of the pro-\nposed private cache to store the personalized language model,\nwe group multiple utterances from a user into virtual dialog ses-\nsions of one, two, or ﬁve turns.\nTable 1: WER and RTF results for different data set and different pre-composition methods.\nDataset Graph Type # of Pre-composed States RTF(p50) RTF(p95) WER\nfully dynamic 0 0.887 1.44 5.48\ncalling 5 steps BFS 1,035,374 0.451 0.793 5.48\ndata driven warmup 19,356,186 0.286 0.484 5.48\nfully dynamic 0 0.851 1.47 7.39\nnon-calling 5 steps BFS 1,035,374 0.402 0.736 7.39\ndata driven warmup 19,356,186 0.241 0.488 7.39\n3.2. Results\nTable 2: RTF results for decoding in session. Decoder will hold\nthe private cache for entire dialog session.\nDataset Session Length RTF(p50) RTF(p95)\n1 0.286 0.484\ncalling 2 0.220 0.390\n5 0.182 0.346\n1 0.241 0.488\nnon-calling 2 0.203 0.408\n5 0.173 0.372\nTable 1 shows the WER and RTF for two corpora with\ndifferent pre-composition methods with ten concurrent speech\nrecognition client requests. The private cache is freed after\ndecoding each utterance. RTF is calculated by tdecode/twav,\nwhere tdecode is the decoding time and twav is the audio du-\nration. We use 50th and 95th percentile values for the RTF\ncomparison. As expected, the WER remains unchanged for the\nsame data set. With pre-composition, the RTF for both calling\nand non-calling is reduced by a factor of three.\nTable 2 shows the additional RTF improvement that can be\nobtained during multi-turn dialogs from the proposed private\ncache. When the dialog session is only a single turn, the RTF re-\nmains unchanged. However, for multi-turn sessions, additional\nRTF reductions are obtained for both the calling and non-calling\ncorpora. The decoding time is reduced by a factor of ﬁve com-\npared to a fully dynamic graph for dialog sessions of ﬁve turns.\nFigure 4 shows the RTF and memory usage for teh different\npre-composition approaches. The upper graph shows the RTF\nfor different steps of naive BFS using the calling data set. The\nﬁgure shows that additional BFS steps improves RTF for both\n50 and 95 percentiles. However, no improvement is observed\nbeyond ﬁve steps, because the most frequent states close to the\nstart state have already been pre-composed. The additional BFS\nsteps only result in more memory usage. With the data-driven\nwarmup, the RTF shows additional improvement. Furthermore,\nthe difference in the p50 and p95 RTF values becomes much\nsmaller than in the BFS approach.\nThe lower graph of Figure 4 shows the memory usage as\na function of the number of concurrent requests. Though the\npre-composed graph may use more memory when we have\nonly a small number of threads, the marginal memory cost for\nadditional requests for a fully dynamic graph is roughly 1.5\ntimes larger than for the pre-composed graph. The data-driven\nmethod has the best marginal memory efﬁciency for a large\nnumber of concurrent requests.\nRTF p50\nRTF p95\n# of pre-composed states\n3 4 5 6 7 8 9 102 1\nfully dynamic\n5 steps BFS\nwarmup\n2\n3\n4\n5\n6\n7\n8Memory Usage (GB)\nNumber of Concurrent Requests\nBFS Steps\n(a)\n(b)\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n1e6\n1e5\n1e4\n1e3\n1e2\n10\n0\n1e7\n1e8\nRTF\nfully dynamic 3 5 7 9 warmup\nNumber of Pre-composed States\nFigure 4: RTF and memory usage comparison. Upper: RTF\nbetween fully dynamic graph, different steps of BFS and data\ndriven pre-composition. Lower: Memory usage for different\ngraphs. A pre-composed graph has a better marginal memory\ncost than a fully dynamic graph.\n4. Conclusions\nIn this work, we propose new methods for improving the efﬁ-\nciency of dynamic WFST decoding with personalized language\nmodels. Experimental results show that using a pre-composed\ngraph can reduce the RTF by a factor of three compared with a\nfully dynamic graph. Moreover, in multi-utterance dialog ses-\nsions, the RTF can be reduced by a factor of 5 using the pro-\nposed private cache without harming WER. Though a fully dy-\nnamic graph uses less memory for the graph, the pre-composed\ngraph has a better marginal memory cost, which is more mem-\nory efﬁcient in large-scale production services that need to sup-\nport a large number of concurrent requests.\nOur results also show that increasing the steps of naive BFS\nwill not help the RTF, since it may compose infrequently en-\ncountered states, resulting in unnecessary memory usage. Us-\ning the proposed data-driven warm-up performs better in both\nmarginal memory efﬁciency and RTF than naive BFS. Both pre-\ncomposition methods can also be combined.\n5. Acknoledgements\nWe would like to thank Mike Seltzer, Christian Fuegen, Julian\nChan, and Dan Povey for useful discussions about the work.\n6. References\n[1] M. Mohri, F. Pereira, and M. Riley, “Speech recognition with\nweighted ﬁnite-state transducers,” Handbook of Speech Pro-\ncessing, Springer, pp. 559–582, 2008.\n[2] ——, “Weighted ﬁnite-state transducersin speech recognition,”\nComputer Speech & Language, vol. 20, no. 1, pp. 69–88, 2002.\n[3] P. Aleksic, C. Allauzen, D. Elson, A. Kracun, D. M. Casado, and\nP. J. Moreno, “Improved recognition of contact names in voice\ncommands,” inICASSP, 2015, pp. 5172–5175.\n[4] C. Allauzen and M. Riley, “Pre-initialized composition for large-\nvocabulary speech recognition,” in INTERSPEECH, 2013, pp.\n666–670.\n[5] C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri,\n“OpenFst: A general and efﬁcient weighted ﬁnite-state trans-\nducer library,” inCIAA, 2007, pp. 11–23.\n[6] C.Allauzen, M.Riley, and J.Schalkwyk, “A generalized composi-\ntion algorithm for weighted ﬁnite-state transducers,” in INTER-\nSPEECH, 2009, pp. 1203–1206.\n[7] T. Oonishi, P. Dixon, K. Iwano, and S. Furui, “Implementation\nand evaluation of fast on-the-ﬂy WFST composition algorithms,”\nin INTERSPEECH, 2008, pp. 2110–2113.\n[8] J. R. Novak, N. Minematsu, and K. Hirose, “Dynamic gram-\nmars with lookahead composition for WFST-based speech recog-\nnition,” inICASSP, 2012.\n[9] P. R. Dixon, C. Hori, and H. Kashioka, “A specialized WFST\napproach for class models and dynamic vocabulary,” in INTER-\nSPEECH, 2012.\n[10] I. McGraw, R. Prabhavalkar, R. Alvarez, M. G. Arenas, K. Rao,\nD. Rybach, O. Alsharif, H. Sak, A. Gruenstein, F. Beaufays, and\nC. Parada, “Personalized speech recognition on mobile devices,”\nin ICASSP, 2016."
}