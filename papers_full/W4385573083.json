{
  "title": "COPEN: Probing Conceptual Knowledge in Pre-trained Language Models",
  "url": "https://openalex.org/W4385573083",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2009379665",
      "name": "Hao Peng",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2102849134",
      "name": "Xiao-Zhi Wang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2998424002",
      "name": "Shengding Hu",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2225476127",
      "name": "Hai-long Jin",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1982465838",
      "name": "Lei Hou",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2129156004",
      "name": "Juanzi Li",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2998696444",
    "https://openalex.org/W4280538731",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2037561321",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2890216969",
    "https://openalex.org/W3173927728",
    "https://openalex.org/W1973644634",
    "https://openalex.org/W2038880450",
    "https://openalex.org/W1487060125",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W102708294",
    "https://openalex.org/W2196674927",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4254751698",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3101717721",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3022006665",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W3105093672",
    "https://openalex.org/W2008491660",
    "https://openalex.org/W2138605095",
    "https://openalex.org/W4205978409",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W2964922231",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2159938093",
    "https://openalex.org/W2022166150",
    "https://openalex.org/W4301785137",
    "https://openalex.org/W1966115499",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3176807265",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W4285799007",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4297801719"
  ],
  "abstract": "Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual knowledge Probing bENchmark. Extensive experiments on different sizes and types of PLMs show that existing PLMs systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are publicly released at https://github.com/THU-KEG/COPEN.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5015–5035\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nCOPEN: Probing Conceptual Knowledge in Pre-trained Language Models\nHao Peng1,2∗, Xiaozhi Wang1,2∗, Shengding Hu1,2, Hailong Jin1,2, Lei Hou1,2†,\nJuanzi Li1,2, Zhiyuan Liu1,2, Qun Liu3\n1Department of Computer Science and Technology, BNRist;\n2KIRC, Institute for Artificial Intelligence,\nTsinghua University, Beijing, 100084, China\n3Huawei Noah’s Ark Lab\n{peng-h21, wangxz20}@mails.tsinghua.edu.cn\nAbstract\nConceptual knowledge is fundamental to hu-\nman cognition and knowledge bases. However,\nexisting knowledge probing works only focus\non evaluating factual knowledge of pre-trained\nlanguage models (PLMs) and ignore concep-\ntual knowledge. Since conceptual knowledge\noften appears as implicit commonsense behind\ntexts, designing probes for conceptual knowl-\nedge is hard. Inspired by knowledge represen-\ntation schemata, we comprehensively evaluate\nconceptual knowledge of PLMs by designing\nthree tasks to probe whether PLMs organize\nentities by conceptual similarities, learn con-\nceptual properties, and conceptualize entities\nin contexts, respectively. For the tasks, we col-\nlect and annotate 24k data instances covering\n393 concepts, which is COPEN, a COncep-\ntual knowledge Probing bENchmark. Exten-\nsive experiments on different sizes and types\nof PLMs show that existing PLMs systemat-\nically lack conceptual knowledge and suffer\nfrom various spurious correlations. We be-\nlieve this is a critical bottleneck for realiz-\ning human-like cognition in PLMs. COPEN\nand our codes are publicly released at https:\n//github.com/THU-KEG/COPEN.\n1 Introduction\nPre-trained language models (PLMs) have achieved\nsuperior performance on most NLP tasks requir-\ning substantial world knowledge (Qiu et al., 2020;\nHan et al., 2021). It is interesting and meaning-\nful to probe the extent and scope of world knowl-\nedge within PLMs. Existing knowledge probing\nworks have evaluated PLMs’ knowledge about en-\ntities (Broscheit, 2019; Tenney et al., 2019a) and\ntheir relations (Petroni et al., 2019; Jiang et al.,\n2020; Roberts et al., 2020), i.e., factual knowledge,\nbut ignore conceptual knowledge.\n∗ Equal contribution\n† Corresponding author: L.Hou\nAnimals \ncan move.\nUK\nGrumpy Cat\n USA\nMammal\nAnimal\nBird\n Country\nDolly\nnationality\nnationality\nRelation\nInstance of\nSubclass of\nMost \nmammals \nhave navels.\nBirds have \nfeathers.\n…\n…\n……\nEntity Graph\nConcept Taxonomy Property\n…\nFigure 1: An example knowledge graph. Entities are or-\nganized by concepts through the Instance of relation\nand concepts are organized into a taxonomy through the\nSubclass of relation. Each concept has certain prop-\nerties. Existing work only probes factual knowledge\nin entity graphs, ignoring conceptual knowledge in the\nconcept taxonomy and Instance of relation.\nConceptual knowledge, especially the abstrac-\ntion ability, is fundamental to all kinds of hu-\nman cognition (Carey, 1991; Collins and Olson,\n2014) including language processing (Waxman and\nMarkow, 1995; Wellsby and Pexman, 2014). Just\nas the quote of psychologist Gregory Murphy, con-\ncepts are the glue that holds our mental world\ntogether (Murphy, 2004). Moreover, knowledge\nbases (Suchanek et al., 2007; Auer et al., 2007;\nVrandeˇci´c, 2012) organize massive entities via con-\ncept taxonomies as illustrated in Figure 1, which\nenable broad applications (Lv et al., 2018; Zhou\net al., 2021). Therefore, probing whether PLMs\nhave human-like conceptual knowledge is neces-\nsary in knowledge probing.\nInspired by the conceptual schema in knowledge\nrepresentations (Sowa, 1976; Decker et al., 2000;\nMcGuinness et al., 2004; Antoniou and Van Harme-\nlen, 2004), we comprehensively evaluate the con-\nceptual knowledge of PLMs by asking three ques-\ntions: Do PLMs organize entities by conceptual\n5015\nsimilarities? Do PLMs know the properties of con-\ncepts? Can PLMs correctly conceptualize entities\nin contexts? In this paper, we design three probing\ntasks for these questions: (1) The conceptual simi-\nlarity judgment (CSJ) task studies whether PLMs\norganize entities by conceptual similarities, which\nis the basis of understanding concepts. Given a\nquery entity, CSJ requires PLMs to choose the most\nconceptually similar entity among candidate enti-\nties. For example, in Figure 1, given Dolly as the\nquery entity, although UK has a direct relation and\nmore co-occurrences with it, PLMs should choose\nGrumpy Cat. (2) The conceptual property judg-\nment (CPJ) task probes whether PLMs have the\nknowledge of conceptual properties, which are the\ngeneric abstractions of factual knowledge. Given a\nstatement about a specific property, such as “have\nfeathers”, CPJ requires PLMs to judge whether it\nis true for a specific concept and also a concept\nchain, which evaluates whether PLMs understand\nthe property transitivity among a chain of hierar-\nchical concepts. (3) The conceptualization in con-\ntexts (CiC) task evaluates the abilities of PLMs\nto correctly conceptualize entities within contexts.\nGiven an entity mentioned in a specific context,\nPLMs are required to choose the most appropri-\nate concept in a concept taxonomy according to\nits context. CiC requires not only disambiguating\nentity mentions, but also distinguishing superordi-\nnate and subordinate concepts. For instance, given\nthe context “Dolly is running on the grassland”,\nPLMs should conceptualize Dolly as an Animal\nsince there is no enough evidence for Mammal.\nBased on the above considerations, we con-\nstruct a conceptual knowledge probing benchmark,\nCOPEN, which contains a concept taxonomy with\n446 concepts and high-quality data of 24K in-\nstances for the three probing tasks. The concept\ntaxonomy is curated by experts based on DBpe-\ndia (Auer et al., 2007) and Wikidata (Vrande ˇci´c\nand Krötzsch, 2014) to form a well-defined hierar-\nchy and cover broad entities. The data instances\nfor three tasks are collected by aligning entities\nin Wikidata and sentences in GenericsKB (Bhak-\nthavatsalam et al., 2020), Wikipedia1, and Simple\nWikipedia2 into the concept taxonomy and then\nmanually annotated by crowd-sourcing annotators.\nWe conduct extensive experiments on COPEN\nto evaluate various widely-used language mod-\n1https://en.wikipedia.org/\n2https://simple.wikipedia.org/\nels (LMs), which include three types: masked\nLMs (Devlin et al., 2019; Liu et al., 2019b), autore-\ngressive LMs (Radford et al., 2019; Black et al.,\n2021), and sequence-to-sequence LMs (Lewis\net al., 2020; Raffel et al., 2020). We conduct the\nexperiments in three settings: (1) zero-shot prob-\ning, which reformulates the probing tasks into pre-\ntraining objectives and lets PLMs score answers\nwithout any training (Petroni et al., 2019); (2) linear\nprobing, which only tunes additional linear classifi-\ncation heads and uses them to handle probing tasks\nwith the frozen representations produced by PLMs;\n(3) fine-tuning, which tunes all the PLM parame-\nters. Experiments show that existing PLMs achieve\nnon-trivial performance but still significantly un-\nderperform ordinary persons on all three probing\ntasks. Further analyses show that PLMs suffer from\nspurious correlations like word co-occurrences and\nout-of-context predictions, and increasing model\nscale brings marginal improvements.\nTo summarize, our contributions are three-fold:\n(1) We propose to probe PLMs for conceptual\nknowledge, which has long been ignored, and de-\nsign three probing tasks inspired by the knowledge\nrepresentation works. (2) We construct COPEN,\na probing benchmark containing high-quality con-\ncept taxonomy and probes. (3) We empirically\nshow that existing PLMs systematically lack con-\nceptual knowledge and analyze the reasons. We\nhope our benchmark and findings could facili-\ntate further research on concept-aware PLMs and\nhuman-like language understandings.\n2 COPEN Benchmark\nIn this session, we introduce our COPEN bench-\nmark, including the construction of the concept\ntaxonomy (§ 2.1) and the datasets for three prob-\ning tasks (§§ 2.2 to 2.4). More construction and\nannotation details are shown in appendix D.\n2.1 COPEN Concept Taxonomy\nDesigning the three probing tasks takes inspira-\ntion from concept schemata in knowledge rep-\nresentations (Decker et al., 2000; McGuinness\net al., 2004), which are widely used in knowledge\ngraphs (Suchanek et al., 2007; Auer et al., 2007;\nVrandeˇci´c, 2012). In general, it uses the instance\nof relation to link the entities (specific instances)\ninto abstract concepts, and uses the subclass of\nrelation to organize the concepts into a taxonomy.\nEach concept has certain properties describing it as\n5016\nInter Milan is conceptually similar with  \nChoose the concept that best ﬁts the context for Dolly according to the context: Dolly is running on the grassland. Dolly is a(n) \n(a)\n(c) \nTemplate \nMammals raise their young on milk. The statement is   \nTemplate\nanimal\nmammal\nSeries A\nPohang Steelers\nMilan\nMilan Fashion Week (b)\nAnimals\nHorses\ntrue | false \ntrue | false \ntrue | false \nAnswerConcept/Concept Chain\nhorse\nQuery Entity\nTemplate Entity Template\nCandidate Entity\nYellow: : Subclass ofConceptPurple:Entity\nFigure 2: Examples for casting the data of three probing tasks into natural language prompts in zero-shot probing.\nThe names of entities or concepts are the text looked up in Wikidata using their IDs. In Figure (b), texts in bold\n(true or false) denote answers. In Figure (b) and (c), the concept chain is Horse –> Mammal –> Animal. In Figure\n(c), for entities with multiple concept chains, each concept will be scored independently by PLMs, i.e., the PLMs\nmake concept-level predictions only. There is no dedicated chain selection procedure.\nthe example shown in Figure 1.\nTo support probing dataset construction, we man-\nually curate a concept taxonomy based on DBpe-\ndia (Auer et al., 2007) and Wikidata (Vrande ˇci´c\nand Krötzsch, 2014) in 3 steps: (1) Obtain a basic\ntaxonomy from DBpedia. We extract the frequent\nconcepts of DBpedia, which are the concepts with\nat least 5 instances, and keep the subclass of\nrelations between them. (2) Align DBpedia and\nWikidata. For each DBpedia concept, we man-\nually find its equivalent Wikidata item and then\nuse the subclass of (P279) relations in Wiki-\ndata to expand the concept taxonomy and use the\ninstance of (P31) relations to link massive Wiki-\ndata entities into the concepts. (3) Simplify the\ntaxonomy. We further remove some unusual con-\ncepts to simplify the taxonomy by the guidance\nfrom Schema.org (Guha et al., 2016). For example,\nPerson is a sub-concept of Animal, Eukaryote,\nand Species in DBpedia, which is reasonable but\ninconvenient for real-world applications. Follow-\ning Schema.org, we set Person as a top-level con-\ncept in the taxonomy. Finally, we achieve a tree-\nstructure concise concept taxonomy, which con-\ntains 446 concepts covering 45 million Wikidata\nentities. There are 23 top-level concepts, and we\nuse 11 of them and their sub-concepts for construct-\ning training and development datasets as well as\nthe other concepts for the testing datasets.\n2.2 Conceptual Similarity Judgment\nThe conceptual similarity judgment (CSJ) task is\na multiple-choice classification task, which probes\nwhether PLMs organize entities by conceptual sim-\nilarities, i.e., whether PLMs learn the instance\nof relation. Given a query entity, CSJ requires\nPLMs to choose the most conceptually similar en-\nTrain Dev Test\nCSJ #Instance 4,462 1 ,116 3 ,909\n#Concept 84 84 90\nCPJ #Instance 3,274 823 4 ,758\n#Concept 215 195 178\nCiC #Instance 2,888 722 2 ,368\n#Concept 193 184 155\nTable 1: COPEN data statistics for three probing tasks.\ntity (instance of the same superordinate concept)\namong some candidates. As in Figure 2 (a), PLMs\nshould choose Pohang Steelersfor Inter Milan\nsince they are both football clubs, although Milan\nand Inter Milan co-occur more frequently. The\nconceptual similarity here is similar to the cohy-\nponym relation in lexical semantics (Cruse, 1986),\nwhich has been shown to be distinct from but eas-\nily influenced by spurious co-occurrence associa-\ntions (Hill et al., 2015). Thus we need to control the\ninfluence of co-occurrences to get faithful results.\nData Collection The data for CSJ is collected\nin two steps: (1) Automatic collection. We first\nsample 174 concepts that are not subordinates to\neach other. Then we retrieve 50 Wikidata entities\nmost frequently showing up in the Wikipedia cor-\npus for each concept, and then build data instances\nby combining them. Each instance consists of a\nquery entity, an answer entity of the same concept,\nand 20 distractor entities, among which 5 are hard\ndistractors of concepts sharing superordinates with\nthe concept of query entity. To check the data qual-\nity, we sample 200 instances and find little noise.\n(2) Co-occurrence-based filtering. To reduce the\ninfluence of co-occurrences, we need to filter out\nthe instances that can be easily solved with co-\n5017\noccurrences. Lastra-Díaz et al. (2019) show that\nGlove word embedding (Pennington et al., 2014)\ncontains rich word co-occurrence information but\nlimited cohyponym knowledge. Hence we use it\nto filter out instances with higher word similarity\nbetween the query and answer entity than distrac-\ntor entities. We finally get 9,487 instances, each\nincluding a query entity and 21 candidate entities.\nThe statistics of data subsets are shown in Table 1.\n2.3 Conceptual Property Judgment\nThe conceptual property judgment (CPJ) task is a\nbinary sentence classification task, which probes\nwhether PLMs know the properties of concepts.\nGiven a statement describing a certain conceptual\nproperty, PLMs are required to judge whether it is\ntrue. For example in Figure 2 (b), PLMs should\npredict “true” for the statement instance Mammals\nraise their young on milk.\nBesides evaluating CPJ at instance level, which\nreflects the PLMs’ knowledge about properties for\ndifferent individual concepts, we also set a chain-\nlevel evaluation, in which a PLM correctly judges\na property if and only if it correctly judges the\nproperty for every concept in a concept chain. As\nthe example in Figure 2 (b), a concept chain is a\nchain of concepts connected with the subclass\nof relation in order. The chain-level evaluation\nevaluates whether PLMs understand the transitivity\nof conceptual properties. It means that a property\nholds for a concept also holds for its subordinate\nconcepts, but may not hold for its superordinate\nconcepts like the case in Figure 2 (b).\nData Collection The data for CPJ is collected\nin two steps: (1) Automatic collection. For each\nconcept in our taxonomy, we align it with the\nstatements of GenericsKB (Bhakthavatsalam et al.,\n2020), a high-quality knowledge base for naturally\noccurring generic statements, by lexical matching\nso as to get positive instances. Then we replace the\nconcept mention with other concept names to ob-\ntain negative instances. (2) Human annotation. To\nensure data quality, we invite annotators to check\nwhether the instances are correctly labeled, gram-\nmatically correct, and describing concept proper-\nties. All annotators are well-trained and pass a qual-\nification before annotation. We finally get 8,855\ninstances for CPJ and the statistics of data subsets\nare shown in Table 1. Additionally, the final test\ndata includes 102 concept chains and correspond-\ning properties used for chain-level evaluation.\n2.4 Conceptualization in Contexts\nThe conceptualization in contexts (CiC) task is a\nmultiple-choice classification task, which probes\nwhether PLMs can correctly conceptualize entities\nwithin contexts. Given an entity mentioned in a\nspecific sentence, PLMs are required to choose the\nmost appropriate concept among a concept chain,\nwhich is a chain of concepts connected with the\nsubclass ofrelation in order. This requires PLMs\nto understand the subclass of relation and cap-\nture the subtle differences of different-level con-\ncepts in a hierarchy. For example in Figure 2\n(c), given the sentence Dolly is running on the\ngrassland. and a concept chain Horse –> Mammal\n–> Animal, PLMs shall choose Animal for Dolly\nsince the context do not support more fine-grained\nconcepts. Sometimes the entity is of multiple con-\ncept chains, for example, Jimmy Carter is both\na Writer and a Politician, which additionally\nrequires PLMs to disambiguate.\nData Collection The data for CiC is collected\nin two steps: (1) Sentence collection. For each\nconcept, we first retrieve 10 Wikidata entities most\nfrequently showing up in the Wikipedia corpus.\nAmong the retrieved entities, we only keep the enti-\nties linked with the concept chains containing more\nthan one concepts and collect 5 sentences for each\nof them from Wikipedia and SimpleWiki, which\nprovides various contexts for conceptualization. A\nsentence, together with an entity mentioned in the\nsentence and concept chains of the entity, consti-\ntutes an instance. (2) Human annotation. We then\norganize crowd-sourcing annotation to obtain the\nlabels. All annotators are well-trained and quali-\nfied. We finally get 5,978 instances for CiC and the\nstatistics of data subsets are shown in Table 1.\n3 Evaluation Setup\nWe introduce the various widely-used PLMs inves-\ntigated in our experiments (§ 3.1) and the three\nadopted probing methods (§ 3.2).\n3.1 Investigated PLMs\nWe investigate three mainstream types of PLMs: (1)\nMasked LM, including BERT (Devlin et al., 2019),\nwhich is pre-trained with the bidirectional masked\nlanguage modeling and next sentence prediction\nobjectives, and RoBERTa (Liu et al., 2019b), which\nis a robustly optimized version of BERT. (2) Au-\ntoregressive LM, including GPT-2 (Radford et al.,\n5018\nModel\nCSJ CPJ CiC\nInstance-Level Chain-Level\nZP LP FT ZP LP FT ZP LP FT ZP LP FT\nRandom 4.8 4 .8 4 .8 50.0 50 .0 50 .0 7.2 7 .2 7 .2 27.7 27 .7 27 .7\nBERTBASE 20.3 16 .10.21 27.30.86 49.4 61 .60.28 68.10.98 22.5 24 .21.22 23.21.22 37.6 34 .30.59 49.50.60\nRoBERTaBASE 15.5 12 .00.21 22.30.51 49.2 61 .90.13 72.00.54 21.6 13 .11.67 18.31.22 31.4 30 .01.98 52.61.02\nGPT-2BASE 7.9 4 .30.24 20.10.23 51.5 64 .81.14 70.40.72 14.7 14 .40.92 20.32.01 32.3 34 .52.08 54.20.12\nGPT-Neo125M 7.9 11 .00.20 18.30.42 52.2 62 .20.21 68.20.62 22.5 15 .02.01 19.02.81 32.6 39 .60.93 47.40.25\nBARTBASE 14.4 8 .40.10 21.00.50 48.7 58 .50.27 68.20.86 20.6 10 .51.22 16.70.80 33.6 43.71.19 51.31.56\nT5BASE 15.2 4 .90.21 27.90.60 55.9 66 .90.25 72.50.28 22.5 18 .00.46 18.03.95 42.3 24.70.66 53.20.18\nHuman 79.5 79 .5 79 .5 91.4 91 .4 91 .4 91.2 91 .2 91 .2 85.6 85 .6 85 .6\nTable 2: Accuracies (%) of various PLMs on the three tasks using different probing methods. ZP: Zero-shot probing.\nLP: Linear probing. FT: Fine-tuning. LP and FT results are Meanstandard deviation over three random trials. Human\nperformance is obtained by ordinary people trained with a few instances.\n2019), which is pre-trained with the unidirectional\nleft-to-right language modeling objective, and GPT-\nNeo (Black et al., 2021), which adopts the same ob-\njective but improves some implementation details.\n(3) Sequence-to-sequence LM, which adopts the\nencoder-decoder architecture. This type includes\nBART (Lewis et al., 2020), which is pre-trained\nwith the text infilling and sentence permutation\nobjectives, and T5 (Raffel et al., 2020), which is\npre-trained with the span-corruption objective and\nmultiple downstream tasks.\nIn § 4, we report the results of the frequently-\nused BASE versions of these PLMs, and results\nfor the other versions are shown in appendix C.\n3.2 Probing Method\nZero-Shot Probing reformulates probing tasks to\nthe format of pre-training language modeling objec-\ntives (Liu et al., 2021a) so that PLMs can do these\ntasks without any training. It is widely adopted\nby knowledge probing work (Petroni et al., 2019;\nTenney et al., 2019a) since it prevents PLMs from\nlearning new knowledge from training data so that\nthe achieved performance reflects PLMs’ intrin-\nsic knowledge. Hence the performance of zero-\nshot probing is commonly interpreted as the lower\nbound of PLMs’ knowledge (Jiang et al., 2020).\nAs illustrated in Figure 2, for each data instance\nof the three probing tasks, we cast its choices into\nnatural language prompts by filling them into man-\nually designed templates, and then let PLMs score\nthe prompts by the likelihood of language model-\ning. The choice with the highest score is regarded\nas the predicted answer of PLMs. Some implemen-\ntation details like taking which parts of the prompts\ninto scoring calculation may influence the PLMs’\nperformance. We search these details with prelimi-\nnary trials and only report the performance of the\nbest configuration in experiments.\nLinear Probing adds an additional shallow lin-\near classifier on top of the output contextualized\nrepresentations of PLMs, and only trains the addi-\ntional classifier while keeping the PLMs’ parame-\nters fixed. Since the model capacity of the shallow\nlinear classifier is too limited to fit the tasks, the\nachieved performance shall mainly come from the\nknowledge in the PLMs’ representations (Alain\nand Bengio, 2017). Hence linear probing is widely\nused in knowledge probing (Tenney et al., 2019b;\nHewitt and Manning, 2019).\nFine-Tuning is the standard method to adapt\nPLMs to downstream tasks, which trains all the\nPLMs’ parameters on the training data with task-\nspecific objectives. Considering the strong model\ncapacity of the PLMs, PLMs will inevitably fit\nthe probing tasks through the information in train-\ning data rather than only resort to their intrinsic\nknowledge. Hence the fine-tuning performance\nshall serve as an upper boundof the PLMs’ con-\nceptual knowledge in our experiments.\nFor CSJ and CiC, we take the filled prompts of\nidentical templates in zero-shot probing as inputs\nand train PLMs with the cross-entropy loss. For\nCPJ, we take the property statements as inputs and\nuse the binary cross entropy loss.\nMore detailed implementations about three prob-\ning methods are shown in appendix A.\n4 Experiment and Analysis\nWe first introduce the overall results in § 4.1 and\nconduct detailed analyses on the three probing tasks\n(§§ 4.2 to 4.4), respectively. We then analyze the\nperformance at different model scales (§ 4.5). More\nobservations and discussions on experimental re-\nsults are placed in appendix B.\n5019\nModel Hard Distractor Easy Distractor\nBERTBASE 25.1 15 .7\nRoBERTaBASE 25.3 15 .7\nGPT-2BASE 21.1 17 .0\nGPT-Neo125M 20.7 17 .1\nBARTBASE 24.2 16 .0\nT5BASE 24.6 15 .9\nTable 3: Mean reciprocal ranks (%) for hard distractors\nand easy distractors on CSJ in zero-shot probing results\nof various PLMs. Larger values for higher ranks.\n4.1 Overall Results\nThe overall experimental results are shown in Ta-\nble 2, from which we can observe that: (1) All the\nPLMs can achieve non-trivial (better than random\nguess) performance on all the probing tasks with\nzero-shot probing or linear probing, which indi-\ncates that existing PLMs capture a certain concep-\ntual knowledge with pre-training on massive texts.\n(2) However, even with fine-tuning, all PLMs’ ac-\ncuracies are still well below human performance,\nwhich urges further efforts on concept-aware pre-\ntraining. (3) The accuracies of PLMs using differ-\nent types of pre-training objectives are generally\non the same level. It suggests that any existing\npre-training objective has no special advantages in\nunderstanding concepts and further improvements\nmay come from targeted pre-training design. We\nprovide some analyses in the following sections to\nhelp targeted concept-aware PLMs development.\n4.2 Conceptual Similarity Judgment\nWe analyze the predictions and performance of\nvarious PLMs on CSJ, and find that:\nPLMs better distinguish coarse-grained con-\ncepts. As mentioned in § 2.2, among 20 distrac-\ntor entities, 5 of them are hard distractors of con-\ncepts sharing superordinates with the concept of\nthe query entity, and the others are easy distrac-\ntors. For example, if the query entity is of Mammal\nconcept, the entities of Bird concept are hard dis-\ntractors and the entities of Country concept are\neasy distractors. Table 3 shows the mean reciprocal\nranks of these two kinds of distractors. We can\nsee that the hard distractors are significantly ranked\nhigher than easy distractors, which indicates that\nPLMs generally better distinguish coarse-grained\nconcepts, such as telling the differences between\nAnimal and Country, but fail in distinguishing fine-\ngrained concepts. It suggests that future methods\nshould focus more on how to capture the subtle\nBERT RoBERTa GPT-2 GPT-Neo BART T5\n78.0 72 .5 64 .6 52 .5 65 .9 58 .3\nTable 4: Percentage (%) of false positive predictions\namong all incorrect predictions in fine-tuning results of\nvarious PLMs on the CPJ dataset.\ndifferences between fine-grained concepts.\n4.3 Conceptual Property Judgment\nWe analyze the error cases on CPJ and find that:\nConceptual transitivity challenges PLMs. Ta-\nble 2 shows that PLMs can achieve high instance-\nlevel accuracies, but all perform poorly in the chain-\nlevel evaluation. It suggests that PLMs can rela-\ntively well recall the properties for individual con-\ncepts like recalling the facts about entities in factual\nknowledge probing, but hardly understand the hier-\narchical relations of concepts and the property tran-\nsitivity. It suggests that further PLM works should\nnot only focus on better memorizing knowledge but\nalso consider how to better organize knowledge.\nPLMs have conceptual hallucination. It has\nbeen observed that PLMs frequently generate non-\nsensical and unfaithful outputs, which are factu-\nally incorrect, and previous work (Rohrbach et al.,\n2018; Reiter, 2018; Ji et al., 2022) dubs this phe-\nnomenon as hallucination. In our experiments, we\nobserve that many PLMs’ failure cases on CPJ task\ncan be described as conceptual hallucination, i.e.,\nPLMs hallucinate that concepts have certain proper-\nties while they actually do not. As shown in Table 4,\nthe errors of most PLMs are generally mainly from\nmaking false positive predictions, i.e., taking false\nconceptual property statements as true. It suggests\nthat PLMs tend to hallucinate the false conceptual\nproperties as true rather than cannot recall the true\nconceptual properties, which is interesting and we\nfurther explore whether there are certain spurious\ncorrelations causing this.\nWord co-occurrence causes conceptual hal-\nlucination. We hypothesize that the word co-\noccurrence in the pre-training corpora causes\nPLMs’ conceptual hallucination. For example, if\na PLM has seen the text “ The temple’s Jufu Hall\nwas included in the 1998 WorldMonuments Watch\nby the WorldMonuments Fund (WMF) ...preser-\nvation of the painteddecoration”3, it may be more\n3https://en.wikipedia.org/wiki/Temple_of_\nAgriculture\n5020\n0 10 20 30 40\nBM25 score\n20\n40\n60\n80\n100False Positive Rate (%)\nR2=0.81, p=2.19 × 10 8\nFigure 3: The false positive rate of BERT’s fine-tuning\nresults on CPJ negative instances with different BM25\nscores. Results of other PLMs are left in appendix C.1.\nlikely to predict the statement “ Monuments are\nused for decoration” as true. We empirically\nfind pieces of evidence supporting this hypothe-\nsis. For each CPJ instance, to assess the word\nco-occurrence in pre-training corpora, we retrieve\nthe most similar document of it from Wikipedia,\nwhich is a widely-used corpus in pre-training, with\nthe BM25 (Robertson et al., 1995) algorithm im-\nplemented in Whoosh (Mchaput, 2016), and use\nthe BM25 score of the top one of retrieved docu-\nments as the indicator of this CPJ instance’s word\nco-occurrence rate in pre-training corpus. We di-\nvide the negative instances of CPJ dataset into dif-\nferent subsets by their BM25 scores and observe\nthe false positive rate of BERT’s fine-tuning pre-\ndictions on them. The results are plotted in Fig-\nure 3, from which we can see that the false positive\nprediction rates, indicating conceptual hallucina-\ntion, have strong positive correlations to the BM25\nscores, indicating word co-occurrence. This sug-\ngests that the conceptual hallucination of PLMs\ncomes from capturing the spurious correlations of\nword co-occurrence in pre-training, and further pre-\ntraining work shall explore to fix it.\n4.4 Conceptualization in Contexts\nWe analyze the error cases on CiC and find that:\nPLMs conceptualize entities over-relying on\nmemories. In CiC, we find that if we remove\nthe contexts, PLMs can still predict a possibly\ncorrect concept, which is similar to previous\nworks (Petroni et al., 2019; Roberts et al., 2020;\nCao et al., 2021) showing that PLMs memorize a\ncertain knowledge about entities’ types. We dub\nthese predictions out-of-context predictions, which\ncan be regarded as the PLMs’ memories obtained\nin pre-training. What we evaluate in CiC is the\nBERT RoBERTa GPT-2 GPT-Neo BART T5\n72.9 75 .9 76 .7 60 .4 71 .8 59 .2\nTable 5: Percentage (%) of out-of-context predictions\namong all incorrect predictions in zero-shot probing\nresults of various PLMs on the CiC dataset.\nin-context conceptualization abilities rather than\nthe memorized knowledge about the concepts of\nentities, which is evaluated by CSJ. Hence rely-\ning on the memories and making out-of-context\npredictions are wrong for handling CiC. However,\nas shown in Table 5, in most of the error cases,\nPLMs wrongly conceptualize the entities within\ncontexts as the default out-of-context predictions.\nIt demonstrates that PLMs conceptualize entities\nby over-relying on memories rather than under-\nstanding the contexts, which reflects the lack of\ngenuine conceptualization abilities. We encourage\nfuture works to study whether the memories inhibit\nlearning to conceptualize during pre-training.\nUnderstanding hierarchy is more difficult than\ndisambiguation. In Table 6, we analyze the two\nerror types on CiC task. Disambiguation indicates\nthe PLM selects a wrong concept chain for the\ngiven entity and Wrong Levelindicates the PLM\nselects a wrong-level concept in the correct chain.\nIn the analysis, we only consider entities with more\nthan one concept chain. The Wrong Levelerrors\ntake up the majority, which shows that understand-\ning concept hierarchy is more difficult than disam-\nbiguation for PLMs and how to teach the PLMs to\nunderstand it is essential.\n4.5 Analysis on Model Scale\nInspired by recent advances showing the superior\nadvantages of large-scale models (Kaplan et al.,\n2020; Lester et al., 2021), we explore how the\nmodel scale influences PLMs’ conceptual knowl-\nedge. We investigate the family of three repre-\nsentative PLMs: BERT, GPT-2 and T5. Since\nfine-tuning extremely-large PLMs is too compu-\ntationally expensive, for models with more than\n2.5 billion parameters, we instead adopt BitFit (Za-\nken et al., 2022), which can achieve similar perfor-\nmance to fine-tuning (He et al., 2021) but requires\nmuch less computation. The results are shown in\nFigure 4, and we have following observations: (1)\nLarger-scale PLMs generally achieve better perfor-\nmance on all the probing tasks, which suggests that\nincreasing model scale can store more conceptual\n5021\nError Type Context Concept Chains\nDisambiguation He was nominated by President Person –> BusinessPerson\n29.0% Jimmy Carterto the court. Person –> Writer\nPerson –> Politician\nWrong Level Dolly is running on the grassland. Horse –> Mammal –> Animal\n71.0%\nTable 6: Error examples sampled from zero-shot probing results of BERTBASE on the CiC dataset. Italics denote\nentities. Underlines denote model predictions. Texts in bold denote answers.\n20 50 100 300 1,000 3,000 11,000\nMillions of parameters\n5\n10\n15\n20\n25\n30\n35\n40\n45Accuracy (%)\nConcept Similarity Judgment\n20 50 100 300 1,000 3,000 11,000\nMillions of parameters\n50\n55\n60\n65\n70\n75\n80Accuracy (%)\nConcept Property Judgment\n20 50 100 300 1,000 3,000 11,000\nMillions of parameters\n25\n30\n35\n40\n45\n50\n55Accuracy (%)\nConceptualization in Contexts\nZero-shot Probing Linear Probing Fine-tuning BERT GPT-2 T5\nFigure 4: Accuracies (%) of various PLMs at different scales. The accuracies on CPJ are instance-level.\nknowledge. However, the improvements brought\nby increasing model scale are generally marginal,\nespecially on CiC task, and the improvements in\nzero-shot probing and linear probing results are\nnot so obvious like in fine-tuning, which poses a\nquestion that whether the fine-tuning improvements\ncome from the intrinsic knowledge of PLMs. (2)\nThe fine-tuning accuracies of T5 11B with 11 bil-\nlion parameters, are still well below ordinary peo-\nple, which demonstrates that acquiring conceptual\nknowledge is quite challenging for existing pre-\ntraining methods, which encourages further efforts\non building concept-aware PLMs.\n5 Related Work\nKnowledge Probing To understand the success\nof PLMs, extensive works explore to know what\nPLMs know, and find PLMs have strong linguis-\ntic knowledge (Liu et al., 2019a; Hewitt and Man-\nning, 2019; Tenney et al., 2019b; Vuli´c et al., 2020).\nMoreover, it has been shown that PLMs have a cer-\ntain world knowledge, which is typically stored\nin world knowledge bases, such as the knowl-\nedge about entities (Broscheit, 2019; Tenney et al.,\n2019a) and their relationships (Petroni et al., 2019;\nRoberts et al., 2020; Jiang et al., 2020; Bouraoui\net al., 2020; Zhong et al., 2021). However, these ex-\nplorations are limited in the scope of factual knowl-\nedge, ignoring the conceptual knowledge, which\nis essential for both knowledge bases (Wu et al.,\n2012; Ji et al., 2019) and intelligence (Carey, 1991;\nCollins and Olson, 2014). Hence we explore the\nconceptual knowledge probing in this paper.\nConceptual Knowledge in PLMs Previous\nworks also explore the concept in PLMs (Michael\net al., 2020; Talmor et al., 2020; Aspillaga et al.,\n2021; Dalvi et al., 2021), which study principally\nsimilar topics with us. However, the concept they\nrefer to is essentially word sense. They focus on\nwhether PLMs discover the word senses and rec-\nognize their hierarchical relations. While in this\nwork, we study the concepts defined in knowledge\nbases to abstract real-world entities, which support\nbroader applications (Lv et al., 2018; Zhou et al.,\n2021; Zeng et al., 2021), and probe knowledge\nabout conceptual similarity and properties of con-\ncepts as well as PLMs’ conceptualization ability.\n6 Conclusion and Future Work\nIn this paper, we systematically analyze the concep-\ntual knowledge in existing PLMs by constructing a\nhigh-quality conceptual knowledge probing bench-\nmark (COPEN). Extensive experiments show that\n5022\nexisting PLMs have a certain conceptual knowl-\nedge, but are significantly worse than humans,\neven with billions of parameters. We further find\nthat PLMs fail in distinguishing fine-grained con-\ncepts and understanding concept hierarchy, and\nsuffer from conceptual hallucination caused by\nword occurrence and out-of-context bias. In the fu-\nture, inspired by works infusing factual knowledge,\nwe will try to develop conceptual knowledgeable\nPLMs by exploring concept-aware pre-training ob-\njectives and knowledge-enhanced architectures.\nLimitations\nIn the section, we discuss the limitations of this\nwork: (1) COPEN benchmark. COPEN only in-\nvolves English corpora, which limits the use of the\nbenchmark to PLMs pre-trained on other languages.\nIn the future, we will consider more languages and\nconstruct multilingual COPEN. (2) Large PLMs.\nWe do not experiment on very large PLMs, such as\nGPT-3 (Brown et al., 2020) and PaLM (Chowdhery\net al., 2022), due to our limited access to them. We\nconduct experiments on T511B with 11 billion pa-\nrameters instead. Experimental results demonstrate\nthat acquiring conceptual knowledge is quite chal-\nlenging for existing pre-training methods, which\nurges concept-aware pre-training objectives and\nmodel architectures. (3) Environmental impact.\nIn this paper, we conduct a lot of experiments with\nvarious PLMs, some of which even contain several\nbillions of parameters. It consumes large amounts\nof energy and causes large amounts of carbon diox-\nide emissions, which incurs negative influence to\nour environment (Strubell et al., 2019). But the\nexperiments are necessary for drawing faithful and\ncomprehensive conclusions. We hope our findings\ncould facilitate further research on more powerful\nPLMs with fewer parameters.\nEthical Considerations\nWe discuss the ethical considerations and broader\nimpact of this work in this section: (1) Intellec-\ntual property. The Wikipedia, Simple Wikipedia\ncorpora, and Wikidata are obtained from the Wiki-\nmedia dump4, which is shared under the CC BY-\nSA 3.0 license 5. The DBpedia 6 is shared under\nthe CC BY-SA 3.0 license and GNU Free Docu-\n4https://dumps.wikimedia.org/\n5https://creativecommons.org/licenses/by-sa/3.\n0/\n6www.dbpedia.org\nmentation License7. The GenericsKB corpus 8 is\nshared under the CC BY 4.0 license 9. These are\nall public and established resources, which are in-\ntended to support broad artificial intelligence and\nNLP research. We believe these resources are well\ndesensitized and anonymized. (2) Data annota-\ntion. We invite 19 annotators without background\nof expertise to annotate our datasets and produce\nhuman performance. They are all employed by\ncommercial data production companies. The in-\nvited annotators are fairly paid according to agreed\nworking hours and prices. The annotators are all\ninformed about how the data will be processed,\nused, and released, and this is confirmed in the data\nproduction contract. (3) Intended use. COPEN\nis a high-quality benchmark used for evaluating\nconceptual knowledge in PLMs and developing\nconcept-knowledgeable PLMs. Researchers can\nuse COPEN to assess new concept-aware objec-\ntives and conceptual-knowledge-enhanced archi-\ntectures. (4) Misuse risks. Considering COPEN is\nbuilt on top of a limited scope of natural texts and\nthe probing methods are inevitably influenced by\nsome spurious correlations, a good enough perfor-\nmance on COPEN cannot fully guarantee that the\ndeveloped methods really understand concepts and\nshall not be used to support relevant commercial\nand political claims. (5) Potential risks control.\nThe texts in COPEN are from public data and do\nnot involve private information, sensitive topics\nand social issues. The three tasks in COPEN also\ndo not involve sensitive topics or social issues. We\nmanually check some randomly sampled instances\nin COPEN and find no sensitive information or\nother risky issues. Hence we believe that COPEN\ndoes not create additional risks.\nAcknowledgements\nThis work is supported by the Key-Area Research\nand Development Program of Guangdong Province\n(2019B010153002), the Institute for Guo Qiang,\nTsinghua University (2019GQB0003), and Huawei\nNoah’s Ark Lab. The authors thank all the anony-\nmous reviewers for their detailed and valuable com-\nments and suggestions. The authors also thank all\nthe annotators for their substantial efforts in the\nannotation process.\n7https://www.gnu.org/licenses/fdl-1.3.html\n8https://allenai.org/data/genericskb\n9https://creativecommons.org/licenses/by/4.0/\n5023\nReferences\nGuillaume Alain and Yoshua Bengio. 2017. Under-\nstanding intermediate layers using linear classifier\nprobes. In Proceedings of ICLR.\nGrigoris Antoniou and Frank Van Harmelen. 2004. A\nsemantic web primer. MIT press.\nCarlos Aspillaga, Marcelo Mendoza, and Alvaro Soto.\n2021. Inspecting the concept knowledge graph en-\ncoded by modern language models. In Findings of\nACL-IJCNLP, pages 2984–3000.\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\nDBpedia: A nucleus for a web of open data. In The\nsemantic web, pages 722–735. Springer.\nSumithra Bhakthavatsalam, Chloe Anastasiades, and\nPeter Clark. 2020. GenericsKB: A knowledge base\nof generic statements. CoRR, abs/2005.00660.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. Zenodo.\nZied Bouraoui, José Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom BERT. In Proceedings of AAAI-IAAI-EAAI,\npages 7456–7463.\nSamuel Broscheit. 2019. Investigating entity knowledge\nin BERT with simple neural end-to-end entity linking.\nIn Proceedings of CoNLL, pages 677–685.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of NeurIPS, pages 1877–1901.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? Revisiting lan-\nguage models as knowledge bases. In Proceedings\nof ACL-IJCNLP, pages 1860–1874.\nSusan Carey. 1991. Knowledge acquisition: Enrich-\nment or conceptual change. The epigenesis of mind:\nEssays on biology and cognition, pages 257–291.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling language\nmodeling with pathways. CoRR, abs/2204.02311.\nJessica A. Collins and Ingrid R. Olson. 2014. Knowl-\nedge is power: How conceptual knowledge trans-\nforms visual cognition. Psychonomic Bulletin & Re-\nview, 21:843–860.\nDavid Alan Cruse. 1986. Lexical semantics. Cambridge\nuniversity press.\nFahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir Dur-\nrani, Jia Xu, and Hassan Sajjad. 2021. Discovering\nlatent concepts learned in BERT. In Proceedings of\nICLR.\nStefan Decker, Sergey Melnik, Frank van Harmelen,\nDieter Fensel, Michel C. A. Klein, Jeen Broekstra,\nMichael Erdmann, and Ian Horrocks. 2000. The\nsemantic web: The roles of XML and RDF. IEEE\nInternet Comput., 4(5):63–74.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT, pages\n4171–4186.\nRamanathan V Guha, Dan Brickley, and Steve Macbeth.\n2016. Schema. org: Evolution of structured data on\nthe web. Communications of the ACM, 59(2):44–51.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao\nLiu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao\nHan, Minlie Huang, et al. 2021. Pre-trained models:\nPast, present and future. Proceedings of AI Open.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\narXiv preprint arXiv:2110.04366.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word repre-\nsentations. In Proceedings of NAACL-HLT, pages\n4129–4138.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Comput. Linguistics,\n41(4):665–695.\n5024\nLei Ji, Yujing Wang, Botian Shi, Dawei Zhang,\nZhongyuan Wang, and Jun Yan. 2019. Microsoft\nconcept graph: Mining semantic concepts for short\ntext understanding. Data Intelligence, 1(3):238–270.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of hal-\nlucination in natural language generation. CoRR,\nabs/2202.03629.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics,\n8:423–438.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nJuan J. Lastra-Díaz, Josu Goikoetxea, Mohamed\nAli Hadj Taieb, Ana García-Serrano, Mohamed Ben\nAouicha, and Eneko Agirre. 2019. A reproducible\nsurvey on word embeddings and ontology-based\nmethods for word similarity: Linear combinations\noutperform the state of the art. Eng. Appl. Artif. In-\ntell., 85:645–665.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of EMNLP, pages 3045–\n3059.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of ACL, pages 7871–\n7880.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of NAACL-HLT,\npages 1073–1094.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nCoRR, abs/2107.13586.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT\nunderstands, too. CoRR, abs/2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nXin Lv, Lei Hou, Juanzi Li, and Zhiyuan Liu. 2018.\nDifferentiating concepts and instances for knowledge\ngraph embedding. In Proceedings of EMNLP, pages\n1971–1979.\nDeborah L McGuinness, Frank Van Harmelen, et al.\n2004. Owl web ontology language overview. W3C\nrecommendation, 10(10):2004.\nMchaput. 2016. Mchaput/whoosh: Pure-python full-\ntext search library. GitHub.\nJulian Michael, Jan A. Botha, and Ian Tenney. 2020.\nAsking without telling: Exploring latent ontologies\nin contextual representations. In Proceedings of\nEMNLP, pages 6792–6812.\nGregory Murphy. 2004. The big book of concepts. MIT\npress.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of EMNLP, pages\n1532–1543.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of EMNLP-IJCNLP,\npages 2463–2473.\nLutz Prechelt. 1996. Early stopping-but when? In\nGenevieve B. Orr and Klaus-Robert Müller, editors,\nNeural Networks: Tricks of the Trade, volume 1524\nof Lecture Notes in Computer Science, pages 55–69.\nSpringer.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872–\n1897.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nEhud Reiter. 2018. Hallucination in Neural NLG. Ehud\nReiter’s Blog.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In Proceedings of EMNLP,\npages 5418–5426.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at TREC-3. Nist Special Publication Sp,\n109:109.\n5025\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object Hal-\nlucination in Image Captioning. In Proceedings of\nEMNLP, pages 4035–4045.\nJohn F Sowa. 1976. Conceptual graphs for a data base\ninterface. IBM Journal of Research and Develop-\nment, 20(4):336–357.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of ACL, pages\n3645–3650.\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2007. Yago: a core of semantic knowledge.\nIn Proceedings of WWW, pages 697–706.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics - On what Lan-\nguage Model Pre-training Captures. Trans. Assoc.\nComput. Linguistics, 8:743–758.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of ACL, pages 4593–4601.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R. Bowman, Dipanjan\nDas, and Ellie Pavlick. 2019b. What do you learn\nfrom context? Probing for sentence structure in con-\ntextualized word representations. In Proceedings of\nICLR.\nDenny Vrande ˇci´c. 2012. Wikidata: A new platform\nfor collaborative data collection. In Proceedings of\nWWW, pages 1063–1064.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78–85.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\nPretrained Language Models for Lexical Semantics.\nIn Proceedings of EMNLP, pages 7222–7240.\nSandra R. Waxman and Dana Markow. 1995. Words\nas Invitations to Form Categories: Evidence from\n12- to 13-Month-Old Infants. Cognitive Psychology,\n29:257–302.\nMichele Wellsby and Penny M. Pexman. 2014. Devel-\noping embodied cognition: Insights from children’s\nconcepts and language processing. Frontiers in Psy-\nchology, 5.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of EMNLP, pages 38–45.\nWentao Wu, Hongsong Li, Haixun Wang, and Kenny Q\nZhu. 2012. Probase: A probabilistic taxonomy for\ntext understanding. In Proceedings of the 2012 ACM\nSIGMOD International Conference on Management\nof Data, pages 481–492.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings of ACL, pages 1–9.\nKaisheng Zeng, Chengjiang Li, Yan Qi, Xin Lv, Lei\nHou, Guozheng Peng, Juanzi Li, and Ling Feng.\n2021. Encoding the meaning triangle (object, entity,\nand concept) as the semantic foundation for entity\nalignment. In Proceedings of WISE, pages 227–241.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning to\nrecall. In Proceedings of NAACL-HLT, pages 5017–\n5033.\nJie Zhou, Shengding Hu, Xin Lv, Cheng Yang, Zhiyuan\nLiu, Wei Xu, Jie Jiang, Juanzi Li, and Maosong Sun.\n2021. KACC: A multi-task benchmark for knowl-\nedge abstraction, concretization and completion. In\nFindings of ACL-IJCNLP, pages 1751–1763.\n5026\nAppendices\nModel model_name\nBERTSMALL prajjwal1/bert-small\nBERTMEDIUM prajjwal1/bert-medium\nBERTBASE bert-base-uncased\nBERTLARGE bert-large-uncased\nRoBERTaBASE roberta-base\nGPT-2BASE gpt2\nGPT-2MEDIUM gpt2-medium\nGPT-2LARGE gpt2-large\nGPT-2XL gpt2-xl\nGPT-Neo125M EleutherAI/gpt-neo-125M\nBARTBASE facebook/bart-base\nT5SMALL t5-small\nT5BASE t5-base\nT5LARGE t5-large\nT53B t5-3b\nT511B t5-11b\nTable 7: The corresponding model_names in Transform-\ners library (Wolf et al., 2020) for different PLMs.\nA Implementation Details\nWe use the implementation code and pre-trained pa-\nrameters of PLMs released in HuggingFace Trans-\nformers library (Wolf et al., 2020) to run our experi-\nments. The model_names we used in Transformers\nfor different PLMs are shown in Table 7. We run\nexperiments for large models (T53B , and T511B ) on\nNVIDIA V100 GPUs, which approximately con-\nsumes 160 GPU hours, and the other PLMs on\nNvidia GEFORCE RTX 3090 GPUs, which con-\nsumes about 300 GPU hours. We will introduce\nthe implementation details for zero-shot probing\n(appendix A.1), linear probing (appendix A.2), and\nfine-tuning (appendix A.3).\nA.1 Zero-Shot Probing\nAs mentioned in § 3.2, we take different text parts\nof the prompts into scoring calculation. Table 8\nshows the text parts used by various PLMs to score\nprompts on the three datasets.\nA.2 Linear Probing\nWe use the final outputs of specific tokens as the\nfeatures extracted by PLMs: [CLS] for BERT; <s>\nfor RoBERTa; the last token for GPT-2, GPT-Neo,\nand BART; the first token for T5. We then tune a\nlightweight linear classifier on the fixed features\nfor BERT, RoBERTa, GPT-2, GPT-Neo, BART and\ntune the final vocabulary classification head for T5.\nMoreover, we reformulate the original instances\ninto the text-to-text format for T5, and the input\nand output formats are shown in Table 9.\nModel CSJ CPJ CiC\nBERTBASE Query Entity Concept All\nRoBERTaBASE Query Entity Concept Concept\nGPT-2BASE All All Concept\nGPT-Neo125M All Concept Concept\nBARTBASE Query Entity Concept Concept\nT5BASE Query Entity Concept All\nTable 8: The text parts used to calculate scores of\nprompts in zero-shot probing on the three datasets. All:\nuse the negative perplexities of prompts as scores. The\nmeanings of the other text parts are shown in Figure 2.\nHyperparameters We set the learning rate as\n1 ×10−3 and apply early stopping (Prechelt, 1996)\non the accuracy on the development dataset with a\npatience of 20 epochs. We keep the other hyperpa-\nrameters the same as in Table 10.\nA.3 Fine-Tuning\nWe follow the fine-tuning methods in original\npapers to fine-tune BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019b), GPT-2 (Radford\net al., 2019), GPT-Neo (Black et al., 2021), and\nBART (Lewis et al., 2020). As in appendix A.2,\nwe reformulate the original instances into the text-\nto-text format for T5 (Raffel et al., 2020), and the\ninput and output formats are shown in Table 9.\nHyperparameters We follow the hyperparame-\nters mostly used in previous literature. The hyper-\nparameters are shown in Table 10. And we apply\nearly stopping (Prechelt, 1996) on the accuracy on\nthe development dataset.\nParameter-efficient Tuning for Big Models\nDue to the limits of computation, we consider the\nparameter-efficient tuning for models with more\nthan 2.5 billion parameters (T53B and T511B ). Pre-\nvious works (He et al., 2021) have proven that\nparameter-efficient tuning methods can save GPU\nmemory, accelerate training for PLMs, and achieve\ncomparable performance to fine-tuning all parame-\nters, especially at large scales. Therefore, we adopt\nBitFit (Zaken et al., 2022) implemented by Open-\nDelta10 to fine-tune big models.\nB More Discussions on Experimental\nResults\nIn the section, we discuss some detailed and inter-\nesting observations.\n10https://github.com/thunlp/OpenDelta\n5027\nConceptual Similarity Judgment\nOriginal Query: Inter Milan\nOriginal Candidates: Milan, Milan Fashion Week, Pohang Steelers, Series A\nOriginal Label: Pohang Steelers\nProcessed Input: choose the most similar entity to Inter Milan: (A) Milan, (B) Milan Fashion Week, (C) Pohang Steelers, (D) Series A.\nProcessed Label: C\nConceptual Property Judgment\nOriginal Statement: Mammals raise their young on milk.\nOriginal Label: True\nProcessed Input: verify: Mammals raise their young on milk.\nProcessed Label: true\nConceptualization in Contexts\nOriginal Context: Dollyis running on the grassland.\nConcept Chain: Horse –> Mammal –> Animal\nOriginal Label: Animal\nProcessed Input: select concept: <entity> Dolly </entity> is running on the grassland. Select a contextually related concept for\nDolly from (A) Horse, (B) Mammal, (C) Animal.\nProcessed Label: C\nTable 9: The input and output format used to linear probe and fine-tune T5 on the three datasets.\nCSJ CPJ CiC\nThe Others T5 The Others T5 The Others T5\nLearning Rate 3 × 10−5 5 × 10−5 3 × 10−5 5 × 10−5 3 × 10−5 5 × 10−5\nWeight Decay 1 × 10−5 1 × 10−5 1 × 10−5 1 × 10−5 1 × 10−5 1 × 10−5\nBatch Size 4 16 64 32 4 16\nWarmup Rate 0.1 0 .1 0 .1 0 .1 0 .1 0 .1\nTable 10: Hyperparameters used to fine-tune PLMs on COPEN.\nCSJ CPJ CiC\nQuery Entity Candidate Entity All Concept Answer All Concept All\nBERTSMALL 15.0 6 .5 8 .1 50.7 48 .5 51 .5 31.9 35 .1\nBERTMEDIUM 16.8 7 .2 10 .0 49.3 46 .7 49 .2 29.6 33 .3\nBERTBASE 20.3 7 .5 11 .3 49.4 47 .2 49 .2 32.6 37 .6\nBERTLARGE 22.3 8 .2 13 .4 50.5 47 .6 50 .4 31.1 36 .9\nRoBERTaBASE 15.5 5 .1 10 .0 49.2 46 .7 47 .6 31.4 25 .5\nGPT-2BASE 2.9 6 .6 7 .9 49.4 48 .4 51 .5 32.3 31 .1\nGPT-2MEDIUM 3.7 8 .6 10 .5 52.0 47 .2 47 .2 30.3 32 .0\nGPT-2LARGE 4.6 9 .0 11 .3 51.8 47 .3 47 .2 34.3 33 .8\nGPT-2XL 3.9 9 .6 11 .7 50.7 47 .2 47 .1 35.3 37 .0\nGPT-Neo125M 2.6 6 .6 7 .9 52.2 47 .2 47 .6 32.6 28 .8\nBARTBASE 14.4 5 .0 7 .1 48.7 48 .4 48 .0 33.6 27 .4\nT5SMALL 11.6 5 .4 6 .5 52.5 47 .6 53 .2 34.9 40 .1\nT5BASE 15.2 7 .2 10 .3 55.9 47 .2 49 .5 39.1 42 .3\nT5LARGE 20.9 7 .8 14 .0 52.4 47 .2 49 .8 40.5 42 .6\nT53B 19.2 7 .9 14 .1 49.4 47 .7 49 .4 38.6 47 .0\nT511B 24.8 7 .8 14 .5 46.7 46 .7 49 .9 37.2 41 .3\nTable 11: Overall zero-shot probing accuracies (%) of using different text parts to score prompts on COPEN.\n5028\nModel Linear Probing Fine-tuning\nSeed=42 Seed=43 Seed=44 Mean Std Seed=42 Seed=43 Seed=44 Mean Std\nConceptual Similarity Judgment\nBERTSMALL 9.1 8 .2 8 .9 8 .7 0 .37 17.6 17 .1 19 .2 18 .0 0 .91\nBERTMEDIUM 13.1 12 .3 13 .1 12 .8 0 .35 20.3 21 .1 21 .6 21 .0 0 .57\nBERTBASE 16.3 16 .3 15 .8 16 .1 0 .21 28.5 26 .6 26 .9 27 .3 0 .86\nBERTLARGE 16.5 16 .9 17 .3 16 .9 0 .31 28.7 30 .2 29 .5 29 .5 0 .61\nRoBERTaBASE 11.8 12 .0 12 .3 12 .0 0 .21 22.8 21 .6 22 .4 22 .3 0 .51\nGPT-2BASE 4.6 4 .1 4 .1 4 .3 0 .24 19.7 20 .1 20 .3 20 .1 0 .23\nGPT-2MEDIUM 5.3 5 .2 5 .2 5 .2 0 .02 24.9 22 .2 23 .0 23 .4 1 .15\nGPT-2LARGE 4.0 6 .8 5 .6 5 .5 1 .13 22.2 24 .0 23 .4 23 .2 0 .77\nGPT-2XL 7.8 15 .0 10 .1 11 .0 3 .00 25.9 24 .2 25 .7 25 .3 0 .75\nGPT-Neo125M 11.1 10 .7 11 .2 11 .0 0 .20 18.8 18 .4 17 .8 18 .3 0 .42\nBARTBASE 8.5 8 .3 8 .4 8 .4 0 .10 20.4 21 .0 21 .7 21 .0 0 .50\nT5SMALL 4.8 4 .8 4 .7 4 .8 0 .05 10.1 17 .6 6 .9 11 .5 4 .48\nT5BASE 5.2 4 .8 4 .7 4 .9 0 .21 27.4 27 .5 28 .7 27 .9 0 .60\nT5LARGE 4.7 4 .9 4 .8 4 .8 0 .09 31.0 33 .4 32 .5 32 .3 1 .01\nT53B 5.0 4 .9 5 .2 5 .0 0 .11 41.0 40 .6 42 .0 41 .2 0 .61\nT511B 4.7 4 .7 4 .7 4 .7 0 .01 43.7 43 .6 43 .8 43 .7 0 .08\nConceptual Property Judgment\nBERTSMALL 57.8 58 .8 57 .8 58 .1 0 .47 66.3 66 .5 67 .2 66 .7 0 .39\nBERTMEDIUM 58.2 59 .6 58 .5 58 .8 0 .59 66.7 67 .5 67 .3 67 .2 0 .35\nBERTBASE 61.2 61 .9 61 .5 61 .6 0 .28 66.8 68 .3 69 .2 68 .1 0 .98\nBERTLARGE 61.6 61 .7 59 .0 60 .8 1 .26 67.8 69 .6 71 .2 69 .5 1 .41\nRoBERTaBASE 61.7 62 .0 61 .9 61 .9 0 .13 71.4 72 .7 71 .8 72 .0 0 .54\nGPT-2BASE 65.2 63 .3 66 .0 64 .8 1 .14 71.3 69 .5 70 .5 70 .4 0 .72\nGPT-2MEDIUM 67.0 67 .4 67 .4 67 .3 0 .17 73.0 68 .6 72 .9 71 .5 2 .07\nGPT-2LARGE 66.2 67 .8 66 .8 66 .9 0 .62 74.5 72 .7 73 .4 73 .5 0 .74\nGPT-2XL 67.8 68 .1 68 .6 68 .2 0 .36 74.5 75 .1 74 .7 74 .8 0 .22\nGPT-Neo125M 61.9 62 .4 62 .1 62 .2 0 .21 68.9 68 .4 67 .4 68 .2 0 .62\nBARTBASE 58.8 58 .2 58 .7 58 .5 0 .27 68.5 69 .2 67 .1 68 .2 0 .86\nT5SMALL 67.7 67 .2 65 .0 66 .6 1 .18 71.3 72 .2 72 .1 71 .9 0 .40\nT5BASE 67.3 66 .8 66 .8 66 .9 0 .25 72.6 72 .1 72 .8 72 .5 0 .28\nT5LARGE 68.9 69 .7 69 .3 69 .3 0 .33 72.5 73 .4 75 .2 73 .7 1 .10\nT53B 69.2 69 .7 69 .5 69 .5 0 .22 76.6 76 .6 76 .2 76 .4 0 .19\nT511B 67.3 66 .5 66 .0 66 .6 0 .53 78.2 78 .3 79 .2 78 .6 0 .46\nConceptualization in Contexts\nBERTSMALL 32.4 32 .7 33 .3 32 .8 0 .38 44.6 47 .0 48 .4 46 .6 1 .55\nBERTMEDIUM 31.6 31 .2 31 .1 31 .3 0 .22 49.4 49 .1 49 .8 49 .4 0 .31\nBERTBASE 33.6 34 .5 35 .0 34 .3 0 .59 49.3 48 .9 50 .3 49 .5 0 .60\nBERTLARGE 35.4 38 .9 35 .3 36 .6 1 .67 50.7 53 .0 51 .6 51 .8 0 .92\nRoBERTaBASE 27.3 32 .0 30 .7 30 .0 1 .98 51.3 52 .6 53 .8 52 .6 1 .02\nGPT-2BASE 31.7 36 .7 35 .1 34 .5 2 .08 54.0 54 .2 54 .3 54 .2 0 .12\nGPT-2MEDIUM 29.3 25 .6 29 .1 28 .0 1 .69 54.6 54 .5 54 .9 54 .7 0 .14\nGPT-2LARGE 32.8 28 .8 33 .7 31 .8 2 .16 53.4 52 .7 53 .6 53 .3 0 .36\nGPT-2XL 27.7 32 .2 29 .9 29 .9 1 .83 52.6 54 .4 54 .4 53 .8 0 .88\nGPT-Neo125M 38.9 38 .9 40 .9 39 .6 0 .93 47.6 47 .0 47 .5 47 .4 0 .25\nBARTBASE 44.1 42 .1 44 .9 43 .7 1 .19 50.8 49 .7 53 .5 51 .3 1 .56\nT5SMALL 25.7 26 .1 24 .9 25 .6 0 .53 43.5 44 .4 45 .0 44 .3 0 .64\nT5BASE 25.5 23 .9 24 .7 24 .7 0 .66 53.2 53 .3 52 .9 53 .2 0 .18\nT5LARGE 24.3 24 .3 25 .3 24 .6 0 .49 52.4 56 .9 57 .2 55 .5 2 .21\nT53B 26.7 27 .5 26 .8 27 .0 0 .35 59.2 57 .5 55 .9 57 .5 1 .35\nT511B 25.1 26 .6 26 .4 26 .0 0 .66 56.7 58 .7 56 .5 57 .3 0 .97\nTable 12: Overall linear probing and fine-tuning accuracies (%) of all PLMs on COPEN. We run experiments 3\ntimes using three seeds: 42, 43, 44. Mean: mean accuracy of the three trials; Std: standard deviation.\n5029\nComparison of Pre-training Method In Fig-\nure 2, we can observe that: (1) For PLMs using\nthe same architecture, T5 generally outperforms\nBART, and BERT generally outperforms RoBERTa.\nThe differences may come from the different pre-\ntraining corpora. (2) Autoregressive LMs (GPT-2,\nGPT-Neo) perform worse on CSJ, which is con-\nsistent with the observations on factual knowledge\nprobing (Liu et al., 2021b). As we are the first to\nstudy conceptual knowledge in PLMs, we focus\non the general question “to what extent do current\nPLMs understand conceptual knowledge?” and\nprovide more general conclusions in the paper. We\nleave the detailed and in-depth analysis of a spe-\ncific PLM, e.g., layer-wise analysis (Dalvi et al.,\n2021), in future works.\nComparison of Probing Method Intuitively,\nzero-shot probing reflects the lower bound of\nPLMs’ knowledge (Jiang et al., 2020), while linear\nprobing learns a task-specific linear classifier and\nperforms better than zero-shot probing, and fine-\ntuning reflects the upper boundof PLMs’ knowl-\nedge. However, as shown in Figure 2, linear prob-\ning sometimes underperforms zero-shot probing,\nespecially in CSJ and chain-level CPJ. The reason\nmay be that the concepts used for training and test-\ning are disjoint, and linear probing involves train-\nable parameters, which may learn spurious or shal-\nlow correlations on training sets and hence strug-\ngles on generalization. Meanwhile, fine-tuning still\nperforms poorly, which demonstrates that existing\nPLMs systematically lack conceptual knowledge.\nComparison of Instance-Level and Chain-Level\nCPJ For chain-level, BERT performs the best,\nbut for instance-level performs worse than T5. The\nreason may be that BERT better understands con-\ncept transitivity (i.e., making more consistent pre-\ndictions) but stores fewer conceptual properties\noverall. A thorough and comprehensive analysis\nis needed on this phenomenon and we leave it in\nfuture works.\nC Additional Experimental Results\nTable 11 shows overall zero-shot probing results on\nCOPEN. The experimental results of linear prob-\ning and fine-tuning are obtained at 3 random trials\nusing seeds 42, 43, 44. Table 12 shows overall\nlinear probing and fine-tuning results on COPEN.\nAnd we provide additional results for the analytical\nexperiments: analysis of conceptual hallucination\nModel Disambiguation Wrong Level\nBERTBASE 29.0% 71 .0%\nRoBERTaBASE 12.8% 87 .2%\nGPT-2BASE 12.5% 87 .5%\nGPT-Neo125M 11.9% 88 .1%\nBARTBASE 11.5% 88 .5%\nT5BASE 32.0% 68 .0%\nTable 13: The proportion of different error types of\nzero-shot probing results on the CiC dataset. We only\nconsider the entities with more than one concept chain.\non the CPJ dataset (appendix C.1), error analysis\non the CiC dataset (appendix C.2), and analysis on\navoiding dataset artifacts (appendix C.3).\nC.1 Conceptual Hallucinationon CPJ\nFigure 5 shows the false negative rates on subsets\nwith different BM25 scores for various PLMs. We\ncan observe that the false positive rates, which indi-\ncates conceptual hallucination, have strong positive\ncorrelations to the BM25 scores, which indicates\nword co-occurrence.\nC.2 Error Analysis on CiC\nTable 13 shows the proportions of different error\ntypes. We can observe that in most wrong predic-\ntions, PLMs select concepts of wrong levels. It\nindicates that PLMs lack a comprehensive under-\nstanding of concept hierarchy and fail to conceptu-\nalize entities according to contexts.\nC.3 Analysis on Avoiding Dataset Artifacts\nDataset artifacts leak shallow information and\ncause the PLMs to learn spurious correlations\nrather than exhibit inner knowledge. When con-\nstruct COPEN, we avoid two kinds of artifacts:\nLexical Overlap means that the query and the an-\nswer have word overlap, which may enable PLMs\nto make correct predictions using spurious corre-\nlations without the correct knowledge. For ex-\nample, in CSJ, if the query entity is Stanford\nUniversity and the answer entity is University\nof California; in CiC, if the context is She grad-\nuated fromStanford University and the answer\nconcept is University; they have lexical overlap.\nWe conduct experiments on the data with lex-\nical overlap. As shown in Table 14, on the data\nwith lexical overlap, PLMs perform much better.\nBut this should be interpreted as they learn shallow\nclues leaked by artifacts since they cannot achieve\nsimilar performance on data without lexical over-\n5030\nBM25 score\n20\n40\n60\n80\n100False Positive Rate (%)\nR2=0.81, p=2.19 × 10 8\nBERTBASE\nBM25 score\n10\n20\n30\n40\n50\n60False Positive Rate (%)\nR2=0.56, p=9.09 × 10 5\nGPT 2BASE\nBM25 score\n10\n20\n30\n40\n50\n60\n70\n80\n90False Positive Rate (%)\nR2=0.88, p=3.35 × 10 10\nBARTBASE\n0 10 20 30 40\nBM25 score\n10\n20\n30\n40\n50\n60\n70\n80False Positive Rate (%)\nR2=0.83, p=8.03 × 10 9\nRoBERTaBASE\n0 10 20 30 40\nBM25 score\n10\n20\n30\n40\n50\n60False Positive Rate (%)\nR2=0.65, p=9.66 × 10 6\nGPT NeoBASE\n0 10 20 30 40\nBM25 score\n10\n20\n30\n40\n50\n60\n70False Positive Rate (%)\nR2=0.81, p=2.40 × 10 8\nT5BASE\nFigure 5: The false positive rate of various PLMs’ fine-tuning results on negative instances of the CPJ dataset with\ndifferent BM25 scores.\nModel CSJ CiC\nw/ LO w/o LO w/ LO w/o LO\nBERTBASE 68.9 20 .3 52.5 37 .6\nRoBERTaBASE 62.2 15 .5 48.5 31 .4\nGPT-2BASE 34.2 7 .9 43.8 32 .3\nGPT-Neo125M 34.0 7 .9 52.4 32 .6\nBARTBASE 75.9 14 .4 53.2 33 .6\nT5BASE 69.2 15 .2 62.7 42 .3\nTable 14: Zero-shot probing accuracies (%) of PLMs on\ndata with lexical overlap (w/ LO) and without lexical\noverlap (w/o LO). We collect 688 and 1, 200 instances\nwith lexical overlap for CSJ and CiC, respectively.\nlap. Hence, we filter out all instances with lexical\noverlap in COPEN to avoid this kind of artifact.\nConcept Overlap is that the same concepts show\nup in both training and test datasets, which may\nleak conceptual knowledge, i.e., the PLMs may\nlearn some knowledge from training data. In\nCOPEN, as mentioned in § 2.1, we split different\ntop-level concepts and their subconcepts into differ-\nent sub-datasets, so as to avoid concept overlap. To\nempirically show the influence of concept overlap,\nwe randomly re-split the datasets into same-size\ntraining, development, and test sets and see the\nfine-tuning performance on the new split.\nThe results of fine-tuning BERT are shown in\nFigure 6, and the results of fine-tuning and linear\nprobing for all PLMs are shown in Table 15. Fine-\nCSJ CPJ CiC\n30\n40\n50\n60\n70Accuracy (%)\nw/o concept overlap\nw/ concept overlap\nFigure 6: Fine-tuning accuracies of BERTBASE on data\nwith and without concept overlap.\ntuning on datasets with concept overlap achieves\nmuch higher accuracies, especially on CSJ. It indi-\ncates that if we do not avoid concept overlap, PLMs\ncan easily learn conceptual knowledge from train-\ning data and lead to false optimistic conclusions.\nD COPEN\nWe provide a detailed introduction to COPEN.\nD.1 COPEN Taxonomy\nDisjoint Concepts We divide all the concepts\ninto two disjoint sets: one set containing 11 top-\nlevel concepts together with all their sub-concepts\nfor constructing training and development datasets,\nand the other set containing the other concepts for\n5031\nModel CSJ CPJ CiC\nw/ CO w/o CO w/ CO w/o CO w/ CO w/o CO\nLinear Probing\nBERTBASE 20.0 16 .1 64.1 61 .6 46.5 34 .3\nRoBERTaBASE 12.3 12 .0 65.9 61 .9 45.4 30 .0\nGPT-2BASE 5.2 4 .3 67.2 64 .8 39.0 34 .5\nGPT-Neo125M 15.4 11 .0 64.6 62 .2 58.3 39 .6\nBARTBASE 9.4 8 .4 62.6 58 .5 50.2 43 .7\nT5BASE 4.7 4 .9 68.8 66 .9 33.9 24 .7\nFine-tuning\nBERTBASE 63.4 27 .3 75.4 68 .1 65.4 49 .5\nRoBERTaBASE 61.0 22 .3 77.0 72 .0 66.6 52 .6\nGPT-2BASE 49.9 20 .1 72.7 70 .4 65.4 54 .2\nGPT-Neo125M 44.3 18 .3 71.2 68 .2 62.5 47 .4\nBARTBASE 54.7 21 .0 73.1 68 .2 67.4 51 .3\nT5BASE 50.6 27 .9 77.6 72 .5 67.6 53 .2\nTable 15: Accuracies (%) of linear probing and fine-tuning on data with concept overlap (w/ CO) and without\nconcept overlap (w/o CO).\n#Concepts Top-Level Concepts\nTraining& 248 Organisation, Name, Award, MeanOfTransportation, Colour, Language, Person,\nDevelopment Holiday, Work, Currency, EthnicGroup\nTesting 198 AnatomicalStructure, Species, Food, Event, TimePeriod, ChemicalSubstance,\nPlace, Device, Disease, Activity, Biomolecule, SportsSeason\nTable 16: The top-level concepts and the number of concepts used for training, development, and testing.\ntesting datasets. As shown in Table 16, there are\n248 concepts including 11 top-level concepts for\ntraining and development datasets and 198 con-\ncepts including 12 top-level concepts for testing.\nConcept Hierarchy We present the concepts\nfor training and development datasets in Figure 7\nand the concepts for testing datasets in Figure 8.\nObject is a virtual concept for visualization and is\nnot included in the overall 446 concepts.\nD.2 Concept Similarity Judgment\nHuman Performance We sample 1, 000 in-\nstances from the testing dataset and invite anno-\ntators with no linguistic background to perform the\nCSJ task. All the annotators are trained with a few\ninstances before the evaluation.\nCo-occurrence-based Filtering We filter out in-\nstances of which query entities and answer entities\nhave a high association, which are estimated by\ncosine similarity of their Glove word embeddings.\nSpecifically, for a query entity, we sample5 answer\nentities and select the entity with the lowest asso-\nciation with the query entity as the answer entity.\nThen we choose distractor entities iteratively fol-\nlowing the rules: (1) Sample a distractor entity, if\nthe entity has a higher association with the query\nentity than the answer entity, then select the distrac-\ntor entity as a candidate entity. (2) If not, select the\ndistractor entity as a candidate entity with a 20%\nprobability, otherwise start the next iteration until\nthe number of distractor entities reaches 20.\nD.3 Conceptual Property Judgment\nHuman Annotation We invite annotators with\nno linguistics background to check whether the\ninstances are correctly labeled, grammatically cor-\nrect, and describing concept properties. All an-\nnotators are well-trained and required to pass a\nqualification before the annotation. The instances\noriginally labeled as false are annotated 4 times,\nand the other instances are annotated once. During\nthe annotation, an author of the paper and another\nexperienced annotator separately sample 10% of\nthe instances to check the quality of annotation.\nThe acceptance criterion of the annotation is that\nthe percentage of obvious annotation errors in the\nsampled instances (e.g., label the statementThe sun\nhas two eyesas true) does not exceed 3%, and the\ninter-annotator agreement rates exceed 85% for the\ninstances annotated 4 times. Major voted results of\nthe instances annotated 4 times together with the\n5032\ninstances annotated once constitute the CPJ dataset.\nHuman Performance We use the 2,159 in-\nstances that are annotated 4 times in the testing\ndataset to evaluate human performance. We con-\nduct a 4-round evaluation: take the major voted\nresults of 3 annotators as labels and the other one\nas human predictions to calculate the accuracy of\nthe round. The mean accuracy of 4 rounds is re-\nported as the human accuracy on the CPJ dataset.\nD.4 Conceptualization in Contexts\nHuman Annotation We invite annotators with\nno linguistics background to annotate the dataset.\nTo ensure quality, all annotators are well-trained\nand required to pass a qualification before the an-\nnotation. All instances are annotated four times.\nMoreover, during the annotation, an author of the\npaper and another experienced annotator separately\nsample 10% of the examples to check the qual-\nity of annotation. The acceptance criterion of the\nannotation is that the percentage of obvious annota-\ntion errors (e.g., Select Horse for Dolly according\nto the context Dolly is running on the grassland.)\ndoes not exceed 3%, and the inter-annotator agree-\nment rates exceed 80%. Major voted results of the\n4 annotated results constitute the final CiC dataset.\nHuman Performance We use all instances in\nthe testing dataset, which are annotated 4 times,\nto evaluate human performance. We conduct a 4-\nround evaluation: take the major voted results of 3\nannotators as labels and the other one as human pre-\ndictions to calculate the accuracy of the round. The\nmean accuracy of 4 rounds is the human accuracy.\n5033\nFigure 7: Concept taxonomy for training and development datasets. Object is a virtual concept without annotated\ninstances.\n5034\nFigure 8: Concept taxonomy for testing datasets. Object is a virtual concept without annotated instances.\n5035",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6965665817260742
    },
    {
      "name": "Cognition",
      "score": 0.5402992963790894
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.5356457233428955
    },
    {
      "name": "Domain knowledge",
      "score": 0.496415913105011
    },
    {
      "name": "Bottleneck",
      "score": 0.4908050000667572
    },
    {
      "name": "Conceptual graph",
      "score": 0.48499342799186707
    },
    {
      "name": "Conceptual model",
      "score": 0.47604990005493164
    },
    {
      "name": "Conceptual framework",
      "score": 0.4457816481590271
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.4440470039844513
    },
    {
      "name": "Knowledge management",
      "score": 0.4253152310848236
    },
    {
      "name": "Cognitive science",
      "score": 0.3402021527290344
    },
    {
      "name": "Artificial intelligence",
      "score": 0.338795006275177
    },
    {
      "name": "Psychology",
      "score": 0.16852712631225586
    },
    {
      "name": "Epistemology",
      "score": 0.09836390614509583
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ]
}