{
  "title": "Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle",
  "url": "https://openalex.org/W3170110950",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5073742611",
      "name": "Yikang Shen",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A5089182977",
      "name": "Shawn Tan",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A5108137151",
      "name": "Alessandro Sordoni",
      "affiliations": [
        "Université de Montréal",
        "Centre Universitaire de Mila"
      ]
    },
    {
      "id": "https://openalex.org/A5102886212",
      "name": "Siva Reddy",
      "affiliations": [
        "Microsoft Research Montréal (Canada)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5112608251",
      "name": "Aaron Courville",
      "affiliations": [
        "McGill University",
        "Centre Universitaire de Mila"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2158349948",
    "https://openalex.org/W2527995245",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3168987555",
    "https://openalex.org/W2985308740",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W2932376173",
    "https://openalex.org/W2970060558",
    "https://openalex.org/W2970619710",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W2949629417",
    "https://openalex.org/W2123893795",
    "https://openalex.org/W2515464445",
    "https://openalex.org/W3098284381",
    "https://openalex.org/W2158567261",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2296332834",
    "https://openalex.org/W2308720496",
    "https://openalex.org/W3034552719",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963227939",
    "https://openalex.org/W2093647425",
    "https://openalex.org/W595069947",
    "https://openalex.org/W2169573153",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2554915555",
    "https://openalex.org/W2164645230"
  ],
  "abstract": "Yikang Shen, Shawn Tan, Alessandro Sordoni, Siva Reddy, Aaron Courville. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1660–1672\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n1660\nExplicitly Modeling Syntax in Language Models\nwith Incremental Parsing and a Dynamic Oracle\nYikang Shen\nMila/Université de Montréal\nyi-kang.shen@umontreal.ca\nShawn Tan\nMila/Université de Montréal\ntanjings@mila.quebec\nAlessandro Sordoni\nMicrosoft Research Montréal\nSiva Reddy\nMila/McGill University\nAaron Courville\nMila/Université de Montréal\nAbstract\nSyntax is fundamental to our thinking about\nlanguage. Failing to capture the structure of\ninput language could lead to generalization\nproblems and over-parametrization. In the\npresent work, we propose a new syntax-aware\nlanguage model: Syntactic Ordered Memory\n(SOM). The model explicitly models the struc-\nture with an incremental parser and maintains\nthe conditional probability setting of a stan-\ndard language model (left-to-right). To train\nthe incremental parser and avoid exposure bias,\nwe also propose a novel dynamic oracle, so\nthat SOM is more robust to wrong parsing\ndecisions. Experiments show that SOM can\nachieve strong results in language modeling,\nincremental parsing and syntactic generaliza-\ntion tests, while using fewer parameters than\nother models.\n1 Introduction\nSeveral recent works have systematically studied\nthe linguistic abilities of modern language models,\nparticularly syntax (Linzen et al., 2016; Marvin and\nLinzen, 2018; Gulordava et al., 2018). They ﬁnd\nthat most language models are good at capturing\nfrequent syntactic structures but do not generalize\nwell to those in the long tail. Moreover, although\nsome excel at having low perplexity scores, this\nis less due to their syntactic ability but more due\nto capturing collocations (frequently co-occurring\nwords). Recently, Hu et al. (2020) show that RNNs\nunderperform on a syntactic generalization (SG)\ntest set, whereas models that have an explicit notion\nof syntax, such as RNNG (Dyer et al., 2016), fare\nwell on SG but at the cost of generally poorer lan-\nguage modeling (higher perplexity). Transformer-\nbased models achieve strong performance when\ntrained with large datasets, but are worse than ran-\ndom when trained on a small dataset.\nThese works showed that building language\nmodels with an explicit internal model of syntax\nFigure 1: The mechanism of SOM. “ Context” is a dis-\ntributed representation of previous sentences. It could\nalso represents the source sentence in a sequence to se-\nquence task. It incrementally build subtrees given the\ninput sentences. A RNN will takes the context represen-\ntation and the representations of subtrees in the current\nsentence to predict next token.\nhelps in achieving better performance in SG tasks\nand is also thought to help learn more efﬁciently\nin low data settings. However, building syntax-\naware models that also obtain strong language mod-\neling performance, when compared with recent\ntransformer-based models, has until now seemed\nelusive. In this work, we propose a new syntax-\naware language model dubbed Syntactic Ordered\nMemory (SOM; Fig. 1), which jointly acts as a\nlanguage model and an incremental parser. SOM\ninherits the syntax representation used in Ordered\nMemory (OM; Shen et al. 2019) in which syntax\ntrees are embedded in a grid-like memory repre-\nsentation. Whereas OM was trained as an unsuper-\nvised parser, SOM is explicitly trained to predict\nboth ground-truth syntax trees incrementally and,\nusing the predicted partial syntactic structure, to\npredict the next token. Fig.1 shows the mechanism\nof SOM.\nSOM factorizes the next-token prediction pro-\ncess into two steps: ﬁrst, we predict the attachment\nposition for the next token with a zero-step look-\nahead parser, trained in a supervised fashion; then,\nwe predict the next token distribution conditioned\non the partially predicted structure. One way of\ntraining the incremental parser is to use teacher-\nforcing. However, this can lead to exposure bias,\ndue to the fact that the model was never exposed\nto its own predictions during training. To avoid\n1661\nthis, we introduce a dynamic oracle (Goldberg and\nNivre, 2012) for our model, so that our model can\nlearn to recover from previous parsing mistakes\nduring inference. We found this to be crucial to\nobtain good performance.\nWe compare SOM with existing methods that\nintegrate syntax into language models. RN-\nNGs (Dyer et al., 2016) and Ordered Neu-\nrons (Shen et al., 2018) are particularly related.\nRNNGs are generative models of language which\ndeﬁne a joint distribution on syntactic structures\nand sequence of words. Ordered Neurons attempt\nto model the hierarchical structure of language by\ndeﬁning an ordering to the hidden states and the\ngates that impose that structure. We show that our\nproposed SOM model can achieve strong language\nmodeling, parsing and SG performance even when\ntrained on small amounts of data.\nIn summary, our contributions are threefold:\n•We introduce SOM, a new syntax-augmented\nlanguage model that learns an incremental\nparser and use its predictions to improve lan-\nguage modeling.\n•We propose a novel dynamic oracle that al-\nlows to reduce the exposure bias and is instru-\nmental to achieving good downstream perfor-\nmance.\n•We report high SG score, language modeling\nand incremental parsing performance for var-\nious dataset sizes. We also ﬁnd that jointly\nlearning both language modelling and parsing\nimproves both these capabilities in the model.\n2 Related Work\nSyntax-aware models There has been work to\nintegrate syntax into our current models of lan-\nguage. Socher et al. (2013) used parse trees for\ncomposing sentences in order to predict sentiment\nover movie reviews. However, having an external\nparser and restriction of batched computations in\nthat early model made the method unwieldy. Bow-\nman et al. (2016) introduced the SPINN model,\nwhich alleviated those issues, turning sentences\ninto a sequence of actions to be executed by a shift-\nreduce parser. Our SOM model is based on shift-\nreduce as well, because of the incremental nature\nof the parsing we want to achieve. RNNG (Dyer\net al., 2016; Kuncoro et al., 2016) was an exam-\nple of integrating syntax information for language\nmodelling.\nThere is also work that attempts to learn these\nsyntactic structures without supervision. Kim et al.\n(2019) later devised an unsupervised version of the\nRNNG, a method which produced good parsing\nperformance. DIORA (Drozdov et al., 2019, 2020)\nwas a method that leveraged the Inside-Outside\nalgorithm to construct sentence embeddings for\ndownstream tasks, with the beneﬁt of being able to\nread off parse trees in the encoding process.\nSwayamdipta et al. (2019) ﬁnds that there are\nno improvements over using ELMo (Peters et al.,\n2018) embeddings when shallow syntactic infor-\nmation is included, concluding that ELMo-style\npretraining has learned the syntactic information.\nHowever, Kuncoro et al. (2019) investigated the im-\nportance of the learnt syntactic knowledge RNNG\nin a large pre-trained model like BERT, they found\nthat syntax information helps with downstream\ntasks. In our experiments, we ﬁnd that explicitly\ntraining OM with syntax (with our dynamic ora-\ncle scheme) improves performance on syntactic\ngeneralization tasks.\nIncremental Parsing & Language Modelling\nIn SOM, we speciﬁcally focus on incremental pars-\ning. Ghezzi and Mandrioli (1979) discusses in-\ncremental parsing in the context of programming\nlanguages, with shift-reduce parsers being a spe-\nciﬁc type of incremental parsing. OM, RNNG, and\nSPINN are models that were designed with shift-\nreduce in mind.\nIncremental parsing lends itself well to the task\nof autoregressive language modelling. Since the\nparser only sees the preﬁx of a sentence, the model\ncan use the partial parse to make a prediction about\nupcoming words. Demberg et al. (2013) sum-\nmarises several empirical results that provide ev-\nidence for incremental and predictive parsing in\nhumans, and makes several connections between\nincrementality (that comprehenders do not wait to\nthe end of the sentence before building a represen-\ntation) and prediction about future words coming\nin the sentence.\nGiven that an incremental parser processes a sen-\ntence from left to right, there are naturally some\nlimitations. Hassan et al. (2009) show why either\na beam or delay is necessary if performing incre-\nmental parsing with monotonic extensions: They\nexperiment with a parser based on Combinatory\nCategorial Grammar (Steedman, 2000). They ﬁnd\nthat without the look-ahead, there is a 30 % point\nreduction in the parsing results. One of our con-\n1662\ntributions in this paper is the one-step lookahead\nwhile performing parsing, but zero-step lookahead\nwhen performing next-word prediction, allowing\nthe model to be trained jointly as a incremental\nparser and language model.\nDespite the left-to-right nature of incremental\nparsing, this setting may aid language modelling\ntoo. Shieber (1983) suggests the biases may corre-\nspond to the way humans parse English, and use\na modiﬁed shift-reduce parser to disambiguate be-\ntween different parses of a sentence. There have\nbeen work that show that incremental parsing can\nimprove language modelling. Köhn and Baumann\n(2016) demonstrate that combining an incremental\ndependency parser with a language model yields\nimprovements in perplexity. Roark (2001) presents\na top-down phrase structure parser that performs\nbeam-search to generate connected intermediate\nstructures for every sentence preﬁx. This model\ncan be used for language modeling and beats tri-\ngram models on the Penn Treebank (Marcus et al.,\n1994)\nDynamic Oracles Since incremental parsing re-\nquires that we break down the problem of structure\nprediction into sequential decisions, we are prone\nto exposure bias. There are techniques to address\nthis by allowing the model to make mistakes and\nsupervising future actions based on the state arrived\nat (Daumé et al., 2009). Goldberg and Nivre (2012)\nintroduces the concept of dynamic oracles for de-\npendency parsing. Coavoux and Crabbé (2016)\nuses this technique for incremental constituency\nparsing, but uses morphological features, and does\nnot perform language modelling. Fried and Klein\n(2018) cover in further detail the related work re-\nlating to dynamic oracles and parsing. We ﬁnd\nthat using dynamic oracles for training is crucial\nin seeing beneﬁts in both language modelling and\nincremental parsing.\nEvaluating Syntactic Generalization Recent\ntests have been developed that attempt to probe the\nlinguistic abilities of language models. Gulordava\net al. (2018) explores the extent to which RNNs\nare able to model grammar, independent of the se-\nmantics of the sentence. Marvin and Linzen (2018)\nevaluate language models on their ability to score\nsentences with and without the proper subject-verb\nagreements over a variety of different settings.\nHu et al. (2020) expands on these ideas, and\npropose a suite of syntactic generalization tests\nFigure 2: The grid view of a tree structure. Blue ar-\nrows represent composing children into parent. Gray\narrows represent copying from previous time step. Or-\nange slots are memories generated at the current time\nstep. Gray slots are memories copied from previous\ntime step.\nfor language models over a series of different sized\ndatasets. They ﬁnd that while GPT-2 performs well,\ntheir performance is highly dependent on the scale\nof the language modeling training dataset, while\nother models remain more robust. In this paper, we\nuse this test suite for the evaluation.\n3 Ordered Memory\nWe ﬁrst provide useful background on Ordered\nMemory. Ordered Memory (OM, Shen et al. 2019)\nis a recurrent neural network that explicitly models\nrecursive structure through memory writing and\nerasing operations. OM maps the latent syntax into\na T ×N memory grid ˜M, where T is the length\nof input sequence and N is the maximum number\nof memory slots. Figure 2 gives an intuition of\nwhat the grid contains. Empty blocks in the ﬁgure\nrepresent memory slots that can be discarded dur-\ning inference. Ideally, the memory network should\ngenerate the t-th column of the grid ˜Mt at time step\nt. But generating ˜Mt requires the model to have ac-\ncess about the tree structure which is usually latent.\nFor this reason, OM induces the latent structure\nthrough inductive biases of its reading and writing\noperations.\nAs a recurrent model, OM performs one-step\nlook-ahead incremental parsing through maintain-\ning three states:\n•Memory Mt: a matrix of dimension N ×D,\nwhere each occupied slot is a distributed repre-\nsentation for a node spanning an subsequence\nin x1,..,x t−1 conditioned on xt, i.e. Mt repre-\nsents a one-step look-ahead parser stack. It’s\nrepresented by gray blocks in Figure 3.\n1663\n(a) The transition from time step 4 to 5. 1 The one-step look-\nahead parser combines ˆMt−1 and Mt−1 considering on the\ncurrent input xt, in this example, the split point of ˆMt−1 and\nMt−1 is i = 2. 2 Current input xt is written into the lower\nslot of new candidate memory ˆMi−1\nt . 3 The rest of new can-\ndidate memories ˆM≥i\nt are generated with bottom-up recurrent\ncomposition.\n(b) Predicting the next token at time step 4. 1 The zero-\nstep look-ahead parser combines Mt and ˆMt at time step\nt. 2 The recurrent network takes the combined memory\nMout\nt as input and output a hidden stateht = f(w≤t). 3\nht is then fed into an linear layer to computep(xt+1|x≤t).\nFigure 3: The recurrent transition (left) and prediction network (right) of SOM. In the recurrent transition, a one-\nstep look-ahead parser predict the syntax once e is observed and can be seen as a posterior over the syntax given\nthe current word. The prediction network uses a zero-step look-ahead parser to predict the location of the next\nphrase and acts as a prior on the syntactic structure.\n•Candidate memory ˆMt: a matrix of dimen-\nsion N ×Dcontains representations for all\npossible new nodes at time step t. At next\ntime step t+ 1, the model will decide whether\nor not to write these candidates into memory\nMt+1 conditioned on xt+1. They are repre-\nsented by orange blocks in Figure 3. if the\nmodel is making correct parsing decisions,\nthen Mt = ˜Mt−1.\n•Memory mask − →πt: − →πt ∈{0,1}N , where\neach entry indicates whether the respective\nslot in ˆMt is occupied by a candidate, e.g., if\n− →πt = (0 ,1,1), then the occupied slots are\nˆM≥2\nt . At next time step, the model can only\nchoose a candidate from masked slots to write\ninto the memory Mt+1.\nAt each time step, the model takes\n[Mt−1, ˆMt−1,− →πt−1] and word embedding\nxt as inputs, returning the outputs [Mt, ˆMt,− →πt].\nTo generate the new memory Mt, we combine\nMt−1 and ˆMt−1 to match ˜Mt−1. The model uses\nxt as its query to attend on previous candidates\nˆMt−1. The attention distribution is pt, which mod-\nels the split point of gray blocks and orange blocks\nin Figure 2. Suppose pt is a one-hot distribution\nand pi\nt = 1. The candidates ˆM≤i\nt−1 are written into\nthe respective memory slot M≤i\nt , while M>i\nt−1 are\ncopied to M>i\nt :\nM≤i\nt = ˆM≤i\nt−1, M >i\nt = M>i\nt−1 (1)\nWe will refer to the process of generating Mt as\na one-step look-ahead parser, since the model is\nusing the current input xt as extra information to\nbuild the partial parse for time step t−1. To gener-\nate new candidates ˆMt, the input embedding xt is\nwritten into ˆMi−1\nt , and ˆM≥i\nt are computed recur-\nrently with eq.3:\nˆM<i−1\nt = ∅, ˆMi−1\nt = xt (2)\nˆMj\nt = cell(Mj\nt , ˆMj−1\nt ), ∀j ≥i (3)\nwhere cell() is the composition function that takes\nits childrens’ representations as input and output\nthe parent’s representation. The non-empty slots in\ncandidate memory are then ˆM≥i−1\nt , and they can\nbe masked by:\n− →π<i−1\nt = 0, − →π≥i−1\nt = 1 (4)\nIn other words, − →πi\nt = ∑\nj≤i+1 pj\nt , and − →πi\nt is mono-\ntonically increasing. More details of the OM can\nbe found in Shen et al. (2019).\n4 Syntactic Ordered Memory\nWe propose two augmentations to OM in order to\nbetter perform language modelling and incremental\nparsing: a prediction network and the dynamic ora-\ncle. a) Previous language models mostly focus on\npredicting the next token or a missing token. In our\ncase, we are explicitly modeling the latent struc-\nture. By predicting the structure for the next token,\nwe exploit this latent structure for word prediction.\n1664\nThis helps the model better organize information\nfor predicting next word, allowing shortcuts to be\ncreated for long-term dependencies, as shown in\nFig.1. b) If the model only observes states result-\ning from correct past decisions at training time, it\nwill not be prepared to recover from its own mis-\ntakes during prediction, suffering from exposure\nbias (Schmidt, 2019; Fried and Klein, 2018). In\nthe experiment section, we demonstrate how this\nphenomenon will signiﬁcantly hurt the language\nmodel performance and, to a lesser extent, also hurt\nthe parsing performance.\n4.1 Prediction Network\nAt time step t, the prediction network takes\n[Mt, ˆMt,− →πt] as input, and produces a probabil-\nity distribution over the next token p(wt+1|w≤t).\nTo do this, we need to have a temporary estimate\nof the local structure. We therefore need to approx-\nimate pt+1 with a zero-step look-ahead prediction\np′\nt:\nαi\nt =\nwAtt\n2 ReLU\n(\nWAtt\n1 ˆMi\nt + b1\n)\n+ b2\n√\nN\n(5)\np′\nt = masked_softmax(αt,mask = − →πt) (6)\nwhere WAtt\n1 is N ×N weight matrix, wAtt\n2 is\na N dimension weight vector, and αi\nt is a scalar.\nWe then sample the slot at index i from the dis-\ntribution p′\nt. i is the zero-step look-ahead pars-\ning decision, which means that the next phrase\nwill be a sibling of node ˆMi\nt . We therefore need\nto predict the next token conditioned on ˆMi\nt and\nits previous contexts. So we feed memory slots\n[MN\nt ,MN−1\nt ,...,M i+1\nt , ˆMi\nt ] into a recurrent neu-\nral network:\nht = RNN\n(\nMN\nt ,MN−1\nt ,...,M i+1\nt , ˆMi\nt\n)\n(7)\nwhere ht is the ﬁnal hidden state of the RNN. As\nshown in Figure 3b, the input sequence are repre-\nsentations of non-overlapping subtrees spanning\nfrom x1 to xt. ht can therefore be seen as a dis-\ntributed representation of the sequence w≤t. In\nthe RNN, we use the same architecture as the cell\nfunction in OM to model the recurrent transition\nfunction:\n\n\nfj\nij\ncj\nuj\n\n= WCell\n2 ReLU\n(\nWCell\n1\n[\nhj+1\nt\nMj\n]\n+ b1\n)\n+ b2\n(8)\nhj\nt = LN(σ(fj) ⊙hj+1\nt + σ(ij) ⊙Mj + σ(cj) ⊙uj)\n(9)\nwhere σ is the sigmoid function, LN is layer nor-\nmalization function, fj,ij,cj are controlling gates,\ncj is cell state, and hN+1\nt is a zero vector. After\nobtaining ht, we can compute the distribution over\nthe next token and the language modelling loss:\np(wt+1|w≤t) = softmax(Wembht + b) (10)\nLLM = −\n∑\nt\nlog(p(wt+1|w≤t)) (11)\n4.2 Dynamic Oracle for SOM\nData: θ1,...,θ T , Γ\nResult: ξ1,...,ξ T\ninitialize ξ1 = N;\nfor i←2 to T do\nj = first_siblingΓ(i);\nµi = max(θj+1,...,θ i−1);\nξi = max(ξj −1,µi);\nend\nAlgorithm 1: The structure label generation algo-\nrithm, where Γ is the ground-truth tree andθi is the\nstructural decisions made by our model. This algo-\nrithm produces a parse close to the original given\nthe errors already made, and that new gold parse is\nconverted into grid decisions. Given Γ, the func-\ntion first_siblingΓ(i) returns the index of the\nﬁrst token in the smallest clause that contains wi,\nand where wi is not the ﬁrst token. Ideally, wi\nshould be written into the slot (ξj −1). For ex-\nample, in Figure 2, cis written into the slot 2, then\nd,e should be written into the slot 1. However, the\nmodel could make a wrong decision between wj\nand wi. If the model has merged information from\nwj into a higher slot µi, xi should be written into\nslot µi as well.\nOne way to provide a supervision signal for pt\nand p′\nt is to train the parser with static oracle: feed\nthe gold tree to the model, and have the model pre-\ndict future decisions. However, static oracle makes\nthe language model overﬁt on the gold tree, result-\ning in bad perplexity scores (Table 2). Inspired\n1665\nType Max Median Mean\nConstituency 29 7 7.7\nDependency 16 4 4.2\nTable 1: Statistics of tree depth for Penn Treebank. De-\npendency trees are converted from constituency tree\nwith Stanford Corenlp toolkit.\nFigure 4: The universal dependency tree is converted\ninto a constituency treeΓ through merging the head and\nits children into one single constituent. Since the grid\nview only works with binary trees, we binarize n-ary\nnodes with a left branching bias.\nby the dynamic oracles proposed in (Goldberg and\nNivre, 2012; Coavoux and Crabbé, 2016), we pro-\npose a dynamic oracle for ordered memory, which\ndynamically changes the reference structure based\non mistakes made by our model on previous steps.\nTo do this, we build the structure label for each time\nstep based on the gold tree and previous decisions\nmade by the model. During training, we sample\nthe model’s decision frompt:\nθt = Multinomial(pt) (12)\nand we make greedy decisions during evaluation:\nθt = argmax(pt) (13)\nThe same operations are applied to p′\nt as well.\nWe use the Algorithm.1 to convert the gold tree\nΓ into labels ξt for pt. Since the zero-step look-\nahead distribution p′\nt should match the one-step\nlook-ahead distribution pt+1 at next time step t+ 1,\nwe use ξt+1 as label for p′\nt. The structure loss is\nthe negative log-likelihood:\nLS = −\n∑\nt\n(\nlog(pt(ξt|w≤t)) + log(p′\nt(ξt+1|w≤t))\n)\nFor our model, the depth of Γ has a linear re-\nlation to the computational complexity and GPU\nmemory consumption. To maximize the model’s\nefﬁciently, the gold tree Γ is constructed from uni-\nversal dependency trees.1 There are two reasons\n1https://universaldependencies.org/\nwe chose universal dependency trees instead of\nconstituency trees: 1) In Table 1, the dependency\ntrees are on average shallower than constituency\ntrees; this means faster computation time and less\nmemory consumption for our model. 2) Univer-\nsal dependency trees can be applied to many more\nlanguages than Penn Treebank-style constituency\ngrammar. Additionally, Penn Treebank-style trees\ncan easily be converted to universal dependency\ntrees. As shown in Figure 4, we convert the uni-\nversal dependency tree into Γ by merging the head\nand its children into one single constituent.\n5 Experiments\nWe present the results of SOM on language model-\ning, syntactic generalization, and incremental pars-\ning. Details of hyperparameters and experiment\nsettings can be found in Appendix B.\n5.1 Language Modeling\nPenn Treebankhas one million words of 1989\nWall Street Journal corpus annotated with con-\nstituency trees. Since SOM primarily focuses on\nsentence-level structure and language modeling,\nwe use the same preprocessing schema as RNNG2\n(Dyer et al., 2016). Sentences are modeled sepa-\nrately, punctuation is retained, and singleton words\nare replaced with the Berkeley parser’s mapping\nrules3, resulting in 23,815-word types. Ortho-\ngraphic case distinction is preserved, and numbers\n(beyond singletons) are not normalized.\nBLLIP is a large Penn Treebank-style parsed cor-\npus of approximately 24 million sentences. We\ntrain and evaluate SOM on three splits of BLLIP:\nBLLIP-XS (40k sentences, 1M tokens), BLLIP-\nSM (200K sentences, 5M tokens), and BLLIP-MD\n(600K sentences, 14M tokens). They are obtained\nby randomly sampling sections from BLLIP 1987-\n89 Corpus Release 1. All models are tested on a\nshared held-out tested set.\nFollowing the settings provided in (Hu et al.,\n2020), datasets are preprocessed into two differ-\nent versions. The ﬁrst setting is similar to the\nPTB dataset. Singleton words are mapped to UNK\nclasses that preserve ﬁne-grained information, such\nas orthographic case distinctions and morpholog-\nical sufﬁxes (e.g. UNK-ed, UNK-ly). The sec-\nond setting use subword-level vocabulary extracted\n22-21 for training, 24 for validation, 23 for evaluation.\n3http://github.com/slavpetrov/\nberkeleyparser\n1666\nModel # parameters ppl pacc UF1 p′acc\nSOM 17.7M 77.68 0.927 87.96 0.870\nSOM −Prediction network 13.0M 83.63 0.923 87.09 –\nSOM −Prediction network −Language Modeling Loss 13.0M – 0.925 86.26 –\nSOM −Dynamic Oracle + Static Oracle 17.7M 129.27 0.913 86.58 0.849\nSOM −Dynamic Oracle + Left-branching Oracle 17.7M 82.01 – – –\nInference with External Trees\nSOM −Predicted tree + Gold tree 17.7M 60.87 0.947 100.00 0.884\nTable 2: Ablation tests on the PTB dataset. “ p acc” and “p′ acc” are the prediction accuracies of the one-step\nlook-ahead and zero-step look-ahead parsers respectively. “UF1” is the parsing performance with respect to the\nconverted constituency tree Γ. “ −Prediction network”: this model uses the last candidate memory slot ˆMN\nt\nto predict the next token, instead of using the ht from the prediction network. “ −Predicted tree + Gold tree”:\nthe model’s parsing decisions were replaced with ground truth decisions; these results can be considered as the\nperformance upper bound of SOM.\nModel PTB\nWithout annotations\nRNNLM 93.2\nPRPN (Shen et al., 2017) 96.7\nURNNG (Kim et al., 2019) 90.6\nWith annotations\nRNNG (Dyer et al., 2016) 88.7\nRNNG →URNNG (Kim et al., 2019) 85.9\nSOM 77.7\nTable 3: Perplexities on Penn Treebank datasets. With\nannotations are models that use the gold tree as super-\nvision signal during training. Baseline results are from\nKim et al. (2019)\n.\nModel XS SM MD\nn-gram 240.21 157.60 106.09\nRNNG 122.46 86.72 69.57\nLSTM 98.19 65.52 59.05\nON-LSTM 71.76 54.00 56.37\nGPT-2 529.90* 183.10* 37.04*\nSOM 70.41 51.47 31.95*\nTable 4: Perplexities on BLLIP datasets achieved by\ndifferent models. Perplexity scores across training\ndataset sizes are not strictly comparable for models that\nuse word-level vocabulary. * results are using GPT-2’s\nsubword vocabulary.\nfrom the GPT-2 pretrained model rather than the\nBLLIP training corpora.\nResults of language modeling are given in Ta-\nble 3 and Table 4. SOM consistently outper-\nforms both the annotated model and non-annotated\nmodels. While GPT-2 seems to fail to learn on\nsmaller datasets, SOM still outperforms GPT-2 on\nthe BLLIP-MD dataset with far fewer parameters\n(34.8M vs 124.4M), and achieves comparable re-\nsults with the GPT-2 that is trained on a 3 times\nlarger dataset BLLIP-LG (Hu et al., 2020).\nThe ablation test results are shown in Table 2.\nThe biggest performance drop comes from replac-\ning the dynamic oracle with static oracle. We be-\nlieve that this is due to the model overﬁtting on\nthe gold tree, and suffering from exposure bias as\na result. Another big performance drop happens\nafter removing the prediction network. This sug-\ngests that predicting the attaching nodes of the next\nphrase with the zero-step look-ahead parsers helps\nto predict the next token. Replacing the gold tree\nlabels with trivial left-branching tree labels also\nhurts the perplexity. This suggests that learning\nsyntactic structure helps language modeling.\n5.2 Syntactic Generalization\nSyntactic Generalization (SG) test suites evaluate\nthe syntactic knowledge of neural language mod-\nels. Hu et al. (2020) proposed a set of 34 test\nsuites to evaluation 6 different aspects of syntax: 1)\nagreement, 2) licensing, 3) garden-path effects, 4)\ngross syntactic expectation, 5) center embedding,\n6) long-distance dependencies.\nFollowing their settings, we evaluate our lan-\nguage models trained on the BLLIP datasets. Lan-\nguage models are presented with a group of sen-\ntences with minor differences. To pass each test,\nthe model needs to assign higher conditional prob-\nabilities to designated phrases in the sentence that\nare more grammatical.\nFigure 6 shows the average accuracy over all\nmodel on the complete set of SG test suites. SOM\nachieves the best average accuracy, outperforms\nmodels with hierarchical structure bias (RNNG,\nON-LSTM), and transformer-based model (GPT-\n2). However, according to Figure 8a in Appendix\nC.1, GPT-2 trained on BLLIP-LG and BLLIP-MD\nstill outperform SOM. This could due to that the\nnumber of parameters in SOM is largely falling\nbehind GPT-2.\n1667\nAgreement\nCenter Embedding Garden-Path Effects Gross Syntactic State\nLicensing\nLong-Distance Dependencies\nCircuit\n0.0\n0.5\n1.0SG score\nSOM\nGPT-2\nRNNG\nON-LSTM\nLSTM\nn-gram\nFigure 5: Evaluation results on all models, split across test suite circuits.\nSOM GPT-2 RNNG ON-LSTM LSTM n-gram\nModel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSG score\nFigure 6: Average SG accuracy by model class.\nFigure 5 provides ﬁne-grained results on six SG\nclasses. SOM achieves strong performance on li-\ncensing, gross syntactic state, center embedding,\nand long-distance embeddings. These classes re-\nquire the model to keep track of syntactic features\nacross large syntactic chunks (e.g., relative or sub-\nordination clauses). SOM can effectively keep\nthis long-term information in higher-level memory\nslots, and revisit the information after the clause in\nthe middle is ended. More detailed results can be\nfound in Appendix C.1.\n5.3 Incremental Parsing\nModel UF1\nPRPN* 41.2\nONLSTM* 47.7\nONLSTM-SYD (Du et al., 2020) 61.3\nIncremental Shift-reduce Parser 56.82\nShift-reduce + LM + Dynamic Oracle 58.04\nSOM 67.27\nOracle Binary Trees 82.5\nTable 5: Incremental parsing results on the standard\nPTB constituency trees. “*” means that the model is\ndoing unsupervised grammar induction. Since we com-\npare UF1 against the standard, nonbinarized trees (per\nconvention), UF1 scores is upper bounded by the oracle\nbinary trees score.\nTo evaluate SOM’s performance on incremental\nparsing, we trained and evaluated our models on\nthe standard PTB constituency trees. Baseline mod-\nels include: a) a standard incremental shift-reduce\nparser with one-step look-ahead; b) a incremental\nshift-reduce parser that equipped with our predic-\ntion network and trained on same dynamic oracle\nand language model loss as our model; c) a re-\ncently proposed ONLSTM-SYD model (Du et al.,\n2020) that is also trained on both language model\nand parsing loss; d) unsupervised ONLSTM; e)\nunsupervised PRPN. As shown in Table 5, SOMs\noutperform all baseline models, including the shift-\nreduce parser that has the same extra components\nas SOMs. For language modelling performance,\noriginal constituency tree based models achieve\nsimilar perplexity as dependency tree based coun-\nterparts. But constituency tree based models re-\nquire 2×GPU time and memory to train and eval-\nuate.\nFor ablation test, we also compare parsing results\ngiven by SOM with binary constituency trees Γ\nconverted from universal dependency trees.4 These\nresults are shown in Table 2. We observe that using\nstatic oracle instead of dynamic oracle results in the\nworst parsing performance. This suggests that our\ndynamic oracle helps the model to learn a better\nparser. After removing the language model loss,\nthe UF1 drops 1.7 points. This suggests that the\nlanguage model loss helps the model to learn better\nrepresentations for syntax.\n6 Conclusion\nIn this work, we propose a new language model\nwith an integrated incremental parser. This was\ndone by augmenting the Ordered Memory model\nwith a prediction network, and by using a dynamic\noracle for training it to perform incremental parsing.\nThe resulting model models the joint distribution\nof syntactic structure and sequence words. We\nﬁnd that by using the dynamic oracle and explic-\nitly modeling the syntax, we can achieve strong\nperformance on language modelling and syntactic\ngeneralization and both these techniques are crucial\nin the model’s performance.\n4UF1 scores are computed by EV ALB https://nlp.\ncs.nyu.edu/evalb/\n1668\nReferences\nSamuel R Bowman, Jon Gauthier, Abhinav Rastogi,\nRaghav Gupta, Christopher D Manning, and Christo-\npher Potts. 2016. A fast uniﬁed model for pars-\ning and sentence understanding. arXiv preprint\narXiv:1603.06021.\nMaximin Coavoux and Benoit Crabbé. 2016. Neural\ngreedy constituent parsing with dynamic oracles.\nHal Daumé, John Langford, and Daniel Marcu. 2009.\nSearch-based structured prediction. Machine learn-\ning, 75(3):297–325.\nVera Demberg, Frank Keller, and Alexander Koller.\n2013. Incremental, predictive parsing with psy-\ncholinguistically motivated tree-adjoining grammar.\nComputational Linguistics, 39(4):1025–1066.\nAndrew Drozdov, Subendhu Rongali, Yi-Pei Chen,\nTim O’Gorman, Mohit Iyyer, and Andrew McCal-\nlum. 2020. Unsupervised parsing with s-diora: Sin-\ngle tree encoding for deep inside-outside recursive\nautoencoders. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4832–4845.\nAndrew Drozdov, Pat Verga, Mohit Yadav, Mohit Iyyer,\nand Andrew McCallum. 2019. Unsupervised latent\ntree induction with deep inside-outside recursive au-\ntoencoders. arXiv preprint arXiv:1904.02142.\nWenyu Du, Zhouhan Lin, Yikang Shen, Timothy J\nO’Donnell, Yoshua Bengio, and Yue Zhang. 2020.\nExploiting syntactic structure for better language\nmodeling: A syntactic distance approach. arXiv\npreprint arXiv:2005.05864.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. arXiv preprint arXiv:1602.07776.\nDaniel Fried and Dan Klein. 2018. Policy gradient as\na proxy for dynamic oracles in constituency parsing.\narXiv preprint arXiv:1806.03290.\nCarlo Ghezzi and Dino Mandrioli. 1979. Incremental\nparsing. ACM Transactions on Programming Lan-\nguages and Systems (TOPLAS), 1(1):58–70.\nYoav Goldberg and Joakim Nivre. 2012. A dynamic or-\nacle for arc-eager dependency parsing. In Proceed-\nings of COLING 2012, pages 959–976.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of NAACL-HLT, pages 1195–1205.\nHany Hassan, Khalil Sima’an, and Andy Way. 2009.\nLexicalized semi-incremental dependency parsing.\nIn Proceedings of the International Conference\nRANLP-2009, pages 128–134.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger P Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. arXiv preprint arXiv:2005.03692.\nYoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kun-\ncoro, Chris Dyer, and Gábor Melis. 2019. Unsu-\npervised recurrent neural network grammars. arXiv\npreprint arXiv:1904.03746.\nArne Köhn and Timo Baumann. 2016. Predictive in-\ncremental parsing helps language modeling.\nAdhiguna Kuncoro, Miguel Ballesteros, Lingpeng\nKong, Chris Dyer, Graham Neubig, and Noah A\nSmith. 2016. What do recurrent neural network\ngrammars learn about syntax? arXiv preprint\narXiv:1611.05774.\nAdhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen\nClark, and Phil Blunsom. 2019. Scalable syntax-\naware language models using knowledge distillation.\narXiv preprint arXiv:1906.06438.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nMitchell Marcus, Grace Kim, Mary Ann\nMarcinkiewicz, Robert MacIntyre, Ann Bies,\nMark Ferguson, Karen Katz, and Britta Schasberger.\n1994. The penn treebank: annotating predicate\nargument structure. In Proceedings of the workshop\non Human Language Technology, pages 114–119.\nAssociation for Computational Linguistics.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. arXiv preprint\narXiv:1808.09031.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nBrian Roark. 2001. Probabilistic top-down parsing\nand language modeling. Computational linguistics,\n27(2):249–276.\nFlorian Schmidt. 2019. Generalization in generation:\nA closer look at exposure bias. arXiv preprint\narXiv:1910.00292.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and\nAaron Courville. 2017. Neural language model-\ning by jointly learning syntax and lexicon. arXiv\npreprint arXiv:1711.02013.\nYikang Shen, Shawn Tan, Arian Hosseini, Zhouhan\nLin, Alessandro Sordoni, and Aaron C Courville.\n2019. Ordered memory. In Advances in Neural In-\nformation Processing Systems, pages 5038–5049.\n1669\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2018. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks.\narXiv preprint arXiv:1810.09536.\nStuart M Shieber. 1983. Sentence disambiguation by\na shift-reduce parsing technique. In Proceedings of\nthe 21st annual meeting on Association for Compu-\ntational Linguistics, pages 113–118. Association for\nComputational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nMark Steedman. 2000. The syntactic process, vol-\nume 24. MIT press Cambridge, MA.\nSwabha Swayamdipta, Matthew Peters, Brendan Roof,\nChris Dyer, and Noah A Smith. 2019. Shallow syn-\ntax in deep water. arXiv preprint arXiv:1908.11047.\n1670\nA Disentangling Semantic and Syntactic\nrepresentations\nGiven the architecture of our model, we can easily\ndisentangle the language model information ﬂow\nand parsing information ﬂow. Figure 7 illustrates\nthe disentangled information and gradient ﬂows in\nour model. The language model depends on both\nprior context and structural inputs, and derivatives\nare computed with respect to both of these inputs\nand backpropagated. However, while the structure\nalso depends on both inputs, we limit backprop-\nagation so that it can only update with respect to\nthe syntactic input. This is because we want the\nparsing component to function independently of\nthe language modelling component, but still lever-\nage the semantic information to deal with syntactic\nambiguity.\nFigure 7: The schema of disentangling syntax from the\nlanguage model. Solid lines represent dependency dur-\ning inference, and gradients ﬂow back during backprop-\nagation. The dashed line represents the dependency\nduring inference, but detached so that the gradients do\nnot ﬂow back during backpropagation.\nIt is possible that existing model architectures\ncould implicitly learn to split these representations,\neven without the explicit disentanglement that we\nproposed here. Yet, Table 2 shows that entangled\nmodel can actually achieve stronger in-domain per-\nformance, thanks to the liberty to allocate capacity\nto the two different functionalities based on the\ntraining set.\nTo do so, we propose splitting word embeddings,\nmemory slots, and intermediate hidden states into\ntwo segments: semantic segment and syntactic seg-\nment. We then replace linear layers in our cell\nfunctions with the following function:\n[ysem\nysyn\n]\n=\n[Wsem2sem Wsyn2sem\n0 Wsyn2syn\n][xsem\nxsyn\n]\nwhere ysem and xsem are the semantic segment\nwhich is optimized to minimize language model-\ning loss, ysyn and xsyn are the syntactic segment\nwhich is optimized to minimize both parsing and\nlanguage modeling loss. This architecture results\nin the solid lines in Figure 7. Additionally, layer\nnormalization functions are replaced with two sep-\narate functions for the two segments respectively.\nMeanwhile, pt still depends on both semantic and\nsyntactic segment, but the structural loss does not\nbackpropagate into the semantic segment:\npt = f(xt,sem,xt,syn, ˆMt,sem, ˆMt,syn) (14)\n∂pt\n∂xt,sem\n= 0, ∂pt\n∂ ˆMt,sem\n= 0 (15)\nand the same for p′\nt:\np′\nt = f( ˆMt,sem, ˆMt,syn) (16)\n∂p′\nt\n∂ ˆMt,sem\n= 0 (17)\nThis gradient detachment is represented by the dash\nline in Figure 7. In the experiment section, the\ndisentangled models are denoted as dSOM, and\nentangled models are denoted as SOM. For dSOM,\nthe dimension of semantic and syntactic segments\nfor memory slots are denoted asDsem and Dsyn re-\nspectively. Among the proposed models, the eSOM\nhas the best performance on the in-domain test sets.\nAppendix C.2 shows that the dSOM slightly outper-\nforms eSOM in perplexity on out-of-domain test\nsets.\nB Hyperparameters\nModel XS SM MD\nRNNG 22.8M 48.4M 81.1M\nLSTM 13.4M 30.5M 52.2M\nONLSTM+AWD 30.8M 44.2M 61.2M\nGPT-2 124.4M 124.4M 124.4M\ndSOM 16.4M 39.5M 34.8M\neSOM 17.8M 41.4M 37.9M\nTable 6: Parameter counts for different models\nDropout is applied before all linear layers in\nour model. They all share the same dropout rate,\nexcept the dropout before language model output\nlayer has a different rate. We also applied embed-\nding dropout which randomly set some embedding\nvectors to 0. Hyperparameters are chosen based on\nthe perplexity on validation set.\nC More Experiment Results\nC.1 Syntactic Generalization results\nC.2 Out of Domain Evaluation\nOut-of-domain Test set contains testsets from\nother English universal dependencies treebanks. It\ncontains corpora of different genres, including aca-\ndemic, email, blog, ﬁction, legal, news, etc. We\nuse these datasets to test the generalization ability\nof models that are trained on PTB.\n1671\nDataset Dsem Dsyn #slots embedding dropout dropout output dropout\nPTB 300 100 15 0.1 0.3 0.5\nBLLIP-XS 300 100 15 0.1 0.3 0.5\nBLLIP-SM 400 100 15 0.1 0.2 0.2\nBLLIP-MD-BPE 400 100 15 0 0.1 0.1\nTable 7: Hyperparameters. The hidden size of eSOM models are always the sum of Dsem and Dsyn\n0 100 200 300\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8SG score\n520 540\nTest perplexity\nRandomRandom\nBLLIP-LG\nLSTM\nBLLIP-MD\nON-LSTM\nBLLIP-SM\nRNNG\nBLLIP-XS\nGPT-2 SOM\n(a) Relationship between SG socre and perplexity on the\nheld-out BLLIP test set.\nSOM GPT-2 RNNG ON-LSTM LSTM n-gram\nModel\n0.0\n0.2\n0.4\n0.6\n0.8SG score\nNo modifier\nWith modifier\n(b) SG score on the pairs of test suites with and without\nintervening modiﬁers: Center Embedding, Cleft, MVRR,\nNPZ-Ambiguous, and NPZ-Object.\nFigure 8\nLSTM ON-LSTM RNNG GPT-2 n-gram SOM\nModel\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nSG score delta\nBLLIP-LG BLLIP-MD BLLIP-SM BLLIP-XS\nCorpus\nLSTM\nON-LSTM\nRNNG GPT-2 n-gram SOM\nFigure 9: Left: Model class has a trong effect on SG scores. Right: Data scale has little effect on SG scores\nDataset GUM EWT ParTUT LinES Pronouns PUD SG\nMetric ppl UF1 ppl UF1 ppl UF1 ppl UF1 ppl UF1 ppl UF1 acc\neSOM 351.5 67.0 400.7 73.1 282.6 76.5 253.3 67.2 552.1 87.1 281.0 75.6 0.614\ndSOM 350.3 66.4 403.0 72.3 269.8 74.3 252.1 66.6 565.8 87.3 280.3 75.1 0.581\nLB 375.5 – 436.5 – 300.9 – 267.5 – 620.4 – 300.7 – 0.513\nTable 8: Out of domain test results. Models are trained on PTB. The test sets are obtained from English universal\ndependencies treebank. “LB” stands for left-branching tree labels. Thanks to the structure information, our models\ngeneralize much better then the left-branching baseline.\n1672\nAgreement\nCenter EmbeddingGarden-Path EffectsGross Syntactic State\nLicensing\nLong-Distance Dependencies\nCircuit\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\nSG score delta\nLSTM\nON-LSTM\nRNNG GPT-2 n-gram SOM\nAgreement\nCenter EmbeddingGarden-Path EffectsGross Syntactic State\nLicensing\nLong-Distance Dependencies\nCircuit\nBLLIP-LG BLLIP-MD BLLIP-SM BLLIP-XS\nFigure 10: Left: differences in model class induce signiﬁcant differences in SG scores for several circuits. Right:\ndifferences in training data size do not reliably account for most of circuits.",
  "topic": "Parsing",
  "concepts": [
    {
      "name": "Parsing",
      "score": 0.7910213470458984
    },
    {
      "name": "Computer science",
      "score": 0.7547533512115479
    },
    {
      "name": "Oracle",
      "score": 0.6955695152282715
    },
    {
      "name": "Syntax",
      "score": 0.6482658386230469
    },
    {
      "name": "Programming language",
      "score": 0.5612281560897827
    },
    {
      "name": "Linguistics",
      "score": 0.5424244403839111
    },
    {
      "name": "Natural language processing",
      "score": 0.5413927435874939
    },
    {
      "name": "Computational linguistics",
      "score": 0.5193130373954773
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48479074239730835
    },
    {
      "name": "Philosophy",
      "score": 0.15128269791603088
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210155582",
      "name": "Centre Universitaire de Mila",
      "country": "DZ"
    },
    {
      "id": "https://openalex.org/I4402554038",
      "name": "Microsoft Research Montréal (Canada)",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    }
  ]
}