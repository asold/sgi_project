{
  "title": "Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and Language Models",
  "url": "https://openalex.org/W2512196305",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A344446316",
      "name": "Thomas Drugman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1066198396",
      "name": "Janne Pylkkönen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A200456398",
      "name": "Reinhard Kneser",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1797288984",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2159391028",
    "https://openalex.org/W2127499922",
    "https://openalex.org/W2095629250",
    "https://openalex.org/W2135241496",
    "https://openalex.org/W2027167280",
    "https://openalex.org/W2295262519",
    "https://openalex.org/W2151023586",
    "https://openalex.org/W2404463488",
    "https://openalex.org/W2034537249",
    "https://openalex.org/W2294722231",
    "https://openalex.org/W2189391786",
    "https://openalex.org/W1978660892",
    "https://openalex.org/W1510761126",
    "https://openalex.org/W2294962864",
    "https://openalex.org/W2184045248"
  ],
  "abstract": "The goal of this paper is to simulate the benefits of jointly applying active learning (AL) and semi-supervised training (SST) in a new speech recognition application. Our data selection approach relies on confidence filtering, and its impact on both the acoustic and language models (AM and LM) is studied. While AL is known to be beneficial to AM training, we show that it also carries out substantial improvements to the LM when combined with SST. Sophisticated confidence models, on the other hand, did not prove to yield any data selection gain. Our results indicate that, while SST is crucial at the beginning of the labeling process, its gains degrade rapidly as AL is set in place. The final simulation reports that AL allows a transcription cost reduction of about 70% over random selection. Alternatively, for a fixed transcription budget, the proposed approach improves the word error rate by about 12.5% relative.",
  "full_text": "Active and Semi-Supervised Learning in ASR:\nBeneﬁts on the Acoustic and Language Models\nThomas Drugman, Janne Pylkk¨onen, Reinhard Kneser\nAmazon\ndrugman@amazon.com, jannepyl@amazon.com, rkneser@amazon.com\nAbstract\nThe goal of this paper is to simulate the beneﬁts of jointly ap-\nplying active learning (AL) and semi-supervised training (SST)\nin a new speech recognition application. Our data selection ap-\nproach relies on conﬁdence ﬁltering, and its impact on both the\nacoustic and language models (AM and LM) is studied. While\nAL is known to be beneﬁcial to AM training, we show that it\nalso carries out substantial improvements to the LM when com-\nbined with SST. Sophisticated conﬁdence models, on the other\nhand, did not prove to yield any data selection gain. Our results\nindicate that, while SST is crucial at the beginning of the label-\ning process, its gains degrade rapidly as AL is set in place. The\nﬁnal simulation reports that AL allows a transcription cost re-\nduction of about 70% over random selection. Alternatively, for\na ﬁxed transcription budget, the proposed approach improves\nthe word error rate by about 12.5% relative.\nIndex Terms : speech recognition, active learning, semi-\nsupervised training, data selection\n1. Introduction\nThis paper aims at the problem of identifying the best approach\nfor jointly selecting the data to be labelled, and maximally lever-\naging the data left as unsupervised. Our application targets\nvoice search as used in various Amazon products. Because\nspeech data transcription is a time-consuming and hence costly\nprocess, it is crucial to ﬁnd an optimal strategy to select the data\nto be transcribed via active learning. In addition, the unselected\ndata might also be helpful in improving the performance of the\nASR system by semi-supervised training. As will be shown in\nthis paper, such an approach allows to reduce the transcription\ncost dramatically while enhancing the customer’s experience.\nActive Learning (AL) refers to the task of minimizing the\nnumber of training samples to be labeled by a human so as\nto achieve a given system performance [1]. Unlabeled data\nis processed and the most informative examples with respect\nto a given cost function are then selected for human labeling.\nAL has been addressed for ASR purpose across various studies,\nwhich mainly differ by the measure of informativeness used for\ndata selection. First attempts were based on so-called conﬁ-\ndence scores [2] and on a global entropy reduction maximiza-\ntion criterion [3]. In [4], a committee-based approach was de-\nscribed. In [5], a min-max framework for selecting utterances\nconsidering both informativeness and representativeness crite-\nria was proposed. This method was used in [6] together with\nan N-best entropy based data selection. Finally, the study in [7]\nfound that HMM-state entropy and letter density are good in-\ndicators of the utterance informativeness. Encouraging results\nwere reported from the early attempts [2, 3] with a 60% reduc-\ntion of the transcription cost over Random Selection (RS).\nIn this paper, we focus on conventional conﬁdence-based\nAL as suggested in [2], although other studies [3, 6, 7] have\nshown some improvement over it. It is however worth high-\nlighting that the details of the baseline conﬁdence-based ap-\nproach were not always clearly described, and that subsequent\nresults were not in line with those reported in [2]. First, various\nconﬁdence measures can be used in ASR. A survey of possible\nconﬁdence measures is given in [8] and several techniques for\nconﬁdence score calibration have been developed in [9]. Sec-\nondly, there are various possible ways of selecting data based\non the conﬁdence scores.\nSemi-Supervised Training (SST) has also recently received\na particular attention in the ASR literature. A method combin-\ning multi-system combination with conﬁdence score calibration\nwas proposed in [10]. A large-scale approach based on con-\nﬁdence ﬁltering together with transcript length and transcript\nﬂattening heuristics was used in [11]. A cascaded classiﬁca-\ntion scheme based on a set of binary classiﬁers was proposed\nin [12]. A reformulation of the Maximum Mutual Informa-\ntion (MMI) criterion used for sequence-discriminative training\nof Deep Neural Networks (DNN) was described in [13]. A\nshared hidden layer multi-softmax DNN structure speciﬁcally\ndesigned for SST purpose was proposed in [14]. The way un-\nsupervised data1 is selected in this paper is inspired from [11],\nas it is based on conﬁdence ﬁltering with possible additional\nconstraints on the length and frequency of the transcripts.\nThis paper aims at addressing the following questions\nwhose answer is left either open or unclear with regard to the\nliterature: i) Do more sophisticated conﬁdence models help im-\nproving data selection? ii) Is AL also beneﬁcial for LM train-\ning, and if so to which extent? iii) How do the gains of AL\nand SST scale up when more and more supervised data is tran-\nscribed? iv) Are the improvements similar after cross-entropy\nand sequence-discriminative training of the DNN AM?\nIn most of existing AL and SST studies (e.g. [2, 3, 6, 7, 13,\n14]), the Word Error Rate (WER) typically ranges between 25\nand 75%. The baseline model in the present work has a WER\nof about 12.5%, which makes the application of AL and SST on\nan industrial task even more challenging.\nThe paper is structured as follows. Section 2 presents the\napproach studied throughout our experiments. Experimental re-\nsults are described in Section 3. Finally, Section 4 concludes the\npaper.\n1In this paper, ”unsupervised data” refers to transcriptions automat-\nically produced by the baseline ASR. In literature, training with au-\ntomatic transcriptions produced by a supervised ASR system is some-\ntimes referred to as ”semi-supervised training”, but we reserve the latter\nterm to situations where both manual and automatic transcriptions are\nused together.\narXiv:1903.02852v1  [cs.CL]  7 Mar 2019\n2. Method\nOur method relies heavily on conﬁdence-based data selection.\nBecause conﬁdence scores play an essential role, several con-\nﬁdence models have been investigated. They are described in\nSection 2.1. The technique of data selection is presented in Sec-\ntion 2.2. Details about AM and LM training are then provided\nrespectively in Sections 2.3 and 2.4.\n2.1. Conﬁdence modeling\nAs mentioned in the introduction, there are various conﬁdence\nmeasures available [8, 9, 15]. First of all, conﬁdence measures\ncan be estimated at the token and utterance levels. The conven-\ntional conﬁdence score at the token level is the token posterior\nfrom the confusion network [8]. It is however in practice a poor\nestimate of the actual probability of the token being correct, and\nis therefore lacking interpretability. This was addressed in [9]\nby calibrating the scores using a maximum entropy model, an\nartiﬁcial neural network, or a deep belief network.\nIn this paper, conﬁdence score normalization is performed\nto match the conﬁdences with the observed probabilities of\nwords being correct, using one of the two following methods: a\npiecewise polynomial which maps the token posteriors to conﬁ-\ndences, or a linear regression model with various features such\nas the token posteriors, the word accuracy priors, the number\nof choices in the confusion network, and the number of token\nnodes and arcs explored. These two models are trained on an\nin-domain held-out data set.\nAs our data selection method processes utterances, it is nec-\nessary to combine the scores from the various tokens to get a\nsingle conﬁdence measure at the utterance level. Conventional\napproaches encompass using an arithmetic or geometrical mean\nrule. In addition, we have also considered training a Multi-\nLayer Perceptron (MLP) to predict either the WER or the Sen-\ntence Error Rate (SER). The MLP took as input a vector con-\nsisting of statistics of the tokens: number of tokens, min/max\nand mean values of their posteriors.\n2.2. Data Selection\nBecause the DNN we use for AM is a discriminative model,\nthe selection of supervised data for AM purpose consists in\nmaximizing the informativeness of the chosen utterances. In-\ntuitively, this translates to selecting utterances with low conﬁ-\ndence scores. Different settings of the conﬁdence ﬁlter will be\ninvestigated in our experiments. Besides, we consider also ﬁl-\ntering out too short utterances.\nThe selection of unsupervised data requires to ﬁnd a bal-\nance between the informativeness and the quality of the auto-\nmatic transcripts. This latter aspect imposes to retain only high\nconﬁdence scores, as errors in the transcripts can be harmful to\nthe training (particularly if it is sequence-discriminative [13]).\nAs suggested in [11], utterance length and frequency ﬁltering\nare additionally applied to ﬂatten the data.\n2.3. AM training\nOur AM is a conventional DNN [16] made of 4 hidden lay-\ners containing 1536 units each. A context-dependent GMM is\nﬁrst trained using the Baum-Welch algorithm and PLP features.\nThe size of the triphone clustering tree is about 3k leaves. The\nGMM is used to produce the initial alignment of the training\ndata and deﬁne the DNN output senones. Our target language in\nthis study is German, but we decided to apply transfer learning\n[17] by initializing the hidden layer weights from a previously-\ntrained English DNN. The output layer was initialized with ran-\ndom weights. The input features are 32 standard Mel-log ﬁlter\nbank energies, spliced by considering a context of 8 frames on\neach side, therefore resulting in 544 dimensional input features.\nThe training consists of 18 epochs of frame-level cross-\nentropy (XE) training followed by boosted Maximum Mu-\ntual Information (bMMI) sequence-discriminative training [18].\nThe Newbob algorithm is used as Learning Rate (LR) scheduler\nduring XE training. The learning rate for bMMI was optimized\nusing a held-out development set. The resulting DNN is used to\nre-align the data and the same procedure of DNN training start-\ning from transfer learning is applied again. The baseline model\non the 50 initial hours was obtained in this way. For the next\nmodels which ingest additional supervised and/or unsupervised\ndata, the baseline model is used to get the alignments, and the\ntraining procedure starting from transfer learning is performed.\n2.4. LM training\nOur LM is a linearly interpolated trigram model consisting of 9\ncomponents. The most important one (with interpolation weight\n> 0.6) is trained on the selected supervised and unsupervised\ndata. For the remaining components, we consider a variety of\nAmazon catalogue and text search data relevant for the voice\nsearch task. All component models are 3-gram models trained\nwith modiﬁed Kneser-Ney smoothing [19]. The interpolation\nparameters are optimized on a held-out development set. The\nsize of the LM is ﬁnally reduced using entropy pruning [20].\n3. Experiments\nThe aim of our experiments is to simulate the possible gains\nobtained by AL and SST for a new application. For this simula-\ntion, we had about 600 hours of transcribed voice search data in\nGerman at our disposal. From this pool, 50 hours are ﬁrst ran-\ndomly selected to build the baseline AM and LM. These models\nare then used to decode the remaining 550 hours. The conﬁ-\ndence models described in Section 2.1 and previously trained\non a held-out set are employed so that each utterance in the\n550h selection pool is assigned one conﬁdence score (per con-\nﬁdence model). From the selection pool, the supervised data is\nselected ﬁrst via conventional RS or via AL. Utterances which\nwere left over are considered as unsupervised data for SST. The\nevaluation is carried out on a held-out dataset of about 8 hours\nof in-domain data. A speaker overlap with the training set is\npossible but the large number of speakers diminishes its poten-\ntial effect. Our target metric is the standard WER.\nIn the next sections the results of the experiments are pre-\nsented. Section 3.1 investigates the inﬂuence of the conﬁdence\nmodel on data selection. The impact of AL and SST on both\nthe AM and the LM is studied in Sections 3.2 and 3.3. Lastly,\nSection 3.4 simulates the ﬁnal gains on a new ASR application.\n3.1. Conﬁdence modeling\nVarious conﬁdence models including a normalization of the to-\nken posteriors and an utterance-level calibration, as described in\nSection 2.1, have been tried for data selection. For each conﬁ-\ndence model, the conﬁdence ﬁlter settings have been optimized\nas will be explained in Section 3.2. Unfortunately, our results\ndid not indicate any AL improvement by using more sophis-\nticated conﬁdence models. Only marginal (below 2% relative\nWER) differences not necessarily consistent across the exper-\niments were observed. Our explanation is two-fold: First, the\nranking across the utterances in the selection pool is not sub-\nstantially affected by the different models. Second, even when\nthe ranking is altered, the informativeness of the switched ut-\nterances is probably comparable, therefore not leading to any\ndramatic difference in recognition performance.\nThe rest of this paper therefore employs a simple conﬁ-\ndence model: a polynomial is used to map the token posteriors\nto the observed word probabilities, which are then combined\nby geometrical mean. The distribution of these scores over the\n550h selection pool is shown in Figure 1. Note that the vari-\nous peaks in the high conﬁdences are due to a dependency on\nthe hypothesis length. As can be seen, the baseline model is\nalready rather good: respectively 11.6, 19.0 and 24.0% of the\nutterances have a conﬁdence score lower than 0.5, 0.7 and 0.8.\n0\t\n0.02\t\n0.04\t\n0.06\t\n0.08\t\n0.1\t\n0.12\t\n0.14\t\n0.16\t\n0.18\t\n0\t\n0.05\t\n0.1\t\n0.15\t\n0.2\t\n0.25\t\n0.3\t\n0.35\t\n0.4\t\n0.45\t\n0.5\t\n0.55\t\n0.6\t\n0.65\t\n0.7\t\n0.75\t\n0.8\t\n0.85\t\n0.9\t\n0.95\t\n1\t\nNormalized\tFrequency\t\nConﬁdence\tscore\t\nFigure 1: Histogram of the standard conﬁdence scores.\n3.2. Impact on the AM\nIn this section, we focus on the impact of AL and SST purely\non the AM. The LM and the vocabulary are therefore ﬁxed to\nthat of the baseline. For both supervised and unsupervised data\nselection, our approach relies on applying a ﬁlter to the conﬁ-\ndence scores where data is selected if the conﬁdence score is\nbetween some given lower and upper bounds.\n3.2.1. Active learning only\nIn a ﬁrst stage, we optimized the ﬁlter used for supervised data\nselection. We varied the lower ﬁlter bound in the [0-0.1] range\nin order to remove possibly uninformative out-of-domain utter-\nances. The upper bound was varied in the [0.4-0.9] range, lead-\ning to a total of 20 ﬁlters. The resulting AMs were analyzed\non the development set. The main ﬁndings were that as long\nas the lower bound does not exceed 0.05 and the upper bound\ndoes not exceed 0.8 (which corresponds to the beginning of the\nmain mode in Figure 1), the results were rather similar (with\ndifferences lower than 1% relative). It seems to be important,\nthough, not to go beyond 0.8 as this would strongly compro-\nmise the informativeness of the selected utterances. In addition,\nwe have tried to apply utterance length ﬁltering in cascade with\nthe conﬁdence-based selection. This operation however did not\nturn out to provide any gain.\nBased on these observations, we have used the [0-0.7] con-\nﬁdence ﬁlter for AL data selection. When 100h of supervised\ndata was added to the baseline, this technique reduced the WER\nby about 2% relative over the RS scheme.\n3.2.2. Including unsupervised data\nIn a second stage, we optimized the method for selecting the\nunsupervised data. On top of the 50h baseline set and the 50h\nof AL data (selected as mentioned in Section 3.2.1) we added\nunsupervised data selected according to different conﬁdence ﬁl-\nters and analyzed again the AM performance after XE training\non the development set.\nOur attempts to integrate utterance length and frequency ﬁl-\ntering as in [11] were not conclusive as no signiﬁcant gains were\nobtained. We also remarked a slight degradation if the upper\n11.5\t\n11.6\t\n11.7\t\n11.8\t\n11.9\t\n12\t\n12.1\t\n12.2\t\n0\t 50\t 100\t 200\t\nWER\t(%)\t\nAmount\tof\tunsupervised\tdata\t(h)\t\nRS\t\nRS[0.3-1.0]\t\nRS[0.7-1.0]\t\nN-highest\t\nFigure 2: Beneﬁts of unsupervised data on a XE-trained AM.\nbound for conﬁdence ﬁltering does not reach the limit of 1.0.\nWe therefore focused on pure conﬁdence ﬁltering with an upper\nbound of 1.0 in the remainder of our experiments. The plot in\nFigure 2 compares 4 techniques of unsupervised data selection:\nunﬁltered random sampling (RS), conﬁdence ﬁltering using two\ndifferent conﬁdence ﬁlters ( RS[0.3-1.0] and RS[0.7-1.0]), and\nchoosing the sentences with the highest conﬁdence scores ( N-\nhighest). We obtained the best results with the [0.7-1.0] con-\nﬁdence ﬁlter. The poor performance of the N-highest scores\napproach can be explained by the fact that it just adds high con-\nﬁdence utterances which contain little new information. On the\nother hand, with a low lower bound of the conﬁdence ﬁlter (as\nin [0.3-1.0] or RS) the label quality becomes worse and the re-\nsults also degrade. A remarkable fact is that the more unsuper-\nvised data, the better the performance of the AM. The addition\nof 200h of unsupervised data yielded an improvement of 4.5%\nrelative. The same experiment was replicated with 100h of AL\ndata, and the conclusions remained similar, except that the gain\nreached 3.5% (and not 4.5%) this time.\n3.3. Impact on the LM\nThe most important component of the interpolated LM is the\none trained on transcriptions of the in-domain utterances. In\nthis section we study the impact of different methods to select\nin-domain data and add it to this component on top of the 50h of\nthe baseline model. All other LM components are kept constant.\nWe consider three data pools from which training data could\nbe taken: supervised data from the 100h AL data pool which\nwas selected using the [0-0.7] conﬁdence ﬁlter as described in\nSection 3.2.1, supervised data from the complete pool of 550h,\nand unsupervised data from the same 550h pool, taken from the\nﬁrst hypothesis of the ASR results of the baseline model.\n3.3.1. Perplexity results\nIn a ﬁrst experiment we calculated perplexities when an increas-\ning amount of data was added to the LM. Since perplexity val-\nues are hard to compare when using different vocabularies, we\nkept the vocabulary ﬁxed to that of the baseline. The dotted\nlines in Figure 3 show the perplexities if data is randomly sam-\n30\t\n31\t\n32\t\n33\t\n34\t\n35\t\n36\t\n37\t\n0\t 55\t 110\t 165\t 220\t 275\t 330\t 385\t 440\t 495\t 550\t\nPerplexity\t\nDo-ed\tlines: \tAddi3onal\tdata\t(h)\t\t\nSolid\tlines:\t\t \tAmount\tof\tsupervised\tdata\t(h)\t\nUnsup\t\nSup/RS\t\nSup/AL\t\nSup/RS\t+\tUnsup\t\nSup/AL\t+\tUnsup\t\nFigure 3: LM perplexity for different types of data.\npled from the supervised data ( Sup/RS), if the data is sampled\nfrom the recognition results (Unsup), and if the data is sampled\nfrom the the AL data pool ( Sup/AL). It can be seen that adding\nmore application data improves the model irrespective of the\nsource. Already just adding unsupervised data gives a big per-\nplexity reduction from 36.3 to 33.0. However, there is a signiﬁ-\ncant gap between the supervised (Sup/RS) and the unsupervised\n(Unsup) case. Adding just the AL data does not perform as well\nas random sampling from the complete pool. On one hand, in\nthis case the label quality is higher compared to the unsuper-\nvised data but on the other hand, due to the selection process,\nthe data is no longer representative to the application. Con-\ntrary to the AM, which is discriminatively trained, the LM is a\ngenerative model which in general is much more vulnerable for\nmissing representativeness.\nIn the next experiments, shown as solid lines in Figure 3,\nwe combine supervised and unsupervised data with the goal to\novercome the bias in the data and to make the best use of all the\ndata. Supervised data was again selected either by RS ( Sup/RS\n+ Unsup) or by AL ( Sup/AL + Unsup) but in addition, all the\nremaining data of the 550h data pool were used in training as\nunsupervised data. This way we always use the complete data\nand thus maintain the representativeness. The beginning of the\ncurves correspond to 550h unsupervised data. In the case of\nSup/RS + Unsupit drops constantly to ﬁnal value of 550h su-\npervised data. Contrary to the previous experiment, when ap-\nplying AL to select the training data ( Sup/AL + Unsup), we no\nlonger suffer from a bias of the data and the model performs\neven slightly better than RS.\n3.3.2. Recognition results\nIt is well known that gains in perplexity do not always corre-\nspond to WER improvements. We therefore ran recognition ex-\nperiments using the LMs from Section 3.3.1. Since it is bene-\nﬁcial to the models we always added the unsupervised data on\ntop of the supervised data. The AM was kept ﬁxed to the base-\nline. As we were no longer restricted by the perplexity measure,\nwe also updated the vocabulary according to the selected super-\nvised training data in these experiments. The results in Figure\n4 show that the improvements in perplexity are also reﬂected in\na better WER even though part of the improvements might also\nbe due to the increased vocabulary coverage. It is interesting\nto observe that, when adding 100 hours of supervised data, the\ngains for AL are much higher than for RS. In total, the impact\nof AL combined with SST on LM is outstanding: after 100h\nof transcribed data, the gain over the RS baseline reaches 5.3%\nrelative. It is also worth emphasizing that 100h AL and roughly\n400h RS are equivalent in terms of LM performance.\n12.2\t\n12.4\t\n12.6\t\n12.8\t\n13\t\n13.2\t\n13.4\t\n13.6\t\n0\t 50\t 100\t 200\t 300\t 550\t\nWER\t(%)\t\nAmount\tof\tsupervised\tdata\t(h)\t\nSup/RS\t+\tUnsup\t\nSup/AL\t+\tUnsup\t\nFigure 4: ASR results with updated LM and vocabulary.\n3.4. Final results\nFinally, we simulate the improvements that would be yielded in\na new application by applying conﬁdence-based AL and SST\nto both the AM and LM. We considered the different LMs as\nsuggested in Section 3.3. For AM building, we limited the un-\nsupervised set to 200h across our experiments. For XE training,\nSST was applied, following the ﬁndings from Section 3.2.2. For\nsequence-discriminative bMMI training, it is known that possi-\nble errors in the transcripts can have a dramatic negative inﬂu-\nence on the quality of the resulting AM [13]. Therefore, two\nstrategies were investigated: i) considering the aggregated set\nof supervised and unsupervised data for bMMI training; ii) dis-\ncard any unsupervised data and only train on the supervised set.\nOur results indicate that the inclusion of unsupervised data led\nto a degradation of about 2.5%, and this despite the relatively\nhigh lower bound used in the conﬁdence ﬁlter (0.7). The ﬁrst\nstrategy was therefore used in the following.\n9.5\t\n10\t\n10.5\t\n11\t\n11.5\t\n12\t\n12.5\t\n0\t 25\t 50\t 75\t 100\t 150\t 200\t 300\t\nWER\t(%)\t\nAmount\tof\tsupervised\tdata\t(h)\t\nSup/RS\t\nSup/AL\t\nSup/RS\t+\tUnsup\t\nSup/AL\t+\tUnsup\t\nFigure 5: Final simulation: both the AM and LM are updated.\nFigure 5 shows the ﬁnal simulation results after bMMI\ntraining. It is worth noting that the results obtained after XE\ntraining were very much in line and led to very similar improve-\nments. Two main conclusions can be drawn from this graph.\nFirst, the unsupervised data is particularly important at the very\nbeginning, where it allows a 6.8% relative improvement. Nev-\nertheless, the gains of SST vanish as more supervised data is\ncollected. In the AL case, the advantage from SST almost com-\npletely disappears after 100h of additional supervised data. Sec-\nondly, AL carries out signiﬁcant improvements over RS. It can\nbe seen that the WER obtained with 100h of AL is comparable\n(even slightly better) to that using 300h of RS data, hence re-\nsulting in a reduction of the transcription budget of about 70%.\nAlternatively, one can observe that, for a ﬁxed transcription cost\nof 100h, AL achieves an appreciable WER reduction of about\n12.5% relative over the range of added supervised data.\n4. Conclusions\nThis paper aimed at simulating the beneﬁts of AL and SST in\na new ASR application by applying conﬁdence-based data se-\nlection. More sophisticated conﬁdence models have been de-\nveloped, but they did not provide any gain for training data se-\nlection for AL. Regarding AM training, AL alone was found to\nyield a 2% relative improvement. Combining it with SST turned\nout to be essential, especially when the amount of supervised\ndata is limited. Adding 200h of unsupervised data to 50h of\nAL gave a 4.5% gain on the AM trained by cross-entropy. On\nthe contrary, any unsupersived data was harmful to sequence-\ndiscriminative bMMI training. Beyond these improvements on\nthe AM, combining AL and SST allowed a signiﬁcant improve-\nment (about 5%) of the LM. Our ﬁnal results indicate that ap-\nplying AL to both AM and LM provides an encouraging 70%\nreduction of the transcription budget over RS, and these gains\nseem to scale up rather well as more and more utterances are\ntranscribed.\n5. References\n[1] D. Cohn, L. Atlas, and R. Ladner, “Improving generalization with\nactive learning,” Machine Learning, vol. 15, no. 2, pp. 201–221,\n1994.\n[2] G. Riccardi and D. Hakkani-T ¨ur, “Active learning: Theory and\napplications to automatic speech recognition,”IEEE Transactions\non Speech and Audio Processing, vol. 13, no. 4, pp. 504–511,\n2005.\n[3] D. Yu, B. Varadarajan, L. Deng, and A. Acero, “Active learning\nand semi-supervised learning for speech recognition: A uniﬁed\nframework using the global entropy reduction maximization cri-\nterion,” Computer Speech and Language, vol. 24, pp. 433–444,\n2010.\n[4] Y . Hamanaka, K. Shinoda, S. Furui, T. Emori, and T. Koshinaka,\n“Speech modeling based on committee-based active learning,”\nICASSP, pp. 4350–4353, 2010.\n[5] S. Huang, R. Jin, and Z. Zhou, “Active learning by querying in-\nformative and repsentative examples,”IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, vol. 36, no. 10, pp. 1936–\n1949, 2014.\n[6] N. Itoh, T. Sainath, D. Jiang, J. Zhou, and B. Ramabhadran, “N-\nbest entropy based data selection for acoustic modeling,”ICASSP,\npp. 4133–4136, 2012.\n[7] T. Fraga-Silva, J. Gauvain, L. Lamel, A. Laurent, V . Le, and\nA. Messaoudi, “Active learning based data selection for limited\nresource stt and kws,”Interspeech, pp. 3159–3162, 2015.\n[8] H. Jiang, “Conﬁdence measures for speech recognition: A sur-\nvey,”Speech Communication, vol. 45, pp. 455–470, 2005.\n[9] D. Yu, J. Li, and L. Deng, “Calibration of conﬁdence measures\nin speech recognition,” IEEE Transactions on Audio, Speech and\nLanguage, vol. 19, no. 8, pp. 2461–2473, 2011.\n[10] Y . Huang, D. Yu, Y . Gong, and C. Liu, “Semi-supervised gmm\nand dnn acoustic model training with multi-system combination\nand conﬁdence re-calibration,” Interspeech, p. 23602364, 2013.\n[11] O. Kapralova, J. Alex, E. Weinstein, P. Moreno, and O. Siohan,\n“A big data approach to acoustic model training corpus selection,”\nInterspeech, p. 20832087, 2014.\n[12] S. Li, Y . Akita, and T. Kawahara, “Discriminative data selection\nfor lightly supervised training of acoustic model using closed cap-\ntion texts,”Interspeech, p. 35263530, 2015.\n[13] V . Manohar, D. Povey, and S. Khudanpur, “Semi-supervised max-\nimum mutual information training of deep neural network acous-\ntic models,”Interspeech, p. 26302634, 2015.\n[14] H. Su and H. Xu, “Multi-softmax deep neural network for semi-\nsupervised training,”Interspeech, p. 32393243, 2015.\n[15] Z. Bergen and W. Ward, “A senone based conﬁdence measure for\nspeech recognition,”Eurospeech, pp. 819–822, 1997.\n[16] G. Hinton, L. Deng, D. Yu, A. Mohamed, N. Jaitly, A. Senior,\nV . Vanhoucke, P. Nguyen, T. Sainath, G. Dahl, and B. Kingsbury,\n“Deep neural networks for acoustic modeling in speech recogni-\ntion,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–\n97, 2012.\n[17] G. Heigold, V . Vanhoucke, A. Senior, P. Nguyen, M. Ranzato,\nM. Devin, and J. Dean, “Multilingual acoustic models using dis-\ntributed deep neural networks,”ICASSP, pp. 8619–8623, 2013.\n[18] K. Vesely, A. Ghoshal, L. Burget, and D. Povey, “Sequence-\ndiscriminative training of deep neural networks,”Interspeech, pp.\n2345–2349, 2013.\n[19] R. Kneser and H. Ney, “Improved backing-off for m-gram lan-\nguage modeling,”ICASSP, vol. 1, pp. 181–184, 1995.\n[20] A. Stolcke, “Entropy-based pruning of backoff language models,”\nProc. DARPA Broadcast News Transcription and Understanding\nWorkshop, pp. 270–274, 1998.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7521834373474121
    },
    {
      "name": "Word error rate",
      "score": 0.69376540184021
    },
    {
      "name": "Transcription (linguistics)",
      "score": 0.6562261581420898
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.6143137812614441
    },
    {
      "name": "Language model",
      "score": 0.5941111445426941
    },
    {
      "name": "Training set",
      "score": 0.5526342391967773
    },
    {
      "name": "Speech recognition",
      "score": 0.5137282609939575
    },
    {
      "name": "Process (computing)",
      "score": 0.4613795876502991
    },
    {
      "name": "Reduction (mathematics)",
      "score": 0.4554573595523834
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.44424617290496826
    },
    {
      "name": "Machine learning",
      "score": 0.436943918466568
    },
    {
      "name": "Word (group theory)",
      "score": 0.42716777324676514
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4189852476119995
    },
    {
      "name": "Labeled data",
      "score": 0.41026774048805237
    },
    {
      "name": "Mathematics",
      "score": 0.1146688163280487
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    }
  ]
}