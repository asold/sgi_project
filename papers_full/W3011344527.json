{
    "title": "Transformer Networks for Trajectory Forecasting",
    "url": "https://openalex.org/W3011344527",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2893304180",
            "name": "Francesco Giuliari",
            "affiliations": [
                "University of Verona"
            ]
        },
        {
            "id": "https://openalex.org/A2306667116",
            "name": "Irtiza Hasan",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A1991389521",
            "name": "Marco Cristani",
            "affiliations": [
                "University of Verona"
            ]
        },
        {
            "id": "https://openalex.org/A2082727544",
            "name": "Fabio Galasso",
            "affiliations": [
                "Sapienza University of Rome"
            ]
        },
        {
            "id": "https://openalex.org/A2893304180",
            "name": "Francesco Giuliari",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2306667116",
            "name": "Irtiza Hasan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1991389521",
            "name": "Marco Cristani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2082727544",
            "name": "Fabio Galasso",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2167052694",
        "https://openalex.org/W3120048558",
        "https://openalex.org/W3007298738",
        "https://openalex.org/W3010099216",
        "https://openalex.org/W2519586580",
        "https://openalex.org/W6732424948",
        "https://openalex.org/W2740185481",
        "https://openalex.org/W2532516272",
        "https://openalex.org/W1571870753",
        "https://openalex.org/W1657213141",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4242688646",
        "https://openalex.org/W2579024533",
        "https://openalex.org/W2766836212",
        "https://openalex.org/W3114753236",
        "https://openalex.org/W2964230007",
        "https://openalex.org/W3003235888",
        "https://openalex.org/W2914524146",
        "https://openalex.org/W4405188194",
        "https://openalex.org/W2133235827",
        "https://openalex.org/W4301861531",
        "https://openalex.org/W2424778531",
        "https://openalex.org/W1970206276",
        "https://openalex.org/W6765361892",
        "https://openalex.org/W2963001155",
        "https://openalex.org/W2985871763",
        "https://openalex.org/W2068251166",
        "https://openalex.org/W2962687116",
        "https://openalex.org/W2005248249",
        "https://openalex.org/W6640598415",
        "https://openalex.org/W1998569958",
        "https://openalex.org/W2082585576",
        "https://openalex.org/W2604679602",
        "https://openalex.org/W6740017579",
        "https://openalex.org/W2792764867",
        "https://openalex.org/W2163899311",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2798930779",
        "https://openalex.org/W2576001372",
        "https://openalex.org/W2951063106",
        "https://openalex.org/W2970219816",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2997264916",
        "https://openalex.org/W2963353290",
        "https://openalex.org/W3106257603",
        "https://openalex.org/W2911273949",
        "https://openalex.org/W3000520642",
        "https://openalex.org/W3032731803",
        "https://openalex.org/W2887556565",
        "https://openalex.org/W1945395050",
        "https://openalex.org/W2980923043",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2804827665",
        "https://openalex.org/W2070681743",
        "https://openalex.org/W1810943226"
    ],
    "abstract": "Most recent successes on forecasting the people motion are based on LSTM models and all most recent progress has been achieved by modelling the social interaction among people and the people interaction with the scene. We question the use of the LSTM models and propose the novel use of Transformer Networks for trajectory forecasting. This is a fundamental switch from the sequential step-by-step processing of LSTMs to the only-attention-based memory mechanisms of Transformers. In particular, we consider both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art on all natural language processing tasks. Our proposed Transformers predict the trajectories of the individual people in the scene. These are \"simple\" model because each person is modelled separately without any complex human-human nor scene interaction terms. In particular, the TF model without bells and whistles yields the best score on the largest and most challenging trajectory forecasting benchmark of TrajNet. Additionally, its extension which predicts multiple plausible future trajectories performs on par with more engineered techniques on the 5 datasets of ETH + UCY. Finally, we show that Transformers may deal with missing observations, as it may be the case with real sensor data. Code is available at https://github.com/FGiuliari/Trajectory-Transformer.",
    "full_text": "Transformer Networks for Trajectory Forecasting\nFrancesco Giuliari\nUniversity of Verona\nIrtiza Hasan\nInception Institute of Artiﬁcial Intelligence\nMarco Cristani\nUniversity of Verona\nFabio Galasso\nSapienza University of Rome\nAbstract—Most recent successes on forecasting the people mo-\ntion are based on LSTM models andall most recent progress has\nbeen achieved by modelling the social interaction among people\nand the people interaction with the scene. We question the use\nof the LSTM models and propose the novel use of Transformer\nNetworks for trajectory forecasting. This is a fundamental switch\nfrom the sequential step-by-step processing of LSTMs to the\nonly-attention-based memory mechanisms of Transformers. In\nparticular, we consider both the original Transformer Network\n(TF) and the larger Bidirectional Transformer (BERT), state-of-\nthe-art on all natural language processing tasks. Our proposed\nTransformers predict the trajectories of the individual people\nin the scene. These are “simple” models because each person\nis modelled separately without any complex human-human nor\nscene interaction terms. In particular, the TF model without\nbells and whistles yields the best score on the largest and most\nchallenging trajectory forecasting benchmark of TrajNet [1]. Ad-\nditionally, its extension which predicts multiple plausible future\ntrajectories performs on par with more engineered techniques\non the 5 datasets of ETH [2]+UCY [3]. Finally, we show\nthat Transformers may deal with missing observations, as it\nmay be the case with real sensor data. Code is available at\ngithub.com/FGiuliari/Trajectory-Transformer.\nI. I NTRODUCTION\nPedestrian forecasting, the goal of predicting future people\nmotion given their past trajectories, has been steadily growing\nin attention by the research community. Further to being a\ncrucial compound of trackers, especially for the cases of large\nmotion and/or missing observations, the topic serves early\naction recognition, surveillance and automotive systems.\nStarting from [4], Long Short-Term Memory (LSTM) net-\nworks have been the workhorse for forecasting and progress\nhas been achieved by devising social pooling mechanisms to\nmodel the people social interaction [4], [5]. The LSTM is\nbased on sequentially processing sequences and storing hidden\nstates to represent knowledge about the people, e.g. its speed,\ndirection and motion pattern. Most modern approaches have\nchallenged each other on the social interaction of pedestrians,\neach modelled with a separate LSTM and exchanging infor-\nmation by means of social pooling mechanisms [4], [5]. In fact\nbest performing approaches additionally include the semantics\nof the scene into the LSTMs [6], [7], [8], [9]. However LSTMs\nhave also been target of criticism: their memory mechanism\nhas been criticised [10], [11] and, most recently, also their\ncapability of modelling social interaction [12], [13], [14].\nAn in-depth understanding of such mechanisms has not been\nsupported by the adopted datasets, such as the 5 datasets of\nETH [2] and UCY [3], where performance measures are close\nto saturation, since leading techniques only report average\nforecasting errors of ∼20cm across 200m-long pavements.\nIn this work we side-step social and map mechanisms,\nand propose to model the trajectories of individual people\nby Transformer Networks [15], for the ﬁrst time. Transformer\nnetworks have been proposed for Natural Language Processing\nto model word sequences. These use attention instead of\nsequential processing. In other words, these estimate which\npart of the input sentence to focus on, when needing to\ntranslate, answer a question or complete the sentence [16],\n[17]. Here we consider for trajectory forecasting the original\nTransformer Network (TF) and the Bidirectional Transformer\n(BERT) models, on which state-of-the-art NLP algorithms are\nbased. Fig. 1 illustrates the fundamental difference between\nTF and LSTM: LSTM sequentially processes the observations\nbefore starting to predict auto-regressively, while TF “looks”\nat all available observations, weighting them according to an\nattention mechanism.\nWe assess the performance of TF and BERT on the Tra-\njNet benchmark [1], in order to have a clean evaluation\n(TrajNet uses a uniﬁed evaluation system with a dedicated\nserver) against 42 forecasting approaches, on a large selection\nof datasets. Our TF outperforms all other techniques, also\nthose including social mechanisms. TF compares favorably\nalso on the ETH+UCY datasets, in particular beating all of\nthe approaches that consider the individual trajectories only.\nFinally we conduct an ablation to highlight the potential of\nthe Transformers, quantitatively and qualitatively. Of particular\ninterest is the ability of TF to still predict from inputs with\nmissing observation data, thanks to its attention mechanism,\nwhich the LSTM cannot do.\nII. R ELATED WORK\nForecasting people trajectories has been studied for over two\ndecades and relevant literature has been surveyed by the work\nof [14], [18]. For the purpose of this paper, we distinguish\ntwo main trends of related work: a ﬁrst which has focused\non progressing sequence modelling and a second which has\nmodelled the interactions between the people and between the\npeople and the scene.\nSequence modelling: Trajectory forecasting has experienced\na steady progress from hand-crafted energy-based optimiza-\ntion approaches to data-driven ones. Early work on human\npath prediction have adopted linear [19] or Gaussian regres-\nsion models [20], [21], time-series analysis [22] and auto-\nregressive models [23], optimizing for hand-crafted energy\nfunctions. By contrast, later models have been most successful\nby the adoption of LSTM [24] and RNN models, trained\nwith copious amounts of data. In particular, LSTM can be\narXiv:2003.08111v3  [cs.CV]  21 Oct 2020\nFig. 1. People trajectory forecasting stands for predicting the future motion of people ( green ground-truth dots ), given an observation interval ( blue dots ).\nLSTM (left) sequentially processes the observations before starting to predict, while TF analyses in one shot all available observations.\nemployed to regress directly the predicted values [13], [5],\n[7], or to produce mean and (diagonal) covariance over the\nx,y coordinates in order to express the uncertainty associated\nto the prediction [4]. In the latter case, we refer to this\nmodel as Gaussian LSTM . Here we argue that Transformer\nNetworks are most suitable to sequence modelling and to\nforecast trajectories, thanks to their better capability to learn\nnon-linear patterns, especially emerging when large amounts\nof data is available.\nSocial models and context: Enabled by the ﬂexibility of\nthe LSTM machinery, best performance has been recently\nachieved by modelling the social interaction [4], [5], [25]\namong people and the scene context [9], [7], [6], aided\nby tracking dynamics [26] and the spatio-temporal relations\namong neighboring people [27], [28]. Much literature has\nrecently criticised the capability of LSTM to model the human-\nhuman interaction [12], [13], [14], maintaining that this limits\nthe model generalization capability [12]. Our work side-steps\nsocial and environmental interactions and focuses on the\nprediction of the motion of each person individually. Somehow\nsurprisingly, our “simple” approach achieves best performance\non the most challenging benchmark of TrajNet.\nIn this work, we leverage ﬁndings and state-of-the-art\ntechniques developed within the NLP ﬁeld to model word\nsequences. In particular, we consider here for trajectory fore-\ncasting the original Transformer Networks [15], ﬁrst to model\nsequences merely by attention mechanisms. Aside TF, we\nconsider the Bidirectional Transformers BERT [16], which\nforms the basis for the current performer on most NLP\ntasks [29]. To the best of our knowledge, this is the ﬁrst work\nadopting NLP technique for trajectory forecasting.\nIII. T HE TRANSFORMER MODEL\nWe propose a multi-agent framework where each person\nis modelled by an instance of our transformer network. Each\nTransformer Network predicts the future motion of the person\nas a result of its previous motion.\nWe describe in this section the model input and output\n(Sec. III-A), the encoder-decoder Transformer Network (TF)\n(Sec. III-B) and the just-encoder BERT model (Sec. III-C) and\nthe implementation details (Sec. III-D).\nA. Model input and output\nFor each person, the transformer network outputs the pre-\ndicted future positions by processing their current and prior\npositions (observations or motion history). We detail here each\nof the input and output information and parallel those with the\nestablished LSTM, with reference to Fig. 2.\na) Observed and predicted trajectories: In formal terms,\nfor person i, we are provided a set Tobs = {x(i)\nt }0\nt=−(Tobs−1)\nof Tobs observed current and prior positions in Cartesian\ncoordinates x ∈ R2, and we are required to predict a set\nTpred = {x(i)\nt }Tpred\nt=1 of Tpred predicted positions. In order to\nlet the transformer deal with the input, this is embedded onto\na higher D-dimensional space by means of a linear projection\nwith a matrix of weights Wx, i.e., e(i,t)\nobs = x(i)⊺\nt Wx.\nIn the same way, the output of our transformer model for\nperson iat time tis the D-dimensional vector e(i,t)\npred, which is\nback-projected to the Cartesian person coordinates x(i)\nt . LSTM\nand TF share this aspect.\nb) Positional encoding: The transformer encodes time\nfor each past and future time instant t with a “positional\nencoding”. In other words, each input embeddinge(i,t)\nobs is time-\nstamped with its time t. The same encoding is used to prompt\nthe model to predict into future instants, as we detail in the\nnext section.\nMore formally, the input embedding e(i,t)\nobs is time-stamped\nat time t by adding a positional encoding vector pt, of the\nsame dimensionality D: ξ(i,t)\nobs = pt + e(i,t)\nobs\nWe use sine/cosine functions to deﬁne pt as in [15]:\npt = {pt,d}D\nd=1 (1)\nwhere pt,d =\n{ sin\n( t\n10000d/D\n)\nfor d even\ncos\n( t\n10000d/D\n)\nfor d odd (2)\nIn other words, each dimension of the positional encoding\nvaries in time according to a sinusoid of different frequency,\nfrom 2π to 10000 ·2π. This ensures a unique time stamp for\nFig. 2. Model illustration of LSTM ( left) and TF ( right). At each time step, LSTM leverages the current-frame information and its hidden state. By contrast,\nTF leverages the encoder representation of the observed input positions and the previously predicted outputs. In purple and grey are the self-attention and\nencoder-decoder attention modules, that allow TF to learn on which past position it needs to focus to predict a correct trajectory.\nsequences of up to 10000 elements and extends unseen lengths\nof sequences.\nIn this aspect, TF differs greatly from LSTM, cf. Fig. 2.\nLSTM processes the input sequentially and the order of input\npositions determine the ﬂow of time. It does not therefore need\na positional encoding. However, LSTM needs to “unroll” at\ntraining time, i.e. back-propagate the signal sequentially across\nthe LSTM blocks processing the observations. By contrast, the\ntraining of TF is parallelizable.\nNotably, thanks to the positional encoding which time-\nstamps the input, TF may deal with missing observations.\nMissing data is just neglected, but the model is aware of the\nrelative time-stamps of the presented observations. In Sec. IV,\nwe experiment on this unique feature, important when dealing\nwith real sensor data.\nc) Regression Vs. classiﬁcation: Regression Vs. clas-\nsiﬁcation is a recurrent question in trajectory forecasting.\nRegression techniques, predicting the ( x,y) coordinates di-\nrectly, generally outperform classiﬁcation-based approaches,\nwhere the inputs are quantized into classes and the input\ndata represented as one-hot-vectors. We test both approaches\nand conﬁrm the better performance of regression. However,\na classiﬁcation approach, which we dub TF q, provides a\nprobabilistic output across the quantized motions.\nWe leverage TF q to sample multiple future predictions,\nwhich we assess both quantitatively and qualitative. The\npredictions of TFq are multi-modal, as we illustrate in Sec. IV.\nB. Encoder-decoder Transformer (TF)\nAs illustrated in Fig. 2, TF is a modular architecture, where\nboth the encoder and the decoder are composed of 6 layers,\neach containing three building blocks: i. an attention module,\nii. a feed-forward fully-connected module, and iii. two residual\nconnections after each of the previous blocks.\nThe capability of the network to capture sequence non-\nlinearities lies mainly in the attention modules. Within each\nattention module, an entry of a sequence, named “query” (Q),\nis compared to all other sequence entries, named “keys” (K)\nby a scaled dot product, scaled by the equal query and key dk\nembedding dimensionality. The output is then used to weight\nthe same sequence entries, named now “values” (V). Attention\nis therefore given by the equation:\nAttention(Q,K,V ) =softmax\n(QKT\n√dk\n)\n(3)\nThe goal of the encoding stage is to create a representation\nfor the observation sequence, which makes the model memory.\nTo this goal, after the encoding of the Tobs input embeddings\nξ(i,t)\ns , the network outputs two vectors of keys Kenc and values\nVenc which would be passed on to the decoder stage.\nThe decoder predicts auto-regressively the future track posi-\ntions. At each new prediction step, a new decoder query Qdec\nis compared against the encoder keys Kenc and values Venc\naccording to Eq. (3) (encoder-decoder attention) and against\nthe previous decoder prediction (self-attention).\nNote the important difference w.r.t. LSTM: TF maintains\nthe encoding output (memory) separate from the decoded\nsequence, while LSTM accumulates both into its hidden state,\nsteering what to memorize or forget at each time. We believe\nthis may contribute to explain how TF outperforms LSTM in\nlong-term horizon predictions, cf. Sec. IV.\nC. BERT\nWe consider for trajectory forecasting a second Transformer\nmodel, BERT [16]. Differently from TF, BERT is only com-\nposed of an encoder and it trains and infers thanks to a\nmasking mechanism. In other words, the model hides (masks)\nfrom the self-attention the output positions which it targets\nfor prediction as the TF decoder also does. During training\nthe model learns to predict masked positions. At inference,\nthe model output predictions for the masked outputs.\nBERT is the de-facto reference model for state-of-the-art\nNLP methods, but larger than TF ( ∼2.2 times larger). As\nwe would illustrate in Sec. IV, training BERT on the current\nlargest trajectory forecasting benchmarks does not keep up to\nthe expectations. We draw inspiration from transfer learning\nand test therefore also how a BERT pre-trained on an NLP task\nperforms on the target task. In particular, we take the lower-\ncased English text using Whole-Word-Masking; we substitute\nfor the word embedding from dictionary keys with similar\nlinear modules encoding (x,y) positions; and then we similarly\nconvert also the output into ( x,y) positions.\nD. Implementation details\nOur TF implementations adopts the parameters of the orig-\ninal Transformer Networks [15], namely dmodel = 512, 6\nlayers and 8 attention heads. We adopt an L2-loss between\nthe predicted and annotated pedestrian positions and train the\nnetwork via backpropagation with the Adam optimizer, linear\nwarm-up phase for the ﬁrst 5 epoch and a decaying learning\nrate afterward; dropout value of 0.1. The normalization of the\nnetwork input inﬂuences its performance, as also maintained in\n[30], [31]. So we normalize the people speeds by subtracting\nthe mean and dividing by the standard deviation of the train\nset. For the TF q, we quantize the people motion by clustering\nspeeds into 1000 joint ( x,y) bins, then encode the position\nby 1000-way one-hot vectors. In order to get a good cluster\ngranularity, we augment the training data by random scaling\nuniformly with scale s∈[0.5,2].\nIV. E XPERIMENTAL EVALUATION\nWe show the capabilities of the proposed Transformer\nnetworks for trajectory forecasting on two recent and large\ndatasets: the TrajNet Challenge [1] dataset and the ETH+UCY\ndataset [2], [3]. Additionally, we perform an ablation study to\nquantify the model robustness, also in comparison with the\nwidely-adopted LSTM. This includes varying the observation\nhorizon and testing the model on missing data, the latter\noccurring when some observation samples are missing due to\nframe-rate drops or excessive uncertainty in the tracking data.\nA. The Trajnet Challenge\nThe TrajNet Dataset:At the moment of writing, the TrajNet\nChallenge1 [1] does represent the largest multi-scenario fore-\ncasting benchmark [32]; the challenge requests to predict 3161\nhuman trajectories, observing for each trajectory 8 consecutive\nground-truth values (3.2 seconds) i.e., t−7,t −6,...,t , in\nworld plane coordinates (the so-called world plane Human-\nHuman protocol) and forecasting the following 12 (4.8 sec-\nonds), i.e., t+ 1,...,t + 12. In fact, TrajNet is a superset\nof diverse datasets that requires to train on four families\nof trajectories, namely 1) BIWI Hotel [2] (orthogonal bird’s\neye ﬂight view, moving people), 2) Crowds UCY [3] (3\ndatasets, tilted bird’s eye view, camera mounted on building or\nutility poles, moving people), 3) MOT PETS [33] (multisensor,\ndifferent human activities) and 4) Stanford Drone Dataset [34]\n(8 scenes, high orthogonal bird’s eye ﬂight view, different\nagents as people, cars etc.), for a total of 11448 trajectories.\nTesting is requested on diverse partitions of BIWI Hotel,\nCrowds UCY , Stanford Drone Dataset, and is evaluated by\na speciﬁc server (ground-truth testing data is unavailable for\napplicants). As a proof of its toughness, it is worth noting that\nmany recent studies restrict on subsets of TrajNet [35], [36],\n[37], adopting their train/test splits [38]. We instead consider\nthe whole TrajNet dataset for our experiments. TrajNet allows\n1http://trajnet.stanford.edu/\nto consider concurrent trajectories, so that it is compliant with\n“social” approaches, that can apply. Conversely, it does not\nallow use raw images, so that approaches which infer on maps\nas [9], [8], [7] cannot apply.\nMetrics: In agreement with most literature on people tra-\njectory forecasting, the TrajNet performance is measured in\nterms of: Mean Average Displacement (MAD, equivalently\nAverage Displacement Error ADE [8]), measuring the general\nﬁt of the prediction w.r.t. the ground truth, averaging the\ndiscrepancy at each time step; Final Average Displacement\n(FAD, equivalently Final Displacement Error FDE [8]), to\ncheck the goodness of the prediction at the last time step. The\naverage of MAD and FAD is used to rank the approaches.\nResults on TrajNet: We report in Table I the complete\nlist of 22 published comparative approaches, for a total of\n39 approaches at the moment of writing; we omit the 18\nunpublished results, all of which, apart from one, nonetheless\nhad lower performance than the previous published top-scoring\napproach REDv3 [14]. In the table, “Rank” indicates the abso-\nlute ranking over all the approaches, including the unpublished\nones; “Year” the year of publication of the method; “Context”\nindicates whether the additional social context (the trajectories\nof the other co-occurring people) is taken into account (“s”)\nor not (“/”).\nThe scores in blue italic refer to the methods proposed in\nthis work (TF, TF q, BERT, BERT NLP pretrained). Surpris-\ningly, the TF model is the new best, with an advantage in\nterms of both MAD and FAD w.r.t. the second REDv3 [14]\nand reducing the total error across the 3161 test tracks by\n∼145 meters.\nIt is of interest that the top four approaches (including\nours) are individual ones, so no social context is taken into\naccount. These results undoubtedly suggest that in ∼3 seconds\nof individual observation of an individual, much information\nabout his future can be extracted, and TF is the most successful\nin doing it. In fact, social approaches appear at lower ranks: the\nﬁrst among them is the SR-LSTM [30], then the highly-cited\nSocial Forces [39] (rank 9 and 27), the MX-LSTM [40] and\nSocial GAN [5]. The quantized TF q ranks 16th, very probably\ndue to quantization errors. For the trajnet challenge the TF q\nwas used in its deterministic mode, i.e. the class with highest\nconﬁdence was selected for the 12 predictions. This is done\nso because TrajNet is not set up to evaluate best-of-N metric\nand only a single prediction can be evaluated by the server.\nBERT trained from scratch on trajectories ranks 25th; its\nNLP-pretrained version, ﬁne-tuned on TrajNet, follows imme-\ndiately. The BERT performance may indicate that the model\ndoes require a way larger amount of training data, which at\nthe present moment is absolutely not comparable to the size of\nan NLP dataset. For this reason, in the rest of the experiments\nwe will concentrate on the TF.\nB. The ETH+UCY Benchmark\nPrior to TrajNet, most literature have benchmarked fore-\ncasting performance on a set of 5 datasets, namely the ETH-\nTABLE I\nTRAJ NET CHALLENGE RESULTS (WORLD PLANE HUMAN -HUMAN\nTRAJ NET CHALLENGE , WEBSITES ACCESSED ON 26/07/2020). Blue italic\nindicates approaches proposed in this work.\nRank Method Avg FAD MAD Context Cit. Year\n2 TF 0.776 1.197 0.356 / 2020\n3 REDv3 0.781 1.201 0.360 / [14] 2019\n4 REDv2 0.783 1.207 0.359 / [14] 2019\n6 RED 0.798 1.229 0.366 / [14] 2018\n7 SR-LSTM 0.816 1.261 0.37 s [30] 2019\n9 S.Forces (EW AP) 0.819 1.266 0.371 s [39] 1995\n12 N-Lin. RNN-Enc-MLP 0.827 1.276 0.377 / [14] 2018\n13 N-Lin. RNN 0.841 1.300 0.381 / [14] 2018\n15 Temp. ConvNet (TCN) 0.841 1.301 0.381 / [10] 2018\n16 TFq 0.858 1.300 0.416 / 2020\n17 N-Linear Seq2Seq 0.860 1.331 0.390 / [14] 2018\n18 MX-LSTM 0.887 1.374 0.399 s [40] 2018\n21 Lin. RNN-Enc.-MLP 0.892 1.381 0.404 / [14] 2018\n22 Lin. Interpolation 0.894 1.359 0.429 / [14] 2018\n24 Lin. MLP (Off) 0.896 1.384 0.407 / [14] 2018\n25 BERT 0.897 1.354 0.440 / [16] 2020\n26 BERTNLPpretrained 0.902 1.357 0.447 / 2020\n27 S.Forces (ATTR) 0.904 1.395 0.412 s [39] 1995\n29 Lin. Seq2Seq 0.923 1.429 0.418 / [14] 2018\n30 Gated TCN 0.947 1.468 0.426 / [10] 2018\n31 Lin. RNN 0.951 1.482 0.420 / [14] 2018\n32 Lin. MLP (Pos) 1.041 1.592 0.491 / [14] 2018\n34 LSTM 1.140 1.793 0.491 / [41] 2018\n36 S-GAN 1.334 2.107 0.561 s [5] 2018\n40 Gauss. Process 1.642 1.038 2.245 / [42] 2010\n42 N-Linear MLP (Off) 2.103 3.181 1.024 / [14] 2018\nuniv and ETH-hotel [2] video sequences and the UCY-zara01,\nUCY-zara02 and UCY-univ [3] videos.\nDatasets and metrics:The ETH+UCY datasets consist overall\nof 5 videos taken from 4 different scenes (Zara1 and Zara2\nare taken from the same camera but at a different time).\nFollowing the evaluation protocol of [4] we sample from the\ndata each 0.4 seconds to get the trajectories. We observe each\npedestrian for 3.2 seconds (8 frames) and get ground-truth\ndata for the next 4.8 seconds (12 frames) to evaluate the\npredictions. The pedestrian positions are converted to world\ncoordinates in meters from the original pixel locations using\nhomography matrices released by the authors. The evaluation\nis done with a LOO approach training for 4 dataset and testing\non the remaining one. Recent works brought up some issues\nwith the ETH+UCY dataset, [12] showed that Hotel contains\ntrajectory that go in a different direction than most of the\nones in the other 4 dataset, so learning an environmental prior\ncan be difﬁcult without data augmentation like rotation; [30]\nbring up the issue that ETH is an accelerated video and so by\nusing a sampling rate of 0.4 seconds the trajectory behave in a\ndifferent way than the ones in the other 4 datasets, they showed\nhow by reducing the sampling rate they were able to improve\ntheir results. We do not take any measure to ﬁx these issues, in\norder to have a fair comparison against all the other methods\nthat use these dataset using the standard protocol, but during\nour internal testing we noticed similar improvement when\nusing their sampling rate for ETH. Performance is evaluated\nusing MAD and FAD, in meters.\nResults: In Table II, we compare on the ETH+UCY against\nthe most recent and best performing approaches: S-GAN [5],\nSocial-BIGAT [6] and Trajectron++ [7]. Additionally we in-\nclude the “individual” version of S-GAN [5], which does not\nleverage the social information. Note in the Table the trend\nTABLE II\nCOMPARISON AGAINST SOA MODELS FOLLOWING THE BEST -OF-20\nPROTOCOL . THE ENTIRETY OF SOA APPROACHES IS ROOTED ON LSTM,\nAND LEVERAGES ADDITIONAL INFORMATION (SOCIAL , SEGMENTED\nMAPS ). T HE MERE QUANTIZED TRANSFORMER TFq IS SUPERIOR TO ALL\nTHE SOCIAL APPROACHES , SECOND ONLY TO TRAJECTRON ++.\nACTUALLY , ONLY S-GAN- IND [5] AND TFq HAVE THE SAME INPUT AND\nARE DIRECTLY COMPARABLE ; ALL OF THE OTHER PERFORMANCES ARE\nREPORTED AS REFERENCE , WRITTEN IN CURSIVE .\nLSTM-based TF-based\nIndividual Social Soc.+ map Ind.\nS-GAN-ind S-GAN Trajectron++ Soc-BIGAT TF q\n[5] [5] [7] [6]\nETH 0.81/1.52 0.87/1.62 0.35/0.77 0.69/1.29 0.61 / 1.12\nHotel 0.72/1.61 0.67/1.37 0.18/0.38 0.49/1.01 0.18 / 0.30\nUCY 0.60/1.26 0.76/1.52 0.22/0.48 0.55/1.32 0.35 / 0.65\nZara1 0.34/0.69 0.35/0.68 0.14/0.28 0.30/0.62 0.22 / 0.38\nZara2 0.42/0.84 0.42/0.84 0.14/0.30 0.36/0.75 0.17 / 0.32\nAvg 0.58/1.18 0.61/1.21 0.21/0.45 0.48/1.00 0.31 / 0.55\nto include and model as much information as possible. The\nthree leading techniques of S-GAN and Trajectron are in fact\n“social”, and one of the best performing ones, Social-BIGAT,\nadditionally ingests the semantic map of the environment\n(“+map”). Additionally, note that best results are obtained by\nsampling 20 multiple plausible futures and selecting the best\none according to best test performance. We dub this here the\nbest-of-20 protocol, which any technique in Table II adopts.\nThe rightmost column in Table II shows our proposed\nTFq model, the only which allow to sample distributions of\ntrajectories. TF q achieves the second best performance, only\n0.10 behind in terms of MAD and 0.10 in terms of FAD.\nConsistently with the TrajNet challenge, an individual\nforecasting TF q technique yields a performance surprisingly\nahead or comparable with the best social techniques, even\nif enclosing additional map information. And trend is\nalso reﬂected by S-GAN [5], slightly under-performing its\nindividual counterpart.\nNote that the best-of-20 protocol is a sort of upper-bound\nreachable by sampling-based approaches; therefore, we an-\nalyze the behavior of our Transformer-based predictors TF\nin the single-trajectory deterministic regime as in [7], where\neach method gives a single prediction. Results are reported in\nTable III.\nThe message is clear: when it comes to individual ap-\nproaches, the transformer predictor is better than any indi-\nvidual LSTM-based approach. Notably, TF is better than the\nSocial-LSTM [5], and it outperforms the Social Attention [6]\nin terms of FAD too, by a large margin. Notably, the only\ncase in which LSTM compares favorably with TF is on Zara1,\nwhich is the less structured of the datasets of the benchmark,\nmostly containing straight lines.\nC. Ablation study and qualitative results\nHere we conduct an ablation study on the proposed TF\nmodel for forecasting, compare it with the LSTM, and ﬁnally\nillustrate qualitative results.\nTABLE III\nCOMPARISON AGAINST SOA MODELS FOLLOWING THE SINGLE TRAJECTORY DETERMINISTIC PROTOCOL (NUMBERS OF OTHER APPROACHES ARE TAKEN\nFROM [7] ). R EGULAR FONT INDICATES APPROACHES WHICH ARE COMPARABLE WITH OUR TRANSFORMER -BASED PREDICTORS , SINCE THEY USE A\nSINGLE INDIVIDUAL OBSERVED TRAJECTORY AS INPUT . THE OTHER APPROACHES HAVE PERFORMANCE IN ITALIC , AND ARE DISPLAYED AS A\nREFERENCE .\nLinear LSTM-based TF-based\nIndividual Individual Social Soc.+ map Individual\nInterpolat. LSTM S-GAN-ind Social Soc. Trajectron++ Trasformer\n[5] [5] LSTM [5] Att. [6] [7] TF (ours)\nETH 1.33/2.94 1.09/2.94 1.13/2.21 1.09/2.35 0.39/3.74 0.50/1.19 1.03/2.10\nHotel 0.39/0.72 0.86/1.91 1.01/2.18 0.79/1.76 0.29/2.64 0.24/0.59 0.36/0.71\nUCY 0.82/1.59 0.61/1.31 0.60/ 1.28 0.67/1.40 0.20/0.52 0.36/0.89 0.53 /1.32\nZara1 0.62/1.21 0.41/0.88 0.42/0.91 0.47/1.00 0.30/2.13 0.29/0.72 0.44/1.00\nZara2 0.77/1.48 0.52/1.11 0.52/1.11 0.56/1.17 0.33/3.92 0.27/0.67 0.34/0.76\navg 0.79/1.59 0.70/1.52 0.74/1.54 0.72/1.54 0.30/2.59 0.34/0.84 0.54/1.17\n1) Changing the Prediction Lengths : As a ﬁrst study case,\nwe compare the stability of the TF and LSTM models when\npredicting longer temporal horizons. Unfortunately, TrajNet\ndoes not allow to set the prediction horizon. We set therefore\nto pursue a test-time experiment of models trained on the large\nand complex TrajNet on longer video dataset. We collect these\nfrom the 5 datasets of ETH+UCY , by selecting those datasets\nwhich are not part of the TrajNet training set, namely ETH\nand Zara01. In Table IV-C1, we vary the observation sequence,\nfrom 12 frames (4.8s) to 32 frames (12.8) at a step of 1.8s.\nBoth TF and LSTM have been trained one-dataset-out with\ntraining sequences of 8 samples and 12 for the prediction.\nTABLE IV\nMAD AND FAD ERRORS WHEN LETTING THE TF AND THE LSTM\nMODELS PREDICT LONGER HORIZONS , I.E. FROM 12 TO 32 TIME STEPS .\nBOTH MODELS WERE TRAINED ON THE TRAJ NET TRAIN SET , WHILE\nERRORS ARE REPORTED OVER THE UNION OF ETH AND ZARA 1\nSEQUENCES (NOT PART OF THE TRAJ NET TRAIN SET ).\nPred. TF (ours) LSTM [41]\nMAD / FAD MAD / FAD\n12 0.71/1.56 0.78/1.70\n16 0.95/2.15 1.15/2.72\n20 1.27/2.90 1.64/3.99\n24 1.66/3.76 2.29/5.55\n28 2.27/5.09 3.07/7.46\n32 2.98/4.52 4.13/9.96\nOn Table IV are reported the average MAD and FAD values\nover the ETH-univ and UCY-zara1. Obviously, performances\nare generally decreasing. TF has a consistent advantage at\nevery horizon Vs. LSTM and the decrease with the horizon\nof LSTM is approximately 25% worse, as LSTM degrades\nfrom 0.78 to 4.13 MAD, while TF degrades from 0.71 to 2.98\nMAD.\n2) Missing and noisy data : To the best of our knowledge,\nthe problem of having missing coordinates in coordinate-based\nlong-term forecasting 2 has been never taken into account.\nOn the contrary, the problem of missing data is common\nin short term-forecasting ( i.e. tracking [44]), or forecasting\nof heterogeneous data [45], [46], [47], [48], [49], where in\ngeneral is treated by designing ad-hoc extensions for ﬁlling\nproperly the missing entries (the so called hindsighting [45]).\nCompared to these techniques, our transformer architecture\nrepresents a novel view, since it does not need to ﬁll missing\ndata; instead, it exploits the remaining samples knowing when\nthey have been observed thanks to the positional encoding. For\nexample, supposing the t−kth sample being missed in the\nobservation sequence, the transformer will use the remaining\nt−7,...,t −k−1,t−k+1,...,t , with 1 ≤k≤8 to perform\nthe prediction of t+ 1,...,t + 12. This structural ability is\nabsent in LSTM and RNN in general (they cannot work with\nmissing data), and in this sense the Transformer is superior.\nIf replacements of missing values can be computed, we found\nthat simple linear interpolation gives slight improvements to\nthe results.\nHaving witnessed the superiority of the transformer over\nLSTM in absolute sense (on TrajNet, and see Tab. I) and\nvarying the forecasting horizons (Sec. IV-C1), we continue\nthis analysis focusing on our proposed model on the same\nTrajNet dataset. The idea is to systematically drop one element\nat observation time, at a ﬁxed position, from the most recent\n(time t, indicated also as the current frame, after that it starts\nthe prediction) to the furthest ( t−7). Results are reported in\nTab. V.\nThe results show that, in a complex scenario such as\nTrajNet, dropping input frames impact the prediction per-\nformance, matching the intuition: the more dropped frames,\nthe larger the performance decrease. Interestingly, the current\nframe plays a key importance, as it is the most recent observed\ninput, from which future predictions start. In fact, dropping the\ncurrent frame together with the most recent 6 nearly doubles\n2Coordinate-based forecasting takes as input ﬂoor coordinates of people,\nand is different to image-based forecasting, where images are processed to\nextract bounding boxes locations on the image plane such as [43].\nTABLE V\nEVALUATION OF MISSING DATA RESULTS FOR TF ON TRAJ NET. WE\nEXPERIMENT DROPPING A VARYING NUMBER OF MOST RECENT\nOBSERVED SAMPLES , EITHER INCLUDING OR EXCLUDING THE CURRENT\nFRAME . FOR EXAMPLE , IN THE CASE OF DROPPING 3 FRAMES , WE DROP\nTobs = {x(i)\nt }0\nt=−2) AND Tobs = {x(i)\nt }−1\nt=−3) RESPECTIVELY .\n# most recent Drop most recent obs. Drop most recent obs.\nframes dropped includingcurrent frame excludingcurrent frame\n(FAD/MAD) (FAD/MAD)\n0 1.197 / 0.356 1.197 / 0.356\n1 1.305/ 0.389 1.267 / 0.373\n2 1.409 / 0.429 1.29 / 0.38\n3 1.602 / 0.495 1.303 / 0.384\n4 1.787 / 0.557 1.313 / 0.387\n5 1.897/ 0.593 1.327 / 0.329\n6 2.128 / 0.669 1.377 / 0.406\nthe error, i.e. degrades performance by 91%, from 0.356 to\n0.669 MAD. By contrast dropping 6 observed frames but\nkeeping the current one only degrades the performance by\n16% (from 0.356 to 0.406 MAD), although the TF may now\nonly leverage 2 observations (farther and closest in time).\n3) Qualitative results : Qualitative results can further mo-\ntivate the numerical results presented so far. In Fig. 3, we\nreport two predictions assessed on TrajNet, built by using\nthe ofﬁcial visualizer of the benchmark. In particular, we\nartiﬁcially superpose the predicted trajectories of LSTM and\nTF to highlight their different behavior. In Fig. 3 a), the subject\nis going south, with a minimal acceleration (not immediately\nvisible by the ﬁgure, but numerically present); LSTM takes\nthis gentle acceleration, predicting a uniform acceleration\ntoward south. TF captures better the dynamics, despite at the\nvery end the ﬁnal direction is not correct.\nIn Fig. 3 b) a similar behavior caused LSTM to predict a\nfaster straight trajectory, while TF followed in this case the\nbending of the GT more precisely.\nIn general, we observed that LSTM generates trajectories\nway more regular than those predicted by TF, and this\nis certainly motivated by its unrolling, opposed to the en-\ncoder+decoder architecture of TF. This is also the reason\nwhy LSTM is so effective on Zara1, consisting essentially\nin straight trajectories, and so scarce on Hotel (and in general\non TrajNet) if compared to TF.\nTo further motivate this, in Fig. 3 c) and d), we show\n100 sampled trajectories by TF q on Zara1, for two different\ncases. Fig. 3 c) presents essentially a monomodal distribution,\nwith the samples concentrated around the GT, enriched by\nfew articulated trajectories, that have low probability (they are\nfew), but are still plausible. Fig. 3 d) shows that TF has learnt\na multimodal distribution, which has at least three modes, one\nturning north, the other going diagonal, the third (with larger\nnumber of trajectories) going east.\nV. C ONCLUSIONS\nWe have proposed the use of Transformers Networks, based\non attention mechanisms, to predict people future trajectories.\nThe Transformers, state-of-the-art on all NLP tasks, also\nperform best on trajectory forecasting. We believe that this\nquestions the widespread use of LSTMs for modelling people\nmotion and that this questions the current formulation of\ncomplex social and environmental interactions, which our\nmodel does not need for best performance.\nIn addition to achieving the best performance on people\nforecasting datasets, the proposed Transfomers have shown\nbetter long-term prediction behavior, the capability to predict\nsensible multiple future trajectories and the unique feature\nof coping with missing input observations, as it may happen\nwhen dealing with real sensor data. Equipped with the better\ntemporal models, we envisage potential to address even larger\ndatasets of long-term sequences, where the importance of\nsocial terms may play more crucial roles.\nVI. A CKNOWLEDGMENTS\nThis work is partially supported by the Italian MIUR\nthrough PRIN 2017 - Project Grant 20172BH297: I-MALL\n- improving the customer experience in stores by intelligent\ncomputer vision, and by the project of the Italian Ministry of\nEducation, Universities and Research (MIUR) ”Dipartimenti\ndi Eccellenza 2018-2022”.\nREFERENCES\n[1] A. Sadeghian, V . Kosaraju, A. Gupta, S. Savarese, and A. Alahi,\n“Trajnet: Towards a benchmark for human trajectory prediction,” arXiv\npreprint, 2018.\na) b) c) d)\n1\n2 3\nFig. 3. Qualitative results: a) and b) showcasing failures of LSTM, c) and d) illustrating the trajectory distributions learned by TF q. Best viewed in colors.\n[2] S. Pellegrini, A. Ess, K. Schindler, and L. Van Gool, “You’ll never walk\nalone: Modeling social behavior for multi-target tracking,” in ICCV,\n2009.\n[3] A. Lerner, Y . Chrysanthou, and D. Lischinski, “Crowds by example,” in\nComputer Graphics Forum, 2007.\n[4] A. Alahi, K. Goel, V . Ramanathan, A. Robicquet, L. Fei-Fei, and\nS. Savarese, “Social LSTM: Human trajectory prediction in crowded\nspaces,” in CVPR, 2016.\n[5] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, “Social gan:\nSocially acceptable trajectories with generative adversarial networks,” in\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2018, cONF.\n[6] V . Kosaraju, A. Sadeghian, R. Mart ´ın-Mart´ın, I. Reid, H. Rezatoﬁghi,\nand S. Savarese, “Social-bigat: Multimodal trajectory forecasting using\nbicycle-gan and graph attention networks,” in Advances in Neural\nInformation Processing Systems , 2019, pp. 137–146.\n[7] T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone, “Trajectron++:\nMulti-agent generative trajectory forecasting with heterogeneous data for\ncontrol,” arXiv preprint arXiv:2001.03093 , 2020.\n[8] B. Ivanovic and M. Pavone, “The trajectron: Probabilistic multi-agent\ntrajectory modeling with dynamic spatiotemporal graphs,” in Proceed-\nings of the IEEE International Conference on Computer Vision , 2019,\npp. 2375–2384.\n[9] A. Sadeghian, V . Kosaraju, A. Sadeghian, N. Hirose, H. Rezatoﬁghi, and\nS. Savarese, “Sophie: An attentive gan for predicting paths compliant to\nsocial and physical constraints,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2019, pp. 1349–1358.\n[10] S. Bai, J. Z. Kolter, and V . Koltun, “An empirical evaluation of generic\nconvolutional and recurrent networks for sequence modeling,” arXiv\npreprint arXiv:1803.01271, 2018.\n[11] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time end-\nto-end 3d detection, tracking and motion forecasting with a single\nconvolutional net,” inProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2018, pp. 3569–3577.\n[12] C. Sch ¨oller, V . Aravantinos, F. Lay, and A. Knoll, “What the constant\nvelocity model can teach us about pedestrian motion prediction,” IEEE\nRobotics and Automation Letters , 2020.\n[13] S. Becker, R. Hug, W. Hubner, and M. Arens, “Red: A simple but\neffective baseline predictor for the trajnet benchmark,” in Proceedings\nof the European Conference on Computer Vision (ECCV), 2018, pp. 0–0.\n[14] S. Becker, R. Hug, W. H ¨ubner, and M. Arens, “An evaluation of\ntrajectory prediction approaches and notes on the trajnet benchmark,”\narXiv preprint arXiv:1805.07663 , 2018.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Transformer attention is all you need,” in\nNIPS, 2017.\n[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert pre-training\nof deep bidirectional transformers for language understanding,” 2019.\n[17] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long\nsequences with sparse transformers,” arXiv preprint:1904.10509, 2019.\n[18] B. T. Morris and M. M. Trivedi, “A survey of vision-based trajectory\nlearning and analysis for surveillance,” IEEE Trans. on Circuits and\nSystems for Video Technology, vol. 18, no. 8, pp. 1114–1127, 2008.\n[19] P. McCullagh and J. A. Nelder, “Generalized linear models, no. 37 in\nmonograph on statistics and applied probability,” 1989.\n[20] J. Qui ˜nonero-Candela and C. E. Rasmussen, “A unifying view of sparse\napproximate gaussian process regression,” Journal of Machine Learning\nResearch, vol. 6, no. 12, pp. 1939–1959, 2005.\n[21] C. K. I. Williams, “Prediction with gaussian processes: From linear\nregression to linear prediction and beyond,” in Learning in graphical\nmodels. Springer, 1998, pp. 599–621.\n[22] M. B. Priestley, Spectral analysis and time series . Academic press,\n1981.\n[23] H. Akaike, “Fitting autoregressive models for prediction,” Annals of the\ninstitute of Statistical Mathematics , vol. 21, no. 1, pp. 243–247, 1969.\n[24] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[25] A. Vemula, K. Muelling, and J. Oh, “Social attention: Modeling attention\nin human crowds,” in 2018 IEEE international Conference on Robotics\nand Automation (ICRA) . IEEE, 2018, pp. 1–7.\n[26] A. Sadeghian, A. Alahi, and S. Savarese, “Tracking the untrackable:\nLearning to track multiple cues with long-term dependencies,” arXiv\npreprint arXiv:1701.01909, 2017.\n[27] H. Su, J. Zhu, Y . Dong, and B. Zhang, “Forecast the plausible paths in\ncrowd scenes,” in Proceedings of the Twenty-Sixth International Joint\nConference on Artiﬁcial Intelligence, IJCAI-17 , 2017, pp. 2772–2778.\n[Online]. Available: https://doi.org/10.24963/ijcai.2017/386\n[28] H. Su, Y . Dong, J. Zhu, H. Ling, and B. Zhang, “Crowd scene\nunderstanding with coherent recurrent neural networks,” in IJCAI, 2016.\n[29] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta a robustly optimized bert\npretraining approach,” arXiv:1907.11692, 2019.\n[30] P. Zhang, W. Ouyang, P. Zhang, J. Xue, and N. Zheng, “Sr-lstm:\nState reﬁnement for lstm towards pedestrian trajectory prediction,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 12 085–12 094.\n[31] A. Graves, “Generating sequences with recurrent neural networks,” arXiv\npreprint arXiv:1308.0850, 2013.\n[32] A. Rudenko, L. Palmieri, M. Herman, K. M. Kitani, D. M. Gavrila, and\nK. O. Arras, “Human motion trajectory prediction: A survey,” arXiv\npreprint arXiv:1905.06113, 2019.\n[33] J. Ferryman and A. Shahrokni, “Pets2009: Dataset and challenge,” in\n2009 Twelfth IEEE International Workshop on Performance Evaluation\nof Tracking and Surveillance , Dec 2009, pp. 1–6.\n[34] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learning\nsocial etiquette: Human trajectory understanding in crowded scenes,”\nin European conference on computer vision . Springer, 2016, pp. 549–\n565.\n[35] N. Deo and M. M. Trivedi, “Trajectory forecasts in unknown\nenvironments conditioned on grid-based plans,” arXiv preprint\narXiv:2001.00735, 2020.\n[36] S. Haddad and S.-K. Lam, “Self-growing spatial graph networks for\npedestrian trajectory prediction,” in The IEEE Winter Conference on\nApplications of Computer Vision , 2020, pp. 1151–1159.\n[37] D. Ridel, N. Deo, D. Wolf, and M. Trivedi, “Scene compliant trajectory\nforecast with agent-centric spatio-temporal grids,” IEEE Robotics and\nAutomation Letters, vol. 5, no. 2, pp. 2816–2823, 2020.\n[38] T. van der Heiden, N. S. Nagaraja, C. Weiss, and E. Gavves, “Safecritic:\nCollision-aware trajectory prediction,” arXiv preprint arXiv:1910.06673,\n2019.\n[39] D. Helbing and P. Molnar, “Social force model for,” Physical review E,\nvol. 51, no. 5, p. 4282, 1995.\n[40] I. Hasan, F. Setti, T. Tsesmelis, A. Del Bue, F. Galasso, and M. Cristani,\n“Mx-lstm: mixing tracklets and vislets to jointly forecast trajectories and\nhead poses,” in CVPR, 2018.\n[41] “LSTM MATLAB implementation,” https://it.mathworks.com/help/\ndeeplearning/ug/long-short-term-memory-networks.html, accessed:\n2019-11-08.\n[42] P. Trautman and A. Krause, “Unfreezing the robot: Navigation in dense,\ninteracting crowds,” in IROS, 2010.\n[43] K. Kitani, B. Ziebart, J. Bagnell, and M. Hebert, “Activity forecasting,”\nin ECCV, 2012.\n[44] S.-H. Bae and K.-J. Yoon, “Conﬁdence-based data association and\ndiscriminative deep appearance learning for robust online multi-object\ntracking,” IEEE transactions on pattern analysis and machine intelli-\ngence, vol. 40, no. 3, pp. 595–610, 2017.\n[45] O. Anava, E. Hazan, and A. Zeevi, “Online time series prediction with\nmissing data,” in International Conference on Machine Learning , 2015,\npp. 2191–2199.\n[46] H. Chen, S. Grant-Muller, L. Mussone, and F. Montgomery, “A study\nof hybrid neural network approaches and the effects of missing data on\ntrafﬁc forecasting,” Neural Computing & Applications , vol. 10, no. 3,\npp. 277–286, 2001.\n[47] P. C. Rodrigues and M. De Carvalho, “Spectral modeling of time series\nwith missing data,” Applied Mathematical Modelling, vol. 37, no. 7, pp.\n4676–4684, 2013.\n[48] M. M. Ghazi, M. Nielsen, A. Pai, M. J. Cardoso, M. Modat, S. Ourselin,\nand L. Sørensen, “Robust training of recurrent neural networks to\nhandle missing data for disease progression modeling,” arXiv preprint\narXiv:1808.05500, 2018.\n[49] N. Golyandina and E. Osipov, “The “caterpillar”-ssa method for analysis\nof time series with missing values,” Journal of Statistical planning and\nInference, vol. 137, no. 8, pp. 2642–2653, 2007."
}