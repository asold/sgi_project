{
  "title": "CAWET: Context-Aware Worst-Case Execution Time Estimation Using Transformers",
  "url": "https://openalex.org/W2911109671",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2352997497",
      "name": "Dai, Zihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229690813",
      "name": "Yang Zhilin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098367636",
      "name": "Yang Yiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288898025",
      "name": "Carbonell, Jaime",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202120575",
      "name": "Le, Quoc V.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3177323215",
      "name": "Salakhutdinov, Ruslan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2894175714",
    "https://openalex.org/W2795285343",
    "https://openalex.org/W2951210602",
    "https://openalex.org/W1800356822",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W2145543707",
    "https://openalex.org/W2778817245",
    "https://openalex.org/W2952723479",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2207587218",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2964059481",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2792764867",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2798702047",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2785366763",
    "https://openalex.org/W2540404261",
    "https://openalex.org/W2963573053",
    "https://openalex.org/W1525783482",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2886490473",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2900096133",
    "https://openalex.org/W2605203995",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2964269252",
    "https://openalex.org/W2952276042",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2767321762",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2804845563",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2891815651",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2326533993",
    "https://openalex.org/W2964347220",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W2118776487",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2793273050",
    "https://openalex.org/W2402302915",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W1591801644"
  ],
  "abstract": "This paper presents CAWET, a hybrid worst-case program timing estimation technique. CAWET identifies the longest execution path using static techniques, whereas the worst-case execution time (WCET) of basic blocks is predicted using an advanced language processing technique called Transformer-XL. By employing Transformers-XL in CAWET, the execution context formed by previously executed basic blocks is taken into account, allowing for consideration of the micro-architecture of the processor pipeline without explicit modeling. Through a series of experiments on the TacleBench benchmarks, using different target processors (Arm Cortex M4, M7, and A53), our method is demonstrated to never underestimate WCETs and is shown to be less pessimistic than its competitors.",
  "full_text": "Transformer-XL: Attentive Language Models\nBeyond a Fixed-Length Context\nZihang Dai∗12, Zhilin Yang∗12, Yiming Yang1, Jaime Carbonell1,\nQuoc V . Le2, Ruslan Salakhutdinov1\n1Carnegie Mellon University, 2Google Brain\n{dzihang,zhiliny,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com\nAbstract\nTransformers have a potential of learning\nlonger-term dependency, but are limited by a\nﬁxed-length context in the setting of language\nmodeling. We propose a novel neural ar-\nchitecture Transformer-XL that enables learn-\ning dependency beyond a ﬁxed length with-\nout disrupting temporal coherence. It con-\nsists of a segment-level recurrence mechanism\nand a novel positional encoding scheme. Our\nmethod not only enables capturing longer-term\ndependency, but also resolves the context frag-\nmentation problem. As a result, Transformer-\nXL learns dependency that is 80% longer than\nRNNs and 450% longer than vanilla Trans-\nformers, achieves better performance on both\nshort and long sequences, and is up to 1,800+\ntimes faster than vanilla Transformers during\nevaluation. Notably, we improve the state-of-\nthe-art results of bpc/perplexity to 0.99 on en-\nwiki8, 1.08 on text8, 18.3 on WikiText-103,\n21.8 on One Billion Word, and 54.5 on Penn\nTreebank (without ﬁnetuning). When trained\nonly on WikiText-103, Transformer-XL man-\nages to generate reasonably coherent, novel\ntext articles with thousands of tokens. Our\ncode, pretrained models, and hyperparameters\nare available in both Tensorﬂow and PyTorch1.\n1 Introduction\nLanguage modeling is among the important prob-\nlems that require modeling long-term dependency,\nwith successful applications such as unsupervised\npretraining (Dai and Le, 2015; Peters et al., 2018;\nRadford et al., 2018; Devlin et al., 2018). How-\never, it has been a challenge to equip neural\nnetworks with the capability to model long-term\ndependency in sequential data. Recurrent neu-\nral networks (RNNs), in particular Long Short-\n∗Equal contribution. Order determined by swapping the\none in Yang et al. (2017).\n1https://github.com/kimiyoung/\ntransformer-xl\nTerm Memory (LSTM) networks (Hochreiter and\nSchmidhuber, 1997), have been a standard solu-\ntion to language modeling and obtained strong\nresults on multiple benchmarks. Despite the\nwide adaption, RNNs are difﬁcult to optimize\ndue to gradient vanishing and explosion (Hochre-\niter et al., 2001), and the introduction of gat-\ning in LSTMs and the gradient clipping tech-\nnique (Graves, 2013) might not be sufﬁcient to\nfully address this issue. Empirically, previous\nwork has found that LSTM language models use\n200 context words on average (Khandelwal et al.,\n2018), indicating room for further improvement.\nOn the other hand, the direct connections be-\ntween long-distance word pairs baked in atten-\ntion mechanisms might ease optimization and en-\nable the learning of long-term dependency (Bah-\ndanau et al., 2014; Vaswani et al., 2017). Re-\ncently, Al-Rfou et al. (2018) designed a set of aux-\niliary losses to train deep Transformer networks\nfor character-level language modeling, which out-\nperform LSTMs by a large margin. Despite the\nsuccess, the LM training in Al-Rfou et al. (2018)\nis performed on separated ﬁxed-length segments\nof a few hundred characters, without any informa-\ntion ﬂow across segments. As a consequence of\nthe ﬁxed context length, the model cannot capture\nany longer-term dependency beyond the prede-\nﬁned context length. In addition, the ﬁxed-length\nsegments are created by selecting a consecutive\nchunk of symbols without respecting the sentence\nor any other semantic boundary. Hence, the model\nlacks necessary contextual information needed to\nwell predict the ﬁrst few symbols, leading to inef-\nﬁcient optimization and inferior performance. We\nrefer to this problem as context fragmentation.\nTo address the aforementioned limitations of\nﬁxed-length contexts, we propose a new architec-\nture called Transformer-XL (meaning extra long).\nWe introduce the notion of recurrence into our\narXiv:1901.02860v3  [cs.LG]  2 Jun 2019\ndeep self-attention network. In particular, instead\nof computing the hidden states from scratch for\neach new segment, we reuse the hidden states ob-\ntained in previous segments. The reused hidden\nstates serve as memory for the current segment,\nwhich builds up a recurrent connection between\nthe segments. As a result, modeling very long-\nterm dependency becomes possible because in-\nformation can be propagated through the recur-\nrent connections. Meanwhile, passing informa-\ntion from the previous segment can also resolve\nthe problem of context fragmentation. More im-\nportantly, we show the necessity of using relative\npositional encodings rather than absolute ones, in\norder to enable state reuse without causing tem-\nporal confusion. Hence, as an additional techni-\ncal contribution, we introduce a simple but more\neffective relative positional encoding formulation\nthat generalizes to attention lengths longer than the\none observed during training.\nTransformer-XL obtained strong results on ﬁve\ndatasets, varying from word-level to character-\nlevel language modeling. Transformer-XL is also\nable to generate relatively coherent long text arti-\ncles with thousands of tokens (see Appendix E),\ntrained on only 100M tokens.\nOur main technical contributions include intro-\nducing the notion of recurrence in a purely self-\nattentive model and deriving a novel positional en-\ncoding scheme. These two techniques form a com-\nplete set of solutions, as any one of them alone\ndoes not address the issue of ﬁxed-length con-\ntexts. Transformer-XL is the ﬁrst self-attention\nmodel that achieves substantially better results\nthan RNNs on both character-level and word-level\nlanguage modeling.\n2 Related Work\nIn the last few years, the ﬁeld of language mod-\neling has witnessed many signiﬁcant advances,\nincluding but not limited to devising novel ar-\nchitectures to better encode the context (Bengio\net al., 2003; Mikolov et al., 2010; Merity et al.,\n2016; Al-Rfou et al., 2018), improving regulariza-\ntion and optimization algorithms (Gal and Ghahra-\nmani, 2016) , speeding up the Softmax computa-\ntion (Grave et al., 2016a) , and enriching the output\ndistribution family (Yang et al., 2017).\nTo capture the long-range context in language\nmodeling, a line of work directly feeds a repre-\nsentation of the wider context into the network\nas an additional input. Existing works range\nfrom ones where context representations are man-\nually deﬁned (Mikolov and Zweig, 2012; Ji et al.,\n2015; Wang and Cho, 2015) to others that rely on\ndocument-level topics learned from data (Dieng\net al., 2016; Wang et al., 2017).\nMore broadly, in generic sequence modeling,\nhow to capture long-term dependency has been a\nlong-standing research problem. From this per-\nspective, since the ubiquitous adaption of LSTM,\nmany efforts have been spent on relieving the\nvanishing gradient problem, including better ini-\ntialization (Le et al., 2015), additional loss sig-\nnal (Trinh et al., 2018), augmented memory struc-\nture (Ke et al., 2018) and others that modify the in-\nternal architecture of RNNs to ease the optimiza-\ntion (Wu et al., 2016; Li et al., 2018). Different\nfrom them, our work is based on the Transformer\narchitecture and shows that language modeling as\na real-world task beneﬁts from the ability to learn\nlonger-term dependency.\n3 Model\nGiven a corpus of tokens x = ( x1,...,x T), the\ntask of language modeling is to estimate the joint\nprobability P(x), which is often auto-regressively\nfactorized as P(x) = ∏\ntP(xt |x<t). With the\nfactorization, the problem reduces to estimating\neach conditional factor. In this work, we stick to\nthe standard neural approach to modeling the con-\nditional probability. Speciﬁcally, a trainable neu-\nral network is used to encode the context x<t into\na ﬁxed size hidden state, which is multiplied with\nthe word embeddings to obtain the logits. The log-\nits are then fed into the Softmax function, yielding\na categorical probability distribution over the next\ntoken.\n3.1 Vanilla Transformer Language Models\nIn order to apply Transformer or self-attention to\nlanguage modeling, the central problem is how to\ntrain a Transformer to effectively encode an arbi-\ntrarily long context into a ﬁxed size representation.\nGiven inﬁnite memory and computation, a sim-\nple solution would be to process the entire con-\ntext sequence using an unconditional Transformer\ndecoder, similar to a feed-forward neural network.\nHowever, this is usually infeasible with the limited\nresource in practice.\nOne feasible but crude approximation is to split\nthe entire corpus into shorter segments of man-\nSegment 1\nx1 x2 x4x3\nSegment 2\nx8x5 x6 x7\n(a) Train phase.\nLimited Context\nx1 x2 x4x3 x5 x6\nLimited Context\nx2 x3 x5x4 x6x1\nLimited Context\nx3 x4 x6x5x2\nx1 (b) Evaluation phase.\nFigure 1: Illustration of the vanilla model with a segment length 4.\nageable sizes, and only train the model within\neach segment, ignoring all contextual information\nfrom previous segments. This is the idea adopted\nby Al-Rfou et al. (2018). We call it the vanilla\nmodel and visualize it in Fig. 1a. Under this\ntraining paradigm, information never ﬂows across\nsegments in either the forward or backward pass.\nThere are two critical limitations of using a ﬁxed-\nlength context. First, the largest possible depen-\ndency length is upper bounded by the segment\nlength, which is a few hundred on character-level\nlanguage modeling (Al-Rfou et al., 2018). There-\nfore, although the self-attention mechanism is less\naffected by the vanishing gradient problem com-\npared to RNNs, the vanilla model is not able to\nfully exploit this optimization advantage. Second,\nthough it is possible to use padding to respect the\nsentence or other semantic boundaries, in practice\nit has been standard practice to simply chunk long\ntext into ﬁxed-length segments due to improved\nefﬁciency (Peters et al., 2018; Devlin et al., 2018;\nAl-Rfou et al., 2018). However, simply chunking\na sequence into ﬁxed-length segments will lead to\nthe context fragmentation problem as discussed in\nSection 1.\nDuring evaluation, at each step, the vanilla\nmodel also consumes a segment of the same length\nas in training, but only makes one prediction at the\nlast position. Then, at the next step, the segment\nis shifted to the right by only one position, and the\nnew segment has to be processed all from scratch.\nAs shown in Fig. 1b, this procedure ensures that\neach prediction utilizes the longest possible con-\ntext exposed during training, and also relieves con-\ntext fragmentation issue encountered in training.\nHowever, this evaluation procedure is extremely\nexpensive. We will show that our proposed archi-\ntecture is able to substantially improve the evalua-\ntion speed.\n3.2 Segment-Level Recurrence with State\nReuse\nTo address the limitations of using a ﬁxed-length\ncontext, we propose to introduce a recurrence\nmechanism to the Transformer architecture. Dur-\ning training, the hidden state sequence computed\nfor the previous segment is ﬁxed and cached to\nbe reused as an extended context when the model\nprocesses the next new segment, as shown in Fig.\n2a. Although the gradient still remains within a\nsegment, this additional input allows the network\nto exploit information in the history, leading to an\nability of modeling longer-term dependency and\navoiding context fragmentation. Formally, let the\ntwo consecutive segments of length L be sτ =\n[xτ,1,··· ,xτ,L] and sτ+1 = [xτ+1,1,··· ,xτ+1,L]\nrespectively. Denoting the n-th layer hidden state\nsequence produced for the τ-th segment sτ by\nhn\nτ ∈ RL×d, where d is the hidden dimension.\nThen, the n-th layer hidden state for segmentsτ+1\nis produced (schematically) as follows,\n˜hn−1\nτ+1 =\n[\nSG(hn−1\nτ ) ◦hn−1\nτ+1\n]\n,\nqn\nτ+1, kn\nτ+1, vn\nτ+1 = hn−1\nτ+1 W⊤\nq , ˜hn−1\nτ+1 W⊤\nk , ˜hn−1\nτ+1 W⊤\nv ,\nhn\nτ+1 = Transformer-Layer(qn\nτ+1, kn\nτ+1, vn\nτ+1) .\nwhere the function SG(·) stands for stop-gradient,\nthe notation [hu ◦hv] indicates the concatenation\nof two hidden sequences along the length dimen-\nsion, and W· denotes model parameters. Com-\npared to the standard Transformer, the critical dif-\nference lies in that the key kn\nτ+1 and value vn\nτ+1\nare conditioned on the extended context ˜hn−1\nτ+1 and\nhence hn−1\nτ cached from the previous segment.\nWe emphasize this particular design by the green\npaths in Fig. 2a.\nWith this recurrence mechanism applied to ev-\nery two consecutive segments of a corpus, it es-\nsentially creates a segment-level recurrence in the\nhidden states. As a result, the effective context be-\ning utilized can go way beyond just two segments.\nHowever, notice that the recurrent dependency be-\ntween hn\nτ+1 and hn−1\nτ shifts one layer downwards\nx1 x2 x4x3 x8x5 x6 x7\nNew Segment\nx12x9 x10 x11\nFixed (No Grad)\nx1 x2 x4x3 x8x5 x6 x7\nFixed (No Grad) New Segment\n(a) Training phase.\nx1 x2 x4x3 x8x5 x6 x7 x12x9 x10 x11\nExtended Context (b) Evaluation phase.\nFigure 2: Illustration of the Transformer-XL model with a segment length 4.\nper-segment, which differs from the same-layer\nrecurrence in conventional RNN-LMs. Conse-\nquently, the largest possible dependency length\ngrows linearly w.r.t. the number of layers as well\nas the segment length, i.e., O(N ×L), as vi-\nsualized by the shaded area in Fig. 2b. This\nis analogous to truncated BPTT (Mikolov et al.,\n2010), a technique developed for training RNN-\nLMs. However, different from truncated BPTT,\nour method caches a sequence of hidden states in-\nstead of the last one, and should be applied to-\ngether with the relative positional encoding tech-\nnique described in Section 3.3.\nBesides achieving extra long context and re-\nsolving fragmentation, another beneﬁt that comes\nwith the recurrence scheme is signiﬁcantly faster\nevaluation. Speciﬁcally, during evaluation, the\nrepresentations from the previous segments can\nbe reused instead of being computed from scratch\nas in the case of the vanilla model. In our ex-\nperiments on enwiki8, Transformer-XL is up to\n1,800+ times faster than the vanilla model during\nevaluation (see Section 4).\nFinally, notice that the recurrence scheme does\nnot need to be restricted to only the previous seg-\nment. In theory, we can cache as many previous\nsegments as the GPU memory allows, and reuse\nall of them as the extra context when processing\nthe current segment. Thus, we can cache a prede-\nﬁned length- M old hidden states spanning (pos-\nsibly) multiple segments, and refer to them as the\nmemory mn\nτ ∈RM×d, due to a clear connection to\nthe memory augmented neural networks (Graves\net al., 2014; Weston et al., 2014). In our experi-\nments, we set M equal to the segment length dur-\ning training, and increase it by multiple times dur-\ning evaluation.\n3.3 Relative Positional Encodings\nWhile we found the idea presented in the pre-\nvious subsection very appealing, there is a cru-\ncial technical challenge we haven’t solved in or-\nder to reuse the hidden states. That is, how can\nwe keep the positional information coherent when\nwe reuse the states? Recall that, in the standard\nTransformer, the information of sequence order is\nprovided by a set of positional encodings, denoted\nas U ∈ RLmax×d, where the i-th row Ui corre-\nsponds to the i-th absolute position within a seg-\nment and Lmax prescribes the maximum possible\nlength to be modeled. Then, the actual input to the\nTransformer is the element-wise addition of the\nword embeddings and the positional encodings. If\nwe simply adapt this positional encoding to our\nrecurrence mechanism, the hidden state sequence\nwould be computed schematically by\nhτ+1 = f(hτ, Esτ+1 + U1:L)\nhτ = f(hτ−1, Esτ + U1:L),\nwhere Esτ ∈ RL×d is the word embedding se-\nquence of sτ, and f represents a transformation\nfunction. Notice that, both Esτ and Esτ+1 are as-\nsociated with the same positional encoding U1:L.\nAs a result, the model has no information to dis-\ntinguish the positional difference betweenxτ,j and\nxτ+1,j for any j = 1,...,L , resulting in a sheer\nperformance loss.\nIn order to avoid this failure mode, the funda-\nmental idea is to only encode the relative posi-\ntional information in the hidden states. Concep-\ntually, the positional encoding gives the model a\ntemporal clue or “bias” about how information\nshould be gathered, i.e., where to attend. For the\nsame purpose, instead of incorporating bias stati-\ncally into the initial embedding, one can inject the\nsame information into the attention score of each\nlayer. More importantly, it is more intuitive and\ngeneralizable to deﬁne the temporal bias in a rela-\ntive manner. For instance, when a query vectorqτ,i\nattends on the key vectors kτ,≤i, it does not need\nto know the absolute position of each key vector\nto identify the temporal order of the segment. In-\nstead, it sufﬁces to know the relative distance be-\ntween each key vectorkτ,j and itself qτ,i, i.e. i−j.\nPractically, one can create a set of relative posi-\ntional encodings R ∈RLmax×d, where the i-th row\nRi indicates a relative distance of ibetween two\npositions. By injecting the relative distance dy-\nnamically into the attention score, the query vector\ncan easily distinguish the representations of xτ,j\nand xτ+1,j from their different distances, making\nthe state reuse mechanism feasible. Meanwhile,\nwe won’t lose any temporal information, as the ab-\nsolute position can be recovered recursively from\nrelative distances.\nPreviously, the idea of relative positional encod-\nings has been explored in the context of machine\ntranslation (Shaw et al., 2018) and music gener-\nation (Huang et al., 2018). Here, we offer a dif-\nferent derivation, arriving at a new form of rel-\native positional encodings, which not only has a\none-to-one correspondence to its absolute coun-\nterpart but also enjoys much better generalization\nempirically (see Section 4). Firstly, in the standard\nTransformer (Vaswani et al., 2017), the attention\nscore between query qi and key vector kj within\nthe same segment can be decomposed as\nAabs\ni,j = E⊤\nxiW⊤\nq WkExj\n  \n(a)\n+ E⊤\nxiW⊤\nq WkUj\n  \n(b)\n+ U⊤\ni W⊤\nq WkExj\n  \n(c)\n+ U⊤\ni W⊤\nq WkUj\n  \n(d)\n.\nFollowing the idea of only relying on rela-\ntive positional information, we propose to re-\nparameterize the four terms as follows\nArel\ni,j = E⊤\nxiW⊤\nq Wk,EExj\n  \n(a)\n+ E⊤\nxiW⊤\nq Wk,RRi−j\n  \n(b)\n+ u⊤Wk,EExj\n  \n(c)\n+ v⊤Wk,RRi−j\n  \n(d)\n.\n•The ﬁrst change we make is to replace all ap-\npearances of the absolute positional embedding\nUj for computing key vectors in term (b) and\n(d) with its relative counterpart Ri−j. This es-\nsentially reﬂects the prior that only the relative\ndistance matters for where to attend. Note that\nR is a sinusoid encoding matrix (Vaswani et al.,\n2017) without learnable parameters.\n•Secondly, we introduce a trainable parameter\nu ∈Rd to replace the query U⊤\ni W⊤\nq in term\n(c). In this case, since the query vector is the\nsame for all query positions, it suggests that the\nattentive bias towards different words should re-\nmain the same regardless of the query position.\nWith a similar reasoning, a trainable parameter\nv ∈Rd is added to substitute U⊤\ni W⊤\nq in term\n(d).\n•Finally, we deliberately separate the two weight\nmatrices Wk,E and Wk,R for producing the\ncontent-based key vectors and location-based\nkey vectors respectively.\nUnder the new parameterization, each term has\nan intuitive meaning: term (a) represents content-\nbased addressing, term (b) captures a content-\ndependent positional bias, term (c) governs a\nglobal content bias, and (d) encodes a global po-\nsitional bias.\nIn comparison, the formulation in Shaw et al.\n(2018) only has terms (a) and (b), dropping the\ntwo bias terms (c) and (d). Moreover, Shaw et al.\n(2018) merge the multiplication WkR into a sin-\ngle trainable matrix ˆR, which abandons the induc-\ntive bias built into the original sinusoid positional\nencoding (Vaswani et al., 2017). In contrast, our\nrelative positional embedding R adapts the sinu-\nsoid formulation. As a beneﬁt of the inductive\nbias, a model trained on a memory of some certain\nlength can automatically generalize to a memory\nseveral times longer during evaluation.\nEquipping the recurrence mechanism with our\nproposed relative positional embedding, we ﬁnally\narrive at the Transformer-XL architecture. For\ncompleteness, we summarize the computational\nprocedure for a N-layer Transformer-XL with a\nsingle attention head here. For n= 1,...,N :\n˜hn−1\nτ =\n[\nSG(mn−1\nτ ) ◦hn−1\nτ\n]\nqn\nτ, kn\nτ, vn\nτ = hn−1\nτ Wn\nq\n⊤, ˜hn−1\nτ Wn\nk,E\n⊤, ˜hn−1\nτ Wn\nv\n⊤\nAn\nτ,i,j = qn\nτ,i\n⊤kn\nτ,j + qn\nτ,i\n⊤Wn\nk,RRi−j\n+ u⊤kτ,j + v⊤Wn\nk,RRi−j\nan\nτ = Masked-Softmax(An\nτ)vn\nτ\non\nτ = LayerNorm(Linear(an\nτ) +hn−1\nτ )\nhn\nτ = Positionwise-Feed-Forward(on\nτ)\nwith h0\nτ := Esτ deﬁned as the word embed-\nding sequence. In addition, it is worth mention-\ning that a naive way to compute A requires com-\nputing Wn\nk,RRi−j for all pairs (i,j), whose cost\nis quadratic w.r.t. the sequence length. How-\never, noticing that the value of i−j only ranges\nfrom zero to the sequence length, we show a sim-\nple computation procedure in Appendix B, which\nreduces the cost to be linear w.r.t. the sequence\nlength.\n4 Experiments\n4.1 Main Results\nWe apply Transformer-XL to a variety of datasets\non both word-level and character-level language\nModel #Param PPL\nGrave et al. (2016b) - LSTM - 48.7\nBai et al. (2018) - TCN - 45.2\nDauphin et al. (2016) - GCNN-8 - 44.9\nGrave et al. (2016b) - LSTM + Neural cache - 40.8\nDauphin et al. (2016) - GCNN-14 - 37.2\nMerity et al. (2018) - QRNN 151M 33.0\nRae et al. (2018) - Hebbian + Cache - 29.9\nOurs - Transformer-XL Standard 151M 24.0\nBaevski and Auli (2018) - Adaptive Input⋄ 247M 20.5\nOurs - Transformer-XL Large 257M 18.3\nTable 1: Comparison with state-of-the-art results on\nWikiText-103. ⋄indicates contemporary work.\nModel #Param bpc\nHa et al. (2016) - LN HyperNetworks 27M 1.34\nChung et al. (2016) - LN HM-LSTM 35M 1.32\nZilly et al. (2016) - RHN 46M 1.27\nMujika et al. (2017) - FS-LSTM-4 47M 1.25\nKrause et al. (2016) - Large mLSTM 46M 1.24\nKnol (2017) - cmix v13 - 1.23\nAl-Rfou et al. (2018) - 12L Transformer 44M 1.11\nOurs - 12L Transformer-XL 41M 1.06\nAl-Rfou et al. (2018) - 64L Transformer 235M 1.06\nOurs - 18L Transformer-XL 88M 1.03\nOurs - 24L Transformer-XL 277M 0.99\nTable 2: Comparison with state-of-the-art results on enwik8.\nmodeling to have a comparison with state-of-the-\nart systems, including WikiText-103 (Merity et al.,\n2016), enwik8 (LLC, 2009), text8 (LLC, 2009),\nOne Billion Word (Chelba et al., 2013), and Penn\nTreebank (Mikolov and Zweig, 2012).\nWikiText-103 is the largest available word-level\nlanguage modeling benchmark with long-term de-\npendency. It contains 103M training tokens from\n28K articles, with an average length of 3.6K to-\nkens per article, which allows testing the abil-\nity of long-term dependency modeling. We set\nthe attention length to 384 during training and\n1600 during evaluation. We adopted adaptive soft-\nmax and input representations (Baevski and Auli,\n2018; Grave et al., 2016a). As shown in Table 1,\nTransformer-XL reduces the previous state-of-the-\nart (SoTA) perplexity from 20.5 to 18.3, which\ndemonstrates the superiority of the Transformer-\nXL architecture.\nThe dataset enwik8 contains 100M bytes of un-\nprocessed Wikipedia text. We compare our ar-\nchitecture with the previous results in Table 2.\nUnder the model size constraint, the 12-layer\nTransformer-XL achieves a new SoTA result, out-\nperforming the 12-layer vanilla Transformer from\nAl-Rfou et al. (2018) by 0.05, while both Trans-\nModel #Param bpc\nCooijmans et al. (2016) - BN-LSTM - 1.36\nChung et al. (2016) - LN HM-LSTM 35M 1.29\nZilly et al. (2016) - RHN 45M 1.27\nKrause et al. (2016) - Large mLSTM 45M 1.27\nAl-Rfou et al. (2018) - 12L Transformer 44M 1.18\nAl-Rfou et al. (2018) - 64L Transformer 235M 1.13\nOurs - 24L Transformer-XL 277M 1.08\nTable 3: Comparison with state-of-the-art results on text8.\nModel #Param PPL\nShazeer et al. (2014) - Sparse Non-Negative 33B 52.9\nChelba et al. (2013) - RNN-1024 + 9 Gram 20B 51.3\nKuchaiev and Ginsburg (2017) - G-LSTM-2 - 36.0\nDauphin et al. (2016) - GCNN-14 bottleneck - 31.9\nJozefowicz et al. (2016) - LSTM 1.8B 30.6\nJozefowicz et al. (2016) - LSTM + CNN Input 1.04B 30.0\nShazeer et al. (2017) - Low-Budget MoE ∼5B 34.1\nShazeer et al. (2017) - High-Budget MoE ∼5B 28.0\nShazeer et al. (2018) - Mesh Tensorﬂow 4.9B 24.0\nBaevski and Auli (2018) - Adaptive Input⋄ 0.46B 24.1\nBaevski and Auli (2018) - Adaptive Input⋄ 1.0B 23.7\nOurs - Transformer-XL Base 0.46B 23.5\nOurs - Transformer-XL Large 0.8B 21.8\nTable 4: Comparison with state-of-the-art results on One Bil-\nlion Word. ⋄indicates contemporary work.\nformer variants have a large margin over conven-\ntional RNN-based models. Notably, our 12-layer\narchitecture achieves the same result as the 64-\nlayer network from Al-Rfou et al. (2018), using\nonly 17% of the parameter budget. In order to see\nwhether better performances can be obtained by\nincreasing the model size, we train 18-layer and\n24-layer Transformer-XLs with increased model\nsizes. With the attention length 784 during train-\ning and 3,800 during evaluation, we obtained a\nnew SoTA result and our method is the ﬁrst to\nbreak through 1.0 on widely-studied character-\nlevel benchmarks. Different from Al-Rfou et al.\n(2018), Transformer-XL does not need any auxil-\niary losses, and thus all beneﬁts are credited to a\nbetter architecture.\nSimilar to but different from enwik8, text8 con-\ntains 100M processed Wikipedia characters cre-\nated by lowering case the text and removing any\ncharacter other than the 26 lettersa through z, and\nspace. Due to the similarity, we simply adapt the\nbest model and the same hyper-parameters on en-\nwik8 to text8 without further tuning. The compari-\nson with previous methods is summarized in Table\n3. Again, Transformer-XL achieves the new SoTA\nresult with a clear margin.\nModel #Param PPL\nInan et al. (2016) - Tied Variational LSTM 24M 73.2\nZilly et al. (2016) - Variational RHN 23M 65.4\nZoph and Le (2016) - NAS Cell 25M 64.0\nMerity et al. (2017) - AWD-LSTM 24M 58.8\nPham et al. (2018) - Efﬁcient NAS 24M 58.6\nLiu et al. (2018) - Differentiable NAS 23M 56.1\nYang et al. (2017) - AWD-LSTM-MoS 22M 55.97\nMelis et al. (2018) - Dropout tuning 24M 55.3\nOurs - Transformer-XL 24M 54.52\nMerity et al. (2017) - AWD-LSTM+Finetune† 24M 57.3\nYang et al. (2017) - MoS+Finetune† 22M 54.44\nTable 5: Comparison with state-of-the-art results on Penn\nTreebank. †indicates using two-step ﬁnetuning.\nOne Billion Word does not preserve any long-\nterm dependency because sentences have been\nshufﬂed. Consequently, this dataset mainly tests\nthe ability of modeling only short-term depen-\ndency. The comparison between Transformer-XL\nand the other methods is shown in Table 4. Al-\nthough Transformer-XL is mainly designed to bet-\nter capture longer-term dependency, it dramati-\ncally improves the single-model SoTA from 23.7\nto 21.8. Speciﬁcally, Transformer-XL signiﬁ-\ncantly outperforms a contemporary method using\nvanilla Transformers (Baevski and Auli, 2018),\nsuggesting the advantage of Transformer-XL is\ngeneralizable to modeling short sequences.\nWe also report the results on word-level Penn\nTreebank in Table 5. Similar to AWD-LSTM\n(Merity et al., 2017), we apply variational dropout\nand weight average to Transformer-XL. With\nproper regularization, Transformer-XL achieves a\nnew SoTA result among models without two-step\nﬁnetuning. Penn Treebank has only 1M training\ntokens, which implies that Transformer-XL also\ngeneralizes well even on small datasets.\n4.2 Ablation Study\nWe conduct two sets of ablation studies to exam-\nine the effects of two proposed techniques used in\nTransformer-XL: the recurrence mechanism and\nthe new positional encoding scheme.\nThe ﬁrst study is performed on WikiText-103,\nwhich requires modeling long-term dependency.\nThe results are reported in Table 6. Among the\ncompared encoding schemes, Shaw et al. (2018) is\nrelative, while Vaswani et al. (2017) and Al-Rfou\net al. (2018) are absolute. “Full” and “half” losses\nrefer to applying a cross entropy loss to all or the\nrecent half positions in the segment. We found\nthat absolute encodings only work well with half\nlosses because half losses exclude positions with\nvery short attention lengths during training for bet-\nter generalization. Table 6 shows that both the\nrecurrence mechanism and our encoding scheme\nare necessary to achieve the best performance, as\nwell as generalizing to longer attention sequences\nduring evaluation time. Although the backprop-\nagation length during training is only 128, with\nthe two techniques the attention length can be in-\ncreased to 640 at test time. In the standard setting\nwith 151M parameters, the perplexity decreases as\nthe attention length increases.\nSince the recurrence mechanism costs addi-\ntional memory, we also compare Transformer-XL\nwith baselines under the same GPU memory con-\nstraints. As shown in Table 10 in Appendix A,\ndespite using a shorter backpropagation length,\nTransformer-XL remains superior to the baselines.\nThe second study targets at isolating the ef-\nfects of resolving the context fragmentation prob-\nlem from the beneﬁt of capturing longer context\nlength. In order to achieve this goal, we deliber-\nately choose a dataset that does not require long-\nterm dependency, so that any improvement from\nestablishing the recurrence can be attributed to\nsolving the context fragmentation. Speciﬁcally,\nwe perform this controlled experiment on the One\nBillion Word dataset, which can only beneﬁt from\nremoving the context fragmentation. We train\na 20-layer Transformer-XL with ∼0.3B parame-\nters for 400K steps. As shown in Table 7, using\nsegment-level recurrence substantially improves\nperformance even when long-term dependency is\nnot needed, which is consistent with our previous\ndiscussion that the recurrence mechanism resolves\nthe context fragmentation problem. Moreover, our\nrelative positional encodings is also superior to\nShaw et al. (2018) on short sequences.\n4.3 Relative Effective Context Length\nKhandelwal et al. (2018) proposed a method to\nevaluate the Effective Context Length (ECL) of a\nsequence model. ECL is the longest length to\nwhich increasing the context span would lead to\na gain more than a threshold. However, ECL ig-\nnores the fact that it is harder to get improve-\nment when a model already achieves a lower per-\nplexity using only a shorter context, and thus it\nis not suitable for fair comparison among mul-\ntiple models. We instead propose a new metric\nRemark Recurrence Encoding Loss PPL init PPL best Attn Len\nTransformer-XL (128M) \u0013 Ours Full 27.02 26.77 500\n- \u0013 Shaw et al. (2018) Full 27.94 27.94 256\n- \u0013 Ours Half 28.69 28.33 460\n- \u0017 Ours Full 29.59 29.02 260\n- \u0017 Ours Half 30.10 30.10 120\n- \u0017 Shaw et al. (2018) Full 29.75 29.75 120\n- \u0017 Shaw et al. (2018) Half 30.50 30.50 120\n- \u0017 Vaswani et al. (2017) Half 30.97 30.97 120\nTransformer (128M)† \u0017 Al-Rfou et al. (2018) Half 31.16 31.16 120\nTransformer-XL (151M) \u0013 Ours Full 23.43\n23.09 640\n23.16 450\n23.35 300\nTable 6: Ablation study on WikiText-103. For the ﬁrst two blocks, we use a slightly smaller model (128M parameters).\n†indicates that the corresponding row is reduced to the same setting as the Transformer network in (Al-Rfou et al., 2018),\nexcept that two auxiliary losses are not implemented in our experiments. “PPL init” refers to using the same length as training.\n“PPL best” indicates the perplexity obtained by using the optimal length. “Attn Len” is the shortest possible attention length\nduring evaluation to achieve the corresponding result (PPL best). Increasing the attention length during evaluation improves\nperformance only when our positional encoding is used. The “Transformer-XL (151M)” setting uses a standard parameter\nbudget as previous work (Merity et al., 2018), where we observe a similar effect when increasing the attention length during\nevaluation.\nMethod PPL\nOurs 25.2\nWith Shaw et al. (2018) encodings 25.7\nWithout recurrence 27.1\nTable 7: Ablation study on One Billion Word, a dataset with-\nout long-term dependency.\nModel r = 0.1 r = 0.5 r = 1.0\nTransformer-XL 151M 900 800 700\nQRNN 500 400 300\nLSTM 400 300 200\nTransformer-XL 128M 700 600 500\n- use Shaw et al. (2018) encoding 400 400 300\n- remove recurrence 300 300 300\nTransformer 128 128 128\nTable 8: Relative effective context length (RECL) compari-\nson. See text for the deﬁnition of RECL and r. The ﬁrst three\nmodels and the last four models are compared as two model\ngroups when we calculate RECL (RECL is computed on a\nmodel group rather than a single model). Each group has the\nsame parameter budget.\ncalled Relative Effective Context Length (RECL).\nRECL is deﬁned on a model group instead of a\nsingle model, and the gain of a long context is\nmeasure by the relative improvement over thebest\nshort context model. As such, the model group\nshares the same baseline to enable fair compari-\nson. RECL also has a parameter r, which means\nconstraining the comparison on top- r hard exam-\nples. See Appedix C for more details about RECL.\nAs shown in Table 8, Transformer-XL manages\nto model dependency of 900 words long on av-\nAttn Len How much Al-Rfou et al. (2018) is slower\n3,800 1,874x\n2,800 1,409x\n1,800 773x\n800 363x\nTable 9: Slowdown in terms of running time during evalua-\ntion. Evaluation is based on per-token time on one GPU.\nerage with r = 0 .1. The RECL of Transformer-\nXL is 80% and 450% longer than recurrent net-\nworks and Transformer respectively. Both the re-\ncurrence mechanism and our positional encodings\ncontribute to a longer RECL. This further substan-\ntiates our argument that Transformer-XL is able to\nmodel longer-term dependency.\n4.4 Generated Text\nTrained only on WikiText-103 which is medium-\nsized, Transformer-XL is already able to generate\nrelatively coherent articles with thousands of to-\nkens without manual cherry picking, despite mi-\nnor ﬂaws. Please refer to Appendix E for samples.\n4.5 Evaluation Speed\nFinally, we compare the evaluation speed of our\nmodel with the vanilla Transformer model (Al-\nRfou et al., 2018). As shown in Table 9, due to\nthe state reuse scheme, Transformer-XL achieves\nan up to 1,874 times speedup during evaluation.\n5 Conclusions\nTransformer-XL obtains strong perplexity results,\nmodels longer-term dependency than RNNs and\nTransformer, achieves substantial speedup during\nevaluation, and is able to generate coherent text\narticles. We envision interesting applications of\nTransformer-XL in the ﬁelds of text generation,\nunsupervised feature learning, image and speech\nmodeling.\nAcknowledgments\nZD and YY were supported in part by National\nScience Foundation (NSF) under the grant IIS-\n1546329 and by the DOE-Ofﬁce of Science un-\nder the grant ASCR #KJ040201. ZY and RS\nwere supported in part by the Ofﬁce of Naval\nResearch grant N000141812861, the NSF grant\nIIS1763562, the Nvidia fellowship, and the Siebel\nscholarship.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv preprint arXiv:1809.10853.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.\n2018. An empirical evaluation of generic convolu-\ntional and recurrent networks for sequence model-\ning. arXiv preprint arXiv:1803.01271.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2016. Hierarchical multiscale recurrent neural net-\nworks. arXiv preprint arXiv:1609.01704.\nTim Cooijmans, Nicolas Ballas, César Laurent, Ça ˘glar\nGülçehre, and Aaron Courville. 2016. Re-\ncurrent batch normalization. arXiv preprint\narXiv:1603.09025.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nYann N Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2016. Language modeling with\ngated convolutional networks. arXiv preprint\narXiv:1612.08083.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAdji B Dieng, Chong Wang, Jianfeng Gao, and John\nPaisley. 2016. Topicrnn: A recurrent neural net-\nwork with long-range semantic dependency. arXiv\npreprint arXiv:1611.01702.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural information\nprocessing systems, pages 1019–1027.\nEdouard Grave, Armand Joulin, Moustapha Cissé,\nDavid Grangier, and Hervé Jégou. 2016a. Efﬁcient\nsoftmax approximation for gpus. arXiv preprint\narXiv:1609.04309.\nEdouard Grave, Armand Joulin, and Nicolas\nUsunier. 2016b. Improving neural language\nmodels with a continuous cache. arXiv preprint\narXiv:1612.04426.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\n2014. Neural turing machines. arXiv preprint\narXiv:1410.5401.\nDavid Ha, Andrew Dai, and Quoc V Le. 2016. Hyper-\nnetworks. arXiv preprint arXiv:1609.09106.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jür-\ngen Schmidhuber, et al. 2001. Gradient ﬂow in re-\ncurrent nets: the difﬁculty of learning long-term de-\npendencies.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Noam Shazeer, Curtis Hawthorne, An-\ndrew M Dai, Matthew D Hoffman, and Douglas Eck.\n2018. An improved relative self-attention mecha-\nnism for transformer with application to music gen-\neration. arXiv preprint arXiv:1809.04281.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv\npreprint arXiv:1611.01462.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2015. Document context lan-\nguage models. arXiv preprint arXiv:1511.03962.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\nAaron van den Oord, Alex Graves, and Koray\nKavukcuoglu. 2016. Neural machine translation in\nlinear time. arXiv preprint arXiv:1610.10099.\nSekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka,\nand Shuichi Adachi. 2018. Sigsoftmax: Reanal-\nysis of the softmax bottleneck. arXiv preprint\narXiv:1805.10829.\nNan Rosemary Ke, Anirudh Goyal ALIAS PARTH\nGOY AL, Olexa Bilaniuk, Jonathan Binas,\nMichael C Mozer, Chris Pal, and Yoshua Ben-\ngio. 2018. Sparse attentive backtracking: Temporal\ncredit assignment through reminding. In Advances\nin Neural Information Processing Systems , pages\n7650–7661.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Ju-\nrafsky. 2018. Sharp nearby, fuzzy far away: How\nneural language models use context. arXiv preprint\narXiv:1805.04623.\nBryon Knol. 2017. cmix v13. http://www.\nbyronknoll.com/cmix.html.\nJan Koutnik, Klaus Greff, Faustino Gomez, and Juer-\ngen Schmidhuber. 2014. A clockwork rnn. arXiv\npreprint arXiv:1402.3511.\nBen Krause, Liang Lu, Iain Murray, and Steve Renals.\n2016. Multiplicative lstm for sequence modelling.\narXiv preprint arXiv:1609.07959.\nOleksii Kuchaiev and Boris Ginsburg. 2017. Factor-\nization tricks for lstm networks. arXiv preprint\narXiv:1703.10722.\nQuoc V Le, Navdeep Jaitly, and Geoffrey E Hin-\nton. 2015. A simple way to initialize recurrent\nnetworks of rectiﬁed linear units. arXiv preprint\narXiv:1504.00941.\nShuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo\nGao. 2018. Independently recurrent neural network\n(indrnn): Building a longer and deeper rnn. In Pro-\nceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pages 5457–5466.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2018. Darts: Differentiable architecture search.\narXiv preprint arXiv:1806.09055.\nMultiMedia LLC. 2009. Large text compression\nbenchmark.\nGábor Melis, Charles Blundell, Tomáš Ko ˇcisk`y,\nKarl Moritz Hermann, Chris Dyer, and Phil Blun-\nsom. 2018. Pushing the bounds of dropout. arXiv\npreprint arXiv:1805.09208.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models. arXiv preprint arXiv:1708.02182.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An analysis of neural language\nmodeling at multiple scales. arXiv preprint\narXiv:1803.08240.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843.\nTomas Mikolov, Armand Joulin, Sumit Chopra,\nMichael Mathieu, and Marc’Aurelio Ranzato. 2014.\nLearning longer memory in recurrent neural net-\nworks. arXiv preprint arXiv:1412.7753.\nTomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nSLT, 12(234-239):8.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nAistats, volume 5, pages 246–252. Citeseer.\nAsier Mujika, Florian Meier, and Angelika Steger.\n2017. Fast-slow recurrent neural networks. In Ad-\nvances in Neural Information Processing Systems ,\npages 5915–5924.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2012. Understanding the exploding gradient prob-\nlem. CoRR, abs/1211.5063.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nHieu Pham, Melody Y Guan, Barret Zoph, Quoc V\nLe, and Jeff Dean. 2018. Efﬁcient neural architec-\nture search via parameter sharing. arXiv preprint\narXiv:1802.03268.\nOﬁr Press and Lior Wolf. 2016. Using the output\nembedding to improve language models. arXiv\npreprint arXiv:1608.05859.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nJack W Rae, Chris Dyer, Peter Dayan, and Tim-\nothy P Lillicrap. 2018. Fast parametric learn-\ning with activation memorization. arXiv preprint\narXiv:1803.10049.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin\nTran, Ashish Vaswani, Penporn Koanantakool, Peter\nHawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\nYoung, et al. 2018. Mesh-tensorﬂow: Deep learning\nfor supercomputers. In Advances in Neural Infor-\nmation Processing Systems, pages 10434–10443.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nNoam Shazeer, Joris Pelemans, and Ciprian Chelba.\n2014. Skip-gram language modeling using sparse\nnon-negative matrix probability estimation. arXiv\npreprint arXiv:1412.1454.\nTrieu H Trinh, Andrew M Dai, Thang Luong, and\nQuoc V Le. 2018. Learning longer-term dependen-\ncies in rnns with auxiliary losses. arXiv preprint\narXiv:1803.00144.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nTian Wang and Kyunghyun Cho. 2015. Larger-\ncontext language modelling. arXiv preprint\narXiv:1511.03729.\nWenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen,\nJiaji Huang, Wei Ping, Sanjeev Satheesh, and\nLawrence Carin. 2017. Topic compositional neural\nlanguage model. arXiv preprint arXiv:1712.09783.\nJason Weston, Sumit Chopra, and Antoine Bor-\ndes. 2014. Memory networks. arXiv preprint\narXiv:1410.3916.\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua\nBengio, and Ruslan R Salakhutdinov. 2016. On\nmultiplicative integration with recurrent neural net-\nworks. In Advances in neural information process-\ning systems, pages 2856–2864.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2017. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. arXiv\npreprint arXiv:1711.03953.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nJulian Georg Zilly, Rupesh Kumar Srivastava,\nJan Koutník, and Jürgen Schmidhuber. 2016.\nRecurrent highway networks. arXiv preprint\narXiv:1607.03474.\nBarret Zoph and Quoc V Le. 2016. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578.\nA Ablation Study with Memory Constraints\nBackprop Len Recurrence Encoding Loss pplx best pplx init Attn Len\n128 \u0013 Ours Full 26.77 27.02 500\n128 \u0013 Ours Partial 28.33 28.69 460\n176 \u0017 Ours Full 27.98 28.43 400\n172 \u0017 Ours Partial 28.83 28.83 120\nTable 10: Ablation study on WikiText-103 with the same GPU memory constraints.\nTable 10 compares Transformer-XL with baseline under the same memory budget. Transformer-XL\nstill outperforms the baseline even with a shorter backprop length.\nB Efﬁcient Computation of the Attention with Relative Positional Embedding\nAs we discussed in section 3.3, the naive way of computing the Wk,RRi−j for all pairs (i,j) is subject\nto a quadratic cost. Here, we present a simple method with only a linear cost. Firstly, notice that the\nrelative distance i−jcan only be integer from 0 to M+ L−1, where M and Lare the memory length\nand segment length respectively. Hence, the rows of the matrix\nQ :=\n\n\nR⊤\nM+L−1\nR⊤\nM+L−2\n...\nR⊤\n1\nR⊤\n0\n\nWk,R⊤=\n\n\n[Wk,RRM+L−1]⊤\n[Wk,RRM+L−2]⊤\n...\n[Wk,RR1]⊤\n[Wk,RR0]⊤\n\n∈R(M+L)×d\nconsist of all possible vector outputs of Wk,RRi−j for any (i,j). Note that we have deﬁned Q in a\nreversed order, i.e., Qk = Wk,RRM+L−1−k, to make further discussion easier.\nNext, we collect the term (b) for all possible i,j into the following L×(M + L) matrix,\nB =\n\n\nq⊤\n0 Wk,RRM ··· q⊤\n0 Wk,RR0 0 ··· 0\nq⊤\n1 Wk,RRM+1 ··· q⊤\n1 Wk,RR1 q⊤\n1 Wk,RR0 ··· 0\n... ... ... ... ... ...\nq⊤\nL−1Wk,RRM+L−1 ··· q⊤\nL−1Wk,RRM+L−1 q⊤\nL−1Wk,RRL−1 ··· q⊤\nL−1Wk,RR0\n\n\n=\n\n\nq⊤\n0 QL−1 ··· q⊤\n0 QM+L−1 0 ··· 0\nq⊤\n1 QL−2 ··· q⊤\n1 QM+L−2 q⊤\n1 QM+L−1 ··· 0\n... ... ... ... ... ...\nq⊤\nL−1Q0 ··· q⊤\nL−1QM q⊤\nL−1QM+1 ··· q⊤\nL−1QM+L−1\n\n\nThen, we further deﬁne\n˜B = qQ⊤=\n\n\nq⊤\n0 Q0 ··· q⊤\n0 QM q⊤\n0 QM+1 ··· q⊤\n0 QM+L−1\nq⊤\n1 Q0 ··· q⊤\n1 QM q⊤\n1 QM+1 ··· q⊤\n1 QM+L−1\n... ... ... ... ... ...\nq⊤\nL−1Q0 ··· q⊤\nL−1QM q⊤\nL−1QM+1 ··· q⊤\nL−1QM+L−1\n\n.\nNow, it is easy to see an immediate relationship between B and ˜B, where the i-th row of B is simply a\nleft-shifted version of i-th row of ˜B. Hence, the computation of B only requires a matrix multiplication\nqQ⊤to compute ˜B and then a set of left-shifts.\nSimilarly, we can collect all term (d) for all possible i,j into another L×(M + L) matrix D,\nD =\n\n\nv⊤QL−1 ··· v⊤QM+L−1 0 ··· 0\nv⊤QL−2 ··· v⊤QM+L−2 v⊤QM+L−1 ··· 0\n... ... ... ... ... ...\nv⊤Q0 ··· v⊤QM v⊤QM+1 ··· v⊤QM+L−1\n\n.\nThen, we can follow the same procedure to deﬁne\n˜d = [Qv]⊤=\n[\nv⊤Q0 ··· v⊤QM v⊤QM+1 ··· v⊤QM+L−1\n]\n.\nAgain, each row of D is simply a left-shift version of ˜d. Hence, the main computation cost comes from\nthe matrix-vector multiplication ˜d = [Qv]⊤, which is not expensive any more.\nC Details About RECL\n(a) Transformer-XL vs RNNs\n (b) Transformer-XL vs Baseline\nFigure 3: Visualizing unnormalized relative perplexity gains with r= 0.1.\n(a) Transformer-XL vs RNNs\n (b) Transformer-XL vs Baseline\nFigure 4: Perplexity vs context length.\nIn this section, we describe the details of the metric RECL. Let M= {m1,m2,··· ,mN}be a model\ngroup consisting of N models. Let li(c,t) denote the loss of model mi on the t-th token in the corpus\nwith a context length c. Concretely, the loss can be written as\nli(c,t) = −log Pmi(xt|xt−1,··· ,xt−c)\nwhere Pmi is the probability distribution given by modelmi, and xt is the t-th token in the corpus. Given\na short context length cand a long context length c′such that c′≥c, we can further deﬁne a baseline for\neach position t,\nb(c,t) =\nN\nmin\ni=1\nli(c,t)\nThe relative loss of mi w.r.t. the model group Mis written as\nfi(c,c′) = 1\n|T|\n∑\nt∈T\nmin\n(\nb(c,t),li(c′,t)\n)\nThe above equation uses the minimum loss of all models on the short length cas a baseline, and only\nlosses smaller than the baseline will be effectively counted towards the relative loss. This enables fair\ncomparison between multiple models because all models with a long context length c′need to improve\nover the same baseline. Sometimes we only care about those positions where the baseline performs\npoorly (which means short-term dependency with context length c is not sufﬁcient), so given a ratio\nparameter r, we deﬁne the set T is the above equation as\nT = top-rpositions twith largest b(c,t)\nThe relative gain is subsequently deﬁned as the relative perplexity reduction:\ngi(c,c′) = exp fi(c,c) −exp fi(c,c′)\nexp fi(c,c)\nGiven a step size ∆, we then use an algorithm to ﬁnd the RECL by thresholding the relative gain:\n1. Set initial short context length c, and long context length c′= c+ ∆\n2. Compute gi(c,c′). If gi(c,c′) <0.01, return RECL = c. If gi(c,c′) ≥0.01, set c= c′,c′= c+ ∆\nand go to step 1.\nIn Figure 3, we visualize the unnormalized relative perplexity gains (exp fi(c,c) −exp fi(c,c′)) with\nvarious pairs of (c,c′) when r = 0.1. It is clear that Transformer-XL has a longer RECL compared to\nRNNs and other baselines because the relative gains are substantially larger.\nFor reference, we plot the perplexities with varying context lengths in Figure 4. The y-axis denotes\nthe “normal” perplexity (not calibrated by baselines).\nD Attention Visualization\nIn this section, we provide some visualization of the attention learned by the SoTA model on the\nWikiText-103 validation set. Recall that, this model has 16 10-head transformer layers and relies on\na memory of length 640.\nFigure 5: Average attention over the previous 640 tokens, where each row corresponds to a attention head and each\ncolumn corresponds to a relative location. There are totally 160 attention heads, and every 10 heads come from a\nsingle layer. Darker colors indicate higher values.\nThe ﬁrst visualization aims at revealing the overall trend of where the model is attending. Speciﬁcally,\nfor each attention head of each layer, we average the attention distributions of all tokens in the validation\nset. This is shown in Fig. 5. As we can see, the overall trend is to focus more on the nearby tokens\nthan the faraway ones. However, it is also very clear that some attention heads have a wider attention\ndistribution over the entire memory span, notably the head 8 from layer 1, head 78 from layer 8, and the\nhead 158 from layer 16.\nSince we are focused on learning long-range dependency, we are especially interested in these heads\nwith a wider attention span. Thus, in the second set of visualization, we pick the three notable heads\nmentioned above, and visualize their attention behavior for a randomly chosen position, as shown in Fig.\n6. Here, we see three different patterns of wider attention:\n•For the head 8 in the 1st layer, we see an almost uniform attention over the entire memory span. This\nis quite intuitive, as lower-level layers needs to screen the entire memory span to decide where to focus\nfor higher-level layers\n(a) Head 8 from layer 1.\n(b) Head 78 from layer 8.\n(c) Head 158 from layer 16.\nFigure 6: Visualization of the three heads with a wide attention range. Each row corresponds to a target loca-\ntion/token and each column corresponds to a context location/token. Tokens in the memory that have top 20%\nattention values are highlighted in red.\n•For the head 78 in the 8th layer (a middle-level layer), we see a very sparse attention pattern scattered\nin all ranges of the memory. Again, this well ﬁts our intuition that as information accumulates, the\nnetwork may focus on some particular position with special interests.\n•For the head 158 in the 16th layer (i.e. the last layer), each target location (corresponding to each row)\nhas its own distinct sparse focus, differing from head 78 where target locations largely share the same\nattentive location in memory. Meanwhile, the pattern is also different from the case of head 8, where\na few locations are clearly attended more than others.\nFinally, as we have discussed in section 3.3, the attention score can be decomposed into four intuitive\nterms. Here, we want to further investigate how these four terms contribute to the overall attention trend\nin Fig. 5. Since the term (c) represents the global content bias, i.e., the prior importance of each word\nregardless of the context, we will leave it out and focus on the terms (a), (b) and (d). So, for each term,\nwe take the Softmax w.r.t. the memory span and average the resulted distribution of all tokens in the\nvalidation set. The results are visualized in Fig. 7:\n•Since term (a) is fully content-based addressing, when averaging over all target words, the result is\nessentially uniform over the entire context, except for a few very close words, which are likely to be\nsemantically similar to the target word.\n•The overall trend of term (b) highly resembles that of the entire attention distribution in Fig. 5. It\nsuggests that the global trend of focusing on the nearby context is largely contributed by this content-\ndependent positional bias.\n•The overall trend of term (d) is also focusing more on nearby words. However, compared to the trend\nof term (b), it is clearly ﬂatter and biases towards a longer context.\n(a) Term (a).\n(b) Term (b).\n(c) Term (d).\nFigure 7: Visualization of the three terms in computing the attention score. Each row corresponds to a attention\nhead and each column corresponds to a relative location.\nE Generated Text\nIn this section, we present some generated text from our best model trained the Wikitext-103 dataset.\nWe seed the our Transformer-XL with a context of at most 512 consecutive tokens randomly sampled\nfrom the test set of Wikitext-103. Then, we run Transformer-XL to generate a pre-deﬁned number of\ntokens (500 or 1,000 in our case). For each generation step, we ﬁrst ﬁnd the top-40 probabilities of the\nnext-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help\nreading, we detokenize the context, the generated text and the reference text. Three generated examples\nare shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the\nﬁrst three examples we generate in the paper. In the text, “= text =”, “= = text = =” and “= = = text = =\n=” denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data\npreprocessing procedure of Wikitext-103 (Merity et al., 2016).\nAs we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating\nlong text articles, particularly in the following aspects:\n•Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia.\n•Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion.\n•Long-range references are common in the generated text.\n•Transformer-XL often generates novel content that is not present in the training data.\nFor more detailed explanation of the interesting observations in each example, please refer to the corre-\nsponding caption.\nDespite the overall excellence of the generation quality, the model can only perceive the seed context\nand hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on.\nAs a result, the generated text sometimes looks clearly relevant but not close enough or to the point\ncompared to what human writer would do. That said, we believe this issue is mostly a problem of limited\ntraining data size and could be alleviated by using a larger training set.\nContext:\nKershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings. On May 4, he had his worst start of his career\nagainst the Milwaukee Brewers at Dodger Stadium, throwing just 57 pitches in 11 / 3 innings, while retiring only four of the 13 batters he faced — including the\npitcher. He was booed loudly upon being pulled from the game. Kershaw said after the game, \" I didn’t give our team any kind of chance. It’s just not a good\nfeeling to let your teammates down, let everybody down. It stings, it hurts. I ’ve got to ﬁgure things out. \" Kershaw rebounded his next start by pitching an 8 inning\ntwo-hitter and out-dueling the then undefeated Ubaldo Jiménez. He credited his control of the slider being the major turning point for him. Later in the season, he\nwas suspended for ﬁve games after hitting Aaron Rowand of the Giants with a pitch in a game on July 20. The incident occurred after both teams were given a\nwarning following Giants ace Tim Lincecum hitting Matt Kemp earlier in the game. He threw his ﬁrst career complete game shutout on September 14, 2010 also\nagainst San Francisco and ﬁnished the season with a record of 13 – 10 and a 2.91 ERA in 32 starts, pitching 2041 / 3 innings and recording 212 strikeouts.\n= = = = 2011 season: 1st Cy Young Award = = = =\nAfter ﬁnishing the 2010 season strong, the Dodgers named Kershaw as the Opening Day Starter for the 2011 season. On May 29, he pitched the second complete-\ngame shutout of his career, striking out 10 while winning a two-hitter against the Florida Marlins, 8 – 0; he also had two singles and an RBI, scoring twice in the\ngame. He produced his third career shutout on June 20, a two-hit, 11-strikeout effort against the Detroit Tigers. Kershaw became the ﬁrst Dodgers starter to strike\nout the side in the 9th inning since Sandy Koufax’s perfect game. In his next start, on June 26, Kershaw pitched another complete game (against the Los Angeles\nAngels of Anaheim ). He became the ﬁrst Dodger starter to have back-to-back complete game victories since Jeff Weaver in the 2005 season and the ﬁrst Dodger\nto have double-digit strikeouts in consecutive starts since Chan-Ho Park in the 2000 season. He was awarded the National League Player of the Week award for the\nweek of June 20 – 26 as a result of those two starts. Midway through June, Kershaw had amassed 32 career victories\nXL Generation: Reference:\n, tying him with Luis Castillo for the third best start total in the league and trail-\ning only Johnny Bench and Eddie Mathews (37). The Dodgers signed Kershaw\nto a one-year deal on July 14, 2011, with a player option for 2012, in a deal\nworth just under $ 300,000. The Dodgers entered the 2011 season as the major\nleague leaders, and Kershaw had the best record in the National League in saves\n(39), games started (78) and walks (120); he also had the lowest win / loss ratio\nin the league (1.87), and he had the worst on-base percentage (.349). He had\nthe best ERA (2.00 ), most wins (16 ), or most shutouts (29) in his career (25),\nwhile leading the league in losses per nine innings pitched (4.01). Following the\n2011 season, the Dodgers traded Kershaw to the Kansas City Royals on July 29\nfor pitcher Javier Torres.\n= = = Kansas City Royals = = =\n= = = = 2012 season = = = =\nDuring spring training, Kershaw played very well. He was selected to spring\ntraining as a relief pitcher for the Royals for the 2012 season. After an injury\nto closer Javier Vázquez, he was activated on April 29 to replace Matt Holliday\nin the Royals ’ starting rotation. In his only start with the Royals, on August 6,\n2012, Kershaw struck out ﬁve batters in seven innings pitched to help the Royals\nto their ﬁrst victory in franchise history. On September 27, 2012, it appeared\nKershaw was going to pitch a complete game shutout against the Detroit Tigers,\nbut did not manage to do so since the Tigers won 3 – 1. At the conclusion\nof the season, Kershaw was named Major League Baseball’s Most Valuable\nPlayer, was chosen to the All-Star Game at Busch Stadium and was named to\nthe All-Star Game as the starting pitcher at shortstop. The Royals announced on\nFebruary 4, 2013 that Kershaw would spend 2013 as starting pitcher, though he\nwas expected to miss the season as a result of a shoulder injury.\n= = = = 2013 season = = = =\nOn May 17, 2013, Kershaw sustained another back injury and did not start in\nAugust and October 2013. He appeared in 22 starts, all starts, ﬁnishing with\na strikeout-to-walk ratio of 1.50 and a 2.91 ERA. He also had the third most\nstrikeouts in the league: 10. On May 20, 2013, he\n, a 3.15 ERA and 593 career strikeouts in 568.2 innings. According to the Elias\nSports Bureau, Kershaw was the ﬁrst 23-year-old pitcher to have that many\nvictories, an ERA that low and an average of more than one strikeout per inning\nsince ERA became an ofﬁcial statistic in 1910. Kershaw was selected to the\nNational League team for the 2011 Major League Baseball All-Star Game, his\nﬁrst All-Star selection. In the month of July, Kershaw was 4 – 1 with a 2.02\nERA and NL-leading 45 strikeouts, earning him the National League Pitcher of\nthe Month Award. On August 23, he struck out Matt Holliday of the St. Louis\nCardinals for his 200th strikeout of the season and became the 10th Dodger\npitcher to record back-to-back 200 strikeout seasons and the ﬁrst since Chan-\nHo Park did it in the 2001 season. Kershaw ﬁnished the 2011 season by leading\nthe NL with 21 wins, 248 strikeouts and a 2.28 ERA, winning the NL pitching\nTriple Crown, the ﬁrst Triple Crown winner since Jake Peavy of the 2007 San\nDiego Padres and the ﬁrst Dodger since Sandy Koufax won it in the 1966 season.\nJustin Verlander of the Detroit Tigers won the American League Triple Crown\nthe same season, marking the ﬁrst major-league season since 1924 to feature\nTriple Crown-winning pitchers in both leagues. Kershaw’s 21 wins were the\nmost by a Dodger pitcher since Orel Hershiser won 23 during the 1988 season.\nHis ERA was the lowest by a Dodger since Hershiser’s 2.03 in the 1985 season,\nhis strikeouts were the most by a Dodger since Koufax’s 317 in 1966 and his 233\n1 / 3 innings pitched were the most since Chan Ho Park pitched 234 in 2001.\nSince 1965 when Koufax did it, Peavy and Kershaw are only two pitchers in the\nNational League have led the league in wins, strikeouts, ERA, and WHIP (walks\nplus hits per inning pitched). Kershaw also became just the second <unk> to\nhave a 240-plus strikeouts in a season before the age of 24, joining Vida Blue.\nAfter the season, Kershaw was awarded the Warren Spahn Award as the best\nleft-handed pitcher in 2011, the Players Choice Award for Most Outstanding\nNational League pitcher, the Gold Glove Award as the top ﬁelding pitcher in\nthe NL and the Sporting News (TSN) National League Pitcher of the Year. He\nwas additionally selected as the starting pitcher for the TSN NL All-Star Team.\nOn November 17, he was honored with the National League Cy Young Award,\nmaking him the youngest Cy Young winner since Dwight Gooden\nTable 11: Example 1 – 500 tokens generated by XL using a snippet from the Wikitext-103 test set as initial context. The\nsample is randomly generated without any cherry picking.\nOriginal Wikipedia page: https://en.wikipedia.org/wiki/Clayton_Kershaw\nThere are many interesting observations from this example:\n• Firstly, Kershaw never went to Royals in real life. Despite that, Transformer-XL stays on the fully imagined topic and keeps\nhallucinating the experience of Kershaw in Royals across the generated text.\n• Secondly, notice that XL correctly tracks the chronological order from 2011 to 2012 and to the ﬁnally 2013 season in the\nsection titles.\n• In addition, notice that Transformer-XL accurately uses the the phrase “another back injury ” in the 2013 season paragraph,\nsince it has talked about one earlier injure in the 2012 season. This shows again Transformer-XL’s ability of capturing\nlong-term dependency.\nContext:\n= = Distribution = =\nSpecies range across the Neotropics from Mexico in the north to Bolivia, Paraguay, and southern Brazil in the south. According to <unk> and coauthors, three\nspecies are found in Mexico, four in Central America, and 62 in South America. Three species are present in the Caribbean — two in Trinidad and Tobago, along\nthe southern edge of the region, and one in Haiti.\n= = Habitat and ecology = =\n<unk> includes both large trees and small acaulescent palms which occupy a number of different ecological niches. Dense stands of some of the larger species are\nconspicuous elements on the landscape, while smaller species are found in both in the forest understorey and in savannas. Disturbance has been implicated in the\nformation of vegetation dominated by large <unk> species. In seasonally dry Amazonian forests the density of large adult A. <unk> palms was correlated with\ncanopy openness; the species also dominates savannas formed by repeated forest ﬁres in Trinidad and Tobago. <unk> speciosa forms pure stands in many parts of\nBrazil where natural forest vegetation has been cleared. Similarly, stands of A. <unk> in Bahia, Brazil (which are cultivated for <unk> ﬁbre) are managed using\nﬁre — the seedlings survive cutting and burning, and are able to dominate burned forest patches. The fruit are dispersed by animals; fruit which are not dispersed\nfrequently suffer seed predation by <unk> beetles. Certain species of <unk> have been mentioned as examples of \" anachronistic \" species which are adapted\nfor dispersal by now-extinct Pleistocene megafauna. On <unk> Island, <unk>, in the Brazilian Amazon, <unk> <unk> fruit were consumed by tapirs, collared\npeccaries, deer and primates. Rodents, including agoutis, fed upon the fruit and, as the fruit availability declined, they fed on the seeds. Other dispersers of <unk>\nfruit include Crested <unk> which consume the fruit and disperse the seeds of A. <unk> in the Brazilian Pantanal.\n= = Uses = =\n<unk> species have a long history of human utilisation. <unk> <unk> <unk> seeds have been found in archaeological sites in Colombia dating back to 9000 BP.\nA variety of species remain important sources of edible oil, thatch, edible seeds and ﬁbre. The leaves of <unk> <unk> and A. <unk> are used extensively for\nthatching. Several species are oil palms, with A. speciosa among the most important economically. Products extracted from A. speciosa were reported to support\nover 300,000 households in the Brazilian state of Maranhão in 2005, and in 1985 it was estimated to support over 450,000 households throughout the Brazil. <unk>\nﬁbres, extracted from the leaf bases of A. <unk>, are commercially important, and generated about US $\nXL Generation: Reference:\n700,000 in 2013 on the black market. In 2004 the plant was reported to be\nresponsible for a net proﬁt of around US $ 3 million in Brazil. A 2012 study\nfound A. speciosa to cause agricultural production at least 2 – 3 times greater\nthan those achieved without prior knowledge of land management.\n= The Tale of Mrs. Tittlemouse =\nThe Tale of Mrs. Tittlemouse is a children’s book written and illustrated by H.\nM. Potter. It was ﬁrst published in February 1910 by Frederick Warne & Co.\nand included illustrations by Frederick Warne. Mrs. Tittlemouse was intended\nto be a children’s book, rather than a musical, and the tale describes her attempts\nto ﬁnd her own home. Mrs. Tittlemouse is an average kitten and the tale follows\na mischievous rabbit named Mr. Tittlemouse who tries to kill her in an attempt\nto get a ride on a horse. Potter later wrote of the rabbit and her attempts to\nkill her, \" If [ she ] were a fox, I should like to have been able to show her the\nway. \" Potter’s work was inspired by The Tale of Little Black Sambo and her\ntwo nieces, Petunia and Petunia. It was the ﬁrst book in the Potter family to be\npublished in a hardback format, and was also the ﬁrst book in the Peter Rabbit\nseries. The tale was released on 13 November 1910 and was accompanied by\na greeting card by Potter’s brother-in-law, Frederick Warne. The tale was well\nreceived by critics and children, and sold over 600,000 copies in its ﬁrst few\nyears. In the ﬁrst year of its publication, Potter’s The Tale of Mrs. Tittlemouse\nwon children’s book awards in England, America and Australia. It was selected\nas a \" recommended reading \" by children in the US and Canada.\n= = Plot summary = =\nThe tale opens with the pet rabbit Mrs. Tittlemouse wandering through a forest\nin a small litter of four. He is shot and badly injured by a deer after she attempts\nto escape. Mrs. Tittlemouse decides that she wants to ﬁnd her own home, be-\ncause she is eager to go on her own. She goes alone to the farm where she makes\na little money by selling a few seeds and building a small cabin in the woods.\nShe is approached by a wealthy hunter named Mr. Tittlemouse, who tries to kill\nher but Mrs. Tittlemouse kills him by stufﬁng a rope into his nose and killing\nhim. She is rescued by Mr. Tittlemouse’s wife Ruth, but Mrs. Tittlemouse then\nleaves the woodland with the baby. When she is spotted by\n20 million in annual income to Brazilian farmers in 1996.\n= The Heart of Ezra Greer =\nThe Heart of Ezra Greer is a 1917 American silent drama ﬁlm produced by the\nThanhouser Company and directed by Emile <unk>. The ﬁlm focuses on Ezra\nGreer, a successful middle-aged man who searches for his college age daughter,\nMary. The wayward Mary was romanced and abandoned by Jack <unk>, later\nbearing his child. Once Ezra becomes broke he ﬁnds employment as the valet\nfor Jack <unk>. After Jack’s engagement to a cabaret girl, Mary becomes upset\nand leaves her child at Jack’s home. Contrary to Jack’s wishes, Ezra keeps the\nchild and Jack ultimately reveals that the child is his own. Ezra convinces Jack\nto make things right and Ezra convinces the cabaret girl to leave Jack. After a\ncarriage accident in which the baby is injured, Ezra and Jack rush to the hospital\nand ﬁnd Mary as a nurse crying over the child. The ﬁlm ends with the marriage\nof Jack and Mary. The ﬁlm was released by Pathé on October 7, 1917. The ﬁlm\nwas the ﬁnal release from Thanhouser and was deemed to be an average ﬁlm\nby most reviewers. Criticism for the ﬁlm hinged on far-fetched coincidences to\ndrive the plot. The ﬁlm is presumed lost.\n= = Plot = =\nThe ﬁlm follows Ezra Greer, a middle-aged man who has worked hard since\nhis youth. He cares deeply for his motherless daughter, Mary, but was unable\nto attend the annual commencement at her co-educational college. He awaits\nfor her to return from college, but Mary leaves with her romantic interest, Jack\n<unk>. On promise of marriage and wealth, Mary is romanced and gives birth\nto a fatherless child. Without word from his daughter, Ezra resigns from his\njob and attempts to seek her out and ﬁnds a poor motherless child, Marie. With\nEzra’s money exhausted he seeks employment and ﬁnds it as the valet of Jack.\nOne day, Mary seeks an announcement of Jack’s engagement to a cabaret girl\nknown as \" The Baby Vamp \". Bitter over the prospect of her child’s future,\nshe leaves the child at Jack’s home during his absence with a note. Jack orders\nEzra to take the baby to an orphanage, but Marie begs Ezra to keep him. After\ncontinually seeing the child, Jack is overcome with remorse and explains to Ezra\nand seeks his advice. Not knowing he was making the case for his own daughter,\nEzra convinces Jack to seek out Mary and forget the Baby Vamp. The Baby\nTable 12: Example 2 – 500 tokens generated by XL using a snippet from the Wikitext-103 test set as initial context. The\nsample is randomly generated without any cherry picking.\nOriginal Wikipedia page: https://en.wikipedia.org/wiki/The_Tale_of_Mrs._Tittlemouse.\nThis example exhibit some additional interesting properties of Transformer-XL:\n• After ﬁnishing the last paragraph of the seed context, both the reference and generated text start a new topic (i.e., Wikipedia\npage), as marked by the single “= title =” line. This suggests the model has the ability of identifying the end of a topic / page,\nand randomly starting with a new topic.\n• Even more interestingly, a newly-started page is on a book called “The Tale of Mrs. Tittlemouse”. Transformer-XL manages\nto copy the same book title and some related information from the training set, but hallucinates novel content of the book.\nThis demonstrates a degree of generalization instead of memorization. Please refer to the original book content at the\nWikipedia page.\nContext:\n= Battle of Dürenstein =\nThe Battle of Dürenstein (also known as the Battle of <unk>, Battle of <unk> and Battle of <unk>; German: <unk> bei <unk> ), on 11 November 1805 was\nan engagement in the Napoleonic Wars during the War of the Third Coalition. Dürenstein (modern <unk>) is located in the <unk> Valley, on the River Danube,\n73 kilometers (45 mi) upstream from Vienna, Austria. The river makes a crescent-shaped curve between <unk> and nearby Krems an der Donau and the battle\nwas fought in the ﬂood plain between the river and the mountains. At Dürenstein a combined force of Russian and Austrian troops trapped a French division\ncommanded by Théodore Maxime Gazan. The French division was part of the newly created VIII Corps, the so-called Corps Mortier, under command of Édouard\nMortier. In pursuing the Austrian retreat from Bavaria, Mortier had over-extended his three divisions along the north bank of the Danube. Mikhail <unk> Kutuzov,\ncommander of the Coalition force, enticed Mortier to send Gazan’s division into a trap and French troops were caught in a valley between two Russian columns.\nThey were rescued by the timely arrival of a second division, under command of Pierre Dupont de l ’Étang. The battle extended well into the night. Both sides\nclaimed victory. The French lost more than a third of their participants, and Gazan’s division experienced over 40 percent losses. The Austrians and Russians\nalso had heavy <unk> to 16 <unk> perhaps the most signiﬁcant was the death in action of Johann Heinrich von Schmitt, one of Austria’s most capable chiefs of\nstaff. The battle was fought three weeks after the Austrian capitulation at Ulm and three weeks before the Russo-Austrian defeat at the Battle of Austerlitz. After\nAusterlitz Austria withdrew from the war. The French demanded a high indemnity and Francis II abdicated as Holy Roman Emperor, releasing the German states\nfrom their allegiance to the Holy Roman Empire.\n= = Background = =\nIn a series of conﬂicts from 1803-15 known as the Napoleonic Wars, various European powers formed ﬁve coalitions against the First French Empire. Like the wars\nsparked by the French Revolution (1789 ), these further revolutionized the formation, organization and training of European armies and led to an unprecedented\nmilitarization, mainly due to mass conscription. Under the leadership of Napoleon, French power rose quickly as the Grande Armée conquered most of Europe,\nand collapsed rapidly\nXL Generation: Reference:\nafter the French conquest of Italy. This victory marked the beginning of the\nSecond Coalition. Napoleon’s rapid advance caused Austria – Russia, Britain\nand Russia to make peace negotiations. The Russian army under Prince Mikhail\nMikhailovich Mikhailovich Chaffee, commander of the Imperial Russian Army\nin Switzerland, was able to drive off the Austrians in the Battle of Stockach\n(1805) and to take Pressburg (modern Ö3 Austria) from the Austrians. At an\nearly stage the Austro-Hungarian ﬂeet had been damaged in a storm off Cape\nMatapan; this allowed the Allies to launch a full-scale invasion of Italy in Febru-\nary 1805. The Battle of Austerlitz was the decisive French victory against\nNapoleon and the largest naval battle in history, involving more modern-day\nEuropean navies. The French military situation worsened as Napoleon faced\nseveral threats related to his newly formed Army of Europe, particularly Russia\nitself, which was now on the verge of collapse. The Russian general and friend\nof Napoleon, Alexander, had been dissatisﬁed with the conduct of the Austri-\nans; he was still in conﬂict with Emperor Napoleon, the French Republic’s king,\nwho had declared war on Austria during the summer of 1804. With the war over,\nNapoleon decided to launch a second invasion of Italy in October 1805.\n= = Prelude = =\nIn July 1805, the French 1st Army entered southern Italy. The army, under the\ncommand of Marshal Marmont, were reinforced by a few battalions of infantry\nunder Claude General Auguste de Marmont at the town of Philippsburg and\nanother battalion at Belluno. On 17 September 1805, the army marched from\nBelluno towards Krems. By 29 September, they had reached Belluno and con-\nducted its advance against a small Austrian force. By 31 September, the whole\nforce had been reinforced by a brigade from the Army of Tyrol under the com-\nmand of Pierre Augereau. The Austrians were now under the command of Mar-\nshal Jean Victor Marie Moreau, a member of the Directory. Moreau had taken\ncommand of the Austrian invasion force in the spring of 1805. His command\nincluded the VI Corps commanded by Jean Baptiste Drouet de Ney and the VI\nCorps commanded by Generals Jean Victor Marie Moreau and Joseph Souham.\nNey’s corps consisted of the III. Corps and VI. Corps, which consisted of the\nIII Corps and VI. Corps, located in the Austrian Netherlands, was commanded\nby Friedrich Joseph, Count Baillet de Latour. Moreau’s army consisted of six\ndivisions and several associated brigades.\n= = Aftermath = =\n= = = First Coalition forces = = =\nOn 9 October 1805 the French Army of the Danube was attacked by an Aus-\ntrian army under Archduke Charles at the Battle of Austerlitz. Although Charles\nand Charles had not had much time to regroup, on 10 October, he launched his\nattack on the Polish forces under Friedrich Joseph, Count of Lauenburg. Af-\nter three days, Charles’ army captured Lauenburg. The French forces pursued\nthe Austrians to the Silesian border, where they encountered strong Austrian\nresistance. These conﬂicts forced the Austrians to retreat into Tyrol and Aus-\ntria agreed to a truce. The Austrian army, commanded by Wenzel Anton Karl,\nCount of Merveldt, was reduced to around 10,000 men. It was initially planned\nthat Archduke Charles would launch a counter-attack against the French army\non the same day, as Napoleon had hoped, but this was not carried out. On 25\nOctober, Merveldt left Styria for Tyrol. On the same day, Austria launched its\nnew offensive against the French at Ulm. Charles withdrew his army from the\nregion for a third time at the Battle of Elchingen, under the overall command of\nthe Austrian generals, Ferdinand and Friedrich Wilhelm of Jülich-Cleves-Berg.\nTo prevent Archduke Charles from escaping from the battleﬁeld, the comman-\nder of the Habsburg army, Archduke Charles, planned to occupy the fortress\nLinz; instead, he decided to force Franz von Hipper to surrender the city. How-\never, as Charles moved to the south, Moreau arrived on the scene with additional\nsoldiers – including the entire Imperial Guard – and defeated the Austrians at\nthe Battle of Hohenlinden on 28 October. The loss of Linz resulted in Austria’s\ncomplete defeat at Hohenlinden. In the meantime, the French Army of Obser-\nvation and Preparedness was reorganized into the Army of the Danube under\nFeldzeugmeister (Colonel-General) Friedrich Freiherr von Hotze. The army\nwas composed of the I, IV , VI, VI, VII, VIII and IX Corps. With reinforcements\nfrom Italy and France, it formed new battalions, companies, and squadrons in\nthe Austrian army. On 17 November 1804, at the Battle of Jena-Auerstadt the\nArmy of Silesia and the Army of Silesia joined forces, but by the time that the\nafter the disastrous invasion of Russia in 1812. Napoleon’s empire ultimately\nsuffered complete military defeat in the 1813 – 14 campaigns, resulting in the\nrestoration of the Bourbon monarchy in France. Although Napoleon made a\nspectacular return in 1815, known as the Hundred Days, his defeat at the Battle\nof Waterloo, the pursuit of his army and himself, his abdication and banishment\nto the Island of Saint Helena concluded the Napoleonic Wars.\n= = Danube campaign = =\nFrom 1803-06 the Third Coalition fought the First French Empire and its client\nstates (see table at right ). Although several naval battles determined control of\nthe seas, the outcome of the war was decided on the continent, predominantly\nin two major land operations in the Danube valley: the Ulm campaign in the\nupper Danube and the Vienna campaign, in the middle Danube valley. Political\nconﬂicts in Vienna delayed Austria’s entry into the Third Coalition until 1805.\nAfter hostilities of the War of the Second Coalition ended in 1801, Archduke\n<unk> emperor’s <unk> advantage of the subsequent years of peace to develop\na military restructuring plan. He carefully put this plan into effect beginning in\n1803 – 04, but implementation was incomplete in 1805 when Karl Mack, Lieu-\ntenant Field Marshal and Quartermaster-General of the Army, implemented his\nown restructuring. Mack bypassed Charles ’ methodical approach. Occurring\nin the ﬁeld, Mack’s plan also undermined the overall command and organiza-\ntional structure. Regardless, Mack sent an enthusiastic report to Vienna on the\nmilitary’s readiness. Furthermore, after misreading Napoleon’s maneuvers in\nWürttemberg, Mack also reported to Vienna on the weakness of French dispo-\nsitions. His reports convinced the war party advising the emperor, Francis II,\nto enter the conﬂict against France, despite Charles ’ own advice to the con-\ntrary. Responding to the report and rampant anti-French fever in Vienna, Fran-\ncis dismissed Charles from his post as generalissimo and appointed his <unk>\nbrother-in-law, Archduke Ferdinand, as commander. The inexperienced Ferdi-\nnand was a poor choice of replacement for the capable Charles, having neither\nmaturity nor aptitude for the assignment. Although Ferdinand retained nomi-\nnal command, day-to-day decisions were placed in the hands of Mack, equally\nill-suited for such an important assignment. When Mack was wounded early\nin the campaign, he was unable to take full charge of the army. Consequently,\ncommand further devolved to Lieutenant Field Marshal Karl Philipp, Prince of\nSchwarzenberg, an able cavalry ofﬁcer but inexperienced in the command of\nsuch a large army.\n= = = Road to Ulm = = =\nThe campaign in the upper Danube valley began in October, with several clashes\nin Swabia. Near the Bavarian town of Wertingen, 40 kilometers (25 mi) north-\nwest of Augsburg, on 8 October the 1st Regiment of dragoons, part of Murat’s\nReserve Cavalry Corps, and grenadiers of Lannes ’ V Corps surprised an Aus-\ntrian force half its size. The Austrians were arrayed in a line and unable to form\ntheir defensive squares quickly enough to protect themselves from the 4,000\ndragoons and 8,000 grenadiers. Nearly 3,000 Austrians were captured and over\n400 were killed or wounded. A day later, at another small town, <unk> south\nof the Danube <unk> French 59th Regiment of the Line stormed a bridge over\nthe Danube and, humiliatingly, chased two large Austrian columns toward Ulm.\nThe campaign was not entirely bad news for Vienna. At Haslach, Johann von\nKlenau arranged his 25,000 infantry and cavalry in a prime defensive position\nand, on 11 October, the overly conﬁdent General of Division Pierre Dupont de\nl’Étang attacked Klenau’s force with fewer than 8,000 men. The French lost\n1,500 men killed and wounded. Aside from taking the Imperial Eagles and\n<unk> of the 15th and 17th Dragoons, Klenau’s force also captured 900 men,\n11 guns and 18 ammunition wagons. Klenau’s victory was a singular success.\nOn 14 October Mack sent two columns out of Ulm in preparation for a breakout\nto the north: one under Johann Sigismund Riesch headed toward Elchingen to\nsecure the bridge there, and the other under Franz von Werneck went north with\nmost of the heavy artillery. Recognizing the opportunity, Marshal Michel Ney\nhurried the rest of his VI Corps forward to re-establish contact with Dupont, who\nwas still north of the Danube. In a two-pronged attack Ney sent one division to\nthe south of Elchingen on the right bank of the Danube. This division began the\nassault at Elchingen. At the same time another division crossed the river to the\neast and moved west against Riesch’s position. After clearing Austrian pickets\nfrom a bridge, the French attacked and captured a strategically located abbey at\nFrench approached Vienna, the Prussians had already surrendered. As the Aus-\ntrians did not want to allow the war to continue, they decided to abandon their\nterritories in the north and move their army to the north and west, cutting off\nCharles from Vienna. The Battle of Warsaw was fought on 23 November 1805\nbetween the French Army of the Danube and the Austrian Army of Styria in\nthe vicinity of Warsaw and Pressburg (modern Trnava, Slovakia). At that time\nHabsburg forces\nthe top of the hill at bayonet point. The Austrian cavalry unsuccessfully tried to\nfend off the French, but the Austrian infantry broke and ran. In this engagement\nalone, the Austrians lost more than half their reserve artillery park, 6,000 (out\nof 8,000 total participants) dead, wounded or captured and four colors. Reisch’s\ncolumn also failed to destroy the bridges across the Danube. Napoleon’s light-\nning campaign exposed the Austrian indecisive command structure and poor\nsupply apparatus. Mack\nTable 13: Example 3 – 1,000 tokens generated by XL using a snippet from the Wikitext-103 test set as initial context. The\nsample is randomly generated without any cherry picking.\nOriginal Wikipedia page: https://en.wikipedia.org/wiki/Battle_of_D%C3%BCrenstein.\n• Although this example is signiﬁcantly longer, we can see that Transformer-XL is still able to stay on the same topic and\nmakes up non-existing stories about the Napoleon wars.\n• Notably, from the second section on, the generated text correctly follows a ﬁne-grained chronological order on the level of\nmonth and day to narrate events in 1805, except a mistake (1804 instead of 1805) near the end of the paragraph. To ease\nreading which we have highlighted all the date related phrases by magenta in the generation.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.781032919883728
    },
    {
      "name": "Computer science",
      "score": 0.7189745306968689
    },
    {
      "name": "Language model",
      "score": 0.6630920171737671
    },
    {
      "name": "Transformer",
      "score": 0.6591395139694214
    },
    {
      "name": "Hyperparameter",
      "score": 0.5559052228927612
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5389028787612915
    },
    {
      "name": "Treebank",
      "score": 0.5254216194152832
    },
    {
      "name": "Natural language processing",
      "score": 0.38897904753685
    },
    {
      "name": "Dependency (UML)",
      "score": 0.2594703435897827
    },
    {
      "name": "Engineering",
      "score": 0.10902303457260132
    },
    {
      "name": "Electrical engineering",
      "score": 0.0865502655506134
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 585
}