{
    "title": "EndoViT: pretraining vision transformers on a large collection of endoscopic images",
    "url": "https://openalex.org/W4393860447",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4364324297",
            "name": "Batić, Dominik",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A4320860964",
            "name": "Holm, Felix",
            "affiliations": [
                "Technical University of Munich",
                "Carl Zeiss (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A4227255771",
            "name": "Özsoy, Ege",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A4221954887",
            "name": "Czempiel, Tobias",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2729164328",
            "name": "Navab, Nassir",
            "affiliations": [
                "Technical University of Munich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4312891522",
        "https://openalex.org/W4315781049",
        "https://openalex.org/W6600213211",
        "https://openalex.org/W3092562667",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3127470776",
        "https://openalex.org/W2998595069",
        "https://openalex.org/W2810650256",
        "https://openalex.org/W2580456502",
        "https://openalex.org/W3155656997",
        "https://openalex.org/W1651609297",
        "https://openalex.org/W3091895136",
        "https://openalex.org/W3198379050",
        "https://openalex.org/W4214520160",
        "https://openalex.org/W2765685204",
        "https://openalex.org/W4294975343",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W2266464013",
        "https://openalex.org/W4296193539",
        "https://openalex.org/W4312804044",
        "https://openalex.org/W3203416417"
    ],
    "abstract": null,
    "full_text": "International Journal of Computer Assisted Radiology and Surgery (2024) 19:1085–1091\nhttps://doi.org/10.1007/s11548-024-03091-5\nORIGINAL ARTICLE\nEndoViT: pretraining vision transformers on a large collection of\nendoscopic images\nDominik Bati´c1 · Felix Holm 1,2 · Ege Özsoy 1 · Tobias Czempiel 1 · Nassir Navab 1\nReceived: 15 January 2024 / Accepted: 28 February 2024 / Published online: 3 April 2024\n© The Author(s) 2024\nAbstract\nPurpose Automated endoscopy video analysis is essential for assisting surgeons during medical procedures, but it faces\nchallenges due to complex surgical scenes and limited annotated data. Large-scale pretraining has shown great success in\nnatural language processing and computer vision communities in recent years. These approaches reduce the need for annotated\ndata, which is of great interest in the medical domain. In this work, we investigate endoscopy domain-speciﬁc self-supervised\npretraining on large collections of data.\nMethods To this end, we ﬁrst collect Endo700k, the largest publicly available corpus of endoscopic images, extracted from\nnine public Minimally Invasive Surgery (MIS) datasets. Endo700k comprises more than 700,000 images. Next, we introduce\nEndoViT, an endoscopy-pretrained Vision Transformer (ViT), and evaluate it on a diverse set of surgical downstream tasks.\nResults Our ﬁndings indicate that domain-speciﬁc pretraining with EndoViT yields notable advantages in complex down-\nstream tasks. In the case of action triplet recognition, our approach outperforms ImageNet pretraining. In semantic\nsegmentation, we surpass the state-of-the-art (SOTA) performance. These results demonstrate the effectiveness of our domain-\nspeciﬁc pretraining approach in addressing the challenges of automated endoscopy video analysis.\nConclusion Our study contributes to the ﬁeld of medical computer vision by showcasing the beneﬁts of domain-speciﬁc\nlarge-scale self-supervised pretraining for vision transformers. We release both our code and pretrained models to facilitate\nfurther research in this direction: https://github.com/DominikBatic/EndoViT.\nKeywords Endoscopy video analysis · Vision transformer · Pretraining\nIntroduction\nMinimally Invasive Surgery (MIS) is quickly becoming one\nof the most common styles of surgical procedures in the\nworld [ 21]. In contrast to open surgery, MIS lowers the\nchance of infection and speeds up the recovery rate. As MIS\nprocedures use endoscopic cameras, it has become possible\nto analyze large amounts of video data, leading to the devel-\nopment of surgical assistance systems. These systems can\ndetect errors and provide decision support to improve patient\noutcomes [ 17]. Additionally, cataloging recorded surgical\nDominik Bati´ c, Felix Holm, and Ege Özsoy have equally contributed\nto this work.\nB Felix Holm\nfelix.holm@tum.de\n1 Chair for Computer Aided Medical Procedures, Technical\nUniversity Munich, Munich, Germany\n2 Carl Zeiss AG, Munich, Germany\nprocedures provides valuable insights to surgeons, enabling\nthem to learn and improve their techniques [ 23]. To achieve\nthese goals, the community has thoroughly investigated the\ntask of Surgical Phase Recognition [ 6, 26] and success-\nfully managed to detect and localize surgical instruments\n[13]. Today, more challenging tasks are being explored, such\nas the newly introduced action triplet recognition [ 21]. It\nrequires not only detecting surgical instruments, actions,\nand anatomies but also determining the relationship between\nthem. Other works focus on the segmentation of tools and\ntissues[5], as well as multi-level learning, combining several\ntasks at once[ 27].\nIn general deep learning, the transformer [ 28] architecture\nhas had a tremendous impact in recent years. Its success can\nbe attributed to the introduction of self-supervised pretrain-\ning methods, such as Masked Language Modeling. The idea\nis straightforward: A percentage of input words are randomly\nmasked out, and the model is tasked with predicting the miss-\ning input. Despite its simplicity, it presents a challenging\n123\n1086 International Journal of Computer Assisted Radiology and Surgery (2024) 19:1085–1091\nself-supervised task. This approach has led to a paradigm\nshift in which a transformer network is ﬁrst pretrained on\nlarge amounts of unlabeled data in order to create a model\nwith a general understanding of the underlying domain. Later\non, this model can be ﬁnetuned for a speciﬁc downstream\ntask using signiﬁcantly fewer annotations. With the advent\nof Vision Transformers (ViT) [ 8], similar strategies such as\nMasked Image Modeling have been developed for computer\nvision [ 2, 9, 29], showing equally high beneﬁt in complex\ncomputer vision tasks.\nDespite the advancements in computer vision and natural\nlanguage processing, the progress of artiﬁcial intelligence\nmethods in the medical ﬁeld has been slower due to the\ninsufﬁcient amount of annotated data for developing data-\ndriven approaches [27]. While the largest endoscopic dataset,\nCholec80 [ 26], only contains 200k images, computer vision\ndatasets can reach hundreds of millions of images [ 25].\nAdditionally, downstream medical tasks requiring complex\nannotations, such as pixel-wise segmentations, often have\nless than 10k images [ 11]. Pretraining models on larger\ndatasets could be used to overcome this challenge. However,\nso far, only natural image datasets are generally available at\nthe required size, which leaves a signiﬁcant domain gap to\nendoscopic videos.\nIn this study, we use endoscopy domain-speciﬁc large-\nscale pretraining to bring advances from the computer vision\ncommunity to the medical domain. Toward this objective,\nour contributions are threefold:\n1. We compile the largest publicly available collection\nof unlabeled endoscopic data, Endo700k, consisting of\nmore than 700,000 images.\n2. We introduce the ﬁrst publicly available endoscopy-\npretrained vision transformer, EndoViT.\n3. We analyze, through extensive experiments and ablation\nstudies, the effect of endoscopy pretraining on the down-\nstream tasks of surgical phase recognition, surgical action\ntriplet recognition, and semantic segmentation.\nMethodology\nDataset preparation\nTo enable effective endoscopy-speciﬁc pretraining, we have\ncreated the largest publicly available collection of raw endo-\nscopic data, Endo700k. Endo700k is formed by combining\nnine publicly available MIS datasets comprising more than\n700,000 images. An overview of the individual datasets is\nprovided in Table 1. Endo700k contains a diverse set of\nendoscopic procedures, both manual and robot-assisted, with\nseveral surgery types such as prostatectomy, cholecystec-\ntomy, gastrectomy, proctocolectomy, rectal resection, and\nsigmoid resection. Furthermore, multiple different surgical\nactions, anatomies, and many surgical instruments, which\nare present in different shapes and sizes, are included. The\ndownstream evaluation experiments are conducted on the\nCholec80 dataset [ 26] and its subvariants CholecT45 [ 21]\nand CholecSeg8k [ 11]. To eliminate any potential data leak-\nage, we exclude any images that appear in their validation or\ntest sets from the pretraining dataset. Furthermore, all syn-\nthetic images are excluded. Outside the previously mentioned\nexceptions, we use all of the images from the nine datasets.\nFor consistency, we always downsample to 1 FPS.\nModel pretraining\nMost existing works [ 5, 6, 13, 21, 26, 27] use ImageNet-\npretrained CNN models as image feature extraction back-\nbones. However, ImageNet [ 7] contains natural images that\ndiffer signiﬁcantly from endoscopic images. Therefore, in\nthis work, we use Endo700k to pretrain a large-scale vision\ntransformer-based [ 8] feature extractor on the endoscopy\ndomain. The goal of the pretraining is to give a general under-\nstanding of the domain of endoscopic procedures to beneﬁt a\nwide range of downstream tasks. For pretraining, we closely\nfollow the approach of MAE [ 9] and employ the Masked\nImage Modeling strategy. The input image is ﬁrst split into\nnon-overlapping patches. Afterward, a large proportion of\nthem is masked out. The network is trained to reconstruct\nthe missing parts of the input. The encoder of the pretrained\nmodel can then be used as a feature extraction backbone in\nthe downstream tasks. An overview of the pretraining proce-\ndure can be seen in Fig. 1. We tailor the MAE approach for\nthe endoscopic setting with three modiﬁcations:\nLayerwise learning rate decay We scale down the learning\nrate of each layer of the MAE encoder and decoder such\nthat the layers closer to the latent space have larger learning\nrates, while those closer to the ends of the model have lower\nlearning rates.\nStochastic weight averaging (SW A) [12]: During the last 5\npretraining epochs, we average the models’ weights at each\nvalidation step.\nFrequent evaluation The evaluation is performed 6 times\nper epoch, and the best SW A model is saved.\nImplementation details\nWe follow most of the practices and hyperparameter choices\nof [ 1, 9]. During pretraining only simple image augmenta-\ntions are applied, including random resized crops and random\nhorizontal ﬂips. We use AdamW optimizer [ 16] with a learn-\ni n gr a t eo f1 . 5 e− 3 and batch size of 256. We pretrain for a\ntotal of 15 epochs. The training starts with 3 linear warmup\nepochs, continues according to the cosine scheduler until\nepoch 10, and ends with a constant learning rate applied\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2024) 19:1085–1091 1087\nTable 1 An overview of the\nindividual datasets that form the\nEndo700k dataset\nDataset Surgery type # Surg. # Unique images\nESAD [ 3] Robot-assisted radical prostatectomy 4 49,544\nLapGyn4 (v1.2) [ 15] Gynecologic laparoscopy >500 38,192\nSurgical Actions160 [ 23] Gynecologic laparoscopy 59 761\nGLENDA (v1.0) [ 14] Gynecologic laparoscopy >400 1,083\nhSDB instrument [ 30] Laparoscopic cholecystectomy 24 35,576\nRobotic gastrectomy 24\nHeiCo [ 18] Laparoscopic proctocolectomy 10 347,257\nLaparoscopic rectal resection 10\nLaparoscopic sigmoid resection 10\nP S I - AVA [27] Robot-assisted radical prostatectomy 8 73,618\nDSAD [ 4] Robot-assisted rectal resection 32 13,195\nCholec80 [ 26] Laparoscopic cholecystectomy 80 184,498\nCholecT45 [ 21] Laparoscopic cholecystectomy 45 0\nCholecSeg8k [ 11] Laparoscopic cholecystectomy 17 0\nThe ﬁrst nine datasets (ESAD—Cholec80) represent a unique collection of roughly 744k raw endoscopic\nimages. Cholec80 and its subvariants CholecT45 and CholecSeg8k are additionally used for downstream\ntasks of surgical phase recognition, action triplet recognition, and semantic segmentation\nFig. 1 EndoViT is ﬁrst pretrained using the Masked Image Modeling\nstrategy (a). An input image is split into non-overlapping patches, and a\nlarge proportion of them is masked out. The network is trained to recon-\nstruct the missing patches using a per-patch MSE loss, gaining a general\nvisual understanding. Later, the EndoViT encoder can be ﬁnetuned and\nused as a powerful feature extraction backbone on downstream tasks\n(b). No Masking is applied during use as a feature extractor\nduring SW A. We use layer-wise learning rate decay of 0.65.\nMean-squared error (MSE) is used as the reconstruction loss.\nWe pretrain three different models, one for each of the down-\nstream tasks. All are pretrained on Endo700k; however, the\npretraining datasets are slightly different, obtained by remov-\ning validation and test datasets of CholecT45, Cholec80,\nand CholecSeg8k, respectively. All models have been imple-\nmented in PyTorch 1.13.0 and trained on 1 Nvidia a40 GPU.\nDownstream tasks\nAfter pretraining our feature extractor backbones, we eval-\nuate their performance on three downstream tasks, namely\nsemantic segmentation, action triplet recognition, and surgi-\ncal phase recognition.\nSemantic segmentation We choose the Dense Predic-\ntion Transformer (DPT) [ 22] architecture to leverage our\nvision transformer backbone for the semantic segmenta-\n123\n1088 International Journal of Computer Assisted Radiology and Surgery (2024) 19:1085–1091\nTable 2 Semantic segmentation\nresults, few shot, and full dataset\n(mean IoU)\nTrain Set ViT w/o Pretr ViT ImageNet EndoViT\nLow Res 1 Video 29.11 ± 2.94 38.35 ± 8.27 40.95 ± 10.32\n2 Videos 36.28 ± 5.06 50.36 ± 2.71 54.02 ± 4.18\n4 Videos 43.29 ± 0.96 54.17 ± 2.35 57.87 ± 2.70\nFull 51.70 ± 0.54 62.45 ± 0.90 65.05 ± 0.67\nHigh Res 1 Video 26.66 ± 6.64 39.06 ± 5.17 41.16 ± 10.75\n2 Videos 35.69 ± 4.45 50.14 ± 4.48 56.05 ± 5.73\n4 Videos 44.16 ± 0.75 56.22 ± 1.52 59.81 ± 3.27\nFull 53.18 ± 1.20 63.40 ± 0.81 65.32 ± 0.56\nBold values represent the best result\ntion task. DPT assembles tokens from various stages of\nthe ViT into image-like representations at various resolu-\ntions and progressively combines them [ 22]. Since the ViT\nbackbone processes the input at high resolution and has a\nglobal receptive ﬁeld, DPT allows for ﬁne-grained and more\nglobally consistent predictions compared to previous CNN\napproaches, especially when a larger amount of data can be\nprovided [ 22]. We replace the encoder of DPT with our ViT\nencoder but otherwise keep the training setup the same. The\nDPT decoder is randomly initialized. We replicate the eval-\nuation setup of [ 24].\nAction triplet recognition We build a straightforward\nmodel consisting of a feature extraction backbone and a linear\nhead to detect the < instrument ,v er b, target > triplets.\nTo avoid data leakage into the pretraining set, we evaluate\nonly on fold 5 as deﬁned by [ 20]. We chose fold 5 specif-\nically, as it yields the most balanced distribution of classes\nacross train, val, and test splits, in the otherwise highly imbal-\nanced CholecT45 dataset. While most works such as [ 19, 21]\nutilize Binary Cross-Entropy loss, we empirically ﬁnd that\nFocal Loss brings signiﬁcant improvement and therefore use\nit in all our experiments.\nSurgical phase recognition In the task of Surgical Phase\nRecognition, the objective is to detect different phases of a\nsurgical procedure based on the surgical video stream. For\nthis task, we choose TeCNO [ 6], a well-known benchmark\nmodel with publicly available code. TeCNO is a two-step\nsurgical phase recognition method. In the ﬁrst step, a single-\nframe ResNet50 model is trained to predict surgical phases.\nIn the second step, a Multi-Stage Temporal Convolutional\nNetwork (MS-TCN) reﬁnes the extracted features using tem-\nporal context. This two-stage approach allows the MS-TCN\nto improve the predictions of any feature extractor regardless\nof the chosen architecture. In our experiments, we replace the\nResnet50 backbone with a ViT model and otherwise stick to\nthe training and evaluation setup of TeCNO.\nExperiments\nWe compare our EndoViT (endoscopy pretrained) with its\nImageNet pretrained ViT counterpart and commonly used\nCNN architectures (ResNet50/ResNet18 [ 10]). We evaluate\nthe performance of the models on the full downstream dataset\nand also evaluate the few-shot learning performance using a\nreduced amount of training videos while keeping the valida-\ntion and test sets the same. We report the mean and standard\ndeviation of the corresponding metrics across 3 runs for each\nnetwork in each setting.\nSemantic segmentation We report the semantic seg-\nmentation results in Table 2. The reported metric is mean\nIntersection over Union (IoU). The results are reported both\nfor EndoViT’s pretraining resolution of 224 x 224 (Low Res)\nand the resolution used in [ 24]o f2 5 6 x 448 (High Res).\nEndoViT outperforms the ImageNet pretrained ViT in all\nscenarios, including few shot, with a margin of 2–6%. We\nreport a comparison to other methods from [ 24]i nF i g . 2 and\nTable3. EndoViT outperforms other Transformers (UNETR)\nas well as various CNN architectures, including U-Net++, the\nprevious SOTA to our knowledge, by a signiﬁcant margin of\n10%. EndoViT shows more globally consistent predictions\nand is better at reconstructing the crucial instrument tips.\nAction triplet recognition We report the results on the\nfull dataset in Table 4. The reported metric is mean Average\nPrecision (mAP) proposed by the authors of the CholecT45\ndataset [ 21]. The results show that EndoViT outperforms\nboth CNN architectures and its ImageNet pretrained counter-\npart by 8% and 2%, respectively, empirically showcasing the\nvalue of using endoscopy-based models. Furthermore, from\nthe performance of the randomly initialized ViT model, it can\nbe seen that pretraining is essential for vision transformers.\nIn Table 5, we report few-shot learning experiment results by\ntraining only on 2, 4, or 8 videos. We observe the same trends\nin our few-shot learning experiments. EndoViT outperforms\nResNet50 by 5–6.5% and the ImageNet model by 2–5%.\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2024) 19:1085–1091 1089\nFig. 2 Qualitative segmentation comparison. EndoViT has more globally consistent outputs (highlighted in black) and is signiﬁcantly better at\nreconstructing instrument tips (highlighted in red)\nTable 3 Semantic segmentation comparison to previous methods\n(mean IoU)\nU-Net++ DynUNet UNETR DeepLab V3+ EndoViT\n55 52 49 50 65.32 ± 0.56\nBold values represent the best result\nResults for methods other than ours from [ 24]\nSurgical phase recognition We report the results on the\nfull dataset in Table 6. The reported metric is Accuracy aver-\naged over all testing videos. We report the performance of\nall models after both stages of TeCNO training. For refer-\nence purposes, we note that the reported performance of\nResNet50 backbone in TeCNO [ 6] is 88.56% ± 0.27%. Once\nagain, the CNN architecture is outperformed by the pre-\ntrained vision transformers. However, the difference between\nEndoViT and ImageNet pretrained backbone is negligible in\nboth stages. We believe there are two causes. One, the seman-\ntic understanding induced by reconstructing image patches is\nnot capable of capturing the long-term relationships required\nTable 5 Action triplet recognition few-shot results (mAP)\nResNet50 ViT ImageNet EndoViT\n2 Videos 10.88 ± 0.50 12.22 ± 1.78 17.59 ± 2.94\n4 Videos 12.37 ± 1.78 14.27 ± 1.73 18.52 ± 2.28\n8 Videos 17.01 ± 1.75 19.71 ± 0.61 21.91 ± 0.12\nBold values represent the best result\nto discriminate between different surgical phases accurately.\nAnd two, the training set used in Cholec80 is relatively large\n(approx. 90k images), making it easier to overcome the pre-\ntraining differences. In Table 7, we report few-shot learning\nexperiment results by training only on 2, 4, or 8 videos.\nResNet50 showcases a signiﬁcant decrease in performance.\nWhen training on 2 videos only, EndoViT outperforms its\nImageNet counterpart in both stages. For 4 and 8 videos, we\nobserve comparable performance.\nTable 4 Action triplet\nrecognition full dataset results\n(mAP)\nResNet18 ResNet50 ViT w/o Pretr ViT ImageNet EndoViT\n21.72 ± 1.17 22.13 ± 1.37 13.93 ± 0.43 27.84 ± 0.39 30.17 ± 0.01\nBold values represent the best result\n123\n1090 International Journal of Computer Assisted Radiology and Surgery (2024) 19:1085–1091\nTable 6 Surgical phase\nrecognition full dataset results\n(mean Accuracy)\nResNet50 ViT w/o Pretr ViT ImageNet EndoViT\nStage 1 79.84 ± 0.30 59.21 ± 0.36 82.94 ± 0.69 82.60 ± 1.26\nStage 2 87.84 ± 0.58 73.42 ± 0.70 89.56 ± 0.65 89.37 ± 0.95\nBold values represent the best result\nTable 7 Surgical phase\nrecognition few-shot results\n(mean accuracy)\nTrain Set ResNet50 ViT ImageNet EndoViT\nStage 1 2 Videos 47.51 ± 1.33 63.59 ± 1.07 67.04 ± 2.92\n4 Videos 57.80 ± 2.67 67.72 ± 0.90 71.80 ± 0.49\n8 Videos 63.71 ± 1.48 75.50 ± 0.32 75.30 ± 1.83\nStage 2 2 Videos 68.23 ± 1.10 77.05 ± 1.71 78.89 ± 1.26\n4 Videos 74.50 ± 1.76 80.00 ± 0.62 80.28 ± 0.71\n8 Videos 77.43 ± 1.68 84.10 ± 0.38 84.68 ± 1.25\nConclusion\nEndoViT performs equally or better than the same ViT model\npretrained on ImageNet in all of our experiments. We observe\nthat the improvements from our endoscopy-pretraining were\nmore pronounced in the more complex downstream tasks.\nIn action triplet recognition, endoscopy-speciﬁc pretrain-\ning signiﬁcantly outperforms ImageNet pretraining. For the\nsimpler task of surgical phase recognition, our pretraining\nwas less impactful, although never hurting performance.\nIn the most complex task of semantic segmentation, our\nEndoViT model outperforms the previous SOTA. Moreover,\nour method performing well on all of the diverse down-\nstream tasks shows that our pretraining implementation,\nwhich reconstructs image patches in pixel space, captures\ngeneral information about the objects and scenes it has seen.\nWe therefore conclude that EndoViT would be an excel-\nlent upgrade to the ImageNet pretrained feature extraction\nbackbones that many surgical video understanding methods\nrely on. We hope that the release of our dataset collec-\ntion Endo700k, pretraining implementation, and pretrained\nEndoViT model will help the community to solve more chal-\nlenging tasks with the small amount of data available.\nFunding Open Access funding enabled and organized by Projekt\nDEAL.\nDeclarations\nConﬂict of interest Holm is supported by Carl Zeiss AG. The other\nauthors declare that they have no conﬂict of interest.\nEthical approval This article does not contain any studies with human\nparticipants or animals performed by any of the authors.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. Assran M, Caron M, Misra I, Bojanowski P , Bordes F, Vincent\nP , Joulin A, Rabbat M, Ballas N (2022) Masked siamese networks\nfor label-efﬁcient learning. In: Computer Vision–ECCV 2022: 17th\nEuropean conference, Tel Aviv, Israel, October 23–27, 2022, Pro-\nceedings, Part XXXI, pp. 456–473. Springer\n2. Bao H, Dong L, Piao S, Wei F (2022) BEiT: BERT pre-training\nof image transformers. In: International conference on learning\nrepresentations\n3. Bawa VS, Singh G, Kaping AF, Skarga-Bandurova I, Oleari E, Lep-\norini A, Landolfo C, Zhao P , Xiang X, Luo G et al (2021) The saras\nendoscopic surgeon action detection (esad) dataset: challenges and\nmethods. arXiv preprint arXiv:2104.03178\n4. Carstens M, Rinner FM, Bodenstedt S, Jenke AC, Weitz J, Distler\nM, Speidel S, Kolbinger FR (2023) The dresden surgical anatomy\ndataset for abdominal organ segmentation in surgical data science.\nSci Data 10(1):1–8\n5. Chen J, Lu Y , Y u Q, Luo X, Adeli E, Wang Y , Lu L, Y uille A,\nZhou Y (2021) Transunet: transformers make strong encoders for\nmedical image segmentation. arXiv preprint arXiv:2102.04306\n6. Czempiel T, Paschali M, Keicher M, Simson W, Feussner H, Kim\nST, Navab N (2020) Tecno: surgical phase recognition with multi-\nstage temporal convolutional networks. In: MICCAI 2020, pp.\n343–352. Springer\n7. Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009) Imagenet:\na large-scale hierarchical image database. In: 2009 IEEE confer-\nence on computer vision and pattern recognition, pp. 248–255\n8. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X,\nUnterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S,\nUszkoreit J, Houlsby N (2021) An image is worth 16x16 words:\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2024) 19:1085–1091 1091\ntransformers for image recognition at scale. In: International con-\nference on learning representations\n9. He K, Chen X, Xie S, Li Y , Dollár P , Girshick R (2022) Masked\nautoencoders are scalable vision learners. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition,\npp. 16000–16009\n10. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for\nimage recognition. In: 2016 IEEE conference on computer vision\nand pattern recognition (CVPR), pp. 770–778\n11. Hong WY , Kao CL, Kuo YH, Wang JR, Chang WL, Shih CS (2020)\nCholecseg8k: a semantic segmentation dataset for laparoscopic\ncholecystectomy based on cholec80. arXiv:2012.12453 [cs.CV]\n12. Izmailov P , Wilson A, Podoprikhin D, V etrov D, Garipov T (2018)\nAveraging weights leads to wider optima and better generalization.\nIn: 34th conference on uncertainty in artiﬁcial intelligence 2018,\nUAI 2018, pp. 876–885\n13. Jha D, Ali S, Emanuelsen K, Hicks SA, Thambawita V , Garcia-Ceja\nE, Riegler MA, de Lange T, Schmidt PT, Johansen HD et al (2021)\nKvasir-instrument: diagnostic and therapeutic tool segmentation\ndataset in gastrointestinal endoscopy. In: MMM 2021, pp. 218–\n229. Springer\n14. Leibetseder A, Kletz S, Schoeffmann K, Keckstein S, Keckstein\nJ (2020) Glenda: gynecologic laparoscopy endometriosis dataset.\nIn: MultiMedia Modeling: 26th International Conference, MMM\n2020, Daejeon, South Korea, January 5–8, 2020, Proceedings, Part\nII 26, pp. 439–450. Springer\n15. Leibetseder A, Petscharnig S, Primus MJ, Kletz S, Münzer B,\nSchoeffmann K, Keckstein J (2018) Lapgyn4: a dataset for 4 auto-\nmatic content analysis problems in the domain of laparoscopic\ngynecology. In: Proceedings of the 9th ACM multimedia systems\nconference, pp. 357–362\n16. Loshchilov I, Hutter F (2019) Decoupled weight decay regulariza-\ntion. In: International conference on learning representations\n17. Maier-Hein L, V edula SS, Speidel S, Navab N, Kikinis R, Park\nA, Eisenmann M, Feussner H, Forestier G, Giannarou S et al\n(2017) Surgical data science for next-generation interventions. Nat\nBiomed Eng 1(9):691–696\n18. Maier-Hein L, Wagner M, Ross T, Reinke A, Bodenstedt S, Full\nPM, Hempe H, Mindroc-Filimon D, Scholz P , Tran TN et al (2021)\nHeidelberg colorectal data set for surgical data science in the sensor\noperating room. Sci Data 8(1):101\n19. Nwoye CI, Gonzalez C, Y u T, Mascagni P , Mutter D, Marescaux\nJ, Padoy N (2020) Recognition of instrument-tissue interactions in\nendoscopic videos via action triplets. In: Medical image comput-\ning and computer assisted intervention – MICCAI 2020, 364–374.\nSpringer International Publishing\n20. Nwoye CI, Padoy N (2023) Data splits and metrics for method\nbenchmarking on surgical action triplet datasets. arXiv:2204.05235\n[cs.CV]\n21. Nwoye CI, Y u T, Gonzalez C, Seeliger B, Mascagni P , Mutter D,\nMarescaux J, Padoy N (2022) Rendezvous: attention mechanisms\nfor the recognition of surgical action triplets in endoscopic videos.\nMed Image Anal 78:102433\n22. Ranftl R, Bochkovskiy A, Koltun V (2021) Vision transformers for\ndense prediction. arXiv:2103.13413 [cs.CV]\n23. Schoeffmann K, Husslein H, Kletz S, Petscharnig S, Muenzer B,\nBeecks C (2018) Video retrieval in laparoscopic video recordings\nwith dynamic content descriptors. Multimed Tools Appl 77:16813–\n16832\n24. Silva B, Oliveira B, Morais P , Buschle L, Correia-Pinto J, Lima\nE, Vilaça JL (2022) Analysis of current deep learning networks\nfor semantic segmentation of anatomical structures in laparoscopic\nsurgery. EMBC 2022:3502–3505\n25. Sun C, Shrivastava A, Singh S, Gupta A (2017) Revisiting unrea-\nsonable effectiveness of data in deep learning era. In: Proceedings\nof the IEEE international conference on computer vision, pp. 843–\n852\n26. Twinanda AP , Shehata S, Mutter D, Marescaux J, De Mathelin M,\nPadoy N (2016) Endonet: a deep architecture for recognition tasks\non laparoscopic videos. IEEE Trans Med Imaging 36(1):86–97\n27. V alderrama N, Ruiz Puentes P , Hernández I, Ayobi N, V erlyck M,\nSantander J, Caicedo J, Fernández N, Arbeláez P (2022) Towards\nholistic surgical scene understanding. In: MICCAI 2022, pp. 442–\n452. Springer\n28. V aswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. Adv\nNeural Inf Process Syst. V ol. 30\n29. Xie Z, Zhang Z, Cao Y , Lin Y , Bao J, Yao Z, Dai Q, Hu H (2022)\nSimmim: a simple framework for masked image modeling. In:\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 9653–9663\n30. Y oon J, Lee J, Heo S, Y u H, Lim J, Song CH, Hong S, Hong S,\nPark B, Park S et al (2021) hsdb-instrument: instrument localization\ndatabase for laparoscopic and robotic surgeries. In: MICCAI 2021,\npp. 393–402. Springer\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123"
}