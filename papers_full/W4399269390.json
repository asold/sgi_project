{
    "title": "LLMs Red Teaming",
    "url": "https://openalex.org/W4399269390",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5098978416",
            "name": "Dragos Ruiu",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4319653969",
        "https://openalex.org/W4385374425",
        "https://openalex.org/W4367701241",
        "https://openalex.org/W4383473937",
        "https://openalex.org/W4402669713",
        "https://openalex.org/W4386907292",
        "https://openalex.org/W4385894687",
        "https://openalex.org/W4387389797",
        "https://openalex.org/W4285210452",
        "https://openalex.org/W3085190015",
        "https://openalex.org/W4386915872",
        "https://openalex.org/W4391724817",
        "https://openalex.org/W4309395891",
        "https://openalex.org/W4386044169",
        "https://openalex.org/W4386722141",
        "https://openalex.org/W4385570946",
        "https://openalex.org/W4391473457",
        "https://openalex.org/W4281623759",
        "https://openalex.org/W4385775119",
        "https://openalex.org/W4366548330"
    ],
    "abstract": "Abstract Prompt red-teaming is a form of evaluation that involves testing machine learning models for vulnerabilities that could result in undesirable behaviors. It is similar to adversarial attacks, but red-teaming prompts appear like regular natural language prompts, and they reveal model limitations that can cause harmful user experiences or aid violence. Red-teaming can be resource-intensive due to the large search space required to search the prompt space of possible model failures. Augmenting the model with a classifier trained to predict potentially undesirable texts is a possible workaround. Red-teaming LLMs is a developing research area, and there is a need for best practices, including persuading people to harm themselves or others and other problematic behaviors, such as memorization, spam, weapons assembly instructions, and the generation of code with pre-defined vulnerabilities. The challenge with evaluating LLMs for malicious behaviors is that they are not explicitly trained to exhibit such behaviors. Therefore, it is critical to continually develop red-teaming methods that can adapt as models become more powerful. Multi-organization collaboration on datasets and best practices can enable smaller entities releasing models to still red-team their models before release, leading to a safer user experience across the board.",
    "full_text": null
}