{
  "title": "Occupational Biases in Norwegian and Multilingual Language Models",
  "url": "https://openalex.org/W4287887298",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5057619458",
      "name": "Samia Touileb",
      "affiliations": [
        "University of Bergen"
      ]
    },
    {
      "id": "https://openalex.org/A5080614776",
      "name": "Lilja Øvrelid",
      "affiliations": [
        "University of Oslo"
      ]
    },
    {
      "id": "https://openalex.org/A5080146945",
      "name": "Erik Velldal",
      "affiliations": [
        "University of Oslo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3159454188",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2952328691",
    "https://openalex.org/W4226462293",
    "https://openalex.org/W3153066653",
    "https://openalex.org/W4287207221",
    "https://openalex.org/W3164886736",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W4289549593",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W3168584517",
    "https://openalex.org/W2802105481",
    "https://openalex.org/W3196248941",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3034779619",
    "https://openalex.org/W3102933348",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W4287642822",
    "https://openalex.org/W2972020530",
    "https://openalex.org/W2972866455",
    "https://openalex.org/W2972668795",
    "https://openalex.org/W2973192523",
    "https://openalex.org/W3037831233"
  ],
  "abstract": "In this paper we explore how a demographic distribution of occupations, along gender dimensions, is reflected in pre-trained language models. We give a descriptive assessment of the distribution of occupations, and investigate to what extent these are reflected in four Norwegian and two multilingual models. To this end, we introduce a set of simple bias probes, and perform five different tasks combining gendered pronouns, first names, and a set of occupations from the Norwegian statistics bureau. We show that language specific models obtain more accurate results, and are much closer to the real-world distribution of clearly gendered occupations. However, we see that none of the models have correct representations of the occupations that are demographically balanced between genders. We also discuss the importance of the training data on which the models were trained on, and argue that template-based bias probes can sometimes be fragile, and a simple alteration in a template can change a model's behavior.",
  "full_text": "Proceedings of the The 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 200 - 211\nJuly 15, 2022 ©2022 Association for Computational Linguistics\nOccupational Biases in Norwegian and Multilingual Language Models\nSamia Touileb\nUniversity of Bergen\nsamia.touileb@uib.no\nLilja Øvrelid\nUniversity of Oslo\nliljao@uio.no\nErik Velldal\nUniversity of Oslo\nerikve@uio.no\nAbstract\nIn this paper we explore how a demographic\ndistribution of occupations, along gender di-\nmensions, is reﬂected in pre-trained language\nmodels. We give a descriptive assessment of\nthe distribution of occupations, and investi-\ngate to what extent these are reﬂected in four\nNorwegian and two multilingual models. To\nthis end, we introduce a set of simple bias\nprobes, and perform ﬁve different tasks com-\nbining gendered pronouns, ﬁrst names, and a\nset of occupations from the Norwegian statis-\ntics bureau. We show that language speciﬁc\nmodels obtain more accurate results, and are\nmuch closer to the real-world distribution of\nclearly gendered occupations. However, we\nsee that none of the models have correct rep-\nresentations of the occupations that are demo-\ngraphically balanced between genders. We\nalso discuss the importance of the training data\non which the models were trained on, and ar-\ngue that template-based bias probes can some-\ntimes be fragile, and a simple alteration in a\ntemplate can change a model’s behavior.\n1 Introduction\nMeasuring the presence of stereotypical representa-\ntions of occupations in pre-trained language models\nhas been an important effort in combating and re-\nducing possible representational harms (Blodgett\net al., 2020). However, and as pointed out by Blod-\ngett (2021), most of the current work is motivated\nby an idealised vision of the world where occupa-\ntions should not be correlated with genders, and\nwhere the expectations are that models should not\nbe stereotypical when e.g., predicting female or\nmale pronouns in relation to occupations. The idea\nthat we are all equal is an important factor in our\nquest of reaching fair and less biased models, and\nreﬂect our normative judgments.\nWhile this is true for most stereotypes, it might\nnot directly apply to occupations. With a descrip-\ntive and realistic view of the society, there clearly\nexists gender disparities in occupations. This is\ninherently tied to many societal constructs and cul-\ntural backgrounds, and are a reality for many occu-\npations. Also pointed out by Blodgett et al. (2020),\nthe importance of the connection between language\nand social hierarchies, has not been considered in\nmost previous work on bias in NLP. It is a reality\nthat most Norwegian nurses are females. Having a\nmodel reﬂecting this reality might not be problem-\natic per se, but using this disparity to for example\nsystematically reject male applicants to a nurse po-\nsition is a very harmful effect.\nIn this paper, we investigate how the real-world\nNorwegian demographic distribution of occupa-\ntions, along the two gender dimensions male ver-\nsus female, is reﬂected in large transformer-based\npre-trained language models. We give a descrip-\ntive assessment of the distribution of occupations,\nand investigate to what extent these are reﬂected in\nfour pre-trained Norwegian and two multilingual\nmodels. More precisely, we focus on the following\nresearch questions:\n• To what extent are demographic distributions\nof genders and occupations represented in pre-\ntrained language models?\n• How are demographically clearly gender-\ncorrelated vs. gender-balanced occupations\nrepresented in pre-trained language models?\nTo address these questions, we investigate the\ncorrelations of occupations with Norwegian gen-\ndered pronouns and names. We analyse ﬁve\ntemplate-based tasks, and compare the outputs of\nthe models to real-world Norwegian demographic\ndistributions of occupations by genders.\nAfter ﬁrst providing a bias statement in Section 2,\nwe give an overview of previous relevant work in\nSection 3. Section 4 describes our experimental\nsetup, and outlines our template-based tasks. We\npresent and discuss our main results and ﬁndings in\n200\nSection 5 and 6. We conclude with a summary of\nour work, and discuss our future plans in Section 7.\n2 Bias statement\nWe follow the bias deﬁnition of Friedman and Nis-\nsenbaum (1996), where bias is deﬁned as the cases\nwhere automated systems exhibit a systematic dis-\ncrimination against, and unfairly process, a certain\ngroup of individuals. In our case, we see this as\nreﬂected in large pre-trained language models and\nhow they can contain skewed gendered representa-\ntions that can be systematically unfair if this bias\nis not uncovered and properly taken into account\nin downstream applications. Another deﬁnition of\nbias that we rely on is that of Shah et al. (2020),\nwhere bias is deﬁned as the discrepancy between\nthe distribution of predicted and ideal outcomes of\na model.\nWe focus on the associations between gendered\n(female and male) pronouns/names and profes-\nsional occupations. We investigate to what de-\ngree pre-trained language models systematically\nassociate speciﬁc genders with given occupations.\nHowever, we explore this from the perspective of\na descriptive assessment: Instead of expecting the\nsystem to treat genders equally, we compare how\nthese gender–occupation representations reﬂect the\nactual and current Norwegian demographics. This\nwill in no way reduce the representational harms\nof stereotypical female and male occupations, that\ncould both be propagated and exaggerated by down-\nstream tasks, but would rather shed light on which\noccupations are falsely represented by such models.\nMoreover, our work will provide knowledge about\nthe biases contained in these models that may be\nimportant to take into account when choosing a\nmodel for a speciﬁc application.\nArguably, a limitation of our work is that we are\nonly able to evaluate correlations between occupa-\ntions and the binary gender categories male/female,\nalthough we acknowledge the fact that gender as\nan identity spans a wider spectrum than this.\n3 Background and related work\nTraining data in NLP tasks may contain various\ntypes of bias that can be inherited by the models\nwe train (Hovy and Prabhumoye, 2021), and that\nmay potentially lead to unintended and undesired\neffects when deployed (Bolukbasi et al., 2016). The\nbias can stem from the unlabeled texts used for pre-\ntraining of Language Models (LMs), or from the\nlanguage or the label distribution used for tuning\na downstream classiﬁer. Since LMs are now the\nbackbone of most NLP model architectures, the\nextent to which they reﬂect, amplify, and spread the\nbiases existing in the input data is very important\nfor the further development of such models, and the\nunderstanding of their possible harmful outcomes.\nEfforts so far have shown a multitude of bi-\nases in pre-trained LMs and contextualized embed-\ndings. Sheng et al. (2019) show that pre-training\nthe LM BERT (Devlin et al., 2019) on a medical\ncorpus propagates harmful correlations between\ngenders, ethnicity, and insurance groups. Hutchin-\nson et al. (2020) show that English LMs contain bi-\nases against disabilities, where persons with disabil-\nities are correlated with negative sentiment words,\nand mental illness too frequently co-occur with\nhomelessness and drug addictions. Both Zhao and\nBethard (2020) and Basta et al. (2019) show that\nELMO (Peters et al., 2018) contains, and even am-\npliﬁes gender bias. Especially, Basta et al. (2019)\ndiscuss the differences of contextualized and non-\ncontextualized embeddings, and which types of\ngender bias are mitigated and which ones are am-\npliﬁed.\nMost work on detecting gender bias has focused\non template-based approaches. These templates\nare simple sentences of the form “ [pronoun]\nis a [description]”, where a description\ncould be anything from nouns referring to occu-\npations, to adjectives referring to sentiment, emo-\ntions, or traits (Stanczak and Augenstein, 2021;\nSaunders and Byrne, 2020; Bhaskaran and Bhalla-\nmudi, 2019; Cho et al., 2019; Prates et al., 2018).\nBhardwaj et al. (2021) investigate the propagation\nof gender biases of BERT in ﬁve downstream tasks\nwithin emotion and sentiment prediction. They\npropose an approach to identify gender directions\nfor each BERT layer, and use the Equity Evalua-\ntion Corpus (Kiritchenko and Mohammad, 2018)\nas an evaluation of their approach. They show that\ntheir approach can reduce some of the biases in\ndownstream tasks. Nozza et al. (2021) also use a\ntemplate- and lexicon-based approach, in this case\nfor sentence completion. They introduce a dataset\nfor the six languages English, French, Italian, Por-\ntuguese, Romanian, and Spanish, and show that\nLMs both reproduce and amplify gender-related\nsocietal stereotypes.\nAnother series of work that have focused on\ntemplate-based datasets are those building on the\n201\nOccupation Female% Male% Occupation Female% Male%\nKnitting craftsman 100 0 Architect 49.9 50.1\nMidwife 99.8 0.2 Lawyer 48.1 51.9\nEsthetician 99.3 0.7 Politician 48.1 51.9\nHealth Secretary 98.8 1.2 Associate Professor 47.2 52.8\nPhD candidate 52.8 47.2 Scaffolding builder 0.5 99.5\nPsychiatrist 52.6 47.4 Chief engineer 0.4 99.6\nDoctor 51.6 48.4 Coastal skipper 0 100\nTable 1: A selection of occupations from the Norwegian statistics bureau, the gold reference distribution of occu-\npations and genders. The occupations presented here are either dominated by more than 98% of either gender, or\nhave a more balanced distribution (underlined percentages) between both female and male genders.\nWinograd Schemas data (Levesque et al., 2012).\nThis dataset was developed for the task of corefer-\nence resolution, and contains a set of manually an-\nnotated templates that requires commonsense rea-\nsoning about coreference. It is used to explore the\nexistence of biases in coreference resolution sys-\ntems, by measuring the dependence of the system\non gendered pronouns along stereotypical and non-\nstereotypical gender associations with occupations.\nSimilarly, the WinoBias (Zhao et al., 2018) dataset\nfocuses on the relationship between gendered pro-\nnouns and stereotypical occupations, and is used\nto explore the existing stereotypes in models. The\nWinoGender dataset (Rudinger et al., 2018) also\ncontains sentences focusing on the relationship be-\ntween pronouns, persons, and occupations. Here,\nthey also include gender-neutral pronouns. Unlike\nWinoBias, WinoGender’s sentences are built such\nthat there is a coreference between pronouns and\noccupations, and between the same pronouns and\npersons. Based on these datasets for coreference\nresolution, WinoMT (Stanovsky et al., 2019) has\nbeen developed for the task of machine translation.\nThe dataset also contains stereotypical and non-\nstereotypical templates used to investigate gender\nbias in machine translation systems.\nMoreover, Bender et al. (2021) point out the dan-\ngers of LMs and how they can potentially amplify\nthe already existing biases that occur in the data\nthey were trained on. They highlight the impor-\ntance of understanding the harmful consequences\nof carelessly using such models in language pro-\ncessing, and how they in particular can hurt minori-\nties. They also discuss the difﬁculty of identifying\nsuch biases, and how complicated it can be to tackle\nthem. This is partly due to poor framework deﬁni-\ntions, i.e., how culturally speciﬁc they are, but also\nhow unreliable current bias evaluation methods are.\nWe focus therefore in this work on investigating\nhow culturally speciﬁc Norwegian demographics\nrelated to gender and occupations are reﬂected in\nfour Norwegian and two multilingual pre-trained\nLMs. Our work differs from previous work in that\nwe ground our bias probes to real-world distribu-\ntions of gender, and rather than expecting the mod-\nels to always have a balanced representation of\ngenders, we explore to which degree they reﬂect\ntrue demographics.\n4 Experimental setup\nFollowing the methodology of previous research\non gender bias in pre-trained language models, and\ndue to the corresponding lack of resources for Nor-\nwegian, we generate our own set of templates that\nwe use with the pre-trained language models to\nmake use of their ability to compute the probabili-\nties of words and sentences. We present an empiri-\ncal analysis of gender biases towards occupational\nassociations. By using the templates we hope to\nreduce variation by keeping the semantic structure\nof the sentence. We analyze the probability dis-\ntributions of returned pronouns, occupations, and\nﬁrst names; and compare them to real-world gold\ndata representing the demographic distribution in\nNorway. Investigating the differences between the\nmodels can also give us insights into the content of\nthe various types of corpora they were trained on.\nData and codes will be made available1.\nBelow we discuss in turn (i) the gold reference\ndistribution of occupations and genders, ( ii) the\ntemplates, (iii) how the templates are used for prob-\ning pre-trained language models, and ﬁnally ( iv)\nthe models that we test.\n1https://github.com/SamiaTouileb/\nBiases-Norwegian-Multilingual-LMs\n202\nReference distribution We use a set of 418 oc-\ncupations. These represent the demographic dis-\ntribution of females and males in the respective\noccupations in Norway2 originating from the Nor-\nwegian statistics bureau. The bureau releases yearly\nstatistics covering various aspects of the Norwe-\ngian society, and all data is made freely available.\nThis list comprises a ﬁne-grained level of occupa-\ntions, where e.g., lege (doctor) and allmennlege\n(general practitioner) are considered two different\noccupations. The gender-to-occupation ratios in\nthese statistics are used as ‘gold standard’ when\nprobing the models.\nIn Table 1 we show some examples of the oc-\ncupations dominated by more than 98% of either\ngender, and those that have a more balanced distri-\nbution (underlined). Culturally speaking, Norway\nis known to strive for gender balance in all occupa-\ntions. While this is true for many instances, there\nare still some occupations that are unbalanced in\ngender-distribution. From the Norwegian statis-\ntics bureau, it is clear that most midwives are still\nwomen, and that most chief engineers are males.\nHowever, for occupations as Phd candidates, psy-\nchiatrist, doctor, architect, lawyer, politician, and\nassociate professor the distribution of genders is\nmore balanced.\nTemplates Our templates combine occupations,\npronouns, and ﬁrst names. We focus on ﬁve\ntemplate-based tasks, and generate the following\ncorresponding templates that we use as bias probes\n(Solaiman et al., 2019):\n1. Task1: [pronoun] is a/an [occupation]\n(original: [pronoun] er [occupation])\n2. Task2: [pronoun] works as a/an [occupation]\n(original: [pronoun] jobber som [occupa-\ntion])\n3. Task3: [name] is a/an [occupation]\n(original: [name] er [occupation])\n4. Task4: [name] works as a/an [occupation]\n(original: [name] jobber som [occupation])\n5. Task5: the [occupation] [name]\n(original: [occupation] [name])\nAs pronouns, our work mainly focuses on hun\nand han (she and he respectively). As demographic\nstatistics are still made using a binary gender dis-\ntribution, we could not include the gender neutral\n2https://utdanning.no/likestilling\npronoun hen (they), which is, in addition, rarely\nused in Norway.\nAs ﬁrst names, we also extract from the Norwe-\ngian statistics bureau3 the 10 most frequent female\nand male names in Norway from 1880 to 2021,\nthis results in 90 female names and 71 male names.\nFor tasks 1–4 we use the full set of 418 occupa-\ntions, while in task 5 we focus on those that either\nhave a balanced distribution between genders or\nare clearly female- or male-dominated. This was\ndecided after an analysis of the distribution of occu-\npations across genders, and resulted in two thresh-\nolds. All occupations that had between 0 and 10%\ndifferences in distribution, were deemed balanced\n(e.g., 51% female and 49% male). All occupations\nthat had more than 75% distribution of one gender\nagainst the other, were deemed unbalanced, and\nare referred to as either clearly female (≥75%) or\nclearly male (≥75%) occupations. This resulted in\na set of 31 clearly female occupations, 106 clearly\nmale occupations, and 49 balanced occupations.\nFor tasks 1 and 2, we mask the pronouns and\ncompute the probability distribution across the oc-\ncupations for female and male pronouns. For tasks\n3, 4, and 5, we mask the occupations and com-\npute the probability distributions in each bias-probe.\nMasking pronouns will allow us to uncover how\nlikely a gendered pronoun is correlated with an oc-\ncupation, and masking the occupation will allow us\nto uncover how likely occupations are correlated\nwith female and male names.\nProbing and evaluation For each task, we ﬁrst\ngenerate the probability distributions of masked to-\nkens in each bias probe. In order to have a compara-\nble distribution to the gold standard (which is given\nas a percentage), we compute a simple percentage\nrepresentation of the probability distributions by\nfollowing the following formula:\nf_pron% = prob f_pron\nprob f_pron+prob m_pron\nWhere f_pron% is the percentage of a female\npronoun, and prob x_pronis the output probability\nof each model for each of the female and male pro-\nnouns. The same simple formula is used in all tasks.\nWe are aware that this is a simpliﬁed representation\nof the output of each model, nevertheless, we be-\nlieve that it will not change the overall distribution.\nOnce probability distributions are mapped to per-\n3https://www.ssb.no/befolkning/navn/\nstatistikk/navn\n203\ncentages, we quantify the difference between fe-\nmale and male scores by simply subtracting the\nscores of males from the scores of female. Posi-\ntive values will represent occupations that are more\nstrongly associated with females than males by the\nmodel, and negative values represent the opposite.\nThis is also applied to the gold standard data. We\nuse the demographic distribution of the occupations\nfrom the Norwegian statistics bureau as gold data.\nBased on this, values greater than 0 are deemed\nfemale-dominated occupations, and values lower\nthat 0 are male-dominated occupation. This is used\nto compute the macro F1 values for each model.\nPre-trained language models We analyse the\npredictions of six pre-trained language models, four\nNorwegian and two multilingual. Note that Norwe-\ngian has two ofﬁcial written standards; Bokmål (lit-\nerally ‘book tongue’) and Nynorsk (literally ‘new\nNorwegian’). While Bokmål is the main variety,\nroughly 15% of the Norwegian population write\nin the Nynorsk variant. All the Norwegian models\nare trained on data comprising both Bokmål and\nNynorsk.\n• NorBERT (Kutuzov et al., 2021): trained on\nthe Norwegian newspaper corpus4, and Nor-\nwegian Wikipedia, comprising about two bil-\nlion word tokens.\n• NorBERT25: trained on the non-copyrighted\nsubset of the Norwegian Colossal Corpus\n(NCC)6 and the Norwegian subset of the C4\nweb-crawled corpus (Xue et al., 2021). In to-\ntal, it comprises about 15 billion word tokens.\n• NB-BERT (Kummervold et al., 2021): trained\non the full NCC, and follows the architecture\nof the BERT cased multilingual model (Devlin\net al., 2019). It comprises around 18.5 billion\nword tokens.\n• NB-BERT_Large7: trained on NCC, and fol-\nlows the architecture of the BERT-large un-\ncased model.\n• mBERT (Devlin et al., 2019): pre-trained on\na set of the 104 languages with the largest\n4https://www.nb.no/sprakbanken/\nressurskatalog/oai-nb-no-sbr-4/\n5https://huggingface.co/ltgoslo/\nnorbert2\n6https://github.com/NbAiLab/notram/\nblob/master/guides/corpus_description.md\n7https://huggingface.co/NbAiLab/\nnb-bert-large\nWikipedia pages.\n• XLM-RoBERTa (Conneau et al., 2020):\ntrained on a collection of 100 languages from\nthe Common Crawl corpus.\nAs can be seen above, each model has been\ntrained on different types of corpora, and are all\nof various sizes. The NCC corpus, is a collec-\ntion of OCR-scanned documents from the Norwe-\ngian library’s collection of newspapers and works\nof ﬁction (with publishing years ranging from\nearly 1800s to present day), government reports,\nparliament collections, OCR public reports, le-\ngal resources such as laws, as well as Norwegian\nWikipedia. In short, some models are trained on\nwell structured texts, that follow a somewhat for-\nmal style, while other models also include less\nstructured texts in the form of online content.\n5 Results\nTable 2 summarizes the overall results for all mod-\nels. We also compute class-level F1 values for each\ntask, these can be found in Table 3 and Figure 5.\nBelow we discuss the task-wise results in more\ndetail.\n5.1 Task1: (she|he) is a/an [occupation]\nIn the ﬁrst task, we mask the pronouns she and\nhe in our bias probes. We focus on the full set of\n418 occupations. As can be seen in Table 2, all\nfour Norwegian models give higher scores than\nthe two multilingual models. NB-BERT and NB-\nBERT_Large have a macro F1 of 0.75, and are the\nhighest performing models overall. It should be\npointed out that these are also the biggest Norwe-\ngian models in terms of token counts. NorBERT is\nthe less performing Norwegian model in this task,\nand has a macro F1 a few percentiles higher than\nthe multilingual model XLM-RoBERTa. We be-\nlieve that this might be impacted by the the size of\nNorBERT, which is the smallest Norwegian model\nin terms of token counts.\nLooking at class-level F1 scores from Table 3,\nall models achieve high F1 scores for the male\nclass, with NB-BERT_Large achieving the highest\nscore with an F1 of 0.84, and mBERT achieving\nthe lowest one with an F1 of 0.74. In contrast, all\nmodels have substantially lower F1 score on the\nfemale class. Again, NB-BERT_Large achieves\nthe highest score with 0.67 F1, and mBERT the\nlowest with 0.30. This shows that the models are\nalready somehow skewed towards the male class.\n204\nmodel Task1 Task2 Task3 Task4 Task5_b Task5_ub\nNorBERT 0.69 0.67 0.60 0.35 0.46 0.83\nNorBERT2 0.73 0.54 0.77 0.72 0.52 0.76\nNB-BERT 0.75 0.74 0.70 0.80 0.69 0.77\nNB-BERT_Large 0.75 0.82 0.80 0.74 0.49 0.76\nmBERT 0.52 0.42 0.52 0.52 0.52 0.55\nXLM-RoBERTa 0.65 0.50 0.68 0.49 0.47 0.56\nTable 2: Macro F1 of models compared to the real-world “gold” distribution. Task1: [pronoun] is a/an\n[occupation], Task2: [pronoun] works as a/an [occupation], Task3: [name] is a/an\n[occupation], Task4: [name] works as a/an [occupation], Task5_b: the [occupation]\n[name] with balanced distributions in gold, Task5_ub: the [occupation] [name] with clearly female\nand male occupation distributions in gold.\nFigure 1: Task1, class-level F1 values focusing on bal-\nanced and unbalanced occupations.\nIn addition to looking at the distribution of all\noccupations, and based on the previous observation\nthat all models seem to reﬂect male occupations\nbut to a lesser extent reﬂect female occupations, we\nhave looked at the occupations that have balanced\nand unbalanced distributions in the gold data. The\nunbalanced occupations as previously mentioned,\nare those which are clearly female or male occu-\npations (more than 75% distribution of one gender\nagainst the other). The balanced distribution are\nthose that have between 0 and 10% differences in\ngender distribution in the gold data. Results are\ndepicted in Figure 1.\nWhen it comes to clearly female occupations,\nthe three biggest Norwegian models, namely Nor-\nBERT2, NB-BERT, and NB-BERT_Large obtain\nhighest F1 values with 0.87, 0.92, and 0.89 respec-\ntively. Followed by XLM-RoBERTa and NorBERT.\nFor clearly male occupations, all models have high\nF1 values, with the three top ones being again Nor-\nBERT2, NB-BERT, and NB-BERT_Large. The\ntwo multilingual models achieve quite high values,\nwith XLM-RoBERTa outperforming NorBERT\nhere again. It is quite clear that the Norwegian mod-\nels have a good representation of clearly female and\nmale occupations. Another compelling result is that\nXLM-RoBERTa has a quite accurate representation\nof these unbalanced occupations, equating the ones\nfrom the smallest Norwegian model NorBERT.\nFocusing on balanced occupations, most models\nexhibit a tendency to represent occupations as male.\nNorBERT, NB-BERT, and XLM-RoBERTa are the\nonly models that seem to have a decent representa-\ntion of female occupations. The expectations here\nare not that the models would give a better represen-\ntation of female occupations, but rather be equally\ngood at representing both genders.\n5.2 Task2: (she|he) works as a/an\n[occupation]\nIn this second task, we also mask the pronouns and\ncompute their probabilities in the bias probes. We\nhere again focus on the full set of occupations, 418\noccupations.\nNB-BERT_Large is the strongest model for this\ntask as well, with all four Norwegian models out-\nperforming the two multilingual ones. Interestingly,\ndespite this task being quite similar to the ﬁrst task,\nmodels do not seem to contain similar representa-\ntions, and a minor change of wording in the bias\nprobe shifts the results such that one model per-\nforms better (NB-BERT_Large), while other mod-\nels show a small decline in performance (NorBERT\nand NB-BERT), and the remaining seem to loose\nquite a few F1 percentiles. We believe that this re-\nﬂects the input data the models are trained on, and\nalso shows the fragility of testing template-based\nbias probes. Focusing on class-level results, only\nNorBERT2 and XLM-RoBERTa achieve higher\nvalues for female occupations. The rest of the mod-\n205\nTask1 Task2 Task3 Task4\nmodel F M F M F M F M\nNorBERT 0.59 0.78 0.57 0.77 0.61 0.60 0.58 0.13\nNorBERT2 0.63 0.83 0.63 0.45 0.71 0.84 0.72 0.71\nNB-BERT 0.66 0.83 0.73 0.74 0.60 0.81 0.77 0.84\nNB-BERT_large 0.67 0.84 0.77 0.87 0.77 0.82 0.74 0.74\nmBERT 0.30 0.74 0.07 0.76 0.34 0.69 0.31 0.73\nXLM-RoBERTa 0.52 0.77 0.60 0.40 0.59 0.76 0.61 0.36\nTable 3: Class-level (Male/Female) F1 when compared to the real-world “gold” distribution for tasks 1–4\nFigure 2: Task2, class-level F1 values focusing on bal-\nanced and unbalanced occupations.\nels mostly represent male occupations, except for\nNB-BERT, which seems to be equally good at rep-\nresenting both.\nSimilarly to Task1, we did a more thorough anal-\nysis by focusing on the balanced and unbalanced\ndistributions of occupations, this can be seen in\nFigure 2.\nFor clearly female occupations, the three Nor-\nwegian models NorBERT2, NB-BERT, and NB-\nBERT_Large have the highest F1 scores, with\nrespectively 0.71, 0.91, and 0.91. The Norwe-\ngian model with the lowest score is NorBERT,\nwhich here too is outperformed by XLM-RoBERTa.\nThe multilingual mBERT model seems to suffer\nfrom representations of clearly female occupa-\ntions. Turning instead to clearly male occupations,\nmBERT is the third best performing model, with\nan F1 of 0.81, preceded by NorBERT2 with 0.87\nF1, and NB-BERT and NB-BERT_Large with both\nan F1 of 0.97. XLM-RoBERTa still has a higher re-\nsult than NorBERT with respectively F1 scores of\n0.45 and 0.22. The overall observation here is that\nthe three largest Norwegian models have a quite\naccurate representation of clearly female and male\noccupations compared to the multilingual ones. It\nalso seems that the size of the training data matters,\nas NorBERT does not equate with other models.\nFor balanced occupations, and compared to the\nﬁrst task, models in Task2 seem to either have a rep-\nresentation of occupations as being female or males\nones. NorBERT2, NB-BERT, and XLM-RoBERTa\nseems to be accurate when it comes to represent-\ning the occupations as female, but performs poorly\nwhen it comes to mapping them to male occupa-\ntions, in particular for XLM-RoBERTa. In contrast,\nNorBERT, NB-BERT_Large and mBERT seem to\nhave a good representation of occupations as be-\ning males ones, with mBERT not portraying any\noccupations as being female occupations.\n5.3 Task3: [name] is a/an [occupation]\nIn this task, we use the set of most frequent Norwe-\ngian ﬁrst names from 1880 to 2021. Contrary to the\nprevious two tasks, here we mask the occupations\n(total of 418), and compute the probability of each\noccupation co-occurring with female and male ﬁrst\nnames. While tasks 3 and 4 are quite similar to\ntasks 1 and 2, we are here switching what is being\nmasked, and focus on more than just two pronouns.\nFrom Table 2, we can see that similarly to the\ntwo previous tasks, NB-BERT_Large is the high-\nest performing model, followed by the two other\nbig Norwegian models NB-BERT and NorBERT2.\nXLM-RoBERTa outperforms the smallest Norwe-\ngian model NorBERT, while mBERT is the least\nperforming one. The results for this task are com-\nparable to the most similar task, Task1.\nZooming in on class-level F1 scores, all four\nNorwegian models are good at representing female\noccupations, outperforming both multilingual mod-\nels. The best performing model is here again NB-\nBERT_Large with mBERT being the least perform-\ning one. For male occupations, all models achieve\nhigh scores, with NorBERT2 achieving the high-\n206\nFigure 3: Task3, class-level F1 values focusing on bal-\nanced and unbalanced occupations.\nest F1 of 0.84, and NorBERT achieving the lowest\nscore of 0.60 F1.\nAs for the two previous tasks, we also look at\nthe balanced and unbalanced occupations from the\ngold data, and explore how each of these are re-\nﬂected in the models using Task3’s bias probe.\nThese can be seen in Figure 3.\nFor clearly female occupations (unbalanced_F),\nall Norwegian models in addition to XLM-\nRoBERTa have high F1 scores. Similarly to previ-\nous tasks, mBERT is the least performing one with\nan F1 score of 0.23. For clearly male occupations\n(unbalanced_M) all models have high F1 scores,\nwith NB-BERT_Large scoring highest with an F1\nof 0.98, followed by NorBERT2 (0.96), NB-BERT\n(0.93), XLM-RoBERTa (0.89), mBERT (0.79), and\nNorBERT (0.71). The three Norwegian models\nNorBERT2, NB-BERT, and NB-BERT_Large, in\naddition to XLM-RoBERTa seem to have a rather\ngood representation of clearly female and male oc-\ncupations. NorBERT seems to lack some of the\nfemale occupations, while mBERT suffers even\nmore.\nFor balanced occupations, where models should\nhave an equally good representation of both gen-\nders, only NorBERT and NB-BERT_Large seem\nto reﬂect this. NorBERT2 and XLM-RoBERTa are\na bit better at representing male occupations, while\nNB-BERT and mBERT seem to be much better at\nrepresenting males than at representing females.\n5.4 Task4: [name] works as a/an [occupation]\nSimilarly to Task3, we mask occupations and inves-\ntigate their correlations with female and male ﬁrst\nnames. As for Task2, we here use the probe ﬁxed\nby the sequence “works as a/an”. From Table 2,\nit is apparent that the three big Norwegian mod-\nels NorBERT2, NB-BERT, and NB-BERT_Large\nFigure 4: Task4, class-level F1 values focusing on bal-\nanced and unbalanced occupations.\nwith respective F1 scores of 0.72, 0.80, 0.74, are\nthe models with the highest scores for the task.\nThe two mulitlingual models mBERT and XLM-\nRoBERTa seem to achieve similar scores, while\nNorBERT gets the lowest F1 score which is maybe\nless surprising. The probe would expect a descrip-\ntion of a person with ﬁrst name followed by the\ndescription of the occupation. As NorBERT is\ntrained on newspaper articles and Wikipedia, the\npresence of such patterns might be less probable\nthan e.g. in books and literary works, which all of\nthe other Norwegian models have been exposed to\nin their training data.\nFor class-level F1 scores, the best model is NB-\nBERT on representing both female and male occu-\npations. NorBERT2 and NB-BERT_Large are also\nvery good at representing both genders. However,\nNorBERT and XLM-RoBERTa seem to be more\naccurate in representing female occupations, while\nmBERT behaves in the opposite direction.\nAs for other tasks, we also explored the behavior\nof the models with regards to balanced and un-\nbalanced distributions of occupations in the gold\nstandard, and how these are reﬂected in the models.\nThis can be seen in Figure 4.\nSimilar to previous tasks NorBERT2, NB-BERT,\nand NB-BERT_Large have good representations\nof clearly female occupations, while NorBERT\nand XLM-RoBERTa have similar performances,\nand mBERT has the lowest performance. For\nclearly male occupations, NorBERT seems to suf-\nfer most, while XLM-RoBERTa performs equally\nfor male representation. The four remaining mod-\nels have high F1 values, with NB-BERT and NB-\nBERT_Large achieving highest scores with an F1\nof 0.97. For balanced occupations, NorBERT, Nor-\nBERT2, NB-BERT_Large, and XLM-RoBERTa\nhave decent F1 scores and seem to represent occu-\n207\npations as female ones. NB-BERT have a good rep-\nresentation of occupations for both genders, while\nmBERT again seem to have a better representation\nof male occupations than those of females.\n5.5 Task5: the [occupation] [name]\nWe here focus on the clearly balanced and non\nbalanced occupations from our gold data. All occu-\npations that have between 0 and 10% differences\nbetween the distribution of genders are referred to\nas balanced occupations. Clearly female occupa-\ntions are those whose distribution exceeds 75%,\nand similarly to the male counterparts, all occupa-\ntions where male represent 75% of the total dis-\ntribution, are referred to as clearly male occupa-\ntions. We create a different set of probes, where\nwe again mask the occupation and investigate their\ncorrelations with female and male ﬁrst names. The\ndifference between this task and say Task 3, is that\nfor the occupation lawyer, advokat in Norwegian,\nthe template in Task3 would be: “Oda er advokat”\n(“Oda is a lawyer”), while in Task5 it would be:\n“advokaten Oda” (“the lawyer Oda”), where the oc-\ncupation is a pre-nominal modiﬁer. While the main\nidea remains the same, exploring occupational bi-\nases in pre-trained language models, we here ex-\nperiment with syntactic variations of the templates\nof bias probes to see how the models behave and\nwhether different probes will give different signs\nof biases.\nFocusing on the balanced occupations, from\nTable 2, all models achieve an F1 score of at\nleast 0.46, with NB-BERT reaching the highest\nF1 value of 0.69. There is no clear difference in\nperformance between the Norwegian and multi-\nlingual models. For the unbalanced occupations,\nNorBERT achieves best F1 score with a value of\n0.83. Followed by NB-BERT, NorBERT2, and\nNB-BERT_Large with respectively 0.77, 0.76, and\n0.76 F1 values. While the two multilingual mod-\nels have at least 0.20 F1 values less than the least\nperforming Norwegian model. That NorBERT is\nthe highest performing here comes perhaps as no\nsurprise. As it has been trained on newspaper arti-\ncles and Wikipedia pages, the form of the template\nseems natural in e.g. reporting cases where people\nare introduced by their occupations.\nClass-based F1 scores can be seen in Figure 5.\nThe four Norwegian models have good represen-\ntations of both clearly female (unbalanced_F) and\nclearly male (unbalanced_M) occupations. With\nFigure 5: Task5, class-level F1 values focusing on bal-\nanced and unbalanced occupations.\nNorBERT achieving higher scores on both genders,\nand being the best model. NorBERT2, NB-BERT,\nand NB-BERT_large have a bit lower F1 values\nfor clearly female occupations, but are still outper-\nforming the multilingual models.\nFor the balanced occupations, NB-BERT and\nNB-BERT_Large are the only models with an F1\nhigher than 0.50 for female occupations, while\nNorBERT, NorBERT2, and XLM-RoBERTa per-\nforming for the ﬁrst time worse than mBERT.\nFor the representation of males in balanced oc-\ncupations, most models achieve good F1 scores,\nwith the exception of NB-BERT_Large with an\nF1 of 0.44. We believe that this is again a\nsign of the input data the models have been ex-\nposed to during their training. Templates as the\n[occupation][name] might not be a frequent\nlanguage use in literary works, or parliament and\ngovernment reports, nor in Wikipedia pages. We\nbelieve that this might have impacted the perfor-\nmance of the models exposed to these types of\ndata.\n6 Discussion\nOne of our main observations is that models be-\nhave differently based on the template used as bias\nprobe. The templates we have used, in e.g., Task1\nand Task2, and Task3 and Task4, differ only by\none token, and do not change the semantics of the\ntemplate even if it changes its syntactic realization.\nThis might both be due to the input data on which\nthe models have been trained on, but can also be a\nmanifestation of the fragility of the template-based\napproach. While these types of approaches do shed\nlight on the inner representations of models, it is\ndifﬁcult to point out why exactly a subtle change\nin the expression of a template can seemingly alter\na model’s representation.\n208\nFigure 6: Example of balanced and unbalanced occupations in gold data, and each model’s prediction in Task1.\nAnother interesting observation, is that language-\nspeciﬁc models seem to be better at identifying\nthe clearly unbalanced occupations, that demo-\ngraphically are clearly female or male occupa-\ntions. While both language-speciﬁc and multi-\nlingual models are not able to correctly repre-\nsent gender-balanced occupations. This in turn\nof course, indicates that these models do contain\nbias, and mostly map gender-balanced occupations\nas male-dominated ones. To give a simple example\nof this phenomenon, we show in Figure 6 a couple\nof handpicked examples of demographically bal-\nanced and unbalanced occupations from our gold\ndata for the ﬁrst task, Task1: [pronoun] is\na/an [occupation]. We compare these real-\nworld representations to those of each of the four\nNorwegian and two multilingual models.\nThe occupations with positive values in gold\n(green bar, ﬁrst to the left in each group) are\nfemale-dominated occupations, and occupations\nwith negative values are male-dominated occupa-\ntions. As previously mentioned, occupations with\nvalues [−10, +10] in gold are deemed to be gender-\nbalanced occupations. In Figure 6, the occupations\ndiplomat, doctor, associate professor, and judge\nare demographically gender-balanced occupations\nin Norway. The occupations midwife, secretary,\nand nurse are female-dominated, and the occu-\npations pilot, plumber, and bricklayer are male-\ndominated. As can be seen from the ﬁgure, all four\nNorwegian models are very good at representing\nthe clearly female- and male-dominated occupa-\ntions (with the exception of NorBERT2 for secre-\ntary). The same holds for the multilingual models,\nexcept for mBERT for nurse, and XLM-RoBERTa\nfor bricklayer.\nWhen it comes to gender-balanced occupations,\nit is quite clear from Figure 6 that all models fail\nto predict probabilities near the real demographic\ndistribution. However, NorBERT gives the clos-\nest distribution for the two occupations diplomat\nand associate professor, while for doctor, it is the\ntwo multilingual models and mBERT and XLM-\nRoBERTa that give the closest distribution.\n7 Conclusion\nWe have presented in this paper an investigation\ninto how a demographic distribution of occupa-\ntions, along two gender dimensions, is reﬂected\nin pre-trained language models. The demographic\ndistribution is a real-world representation from the\nNorwegian statistics bureau. Instead of giving a\nnormative analysis of biases, we give a descriptive\nassessment of the distribution of occupations, and\ninvestigate how these are reﬂected in four Norwe-\ngian and two multilingual language models.\nWe have generated simple bias probes for ﬁve\ndifferent tasks combining pronouns and occupa-\ntions, and ﬁrst names and occupations. Our main\nobservations are that Norwegian language-speciﬁc\nmodels give closer results to the real-world distribu-\ntion of clearly gendered occupations. Moreover, all\n209\nmodels, language-speciﬁc and multilingual, have a\nbiased representation of gender-balanced occupa-\ntions. Our investigations also show the fragility of\ntemplate-based approaches, and the importance of\nthe models’ training data.\nIn future work, we plan to extend our investiga-\ntions and include several demographic distributions\nfrom other countries, and compare them to their\nrespective language-speciﬁc pre-trained language\nmodels to corroborate our ﬁndings.\nAcknowledgment\nThis work was supported by industry partners and\nthe Research Council of Norway with funding to\nMediaFutures: Research Centre for Responsible\nMedia Technology and Innovation, through the cen-\nters for Research-based Innovation scheme, project\nnumber 309339.\nReferences\nChristine Basta, Marta R. Costa-jussà, and Noe Casas.\n2019. Evaluating the underlying gender bias in con-\ntextualized word embeddings. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 33–39, Florence, Italy. Associa-\ntion for Computational Linguistics.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big?. In Proc. of the 2021 ACM Conference\non Fairness, Accountability, and Transparency.\nRishabh Bhardwaj, Navonil Majumder, and Soujanya\nPoria. 2021. Investigating gender bias in bert. Cog-\nnitive Computation, 13(4).\nJayadev Bhaskaran and Isha Bhallamudi. 2019. Good\nsecretaries, bad truck drivers? occupational gender\nstereotypes in sentiment analysis. In Proceedings of\nthe First Workshop on Gender Bias in Natural Lan-\nguage Processing, pages 62–68, Florence, Italy. As-\nsociation for Computational Linguistics.\nSu Lin Blodgett. 2021. Sociolinguistically driven ap-\nproaches for just natural language processing.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. Ad-\nvances in neural information processing systems, 29.\nWon Ik Cho, Ji Won Kim, Seok Min Kim, and\nNam Soo Kim. 2019. On measuring gender bias in\ntranslation of gender-neutral pronouns. In Proceed-\nings of the First Workshop on Gender Bias in Natu-\nral Language Processing, pages 173–181, Florence,\nItaly. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBatya Friedman and Helen Nissenbaum. 1996. Bias in\ncomputer systems. ACM Transactions on Informa-\ntion Systems (TOIS), 14(3).\nDirk Hovy and Shrimai Prabhumoye. 2021. Five\nsources of bias in natural language processing. Lan-\nguage and Linguistics Compass, 15(8).\nBen Hutchinson, Vinodkumar Prabhakaran, Emily\nDenton, Kellie Webster, Yu Zhong, and Stephen De-\nnuyl. 2020. Social biases in NLP models as barriers\nfor persons with disabilities. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5491–5501, Online. As-\nsociation for Computational Linguistics.\nSvetlana Kiritchenko and Saif Mohammad. 2018. Ex-\namining gender and race bias in two hundred sen-\ntiment analysis systems. In Proceedings of the\nSeventh Joint Conference on Lexical and Compu-\ntational Semantics, pages 43–53, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nPer Egil Kummervold, Javier de la Rosa, Freddy Wet-\njen, and Svein Arne Brygfjeld. 2021. Operationaliz-\ning a national digital library: The case for a norwe-\ngian transformer model. In Proc. of the 23rd Nordic\nConference on Computational Linguistics (NoDaL-\niDa 2021).\nAndrey Kutuzov, Jeremy Barnes, Erik Velldal, Lilja\nØvrelid, and Stephan Oepen. 2021. Large-scale con-\ntextualised language modelling for norwegian. In\nProc. of the 23rd Nordic Conference on Computa-\ntional Linguistics (NoDaLiDa 2021).\n210\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth international conference on the princi-\nples of knowledge representation and reasoning.\nDebora Nozza, Federico Bianchi, and Dirk Hovy.\n2021. HONEST: Measuring hurtful sentence com-\npletion in language models. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Online. Association\nfor Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMarcelo O. R. Prates, Pedro H. C. Avelar, and Luis\nLamb. 2018. Assessing gender bias in machine\ntranslation – a case study with google translate.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 8–14, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nDanielle Saunders and Bill Byrne. 2020. Addressing\nexposure bias with document minimum risk train-\ning: Cambridge at the WMT20 biomedical transla-\ntion task. In Proceedings of the Fifth Conference on\nMachine Translation, pages 862–869, Online. Asso-\nciation for Computational Linguistics.\nKrunal Shah, Nitish Gupta, and Dan Roth. 2020. What\ndo we expect from multiple-choice QA systems? In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 3547–3553, Online.\nAssociation for Computational Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Rad-\nford, Gretchen Krueger, Jong Wook Kim, Sarah\nKreps, et al. 2019. Release strategies and the so-\ncial impacts of language models. arXiv preprint\narXiv:1908.09203.\nKarolina Stanczak and Isabelle Augenstein. 2021. A\nsurvey on gender bias in natural language process-\ning.\nGabriel Stanovsky, Noah A. Smith, and Luke Zettle-\nmoyer. 2019. Evaluating gender bias in machine\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1679–1684, Florence, Italy. Association for\nComputational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nOnline. Association for Computational Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15–20,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nYiyun Zhao and Steven Bethard. 2020. How does\nBERT’s attention change when you ﬁne-tune? an\nanalysis methodology and a case study in negation\nscope. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4729–4747, Online. Association for Computa-\ntional Linguistics.\n211",
  "topic": "Norwegian",
  "concepts": [
    {
      "name": "Norwegian",
      "score": 0.9572135210037231
    },
    {
      "name": "Computer science",
      "score": 0.6428516507148743
    },
    {
      "name": "Distribution (mathematics)",
      "score": 0.6234189867973328
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6016417741775513
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5252587199211121
    },
    {
      "name": "Natural language processing",
      "score": 0.49359023571014404
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4251289963722229
    },
    {
      "name": "Linguistics",
      "score": 0.3724147081375122
    },
    {
      "name": "Mathematics",
      "score": 0.1595095992088318
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}