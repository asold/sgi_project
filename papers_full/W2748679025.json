{
  "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models",
  "url": "https://openalex.org/W2748679025",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2613029335",
      "name": "Anuroop Sriram",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2613753412",
      "name": "Heewoo Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113134335",
      "name": "Sanjeev Satheesh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141511558",
      "name": "Adam Coates",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950903920",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2951991713",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2193413348",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2284660317",
    "https://openalex.org/W2550837020",
    "https://openalex.org/W2953022181",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2949640717",
    "https://openalex.org/W2259472270"
  ],
  "abstract": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.",
  "full_text": "Cold Fusion: Training Seq2Seq Models Together with Language Models\nAnuroop Sriram 1 Heewoo Jun 1 Sanjeev Satheesh 1 Adam Coates 1\nAbstract\nSequence-to-sequence (Seq2Seq) models with\nattention have excelled at tasks which in-\nvolve generating natural language sentences such\nas machine translation, image captioning and\nspeech recognition. Performance has further\nbeen improved by leveraging unlabeled data, of-\nten in the form of a language model. In this work,\nwe present the Cold Fusion method, which lever-\nages a pre-trained language model during train-\ning, and show its effectiveness on the speech\nrecognition task. We show that Seq2Seq models\nwith Cold Fusion are able to better utilize lan-\nguage information enjoying i) faster convergence\nand better generalization, and ii) almost complete\ntransfer to a new domain while using less than\n10% of the labeled training data.\n1. Introduction\nSequence-to-sequence (Seq2Seq) (Bahdanau et al., 2015)\nmodels have achieved state-of-the-art results on many nat-\nural language processing problems including automatic\nspeech recognition (Bahdanau et al., 2016; Chan et al.,\n2015), neural machine translation (Wu et al., 2016), con-\nversational modeling (Vinyals & Le, 2015) and many more.\nThese models learn to generate a variable-length sequence\nof tokens (e.g. texts) from a variable-length sequence of\ninput data (e.g. speech or the same texts in another lan-\nguage). With a sufﬁciently large labeled dataset, vanilla\nSeq2Seq can model sequential mapping well, but it is often\naugmented with a language model to further improve the\nﬂuency of the generated text.\nBecause language models can be trained from abundantly\navailable unsupervised text corpora which can have as\nmany as one billion tokens (Jozefowicz et al., 2016;\nShazeer et al., 2017), leveraging the rich linguistic in-\nformation of the label domain can considerably improve\nSeq2Seq’s performance. A standard way to integrate lan-\nguage models is to linearly combine the score of the task-\n1Baidu Research, Sunnyvale, CA, USA.\nCorrespondence to: Anuroop Sriram <srirama-\nnuroop@baidu.com>.\nspeciﬁc Seq2Seq model with that of an auxiliary langauge\nmodel to guide beam search (Chorowski & Jaitly, 2016;\nSutskever et al., 2014a). Gulcehre et al. (2015) proposed\nan improved algorithm called Deep Fusion that learns to\nfuse the hidden states of the Seq2Seq decoder and a neu-\nral language model with a gating mechanism, after the two\nmodels are trained independently.\nWhile this approach has been shown to improve perfor-\nmance over the baseline, it has a few limitations. First, be-\ncause the Seq2Seq model is trained to output complete la-\nbel sequences without a language model, its decoder learns\nan implicit language model from the training labels, taking\nup a signiﬁcant portion of the decoder capacity to learn re-\ndundant information. Second, the residual language model\nbaked into the Seq2Seq decoder is biased towards the train-\ning labels of the parallel corpus. For example, if a Seq2Seq\nmodel fully trained on legal documents is later fused with\na medical language model, the decoder still has an inherent\ntendency to follow the linguistic structure found in legal\ntext. Thus, in order to adapt to novel domains, Deep Fu-\nsion must ﬁrst learn to discount the implicit knowledge of\nthe language.\nIn this work, we introduce Cold Fusionto overcome both\nthese limitations. Cold Fusion encourages the Seq2Seq de-\ncoder to learn to use the external language model during\ntraining. This means that Seq2Seq can naturally leverage\npotentially limitless unsupervised text data, making it par-\nticularly proﬁcient at adapting to a new domain. The lat-\nter is especially important in practice as the domain from\nwhich the model is trained can be different from the real\nworld use case for which it is deployed. In our experi-\nments, Cold Fusion can almost completely transfer to a new\ndomain for the speech recognition task with 10 times less\ndata. Additionally, the decoder only needs to learn task rel-\nevant information, and thus trains faster.\nThe paper is organized as follows: Section 2 outlines\nthe background and related work. Section 3 presents the\nCold Fusion method. Section 4 details experiments on\nthe speech recognition task that demonstrate Cold Fusion’s\ngeneralization and domain adaptation capabilities.\narXiv:1708.06426v1  [cs.CL]  21 Aug 2017\nCold Fusion: Training Seq2Seq Models Together with Language Models\n2. Background and Related work\n2.1. Sequence-to-Sequence Models\nA basic Seq2Seq model comprises an encoder that maps an\ninput sequence x = (x1,...,x T ) into an intermediate rep-\nresentation h, and a decoder that in turn generates an out-\nput sequence y = (y1,...,y K) from h (Sutskever et al.,\n2014b). The decoder can also attend to a certain part of\nthe encoder states with an attention mechanism. The atten-\ntion mechanism is called hybrid attention (Chorowski et al.,\n2015b), if it uses both the content and the previous context\nto compute the next context. It is soft if it computes the\nexpectation over the encoder states (Bahdanau et al., 2015)\nas opposed to selecting a slice out of the encoder states.\nFor the automatic speech recognition (ASR) task, the\nSeq2Seq model is called anacoustic model(AM) and maps\na sequence of spectrogram features extracted from a speech\nsignal to characters.\n2.2. Inference and Language Model Integration\nDuring inference, we aim to compute the most likely se-\nquence ˆ y:\nˆ y= argmax\ny\nlog p(y|x). (1)\nHere, p(y|x) is the probability that the task-speciﬁc\nSeq2Seq model assigns to sequencey given input sequence\nx. The argmax operation is intractable in practice so we\nuse a left-to-right beam search algorithm similar to the one\npresented in (Sutskever et al., 2014a). We maintain a beam\nof K partial hypothesis starting with the start symbol ⟨s⟩.\nAt each time-step, the beam is extended by one additional\ncharacter and only the top K hypotheses are kept. De-\ncoding continues until the stop symbol ⟨/s⟩is emitted, at\nwhich point the hypothesis is added to the set of completed\nhypotheses.\nA standard way to integrate the language model with the\nSeq2Seq decoder is to change the inference task to:\nˆ y= argmax\ny\nlog p(y|x) +λlog pLM(y), (2)\nwhere pLM(y) is the language model probability assigned\nto the label sequence y. (Chorowski & Jaitly, 2016; Wu\net al., 2016) describe several heuristics that can be used\nto improve this basic algorithm. We refer to all of these\nmethods collectively as Shallow Fusion, since pLM is only\nused during inference.\n(Gulcehre et al., 2015) proposed Deep Fusionfor machine\ntranslation that tightens the connection between the de-\ncoder and the language model by combining their states\nwith a parametric gating:\ngt = σ(v⊤sLM\nt + b) (3a)\nsDF\nt = [st; gtsLM\nt ] (3b)\nyt = softmax(DNN(sDF\nt )), (3c)\nwhere st, sLM\nt and sDF\nt are the states of the task speciﬁc\nSeq2Seq model, language model and the overall deep fu-\nsion model. In (3c), DNN can be a deep neural network\nwith any number of layers. [a; b] is the concatenation of\nvectors aand b.\nIn Deep Fusion, the Seq2Seq model and the language\nmodel are ﬁrst trained independently and later combined\nas in Equation (3). The parameters v and b are trained\non a small amount of data keeping the rest of the model\nﬁxed, and allow the gate to decide how important each of\nthe models are for the current time step.\nThe biggest disadvantage with Deep Fusion is that the task-\nspeciﬁc model is trained independently from the language\nmodel. This means that the Seq2Seq decoder needs to learn\na language model from the training data labels, which can\nbe rather parsimonious compared to the large text corpora\navailable for language model training. So, the fused out-\nput layer of (3) should learn to overcome this bias in or-\nder to incorporate the new language information. This also\nmeans that a considerable portion of the decoder capacity\nis wasted.\n2.3. Semi-supervised Learning in Seq2Seq Models\nA few methods have been proposed for leveraging unla-\nbeled text corpora in the target domain, for both better gen-\neralization and domain transfer.\nSennrich et al. (2015) proposed backtranslation as a way\nof using unlabeled data for machine translation. Backtrans-\nlation improves the BLEU score by increasing the parallel\ntraining corpus of the neural machine translation model by\nautomatically translating the unlabeled target domain text.\nHowever, this technique does not apply well to other tasks\nwhere backtranslation is infeasible or of very low quality\n(like image captioning or speech recogntion).\nRamachandran et al. (2016) proposed warm starting the\nSeq2Seq model from language models trained on source\nand target domains separately. Unsupervised pre-training\nshows improvements in the BLEU scores. (Ramachandran\net al., 2016) also show that this improvement is from im-\nproved generalization, and not only better optimization.\nWhile this is a promising approach, the method is poten-\ntially difﬁcult to leverage for the transfer task since training\non the parallel corpus could end up effectively erasing the\nknowledge of the language models. Both back-translation\nand unsupervised pre-training are simple methods that re-\nquire no change in the architecture.\nCold Fusion: Training Seq2Seq Models Together with Language Models\nModel Prediction\nGround Truth where’s the sport in that greer snorts and leaps greer hits the dirt hard and rolls\nPlain Seq2Seq where is the sport and that through snorks and leaps clear its the dirt card and rules\nDeep Fusion where is the sport and that there is north some beliefs through its the dirt card and rules\nCold Fusion where’s the sport in that greer snorts and leaps greer hits the dirt hard and rolls\nCold Fusion (Fine-tuned) where’s the sport in that greer snorts and leaps greer hits the dirt hard and rolls\nGround Truth jack sniffs the air and speaks in a low voice\nPlain Seq2Seq jacksonice the air and speech in a logos\nDeep Fusion jacksonice the air and speech in a logos\nCold Fusion jack sniffs the air and speaks in a low voice\nCold Fusion (Fine-tuned) jack sniffs the air and speaks in a low voice\nGround Truth skipper leads her to the dance ﬂoor he hesitates looking deeply into her eyes\nPlain Seq2Seq skip er leadure to the dance ﬂoor he is it takes looking deeply into her eyes\nDeep Fusion skip er leadure to the dance ﬂoor he has it takes looking deeply into her eyes\nCold Fusion skipper leads you to the dance ﬂoor he has a tates looking deeply into her eyes\nCold Fusion (Fine-tuned) skipper leads her to the dance ﬂoor he hesitates looking deeply into her eyes\nTable 1.Some examples of predictions by the Deep Fusion and Cold Fusion models.\n3. Cold Fusion\nOur proposed Cold Fusion method is largely motivated\nfrom the Deep Fusion idea but with some important dif-\nferences. The biggest difference is that in Cold Fusion, the\nSeq2Seq model is trained from scratch together with a ﬁxed\npre-trained language model.\nBecause the Seq2Seq model is aware of the language model\nthroughout training, it learns to use the language model for\nlanguage speciﬁc information and capture only the relevant\ninformation conducive to mapping from the source to the\ntarget sequence. This disentanglement can increase the ef-\nfective capacity of the model signiﬁcantly. This effect is\ndemonstrated empirically in Section 4 where Cold Fusion\nmodels perform well even with a very small decoder.\nWe also improve on some of the modeling choices of the\nfusion mechanism.\n1. First, both the Seq2Seq hidden state st and the lan-\nguage model hidden statesLM\nt can be used as inputs to\nthe gate computation. The task-speciﬁc model’s em-\nbedding contains information about the encoder states\nwhich allows the fused layer to decide its reliance on\nthe language model in case of input uncertainty. For\nexample, when the input speech is noisy or a token\nunseen by the Seq2Seq model is presented, the fu-\nsion mechanism learns to pay more attention to the\nlanguage model.\n2. Second, we employ ﬁne-grained (FG) gating mecha-\nnism as introduced in (Yang et al., 2016). By using a\ndifferent gate value for each hidden node of the lan-\nguage model’s state, we allow for greater ﬂexibility\nin integrating the language model because the fusion\nalgorithm can choose which aspects of the language\nmodel it needs to emphasize more at each time step.\n3. Third, we replace the language model’s hidden state\nwith the language model probability. The distribution\nand dynamics of sLM\nt can vary considerably across\ndifferent language models and data. As a concrete ex-\nample, any fusion mechanism that uses the LM state is\nnot invariant to the permutation of state hidden nodes.\nThis limits the ability to generalize to new LMs. By\nprojecting the token distribution onto a common em-\nbedding space, LMs that model novel uses of the lan-\nguage can still be integrated without state discrepancy\nissues. This also means that we can train with or swap\non n-gram LMs during inference.\nThe Cold Fusionlayer works as follows:\nhLM\nt = DNN(ℓLM\nt ) (4a)\ngt = σ(W[st; hLM\nt ] +b) (4b)\nsCF\nt = [st; gt ◦hLM\nt ] (4c)\nrCF\nt = DNN(sCF\nt ) (4d)\nˆP(yt|x,y<t) = softmax(rCF\nt ) (4e)\nℓLM\nt is the logit output of the language model,st is the state\nof the task speciﬁc model, and sCF\nt is the ﬁnal fused state\nused to generate the output. Since logits can have arbitrary\noffsets, the maximum value is subtracted off before feeding\ninto the layer. In (4a), (4d), the DNN can be a deep neural\nnetwork with any number of layers. In our experiments,\nwe found a single afﬁne layer, with ReLU activation prior\nto softmax, to be helpful.\nCold Fusion: Training Seq2Seq Models Together with Language Models\nFigure 1.Cross-entropy loss on the dev set for the baseline model\n(orange) and the proposed model (purple) as a function of training\niteration. Training with a language model speeds up convergence\nconsiderably.\n4. Experiments\n4.1. Setup\nFor our experiments, we tested the Cold Fusion method on\nthe speech recognition task. The results are compared using\nthe character error rate (CER) and word error rate (WER)\non the evaluation sets. For all models which were trained\non the source domain, the source CER and WER indicate\nin-domain performance and the target CER and WER indi-\ncate out-of-domain performance.\nWe collected two data sets: one based on search queries\nwhich served as our source domain, and another based on\nmovie transcripts which served as our target domain. For\neach dataset, we used Amazon Mechanical Turk to col-\nlect audio recordings of speakers reading out the text. We\ngave identical instructions to all the turkers in order to en-\nsure that the two datasets only differed in the text domain.\nThe source dataset contains 411,000 utterances (about 650\nhours of audio), and the target dataset contains 345,000 ut-\nterances (about 676 hours of audio). We held out 2048 ut-\nterances from each domain for evaluation.\nThe text of the two datasets differ signiﬁcantly. Table 2\nshows results of training character-based recurrent neural\nnetwork language models (Mikolov, 2012) on each of the\ndatasets and evaluating on both datasets. Language mod-\nels very easily overﬁt the training distribution, so models\ntrained on one corpus will perform poorly on a different\ndistribution. We see this effect in Table 2 that models opti-\nmized for the source domain have worse perplexity on the\ntarget distribution.\n4.2. Neural Network Architectures\nThe language model described in the ﬁnal row of Table 2\nwas trained on about 25 million words. This model con-\nTable 2.Dev set perplexities for character RNN language models\ntrained on different datasets on source and target domain. Note\nthat i) the model trained on source domain does poorly on target\ndomain and vice-versa indicating that the two domains are very\ndifferent, and ii) the best model on both domains is a larger model\ntrained on a superset of both corpuses. We use the model trained\non the full dataset (which contains the source and target datasets\nalong with some additional text) for all of the LM integration ex-\nperiments.\nModel Domain Word Perplexity\nCount Source Target\nGRU (3 ×512) Source 5.73M 2.670 4.463\nGRU (3 ×512) Target 5.46M 3.717 2.794\nGRU (3 ×1024) Full 25.16M 2.491 2.325\ntains three layers of gated recurrent units (GRU) (Chung\net al., 2014) with a hidden state dimension of 1024. The\nmodel was trained to minimize the cross-entropy of pre-\ndicting the next character given the previous characters.\nWe used the Adam optimizer (Kingma & Ba, 2014) with\na batch size of 512. The model gets a perplexity of 2.49 on\nthe source data and 2.325 on the target data.\nFor the acoustic models, we used the Seq2Seq architec-\nture with soft attention based on (Bahdanau et al., 2016).\nThe encoder consists of 6 bidirectional LSTM (BLSTM)\n(Hochreiter & Schmidhuber, 1997) layers each with a di-\nmension of 480. We also use max pooling layers with\na stride of 2 along the time dimension after the ﬁrst two\nBLSTM layers, and add residual connections (He et al.,\n2015) for each of the BLSTM layers to help speed up the\ntraining process. The decoder consisted of a single layer of\n960 dimensional Gated Recurrent Unit (GRU) with a hy-\nbrid attention (Chorowski et al., 2015b).\nThe ﬁnal Cold Fusion mechanism had one dense layer of\n256 units followed by ReLU before softmax.\n4.3. Training\nThe input sequence consisted of 40 mel-scale ﬁlter bank\nfeatures. We expanded the datasets with noise augmenta-\ntion; a random background noise is added with a 40% prob-\nability at a uniform random SNR between 0 and 15 dB. We\ndid not use any other form of regularization.\nWe trained the entire system end-to-end with Adam\n(Kingma & Ba, 2014) with a batch size of 64. The learning\nrates were tuned separately for each model using random\nsearch. To stabilize training early on, the training examples\nwere sorted by increasing input sequence length in the ﬁrst\nepoch (Amodei et al., 2015). During inference, we used\nbeam search with a ﬁxed beam size of 128 for all of our\nexperiments.\nCold Fusion: Training Seq2Seq Models Together with Language Models\nTable 3.Speech recognition results for the various models discussed in the paper.\nModel Train Domain Test on Source Test on Target\nCER WER CER WER Domain Gap\nBaseline Attention Model Source 7.54% 14.68% 23.02% 43.52% 100%\nBaseline Attention Model Target 8.84% 17.61% 0%\nBaseline + Deep Fusion Source 7.64% 13.92% 22.14% 37.45% 76.57%\n+ sAM in gate Source 7.61% 13.92% 21.07% 37.9% 78.31%\n+ Fine-Grained Gating Source 7.47% 13.61% 20.29% 36.69% 73.64%\n+ ReLU layer Source 7.50% 13.54% 21.18% 38.00% 78.70%\nBaseline + Cold Fusion\n+ sAM in gate Source 7.25% 13.88% 15.63% 30.71% 50.56%\n+ Fine-Grained Gating Source 6.14% 12.08% 14.79% 30.00% 47.82%\n+ ReLU layer Source 5.82% 11.52% 14.89% 30.15% 48.40%\n+ Probability Projection Source 5.94% 11.87% 13.72% 27.50% 38.17%\nWe also used scheduled sampling (Bengio et al., 2015) with\na sampling rate of 0.2 which was kept ﬁxed throughout\ntraining. Scheduled sampling helped reduce the effect of\nexposure bias due to the difference in the training and in-\nference mechanisms.\n4.4. Improved Generalization\nLeveraging a language model that has a better perplexity\non the distribution of interest should directly mean an im-\nproved WER for the ASR task. In this section, we compare\nhow the different fusion methods fare in achieving this ef-\nfect.\nSwapping the language model is not possible with Deep\nFusion because of the state discrepancy issue motivated in\nSection 3. All fusion models were therefore trained and\nevaluated with the same language model that achieved a\nTable 4.Effect of decoder dimension on the model’s performance.\nThe performance of cold fusion models degrades more slowly as\nthe decoder size decreases. This corroborates the fact that the\ndecoder only needs to learn the task not label generation. Its ef-\nfective task capacity is much larger than without fusion.\nModel Decoder size Source\nCER WER\nAttention 64 16.33% 33.98%\n128 11.14% 24.35%\n256 8.89% 18.74%\n960 7.54% 14.68%\nCold Fusion 64 9.47% 17.42%\n128 7.96% 15.15%\n256 6.71% 13.19%\n960 5.82% 11.52%\nlow perplexity on both the source and target domains (See\nTable 2). This way, we can measure improvements in trans-\nfer capability over Deep Fusion due to the training and ar-\nchitectural changes.\nTable 3 compares the performance of Deep Fusion and\nCold Fusion on the source and target held-out sets. Clearly,\nCold Fusion consistently outperforms on both metrics on\nboth domains than the baselines. For the task of predicting\nin-domain, the baseline model gets a word error of 14.68%,\nwhile our best model gets a relative improvement of more\nthan 21% over that number. Even compared to the recently\nproposed Deep Fusion model (Gulcehre et al., 2015), the\nbest Cold Fusion model gets a relative improvement of\n15%.\nWe get even bigger improvements in out-of-domain results.\nThe baseline attention model, when trained on the source\ndomain but evaluated on the target domain gets, 43.5%\nWER. This is signiﬁcantly worse than the 17.6% that we\ncan get by training the same model on the target dataset.\nThe goal of domain adaptation is to bridge the gap between\nthese numbers. The ﬁnal column in Table 3 shows the re-\nmaining gap as a fraction of the difference for each model.\nThe Deep Fusion models can only narrow the domain gap\nto 76.57% while Cold Fusion methods can reduce it to\n38.17%. The same table also shows the incremental effects\nof the three architectural changes we have made to the Cold\nFusion method. Note that applying the same changes to the\nDeep Fusion method does not yield much improvements,\nindicating the need for cold starting Seq2Seq training with\nlanguage models. The use of probability projection instead\nof the language model state in the fusion layer substantially\nhelps with generalization. Intuitively, the character proba-\nbility space shares the same structure across different lan-\nguage models unlike the hidden state space.\nCold Fusion: Training Seq2Seq Models Together with Language Models\nTable 5.Results for ﬁne tuning the acoustic model (ﬁnal row from\nTable 3) on subsets of the target training data. ∗The ﬁnal row\nrepresents an attention model that was trained on all of the target\ndomain data.\nModel Target Target\nData CER WER Domain Gap\nCold Fusion 0% 13.72% 27.50% 38.17%\nCold Fusion 0.6% 11.98% 23.13% 21.30%\n+ ﬁnetuning 1.2% 11.62% 22.40% 18.49%\n2.4% 10.79% 21.05% 13.28%\n4.8% 10.46% 20.46% 11.00%\n9.5% 10.11% 19.68% 7.99%\nAttention∗ 100% 8.84% 17.61% 0.00 %\n4.5. Decoder Efﬁciency\nWe test whether cold fusion does indeed relieve the decoder\nof learning a language model. We do so by checking how\na decrease in the decoder capacity affected the error rates.\nAs evidenced in Table 4, the performance of the Cold Fu-\nsion models degrades gradually as the decoder cell size is\ndecreased whereas the performance of the attention mod-\nels deteriorates abruptly beyond a point. It is remarkable\nthat the Cold Fusion decoder still outperforms the full at-\ntentional decoder with 4×fewer number of parameters.\nAlso, we ﬁnd that training is accelerated by a factor of 3\n(see Figure 1). Attention models typically need hundreds\nof thousands of iterations to converge (Chorowski et al.,\n2015a). Most of the training time is spent in learning the at-\ntention mechanism. One can observe this behavior by plot-\nting the attention context over time and seeing that the diag-\nonal alignment pattern emerges in later iterations. Because\nthe pretrained, ﬁxed language model infuses the model with\nlower level language features like the likely spelling of a\nword, error signals propagate more directly into the atten-\ntion context.\n4.6. Fine-tuning for Domain Adaptation\nIn the presence of limited data from the target distribution,\nﬁne tuning a model for domain transfer is often a promising\napproach. We test how much labeled data from the target\ndistribution is required for Cold Fusion models to effec-\ntively close the domain adaptation gap.\nThe same language model from Section 4.4 trained on both\nthe source and target domains was used for all ﬁne-tuning\nexperiments. The learning rate was restored to its initial\nvalue. Then, we ﬁne-tuned only the fusion mechanism\nof the best Cold Fusion model from Table 3 on various\namounts of the labeled target dataset.\nResults are presented in Table 5. With just 0.6% of labeled\ndata, the domain gap decreases from 38.2% to 21.3%. With\nless than 10% of the data, this gap is down to only 8%. Note\nthat because we keep the Seq2Seq parameters ﬁxed during\nthe ﬁne-tuning stage, all of the improvements from ﬁne-\ntuning come from combining the acoustic and the language\nmodel better. It’s possible that we can see bigger gains by\nﬁne-tuning all the parameters. We do not do this in our\nexperiments because we are only interested in studying the\neffects of language model fusion in the Seq2Seq decoder.\nSome examples are presented in Table 1. Recall that all\nmodels are trained on the source domain consisting of the\nread speech of search queries and evaluated on the read\nspeech of movie scripts to measure out-of-domain perfor-\nmance. Because search queries tend to be sentence frag-\nments, we see that the main mode of error for vanilla atten-\ntion and Deep Fusion is due to weak grammar knowledge.\nCold Fusion on the other hand demonstrates a better grasp\nof grammar and is able to complete sentences.\n5. Conclusion\nIn this work, we presented a new general Seq2Seq model\narchitecture where the decoder is trained together with a\npre-trained language model. We study and identify archi-\ntectural changes that are vital for the model to fully lever-\nage information from the language model, and use this to\ngeneralize better; by leveraging the RNN language model,\nCold Fusion reduces word error rates by up to 18% com-\npared to Deep Fusion. Additionally, we show that Cold\nFusion models can transfer more easily to new domains,\nand with only 10% of labeled data nearly fully transfer to\nthe new domain.\nReferences\nAmodei, Dario, Anubhai, Rishita, Battenberg, Eric, Case,\nCarl, Casper, Jared, Catanzaro, Bryan, Chen, Jingdong,\nChrzanowski, Mike, Coates, Adam, Diamos, Greg, et al.\nDeep speech 2: End-to-end speech recognition in english\nand mandarin. arXiv preprint arXiv:1512.02595, 2015.\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio,\nYoshua. Neural machine translation by jointly learning\nto align and translate. In ICLR, 2015.\nBahdanau, Dzmitry, Chorowski, Jan, Serdyuk, Dmitriy,\nBrakel, Philemon, and Bengio, Yoshua. End-to-end\nattention-based large vocabulary speech recognition. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2016 IEEE International Conference on, pp. 4945–4949.\nIEEE, 2016.\nBengio, Samy, Vinyals, Oriol, Jaitly, Navdeep, and\nShazeer, Noam. Scheduled sampling for sequence pre-\nCold Fusion: Training Seq2Seq Models Together with Language Models\ndiction with recurrent neural networks. In Advances in\nNeural Information Processing Systems, pp. 1171–1179,\n2015.\nChan, William, Jaitly, Navdeep, Le, Quoc V , and Vinyals,\nOriol. Listen, attend and spell. arXiv preprint\narXiv:1508.01211, 2015.\nChorowski, Jan and Jaitly, Navdeep. Towards better decod-\ning and language model integration in sequence to se-\nquence models. arXiv preprint arXiv:1612.02695, 2016.\nChorowski, Jan, Bahdanau, Dzmitry, Serdyuk, Dmitry,\nCho, Kyunghyun, and Bengio, Yoshua. Attention-based\nmodels for speech recognition. abs/1506.07503, 2015a.\nhttp://arxiv.org/abs/1506.07503.\nChorowski, Jan K, Bahdanau, Dzmitry, Serdyuk, Dmitriy,\nCho, Kyunghyun, and Bengio, Yoshua. Attention-based\nmodels for speech recognition. In Advances in Neural\nInformation Processing Systems, pp. 577–585, 2015b.\nChung, Junyoung, Gulcehre, Caglar, Cho, KyungHyun,\nand Bengio, Yoshua. Empirical evaluation of gated re-\ncurrent neural networks on sequence modeling. arXiv\npreprint arXiv:1412.3555, 2014.\nGulcehre, Caglar, Firat, Orhan, Xu, Kelvin, Cho,\nKyunghyun, Barrault, Loic, Lin, Huei-Chi, Bougares,\nFethi, Schwenk, Holger, and Bengio, Yoshua. On us-\ning monolingual corpora in neural machine translation.\narXiv preprint arXiv:1503.03535, 2015.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Deep residual learning for image recognition.\nCoRR, abs/1512.03385, 2015. URL http://arxiv.\norg/abs/1512.03385.\nHochreiter, Sepp and Schmidhuber, J ¨urgen. Long short-\nterm memory. Neural Computation, 9(8):1735—1780,\n1997.\nJozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer,\nNoam, and Wu, Yonghui. Exploring the limits of\nlanguage modeling. arXiv preprint arXiv:1602.02410,\n2016.\nKingma, Diederik and Ba, Jimmy. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nMikolov, T. Statistical Language Models Based on Neural\nNetworks. PhD thesis, Brno University of Technology,\n2012.\nRamachandran, Prajit, Liu, Peter J, and Le, Quoc V . Unsu-\npervised pretraining for sequence to sequence learning.\narXiv preprint arXiv:1611.02683, 2016.\nSennrich, Rico, Haddow, Barry, and Birch, Alexandra. Im-\nproving neural machine translation models with mono-\nlingual data. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics, pp.\n86–96, 2015.\nShazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysztof,\nDavis, Andy, Le, Quoc, Hinton, Geoffrey, and Dean,\nJeff. Outrageously large neural networks: The\nsparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V . Se-\nquence to sequence learning with neural networks.\nIn Proceedings of the 27th International Conference\non Neural Information Processing Systems, NIPS’14,\npp. 3104–3112, Cambridge, MA, USA, 2014a. MIT\nPress. URL http://dl.acm.org/citation.\ncfm?id=2969033.2969173.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V . Sequence\nto sequence learning with neural networks. 2014b.\nhttp://arxiv.org/abs/1409.3215.\nVinyals, Oriol and Le, Quoc V . A neural conversational\nmodel. CoRR, abs/1506.05869, 2015. URL http://\narxiv.org/abs/1506.05869.\nWu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc V ,\nNorouzi, Mohammad, Macherey, Wolfgang, Krikun,\nMaxim, Cao, Yuan, Gao, Qin, Macherey, Klaus, et al.\nGoogle’s neural machine translation system: Bridging\nthe gap between human and machine translation. arXiv\npreprint arXiv:1609.08144, 2016.\nYang, Zhilin, Dhingra, Bhuwan, Yuan, Ye, Hu, Junjie, Co-\nhen, William W, and Salakhutdinov, Ruslan. Words or\ncharacters? ﬁne-grained gating for reading comprehen-\nsion. arXiv preprint arXiv:1611.01724, 2016.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8243209719657898
    },
    {
      "name": "Closed captioning",
      "score": 0.8164515495300293
    },
    {
      "name": "Task (project management)",
      "score": 0.6883041262626648
    },
    {
      "name": "Machine translation",
      "score": 0.6827793121337891
    },
    {
      "name": "Language model",
      "score": 0.6066320538520813
    },
    {
      "name": "Generalization",
      "score": 0.6007861495018005
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5997653603553772
    },
    {
      "name": "Natural language processing",
      "score": 0.5976083278656006
    },
    {
      "name": "Training set",
      "score": 0.5522949695587158
    },
    {
      "name": "Speech recognition",
      "score": 0.5273153185844421
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4925854802131653
    },
    {
      "name": "Natural language",
      "score": 0.4861149787902832
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4546712636947632
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4222137928009033
    },
    {
      "name": "Convergence (economics)",
      "score": 0.42200303077697754
    },
    {
      "name": "Fusion",
      "score": 0.4200981557369232
    },
    {
      "name": "Image (mathematics)",
      "score": 0.21032676100730896
    },
    {
      "name": "Linguistics",
      "score": 0.12962743639945984
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I11983389",
      "name": "Manchester Metropolitan University",
      "country": "GB"
    }
  ]
}