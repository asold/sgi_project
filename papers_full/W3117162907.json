{
    "title": "Language Model Transformers as Evaluators for Open-domain Dialogues",
    "url": "https://openalex.org/W3117162907",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2907041504",
            "name": "Rostislav Nedelchev",
            "affiliations": [
                "University of Bonn"
            ]
        },
        {
            "id": "https://openalex.org/A2101122831",
            "name": "Jens Lehmann",
            "affiliations": [
                "University of Bonn",
                "Fraunhofer Institute for Intelligent Analysis and Information Systems"
            ]
        },
        {
            "id": "https://openalex.org/A1905735547",
            "name": "Ricardo Usbeck",
            "affiliations": [
                "University of Bonn",
                "Fraunhofer Institute for Intelligent Analysis and Information Systems"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2140054881",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2908854766",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2892228078",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2997283613",
        "https://openalex.org/W2894060751",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2962883855",
        "https://openalex.org/W3088059392",
        "https://openalex.org/W2531327146",
        "https://openalex.org/W2913129712",
        "https://openalex.org/W1990005915",
        "https://openalex.org/W1518951372",
        "https://openalex.org/W2963903950",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2893684749",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W4300427683",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W3121541553",
        "https://openalex.org/W2419539795",
        "https://openalex.org/W2163929346",
        "https://openalex.org/W2953039584",
        "https://openalex.org/W2909777606",
        "https://openalex.org/W2584220694",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W10957333",
        "https://openalex.org/W2913443447",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2996403597",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2963527228",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2963748792",
        "https://openalex.org/W2979702391",
        "https://openalex.org/W2964178377",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W2136939460",
        "https://openalex.org/W2885721522",
        "https://openalex.org/W2991223644",
        "https://openalex.org/W3032834023",
        "https://openalex.org/W2904443424"
    ],
    "abstract": "Computer-based systems for communication with humans are a cornerstone of AI research since the 1950s. So far, the most effective way to assess the quality of the dialogues produced by these systems is to use resource-intensive manual labor instead of automated means. In this work, we investigate whether language models (LM) based on transformer neural networks can indicate the quality of a conversation. In a general sense, language models are methods that learn to predict one or more words based on an already given context. Due to their unsupervised nature, they are candidates for efficient, automatic indication of dialogue quality. We demonstrate that human evaluators have a positive correlation between the output of the language models and scores. We also provide some insights into their behavior and inner-working in a conversational context.",
    "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6797–6808\nBarcelona, Spain (Online), December 8-13, 2020\n6797\nLanguage Model Transformers as\nEvaluators for Open-domain Dialogues\nRostislav Nedelchev1 Jens Lehmann1,2 Ricardo Usbeck1,2\n1Smart Data Analytics Group, University of Bonn, Germany\n2Fraunhofer IAIS, Sankt Augustin and Dresden, Germany\nrostislav.nedelchev@uni-bonn.de, jens.lehmann@cs.uni-bonn.de\nricardo.usbeck@iais.fraunhofer.de\nAbstract\nComputer-based systems for communication with humans are a cornerstone of AI research since\nthe 1950s. So far, the most effective way to assess the quality of the dialogues produced by these\nsystems is to use resource-intensive manual labor instead of automated means. In this work, we\ninvestigate whether language models (LM) based on transformer neural networks can indicate the\nquality of a conversation. In a general sense, language models are methods that learn to predict\none or more words based on an already given context. Due to their unsupervised nature, they are\ncandidates for efﬁcient, automatic indication of dialogue quality. We demonstrate that human\nevaluators have a positive correlation between the output of the language models and scores. We\nalso provide some insights into their behavior and inner-working in a conversational context.\n1 Introduction\nLately, deep learning conversational systems have seen increasing interest from industry and academia\nalike (Chen et al., 2017). These systems ﬁnd usage in various contexts, starting from personal speech\nassistants like Google Assistant through the ”chatbots” on instant messaging platforms like Facebook\nMessenger, and ﬁnally, conversational services like LUIS1. Many of these applications serve the objective\nof completing a speciﬁc function like purchasing a product or booking services (e.g., hotels, ﬂights).\nNonetheless, these applications can still proﬁt from open-domain dialogue skills like chit-chatting, which\nwould provide a more human-like interaction with users.\nPresently, scientists and engineers working on computer-based conversational systems need human-\nbased evaluation to assess the quality and usability of their work (Dinan et al., 2019; Logacheva et al.,\n2018; Yoshino et al., 2019). These evaluations are costly in terms of resources. Thus, the ﬁeld of dialogue\nsystems could take advantage of an automated method for assessing conversations.\nSeminal works in text summarization and machine translation have already proposed their ﬁeld-\nspeciﬁc metrics for automated assessments - for the former ROUGE (Lin, 2004), and, for the latter,\nBLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). Dialogue system research (Rit-\nter et al., 2011; Serban et al., 2016; Yoshino et al., 2019) constantly uses these metrics. However, Liu\net al. show that these metrics based on word-overlap between prediction and references are not reliable\nfor evaluating the usefulness of dialogue systems (2016). Hence, the ﬁeld should use more sophisticated\nmethods that consider the previous utterances of a conversation and its semantic meaning.\nWhen human annotators evaluate a dialogue, they do not use an explicit reference or necessarily seek\nword overlap between context and response (or the lack of it). Their assessment bases itself on experience\nwith the language and the implicit knowledge they have about it. The core principle of statistical language\nmodels (LM) is to capture and reproduce these properties. LM have proven themselves invaluable in\nstate-of-the-art approaches in natural language processing, and natural language understanding (Peters\net al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019).\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details:\nhttp://creativecommons.org/licenses/by/4.0/.\n1https://www.luis.ai/\n6798\nThus, the main aim of this work 2 is to investigate their usability as means for evaluating dialogues\nsince they do not need a reference or supervision. We demonstrate that there is a signiﬁcant positive\ncorrelation between the predictions of language models and human evaluation scores. Furthermore, we\nprovide insights into the inner-workings and behavior of language models in the dialogue context.\n2 Related Work\nIn this section, we present earlier work that focuses on dialogue evaluation. Furthermore, we provide\na concise introduction to language model transformers and recent advances in this particular set of ap-\nproaches.\n2.1 Dialogue Evaluation\nLowe et al. present a cornerstone work in dialogue evaluation. (2017). They propose an automatic\ndialogue evaluation model (ADEM) that employs a neural network approach that approximates human\njudgment using scored dialogues together with the context, reference response, and one generated by\na dialogue system. Reference responses and human annotation scores are hard to obtain. That is, it is\nchallenging to employ the approach on large dialogue datasets. Another cornerstone is the work of Tao\net al. (2018), a Referenced metric, and Unreferenced metric Blended Evaluation Routine (RUBER). They\nsuggest a method consisting of two elements: The ﬁrst one captures the resemblance between a generated\nand reference response using word vector pooling. The second one uses a neural network to estimate the\nrelevance of a reply. The model is trained to distinguish whether an answer in a dialogue is the original\none or a random one from another conversation. A drawback of both approaches above is that they use\nreference responses to derive a score. Furthermore, Sai et al. (Sai et al., 2019) demonstrate that machine\nlearning approaches for dialogue evaluation like ADEM are susceptible to adversarial attacks.\nOther works focus on addressing the issue that there is more than one possible response for a given\ndialogue context by considering multiple reference responses. For example, Galley et al. (2015) suggest\nan augmented version of BLEU that uses synthetically generated responses. The algorithm in Sordoni\net al. (2015) operates similarly. Sugiyama et al (2019) develop a Support Vector Regression approach to\nconsider multiple references. Gupta et al. (2019) investigate a framework of dialogue-modeling methods\ncombined with a variety of metrics, where they evaluate dialogues using various references.\nZhang et al. (2020) propose BERTscore to calculate text similarity using contextual embeddings. Their\nwork can be used for evaluating text generation against a reference. Unfortunately, it offers no way to\nevaluate dialogues without a speciﬁed ground truth. On another note, Kann et al. (2018) suggest a\nsentence level ﬂuency metric derived from the perplexity score of a language model given a sentence\nwithout involving any references. Their results demonstrate signiﬁcant positive correlations with human\nannotators. Nedelchev et al. (2020) experiment with an anomaly detection approach where erroneous\ndialogues are seen as anomalies.\n2.2 Language Models\nThe ﬁrst application of n-gram-based language models is recorded in the mid-1970s by two independent\nworks of Jelinek (1976) and Baker (1975). Given a sequence of tokens, T = {t1,...,t N }, a forward\nlanguage model computes the probability of the sequence by modeling the probability of a token tK\n(K ≤N) which has a history up to the K-th token (Peters et al., 2018). Some of the initial neural\nnetwork models (Melis et al., 2017) use initially a context-independent vector representation for a token,\nwhich all pass through one or more LSTM layers (Hochreiter and Schmidhuber, 1997). In the end, they\nproduce a context-dependent vector that serves as input to a softmax layer to predict the next token. In a\nreversed fashion, backward LM use the context to the right of the target token to predict it. In contrast,\nbi-directional language models use a combination of both to predict the target word.\nRadford et al. (2019) propose generative pre-training (GPT2), where they use the transformer (Vaswani\net al., 2017) as a forward language model, due to its superiority in terms of long-term memory when\n2Code and resources to reproduce the results are available on the following link:\nhttps://github.com/SmartDataAnalytics/transformers_dialogue_evaluators\n6799\ncontrasted to recurrent neural networks like LSTMs.\nFurthermore, Devlin et al. (2019) suggest an innovative way to train language models, also utilizing\ntransformers, speciﬁcally Bidirectional Encoder Representations from Transformers (BERT). They in-\nvent the masked language model (MLM) where a random subset of tokens from a sequence is masked\nor replaced, which the model then predicts by using the remaining original context. Furthermore, BERT\nuses an additional LM objective: next sentence prediction (NSP). It works by teaching a model to recog-\nnize whether two sentences appear sequentially in a corpus or not.\nYet another innovative transformer-based language model is XLNet by Yang et al (2019). It com-\nbines the best features of a generative LM like GPT2 and a masked LM like BERT by proposing to\nuse the permutations of all factorization orders of a sequence to train. Thanks to it, XLNet learns to\nutilize knowledge from both sides of the target token, but also the respective context of other positions.\nGolovanov et al. (2019) demonstrate that pre-trained transformer language models provide beneﬁts for\nconversational agents.\nFor completeness, we mention other language models below that utilize transformers but are not inte-\ngral to this work. We do not employ them in this work because the architectures discussed above already\nsupersede them, or we deem their additions as not adequate for modeling dialogues.\nDai et al. (2019) propose Transformer-XL, a new approach that allows transformers to model even\nlonger sequences by caching and reusing intermediate hidden states. XLNet also utilizes the method in its\nimplementation. Cross-lingual Language Model, by Lample and Conneau (2019), introduces Translation\nLanguage Modeling, i.e., randomly masks words in parallel sequences in two languages to teach the\nmodel leveraging multi-lingual context. Liu et al. (2019) present Robustly optimized BERT, by just\ndropping BERTs next sentence prediction and a few other modiﬁcations in training. Raffel et al. (2019)\nintroduce the Text-to-Text Transfer Transformer, where the language-modeling objective is using a text-\nto-text perspective. Conditional Transformer Language model, by Keskar et al. (2019), incorporates\nconditioning on control codes to guide the generation of tokens.\nBesides capturing syntax, LM are also capable to model semantics of sentences. The results of Ten-\nney et al. (Tenney et al., 2019) suggest that contextual word embeddings can encode both syntax and\nsemantics on a sub-sentence level. Furthermore, Zhou et al. (Zhou et al., 2019) conduct a systematic\nbenchmark to evaluate seven LM for their commonsense knowledge and reasoning. Their work suggests\nthat they have a certain degree of those abilities. Commonsense is what would also help for evaluating\nopen-domain dialogues.\n3 Methodology\nIn this section, we report on the used datasets for assessing the usability of transformer language mod-\nels for evaluating dialogue quality, introduce the used approaches in greater detail and describe their\nrelevance to the task at hand.\n3.1 Datasets\nWe use the data gathered during the ConvAI1 3 (Burtsev et al., 2018; Logacheva et al., 2018) and Con-\nvAI24 (Zhang et al., 2018; Dinan et al., 2019) challenges. The organizers invited competitors to develop\ndialogue systems that had to address speciﬁc tasks. For ConvAI1, the participating systems needed to be\nable to converse about a topic. In the other competition, the chatbots had to engage in a small-talk while\nimpersonating a pre-deﬁned personality proﬁle (”persona”). In both cases, human annotators evaluated\nthe capability of the dialogue systems to converse by interacting with them and giving a score at the end.\nFor both competitions, the scoring is on dialogue level. In Table 1 and in Figure 1, we present some\nadditional details about the data. However, we do not evaluate the two challenges speciﬁcally (topic\ndiscussion and role acting). Instead, we aim at general open-domain dialogue evaluation, which implies\nrelevance, coherence, and ﬂuency of the utterances.\n3http://convai.io/2017/data/\n4http://convai.io/data/\n6800\nFigure 1: Kernal density estimation of the distri-\nbution of annotator scores of the dialogues in Con-\nvAI1 and ConvAI2. We see that the majority of\ndialogues are evaluated as low quality.\nFeature ConvAI1 ConvAI2\n# Dialogues 2154 2237\nAvg # Utterances 13.9 18.1\nAvg # Words\nper Utterance 7.3 8.2\nTask Topic\ndiscussion\nRole\nacting\nTable 1: Key features of the dialogue\ndatasets. Only dialogues with three or more\nutterances were considered as part of this\nwork. From our point of view, a dialogue\nwith two turns cannot reﬂect the semantic\ncomplexities of language.\n3.2 Language Model Evaluators\nIn Section 2.2, we presented a concise introduction into transformer-based language models. In the\ncurrent subsection, we will provide more details about three of those architectures, and how we use\nthem for conducting this study. Our main goal is to use the LM to assign a probability to the utterances\nin a conversation. We used HuggingFace’s Transformers 5 (Wolf et al., 2019) for implementation and\npre-trained weights of transformer-based language models.\nSince intuition dictates that responses are dependent on their preceding context, we condition the target\nreply on its history to measure its relevance. Kann et al. (2018) showed how language models could serve\nas good sentence-level ﬂuency indicators. Thus, the calculated probability from the transformer-based\nLM can serve as a combined score for ﬂuency and coherency. The following LM are used in this work:\n1.) As previously mentioned, BERT (Devlin et al., 2019) is using two language modeling objectives:\nmasked language modeling (MLM) and next sentence prediction. MLM provides no viable way for\ncomputing the probability of a target response because it originally substitutes only a random subset of\ntokens. Thus, there is no consistent and deterministic way to use masked language modeling for assigning\na probability score to a response given its context. However, BERT’s next sentence prediction is an\nexcellent approach for the current task. It can judge if an utterance is the next one given its contextual\npredecessor. Thus, we pair up the sequentially appearing sequences in a conversation and compute a\nprobability score for the second reply:\nP(u2|u1) =P(t21,t22,...,t 2n|t11,t12,...,t 1m) (1)\n,\nwhere P(u2|u1) is the probability score of the target response, while (t11,t12,...,t 1m) and\n(t21,t22,...,t 2n) are the tokens belonging to the query and response utterances prospectively.\n2.) The approach of GPT2 (Radford et al., 2019) is the standard language model approach that factor-\nizes the joint probability over the sequence tokens (t1,t2,...,t n) as a product of the conditional proba-\nbilities (Peters et al., 2018):\nP(x) =\ni∏\ni=1\nP(tn|t1,t2,...,t i−1) (2)\nIn our problem domain, we need to consider two consecutive sequences and capture the coherence\nbetween them. Thus, we concatenate them into one, where the context appears ﬁrst and is then followed\nby the second utterance. We then compute the joint probability for the second part conditioned on the\npast:\n5https://github.com/huggingface/transformers\n6801\nP(x) =\nm+n∏\ni=m+1\nP(tm+n|ti,tn+1,...,t m+n−1) (3)\nwhere mis the length of the context, and nis the length of the target utterance.\n3.) XLNet (Yang et al., 2019) follows the same general language model approach as GPT2, however,\nwith some additions to its training objective and neural network architecture. First of all, unlike GPT2,\nXLNet optimizes the model over a sequence w.r.t. all possible permutations of the factorization orders\nrather than each one separately. Secondly, compared to conventional neural transformers, XLNet adds\none more attention stream that includes the positional information of the target token but excluding the\ncontent to maintain the autoregressive properties. To compute probabilities for the utterances, we follow\nthe same procedure as described above for GPT2.\nIn this work, we use a set of hyper-parameter conﬁgurations for each of the three language models.\nWe present them in Table 2.\nName Details\nbert-base-uncased 12-layer, 768-hidden, 12-heads BooksCorpus English Wikipedia\nbert-large-uncased 24-layer, 1024-hidden, 16-heads BooksCorpus & English Wikipedia\ngpt2 12-layer, 768-hidden, 12-heads news, Wikipedia, ﬁction books\ngpt2-medium 24-layer, 1024-hidden, 16-heads news, Wikipedia, ﬁction books\ngpt2-large 36-layer, 1280-hidden, 20-heads news, Wikipedia, ﬁction books\nxlnet-base-cased 12-layer, 768-hidden, 12-heads same as BERT + news\nxlnet-large-cased 24-layer, 1024-hidden, 16-heads same as BERT + news\nTable 2: Hyper-parameter conﬁgurations (number of layers, size of the hidden state, number of attention\nheads) of the models and used corpora to pre-train them. Source: https://huggingface.co/\ntransformers/pretrained_models.html\n3.3 Scoring\nIn Equations 2 and 3, we showed how language models compute a probability score for a whole sequence.\nHowever, as an aggregated score over the tokens, it is losing the initial probabilistic distribution over\nthe tokens. Furthermore, since we are dealing with dialogues, i.e., a sequence of utterances, we need\nto perform two levels of aggregation. The ﬁrst level is an aggregation of the word tokens within an\nutterance, while the second is the done while aggregating over the utterances.\nThus, we investigate other possible ways to derive an aggregated score over the word tokens and over\nthe utterances within a dialogue. Besides a product of probabilities, we also look into a sum and an\nunweighted average, which capture the length of the sequences (utterance or dialogue), which might\nprove beneﬁcial for a correlation study with human annotators. We normalize all of the scores such that\nthey range between 0.0 (population minimum) and 1.0 (population maximum).\nFor GPT2 and XLNet, our experiments show that the following formulation correlates the highest with\nhuman annotator scores:\nlm dialog score=\nUtterances∑\nu=1\n(∑Words\nw=1 P(w=w)\n#Words\n)\n(4)\nWe investigated other means to compute an aggregated score on the dialogue level. We present the\nother results with low correlation coefﬁcients and signiﬁcance values in the appendix.\n3.4 Baseline\nWe take RUBER from Tao et al (2018) as a baseline. The approach initially employs two components\nthat perform two functions. The ﬁrst one is to calculate a resemblance score using word vector pooling\n6802\nand references. We aim for an unreferenced evaluation approach akin to a human evaluator. Thus, we\nuse only the second component of the method. This second component can calculate a relevance score\nfor a given response based on its preceding context. It uses a bidirectional GRU network and negative\nsampling. To reproduce as best as possible the original results of RUBER, we sample 1,449,218 pairs of\nsequential utterances from the OpenSubtitles dataset (Lison and Tiedemann, 2016).\n4 Evaluation\nIn this part of the paper, we will conduct a correlation analysis between the calculated probabilities\nfrom the LM and the scores given to dialogues by human evaluators. We provide a closer look at some\nauxiliary model outputs as well.\n4.1 Quantitative Assessment\nIn Table 3, we report the noteworthy Pearson’s and Spearman’s correlation coefﬁcients between the\naggregated probability scores and the evaluations of the dialogues.\nThe immediate observation of using language models as dialogue evaluators shows that there are gaps\nin terms of performance between the three different approaches. Most evident is the difference between\nBERT and the others. Its next sentence prediction objective explains this behavior. Unlike the other two,\nBERT takes the most structured approach to modeling two sequences. It recognizes the two utterances\nas separate and captures their information as a whole. Thus, when we compare it to GPT2 and XLNet, it\nhas the advantage of not needing score aggregation on utterance level, because it produces a probability\nfor the whole sentence rather than word for word.\nDataset ConvAI1 ConvAI2\nr ρ r ρ\nbert-nsp-d-sum 0.169 0.273 0.205 0.490\nbert-large-nsp-d-sum 0.172 0.277 0.205 0.485\ngpt2-u-avg-d-sum -0.027 0.068 0.152 0.323\ngpt2-md-u-avg-d-sum -0.005 0.069 0.144 0.325\ngpt2-lg-u-avg-d-sum -0.038 0.048 0.127 0.325\nxlnet-u-avg-d-sum 0.068 0.157 0.206 0.435\nxlnet-lg-u-avg-d-sum 0.087 0.169 0.225 0.437\nRUBER-U 0.154 0.129 0.013 -0.005\nTable 3: Pearson’sr, and Speaman’sρ, correlation coefﬁcients\non the two dialogue datasets’ human scores and various ag-\ngreggated scores from the language models. ”u-avg-d-sum”\nstands for averaged probabilities on utterance level and then\nsummed up on conversation level. Most of the scores are with\na conﬁdence of p <= 0.001. Exceptions are GPT2-medium\nand GPT2 in ConvAI1 with 0.812 and 0.212 respectively, as\nwell as, RUBER-U for ConvAI2, both r and ρ, with 0.5309\nand 0.8166, respectively.\nAlso, there is a smaller differ-\nence in performance between GPT and\nXLNet. First of all, they share a\ncore foundation as autoregressive lan-\nguage models, thus are more similar\nto each other than BERT, which also\nexplains their overall behavioral sim-\nilarity. However, XLNet has a struc-\ntural improvement in its architecture.\nUnlike GPT2, it also encodes the po-\nsitional information of the target to-\nken. Thus, similarly to BERT, it can\ncapture more information about a se-\nquence and consequently have a better\ncorrelation score.\nAdditionally, we investigate the ef-\nfect of model size. The differ-\nence in correlation coefﬁcients be-\ntween the hyperparameter conﬁgura-\ntions is marginal and, in one of the\ncases, even non-existent. The most\nevident example is the spectrum dis-\nplayed by the three GPT2 settings. Ul-\ntimately, we can conclude that smaller\nmodels perform similarly at a much\nsmaller energy cost.\nIn regards to score aggregation, all the approaches unanimously show that averaging on utterance level\nand summing up the whole conversation is the most informative for dialogue evaluation. At the same\ntime, the using a product or an unweighted average produce correlation coefﬁcients very close to zero\nand with an extremely low signiﬁcance (e.g., p−valueranging from 0.4 to 0.8). The behavior indicates\n6803\nthat while utterance length is insigniﬁcant, the duration of the conversation strongly dictates its quality\nscore.\n4.2 Qualitative Assessment\n(a) ConvAI1\n (b) ConvAI2\nFigure 2: Regression plots showing the relation between quality score and utterance length in the Con-\nvAI1 and ConvAI2 datasets. The shaded area around the line represents a conﬁdence interval.\nIn Figures 2a and 2b, the regression models show the interaction between the annotator quality score\nand the length of a conversation in ConvAI1 and ConvAI2, respectively. In both cases, the regression\nshows a positive trend that the longer a dialog is, the better its assessment is. We also see that in the\ncase of ConvAI1, the conﬁdence area is much wider than in ConvAI2. This behavior further supports\nthe results in Table 3, where the language models have considerably lower correlation coefﬁcients for\nConvAI1.\nFurthermore, we manually investigated short conversations from both datasets that also have low qual-\nity. Many of the short dialogues show that the system would indeed perform poorly by not responding\nat all, or the ﬁrst couple of utterances would be not diverse or even the same. Thus, the annotator would\nterminate the session and evaluate the dialogue with a low score. In contrast, conversations that were\nmore interactive and had longer duration also performed better in their assessment.\nOriginal Context Original Response\n(as in dataset)\nGenerated Response\n(generated by transformer)\n”Wow! Are you man or woman?” ”I am! i am a woman.” ” ’m a I am a man! I”\n”How nice! Do you have a\nboyfriend?” ”I do not. i am a single mom.” ” ’m . . I am a virgin woman. i”\n”What do you mean?” ”granted the right to accept\nonly one religion” ”anted, fact to be or the of”\n”Do you know Utrecht?” ”granted the right to\naccept only one religion”\n”ind, title to use donations\nDutch application”\nTable 4: Sample dialogue exchanges as originally seen in the ConvAI1 and ConvAI2 datasets together\nwith alternative responses generated by GPT2 by just taking the most likely word. Coherent examples\ninduces the language models to generate also good response. The top two examples have high human\nannotator scores, while the bottom two are rated lowly.\n6804\n4.3 What Would a Language Model Say?\nIn this subsection, we report the correlation scores between the maximum probabilities for each token\nand the annotator scores. The intuition is that besides being renown for advancing the state-of-the-art\nin various NLP benchmarks, language models are prominent for being capable generators of natural\nlanguage. Furthermore, Hendrycks and Gimpel (2017) have demonstrated that the maximum class prob-\nability of a neural network classiﬁer tends to output lower values for samples that are out of distribution.\nThus, we set to investigate whether the predicted maximum classes of language models can also indicate\nthe quality of dialogues.\nAlthough there are some studies (Wang et al., 2019) demonstrating BERT generating text, we will not\nconsider it in this part of the work due to the nature of its masked language modeling, which does not\naim at generating text. Considering GPT2 and XLNet, we look into what are the most likely words they\npredict for each token of the sequence instead of the original ones.\nFor the context of dialogue evaluation, it means that on averagemaxscores should be higher for ﬂuent\nand coherent text like the one used for pretraining the language models. At the same time, erroneous\nsamples should have lower maximum probabilities.\nFirstly, we investigate the quantitative relation of themaxscores to human annotator scores. Similarly\nto what we did in Section 4.1, we have calculated the aggregated probability scores for the most likely\nwords according to the language models:\nlm dialog scoremax =\nUtterances∑\nu=1\n(∑Words\nw=1 P(w=wmax)\n#Words\n)\n(5)\nDataset ConvAI1 ConvAI2\nr ρ r ρ\ngpt2-u-avg-d-sum 0.133 0.261 0.193 0.477\ngpt2-md-u-avg-d-sum 0.144 0.263 0.196 0.476\ngpt2-lg-u-avg-d-sum 0.146 0.267 0.196 0.477\nxlnet-u-avg-d-sum 0.157 0.263 0.211 0.471\nxlnet-lg-u-avg-d-sum 0.137 0.251 0.209 0.475\nTable 5: Pearson’s correlation coefﬁcients, r, and Spea-\nman’s correlation coefﬁcients, ρ, on the two dialogue\ndatasets’ human scores and various aggregated scores for\nthe maxword instead of the target. ”u-avg-d-sum” stands\nfor averaged probabilities on utterance level and then\nsummed up on conversation level. All of the scores are\nwith a conﬁdence of p<= 0.001.\nWe present the results in Table 5. When\ncompared to the analogous results in Ta-\nble 3, we see that GPT2 and XLNet\ndemonstrate noticeably higher correlation\ncoefﬁcients, especially for the dialogues\nin the ConvAI1 dataset. This discrep-\nancy suggests that for some of the cases,\nthe models can generate text that would\nﬁt better into the conversation. Since,\nConvAI1 and ConvAI2 happened before\nthe introduction of transformer-based lan-\nguage models, it is save to assume that the\nparticipating systems are inferior.\nIn Table 4, we present some short sam-\nple conversations together with a gener-\nated text by a language model. The top\ntwo examples have high scores by the hu-\nman annotators, while the rest are of low\nquality. The model can reconstruct sensi-\nble responses that make sense and are still different from the original reply. On the other hand, whenever\nthere is an incoherent conversation like the third and fourth examples, GPT2 and XLNet are not able to\nrecreate a response that is either somewhat ﬂuent or related to the current context. Another peculiarity is\nthat the language model possesses in a sense, common knowledge. This is demonstrated by the fourth\nexample, while in the preceding utterance, we see Utrecht, a Dutch city, and the model is then induced\nto predict ”Dutch” as one of the response tokens.\n5 Conclusion\nIn this study, we investigated whether transformer-based language models can evaluate dialogues in\nterms of coherency and ﬂuency. Overall, Pearson’s and Spearman’s correlation coefﬁcients demonstrate\n6805\nthat BERT, GPT2, and XLNet can indicate a conversation’s quality without any additional supervision or\nreference. While, in their core, the three use the same approach, transformers, they have further structural\nmodiﬁcations that set them apart when considered for the current problem domain.\nGPT2 performs worst due to its standard language modeling approach that incorporates the least struc-\ntural information about a sequence. XLNet achieves an inmprovement in terms of its correlation score\nby taking advantage of additional positional information when predicting a target token. Finally, BERT’s\nnext sentence prediction approach delivered the highest performance thanks to its structured approach in\nregards to separate utterances.\nWhile LM-based dialogue evaluators cannot yet replace human annotators, they have additional value\nwhen compared to word-over metrics like BLUE or ones that use word-embeddings. Although they can-\nnot completely replace human evaluators, They can support as weak indicators for quality. Additionally,\nwe have shown that they can perform better than competing approaches like the unreferenced component\nof RUBER.\nFurthermore, the autoregressive language models, GPT2 and XLNet, demonstrate an excellent initial\naptitude for conducting dialogues. They can provide alternative responses that are also coherent with the\ncontext of a discussion.\nThe LM-based method in this work considers dialogues as a series paired up utterances or question-\nanswers rather than one whole sequence. As future work, we will investigate how to extend the procedure\nso that it is more adept at capturing long-term information over the entire conversation.\nAcknowledgments\nWe acknowledge the support of the EU projects Cleopatra (GA 812997) and TAILOR (GA 952215), the\nFederal Ministry for Economic Affairs and Energy (BMWi) project SPEAKER (FKZ 01MK20011A),\nthe German Federal Ministry of Education and Research (BMBF) projects and excellence clusters ML2R\n(FKZ 01 15 18038 A/B/C), MLwin (01S18050 D/F), ScaDS.AI (01/S18026A) as well as the Fraunhofer\nZukunftsstiftung project JOSEPH.\nReferences\nJames Baker. 1975. The dragon system–an overview. IEEE Transactions on Acoustics, speech, and signal\nProcessing, 23(1):24–29.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correla-\ntion with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures\nfor machine translation and/or summarization, pages 65–72.\nMikhail Burtsev, Varvara Logacheva, Valentin Malykh, Iulian Vlad Serban, Ryan Lowe, Shrimai Prabhumoye,\nAlan W Black, Alexander Rudnicky, and Yoshua Bengio. 2018. The ﬁrst conversational intelligence challenge.\nIn The NIPS’17 Competition: Building Intelligent Systems, pages 25–46. Springer.\nHongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A survey on dialogue systems: Recent advances\nand new frontiers. Acm Sigkdd Explorations Newsletter, 19(2):25–35.\nAlexis CONNEAU and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances in\nNeural Information Processing Systems 32, pages 7057–7067. Curran Associates, Inc.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-\nXL: Attentive language models beyond a ﬁxed-length context. In Proceedings of ACL 2019, pages 2978–2988,\nFlorence, Italy, July. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of NAACL 2019, pages 4171–4186.\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela,\nArthur Szlam, Iulian Serban, Ryan Lowe, et al. 2019. The second conversational intelligence challenge (con-\nvai2). arXiv preprint arXiv:1902.00098.\n6806\nMichel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell,\nJianfeng Gao, and Bill Dolan. 2015. deltableu: A discriminative metric for generation tasks with intrinsically\ndiverse targets. In Proceedings of ACL-IJNLP 2015), pages 445–450.\nSergey Golovanov, Rauf Kurbanov, Sergey Nikolenko, Kyryl Truskovskyi, Alexander Tselousov, and Thomas\nWolf. 2019. Large-scale transfer learning for natural language generation. In Proceedings of ACL 2019, pages\n6053–6058.\nPrakhar Gupta, Shikib Mehri, Tiancheng Zhao, Amy Pavel, Maxine Eskenazi, and Jeffrey P Bigham. 2019. Inves-\ntigating evaluation of open-domain dialogue systems with human generated multiple references. InProceedings\nof the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 379–391.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline for detecting misclassiﬁed and out-of-distribution examples\nin neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings. OpenReview.net.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Lstm can solve hard long time lag problems. In Advances in\nneural information processing systems, pages 473–479.\nFrederick Jelinek. 1976. Continuous speech recognition by statistical methods. Proceedings of the IEEE ,\n64(4):532–556.\nKatharina Kann, Sascha Rothe, and Katja Filippova. 2018. Sentence-level ﬂuency evaluation: References help,\nbut can be spared! In Proceedings of CoNLL 2018, pages 313–323.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858.\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches\nout, pages 74–81.\nPierre Lison and J ¨org Tiedemann. 2016. Opensubtitles2016: Extracting large parallel corpora from movie and\ntv subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation\n(LREC’16), pages 923–929.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not\nto evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response\ngeneration. In Proceedings of EMNLP 2016, pages 2122–2132.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nVarvara Logacheva, Mikhail Burtsev, Valentin Malykh, Vadim Polulyakh, and Aleksandr Seliverstov. 2018. Con-\nvai dataset of topic-oriented human-to-chatbot dialogues. In The NIPS’17 Competition: Building Intelligent\nSystems, pages 47–57. Springer.\nRyan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Bengio, and Joelle\nPineau. 2017. Towards an automatic turing test: Learning to evaluate dialogue responses. In Proceedings\nof ACL 2017, pages 1116–1126.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On the state of the art of evaluation in neural language models.\narXiv preprint arXiv:1707.05589.\nRostislav Nedelchev, Ricardo Usbeck, and Jens Lehmann. 2020. Treating dialogue quality evaluation as an\nanomaly detection problem. In Proceedings of The 12th Language Resources and Evaluation Conference ,\npages 501–505, Marseille, France, May. European Language Resources Association.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of ACL 2002, pages 311–318. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLP 2018, pages 2227–\n2237.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners.\n6807\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\nAlan Ritter, Colin Cherry, and William B Dolan. 2011. Data-driven response generation in social media. In\nProceedings of EMNLP 2011, pages 583–593. Association for Computational Linguistics.\nAnanya B Sai, Mithun Das Gupta, Mitesh M Khapra, and Mukundhan Srinivasan. 2019. Re-evaluating adem: A\ndeeper look at scoring dialogue responses. arXiv preprint arXiv:1902.08832.\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016. Building end-\nto-end dialogue systems using generative hierarchical neural network models. In Thirtieth AAAI Conference on\nArtiﬁcial Intelligence.\nAlessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie,\nJianfeng Gao, and Bill Dolan. 2015. A neural network approach to context-sensitive generation of conversa-\ntional responses. In Proceedings of NAACL-HLT 2015, pages 196–205.\nHiroaki Sugiyama, Toyomi Meguro, and Ryuichiro Higashinaka. 2019. Automatic evaluation of chat-oriented\ndialogue systems using large-scale multi-references. In Advanced Social Interaction with Agents, pages 15–25.\nSpringer.\nChongyang Tao, Lili Mou, Dongyan Zhao, and Rui Yan. 2018. Ruber: An unsupervised method for automatic\nevaluation of open-domain dialog systems. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van\nDurme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? probing for\nsentence structure in contextualized word representations. In International Conference on Learning Represen-\ntations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008.\nAlex Wang, Kyunghyun Cho, and CIFAR Azrieli Global Scholar. 2019. Bert has a mouth, and it must speak: Bert\nas a markov random ﬁeld language model. NAACL HLT 2019, page 30.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-\nart natural language processing. ArXiv, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xl-\nnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information\nProcessing Systems 32, pages 5754–5764. Curran Associates, Inc.\nKoichiro Yoshino, Chiori Hori, Julien Perez, Luis Fernando D’Haro, Lazaros Polymenakos, Chulaka Gunasekara,\nWalter S Lasecki, Jonathan K Kummerfeld, Michel Galley, Chris Brockett, et al. 2019. Dialog system technol-\nogy challenge 7. arXiv preprint arXiv:1901.03461.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing\ndialogue agents: I have a dog, do you have pets too? In Proceedings of ACL 2018, pages 2204–2213.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating\ntext generation with bert. In International Conference on Learning Representations.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang. 2019. Evaluating commonsense in pre-trained language\nmodels. arXiv preprint arXiv:1911.11931.\n6808\nA Equations for Aggregating Scores on Dialogue Level\nIn this section, we list the different aggregation measures that we experimented with. The correlation\ncoefﬁcients between these aggregations scores and the human annotation are either of low values, are\ninsigniﬁcant (low p-value), or both.\nlm dialog score=\nUtterances∑\nu=1\n(Words∑\nw=1\nP(w=w)\n)\n(6)\nlm dialog score= 1\n#Utterances\nUtterances∑\nu=1\n(∑Words\nw=1 P(w=w)\n#Words\n)\n(7)\nlm dialog score= 1\n#Utterances\nUtterances∑\nu=1\n(Words∑\nw=1\nP(w=w)\n)\n(8)\nlm dialog score=\nUtterances∏\nu=1\n(Words∑\nw=1\nP(w=w)\n)\n(9)\nlm dialog score=\nUtterances∏\nu=1\n(∑Words\nw=1 P(w=w)\n#Words\n)\n(10)\nlm dialog score=\nUtterances∏\nu=1\n(Words∏\nw=1\nP(w=w)\n)\n(11)"
}