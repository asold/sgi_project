{
    "title": "Hot or Cold? Adaptive Temperature Sampling for Code Generation with Large Language Models",
    "url": "https://openalex.org/W4393158884",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2101042212",
            "name": "yuqi Zhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2020097424",
            "name": "Jia Li",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A1993360758",
            "name": "Ge Li",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2132877457",
            "name": "Yunfei Zhao",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2110400062",
            "name": "Zhi Jin",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2096166354",
            "name": "Hong Mei",
            "affiliations": [
                "Peking University",
                "Ministry of Education",
                "Institute of Software"
            ]
        },
        {
            "id": "https://openalex.org/A2101042212",
            "name": "yuqi Zhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2020097424",
            "name": "Jia Li",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A1993360758",
            "name": "Ge Li",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2132877457",
            "name": "Yunfei Zhao",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2110400062",
            "name": "Zhi Jin",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2096166354",
            "name": "Hong Mei",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Peking University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6660690085",
        "https://openalex.org/W6682264535",
        "https://openalex.org/W6798182279",
        "https://openalex.org/W6750305986",
        "https://openalex.org/W3102516861",
        "https://openalex.org/W4224060952",
        "https://openalex.org/W6761551260",
        "https://openalex.org/W4307932749",
        "https://openalex.org/W4320854285",
        "https://openalex.org/W1881604308",
        "https://openalex.org/W4226485558",
        "https://openalex.org/W6680571093",
        "https://openalex.org/W3137269947",
        "https://openalex.org/W4376312043",
        "https://openalex.org/W2149741699",
        "https://openalex.org/W4362508616",
        "https://openalex.org/W4287270774",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W4384345649",
        "https://openalex.org/W4361866100",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W4387724059",
        "https://openalex.org/W4255825300",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W2983962589"
    ],
    "abstract": "Recently, Large Language Models (LLMs) have shown impressive abilities in code generation. However, existing LLMs' decoding strategies are designed for Natural Language (NL) generation, overlooking the differences between NL and programming languages (PL). Due to this oversight, a better decoding strategy for code generation remains an open question. In this paper, we conduct the first systematic study to explore a decoding strategy specialized in code generation. With an analysis of loss distributions of code tokens, we find that code tokens can be divided into two categories: challenging tokens that are difficult to predict and confident tokens that can be easily inferred. Among them, the challenging tokens mainly appear at the beginning of a code block. Inspired by the above findings, we propose a simple yet effective method: Adaptive Temperature (AdapT) sampling, which dynamically adjusts the temperature coefficient when decoding different tokens. We apply a larger temperature when sampling for challenging tokens, allowing LLMs to explore diverse choices. We employ a smaller temperature for confident tokens avoiding the influence of tail randomness noises. We apply AdapT sampling to LLMs with different sizes and conduct evaluations on two popular datasets. Results show that AdapT sampling significantly outperforms state-of-the-art decoding strategy.",
    "full_text": "Hot or Cold? Adaptive Temperature Sampling for Code Generation with Large\nLanguage Models\nYuqi Zhu1, Jia Li2, Ge Li*2, YunFei Zhao2, Jia Li2, Zhi Jin2, Hong Mei2,3\n1Academy of Military Sciences, China\n2Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education; School of Computer\nScience, Peking University, Beijing, China\n3Advanced Institute of Big Data, Beijing, China\nzhuyuqi1997@126.com, lijia@stu.pku.edu.cn, {lige, zhaoyunfei, lijiaa, zhijin, meih}@pku.edu.cn\nAbstract\nRecently, Large Language Models (LLMs) have shown\nimpressive abilities in code generation. However, existing\nLLMs’ decoding strategies are designed for Natural Lan-\nguage (NL) generation, overlooking the differences between\nNL and programming languages (PL). Due to this oversight, a\nbetter decoding strategy for code generation remains an open\nquestion. In this paper, we conduct the first systematic study\nto explore a decoding strategy specialized in code genera-\ntion. With an analysis of loss distributions of code tokens,\nwe find that code tokens can be divided into two categories:\nchallenging tokens that are difficult to predict and confident\ntokens that can be easily inferred. Among them, the challeng-\ning tokens mainly appear at the beginning of a code block.\nInspired by the above findings, we propose a simple yet effec-\ntive method: Adaptive Temperature (AdapT) sampling, which\ndynamically adjusts the temperature coefficient when decod-\ning different tokens. We apply a larger temperature when\nsampling for challenging tokens, allowing LLMs to explore\ndiverse choices. We employ a smaller temperature for confi-\ndent tokens avoiding the influence of tail randomness noises.\nWe apply AdapT sampling to LLMs with different sizes and\nconduct evaluations on two popular datasets. Results show\nthat AdapT sampling significantly outperforms state-of-the-\nart decoding strategy.\nIntroduction\nCode generation aims to automatically generate a pro-\ngram that satisfies a natural language requirement (Li et al.\n2023d,a,b). In recent years, Large Language Models (LLMs)\nhave attracted great attention for their potential for automat-\ning coding (Li et al. 2023c,e). Noteworthy models like Al-\nphaCode (Li et al. 2022) and Codex (Chen et al. 2021) have\ndemonstrated their impressive ability to solve unforeseen\nprogramming challenges.\nLLMs rely on a decoding strategy to generate code. Exist-\ning LLM’s decoding strategies for code generation mainly\nfall into two categories. The first category is search-based\nmethods, which aim to maximize the probability of the\nnext generated token, including greedy search (Black and\nE 2012) and beam search (Freitag and Al-Onaizan 2017).\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: An illustration of a code snippet and its corre-\nsponding loss distribution. The challenging tokens are high-\nlighted in yellow.\nNonetheless, the results generated by these methods lack di-\nversity and are prone to generating empty or repetitive re-\nsults (Zhang et al. 2021). The second category is sampling-\nbased methods, which randomly sample the next token\nbased on the probability distribution. The state-of-the-art\n(SOTA) approach (Chen et al. 2021) uses temperature sam-\npling that reshapes the probability distribution by introduc-\ning a temperature coefficient to control the level of sampling\nrandomness.\nDespite the promising results, temperature sampling has\nlimitations in code generation. Since it is initially used to\ngenerate Natural Language (NL) with a flexible syntax, its\neffectiveness decreases when transitioning to code genera-\ntion, which is a high-accuracy demanding task. For instance,\nCodeGeeX (Zheng et al. 2023), when utilizing tempera-\nture sampling, attains 36.0% Pass@15 on the HumanEval\ndataset. Thus, it is necessary to explore more advanced de-\ncoding strategies to improve the accuracy of LLMs in code\ngeneration.\nIn this paper, we present the first systematic study to ex-\nplore a decoding strategy for code generation with LLM.\nOur contributions can be summarized as follows:\n(1) By analyzing the loss distribution of code tokens,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n437\nwe categorize code tokens into challenging tokens and\nconfident tokens. We design comparative experiments to in-\nvestigate the differences in loss distributions between source\ncode and NL text. Our analysis reveals that the source code\nsuffers lesser variation in loss values during generation than\nNL text. Next, we visualize the loss distribution of source\ncode. We find an apparent discrepancy in loss values among\ndifferent code tokens. In this study, we refer to these tokens\nwith high loss values as challenging tokens. With statistical\nanalysis, we find that challenging tokens frequently appear\nat the beginning of a code block. An illustration of the chal-\nlenging tokens is shown in Figure 1. The remaining tokens,\ncharacterized by low loss values for LLMs, are referred to\nas confident tokens.\n(2) In light of our findings, we propose Adaptive Tem-\nperature (AdapT) sampling, which dynamically adjusts\nthe temperature coefficient. Compared to standard tem-\nperature sampling, AdapT sampling dynamically adjusts the\ntemperature coefficient T according to the type of next-\ntokens. Our motivation is that LLMs require exploring di-\nverse choices for challenging tokens. For confident tokens,\nwe should select tokens with high probabilities. Specifically,\nfor challenging tokens, AdapT sampling utilizes a high T\nvalue to increase sample diversity. AdapT sampling uses a\nlow T value for confident tokens to minimize randomness\nnoise.\n(3) Experimental results show that AdapT sampling\ncan improve the pass@k metric on HumanEval and\nMBPP datasets with different sizes of LLMs. We apply\nthe AdapT sampling to multiple LLMs with various sizes\n(from 2B to 13B) and conduct evaluations on two repre-\nsentative code generation datasets (HumanEval and MBPP).\nExperimental results show AdapT sampling outperforms the\nSOTA decoding strategy which uses a standard tempera-\nture sampling. For example, it surpasses the pass@15 met-\nric over the SOTA method by up to 13.6% on HumanEval.\nWe further investigate the robustness of AdapT sampling\nto different hyperparameter settings and the quality of the\ncode generated by AdapT sampling. Our code is available at\nhttps://github.com/LJ2lijia/AdapT.\n(4) Future directions. Based on our findings, we list the\ncurrent challenges and propose future research directions on\ndeveloping effective decoding strategies for code generation.\nBackground\nCode Generation with LLMs\nLLMs are transformer-based models that are trained using\nlarge corpora of NL text and source code. In recent years,\nLLMs have achieved impressive results in automatic code\ngeneration. Among LLMs, the GPT family of LLMs from\nOpenAI is popular and powerful, including GPT-3 (175B\nparameters) (Brown et al. 2020), Codex (175B parameters)\n(Chen et al. 2021), etc. Since OpenAI LLMs are closed-\nsource, there have been many attempts to reproduce similar\nLLMs, such as CodeGen (Nijkamp et al. 2022a), CodeGeeX\n(Zheng et al. 2023), InCoder (Fried et al. 2022).\nDecoding Strategy\nGiven a requirement x, LLMs rely on a decoding strategy to\ngenerate the code auto-regressively. The decoding strategy\ndetermines how LLMs select the next token yt based on the\ncontext y<t, x. y<t is the token sequence that has been gen-\nerated. There has been a series of works exploring decoding\nstrategies for NL generation, and these methods have been\nsubsequently applied to code generation. They can be clas-\nsified into two categories: search-based and sampling-based\nmethods.\nSearch-based Decoding Stratrgy Greedy search (Mou\net al. 2015) is one of the most commonly used de-\ncoding strategies. In greedy search, the model selects\nthe next token which maximizes the probability: yt =\narg maxy p(yt|y<t, x). Despite its simplicity, it may lead to\noverly conservative results and a lack of diversity (See et al.\n2019). Beam search (Freitag and Al-Onaizan 2017) is an im-\nproved version of greedy search. This algorithm retains top\nB (beam size) tokens with the highest probability. However,\nit has been found that beam search results in degenerations\nsuch as repetitions and empty (Holtzman et al. 2019).\nSampling-based Decoding Strategy The degenerations\nsuch as empty sequences and repetitions can be alleviated\nusing sampling decoding, which randomly selects the next\ntoken based on predicted probability.\nTemperature sampling (Ackley, Hinton, and Sejnowski\n1985) has been applied widely, it uses a temperature coef-\nficient T (usually ∈ [0, 1]) to control the sampling random-\nness. Given the logits u and temperature T, the softmax dis-\ntribution p′ is re-estimated as:\np′(yt|y<t, x) = exp((u(yt|y<t,x))\nT )\nPn\nj=1 exp((u(yj|y<t,x))\nT )\n(1)\nIn addition, researchers propose Top-k (Fan, Lewis, and\nDauphin 2018) sampling to further improve performance. At\neach step, Top-k sampling filters the k most probable next\ntokens and redistributes the probability among these k to-\nkens for sampling. However, the unreliable tail problem in\nthe Top-k sampling may affect the sampling quality. Top-p\nsampling (Holtzman et al. 2019) eliminates the unreliable\ntail problem by sampling from the smallest token set whose\ncumulative probability reaches the threshold p.\nLLMs usually use the method of combining tempera-\nture sampling and Top-p sampling to achieve SOTA results\n(Chen et al. 2021; Nijkamp et al. 2022a; Fried et al. 2022).\nSpecifically, the logits are first rescaled with temperature T.\nAfter this, Top-p sampling is employed to derive the final\nresults. Existing work (Chen et al. 2021) finds out that tem-\nperature coefficient T has an obvious influence on the code\ngeneration results. Increasing the T value can enhance the\nchance of exploring the correct answers, but this comes at\nthe cost of introducing more errors in the generation results.\nTherefore there is a need to develop more effective sampling\nmethods specifically for code generation.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n438\n NL            code\n0.0 \n2.0 \n4.0 \n6.0 \n8.0 \n10.0 \n12.0 \n1 11 21 31 41 51 61 71 81 91\n-LOG(P)\nTIMESTEP\nMBPP/97\n0.0 \n2.0 \n4.0 \n6.0 \n8.0 \n1 11 21 31 41 51 61 71\n-LOG(P)\nTIMESTEP\nHumanEval/74\n0.0 \n2.0 \n4.0 \n6.0 \n8.0 \n10.0 \n1 11 21 31 41 51 61 71 \n-LOG(P)\nTIMESTEP\nHumanEval/131\n0.0 \n1.0 \n2.0 \n3.0 \n4.0 \n5.0 \n6.0 \n1 11 21 31 41 51\n-LOG(P)\nTIMESTEP\nMBPP/130\nstatistical evaluation of all 40 samples\nFigure 2: Comparison of loss distributions on NL text and source code. The left table shows the statistical results of different\ndistributions on 40 samples. The right figures show several examples of loss distributions on NL text and source code.\nAnalysis of the Code Generation Process\nIn this section, we first investigate the differences between\nNL generation with LLMs and code generation with LLMs.\nWe compared NL text’s loss distributions (i.e., cross-entropy\nloss (Brownlee 2019)) to ones of source code.\nFurthermore, we analyze the fluctuations of loss values of\ncode tokens within code snippets. We find that code tokens\ncan be categorized into challenging tokens and confident to-\nkens. Based on the analysis, we discuss the challenges en-\ncountered in code generation.\nLoss Distribution Comparison\nIn this section, we analyze the differences between the pro-\ncess of generating NL text and source code with LLMs. We\nchoose loss distribution as the comparison metric.\nWe conduct experiments with a powerful LLM for source\ncode - CodeGen. We select the CodeGen-mono with 2 bil-\nlion parameters (CodeGen-2B) as the base model. This\nmodel is trained with a 635GB code corpus and 1159GB\nEnglish text data. Therefore, CodeGen can generate NL text\nand code. The datasets used in this section are HumanEval\n(Chen et al. 2021) and MBPP (Austin et al. 2021), which are\nrepresentative datasets in code generation.\nFirst, we randomly select 20 code samples each from Hu-\nmanEval and MBPP datasets. Then, we manually write NL\ndescriptions for each code snippet, which describes the func-\ntionality of the code. We keep the alignment of the length of\nNL and code as much as possible while maintaining text flu-\nency. As a result, we obtain 40 NL descriptions aligned with\ntheir corresponding code snippets. Next, we gather the loss\nvalues of the CodeGen model on these NL descriptions and\ncode snippets, respectively.\nWe use various metrics (e.g. mean value (Runnenburg\n1978), standard deviation, skewness, and perplexity) to com-\npare the loss distributions of NL descriptions and source\ncode. Standard deviation (Bland and Altman 1996) reflects\nthe average amount of variability. Skewness (Brown 2011)\nis a measure of the asymmetry of the probability distribution\nof a real-valued random variable about its mean. Perplexity\n(Brown et al. 1992) is a measurement of how confidently\nan LLM predicts a sample. As shown in the table in Figure\n2, the average value of losses on NL descriptions is higher\nthan the one on the source code. When compared with code,\nNL suffers greater variation in prediction loss during gener-\nation. The value of the skewness shows that there are more\ntokens with large loss values in NL descriptions than in the\nsource code. The LLM also has a higher perplexity for NL\ndescriptions than for source code. We show a few examples\nin Figure 2 to visualize the differences.\nThese differences arise because source code has a more\nstrict syntax and semantics compared to NL. Therefore,\nwhen generating code, some tokens can be easily inferred\nbased on grammatical rules, and LLMs can confidently gen-\nerate these tokens with low loss values. In contrast, NL al-\nlows greater freedom in word usage and often presents mul-\ntiple viable choices for the same context, which results in\nhigh loss values.\nAdditionally, we find that a number of peaks occur in\nloss distributions of source code, i.e., tokens with a higher\nloss value than nearby tokens. A higher loss value means\nthat it is more difficult to make correct predictions at these\nlocations. We call the tokens with peak loss valueschalleng-\ning tokens. As for the tokens that have low loss values, the\nmodel has more confidence in predicting them correctly. We\nrefer to them as confident tokens.\nIn-depth Study of Code Tokens\nThis section provides a detailed investigation of the chal-\nlenging tokens and confident tokens. We analyze samples\nfrom the MBPP (Austin et al. 2021) (500 samples), Hu-\nmanEval (Chen et al. 2021) (164 samples), and APPS\n(Hendrycks et al. 2021) (train set: 5000 samples) datasets.\nWe use CodeGen-2B to generate the ground truth code in\nthese datasets and collect the corresponding loss values.\nFirst, we define the predictive difficulty (PD) of a token,\nwhich is the rank (%) of the token loss among all token loss\nvalues in the code snippet, and compute PDs for all tokens.\nThen, we split each code snippet into code lines and inte-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n439\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n1 6 11 16 21 26 31 36\nposition prediction difficulty\ntoken position\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n1 6 11 16 21 26 31\nposition prediction difficulty\ntoken position\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n1 6 11 16 21 26 31 36 41\nposition prediction difficulty\ntoken position\nMBPPHumanEval APPS\nFigure 3: The prediction difficulty of different positions, the x-axis represents the position of the token within the line of code,\nand the y-axis represents the prediction difficulty of the token.\ngrate (average) the tokens’ PDs in the same position of dif-\nferent code lines to obtain the position prediction difficulty.\nFinally, we statistically average the position prediction diffi-\nculty across all codes in the dataset. We omit positions where\nthe cumulative data numbers of these positions are below 5%\nof the overall token numbers. Figure 3 shows the position\nprediction difficulty of different token positions. The results\nreveal the regularity that the tokens in the first position of\ncode lines have the highest prediction difficulty of 69.0%\n(HumanEval), 73.1% (MBPP), and 71.9% (APPS). There-\nfore, we assume that the first position in each code line is\nwhere challenging tokens tend to appear and conduct further\nexperiments.\nWe distinguish between challenging tokens and confident\ntokens based on PD. We introduce a threshold H to sepa-\nrate two types of tokens. Tokens with PD > H are cate-\ngorized as challenging tokens, while tokens with PD < H\nare categorized as confident tokens. We set H=0.9, and the\nproportion of challenging tokens appearing in the first posi-\ntion was 24.8% (HumanEval), 22.6% (MBPP), and 28.1%\n(APPS), respectively. The results confirm our assumption\nthat the challenging tokens are not randomly distributed for\ndifferent locations, they tend to appear at the first position of\neach code line.\nTo further investigate the properties of challenging tokens,\nwe distinguish the tokens at the first position of each line\nbased on whether it is the initial token of a code block. A\ncode block in Python is a piece of Python program text that\ncan be executed as a unit. The code block begins at the first\nindented statement and continues until the indentation re-\nturns to a previous level. The initial token of the code block\nhas a prediction difficulty of 79.8% (HumanEval), 83.4%\n(MBPP), and 82.5% (APPS) which is significantly higher\nthan the first positions of other code lines. We derive that\namong the first positions of code lines, the initial token of\neach code block is the most likely place for a challenging\ntoken to appear.\nThis can be attributed to the fact that LLMs need to de-\ntermine the next control structure after a block of code is\npresented, which increases prediction difficulty. LLMs can\neasily generate the next token after receiving the initial to-\nken since the strict syntax rules limit the scope of variations.\nAdapT Sampling\nIn light of our findings, we propose a simple yet effec-\ntive decoding method, AdapT sampling (Adaptive Tempera-\nture Sampling), which adjusts the temperature coefficient T\nfor different tokens. Specifically, for the challenging tokens,\nwhich LLMs struggle to predict correctly, AdapT sampling\nuses a high temperature coefficient which introduces more\ndiverse tokens. On the other hand, for confident tokens, the\ntemperature coefficient is set to a small value to minimize\nrandomness noises. Specifically, we formulate AdapT sam-\npling as follows:\np′(yt|y<t, x)) =\nexp((u(yt|y<t,x))\nT(t) )\nPn\nj=1 exp((u(yj|y<t,x))\nT(t) )\n(2)\nT(t) =\n\u001a\na if yt is the code block initial token\nb else (3)\nwhere T is the temperature coefficient and t is the sample\ntimestep. a, b∈ [0, 1] (a > b) are hyperparameters that con-\ntrol the degree of sampling randomness.\nExperiments\nBenchmarks\nHumanEval (Chen et al. 2021) is a Python code generation\nbenchmark with 164 test samples. Each sample consists of\na manually written programming problem, which consists\nof a natural language requirement, a function signature, and\nseveral unit tests. It asks LLMs to generate the function body\nbased on the requirement and the signature. The unit tests are\nused to check the correctness of generated functions.\nMBPP (Austin et al. 2021) contains 500 programming prob-\nlems collected from real-world communities. Solving these\nprogramming problems requires simple numeric manipula-\ntions and the basic usage of standard libraries. Each problem\ncontains an English requirement, a Python function signa-\nture, and three test cases. We take the requirement and the\nfunction signature as input and leverage LLMs to generate\nthe function body. Then, the generated code is evaluated us-\ning test cases.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n440\nMetric pass@5\npass@10 pass@15\nCodeGen:\nSP, T=0.2\n29.2 31.3 32.3\nSP, T=0.4 33.4 37.9 40.8\nSP, T=0.6 33.1 38.0 40.8\nSP, T=0.8 32.7 39.7 43.9\nAdapT 34.4 40.1 43.9\nInCoder:\nSP, T=0.2\n22.8 25.9 27.4\nSP, T=0.4 25.0 29.8 32.3\nSP, T=0.6 24.5 30.0 32.9\nSP, T=0.8 22.3 28.6 32.9\nAdapT 25.8 31.6 35.3\nCodeGeeX:\nSP, T=0.2\n24.1 27.1 28.7\nSP, T=0.4 27.0 30.5 32.3\nSP, T=0.6 27.7 32.5 35.4\nSP, T=0.8 27.1 33.0 36.0\nAdapT 29.4 36.3 40.9\nTable 1: The performance (pass@5, 10, 15) for CodeGen-\n2B, InCoder-6B, and CodeGeeX-13B on the HumanEval\ndataset using AdapT sampling and SOTA (SP) method.\nBase Models\nIn this paper, we select three representative open-source\nLLMs as base models: CodeGen, InCoder, and CodeGeeX.\nCodeGen (Nijkamp et al. 2022b) is a family of LLMs pre-\ntrained on a large amount of NL texts and source code. It\nis trained with a 635GB code corpus and 1159GB English\ntext data. In this paper, we select the CodeGen-mono with 2\nbillion parameters as the base model.\nInCoder (Fried et al. 2023) is pre-trained with a large cor-\npus of permissively licensed code (216GB). It can perform\ncode generation and code infilling. In this paper, we use a\nversion with 6.7 billion parameters, named InCoder-6B, for\ncode generation.\nCodeGeeX (Zheng et al. 2023) is a multilingual LLM with\n13 billion parameters. It is pre-trained on a large corpus of\nmore than 20 programming languages and achieves impres-\nsive performance on code generation.\nDespite OpenAI models’ impressive performance, we can\nnot access the probability distribution with only API calls.\nTherefore, we ignore these models in this paper.\nBaselines\nIn this paper, we choose the SOTA approach mentioned\nin the background section as the baseline. For the sake of\nbrevity, we refer to the baseline (reshaping distribution with\ntemperature sampling) as SP in the following sections.\nImplementation Details\nWe run all of our experiments on 2 NVIDIA V100 GPUs\nwith 32GB memory. For our experimental datasets, the max-\nimum generated length is 500. All experiments are con-\nducted in a zero-shot setting which means we directly feed\nthe input requirement into LLMs without examples. Then\nwe extract generated programs from the model’s output.\nMetric pass@5\npass@10 pass@15\nCodeGen:\nSP, T=0.2\n33.1 36.5 38.4\nSP, T=0.4 37.0 42.1 45.0\nSP, T=0.6 37.1 43.5 47.0\nSP, T=0.8 35.2 43.1 47.0\nAdapT 37.2 44.4 48.2\nInCoder:\nSP, T=0.2\n23.0 27.4 29.2\nSP, T=0.4 26.0 29.6 32.4\nSP, T=0.6 24.6 30.4 34.6\nSP, T=0.8 23.6 30.6 34.6\nAdapT 26.8 32.9 36.8\nCodeGeeX:\nSP, T=0.2\n20.7 23.3 22.4\nSP, T=0.4 23.7 28.4 31.0\nSP, T=0.6 24.8 31.2 34.8\nSP, T=0.8 25.4 31.2 35.6\nAdapT 25.8 32.2 36.0\nTable 2: The performance (pass@5, 10, 15) for CodeGen-\n2B, InCoder-6B, and CodeGeeX-13B on the MBPP dataset\nusing AdapT sampling and SOTA (SP) method.\nMetric\nPass@k Pass@k (Chen et al. 2021) measures the func-\ntional correctness of the generated code by executing test\ncases. We use the unbiased version of pass@k, where n ≥ k\nsamples are generated for each problem, and c ≤ n is the\nnumber of correct samples that pass test cases. Pass@k is\ncalculated as follows:\npass@k = EProblems\n\n1 −\n\u0012\nn − c\nk\n\u0013\n\u0012\nn\nk\n\u0013\n\n (4)\nMain Results\nWe apply AdapT sampling to three base models on two code\ngeneration datasets. The performance is shown in Table 1\n12.0%\n17.0%\n22.0%\n27.0%\n32.0%\n37.0%\n42.0%\npass@5 pass@10 pass@15\npass@\nk vs. k, temperature\ngreedy T=0.1 T=0.2 T=0.3 T=0.4 T= 0.5\nT=0.6 T=0.7 T=0.8 T=0.9 AdapT\ngreedy search\nSP with \ndifferent T\nAdapT \nFigure 4: The performance of CodeGeeX-13B on the Hu-\nmanEval dataset with dense temperature settings.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n441\nDataset HumanEval MBPP\nCodeGen:\nGreedy Search 23.8 24.0\nSP-best 22.2 24.6\nAdapT 23.3 24.9\nInCoder:\nGreedy Search 14.0 12.3\nSP-best 15.3 13.8\nAdapT 15.2 14.8\nCodeGeeX:\nGreedy Search 18.9 13.6\nSP-best 18.0 13.4\nAdapT 18.8 13.5\nTable 3: Pass@1 results for CodeGen-2B, InCoder-6B, and\nCodeGeeX-13B using different decoding strategies on Hu-\nmanEval and MBPP datasets.\nand Table 2. Following the previous works (Nijkamp et al.\n2022a; Zheng et al. 2023; Fried et al. 2022), we set p of\ntop-p sampling as 0.95 for all our experiments. Following\nthe previous work (Chen et al. 2021), we set the values of\ntemperature of baseline as 0.2, 0.4, 0.6, and 0.8 respectively.\nWe experimentally select the values of a and b. The sam-\npling number n is 15. As shown in Table 1 and Table 2,\nAdapT sampling outperforms the SOTA method in terms of\npass@5, pass@10, and pass@15 on HumanEval and MBPP\ndatasets. Pass@15 represents the number of problems solved\nsince the sampling numbern is 15. AdapT sampling with the\nhighest pass@15 indicates that it solves the most problems.\nNotably, on the HumanEval dataset, AdapT sampling can\nenhance the pass@15 of CodeGeeX from 36.0% to 40.9%,\nreaching a 13.6% improvement. Meanwhile, the number\nof solved problems increased from 60 to 67. On the Hu-\nmanEval and MBPP datasets, CodeGeeX-13B can solve 14\nand 36 previously unsolved problems with AdapT, respec-\ntively. When run with a constant value of T, increasing T\nwill improve the number of problems solved, but it may re-\nduce pass@5 (indicates the proportion of correct answers\nsampled for each question) due to the introduction of ran-\ndomness (Chen et al. 2021; Holtzman et al. 2019). On the\nother hand, AdapT sampling can dynamically adjust the\nsampling randomness, thus minimizing the noise associated\nwith increasing T, hence consistently demonstrating an im-\nprovement in pass@5, 10, and 15.\nAnalysis and Discussion\nWe conduct an in-depth and comprehensive analysis of\nAdapT sampling’s capabilities.\npass@1 The pass@1 metric represents the probability\nthat, among multiple pieces of generated code, the first piece\nchosen would successfully pass a given test case. Note that\nthe pass@1 are very strict metrics and are hard to improve.\nWe compare the pass@1 results of AdapT sampling with\ngreedy search (which usually has a high pass@1 value) and\nthe best pass@1 results of SOTA (SP-best) and show the re-\nsults in Table 3. Our method outperforms SP-best in 83.3%\ncases on the pass@1 metric. Meanwhile, AdapT sampling\ncan reach a comparable pass@1 when compared with greedy\nsearch. Greedy search can only sample one answer per ques-\ntion, whereas our method can samplen answers and increase\nthe number of solved questions.\nCompare with Different T Settings To explore the upper\nbound performance of SP, we set the temperature value in\nSP more densely from 0 to 1, taking a value every 0.1 in-\ntervals. The results are shown in Figure 4. The performance\nis not displayed on the resulting graph when T ≥ 1, since\nit drops significantly when T ≥ 1. AdapT sampling sig-\nnificantly outperforms temperature sampling at all settings\non pass@5, pass@10, and pass@15. The LLM can only\nanswer 31 questions correctly with a greedy search. Using\nthe AdapT sampling method, LLM can solve twice as many\nproblems (67) as greedy search.\nHyperparameters Analysis There are two hyperparame-\nters involved in AdapT sampling: a and b. In this section,\nwe examine how changing these two parameters affects the\ncode generation results.\nFirst, we fix a = 0.8, and we take a b value every 0.1\nstep from 0.1 to 0.9, the experimental results are displayed\nin the upper part of Figure 5. AdapT sampling outperforms\nSP (T = 0.8) on pass@1, pass@5, pass@10, and pass@15\nmetrics when b equals 0.3, 0.5, 0.6, and 0.7. Then we set\nb = 0.3, and vary the value of a from 0.1 to 0.9 with a step\nof 0.1. The results are shown at the bottom half of Figure\n5. It can be seen from the experimental results that when\na > b, the results of pass@5, pass@10, and pass@15 can be\neffectively improved. Additionally, changing a from 0.2 to\n0.9 can increase pass@15 from 32.9% to 37.8%, achieving\na 14.9% improvement.\nThe results indicate that AdapT sampling can outperform\nSP under a variety of settings, which confirms the robustness\nof AdapT sampling over hyperparameters. The empirical hy-\nperparameter guidelines are: for optimizing pass@k(k >1),\nset a to approximately 0.8 and b to around 0.5; for optimiz-\ning pass@1, set a to approximately 0.2 andb to around 0.01.\nCode Quality Evaluation In this section, we analyze the\nquality of code generated by different sampling methods on\nthe HumanEval dataset. We collect the execution results of\nthe generated code of three models and present them in Fig-\nure 6. AdapT sampling can generate more correct samples\n(passed) than baselines on all models. Using AdapT sam-\npling, CodeGen-2B, InCoder-6B, and CodeGeeX-13B can\ngenerate 517, 333, 382 correct codes, which outperforms SP\nup to 17.2%, 8.8%, 14.0% higher than sampling with the SP\nmethod, respectively.\nThere are fewer TypeError and SyntaxError in the code\ngenerated by the three models using AdapT sampling than\nSP. By using AdapT sampling in CodeGen and CodeGeeX\nmodels, the occurrence of NameError can be reduced by\n44.3% and 23.4%. In CodeGen and CodeGeeX, AdapT sam-\npling reduces the incidence of NameError by 44.3% and\n23.4%. The else section in Figure 6 contains Indentation-\nErrors, RecursionErrors, UnboundLocalErrors, RuntimeEr-\nror, etc. These types of errors rarely occur in our AdapT\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n442\nFigure 5: Quantitative analysis of the two hyperparameters (a, b) in AdapT sampling. The upper half shows the results vary b\nfrom 0.1 to 0.9 when fixing a = 0.8. The bottom half shows the results of fixing b = 0.3 and taking values of a from 0.1 to 0.9.\nWhen a > b, the AdapT sampling can outperform SP with multiple settings. The model and dataset used in this section are\nCodeGeeX-13B and HumanEval.\nFigure 6: Evaluation of the generated code quality of AdapT\nsampling. Compared with the best-performed SP, AdapT\nsampling can reduce syntax errors and increase correct code\nsample numbers.\nsampling. The reason for this is that AdapT sampling uses\na smaller T inside the code block, which improves the co-\nherence of the sampled code, and therefore reduces syntax\nerrors. For AdapT sampling, the most common error type is\nthe wrong answer, showing that our method suffers from in-\ncorrect code logic. Taking steps to solve this issue can be a\ngreat improvement in the future.\nFuture Work\nIn this section, we discuss the remaining challenges of de-\ncoding strategies in code generation and provide some pos-\nsible directions to facilitate other researchers.\n• We recognize some challenging tokens within state-\nments, but their distributions do not show a clear statis-\ntical pattern. In the method designing process, we have\nexperimented with various temperature tuning functions,\nsuch as linear decay function, exponential decay func-\ntion, etc., without obtaining substantial improvement. In\nfuture work, we plan to use learning-based methods to\nadjust the temperature coefficients.\n• In practice, software development often relies on spe-\ncific domain knowledge, such as private code libraries\nand code specifications. Existing decoding strategies ig-\nnore these issues. In the future, we can introduce domain\nknowledge into the decoding process, improving the us-\nability of code-generation LLMs in real-world scenarios.\n• As shown in Figure 6, LLMs are suffering from incor-\nrect code logic, and generating code from scratch is very\nchallenging. In the future, we can design a multi-stage\ndecoding strategy, which steers LLMs to generate code\nprogressively. For example, LLMs first generate a natural\nlanguage plan and then generate an executable program\nbased on the plan.\nConclusion\nThis paper is the first attempt at the LLM’s decoding strat-\negy for code generation. We statistically analyze the loss dis-\ntribution of source code and find out that code tokens can\nbe categorized into challenging tokens and confident tokens.\nMoreover, challenging tokens often appear in the initial of a\ncode block. Based on the insights, we propose AdapT sam-\npling which dynamically adjusts the temperature coefficient\nthrough sampling and has proven its effectiveness on code\ngeneration datasets. Finally, we present several challenges\nand insights in developing a more advanced decoding strat-\negy for code generation and we look forward to further ex-\nploring its potential in future research.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n443\nAcknowledgements\nThis work is supported by the National Natural Sci-\nence Foundation of China under Grant Nos.62192731,\n62072007, 62192733, 61832009, 62192730, and\n62332012, the National Key R&D Program under Grant\nNo.2023YFB4503801, and the Key Program of Hubei\nunder Grant JD2023008.\nReferences\nAckley, D. H.; Hinton, G. E.; and Sejnowski, T. J. 1985. A\nlearning algorithm for Boltzmann machines. Cognitive sci-\nence, 9(1): 147–169.\nAustin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski,\nH.; Dohan, D.; Jiang, E.; Cai, C.; Terry, M.; Le, Q.; et al.\n2021. Program Synthesis with Large Language Models.\nBlack; and E, P. 2012. greedy algorithm, Dictionary of Al-\ngorithms and Data Structures. US Nat. Inst. Std. & Tech\nReport, 88: 95.\nBland, J. M.; and Altman, D. G. 1996. Measurement error.\nBMJ: British medical journal, 312(7047): 1654.\nBrown, P. F.; Della Pietra, S. A.; Della Pietra, V . J.; Lai, J. C.;\nand Mercer, R. L. 1992. An estimate of an upper bound for\nthe entropy of English. Computational Linguistics, 18(1):\n31–40.\nBrown, S. 2011. Measures of shape: Skewness and kurtosis.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. CoRR, abs/2005.14165.\nBrownlee, J. 2019. Probability for machine learning: Dis-\ncover how to harness uncertainty with Python. Machine\nLearning Mastery.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; de Oliveira Pinto,\nH. P.; Kaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brock-\nman, G.; Ray, A.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf,\nH.; Sastry, G.; Mishkin, P.; Chan, B.; Gray, S.; Ryder, N.;\nPavlov, M.; Power, A.; Kaiser, L.; Bavarian, M.; Winter, C.;\nTillet, P.; Such, F. P.; Cummings, D.; Plappert, M.; Chantzis,\nF.; Barnes, E.; Herbert-V oss, A.; Guss, W. H.; Nichol, A.;\nPaino, A.; Tezak, N.; Tang, J.; Babuschkin, I.; Balaji, S.;\nJain, S.; Saunders, W.; Hesse, C.; Carr, A. N.; Leike, J.;\nAchiam, J.; Misra, V .; Morikawa, E.; Radford, A.; Knight,\nM.; Brundage, M.; Murati, M.; Mayer, K.; Welinder, P.; Mc-\nGrew, B.; Amodei, D.; McCandlish, S.; Sutskever, I.; and\nZaremba, W. 2021. Evaluating Large Language Models\nTrained on Code. CoRR, abs/2107.03374.\nFan, A.; Lewis, M.; and Dauphin, Y . 2018. Hierarchical\nNeural Story Generation. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 889–898. Melbourne, Australia:\nAssociation for Computational Linguistics.\nFreitag, M.; and Al-Onaizan, Y . 2017. Beam Search Strate-\ngies for Neural Machine Translation. In Luong, T.; Birch,\nA.; Neubig, G.; and Finch, A. M., eds., Proceedings of the\nFirst Workshop on Neural Machine Translation, NMT@ACL\n2017, Vancouver, Canada, August 4, 2017, 56–60. Associa-\ntion for Computational Linguistics.\nFried, D.; Aghajanyan, A.; Lin, J.; Wang, S.; Wallace, E.;\nShi, F.; Zhong, R.; Yih, S.; Zettlemoyer, L.; and Lewis, M.\n2022. InCoder: A Generative Model for Code Infilling and\nSynthesis. In The Eleventh International Conference on\nLearning Representations.\nFried, D.; Aghajanyan, A.; Lin, J.; Wang, S.; Wallace, E.;\nShi, F.; Zhong, R.; Yih, S.; Zettlemoyer, L.; and Lewis, M.\n2023. InCoder: A Generative Model for Code Infilling and\nSynthesis. In The Eleventh International Conference on\nLearning Representations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net.\nHendrycks, D.; Basart, S.; Kadavath, S.; Mazeika, M.;\nArora, A.; Guo, E.; Burns, C.; Puranik, S.; He, H.; Song, D.;\nand Steinhardt, J. 2021. Measuring Coding Challenge Com-\npetence With APPS. In Vanschoren, J.; and Yeung, S., eds.,\nProceedings of the Neural Information Processing Systems\nTrack on Datasets and Benchmarks 1, NeurIPS Datasets and\nBenchmarks 2021, December 2021, virtual.\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2019. The Curious Case of Neural Text Degeneration. In\nInternational Conference on Learning Representations.\nLi, J.; Li, G.; Li, Z.; Jin, Z.; Hu, X.; Zhang, K.; and Fu, Z.\n2023a. CodeEditor: Learning to Edit Source Code with Pre-\nTrained Models. ACM Trans. Softw. Eng. Methodol.Just\nAccepted.\nLi, J.; Li, G.; Tao, C.; Zhang, H.; Liu, F.; and Jin, Z.\n2023b. Large Language Model-Aware In-Context Learning\nfor Code Generation. arXiv preprint arXiv:2310.09748.\nLi, J.; Li, Y .; Li, G.; and Jin, Z. 2023c. Structured Chain-\nof-Thought Prompting for Code Generation. arXiv preprint\narXiv:2305.06599.\nLi, J.; Li, Y .; Li, G.; Jin, Z.; Hao, Y .; and Hu, X. 2023d.\nSkCoder: A Sketch-based Approach for Automatic Code\nGeneration. In 45th IEEE/ACM International Conference\non Software Engineering, ICSE 2023, Melbourne, Australia,\nMay 14-20, 2023, 2124–2135. IEEE.\nLi, J.; Zhao, Y .; Li, Y .; Li, G.; and Jin, Z. 2023e. AceCoder:\nUtilizing Existing Code to Enhance Code Generation. arXiv\npreprint arXiv:2303.17780.\nLi, Y .; Choi, D.; Chung, J.; Kushman, N.; Schrittwieser, J.;\nLeblond, R.; Eccles, T.; Keeling, J.; Gimeno, F.; Dal Lago,\nA.; et al. 2022. Competition-level code generation with al-\nphacode. Science, 378(6624): 1092–1097.\nMou, L.; Men, R.; Li, G.; Zhang, L.; and Jin, Z. 2015.\nOn End-to-End Program Generation from User Intention by\nDeep Neural Networks. CoRR, abs/1510.07211.\nNijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou,\nY .; Savarese, S.; and Xiong, C. 2022a. CodeGen: An Open\nLarge Language Model for Code with Multi-Turn Program\nSynthesis. In The Eleventh International Conference on\nLearning Representations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n444\nNijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou,\nY .; Savarese, S.; and Xiong, C. 2022b. A Conversational\nParadigm for Program Synthesis. CoRR, abs/2203.13474.\nRunnenburg, J. T. 1978. Mean, median, mode. Statistica\nNeerlandica, 32(2): 73–79.\nSee, A.; Pappu, A.; Saxena, R.; Yerukola, A.; and Man-\nning, C. D. 2019. Do Massively Pretrained Language Mod-\nels Make Better Storytellers? In Proceedings of the 23rd\nConference on Computational Natural Language Learn-\ning (CoNLL), 843–861. Hong Kong, China: Association for\nComputational Linguistics.\nZhang, X.; Sun, M.; Liu, J.; and Li, X. 2021. Improving\nDiversity of Neural Text Generation via Inverse Probability\nWeighting. CoRR, abs/2103.07649.\nZheng, Q.; Xia, X.; Zou, X.; Dong, Y .; Wang, S.; Xue, Y .;\nWang, Z.; Shen, L.; Wang, A.; Li, Y .; Su, T.; Yang, Z.; and\nTang, J. 2023. CodeGeeX: A Pre-Trained Model for Code\nGeneration with Multilingual Evaluations on HumanEval-X.\nCoRR, abs/2303.17568.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n445"
}