{
  "title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
  "url": "https://openalex.org/W4389523995",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2634442158",
      "name": "Patrick Fernandes",
      "affiliations": [
        "Carnegie Mellon University",
        "Instituto Superior Técnico",
        "Instituto de Telecomunicações"
      ]
    },
    {
      "id": "https://openalex.org/A2342646302",
      "name": "Daniel Deutsch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5024699085",
      "name": "Mara Finkelstein",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2588096397",
      "name": "Parker Riley",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122534586",
      "name": "André Martins",
      "affiliations": [
        "Instituto Superior Técnico",
        "Instituto de Telecomunicações"
      ]
    },
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": [
        "Inspire",
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2122184959",
      "name": "Ankush Garg",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965776567",
      "name": "Jonathan Clark",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111277974",
      "name": "Markus Freitag",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1941184757",
      "name": "Orhan Firat",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W4389518754",
    "https://openalex.org/W4324134461",
    "https://openalex.org/W4385571776",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4390035079",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W3186081172",
    "https://openalex.org/W4389524038",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W1654441844",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2970791445",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3119881489",
    "https://openalex.org/W4379474731",
    "https://openalex.org/W3159892921",
    "https://openalex.org/W4297833460",
    "https://openalex.org/W2750811310",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2971120958"
  ],
  "abstract": "Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, Orhan Firat. Proceedings of the Eighth Conference on Machine Translation. 2023.",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 1066–1083\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n1066\nThe Devil is in the Errors: Leveraging Large Language Models for\nFine-grained Machine Translation Evaluation\nPatrick Fernandes∗,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1\nAndré F. T. Martins3,4,5 Graham Neubig2,6\nAnkush Garg1 Jonathan H. Clark1 Markus Freitag1 Orhan Firat1\n1Google 2Carnegie Mellon University 3Instituto Superior Técnico\n4Instituto de Telecomunicações 5Unbabel 6Inspired Cognition\npfernand@cs.cmu.edu\nAbstract\nAutomatic evaluation of machine translation\n(MT) is a critical tool driving the rapid itera-\ntive development of MT systems. While con-\nsiderable progress has been made on estimat-\ning a single scalar quality score, current met-\nrics lack the informativeness of more detailed\nschemes that annotate individual errors, such\nas Multidimensional Quality Metrics (MQM).\nIn this paper, we help ﬁll this gap by proposing\nAUTO MQM, a prompting technique which\nleverages the reasoning and in-context learning\ncapabilities of large language models (LLMs)\nand asks them to identify and categorize errors\nin translations. We start by evaluating recent\nLLMs, such as PaLM and PaLM-2, through\nsimple score prediction prompting, and we\nstudy the impact of labeled data through in-\ncontext learning and ﬁnetuning. We then eval-\nuate AUTO MQM with PaLM-2 models, and\nwe ﬁnd that it improves performance compared\nto just prompting for scores (with particularly\nlarge gains for larger models) while providing\ninterpretability through error spans that align\nwith human annotations.\n1 Introduction\nEvaluating natural language generation systems has\nalways been challenging, and as the output qual-\nity of these systems has improved, evaluation has\nbecome even more challenging and critical. For ex-\nample, in Machine Translation (MT), a ﬁeld where\nevaluation has garnered considerable attention, pre-\nvious standard automatic surface-level metrics such\nas BLEU (Papineni et al., 2002) are becoming less\nreliable as the quality of generation systems im-\nproves, with little remaining correlation with hu-\nman judgments (Freitag et al., 2022).\nTo keep pace with the constantly improving qual-\nity of MT output, the next generation of automatic\nmetrics is rapidly evolving. Learned automatic\nmetrics that leverage human-judgments to ﬁnetune\n∗ Work done while working part-time at Google.\nSource: “Avaliar tradução \nautomática é difícil.”\nCandidate: “Evaluating \nautomatic translation are easy.”\nScore the following translation from 0 to 100:\nPortuguese: {source}; English:{candidate}\nIdentify the errors in the translation\nPortuguese: {source}; English:{candidate}\nScore: 25\nErrors: ‘easy’ - major/accuracy; ‘are’ - minor/ﬂuency\nMQM Score: -5x1(major) - 1x1(minor) = -6\nAᴜᴛᴏMQM\nScore Prediction\nFigure 1: Illustration of how AUTO MQM uses LLMs\nto assess the quality of a translation. Rather than asking\nfor a single quality score, AUTO MQM prompts mod-\nels to identify and classify errors, and uses the MQM\nframework to produce a score.\nlanguage models (Sellam et al., 2020; Rei et al.,\n2022a) currently represent the state-of-the-art in au-\ntomatic evaluation benchmarks like the WMT Met-\nrics task (Freitag et al., 2022), and show high corre-\nlation with human judgments. However, these met-\nrics typically output a single, uninterpretable qual-\nity score, making it difﬁcult to understand the type\nand extent of errors identiﬁed by them. The lack of\ninsights makes it difﬁcult for model developers to\nleverage these metrics to improve their systems.\nUnlike automatic metrics that only provide a\nsingle scalar value as quality score, state-of-the-art\nhuman evaluation methodologies like Multidi-\nmensional Quality Metrics (MQM; Lommel\net al., 2014; Freitag et al., 2021a) ask professional\nannotators to identify and label error spans with\na category and severity. This much richer feedback\ncan be used to gain a better understanding of the\ncurrent limitations of the model under evaluation\nand improve it.\nIn this paper, we ask whether large language\n1067\nmodels (LLMs) in combination with a few human\nannotations can be used to design an automatic\nmetric that generates rich feedback similar to that\ngenerated by human experts in MQM. This work\nis motivated by recent papers that demonstrated\nthat LLMs can be used as automatic metrics (Liu\net al., 2023b) to generate a single quality score. In\nparticular, Kocmi and Federmann (2023) showed\nthat LLMs can be prompted to assess the quality\nof machine-generated translations, even achieving\nstate-of-the-art performance on assessing system-\nlevel quality. However, previous work only pro-\nvides a limited view of the capabilities of LLMs for\nmachine translation evaluation: the focus has pre-\ndominantly been on score prediction (i.e. predict-\ning a numerical value for quality), without consid-\nering the use of any annotated data (either through\nin-context learning or ﬁnetuning), and only in high-\nresource language pairs.\nWe provide a large-scale study of the capabilities\nof LLMs (from the PaLM and PaLM-2 families;\nChowdhery et al., 2022; Anil et al., 2023) for ma-\nchine translation evaluation (both with and without\na reference translation), provide a novel compari-\nson between prompting and ﬁnetuning, and investi-\ngate the performance in the low-resource scenario.\nInspired by ﬁndings that the performance of LLMs\ncan be improved by prompting them for rationales\nof their predictions (Wei et al., 2022; Lu et al.,\n2023), we also propose AUTO MQM, a prompt-\ning technique for MT evaluation that asks LLMs\nto identify error spans in a translation and to clas-\nsify these errors according to the MQM framework,\nwith a quality score derived automatically from the\nidentiﬁed errors. A key advantage of AUTO MQM\nis its interpretability, as users can inspect the errors\nresponsible for a score (Figure 1).\nOur contributions can be summarized as follows:\n•We conﬁrm the ﬁnding of Kocmi and Feder-\nmann (2023) that LLMs are zero-shot state-of-\nthe-art system-level evaluators, but show low\ncorrelation with human judgment compared\nto learned metrics at the segment-level.\n•We show that ﬁnetuning an LLM with hu-\nman judgment mitigates its low segment-level\nperformance (particularly for smaller LLMs),\nshowing similar correlations with human judg-\nment at both the system-level and segment-\nlevel to state-of-the-art learned metrics.\n•We are the ﬁrst to evaluate LLM-based evalu-\nation methods on low-resource language pairs.\nWe ﬁnd that their performance is promising,\nbut lags behind state-of-the-art learned met-\nrics.\n•We ﬁnd that, with AUTO MQM, PaLM-2 mod-\nels can be prompted to generate rich MQM-\nlike annotations, outperforming their score\nprediction counterparts at the segment-level.\n•Furthermore, annotations predicted by PaLM-\n2 models correctly identify over 50% of words\nthat are part of major errors, and are compa-\nrable to the ones produced by state-of-the-art\nsupervised word-level evaluators.\nOur ﬁndings might have signiﬁcant implica-\ntions for not only MT evaluation, but evaluation\nof machine-generated text in general, and further\nhighlight the potential of using LLMs to provide\nAI Feedback (Fernandes et al., 2023).\nThe outputs of our models prompted with\nAUTO MQM are available at github.com/google-\nresearch/google-research\n2 Background: MT Evaluation\nMachine translation evaluation is one of the most\nwell-studied evaluation problems in NLP (Callison-\nBurch et al., 2008; Freitag et al., 2022). In this task,\ngiven\n1. a source sentence in a (source) language\n2. a candidate translation in a (target) language\nan evaluation metric assesses the quality of the\ncandidate translation by how well it conveys the\nmeaning of the source sentence while considering\nother factors like ﬂuency. Like many other natu-\nral language generation evaluation problems, this\ntask is difﬁcult because the set of correct transla-\ntions for a given source sentence is often very large\nand not entirely known in advance. To simplify\nthe problem of machine translation evaluation, of-\nten (3) a reference translation (typically created\nby a professional human translator) is included as\nadditional information when assessing the candi-\ndate translation. This sub-problem is known as\nreference-based evaluation (as opposed reference-\nless evaluation or quality estimation).\nUp until recently, human evaluation of machine\ntranslation was carried out predominantly with the\naim of assigning a single quality score to a can-\ndidate translation. Consequently, learned metrics,\nwhich leverage collected human judgment data, are\ntrained for and evaluated on the same task of score\n1068\nprediction (i.e., assigning a single quality score to\na candidate translation), and can achieve high cor-\nrelation with human-provided scores (Freitag et al.,\n2022).\nHowever, framing machine translation evalu-\nation as a score prediction task is problematic:\nany scoring or ranking of translations is implicitly\nbased on an identiﬁcation of errors in the candidate\ntranslations, and asking raters to solely provide a\nsingle score can lead to rushed and noisy judgments\n(Freitag et al., 2021a).\nThis insight has led to the adoption of the\nMultidimensional Quality Metrics (MQM) frame-\nwork (Lommel et al., 2014; Freitag et al., 2021a)\nas the gold standard for evaluating machine transla-\ntion. The MQM framework asks human evaluators\nto identify error spans in candidate translations and\nclassify those errors according to various dimen-\nsions, e.g., ﬂuency, accuracy, ... (see Appendix A\nfor a more detailed description of MQM). Impor-\ntantly, the MQM framework does not ask anno-\ntators to provide a quality score for each transla-\ntion, and instead derives one automatically from\nthe identiﬁed error spans and their classiﬁcations.\nHowever, despite its richness, most automatic met-\nrics that leverage MQM data only use the ﬁnal qual-\nity score produced by the framework and discard\nthe error span information and classiﬁcation.\n3 Related Work\nThe success of learned machine translation met-\nrics (Sellam et al., 2020; Rei et al., 2022a; Freitag\net al., 2022; Qin et al., 2022), which ﬁnetune neu-\nral network models pretrained on large amounts of\n(unsupervised) data, highlighted the importance of\nleveraging transfer learning to achieve metrics with\nbetter correlation with human judgments. More re-\ncently, generative LLMs (OpenAI, 2023; Anil et al.,\n2023) have consistently demonstrated impressive\nresults in natural language understanding and zero-\nand few-shot transfer and, naturally, interest in em-\nploying these models for (translation) evaluation\nhas increased. Kocmi and Federmann (2023) ﬁrst\nexplored the use of GPT models for evaluating\nmachine translation tasks, showing their potential\nas zero-shot evaluators, and others have since ex-\ntended GPT-based evaluation to other generation\nproblems (Jain et al., 2023; Liu et al., 2023b).\nPerrella et al. (2022) ﬁrst highlighted that MQM\nannotations could be leveraged to allow pretrained\nmodels to predict major and minor errors and, sim-\nilarly to AUTO MQM , used the identiﬁed errors\nto automatically score translations. However, their\napproach relied on weaker encoder-only or encoder-\ndecoder language models, required supervised data\nto work, and overall underperformed other top met-\nrics. We compare against their MaTASe metric in\nour experiments. Lu et al. (2023) showed that do-\ning error analysis, a prompting technique similar to\nAUTO MQM , could lead to better ChatGPT-based\nevaluators. However, they still relied on the LLM\nto provide a score once it identiﬁed errors (rather\nthan do it automatically using something like the\nMQM framework). Furthermore, they provided\na very limited meta-evaluation using only 40 ex-\namples per language pair. Concurrently with our\nwork, Xu et al. (2023) proposed INSTRUCT SCORE ,\na LLaMA-based evaluator that asks models to iden-\ntify and categorize errors in translation (as well as\nproviding a natural language explanation for each\nerror). However, the authors only explore a 7B\nparameter model and don’t leverage zero- and few-\nshot capabilities of models as in this work. Instead,\nthey rely on a more complex approach of distilling\nthe knowledge of a more capable GPT-4 LLM.\nAdditionally, WMT Word-Level Quality Esti-\nmation shared tasks (Fonseca et al., 2019; Zerva\net al., 2022) leverage MQM data by converting\nspan-level annotations of errors (normally of ma-\njor severity) to word-level tags and Task 2 in the\nWMT19 Quality Estimation shared task evaluation\nexplicitly evaluated submissions of span-level anno-\ntations (although most submissions still consisted\nof models that predicted word-level tags which\nwere converted to spans). We also compare against\nstate-of-the-art word-level quality estimation mod-\nels.\n4 Using LLMs to Predict Quality Scores\nRecent works have shown that large language mod-\nels are versatile, general-purpose models that can\nbe used to tackle many problems in NLP, includ-\ning evaluation (Kocmi and Federmann, 2023; Jain\net al., 2023; Liu et al., 2023b). We begin by explor-\ning how LLMs can be used for machine translation\nevaluation through score prediction.\n4.1 Prompting\nWe start by measuring how far we can push the\nperformance of LLMs with just prompting (Liu\net al., 2023a): by deﬁning the task of MT evaluation\nand quality estimation as textual templates (with\n1069\na general description of the problem and “slots”\nfor the inputs and outputs), we can use general-\npurpose LLMs to perform these tasks at inference-\ntime, without any parameter updates.\nThroughout the paper, we choose to use Kocmi\nand Federmann (2023)’s GEMBA-SQM prompt\n(Figure 9, Appendix C), which asks models to gen-\nerate (a string representation of) a score from 0-\n100. We choose this prompt for two reasons: ﬁrstly,\nearly explorations with various prompts showed\nthat this generally performed well. Secondly, us-\ning a single prompt ensures a fairer comparison\nbetween the capabilities of different models.1\nIn-Context Learning A surprising emergent ca-\npability of LLMs is their ability to improve on\nprompting-based tasks by including a very small\namount of labeled data as part of the prompt/con-\ntext (Brown et al., 2020) and without parameter up-\ndates, a technique called in-context learning (ICL)\nor few-shot prompting. We thus investigate the\nimpact that ICL has on LLMs’ ability to assess\ntranslation quality. Recent works have shown that\nthe impact of ICL is tightly tied with the exact\nexamples included in the prompt, with a poor selec-\ntion procedure leading to no improvements or even\nworse performance than the zero-shot case (Jain\net al., 2023). We therefore explore two sampling\napproaches to select in-context examples from a\npre-deﬁned “pool” of translation quality assess-\nments: uniform and stratiﬁed sampling, where\nthe example pool is bucketed by score ranges and\nexamples are sampled from each bucket.\n4.2 Finetuning\nIt has previously been shown that LLMs are capa-\nble of zero-shot evaluation (Kocmi and Federmann,\n2023), but the extent to which ﬁnetuning on human\njudgment data can further boost the performance of\nLLMs has not been studied. In the WMT’22 Met-\nrics Shared Task (Freitag et al., 2022), all top sub-\nmissions were learned metrics; that is, pretrained\nmodels ﬁnetuned on human judgment data2.\nThus, we investigate whether LLMs are\namenable to ﬁnetuning on human judgment data.\nLLMs used in top-performing metrics are gener-\nally much larger than the pretrained language mod-\nels leveraged by previous learned metrics (which\n1While this prompt wasn’t the best forsystem-level, it led\nto the best segment-level performance in GEMBA.\n2While these metrics all leverage powerful pretrained (lan-\nguage) models, these generally aren’t considered LLMs\ngenerally have fewer than 1 billion parameters).\nMoreover, most learned metrics leverage pretrained\nencoder-only rather than (decoder-only) preﬁx lan-\nguage models. We experiment with ﬁnetuning\nLLMs using two objectives:\n•Regression (R): Commonly used for training\nlearned metrics (Rei et al., 2022a), the ob-\njective here is a regression loss (e.g., mean\nsquared error) between continuous scores ob-\ntained from the model (for example, with a\nregression head) and the human scores.\n•Generative Classiﬁcation (GC): We bucket\nscores into discrete classes (e.g. \"bad\", \"ok\"\nand \"good\") and treat the MT evaluation task\nas a text-to-text classiﬁcation problem (Raffel\net al., 2020) by having the model generate a\ntemplate sentence with the class. See §6.1 for\nmore details.\n5 Using LLMs to Predict Error Spans\nWhile producing quality scores that correlate with\nhuman judgments is an important part of transla-\ntion quality assessment, metrics that solely do score\nprediction suffer from problems of interpretabil-\nity: if a metric assigns a low score, the downstream\nusers are left in the dark about which parts of the\ntranslation were responsible for the score and thus\nneed to be corrected. This is especially problematic\nin cases where the metric assigns a wrong score to\na translation, as it is much harder to diagnose why\nthe evaluation model made a mistake, and iden-\ntify and prevent similar mistakes in the future. In\nfact, reducing translation quality to a single score\nhas proven problematic even for human annotators:\nasking raters to solely provide a single score can\nlead to rushed and noisy judgments (Freitag et al.,\n2021a) and the current gold standard for transla-\ntion quality evaluation involving human annotators\nis instead based on methodologies like the MQM\nframework (see §2) , which provide richer feedback\nby identifying error spans, categorizing them, and\nevaluating their severity.\nInterestingly, another emergent phenomenon in\nLLMs is the success of chain-of-thought prompt-\ning (Wei et al., 2022): when deﬁning a prompt\nfor a particular task, if we instruct the model to\nproduce a series of intermediate reasoning steps\n(“let’s think step-by-step”), it tends to generate\na free-text rationale before generating an output,\nand this often improves the performance on the\n1070\nBased on the given source and reference, identify the major and minor errors in this\ntranslation. Note that Major errors refer to actual translation or grammatical errors,\nand Minor errors refer to smaller imperfections, and purely subjective opinions about\nthe translation.\n{src_lang} source: \"{source}\"\n{tgt_lang} human reference: \"{reference}\"\n{tgt_lang} translation: \"{candidate}\"\nErrors: {error1:span} - {error1:severity}/{error1:category}; {error2:span} - ...\nFigure 2: The AUTO MQM prompt used in this paper. Parts in purple are only included for reference-based\nevaluation, while parts in orange represent slots for outputs, and are only included for in-context examples.\ntask at hand (Liu et al., 2023b). Furthermore, this\nchain-of-thought prompting can be used to obtain\nstructured rationales from LLMs, and this can lead\nto better performance than with free-text rationales\n(Lu et al., 2023).\nMotivated by these ﬁndings, we propose\nAUTO MQM, a prompting technique for transla-\ntion quality assessment that instructs LLMs toiden-\ntify errors in a translation, andcategorize the type of\nerror according to the MQM framework (Lommel\net al., 2014). Furthermore, we don’task the model\nto produce a score, as the MQM framework pro-\nvides an algorithmic procedure to obtain one from\nidentiﬁed errors: the total score is the sum of penal-\nties for all errors identiﬁed, where (roughly) major\nerrors get penalized with −5 and minors with −1\n(see Appendix A for a more detailed description of\nthe scoring algorithm).3 Figure 2 shows the main\nAUTO MQM prompt used in this paper.\nImportantly, obtaining meaningful AUTO MQM\nresults in a zero-shot setting is a substantially more\nchallenging task compared to score prediction: we\nfound that, without any in-context examples, LLMs\ntend to produce outputs that are either uninforma-\ntive or difﬁcult to parse. Thus we only consider the\nAUTO MQM task in the few-shot scenario. Based\non the ﬁndings from §6.2, we explore the impact\nof in-context learning by sampling from the exam-\nple pool using stratiﬁed sampling extended with a\nset of rejection criteria (Appendix D), which en-\nsures that the example set has a balance between\nmajor and minor errors as well as diversity in the\ncategories of errors.\n6 Experiments\n6.1 Experimental Setup\nData The metrics in this work are evaluated on\nboth high-resource and low-resource language\n3This is similar to methods that leverage externalexecutors\nto improve the performance of LLMs (Gao et al., 2022)\npairs. The three high-resource language pairs come\nfrom the WMT’22 Metrics Shared Task (Freitag\net al., 2022): en →de, zh→en, and en →ru. The\nground-truth translation quality scores are derived\nfrom MQM ratings in which expert annotators\nmarked error spans in the translations with different\nseverity levels which are automatically converted\nto a numeric score (see §2). The four low-resource\nlanguage pairs come from the WMT’19 Metrics\nShared Task (Ma et al., 2019): en↔gu and en↔kk.\nSince MQM ratings are not available for the low-\nresource pairs, the ground truth quality scores are\ndirect assessment (DA) scores. DA scores are qual-\nity assessments assigned by non-expert raters on a\nscale from 0-100, normalized per rater. See Table 9\n(Appendix B) for statistics about the number of\nMT systems and segments for every language pair.\nAdditionally, in our experiments, AUTO MQM\nrequired in-context examples with MQM anno-\ntations to work, so we restrict our evaluation of\nAUTO MQM to en→de and zh→en because there\nare available MQM ratings from the WMT’21 Met-\nrics Shared Task (Freitag et al., 2021b) that we can\nuse as in-context learning example pools.\nModels We base most of our experiments on the\nfollowing LLMs:\n•PaLM: A 540 billion parameter autoregres-\nsive Transformer model trained on 780 billion\ntokens of high-quality text (Chowdhery et al.,\n2022). It showed remarkable performance on\na wide-range of NLP tasks, including Machine\nTranslation (Vilar et al., 2022).\n•PaLM-2: The successor to PaLM, the\nPaLM-2 family of LLMs (Anil et al., 2023)\nbuilds upon recent research insights, such as\ncompute-optimal scaling, a more multilingual\nand diverse pre-training mixture, and architec-\ntural/optimization improvements. We mainly\nuse two model sizes in the family: PaLM-2BI-\n1071\nSON and (the larger) PaLM-2-UNICORN .4 In\naddition we explore the impact of instruction-\ntuning by using a UNICORN model ﬁnetuned\non the FLAN dataset (Wei et al., 2021).\nFor score prediction, we compare PaLM and\nPaLM-2 against the GPT family of LLMs (Brown\net al., 2020; OpenAI, 2023) by leveraging the\nresults and outputs from the GEMBA evaluator\n(Kocmi and Federmann, 2023). We then evaluate\nthe performance of AUTO MQM with only PaLM-2\nmodels (which performed best in score prediction).\nAdditionally, for the high-resource languages,\nwe compare to a set of strong baseline eval-\nuation metrics, MetricX-XXL and COMET-22,\nwhich were the two top-performing metrics in the\nWMT’22 Metrics Shared Task. MetricX-XXL and\nCOMET-22 are both ﬁnetuned regression models\ntrained on DA data from WMT that are initialized\nwith mT5 (Xue et al., 2021) and XLM-R (Conneau\net al., 2020), respectively.\nFor the AUTO MQM experiments, we also com-\npare against MATES E, a comparable submission\nto the WMT’22 Metrics Shared task that ﬁnetuned\na XLM-R model to identify major and minor errors,\nand computed a score automatically. Since we were\nunable to obtain the span-level predictions for the\nMATES E submission, we also compare against the\ntop submission to the WMT’22 Word-Level Qual-\nity Estimation Shared Task (Zerva et al., 2021):\nword-level COMET KIWI (COMET-WL) (Rei et al.,\n2022b), also based on an XLM-R model trained on\na combination of sentence- and word-level data. To\ndo so, we re-run this model on the WMT’22 Met-\nrics Shared Task data, and convert the predicted\nword-level OK/BAD tags into spans.5\nFinetuning For regression ﬁnetuning, we use a\nreal-valued logit, extracted from a ﬁxed index in the\nﬁrst target token’s logit vector, as the quality signal.\n(In particular, we leverage a special,unused, vocab-\nulary token.) This was the technique used to train\nMetricX-XXL in the WMT 2022 Shared Task sub-\nmission (Freitag et al., 2022). The regression-based\nmodel was trained on WMT direct assessment (DA)\ndata from the years 2015 through 2020.\nFor generative classiﬁcation, we bucket the\nscores in the training data into ﬁve classes, where\n4Information about exact number of parameters of PaLM-2\nmodels is not publicly available.\n5We consider a span as any maximal consecutive sequence\nof words marked as BAD, assigning every span the major\nseverity.\nclass boundaries are assigned so that each class\ncontains an equal number of training examples. We\nthen map labels to verbal ratings from the follow-\ning set, based on their bucket: [\"very bad\", \"bad\",\n\"ok\", \"good\", \"very good\"]. To evaluate the model,\npredictions are mapped back to integer labels from\n1 to 5. Any predictions not containing a substring in\nthe label set are considered invalid and are mapped\nto 0. We experimented with ﬁnetuning on both DA\nand MQM 2020 (Freitag et al., 2021a) data, and\nfound that the latter performed slightly better.\nTo assess the impact of model size , we also\nﬁnetune two additional (smaller) PaLM-2 models,\nwhich we call Sand M, comparing their ﬁnetuned\nand zero-shot performance.6\nMetric Meta-Evaluation The quality of an au-\ntomatic evaluation metric is estimated by compar-\ning the agreement between the metric scores and\nground-truth quality scores on a large number of\ntranslations from different MT systems, a process\nknown as metric meta-evaluation. This work re-\nports three different agreement scores, as follows.\nThe ﬁrst is system-level accuracy, which calcu-\nlates the percent of system pairs that are ranked the\nsame by the metric and ground-truth scores, micro-\naveraged over a set of language pairs (Kocmi et al.,\n2021). System-level scores are deﬁned as the aver-\nage score across all segments.\nAt the segment-level, the standard correlation\nthat is reported by WMT is Kendall’sτ. However,\nrecent work pointed out problems with Kendall’sτ\nwith respect to ties (Deutsch et al., 2023). In short,\ndifferent variants of τ are inconsistent with respect\nto ties and even biased against metrics that predict\nties, as our metrics do in this work. Deutsch et al.\n(2023) recommend reporting a pairwise accuracy\nscore, which rewards metrics for correctly ranking\ntranslations as well as correctly predicting ties, in\ncombination with a tie calibration procedure that\nautomatically introduces ties into metric scores so\nthat the meta-evaluation is fairer. This accuracy\nscore, denoted acc∗, ranges between 0 and 1, and\na random metric would achieve 33% accuracy. We\nreport the “group-by-item” variant of the pairwise\naccuracy score from Deutsch et al. (2023) in ad-\ndition to Pearson’s ρ, a complementary signal to\nrank-based correlations that measure the strength of\nthe linear relationship between two variables (and\none of the standard correlations reported in WMT).\n6We use a small variation of the zero-shot prompt, asking\nmodels for scores from the same 5 buckets used in ﬁnetuning.\n1072\nSystem-Level Segment-Level\nAll (3 LPs) EN-DE ZH-EN EN-RU\nModel Ref? Accuracy ρ acc⋆ ρ acc⋆ ρ acc⋆\nBaselines\nMetricX-XXL \u0013 85.0% 0.549 61.1% 0.581 54.6% 0.495 60.6%\nCOMET-22 \u0013 83.9% 0.512 60.2% 0.585 54.1% 0.469 57.7%\nCOMET-QE \u0017 78.1% 0.419 56.3% 0.505 48.8% 0.439 53.4%\nPrompting\nPaLM 540B \u0013 90.1% 0.247 55.4% 0.255 48.5% 0.180 48.6%\nPaLM-2 BISON \u0013 88.7% 0.394 56.8% 0.322 49.3% 0.322 52.8%\nPaLM-2 UNICORN \u0013 90.1% 0.401 56.3% 0.349 51.1% 0.352 55.3%\nFLAN-PaLM-2 UNICORN \u0013 75.9% 0.197 55.6% 0.139 46.1% 0.198 52.0%\nPaLM 540B \u0017 84.3% 0.239 56.1% 0.270 43.1% 0.300 51.8%\nPaLM-2 BISON \u0017 85.0% 0.355 57.0% 0.299 48.6% 0.303 53.1%\nPaLM-2 UNICORN \u0017 84.3% 0.275 56.1% 0.252 48.3% 0.209 49.8%\nFLAN-PaLM-2 UNICORN \u0017 69.7% 0.116 54.6% 0.112 43.8% 0.156 47.8%\nFinetune\nPaLM-2 BISON (R) \u0013 88.0% 0.511 61.0% 0.459 51.5% 0.458 59.5%\nPaLM-2 BISON (GC) \u0013 86.1% 0.400 59.2% 0.444 49.3% 0.365 56.0%\nPaLM-2 UNICORN (R) \u0013 87.6% 0.508 61.1% 0.412 52.6% 0.460 60.4%\nPaLM 2 BISON (R) \u0017 87.6% 0.490 59.9% 0.439 53.4% 0.437 59.2%\nPaLM 2 BISON (GC) \u0017 86.1% 0.368 57.5% 0.420 47.3% 0.390 54.9%\nPaLM 2 UNICORN (GC) \u0017 86.1% 0.407 57.9% 0.402 45.6% 0.411 55.3%\nTable 1: Meta-evaluation results at system and segment-level for thehigh-resource language pairs. Finetuned (R)\nand (GC) represent the regressionand generative classiﬁcation objectives (§4.2). \u0013and \u0017 represent reference-based\nand reference-less metrics, respectively.\nSpan Meta-Evaluation Since AUTO MQM pro-\nvides not only scores but also the identiﬁed error\nspans, we can compare the predicted spans with the\nerrors marked by annotators in the MQM annota-\ntions. We evaluate quality of predicted spans using:\n(1) Span Precision (SP), which measures the over-\nlap of predicted spans and gold (annotated) spans;\nand (2) Major recall (MR), which captures the per-\ncentage of gold major errors that were predicted as\nerrors (either minor or major).\nMore formally, consider the set of ground truth\nspans S⋆, where each span consists of a sequence of\nwords, i.e., si = (w(a),w(a+1),···). Let S⋆\nmaj ⊆\nS⋆ be the subset containing only the major errors.\nGiven a span set S, we deﬁne its positional set\nP(S) as the set containing the positions of all the\nwords in every span in S. For example, assuming a\nspan si = (w(n),w(n+1),···) in Sstarts at the nth\nposition in the text, its corresponding positional set\nwill include the positions{n,n+1,...,n +len(si)−\n1}. Then for a set of predicted spans ˆS, SP and\nMR are deﬁned as:\nSP( ˆS) =|P( ˆS) ∩P(S⋆)|\n|P( ˆS)|\n(1)\nMR( ˆS) =\n|P( ˆS) ∩P(S⋆\nmaj)|\n|P(S⋆\nmaj)| (2)\nIntuitively, we care for overall precision (regard-\nless of severity) since we want to make sure pre-\ndicted errors tend to be marked by annotators as\nwell, but for recall we care mostly for major errors,\nas these have a larger impact on translation qual-\nity and are more critical to identify. Additionally,\nwe also report the (3) Matthews Correlation Coefﬁ-\ncient (MCC), one of the ofﬁcial metrics in the word-\nlevel quality estimation tasks (Zerva et al., 2022).\n6.2 Results\n6.2.1 Score Prediction\nTable 1 summarizes the meta-evaluation results, at\nthe system and segment level, for both the zero-shot\nprompting and ﬁnetuning settings.\nPrompting A ﬁrst observation is almost all zero-\nshot LLM evaluators have higher system-level per-\nformance than learned metrics (with and without\nreferences), with PaLM 540B and PaLM-2 UNI-\nCORN achieving the best performance. At the seg-\nment level, the story is more complicated: similarly\nto Kocmi et al. (2022), we ﬁnd that none of the\nLLMs we explored was able to consistently out-\nperform the baseline learned metrics. We see that\nPaLM-540B is a particularly poor reference-based\nevaluator, which is surprising given its system-level\nperformance. Unexpectedly, instruction-tuning\nwith FLAN seems to degrade performance, with\nFLAN-PaLM-2 UNICORN achieving poor perfor-\nmance at both the system and segment levels.7\nNevertheless, PaLM-2 models achieve high cor-\nrelations with human judgments, and the reference-\n7Note that this might be a problem with the FLAN dataset\nand not instruction-tuning in general, as the GPT models are\nalso instruction-tuned and perform well.\n1073\nSystem Segment acc⋆\nModel Ref? All EN-DE ZH-EN EN-RU\nGEMBA\nGPT-3.5 \u0013 85.4% 54.9% 49.5% 47.5%\nGPT-4 \u0013 88.7% 57.8% 52.6% 55.0%\nGPT-3.5 \u0017 82.5% 56.1% 49.7% 49.3%\nGPT-4 \u0017 89.1% 56.4% 53.4% 54.8%\nBISON \u0013 88.7% 56.8% 49.3% 52.8%\nUNICORN \u0013 90.1% 56.3% 51.1% 55.3%\nBISON \u0017 85.0% 57.0% 48.6% 53.1%\nUNICORN \u0017 84.3% 56.1% 48.3% 49.8%\nTable 2: Comparison between PaLM-2 and GPT-based\nGEMBA (Kocmi et al., 2022) at the system and segment levels\nfor the high-resource language pairs.\nless PaLM-2 BISON is competitive with thelearned\nbaselines, particularly at assessing alternative trans-\nlations of the same sentence ( acc∗). When com-\nparing PaLM-2 models with Kocmi et al. (2022)’s\nGPT-based GEMBA evaluator (Table 2), we see\nthat both families of LLMs perform similarly,\nwith PaLM-2 models exhibiting higher system-\nlevel performance than GPT-based GEMBA, while\nGEMBA achieves better segment-level accuracy,\nparticularly in the reference-less setting.\nFigure 3: Distribution of scores for various LLM reference-\nbased evaluators, on the EN-DE test set. Note that the yaxis\nis in log-scale.\nFigure 3 shows the distribution of scores pro-\nduced by PaLM- and PaLM-2-based evaluators.\nWe ﬁnd that, despite being prompted to give a score\nin the 0-100 range, these models almost always out-\nput one of a very limited set of scores (e.g. 0, 50,\n90, 95). Given Kocmi and Federmann (2023)’s\nsimilar ﬁndings with GPT models, it seems that\nthis is a consequence of the pretraining objective.\nFinetuning Despite their already-great perfor-\nmance in the zero-shot setting, we ﬁnd that ﬁne-\ntuning LLMs can further improve LLM evaluators’\nsegment-level scores. This is particularly obvious\nfor the reference-less evaluators, where a ﬁnetuned\nPaLM-2 BISON achieves state-of-the-art perfor-\nmance in segment-level correlations and compa-\nrable system-level accuracy across all language\npairs. Moreover, when we look at how perfor-\nmance scales with parameter count (Figure 4), we\nobserve an interesting trend: while smaller models\nare not capable of being effective zero-shot evalu-\nators, ﬁnetuning them leads to competitive perfor-\nmance, and only a slight decrease when compared\nto their larger ﬁnetuned counterparts.\nS M Bison\nModel\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Pearson (EN-DE)\nZero-Shot\nFinetune\nFigure 4: Behavior of Pearson as we scale the LLM’s param-\neter count. Note that the xaxis is not to-scale with regard to\nparameter count.\nIn-context Learning Figure 5 shows the mean\nand interquartile range (IQR) of the performance\nas we increase the number of in-context examples\nk(with 100 example sets per k) sampled with strat-\niﬁed sampling (see Appendix E for uniform). Sur-\nprisingly, despite evidence of the beneﬁts of in-\ncontext learning for many tasks, we found that\nincluding in-context examples during evaluation\n(almost) never led to better performance, either\nwith uniform or stratiﬁed sampling.\n0 1 2 3 4\n# of in-context examples\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Pearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\nFigure 5: Mean Pearson and its interquartile range (IQR) in\nthe WMT22 EN-DE test set, as we increase the number of\nin-context examples with stratiﬁed sampling\nTo investigate the cause of this disappointing per-\nformance, we looked at how particular in-context\nexample sets affect the distribution of scores pro-\nduced by LLM-based evaluators. Figure 6 shows\nthe distribution of scores over the whole test set\nfor the 1-shot and 2-shot settings, with different\nin-context examples sets. We can see that output\ndistribution is heavily biased by the scores in the\nin-context examples: despite never predicting 79\n1074\nFigure 6: Distribution of scores for PaLM-2 (BISON ) models\nfor 1-shot (top) and 2-shot (bottom) setups, with various in-\ncontext learning sets for each (and their scores in the legend)\nin the zero-shot setting, when a single example\nwith that score is included, it starts to dominate\nthe model predictions. This seems to hint that\nLLMs “overﬁt” to the speciﬁc scores provided as\nexamples, rather than generalizing to the broader\nevaluation task, which could explain the lackluster\nperformance of in-context learning.\n6.3 Low Resource Languages\nTable 3 shows the performance of PaLM-2 mod-\nels at score prediction for low-resource transla-\ntion. Overall, we ﬁnd that similar to high-resource\nLPs, these models are good zero-shot evaluators,\nwith system-level accuracies around 90%. How-\never, zero-shot LLMs underperform learned met-\nrics, even when these metrics also weren’t exposed\nto data in these low-resource languages.\nSystem Segment ρ\nModel Ref? All EN-KK EN-GU KK-EN GU-EN\nBaseline\nMetricX-XXL⋆ \u0013 94.0% 0.666 0.701 0.539 0.409\nPrompting\nBISON \u0013 92.2% 0.605 0.540 0.462 0.339\nUNICORN \u0013 87.4% 0.609 0.621 0.495 0.384\nBISON \u0017 89.8% 0.5670.4780.3810.313\nUNICORN \u0017 84.4% 0.5360.5230.4330.334\nTable 3: Meta-evaluation results for system-level accuracy\nand segment-level Pearson on the low-resource languages,\nusing PaLM-2 for score prediction. ⋆Note that the baseline is\nslightly different from the high-resource case, being trained on\nthe same data but without these low-resource language pairs.\n6.3.1 A UTO MQM\nFigure 14 shows the mean and interquartile range\n(IQR) of the performance of PaLM-2 BISON with\nAUTO MQM , as we increase the number of in-\ncontext examples (again, with 100 example sets per\nk). Contrary to the performance with score predic-\ntion, we ﬁnd that performance with AUTO MQM\nseems to (mostly) scale with the number of in-\ncontext examples: performance increases monoton-\nically with up to 4 in-context examples and plateaus\nthereafter. Additionally, the variance across the in-\ncontext learning sets seems to be lower, with most\nexample sets exhibiting less than 0.05 Pearson dif-\nference from the best-performing sets. All this sug-\ngests that LLM evaluators are much more robust to\nthe choice of in-context examples when prompted\nfor AUTO MQM rather than for score prediction.\nWe also ﬁnd that the behavior of in-context learn-\ning is quite similar for both reference-based and\nreference-less evaluation tasks. Finally, we observe\nthat the example sets that perform well for one task\ngenerally work well for the other, with performance\non both settings given a ﬁxed in-context set being\nhighly correlated, as shown in Figure 7.\n0.0 0.1 0.2 0.3\nPearson w/o Reference\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Pearson with Reference\nCorr = 0.941\n1\n2\n3\n4\n5\n6\nNumber of Examples\nFigure 7: Scatter plot of the Pearson of PaLM-2 (BISON )\nmodels, with/without including the reference in the prompt,\nfor each in-context learning setting tried.\nTable 4 shows the meta-evaluation results for\nPaLM-2 BISON and UNICORN prompted with\nAUTO MQM (using the best-performing in-context\nlearning sets in Figure 14). For ease of comparison,\nwe also report their performance when prompted\nfor score prediction, as well as the performance\nof the baselines. Overall, prompting LLMs with\nAUTO MQM seems to lead to signiﬁcant improve-\nments in evaluating machine translation quality,\nparticularly for larger models: UNICORN achieves\nbetter performance (across all meta evaluations)\nwith it than when prompted for score prediction,\n1075\nSystem-Level Segment-Level\nAll (2 LPs) EN-DE ZH-EN\nModel Ref? Accuracy ρ acc⋆ ρ acc⋆\nBaselines\nMetricX-XXL \u0013 81.1% 0.549 61.1% 0.581 54.6%\nMATES E \u0013 79.9% 0.391 58.8% 0.528 51.5%\nCOMET-QE \u0017 76.9% 0.419 56.3% 0.505 48.8%\nMATES E-QE \u0017 73.4% 0.298 57.9% 0.468 50.1%\nCOMET-WL \u0017 71.6% 0.418 57.1% 0.406 51.5%\nScore Prediction\nPaLM-2 BISON \u0013 86.4% 0.394 56.8% 0.322 49.3%\nPaLM-2 UNICORN \u0013 86.4% 0.401 56.3% 0.349 51.1%\nPaLM-2 BISON \u0017 84.0% 0.355 57.0% 0.299 48.6%\nPaLM-2 UNICORN \u0017 80.5% 0.275 56.1% 0.252 48.3%\nAutoMQM\nPaLM-2 BISON \u0013 84.0% 0.369 59.2% 0.355 48.4%\nPaLM-2 UNICORN \u0013 87.6% 0.432 59.1% 0.442 51.8%\nPaLM 2 BISON \u0017 87.6% 0.297 55.2% 0.331 48.0%\nPaLM 2 UNICORN \u0017 83.4% 0.368 56.4% 0.429 50.2%\nTable 4: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and\nsegment levels for multiple language pairs.\nand its reference-less version is competitive with\nthe best learned metric even at the segment level.\nHowever, for the smaller BISON , the beneﬁts of\nAUTO MQM are less clear, with both techniques\nperforming comparably. This hints that scale is\nnecessary for zero- and few- shot ﬁne-grained evalu-\nation (like with AUTO MQM). We also ﬁnd that the\ndistribution of scores produced by LLMs prompted\nwith AUTO MQM is much closer to the gold MQM\ndistribution, with models outputting a much larger\nset of scores, and in the same ranges as annotators\ndo (see Figure 8).\nFigure 8: Distribution of scores for PaLM-2 models using\nAUTO MQM, on WMT22 EN-DE\nFinally, when evaluating the error spans pro-\nduced by LLMs prompted with AUTO MQM (Ta-\nble 5), we ﬁnd that PaLM-2 models are able to\nidentify most of the major errors. However, it does\nseem to over-predict errors (with errors predicted\nby UNICORN having on average∼5 words per span\nvs ∼2 words in the ground truth) and have overall\nEN-DE ZH-EN\nModel R? SP MR MCC SP MR MCC\nBaselines\nCOMET-WL\u0017 0.2670.2500.1610.3640.1780.152\nAutoMQM\nBISON \u0013 0.095 0.749 0.060 0.252 0.255 0.109\nUNICORN \u0013 0.175 0.628 0.193 0.238 0.476 0.143\nBISON \u0017 0.1190.5200.0920.2240.3110.091\nUNICORN \u0017 0.1500.5800.1500.2290.4880.133\nTable 5: Span-level meta-evaluation on WMT22 for PaLM-2\nmodels using AutoMQM. SR and MR represent span precision\nand major recall, respectively.\nlow span precision. Similarly to overall score cor-\nrelations, scale also seems to be important for the\nquality of spans produced by AUTO MQM , with\nUNICORN outperforming BISON at most metrics.\nAdditionally, UNICORN prompted with AutoMQM\npredicts spans of comparable quality to the ones\nproduced by current state-of-the-art learned word-\nlevel evaluators (trained on a considerable number\nof ﬁne-grained annotations derived from MQM):\nwhile word-level models are more precise, their\noverall span correlation (MCC) is comparable, and\nthey miss considerably more major errors than\nLLMs (despite only leveraging a handful of an-\nnotations).\n7 Conclusion\nIn this study, we have systematically investi-\ngated the capabilities of large language models\nfor machine translation evaluation through score\nprediction, and proposed AUTO MQM , a novel\n1076\nprompting technique that leverages the Multidi-\nmensional Quality Metrics (MQM) framework for\ninterpretable MT evaluation using LLMs.\nWe demonstrated that just prompting LLMs for\nscore prediction leads to state-of-the-art system-\nlevel evaluators, but still falls short of the best\nlearned metrics at the segment-level (with ﬁne-\ntuning being necessary to close this gap). Then\nwe showed that AUTO MQM can further improve\nthe performance of LLMs without ﬁnetuning while\nproviding interpretability through error spans that\nalign with human annotations.\nOur ﬁndings surrounding ﬁnetuning LLMs for\nscore prediction hint that LLMs’ performance in\nmachine translation evaluation could be further im-\nproved by ﬁnetuning these models on ﬁne-grained\nhuman judgment data (like MQM) and is a direc-\ntion we are actively pursuing. Additionally, the\ngeneral-purpose nature of LLMs may enable the\napplication of similar prompting techniques (lever-\naging some ﬁne-grained evaluation schemes) to\nother evaluation problems (Wu et al., 2023).\nAcknowledgements\nWe would like to thank Ricardo Rei, Marcos Tre-\nviso and Chryssa Zerva for helping run the word-\nlevel QE baselines, and George Foster who pro-\nvided feedback on an earlier version of this work.\nThis work was partially supported by EU’s Horizon\nEurope Research and Innovation Actions (UTTER,\ncontract 101070631), by the project DECOLLAGE\n(ERC-2022-CoG 101088763), by the Portuguese\nRecovery and Resilience Plan through project\nC645008882- 00000055 (Center for Responsible\nAI), and the Fundação para a Ciência e Tecnolo-\ngia through contracts SFRH/BD/150706/2020 and\nUIDB/50008/2020.\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nChris Callison-Burch, Philipp Koehn, Christof Monz,\nJosh Schroeder, and Cameron Shaw Fordyce, editors.\n2008. Proceedings of the Third Workshop on Statisti-\ncal Machine Translation. Association for Computa-\ntional Linguistics, Columbus, Ohio.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\n1077\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nDaniel Deutsch, George Foster, and Markus Freitag.\n2023. Ties Matter: Modifying Kendall’s Tau for\nModern Metric Meta-Evaluation.\nPatrick Fernandes, Aman Madaan, Emmy Liu, António\nFarinhas, Pedro Henrique Martins, Amanda Bertsch,\nJosé G. C. de Souza, Shuyan Zhou, Tongshuang\nWu, Graham Neubig, and André F. T. Martins. 2023.\nBridging the gap: A survey on integrating (human)\nfeedback for natural language generation.\nErick Fonseca, Lisa Yankovskaya, AndrÃ c⃝F. T. Mar-\ntins, Mark Fishel, and Christian Federmann. 2019.\nFindings of the wmt 2019 shared tasks on quality esti-\nmation. In Proceedings of the Fourth Conference on\nMachine Translation (Volume 3: Shared Task Papers,\nDay 2), pages 1–12, Florence, Italy. Association for\nComputational Linguistics.\nMarkus Freitag, George Foster, David Grangier, Viresh\nRatnakar, Qijun Tan, and Wolfgang Macherey. 2021a.\nExperts, errors, and context: A large-scale study of\nhuman evaluation for machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1460–1474.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André F. T. Martins.\n2022. Results of WMT22 metrics shared task: Stop\nusing BLEU – neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference\non Machine Translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, George Foster, Alon Lavie, and Ondˇrej\nBojar. 2021b. Results of the WMT21 metrics shared\ntask: Evaluating metrics with expert-based human\nevaluations on TED and news domain. In Proceed-\nings of the Sixth Conference on Machine Translation,\npages 733–774, Online. Association for Computa-\ntional Linguistics.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\nSameer Jain, Vaishakh Keshava, Swarnashree Mysore\nSathyendra, Patrick Fernandes, Pengfei Liu, Gra-\nham Neubig, and Chunting Zhou. 2023. Multi-\ndimensional evaluation of text summarization with\nin-context learning.\nTom Kocmi, Rachel Bawden, Ond ˇrej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grund-\nkiewicz, Barry Haddow, Rebecca Knowles, Philipp\nKoehn, Christof Monz, Makoto Morishita, Masaaki\nNagata, Toshiaki Nakazawa, Michal Novák, Martin\nPopel, and Maja Popovi ´c. 2022. Findings of the\n2022 conference on machine translation (WMT22).\nIn Proceedings of the Seventh Conference on Ma-\nchine Translation (WMT), pages 1–45, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality.\nTom Kocmi, Christian Federmann, Roman Grund-\nkiewicz, Marcin Junczys-Dowmunt, Hitokazu Mat-\nsushita, and Arul Menezes. 2021. To ship or not to\nship: An extensive evaluation of automatic metrics\nfor machine translation. In Proceedings of the Sixth\nConference on Machine Translation, pages 478–494,\nOnline. Association for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9).\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nArle Lommel, Hans Uszkoreit, and Aljoscha Burchardt.\n2014. Multidimensional quality metrics (MQM): A\nframework for declaring and describing translation\nquality metrics. Revista Tradumàtica: tecnologies de\nla traducció.\nQingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and\nDacheng Tao. 2023. Error analysis prompting en-\nables human-like translation evaluation in large lan-\nguage models: A case study on chatgpt. arXiv\npreprint.\nQingsong Ma, Johnny Wei, Ondˇrej Bojar, and Yvette\nGraham. 2019. Results of the WMT19 metrics\nshared task: Segment-level and strong MT sys-\ntems pose big challenges. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1) , pages 62–90, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\n1078\nOpenAI. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nStefano Perrella, Lorenzo Proietti, Alessandro Scirè,\nNiccolò Campolungo, and Roberto Navigli. 2022.\nMaTESe: Machine translation evaluation as a se-\nquence tagging problem. In Proceedings of the Sev-\nenth Conference on Machine Translation (WMT) ,\npages 569–577, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nYiwei Qin, Weizhe Yuan, Graham Neubig, and\nPengfei Liu. 2022. T5score: Discriminative ﬁne-\ntuning of generative evaluation metrics. ArXiv,\nabs/2212.05726.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022a. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRicardo Rei, Marcos Treviso, Nuno M. Guerreiro,\nChrysoula Zerva, Ana C Farinha, Christine Maroti,\nJosé G. C. de Souza, Taisiya Glushkova, Duarte\nAlves, Luisa Coheur, Alon Lavie, and André F. T.\nMartins. 2022b. CometKiwi: IST-unbabel 2022 sub-\nmission for the quality estimation shared task. In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT) , pages 634–645, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning robust metrics for text genera-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881–7892, Online. Association for Computational\nLinguistics.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2022. Prompt-\ning palm for translation: Assessing strategies and\nperformance.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2021. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane\nSuhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari\nOstendorf, and Hannaneh Hajishirzi. 2023. Fine-\ngrained human feedback gives better rewards for lan-\nguage model training.\nWenda Xu, Danqing Wang, Liangming Pan, Zhenqiao\nSong, Markus Freitag, William Yang Wang, and Lei\nLi. 2023. Instructscore: Towards explainable text\ngeneration evaluation with automatic feedback.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nChrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat\nLertvittayakumjorn, José G. C. de Souza, Steffen\nEger, Diptesh Kanojia, Duarte Alves, Constantin\nOr˘asan, Marina Fomicheva, André F. T. Martins, and\nLucia Specia. 2022. Findings of the WMT 2022\nshared task on quality estimation. In Proceedings\nof the Seventh Conference on Machine Translation\n(WMT), pages 69–99, Abu Dhabi, United Arab Emi-\nrates (Hybrid). Association for Computational Lin-\nguistics.\nChrysoula Zerva, Daan van Stigt, Ricardo Rei, Ana C\nFarinha, Pedro Ramos, José G. C. de Souza, Taisiya\nGlushkova, Miguel Vera, Fabio Kepler, and André\nF. T. Martins. 2021. IST-unbabel 2021 submission\nfor the quality estimation shared task. In Proceed-\nings of the Sixth Conference on Machine Translation,\npages 961–972, Online. Association for Computa-\ntional Linguistics.\nA Multidimensional Quality Metric\n(MQM)\nThe Multidimensional Quality Metrics (MQM)\nframework is a ﬂexible human-evaluation frame-\nwork developed to evaluate and categorize errors in\ntranslations. Annotators are instructed to identify\nall errors within each segment in a document, pay-\ning particular attention to document context. See\nTable 6 for the annotator guidelines provided.\nAnnotators are asked to assign both an error\nseverity and category. Error severity (either ma-\njor or minor) is assigned independently of category.\nSpans with no marked errors have neutral sever-\nity and no category. Possible error categories are\ndisplayed in Table 7.\n1079\nYou will be assessing translations at the segment level, where a segment may contain one or more\nsentences. Each segment is aligned with a corresponding source segment, and both segments are\ndisplayed within their respective documents. Annotate segments in natural order, as if you were reading\nthe document. You may return to revise previous segments.\nPlease identify all errors within each translated segment, up to a maximum of ﬁve. If there are more than\nﬁve errors, identify only the ﬁve most severe. If it is not possible to reliably identify distinct errors because\nthe translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error\nthat spans the entire segment.\nTo identify an error, highlight the relevant span of text, and select a category/sub-category and severity\nlevel from the available options. (The span of text may be in the source segment if the error is a source\nerror or an omission.) When identifying errors, please be as ﬁne-grained as possible. For example, if a\nsentence contains two words that are each mistranslated, two separate mistranslation errors should be\nrecorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most\nsevere. If all have the same severity, choose the ﬁrst matching category listed in the error typology (eg,\nAccuracy, then Fluency, then Terminology, etc).\nPlease pay particular attention to document context when annotating. If a translation might be questionable\non its own but is ﬁne in the context of the document, it should not be considered erroneous; conversely,\nif a translation might be acceptable in some context, but not within the current document, it should be\nmarked as wrong.\nThere are two special error categories: Source error and Non-translation. Source errors should be\nannotated separately, highlighting the relevant span in the source segment. They do not count against the\nﬁve-error limit for target errors, which should be handled in the usual way, whether or not they resulted\nfrom a source error. There can be at most one Non-translation error per segment, and it should span the\nentire segment. No other errors should be identiﬁed if Non-Translation is selected.\nTable 6: MQM annotator guidelines\nSince MQM doesn’t ask annotators for quality\nscores, those scores are derived automatically from\nthe identiﬁed error spans and their classiﬁcations,\nbased on a weighting of each error severity and cat-\negory. Table 8 summarizes this weighting scheme,\nin which segment-level scores can range from 0\n(perfect) to 25 (worst). The ﬁnal segment-level\nscore is an average over scores from all annotators.\nIn some settings (e.g. calculating correlation for\nlearned metrics), the scores are negated.\nWe use the same weighting to obtain scores from\nerrors identiﬁed by AUTO MQM.\nB Datasets’ Statistics\nSee Table 9 for a summary of the number of sys-\ntems and annotated segments per system in the\nevaluation datasets used in this work.\nC Score Prediction Prompt\nFigure 9 contains the GEMBA-SQM prompt that\nwe used for our 0-shot experiments.\nD Sampling in-context learning\nexamples for AutoMQM\nFigure 10 shows the rejection criteria used when\nsampling example sets as discussed in §4.\nE Additional Results\nFigures 11, 12, 13 and 8 present additional experi-\nmental results.\n1080\nError Category Description\nAccuracy Addition Translation includes information not present in the source.\nOmission Translation is missing content from the source.\nMistranslation Translation does not accurately represent the source.\nUntranslated text Source text has been left untranslated.\nFluency Punctuation Incorrect punctuation (for locale or style).\nSpelling Incorrect spelling or capitalization.\nGrammar Problems with grammar, other than orthography.\nRegister Wrong grammatical register (eg, inappropriately informal pronouns).\nInconsistency Internal inconsistency (not related to terminology).\nCharacter encoding Characters are garbled due to incorrect encoding.\nTerminology Inappropriate for contextTerminology is non-standard or does not ﬁt context.\nInconsistent use Terminology is used inconsistently.\nStyle Awkward Translation has stylistic problems.\nLocale Address format Wrong format for addresses.\nconvention Currency format Wrong format for currency.\nDate format Wrong format for dates.\nName format Wrong format for names.\nTelephone format Wrong format for telephone numbers.\nTime format Wrong format for time expressions.\nOther Any other issues.\nSource error An error in the source.\nNon-translation Impossible to reliably characterize distinct errors.\nTable 7: MQM hierarchy.\nScore the following translation from {src_lang} to {tgt_lang} with respect to the\nhuman reference on a continuous scale from 0 to 100 that starts with \"No meaning\npreserved\", goes through \"Some meaning preserved\", then \"Most meaning preserved\nand few grammar mistakes\", up to \"Perfect meaning and grammar\".\n{src_lang} source: \"{source}\"\n{tgt_lang} human reference: \"{reference}\"\n{tgt_lang} translation: \"{candidate}\"\nScore (0-100): {score}\nFigure 9: The score prediction prompt used in this paper. Equivalent to the GEMBA-SQM prompt in Kocmi and\nFedermann (2023). Parts in purple are only included for reference-based evaluation, while parts in orange represent\nslots for outputs and are only included for in-context examples.\nSeverity Category Weight\nMajor Non-translation 25\nall others 5\nMinor Fluency/Punctuation 0.1\nall others 1\nNeutral all 0\nTable 8: MQM error weighting.\nLP #Sys #Seg\nen→de 13 1315\nzh→en 14 1875\nen→ru 15 1315\nLP #Sys #Seg\nen→kk 11 998\nkk→en 11 1000\nen→gu 11 998\ngu→en 11 1016\nTable 9: The number of systems and segments that\nhave MQM scores (left) and DA scores (right) used as\nground-truth in this work.\n1081\n1 def check_icl_set(\n2 examples: pd.DataFrame,\n3 min_errors=3,\n4 majmin_threshold=2,\n5 cat_diversity=2,\n6 min_clen=20,\n7 max_clen=400,\n8 ):\n9 # Check if they have the same number of spans as severity/category\n10 if not examples.apply(\n11 lambda r:\n12 len(r[’span’]) == len(r[’severity’]) and len(r[’span’]) == len(r[’category’]),\n13 axis=1\n14 ).all():\n15 return False\n16\n17 # Check if there are at least min_errors\n18 if examples[’severity’].apply(lambda svs: len(svs)).sum() < min_errors:\n19 return False\n20\n21 # Check that there’s a balance of major and minor errors.\n22 major_count = examples[’severity’].apply(lambda svs: sum([s==’major’ for s in svs])).sum()\n23 minor_count = examples[’severity’].apply(lambda svs: sum([s==’minor’ for s in svs])).sum()\n24 if abs(major_count - minor_count) > majmin_threshold:\n25 return False\n26\n27 # Check that at least cat_diversity error types are represented.\n28 categories = examples[’category’].apply(lambda cs: [c.split(\"/\")[0] for c in cs])\n29 represented_error_types = set().union(*categories.tolist())\n30 if len(represented_error_types) < cat_diversity:\n31 return False\n32\n33 top_clen = examples.apply(\n34 lambda row: max(len(row[s]) for s in (’source’, ’reference’, ’candidate’)\n35 ), axis=1).max()\n36 bot_clen = examples.apply(\n37 lambda row: min(len(row[s]) for s in (’source’, ’reference’, ’candidate’)),\n38 axis=1).min()\n39\n40 if top_clen > max_clen or bot_clen < min_clen:\n41 return False\n42\n43 # All checks passed.\n44 return True\nFigure 10: Rejection criteria used when sampling in-context learning examples for AUTO MQM.\n1082\n0 1 2 3 4\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45Pearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\n0 1 2 3 4\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45Pearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\nFigure 11: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the\nscore prediction prompt, sampled with uniform (left) and stratiﬁed (right) sampling, for WMT22 EN-DE.\n0 1 2 3 4\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Pearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\n0 1\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Pearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\nFigure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the\nscore prediction prompt, sampled with uniform (left) and stratiﬁed (right) sampling, for WMT22 ZH-EN.\nFigure 13: Distribution of scores for various LLM reference-\nbased evaluators, on the ZH-EN test set. Note that the yaxis\nis in log-scale.\n1083\n0 1 2 3 4 5 6\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Pearson (EN-DE)\nPaLM-2 (Bison)\nPaLM-2 (Bison) ref-free\n0 1 2 3 4 5 6\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Pearson (ZH-EN)\nPaLM-2 (Bison)\nPaLM-2 (Bison) ref-free\nFigure 14: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the\nAUTO MQM prompt, for EN-DE (left) and ZH-EN (right).",
  "topic": "Finkelstein's test",
  "concepts": [
    {
      "name": "Finkelstein's test",
      "score": 0.7309987545013428
    },
    {
      "name": "Machine translation",
      "score": 0.7199485301971436
    },
    {
      "name": "Computer science",
      "score": 0.588678777217865
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4846077859401703
    },
    {
      "name": "Translation (biology)",
      "score": 0.4801718294620514
    },
    {
      "name": "Natural language processing",
      "score": 0.4371602535247803
    },
    {
      "name": "Chemistry",
      "score": 0.0652850866317749
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physical therapy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387152517",
      "name": "Instituto Superior Técnico",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210120471",
      "name": "Instituto de Telecomunicações",
      "country": "PT"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210123353",
      "name": "Inspire",
      "country": "CH"
    }
  ]
}