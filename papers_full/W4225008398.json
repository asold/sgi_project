{
    "title": "View-Disentangled Transformer for Brain Lesion Detection",
    "url": "https://openalex.org/W4225008398",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5077081913",
            "name": "Haofeng Li",
            "affiliations": [
                "Chinese University of Hong Kong, Shenzhen",
                "Shenzhen Research Institute of Big Data"
            ]
        },
        {
            "id": "https://openalex.org/A5069576355",
            "name": "Junjia Huang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A5042965510",
            "name": "Guanbin Li",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A5100712549",
            "name": "Zhou Liu",
            "affiliations": [
                null,
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A5101265825",
            "name": "Yihong Zhong",
            "affiliations": [
                null,
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A5100394751",
            "name": "Yingying Chen",
            "affiliations": [
                null,
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A5100413261",
            "name": "Yunfei Wang",
            "affiliations": [
                null,
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A5030367905",
            "name": "Xiang Wan",
            "affiliations": [
                null,
                "Chinese University of Hong Kong, Shenzhen",
                "Shenzhen Research Institute of Big Data"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3009292867",
        "https://openalex.org/W3171468293",
        "https://openalex.org/W6773358947",
        "https://openalex.org/W2905573335",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2964241181",
        "https://openalex.org/W3035694605",
        "https://openalex.org/W3107867277",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W2164160732",
        "https://openalex.org/W2903503415",
        "https://openalex.org/W2295506082",
        "https://openalex.org/W6636827957",
        "https://openalex.org/W3027296076",
        "https://openalex.org/W3010697071",
        "https://openalex.org/W3011245295",
        "https://openalex.org/W3201641086",
        "https://openalex.org/W2979564444",
        "https://openalex.org/W6620707391",
        "https://openalex.org/W2031489346",
        "https://openalex.org/W1641498739",
        "https://openalex.org/W3008128973",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3092462694"
    ],
    "abstract": "Deep neural networks (DNNs) have been widely adopted in brain lesion\\ndetection and segmentation. However, locating small lesions in 2D MRI slices is\\nchallenging, and requires to balance between the granularity of 3D context\\naggregation and the computational complexity. In this paper, we propose a novel\\nview-disentangled transformer to enhance the extraction of MRI features for\\nmore accurate tumour detection. First, the proposed transformer harvests\\nlong-range correlation among different positions in a 3D brain scan. Second,\\nthe transformer models a stack of slice features as multiple 2D views and\\nenhance these features view-by-view, which approximately achieves the 3D\\ncorrelation computing in an efficient way. Third, we deploy the proposed\\ntransformer module in a transformer backbone, which can effectively detect the\\n2D regions surrounding brain lesions. The experimental results show that our\\nproposed view-disentangled transformer performs well for brain lesion detection\\non a challenging brain MRI dataset.\\n",
    "full_text": "VIEW-DISENTANGLED TRANSFORMER FOR BRAIN LESION DETECTION\nHaofeng Li1 Junjia Huang2 Guanbin Li2 Zhou Liu3 Yihong Zhong3\nYingying Chen3 Yunfei Wang3 Xiang Wan1,4\n1Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong (Shenzhen)\n2School of Computer Science and Engineering, Sun Yat-sen University\n3Cancer Hospital & Shenzhen Hospital, Chinese Academy of Medical Sciences\n4Pazhou Lab, Guangzhou, 510330, China\nABSTRACT\nDeep neural networks (DNNs) have been widely adopted in\nbrain lesion detection and segmentation. However, locating\nsmall lesions in 2D MRI slices is challenging, and requires\nto balance between the granularity of 3D context aggrega-\ntion and the computational complexity. In this paper, we pro-\npose a novel view-disentangled transformer to enhance the\nextraction of MRI features for more accurate tumour detec-\ntion. First, the proposed transformer harvests long-range cor-\nrelation among different positions in a 3D brain scan. Second,\nthe transformer models a stack of slice features as multiple 2D\nviews and enhance these features view-by-view, which ap-\nproximately achieves the 3D correlation computing in an ef-\nï¬cient way. Third, we deploy the proposed transformer mod-\nule in a transformer backbone, which can effectively detect\nthe 2D regions surrounding brain lesions. The experimen-\ntal results show that our proposed view-disentangled trans-\nformer performs well for brain lesion detection on a challeng-\ning brain MRI dataset.\nIndex Termsâ€” Transformer, lesion detection, brain MRI\n1. INTRODUCTION\nDeep convolutional neural networks (CNNs) have achieved\ngreat success in medical image analysis [1, 2] and can even\noutperform human experts on some task. CNN models have\nbecome an important component in computer-aided diagnosis\nsystems. Locating brain tumors including primary tumor and\nmetastasis from magnetic resonance imaging (MRI) is a fun-\ndamental task for radiologists. However, brain metastases at\nthe early stage are so small that they could be easily missed or\nmixed with vessels. Recently, thin MRI technique has been in\nwidespread use, which signiï¬cantly improves the resolution\nof 3D scans but also produces a much larger number of 2D\nslices. Going through more 2D slices increases the workload\nof radiologists, which may cause visual fatigue and higher\nHaofeng Li and Junjia Huang contribute equally. Guanbin Li is the cor-\nresponding author.\nmissing rate of brain lesion detection. Thus, we aim to design\na novel CNN-based method that helps a radiologist localize a\nbrain tumor as efï¬ciently as possible.\nAutomatic brain tumor and lesion detection has been\nstudied for years. A group of traditional methods is template\nmatching [3] that computes the correlation between pre-\ndeï¬ned tumor templates and each image position. But these\nmethods are limited by the handcrafted features and tem-\nplates. Another group of brain lesion detection methods con-\nducts binary classiï¬cation for each image position, which is\nusually referred to as brain lesion segmentation [4, 5]. How-\never, these methods require to label each image pixel/voxel,\nwhich is expensive and also unnecessary if a radiologist only\nneeds to know the rough locations of brain tumors. Besides,\nsome of existing methods are designed with the brain MRI\ndataset [6] of large-size tumors, which are not satisfactory\nfor small tumor detection in clinical applications. Some\nother methods [7, 8] adopt existing 2D object detection net-\nworks [9] to predict the bounding-box of brain lesions in a 2D\nslice. However, these models suffer from the lack of 3D con-\ntext fusion, namely aggregating the CNN features of different\nMRI slices. Universal lesion detection [1, 2, 10, 11](ULD)\nmethods, which aim at locating universal lesions in various\norgans for CT slices, could be applied to brain tumor detec-\ntion in MRI slices. The recent advances [2] in ULD focus on\nmerging features from different slices but they seldom study\nthe long-range correlations between 3D spatial positions.\nTo better model 3D features for brain lesions, we conceive\na novel view-disentangled transformer module. The key idea\nis to enhance stacked 2D slice features with the long-range\ncorrelation [12, 13] between each pair of 3D spatial posi-\ntions. To obtain the correlations for some target positions,\nthe target feature acts as a query and their similarities with\nthe feature of all positions are densely computed. These cor-\nrelations act as weights to aggregate all the features to update\nthe target one. For the features of normal brain tissue, they\nare similar and used to update each other, which reduces the\nfeature noises. Besides, the contrast between a lesion feature\nand normal brain features could be well preserved and even\narXiv:2209.09657v1  [cs.CV]  20 Sep 2022\nsharpened. However, directly measuring the dense correla-\ntions is computationally prohibited since we need to maintain\nhigh resolution of a slice feature to detect small lesions. Thus,\nwe introduce a view-disentangled mechanism that deals with\na 3D feature from three partial views in a sequential manner.\nFor each single view, the feature correlations are calculated in\na 2D form so that the computational costs are effectively re-\nduced and become affordable. We further apply the approach\nof divide-and-conquer to improve the efï¬ciency by only com-\nputing the correlations of features in the same sub-region.\nIn overall, our contributions are in three folds: ï¬rstly,\nwe introduce a view-disentangled transformer to harvest 3D\nlong-range contexts from multiple 2D views; secondly, we\ndevelop a view-disentangled detection network by deploying\nthe proposed module in a transformer-based detection model;\nlastly, we conduct experiments with a challenging Brain MRI\ndataset to verify that the proposed network is competitive and\neven superior to existing lesion detection methods.\n2. METHOD\nIn this section, we ï¬rst propose a novel view-disentangled\ntransformer (VD-Former). Then we introduce a transformer-\nbased lesion detection backbone and how the proposed VD-\nFormer is integrated with the backbone.\n2.1. View-Disentangled Transformer\nTo locate small lesions, it is desirable to preserve high-\nresolution features of brain MRI. On the other hand, brain\nlesions and normal tissues are naturally 3D structures so it\nis common to extract brain features from a 3D view. How-\never, applying 3D convolution layers (C3D) to a whole 3D\nbrain image not only costs a prohibited size of GPU memory,\nbut also require a large number of annotated 3D MRI scans\nfor training. To obtain a tradeoff between feature resolution\nand computing cost, we ï¬rst calculate 2D features for each\nMRI slice. Then a window of slice features are fused into\na single one with a new view-disentangled transformer. The\nresulted feature is considered to contain the 3D context. The\nmechanism of the proposed VD-Former can be formulated\nas:\nxâ€²\nt = F([xtâˆ’âŒŠT/2âŒ‹, Â·Â·Â· , xt, Â·Â·Â· , xt+âŒŠT/2âŒ‹]) (1)\nwhere F(Â·) denotes the VD-Former and xt is the 2D feature\nof t-th slice in a MRI scan. If k âˆˆ{tâˆ’âŒŠT/2âŒ‹, Â·Â·Â· , t,Â·Â·Â· , t+\nâŒŠT/2âŒ‹}surpasses the valid range of the brain scan x, xk is\npadded with zeros. [Â·Â·Â·] denotes a concatenation of T con-\nsecutive slice features and returns Xt,T . The shape of Xt,T\nis C Ã—H Ã—W Ã—T where C is the channel number. H and\nW are the two spatial dimensions of a 2D slice. xâ€²\nt denotes\nan enhanced feature of t-th slice, and is considered to harvest\n3D information from the T slices surrounding xt. The shape\nof xt and xâ€²\nt is C Ã—H Ã—W Ã—1.\nğ‘Š\nğ»\nğ‘‡\nğ¶ Ã—ğ» Ã—ğ‘Š Ã—ğ‘‡ ğ‘‡Ã—ğ‘Š view ğ‘‡Ã—ğ» view ğ» Ã—ğ‘Š view\nğ¶ dense pairwise correlations \nin a 2D plane\n(a)   View-Disentangled Transformer\nğ¶1\nğ¶2\nğ¶3\nğ¶4\nğ¶5\nğ‘ƒ2\nğ‘ƒ3\nğ‘ƒ4\nğ‘ƒ5\nğ‘ƒ6\nğ‘ƒ2\nğ‘ƒ3\nğ‘ƒ4\nğ‘ƒ5\nVD-Former Cas-RCNN\nVD-Former\nVD-Former\nVD-Former\nVD-Former\nCas-RCNN\nCas-RCNN\nCas-RCNN\nCas-RCNN\nğ‘ƒğ‘¡+ğ‘‡/2,ğ‘–\nğ‘ƒğ‘¡,ğ‘–\nğ‘ƒğ‘¡âˆ’ğ‘‡/2,ğ‘–\nVD-\nFormer ğ‘ƒâ€²ğ‘¡,ğ‘–\nğ‘ƒâ€²ğ‘¡âˆ’ğ‘‡/2,ğ‘–\nğ‘ƒâ€²ğ‘¡+ğ‘‡/2,ğ‘–\n(b)   VD-Former based Brain Lesion Detector\nğ¼ğ‘¡âˆ’ğ‘‡/2 ğ¼ğ‘¡ ğ¼ğ‘¡+ğ‘‡/2â€¦â€¦ â€¦â€¦\nâ€¦â€¦\nâ€¦â€¦\nFig. 1. (a) shows the idea of View-Disentangled Transformer\nthat approximately computes 3D correlations from multiple\n2D views. (b) is the proposed brain lesion detector based on\nthe VD-Former.\nSince we aim to efï¬ciently attain dense voxel-level cor-\nrelations in Xt,T to enhance xt, we implement the proposed\nVD-Former as:\nF(Xt,T ) =VH,W (VH,T (VW,T (Xt,T )))[t] (2)\nwhere VW,T (Â·) is a transformer that computes the correlations\nbetween any two vectors of size C within each W Ã—T plane.\nAs the input ofVW,T (Â·), Xt,T is transposed fromCÃ—HÃ—WÃ—\nT to H Ã—C Ã—WT where the 1st dimension of H elements\nis processed in parallel. VH,T (Â·) and VH,W (Â·) are similar to\nVW,T (Â·), but correspond to the other two views. As shown\nin Eq. (2) and Fig. 1(a), a cascade of 2D transformers from\nthree different views could efï¬ciently approximate a vanilla\n3D transformer to extract inter-slice features. [t] is to return\nthe centering feature among T slices. The returned feature is\nalso denoted as xâ€²\nt in Eq. (1).\nTo further reduce the computational overhead, we im-\nplement each 2D transformer ( VH,W , VH,T and VW,T ) with\nWindow-based Multi-head Self-Attention (W-MSA). Take\nVW,T as an example. Given a window size w, the W Ã—T\nplane is cropped into âŒŠW/wâŒ‹Ã—âŒŠ T/wâŒ‹windows of size\nw Ã—w. Only if two feature vectors belong to the same win-\ndow, their correlations are measured and used to updateXt,T .\nTo produce inter-window features, a Shifted-Window MSA\nmodule is adopted following the above W-MSA module. The\noverall process of VW,T is formulated as:\nVW,T (X) =ST,W\nâˆ’w/2(Aw(SW,T\nw/2 (Aw(X)))) (3)\nwhere Aw(Â·) denotes a W-MSA module with a window size\nw. SW,T\nw/2 (Â·) is to cyclically shift its input feature along the di-\nmensions of W and T. ST,W\nâˆ’w/2(Â·) is to reversely shift and align\nthe feature with the original input X. The shifted-window\nMSA is implemented as ST,W\nâˆ’w/2(Aw(SW,T\nw/2 (Â·))). The details\nof the W-MSA module Aw can be found in [14].\n2.2. Overall Brain Lesion Detection Architecture\nTo extract multi-scale features for each 2D MRI slice, we\nadopt a transformer-based feature pyramid network (FPN)\nthat consists of en encoder and a decoder. The encoder con-\ntains a patch embedding layer at the beginning and four basic\nSwin Transformer blocks [14]. Between two consecutive\ntransformer blocks, a patch merging layer is used to reduce\nthe feature resolution by converting the features of each2 Ã—2\npatch to a feature vector. The output feature of the patch\nembedding layer is denoted as C1. The output of the last\ntransformer block is denoted as C5. The intermediate outputs\nof the patch merging layers are denoted as C2-C4. The de-\ncoder aligns the channel number of Ci, fuses Ci from high to\nlow levels, and yields the fused features Pi. The multi-scale\nfeature fusion can be formulated as:\nPi =\n{\nConv(Ci), i = 5,\nConv(Ci) +Up(Ci+1), 2 â‰¤i <5, (4)\nwhere Conv(Â·) denotes a convolution layer converting the\nchannel number to 256. Up(Â·) is to up-sample Ci+1 so that\nCi+1 has the same resolution as Ci. To obtain a higher-level\nfeature, P6 is computed by applying a max-pooling operator\nto P5.\nIn a baseline without using our proposed view-disentangled\ntransformer, P1-P6 are used to predict the bounding box of\nbrain lesions in a Cascade R-CNN [15] way. Cascade R-\nCNNs are based on two-stage detection. At the ï¬rst stage,\na sub-network takes {Pi}as input to predict the region pro-\nposals of brain lesions. At the second stage, a sequence\nof different detectors are employed to regress the bounding\nboxes iteratively. To develop a model with the proposed VD-\nFormer, we set up a VD-Former module after each Pi in the\nabove-mentioned baseline, as shown in Fig. 1(b). Since Pi is\na feature of a 2D MRI slice, Pi can be denoted as Pt,i where\nt is the slice index. Pt,i is updated as Pâ€²\nt,i by our proposed\nVD-Former using T âˆ’1 neighboring slices, which can be\nformulated as:\nPâ€²\nt,i = F([Ptâˆ’âŒŠT/2âŒ‹,i, Â·Â·Â· , Pt,i, Â·Â·Â· , Pt+âŒŠT/2âŒ‹,i]) (5)\nwhere Pâ€²\nt,i and Pt,i correspond to xâ€²\nt,i and xt,i in Eq. (1). F\nis the view-disentangled transformer. Then Pâ€²\nt,1-Pâ€²\nt,6 will re-\nplace Pt,1-Pt,6 to be used for lesion detection in thet-th slice.\nNote that the input of the brain lesion detector isT 2D images\nwhich has 3 channels corresponding to 3 consecutive slices.\nFor examples, the center one of these T images can be de-\nnoted as [Itâˆ’1, It, It+1] where It is the t-th 2D MRI slice.\nEach time T images are sent into the detector, only the results\nof the centering slice are predicted.\n3. EXPERIMENTS\n3.1. Implementation details\nWe collect an in-house brain MRI dataset of 266 patients and\n14,530 2D lesion boxes. Each MRI scan has more than 1\nbounding box of lesions which are of 3 types, metastasis, pri-\nmary tumour and benign lesion. We only focus on 1-category\nlesion detection regardless of lesion types. In practice, ra-\ndiologists can predict the ï¬ne-grained types with the lesion\nlocations. We use the MRI modality of T1CE. Each MRI is\nof size 512 Ã—512Ã—{100âˆ¼300}. Each 2D slice is combined\nwith its adjacent slices to form a 3-channel image as an input.\nThe channel of unavailable slices are padded with zeros. The\ndataset is randomly split into 3 subsets of 128, 48, 90 patients,\nfor training, validation and testing respectively. For evalu-\nation, we use sensitivity [16] and mean Average Precision\n(mAP) [17] with an IoU threshold of 0.5. We report the sen-\nsitivity when the average number of false positives per scan\nis 1/2/4/8. The experiments are run on a NVIDIA V100 GPU\nof 32GB. Our model is initialized by the ImageNet-pretrained\nweights and trained for 36 epochs with an AdamW optimizer,\nan initial learning rate of 1e-4, a weight decay of 0.05, a batch\nsize of 1. T is set as 3. 5 slices are input to the network at\nonce. Cross-entropy loss and Smooth L1 loss are adopted to\nclassify and regress lesion boxes respectively.\n3.2. Comparison with the state-of-the-art\nWe verify the effectiveness of our proposed view-disentangled\ntransformer based detector by comparing to the existing le-\nsion detection models. For comparisons we select two groups\nof existing methods. The ï¬rst group is universal lesion de-\ntection (ULD) methods including MULAN [1], ACS [10]\nand A3D [2], which are proposed to locate nodules in CT for\ndifferent organs. These ULD methods model 3D features by\nfusing 2D features of multiple slices but they do not resort\nto dense pairwise correlations. The second group, which is\nbased on 2D object detection, includes Faster RCNN (with\nFPN) [9], DHRCNN [18], Dynamic RCNN [19], Deformable\nDETR [20], and Swin Cascade RCNN [14]. Swin Cascade\nRCNN is implemented by combining a Swin Transformer\nmodel with a Cascade R-CNN. The input of these 2D object\ndetection methods is a 3-channel 2D image that corresponds\nto a stack of 3 consecutive MRI slices. Thus these methods\nhave access to the basic 3D contexts. These methods only\npredict lesion boxes for the slice at the centering input chan-\nnel. As Table 1 shows, our proposed method (denoted as\nâ€˜Oursâ€™) signiï¬cantly outperforms both two groups of exist-\ning models with mAP and sensitivity. The proposed method\nachieves the highest mAP of 0.414 that is 1.6% higher than\nthe second best A3D. Our model obtains the best Average\nTable 1. Comparison between the state-of-the-art models and our proposed method.\nModel Year mAP Sensitivity at FPs / scan Params1 2 4 8 Average\nFaster RCNN 2015 0.352 0.233 0.291 0.371 0.500 0.381 41.12M\nMULAN 2019 0.329 0.130 0.216 0.318 0.406 0.311 26.03M\nDHRCNN 2020 0.386 0.217 0.286 0.360 0.468 0.368 46.71M\nDynamic RCNN 2020 0.352 0.143 0.219 0.309 0.409 0.313 41.12M\nDeformable DETR 2021 0.346 0.145 0.216 0.291 0.381 0.295 40.8M\nACS 2021 0.283 0.170 0.216 0.279 0.356 0.283 41.12M\nA3D 2021 0.398 0.158 0.225 0.285 0.351 0.285 74.04M\nSwin Cascade RCNN 2021 0.387 0.200 0.278 0.370 0.468 0.371 97.8M\nOurs 0.414 0.246 0.332 0.449 0.564 0.449 109.68M\nTable 2. Effectiveness of our proposed view-disentangled\ntransformer.\nModel mAP Sensitivity at FPs/scan\n1 2 4 8 Average\nBaseline 0.387 0.200 0.278 0.370 0.468 0.371\n+P3D 0.406 0.229 0.304 0.391 0.486 0.394\n+C3D 0.412 0.218 0.319 0.411 0.523 0.415\n+VDFormer 0.414 0.246 0.332 0.449 0.564 0.449\nSensitivity of 0.449 which is 6.8% higher than the second\nbest Faster RCNN of 0.381. As Fig. 2 displays, our pro-\nposed method locates all 4 lesions while the existing methods\nFRCNN, MULAN and DHRCNN have missed 1-2 regions.\n3.3. Effectiveness of the View-Disentangled Transformer\nWe show the effectiveness of our proposed VD-Former mod-\nule. In Table 2, the model â€˜Baselineâ€™ has been described in\nSec 2.2 and is based on [14]. â€˜+VD-Formerâ€™ is developed\nby deploying our proposed module at the baseline (shown\nin Fig. 1(b). â€˜+C3Dâ€™ and â€˜+P3Dâ€™ denote two models imple-\nmented by replacing all the VD-Former modules with C3D\nand P3D modules respectively. The C3D module is to apply\na vanilla 3D convolution to a stack of 2D slice features. The\npseudo 3D (P3D) module joints 1D and 2D convolutions to\napproximate a C3D layer, which is adopted in MULAN [1].\nAs Table 2 displays, the model with the VD-Former surpasses\nthe baseline without 3D fusion by 2.7% mAP, which shows\nthe effectiveness of VD-Former. Besides, the Average Sen-\nsitivity of +VD-Former is 3.4% and 5.5% higher than those\nof +C3D and +P3D respectively. As Fig. 2 shows, the base-\nline and the model with P3D fusion predict 1-2 false neg-\natives while our method with the VD-Former does not out-\nput any FPs. The above results suggest that our proposed\nVD-Former is a competitive module for 3D feature fusion.\nTo understand the efï¬ciency of VD-Former, we try to deploy\nvanilla 3D transformers (3D-Formers) at the baseline but ap-\nplying 3D-Formers to P4-P6 is already prohibited ( >32 GB\nOursBaseline+P3DBaseline\nFRCNN MULAN DHRCNN\nFig. 2. Visual results of existing methods, the baseline, the\nbaseline+P3D and our method. Real lesions and the predicted\nones are marked with blue and red boxes respectively.\nGPU memory). In contrast, applying VD-Formers to the fea-\nture maps P4-P6 (see Fig. 1(b)) takes 7.3 GB and equipping\nP2-P6 needs 26 GB. More details are in the supplemental ma-\nterials.\n4. CONCLUSION\nIn this paper we introduce a new way to enhance 3D MRI fea-\ntures for locating brain lesions. The proposed neural network\nmodule, View-Disentangled Transformer, is able to model\ncontrast and spatial coherence by harvesting dense correla-\ntions among 3D spatial positions in a brain. The proposed\nVD-Former separates a 3D feature into multiple 2D views,\naggregates these 2D-view correlations to approximate the 3D\ncorrelation computing. We further develop a brain lesion de-\ntection network based on the VD-Former, and experimentally\nshow that the proposed VD-Former based detector obtains the\nstate-of-the-art performance in comparison to existing object\ndetection and universal lesion detection methods.\n5. COMPLIANCE WITH ETHICAL STANDARDS\nWe claim that we do not have any compliance for this work.\n6. ACKNOWLEDGEMENTS\nThis work is supported in part by the Chinese Key-Area Re-\nsearch and Development Program of Guangdong Province\n(2020B0101350001), in part by the National Natural Science\nFoundation of China under Grant No.62102267, in part by\nthe Guangdong Basic and Applied Basic Research Foun-\ndation under Grant No.2020B1515020048, in part by the\nNational Natural Science Foundation of China under Grant\nNo.61976250, in part by the Guangzhou Science and Tech-\nnology Project under Grant 202102020633, in part by the\nNational Natural Science Foundation of China under Grant\nNo.12026610, and in part by the Guangdong Provincial Key\nLaboratory of Big Data Computing, The Chinese University\nof Hong Kong, Shenzhen.\n7. REFERENCES\n[1] K. Yan, Y . Tang, Y . Peng, V . Sandfort, M. Bagheri,\nZ. Lu, and R. Summers, â€œMulan: Multitask universal le-\nsion analysis network for joint lesion detection, tagging,\nand segmentation,â€ in MICCAI, 2019, pp. 194â€“202.\n[2] J. Yang, Y . He, K. Kuang, Z. Lin, H. Pï¬ster, and B. Ni,\nâ€œAsymmetric 3d context fusion for universal lesion de-\ntection,â€ in MICCAI, 2021, pp. 571â€“580.\n[3] Â´U. P Â´erez-RamÂ´Ä±rez, E. Arana, and D. Moratal, â€œBrain\nmetastases detection on mr by means of three-\ndimensional tumor-appearance template matching,â€ J.\nMagn. Reson. Imag., vol. 44, no. 3, pp. 642â€“652, 2016.\n[4] S Tchoketch Kebir, S Mekaoui, and M Bouhedda, â€œA\nfully automatic methodology for mri brain tumour de-\ntection and segmentation,â€ The Imaging Science Jour-\nnal, vol. 67, no. 1, pp. 42â€“62, 2019.\n[5] D. Karimi, J. M Peters, A. Ouaalam, S. P Prabhu,\nM. Sahin, D. A Krueger, A. Kolevzon, C. Eng, S. K\nWarï¬eld, and A. Gholipour, â€œLearning to detect brain\nlesions from noisy annotations,â€ in ISBI, 2020, pp.\n1910â€“1914.\n[6] B. H Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer,\nK. Farahani, et al., â€œThe multimodal brain tumor image\nsegmentation benchmark (brats),â€ IEEE TMI, vol. 34,\nno. 10, pp. 1993â€“2024, 2014.\n[7] Z. Zhou, J. W Sanders, J. M Johnson, M. K Gule-\nMonroe, et al., â€œComputer-aided detection of brain\nmetastases in t1-weighted mri for stereotactic radio-\nsurgery using deep learning single-shot detectors,â€ Ra-\ndiology, vol. 295, no. 2, pp. 407â€“415, 2020.\n[8] M. Zhang, G. S Young, H. Chen, J. Li, L. Qin, J R.\nMcFaline-Figueroa, D. A Reardon, X. Cao, X. Wu, and\nX. Xu, â€œDeep-learning detection of cancer metastases\nto the brain on mri,â€ J. Magn. Reson. Imaging, vol. 52,\nno. 4, pp. 1227â€“1236, 2020.\n[9] S. Ren, K. He, R. Girshick, and J. Sun, â€œFaster r-cnn:\nTowards real-time object detection with region proposal\nnetworks,â€ NeurIPS, vol. 28, pp. 91â€“99, 2015.\n[10] J. Yang, X. Huang, Y . He, J. Xu, C. Yang, G. Xu, and\nB. Ni, â€œReinventing 2d convolutions for 3d images,â€\nIEEE JBHI, 2021.\n[11] H. Zhou, C. Wang, H. Li, G. Wang, S. Zhang, W. Li, and\nY . Yu, â€œSsmd: semi-supervised medical image detection\nwith adaptive consistency and heterogeneous perturba-\ntion,â€ MIA, vol. 72, pp. 102117, 2021.\n[12] H. Li, G. Li, B. Yang, G. Chen, L. Lin, and Y . Yu,\nâ€œDepthwise nonlocal module for fast salient object de-\ntection using a single thread,â€ IEEE TCyb, 2020.\n[13] X. He, S. Yang, G. Li, H. Li, H. Chang, and Y . Yu, â€œNon-\nlocal context encoder: Robust biomedical image seg-\nmentation against adversarial attacks,â€ in AAAI, 2019,\nvol. 33, pp. 8417â€“8424.\n[14] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin,\nand B. Guo, â€œSwin transformer: Hierarchical vision\ntransformer using shifted windows,â€ inICCV, 2021, pp.\n10012â€“10022.\n[15] Z. Cai and N. Vasconcelos, â€œCascade r-cnn: Delving\ninto high quality object detection,â€ in CVPR, 2018, pp.\n6154â€“6162.\n[16] B. Van Ginneken, S. G Armato III, B. de Hoop, et al.,\nâ€œComparing and combining algorithms for computer-\naided detection of pulmonary nodules in computed to-\nmography scans: the anode09 study,â€ Medical Image\nAnalysis, vol. 14, no. 6, pp. 707â€“722, 2010.\n[17] M. Everingham, L. Van Gool, C. KI Williams, J. Winn,\net al., â€œThe pascal visual object classes (voc) challenge,â€\nIJCV, vol. 88, no. 2, pp. 303â€“338, 2010.\n[18] Y . Wu, Y . Chen, L. Yuan, Z. Liu, L. Wang, H. Li, and\nY . Fu, â€œRethinking classiï¬cation and localization for\nobject detection,â€ in CVPR, 2020, pp. 10186â€“10195.\n[19] H. Zhang, H. Chang, B. Ma, N. Wang, and X. Chen,\nâ€œDynamic r-cnn: Towards high quality object detection\nvia dynamic training,â€ in ECCV, 2020, pp. 260â€“275.\n[20] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, â€œDe-\nformable detr: Deformable transformers for end-to-end\nobject detection,â€ in ICLR, 2021."
}