{
  "title": "Less Is More: Pay Less Attention in Vision Transformers",
  "url": "https://openalex.org/W3170642968",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3045690129",
      "name": "Zizheng Pan",
      "affiliations": [
        "Australian Regenerative Medicine Institute",
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2337603917",
      "name": "Bohan Zhuang",
      "affiliations": [
        "Monash University",
        "Australian Regenerative Medicine Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2129072291",
      "name": "Haoyu He",
      "affiliations": [
        "Australian Regenerative Medicine Institute",
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1479773632",
      "name": "Jing Liu",
      "affiliations": [
        "Australian Regenerative Medicine Institute",
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2093782331",
      "name": "Jianfei Cai",
      "affiliations": [
        "Australian Regenerative Medicine Institute",
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A3045690129",
      "name": "Zizheng Pan",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2337603917",
      "name": "Bohan Zhuang",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2129072291",
      "name": "Haoyu He",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1479773632",
      "name": "Jing Liu",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2093782331",
      "name": "Jianfei Cai",
      "affiliations": [
        "Monash University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3179517581",
    "https://openalex.org/W3177313544",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W2989226908",
    "https://openalex.org/W6736170873",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2569272946",
    "https://openalex.org/W6762215367",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W6758232405",
    "https://openalex.org/W6730903564",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2556967412",
    "https://openalex.org/W3138796575",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W6789317445",
    "https://openalex.org/W3034243376",
    "https://openalex.org/W6789425149",
    "https://openalex.org/W6762537594",
    "https://openalex.org/W2938428612",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3135921327",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W2937549930",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2902303185",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2949846184",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W3159663321",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2962900737",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2953307569",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W3204076343",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2966926453",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W3202742610",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3204182250",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W4298395628",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2963112696",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4299802238",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3204801262",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3202715235",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W4376983087",
    "https://openalex.org/W4294498805",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W2995575179"
  ],
  "abstract": "Transformers have become one of the dominant architectures in deep learning, particularly as a powerful alternative to convolutional neural networks (CNNs) in computer vision. However, Transformer training and inference in previous works can be prohibitively expensive due to the quadratic complexity of self-attention over a long sequence of representations, especially for high-resolution dense prediction tasks. To this end, we present a novel Less attention vIsion Transformer (LIT), building upon the fact that the early self-attention layers in Transformers still focus on local patterns and bring minor benefits in recent hierarchical vision Transformers. Specifically, we propose a hierarchical Transformer where we use pure multi-layer perceptrons (MLPs) to encode rich local patterns in the early stages while applying self-attention modules to capture longer dependencies in deeper layers. Moreover, we further propose a learned deformable token merging module to adaptively fuse informative patches in a non-uniform manner. The proposed LIT achieves promising performance on image recognition tasks, including image classification, object detection and instance segmentation, serving as a strong backbone for many vision tasks. Code is available at https://github.com/zip-group/LIT.",
  "full_text": "Less is More: Pay Less Attention in Vision Transformers\nZizheng Pan, Bohan Zhuang*, Haoyu He, Jing Liu, Jianfei Cai\nData Science & AI, Monash University, Australia\n{zizheng.pan, bohan.zhuang, haoyu.he, jing.liu1, jianfei.cai}@monash.edu\nAbstract\nTransformers have become one of the dominant architectures\nin deep learning, particularly as a powerful alternative to con-\nvolutional neural networks (CNNs) in computer vision. How-\never, Transformer training and inference in previous works\ncan be prohibitively expensive due to the quadratic complex-\nity of self-attention over a long sequence of representations,\nespecially for high-resolution dense prediction tasks. To this\nend, we present a novel Less attention vIsion Transformer\n(LIT), building upon the fact that the early self-attention lay-\ners in Transformers still focus on local patterns and bring\nminor benefits in recent hierarchical vision Transformers.\nSpecifically, we propose a hierarchical Transformer where we\nuse pure multi-layer perceptrons (MLPs) to encode rich local\npatterns in the early stages while applying self-attention mod-\nules to capture longer dependencies in deeper layers. More-\nover, we further propose a learned deformable token merg-\ning module to adaptively fuse informative patches in a non-\nuniform manner. The proposed LIT achieves promising per-\nformance on image recognition tasks, including image classi-\nfication, object detection and instance segmentation, serving\nas a strong backbone for many vision tasks. Code is available\nat https://github.com/zip-group/LIT.\nIntroduction\nTransformers have made substantial strides in natural lan-\nguage processing (NLP) (e.g., (Vaswani et al. 2017; Devlin\net al. 2019)) and recently in the computer vision (CV) field\n(e.g., (Dosovitskiy et al. 2021; Touvron et al. 2021a)). In-\nspired by the pyramid design (Lin et al. 2017a) in CNNs, re-\ncent hierarchical vision Transformers (HVTs) (Wang et al.\n2021; Liu et al. 2021; Wu et al. 2021; Yan et al. 2021)\ndivide transformer blocks into several stages and progres-\nsively shrink feature maps as the network goes deeper. How-\never, high-resolution feature maps in the early stages result\nin long token sequences, which brings huge computational\ncost and memory consumption due to the quadratic com-\nplexity of self-attention. For instance, a feature map of size\n56 √ó56 √ó96 costs 2.0G FLOPs in one Multi-head Self-\nAttention (MSA) (Vaswani et al. 2017), while the entire\nmodel of ResNet-18 (He et al. 2016) only requires 1.8G\n*Corresponding author.\nCopyright ¬© 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFLOPs. Such a huge computing cost makes it difficult to\napply Transformers into broad computer vision tasks.\nSeveral emerging efforts have been made to reduce the\ncomputational cost in the early stages of HVTs. For exam-\nple, some works (Wu et al. 2021; Vaswani et al. 2021) reduce\nthe number of self-attention heads in an MSA layer or fur-\nther decreasing the number of Transformer blocks (Dosovit-\nskiy et al. 2021). Another line of works proposes to trade-off\naccuracy and efficiency for MSA via heuristic approxima-\ntion, such as spatial reduction attention (SRA) (Wang et al.\n2021) and shifted window based multi-head self-attention\n(SW-MSA) (Liu et al. 2021). There are also studies simply\nemploy convolutional layers (Srinivas et al. 2021; Graham\net al. 2021) when the resolution of feature maps are consid-\nerably large. However, how much the early adoption of self-\nattention layers really contributes to the final performance\nremains unclear.\nIn this paper, we present a Less attention vIsion\nTransformer (LIT) to address the aforementioned problem\nfor HVTs. Specifically, we propose to exclusively use MLP\nlayers to capture local patterns in the early stages while in-\ntroducing MSA layers with sufficient number of heads to\nhandle long range dependencies in the later stages. The mo-\ntivation comes from two aspects. First, previous studies in\nboth CNNs and Transformers have shown that shallow lay-\ners focus on local patterns and deeper layers tend to cap-\nture high-level semantics or global relationships (Wu, Su,\nand Huang 2019; Hou et al. 2019; Tenney, Das, and Pavlick\n2019), arising the question of whether using self-attention\nat the early stages is necessary. Second, from the theoreti-\ncal perspective, a self-attention layer with sufficient heads\napplied to images can express any convolutional layer (Cor-\ndonnier, Loukas, and Jaggi 2020). However, fewer heads in\nan MSA layer theoretically hinder the ability of approximat-\ning a convolutional layer with a large kernel size, where the\nextreme case is as expressive as a 1 √ó1 convolution that\ncan be viewed as a standard FC layer applied to each pixel\nindependently. While recent HVTs adopt very few heads at\nthe early stages to deliver pyramid representations, we ar-\ngue that this is not optimal as such setting introduces high\ncomputational and memory cost but brings minor benefits.\nTo be emphasized, by exploiting MLP blocks in the\nearly stages, the model avoids the huge computational cost\nand memory footprint arising from self-attention on high-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2035\nresolution feature maps. Moreover, applying self-attention\nin the later stages to capture long range dependencies is quite\nefficient due to the progressive shrinking pyramid. Our com-\nprehensive results show that such a simple architecture de-\nsign brings a sweet spot between model performance and\nefficiency.\nFurthermore, recent HVTs either adopt a standard convo-\nlutional layer or a linear projection layer to merge nearby\ntokens (Wang et al. 2021; Liu et al. 2021), aiming to con-\ntrol the scale of feature maps. However, such methods hin-\nder the representational power for vision Transformers to\nmodel geometric transformations, considering that not ev-\nery pixel equally contributes to an output unit (Luo et al.\n2016). To this end, we propose a Deformable Token Merging\n(DTM) module, inspired by deformable convolutions (Dai\net al. 2017; Zhu et al. 2019), where we learn a grid of off-\nsets to adaptively augment the spatial sampling locations for\nmerging neighboring patches from a sub-window in a fea-\nture map. In this way, we can obtain more informative down-\nsampled tokens for subsequent processing.\nOur contributions can be summarized as follows. First, we\nidentify the minor contribution of the early MSA layers in\nrecent HVTs and propose a simple HVT structure with pure\nMLP blocks in the early stages. Second, we propose a de-\nformable token merging module to adaptively merge more\ninformative patches to deliver hierarchical representations,\nwith enhanced transformation modeling capability. Finally,\nwe conduct extensive experiments to show that the proposed\nLIT performs favorably against several state-of-the-art vi-\nsion Transformers with similar or even reduced computa-\ntional complexity and memory consumption.\nRelated Work\nVision Transformers. Vision Transformers are models\nwhich adopt the self-attention mechanism (Vaswani et al.\n2017) into CV tasks. Recent works towards Vision Trans-\nformers either follow a hybrid architecture that combines\nconvolution and self-attention (Carion et al. 2020; Srini-\nvas et al. 2021), or design a pure self-attention architec-\nture without convolution (Parmar et al. 2019; Hu et al.\n2019). More recently, Dosovitskiy et al. (Dosovitskiy et al.\n2021) propose a Vision Transformer (ViT) which achieves\npromising results on ImageNet. Since then, a few subsequent\nworks have been proposed to improve ViT from different\naspects. For example, some works (Yuan et al. 2021b; Li\net al. 2021) seek to bring locality into ViT as they find ViT\nfailed to model the important local structures (e.g., edges,\nlines). Another line of works aims to explore deeper archi-\ntectures (Zhou et al. 2021; Touvron et al. 2021b) by stack-\ning more Transformer blocks. Some studies (Chen et al.\n2021a,b) also try to search a well-performed ViT with neural\narchitecture search (NAS).\nThere is also a prevailing trend to introduce hierarchi-\ncal representations into ViT (Pan et al. 2021; Yuan et al.\n2021a; Wang et al. 2021; Liu et al. 2021; Wu et al. 2021;\nHeo et al. 2021; Vaswani et al. 2021). To do so, these works\ndivide Transformer blocks into several stages and downsam-\nple feature maps as the network goes deeper. However, high-\nresolution feature maps in the early stages inevitably result\nin high computational and memory costs due to the quadratic\ncomplexity of the self-attention module. Targeting at this\nproblem, Wang et al. (Wang et al. 2021) propose to reduce\nthe spatial dimensions of attention‚Äôs key and value matrices.\nLiu et al. (Liu et al. 2021) propose to limit self-attention\nin non-overlapped local windows. However, these replace-\nments seek to shrink the global attention maps for efficiency.\nIn this paper, we elaborately design the shallow layers with\npure MLPs, that are powerful enough to encode local pat-\nterns. This neat architecture design keeps the capability for\nmodelling global dependencies in the later stages while eas-\ning the prohibitively expensive complexity introduced by\nhigh-resolution feature maps, especially in the dense predic-\ntion tasks.\nDeformable Convolutions. Deformable convolutions\n(DC) are initially proposed by Dai et al. (Dai et al. 2017) in\nobject detection and semantic segmentation tasks. Different\nfrom the regular sampling grid of a standard convolution,\nDC adaptively augments the spatial sampling locations with\nlearned offsets. One following work by Zhu et al. (Zhu\net al. 2019) improves DC with a modulation mechanism,\nwhich modulates the input feature amplitudes from different\nspatial locations. With the advantage on modeling geometric\ntransformations, many works adopt DC to target various CV\nproblems. For example, Zhuet al. (Zhu et al. 2021) propose\na deformable attention module for object detection. Shim et\nal. (Shim, Park, and Kweon 2020) construct a similarity\nsearch and extraction network built upon DC layers for\nsingle image super-resolution. Thomas et al. (Thomas\net al. 2019) introduce a deformable kernel point convolution\noperator for point clouds. In this paper, we propose a\ndeformable token merging module to adaptively merge\nmore informative image tokens. Unlike previous works that\nmerge tokens from a regular grid, DTM introduces better\ntransformation modeling capability for HVTs.\nProposed Method\nOverall Architecture\nThe overall architecture of LIT is illustrated in Figure 1. Let\nI‚àà RH√óW√ó3 be an input RGB image, whereH and W rep-\nresent the height and width, respectively. We first splitIinto\nnon-overlapping patches with a patch size of4 √ó4, and thus\nthe initial feature dimension of each patch is4√ó4√ó3 = 48.\nNext, a linear embedding layer is exploited to project each\npatch into dimension C1, serving as the initial input for the\nfollowing pipeline. The entire model is divided into 4 stages.\nLetting s ‚àà[1, 2, 3, 4] be the index of a stage, we employLs\nblocks at each stage, where the first two stages solely utilise\nMLP blocks to encode local representations and the last two\nstages employ standard Transformer blocks (Dosovitskiy\net al. 2021) to handle longer dependencies. At each stage, we\nscale the input feature maps into Hs‚àí1\nPs\n√óWs‚àí1\nPs\n√óCs, where\nPs and Cs represent the patch size and the hidden dimension\nat the s-th stage, respectively. For the last two stages, we set\nNs self-attention heads in each Transformer block.\n2036\nLinearEmbeddingMLPBlock\nPatchPartition\nDTMMLPBlock\nDTMTransformerBlock\nDTMTransformerBlock\nùêª4√óùëä4√óùê∂! ùêª8√óùëä8√óùê∂\" ùêª16√óùëä16√óùê∂# ùêª32√óùëä32√óùê∂$\n√óùêø! √óùêø\" √óùêø# √óùêø$\nStage1 Stage2 Stage3 Stage4\nùêª√óùëä√ó3\nMLPNorm\nMSANorm\nMLPNorm\nFigure 1: Overall architecture of LIT. The model is divided into four stages, where we apply MLP blocks in the first two\nstages and employ standard Transformer blocks in the last two stages. ‚ÄúDTM‚Äù denotes our proposed deformable token merging\nmodule.\nBlock Design in LIT\nAs shown in Figure 1, LIT employs two types of blocks:\nMLP blocks and Transformer blocks. In the early stages,\nwe apply MLP blocks. Concretely, an MLP block is built\nupon an MLP which consists of two FC layers with\nGELU (Hendrycks and Gimpel 2016) non-linearity in be-\ntween. For each MLP at the s-th stage, an expansion ratio\nof Es is used. Specifically, the first FC layer expands the di-\nmension of a token from Cs to Es √óCs, and the other FC\nlayer reduces the dimension back to Cs. Formally, letting\nX ‚ààR(\nHs‚àí1\nPs √ó\nWs‚àí1\nPs )√óCs be the input of the s-th stage and l\nbe the index of a block, an MLP block can be formulated as\nXl = Xl‚àí1 + MLP(LN(Xl‚àí1)), (1)\nwhere LN indicates the layer normalization (Ba, Kiros, and\nHinton 2016) and MLP denotes an MLP. In the last stages,\na Transformer block as described in ViT (Dosovitskiy et al.\n2021) contains an MSA layer and an MLP, which can be\nexpressed as\nX\n‚Ä≤\nl‚àí1 = Xl‚àí1 + MSA(LN(Xl‚àí1)), (2)\nXl = X\n‚Ä≤\nl‚àí1 + MLP(LN(X\n‚Ä≤\nl‚àí1)). (3)\nWith this architecture, our model benefits from two main ad-\nvantages: First, we avoid the huge computational costs and\nmemory footprint that are introduced by long sequences in\nthe early stages. Second, unlike recent works that shrink the\nattention maps using sub-windows (Liu et al. 2021) or re-\nduce the spatial dimensions of the key and value matrices,\nwe keep standard MSA layers in the last two stages so as\nto maintain the capability of LIT to handle long range de-\npendencies while keeping mild FLOPs due to the pyramid\nstructure. We will show in the ablation study that our simple\narchitecture design outperforms the state-of-the-art hierar-\nchical ViT variants on ImageNet with comparable FLOPs.\nRemark. Here we justify the rationality of applying pure\nMLP blocks in the early stages by considering the relation-\nship among a convolutional layer, an FC layer and an MSA\nlayer. Firstly, we begin with a review of a standard convolu-\ntional layer. Let X ‚ààRH√óW√óCin be the input feature map,\nand let W ‚ààRK√óK√óCin√óCout be the convolutional weight\ntensor, whereK is the kernel size,Cin and Cout are the input\nand output channel dimensions, respectively. For simplic-\nity, we omit the bias term and use Xp,: to represent Xi,j,:,:,\nwhere (i, j) denotes the pixel index and p ‚àà [H] √ó[W].\nGiven a convolutional kernel of K √óK sampling locations,\nthe output for a pixel p can be formulated as\nConv(X)p,: =\nX\nk‚àà[K√óK]\nXp+g(k),:Wg(k),:,:, (4)\nwhere g : [K √óK] ‚Üí‚àÜK is a bijective mapping of sam-\npling indexes onto the pre-specified offsets ‚àÜK. For exam-\nple, let ‚àÜK = {(‚àí1, ‚àí1), (‚àí1, 0), ...,(0, 1), (1, 1)}be a\n3 √ó3 kernel with dilation 1, then g(0) = ( ‚àí1, ‚àí1) rep-\nresents the first sampling offset.\nWhen K = 1 , the weight tensor W is equivalent to a\nmatrix, such that W ‚ààRCin√óCout. In this case, Eq. (4) can\nexpress an FC layer, and the output for a pixel p is defined\nby\nFC(X)p,: = Xp,:W:,:. (5)\nLast, let Nh be the number of heads in an MSA layer and\nW(h) ‚àà RCin√óCout be learnable parameters of the h-th\nhead. Under a specific relative positional encoding scheme,\nCordonnier et al. (Cordonnier, Loukas, and Jaggi 2020)\nprove that the output from an MSA layer at pixel p can be\nformulated as\nMSA(X)p =\nX\nh‚àà[Nh]\nXp+f(h),:W(h), (6)\nwhere f : [ Nh] ‚Üí ‚àÜK is a bijective mapping of heads\nonto pixel shifts. In that case, Eq. (6) can be seen as an ap-\nproximation to a convolutional layer with a kernel size of\n2037\n‚àöNh √ó‚àöNh. We refer detailed explanations to (Cordon-\nnier, Loukas, and Jaggi 2020).\nFrom Eqs. (4)-(6), we observe that while an MSA layer\nwith sufficient heads is able to approximate any convolu-\ntional layer, fewer heads theoretically limit the ability of\nsuch approximation. As an extreme case, an MSA layer with\none head is only capable of approximating an FC layer. Note\nthat an MSA layer is certainly not equivalent to a convolu-\ntional layer in practice. However, d‚ÄôAscoli et al. (d‚ÄôAscoli\net al. 2021) observe that the early MSA layers can learn to\nbehave convolutional upon training. Considering most re-\ncent HVTs (Wang et al. 2021; Wu et al. 2021) adopt very few\nheads in the early stages, such convolutional behavior could\nbe limited within small receptive fields. In Figure 3, we show\nin visualizations that the early MSA layers in PVT-S (Wang\net al. 2021) indeed only attend to a tiny area around the query\npixel, while removing them costs a minor performance drop\nbut achieves significant reduction in model complexity. This\njustifies our method of applying pure MLP blocks in the first\ntwo stages without self-attention.\nDeformable Token Merging\nPrevious works on HVTs (Wang et al. 2021; Liu et al. 2021)\nrely on patch merging to achieve pyramid feature representa-\ntions. However, they merge patches from a regular grid and\nneglect the fact that not every patch contributes equally to an\noutput unit (Luo et al. 2016). Inspired by deformable con-\nvolutions (Dai et al. 2017; Zhu et al. 2019), we propose a\ndeformable token merging module to learn a grid of offsets\nto adaptively sample more informative patches. Formally, a\ndeformable convolution is formulated as\nDC(X)p,: =\nX\nk‚àà[K√óK]\nXp+g(k)+‚àÜg(k),:Wg(k),:,:. (7)\nCompared to a standard convolution operation as in Eq. (4),\nDC learns an offset‚àÜg(k) for each pre-specified offsetg(k).\nLearning ‚àÜg(k) requires a separate convolutional layer,\nwhich is also applied over the input feature mapX. To merge\npatches in an adaptive manner, we adopt one DC layer in a\nDTM module, which can be formulated by\nDTM(X) = GELU(BN(DC(X))), (8)\nwhere BN denotes the batch normalization (Ioffe and\nSzegedy 2015) and we employ the GELU non-linearity. We\nwill show in the ablation study that the sampling locations\nin DTM are adaptively adjusted when objects‚Äô scales and\nshapes change, benefiting from the learned offsets. Also note\nthat our light-weight DTM introduces negligible FLOPs and\nparameters compared to regular grid sampling in baselines,\nthus making it a plug-and-play module for recent HVTs.\nExperiments\nImageNet Classification\nWe conduct experiments on ImageNet (ILSVRC2012) (Rus-\nsakovsky et al. 2015) dataset. ImageNet is a large-scale\ndataset which has ‚àº1.2M training images from 1K cate-\ngories and 50K validation images. We compare with CNN-\nbased ResNet (He et al. 2016) and Transformer-based mod-\nels including DeiT (Touvron et al. 2021a), PVT (Wang et al.\n2021) and Swin (Liu et al. 2021). For simplicity, we de-\nnote them as ‚ÄúModel-Ti/S/M/B‚Äù to refer to their tiny, small,\nmedium and base variants. Similarly, we define four vari-\nants of our LIT models: LIT-Ti, LIT-S, LIT-M and LIT-B.\nDetailed architecture specifications are included in the sup-\nplementary material. For better comparison, we design LIT-\nTi as a counterpart to PVT-S, where both models adopt the\nabsolute positional encoding. Our LIT-S, LIT-M and LIT-B\nuse the relative positional encoding, and these three mod-\nels can be seen as competitors to Swin-Ti, Swin-S, Swin-B,\nrespectively.\nImplementation details. In general, all models are trained\non ImageNet with 300 epochs and a total batch size of 1024.\nFor all ImageNet experiments, training images are resized\nto 256 √ó256, and 224 √ó224 patches are randomly cropped\nfrom an image or its horizontal flip, with the per-pixel mean\nsubtracted. We use the single-crop setting for testing. We\nuse AdamW optimizer (Loshchilov and Hutter 2019) with\na cosine decay learning rate scheduler. The initial learning\nrate is 1 √ó10‚àí3, and the weight decay is set to 5 √ó10‚àí2.\nThe initial values of learnable offsets in DTM are set to 0,\nand the initial learning rate for offset parameters is set to\n1√ó10‚àí5. The kernel sizes and strides in DTM are consistent\nwith that of patch merging layers in PVT and Swin. For a fair\ncomparison, we adopt the same training strategies as PVT\nand Swin when comparing our models to each of them.\nResults on ImageNet. In Table 1, we compare LIT with\nseveral state-of-the-art methods on ImageNet, ImageNet-\nReal and ImageNet-V2. In general, all LIT models have\nfewer parameters, less FLOPs and faster throughput than\ntheir counterparts without applying any existing efficient\nself-attention mechanisms. For memory consumption, at the\ntraining time, we observe LIT-Ti and LIT-S require less\nmemory than PVT-S and Swin-Ti while things are on the\ncontrary when comparing LIT-M/B with Swin-S/B. The rea-\nson is due to the increased activation memory of standard\nMSA layers at the later stages. However, at the testing stage,\nLIT models show advantages over baselines as all of them\nconsume less memory than their counterparts.\nIn terms of model performance, on ImageNet, LIT-Ti out-\nperforms PVT-S by 1.3% on the Top-1 accuracy while the\nFLOPs is reduced by 0.2G. LIT-S surpasses Swin-Ti by\n0.2% with 0.4G less FLOPs. LIT-M achieves on par per-\nformance with Swin-S, whereas the FLOPs of LIT-M is\nreduced. LIT-B brings 0.1% Top-1 accuracy increase over\nSwin-B while using 0.4G less FLOPs. For ResNet and\nDeiT, LIT demonstrates better performance when compared\nto them with the same magnitude of FLOPs and param-\neters (e.g., ResNet-50 v.s. LIT-S, DeiT-B v.s. LIT-B). On\nImageNet-real, LIT-Ti improves PVT-S by 0.8% on the Top-\n1 accuracy, while LIT-S/M/B achieve slightly lower accu-\nracy than Swin models. Finally, on ImageNet-V2, LIT mod-\nels achieve on par or better performance than PVT-S and\nSwin models. Overall, LIT models present competitive per-\nformance across the three datasets, challenging the full self-\nattention models in recent works.\n2038\nMethod Params\nFLOPs Throughput Train\nMemory Test Memory ImageNet@Top-1\nReal@Top-1 V2@Top-1\nResNet-18 12M 1.8G\n4,454 2,812 1,5\n11 69.8 77.3\n57.1\nResNet-50 26M 4.1G\n1,279 8,051 2,9\n08 76.2 82.5\n63.3\nResNet-101 45M 7.9G\n722 10,710 3,053 77.4 83.7\n65.7\nDeiT-T\ni 5M 1.3G\n3,398 2,170 3\n14 72.2 80.1\n60.4\nDeiT-S 22M 4.6G\n1,551 4,550 7\n13 79.8 85.7\n68.5\nDeiT-B 86M 17.5G\n582 10,083 1,939 81.8 86.7\n71.5\nPVT-T\ni 13M 1.9G\n1,768 4,316 1,2\n69 75.1 82.2\n63.0\nPVT-S 25M 3.8G\n1,007 6,996 1,3\n56 79.8 85.8\n68.4\nPVT-M 44M 6.7G\n680 9,546 1,5\n06 81.2 86.7\n70.1\nPVT-L 61M 9.8G\n481 13,343 1,637 81.7 87.0\n71.2\nSwin-Ti 28M 4.5G\n961 6,211 1,4\n93 81.3 86.6\n69.7\nSwin-S 50M 8.7G\n582 9,957 1,6\n97 83.0 87.6\n72.1\nSwin-B 88M 15.4G\n386 13,705 2,453 83.3 87.7\n72.3\nLIT-T\ni 19M 3.6G\n1,294 5,868 1,1\n94 81.1 86.6\n70.4\nLIT-S 27M 4.1G\n1,298 5,973 1,2\n64 81.5 86.4\n70.4\nLIT-M 48M 8.6G\n638 12,248 1,473 83.0 87.3\n72.0\nLIT-B 86M 15.0G\n444 16,766 2,150 83.4 87.6\n72.8\nTable\n1: Comparisons with several state-of-the-art methods on ImageNet (Russakovsky et al. 2015), ImageNet-Real (Beyer et al.\n2020) and ImageNet-V2 matched frequency (Recht et al. 2019). All models are trained and evaluated with the input resolution\nof 224 √ó224. Throughput (imgs/s) is measured on one NVIDIA RTX 3090 GPU, with a batch size of 64 and averaged over 30\nruns. Training time and testing time memory consumption is measured with a batch size of 64 in Megabyte (MB).\nModel Params\nFLOPs Top-1 Acc. (%)\nPVT-S\n(Wang et al. 2021) 25M 3.8G\n79.8\nLIT-Ti* 19M 3.6G\n80.4\nSwin-Ti\n(Liu et al. 2021) 28M 4.5G\n81.3\nLIT-S* 27M 4.1G 81.3\nTable\n2: Effect of our architecture design principle. * denotes\nthe LIT model which adopts the same uniform patch merging\nstrategies as in PVT-S or Swin-Ti.\nModel Params\nFLOPs Top-1 Acc. (%)\nPVT-S\n(Wang et al. 2021) 25M 3.8G\n79.8\nPVT-S + DTM 25M 3.8G 80.5\nSwin-Ti\n(Liu et al. 2021) 28M 4.5G\n81.3\nSwin-Ti + DTM 28M 4.5G 81.6\nTable\n3: Effect of the proposed deformable token merging\nmodule. We replace the uniform patch merging strategies in\nPVT-S and Swin-Ti with our DTM.\nAblation Studies\nEffect of the architecture design. To explore the effect\nof our architecture design principle in LIT, we conduct ex-\nperiments on ImageNet and compare the architecture of LIT\nwith two recent HVTs: PVT-S (Wang et al. 2021) and Swin-\nTi (Liu et al. 2021). The results are shown in Table 2. In\ngeneral, our architecture improves baseline PVT-S by 0.6%\nin Top-1 accuracy while using less FLOPs (3.6G v.s. 3.8G).\nFor Swin-Ti, our method reduces the FLOPs by 0.4G while\nachieving on par performance. It is also worth noting that\nthe total number of parameters is reduced for both PVT-S\nand Swin-Ti. The overall performance demonstrates the ef-\nfectiveness of the proposed architecture, which also empha-\nsizes the minor benefits of the early MSAs in PVT and Swin.\nEffect of deformable token merging. To verify the ef-\nfectiveness of our proposed DTM strategy, we replace the\ndefault patch merging scheme in PVT-S and Swin-Ti with\nDTM and train the models on ImageNet. The results are\nshown in Table 3. We observe that for both models, DTM\nintroduces negligible FLOPs and parameters while improv-\ning PVT-S and Swin-Ti by 0.7% and 0.3% in terms of the\nFigure 2: Visualization of learned offsets by the proposed\ndeformable token merging modules. Each image shows 43\nsampling locations (red dots) in three DTM modules with\n2√ó2 filter. The green rectangle outlines a32√ó32 patch of the\noriginal image, which also indicates a regular sampling field\nof previous methods. Best viewed in color. More examples\ncan be found in the supplementary material.\n2039\nModel Stage P\narams FLOPs Top-1 (%)\nPVT-S 0 25M\n3.8G 79.8\nPVT-S w/ MSA 0 20M\n8.4G 80.9\nPVT-S w/ MSA 1 20M\n4.5G 80.8\nPVT-S w/ MSA 1,2 19M\n3.6G 80.4\nPVT-S w/ MSA 1,2,3 17M\n3.0G 75.0\nPVT-S w/ MSA 1,2,3,4 14M\n2.8G 66.8\nTable\n4: Impact of the MSA layers at each stage. Note that\nPVT-S has four stages, which adopts SRA at all blocks in-\nstead of MSA. Here we denote ‚Äúw/ MSA‚Äù as PVT-S with\nstandard MSA layers. ‚ÄúStage‚Äù refers to the stages where we\nremove all self-attention layers. For example, ‚Äú1,2‚Äù means\nwe remove the self-attention layers in the first two stages.\nNote that ‚Äú0‚Äù means a model without removing any self-\nattention layers.\nTop-1 accuracy, respectively. Furthermore, we visualize the\nlearned offsets in Figure 2. As it shows, unlike previous\nuniform patch merging strategy where sampled locations\nare limited within the green rectangle, our DTM adaptively\nmerges patches according to objects‚Äô scales and shapes (e.g.,\nkoala leg, cat tail).\nEffect of MSA in each stage. To explore the effect of self-\nattention in recent HVTs, we train PVT-S on ImageNet and\ngradually remove self-attention layers at each stage. The re-\nsults are presented in Table 4. First, after replacing the SRA\nlayers in PVT-S with standard MSA layers, we observe 1.1%\nimprovement on the Top-1 accuracy whereas the FLOPs is\nalmost doubled. Next, by gradually removing MSA layers in\nthe first two stages, the Top-1 accuracy only drops by 0.1%,\n0.5%, respectively. It implies that the self-attention layers in\nthe early stages of PVT contribute less than expected to the\nfinal performance, and they perform not much better than\npure MLP layers. It can be attributed to the fact that shal-\nlow layers focus more on encoding local patterns. However,\nwe observe a huge performance drop when removing self-\nattention layers in the last two stages. The results show that\nthe self-attention layers play an important role in the later\nstages and capturing long range dependencies is essential\nfor well-performed hierarchical vision Transformers.\nTo better understand the phenomenon, we visualize the at-\ntention probabilities for PVT-S without removing any MSA\nlayers, which are depicted in Figure 3. First, the attention\nmap at the first stage shows that the query pixel almost pays\nno attention to other locations. At the second stage, the re-\nceptive field of the query pixel is slightly enlarged, but sim-\nilar to the first stage. Considering that PVT-S only has one\nhead at the first stage and two heads at the second stage,\nthis strongly supports our hypothesis that very few heads in\nan MSA layer result in a smaller receptive field, such that a\nself-attention layer is almost equivalent to an FC layer. Fur-\nthermore, we observe relatively larger receptive fields from\nthe attention maps of the last two stages. As a large recep-\ntive field usually helps to model longer dependencies, this\nexplains the huge performance drop in Table 4 after we re-\nmove the MSA layers in the last two stages.\nModel RetinaNet\n#P AP AP 50 AP75 APS APM APL\nR-50 38M 36.3 55.3\n38.6 19.3 40.0\n48.8\nPVT-S 34M 40.4 61.3\n43.0 25.0 42.9\n55.7\nLIT-Ti 30M 41.6 62.8\n44.7 25.7 44.4\n56.4\nR-101 57M 38.5 57.8\n41.2 21.4 42.6\n51.1\nSwin-Ti 39M 41.5 62.1 44.2 25.1 44.9 55.5\nLIT-S 39M 41.6 62.7 44.1 25.6 44.7 56.5\nTable\n5: Object detection performance on the COCO\nval2017 split using the RetinaNet framework. ‚Äú#P‚Äù refers\nto the number of parameters.\nModel Mask R-CNN\n#P APb APb\n50 APb\n75\nAPm APm\n50 APm\n75\nR-50 44M 38.0 58.6\n41.4 34.4 55.1\n36.7\nPVT-S 44M 40.4 62.9\n43.8 37.8 60.1\n40.3\nLIT-Ti 40M 42.0 64.9\n45.6 39.1 61.9\n41.9\nR-101 63M 40.4 61.1\n44.2 36.4 57.7\n38.8\nSwin-Ti 48M 42.2 64.6\n46.2 39.1 61.6\n42.0\nLIT-S 48M 42.9 65.6\n46.9 39.6 62.3\n42.4\nTable\n6: Object detection and instance segmentation perfor-\nmance on the COCO val2017 split using the Mask R-\nCNN framework. APb and APm denote the bounding box\nAP and mask AP, respectively. ‚Äú#P‚Äù refers to the number of\nparameters.\nObject Detection and Instance Segmentation on\nCOCO\nIn this section, we conduct experiments on COCO 2017 (Lin\net al. 2014) dataset to show the performance of LIT on object\ndetection and instance segmentation. COCO is a large-scale\ndataset which contains ‚àº118K images for the training set\nand ‚àº5K images for the validation set. For a fair compar-\nison, we evaluate LIT-Ti and LIT-S on two base detectors:\nRetinaNet (Lin et al. 2017b) and Mask R-CNN (He et al.\n2017). For the experiments with both detectors, we con-\nsider CNN-based ResNet (He et al. 2016) and Transformer-\nbased models including PVT-S (Wang et al. 2021) and Swin-\nTi (Liu et al. 2021). Following common practice (Carion\net al. 2020; Wang et al. 2021), we measure the performance\nof all models by Average Precision (AP) in COCO.\nImplementation details. All models are trained on 8 V100\nGPUs, with 1√óschedule (12 epochs) and a total batch size\nof 16. We use AdamW (Loshchilov and Hutter 2019) opti-\nmizer with a step decay learning rate scheduler. Following\nPVT (Wang et al. 2021), the initial learning rates are set to\n1 √ó10‚àí4 and 2 √ó10‚àí4 for RetinaNet and Mask R-CNN, re-\nspectively. The weight decay is set to1√ó10‚àí4 for all models.\nResults of Swin-Ti based detectors are adopted from Chu et\nal. (Chu et al. 2021). At the training stage, we initialize\nthe backbone with the pretrained weights on ImageNet. The\ntraining images are resized to the shorter size of 800 pixels,\nand the longer size is at most 1333 pixels. During inference,\nwe fix the shorter side of an image to 800 pixels.\nResults on COCO. Table 5 shows the comparisons of dif-\n2040\nBlock 1Block 2\nStage 1\nBlock 1Block 2\nStage 2\nBlock 1Block 2\nStage 3\nBlock 1Block 2\nStage 4\nFigure 3: Attention probabilities of PVT-S with standard MSA layers. For each stage, we visualize the attention map of each\nhead (columns) at selected blocks (rows). All attention maps are averaged over 100 validation images. Each map shows the\nattention probabilities of a query pixel (green rectangle) to other pixels. Darker color indicates higher attention probability and\nvice versa. Best viewed in color. We provide visualizations of all blocks in the supplementary material.\nferent backbones on object detection based on RetinaNet.\nBy comparing LIT with PVT and ResNet counterparts, we\nfind that our model outperforms both backbones on object\ndetection in almost all metrics. Similar results can be found\nin Table 6, where LIT again surpasses compared methods on\nobject detection and instance segmentation using the Mask\nR-CNN framework.\nSemantic Segmentation on ADE20K\nWe conduct experiments on ADE20K (Zhou et al. 2019) to\nshow the performance of LIT models on semantic segmen-\ntation. ADE20K is a widely adopted dataset for semantic\nsegmentation, which has ‚àº20K training images, ‚àº2K val-\nidation images and ‚àº3K test images. For a fair compari-\nson, we evaluate LIT models with Semantic FPN (Kirillov\net al. 2019). Following the common practice in (Wang et al.\n2021), we measure the model performance by mIoU.\nImplementation details. All models are trained on 8 V100\nGPUs, with 8K steps and a total batch size of 16. The\nAdamW (Loshchilov and Hutter 2019) optimizer is adopted\nwith an initial learning rate of 1 √ó10‚àí4. Learning rate is de-\ncayed by the polynomial decay schedule with the power of\n0.9. We set the weight decay to1 √ó10‚àí4. All backbones are\ninitialized with the pretrained weights on ImageNet. At the\ntraining stage, we randomly resize and crop the images to\n512 √ó512. During inference, images are scaled to the short\nsize of 512.\nResults on ADE20K. We compare different backbones on\nthe ADE20K validation set in Table 7. From the results,\nwe observe that LIT-Ti outperforms ResNet-50 and PVT-\nS by 4.6% and 1.5% mIoU, respectively. For LIT-S, our\nmodel again surpasses ResNet-101 and Swin-Ti, with 2.9%\nand 0.2% improvement on mIoU, respectively. The overall\nperformance demonstrates the effectiveness of the proposed\nLIT models for dense prediction tasks.\nBackbone Semantic FPN\nParams\n(M) mIoU (%)\nResNet-50 29 36.7\nPVT\n-S 28 39.8\nLIT\n-Ti 24 41.3\nResNet-101 48 38.8\nSwin-T\ni 32 41.5\nLIT\n-S 32 41.7\nTable\n7: Semantic segmentation performance with different\nbackbones on the ADE20K validation set.\nConclusion and Future Work\nIn this paper, we have introduced LIT, a hierarchical vision\ntransformer which pays less attention in the early stages to\nease the huge computational cost of self-attention modules\nover high-resolution representations. Specifically, LIT ap-\nplies MLP blocks in the first two stages to focus on local\npatterns while employing standard Transformer blocks with\nsufficient heads in the later stages to handle long range de-\npendencies. Moreover, we have proposed a deformable to-\nken merging module, which is learned to adaptively merge\ninformative patches to an output unit, with enhanced ge-\nometric transformations. Extensive experiments on Ima-\ngeNet, COCO and ADE20K have demonstrated that LIT\nachieves better performance compared with existing state-\nof-the-art HVT methods. Future works may include finding\nbetter architectural configurations of LIT with neural archi-\ntecture search (NAS) and improving MLP blocks in the early\nstages to enhance the capability of LIT to encode local pat-\nterns. Besides, one may also consider applying efficient self-\nattention mechanisms (Peng et al. 2021; Wang et al. 2020)\nin the later stages to achieve better efficiency.\n2041\nReferences\nBa, J.; Kiros, J.; and Hinton, G. E. 2016. Layer Normaliza-\ntion. ArXiv, abs/1607.06450.\nBeyer, L.; H ¬¥enaff, O. J.; Kolesnikov, A.; Zhai, X.; and\nvan den Oord, A. 2020. Are we done with ImageNet? arXiv\npreprint arXiv:2006.07159.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-End Object Detection\nwith Transformers. In ECCV, 213‚Äì229.\nChen, B.; Li, P.; Li, C.; Li, B.; Bai, L.; Lin, C.; Sun, M.;\nYan, J.; and Ouyang, W. 2021a. GLiT: Neural Architecture\nSearch for Global and Local Image Transformer. In ICCV.\nChen, M.; Peng, H.; Fu, J.; and Ling, H. 2021b. Auto-\nFormer: Searching Transformers for Visual Recognition. In\nICCV.\nChu, X.; Tian, Z.; Wang, Y .; Zhang, B.; Ren, H.; Wei, X.;\nXia, H.; and Shen, C. 2021. Twins: Revisiting the Design\nof Spatial Attention in Vision Transformers. Arxiv preprint\n2104.13840.\nCordonnier, J.; Loukas, A.; and Jaggi, M. 2020. On the Rela-\ntionship between Self-Attention and Convolutional Layers.\nIn ICLR.\nDai, J.; Qi, H.; Xiong, Y .; Li, Y .; Zhang, G.; Hu, H.; and Wei,\nY . 2017. Deformable Convolutional Networks. In ICCV,\n764‚Äì773.\nd‚ÄôAscoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.;\nBiroli, G.; and Sagun, L. 2021. ConViT: Improving Vision\nTransformers with Soft Convolutional Inductive Biases. In\nICML, 2286‚Äì2296.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In ACL, 4171‚Äì4186.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. ICLR.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J¬¥egou, H.; and Douze, M. 2021. LeViT: a Vision Trans-\nformer in ConvNet‚Äôs Clothing for Faster Inference. arXiv\npreprint arXiv:22104.01136.\nHe, K.; Gkioxari, G.; Doll ¬¥ar, P.; and Girshick, R. B. 2017.\nMask R-CNN. In ICCV, 2980‚Äì2988.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. In CVPR, 770‚Äì778.\nHendrycks, D.; and Gimpel, K. 2016. Gaussian Error Linear\nUnits (GELUs). arXiv: Learning.\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking spatial dimensions of vision transformers.\nIn ICCV.\nHou, Q.; Cheng, M.; Hu, X.; Borji, A.; Tu, Z.; and Torr,\nP. H. S. 2019. Deeply Supervised Salient Object Detection\nwith Short Connections. TPAMI, 41(4): 815‚Äì828.\nHu, H.; Zhang, Z.; Xie, Z.; and Lin, S. 2019. Local Relation\nNetworks for Image Recognition. In ICCV, 3463‚Äì3472.\nIoffe, S.; and Szegedy, C. 2015. Batch Normalization: Ac-\ncelerating Deep Network Training by Reducing Internal Co-\nvariate Shift. In ICML, 448‚Äì456.\nKirillov, A.; Girshick, R. B.; He, K.; and Doll ¬¥ar, P. 2019.\nPanoptic Feature Pyramid Networks. In CVPR, 6399‚Äì6408.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021. LocalViT: Bringing Locality to Vision Transformers.\narXiv preprint arXiv:2104.05707.\nLin, T.; Doll ¬¥ar, P.; Girshick, R. B.; He, K.; Hariharan, B.;\nand Belongie, S. J. 2017a. Feature Pyramid Networks for\nObject Detection. In CVPR, 936‚Äì944.\nLin, T.; Goyal, P.; Girshick, R. B.; He, K.; and Doll ¬¥ar, P.\n2017b. Focal Loss for Dense Object Detection. In ICCV,\n2999‚Äì3007.\nLin, T.; Maire, M.; Belongie, S. J.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ¬¥ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. In Fleet, D. J.; Pajdla,\nT.; Schiele, B.; and Tuytelaars, T., eds.,ECCV, 740‚Äì755.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled weight decay\nregularization. In ICLR.\nLuo, W.; Li, Y .; Urtasun, R.; and Zemel, R. S. 2016. Un-\nderstanding the Effective Receptive Field in Deep Convolu-\ntional Neural Networks. In NeurIPS, 4898‚Äì4906.\nPan, Z.; Zhuang, B.; Liu, J.; He, H.; and Cai, J. 2021. Scal-\nable visual transformers with hierarchical pooling. In ICCV.\nParmar, N.; Ramachandran, P.; Vaswani, A.; Bello, I.; Lev-\nskaya, A.; and Shlens, J. 2019. Stand-Alone Self-Attention\nin Vision Models. In NeurIPS, 68‚Äì80.\nPeng, H.; Pappas, N.; Yogatama, D.; Schwartz, R.; Smith,\nN. A.; and Kong, L. 2021. Random feature attention. In\nICLR.\nRecht, B.; Roelofs, R.; Schmidt, L.; and Shankar, V . 2019.\nDo ImageNet Classifiers Generalize to ImageNet? In ICML,\n5389‚Äì5400.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition chal-\nlenge. IJCV, 211‚Äì252.\nShim, G.; Park, J.; and Kweon, I. S. 2020. Robust\nReference-Based Super-Resolution With Similarity-Aware\nDeformable Convolution. In CVPR, 8422‚Äì8431.\nSrinivas, A.; Lin, T.-Y .; Parmar, N.; Shlens, J.; Abbeel, P.;\nand Vaswani, A. 2021. Bottleneck transformers for visual\nrecognition. In CVPR.\nTenney, I.; Das, D.; and Pavlick, E. 2019. BERT Rediscovers\nthe Classical NLP Pipeline. In Korhonen, A.; Traum, D. R.;\nand M`arquez, L., eds., ACL, 4593‚Äì4601.\nThomas, H.; Qi, C. R.; Deschaud, J.; Marcotegui, B.;\nGoulette, F.; and Guibas, L. J. 2019. KPConv: Flexible and\nDeformable Convolution for Point Clouds. In ICCV, 6410‚Äì\n6419.\n2042\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J¬¥egou, H. 2021a. Training data-efficient image trans-\nformers & distillation through attention. In ICML, 10347‚Äì\n10357.\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and\nJ¬¥egou, H. 2021b. Going deeper with image transformers.\narXiv preprint arXiv:2103.17239.\nVaswani, A.; Ramachandran, P.; Srinivas, A.; Parmar, N.;\nHechtman, B.; and Shlens, J. 2021. Scaling Local Self-\nAttention For Parameter Efficient Visual Backbones. In\nCVPR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In NeurIPS, 5998‚Äì6008.\nWang, S.; Li, B.; Khabsa, M.; Fang, H.; and Ma, H. 2020.\nLinformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In ICCV.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. In ICCV.\nWu, Z.; Su, L.; and Huang, Q. 2019. Cascaded Partial De-\ncoder for Fast and Accurate Salient Object Detection. In\nCVPR, 3907‚Äì3916.\nYan, H.; Li, Z.; Li, W.; Wang, C.; Wu, M.; and Zhang, C.\n2021. ConTNet: Why not use convolution and transformer\nat the same time? arXiv preprint arXiv:2104.13497.\nYuan, K.; Guo, S.; Liu, Z.; Zhou, A.; Yu, F.; and Wu,\nW. 2021a. Incorporating Convolution Designs into Visual\nTransformers. arXiv preprint arXiv:2103.11816.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Tay, F. E.;\nFeng, J.; and Yan, S. 2021b. Tokens-to-token vit: Training\nvision transformers from scratch on imagenet. In ICCV.\nZhou, B.; Zhao, H.; Puig, X.; Xiao, T.; Fidler, S.; Barriuso,\nA.; and Torralba, A. 2019. Semantic Understanding of\nScenes Through the ADE20K Dataset. IJCV, 302‚Äì321.\nZhou, D.; Kang, B.; Jin, X.; Yang, L.; Lian, X.; Hou, Q.; and\nFeng, J. 2021. Deepvit: Towards deeper vision transformer.\narXiv preprint arXiv:2103.11886.\nZhu, X.; Hu, H.; Lin, S.; and Dai, J. 2019. Deformable\nConvNets V2: More Deformable, Better Results. In CVPR,\n9308‚Äì9316.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. In ICLR.\n2043",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7468887567520142
    },
    {
      "name": "Transformer",
      "score": 0.7200571298599243
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6211708784103394
    },
    {
      "name": "Inference",
      "score": 0.5344179272651672
    },
    {
      "name": "Segmentation",
      "score": 0.5168962478637695
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4890161454677582
    },
    {
      "name": "Perceptron",
      "score": 0.41033926606178284
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40308719873428345
    },
    {
      "name": "Machine learning",
      "score": 0.37145519256591797
    },
    {
      "name": "Computer vision",
      "score": 0.34786784648895264
    },
    {
      "name": "Artificial neural network",
      "score": 0.3389618396759033
    },
    {
      "name": "Engineering",
      "score": 0.10683253407478333
    },
    {
      "name": "Voltage",
      "score": 0.08122250437736511
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}