{
  "title": "SiT: Self-supervised vIsion Transformer",
  "url": "https://openalex.org/W3143083666",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5037459105",
      "name": "Sara Atito Ali Ahmed",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100778579",
      "name": "Muhammad Awais",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028209738",
      "name": "Josef Kittler",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2962742544",
    "https://openalex.org/W2122538988",
    "https://openalex.org/W2558661413",
    "https://openalex.org/W2551176409",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2970241862",
    "https://openalex.org/W2599837529",
    "https://openalex.org/W3104240813",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3104683525",
    "https://openalex.org/W2963826423",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W2122925692",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W219040644",
    "https://openalex.org/W2990500698",
    "https://openalex.org/W2798680770",
    "https://openalex.org/W2971155163",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W2562637781",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2411541852",
    "https://openalex.org/W2883725317",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W2963826370",
    "https://openalex.org/W2148349024",
    "https://openalex.org/W2788552663",
    "https://openalex.org/W2913939497",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W343636949",
    "https://openalex.org/W3016181583",
    "https://openalex.org/W2308529009",
    "https://openalex.org/W3045599073",
    "https://openalex.org/W2025378551",
    "https://openalex.org/W2118858186",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W2963495051",
    "https://openalex.org/W2887997457"
  ],
  "abstract": "Self-supervised learning methods are gaining increasing traction in computer vision due to their recent success in reducing the gap with supervised learning. In natural language processing (NLP) self-supervised learning and transformers are already the methods of choice. The recent literature suggests that the transformers are becoming increasingly popular also in computer vision. So far, the vision transformers have been shown to work well when pretrained either using a large scale supervised data or with some kind of co-supervision, e.g. in terms of teacher network. These supervised pretrained vision transformers achieve very good results in downstream tasks with minimal changes. In this work we investigate the merits of self-supervised learning for pretraining image/vision transformers and then using them for downstream classification tasks. We propose Self-supervised vIsion Transformers (SiT) and discuss several self-supervised training mechanisms to obtain a pretext model. The architectural flexibility of SiT allows us to use it as an autoencoder and work with multiple self-supervised tasks seamlessly. We show that a pretrained SiT can be finetuned for a downstream classification task on small scale datasets, consisting of a few thousand images rather than several millions. The proposed approach is evaluated on standard datasets using common protocols. The results demonstrate the strength of the transformers and their suitability for self-supervised learning. We outperformed existing self-supervised learning methods by large margin. We also observed that SiT is good for few shot learning and also showed that it is learning useful representation by simply training a linear classifier on top of the learned features from SiT. Pretraining, finetuning, and evaluation codes will be available under: this https URL.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 1\nSiT: Self-supervised vIsion Transformer\nSara Atito, Member IEEE, Muhammad Awais, Member IEEE, and Josef Kittler, Life Member, IEEE\nAbstract—In Natural Language Processing (NLP), Self-supervised Learning (SSL) and transformers are already the methods of\nchoice due to the tremendous success of attention based self-supervised transformer models like BERT [1] and GPT [2]. So far, the\nvision transformers, adopted from NLP transformers, have been shown to work well when pretrained either using a large scale\nsupervised data [3] or with some kind of co-supervision, e.g. in terms of teacher network [4]. These supervised pretrained vision\ntransformers achieve outstanding results in downstream tasks with minimal changes [3], [4], [5]. Self-supervised Pretraining (SSP) is\nstill not the method of choice for computer vision due to performance gap [3], however, SSL is gaining increasing traction in computer\nvision as the performance gap between Supervised Pretraining (SP) and SSP is reducing for downstream applications, like\nclassiﬁcation, localisation, segmentation, etc. Self-supervised vision Transformers (SiT) is the ﬁrst work which establishes that SSP\ncan outperform SP for downstream applications, establishing SSP as a more suitable choice for pretraining vision transformers.\nSiT is the ﬁrst masked image modelling work for vision transformers. At its core SiT builds the idea of Group Masked Model Learning\n(GMML), a simple masked autoencoder framework to obtain a pretext model. The architectural ﬂexibility of vision transformers allows\nus to use SiT as an autoencoder and work with multiple self-supervised tasks seamlessly. The proposed approach is evaluated on\nstandard datasets using common protocols. The results demonstrate the suitability of the GMML framework for SSL and vision\ntransformers. SiT consistently outperforms supervised pretraining as well as prior arts with a large margin. Unlike other vision\ntransformer based pretraining methods, SiT performs very strongly on small and medium scale datasets as well. Thanks to SiT, the\nvision transformers can outperform (perform on par with) Convolutional Neural Network (CNN) counterpart for small and medium\ndatasets without using any external data for pretraining, overcoming the problem of data-hungry vision transformers. Pretraining,\nﬁnetuning, and evaluation codes are available under: https://github.com/Sara-Ahmed/SiT.\nImpact: We proposed GMML framework in SiT for self-supervised learning of vision transformers at the beginning of 2021 using\nmasked autoencoder with reconstruction loss, however the idea is generally applicable to other losses as shown in later studies [6], [7],\n[8]. At the time of conception of SiT, the merits of GMML were shown employing small models and small/medium scale datasets due to\nextremely restricted computational resources. Since then, GMML has been widely adopted in computer vision and other related ﬁelds.\nTowards the end of 2021, SIMMIM [9] and MAE [10] extended GMML with reconstruction loss using huge vision transformers on large\nscale datasets, like ImageNet-1K [11]. GMML is now the leading SSL framework on multiple application areas, giving sate-of-the-art\nresults for image classiﬁcation [7], segmentation [9], audio analysis [12], medical image analysis [13], [14], video representation [15],\netc. In short MIM/GMML is enabling the computer vision community to enjoy the same success in SSL which NLP community has\nenjoyed for BERT. SiT performs much better than prior art and post art when trained using small to medium scale dataset without any\nexternal data and performs better than prior art and on par with post art which have adopted GMML framework when pretrained on\nlarge scale datasets.\nIndex Terms—Masked Image Modelling (MIM), Masked autoencoders, Group Masked Model Learning (GMML), Vision Transformer,\nSelf-supervised Learning, Discriminative Learning, Image Classiﬁcation, Transformer-based Autoencoders.\n!\n1 I NTRODUCTION\nR\nECENT trends, particularly in NLP , showed that self-\nsupervised pretraining can improve the performance\nof downstream tasks signiﬁcantly [1], [16]. Similar trends\nhave been observed in speech recognition [17] and com-\nputer vision applications [18], [19], [20], [21]. The self-\nsupervised pretraining, particularly in conjunction with\ntransformers [22], are the models of choice for NLP [1],\n[16]. The success of SSL comes at the cost of massive\ndatasets and huge capacity models. For instance, NLP based\ntransformers are trained on hundreds of billions of words\nconsisting of models with several billion parameters [16].\nThe recent success of Transformers in image classiﬁcation [3]\ngenerated a lot of interest in the computer vision commu-\nnity. However, the pretraining of vision transformer mainly\nthrive using very large scale datasets using supervised\nlearning, e.g., datasets consisting of hundred of millions of\n• Centre for Vision, Speech and Signal Processing (CVSSP), University of\nSurrey, Guildford, United Kingdom\n• {s.a.ahmed,muhammad.awais,j.kittler}@surrey.ac.uk\nManuscript received ?? ??, 2021; revised ?? ??, 2021.\nlabelled samples [3]. This particular data-hungry nature of\nvision transformers arise due to lack of so called inductive\nbias [7]. Very recently vision transformer have been shown\nto perform well on ImageNet-1K without external data [4].\nHowever, they need distillation approaches and guidance\nfrom CNNs counterparts. In short, pretraining a neural\nnetwork using large-scale supervised datasets is a norm\nin computer vision in order to obtain better performance.\nHowever, the manual annotation of training data is quite\nexpensive, despite the engineering innovations of crowd\nsourcing campaigns. More importantly development of the\nvisual cortex and visual memory seems to depends upon the\nvisual experience [23], [24]. This is suggested by the early\nplasticity experiments on kittens [23], [24] which support\nthe argument that some important aspect of visual percep-\ntion are acquired through learning and visual experiences.\nTraining of DNNs via supervised learning with one label per\nimage may corresponds to having limited visual experience\nfor trained DNNs because DNNs may not learn from rich\nvisual information present in other concepts in the natural\nimages. This may effect the generalisation capability of the\narXiv:2104.03602v3  [cs.CV]  26 Dec 2022\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 2\nDNNs. Furthermore, learning using labels as a supervisory\nsignal, particularly one label per natural image, can be\nthough of as an ill-posed problem. The DNNs may map an\ninput image to a target class and in the process have to be\ninvariant to other concepts. In natural images there could be\nmultiple concepts common between different images while\nthe single annotated label could be different among them.\nThis may cause confusion for the DNN and may result in\nsub-expressive features from labelled data. Labelling every\nsalient concept in every images be also be infeasible. To\naddress these limitation, SSL methods [25], [26], [27], [18],\n[20], [21], [28] have been proposed to train more gener-\nalisable DNNs suitable for several downstream tasks and\nconstruct image representations that are semantically mean-\ningful from unlabelled data.\nSelf-supervised methods can roughly be categorised in\nto generative and discriminative approaches. Generative ap-\nproaches [29], [30], [31] learn to model the distribution of the\ndata. However, data modelling generally is computationally\nexpensive and may not be necessary for representation\nlearning in all scenarios. On the other hand, discriminative\napproaches, typically implemented in a contrastive learning\nframework [32], [33], [19], [34] or using pre-text tasks [35],\n[36], [37], demonstrate the ability to obtain better gener-\nalised representations with modest computational require-\nments.\nThe primary focus of contrastive learning is to learn\nimage embeddings that are invariant to different augmented\nviews of the same image while being discriminative among\ndifferent images. Despite the impressive results achieved\nby contrastive learning methods, they often disregard the\nlearning of contextual representations as they are focusing\non one global transformation invariant representation for\nthe whole image. While each and every concept in the\nimage and context within that concept and context around\nthat concept is important for in depth understanding of\nthe image. Moreover, most contrastive learning approaches\nsuffer from collapse, a trivial constant solutions. To avoid\nthe collapse these, methods use careful implementation\ndetails, e.g. stop gradients, large batch size, exponential\nmoving average based teacher network, centring, asym-\nmetric projection head, etc. For more detail and context\naware representations, alternative pretext tasks, such as\nreconstruction or recovery of missing information based\napproaches, might be better suited. In recent years, a stream\nof novel pretext tasks have been proposed in the literature,\nincluding inpainting patches [38], colourisation [39], [40],\n[35], relative patch location [29], solving jigsaw puzzles [41],\n[42], cross-channel prediction [43], predicting noise [44],\npredicting image rotations [36], spotting artefacts [37], etc.\nThese pretext tasks have been explored for SSL using CNNs\nframeworks. Different from them we developed pretext\nframework for Vision Transformers (ViT) which can capture\nlocal and global context seamlessly. Unlike CNNs trans-\nformers do not make any assumption about local inductive\nbias (statistical correlation in the neighbourhood), hence,\nin order to model useful inductive bias, ViTs require huge\namount of data to perform on par with CNNs. The proposed\nGMML framework enables ViTs to learn the useful local\ninductive bias even from small amount of data and enables\nViTs to perform on par with CNNs even on small data while\nmaintaining the advantage on large data.\nThe core of SiT is built upon the simple idea of GMML.\nDifferent from existing SSL approaches, GMML leverage the\ninformation redundancy and complementarity in the vision\ntransformers by learning to recover/reconstruct local con-\ntent by linking it to context. In spirit, this principle is similar\nto the masked language modelling (MLM) used in BERT [1]\nwhich recover masked words from context. The principle\nis also inspired from word2vec [45] which predict words\nfrom the context. In computer vision, we take the inspiration\nfrom the principle of denoising autoencoder [46] and from\nthe idea of context encoder [38] which has been studied\nfor unsupervised learning using CNNs. The GMML extends\nthe principles of MLM, denoising autoencoders, and context\nencoders to vision transformers for self-supervised learning.\nThis is achieved by three principles: i) learning to recover\nthe input stimulus by a mechanism akin to autoencoding, im-\nplemented by means of random data tokens perturbation using\nmasking of groups of connected tokens, etc. ii) a perception-action\nmechanism [47], which learns to recognise an action from its\nimpact on perception , and iii) learning the notion of similarity\nof content from the preservation of content identity in the data.\nThe proposed SSL approach is instrumental in extracting\nan intrinsic data model and is admirably able to adapt to\ndownstream tasks by ﬁnetuning. The GMML establishes\nitself as a strong standalone SSL framework surpassing\nall existing SSL methods and additionally outperforming\nsupervised pretraining for the ﬁrst time. Thanks to architec-\ntural ﬂexibility of transformers, SiT further extend GMML\nand leverages the advantages of both contrastive learning\nand pre-text approaches.\nThe main contributions of this study are summarised as\nfollows:\n1) We propose Group Masked Model Learning (GMML), a\nnovel framework for self-supervised learning of visual\nrepresentations using vision transformers. GMML trains\nDNNs and learns rich representations by recovering\nlarge amount (upto 70%) of missing visual information\nby groups of masked tokens using the context present in\nthe visible tokens.\n2) We endow the GMML architecture with a decoder and\ndemonstrate that it can be implemented by essentially\nusing a 2-layer perceptron, thanks to the intrinsic char-\nacteristics of the transformer. This transformer based\nautoencoder avoids the need for a whole decoder block\nwhich is typically present in CNNs based encoder-\ndecoder architectures.\n3) Drawing on the natural ability of the autoencoding\ntransformer to support multi-task learning, we develop\na strong self-supervised framework which jointly opti-\nmises the reconstruction (GMML) and contrastive losses.\n4) We illustrate the effectiveness of the proposed framework\non standard benchmarks following different evaluation\nprotocols including domain transfer and ﬁnetuning.\n5) We outperform the concurrent and post arts in different\ndatasets with a large margin reaching +5.4% improve-\nments when the models are pretrained on small datasets\nand obtain on par performance with the state-of-the-art\nwhen the models are pretrained on large-scale datasets.\nThere are three key observations summarised as follow:\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 3\n1) The ability of SiT in clustering the data without any form\nof supervision.\n2) With Sit, it is possible to train data hungry transformers\non tiny datasets with just a few thousand sample such as\nFlowers, Pets, CIFAR10, etc without distillation.\n3) Most importantly to best of our knowledge at the be-\nginning of 2021 SiT became the ﬁrst work showing that\nself-supervised pretraining can consistently outperforms\nsupervised pretraining for the vision classiﬁcation down-\nstream tasks using transformers.\nThe paper is structured as follows. Section 2 provides\na background on the state-of-the-art self-supervised tech-\nniques. In Section 3, the proposed self-supervised frame-\nwork using vision transformer is explained. The experimen-\ntal analysis and a discussion of the obtained results are\nshown in Section 4. Finally, conclusions of this study are\npresented in Section 5.\n2 R ELATED WORKS\n2.1 Comparison with Prior Art\nDiscriminative approaches to SSL [32], [33], [19], [34] typi-\ncally have been demonstrated to learn better representations\nthan generative approaches [29], [30], [31] and hence will\nbe focus on the literature review. Discriminative approaches\nare typically implemented using pre-text tasks or in a con-\ntrastive learning framework. The basic pretraining mecha-\nnism of the handcrafted pre-text tasks is autoencoding [48],\nwhich forces a network to ﬁnd a representation that allows\nthe reconstruction of the input image, even if corrupted\nby perturbations or noise. Many self-supervised pretext\ntasks manipulate the input data to obtain better image\nrepresentations. For example, Pathak et al. [38] trained a\nconvolutional network to predict the content of arbitrary\nmissing regions in an image based on the rest of the image.\nThe motivation behind this work is that for the network\nto produce plausible hypothesis for the missing parts, the\nencoder needs to understand the content of the entire im-\nage. In the same line, Zhang et al. [39] proposed an image\ncolourisation task by predicting a coloured version of the\ngiven grey-scale input image and used class re-balancing to\nincrease the diversity of the predicted colours. Furthermore,\nDoersch et al. [29] presented one of the pioneering works\nof using spatial context information for feature learning by\ntraining a convolutional network to recognise the relative\npositions of random pairs of image patches. Following this\nidea, several methods were proposed to learn image features\nby solving even more difﬁcult spatial context tasks (e.g.\njigsaw puzzles [41], [42]). Training the network with such\nobjective by using the within-image context encourages the\nnetwork to learn local feature representations while ignore\nglobal context. Gidaris et al. [36] proposed RotNet, a con-\nvolutional network that learns image features by training\nthe network to recognise a pre-deﬁned 2d rotation that is\napplied to the input image. Using this simple task DNNs\ncan learn global image level visual representations based\non the assumption that network will have understanding of\nthe object if it predict the objects’ orientation. Overall, such\npretext based approaches are powerful in learning useful\nrepresentations from unlabeled data, yet, they limit the gen-\nerality of learning discriminative representations between\ndifferent samples, where contrastive approaches are better\nsuited.\nContrastive approaches [49], [50], [33], [51], [19], [34],\n[20] train the network by bringing the representations\nof different augmented views of the same image closer\nand spreading the representations of views from different\nimages apart. In general contrastive learning based ap-\nproaches tend to perform better than pretext task based\napproaches. Chen et al.[19] proposed SimCLR, a contrastive\nself-supervised learning algorithm without requiring spe-\ncialised architectures or a memory bank. SimCLR is a\nsimple framework to learn representations from unlabeled\nimages based on heavy data augmentation by maximising\nthe similarity between two augmented views coming from\nthe same image. Training the network with such objective\nimproves the quality of the learnt representations in dis-\ncriminating between samples drastically. Contrastive learn-\ning approaches either use large batch sizes [19] or memory\nbanks [50], [18] in order to have informative negative sam-\nples in the batch. These approaches typically use various\ntricks to avoid representation collapse.\nDeep clustering-based methods [32], [52], [53], [21], [54]\nlearn representation by clustering the images in the em-\nbedding space. DeepCluster [32] clusters data points using\nthe current representation to produce labels for the next\nrepresentation. The cluster index of each sample is then\nused as a classiﬁcation target for the new representation.\nThis approach is computationally expensive as it requires\na clustering phase with precautions to avoid collapsing to\ntrivial solutions.\nHjelm et al. [33] investigated the use of mutual infor-\nmation for unsupervised representation learning through\nDeep InfoMax by maximising the mutual information in\nglobal and local scales across structural patches in an image\nfollowing the InfoMax principle [55].\nPatacchiola and Storkey [34] proposed a self-supervised\nformulation of relational reasoning that allows a learner to\nbootstrap a signal from the information implicit in unla-\nbelled data. Speciﬁcally, they used a relation network as\na learnable function on the unlabeled dataset to quantify\nthe relationships between augmented views of the same\nobject (i.e. intra-reasoning) and between different objects\nin different scenes (i.e. inter-reasoning) which could help\nlearners to distinguish the object based on their differences.\nIn this work, we leverage the advantage of both pre-text\napproaches and contrastive learning approaches to learn\nuseful as well as discriminative representations between\ndifferent samples employing a simple transformer-based\nframework for self-supervised learning.\n2.2 Comparison with Post Art\nRecently, a manifold of methods have used the principals\noutlined in GMML at the beginning of 2021. In this section,\nwe will brieﬂy introduce the similarities and differences\nbetween GMML and some of the most popular post art.\nTwo notable post arts are SimMIM [56] and MAE [57].\nSimilar to GMML, both SimMIM and MAE use the principal\nof transformer based masked autoencoder. Both of them\nmask a high proportion of data-tokens randomly. However,\nwe note that masking a very high proportion of the data-\ntoken essentially deﬁnes groups of connected tokens to be\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 4\nLinear Projection of Flattened Patches \n Projection to Image Space \nVision Transformer \nData Tokens / Image\nPixel Corruption\nOriginal Image\nReconstructed Image\nPosition\nEmbedding\nContrastive\nHead \nContrastive\nEmbedding\nFig. 1: Self-supervised vIsion Transformer (SiT)\nmasked. We also note that their optimal masking proportion\nis very similar to GMML. Following DropToken idea in\nVATT [58], MAE discard the masked tokens for encoder and\nuse them in decoder to reconstruct the image. However, this\ndropping of masked tokens require MAE to use complex\ndecoder consisting of six to twelve layers of transformers,\nunlike GMML which use 2 pointwise convolutional layers.\nWe noticed that wall clock time for the pretraining of MAE\nand GMML is similar for ViT-B, while training time for ViT-\nS is much slower for MAE as compared to GMML due to\ncomplex decoder of MAE. Furthermore, due to lack of mod-\nelling the inductive bias, the performance of MAE degrade\nlargely for small datasets and MAE only performs on par\nwith GMML for large dataset. SimMIM is very similar to\nGMML the only meaningful difference is that GMML uses\nnoise and alien concepts in addition of masking with zero\nwhile SimMIM just uses masking with zeros. Besides, the\ncorruption in SimMIM is applied after the patch projection\nblock whilst in GMML, the corruption is applied directly to\nthe image pixels.\nAnother noticeable method in post art is BeIT [6]. BeIT\nuses external knowledge by using an encoder trained with-\nout supervision, to group visual patches in order to deﬁne\na visual vocabulary. This enables the use of cross entropy as\na loss function, like in BERT [1]. However, unlike BERT the\nclasses are coming from external knowledge source albeit\ntrained unsupervisedly. It can be considered as an expensive\nand extreme case of patch level distillation via supervised\nor unsupervised encoder. Secondly, it will inherit issues of\nvisual vocabulary, like, a ﬁxed number of visual words,\na quantisation error, visual ambiguity when assigning to\ncluster centres etc.\n3 M ETHODOLOGY\nSupervised learning, as demonstrated in [3], allows the\ntransformer to learn a bottleneck representation where the\nmixing of content and context is centred primarily about\nthe class token. This creates a rather superﬁcial model of\nthe data, and its linking to labels requires a huge number\nof samples for training. In contrast, GMML based unsuper-\nvised learning exploits information redundancy and com-\nplementarity in the image data by learning to reconstruct lo-\ncal content by integrating it with context. The proposed self-\nsupervised learning approach is instrumental in extracting\nan intrinsic data model, that is robust to perturbations and is\nadmirably able to adapt to downstream tasks by ﬁnetuning.\nThe proposed approach offers remarkable advantages:\n• The self-supervised transformer can be trained with\nunlabelled data.\n• The amount of labelled training data required for ﬁne-\ntuning to learn a downstream task is two orders of\nmagnitude lower than the counterpart needed for direct\ntraining.\n• The total amount of training data (labelled and unla-\nbelled) is also several orders of magnitude lower.\n• The performance achieved is signiﬁcantly better than\nstate-of-the-art self-supervised methods.\nThe proposed methodology of transformer pretraining by\nself-supervision is expected to have a signiﬁcant impact on\nthe advancement of science by enabling the wider research\ncommunity starved of resources to contribute to deep learn-\ning.\nThus the main goal of this work is to learn a repre-\nsentation of the data in an unsupervised fashion. This is\nachieved by recovering partially masked or transformed\nlocal parts of the image represented by data-tokens at the\ninput of the vision transformer. The underlying hypothesis\nis that, by recovering the corrupted tokens/parts of an im-\nage from the uncorrupted tokens/part based on the context\nfrom the whole visual ﬁeld, the network will implicitly\nlearn the notion of visual integrity. This notion of visual\nintegrity is further enhanced by using pseudo labels that\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 5\ncan be generated automatically based on some attributes\nof the data. Learning from recovery of the transformed\nparts and learning from pseudo label may seem different\nbut the underlying motivation behind both kinds of self-\nsupervised learning mechanisms is the same, i.e., learning\nvisual integrity. For example, intuitively the network will\nonly be able to recover the pseudo labels if it learns the\ncharacteristic properties of visual stimuli corresponding to\nspeciﬁc actions impacting on the visual input. The weights\nof the learned model can then be employed as an initialisa-\ntion point for any downstream task like image classiﬁcation,\nobject detection, segmentation, etc. To achieve this goal, we\npropose a Self-supervised vIsion Transformer (SiT) in which\nthe model is trained via group masked model learning and\nto estimate different geometric transformations applied to\nthe input image and hence, better image representation can\nbe obtained.\n3.1 Self-Supervised Vision Transformer\nTransformer [22] has shown great success in various natural\nlanguage processing tasks [1], [59], [16]. Recently, many\nresearchers attempted to explore the beneﬁts of transformer-\nbased models in computer vision tasks [3], [4]. In this\nwork, we introduce Self-supervised vIsion Transformer\n(SiT) adapted from Vision Transformer (ViT) proposed by\n[3] with some modiﬁcations.\nVision Transformer [3] receives as input a sequence\nof patches obtained by tokenizing the input image x ∈\nRH×W×C into n ﬂattened tow-dimensional patches of size\np1 ×p2 ×Cpixels, where H, W, and Care the height, width,\nand number of channels of the input image, ( p1 ×p2) is the\npatch size, and nis the number of patches, i.e. n= H\np1\n×W\np2\n.\nEach patch is then projected with a linear layer to Dhidden\ndimensions. The whole operation can also be implemented\nsimply by a convolutional layer with kernel size p1 ×p2,\nnumber of input and output channels Cand Drespectively.\nIn order to retain the relative spatial relation between the\npatches, ﬁxed or learnable position embeddings are added\nto the patch embeddings as an input to the Transformer\nencoder.\nThe Transformer encoder consists of L consecutive\nMulti-head Self-Attention (MSA) and Multi-Layer Percep-\ntron (MLP) blocks. The MSA layer is deﬁned by h self-\nattention heads where each head outputs a sequence of size\nn×d. The employed self attention mechanism is based on a\ntrainable triplet of (query, key, value). Each query vector in\nQ ∈Rn×dis matched against a set of key vectorsK ∈Rn×d,\nand the output is then normalised with a softmax function\nand multiplied by a set of values V ∈ Rn×d. Thus, the\noutput of the self-attention block is the weighted sum of\nV as shown in Equation 1. The output sequences of each\nblock are then concatenated into n×dhand projected by a\nlinear layer into n×Dsequence.\nSelfAttention(Q,K,V) = Softmax(QKT\n√\nd\n)V (1)\nTo serve the classiﬁcation task, a trainable vector (i.e\nclass token) is appended to the input sequence of the patch\ntokens and goes through the Transformer encoder. Finally, a\nclassiﬁcation head is added to the output of the Transformer\nencoder corresponding to the class token. The classiﬁcation\nhead is implemented by a single linear layer that projects\nthe class embeddings to the number of classes.\nUnlike ViT [3], our work is based on unsupervised learn-\ning/pretraining, hence, classiﬁcation token is not required.\nInstead we have a contrastive token, beside the data tokens\n(image patches token) used for image reconstruction, to\nserve the proposed self-supervised pretraining tasks. The\ncontrastive token adopted from SimCLR [19], described in\nSection 3.2.2, is employed to serve the contrastive prediction\ntask. The main architecture of our proposed network is\nshown in Figure 1.\n3.2 Self-Supervised Tasks\nAs noted earlier, the transformer architecture allows seam-\nless integration of multiple task learning simultaneously.\nWe leverage this strength of the transformers to train SiT\nwith two different objectives: (1) Image reconstruction based\non GMML and (2) Contrastive learning. In the rest of the\nsection, we describe the different types of self-supervised\ntasks employed in this work.\n3.2.1 Task #1: Image Reconstruction\nFor image reconstruction, we propose to use the transformer\nas an autoencoder, i.e., visual transformer autoencoder. Un-\nlike CNNs based autoencoders which require encoder and\nexpensive decoders consisting of convolutional and trans-\nposed convolution layers, the decoder in the transformer\nautoencoder can be implemented using a light decoder. One\nlimitation of CNN based encoder is the step of information\nsummarisation in which the information is essentially dis-\ncarded using strided convolutions or max pool operations.\nThis information is then recovered by series of upsampling\nand convolution operations (or transposed convolutions)\nwith skip connection between encoder and decoder. By\nanalogy to auto-encoders, our network is trained to re-\nconstruct the input image through the output tokens of\nthe transformer. To learn better semantic representations of\nthe input images, we apply Group Masked Model Learn-\ning (GMML) by applying several transformations to local\npatches of the image. Unlike standard masked tokens in\nBERT, we apply these local transformations to a block of\nneighbouring tokens arranged spatially (in 2D rather than in\nsequence only). In BERT and other NLP based transformers,\nit makes sense to mask only one token, as a single token can\nrepresent semantic meaning. However, for visual signals,\nit is critical to transform neighbouring tokens consisting\nof either one or more semantic concepts. The aim is to\nrecover these transformed local parts at the output of SiT.\nIn doing so, SiT implicitly learns the semantic concepts\nin the image. It should be noted that these transformed\ntokens can be either on the foreground object or on the\nbackground, and recovering these tokens is equally valid for\nboth scenarios. Indeed while modelling the visual signal in\nthis way we are moving away from the notion of foreground\nand background, and every part of the content is considered\nas a semantic concept, whether it is a horse grazing in\na meadow, or the meadow itself. The intuition is that by\nmodelling all semantic concepts, SiT will generalise better\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 6\nfor unseen tasks, whether they are related to an object, a\ndistributed object, or to the whole visual signal.\nImage inpainting is a simple but effective pre-text task\nfor self-supervision, which proceeds by training a network\nto predict arbitrary transformed regions based on the con-\ntext. This context can be from the same object on which\nthe transformed region is applied or from the surrounding\nobjects/concepts. With CNNs this context is deﬁned by\nthe, so called, receptive ﬁeld, while with transformers the\ncontext consists of the whole image. The motivation behind\nimage inpainting is that the network is required to learn\nthe knowledge including the colour, texture and structure of\nthe objects/concepts to infer the missing areas. In this work,\nwe employed two types of image inpainting, random drop-\nping, by randomly replacing neighbouring patches from the\nimage with random noise, and random replacement, by\nrandomly replacing patches from the image with patches\nfrom another image.\nThe objective of the image reconstruction is to restore the\noriginal image from the corrupted image. For this task, we\nuse the ℓ1-loss between the original and the reconstructed\nimage as shown in Equation 2. Although, ℓ2-loss gener-\nally converges faster than ℓ1-loss, ℓ2-loss is prone to over-\nsmooth the edges for image restoration [60]. Therefore, ℓ1-\nloss is commonly used for image-to-image processing more\nthan ℓ2-loss.\nLrecons(W) = 1\nN\nN∑\ni\n||xi −SiTrecons( ¯xi)|| (2)\nWhere ||.||is the ℓ1 norm, xi is the input image, ¯xi is the\ncorrupted image, N is the batch size, and W denotes the\nparameters of the transformer to be learned during train-\ning. Finally, SiTrecons(.) returns the reconstructed image\nby feeding the output of the data tokens of the backbone\nE(.), i.e. vision transformer, to the light decoder D(.). Thus,\nSiTrecons(.) = D(E(.)[datatokens])\n3.2.2 Task #2: Contrastive learning\nIn self-supervised learning, we do not have any concept\nlabels for the training data. However, by applying geometric\nand perturbation transformations to a training sample, we\ndo not change the perceptual identity of the content, and\nthe transformer should produce for all such synthetically-\ngenerated content-matching pairs a similar output. We\nadopt the cosine similarity as the underlying measure of\nsimilarity of representation. Inspired by recent contrastive\nlearning algorithms [61], we incorporated a contrastive loss\nto the objective function where the network is trained\nto minimise the distance between positive pairs, i.e. aug-\nmented images coming from the same input image, and\nmaximise the distance between negative pairs, i.e. samples\ncoming from different input images. In particular, we em-\nployed the normalised temperature-scaled softmax similar-\nity [50], [33], [19] between a sample xi and any other point\nxj deﬁned as follows:\nℓxi,xj\ncontr(W) = esim(SiTcontr(xi), SiTcontr(xj))/τ\n∑2N\nk=1,k̸=iesim(SiTcontr(xi), SiTcontr(xk))/τ (3)\nwhere SiTcontr(.) denotes the image embedding coming\nfrom the contrastive head by feeding the output of the class\ntoken of the backbone E(.), to the light contrastive head\nContr(.). Thus, SiTContr(.) = Contr(E(.)[classtoken]). As\nfor sim(., .), it is the dot product of theℓ2 normalised inputs,\ni.e. cosine similarity, and τ denotes a constant temperature\nparameter which we set to 0.2 throughout the experiments\nas suggested by [19]. Rather than using the cosine similarity\nmeasure directly, the normalised softmax in (3) has the\nadvantage that its optimisation enhances the similarity of\nmatching pairs, as well as the dissimilarity of negative pairs.\nNow, let x˜j be a sample matching in content the sample,\nxj. The contrastive loss, Lcontr(W), is then deﬁned as the\narithmetic mean over all positive pairs in the batch of the\ncross entropy of their normalised similarities, i.e.\nLcontr(W) = −1\nN\nN∑\nj=1\nlog ℓ\nxj,x˜j\ncontr(W) (4)\nTo perform the contrastive learning, we feed the network\nthe 2 clean and corrupted augmented views from each\nsample. The loss is then calculated by matching the cross\nrepresentation between the two views, i.e. matching the\nrepresentation of the clean ﬁrst view with the corrupted\nsecond view and the representation of the clean second view\nwith the corrupted ﬁrst view.\nThere are two ways to obtain the representation of\nthe clean and corrupted views: (1) Feeding both the clean\nimages and the corrupted images to the same encoder (as\na siamese network) and stop the back-propagation when\nclean images are passed to the encoder, (2) Adopting a\nmomentum encoder where the corrupted images are passed\nto the encoder and the clean images are passed to the\nmomentum encoder.\nIn this work, we adopted the second strategy as we\nfound that the performance when employing a momentum\nencoder is slightly better. For the momentum encoder, we\nemployed the same architecture as the encoder and the\nweights of the momentum encoder is updated using expo-\nnential moving average of the encoder weights.\n3.3 End-to-End Self-Supervised Training\nFor a given mini-batch consisting of N samples, two dif-\nferent random augmentations are applied to each sample.\nAugmented images created from the same samples are\nconsidered to constitute positive pairs and images coming\nfrom different samples are considered as negative pairs.\nNext, GMML image-based manipulation is applied to both\nimages. The network is then trained to reconstruct the\noriginal image after the applied corruptions and maximise\nthe cosine similarity between the positive pairs within the\nbatch. The overall loss function is shown in Equation 5.\nLtotal(W) = α×Lrecons(W) + Lcontr(W) (5)\nNote that α is the scaling factors of our multi-task\nobjective function. We set αto a high value, i.e. α= 5 when\nthe model is pretrained on small-scale datasets to enables\nViTs to learn the useful local inductive bias. In the case of\nlarge-scale datasets, e.g. ImageNet-1K, we set α= 1.\nAfter the self-supervised training, we apply transfer\nlearning by replacing the contrastive head to output cnodes\ncorresponding to the number of classes in the downstream\ntask.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 7\n(a) Samples from training set\n (b) Samples from testing set\n (c) Images from internet\nFig. 2: Reconstructed images from our trained SiT model. The images are randomly obtained from (a) Training data, (b) Test\ndata, and (c) From the internet. Each row refers to the original images, corrupted images, and the reconstructed images,\nrespectively. We applied moderate corruption for demonstration purposes.\n4 E XPERIMENTAL RESULTS\nThe common evaluation to demonstrate the generalisation\nof the learnt features by self-supervised methods is to\npretrain the model in unsupervised fashion, followed by\nﬁnetuning the model on a downstream task like image clas-\nsiﬁcation, object detection, segmentation, etc. In this work,\nwe conduct several experiments on different well-known\nmulti-class and multi-label datasets (Table 1) as well as on\nvideo instance segmentation task to show the effectiveness\nof our proposed self-supervised vision transformer.\nTABLE 1: Statistics of the employed datasets.\nDataset # Classes # Training\nSamples\n# Testing\nSamples\nMulti-class datasets\nFlowers [62] 102 2,040 6,149\nPets [63] 37 3,680 3,669\nSTL10 [64] 10 5,000 8,000\nCUB200 [65] 200 5,994 5,794\nAircraft [66] 100 6,667 3,333\nCars [67] 196 8,144 8,041\nCIFAR10 [68] 10 50,000 10,000\nCIFAR100 [68] 100 50,000 10,000\nImageNet-1K[11] 1,000 1.28M 50,000\nMulti-label datasets\nPascal VOC [69] 20 5,011 4,952\nMS-COCO [70] 80 82,081 40,137\nVisual-Genome [71] 500 98,249 10,000\n4.1 Implementation Details\nWe implement the self-supervised architecture using vision\ntransformer [3], mostly employing the small variant of vi-\nsion transformers (ViT-S) with 224 ×224 input image size,\n16 ×16 patch size, 384 hidden dimension, 12 consecutive\nMSA and MLP blocks, and 6 heads on each multi-head self-\nattention layer. There are 21 million parameters in total in\nthis architecture.\nFor the image corruption, we either replace random\npatches from the image with noise or with patches from\nother images. The width and height of the corrupted patches\nvaries from 5% to 25% of the input image size with the\noverall replacement rate of up to 70% in the case of noise\nreplacement and up to 30% in the case of replacement from\nother images. The image reconstruction head consists of 2\nfully connected layers with 2048 neurons and GeLU [74]\nnon-linearity each, followed by a transposed convolution to\nreturn back to the image space.\nFor the contrastive learning, we employed SimCLR [19]\nwith the temperature parameter set to 0.2. The contrastive\nlearning head consists of two fully connected layers with\n4096 neurons, batch normalisation layer, and GeLU non-\nlinearity each, followed by a linear layer with 256 output\nnodes that represent the image embeddings. The momen-\ntum encoder is updated using exponential moving average\nof the encoder weights with λ following a cosine schedule\nfrom 0.996 to 1 during training.\nFor the optimisation of self-supervised models, we\ntrained all the models using the Adam optimiser [75] with\nbatch size 64, momentum 0.9, weight decay 0.05 and learn-\ning rate of 5e−4 for 800 epochs in total for ImageNet-1K\npretraining and 3000 epochs for small datasets pretraining.\nIn fact, we mostly rely on the vision transformer developer’s\ndefault hyper-parameters. We believe that further improve-\nments can be obtained by tuning the hyper-parameters for\nthe self-supervised model.\nSimple data augmentation techniques are applied during\nthe self-supervised training. We found that to learn low-\nlevel features as well as higher-level semantic information,\naggressive data augmentation like MixUp [76] and Auto-\nAugment [77] hurts the training with the objective functions\nin hand. Therefore, we used only cropping, horizontal ﬂip-\nping, colour jittering, solarization, and Gaussian blurring.\nFor every instance created by augmentation, GMML-\nbased image manipulation described in Section 3 is applied\nto perturb the image and the network is optimised together\nwith the contrastive learning to reconstruct the image after\ndistortion. In order to get a feel for the reconstruction\ncapability of SiT, in Figure 2 we show the reconstruction\nof randomly selected images from the training data, testing\ndata, and samples from internet after applying moderate\ncorruption to them for demonstration purposes.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 8\nTABLE 2: Comparison with state-of-the-art methods when pretrained and ﬁnetuned on the target dataset, i.e. no external\ndata is used, employing ViT-S/16.\nMethod Flowers Pets CUB Aircraft STL10 Cars CIFAR10 CIFAR100\nRandom init. 68.8 47.5 25.3 31.1 77.1 27.4 96.9 77.8\nComparison with concurrent works\nMoCo-v3 [72] 88.9 69.0 53.1 62.5 95.4 84.0 97.3 83.4\nComparison with post arts\nDino [73] 82.4 58.0 43.6 49.3 92.1 73.0 96.8 78.9\nMAE [57] 86.9 73.0 59.4 69.0 – 91.0 – –\nSiT 92.8 84.7 71.2 77.8 96.5 92.1 98.2 85.2\nTABLE 3: Domain Transfer of SiT pretrained on ImageNet-1K dataset.\nMethod Flowers Pets CUB Aircraft STL10 Cars CIFAR10 CIFAR100 ImageNet-1K\nViT-S/16\nRandom init. 68.8 47.5 25.3 31.1 77.1 27.4 96.9 77.8 –\nSupervised [4] 98.1 91.1 82.7 80.8 98.2 91.7 98.3 86.9 79.9\nComparison with concurrent works\nMoCo-v3 [72] 97.7 92.3 82.6 87.3 98.0 93.0 98.2 86.6 81.4\nDino* [73] 97.8 89.4 80.8 83.8 96.7 93.1 98.6 87.1 81.5\nSiT 98.2 92.6 84.6 87.6 98.8 93.2 99.0 90.8 82.0\nViT-B/16\nComparison with concurrent works\nMoCo-v3 [72] 98.3 93.7 84.1 87.2 98.4 93.4 98.2 87.3 83.2\nDino [73] 98.4 90.2 80.7 81.5 97.2 93.0 98.2 87.1 82.8\nComparison with post arts\nSimMIM* [56] 97.2 92.3 81.8 83.4 97.8 91.5 98.9 88.6 83.6\nMAE* [57] 98.9 92.8 84.2 88.4 98.2 93.5 99.0 – 83.4\nSiT 98.4 93.0 84.6 88.0 98.4 93.7 99.0 89.2 83.5\nFinally, for the ﬁnetuning step, the reconstruction and\ncontrastive heads are dropped and an output layer with\nc nodes corresponding to the number of classes in the\ndownstream task is appended to the class token of the\nteacher network.\n4.2 Multi-class Classiﬁcation\n4.2.1 Multi-class Classiﬁcation on Small Datasets\nIt is well known that transformers are data-hungry which\nmake them hard to train, mostly, due to the lack of the\ntypical convolutional inductive bias. Consequently, the com-\nmon protocol for self-supervised learning with transformers\nis to pretrain the model on a large scale dataset, such as\nImageNet-1K or even larger datasets. The compute and\ndata demand of the vision transformers limit their adoption,\nparticularly by AI researchers with smaller resource budget.\nTherefore, in the ﬁrst set of experiments we investigate\nthe applicability of training transformers from scratch with\nlimited data. Particularly, we compare our proposed SiT\napproach with the state-of-the-art SSL methods when the\npretraining and ﬁnetuning are performed only on the target\ndataset. Unfortunately, few works in the literature show\nthe impact of their proposed methods when pretrained\nwith limited data. To compare with the concurrent and\npost arts in the literature, we pretrained and ﬁnetuned the\nself-supervised state-of-the-art methods using the publicly\navailable codes and the default parameters as suggested by\nthe authors. Particularly, we compare with MoCo-V3 [72],\nDino [73], MAE [57], and SimMIM [56]. For the ﬁnetuning\nstep, we rely on the vision transformer developer’s default\nhyper-parameters [4].\nAs shown in Table 2, the self-supervised pretraining of\nSiT consistently enhances the performance on all datasets\ncompared to when trained from scratch with a large margin\n(up to 64.7% in the case of Cars dataset).\nFurther, our method outperforms the concurrent and\npost arts in SSL with a large margin with an improvement\nof +3.9%, +11.7%, +11.8%, +8.8%, +1.1%, +1.1%, +0.8%,\nand +1.8% on Flowers, Pets, CUB, Aircraft, STL10, Cars,\nCIFAR10, and CIFAR100 datasets, respectively. Note that the\nperformance of SimMIM on small datasets is not included\nin Table 2 as the pretrained models did not converge, result-\ning in poor ﬁnetuning performance. Presumably, different\nrecipes might be required to pretrain SimMIM on small\ndatasets.\nIn general, contrastive SSL approaches require special\ndata-speciﬁc design or hyper-parameter tuning [79], making\nthem not suitable for small datasets which justify the poor\nperformance of MoCo-V3 and Dino in Table 2. The high per-\nformance of SiT on small stand-alone datasts is attributed to\nmodelling of local statistical correlations (which is missing\nin transformers) and global information dictated by the\ndata itself. Even though MAE is also using masked image\nmodelling, the design choice of MAE models the inductive\nbias mainly in the heavy decoder which is then passed\ndown to last layers of encoder. This counter intuitive way\nto modelling information makes MAE not suitable for small\ndatasets.\n4.2.2 Multi-class Classiﬁcation on Large-scale Datasets\nIn this section, we show the effectiveness of SiT when\npretrained on large-scale dataset, i.e. ImageNet-1K, and\nﬁnetuned on several multi-class classiﬁcation datasets. In\nTable 3, we show that our proposed method outperforms\nsupervised pretraining in most of the datasets, and achieve\nan improvement of 1.1% when ﬁnetuned on ImageNet-1K\ndataset (in the case of ViT-S/16). Further, the performance\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 9\nTABLE 4: mAP (mean Average Precision) of regular inference on the PASCAL VOC 2007, VG-500, and MS-COCO datasets.\n* are run by us using ofﬁcial pre-trained weights. All the models are pre-trained using ViT-S/16 vision transformer (unless\nmentioned otherwise) with 224 ×224 input resolutions and supervised ﬁnetuning with 448 ×448 resolution.\nMethod PASCALVOC MSCOCO Visual-Genome\nPre-training using the given dataset\nSupervised training from scratch 34.1 47.9 23.7\nSSL pre-training (SiT) 74.7 75.0 31.7\nPre-training using ImageNet-1K dataset\nSupervised pre-training (ResNet-101 [78]) 92.9 78.6 30.9\nSupervised pre-training (ViT-S/16 [4])* 92.6 81.4 33.0\nSSL pre-training (DeepCluster −[AlexNet] [32]) 73.7 – –\nSSL pre-training (SimCLR −[ResNet-50] [19]) 84.1 – –\nSSL pre-training (MoCo v3 [72])* 86.0 77.8 32.3\nSSL pre-training (Dino [73])* 91.6 80.8 33.4\nSSL pre-training (SiT) 92.0 81.9 34.0\nof our proposed method is outperforming or on par with\nthe concurrent and post art methods in most of the small\ndatasets as well as large-scale dataset when it is pretrained\nemploying bigger transformer architecture, i.e. ViT-B/16.\n4.3 Multi-Label Classiﬁcation\nIn Table 4, we compare the proposed SiT with DeiT [4],\nMoCo-v3 [72], and DINO [73] frameworks on three different\nmulti-label datasets, PASCAL VOC, MS-COCO, and Visual-\nGenome, respectively.\nAll the models are ﬁnetuned using 1 Nvidia Tesla V100\n32GB GPU card for 60 epochs with 16 batch size and 480 ×\n480 input size, employing Adam optimiser and binary cross\nentropy loss function. For evaluation, we employ the mean\naverage precision (mAP).\nFirst, we show the results when ViT-S/16 is trained from\nscratch on the downstream task. Then, we show the per-\nformance when the model is pretrained and ﬁnetuned em-\nploying the same downstream dataset. Finally, we report the\naccuracy when the models are pretrained with ImageNet-1K\nemploying SiT, MoCo v3, and DINO frameworks, and then\nﬁnetuned on the multi-label downstream datasets.\nFrom the reported results, it is evident that the training\nfrom random initialisation has produced low accuracies as\nthe amount of data available is insufﬁcient to train the\ntransformer. The results signiﬁcantly improved when the\nmodels are pretrained using SiT without any external data\nwith +40.6, +27.1, and +8.0 absolute mAP improvement\nin PASCAL VOC, MS-COCO, and Visual-Genome datasets,\nrespectively. Further, pretraining with the SiT framework\non ImageNet-1K outperforms SSL SOTA frameworks with\n+0.5 and +0.6 absolute mAP improvement in MS-COCO\nand Visual-Genome datasets.\n4.4 Instance Segmentation\nIn this experiment, we demonstrate the generalisation of\nthe frozen learnt features by SiT, pretrained on ImageNet-\n1K, on the DAVIS-2017 video instance segmentation bench-\nmark [80]. Speciﬁcally, the features of the data tokens of\nthe DAVIS-2017 videos are extracted by passing the videos\nto the frozen pretrained SiT model and then follow the\nexperimental protocol in [81] by segmenting the scenes with\nnearest neighbour between consecutive frames.\nAs shown in Table 5, SiT outperforms state-of-the-art su-\npervised and self-supervised methods with a large margin.\nNote that the poor performance of MAE is expected as MAE\nrepresentations are less linearly separable than contrastive\nlearning based methods.\nTABLE 5: DAVIS 2017 Video object segmentation. We report\nthe mean region similarity (Jm) and the mean contour-based\naccuracy ( Fm) on video instance tracking. All the models\nare pretrained using ViT-S/16 vision transformer (unless\nmentioned otherwise) with 224 ×224 input resolutions on\nImageNet-1K dataset and evaluated with 448 ×448 resolu-\ntion on DAVIS-2017 dataset.\nMethod Architecture Data (J&F)m Jm Fm\nSupervised Learning\nDeit [4] ViT-S/16 INet 57.3 55.6 59.0\nSelf-Supervised Learning\nMoCo-v3 [72]\nViT-S/16 INet\n56.0 53.4 58.5\nDINO [73] 61.8 60.2 63.4\nSiT 63.1 61.3 64.9\nMAE [57] ViT-B/16 INet 51.0 49.4 52.6\n4.5 Ablation Study\nFor the ablation studies, we used the image recognition task\ndeﬁned on a subset of ImageNet-1K dataset as a vehicle\nfor measuring the impact of several conﬁgurations of the\npretext self-supervised pretraining. Due to the limited re-\nsources, all the models are conducted on 10% of ImageNet-\n1K for pretraining and evaluated on the full validation set\nof ImageNet-1K. The models are pretrained for 200 epochs\nemploying the ViT-S/16 architecture as the backbone of the\nmodel and the momentum encoder.\nEffect of Different Components of SiT.The aim of this\nablation study is to investigate the effect of the individual\nelements of the pretext learning. From the results reported\nin Table 6, it is evident that the training from random\ninitialisation has produced poor performance of less than\n40%, as the amount of data available is insufﬁcient to train\nthe transformer. We then pretrained SiT using just the con-\ntrastive learning head without momentum encoder, which is\nessentially equivalent to SimCLR [19]. The accuracy jumped\nto 56.8 with an improvement of 19.5%. Subsequently, we\nincluded a momentum encoder (similar to MoCo-v3 [72])\nwhich further improved the accuracy with around 1%.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 10\n50 100 200 400 800\n55\n57\n59\n# Epochs \nAccuracy\n(a)\n(a) (b) (c) (d) (e)\n57.7\n57.9\n58.1\nType of Corruption \nAccuracy (b)\n10 30 50 70 90\n56\n57\n58\nCorruption (%)\nAccuracy (c)\nFig. 3: Ablation studies of the effect of (a) Longer pretraining, (b) Type of corruption, and (c) Percentage of corruption.\nTABLE 6: Effect of the different components of SiT for self-\nsupervised pretraining. Models are pretrained on 10% of\nImageNet-1K dataset for 200 epochs, followed by ﬁnetuning\non validation set of ImageNet-1K.\nMethod Con-\ntrastive\nHead\nMomen-\ntum\nEncoder\nImage\nCorrup-\ntion\nRecons.\nHead\nTop-1\nAccuracy\nRandom init. \u0017 \u0017 \u0017 \u0017 37.3\nSimCLR [19] \u0013 \u0017 \u0017 \u0017 56.2\nMoCo-v3 [72] \u0013 \u0013 \u0017 \u0017 57.0\nSiT\n\u0017 \u0017 \u0017 \u0013 39.5\n\u0017 \u0017 \u0013 \u0013 57.5\n\u0013 \u0017 \u0013 \u0013 57.9\n\u0013 \u0013 \u0013 \u0013 58.1\nAfter that, we investigated the effectiveness of training\ntransformers as an autoencoder to reconstruct the input\nimage, i.e. D(E(x)) = x, where x is the input image, E is\nthe encoder which is ViT-S/16 in our case, and D is a\nlightweight reconstruction decoder. Expectedly, the perfor-\nmance was similar to the performance of a model trained\nfrom scratch. Indeed, this is attributed to the fact that\nwithout proper choice of constraints, autoencoders are ca-\npable of learning identity mapping, i.e. memorising the\ninput without learning any useful discriminative features.\nTo regularise the transformer-based autoencoder, we incor-\nporated GMML version of the transformer working with the\nreconstruction error loss function. The performance jumped\nto 57.5% which is even better than using contrastive learning\nwith momentum.\nFinally, we performed an experiment of using both con-\ntrastive learning and GMML, without momentum encoder.\nThe pairing of the self-supervision training mechanisms\nhad an egalitarian effect, pushing the ﬁnetuning test perfor-\nmance to 57.9%. This was slightly improved by employing\na momentum encoder.\nIn summary, using the reconstruction loss on its own\nas a means of self-supervision provided an effective start-\ning point for efﬁcient downstream task ﬁnetuning. Further\nmarginal improvements can be made by extending the\nrange of mechanisms for self-supervised pretraining.\nEffect of Longer Self-supervised Pretraining.In Figure 3-\na, we show the quality of SiT when pretrained for longer\nnumber of epochs. We observe that the performance steadily\nincreases with the number of epochs and it does not saturate\neven after pretraining the model for 800 epochs.\nEffect of Type of Corruption.In Figure 3-b, we show the\neffect of different type of corruption, i.e. corrupting the\nimages with (a) zeros, (b) noise, (c) replace with patches\nfrom another image, (d) either by zeros or replacing from\nanother image, and (e) either by noise or replacing from\nanother image. We found that the best individual inpainting\ntask is “replace” where connected patches are randomly\nreplaced with patches from another image. Further, we\nobtained a better performance when “replace” is combined\nwith “noise”. On the other hand, the accuracy slightly\ndropped when “zeros” combined with “replace”.\nEffect of the Percentage of Image Corruption.Figure 3-c\nshows the top-1 accuracy when the models are pretrained\nwith different corruption percentages. We found that the\noptimal ratio is between 50% to 70%. This behaviour was\nexpected as the masking encourages the network to learn\nsemantic information from the uncorrupted patches sur-\nrounding the groups of masked tokens in order to recover\nthe missing information.\nEffect of Aligning the Corruption with the Patch size.We\nobserved that the speed of convergence and generalisation\nof pretraining slightly improve when the masking of tokens\nis not aligned to patch boundaries especially when pretrain-\ning on small datasets.\nQualitative Results.To gain additional insight and a better\nunderstanding of the effect of pretraining the SiT model\nusing unlabeled data, we visualise the t-SNE [82] projection\nof the learnt representations in different scenarios. In Figure\n4-a, we visualise the projection of the embeddings of the\ntest set of STL-10 dataset before training (the model is\nrandomly initialised). Following, in Figure 4-b, we show\nthe projection of the embeddings when SiT is pretrained in\nunsupervised fashion for 3000 epochs using only the labeled\ntraining set of STL-10 dataset that consists of 5,000 images.\nNotice that even without the label information, SiT is able\nto discriminate between different classes. Finally, in Figure\n4-c, the projection of the embeddings are shown when the\nmodel is pretrained using the labeled and unlabeled sets\nof STL-10 dataset which in total consists of 105,000 images\nfor 1,000 epochs. It is evident that with more unlabeled\ndata in the pretraining stage, the pretrained model has\nbetter discrimination ability between the different classes.\nWe also show the t-SNE visualisation after the ﬁnetuning\nstage in Figure 4-d, 4-e, and 4-f. The models are ﬁnetuned\nin the traditional supervised fashion with top-1 validation\naccuracy of 76%, 92%, and 96.5%, respectively.\nAttention Visualisation.In Figure 5, we provide visualisa-\ntions of the attention of the class token after the pretraining\nstage of SiT employing the small variant of image trans-\nformers, i.e. ViT-S/16. The images are randomly selected\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 11\nPretraining\n-10 -5 0 5 10-10\n-8\n-6\n-4\n-2\n0\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a) Random Initialisation\n-10 -5 0 5 10-10\n-8\n-6\n-4\n-2\n0\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 (b) Pretrained [Labeled data]\n-10 -5 0 5 10-10\n-8\n-6\n-4\n-2\n0\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 (c) Pretrained [Labeled+Unlabeled data]\nFinetuning\n-10 -5 0 5 10-10\n-8\n-6\n-4\n-2\n0\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(d) Training from Scratch\n-10 -5 0 5 10-10\n-8\n-6\n-4\n-2\n0\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 (e) Finetuning from (b)\n-10 -5 0 5 10-10\n-8\n-6\n-4\n-2\n0\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 (f) Finetuning from (c)\nFig. 4: t-SNE visualisation of the embeddings of the 8,000 test set images of STL-10 dataset extracted from the pretrained\nand ﬁnetuned SiT models.\nFig. 5: Attention of the class token [CLS] after the pre-training stage of SiT in an unsupervised fashion on ImageNet dataset\nemploying ViT-S/16 vision transformer.\nfrom the validation set of ImageNet dataset which are not\nused during the pretraining stage of SiT.\n5 C ONCLUSION\nIn this work we present a self-supervised vision transformer,\ntrained with unlabelled data to perform pretext tasks, and\nused the pretrained model as initialisation for ﬁnetuning\nfor a downstream classiﬁcation task. We proposed to use\ntransformers as an autoencoder, which is realisable by using\na 2-layer perceptron at the output (thanks to the transformer\narchitecture). We leveraged the attractive property of the\ntransformer architecture of being particularly suited for\ncombining different loss functions along with reconstruction\nloss. We added an extra token for contrastive learning along\nwith reconstruction loss. The proposed SiT outperformed\nstate-of-the-art self-supervised methods with wide margins.\nThis work focused on image classiﬁcation as a downstream\ntask. We believe that the SiT is admirably suitable for many\nother downstream tasks like segmentation and detection,\nhowever, this conjecture is left for future investigation.\nREFERENCES\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805, 2018.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 12\n[2] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,\net al. Improving language understanding by generative pre-\ntraining. 2018.\n[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[4] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa,\nAlexandre Sablayrolles, and Herv ´e J ´egou. Training data-efﬁcient\nimage transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[5] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herv ´e\nJ´egou. Training vision transformers for image retrieval. arXiv\npreprint arXiv:2102.05644, 2021.\n[6] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of\nimage transformers. arXiv preprint arXiv:2106.08254, 2021.\n[7] Sara Atito, Muhammad Awais, Ammarah Farooq, Zhenhua\nFeng, and Josef Kittler. Mc-ssl0. 0: Towards multi-concept self-\nsupervised learning. arXiv preprint arXiv:2111.15340, 2021.\n[8] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie,\nAlan Yuille, and Tao Kong. ibot: Image bert pre-training with\nonline tokenizer. arXiv preprint arXiv:2111.07832, 2021.\n[9] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao,\nZhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework\nfor masked image modeling. arXiv preprint arXiv:2111.09886, 2021.\n[10] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ´ar,\nand Ross Girshick. Masked autoencoders are scalable vision\nlearners. arXiv preprint arXiv:2111.06377, 2021.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\nFei. Imagenet: A large-scale hierarchical image database. In 2009\nIEEE conference on computer vision and pattern recognition , pages\n248–255. Ieee, 2009.\n[12] Yuan Gong, Cheng-I Jeff Lai, Yu-An Chung, and James Glass.\nSsast: Self-supervised audio spectrogram transformer. arXiv\npreprint arXiv:2110.09784, 2021.\n[13] Lei Zhou, Huidong Liu, Joseph Bae, Junjun He, Dimitris Samaras,\nand Prateek Prasanna. Self pre-training with masked autoen-\ncoders for medical image analysis. arXiv preprint arXiv:2203.05573,\n2022.\n[14] Zekai Chen, Devansh Agarwal, Kshitij Aggarwal, Wiem Safta,\nMariann Micsinai Balan, Venkat Sethuraman, and Kevin Brown.\nMasked image modeling advances 3d medical image analysis.\narXiv preprint arXiv:2204.11716, 2022.\n[15] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Video-\nmae: Masked autoencoders are data-efﬁcient learners for self-\nsupervised video pre-training. arXiv preprint arXiv:2203.12602 ,\n2022.\n[16] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, et al. Language models\nare few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[17] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. arXiv preprint arXiv:2006.11477,\n2020.\n[18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Gir-\nshick. Momentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9729–9738, 2020.\n[19] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey\nHinton. A simple framework for contrastive learning of visual\nrepresentations. In International conference on machine learning ,\npages 1597–1607. PMLR, 2020.\n[20] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tal-\nlec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch,\nBernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent: A new approach to\nself-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\n[21] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr\nBojanowski, and Armand Joulin. Unsupervised learning of vi-\nsual features by contrasting cluster assignments. arXiv preprint\narXiv:2006.09882, 2020.\n[22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[23] David H Hubel and Torsten N Wiesel. The period of susceptibility\nto the physiological effects of unilateral eye closure in kittens. The\nJournal of physiology, 206(2):419–436, 1970.\n[24] Colin Blakemore and Grahame F Cooper. Development of the\nbrain depends on the visual environment. Nature, 228(5270):477–\n478, 1970.\n[25] J ¨urgen Schmidhuber. Evolutionary principles in self-referential learn-\ning, or on learning how to learn: the meta-meta-... hook . PhD thesis,\nTechnische Universit¨at M ¨unchen, 1987.\n[26] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of\nvisual representations using videos. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2794–2802, 2015.\n[27] Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, and\nAbhinav Gupta. The curious robot: Learning visual representa-\ntions via physical interactions. In European Conference on Computer\nVision, pages 3–18. Springer, 2016.\n[28] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun,\nDavid Luan, and Ilya Sutskever. Generative pretraining from\npixels. In International conference on machine learning , pages 1691–\n1703. PMLR, 2020.\n[29] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised\nvisual representation learning by context prediction. InProceedings\nof the IEEE international conference on computer vision , pages 1422–\n1430, 2015.\n[30] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropi-\netro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adver-\nsarially learned inference. arXiv preprint arXiv:1606.00704, 2016.\n[31] Jeff Donahue and Karen Simonyan. Large scale adversarial rep-\nresentation learning. In Advances in Neural Information Processing\nSystems, volume 32, 2019.\n[32] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs\nDouze. Deep clustering for unsupervised learning of visual\nfeatures. In Proceedings of the European Conference on Computer\nVision (ECCV), pages 132–149, 2018.\n[33] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan\nGrewal, Phil Bachman, Adam Trischler, and Yoshua Bengio.\nLearning deep representations by mutual information estimation\nand maximization. In International Conference on Learning Represen-\ntations (ICLR), 2019.\n[34] Massimiliano Patacchiola and Amos J Storkey. Self-supervised\nrelational reasoning for representation learning. In Advances in\nNeural Information Processing Systems, volume 33, pages 4003–4014,\n2020.\n[35] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Col-\norization as a proxy task for visual understanding. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\npages 6874–6883, 2017.\n[36] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsuper-\nvised representation learning by predicting image rotations. arXiv\npreprint arXiv:1803.07728, 2018.\n[37] Simon Jenni and Paolo Favaro. Self-supervised feature learning\nby learning to spot artifacts. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 2733–2742, 2018.\n[38] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Dar-\nrell, and Alexei A Efros. Context encoders: Feature learning by\ninpainting. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2536–2544, 2016.\n[39] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image\ncolorization. In European conference on computer vision , pages 649–\n666. Springer, 2016.\n[40] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich.\nLearning representations for automatic colorization. In European\nconference on computer vision, pages 577–593. Springer, 2016.\n[41] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles. In European conference\non computer vision, pages 69–84. Springer, 2016.\n[42] Dahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon.\nLearning image representations by completing damaged jigsaw\npuzzles. In 2018 IEEE Winter Conference on Applications of Computer\nVision (WACV), pages 793–802. IEEE, 2018.\n[43] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain\nautoencoders: Unsupervised learning by cross-channel prediction.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1058–1067, 2017.\n[44] Piotr Bojanowski and Armand Joulin. Unsupervised learning by\npredicting noise. In International Conference on Machine Learning ,\npages 517–526. PMLR, 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 13\n[45] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff\nDean. Distributed representations of words and phrases and their\ncompositionality. Advances in neural information processing systems ,\n26, 2013.\n[46] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-\nAntoine Manzagol. Extracting and composing robust features\nwith denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning, pages 1096–1103, 2008.\n[47] G ¨osta H. Granlund. Special issue on perception, action and\nlearning. Image Vis. Comput., 27(11):1639–1640, 2009.\n[48] Mark A Kramer. Nonlinear principal component analysis using\nautoassociative neural networks. AIChE journal , 37(2):233–243,\n1991.\n[49] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representa-\ntion learning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018.\n[50] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsuper-\nvised feature learning via non-parametric instance discrimination.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3733–3742, 2018.\n[51] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learn-\ning representations by maximizing mutual information across\nviews. arXiv preprint arXiv:1906.00910, 2019.\n[52] Philip Haeusser, Johannes Plapp, Vladimir Golkov, Elie Aljalbout,\nand Daniel Cremers. Associative deep clustering: Training a\nclassiﬁcation network with no labels. In German Conference on\nPattern Recognition, pages 18–32. Springer, 2018.\n[53] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi.\nSelf-labelling via simultaneous clustering and representation\nlearning. arXiv preprint arXiv:1911.05371, 2019.\n[54] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Proto-\ntypical contrastive learning of unsupervised representations.arXiv\npreprint arXiv:2005.04966, 2020.\n[55] Ralph Linsker. Self-organization in a perceptual network. Com-\nputer, 21(3):105–117, 1988.\n[56] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao,\nZhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework\nfor masked image modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 9653–\n9663, 2022.\n[57] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ´ar,\nand Ross Girshick. Masked autoencoders are scalable vision\nlearners. arXiv preprint arXiv:2111.06377, 2021.\n[58] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang,\nShih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for\nmultimodal self-supervised learning from raw video, audio and\ntext. Advances in Neural Information Processing Systems , 34:24206–\n24221, 2021.\n[59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\n[60] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss func-\ntions for image restoration with neural networks.IEEE Transactions\non computational imaging, 3(1):47–57, 2016.\n[61] Taesung Park, Alexei A. Efros, Richard Zhang, and Jun-Yan Zhu.\nContrastive learning for unpaired image-to-image translation,\n2020.\n[62] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower\nclassiﬁcation over a large number of classes. In 2008 Sixth Indian\nConference on Computer Vision, Graphics & Image Processing , pages\n722–729. IEEE, 2008.\n[63] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In 2012 IEEE conference on computer\nvision and pattern recognition, pages 3498–3505. IEEE, 2012.\n[64] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of\nsingle-layer networks in unsupervised feature learning. InProceed-\nings of the fourteenth international conference on artiﬁcial intelligence\nand statistics , pages 215–223. JMLR Workshop and Conference\nProceedings, 2011.\n[65] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and\nSerge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n[66] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and\nAndrea Vedaldi. Fine-grained visual classiﬁcation of aircraft.arXiv\npreprint arXiv:1306.5151, 2013.\n[67] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object\nrepresentations for ﬁne-grained categorization. In 4th International\nIEEE Workshop on 3D Representation and Recognition (3dRR-13) ,\nSydney, Australia, 2013.\n[68] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers\nof features from tiny images. Technical report, Citeseer, 2009.\n[69] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI\nWilliams, John Winn, and Andrew Zisserman. The pascal visual\nobject classes challenge: A retrospective. International journal of\ncomputer vision, 111(1):98–136, 2015.\n[70] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence Zitnick.\nMicrosoft coco: Common objects in context. In European conference\non computer vision, pages 740–755. Springer, 2014.\n[71] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji\nHata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li,\nDavid A Shamma, et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. International\njournal of computer vision, 123(1):32–73, 2017.\n[72] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of\ntraining self-supervised vision transformers. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 9640–\n9649, 2021.\n[73] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien\nMairal, Piotr Bojanowski, and Armand Joulin. Emerging proper-\nties in self-supervised vision transformers. In Proceedings of the\nInternational Conference on Computer Vision (ICCV), 2021.\n[74] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units\n(gelus). arXiv preprint arXiv:1606.08415, 2016.\n[75] Ilya Loshchilov and Frank Hutter. Fixing weight decay regular-\nization in adam. ArXiv, abs/1711.05101, 2017.\n[76] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David\nLopez-Paz. mixup: Beyond empirical risk minimization. arXiv\npreprint arXiv:1710.09412, 2017.\n[77] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,\nand Quoc V Le. Autoaugment: Learning augmentation strategies\nfrom data. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 113–123, 2019.\n[78] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , pages 770–778,\n2016.\n[79] Yun-Hao Cao, Hao Yu, and Jianxin Wu. Training vision transform-\ners with only 2040 images. arXiv preprint arXiv:2201.10728, 2022.\n[80] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel´aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017\ndavis challenge on video object segmentation. arXiv preprint\narXiv:1704.00675, 2017.\n[81] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time cor-\nrespondence as a contrastive random walk. Advances in neural\ninformation processing systems, 33:19545–19560, 2020.\n[82] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data\nusing t-sne. Journal of machine learning research, 9(11), 2008.\nSara Atito Ali Ahmed received her Bsc. in com-\nputer science from Ain Shams University, Egypt,\nin 2011. Her Msc. degree was a collaboration\nbetween Nile University, Egypt and TU Berlin,\nGermany in 2014. She received her PhD from\nSabanci University (2021), with a thesis on deep\nlearning ensembles for image understanding.\nCurrently, she is a research fellow in Centre for\nVision, Speech and Signal Processing (CVSSP),\nUniversity of Surrey, UK, working on developing\ngeneric self-supervised learning approaches.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, ?? ?? 14\nMuhammad Awais received the B.Sc. degree\nin Mathematics and Physics from the AJK Uni-\nversity in 2001, B.Sc. degree in computer engi-\nneering from UET Taxila in 2005, M.Sc in signal\nprocessing and machine intelligence and PhD in\nmachine learning from the University of Surrey in\n2008 and 2011. He is currently a senior lecturer\nin trustworthy and responsible AI at Surrey Insti-\ntute for People-Centred Artiﬁcial Intelligence and\nCentre for Vision, Speech and Signal Process-\ning (CVSSP). His research interests include ma-\nchine learning, deep learning, self(un,semi)-supervised learning, NLP ,\naudio-visual analysis, medical image analysis and computer vision.\nJosef Kittler (M’74-LM’12) received the B.A.,\nPh.D., and D.Sc. degrees from the University of\nCambridge, in 1971, 1974, and 1991, respec-\ntively. He is a distinguished Professor of Machine\nIntelligence at the Centre for Vision, Speech and\nSignal Processing, University of Surrey, Guild-\nford, U.K. He conducts research in biometrics,\nvideo and image dataset retrieval, medical im-\nage analysis, and cognitive vision. He published\nthe textbook Pattern Recognition: A Statistical\nApproach and over 700 scientiﬁc papers. His\npublications have been cited more than 68,000 times (Google Scholar).\nHe is series editor of Springer Lecture Notes on Computer Science.\nHe currently serves on the Editorial Boards of Pattern Recognition\nLetters, Pattern Recognition and Artiﬁcial Intelligence, Pattern Analysis\nand Applications. He also served as a member of the Editorial Board of\nIEEE Transactions on Pattern Analysis and Machine Intelligence during\n1982-1985. He served on the Governing Board of the International\nAssociation for Pattern Recognition (IAPR) as one of the two British rep-\nresentatives during the period 1982-2005, President of the IAPR during\n1994-1996. Currently he is a member of the KS Fu Prize Committee of\nIAPR.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8045563697814941
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6800244450569153
    },
    {
      "name": "Supervised learning",
      "score": 0.6636180877685547
    },
    {
      "name": "Machine learning",
      "score": 0.6286051273345947
    },
    {
      "name": "Transformer",
      "score": 0.6121776700019836
    },
    {
      "name": "Autoencoder",
      "score": 0.522772490978241
    },
    {
      "name": "Semi-supervised learning",
      "score": 0.48516279458999634
    },
    {
      "name": "Feature learning",
      "score": 0.4151621162891388
    },
    {
      "name": "Deep learning",
      "score": 0.35780107975006104
    },
    {
      "name": "Artificial neural network",
      "score": 0.22241005301475525
    },
    {
      "name": "Engineering",
      "score": 0.08380204439163208
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "topic": "Computer science"
}