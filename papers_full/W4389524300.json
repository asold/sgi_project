{
  "title": "VIST5: An Adaptive, Retrieval-Augmented Language Model for Visualization-oriented Dialog",
  "url": "https://openalex.org/W4389524300",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2186878491",
      "name": "Henrik Voigt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1943359396",
      "name": "Nuno Carvalhais",
      "affiliations": [
        "Max Planck Institute for Biogeochemistry"
      ]
    },
    {
      "id": "https://openalex.org/A2468732558",
      "name": "Monique Meuschke",
      "affiliations": [
        "University Hospital Magdeburg"
      ]
    },
    {
      "id": "https://openalex.org/A1966552824",
      "name": "Markus Reichstein",
      "affiliations": [
        "Max Planck Institute for Biogeochemistry"
      ]
    },
    {
      "id": "https://openalex.org/A5093458303",
      "name": "Sina Zarrie",
      "affiliations": [
        "Bielefeld University"
      ]
    },
    {
      "id": "https://openalex.org/A1094438419",
      "name": "Kai Lawonn",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2342249984",
    "https://openalex.org/W4288025992",
    "https://openalex.org/W4226277810",
    "https://openalex.org/W1961845056",
    "https://openalex.org/W3214600982",
    "https://openalex.org/W2753472863",
    "https://openalex.org/W2795226127",
    "https://openalex.org/W4280516735",
    "https://openalex.org/W3021265955",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2995255438",
    "https://openalex.org/W1516293359",
    "https://openalex.org/W2073800769",
    "https://openalex.org/W3172214016",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W2428096022",
    "https://openalex.org/W4319452276",
    "https://openalex.org/W4237375617",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4244888246",
    "https://openalex.org/W3163247491",
    "https://openalex.org/W3202563689",
    "https://openalex.org/W2918035772",
    "https://openalex.org/W2752843814",
    "https://openalex.org/W4287890454",
    "https://openalex.org/W3081277912",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3010400351",
    "https://openalex.org/W3213578841",
    "https://openalex.org/W3198767185",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3155366422",
    "https://openalex.org/W3028632643",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4248741877"
  ],
  "abstract": "Henrik Voigt, Nuno Carvalhais, Monique Meuschke, Markus Reichstein, Sina Zarrie, Kai Lawonn. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 70–81\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nVIST5: An Adaptive, Retrieval-Augmented Language Model for\nVisualization-oriented Dialog\nHenrik Voigt1, Nuno Carvalhais2, Monique Meuschke3,\nMarkus Reichstein2, Sina Zarrieß4, Kai Lawonn1\n1University of Jena 2MPI Biogeochemistry 3University of Magdeburg 4Bielefeld University\n1first.last@uni-jena.de, 2first.last.bgc-jena.mpg.de\n3last@isg.cs.uni-magdeburg.de, 4first.last@uni-bielefeld.de\nFigure 1: VIST5 makes it easy for researchers and professionals to explore their data using natural language.\nUsers articulate their visualization preferences in a chat window, displayed in the left column. The panel lists the\nresponses of the dialog agent, containing both text and custom Vega-Lite visualization code. The right column\ncontains two visualization tools that can be controlled from the chat. At the top, a geographical map displays\ngeo-related plots, such as flow visualizations of wind directions. Below is a display area forVega-Lite visualizations\nthat are generated based on user queries to the dataset.\nAbstract\nThe advent of large language models has\nbrought about new ways of interacting with\ndata intuitively via natural language. In re-\ncent years, a variety of visualization systems\nhave explored the use of natural language\nto create and modify visualizations through\nvisualization-oriented dialog. However, the ma-\njority of these systems rely on tailored dialog\nagents to analyze domain-specific data and op-\nerate domain-specific visualization tools and\nlibraries. This is a major challenge when trying\nto transfer functionalities between dialog in-\nterfaces of different visualization applications.\nTo address this issue, we propose VIST5 , a\nvisualization-oriented dialog system that fo-\ncuses on easy adaptability to an application do-\nmain as well as easy transferability of language-\ncontrollable visualization library functions be-\ntween applications. Its architecture is based\non a retrieval-augmented T5 language model\nthat leverages few-shot learning capabilities to\nenable a rapid adaptation of the system.\n1 Introduction\nThe field of visualization has witnessed a surge of\ninterest in integrating dialogue interfaces into visu-\nalization applications, leading to the development\nof various visualization-oriented natural language\ninterfaces (V-NLI) (Narechania et al., 2020; Luo\net al., 2021b; Liu et al., 2021; Kim et al., 2021).\nThe goal of these systems is to generate visualiza-\ntions from natural language queries and modify\nthem accordingly in interaction with the user. How-\never, visualization applications exist in various do-\nmain contexts, which require specific vocabulary\nto be parsed and mapped to custom functionalities.\n70\nFor example, a visualization application that helps\nresearchers analyze climate data will handle differ-\nent user intent and different, domain-specific visu-\nalization libraries than an application in a medical\ncontext (Srinivasan et al., 2021; Gao et al., 2015).\nCertain types of visualization techniques, such as\nbar charts, line charts, or scatter plots, are very\ngeneral, so they can be used in almost any domain.\nOthers, such as flow maps for the visualization\nof wind vectors, are not and their access via the\nnatural language interface must be integrated with\ngreat effort. Transferring a set of solutions, such\nas successfully mapping user queries to visualiza-\ntion library functions, from one V-NLI to another\nwithout writing new code is still a challenging task.\nIt would be ideal if created functionality could be\ntransferred between V-NLI applications by simply\nshowing the system how to use a particular library\nwith a few examples.\nIn this paper, we introduce VIST5 , a V-NLI that\nhelps users perform text-related visualization tasks\nwhile being adaptive to the visualization libraries\nof the application domain. The system implements\na retrieval-augmented language model trained on\na mixture of visualization-specific text generation\ntasks and a large collection of general text-to-text\ntranslation tasks. Its retrieval augmentation allows\nmodular extension with domain-specific user com-\nmands and portability of functionality between ap-\nplications. Moreover, the language model meets\nthe requirements of small model size, fast trainabil-\nity, and fast inference on commodity hardware. We\nillustrate the adaptation to the specifics of a domain\nusing the example of climate data exploration.\nOur contributions can be summarized as follows:\n• Efficient Multi-Task Architecture. Introduc-\ntion of an efficient and generic multi-task ar-\nchitecture for text-related visualization tasks.\n• Retrieval-Augmented Dialogue System.\nThe presentation of a dialog system that uses\nan information retrieval component to ground\nthe dialog in knowledge retrieval from exter-\nnal resources. This allows a smaller model\nsize while exploiting knowledge from exter-\nnal databases.\n• Modular Extensibility via Few-Shot\nParadigm. Leveraging the few-shot capabili-\nties of the language model to enable modular\nextensibility and portability of user intents\nbetween applications, as well as integration\nof new custom intents in minutes.\nFor a demo video of the VIST5 system please visit\nhttps://youtu.be/bsgaV7hjlGs.\n2 Related Work\nNatural language interfaces for data visualization\nhave recently emerged as a powerful combination\nof visualization and NLP techniques. In their\ncomprehensive survey, Shen et al. (2021) provide\nan overview of how natural language interaction\ncan be integrated into the visualization pipeline\nof Card (1999). V oigt et al. (2021, 2022) elaborate\non the different visualization tasks that can be\nfacilitated by natural language interactions. The\nresulting V-NLI pipeline is shown in Figure 2.\nThe following is a sequential listing of the steps in\nthe V-NLI pipeline paired with recent work in each\nstep.\nQuery Interpretation. Interpreting the query is\nabout identifying the subset of the data the user\nwants to see and the actions the user wants to per-\nform on the data. Setlur et al. (2016) introduced\nEviza, which leverages a probabilistic grammar\ndefining a rule-based interaction schema on how to\nreact to specific types of queries. Flowsense (Yu\nand Silva, 2019), another rule-based semantic pars-\ning approach, matches special utterances and maps\nthem to visualizations in a data flow architecture.\nOther works focus on resolving linguistic ambi-\nguity and vagueness in expressions using senti-\nment analysis and word co-occurrence (Hearst\net al., 2019; Setlur et al., 2019). Recent systems\nhave introduced neural sequence-to-sequence ap-\nproaches that translate queries directly into visual-\nizations (Luo et al., 2021b). Maddigan and Susn-\njak (2023) have conducted an investigation on di-\nverse prompt designs for ChatGPT (Ouyang et al.,\n2022), OpenAI Codex (Chen et al., 2021), and\nGPT-3 (Brown et al., 2020), demonstrating the re-\nmarkable capability of these LLMs in producing\nhigh-fidelity visualizations from natural language\ninput. Our work takes a different approach, consid-\nering that training and inferring such large models\ncan be expensive and hardware-intensive, making\nthem unsuitable for computationally constrained\nuse cases. Instead, we concentrate on open access,\nextensibility, and modularity, offering an alterna-\ntive perspective.\n71\nFigure 2: V-NLI Pipeline. Given a user query, the data is first transformed, then mapped to visual structures, and\nthen displayed in a view. The user, on the other hand, uses the interface by accessing different stages of the pipeline\nvia language to solve a visualization task action by action.\nData Transformation. Transforming the data ac-\ncording to the action specified by the user is the\nnext step in the V-NLI pipeline (e.g. by aggre-\ngation, filtering, binning, or grouping ). A set\nof approaches identifies transformation functions\nfrom visualization libraries through phrase match-\ning (Gao et al., 2015; Hoque et al., 2017; Sun et al.,\n2010; Srinivasan and Stasko, 2017; Dhamdhere\net al., 2017), others make use of a common data\ninterface such as SQL (Zhong et al., 2017; Wang\net al., 2019; Scholak et al., 2021; Xie et al., 2022;\nQi et al., 2022).\nVisual Mapping. In V-NLI systems, the mapping\nfrom data to visual representation is usually seen in\none of two flavors: 1) the data transformation (e.g.\nselection of table, column, conditions) and the gen-\neration of the visualization specification (e.g. chart\ntype, color) are integrated, as in ncnet (Luo et al.,\n2021b), or 2) the data transformation and visualiza-\ntion specification are separated, with an appropriate\nvisualization for the resulting data being suggested\nafter the query is executed (Wongsuphasawat et al.,\n2015, 2016; Zhu et al., 2020; Luo et al., 2018).\nQuda (Fu et al., 2020) and ADVISor (Liu et al.,\n2021) use neural intent classification methods that\nare more flexible for integrating custom visualiza-\ntion library functions, but still have the problem\nof being difficult to extend and adapt to new user\nintents without retraining.\nView Transformation. In current systems, manip-\nulation of visual elements in the view is primarily\nenabled through other channels of multimodal in-\nteraction, such as touch and gesture (Kim et al.,\n2021; Srinivasan et al., 2020b), as exemplified by\nInChorus (Srinivasan et al., 2020a). Orko (Srini-\nvasan and Stasko, 2017) combines written or spo-\nken text input with touch gestures to manipulate\nview properties, as does Valletto (Kassel and Rohs,\n2018).\n3 VIST5 System\nThe VIST5 system is composed of a language\nmodel (Section 3.1), a dialog management\ncomponent (Section 3.2) that controls the mem-\nory and API calls to the various visualization\nlibraries used, and a user interface (Section 3.3).\nThe system architecture and the query exe-\ncution process are shown in Figure 3. The\nopen-source code of the system is available\nat https://github.com/clause-bielefeld\n/VIST5.git.\n3.1 Language Model\nThe model architecture closely aligns with T5-base\nand features 12 encoder and decoder blocks with\na token embedding dimension of 768 (Raffel\net al., 2020). We employ an input context width\nof 2048 tokens to match the length of the input\nprompt. Natural language queries are tokenized\nusing the SentencePiece tokenizer from Kudo and\nRichardson (2018) based on a 32,000 subword\nvocabulary. In total, this results in a size of 220\nmillion parameters. The model is quantized and\ndeployed in an ONNX runtime, which leads to a\nsmall memory footprint of only 225 MB (ONNX\nRuntime developers, 2018). We initialize with\npre-trained FLAN-T5-base (Chung et al., 2022)\nmodel weights, which are obtained from the\nhuggingface model hub (Wolf et al., 2020).\nDatasets. We fine-tune the language model using\nthe following datasets:\n• nvbench. nvbench is the largest dataset avail-\nable for the NL2VIS task (Luo et al., 2021a).\nIn nvbench, text queries are translated into\nVega-Lite JSON specifications. The dataset\ncontains a large number of 25,750 examples\nfrom 750 data tables in 105 domains.\n72\nFigure 3: VIST5 system architecture. An example query interpretation includes the following steps: 1) The query\nis tokenized and embedded into a neural embedding vector. 2) The retrieval component returns examples relevant to\nthe query from long-term memory. 3) If similar examples are found, they are included in the prompt along with the\nvisualization state, table state and dialog history. 4) The prompt is fed into the model, which predicts an action and\narguments for that action. 5) The action is validated by the dialog management component and then executed. 6)\nThe output of the action is passed on to the frontend, where it leads to an update of the visualization.\n• NIv2. The natural instructions dataset is used\nfor few-shot instruction fine-tuning (Wang\net al., 2022). The model is trained in such a\nway that it first sees three similar input/output\nexamples in the prompt before generating a\nresponse to the current query. This training\nobjective was explicitly chosen to train the T5\nmodel on cases where few-shot examples are\navailable in addition to an input. The goal is to\ntrain it to derive a solution (e.g., how to call a\nparticular function) based on given examples\nand then apply it to the input.\n• Domain-Specific Dialogs. The VIST5 sys-\ntem is equipped with an online annotation\ntool to capture domain-specific utterances and\ncommands during runtime. We employed it\nto collect 300 dialog turns from researchers\nexploring the system. This very small dataset\ncontains contextual queries from the domain\nof climate science. It is used as a showcase to\ndemonstrate how the annotation tool can be\nused to adapt the model to a specific domain.\nFrom the above datasets, we use nvbench and the\ndomain-specific dialogs in their entirety. From\nNIv2, we take a random sample of 50k. We then\nuse an NVIDIA A6000 GPU to fine-tune the lan-\nguage model for four hours (one epoch).\n3.2 Dialog Management\nTo manage the dialog, we use two additional com-\nponents. The first is the agent’s short-term memory,\nwhich stores the status of the visualization and the\ncurrently selected data table as well as the most\nrecent dialog history. The second is a long-term\nmemory, which is a vector database of domain-\nspecific few-shot examples.\n3.2.1 Short Term Memory\nThe visualization state in our application consists\nof the composition of the currently displayed Vega-\nLite chart. This is a JSON object that contains all\nthe properties of the visualization such asmark and\nchannel encodings as well as data transformations\nlike filters or aggregations. The Vega-Lite JSON\nobject is flattened and converted to a normalized\nJSON string (Wes McKinney, 2010). The table\nstate consists of a Pandas dataframe (pandas de-\nvelopment team, 2020), which is serialized as the\nheader, followed by the first three rows. Thedialog\nhistory is stored as a sequence of query/response\npairs.\n3.2.2 Long Term Memory\nThe main task of the long-term memory is to adapt\nthe application to the context of use, e.g., domain-\nspecific utterances, libraries, and functions that are\nused during the analysis of climate data. This is\nrealized by storing a list of application-specific\nfew-shot examples. A few-shot example is an\ninput-output pair that contains an example user\ninput and the desired action, as well as the argu-\nments that the model should use to execute that\naction. An example to call a function of a do-\nmain specific library looks like this: INPUT: show\nme a heat map of temperature, OUTPUT:\naction: create_heat_map; args: \"column\":\n\"temperature\". During runtime, a Sentence-\nTransformer (Reimers and Gurevych, 2019) is used\nto encode the input query into a neural embedding\nvector. Then, the cosine similarities between the\nencoded query vector and all stored encoded few-\nshot example vectors are computed. All examples\nthat exceed a similarity threshold α are kept. We\nset α to a similarity value of 0.8. This ensures that\n73\nonly very relevant examples are returned. Of the\nretrieved examples, the top 3 are then passed into\nthe prompt. If no example exceeds the threshold,\nno example is returned and the model must respond\nto the input without further assistance based on the\nknowledge contained in its weights.\n3.2.3 API Orchestration\nTo manage the different visualization libraries used,\nall functionalities (= function names and their argu-\nments in JSON format) are listed in anaction space.\nThe interpretation of a request from perception to\nfinal response is as follows: Upon receiving a user\nrequest, relevant examples are first retrieved from\nlong-term memory. The prompt is then assembled\nfrom these (potentially) retrieved few-shot exam-\nples, the current visualization state as a Vega-Lite\nJSON string, the table state, and the user input (see\nAppendix A for details). Based on this prompt,\nthe model generates an action and the correspond-\ning arguments. After generation, the control loop\nchecks to see if the generated action exists in the\naction space, and if it does, the function is called\nand executed with the specified arguments. The\noutput of this function is then sent to the frontend,\nwhere it causes a change in the targeted visualiza-\ntion display.\n3.3 User Interface\nThe user interface is built in HTML, CSS, and\nJavaScript (see Figure 1). The backend, which\nserves the website and hosts the language model\nfor inference, is based on fastAPI (tiangolo, 2023).\nVisualization Display. The visualization area con-\nsists of a geographic map onto which the climate\ndata is projected. To create the map the visualiza-\ntion library leaflet (leaflet, 2023) is used. Below the\nmap, a display for Vega-Lite visualizations (Satya-\nnarayan et al., 2018) is provided. The visualization\nis dynamically updated with new visualization spec-\nifications generated by the language model based\non user requests.\nChat Window. On the left side, there is a chat\nwindow that contains the dialog history of the con-\nversation. It allows the user to submit requests to\nthe system and view the exact system responses\nincluding the generated Vega-Lite specs.\nOnline Annotation Tool. After receiving a re-\nsponse, the user can interactively edit the created\nVega-Lite specification if desired. If a customized\nVega-Lite specification is to be used as a training\nexample in the future, it can also be immediately\nsubmitted back to the system in this manner.\nData Display. The Vega-Lite display can be\nswitched to a data display. It shows an overview of\nthe selected data set with the column headers of the\ndata frame, their data types, and the first 1k rows\nof the data set.\n4 Features\nThe focus of the system is to provide visualiza-\ntions in response to user queries to help users solve\napplication-specific visualization tasks as defined\nby Brehmer and Munzner (2013). In the VIST5\nsystem, this involves three main tasks: 1) trans-\nlating a natural language query into a visualiza-\ntion specification, 2) engaging in a domain-specific\nanalytical conversation by exchanging contextual\nqueries to gain insight into the data, and 3) cus-\ntomizing a visualization specification to meet user\nneeds. To measure the response quality of the sys-\ntem in these tasks, we conducted a user study with\n24 participants. It revealed that the system pro-\nvided high-quality responses to diverse visualiza-\ntion requests, and that the vast majority of few-shot\nrequests were also successful. Of particular note is\nthat the users felt really engaged with the system,\nas evidenced by the high average number of user\nturns per dialog of 11.6. A detailed description of\nthe study can be found in Appendix B.\n4.1 Natural Language Query to Visualization\nThe Natural Language Query to Visualization\n(NL2VIS) task is the most prominent task sup-\nported by the system (Luo et al., 2021a). Given a\nquery, the system responds with a Vega-Lite speci-\nfication that it believes is the best one to help users\nanswer their question. To demonstrate, consider\nthe query: \"Show me Seattle’s temperature\nin 2018 as a line chart\" . The query is en-\ntered into the dialog interface and sent to the back-\nend. Since the model was trained on this task,\nthere are no few-shot examples stored in long-\nterm memory for it. As a result, no examples are\nadded to the prompt. The prompt is then fed to\nthe model. The model recognizes the NL2VIS\nrequest and generates a create_vegalite ac-\ntion with the appropriate arguments \"mark\":\n\"line\", \"encoding_x_field\": \"date\",\n\"encoding_x_type\": \"temporal\", ... . The\ngenerated specification is then converted from a\nnormalized JSON string back to a JSON object,\npassed to the front end, and displayed to the user.\n74\n4.2 Analytical Conversations\nAnalytic conversations, consisting of a back-and-\nforth of contextual queries and responses, are crit-\nical because, in data exploration, no one knows\nwhere insights will be found until they see the\ndata. Often, interest in certain aspects of the data\nis highly situational, leading to contextual queries.\nFor example, a user might first query the temper-\nature in Seattle, as in the previous example. Af-\nter viewing the output, the user is interested in\ncomparing this temperature curve to the city of\nNew York, which is on the other side of the conti-\nnent. In this context, given the initial visualization,\nthe user might simply ask, \"Okay, now add the\ntemperature in New York to the plot. This\nrequest implies to the model that 1) the user wants\nto keep the temperature in Seattle in the plot, 2)\nthe user wants to add the temperature in New York\nto the plot, 3) the year of focus is 2018, and 4) it\nmight be better to color the curves for the two cities\ndifferently, otherwise it will be difficult for the user\nto compare the two. Extending a language inter-\nface from single-turn interactions, such as NL2VIS\nqueries, to contextual queries greatly increases its\nflexibility, since practical use is always contextual.\nVisualization Customization. Since the Vega-\nLite specifications are available to the model in the\nprompt, users can also customize data-only visual-\nizations by adding titles, labels, changing colors, or\nswapping axes on the fly. After completing their ex-\nploration, users may want to share a plot with their\ncolleagues to discuss an interesting trend in the\ntemperature curves for New York and Seattle that\nthey observed during the exploration. To accom-\nplish this, a user could give the instruction: \"Add\na title to the chart that reads Seattle\nvs. New York Temperature 2018\". The model\nwill update the plot, and once received, the user\ncan share the visualization with a colleague.\nDomain-Specific Visualizations. The analysis of\nclimate data depends heavily on the interpretation\nof the measurements in the context of the geograph-\nical location of a weather station. Only when the\ncharacteristics of the environment in terms of alti-\ntude, vegetation, and urbanization can be consid-\nered together with the data, reliable conclusions\ncan be drawn. To this end, we integrate three geo-\nspecific plot types to expand the range of options\navailable to climatologists working with VIST5 .\nFor example, we enable marker plots of weather\nstations on the leaflet map, giving the user an\noverview of where weather stations are located.\nA second function is the generation of heat maps,\nwhich can be specified by naming the column in\nthe dataset from which a heat map is to be gener-\nated. An example would be \"Show me a heat\nmap of precipitation\" . This is an instruction\nthat the model has never seen during training, but it\ncan be solved by seeing a few examples. The third\ngeospatial map we have integrated following this\nparadigm is flow maps to visualize wind directions.\nCustom Functionalities. Custom functionalities\nare functions that are provided by the application\nbut usually have to be integrated into the language\ninterface by hand, otherwise, they are inacces-\nsible without training data. Using the few-shot\nparadigm, we integrate a function to export plots\nand share them with colleagues. Furthermore, it\nis possible to change the map type between satel-\nlite/dark/street/hybrid, depending on the interest of\nthe exploration scenario. Finally, it is also possible\nto ask the model to update the weather dataset with\nfresh data points from the Open Meteo Weather\nAPI (open meteo, 2023). When exploring climate\ndata on maps, it is particularly helpful to use large\nscreens. A drawback for the language interface,\nin this case, is that typing-based chat is very im-\npractical, as it is annoying to switch back and forth\nbetween the keyboard and the screen. We, there-\nfore, decided to include a number of voice loco-\nmotion interactions in the form of few-shot exam-\nples. We use a text-to-speech service based on\nthe VOSK library (Shmyrev and other contribu-\ntors, 2022). Interactions include zoom in/out, move\nleft/right/up/down, and navigating to a specific lo-\ncation by naming it as in\"Navigate to the city\nof London, please.\" . The map adjusts seam-\nlessly and exploration can continue hands-free.\n5 Conclusion\nIn this work, we have proposed VIST5 , a system\nthat demonstrates the adaptation of a V-NLI to an\napplication domain using online annotation and\nfew-shot learning techniques. The system performs\na retrieval-augmented dialog by using the external\nknowledge contained in few-shot examples to gen-\nerate responses to user input. This makes it fast,\nmodular, and easily adaptable to a user-defined\ndomain. Unlike large language models, VIST5\nfocuses on small model size, fast trainability, and\nfast inference on commodity hardware to meet the\nneeds of applications with privacy concerns or lim-\n75\nited computational resources. We hope that the\nsystem will inspire the community to further im-\nprove the architecture and create more applications\nand datasets for visualization-oriented dialogue to\npromote the combination of NLP and visualization\ntechniques.\nLimitations\nCompared to very large models such as GPT-4,\nPaLM2, or ChatGPT, VIST5 ’s capabilities are lim-\nited to a much smaller set of tasks. The model is not\na general dialog agent like, e.g., ChatGPT and only\nworks on tasks for which it has been trained, or if\nit is provided with sufficient few-shot examples by\nthe retrieval mechanism. We see this limitation as\na clear trade-off that the application developer has\nto make between the size of the model that can be\nused in their application and the model properties\nthat are needed for the current application.\nA second limitation we see is the collision of\nsimilar few-shot examples when the number of\ntasks to be integrated via the few-shot paradigm\nbecomes very large. This can lead to the retrieval\nmechanism not always returning the optimal exam-\nples and thus providing the model with incorrect\nstarting points that reduce the response quality. A\npossible compromise here could be to fine-tune the\nsentence transformer model on the large set of few-\nshot examples to ensure that the optimal examples\nare always retrieved.\nA third limitation we see is the limitation of the\nmodel to generate complete visualization specifi-\ncations only from the Vega-Lite visualization li-\nbrary. Adding functionality from other visualiza-\ntion libraries such as D3.js or Observable Plot is\npossible via the few-shot paradigm, but the longer\nthe visualization specifications to be generated, the\nmore error-prone the few-shot approach becomes\nfor small models such as T5-base (e.g., large Vega-\nLite specifications can contain more than a hundred\nproperties). We see three approaches as promising\ndirections for the future: 1) visualization specifica-\ntions for general plots, e.g. bar charts, are speci-\nfied in a library-independent way and can then be\nparsed from the general specification into the re-\nspective library, 2) methods for integrating code\ndocumentation of specific libraries into the prompt\nand making it usable so that even small language\nmodels can benefit from it need to be explored, 3)\nfor large plot specifications of specific visualization\nlibraries, training data needs to be generated either\nby humans or (depending on quality requirements)\nby larger models, e.g. GPT-4.\nEthics Statement\nThe nvbench and NIv2 datasets, as well as the\nT5 and FLAN-T5 models, are available for re-\nsearch and non-commercial use. We explicitly\nstate that the intended use of our model is to as-\nsist researchers and domain experts in their data\nexploration procedures by allowing them to eas-\nily generate visualizations from natural language\ndescriptions. The reliability of the generated vi-\nsualizations and their one-to-one correspondence\nwith the underlying data set must always be verified\nby the user of the VIST5 system. The language\nmodel generates visualizations based on the input\nquery and the information contained in the prompt,\nwithin its capabilities. During generation, misin-\nterpretations or misapplied data transformations\nmay occur, leading to incorrect results. Therefore,\nwe encourage users not to take the results gener-\nated by the model for granted, but to verify the\ngeneration process by always double-checking the\nspecifications provided in the chat window for the\ngenerated visualizations and making sure that they\nmake sense in the current context given the query\nand dataset at hand.\nAcknowledgments\nThis work was supported by the Carl Zeiss Foun-\ndation in the context of the \"A Virtual Workshop\nfor Digitization in the Sciences\" and \"Interactive\nInference\" projects.\nReferences\nRobert Amar, James Eagan, and John Stasko. 2005.\nLow-level components of analytic activity in infor-\nmation visualization. In IEEE Symposium on Infor-\nmation Visualization, 2005. INFOVIS 2005., pages\n111–117. IEEE.\nMatthew Brehmer and Tamara Munzner. 2013. A multi-\nlevel typology of abstract visualization tasks. IEEE\ntransactions on visualization and computer graphics,\n19(12):2376–2385.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\n76\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Inigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši ´c. 2018. Multiwoz–a\nlarge-scale multi-domain wizard-of-oz dataset for\ntask-oriented dialogue modelling. arXiv preprint\narXiv:1810.00278.\nMackinlay Card. 1999. Readings in information visual-\nization: using vision to think. Morgan Kaufmann.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKedar Dhamdhere, Kevin S McCurley, Ralfi Nahmias,\nMukund Sundararajan, and Qiqi Yan. 2017. Analyza:\nExploring data with conversation. In Proceedings\nof the 22nd International Conference on Intelligent\nUser Interfaces, pages 493–504.\nSiwei Fu, Kai Xiong, Xiaodong Ge, Siliang Tang,\nWei Chen, and Yingcai Wu. 2020. Quda: natural\nlanguage queries for visual data analytics. arXiv\npreprint arXiv:2005.03257.\nTong Gao, Mira Dontcheva, Eytan Adar, Zhicheng Liu,\nand Karrie G Karahalios. 2015. Datatone: Managing\nambiguity in natural language interfaces for data vi-\nsualization. In Proceedings of the 28th annual acm\nsymposium on user interface software & technology,\npages 489–500.\nMarti Hearst, Melanie Tory, and Vidya Setlur. 2019. To-\nward interface defaults for vague modifiers in natural\nlanguage interfaces for visual analysis. In 2019 IEEE\nVisualization Conference (VIS), pages 21–25. IEEE.\nEnamul Hoque, Vidya Setlur, Melanie Tory, and Isaac\nDykeman. 2017. Applying pragmatics principles for\ninteraction with visual analytics. IEEE transactions\non visualization and computer graphics, 24(1):309–\n318.\nJan-Frederik Kassel and Michael Rohs. 2018. Valletto:\nA multimodal interface for ubiquitous visual analyt-\nics. In Extended Abstracts of the 2018 CHI Confer-\nence on Human Factors in Computing Systems, pages\n1–6.\nYoung-Ho Kim, Bongshin Lee, Arjun Srinivasan, and\nEun Kyoung Choe. 2021. Data@ hand: Fostering\nvisual exploration of personal data on smartphones\nleveraging speech and touch interaction. In Proceed-\nings of the 2021 CHI Conference on Human Factors\nin Computing Systems, pages 1–17.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nleaflet. 2023. Leaflet/leaflet.\nCan Liu, Yun Han, Ruike Jiang, and Xiaoru Yuan. 2021.\nAdvisor: Automatic visualization answer for natural-\nlanguage question on tabular data. In2021 IEEE 14th\nPacific Visualization Symposium (PacificVis), pages\n11–20. IEEE.\nYuyu Luo, Xuedi Qin, Nan Tang, and Guoliang Li. 2018.\nDeepeye: Towards automatic data visualization. In\n2018 IEEE 34th international conference on data\nengineering (ICDE), pages 101–112. IEEE.\nYuyu Luo, Jiawei Tang, and Guoliang Li. 2021a.\nnvbench: A large-scale synthesized dataset for cross-\ndomain natural language to visualization task. arXiv\npreprint arXiv:2112.12926.\nYuyu Luo, Nan Tang, Guoliang Li, Jiawei Tang,\nChengliang Chai, and Xuedi Qin. 2021b. Natural lan-\nguage to visualization by neural machine translation.\nIEEE Transactions on Visualization and Computer\nGraphics, 28(1):217–226.\nPaula Maddigan and Teo Susnjak. 2023. Chat2vis: Gen-\nerating data visualisations via natural language us-\ning chatgpt, codex and gpt-3 large language models.\narXiv preprint arXiv:2302.02094.\nArpit Narechania, Arjun Srinivasan, and John Stasko.\n2020. Nl4dv: A toolkit for generating analytic speci-\nfications for data visualization from natural language\nqueries. IEEE Transactions on Visualization and\nComputer Graphics, 27(2):369–379.\nONNX Runtime developers. 2018. ONNX Runtime.\nhttps://onnxruntime.ai.\nopen meteo. 2023. open-meteo/open-meteo.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nThe pandas development team. 2020. pandas-\ndev/pandas: Pandas.\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan,\nChenghu Zhou, Xinbing Wang, Quanshi Zhang, and\nZhouhan Lin. 2022. Rasat: Integrating relational\nstructures into pretrained seq2seq model for text-to-\nsql. arXiv preprint arXiv:2205.06983.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\n77\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nCoRR, abs/1908.10084.\nArvind Satyanarayan, Dominik Moritz, Kanit Wong-\nsuphasawat, and Jeffrey Heer. 2018. Vega-lite: A\ngrammar of interactive graphics. IEEE Transactions\non Visualization and Computer Graphics , 23:341–\n350.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. Picard: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. arXiv preprint arXiv:2109.05093.\nVidya Setlur, Sarah E Battersby, Melanie Tory, Rich\nGossweiler, and Angel X Chang. 2016. Eviza: A\nnatural language interface for visual analysis. In\nProceedings of the 29th annual symposium on user\ninterface software and technology, pages 365–377.\nVidya Setlur, Melanie Tory, and Alex Djalali. 2019. In-\nferencing underspecified natural language utterances\nin visual analysis. In Proceedings of the 24th Inter-\nnational Conference on Intelligent User Interfaces,\npages 40–51.\nLeixian Shen, Enya Shen, Yuyu Luo, Xiaocong Yang,\nXuming Hu, Xiongshuai Zhang, Zhiwei Tai, and Jian-\nmin Wang. 2021. Towards natural language inter-\nfaces for data visualization: A survey. arXiv preprint\narXiv:2109.03506.\nNickolay V . Shmyrev and other contributors. 2022.\nV osk Speech Recognition Toolkit: Offline speech\nrecognition API for An- droid, iOS, Raspberry Pi\nand servers with Python, Java, C and Node. https:\n//github.com/alphacep/vosk-api.\nArjun Srinivasan, Bongshin Lee, Nathalie Henry Riche,\nSteven M Drucker, and Ken Hinckley. 2020a. Incho-\nrus: Designing consistent multimodal interactions\nfor data visualization on tablet devices. In Proceed-\nings of the 2020 CHI conference on human factors in\ncomputing systems, pages 1–13.\nArjun Srinivasan, Bongshin Lee, and John Stasko.\n2020b. Interweaving multimodal interaction with\nflexible unit visualizations for data exploration. IEEE\nTransactions on Visualization and Computer Graph-\nics, 27(8):3519–3533.\nArjun Srinivasan, Nikhila Nyapathy, Bongshin Lee,\nSteven M Drucker, and John Stasko. 2021. Collect-\ning and characterizing natural language utterances\nfor specifying data visualizations. In Proceedings\nof the 2021 CHI Conference on Human Factors in\nComputing Systems, pages 1–10.\nArjun Srinivasan and John Stasko. 2017. Orko: Facili-\ntating multimodal interaction for visual exploration\nand analysis of networks. IEEE transactions on visu-\nalization and computer graphics, 24(1):511–521.\nYiwen Sun, Jason Leigh, Andrew Johnson, and Sangy-\noon Lee. 2010. Articulate: A semi-automated model\nfor translating natural language queries into meaning-\nful visualizations. In Smart Graphics: 10th Interna-\ntional Symposium on Smart Graphics, Banff, Canada,\nJune 24-26, 2010 Proceedings 10 , pages 184–195.\nSpringer.\ntiangolo. 2023. tiangolo/fastapi.\nHenrik V oigt, Özge Alaçam, Monique Meuschke, Kai\nLawonn, and Sina Zarrieß. 2022. The why and the\nhow: A survey on natural language interaction in\nvisualization. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 348–374.\nHenrik V oigt, Monique Meuschke, Kai Lawonn, and\nSina Zarrieß. 2021. Challenges in designing natu-\nral language interfaces for complex visual models.\nIn Proceedings of the First Workshop on Bridging\nHuman–Computer Interaction and Natural Language\nProcessing, pages 66–73.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2019. Rat-sql:\nRelation-aware schema encoding and linking for text-\nto-sql parsers. arXiv preprint arXiv:1911.04942.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al. 2022.\nSuper-naturalinstructions:generalization via declara-\ntive instructions on 1600+ tasks. In EMNLP.\nWes McKinney. 2010. Data Structures for Statistical\nComputing in Python. In Proceedings of the 9th\nPython in Science Conference, pages 56 – 61.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nKanit Wongsuphasawat, Dominik Moritz, Anushka\nAnand, Jock Mackinlay, Bill Howe, and Jeffrey Heer.\n2015. V oyager: Exploratory analysis via faceted\nbrowsing of visualization recommendations. IEEE\ntransactions on visualization and computer graphics,\n22(1):649–658.\nKanit Wongsuphasawat, Dominik Moritz, Anushka\nAnand, Jock Mackinlay, Bill Howe, and Jeffrey Heer.\n2016. Towards a general-purpose query language\nfor visualization recommendation. In Proceedings of\nthe Workshop on Human-In-the-Loop Data Analytics,\npages 1–6.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I Wang,\n78\net al. 2022. Unifiedskg: Unifying and multi-tasking\nstructured knowledge grounding with text-to-text lan-\nguage models. arXiv preprint arXiv:2201.05966.\nBowen Yu and Cláudio T Silva. 2019. Flowsense: A\nnatural language interface for visual data exploration\nwithin a dataflow system. IEEE transactions on visu-\nalization and computer graphics, 26(1):1–11.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv\npreprint arXiv:1709.00103.\nSujia Zhu, Guodao Sun, Qi Jiang, Meng Zha, and\nRonghua Liang. 2020. A survey on automatic in-\nfographics and visualization recommendations. Vis.\nInformatics, 4:24–40.\n79\nA Prompt Design\nThe prompt is assembled as a sequence of the visu-\nalization state and the table state. Below that we\nput the dialog history, followed by a new line sig-\nnaling the new input query. After the input query,\nrelevant examples from long-term memory are dis-\nplayed. A visual summary of the prompt design\ncan be seen in Figure 4.\nFigure 4: Example prompt of the VIST5 language\nmodel. Blue: The visualization state contains the stringi-\nfied Vega-Lite specification. Black: The table state con-\ntains a stringified version of the column header and the\nfirst three rows of the Pandas data frame of the currently\nused dataset. Green: The conversation history contains\nup to eight previous turns in the dialog. Red: The new\ninput field contains the current user query. Purple: The\nexamples section contains up to three possible retrieved\nfew-shot examples from long-term memory. Orange:\nThe word OUTPUT is the last word entered into the model,\nsignaling the start of the generation process. The subse-\nquent action and arguments are possible outputs to be\ngenerated by the model given the preceding prompt.\nB Evaluation\nWe evaluated the system by conducting an active\nuser study engaging 24 users with the VIST5\ndialog assistant. The user study was conducted\nwith people of academic background (58.3% male,\n37.5% female, 4.2% prefer not to say). 8.4% of\nthe participants are in NLP, 54.2% are in Visualiza-\ntion, 20.8% are in climate science, and 16.6% are\npeople from other fields subsumed under ’Others’.\n62.5% of the participants were between the ages\nof 20 and 30, 29.2% were between 30 and 40, and\n8.3% were between 40 and 50. 29.2% had less than\nthree years of experience in their domain, 37.5%\nbetween three and five years, and 33.3% more than\nfive years.\nB.1 Method\nThe main goal of our study was to find out:\n1. The quality of the answers given by the system\nwith respect to the different types of queries\nin the NL2VIS task.\n2. The system’s response quality on few-shot\ntasks.\nWe put participants into a task-oriented dialog sit-\nuation. Users were given the option to choose\nfrom a set of seven different climate data sets. To\ngenerate goals for users to achieve with the sys-\ntem, we generate visualization tasks from the pool\nof common low-level visualization tasks specified\nby Amar et al. (2005): : characterize distribu-\ntion, compute derived value, correlate, determine\nrange, filter, find extremum, find anomalies, clus-\nter, retrieve value, sort. Every user is randomly\nassigned two of those tasks. A low-level visual-\nization task is presented to the user as a general\ninstruction, e.g., to filter the dataset according to a\ncertain condition. The user must then try to solve\nthe task by interacting with the chatbot. Further,\nevery participant was assigned one few-shot task\nfrom the pool of few-shot categories: custom visu-\nalization, custom functionality, locomotion which\neach is comprised of several few-shot tasks, but we\nare mainly interested in the response quality per\ncategory. The custom visualizations that can be\ncreated are marker plots, heat maps, flow visual-\nizations. Custom functions to be invoked include\nexporting visualizations, changing map style, and\nupdating the dataset. Locomotion few shot tasks\ninclude zooming in/out, moving left/right/up/down,\nand navigating to a city of choice. To solve a task,\na user can ask as many questions as necessary. Dur-\ning the interaction, users are prompted to rate the\nquality of each response from the chatbot on a Lik-\nert scale from 1 (poor) to 5 (very good), i.e. how\nappropriate the response was given the query. In\naddition, users are asked to provide textual feed-\nback on what they consider to be particularly good\nor bad answers. This helps us understand these ex-\ntreme cases better in hindsight and learn from them.\nBefore the study began, users were shown a video\nof a short sample conversation (less than 10 turns)\nbetween a user and the chatbot, explaining how to\nrate responses and where to provide feedback.\nOnce all tasks have been completed, we allow\nthe participants to explore the system freely in an\n80\nFigure 5: Results of the user evaluation on the ten low-\nlevel tasks of Amar et al. (2005): a) characterize the\ndistribution, b) compute a derived value, c) correlate,\nd) determine range, e) filter, f) find extremum, g) find\nanomalies, h) cluster, i) retrieve value, j) sort. The mean\nis provided in the first row of the table below, std in the\nsecond.\nunbounded way. The unconstrained interaction\nhelps us get additional feedback for a broader hori-\nzon of uses that we may not have thought of be-\nfore. This feedback is interesting for guiding future\nwork.\nB.2 Results\nAll in all, we collected a set of 279 dialog turns\nfrom the users during the study. The average di-\nalog has a number of 11.6 user turns, which is\nhigher than the average number of user turns in\ncurrent task-oriented dialog datasets such as Multi-\nWOZ (Budzianowski et al., 2018).\nNL2VIS Tasks. The results on the low-level visu-\nalization tasks are shown in Figure 5. The mean\nLikert score across all tasks is 3.82. The standard\ndeviation across all tasks is 1.53. The mean for\neach task is shown in the first row of the table in\nFigure 5, and the standard deviation is shown in the\nsecond row. We can see that the mean score for the\ntasks compute derived value, determine range, find\nextremum, find anomalies, retrieve valueand sort is\nvery high, with an average value above 4. This tells\nus that the system provides high-quality responses\nfor these subsets of low-level visualization tasks.\nTasks like characterize distribution, correlate,\nfilter and cluster have an average value above 3,\nbut also show a larger standard deviation. This\nshows that for these tasks the response quality\nvaries more between appropriate and inappropri-\nate responses, but the tendency is towards positive\nresponses. Overall, the system does not perform\nbelow average on any of the tasks.\nFew-Shot Tasks. The results on the few-shot tasks\nare shown in Figure 6. The average rating over all\nFigure 6: Results of the user evaluation on the three few-\nshot task categories: k) custom functionality, l) custom\nvisualization, m) locomotion. The mean is provided in\nthe first row of the table below, std in the second.\ntasks is 3.77. The standard deviation over all tasks\nis 1.65. The mean for each task is shown in the first\nrow of the table in Figure 6, and the standard devi-\nation is shown in the second row. We can see that\nthe means for the custom visualization task and the\nlocomotion task are very high with values above\n4. This shows that the system had no problems\nfinding out how to create custom visualizations on\nthe leaflet map and navigating it based on a few\nexamples. The mean scores for the custom func-\ntionality task are above 3 and show higher standard\ndeviations, indicating that the response quality is\nmore variable for this few-shot category. We found\na possible explanation for this in the vulnerability\nof the few-shot paradigm to typos. In particular,\ntypos when changing the map type or selecting\ncolumn names cause problems because the system\nusually passes the arguments as they are given in\nthe input to the function, which then leads to errors\nin execution. The integration of a spell checker\nor the use of system-initiated check questions in\ncase of uncertainty are possible levers for future\nimprovements in this respect.\nOverall, the system always scores above the\nmean of 3 for all tasks. This shows that, on average,\nusers found the responses to be helpful. However,\nit also shows that while the system performed well\non the majority of responses, it did not perform\noptimally on all inputs.\n81",
  "topic": "Dialog box",
  "concepts": [
    {
      "name": "Dialog box",
      "score": 0.7448751330375671
    },
    {
      "name": "Computer science",
      "score": 0.7096306681632996
    },
    {
      "name": "Visualization",
      "score": 0.6898325085639954
    },
    {
      "name": "Natural language processing",
      "score": 0.5379633903503418
    },
    {
      "name": "Dialog system",
      "score": 0.43333783745765686
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3898840546607971
    },
    {
      "name": "Artificial intelligence",
      "score": 0.380179226398468
    },
    {
      "name": "World Wide Web",
      "score": 0.24098578095436096
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210154168",
      "name": "Max Planck Institute for Biogeochemistry",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210138551",
      "name": "University Hospital Magdeburg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I20121455",
      "name": "Bielefeld University",
      "country": "DE"
    }
  ]
}