{
    "title": "PolyNC: a natural and chemical language model for the prediction of unified polymer properties",
    "url": "https://openalex.org/W4389374230",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2963355555",
            "name": "Haoke Qiu",
            "affiliations": [
                "Changchun Institute of Applied Chemistry",
                "Chinese Academy of Sciences",
                "State Key Laboratory of Polymer Physics and Chemistry",
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2322850351",
            "name": "Lunyang Liu",
            "affiliations": [
                "State Key Laboratory of Polymer Physics and Chemistry",
                "Changchun Institute of Applied Chemistry",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2136556016",
            "name": "Xuepeng Qiu",
            "affiliations": [
                "Changchun Institute of Applied Chemistry",
                "University of Science and Technology of China",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2134327954",
            "name": "Xuemin Dai",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Changchun Institute of Applied Chemistry"
            ]
        },
        {
            "id": "https://openalex.org/A2256516864",
            "name": "Xiangling Ji",
            "affiliations": [
                "Changchun Institute of Applied Chemistry",
                "State Key Laboratory of Polymer Physics and Chemistry",
                "Chinese Academy of Sciences",
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A4212029650",
            "name": "Zhao-Yan Sun",
            "affiliations": [
                "University of Science and Technology of China",
                "State Key Laboratory of Polymer Physics and Chemistry",
                "Changchun Institute of Applied Chemistry",
                "Chinese Academy of Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3014674650",
        "https://openalex.org/W4221026569",
        "https://openalex.org/W3210612530",
        "https://openalex.org/W3161361893",
        "https://openalex.org/W4323653000",
        "https://openalex.org/W3040871550",
        "https://openalex.org/W2754122850",
        "https://openalex.org/W4365452029",
        "https://openalex.org/W2952832141",
        "https://openalex.org/W4221085453",
        "https://openalex.org/W3093821973",
        "https://openalex.org/W4383982233",
        "https://openalex.org/W4291238127",
        "https://openalex.org/W4307975993",
        "https://openalex.org/W4366769286",
        "https://openalex.org/W4383955629",
        "https://openalex.org/W2791355014",
        "https://openalex.org/W2044834685",
        "https://openalex.org/W4318066339",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W4293232267",
        "https://openalex.org/W4220989818",
        "https://openalex.org/W2973074478",
        "https://openalex.org/W3033201772",
        "https://openalex.org/W4283463809",
        "https://openalex.org/W3129084747",
        "https://openalex.org/W2291908932",
        "https://openalex.org/W4285987580",
        "https://openalex.org/W3090892334",
        "https://openalex.org/W4295760497",
        "https://openalex.org/W4382652848",
        "https://openalex.org/W4229451304",
        "https://openalex.org/W4220902634",
        "https://openalex.org/W4297253404",
        "https://openalex.org/W4292639835",
        "https://openalex.org/W4296154431",
        "https://openalex.org/W4386766213",
        "https://openalex.org/W3113492468",
        "https://openalex.org/W3095106934",
        "https://openalex.org/W3157080125",
        "https://openalex.org/W2970815869",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2562607067",
        "https://openalex.org/W3008588639",
        "https://openalex.org/W4213233019",
        "https://openalex.org/W2947423323",
        "https://openalex.org/W4318262876",
        "https://openalex.org/W4312752637",
        "https://openalex.org/W2604296437",
        "https://openalex.org/W4296437429",
        "https://openalex.org/W2892181857",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3155755963",
        "https://openalex.org/W3103092523",
        "https://openalex.org/W4297632148",
        "https://openalex.org/W4318718899"
    ],
    "abstract": "PolyNC directly infers properties based on human prompts and polymer structures, enabling an end-to-end learning that encourages the model to autonomously acquire fundamental polymer knowledge, in a multi-task, multi-type unified model manner.",
    "full_text": "PolyNC: a natural and chemical language model for\nthe prediction of uniﬁed polymer properties†\nHaoke Qiu, ab Lunyang Liu, *a Xuepeng Qiu, bc Xuemin Dai,c Xiangling Ji ab\nand Zhao-Yan Sun *ab\nLanguage models exhibit a profound aptitude for addressing multimodal and multidomain challenges,\na competency that eludes the majority of o ﬀ-the-shelf machine learning models. Consequently,\nlanguage models hold great potential for comprehending the intricate interplay between material\ncompositions and diverse properties, thereby accelerating material design, particularly in the realm of\npolymers. While past limitations in polymer data hindered the use of data-intensive language models, the\ngrowing availability of standardized polymer data and e ﬀective data augmentation techniques now\nopens doors to previously uncharted territories. Here, we present a revolutionary model to enable rapid\nand precise prediction of Polymer propertiesvia the power of Natural language and Chemical language\n(PolyNC). To showcase the e ﬃcacy of PolyNC, we have meticulously curated a labeled prompt –\nstructure– property corpus encompassing 22 970 polymer data points on a series of essential polymer\nproperties. Through the use of natural language prompts, PolyNC gains a comprehensive understanding\nof polymer properties, while employing chemical language (SMILES) to describe polymer structures. In\na uniﬁed text-to-text manner, PolyNC consistently demonstrates exceptional performance on both\nregression tasks (such as property prediction) and the classi ﬁcation task (polymer classi ﬁcation).\nSimultaneous and interactive multitask learning enables PolyNC to holistically grasp the structure –\nproperty relationships of polymers. Through a combination of experiments and characterizations, the\ngeneralization ability of PolyNC has been demonstrated, with attention analysis further indicating that\nPolyNC eﬀectively learns structural information about polymers from multimodal inputs. This work\nprovides compelling evidence of the potential for deploying end-to-end language models in polymer\nresearch, representing a signiﬁcant advancement in the AI community's dedicated pursuit of advancing\npolymer science.\n1 Introduction\nPolymers possess a large theoretically feasible chemical space.\nOver the past few decades, experimentalists and computational\nscientists have conducted extensive and valuable explorations\nto navigate the chemical space of polymers, accumulating\ninvaluable data.\n1–7 By harnessing the wealth of available data,\nit's promising that precise and e ﬃcient polymer discovery\nguidelines can be deduced, ultimately reducing the time from\nthe laboratory to market. Encouragingly, signicant progress\nhas been made in utilizing machine learning (ML) models to\naccurately and eﬃciently infer polymer properties.8–16 These ML\nmodels can be categorized into three main approaches: hand-\ncraed descriptor-based, graph-based, and sequence-based\nmethods. In the context of handcraed descriptor-based ML,\nthe essential steps involve the extraction of numerical repre-\nsentations capable of describing molecular structures,\ncommonly leveraging well-established tools such as RDKit,\n17\nMordred18 and molecular ngerprints.19 These handcraed\ndescriptor-based models perform exceptionally well in specic\npolymer tasks, especially with small datasets. To strive for end-\nto-end learning, there has been a growing interest in graph-\nbased approaches.\n20–23 Capturing end-to-end representations\nof polymer structures oﬀers great exibility for ML models to\ndirectly learn from raw data.24 Among these approaches, the\ngraph convolutional neural network (GCN) is commonly\nemployed. GCN treats atoms as nodes and bonds as edges,\nutilizing message-passing mechanisms\n25 to continuously\naggregate neighborhood features and capture local molecular\nmotifs and molecular topologies.\n26,27 Graph-based models\naState Key Laboratory of Polymer Physics and Chemistry, Changchun Institute of\nApplied Chemistry, Chinese Academy of Sciences, Changchun 130022, China.\nE-mail: lyliu@ciac.ac.cn; zysun@ciac.ac.cn\nbSchool of Applied Chemistry and Engineering, University of Science and Technology of\nChina, Hefei 230026, China\ncCAS Key Laboratory of High-Performance Synthetic Rubber and its Composite\nMaterials, Changchun Institute of Applied Chemistry, Chinese Academy of Sciences,\nChangchun 130022, China\n† Electronic supplementary information (ESI) available. See DOI:\nhttps://doi.org/10.1039/d3sc05079c\nCite this:Chem. Sci.,2 0 2 4 ,15,5 3 4\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 27th September 2023\nAccepted 4th December 2023\nDOI: 10.1039/d3sc05079c\nrsc.li/chemical-science\n534 | Chem. Sci.,2 0 2 4 ,15, 534–544 © 2024 The Author(s). Published by the Royal Society of Chemistry\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nconsider more molecular topological structures, making them\nparticularly promising for predicting alternating, random, and\nblock copolymers.24\nBesides, sequence-based language models also oﬀer a prom-\nising solution for polymer property prediction. Polymer structures\ncan be e ﬀectively represented as language-strings such as\nSMILES,\n28 SELFIES,29 and big-SMILES,30 and language models have\nfewer restrictions on input format compared to conventional ML\nmodels, allowing for the utilization of custom and non-\nconventional polymeric patterns as inputs, especially when\nconsidering stoichiometry, molecular weight and additives. This\nwill alleviate the burden on researchers in obtaining molecular\nrepresentations that satisfy specic requirements like input shapes\nand makes language models highly promising for AI-driven poly-\nmer discovery. In the lastve years, there has been an emergence\nof language-like models in theeld of polymers. For instance, long\nshort-term memory (LSTM)\n31,32 and recurrent neural networks\n(RNNs)33 have been employed to learn the sequence representation\nof polymer structures. More recently, transformer models\nspecialized for sequence-based tasks have achieved signicant\nsuccess in addressing polymer scientic challenges incorporating\nself-attention mechanisms\n34 in a pretrained andnetune manner.\nNotable examples include TransPolymer 15 and polyBERT.16\nTransPolymer employed the RoBERTa 35 model to generate\nmachine-based ngerprints through an unsupervised approach,\nutilizing 5 million unlabeled polymer SMILES. Similarly, polyBERT\nemployed a DeBERTa-based36 transformer to convert SMILES\nstrings into numerical representations suitable for downstream\nmulti-task regression models. These remarkablendings demon-\nstrate the signicant eﬃcacy of the transformer family in theelds\nof polymers.\nIndeed, polymer property prediction tasks are similar to text-\nbased language model tasks such as machine translation and\ntext generation. Text-based language models generate corre-\nsponding outputs based on given text inputs. Similarly, in polymer\nproperty prediction tasks, properties are predicted based on given\nprompts and chemical structures. In the past, attempts to solely\nrely on language models for polymer property prediction tasks\nwere hindered by the scarcity and unattainability of high-quality\nlabeled polymer datasets,\n37 while the availability of high-quality\nopen-source polymer datasets is steadily increasing.38–41 More\nencouragingly, extensive work has shown that data augmentation-\nbased approaches are eﬀective in addressing the scarcity of poly-\nmer data,15,42,43and harnessing the intelligence of general language\nmodels proves benecial for comprehending scientic language\nvia language models.44–47 T ot h eb e s to fo u rk n o w l e d g e ,\na completely end-to-end language-based approach for directly\npredicting the properties of polymers from natural and chemical\nlanguages, rather than being used as intermediates to connect\nmolecular structures to downstream models, is currently lacking.\nThis concept draws parallels to how chemists can infer funda-\nmental properties of common molecular structures through visual\nobservation, without the need for additional analytical character-\nization (Fig. 1). By integrating natural language, chemical language\n(e.g., SMILES) and chemical knowledge (properties), language-to-\nproperty AI agents hold promise to perceive and establish\na multi-domain and multi-task understanding directly from the\npolymer structure to its diverse properties, which presents an\nopportunity to drive advancements in existing robo-chemists and\nautonomous laboratories.\n48–50 In addition to regression-based\nproperty prediction tasks, we also aim for a unied model that\ncan simultaneously handle multiple types of tasks, such as both\nclassication and regression tasks. The ability to handle multiple\ntypes of tasks simultaneously is a capability that milestone models\nmentioned earlier have yet to explore owing to the challenges\nstemming from data distribution shis\n51 and the inherent speci-\ncity of ML models themselves.\nHerein, we propose the PolyNC, a fully end-to-end and multi-\ntask language model for polymer discovery. Our model enables\nthe execution of complex polymer discovery workows with\na single model, a previously unreported ability, surpassing even\nthe capabilities of prevailing LLMs like ChatGPT, Claude, Llama\nand PaLM, due to their lack of domain knowledge. Given the\nlimited information that can be directly extracted from SMILES\nby chemists, our model introduces a new paradigm for polymer\ndiscovery, design and development based on SMILES, oﬀering\nremarkable convenience. In comparison to descriptor-based\nand graph-based models, PolyNC exhibits impressive perfor-\nmance across the four tasks central to our polymer research:\nthree property prediction tasks (regression) and one polymer\nclassication task (classi cation). Handling multi-task and\nmulti-type problems is a capability hitherto unattainable by\nprevious ML models. Notably, PolyNCs' ability to generalize to\nunknown structures is particularly impressive, as conrmed\nthrough experimental validation. Attention analysis reveals that\nthis generalization capacity stems from the model's compre-\nhension of both natural language and chemical language. This\nwork extends the powerful natural language understanding\ncapabilities of AI to theeld of polymer research, marking an\nimpressive step towards the development of expert models and\nhuman-level AI for understanding polymer knowledge.\n2 Results and discussion\n2.1D e nition of polymer-specic tasks\nThe properties of polymers are multi-faceted and oen sparse in\nspecic property datasets. Therefore, we focused on four\nFig. 1 Vision of the collaborative workﬂow between chemists and\nartiﬁcial intelligence (AI). Chemists can infer the chemical composition\nand other superﬁcial properties of molecules based on their expertise.\nLanguage-based AI models can predict elusive material properties\nfrom complex SMILES that are diﬃcult to anticipate.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 4 ,15,5 3 4–544 | 535\nEdge Article Chemical Science\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nsignicant polymer properties with publicly available\ndatasets21,52–54 for training PolyNC. The included tasks consist of\nthree general problems in a polymer domain (regression tasks)\nand one critical task specic to a particular polymer (classi-\ncation task). They are as follows:\n(a) Glass transition temperature (T\ng). Tg is a critical property\nthat characterizes the transition from a rigid, glassy state to\na moreexible, rubbery state in polymers, which is essential in\nunderstanding the processing, stability, and mechanical\nbehavior of polymers, making it a key parameter in material\ndesign and applications.\n(b) Band gap of polymer crystals (BC). The BC of polymer\ncrystals refers to the energy diﬀerence between the highest\noccupied molecular orbital (HOMO) and the lowest unoccupied\nmolecular orbital (LUMO) in the crystalline state of a polymer,\nwhich is crucial for developing polymer-based electronic and\nphotonic devices, as it inuences their performance in areas\nlike organic photovoltaics and light-emitting diodes.\nTable 1 Summary of the datasets. In total, we have studied four properties of polymers, which are glass transition temperature (Tg), band gap of\npolymer crystals (BC), atomization energy (AE) and heat resistance class (HRC). Each dataset was augmented speciﬁc times (# Aug. times) with an\nequal mixing strategy to expand each dataset with # Aug. entry data and balance the amount of data of each property\nProperty Source Unit # Entries (training/test) # Aug. times # Aug. entries\nTg DFT & exp. °C 685(615/70) 10 6850\nBC DFT eV 236(212/24) 20 4720\nAE DFT eV 390(351/39) 15 5850\nHRC Exp. — 370(333/37) 15 5550\nTotal DFT & exp. — 1681 — 22 970\nFig. 2 Schematic of PolyNC. (a) Combining natural language and chemical language as inputs to PolyNC, it comprehends human prompts and\nreveals the underlying molecular properties from chemical language. (b) Illustration of the transformer architecture in PolyNC. Natural language\nand chemical language are tokenized separately, and the positional information of each token is incorporated as input to the multi-domain\nencoder. The multi-domain encoder integrates both natural language and chemical language. The multi-domain decoder utilizes the rich\ninformation encoded by the encoder to predict speciﬁc properties or predict the heat resistance class.\n536\n| Chem. Sci.,2 0 2 4 ,15,5 3 4–544 © 2024 The Author(s). Publish ed by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n(c) Atomization energy (AE). AE represents the energy\nrequired to completely separate the constituent atoms of\na polymer molecule. It reects the strength of the chemical\nbonds within the polymer and provides insights into its stability\nand reactivity. Atomization energy is relevant in various aspects\nof polymer chemistry, including synthesis, degradation, and\nunderstanding the relationship between structure and\nproperties.\n(d) Heat resistance class (HRC). To assess the capability of\nPolyNC across various task types, we also established this\nclassication task as an example. HRC refers to the ability of\npolymers to withstand high temperatures without signicant\ndegradation or loss of its essential properties, particularly\nimportant in the case of high-end polymers like polyimides\n(PIs). Therefore, we focus on the heat resistance of PIs in this\nstudy while also investigating the performance of PolyNCs in\nhandling tasks related to polymers. Based on theT\ng of PIs and\nindustry standards,55 we can classify them into three categories:\nclass 1, class 2, and class 3. Class 1 indicates PIs with aTg above\n400 °C, class 2 represents PIs with aTg ranging from 300 to 400 °\nC, and class 3 refers to PIs with aTg below 300 °C.\nDue to the data-hungry nature of language models, data\naugmentation is implemented to improve model performance\nunder an equal mixing strategy.\n56 A summary of the four data-\nsets for downstream tasks is shown in Table 1.\n2.2 Experimental settings\nPolymeric Prompt Engineering (PPE, detailed in Methods\nsection) has been employed to acquire the training corpus for\nPolyNC, enabling the model to learn natural language prompts\nand corresponding polymer structures. We applied data\naugmentation to the training corpus with the equal mixing\nstrategy\n56 to ensure a relatively balanced representation of each\ntask. The training corpus was divided into 90% training set (20\n673) and 10% test set (2297). To the best of our knowledge, this\nis one of the largest labeled datasets available for polymer ML\ntasks. These input prompts are tokenized at the character level,\ndividing them into natural language tokens and chemical\ntokens. This tokenization strategy has been proven to provide\nbetter performance and stronger expressive capabilities. By\nobserving the distribution of token sizes in the training and\nvalidation sets (depicted in the ESI, S1†), we determined to set\nthe input token size to 150 and the output token size to 8 to\naccommodate all instances. To initialize our model, we chose\nboth the t5-base and Text + Chem T5.\n56,57 The former is a pre-\ntrained model based on natural language text, while the latter\nis a pre-trained model specically designed for chemical text\ntasks such as molecular descriptions. We found that the\ninclusion of scientic domain knowledge weighting signi-\ncantly enhances the performance of the model on polymer\ndomain tasks (as detailed in the Ablation studies section),\nwhich implies the transferability of PolyNC to other polymeric\ntasks.\nPolyNC used a whole encoder and decoder architecture each\nwith 12 layers and 12 attention heads (Fig. 2). The encoder is\nresponsible for extracting semantic information from the multi-\ndomain input, while the decoder analyzes this semantic infor-\nmation and generates outputs based on the given conditions.\nWithin them, self-attention was used to maintain the relation-\nships among tokens. In the decoder, in addition to self-\nattention for capturing relationships within a single sequence,\ncross-attention is also utilized to capture relationships between\nthe input and output sequences. This helps in learning the\nmapping between natural language and chemical language and\ntheir respective properties. For each output of attention block,\na fully connected network is used to perform non-linear\nprojection. We set 768 as the hidden dimension for PolyNC\nand 3072 for the intermediate feed forward layer with the ReLU\nactivation function and a dropout rate of 10%. The output head\nfor all tasks is the same as the output layer of t5-base from the\nhuggingface transformers package.\n58 What sets our work apart\nfrom previous e ﬀorts (TransPolymer and polyBERT) is our\nability to directly handle multitasking in a single unied model\nwithout the need for separate regression or classication heads,\nthus enabling PolyNC to achieve seamless end-to-end property\nprediction. A cosine learning rate decay strategy with a 20%\nFig. 3 (a) Distributions of chemical space for each dataset based on t-\nSNE.59 It can be observed that the majority of molecules corre-\nsponding to each property are distinct, which aids the models in\nlearning a more comprehensive mapping between molecular struc-\ntures and properties from limited data. Additionally, for each individual\ntask, the distribution of the training and testing sets is uniform, which\nhelps assess the model's generalization ability. (b) Value distribution of\neach dataset. They-axis is plotted on a logarithmic scale. This sub-\nﬁgure highlights the signiﬁcant diﬀerences in the value ranges among\nthe diﬀerent properties. (c)R\n2 metric ([). (d) MAE metric (Y). (e) RMSE\nmetric (Y). PolyNC demonstrated impressive performance in each\nprediction task, particularly excelling in the multi-property prediction\ntask, showcasing its powerful capability in handling multi-task\nscenarios.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 4 ,15, 534–544 | 537\nEdge Article Chemical Science\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nwarmup ratio is used to dynamically adjust the learning rate to\nspeed up convergence and avoid skipping optimal solutions on\ntwo RTX 3090 GPUs.\n2.3 Model performance on multi-task\n2.3.1 Single-property prediction and multi-property\nprediction. The distributions of the chemical space of each\ndataset based on t-SNE59 are shown in Fig. 3a, and it can be\nobserved that the chemical structures corresponding to each\nproperty are distinguishable. This aids ML models in learning\nmore structural information from the limited data. We assessed\nthe ability of PolyNC to handle regression tasks by evaluating it\non two major types of tasks: (1) single-property prediction and\n(2) multi-property prediction. Common descriptor-based and\ngraph-based ML models were used as baselines: Linear\nRegression (LR), Supporting Vector Regression (SVR), Gaussian\nProcess Regression (GPR), Random Forest (RF), Bagging\nRegression (BAG), Ridge Regression (RR), AdaBoost Regression\n(ADA) and extraTrees Regression (EXT) which were imple-\nmented with the scikit-learn package\n60 and the Graph Con-\nvolutional Network (GCN) implemented with the Deepchem\npackage.61\nUnder the same chemical space, we trained and validated\nPolyNC and these baseline models. In this study, the descriptors\nof baseline models implemented using the scikit-learn package\nwere computed using RDKit, where the descriptors for each\nmolecule were computed using the Descriptors.descList\nmodule, yielding a total of 209 descriptors, and descriptors\ncontaining missing values were excluded, resulting in anal set\nof 197 valid descriptors (the details are publicly available at\nhttps://github.com/HKQiu/Unied_ML4Polymers/blob/main/\ndata/exp_val/data_with_descriptors.csv). The descriptors of the\nbaseline model implemented using Deepchem were computed\nusing its default settings, with 75 features for each atom (the\ndetails are publicly available athttps://github.com/deepchem/\ndeepchem/blob/master/deepchem/feat/\ngraph_features.py#L282). The coeﬃcient of determination (R\n2),\nmean absolute error (MAE) and mean squared error (MSE) were\nused as evaluation metrics for these models, as detailed in\nS3.1.†\nSingle-property prediction tasks were used to evaluate the\nperformance of the models across diﬀerent properties. And due\nto the interdependencies among the properties of polymers,\naccurately predicting multiple properties simultaneously is of\ninterest to polymer scientists. Therefore, we employed a multi-\nproperty prediction task (denoted as‘All’) to test the models'\nability to predict multiple properties simultaneously. This task\nposes a challenge due to the signicantly varied value ranges of\ndiﬀerent properties, as shown in Fig. 3b, thus it can serve as an\nindication of the model's potential in handling multi-task\nscenarios.\nThe performance comparison of the di ﬀerent models is\nshown in Fig. 3c–e and S3.1.† PolyNC demonstrated impressive\nperformance for each single-property prediction task, demon-\nstrating that the entirely language-based model PolyNC can\nachieve prediction accuracy comparable to other ML\napproaches. For the multi-property prediction task scenario, the\npredictive performances of almost all ML models exhibited\na decrease compared to single-task settings. This is because\ntraditional ML models struggle to t datasets with diﬀerent\ndistributions.\nOf note, both GCN and PolyNC showed improvements in\nperformance for this task. In the case of GCN, GCN takes into\naccount the topology and connectivity of molecules, allowing it\nto extract more useful information compared to handcraed\ndescriptors. This enables GCN to exhibit superior performance\nin multi-task settings. This nding also underscores the\nimportance of extracting as much raw information as possible\nfrom molecules. Similarly, beneting from its learning of both\nnatural language and chemical language, PolyNC exhibits\na deeper understanding of the structure–property mapping of\npolymers and facilitates PolyNC in learning multiple properties\nof diverse molecules simultaneously, resulting in the best\nperformance, which highlights the signi cant potential of\nlanguage models in constructing polymer property–structure\nlandscapes.\n2.3.2 Multiclass classi cation. We not only assessed the\nPolyNC's performance in handling regression problems, but\nalso assessed its ability to handle the HRC classication task for\npolyimides (PIs), where diﬀerent heat resistance levels corre-\nspond to distinct application scenarios. As mentioned earlier,\nwe categorized the PIs into three classes: PIs withT\ng exceeding\n400 °C were assigned to class 1, those withTg between 300 °C\nand 400 °C were assigned to class 2, and those withTg below 300\n°C were assigned to class 3.\nWe compared the performance of PolyNC with eight baseline\nmodels for the classication task, including Logistic Regression\n(LRC), Naive Bayes (NBC), Support Vector Machine (SVC), Ada-\nBoost (ADAC), Decision Tree (DTC), Random Forest (RFC), K-\nNearest Neighbors (KNNC) and XGBoost (XGBC). While\nXGBoost was implemented using the xgboost package,\n62 the\nremaining ML models were implemented using the scikit-learn\npackage.\n60 We used four evaluation metrics for classication\nproblems, namely Accuracy, Precision, Recall, and F1 Score. The\nHRC task is characterized by an imbalanced dataset, which we\naddressed by ensuring that each class had a consistent\nproportion in both the training and testing sets and we assigned\na weight\ni to class i when assessing the model performance, as\ndetailed in S3.2.†\nThe performance results of each model are depicted in Fig. 4\nand S3.† Impressively, PolyNC achieves the best performance in\nthe HRC task, with all metrics exceeding 0.81, beneting from\nthe intelligence of language models in handling classication\ntasks, such as sentiment classication.63 As a point of compar-\nison, the baseline models achieved an accuracy of around 0.7.\nCompared to LRC and SVC, PolyNC did not make any highly\ninaccurate prediction such as predicting class 3 for samples\nactually belonging to class 1 orvice versa though class 1 had\nfewer samples and class 3 had more. This demonstrates that\nPolyNC is not signicantly aﬀected by imbalanced sample sizes,\nthus avoiding the generation of biased outputs. Besides, it is\nworth noting that during the training phase, PolyNC learns\nsimultaneously for all tasks (regression and classi cation),\n538 | Chem. Sci.,2 0 2 4 ,15,5 3 4–544 © 2024 The Author(s). Publish ed by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nallowing it to capture correlations between properties. Although\nthe mapping relationship betweenTg and HRC is not explicitly\ninformed to PolyNC, it can also spontaneously and implicitly\nlearn these details from latent data. As evidenced in Fig. S2d,†\nPolyNC tends to underestimateTg predictions, which in turn\naﬀects its classi cation performance, as re ected in mis-\nclassifying 44% of class 1 as class 2 and 32% of class 2 as class 3.\nDespite the negative impact, this also serves as evidence that\nPolyNC learns the correlations between diﬀerent tasks. Though\nunder these perplexing interferences, PolyNC also out-\nperformed other baseline models in this classication task,\nmaking it a unied model capable of handling both regression\nand classication tasks simultaneously. As far as we know, an\nML model that can simultaneously handle classication and\nregression problems in theeld of polymers has not been seen\nbefore.\n2.4 Attention analysis\nThe self-attention mechanism is used to eﬃciently consider\ndistances between sequence elements, or the level of attention\nthat one element pays to other elements, which can aﬀect the\nrepresentation of the entire input, thereby leading to diﬀerent\noutputs.\n34,64 To unveil the intrinsic knowledge learned by Pol-\nyNC from both natural language and chemical language, we\nanalyzed the attention scores within the encoder using theT\ng\ntask as an example. This analysis helps reveal which tokens are\nassigned higher correlations by PolyNC. Firstly, we constructed\na Polymer Tree for all structures in theT\ng task, visualizing the\ndataset based on the similarity of chemical structures using\nTMAP.\n65 Since the SMILES of a ring structure is more complex\nthan the SMILES of a linear structure, to evaluate PolyNC's\nability to recognize SMILES, we designed two heterocyclic\nstructures that have never been seen before (a new branch of the\nPolymer Tree, denoted as PI-1 and PI-2). These two unseen\nmolecules can also serve as a measure of the model's general-\nization ability to some extent. Secondly, we conducted rigorous\nsynthesis and characterization (as detailed in S4†) for these two\nnew structures, and found that PolyNC demonstrated optimal\nperformance in predicting theT\ng of these unknown structures,\nwith the least deviation (5 °C and 20 °C) between predicted and\nground truth values for these two samples compared to other\nbaseline models (Fig. 5b and c).\nThen, we analyzed the attention scores of PolyNC with\nrespect to the input sequences corresponding to these two\nexamples. The encoder of PolyNC consists of 12 attention\nheads, each focusing on diﬀerent contexts to extract distinct\nknowledge (all the 12 attention heads of the encoder are as\ndepicted in S5†). The attention scores for theh and ninth\nattention heads of PI-1 and PI-2 are shown in Fig. 6. It can be\nobserved that the h attention head primarily attends to\nadjacent tokens for each token to obtain local environments,\nwhile attention head 9 mainly focuses on the tokens them-\nselves. From Fig. 6, we can summarize PolyNC's ability to\nrecognize complex SMILES in three aspects. (1) PolyNC exhibits\nhigher attention scores in the feature groups (imide groups) of\nPI-1 and PI-2 (seen in the pink and light yellow parts in the\ngure). (2) PolyNC also assigns higher attention scores to the\nnatural language part and polymerization sites corresponding\nto human cues and polymerization information (seen in the\nlight purple part in the gure). (3) The structural diﬀerence\nbetween PI-1 and PI-2 is the presence of an additional benzene\nring in PI-2 (seen in the green part in Fig. 6b). Adding a benzene\nring changes the order of elements in the SMILES and makes it\nmore challenging for human interpretation. However, PolyNC\nrecognizes the benzene ring structure in complex SMILES, as\nevident in the corresponding attention matrices in the le panel\nof Fig. 6b (the black dashed box regions, which exhibit similar\nattention matrices along the diagonal direction). Specically,\nsince the purple benzene ring connects to a diﬀerent chemical\nstructure (imide structures), the lower edge of the purple\nattention matrix also changes accordingly. The aforementioned\nndings suggest that PolyNC possesses intelligent chemical\nperception, enabling it to pave the way for the recognition of\nFig. 4 Performance of PolyNC in the HRC classiﬁcation task. (a) Colors and sizes correspond to the values of various evaluation metrics. PolyNC\nachieves the best performance in the HRC task, with all metrics exceeding 81%. (b) Confusion matrix of predicted values and the ground truth\nvalues. PolyNC did not make any highly inaccurate prediction under an imbalanced dataset and outperformed other baseline models in this\nclassiﬁcation task.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 4 ,15,5 3 4–544 | 539\nEdge Article Chemical Science\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\ncomplex SMILES expressions for further molecular property\nprediction and inverse design.\n2.5 Ablation studies\nIn PolyNC, we employed a character-wise tokenization\napproach. However, for molecular structures, there is also\na group-wise tokenization approach based on functional\ngroups. We compared the performance of these two tokeniza-\ntion methods. For the group-wise tokenization, we utilized the\nseyonec/PubChem10M_SMILES_BPE_450k tokenizer from\nhuggingface: https://huggingface.co/seyonec/\nPubChem10M_SMILES_BPE_450k/tree/main (denoted as\nSMIT). SMIT is trained on a large number of SMILES\nrepresentations of molecules and aims to tokenize SMILES\nbased on functional groups. SMIT, through the use of the byte\npair encoding (BPE) algorithm, has learned multiple\nfunctional group tokens and established a vocabulary\nspecically tailored to molecules. For instance, it is possible\nto cluster local chains composed of multiple carbon atoms in\nSMIT. As an illustration, a segment consisting of four carbon\natoms can be tokenized as‘CCCC’. The results can be seen in\nFig. 7a. It can be observed that the character-wise tokeniza-\ntion method outperforms the group-wise approach. This is\nbecause the one-dimensional SMILES representation itself loses\na lot of chemical information, and adjacent characters are likely\nnot part of the same functional group. Therefore, the group-\nwise tokenization approach may lead to mis-tokenization.\nAdditionally, there is a wide variety of chemical functional\ngroups, and each functional group may have diﬀerent SMILES\nencoding methods, making it diﬃcult to cover all functional\ngroups comprehensively. The complexity of molecules and the\ncomplexity of SMILES representation have led to successful\nlanguage models in theeld of molecular research predomi-\nnantly employing character-wise tokenization.\n15,16,67,68\nFig. 5 (a)The Polymer Tree of each structure within the training and\ntest datasets ofTg.D iﬀerent molecules with distinct chemical struc-\ntures are located in diﬀerent branches of the Polymer Tree. For\ninstance, the left portion of the graph primarily consists of structures\nwith fewer heterocycles, predominantly linear polymers, while the\nright portion comprises structures with a higher number of hetero-\ncycles (see our repository https://github.com/HKQiu/\nUniﬁed_ML4Polymers/tree/main/TMAP or https://try-\ntmap.gdb.tools/tmap/discreet-ammonite-of-mathematics for more\ndetails of Polymer Tree). (b) and (c) Generalization ability of PolyNC\nin the estimation of T\ng as an example. PolyNC demonstrates\nexceptional performance in predicting theTg of unknown structures\nwith a 5 °C and 20 °C deviation for each sample. The limited\ngeneralizability is a universal issue for o ﬀ-the-shelf ML models,66\nwhere PolyNC might have learned more appropriate polymer\nrepresentations.\nFig. 6 The attention scores for theﬁfth (left) and ninth attention (right)\nheads of PI-1 (a) and PI-2 (b). It can be observed that theﬁfth attention\nhead primarily attends to adjacent tokens for each token, while\nattention head 9 mainly focuses on the tokens themselves. For linear\npolymers, the order of elements in their SMILES representation aligns\nwith the order of atoms in the molecular structure, making it easy to\nobserve the molecular structure from the SMILES. However, for\nmolecules with many rings, things become more complicated. Based\non the rules of SMILES, the rings will be broken andﬂattened at speciﬁc\natoms, causing adjacent atoms to appear in non-adjacent positions in\nthe SMILES, which poses challenges for parsing SMILES. The input\nsequences of PI-1 and PI-2 were ‘Predict the Tg of the following\nSMILES: [ *]C1]CC]C(C2]CC]C(N3C(C(C]CC(C4]CC]\nCC(C(C5[*])]O)]C4C5]O)]C6)]C6C3]O)]O)C]C2)C]C1’\nand ‘predict theT\ng of the following SMILES: [*]C1]CC]C(C2]CC]\nC(C3]CC]C(N4C(C(C]CC(C5]CC]CC(C(C6[*])]O)]C5C6]\nO)]C7)]C7C4]O)]O)C]C3)C]C2)C]C1’, respectively. PolyNC\nsucceeded in extracting and di ﬀerentiating subfunctional group\ninformation and structural di ﬀerence directly from SMILES. The\nattention scores for all the 12 attention heads of the encoder can be\nfound in S5.†\n540\n| Chem. Sci.,2 0 2 4 ,15,5 3 4–544 © 2024 The Author(s). Publish ed by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nTo assess the transferability of domain knowledge across\ndiﬀerent domains, we tested two initial weight congurations:\none based on natural language weights (t5-base) and the other\nbased on chemical text tasks, such as molecular descriptions\n(Text + Chem T5).\n44,56 Based on these two weight congurations,\nPolyNC was trained from scratch under the same parameters,\nand their learning curves are depicted in Fig. 7b. It can be\nobserved that PolyNC, based on the Text + Chem T5 congu-\nration, outperforms the performance achieved by the t5-base\nconguration (denoted as T5). This indicates the preservation\nof domain knowledge during transfer, ultimately enabling the\nmodel to develop a stronger understanding of downstream\ntasks within the same training duration. Of particular inspira-\ntion, this nding also reveals the potential success of ne-\ntuning PolyNC as a foundational model in other polymeric\ntasks.\n3 Conclusion\nWe propose PolyNC, a model capable of comprehending both\nnatural language and chemical language, for polymer discovery.\nDiverging from conventional ML models based on descriptors\nor graphs, PolyNC takes an end-to-end approach to directly\nextract semantic information from human prompts and\nchemical language for property prediction. This more abstract\nand unbiased strategy enables PolyNC to acquire a more\ngenuine understanding of chemical knowledge, resulting in\nexceptional performance in multiple polymer tasks. Notably,\nPolyNC is a multi-task, multi-domain model that integrates the\ncapabilities of both classi cation and regression models,\na distinctive feature absent in previous ML models. Moreover,\ndue to its domain specicity, PolyNC exhibits a remarkable\ncomprehension of chemical language that surpasses the\ncurrent popular commercial LLMs (S6.1†). PolyNC contributes\nto inspiring modeling approaches in polymer informatics,\nshiing from a ”feature-extraction-rst” approach to a direct\nlanguage-based paradigm. This shi signicantly reduces data\npreprocessing time, expands the scope of available data, and\nenhances the intelligence of the model. As the availability of\nhigh-quality polymer corpora continues to grow, PolyNC's\nprociency in understanding polymer content is poised to\nstrengthen progressively, even grooking the depths of intricate\npolymer structures and properties, which contributes to\nadvancing materials research and the automation of laboratory\nprocesses. Last but not least, due to the inherent universality of\nlanguage model principles, this paradigm can easily be\nextended and revolutionize other scientic research domains.\n4 Discussion\nIndeed, general large language models like ChatGPT are limited\nby their understanding of domain-specic knowledge, which\nrestricts their widespread application in scientic research. Our\nwork demonstrated the success of polymer prediction tasks\nsolely based on natural language prompts and chemical\nlanguage prompts utilizing large language models. We used\nSMILES to describe the structures of polymers, which describes\ncommon homopolymers, regular copolymers, and stereo-\nchemistry, among others (please refer to S8† for more details).\nHowever, in reality, the structure of polymers is much more\ncomplex, involving various monomer compositions, diﬀerent\nbonding patterns, and chain architectures, which go beyond the\nscope of SMILES, but they can be described using natural\nlanguage with further enrichment of training data. We will\ncontinue to mine data containing these details and further\nempower PolyNC.\nGenerative language models produce content probabilisti-\ncally. To ensure result reproducibility, PolyNC was congured to\nselect the token with the highest probability as the generated\ncontent during inference, while also controlling in uential\nFig. 7 (a) The impact of character-wise tokenization and group-wise tokenization methods on model performance. The model utilized the\ncharacter-wise tokenization which consistently achieved lower losses on both training and test sets, providing evidence for the superiority of\ncharacter-wise tokenization. (b) Model performance of PolyNC based on Text + Chem T5 and t5-base. PolyNC based on Text + Chem T5\nexhibits a lower loss, which provides evidence for the eﬀectiveness of the transferability of domain knowledge across diﬀerent domains.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 4 ,15,5 3 4–544 | 541\nEdge Article Chemical Science\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nfactors like token size. Moreover, language models should\nanswer scientic facts objectively, even if diﬀerent prompts are\ngiven. We veried the sensitivity of PolyNC to diﬀerent SMILES\nof the same molecule, and the results (S7†) demonstrate that\nPolyNC gives satisfactory results for diﬀerent SMILES of the\nsame molecule to a great extent. Of note, akin to language\nmodels like ChatGPT, PolyNC does not evaluate the plausibility\nof the input and is capable of generating inference outcomes for\nany given input. Therefore, it is imperative that individuals\nemploy PolyNC under the supervision of procient chemists to\nmitigate potential risks.\n5 Method\n5.1 Model\nOur model is implemented based on the text-to-text transfer\ntransformer (T5),56 an encoder–decoder model belonging to the\ntransformer34 family. T5 is speci cally designed to convert\nvarious language problems into a text-to-text format, enabling it\nto handle multi-domain and multi-task problems simulta-\nneously. By employing appropriate prompt engineering, T5 can\nbe adapted to handle tasks across diverse domains. To eﬀec-\ntively capture both natural language and chemical language, we\nemploy a joint encoder that encompasses both domains.\n44 This\nenables our model to excel in multi-domain and multi-task\nscenarios. Additionally, we utilize the original T5 decoder for\ngenerating the output.\n5.2 Polymeric prompt engineering (PPE)\nTo enable our model to simultaneously predict multiple poly-\nmer properties, we employed customized polymeric prompt\nengineering (PPE). Di ﬀerent tasks may require distinct\nprompts, allowing us to diﬀerentiate among them. Each task-\nspecic natural language prompt is formulated as an English\nsentence that does not exist in the original SMILES or the T5\ntokenizer vocabulary. Thus, these prompts are added as special\ntokens to the vocabulary considering the relatively limited\namount of available data. For instance, the prompt for pre-\ndicting the glass transition temperature (T\ng) is represented as\n“Predict the Tg of the following SMILES:” followed by the cor-\nresponding SMILES. This construction forms a multi-domain\nprompt that combines both natural language and chemical\nlanguage which supports complex cross-modal understanding,\nreasoning and sophisticated multimodal content generation. In\ntotal, we considered four diﬀerent polymer tasks, includingT\ng,\nband gap crystal (BC), atomization energy (AE) and heat resis-\ntance class (HRC). These tasks are sourced from publicly\navailable datasets.21,52–54\n5.3 Data augmentation\nRecognizing that language models are data-greedy69 and exist-\ning databases are insu ﬃcient for training high-performing\nmodels, we employed data augmentation techniques; one\napproach involved enumerating diﬀerent SMILES representa-\ntions for the same molecule to enhance the training dataset,\n70 as\ndiﬀerent SMILES representations of the same structure are\ntreated as diﬀerent inputs by language models. We initially\ndivided the dataset for each task into training and test sets\nusing a ratio of 0.9/0.1. During data augmentation, we followed\nan equal mixing strategy\n56 to select diﬀerent augmentation\nfactors for each task, ensuring a balanced representation of\neach property, resulting in a proportional distribution of entries\nfor each property. Through this balanced data augmentation\nprocess, we obtained a labeled dataset of polymer properties\nconsisting of 22 970 entries, where the ‘*’ sign in SMILES\nrepresents the polymerization points.\n5.4 SMILES tokenization\nFor tokenizing SMILES, we adopted a character-level tokeniza-\ntion approach. This choice was motivated by the diverse and\nextensive range of functional groups and their combinations\npresent in polymers, making it challenging to exhaustively\nenumerate them and then tokenization. Character-level toke-\nnization allows us to minimize the size of the vocabulary as\nmuch as possible. Moreover, the superior performance of\ncharacter-level tokenization has been demonstrated in multiple\nmodels\n15,16,45,68 and Ablation studies section of this work.\nData availability\nAll data and code are available in this repository ( https://\ngithub.com/HKQiu/Unied_ML4Polymers). PolyNC is\navailable for trial athttps://huggingface.co/hkqiu/PolyNC.\nAuthor contributions\nHaoke Qiu: investigation, methodology, data curation, visuali-\nzation, soware and writing – original dra. Lunyang Liu:\ninvestigation, methodology and writing – review & editing.\nXuepeng Qiu, Xuemin Dai and Xiangling Ji: resources, valida-\ntion. Zhao-Yan Sun: conceptualization, funding acquisition,\nproject administration, resources, supervision and writing –\nreview & editing.\nConﬂicts of interest\nThere are no conicts to declare.\nAcknowledgements\nWe are grateful for the support from the National Key R&D\nProgram of China (No. 2022YFB3707303), and the National\nNatural Science Foundation of China (No. 21833008 and\n52293471). We are also grateful to the Network and Computing\nCenter in the Changchun Institute of Applied Chemistry for the\nhardware support.\nReferences\n1 F. M. Haque and S. M. Grayson,Nat. Chem., 2020, 12, 433–\n444.\n2 Y. Zheng, S. Zhang, J. B.-H. Tok and Z. Bao,J. Am. Chem. Soc.,\n2022, 144, 4699–4715.\n542 | Chem. Sci.,2 0 2 4 ,15,5 3 4–544 © 2024 The Author(s). Publish ed by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n3 M. J. Sobkowicz,Science, 2021,374, 540.\n4 Q. A. Besford, H. Yong, H. Merlitz, A. J. Christoﬀerson,\nJ.-U. Sommer, P. Uhlmann and A. Fery,Angew. Chem., Int.\nEd., 2021,60, 16600–16606.\n5 S.-M. Wen, S.-M. Chen, W. Gao, Z. Zheng, J.-Z. Bao, C. Cui,\nS. Liu, H.-L. Gao and S.-H. Yu,Adv. Mater., 2023,35, 2211175.\n6 G. Wang, L.-W. Feng, W. Huang, S. Mukherjee, Y. Chen,\nD. Shen, B. Wang, J. Strzalka, D. Zheng, F. S. Melkonyan,\nJ. Yan, J. F. Stoddart, S. Fabiano, D. M. DeLongchamp,\nM. Zhu, A. Facchetti and T. J. Marks,Proc. Natl. Acad. Sci.\nU.S.A., 2020,117, 17551–17557.\n7 D. J. Audus and J. J. de Pablo,ACS Macro Lett., 2017,6, 1078–\n1082.\n8 L. Tao, J. He, N. E. Munyaneza, V. Varshney, W. Chen, G. Liu\nand Y. Li,Chem. Eng. J., 2023,465, 142949.\n9 S. Wu, Y. Kondo, M.-a. Kakimoto, B. Yang, H. Yamada,\nI. Kuwajima, G. Lambard, K. Hongo, Y. Xu, J. Shiomi,\nC. Schick, J. Morikawa and R. Yoshida,npj Comput. Mater.,\n2019, 5, 66.\n10 R. Ma, H. Zhang and T. Luo,ACS Appl. Mater. Interfaces,\n2022, 14, 15587–15598.\n11 M. A. Webb, N. E. Jackson, P. S. Gil and J. J. de Pablo,Sci.\nAdv., 2020,6, eabc6216.\n12 Y. Zhao, R. J. Mulder, S. Houshyar and T. C. Le,Polym.\nChem., 2023,14, 3325–3346.\n13 H. Qiu, W. Zhao, H. Pei, J. Li and Z.-Y. Sun,Polymer, 2022,\n256, 125216.\n14 E. R. Antoniuk, P. Li, B. Kailkhura and A. M. Hiszpanski,J.\nChem. Inf. Model., 2022,62(22), 5435–5445.\n15 C. Xu, Y. Wang and A. Barati Farimani,npj Comput. Mater.,\n2023, 9, 64.\n16 C. Kuenneth and R. Ramprasad,Nat. Commun., 2023, 14,\n4099.\n17 G. Landrum\net al., Rdkit: A soware suite for cheminformatics,\ncomputational chemistry, and predictive modeling , Greg\nLandrum, 2013, vol. 8, p. 31.\n18 H. Moriwaki, Y.-S. Tian, N. Kawashita and T. Takagi, J.\nCheminf., 2018,10,4 .\n19 H. L. Morgan,J. Chem. Doc., 1965,5, 107–113.\n20 D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre,\nR. G ´omez-Bombarelli, T. Hirzel, A. Aspuru-Guzik and\nR. Adams, Adv. Neural Inf. Process. Sys., 2015, 2015, 2224–\n2232.\n21 H. Qiu, X. Qiu, X. Dai and Z.-Y. Sun,J. Mater. Chem. C, 2023,\n11, 2930–2940.\n22 P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o\nand Y. Bengio,Int. Conf. Learn. Represent., 2017.\n23 K. Yang, K. Swanson, W. Jin, C. Coley, P. Eiden, H. Gao,\nA. Guzman-Perez, T. Hopper, B. Kelley, M. Mathea,\nA. Palmer, V. Settels, T. Jaakkola, K. Jensen and\nR. Barzilay,J. Chem. Inf. Model., 2019,59, 3370–3388.\n24 M. Aldeghi and C. W. Coley,Chem. Sci., 2022, 13, 10486–\n10498.\n25 L. Zhang, M. Chen, A. Arnab, X. Xue and P. H. S. Torr,IEEE\nTrans. Pattern Anal. Mach. Intell., 2022, 1–17.\n26 R. A. Patel, C. H. Borca and M. A. Webb,Mol. Syst. Des. Eng.,\n2022, 7, 661–676.\n27 S. Mohapatra, J. An and R. G´omez-Bombarelli, Mach. Learn.:\nSci. Technol., 2022,3, 015028.\n28 D. Weininger,J. Chem. Inf. Model., 1988,28,3 1–36.\n29 M. Krenn, F. H¨ase, A. Nigam, P. Friederich and A. Aspuru-\nGuzik, Mach. Learn.: Sci. Technol., 2020,1, 045024.\n30 T.-S. Lin, C. W. Coley, H. Mochigase, H. K. Beech, W. Wang,\nZ. Wang, E. Woods, S. L. Craig, J. A. Johnson, J. A. Kalow,\nK. F. Jensen and B. D. Olsen,ACS Cent. Sci., 2019, 5, 1523–\n1531.\n31 M. A. Webb, N. E. Jackson, P. S. Gil and J. J. De Pablo,Sci.\nAdv., 2020,6, eabc6216.\n32 L. Simine, T. C. Allen and P. J. Rossky,Proc. Natl. Acad. Sci.\nU.S.A., 2020,117, 13945–13948.\n33 D. Bhattacharya, D. C. Kleeblatt, A. Statt and W. F. Reinhart,\nSo Matter, 2022,18, 5037–5051.\n34 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser and I. Polosukhin,Adv. Neural Inf.\nProcess. Syst., Red Hook, NY, USA, 2017, pp. 6000–6010.\n35 Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer and V. Stoyanov, CoRR, 2019,\n471–484.\n36 P. He, X. Liu, J. Gao, W. Chen,Deberta: decoding-enhanced\nbert with disentangled attention , OpenReview.net (2021),\nAustria, 2020, https://openreview.net/forum?\nid=XPZIaotutsD.\n37 A. J. Gormley and M. A. Webb,Nat. Rev. Mater., 2021,6, 642–\n644.\n38 T. D. Huan, A. Mannodi-Kanakkithodi, C. Kim, V. Sharma,\nG. Pilania and R. Ramprasad,Sci. Data, 2016,3, 160012.\n39 J. Yang, L. Tao, J. He, J. R. McCutcheon and Y. Li,Sci. Adv.,\n2022, 8, eabn9545.\n40 R. Ma and T. Luo,J. Chem. Inf. Model., 2020,60, 4684–4690.\n41 N. Andraju, G. W. Curtzwiler, Y. Ji, E. Kozliak and\nP. Ranganathan, ACS Appl. Mater. Interfaces , 2022, 14,\n42771–42790.\n42 S. Lo, M. Seifrid, T. Gaudin and A. Aspuru-Guzik,J. Chem. Inf.\nModel., 2023,63, 4266–4276.\n43 J. G. Ethier, R. K. Casukhela, J. J. Latimer, M. D. Jacobsen,\nB. Rasin, M. K. Gupta, L. A. Baldwin and R. A. Vaia,\nMacromolecules, 2022,\n55, 2691–2702.\n44 D. Christodellis, G. Giannone, J. Born, O. Winther, T. Laino\nand M. Manica,Int. Conf. Mach. Learn., 2023.\n45 J. Lu and Y. Zhang,J. Chem. Inf. Model., 2022,62, 1376–1387.\n46 R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon and T.-Y. Liu,\nBriengs Bioinf., 2022,23, bbac409.\n47 W. Ahmad, E. Simon, S. Chithrananda, G. Grand and\nB. Ramsundar, ChemBERTa-2: Towards Chemical\nFoundation Models, 2022.\n48 M. Seifrid, R. Pollice, A. Aguilar-Granda, Z. Morgan Chan,\nK. Hotta, C. T. Ser, J. Vestfrid, T. C. Wu and A. Aspuru-\nGuzik, Acc. Chem. Res., 2022,55, 2454–2466.\n49 Q. Zhu, F. Zhang, Y. Huang, H. Xiao, L. Zhao, X. Zhang,\nT. Song, X. Tang, X. Li, G. He, B. Chong, J. Zhou, Y. Zhang,\nB. Zhang, J. Cao, M. Luo, S. Wang, G. Ye, W. Zhang,\nX. Chen, S. Cong, D. Zhou, H. Li, J. Li, G. Zou, W. Shang,\nJ. Jiang and Y. Luo,Natl. Sci. Rev., 2022,9, nwac190.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 4 ,15, 534–544 | 543\nEdge Article Chemical Science\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n50 G. Turon, J. Hlozek, J. G. Woodland, A. Kumar, K. Chibale\nand M. Duran-Frigola,Nat. Commun., 2023,14, 5736.\n51 Q. Dou, D. Coelho de Castro, K. Kamnitsas and B. Glocker,\nAdv. Neural Inf. Process. Sys., 2019,32, 6450–6461.\n52 M. A. F. Afzal, A. R. Browning, A. Goldberg, M. D. Halls,\nJ. L. Gavartin, T. Morisato, T. Hughes, D. J. Giesen and\nJ. E. Goose,ACS Appl. Polym. Mater., 2020,3, 620–630.\n53 C. Kuenneth, A. C. Rajan, H. Tran, L. Chen, C. Kim and\nR. Ramprasad,Patterns, 2021,2, 100238.\n54 D. Kamal, H. Tran, C. Kim, Y. Wang, L. Chen, Y. Cao,\nV. R. Joseph and R. Ramprasad,J. Chem. Phys., 2021, 154,\n174906.\n55 P. Ma, C. Dai, H. Wang, Z. Li, H. Liu, W. Li and C. Yang,\nCompos. Commun., 2019,16,8 4–93.\n56 C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li and P. J. Liu,J. Mach. Learn\nRes., 2020,21,1 –67.\n57 D. Christodellis, G. Giannone, J. Born, O. Winther, T. Laino\nand M. Manica, Unifying Molecular and Textual\nRepresentations via Multi-task Language Modelling, 2023.\n58 T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite,\nJ. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest\nand A. Rush, Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: System\nDemonstrations, 2020, pp. 38–45.\n59 L. Van der Maaten and G. Hinton,J. Mach. Learn Res., 2008,\n9, 2579–2605.\n60 F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot and E. Duchesnay, J. Mach. Learn\nRes., 2011,12, 2825–2830.\n61 B. Ramsundar, P. Eastman, P. Walters, V. Pande, K. Leswing\nand Z. Wu,Deep Learning for the Life Sciences, O'Reilly Media,\n2019.\n62 T. Chen and C. Guestrin, Proceedings of the 22nd ACM\nSIGKDD Int. Conf. on Knowledge Discovery and Data Mining,\n2016, pp. 785–794.\n63 Y. Wang, M. Huang, X. Zhu and L. Zhao,Proceedings of the\n2016 Conference on Empirical Methods in Natural Language\nProcessing, 2016, pp. 606–\n615.\n64 P. Shaw, J. Uszkoreit and A. Vaswani,Proceedings of the 2018\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies ,\n2018, vol. 2, Short Papers, pp. 464–468.\n65 D. Probst and J.-L. Reymond,J. Cheminf., 2020,12, 12.\n66 T. Stuyver and C. W. Coley,J. Chem. Phys., 2022,156, 084104.\n67 P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter,\nC. Bekas and A. A. Lee,ACS Cent. Sci., 2019,5, 1572–1583.\n68 Z. Cao, R. Magar, Y. Wang and A. Barati Farimani,J. Am.\nChem. Soc., 2023,145, 2958–2967.\n69 K. Anoop, G. P. Manjary, P. Deepak, V. L. Lajish,Responsible\nData Science, Springer, Singapore, 2022, vol. 940, pp. 13–45.\n70 E. J. Bjerrum,arXiv, preprint, arXiv:1703.07076, 2017, DOI:\n10.48550/arXiv.1703.07076.\n544 | Chem. Sci.,2 0 2 4 ,15,5 3 4–544 © 2024 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 06 December 2023. Downloaded on 11/5/2025 3:10:52 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online"
}