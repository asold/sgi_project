{
  "title": "Transformer Based Language Models for Similar Text Retrieval and Ranking",
  "url": "https://openalex.org/W3023843058",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Qadrud-Din, Javed",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Rabiou, Ashraf Bah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749833461",
      "name": "Walker Ryan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743213111",
      "name": "Soni Ravi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4303900821",
      "name": "Gajek, Martin",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Pack, Gabriel",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Rangaraj, Akhil",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2239566685",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964073004",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2936945731",
    "https://openalex.org/W2942607211",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2069870183",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1482214997",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2593864460",
    "https://openalex.org/W3102286003",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3104055892",
    "https://openalex.org/W2923890923",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2945127593",
    "https://openalex.org/W2738347542",
    "https://openalex.org/W2234701313",
    "https://openalex.org/W2963026768"
  ],
  "abstract": "Most approaches for similar text retrieval and ranking with long natural language queries rely at some level on queries and responses having words in common with each other. Recent applications of transformer-based neural language models to text retrieval and ranking problems have been very promising, but still involve a two-step process in which result candidates are first obtained through bag-of-words-based approaches, and then reranked by a neural transformer. In this paper, we introduce novel approaches for effectively applying neural transformer models to similar text retrieval and ranking without an initial bag-of-words-based step. By eliminating the bag-of-words-based step, our approach is able to accurately retrieve and rank results even when they have no non-stopwords in common with the query. We accomplish this by using bidirectional encoder representations from transformers (BERT) to create vectorized representations of sentence-length texts, along with a vector nearest neighbor search index. We demonstrate both supervised and unsupervised means of using BERT to accomplish this task.",
  "full_text": "  \nTransformer-Based Language Models for Similar Text Retrieval\n \nand Ranking\n \n \nJaved Qadrud-Din   Ashraf Bah Rabiou   Ryan Walker   Ravi Soni   Martin Gajek   Gabriel Pack   Akhil Rangaraj\n \nCasetext Machine Learning\n \n{javed,ashraf,ryan,ravi,martin,gabe,akhil}@casetext.com\n \n \nABSTRACT\n \nMost\napproaches\nfor\nsimilar\ntext\nretrieval\nand\nranking\nwith\nlong\n \n \n \n \n \n \n \n \n \n \nnatural\nlanguage\nqueries\nrely\nat\nsome\nlevel\non\nqueries\nand\n \n \n \n \n \n \n \n \n \n \nresponses\nhaving\nwords\nin\ncommon\nwith\neach\nother.\nRecent\n \n \n \n \n \n \n \n \n \napplications\nof\ntransformer-based\nneural\nlanguage\nmodels\nto\ntext\n \n \n \n \n \n \n \n \nretrieval\nand\nranking\nproblems\nhave\nbeen\nvery\npromising,\nbut\nstill\n \n \n \n \n \n \n \n \n \n \ninvolve\na\ntwo-step\nprocess\nin\nwhich\nresult\ncandidates\nare\nfirst\n \n \n \n \n \n \n \n \n \n \nobtained\nthrough\nbag-of-words-based\napproaches,\nand\nthen\n \n \n \n \n \n \nreranked\nby\na\nneural\ntransformer.\nIn\nthis\npaper,\nwe\nintroduce\n \n \n \n \n \n \n \n \n \n \nnovel\napproaches\nfor\neffectively\napplying\nneural\ntransformer\n \n \n \n \n \n \n \nmodels\nto\nsimilar\ntext\nretrieval\nand\nranking\nwithout\nan\ninitial\n \n \n \n \n \n \n \n \n \n \nbag-of-words-based\nstep.\nBy\neliminating\nthe\nbag-of-words-based\n \n \n \n \n \n \nstep,\nour\napproach\nis\nable\nto\naccurately\nretrieve\nand\nrank\nresults\n \n \n \n \n \n \n \n \n \n \n \neven\nwhen\nthey\nhave\nno\nnon-stopwords\nin\ncommon\nwith\nthe\n \n \n \n \n \n \n \n \n \n \nquery.\nWe\naccomplish\nthis\nby\nusing\nbidirectional\nencoder\n \n \n \n \n \n \n \n \nrepresentations\nfrom\ntransformers\n(BERT)\nto\ncreate\nvectorized\n \n \n \n \n \n \n \nrepresentations\nof\nsentence-length\ntexts,\nalong\nwith\na\nvector\n \n \n \n \n \n \n \n \nnearest\nneighbor\nsearch\nindex.\nWe\ndemonstrate\nboth\nsupervised\n \n \n \n \n \n \n \n \nand\nunsupervised\nmeans\nof\nusing\nBERT\nto\naccomplish\nthis\ntask,\n \n \n \n \n \n \n \n \n \n \nand\nshow\nthat\nour\napproaches\noutperform\nseveral\nother\n \n \n \n \n \n \n \n \ntechniques.\n \n1\n​ \n \nIntroduction\n \nMost\nexisting\napproaches\nfor\nretrieving\nsimilar\ntext\nrely\nin\nsome\n \n \n \n \n \n \n \n \n \n \nway\non\nword\nmatching.\nWord\nmatching\nbased\nsystems\nfail\nwhen\n \n \n \n \n \n \n \n \n \n \ntexts\nshare\nthe\nsame\nmeaning\nbut\nhave\nfew\nor\nzero\nnon-stopwords\n \n \n \n \n \n \n \n \n \n \n \nin\ncommon.\nThey\nalso\nfail\nwhen\ntexts\nthat\nhave\ndissimilar\n \n \n \n \n \n \n \n \n \n \nmeanings\ndo\nhave\nmany\nnon-stopwords\nin\ncommon\n(e.g.\n“Will\n \n \n \n \n \n \n \n \n \nwent to the store” vs “I will store it”). \n \nIn\nrecent\nyears,\nmachine\nlearning\nhas\nbeen\nused\nfor\nsimilar\ntext\n \n \n \n \n \n \n \n \n \n \n \nretrieval\nby\nemploying\nmodels\ntrained\nto\nre-rank\nresults\noriginally\n \n \n \n \n \n \n \n \n \nretrieved\nby\nword-match\nbased\ninformation\nretrieval\ntechniques.\n \n \n \n \n \n \n \nSuch\nsystems\noften\noutperform\nword-match\nbased\nretrieval\nalone,\n \n \n \n \n \n \n \n \nbut\nstill\ncannot\nreturn\nresults\nthat\nhave\nno\nnon-stopwords\nin\n \n \n \n \n \n \n \n \n \n \ncommon with the query text.\n \nRecent\nadvances\nin\nneural\ntransformer\nlanguage\nmodel\n \n \n \n \n \n \n \narchitectures\nutilizing\nunsupervised\npretraining\non\nlarge\ndatasets\n \n \n \n \n \n \n \n(such\nas\nBERT)\nhave\nmade\nit\npossible\nto\nencode\nsome\nof\nthe\n \n \n \n \n \n \n \n \n \n \n \n \nsemantic\ninformation\ncontained\nin\nsentence-length\npieces\nof\ntext.\n \n \n \n \n \n \n \n \nIn\nthis\npaper,\nwe\nintroduce\nan\neffective\nmeans\nfor\nusing\nBERT\nin\n \n \n \n \n \n \n \n \n \n \n \n \nconjunction\nwith\na\nvector\nnearest\nneighbor\nsearch\nindex\nfor\n \n \n \n \n \n \n \n \n \nsimilar\ntext\nretrieval\nand\nranking\nend-to-end,\nobviating\nthe\nneed\n \n \n \n \n \n \n \n \n \nfor a prior word-match-based information retrieval step.\n \nWe\npresent\nan\nablation\nstudy\nin\nwhich\nwe\ndemonstrate\nthe\n \n \n \n \n \n \n \n \n \n \nrelative\neffectiveness\nof\nword-match-based\napproaches\nalong\nwith\n \n \n \n \n \n \n \nvarious\nBERT\nbased\napproaches\nboth\nwith\nand\nwithout\ndifferent\n \n \n \n \n \n \n \n \n \ntypes\nof\ntask-specific\nfinetuning\non\na\ncorpus\nof\nover\n8\nmillion\n \n \n \n \n \n \n \n \n \n \n \nsentences\ndrawn\nfrom\ntext\nin\nthe\nlegal\ndomain\n(US\njudicial\n \n \n \n \n \n \n \n \n \n \nopinions).\n \n2\nBackground and Related Work\n \nPretrained\nneural\ntransformer\narchitectures\nhave\nbeen\nused\nwith\n \n \n \n \n \n \n \n \nsuccess\nin\nthe\npast\nyear\nand\na\nhalf\non\na\nwide\nrange\nof\nnatural\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlanguage\nprocessing\ntasks\n[2],\n[4],\n[7],\n[12],\n[19],\n[24].\nThey\nare\n \n \n \n \n \n \n \n \n \n \n \noperate\nby\npretraining\non\nlarge\nunlabeled\ndatasets\nand\nthen\n \n \n \n \n \n \n \n \n \nfinetuning\nto\nspecific\nlabeled\ntasks.\nThrough\nthe\npretraining\n \n \n \n \n \n \n \n \nprocess,\nthey\nlearn\nto\nproduce\nrepresentations\nof\ntokens\nin\ncontext\n \n \n \n \n \n \n \n \n \n \nthat\nare\ngeneral\nenough\nto\nform\nthe\nbasis\nfor\ntransfer\nlearning\nto\na\n \n \n \n \n \n \n \n \n \n \n \n \n \nwide range of tasks. \n \nPrior\nto\npretrained\ntransformers,\nNeural\nInformation\nRetrieval\n \n \n \n \n \n \n \n(NIR)\nmodels\nwere\nshown\nto\nproduce\nimprovements\nonly\nover\n \n \n \n \n \n \n \n \n \nweak\nbaselines\n[22].\nHowever,\nmore\nrecent\nNIR\nmodels\nbased\non\n \n \n \n \n \n \n \n \n \n \nContextual\nNeural\nInformation\nRetrieval\n(CNIR)\nhave\nshown\n \n \n \n \n \n \n \nsome\npromise,\nsuch\nas\nthe\nBERT-based\nre-rankers\nproposed\nby\n \n \n \n \n \n \n \n \n \nNogueira & Cho [17], and Yang et. al. [23].\n \nPretraining\napproaches\nprior\nto\npretrained\ntransformers\n \n \n \n \n \n \nrepresented\neach\nword\nin\none\nfixed\nway\nat\ninference\ntime,\n \n \n \n \n \n \n \n \n \n \nregardless\nof\nthe\nspecific\ncontext\nin\nwhich\nit\nis\nused\n[9],\n[15].\n \n \n \n \n \n \n \n \n \n \n \n \nSeveral\nadaptations\nhave\nbeen\nmade\nto\nthese\nmodels\nfor\n \n \n \n \n \n \n \n \n \napplication\nto\ninformation\nretrieval\n[5].\nBut\nthe\ngains\nattributed\nto\n \n \n \n \n \n \n \n \n \n \nthese\nnon-contextual\npretrained\nmodels\nhave\nbeen\nshown\nnot\nto\nbe\n \n \n \n \n \n \n \n \n \n \ncompetitive\nwith\nstrong\nwell-tuned\ntraditional\nbag-of-words\n \n \n \n \n \n \nmodels\n[22].\nWe\nhave\nchosen\nto\nfocus\non\nCNIR\nmodels\ninstead\nin\n \n \n \n \n \n \n \n \n \n \n \n \nthe\nhope\nthat\nthey\nwill\nyield\ngreater\ngains\nthan\nnon-contextual\n \n \n \n \n \n \n \n \n \n \npretrained models.\n \n  \n \nMost\nexisting\nCNIR\nsolutions\nproceed\nby\nfirst\nusing\na\n \n \n \n \n \n \n \n \n \nbag-of-words\nmodel\nsuch\nas\nBM25\nto\nretrieve\nthe\ninitial\nset\nof\n \n \n \n \n \n \n \n \n \n \n \nrelevant\ndocuments,\nand\nthen\nusing\na\nBERT-based\nmodel\nto\n \n \n \n \n \n \n \n \n \nre-rank\nthe\ntop\nresults\n[3],\n[13],\n[17],\n[23].\nThis\ntwo-step\napproach\n \n \n \n \n \n \n \n \n \n \n \nhas\nbeen\nadopted\nbecause\nthe\nrecent\ntransformer-based\nmodels\nare\n \n \n \n \n \n \n \n \n \nlarge,\nand\nusing\nsuch\na\nmodel\nto\ndirectly\ncompare\nthe\nquery\nwith\n \n \n \n \n \n \n \n \n \n \n \n \neach\nsentence\nin\na\ncorpus\nwould\nbe\ntoo\ncomputationally\n \n \n \n \n \n \n \n \n \nexpensive,\neven\nif\nthe\ncorpus\nwere\nrelatively\nsmall.\nIn\nour\nwork,\n \n \n \n \n \n \n \n \n \n \n \nwe\nexplore\nthe\nrelative\neffectiveness\nof\nseveral\ndifferent\n \n \n \n \n \n \n \n \ncustomized\ntransformer-based\nmodels\ndesigned\nto\nwork\nwith\n \n \n \n \n \n \n \nvector\nnearest\nneighbor\nsearch\nindices.\nBy\nusing\nvector-based\n \n \n \n \n \n \n \n \nsearch\nindices,\nour\ntechniques\nneed\nnot\nrely\non\nword\nmatching\nto\n \n \n \n \n \n \n \n \n \n \n \ngenerate\nthe\ninitial\nset\nof\ndocuments,\nand\nare\nthus\nable\nto\nreturn\n \n \n \n \n \n \n \n \n \n \n \n \nrelevant\nresults\nthat\ndo\nnot\nnecessarily\nshare\nwords\nwith\nthe\nquery.\n \n \n \n \n \n \n \n \n \n \n \nThe techniques we explore are as follows: \n \n1.\nA\nnon-finetuned\nBERT-based\nranker\nbased\non\nTF/IDF\n \n \n \n \n \n \n \nweighted\naveraging\nof\nBERT\noutput\ntoken\nvectors\n \n \n \n \n \n \n \ncoupled\nwith\na\nvector\nnearest\nneighbor\nsearch\nindex.\n \n \n \n \n \n \n \n \nWe\ncall\nthis\napproach\nthe\nUnsupervised\nBERT\n \n \n \n \n \n \n \napproach. \n \n2.\nA\nBERT-based\nranker\nfinetuned\nto\na\nsentence\nsimilarity\n \n \n \n \n \n \n \n \ntask\nusing\nthe\nsiamese\nnetwork\nsentence\nsimilarity\n \n \n \n \n \n \n \napproach\noutlined\nby\nReimers\n&\nGurevynch\n[20],\n \n \n \n \n \n \n \ncoupled with a vector nearest neighbor search index. \n \n3.\nA\nBERT-based\nre-ranker,\nfinetuned\non\nquery/response\n \n \n \n \n \n \npairs\nin\nthe\nsame\nmanner\nas\nthe\nMRPC\ntask\nin\nthe\n \n \n \n \n \n \n \n \n \n \n \noriginal\nBERT\npaper\n[4],\nre-ranking\nthe\ntop\nresults\n \n \n \n \n \n \n \n \ngenerated by approach number 2 above. \n \n4.\nEnsemble\nmodels\nthat\nfuse\nthe\nrankings\nof\none\nof\nthe\n \n \n \n \n \n \n \n \n \n \nBERT-based\ncontextual\nneural\nretrieval\nmodels\nwith\nthe\n \n \n \n \n \n \n \nrankings\nof\nbag-of-words\nmodels\nsuch\nas\nBM25\nand\n \n \n \n \n \n \n \n \nsequential dependence models.\n \n \nAdditionally,\nunlike\nexisting\nCNIR\nresearch\nthat\nfocused\n \n \n \n \n \n \n \nprimarily\non\nshort\nkeyword-based\nqueries,\nour\ninvestigation\n \n \n \n \n \n \n \nfocuses\non\nsentence-length\nnatural\nlanguage\nqueries.\nWe\nfocus\non\n \n \n \n \n \n \n \n \n \nthis\ntype\nof\nquery\nbecause\nsentence-length\nqueries\nare\nuseful\n \n \n \n \n \n \n \n \n \nin similar\nlegal\nargument\nretrieval,\nwhich\nis\nan\nimportant\ntask\nin\n \n \n \n \n \n \n \n \n \n \nthe\nlegal\ndomain.\nSentence-length\nnatural\nlanguage\nqueries\nhave\n \n \n \n \n \n \n \n \nalso\nbeen\nshown\nto\nbe\nmore\nchallenging\nfor\nsearch\nsystems\n[6].\n \n \n \n \n \n \n \n \n \n \n \nHence,\ntackling\nthis\nproblem\nin\nparticular\nis\nan\ninteresting\nand\n \n \n \n \n \n \n \n \n \n \nchallenging\nendeavor,\nand\nlikely\nuseful\noutside\nthe\nlegal\ndomain\n \n \n \n \n \n \n \n \n \nas\nwell.\nUsing\ncontextual\nneural\nmodels\nlike\nBERT\nto\ntackle\nthis\n \n \n \n \n \n \n \n \n \n \n \nchallenge\nis\nparticularly\npromising,\nas\nrecent\nresearch\nhas\nshown\n \n \n \n \n \n \n \n \n \nthat\ncontextual\nneural\nmodels\ncan\noutperform\nbag-of-words\n \n \n \n \n \n \n \nmodels\non\nlonger\nnatural\nlanguage\nqueries\n[3].\nBERT\nis\nable\nto\n \n \n \n \n \n \n \n \n \n \n \ncapture\ngrammatical\nstructure\nand\nword\ndependencies\nwhen\n \n \n \n \n \n \n \nprocessing\na\nnatural\nlanguage\nquery,\nwhereas\ntraditional\n \n \n \n \n \n \n \nbag-of-words\nmodels\ndo\nnot\nintegrate\nthese\nkey\nlanguage\n \n \n \n \n \n \n \n \nelements [3].\n \nIt\nhas\nbeen\nshown\nthat\ntraining\nsets\nacross\ndifferent\ndomains\n \n \n \n \n \n \n \n \n \n \ncan\nbe\nused\nto\nfine-tune\ncontextual\nneural\nlanguage\nmodels\n[25].\n \n \n \n \n \n \n \n \n \n \nSimilarly,\ntraining\ndata\ndesigned\nfor\none\ntask\ncan\nsometimes\nyield\n \n \n \n \n \n \n \n \n \n \nimprovements\nwhen\napplied\nto\na\ndifferent\ntask.\nFor\nexample,\nby\n \n \n \n \n \n \n \n \n \n \nleveraging\nQuestion\nAnswering\ndata,\nYang\net\nal.\nwere\nable\nto\n \n \n \n \n \n \n \n \n \n \nimprove\neffectiveness\non\na\nrelevance\nmatching\ntask\n[23].\nIn\nour\n \n \n \n \n \n \n \n \n \n \nwork,\nwe\nmade\nuse\nof\ntraining\ndata\nfrom\na\ndifferent\ndomain\nto\n \n \n \n \n \n \n \n \n \n \n \n \nimprove\nour\nSiamese\nnetwork-based\nBERT\nRanker\nmodel\n \n \n \n \n \n \n \ndescribed in Section 3.4.\n \nFor\nvector\nnearest-neighbor\nretrieval\nat\nscale,\nvarious\n \n \n \n \n \n \n \ntechniques\nexist\nfor\ncreating\ntree-like\ndata\nstructures\nfor\nfast\n \n \n \n \n \n \n \n \n \napproximate\nnearest\nneighbor\nsearch.\nWe\nselected\nthe\nFAISS\n \n \n \n \n \n \n \n \nlibrary [10] for our investigations. \n \n3\n​ \n \nMethods\n \n3.1\n​ \n \nTraining and finetuning datasets\n \nThe\npretraining\ndataset\nconsists\nof\na\nlarge\nlegal\ndomain\ncorpus\nof\n \n \n \n \n \n \n \n \n \n \n \nall\njudicial\nopinions\nwritten\nin\nthe\nUnited\nStates\nabove\nthe\nappeals\n \n \n \n \n \n \n \n \n \n \n \ncourt level. This dataset consists of 750M sentences .\n \nThe\nmodel\nfinetuning\nprocedures\ndescribed\nbelow\nrequire\n \n \n \n \n \n \n \nlabeled\ndatasets.\nIn\norder\nto\ngenerate\nlabeled\ndata\nfor\nthis\ntask,\nwe\n \n \n \n \n \n \n \n \n \n \n \n \npresented\nlawyers\nwith\nquery-response\nsentence\npairs.\nThe\n \n \n \n \n \n \n \nlawyers\nrated\nthe\npairs\nusing\ntwo\ngrades:\ngood\nor\nbad\nmatch.\nThe\n \n \n \n \n \n \n \n \n \n \n \n \npairs\nwere\ngenerated\nby\nrunning\nlegal-domain\nsentence\nqueries\n \n \n \n \n \n \n \n \nthrough\ntwo\ndifferent\nsources\nof\nresults:\n(1)\nthe\nUnsupervised\n \n \n \n \n \n \n \n \n \nBERT\nRanker\ndescribed\nbelow\nand\n(2)\nElasticSearch\n“more\nlike\n \n \n \n \n \n \n \n \n \nthis”,\nwhich\nselects\na\nset\nof\nrepresentative\nterms\nfrom\nthe\ninput\n \n \n \n \n \n \n \n \n \n \n \nsentences\nand\nexecutes\nthe\nresulting\nquery\nusing\nBM25.\nThese\n \n \n \n \n \n \n \n \n \nbinary\ngood/bad\nratings\nof\nexisting\nretrieval\nsystem\nresponses\n \n \n \n \n \n \n \n \nwere\nused\nas\ntraining\ndata\nfor\nthe\nBERT\nReranker.\nThis\nwas\n \n \n \n \n \n \n \n \n \n \n \neffective\nfor\nthe\nReranker\nbecause\nthe\nReranker\nneed\nonly\n \n \n \n \n \n \n \n \n \ndiscriminate\nbetween\nresponses\nthat\nan\nexisting\nsearch\nranking\n \n \n \n \n \n \n \n \nsystem generates. \n \nThe\nBERT\nRanker\nmodel\n(the\nSiamese\nBERT\nnetwork\n \n \n \n \n \n \n \n \ndescribed\nin\nsection\n3.4\nbelow),\nhowever,\nmust\ndiscern\ngood\n \n \n \n \n \n \n \n \n \nresults\nfrom\nany\npossible\nresult\nin\nthe\ncorpus.\nTo\nmore\nclosely\n \n \n \n \n \n \n \n \n \n \n \nmatch\nthe\ntask\nof\npicking\ngood\nresults\nfrom\nthe\nwhole\ncorpus,\n \n \n \n \n \n \n \n \n \n \n \nadditional\ntraining\ndata\nwas\ncreated\nfor\nthe\nBERT\nRanker\nmodel\n \n \n \n \n \n \n \n \n \n \nin\nwhich\nquery\nsentences\nwere\npaired\nwith\nrandom\nsentences\nfrom\n \n \n \n \n \n \n \n \n \n \nthe\ncorpus.\nThe\ntraining\ndata\nfor\nthe\nBERT\nRanker\nwas\nthus\nthe\n \n \n \n \n \n \n \n \n \n \n \n \nsame\nas\nthat\nfor\nthe\nReranker\nexcept\nadditional\nrandom\npairs\nwere\n \n \n \n \n \n \n \n \n \n \n \nincluded.\nPairs\nrated\nas\n“good”\nby\nhumans\nwere\nlabeled\n“1”,\npairs\n \n \n \n \n \n \n \n \n \n \n \nhumans\nrated\nas\n“bad”\nwere\nlabeled\n“0.5”,\nand\nrandom\npairs\nwere\n \n \n \n \n \n \n \n \n \n \n \nlabeled “0”.\n \n \n3.2\n​ \n \nTest data collection\n \nOur\ntest\ncollection\nconsists\nof\n200\nqueries\nthat\nwe\nrun\nagainst\na\n \n \n \n \n \n \n \n \n \n \n \n \ncorpus\nof\n8\nmillion\nsentences\ndrawn\nat\nrandom\nfrom\ntext\nin\nthe\n \n \n \n \n \n \n \n \n \n \n \n \nlegal\ndomain\n(US\njudicial\nopinions). \nOur\ntask\nconsists\nin\n \n \n \n \n \n \n \n \n \nretrieving\nand\nranking\nsentences\nthat\nare\nthe\nmost\nsimilar\nto\na\n \n \n \n \n \n \n \n \n \n \n \ngiven\nquery.\nAll\nof\nour\nqueries\nare\nlong\nnatural\nlanguage\nqueries,\n \n \n \n \n \n \n \n \n \n \n \n  \n \neach\nof\nwhich\nis\na\nsentence\npertaining\nto\nthe\nlegal\nfield.\nThe\n \n \n \n \n \n \n \n \n \n \n \n \nmedian\nof\nthe\nquery\nlengths\nis\n13\nterms,\nwhereas\nthe\nmedian\n \n \n \n \n \n \n \n \n \n \n \nlength of the sentences in the corpus is 26 terms.\n \n \nTo\ngenerate\nquery\nrelevance\njudgments\nfor\ntest\ndata,\nwe\nran\nall\n \n \n \n \n \n \n \n \n \n \n \nour\nexperiments\nand\nbaselines\non\nthe\ntest\nqueries,\nand\npooled\nthe\n \n \n \n \n \n \n \n \n \n \n \ntop-10\nresults\nfrom\ntheir\nrankings.\nWe\nasked\nlawyers\nto\nrate\neach\n \n \n \n \n \n \n \n \n \n \n \nquery-response\npair\non\na\n4-grade\nscale:\nexactly\non-point,\nrelevant,\n \n \n \n \n \n \n \n \n \nsomewhat\nrelevant,\nand\nirrelevant.\nEach\nquery-sentence\npair\nwas\n \n \n \n \n \n \n \n \nrated\nby\none\nassessor.\nWe\nused\nthese\nquery\nrelevance\njudgments\n \n \n \n \n \n \n \n \n \n \nto compute evaluation measures.\n \n \n3.3\n​ \n \nUnsupervised BERT Ranker\n \nBERT\nemploys\na\nmasked\nlanguage\nmodeling\napproach\nwhereby\n \n \n \n \n \n \n \n \nthe\nmodel\nis\ntrained\nto\nreconstruct\nan\noutput\nsequence\nfrom\nan\n \n \n \n \n \n \n \n \n \n \n \ninput\nsequence\nin\nwhich\na\nset\nfraction\n(15%)\nof\ntokens\nis\n \n \n \n \n \n \n \n \n \n \n \ncorrupted\nand/or\nmasked.\nThis\napproach\nforces\nthe\nmodel\nto\nlearn\n \n \n \n \n \n \n \n \n \n \nlanguage\nembeddings\nin\nan\nunsupervised\nfashion\nfrom\na\nlarge\n \n \n \n \n \n \n \n \n \nunlabeled\ndataset.\nThe\nmodel\nis\nfurther\ntrained\nusing\na\n \n \n \n \n \n \n \n \n \nclassification\ntask\nin\nwhich\nthe\nmodel\npredicts\nwhether\n2\n \n \n \n \n \n \n \n \n \nsentences\npicked\nfrom\nthe\ntraining\nset\nare\nconsecutive\nor\nnot.\nThe\n \n \n \n \n \n \n \n \n \n \n \noriginal\nBERT\nas\nreleased\nby\ngoogle\nwas\ntrained\non\na\ncombined\n \n \n \n \n \n \n \n \n \n \n \ncorpus\nof\nabout\n3.2B\ntokens\nbased\non\nBooksCorpus\nand\n \n \n \n \n \n \n \n \n \nWikipedia.\nMirroring\nthe\nmethodology\nused\nto\npretrain\nthe\n \n \n \n \n \n \n \n \noriginal\nBERT,\nour\nmodel\nwas\ntrained\nusing\na\nsequence\nlength\nof\n \n \n \n \n \n \n \n \n \n \n \n128\nfor\n1.3\nmillion\nsteps\nusing\na\nbatch\nof\n128\nand\nthen\nthe\ntraining\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nwas finalized using a sequence length of 256 for 44,000 steps.\n \n \nAfter\npre-training\nwe\nused\na\nTF/IDF\nweighted\naverage\npooling\n \n \n \n \n \n \n \n \n \nof\nthe\nhidden\nstates\nof\nthe\npenultimate\nhidden\nlayer\nas\nour\n \n \n \n \n \n \n \n \n \n \n \nsentence embeddings.\n \nWe\nfound\nthat\nthe\ncomponents\nof\nBERT\ntoken\nvectors\nvary\n \n \n \n \n \n \n \n \n \n \nwidely\nin\nscale.\nThis\nwide\nscale\nvariance\nfrustrates\nuse\nof\ndistance\n \n \n \n \n \n \n \n \n \n \n \nmetrics\nfor\nvector\ncomparison,\nincluding\nthe\ncosine\ndistance\n \n \n \n \n \n \n \n \nmetric,\nbecause\ncertain\ncomponents\ndominate\nthe\ncomparison,\n \n \n \n \n \n \n \ndrowning\nout\nothers.\nThis\ntype\nof\neffect\nhas\nbeen\nnoticed\nin\nthe\n \n \n \n \n \n \n \n \n \n \n \n \npast\nin\nword\nembedding\nsystems\nlike\nword2vec\n[15].\nTo\ndeal\nwith\n \n \n \n \n \n \n \n \n \n \n \nthis\neffect,\nwe\nnormalized\nthe\ncomponent\nscales\nby\ndividing\neach\n \n \n \n \n \n \n \n \n \n \ncomponent by its standard deviation.\n \nThe i-th sentence output embedding component \n is given by:\ny\ni\n \nF\nI\nD\nF\ny\ni\n=\n∑\n \nj\nT\n(\nj\n,\n)\nD\nx\ni\n,\nj\n√\nv\na\nr\ni\n \n \nWhere\nj\nrefers\nto\nthe\nj-th\ntoken\nin\nthe\nsentence,\nare\nBERT\n \n \n \n \n \n \n \n \n \n \nx\ni\n,\nj\n \n \n \noutput\nembeddings,\nand\nis\nthe\nvariance\nof\nall \ni-th\n \n \n \nv\na\nr\ni\n \n \n \n \n \n \n \nembedding components corresponding to all tokens in the dataset.\n \n3.4\n​ \n \nFine-tuned\nBERT\nRanker:\nSiamese-Network\n \n \n \n \nSentence transformer approach\n \nA\npretrained\nmodel\nsuch\nas\nBERT\ncan\nbe\nfinetuned\nfor\nspecific\n \n \n \n \n \n \n \n \n \n \n \ntasks\nin\norder\nto\nimprove\nperformance.\nThis\ntypically\nimplies\n \n \n \n \n \n \n \n \n \nadding\nfurther\nlayers\non\ntop\nof\nthe\nmodel\nthat\nconvert\nthe\noutput\n \n \n \n \n \n \n \n \n \n \n \n \nembeddings\ninto\nuseful\nfeatures\nfor\nthose\ndownstream\ntasks,\nand\n \n \n \n \n \n \n \n \n \nfurther\ntraining\nthe\ncombined\nmodel.\nIn\nthe\ncontext\nof\ntext\n \n \n \n \n \n \n \n \n \n \nsimilarity\nat\na\nscale\nwhere\npairwise\ncomparison\nby\nthe\nmodel\nat\n \n \n \n \n \n \n \n \n \n \n \nruntime\nis\ncomputationally\ninfeasible,\nthe\nmodel\nmust\nbe\ntrained\n \n \n \n \n \n \n \n \n \nto\nproduce\nvector\nrepresentations\nof\ntext\nthat\ncan\nbe\nstored\nand\n \n \n \n \n \n \n \n \n \n \n \nthen\ncompared\nat\nruntime\nusing\na\ncomputationally\ncheaper\n \n \n \n \n \n \n \n \ndistance\nmetric.\nFinetuning\nfor\nthis\ntask\nis\nperformed\nby\n \n \n \n \n \n \n \n \n \nsequentially\nsupplying\nthe\nmodel\nwith\npairs\nof\n \n \n \n \n \n \n \nsimilarity-judgement-labeled\nsentences\nand\nminimizing\nthe\ncosine\n \n \n \n \n \n \nloss\nbetween\nthe\nmean\npooled\noutput\nembeddings\nof\nsaid\n \n \n \n \n \n \n \n \n \nsentences.\nThis\napproach\nis\ndepicted\nin\nthe\nliterature\nas\na\n“siamese\n \n \n \n \n \n \n \n \n \n \n \nnetwork”\nwhere\ntwo\nidentical\nnetworks\nwith\nidentical\nand\ntied\n \n \n \n \n \n \n \n \n \nweights\noperate\non\ntwo\ndistinct\ninput\nsentences\nand\nthe\nloss\nis\n \n \n \n \n \n \n \n \n \n \n \ncomputed\nbetween\nthe\noutput\nof\nthose\n2\nnetworks\n[20].\nOur\nmodel\n \n \n \n \n \n \n \n \n \n \n \nwas\nfinetuned\non\ntwo\ndatasets\nnamely\nthe\nSTSb\ndataset\n[1]\n​ \nfor\n2\n \n \n \n \n \n \n \n \n \n \n \n \nepochs\nand\n1\nepoch\nof\nour\ncustom\nlaw\ndataset\ndescribed\nin\nthe\n \n \n \n \n \n \n \n \n \n \n \n \nthird paragraph of Section 3.1\n​ \n.\n \n3.5\n​ \n \nReranked Finetuned BERT Ranker\n \nReranking\nis\na\nsupplemental\nsupervised\nfinetuning\nprocedure\nin\n \n \n \n \n \n \n \n \nwhich\nthe\nmodel\nis\nfitted\nwith\na\nsimple\nbinary\nclassification\nhead\n \n \n \n \n \n \n \n \n \n \n \nwith\nbinary\ncross-entropy\nloss\n[17].\nThis\nclassification\nhead\nis\nfed\n \n \n \n \n \n \n \n \n \n \nvia\nthe\n[CLS]\noutput\nembedding\nof\nthe\nBERT\nmodel.\nThe\nmodel\n \n \n \n \n \n \n \n \n \n \n \nis\nthen\npresented\nsimultaneously\nwith\npairs\nof\ninputs,\nnamely\na\n \n \n \n \n \n \n \n \n \n \nquery\nsentence\nand\na\nresult\nsentence\nunder\nthe\nform\n<[CLS]\nquery\n \n \n \n \n \n \n \n \n \n \n \n[SEP]\nresult\n[PAD]>\nand\nfinetune\ntraining\nis\nperformed\nby\nusing\n \n \n \n \n \n \n \n \n \n \nthe\nbinary\nlabels\nthat\ndenote\nwhether\nor\nnot\nthe\nresult\nsentence\nis\na\n \n \n \n \n \n \n \n \n \n \n \n \n \ngood\nresponse\nfor\nthe\nquery.\nThe\narchitecture\nof\nthe\nfull\n \n \n \n \n \n \n \n \n \n \nBERT-based\nsimilar\nsentence\nretrieval\nand\nranking\nsystem\n \n \n \n \n \n \n \nincluding the BERT Reranker is illustrated in Figure 1.\n \n \nFigure\n1:\nHigh-level\narchitecture\nof\nthe\nfull\nBERT-based\nsimilar\n \n \n \n \n \n \n \n \n \ntext\nretrieval\nand\nranking\nsystem.\nThis\nfull\nsystem\nperformed\nbest\n \n \n \n \n \n \n \n \n \n \nof all our experiments as measured by ncdg@5.\n \n3.6\n​ \n \nRank Fusion Method\n \nWhile\nsupervised\nmethods\nsuch\nas\nour\nCNIR\nmethods\ndescribed\n \n \n \n \n \n \n \n \n \nabove\nare\nreceiving\nmuch\nattention\nand\nproducing\npromising\n \n \n \n \n \n \n \n \nresults\nfor\nranking\nand\nother\nproblems,\nunsupervised\nrank\nfusion\n \n \n \n \n \n \n \n \n \nmodels\ncan\nalso\nbe\nused\nto\nimprove\nresults.\nWe\nexplored\none\n \n \n \n \n \n \n \n \n \n \n \nsimple\nrank\nfusion\nmodel.\nOne\nbenefit\nof\nthis\nmodel\nis\nthat\nit\nis\n \n \n \n \n \n \n \n \n \n \n \n \n \nagnostic\nto\nthe\nunderlying\nmodels\nwhose\nrankings\nit\nis\nfusing.\nWe\n \n \n \n \n \n \n \n \n \n \n \nused\nit\nto\nfuse\nrankings\nfrom\nour\nreranked\nCNIR\nmodel\ndescribed\n \n \n \n \n \n \n \n \n \n \n \n  \n \nin\nsection\n3.5\nand\nour\nBM25\nbaseline\ndescribed\nin\nsection\n4.2.\nWe\n \n \n \n \n \n \n \n \n \n \n \n \ndescribe and analyze the results in section 5.\n \n \nThe\nfusion\nprocedure\nwas\nas\nfollows.\nLet\nD\nbe\nthe\nset\nof\n \n \n \n \n \n \n \n \n \n \n \n \ndocuments\nto\nbe\nranked,\nand\nR\nbe\nthe\nset\nof\nrankings\nto\nbe\nfused,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \neach\nof\nwhich\nis\npermutation\non\n1...|D|\ndocuments.\nThen\nour\nfinal\n \n \n \n \n \n \n \n \n \n \n \nfused ranking can be formulated as:\n \nc\no\nr\ne\nw\n(\nr\n)\nS\n(\nd\n)\n∈\nD\n=\n∑\n \nr\n∈\nR\nD\n|\n|\nD\n−\np\no\ns\n(\nd\n)\n+\n1\n|\n|\nr\n \nwhere\nis\nthe\nposition\nof\ndocument\nd\nin\nranking\n,\nand\n \np\no\ns\nd\n \n \n \n \n \n \n \n \n \nr\n \n \n(\nr\n)\nw\n \nis\nthe\nrelative\nweight\nor\nimportance\nassigned\nto\nranking\n​ \nr\n​ \n.\nis\n \n \n \n \n \n \n \n \n \n \n(\nr\n)\nw\n \n \na positive integer.\n \nIn\none\nof\nour\nexperiments,\nwe\nchoose\n=\n2\nfor\nthe\n \n \n \n \n \n \n \n(\nr\n)\nw\n \n \n \n \n \nreranked\nCNIR\nmodel,\nand\n=\n1\nfor\nthe\nbaseline\nBM25.\nIn\n \n \n \n \n(\nr\n)\nw\n \n \n \n \n \n \n \n \nanother experiment, we set both to 1.\n \n4\n​ \n \nExperiments\n \n4.1\n​ \n \nEvaluation Measures\n \nWe\nshow\nnDCG@5\nand\nnDCG@10\nresults.\nnDCG,\nnormalized\n \n \n \n \n \n \n \n \ndiscounted\ncumulative\ngain\n[8]\nhas\nbeen\nused\nextensively\nin\n \n \n \n \n \n \n \n \n \nseveral\ninformation\nretrieval\npapers,\nand\nis\none\nof\nthe\nprimary\n \n \n \n \n \n \n \n \n \n \nmeasures\nfor\nseveral\nTREC\ntracks,\nincluding\nWeb,\nSessions\nand\n \n \n \n \n \n \n \n \n \nMicroblog\ntracks.\nnDCG\nis\na\ngraded\nrelevance\nmeasure\nthat\n \n \n \n \n \n \n \n \n \nrewards\ndocuments\nwith\nhigh\nrelevance\ngrades\nand\ndiscounts\nthe\n \n \n \n \n \n \n \n \n \ngains of documents that are ranked at lower positions.\n \n4.2\n​ \n \nBaselines\n \nWe\nuse\ntwo\nbaselines.\nOur\nfirst\nbaseline,\nOkapi\nBM25,\nis\na\n \n \n \n \n \n \n \n \n \n \n \nTF/IDF-like\nranking\nfunction\nbased\non\na\nprobabilistic\nretrieval\n \n \n \n \n \n \n \n \nframework introduced by Robertson and Jones [21].\n \nOur\nsecond\nbaseline\nis\nthe\nsequential\nDependence\nModel\n \n \n \n \n \n \n \n \n(SDM)\nintroduced\nby\nMetzler\nand\nCroft\nas\na\nMarkov\nRandom\n \n \n \n \n \n \n \n \n \n \nField\nin\n[14].\nSDM\nis\na\ndiscriminative\nprobabilistic\nmodel\nthat\n \n \n \n \n \n \n \n \n \n \nattempts\nto\ncapture\nthe\nfact\nthat\nrelated\nterms\nare\nlikely\nto\nappear\n \n \n \n \n \n \n \n \n \n \n \n \nin\nclose\nproximity\nto\neach\nother.\nIn\nour\nSDM\nimplementation,\n \n \n \n \n \n \n \n \n \n \ndocuments\nin\nwhich\nadjacent\nquery\nterms\nappear\nin\nthe\nsame\n \n \n \n \n \n \n \n \n \n \norder\nare\nrewarded,\nand\nso\nare\ndocuments\nin\nwhich\nquery\nterms\n \n \n \n \n \n \n \n \n \n \n \nare in close proximity.\n \n5\n​ \n \nResults and Discussion\n \nTable\n1\nshows\nthe\nresults\nof\napplying\nvarious\nimplementations\nof\n \n \n \n \n \n \n \n \n \n \ncontext-sensitive\nneural\nlanguage\nmodels\nto\nlong\nnatural\nlanguage\n \n \n \n \n \n \n \n \nqueries,\nagainst\nour\ncorpus\ncontaining\n8\nmillion\nsentences.\nThe\n \n \n \n \n \n \n \n \n \nfirst\ntwo\nrows\nshow\nthe\nresults\nof\nour\nbaseline\nbag-of-words\n \n \n \n \n \n \n \n \n \n \nmodels.\nWe\nposited\nthat\nthe\nsequential\ndependence\nmodel,\nSDM,\n \n \n \n \n \n \n \n \n \nwould\nimprove\nperformance\nover\nsimple\nBM25,\nwhich\ndoes\nnot\n \n \n \n \n \n \n \n \n \naccount\nfor\nterm\nproximity.\nHowever,\nour\nresults\nshow\nthat\nBM25\n \n \n \n \n \n \n \n \n \n \nperforms\nbetter\nthan\nSDM\non\nlong\nnatural\nlanguage\nqueries.\nThis\n \n \n \n \n \n \n \n \n \n \ncould\nsuggest\nthat,\nfor\nqueries\nof\nthis\nlength,\naccounting\nfor\nterm\n \n \n \n \n \n \n \n \n \n \n \nproximity\nthe\nway\nSDM\ndoes\ncould\nbe\nthe\nwrong\nway\nof\n \n \n \n \n \n \n \n \n \n \n \ncapturing\ncontext.\nFor\na\nbetter\nunderstanding\nof\nthis\nresult,\nfurther\n \n \n \n \n \n \n \n \n \n \ninvestigation\ncomparing\nthe\neffects\nof\nSDM\non\nlong\nnatural\n \n \n \n \n \n \n \n \n \nlanguage\nqueries\nto\nits\neffects\non\nshort\nnatural\nlanguage\nqueries\n \n \n \n \n \n \n \n \n \n \nand bag of words, would be needed.\n \n \nndcg@5\n \nndcg@10\n \nBM25\n \n0.6712\n \n0.6609\n \nSDM\n \n0.6114\n \n0.6105\n \nBERT-Reranked BM25\n \n0.7375\n \n0.7132\n \nUnsupervised BERT\n \n0.695\n \n0.7094\n \nFine-Tuned BERT\n \n0.7483\n \n0.772\n \nReranked\nFine-Tuned\n \n \nBERT\n \n0.8082\n \n0.8094\n \nFused Reranked\n \nFine-Tuned\nBERT\nx\n \n \n \nBM25\n \n0.8145\n \n0.79\n \n \nTable\n1:\nExperimental\nresults\napplying\ncontext-sensitive\nneural\n \n \n \n \n \n \n \nlanguage models to long natural language queries.\n \n \nThe\nthird\nrow\nin\nTable\n1\nshows\nthe\nresults\nof\nusing\nBERT\nto\n \n \n \n \n \n \n \n \n \n \n \n \n \nrerank\nthe\ntop-100\nresults\nreturned\nby\nBM25,\nin\na\nfashion\nsimilar\n \n \n \n \n \n \n \n \n \n \n \nto\nprevious\nstudies\n[23].\nAs\nexpected,\nthis\nmodel\nleads\nto\n \n \n \n \n \n \n \n \n \n \nsignificant gains over traditional bag-of-words models.\n \nThe\nfourth\nrow\nin\nTable\n1\nshows\nthat\nranking\nsentences,\nafter\n \n \n \n \n \n \n \n \n \n \n \nfirst\npretraining\nBERT\non\nthe\nlaw\nand\nthen\nusing\nour\n \n \n \n \n \n \n \n \n \n \nUnsupervised\nBERT\napproach,\nleads\nto\nan\nimprovement\nover\nour\n \n \n \n \n \n \n \n \n \nstrongest\nbag-of-words\nbaseline\n(3.5%\nndcg@5\nincrease,\nand\n7%\n \n \n \n \n \n \n \n \nndcg@10\nincrease).\nHowever,\nthe\nimprovement\nis\nsmall\n \n \n \n \n \n \n \ncompared\nto\nthe\nimprovements\nobtained\nthrough\nall\nthe\nother\n \n \n \n \n \n \n \n \n \nBERT-based methods.\n \nThe\nresults\nin\nrows\n5\nand\n6\nof\nTable\n1\npresent\nsignificant\ngains\n \n \n \n \n \n \n \n \n \n \n \n \n \nover\nboth\nthe\nbag-of-words\nmodels\nand\nthe\nUnsupervised\nBERT\n \n \n \n \n \n \n \n \n \nmodel.\nDirectly\napplying\nFinetuned\nBERT\nto\nrank\nsentences\nleads\n \n \n \n \n \n \n \n \n \nto\n11%\nand\n17%\nimprovements\nin\nndcg@5\nand\nndcg@10\n \n \n \n \n \n \n \n \n \nrespectively\nover\nthe\nstrongest\nbaseline.\nThe\n“Siamese\nnetwork”\n \n \n \n \n \n \n \n \nFinetuned\nBERT\n(which\nproduces\nresults\nusing\na\nvector\nnearest\n \n \n \n \n \n \n \n \n \nneighbor\nsearch\nindex)\nalso\noutperforms\nthe\nmodel\nthat\nuses\n \n \n \n \n \n \n \n \n \nBERT\nto\nrerank\nthe\ntop-100\nresults\nreturned\nby\nBM25.\nOur\nresults\n \n \n \n \n \n \n \n \n \n \n \nsuggest\nthat,\nfor\nlong\nnatural\nlanguage\nqueries,\ndirectly\nranking\n \n \n \n \n \n \n \n \n \nsentences\nusing\nrepresentations\ncreated\nby\na\nfinetuned\nCNIR\n \n \n \n \n \n \n \n \nmodel\nyields\nbetter\nresults\nthan\nBERT\nmodels\nthat\nrerank\n \n \n \n \n \n \n \n \n \nbag-of-words\nresults.\nThis\nmay\nbe\nexplained\nby\nthe\ncontextual\n \n \n \n \n \n \n \n \n \nneural\nlanguage\nmodel’s\nability\nto\nretrieve\na\nbetter\ninitial\nset\nof\n \n \n \n \n \n \n \n \n \n \n \nresults than a traditional bag-of-words model like BM25. \n \nFurthermore,\nwe\nfind\nthat,\nstarting\nwith\nan\ninitial\nlist\nof\n \n \n \n \n \n \n \n \n \n \nsentences\nranked\nby\nthe\nSiamese\nnetwork\nfine-tuned\nBERT-based\n \n \n \n \n \n \n \n \nmodel,\nand\nthen\nreranking\nthat\ninitial\nlist\nusing\nthe\nBERT\n \n \n \n \n \n \n \n \n \n \n  \n \nReranker,\nleads\nto\nfurther\nimprovements.\nReranking\nled\nto\n8%\nand\n \n \n \n \n \n \n \n \n \n \n5%\nincreases\nrespectively\nin\nndcg@5\nand\nndcg@10\nover\nthe\n \n \n \n \n \n \n \n \n \ninitial Siamese network-based Finetuned BERT Ranker.\n \nFinally,\nin\nthe\nlast\nrow\nof\nTable\n1,\nwe\nshow\nthe\nresults\nof\nthe\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nRank\nFusion\n(RF)\nmodel\nthat\ngives\nmore\nimportance\nto\nthe\n \n \n \n \n \n \n \n \n \n \nReranked\nFine-Tuned\nBERT\nranker\n(\n=\n2\nfor\nthe\nReranked\n \n \n \n \n(\nr\n)\nw\n \n \n \n \n \n \nFine-Tuned\nBERT\nranker\nmodel,\nand\n=\n1\nfor\nthe\nbaseline\n \n \n \n \n \n(\nr\n)\nw\n \n \n \n \n \n \nBM25\nmodel).\nThe\nresults\nare\nclose,\nbut\nit\nis\nworth\nnoting\nthat\n \n \n \n \n \n \n \n \n \n \n \n \nndcg@5\nresults\nfor\nthe\nRF\nmodel\nare\nhigher\nthan\nboth\nmodels\nthat\n \n \n \n \n \n \n \n \n \n \n \n \nare\nbeing\nfused,\nsuggesting\nthat\nthe\nRF\nmodel\nmay\nbe\na\ngood\nway\n \n \n \n \n \n \n \n \n \n \n \n \n \nto promote better sentences to the top of the ranked list.\n \n4\n​ \n \nConclusion and Future Work\n \nIn\nthis\npaper,\nwe\nshowed\nthat\nwe\ncan\nobtain\nsubstantial\ngains\nin\n \n \n \n \n \n \n \n \n \n \n \n \nranking\neffectiveness\nfor\nlong\nnatural\nlanguage\nqueries\nby\nmaking\n \n \n \n \n \n \n \n \n \nmodifications\nto\na\ncontextual\nneural\nlanguage\nmodel,\nBERT.\n \n \n \n \n \n \n \n \nAdditionally,\nwe\nshowed\nthat\ndirectly\nusing\na\nfinetuned\nBERT\n \n \n \n \n \n \n \n \n \nmodel\nwith\na\n“siamese\nnetwork”\narchitecture\nto\nrank\nsentences\n \n \n \n \n \n \n \n \n \noutperforms\nusing\nBERT\nto\nrerank\nan\ninitial\nlist\nof\nsentences\n \n \n \n \n \n \n \n \n \n \nretrieved\nby\nBM25.\nWe\nobtained\neven\nfurther\ngains\nwhen\nusing\na\n \n \n \n \n \n \n \n \n \n \n \nfinetuned\nBERT-based\nreranker\nto\nrerank\nsentences\nthat\nwere\n \n \n \n \n \n \n \n \ninitially ranked by the BERT-based ranker.\n \nInteresting\nfuture\nwork\nincludes\ninvestigating\nand\ncomparing\n \n \n \n \n \n \n \nthese\nBERT-based\nranking\nmodels\nnot\njust\non\nlong\nnatural\n \n \n \n \n \n \n \n \n \nlanguage\nqueries,\nbut\nalso\non\nqueries\nof\nvaried\nlengths.\n \n \n \n \n \n \n \n \n \nAdditionally,\nthe\nfact\nthat\nthese\nBERT-based\nmethods\nwork\nwell\n \n \n \n \n \n \n \n \n \nsuggests\nthat\nmore\nrecent\nneural\ntransformer\nbased\nlanguage\n \n \n \n \n \n \n \n \nmodels,\nwhich\nhave\nbeen\nshown\nto\noutperform\nBERT\nacross\na\n \n \n \n \n \n \n \n \n \n \nrange\nof\nnatural\nlanguage\ntasks\n[2],\n[7],\n[12],\n[19],\n[24],\nmay\nyield\n \n \n \n \n \n \n \n \n \n \n \n \nfurther gains for similar sentence retrieval and ranking.\n \nREFERENCES\n \n[1]\nCer,\nD.,\nDiab,\nM.,\nAgirre\nE.,\nLopezGazpio\nIigo.,\nand\nSpecia\nL..\n2017.\n \n \n \n \n \n \n \n \n \n \n \n \nSemEval-2017\nTask\n1:\nSemantic\nTextual\nSimilarity\nMultilingual\nand\n \n \n \n \n \n \n \n \nCrosslingual Focused Evaluation. In SemEval.\n \n[2]\nClark,\nK.,\nLuong,\nM.,\nLe,\nQ.,\nManning,\nC.\n(2020,\nMarch).\nELECTRA:\n \n \n \n \n \n \n \n \n \n \n \nPre-training\nText\nEncoders\nas\nDiscriminators\nRather\nThan\nGenerators.\narXiv\n \n \n \n \n \n \n \n \n \npreprint arXiv:2003.10555.\n \n[3]\nDai,\nZ.,\n&\nCallan,\nJ.\n(2019,\nJuly).\nDeeper\ntext\nunderstanding\nfor\nIR\nwith\n \n \n \n \n \n \n \n \n \n \n \n \n \ncontextual neural language modeling. In SIGIR.\n \n[4]\nDevlin,\nJ.,\nChang,\nM.,\nLee,\nK.,\nToutanova,\nK.\n(2018,\nNovember).\nBERT:\n \n \n \n \n \n \n \n \n \n \n \nPre-training\nof\nDeep\nBidirectional\nTransformers\nfor\nLanguage\nUnderstanding.\n \n \n \n \n \n \n \n \nIn Proc. of NAACL-HLT..\n \n[5]\nFan,\nY.,\nPang,\nL.,\nHou,\nJ.,\nGuo,\nJ.,\nLan,\nY.,\n&\nCheng,\nX.\n(2017).\nMatchzoo:\nA\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntoolkit for deep text matching. arXiv preprint arXiv:1707.07270.\n \n[6]\nGupta,\nM.,\n&\nBendersky,\nM.\n(2015).\nInformation\nretrieval\nwith\nverbose\nqueries.\n \n \n \n \n \n \n \n \n \n \n \nFoundations and Trends® in Information Retrieval, 9(3-4), 209-354.\n \n[7]\nHoward,\nJ.,\nRuder,\nS.\n(2018,\nMay).\nUniversal\nLanguage\nmodel\nFine-tuning\nfor\n \n \n \n \n \n \n \n \n \n \n \nText Classification. In Proc. of ACL.\n \n[8]\nJärvelin,\nK.,\n&\nKekäläinen,\nJ.\n(2002).\nCumulated\ngain-based\nevaluation\nof\nIR\n \n \n \n \n \n \n \n \n \n \n \ntechniques. ACM TOIS, 20(4), 422-446.\n \n[9]\nJeffrey\nPennington,\nRichard\nSocher,\nand\nChristopher\nD.\nManning.\n2014.\nGlove:\n \n \n \n \n \n \n \n \n \n \nGlobal Vectors for Word Representation. In EMNLP.\n \n[10]\nJohnson,\nJ.,\nDouze,\nM.,\nJegou,\nH.\n(2017,\nFebruary).\nBillion-scale\nsimilarity\n \n \n \n \n \n \n \n \n \n \nsearch with GPUs. arXiv preprint arXiv:1702.08734.\n \n[11]\nLin,\nJ.,\nEfron,\nM.,\nWang,\nY.,\n&\nSherman,\nG.\n(2014).\nOverview\nof\nthe\ntrec-2014\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmicroblog track. In TREC.\n \n[12]\nLiu,\nY.,\nOtt,\nM.,\nGoyal,\nN.,\nDu,\nJ.,\nJoshi,\nM.,\nChen,\nD.,\nLevy,\nO.,\nLewis,\nM.,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nZettlemoyer,\nL.,\nStoyanov,\nV.\n(2019,\nJuly).\nRoBERTa:\nA\nRobustly\nOptimized\n \n \n \n \n \n \n \n \n \n \nBERT Pretraining Approach. arXiv preprint arXiv:1907.11692.\n \n[13]\nMacAvaney,\nS.,\nYates,\nA.,\nCohan,\nA.,\n&\nGoharian,\nN.\n(2019).\nContextualized\n \n \n \n \n \n \n \n \n \n \n \nWord\nRepresentations\nfor\nDocument\nRe-Ranking.\narXiv\npreprint\n \n \n \n \n \n \n \narXiv:1904.07094.\n \n[14]\nMetzler,\nD.,\n&\nCroft,\nW.\nB.\n(2005,\nAugust).\nA\nMarkov\nrandom\nfield\nmodel\nfor\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nterm dependencies. In SIGIR.\n \n[15]\nMikolov,\nT.,\nSutskever,\nI.,\nChen,\nK.,\nCorrado,\nG.\nS.,\n&\nDean,\nJ.\n(2013).\n \n \n \n \n \n \n \n \n \n \n \n \n \nDistributed\nrepresentations\nof\nwords\nand\nphrases\nand\ntheir\ncompositionality.\nIn\n \n \n \n \n \n \n \n \n \n \nNIPS.\n \n[16]\nMu,\nJ.,\nViswanath,\nP.\n(2018,\nMarch).\nAll-but-the-top:\nSimple\nand\nEffective\n \n \n \n \n \n \n \n \n \n \n \nPost-processing for Word Representations. In ICLR.\n \n \n[17]\nNogueira,\nR.,\n&\nCho,\nK.\n(2019).\nPassage\nRe-ranking\nwith\nBERT.\narXiv\n \n \n \n \n \n \n \n \n \n \n \npreprint arXiv:1901.04085.\n \n[18]\nPadigela,\nH.,\nZamani,\nH.,\n&\nCroft,\nW.\nB.\n(2019).\nInvestigating\nthe\nsuccesses\n \n \n \n \n \n \n \n \n \n \n \n \nand failures of BERT for passage re-ranking. arXiv preprint arXiv:1905.01758.\n \n[19]\nRaffel,\nC.,\nShazeer,\nN.,\nRoberts,\nA.,\nLee,\nK.,\nNarang,\nS.,\nMatena,\nM.,\nZhou,\nY.,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nLi,\nW.,\nLiu,\nP.\n(2019,\nOctober).\nExploring\nthe\nLimits\nof\nTransfer\nLearning\nwith\n \n \n \n \n \n \n \n \n \n \n \n \n \nUnified Text-to-Text Transformer. arXiv preprint arXiv:1910.10683.\n \n[20]\nReimers,\nN\n&\nGurevych,\nI\n(2019)\nSentence-BERT:\nSentence\nEmbeddings\nusing\n \n \n \n \n \n \n \n \n \n \nSiamese BERT-Networks. In EMNLP-IJCNLP.\n \n[21]\nRobertson,\nS.\nE.,\nWalker,\nS.,\nJones,\nS.,\nHancock-Beaulieu,\nM.\nM.,\n&\nGatford,\n \n \n \n \n \n \n \n \n \n \n \n \nM. (1995). Okapi at TREC-3. In NIST Special Publication.\n \n[22]\nYang,\nW.,\nLu,\nK.,\nYang,\nP.,\n&\nLin,\nJ.\n(2019,\nJuly).\nCritically\nExamining\nthe\"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nNeural\nHype\"\nWeak\nBaselines\nand\nthe\nAdditivity\nof\nEffectiveness\nGains\nfrom\n \n \n \n \n \n \n \n \n \n \n \nNeural Ranking Models. In SIGIR.\n \n[23]\nYang,\nW.,\nZhang,\nH.,\n&\nLin,\nJ.\n(2019).\nSimple\napplications\nof\nBERT\nfor\nad\nhoc\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndocument retrieval. arXiv preprint arXiv:1903.10972.\n \n[24]\nYang,\nZ.,\nDai,\nZ.,\nYang,\nY.,\nCarbonell,\nJ.,\nSalakhutdinov,\nR.,\nLe,\nQ.\n(2019,\n \n \n \n \n \n \n \n \n \n \n \n \n \nJune).\nXLNet:\nGeneralized\nAutoregressive\nPretraining\nfor\nLanguage\n \n \n \n \n \n \n \nUnderstanding. arXiv preprint arXiv:1906.08237.\n \n[25]\nYilmaz,\nZ.\nA.,\nYang,\nW.,\nZhang,\nH.,\n&\nLin,\nJ.\n(2019,\nNovember).\n \n \n \n \n \n \n \n \n \n \n \n \nCross-domain\nmodeling\nof\nsentence-level\nevidence\nfor\ndocument\nretrieval.\nIn\n \n \n \n \n \n \n \n \n \nEMNLP-IJCNLP.\n \n  ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6363193392753601
    },
    {
      "name": "Transformer",
      "score": 0.6338076591491699
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5803529620170593
    },
    {
      "name": "Natural language processing",
      "score": 0.5131884217262268
    },
    {
      "name": "Information retrieval",
      "score": 0.5096840858459473
    },
    {
      "name": "Language model",
      "score": 0.42078089714050293
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4079127907752991
    },
    {
      "name": "Engineering",
      "score": 0.09430655837059021
    },
    {
      "name": "Electrical engineering",
      "score": 0.05167710781097412
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}