{
  "title": "Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model",
  "url": "https://openalex.org/W3094916564",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5071994298",
      "name": "Jason Obeid",
      "affiliations": [
        "York University"
      ]
    },
    {
      "id": "https://openalex.org/A5067722075",
      "name": "Enamul Hoque",
      "affiliations": [
        "York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949417144",
    "https://openalex.org/W2962841871",
    "https://openalex.org/W2983105506",
    "https://openalex.org/W2400082606",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2948556812",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1991278297",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2072181127",
    "https://openalex.org/W3028907449",
    "https://openalex.org/W3022814719",
    "https://openalex.org/W2054362711",
    "https://openalex.org/W2146751667",
    "https://openalex.org/W2038909631",
    "https://openalex.org/W2963290255",
    "https://openalex.org/W2003170434",
    "https://openalex.org/W2969478830",
    "https://openalex.org/W1485106551",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2020776366",
    "https://openalex.org/W2888660171",
    "https://openalex.org/W3019839800",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W103848519",
    "https://openalex.org/W2116716943",
    "https://openalex.org/W2963592583",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W1996457856"
  ],
  "abstract": "Information visualizations such as bar charts and line charts are very popular for exploring data and communicating insights. Interpreting and making sense of such visualizations can be challenging for some people, such as those who are visually impaired or have low visualization literacy. In this work, we introduce a new dataset and present a neural model for automatically generating natural language summaries for charts. The generated summaries provide an interpretation of the chart and convey the key insights found within that chart. Our neural model is developed by extending the state-of-the-art model for the data-to-text generation task, which utilizes a transformer-based encoder-decoder architecture. We found that our approach outperforms the base model on a content selection metric by a wide margin (55.42% vs. 8.49%) and generates more informative, concise, and coherent summaries.",
  "full_text": "Proceedings of The 13th International Conference on Natural Language Generation, pages 138–147,\nDublin, Ireland, 15-18 December, 2020.c⃝2020 Association for Computational Linguistics\n138\nChart-to-Text: Generating Natural Language Descriptions for Charts by\nAdapting the Transformer Model\nJason Obeid\nSchool of Information Technology\nYork University, Canada\njobeid98@my.yorku.ca\nEnamul Hoque\nSchool of Information Technology\nYork University, Canada\nenamulh@yorku.ca\nAbstract\nInformation visualizations such as bar charts\nand line charts are very popular for exploring\ndata and communicating insights. Interpreting\nand making sense of such visualizations can\nbe challenging for some people, such as those\nwho are visually impaired or have low visual-\nization literacy. In this work, we introduce a\nnew dataset and present a neural model for au-\ntomatically generating natural language sum-\nmaries for charts. The generated summaries\nprovide an interpretation of the chart and con-\nvey the key insights found within that chart.\nOur neural model is developed by extending\nthe state-of-the-art model for the data-to-text\ngeneration task, which utilizes a transformer-\nbased encoder-decoder architecture. We found\nthat our approach outperforms the base model\non a content selection metric by a wide margin\n(55.42% vs. 8.49%) and generates more infor-\nmative, concise, and coherent summaries.\n1 Introduction\nInformation visualizations such as bar charts and\nline charts are commonly utilized by people to get\ninsights from data and make informed decisions.\nHowever, understanding and getting insights from\ncharts can be difﬁcult and time-consuming. This\nis because people often need to visually compare\nbetween several graphical marks (e.g. bars) of a\nchart to infer key insights from data, which may\nbe challenging when the chart involves many data\nitems (Kim et al., 2020).\nGenerating a natural language summary to ex-\nplain a chart has numerous beneﬁts and potential\napplications. It can help users in understanding\nand interpreting charts by conveying key points\nabout the chart by focusing on temporal, causal,\nand evaluative aspects (Carenini et al., 2013). It can\nhelp people identify insights from charts that they\notherwise might have missed. A previous study\nconducted on a chart corpus found that the text\nassociated with the chart failed to convey any in-\nsight from that chart in 35% of the instances, while\nin another 26% of cases the text conveyed only a\nportion of the chart’s intended message (Carberry\net al., 2006). Therefore, effective chart summariza-\ntion could help data analysts, business analysts, or\njournalists in better preparing reports from data.\nSuch summaries could also enable people who are\nvisually impaired or have low cognitive abilities\nto comprehend charts and perform analytical tasks\nthrough audio (Ferres et al., 2013).\nHowever, natural language generation (NLG)\nsystems for charts are still in their infancy. Early\nwork mostly focused on statistical methods for ﬁnd-\ning salient information from charts and planning-\nbased approaches for structuring content (Reiter,\n2007) to generate textual captions from basic\ncharts (Fasciano and Lapalme, 1996; Mittal et al.,\n1998; Green et al., 2004; Demir et al., 2012).\nCommercial systems such as Quill (Quill, 2020)\nand Wordsmith (Wordsmith, 2020) have similarly\nadopted statistical algorithms and template-based\nNLG methods which are used to produce data\nfacts in textual form. Unfortunately, the predeﬁned\ntemplate-based NLG methods and planning-based\narchitecture for generating summaries often lack\ngenerality and may not offer variations in style.\nMoving beyond template and planning-based ap-\nproaches (Reiter, 2007), more recently researchers\nconsidered data-driven neural models for generat-\ning text from data tables (Mei et al., 2016; Gong\net al., 2019). However, they do are not designed\nfor chart summarization as they do not consider\nthe chart speciﬁc features (e.g. chart type). To\nour knowledge, there have not been any efforts to\ndevelop deep neural models that are speciﬁcally\ntailored for generating chart summaries.\nIn this paper, we present a neural model for chart\nsummarization by extending a transformer-based\n139\nFigure 1: An example of a generated natural language\nexplanation from a chart from our model.\nmodel that was originally designed for the data-\nto-text generation task. A key challenge in devel-\noping neural models for chart summarization is\nthe lack of a suitable dataset containing a large set\nof charts and corresponding human-written sum-\nmaries. To address the challenge we ﬁrst devel-\noped a corpus of 8,305 charts, where for each chart\nwe crawled the corresponding data table and the\nhuman-written summary. Our model learns content\nselection and text generation from such a collection\nof chart-summary pairs to generate summaries of\ncharts. We also introduce a data variable substitu-\ntion method that replaces the tokens referring to\nchart data with variables to generate more factually\ncorrect statements than a competitive baseline. As\nshown in Figure 1, the resulting summary is quite\ninformative, concise, and coherent.\nThe contributions of our paper are three-fold:\n• First, we introduce a new large-scale corpus\non chart summarization consisting of human-\nwritten summaries of charts along with chart\nimages and their underlying data.\n• Second, we adopt a transformer-based model\nto generate chart summaries, which learns\nfrom chart-summary pairs from our dataset.\nTo our knowledge, our work is the ﬁrst to in-\nvestigate the problem of chart summarization\nusing a data-driven deep neural model.\n• Finally, we perform a series of evaluations to\ncompare our model’s performance with a base-\nline model derived from Gong et al. (Gong\net al., 2019). As a secondary contribution,\nwe will make our source codes and the new\ndataset used in this research publicly avail-\nable.\n2 Related Work\n2.1 Chart Summarization\nEarly work on generating natural language summa-\nrization follows some planning-based approaches\nwhich can be captured by Reiter’s NLG architec-\nture for data-to-text generation (Reiter, 2007). For\nexample, (Mittal et al., 1998) developed a caption\ngeneration system which determines the structure\nof the caption based on the mappings between the\ndata and marks of the charts (such as text, lines,\nand bars) and chooses a complexity metric to select\ndetails of the caption. It then uses a text plan-\nner to generate the description of the chart. The\niGRAPH-Lite system (Ferres et al., 2013) aims to\nmake charts accessible to blind users via generat-\ning captions and supporting navigation through the\nchart using a keyboard. Like (Mittal et al., 1998)\nit uses templates to provide a short description of\nwhat the chart looks like. Nonetheless, these sys-\ntems describe the chart only in terms of how to\ninterpret the chart rather than explaining high-level\ninsights conveyed by the chart.\nOther research has focused on generating multi-\nmedia presentations combining both text summary\nand charts (Fasciano and Lapalme, 2000; Green\net al., 2004). PostGraphe takes user-speciﬁed inten-\ntion (e.g. increasing trend) and a spreadsheet ﬁle as\ninput and then generates a simple chart and caption\nseparately using some heuristics (Fasciano and La-\npalme, 2000). Autobrief generates presentations in\ntext and information graphics (Green et al., 2004)\nusing an integrated planning-based approach in the\ndomain of transportation scheduling.\nThere has also been growing interest to automat-\nically extract insights from a dataset and present\nthem using template-based NLG. Examples of\nsuch automatic insight generation include research\nprototypes such as DataSite (Cui et al., 2019),\nDataShot (Wang et al., 2019) and V oder (Srini-\nvasan et al., 2018) as well as commercial systems\nlike Quill1, Arria2, and Wordsmith3. These systems\ntypically perform some statistical analysis to infer\npotentially important or interesting facts about the\ndata and then present them in natural language sen-\ntences and charts. (Demir et al., 2012) present a\n1Narrative Science. Quill\n2Arria, Arria\n3Wordsmith Wordsmith\n140\nmethod that computes some statistics (e.g. min,\nmax, trends) and identiﬁes the intended message\nthat a bar chart is conveying using a Bayesian in-\nference system. Then, it generates the summary in\na bottom–up approach to simultaneously construct\nthe discourse and sentence structures of textual\nsummaries. More recently, (Chen et al., 2019) uses\nthe encoder-decoder architecture where a Residual\nNetwork (ResNet) (He et al., 2016) in the encoder\nis used to recognize the input chart from an im-\nage, and Long Short-Term Memory (LSTM) and\nattention in the decoder to create template-based\ncaptions.\nA common limitation of the above body of work\nis that the sentences are generated using predeﬁned\ntemplate-based approaches which may lack gen-\nerality and offer fewer variations in grammatical\nstyle and lexical choices compared to data-driven\nmodels. In contrast, we focus on learning language\nvariations and automatically ﬁnding important in-\nsights from charts using a deep learning model on\na large collection of chart-summary pairs.\n2.2 Data-to-Text Generation\nThe objective of data-to-text generation is to gen-\nerate a descriptive summary given structured data.\nData-to-text generation focuses on creating a de-\nscriptive summary from structured data which can\nbe encoded as a table of records. Data to text gener-\nation has been explored for various domain-speciﬁc\ntasks such as summarizing sport game data (Barzi-\nlay and Lapata, 2005; Liang et al., 2009; Wiseman\net al., 2017), weather-forecast data (Reiter et al.,\n2005), recipe generation (Yang et al., 2017) and\nbiography generation (Lebret et al., 2016).\nSeveral recent methods have primarily focused\non using sequence-to-sequence learning methods\n(Mei et al., 2016; Lebret et al., 2016; Wiseman\net al., 2017). For example, (Mei et al., 2016) pro-\npose an encoder-decoder model that uses recurrent\nneural networks with LSTM units for jointly learn-\ning content selection and surface realization for\nweather forecast and soccer commentaries. (Wise-\nman et al., 2017) present a new dataset on NBA\ngame summarization and evaluate several models\nincluding attention-based encoder-decoder models.\nThey found that neural text generation techniques\nfrom data perform poorly at content selection and\nlack inter-sentential coherence. (Puduppully et al.,\n2019) attempt to address this problem by incorpo-\nrating content selection and planning mechanisms\nwithin the neural model. (Gong et al., 2019) found\nthat the transformer model yielded outputs more\nﬂuent and coherent when compared to their seq2seq\ncounterparts, which is why we use it as the base\nmodel.\n3 Chart Summarization Dataset\nThere have been several benchmark datasets that\nare made available recently for the data-to-text gen-\neration tasks (Lebret et al., 2016; Wiseman et al.,\n2017; Chen et al., 2020; Parikh et al., 2020). How-\never, to the best of our knowledge, there are no pub-\nlicly available large datasets of chart data paired\nwith human-generated summaries. While some\nprior work on generating captions and summaries\nfrom charts (e.g. (Mittal et al., 1998; Green et al.,\n2004; Demir et al., 2012)) exist, to our knowledge\nthey do not provide any large datasets with chart-\nsummary pairs.\nWith the above challenges in mind, we create\na new dataset for chart summarization which is\navailable at https://github.com/JasonObeid/\nChart2Text. In order to ﬁnd the best source of\ndata for our corpus, we have analyzed publicly\navailable charts from various sources such as text-\nbooks, research papers, news articles, and websites\nthat contain data charts and facts. This led us to\nchoose Statista4 as our data source. Statista regu-\nlarly publishes charts from data collected by market\nand opinion research institutes, and data derived\nfrom the economic sector. Since Statista has all\nthe necessary metadata including the data tables,\ntitles, and concise summaries of charts it was a\nsuitable source for our purpose. The dataset was\ncrawled from 23,382 freely accessible pages from\nstatista.com in early March of 2020, yielding a to-\ntal of 8,305 charts, and associated summaries. For\neach chart, we downloaded the chart image, the\nunderlying data table, the title, the axis labels, and\na human-written summary describing the statistic.\nAfter examining the crawled dataset, we found\nthat out of 8,305 charts, 7,726 charts were lack-\ning the x-axis labels. To address this problem we\nsearched by regular expressions and applied named\nentity recognition using CoreNLP (Manning et al.,\n2014) on the data table to automatically identify\nif the x-axis represented common temporal dimen-\nsions such as years or months. For the remaining\n1,353 missing labels we used human annotators\nwho examined the title, summary and the chart to\n4https://www.statista.com/\n141\nLine Bar Total:\nSimple 3564 3199 6763\nComplex 902 640 1542\nTotal: 4466 3839\nTable 1: Chart type distribution\nStatistic Value\nMean Token Count 113.4\nMean Sentence Count 5.2\nV ocab Size 19,150\nTotal Tokens 941.8k\nMean Data Cells 32.3\nTable 2: Dataset statistics\ncome up with short descriptive labels for the x-axis.\nThe resulting dataset can be described as fol-\nlows: let the set of chart-summaries CS =\n[I, D, T, L, S], where for each chart-summary in\nCS there is a chart image i ∈ I, data table d ∈ D,\ntitle t ∈ T, axis labels l ∈ L, and summary s ∈ S.\nThe dataset consists of both bar and line charts (Ta-\nble 2). A simple bar chart only has a set of bars, and\na simple line chart contains a single line. Complex\nbar charts include stacked and grouped bar charts\nwhile complex line charts have multiple lines.\nThe summaries of the charts are concise, with an\naverage sentence count of 5.2, and an average to-\nken count of 113.4. The data tables were moderate\nin size with an average of 32.3 cells. The discourse\nstructures of these summaries were usually simi-\nlar. They usually start by describing the chart at\na high-level in terms of what this chart is about\nand then describing and comparing salient points\nin the chart. Some common salient information and\nstatistics are extremes (e.g. highest/lowest values)\nor trends (upward/downward tendencies) or simple\nvalue retrieval (e.g. mentioning the ﬁrst/last data\npoint’s value).\n4 The Chart-to-Text Model\nIn this section, we ﬁrst describe the Transformer-\nbased Model for Data-to-Text (Gong et al., 2019)\nwhich we use as our base model, followed by our\nadaptations to this model for generating chart sum-\nmaries. Then, we describe our experiments with\nmodel parameters and the training procedure.\n4.1 Base Model\nOur base model (Gong et al., 2019) extends the\nstandard transformer (Vaswani et al., 2017) by\nadding a binary prediction layer and a content se-\nlection training step. The input layer of the model\naccepts a tuple of four features (entity, type, value,\ninformation) as input. The model then generates\nthe latent representation of each record in the input\nsequence and passes it through the binary predic-\ntion layer. The decoder of this model is the same\nas the original Transformer model and predicts the\nsummary based on encoder’s output. The model\nalso removes the positional embedding of the trans-\nformer encoder, as there is no ordered relationship\nwithin the records of their dataset.\n4.2 Our Proposed Approach\nFigure 2 shows an overview of our proposed\napproach. The model takes data records and\nother chart information as input. Like the base\nmodel (Gong et al., 2019), it uses the self-attention\nmechanism of the transformer encoder to gener-\nate the latent representations of the input, and the\nbinary prediction at the top of the Transformer en-\ncoder output to decide whether or not a record will\nbe mentioned in the target summary. Finally, the\ndecoder predicts the next token in the context of\nthe encoder output and the previous tokens from\nthe summary sequence.\nIn order to adapt the enhanced transformer\nmodel (Gong et al., 2019) for generating chart sum-\nmarization, we introduce three main changes. First,\nwe modify the four features of the record tuples\nused as input to the model. This is done to include\nadditional information which is important for chart\nsummarization. Second, we reintroduce positional\nembeddings to the encoder since charts tend to con-\ntain ordered relationships. Finally, we introduce a\nprocess of substituting tokens with data variables\nto minimize the amount of generated hallucina-\ntions, a problem commonly found in NLG where a\npredicted word is not grounded in truth (Wiseman\net al., 2017; Parikh et al., 2020). We now discuss\neach of these changes in more details:\nInput embedding: For each data table d ∈ D,\nwe pre-process it into a set of records r ∈ R. Each\nof these records are placed in a tuple with four\nfeatures as follows:\n• ri(0) contains the column header,\n• ri(1) contains the table cell value,\n• ri(2) contains the column index, and\n142\nFigure 2: An overview of the proposed approach for chart summarization using a transformer-based model. The\nmodel takes the data table and some chart metadata as input (on the left) and generates a summary containing data\nvariables that refer values within the data table.\n• ri(3) contains the chart type\nEach feature is embedded into a vector, and to-\ngether they are concatenated to represent the record\nas shown below:\n∀ r ∈ R:\nri = [ri(0); ri(1); ri(2); ri(3)]\nOur model takes each of these records ri as input,\nand outputs a set of predicted tokens which we will\ndenote as y ∈ Y . To get the ﬁnal summary, for each\ntoken yi in y, if the token is a data variable then it\nis substituted with the corresponding data value.\nPositional embedding: Unlike the sports\ndataset used by (Gong et al., 2019), chart data of-\nten involves an ordered dimension such as temporal\ndata (e.g. year, month), or ordinal categories in bar\ncharts. In order to generate summaries that bet-\nter capture salient features from such sequential\ndata, we re-introduce positional embeddings to our\nmodel.\nData variable substitution: A critical issue\nwith various existing models for data-to-text gener-\nation problem such as (Gong et al., 2019) is that\nthey treat data records mentioned in the summaries\nas regular tokens which often results in hallucina-\ntion problem. As a consequence, the model some-\ntimes predicts tokens that are irrelevant to the chart\nand thus results in factually incorrect sentences.\nThis problem becomes even more serious for our\nchart dataset which is not focused on a speciﬁc do-\nmain (e.g. sports). In order to improve the factual\naccuracy of the generated summaries we introduce\na data variable substitution method.\nThe idea is that before training we ﬁrst modify\nthe gold summaries so that whenever a token ref-\nerences something from the data table, chart title,\nor axis labels we replace them with one of the pre-\ndeﬁned data variables. We then use these modiﬁed\nsummaries to train the model so that it learns how\nto generate the summary more generally with data\nvariables as opposed to actual values in the data\ntable or tokens from the title. Then, during the\ntesting phase, if a generated token matches a prede-\nﬁned data variable, we make a look-up operation to\nconvert the data variable into the referenced chart\ndata (see Figure 3).\nWe deﬁne seven categories of data variables\nlisted in order of priority: subjects, dates, axis la-\nbels, titles, table cells, trends, and scales. Here\nsubjects refer to entities relevant to the data table\n(e.g. ‘Liverpool FC’) and trends refer to tokens\nthat indicate positive or negative trends (e.g. grow-\ning, decreasing, etc). Scales refer to tokens such as\n‘millions’ and ‘percentage’ that are associated with\nnumeric tokens. For each token ti in a gold sum-\nmary s, if that token matches any of these variable\ncategories, then it is substituted by that variable.\nWe apply named entity recognition (Manning et al.,\n2014) to detect the subjects and dates. The rest\nof the variables are detected using simple string\npattern matching techniques. For each variable,\nwe also assign the relevant index position in the\ndata table or other chart data. For example, given\na sentence in the gold summary “Broadcasting is\nthe largest source of revenue for Liverpool FC”,\n143\nFigure 3: Demonstration of data variable substitution.\nthe system replaces the token ‘Broadcasting’ with\n< templateLabel[2][0] > which refers to the third\ncolumn’s header label.\nDuring the testing phase, our approach performs\nthe reverse operation where it substitutes data vari-\nables with their referenced tokens. Figure 3 demon-\nstrates how the data variables are substituted with\nactual tokens from the chart data. Here, the corre-\nsponding tokens and variables are highlighted by\nthe same color.\n4.3 Training\nThe model requires two types of labels for its train-\ning step. The ﬁrst type of label is for the chart’s\ndata, which for each record ri in the set of records r\nthere is either a label of 1 if ri is mentioned in the\nsummary s, or a 0 if not. The second type of label\nis for the chart’s summary, where for each token ti\n∈ a gold summary s there is a label of 1 if t is also\npresent in some records in r, or a 0 otherwise.\nThe dataset was divided into training, valida-\ntion, and testing sets with a 70%:15%:15% ratio\nfor training. We trained the model for 80 epochs\nwith an epoch size of 1000, using the following\nhyper-parameters: 1 encoder layer, 6 decoder lay-\ners, embedding size of 512, batch size of 6, beam\nsize of 4, sinusoidal positional embeddings, and\nGELU (Hendrycks and Gimpel, 2016) activations.\n5 Evaluation\nWe ﬁrst perform an automatic evaluation to approx-\nimately measure the performance of our approach\ncompared to the base model (Gong et al., 2019) and\nthen we conduct a human evaluation. Finally, we\nperform some qualitative analysis to have a better\nSummarization Method\nEvaluation Method Our Model Gong et al.\nBLEU Score 18.54 17.06\nContent Selection 55.42 (24.06) 8.49 (9.99)\nTable 3: Results of automatic evaluation for two sum-\nmarization methods (The content section measure is\nshown in percentage and the standard deviation is men-\ntioned in the bracket).\nunderstanding of the effectiveness and trade-offs\nof our approach.\n5.1 Automatic Evaluation\nFor automated evaluation of our summary quality,\nwe employ two different metrics: (1) the BLEU\nscore ( ?), and (2) a content selection metric in-\nspired by (Wiseman et al., 2017). We calculate\nour content selection metric as the percentage of\nrecords mentioned in the generated summary that\nare mentioned in the gold summary.\nTable 3 shows the results of automatic evalua-\ntion of our model compared to the base model. We\nobserve that our model slightly improves over the\nbaseline on the BLEU metric and outperforms it\non content selection by a wide margin. In particu-\nlar, the huge improvement on the content selection\nmetric suggests that by learning how to generate\nsummaries in terms of data variables rather than\nactual data values, our approach largely addresses\nthe hallucination problem.\n5.2 Human Evaluation\nTo further investigate the quality of the generated\ntext, we perform a human evaluation. For this\npurpose, we randomly sample 40 different charts\nwhere we have 10 charts from each of four chart\ntypes (simple bar, complex bar, simple line, com-\nplex line). We use the model from (Gong et al.,\n2019) trained on our dataset with positional embed-\ndings enabled as our baseline. We created a Me-\nchanical Turk study and surveyed three unique re-\nspondents per statistic. We use the survey to assess\nthe quality of each summary from four independent\nperspectives: (1) Informativeness: How informa-\ntive is the summary of the chart? (2) Concise-\nness: How concise is the summary of the chart?,\n(3) Coherence: How coherent is the summary of\nthe chart? and (4) Fluency: How ﬂuent or gram-\nmatically correct are the sentences in the summary\nof the chart? The questions were asked using a\n144\nSummary Method\nEvaluation Category Our Model Gong et al.\nInformativeness 3.42 (1.16) 2.13 (1.44)\nConciseness 3.58 (1.11) 2.40 (1.47)\nCoherence 3.32 (1.25) 2.27 (1.43)\nFluency 3.73 (1.07) 3.78 (0.85)\nTable 4: Results of human evaluation (standard devia-\ntion in brackets)\nSummary Method\nResponse Our Model Gong et al.\nYes 51.02% 22.70%\nNo 26.30% 66.02%\nPartial 15.56% 7.24%\nCan’t decide 7.13% 4.04%\nTable 5: Responses for factual correctness\n5-point Likert scale from 1 (the worst) to 5 (the\nbest).\nAs shown in Table 4, on average our model\noutperforms the base model in terms of informa-\ntiveness, conciseness, and coherence by at least\nover 1 point. Overall, it indicates that our method\ncan generate summaries that are more insightful,\nthat have better connections between sentences and\nwith fewer repetitions. In terms of ﬂuency, the\nbase model performs slightly better (3.78) than our\nmodel (3.73).\nWe also wanted to evaluate the factuality aspect\nof the generated summaries. For this purpose, we\nasked respondents to evaluate whether the facts\nstated in each sentence of the summary were sup-\nported by the chart. There were four possible re-\nsponses to this question: yes, no, partially, and\ncan’t decide (explain why). Table 5 shows the per-\ncentage of statements within the summary that were\nperceived as factually correct or not. We ﬁnd that\nover 50% sentences generated by our model were\nveriﬁed as factually correct, whereas only 22% sen-\ntences from the baseline model’s summaries were\nperceived to be correct. Another 15.56% state-\nments generated from our model were veriﬁed as\npartially correct compared to 7.24% from the base\nmodel. This result suggests that our model gener-\nates more factually correct statements compared to\nthe base model.\nFigure 4: Comparison of summary generation meth-\nods. Here, factually correct statements are highlighted\nin green and incorrect statements are highlighted in red.\n5.3 Case Study\nIn order to better understand how our approach\nworks we reviewed one random sample from our\ndataset (see Figure 4). We notice that the gold sum-\nmary is relatively longer than the two generated\nsummaries with some additional external facts that\nare not mentioned in the chart data (e.g. ‘the New\nEngland Patriots’ and ‘their Twitter following’).\nThis illustrates the challenge for generating chart\nsummaries because without direct connections be-\ntween the chart and its summary, it becomes more\ndifﬁcult for the model to learn what is relevant. Our\nmodel’s summary is similar in structure to the gold,\nbut without mention of these external statistics.\nThe summary is mostly accurate, as it correctly\nmentions the most and least followed teams along\nwith their corresponding fan counts (highlighted in\ngreen), but it incorrectly predicts tokens related to\ngame wins, and also incorrectly mentions the third\nhighest team (highlighted in red) right after it cor-\n145\nFigure 5: Comparison of generated summaries with\npositional embeddings enabled and disabled (Here the\ntext that refers to a bar in the chart is highlighted with\nthe same ﬁll-color of that bar).\nrectly mentioned the lowest team. When analyzing\nthe summary generated from (Gong et al., 2019),\nwe can readily see that it makes more false predic-\ntions than our model. It correctly mentions the ﬁrst\nteam name, but then after that it suffers from the\nhallucination problem as it repeats a memorized\nsummary which it was trained on. As a result, this\nsummary is mostly irrelevant to the given chart.\nThis is also reﬂected from the automatic evalua-\ntion (Table 3) where the model from Gong et al.\nperformed poorly on the content selection metric.\nWe also investigated the impact of positional em-\nbeddings on our model’s performance. As we can\nsee from Figure 5, for a sample chart the model that\nenables positional embeddings yielded summaries\nwith a more meaningful discourse structure by bet-\nter connecting between sentences and with more\nmeaningful salient information. This support that\npositional embeddings allow the model to track se-\nquential data as suggested by (Vaswani et al., 2017)\nfor the original Transformer model.\n5.4 Error Analysis\nWhile our evaluation reveals that our model is more\neffective at chart summary generation than the base\nmodel, it still has much room for improvement.\nIn particular, we identify two categories of com-\nmon errors which are present in our generated sum-\nFigure 6: Comparison of generated summaries by our\nmodel based on data coverage.\nmaries. The most common error is the fact halluci-\nnation, which is demonstrated in Figure 4. While\nour model has largely addressed this issue, irrel-\nevant tokens still occasionally occur. Our error\nanalysis on 50 random samples indicates that hallu-\ncination can occur in two main ways in our model.\nFirst, sometimes the model predicts a data variable\nat an incorrect index, for example it may try to refer\nto one index of a table row, but the correct index is\nactually that of another index. The second halluci-\nnation method is when the model simply predicts a\ntoken which is irrelevant to the data, which we see\ncommonly in other data-to-text works (Wiseman\net al., 2017; Parikh et al., 2020). This error usu-\nally occurs when the model generates summaries\nof charts about a domain that has low coverage\nin the training set. For example, our training set\nhas several charts on ﬁnance and sports. For these\ncharts, the generated summaries are quite ﬂuent\nand tokens that are irrelevant to the corresponding\nchart are rare. Figure 6 shows how the generated\nsummaries differ in ﬂuency and amount of irrel-\nevant tokens for two different charts: one from a\ndomain with low training data coverage and the\nother with high training data coverage.\n146\n6 Conclusion and Future Work\nIn this paper, we tackle the challenge of automatic\nchart summarization by introducing a new dataset\nand proposing a neural approach based on the trans-\nformer architecture. Our approach learns how to\ngenerate natural language descriptions from charts\nby modifying the gold summaries to replace refer-\nences to chart data values with data variables. As\na result, the model learns how to summarize in a\nmore generalized way, and generates more factu-\nally correct statements compared to the base model.\nOur model also generates summaries that are more\ninformative, concise, and coherent according to\nhuman evaluations. We hope that our work will\nmotivate researchers to further improve the quality\nof summaries automatically generated from charts,\nwhich is a highly under-explored research area.\nIn the future, we would like to develop larger\ndatasets that cover more diverse domains and ad-\nditional chart types (e.g. pie charts, scatterplots,\nheatmaps etc.) to further improve the quality and\ngeneralizability of our model. Also, while we\ncompared our method with a strong baseline from\n(Gong et al., 2019), we will perform further com-\nparisons with other models which were developed\nfor the data-to-text generation problem. Finally, we\nwould like to build applications such as an interac-\ntive chart summarization system with a focus on en-\nhancing the accessibility of charts so that blind and\nvisually impaired people can comprehend charts\nvia audio.\nAcknowledgments\nThis work was supported by the Natural Sci-\nences and Engineering Research Council (NSERC),\nCanada. We thank Dhruv Nayyar for helping with\npreparing the dataset.\nReferences\nRegina Barzilay and Mirella Lapata. 2005. Collective\ncontent selection for concept-to-text generation. In\nProceedings of Human Language Technology Con-\nference and Conference on Empirical Methods in\nNatural Language Processing, pages 331–338, Van-\ncouver, British Columbia, Canada. Association for\nComputational Linguistics.\nSandra Carberry, Stephanie Elzer, and Seniz Demir.\n2006. Information graphics: an untapped resource\nfor digital libraries. In Proceedings of the 29th an-\nnual international ACM SIGIR conference on Re-\nsearch and development in information retrieval ,\npages 581–588.\nGiuseppe Carenini, Cristina Conati, Enamul Hoque,\nand Ben Steichen. 2013. User task adaptation in\nmultimedia presentations. In Proceedings of the 1st\nInternational Workshop on User-Adaptive Informa-\ntion Visualization in conjunction with the 21st con-\nference on User Modeling, Adaptation and Person-\nalization (UMAP).\nCharles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul\nKim, Scott Cohen, Tong Yu, Ryan A. Rossi, and\nRazvan C. Bunescu. 2019. Figure captioning\nwith reasoning and sequence-level training. CoRR,\nabs/1906.02850.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural lan-\nguage generation from open-domain tables. arXiv\npreprint arXiv:2004.10404.\nZhe Cui, Sriram Karthik Badam, M Adil Yalc ¸in, and\nNiklas Elmqvist. 2019. Datasite: Proactive vi-\nsual data exploration with computation of insight-\nbased recommendations. Information Visualization,\n18(2):251–267.\nSeniz Demir, Sandra Carberry, and Kathleen F. McCoy.\n2012. Summarizing information graphics textually.\nComputational Linguistics, 38(3):527–574.\nMassimo Fasciano and Guy Lapalme. 1996. Post-\ngraphe: a system for the generation of statistical\ngraphics and text. In Eighth International Natural\nLanguage Generation Workshop.\nMassimo Fasciano and Guy Lapalme. 2000. Intentions\nin the coordinated generation of graphics and text\nfrom tabular data. Knowledge and Information Sys-\ntems, 2:310–339.\nLeo Ferres, Gitte Lindgaard, Livia Sumegi, and Bruce\nTsuji. 2013. Evaluating a tool for improving acces-\nsibility to charts and graphs. ACM Trans. Comput.-\nHum. Interact., 20(5).\nLi Gong, Josep M Crego, and Jean Senellart. 2019.\nEnhanced transformer model for data-to-text genera-\ntion. In Proceedings of the 3rd Workshop on Neural\nGeneration and Translation, pages 148–156.\nNancy L Green, Giuseppe Carenini, Stephan Kerped-\njiev, Joe Mattis, Johanna D Moore, and Steven F\nRoth. 2004. Autobrief: an experimental system for\nthe automatic generation of brieﬁngs in integrated\ntext and information graphics. International Jour-\nnal of Human-Computer Studies, 61(1):32–70.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n147\nDae Hyun Kim, Enamul Hoque, and Maneesh\nAgrawala. 2020. Answering questions about charts\nand generating visual explanations. In Proceedings\nof the 2020 CHI Conference on Human Factors in\nComputing Systems, CHI 20, page 1–13.\nR´emi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with\napplication to the biography domain. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 1203–1213,\nAustin, Texas. Association for Computational Lin-\nguistics.\nPercy Liang, Michael I Jordan, and Dan Klein. 2009.\nLearning semantic correspondences with less super-\nvision. In Proceedings of the Joint Conference of\nthe 47th Annual Meeting of the ACL and the 4th In-\nternational Joint Conference on Natural Language\nProcessing of the AFNLP, pages 91–99.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Association for Compu-\ntational Linguistics (ACL) System Demonstrations ,\npages 55–60.\nHongyuan Mei, TTI UChicago, Mohit Bansal, and\nMatthew R Walter. 2016. What to talk about and\nhow? selective generation using lstms with coarse-\nto-ﬁne alignment. In Proceedings of NAACL-HLT,\npages 720–730.\nVibhu O. Mittal, Johanna D. Moore, Giuseppe\nCarenini, and Steven Roth. 1998. Describing com-\nplex charts in natural language: A caption genera-\ntion system. Computational Linguistics, 24(3):431–\n467.\nAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann,\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang,\nand Dipanjan Das. 2020. Totto: A controlled\ntable-to-text generation dataset. arXiv preprint\narXiv:2004.14373.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-text generation with content selection and\nplanning. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, pages 6908–6915.\nQuill. 2020. Narrative science.\nEhud Reiter. 2007. An architecture for data-to-text\nsystems. In Proceedings of the Eleventh European\nWorkshop on Natural Language Generation , pages\n97–104. Association for Computational Linguistics.\nEhud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,\nand Ian Davy. 2005. Choosing words in computer-\ngenerated weather forecasts. Artiﬁcial Intelligence,\n167(1-2):137–169.\nArjun Srinivasan, Steven M Drucker, Alex Endert, and\nJohn Stasko. 2018. Augmenting visualizations with\ninteractive data facts to facilitate interpretation and\ncommunication. IEEE transactions on visualization\nand computer graphics, 25(1):672–681.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYun Wang, Zhida Sun, Haidong Zhang, Weiwei Cui,\nKe Xu, Xiaojuan Ma, and Dongmei Zhang. 2019.\nDatashot: Automatic generation of fact sheets from\ntabular data. IEEE transactions on visualization and\ncomputer graphics, 26(1):895–905.\nSam Wiseman, Stuart M Shieber, and Alexander M\nRush. 2017. Challenges in data-to-document gen-\neration. arXiv preprint arXiv:1707.08052.\nWordsmith. 2020. Wordsmith by automated insights,\ninc.\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\nLing. 2017. Reference-aware language models. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1850–1859.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8463008403778076
    },
    {
      "name": "Bar chart",
      "score": 0.7876760959625244
    },
    {
      "name": "Transformer",
      "score": 0.6858460903167725
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.680091917514801
    },
    {
      "name": "Natural language processing",
      "score": 0.6024028658866882
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5997204780578613
    },
    {
      "name": "Chart",
      "score": 0.5703046917915344
    },
    {
      "name": "Natural language",
      "score": 0.5097472071647644
    },
    {
      "name": "Language model",
      "score": 0.49312564730644226
    },
    {
      "name": "Natural language understanding",
      "score": 0.4396970570087433
    },
    {
      "name": "Encoder",
      "score": 0.4358888566493988
    },
    {
      "name": "Metric (unit)",
      "score": 0.42332905530929565
    },
    {
      "name": "Natural language generation",
      "score": 0.41230738162994385
    },
    {
      "name": "Architecture",
      "score": 0.4105827212333679
    },
    {
      "name": "Information retrieval",
      "score": 0.382398396730423
    },
    {
      "name": "Machine learning",
      "score": 0.3394845426082611
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I192455969",
      "name": "York University",
      "country": "CA"
    }
  ]
}