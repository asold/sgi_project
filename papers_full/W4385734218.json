{
  "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
  "url": "https://openalex.org/W4385734218",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3121372713",
      "name": "Jinheon Baek",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5111961649",
      "name": "Alham Aji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097703981",
      "name": "Amir Saffari",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2251079237",
    "https://openalex.org/W2963546833",
    "https://openalex.org/W3034862985",
    "https://openalex.org/W3204877056",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4300506197",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3166938950",
    "https://openalex.org/W2891420675",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W2511149293",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3186545525",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4287889327",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3208821253",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4285151662",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3199020415",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3188542058",
    "https://openalex.org/W3154670582",
    "https://openalex.org/W3104748221",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4224247158",
    "https://openalex.org/W3104616515",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W2613678836",
    "https://openalex.org/W2890961898",
    "https://openalex.org/W4302306557",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3156470785"
  ],
  "abstract": "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
  "full_text": "Proceedings of the First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023), pages 70–98\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nKnowledge-Augmented Language Model Prompting\nfor Zero-Shot Knowledge Graph Question Answering\nJinheon Baek1∗ Alham Fikri Aji2 Amir Saffari3\nKAIST1 MBZUAI2 Amazon3\njinheon.baek@kaist.ac.kr alham.fikri@mbzuai.ac.ae amsafari@amazon.com\nAbstract\nLarge Language Models (LLMs) are capable\nof performing zero-shot closed-book question\nanswering tasks, based on their internal knowl-\nedge stored in parameters during pre-training.\nHowever, such internalized knowledge might\nbe insufficient and incorrect, which could lead\nLLMs to generate factually wrong answers.\nFurthermore, fine-tuning LLMs to update their\nknowledge is expensive. To this end, we pro-\npose to augment the knowledge directly in the\ninput of LLMs. Specifically, we first retrieve\nthe relevant facts to the input question from\nthe knowledge graph based on semantic simi-\nlarities between the question and its associated\nfacts. After that, we prepend the retrieved facts\nto the input question in the form of the prompt,\nwhich is then forwarded to LLMs to gener-\nate the answer. Our framework, Knowledge-\nAugmented language model PromptING (KAP-\nING), requires no model training, thus com-\npletely zero-shot. We validate the performance\nof our KAPING framework on the knowledge\ngraph question answering task, that aims to an-\nswer the user’s question based on facts over a\nknowledge graph, on which ours outperforms\nrelevant zero-shot baselines by up to 48% in\naverage, across multiple LLMs of various sizes.\n1 Introduction\nPre-trained Language Models (LMs) (Devlin et al.,\n2019; Raffel et al., 2020), which are trained on a\nlarge amount of text corpora with self-supervised\nlearning, can perform closed-book Question An-\nswering (QA) tasks that aim to answer the user’s\nquestion based only on their internal knowledge\nin parameters, without using any external knowl-\nedge (Petroni et al., 2019; Roberts et al., 2020).\nAlso, when we increase the LM sizes, Large Lan-\nguage Models (LLMs) can generate the answer\nfor the question without any additional fine-tuning\n∗ Work done while interning at Amazon. Corresponding\nauthor: Jinheon Baek (jinheon.baek@kaist.ac.kr)\n[Prompt]Question: Which member of Black Eyed Peas appeared in Poseidon?Answer:\n(a) Language ModelPrompting w/o Knowledge Augmentation\n[Generated Answer]Tariq Ali\n[Prompt]Below are the facts that might be relevant to answer the question:(Black Eyed Peas, has part, Fergie), (Black Eyed Peas, has part, Kim Hill),(Poseidon, cast member, Fergie)Question: Which member of Black Eyed Peasappeared in Poseidon?Answer:\n(b) Knowledge-Augmented Language Model Prompting\n[Generated Answer]Fergie\nKnowledge GraphBlack Eyed PeasFergieHas_part\nMusical GroupInstance of\nPoseidonCast_member\nKim HillHas_part\nRetrieval\nFigure 1: (a) For the input question in the prompt, the large\nlanguage model, GPT-3 (Brown et al., 2020), can generate\nthe answer based on its internal knowledge in parameters,\nbut hallucinates it which is highlighted in yellow. (b) Our\nKnowledge-Augmented language model PrompTING (KAP-\nING) framework first retrieves the relevant facts in the knowl-\nedge graph from the entities in the question, and then augments\nthem to the prompt, to generate the factually correct answer.\nsteps, called LM prompting (Brown et al., 2020;\nLiu et al., 2021). However, since the knowledge\nin LLMs might be incomplete, incorrect, and out-\ndated, they often generate factually wrong answers,\nknown as hallucination (Rohrbach et al., 2018)\n(See Figure 1a). Also, refining the knowledge in\nLLMs with parameter updates is costly, especially\nwhen knowledge is constantly changing (e.g., ex-\nchange rates of money). Lastly, whether LLMs are\nfetching the correct knowledge for QA is unclear.\nTo overcome those limitations, we propose to re-\ntrieve and inject the relevant knowledge directly as\nan input, called a prompt, to LLMs (Figure 1b). As\na knowledge source, we use a Knowledge Graph\n(KG) consisting of symbolic knowledge in the form\nof a triple: (head entity, relation, tail entity). There-\nfore, to extract the relevant facts to the input ques-\ntion, we first match entities in the question with\nentities in the KG. After that, triples associated to\n70\nentities in the KG are verbalized (i.e., transforming\nthe symbolic relational knowledge to the textual\nstring) and prepended to the input question, which\nare then forwarded to LLMs to generate the answer.\nConsequently, LLMs conditioned on the factual\nknowledge are able to generate the factual answers,\nalleviating the hallucination issue, while keeping\nLLMs’ parameters unchanged: fine-tuning is not\nrequired for knowledge updates. We refer to our\noverall framework as Knowledge-Augmented lan-\nguage model PromptING (KAPING), which is\ncompletely zero-shot and can be done with any\noff-the-shelf LLMs, without additional training.\nWhile the above scheme looks simple yet effec-\ntive, there is a couple of challenges. First, most\nretrieved triples associated with the question enti-\nties are unrelated to answer the given question. For\nexample, when we retrieve the associated triples for\nthe question entity (e.g., Poseidon) in Figure 1 in\nthe Wikidata KG (Vrandecic and Krötzsch, 2014),\nthere exist 60 triples, and most of them (e.g., genre,\npublication date, to name a few) are irrelevant to\nanswer the question. Therefore, they might mis-\nlead the model into generating incorrect answers.\nOn the other hand, the number of triples for the\nquestion entities is occasionally large (e.g., 27%\nsamples for the WebQSP dataset (Yih et al., 2016)\nhave more than 1,000 triples), thereby encoding\nall triples including unnecessary ones yields high\ncomputational costs, especially on LLMs.\nTo overcome such challenges, we further pro-\npose to filter out unnecessary triples based on their\nsemantic similarities to the input question, inspired\nby the information retrieval (Bast et al., 2016). To\nbe specific, we first represent the question and\nits associated verbalized triples in the embedding\nspace. Then, we retrieve the small number of triples\nwhose embeddings are more close to the input ques-\ntion’s embedding than others. By doing so, we can\nprepend only the more relevant triples to the given\nquestion, which can effectively prevent LLMs from\ngenerating irrelevant answers with high computa-\ntional efficiencies, unlike the one that augments all\ntriples. Note that, our filtering approach uses off-\nthe-shelf sentence embedding models (Song et al.,\n2020; Hofstätter et al., 2021); thus no additional\ntraining is required in every part of our pipeline.\nWe then validate our KAPING framework on\nKnowledge Graph Question Answering (KGQA)\ntasks. The results show that our KAPING signif-\nicantly outperforms relevant zero-shot baselines.\nAlso, the detailed analyses support the importance\nof knowledge retrieval and augmentation schemes.\nOur contributions in this work are threefold:\n• We present a new knowledge-augmented LM\nprompting framework that leverages the fac-\ntual knowledge from KGs, for zero-shot QA.\n• We propose to retrieve and augment relevant\nfacts from KGs, based on semantic similarities\nbetween the question and its associated triples.\n• We validate our KAPING on KGQA bench-\nmark datasets, on which ours impressively\noutperforms relevant zero-shot baselines.\n2 Related Work\nLanguage Model Prompting Language model\npre-training, which trains Transformers (Vaswani\net al., 2017) on unannotated text corpora with auto-\nencoding (Devlin et al., 2019; Liu et al., 2019) or\nauto-regressive (Yang et al., 2019; Radford et al.,\n2018) objectives, becomes an essential approach\nfor natural language tasks. Also, Large Language\nModels (LLMs) (Brown et al., 2020; Raffel et al.,\n2020; Chowdhery et al., 2022; Soltan et al., 2022)\nare able to perform zero-shot learning, for example,\ngenerating the answer for the input textual prompt,\nbased on the knowledge stored in pre-trained pa-\nrameters (Petroni et al., 2019; Roberts et al., 2020;\nSung et al., 2021), without additional parameter\nupdates as well as labeled datasets. To further im-\nprove their performances, some work (Rubin et al.,\n2022; Liu et al., 2022a) proposes retrieving rele-\nvant samples to the input question from the training\ndataset and prepending them in the prompt under\nfew-show learning. Recent few work (Sanh et al.,\n2022; Wei et al., 2022a) further shows that, when\nLLMs are fine-tuned on a collection of instruc-\ntions phrased from natural language tasks, they can\nhave strong generalization performance on unseen\nzero-shot tasks. However, the knowledge inside\nLMs might be insufficient to tackle factual ques-\ntions, which gives rise to knowledge-augmented\nLMs. Notably, our LM prompting is different from\nprompt-tuning literature (Lester et al., 2021a; Chen\net al., 2022a) that additionally tunes LMs with\nmodel training (See Appendix C for discussions).\nKnowledge-Augmented LMs Recent work pro-\nposes to integrate the knowledge, such as docu-\nments from unstructured corpora (e.g., Wikipedia)\nand facts from Knowledge Graphs (KGs), into LMs.\nTo mention a few, REALM (Guu et al., 2020) and\n71\nRAG (Lewis et al., 2020) learn to retrieve docu-\nments and augment LMs with them. In addition,\nKGs could be another knowledge source, where\nthe knowledge is succinctly encoded in the most\ncompact form, and some methods augment such\nfacts in KGs into LMs (Galetzka et al., 2021; Rony\net al., 2022; Kang et al., 2022). However, all afore-\nmentioned approaches require massive amount of\ntraining data and model updates for downstream\ntasks. While more recent work (Izacard et al., 2022)\nshows retrieval-augmented LM can have strong per-\nformance with few-shot learning, it still requires\nextra training steps, which is different from ours\nfocusing on LM prompting for entirely zero-shot.\nRecently, there are few studies augmenting the\nknowledge in the LM prompting scheme. At first,\nsome work proposes to extract the knowledge in\nthe parameters of LLMs themselves via prompting,\nand then use the extracted knowledge to answer\nthe question (Kojima et al., 2022; Liu et al., 2022b;\nWei et al., 2022b; Wang et al., 2022). However,\nsince LLMs’ parameters might be insufficient to\nstore all the world knowledge, the extracted knowl-\nedge and generated answers might be inaccurate.\nOn the other hand, most recently, Lazaridou et al.\n(2022) propose to use the Google Search to retrieve\ndocuments on the Web, and then prepend the re-\ntrieved documents to the input question along with\nfew-shot demonstrations, to answer the question\nunder few-shot LLM prompting schemes. How-\never, our focus on zero-shot prompting with KGs is\northogonal to the previous study working on doc-\numents with few-shot prompting, and leveraging\nKGs can bring additional advantages. Specifically,\nsince KGs can succinctly encode the knowledge in\nthe compact triple form, for QA tasks, ours makes\nLLM prompting more efficient (i.e., reducing the\ninput sequence length compared to the document\ncase), as well as more effective on the zero-shot QA\nscheme: LLMs need to select one triple containing\nthe answer entity in the prompt, instead of looking\nthrough lengthy documents having various entities.\nKnowledge Graph Question Answering The\ngoal of our target Knowledge Graph Question An-\nswering (KGQA) tasks is to answer the input ques-\ntion based on a set of facts over KGs (Chakraborty\net al., 2019; Fu et al., 2020). Previous approaches\nare broadly classified into neural semantic parsing-\nbased methods (Yih et al., 2015; Bao et al., 2016;\nLuo et al., 2018), information retrieval-based meth-\nods (Sun et al., 2018; Saxena et al., 2020; Yasunaga\net al., 2021), and differentiable KG-based meth-\nods (Cohen et al., 2020; Saffari et al., 2021; Sen\net al., 2021), which, however, require annotated\ndata with additional model training. While Zhou\net al. (2021) aim to transfer the KGQA model to the\ntarget language domains without any training data\non them, this work indeed needs the labeled data\nto train the model on data-rich source domains first\nbefore transferring the model to the target domains.\nIn contrast to all the aforementioned methods, we\nexplore the novel zero-shot KGQA mechanism,\nwhich does not require any annotated QA pairs and\nadditional training, leveraging LM prompting.\n3 Method\nWe now describe our Knowledge-Augmented lan-\nguage model PromptING (KAPING) framework.\n3.1 LM Prompting for Zero-Shot QA\nWe begin with the zero-shot question answering,\nand then explain the language model prompting.\nZero-Shot Question Answering Given an input\nquestion x, the Question Answering (QA) system\nreturns an answer y, where x and y consist of se-\nquences of tokens: x = [w1, w2, . . . , w|x|]. Let P\nbe a QA model based on the generative Language\nModel (LM) (Raffel et al., 2020; Brown et al.,\n2020), which generates the conditional probability\nof answer y for question x as follows: P(y|x).\nThen, in contrast to supervised learning that trains\nmodel P with a set of annotated (x, y) pairs, zero-\nshot learning does not use any labeled samples and\nmodel training. Notably, we are interested in this\nzero-shot QA, since collecting the dataset and then\nfine-tuning the existing LMs for every new domain\nare known to be expensive and sometimes infeasi-\nble (Houlsby et al., 2019; Lester et al., 2021b).\nLM Prompting LMs are often pre-trained by\npredicting the next token based on previous tokens,\nwhich is known as auto-regressive language mod-\neling (Radford et al., 2018; Raffel et al., 2020).\nThen, thanks to this pre-training objective, LLMs\ncan perform zero-shot instruction learning. Specif-\nically, when we provide a question as well as an\ninstruction (e.g., \"Please answer the question: Who\nis the author of Lady Susan?\") to the LLM (i.e.,\nP), such the LLM, conditioned by the input text,\ncan sequentially generate the probability of output\ntokens, which might be an answer, \"Jane Austen\".\nTo be more formal, for every input question x,\nwe first modify it with a particular instruction tem-\n72\nplate T into a textual string x′called a prompt, as\nfollows: T : x ↦→x′. For example, if we have the\nprevious question x = \"Who is the author of Lady\nSusan?\" along with the previous instruction tem-\nplate \"Please answer the question:\", the resulting\nprompt x′would be T(x) =\"Please answer the\nquestion: Who is the author of Lady Susan?\". Then,\nwe forward the prompt x′ to the LLM (i.e., P),\nwhich then generates the answer (i.e., y) through\nP(y|x′). Note that this LM prompting scheme\ndoes not require any additional model parameter\nupdates (i.e., fine-tuning) on the labeled data, thus\nappropriate for the target zero-shot QA task.\nHowever, there are multiple challenges in this\nnaive zero-shot prompting for QA. First, LLMs,\nwhich rely on the knowledge in parameters, are\nvulnerable from generating the factually incorrect\nanswer, since the knowledge in LLMs might be in-\naccurate, and outdated: knowledge can be emerged\nand changed over time. Also, refining the internal-\nized knowledge with additional parameter updates\nis expensive, while it is necessary to reflect the\nwrong and ever growing knowledge. Lastly, which\nknowledge LLMs memorize and utilize when gen-\nerating the answer to the question prompt is unclear,\nwhich limits their explainability on the outputs.\n3.2 Knowledge-Augmented LM Prompting\nIn order to tackle the aforementioned limitations\nof the existing LM prompting scheme, we propose\nto inject the relevant knowledge to the input ques-\ntion from the Knowledge Graph (KG), which we\nrefer to as Knowledge-Augmented language model\nPromptING (KAPING). In this subsection, we first\ndefine the main objective of our KAPING frame-\nwork, and then introduce the ingredients for aug-\nmenting the knowledge over KGs to LM prompts.\nLM Prompting with Knowledge Graphs In-\nstead of relying on the knowledge internalized in\nparameters, we propose to additionally access and\ninject the knowledge from the external KG, which\ncontains accurate and up-to-date facts helpful to an-\nswer the question. Formally, a knowledge graph G\nconsists of a set of factual triples {(s, r, o)}, where\ns and o denote subject and object entities, and r\nis a specific type of a relation between them. For\nexample, one relational knowledge \"Lady Susan\nwas written by Jane Austen\" can be represented as\na triple consisting of two entities s = \"Lady Su-\nsan\" and o = \"Jane Austen\" along with a relation\nr = \"written by\". Then, for the question prompt x′\ntransformed from the example question x = \"Who\nis the author of Lady Susan?\" via the template T,\nwe additionally augment its relevant triple: (Lady\nSusan, written by, Jane Austen), to the LM prompt-\ning scheme. By doing so, LLMs can generate the\ncorrect answer with regard to the augmented knowl-\nedge from KGs, formalized as follows: P(y|x′, G).\nNote that, since we can provide specific and valid\nfacts in KGs to LLMs whenever they exist, our\nframework can alleviate hallucination issue, origi-\nnated from inaccurate and outdated knowledge in\nLLMs, without costly updating their model param-\neters. Furthermore, we can confirm whether LLMs\ngenerate answers based on augmented facts, thus\nimproving the explainability of LM prompting.\nThe remaining questions are then how to access\nthe relational symbolic facts over the KG from\nthe input question, verbalize the symbolic knowl-\nedge to the textual string, and inject the verbalized\nknowledge into the LM prompting scheme. We ex-\nplain them one by one in the following paragraphs.\nKnowledge Access In order to utilize the related\nfacts to the input question, we first extract the enti-\nties in the question. For example, for the question\n\"Who is the author of Lady Susan?\", we extract the\nentity \"Lady Susan\". Then, based on the extracted\nentity, we find its corresponding entity over the KG,\nwhose incident triples then become associated facts\nto the input question. Note that entity matching can\nbe done by existing entity linking techniques (Wu\net al., 2020; Li et al., 2020; Ayoola et al., 2022).\nKnowledge Verbalization LLMs are working\non textual inputs, whereas factual triples are repre-\nsented over the symbolic graph. Therefore, before\ninjecting the symbolic fact from KGs to LLMs,\nwe first transform the triple consisting of (s, r, o)\ninto its textual string, called verbalization. While\nthere exists recent methods (Oguz et al., 2022; Ma\net al., 2022) that particularly design or even learn\nthe graph-to-text transformation, in this work, we\nuse the linear verbalization: concatenating the sub-\nject, relation, and object texts in the triple, which\nwe observe works well in LM prompting (See Ap-\npendix B.5). For instance, one triple (Lady Susan,\nwritten by, Jane Austen) is used as is: \"(Lady Susan,\nwritten by, Jane Austen)\", for an LLM’s input.\nKnowledge Injection Based on verbalized facts\nassociated with the input question, the remaining\nstep is to realize the knowledge injection mecha-\nnism, which allows LLMs to be grounded on the\n73\nexternal knowledge, useful to generate the answer.\nLet assume we have a set of N associated triples\nk = {(si, ri, oi)}N\ni=1 for question x. Then, simi-\nlar to instruction template T : x ↦→x′described\nin Section 3.1, we modify N verbalized triples k\nalong with the instruction for the knowledge in-\njection into the knowledge prompt k′, as follows:\nT : k ↦→k′. One particular template we use for\nconstructing the prompt is that, we first enumer-\nate N verbalized triples line-by-line and then add\nthe specific instruction: \"Below are facts in the\nform of the triple meaningful to answer the ques-\ntion.\", at the top of the prompt. After that, such\nthe knowledge prompt string, k′, is prepended to\nthe question prompt x′, and LLMs conditioned by\nknowledge and question prompts then sequentially\ngenerate the answer tokens, formalized as follows:\nP(y|[k′, x′]), where [·] denotes concatenation.\n3.3 Question-Relevant Knowledge Retrieval\nThe proposed KAPING framework in Section 3.2,\nallows LLMs to leverage the knowledge from KGs\nfor zero-shot QA. However, there are critical chal-\nlenges that the number of triples associated to ques-\ntions is often too large to forward in LLMs. Also,\nmost of them are unrelated to the question, mislead-\ning LLMs into generating the irrelevant answer.\nKnowledge Retriever To overcome those limita-\ntions, we further propose to retrieve and augment\nonly the relevant triples to the question. Note that\nthere exists a document-retrieval scheme (Lin et al.,\n2021), whose goal is to retrieve relevant documents\nfor the given query based on their embedding simi-\nlarities, which motivates us to retrieve, in our case,\nthe triples for the user’s question. In particular,\nthanks to the verbalizer defined in Section 3.2, we\ncan play with triples, obtained from symbolic KGs,\nover the text space. Therefore, for the verbalized\ntriple and the question, we first embed them onto\nthe representation space with off-the-shelf sentence\nembedding models for text retrieval (Song et al.,\n2020; Karpukhin et al., 2020; Xiong et al., 2021),\nand then calculate their similarities. After that, we\nuse only the top-K similar triples, instead of using\nall N triples, associated to the given question. Note\nthat, unlike few recent studies (Oguz et al., 2022;\nMa et al., 2022; Kang et al., 2022) that aim at im-\nproving KG retrievers themselves under supervised\ntraining, we focus on zero-shot LM prompting with\nKGs, thus we use any off-the-shelf retrievers as a\ntool to filter out unnecessary triples for questions.\n4 Experimental Setups\nWe explain datasets, models, metrics, and imple-\nmentations. For additional details, see Appendix A.\n4.1 Datasets\nWe evaluate our Knowledge-Augmented language\nmodel PromptING (KAPING) framework on two\nKnowledge Graph Question Answering (KGQA)\ndatasets, namely WebQuestionsSP and Mintaka.\nWebQuestionsSP (WebQSP) This dataset (Be-\nrant et al., 2013; Yih et al., 2016) is designed with a\nFreebase KG (Bollacker et al., 2008). It consists of\n1,639 test samples, which we use for zero-shot eval-\nuation. Additionally, since Freebase is outdated,\nwe further use the Wikidata KG (Vrandecic and\nKrötzsch, 2014) by using available mappings from\nFreebase ids to Wikidata (Diefenbach et al., 2017).\nThis additional dataset consists of 1,466 samples.\nMintaka This dataset (Sen et al., 2022) is re-\ncently designed with the Wikidata KG for complex\nKGQA tasks. Among 8 different languages, we\nuse English test sets consisting of 4,000 samples.\n4.2 Large Language Models\nTo verify the performance of our KAPING frame-\nwork on Large Language Models (LLMs), as well\nas benchmarking them on zero-shot KGQA, we\nuse various LLMs with different sizes. Specifically,\nwe use T5 (Raffel et al., 2020) (0.8B, 3B, 11B),\nT0 (Sanh et al., 2022) (3B, 11B), OPT (Zhang et al.,\n2022) (2.7B, 6.7B) and GPT-3 (Brown et al., 2020)\n(6.7B, 175B). We provide details in Appendix A.2.\n4.3 Baselines and Our Model\nIn this subsection, we explain four zero-shot LM\nprompting baselines and our KAPING framework.\nNo Knowledge This is a naive LM prompting\nbaseline, which generates answers from input ques-\ntions without knowledge augmentation from KGs.\nRandom Knowledge This is an LM prompt-\ning baseline, which additionally augments the ran-\ndomly sampled K triples, associated to the entities\nappeared in the question, to the prompt.\nPopular Knowledge This is an LM prompting\nbaseline, which augments K popular triples among\nall triples from the question entities, based on rela-\ntions that appear the most frequently in the KG.\nGenerated Knowledge This is an LM prompting\nbaseline, which first extracts the knowledge from\nLLMs themselves based on prompting, and then\n74\nTable 1: Main results of language model prompting , where we report the generation accuracy. The number inside the\nparentheses in the first row denotes the parameter size of language models, and best scores are emphasized in bold.\nDatasets Methods T5 (0.8B)T5(3B) T5(11B) OPT(2.7B)OPT(6.7B)OPT(13B) T0(3B) T0(11B) GPT-3(6.7B)GPT-3(175B)AlexaTM(20B) Average\nWebQSPw/ Freebase\nNo Knowledge 6.95 13.40 9.48 19.85 29.77 28.38 21.43 40.77 44.63 63.59 46.79 29.55Random Knowledge 21.55 19.15 17.57 28.07 31.73 33.31 32.62 51.20 51.01 65.87 57.37 37.22Popular Knowledge 15.30 16.88 18.39 28.32 28.13 24.21 27.05 47.22 45.58 62.26 54.91 33.48Generated Knowledge 6.19 7.84 6.76 7.46 11.50 8.22 19.41 38.81 45.89 62.14 35.13 22.67\nKAPING (Ours) 34.70 25.41 24.91 41.09 43.93 40.20 52.28 62.85 60.37 73.89 67.67 47.94\nWebQSPw/ Wikidata\nNo Knowledge 10.30 18.42 15.21 23.94 33.77 32.40 24.56 44.20 48.50 67.60 42.41 32.85Random Knowledge 17.94 22.78 24.28 37.24 35.61 38.27 28.85 47.68 52.05 60.64 55.63 38.27Popular Knowledge 15.35 20.80 20.74 30.83 30.01 27.83 24.83 48.02 47.41 63.37 53.92 34.83Generated Knowledge 11.94 13.30 12.28 11.26 17.53 14.19 22.92 41.34 48.77 65.89 31.16 26.42\nKAPING (Ours) 23.67 40.38 35.47 49.52 53.34 51.57 49.86 58.73 60.44 69.58 65.04 50.69\nMintakaw/ Wikidata\nNo Knowledge 11.23 14.25 17.06 19.76 27.19 26.83 14.75 23.74 34.65 56.33 41.97 26.16Random Knowledge 17.59 18.19 18.83 28.11 26.58 28.36 16.10 26.15 32.98 51.56 46.02 28.22Popular Knowledge 17.56 18.09 18.73 26.97 27.08 23.10 16.74 27.15 32.48 53.16 46.41 27.95Generated Knowledge 13.61 14.61 14.29 11.87 14.96 16.24 14.46 23.13 33.12 55.65 34.58 22.41\nKAPING (Ours) 19.72 22.00 22.85 32.94 32.37 33.37 20.68 29.50 35.61 56.86 49.08 32.27\n1-Hop Retrieval 2-Hop Retrieval\nDatasets Retrievers MRR Top-1 Top-10 Top-30 MRR Top-1 Top-10 Top-30\nWebQSPw/ Freebase\nRandom 12.50 7.21 25.09 34.64 1.50 0.70 2.65 5.37Popular 8.58 5.31 15.93 24.53 1.59 0.95 2.72 4.68MPNet 47.27 40.27 60.56 64.48 41.64 33.12 58.47 65.23\nWebQSPw/ Wikidata\nRandom 9.50 3.62 22.58 40.72 1.31 0.00 2.80 8.59Popular 8.52 4.57 15.89 35.47 4.63 4.02 5.53 6.62MPNet 43.46 33.36 64.39 70.67 40.42 30.56 62.62 71.56\nMintakaw/ Wikidata\nRandom 4.80 1.85 11.48 22.03 0.91 0.14 1.78 5.15Popular 6.09 3.09 12.51 20.47 0.24 0.04 0.28 1.24MPNet 13.01 7.50 25.44 35.43 13.00 6.82 26.65 40.01\nTable 2: Retriever results. We compare random model, popular\nmodel, and MPNet (Song et al., 2020), on 1- and 2-hop retrievals.\nOPT (6.7B) T0 (11B)\n20\n30\n40\n50\n60\n70Generation Accuracy\nWebQSP w/ Freebase\nRetrieval\nNo Knowledge\nRandom Knowledge\nKAPING (Ours)\nOPT (6.7B) T0 (11B)\n20\n30\n40\n50\n60\n70\nWebQSP w/ Wikidata\nOPT (6.7B) T0 (11B)\n0\n8\n16\n24\n32\n40\nMintaka w/ Wikidata\nFigure 2: Comparisons of retrieval and LM prompting. Re-\ntrieval is the Top-1 result of the MPNet (Song et al., 2020).\naugments them as the form of the prompt (Liu et al.,\n2022b), which is similar to Kojima et al. (2022).\nKAPING (Ours) This is our Knowledge Aug-\nmented language model PromptING (KAPING)\nframework, which first retrieves the top-K similar\ntriples to the question with the knowledge retriever,\nand then augments them as the form of the prompt.\n4.4 Evaluation Metrics\nGeneration Following the evaluation protocol\nof generative KGQA (Yin et al., 2016; Sen et al.,\n2022; Mavi et al., 2022), we use accuracy, which\nmeasures whether the generated tokens from the\ngiven prompt include one of the answer entities.\nNote that we further consider aliases – a set of\nalternative names – of answer entities available in\nFreebase and Wikidata KGs, for evaluation.\nRetrieval We also measure the retriever perfor-\nmance, to see how much the retrieved triples are\nhelpful for answer generation. As metrics, we use\nMean Reciprocal Rank (MRR) and Top-K accuracy\n(Top-K), which are calculated by ranks of correctly\nretrieved triples containing answer entities among\nall triples associated to question entities.\n4.5 Implementation Details\nFor the knowledge injection, we set the number of\nretrieved facts as 10 ( K = 10), and the hop for\ntriple retrieval as one. For the text-based retriever,\nwe experiment with MPNet (Song et al., 2020) that\nuses the same encoder for embedding question and\ntriples. See Appendix A.4 for additional details.\n5 Experimental Results and Analyses\nWe provide the overall results of our KAPING\nframework along with its comprehensive analyses.\nMain Results As shown in Table 1, our KAP-\nING framework significantly outperforms all LM\nprompting baselines, on zero-shot KGQA tasks. In\nparticular, the generated knowledge model mostly\ndegenerates the performance compared to the no\nknowledge model, since the extracted knowledge\nfrom LLMs themselves might be inaccurate. On\nthe other hand, the random and popular knowledge\nbaselines bring performance improvements, since\nthe augmented knowledge from KGs are sometimes\nuseful to answer the question. However, ours out-\nperforms them, which suggests that, for zero-shot\nLM prompting for QA, the knowledge internalized\nin LLMs is insufficient to generate factual answers,\nand it is important to use only the relevant facts.\nIn addition, we also observe larger performance\nimprovements when LMs are relatively small. In\nother words, since smaller models have insufficient\nparameter spaces to memorize the knowledge dur-\ning pre-training, they are more likely to generate\nfactually incorrect answers. However, when the ap-\npropriate knowledge is given to them, their perfor-\nmances sometimes become similar to larger models\n(e.g., different sizes of OPT have similar perfor-\nmances by our KAPING). Therefore, for tasks that\nrequire factual knowledge under low-resource se-\ntups (e.g., production), augmenting the knowledge\nwould be beneficial, instead of increasing model\nsizes to handle the huge volume of knowledge.\n75\nFigure 3: Comparisons of correct and incorrect retrieval\nfor the generation performance on the GPT-3 (6.7B) model.\nCorrect Incorrect\n10\n30\n50\n70\n90Generation Accuracy\nWebQSP w/ Freebase\nCorrect Incorrect\n10\n30\n50\n70\n90\nWebQSP w/ Wikidata\nCorrect Incorrect\n20\n35\n50\n65\n80\nMintaka w/ Wikidata\nNo Knowledge\nKAPING (Ours)\nOPT (6.7B) T0 (11B) GPT-3 (6.7B)\n20\n30\n40\n50\n60\n70\n80Generation Accuracy\nWebQSP w/ Freebase\nRelevant Facts at Top\nRelevant Facts at Random\nRelevant Facts at Bottom\nOPT (6.7B) T0 (11B) GPT-3 (6.7B)\n20\n30\n40\n50\n60\n70\n80\nWebQSP w/ Wikidata\nOPT (6.7B) T0 (11B) GPT-3 (6.7B)\n0\n8\n16\n24\n32\n40\nMintaka w/ Wikidata\nFigure 4: Performances with varying the knowledge order,\nwhere we change the location – top, bottom, or random – of\nmore relevant triples for the question in the prompt of LLMs.\nRetriever Results To see how relevant the aug-\nmented knowledge is, we further measure the re-\ntrieval performances. As shown in Table 2, the\nexisting retrieval model (i.e., MPNet) shows supe-\nrior performances against naive models: random\nand popular retrievers. This result suggests that\nour simple graph-to-text verbalization works well\nwith the existing retriever, which further confirms\nthat our KAPING augments useful facts in the LM\nprompt. Regarding the number of hops for the can-\ndidate triples to retrieve, we observe that, when we\nincrease the hop-size from one to two, the retriever\nis more likely to retrieve irrelevant triples that does\nnot include answer entities, as shown in Table 2.\nTherefore, in our experiments, we retrieve knowl-\nedge among 1-hop triples of question entities.\nAdditionally, since we can alternatively answer\nthe input question based on entities in the Top-1\ntriple from the retriever, we compare the generation\nperformance of LLMs to the retrieval performance.\nAs shown in Figure 2, LM prompting schemes even\nwithout knowledge augmentation (i.e., no knowl-\nedge) are superior than simply answering with the\nentity in the retrieved triple, except for the We-\nbQSP w/ Freebase dataset. Also, we observe huge\ngaps between our KAPING framework and the sim-\nple retrieval scheme on all datasets. These results\nsuggest that, for zero-shot KGQA, it would be help-\nful to leverage LLMs to generate answers based\non their internalized and external facts, instead of\ndirectly searching answer entities over KGs.\nImpact of Correct & Incorrect Retrievals We\nconduct analyses on how much the correctly re-\ntrieved triples, having answer entities, bring perfor-\nmance improvements, and how performances are\naffected by the incorrectly retrieved triples, which\nFigure 5: Performances with varying knowledge amount,\nwhere we change the number of retrieved triples to augment.\n01 3 5 10 15 30\n# of Augmented Facts\n20\n40\n60Generation Accuracy\nWebQSP w/ Freebase\n01 3 5 10 15 30\n# of Augmented Facts\n20\n40\n60\nWebQSP w/ Wikidata\n01 3 5 10 15 30\n# of Augmented Facts\n10\n20\n30\n40\n Mintaka w/ Wikidata\nOPT (2.7B) OPT (6.7B) T0 (3B) T0 (11B) GPT-3 (6.7B)\nRelative TimeModels # of Retrieved Facts T0 (3B) OPT (2.7B)No Knowledge 0 1.00 1.00\nKAPING (Ours)\n1 0.49 1.125 0.73 1.4810 1.07 1.8915 1.54 2.3630 2.49 3.77\nTable 3: Efficiencies with varying the knowledge amount,\nwhere we measure the wall-clock time of every model for\ngenerating the answer on the WebQSP w/ Wikidata dataset.\ndo not include answer entities. As shown in Fig-\nure 3, when retrieved triples contain answer entities,\nperformances of LLMs are significantly improved,\ncompared to models without knowledge augmenta-\ntion. However, when retrievers fail, performances\nare lower than models of no knowledge augmenta-\ntion. These results suggest, when relevant knowl-\nedge is augmented, LLMs can contextualize and\ngenerate answers accurately. Meanwhile, incor-\nrectly retrieved knowledge makes LLMs condition\non irrelevant facts, and generate wrong answers.\nVarying the Amount of Knowledge We change\nthe number of facts, to see which triple amounts\nare optimal to augment in the prompt, by compar-\ning trade-off between the generation performance\nand the wall-clock time. First of all, as shown in\nFigure 5, most LLMs reach the somewhat highest\nperformance, when the number of triples is 5 or\n10. Also, when we further increase the augmented\ntriple size to 15 and 30, performances of OPT mod-\nels are largely decreasing. This result suggests that\nsome LMs might be distracted by irrelevant triples\nwhen their volumes are high, therefore, failing to\nselect and generate the answer entity.\nWe then measure the wall-clock time of the an-\nswer generation, for the encoder-decoder (T0) and\ndecoder-only (OPT) models with varying the num-\nber of augmented triples in the prompt. As shown\nin Table 3, regarding the encoder-decoder model,\nour KAPING framework with less than 10 triples\nis faster than the model without knowledge aug-\nmentation. We observe this is because, when the\nknowledge is augmented to the model, the model\ntends to generate shorter answers, which can reduce\nthe decoding time. More specifically, the length of\ngenerated tokens for the T0 model with 10 triples is\n76\nTable 4: Generation examples of the prompted GPT-3 for the input question with augmented triples from the retriever, where, in\nthe last row, we change the knowledge of augmented facts to see whether the model is able to adapt to the changed knowledge.\nQuestion: Where did Alex Chilton die?\nRetrieved triples: (Alex Chilton, place of death,NewOrleans), (Alex Chilton, manner of death, natural causes), (Alex Chilton,\ncause of death,myocardialinfarction), (Alex Chilton, date of death, time:+2010-03-17), ...\nAnswer: NewOrleans\nPrompts Outputs\nNo Knowledge Question: Where did Alex Chilton die? Answer: Alex Chilton died in acaraccident onFebruary10,1979.\nKAPING (Ours)\nBelow are facts in the form of the triple meaningful to answer the\nquestion. ... (Alex Chilton, date of death, time:+2010-03-17) (Alex\nChilton, cause of death,myocardialinfarction) (Alex Chilton, manner\nof death, natural causes) (Alex Chilton, place of death,NewOrleans)\nQuestion: Where did Alex Chilton die? Answer:\nAlex Chilton died onMarch17,2010 inNewOrleans,\nLouisiana due to amyocardialinfarction.\nKAPING (Ours)\nw/ Knowledge Change\nBelow are facts in the form of the triple meaningful to answer the\nquestion. ... (Alex Chilton, date of death, time:+2000-09-01) (Alex\nChilton, cause of death,pancreaticcancer) (Alex Chilton, manner of\ndeath, natural causes) (Alex Chilton, place of death,LosAngeles)\nQuestion: Where did Alex Chilton die? Answer:\nAlex Chilton died inLosAngeles, California onSeptem-\nber1,2000 frompancreaticcancer.\nModels T5 (3B)T5(11B)OPT(2.7B)OPT(13B)T0(3B)T0(11B)\nNo Knowledge 14.25 17.06 19.76 26.83 14.75 23.74\nRandom Knowledge 18.19 18.83 28.11 28.36 16.10 26.15Random Knowledge w/ EL 15.99 17.98 23.10 26.47 15.60 24.66\nKAPING 22.00 22.85 32.94 33.37 20.68 29.50KAPING w/ EL 18.94 20.58 26.87 28.39 18.51 27.11\nTable 5: Results with entity linking, where the model w/ EL\nuses entities extracted from the entity linking technique (Ay-\noola et al., 2022), instead of using labeled ones, on Mintaka.\n15, whereas, the no knowledge model generates 32\ntokens in average. However, for the decoder-only\nmodel (OPT), the more knowledge we augment,\nthe slower the model becomes, because of its auto-\nregressive characteristic for digesting the input.\nImpact of Orders of Retrieved Triples In few-\nshot LM prompting where LLMs additionally ob-\nserve few examples in the prompt, they are known\nto be sensitive to the order of examples (Lu et al.,\n2022), and they tend to follow the answer in the last\nexample (Zhao et al., 2021). Based on those obser-\nvations, we also conduct an analysis on whether the\norder of retrieved triples affects the performance.\nIn particular, we vary the location of more similar\ntriples for the question, by locating them at the Top,\nBottom, or Random position of the prompt. As\nshown in Figure 4, our KAPING is not sensitive to\nthe location of retrieved triples, except for the OPT\nmodel on the WebQSP dataset. In other words, the\nOPT model tends to generate the entity located at\nthe first part of the prompt input. Meanwhile, other\nLLMs can contextualize the entire prompt input,\nand generate the entity regardless of its position.\nEffectiveness with Entity Linking Following\nthe conventional KGQA evaluation (Cohen et al.,\n2020), we use question entities labeled in datasets,\nto retrieve facts in KGs. However, to see the per-\nformance with entities identified by Entity Linking\n(EL) technique, we further conduct experiments\nwith the EL model, namely ReFinED (Ayoola et al.,\n2022). As shown in Table 5, while the performance\nof KAPING w/ EL is slightly decreasing from the\nmodel with labeled entities due to the performance\nof EL, we consistently observe meaningful perfor-\nmance improvements from a No Knowledge model.\nCase Study We conduct a case study in Table 4.\nIn particular, when the knowledge is not given to\nthe LM, it hallucinates the factually incorrect an-\nswer. However, when related facts are retrieved\nand augmented in the prompt, it can generate the\ncorrect answer. In addition, we analyze whether\nour KAPING can adapt to the updated knowledge,\nmotivated by that some knowledge can be changed\nover time, while the knowledge in LMs remains\nstatic. To do so, as shown in the last row of Table 4,\nwe replace object entities of triples, and then for-\nward the prompt with the modified facts to the LM.\nThen, the result shows that the LM can generate\nthe output based on the updated facts, which sug-\ngests the potential of adapting LMs without costly\nupdating their parameters.\nAdditional Results Note that we further provide\nadditional experimental results in Appendix B. In\nparticular, we compare the performance of retriev-\ners in Appendix B.1, conduct the sensitivity anal-\nysis on template texts in Appendix B.2, provide\nthe results with additional metrics including human\nevaluation in Appendix B.3, validate our KAPING\nunder few-shot setups in Appendix B.4, provide\nthe analysis on verbalization in Appendix B.5, and\nprovide the efficiencies in Appendix B.6.\n6 Conclusion\nIn this work, we focused on the limitation of ex-\nisting LM prompting schemes, which rely on the\n77\nstatic knowledge internalized in parameters; there-\nfore, when such knowledge are incomplete, inaccu-\nrate, and outdated, LLMs may generate factually\nincorrect answers. To tackle this challenge, we in-\ntroduced a novel Knowledge-Augmented language\nmodel PrompTING (KAPING) framework, which\naugments the knowledge for the input question\nfrom KGs directly in the input prompt of LLMs,\nwith the fact retriever to inject only the relevant\nknowledge. The proposed framework is completely\nzero-shot, and versatile with any LMs, without ad-\nditional parameter updates and training datasets.\nWe validated that our KAPING yields huge perfor-\nmance gaps from the LM prompting model relying\non its internal knowledge, especially with smaller\nLMs, on the KGQA tasks. We believe our new\nmechanism for augmenting facts from KGs to the\nLM prompt will bring substantial practical impacts\nin generating knowledge-grounded answers.\nLimitations\nIn this section, we faithfully discuss the current lim-\nitations and potential avenues for future research.\nFirst of all, the generation performance of our\nknowledge-augmentation framework largely de-\npends on the efficacy of retrievers. In other words,\nif the retriever fails to retrieve the relevant facts to\nthe input question, the prompted LLM, conditioned\non the irrelevant facts, is likely to generate the in-\ncorrect answer (See Figure 3). Similarly, if the re-\ntriever is not designed to retrieve the facts in 2-hop\nneighborhoods of the question entities, LLMs are\nless likely to generate the answer requiring 2-hop\nknowledge. Note that, for the Mintaka dataset (Sen\net al., 2022), the number of answerable questions\nwith 1-hop facts is only 40% of total samples. How-\never, when we include 2-hop triples, the number\nof answerable questions becomes 62%, which sug-\ngests the necessity of 2-hop retrievals, which is yet\nchallenging (See Table 2). Thus, future work may\nimprove the retrieval scheme itself to provide more\naccurate facts including multi-hops to the LLM, or\nmay develop the mechanism to prevent the LLM\nfrom being misled by unrelated facts.\nOn the other hand, the evaluation metric for the\ngeneration performance of prompted LLMs may be\nfurther improved. Specifically, regarding our target\nKGQA tasks, the answer for the question is the en-\ntity in KGs. However, the prompted LLMs without\nadditional training (i.e., zero-shot) tend to gener-\nate the answer as the sentence. For instance, the\nlabel entity for the question (e.g., Where did Alex\nChilton die?) in Table 4 is \"New Orleans\", how-\never, the LLMs often generate the sentence-level\noutput: \"Alex Chilton died on March 17, 2010\nin New Orleans, Louisiana due to a myocardial\ninfarction\". We currently evaluate the model per-\nformance by measuring whether generated tokens\ncontain the answer entity or not; however, it would\nbe worthwhile to develop the additional metric to\ncompare the sentence-level output from LLMs to\nthe word-level answer in KGs in a more effective\nway. Note that we also try other available metrics\n(See Appendix B.3), such as F1 and Exact Match\n(EM) scores (Rajpurkar et al., 2016), however, they\nlargely penalize the longer sentences (e.g., EM of\ncorrect examples in Table 4 are 0), thus may not be\nappropriate for evaluating LM prompting schemes.\nLastly, since we focus on the improvement of\nknowledge injection in LM prompting, we use the\nlabeled entities in KGQA datasets when evaluating\nmodels, following the existing KGQA evaluation\nsetups (Cohen et al., 2020; Sen et al., 2021). How-\never, in real-world applications where the entities\nin the question are mostly not provided, we first\nneed to extract entities in the question with exist-\ning entity linking techniques; therefore, our model\nperformance depends on the efficacy of entity link-\ning. In particular, regarding the result with entity\nlinking in Table 5, the portion of answerable ques-\ntions from labeled entities in the dataset is 40%,\nhowever, the portion of them with entities from the\nentity linking model (Ayoola et al., 2022) is 22%.\nTherefore, since the improved entity linking perfor-\nmance would contribute to the performance gain of\nour KAPING framework, for KGQA tasks, future\nwork may advance such the entity linking scheme.\nEthics Statement\nFor a user’s question, our knowledge-augmentation\nscheme can allow prompted LMs generate a fac-\ntually correct answer, grounded by the provided\nknowledge, for KGQA tasks. However, the per-\nformance of our KAPING framework is still far\nfrom perfect, due to potential failures in entity link-\ning, fact retrieval, and knowledge generation itself.\nThus, we should be aware whether LMs generate\ncorrect answers, especially on high-risk domains.\nAcknowledgements\nWe thank the members of the End-to-End Reason-\ning team of Alexa AI at Amazon and the anony-\nmous reviewers for their constructive comments.\n78\nReferences\nTom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos\nChristodoulopoulos, and Andrea Pierleoni. 2022.\nRefined: An efficient zero-shot-capable approach\nto end-to-end entity linking. arXiv preprint\narXiv:2207.04108.\nJunwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and\nTiejun Zhao. 2016. Constraint-based question an-\nswering with knowledge graph. In COLING. ACL.\nHannah Bast, Björn Buchhold, and Elmar Haussmann.\n2016. Semantic search on text and knowledge bases.\nFound. Trends Inf. Retr.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2013, 18-21 October\n2013, Grand Hyatt Seattle, Seattle, Washington, USA,\nA meeting of SIGDAT, a Special Interest Group of the\nACL. ACL.\nKurt D. Bollacker, Colin Evans, Praveen K. Paritosh,\nTim Sturge, and Jamie Taylor. 2008. Freebase: a\ncollaboratively created graph database for structuring\nhuman knowledge. In Proceedings of the ACM SIG-\nMOD International Conference on Management of\nData, SIGMOD 2008, Vancouver, BC, Canada, June\n10-12, 2008. ACM.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nNilesh Chakraborty, Denis Lukovnikov, Gaurav Ma-\nheshwari, Priyansh Trivedi, Jens Lehmann, and Asja\nFischer. 2019. Introduction to neural network based\napproaches for question answering over knowledge\ngraphs. arXiv preprint arXiv:1907.09361.\nXiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang,\nShumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022a. Decoupling knowledge from\nmemorization: Retrieval-augmented prompt learning.\narXiv preprint arXiv:2205.14704.\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022b. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. In WWW, pages 2778–2788.\nACM.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nWilliam W. Cohen, Haitian Sun, R. Alex Hofer, and\nMatthew Siegler. 2020. Scalable neural methods for\nreasoning with a symbolic knowledge base. In ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL. Association for Computational\nLinguistics.\nDennis Diefenbach, Thomas Pellissier Tanon, Ka-\nmal Deep Singh, and Pierre Maret. 2017. Question\nanswering benchmarks for wikidata. In Proceedings\nof the ISWC 2017 Posters & Demonstrations and\nIndustry Tracks co-located with 16th International\nSemantic Web Conference (ISWC 2017), Vienna, Aus-\ntria, October 23rd - to - 25th, 2017, CEUR Workshop\nProceedings. CEUR-WS.org.\nBin Fu, Yunqi Qiu, Chengguang Tang, Yang Li,\nHaiyang Yu, and Jian Sun. 2020. A survey on\ncomplex question answering over knowledge base:\nRecent advances and challenges. arXiv preprint\narXiv:2007.13069.\nFabian Galetzka, Jewgeni Rose, David Schlangen, and\nJens Lehmann. 2021. Space efficient context encod-\ning for non-task-oriented dialogue generation with\ngraph attention transformer. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, August 1-6, 2021 . Association\nfor Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\n79\nSebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\nficiently teaching an effective dense retriever with\nbalanced topic aware sampling. In SIGIR ’21: The\n44th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\nCanada, July 11-15, 2021. ACM.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nICML, Proceedings of Machine Learning Research.\nPMLR.\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan\nLiu, Jingang Wang, Juanzi Li, Wei Wu, and Maosong\nSun. 2022. Knowledgeable prompt-tuning: Incor-\nporating knowledge into prompt verbalizer for text\nclassification. In ACL, pages 2225–2240, Dublin,\nIreland. Association for Computational Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nMinki Kang, Jin Myung Kwak, Jinheon Baek, and\nSung Ju Hwang. 2022. Knowledge-consistent di-\nalogue generation with knowledge graphs. In ICML\n2022 Workshop on Knowledge Retrieval and Lan-\nguage Models.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing, EMNLP 2020, November\n16-20, 2020. Association for Computational Linguis-\ntics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021a.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 3045–\n3059. Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021b.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021 / Punta Cana, Dominican Republic,\n7-11 November, 2021. Association for Computational\nLinguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In NeurIPS.\nBelinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar\nMehdad, and Wen-tau Yih. 2020. Efficient one-pass\nend-to-end entity linking for questions. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Novem-\nber 16-20, 2020. Association for Computational Lin-\nguistics.\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021.\nPretrained Transformers for Text Ranking: BERT and\nBeyond. Synthesis Lectures on Human Language\nTechnologies. Morgan & Claypool Publishers.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022a. What\nmakes good in-context examples for gpt-3? In Pro-\nceedings of Deep Learning Inside Out: The 3rd Work-\nshop on Knowledge Extraction and Integration for\nDeep Learning Architectures, DeeLIO@ACL 2022,\nDublin, Ireland and Online, May 27, 2022 , pages\n100–114. Association for Computational Linguistics.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022b. Generated knowledge prompting\nfor commonsense reasoning. In ACL. Association for\nComputational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In ACL. Association\nfor Computational Linguistics.\nKangqi Luo, Fengli Lin, Xusheng Luo, and Kenny Q.\nZhu. 2018. Knowledge base question answering via\nencoding of complex query graphs. In Proceedings\nof the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, Brussels, Belgium, Octo-\nber 31 - November 4, 2018. Association for Compu-\ntational Linguistics.\n80\nKaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg,\nand Jianfeng Gao. 2022. Open domain question an-\nswering with A unified knowledge interface. In ACL.\nAssociation for Computational Linguistics.\nVaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022.\nA survey on multi-hop question answering and gen-\neration. arXiv preprint arXiv:2204.09140.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In Proceedings of\nthe Workshop on Cognitive Computation: Integrat-\ning neural and symbolic approaches 2016 co-located\nwith the 30th Annual Conference on Neural Infor-\nmation Processing Systems (NIPS 2016), Barcelona,\nSpain, December 9, 2016, CEUR Workshop Proceed-\nings. CEUR-WS.org.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin,\nStan Peshterliev, Dmytro Okhonko, Michael Sejr\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\nScott Yih. 2022. Unik-qa: Unified representations\nof structured and unstructured knowledge for open-\ndomain question answering. In Findings of the As-\nsociation for Computational Linguistics: NAACL .\nAssociation for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In NeurIPS. Curran Associates, Inc.\nTed Pedersen, Siddharth Patwardhan, and Jason Miche-\nlizzi. 2004. Wordnet: : Similarity - measuring the\nrelatedness of concepts. In Proceedings of the Nine-\nteenth National Conference on Artificial Intelligence,\nSixteenth Conference on Innovative Applications of\nArtificial Intelligence, July 25-29, 2004, San Jose,\nCalifornia, USA, pages 1024–1025. AAAI Press /\nThe MIT Press.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 . Association for Computational Lin-\nguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016. The Association\nfor Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In EMNLP.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object hallu-\ncination in image captioning. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018. Association for Computational\nLinguistics.\nMd. Rashad Al Hasan Rony, Ricardo Usbeck, and\nJens Lehmann. 2022. Dialokg: Knowledge-structure\naware task-oriented dialogue generation. In Find-\nings of the Association for Computational Linguistics:\nNAACL. Association for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 2655–2671. Association for\nComputational Linguistics.\nAmir Saffari, Armin Oliya, Priyanka Sen, and Tom\nAyoola. 2021. End-to-end entity resolution and\nquestion answering using differentiable knowledge\ngraphs. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021 / Punta Cana, Dominican Republic,\n7-11 November, 2021. Association for Computational\nLinguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In ICLR.\nApoorv Saxena, Aditay Tripathi, and Partha P. Talukdar.\n2020. Improving multi-hop question answering over\n81\nknowledge graphs using knowledge base embeddings.\nIn ACL. Association for Computational Linguistics.\nPriyanka Sen, Alham Fikri Aji, and Amir Saffari.\n2022. Mintaka: A complex, natural, and multilin-\ngual dataset for end-to-end question answering. In\nCOLING. International Committee on Computational\nLinguistics.\nPriyanka Sen, Armin Oliya, and Amir Saffari. 2021.\nExpanding end-to-end question answering on differ-\nentiable knowledge graphs with intersection. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2021\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021. Association for Computational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 4222–4235. Association for\nComputational Linguistics.\nSaleh Soltan, Shankar Ananthakrishnan, Jack FitzGer-\nald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith\nPeris, Stephen Rawls, Andy Rosenbaum, Anna\nRumshisky, Chandana Satya Prakash, Mukund Srid-\nhar, Fabian Triefenbach, Apurv Verma, Gökhan Tür,\nand Prem Natarajan. 2022. Alexatm 20b: Few-shot\nlearning using a large-scale multilingual seq2seq\nmodel. arXiv preprint arXiv:2208.01448.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding. In NeurIPS.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence, February\n4-9, 2017, San Francisco, California, USA , pages\n4444–4451. AAAI Press.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William W. Co-\nhen. 2018. Open domain question answering using\nearly fusion of knowledge bases and text. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018. Association for Com-\nputational Linguistics.\nMujeen Sung, Jinhyuk Lee, Sean S. Yi, Minji Jeon,\nSungdong Kim, and Jaewoo Kang. 2021. Can lan-\nguage models be biomedical knowledge bases? In\nEMNLP. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nDenny Vrandecic and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. arXiv preprint arXiv:2203.11171.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In ICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations. Association for Computational Lin-\nguistics.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian\nRiedel, and Luke Zettlemoyer. 2020. Scalable zero-\nshot entity linking with dense entity retrieval. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, November 16-20, 2020. Association for Com-\nputational Linguistics.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In ICLR.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nreasoning with language models and knowledge\ngraphs for question answering. In NAACL. Asso-\nciation for Computational Linguistics.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jian-\nfeng Gao. 2015. Semantic parsing via staged query\ngraph generation: Question answering with knowl-\nedge base. In ACL. The Association for Computer\nLinguistics.\nWen-tau Yih, Matthew Richardson, Christopher Meek,\nMing-Wei Chang, and Jina Suh. 2016. The value of\nsemantic parse labeling for knowledge base question\n82\nanswering. In ACL. The Association for Computer\nLinguistics.\nJun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang\nLi, and Xiaoming Li. 2016. Neural generative ques-\ntion answering. In Proceedings of the Twenty-Fifth\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2016, New York, NY, USA, 9-15 July\n2016, pages 2972–2978. IJCAI/AAAI Press.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: open pre-\ntrained transformer language models. arXiv preprint\narXiv:2205.01068.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nICML, Proceedings of Machine Learning Research.\nPMLR.\nYucheng Zhou, Xiubo Geng, Tao Shen, Wenqiang\nZhang, and Daxin Jiang. 2021. Improving zero-shot\ncross-lingual transfer for multilingual question an-\nswering over knowledge graph. In NAACL. Associa-\ntion for Computational Linguistics.\n83\nA Additional Experimental Setups\nHere we provide additional experimental setups.\nA.1 Datasets\nWe provide the additional details for two Knowl-\nedge Graph Question Answering (KGQA) datasets,\nnamely WebQuestionsSP and Mintaka, which we\nuse for evaluating baselines and our model.\nWebQuestionsSP (WebQSP) A question and its\ncorresponding answer are annotated with Freebase\nentities (Bollacker et al., 2008), and refined with\nadditional cleaning steps (Yih et al., 2016): filter-\ning out samples with invalid annotations, from the\noriginal WebQuestions dataset (Berant et al., 2013).\nMintaka This dataset (Sen et al., 2022) is de-\nsigned for complex KGQA tasks including superla-\ntive and comparative questions, where question-\nanswer pairs are collected from crowdsourcing with\nWikidata entities (Vrandecic and Krötzsch, 2014).\nA.2 Large Language Models\nWe describe the specific details of Large Language\nModels (LLMs) that we use for LM prompting.\nT5 This model (Raffel et al., 2020) is an encoder-\ndecoder model, and, among different variants, we\nuse the LM-adapted version1, which is additionally\npre-trained with auto-regressive language modeling\nobjective (Radford et al., 2018) for LM prompting.\nT0 This model (Sanh et al., 2022) is further fine-\ntuned from T5 (Raffel et al., 2020) over prompted\ntext-to-text tasks, for improved zero-shot general-\nization performance with LM prompting.\nGPT-3 This model (Brown et al., 2020) is a de-\ncoder only model, which we access via API2.\nOPT This model (Zhang et al., 2022) is a decoder\nonly model, freely available for researchers.\nAlexaTM This model (Soltan et al., 2022) is an\nencoder-decoder model, pre-trained with denoising,\nwhich reconstructs the context of 15% dropped\ntokens, and auto-regressive, which predicts the next\ntokens based on their previous tokens, objectives.\nA.3 Evaluation Metrics\nWe provide more details for evaluation metrics.\n1https://github.com/google-research/text-to-text-transfer-\ntransformer/blob/main/released_checkpoints.md\n2https://openai.com/api/\nAliases For generative question answering tasks,\nthere can be alternative names of entities, called\naliases, and we consider them for evaluation. For\nexample, one Wikidata entity, \"William Shake-\nspeare\" (Q692), has alternative names, such as\n\"Shakespeare\" and \"The Bard\", and we consider\nthem when measuring the generation performance.\nFiltering Unnamed Entities For evaluating gen-\nerative models, the name of entities are required.\nHowever, we sometime cannot find the name of the\nanswer entities from their ids on Freebase and Wiki-\ndata KGs. This is because the annotated answer\nentities are sometimes not entities but categories,\nand the entity ids in KGs could be changed but we\ncannot find the KG dumps that are used to anno-\ntate datasets. Therefore, we filter out samples that\ndo not have literal name texts for the answer enti-\nties. This filtering step results in 1,582 test samples\nfor the WebQSP w/ Freebase dataset, 1,466 test\nsamples for the WebQSP w/ Wikidata dataset, and\n2,814 test samples for the Mintaka dataset.\nA.4 Implementation Details\nIn this subsection, we provide additional details for\nimplementing our KAPING framework.\nKnowledge Injection Schemes There are differ-\nent choices in knowledge injection schemes, from\nthe number of facts to retrieve, to the number of\nhops for candidate triples, to the order of retrieved\nfacts in the prompt (i.e., where the most relevant\nknowledge should be located in the prompt), to\nthe template of prompts including their instruction\ntexts. While search spaces of them are extremely\nhuge, we aim to to find the optimal one (See analy-\nses in Section 5). Specifically, as reported in Sec-\ntion 4.5, the best settings we find are the number\nof retrieved facts of 10, and the number of hops\nfor the triples to retrieve from the question enti-\nties of one. Also, we locate more relevant triples\nto the input question closer to the question text in\nthe prompt, inspired by the observation that the\nmodel tends to rewrite answers that appeared at the\nend of the prompt (Zhao et al., 2021). Further, we\nexamine different instruction templates for gener-\nating answers, such as \"Question: {x}Answer: \"\nor \"Please answer the following question: {x}\",\nwhere x is the literal question. Regarding instruc-\ntion templates, we observe that the performances\nof LLMs are sensitive across different instructions\n(See Appendix B.2), therefore, we try both of them\nand then report the best result.\n84\n1-Hop Retrieval 2-Hop Retrieval\nDatasets Retrievers MRR Top-1 Top-10 Top-30 MRR Top-1 Top-10 Top-30\nWebQSPw/ FreebaseMPNet 47.27 40.27 60.56 64.48 41.64 33.12 58.47 65.23TAS-B 51.62 45.76 61.76 64.41 37.08 25.85 58.66 64.48\nWebQSPw/ WikidataMPNet 43.46 33.36 64.39 70.67 40.42 30.56 62.62 71.56TAS-B 46.68 37.65 65.08 70.67 41.92 32.20 62.21 72.17\nMintakaw/ WikidataMPNet 13.01 7.50 25.44 35.43 13.00 6.82 26.65 40.01TAS-B 13.21 7.57 25.20 35.04 12.36 6.79 24.13 36.07\nTable 6: Results of two different retrievers , namely MP-\nNet (Song et al., 2020) and TAS-B (Hofstätter et al., 2021).\nRetrieval Models To augment only the relevant\ntriples to the input question under the zero-shot\nsetup, we use off-the-shelf text-based retriever mod-\nels. Specifically, we experiment with two different\ntypes of retrievers: symmetric retriever that uses\nthe same encoder for question and triples; asymmet-\nric one that uses individual encoders for them. For\nthe symmetric retriever, we use MPNet (Song et al.,\n2020), which is trained on 1B sentence pairs3. Also,\nfor the asymmetric retriever, we use TAS-B (Hof-\nstätter et al., 2021), which is trained on the MS-\nMARCO dataset (Nguyen et al., 2016). We mainly\nreport the results with MPNet, unless noted, since\nthere performances are similar (See Appendix B.1).\nA.5 Hyperparameters and Resources\nWe evaluate all models with PyTorch (Paszke et al.,\n2019) and Transformers (Wolf et al., 2020) li-\nbraries. We set the maximum number of input\ntoken lengths of LMs as 1,024 and the maximum\nnumber of output token lengths as 128, for encoder-\ndecoder models. For decoder-only models, we set\nthe maximum token lengths as 1,152 (1,024 + 128).\nFor computing resources, we run all models with\n8 V100 GPUs, having 8 ×32GB GPU memory,\nin which every model is runnable within one day.\nNote that, due to the expensive computational costs\nfor model prompting with LLMs, we run every\nmodel one time, and then report the results, with-\nout additional hyperparameter tuning unless noted.\nB Additional Experiment Results\nIn this section, we provide additional experimental\nresults, on the comparisons of available text-based\nretrieval models in Section B.1, the sensitive analy-\nses on template texts of the prompt in Section B.2,\nand the extra evaluation metrics in Section B.3.\nB.1 Performance Comparisons of Retrievers\nIn Table 6, we compare existing symmetric and\nasymmetric retrievers named MPNet (Song et al.,\n3https://huggingface.co/sentence-transformers/all-mpnet-\nbase-v2\nDatasets Models Templates T5(11B)T0(11B)OPT(6.7B)GPT-3(6.7B)\nWebQSPw/ Freebase\nNo KnowledgeDefault 9.48 34.70 29.77 44.63Please 3.03 40.77 18.71 42.48\nKAPINGDefault 24.91 62.58 43.93 60.37Please 17.45 61.19 34.07 60.43\nWebQSPw/ Wikidata\nNo KnowledgeDefault 15.21 38.88 33.77 48.50Please 5.12 44.20 22.71 48.29\nKAPINGDefault 35.47 58.73 53.34 60.44Please 20.12 56.89 48.16 59.69\nMintakaw/ Wikidata\nNo KnowledgeDefault 17.06 22.60 27.19 35.00Please 5.47 23.74 17.70 34.65\nKAPINGDefault 22.85 29.50 32.37 33.55Please 14.68 29.18 28.18 35.61\nTable 7: Results with varying instruction templates , for\nvarious LLMs on the WebQSP and Mintaka datasets.\n2020) and TAS-B (Hofstätter et al., 2021), ex-\nplained in Section A.4, on 1- and 2-hop retrievals.\nAs shown in Table 6, we observe similar perfor-\nmances between symmetric (MPNet) and asym-\nmetric (TAS-B) retrievers, which suggests that our\nsimple graph-to-text verbalization is robust across\ndifferent text-based retrieval schemes. Note that,\nsince retrieval performances of both are similar, we\nconduct experiments mainly with MPNet, to reduce\nexpensive computational costs for GPU usages.\nB.2 Sensitivity Analyses on Template Texts\nFollowing the observation in Zhao et al. (2021),\nthe performances of LLMs vary across different\ntemplates in the prompt. In our experiments, since\nit is computationally infeasible to try all different\nprompt templates on various LLMs, we consider\ntwo types of question templates, described in Ap-\npendix A.4. In particular, for the question x, we\nuse either \"Question: {x}Answer: \", which we\nrefer to as default template, or \"Please answer the\nfollowing question: {x}\", referred to as please tem-\nplate. As shown in Table 7, for the T5 model, the\ndefault template is superior than the please tem-\nplate. Meanwhile, for the OPT model, the please\ntemplate is superior than the other. However, for\nT0 and GPT-3 models, performance differences be-\ntween default and please templates are marginal.\nTherefore, these results suggest that we may need\nto select instruction templates carefully across dif-\nferent LLMs for achieving optimal performances.\nAdditionally, regarding the knowledge-injection\ntemplate described in Section 3.2, we also observe\nthat the generation performance of GPT-3 depends\non the instruction text in the template. In particular,\nwe mainly conduct experiments with the template:\n\"Below are facts in the form of the triple meaning-\nful to answer the question.\"; however, we observe\nthe performance degeneration when the augmented\ntriples are irrelevant to the given question as shown\n85\nT5(0.8B) T5(3B) T5(11B) OPT(2.7B) OPT(6.7B) OPT(13B)\nDatasets Methods Acc. F1 EM Acc. F1 EM Acc. F1 EM Acc. F1 EM Acc. F1 EM Acc. F1 EM\nWebQSPw/ Freebase\nNo Knowledge 6.95 5.20 0.00 13.40 8.11 0.00 9.48 8.25 0.06 19.85 7.20 0.38 29.77 10.60 0.06 28.38 7.92 0.70Random Knowledge 21.55 9.74 0.00 19.15 8.08 0.00 17.57 7.50 0.19 28.07 13.33 0.06 31.73 13.01 0.00 33.31 12.41 0.00Popular Knowledge 15.30 8.75 0.06 16.88 8.19 0.00 18.39 8.95 0.19 28.32 13.78 0.06 28.13 12.21 0.00 24.21 9.86 0.00Generated Knowledge 6.19 7.96 0.00 7.84 7.56 0.06 6.76 6.51 0.00 7.46 4.59 0.00 11.50 4.95 0.00 8.22 4.59 0.00\nKAPING (Ours)34.70 15.39 0.00 25.41 8.31 0.06 24.91 11.02 0.32 41.09 16.32 0.00 43.93 15.15 0.00 40.20 13.32 0.00\nWebQSPw/ Wikidata\nNo Knowledge 10.30 5.60 0.00 18.42 8.48 0.00 15.21 8.94 0.07 23.94 7.90 0.48 33.77 11.41 0.07 32.40 8.45 0.75Random Knowledge 17.94 7.81 0.00 22.78 7.74 0.07 24.28 9.41 0.34 37.24 16.78 0.00 35.61 12.54 0.00 38.27 14.61 0.07Popular Knowledge 15.35 8.01 0.00 20.80 8.48 0.00 20.74 9.20 0.14 30.83 15.65 0.00 30.01 13.32 0.00 27.83 11.95 0.00Generated Knowledge 11.94 8.64 0.00 13.30 8.19 0.07 12.28 7.11 0.00 11.26 5.06 0.00 17.53 5.60 0.00 14.19 4.94 0.00\nKAPING (Ours)23.67 10.46 0.00 40.38 13.25 0.00 35.47 11.50 0.34 49.52 20.17 0.00 53.34 16.62 0.00 51.57 16.73 0.14\nMintakaw/ Wikidata\nNo Knowledge 11.23 6.77 0.00 14.25 9.81 0.00 17.06 10.28 0.00 19.76 6.63 0.28 27.19 10.60 0.04 26.83 9.82 0.43Random Knowledge 17.59 10.48 0.18 18.19 9.24 0.00 18.83 9.82 0.57 28.11 14.47 0.00 26.58 12.80 0.00 28.36 14.02 0.11Popular Knowledge 17.56 9.88 0.00 18.09 10.47 0.07 18.73 10.07 0.53 26.97 13.76 0.00 27.08 12.95 0.07 23.10 11.28 0.00Generated Knowledge 13.61 9.23 0.00 14.61 8.85 0.00 14.29 7.51 0.04 11.87 6.34 0.00 14.96 5.81 0.04 16.24 7.14 0.00\nKAPING (Ours)19.72 11.36 0.04 22.00 11.17 0.00 22.85 10.91 0.43 32.94 14.99 0.00 32.37 14.37 0.04 33.37 14.65 0.11\nT0(3B) T0(11B) AlexaTM(20B) GPT-3(6.7B) GPT-3(175B) Average\nDatasets Methods Acc. F1 EM Acc. F1 EM Acc. F1 EM Acc. F1 EM Acc. F1 EM Acc. F1 EM\nWebQSPw/ Freebase\nNo Knowledge 21.43 22.70 9.99 40.77 46.10 34.39 46.79 17.65 0.00 44.63 21.12 1.77 63.59 32.75 8.47 29.55 17.05 5.07Random Knowledge 32.62 36.48 26.55 51.20 55.98 46.90 57.37 20.91 0.00 51.01 28.04 6.19 65.87 41.28 18.46 37.22 22.43 8.94Popular Knowledge 27.05 31.38 20.23 47.22 52.44 42.04 54.91 20.45 0.00 45.58 25.94 4.87 62.26 38.84 17.00 33.48 20.98 7.68Generated Knowledge 19.41 23.15 10.56 38.81 43.43 31.23 35.13 14.42 0.00 45.89 27.98 9.48 62.14 38.79 17.57 22.67 16.72 6.26\nKAPING (Ours)52.28 55.27 48.04 62.85 66.11 58.53 67.67 23.16 0.00 60.37 32.89 8.34 73.89 43.15 20.6747.94 27.28 12.36\nWebQSPw/ Wikidata\nNo Knowledge 24.56 24.20 10.98 44.20 49.27 37.65 42.41 16.43 0.00 48.50 24.01 3.96 67.60 34.31 10.30 32.85 18.09 5.84Random Knowledge 28.85 33.08 22.37 47.68 52.34 42.50 55.63 19.88 0.06 52.05 25.37 2.18 60.64 36.88 13.92 38.27 21.49 7.41Popular Knowledge 24.83 27.89 16.03 48.02 52.84 41.88 53.92 19.77 0.00 47.41 24.36 3.75 63.37 37.08 14.73 34.83 20.78 6.96Generated Knowledge 22.92 25.28 11.80 41.34 45.70 33.83 31.16 13.36 0.00 48.77 29.72 11.19 65.89 39.52 17.87 26.42 17.56 6.80\nKAPING (Ours)49.86 50.75 41.27 58.73 61.90 53.27 65.04 22.72 0.00 60.44 31.18 6.82 69.58 41.83 19.7150.69 27.01 11.05\nMintakaw/ Wikidata\nNo Knowledge 14.75 20.84 11.34 23.74 28.69 20.86 41.97 17.05 0.00 34.65 17.67 2.31 56.33 26.77 6.11 26.16 14.99 3.76Random Knowledge 16.10 23.08 14.14 26.15 31.70 22.85 46.02 17.02 0.00 32.98 17.55 1.39 51.56 25.98 6.29 28.22 16.92 4.14Popular Knowledge 16.74 23.13 14.53 27.15 32.17 23.45 46.41 17.31 0.00 32.48 20.07 4.41 53.16 27.44 6.86 27.95 17.14 4.54Generated Knowledge 14.46 20.08 11.98 23.13 27.34 18.76 34.58 14.91 0.00 33.12 18.29 3.09 55.65 30.69 11.73 22.41 14.20 4.15\nKAPING (Ours)20.68 27.80 18.12 29.50 34.83 26.23 49.08 17.90 0.00 35.61 20.80 5.79 56.86 28.63 7.6432.27 18.86 5.31\nTable 8: LM prompting results with additional metrics: F1 and Exact Match (EM), along with accuracy (Acc.) scores.\nin Figure 3. Therefore, to improve the performance\non incorrect retrievals, we further experiment with\nthe additional template: \"Below are facts in the\nform of the triple that might be meaningful to\nanswer the question.\". Then, the GPT-3 (175B)\nmodel with the previous template achieves 74.16\nand 42.80 accuracies for correct and incorrect re-\ntrievals, respectively. Meanwhile, the same model\nwith the instruction template containing \"might be\"\nachieves 72.91 and 51.38 accuracies for correct and\nincorrect retrievals, respectively. Thus, these re-\nsults suggest that the knowledge-injection template\nwith \"might be\" statement makes the model less\nselective on the augmented triples while focusing\nmore on the internalized knowledge in parameters,\nthus improving the incorrect retrieval performance\nwhile degenerating the correct retrieval.\nB.3 Additional Evaluation Metrics\nAs described in Section 4.4, we evaluate the perfor-\nmance of LLMs based on whether generated tokens\nfor the input question contain answer entities or not.\nThis is because, as explained in Section 6 of the\nlimitation, pre-trained LLMs without further fine-\ntuning tend to generate the answer as the sentence,\nwhile the answer for the KGQA task is the entity\nconsisting of few tokens. In this subsection, we\nfurther provide experiment results with additional\nevaluation metrics (Rajpurkar et al., 2016), namely\nF1 and Exact Match (EM) scores. Note that they\nare frequently used for evaluating extractive QA\nmodels, whose goal is to classify the answer span in\nthe given context, without generation. As shown in\nTable 8, since the F1 score penalizes the longer sen-\ntence too much, the performances of LLMs evalu-\nated by F1 scores are largely decreasing, except for\nthe T0 model that is further fine-tuned by prompted\ntext-to-text tasks, including QA, thus capable of\ngenerating entity-level outputs. Similarly, except\nfor the T0, it is highly suboptimal to evaluate the\nperformance of prompted LMs with EM scores,\ndue to differences in output lengths. Thus, it would\nbe promising direction to further develop better\nevaluation metrics for KGQA under LM prompting\nschemes, which we leave as future work.\nWhile such F1 and EM scores, used for extrac-\ntive QA tasks, might be suboptimal to evaluate\ngenerative LM prompting schemes, our KAPING\nframework consistently outperforms all the other\nbaselines based on averaged F1 and EM scores as\nwell, by large margins. Note that the superior EM\nand F1 scores of the generated knowledge base-\nline with GPT-3 on few cases, even though they\nare rarely happen, is because, for this baseline, the\nGPT-3 model generates entity-level outputs, unlike\nours that generates sentence-level outputs. In other\nwords, the sentence-level outputs from our KAP-\nING is often longer than the answer entities, since\nour model is grounded by retrieved facts from KGs\nas shown in Table 15; however, longer sentences\npenalize F1 and EM scores. More specifically, the\naverage number of output sequence lengths of the\n86\nLLMs Models Correct Semi-Correct Incorrect\nT0 (3B) No Knowledge 7 1 22\nKAPING (Ours) 17 0 13\nT0 (11B) No Knowledge 14 0 16\nKAPING (Ours) 20 0 10\nGPT-3 (6.7B)No Knowledge 12 4 14\nKAPING (Ours) 19 4 17\nGPT-3 (175B)No Knowledge 22 1 7\nKAPING (Ours) 26 1 3\nTable 9: Human evaluation results , where we randomly\nsample 30 examples from the WebQSP w/ Freebase dataset.\nModels Shots T5 (3B) OPT(6.7B) T0(11B)\nNo Knowledge\nZero-Shot 18.42 33.77 44.20\nOne-Shot 18.28 36.90 41.13\nThree-Shots 17.87 37.65 37.38\nKAPING (Ours)\nZero-Shot 40.38 53.34 58.73\nOne-Shot 18.42 52.25 48.70\nThree-Shots 10.16 50.34 43.45\nTable 10: KGQA results with few-shot learning. We vary\nthe number of examples (i.e., shots) in the prompt, and report\nthe performances on the WebQSP w/ Wikidata dataset.\ngenerated knowledge model is 67.77, meanwhile,\nours is 74.92. However, when we compare the gen-\nerated knowledge baseline to our KAPING with\nother LLMs but also with other metrics, our KAP-\nING significantly outperforms this baseline.\nHuman Evaluation Additionally, similar to the\nprevious generative QA work (Roberts et al., 2020),\nwe manually inspect 30 samples from the WebQSP\nw/ Freebase dataset, to see whether the generated\nsentence is factually correct to the input question.\nFor this experiment, we evaluate four LLMs: T0\n(3B), T0 (11B), GPT-3 (6.7B), and GPT-3 (175B),\nwith no knowledge baseline and our KAPING.\nAlso, we use three different ratings for each genera-\ntion example: 1) we label it as correct if all informa-\ntion in the generated sentence is factually correct to\nthe question; 2) we label it as semi-correct if some\ninformation in the generated sentence is factually\nincorrect which yet contains at least one answer\nentity; 3) we label it as incorrect for all the other\ncases. As shown in Table 9, we observe that our\nKAPING framework can generate the factually cor-\nrect answer more, compared to the no knowledge\nbaseline, which are consistent with the results from\navailable evaluation metrics in Table 1 and Table 8.\nWe provide generated answers, which we use for\nhuman evaluation in Table 9, for GPT-3 (175B) and\nT0 (3B) models in Table 15 and Table 16.\nB.4 Performances of Few-Shot Learning\nWhile the focus of our work is zero-shot as outlined\nin the main paper, in this subsection, we addition-\nally extend this zero-shot setting to the few-shot\nRetrievers MRR Top-1 Top-10 Top-30\nRandom Retrieval 9.50 3.62 22.58 40.72\nPopular Retrieval 8.52 4.57 15.89 35.47\nRetrieval with Free-Form Texts 41.33 31.11 62.07 69.92\nRetrieval with Triple-Form Texts 43.46 33.36 64.39 70.67\nTable 11: Retrieval results with different verbalizers. We\nuse the graph-to-text transformation model proposed in Ma\net al. (2022) for obtaining free-form texts. For triple-form\ntexts, we use the verbalization technique described in Sec-\ntion 3.2. MPNet (Song et al., 2020) is used as the retriever,\nand the performance is reported on WebQSP w/ Wikidata.\nRetrievers T5 (3B) OPT(6.7B)T0(3B) T0(11B)\nNo Knowledge 18.42 33.77 24.56 44.20\nKAPING with Free-Form Texts 43.25 53.00 47.75 53.21\nKAPING with Triple-Form Texts 40.38 53.34 49.86 58.73\nTable 12: KGQA results with different verbalizers. We use\nthe graph-to-text transformation model proposed in Ma et al.\n(2022) for obtaining free-form texts. For triple-form texts,\nwe use the verbalization technique described in Section 3.2.\nWe then inject the verbalized triples in the input prompt. We\nreport the generation accuracy on WebQSP w/ Wikidata.\nsetting, where we prepend the few examples about\nthe input-output pairs in the prompt of LLMs. As\nshown in Table 10, for the KGQA task, the per-\nformances are decreasing when we increase the\nnumber of samples (i.e., shots) in the input prompt,\nexcept for the OPT model. We suggest this might\nbe because, the injected examples in the prompt are\nless relevant to the given factual question, mislead-\ning the model to focus on unrelated contexts on the\ninjected examples. This phenomenon is even more\nsevere in our KAPING framework; this is similarly\nbecause our KAPING augments the retrieved facts,\nand if the facts on the other few-shot examples are\nfurther injected in the input prompt, the model is\nmore likely to be confused by those irrelevant facts.\nFor the OPT model, we observe a slight perfor-\nmance improvement in the No Knowledge model,\nsince few injected examples provide a hint on how\nthe output format looks like. We leave further ex-\ntending our zero-shot KAPING framework to the\nfew-shot learning mechanism as future work.\nB.5 Analyses on Knowledge Verbalization\nAs described in the Knowledge Verbalization para-\ngraph of Section 3.2, we use the linear triple ver-\nbalization technique, which simply concatenates\nthe tokens of subject, relation, and object in the\ntriple, instead of using the sophisticated techniques\nthat use the particular graph-to-text transformation\nmethods (Oguz et al., 2022; Ma et al., 2022). This\nis because, we observe that our simple verbaliza-\ntion technique works well, and, in this subsection,\nwe concretely show performance differences be-\ntween our and existing verbalization techniques in\n87\nRelative Time\nModels # of Augmented Knowledge T5 (0.8B) T5 (3B) T5 (11B) OPT (2.7B) OPT (6.7B) OPT (13B) T0 (3B) T0 (11B)\nNo Knowledge 0 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\nDocument (Web)Augmentation\n1 1.20 1.45 2.13 1.43 1.65 1.63 1.60 2.295 2.78 4.16 6.80 3.42 3.90 3.66 2.98 9.0110 OOL OOL OOL 6.44 7.36 6.67 OOL OOL15 OOL OOL OOL 9.35 10.71 OOM OOL OOL30 OOL OOL OOL OOL OOL OOL OOL OOL\nKAPING (Ours)\n1 1.08 0.97 1.35 1.12 1.21 1.19 0.49 1.285 1.22 1.50 2.13 1.48 1.65 1.60 0.73 2.1810 1.53 2.10 3.11 1.89 2.20 2.10 1.07 3.8315 1.84 2.74 4.02 2.36 2.76 2.58 1.54 4.5930 2.82 4.42 6.05 3.77 4.28 4.06 2.49 7.76\nTable 13: Efficiencies results, where we measure the wall-clock time of every model for generating answers on the WebQSP\nw/ Wikidata dataset. The document augmentation model (Lazaridou et al., 2022) augments documents listed in their paper,\nmeanwhile, ours augments relevant triples to the question retrieved from KGs. We set the maximum number of input sequences\nfor T5 and T0 models as 1,024, and for OPT as 2,048. OOL denotes the out-of-length errors, where the input prompt length\nexceeds the maximum input token lengths. OOM denotes the out-of-memory error on the machine having eight V100 GPUs.\nboth the knowledge retrieval and injection steps.\nNote that, for the comparison, we use the trained\nknowledge verbalizer proposed in Ma et al. (2022).\nWe first provide the fact retrieval performances\nacross the different knowledge verbalization meth-\nods in Table 11. As shown in Table 11, we observe\nthat our simple triple-form text verbalization is su-\nperior to the free-form text verbalization in the fact\nretrieval. This might be because the free-form ver-\nbalization model, transforming the graph to the text,\nmight generate the incorrect output that is semanti-\ncally different from the original triple, leading to\nthe degenerated retrieval performances.\nOn the other hand, we also report the genera-\ntion results of KGQA with two different knowl-\nedge verbalizers on our KAPING framework in Ta-\nble 12. As shown in Table 12, we observe that the\nperformances between the free-form texts and the\ntriple-form texts are comparable when augmented\nto LLMs with our KAPING framework. More\nspecifically, for the T5 model, which is pre-trained\non the unlabeled corpus without additional instruc-\ntion tuning, the free-form text works well. Mean-\nwhile, for the T0 model, which is further fine-tuned\nwith natural language instruction tasks, it is benefi-\ncial to use our linear triple verbalizaton scheme.\nB.6 Additional Efficiency Comparisons\nIn this subsection, we further provide efficiency\nresults of all LLMs that we use in our main ex-\nperiments across three different models: no knowl-\nedge model, document augmentation (i.e., web aug-\nmentation) model (Lazaridou et al., 2022), and our\nKAPING framework. We note that, as discussed\nin the Knowledge-Augmented LMs paragraph of\nSection 2, the web augmentation method augments\ndocuments searched from Google with the few-\nshot learning setup. However, as we discuss there,\nthis web augmentation is orthogonal to ours, since\nwe use the completely different knowledge source\n(i.e., KGs) and our work is under the zero-shot\nlearning setup; from which our core mechanisms\nof how to retrieve and augment relevant knowledge\nwith LM prompting is clearly different and novel.\nFurthermore, as discussed in Section 2, this web\naugmentation method is infeasible to experimen-\ntally compare as well, since individual researches\ncannot freely access the Google Search API to re-\ntrieve documents for every question in the world.\nAlso, it is computationally expensive to augment\ndocuments consisting of hundreds to thousands to-\nkens (Lazaridou et al., 2022) in LLMs, unlike our\ntriple cases consisting of few tokens. In this sub-\nsection, to experimentally validate the latter issue,\nwe further make the comparisons of computational\ncosts between document augmentation and our fact\naugmentation. In particular, as shown in Table 13,\nthe answer generation speed of the web augmen-\ntation mechanism is significantly slower than our\ntriple augmentation mechanism, since it requires\nmore time to encode and condition documents in\nthe input prompt compared to triples. Also, fol-\nlowing the original paper (Lazaridou et al., 2022),\nthe suggested number of documents to augment is\n15, however, in the most cases, we observe out-of-\nlength (OOL) errors, since the length of the input\nprompt with 15 documents is longer than the maxi-\nmum input sequence length of LLMs. While our\nfact augmentation scheme is slower than the model\nwithout augmentation, we believe that, given the\nsubstantially improved performance in Table 1 and\nthe high efficiency compared to document augmen-\ntation in Table 13, KAPING is highly beneficial.\nB.7 Result Analyses Across Question Types\nFor the Mintaka dataset (Sen et al., 2022), each\nquestion is belong to one of the following cate-\ngories: Generic, Multihop, Intersection, Differ-\nence, Comparative, Superlative, Ordinal, Count,\nand Yes/No, which defines the complexity of ques-\n88\ntions. Therefore, to see which complexity category\nour knowledge-augmentation framework is helpful,\nand which category we should further improve on,\nwe breakdown the performance of LLMs according\nto question types in Table 14. Note that, following\nthe evaluation protocol in Section A.3 where we\nfilter out questions that do not have answer names,\nthe Yes/No type questions are not considered.\nAs shown in the last row of Table 14 where we\naverage the performance of all LLMs per category,\nour KAPING framework brings significant perfor-\nmance improvements on all categories except for\nthe Comparative type. One particular comparative-\ntype question is \"Who has won more NBA Season\nMVPs, LeBron James or Steph Curry\", and, since it\nis hard to retrieve and associate relevant triples for\nsuch the comparative-type question, our KAPING\nunderperforms simple knowledge-injection base-\nlines: random knowledge and popular knowledge.\nHowever, the KG-augmented models (e.g., random\nknowledge, popular knowledge, and our KAPING)\noutperform other baselines, which suggests that\nknowledge-augmentation mechanism is meaning-\nful to tackle comparative questions, and one might\nfurther improve the retrieval scheme or the input\nprompt itself, which we leave as future work.\nOn the other point we would like to mention\nis that, for the Count category, performances of\nT0 models are significantly low compared to other\nLLMs. This is surprising, since T0 models are fur-\nther fine-tuned on the prompted text-to-text tasks,\nand they have strong performances on the other cat-\negories, thanks to fine-tuning. We believe such\nthe low performance on the Count category is\nbecause, in the fine-tuning of T0 models, there\nare no prompted tasks related to counting, which\nmakes T0 models hard to count particular instances.\nTherefore, to further improve the generalization\nperformance of T0 models, one may additionally\ninclude more diverse prompted tasks, including the\ncounting one, during the fine-tuning process.\nB.8 Generation Examples\nWe provide generation examples for comparisons\nbetween the no knowledge baseline and our KAP-\nING framework in Table 15 and Table 16 for GPT-\n3 and T0 language models, respectively. We also\nprovide retrieved and generation examples of our\nKAPING framework with four different LLMs: T5\n(11B), OPT (13B), T0 (11B), and GPT-3 (175B)\non the WebQSP w/ Wikidata dataset in Table 17.\nC Discussions on Prompt Design/Tuning\nWe discuss differences between prompt design and\nprompt tuning, along with additional relevant work\nin the prompt tuning literature. As described in\nSection 3.1, given an input question, the large lan-\nguage model can generate the answer text, which is\ncalled LM prompting (Brown et al., 2020; Liu et al.,\n2021). However, to further enhance the perfor-\nmance of models under the LM prompting scheme,\nprior work particularly designs the content in the\nprompt, which is called prompt design (Shin et al.,\n2020; Lu et al., 2022). More specifically, Shin et al.\n(2020) additionally include the particular trigger\ntokens, meaningful to the down-stream tasks, in the\nprompt, and Lu et al. (2022) change the order of\ndemonstrations in the prompt under the few-shot\nLM prompting setup. Our method is in line with\nsuch the prompt design literature, and we introduce\nthe method of knowledge augmentation in the in-\nput prompt with facts from KGs, to allow LLMs\ncondition on factual knowledge for zero-shot QA.\nOn the other hand, there exists prompt tuning\nliterature (Lester et al., 2021a), which additionally\ntrains the prompt-relevant parameters with super-\nvised learning objectives, while keeping the pa-\nrameters of LLMs unchanged. While this prompt\ntuning approach can be beneficial in few-shot learn-\ning scenarios where the model is additionally tuned\nwith few training examples, it is not suitable for our\nzero-shot learning. Also, unlike the prompt design\napproach, it is difficult to interpret and manipulate\nthe prompt represented in the embedding space.\nNote that, recently, there are few knowledge-\naware prompt tuning work (Chen et al., 2022b; Hu\net al., 2022; Chen et al., 2022a), and, while they are\nfundamentally different from our LM prompting\n(i.e., prompt design), we additionally discuss them.\nFirst of all, Chen et al. (2022b) tackle the relation\nextraction problem with prompt tuning, where they\npropose to embed the particular words related to the\nrelation class in the embedding space. For example,\nfor the relation type to classify: \"county of birth\",\nthey embed person and country information in the\nrepresentation space with training signals from su-\npervised learning, for improved relation classifica-\ntion performance. Also, Hu et al. (2022) tackle the\ntext classification task with prompt tuning, where\nthey propose to not only consider the classifica-\ntion label word itself, but also the label word’s\nrelated words. For example, for the sentence label\n\"science\", they further consider its related words:\n89\n\"physics\" and \"mathematics\", defined in particular\nknowledge bases, such as WordNet (Pedersen et al.,\n2004) and ConceptNet (Speer et al., 2017). Lastly,\nChen et al. (2022a) tackle the similar text classifica-\ntion task with prompt tuning, where they propose\nto retrieve the data instance (i.e., a sentence and its\nlabel) in the training dataset based on the retriever\ntraining with supervised classification objectives.\nHowever, all the above knowledge-aware prompt\ntuning methods are clearly different from our pro-\nposed KAPING framework. At first, they are re-\nstricted to cloze-style prediction, in which they\nfirst include the particular mask token in the in-\nput prompt, and then classify the label (e.g., senti-\nment of the sentence, or relation in the given sen-\ntence) of the mask token, similar to the masked\nlanguage modeling objective (Devlin et al., 2019;\nLiu et al., 2019). Therefore, their cloze-style pre-\ndiction schemes cannot be used for QA tasks, since\nthe answer of the user’s question is not the single\ntoken, and it is unclear to convert the predicted\nlabel token from the masked token to all differ-\nent answers in the world. In contrast to them, our\nKAPING does not rely on the masked token clas-\nsification scheme, thus ours is more flexible, and\nnot restricted to cloze-style classification; suitable\nfor answering any user’s questions. Furthermore,\nsome of them (Chen et al., 2022a,b) rely on training\nsignals from the training dataset with supervised\nlearning, meanwhile, ours is completely zero-shot.\nWhile Chen et al. (2022a) show the model’s zero-\nshot ability, they require the training dataset as\ndiscussed in their paper, thus not suitable for our\nzero-shot QA as well. Lastly, we augment the\nfactual knowledge by matching the entity in the\nquestion to its associated triples in KGs, however,\nprior work considers different knowledge source,\nwhich might not be helpful for QA tasks, such as\nrelationships between words (Hu et al., 2022), rela-\ntionships between the relation class and particular\nwords (Chen et al., 2022b), and a pair of sentence\nand its label in training data (Chen et al., 2022a).\n90\nLLMs Models Generic (557)Multihop(220)Intersection(396)Difference(349)Comparative(223)Superlative(384)Ordinal(307)Count(378)\nT5 (0.8B)\nNo Knowledge 7.00 3.64 8.08 7.45 69.06 2.86 2.61 10.05\nRandom Knowledge 11.49 5.45 8.33 11.75 86.10 6.77 8.14 26.98\nPopular Knowledge 13.82 5.91 11.62 8.60 87.00 8.33 5.86 22.22\nGenerated Knowledge 7.72 2.73 5.81 8.02 82.06 3.39 1.95 21.43\nKAPING (Ours) 18.85 6.36 15.40 10.32 83.41 9.64 7.49 24.60\nT5 (3B)\nNo Knowledge 10.41 4.09 9.60 9.74 71.30 5.47 4.56 17.99\nRandom Knowledge 17.41 6.82 13.64 14.61 55.16 8.59 7.82 30.42\nPopular Knowledge 14.90 6.82 14.90 13.75 57.40 8.85 10.75 28.84\nGenerated Knowledge 7.90 3.64 8.33 8.31 82.51 4.69 3.91 21.96\nKAPING (Ours) 25.31 12.27 20.96 15.76 47.98 10.68 9.77 35.71\nT5 (11B)\nNo Knowledge 10.23 5.00 10.35 8.60 92.83 7.55 3.58 24.87\nRandom Knowledge 20.29 7.27 11.87 12.89 60.99 10.68 9.12 27.51\nPopular Knowledge 16.88 7.27 12.88 13.18 72.20 9.11 10.42 24.34\nGenerated Knowledge 7.72 2.73 5.30 7.45 89.24 3.91 2.28 22.49\nKAPING (Ours) 24.42 8.64 18.69 16.05 65.92 11.98 11.07 34.66\nOPT (2.7B)\nNo Knowledge 24.06 10.00 16.67 10.32 54.26 20.05 14.98 14.29\nRandom Knowledge 29.44 13.18 23.74 18.34 93.27 15.62 14.01 34.13\nPopular Knowledge 28.90 14.09 20.45 18.62 90.58 12.76 13.36 34.13\nGenerated Knowledge 7.90 6.82 10.35 8.02 44.84 4.19 4.56 20.11\nKAPING (Ours) 33.75 15.91 34.85 20.63 93.27 15.89 19.54 43.65\nOPT (6.7B)\nNo Knowledge 29.62 12.73 37.37 20.06 62.78 20.83 22.80 16.93\nRandom Knowledge 23.52 14.09 19.44 20.92 89.69 13.02 15.31 36.77\nPopular Knowledge 24.42 13.18 24.24 22.92 83.86 14.84 17.26 32.80\nGenerated Knowledge 11.67 8.64 16.92 12.61 43.95 7.55 6.51 20.90\nKAPING (Ours) 33.39 11.36 33.08 20.92 87.44 17.19 20.2 45.77\nOPT (13B)\nNo Knowledge 33.57 16.82 34.85 18.91 48.43 19.27 19.22 22.75\nRandom Knowledge 31.60 17.27 26.77 23.78 59.19 16.93 20.85 35.45\nPopular Knowledge 22.98 13.64 24.49 18.34 59.64 11.72 12.05 30.69\nGenerated Knowledge 17.95 10.00 19.44 12.03 47.98 8.07 9.77 12.70\nKAPING (Ours) 40.04 17.27 35.61 23.50 56.05 19.53 27.36 45.24\nT0 (3B)\nNo Knowledge 13.82 10.00 14.39 10.89 49.33 14.06 8.79 7.94\nRandom Knowledge 19.57 9.09 15.66 12.32 58.30 8.59 9.77 6.88\nPopular Knowledge 19.21 10.00 18.69 12.03 60.09 8.33 8.79 8.73\nGenerated Knowledge 13.11 11.36 12.63 12.61 54.71 12.50 10.10 3.70\nKAPING (Ours) 29.98 10.45 26.01 12.32 55.16 12.24 11.40 10.85\nT0 (11B)\nNo Knowledge 33.93 18.18 33.08 18.05 54.71 19.53 13.68 1.59\nRandom Knowledge 36.98 22.27 34.60 21.78 58.74 18.75 19.22 1.59\nPopular Knowledge 38.42 24.09 38.64 24.36 58.74 17.45 18.57 1.06\nGenerated Knowledge 33.21 17.73 34.09 17.48 51.12 18.23 14.33 0.79\nKAPING (Ours) 45.60 27.27 41.16 22.35 56.05 18.75 23.45 1.59\nGPT-3 (6.7B)\nNo Knowledge 40.39 28.18 34.34 24.36 74.44 26.04 24.76 33.07\nRandom Knowledge 39.68 26.82 30.05 23.78 77.13 19.53 23.13 33.86\nPopular Knowledge 40.57 25.00 32.83 22.64 70.85 21.35 21.17 31.48\nGenerated Knowledge 40.75 23.64 33.59 28.08 71.75 20.83 22.15 30.16\nKAPING (Ours) 46.14 24.09 33.33 24.36 77.58 19.53 24.76 35.71\nGPT-3 (175B)\nNo Knowledge 71.10 52.73 64.90 49.00 80.72 42.45 50.81 38.62\nRandom Knowledge 62.30 46.82 56.31 43.55 86.10 38.54 48.21 36.51\nPopular Knowledge 68.40 54.09 58.84 46.42 81.61 37.76 47.88 33.60\nGenerated Knowledge 70.56 56.82 64.14 48.14 85.65 44.79 49.19 29.63\nKAPING (Ours) 69.48 56.36 63.89 48.14 82.96 45.57 49.84 41.01\nAverage\nNo Knowledge 27.41 16.14 26.36 17.74 65.79 17.81 16.58 18.81\nRandom Knowledge 29.23 16.91 24.04 20.37 72.47 15.70 17.56 27.01\nPopular Knowledge 28.85 17.41 25.76 20.09 72.20 15.05 16.61 24.79\nGenerated Knowledge 21.85 14.41 21.06 16.28 65.38 12.82 12.48 18.39\nKAPING (Ours)36.70 19.00 32.30 21.44 70.58 18.10 20.49 31.88\nTable 14: Results across question types – Generic, Multihop, Intersection, Difference, Comparative, Superlative, Ordinal, and\nCount – in the Mintaka dataset with various LLMs, where we report the generation accuracy. The number inside the parentheses\nin the first row denotes the number of data instances per category. We provide averaged results in the last row, namelyAverage.\n91\nTable 15: Generation examples of GPT-3 (6.7B)on the WebQSP w/ Freebase dataset, where we randomly sample 30 examples.\nIndex Question Answer Entities No Knowledge KAPING\n1 who is tatyana ali parents? [Sheriff Ali, Sonia Ali] Tatyana Ali’s parents are unknown. Tatyana Ali’s parents are Sheriff Ali\nand Sonia Ali.\n2 what language do most\naustralians speak?\n[English Language, Esperanto\nLanguage, Lojban]\nMost Australians speak English. Most Australians speak English.\n3 who played darth vader in\nthe first movie?\n[David Prowse, James Earl\nJones]\nJames Earl Jones Mark Hamill\n4 who does michael keaton\nplay in cars?\n[Chick Hicks, Chick Hicks] Michael Keaton plays the role of Larry\n\"Lester\" Fenton in Cars.\nThe role of Michael Keaton in Cars is\nthat of a voice actor.\n5 what was pink floyd’s first\nalbum?\n[Psychedelic Session-\ncompilation album]\nThe first album released by Pink Floyd\nwas the self-titled \"Pink Floyd\" in 1965.\nPink Floyd’s first album was called\n\"Pink Floyd\".\n6 where is walton county\nga?\n[Atlanta-Sandy Springs-\nMarietta, GA Metropolitan\nStatistical Area, Georgia]\nWalton County is located in Georgia. According to the US Census 2000, Wal-\nton County is located in between 30641\nand 30025, in Gratis, Georgia.\n7 what kind government\ndoes egypt have?\n[Semi-presidential system, Pro-\nvisional government]\nEgypt has a presidential system. Egpyt has a semi-presidential system of\ngovernment.\n8 where is english spoken\naround the world?\n[Bahamas, Bangladesh, ...,\nUnited Kingdom, United States\nof America, Australia, New\nZealand, Canada]\nEnglish is spoken around the world. English is spoken in different countries\naround the world. England, United\nStates of America, United Kingdom\n9 who did steve spurrier\nplay pro football for?\n[San Francisco 49ers, Tampa\nBay Buccaneers]\nSteve Spurrier played professional foot-\nball for the Tampa Bay Buccaneers\nfrom 1976 to 1978.\nSt. Louis Rams\n10 what kind of music did\nfranz liszt compose?\n[Opera, Romantic music, Art\nsong, Classical music, Cham-\nber music]\nFranz Liszt composed a great deal of\nclassical music, including symphonies,\noperas, and piano sonatas.\nFranz Liszt composed both classical\nand popular music. Some of his most\nfamous works include the opera Don\nGiovanni, the symphonies Annabella\nand Egmont, and the song Totentanz.\n11 what was the currency\nused in france before the\neuro?\n[Assignat, French franc] Until the late 1990s, France used the\nfranc as its official currency.\nthe currency used in France before the\neuro was the French franc.\n12 where is the riviera\ncasino?\n[Las Vegas] The Riviera Casino is located in Las\nVegas, NV .\nRiviera casino is located on the Las Ve-\ngas strip in Nevada.\n13 what does wh smith stand\nfor?\n[WHSmith] Whit Smith is an American printing\nand publishing company. It exclusively\npublishes books for children and young\nadults.\nWilliam Henry Smith\n14 who will play mr gray in\nthe film?\n[Jamie Dornan] Mr. Gray has not been announced, but\nit is likely that the role will be played\nby a younger actor.\nThe film will be based on the book\n\"Fifty Shades of Grey\" by E.L. James,\nand the character Mr. Grey will be\nplayed by Jamie Dornan.\n15 what religions are popular\nin france?\n[Catholicism] There are a number of religions popular\nin France, some of which are Christian-\nity, Islam, Hinduism, and Judaism.\nAccording to the 2016 census, 51.6%\nof the population of France identify as\nChristians, including 35.9% who iden-\ntify as Catholic and 14.4% as Protes-\ntant. The remaining 5.1% of the popu-\nlation identify with other religions. Fur-\nthermore, according to the 2012 census,\nFrench is the language most commonly\nspoken at home by the population of\nFrance. It is also the language most\ncommonly spoken in the administrative\nregions of metropolitan France, with\nItalian second and French Creole third.\n16 who did nasri play for be-\nfore arsenal?\n[Olympique de Marseille] he played for Universidad de Chile and\nMonaco before joining Arsenal in 2011.\nSamir Nasri played for different teams\nbefore Arsenal, including Manchester\nCity and Marseille.\nContinued on the next page\n92\nTable 15 – Continued from the previous page\nIndex Question Answer Entities No Knowledge KAPING\n17 who plays ziva david on\nncis?\n[Cote de Pablo] Ziva David is played by Sandra Oh. Ziva David is played by actress Debra\nMessing in the NBC television series\nNCIS.\n18 where did adolf hitler die? [Berlin] Adolf Hitler died in his home town of\nBraunau am Inn, Austria on April 30,\n1945, days before German forces un-\nconditionally surrendered in World War\nII.\nAdolf Hitler committed suicide in\nBerlin, Germany.\n19 what was hitler the leader\nof?\n[Hitler Youth, Gestapo, 1st\nSS Panzer Division Leibstan-\ndarte SS Adolf Hitler, Nazi\nParty, Schutzstaffel, Waffen-SS,\nWehrmacht, Sturmabteilung]\nHitler was the leader of Nazi Germany,\nwhich was a fascist dictatorship in Eu-\nrope during the 1930s and 1940s.\nThe leader of Nazi Germany was Adolf\nHitler.\n20 what types of art did andy\nwarhol do?\n[Modern art, Pop art] Andy Warhol did a variety of art, includ-\ning paintings, sculptures, and prints.\nvisual_art.visual_artist.artworks, men\nin her life\n21 who owns the portland\npress herald?\n[Blethen Maine Newspapers,\nInc.]\nThe Portland Press Herald is not owned\nby any specific individual or entity. It\nis an independent, locally owned news-\npaper.\nThe Portland Press Herald is owned by\nthe Blethen Maine Newspapers, Inc.\n22 where was the vietnam\nwar location?\n[Vietnam, Cambodia, Laos,\nSoutheast Asia, South Vietnam,\nNorth Vietnam]\nThe Vietnam War was fought in Viet-\nnam, Cambodia, and Laos\nThe Vietnam War was fought in Cam-\nbodia, Laos, North Vietnam, and South\nVietnam.\n23 who was isaac newton and\nwhat did he do?\n[Chemist, Philosopher, Math-\nematician, Physicist, Scientist,\nAstronomer]\nIsaac Newton was a physicist, mathe-\nmatician, astronomer, and author of the\nNewtonian Theory of Motion. He is\nmost famous for his three laws of mo-\ntion and his theory of universal gravita-\ntion.\nIsaac Newton was a scientist, mathe-\nmatician, and philosopher.\n24 what standard time is\nmichigan?\n[UTC-05:00, Central Time\nZone, Eastern Time Zone]\nDaylight Saving Time is in effect in\nMichigan from the first Sunday in April\nto the last Sunday in October.\nMichigan is in the Eastern Time Zone.\n25 what is the kentucky state\nbird?\n[Cardinal] The Kentucky state bird is the cardinal. The kentucky state bird is the pigeon.\n26 what do people in jamaica\nspeak?\n[Jamaican English, Jamaican\nCreole English Language, Ja-\nmaican English]\npeople in Jamaica speak English. Jamaican Creole English Language\n27 who wrote the jana gana\nmana?\n[Ram Singh Thakur, Ra-\nbindranath Tagore]\nThe author of the Jana Gana Mana is\nunknown.\nJana Gana Mana was written by the\ncomposer Ram Singh Thakur.\n28 what is the currency of the\nczech republic?\n[Czech koruna] The Czech Republic currency is the\nCzech koruna.\nThe currency of the Czech Republic is\nthe Czech koruna.\n29 what highschool did\nharper lee go to?\n[Monroe County High School] Harper Lee attended the Monroeville,\nAlabama high school.\nHarper Lee attended Monroeville High\nSchool.\n30 what did michael crabtree\ndo?\n[Wide receiver, Athlete, Amer-\nican football player, Wide re-\nceiver]\nMichael Crabtree is a former football\nplayer who played for the Baltimore\nRavens. Crabtree caught a pass in the\nend zone to tie the game against the San\nFrancisco 49ers in the final seconds of\nthe game in 2013.\nHe is an American football player.\n93\nTable 16: Generation examples of T0 (3B) on the WebQSP w/ Freebase dataset, where we randomly sample 30 examples.\nIndex Question Answer Entities No Knowledge KAPING\n1 who is tatyana ali parents? [Sheriff Ali, Sonia Ali] ali is the daughter of ali khan and\ntatyana ali\nSheriff Ali\n2 what language do most\naustralians speak?\n[English Language, Esperanto\nLanguage, Lojban]\nAustralian English English Language\n3 who played darth vader in\nthe first movie?\n[David Prowse, James Earl\nJones]\nDarth Vader George Lucas\n4 who does michael keaton\nplay in cars?\n[Chick Hicks, Chick Hicks] a narrator George A. Douglas\n5 what was pink floyd’s first\nalbum?\n[Psychedelic Session-\ncompilation album]\nThe Pink Floyd album Pink Floyd\n6 where is walton county\nga?\n[Atlanta-Sandy Springs-\nMarietta, GA Metropolitan\nStatistical Area, Georgia]\nWalton County is a county in the U.S.\nstate of Ohio.\nWalton County, Georgia\n7 what kind government\ndoes egypt have?\n[Semi-presidential system, Pro-\nvisional government]\ngovernment Semi-presidential system\n8 where is english spoken\naround the world?\n[Bahamas, Bangladesh, ...,\nUnited Kingdom, United States\nof America, Australia, New\nZealand, Canada]\nThe English language is spoken by ap-\nproximately 6.1 million people in the\nUnited Kingdom, with the remainder\nin the United States, Canada, Australia,\nNew Zealand, Canada, and the United\nStates Virgin Islands.\nEngland\n9 who did steve spurrier\nplay pro football for?\n[San Francisco 49ers, Tampa\nBay Buccaneers]\nthe New York Giants the New York Giants\n10 what kind of music did\nfranz liszt compose?\n[Opera, Romantic music, Art\nsong, Classical music, Cham-\nber music]\nclassical Art song\n11 what was the currency\nused in france before the\neuro?\n[Assignat, French franc] The pound was used in France before\nthe euro.\nFrench franc\n12 where is the riviera\ncasino?\n[Las Vegas] Riviera Maya, Mexico Las Vegas\n13 what does wh smith stand\nfor?\n[WHSmith] smith is a scottish slang word for a\nsmith\nWilliam Henry Smith\n14 who will play mr gray in\nthe film?\n[Jamie Dornan] Corey Haim Christian Grey\n15 what religions are popular\nin france?\n[Catholicism] Roman Catholicism Protestantism\n16 who did nasri play for be-\nfore arsenal?\n[Olympique de Marseille] Arsenal Arsenal\n17 who plays ziva david on\nncis?\n[Cote de Pablo] sarah savage Donald P. Bellisario\n18 where did adolf hitler die? [Berlin] in a concentration camp Berlin\n19 what was hitler the leader\nof?\n[Hitler Youth, Gestapo, 1st\nSS Panzer Division Leibstan-\ndarte SS Adolf Hitler, Nazi\nParty, Schutzstaffel, Waffen-SS,\nWehrmacht, Sturmabteilung]\nthe Nazi Party World War I\n20 what types of art did andy\nwarhol do?\n[Modern art, Pop art] art visual art.visual_artist.artworks, Men in\nHer Life\n21 who owns the portland\npress herald?\n[Blethen Maine Newspapers,\nInc.]\nThe Portland Press Herald is a local, in-\ndependent newspaper based in Portland,\nOregon, United States.\nBlethen Maine Newspapers\nContinued on the next page\n94\nTable 16 – Continued from the previous page\nIndex Question Answer Entities No Knowledge KAPING\n22 where was the vietnam\nwar location?\n[Vietnam, Cambodia, Laos,\nSoutheast Asia, South Vietnam,\nNorth Vietnam]\nVietnam War Vietnam\n23 who was isaac newton and\nwhat did he do?\n[Chemist, Philosopher, Math-\nematician, Physicist, Scientist,\nAstronomer]\nwas a british physicist physicist\n24 what standard time is\nmichigan?\n[UTC-05:00, Central Time\nZone, Eastern Time Zone]\nStandard time is local time in the United\nStates.\nMidwestern United States\n25 what is the kentucky state\nbird?\n[Cardinal] a red-winged blackbird Pigeon\n26 what do people in jamaica\nspeak?\n[Jamaican English, Jamaican\nCreole English Language, Ja-\nmaican English]\nPeople in Jamaica speak English Jamaican Creole English Language\n27 who wrote the jana gana\nmana?\n[Ram Singh Thakur, Ra-\nbindranath Tagore]\nthe author of the jana gana mana is a\npoet\nRabindranath Tagore\n28 what is the currency of the\nczech republic?\n[Czech koruna] the czech koruna Czech koruna\n29 what highschool did\nharper lee go to?\n[Monroe County High School] Harper Lee High School Monroeville High School\n30 what did michael crabtree\ndo?\n[Wide receiver, Athlete, Amer-\nican football player, Wide re-\nceiver]\ndid not have a job American football player\n95\nTable 17: Generation examples of our KAPING framework with four different models: T5 (11B), OPT (13B), T0 (11B), and\nGPT-3 (175B) on the WebQSP w/ Wikidata dataset. Note that the answer entities are labeled entities in the test dataset.\nIndex Question Retrieved Triples Answer Entities Generated Answers\n1\nwhat is the name of\nthe currency used in\nchina?\n(People’s Republic of China, currency, renminbi),\n(People’s Republic of China, short name, text: Chine),\n(People’s Republic of China, short name, text: Chiny),\n(People’s Republic of China, language used, Chinese),\n(People’s Republic of China, central bank, People’s\nBank of China), (People’s Republic of China, language\nused, China Buriat), (People’s Republic of China,\ndemonym, text: Chinesin), (People’s Republic of\nChina, language used, Jingpho), (People’s Republic of\nChina, language used, Langnian Buyang), (People’s\nRepublic of China, language used, Then)\n[Renminbi]\nT5: Renminbi\nOPT: Renminbi\nT0: renminbi\nGPT-3: The name of the currency used in\nchina is renminbi.\n2\nwhat does george\nwilson do for a\nliving?\n(George Wilson, country of citizenship, United States\nof America), (George Wilson, occupation, American\nfootball player), (George Wilson, sex or gender, male),\n(George Wilson, given name, George), (George\nWilson, instance of, human), (George Wilson, place of\nbirth, Paducah), (George Wilson, member of sports\nteam, Arkansas Razorbacks football), (George Wilson,\nmember of sports team, Detroit Lions), (George\nWilson, member of sports team, Buffalo Bills),\n(George Wilson, educated at, Paducah Tilghman High\nSchool)\n[American\nfootball player]\nT5: Wilson, George was born on August\n10, 1972 in Paducah, Kentucky. He is an\nAmerican football player.\nOPT: George Wilson is an American foot-\nball player.\nT0: American football player\nGPT-3: George Wilson is an American\nfootball player.\n3 what was augustus\ncaesar famous for?\n(Augustus, Roman cognomen, Caesar), (Augustus,\nfather, Julius Caesar), (Augustus, occupation, Ancient\nRoman politician), (Augustus, child, Lucius Caesar),\n(Augustus, child, Gaius Caesar), (Augustus, Roman\ncognomen, Caepias), (Augustus, relative, Julius\nCaesar), (Augustus, Roman praenomen, Gaius),\n(Augustus, father, Gaius Octavius), (Augustus, family,\nJulii Caesares)\n[Roman emperor,\nPolitician]\nT5: Augustus was a Roman politician who\nruled from 14 BC to 14 AD. He was the\nson of Gaius Octavius and Octavia. He\nwas the first emperor of the Roman Em-\npire.\nOPT: Augustus Caesar was famous for\nbeing the first Roman emperor.\nT0: He was the first Roman Emperor\nGPT-3: Augustus Caesar was famous for\nbeing the first Roman Emperor.\n4\nwhat document did\njames madison\nwrite?\n(James Madison, described by source, Obalky\nknih.cz), (James Madison, described by source, The\nNuttall Encyclopædia), (James Madison, Commons\nCreator page, text: James Madison), (James Madison,\nwriting language, English), (James Madison, described\nby source, Brockhaus and Efron Encyclopedic\nDictionary), (James Madison, notable work, Federalist\nPapers), (James Madison, occupation, statesperson),\n(James Madison, occupation, writer), (James Madison,\ndescribed by source, Library of the World’s Best\nLiterature), (James Madison, position held, President\nof the United States)\n[Federalist\nPapers]\nT5: federalist papers\nOPT: I think the answer is \"The Federalist\nPapers\"\nT0: Federalist Papers\nGPT-3: the Federalist Papers\n5\nwhere did george\nmichael go to\nschool?\n(George Michael, educated at, Bushey Meads School),\n(George Michael, occupation, singer), (George\nMichael, occupation, singer-songwriter), (George\nMichael, place of birth, East Finchley), (George\nMichael, occupation, record producer), (George\nMichael, occupation, multi-instrumentalist), (George\nMichael, occupation, recording artist), (George\nMichael, occupation, composer), (George Michael,\noccupation, songwriter), (George Michael, country of\ncitizenship, United Kingdom)\n[Bushey Meads\nSchool]\nT5: . Question: where did george michael\ngo to school? Answer: bushey meads\nschool\nOPT: Bushey Meads School\nT0: Bushey Meads School\nGPT-3: Bushey Meads School\nContinued on the next page\n96\nTable 17 – Continued from the previous page\nIndex Question Retrieved Triples Answer Entities Generated Answers\n6 what college did\njoakim noah?\n(Joakim Noah, educated at, University of Florida),\n(Joakim Noah, educated at, Lawrenceville School),\n(Joakim Noah, educated at, United Nations\nInternational School), (Joakim Noah, educated at, Poly\nPrep Country Day School), (Joakim Noah, sport,\nbasketball), (Joakim Noah, league, National Basketball\nAssociation), (Joakim Noah, league, NCAA Division I\nmen’s basketball), (Joakim Noah, member of sports\nteam, Florida Gators men’s basketball), (Joakim Noah,\nmember of sports team, Florida Gators men’s\nbasketball), (Joakim Noah, league, NBA G League)\n[University of\nFlorida]\nT5: Joakim Noah, college, University of\nFlorida\nOPT: University of Florida Question:\nwhat sport did joakim noah? Answer: bas-\nketball Question: what league did joakim\nnoah? Answer: NBA\nT0: University of Florida\nGPT-3: Joakim Noah attended the Univer-\nsity of Florida.\n7 where does tim\ncook work?\n(Tim Cook, occupation, businessperson), (Tim Cook,\noccupation, engineer), (Tim Cook, occupation,\ncomputer scientist), (Tim Cook, place of birth,\nMobile), (Tim Cook, occupation, chief executive\nofficer), (Tim Cook, described by source, Lentapedia),\n(Tim Cook, country of citizenship, United States of\nAmerica), (Tim Cook, employer, Apple Inc.), (Tim\nCook, residence, Palo Alto), (Tim Cook, educated at,\nFuqua School of Business)\n[Apple Inc.]\nT5: Tim Cook, educated at, Fuqua School\nof Business Question: where does tim\ncook work? Answer: Tim Cook, resi-\ndence, Palo Alto Question: where does\ntim cook work? Answer: Tim Cook, em-\nployer, Apple Inc.\nOPT: I think the answer is \"Apple Inc.\"\nT0: Apple Inc.\nGPT-3: Tim Cook works at Apple Inc.\n8\nwhat is the political\nsystem of south\nafrica?\n(South Africa, basic form of government,\nrepresentative democracy), (South Africa, basic form\nof government, parliamentary republic), (South Africa,\nDemocracy Index, quantity: +7.05 - unit: 1), (South\nAfrica, Democracy Index, quantity: +7.24 - unit: 1),\n(South Africa, Democracy Index, quantity: +7.24 -\nunit: 1), (South Africa, Democracy Index, quantity:\n+7.24 - unit: 1), (South Africa, legislative body,\nParliament of South Africa), (South Africa, executive\nbody, Government of South Africa), (South Africa,\noffice held by head of government, State President of\nSouth Africa), (South Africa, member of, Southern\nAfrican Development Community)\n[Constitutional\nrepublic,\nParliamentary\nrepublic]\nT5: Question: what is the political system\nof south africa? Answer: Answer: parlia-\nmentary republic\nOPT: South Africa is a parliamentary re-\npublic. Question: what is the capital of\nsouth africa? Answer: Pretoria is the capi-\ntal of South Africa. Question: what is the\ncurrency of south africa? Answer: South\nAfrican Rand. Question: what is the pop-\nulation of south africa? Answer: South\nAfrica has a population of about 50 mil-\nlion. Question: what is the official lan-\nguage of south africa? Answer: Afrikaans\nis the official language of South Africa.\nT0: parliamentary republic\nGPT-3: The political system of South\nAfrica is a parliamentary republic.\n9\nwhat record label is\nkelly clarkson\nsigned with?\n(Kelly Clarkson, record label, RCA Records), (Kelly\nClarkson, record label, Atlantic Records), (Kelly\nClarkson, record label, Syco Music), (Kelly Clarkson,\nrecord label, 19 Recordings), (Kelly Clarkson,\noccupation, record producer), (Kelly Clarkson,\noccupation, recording artist), (Kelly Clarkson,\noccupation, musician), (Kelly Clarkson, occupation,\nsinger), (Kelly Clarkson, genre, pop music), (Kelly\nClarkson, given name, Kelly)\n[RCA Records,\nSony BMG, Sony\nMusic\nEntertainment, 19\nRecordings]\nT5: Kelly Clarkson is a singer, song-\nwriter, and actress. She is signed to RCA\nRecords, a division of RCA Records.\nOPT: 19 Recordings Question: what is\nkelly clarkson’s first name? Answer: kelly\nQuestion: what is kelly clarkson’s last\nname? Answer: clarkson\nT0: RCA Records\nGPT-3: RCA Records\nContinued on the next page\n97\nTable 17 – Continued from the previous page\nIndex Question Retrieved Triples Answer Entities Generated Answers\n10\nwhat kind of\nproducts does nike\nsell?\n(Nike, product or material produced, sports\nequipment), (Nike, product or material produced,\nclothing), (Nike, product or material produced,\nfootwear), (Nike, industry, retail), (Nike, industry,\nclothing industry), (Nike, instance of, brand), (Nike,\nsubsidiary, Air Jordan product line), (Nike, industry,\nindustry), (Nike, Commons category, text: Nike, Inc.),\n(Nike, subsidiary, Converse)\n[Athletic Shoes\n& Apparel,\nFootwear\nManufacturing,\nFashion accessory,\nSports equipment,\nClothing,\nFootwear]\nT5: Nike, Inc. is a multinational corpora-\ntion headquartered in Beaverton, Oregon,\nUnited States. It is the world’s largest\nmanufacturer of athletic shoes and apparel,\nand the second-largest manufacturer of\nsports equipment. It is the world’s largest\nsportswear manufacturer, and the world’s\nlargest manufacturer of footwear for men,\nwomen and children.\nOPT: shoes\nT0: Footwear\nGPT-3: Nike sells products in the cate-\ngories of footwear, clothing, and sports\nequipment.\n98",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.7332252264022827
    },
    {
      "name": "Computer science",
      "score": 0.6643895506858826
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5697310566902161
    },
    {
      "name": "Knowledge graph",
      "score": 0.5571016073226929
    },
    {
      "name": "Graph",
      "score": 0.5541107058525085
    },
    {
      "name": "Task (project management)",
      "score": 0.5484408736228943
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.46667635440826416
    },
    {
      "name": "Domain knowledge",
      "score": 0.43345725536346436
    },
    {
      "name": "Natural language processing",
      "score": 0.42484933137893677
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4016949236392975
    },
    {
      "name": "Information retrieval",
      "score": 0.32564330101013184
    },
    {
      "name": "Theoretical computer science",
      "score": 0.18594446778297424
    },
    {
      "name": "Linguistics",
      "score": 0.15327408909797668
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}