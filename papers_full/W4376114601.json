{
  "title": "Weakly supervised detection and classification of basal cell carcinoma using graph-transformer on whole slide images",
  "url": "https://openalex.org/W4376114601",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5000305424",
      "name": "Filmon Yacob",
      "affiliations": [
        "AI Sweden"
      ]
    },
    {
      "id": "https://openalex.org/A5082124813",
      "name": "Jan Siarov",
      "affiliations": [
        "University of Gothenburg"
      ]
    },
    {
      "id": "https://openalex.org/A5024415188",
      "name": "Kajsa Villiamsson",
      "affiliations": [
        "University of Gothenburg"
      ]
    },
    {
      "id": "https://openalex.org/A5004695318",
      "name": "Juulia T. Suvilehto",
      "affiliations": [
        "Sahlgrenska University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5068520289",
      "name": "Lisa Sjöblom",
      "affiliations": [
        "Sahlgrenska University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5071711750",
      "name": "Magnus Kjellberg",
      "affiliations": [
        "Sahlgrenska University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5046975245",
      "name": "Noora Neittaanmäki",
      "affiliations": [
        "University of Gothenburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2041656630",
    "https://openalex.org/W3047478770",
    "https://openalex.org/W2803312959",
    "https://openalex.org/W4252865728",
    "https://openalex.org/W1979856281",
    "https://openalex.org/W2093742027",
    "https://openalex.org/W2936504617",
    "https://openalex.org/W2907285477",
    "https://openalex.org/W2605194029",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2943370629",
    "https://openalex.org/W2953048903",
    "https://openalex.org/W4282821867",
    "https://openalex.org/W4310273766",
    "https://openalex.org/W2894398812",
    "https://openalex.org/W2400484660",
    "https://openalex.org/W2956228567",
    "https://openalex.org/W2560886373",
    "https://openalex.org/W3126827997",
    "https://openalex.org/W3176719058",
    "https://openalex.org/W6629161641",
    "https://openalex.org/W3042024476",
    "https://openalex.org/W3207294617",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4318566866",
    "https://openalex.org/W3211647829",
    "https://openalex.org/W4210444392",
    "https://openalex.org/W2941548848",
    "https://openalex.org/W3104476988",
    "https://openalex.org/W2162489686",
    "https://openalex.org/W1951624856",
    "https://openalex.org/W1977653087",
    "https://openalex.org/W4205876996",
    "https://openalex.org/W3016224267",
    "https://openalex.org/W2002185973",
    "https://openalex.org/W4281257868"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports\nWeakly supervised detection \nand classification of basal cell \ncarcinoma using graph‑transformer \non whole slide images\nFilmon Yacob 1,2, Jan Siarov 3, Kajsa Villiamsson 3, Juulia T. Suvilehto 2, Lisa Sjöblom 2, \nMagnus Kjellberg 2 & Noora Neittaanmäki 3*\nThe high incidence rates of basal cell carcinoma (BCC) cause a significant burden at pathology \nlaboratories. The standard diagnostic process is time‑consuming and prone to inter‑pathologist \nvariability. Despite the application of deep learning approaches in grading of other cancer types, there \nis limited literature on the application of vision transformers to BCC on whole slide images (WSIs). \nA total of 1832 WSIs from 479 BCCs, divided into training and validation (1435 WSIs from 369 BCCs) \nand testing (397 WSIs from 110 BCCs) sets, were weakly annotated into four aggressivity subtypes. \nWe used a combination of a graph neural network and vision transformer to (1) detect the presence \nof tumor (two classes), (2) classify the tumor into low and high‑risk subtypes (three classes), and (3) \nclassify four aggressivity subtypes (five classes). Using an ensemble model comprised of the models \nfrom cross‑validation, accuracies of 93.5%, 86.4%, and 72% were achieved on two, three, and five \nclass classifications, respectively. These results show high accuracy in both tumor detection and \ngrading of BCCs. The use of automated WSI analysis could increase workflow efficiency.\nBasal cell carcinoma is the most common form of skin cancer in humans. The incidence is as high as the incidence \nof all other cancers  combined1. Further, the number of BCC cases is increasing  globally2–4. Although metastasis \nand death are rare, BCCs can cause significant morbidity due to aggressive and destructive local  growth5.\nBCCs are a heterogeneous group of tumors with different growth patterns. Internationally, BCCs are classified \ninto two broad categories based on histopathologic features: low-risk and high-risk  subtypes6. These categories \ncan be further classified in subclasses. Swedish pathologists, for example, classify BCCs according to the “Sab -\nbatsberg model” which includes three risk categories: (a) “low-aggressive” subtypes which are further divided \ninto superficial (type Ib) and nodular (type Ia), and (b) “medium-aggressive” (type II) which includes less \naggressive infiltrative subtypes that grow in a more well-defined manner and more superficially compared to \nthe high-aggressive tumors and (c) “high-aggressive” (type III), more aggressive, infiltrative and morphea form \n subtypes7. The correct assessment of the subtype is crucial for planning the relevant treatment. However, there \nis a significant inter-pathologist variability when grading  tumors8 and reporting the  subtype9,10.\nMoreover, given the time-consuming process of evaluating histological slides combined with an increasing \nnumber of samples delays diagnosis and increases  costs11. To reduce diagnosis time and inter-observer varia-\ntions, deep  learning12 approaches have been actively investigated. Deep learning enables the implementation \nof computational image analysis in pathology, which provides the potential to increase classification accuracy \nand reduce interobserver  variability13,14. Interestingly, even unknown morphological features associated with \nmetastatic risk, disease-free survival, and prognosis may be  revealed15,16.\nIn early research works computational histology methods required pixel-wise annotations, i.e., delineating \nspecific regions on WSI by  pathologists17. Using pixel-wise annotation, however, is time-consuming. Further, such \napproaches do not generalize to real-world  data18. As an alternative, a weakly supervised learning framework has \nbeen a widely adopted method for WSI classification. The common technique within weakly supervised learn -\ning is multi-instance learning (MIL)19. This approach can use WSI-level labels, i.e., labels not associated with a \nspecific region, without losing  performance20. The technique treats the set of instances (patches of a WSI) as a \nOPEN\n1AI Sweden, Gothenburg, Sweden. 2AI Competence Center, Sahlgrenska University Hospital, Gothenburg, \nSweden. 3Department of Laboratory Medicine, Institute of Biomedicine, Sahlgrenska Academy, University of \nGothenburg, Gothenburg, Sweden. *email: noora.neittaanmaki@fimnet.fi\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\nbag. The mere instance of a positive case patch makes the bag (WSI), positive, otherwise, it is treated as nega -\ntive. MIL requires that the WSI are partitioned into a set of patches, often without the need for data  curation18.\nThe later works have increasingly added a self-supervised contrastive learning paradigm in extracting better \nfeature vectors. In these paradigms pre-trained CNN models are tuned using a contrastive learning frame-\nwork in a contained  manner21. Adding these components into MIL approaches has proven to provide better \n performance22,23. However, the MIL framework fundamentally assumes the patches as independently and identi-\ncally distributed, neglecting the correlation among the  instances19,24. Neglecting the correlation affects the overall \nperformance of the classification models. Instead, the spatial correlation can be captured using the graph neural \nnetworks, which in turn increases model  performance25–27.\nRecently,  Transformers28 have made a great leap in the AI front by introducing the capability to incorporate \ncontext among a sequence of tokens in natural language processing tasks e.g. GPT-329. Inspired by the success of \ntransformers in natural language processing, Dosovitskiy et al.30 proposed Vision Transformer (ViT), a method \nfor image classification tasks that takes patches of an image as input. This enables capturing the sequence of \npatches (tokens) and considers the position of images (context) using positional embeddings. Consideration of \nthe positional relationship (contextual information) shows that ViT can perform better than CNN, especially \nwhen using features obtained from self-supervised contrastive models. In addition, vision transformers require \nsubstantially fewer data and compute resources relative to many CNN-based  approaches30,31. Further, the rela-\ntive resilience to noise, blur, artifacts, semantic changes, and out-of-distribution samples could contribute to \nbetter  performance32.\nIn medical images, transformers have been applied in image classification, segmentation, detection, recon -\nstruction, enhancement, and registration  tasks32. Specifically, in histological images, vision transformers have \nbeen successfully applied to different histological images related tasks, including in the detection of breast \ncancer metastases, and in the classification of cancer subtypes of lung, kidney and colorectal  cancer33,34. Given \nthe success of vision transformers in many medical applications and the capability of graph neural networks to \ncapture correlation among patches, we adopt the combination of graph neural networks and Transformers to \ndetect and classify BCCs.\nResults\nThe accuracies of the ensembles comprised of the 5 graph-transformer models on the test set were 93.5%, 86.4%, \nand 72.0% for the two-class, three-class, and five-class classification tasks, respectively. Moreover, the sensitivity \nof detecting healthy skin and tumors reached 96% and 91.9%, respectively. The performance of the ensemble \nmodels on the test set is summarized in Table 1 and the associated confusion matrices are shown in Fig. 1. Fig-\nure 2 shows the average ROC curve of the separate cross-validation models against the test set. Heatmaps were \nused to visualize the regions of WSI that are highly associated with the label. Figure  3 shows tumor regions of \ndifferent BCC subtypes that were correctly identified by a Graph-Transformer model.\nDiscussion\nIn this paper, we used a graph-transformer for the detection and classification of WSIs of extraction with BCC. \nThe developed deep-learning method showed high accuracy in both tumor detection and grading. The use of \nautomated image analysis could increase workflow efficiency. Given the high sensitivity in tumor detection, the \nmodel could assist pathologists in identifying the slides containing tumor and indicating the tumor regions on the \nslides and possibly reduce the time needed for the diagnostic process in daily practice. The use of high-accuracy \nautomated tumor grading could further save time and potentially reduce inter- and intra-pathologists’ variability.\nOur study is among the first to apply two and four grading of BCC on WSI using deep learning approaches. \nOur method reached high AUC values of 0.964–0.965, 0.932–0.975 and 0.843–0.976 on two, three (two grades), \nand five class (4 grades) classifications, respectively. Previously, Campanella et al. 18 used a significantly larger \ndataset of totally 44,732 WSIs including 9,962 slides with wide range of neoplastic and non-neoplastic skin \nlesions of which 1,659 were BCCs. They achieved high accuracy in tumor detection and suggested that up to \n75% of the slides could safely be removed from the workload of pathologists. Interestingly, Gao et al.35 compared \nTable 1.  Model performance with 1, 2, and 4 BCC grades on the test set based on an ensemble model \ncomprising of the 5 cross-validation model splits.\nTasks Sub-class Accuracy (%) Sensitivity (%) Specificity (%)\n2 classes (Task 1)\n0-Healthy skin\n93.5\n96.0 91.9\n1-Tumor 91.9 96.0\n3 classes (Task 2)\n0-Healthy skin\n86.4\n98.0 92.3\n1-Low risk 83.5 91.5\n2-High risk 76.2 96.1\n5 classes (Task 3)\n0-Healthy skin\n72.0\n98.0 89.4\n1-Superfial low 83.0 96.8\n2-Nodular low 64.0 93.1\n3-medium aggressive 31.6 94.0\n4-High aggressive 57.8 90.7\n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\nFigure 1.  Confusion matrices of the ensemble models for the three different classification tasks (T) on the test \nset. (a) binary classification (T1, tumor or no tumor), (b) three class classification (T2, no tumor and two grades \nof tumor), (c) five class classification (T3, no tumor and four grades of tumor).\nFigure 2.  Mean ROC curves of the five-fold cross-validation models based on a test set for the different \nclassification tasks (T). (a) binary classification (T1), (b) three class classification (T2), (c) five class classification \n(T3).\nFigure 3.  Visualization of class activation maps (rows 2 and 3) and corresponding H&E images (rows 1 and \n4). The class activation maps are built for the binary classification task (no tumor, tumor) with the areas of \ntumor emphasized. Representative examples are shown for all four BCC grades: (a) superficial low aggressive, \n(b) nodular low aggressive, (c) medium aggressive, (d) high aggressive. Rows 3 and 4 represent close up images \nfrom the areas marked with black boxes. The slides have been cropped to focus on the tissue after running the \nmodel.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\nWSIs and smartphone-captured microscopic ocular images of BCCs for tumor detection with high sensitivity \nand specificity for both approaches. However, no tumor grading was applied in these studies. To the best of our \nknowledge, there is no open-source dataset on grading of BCC. This makes it difficult to compare the results of \nthis work against a baseline. One advantage of our study is that data is available as an open data set which will \nenable progress in this area.\nIn another study regarding BCC detection attention patterns of AI were compared to attention patterns of \npathologists and observed that the neural networks distribute their attention over larger tissue areas integrat -\ning the connective tissue in its decision-making36. Our study used weakly supervised learning, where the labels \nwere assigned on a slide level. This approach instead of focusing on small pixelwise annotated areas, gives the \nalgorithm freedom to evaluate larger areas including the tumoral stroma. Furthermore, slide-wise annotation is \nsignificantly less time-consuming than pixel-wise annotations.\nA limitation of our study is the somewhat limited size of the dataset. As the number of classes increases the \nperformance reduced significantly. This could be attributed to a reduced number of WSI per class in the training \nset. For example, it was more difficult for the model to differentiate between the BCC subtype Ia and subtype \nIb in 5 class classification tasks but relatively easier to differentiate the low and high aggressive classes in 3 class \nclassification task, Fig. 2. With the availability of more data, the performance would most likely increase.\nEven though this work didn’t make systematic inter-observer variability analysis, the two pathologists involved \nin the annotation of the dataset into 4 different grades (5-class classification) differed in 6.7% of WSIs. The anno-\ntation of those WSIs were corrected with consensus along with a third senior pathologist, which is not the case in \nreal life situations. Using tools, such as the one proposed in this work, would likely reduce the inter-pathologist \nvariability. More studies on the subject are warranted.\nA limitation in our study is the imbalance in the dataset in different tasks. We included several (1–18 slides) \nper tumor. Each slide was classified individually. Even though we aimed to include as many WSIs in each tumor \ngroup there were differences between the groups. The more aggressive tumors were bigger and thus had more \nslides. Also the fact that within the same tumor several BCC subtypes were presented affected the number of \nthe WSIs in each group. Since we included several slides from the same tumor not all slides showed tumor. Thus \ntotally 744 included slides represented healthy skin as shown in Table  2. This caused imbalance in the dataset \nespecially in the task 2 and 3 were the largest group was the healthy skin. Furthermore, the fact that a few BCC \ncases did not show any tumor slides could be because of some slides needed to be removed due to low quality \nin the scanning.\nFurthermore, many of the WSIs had composite subtypes and these sometimes were present on the same slide. \nSuch cases are typical in BCC to have an admix from multiple types, i.e., cases with more than one pathologic \n pattern37. The proportion of mixed histology cases can reach up to 43% of all  cases38. Up to 70% of mixed BCC \ncases can contain one or more aggressive  subtypes39. Despite such characteristics of mixed pattern per WSI, our \nmodels were able to detect the worst BCC subtype per slide with an accuracy of 86.4% in the three-class clas-\nsification, and 72.0% in the five-class classification tasks, as shown in Table 1.\nFurther, each slide had pen marks that indicate extraction index (corresponding to extraction id) in which \nsome cases can be as large as the tissue on the WSI. Since the dataset is split based on a patient index, the pen \nmarks in the training set are different from that of test set, and the model is not affected by the similarities of the \nhandwritten characters. The pen marks were not identified as tissue by the tiler and were therefore not included in \nthe training patches. Moreover, the WSIs had different colors and artifacts, slice edges, inconsistencies, scattered \nTable 2.  The distribution of BCCs and WSIs into the different classes. BCCs were classified according to the \nWSI with the worst grade tumor belonging to that BCC. *These represent slides from BCC excisions which \nrepresented healthy skin.\nAll included\nTraining and validation \nset Hold-out test set\nNumber of cases WSIs Number of cases WSIs Number of cases WSIs\nTotal 479 1832 369 1435 110 397\n2 classes (Task 1)\n No tumor 4 745* 2 594 2 151\n Tumor 475 1087 367 841 108 246\n3 classes (Task 2)\n No tumor 4 745 2 594 2 151\n Low risk 219 506 178 403 41 103\n High risk 256 581 189 438 67 143\n5 classes (Task 3)\n No tumor 4 745 2 594 2 151\n Low aggressive superficial 81 230 63 177 18 53\n Low aggressive nodular 138 276 115 226 23 50\n Medium aggressive 138 294 98 215 40 79\n High aggressive 118 287 91 223 27 64\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\nsmall tissues, spots, and holes. Despite these variations among the WSIs, the models treated handwritten char -\nacters as background and other variations as noise.\nThis work, to the best of our knowledge, is the first approach that uses transformers in the grading of BCC \non WSI. The results show high accuracy in both tumor detection and grading of BCCs. Successful deployment \nof such approaches could likely increase the efficiency and robustness of histological diagnostic processes.\nMethods\nDataset. The dataset was retrospectively collected at Sahlgrenska University Hospital, Gothenburg, Sweden \nfrom the time period 2019–2020. The complete dataset contains 1831 labeled WSI from 479 BCC excisions (1 to \n18 glass slides per tumor), Table 2. The slides were scanned using a scanner NanoZoomer S360 Hamamatsu at \n40X magnification. The slide labels were then removed using an open-source package called anonymize-slide40.\nThe dimensions of the WSIs ranged from 71,424 to 207,360 px, with sizes ranging 1.1 GB to 5.3 GB (in total \n5.6 TB). Moreover, almost all samples had multiple sectioning levels per glass slide. Before scanning, the glass \nslides were marked with letter ‘B’ and up to 3 digits indicating which slides represented the same tumor.\nThe scanned slides were then annotated at WSI-level into 5 classes (no-tumor and 4 grades of BCC tumor), \nin accordance with the Swedish classification system. When several growth patterns of tumors were detected, the \nWSIs were classified according to the worst possible subtype. The annotations were performed by two patholo-\ngists separately. In the cases where the two main annotators had differing opinions (6.7% of WSIs), a third \nsenior pathologist was brought in, and a final annotation decision was made as a consensus between the three \npathologists.\nThe dataset was set out for use for 3 classification tasks. The first task (T1) was detecting the presence of \ntumors through binary classification (tumor or no tumor). The second task (T2) was classified into three classes \n(no tumor, low-risk and high-risk tumor, in line with WHO grading systems). The third task (T3) was classing \nthe dataset into 5 classes (no tumor, and 4 grades of BCC; low aggressive superficial, low aggressive nodular, \nmedium aggressive and highly aggressive, in line with the Swedish classification system). In the two-grade clas-\nsification tasks, the labels were converted to cases of low aggressive (Ia and Ib) and high aggressive (II and III). \nFigure 4 shows patches of BCCs and their corresponding classes in the three classification tasks (indicated as \nT1, T2, and T3).\nFeature extraction. An overview of the method is shown in Fig.  5. Given that WSIs were large, conven-\ntional machine-learning models could not ingest them directly. Hence the WSIs were first tiled into patches. \nThe WSIs were tiled into 224 by 224 patches at 10X magnification with no overlap using  OpenSlide41. The \npatches with at least 15% tissue areas were kept while others were discarded. The number of patches ranged from \n22–14,710 patches per WSI. In total, 5.2 million patches were generated for the training set. As stated above, \nthere was variability among the WSIs including color differences, artifacts, etc. Despite the differences among \npatches, no image processing was made before or after tiling.\nOnce the patches were tiled, features were extracted using a self-supervised learning framework,  SimCLR21. \nUsing a contrastive learning approach, the data was augmented, and sub-images were then used to generate \na generic representation of a dataset. The algorithm then reduced the distance between the same image and \nincreased the distance between different images (negative pairs) 21. In this step, using Resnet18 as a backbone \nand all patches as a training set, except the patches from the hold-out test set, a feature vector for each patch was \nextracted. For training SimCLR, Adam optimizer with weight decay of  10–6, and batch size of 512 and 32 epochs \nwere used. The initial learning rate  10–4 was scheduled using cosine annealing.\nGraph convolutional network construction. The features generated from self-supervised contrastive \nlearning were used to construct the graph neural networks. Using contrastive learning feature vectors of each \npatch were extracted. Since each patch is connected to the nearest neighbor patch by its edges and corners, til-\ning breaks the correlation among the patches. The correlation among patches is typically captured via positional \n embeddings30. Since histological patches are spatially correlated in a 2D space, the positional embeddings could \nbetter be captured via a graph  network27.\nA patch is connected to a neighboring patch by 4 sides and 4 corners, hence in total 8 edges. A set of 8-node \nadjacency matrices was used to create a graph representation of a WSI. Then the positional embedding captured \nvia the adjacency matrix is used to construct a graph convolutional network. The feature vectors of the patches \nbecame the nodes of the graphs.\nZheng et al.27 showed results of using a fully connected graph, that is, a single tissue per slide. In this work, \nwe show that the same approach works with a disconnected graph representing multiple tissues per WSI. It is \nworth noting that almost all WSIs in our dataset had multiple tissues per slide, i.e., there were no correlations \namong the separate tissues due to non-tissue regions. This results in a disconnected graph as shown in Fig. 6. It \nis worth noting that the distance between the components of the disconnected graph as well as their position in \nspace has no effect on the performance of the model.\nVision transformer. Once the graph convolutional network was built, the network was fed to a ViT. Gener-\nally, the transformer applies an attention mechanism that mimics the way humans extract important informa-\ntion from a specific image or text, ignoring the information surrounding the image or  text42. Self-attention28 \nintroduced a function that uses queries, keys, and values vectors, mapped from the input features. Using these \nvectors, it applies multiple-head self-attention to extract refined features, allowing it to understand the image as \na whole rather than just focusing on individual parts. Further, the self-attention function is accompanied by a \n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\nmultilayer perceptron (MLP) block which is used in determining classes. In this work, we used the standard ViT \nencoder architecture along with graph convolutional network for classification of BCC subtypes.\nMoreover, the computational cost of training ViT can be high depending on the input size. The number of \npatches can be large depending on the size of images and tissue size relative to the WSI. This resulted in a large \nnumber of nodes, which were computationally hard to be applied directly as input to the transformer. To reduce \nthe number of nodes to the extent that the ViT can digest the inputs, a pooling layer was added.\nTraining the graph‑transformer. There were 369 extractions (1435 WSIs) in the combined training \nand validation set. An additional dataset of 110 extractions (397 WSIs) were scanned separately to comprise a \nFigure 4.  Samples of BCC subtypes used in the three classification tasks (T): T1 (tumor or no tumor), T2 (no \ntumor and two grades of tumor), and T3 (no tumor and four grades of tumor), arranged by a pathologist in \naccordance with “Sabbatsberg model”7. Depending on the classification task at hand, the samples in each row \nare assigned a different grade of tumor.\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\nhold-out test set. The test set was handled separately and was held out from both SimCLR and graph-transformer \nmodels.\nFor training and validation, all slides relating to a specific extraction were always placed in the same set to \navoid data leakage from similar slides. This necessitated dividing the dataset on the extraction level, resulting in \nuneven splits for cross-validation. Hence, a fivefold cross-validation was used for training. The outputs of the 5 \nmodels from the cross-validation folds were combined into one ensemble model through majority vote to provide \nfinal predictions against the test set. This step was performed for the two-, three-, and five-class classification \ntasks separately, Supplementary Table S1.\nIn training the models, the same hyperparameters were used for all the tasks. The models were configured \nwith MLP size of 128, 3 self-attention blocks, and trained with batch size 4, 100 epochs and Adam optimizer’s \nweight decay  10–5, learning rate  10–3 with decay at steps 40 and 80 by  10–1. The training was performed on 2 GPUs \non DGX A100. The training of SimCLR model took around 3 days. The training for graph transformers took \naround 25 min on average to converge. For a given WSI in the test set, from tiling to inference, took around 30 s.\nVisualization. To visualize and interpret the predicted results, a graph-based class activation  mapping27 was \nused. The method computed the class activation map from the class label to a graph representation of the WSI by \nutilizing precomputed transformer and graph relevance maps. Using the method, heatmaps were overlayed on \nregions of the WSI associated with the WSI label.\nFigure 5.  Method overview (adapted from Zheng et al.27). The WSI is first tiled into patches and feature \nextracted via self-supervised learning. The extracted features become the nodes of a graph network, which \nbecome the inputs to a graph-transformer classifier.\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\nData availability\nThe datasets generated and/or analysed during the current study are available at https:// doi. org/ 10. 23698/ aida/ \nbccc.\nReceived: 20 January 2023; Accepted: 20 April 2023\nReferences\n 1. Levell, N. J., Igali, L., Wright, K. A. & Greenberg, D. C. Basal cell carcinoma epidemiology in the UK: The elephant in the room. \nClin. Exp. Dermatol. 38, 367–369 (2013).\n 2. Dika, E. et al. Basal cell carcinoma: A comprehensive review. Int. J. Mol. Sci. 21, 5572 (2020).\n 3. Cameron, M. C. et al. Basal cell carcinoma. J. Am. Acad. Dermatol. 80, 321–339 (2019).\n 4. Wong, C. S. M. Basal cell carcinoma. BMJ 327, 794–798 (2003).\n 5. Lo, J. S. et al. Metastatic basal cell carcinoma: Report of twelve cases with a review of the literature. J. Am. Acad. Dermatol. 24, \n715–719 (1991).\n 6. Elder, D. E., Massi, D., Scolyer, R. A. & Willemze, R. WHO Classification of Skin Tumours 4th edn. (WHO, Berlin, 2018).\n 7. Jernbeck, J., Glaumann, B. & Glas, J. E. Basal cell carcinoma. Clinical evaluation of the histological grading of aggressive types of \ncancer]. Lakartidningen 85, 3467–70 (1988).\n 8. Jagdeo, J., Weinstock, M. A., Piepkorn, M. & Bingham, S. F . Reliability of the histopathologic diagnosis of keratinocyte carcinomas. \nJ. Am. Acad. Dermatol. 57, 279–284 (2007).\n 9. Moon, D. J. et al. Variance of basal cell carcinoma subtype reporting by practice setting. JAMA Dermatol. 155, 854 (2019).\n 10. Al-Qarqaz, F . et al. Basal cell carcinoma pathology requests and reports are lacking important information. J. Skin Cancer  2019, \n1–5 (2019).\n 11. Migden, M. et al. Burden and treatment patterns of advanced basal cell carcinoma among commercially insured patients in a \nUnited States database from 2010 to 2014. J. Am. Acad. Dermatol. 77, 55-62.e3 (2017).\n 12. LeCun, Y ., Bengio, Y . & Hinton, G. Deep learning. Nature 521, 436–444 (2015).\n 13. Niazi, M. K. K., Parwani, A. V . & Gurcan, M. N. Digital pathology and artificial intelligence. Lancet Oncol. 20, e253–e261 (2019).\n 14. Komura, D. & Ishikawa, S. Machine learning approaches for pathologic diagnosis. Virchows Arch. 475, 131–138 (2019).\n 15. Knuutila, J. S. et al. Identification of metastatic primary cutaneous squamous cell carcinoma utilizing artificial intelligence analysis \nof whole slide images. Sci. Rep. 12, 1–14 (2022).\n 16. Comes, M. C. et al. A deep learning model based on whole slide images to predict disease-free survival in cutaneous melanoma \npatients. Sci. Rep. 12, 20366 (2022).\nFigure 6.  An example of a WSI and its graph network. (a) WSI with six tissue sections, (b) six disconnected \ncomponents of a graph network. The disconnected components are randomly placed in space. Each node \nrepresents a patch (patches not shown in the figure for better visualization).\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\n 17. Olsen, T. G. et al. Diagnostic performance of deep learning algorithms applied to three common diagnoses in dermatopathology. \nJ. Pathol. Inform. 9, 32 (2018).\n 18. Campanella, G. et al. Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. Nat. \nMed. 25, 1301–1309 (2019).\n 19. Carbonneau, M.-A., Cheplygina, V ., Granger, E. & Gagnon, G. Multiple instance learning: A survey of problem characteristics and \napplications. Pattern Recogn. 77, 329–353 (2018).\n 20. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple instance learning. In International Conference on Machine \nLearning 2127–2136 (2018).\n 21. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple framework for contrastive learning of visual representations. In Inter-\nnational Conference on Machine Learning 1597–1607 (PMLR, 2020).\n 22. Li, J. et al. A multi-resolution model for histopathology image classification and localization with multiple instance learning. \nComput. Biol. Med. 131, 104253 (2021).\n 23. Li, B., Li, Y . & Eliceiri, K. W . Dual-stream multiple instance learning network for whole slide image classification with self-\nsupervised contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) \n14318–14328 (2021).\n 24. Zhou, Z. H. & Xu, J. M. On the relation between multi-instance learning and semi-supervised learning. ACM Int. Conf. Proc. Ser. \n227, 1167–1174 (2007).\n 25. Tu, M., Huang, J., He, X. & Zhou, B. Multiple instance learning with graph neural networks. arXiv preprint arXiv: 1906. 04881 (2019).\n 26. Adnan, M., Kalra, S. & Tizhoosh, H. R. Representation learning of histopathology images using graph neural networks. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops 988–989 (2020).\n 27. Zheng, Y . et al. A graph-transformer for whole slide image classification. arxiv.org (2022).\n 28. Vaswani, A. et al. Attention is all you need. Advances in Neural Information Processing Systems 30 (NIPS) (2017).\n 29. Brown, T. B. et al. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (NeurIPS) (2020).\n 30. Dosovitskiy, A. et al. An image is worth 16x16 Words: Transformers for image recognition at scale. arxiv.org (2020).\n 31. Deininger, L. et al. A comparative study between vision transformers and CNNs in digital pathology. arxiv.org (2022).\n 32. Li, J. et al. Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and \nfuture perspectives. arxiv.org (2022).\n 33. Shao, Z. et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. In proceed-\nings.neurips.cc (2021).\n 34. Zeid, M. A.-E., El-Bahnasy, K. & Abo-Y oussef, S. E. Multiclass colorectal cancer histology images classification using vision \ntransformers. In 2021 Tenth International Conference on Intelligent Computing and Information Systems (ICICIS) 224–230 (IEEE, \n2021). https:// doi. org/ 10. 1109/ ICICI S52592. 2021. 96941 25.\n 35. Jiang, Y . Q. et al. Recognizing basal cell carcinoma on smartphone-captured digital histopathology images with a deep neural \nnetwork. Br. J. Dermatol. 182, 754–762 (2020).\n 36. Kimeswenger, S. et al. Artificial neural networks and pathologists recognize basal cell carcinomas based on different histological \npatterns. Mod. Pathol. 34, 895–903 (2021).\n 37. Crowson, A. N. Basal cell carcinoma: Biology, morphology and clinical implications. Mod. Pathol. 19, S127–S147 (2006).\n 38. Cohen, P . R., Schulze, K. E. & Nelson, B. R. Basal cell carcinoma with mixed histology: A possible pathogenesis for recurrent skin \ncancer. Dermatol. Surg. 32, 542–551 (2006).\n 39. Kamyab-Hesari, K. et al. Diagnostic accuracy of punch biopsy in subtyping basal cell carcinoma. Wiley Online Library 28, 250–253 \n(2014).\n 40. Gilbert, B. Anonymize-slide. https:// github. com/ bgilb ert/ anony mize- slide.\n 41. Goode, A., Gilbert, B., Harkes, J., Jukic, D. & Satyanarayanan, M. OpenSlide: A vendor-neutral software foundation for digital \npathology. J. Pathol. Inform. 4, 27 (2013).\n 42. Bahdanau, D., Cho, K. H. & Bengio, Y . Neural machine translation by jointly learning to align and translate. 3rd International \nConference on Learning Representations, ICLR 2015: Conference Track Proceedings (2015).\nAcknowledgements\nThe study was financed by grants from the Swedish state under the agreement between the Swedish government \nand the county councils, the ALF-agreement (Grant ALFGBG-973455).\nAuthor contributions\nConception and design: F .Y ., J.S., K.V ., J.T.S., N.N. Development of methodology: F .Y ., J.T.S. Acquisition of data: \nJ.S., K.V ., N.N. Annotating the dataset: J.S., K.V ., N.N., Analysis and interpretation of data: F .Y ., J.T.S., J.S., K.V ., \nN.N. Writing, review, and revision of the manuscript: F .Y ., J.S., K.V ., J.T.S., N.N., L.S., Study supervision: N.N., \nJ.T.S., M.K. Acquisition of funding: N.N., M.K.\nFunding\nOpen access funding provided by University of Gothenburg.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 023- 33863-z.\nCorrespondence and requests for materials should be addressed to N.N.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:7555  | https://doi.org/10.1038/s41598-023-33863-z\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023, corrected publication 2023",
  "topic": "Grading (engineering)",
  "concepts": [
    {
      "name": "Grading (engineering)",
      "score": 0.7645626664161682
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6602784395217896
    },
    {
      "name": "Workflow",
      "score": 0.5865718722343445
    },
    {
      "name": "Computer science",
      "score": 0.5759926438331604
    },
    {
      "name": "Basal cell",
      "score": 0.5040675401687622
    },
    {
      "name": "Basal cell carcinoma",
      "score": 0.45645803213119507
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3863353729248047
    },
    {
      "name": "Machine learning",
      "score": 0.34817230701446533
    },
    {
      "name": "Pathology",
      "score": 0.2772524058818817
    },
    {
      "name": "Medicine",
      "score": 0.27572160959243774
    },
    {
      "name": "Biology",
      "score": 0.13231873512268066
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387152838",
      "name": "AI Sweden",
      "country": null
    },
    {
      "id": "https://openalex.org/I881427289",
      "name": "University of Gothenburg",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I2801112325",
      "name": "Sahlgrenska University Hospital",
      "country": "SE"
    }
  ]
}