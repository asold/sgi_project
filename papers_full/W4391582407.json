{
  "title": "Revolutionizing Cyber Threat Detection With Large Language Models: A Privacy-Preserving BERT-Based Lightweight Model for IoT/IIoT Devices",
  "url": "https://openalex.org/W4391582407",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2597182638",
      "name": "Mohamed Amine Ferrag",
      "affiliations": [
        "Technology Innovation Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4321418996",
      "name": "Mthandazo Ndhlovu",
      "affiliations": [
        "Technology Innovation Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2004499786",
      "name": "Norbert Tihanyi",
      "affiliations": [
        "Technology Innovation Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2117275544",
      "name": "Lucas C. Cordeiro",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A1224291158",
      "name": "Merouane Debbah",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2129639951",
      "name": "Thierry Lestable",
      "affiliations": [
        "Technology Innovation Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5093874303",
      "name": "Narinderjit Singh Thandi",
      "affiliations": [
        "Technology Innovation Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4378365142",
    "https://openalex.org/W4313648836",
    "https://openalex.org/W4311165836",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4205635956",
    "https://openalex.org/W4285115684",
    "https://openalex.org/W4309623083",
    "https://openalex.org/W4226319939",
    "https://openalex.org/W4317600390",
    "https://openalex.org/W4319079731",
    "https://openalex.org/W4226346873",
    "https://openalex.org/W3217476834",
    "https://openalex.org/W4312581133",
    "https://openalex.org/W4387501456",
    "https://openalex.org/W4312156850",
    "https://openalex.org/W4220886040",
    "https://openalex.org/W4327893913",
    "https://openalex.org/W4387298393",
    "https://openalex.org/W4294405846",
    "https://openalex.org/W4320002857",
    "https://openalex.org/W4367663129",
    "https://openalex.org/W4314446179",
    "https://openalex.org/W4376616730",
    "https://openalex.org/W4362683503",
    "https://openalex.org/W4323312549",
    "https://openalex.org/W4315642187",
    "https://openalex.org/W2991507433",
    "https://openalex.org/W4384834994",
    "https://openalex.org/W6768817161",
    "https://openalex.org/W6841387716",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3006197067",
    "https://openalex.org/W4386951843",
    "https://openalex.org/W4386825138",
    "https://openalex.org/W4323349130",
    "https://openalex.org/W4313413509",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3178985214",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4290857278"
  ],
  "abstract": "The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining PPFLE with the Byte-level Byte-Pair Encoder (BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms traditional Machine Learning (ML) and Deep Learning (DL) methods, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), in cyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, our experimental analysis shows that SecurityBERT achieved an impressive 98.2&#x0025; overall accuracy in identifying fourteen distinct attack types, surpassing previous records set by hybrid solutions such as GAN-Transformer-based architectures and CNN-LSTM models. With an inference time of less than 0.15 seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT is ideally suited for real-life traffic analysis and a suitable choice for deployment on resource-constrained IoT devices.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nRevolutionizing Cyber Threat Detection\nwith Large Language Models: A\nprivacy-preserving BERT-based\nLightweight Model for IoT/IIoT Devices\nMOHAMED AMINE FERRAG1 (Senior Member, IEEE), MTHANDAZO NDHLOVU1, NORBERT\nTIHANYI1, (Member, IEEE), LUCAS C. CORDEIRO,2 MEROUANE DEBBAH3 (Fellow, IEEE),\nTHIERRY LESTABLE,1 NARINDERJIT SINGH THANDI1\n1Technology Innovation Institute, 9639 Masdar City, Abu Dhabi, UAE (e-mail: firstname.lastname@tii.ae)\n2The University of Manchester, Manchester, UK and Federal University of Amazonas, Manaus, Brazil (e-mail: lucas.cordeiro@manchester.ac.uk)\n3KU 6G Research Center, Khalifa University of Science and Technology, P O Box 127788, Abu Dhabi, UAE (email: merouane.debbah@ku.ac.ae)\nABSTRACT The field of Natural Language Processing (NLP) is currently undergoing a revolutionary\ntransformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreak-\ning Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the\nimportance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting\nin a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks\nwith both high precision and minimal computational requirements. This paper presents SecurityBERT, a\nnovel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT)\nmodel for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated\na novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE).\nWe effectively represented network traffic data in a structured format by combining PPFLE with the Byte-\nlevel Byte-Pair Encoder (BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms\ntraditional Machine Learning (ML) and Deep Learning (DL) methods, such as Convolutional Neural\nNetworks (CNNs) or Recurrent Neural Networks (RNNs), in cyber threat detection. Employing the Edge-\nIIoTset cybersecurity dataset, our experimental analysis shows that SecurityBERT achieved an impressive\n98.2% overall accuracy in identifying fourteen distinct attack types, surpassing previous records set by\nhybrid solutions such as GAN-Transformer-based architectures and CNN-LSTM models. With an inference\ntime of less than 0.15 seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT\nis ideally suited for real-life traffic analysis and a suitable choice for deployment on resource-constrained\nIoT devices.\nINDEX TERMS Cyber Threat Detection, IoT Networks, Generative AI, BERT, Large Language Models.\nI. INTRODUCTION\nAccording to a Statista report [1], it is projected that the\nglobal number of Internet of Things (IoT) connected devices\ncould potentially reach 30 billion by the year 2030. With the\nrise in the number of IoT devices, there is also a growing in-\ncidence of cyber threats, posing substantial challenges to the\nsecurity of diverse systems and networks [2]. As adversaries\nconsistently evolve their tactics, the need for advanced and\neffective detection mechanisms becomes paramount. Manual\ndetection methods and conventional approaches are becom-\ning outdated, and various Machine Learning (ML) techniques\nhave emerged, combating these new threats more effec-\ntively. In this context, Natural Language Processing (NLP)\ntechniques are gaining attention as a promising approach\nfor cyber threat detection [3]. Among these techniques, the\nBidirectional Encoder Representations from Transformers\n(BERT) model [4], a pre-trained transformer-based language\nmodel, has achieved remarkable success in several NLP ap-\nVOLUME 1, 2024 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nplications. By exploiting BERT’s contextual understanding,\nsecurity researchers have found unique techniques to handle\ndiverse cybersecurity concerns [5]. Researchers have recently\nbeen exploring using BERT and pre-trained language mod-\nels in a wide range of cybersecurity applications, including\nmalware detection in Android applications, identification of\nspam emails, intrusion detection in automotive systems, and\nanomaly detection in system logs [6]–[8]. Network-based\ntraffic, such as port scans and packet floods, primarily con-\nsists of numerical data rather than textual information. This\ncharacteristic poses a challenge when attempting to leverage\nmodels like BERT to understand the semantic relationships\nbetween various types of network packets. While employ-\ning complex Large Language Models (LLMs) with billions\nof parameters can improve threat detection accuracy, their\nextensive computational needs render them impractical for\nimplementation in embedded devices.\nWe presentSecurityBERT, a novel lightweight privacy-\npreserving architecture for cyber threat detection in IoT\nnetworks. By employing a dedicated encoding technique\ndesigned for this specific purpose, we surpassed the perfor-\nmance of all existing ML algorithms and models in cyber\nthreat detection. During the design of SecurityBERT, we\nhad three primary goals in mind:\n• To create an exceptionally compact model capable of\nexecuting rapid inferences without causing noticeable\ndelays. This design choice enables real-time traffic anal-\nysis and facilitates embedding the model in IoT devices;\n• To maintain the confidentiality of the extracted network\ndata, ensuring that classification can be performed on\nuntrusted servers;\n• To surpass the accuracy levels of previous ML models\nin this field.\nAchieving superior accuracy compared to existing hybrid so-\nlutions has proven a significant challenge in our architectural\ndesign. Striking the right balance is crucial. If the architecture\nbecomes overly complex, it may become impractical for real-\nlife traffic analysis. Conversely, if the model is overly sim-\nplistic, it may not provide the necessary accuracy for effective\nmulti-classification, thus hindering its overall performance.\nOur original contributions are as follows:\n• Our research introduces a novel privacy-preserving\nencoding approach called Privacy-Preserving Fixed-\nLength Encoding (PPFLE). By combining PPFLE with\nthe Byte-level Byte-Pair Encoder (BBPE) tokenizer,\nwe can effectively represent network traffic data in a\nstructured manner. By implementing this technique, we\nhave achieved significant performance improvements\ncompared to using text data with varying sizes;\n• We have designed a 15-layer BERT-based architecture\nwith only 11 million parameters for multi-category clas-\nsification. We trained the model on PPFLE encoded\ndata, which we refer to as SecurityBERT;\n• We evaluated the efficiency of our proposed approach\nusing the Edge-IIoTset cyber security dataset [9]. Var-\nious ML techniques have recently been tested on this\ndataset, providing a solid foundation for fair com-\nparison. According to our experimental analysis, our\nmethod effectively identifies fourteen distinct types of\nattacks on an average CPU in less than 0.3 seconds,\nachieving an overall accuracy of 98.2%. To the best of\nour knowledge, this achievement showcases the high-\nest accuracy ever attained among all ML algorithms,\noutperforming both the Convolutional Neural Network\n(CNN) and Transformer models.\nThis paper is organized as follows: Section II presents\nan exploration of the related work. Subsequently, Sec-\ntion III outlines the significant steps in developing\nSecurityBERT. In Section IV, we evaluate the perfor-\nmance of the proposed model. Lastly, we conclude our\nresearch and provide insight into potential future research\ndirections of interest in Section V.\nII. RELATED WORK\nAs various researchers have already demonstrated, the BERT\nmodel proves to be an exceptional starting point for identify-\ning cybersecurity threats. BERT has been utilized in various\nfields, from detecting log anomalies to identifying malicious\nweb requests.\nA noteworthy study by Alkhatib et al. [10] demonstrated\nthe feasibility of using BERT for learning the sequence of\narbitration identifiers (IDs) in a Controller Area Network\n(CAN) via a “masked language model” unsupervised training\nobjective. They proposed the CAN-BERT transformer model\nfor anomaly detection in current automotive systems and\nshowed that the BERT model outperforms its predecessors\nregarding accuracy and F1-score. Rahali et al. [6] introduced\nMalBERT, a tool that conducts static analysis on the source\ncode of Android applications. They used BERT to compre-\nhend the contextual relationships of code words and classify\nthem into representative malware categories. Their results\nfurther underscored the high performance of transformer-\nbased models in malicious software detection.\nChen et al. [8] introduced BERT-Log, an anomaly de-\ntection and fault diagnosis approach in large-scale com-\nputer systems that treat system logs as natural language\nsequences. They leveraged a pre-trained BERT model to\nlearn the semantic representation of normal and anomalous\nlogs, fine-tuning the model with a fully connected neural\nnetwork to detect abnormalities. Seyyar et al. [7] proposed\na model for detecting anomalous HTTP requests in web\napplications, employing Deep Learning (DL) techniques and\nBERT. Aghaeiet al. [11] presented SecureBERT1, a language\nmodel tailored explicitly for cybersecurity tasks, focusing\non Cyber Threat Intelligence (CTI) and automation. The\nSecureBERT model offers a practical way of transforming\nnatural language CTI into machine-readable formats, thereby\nminimizing the necessity for labor-intensive manual analysis.\n1While the names may sound similar, it is important to note that Secure-\nBERT is separate from our recently introduced SecurityBERT.\n2 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nThe authors devised a unique tokenizer and a method for\nadjusting pre-trained weights to ensure that SecureBERT\nunderstands general English and cybersecurity-related text.\nHowever, SecureBERT is not designed to process network-\nbased cyber threat attacks.\nCyBERT, introduced by Ranade et al. [12], is a custom\nversion of BERT designed specifically for cybersecurity ap-\nplications. This model has been fine-tuned using a vast corpus\nof cybersecurity data to enhance its ability to process intricate\ninformation concerning threats, attacks, and vulnerabilities.\nK. Yu et al. [13] explored a deep-learning-based approach for\ndetecting advanced persistent threats (APTs) in the Industrial\nInternet of Things (IIoT), using the BERT model to address\nthe challenges of long attack sequences. Their experimental\nresults demonstrate the method’s effectiveness, yielding high\naccuracy and a low false alarm rate in APT detection. B.\nBreve et al. [14] proposed using NLP techniques, specifically\na BERT-based model, to detect potentially harmful automa-\ntion rules in trigger-action IoT platforms that could breach\nuser security or privacy. Their evaluation on the If-This-\nThen-That platform with over 76,000 rules demonstrated\nhigh accuracy, significantly outperforming traditional infor-\nmation flow analysis methods. Recently Z. Wang et al. [15]\ndeveloped BERT-of-Theseus, Vision Transformer, and Pool-\nFormer (BT-TPF) , an IoT intrusion detection model tailored\nfor resource-limited IoT environments, using a knowledge-\ndistillation approach. The model employs a Siamese net-\nwork for feature reduction and a Vision Transformer to\ntrain a compact Poolformer model, achieving a significant\nparameter reduction while maintaining high accuracy. The\naforementioned studies leverage pre-trained BERT models\nand customize them to meet their unique security needs by\nfine-tuning or using them as feature generators. These models\nbenefit from the textual form and sequential nature of their\nsecurity-related data, including sources such as code, emails,\nand log sequences. These studies effectively utilize BERT’s\nability to comprehend contextual relationships within se-\nquences to carry out precise detection and classification tasks.\nIn cyber threat detection, it is vital to compare different\nresearch efforts. In real-world cyber threat detection scenar-\nios, support is crucial for extracting features from network\ntraffic, often relying on PCAP files. In addition to analyzing\nreal packet data and detecting cyber threats on networks, it\nis important to consider privacy in training data, especially\nsince IoT devices and network data may contain sensitive\ninformation. Given the uniqueness of each network infras-\ntructure and the need for high accuracy through fine-tuning\nor new training, sharing actual network traffic data for train-\ning purposes can raise privacy concerns. SecurityBERT\nhas been developed as a pioneering, lightweight, privacy-\npreserving architecture specifically designed with this con-\nsideration in mind. TABLE 1 provides a comparison of\nvarious recent works on cyber threat detection in terms of\nfour key parameters:\n• D = Detect:Network-based Cyber Threat Detection\n• L = LLM:Utilization of LLMs\n• N = Network PCAP:Packet data analysis of a traffic\n• P = Privacy:Privacy-preserving training data\nTABLE 1. Comparison with recent works on cyber threat detection\nFrameworks Year D L N P\nAlkhatib et al. [10] 2022 ✗ ✓ ✗ ✗\nRahali et al. [6] 2022 ✗ ✓ ✗ ✗\nAghaei et al. [11] 2022 ✗ ✓ ✗ ✗\nThapa et al. [4] 2022 ✗ ✓ ✗ ✗\nHamouda et al. [16] 2022 ✓ ✗ ✓ ✗\nFriha et al. [17] 2022 ✓ ✗ ✓ ✗\nChen et al. [8] 2022 ✗ ✓ ✗ ✗\nSeyyar et al. [7] 2022 ✗ ✓ ✗ ✗\nSelvaraja et al. [18] 2023 ✓ ✗ ✓ ✗\nB. Breve et al. [14] 2023 ✗ ✓ ✗ ✗\nChen et al. [19] 2023 ✗ ✓ ✗ ✗\nDouiba et al. [20] 2023 ✓ ✗ ✓ ✗\nJahangir et al. [21] 2023 ✓ ✗ ✓ ✗\nHu et al. [22] 2023 ✓ ✗ ✓ ✗\nK. Yu et al. [13] 2023 ✓ ✓ ✗ ✗\nHu et al. [22] 2023 ✓ ✗ ✓ ✗\nFriha et al. [23] 2023 ✓ ✗ ✓ ✗\nChakraborty et al. [24] 2023 ✓ ✗ ✓ ✗\nWang et al. [25] 2023 ✓ ✗ ✓ ✗\nLiu et al. [26] 2023 ✓ ✗ ✓ ✗\nAouedi et al. [27] 2023 ✓ ✗ ✓ ✗\nZ. Wang et al. [15] 2023 ✓ ✓ ✓ ✗\nSecurityBERT 2023 ✓ ✓ ✓ ✓\n✓: Supported, ✗: Not Supported.\nThe majority of the research conducted in 2022, including\n[6], [11], and [10], integrated LLMs, but they did not support\ndetection, nor did they utilize packet data. However, contrary\nto the norm, Hamouda et al. [16] (2022) and Friha et al. [17]\n(2022) demonstrated support for cyber threat detection and\nutilized packet data but did not rely on the capabilities\nof LLMs. Works from 2023, such as [18], [20], and [25],\nemphasize more cyber threat detection and the use of packet\ndata, but they largely lack in the application of LLMs.\nIII. SECURITYBERT ARCHITECTURE DESIGN\nThe SecurityBERT architecture comprises four essen-\ntial components. Firstly, it incorporates Feature Extrac-\ntion to extract relevant information. Secondly, it em-\nploys a novel encoding technique called Privacy-Preserving\nFixed-Length Encoding (PPFLE), specifically designed for\nSecurityBERT. Thirdly, it utilizes the Byte-level Byte-\nPair Encoder (BBPE) Tokenizer. Finally, the BERT encoder\nand Softmax classifier are integrated into the system. By\ncombining PPFLE with BBPE Tokenizer, text traffic data can\nbe represented in a structured and effective manner for further\nprocessing. Within the framework of SecurityBERT, the\nfinal stage involves attack classification, and this data can be\nintegrated into various intrusion detection systems to improve\nthe accuracy of pre-existing systems.\nFIGURE 1 visually presents the comprehensive workflow\nof the model, encompassing all relevant steps from dataset\npreparation to classification. Each of these steps will be\nextensively covered in this section. Developing a BERT\nmodel from the ground up for network-based cyber threat\nVOLUME 1, 2024 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\ndbaf efb ejf447\nTokenized Trafic Data\nByteLevelBPETokenizer\nToken1Token2Token3Token4\nText Data Input (PPFLE output)\nTextual Representation\nusing PPFLE\nOriginal Network Traffic  data\n1) Dataset Preparation 3) Model Training\nBERT Encoder\nContextuel Representation\nBERT Embedding\nFeature extraction\n2) Dataset Tokenization        4) Classification\nA Pre-trained Language Model (SecurityBERT)\nFeatures\n Category\n1AE4Fd067EC23AB\nRansomware\nAEC89021D56FA231\n Normal\nDDoS UDP\nDDoS ICMP\nDDoS TCP\nNormal\nUploading\nPassword\nBackdoor\nPort Scan\nXSS\nRansomware\nDDoS HTTP\nMITM\nFingerprint\nSQL injection\nVuln Scan\nSoftmax Classifier\nFIGURE 1. High-level workflow of our SecurityBERT model.\ndetection demands a thorough and intricate approach. Below\nis a comprehensive outline detailing the main steps in the\nprocess:\nSTEPS 1Main steps of building SecurityBERT\n1: Dataset Utilization\n2: Feature Extraction\n3: Privacy-Preserving Fixed-Length Encoding (PPFLE)\n4: Byte-level BPE (BBPE) Tokenizer\n5: SecurityBERT Embedding\n6: Contextual Representation\n7: Training SecurityBERT\n• Text Normalization\n• Text Tokenization\n• Frequency Filtering\n• V ocabulary Creation\n• Special Token Addition\n• Tokenizer Training\n8: Fine-tuning with Softmax activation function\nA. DATASET UTILIZATION (EDGE-IIOTSET DATASET)\nGenerating our dataset through real-life traffic analysis would\nbe time-consuming, and there’s the risk of specific attacks\nnot being adequately simulated or missing from our dataset.\nHence, acquiring and utilizing realistic datasets for our re-\nsearch is crucial.\nCybersecurity and network security data can be gathered\nfrom various online sources using open-source databases\nand repositories. Notable examples include the Common\nVulnerabilities and Exposures (CVE) database, the Open\nWeb Application Security Project (OW ASP), and numerous\nothers for network security [28]. The primary challenge\npresented by these sources is their heavy reliance on artificial\nscenarios, which results in a deficiency of authentic data.\nTraining a model exclusively on such data can potentially\nlead to unrealistic outcomes. Furthermore, most of these\ndatabases do not include packet network data, which poses\na challenge in simulating realistic scenarios. Our primary\naim is to opt for a dataset that tackles this constraint by\nstrongly emphasizing genuine network data. Furthermore, we\nintend to ensure maximum diversity within this dataset, en-\ncompassing a comprehensive range of attack types, including\nransomware, XSS, SQL injection, DoS, and other widely rec-\nognized attack categories. This diversified dataset’s rationale\nis to comprehensively assess the classification capabilities of\nour newly proposed model. In 2022, Ferrag et al. introduced\nEdge-IIoTset [9], a new and extensive cybersecurity dataset\nspecifically designed for IoT and IIoT applications. This\ndataset serves as a valuable resource for ML-based intrusion\ndetection systems. The Edge-IIoTset dataset includes diverse\ndevices, sensors, protocols, and cloud/edge configurations,\nrendering it highly representative of real-world scenarios and\naligning perfectly with our research objectives. This dataset\ncontains fifteen (15) attacks related to the Internet of Things\n(IoT) and Industrial IoT (IIoT) connectivity protocols, cat-\negorized into five threats: DoS/DDoS attacks, Information\ngathering, Man-in-the-middle (MITM), Injection attacks, and\nMalware attacks, which can be seen in Figure 2. The DoS/D-\nDoS attack category encompasses TCP SYN Flood, UDP\nflood, HTTP flood, and ICMP flood attacks. The Informa-\ntion Gathering category includes attacks like port scanning,\noperating system fingerprinting, and vulnerability scanning.\nMITM attacks include tactics such as DNS Spoofing and\nARP Spoofing. Injection attacks include Cross-Site Scripting\n(XSS), SQL injection, and file-uploading attacks. Lastly, the\nMalware category covers backdoors, password crackers, and\nransomware attacks.\nB. FEATURES EXTRACTION\nGiven a PCAP file with a network traffic log, we extract\nrelevant features from a specific time window and return\n4 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nEdge-IIoTset Dataset\nDoS/DDoS\n Information\nGathering\n Malware Attacks\n Man-in-the-middle\n(MITM)\n Injection Attacks\nTCP-SYN flood\nUDP flood\nHTTP flood\nICMP flood\nPort Scanning\nOS Fingerprinting\nVulnerability\nScanning\nBackdoor attack\nPassword\ncracking\nRansomware\nattack\nDNS Spoofing\nARP Spoofing\nXSS\nSQL Injection\nUploading attack\nFIGURE 2. Categories of the Edge-IIoTset dataset\nthem in a structured format suitable for analysis. Specifically,\nwe identify and separate each network flow in the PCAP\nfile. For each flow identified, we extract a set of predefined\nfeatures. Then, we organize the extracted features into a\nCSV file format for analysis. During our initial exploration,\nwe removed null features from the Edge-IIoTset dataset,\nidentifying 61 distinct and diverse features. These features\nare sufficiently various to distinguish the distinctive patterns\nof network attacks exclusively.\nThe Edge-IIoTset dataset comprises features gathered\nfrom various sources, including network traffic, logs, system\nresources, and alerts. To better understand these features, the\ninitial 15 can be seen in TABLE 2. For a comprehensive view\nof all 61 features, please see Table 7 in [9].\nN° Name Prot. Layer Type\n1 frame.time Frame Date and time\n2 ip.src_host IP Character string\n3 ip.dst_host IP Character string\n4 arp.dst.proto_ipv4 ARP IPv4 address\n5 arp.opcode ARP Unsigned integer\n6 arp.hw.size ARP Unsigned integer\n7 arp.src.proto_ipv4 ARP IPv4 address\n8 icmp.checksum ICMP Unsigned integer\n9 icmp.seq_le ICMP Unsigned integer\n10 icmp.transmit_timestamp ICMP Unsigned integer\n11 icmp.unused ICMP Sequence of bytes\n12 http.file_data HTTP Character string\n13 http.content_length HTTP Unsigned integer\n14 http.request.uri.query HTTP Character string\n15 http.request.method HTTP Character string\n. · · · · · · · · ·\n. · · · · · · · · ·\n61 mbtcp.unit_id Modbus/TCP Unsigned Integer\nTABLE 2. The first 15 features gathered from PCAP files.\nNumerous studies have already shown that these 61 dis-\ntinct features are sufficient to detect specific network-based\ncyberattacks. This dataset, therefore, serves as an optimal\nfoundation for comparing various ML algorithms [29]. After\ndiscussing the exact architectural design, the evaluation and\ncomparison of SecurityBERT with other research will be\ndetailed in Section IV.\nC. PRIVACY-PRESERVING FIXED-LENGTH ENCODING\nA pivotal aspect of the design involves representing the\nunstructured network data in a manner that allows BERT to\ncomprehend the context and relationships between various\nfeatures. BERT is designed to understand English profi-\nciently, but it may not be the most suitable ML model for\ncomprehending relationships between numbers. In our case,\nmany features are numerical values, i.e., unsigned integers,\nnot strings (as illustrated in TABLE 2), making it difficult\nto discern their interrelationships using natural language pro-\ncessing methods.\nTo leverage the power of BERT natural language un-\nderstanding, we process the dataset, comprising numerical\nand categorical values, and transform it into textual rep-\nresentation. Specifically, we added context to the data by\nincorporating column names and concatenating them with\ntheir respective values. Then, each new value is hashed and\ncombined with other hashed values within the same instance,\nresulting in the generation of a sequence. By employing this\ntechnique, we have developed a new language comprehen-\nsible to BERT and introduced privacy into the training data\nthrough cryptographic hash functions. We call this novel tex-\ntual representation technique as Privacy-Preserving Fixed-\nLength Encoding (PPFLE).\nSignificant similarities in log files, TCP scans, and mem-\nory dumps may lead to misinterpretation and incorrect classi-\nfication of various attacks. Employing a hash function allows\nfor handling even minor deviations in the data, effectively\nrepresenting them as distinct data points for ML. Moreover,\nspecific attacks, like UDP scans and others that are challeng-\ning to represent as plain text, can be better understood by\nthe model when they are converted into hashed values. Put\nsimply, we have developed a new linguistic format that the\nBERT model comprehends much more effectively than mere\nnumerical data, and it aligns more closely with the natural\nEnglish language for which the BERT model is specifically\ntailored.\nThrough this method, we fashioned a representation of the\nnumbers that closely align with natural language, allowing\nthe model to attain enhanced classification accuracy, as de-\ntailed in Section IV-A. Correctly converting network data and\napplying PPFLE can achieve higher accuracy than using the\noriginal pre-trained BERT model architecture.\nPPFLE description\nThe objectives of PPFLE are twofold. On the one hand,\nit is designed to convert unstructured network data into a\nstructured format that better mimics the natural English lan-\nguage, aligning well with the BERT model’s specialization.\nOn the other hand, it focuses on maintaining privacy by\nensuring that only encoded data is observed, thereby hiding\nsensitive information in the network data while preserving\nkey classification features.\nVOLUME 1, 2024 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nLet us define a matrix denoted by M with i rows and j\ncolumns. Here, M[i, j] represents the matrix element at the\nintersection of the ith row and jth column in M. We denote\nthe ith row ofMby ri = M[i, :]. In M, the first row contains\nthe column names, which serve as labels or identifiers for\neach column. We denote these column names as cj, where j\nrepresents the column index, i.e., M[1, j] =cj.\nLet us define s(i, j) as a concatenation operation where\nthe column name cj, a dollar sign, and the value of the jth\ncolumn in the (i + 1)th row ri are concatenated into a single\nstring, excluding the first row which contains the column\nnames, i.e.,\ns(i, j) =cj ∥ \"$\" ∥ M[i + 1, j] (1)\nNext, define H(x) as a hashing operation on a string x and\nlet L is a list where each element is separated by a space, i.e.,\nL = {l1 l2 l3 . . . lk}.\nThen, the textual representation of each rowi in the matrix\nMcan be expressed as follows:\nL ← L ∪ {H(s(i, n))} ∀(1 ≤ n ≤ j) (2)\nRepeating this procedure for each row in M, we obtain a\nnew matrix called DataList denoted as DL. In DL, each row\nrepresents an L list, i.e.;\nDL =\n\n\nL1 = [H(s(1, 1)) H(s(1, 2)) . . . H(s(1, j))]\nL2 = [H(s(2, 1)) H(s(2, 2)) . . . H(s(2, j))]\n...\nLi = [H(s(i, 1)) H(s(i, 2)) . . . H(s(i, j))]\n\n\nTEST ID ENC\n51 6 72 \n13 9 8 \nH(TEST$51) H(ID$6) H(ENC$72)\nH(TEST$13) H(ID$9) H(ENC$8)\n=\nFIGURE 3. Creating DataList example.\nIn other words, the DataList DL lists where each inner\nlist contains the hashed, concatenated column values for each\nrow in the matrix M. The M matrix in ML is commonly\ncalled a DataFrame. FIGURE 3 showcases a simple DataList\ncreation example. The pseudocode of the PPFLE algorithm\ncan be seen in Algorithm 1.\nThe PPFLE algorithm, despite its simplicity, effectively\ntranslates unstructured data into a fixed-length format. This\nrepresentation mirrors the characteristics of natural lan-\nguages, offering considerable advantages when utilized by\nML algorithms.\nAlgorithm 1Privacy-Preserving Fixed-Length Encoding\nRequire: Matrix Mwith i rows and j columns\n1: procedure PPFLE( M)\n2: DL ←[] ▷ Initialize DL to be empty\n3: for m = 1to i do ▷ Iterate through rows in M\n4: L = [] ▷ Initialize L to be an empty list\n5: for n = 1to j do ▷ Column iteration\n6: L ← H(s(m, n)) ▷ Append H(x) to L\n7: end for\n8: DL ←L ▷ Append L to DL\n9: end for\n10: return DL\n11: end procedure\nRemoved features for PPFLE encoding\nA natural question arises as to whether all 61 features are\nsuitable for PPFLE encoding. For instance, features like\n\"ip.src_host\" and \"ip.dst_host\" contain IP addresses, which\ncan lead to overfitting, especially if they have unique identi-\nfiers or particular details that don’t generalize well in differ-\nent network. Similarly, hashing timestamps with millisecond\nprecision could introduce confusion during training, so it\nmay be necessary to remove such features if one intends\nto apply PPFLE encoding. For this reason, several features\nrelated to network traffic and packet captures were excluded.\nHigh-cardinality features such as \"http.request.full_uri\" can\nbe challenging to encode effectively and might not offer gen-\neralizable patterns. Features with potential redundancy, like\nthe presence of both \"ip.src_host\" and \"arp.src.proto_ipv4\",\ncould introduce multicollinearity, affecting model stabil-\nity. Features such as \"frame.time\", indicating packet cap-\nture timestamps, might not directly relate to the predic-\ntive modeling task. Other columns like \"tcp.payload\" and\n\"http.file_data\" represent raw data payloads, which, without\nextensive preprocessing, could introduce noise rather than\nclarity. Removing these columns streamlines the dataset,\nenhancing computational efficiency and ensuring the model\nfocuses on the most relevant and generalizable patterns while\nmaintaining user privacy.\nD. BYTE-LEVEL BPE (BBPE) TOKENIZER\nTokenization is performed on the PPFLE-encoded data. This\nensures that no sensitive information is fed to the model\nduring training. A natural question arises: Doesn’t the PPFLE\ncompromise the semantics of network data, rendering to-\nkenization unfeasible? By applying PPFLE encoding, we\nconvert numerical values to align with the characteristics of\nnatural language more closely. Each feature is encoded inde-\npendently, allowing the adjacent hashed values to provide the\nmodel with sufficient information about the type of attacks it\nencounters. Hashing all 61 features together, however, would\ndestroy the semantics of the attacks.\nFigure 4 demonstrates the functionality of the PPFLE\nalgorithm, including tokenization.\n6 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nHTTP.METHODTCP.ACK TCP.ACK_RAW TCP.DSTPORT\nGET 1  2649986338 80\nPOST 13 1815315521 443\nH(HTTP.METHOD$GET) H(TCP.ACK$1) \nH(TCP.ACK_RAW$2649986338) H(TCP.DSTPORT$80) \n H(HTTP.METHOD$POST) H(TCP.ACK$13) \nH(TCP.ACK_RAW$1815315521) H(TCP.DSTPORT$443) \n \n=\nCSV=\nPPFLE algorithm\nD75B108CD2FDF2CD59A1A5DD694F3B85\nC4E0A295930F73AD87AB30DB4C2F202B\n43C771A79AD860AA0157E9FAEE32D093\n735CEA3C563AF202F26320A73BD54771\n \n \n4EE85AA84271C569937A6624BBD7E9AF \nA136C4F73BD9437F91217F4BA7893ED6\n 310A0ED9E67E7CE83B7BB04FCFF5C75B\n250752D7E3AB1336E717F0AE58851A26 \n \n40 318 148438 39 40\n24 41 65441 37 37\n3231150 39 1085 371059\n... ... ...... ... ...\n...... 394 612 37737\n22 42 40 42 22 39\n... ... ... ... ... ...\n... ... 38 40 2089 323\nTokenization on PPFLE encoded data\nFIGURE 4. PPFLE encoding and BBPE tokenization example\nFor instance, using PPFLE to encode a feature for an\nattack on port 443 with the GET method would appear\nas: H(TCP.DSTPORT$443) H(HTTP.METHOD$GET).\nConversely, a DNS poisoning attack would have a distinct\nrepresentation, lacking any HTTP.METHOD and thus consis-\ntently hashed as H(HTTP.METHOD$0). It has turned out\nduring our experimental analysis that these 61 features are\nhighly effective in representing different types of network\nattacks with great accuracy. Furthermore, the model can\nrecognize all attack patterns based on these features, even\nwhen hashed. For the data encoded with PPFLE, we em-\nployed the ByteLevelBPETokenizer from the Hugging\nFace Transformers library. This tokenizer, initially utilized\nfor GPT-2 [30], breaks down text into subword units for\ntokenization. It is based on the Byte-Pair Encoding (BPE)\nalgorithm [31], a data compression technique that replaces\nthe most frequent pair of consecutive bytes in a sequence with\na single, unused byte. The ByteLevelBPETokenizer\nis particularly useful for handling out-of-vocabulary (OOV)\nwords, which are not present in the tokenizer’s vocabulary\nof human language [32]. By breaking down our language\npresentation of network traffic data into smaller subwords\nlikely present in the tokenizer’s vocabulary as a sequence of\nbytes, we can efficiently process traffic data by leveraging the\npower of BERT.\nDuring the training of the tokenizer, a vocabulary size\nof 5000 was employed, along with a set of specific tokens,\nincluding [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\",\n\"<mask>\"]. The tokenizer’s training involved utilizing the\nfile name, setting the vocabulary size, establishing a mini-\nmum frequency of 2, and incorporating the list of special\ntokens. For a visual representation of the various tokens\nwithin the PPFLE encoded data, refer to FIGURE 4. Under-\nstanding the semantics of these tokens functions similarly to\ninterpreting a typical sentence. The hash output for a specific\nattack remains constant; thus, if these subword hexadecimal\nvalues appear in a particular sequence, BERT can recognize\nthat this unique order corresponds to a hash output and\ncategorize it as a specific attack.\nE. SECURITYBERT EMBEDDING\nAlgorithm 2 showcases the SecurityBERT embedding.\nThe algorithm starts by setting the chunk_size to 5000.\nAlgorithm 2Encode Evaluation Data Sequences\n1: chunk_size ← 5000\n2: num_chunks ← ⌈len(eval_data) / chunk_size⌉\n3: input_ids_eval ← [] ▷ Initialize as an empty list\n4: attention_masks_eval ← [] ▷ Initialize as an empty\nlist\n5: for i = 0 to num_chunks do\n6: start_idx ← i × chunk_size\n7: end_idx ← (i + 1)× chunk_size\n8: chunk ← eval_data[start_idx : end_idx]\n9: encoded_seqs ← encode(chunk)\n10: iic, amc← UNPACK(encoded_seqs)\n11: append icc to input_ids_eval\n12: append amc to attention_masks_eval\n13: end for\n14: concatenate the input IDs and attention masks as\ninput_ids_eval, attention_masks_eval\nIt then calculates the number of chunks, num_chunks,\nby dividing the length of the eval_data by chunk_size,\nand rounding up to the nearest integer. Two empty lists,\ninput_ids_eval and attention_masks_eval, are initialized\nto hold the encoded input IDs and attention masks, respec-\ntively. The algorithm then enters a loop, iterating from 0\nto num_chunks. This loop determines the start and end\nVOLUME 1, 2024 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nindices for each chunk of the eval_data. It retrieves a\nchunk of data using these indices and encodes each se-\nquence in the chunk, storing the result in encoded_seqs.\nThis encoded data is then unpacked into two components:\ninput_ids_chunk and attention_masks_chunk,\ndenoted by iic and amc, respectively. These com-\nponents are appended to the input_ids_eval and\nattention_masks_eval lists. Once all chunks have\nbeen processed, the algorithm concatenates all the in-\nput IDs and attention masks in input_ids_eval and\nattention_masks_eval respectively, along dimension\n0, thereby creating a complete set of input IDs and at-\ntention masks for the evaluation data. Here we note that\nthe input_ids_eval and attention_masks_eval\nare important components of the input to transformer-based\nmodels. The input_ids_eval is a sequence of integers\nrepresenting the input data after being tokenized. Each inte-\nger maps to a token in the model’s vocabulary.\nThe attention_masks_eval informs the model\nabout which tokens should be attended to and which should\nnot. In many cases, sequences are padded with special tokens\nto make all sequences the same length for batch processing.\nAttention masks prevent the model from attending to these\npadding tokens. Typically, an attention mask has the same\nlength as the corresponding input_ids sequence and con-\ntains a 1 for real tokens and a 0 for padding tokens.\nF. CONTEXTUAL REPRESENTATION\nWe adopted the BERT architecture, which leverages trans-\nformers for textual representation and cyber threat classi-\nfication. Specifically, we pre-trained our SecurityBERT\nusing our newly created tokenized dataset. In this process,\nSecurityBERT takes each token from the tokenized text\nand represents it as an embedding vector, denoted as X ∈\nRd, where d represents the dimensionality of the embedding\nspace. Then SecurityBERT utilizes a transformer-based\narchitecture consisting of multiple encoder layers. Each en-\ncoder layer comprises multi-head self-attention mechanisms\nand position-wise feed-forward neural networks. The self-\nattention mechanism [33] allows the model to capture depen-\ndencies and relationships between words within a sentence,\nthus facilitating contextual understanding. The self-attention\nmechanism in BERT can be mathematically expressed as\nfollows:\nAttention(Q, K, V) = σ\n\u0012QKT\n√dk\n\u0013\nV (3)\n, where σ is the softmax function, Q, K, and V are the\nquery, key, and value matrices, respectively, dk represents\nthe dimensionality of the keys vector, and T denotes the\ntranspose operation. Through self-attention, BERT encodes\ncontextual representations by capturing the importance of\ndifferent words within a sentence based on their semantic and\nsyntactic relationships. The resulting contextual embeddings\nare obtained through feed-forward operations and layer nor-\nmalization.\nG. TRAINING SECURITYBERT\nThe training of SecurityBERT involves several crucial\nsteps, each carefully calibrated to ensure optimal perfor-\nmance in security-centric tasks. These steps include data\ncollection and preprocessing, tokenizer training, model con-\nfiguration, and the training process itself. SecurityBERT\nworks with PPFLE-encoded data, simplifying certain steps\nin the tokenizer training process and requiring alternative ap-\nproaches for other steps. Here, we detail the distinct aspects\nof SecurityBERT’s training process.\n1) Text Normalization\nn(D) ={n(d)|d ∈ D} (4)\nIn this function, n(D) represents the normalization process\napplied to each document d in the set of all documents\nD. Text normalization typically involves converting all text\nto lowercase, removing punctuation, and sometimes even\nstemming or lemmatizing words (reducing them to their root\nform). This process is part of the original BERT architecture;\nhowever, when working with PPFLE-encoded data, this el-\nement becomes unnecessary and does not provide any extra\nvalue to our architecture.\n2) Tokenization\nt(d) ={t(w)|w ∈ d, d∈ D} (5)\nThe tokenization function t(d) breaks down each document\nd in the set D into its constituent words or tokens w. These\ntokens are the basic units of text that a machine-learning\nmodel can understand and process.\n3) Frequency Filtering\nf(D, F) ={w ∈ D|freq(w, D) ≥ F} (6)\nThis function f(D, F) defines a High-Pass filter, cutting out\ntokens w that have a frequency of occurrence, freq(w, D) less\nthan the minimum frequency F in the set of all documents\nD. This is to remove rare words that might not provide much\ninformational value for further processing or model training.\n4) Vocabulary Creation\nv(D, V) ={w|w ∈ D, rank(w, D) ≤ V } (7)\nHere, the function v(D, V) creates a vocabulary by choosing\nthe top V words w from the set of all documents D based on\ntheir frequency rank rank (w, D). This forms the vocabulary\nthat the model will recognize.\n5) Special Token Addition\nv′ = v ∪ S (8)\nThis states that the new vocabulary, v′, is a union of the\noriginal vocabulary v and the set of special tokens S. These\nspecial tokens typically include markers for the start and end\nof sentences, unknown words, padding, etc., and are essential\nfor certain SecurityBERT operations.\n8 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nFeature BertBase BertLarge SecurityBERT\nModel Type Pretrained model on English\nlanguage\nPretrained model on English\nlanguage\nPretrained model on Network\nData\nWord Embeddings Size 30522 x 768 30522 x 1024 30522 x 256\nPosition Embeddings Size 512 x 768 512 x 1024 512 x 256\nToken Type Embeddings Size 2 x 768 2 x 1024 2 x 256\nNumber of Layers in Encoder 12 24 4\nSize of Query, Key, Value in At-\ntention\n768 1024 256\nSize of Intermediate Layer in\nTransformer\n3072 4096 1024\nOutput Size of Transformer Layer 768 1024 256\nPooler Output Size 768 1024 256\nNumber of Parameters 110M 340M 11M\nAdditional Components None (General-purpose\nmodel)\nNone (General-purpose\nmodel)\nDropout (for regularization)\nand Classifier (linear layer\nfor classification tasks)\nTABLE 3. Comparison of Original BertBase, BertLarge, and SecurityBERT\n6) Tokenizer Training\nTd = map(v′) (9)\nFinally, the function map (v′) trains the tokenizer Td.\nThe trained tokenizer now maps future text inputs to the\nestablished vocabulary v′, effectively turning unstructured\ntext into a form that the SecurityBERT can process. The\ntrained tokenizer Td can take any text segment from the\ndocument ′d′ into a series of tokens that SecurityBERT\ncan understand.\nThese steps allow us to transform the raw text into a\nnumerical representation that SecurityBERT can process\neffectively.\nH. FINE TUNING SECURITYBERT\nAfter the pre-training stage, we fine-tuned SecurityBERT\nfor the cyber threat detection classification task. We added\none linear layer followed by a Softmax activation function on\ntop of the pre-trainedSecurityBERT model, and the entire\nnetwork is fine-tuned using our labeled data. This process\nenables SecurityBERT to adapt its learned contextual\nrepresentations to the specific threat detection requirements,\nimproving performance.\n1) Training setup\nThe training and fine-tuning were conducted on an Intel\nXenon(R) 2.20 GHz CPU and an Nvidia A100 GPU with\n40GB of RAM. The training on this specific hardware con-\nfiguration was completed in 1 hour and 47 minutes. The fol-\nlowing Section will extensively discuss a comprehensive per-\nformance evaluation of our novel SecurityBERT model.\nI. LAYERS OF THE SECURITYBERT MODEL\nThroughout the research, the primary objective was to attain\nexceptional accuracy in data classification while maintaining\nthe model’s size as compact as possible, taking performance\nconsiderations into account. After extensive experiments, the\nfinal model comprises 15 layers, specifically engineered to\naccurately comprehend PPFLE data while mitigating over-\nfitting issues by incorporating suitable dropout layers. The\ncomprehensive structure of the 15-layered SecurityBERT\nis illustrated in FIGURE 5. In the architectural design of\nSecurityBERT, we utilized just 4 Encoder Layers and\nmodified the original parameters to suit our problem bet-\nter. Additionally, we introduced a new layer in the final\nstage, comprising a Dropout layer, another new layer, and\na Classifier Layer. TABLE 3 highlights the key parameter\ndifferences between the original two BERT models and\nSecurityBERT.\n1) BERT Embeddings\nThe BERT Embeddings section starts with Word embed-\ndings, succeeded by Position embeddings and then Token\ntype embeddings. To stabilize the activations, there is a Layer\nNormalization with a size of 128, followed by a Dropout\nlayer with a rate of 0.1.\n2) BERT Self Attention\nThe BERT Self Attention comprises three primary linear\ntransformations for the Key, Query, and Value. Each of these\ntransformations has input and output features sized at 128.\nAnother Dropout layer with a rate of 0.1 is included to\nprevent overfitting.\n3) BERT Self Output\nThe BERT Self Output section features a Linear dense layer\nwith an input and output feature size of 128. A Layer Normal-\nization complements this, also sized at 128, and a Dropout\nlayer with a rate of 0.1 for regularization.\n4) BERT Intermediate\nIn the BERT Intermediate part, there’s a dense layer with\ninput features of 128 and output features expanded to 512.\nThis section employs the GELU activation function.\nVOLUME 1, 2024 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nPPFLE encoding\nBERT Embeddings\nEmbedding(512, 128)\nEmbedding(2, 128)\nLayerNorm((128,), eps=1e-12,\nelementwise_affine=True)\nDropout(p=0.1, inplace=False)\nEmbedding(30522, 128,\npadding_idx=0)\nW ord \nembeddings \nPosition \nembedding \nT oken \ntype \nembeddings \nNormalization \nlayer \nDropout \nlayer \nBERT Self Attention\nLinear(in_features=128,\nout_features=128, bias=True)\nLinear(in_features=128,\nout_features=128, bias=True)\nDropout(p=0.1, inplace=False)\nLinear(in_features=128,\nout_features=128, bias=True)Query \nKey \nV alue \nDropout \nlayer \nBERT Self Output\nLayerNorm((128,), eps=1e-12,\nelementwise_affine=True)\nDropout(p=0.1, inplace=False)\nLinear(in_features=128,\nout_features=128, bias=True)\nDropout \nlayer \nNormalization \nLayer \nDense \nLayer \nBERT Intermediate\nGELUActivation()\nLinear(in_features=128,\nout_features=512, bias=True)\nDense \nLayer \nIntermediate_act_fn \nlayer \nBERT Attention\nBERT output\nLayerNorm((128,), eps=1e-12,\nelementwise_affine=True)\nLinear(in_features=512,\nout_features=128, bias=True)\nDense \nLayer \nDropout(p=0.1, inplace=False)\nDropout \nlayer \nNormalization \nLayer \nBERT Pooler\nLinear(in_features=128,\nout_features=128, bias=True)\nTanh()\nDense \nlayer \nActivation \nlayer \nDropout(p=0.1, inplace=False)\nLinear(in_features=128,\nout_features=15, bias=True)\nClassifier \nLayer \nDropout \nlayer \nBERT FINAL\nOUTPUT\nINPUT\nFIGURE 5. SecurityBERT architecture.\n10 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\n5) BERT Output\nIn the BERT Output segment of the model, the final layer is\na Linear dense layer that transforms the 512 features back to\n128.\n6) BERT Pooler+BERT Final\nAfter a Tanh activation in the Bert Pooler, the output is\nstreamlined through another Linear layer, further reducing\nthe feature size to 15, representing the final output. This\nreduction is a crucial aspect of the model, preparing it for the\n15 distinct classification tasks (14 attacks + 1 normal traffic).\nJ. MODEL PARAMETERS\nThe precise parameter choices are among the most critical as-\npects of a BERT model. Incorrectly selected parameters can\nsignificantly influence the model’s performance. The model\nuses a Byte-Pair Encoding Tokenizer, which provides a reli-\nable and effective means of splitting input data into manage-\nable tokens. The training data utilized by the model amounts\nto 661,767,168 tokens, with a limited vocabulary size of\n5000. The minimum token frequency for SecuriyBERT\nis set at 2, while the model supports a maximum sequence\nlength of 737 and a minimum sequence length of 619. The\ntruncation settings limit the sequence length to a maximum\nof 512, ensuring data consistency and model stability. Special\ntokens used by the model include <s>, <pad>, </s>, <unk>,\nand <mask>. Regarding processing power, the model works\nwith a batch size of 128 and a hidden size of 128. The model\narchitecture comprises two hidden layers and utilizes four\nattention heads to process the input data. The intermediate\nsize is set at 512, and the maximum position embeddings at\n512, providing enough room for extensive and complex com-\nputations. The model can identify and respond to 14 different\nattacks, demonstrating its versatility and broad applicability.\nThe SecuriyBERT is based on 11,174,415 parameters that\nare fine-tuned for optimal performance. Lastly, the model\nruns on an Nvidia A100 GPU, a powerful hardware accelera-\ntor that enables rapid data processing and real-time response\ncapabilities.\nTABLE 4 summarizes the experimental parameter con-\nfiguration, carefully designed to optimize performance and\nfunctionality. With the application of these parameters, our\nnew SecurityBERT model exhibits the capability to iden-\ntify fourteen distinct types of attacks with remarkable accu-\nracy.\nIV. PERFORMANCE EVALUATION OF SECURITYBERT\nIn this section, we evaluate the performance of the newly\nproposed SecurityBERT model, through rigorous testing\nand comparative analysis. We show that the newly proposed\nmodel achieves a remarkable accuracy of 98.2%, which, to\nthe best of our knowledge, stands as the highest accuracy\never attained using an ML algorithm detecting IoT attacks\non realistic real-world network traffic.\nTABLE 4. Configuration and parameres of SecurityBERT.\nParameter Value\nTokenizer type Byte-Pair Encoding Tokenizer\nTraining data 661,767,168 tokens\nV ocabulary size 5000\nMinimum token frequency 2\nMaximum sequence length 737\nMinimum sequence length 619\nTruncation settings Max length=512\nSpecial tokens <s>, <pad>, </s>, <unk>,<mask>\nBatch size 128\nHidden size 128\nNumber of hidden layers 2\nNumber of attention heads 4\nIntermediate-size 512\nMaximum position embeddings 512\nNumber of attacks 14\nTotal number of parameters 11,174,415\nHardware accelerator GPU Nvidia A100\nA. EXPERIMENTAL RESULTS\nTo ensure appropriate comparisons with results from other\nmodels, we rely on standard measurements, namely Preci-\nsion, Recall, F1-Score, and Support measurements. These\nmetrics are crucial in comprehensively evaluating the\nmodel’s performance and providing a meaningful assessment\nof its capabilities.\nWe partitioned the Edge-IIoTset dataset conventionally,\nallocating 80% of the samples for training and reserving 20%\nfor evaluation. The model has not previously been exposed\nto the evaluation data, and we assess its effectiveness using\nthose samples. TABLE 5 presents the distribution of different\ntypes of cyber attack samples across training and evaluation\ndata sets.\nTABLE 5. Distribution of Data Across 14 Attack Types.\nAttack Type Nb. of Samples Train. Data Eval. Data\nNormal 1,615,643 1,292,514 323,129\nDDoS_UDP 121,568 88,027 22,007\nDDoS_ICMP 116,436 93,149 23,287\nSQL_injection 51,203 40,962 10,241\nPassword 50,153 40,122 10,031\nVulnerability_scanner 50,110 40,088 10,022\nDDoS_TCP 50,062 40,050 10,012\nDDoS_HTTP 49,911 39,929 9,982\nUploading 37,634 30,107 7,527\nBackdoor 24,862 19,890 4,972\nPort_Scanning 22,564 18,051 4,513\nXSS 15,915 12,732 3,183\nRansomware 10,925 8,740 2,185\nMITM 1,214 320 80\nFingerprinting 1,001 801 200\nMax Count 2,219,201 1,765,482 441,371\nTABLE 6 presents the detailed classification report for the\nSecuriyBERT model on various network attack classes.\nFIGURE 6 shows the accuracy and loss history during the\nSecuriyBERT training changes over four epochs.\n1) ROC AUC Scores for Cyber Threat Classification\nFIGURE 7 presents various classes’ Receiver Operating\nCharacteristic Area Under the Curve (ROC AUC) scores.\nVOLUME 1, 2024 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nFIGURE 6. Accuracy and loss history of SecurityLLM training in 4 epochs.\nTABLE 6. Classification Report of SecurityBERT.\nClass Precision Recall F1-Score Support\nNormal 1.00 1.00 1.00 323,129\nDDoS_UDP 1.00 1.00 1.00 22,007\nDDoS_ICMP 1.00 1.00 1.00 23,287\nSQL_injection 0.85 0.83 0.84 10,241\nPassword 0.85 0.81 0.83 10,031\nDDoS_TCP 1.00 1.00 1.00 10,012\nDDoS_HTTP 0.89 0.99 0.94 9,982\nVul_scanner 1.00 0.94 0.97 10,022\nUploading 0.79 0.86 0.83 7,527\nBackdoor 0.82 0.94 0.87 4,972\nPort_Scanning 0.87 1.00 0.93 4,513\nXSS 0.94 0.76 0.84 3,183\nRansomware 1.00 0.40 0.57 2,185\nFingerprinting 0.00 0.00 0.00 200\nMITM 1.00 1.00 1.00 80\nMacro Avg 0.87 0.84 0.84 441,371\nWeighted Avg 0.98 0.98 0.98 441,371\nAccuracy 0.982 (98.2%)\nThese scores indicate the SecurityBERT model’s perfor-\nmance, with a value of 1.0 being perfect. Classes \"Normal\",\n\"UDP\", \"TCP\", and \"MITM\" demonstrate perfect classifica-\ntion with an AUC score of 1.0, which suggests that the model\ncan flawlessly differentiate these classes from the others. The\nclasses \"ICMP\", \"SQL\", \"Pass\", \"HTTP\", \"Scan\", \"Upload\",\n\"Back\", \"Port\", \"XSS\", and \"Rans\" all have AUC scores ex-\nceedingly close to 1.0, ranging from approximately 0.9976 to\n0.999988. This implies a near-perfect classification for these\nclasses, with very minor misclassifications. On the lower end\nof the performance spectrum, the class \"Fing\" has an AUC\nscore of 0.991569, which, while still indicative of strong\nperformance, means it has a slightly higher misclassification\nrate than the other classes. The SecurityBERT model\ngenerally exhibits stellar performance across all classes, with\nalmost all of them achieving near-perfect or perfect classifi-\ncation.\nFIGURE 7. ROC AUC Scores for Cyber Threat Classification.\n2) Confusion Matrix\nA visual depiction of the confusion matrix from the\nSecuriyBERT classification is presented in FIGURE 8.\nFor the ’Normal’ class and most types of DDoS attacks,\nincluding ’DDoS_UDP’, ’DDoS_ICMP’, and ’DDoS_TCP’,\nthe model achieved perfect scores in terms of precision,\nrecall, and F1-score, showing a high accurate classification\nperformance on these types (c.f. TABLE 6). It is noteworthy\nto mention the high support count for the ’Normal’ class,\nwhich amounts to 323, 129 instances. The performance on\n’SQL_injection’, ’Password’, ’DDoS_HTTP’, ’Uploading’,\n’Backdoor’, and ’Port_Scanning’ classes was relatively lower\nbut still commendable, with F1-scores ranging from 0.83 to\n0.94. Notably, ’DDoS_HTTP’ and ’Port_Scanning’ achieved\na remarkably high recall of 0.99 and 1.00, respectively,\nindicating that the model could identify almost all instances\nof these attacks when they occur. ’Vul_scanner’ had a high\nprecision and a slightly lower recall of 0.94, resulting in an\n12 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nFIGURE 8. Confusion matrix of SecurityBERT classification.\nF1-score of 0.97, showing good performance in identifying\nthis type of attack. ’Ransomware’ showed a high precision of\n1.00, but with a significantly lower recall of 0.40, resulting\nin an F1-score of 0.57, suggesting that while the model made\ncorrect predictions for the ’Ransomware’ class, it missed a\nsignificant portion of actual instances.\nAn examination of the confusion matrix reveals that, while\nthe ransomware classification did experience misclassifica-\ntion in a substantial proportion of instances, most misclas-\nsifications occurred within the ’Backdoors’ category. This\ncategory bears notable similarities with the ransomware cat-\negory in real-life traffic data. Consequently, if our model\nmisclassifies ransomware as a backdoor, it will not have a\nsignificant impact, maintaining satisfactory results in prac-\ntical applications. The classes ’XSS’ and ’MITM’ showed\ngood performance with F1-scores of 0.84 and 1.00, respec-\ntively, demonstrating that the model handled these classes\nwell. Interestingly, the ’Fingerprinting’ class had a precision,\nrecall, and F1-score of 0, indicating a complete misclassifi-\ncation for these instances by the model. We again highlight\nthat, much like the backdoor-ransomware misclassification\nscenario, a considerable proportion of the ’Fingerprint’ mis-\nclassifications pertain to the ’ICMP’ class. Misclassifying\n’Fingerprint’ as ’XSS,’ for example, would ordinarily be a\nsubstantial issue in misclassification. However, in practical\napplications, these misclassifications bear no real conse-\nquence since the ’Fingerprint’ and ’ICMP’ classes closely\nresemble each other.\nThe average recall and F1-score were all 0.84 on the\nmacro level. The weighted average was considerably higher\nat 0.98 for all three metrics, suggesting a good performance\noverall. The slight difference between these two averages\nmay be due to the imbalanced nature of the dataset, as classes\nwith larger support have a greater influence on the weighted\naverage. The overall accuracy of the model, measuring the\nproportion of correct predictions made out of all predictions,\nwas 0.982, showing a high degree of the predictive power of\nthe SecurityBERT model in identifying different types of\nnetwork attacks.\n3) WeightWatcher - Empirical Spectral Density (ESD)\nWeightWatcher (WW) is an open-source diagnostic tool de-\nsigned for examining Deep Neural Networks (DNNs) and\ncan analyze various layers within a model. WeightWatcher\ncan assist in identifying signs of overfitting and underfit-\nting within particular layers of pre-trained or trained DNNs.\nWe employed WW to optimize performance throughout our\nexperiments, modifying the model’s parameters to achieve\noptimal results. FIGURE 9 presents the Power Law (PL)\nexponent (α) values when plotted against layer IDs, reveal-\ning intriguing insights into the weight matrix properties of\nSecurityBERT. FIGURE 10 presents the Empirical Spec-\ntral Density (ESD) for Layer 14. FIGURE 11 presents the\nLog-Lin Empirical Spectral Density (ESD) for Layer 14.\nFIGURE 9. Power Law (PL) exponent (α) values.\nInitial layers, especially the first, show a significantly\nhigh α value of around 10.43, suggesting a distinct weight\ninitialization or early layer behavior. As we progress deeper\ninto the network, the α values stabilize around 2 to 3, with\nmany layers hovering close to the 2 mark. An α value near 2\nindicates weight matrices possessing heavy-tailed properties,\nwhich, according to [34], smaller values ( α ≈ 2) are associ-\nated with models that generalize better.\nAccording to the measurements presented,SecurityBERT\ncan generalize effectively to new data that closely resembles\nthe patterns observed during testing on the training dataset.\nB. PERFORMANCE COMPARIOSN\nNumerous research studies have assessed the accuracy of\ndetecting the 14 attacks in the Edge-IIoTset dataset. In this\nVOLUME 1, 2024 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nTABLE 7. Comparison of SecurityBERT with traditional ML and DL models.\nAI type Authors Year AI Model Accuracy\nTraditional ML\nFerrag et al. [9] 2022 Decision Tree (DT) 67.11%\nFerrag et al. [9] 2022 Random Forest (RF) 80.83%\nFerrag et al. [9] 2022 Support Vector Machines (SVM) 77.61%\nAouedi et al. [27] 2023 DT + RF / FL 90.91%\nZhang et al. [35] 2023 K-Nearest Neighbor (KNN) 93.78%\nFerrag et al. [9] 2022 K-Nearest Neighbor (KNN) 79.18%\nDeep learning models\nFriha et al. [23] 2023 CNN / CL / No-DP 94.84%\nFriha et al. [23] 2023 CNN / FL / No-DP 93.96%\nAljuhani et al. [36] 2023 CSAE + ABiLSTM 94.40%\nFriha et al. [17] 2022 Recurrent Neural Network (RNN) 94%\nDing et al. [37] 2023 Long short-term memory (LSTM) 94.96%\nFerrag et al. [9] 2022 Deep Neural Network (DNN) 94.67%\nFriha et al. [17] 2022 Deep Neural Network (DNN) 93%\nE. M.d. Elias et al. [38] 2022 CNN-LSTM 97.14%\nFerrag et al. [39] 2023 Transformer model w/o Tokenization and Embedding 94.55%\nLarge language model - - BERT without PPFLE 51.3%\nThis work 2023 SecurityBERT with PPFLE 98.20%\nCNN: Convolutional Neural Network, CL: Centralized Learning, FL: federated learning, DP: Differential Privacy, CSAE: Contractive Sparse AutoEncoder,\nABiLSTM: Attention-based Bidirectional Long Short Term Memory.\nFIGURE 10. Empirical Spectral Density (ESD) for Layer 14.\nsection, we have specifically analyzed research conducted by\nvarious authors.\nThe creators of the Edge-IIoTset dataset tested various tra-\nditional ML algorithms on it, including Decision Tree (DT),\nRandom Forest (RF), Support Vector Machines (SVM), and\nK-Nearest Neighbor (KNN). Among these traditional meth-\nods, DT exhibited the lowest performance with an accu-\nracy of only 67.11%, while RF outperformed the others\nwith an accuracy of 80.83%. In addition to these traditional\nalgorithms, a Deep Neural Network (DNN) test was con-\nducted, which outperformed the others, boasting an accuracy\nof 94.67%. The ultimate objective of this research is to\ndevelop a model capable of achieving nearly flawless real-\ntime accuracy while maintaining a relatively compact model\nsize suitable for deployment on IoT-embedded devices. This\nrequirement explicitly rules out resource-intensive solutions\nFIGURE 11. Log-Lin Empirical Spectral Density (ESD) for Layer 14.\nlike utilizing pre-trained LLMs, which, although capable\nof delivering high accuracy, are impractical for constrained\ndevices regarding real-time packet analysis due to their\nsignificant resource demands. Following the initial dataset\nrelease, numerous authors tried to enhance accuracy using\nvarious model combinations and novel architectural designs.\nTABLE 7 presents the comparative accuracy of the proposed\nmodel, namely SecurityBERT, against the traditional ML\nmodels and Deep Learning (DL) models.\nFriha et al. [23] explored the potential of CNNs to exceed\nthe 95% accuracy mark. They experimented with various\nsetups, including Centralized Learning (CL) and Federated\nLearning (FL), both with and without Differential Privacy\n(DP). Using CL without DP, their best model attained an\naccuracy of 94.84%. E. M.d. Elias et al. combined the\nCNN approach with Long-Short Term Memory (LSTM).\n14 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nThis combination surpassed the 95% threshold, reaching an\nimpressive accuracy of 97.14% on the Edge-IIoTset dataset,\nsolely by extracting features from the transport and network\nlayers. A recent research paper by Ferrag et al. [29] explored\nan innovative method. They introduced a simple GAN and\nTransformer-based architecture without any tokenization or\nembeddings. The model obtained a 94.55% accuracy rate,\nand in this study, they raised the question of whether tok-\nenization could pose challenges when applied to IoT datasets\ndue to the unstructured nature of IoT network data, making\nit challenging to capture the semantics of closely resembling\npatterns like TCP scans or UDP scans.\nThe main objective of our paper was to create a novel\nmodel capable of exceeding an already exceptionally high\nlevel of accuracy. To the best of our knowledge, we have\nreached a record-breaking accuracy of 98.2% in classifying\nthe 14 types of attacks, as showcased in TABLE 6. This\nachievement represents the highest accuracy ever achieved\nin the multiclassification of these attack categories.\nC. REAL-LIFE ENVIRONMENT INTEGRATION\nIn the original Edge-IIoTset dataset, feature extraction is de-\nrived from genuine PCAP files. This implies that replicating\nthe same results in real-life scenarios becomes feasible if we\npossess real traffic data. We can substitute the PCAP files\nused in the Edge-IIoTset dataset [9] with real-life internal\nnetwork traffic, employing a suitable sniffing tool to generate\nthe PCAP file. FIGURE 12 provides a visual representa-\ntion of the experimental setup where SecurityBERT is\nseamlessly integrated into a real-life network environment.\nThis system is designed to detect network incidents with\nremarkable accuracy, leveraging real-time network packet\ndata.\n1) Inference time\nTo implement the model on IoT devices, evaluating whether\nthe inference time is sufficiently fast is essential. If the\nmodel is overly complex and exhibits slow inference times\non an average CPU, its viability in real-world environments\nbecomes questionable. TABLE 8 offers a detailed compara-\ntive analysis of computation times across different hardware\nplatforms, specifically focusing on the inference task of the\nSecuriyBERT model. The figures in the table denote the\naverage inference time derived from 1000 measurements.\nHardware Average Inference Time (sec)\nA100 GPU 0.0164\nT4 GPU 0.0244\nV100 GPU 0.0277\nTPU 0.0327\nCPU* 0.1582\n*Intel(R) Xeon(R) CPU @ 2.20GHz\nTABLE 8. Inference Time of SecuriyBERT Across Different Hardware\nPlatforms\nThe devices evaluated include three NVIDIA GPU models\n(A100, T4, and V100), Google’s Tensor Processing Unit\n                         \n                        \nZeek\nPCAP\nPacket\nsnififng\nTshark\nFeature extraction\nData collection\nFLLE encoding\nNormal DoS\nInternal network\nSecurity\nBERT\nClassification\nFIGURE 12. Real-life experimental setup using SecurityBERT.\n(TPU), and a general-purpose CPU. Each entry, denoted in\nseconds, reflects each hardware platform’s time to perform\nthe inference using SecuriyBERT. The A100 GPU is\nthe most efficient in this context, completing the inference\nquickly. The pivotal metric here is the 0.15 sec CPU inference\ntime, signifying that the model can be efficiently deployed\non resource-limited devices for analyzing real-life traffic.\nAdditionally, given its compact size of just 16.7 MB, the\nmodel is well-suited for deployment in embedded devices.\n2) Reducing MTTR\nIntegrating SecurityBERT into an embedded device and\ndeploying it within an IoT network makes it possible to sub-\nstantially enhance detection accuracy and leverage its high-\nspeed performance to quickly identify malicious activities\nwithin internal networks in real-time. This, in turn, can lead\nVOLUME 1, 2024 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nto a notable reduction in Mean Time to Remediate (MTTR).\nImplementing AI in software security and incident han-\ndling is not a recent development in software security. For\ninstance, companies like Rubrik2 and Microsoft have adopted\ngenerative AI models to optimize operations and enhance\nefficiency. For instance, if Rubrik’s Security Cloud machine\ndetects abnormal behavior, it automatically generates an in-\ncident in Microsoft’s Sentinel. By employing this proactive\napproach, they can achieve faster response times and more\neffective management of potential security threats.\nSimilarly, SecurityBERT can be seamlessly integrated\ninto existing real-world systems, thereby augmenting the\noverall accuracy and detection rate of these pre-existing\nsystems.\nV. CONCLUSION\nThe innovative application of BERT architecture for cyber\nthreat detection, embodied in SecurityBERT, demon-\nstrated remarkable efficiency, contradicting initial assump-\ntions regarding its incompatibility due to the reduced sig-\nnificance of syntactic structures. Experimental results under-\nscored the superiority of this approach over conventional ML\nand DL models, including CNNs, deep learning networks,\nand recurrent neural networks. TheSecurityBERT model,\ntested on a collected cybersecurity dataset, exhibited an\noutstanding capability to identify fourteen distinct types of\nattacks with an accuracy rate of 98.2%.\nWhile this paper has made significant progress in ad-\nvancing the use of LLMs in cybersecurity, future research\ndirections can take several routes to enhance these promising\nfindings further. One potential avenue involves fine-tuning\nand expanding the SecurityBERT model to enhance its\nperformance across various attack types, incorporating adver-\nsarial attacks and more complex threats. In addition, due to\nthe evolving nature of cyber threats, continuous updating and\ntraining of SecurityBERT model on the latest real-world\ndatasets will be imperative to maintain its efficacy.\nAn exciting and promising avenue for future research is\ndelving into methods to autonomously implement mitiga-\ntions based on the classification of SecurityBERT. This\nadvancement could lead to automated patch management,\nantivirus management, network reconfiguration, port man-\nagement, and numerous other facets of cybersecurity man-\nagement.\nREFERENCES\n[1] Statista Report. Internet of Things (IoT) - Statistics & Facts. Blog post,\nSept. 2023.\n[2] Nour Moustafa, Nickolaos Koroniotis, Marwa Keshk, Albert Y Zomaya,\nand Zahir Tari. Explainable intrusion detection for cyber defences in the\ninternet of things: Opportunities and solutions. IEEE Communications\nSurveys & Tutorials, 2023.\n[3] Stefano Silvestri, Shareeful Islam, Spyridon Papastergiou, Christos Tza-\ngkarakis, and Mario Ciampi. A machine learning approach for the\nnlp-based analysis of cyber threats and vulnerabilities of the healthcare\necosystem. Sensors, 23(2):651, 2023.\n2https://www.rubrik.com/products\n[4] Chandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, Seyit Camtepe,\nJosef Pieprzyk, and Surya Nepal. Transformer-based language models\nfor software vulnerability detection. In Proceedings of the 38th Annual\nComputer Security Applications Conference, pages 481–496, 2022.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805, 2018.\n[6] Abir Rahali and Moulay A. Akhloufi. Malbert: Malware detection using\nbidirectional encoder representations from transformers*. 2021 IEEE\nInternational Conference on Systems, Man, and Cybernetics (SMC), pages\n3226–3231, 2021.\n[7] Yunus Emre Seyyar, Ali Gökhan Yavuz, and Halil Murat Ünver. An\nattack detection framework based on bert and deep learning. IEEE Access,\n10:68633–68644, 2022.\n[8] Song Chen and Hai Liao. Bert-log: Anomaly detection for system logs\nbased on pre-trained language model. Applied Artificial Intelligence, 36,\n2022.\n[9] Mohamed Amine Ferrag, Othmane Friha, Djallel Hamouda, Leandros\nMaglaras, and Helge Janicke. Edge-iiotset: A new comprehensive realistic\ncyber security dataset of iot and iiot applications for centralized and\nfederated learning. IEEE Access, 10:40281–40306, 2022.\n[10] Natasha Alkhatib, Maria Mushtaq, Hadi Ghauch, and Jean-Luc Danger.\nCan-bert do it? controller area network intrusion detection system based on\nbert language model. In 2022 IEEE/ACS 19th International Conference on\nComputer Systems and Applications (AICCSA), pages 1–8. IEEE, 2022.\n[11] Ehsan Aghaei, Xi Niu, Waseem Shadid, and Ehab Al-Shaer. Securebert:\nA domain-specific language model for cybersecurity. In International\nConference on Security and Privacy in Communication Systems, pages\n39–56. Springer, 2022.\n[12] Priyanka Ranade, Aritran Piplai, Anupam Joshi, and Tim Finin. Cybert:\nContextualized embeddings for the cybersecurity domain. 2021 IEEE\nInternational Conference on Big Data (Big Data), pages 3334–3342, 2021.\n[13] Keping Yu, Liang Tan, Shahid Mumtaz, Saba Al-Rubaye, Anwer Al-\nDulaimi, Ali Kashif Bashir, and Farrukh Aslam Khan. Securing critical\ninfrastructures: Deep-learning-based threat detection in iiot. IEEE Com-\nmunications Magazine, 59(10):76–82, 2021.\n[14] Bernardo Breve, Gaetano Cimino, and Vincenzo Deufemia. Identifying\nsecurity and privacy violation rules in trigger-action iot platforms with nlp\nmodels. IEEE Internet of Things Journal, 10(6):5607–5622, 2023.\n[15] Zhendong Wang, Jingfei Li, Shuxin Yang, Xiao Luo, Dahai Li, and\nSoroosh Mahmoodi. A lightweight iot intrusion detection model based on\nimproved bert-of-theseus. Expert Systems with Applications, 238:122045,\n2024.\n[16] Djallel Hamouda, Mohamed Amine Ferrag, Nadjette Benhamida, and\nHamid Seridi. Ppss: A privacy-preserving secure framework using\nblockchain-enabled federated deep learning for industrial iots. Pervasive\nand Mobile Computing, page 101738, 2022.\n[17] Othmane Friha, Mohamed Amine Ferrag, Lei Shu, Leandros Maglaras,\nKim-Kwang Raymond Choo, and Mehdi Nafaa. Felids: Federated\nlearning-based intrusion detection system for agricultural internet of\nthings. Journal of Parallel and Distributed Computing, 165:17–31, 2022.\n[18] Shitharth Selvarajan, Gautam Srivastava, Alaa O Khadidos, Adil O Khadi-\ndos, Mohamed Baza, Ali Alshehri, and Jerry Chun-Wei Lin. An artificial\nintelligence lightweight blockchain security model for security and privacy\nin iiot systems. Journal of Cloud Computing, 12(1):38, 2023.\n[19] Yizheng Chen, Zhoujie Ding, Xinyun Chen, and David Wagner. Di-\nversevul: A new vulnerable source code dataset for deep learning based\nvulnerability detection. arXiv preprint arXiv:2304.00409, 2023.\n[20] Maryam Douiba, Said Benkirane, Azidine Guezzaz, and Mourade Azrour.\nAn improved anomaly detection model for iot security using decision tree\nand gradient boosting. The Journal of Supercomputing, 79(3):3392–3411,\n2023.\n[21] Hamidreza Jahangir, Subhash Lakshminarayana, Carsten Maple, and Gre-\ngory Epiphaniou. A deep learning-based solution for securing the power\ngrid against load altering threats by iot-enabled devices. IEEE Internet of\nThings Journal, 2023.\n[22] Fei Hu, Wuneng Zhou, Kaili Liao, Hongliang Li, and Dongbing Tong.\nTowards federated learning models resistant to adversarial attacks. IEEE\nInternet of Things Journal, 2023.\n[23] Othmane Friha, Mohamed Amine Ferrag, Mohamed Benbouzid, Tarek\nBerghout, Burak Kantarci, and Kim-Kwang Raymond Choo. 2df-ids:\nDecentralized and differentially private federated learning-based intrusion\ndetection system for industrial iot. Computers & Security, page 103097,\n2023.\n16 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\n[24] Chinmay Chakraborty, Senthil Murugan Nagarajan, Ganesh Gopal De-\nvarajan, TV Ramana, and Rajanikanta Mohanty. Intelligent ai-based\nhealthcare cyber security system using multi-source transfer learning\nmethod. ACM Transactions on Sensor Networks, 2023.\n[25] Xin Wang, Chongrong Fang, Ming Yang, Xiaoming Wu, Heng Zhang,\nand Peng Cheng. Resilient distributed classification learning against\nlabel flipping attack: An admm-based approach. IEEE Internet of Things\nJournal, 2023.\n[26] Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and\nJingyi Zhang. Cps attack detection under limited local information in cyber\nsecurity: An ensemble multi-node multi-class classification approach.\nACM Transactions on Sensor Networks, 2023.\n[27] Ons Aouedi and Kandaraj Piamrat. F-bids: Federated-blending based\nintrusion detection system. Pervasive and Mobile Computing, 89:101750,\n2023.\n[28] Mohamed Amine Ferrag, Leandros Maglaras, Sotiris Moschoyiannis, and\nHelge Janicke. Deep learning for cyber security intrusion detection:\nApproaches, datasets, and comparative study. Journal of Information\nSecurity and Applications, 50:102419, 2020.\n[29] Mohamed Amine Ferrag, Merouane Debbah, and Muna Al-Hawawreh.\nGenerative ai for cyber threat-hunting in 6g-enabled iot networks. 2023\nIEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet\nComputing Workshops (CCGridW), pages 16–25, 2023.\n[30] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement\nDelangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan\nFuntowicz, et al. Huggingface’s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771, 2019.\n[31] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda,\nAyumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair\nencoding: A text compression scheme that accelerates pattern matching.\nResearchgate, 1999.\n[32] Ali Araabi, Christof Monz, and Vlad Niculae. How effective is byte\npair encoding for out-of-vocabulary words in neural machine translation?\narXiv preprint arXiv:2208.05225, 2022.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. Advances in neural information processing systems, 30,\n2017.\n[34] Charles H Martin, Tongsu Peng, and Michael W Mahoney. Predicting\ntrends in the quality of state-of-the-art neural networks without access to\ntraining or testing data. Nature Communications, 12(1):4122, 2021.\n[35] Xixi Zhang, Liang Hao, Guan Gui, Yu Wang, Bamidele Adebisi, and\nHikmet Sari. An automatic and efficient malware traffic classification\nmethod for secure internet of things. IEEE Internet of Things Journal,\n2023.\n[36] Ahamed Aljuhani, Prabhat Kumar, Rehab Alanazi, Turki Albalawi, Okba\nTaouali, AKM Najmul Islam, Neeraj Kumar, and Mamoun Alazab. A deep\nlearning integrated blockchain framework for securing industrial iot. IEEE\nInternet of Things Journal, 2023.\n[37] Weiping Ding, Mohamed Abdel-Basset, and Reda Mohamed. Deepak-iot:\nAn effective deep learning model for cyberattack detection in iot networks.\nInformation Sciences, 634:157–171, 2023.\n[38] Erik Miguel de Elias, Vinicius Sanches Carriel, Guilherme Werneck\nDe Oliveira, Aldri Luiz Dos Santos, Michele Nogueira, Roberto Hirata\nJunior, and Daniel Macêdo Batista. A hybrid cnn-lstm model for iiot\nedge privacy-aware intrusion detection. In 2022 IEEE Latin-American\nConference on Communications (LATINCOM), pages 1–6. IEEE, 2022.\n[39] Mohamed Amine Ferrag, Merouane Debbah, and Muna Al-Hawawreh.\nGenerative ai for cyber threat-hunting in 6g-enabled iot networks. In 2023\nIEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet\nComputing Workshops (CCGridW), pages 16–25, 2023.\nMOHAMED AMINE FERRAG earned his Bach-\nelor’s, Master’s, Ph.D., and Habilitation degrees in\nComputer Science from Badji Mokhtar—Annaba\nUniversity in Annaba, Algeria, completing his\nstudies in 2008, 2010, 2014, and 2019, respec-\ntively. He served as an Associate Professor in\nthe Department of Computer Science at Guelma\nUniversity, Algeria, from 2014 until 2022. Con-\ncurrently, from 2019 to 2022, he held the position\nof Senior Researcher at the NAU-Lincoln Joint\nResearch Center of Intelligent Engineering, based at Nanjing Agricultural\nUniversity in China. As of 2022, Dr. Ferrag is the Lead Researcher at the\nArtificial Intelligence & Digital Science Research Center at the Technology\nInnovation Institute in Abu Dhabi, United Arab Emirates. Dr. Ferrag’s\nresearch is primarily focused on a spectrum of topics within the cyber\nsecurity domain, including wireless network security, network coding secu-\nrity, applied cryptography, blockchain technology, generative AI, software\nsecurity, and the application of AI in cyber security. His scholarly output\nincludes over 140 papers published in international journals and conference\nproceedings. Dr. Ferrag has spearheaded numerous projects in research\nand development, fostering collaborative ties with academic institutions\nin the UK, Australia, USA, Canada, and China. His contributions to the\nfield include the creation of two cybersecurity datasets, namely, Edge-IIoT\ndataset and FormAI dataset, which have become essential resources for AI\nresearchers worldwide. His academic contributions have been recognized\nwith the 2021 IEEE TEM Best Paper Award, the 2022 Scopus Algeria\nAward, and many best paper conference awards. He has consistently been\nnamed on Stanford University’s list of the world’s top 2% of scientists four\ntimes from 2020 through 2023. Dr. Ferrag also contributes to the academic\ncommunity as an associate editor for prestigious journals, such as the IEEE\nInternet of Things Journal and ICT Express (Elsevier). In addition to his\nresearch and editorial roles, Dr. Ferrag is a Senior Member of the Institute\nof Electrical and Electronics Engineers (IEEE).\nMTHANDAZO NDHLOVU is a Senior Secu-\nrity Researcher for the Digital Security Unit at\nTechnology Innovation Institute (TII), Abu Dhabi,\nUnited Arab Emirates. He is an alumni of both 42\nParis, France and the National University of Sci-\nence and Technology, Bulawayo, Zimbabwe. He\nholds the Offensive Security Certified Professional\n(OSCP) certification and Certified Red Team Pro-\nfessional certification (CRTP). He is highly skilled\nin penetration testing, reverse engineering, soft-\nware development, n-day research and malware research. His technology\ninterests include mobile and windows security, red team operations and NLP.\nHis research interests include software security, malware analysis, program\nanalysis and AI for cyber security.\nNORBERT TIHANYI holds a B.Sc in Security\nEngineering, an M.Sc degree in Safety Engineer-\ning and an another M.Sc degree in IT Engineering\n(with honors). He received his Ph.D. in Informa-\ntion Science and Technology from Eötvös Loránd\nUniversity, Budapest, Hungary, in 2020. He is a\nPublic Body member of the Hungarian Academy\nof Science. Currently, he is a Lead Researcher at\nTechnology Innovation Institute (TII), Abu Dhabi,\nUnited Arab Emirates. His research interests in-\nclude cryptanalysis, security of embedded devices and cryptography-related\nprime number theory.\nVOLUME 1, 2024 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFerrag et al. Revolutionizing Cyber Threat Detection with Large Language Models\nLUCAS C. CORDEIRO received a B.Sc. degree\nin electrical engineering and an M.Sc. degree in\ncomputer engineering from the Federal University\nof Amazonas (Brazil) in 2005 and 2007, respec-\ntively. He received a Ph.D. in computer science\nfrom the University of Southampton (UK) in 2011.\nHe is a Reader in the Department of Computer\nScience at the University of Manchester (UK),\nwhere he leads the Systems and Software Security\n(S3) Research Group. Dr. Cordeiro is also the Arm\nCentre of Excellence Director at UoM. In addition, he is affiliated with the\nTrusted Digital Systems Cluster at the Centre for Digital Trust and Society,\nthe Formal Methods Group at the University of Manchester, UK, and the\nPost-Graduate Programs in Electrical Engineering (PPGEE) and Informatics\n(PPGI) at the Federal University of Amazonas, Brazil. Before joining the\nUniversity of Manchester, he worked as a post-doctoral researcher at the\nUniversity of Oxford and as a research engineer at Diffblue. In addition,\nDr. Cordeiro worked for five years as a software engineer at Siemens /\nBenQ Mobile and CTPIM / NXP semiconductors. His work focuses on\nsoftware model checking, automated testing, program synthesis, software\nsecurity, embedded and cyber-physical systems. He has co-authored more\nthan 150 peer-reviewed publications in the most prestigious venues (e.g.,\nICSE, CA V , TACAS, FSE, ASE, ISSTA, TSE, TR, TC). He has received\nvarious international awards, including the Most Influential Paper Award at\nASE’23, the Distinguished Paper Award at ICSE’11, and 46 awards from\nthe international competitions on software verification (SV-COMP) and\ntesting (Test-Comp) 2012-2024. He has a proven track record of securing\nresearch funding from EPSRC, Intel, Motorola, Samsung, Nokia Institute of\nTechnology, CNPq, FAPEAM, British Council, and Royal Society (career\ntotal over USD 13M).\nMEROUANE DEBBAH (images/Fellow, IEEE)\nreceived the M.Sc. and Ph.D. degrees from Ecole\nNormale Supérieure Paris-Saclay, France. He was\nwith Motorola Lab, Saclay, France, from 1999 to\n2002, and then with the Vienna Research Cen-\nter for Telecommunications, Vienna, Austria, un-\ntil 2003. From 2003 to 2007, he was an Assis-\ntant Professor with the Mobile Communications\nDepartment, Institut Eurecom, Sophia Antipolis,\nFrance. Since 2007, he has been a Full Professor\nwith CentraleSupelec, Gif-sur-Yvette, France. From 2007 to 2014, he was\nthe Director of the Alcatel-Lucent Chair on Flexible Radio. From 2014 to\n2021, he was the Vice-President of the Huawei France Research Center.\nHe was jointly the Director of the Mathematical and Algorithmic Sciences\nLaboratory and the Director of the Lagrange Mathematical and Computing\nResearch Center. From 2021 to 2023, he was the Chief Researcher at the\nTechnology Innovation Institute and led the AI and Digital Science Research\nCenter, Technology Innovation Institute. He was also an Adjunct Professor\nwith the Department of Machine Learning, Mohamed bin Zayed University\nof Artificial Intelligence, Abu Dhabi. Since 2023, he has been a Professor\nat the Khalifa University of Science and Technology, Abu Dhabi, and the\nFounding Director of the 6G Center. He is currently a Professor at the\nKhalifa University of Science and Technology. He has managed eight EU\nprojects and over 24 national and international projects. He holds more\nthan 40 patents. His research interests include fundamental mathematics,\nalgorithms, statistics, information, and communication sciences research. He\nis a WWRF Fellow, a EURASIP Fellow, an AAIA Fellow, an Institut Louis\nBachelier Fellow, and a Membre émérite SEE. He received the ERC Grant\nMORE (Advanced Mathematical Tools for Complex Network Engineering)\nfrom 2012 to 2017. He received multiple prestigious distinctions, prizes, and\nbest paper awards (more than 35 best paper awards) for his contributions to\nboth fields and, according to research.com, is ranked as the best scientist in\nFrance in electronics and electrical engineering.\nTHIERRY LESTABLE is an Acting Chief Re-\nsearcher, Digital Security & Telecommunications\nUnits, Artificial Intelligence & Digital Science Re-\nsearch Center (AIDRC) - Technology Innovation\nInstitute, UAE, covering multi-disciplinary units\nsuch as AI, Telecom (B5G/6G) and (Cyber) Se-\ncurity. Before, Dr. Lestable was Director, heading\nthe e-Mobility & Automotive Business Depart-\nment from Sagemcom (France), addressing Mar-\nket and Products challenges raised from Intelligent\nand Connected Vehicles (Telematics, DMS, ADAS) to micro-mobility (e-\nBikes). Dr. Lestable was a member of the Steering Committee from FIEV\nFederation, representing major Automotive Equipment Suppliers, where\nhe promoted the benefits of A.I for road safety. Before Automotive, he\nheaded the Industrial IoT Business Department, designing and manufac-\nturing connected devices and platforms for the industry (Smart City, Asset\nManagement, Smart Grid, etc.), rolling out LPW AN Networks worldwide.\nEarlier, as Head of IoT Business Development & Strategic Partnerships, he\nactively participated to the Launch of IoT LoRa™ Alliance, being granted\nthe position of Vice-chair for the Strategy Committee in June 2015. Then,\nfrom October 2015 till 2017, he was the Vice-Chairman of the Alliance. Dr.\nLestable has +25 years’ experience in the Telecom, Mobility, and Energy\nindustry, steering industrial innovation from spark of ideas to prototypes,\nproducts, standards, and systems, rolling out networks while leading new\nbusinesses within organizations such as Alcatel, Samsung, and Sagemcom.\nHe received both Engg. Degree from Supélec (France) in 1997, and Ph.D.\nin 2003 from Supélec/Paris Sud-Orsay. Dr. Lestable is the author of 35+\ninternational publications, 20+ patents, editor of 1 Wiley Book on Advanced\nChannel Coding, co-author in 2 Wiley Books on B3G/4G systems, was\nTelecom Expert for the European Commission, Eureka Cluster CELTIC, and\nthe French National Research Agency (ANR). He was Chair of the M2M\nExpert Group in eMobility European Technology Platform. In 2010, Dr.\nLestable was a member of the Telecom Steering Committee of Systematic,\na competitiveness pole, and created the IoT/M2M course in Supélec, where\nhe was a visiting lecturer for seven years.\nNARINDERJIT SINGH THANDI extensive ex-\nperience in Defence with 23 years across UK MoD\nresearch and Defence Companies in the UK and\nEurope as a technical expert focusing on Land and\nAir secure systems. Worked across strategy, opera-\ntions, capability development, technology and im-\nplementation but his real strengths are C4ISR and\nData Exploitation across secure communication\nsystems. Narinder has worked extensively in UK,\nUS, EU, UAE and KSA, focusing on capability\ndevelopment, within Defence secruity.\n18 VOLUME 1, 2024\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3363469\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8834944367408752
    },
    {
      "name": "Deep learning",
      "score": 0.5971101522445679
    },
    {
      "name": "Artificial intelligence",
      "score": 0.553119421005249
    },
    {
      "name": "Encoder",
      "score": 0.5359445810317993
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5285890102386475
    },
    {
      "name": "Autoencoder",
      "score": 0.527205765247345
    },
    {
      "name": "Software deployment",
      "score": 0.5028769373893738
    },
    {
      "name": "Machine learning",
      "score": 0.4556875228881836
    },
    {
      "name": "Transformer",
      "score": 0.4426228702068329
    },
    {
      "name": "Artificial neural network",
      "score": 0.44041919708251953
    },
    {
      "name": "Recurrent neural network",
      "score": 0.42842280864715576
    },
    {
      "name": "Language model",
      "score": 0.42163556814193726
    },
    {
      "name": "Inference",
      "score": 0.41022229194641113
    },
    {
      "name": "Computer security",
      "score": 0.3454301655292511
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210087059",
      "name": "Technology Innovation Institute",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I62885914",
      "name": "Universidade Federal do Amazonas",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I28407311",
      "name": "University of Manchester",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I176601375",
      "name": "Khalifa University of Science and Technology",
      "country": "AE"
    }
  ],
  "cited_by": 159
}