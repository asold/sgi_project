{
  "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning",
  "url": "https://openalex.org/W2984864519",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2939534090",
      "name": "Ye, Zihao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2203536932",
      "name": "Guo Qipeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144150044",
      "name": "Gan Quan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378878701",
      "name": "Qiu Xipeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003519119",
      "name": "Zhang Zheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2184135559",
    "https://openalex.org/W2786396726",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2947881255",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2971933740",
    "https://openalex.org/W2953830716",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W2953391617",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W2540404261",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2553397501",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2886490473"
  ],
  "abstract": "The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields $O(k\\cdot n\\log (n/k))$ connections where $k$ is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.",
  "full_text": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning\nZihao Ye‚Ä†, Qipeng Guo‚Ä†‚Ä°‚àó, Quan Gan‚Ä†, Xipeng Qiu‚Ä°, Zheng Zhang‚Ä†¬ß\n‚Ä†AWS Shanghai AI Lab\n‚Ä°Fudan University\n¬ßNew York University Shanghai\n{yeziha, gqipeng, quagan, zhaz}@amazon.com, xpqiu@fudan.edu.cn\nAbstract\nThe Transformer model is widely successful\non many natural language processing tasks.\nHowever, the quadratic complexity of self-\nattention limit its application on long text. In\nthis paper, adopting a Ô¨Åne-to-coarse attention\nmechanism on multi-scale spans via binary\npartitioning (BP), we propose BP-Transformer\n(BPT for short). BPT yields O(k ¬∑n log(n/k))\nconnections where k is a hyperparameter to\ncontrol the density of attention. BPT has a\ngood balance between computation complex-\nity and model capacity. A series of experi-\nments on text classiÔ¨Åcation, machine transla-\ntion and language modeling shows BPT has a\nsuperior performance for long text than previ-\nous self-attention models. Our code, hyperpa-\nrameters and CUDA kernels for sparse atten-\ntion are available in PyTorch1.\n1 Introduction\nTransformer, a self-attention based model, has\nachieved many impressive results on Natural Lan-\nguage Processing (NLP) tasks, notably machine\ntranslation (Vaswani et al., 2017), language mod-\neling (Radford et al., 2018), and text classiÔ¨Åcation\n(Devlin et al., 2018). However, its self-attention\nmechanism imposes a quadratic cost with respect\nto sequence length, limiting its wider application,\nespecially for long text.\nTo address this problem, some previous works\nhave explored different directions. (1) Hierarchi-\ncal Transformers (Miculicich et al., 2018; Liu and\nLapata, 2019) uses two Transformers in a hierar-\nchical architecture: one Transformer models the\nsentence representation with word-level context,\nand another the document representation with the\nsentence-level context. (2) Lightweight Trans-\nformers (Child et al., 2019; Sukhbaatar et al.,\n‚àóWork done during internship at AWS Shanghai AI Lab.\n1https://github.com/yzh119/BPT\n2 3 4 5 61 7\n6 7 8 95\n2 3 41 5\nTransformerTransformer-XL\nBPT\n[5,6] [10,11]\n3 421 1011\n[1,4]\nFigure 1: Different attention pattern in Transformer-\nlike models. Solid line refers to direct attention, while\nthe dashed line denotes dependency. Unrelated connec-\ntions and self loops are omitted for clarity.\n2019; Guo et al., 2019; Dai et al., 2019) reduce\nthe complexity by reconstructing the connections\nbetween tokens.\nBesides the computational cost, the fully-\nconnected nature of Transformer does not incor-\nporate the commonsensible inductive bias of lan-\nguage, such as sequential or syntax structure. The\ndependency relations between tokens are totally\nlearned from scratch. Therefore, Transformer usu-\nally performs better on huge datasets and is easy to\noverÔ¨Åt on small datasets (Guo et al., 2019).\nThe above observation motivates us to explore\nbetter structure for self-attention models to bal-\nance the capability and computation complexity.\nIn this paper, we propose a new architecture called\nBP-Transformer (BPT for short), which parti-\ntions the input sequence into different multi-scale\nspans via binary partitioning (BP). BPT incor-\nporates an inductive bias of attending the context\ninformation from Ô¨Åne-grain to coarse-grain as the\nrelative distance increases. The farther the context\ninformation is, the coarser its representation is.\nBPT can be regard as graph neural network, whose\nnodes are the multi-scale spans. A token node\ncan attend the smaller-scale span for the closer\ncontext and the larger-scale span for the longer-\narXiv:1911.04070v1  [cs.CL]  11 Nov 2019\ndistance context. The representations of nodes\nare updated with Graph Self-Attention (Velickovic\net al., 2018).\nMoreover, to better represent the position infor-\nmation of the span nodes and token nodes, we gen-\neralize the notion of relative position (Shaw et al.,\n2018) from sequences to trees and show that it bet-\nter captures position bias.\nThus, BPT incorporates the advantages of both\nhierarchical and lightweight Transformerss: (1) it\nmodels the long-range context in an hierarchical\nfashion, (2) reduces computation cost with fewer\nedges, and Ô¨Ånally, (3) introduces coarse-to-Ô¨Åne\nconnections to approximate the reasonable induc-\ntive bias of language, with a net effect of making\nBPT easier to train.\nWe evaluate BPT on a variety of Sentence-Level\nand Document-Level NLP tasks: language mod-\neling, machine translation and text classiÔ¨Åcation.\nThe experiment results show that BPT consistently\noutperforms previous self-attention based models.\nWe also show that the inductive bias of BPT works\nnicely on short text and can scale to large datasets.\nFinally, we show BPT is faster and more mem-\nory efÔ¨Åcient than vanilla Transformer when deal-\ning with long sequence.\n2 Related Work\n2.1 Recap: Transformer\nGiven a sentence with n input tokens, the Trans-\nformer model iteratively computes at layer t the\nd-dimensional representations of each input token\nHt ‚ààRn√ód, where H0 represents the initial to-\nken embeddings. The core of a Transformer step\nis Multi-head Self-Attention (MSA), which can be\nformulated as follows:\nMSA(H) = [head1, ¬∑¬∑¬∑ , headh]WO,\nheadi = softmax\n(QiKT\ni‚àö\nd\n)\nVi,\nQi = HWQ\ni , Ki = HWK\ni , Vi = HWV\ni ,\n(1)\nwhere h is the number of heads, and WQ\ni , WK\ni ,\nWV\ni , WO are learnable parameters.\nTransformer then computes Ht+1 from Ht:\nZt = norm(Ht + MSA(Ht)), (2)\nHt+1 = norm(Zt + FFN(Zt)), (3)\nwhere norm represents the layer normalization (Ba\net al., 2016) and FFN stands for the Position-wise\nFeed-Forward Network in (Vaswani et al., 2017).\nNote that each step t has its own parameters.\n2.2 Hierarchical Attention\nSome previous work has explored the direction\nof applying self-attention on hierarchical fea-\ntures: HAN (Yang et al., 2016) exploits a two-\nlevel attention mechanism that Ô¨Årst applies self-\nattention on word features to get a sentence rep-\nresentation, then uses self-attention on sentence\nlevel features to get a document level features.\nShen et al. (2018) proposed a network struc-\ntured called ‚Äúbi-directional block self-attention\nnetwork(Bi-BloSAN)‚Äù that divides a sequence\ninto blocks, and sequentially applies intra-block\nattention and inter-block attention inside a layer.\nMiculicich et al. (2018) uses a HAN structure\nto get sentence-level feature in Transformers for\nDocument-Level Machine Translation. Different\nfrom them, our model updates hierarchical fea-\ntures synchronously inside a layer, and update\nthem iteratively by stacking layers.\n2.3 Lightweight Self-Attention\nRecently there has also been several works focus-\ning on reducing the computational cost of Self-\nAttention in Transformers: T-DMCA (Liu et al.,\n2018) reduced the memory usage by Ô¨Årst divid-\ning the sequence tokens into blocks with simi-\nlar length and performing attention inside each\nblock independently. Sparse Transformer (Child\net al., 2019) decomposes attention into two cate-\ngories: for a sequence with length n, we divide it\ninto ‚àön equal-sized blocks. Each token attends\nto its previous tokens inside a ‚àön block it lies\nin, and to ‚àön previous blocks. Compared to our\nmodel, the Sparse Transformer does not maintain\nthe representations of hierarchical features, and\nthe computational cost of Sparse Transformer is\nO(n‚àön) while ours is O(n log n). Transformer-\nXL (Dai et al., 2019) introduces the notion of\nrecurrence into Transformer. It divides the in-\nput sequence into multiple segments and recur-\nrently attends to the hidden states of the previous\nsegments. They achieved state-of-the-art on sev-\neral language modeling benchmarks. Compared\nto our model, Transformer-XL could only model\nsequences in one direction, making it hard to deal\nwith tasks where bi-directional information is re-\nquired. Sukhbaatar et al. (2019) proposed a adap-\ntive mechanism to learn optimal context length in\ntransformers for each head per layer, thus reducing\nthe total computational and memory cost of trans-\nformers. Guo et al. (2019) suggest that the fully-\nconnected nature of self-attention in Transformer\nis not a good inductive bias, they proposed Star-\nTransformer which links adjacency words coupled\nwith a central relay node to capture both local and\nglobal dependencies, with such reduction, Star-\nTransformer achieved signiÔ¨Åcant improvements\nagainst standard Transformer on moderate sized\ndatasets. However, Star-Transformer is not suit-\nable for auto-regressive models in which each\nword should only be conditioned on its previous\nwords, while the relay node in Star-Transformer\nsummarizes the whole sequence.\n3 Proposed Model\nIn this paper, we balance the model capability and\ncomputation complexity by incorporating the in-\nductive bias. The key insight is that not every\ntoken needs to be attended to for context rep-\nresentation. Instead, for an given input token,\nwe can group its context into different-scale non-\noverlapping spans, and the scale of a span in-\ncreases with its relative distance. That is, instead\nattending to every token, the input token attends\nto different spans away from it in a Ô¨Åne-to-coarse\nfashion.\nWe now describe our model as graph neural net-\nwork and detail it in the following sections.\n3.1 Transformer as Graph Neural Networks\nA valid perspective is to view information fus-\ning with self-attention in Transformer as message\npassing on a fully-connected graph, with input to-\nkens as nodes and attentions between nodes as\nedges (Battaglia et al., 2018). In particular, such\na process is very similar to Graph Attention Net-\nwork (Velickovic et al., 2018). Thus, different\ngraph structure encodes different inductive bias of\nattention and results in different time/space com-\nplexity.\nTo describe Transformer in GNN framework,\nwe Ô¨Årst construct a fully-connected graph G, in\nwhich each node is a token of the input sequence.\nAll nodes in Gare interconnected and each node\nhas a self-loop edge.\nWe extend the self-attention mechanism of\nTransformer to graph, called Graph Self-\nAttention (GSA). For a given node u, we up-\ndate its representation according to its neighbour\nnodes, formulated as hu ‚ÜêGSA(G, hu).\nLet A(u) denote the set of the neighbour nodes\nof u in G, GSA(G, hu) is detailed as follows:\nAu = concat({hv |v ‚ààA(u)}), (4)\nQu\ni = HkWQ\ni ,Ku\ni = AuWK\ni ,Vu\ni = AuWV\ni ,\n(5)\nheadu\ni = softmax\n(Qu\ni Ku\ni\nT\n‚àö\nd\n)\nVu\ni , (6)\nGSA(G, hu) = [headu\n1 , ¬∑¬∑¬∑ , headu\nh]WO, (7)\nwhere d is the dimension of h, and\nWQ\ni , WK\ni , WV\ni are trainable parameters of\nthe i-th attention head.\n3.2 Graph Construction\n3.2.1 Node Construction\nTo achieve the effect of Ô¨Åne-to-coarse attention,\nwe partition a sequence into multi-granular spans\nvia binary partitioning (BP).\nBinary partitioning is a generic process of re-\ncursively dividing a sequence into two until the\npartitioning satisÔ¨Åes one or more requirements. In\nthis paper, we use a simple rule to stop subdivid-\ning when a partition just contains a single token.\nFor a sequence with lengthn, there are 2n‚àí1 par-\ntitions. Figure 1 illustrates the process of binary\npartitioning over a sequence. Each partition can\nbe regarded as a node in GNN and its representa-\ntion is computed according to its contained tokens.\nI\twas\tbusy\twriting\tmy\tnext\tpaper\t.\nI\twas\tbusy\twriting my\tnext\tpaper\t.\t\nI\twas busy\twriting my\tnext paper\t.\nI was busy writing my next paper .\nFigure 2: Binary partitioning of a sequence.\nThe binary partitioning of a sequence constructs\na perfect binary tree in which all internal nodes\nhave two children and all leaf nodes have the same\ndepth. Each leaf node corresponds to an input to-\nken in the sequence.\nWe simply divide the nodes into two types, to-\nken and span, both of which are used as nodes in\nour GNN construction:\nToken nodes the leaf nodes in the binary partition\ntree.\nFour and seven years agoscore our forth thisonfathers brought\nùëü0,1\nùëôùëíùëìùë°\nùëü0,1\nùëüùëñùëî‚Ñéùë° ùëü0,2\nùëüùëñùëî‚Ñéùë°\nùëü1,1\nùëüùëñùëî‚Ñéùë° ùëü1,2\nùëüùëñùëî‚Ñéùë° ùëü2,1\nùëüùëñùëî‚Ñéùë°\nùëü1\nùëéùëõùëê\nùëü2\nùëéùëõùëê\nùëü3\nùëéùëõùëê\nLevel 3\nLevel 2\nLevel 1\nLevel 0\n(leaves)\na nationnewcontinent\nùëü2,2\nùëüùëñùëî‚Ñéùë°\nFour score and seven years ago our fathersLevel 4\n(root)\nyears ago our fathersFour score and seven brought forth on this continent a new nation\nyears ago our fathersFour score and seven brought forth on this continent a new nation\nFour score and seven years ago our fathers brought forth on this continent a new nation \nùëü4\nùëéùëõùëê\nùëüùë†ùëíùëôùëì\nFigure 3: The Ô¨Ågure illustrates how to build the graph: nodes at different levels are colored differently, dashed lines\nare edges connects token nodes to span nodes; solid lines are edges connect to token nodes. The r‚àó\n‚àó are relative\npositions assigned to edges.\nSpan nodes the internal node of the tree, each has\nat least two child nodes.\n3.2.2 Edge Construction\nThe binary partitioning generating a binary tree.\nFor a sequence with n tokens, we have n token\nnodes and n‚àí1 span nodes. Formally, let ul,m de-\nnote the m-th node at levell. The level increases in\nbottom-up fashion. The level of token nodes is set\nto 0. A span node ul,m represents a partition con-\nsisting of token nodes u0,2l‚àóm+1, ¬∑¬∑¬∑ , u0,2l‚àó(m+1).\nTo reduce the distance of information transmis-\nsion, we do not directly use the tree structure to\nconstruct the edges in graph since the path is long\nfor two long-distance tokens in the tree structure.\nWe construct two kinds of edges:\nAfÔ¨Åliated Edges Given a span node ul,m, we\nadd a directed edge from each of its contained to-\nken nodes u0,2l‚àóm+1, ¬∑¬∑¬∑ , u0,2l‚àó(m+1). There are\n2l edges u0,2l‚àóm+i ‚Üíul,m(1 ‚â§i ‚â§2l).\nThe role of afÔ¨Åliated edges is to shorten the path\nbetween a span node and its corresponding token\nnodes. With the afÔ¨Åliated edges, the representation\nof a span node is computed by directly aggregating\nthe information from its contained token nodes.\nAlthough we do not adopt the tree structure, the\nafÔ¨Åliated edges still can incorporate the inductive\nbias of the hierarchical linguistic structure within\nthe sentence.\nContextual Edges The power of Transformer\ncomes from relating every pair of tokens. To\nreduce the computation complexity while retain-\ning the ability to capture long-range context, we\nmodel the context with a Ô¨Åne-to-coarse strategy.\nFor a leaf node, to model its local context, we con-\nnect it to neighbor token nodes or lower-level span\nnodes. Similarly, we connect it to higher-level\nspan nodes for long-range context.\nIn detail, for a leaf node u0,i, we add the in-\ncoming edges from the different granularity. For\nsimplicity, we describe the process of construct-\ning edges from its right context of node u0,i. The\nedges from the left context is conducted similarly.\nWe use a hyper-parameter k to determine the\nconnection density of the graph. We add k edges\nper level to capture the information from the right\ncontext.\nFor node u0,i, its contextual nodes are\nu0,p0 , ¬∑¬∑¬∑ , u0,p0+k‚àí1, (8)\nu1,p1 , ¬∑¬∑¬∑ , u1,p1+k‚àí1, (9)\n¬∑¬∑¬∑ (10)\nul,pl, ¬∑¬∑¬∑ , u1,pl+k‚àí1, (11)\n¬∑¬∑¬∑ , (12)\nwhere pl is the start index at level l and can be\ncomputed recursively: pl = parent(pl‚àí1 + k) and\np0 = i + 1.\nFor the sake of computation efÔ¨Åciency, when\nthe indexpl+k‚àí1 is odd, we also add its next node\nin the same layer as the contextual nodes. Thus,\nthe start index at next level is pl+1 = parent(pl +\nk + 1).\nIn practice, it is easy to Ô¨Ånd the contextual\nnodes in a recursive fashion. Given a leaf node u,\nthe whole procedure is described in Algorithm 1.\nAfter collect all the contextual nodes, we add a\ndirected edge from each contextual node to node\nu0,i.\nAlgorithm 1 Finding contextual nodes\nfunction NEIGHBORS (u, k)\nN‚Üê{ u}, l‚Üêleft(u), r‚Üêright(u)\nrepeat\nfor i ‚Üê1 to k do\nN‚ÜêN‚à™{ l, r}\nl ‚Üêleft(l)\nr ‚Üêright(r)\nend for\nl ‚Üêparent(l)\nr ‚Üêparent(r)\nuntil l and r reach the boundary\nreturn N\nend function\nFinally, for a sequence with length n, we can\nconstruct a directed graphG. The number of nodes\nis O(2n), the number of edges is O(kn log n/k).\nWe can see that the distances between any two to-\nken nodes are no greater than 2 in graph G. This\nproperty enables our model to learn long-term de-\npendencies easily.\n3.3 Graph Update\nAfter graph Gbeing constructed, we update rep-\nresentations of all nodes via Graph Self-Attention\n(GSA) described in Section 3.1.\nSince Gis a directed graph, for a given node\nu, its neighbours A(u) is set to all its predecessor\nnodes in G. If we set A(u) to all the token nodes,\nwe recover the model to the vanilla Transformer.\nRecall that the predecessors of a token node is\nthe multi-scale spans it attending to, while the pre-\ndecessors of a span node are all its contained token\nnodes, as illustrated in Figure 3. Therefore, BPT\nconnected each two tokens via at most two edges.\nIn our experiments, we update all nodes syn-\nchronously within the graph layer. The representa-\ntions of span nodes are initialized with all zeroes,\nwhile the representations of token nodes are ini-\ntialized with the corresponding word embeddings.\nWe can stack multiple graph layers as in vanilla\nTransformer, where each layer gets its own W¬∑\n¬∑\nand WO. Algorithm 2 demonstrates the overall\nupdate algorithm.\nDepending on the downstream tasks, we either\ntake as output of representation of the root node\nin the Ô¨Ånal layer (e.g. in text classiÔ¨Åcation and\nnatural language inference), or the representations\nof all the token nodes in the Ô¨Ånal layer (e.g. in\nlanguage modeling and machine translation).\nAlgorithm 2 The update of graph\nRequire: G= (V, E) the underlying graph,N the\nnumber of layers, H0 initial hidden states\n1: for i := 1 to N do:\n2: Zi ‚Üênorm\n(\nHi‚àí1 + GSA(i) (\nG, Hi‚àí1))\n3: Hi ‚Üênorm\n(\nZi + FFN(i) (\nZi))\n4: end for\n5: return HN\n3.4 Relative Positional Encoding on Tree\nAs in (Shaw et al., 2018), introducing the relative\ndistances between words in computing the self-\nattention helps encode the relative order among to-\nkens. Here we draw a similar analogy on the tree.\nFor each node v in A(u), we consider the relative\npositional difference on the tree between u and v,\nand assign a latent representation rv,u of such dif-\nference:\n‚Ä¢rv,u = rself if v = u.\n‚Ä¢rv,u = rleft\nj,i or rright\nj,i , if v is the i-th left/right\nnode to join the neighborhood set of u at the\nj-th level in Algorithm 1 of Ô¨Ånding top-down\ncontext nodes.\n‚Ä¢rv,u = ranc\nj , if u is the ancestor of v in the\ntree at level j.\nAll the rself, rleft\nj,i , rright\nj,i and ranc\nj are trainable pa-\nrameters.\nThen, we modify Eq. (6) to include positional\nrepresentations:\nRu = concat({rv,u |v ‚ààA(u)}),\nheadu\ni = softmax\n(Qu\ni (Ku\ni + Ru)T\n‚àö\nd\n)\nVu\ni .\n(13)\nNote that the relative positional representations are\nshared across attention heads, which is the same as\nin (Shaw et al., 2018), and each layer gets its own\nset of positional representations.\nWhen k is set to be larger then the sentence\nlength, our model degenerates to Vanilla Trans-\nformer with positional encodings. In the follow-\ning section we will show that a small k (e.g. 4) is\nenough for achieving good performance in word\nlevel NLP tasks.\n4 Experiments\nWe measure the performance of BPT on variety of\ntasks at both sentence level and document level.\nOn document level tasks, we achieved state-of-\nthe-art performance on language modeling, ma-\nchine translation and text classiÔ¨Åcation. For sen-\ntence level tasks, BPT performs consistently bet-\nter then vanilla Transformer and Star Transformer,\nsuggesting the inductive bias encoded by BPT is\nreasonable and effective for natural language. The\nexperimental results show the superior ability of\nBPT in modeling the long-range context.\n4.1 Text ClassiÔ¨Åcation\nWe use SST-5 dataset (Socher et al., 2013) and\nIMDB dataset (Maas et al., 2011) to measure the\nperformance of our model on classiÔ¨Åcation for\nshort and long text. The former has Ô¨Åne-grained\nlabels with 215,154 phrases in 11,855 sentences\nwith average length of 19, and the latter has pos-\nitive/negative labels on 50,000 multi-sentence re-\nviews with average length 294. We use pre-trained\nGloVe embedding (Pennington et al., 2014) as in-\nput features and Ô¨Åxed them during training. The\nhidden size of all our models are set to 300. For\nIMDB, we apply the same training/validation set\nsplit ratio (0.9) as in McCann et al. (2017).\nModel SST-5 IMDB\nBPT 52.71(0.32) 92.12(0.11)\nStar Transformer 52.9 90.50\nTransformer 50.4 89.24\nBi-LSTM (Li et al., 2015) 49.8 -\nTree-LSTM (Socher et al., 2013) 51.0 -\nQRNN (Bradbury et al., 2017) - 91.4\nBCN+Char+CoVe (McCann et al.,\n2017)\n53.7 91.8\nTable 1: Test accuracy on SST-5 and IMDB. In BPT,\nk = 2 and k = 4 for SST and IMDB respectively.\nThe last model used word embeddings pretrained with\ntranslation and additional character-level embeddings.\nWe report the average test accuracy of BPT of\n10 runs in Table 1, the value inside brackets in-\ndicates standard derivation. On SST-5, our model\noutperforms Transformer and LSTM based mod-\nels. On IMDB, our proposed model outperforms\na bidirectional LSTM initialized with pre-trained\ncharacter embedding and CoVe embedding (Mc-\nCann et al., 2017).\nOn IMDB, our model outperforms Vanilla\nTransformer and Star Transformer by a large mar-\ngin: 1.62 and 2.88 respectively. To study the\neffect of k on Ô¨Ånal accuracy, we tried different\nk ‚àà{1, 2, 4, 8, 16, 32, 64}. Figure 4 shows a large\nk does not bring beneÔ¨Åts, though it increases the\ngraph density and time/memory cost of BPT. The\nbest performance was obtained atk = 2and k = 4\nfor SST and IMDB respectively, which is a small\nvalue.\n2 4 8 16 32 64\n50\n52\n54\n56\nk\nSST test accuracy\n86\n88\n90\n92\nIMDB test accuracy\n92.12\n52.71\nFigure 4: Effects of hyperparameter k.\n4.1.1 Sensitivity to Sequence Shift\nSince BPT divide sequence in binary fashion, a\nconcern is whether a shift in sequence affects its\nperformance. To measure if the output of BPT\nis sensitive to shift, we take the model trained\non SST with best validation loss and evaluate it\nin a setting different from training: we append n\nplaceholder symbols in the front of each sentence,\nand initialize their embedding with all zeros. We\nvaries n from 0 to 7 and found out the test accuracy\nchanges very little as shown in Table 2, suggesting\nour model is robust towards shift.\nShift Offset Test Accuracy Shift Offset Test Accuracy\n0 52.71(0.32) 4 52.18(0.22)\n1 52.50(0.29) 5 51.90(0.16)\n2 52.81(0.18) 6 51.85(0.35)\n3 52.56(0.22) 7 51.55(0.29)\nTable 2: Accuracy with different sequence shift on\nSST-5.\n4.2 Language Modeling\nTo see how BPT exploits with long-term depen-\ndencies, we evaluate our model on Character Level\nLanguage Modeling datasets of moderate size:\nEnwiki8 (LLC., 2009) and Text8 (LLC., 2009).\nWe use bits-per-character(bpc for short, the lower\nthe better) to measure the performance of our\nmodel.\nCharacter level tasks require more Ô¨Åne-grained\ninteractions between characters, we select a much\nlarger k = 64 for such tasks. The baseline mod-\nels we select are multi-scale RNN based models\n(Chung et al., 2017; Zilly et al., 2017; Krause\net al., 2016) and Transformer-based models (Al-\nRfou et al., 2018; Dai et al., 2019; Sukhbaatar\net al., 2019). All Transformers use the same base\nsetting (12 layers, d = 512, dff = 2048) for fair\ncomparison.\nModel Enwiki8 Text8 Params\nHM-LSTM (Chung et al., 2017) - 1.29 35M\nRecurrent Highway (Zilly et al.,\n2017)\n- 1.27 45M\nmLSTM (Krause et al., 2016) 1.24 1.27 45M\nTransformer (Al-Rfou et al., 2018) 1.11 1.18 44M\nTransformer-XL (Dai et al., 2019) 1.06 - 41M\nAdaptive Span (Sukhbaatar et al.,\n2019)\n1.02 1.11 39M\nBPT (k = 64, l = 8192) 1.02 1.11 38M\nTable 3: Test BPC on Enwiki8/Text8. Note that\nTransformer-XL can be only used for language mod-\neling. l denotes the context length.\nIn Table 3, we show that BPT can achieve state-\nof-the-art performance on both datasets with a\nsmall number of parameters.\nTo compare different sparse attention patterns,\nwe Ô¨Åx the context length: l = 512 and see\nhow the performance of different models varies\nas we change the attention degree (the num-\nber of incoming edges of each token in the\ncontext of viewing Transformer as Graph Neu-\nral Networks). For BPT, we select different\nk ‚àà{1, 2, 4, 8, 16, 32, 64, 128}, for Sparse Trans-\nformer (Child et al., 2019), we use the default set-\nting described in the paper (c = 8, stride = 128);\nfor Restricted Transformer (Vaswani et al., 2017)\n(restrict self-attention to a neighborhood window\nof size w), we select w ‚àà{32, 64, 128, 256, 512}.\nFigure 5 suggests that BPT‚Äôs Ô¨Åne-to-coarse\nsparse attention is more effective than Restricted\nTransformer and Sparse Transformer: with the\nsame attention degree, BPT always gets better per-\nformance.\nTo see how BPT exploits long-term depen-\ndency, we Ô¨Åxed k to 64 and varies context length\nin {512, 1024, 2048, 4096, 8192}. We do not try\ncontext length longer than 8192 because its ex-\nceeds the average article length in Enwik8 and\nText8. As shown in Table 4, the performance in-\ncreases with the context length.\n100 200 300 400 500\n1.1\n1.2\n1.3\nAttention Degree\nbits per character (BPC)\nBPT\nRestricted Attention\nSparse Transformer\nFigure 5: Test BPC on Enwiki8 with different k.\nContext Length Enwik8 Text8\n512 1.07 1.16\n1024 1.05 1.14\n2048 1.03 1.13\n4096 1.02 1.12\n8192 1.02 1.11\nTable 4: Test BPC on Enwiki8/Text8 with different\ncontext lengths.\n4.3 Machine Translation\nBPT can also be applied to Encoder-Decoder\nframeworks by replacing backbone network in\nVaswani et al. (2017) from Transformer to BPT.\nIn this section we evaluate two settings:\nDocument-Level and Sentence-Level Machine\nTranslation. In Document-Level Machine Trans-\nlation tasks, the self-attention in both encoder and\ndecoder are applied at document level, while the\nattention between encoder and decoder are applied\nbetween aligned sentences. For a mini-batch of\nsentence pairs with source sentences of lengths\n{ni}and target sentences of lengths {mi}, the\nnumber of connections are ‚àë\ni kni log(‚àë\ni ni/k)\nfor encoder, ‚àë\ni kmi log(‚àë\ni mi/k) for decoder,\nand ‚àë\ni ni ¬∑mi for attention between encoder and\ndecoder.\n4.3.1 Document Level Machine Translation\nWe conduct experiments with TED Talks Chinese-\nto-English(Zh-En) dataset from IWSLT 2014 and\n2015 (Cettolo et al., 2012, 2016), the average doc-\nument length is 120 (in sentences). For each\nsentence, we take its preceding context of Ô¨Åxed\nlength, and their corresponding translations as a\nsingle sample.\nThe baseline models are HAN-NMT (Miculi-\ncich et al., 2018) and Transformer+cache (Tu\net al., 2018). We follow the setting of Miculicich\net al. (2018) with a vocabulary size of 30k for both\nChinese and English, and use dev2010 for devel-\nopment and tst2010-2013 for testing. Unlike pre-\nvious models, our model is trained from scratch\nand do not require pre-training on sentence-level\ntranslation tasks.\nModel BLEU\nTransformer (Vaswani et al., 2017) 16.87\nTransformer+cache (Tu et al., 2018) 17.32\nHAN-NMT (Miculicich et al., 2018) 17.78\nTransformer (ours, single sentence) 18.91\nBPT (k = 4, single sentence) 19.19\nBPT (k = 4, l= 64) 19.84\nTable 5: BLEU score on IWSLT 2015 Zh-En\nIn Table 5 we show that with careful selec-\ntion of hyper-parameters, Transformer trained at\nsentence-level could beat reported results of pre-\nvious Document-Level models. BPT with k = 4\nand context length of 32 could further improve the\nbaseline result by 0.93 in terms of BLEU score,\nwhich is a signiÔ¨Åcant margin.\nWe also examine the effect of context length\nand k on Ô¨Ånal BLEU scores, the results are shown\nin Table 6. Similar to Tu et al. (2018) and Mi-\nculicich et al. (2018), we found a small context\nlength is enough for achieving good performance\non IWSLT for Document-Level Translation. How-\never, as we increases context size, the performance\nof BPT does not get worse as these models and\nTransformers, suggesting the inductive bias en-\ncoded by BPT makes the model less likely to over-\nÔ¨Åt.\nContext length 0 32 64 128\nTransformer 18.85 18.66 17.59 15.55\nBPT (k=4) 19.19 19.84 19.71 19.84\nBPT (k=8) 19.13 19.59 19.78 19.60\nTable 6: BLEU score vs context length on different\nmodels\n4.3.2 Sentence Level Machine Translation\nIWSLT is a relatively small dataset with 0.21M\nsentence pairs, to see if BPT scales to large\ndataset, we train a BPT on WMT14 English-to-\nGerman dataset with 4.5M sentence pairs.\nWe follow the same setting as (Vaswani et al.,\n2017), but to replace the Transformer encoder/de-\ncoder with a BPT encoder/decoder. The number of\nparameters remains the same. The baseline model\nwe select is Transformer(base). We trained the\nnetwork for 40 epochs and take the average of last\n10 checkpoint for decoding, the beam size is set to\n5.\nModel BLEU\nByteNet (Kalchbrenner et al., 2016) 23.75\nGNMT+RL (Wu et al., 2016) 24.6\nConvS2S (Gehring et al., 2017) 25.16\nTransformer (Vaswani et al., 2017) 27.3\nTransformer (our implementation) 27.2\nBPT (k = 1) 26.9\nBPT (k = 2) 27.4\nBPT (k = 4) 27.6\nBPT (k = 8) 26.7\nTable 7: BLEU score on newstest 2014\nTable 7 report the de-tokenized SacreBLEU\nscore 2 (Post, 2018) of BPT and Vanilla Trans-\nformer on test set: newstest 2014. In the setting\nof k = 2 and k = 4, BPT outperforms Vanilla\nTransformer with the same number of parameters\nand a sparse attention pattern.\nThe best setting of BPT on WMT14 is k = 4,\nthe same as the best setting of BPT on Document-\nLevel Machine Translation(IWSLT) and Text\nClassiÔ¨Åcation(IMDB), suggesting k = 4a general\nsetting for word-level NLP tasks, on both small\nand large datasets.\n4.4 Throughput and GPU Memory Footprint\nBPT improves the time/space complexity of\nTransformer models from O(d ¬∑n2) to O(d ¬∑\nk ¬∑n log n/k) in theory, such speedup cannot be\nachieved by tensor-based attention operators. To\naddress this problem, we designed a set of CUDA\nkernels for sparse attentions3.\nWe compare the GPU memory footprint and\nthroughput of BPT and vanilla Transformer dur-\ning inference under the same setting4 for language\nmodeling. The k is set to 1, 4, 16, 64 respectively,\ncovering best settings for word-based tasks(k = 4)\nand character-based tasks( k = 64). We Ô¨Åx the\nnumber of tokens to 8192 each batch and varies\nthe sequence length. Figure 6 and 7 depicts how\nthe GPU memory and speed varies as we increases\n2Setting: BLEU+c.mixed+l.en-de+#.1+s.exp+t\n.wmt14+tok.intl+v.1.4.1\n3the speed of BPT could be further improved with better\noptimized kernels\n4N = 6, d = 512, dff = 2048, h = 8\nsequence length.\n2,000 4,000 6,000 8,000\n2\n4\n6\n8\n10\nlength of sequence\nGPU Memory(GB)\nTransformer\nBPT(k = 1)\nBPT(k = 4)\nBPT(k = 16)\nBPT(k = 64)\nFigure 6: GPU memory cost vs sequence lengh\n2,000 4,000 6,000 8,000\n0.5\n1\n¬∑105\nlength of sequence\nThroughput (tokens/s)\nTransformer\nBPT(k = 1)\nBPT(k = 4)\nBPT(k = 16)\nBPT(k = 64)\nFigure 7: Throughput vs sequence length\nWe show that BPT consistently utilizes less\nGPU memory compared to Transformer, making\nit possible to be applied on tasks that require long\nsequence modeling such as time-series prediction.\nAs for speed, BPT increases the number of\nnodes from n to 2n which brings additional over-\nhead linear to sequence length, rendering BPT not\nas fast as Transformer when dealing with short\ntext. However, as the sequence length grows, the\nspeed of BPT is steady while Transformer become\ntoo slow for practical use.\n5 Conclusion and Future Work\nThis paper introduces a hierarchical Ô¨Åne-to-coarse\nself-attention based model that is versatile and\nÔ¨Çexible for a variety of natural language process-\ning tasks. By imposing structural inductive bias\nthis way we are able to strike a balance between\nthe power of the model and training/computational\nefÔ¨Åciency.\nThis work can be extended in a number of inter-\nesting ways. The representations have not yet nat-\nurally captured syntactic and semantic meanings.\nInstead of only using the root and the token repre-\nsentations, other intermediate representations can\nbe more directly exposed.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst,\nAlvaro Sanchez-Gonzalez, Vinicius Zambaldi, Ma-\nteusz Malinowski, Andrea Tacchetti, David Raposo,\nAdam Santoro, Ryan Faulkner, et al. 2018. Rela-\ntional inductive biases, deep learning, and graph net-\nworks. arXiv preprint arXiv:1806.01261.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-recurrent neural net-\nworks. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nMauro Cettolo, Christian Girardi, and Marcello Fed-\nerico. 2012. Wit3: Web inventory of transcribed and\ntranslated talks. In Conference of European Associ-\nation for Machine Translation, pages 261‚Äì268.\nMauro Cettolo, Niehues Jan, St ¬®uker Sebastian, Luisa\nBentivogli, Roldano Cattoni, and Marcello Federico.\n2016. The iwslt 2016 evaluation campaign. In In-\nternational Workshop on Spoken Language Transla-\ntion.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. URL\nhttps://openai.com/blog/sparse-transformers.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2017. Hierarchical multiscale recurrent neural net-\nworks. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a Ô¨Åxed-length context. arXiv\npreprint arXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N. Dauphin. 2017. Convo-\nlutional sequence to sequence learning. In Pro-\nceedings of the 34th International Conference on\nMachine Learning, ICML 2017, Sydney, NSW, Aus-\ntralia, 6-11 August 2017, pages 1243‚Äì1252.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019. Star-\ntransformer. arXiv preprint arXiv:1902.09113.\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\nAaron van den Oord, Alex Graves, and Koray\nKavukcuoglu. 2016. Neural machine translation in\nlinear time. arXiv preprint arXiv:1610.10099.\nBen Krause, Liang Lu, Iain Murray, and Steve Renals.\n2016. Multiplicative lstm for sequence modelling.\narXiv preprint arXiv:1609.07959.\nJiwei Li, Minh-Thang Luong, Dan Jurafsky, and Eu-\ndard Hovy. 2015. When are tree structures necessary\nfor deep learning of representations? arXiv preprint\narXiv:1503.00185.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. In 6th International Conference\non Learning Representations, ICLR 2018, Vancou-\nver, BC, Canada, April 30 - May 3, 2018, Confer-\nence Track Proceedings.\nYang Liu and Mirella Lapata. 2019. Hierarchical trans-\nformers for multi-document summarization. arXiv\npreprint arXiv:1905.13164.\nMultiMedia LLC. 2009. Large text compression\nbenchmark.\nAndrew L Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the as-\nsociation for computational linguistics: Human lan-\nguage technologies-volume 1, pages 142‚Äì150. Asso-\nciation for Computational Linguistics.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6294‚Äì6305.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 2947‚Äì2954, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532‚Äì1543.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186‚Äì\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 2 (Short Papers),\npages 464‚Äì468.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nand Chengqi Zhang. 2018. Bi-directional block self-\nattention for fast and memory-efÔ¨Åcient sequence\nmodeling. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631‚Äì1642.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive\nattention span in transformers. arXiv preprint\narXiv:1905.07799.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong\nZhang. 2018. Learning to remember translation his-\ntory with a continuous cache. Transactions of the\nAssociation for Computational Linguistics, 6:407‚Äì\n420.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998‚Äì6008.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li `o, and Yoshua Bengio.\n2018. Graph attention networks. In 6th Inter-\nnational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings.\nMinjie Wang, Lingfan Yu, Da Zheng, Quan Gan,\nYu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang,\nChao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang,\nHaibin Lin, Junbo Zhao, Jinyang Li, Alexander J.\nSmola, and Zheng Zhang. 2019. Deep graph li-\nbrary: Towards efÔ¨Åcient and scalable deep learning\non graphs. CoRR, abs/1909.01315.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google‚Äôs neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchi-\ncal attention networks for document classiÔ¨Åcation.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1480‚Äì1489.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan\nKoutn¬¥ƒ±k, and J¬®urgen Schmidhuber. 2017. Recurrent\nhighway networks. In Proceedings of the 34th In-\nternational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017,\npages 4189‚Äì4198.\nA Appendix\nA.1 Implementation Details\nWe use Deep Graph Library (Wang et al., 2019)\nfor building Binary Partition graphs.\nThe following table summarizes the hyper-\nparameters used in BPT.\nnotation meaning\nBtok number of tokens in a batch\nBsent number of sentences in a batch\nN number of (encoder) layers.\nM number of (decoder) layers.\nh number of heads.\nk connection density in BPT\ndemb embedding size\nd hidden size of the model\ndff Ô¨Ålter size in FFN sublayer\npi dropout rate on embedding layer\nph dropout rate on hidden layers\npa dropout rate on attention weight\npc dropout rate before classiÔ¨Åer\navg model average checkpoints\nsteps training steps\nepochs training epochs\nTable 8, 9 and 10 lists the hyper-parameters we\nuse in Text ClassiÔ¨Åcation, language Modeling and\nMachine Translation respectively.\nSST IMDB\nN 4 5\ndemb 300 300\nd 300 300\ndff 600 600\nh 6 6\npi 0.4 0.5\nph 0.1 0.1\npa 0.3 0.3\npc 0.4 0.5\nBsent 1024 32\nepochs 40 40\nTable 8: Hyper-parameters for Text ClassiÔ¨Åcation\nenwik8 text8\nN 12 12\ndemb 512 512\nd 512 512\ndff 2048 2048\nh 8 8\npi 0.1 0.1\nph 0.1 0.1\npa 0.1 0.1\npc 0.1 0.1\nBtok 32768 32768\nsteps 400000 600000\nTable 9: Hyper-parameters for Language Modeling\nIWSLT WMT\nN 6 6\nM 6 6\ndemb 512 512\nd 512 512\ndff 2048 2048\nh 8 8\npi 0.1 0.1\nph 0.1 0.1\npa 0.1 0.1\npc 0.1 0.1\nBsent 128 1024\navg 10 10\nepochs 40 40\nTable 10: Hyper-parameters for Machine Translation\nFor full details please refer to the conÔ¨Ågurations\nin our source code: https://github.com/\nyzh119/BPT/tree/master/configs.",
  "topic": "Hyperparameter",
  "concepts": [
    {
      "name": "Hyperparameter",
      "score": 0.8075942993164062
    },
    {
      "name": "Computer science",
      "score": 0.6337188482284546
    },
    {
      "name": "Binary number",
      "score": 0.5413569808006287
    },
    {
      "name": "Computation",
      "score": 0.5080320835113525
    },
    {
      "name": "Transformer",
      "score": 0.5041133165359497
    },
    {
      "name": "Quadratic equation",
      "score": 0.4760946035385132
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4041975736618042
    },
    {
      "name": "Algorithm",
      "score": 0.37464839220046997
    },
    {
      "name": "Theoretical computer science",
      "score": 0.37045201659202576
    },
    {
      "name": "Mathematics",
      "score": 0.21836596727371216
    },
    {
      "name": "Arithmetic",
      "score": 0.1770874559879303
    },
    {
      "name": "Physics",
      "score": 0.10553866624832153
    },
    {
      "name": "Voltage",
      "score": 0.08233523368835449
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": []
}