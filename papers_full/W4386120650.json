{
    "title": "Evaluating large language models on medical evidence summarization",
    "url": "https://openalex.org/W4386120650",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2100224089",
            "name": "Liyan Tang",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2233794630",
            "name": "Zhaoyi Sun",
            "affiliations": [
                "Cornell University",
                "Weill Cornell Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4286553684",
            "name": "Betina Idnay",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2765106379",
            "name": "Jordan G. Nestor",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2036204426",
            "name": "ali soroush",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A4366915027",
            "name": "Pierre A. Elias",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2155053510",
            "name": "Ziyang Xu",
            "affiliations": [
                "Massachusetts General Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2099000120",
            "name": "Ying Ding",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A1978278429",
            "name": "Greg Durrett",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2787694935",
            "name": "Justin F. Rousseau",
            "affiliations": [
                "The University of Texas at Austin",
                "The University of Texas Southwestern Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2113027333",
            "name": "Chunhua Weng",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2099815532",
            "name": "Yifan Peng",
            "affiliations": [
                "Weill Cornell Medicine",
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2100224089",
            "name": "Liyan Tang",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2233794630",
            "name": "Zhaoyi Sun",
            "affiliations": [
                "Cornell University",
                "Weill Cornell Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4286553684",
            "name": "Betina Idnay",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2765106379",
            "name": "Jordan G. Nestor",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2036204426",
            "name": "ali soroush",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A4366915027",
            "name": "Pierre A. Elias",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2155053510",
            "name": "Ziyang Xu",
            "affiliations": [
                "Massachusetts General Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2099000120",
            "name": "Ying Ding",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A1978278429",
            "name": "Greg Durrett",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2787694935",
            "name": "Justin F. Rousseau",
            "affiliations": [
                "The University of Texas Southwestern Medical Center",
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2113027333",
            "name": "Chunhua Weng",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2099815532",
            "name": "Yifan Peng",
            "affiliations": [
                "Weill Cornell Medicine",
                "Cornell University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4221143046",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4367051110",
        "https://openalex.org/W2012107441",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2962849707",
        "https://openalex.org/W3159259047",
        "https://openalex.org/W4385572316",
        "https://openalex.org/W2937477683",
        "https://openalex.org/W2911471385",
        "https://openalex.org/W6634199162",
        "https://openalex.org/W4297458847",
        "https://openalex.org/W4285299836",
        "https://openalex.org/W3199259836",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4285110479",
        "https://openalex.org/W4387356888",
        "https://openalex.org/W2034978010",
        "https://openalex.org/W4225491237",
        "https://openalex.org/W1575164208",
        "https://openalex.org/W4319080205"
    ],
    "abstract": null,
    "full_text": "ARTICLE OPEN\nEvaluating large language models on medical evidence\nsummarization\nLiyan Tang1, Zhaoyi Sun 2, Betina Idnay 3, Jordan G. Nestor 4, Ali Soroush 4, Pierre A. Elias 3, Ziyang Xu5, Ying Ding1,\nGreg Durrett6, Justin F. Rousseau 7,8 ✉, Chunhua Weng 3 ✉ and Yifan Peng 2 ✉\nRecent advances in large language models (LLMs) have demonstrated remarkable successes in zero- and few-shot performance on\nvarious downstream tasks, paving the way for applications in high-stakes domains. In this study, we systematically examine the\ncapabilities and limitations of LLMs, speciﬁcally GPT-3.5 and ChatGPT, in performing zero-shot medical evidence summarization\nacross six clinical domains. We conduct both automatic and human evaluations, covering several dimensions of summary quality.\nOur study demonstrates that automatic metrics often do not strongly correlate with the quality of summaries. Furthermore,\ninformed by our human evaluations, we deﬁne a terminology of error types for medical evidence summarization. Ourﬁndings\nreveal that LLMs could be susceptible to generating factually inconsistent summaries and making overly convincing or uncertain\nstatements, leading to potential harm due to misinformation. Moreover, weﬁnd that models struggle to identify the salient\ninformation and are more error-prone when summarizing over longer textual contexts.\nnpj Digital Medicine          (2023) 6:158 ; https://doi.org/10.1038/s41746-023-00896-7\nINTRODUCTION\nFine-tuned pre-trained models have been the leading approach in\ntext summarization research, but they often require sizable\ntraining datasets which are not always available in speci ﬁc\ndomains, such as medical evidence in the literature. The recent\nsuccess of zero- and few-shot prompting with large language\nmodels (LLMs) has led to a paradigm shift in NLP research\n1–4. The\nsuccess of prompt-based models (GPT-3.55 and recently ChatGPT)\nbrings new hope for medical evidence summarization, where the\nmodel can follow human instructions and summarize zero-shot\nwithout updating parameters. Medical evidence summarization\nrefers to the process of extracting and synthesizing key\ninformation from a large number of medical research studies\nand clinical trials into a concise and comprehensive summary.\nWhile recent work has analyzed and evaluated this strategy for\nnews summarization\n6 and biomedical literature abstract genera-\ntion7, there is no study yet on medical evidence summarization\nand appraisal.\nIn this study, we conduct a systematic study of the potential\nand possible limitations of zero-shot prompt-based LLMs on\nmedical evidence summarization using GPT-3.5 and ChatGPT\nmodels. We then explore their impact on the summarization of\nmedical evidence ﬁndings in the context of evidence synthesis\nand meta-analysis.\nRESULTS\nStudy overview\nWe used Cochrane Reviews obtained from the Cochrane Library\nand focused on six distinct clinical domains—Alzheimer’s disease,\nKidney disease, Esophageal cancer, Neurological conditions, Skin\ndisorders, and Heart failure (Table1). Domain experts veriﬁed each\nreview to conﬁrm that they have signiﬁcant research objectives.\nIn our study, we tackle the single-document summarization\nsetting where we focus on the abstracts of Cochrane Reviews.\nAbstracts of Cochrane Reviews can be read as stand-alone\ndocuments\n8. They summarize the key methods, results and\nconclusions of the review. An abstract does not contain any\ninformation that is not in the main body of the review, and the\noverall messages should be consistent with the conclusions of the\nreview. In addition, abstracts of Cochrane Reviews are freely\navailable on the Internet (e.g., MEDLINE). As some readers may be\nunable to access the full review, abstracts may be the only source\nreaders have to understand the review results.\nEach abstract includes the Background, Objectives of the\nreview, Search methods, Selection criteria, Data collection and\nanalysis, Main results, and Author ’s conclusions. The average\nlength of the abstracts we evaluated can be found in Table1.W e\nchose the Author’s Conclusions section, the last section of the\nabstract, as the human reference summary for the Cochrane\nReviews in our study. This section contains the most salient details\nof the studies analyzed within the speci ﬁc clinical context.\nClinicians often consult the conclusions ﬁrst when seeking\nanswers to a clinical question, before deciding whether to read\nthe full abstract and subsequently the entire study. It also allows\nthe authors to interpret the evidence presented in the review,\nassess the strength of the evidence, and provide their own\nconclusions or recommendations concerning the ef ﬁcacy and\nsafety of the intervention under review.\nWe assessed the zero-shot performance of medical evidence\nsummarization using two models: GPT-3.5\n5 (text-davinci-003) and\nChatGPT9. To evaluate the models’ capabilities, we designed two\ndistinct experimental setups. In theﬁrst setup, the models were\n1School of Information, The University of Texas at Austin, Austin, TX, USA.2Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, USA.3Department of\nBiomedical Informatics, Columbia University, New York, NY, USA.4Department of Medicine, Columbia University, New York, NY, USA.5Department of Medicine, Massachusetts\nGeneral Hospital, Boston, MA, USA.6Department of Computer Science, The University of Texas at Austin, Austin, TX, USA.7Departments of Population Health and Neurology, Dell\nMedical School, The University of Texas at Austin, Austin, TX, USA.8Department of Neurology, University of Texas Southwestern Medical Center, Dallas, TX, USA.\n✉email: justin.rousseau@utsouthwestern.edu; cw2384@cumc.columbia.edu; yip4002@med.cornell.edu\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\ngiven the entire abstract, excluding the Author’s Conclusions\n(ChatGPT-Abstract). In the second setup, the models received both\nthe Objectives and the Main Results sections from the abstract as\nthe input (ChatGPT-MainResult and GPT3.5-MainResult). Here, we\nchose Main Results as the input document because it includes the\nﬁndings of all important bene ﬁt and harm outcomes. It also\nsummarizes the impact of the risk of bias on trial design, conduct,\nand reporting. We did not evaluate GPT3.5-Abstract because our\npilot study indicated that ChatGPT-MainResult performs generally\nbetter than ChatGPT-Abstract. In both settings, we used[input] +\n“Based on the Objectives, summarize the above systematic review in\nfour sentences ” as a prompt, emphasizing the importance of\nreferring to theObjectives section for aspect-based summarization.\nWe decided to summarize the review into four sentences since it is\nclose to the length of human reference summaries on average.\nThe average words of the generated summary for each system are\n107, 111, and 95 for ChatGPT-MainResult, ChatGPT-Abstract, and\nGPT3.5-MainResult, respectively. More details can be found in\nSupplementary Table 1.\nAutomatic evaluation\nTo evaluate the quality of the automatically generated summaries,\nwe employed various automatic metrics (Fig. 1a), including\nROUGE-L\n10, METEOR11, and BLEU12, comparing them against a\nreference summary. Their values range from 0.0 to 1.0, with a\nscore of 1.0 indicating the generated summaries are identical to\nthe reference summary. Ourﬁndings reveal that all models exhibit\nsimilar performance with respect to these automatic metrics. A\nrelatively high ROUGE score demonstrates that these models can\neffectively capture the key information from the source document.\nIn contrast, a low BLEU score implies that the generated summary\nis written differently from the reference summary. Consistent\nMETEOR scores across the models suggest that the summaries\nmaintain a similar degree of lexical and semantic similarity to the\nreference summary.\nWe also assessed the degree of abstraction by measuring the\nextractiveness\n13 and the percentage of novel n-grams in the\nsummary with respect to the input. Compared to human-written\nsummaries, those generated by LLMs tend to be more extractive,\nexhibiting signiﬁcantly lower n-gram novelty (Fig. 1b). Notably,\nChatGPT-MainResult demonstrates a higher level of abstraction\ncompared to ChatGPT-Abstract and GPT3.5-MainResult, but there\nremains a substantial gap between ChatGPT-MainResult and\nhuman reference. Finally, approximately half of the reviews are\nfrom 2022 and 2023, which is after the cutoff date of GPT3.5 (June\n2021) and ChatGPT (September 2021). However, we observed no\nsigniﬁcant difference in quality metrics before and after 2022.\nHuman evaluation\nTo obtain a comprehensive understanding of the summarization\ncapabilities of LLMs, we conducted an extensive human evalua-\ntion of the model-generated summaries, which goes beyond the\ncapabilities of automatic metrics\n14. Speci ﬁcally, the lack of\nstandardized terminology of error types for medical evidence\nsummarization necessitated our use of human evaluation to\ninvent new error deﬁnitions. Our evaluation methods drew from\nqualitative methods in grounded theory, which involved open\ncoding of qualitative descriptions of factual inconsistencies,\nfurther contributing to the development of error de ﬁnitions.\nAdditionally, we included a measure of perceived potential for\nharm, as it is a clinically relevant outcome that automatic metrics\nare unable to capture. Our evaluation deﬁned summary quality\nalong four dimensions: (1) Coherence; (2) Factual Consistency; (3)\nComprehensiveness; and (4) Harmfulness and the results are\npresented in Fig.2a–d.\nCoherence refers to the capability of a summary to create a\ncoherent body of information about a topic through connections\nbetween sentences. Figure2a shows that annotators rated most of\nthe summaries as coherent. Speciﬁcally, summaries generated by\nChatGPT are more cohesive than those generated by GPT3.5-\nMainResult (64% vs. 55% in Strong agreement).\nFactual Consistency measures whether the statements in the\nsummary are supported by the source document. As illustrated in\nFig. 2b, fewer than 10% of summaries produced by ChatGPT-\nMainResult exhibit factual inconsistency errors, which is signiﬁ-\ncantly lower compared to those generated by other LLM\nconﬁgurations. Medical evidence summaries should be perfectly\naccurate. To understand the types of factual inconsistency errors\nthat LLMs produce, we categorize these errors into three types of\nerrors using an open coding approach on annotators’ comments\n(Supplementary Fig. 1). Examples can be found in Supplementary\nTable 2.\nComprehensiveness refers to whether a summary contains\ncomprehensive information to support the systematic review. As\nshown in Fig.2c, both ChatGPT-MainResult and ChatGPT-Abstract\nprovide comprehensive summaries more than 75% of the time,\nwith ChatGPT-MainResult having signi ﬁcantly more summaries\nannotated as Strongly Agree. In contrast, GPT3.5-MainResult\ngenerates noticeably less comprehensive summaries. It would\nbe possible that extending the length of the summary would lead\nto a more comprehensive summary. However, ChatGPT-\nMainResult strikes a balance between providing enough informa-\ntion and being concise. The next evaluation was conducted to\ndetermine whether the omission of information relevant to the\nobjectives might lead to medical harm.\nHarmfulness refers to the potential of a summary to cause\nphysical or psychological harm or undesired changes in therapy or\ncompliance due to the misinterpretation of information. Figure2d\nshows the error type distributions from summaries that contain\nharmful information, with ChatGPT-MainResult generating the\nfewest medically harmful summaries (<10%).\nSupplementary Fig. 2 further breaks down the human evalua-\ntion for each domain. We observe annotation variations across six\ndifferent clinical domains, and these variations can be attributed\nto several factors. (1) The complexity of speciﬁc domains or review\ntypes may contribute to the observed variability, as some may be\nless complex than others, making it easier for LLMs to summarize.\n(2) Domain experts might evaluate the summaries according to\ntheir unique internal interpretations of quality metrics. (3)\nIndividual preferences may inﬂuence the decision on what key\ninformation should be incorporated in the summary.\nHuman preference\nFigure 3 shows the percentage of times humans express a\npreference for summaries generated by a speciﬁc summarization\nTable 1. Characteristics of the summarization dataset used for human\nevaluation.\nDomain # Review #w Abstract #w Main\nresults\n#w Conclusion\nAlzheimer’s\ndisease\n10 678 449 114\nKidney disease 10 887 564 95\nNeurological\nconditions\n10 791 480 106\nSkin disorders 9 1008 763 138\nHeart failure 7 804 542 103\nEsophageal\ncancer\n7 632 397 119\n#w— the average number of words.\nL. Tang et al.\n2\nnpj Digital Medicine (2023)   158 Published in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nmodel. Notice that we allow multiple summaries to be selected as\nthe most or least preferred for each source document. As shown in\nFig. 3a, ChatGPT-MainResult is signiﬁcantly more preferred among\nthe three LLMs con ﬁgurations, generating the most preferred\nsummaries approximately half of the time, outperforming its\ncounterparts by a considerable margin. In Fig.3b, we categorize\nthe considerations driving such preference. Weﬁnd that ChatGPT-\nMainResult is favored because it produces the most comprehen-\nsive summary and includes more salient information. In Fig.3c, the\nleading reasons for choosing a summary as the least preferred are\nmissing important information, fabricated errors, and misinterpre-\ntation errors. This aligns with ourﬁnding that ChatGPT-MainResult\nis the most preferred since it commits the fewest amount of\nfactual inconsistency errors and contains the least harmful or\nmisleading statements.\nDISCUSSION\nAre existing automatic metrics well-suited for evaluating medical\nevidence summaries? Research has demonstrated that automatic\nmetrics often do not strongly correlate with the quality of\nsummaries\n14. Moreover, there is no off-the-shelf automatic\nevaluation metric speciﬁcally designed to assess the factuality of\nsummaries generated by the most recent summarization\nsystems6,15. We believe this likely extends to the absence of a\ntailored factuality metric for evaluating medical evidence summa-\nries generated by LLMs as well. In our study, we observed similar\nresults for three model settings when using automatic metrics,\nwhich fall short of accurately measuring factual inconsistency,\npotential for medical harmfulness, or human preference for LLM-\ngenerated summaries. Therefore, human evaluation becomes an\nessential component to properly assess the quality and factuality\nof medical evidence summaries generated by LLMs at this time,\nand more effective automatic evaluation methods should be\ndeveloped for thisﬁeld.\nWhat causes Factual Inconsistency? We categorize factual\ninconsistency errors into three types of errors using an open\ncoding approach on annotators’ comments (Supplementary Fig.\n2). Examples can be found in Supplementary Table 1.\nFirst, through our qualitative analysis of the annotators'\ncomments, we discover that auto-generated summaries often\ncontain Misinterpretation errors. These errors can be problematic,\nas readers might trust the summary ’s accuracy without being\naware of the potential for falsehoods or distortions. To better\nunderstand these errors, we further categorize them into two\nmain subtypes. Theﬁrst is Contradiction, which arises when there\nis a discrepancy between the conclusions drawn from the medical\nevidence results and the summary. For example, a summary might\nassert that atypical antipsychotics are effective on psychosis in\ndementia, whereas the review indicates that the effect is\nnegligible\n16. The other is theCertainty Illusion, which occurs when\nthere is an inconsistency in the degree of certainty between the\nsummary and the source document. Such errors may cause\nsummaries to be overly convincing or uncertain, potentially\nleading readers to rely too heavily on the accuracy of the\npresented information. For instance, the abstract of Cochrane\nReview\n17 asserts moderate-certainty evidence that endovascular\ntherapy (ET) plus conventional medical treatment (CMT) compared\nto CMT alone causes a higher risk of short‐term stroke and death.\nHowever, we found that the generated summary conveys low-\nquality conﬁdence.\nFabricated errorsarise when a statement appears in a summary,\nbut no evidence from the source document can be found to\nsupport or refute the statement. For instance, a summary states\nthat exercise could enhance satisfaction and quality of life for\npatients with chronic neck pain, but the review does not mention\nthose two outcomes for patients\n18. Interestingly, in our human\nevaluation, we did not ﬁnd ChatGPT producing any fabricated\nerrors.\nFinally, Attribute errors refer to any errors on non-key elements\nin the review question (i.e., Patient/Problem, Intervention,\nComparison, and Outcome) and may arise in summaries under\nthree circumstances: (a) Fabricated attribute : This error occurs\nwhen a summary incorporates an attribute for a speciﬁc symptom\nor outcome that is not referenced in the source document. For\nexample, a review\n17 draws conclusions about a population with\nintracranial artery stenosis (ICAS), but the summary states the\npopulation with recent symptomatic severe ICAS, where“recent”\n0.0\n0.1\n0.2\n0.3\nROUGE−L METEOR BLEU\nReference−based Metricsa\n0.00\n0.25\n0.50\n0.75\nExtractiveness % novel 1−gram % novel 2−gram % novel 3−gram\nExtractiveness Metricsb\nChatGPT−MainResult ChatGPT−Abstract GPT3.5−MainResult Reference−MainResult\nFig. 1 Performance of different summarization systems in automatic evaluations. aReference-based metrics (higher scores indicate better\nsummaries). b Extractiveness metrics.\nL. Tang et al.\n3\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   158 \nand “severe” cannot be inferred from the document and would\nimpact one’s interpretation of the review; (b) Omitted attribute:\nThis error occurs when a summary neglects an attribute for a\nspeciﬁc symptom or outcome, such as neglecting to specify the\nsubtype of dementia discussed in the review, leading to\novergeneralization of the conclusion\n16; and (c)Distorted attribute:\nThis error occurs when the speciﬁed attribute is incorrect, like\nstating four trials are included in the study while the source\ndocument indicates that only two trials are included19.\nWe observed that ChatGPT-MainResult has the lowest propor-\ntion of all three types of errors when compared to the other two\nmodel conﬁgurations. Moreover, it is important to note that LLMs\ngenerally generate summaries with few fabricated errors. This is a\npromising ﬁnding, as it is crucial for generated statements to be\nsupported by source documents. However, LLMs do display a\nnoticeable occurrence of attribute errors and misinterpretation\nerrors, with ChatGPT-Abstract and GPT3.5-MainResult displaying a\nhigher incidence of the latter. Drawing inaccurate conclusions or\nconveying incorrect certainty regarding evidence could lead to\nmedical harm as shown in later sections.\nWhat causes medical harmfulness? We further identify three\nreasons that could potentially cause medical harmfulness:\nmisinterpretation errors, fabricated errors, and incomprehensive-\nness (such as missing Population, Intervention, Comparison,\nOutcome elements (PICO)). Notably, we did notﬁnd any instances\nof medical harmfulness resulting from attribute errors in the\nn.s. *\n***\n0.0\n0.5\n1.0\nChatGPT−MainResult ChatGPT−Abstract GPT3.5−MainResult\nCoherencea\nn.s. ****\n****\n0.0\n0.5\n1.0\nChatGPT−MainResult ChatGPT−Abstract GPT3.5−MainResult\nFactual Consistencyb\n**** ****\n****\n0.0\n0.5\n1.0\nChatGPT−MainResult ChatGPT−Abstract GPT3.5−MainResult\nComprehensivenessc\n** n.s.\n****\n0.0\n0.5\n1.0\nChatGPT−MainResult ChatGPT−Abstract GPT3.5−MainResult\nHarmfulnessd\nStrongly\nAgree Agree Neutral Disagree Strongly\nDisagree\nFig. 2 Performance of different summarization systems in human evaluations. aCoherence, b factual consistency,c comprehensiveness,\nand d harmfulness. Statistical analysis by Mann–Whitney U test, *p-value ≤ 0.05, **p-value ≤ 0.01, ***p-value ≤ 0.001, ****p-value ≤ 0.0001.\nL. Tang et al.\n4\nnpj Digital Medicine (2023)   158 Published in partnership with Seoul National University Bundang Hospital\nsummaries we analyzed. However, given the limited number of\nsummaries we examined, we cannot deﬁnitively conclude that\nattribute errors could never cause harm. Our study suggests that\nmedical harmfulness caused by LLMs is mainly due to misinter-\npretation errors and incomprehensiveness. Although our human\nevaluation showed that LLMs tend to make relatively few\nfabricated errors when completing our tasks, we cannot exclude\nthe possibility that such errors could lead to harmful conse-\nquences. However, not all summaries with these errors would\nbring medical harm. For example, although the summary makes a\nsigniﬁcant error by misspecifying the number of trials in a study\n19,\nour domain experts do not think this could bring medical harm.\nHow do human-generated summaries compare to LLM-\ngenerated summaries? We observe that human-generated sum-\nmaries contain a higher proportion (28%) of fabricated errors,\nresulting in more factual inconsistency and potential for harmful-\nness in human references. However, it is essential to approach this\nﬁnding with caution, as our human evaluation on human-\ngenerated summaries relies solely on the abstracts of Cochrane\nReviews as a proxy. There is a possibility that statements deemed\nto contain fabricated errors could, in fact, be validated by other\nsections within the full-length Cochrane Review. For example, the\nseverity of intracranial atherosclerosis (ICAS) of the studied\npopulation (see the example for Fabricated Attribute in Supple-\nmentary Table 1) is not mentioned in the abstract of the review\nbut is mentioned in the Results section of the whole review.\nTherefore, we decide to exclude the human reference from our\ncomparison ﬁgures. Nevertheless, despite these errors, human-\ngenerated summaries are still preferred (34%) compared to\nChatGPT-Abstract (26%) and GPT3.5-MainResult (21%). It is worth\nnoting that human-generated summaries may contain valuable\ninterpretations of reviews, which account for why they are chosen\nas the best summaries. However, it is important to avoid over-\nextrapolating from the source document, as this could lead to less\ndesirable outcomes (as illustrated in Fig.3b and c).\nDoes providing longer input lead to better summaries\ngenerated by LLMs? It is important to emphasize that the main\ndifference between ChatGPT-MainResult and ChatGPT-Abstract is\nthat the latter generates summaries based on the entire abstract.\nOur ﬁndings show that having longer text actually negatively\nimpacts ChatGPT’s capability to identify and extract the most\npertinent information, as evidenced by the lower comprehensive-\nness. Furthermore, a longer context leads to an increased\nlikelihood of ChatGPT making factual inconsistency errors and\ngenerating summaries that are more misleading. These factors\ncombined make ChatGPT-Abstract less preferred compared to\nChatGPT-MainResult in human evaluation.\nHow can we automatically detect factual inconsistency and\nimprove summaries? Given that the types of errors made by the\nmost recent summarization systems are constantly evolving\n15,\nfuture factuality and harmfulness evaluation should be adaptable\nto these shifting targets. One possible approach is to leverage the\npower of LLMs to identify potential errors within summaries.\nHowever, effectively identifying the most important information\nfrom long contexts and making high-quality summaries remains a\nchallenging task for LLMs that we have evaluated. Methods such\nas segment-then-summarize\n20 and extract-then-abstract 21 for\nhandling summarizing long context are shown to not work well\n0.0\n0.2\n0.4\n0.6\nMost Preferred Summary Least Preferred Summary\nChatGPT−MainResult ChatGPT−Abstract GPT3.5−MainResult\nPreferencea\n0\n10\n20\n30\nHelpful\ninterpretation\nHighest\nclarity\nHighest\naccuracy\nIncluding\nmore\nimportant\ninformation\nHighest\ncomprehensiveness\nCount\nReasons for Choosing the Most Preferred Summaryb\n0\n5\n10\n15\n20\n25\nLeast\naccurate\nNot\nrelated\nto the\nobjective\nLeast\ncoherency\nLeast\nclarity\nOver\nextrapolation\nMisinterpretation\nerrors\nFabricated\nerrors\nMissing\nimportant\ninformation\nCount\nReasons for Choosing the Least Preferred Summaryc\nFig. 3 Annotator vote distribution across all clinical domains and models. aThe most and least preferred summaries.b The reasons for\nchoosing the most preferred summaries.c The reasons for choosing the least preferred summaries.\nL. Tang et al.\n5\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   158 \nfor zero-shot LLM6,15. Furthermore, the presence of non-textual\ndata, such as tables andﬁgures in Cochrane Reviews, may increase\nthe complexity of the summarization task. To address these\nchallenges and improve the quality of summaries, future work\ncould explore and evaluate the efﬁcacy of GPT-4 in summarizing\nreviews with longer contexts and multiple modalities, while also\nincorporating techniques for detecting factuality inconsistencies\nand medical harmfulness.\nOur study has limitations. Our evaluation of ChatGPT and GPT-\n3.5 is based on a semi-synthetic task, which involves summarizing\nCochrane Reviews using only their abstracts or part of the\nabstracts. A more genuine task here would be a multi-document\nsummarization setting that involves summarizing all relevant\nstudy reports within a review addressing speci ﬁc research\nquestions. The rationale behind this choice is three-fold. First,\nthe abstracts of Cochrane Reviews is a stand-alone documents\n8\nnot contain any information that is not in the main body of the\nreview, and the overall messages should be consistent with the\nconclusions of the review. Second, abstracts of Cochrane Reviews\nare freely available and may be the only source readers can assess\nthe review results. Finally, we need to accommodate the input\nlength constraints of large language models, as the full Cochrane\nReview would surpass their capacity. As a result, our experiment\nassesses these LLMs' ability to summarize medical evidence under\na modiﬁed summarization framework. Our rigorous systematic\nevaluation ﬁnds that ChatGPT tends to generate less factually\naccurate summaries when conditioned on the entire abstract,\nwhich could potentially indicate that the model may be\nsusceptible to distraction from irrelevant information within\nlonger contexts. This ﬁnding raises concerns about the model’s\neffectiveness when presented with the full scope of a Cochrane\nReview, and suggests that it may not perform optimally in such\nscenarios.\nSecondly, the prompt in this study is adapted from previous\nwork\n6. Given the lack of a systematic method for searching over\nthe prompt space, it is conceivable that future studies could\npotentially discover more effective prompts.\nThird, ChatGPT and GPT-3.5 were not required to produce a\nparticular word count, but merely a speci ﬁc sentence length.\nConsequently, the enhanced performance of ChatGPT-MainResult\nin comparison to ChatGPT3.5-MainResult may be attributed to its\ngeneration of summaries of a slightly longer length, averaging\nabout 8 words (Supplementary Table 1).\nOur evaluation of LLM-generated summaries focused on six\nclinical domains, with one designated expert assigned to each\ndomain. Such evaluation requires domain knowledge, making it\ndifﬁcult for non-experts to carry out the evaluation. This constraint\nlimits the total amount of summaries we are able to annotate.\nFurther, we chose to use only the abstracts of Cochrane Reviews\nto evaluate human-generated summaries ( Author’s Conclusions\nsection) since examining the entire Cochrane Review is a time-\nconsuming process. Therefore, it is possible that some of the\nerrors identiﬁed in human reference summaries may actually be\nsubstantiated by other sections of the full-length review.\nMETHODS\nMaterials\nA Cochrane Review is a systematic review of scientiﬁc evidence\nthat aims to provide a comprehensive summary of all relevant\nstudies related to a speciﬁc research question. Reviews follow a\nrigorous methodology, which includes a comprehensive search for\nrelevant studies, the critical appraisal of study quality, and the\nsynthesis of study ﬁndings. The primary objective of Cochrane\nReviews is to provide an unbiased and comprehensive summary\nand meta-analysis of the available evidence, to help healthcare\nprofessionals make informed decisions about the most effective\ntreatment options.\nIn this work, we utilized Cochrane Reviews extracted from the\nCochrane Library, which is a large database that provides high-\nquality and up-to-date information about the effects of healthcare\ninterventions. It covers a diverse range of healthcare topics, and\nour study focuses on six speciﬁc topics drawn from this resource—\nAlzheimer’s disease, Kidney disease, Esophageal cancer, Neurolo-\ngical conditions, Skin disorders, and Heart failure. In particular, we\nhave collected up to ten of the most recent reviews on each topic.\nEach review was veriﬁed by domain experts to ensure they had\nimportant research objectives. Table1 shows the basic summary\nstatistics. We focus on the abstracts of Cochrane Reviews in our\nstudy, which can be read as stand-alone documents. Each abstract\nincludes the Background, Objectives of the review, Search\nmethods, Selection criteria, Data collection and analysis, Main\nresults, and Author’s conclusions.\nExperimental setup\nIn this study, we aim to evaluate the zero-shot performance of\nsummarizing systematic reviews using two OpenAI-developed\nmodels: GPT-3.5 (text-davinci-003) and ChatGPT. GPT-3.5 is built\nupon the GPT-3 model but has undergone further training using\nreinforcement learning with a human feedback procedure to\nprovide better outputs preferred by humans. ChatGPT has\ngathered signiﬁcant attention due to its ability to generate high-\nquality and human-like responses to conversational text prompts.\nDespite its impressive capabilities, it remains unclear whether\nChatGPT can generalize and perform high-quality zero-shot\nsummarization of medical evidence reviews. Therefore, we seek\nto investigate the comparative performance of ChatGPT and GPT-\n3.5 in summarizing systematic reviews of medical evidence data.\nTo evaluate the capabilities of the models, we have designed\ntwo distinct setups for input. In theﬁrst setup, the models take the\nwhole abstract except for the Author’s Conclusions as the input\n(ChatGPT-Abstract). The second setup takes both the objectives\nand the Main results sections of the abstract as the model input\n(ChatGPT-MainResult and GPT3.5-MainResult). The objective sec-\ntion outlines the speciﬁc research question in the PICO formula-\ntion that the review aims to address, while the Main results section\nprovides a summary of the results of the studies included in the\nreview, including key outcomes and any statistical data while\nhighlighting the strengths and limitations of the evidence.\nIn both settings, we use [input] + “Based on the Objectives,\nsummarize the above systematic review in four sentences” to prompt\nthe model to perform summarization, where we emphasize the\npurpose of the summary by providing theObjectives section. We\nuse the Author’s Conclusions section as the human reference (see\nexplanations in the“Introduction” section) and compare it against\nthe models’ generated outputs.\nAutomatic evaluation metrics\nWe select several metrics widely used in text generation and\nsummarization. ROUGE-L10 measures the overlap between the\ngenerated summary and the reference summary, focusing on the\nrecall of the n-grams. METEOR\n11 measures the harmonic mean of\nunigram precision and recall, based on stemming and synonym\nmatching. BLEU12 measures the overlap between the generated\nsummary and the reference summary, focusing on the precision of\nthe n-grams. These metrics have values ranging from 0 to 1, where\n1 indicates perfect matching between the generation and the\nreference, while 0 indicates no overlap. In addition, we selected\ntwo reference-free metrics. Extractiveness\n13 measures the percen-\ntage of words in a summary from some extractive fragments of\nthe source document, where extractive fragments are a set of\nshared sequences of tokens in the document and the summary.\nThe percentage of novel n-grams signi ﬁes the proportion of\nL. Tang et al.\n6\nnpj Digital Medicine (2023)   158 Published in partnership with Seoul National University Bundang Hospital\nn-grams in the summary that differ from the original source\ndocument.\nDesign of the human evaluation\nWe systematically evaluate the quality of generated summaries via\nhuman evaluation. We propose to evaluate summary quality along\nseveral dimensions: (1) Factual consistency; (2) Medical harmful-\nness; (3) Comprehensiveness; and (4) Coherence. These dimen-\nsions have been previously identiﬁed and serve as essential factors\nin evaluating the overall quality of generated summaries\n22–24.\nFactual consistency measures whether the statements in the\nsummary are supported by the systematic review. Medical\nharmfulness refers to the potential of a summary that leads to\nphysical or psychological harm or unwanted changes in therapy or\ncompliance due to the misinterpretation of information. Compre-\nhensiveness evaluates whether a summary contains suf ﬁcient\ninformation to cover the objectives of the systematic review.\nCoherence refers to the ability of a summary to build a coherent\nbody of information about a topic through sentence-to-sentence\nconnections.\nTo assess the quality of the generated summaries, we include\nsix domain experts, with each annotating summaries for a speciﬁc\ntopic. During the annotation process, participants are presented\nwith the whole abstract of the systematic review, along with four\nsummaries: (1) the Authors’ conclusion section; (2) ChatGPT-\nMainResult; (3) ChatGPT-Abstract; and (4) GPT3.5-MainResult. The\norder in which the summaries are presented is randomized to\nminimize potential order effects during the evaluation process. We\nutilize a 5-point Likert scale for the evaluation of each dimension.\nIf the summary received a low score on any of the dimensions, we\nfurther asked participants to explain the reason for the low score\nin a provided text box for each dimension. This approach enables\nus to perform a qualitative analysis of the responses and identify\ncommon themes to de ﬁne a terminology of error types for\nmedical evidence summarization where none exists. In addition to\nevaluating the quality of the summaries, we also asked\nparticipants to indicate their most and least preferred summaries\nand to provide reasons for their choices. This approach enables us\nto identify speciﬁc subcategories of reasons and gain insights into\nthe potential of using model-generated summaries to assist in\ncompleting the systematic review process.\nThe 5-point Likert scales between models were assessed by the\nMann–Whitney U test\n25. The response categories of a 5-point\nLikert item are coded 1–5 which were used as numerical scores in\nthe Mann–Whitney U test for differences. Thep-value reﬂects if the\nresponses of the summaries generated by the two models are\ndifferent, assuming the null hypothesis means there is no\ndifference between the results generated by the two models.\nWe used 1000 bootstrap samples to obtain a distribution of the\nLikert scales and reportedp-values.\nReporting summary\nFurther information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nDATA AVAILABILITY\nThe summarization data that support the ﬁndings of this study can be assessed\nthrough the PubMed ID. The AI-generated text data that support theﬁndings of this\nstudy are available from the corresponding author upon reasonable request. The\nauthors declare that all other data supporting theﬁndings of this study are available\nwithin the paper and its supplementary informationﬁles.\nCODE AVAILABILITY\nThe codes that support the ﬁndings of this study are available from the\ncorresponding author upon reasonable request.\nReceived: 25 May 2023; Accepted: 3 August 2023;\nREFERENCES\n1. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language\nmodels. InAdvances in Neural Information Processing SystemsVol. 35 (eds Koyejo,\nS. et al.) 24824–24837 (Curran Associates, Inc., 2022).\n2. Brown, T. et al. Language models are few-shot learners. InAdvances in Neural\nInformation Processing SystemsVol. 33 (eds Larochelle, H., Ranzato, M., Hadsell, R.,\nBalcan, M. F. & Lin, H.) 1877–1901 (Curran Associates, Inc., 2020).\n3. Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at\nhttps://arxiv.org/abs/2204.02311 (2022).\n4. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large language models are\nzero-shot reasoners. InAdvances in Neural Information Processing SystemsVol. 35\n(eds Koyejo, S. et al.) 22199–22213 (Curran Associates, Inc., 2022).\n5. Ouyang, L. et al. Training language models to follow instructions with human\nfeedback. In Advances in Neural Information Processing Systems Vol. 35 (eds\nKoyejo, S. et al.) 27730–27744 (Curran Associates, Inc., 2022).\n6. Goyal, T., Li, J. J. & Durrett, G. News summarization and evaluation in the era of\nGPT-3. Preprint athttps://arxiv.org/abs/2209.12356 (2022).\n7. Gao, C. A. et al. Comparing scientiﬁc abstracts generated by ChatGPT to real\nabstracts with detectors and blinded human reviewers. npj Digit. Med. 6,7 5\n(2023).\n8. Beller, E. M. et al. PRISMA for abstracts: reporting systematic reviews in journal\nand conference abstracts.PLoS Med. 10, e1001419 (2013).\n9. OpenAI. Introducing ChatGPT.https://openai.com/blog/chatgpt (2023).\n10. Lin, C.-Y. ROUGE: a package for automatic evaluation of summaries. In Text\nSummarization Branches Out. 8 74–81, Barcelona, Spain (Association for Com-\nputational Linguistics, 2004).\n11. Banerjee, S. & Lavie, A. METEOR: an automatic metric for MT evaluation with\nimproved correlation with human judgments. InProc. ACL Workshop on Intrinsic\nand Extrinsic Evaluation Measures for Machine Translation and/or Summarization\n65–72, Ann Arbor, Michigan (Association for Computational Linguistics, 2005).\n12. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. BLEU: a method for automatic\nevaluation of machine translation. InProc. 40th Annual Meeting on Association for\nComputational Linguistics 311–318 (Association for Computational Linguistics,\n2002).\n13. Grusky, M., Naaman, M. & Artzi, Y. Newsroom: a dataset of 1.3 million summaries\nwith diverse extractive strategies. InProc. 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Tech-\nnologies,1( Long Papers) 708–719 (Association for Computational Linguistics,\n2018).\n14. Fabbri, A. R. et al. SummEval: re-evaluating summarization evaluation. Trans.\nAssoc. Comput. Linguist.9, 391–409 (2021).\n15. Tang, L. et al. Understanding factual errors in summarization: errors, summarizers,\ndatasets, error detectors. In Proc. 61st Annual Meeting of the Association for\nComputational Linguistics\n(Vol. 1: Long Papers ) 11626–11644 (Association for\nComputational Linguistics, Toronto, Canada, 2023).\n16. Mühlbauer, V. et al. Antipsychotics for agitation and psychosis in people with\nAlzheimer’s disease and vascular dementia. Cochrane Database Syst. Rev. 12,\nCD013304 (2021).\n17. Luoa, J. et al. Endovascular therapy versus medical treatment for symptomatic\nintracranial artery stenosis.Cochrane Database Syst. Rev.8, CD013267 (2023).\n18. Gross, A. et al. Exercises for mechanical neck disorders.Cochrane Database Syst.\nRev. 1, CD004250 (2015).\n19. Kamo, T. et al. Repetitive peripheral magnetic stimulation for impairment and\ndisability in people after stroke.Cochrane Database Syst. Rev.9, CD011968 (2022).\n20. Zhang, Y. et al. SummN: a multi-stage summarization framework for long input\ndialogues and documents. In Proc. 60th Annual Meeting of the Association for\nComputational Linguistics (Vol. 1: Long Papers) 1592–1604 (Association for Com-\nputational Linguistics, 2022).\n21. Zhang, Y. et al. An exploratory study on long dialogue summarization: what\nworks and what’s next. InFindings of the Association for Computational Linguistics:\nEMNLP 2021 4426–4433 (Association for Computational Linguistics, 2021).\n22. Singhal, K. et al. Large language models encode clinical knowledge.Nature 620,\n172–180 (2023).\n23. Tang, L. et al. EchoGen: generating conclusions from echocardiogram notes. In\nProc. 21st Workshop on Biomedical Language Processing359–368 (Association for\nComputational Linguistics, 2022).\n24. Jeblick, K. et al. ChatGPT makes medicine easy to swallow: an exploratory case\nstudy on simpli ﬁed radiology reports. Preprint at https://arxiv.org/abs/\n2212.14882 (2022).\nL. Tang et al.\n7\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   158 \n25. Clason, D. & Dormody, T. Analyzing data measured by individual Likert-type\nitems. J. Agric. Educ.35,3 1–35 (1994).\nACKNOWLEDGEMENTS\nThis work was supported by the National Library of Medicine (NLM) of the National\nInstitutes of Health (NIH) under grant number 4R00LM013001, 1R01LM014306, and\n5R01LM009886, NIH Bridge2AI (OTA-21-008), National Cancer Institute (NCI) of NIH\nunder grant number P30CA013696, National Science Foundation under grant\nnumbers 2145640, 2019844, and 2303038, and Amazon Research Award. The\ncontent is solely the responsibility of the authors and does not necessarily represent\nthe ofﬁcial views of the NIH and NSF.\nAUTHOR CONTRIBUTIONS\nL.T. implemented the methods, conducted the experiments, and wrote the paper. Z.S.\nimplemented the methods and conducted the experiments. B.I., J.G.N., A.S., P.A.E.,\nZ.X., and J.R. conducted the human evaluation. Y.D. and G.D. edited the paper. J.R.,\nC.W., and Y.P. advised on all aspects of the work involved in this project and assisted\nin the paper writing. All authors read and approved theﬁnal manuscript.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41746-023-00896-7.\nCorrespondence and requests for materials should be addressed to Justin F.\nRousseau, Chunhua Weng or Yifan Peng.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nL. Tang et al.\n8\nnpj Digital Medicine (2023)   158 Published in partnership with Seoul National University Bundang Hospital"
}