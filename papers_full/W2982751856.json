{
  "title": "Transformer-CNN: Fast and Reliable Tool for QSAR",
  "url": "https://openalex.org/W2982751856",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2735527448",
      "name": "Pavel Karpov",
      "affiliations": [
        "Helmholtz Zentrum München"
      ]
    },
    {
      "id": "https://openalex.org/A2114590262",
      "name": "guillaume godin",
      "affiliations": [
        "Firmenich (Switzerland)"
      ]
    },
    {
      "id": "https://openalex.org/A4267851301",
      "name": "Igor Tetko",
      "affiliations": [
        "Helmholtz Zentrum München",
        "Institute of Groundwater Ecology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2081320283",
    "https://openalex.org/W2790808809",
    "https://openalex.org/W2735246657",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2169678694",
    "https://openalex.org/W2008381136",
    "https://openalex.org/W2914757825",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2087563523",
    "https://openalex.org/W2901476322",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2096541451",
    "https://openalex.org/W1972234779",
    "https://openalex.org/W4233253307",
    "https://openalex.org/W2076498053",
    "https://openalex.org/W4205896710",
    "https://openalex.org/W1966441763",
    "https://openalex.org/W2342596260",
    "https://openalex.org/W2153635508",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W1504991194",
    "https://openalex.org/W2907473220",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W1775434803",
    "https://openalex.org/W2765224015",
    "https://openalex.org/W2952254971",
    "https://openalex.org/W2951784549",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2165495560",
    "https://openalex.org/W2436108096",
    "https://openalex.org/W2972608805",
    "https://openalex.org/W2916877561",
    "https://openalex.org/W2527197703",
    "https://openalex.org/W3102476541",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2771577156",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2034562813",
    "https://openalex.org/W1968756812",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2094836090",
    "https://openalex.org/W2070955875",
    "https://openalex.org/W2145056192",
    "https://openalex.org/W2785720803",
    "https://openalex.org/W2241011319",
    "https://openalex.org/W2904147875",
    "https://openalex.org/W1965216736",
    "https://openalex.org/W4298069192",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2972498877",
    "https://openalex.org/W1990399577",
    "https://openalex.org/W2013894207",
    "https://openalex.org/W2136605707",
    "https://openalex.org/W2982978720",
    "https://openalex.org/W4294502208",
    "https://openalex.org/W2042278642",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964045325",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2962764460",
    "https://openalex.org/W4293508416",
    "https://openalex.org/W3210732293",
    "https://openalex.org/W2279490987",
    "https://openalex.org/W1977224352"
  ],
  "abstract": "We present SMILES-embeddings derived from internal encoder state of a Transformer model trained to canonize SMILES as a Seq2Seq problem. Using CharNN architecture upon the embeddings results in a higher quality QSAR/QSPR models on diverse benchmark datasets including regression and classification tasks. The proposed Transformer-CNN method uses SMILES augmentation for training and inference, and thus the prognosis grounds on an internal consensus. Both the augmentation and transfer learning based on embedding allows the method to provide good results for small datasets. We discuss the reasons for such effectiveness and draft future directions for the development of the method. The source code and the embeddings are available on https://github.com/bigchem/transformer-cnn, whereas the OCHEM environment (https://ochem.eu) hosts its on-line implementation.",
  "full_text": "Transformer-CNN: Fast and Reliable tool for QSAR\nPavel Karpov,1,2 Guillaume Godin3 and Igor V. Tetko1,2\n1Helmholtz Zentrum München - Research Center for Environmental Health (GmbH), Institute of Structural\nBiology, Ingolstädter Landstraße 1, D-85764 Neuherberg, Germany, \n2BigChem GmbH,Ingolstädter Landstraße 1, b. 60w, D-85764 Neuherberg, Germany \npavel.karpov and i.tetko“@” helmholtz-muenchen.de\n3Firmenich International SA, Digital Lab, Switzerland, Geneva, Lausanne\nguillaume.godin@firmenich.com\nCorresponding author: pavel.karpov@helmholtz-muenchen.de\nAbstract \nWe  present  SMILES-embeddings  derived  from  internal  encoder  state  of  a\nTransformer[1] model trained to canonize SMILES as a Seq2Seq problem. Using CharNN [2]\narchitecture upon the embeddings results in a higher quality QSAR/QSPR models on diverse\nbenchmark datasets including regression and classification tasks. The proposed Transformer-\nCNN method uses SMILES augmentation for training and inference, and thus the prognosis\ngrounds  on  an  internal  consensus. Both the augmentation and  transfer learning based on\nembedding  allows  the  method  to  provide  good  results  for  small  datasets.  We discuss  the\nreasons for such effectiveness and draft future directions for the development of the method.\nThe source code and the embeddings are available on https://github.com/bigchem/transformer-\ncnn, whereas the OCHEM[3] environment  (https://ochem.eu) hosts its on-line implementation.\nKeywords: Transformer model, Convolutional neural neural networks, Augmentation, QSAR,\nSMILES, Embeddings, Character-based models, Cheminformatics, Regression, Classification.\nIntroduction\nQuantitative Structure-Activity (Property) Relationship (QSAR/QSPR) approaches find a\nnonlinear function, often modeled as an artificial neural network (ANN), that estimates the\nactivity/property based on a chemical structure. In the past, the most QSAR works heavily relied\non descriptors[4] that represent in a numerical way some features of a complex graph structure\nof a compound. Amongst numerous families of descriptors, the fragment descriptors that count\noccurrences of a subgraph in a molecule graph, hold a distinctive status due to simplicity in the\ncalculation and moreover, there is a theoretical proof that one can successfully build any QSAR\nmodel with them [5]. Even a small database of compounds contains thousands of fragmental\n1\ndescriptors  and  some  feature  selection  algorithm  was  used  to  find  a  proper  subset  of\ndescriptors  for  better  quality and  speed  up  of  the  whole  modeling  process.  Thus,  feature\nselection in conjunction with a suitable machine learning method was a key to success [6]. Rise\nof deep learning[7] allows getting rid of tiresome expert and domain-wise feature construction by\ndelegating this task to a neural network that can extract the most valuable traits of the raw input\ndata required for modeling problem at hand[8, 9]. \nIn this setting, the whole molecule as a SMILES-strings  [10, 11]  (Simplified Molecular\nInput Line Entry System)  or a graph [12, 13] serves as the input to the neural network. SMILES\nnotation allows writing any complex formula of an organic compound in a string facilitating\nstorage and retrieval information about molecules in databases [14]. It contains all information\nabout the compound sufficient to derive the entire configuration (3D-structure) and has a direct\nconnection to the nature of fragmental descriptors, Fig. 1, thus, making SMILES one of the best\nrepresentation for QSAR studies. \nFig. 1.  Benzylpenicillin canonical SMILES at the top, 2D and 3D structures derived from SMILES with\nOpenBabel[15] in the middle, and three non-canonical SMILES examples at the bottom. A substructure of\nthe phenyl ring is written in bold font.\nOne of the first work exploiting direct SMILES input as descriptors used fragmentation of\nstrings into groups of overlapping substrings forming a SMILES-like set or a hologram of a\nmolecule[16]. Within this approach, there was no need to derive a 2D/3D configuration of the\nmolecule with subsequent calculation of descriptors keeping the quality of the models at the\nsame level as with classical descriptors or even better.\nIn the first place, SMILES strings are sequences of characters; therefore, they can be\n2\n\nanalyzed by machine-learning methods suitable for text processing, namely with convolutional\nand recurrent neural networks. After the demonstration of text understanding from character-\nlevel  inputs[17],  the  technique  was  adopted  in  chemoinformatics[11,  18–21].  Recently  we\nshowed that augmentation of SMILES (using canonical as well as non-canonical SMILES during\nmodel training and inference) increases the performance of convolutional models for regression\nand classification tasks[22].\nTechnically modern machine-learning models consist of two parts working together. The\nfirst one encodes the input data and extracts the most robust features by applying convolutional\nfilters with different receptive fields (RF) or recurrent layers, whereas the second part directly\nbuilds the regular model based on these features using standard dense layers as building\nblocks (so called classical “MLP”), Fig. 2. Though powerful convolutional layers can effectively\nencode the input to its internal representation, usually one needs a considerable training dataset\nand computational \nFig. 2. Scheme of modern QSAR models based on ANN. The encoder part (left) extracts main features of\nthe input data by means of RNN (top) or convolutional layers (bottom). Then the feature vector as usual\n3\n\ndescriptors feeds to the dense layer part consisted of residual and highway connections, normalization\nlayers, and dropouts.\nresources to train the encoder part of a network. The concept of embeddings mitigates the\nproblem by using the pre-trained weights designed for image [23] or text processing [24] tasks.\nIt allows transfer learning from previous data and speeding up the training process for building\nmodels with significantly smaller datasets inaccessible for training from scratch. Typically, QSAR\ndatasets contain only several hundreds of molecules, and SMILES-embeddings could improve\nmodels by developing better features.\nOne  way  of  separately  obtaining  SMILES  embeddings  is  to  use  classical\nautoencoder[25] approach where the input is the same as the output. In the case of SMILES,\nhowever, it would be more desirable to explore a variety of SMILES belonging to the same\nmolecule due to redundant SMILES grammar, Fig. 1. We hypothesized that it is possible to train\na  neural  network  to  conduct  a  SMILES  canonization  task  in  a  Sequence-to-Sequence\n(Seq2Seq) manner like machine translation problem, where on the left side are non-canonical\nSMILES,  and  on  the  right  side  are  their  canonical  equivalents.  Recently,  Seq2Seq  was\nsuccessfully applied to translation from InChi [26] codes to SMILES (Inchi2Sml) as well as from\nSMILES arbitrary to canonical SMILES (Sml2canSml), and to build QSAR models on extracted\nlatent variables[27]. \nThe state-of-the-art neural architecture for machine translation consists of stacked Long\nShort-Term Memory (LSTM) cells [28]. Training process for such networks has inherent for all\nkinds of Recurrent Neural Networks difficulties, i.g., vanishing gradients, and the impossibility of\nparallelization. Recently, a Transformer model  [1] was proposed where all recurrent units are\nreplaced  with  convolutional  and  element-wise  feed-forward  layers.  The  whole  architecture\nshows  a  significant  speed-up  during  training  and  inference  with  improved  accuracy  over\ntranslation  benchmarks.  The  Transformer  model  was  applied  for  prediction  of  reaction\noutcomes[29] and for retrosynthesis [30].  \nOur contributions in the article are as follows:\n● we present a concept of dynamic SMILES embeddings that may be useful for a wide\nrange of cheminformatics tasks;\n● we  scrutinize  CharNN  models  based  on  these  embeddings  for  regression  and\nclassification tasks and show that the method outperforms the state-of-the-art models;\n● our implementation as well as source codes and SMILES-embeddings are available on\n4\nhttps://github.com/bigchem/transformer-cnn.  We  also  provide  ready-to-use\nimplementation on https://ochem.eu within  the OCHEM[3] environment.\nMethods\nSMILES canonization model\nDataset\nTo  train  the  ANN  to  perform  SMILES  canonization  task,  we  used  the  ChEMBL\ndatabase[31] with length of SMILES less than or equal 110 characters (>93% of the entire\ndatabase). The original dataset was augmented 10 times up to 17,657,995 canonization pairs\nwritten in reactions format separated  by ‘>>’. Each pair contained  on  the left side  a  non-\ncanonical,  and  on  the  right  side  –  a  canonical  SMILES  for  the  same  molecule.  Such  an\narrangement of the training dataset allowed us to re-use the previous Transformer code, which\nwas  originally  applied  for  retrosynthesis  task[30].  For  completeness,  we  added  for  every\ncompound a line where both left and right sides were identical, i.e. canonical SMILES,  Fig. 3.\nThus each molecule was present in the training set 11 times.\nFig.  3.  Example  of  the  data  in  the  training  file  for  canonization  model  of  a  small  molecule\nCHEMBL351484. Every line contains a pair of non-canonical (left) and canonical (right) separated by\n“>>”. One line has identical SMILES on both sides, stressed with the red box.\nModel input\nSeq2Seq models use one-hot encoding vector for the input. Its values are zero everywhere\nexcept the position of the current token which is set to one. Many works on SMILES use\ntokenization procedure [32, 33] that combines some characters, for example ‘B’ and ‘r’ to one\ntoken  ‘Br’.  Other  rules  for  handling  most  common  two-letters  elements,  charges,  and\n5\n\nstereochemistry also are used for preparing the input for the neural network. According to our\nexperience, the use of more complicated schemes instead of simple character-level tokenization\ndid not increase the accuracy of models [30]. Therefore a simple character-level tokenization\nwas used in this study. The vocabulary of our model consisted of all possible characters from\nChEMBL dataset and has 66 symbols:\n ^#%()+-./0123456789=@ABCDEFGHIKLMNOPRSTVXYZ[\\]abcdefgilmnoprstuy$\nThus, the model could handle all diversity of drug-like compounds including stereochemistry,\ndifferent charges, and inorganic ions. Two special characters were added to the vocabulary: ‘^’\nto indicate the start of the sequence, and ‘$’ to inform the model about the  end of the data\ninput. \nTransformer model\nThe canonization model used in this work was based upon Transformer architecture\nconsisting of two separate stacks of layers for the encoder and the decoder, respectively. Each\nlayer incorporated some portion of knowledge written in its internal memory (V) with indexed\naccess by keys (K). When new data arrived (Q), the layer calculated attention, to what it has\nalready learned, and modified the input accordingly (see the original work on Transformers [1]),\nthus, forming the output of the self-attention layer and backlighting those parts that carry the\nessential  information.  Besides  self-attention  mechanism,  the  layer  also  contained  several\nposition-wise dense layers, normalization layer, and residual connections [1, 34] . Our model\nutilized three layers architecture of Transformer with 10 blocks of self-attention, i.e. the same\none as used in our previous study [30]. After the encoding process was finished, the output of\nthe top encoder layer contained a  representation  of a molecule suitable for decoding into\ncanonical  SMILES.   In  this  study  we  used  this  representation  as  a  well-prepared  latent\nrepresentation for QSAR modeling. \nTensorflow v1.12.02[35] was used as machine-learning framework to develop all parts of\nthe  Transformer,  whereas  RDKit  v.2018.09.2[36] was  used  for  SMILES  canonization,  and\nOpenBabel v2.3.1[15] for data augmentation. \nQSAR model\nWe call the output of the Transformer's encoder part as a dynamic SMILES-embedding,\nFig. 4. For a molecule with N-characters, the encoder produces the matrix with dimensions ( N,\nEMBEDDINGS).  Though  technically  this  matrix  is  not  an  embedding  because  equivalent\n6\ncharacters have different values depending on position and surroundings, it can be considered\nso due to its role: to convert input one-hot raw vectors to real-value vectors in some latent\nspace. Because these embeddings has variable lengths, we used a series of 1D convolutional\nfilters as implemented in DeepChem[37] TextCNN method (https://github.com/deepchem). \nFig. 4. The architecture of Transformer-CNN network.\nEach convolution had a kernel size from the list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20] and\nproduced the following number of filters [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160,\n160], respectively. After GlobalMaxPool operation and the following concatenation of the pooling\nresults, the data went throw Dropout [38](rate=0.25), Dense(N=512), Highway [39] layers, and,\nfinally, converted to the output layer which consisted of only one neuron for regression and two\nneurons  for  classification  tasks.  The  weights  of  the  Transformer’s  part  were  frozen  in  all\nexperiments. All models used Adam optimizer with Mean Squared Error or Binary Cross-Entropy\nloss depending on the problem at hand.  A fixed learning rate λ = 10 -4 was used. Early-stopping\nwas  used  to  prevent  overfitting,  select  a  best  model  and  reduce  training  time.  OCHEM\ncalculations were performed using canonical SMILES as well as ten-times augmentation of\nSMILES during both training and prognosis. This number of SMILES augmentations was found\nto be an optimal one in our previous study[40]. An average value of the individual predictions for\ndifferent representation of the same molecule were used as final model predictions to calculate\nstatistical parameters. \nThe same five-fold cross-validation procedure was used to compare the models with\nresults of our previous study[40]. The coefficients of determination[41]\nr2 = 1 - SSres/SStot (1)\nwhere SS tot   is total variance of data and SS res   is residual unexplained variance of data was\nused  to  compare  regression  models  and  Area  Under  the   Curve  (AUC)  was  used  for\nclassification tasks.\n7\nValidation datasets\nWe used the same datasets (9 for regression and 9 for classification) that were exploited\nin our previous studies [11, 22]. Short information about these sets as well as links to original\nworks  are  provided  in  Table  1.  The  datasets  are  available  at  OCHEM  environment  on\nhttps://ochem.eu.\nTable 1. Descriptions of datasets used in the work. \nCode Description Size Code Description Size\nRegression tasks Classification tasks\nMP Melting point [42] 19,104 HIV Inhibition of HIV \nreplication [43]\n41,127\nBP Boiling point [44] 11,893 AMES Mutagenicity [45] 6,542\nBCF Bioconcentration \nfactor[44]\n378 BACE Human β-secretase 1 \n(BACE-1) inhibitors[43]\n1,513\nFreeSolv Free solvation energy \n[43]\n642 Clintox Clinical trial toxicity [43] 1,478\nLogS Solubility [46] 1,311 Tox21 In-vitro toxicity [43] 7,831\nLipo Lipophilicity [47] 4,200 BBBP Blood-brain barrier [43] 2,039\nBACE IC50 of human β-\nsecretase 1 (BACE-1) \ninhibitors [43] \n1,513 JAK3 Janus kinase 3 \ninhibitor[48]\n886\nDHFR Dihydrofolate reductase\ninhibition[49]\n739 BioDeg Biodegradability [50] 1,737\nLEL Lowest effect level [51] 483 RP AR Endocrine disruptors [52] 930\nResults and discussion\nSMILES canonization model\nThe Transformer model was trained for 10 epochs with learning rate changing according\nto the formula:\nλ=factor∗min(1.0, step /warmup)/max(step , warmup)(1)\nwhere factor = 20, warmup = 16,000 steps, and if λ < 10 -4 then λ = 10 -4. The settings for the\nlearning rate were similar to those used in our retro-synthesis study. Each epoch contained\n8\n275,907 steps (batches). No early-stopping or weights averaging was applied. Learning curves\nare shown in Fig. 5.  \nTo validate the model, we sampled 500,000 ChEMBL-like SMILES (only 8,617 (1.7%) of\nthem were canonical) from a generator [53] and checked how accurately the model can restore\ncanonical  SMILES  for  these  molecules.  We  intentionally  selected  the  generated  SMILES\nkeeping in mind possible application of the proposed method in the artificial intelligence-driven\npipelines of  de-novo development of new drugs. The model correctly canonized  83,6%  of all\nsamples, Table 2.\nTable 2. Validation of canonization model.\nStrings All Correctly canonized\nAll 500,000 418,233 (83.6%)\nStereo (with @) 77,472 28,821 (37,2%)\nCis/trans (with / or \\) 54,727 40,483 (73,9%)\nFig. 5. Learning curves: 1) learning rate schedule (axes bottom and right), and 2) character-based \naccuracy (axes bottom and left) on the training dataset for the first four epochs.\nQSAR modeling \n9\nFor the QSAR modelling the saved embedding was used. The training was done using fixed\nlearning rate λ = 0.001 for n=100 epochs. The early stopping with 10% of randomly selected\nSMILES was used to identify the optimal model. Table 2, Fig. 6 compare results for regression\ndatasets while Table 3, Fig. 7 does it for classification tasks. The standard mean errors of the\nvalues were calculated using bootstrap procedure as explained elsewhere [50].\nWith an exception of few datasets, the proposed method provided similar or better results than\nthose of calculated using descriptor-based approaches as well as of  the other SMILES-based\napproaches  investigated  in  our  previous  study.[40] The  data  augmentation  was  critically\nimportant  for  the  Transformer-CNN  method  to  achieve  its  high  performance.  We  used\naugmentation  n=10,  i.e.,  10  SMILES  were  randomly  generated  and  used  for  model\ndevelopment  and  application,  which  was  found  as  an  optimal  one  in  the  aforementioned\nprevious study.\nTable 2. Coefficient of determination, r2, calculated for regression sets (higher values are better)1\nDataset Descriptor \nbased \nmethods2\nSMILES \nbased \n(augm=10)2\nTransformer-\nCNN, no \naugm.\nTransformer-\nCNN, \naugm=10\nCDDD \ndescriptors3\nMP 0.83 0.85 0.83 0.86 0.85\nBP 0.98 0.98 0.97 0.98 0.98\nBCF 0.85 0.85 0.71±0.02 0.85 0.81\nFreeSolv 0.94 0.93 0.72±0.02 0.91 0.93\nLogS 0.92 0.92 0.85 0.91 0.91\nLipo 0.7 0.72 0.6 0.73 0.74\nBACE 0.73 0.72 0.66 0.76 0.75\nDHFR 0.62±0.03 0.63±0.03 0.46±0.03 0.67±0.03 0.61±0.03\nLEL 0.19±0.04 0.25±0.03 0.2±0.03 0.27±0.04 0.23±0.04\n1-We omitted the standard mean errors, which are 0.01 or less, for the reported values. 2-\nresults from our previous study  [22]. 3 - Best performance calculated with CDDD descriptors\nobtained using autoencoder Sml2canSml from [27].  \n10\nFig. 6. Coefficient of determination, r2, calculated for regression sets (higher values are better).\nTable 3. AUC calculated for classification sets (higher values are better)\nDataset Descriptor \nbased \nmethods2\nSMILES \nbased \n(augm=10)2\nTransformer-\nCNN, no \naugm.\nTransformer-\nCNN, \naugm=10\nCDDD \ndescriptors3\nHIV 0.82 0.78 0.81 0.83 0.74\nAMES 0.86 0.88 0.86 0.89 0.86\nBACE 0.88 0.89 0.89 0.91 0.9\nClintox 0.77±0.03 0.76±0.03 0.71±0.02 0.77±0.02 0.73±0.02\nTox21 0.79 0.83 0.81 0.82 0.82\nBBBP 0.90 0.91 0.9 0.92 0.89\nJAK3 0.79±0.02 0.8±0.02 0.70±0.02 0.78±0.02 0.76±0.02\nBioDeg 0.92 0.93 0.91 0.93 0.92\nRP AR 0.85 0.87 0.83 0.87 0.86\n1-We omitted the standard mean errors, which are 0.01 or less, for the reported values. 2-\nresults from our previous study [22] . 3 - Best performance calculated with CDDD descriptors\nobtained using Sml2canSml autoencoder from [27].  \n11\nFig.7. AUC calculated for classification sets (higher values are better).\nSimilar to Transformer-CNN the Sml2canSml used internal representation, which was\ndeveloped from mapping of arbitrary SMILES to canonical SMILES. The difference was that\nSml2canSml  generated  a  fixed  set  of  512  latent  variables  (CDDD  descriptors),  while\nTransformer-CNN representation had about the same length as the initial SMILES. Sml2canSml\nCDDD  could  be  used  as  descriptors  with  any  traditional  machine  learning  methods  while\nTransformer-CNN required convolutional neural network to process variable length output and\nto correlate it with the analysed properties. Sml2canSml was added as CDDD descriptors to\nOCHEM. These descriptors were analysed by the same methods as used in the previous work,\ni.e., LibSVM[54], Random Forest [55], XGBoost[56] as well as by Associative Neural Networks\n(ASNN)[57] and Deep Neural Networks [58]. Exactly the same protocol, 5 fold cross-validation,\nwas used for all calculations. The best performance using the CDDD descriptors was calculated\nby ASNN and LibSVM methods, which contributed models with the highest accuracies for seven\nand five datasets, respectively (LibSVM method provided the best performance in the original\nstudy). Transformer-CNN provided better or similar results compared to the CDDD descriptors\nfor all datasets with an exception of Lipo and FreeSolv. It should be also mentioned, that CDDD\ndescriptors could only process molecules which satisfy the following conditions: \nlogP ∈ (-5,7) and \n12\nmol_weight ∈ (12,600) and \nnum_heavy_atoms ∈ (3, 50)  and \nmolecule is organic.\nThese limitations appeared due to the preparation of the training set to develop Sml2canSml\nencoder. The limitations resulted in the exclusion of a number of molecules, which failed one or\nseveral of the above conditions. Contrary to Sml2canSml encoder, we trained Transformer-CNN\nwith very diverse molecules from ChEMBL and thus the developed models could be applied to\nany molecule, which is processed by RDKiT. Actually, exclusion of molecules, for which CDDD\ndescriptors failed to be calculated, did not significantly changed results of Transformer models:\nsome  models  improved  while  other  decreased  their  accuracy  for  about  ~0.01  respective\nperformance values. For example, for Lipo and FreeSolv sets the accuracy of Transformer-CNN\nmodel increased to r2 = 0.92 and 0.75, respectively while for BBB AUC  decreased to 0.91.\nConclusions and outlook\nWe propose  a  SMILES  canonization  model  based  on  Transformer  architecture  that\nextracts information-rich real-value embeddings during the encoding process and exposes them\nfor further QSAR-oriented blocks to model biological activity or physico-chemical properties.\nTextCNN approaches can efficiently work with these embeddings, and the final quality of the\nQSAR models is higher compared to the models obtained with the state-of-the-art methods on\nthe majority of diverse benchmark datasets. The Transformer-CNN architecture requires less\nthan a hundred iterations to converge for new tasks. It can be easily embed it into de-novo drug\ndevelopment pipelines. The code is available on https://github.com/bigchem/transformer-cnn as\nwell as on-line version on http://ochem.eu.\nThe  method  developed  predicts  the  endpoint  based  on  an  average  of  individual\nprognosis for a batch of augmented SMILES belonging to the same molecule. The deviation\nwithin the batch can serve as a measure of a confidence interval of the prognosis, whereas the\npossibility to canonize SMILES can be used for deriving applicability domains of models. These\nquestions will be addressed in the upcoming studies.\nAbbreviations\nANN: Artificial Neural Network; CNN: Convolutional Neural Network; LSTM: Long Short-Term\nmemory; OCHEM: On-line chemical database and modeling environment; SMILES: Simplified\n13\nMolecular-Input  Line-Entry  System;  QSAR/QSPR:  Quantitative  Structure  ActivityProperty\nRelationship; RF: Receptive Field; RNN: Recurrent Neural Network.CNN: Convolutional Neural\nNetwork\nDeclarations\nAvailability of data and materials\nThe source code of Transformer-CNN is available on https://github.com/bigchem/transformer-\ncnn. Ready-to-use implementation as well as training datasets, and final QSAR models are \nstored on https://ochem.eu within the OCHEM environment.\nCompeting interests\nThe authors declare that they have no actual or potential conflicts of interests.\nFunding\nThis  study  was  funded  by  the  European  Union’s  Horizon  2020  research  and   innovation\nprogram  under  the  Marie  Skłodowska-Curie  grant  agreement  No.  676434,  “Big  Data  in\nChemistry” and ERA-CVD \"CardioOncology\" project, BMBF 01KL1710.\nAuthors’ contributions\nPK implemented the method, IVT and GC performed the analysis and benchmarking. All \nauthors interpreted results, read and approved the manuscript.\nAcknowledgments\nThe authors thank NVIDIA Corporation for donating Quadro P6000, Titan Xp, and Titan V\ngraphics cards for this research work.\nReferences\n1. Vaswani A, Shazeer N, Parmar N, et al (2017) Attention Is All You Need. ArXiv\n2. Zhang X, Zhao J, LeCun Y (2015) Character-level Convolutional Networks for Text \nClassification. arXiv [cs.LG]\n14\n3. Sushko I, Novotarskyi S, Körner R, et al (2011) Online chemical modeling environment \n(OCHEM): web platform for data storage, model development and publishing of chemical \ninformation. J Comput Aided Mol Des 25:533–554. https://doi.org/  10.1007/s10822-011-\n9440-2\n4. Mauri A, Consonni V, Pavan M, Todeschini R (2006) Dragon software: An easy approach to \nmolecular descriptor calculations. Match 56:237–248\n5. Baskin I, Varnek A (2009) Fragment descriptors in SAR/QSAR/QSPR studies, molecular \nsimilarity analysis and in virtual screening. ChemInform 40:i\n6. Eklund M, Norinder U, Boyer S, Carlsson L (2014) Choosing feature selection and learning \nalgorithms in QSAR. J Chem Inf Model 54:837–843. https://doi.org/  10.1021/ci400573c\n7. Chen H, Engkvist O, Wang Y , et al (2018) The rise of deep learning in drug discovery. Drug \nDiscov Today 23:1241–1250. https://doi.org/  10.1016/j.drudis.2018.01.039\n8. Duvenaud D, Maclaurin D, Aguilera-Iparraguirre J, et al (2015) Convolutional Networks on \nGraphs for Learning Molecular Fingerprints. arXiv [cs.LG]\n9. Coley CW, Barzilay R, Green WH, et al (2017) Convolutional Embedding of Attributed \nMolecular Graphs for Physical Property Prediction. J Chem Inf Model 57:1757–1772. \nhttps://doi.org/  10.1021/acs.jcim.6b00601\n10. Gómez-Bombarelli R, Wei JN, Duvenaud D, et al (2018) Automatic Chemical Design Using \na Data-Driven Continuous Representation of Molecules. ACS Central Science 4:268–276. \nhttps://doi.org/  10.1021/acscentsci.7b00572\n11. Kimber TB, Engelke S, Tetko IV, et al (2018) Synergy Effect between Convolutional Neural \nNetworks and the Multiplicity of SMILES for Improvement of Molecular Prediction. arXiv\n12. Gilmer J, Schoenholz SS, Riley PF, et al (2017) Neural Message Passing for Quantum \nChemistry. arXiv [cs.LG]\n13. Shang C, Liu Q, Chen K-S, et al (2018) Edge Attention-based Multi-Relational Graph \nConvolutional Networks. arXiv [stat.ML]\n14. Weininger D (1988) SMILES, a chemical language and information system. 1. Introduction \nto methodology and encoding rules. J Chem Inf Comput Sci 28:31–36. \nhttps://doi.org/  10.1021/ci00057a005\n15. O’Boyle NM, Banck M, James CA, et al (2011) Open Babel: An open chemical toolbox. J \nCheminform 3:33. https://doi.org/  10.1186/1758-2946-3-33\n16. Vidal D, Thormann M, Pons M (2005) LINGO, an efficient holographic text based method to\ncalculate biophysical properties and intermolecular similarities. J Chem Inf Model 45:386–\n393. https://doi.org/  10.1021/ci0496797\n17. Zhang X, LeCun Y (2015) Text Understanding from Scratch. arXiv [cs.LG]\n18. Goh GB, Hodas NO, Siegel C, Vishnu A (2017) SMILES2Vec: An Interpretable General-\nPurpose Deep Neural Network for Predicting Chemical Properties. arXiv [stat.ML]\n15\n19. Jastrzębski S, Leśniak D, Czarnecki WM (2016) Learning to SMILE(S). arXiv [cs.CL]\n20. Goh GB, Siegel C, Vishnu A, Hodas NO (2017) Using Rule-Based Labels for Weak \nSupervised Learning: A ChemNet for Transferable Chemical Property Prediction. arXiv \n[stat.ML]\n21. Zheng S, Yan X, Yang Y , Xu J (2019) Identifying Structure-Property Relationships through \nSMILES Syntax Analysis with Self-Attention Mechanism. J Chem Inf Model 59:914–923. \nhttps://doi.org/  10.1021/acs.jcim.8b00803\n22. Igor V. Tetko, Pavel Karpov, Eric Bruno,Talia B. Kimber, Guillaume Godin Augmentation is \nWhat You Need! In: Igor V. Tetko, Fabian Theis, Pavel Karpov, Vera Kurkova (ed) 28th \nInternational Conference on Artificial Neural Networks, Munich, Germany, September 17–\n19, 2019 Proceedings, Part V, Workshop and Special sessions. Springer\n23. Kiela D, Bottou L (2014) Learning image embeddings using convolutional neural networks \nfor improved multi-modal semantics. In: Proceedings of the 2014 Conference on Empirical \nMethods in Natural Language Processing (EMNLP). pp 36–45\n24. Pennington J, Socher R, Manning CD (2014) Glove: Global vectors for word representation.\nIn: In EMNLP\n25. Hinton GE, Salakhutdinov RR (2006) Reducing the dimensionality of data with neural \nnetworks. Science 313:504–507. https://doi.org/  10.1126/science.1127647\n26. Heller S, McNaught A, Stein S, et al (2013) InChI - the worldwide chemical structure \nidentifier standard. J Cheminform 5:7. https://doi.org/  10.1186/1758-2946-5-7\n27. Winter R, Montanari F, Noé F, Clevert D-A (2019) Learning continuous and data-driven \nmolecular descriptors by translating equivalent chemical representations. Chem Sci \n10:1692–1701. https://doi.org/  10.1039/c8sc04175j\n28. Hochreiter S, Schmidhuber J (1997) Long Short-Term Memory. Neural Comput 9:1735–\n1780. https://doi.org/  10.1162/neco.1997.9.8.1735\n29. Schwaller P , Laino T, Gaudin T, et al (2018) Molecular Transformer for Chemical Reaction \nPrediction and Uncertainty Estimation. arXiv\n30. Pavel Karpov, Guillaume Godin, Igor V. Tetko A Transformer Model for Retrosynthesis. In: \nIgor V. Tetko, Fabian Theis, Pavel Karpov, Vera Kurkova (ed) 28th International Conference\non Artificial Neural Networks, Munich, Germany, September 17–19, 2019 Proceedings, Part\nV, Workshop and Special sessions. Springer\n31. Gaulton A, Bellis LJ, Bento AP , et al (2012) ChEMBL: a large-scale bioactivity database for \ndrug discovery. Nucleic Acids Res 40:D1100–7. https://doi.org/  10.1093/nar/gkr777\n32. Segler MHS, Kogej T, Tyrchan C, Waller MP (2017) Generating Focussed Molecule \nLibraries for Drug Discovery with Recurrent Neural Networks\n33. Gupta A, Múller AT, Huisma BJH, et al (2018) Generative Recurrent Networks for De Novo \nDrug Design. Mol Inform 37:1700111\n34. Rush A (2018) The Annotated Transformer. In: Proceedings of Workshop for NLP Open \n16\nSource Software (NLP-OSS). pp 52–60\n35. Abadi M, Barham P , Chen J, et al (2016) TensorFlow: A system for large-scale machine \nlearning\n36. Landrum G RDKit: Open-source cheminformatics.   http://www.rdkit.org\n37. Ramsundar B, Eastman P , Walters P , Pande V (2019) Deep Learning for the Life Sciences: \nApplying Deep Learning to Genomics, Microscopy, Drug Discovery, and More. “O’Reilly \nMedia, Inc.”\n38. Srivastava N, Hinton G, Krizhevsky A, et al (2014) Dropout: A Simple Way to Prevent \nNeural Networks from Overfitting. J Mach Learn Res 15:1929–1958\n39. Srivastava RK, Greff K, Schmidhuber J (2015) Highway Networks. arXiv [cs.LG]\n40. Tetko IV, Karpov P , Bruno E, et al (2019) Augmentation Is What You Need!: 28th \nInternational Conference on Artificial Neural Networks, Munich, Germany, September 17–\n19, 2019, Proceedings. In: Tetko IV, Kůrková V, Karpov P , Theis F (eds) Artificial Neural \nNetworks and Machine Learning – ICANN 2019: Workshop and Special Sessions. Springer\nInternational Publishing, Cham, pp 831–835\n41. Draper NR, Smith H (2014) Applied Regression Analysis. John Wiley & Sons\n42. Tetko IV, Sushko Y , Novotarskyi S, et al (2014) How accurately can we predict the melting \npoints of drug-like compounds? J Chem Inf Model 54:3320–3329. \nhttps://doi.org/  10.1021/ci5005288\n43. Wu Z, Ramsundar B, Feinberg EN, et al (2018) MoleculeNet: a benchmark for molecular \nmachine learning. Chemical Science 9:513–530\n44. Brandmaier S, Sahlin U, Tetko IV, Öberg T (2012) PLS-Optimal: A Stepwise D-Optimal \nDesign Based on Latent Variables. Journal of Chemical Information and Modeling 52:975–\n983\n45. Sushko I, Novotarskyi S, Körner R, et al (2010) Applicability Domains for Classification \nProblems: Benchmarking of Distance to Models for Ames Mutagenicity Set. Journal of \nChemical Information and Modeling 50:2094–2111\n46. Delaney JS (2004) ESOL: estimating aqueous solubility directly from molecular structure. J \nChem Inf Comput Sci 44:1000–1005. https://doi.org/  10.1021/ci034243x\n47. Huuskonen JJ, Livingstone DJ, Tetko IV IV (2000) Neural network modeling for estimation \nof partition coefficient based on atom-type electrotopological state indices. J Chem Inf \nComput Sci 40:947–955\n48. Suzuki K, Nakajima H, Saito Y , et al (2000) Janus kinase 3 (Jak3) is essential for common \ncytokine receptor γ chain (γc)-dependent signaling: comparative analysis of γc, Jak3, and \nγc and Jak3 double-deficient mice. International Immunology 12:123–132\n49. Sutherland JJ, Weaver DF (2004) Three-dimensional quantitative structure-activity and \nstructure-selectivity relationships of dihydrofolate reductase inhibitors. Journal of Computer-\nAided Molecular Design 18:309–331\n17\n50. Vorberg S, Tetko IV (2014) Modeling the Biodegradability of Chemical Compounds Using \nthe Online CHEmical Modeling Environment (OCHEM). Mol Inform 33:73–85. \nhttps://doi.org/  10.1002/minf.201300030\n51. Novotarskyi S, Abdelaziz A, Sushko Y , et al (2016) ToxCast EPA in Vitro to in Vivo \nChallenge: Insight into the Rank-I Model. Chem Res Toxicol 29:768–775. \nhttps://doi.org/  10.1021/acs.chemrestox.5b00481\n52. Rybacka A, Rudén C, Tetko IV, Andersson PL (2015) Identifying potential endocrine \ndisruptors among industrial chemicals and their metabolites – development and evaluation \nof in silico tools. Chemosphere 139:372–378\n53. Zhonghua Xia, Pavel Karpov, Grzegorz Popowicz, Igor V. Tetko (2019) Focused Library \nGenerator: Case of Mdmx Inhibitors. Accepted to print\n54. Chang C-C, Lin C-J (2011) LIBSVM: A library for support vector machines. ACM \nTransactions on Intelligent Systems and Technology (TIST) 2:27. \nhttps://doi.org/  10.1145/1961189.1961199\n55. Breiman L (2001) Random Forests. Mach Learn 45:5–32. \nhttps://doi.org/  10.1023/A:1010933404324\n56. Chen T, Guestrin C (2016) XGBoost: A Scalable Tree Boosting System. arXiv [cs.LG]\n57. Tetko IV (2002) Associative Neural Network. Neural Process Letters 16:187–199. \nhttps://doi.org/  10.1023/A:1019903710291\n58. Sosnin S, Karlov D, Tetko IV, Fedorov MV (2019) Comparative Study of Multitask Toxicity \nModeling on a Broad Chemical Space. J Chem Inf Model 59:1062–1072. \nhttps://doi.org/  10.1021/acs.jcim.8b00685\n18",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7495712041854858
    },
    {
      "name": "Computer science",
      "score": 0.7427687644958496
    },
    {
      "name": "Inference",
      "score": 0.7177159190177917
    },
    {
      "name": "Embedding",
      "score": 0.6142421364784241
    },
    {
      "name": "Encoder",
      "score": 0.5619067549705505
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5252187848091125
    },
    {
      "name": "Transfer of learning",
      "score": 0.48030537366867065
    },
    {
      "name": "Quantitative structure–activity relationship",
      "score": 0.4689086675643921
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4535275101661682
    },
    {
      "name": "Machine learning",
      "score": 0.44101792573928833
    },
    {
      "name": "Engineering",
      "score": 0.07383796572685242
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3018134672",
      "name": "Helmholtz Zentrum München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2801043613",
      "name": "Firmenich (Switzerland)",
      "country": "CH"
    }
  ],
  "cited_by": 6
}