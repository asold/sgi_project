{
  "title": "MEDT: Using Multimodal Encoding-Decoding Network as in Transformer for Multimodal Sentiment Analysis",
  "url": "https://openalex.org/W4226150137",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2231091817",
      "name": "Qingfu Qi",
      "affiliations": [
        "Tianjin University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2161680974",
      "name": "Li-Yuan Lin",
      "affiliations": [
        "Tianjin University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Tianjin University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4227458717",
      "name": "Chengrong Xue",
      "affiliations": [
        "Tianjin University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2787293089",
    "https://openalex.org/W6741954564",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W6719667659",
    "https://openalex.org/W6753277404",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W6748551036",
    "https://openalex.org/W2886193235",
    "https://openalex.org/W2899197626",
    "https://openalex.org/W2062249044",
    "https://openalex.org/W2963032608",
    "https://openalex.org/W2740550900",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2619383789",
    "https://openalex.org/W2998570557",
    "https://openalex.org/W2310404790",
    "https://openalex.org/W2963702064",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W3093051361",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6789972913",
    "https://openalex.org/W6676984168",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6748726628",
    "https://openalex.org/W2964346351",
    "https://openalex.org/W3128412859",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963710346",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2465534249",
    "https://openalex.org/W2738734060",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2787581402",
    "https://openalex.org/W2520314979",
    "https://openalex.org/W3104739527"
  ],
  "abstract": "Multimodal sentiment analysis is a challenging task in the field of natural language processing (NLP). It uses multimodal signals (natural language, facial gestures, and acoustic behavior) in videos to generate emotional understanding. However, the importance of single modality data in the video to emotional outcomes is not static. With the extension of the time dimension, the emotional attributes of a specific natural language will be affected by non-natural language data, resulting in a vector shift in the feature space. At the same time, long-term dependencies within a specific modality and long-term dependencies between multiple modalities that are &#x201C;unaligned&#x201D; need to be considered. In response to the above problems, this paper proposes Multimodal Encoding-Decoding Network with Transformer. The network model encodes multimodal data through a Bidirectional Encoder Representations from Transformers (BERT) network and Transformer encoder to resolve long-term dependencies within modalities. And the network reconstructs the Transformer decoder to solve the weight problem of multimodal data in an iterative way. The network fully considers the long-term dependencies between modalities and the offset effect of non-natural language data on natural language data. Under the same experimental conditions, we validated our model on general multimodal sentiment analysis datasets. Compared with state-of-the-art models, the network achieves good progress and strong stability.",
  "full_text": "Received February 9, 2022, accepted February 26, 2022, date of publication March 11, 2022, date of current version March 18, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3157712\nMEDT: Using Multimodal Encoding-Decoding\nNetwork as in Transformer for Multimodal\nSentiment Analysis\nQINGFU QI\n1,2, LIYUAN LIN\n 1, RUI ZHANG1,2, AND CHENGRONG XUE1,2\n1College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin 300222, China\n2School of Software and Communications, Tianjin Sino-German University of Applied Sciences, Tianjin 300222, China\nCorresponding author: Rui Zhang (zhangrui@tsguas.edu.cn)\nThis work was supported in part by the Tianjin Sino-German University of Applied Sciences Technology Project under Grant\nZDKT2018-006, and in part by the Tianjin Sino-German University of Applied Sciences Video Image Intelligent Analysis and Processing\nTechnology Innovation Team and Tianjin ‘‘131’’ Innovative Talent Team.\nABSTRACT Multimodal sentiment analysis is a challenging task in the ﬁeld of natural language processing\n(NLP). It uses multimodal signals (natural language, facial gestures, and acoustic behavior) in videos to\ngenerate emotional understanding. However, the importance of single modality data in the video to emotional\noutcomes is not static. With the extension of the time dimension, the emotional attributes of a speciﬁc\nnatural language will be affected by non-natural language data, resulting in a vector shift in the feature space.\nAt the same time, long-term dependencies within a speciﬁc modality and long-term dependencies between\nmultiple modalities that are ‘‘unaligned’’ need to be considered. In response to the above problems, this\npaper proposes Multimodal Encoding-Decoding Network with Transformer. The network model encodes\nmultimodal data through a Bidirectional Encoder Representations from Transformers (BERT) network and\nTransformer encoder to resolve long-term dependencies within modalities. And the network reconstructs\nthe Transformer decoder to solve the weight problem of multimodal data in an iterative way. The network\nfully considers the long-term dependencies between modalities and the offset effect of non-natural language\ndata on natural language data. Under the same experimental conditions, we validated our model on general\nmultimodal sentiment analysis datasets. Compared with state-of-the-art models, the network achieves good\nprogress and strong stability.\nINDEX TERMSAuxiliary information, long-term dependence, multimodal sentiment analysis, transformer,\nvector offsets.\nI. INTRODUCTION\nSentiment analysis has always been a popular research direc-\ntion in the ﬁeld of NLP. In the early days, most of the\nwork was focused on unimodal research–mainly plain text\nsentiment analysis [1], [2] –in which the investigations were\nlimited to determining the usage of words in positive and\nnegative scenarios [3] and obtaining emotional results by\nanalyzing the meaning of speciﬁc word combinations. Fur-\nther analysis of human behavior shows that humans trans-\nmit information not only through natural language but also\nthrough non-natural language (visual and acoustic) [4]. This\nrich behavioral information can better help us understand\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Mostafa Rahimi Azghadi\n.\nhuman emotional intentions [5]. This behavioral information\nis considered to be the multimodal language of human beings.\nWith the rapid development of online media, more and more\npeople tend to use video to record their comments and opin-\nions on products or movies. This requires a multi-dimensional\nanalysis of people’s opinions and emotions in the video to bet-\nter understand the information it conveys [6]. Moreover, with\nthe maturity of audio and video feature extraction methods\n[7], the research progress of multimodal sentiment analysis\nhas also been accelerated. Currently, modeling multimodal\nlanguage for emotional understanding has become the central\nresearch direction of NLP and multimodal machine learn-\ning [8]–[10].\nWith further research, we found that although multimodal\nlanguage information is processed at the same time, it is still\n28750 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 10, 2022\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\nthe natural language that plays a decisive role in the ﬁnal\nemotional understanding. It is difﬁcult for us to analyze the\nintentions of an actor by relying only on visual or acous-\ntic behaviors because the non-natural language behaviors of\npeople expressing the same emotion are usually different.\nSuppose a person shows an emotional state for a certain\nthing, but it is almost difﬁcult for us to determine whether\nit is a positive emotion or a negative emotion through facial\nexpressions. When we combine facial behavior with different\nnatural language descriptions, we can clearly understand an\nactor’s emotional intentions, but this will enhance or weaken\nthe original emotion expressed in the current natural lan-\nguage. This leads to another problem. Since multimodal lan-\nguage communication occurs through natural language and\nnon-natural language channels, the meanings of words and\nsentences transmitted by humans through natural language\nchange dynamically in different non-natural language con-\ntexts [11], [12]. In other words, for a sentence that expresses\npositive emotions in the ﬁeld of purely natural language, the\nmeanings of words within the language are ﬁxed in the vector\nspace. When nonnatural language behavior is introduced,\nit will cause the words to shift in the original vector space.\nThe speciﬁc change is reﬂected in the strength and direction\nand even causes its meaning to be biased to the opposite side.\nFurthermore, the heterogeneity of cross-modality typically\nincreases the difﬁculty of the analysis of human language\nbecause the variable sampling rate for each modal sequence\ncan lead to misaligned inherent data [13], expressed as\nan ‘‘unaligned’’ multimodal language sequence. Therefore,\nthe ﬁnal result of multimodal emotional discrimination is\naffected not only by the long-term dependence relationship\nwithin a speciﬁc mode but also by the long-term depen-\ndence relationship between ‘‘unaligned’’ multiple modalities.\nTherefore, how to coordinate the long-term dependencies\nwithin the modalities and the long-term dependencies\nbetween ‘‘unaligned’’ multiple modalities is a very important\nresearch topic.\nIn response to the above problems, we proposed the mul-\ntimodal encoding-decoding network with transformer, which\nis a model for handling human ‘‘unaligned’’ multimodal lan-\nguages. The main contributions of this paper are:\n• A new model for processing multi-modal data, which\nis used to solve the problem of the dynamic change of\nthe weight of multi-modal data in the time dimension,\nand update the cross-modal weight value in an iterative\nmanner.\n• Solve long-term dependencies within a single modality\nand long-term dependencies across modalities, focusing\non solving the problem of the offset of the meaning of\nwords in natural language data caused by non-natural\nlanguage data.\nIn order to verify the performance of our model in mul-\ntimodal sentiment analysis, we conducted experiments on\nthe benchmark CMU-MOSI and CMU-MOSEI datasets.\nWe retrained our model and the latest model previously\nproposed in the same experimental environment and evalu-\nated and compared the ﬁnal results. In all benchmark tests,\nour model can outperform the benchmark and is more stable\nthan other models.\nThe remainder of this article is organized as follows.\nIn Sect. II, we introduce some related work on multimodal\nemotion recognition. In Sect. III, we elaborate on the overall\narchitecture of our model. In Sect. IV, we describe the data\nset and baseline model used in the experiment in detail.\nIn Sect. V, we present the results of the experiment and\nreport the necessary analysis. We summarized our model and\nelaborated on future work in Sect. VI.\nII. RELATED WORKS\nIn this section, we mainly discuss the related work of multi-\nmodal sentiment analysis and brieﬂy introduce the two basic\nmodels we will use.\nA. MULTIMODAL SENTIMENT ANALYSIS\nMultimodal sentiment analysis is now a popular research\ndirection. It models natural language and non-natural lan-\nguage to gain emotional understanding. With the emer-\ngence of a large number of multimodal datasets (such as\nCMU-MOSI [14] and CMU-MOSEI [15]), scholars have\nsuccessively proposed many models for multimodal senti-\nment analysis. In early work, fusion methods directly con-\nnected multiple modal data [16]–[19], and the primary and\nsecondary relationships between the modes were not studied.\nFor example, in literature [16], the author regards the prob-\nlem of multimodal sentiment analysis as dynamic modeling\nwithin and between modalities. The single-mode, dual-mode,\nand three-mode dynamics are explicitly modeled by calculat-\ning the vector ﬁeld of the triple Cartesian product, and the\nmultimodal emotion fusion tensor is calculated. In [17], the\nauthor applies LSTM to each modal view to learn the inter-\naction of a speciﬁc view and reconstructs the LSTM memory\nnetwork to learn multimodal cross-attempt interaction infor-\nmation. In [18], the author decomposes the fusion problem\ninto multiple stages, and each stage focuses on a subset of\nmultimodal signals for specialized and effective fusion. Then,\nthe fusion method is combined with the recurrent neural net-\nwork system to model the interactions in time and modalities.\nIn literature [19], the author proposed a multimodal attention\nframework based on recurrent neural networks to learn the\njoint relationships between multiple modalities and discourse\nand used contextual information for discourse-level emotion\nprediction. The multimodal fusion methods mentioned above\nall put multiple modal information in the same position\nwithout emphasizing the primary and secondary relationships\nbetween each modal information. Our research is closer to\nthe work reported in [12], [20], [21] conﬁrming that natu-\nral language information occupies an important position in\nmultimodal sentiment analysis. In literature [12], for the ﬁrst\ntime, the author proposed that a speaker’s intentions usu-\nally change dynamically according to different nonlanguage\nenvironments. When modeling human language, not only the\nVOLUME 10, 2022 28751\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\nTABLE 1. A summary and comparison of two multimodal fusion methods.\nliteral meaning of words but also the nonverbal context in\nwhich these words appear must be considered. To this end,\nthe author proposes a gated modal hybrid network, which\ndynamically moves word representations based on nonverbal\ncues. In literature [21], the author combines the gated modal\nhybrid network mentioned above with the BERT model with-\nout changing the basic structure of the BERT model, which\ncan actually accept nonverbal information. These studies all\nuse textual information as an important carrier and then intro-\nduce nonverbal behavior as auxiliary information to form a\nmultimodal emotional understanding. The core of our work is\nto use natural language information as the dominant modality\nand non-natural language information as the auxiliary modal-\nity to obtain the fusion vector representation in the natural\nlanguage vector space. Table. 1 summarizes and compares the\ntwo multimodal fusion methods.\nB. TRANSFORMER AND BERT\nIn our experiment, two basic encoding networks, the\ntransformer [22] and BERT (Bidirectional Encoder Repre-\nsentations from Transformers) [23], are mainly involved.\nA transformer is an acyclic neural architecture designed for\nsequence data modeling. It discards the RNN and CNN as\nthe basic models of sequence learning and completely adopts\nthe attention mechanism; therefore, the architecture does not\nhave the ability to capture sequential sequences. For this rea-\nson, the author uses position embedding in the architecture to\nrepresent time-series information and ﬁnally achieves better\nperformance than the loop structure in terms of results, speed,\nand depth. BERT is a successful application of a transformer\nand a successful language model. The input embedding of\nthis model is generated by adding token embedding, segment\nembedding, and position embedding. Then, multiple encoder\nlayers are applied on top of these input embeddings. Each\nencoder has a multi-head attention layer and a feedforward\nlayer, and each layer has a residual connection with layer\nnormalization. BERT adopts the automatic coding method\nto learn the vector representation of the masked mark in the\nprocess.\nIII. PROPOSED APPROACH\nIn this section, we introduce in detail the Multimodal\nEncoding-Decoding Network with Transformer(MEDT).\nThe purpose of MEDT is to solve the problem of\nthe ‘‘unalignment’’ characteristic of multimodal language\nsequences, introduce word transfer representation, and ﬁnally\nobtain multimodal emotional fusion vector representation.\nDifferent from the previous strategy, we adopted a joint\nencoding and decoding method with text as the main informa-\ntion and sound and image as auxiliary information to obtain\nthe emotion fusion vector representation. Our model can be\ndivided into two parts: 1) The unimodal encoder, which is\nused to handle the long-term dependencies within the modal\nand to encode unimodal information; and 2) The multimodal\njoint-decoder, which is used to solve the long-term dependen-\ncies between ‘‘unaligned’’ multiple modalities, dynamically\nupdate the weight attribute values between different modal-\nities at different times, and ﬁnally obtain multimodal fusion\nfeature representation. Fig. 1 shows the overall architecture\nof the model.\nThe input of the MEDT is multimodal sequence data. This\narticle mainly handles the following three types of multi-\nmodal sequence data: natural language {Language(l)}and\nnon-natural language {Visual(v),Acoustic(a)}, where Lan-\nguage is the original text data Il input into the BERT model\n(see Sect. III-A1). The initial feature vector of Visual and\nAcoustic is expressed as Im ∈RTm×dm (m ={a,v}), where dm\nand Tm represent their respective time dimension and feature\ndimension.\nA. UNIMODAL ENCODER\nIn this part, we explained the unimodal encoder, and we used\ndifferent encoding methods for natural language and non-\nnatural language.\n1) NATURAL LANGUAGE ENCODER\nWe used a pre-trained BERT [23] model that performed well\nin the text domain to encode plain text to extract sentence\nrepresentations with long-term dependencies. We consider\nthat text information plays a leading role in the ﬁnal results\nof sentiment analysis. In order to ensure that the BERT model\ncan extract sentence representations containing sentiment\ninformation, we roughly ﬁne-tuned the BERT network on\nthe pure text sentiment classiﬁcation dataset. The ﬁne-tuned\nBERT model does not need to achieve the best accuracy and\nis only used to ensure that a general sentence representation\nwith emotional attributes is obtained. When we apply the\nmodel to text in multimodal data, we will also perform syn-\nchronous training. We apply the 12-layer BERT to the IMDB\ndataset [24], which contains 50,000 positive and negative\n28752 VOLUME 10, 2022\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\nreviews from the movie database, including 36,000 training\nset samples, 4,000 validation set samples, and 10,000 tests\nset samples. The ﬁne-tuned BERT model can achieve 89.42%\naccuracy on the IMDB dataset sentiment dual classiﬁcation\ntask.\nGiven the original text data Il =[I1,I2,··· ,IN ], N is the\nnumber of samples. Each sample In (n ∈N) is a language\nsequence In =[i1,i2,··· ,iT ] ∈RT ×d that carries T word-\npiece tokens. Two special tokens [CLS] and [SEP] are added\nto In, and we will use the former to predict emotions later.\nThen, we input In into the input embedder, and its output is\nthe input encoding vector En =[eCLS ,e1,e2,··· ,eT ,eSEP]\nof BERT after adding markers, segments, and position\nembedding.\nEn =InputEmbedder (In)∈RTl ×d (1)\nTl is equal to T plus two special symbols. d is the initial\nencoding vector dimension. Finally, we input En into the\nﬁne-tuned BERT model and obtain the lexical embedding\nXn of the last layer as the text embedding Xl with long-term\ndependencies.\nXl =Xn =Finetuned −BERT (In)∈RTl ×dl (2)\nwhere dl represents the feature dimension of the language\nmodality after passing through the BERT network, which is\n768 dimensions.\n2) NON-NATURAL LANGUAGE ENCODER\nFor visual and acoustic data Im ∈ RTm×dm (m ={a,v}),\nwe emulate the way the transformer encodes text data, and we\napply the encoder of the transformer to non-natural language\ndata. In this paper, for convenience, we call it a Non-natural\nLanguage Transformer Encoder (NNLE). For any natural lan-\nguage, the position and order of words in a sentence are very\nimportant. They are not only part of the grammatical struc-\nture of a sentence but also important concepts that express\nsemantics. If the position or sequence of a word in a sentence\nis different, the meaning of the entire sentence may deviate.\nSimilar to natural language, for non-natural language data\nwith a time dimension, such as continuous changes of facial\nexpressions or voice intonations, if the arrangement order is\ndifferent, the meaning expressed will also be affected.\nSince the transformer model discards the RNN (Recurrent\nNeural Network) and CNN (Convolutional Neural Network)\nas the basic models of sequence learning, it completely\nadopts the attention mechanism, which means that the trans-\nformer model does not have the ability to capture time\nseries. In order to enable the sequence to carry time infor-\nmation, following [22], we add position information embed-\nding (PE) to Im and then apply a Position-wise Feedforward\nNetwork (PFN) to obtain non-natural language embedding\ndata Pm ∈ RTm×dm (m ={a,v}) with relative position\ninformation:\nPFN =xW +b\nPm =PFN (Im +PE (Im)) (3)\nThe reason why the network is position-wise is that the\ntransformation parameters of each position t are the same\nwhen passing the linear layer. PE (Im) ∈RTm×dm calculates\nthe ﬁxed position embedding of each position index of the\nnon-natural language data in the time dimension. We leave\nmore details of the positional embedding to I.\nNNLE is the same as the traditional transformer encoder\n(see Fig. 1, right). It consists of N identical coding lay-\ners, each layer consists of two sublayers, and each sublayer\nintroduces residual connections and layer normalization. The\noverall structure is summarized as:\nLayerNorm (x +SubLayer (x)) (4)\nThe two sublayers are the multi-head attention mech-\nanism (MHA) and position-wise fully connected feed-\nforward network (FFN). The ﬁrst sublayer MHA utilizes a\nself-attention block deﬁned as a scaled dot product function:\nAttention (Q,K,V )=softmax\n(QKT\n√dk\n)\nV (5)\nwhere Q, K, and V are input vectors with the same shape.\nExpression √dk is a scaling factor, where dk is the feature\ndimension of the input vector. Multi-head means projecting\nQ, K, and V through h different linear transformations and\nﬁnally stitching together different attention results:\nMultiHeadAttention (Q,K,V ) =Concat (head1,..., headh) W o\nwhereheadi =Attention\n(\nQW Q\ni ,KW K\ni ,VW V\ni\n)\n(6)\nThe second sublayer FFN consists of two linear transfor-\nmations, and the ﬁrst linear transformation is followed by a\nReLU activation function. Similar to the PFN, the FFN is\nposition-wise because the transformation parameters of each\nposition t are the same when passing through the linear layer:\nFeedForward(x)=max (0,xW1 +b1)W2 +b2 (7)\nAssume that the input of the 0th layer Z[0]\nm =Pm. In the\nith coding layer, the output Z[i−1]\nm of the previous layer ﬁrst\npasses through the multihead attention block to obtain the\nintermediate output\nZ[i]\nm , which can be expressed as:\nZ[i]\nm =LayerNorm\n×\n(\nZ[i−1]\nm +MultiHeadAttention\n(\nZ[i−1]\nm ,Z[i−1]\nm ,Z[i−1]\nm\n))\n(8)\nThen, through the second sublayer feedforward network,\nthe ﬁnal output Z[i]\nm of the ith coding layer is obtained:\nZ[i]\nm =LayerNorm\n(\nZ[i]\nm +FeedForward\n(\nZ[i]\nm\n))\n(9)\nFinally, after N coding layers, we obtain the non-natural\nlanguage embedding Xm =Z[N]\nm ∈RTm×dm (m ∈{a,v})with\ntiming information.\nVOLUME 10, 2022 28753\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\nFIGURE 1. The overall architecture of the MEDT. It consists of two parts: 1) the unimodal encoder and 2) the\nmultimodal joint-decoder. The unimodal encoder is divided into the natural language encoder of the fine-tuned\nBERT and the non-natural language encoder (NNLE), and the PFN is the Positionwise Feedforward Network. The\nmultimodal joint-decoder is a reconstruction of the transformer’s decoder. The right side of the figure is the\nencoder architecture of the transformer.\nFIGURE 2. The left side shows the overall architecture of the multimodal joint decoder, and the right side shows the\ncross-modal attention mechanism we use.\nB. MULTIMODAL JOINT-DECODER\nIn this part, we reconstruct the decoder of the transformer\nto obtain the multimodal fusion embedding representation,\nwhich is called the multimodal joint-decoder. First, this net-\nwork considers the characteristics of word vectors in the fea-\nture space that are affected by non-natural language data; and\nsecond, the network solves the problem of long-term depen-\ndence between cross-modalities. (Fig. 2 shows the overall\nstructure). The multimodal joint-decoding layer is composed\nof two sublayers. The second sublayer adopts a position-wise\nfully connected feed-forward network such as NNLE. The\ndifference between the two networks is that in the multi-\nhead cross-modal attention mechanism, we use a cross-modal\nattention block (CM ) containing a scaled dot product\nfunction [13].\nIn the cross-modal attention block (CM ), two different\nmodal vectors Xβ ∈RTβ×dβ and Xα ∈RTα×dα are given.\nWe deﬁne the queries as Qβ =XβWQβ, the keys as Kα =\nXαWkα, and the values as Vα = XαWVα, where WQβ ∈\nRdβ×dk , WKα ∈Rdα×dk , and WVα ∈Rdα×dk are the weights of\n28754 VOLUME 10, 2022\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\nthe linear transformation. Therefore, the cross-modal embed-\nding Yβ ∈RTβ×dk from βto αcan be obtained:\nYβ =CMα→β\n(\nQβ,Kα,Vα\n)\n=softmax\n(QβKT\nα√dk\n)\nVα\n=softmax\n(\nXβWQβ\n(\nXαWkα\n)T\n√dk\n)\nXαWVα (10)\nAmong the variables, Yβ and Qβ have the same length\n(that is, Tβ), but the data information comes from the feature\nspace of Vα. Expression √dk is a scaling factor. In particular,\nthe function softmax calculates the attention score matrix\nS ∈ RTβ×Tα from modality β to modality α, where the\n(i,j)th score indicates the degree of correlation between the\ninformation at the ith time step of modality β and the infor-\nmation at the jth time step of modality α. Hence, the ith time\nstep of Yβ is a weighted summary of Vα, with the weight\ndetermined by the ith row in the attention score matrix S.\nHowever, CMα→β is just single-head cross-modal attention.\nThe multi-head uses h different linear transformations to\nproject Xβ and Xα and ﬁnally splices the different attention\nresults to obtain the cross-modal embedding representation\nXα→β =Xβ ∈RTβ×dβ of modality βto modality α:\nXα→β =MultiheadCMα→β\n(\nXβ,Xα\n)\n=Concat (head1,··· ,headh)W O\nwhereheadi =CMα→β\n(\nXβW i\nQβ,XαW i\nKα,XαW i\nVα\n)\n(11)\nFor the convenience of the following description, we sum-\nmarize the multi-head cross-modal attention mechanism as:\nLayerNorm\n(\nXβ +MultiHeadCMα→β\n(\nXβ,Zα\n))\n(12)\nThe input of the multimodal joint-decoder is three types of\ncoded multimodal embeddings Xl ∈RTl ×dl , Xa ∈RTa×da ,\nand Xv ∈RTv×dv , where Xl is the text embedding through\nthe BERT encoder, and Xa and Xv are the acoustic and\nvisual embeddings with time information through the NNLE,\nrespectively. In this paper, we always use the natural language\ndata Xl as the query vector and use Xa and Xv as the key and\nvalue vectors, respectively, to obtain the cross-modal fusion\nembedding representation after the text embedding is offset\nunder the inﬂuence of non-natural language data.\nThe multimodal decoder is composed of N multimodal\ndecoding layers. Assuming that the text embedding of the\n0th layer is represented as Z[0]\nl =Xl ∈RTl ×dl , for the ith\ndecoding layer, we ﬁrst use audio Xa as the initial keys and\nvalues to obtain the cross-modal text embedding representa-\ntion Z∗[i]\na→l ∈RTl ×dl :\nZ∗[i]\na→l =LayerNorm\n×\n(\nZ[i−1]\nl +MultiHeadCMa→l\n(\nZ[i−1]\nl ,Za\n))\n(13)\nAt this time, the data in Z∗[i]\na→l belong to the text feature\nspace, but compared to before, the data shift in direction under\nthe inﬂuence of audio information. Then, Xv is passed into the\ncross-modal attention mechanism as the new keys and values,\nand Z∗[i]\na→l is used as the query vector to further obtain the new\ncross-modal text embedding representation Z∗[i]\nv→l ∈RTl ×dl :\nZ∗[i]\nv→l =LayerNorm\n×\n(\nZ∗[i]\na→l +MultiHeadCMv→l\n(\nZ∗[i]\na→l ,Zv\n))\n(14)\nHere, Z∗[i]\nv→l is a text embedding representation containing\ntwo types of non-natural language information. Finally, after\na position-wise feed-forward network, the cross-modal text\nembedding representation Z[i]\nl of the ith layer is obtained.\nZ[i]\nl =LayerNorm\n(\nZ∗[i]\nv→l +FeadForward\n(\nZ∗[i]\nv→l\n))\n(15)\nFinally, after n decoding layers, we obtain the ﬁnal\ncross-modal text embedding representation X(a,v)→l =\nZ[N]\nl ∈RTl ×dl . Then, we choose the feature vector of the\nspecial symbol token [CLS] in the text embedding as the\nembedding representation Xf ∈ Rdl of multimodal fusion\ninformation and further use it for sentiment analysis.\nIV. EXPERIMENTAL SETTINGS\nIn this section, we introduce our experimental set-\ntings, including the experimental datasets, baselines, and\nevaluations.\nA. DATASETS\nIn this work, we use two public multimodal sentiment analy-\nsis datasets: MOSI and MOSEI. Here, we give a brief intro-\nduction to the above datasets.\n1) MOSI\nThe CMU-MOSI [14] dataset is one of the most pop-\nular benchmark datasets for multimodal sentiment analy-\nsis. It comprises 2,199 short monologue video clips taken\nfrom 93 YouTube movie review videos. Human annotators\nlabel each sample with a sentiment score from -3 (strongly\nnegative) to 3 (strongly positive). We further processed the\ndataset and divided it into a training set containing 1,284\nsamples, a validation set containing 229 samples, and a test\nset containing 686 samples.\n2) MOSEI\nThe CMU-MOSEI [15] dataset expands its data with a higher\nnumber of utterances and greater variety in samples, speakers,\nand topics than CMU-MOSI. The dataset contains 22,856\nannotated video segments (utterances) from 5,000 videos,\n1,000 distinct speakers and 250 different topics. We also\nprocessed the dataset further and divided it into a training set\ncontaining 16,326 samples, a validation set containing 1,871\nsamples, and a test set containing 4,659 samples.\nB. BASELINES\nIn order to verify the performance of the MEDT, we con-\nducted a fair comparison with the following various state-\nof-the-art models for multimodal language analysis. These\nVOLUME 10, 2022 28755\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\nTABLE 2. Results for multimodal sentiment analysis on CMU-MOSI and CMU-MOSEI with aligned and unaligned multimodal sequences. For the\nperformance indicators,h means higher is better andℓ means lower is better. (SD) is the standard deviation of the results of five experiments. For the\nmodel, (B) means that the language features are based on BERT. In Acc-2 and F1-Score, the left of the ‘‘/’’ is calculated as ‘‘neg./non-neg. ’’ and the right is\ncalculated as ‘‘neg./pos. ’’ .\nmodels are trained using extracted BERT word embeddings\nas their language input:\nTFN: The Tensor Fusion Network [16] creates a multidi-\nmensional tensor to capture unimodal, bimodal, and trimodal\ninteractions and explicitly model intramodal and intermodal\ndynamics.\nLMF: Low-rank Multimodal Fusion [25] is an improve-\nment of the TFN that uses a low-rank tensor to perform\nmultimodal fusion to improve efﬁciency.\nMulT: The Multimodal Transformer [13] adopts direc-\ntional pairwise cross-modal attention, which attends to inter-\nactions between multimodal sequences across distinct time\nsteps and latently adapts streams from one modality to\nanother.\nMISA: Modality-Invariant and -Speciﬁc Representa-\ntions [20] project each modality to two subspaces of modal\ninvariant and speciﬁc modalities to capture cross-modal com-\nmonality and unimodal private features for task prediction\nfusion.\nSelF-MM: The Self-Supervised Multitask Multimodal\nsentiment analysis network [21] obtains an informative uni-\nmodal representation by jointly learning a multimodal task\nand three unimodal subtasks. Among the subtasks, the label\nof the unimodal subtask is obtained through a label generation\nmodule based on a self-supervised learning strategy. Then,\nthe multimodal and single-modal tasks are jointly trained to\nlearn consistency and differences, respectively.\nC. EXPERIMENTAL DESIGN\n1) EXPERIMENTAL DETAILS\nWe use Adam as the optimizer and the initial learning rate\nof 5e-5 for the BERT natural language encoder. The learn-\ning rate of the two non-natural language encoders is 0.001,\nand the learning rate of the multimodal decoder and other\nnetworks is 0.0001. For a fair comparison, we conducted\nexperiments on our model and the model mentioned above\nunder the same experimental conditions. We ran each model\nﬁve times and reported the average performance.\n2) EVALUATION METRICS\nFollowing previous work [15], [20], the emotional inten-\nsity predictions using the MOSI and MOSEI datasets are\nregression tasks, and the mean absolute error (MAE) and\nPearson correlation (Corr) are used as the performance indi-\ncators. Additionally, the benchmark also involves classiﬁ-\ncation scores that include seven-class accuracy (Acc-7) and\nﬁve-class accuracy (Acc-5) ranging from -3 to 3, binary\naccuracy (Acc-2), and the F-Score. For the binary accuracy\nscore, we chose two different evaluation methods. The ﬁrst\nis negative/non-negative classiﬁcation, where non-negative\nlabels are based on scores ≥0 [26]. The second is the more\naccurate negative/positive classiﬁcation, where negative and\npositive classes are assigned to sentiment scores of < 0 and\n>0, respectively [13]. We use the segment mark -/- to report\nthe results of these two indicators, where the score on the left\nrepresents neg./non-neg. and the score on the right is neg./pos.\nFurthermore, we calculate the standard deviation (SD) of the\nﬁve experimental results of the abovementioned evaluation\nindexes and use it as the stability index of the model.\nV. RESULTS AND DISCUSSION\nIn this section, we have conducted a detailed analysis and\ndiscussion of the experimental results on the CMU-MOSI and\nCMU-MOSEI datasets.\nA. RESULTS\nTable. 2 shows the comparison results on the MOSI and\nMOSEI datasets. For a fair comparison, we experimented\nwith our model and the benchmark models under the same\nexperimental conditions. Following previous work [20], [21],\n28756 VOLUME 10, 2022\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\nTABLE 3. Examples from the CMU-MOSI data set. The true emotional label lies between strongly negative (−3) and strongly positive (+3). According to\nthe different ‘‘data settings’’ , we performed fitting experiments on the ‘‘aligned’’ and ‘‘unaligned’’ data.\nwe tested our model (MEDT) on ‘‘aligned’’ data and\n‘‘unaligned’’ data according to the different ‘‘data settings’’\nand compared its results with those of the benchmark mod-\nels. First, we apply our model and the benchmark mod-\nels to ‘‘unaligned’’ data. Compared with the benchmark\nmodels, our model achieved signiﬁcant improvements in\nall evaluation indicators. As mentioned earlier, when TFN\nand LMF networks perform multimodal sentiment analysis,\neach modality data has an equal effect on the ﬁnal senti-\nment result. Our model MEDT takes natural language data\nas the dominant information iteratively updates the weight\nratio of non-natural language data to natural language data,\nand dynamically obtains multimodal fusion feature repre-\nsentations to obtain emotional results. As can be seen from\nTable. 2, our method has a substantial improvement in the\nevaluation indicators of classiﬁcation or regression com-\npared with the previous two networks. For example, on the\n‘‘unaligned’’ MOSI data, especially in the regression task, the\nmean squared error (MAE) dropped directly by 23.61 points,\nand in the binary classiﬁcation accuracy metric and F1 score,\nit also steadily increased by 5 points. Moreover, compared\nwith the state-of-the-art model Self-MM, the accuracy has\nalso been improved. Then, for the ‘‘aligned’’ data, we applied\nthe MISA and Self-MM models that performed well on the\n‘‘unaligned’’ data to the ‘‘aligned’’ data, and our model still\nhad the best results. In addition to the basic evaluation indi-\ncators, we also recorded the results of ﬁve experiments and\ncalculated the standard deviation to show the stability of the\nmodel. The results show that the standard deviation of our\nmodel for all evaluation indicators is low, and the ﬂuctua-\ntion is generally maintained between 0.2-0.9. Compared with\nother models, our model has strong stability and resistance to\nrandomness, which means that the model can obtain relatively\nstable output results under different conditions. In Fig. 3,\nwe show the results of ﬁve experiments on the Pearson corre-\nlation (Corr) indicator of the two datasets in the regression\ntask. Our model guarantees a high Corr index while also\nensuring the stability of the model.\nB. DISCUSSION\nWeighting each modality data and enhancing the associ-\nated modality data for a particular task can better achieve\nthe desired results. Our model further validates this idea.\nHowever, our model is less ﬂexible than the method that\nautomatically obtains the dominant mode data by building\na weight distribution model. Taking the three modalities of\nnatural language (language) and non-natural language (audio\nand visual) in this paper as examples, our model assumes in\nadvance that natural language is dominant for emotion acqui-\nsition. Taking non-natural language as the inﬂuencing factor\nof natural language offset dynamically adjusts the weight\nvalue between natural language and non-natural language.\nThis idea is slightly different from the above, the acquisition\nof the dominant modal data is changed from the model’s self-\nlearning to artiﬁcial assumption, but the ﬁnal presentation\nresult is ideal. Therefore, this provides a good idea for our\nnext step, that is, how to let the model learn and determine\nthe dominant modal data autonomously. At the same time,\nit can be seen from Table. 2 that the evaluation results of our\nmodel on aligned data and unaligned data are also slightly\ndifferent, which is reﬂected in the fact that the evaluation indi-\ncators of unaligned data are generally better than aligned data.\nThis is because the multi-modal feature fusion used in the\nearly stage is based on the cascade of feature vectors, which\nrequires multiple modal sequence data to be consistent in\nthe time dimension, which is convenient for model building.\nBut this leads to the unavoidable loss of some information.\nOur model does not need to take this factor into account,\nand access to more information ensures the superiority of\nour model.\nC. QUALITATIVE ANALYSIS\nIn order to verify the more intuitive performance of our model\nin regression tasks, in Table. 3, we selected multimodal sam-\nples from the MOSI dataset to display the results. We applied\nthe model to ‘‘aligned’’ data and ‘‘unaligned’’ data and ﬁtted\nthe true values separately. Our model has a better ﬁtting effect\non samples showing strong emotions. Regardless of whether\nit is for strong positive emotions or negative emotions, the\nﬁtting error of our two experimental results to the true value\nis maintained at ±0.1; furthermore, for the neutral sample,\nthe ﬁtting error of the emotional result is approximately\n±0.25. Overall, our model shows an excellent emotional\nﬁtting effect.\nVOLUME 10, 2022 28757\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\nFIGURE 3. On the MOSI and MOSEI datasets, we show the standard\ndeviations of the five experimental results on the Corr indicator on the\naligned data and the unaligned data, respectively.\nVI. CONCLUSION\nIn this article, we introduce a Multimodal Encoding-Decoding\nNetwork with Transformer (MEDT) for multimodal senti-\nment analysis. We use different encoding methods for the\nthree multimodal information: for language data, we use a\npre-trained BERT model to obtain lexical embedding; and for\nvisual and acoustic language data, we use the transformer’s\nencoder to encode non-natural language data to obtain\nembedding representation. Finally, we also reconstructed the\ndecoder to obtain cross-modal multimodal embedding repre-\nsentation. Our model ﬁnally solves the long-term dependence\nrelationship between speciﬁc modalities and multimodalities\nand considers the offset characteristics of text embedding\nunder the inﬂuence of non-natural language information. Our\nexperiments have proven the superior performance of the\nMDET. However, our model was designed around the idea\nof taking natural language as the dominant and non-natural\nlanguage as auxiliary information. This greatly limits the\nﬂexibility to switch dominance between multiple modalities\nin the model. Next, we will focus on the idea of ﬂexibly\nobtaining dominant information among multiple modal infor-\nmation, and propose a better model for multimodal sentiment\nanalysis. In addition to this, we will also envision better\nmultimodal data alignment methods to better ﬁt our models.\nDISCLOSURES\nThe authors declare no conﬂict of interest.\nAPPENDIX I. POSITIONAL EMBEDDING\nBecause the Transformer model abandons RNN and CNN as\nthe basic model of sequence learning and completely adopts\nthe attention mechanism, the Transformer model does not\nhave the ability to capture sequential sequences. At the same\ntime, the order of arranging the input sequence does not\nchange the behavior of the Transformer or change its output.\nTo solve this problem, following the work of [13], we use\nsin and cos functions to encode the position information of\nthe sequence of length T , and the frequency is determined\nby the feature dimension index. In particular, we deﬁne the\npositional embedding (PE) of the sequence X ∈RT ×d (where\nT is the length) as a matrix, where:\nPE[pos,2i] =sin\n( pos\n10000\n2i\nd\n)\nPE[pos,2i +1] =cos\n( pos\n10000\n2i\nd\n)\n(16)\nwhere pos is the position index in the time dimension, i is the\ndimension, and the value is 0,\n[d\n2\n]\n. Therefore, each feature\ndimension of PE is a position value showing a sinusoidal\npattern. After calculation, the position embedding is directly\nadded to the sequence, so that X +PE encodes the position\ninformation of the element at each time step.\nREFERENCES\n[1] P. Bo and L. Lillian, ‘‘A sentimental education: Sentiment analysis using\nsubjectivity summarization based on minimum cuts,’’ in Proc. Assoc.\nComput. Linguistics, 2004, p. 271, doi: 10.3115/1218955.1218990.\n[2] G. Vinodhini and R. M. Chandrasekaran, ‘‘Sentiment analysis and opin-\nion mining: A survey,’’ Int. J., vol. 2, no. 6, pp. 282–292, 2012, doi:\n10.1007/978-1-4899-7502-7.\n[3] H. Pham, T. Manzini, P. Liang, and B. Poczos, ‘‘Seq2seq2sentiment:\nMultimodal sequence to sequence models for sentiment analysis,’’ in Proc.\nChallenge-HML, Jul. 2018, pp. 53–63, doi: 10.18653/v1/w18-3308.\n[4] R. G. Milo, ‘‘Tools, language and cognition in human evolution,’’\nInt. J. Primatol., vol. 16, no. 6, pp. 1029–1031, Dec. 1995, doi:\n10.1007/BF02696116.\n[5] C. Manning, M. Surdeanu, and J. Bauer, ‘‘The Stanford CoreNLP nat-\nural language processing toolkit,’’ in Proc. Assoc. Comput. Linguistics,\nJun. 2014, pp. 55–60, doi: 10.3115/v1/P14-5010.\n[6] S. Poria, E. Cambria, and D. Hazarika, ‘‘Context-dependent sentiment\nanalysis in user-generated videos,’’ in Proc. Assoc. Comput. Linguistics,\nvol. 1, Jul. 2017, pp. 873–883, doi: 10.18653/v1/P17-1081.\n[7] A. Abate, P. Barra, S. Barra, C. Molinari, and M. Nappi, ‘‘Clustering facial\nattributes: Narrowing the path from soft to hard biometrics,’’ IEEE Access,\nvol. 8, pp. 9037–9045, 2020, doi: 10.1109/ACCESS.2019.2962010.\n[8] T. Baltrušaitis, C. Ahuja, and L.-P. Morency, ‘‘Multimodal\nmachine learning: A survey and taxonomy,’’ IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 41, no. 2, pp. 423–443, Feb. 2019, doi:\n10.1109/TPAMI.2018.2798607.\n[9] N. Majumder, D. Hazarika, A. Gelbukh, E. Cambria, and S. Poria,\n‘‘Multimodal sentiment analysis using hierarchical fusion with context\nmodeling,’’ Knowl.-Based Syst., vol. 161, pp. 124–133, Dec. 2018, doi:\n10.1016/j.knosys.2018.07.041.\n[10] A. Hu and S. Flaxman, ‘‘Multimodal sentiment analysis to explore the\nstructure of emotions,’’ in Proc. Assoc. Comput. Mach., New York, NY ,\nUSA, vol. 9, Jul. 2018, pp. 350–358, doi: 10.1145/3219819.3219853.\n[11] R. Bamler and S. Mandt, ‘‘Dynamic word embeddings,’’ in Proc.\nJMLR, Sydney, NSW, Australia, vol. 70, 2017, pp. 380–389, doi:\n10.5555/3305381.3305421.\n[12] Y . Wang, Y . Shen, Z. Liu, P. P. Liang, and A. Zadeh, ‘‘Words can shift:\nDynamically adjusting word representations using nonverbal behaviors,’’\nin Proc. AAAI Conf. Artif. Intell., Jul. 2019, vol. 33, no. 1, pp. 7216–7223.\n[Online]. Available: https://arxiv.org/abs/1811.09362\n[13] Y . Tsai, S. Bai, P. Liang, and J. Kolter, ‘‘Multimodal transformer for\nunaligned multimodal language sequences,’’ in Proc. Assoc. Comput. Lin-\nguistics, Jul. 2019, pp. 6558–6569, doi: 10.18653/v1/P19-1656.\n[14] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, ‘‘MOSI: Multimodal\ncorpus of sentiment intensity and subjectivity analysis in online opinion\nvideos,’’CoRR, vol. abs/1606.06259, pp. 1–10, Jul. 2016.\n[15] A. Zadeh, P. Liang, and S. Poria, ‘‘Multimodal language analysis in the\nwild: CMU-MOSEI dataset and interpretable dynamic fusion graph,’’ in\nProc. Assoc. Comput. Linguistics, vol. 1, Jul. 2018, pp. 2236–2246, doi:\n10.18653/v1/P18-1208.\n[16] A. Zadeh, M. Chen, and S. Poria, ‘‘Tensor fusion network for multimodal\nsentiment analysis,’’ in Proc. Assoc. Comput. Linguistics, Sep. 2017,\npp. 1103–1114, doi: 10.18653/v1/d17-1115.\n28758 VOLUME 10, 2022\nQ. Qiet al.: MEDT: Using Multimodal Encoding-Decoding Network as in Transformer\n[17] A. Zadeh, P. Liang, N. Mazumder, and S. Poria, ‘‘Memory fusion network\nfor multi-view sequential learning,’’ in Proc. AAAI Conf. Artif. Intell.,\nApr. 2018, vol. 32, no. 1, pp. 5634–5641.\n[18] P. Liang, Z. Liu, A. Zadeh, and L. Morency, ‘‘Multimodal language analy-\nsis with recurrent multistage fusion,’’ in Proc. Assoc. Comput. Linguistics,\nNov. 2018, pp. 150–161, doi: 10.18653/v1/D18-1014.\n[19] D. Ghosal, M. S. Akhtar, D. Chauhan, and S. Poria, ‘‘Contextual inter-\nmodal attention for multi-modal sentiment analysis,’’ in Proc. Assoc. Com-\nput. Linguistics, Nov. 2018, pp. 3454–3466, doi: 10.18653/v1/D18-1382.\n[20] D. Hazarika, R. Zimmermann, and S. Poria, ‘‘MISA: Modality-invariant\nand speciﬁc representations for multimodal sentiment analysis,’’ in\nProc. Assoc. Comput. Mach., New York, NY , USA, vol. 10, 2020,\npp. 1122–1131, doi: 10.1145/3394171.3413678.\n[21] W. Yu, H. Xu, Z. Yuan, and J. Wu, ‘‘Learning modality-speciﬁc represen-\ntations with self-supervised multi-task learning for multimodal sentiment\nanalysis,’’ in Proc. AAAI Conf. Artif. Intell., Nov. 2021, vol. 35, no. 12,\npp. 10790–10797, doi: 10.18653/v1/D18-1382.\n[22] A. Vaswani, N. Shazeer, and N. Parmar, ‘‘Attention is all you need,’’ in\nProc. Adv. Neural Inf. Process. Syst., Nov. 2017, pp. 5998–6008.\n[23] J. Devlin, M. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Tech-\nnol., vol. 1, Nov. 2019, pp. 4171–4186, doi: 10.18653/v1/N19-1423.\n[24] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts,\n‘‘Learning word vectors for sentiment analysis,’’ in Proc. 49th Annu.\nMeeting Assoc. Comput. Linguistics, Hum. Lang. Technol., vol. 1, 2011,\npp. 142–150, doi: 10.5555/2002472.2002491.\n[25] Z. Liu, Y . Shen, and V . Lakshminarasimhan, ‘‘Efﬁcient low-rank mul-\ntimodal fusion with modality-speciﬁc factors,’’ in Proc. Assoc. Comput.\nLinguistics, Jul. 2018, pp. 2247–2256, doi: 10.18653/v1/P18-1209.\n[26] A. Zadeh, P. Liang, S. Poria, P. Vij, and E. Cambria, ‘‘Multi-attention\nrecurrent network for human communication comprehension,’’ in Proc.\n32nd AAAI Conf. Artif. Intell., 2018, pp. 5642–5649.\nQINGFU QI was born in Liaocheng, Shandong,\nin 1997. He received the bachelor’s degree from\nthe Qilu University of Technology, in 2019.\nIn 2019, he studied for a master’s degree in con-\ntrol engineering at Tianjin University of Science\nand Technology. He is currently pursuing the joint\nmaster’s degree with the Tianjin University of Sci-\nence and Technology and the Tianjin Sino-German\nUniversity of Applied Sciences. Currently, he has\npublished an academic paper as the ﬁrst author.\nHis research interest includes multimodal sentiment analysis.\nLIYUAN LINreceived the M.S. degree in commu-\nnication engineering and the Ph.D. degree in infor-\nmation communication engineering from Tianjin\nUniversity, China, in 2009 and 2016, respectively.\nShe is currently working with the Tianjin Uni-\nversity of Science and Technology. Her research\ninterests include computer vision, virtual reality,\nand deep learning theory.\nRUI ZHANG received the Doctorate degree\nin information and communication engineer-\ning. He is currently the Deputy Dean of the\nSchool of Software and Communication, Tianjin\nSino-German University of Applied Sciences,\na member of the Communist Party of China,\nan Associate Professor, a Master Tutor, and a\nperson in charge of communication engineering.\nHe is a member of Tianjin ‘‘131’’ Innovative Talent\nTeam. In the past few years, he has published more\nthan ten high-level papers, of which the ﬁrst author or corresponding author\nhas published two SCI searches, six EI searches, and four national invention\npatents. The main research projects consist of four horizontal topics with\nfunding of more than 1.5 million and three provincial and ministerial-level\ntopics with funding of more than ten million. His research interests include\nmachine learning, multi-modal analysis, water, air communication, and het-\nerogeneous detection.\nCHENGRONG XUE was born in Tongcheng,\nAnhui, in 1996. He received the bachelor’s degree\nfrom Huaibei Normal University, in 2019. He is\ncurrently pursuing the master’s degree in elec-\ntronic information with the Tianjin University of\nScience and Technology, with a focus on natural\nlanguage processing and machine vision.\nVOLUME 10, 2022 28759",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8132880926132202
    },
    {
      "name": "Decoding methods",
      "score": 0.6035537719726562
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5758591890335083
    },
    {
      "name": "Modalities",
      "score": 0.5723773241043091
    },
    {
      "name": "Encoder",
      "score": 0.5693144202232361
    },
    {
      "name": "Transformer",
      "score": 0.5431079864501953
    },
    {
      "name": "Natural language",
      "score": 0.5352134704589844
    },
    {
      "name": "Gesture",
      "score": 0.49227848649024963
    },
    {
      "name": "Natural language processing",
      "score": 0.3853468894958496
    },
    {
      "name": "Speech recognition",
      "score": 0.35902369022369385
    },
    {
      "name": "Machine learning",
      "score": 0.34682923555374146
    },
    {
      "name": "Algorithm",
      "score": 0.08313301205635071
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I132369690",
      "name": "Tianjin University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210102575",
      "name": "TianjinSino-German University of Applied Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 41
}