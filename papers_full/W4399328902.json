{
  "title": "Combining LoRA to GPT-Neo to Reduce Large Language Model Hallucination",
  "url": "https://openalex.org/W4399328902",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4275272051",
      "name": "Shi-Han Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2362139827",
      "name": "Chia-Yu Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4399009725",
    "https://openalex.org/W4399154003",
    "https://openalex.org/W4389519128",
    "https://openalex.org/W4394770144",
    "https://openalex.org/W4398176048",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4398131214",
    "https://openalex.org/W4399177300",
    "https://openalex.org/W4392593764",
    "https://openalex.org/W4396762959",
    "https://openalex.org/W4396914777",
    "https://openalex.org/W4396833376",
    "https://openalex.org/W4389975983",
    "https://openalex.org/W4396701991",
    "https://openalex.org/W4396213476",
    "https://openalex.org/W4398196557",
    "https://openalex.org/W4388691793",
    "https://openalex.org/W4386639943",
    "https://openalex.org/W4377164404",
    "https://openalex.org/W4311431702",
    "https://openalex.org/W4390298466",
    "https://openalex.org/W4388708806",
    "https://openalex.org/W4390176146",
    "https://openalex.org/W4396675319"
  ],
  "abstract": "<title>Abstract</title> The deployment of Large Language Models (LLMs) often suffers from generating hallucinations, leading to outputs that appear plausible but are factually inaccurate or nonsensical. Incorporating Low-Rank Adaptation (LoRA) into GPT-Neo presents a novel approach to mitigating these hallucinations by leveraging the efficiency of low-rank approximations. This research details the integration of LoRA into GPT-Neo, demonstrating significant improvements in predictive performance, factual accuracy, and reduction in hallucination rates. The augmented model shows enhanced robustness and efficiency, making it more suitable for applications requiring high accuracy and reliability. Through comprehensive evaluations involving perplexity, BLEU, ROUGE-L scores, and qualitative analysis, the study highlights the augmented model's ability to generate more coherent and contextually appropriate text. The findings demonstrate the potential of LoRA to transform LLM deployment by reducing computational complexity and memory footprint, thus facilitating the use of large-scale models in resource-constrained environments. This advancement opens new possibilities for LLM applications across various domains, ensuring the accuracy and coherence of generated content.",
  "full_text": null,
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8446307182312012
    },
    {
      "name": "Computer science",
      "score": 0.7622386813163757
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5817927718162537
    },
    {
      "name": "Software deployment",
      "score": 0.5491889715194702
    },
    {
      "name": "Language model",
      "score": 0.5026512145996094
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.46305179595947266
    },
    {
      "name": "Machine learning",
      "score": 0.4167366027832031
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41462060809135437
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}