{
    "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
    "url": "https://openalex.org/W4389520485",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2096961312",
            "name": "Yi Wang",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A3091445163",
            "name": "Jiafei Duan",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2231782831",
            "name": "Dieter Fox",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2785843393",
            "name": "Siddhartha Srinivasa",
            "affiliations": [
                "University of Washington"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3207409407",
        "https://openalex.org/W4249906200",
        "https://openalex.org/W4383097638",
        "https://openalex.org/W4323366518",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4288262262",
        "https://openalex.org/W3034758614",
        "https://openalex.org/W4386506836",
        "https://openalex.org/W2746097825",
        "https://openalex.org/W4386075660",
        "https://openalex.org/W1505952289",
        "https://openalex.org/W3035507081",
        "https://openalex.org/W4285429195",
        "https://openalex.org/W2774005037",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W3090536218",
        "https://openalex.org/W2963159690",
        "https://openalex.org/W4288374511",
        "https://openalex.org/W2397288399",
        "https://openalex.org/W2794325560",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W4388720459",
        "https://openalex.org/W2962684798",
        "https://openalex.org/W3171654528",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W3214326397",
        "https://openalex.org/W3198961067",
        "https://openalex.org/W4285102264",
        "https://openalex.org/W4296557505"
    ],
    "abstract": "Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9743–9758\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nNEWTON:\nAre Large Language Models Capable of Physical Reasoning?\nYi Ru Wang† Jiafei Duan† Dieter Fox†‡ Siddhartha Srinivasa†\n†University of Washington ‡NVIDIA\n{yiruwang, duanj1, fox, siddh}@cs.washington.edu\nnewtonreasoning.github.io\nAbstract\nLarge Language Models (LLMs), through their\ncontextualized representations, have been em-\npirically proven to encapsulate syntactic, se-\nmantic, word sense, and common-sense knowl-\nedge. However, there has been limited ex-\nploration of their physical reasoning abilities,\nspecifically concerning the crucial attributes\nfor comprehending everyday objects. To ad-\ndress this gap, we introduce\n NEWTON ,\na repository and benchmark for evaluating\nthe physics reasoning skills of LLMs. Fur-\nther, to enable domain-specific adaptation of\nthis benchmark, we present a pipeline to en-\nable researchers to generate a variant of this\nbenchmark that has been customized to the ob-\njects and attributes relevant for their applica-\ntion. The NEWTON repository comprises a\ncollection of 2800 object-attribute pairs, pro-\nviding the foundation for generating infinite-\nscale assessment templates. The NEWTON\nbenchmark consists of 160K QA questions, cu-\nrated using the NEWTON repository to in-\nvestigate the physical reasoning capabilities\nof several mainstream language models across\nfoundational, explicit, and implicit reasoning\ntasks. Through extensive empirical analysis,\nour results highlight the capabilities of LLMs\nfor physical reasoning. We find that LLMs\nlike GPT-4 demonstrate strong reasoning ca-\npabilities in scenario-based tasks but exhibit\nless consistency in object-attribute reasoning\ncompared to humans (50% vs. 84%). Further-\nmore, the NEWTON platform demonstrates\nits potential for evaluating and enhancing lan-\nguage models, paving the way for their inte-\ngration into physically grounded settings, such\nas robotic manipulation. Project site: https:\n//newtonreasoning.github.io\n1 Introduction\nNatural Language Processing (NLP) has made re-\nmarkable progress using contextualized represen-\ntations trained on extensive unprocessed text data\nFigure 1. As works begin leveraging LLMs in physically\ngrounded contexts, it is crucial to understand whether such\nmodels possess the ability to reason about everyday scenarios.\nNEWTON , a repository, pipeline, and benchmark, facilitates\nevaluation of various LLMs in a physical reasoning context.\n(Zhao et al., 2023). As we start using LLMs in phys-\nically embodied pipelines (Driess et al., 2023; Ahn\net al., 2022; Wu et al., 2023), it is crucial to com-\nprehensively understand the extent that LLMs can\nperform physical reasoning. Some studies have pro-\nposed generalized frameworks to assess language\nmodel performance (Ribeiro et al., 2020; Kiela\net al., 2021), while others have designed question\nanswering and reading comprehension datasets to\nprobe LLMs (Zellers et al., 2018; Chen et al., 2019;\nRogers et al., 2023). However, few have explored\nthe physical reasoning ability of LLMs.\nPhysical reasoning involves the cognitive pro-\ncess of comprehending and predicting the dynam-\nics of physical systems based on observable phe-\nnomena and fundamental principles (McCloskey\net al., 1983; Carey, 2000). It encompasses the\ncapacity to make sense of the world by applying\nknowledge of attributes, such as brittleness, mal-\nleability, etc. By considering brittleness, for in-\nstance, we recognize the need to handle an object\nwith caution, while malleability suggests that an ob-\nject can be easily reshaped under pressure without\nfracturing. These abstract concepts enables reason-\n9743\ning about the response of objects to interactions or\nchanges in the environment, as shown in Figure 1.\nNevertheless, creating an evaluation framework\nfor physical reasoning is difficult, primarily due to\nthe lack of paired object-attribute data. Humans\npossess a wealth of knowledge regarding the inter-\nnal structure of objects and their interactions with\nthe physical world. However, this knowledge is of-\nten implicitly acquired, making it difficult to explic-\nitly represent such information. Previous studies in\nthe field of physical reasoning, such as Bisk et al.\n(2020); Aroca-Ouellette et al. (2021), have focused\non common-sense reasoning or small-scale valida-\ntion. As a result, the need for a comprehensive\nand systematic assessment of physical reasoning\nremains an open problem.\nTo address this gap, we propose NEWTON , a\nrepository, pipeline, and benchmark designed to\nevaluate the physical reasoning capability of LLMs.\nThe NEWTON repository consists of labeled\nobject-centric data, crowd-sourced for 700+ ob-\njects across 8 physical attributes. The NEWTON\npipeline introduces a method for systematically\ngenerating infinite evaluation questions tailored to\nspecific use cases. The NEWTON benchmark\nconsists of 160K pre-generated questions of pro-\ngressive difficulty, spanning tasks of foundational\nattribute comprehension, explicit application, and\nimplicit scenario-based analysis. Extensive empiri-\ncal findings demonstrate the unique contributions\nof NEWTON , revealing its usefulness for evalu-\nating LLMs’ understanding of underlying physics\nprinciples that dictate the behavior and properties\nof objects in everyday scenarios. Moreover, NEW-\nTON effectively compliments the existing reper-\ntoire of reasoning benchmarks and datasets, further\nenhancing the potential to assess and refine the\nphysical reasoning capabilities of LLMs.\n2 NEWTON Repository\nAt the core of constructing an evaluation frame-\nwork for physical reasoning lies the need for a\nrepresentation that captures the essential attributes\nof objects. In this section, we highlight the NEW-\nTON repository, including the identification and\na shortlist of objects and attributes, and obtaining a\nset of consistent object-attribute annotations.\n2.1 Objects and Attributes\nTo preserve grounding to physical objects, we lever-\nage the mainstream 3D object datasets: Objaverse\n(Deitke et al., 2023), YCB (Calli et al., 2015),\nScanned Objects by Google (Downs et al., 2022),\nand Amazon Berkeley Objects (Collins et al., 2022).\nTo establish a common representation of categories\nfor the combined objects, we match the object ti-\ntle of each 3D object and approximate it to the\nnearest WordNet Synset (Miller, 1995). Object cat-\negories are then filtered for redundancy (repeated\ncategories), ambiguity (uncommon, abstract cate-\ngories), and irrelevance (non-manipulable objects),\nresulting in a curated collection of 700 common\nhousehold manipulable objects. Using a subset of\nthe objects (5%), iterations of pilot studies were\nrun to fine-tune the format of human-facing ques-\ntions, and identify the most comprehensive yet non-\nredundant set of physics attributes. Ultimately, we\nidentified categories of malleability, elasticity, stiff-\nness, softness, sharpness, surface smoothness, sur-\nface hardness, and brittleness.\n2.2 Crowdsourcing Framework\nIn order to acquire accurate ground truth annota-\ntions for object-attribute pairs, we have devised a\nlikert-scale annotation setup where annotators are\ntasked with selecting the most appropriate option\nfrom a 3-point scale to depict the given pair. The\nutilization of a 3-point likert scale serves two pri-\nmary purposes: simplicity and representation of\nextremes. The objective of the interface is to col-\nlect responses that facilitate the categorization of\ndata into distinctly opposing groups based on at-\ntribute conditions. Thus, the 3-point scale offers a\nstraightforward decision-making process for users,\nminimizing complexity, and allowing for the selec-\ntion of extreme options when there is a high level\nof certainty, while uncertain responses prompt the\nselection of the middle option. To gather these\nannotations, we have employed Label Studio En-\nterprise 1, to create tasks where a minimum of four\nannotators contribute overlapping annotations for\neach object-attribute pair.\n2.3 Annotation Process\nWe provide an example of the annotation interface\nin Figure 2. Each annotation task consists of three\nmain components: a question which specifies the\nobject category and attribute, a set of 10 randomly-\nsampled images illustrating common objects within\nthe object category, and three answer choices. The\nquestion is designed to incorporate some descrip-\n1https://humansignal.com/platform/\n9744\nFigure 2. NEWTON Pipeline. In addition to facilitating the curation of the NEWTON repository and benchmark, the\nNEWTON pipeline enables convenient extensibility of evaluation measures to suit any scenario. The pipeline consists of four\nmain components, including annotation, filtering, template generation, and LLM evaluation. The annotation component starts\nwith retrieving object categories from 3D object datasets, the categories of which will be filtered for irrelevancy, redundancy,\nand ambiguity, and matched with the WordNet Synset to remove overlapping categories. We then obtain the object-attribute\ntemplates after combination with the physical attributes, and conduct the crowdsourcing process. Each object-attribute sample\nhas a minimum of four overlapping annotations, the agreement between which is used to filter the annotations and form the\nNEWTON repository of object-attribute pairs. The template generation step begins with a generic template, which will be filled\nthrough condition specification and object sampling. With the generated questions, we form a benchmark of 160K questions.\nThe pipeline also enables formulation of infinite personalized evaluation prompts to suit any intended scenario.\ntion of the attribute of focus, and prompts the an-\nnotator to select an option which best describes the\nobject, given a particular attribute topic. The an-\nnotators are asked to focus on the textual category,\nhowever, a visualization of 10 randomly-sampled\nthumbnails are included for reference. We provide\nthe suite of prompts used for different attributes and\npotential options in Table 1. For each task, a mini-\nmum of four annotations are collected, resulting in\na total of 20 000+ annotations.\n3 NEWTON Pipeline\nLeveraging the object-attribute repository, we in-\ntroduce a pipeline which enables systematic gen-\neration of diverse questions based on pre-defined\npersonalized templates, as shown in Figure 2. The\nprocess starts with crowd-sourcing human anno-\ntations on object attributes, which is then filtered\nbased on inter-annotator agreement. The filtered\ndata is then used to populate template-based ques-\ntions in a systematic manner, a process which is\nextensible to a diverse range of personalized sce-\nnarios, beyond those established in the paper.\n3.1 Annotation\nDetailed analysis of the annotation interface and\nworkflow is described in Section 2.2 and 2.3. A\nminimum of four annotators contributed overlap-\nping annotations for each pair. Over 20 000 anno-\ntations were collected, featuring 700+ objects and\n8 unique physical reasoning attributes.\n3.2 Filtering\nThe selection of confident object-attribute pairs is\ncontingent upon the level of agreement between\nannotators. To ensure reliability, a minimum of\nfour annotations that overlap are mandated for each\nobject-attribute pair, enabling the calculation of\ninter-annotator agreement. Inter-annotator agree-\nment is calculated as a percentage of annotators\nwho agreed on the majority response of each object-\nattribute task. A stringent filtering threshold of 0.75\nis applied, necessitating at least three out of the four\nannotations to exhibit agreement. Additionally, an-\nnotations with extreme likert scores (either 1 or\n3) are preserved, thereby eliminating ambiguous\nresponses and maintaining clarity in the dataset.\n3.3 Template Generation\nLeveraging the NEWTON repository of\nobject-attribute pairs, pre-defined templates\ncan be filled. Each template is defined with\nassociated object slots and attribute condi-\ntions. The condition statements are defined as\n{attribute, polarity, padding}, where attribute\ndefines the physics attribute used to group,\npolarity defines whether to employ the highest\nor lowest extremity, and padding specifies how\nmany objects of the opposite polarity to use.\nEach condition statement is designed to obtain a\ngroup of n objects, consisting of one object of the\nspecified attribute and polarity, and n −1 objects\nof the negative polarity. Using the condition\n9745\nTask Category Template\nTrack 1:FoundationalAttributeUnderstanding\nElasticity If you were to deform <MASK> (by squeezing, hitting, etc.) can it recover to its original form?a) No: Object will not be able to return to its original form.b) Somewhat: Object may recover to its original form but will have signs of previous deformations.c) Yes: Object is capable of recovering near perfectly to the original form.SurfaceSmoothnessWhat is the surface smoothness of <MASK>?a) Low: majority of surface is very rough, or has many bumps.b) Moderate: some parts of surface are smooth, while some parts are not.c) High: majority of surface is smooth.SurfaceHardnessHow resistant to scratches or bruises is the surface of <MASK>?.a) Low: surface can be easily scratched or bruised.b) Moderate: surface is resistant to most unintentional damage, but could be scratched if force is applied.c) High: object does not get scratched or bruised easily.Brittleness How easy is it for <MASK> to break (shatter, crack) when a sudden impact force is applied?a) Low: object can withstand most impact forces (drop, smash, etc.).b) Moderate: object can withstand minor impact forces.c) High: object shatters easily with impact force.Softness What is the softness (fluffyness, squishiness) of <MASK>?a) Low: object is hard.b) Moderate: some parts of the object are soft.c) High: the majority of the object is soft.Sharpness What is sharpness of <MASK>?a) Low: object does not have sharp corners/edges and is not capable of piercing.b) Moderate: object may have sharp corners / edges but is not designed to pierce.c) High: object is designed to pierce.Stiffness What is the amount of continued weight that <MASK> can withstand before deforming or cracking?a) Low: object can support very minimal weight before deforming or cracking.b) Moderate: object can withstand a small amount of weight, but something heavy can break the object.c) High: object can easily withstand more than a 10 kg weight.Malleability Can <MASK> be reshaped to other forms?a) No: object cannot be reshaped.b) Somewhat: object can be slightly reshaped.c) Yes: the object can be reshaped to most arbitrary forms.\nTrack 2:ExplicitApplication\nBoolean True/False: <object> is <attribute>.True/False: <object> is not <attribute>.Boolean True/False: <object 1> is more <attribute> than <object 2>.True/False: <object 1> is less <attribute> than <object 2>.MultipleChoiceWhich object is the most <attribute>? Options: <object1>, <object 2>, <object 3>, <object 4>.Which object is the least <attribute>? Options: <object1>, <object 2>, <object 3>, <object 4>.\nTrack 3:ImplicitScenario-based Analysis\nScenario 1 Attribute: Stiffness (high), Brittleness (low)Context: I am packing a backpack.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> should I put at the bottom?Scenario 2 Attribute: Malleability (high), Elasticity (high)Context: I have an irregularly shaped space in my suitcase, and four objects with the same volume.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> could fit in that space?Scenario 3 Attribute: Surface Hardness (high), Surface Smoothness (low)Context: I need an object to place sandpaper above.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> is the most suitable?Scenario 4 Attribute: Sharpness (high)Context: I am trying to open some plastic packaging.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> can help me open?Scenario 5 Attribute: Softness (high), Elasticity (high)Context: I am wrapping fragile gifts and want to protect them from impact.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> should I choose for cushioning?Scenario 6 Attribute: Surface hardness (high), brittleness (low), stiffness (high)Context: I need to hammer a nail into a solid wooden board.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> should I choose?Scenario 7 Attribute: Sharpness (high)Context: I need to prepare a work table as a play area for kids.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> should I remove?Scenario 8 Attribute: Brittleness (low), Elasticity (high)Context: I have a robot which sorts objects by tossing them to bins.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> should I remove?Scenario 9 Attribute: Softness (high), Elasticity (high)Context: I need an object to provide insulation from a sharp edge on a piece of furniture.Question: Which of <object 1>, <object 2>, <object 3>, <object 4> should I choose?\nTable 1.Question Templates. Styles of questions across the\nthree tracks in the NEWTON benchmark. There are three\ntracks: foundational attribute comprehension, explicit applica-\ntion, implicit scenario-based analysis. Track 1: Foundational\nattribute comprehension consist of identical questions to those\nused in the human-annotation process. Track 2: Explicit appli-\ncation consist of questions where the object attribute is men-\ntioned explicitly in the query, which is formatted in Boolean\nor Multiple Choice style. Track 3: Implicit Scenario-based\nAnalysis consists of implicit questions, where the attribute(s)\nof focus is not explicitly mentioned.\nstatements as a filter, we can obtain groups of\nobject-attribute pairs which satisfy the given\nconditions and fill in the object slots, the process\nof which is shown in Figure 2.\n4 NEWTON Benchmark\nWe introduce the NEWTON benchmark, a tool\nto assess the cognitive ability of language models\nto understand and reason about physical attributes\nof everyday objects. The NEWTON benchmark\ncomprises of three progressively challenging tracks,\nand have a combined 160k questions covering 700+\nobjects and 8 unique attribute categories.\n4.1 Tasks\nNEWTON benchmark has 160k questions dis-\ntributed over three reasoning tracks, namely Foun-\ndational Attribute Comprehension, Explicit Ap-\nplication, and Implicit Scenario-Based Analysis.\nThese tracks are selected to align with facets within\nBloom’s cognitive taxonomy, including compre-\nhension, application, and analysis (Adams, 2015).\nThe underlying task involves multiple choice ques-\ntion answering, where given a query q and up to\nfour possible candidate choices, c1...4 the language\nmodel must select the correct option, ccorrect of\nwhich there is exactly one for any given query.\nFoundational Attribute Comprehension. The\nfirst step to understanding concepts in an object-\ncentric manner is to make the connection between\nobjects and their attributes. This one-dimensional\nreasoning is the core assessment strategy within\nthe Foundational Attribute Comprehension track,\nwhere each question involves understanding a sin-\ngle object-attribute pair. This track serves as a\nmeans to gauge the disparities in the distribution\nof comprehension between humans and language\nmodels regarding object attributes. Questions in\nthis track closely mirror those presented to hu-\nman annotators, as shown in Table 1, with mini-\nmal adjustments made to accommodate the diverse\nprompting formats required for different models.\nExplicit Application. To be able to apply knowl-\nedge of understanding attributes in reasoning-type\ntasks is crucial for language-model integration in\ndownstream tasks. Hence, the explicit application\ntask aims to evaluate the language model’s capacity\nto effectively apply their understanding of object at-\ntributes in explicit reasoning tasks. Through a com-\nbination of Boolean and multiple choice questions,\nlanguage models must reason about the correctness\nof statements concerning individual objects or pairs\nof objects, as shown in Table 1.\nImplicit Scenario-based Analysis. This track as-\nsesses the language model’s aptitude for reasoning\nin scenario-based tasks where the attribute to be\ninferred is not explicitly mentioned. Each prompt\npresented within this track has two components:\ncontext and question. The context serves as a de-\nscription of the scenario, and implicitly highlights\nthe attributes(s) of focus. The question presents\ncandidate objects, one of which is the correct an-\nswer. We show example templates in Table 1.\n4.2 Statistics\nIn total, NEWTON benchmark consists of 160K\nquestions distributed over the three tasks of Foun-\ndational Attribute Understanding, Explicit Appli-\n9746\ncation, and Implicit Scenario-Based Analysis. We\nprovide a visualization of the distribution and anal-\nysis of the data in the Appendix. In comparison\nto other datasets and benchmarks which have ex-\namined the topic of Physical Reasoning, NEW-\nTON differs in its object-centric focus, providing\nunmatched diversity and scale as shown in Table 2.\nFactors PIQA PROSTNEWTON (ours)\nObject-Attribute Pairs✗ ✓ ✓\nPhysics Attributes N/A 3 8\nObjects N/A 20 600+\nQuestion Style 2 answers 4 answers 2-4 answers\nQuestions 16k 19k 160k\nMulti-level Evaluation✗ ✗ ✓\nTable 2.Dataset Comparison. Comparison of NEWTON\nwith two other benchmarks aimed at Physics Understand-\ning and Reasoning. PIQA (Bisk et al., 2020) is a dataset\naimed at physics common-sense reasoning, without a focus\non object-centric attribute understanding. PROST (Aroca-\nOuellette et al., 2021) tackles physical and affordance reason-\ning from an object-centric approach, using a small subset of\nobjects. Our dataset examines the understanding of language\nmodels from a physical reasoning perspective, with a rich and\ndiverse set of objects, attributes, and questions.\n4.3 Synthesis of Additional Challenge Sets\nThe NEWTON Repository of Object-Attribute\npairs can be used to synthesize additional chal-\nlenge sets, as seen in Figure 3. The process begins\nwith Context and Attribute Specification, where\nusers identify a context, and relevant attributes.\nNext, object filtering involves using the identified\nattribute(s) to automatically filter objects into a\npositive set and a negative set. Using the grouped\nobjects, one can specify query templates, and au-\ntomatically populate the templates to synthesize a\ndiverse and customized challenge set. Using this,\nlanguage models can be evaluated for accuracy in\nthe specific user-identified context to find the op-\ntimal prompting strategy and model. Examples of\nthe generated dataset are shown in the Appendix.\n5 Results\nIn this section, we evaluate the performance of\nstate-of-the-art models on NEWTON . Specifically,\nwe quantitatively analyze the performance of the\nmodels on the tasks of foundational attribute com-\nprehension, explicit application, and implicit anal-\nysis. We also qualitatively examine the patterns in\nerrors made by the models.\n5.1 Experimental Design\nQuery Templates. We make minimal changes to\nthe prompts of language models between evalua-\nFigure 3. Synthesis of additional challenge sets. Using\nNEWTON ’s broad object-attribute combinations, users can\ndesign custom challenge sets for their needs. The process\nstarts with Context and Attribute Specification. Objects are\nthen automatically filtered based on chosen attributes into\npositive and negative sets. With these, users can quickly create\ntailored challenge templates. This helps evaluate language\nmodels in user-defined contexts, optimizing prompt strategies\nand model selection.\nModel Parameters Foundation Data\nFlan-Alpaca-GPT4-XL 3 B T5 Flan, GPT4-AlpacaDolly-v2 6.9 B Pythia databricks-dolly-15kFlan-T5-Small 80 M T5 Flan CollectionFlan-T5-Base 250 M T5 Flan CollectionFlan-T5-Large 780 M T5 Flan CollectionFlan-T5-XL 3 B T5 Flan CollectionAlpaca 7-30 B LLaMA Alpaca DataBERT-SW AG 110.1 M - 336.2 M BERT SW AG DatasetGPT-Turbo 154 B GPT-3 –GPT-4 – – –\nTable 3. Model Details. We provide the details of models\nevaluated using NEWTON , including the name of the model,\nnumber of parameters, the underlying foundation model, and\nthe instructed or QA datasets used to fine-tune the foundation\nmodel. Note that dashes represent undisclosed details.\ntions of different models. Aside from the particular\nformatting requirements necessary for inference by\ndifferent models, we make no changes to the for-\nmat of the prompt. Prompt structures for different\nfamilies of models are illustrated in the Appendix.\nModels. We provide an outline of the various mod-\nels bench-marked with NEWTON in Table 3. We\nconsider several families of large-scale pre-trained\nmodels, fine-tuned on different instructed or ques-\ntion answering datasets.\nMetrics. Two metrics are used in the evaluation\nof LLMs’ performance across the three benchmark\ntracks: Agreement (%) and Accuracy (%). Agree-\nment (%) is used to evaluate Track 1, while accu-\nracy is used to evaluate Track 2 and 3.\nAgreement=|RLM=RHM||RLM| ×|RHM=RH||RH| Accuracy=|RLM=RHM||RLM| (1)\nWhere RLM denotes the response from the lan-\nguage model, RHM denotes the majority human\nresponse, and RH denotes the human response. The\n9747\nagreement metric regards the human agreement per-\ncentage as an upper limit and adjusts the calculated\naccuracy based on this percentage. This adjustment\nresults in a higher emphasis on questions with sub-\nstantial human agreement, while assigning compar-\natively less significance to questions where human\nannotator responses exhibit greater diversity. This\nmetric aids in evaluating the extent to which lan-\nguage model responses align with those of humans.\nOn the other hand, the accuracy metric considers\nthe human-majority response as the definitive label\nand thus has a maximum attainable value of 100%.\nThis metric gauges the proportion of responses that\nalign with the majority-voted human response.\n5.2 NEWTON Benchmark as a Diagnostic\nfor Knowledge on Physical Attributes\nThe three-track setup of NEWTON benchmark\nenables analysis of Language Models’ ability to\ncomprehend, apply, and analyze physics attributes\nof everyday objects. Through a quantitative analy-\nsis, we draw several insights.\nLanguage models have a non-uniform compre-\nhension of physics concepts of objects. The foun-\ndational attribute understanding task requires clas-\nsification of objects based on physical attributes.\nThrough the task, as shown in Table 4, we find\nthat there is an inconsistent performance of dif-\nferent models across the physical attributes en-\ncompassed by NEWTON . While GPT-4 (OpenAI,\n2023) dominates in terms of overall performance,\ndifferent models excel at different attributes. For\ninstance, Flan-T5-Base (Chung et al., 2022) had\nstrong performance across elasticity, stiffness, sur-\nface smoothness, and malleability, while Flan-T5-\nXL (Chung et al., 2022) excelled at softness. Qual-\nitatively analyzing the common responses of lan-\nguage models, we find that typical errors occur\ndue to one of three reasons: hallucination, con-\nservatism, and misunderstanding. We illustrate\nexamples of model outputs in the Appendix.\nLanguage models are capable of applying their\nknowledge of objects to comparison-based tasks.\nBy utilizing the templates highlighted in Table\n1, we observe that language models exhibit pro-\nficiency in boolean and multiple-choice reasoning\ntasks for physical reasoning of object attributes,\nsimilar to the observations highlighted in Kada-\nvath et al. (2022). Notably, GPT-4 (OpenAI, 2023)\nconsistently performs well across both boolean\nand multiple-choice style questions, showcasing\nits dominance in attributes such as surface hard-\nness, softness, malleability, and sharpness. Alpaca-\nLoRA-7B (Hu et al., 2021) also excels in boolean\nquestions, particularly in the areas of stiffness,\nsurface smoothness, and brittleness. Addition-\nally, other models like Flan-Alpaca-GPT4-XL ,\nFlan-T5-XL (Chung et al., 2022), and GPT-3.5-\nTurbo demonstrate competitive results in various\nattributes.\nSome language models are capable of decision-\nmaking in implicit scenario-based tasks.In Table\n6, we present a quantitative evaluation of language\nmodels across nine scenario templates. The results\nshow that GPT-4 (OpenAI, 2023) consistently out-\nperforms other models by a significant margin in\nthe majority of scenarios. It achieved an impres-\nsive overall average of 87.7% across the defined\nscenarios, compared to the average of 44.5% for all\nthe evaluated models. However, it is important to\nnote that GPT-4 struggles to provide consistently\naccurate responses in certain scenarios, particularly\nin Scenario 8. This indicates the need for evalu-\nation schemes that carefully consider the specific\ndeployment situations in which language models\nwill be utilized.\n5.3 Ablative Studies\nIn this section, we provide an analysis of NEW-\nTON, focusing on potential ways of leveraging\nNEWTON to enhance model performance in a\nphysical reasoning context, and examining the con-\nsistency of LLMs with regard to model size, ques-\ntion polarity, and answer positioning.\nFine-tuning using NEWTON . Aside from using\nNEWTON to create evaluation tools, it serves as a\nresource for fine-tuning pre-trained LMs, improv-\ning their grasp of physical concepts. We experi-\nment with fine-tuning on Track 2 questions that\nexplicitly address object attributes, then evaluate\non Track 3 involving implicit attribute reasoning.\nTrack 2 and Track 3 questions, detailed in Table\n1, are distinct. Fine-tuning focuses on multiple-\nchoice tasks using the base BERT model (Devlin\net al., 2019), initially trained on SWAG (Zellers\net al., 2018). We fine-tune with subsets of NEW-\nTON’s Track 2—5k, 10k, and 40k samples. Mod-\nels are assessed on Track 3’s implicit reasoning\nusing NEWTON . Figure 4A reveals significant en-\nhancement in language models (e.g., BERT) when\nNEWTON is part of pre-training, with increas-\ning performance as fine-tuning samples rise. This\n9748\nLanguage Model Agreement (%)Elasticity Stiffness Surface Smoothness Surface Hardness Softness Brittleness Malleability Sharpness Overall\nDolly-V2-7B 23.7 11.7 4.1 3.5 1.3 1.3 8.6 2.5 7.6Flan-Alpaca-GPT4-XL9.3 1.1 59.7 19.9 25.1 6.1 9.5 7.4 15.2Flan-T5-small 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0Flan-T5-Base 79.8 45.9 61.9 28.1 25.4 6.1 77.0 4.7 41.7Flan-T5-Large 0.0 0.4 61.9 30.1 25.4 6.1 9.8 4.1 13.7Flan-T5-XL 9.3 4.4 15.6 42.4 59.9 6.1 9.9 54.6 27.5UnifiedQA-V2-T5-Large0.0 0.0 61.9 30.5 10.2 6.1 9.5 23.5 14.3Alpaca-LoRa-7B 9.3 0.0 44.3 0.6 14.4 3.9 9.5 1.9 10.0GPT-Turbo 22.5 0.0 0.7 13.5 6.6 6.2 58.8 1.1 16.4GPT-4 51.3 8.0 14.1 47.1 59.5 76.5 44.1 58.4 49.7\nAverage 20.5 7.2 32.4 21.6 22.8 11.8 23.7 15.8 19.6\nHuman 89.1 78.7 79.2 78.9 89.4 82.6 86.8 90.5 84.4\nTable 4. Track 1: Foundational attribute comprehension results for various language models. We report the agreement\npercentage, computed as a percentage of responses which agree with the majority voted human response, weighted by the\ninter-annotator agreement. We also provide the overall averaged agreement across language models, and also across attributes.\nIn addition, we report the inter-annotator agreement average for the listed attributes for reference.\nunderscores NEWTON ’s potential for improving\nLLMs’ physical reasoning through fine-tuning.\nLanguage Model Size and Impact on Perfor-\nmance. To assess the influence of model size on\nperformance, we focus on the Flan-T5 series and\nanalyze the performance of small, base, large, and\nXL model sizes, as shown in Table 7. We observe\nthat, in general, larger model sizes lead to improved\nperformance for Track 2 and 3. However, for Track\n1, the Flan-T5-Base model demonstrates the best\nperformance. This inconsistency is likely attributed\nto the nature of the questions, as those in Track 1\nare more descriptive in nature, while questions in\nTracks 2 and 3 are more concise. The contrasting\nquestion styles likely account for the varying out-\ncomes observed across different model sizes, as\ncertain models may excel in handling longer, more\ndetailed queries while others excel in providing\nresponses to shorter, more focused questions.\nPolarity and Position Bias in Language Mod-\nels. We explore the impact of question polarity on\nmodel performance using NEWTON benchmark -\nTrack 2. Prompting questions are categorized into\nPositive and Negative polarity, where Positive po-\nlarity questions include phrases like \"is,\" \"is more,\"\nand \"is the most,\" while Negative polarity ques-\ntions involve phrases like \"is not,\" \"is less,\" and \"is\nthe least\" (as outlined in Table 1). Conducting a\nt-test reveals that GPT-4 and Dolly-V2-7B have no\nsignificant difference between the means of the dif-\nferent polarity groups, while for other models, there\nis a statistically significant difference ( p <0.05)\nbetween the means of the different polarities, in-\ndicating the presence of a polarity bias, as shown\nin Figure 4B. Additionally, we investigate the pres-\nence of bias based on answer position by grouping\nquestions according to the position of the ground\ntruth answer, as shown in Figure 4C. Among the\nmodels, UnifiedQA demonstrates the lowest differ-\nence in the accuracy between highest and lowest\nscoring accuracy positions, as quantified by a t-\nstatistic of 15.0 and p < 0.0001. On the other\nhand, Flan-Alpaca-GPT4-XL exhibits the largest\ndifference between the highest and lowest position\naccuracy, quantified by a t-statistic of 170.1 and\np <0.0001. The observation that models exhibit\ninconsistencies in accuracy by position is also high-\nlighted in (Aroca-Ouellette et al., 2021).\nEffect of Prompt Engineering. We analyze the in-\nfluence of prompt engineering on both GPT-Turbo\nand GPT-4. Table 8 illustrates our exploration of\nfive distinct approaches to formulating prompts.\nThese strategies are denoted as V1, V2, and V3,\nrepresenting three different versions of prompts\ndistinguished by their word choices. The Ensem-\nble method amalgamates the outcomes from V1,\nV2, and V3, utilizing a majority voting system to\ndetermine the final answer. Meanwhile, theInstruc-\ntion strategy supplements each prompt with a set of\ninstructions, comprising eight example questions\nand corresponding answers that cover diverse at-\ntributes. Notably, these examples do not overlap\nwith any of the tested questions. It’s noteworthy\nthat the Ensemble and Instruction-based prompt\ntechniques enhance performance for Boolean-style\nquestions. However, the performance remains com-\nparable or, in some cases, even less favorable for\nMultiple Choice-style questions. These findings\nunderscore the significance of identifying an ap-\npropriate prompt and crafting variations that are\nattuned to diverse question styles.\n9749\nLanguage Model Elasticity Stiffness Surface Smoothness Surface Hardness Softness Brittleness Malleability Sharpness OverallBoolean (Accuracy %)\nDolly-V2-7B 3.1 2.6 3.1 4.4 2.8 2.5 3.1 2.4 3.0Flan-Alpaca-GPT4-XL54.8 61.5 49.8 59.5 58.1 49.2 48.8 68.1 56.2Flan-T5-Small 53.5 50.3 48.5 49.3 51.4 48.3 52.4 55.4 51.2Flan-T5-Base 36.8 35.3 34.8 34.6 30.8 40.8 37.1 26.3 34.5Flan-T5-Large 42.3 40.1 43.8 20.6 50.2 29.8 38.8 52.1 40.1Flan-T5-XL 61.4 60.4 50.5 65.3 74.0 51.4 59.2 78.8 62.9UnifiedQA-V2-T5-Large46.6 51.7 49.8 57.4 61.1 50.6 49.7 54.5 52.7Alpaca-LoRa-7B 61.0 68.2 67.9 63.9 62.3 69.5 62.7 66.8 65.2GPT-Turbo 49.5 58.5 51.6 54.4 67.5 51.7 61.5 81.5 59.8GPT-4 66.6 61.7 60.8 66.8 78.0 66.8 71.5 82.3 69.6\nAverage 47.6 49.0 46.1 47.6 53.6 46.1 48.5 56.8 49.5\nMultiple Choice (Accuracy %)\nDolly-V2-7B 14.4 9.8 11.7 13.2 12.6 11.7 11.4 10.6 11.9Flan-Alpaca-GPT4-XL 76.6 79.6 37.2 80.5 78.3 22.6 60.4 91.8 65.9Flan-T5-Small 23.6 24.8 25.1 23.9 23.7 24.0 23.9 24.4 24.2Flan-T5-Base 39.9 36.9 23.2 42.0 40.4 14.8 32.1 57.1 35.8Flan-T5-Large 61.0 56.9 23.0 72.4 64.6 32.8 38.3 90.4 54.9Flan-T5-XL 70.5 77.9 42.5 82.5 77.6 30.0 54.7 92.9 66.1UnifiedQA-V2-T5-Large61.7 59.8 28.8 71.3 63.7 27.4 50.3 75.1 54.8Alpaca-LoRa-7B 26.2 27.8 24.1 26.6 26.3 24.3 24.6 31.2 26.4GPT-Turbo 49.0 69.1 42.8 81.1 73.6 44.7 43.3 92.7 62.0GPT-4 75.4 73.8 64.6 84.8 91.7 67.8 72.8 98.5 78.7\nAverage 49.8 51.6 32.3 57.8 55.2 30.0 41.2 66.5 48.1\nCombined (Accuracy %)\nDolly-V2-7B 8.3 6.1 7.3 8.8 7.3 6.9 6.9 6.2 7.2Flan-Alpaca-GPT4-XL64.9 70.4 43.7 69.9 67.4 36.5 54.2 79.1 60.8Flan-T5-Small 39.6 37.8 37.1 36.7 38.6 36.7 39.2 41.1 38.4Flan-T5-Base 38.2 36.1 29.1 38.3 35.3 28.4 34.8 40.6 35.1Flan-T5-Large 51.0 48.4 33.6 46.4 56.8 31.2 38.6 69.8 47.1Flan-T5-XL 65.6 69.0 46.6 73.9 75.7 41.2 57.1 85.3 64.4UnifiedQA-V2-T5-Large53.6 55.6 39.6 64.3 62.3 39.5 50.0 64.1 53.7Alpaca-LoRa-7B 44.9 48.4 46.5 45.4 45.7 48.0 45.0 50.3 46.8GPT-Turbo 49.3 63.7 47.3 67.7 70.3 48.4 53.0 86.7 60.9GPT-4 70.7 67.6 62.7 75.7 84.3 67.2 72.1 89.8 73.9\nAverage 48.6 50.3 39.4 52.7 54.4 38.4 45.1 61.3 48.8\nTable 5.Track 2: Explicit application evaluation results on various LLMs. We separate the questions into two streams, Boolean,\nwhich consists of True/False style questions, and Multiple Choice, which consist of QA style questions with four answer choices.\nWe report the model accuracy across each stream, as well as the combined accuracy. For each stream, we report an averaged\naccuracy percentage across all models for each physical reasoning attribute. We also report an averaged accuracy percentage\nacross all attributes, to gauge the overall understanding of language models across all physical reasoning attributes.\nScenariosLanguage Model1 2 3 4 5 6 7 8 9 OverallDolly-V2-7B 9.8 7.0 13.2 8.1 14.3 7.9 7.0 9.1 5.7 8.8Flan-Alpaca-GPT4-XL39.2 81.8 71.5 72.0 27.1 65.4 22.0 61.1 23.9 48.9Flan-T5-Small27.9 33.2 18.6 33.8 30.2 31.1 26.8 28.8 30.1 29.8Flan-T5-Base56.9 47.2 30.0 28.1 33.8 36.7 18.9 31.5 20.2 34.2Flan-T5-Large65.7 57.5 76.9 42.2 16.9 79.1 20.6 44.1 29.4 46.2Flan-T5-XL 57.6 78.7 96.9 71.5 30.1 74.4 17.472.930.1 54.2UnifiedQA-V2-T5-Large55.6 70.9 83.4 66.0 33.8 63.5 39.5 68.5 45.1 55.4Alpaca-LoRa-7B24.4 24.6 26.2 24.4 24.1 26.4 24.3 28.6 24.5 24.8GPT-Turbo 83.1 66.9 77.8 60.9 38.4 63.4 22.1 64.1 37.4 54.7GPT-4 95.7 92.5 100.0 77.9 75.2 96.8 91.850.884.2 87.7Average 51.6 56.0 59.4 48.5 32.4 54.5 29.0 46.0 33.1 44.5\nTable 6.Implicit scenario-based analysis. We present accu-\nracies of language models on the nine scenario-based tasks\nfrom the NEWTON benchmark. Scenario 1 to 9 represents a\nrange of scenario based questions, ranging from arrangement,\nto tool-use, to safety. We provide an average across each sce-\nnario as a measurement for the overall scenario complexity, as\nwell as an average encompassing the overall performance of\nthe model across the nine given scenarios.\n6 Related Works\nEvaluation of Reasoning Abilities. In the past\nyear, significant progress has been made in explor-\ning Natural Language Processing (NLP) develop-\nments attributable to the advent of large language\nmodels. Prior studies like Bakhtin et al. (2019)\nfocused on basic physical reasoning without lan-\nModel Parameters Track 1 Track 2 Track 3\nFlan-T5-Small 80 M 0.0 38.4 29.8Flan-T5-Base 250 M 41.7 35.1 34.2Flan-T5-Large 780 M 13.7 47.1 46.2Flan-T5-XL 3 B 27.5 64.4 54.2\nTable 7.Model Size’ Impact on Performance. We provide a\ncomparison of different sizes of the Flan-T5 models evaluated\nusing the different tracks of the NEWTON benchmark.\nguage, works like Hong et al. (2021); Duan et al.\n(2021) have investigated physical reasoning in vi-\nsual contexts, and datasets like (Clark et al., 2018;\nKembhavi et al., 2017; Zellers et al., 2018) mainly\nassessed physical reasoning through physics ques-\ntions for benchmarking pretrained models. Some\nstudies have assessed model proficiency in the con-\ntext of physical reasoning: PIQA (Bisk et al., 2020)\ntests models on physical commonsense, while\nPROST (Aroca-Ouellette et al., 2021) offers ques-\ntions on physical reasoning concepts. In contrast,\nour work (NEWTON ) introduces a framework for\nevaluating and improving large language models’\n9750\nFigure 4.Ablations. From left to right: A) BERT fine-tuning results using NEWTON , note the increase in accuracy on the\nunseen implicit questions after finetuning on NEWTON , using of sample of 5000, 10000, and 40000, respectively. B) Accuracy\nby question polarity, where positive polarity represents questions phrased with is, is more, and is the most, while negative polarity\nrepresents questions phrased with is not, is less, and is the least. C) Accuracy by position, where the position value indicates the\nplacement of the correct answer within the sequence of possible options in the question template.\nLLM + Prompting MethodE St SS SH So B M Sh OverallBoolean (Accuracy %)\nGPT-Turbo + V151.5 60.5 53.0 56.0 71.0 53.5 57.0 80.5 60.4GPT-Turbo + V256.5 71.5 64.0 52.0 49.5 68.5 56.5 61.0 59.9GPT-Turbo + V354.0 53.5 59.0 60.5 59.0 61.0 56.0 60.0 57.9GPT-Turbo + Ensemble55.0 68.5 64.5 60.5 58.0 62.5 57.0 69.0 61.9GPT-Turbo + Instruction62.0 68.0 56.5 68.0 57.5 55.0 66.0 80.5 64.2\nGPT-4 + V1 68.5 67.0 66.5 65.5 82.5 63.5 73.0 86.0 71.6GPT-4 + V2 66.0 80.0 58.0 72.0 71.5 83.5 71.5 69.5 71.5GPT-4 + V3 75.5 71.5 61.0 88.5 73.0 88.0 72.0 83.5 76.6GPT-4 + Ensemble68.5 76.5 61.5 79.0 77.0 85.5 75.0 83.0 75.8GPT-4 + Instruction77.5 85.5 75.5 90.5 80.0 68.5 72.0 96.5 80.8\nMultiple Choice (Accuracy %)\nGPT-Turbo + V146.5 72.5 48.5 82.5 78.0 41.5 41.0 92.0 62.8GPT-Turbo + V225.5 56.0 36.5 52.5 55.0 44.5 38.5 61.5 46.2GPT-Turbo + V354.5 55.0 31.5 69.5 53.5 57.0 36.0 66.5 52.9GPT-Turbo + Ensemble36.5 54.5 33.0 67.0 56.5 45.0 34.0 65.0 48.9GPT-Turbo + Instruction30.5 53.5 34.0 63.5 49.5 32.0 25.0 69.5 44.7\nGPT-4 + V1 74.0 76.5 70.0 87.5 91.5 65.0 71.0 98.0 79.2GPT-4 + V2 49.5 66.5 37.5 83.5 70.0 73.5 69.0 82.0 66.4GPT-4 + V3 75.5 68.0 40.5 84.0 75.0 78.5 70.5 76.0 71.0GPT-4 + Ensemble65.5 68.5 47.0 84.5 81.5 77.0 71.5 87.5 72.9GPT-4 + Instruction62.5 57.5 54.0 71.5 65.0 50.5 31.0 74.0 58.2\nCombined (Accuracy %)\nGPT-Turbo + V149.0 66.5 50.7 69.2 74.5 47.5 49.0 86.2 61.6GPT-Turbo + V241.0 63.7 50.2 52.2 52.2 56.5 47.5 61.3 53.1GPT-Turbo + V354.2 54.2 45.2 65.0 56.2 59.0 46.0 63.2 55.4GPT-Turbo + Ensemble45.8 61.5 48.8 63.7 57.2 53.8 45.5 67.0 55.4GPT-Turbo + Instruction46.2 60.8 45.2 65.8 53.5 43.5 45.5 75.0 54.4\nGPT-4 + V1 71.2 71.8 68.2 76.5 87.0 64.2 72.0 92.0 75.4GPT-4 + V2 57.8 73.2 47.8 77.8 70.8 78.5 70.2 75.8 69.0GPT-4 + V3 75.5 69.8 50.7 86.2 74.0 83.2 71.2 79.8 73.8GPT-4 + Ensemble67.0 72.5 54.2 81.8 79.2 81.2 73.2 85.2 74.3GPT-4 + Instruction70.0 71.5 64.8 81.0 72.5 59.5 51.5 85.2 69.5\nTable 8. Prompt Engineering. We tested five prompting\nstrategies on GPT-Turbo and GPT-4: three variations (V1,\nV2, V3) by word choice, an Ensemble majority vote from\nV1-V3, and an Instruction method with eight sample Q&As.\nQuestions were split into Boolean (True/False) and Multiple\nChoice types. We present accuracy for each stream and overall,\nas well as average accuracy per physical reasoning attribute.\nE, St, SS, SH, So, B, M, Sh, represents Elasticity, Stiffness,\nSurface Smoothness, Surface Hardness, Softness, Brittleness,\nMalleability, and Sharpness, respectively.\nperformance in physical reasoning. With 2800\nobject-attribute pairs and 160K QA prompts, it pro-\nvides unmatched scale and extensibility, potentially\nsetting a new standard for evaluation.\nApplications of Language Models in Physically\nGrounded Contexts. With the rising popularity\nof pre-trained LLMs, it’s natural to consider their\nrole in physically grounded scenarios like robotics.\nBefore the LLM surge, efforts were made to link\nvision and language (Das et al., 2017; Gordon et al.,\n2018; Shridhar et al., 2020; Duan et al., 2020).\nMost of these were in embodied contexts, where\nagents answered questions by exploring environ-\nments (Das et al., 2017; Gordon et al., 2018), per-\nformed tasks with sub-steps (Shridhar et al., 2020),\nor rearranged scenes given instructions (Szot et al.,\n2022). Unlike these tasks which focus on visual\nand semantic attributes, NEWTON challenges lan-\nguage models with questions about explicit/implicit\nphysical object properties, some beyond visual in-\nference. More recently, LLMs are being explored\nfor grounding robotic manipulation, often gener-\nating code or instructions for actions/skills (Liang\net al., 2022; Ahn et al., 2022). Yet, there’s a gap\nin using LLMs for object-centric physical reason-\ning in manipulation, which stems from limited ev-\nidence of LLMs’ grounding abilities. Our work\nfocuses on this by exploring and evaluating such\nabilities in physically grounded settings.\n7 Conclusion\nIn this work, we present NEWTON , a repository,\npipeline, and benchmark to support the assess-\nment and refinement of LLMs in the context of\nphysical attribute understanding of everyday ob-\njects. The NEWTON repository offers a large\ncollection of object-attribute pairs, enabling the\ngeneration of infinite test scenarios for evaluating\nlanguage models, using the NEWTON pipeline.\nTo demonstrate its usefulness and potential, we in-\ntroduce the NEWTON benchmark, which consists\nof 160K questions involving 700+ objects and 8 at-\ntributes. These questions cover three distinct tracks:\nfundamental object understanding, explicit applica-\ntion, and implicit analysis. NEWTON is built to\nenable comprehensive and automatic evaluation of\nlanguage models across an array of scenarios, and\nsupport the reliable incorporation of LLMs into\nphysically grounded contexts and applications.\n9751\nLimitations\nWhile this paper sheds light on language models’\nphysics reasoning abilities, it’s crucial to note its\nlimitations. Firstly, data collection through crowd-\nsourcing introduces potential human errors, despite\nextensive filtering efforts by the authors. Secondly,\nthe dataset’s categories don’t encompass the full\nrange of real-world objects, limiting evaluation\nscenarios to those described. Thirdly, the paper\ndoesn’t explore different language models’ suitabil-\nity for various prompting strategies. Nonetheless,\nNEWTON could potentially be adapted for such\nevaluations in the realm of physical reasoning. De-\nspite these limitations, we hope NEWTON encour-\nages further research in benchmarking physical\nreasoning and enriching language models’ under-\nstanding in a physically grounded context.\nAcknowledgements\nThe authors would like to thank members of\nthe Personal Robotics Lab (PRL), Robotics and\nState Estimation Lab (RSELab), and UW NLP\nGroup for fruitful discussions and insightful feed-\nback on the manuscript. Yi Ru Wang is sup-\nported by the Natural Sciences and Engineer-\ning Research Council of Canada (NSERC). This\nwork was (partially) funded by the National\nScience Foundation NRI (#2132848) and CHS\n(#2007011), DARPA RACER (#HR0011-21-C-\n0171), the Office of Naval Research (#N00014-17-\n1-2617-P00004, #2022-016-01 UW, and #1140209-\n405780), and Amazon.\nReferences\nNancy E Adams. 2015. Bloom’s taxonomy of cognitive\nlearning objectives. Journal of the Medical Library\nAssociation: JMLA, 103(3):152.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex\nHerzog, et al. 2022. Do as i can, not as i say: Ground-\ning language in robotic affordances. arXiv preprint\narXiv:2204.01691.\nStéphane Aroca-Ouellette, Cory Paik, Alessandro Ron-\ncone, and Katharina Kann. 2021. Prost: Physical\nreasoning of objects through space and time.\nAnton Bakhtin, Laurens van der Maaten, Justin Johnson,\nLaura Gustafson, and Ross Girshick. 2019. Phyre: A\nnew benchmark for physical reasoning. Advances in\nNeural Information Processing Systems, 32.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nBerk Calli, Arjun Singh, Aaron Walsman, Siddhartha\nSrinivasa, Pieter Abbeel, and Aaron M Dollar. 2015.\nThe ycb object and model set: Towards common\nbenchmarks for manipulation research. In 2015 in-\nternational conference on advanced robotics (ICAR),\npages 510–517. IEEE.\nSusan Carey. 2000. The origin of concepts. Journal of\nCognition and Development, 1(1):37–41.\nMichael Chen, Mike D’Arcy, Alisa Liu, Jared Fernan-\ndez, and Doug Downey. 2019. Codah: An adversar-\nially authored question-answer dataset for common\nsense. arXiv preprint arXiv:1904.04365.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language mod-\nels.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nJasmine Collins, Shubham Goel, Kenan Deng, Achlesh-\nwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang,\nTomas F Yago Vicente, Thomas Dideriksen, Himan-\nshu Arora, et al. 2022. Abo: Dataset and benchmarks\nfor real-world 3d object understanding. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 21126–21136.\nAbhishek Das, Samyak Datta, Georgia Gkioxari, Stefan\nLee, Devi Parikh, and Dhruv Batra. 2017. Embodied\nquestion answering.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca\nWeihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and\nAli Farhadi. 2023. Objaverse: A universe of anno-\ntated 3d objects. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 13142–13153.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\n9752\nLaura Downs, Anthony Francis, Nate Koenig, Brandon\nKinman, Ryan Hickman, Krista Reymann, Thomas B\nMcHugh, and Vincent Vanhoucke. 2022. Google\nscanned objects: A high-quality dataset of 3d scanned\nhousehold items. In 2022 International Conference\non Robotics and Automation (ICRA) , pages 2553–\n2560. IEEE.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nJiafei Duan, Samson Yu, Soujanya Poria, Bihan Wen,\nand Cheston Tan. 2021. Pip: Physical interaction\nprediction via mental imagery with span selection.\narXiv preprint arXiv:2109.04683, 1.\nJiafei Duan, Samson Yu, Hui Li Tan, and Cheston Tan.\n2020. Actionet: An interactive end-to-end platform\nfor task-based data collection and augmentation in\n3d environment. In 2020 IEEE International Confer-\nence on Image Processing (ICIP), pages 1566–1570.\nIEEE.\nDaniel Gordon, Aniruddha Kembhavi, Mohammad\nRastegari, Joseph Redmon, Dieter Fox, and Ali\nFarhadi. 2018. Iqa: Visual question answering in\ninteractive environments. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4089–4098.\nYining Hong, Li Yi, Josh Tenenbaum, Antonio Torralba,\nand Chuang Gan. 2021. Ptr: A benchmark for part-\nbased conceptual, relational, and physical reasoning.\nAdvances in Neural Information Processing Systems,\n34:17427–17440.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know.\nAniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Ha-\njishirzi. 2017. Are you smarter than a sixth grader?\ntextbook question answering for multimodal machine\ncomprehension. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern recognition ,\npages 4999–5007.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh\nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ring-\nshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,\nZeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit\nBansal, Christopher Potts, and Adina Williams. 2021.\nDynabench: Rethinking benchmarking in nlp.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\nHausman, Brian Ichter, Pete Florence, and Andy\nZeng. 2022. Code as policies: Language model\nprograms for embodied control. In arXiv preprint\narXiv:2209.07753.\nMichael McCloskey, Allyson Washburn, and Linda\nFelch. 1983. Intuitive physics: the straight-down be-\nlief and its origin. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition, 9(4):636.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM, 38(11):39–41.\nOpenAI. 2023. Gpt-4 technical report.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Behav-\nioral testing of nlp models with checklist.\nAnna Rogers, Matt Gardner, and Isabelle Augenstein.\n2023. Qa dataset explosion: A taxonomy of nlp\nresources for question answering and reading com-\nprehension. ACM Computing Surveys, 55(10):1–45.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. Alfred: A bench-\nmark for interpreting grounded instructions for ev-\neryday tasks. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 10740–10749.\nAndrew Szot, Karmesh Yadav, Alex Clegg, Vincent-\nPierre Berges, Aaron Gokaslan, Angel Chang, Mano-\nlis Savva, Zsolt Kira, and Dhruv Batra. 2022.\nHabitat rearrangement challenge 2022. https://\naihabitat.org/challenge/rearrange_2022.\nJimmy Wu, Rika Antonova, Adam Kan, Marion Lep-\nert, Andy Zeng, Shuran Song, Jeannette Bohg, Szy-\nmon Rusinkiewicz, and Thomas Funkhouser. 2023.\nTidybot: Personalized robot assistance with large\nlanguage models. arXiv preprint arXiv:2305.05658.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. arXiv preprint\narXiv:1808.05326.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models.\n9753\nA Appendix\nA.1 NEWTON Benchmark Statistics\nNEWTON has three main tracks: fundamental\nattribute understanding, explicit application, and\nimplicit scenario-based analysis. The three tracks\ncomprise of a total of 160K questions, covering\n700+ objects and 8 attributes. Each object is anno-\ntated with 1-8 tags of object attributes. We present\nvisualizations of the data-statistics for Track 1, 2,\nand 3 in Figure 5, 6, and 7, respectively. Figure\n5 illustrates the data distribution of questions and\nattributes, total number of tokens per attribute, and\nnumber of attributes remaining after filtering for\neach object category. Figure 6 illustrates the data\ndistribution for the explicit application track, and\nincludes an analysis on the distribution of the top\n100 highest occurring categories, percentage dis-\ntribution of different attribute occurrences within\nthe track, counts of question polarity and type, and\ntotal tokens for the questions in each attribute. Fig-\nure 7 illustrates the data distribution for the implicit\nscenario-based analysis track, and includes a per-\ncentage fraction of different scenarios, and the total\nnumber of tokens for questions in each scenario.\nTaking a closer look, we can see the initial set of\nattributes have a bias towards the elasticity, sharp-\nness, softness, and malleability attributes, due to\nthe process of filtering inconsistencies. To ensure\na balanced dataset, template formation of Track 2\nquestions involved an additional re-sampling step\nto ensure the questions cover the attributes in a uni-\nform way, hence why Track 2 questions are more\nuniform in nature. Track 3 scenarios are manually\ndesigned for each scenario, and hence data balance\nwas not a primary focus.\nA.2 Detailed Results for NEWTON\nBenchmark Track 2\nTrack 2 involves a mixture of question types\n(Boolean, multiple choice), and questions which\ndiffer by polarity (positive, negative). We provide a\ndetailed breakdown of results by polarity for mod-\nels evaluated on the NEWTON Benchmark, Track\n2, in Table 9.\nA.3 Qualitative Results\nWe present an example for model outputs for the\nFoundational Attribute Understanding (Track 1)\nin Figure 8. Note the common mistakes which\nlanguage models make include (1) a tendency to\nchoose the middle option, (2) hallucination, and (3)\ninaccurate understanding. In Figure 9, we examine\nthe common failure modes of language models for\na set of 10 questions from Track 2: Explicit At-\ntribute Understanding. We find that for most mod-\nels, the failure mode occurs due to the tendency to\nhallucinate. For instance, in Figure 9, for question\n6, 7, and 8, Dolly fails to provide an answer which\nreflects the given options.\nA.4 Query Templates for Language Models\nDifferent models require different variations of\nprompt input, and the unsuitable prompt could lead\nto an inaccurate assessment of the capability of\nlanguage models. Hence, we adapt the question\ntemplate for different families of models to adhere\nto the most effective prompting strategy, as shown\nin Figure 10.\nA.5 Synthesizing Additional Challenge Sets\nwith NEWTON\nWe show a snapshot of the generated dataset using\nthe procedure for synthesizing additional challenge\nsets in Figure 11. Note that all objects also have\ncorresponding 3D models and 2D thumbnails, as\nshown in Figure 12 should the user wish to extend\nthe dataset to a vision-language setting.\nA.6 Comparing Agreement and Accuracy\nScores\nIn Table 10, we show the performance of various\nmodels when evaluated using the accuracy mea-\nsure. In comparison to Table 4, which provides the\nquantitative results using the agreement measure,\nthere is an overall increase in the absolute percent-\nages, since the upper-threshold for the maximum\nattainable value has increased from human agree-\nment percentage to 100%. However, we note that\nthe conclusions mentioned in Section 5.2 remain\nthe same.\nA.7 Dataset Examples\nIn this section, we provide a snapshot of the dataset,\nas shown in Figure 12. Each generated question is\ntagged with corresponding choices, ground truth\nresponse, question polarity, and relevant 2D RGB\nthumbnails and 3D object models. While NEW-\nTON is designed for prompting and assessing lan-\nguage models, potential extensions could also lever-\nage the paired RGB and 3D models to assess visu-\nally grounded models.\n9754\nFigure 5.NEWTON Benchmark Track 1 data statistics. We highlight the data distribution of questions and attributes, total\nnumber of tokens per attribute, and number of attributes remaining after filtering for each object category.\nFigure 6.NEWTON Benchmark Track 2 data statistics. We examine the data distribution for the explicit application track,\nand illustrate the percentage distribution of different attribute occurrences within the track, counts of question polarity and type,\nand total tokens for the questions in each attribute.\n9755\nFigure 7.NEWTON Benchmark Track 3 data statistics. Graphs show the percentage fraction of different scenarios, and the\ntotal number of tokens for questions in each scenario.\nLanguage ModelElasticity Stiffness Surface Smoothness Surface Hardness Softness Brittleness Malleability Sharpness OverallBoolean (overall/positive/negative)\nDolly-V2-7B 3.1 / 3.3 / 2.8 2.6 / 2.9 / 2.2 3.1 / 3.6 / 2.6 4.4 / 5.2 / 3.6 2.8 / 2.9 / 2.6 2.5 / 2.5 / 2.4 3.1 / 3.2 / 2.9 2.4 / 2.8 / 2.0 3.0 / 3.3 / 2.6Flan-Alpaca-GPT-4-XL54.8 / 57.7 / 52.0 61.5 / 73.4 / 49.6 49.8 / 51.6 / 48.0 59.5 / 67.8 / 51.1 58.1 / 54.3 / 62.0 49.2 / 43.9 / 54.4 48.8 / 45.9 / 51.6 68.1 / 77.0 / 59.2 56.2 / 58.8 / 53.7Flan-T5-Small53.5 / 53.1 / 53.9 50.3 / 52.2 / 48.4 48.5 / 52.0 / 45.0 49.3 / 51.7 / 46.9 51.4 / 48.4 / 54.4 48.3 / 43.6 / 53.0 52.4 / 48.1 / 56.7 55.4 / 52.3 / 58.6 51.2 / 50.1 / 52.3Flan-T5-Base 36.8 / 43.0 / 30.6 35.3 / 37.8 / 32.7 34.8 / 37.6 / 31.9 34.6 / 30.0 / 39.3 30.8 / 32.2 / 29.4 40.8 / 32.2 / 49.4 37.1 / 34.4 / 39.8 26.3 / 30.2 / 22.4 34.5 / 34.7 / 34.3Flan-T5-Large42.3 / 42.3 / 42.4 40.1 / 40.3 / 40.0 43.8 / 43.9 / 43.7 20.6 / 16.3 / 24.9 50.2 / 53.3 / 47.0 29.8 / 30.2 / 29.3 38.8 / 39.5 / 38.2 52.1 / 57.3 / 46.9 40.1 / 40.8 / 39.3Flan-T5-XL 61.4 / 66.0 / 56.8 60.4 / 54.1 / 66.8 50.5 / 50.9 / 50.1 65.3 / 57.4 / 73.2 74.0 / 78.1 / 69.9 51.4 / 51.7 / 51.2 59.2 / 67.8 / 50.6 78.8 / 79.2 / 78.4 62.9 / 63.6 / 62.1UnifiedQA-V2-T5-Large46.6 / 47.2 / 45.9 51.7 / 52.7 / 50.6 49.8 / 51.8 / 47.9 57.4 / 56.8 / 58.0 61.1 / 62.2 / 60.1 50.6 / 48.7 / 52.5 49.7 / 44.2 / 55.2 54.5 / 61.8 / 47.3 52.7 / 53.2 / 52.2Alpaca-LoRa-7B61.0 / 71.0 / 51.0 68.2 / 66.7 / 69.8 67.9 / 78.1 / 57.7 63.9 / 65.9 / 62.0 62.3 / 69.3 / 55.3 69.5 / 66.0 / 73.1 62.7 / 73.7 / 51.6 66.8 / 84.2 / 49.5 65.2 / 72.0 / 58.5GPT-Turbo 49.5 / 41.3 / 57.7 58.5 / 62.5 / 54.4 51.6 / 53.2 / 50.1 54.4 / 56.5 / 52.2 67.5 / 67.2 / 67.8 51.7 / 45.9 / 57.4 61.5 / 66.7 / 56.3 81.5 / 81.8 / 81.2 59.8 / 59.6 / 60.0GPT-4 66.6 / 63.0 / 70.1 61.7 / 70.1 / 53.4 60.8 / 61.6 / 60.0 66.8 / 74.4 / 59.1 78.0 / 74.8 / 81.1 66.8 / 64.5 / 69.0 71.5 / 70.2 / 72.7 82.3 / 81.4 / 83.2 69.6 / 70.1 / 69.1\nMultiple Choice (overall/positive/negative)\nDolly-V2-7B 14.4 / 13.7 / 15.2 9.8 / 10.4 / 9.1 11.7 / 12.0 / 11.4 13.2 / 10.6 / 15.8 12.6 / 12.7 / 12.4 11.7 / 12.5 / 10.8 11.4 / 9.4 / 13.4 10.6 / 9.6 / 11.6 11.9 / 11.4 / 12.5Flan-Alpaca-GPT4-XL76.6 / 83.2 / 70.0 79.6 / 77.6 / 81.6 37.2 / 27.5 / 46.9 80.5 / 80.2 / 80.7 78.3 / 84.9 / 71.7 22.6 / 30.8 / 14.4 60.4 / 62.8 / 58.0 91.8 / 98.6 / 85.0 65.9 / 68.2 / 63.5Flan-T5-Small23.6 / 27.3 / 20.0 24.8 / 29.5 / 20.0 25.1 / 24.3 / 26.0 23.9 / 26.3 / 21.6 23.7 / 24.8 / 22.6 24.0 / 22.6 / 25.4 23.9 / 24.8 / 23.0 24.4 / 34.6 / 14.2 24.2 / 26.8 / 21.6Flan-T5-Base 39.9 / 66.2 / 13.6 36.9 / 50.6 / 23.3 23.2 / 22.6 / 23.9 42.0 / 48.6 / 35.5 40.4 / 67.4 / 13.4 14.8 / 12.6 / 17.1 32.1 / 39.2 / 25.0 57.1 / 87.2 / 27.0 35.8 / 49.3 / 22.3Flan-T5-Large61.0 / 74.3 / 47.7 56.9 / 49.8 / 64.0 23.0 / 16.2 / 29.7 72.4 / 69.2 / 75.5 64.6 / 69.2 / 60.0 32.8 / 48.8 / 16.7 38.3 / 32.0 / 44.6 90.4 / 94.6 / 86.2 54.9 / 56.8 / 53.1Flan-T5-XL 70.5 / 81.2 / 59.9 77.9 / 76.9 / 79.0 42.5 / 39.2 / 45.8 82.5 / 85.3 / 79.6 77.6 / 82.8 / 72.4 30.0 / 37.2 / 22.8 54.7 / 57.4 / 51.9 92.9 / 98.2 / 87.6 66.1 / 69.8 / 62.4UnifiedQA-V2-T5-Large61.7 / 79.6 / 43.9 59.8 / 55.2 / 64.3 28.8 / 25.9 / 31.6 71.3 / 67.3 / 75.3 63.7 / 70.6 / 56.8 27.4 / 29.6 / 25.2 50.3 / 40.6 / 60.0 75.1 / 85.1 / 65.2 54.8 / 56.7 / 52.8Alpaca-LoRa-7B26.2 / 27.0 / 25.4 27.8 / 28.6 / 27.0 24.1 / 22.8 / 25.4 26.6 / 26.6 / 26.5 26.3 / 26.8 / 25.8 24.3 / 25.7 / 22.9 24.6 / 21.8 / 27.4 31.2 / 36.5 / 25.9 26.4 / 27.0 / 25.8GPT-Turbo 49.0 / 50.4 / 47.6 69.1 / 73.8 / 64.4 42.8 / 33.3 / 52.3 81.1 / 81.2 / 81.1 73.6 / 74.9 / 72.3 44.7 / 52.0 / 37.4 43.3 / 27.7 / 59.0 92.7 / 97.5 / 87.8 62.0 / 61.3 / 62.7GPT-4 75.4 / 67.5 / 83.4 73.8 / 76.3 / 71.3 64.6 / 59.4 / 69.8 84.8 / 88.8 / 80.7 91.7 / 91.8 / 91.6 67.8 / 75.1 / 60.4 72.8 / 68.8 / 76.9 98.5 / 99.0 / 98.0 78.7 / 78.3 / 79.0\nCombined (overall/positive/negative)\nDolly-V2-7B 8.3 / 8.1 / 8.5 6.1 / 6.6 / 5.6 7.3 / 7.7 / 6.9 8.8 / 7.9 / 9.7 7.3 / 7.4 / 7.1 6.9 / 7.3 / 6.4 6.9 / 6.1 / 7.8 6.2 / 5.9 / 6.4 7.2 / 7.1 / 7.3Flan-Alpaca-GPT4-XL64.9 / 69.5 / 60.3 70.4 / 75.4 / 65.3 43.7 / 39.8 / 47.5 69.9 / 74.0 / 65.9 67.4 / 68.4 / 66.5 36.5 / 37.7 / 35.3 54.2 / 53.8 / 54.6 79.1 / 87.0 / 71.1 60.8 / 63.3 / 58.3Flan-T5-Small39.6 / 41.1 / 38.2 37.8 / 41.1 / 34.5 37.1 / 38.5 / 35.7 36.7 / 39.1 / 34.3 38.6 / 37.5 / 39.7 36.7 / 33.6 / 39.9 39.2 / 37.3 / 41.0 41.1 / 44.1 / 38.1 38.4 / 39.0 / 37.7Flan-T5-Base 38.2 / 53.8 / 22.7 36.1 / 44.1 / 28.1 29.1 / 30.2 / 28.0 38.3 / 39.2 / 37.4 35.3 / 48.5 / 22.0 28.4 / 22.9 / 34.0 34.8 / 36.6 / 32.9 40.6 / 56.6 / 24.5 35.1 / 41.6 / 28.6Flan-T5-Large51.0 / 57.1 / 44.8 48.4 / 45.0 / 51.8 33.6 / 30.4 / 36.9 46.4 / 42.6 / 50.1 56.8 / 60.6 / 53.0 31.2 / 39.1 / 23.3 38.6 / 36.0 / 41.1 69.8 / 74.6 / 65.1 47.1 / 48.4 / 45.8Flan-T5-XL 65.6 / 73.0 / 58.2 69.0 / 65.3 / 72.8 46.6 / 45.2 / 48.0 73.9 / 71.3 / 76.4 75.7 / 80.3 / 71.0 41.2 / 44.8 / 37.7 57.1 / 63.0 / 51.2 85.3 / 88.0 / 82.6 64.4 / 66.5 / 62.2UnifiedQA-V2-T5-Large53.6 / 62.2 / 45.0 55.6 / 54.0 / 57.3 39.6 / 39.2 / 39.9 64.3 / 62.0 / 66.6 62.3 / 66.1 / 58.6 39.5 / 39.6 / 39.5 50.0 / 42.5 / 57.5 64.1 / 72.6 / 55.6 53.7 / 54.9 / 52.5Alpaca-LoRa-7B44.9 / 50.6 / 39.1 48.4 / 48.0 / 48.8 46.5 / 51.1 / 42.0 45.4 / 46.4 / 44.4 45.7 / 49.7 / 41.7 48.0 / 46.8 / 49.2 45.0 / 49.6 / 40.4 50.3 / 62.1 / 38.6 46.8 / 50.6 / 42.9GPT-Turbo 49.3 / 45.5 / 53.1 63.7 / 68.0 / 59.3 47.3 / 43.5 / 51.2 67.7 / 68.8 / 66.5 70.3 / 70.7 / 69.9 48.4 / 48.8 / 47.9 53.0 / 48.6 / 57.5 86.7 / 89.1 / 84.2 60.9 / 60.4 / 61.3GPT-4 70.7 / 65.1 / 76.2 67.6 / 73.1 / 62.2 62.7 / 60.5 / 64.8 75.7 / 81.6 / 69.9 84.3 / 82.6 / 86.0 67.2 / 69.6 / 64.9 72.1 / 69.5 / 74.7 89.8 / 89.6 / 90.1 73.9 / 74.0 / 73.8\nTable 9.Track 2: Explicit application evaluation results on various language models. We separate the questions into two\nstreams, Boolean, which consists of True/False style questions, and Multiple Choice, which consist of QA style questions\nwith four answer choices. We report the model accuracy across each stream, as well as the combined accuracy, separated by\nthe polarity of the question. For each stream, we report an averaged accuracy percentage across all models for each physical\nreasoning attribute. We also report an averaged accuracy percentage across all attributes, to gauge the overall understanding of\nlanguage models across all physical reasoning attributes.\nLanguage Model Agreement (%)Elasticity Stiffness Surface Smoothness Surface Hardness Softness Brittleness Malleability Sharpness Overall\nDolly-V2-7B 26.0 15.3 5.5 4.7 1.4 1.4 10.0 2.6 8.6Flan-Alpaca-GPT4-XL11.2 1.5 75.5 25.8 28.4 7.5 12.0 8.2 18.3Flan-T5-small 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0Flan-T5-Base 88.8 58.6 78.6 36.7 28.8 7.5 87.8 5.2 48.5Flan-T5-Large 0.0 0.5 78.6 39.1 28.8 7.5 12.2 4.6 16.6Flan-T5-XL 11.2 5.4 19.1 53.9 66.3 7.5 12.4 59.6 31.3UnifiedQA-V2-T5-Large0.0 0.0 78.6 39.1 11.4 7.5 12.0 26.0 17.2Alpaca-LoRa-7B11.2 0.0 56.4 0.8 16.4 4.6 12.0 2.0 12.1GPT-Turbo 26.0 0.0 0.9 16.4 7.5 7.0 67.6 1.2 18.9GPT-4 57.8 10.3 17.7 58.6 66.3 92.5 50.6 64.8 57.1\nTable 10.Accuracy Performance of Models on Foundational Attribute Comprehension. We report the accuracy percentage,\ncomputed as a percentage of responses which agree with the majority voted human response. The upper threshold is 100%,\nwhich represents perfect overlap with the ground truth (majority voted human response).\n9756\nFigure 8. Example of responses from NEWTON Track\n1. Note that the query is marked in the yellow text box, and\nthe correct answer to choose is marked with a green highlight.\nResponses from different LLMs are shown in blue text-boxes,\nand correct responses are marked with a green check-mark.\nFigure 9. Success vs. Failure Examples. Typical failure\nmodes of various models on a sample ofNEWTON questions.\nModels typically fail due to hallucination and inaccurate un-\nderstanding.\nFigure 10.Prompting Templates. We show examples of the\nqueries used for evaluating language models.\n9757\nFigure 11. Snapshot of generated challenge-set example. In this example, we demonstrate synthesis of a multiple choice\nchallenge set. We see that each synthesized sample has context, question, and ground truth annotations.\nFigure 12.Dataset Sample. We provide a snapshot of the NEWTON Benchmark. Questions are populated automatically with\nmeaningful object types. Each question is labelled with the attribute, object category, ground-truth object, question polarity,\nquestion type, and several candidate object options. Each candidate object also has several corresponding 3D object models,\nshould the user wish to use NEWTON with perception in the loop.\n9758"
}