{
  "title": "CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval",
  "url": "https://openalex.org/W3212304713",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2139486338",
      "name": "Zijian Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136278240",
      "name": "Jingyu Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1549144106",
      "name": "Sheng Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2914473934",
      "name": "Dedan Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095945326",
      "name": "Hao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100450749",
      "name": "Jinwei Yuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3122640483",
    "https://openalex.org/W3204670646",
    "https://openalex.org/W3152798676",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3176799298",
    "https://openalex.org/W3145807616",
    "https://openalex.org/W2425121537",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3211483028",
    "https://openalex.org/W3043840704",
    "https://openalex.org/W3180463990",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W3129719298",
    "https://openalex.org/W2965458216",
    "https://openalex.org/W2885775891",
    "https://openalex.org/W3204588463",
    "https://openalex.org/W3197804339"
  ],
  "abstract": "Modern video-text retrieval frameworks basically consist of three parts: video encoder, text encoder and the similarity head. With the success on both visual and textual representation learning, transformer based encoders and fusion methods have also been adopted in the field of video-text retrieval. In this report, we present CLIP2TV, aiming at exploring where the critical elements lie in transformer based methods. To achieve this, We first revisit some recent works on multi-modal learning, then introduce some techniques into video-text retrieval, finally evaluate them through extensive experiments in different configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset, outperforming the previous SOTA result by 4.1%.",
  "full_text": "CLIP2TV: Align, Match and Distill for\nVideo-Text Retrieval\nZijian Gao1, Jingyu Liu1, Weiqi Sun1,\nSheng Chen1, Dedan Chang1, and Lili Zhao 1\nOVBU, PCG, Tencent\ngzjbupt2016@bupt.edu.cn, sunweiqi@buaa.edu.cn,\n{messijyliu, carlschen, dedanchang, lilillzhao}@tencent.com\nAbstract. Modern video-text retrieval frameworks basically consist of\nthree parts: video encoder, text encoder and the similarity head. With the\nsuccess of both visual and textual representation learning, transformer-\nbased encoders and fusion methods have also been adopted in the field of\nvideo-text retrieval. In this paper, We propose a new CLIP-based frame-\nwork called CLIP2TV, which consists of a video-text alignment module\nand a video-text matching module. The two modules are trained end-to-\nend in a coordinated manner, and boost the performance to each other.\nMoreover, to address the impairment brought by data noise, especially\nfalse negatives introduced by vague description in some datasets, we pro-\npose similarity distillation to alleviate the problem. Extensive experimen-\ntal results on various datasets validate the effectiveness of the proposed\nmethods. Finally, on common datasets of various length of video clips,\nCLIP2TV achieves better or competitive results towards previous SOTA\nmethods.\nKeywords: Video-Text Retrieval, Multi-Modal Learning\n1 Introduction\nMulti-modal retrieval is a fundamental task in both research fields of multi-\nmodal learning and industrial applications. Specifically, visual-text retrieval is\nattracting more attention to meet the needs of increasing tons of uploaded texts,\nimages and videos. This work focuses on the task of video-text retrieval. Recently,\nboth the research community [44,4,41,18,47,1,25,3] and the industry are putting\nmore efforts into it. Given the textual input, the system is required to retrieve\ncorresponding videos, which complements traditional search engines relying on\npure textual labels. In reverse, given the video input, the system could also\noutput textual descriptions, which is useful for videos without captions.\nA modern video-retrieval system typically consists of three parts: text en-\ncoder, video encoder, and similarity head. With the success of transformers [40]\nin natural language processing, visual representation learning [8], and multi-\nmodal learning [6], the three parts have all been benefited from transformer\nmechanisms. For text encoder, from BERT-like models pretrained on pure texts,\narXiv:2111.05610v2  [cs.CV]  21 Jul 2022\n2 Zi. Gao et al.\nvague descriptiontypo misdescriptionThis is a cartoona family is having coversationpeople are cheering players\nFig. 1.Three types of noise exist in descriptions. Vague description, or referred\nto low-quality description, can describe multiple distinct videos. Typo will result in\nthe loss of some key information. Misdescription is a false description of some video\ncontents or an error caption that is completely irrelevant to the video.\nto CLIP-like models pretrained on texts and images, downstream tasks can ex-\nploit them either via fine-tuning or prompt learning. For video encoder, transformer-\nbased methods are also showing great potentials. For instance, ViT [8] and video\nSwin-Transformer [26] are used in action recognition and other related tasks.\nFor similarity head, which is also interpreted as multi-modal fusion, can be im-\nplemented as multi-modal transformers. Heavy models like ViLBERT [27] and\nUNITER [6] fuse texts and images as input tokens, with self and mutual atten-\ntion learning via heavy layers of attention networks. To fully realize the potential\nof transformer-based backbones, we adopt text and image encoders from CLIP\n[34] as text and video encoders in our proposed CLIP2TV.\nBasically, our work follows the line of recent CLIP-based models [28], [10].\nA similarity head is added on two streams of visual and textual encoders. The\noutputs of these two modules are projected and aligned in the common multi-\nmodal space, to formulate the video-text alignment (vta) module. Besides it, we\nare different from [28] in two respects:\n(i) For similarity head, we propose a video-text matching (vtm) module ini-\ntialized from multiple layers of transformers. The vtm module takes as inputs\nonline positive and negative pairs of embeddings from text and video encoders,\nand formulates a matching loss like previous multi-modal transformers [27], [6].\nIn [28], a transformer head is also added on two streams, but shows worse results\ntowards simpler heads. We claim that alignment in the first place is critical to\nmake vtm work. On the other hand, by back propagating the gradients in an\nend-to-end manner, the vtm module can improve the accuracy of vta. This two\nmodules work in a coordinated manner and boost performance to each other.\nSimilar structure in image-text retrieval is [21], which also uses an image-text\nalignment module to input embeddings to the image-text matching module. In-\nstead of [21] using matching score of the similarity head of vtm as the final\nCLIP2TV: Align, Match and Distill for Video-Text Retrieval 3\nretrieval result, we still use nearest neighbors in the common space from vta as\nthe retrieval results. Therefore CLIP2TV is efficient for inference.\n(ii) In the training process, we observe that vtm is sensitive to noisy data\nthus oscillates in terms of validation accuracy. We attribute the cause of it to the\ndata noise. There are mainly three types of noise in video-text retrieval datasets:\nvague description, typo and misdescription, as showed in Fig.1. Among them,\nvague descriptions are the most frequent. For instance, in MSR-VTT [44], some\ndescriptions are too general like “This is a cartoon” for a cartoon video. Also in\nDiDeMo [1], texts like “we see only the sky” make the target video hardly stand\nout. This will impair the contrastive training process where negative samples\nmight be semantically close to the anchor sample. Typo and misdescription also\nexist, which will result in inaccurate description of videos and impair the relation\nbetween the paired text and video. To alleviate this problem, we aim to find\nthe metric that reflects more accurately the similarity between two modalities.\nTherefore we propose similarity distillation, which distills 3 types of similarities\nfrom inter-, intra- or joint space before vtm. Then we use them as additional\nsoft labels to supervise vtm.\nIn summary, we propose CLIP2TV, a new CLIP-based framework to address\nvideo-text retrieval. Our contributions are threefold:\n1. The framework is comprised of two modules: video-text alignment (vta)\nand video-text matching (vtm). They are trained end-to-end in a coordinated\nmanner and benefit each other.\n2. With data exploration and extensive experiments, we find that network\ntraining suffers impairment brought by data noise, especially videos with vague\ndescriptions. Therefore we propose similarity distillation to alleviate the problem.\n3. We conduct comprehensive experiments on multiple datasets, achieving\nsignificant improvements on the strong baseline of CLIP4Clip, and finally get\nbetter or competitive results to previous SOTA methods on common datasets.\nWe believe this work can bring useful insights and practical expertise to both\nresearch community and industry.\n2 Related Work\nVideo-Language Modeling. VideoBERT [39] firstly extends BERT [40] to\nvideo representation learning and adapts it for modeling visual-linguistic re-\nlationship and joint representation learning. The following work of CBT [38]\nlightens VideoBERT [39] by combining video and text representations after sep-\narate modality training. Then HERO [22] proposes a hierarchical structure for\nencoding video and subtitle/speech together. While ActBERT [51] utilizes global\naction information to bridge the gap between video and language and introduces\ntransformer block to encode global actions, local objects, and text descriptions.\nDifferent from canonical approaches, ClipBERT [19] learns video and text rep-\nresentations by an end-to-end manner, and adopts the sparse sampling strategy\nand initializes model with pretrained 2D model for efficiency and lower com-\nputational burden. Further more, to fully exploit the advantage of transformer,\n4 Zi. Gao et al.\nFrozen [3] leverages a dual transformer encoder structure with end-to-end train-\ning. Nowadays, models such as CLIP4Clip [28] and CLIP2Video [10] extended\nfrom a large-scale image-text pretrained model CLIP [34] are continuously pro-\nposed.\nImage/Video-Text Retrieval. Multi-modal retrieval, as one of the most\npopular tasks in multi-modal learning, attracts lots of attention to produce valu-\nable works [27,6,34,21,20,22,45,24,28,10]. ViLBERT [27] and UNITER [6] use a\nshared transformer for image-text joint representation learning and retrieval.\nCLIP [34] uses two independent transformer to encode image and text sepa-\nrately, yet ALBEF [21] adopts pyramid-like structure that images and texts are\nseparately encoded in shallow layers and fused in deeper layers for matching. Re-\ncently, video-text retrieval task earns much more attention. HiT [24] and TACo\n[45] use multi-level feature alignment for better cross-modal learning. Moreover,\nTVR [20] and HERO [22] extend video-text retrieval task to video corpus mo-\nment retrieval task aiming to retrieval a whole video from a large video corpus\nand localize the related moment within the video by a query. Meanwhile, in-\nspired by image-text retrieval, many studies [3,28,10] try to transfer knowledge\nfrom large-scale image-text pretraining model into video-text retrieval or train\nmodels with both images and videos.\nLearning with Noise. Learning with noise is quite popular in recent years.\nMany works focus on supervised learning from noise[35,31,23,13,50,7,15,17,16].\nSince deep neural networks can easily overfit to noisy labels [48], which re-\nsults in poor generalization. Several techniques have been developed to enhance\nthe robustness to label noise, including losses that reduce the impact of out-\nliers [50,42,12], loss correction approaches that model the source of label noise\n[31,35,14,36,2], and regularization procedures tailored to lower the impact of\nnoise [49,32]. We refer readers to [37] for a detailed survey of prior work on\nlearning with label noise. In this work, we consider that existing video retrieval\nbenchmarks are based on datasets collected for other tasks (e.g. video caption-\ning) [43], which often leads to ambiguity of the captions (i.e. can be matched to\nmany videos). [43] proposes multi-query training and inference method to reduce\nthe impact of vague or low quality queries.\n3 Proposed Method\nGiven a set of captions and a set of videos, video-text retrieval aims at seeking a\nmatching function calculating similarities between captions and videos. Recent\nworks like [28] have showed the benefits of image-text retrieval pre-training and\nadvantages of end-to-end training. Inspired by these, we adopt CLIP [34] as our\nmulti-modal encoder and base our work on [28]. Fig.2 depicts the framework\nof CLIP2TV. It is comprised of a video-text alignment (vta) module and a\nvideo-text matching (vtm) module. In the following, we show how vta and vtm\ninteract with each other in detail, and how similarity distillation address the\nproblem caused by noisy samples.\nCLIP2TV: Align, Match and Distill for Video-Text Retrieval 5\nText Encoder (BERT)\namandrivinginacarlookingat…\nToken Embedding + Position EmbeddingCosine SimilarityVideo-Text Align\nMatching HeadMatching ScoreVideo-Text Match\nDistilled Similarity\n[eos]\nPatch Embedding + Patch Position Embedding\n0123456\nLinear Projection of Flattened Parches\n∗⋯\nVideo Encoder (ViT)\n⋯\n∗[cls] token\nTemporal Transformer 05 1234\nFrame Embedding + Frame Position Embedding\ntime\nVideo Representation\nText Representation\nText Representation\nVideo RepresentationVideo Embeddings\nText Embeddings\nJoint-modal Similarity\nGround-truthLabel\ninacarlookingathiswatchthenstopping…\namandriving\nframe0frame1frame2frame3frame4…\n[eos]\nMulti-Modal TransformerHard Negatives\n⋯\ntime\nDistilled Similarity\nVideo-Text AlignmentVideo-Text Matching\nFig. 2. Framework of our method.Vta: Dual transformer encoders encode video\nand text representations, which are then projected and aligned in the common multi-\nmodal space. Vtm: The following multi-modal transformer and matching head take as\ninput concatenated video-text embeddings and output the matching score. Similarity\ndistillation: distilled similarity from intra- or inter-space serve as soft labels for video-\ntext matching.\n3.1 Framework\nVideo Encoder:Given a video, we first split it into equal-length segments, then\nrandomly extract one frame from each segment. For a video clip v = {vi}Nv\ni=1,\nNv is number of frames. A transformer-based Video Encoder, i.e., ViT used\nin CLIP, encodes it into frame embeddings Vemb ∈ RNv×Dv , where Dv is the\ndimension of frame embedding. After that, a temporal transformer with frame\nposition embedding and residual connection is applied to frame embeddings to\nenhance temporal information. Then, a MLP projector projects contextualized\nframe embeddings into a multi-modal common space. For simplicity,we re-use\nVemb ∈ RNv×D as enhanced temporal embeddings, where D is the dimension.\nThe final video embedding is calculated as v = mean-pooling(Vemb) ∈ RD.\nText Encoder:The bert-like transformer used in CLIP is adopted as our text\nencoder. The caption denoted as t = {ti}Nt\ni=1 is fed into encoder to acquire token\nembeddings Temb ∈ RNt×Dt , where Nt and Dt are the number and dimension of\ntoken embeddings. Same with the procedure of encoding videos, a text projector\nprojects token embeddings into a common sub space with dimension D. Finally,\nthe representation of [EOS] token will serve as the embedding of the whole\ncaption t ∈ RD.\nContrastive Learning by Alignment (vta):To align video embeddings v\nand caption embeddings t in the common space, we adopt contrastive loss to\nmaximize similarity between positive pairs and minimize similarity between neg-\native pairs. The cosine similarity is calculated between normalized embeddings\nfrom each modality. Given a batch B with B video-text pairs, cross-entropy loss\n6 Zi. Gao et al.\nadds on softmax-normalized similarity gives the InfoNCE loss:\npv2t\nvta(v) ∼ exp(s(v, t)/τ1)P\nt∈B exp(s(v, t)/τ1), (1)\nLv2t\nvta = Ev∼B\n\u0002\nH(pv2t\nvta(v), yv2t\nvta(v))\n\u0003\n, (2)\nwhere s(·, ·) denotes cosine similarity, τ1 represents a learnable temperature pa-\nrameter, yv2t\nvta(·) is the ground-truth binary label that positive pairs and negative\npairs are 1 and 0 respectively, and H(·, ·) is cross-entropy formulation. Mean-\nwhile, Lt2v is calculated vice versa. Then, the contrastive loss for alignment is\nformulated as:\nLvta = 1\n2(Lv2t\nvta + Lt2v\nvta) (3)\nContrastive Learning by Matching (vtm):To fully exploit the abundant\ninformation between two modalities and enhance cross-modal interactions, we\nutilize a multi-modal fusion and matching strategy to predict whether a video-\ntext pair is positive or negative. Similar with TACo [45] and ALBEF [21], our\nmulti-modal encoderconsists of self-attention layers. It takes as input video\nframe embedding Vemb ∈ RNv×D and caption token embedding Temb ∈ RNt×D\nand outputs fusion embedding Femb ∈ R(Nv+Nt)×D. We adopt embedding of\n[EOS] token as fusion feature of video-text pairs, which is f ∈ RD. Then, a\nmatching head g(·) composed of LN-FC-ReLU-FC calculates the matching score.\nThe training objective of multi-modal fusion is also InfoNCE loss:\npvtm(v, t) ∼ exp(g(f(v,t)))P\n(v′,t′)∈P exp(g(f(v′,t′))), (4)\nLvtm = E(v,t)∈B [H(pvtm(v, t), yvtm(v, t))] , (5)\nwhere P denotes the pair set that consists of a ground truth video-text pair and\nthe negative sample pairs related to them. pvtm(·, ·) represents the probability\nwhether a pair in the pair set is positive. And yvtm(·, ·) is the ground-truth\none-hot label of the pair set, while H(·, ·) is cross-entropy formulation.\nIn order to improve efficiency and reduce computational cost, we only se-\nlect in-batch hard negative samples for multi-modal fusion. Concretely, for each\nvideo, we choose top- K negative text samples during real-time training accord-\ning to eq.(1) and vice versa. Finally, B ×(2K + 1) pairs are fed into multi-modal\nfusion.\n3.2 Similarity Distillation on vtm\nAs mentioned before, in the training process, we observe that vtm suffers severer\noscillation than vta, and ends up with lower accuracy. We attribute this to two\nreasons: 1. Data noise including vague description, typo and misdescription.\nAmong them vague description is the most frequent. Since most datasets are\noriginally designed for video captioning, which are not particularly labeled to\nbe distinct. This problem is also mentioned in [43]. 2. The inherent nature of\nCLIP2TV: Align, Match and Distill for Video-Text Retrieval 7\nmulti-modal transformer in vtm. For instance, anchor textual embedding and\npositive/negative video embedding are fused in an earlier stage, thus all the\nfinal outputs contain the information of the anchor embedding, which results in\nmore difficulty in separation.\nTo seek for a more accurate metric on the similarity between positive-positive\npairs and positive-negative pairs, we refer to the output embeddings from vta.\nThere are three sub-spaces in vta: textual space, video space and the aligned\nmulti-modal space. Each space has its own metric on the similarity of a pair of\ninstances. This raises curiosity to us: can we refer to the similarity in them and\nwhich will be the best. Without loss of generality, we will try three variants of\nstrategies.\n1.Inter-modal: It is natural to adopt inter-modal similarity from vta as the\nsoft label to guide video-text matching as:\nyt2v\ninter = softmax (s(t, v′)/τ2) and yv2t\ninter = softmax (s(v, t′)/τ2) (6)\nwhere yinter denotes inter soft-label. τ2 is temperature to control distillation. v\nand t denote video embedding and text embedding, respectively.\n2.Intra-modal: We utilize video-video similarity in video space on text-\nto-video matching, and text-text similarity in textual space on video-to-text\nmatching. Formally,\nyt2v\nintra = softmax (s(v, v′)/τ2) and yv2t\nintra = softmax (s(t, t′)/τ2) (7)\nwhere yintra denotes intra soft-label.\n3.Joint-modal: Joint soft-label utilize intra-modal similarity distilled knowl-\nedge from video-modal space and text-modal space jointly to supervise vtm mod-\nule. Concretely, we compute average intra-modal similarity of both modalities\nto obtain the joint soft-label:\nyt2v,v2t\njoint = softmax (s(t, t′) + s(v, v′)/2τ2) (8)\nwhere yjoint denotes joint soft-label.\nThen we compute soft InfoNCE loss for multi-modal encoder:\nLsoft−vtm = E(v,t)∈B [H(pvtm(v, t), y∗(v, t))] (9)\ny∗ denotes soft-label in terms of intra, inter or joint. Finally, fuse Lvtm by:\nLfinal−vtm = (1 − β) · Lvtm + β · Lsoft−vtm (10)\nwhere β is hyperparameter balancing vanilla vtm and soft vtm.\n3.3 Training Objective\nCombining video-text alignment and video-text matching, we formulate the\ntraining objective as following:\nL = Lvta + Lfinal−vtm. (11)\n8 Zi. Gao et al.\n4 Experiments\n4.1 Datasets and Evaluation Metric\nWe report our experimental results on five public available video-text retrieval\ndatasets. Following [28], we use recall at rank K (R@K), median rank (MdR)\nand mean rank (MnR) as metrics to evaluate our model.\nMSR-VTT [44] is the most popular benchmark for video-text retrieval task. It\nconsists of 10000 videos with 20 captions for each. We report our results on the\nstandard full split [9] and 1K-A split [46]. The former contains 6513 videos for\ntraining, 497 videos for validating and 2990 videos for testing. And the latter\nuses 9000 videos for training and 1000 video-text pairs for testing.\nMSVD [4] collects 1970 videos with 1200 for training, 100 for validation and\n670 for testing. Each video has a length of during 1 to 62 seconds and is paired\nwith approximately 40 sentences.\nVATEX [41] is a large-scale multilingual video description dataset including\nover 41,250 videos and 825,000 captions. For fair comparison, we follow evalu-\nation protocal used in HGR [5] with 25,991 videos for training, 1500 videos for\nvalidation and another 1500 videos for testing.\nDiDeMo [1] includes 10,611 videos collected from Flicker and length of each\nis a maximum of 30 seconds. Following [25], [19], [3], we treat it as a video-\nparagraph retrieval task that all of the captions belonging to the same video will\nbe concatenated to form a paragraph.\nActivityNet [18] is comprised of 19,994 videos downloaded from YouTube. We\nfollow [47], [11] to concatenate all of the video related descriptions to form a\nparagraph for retrieval and evaluate our model on ’val1’ split.\n4.2 Implementations Details\nThe basic video encoder and text encoder are both initialized by CLIP [34].\nBesides basic encoders, frame position embedding is initialized with position\nembedding used in CLIP’s text encoder. Both temporal transformer and multi-\nmodal transformer are initialized by the part of layers from text encoder in\nCLIP. Concretely, the width, heads, and layers are 512, 8 and 4, respectively.\nThe fixed video length is 12 and caption length is 32 for MSR-VTT [44], MSVD\n[4], VATEX [41]. As for DiDeMo [1] and ActivityNet [18], we set both video\nlength and caption length to 64. β and τ2 for similarity distillation are 0 .5 and\n0.5, respectively. Following [28], we finetune our model with Adam optimizer in\n10 epochs with 256 batch size and 8 for K. The learning rate is 1 e − 7 for the\nbasic video encoder and text encoder, and 1 e − 4 for new layers such as frame\nposition embedding, temporal transformer and multi-modal encoder.\n4.3 Comparison with State-of-the-Art Methods\nWe compare our model with other state-of-the-art methods. Table.1 shows the\ntext-to-video and video-to-text retrieval results of CLIP2TV on MSR-VTT 1kA.\nCLIP2TV: Align, Match and Distill for Video-Text Retrieval 9\nTable 1.Retrieval result on MSR-VTT 1kA split. SD means Similarity Distillation.\nAnd the CLIP2TV-ViT16 is always combined with SD. We copy the results of other\nmethods from corresponding papers. Recall at rank 1(R@1) ↑, rank 5(R@5) ↑, rank\n10(R@10)↑, Median Rank(MdR)↓ and Mean Rank(MnR)↓ are reported.\nText-to-Video Video-to-Text\nType Method R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR\nOthers\nJSFusion [46] 10.2 31.2 43.2 13.0 - - - - - -\nHT-pretrained [29] 14.9 40.2 52.8 9.0 - - - - - -\nCE [25] 20.9 48.8 62.4 6.0 28.2 20.6 50.3 64.0 5.3 25.1\nClipBERT [19] 22.0 46.8 59.9 6.0 - - - - - -\nTACo [45] 26.7 54.5 68.2 4.0 - - - - - -\nMMT-pretrained [11] 26.6 57.1 69.6 4.0 24.0 27.0 57.5 69.7 3.7 21.3\nSUPPORT-SET [30] 27.4 56.3 67.7 3.0 - 26.6 55.1 67.5 3.0 -\nFROZEN [3] 31.0 59.5 70.5 3.0 - - - - - -\nHIT-pretrained [24] 30.7 60.9 73.2 2.6 - 32.1 62.7 74.1 3.0 -\nCLIP-based\nCLIP-straight [33] 31.2 53.7 64.2 4.0 - 27.2 51.7 62.6 5.0 -\nMDMMT [9] 38.9 69.0 79.7 2.0 16.5 - - - - -\nCLIP4Clip-meanP [28] 43.1 70.4 80.8 2.0 16.2 43.1 70.5 81.2 2.0 12.4\nCLIP4Clip-seqTransf [28] 44.5 71.4 81.6 2.0 15.3 42.7 70.9 80.6 2.0 11.6\nCLIP2Video [10] 45.6 72.6 81.7 2.0 14.6 43.5 72.3 82.1 2.0 10.2\n70.8 81.1 2.0 15.4 45.6 72.3 83.2 2.0 11.5\nOurs\nCLIP2TV 45.6 71.1 80.8 2.0 15.0 43.9 70.9 82.2 2.0 12.0\nCLIP2TV+SD 46.1 72.5 82.9 2.0 15.2 43.9 73.0 82.8 2.0 11.1\nCLIP2TV-ViT16 49.3 74.7 83.6 2.0 13.5 46.9 75.0 85.1 2.0 10.0\nAs non-CLIP-based methods are usually trained beyond feature-level, which\nmeans models adopt video frame features extracted by off-the-shelf pretrained\nvisual models as inputs, they cannot exceed the zero-shot performance of CLIP\nwhich is trained end-to-end with much more data. We categorize these ap-\nproaches into CLIP-based and others for a fair comparison. On MSR-VTT 1kA,\nwe achieve state-of-the-art results on both text-to-video and video-to-text re-\ntrieval. Upon strong baseline of CLIP4clip, CLIP2TV improves R1 of text-to-\nvideo to 45.6, and R1 of video-to-text to 43.9. CLIP2TV+SD shows the re-\nsults with similarity distillation. It further improves CLIP2TV on nearly on all\nmetrics. Particularly, when replace ViT-32 with ViT-16, we can acquire higher\nperformance(with 3.2% improvement in text-to-video retrieval at R@1).\nTable.2-Table.6 present the text-to-video retrieval results on MSR-VTT full,\nMSVD, DiDeMo, ActivityNet and VATEX. For DiDeMo and ActivityNet, we re-\nproduce results based on the provided code of CLIP4Clip and denote as CLIP4Clip-\nmeanP*. On MSR-VTT full, MSVD, and VATEX, we obtain comparable per-\nformance to previous SOTA CLIP2Video with ViT32. On MSVD, though the\ntemporal transformer hardly learns temporal representation with very few frames\n[28,10], we still boost the performance close to CLIP2Video. We also note that\non VATEX, similarity distillation boosts the performance with small margin.\nThis is mainly due to the reason that captions in VATEX are relatively detailed\nand distinct, which just validates our claim. Our method outperforms previous\n10 Zi. Gao et al.\nTable 2.MSR-VTT full\nMethod Text-to-Video\nR@1 R@5 R@10 MdR MnR\nCE [25] 10.0 29.0 41.2 16.0 86.2\nHT-pretrained [29] 14.9 40.2 52.8 9.0 -\nCLIP-straight [33] 21.4 41.1 50.4 10.0 -\nMDMMT [9] 23.1 49.8 61.8 6.0 52.8\nCLIP2Video [10] 29.8 55.5 66.2 4.0 45.5\nCLIP2TV 29.6 55.4 66.0 4.0 48.2\nCLIP2TV+SD 29.9 55.6 66.3 4.048.1\nCLIP2TV-ViT16 32.4 58.2 68.6 3.0 43.6\nTable 3.MSVD\nMethod Text-to-Video\nR@1 R@5 R@10 MdR MnR\nCE [25] 19.8 49.0 63.8 6.0 -\nSUPPORT-SET [30] 28.4 60.0 72.9 4.0 -\nFrozen [3] 33.7 64.7 76.3 3.0 -\nCLIP-straight [33] 37.0 64.1 73.8 3.0 -\nCLIP4Clip-meanP [28] 46.2 76.1 84.6 2.0 10.0\nCLIP4Clip-seqTransf [28] 45.2 75.5 84.3 2.0 10.3\nCLIP2Video [10]47.0 76.8 85.92.0 9.6\nCLIP2TV 46.3 76.1 85.3 2.0 10.0\nCLIP2TV+SD 47.076.5 85.1 2.0 10.1\nCLIP2TV-ViT16 50.2 79.8 87.9 1.0 8.6\nTable 4.DiDeMo\nMethod Text-to-Video\nR@1 R@5 R@10 MdR MnR\nCE [25] 16.1 41.1 - 8.3 43.7\nClipBERT [19] 20.4 48.0 60.8 6.0 -\nFrozen [3] 34.6 65.0 74.7 3.0 -\nCLIP4Clip-meanP [28] 43.4 70.2 80.6 2.0 17.5\nCLIP4Clip-seqTransf [28] 43.4 69.9 80.2 2.0 17.5\nCLIP4Clip-meanP* [28] 42.1 69.3 79.6 2.0 18.0\nCLIP2TV 43.970.579.8 2.0 16.6\nCLIP2TV+SD 45.569.780.6 2.017.1\nTable 5.ActivityNet\nMethod Text-to-Video\nR@1 R@5 R@50 MdR MnR\nCE [25] 18.2 47.7 91.4 6.0 23.1\nClipBERT [19] 21.3 49.0 - 6.0 -\nSUPPORT-SET [30] 26.8 58.1 93.5 3.0 -\nMMT-pretrained [11] 28.7 61.4 94.5 3.3 16.0\nHIT-pretrained [24] 29.6 60.7 95.6 3.0 -\nCLIP4Clip-meanP [28] 40.5 72.4 98.1 2.0 7.4\nCLIP4Clip-seqTransf [28] 40.5 72.4 98.2 2.0 7.5\nCLIP4Clip-meanP* [28] 40.0 71.2 98.0 2.0 7.7\nCLIP2TV 40.8 72.9 98.0 2.0 7.3\nCLIP2TV+SD 44.1 75.2 98.4 2.0 6.5\nSOTA methods by a large margin on paragraph-to-video retrieval on DiDeMo\nand ActivityNet, which further demonstrates our superiority on longer videos\nwith longer descriptions. Since our method is orthogonal to CLIP2Video, it is\nexpected that combining CLIP2TV with CLIP2Video might further improve the\nresults. We leave this for future study.\n4.4 Ablation Study\nWhy alignment with matching?To evaluate the effectiveness of alignment\nand matching, we test various combinations of vta and vtm. The ablation results\nare presented in Table.7. The 1st row shows the result of vta without vtm. It\ncan be seen as fine-tuning CLIP on other datasets. The result is still competitive\ndue to the outstanding ability of CLIP pretrained on huge data.\nHowever, as shown in the 2nd row of Table.7, when we remove the alignment\npart and train our model with only vtm, the result is severely degraded. Worse\non ActivityNet, the training is totally decayed. We attribute this catastrophe to\nthe nature of the dataset. On the one hand, videos in ActivityNet are usually\nlonger than 60 seconds, and some videos can be as long as 100 seconds or even\nmore than 200 seconds, so that input frames with fixed length of 64 cannot cover\nall the scenes described in paragraph. On the other hand, the paragraph paired\nwith a video consists of several sentences only describing the corresponding video\nsegments, while such video segments may be discontinuous in the video, which\nmeans some input frames sampled from the interval of two segments are invalid.\nCLIP2TV: Align, Match and Distill for Video-Text Retrieval 11\nTable 6.VATEX. CLIP4Clip-seqTransf* means there are no results about VATEX of\nHGR split reported in [28] and [10], so we conduct it by ourselves.\nMethod R@1 R@5 R@10 MdR MnR\nCLIP-straight [33] 39.7 72.3 82.2 2.0 12.8\nSUPPORT-SET [30] 44.9 82.1 89.7 1.0 -\nCLIP4Clip-seqTransf* [28] 60.3 89.1 94.4 1.0 4.5\nCLIP2Video [10] 61.2 90.9 95.6 1.0 3.4\nCLIP2TV 61.4 90.6 95.2 1.0 3.7\nCLIP2TV+SD 61.5 90.9 95.6 1.0 3.7\nCLIP2TV-ViT16 65.4 92.7 96.6 1.0 2.9\nTable 7. Evaluation of Multi-Modal Fusion with vanilla vtm on different datasets.\nRecall at rank 1 (R@1)↑ and Mean Rank (MnR)↓ of text-to-video retrieval are reported.\nM-V and Anet mean MSR-VTT and ActivityNet, respectively. The underline indicates\nthat retrieval results are inferred from this module.\nvta vtm M-V 1kA M-V full MSVD VATEX DiDeMo Anet\nR@1 MnR R@1 MnR R@1 MnR R@1 MnR R@1 MnR R@1 MnR\n✓ 43.8 15.8 29.2 49.5 45.8 11.1 60.3 4.5 41.6 18.9 42.2 7.4\n✓ 30.4 67.4 17.3 357.4 31.7 16.1 56.2 4.0 16.7 53.2 0.2 1851.2\n✓ ✓ 45.6 15.0 29.6 48.2 46.3 10.0 61.4 3.7 43.9 16.6 40.8 7.3\n✓ ✓ 39.9 - 26.2 - 37.2 - 59.0 - 34.6 - 33.5 -\nThese two aspects make it difficult for multi-modal encoder to model the re-\nlationship between text words and video frames. This justifies the necessity of\naligning two modalities before sending them to multi-modal transformer.\nThe 3rd row is CLIP2TV combining vta with vtm, retrieving the result from\nvta. The result shows that it consistently outperforms vta except on ActivityNet.\nThe last row shares the same structure with the 3rd row, but retrieves the result\nfrom vtm. For inference efficiency, we only select top-200 candidates from vta\nas inputs to vtm for re-ranking. Since the ground truth target might not be in\nthe 200 candidates, the mean ranking of ground truth will be absent from vtm\nin such case, therefore we neglect MnR results here. We can see that inference\nresult from vtm is much worse than it from vta. This observation is different\nfrom [21] on image-text retrieval. We speculate the reason is vtm pretrained on\nimages and texts lacks enough videos for further pretrain.\nTo sum up, vta and vtm work in a coordinated manner and benefit each\nother. The alignment is essential and vta can generate hard negatives to vtm.\nIn reverse, vtm tunes vta more finely via back propagating gradients. Moreover,\nsince we retrieve the targets using vta, CLIP2TV avoids computing candidates\nin vtm with the heavy load, and is practically friendly.\nWhat is the best negative sampling strategy for vtm?As we know,\nnegative samples plays a crucial role in contrastive learning. Will a larger number\nof negatives bring benefits as in self-supervised learning? With these questions,\n12 Zi. Gao et al.\nwe design thorough experiments to find the answer. We choose MSR-VTT 1kA\nwith noisy captions and VATEX with more detailed descriptions. We use K to\ndenote number of negatives. Results shown in Table.8 demonstrate that with\nthe increase of K, it brings more noise and ambiguity in pairs which hurts\nthe performance. The model decays or saturates when the number of negative\nsamples reaches a certain amount. As MSR-VTT has more noisy samples, a\nsmall K is good for it, while VATEX can support larger K value.\nTable 8. Evaluation of the number of negative pairs K. We report our experiment\nresults on MSR-VTT 1kA and VATEX.\nK MSR-VTT 1kA VATEX\nR@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR\n4 45.1 71.1 81.7 2.0 15.3 60.7 90.4 95.4 1.0 3.7\n8 45.6 71.1 80.8 2.0 15.0 61.4 90.6 95.2 1.0 3.7\n16 44.5 71.0 81.1 2.0 14.5 61.3 90.6 95.4 1.0 3.8\nHow to distill similarity for vtm?We show performance of three variants\nof similarity distillation on MSR-VTT 1kA in Table.9. We can see that all three\nvariants can improve the results above vanilla vtm. Specifically, results from joint\nand intra are nearly the same, and intra is slightly better than inter. This implies\nthat intra-modal similarity reflects more accurate relative distances between an\nanchor representation of text/video, towards a group of video/text representa-\ntions. Moreover, joint inner-modal metric from video and text is more accurate\nthan single modal. To better illustrate the difference of intra- and inter-modal\nsimilarity, we show similarity matrices in Figure.3, we can see that overall sim-\nilarity values are gradually increasing along (a)text-video, (b)video-video, and\n(c)text-text. (d) is the average similarity of text-text and video-video.\nIn Table.10, we show the results of vanilla vtm, soft-vtm and mean of them\nby changing the value of β, across datasets of MSR-VTT, DiDeMo and Activ-\nityNet. Since these datasets contain relatively more data noise, thus similarity\ndistillation contributes more on them. Also, combining vanilla vtm and soft-vtm\nachieves the best results. We leave more choices of β for future study.\nTable 9.Evaluation of soft-label variants. We report our experimental results on the\nMSR-VTT 1kA.\nsoft-label R@1 R@5 R@10 MdR MnR\ninter 45.8 72.5 81.6 2.0 14.6\nintra 45.9 72.3 81.5 2.0 15.3\njoint 46.1 72.5 82.9 2.0 15.2\nCLIP2TV: Align, Match and Distill for Video-Text Retrieval 13\nTable 10.Evaluation of soft vtm. We report our experimental results on MSR-VTT\n1kA, DiDeMo and ActivityNet.\nvtm MSR-VTT 1kA DiDeMo ActivityNet\nR@1 R@5 R@10 MnR R@1 R@5 R@10 MnR R@1 R@5 R@50 MnR\nvanilla 45.6 71.1 80.8 15.0 43.9 70.5 79.8 16.6 40.8 72.9 98.0 7.3\nsoft(β = 1.0) 44.8 71.8 82.3 15.2 42.7 68.3 79.3 17.5 43.4 74.7 98.1 6.8\nsoft(β = 0.5) 46.1 72.5 82.9 15.2 45.5 69.7 80.6 17.1 44.1 75.2 98.4 6.5\nFig. 3.Visualized similarity matrix. (a) text-video similarity. (b) text-text similarity.\n(c) video-video similarity. (d) joint-modal similarity which is average of text-text and\nvideo-video similarity.\n4.5 Qualitative Results\nTo illustrate the advantage of the proposed method and the impact of noise, we\nvisualize some text-to-video retrieval results on MSR-VTT 1kA shown in Fig.4.\nThe 1st row is the text for query, and the 2nd row is the corresponding ground-\ntruth video followed by its retrieval rank obtained from CLIP4Clip, CLIP2TV,\nand CLIP2TV+SD, respectively. The 3rd-5th rows present the retrieved R@1\nvideo by each method and its paired caption.\nAs shown in Fig. 4, our proposed CLIP2TV can perform better and obtain\nmore accurate retrieval results than CLIP4Clip. With the help of similarity dis-\ntillation from joint soft-label on vtm module, CLIP2TV+SD can make a great\nimprovement when querying a detailed text. On the other hand, CLIP2TV+SD\nis more robust to vague (3rd column of Fig.4) and typo (4th column of Fig.4)\nquery, which illustrates that similarity distillation can alleviate the negative ef-\nfects of data noise.\n5 Conclusions\nIn this work, we propose CLIP2TV, a new CLIP-based framework on video-text\nretrieval. CLIP2TV is composed of a video-text alignment module and a video-\ntext matching module. The two modules are trained in a coordinated manner and\nbenefit each other. To address the problem brought by data noise from popular\ndatasets, we propose similarity distillation and explore different variants based\n14 Zi. Gao et al.\nQueryGroundTruthCLIP4Clip-TOP1CLIP2TV-TOP1CLIP2TV+SD-TOP1\nnewscasters speak about a school shooting on the news program info wars\nRank:6/5/1\naman is showing off a new vehicle\nRank:5/3/1\nthey are singing a song and playing a guitar in the stage\nRank:9/7/5\namanandwomanperformingin front of judes\nRank:12/11/9\naclipfromfoxnewsontheshelbynorthcarolinashooting\naclipfromfoxnewsontheshelbynorthcarolinashooting\nnewscasters speak about a school shooting on the news program info wars\nsomeone is taking about a car\njeremyis describing a car\naman is showing off a new vehicle\narockbandperformsonstage\naband playsonastage\namanisplayingaguitarwithabandinaliveconcert\nacoupledancingdoingsalsa\napersoniscooking onstage\ntwo people welcome people to an episode of their show\nFig. 4.Visualized text-to-video retrieval results on MSR-VTT 1kA. The 1st row is\nthe query, and the 2nd row is the corresponding ground-truth video followed by ranks\nacquired from CLIP4Clip, CLIP2TV, and CLIP2TV+SD, respectively. The 3rd-5th\nrows are the retrieved top-1 video by each method and its paired caption. The word\n“judes” in red is a typo.\nin intra- and inter-modal space. To validate our motivation and explore best\npractice in this field, we conduct extensive experiments across several datasets.\nExperimental results prove the effectiveness of ours and we achieve better or com-\npetitive results towards previous SOTA methods. CLIP2TV is fast in inference\nand orthogonal to current CLIP-based frameworks, thus can be easily combined\nwith them. We believe our work can bring insights and practical expertise to\nboth community and industry.\nCLIP2TV: Align, Match and Distill for Video-Text Retrieval 15\nReferences\n1. Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.:\nLocalizing moments in video with natural language. In: Proceedings of the IEEE\ninternational conference on computer vision. pp. 5803–5812 (2017)\n2. Arazo, E., Ortego, D., Albert, P., O’Connor, N., McGuinness, K.: Unsupervised\nlabel noise modeling and loss correction. In: International conference on machine\nlearning. pp. 312–321. PMLR (2019)\n3. Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650 (2021)\n4. Chen, D., Dolan, W.B.: Collecting highly parallel data for paraphrase evaluation.\nIn: Proceedings of the 49th annual meeting of the association for computational\nlinguistics: human language technologies. pp. 190–200 (2011)\n5. Chen, S., Zhao, Y., Jin, Q., Wu, Q.: Fine-grained video-text retrieval with hierar-\nchical graph reasoning. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 10638–10647 (2020)\n6. Chen, Y.C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.:\nUniter: Universal image-text representation learning. In: European conference on\ncomputer vision. pp. 104–120. Springer (2020)\n7. Chuang, C.Y., Hjelm, R.D., Wang, X., Vineet, V., Joshi, N., Torralba, A., Jegelka,\nS., Song, Y.: Robust contrastive learning against noisy views. arXiv preprint\narXiv:2201.04309 (2022)\n8. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n9. Dzabraev, M., Kalashnikov, M., Komkov, S., Petiushko, A.: Mdmmt: Multidomain\nmultimodal transformer for video retrieval. arXiv preprint arXiv:2103.10699 (2021)\n10. Fang, H., Xiong, P., Xu, L., Chen, Y.: Clip2video: Mastering video-text retrieval\nvia image clip. arXiv preprint arXiv:2106.11097 (2021)\n11. Gabeur, V., Sun, C., Alahari, K., Schmid, C.: Multi-modal transformer for video\nretrieval. In: European Conference on Computer Vision (ECCV). vol. 5. Springer\n(2020)\n12. Ghosh, A., Kumar, H., Sastry, P.: Robust loss functions under label noise for deep\nneural networks. In: Proceedings of the AAAI conference on artificial intelligence.\nvol. 31 (2017)\n13. Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., Sugiyama, M.:\nCo-teaching: Robust training of deep neural networks with extremely noisy labels.\nAdvances in neural information processing systems 31 (2018)\n14. Hendrycks, D., Mazeika, M., Wilson, D., Gimpel, K.: Using trusted data to train\ndeep networks on labels corrupted by severe noise. Advances in neural information\nprocessing systems 31 (2018)\n15. Hoffmann, D.T., Behrmann, N., Gall, J., Brox, T., Noroozi, M.: Ranking info noise\ncontrastive estimation: Boosting contrastive learning via ranked positives. arXiv\npreprint arXiv:2201.11736 (2022)\n16. Hu, P., Peng, X., Zhu, H., Zhen, L., Lin, J.: Learning cross-modal retrieval with\nnoisy labels. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp. 5403–5413 (2021)\n17. Huang, Z., Niu, G., Liu, X., Ding, W., Xiao, X., Wu, H., Peng, X.: Learning with\nnoisy correspondence for cross-modal matching. Advances in Neural Information\nProcessing Systems 34 (2021)\n16 Zi. Gao et al.\n18. Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Carlos Niebles, J.: Dense-captioning\nevents in videos. In: Proceedings of the IEEE international conference on computer\nvision. pp. 706–715 (2017)\n19. Lei, J., Li, L., Zhou, L., Gan, Z., Berg, T.L., Bansal, M., Liu, J.: Less is more:\nClipbert for video-and-language learning via sparse sampling. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n7331–7341 (2021)\n20. Lei, J., Yu, L., Berg, T.L., Bansal, M.: Tvr: A large-scale dataset for video-subtitle\nmoment retrieval. In: European Conference on Computer Vision. pp. 447–463.\nSpringer (2020)\n21. Li, J., Selvaraju, R.R., Gotmare, A.D., Joty, S., Xiong, C., Hoi, S.: Align be-\nfore fuse: Vision and language representation learning with momentum distillation.\narXiv preprint arXiv:2107.07651 (2021)\n22. Li, L., Chen, Y.C., Cheng, Y., Gan, Z., Yu, L., Liu, J.: Hero: Hierarchical\nencoder for video+ language omni-representation pre-training. arXiv preprint\narXiv:2005.00200 (2020)\n23. Li, Y., Yang, J., Song, Y., Cao, L., Luo, J., Li, L.J.: Learning from noisy labels with\ndistillation. In: Proceedings of the IEEE International Conference on Computer\nVision. pp. 1910–1918 (2017)\n24. Liu, S., Fan, H., Qian, S., Chen, Y., Ding, W., Wang, Z.: Hit: Hierarchical\ntransformer with momentum contrast for video-text retrieval. arXiv preprint\narXiv:2103.15049 (2021)\n25. Liu, Y., Albanie, S., Nagrani, A., Zisserman, A.: Use what you have: Video retrieval\nusing representations from collaborative experts. arXiv preprint arXiv:1907.13487\n(2019)\n26. Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin trans-\nformer. arXiv preprint arXiv:2106.13230 (2021)\n27. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. Advances in neural information\nprocessing systems 32 (2019)\n28. Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An empir-\nical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860\n(2021)\n29. Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.:\nHowto100m: Learning a text-video embedding by watching hundred million nar-\nrated video clips. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 2630–2640 (2019)\n30. Patrick, M., Huang, P.Y., Asano, Y., Metze, F., Hauptmann, A., Henriques, J.,\nVedaldi, A.: Support-set bottlenecks for video-text representation learning. arXiv\npreprint arXiv:2010.02824 (2020)\n31. Patrini, G., Rozza, A., Krishna Menon, A., Nock, R., Qu, L.: Making deep neural\nnetworks robust to label noise: A loss correction approach. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition. pp. 1944–1952 (2017)\n32. Pereyra, G., Tucker, G., Chorowski, J., Kaiser,  L., Hinton, G.: Regularizing\nneural networks by penalizing confident output distributions. arXiv preprint\narXiv:1701.06548 (2017)\n33. Portillo-Quintero, J.A., Ortiz-Bayliss, J.C., Terashima-Mar´ ın, H.: A straightfor-\nward framework for video retrieval using clip. arXiv preprint arXiv:2102.12443\n(2021)\nCLIP2TV: Align, Match and Distill for Video-Text Retrieval 17\n34. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models\nfrom natural language supervision. arXiv preprint arXiv:2103.00020 (2021)\n35. Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., Rabinovich, A.: Train-\ning deep neural networks on noisy labels with bootstrapping. arXiv preprint\narXiv:1412.6596 (2014)\n36. Song, H., Kim, M., Lee, J.G.: Selfie: Refurbishing unclean samples for robust deep\nlearning. In: International Conference on Machine Learning. pp. 5907–5915. PMLR\n(2019)\n37. Song, H., Kim, M., Park, D., Shin, Y., Lee, J.G.: Learning from noisy labels with\ndeep neural networks: A survey. arXiv preprint arXiv:2007.08199 (2020)\n38. Sun, C., Baradel, F., Murphy, K., Schmid, C.: Learning video representations using\ncontrastive bidirectional transformer. arXiv preprint arXiv:1906.05743 (2019)\n39. Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.: Videobert: A joint model\nfor video and language representation learning. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. pp. 7464–7473 (2019)\n40. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n41. Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.F., Wang, W.Y.: Vatex: A large-scale,\nhigh-quality multilingual dataset for video-and-language research. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 4581–4591\n(2019)\n42. Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., Bailey, J.: Symmetric cross entropy for\nrobust learning with noisy labels. In: Proceedings of the IEEE/CVF International\nConference on Computer Vision. pp. 322–330 (2019)\n43. Wang, Z., Wu, Y., Narasimhan, K., Russakovsky, O.: Multi-query video retrieval.\narXiv preprint arXiv:2201.03639 (2022)\n44. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for\nbridging video and language. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition. pp. 5288–5296 (2016)\n45. Yang, J., Bisk, Y., Gao, J.: Taco: Token-aware cascade contrastive learning for\nvideo-text alignment. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision. pp. 11562–11572 (2021)\n46. Yu, Y., Kim, J., Kim, G.: A joint sequence fusion model for video question an-\nswering and retrieval. In: Proceedings of the European Conference on Computer\nVision (ECCV). pp. 471–487 (2018)\n47. Zhang, B., Hu, H., Sha, F.: Cross-modal and hierarchical modeling of video and\ntext. In: Proceedings of the European Conference on Computer Vision (ECCV).\npp. 374–390 (2018)\n48. Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O.: Understanding deep\nlearning (still) requires rethinking generalization. Communications of the ACM\n64(3), 107–115 (2021)\n49. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412 (2017)\n50. Zhang, Z., Sabuncu, M.: Generalized cross entropy loss for training deep neural\nnetworks with noisy labels. Advances in neural information processing systems 31\n(2018)\n51. Zhu, L., Yang, Y.: Actbert: Learning global-local video-text representations. In:\nProceedings of the IEEE/CVF conference on computer vision and pattern recog-\nnition. pp. 8746–8755 (2020)",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.8288211226463318
    },
    {
      "name": "Computer science",
      "score": 0.7735658288002014
    },
    {
      "name": "Transformer",
      "score": 0.7726508378982544
    },
    {
      "name": "Information retrieval",
      "score": 0.5262357592582703
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48510444164276123
    },
    {
      "name": "Natural language processing",
      "score": 0.32503852248191833
    },
    {
      "name": "Engineering",
      "score": 0.08195620775222778
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 28
}