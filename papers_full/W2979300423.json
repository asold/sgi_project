{
  "title": "Real Life Application of a Question Answering System Using BERT Language Model",
  "url": "https://openalex.org/W2979300423",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2990219105",
      "name": "Francesca Alloatti",
      "affiliations": [
        "University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A2153458393",
      "name": "Luigi Di Caro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2980174715",
      "name": "Gianpiero Sportelli",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2562439797",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2511929605",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962829230",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2922551710",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2962739339"
  ],
  "abstract": "It is often hard to apply the newest advances in research to real life scenarios. They usually require the resolution of some specific task applied to a restricted domain, all the while providing small amounts of data to begin with. In this study we apply one of the newest innovations in Deep Learning to a task of text classification. We created a question answering system in Italian that provides information about a specific subject, e-invoicing and digital billing. Italy recently introduced a new legislation about e-invoicing and people have some legit doubts, therefore a large share of professionals could benefit from this tool.",
  "full_text": "Proceedings of the SIGDial 2019 Conference, pages 250–253\nStockholm, Sweden, 11-13 September 2019.c⃝2019 Association for Computational Linguistics\n250\nReal Life Application of a Question Answering System Using BERT\nLanguage Model\nFrancesca Alloatti1,2, Luigi Di Caro2 , Gianpiero Sportelli1\n1CELI - Language Technology, Italy\n2Department of Computer Science - Università degli Studi di Torino, Italy\n{francesca.alloatti, gianpiero.sportelli}@celi.it\nluigi.dicaro@unito.it\nAbstract\nReal life scenarios are often left untouched by\nthe newest advances in research. They usually\nrequire the resolution of some speciﬁc task ap-\nplied to a restricted domain, all the while pro-\nviding small amounts of data to begin with.\nIn this study we apply one of the newest in-\nnovations in Deep Learning to a task of text\nclassiﬁcation. The goal is to create a question\nanswering system in Italian that provides in-\nformation about a speciﬁc subject, e-invoicing\nand digital billing. Italy recently introduced\na new legislation about e-invoicing and peo-\nple have some legit doubts, therefore a large\nshare of professionals could beneﬁt from this\ntool. We gathered few pairs of question and\nanswers; afterwards, we expanded the data,\nusing it as a training corpus for BERT lan-\nguage model. Through a separate test corpus\nwe evaluated the accuracy of the answer pro-\nvided. Values show that the automatic system\nalone performs surprisingly well. The demo\ninterface is hosted on Telegram, which makes\nthe system immediately available to test.\n1 Introduction\nPre-trained models have proven to be of great\nhelp in accomplishing many NLP tasks, such as\nnatural language inference, text classiﬁcation and\nquestion-answering. All of these paradigms con-\ntain a semi-supervised language model trained on\nlarge corpora of data; they are later ﬁne-tuned to\nwork on downstream tasks (Peters et al., 2018;\nHoward and Ruder, 2018; Radford et al., 2018).\nHowever, real life applications can’t often bene-\nﬁt from these advances, for many reasons: lack\nof data, lack of time and resources to reach a\nsufﬁcient accuracy level, or the need to address\nsome very speciﬁc domain that elude the scope\nof a general-purpose architecture. As a result,\nmany concrete scenarios of applications are left\nuntouched by the scientiﬁc progress, even though\nthese obstacles are far from impossible to over-\ncome.\nThe goal of this study is to build a question-\nanswering systems using only BERT (Bidirec-\ntional Encoder Representations from Transform-\ners) language model (Devlin et al., 2018), with-\nout exploiting any rule-based reﬁnement system or\nany other proprietary algorithm. This process al-\nlows to prevail over the obstacles previously listed:\nscarce original data was expanded mostly by using\ngenerative grammars; the whole project (data ex-\npansion plus the various training phases) took no\nmore than eight days to complete, and the com-\nputational resources required were fairly afford-\nable 1. Moreover, the application domain is very\nspeciﬁc, such that the ﬁne-tuning of the linguistic\nmodel signiﬁcantly increased the performances 2.\nThe architecture is simple yet effective (as shown\nin Figure 1) and the output of the system can be\ntested immediately through a Telegram bot.\n2 Related Works\nSince its ﬁrst appearance, BERT has gained a lot\nof popularity in the academic community. It has\nbeen applied to various NLP tasks, including text\nclassiﬁcation for question answering. The origi-\nnal work by Devlin et al. (2018) contained results\non BERT’s performance over the Stanford Ques-\ntion Answering Dataset task (Rajpurkar et al.,\n2016), where the system had to predict the answer\nspan for a speciﬁc question in a Wikipedia pas-\nsage. Yang et al. (2019) went further, creating a\nquestion answering system deployed as a chatbot.\nHowever, both these studies tackled the task of\nopen-domain question answering, while we focus\non cases where BERT was exploited to develop\nsystems for real life applications. For instance,\n1CPU 8 core, GPU 28 GB, RAM 32 GB\n2See the Results section for details on the performance.\n251\nFigure 1: Architecture of our question answering sys-\ntem\nLee et al. (2019) created a new BERT language\nmodel pre-trained on the biomedical ﬁeld to solve\ndomain-speciﬁc text mining tasks (BioBERT). Its\nresults are impressive, but BioBERT is capable to\nperform well on domain speciﬁc knowledge be-\ncause of its large pre-training process. While the\npre-training surely yields better performances, it\nis highly expensive with regard to computational\ncosts and time consumption. Our results show\ngood performances even without any pre-training.\nJESSI, a Joint Encoders for Stable Suggestion\nInference (Park et al., 2019), was created upon the\nknowledge that BERT is severely unstable for out-\nof-domain samples. This is true for every system\nthat does not implement any other tool other than\nthe language model, such as ours. To solve this\nproblem, Park et al. (2019) combined BERT with\na non-BERT encoder and used a RNN classiﬁer\non top of BERT. In our case, an heuristic could\nbe applied to the answers given by the system. It\nwould allow to maximize the probability that the\noutput is the correct match and not solely the one\nwith a higher conﬁdence score.\nOther studies focus on generating pre-trained\nembeddings for speciﬁc domains (Beltagy et al.,\n2019; Alsentzer et al., 2019), but they do not test\nthem on speciﬁc tasks.\n3 BERT’s Head start as a Language\nModel\nBERT’s architecture is built as a multi-layer bidi-\nrectional encoder and it is based on the Trans-\nformer model originally proposed by Vaswani et\nal. (2017). Although BERT has been widely used\nin the past year, it is not the only tool available to\nautomatically build a working question-answering\nsystem. Attention based RNN models, especially\nwith the addition of a LSTM or GRU module, have\nyielded good results on a variety of tasks (Wang\net al., 2016; Zhou et al., 2016). The use of a recur-\nrent neural network for our work was eventually\nruled out for two main reasons: ﬁrst, BERT en-\ncoder architecture is already trained to work as a\nlanguage model on more than 104 languages (in-\ncluding Italian) and needs to be reﬁned only for\nthe speciﬁc task of text classiﬁcation. The training\nof a RNN needs to be done for both the language\nmodel creation and the ﬁne-tuning part, which re-\nquires a higher volume of data.\nSecond, the RNN training activities cannot be\ncarried out simultaneously due to network con-\nstraints. This causes a more time-consuming and\ncostly process.\n4 Data and Fine-tuning Process\nSince this aims to be a real life application, the\nchosen domain was e-invoicing and all the new\nregulations revolving around the theme of digital\nbilling that was recently introduced by the Italian\nlegislation. The ﬁeld is very technical and spe-\nciﬁc; the data needed to reﬂect the features of the\nlanguage employed to discuss such subject.\nWe ﬁrst gathered pairs of clauses coherent to\nthe domain. The data was cleaned from duplicated\nquestions and badly written sentences, resulting in\na corpus of approximately 300 pairs of sentences\n(a question and an answer). Half of the questions\nwas expanded manually, while the other half - that\npresented recurrent linguistic patterns - was ex-\npanded using generative grammars. A grammar\nis written as follows:\n{vb_might} {vb_collect} an\n{n_invoice} ?\nResulting is sentences such as Is it possible to\ncollect an e-invoice? together with all its mean-\ningful variations. The two expansion methods cre-\nated a corpus of more than 210.000 sentence pairs.\nNo expansion was operated on the answers, since\nthe goal is to match the correct answer to any pos-\nsible expression of a question, and not to produce\nvariegated answers.\nSeparately, a different corpus of 200 questions\nwas obtained on a voluntary base from people who\ndid not take part in the expansion process (other-\nwise, they would have had knowledge of the exist-\ning sentences in the training corpus). This distinct,\nunbiased corpus served as a test set.\n4.1 The Fine-tuning\nDuring the ﬁne-tuning process the goal is to ex-\npand the network architecture and to train it to-\n252\nwards a speciﬁc task. A new layer is created\nwhile the weights of the underlying original layer\nare modiﬁed according to the text classiﬁcation\njob. The ﬁnal training corpus consisted of 2300\nsentences (obtained from the 210.000 previously\nmentioned). This number resulted from balanc-\ning the total of manually expanded questions with\nthe automatically expanded ones. Otherwise we\nwould have had an overﬁtting problem, since\nthe automatic expansion generates way more sen-\ntences than the manual one. Afterwards, we used\nthe test corpus to verify the output of the new net-\nwork.\nValues such as accuracy, precision and recall\nare not taken into consideration during the train-\ning process. Instead, the goal is to optimize, i.e.\nminimize, a loss function. For this study the loss\nfunction is a Cosine Proximity (1). To compute\nit we created a One Hot Vector that represented\nthe 300 original sentences - each one of them as\na label. The loss function takes into account two\nvalues: the One Hot Vector and the logarithm of\nthe network output’s softmax.\nL = − y ·ˆy\n||y||2 ·||ˆy||2\n= −\nN∑\ni=1\nyi ·ˆyi\n√\nN∑\ni=1\ny2\ni ·\n√\nN∑\ni=1\nˆy2\ni\n(1)\nCosine Proximity Loss function\nEach experimental round takes approximately\none hour.\n5 Results\nTo assess the performance, different experiments\nwere conducted in a subsequent way to evaluate\nthe accuracy of the test set. The ﬁrst attempts were\nconsidered baseline for the following ones. When\nBERT model was used without applying any ﬁne-\ntuning the accuracy reached 3,6 % for 40 epochs.\nFine-tuning proved to be essential: accuracy on\nthe ﬁrst answer selected by the system is 86%.\nWhen considering the ﬁrst three answers, the value\nrises up to 93,6%. The most recent experiment op-\nerates on the pre-trained language model too see\nif further improvement could be reached on that\nfront. The language model was trained with new\ndata extracted from reliable sources (operational\nhandbooks from the Italian Fiscal Agency) and\nlater ﬁne-tuned with the same data of the previ-\nous trial. Accuracy gained +2 points, achieving 88\n% on ﬁrst answer.\nWe also compared our results to other intent\nmatching systems such as Google DialogFlow.\nUsing external API for intent detection accuracy\nreached 84%, which is slightly lower to our ﬁrst\nexperiment.\nAn example of the matched question (and its an-\nswer) is presented in Table 1. The user can give a\nfeedback on each answer received, and the posi-\ntive or negative feedback will add up to constantly\nimprove the performance for the next questions.\nThe average time to obtain a single answer is 0.2\nseconds on a CPU architecture. It is therefore per-\nfectly viable for a real time employ as a question\nanswering system.\nUnfortunately, it is impossible to compare these\nresults with those obtained from other studies, be-\ncause of the speciﬁcity of this domain, which has\nnever been considered in this kind of experiments\n(at least for the Italian language).\n6 Conclusion and Next Steps\nWe have demonstrated that it is possible to create a\nquestion answering system in a few days. The hu-\nman effort was minimized - no rule was handwrit-\nten and no other algorithm was implemented - and\noverall the computational cost was bearable. We\nalso showed that scarce data is not always an in-\nsurmountable obstacle, since the expansion effort\ncan be split between manual work and automatic\none. The results show that such a system can al-\nready be used with a decent degree of success. In\nthe next future some improvements are going to be\nmade regarding the context management and the\ncomparison between BERT and other tools.\nTo improve the spectrum of questions that are\ncorrectly matched, we propose two ways to man-\nage the dialog context using BERT:\n•External operation.The context is given as\nan external factor to the model through the\nwriting of speciﬁc rules. It modiﬁes the la-\nbeling, i.e. the probability assigned to a label\nthat selects a matching question.\n•Internal operation. In this case, BERT\nneeds to be trained towards two inputs, where\none is always the context. The network\nchanges its way of calculating the probabil-\nity from p (l|t)(l being the label and t the text\n253\nType of Sentence Content\nQuestion posed Hello, I have a question: do I have to issue an invoice also for private\nclients? Even though they don’t refer to any V AT number?\nQuestion matched Does an invoice need to be issued also towards people without a V AT\nnumber?\nAnswer provided Yes, the electronic document has to be issued towards private clients\nwithout a V AT number\nTable 1: Given a certain question posed by an user, the system matches one of the example in his knowledge bases\nand sends out the correct answer. The sentences have been translated from Italian into English for the purpose of\nthis paper.\nof the sentence) to p (l|t∩c).\nRegarding the other tools, our goal is to verify if\nother models could perform equally or better given\nthe same dataset. Many platforms are currently\nunder review, such as Amazon Lex.\nReferences\nEmily Alsentzer, John R. Murphy, Willie Boag,\nWei-Hung Weng, Di Jin, Tristan Naumann, and\nMatthew B. A. McDermott. 2019. Publicly avail-\nable clinical bert embeddings. arXiv preprint,\narXiv:1904.03323.\nIz Beltagy, Arman Cohan, and Kyle Lo. 2019. Scibert:\nPretrained contextualized embeddings for scientiﬁc\ntext. arXiv preprint, arXiv:1903.10676.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint, arXiv:1810.04805v1.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 328–\n339.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2019. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. arXiv preprint, arXiv:1901.08746v3.\nCheoneum Park, Juae Kim, Hyeon gu Lee,\nReinald Kim Amplayo, Harksoo Kim, Jungyun\nSeo, and Changki Lee. 2019. This is compe-\ntition at semeval-2019 task 9: Bert is unstable\nfor out-of-domain samples. arXiv preprint ,\narXiv:1904.03339v1.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, , and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. NAACL.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding by generative pre-training. OpenAI\npreprint.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100, 000+ ques-\ntions for machine comprehension of text. CoRR,\nabs/1606.05250.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Lukasz Kaiser Aidan\nN Gomez, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 6000–6010.\nBingning Wang, Kang Liu, and Jun Zhao. 2016. In-\nner attention based recurrent neural networks for an-\nswer selection. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1288–1297. Association for Compu-\ntational Linguistics.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nbertserini. CoRR, abs/1902.01718.\nXinjie Zhou, Xiaojun Wanand, and Jianguo Xiao.\n2016. Attention-based lstm network for cross-\nlingual sentiment classiﬁcation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 247–256. Associ-\nation for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7753393054008484
    },
    {
      "name": "Task (project management)",
      "score": 0.7325596809387207
    },
    {
      "name": "Question answering",
      "score": 0.7030897736549377
    },
    {
      "name": "Subject (documents)",
      "score": 0.6296020150184631
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5586432814598083
    },
    {
      "name": "Legislation",
      "score": 0.5232823491096497
    },
    {
      "name": "World Wide Web",
      "score": 0.42987060546875
    },
    {
      "name": "Data science",
      "score": 0.4054204821586609
    },
    {
      "name": "Natural language processing",
      "score": 0.3917419910430908
    },
    {
      "name": "Information retrieval",
      "score": 0.3834192156791687
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37383365631103516
    },
    {
      "name": "Engineering",
      "score": 0.1062048077583313
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55143463",
      "name": "University of Turin",
      "country": "IT"
    }
  ],
  "cited_by": 15
}