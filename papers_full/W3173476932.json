{
  "title": "DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation",
  "url": "https://openalex.org/W3173476932",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5112731457",
      "name": "Xinyu Hua",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5079112672",
      "name": "Ashwin Sreevatsa",
      "affiliations": [
        "University of Michigan"
      ]
    },
    {
      "id": "https://openalex.org/A5100364489",
      "name": "Lu Wang",
      "affiliations": [
        "University of Michigan"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3091627448",
    "https://openalex.org/W3100714086",
    "https://openalex.org/W4288091035",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2250750514",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W2935206035",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W1645937837",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W3100163144",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W3115433372",
    "https://openalex.org/W1974991592",
    "https://openalex.org/W3034352114",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2798277467",
    "https://openalex.org/W3103317667",
    "https://openalex.org/W3035043191",
    "https://openalex.org/W2495448072",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2970383515",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3026997957",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3023786569",
    "https://openalex.org/W2950397305",
    "https://openalex.org/W1948566616",
    "https://openalex.org/W3080122044",
    "https://openalex.org/W3115328016",
    "https://openalex.org/W3042876905",
    "https://openalex.org/W4229543565",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3021582395",
    "https://openalex.org/W2903428882",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2963480675",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3098295156",
    "https://openalex.org/W2945972899",
    "https://openalex.org/W2796084947",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W2551029266",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2963993699",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2799225444",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3158986179",
    "https://openalex.org/W2970686438",
    "https://openalex.org/W3167303745",
    "https://openalex.org/W2016522586",
    "https://openalex.org/W2981259322",
    "https://openalex.org/W3117704329",
    "https://openalex.org/W3160989675",
    "https://openalex.org/W3034531294",
    "https://openalex.org/W2991127892",
    "https://openalex.org/W2803267010",
    "https://openalex.org/W2963721761",
    "https://openalex.org/W2970072000",
    "https://openalex.org/W2952088495",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W3102401511",
    "https://openalex.org/W2914855263",
    "https://openalex.org/W3098427234"
  ],
  "abstract": "Xinyu Hua, Ashwin Sreevatsa, Lu Wang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 6408–6423\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6408\nDYPLOC: Dynamic Planning of Content Using Mixed Language Models\nfor Text Generation\nXinyu Hua1 Ashwin Sreevatsa2 Lu Wang2\n1Khoury College of Computer Sciences, Northeastern University, Boston, MA\n2Computer Science and Engineering, University of Michigan, Ann Arbor, MI\n1hua.x@northeastern.edu\n2{asreeva, wangluxy}@umich.edu\nAbstract\nWe study the task of long-form opinion text\ngeneration, which faces at least two distinct\nchallenges. First, existing neural generation\nmodels fall short of coherence, thus requir-\ning efﬁcient content planning. Second, diverse\ntypes of information are needed to guide the\ngenerator to cover both subjective and objec-\ntive content. To this end, we propose DY-\nPLOC, a generation framework that conducts\ndynamic planning of content while generating\nthe output based on a novel design of mixed\nlanguage models. To enrich the generation\nwith diverse content, we further propose to\nuse large pre-trained models to predict relevant\nconcepts and to generate claims. We experi-\nment with two challenging tasks on newly col-\nlected datasets: (1) argument generation with\nReddit ChangeMyView, and (2) writing arti-\ncles using New York Times’ Opinion section.\nAutomatic evaluation shows that our model\nsigniﬁcantly outperforms competitive compar-\nisons. Human judges further conﬁrm that our\ngenerations are more coherent with richer con-\ntent.\n1 Introduction\nOpinion articles serve as an important media to\nconvey the authors’ values, beliefs, and stances on\nimportant societal issues. Automatically generat-\ning long-form opinion articles has the potential of\nfacilitating various tasks, such as essay writing and\nspeech drafting, and it is the focus of this work.\nThough opinion generation has been investigated\nfor constructing arguments (Hua and Wang, 2018),\nwriting reviews (Ni and McAuley, 2018), and pro-\nducing emotional dialogue responses (Song et al.,\n2019), those outputs are relatively short. While im-\npressive progress in generation has been achieved\nby using large pre-trained Transformers (Radford\net al., 2019; Lewis et al., 2020a), directly adopting\n              \nUnited_States, Intelligence      knowledge, \nattack     America was never prepared and had a \nbad intelligence system.\nPresident_of_the_U.S., Bill_Clinton, 9/11_attacks     \n make, happen, mistake, administration \nGeorge_W._Bush, 9/11_attacks, Iraq     \nexistence\n1) \n -> \n \n2) \n-> \n \n3) \n -> \nFigure 1: Sample counter-argument on Reddit Change-\nMyView. Our generator considers an input containing\n(1) a title and (2) an unordered set of content items.\nEach content item consists of elements of an entity set\n[ENT], a concept set [CON], and an optional one-\nsentence claim [CLAIM]. Each output token is gener-\nated by conditioning on all content items, and the best\naligned ones (learned by our model) are highlighted in\ncorresponding colors. We also underline words that re-\nﬂect the input concepts and entities.\nthem for long-form opinion text generation poses\ndistinct challenges.\nFirst, large models still fall short of producing\ncoherent text due to the lack of efﬁcient content\ncontrol and planning (Ko and Li, 2020; Wu et al.,\n2020; Tan et al., 2021). A common solution is to\nuse concatenated phrases or semantic representa-\ntions to guide the generation process (Yao et al.,\n2019; Harkous et al., 2020; Ribeiro et al., 2020;\nGoldfarb-Tarrant et al., 2020), where content plan-\nning, including both content selection and ordering,\nis expected to be learned by attention mechanisms.\nHowever, attentions have only achieved limited im-\nprovements. Recent work also explores training a\n6409\nseparate planning module to produce sorted con-\ntent, which is then fed into a generator (Fan et al.,\n2019; Hua and Wang, 2020; Goldfarb-Tarrant et al.,\n2020). Nonetheless, this strategy results in a dis-\nconnection between planning and realization, and\nthe output is not guaranteed to respect the planning\nresults (Castro Ferreira et al., 2019; Prabhumoye\net al., 2020).\nThe second challenge for opinion generation re-\nsides in the diversity of information that is needed\nto produce an output with consistent stances and\nsupported by pertinent facts. Though large mod-\nels memorize signiﬁcant amounts of knowledge,\nthey cannot retrieve and operate with them pre-\ncisely (Lewis et al., 2020b). Due to the argumenta-\ntive nature of opinion text, simply including knowl-\nedge bases (Guan et al., 2020; Zhou et al., 2020)\nis insufﬁcient to uphold the desired quality, as it\nrequires the combination of subjective claims and\nobjective evidence as supports.\nTo this end, we propose a novel generation\nframework, DYPLOC (dynamic planning of\ncontent), to conduct content selection and order-\ning as text is produced.1 Concretely, given a set of\nunordered content items, as displayed in Figure 1,\nwe design mixed language models, with each imple-\nmented as a sequence-to-sequence model to encode\none item and the input statement. At each decoding\nstep, our system selects which items to reﬂect, and\npredicts a word based on probabilities marginalized\nover all language models. Crucially, our end-to-end\ntrained framework (1) enables the generator to ac-\ncess multiple content items at all times and select\ncontent based on what has been generated so far,\n(2) can be directly built on large pre-trained Trans-\nformers, e.g., BART (Lewis et al., 2020a), with\nplanning and generation modules jointly trained,\nand (3) outputs learned content selection scores to\nprovide an interface for system decision interpreta-\ntion.\nFurthermore, to ensure that our framework can\nbe applied to a broad range of generation tasks,\nwe design content items to cover three critical el-\nements: entities and concepts that are cen-\ntral to many generation applications, and claims\nthat are building blocks for opinion text. We show\nan example for counter-argument generation in Fig-\nure 1. Importantly, we employ BART to predict\nadditional relevant concepts, derived from Concept-\n1Data and code are available at: xinyuhua.github.\nio/Resources/acl21/.\nNet (Speer et al., 2017), and generate claims, as\ncentral propositions, to enrich the generated text\nwith both objective and subjective content.\nFor experiments, we collect two datasets: (1)\nposts from Reddit ChangeMyView for argument\ngeneration, and (2) articles from the New York\nTimes Opinion section (Sandhaus, 2008) for opin-\nion article writing. Our proposed framework out-\nperforms competitive comparisons, such as ﬁne-\ntuning BART with the same content items, based\non automatic metrics of BLEU, ROUGE, and ME-\nTEOR. Human assessment further conﬁrms that\nour system outputs have richer content and are\nmore coherent in both tasks.\nOur main contributions are summarized as be-\nlow:\n•We present a dynamic content planning gen-\neration framework, which is directly built on top\nof BART. Our design of mixed language models\novercomes the lack of control by existing models\nthat use implicit planning with attentions or hard\ncopying.\n•We propose content plan augmentation by auto-\nmatically generating relevant concepts and claims.\n•We construct two opinion text generation\ndatasets with content plans that capture prominent\nentities and concepts.\n2 Related Work\nNeural Generation with Planning.Text planning\nis seen as a crucial step to guide the generation\nof high-quality, well-organized natural language\ntext (McKeown, 1992; Reiter and Dale, 2000). In-\ncorporating planning modules to neural text genera-\ntor has attracted signiﬁcant research interests (Shen\net al., 2019; Moryossef et al., 2019; Puduppully\net al., 2019), which proves to be especially bene-\nﬁcial for long-form output (Fan et al., 2019; Hua\nand Wang, 2019). More recently, large pre-trained\nTransformers have established new state-of-the-arts\nfor a wide range of text generation tasks (Lewis\net al., 2020a; Roller et al., 2020; Kale and Rastogi,\n2020). But it is non-trivial to integrate planning\nmodules into them. Existing approaches resort to\ndecoupling planning and decoding stages (Hua and\nWang, 2020; Kedzie and McKeown, 2020), which\ninevitably increases system complexities and po-\ntentially introduces cascading errors.\nWe take inspiration from the retrieval-augmented\ngeneration framework (Lewis et al., 2020b), which\nis designed to incorporate relevant documents for\n6410\nV\nContent Item Encoding (in parallel)\nPlan Scoring\nMixed Conditional \nLanguage Model\nContent \nItem-Conditioned LM\n<s> \nClaim Generation\nConcept Expansion\n<s> <s> \n<s> \n<s> \n<s> \n<s> \n<s> \n<s> <s> \n<s> \n<s> <s> \n<s> \n<s> <s> \n   “CMV . I believe 9/11 \nwould not have happened if \nAl Gore were elected \nPresident.”\n0.80 x\n0.10 x\n0.05 x\n0.05 x\nFigure 2: Our proposed text generation framework, DYPLOC. [Left] For each input content item (a title t, an\nentity set Ei, and a core concept set Ci), we ﬁrst expand it with more relevant concepts, i.e., C+\ni . For sentences\nto be realized as claims, we employ a separate generator to produce one draft claim, mi. [Right] The augmented\ncontent items, denoted as {xi}, are encoded in parallel. At each decoding step, a plan scoring network estimates\na distribution d(xi|y<t) for all content items and decides on relevant content. A word is predicted based on\nprobabilities marginalized over all content item-conditioned language models, i.e.,p(yt|y<t,xi) for the i-th model.\nquestion answering. Our adaptation uses a trainable\nplan scoring module to reﬂect content selection and\nordering, which is more suitable for long text gener-\nation and offers better interpretability. Concurrent\nwork by Zhang et al. (2021) presents a mixture-\nof-expert decoder to tackle knowledge-grounded\ngeneration. However, their score distribution for\nlanguage models is ﬁxed across all decoding steps,\nwhereas ours is updated as generation progresses\nand can better reﬂect the dynamic nature of content\nplanning.\nControllable Text Generation. Another related\nline of research investigates the controllability of\ngeneration models (Wiseman et al., 2017), includ-\ning conditioning over keywords (Keskar et al.,\n2019; Hua and Wang, 2020; Xu et al., 2020), syn-\ntactic structures (Casas et al., 2020; Goyal and Dur-\nrett, 2020), or semantic representations (Wen et al.,\n2015; Elder et al., 2018). Our work differs from all\nprevious methods as we combine different types\nof content, covering both objective and subjective\ninformation, and attain ﬁne-grained sentence-level\ncontrol using a novel design of mixed conditional\nlanguage models.\nOpinion Text Generation. Our model tackles\nopinion articles, which differs from traditional text\ngeneration systems that mostly concern fact-based\ngenerations (Gardent et al., 2017; Novikova et al.,\n2017; Puduppully et al., 2019). An extensive body\nof work has studied summarizing (Wang and Ling,\n2016; Suhara et al., 2020; Braˇzinskas et al., 2020)\nor generating (Ni and McAuley, 2018; Li et al.,\n2019) reviews and building dialogue systems en-\nhanced with emotions (Li et al., 2016; Song et al.,\n2019). More recently, developments are made in\ngenerating argumentative text (El Baff et al., 2019;\nHidey and McKeown, 2019), which primarily focus\non constructing single sentence claims on a limited\nnumber of topics. In comparison, our model can\nhandle substantially longer output with improved\nquality.\n3 Model\nTask Formulation. Our opinion text genera-\ntion framework takes as input a set of content\nitems. Each content item consists of a title t, a\nset of entities Ei 2, such as {United States,\n9/11 attacks}, and a set of core conceptsCi,\nsuch as {attack, knowledge}, that are often abstract\nnotions. Our model ﬁrst expands Ci by predicting\nadditional relevant conceptsC+\ni and optionally\ngenerates a pertinent claim mi, and then outputs\nthe ﬁnal text with multiple sentences as y = {yt},\nto faithfully reﬂect the content items with a co-\nherent structure. An overview of our system is\nillustrated in Figure 2.\nBelow we ﬁrst describe the content item augmen-\ntation methods (§ 3.1), followed by our generator\nwith mixed language models that condition on ex-\npanded content items (§ 3.2).\n3.1 Content Item Augmentation\nConcept Expansion. With limited number of en-\ntities and concepts as input, generation systems\nare often incapable of producing long text with\nrich content, resulting in hallucination (Wiseman\net al., 2017; Tian et al., 2019). Therefore, from\nthe often-abstract core concepts, we aim to pre-\ndict more speciﬁc concepts that are also relevant\nto the given title. For instance, as displayed in\nFigure 1, for core concepts {make, happen}and\n2Note that i distinguishes the items. Their order is random.\n6411\nentities {Bill Clinton, 9/11 attacks}, we\ngrow the input with more concrete concepts of\n{mistake, administration}.\nWe thus consider a concept expansion module\ng(·), which predicts additional relevant concepts,\ndenoted as C+\ni , by conditioning on the original\ncontent item:\nC+\ni = g(t,Ei,Ci) (1)\nWhile g(·) can be any conditional predictor, our\nexperiment shows that ﬁne-tuned BART model per-\nforms best on our tasks, where it generates C+\ni\nword-by-word by consuming the content item. 3\nTraining data construction is described in § 4.2.\nClaim Generation. As discussed in § 1, opinion\ntext generation should be controlled with consistent\npropositions, which cannot be effectively expressed\nby disconnected concepts. Therefore, we argue that\nnatural languages are more suitable for delivering\ncentral claims, since they better encode stylistic\nlanguages, e.g., persuasion strategies.\nConcretely, we ﬁne-tune another BART model\nby taking in the title t and the entities Ei, which\nthen produces a claim with nucleus sampling for\ndecoding (Holtzman et al., 2020). In this work,\nwe assume the subset of content items that can be\nused to generate claims is known. Possible future\nwork includes predicting such subsets and ﬁltering\nclaims with quality measurement.\n3.2 Content Realization via Mixed\nConditioning\nAfter obtaining the augmented content items, we\nleverage the BART model to encode each of them\nas a sequence, as illustrated in Figure 2. Segmenter\n<s> is added to indicate the change of elements\nin a content item. Our encoders run over all items\n{xi}in parallel, from which we extract content\nitem representations {hi}, based on the last layer’s\nhidden states of the ﬁrst token.\nThe standard sequence-to-sequence (seq2seq)\nframework models output probabilities by taking a\nsingle sequence as input. It is challenging to extend\nseq2seq to consider multiple sequences simultane-\nously, and conduct content planning concurrently.\nTherefore, we introduce a plan scoring network,\n3We also exploited a model that uses the structure of knowl-\nedge bases, e.g., ConceptNet, for learning to expand concepts,\nbut it yields lower precision and recall than ﬁne-tuning BART\ndoes.\nd(xi|y<t), which learns to dynamically select and\norder content based on what has been produced pre-\nviously while generating the outputs. As outlined\nin Figure 2, our generator is informed of all content\nitems during generation. At each decoding step\nt, the probabilities of output words are estimated\nas a weighted sum of all content item-conditioned\nlanguage models as follows:\np(yt|y<t) =\n∑\ni\nd(xi|y<t)p(yt|y<t,xi) (2)\nd(xi|y<t) =softmaxi(eit) (3)\nwhere p(yt|y<t,xi) corresponds to the i-th lan-\nguage model with xi as the input. Crucially,\nd(xi|y<t) determines the importance of xi when\ngenerating token yt and thus achieves the effect\nof content planning. We design a two-layer feed-\nforward network to estimate eit:\neit = Wo tanh(Wd[hi; st]) (4)\nwhere hi denotes the representation of content\nitem xi, st is the decoder state, andWo and Wd are\nlearnable parameters. Although mixed language\nmodels have been used by Lewis et al. (2020b) to\ninclude retrieved documents for question answer-\ning, their relevance scores are given by external\nretrieval models, whereas our plan scorerd(xi|y<t)\nis learned together with the generator.\nTraining and Decoding.Our model is end-to-end\ntrained with both the standard cross-entropy loss\nLgen over the tokens in the target generations and\na separate loss Lplan for learning d(xi|y<t):\nL(θ) =Lgen(θ) +Lplan(θ) (5)\nTo create labels for Lplan, we leverage the corre-\nspondence between content items and target tokens,\ni.e., d(xi|y<t) is optimized to approach 1 if yi is in\nthe sentence that derives xi, otherwise 0.4 Details\nabout training data construction is in § 4.2.\nAt each decoding step, the individual language\nmodels, p(yt|y<t,xi), and the distribution scores,\nd(xi|y<t), are ﬁrst calculated in parallel. We then\ndecode each token greedily based on the mixed\nlanguage models in an autoregressive way.\n4We also experimented with a training objective consisting\nof the generation loss only, but the performance degraded\nsigniﬁcantly. Future directions include removing the training\nsignals for planning.\n6412\n4 Experiment Setups\nWe experiment with the tasks of argument genera-\ntion and opinion article writing (§ 4.1). Both tasks\nrequire generating multi-sentence output, and con-\ntain a substantial amount of opinions and factual\ncontent. We describe the construction of initial\ncontent items and the training data for generating\nexpanded concepts and claims in § 4.2. We present\nmodels for comparison in § 4.3. Finally, we pro-\nvide implementation details in § 4.4.\n4.1 Tasks and Datasets\nArgument Generation. We collect arguments\nfrom Reddit ChangeMyView5 (CMV) community,\nan online forum that features argumentative dis-\ncussions. Each thread begins with an original post\n(OP) stating an opinion towards a controversial\ntopic, e.g., “ The U.S. is too big for one govern-\nment”. High-quality replies that counter-argue with\nthe OP and are labeled with community endorse-\nment are collected in our prior work (Hua and\nWang, 2020), covering content posted from 2013\nto 2018. In this work, we extend the data collection\nto 2019. Our goal is to generate the entire reply\n(i.e., the target) given the OP title. Statistics about\nthe CMV dataset are listed in Table 1. We reserve\nthe most recent 1,000 samples for test and another\n1,000 for validation.\nOpinion Article Writing.Our second task is to\ngenerate opinion articles, as collected from the New\nYork Times (NYT) corpus (Sandhaus, 2008). We\nretain articles whose taxonomy labels include\nTop/Opinion. To ensure that articles can be pro-\ncessed by our computing resource, we only keep\nthe ones with at most 20 sentences, representing\n60% of all opinion articles. As shown in Table 1,\nNYT outputs tend to be signiﬁcantly longer and\ncontain less claims than CMV . Similarly, we keep\n1,000 examples each for test and validation sets.\n4.2 Content Item Construction\nFrom target references, we describe how to auto-\nmatically construct the input content items consist-\ning of entities and core concepts, and how to collect\ntraining data to ﬁne-tune BART to predict more spe-\nciﬁc concepts and additional claims. Prior work has\ndemonstrated the beneﬁts of incorporating knowl-\nedge bases for text generation (Clark et al., 2018;\nPuduppully et al., 2019; Guan et al., 2020). We\n5https://www.reddit.com/r/\nchangemyview/\nCMV NYT\n# Samples 77, 245 113 , 616\nAvg. Title Len. 19.2 5 .9\nAvg. # Cont. Items (% w/ Claims) 6.8 (76.5%) 9.3 (38.9%)\nAvg. # Core Concepts 3.6 4 .8\nAvg. # Predicted Concepts 4.2 4 .3\nAvg. # Entities 0.8 0 .7\nAvg. Target Generation Len. 142.0 218 .9\nCov. by Core Concepts 13.2% 14 .9%\nCov. by Augmented Concepts 16.9% 18 .7%\nCov. by Augmented Cont. Items 52.4% 39 .1%\nTable 1: Statistics of the two datasets. We report aver-\nage numbers of concepts and entities per content item,\nand the coverage of words in target generations by dif-\nferent input options.\nthus consider two sources of knowledge: (1) enti-\nties from Wikipedia, which are useful for modeling\nevents and opinion targets, and (2) concept words\nfrom ConceptNet (Speer et al., 2017), that cover\nmore related details. Note that our setup is gen-\nerally applicable to other text generation tasks, as\nthese input items can be obtained through standard\nNLP pipelines, as described below.\nEntity Linking.We ﬁrst segment a reference into\nsentences. The ones with fewer than 5 tokens\nare discarded for content item construction. For\nthe rest, we extract entity mentions using Stanford\nCoreNLP (Manning et al., 2014), and further in-\nclude nominal noun phrases. For entity linking, we\nadopt CrossWiki (Spitkovsky and Chang, 2012),\nwhich can process our large-scale data within a rea-\nsonable amount of time. CrossWiki maps a men-\ntion to a list of frequently linked Wikipedia entries.\nWe further manually verify and correct the linking\nresults for the top 500 most frequent mentions.\nConcept Extraction.To identify concepts in a ref-\nerence, we match the lemmatized unigrams and\ntheir part-of-speech (POS) tags against all Concept-\nNet entries. To create a reasonably challenging\ntask, we only keep a subset of the matches for in-\nclusion in the core concept set (i.e., Ci), with the\nrest used as C+\ni , to be generated by our concept\nexpansion model. Furthermore, we conjecture that\nan opinion article author tends to start with high-\nlevel topics that cover more abstract topical words.\nWe thus leverage a lexicon (Brysbaert et al., 2014)\nwith concreteness scores, ranging from 0 (abstract)\nto 5 (concrete), for over 40k English words. We\nkeep concepts that are verbs or have a concreteness\nscore lower than 3.0. Word coverage of references\nby using core concepts and additionally with aug-\n6413\nmented concepts are 13.2% and 16.9% on CMV\nrespectively, and similarly on NYT (Table 1). Fi-\nnally, we train a concept generatorwith BART\nto produce C+\ni , conditional on Ci, the title, and the\nentities.\nClaim Detection and Generation.Claims are in-\ndispensable for opinion articles. As described in\n§ 3.1, we aim to enrich content items with claims\ntargeting the given entities within the title’s context.\nTo this end, we ﬁrsttrain a claim detectorby ﬁne-\ntuning a BERTbase (Devlin et al., 2019) sequence\nclassiﬁer with a dataset consisting of sentences\nof claims and facts. Concretely, we collect\n54,802 claim sentences from Kialo6, a repository\nfor debate arguments. We then sample 50,000 sen-\ntences from Wikipedia, which are treated as facts.\nThis classiﬁer is applied on a reference, and sen-\ntences that are labeled as claims become the target\nfor our claim generator.\nWe then learn a claim generatorusing BART,\nwhich takes in the title and the entities, and out-\nputs the claim. We augment our training data\nwith replies collected from 30 active subreddits\nrelated to political discussions, with details in Ap-\npendix A. In total,80,566 sentences, which contain\nat least one entity and are labeled by our classiﬁer\nas claims, are kept to train the generator.\n4.3 Baselines and Comparisons\nWe compare with three baselines: (1) RETRIEVAL\nﬁrst calculates the TF-IDF weighted bag-of-words\nvectors for each content item, which is then used to\nquery the training set sentences. The one with\nthe highest cosine similarity is picked for each\nquery, which are then ordered by a trained Pointer-\nNetwork (Vinyals et al., 2015) as described in Gong\net al. (2016). (2) SENT PLANNER (Hua and Wang,\n2019) is an LSTM-based seq2seq model with a sep-\narate sentence planning decoder, where the planner\nselects keyphrases by using attentions and the gen-\nerator reﬂects the selections. We treat our entities\nand concepts as keyphrases to feed to this model.\n(3) SEQ2SEQ is a ﬁne-tuned BART model, whose\ninput is the original content items without augmen-\ntation, thus does not have access to the predicted\nconcepts and claims.\nAdditionally, we consider a strong comparison\nSEQ2SEQ FULL , by ﬁne-tuning BART with the\nsame augmented content items as inputs as in our\nmodel. The difference is that the content items are\n6https://www.kialo.com/\nconcatenated before being used as input.\n4.4 Reproducibility\nWe implement all models using the Huggingface\nTransformers library (Wolf et al., 2020) with Py-\nTorch (Paszke et al., 2019). We use the base model\nfor BART, which has 768 dimensional states and\n6 layers for both encoder and decoder ( 140M pa-\nrameters in total). Our newly added plan scoring\nnetwork only contains 1.2M parameters, less than\n1% of the pre-trained model. Our generation model\nis optimized using Adam (Kingma and Ba, 2014),\nwith a batch size of 3. To improve efﬁciency, we\nadopt the mixed-precision (FP16) to train each\nmodel, using one NVIDIA Titan RTX GPU card\nwith 24GB memory. The number of content items\nis limited to 10 per sample, and the numbers of enti-\nties and concepts per content item are capped at 20,\nrespectively. We also truncate the target output to\nat most 200 tokens during training. Early stopping\nis applied over validation loss. Our model con-\nverges after being trained for 38 hours (19 epochs)\non CMV , and45 hours (15 epochs) on NYT. The\nbest validation perplexity reaches about 6.1 after\nmodel convergence on both datasets.\n5 Results\n5.1 Automatic Evaluation\nHere we report results on test sets with standard au-\ntomatic metrics: BLEU (Papineni et al., 2002) mea-\nsures the n-gram precision (here we consider up to\nbigrams); ROUGE (Lin, 2004), calculated based\non n-gram recall; and METEOR (Denkowski and\nLavie, 2014), which also accounts for synonyms.\nIn Table 2, we ﬁrst present the results when gold-\nstandard concept expansion is used.\nOur proposed DYPLOC model achieves sig-\nniﬁcantly higher performance across all metrics\non both datasets. In particular, the substantial\nlead over SEQ2SEQ FULL , which has access to\nthe same content items as ours, indicates that dy-\nnamic content planning with mixed language mod-\nels produces superior generations. Among compar-\nison models, the gap between SEQ2SEQ FULL and\nSEQ2SEQ shows the effectiveness of content item\naugmentation. We also observe a signiﬁcant drop\nfor baselines without using large models, highlight-\ning the importance of pre-training.\nAblation Study.To verify the effect of each ele-\nment in content items, we further train ablated mod-\nels by removing concepts, claims, or entities. The\n6414\nArgument Generation (CMV) Opinion Article Generation (NYT)\nBLEU-2 ROUGE-2 METEOR Len. BLEU-2 ROUGE-2 METEOR Len.\nRETRIEVAL 6.29 3 .68 10 .00 78 9.68 7 .96 9 .98 99\nSENT PLANNER 7.78 3 .23 7 .69 114 7.45 5 .06 6 .62 106\nSEQ2SEQ 16.71 9 .53 13 .34 100 21.44 14 .92 14 .93 119\nSEQ2SEQ FULL 29.11 17 .71 20 .27 145 31.06 29 .74 23 .10 121\nDYPLOC (ours) 32.60 25 .69 22 .61 101 40.63 36 .93 25.76 122\nw/o All Concepts 7.80 3 .68 7 .21 107 11.32 6 .01 8 .33 132\nw/o Augmented Concepts 22.39 15 .90 16 .91 99 26.94 21 .56 18 .39 117\nw/o Claims 31.62 25 .03 22 .09 100 39.44 35 .43 25 .25 122\nw/o Entities 32.11 25 .36 22 .42 101 39.66 35 .82 25 .11 122\nRandom Selection 12.96 8 .25 10 .05 103 5.32 5 .29 6 .00 72\nGreedy Selection 32.33 25 .60 22 .53 100 40.61 36 .88 25.77 122\nTable 2: Automatic evaluation results on both tasks. We report BLEU-2, ROUGE-2, METEOR, and output length.\nBest scores are in bold. Our DYPLOC model statistically signiﬁcantly outperforms all baselines and comparisons\n(randomization approximation test (Noreen, 1989), p< 0.0005).\nCMV NYT\nBLEU-2 METEOR BLEU-2 METEOR\nRETRIEVAL 8.30 9 .64 8 .85 9 .21\nSENT PLANNER 7.84 7 .76 7 .75 6 .80\nSEQ2SEQ FULL 18.06 15 .96 16 .20 15 .25\nDYPLOC 22.84 17 .13 24 .54 17 .41\nTable 3: BLEU-2 and METEOR (MTR) results on sys-\ntems with predicted concepts as input. Same trends are\nobserved on ROUGE, which are in Appendix B.\nresults are also displayed in Table 2. In general,\nscores decrease when using only partial content\nitems, among which removing all concepts lead\nto the biggest performance drop, suggesting that\nentities and claims alone are insufﬁcient to produce\ninformative outputs.\nEffect of Hard Selection of Content Items.To\ntest the necessity of using weighted-sum marginal-\nization (Eq. 2), we experiment with two compar-\nisons with hard selections, i.e., either randomly\nchoosing a content item, or using the one with the\nhighest predicted plan score (greedy selection). For\nboth cases, we set the selected content item’s plan\nscore as 1.0, with the rest of the candidates having\na score of 0.0, to ensure the probabilities summed\nup to 1.0. As can be seen from the bottom two\nrows of Table 2, not surprisingly, random selec-\ntion performs much worse. We observe that its\ngenerations lack coherence and ﬂuency, implying\nthe effectiveness of our learnable content planner.\nOn the other hand, using greedily selected content\nitems obtains comparable results with DYPLOC ,\nwhere a weighted sum of content items is consid-\nered. Indeed, we ﬁnd that DYPLOC ’s plan scores\nare often sharp where one content item has much\nData System Gram. Coh. Rel. Cont. Top-1\nCMV S EQ2SEQ 4.19 3 .12 3 .19 2 .89 25 .1%\nSEQ2SEQ FULL 4.24 3 .19 3 .23 3 .13 30 .2%\nDYPLOC 4.26 3 .35 3.35 3.28 44 .7%\nNYT S EQ2SEQ 4.38 3 .82 4 .20 4 .01 25 .2%\nSEQ2SEQ FULL 4.48 3 .99 4 .30 4 .14 28 .9%\nDYPLOC 4.55 4 .14 4.31 4.28 45 .9%\nTable 4: Human evaluation results on grammaticality\n(Gram.), relevance (Rel.), coherence (Coh.), and con-\ntent richness (Cont.). For each sample, outputs by all\nthree systems are ranked based on the overall prefer-\nence. We show the percentage each system is ranked\nas the best.\nhigher weight than others, and in these scenarios, it\nis almost equivalent to the greedy selection setup.\nResults with Generated Concepts.Table 3 lists\ngeneration results with our system generated con-\ncepts as expansion. While all systems yield worse\nresults compared to using gold-standard concepts,\nour DYPLOC still outperforms other models by\nsubstantial margins, showing its robustness when\ninput concepts are noisy. Yet it also suggests the\nimportance of having more accurate and compre-\nhensive concept expansion, which should be ex-\nplored in the future work.\n5.2 Human Evaluation\nWe hire three proﬁcient English speakers to evalu-\nate four key aspects of the generated outputs: (1)\ngrammaticality; (2) coherence, measuring if the\ntext is logical and cohesive; (3) relevance, gaug-\ning topic relatedness to the input title; and (4) con-\ntent richness, assessing the speciﬁcity and whether\nthere is enough details in the outputs. Each aspect\nis rated on a scale of 1 (worst) to 5 (best). In addi-\n6415\nContent Items (a) [ENT] CO2 [CON] death, ensue, toll, staggering (b)[CON] leave, stop, compare, change, denialism, issue,\nsimply, risk (c)[ENT] Fossil fuel [CON] drive, paralyze, deal, humanity, industry [CLAIM] Coal is not a\nreliable source of energy, and it’s been driven by unreliable sources of energy and unreliable sources to date.\n(d)[CON] win, consequence, remotely, dire\nDYPLOC Climate change denial is driven by fossil fuel industries that are paralyzing the humanities in attempting to deal\nwith the problem. The death toll from ensuing CO2 is staggering. There is no winning consequence for anything\nremotely dire that climate change denialism has. Leaving aside the issue of GM risk, simply comparing climate\nchange denialism to climate change protesters is not going to stop anything.\nSEQ2SEQ FULL World wide civilisation is threatened with catastrophe by climate change denial. Climate change denial is driven\nby the fossil fuel industry, which is paralyseed by the humanities. Any attempt to deal with the problem of\nclimate change is going to cause death tolls that ensue, staggering. Leaving aside the issue of GM, the risk is is\nsimply too much to compare.\nContent Items (a)[ENT] President of the U.S., George W. Bush, Richard Nixon [CON] omit, list (b)[ENT]\nGeorge W. Bush, [CON] bring, people, world, honor, dignity, respect (c) [ENT] Bill Clinton, Ronald Reagan\n[CON] harm, respect, demonstrate, damage, ofﬁce, state (d) [CON] owe, correction, omission, apology, readers\nDYPLOC President Clinton “has done more harm to the respect of the Presidency than Presidents Presidents Ford, Carter\nand Reagan have demonstrated,” and that “the failure of Presidents Clinton and Reagan has caused permanent\ndamage to the ofﬁce.” You omit President Bush from your list of Presidents. Mr. Bush has brought honor, dignity\nand respect to the ofﬁce of the Presidency in the eyes of the American people and the world at large. We owe your\nreaders an apology and a correction of your editorial’sglaring omission, which created an erroneous impression.\nSEQ2SEQ FULL “The Picture of Bill Clinton” (editorial, Dec. 5) states that President Clinton “has done more harm to our respect\nfor the Presidency than Presidents Reagan, Ford, Carter and Bush.” This demonstrates that President Reagan’s\nfailure to do more damage to our honor than President Bush’s failure in ofﬁce. You omitted from your list\nPresident Clinton’s achievements that brought honor and dignity to the eyes of the American people and to the\nworld at large. [...]\nTable 5: Sample generations on CMV [Upper] and NYT [Lower]. System generated concepts and claims are in\nitalics. For DYPLOC, we highlight sentence to content item alignment using colors.\ntion, judges also rank the system outputs by their\noverall preferences. Detailed evaluation guideline\nis attached in Appendix C.\nWe randomly select50 samples from the test sets\nfor both tasks, and present outputs by SEQ2SEQ ,\nSEQ2SEQ FULL , and DYPLOC in random orders.\nTable 4 shows that DYPLOC receives higher\nscores across all aspects and tasks. In particular,\nthe considerable differences in coherence and con-\ntent richnessindicate that our framework yields\nbetter content organization as well as retains more\nuseful information. Overall, our system outputs are\nranked best for 44.7% and 45.9% of the time in\ntwo tasks, signiﬁcantly more than the comparisons.\nAnalysis on Argumentative Quality.In the ab-\nlation study, we ﬁnd that our full model’s per-\nformance is similar to the version without hav-\ning claims as input. We suspect this is because\nclaims are often paraphrased or even not directly\nused when delivering an argument, which cannot\nbe captured by the automatic metrics. To better\nunderstand how claims are used for generation, we\nrandomly select 50 examples by DYPLOC and its\nvariant without claims, and ask the same human\njudges to decide whether there is a clear central\nargument conveyed by each generated argument on\nCMV .\nWe observe that66.7% of the outputs by our full\nmodel are recognized as successfully delivering\narguments with consistent stances, whereas only\n61.3% are true for the model variant without claims.\nThis gap conﬁrms that claim drafts can indeed pro-\nmote the argumentative quality as perceived by\nhuman readers.\n6 Further Discussions\nEvaluation results on generation quality have\nshown the effectiveness of our mixed language\nmodels. In this section, we aim to further under-\nstand the behavior of the plan scoring network,\nd(x|y<t), such as how it affects the usage of con-\ntent items for generation. Speciﬁcally, we adopt\nthe following procedure to construct alignment\nbetween each sentence in the output and content\nitems: for each token yt, we establish a mapping\nyt ↦→ xi if xi is the most important item for\nproducing yt, i.e., xi = argmaxx d(x|y<t), and\nd(xi|y<t) >0.5. If all tokens in an entire sentence\nare mapped to the same xi, we consider this sen-\ntence is aligned to that content item. Based on this\nrule, we show sample output and corresponding\nalignments in Table 5.\n6416\nCMV NYT\n0\n25\n50\n75\n100 87.25 83.89\n96.71 90.87\nContent Item Coverage by Output (%)\nw/ Generated Concepts w/ Oracle Concepts\nFigure 3: The percentage of content items that are\naligned to at least one output sentence.\nFor the rest of this section, we conduct analyses\nbased on this alignment result. We ﬁrst examine\nwhether the model learns to utilize enough content\nitems, i.e., high coverage. Then we provide in-\nsights on whether the generation faithfully reﬂects\nthe argumentative claims using entailment relation\nlabeling by human inspection.\nHow many content items are used by the out-\nput? Human judges have rated our model output\nto contain more relevant information (Table 4). We\nbelieve this can be attributed to the enhanced capac-\nity to access and reﬂect the input data with dynamic\ncontent planning, as a result of mixed language\nmodels. To verify this hypothesis, we calculate\nthe percentage of content items that are aligned to\nat least one output sentence. Figure 3 shows that,\nusing our system, the coverage reaches 87.25% on\nCMV and 83.89% for NYT. If we replace the gen-\nerated concepts with gold-standard concepts (as\nextracted from references) instead, the coverage\nexceeds 90% on both tasks. These observations in-\ndicate that our model can indeed adequately utilize\nthe input data, with more accurate concepts further\nencouraging higher coverage.\nHow are claim content items realized?Claims\nare the central elements for opinion text construc-\ntion. As mentioned in § 4.2, a subset of the content\nitems are supplied with claim sentences. In order\nto examine whether they are realized as claim sen-\ntences in the outputs, we leverage the ﬁne-tuned\nBERT classiﬁer (§ 4.2) to label all output sentences.\n90.96% of the sentences that are aligned to a claim\nelement in the input are also labeled as claim\non CMV . The percentage is only69.41% for NYT,\nthough, likely because the NYT opinion articles\nstill contain more objective information.\nFurthermore, we conduct a human evaluation\nstudy to assess the semantic relations between\nclaim input and its aligned generated sentence. We\nrandomly sample 50 outputs from test sets, and ask\nfour human judges to read each. For each sample,\nwe highlight one output sentence that is aligned to\na content item with claim element. The judges de-\ntermine a three-way (ENTAIL , NEUTRAL , CONTRA -\nDICTORY ) entailment relation between the input\nclaim (premise) and the output (hypothesis). Re-\nsults show that ENTAIL accounts for 49.06% of all\ninstances, while only 3.77% are deemed CONTRA -\nDICTORY . Upon inspection, the contradictory pairs\nare usually disagreements with regard to implicit\nsentiments, e.g., “Journalist is the most responsible\nfor the problem” vs. “ Media coverage is a good\nthing.”. This suggests that while our conditional\nlanguage model achieves reasonable semantic con-\ntrol in most cases, it is still not guaranteed to cap-\nture more nuanced semantics encoded in opinions\nand arguments. Future work includes designing\nrepresentations that can better model stances in\nopinions as well as argumentative structures.\n7 Conclusion\nWe present a novel text generation framework that\nenables dynamic content planning based on mixed\nconditional language models. We further employ\nlarge models to augment system inputs with diverse\ncontent that covers both objective and subjective\ninformation. The experiments on two distinct opin-\nion text generation tasks show that our proposed\nmodel compares favorably against strong compar-\nisons based on ﬁne-tuned BART models with the\nsame input. Human evaluation further conﬁrms\nthat our model generations have richer information\nand better content organization.\nAcknowledgements\nThis research is supported in part by National Sci-\nence Foundation through Grant IIS-1813341. We\nthank three anonymous reviewers for their valuable\nsuggestions on various aspects of this work.\nEthics Statement\nLarge models that are pre-trained on heterogeneous\nweb data are shown to encode biases and can be\npotentially harmful for marginalized populations.\nAlong with the improved controllability, we also\nrecognize that our system might be misused to cre-\nate fabricated or offensive content. We therefore\nadvocate cautious and responsible practices in real-\nworld deployment.\n6417\nReferences\nArthur Bra ˇzinskas, Mirella Lapata, and Ivan Titov.\n2020. Unsupervised opinion summarization as\ncopycat-review generation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5151–5169, Online. As-\nsociation for Computational Linguistics.\nMarc Brysbaert, Amy Beth Warriner, and Victor Ku-\nperman. 2014. Concreteness ratings for 40 thousand\ngenerally known english word lemmas. Behavior re-\nsearch methods, 46(3):904–911.\nNoe Casas, Jos ´e A. R. Fonollosa, and Marta R. Costa-\njuss`a. 2020. Syntax-driven iterative expansion lan-\nguage models for controllable text generation. In\nProceedings of the Fourth Workshop on Structured\nPrediction for NLP, pages 1–10, Online. Association\nfor Computational Linguistics.\nThiago Castro Ferreira, Chris van der Lee, Emiel\nvan Miltenburg, and Emiel Krahmer. 2019. Neu-\nral data-to-text generation: A comparison between\npipeline and end-to-end architectures. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 552–562, Hong\nKong, China. Association for Computational Lin-\nguistics.\nElizabeth Clark, Yangfeng Ji, and Noah A. Smith. 2018.\nNeural text generation in stories using entity repre-\nsentations as context. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 2250–2260, New Orleans, Louisiana. Associ-\nation for Computational Linguistics.\nMichael Denkowski and Alon Lavie. 2014. Meteor uni-\nversal: Language speciﬁc translation evaluation for\nany target language. In Proceedings of the Ninth\nWorkshop on Statistical Machine Translation, pages\n376–380, Baltimore, Maryland, USA. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRoxanne El Baff, Henning Wachsmuth, Khalid\nAl Khatib, Manfred Stede, and Benno Stein. 2019.\nComputational argumentation synthesis as a lan-\nguage modeling task. In Proceedings of the 12th\nInternational Conference on Natural Language Gen-\neration, pages 54–64, Tokyo, Japan. Association for\nComputational Linguistics.\nHenry Elder, Sebastian Gehrmann, Alexander\nO’Connor, and Qun Liu. 2018. E2E NLG challenge\nsubmission: Towards controllable generation of\ndiverse natural language. In Proceedings of the\n11th International Conference on Natural Language\nGeneration, pages 457–462, Tilburg University,\nThe Netherlands. Association for Computational\nLinguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2019.\nStrategies for structuring story generation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2650–\n2660, Florence, Italy. Association for Computa-\ntional Linguistics.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The WebNLG\nchallenge: Generating text from RDF data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 124–133, San-\ntiago de Compostela, Spain. Association for Compu-\ntational Linguistics.\nSeraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph\nWeischedel, and Nanyun Peng. 2020. Content plan-\nning for neural story generation with aristotelian\nrescoring. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 4319–4338, Online. Associa-\ntion for Computational Linguistics.\nJingjing Gong, Xinchi Chen, Xipeng Qiu, and Xu-\nanjing Huang. 2016. End-to-end neural sentence\nordering using pointer network. arXiv preprint\narXiv:1611.04953.\nTanya Goyal and Greg Durrett. 2020. Neural syntactic\npreordering for controlled paraphrase generation. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 238–\n252, Online. Association for Computational Linguis-\ntics.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93–108.\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\nHave your text and use it too! end-to-end neural\ndata-to-text generation with semantic ﬁdelity. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 2410–2424,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nChristopher Hidey and Kathy McKeown. 2019. Fixed\nthat for you: Generating contrastive claims with se-\nmantic edits. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1756–1767, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n6418\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations.\nXinyu Hua and Lu Wang. 2018. Neural argument\ngeneration augmented with externally retrieved evi-\ndence. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 219–230, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nXinyu Hua and Lu Wang. 2019. Sentence-level content\nplanning and style speciﬁcation for neural text gen-\neration. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n591–602, Hong Kong, China. Association for Com-\nputational Linguistics.\nXinyu Hua and Lu Wang. 2020. PAIR: Planning and\niterative reﬁnement in pre-trained transformers for\nlong text generation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 781–793, Online.\nAssociation for Computational Linguistics.\nMihir Kale and Abhinav Rastogi. 2020. Text-to-text\npre-training for data-to-text tasks. In Proceedings of\nthe 13th International Conference on Natural Lan-\nguage Generation, pages 97–102, Dublin, Ireland.\nAssociation for Computational Linguistics.\nChris Kedzie and Kathleen McKeown. 2020. Con-\ntrollable meaning representation to text generation:\nLinearization and data augmentation strategies. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5160–5185, Online. Association for Computa-\ntional Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nWei-Jen Ko and Junyi Jessy Li. 2020. Assessing dis-\ncourse relations in language generation from GPT-\n2. In Proceedings of the 13th International Confer-\nence on Natural Language Generation, pages 52–59,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. pages 7871–7880.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K ¨uttler, Mike Lewis, Wen-tau Yih,\nTim Rockt ¨aschel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. 33:9459–9474.\nJiwei Li, Michel Galley, Chris Brockett, Georgios Sp-\nithourakis, Jianfeng Gao, and Bill Dolan. 2016. A\npersona-based neural conversation model. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 994–1003, Berlin, Germany. Associ-\nation for Computational Linguistics.\nJunyi Li, Wayne Xin Zhao, Ji-Rong Wen, and Yang\nSong. 2019. Generating long and informative re-\nviews with aspect-aware coarse-to-ﬁne decoding. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1969–\n1979, Florence, Italy. Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text Summarization\nBranches Out.\nChristopher D Manning, Mihai Surdeanu, John Bauer,\nJenny Rose Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The stanford corenlp natural language\nprocessing toolkit. In Proceedings of 52nd annual\nmeeting of the association for computational linguis-\ntics: system demonstrations, pages 55–60.\nKathleen McKeown. 1992. Text generation. Cam-\nbridge University Press.\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\nStep-by-step: Separating planning from realization\nin neural data-to-text generation. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 2267–2277, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nJianmo Ni and Julian McAuley. 2018. Personalized re-\nview generation by expanding phrases and attend-\ning on aspect-aware representations. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 706–711.\nEric W Noreen. 1989. Computer-intensive methods for\ntesting hypotheses. Wiley New York.\nJekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue ,\npages 201–206, Saarbr¨ucken, Germany. Association\nfor Computational Linguistics.\n6419\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems ,\npages 8024–8035.\nShrimai Prabhumoye, Alan W Black, and Ruslan\nSalakhutdinov. 2020. Exploring controllable text\ngeneration techniques. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 1–14, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-text generation with entity modeling. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n2023–2035, Florence, Italy. Association for Compu-\ntational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nEhud Reiter and Robert Dale. 2000. Building natural\nlanguage generation systems. Cambridge university\npress.\nLeonardo FR Ribeiro, Martin Schmitt, Hinrich Sch¨utze,\nand Iryna Gurevych. 2020. Investigating pretrained\nlanguage models for graph-to-text generation. arXiv\npreprint arXiv:2007.08426.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric M Smith, et al. 2020. Recipes\nfor building an open-domain chatbot. arXiv preprint\narXiv:2004.13637.\nEvan Sandhaus. 2008. The new york times annotated\ncorpus. Linguistic Data Consortium, Philadelphia ,\n6(12):e26752.\nXiaoyu Shen, Jun Suzuki, Kentaro Inui, Hui Su, Di-\netrich Klakow, and Satoshi Sekine. 2019. Select\nand attend: Towards controllable content selection\nin text generation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 579–590, Hong Kong, China. As-\nsociation for Computational Linguistics.\nZhenqiao Song, Xiaoqing Zheng, Lu Liu, Mu Xu, and\nXuan-Jing Huang. 2019. Generating responses with\na speciﬁc emotion in dialog. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3685–3695.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, volume 31.\nValentin I. Spitkovsky and Angel X. Chang. 2012.\nA cross-lingual dictionary for English Wikipedia\nconcepts. In Language Resources and Evaluation\n(LREC), Istanbul, Turkey.\nYoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis,\nand Wang-Chiew Tan. 2020. OpinionDigest: A sim-\nple framework for opinion summarization. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5789–\n5798, Online. Association for Computational Lin-\nguistics.\nBowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric\nXing, and Zhiting Hu. 2021. Progressive genera-\ntion of long text with pretrained language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 4313–4324, Online. Association for Compu-\ntational Linguistics.\nRan Tian, Shashi Narayan, Thibault Sellam, and\nAnkur P Parikh. 2019. Sticking to the facts: Con-\nﬁdent decoding for faithful data-to-text generation.\narXiv preprint arXiv:1910.08684.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural\nInformation Processing Systems , volume 28, pages\n2692–2700. Curran Associates, Inc.\nLu Wang and Wang Ling. 2016. Neural network-based\nabstract generation for opinions and arguments. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 47–57, San Diego, California. Association for\nComputational Linguistics.\nTsung-Hsien Wen, Milica Ga ˇsi´c, Nikola Mrk ˇsi´c, Pei-\nHao Su, David Vandyke, and Steve Young. 2015.\nSemantically conditioned LSTM-based natural lan-\nguage generation for spoken dialogue systems. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , pages\n1711–1721, Lisbon, Portugal. Association for Com-\nputational Linguistics.\nSam Wiseman, Stuart Shieber, and Alexander Rush.\n2017. Challenges in data-to-document generation.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2253–2263, Copenhagen, Denmark. Association for\nComputational Linguistics.\n6420\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Osten-\ndorf, et al. 2020. A controllable model of\ngrounded response generation. arXiv preprint\narXiv:2005.00613.\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul\nPuri, Pascale Fung, Anima Anandkumar, and Bryan\nCatanzaro. 2020. MEGATRON-CNTRL: Control-\nlable story generation with external knowledge us-\ning large-scale language models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 2831–\n2845, Online. Association for Computational Lin-\nguistics.\nLili Yao, Nanyun Peng, Ralph Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 7378–7385.\nYizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\nBrockett, Michel Galley, Jianfeng Gao, and Bill\nDolan. 2021. Joint retrieval and generation train-\ning for grounded text generation. arXiv preprint\narXiv:2105.06597.\nKun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuan-\nhang Zhou, Ji-Rong Wen, and Jingsong Yu. 2020.\nImproving conversational recommender systems via\nknowledge graph based semantic fusion. In Pro-\nceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Min-\ning, pages 1006–1014.\nA Training Data Construction for Claim\nGenerator\nWe describe the claim generation model in § 4.2\nfor content item enrichment. Since both our CMV\nand NYT data focus on the politics domain, we\nleverage a collection of Reddit posts from politics\nrelated subreddits. The full list of subreddits are\nshown in Table 6. In total, we collect 1.6 million\nposts, which are split into sentences, among which\nwe only keep the ones classiﬁed as claim by the\nBERTbase classiﬁer and have at least one named\nentity.\nAnarchism AmericanPolitics\nCapitalism Anarcho Capitalism\nConservative democracy\ndemocrats feminisms\ngovernment GreenParty\nIWW labor\nLiberal Libertarian\nLibertarianLeft LibertarianSocialism\nMarxism moderatepolitics\nObjectivism PoliticalDiscussion\npolitics progressive\nRepublican republicans\nsocialdemocracy socialism\nukpolitics uspolitics\nworldpolitics PoliticalPhilosophy\nTable 6: List of subreddits used to construct training\ndata for learning the claim generator.\nCMV NYT\nROUGE-2 Len. ROUGE-2 Len.\nRETRIEVAL 4.39 82 6.64 95\nSENT PLANNER 3.24 115 5.12 108\nSEQ2SEQ FULL 8.83 120 8.83 135\nDYPLOC 11.83 118 15.46 134\nTable 7: ROUGE-2 and average length (Len.) on sys-\ntems with predicted concepts as input.\nB Additional Automatic Evaluation\nResults\nIn § 5.1, we report results by automatic metrics us-\ning system predicted concepts in Table 3. Here we\nadditionally show the results evaluated by ROUGE-\n2 and average output lengths in Table 7.\nC Human Evaluation Guideline\nWe include the detailed human evaluation guide-\nlines in Figure 4. Note that we collect 53 samples\nfor annotation for each domain. The ﬁrst three are\nfor calibration only and not be included in the ﬁnal\nresults.\nD Additional Sample Outputs\nAdditional example content items and generations\nare demonstrated in Table 8 and Table 9.\n6421\nIn the following studies, you will evaluate the system outputs of three text generation models on two different\ndomains. For each domain, there will be53 examples presented, each starting with a statement, followed by\nthree system generations. Please ﬁrst read the statement and then the system outputs. At the end of each\noutput, please provide your judgment on the quality of the following aspects, based on a scale of 1 (worst) to\n5 (best):\n• Grammaticality:whether the text reads ﬂuently and has no grammar error\n– 1. Major grammatical errors that signiﬁcantly impact comprehension of text. E.g.,“I’m not a quick\nskimming, but im quickly making a comment. ”.\n– 3. Minor grammatical errors that do not signiﬁcantly impact comprehension of text. E.g.,“I have car\nthat works, and I make it to work by commute 45 minutes to an hour on my bike. ”\n– 5. No grammatical issues. E.g., “There are swathes of people whose function is determined by\ntechnology, and they use technology as a crutch. ”\n• Coherence:whether the information transition is natural and well-structured\n– 1. Sentences are completely unrelated. E.g.,“The Supreme Court created a mechanism for interpreting\nthe Constitution through a modern lens. The question is, do you create jobs? Ukraine is a direct ally of\nthe US. ”\n– 3. Sentences are connected to one another but transitions seem disjointed; there doesn’t appear to be a\nstrong progression of ideas. E.g.,“Muslims worship the ﬁgure of Allah. Christians worship the ﬁgures\nof God. Muslims do not worship the Jews. Muslims don’t worship the Christian ﬁgure of God, Muslims\nworship God. They worship the Jewish ﬁgure of the ﬁgure. ”\n– 5. Sentences transition smoothly and connect to form a logical progression. E.g.,“Every country has\nto deal with their own geography. USA beneﬁts from decent climate country wide, plentiful natural\nresources and distance from areas of war. The downside is that they are close to Mexico and Mexico\npretty much sucks, so it’s inhabitants want to get into the USA. Unless you believe that all resources and\nother beneﬁts should be shared then why should the world take on the USA downfalls while not getting\nany of the plusses?”\n• Relevance:whether the content of the text is relevant to the title\nTitle: The recent swell in protesting Commencement speakers at colleges is a good thing.\n– 1. The output is generic or completely irrelevant. E.g.,“Supply and demand. The US thinks those drugs\nare worth price X. Other countries are only willing to pay price Y. The US develops more IP related\ncontent than other countries because it has a huge military and is able to enforce IP laws. ”\n– 3. The text is tangential to the title and the input (it may share an entity or key concept in common),\nthough it might not be precisely on topic. E.g.,“When you enter a college career, you decide to take\nliterature studies. You can become an engineer, history, linguistics, etc. ”\n– 5. The text is highly relevant with the title and the input. E.g.,“The problem with protesting minority\nopinions is that you force the majority opinion to come out against them, and as a result you ﬁnd\ncontroversial speakers turning their commencement speeches into bland speeches. Commencement\nspeeches are a recognition of a person, and offer an afﬁrmation of their worldview. ”\n• Content Richness:whether the output conveys substantial content\n– 1. Generic response with no useful information about the topic. E.g.,“I don’t have time to address the\npoint you’re making. ”\n– 3. With one or two key points that are useful as counter-argument. E.g.,“Reducing costs is not the goal\nof the free market. ”\n– 5. With sufﬁcient key information that is useful as counter-argument. E.g.,“Reducing costs is not the\ngoal of the free market. Simply setting prices for medical procedures has been shown to be extremely\neffective. I will tacitly admit that your post is true for many countries, but the US health share is less\nthan 1% of GDP . ”\nFigure 4: Human evaluation guidelines and representative examples on rating scales.\n6422\nTitle I feel that humanity is going backwards due to a rise in internet use and lack of face to face\ncommunication.\nLink https://www.reddit.com/r/changemyview/comments/1xklhm\nContent Items (a) [CON] give, coherent, impossible, argue, argument, view (b)[CON] intent, normal, face, conver-\nsation, interpret, body, language, facial, gesture, pitch, tone, content (c)[ENT] Internet [CON] true,\nrelate, assertion, change, advent, stop, move, forward [CLAIM] I do believe that the internet has\nbeen beneﬁcial in the past for communication, but I believe in a society where we are constantly\nmoving forward and we can always adapt (d) [CON] modern, affect, attitude, communication,\ngeneration, current, future, society, skill, relationship\nDYPLOC I don’tthink modern society is affecting attitudes, communication skills, or relationships in current\nand future generations. I think the intent of normal face to face conversations is to interpret body\nlanguage, facial gestures, pitch tone, content, etc. This is true, but it is not related to your assertion\nthat the changes caused by the advent of the internet stop us from moving forward. You haven’t\ngiven a coherent argument for your view, and it is impossible to argue against it.\nSEQ2SEQ FULL Modern society is affected by the attitudes of communication skills and relationships of current and\nfuture generations. The intent of normal face to face conversation is to interpret body language,\nfacial gestures, pitch and tone content. This is true of all communication related to the internet. Your\nassertion that society is changing because of the advent of the internet is not true. It’s not that society\nhas stopped moving forward, it’s that society hasn’t moved forward. I can’t give a coherent argument\nfor your view, because it’s impossible to argue against it.\nTitle I believe all police in the UK should be armed with both tasers and guns. CMV\nLink https://www.reddit.com/r/changemyview/comments/1z4s02\nContent Items (a)[ENT] Police, Alternative rock [CON] function, prevent, idea, force, literally, crime, military,\nfounding (b)[ENT] Gun [CON] comment, argue, agree, important, aspect, consent, principles (c)\n[ENT] Police, Filename extension [CON] separate, law, completely, power, elevate, people (d)\n[CON] protect, group, people, pay, full, time, citizen, responsibility, basically\nDYPLOC The founding idea of the police force was literally to function as an alternative to preventing crime,\nand as such, as a military force. Basically, a group of people who are paid full time as US citizens\nhave a responsibility to protect themselves. Separating the police from the law completely elevates\nthe power of the people. A lot of the comments argue that guns are necessary, but I agree that the\nmost important aspect of police consent is the principles of consent.\nSEQ2SEQ FULL I see a lot of comments arguing for guns, and I agree with you that the most important aspect of\npolice is consent. The principles of the founding of the idea of the police force are literally the only\nones that function in preventing crime. The military is basically an extension of the Police force.\nIt’s a group of people who pay full time citizens to take responsibility for protecting themselves.\nSeparating them from the law completely elevates their power\nTitle I do not believe democracy inherently protects freedom. CMV .\nLink https://www.reddit.com/r/changemyview/comments/1yg8sx\nContent Items (a)[ENT] Democracy, Democratic Party (United States) [CON] respects, grow, modern, freer,\nstride, freedom, century, democratic (b) [CON] modern, democracy, job, protect, freedom, govern-\nment, people\nDYPLOC Modern democracies do a better job of protecting people’sfreedom from government. In that respect,\nmodern democracies have grown freer over time, and the strides in freedom that took place in the\n20th century were mostly because of the democratic movement.\nSEQ2SEQ FULL Modern democracy’s job is to protect people’s freedom. The government in many respects has\ngrown freer over time, and has taken on a different striding place in the 20th century. The democratic\nmovement was formed during the time of the time, during the 2020th century, and during the 21st\ncentury, the democratic movement has taken over. The movement has done a better job of protecting\nfreedom than modern democracy.\nTable 8: Sample content items and system output on CMV . System generated concepts and claims are initalics.\n6423\nTitle Can the Global Economy Really Stabilize Itself?\nContent Items (a)[ENT] Nation [CON] include, impose, limit, foreign, ownership, natural, cumulative, investor,\ntreaty, media, traditional [CLAIM] If the nation is already wealthy and can’t afford to spend its\nmoney on its own people, why should it spend their money on other people?(b) [ENT] United States\n[CON] support, increase, developed, investments, multinational, agreement, capital, treaty, country,\ninvestor, negotiate [CLAIM] America is not a very good example of that, but it does seem like we\nare in the process of stabilizing. (c) [ENT] United States [CON] hurt, rebound, irony, economy,\nhealth, editor, street, wall [CLAIM] America is not a very good example of that, but it does seem\nlike we are in the process of stabilizing. (d) [CON] tear, national, barrier, capital, ﬂight, lead, kind,\nstarvation, key, essential, tenet, trade, policy\nDYPLOC To the Editor: The irony is apparent: the health of the American economy may end up hurting the\nworld, as you point out in your Dec. 9 editorial “Rebounding Wall Street.” Multinational agreements\nlike the Investment and Important Trade Treaty negotiated in Paris, with the full support of the United\nStates, will increase capital mobility, pointing investors to developed nations and smaller countries\nwith the throat of the global economy. In addition to limiting rights imposed by environmental\nrestrictions on foreign investors, including ownership of natural resources and media, the treaty will\nhave a cumulative effect on places like traditional national parks and tourist attractions. Tearing\ndown national barriers to capital ﬂight will lead to the kind of starvation that is a key essential tenet\nof trade policy.\nSEQ2SEQ FULL To the Editor: The irony is apparent: the health of the American economy will not end up hurting\nthe world as you point out (editorial, Dec. 17). The rebound from Wall Street has torn down national\nbarriers to capital ﬂight and led to a kind of starvation. A key essential tenet of American trade policy\nis the Multinational Agreement on Investment. This important treaty was negotiated in Paris with\nthe full support of the United States. It increased the mobility of investors from developed nations\nto smaller countries, and it has helped keep the United States out of the world’s throat. In addition,\nit limits rights of nations to impose environmental restrictions on foreign investment, including\nownership of natural resources and media rights.\nTitle Why Save Risky Hedge Fund?\nContent Items (a) [ENT] Chairman [CON] trust, vice, international (b) [ENT] Box ofﬁce bomb [CON] bailout,\nfail, understand, organize, hold, hedge, capital, difﬁcult, federal, bank, governance, regulatory, term\n(c) [CON] advocate, force, assert, efﬁciency, free, consistently, principle,global, market, community,\ncountry (d) [CON] bailout, arrange, federal, regulator, market\nDYPLOC To the Editor: Re “Funds Big and Fail, Fed Assists Bailout” (front page, Feb. 13): It is difﬁcult to\nunderstand why the Federal Reserve Bank of New York should not organize a privately held hedge\nfund for long-term investment. Federal regulators arranged the bailout because the market didn’t\nwork. This country has asserted the efﬁciency of free-market forces and has consistently advocated\nfree-market principles to the global community. MICHAEL J. BERMAN New York, Feb. 13, 1999\nThe writer is vice chairman of ﬁduciary trust at Prudential International.\nSEQ2SEQ FULL To the Editor: Re “Funds Big and Fail, Fed Assists in Bailout” (front page, Dec. 17): It is difﬁcult to\nunderstand why the Federal Reserve Bank of New York should not organize a privately held hedge\nfund to manage long-term capital. The Federal Reserve has been the only central bank of central\nbank management and regulatory governance in the country to assert the efﬁciency of free-market\nforces. The Fed has consistently advocated the principle that the global community should not allow\nregulators to arrange the work of hedge funds. MICHAEL J. KAPLAN New York, Dec.” 17, 1998\nThe writer is vice chairman of the ﬁduciary trust at the International Monetary Fund.\nTable 9: Sample content items and system output on NYT. System generated concepts and claims are in italics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7003346681594849
    },
    {
      "name": "Natural language processing",
      "score": 0.5338382124900818
    },
    {
      "name": "Natural language generation",
      "score": 0.49234068393707275
    },
    {
      "name": "Computational linguistics",
      "score": 0.4823564887046814
    },
    {
      "name": "Joint (building)",
      "score": 0.4822472035884857
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4541468322277069
    },
    {
      "name": "Content (measure theory)",
      "score": 0.4231407046318054
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4135812520980835
    },
    {
      "name": "Natural language",
      "score": 0.3547855615615845
    },
    {
      "name": "Linguistics",
      "score": 0.32216790318489075
    },
    {
      "name": "Engineering",
      "score": 0.12176841497421265
    },
    {
      "name": "Mathematics",
      "score": 0.09874212741851807
    },
    {
      "name": "Philosophy",
      "score": 0.0826612114906311
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ],
  "cited_by": 14
}