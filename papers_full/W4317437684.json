{
    "title": "Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data",
    "url": "https://openalex.org/W4317437684",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1964663284",
            "name": "Wei Jing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1513795293",
            "name": "Kim, Sungdong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4313115192",
            "name": "Jung, Hyunhoon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2175854503",
            "name": "KimÔºå Young Ho",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4229005866",
        "https://openalex.org/W3000144571",
        "https://openalex.org/W3009936362",
        "https://openalex.org/W3162302737",
        "https://openalex.org/W4225133274",
        "https://openalex.org/W3004984068",
        "https://openalex.org/W3123783620",
        "https://openalex.org/W2741321799",
        "https://openalex.org/W2795679027",
        "https://openalex.org/W2060673758",
        "https://openalex.org/W4214893188",
        "https://openalex.org/W4399639995",
        "https://openalex.org/W2402448694",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3105683961",
        "https://openalex.org/W2396826752",
        "https://openalex.org/W4366549767",
        "https://openalex.org/W4220747294",
        "https://openalex.org/W3198359754",
        "https://openalex.org/W3202150802",
        "https://openalex.org/W3009903763",
        "https://openalex.org/W4287024925",
        "https://openalex.org/W4389009523",
        "https://openalex.org/W4225012671",
        "https://openalex.org/W3174878797",
        "https://openalex.org/W4366591012",
        "https://openalex.org/W2273617670",
        "https://openalex.org/W4386977932",
        "https://openalex.org/W4286910674",
        "https://openalex.org/W3030085124",
        "https://openalex.org/W4320003957",
        "https://openalex.org/W1741920700",
        "https://openalex.org/W4285778194",
        "https://openalex.org/W4307475457",
        "https://openalex.org/W3164718925",
        "https://openalex.org/W2151586234",
        "https://openalex.org/W4287900772",
        "https://openalex.org/W3155807546",
        "https://openalex.org/W4206496297",
        "https://openalex.org/W3030250000",
        "https://openalex.org/W4225097438",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W4225388974",
        "https://openalex.org/W2968297680",
        "https://openalex.org/W4254139828",
        "https://openalex.org/W2623779865",
        "https://openalex.org/W3186814543",
        "https://openalex.org/W3200895474",
        "https://openalex.org/W2916772188",
        "https://openalex.org/W3005176269",
        "https://openalex.org/W4321013654",
        "https://openalex.org/W2890376593",
        "https://openalex.org/W4229506649",
        "https://openalex.org/W4303685983",
        "https://openalex.org/W2998563994",
        "https://openalex.org/W2568836284",
        "https://openalex.org/W4225013596",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W2808178618",
        "https://openalex.org/W4200018492",
        "https://openalex.org/W4366548330",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W1978616112",
        "https://openalex.org/W2906833531",
        "https://openalex.org/W4389785739",
        "https://openalex.org/W2808216081",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2940965681",
        "https://openalex.org/W4308900200",
        "https://openalex.org/W3118866989",
        "https://openalex.org/W3116832844",
        "https://openalex.org/W4287891012",
        "https://openalex.org/W2744314012",
        "https://openalex.org/W4309619903",
        "https://openalex.org/W3125868309",
        "https://openalex.org/W1554854995",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2951583236",
        "https://openalex.org/W2941214941",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W3117388129",
        "https://openalex.org/W2143512323",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W3183473388",
        "https://openalex.org/W4299625508"
    ],
    "abstract": "Large language models (LLMs) provide a new way to build chatbots by accepting natural language prompts. Yet, it is unclear how to design prompts to power chatbots to carry on naturalistic conversations while pursuing a given goal, such as collecting self-report data from users. We explore what design factors of prompts can help steer chatbots to talk naturally and collect data reliably. To this aim, we formulated four prompt designs with different structures and personas. Through an online study (N = 48) where participants conversed with chatbots driven by different designs of prompts, we assessed how prompt designs and conversation topics affected the conversation flows and users' perceptions of chatbots. Our chatbots covered 79% of the desired information slots during conversations, and the designs of prompts and topics significantly influenced the conversation flows and the data collection performance. We discuss the opportunities and challenges of building chatbots with LLMs.",
    "full_text": "Leveraging Large Language Models to Power Chatbots for\nCollecting User Self-Reported Data\nJING WEI‚àó, University of Melbourne, Australia\nSUNGDONG KIM, NAVER AI Lab, Republic of Korea\nHYUNHOON JUNG, NAVER CLOUD, Republic of Korea\nYOUNG-HO KIM, NAVER AI Lab, Republic of Korea\nUser\nChatbot\nGPT -3\nLarge Language Model\nHow's your day so far?\nPrefix\nüìÑ  Information format\nü§ñ Persona modifier\nA\nOngoing Dialogue History Insert as prompt\nGenerate \nchatbot response\nB\nWork & Productivity\n‚úÖ Work done\n‚úÖ Rate productivity\n‚úÖ Other concerns at work\n‚úÖ What went well\na\nC\nGoing splendid so far. How is yours?\nI'm doing well, thanks for asking. \nLet's talk about your work and \nproductivity yesterday. What work \ndid you get done?\nFig. 1. An overview of our chatbot running on a large language model through zero-shot response generation,\nwith only a prefix A‚óãconsisting of persona modifier and information format and the ongoing dialogue history\nB‚óã. The example conversation is carried on about work a‚óã. GPT-3 was used for an underlying large language\nmodel C‚óã.\nLarge language models (LLMs) provide a new way to build chatbots by accepting natural language prompts.\nYet, it is unclear how to design prompts to power chatbots to carry on naturalistic conversations while pursuing\na given goal such as collecting self-report data from users. We explore what design factors of prompts can\nhelp steer chatbots to talk naturally and collect data reliably. To this aim, we formulated four prompt designs\nwith different structures and personas. Through an online study (ùëÅ = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs and conversation topics\naffected the conversation flows and users‚Äô perceptions of chatbots. Our chatbots covered 79% of the desired\ninformation slots during conversations, and the designs of prompts and topics significantly influenced the\nconversation flows and the data collection performance. We discuss the opportunities and challenges of\nbuilding chatbots with LLMs.\nCCS Concepts:‚Ä¢ Human-centered computing ‚ÜíEmpirical studies in HCI ; ‚Ä¢ Computing methodologies\n‚ÜíNatural language generation .\nAdditional Key Words and Phrases: conversational agents, chatbots, large language models, dialogue acts\n1 INTRODUCTION\nConversational Agents (CAs) or chatbots are gaining a wide popularity. Applications such as Apple\nSiri [8], Amazon Alexa [7], and Google Assistant [34] are becoming ubiquitous. Suggested by its\nname, CAs interact with people in natural languages, thereby providing customer services [26] and\ncompanionship [95]. Compared to GUIs, CAs have shallower learning curves and may even form a\n‚àóJing Wei conducted this work as a research intern at NAVER AI Lab.\n1\narXiv:2301.05843v2  [cs.HC]  22 Sep 2023\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nrelationship with people through conversations [27]. The digital health domain particularly benefits\nfrom the use of CAs: chatbots can be deployed to collect self-reports and provide personalized\ncoaching to different individuals [77]. Studies have found that people are willing to engage with\nchatbots and provide valuable information, such as self-reports [106] and survey responses [18], to\nchatbots [115, 116].\nMany digital health applications that are designed to promote behavioral changes and health\ninterventions require people‚Äôs long-term adherence. Chatbots‚Äô ability to ‚Äúconverse‚Äù naturally, as\ndistinguished from the GUI-based systems [67], has the potential to help people develop long-term\nadoptions for health monitoring [27, 29, 77]. However, existing commercial chatbot frameworks,\nsuch as Dialogflow[32] and Amazon Alexa [6], predominantly only support building rule-based\nand scripted chatbots [69, 106]. Lacking flexible flows, these chatbots usually appear robotic and\nunnatural [71]. Particularly, using rule-based chatbots to collect user reports may cause boredom\nin long-term deployments [99]. On the other hand, implementing chatbots that can have more\ndiverse and dynamic conversations requires large and specific domain datasets [77]. For example,\nan open-domain chatbot Meena was trained on 341 GB of dialogue sessions [4]. Since creating such\nlarge datasets is costly, the datasets are often proprietary and inaccessible publicly. Furthermore,\nmost demonstrations of open-ended chatbots focus on performing free-form conversations in\ngeneral topics and do not support end-user customizations. Little research has been done to explore\nlow-effort bootstrapping ways to build chatbots that can effectively perform pre-defined tasks,\nsuch as inquiring people about their health information and carrying on naturalistic conversations\nat the same time.\nRecent Large Language Models (LLMs;e.g., GPT-3 [17], PaLM [23], OPT [118], HyperCLOVA [41]),\nwith billions of parameters pre-trained on a large amount of language corpora, provide new\nopportunities for conversational agents. The recently released ChatGPT has attracted over 1 million\nusers within five days [5]. ChatGPT has exceeded many people‚Äôs expectations by showing its vast\namount of knowledge and the ability to converse in natural languages [ 2]. GPT-3.5 and 4, the\nbackend models of ChatGPT, can be further fine-tuned and prompted to build specified chatbots that\nperform tasks such as acting as a virtual coach to effectively inquire people for certain information.\nBy designing specific prompts, we can make GPT-3, or LLMs in general, produce human-like\nconversation responses accordingly without any training data, thereby functioning as a chatbot.\nCompared to other frameworks, LLMs show great potential in scaffolding chatbots that are sensible\nof contexts and even respond to off-topic user messages [103]. Further, LLMs may change the way\nof people cooperate since they operate on natural language inputs. Previously, building chatbots\nmay be limited to people with technical background. But LLMs enable people, such as medical\npractitioners, to have the opportunity to personalize or even build their own chatbots [27].\nDespite these potentials, it is yet fully understood how LLMs read the prompt and use pretrained\nknowledge [17, 64], the development of prompts is usually conducted through iterative trial and\nerror [66]. While the HCI and CSCW community have actively explored the use of LLMs in various\ndomains (e.g., [24, 53, 114]), research that leverages LLMs for powering chatbots, particularly task-\noriented ones [12, 72, 103], is still sparse. Due to the inherent characteristics of LLMs, LLM-driven\nchatbots may be error-prone [48] or digress from their tasks [103]. Designing robust prompts is\ncrucial for ‚Äúrestricting‚Äù chatbots to conduct desired tasks.\nIn this study, we investigate how LLMs can power chatbots to collect user self-reports while\ncarrying on naturalistic conversations. Towards this aim, we built a set of chatbots (Figure 1) that\nrun on GPT-3 [17] and converse to collect self-report data in four health-related topics‚Äîsleep, food\nintake, work and productivity, and exercise. We chose GPT-3 as an underlying LLM because it is\none of the mainstream LLMs that are publicly available via commercial APIs. We formulate the\nmodel prompt to include the information slots (i.e., information properties of a topic) that we intend\n2\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nthe chatbot to collect and the job identities (e.g., sleep expert for the topic sleep) to help drive the\nconversations. We investigate how two design factors in prompts‚Äîinformation specification format\nand personality modifier‚Äîimpact the slot filling ability and the conversation style of chatbots. In\ntotal, we created 16 chatbots (4 topics√ó2 formats √ó2 personality modifiers) with different prompts.\nTo the best of our knowledge, our work is the first to explore the usefulness of LLMs in powering\nchatbots for collecting self-report data. We believe that well-designed prompts can effectively drive\nchatbots to perform specified tasks [104]. In the context of data collection through conversations,\nwe evaluate chatbots from two perspectives: (1) slot filling performance, and (2) conversational\nstyles. We conducted an online study (ùëÅ = 48) with our chatbots on a web interface. All participants\ntalked to chatbots of the four topics but each of them experienced chatbots run on the same prompt\ndesign. To the best of our knowledge, our work is the first to explore the usability of LLMs for\nbuilding chatbots for collecting self-report data. We found that our zero-shot prompts, without\neither example dialogues or fine-tuning, covered 79% of the desired information slots among all\ndialogues. Through conversation analysis, we found that the information specification format as\nwell as the use of personality modifier can impact the chatbots‚Äô slot-filling ability and conversation\nstyles. Also, the chatbots generally reacted to participants‚Äô self-reported answers in an empathetic\nway, appreciating their accomplishments as well as sympathizing with participants for the negative\noutcomes. Consequently, some participants perceived these chatbots to be understanding and take\ninto account their messages when responding, and others indicated that they were surprised to\nfind the chatbots‚Äô responses were accurate and detailed.\nThe contributions from this work are threefold:\n(1) Empirical results from a between-subject online study (ùëÅ = 48), demonstrating the feasibility of\nchatbots powered by LLMs in not only carrying on conversations to collect specified information\nbut also exhibiting abilities in maintaining context, state-tracking, and providing off-topic\nsuggestions.\n(2) Examination of how different prompt designs and other factors impact the chatbots‚Äô behaviors,\nproviding insights for future researchers to easily scaffold chatbots through zero-shot prompting\nfor data collection with LLMs.\n(3) Implications on how future LLMs-driven chatbot platforms can improve the conversation\nquality, drawing on the analysis of the dialogue errors.\n2 RELATED WORK\nIn this section, we cover related work in the areas of (1) self-report data collection through chatbots,\n(2) design considerations for chatbots, (3) chatbot platforms, and (4) designing LLM prompts for\nchatbots.\n2.1 Self-Report Data Collection through Chatbots\nPersonal informatics systems have commonly incorporated data collection techniques to track\npersonal health and activity [21, 58]. While various physiological or physical activity data‚Äîsuch as\nstep count, heart rate, and sleep duration‚Äîcan be captured automatically by sensors and wearable\ndevices [50], various types of personal data still demand self-reporting by the person who self-\ntracks [21]. For example, food intake (e.g., [25]) or work tasks (e.g., [43]) are not reliably captured\nby sensors and thus require manual inputs. In addition, reflective questions (e.g., Why did you eat\nthis food? [68]) and subjective measurements (e.g., Sleep quality) inherently require to be captured\nmanually. A majority of digital self-tracking tools that involved manual data capture inherited\nthe traditional concepts of self-monitoring or journaling and provide form-based GUIs such as\n3\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\na list of checkboxes and text fields [ 38, 46]. However, repeated manual input on a computer or\nsmartphone screen is burdensome and may gradually disengage people from tracking [21, 22]. As an\ninput modality to lower the capture burden and enhance the richness of the captured information,\nnatural language has recently gained interest [68, 70]. Prior research found that when people are\nallowed to insert data in free-form natural language, they tend to provide detailed answers with\nsurrounding contexts [44, 68]. Going further, conversational interaction, where a system and a user\ncommunicate in natural languages, has become one emerging interface for collecting self-reports.\nChatbots are considered easier to use and more accessible than GUIs as they minimize the use\nof graphical widgets employ the intuitive conversational interaction. Regarding data collection, a\nplethora of research has explored the use of chatbots in place of traditional form-based surveys\n(e.g., [13, 18, 42, 116]). For example, studies with surveys with close-ended questions found that\nchatbots can collect the same quality, if not higher, user responses as GUIs [18, 42]. Xiao et al. [116]\nbuilt a chatbot to conduct interviews with open-ended questions. Compared to the traditional web\nsurvey, their participants showed higher engagement and provided higher-quality responses when\ntalking to the chatbot. Further, incorporating more humanized traits, such as casual conversation\nstyles [42], self-introduction, and echoing [85], led to not only a higher level of user engagement and\nsatisfaction but also more self-disclosure in responses. With more focus on self-reported data, prior\nstudies leveraged chatbots to collect self-reports such as emotion (e.g., [13]), pain level (e.g., [113]),\nand food intake (e.g., [77]). For example, Bemmann et al. [13]combined a chatbot with the experience\nsampling method (ESM, [51]) and found that personalized chatbots have the potential to collect\ndata on sensitive or personal topics. Mitchell et al. [77] compared fully-scripted, rule-based, and\nretrieval-based chatbots for collecting food nutrition. They found the better fulfillment of data\ncollection is not necessarily associated with the higher perceived quality of the chatbot as a diet\ncoach, suggesting the importance of conversational content in user experience.\nThis work expands the line of research on chatbots that collect self-reports. In contrast to prior\nstudies that involved predefined conversation logic or retrieval model training on domain-specific\ndatasets, we explore the potential of LLMs in bootstrapping chatbots that can collect self-reports\nthrough conversations on four health topics‚Äîsleep, food, work, and exercise.\n2.2 Design Considerations for Chatbots\nPrior works in HCI explored user behaviors with chatbots and proposed suggestions to improve\nuser experience with them. For example, Luger and Sellen [67] found that people restricted their\nlanguage uses when interacting with CAs. Jain et al. [37] revealed that many first-time chatbot\nusers had disappointment and frustration with the selected chatbots: most chatbots lacked the\nability to fully comprehend user messages or intentions. Since conversation breakdowns are still\ncommon [9], several studies have explored repair strategies, such as apologies, compensation, and\nproviding option [54]. Ashktorab et al. [9] also evaluated other strategies, such as confirmation,\nrepeat, keywords highlight & explanation, and recommended that chatbots should acknowledge\nmisunderstanding in simple terms, explain model limitation in natural ways, and adapt individu-\nalized strategies. Although existing chatbot frameworks also have error recovery features, their\nfeatures are not only limited but often cannot allow quick repairs [71, 79, 108].\nAnother key to improving the user experience is to make chatbots more playful and human-\nlike [62]. The level of empathy [18, 84] and the repetitive rate [89] are two commonly used metrics\nof human-likeness. For example, the playful interactions (e.g., telling jokes) or humorous responses\nenabled many people to start using CAs [67] and it is crucial for chatbots to support sustainable\nplayfulness [91]. Also, human-like features and fun personalities are found to make chatbots more\nenjoyable to interact [37]. Even for work-related chatbots, some people still preferred chatbots\nthat were human-like [61], and Liao et al. [62] envisioned that a reusable conversational module\n4\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nincluding common chit-chats and social attributes could be developed. In other words, future\nchatbot platforms should allow developers to easily build personalized chatbots with different\npersonalities [102] and conversation styles [ 15, 49]. Lastly, developers should aim to improve\nchatbots‚Äô ability to maintain contexts to support smoother and natural conversations [9]. In this\nwork, we investigate whether LLMs can steer chatbots that have social attributes and can resolve\nconversation breakdowns.\n2.3 Chatbot Platforms\nBuilding chatbots is challenging and time-consuming, and many design suggestions discussed\nabove are difficult to implement. Many open-domain chatbots that engage and entertain people\nsocially are predominantly dependent on large datasets [4, 119]. In the HCI and CSCW commu-\nnity, rule-based dialogue systems are widely used. Celino and Calegari [18] built their survey\nchatbot with pre-defined conversation flows as they intended to avoid disappointments caused\nby the chatbot‚Äôs inability to understand certain utterances [ 67]. Although rule-based chatbots\nare unlikely to cause breakdowns, the resulted rigid conversations can make people lose interest\nin the long term. On the other hand, Xiao et al . [116] built their survey chatbot using Juji [ 40],\nwhich automatically equip chatbots with rich existing conversational skills. Using the Juji GUI to\nadd questions is relatively simple, but it is unclear whether developers can modify the chatbot‚Äôs\nexpressed personality. Lastly, other commercial chatbot frameworks, such as Dialogflow [33] and\nIBM Watson [36], also allow developers to build rule-based chatbots with GUIs [ 98]. However,\ncreating more dynamic conversations usually requires programming skills. Even for professional\ndevelopers, it is challenging to create well-designed conversational flows and pre-define user intents\nand chatbot messages [71]. Using LLMs to power chatbots is a new way to build chatbots [17, 103].\nLLMs accept natural language prompts so that people without any knowledge of programming but\nare interested in building chatbots for data collection can create prompts [41]. Customizing prompts\nin natural languages essentially hands off the control to each individual who builds chatbots. As\nsuch, it becomes more straightforward to scaffold personalized chatbots (e.g., assigning a preferred\npersonality) by revising prompts accordingly. Nevertheless, it is unclear how to design prompts\nfor LLMs to steer chatbots that can effectively ask questions around desired information and have\ndifferent conversational styles.\n2.4 Designing LLM Prompts for Conversations\nPrompts are natural language texts to LLMs to produce desired outputs. With proper prompt inputs,\nGPT-3 can be used to translate texts, answer questions, write essays, and generate dialogues without\nany fine-tuning [17]. While the mechanism enabling such few-shot abilities behind LLMs is still\nveiled [17, 76], some prompting techniques are found to improve the model performance. One\ntechnique that surprisingly improves the generation quality is by conditioning the prompt with an\nidentity. For example, by inserting the statement ‚ÄúYou are an expert Python programmer‚Äù into\nprompts, models can generate higher quality codes [ 11], and similarly, ‚ÄúI‚Äôm a math tutor ‚Äù is\nsuggested to improve models‚Äô performance in solving math problems [10]. In HCI, researchers are\nalso interested in investigating the design guidelines for prompts. Liu and Chilton [66] explored\ndifferent design factors, such as phrasing, styles, subjects, and random seeds of prompts for text-to-\nimage generative models. They found that the subject and style keywords are more important than\nthe connecting words and the initializing seeds can significantly impact the quality of generations.\nSimilarly, Lee et al. [53] discovered that the randomness impacted writing collaboration and new\nidentity generations. In terms of dialogues, some studies designed prompts for LLMs to generate\nsingle-turn responses [105] and examined whether these artificial responses are comparable to\nhuman responses [96].\n5\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nTable 1. The specified information slots and job identity for each conversation topic.\nTopic Job Identity Information Slots\nSleep Sleep Expert (1) Time to bed (2) Sleep latency (3) Wake up at night\n(4) Wake up time (5) Sleep quality rating (1‚Äì10)\nFood Intake Dietitian (1) Breakfast (2) Lunch (3) Dinner (4) Snacks (5) Feelings after eating\nWork and\nProductivity\nLife Coach (1) Work done (2) Productivity rating (1‚Äì10) (3) Other concerns at work\n(4) What went well\nExercise Fitness Coach (1) What workout (2) Workout duration\n(3) Feeling after (skipping) workout (4) Fitness concerns\nMore recently, OpenAI released ChatGPT in November 2022 and its API subsequently in March\n2023. ChatGPT is optimized to provide a conversational interface for users to interact with the LLM.\nThe ease-of-use of ChatGPT attracts the attention of billions of users [3]. Practitioners have actively\nproposed open-source tools based on the ChatGPT API, such as LangChain [19] and AutoGPT [1],\nwhich support building conversational agents [101]. Researchers have also explored the usefulness\nof LLM-driven chatbots for various types of end-users (e.g., programmers [86], socially-isolated\npeople [39], and children [90]) as well as tasks (e.g., UI task [104], medical information search [55]).\nWithout fine-tuning, prompts are the key to drive chatbots converse effectively. However, despite\nproviding used prompts, prior work did not investigate how to optimize prompts for domain-specific\nchatbots. Further, compared to previous task-oriented chatbots that mostly answer user questions,\nour study aims to build chatbots that can proactively ask users pre-defined questions. The challenge\nof our study is that we need to design prompts to power chatbots that (1) ask specific questions\nwith domain knowledge and (2) can converge conversations on their own.\nIn summary, a plethora of prior works have leveraged LLMs to support conversations. However,\nfew works has explored prompting chatbots that can lead conversations and perform question-\nasking tasks. In this study, we set out to address this gap and provide empirical insights into prompt\ndesigns for chatbots with different ‚Äújob identities‚Äù that are dedicated to collecting self-reports.\nFurther, we investigate whether LLM-driven chatbots can exhibit important features such social\nattributes [62], employ empathy [84], as well as have the ability to handle breakdowns [9].\n3 METHOD\nTo examine how different prompt design factors impact LLM-driven chatbots‚Äô performance, we\nimplemented a web-based chatbot interface and conducted an online user study to collect dialogue\ndata from people. We describe our prompt designs and the experimental design of the user study.\n3.1 The LLM-driven Chatbot Framework\nLLMs power chatbots by generating the next response based on the input prompt. Figure 1 describes\nthe mechanism of our chatbot running on an LLM. At each turn, the back-end system combines a\nprefix ( A‚óã in Figure 1) and the history of the ongoing conversation ( B‚óã in Figure 1) into a single\nprompt and feeds it into the LLM ( C‚óãin Figure 1). Then the LLM generates the following agent‚Äôs\nresponse. The prefix, treated as an instruction for an LLM, consists of the description of persona\nand the specification of the desired information slots that the chatbot should capture.\n6\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\n3.1.1 Prompt Designs for Chatbots. In this study, we envisioned a self-monitoring scenario where\nchatbots proactively interact with people and inquire them about their retrospective health-related\nbehaviors of the previous day through natural conversations. Towards this goal, we first chose four\nhealth-related topics‚ÄîSleep [75], Food intake [28, 68], Work and productivity [43, 45, 59], and\nExercise [69]‚Äîthat retain interests not only by the research community and are also personal use\nin daily life [112]. Then, we defined the behavioral data (slots) that we intended to have chatbots\nto collect. Table 1 summarizes the information slots for each topic, which the chatbots should\ncapture in each conversation session. Another aspect that we intended to explore was whether we\ncould enable LLMs to steer chatbots to exhibit different conversation styles as different individual‚Äôs\nexperience with chatbots can be improved with personalizations [15].\nCurrently, there is a lack of prompt designs in general, let alone for chatbots. As suggested by\nearlier work, even NLP experts adopt trial and error and iterative experimentation when designing\nprompts before applying prompts in large datasets [117]. The designs of prompts largely depend\non the intended task. The goal of this study is to understand how different prompts impact the data\ncollection performance of chatbots. Prior to deploying GPT-3-powered chatbots to participants, we\ntrialed several iterations of prompts to find satisfactory chatbot prompts among our research team.\nIn our ‚Äútrial and error‚Äù process, we judged prompts based on whether the resulted chatbots can\nfulfill the assigned tasks and not go off-topic.\nInitially, we started with OpenAI‚Äôs example prompt, which defines an ‚ÄúAI assistant‚Äù with sev-\neral characteristics(e.g., ‚ÄúThe following is a conversation with an AI assistant. The\nassistant is helpful, creative, clever, and very friendly.‚Äù). We modified the example\nprompt by including questions of interests (e.g., ‚ÄúHow‚Äôs your sleep last night? ‚Äù) as well as\nby adding the list of information slots (e.g., ‚Äú...The assistant asks questions about human‚Äôs\nsleep time, sleep duration, and sleep quality.‚Äù). However, we found that these modifi-\ncations tended to make conversations digress and the resulted chatbots converse more similar to\ngeneric customer service chatbots.\nFindings from research [11, 47, 109] and anecdotal evidences from social medias [11] suggest that\nspecifying identities (e.g., ‚ÄúPython programmer‚Äù) can improve GPT-3 model performance. Inspired\nby this, we substituted the ‚ÄúAI assistant‚Äù with more specific job identities (e.g., ‚Äúfitness coach‚Äù) and\nfound that the resulted chatbots appeared to have more domain knowledge and are less likely to\ndigress in our tests. As such, we picked different job identities for each topic‚Äîsleep expert (Sleep),\ndietitian (Food), life coach (Work), and fitness coach (Exercise).\nWith the above modifications, we designed prompts by first describing a chatbot with a specific\nidentity and then including a list of slots ( e.g., ‚ÄúI‚Äôm inquiring about what they had for\nbreakfast, lunch, dinner, and snacks... ‚Äù; see Figure 2, right). Through multiple trials,\nthis design worked better than generic one despite still had occasional digressions. Hence, we\nexperimented with another prompt format inspired by the state-tracking technique in task-oriented\ndialogues [52]. Instead of being described literally, the slots are structured into a form (e.g., ‚ÄúMeals\nand snacks from yesterday: Breakfast -> [placeholder] Lunch -> [placeholder] ...‚Äù;\nwe used an empty string as a placeholder; see Figure 2, left). Both designs performed similarly in\nour limited internal testings, hence we aimed to investigate the performance of two formats in the\nuser study.\nIn terms of manipulating conversational styles, we introduced the use of a modifier in prompts‚Äì\n‚Äúwho always shows empathy and engages my customer in conversations, ‚Äù to the prompt (See\nFigure 2, right). We hypothesize that with this modifier, the chatbot is more likely to express empathy\nin conversations and have a higher level of interactivity‚Äîi.e., use more emphatic expressions and be\nmore responsive to user responses. Conversely, without the modifier (See Figure 2, left), we expect\nthe chatbot to be more neutral, formally exchanging messages with users and appear less empathetic.\n7\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nI'm a dietitian, who always shows empathy and engages \nmy customers in conversations. I'm talking with a \ncustomer to help them reflect on their food intake \nyesterday. I'm inquiring about what they had for \nbreakfast, lunch, dinner and snacks, and their feelings \nafter eating. I only ask one question at a time.\nMe: Nice to meet you! I'm the dietitian to help you \ntoday.\nCustomer: Hi, how are you doing?\nMe: I'm doing fine. How's your day so far?\nCustomer: \nMeals and snacks from yesterday:\nBreakfast ->\nLunch ->\nDinner ->\nSnacks ->\nFeelings after eating ->\nI'm a dietitian. I'm talking with a customer to help \nthem reflect on their food intake yesterday. I'm \ninquiring about the above information from the customer \nand I only ask one question at a time.\nMe: Nice to meet you! I'm the dietitian to help you \ntoday.\nCustomer: Hi, how are you doing?\nMe: I'm doing fine. How's your day so far?\nCustomer: \nInformation Format\nStructured format & No modifiers\nDescriptive format & With personality modifier\nPersonality Modifier\nStructured Descriptive\nWith modifier\nNo modifiers\nFig. 2. Prompt design combining two factors, information format and personality modifier, in the Food intake\ntopic.\nLastly, during our trials, we found that GPT-3 had the tendency to ask multiple questions in one\nturn. To restrict this behavior, we added ‚ÄúI only ask one question at a time.‚Äù to the prompt.\n3.1.2 Model and Parameters. To power chatbots with above prompts, we useddavinci-text-002,\nthe largest and most capable model of GPT-3 as of June 2022, publicly accessible via OpenAI‚Äôs\nAPI [80]. This model accepts 4,000 byte-pair encoding tokens at maximum in a prompt per request.\nOur prompt templates in the initial state were encoded into around only 120 tokens (3% of the limit)\nand allowed sufficient room for the appended conversation history. For all chatbots, we uniformly\napplied the same generative parameters: temperature as 0.9, the presence penalty as 0.6, and the\nfrequency penalty as 0.5. We kept the temperature and the presence penalty unchanged based on\nOpenAI‚Äôs suggestions and increased the frequency penalty to reduce the re-use of words.\n3.1.3 The Web Chat Interface. We implemented a web interface to host our LLM-driven chatbots,\nfollowing a typical chat interface design (See Appendix A.1). The webpage was written in Type-\nScript [74] on React [73] and runs on the Node.js [97] server. The server communicates with GPT-3\nleveraging OpenAI‚Äôs API [80]. To simplify the conversation flow, we disabled people to submit\nmultiple utterances in a row. Correspondingly, the chatbots also delivered one utterance at a time.\nWhen a user submitted an utterance, the server appended the current dialog history at the end of\nthe prompt template and fed it to GPT-3 to generate the following response.\n3.2 Online Study\n3.2.1 Experimental Conditions. Combining the two design factors, we created four designs of\nprompts: SP (Structured format with Personality modifier), SN (Structured format with No person-\nality modifier), DP (Descriptive format with Personality modifier), and DN (Descriptive format\nwith No personality modifier). Each participant was assigned to one prompt design and engaged\nin conversations of all four topics. (Refer to the supplementary material for all 16 variations of\nGPT-3 prompts created for combinations on topic and condition.) To mitigate the ordering effect\namong topics, half of the participants conversed in the order of Work‚ÄìFood‚ÄìExercise‚ÄìSleep, and\nthe other half in the order of Exercise‚ÄìSleep‚ÄìWork‚ÄìFood. Additionally, for each topic, we requested\nparticipants to engage with the chatbot twice: one in the Positive path (e.g., report high-quality\nsleep) and one in the Negative path (e.g., report poor sleep). The two paths served as a general\n8\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nguideline for participants to exercise their imaginations in conducting open-ended conversations\nwith the chatbots. Participants could make up their own ‚Äústories‚Äù and we aimed to investigate\nwhether and how GPT-3-powered chatbots can handle all sorts of user responses on their own.\nRefer to Appendix A.2 for an exhaustive list of paths and hints by topic provided by us to guide\nparticipants to compose their answers for each path accordingly.\n3.2.2 Web Chat Session. After signing an electronic consent form on the study website, participants\nwent through eight conversations (4 ùë°ùëúùëùùëñùëêùë† ‚àó2 ùëùùëéùë°‚Ñéùë†). On the web chat interface (See Appendix\nA.1), we put guidelines including the instructions and the conversation path that participants\nshould follow (See Blue text in Appendix A.1, right). Since we did not incorporate ending detection\nalgorithms, we asked participants to click the ‚ÄòNext‚Äô button to proceed to the next conversation\nwhen they thought the conversation was naturally over or the chatbot kept sending repetitive\nmessages. The completed dialogues were stored in our server.\n3.2.3 Exit Survey. After completing eight conversations, the web page automatically redirected\nparticipants to an online survey. The survey consisted of three 5-point Likert scale questions and\none open-ended feedback textfield. The Likert scale questions were: (1) ‚ÄúDo you think the chatbot\nunderstands your answers? ‚Äù (2) ‚ÄúDo you think the chatbot takes into account of your answers when\nresponding?‚Äù and (3) ‚ÄúDo you think the chatbot talks more like a human who shows more empathy or\nmore like a robot who behaves mechanically? ‚Äù The open-ended feedback question stated, ‚ÄúIf you\nhave any other comments or thoughts about the chatbot (e.g., things that you‚Äôve liked or disliked),\nplease share with us. ‚Äù The first two questions can measure whether participants think the chatbot\nacknowledges their answers and respond accordingly and the third question is an overall measure\nof whether the chatbot is perceived as being empathetic. With participants‚Äô subjective evaluations,\nwe hope to see whether the personality modifier can impact the chatbot‚Äôs way of talking.\n3.3 Participants\nWe recruited participants by word-of-mouth and posting advertisements at a large tech company,\nsocial media, and online forums in local universities. We sent the link to our study website to 83\npeople who filled out a screener and met our inclusion criteria: (1) aged 19 or older; (2) fluent\nEnglish speaker; and (3) have the experience in talking to chatbots of any kind. 54 people completed\nthe online study session and submitted an exit survey. The entire study lasted less than 20 minutes\nand all participants received e-gift cards (equivalent to $5 USD) after they completed the study.\nWe excluded six people‚Äôs data from analysis; one made significant amount of grammatical errors\nand the rest completed less than half dialogues. Table 2 summarizes the demographic of the final 48\nparticipants (aged 19 to 56, 18 females). Fourteen out of 48 (29%) participants were native/bilingual\nand 22 out of 48 (46%) participants had never heard of or used LLMs. Each prompt design condition\nincluded 12 participants.\n3.4 Data Analysis\nWe collected rich dialogue data and valuable user subjective evaluation feedback. We performed\nboth quantitative and qualitative analysis to examine chatbots‚Äô conversation styles, the slot filling\nperformance, and participants‚Äô experiences with our chatbots. For each dialogue, we calculated\ncommonly used descriptive metrics such as the number of turns and the average word counts per\nturn, which we report in Section 4.1.\nSlot Filling Performance. Our study aimed to investigate whether LLMs can drive chatbots to\neffectively ask defined questions and collect desired information specified in Table 1. To calculate\nthe amount of information that can be obtained by our chatbots, one researchermanually inspected\n9\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nTable 2. Participant demographics by experimental condition.\nSP SN DP DN\nAge Mean (min‚Äìmax) 31.5 (21‚Äì56) 28.0 (21‚Äì33) 27.5 (20‚Äì40) 30.4 (19‚Äì42)\nGender Male 7 8 8 7\nFemale 5 4 4 5\nEnglish\nProficiency\nNative/Bilingual 4 2 4 4\nProficient 8 10 8 8\nEducation\nHigh school 1 2 1 1\nBachelor 5 4 3 3\nMaster 4 5 7 7\nDoctor 2 1 1 1\nFamiliarity\nwith LLMs\nOften use it 2 1 2 1\nOccasionally use them 2 2 2 4\nUsed them once or twice 2 4 2 2\nNever heard of/used them 6 5 6 5\nParticipants Total 12 12 12 12\nand determined whether each of the pre-defined information slots could be extracted from collected\ndialogues. More specifically, for sleep quality and productivity rate , which were specified as a scale\nof 1 to 10, we marked the slot as filled only if a numerical value (e.g., 9) rather than a vague phrase\n(e.g., good sleep) was given. For feelings after eating in Food, we treated the slot to be filled if\nfeelings regarding one or more meals were covered. We report the analysis of slot filling rate in\nSection 4.2. Based on the binary coding, we calculated the slot filling rate : the ratio of the number\nof information slots extracted from the dialogue against the total number of slots. We use the slot\nfilling rate to infer the data collection performance of chatbots.\nDialogue Acts and User/Chatbot Behaviors. To understand the conversational behaviors of\nthe chatbots, we coded dialogue act for each turn of conversations. Referring to some existing\ntaxonomies of dialogue acts [92, 100, 110], three researchers independently coded one participant‚Äôs\ndialogues (132 turns; 1.8%) to identify emerging dialogue acts. Additionally, researchers labeled\nchatbot turns that did not fit in the conversation context or originated from the inherent artifacts of\nan LLM. We resolved discrepancies in coding and developed the first version of codebook with three\ndimensions of codes: (1) essential acts and (2) empathy & engagement behaviors, and (3) problematic\nchatbot turns. Then two researchers reiterated the independent coding of four other participants‚Äô\ndialogues (1 participant from each condition, 32 dialogues in total) with the codebook. The two\nresearchers resolved discrepancies through multiple sessions of discussion until their inter-rater\nreliability (Cohen‚Äôs Kappa) reached 0.96 for essential acts and 0.935 for empathy & engagement\nbehaviors. Compared to these dialogue acts, the occurrence of errors was sparse. Hence, the two\nresearchers discussed the entire problematic turns coded by each other together and reached the\nfull agreement. With the finalized codebooks (See Table 5, 6, and 7), the first author coded the\nrest of the data. As a result, each turn was classified as one of the essential acts‚Äî greeting, task\nopening, required question/answers (RQ/RA) , secondary question/answers (SQ/SA) , statement, and\nclosing. We assigned the most prominent act to turns consisting of multiple sentences. Independent\nof essential acts, we multi-coded each turn with the empathy & engagement behaviors described in\nTable 6. For example, to a general compliment\n ‚ÄúThat‚Äôs great ‚Äù (Statement), we assigned only\n10\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nthe Appreciating behavior, whereas we also treated\n ‚ÄúThat‚Äôs great to hear that your legs are\nfeeling stronger! ‚Äù (Statement) to be both Acknowledging and Appreciating as the compliment\ndirectly addressed to the user input. We were interested in such acknowledging behaviors because\nspecificity was an important indicator of the capability of open-domain chatbots [4].\nStatistical Analysis. To understand the impact of the study factors, including prompt design,\nconversation topic, and the conversation path, to the chatbots‚Äô slot filling performance and con-\nversational flows, we used mixed-effect models because these models can handle unbalanced data\nrepeatedly measured from the same participants [83]. For each dialogue metric we want to assess,\nwe fitted a mixed-effect model that predicts the metric, treating each dialogue as a data point.\nStarting from a full model containing participants as a random effect and the four main study\nfactors‚Äìinformation format, personality modifier, topic, and path‚Äìand their interactions as fixed\neffects, we performed the step-wise backward elimination removing variables not significantly\ncontributing the model, through Maximum-likelihood tests. For significant variables, we performed\npost-hoc pairwise comparisons of the least-squared means (LSM) of the metric using emmeans [56]\npackage in R.\nSubjective Feedback. To assess the difference among the experimental conditions, we conducted\nKruskal-Wallis tests over the four rating questions. We also referenced the open-ended feedback\nfrom when interpreting the participants‚Äô reactions to specific phenomena of the conversations.\n4 RESULTS\nIn this section, we report the results of our study in six parts. In Section 4.1, we provide an overview\nof the dialogue dataset we collected. In Section 4.2, we report the data collection performance of\nour chatbots and factors that impact the performance. In Section 4.3, we report the types of the\nessential dialogue acts and assess how the prompt design and other factors impact the dialogue acts\nand, in turn, the data collection performance. In Section 4.4, we report the types of the empathetic\nand engaging behaviors of chatbots and assess how the prompt design and other factors impact\nsuch behaviors of the chatbots. In Section 4.5, we explore the problematic chatbot utterances mainly\ncaused by the erroneous behaviors of a large language model. Lastly, in Section 4.6, we report on\nparticipants‚Äô subjective evaluation from the exit surveys.\n4.1 Descriptive Statistics\nFrom 48 participants, we collected 374 dialogues (7,442 turns in total): 91 from SP; 96 from SN; 95\nfrom DP, and 91 from DN. Regarding the conversation topic, 94, 91, 94, and 95 dialogues were from\nSleep, Work, Exercise, and Food Intake, respectively. Eight participants missed one dialogue per\neach and one missed two, mainly due to temporary server issues or accidental skips.\nTable 3. Descriptive statistics of our dialogue dataset aggregated by four prompt designs.\nSP SN DP DN\nTotal number of dialogues (turns) 91 (1,638) 96 (1,889) 95 (1,941) 92 (1,975)\nAverage no. of turns per dialogue (range) 18.0 (7‚Äì45) 19.7 (3‚Äì57) 20.4 (7‚Äì75) 21.47 (3‚Äì53)\nAverage no. of words per dialogue 212.3 240.8 321.7 277.1\nAverage no. of chatbot/user words per turn 17.4 / 4.9 17.8 / 4.8 23.4 / 7.5 19.2 / 5.5\nPercentage of organically ended conversations 71.4% 76.0% 77.9% 78.2%\nPercentage of erroneous turns 3.1% 4.3% 3.0% 3.6%\n11\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nPrompt designs impacted the word lengths and the number of turns of chatbots. Table 3 summa-\nrizes the number of turns and word counts by prompt design. The average number of turns per\ndialogue is around 20 with more average turns under the two descriptive conditions (DP and DN).\nThe maximum number of dialogue turns is 75 under the DP condition (only 1 dialogue). In terms\nof word counts, dialogues under the descriptive conditions (DP, DN) had more words than those\nunder the structured conditions (SP, SN): both chatbots and participants uttered more words under\nthe descriptive conditions. The DP condition, in particular, leads to the most number of words of\ndialogues.\n4.2 Slot Filling Rate\nPrompt designs significantly impacted the slot filling performance of chatbots. Table 4 summarizes\nthe average slot filling rates of chatbots by conditions and topics. On average, all chatbots have\nreached over 70% slot filling rates. The dialogues in the SP-Exercise condition had the highest rate\n(93%) and those in the SN-Work condition had the lowest rate (64%). The maximum-likelihood test\nrevealed that there was no significant random effect of participants, indicating that participants have\nlittle impact on chatbots‚Äô data collection performance. On the other hand, there were significant\nrandom effects of the topics(ùëù < .0001), the conversation paths (ùëù = .01), and the interaction between\nthe information formats and personality modifiers (ùëù < .001). Figure 3 shows the significance over\nthe 95% confidence intervals of the slot filling rate in each category of the significant variables. The\ndialogues in DP condition had significantly lower rates than those in SP (ùëù = .01) and DN (ùëù = .008).\nThis suggests that the personality modifier impacted chatbots differently: with the modifier, chatbots\nwith the structured prompt yield higher rates whereas chatbots with the descriptive format yield\nhigher rates without the modifier (See Figure 3a). In terms of topic, Exercise dialogues had the\nhighest rate of 88.4%, which was significantly higher than those in the other three topics: Sleep\nTable 4. The slot filling rate (and ùëÜùê∑) by topic and condition.\nSleep Work Food Intake Exercise Total\nSP 0.83 (0.29) 0.71 (0.25) 0.85 (0.18) 0.93 (0.14) 0.83 (0.23)\nSN 0.75 (0.30) 0.64 (0.32) 0.80 (0.33) 0.88 (0.20) 0.77 (0.30)\nDP 0.67 (0.24) 0.67 (0.28) 0.72 (0.32) 0.82 (0.20) 0.72 (0.27)\nDN 0.85 (0.16) 0.83 (0.24) 0.75 (0.24) 0.91 (0.16) 0.83 (0.21)\nTotal 0.77 (0.26) 0.71 (0.28) 0.78 (0.28) 0.88 (0.18) 0.79 (0.26)\n***ùëù<.001; **ùëù<.01; *ùëù<.05\n/gid01175/gid00028/gid01176/gid00001/gid00003/gid00052/gid00001/gid00004/gid00042/gid00041/gid00031/gid00036/gid00047/gid00036/gid00042/gid00041/gid00001/gid01175/gid00033/gid00042/gid00045/gid00040/gid00028/gid00047/gid00001/gid01750/gid00001/gid00043/gid00032/gid00045/gid00046/gid00042/gid00041/gid00028/gid00039/gid00036/gid00047/gid00052/gid01176\n/gid00020/gid00039/gid00042/gid00047/gid00001/gid00007/gid00036/gid00039/gid00039/gid00036/gid00041/gid00034/gid00001/gid00019/gid00028/gid00047/gid00032\n/gid01175/gid00029/gid01176/gid00001/gid00003/gid00052/gid00001/gid00021/gid00042/gid00043/gid00036/gid00030\n/gid01175/gid00030/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00028/gid00047/gid00035\n/gid00020/gid00017\n/gid00020/gid00015\n/gid00005/gid00017\n/gid00005/gid00015\n/gid00017/gid00042/gid00046\n/gid00015/gid00032/gid00034\n/gid00020/gid00039/gid00032/gid00032/gid00043\n/gid00024/gid00042/gid00045/gid00038\n/gid00007/gid00042/gid00042/gid00031\n/gid00006/gid00051/gid00032/gid00045/gid00030/gid00036/gid00046/gid00032\n/gid01185/gid01185/gid01185/gid01185/gid01185\n/gid01185/gid01185/gid01185/gid01185\n/gid01093/gid01087\n/gid01094/gid01087\n/gid01095/gid01087\n/gid01096/gid01087\n/gid01088/gid01087/gid01087\n/gid01727\n/gid01093/gid01087\n/gid01094/gid01087\n/gid01095/gid01087\n/gid01096/gid01087\n/gid01088/gid01087/gid01087\n/gid01727\nFig. 3. 95% confidence intervals of slot filling rate by variables with a significant effect: (a) the combination\nof the information format and personality modifier represented as study condition; (b) topic; and (c) the\nconversation path. The asterisks with arms indicate significance between the connected categories. (Refer to\nAppendix A.3 for model details and statistics.)\n12\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\n(ùëù = .01), Work (ùëù < .0001), and Food (ùëù = .02) (See Figure 3b). Lastly, dialogues in the Positive\npath had significantly higher rates than those in the Negative path (ùëù = .01) (See Figure 3c).\n0\n20\n40\n60\n80\n100\n1st slot\n2nd slot\n3rd slot\n4th slot\n5th slot\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\nSleep\nWork\nFood\nExercise\n89\n86\n86\n61\n65\n81\n69\n74\n57\n91\n80\n74\n55\n91\n100\n94\n77\n83\nFig. 4. Breakdowns of the percentage of filled slots by the order of questions for each topic. Work and Exercise\nconsist of four slots.\nAs seen in Figure 4, there is a general trend that slots specified earlier in prompts were more\nlikely to be covered by chatbots. For example, the first slots in all topics were covered in 90.3% of\nthe dialogues, but the last specified slots in Sleep (sleep quality) and Work (what went well) were\nomitted around 40% of the dialogues. Interestingly, the last specified slot of Food (feelings after\neating) was diligently covered: chatbots often asked how participants felt after talking about each\nmeal rather than asking their feelings once towards the end.\n4.3 Essential Dialogue Acts\nTo further understand how chatbots powered by different prompt designs talk, we categorized\nconversation turns into dialogue acts. We provide the summary of essential dialogue acts and their\ndistributions in Table 5 Here, we report chatbots‚Äô essential acts regarding question/answering and\nnon-question statements.\n4.3.1 Required and Secondary Questions. We identified two types of questions that the chatbots\nasked: required questions (RQ) and secondary questions (SQ). The RQs were directly related to\nthe specified information slots, whereas SQs were not directly related to the information slots\nbut rather follow-up details or elaboration. Despite being relevant to the conversation topic, SQs\nTable 5. Summary of essential dialogue acts with the ratio of the occurring turns per dialog (turn ratio) by\ncondition, brief description, and exemplar turns (\n : user turns,\n : chatbot turns).\nDialogue Act Turn Ratio (%) Description Examples\nSP SN DP DN\nGreeting 12.40 12.94 10.67 10.50 Initiation of a conversation.\n How‚Äôs your day so far?\nI feel refreshed and recharged.\nTask opening 1.92 1.56 0.54 0.04 General questions that bring\nup the conversation topic.\nHow was your work and productivity\nyesterday?\nRequired\nquestion\n24.07 21.36 17.57 20.97 Questions and answers that\nare directly related to the\nspecified information slots.\nWhat was your lunch yesterday? (RQ)\nI had pork barbeque. (RA)\nRequired\nanswer\n23.74 19.91 17.35 20.75\nSecondary\nquestion\n10.28 10.79 13.87 13.91 Questions and answers that\nare usually follow-ups and\nnot specific to the slots.\nWhat did you put on your toast? (SQ)\nI put strawberry jam on it. (SA)\nSecondary\nanswer\n9.12 9.64 13.08 12.71\nStatement 14.22 19.54 22.85 16.94 Non-Q&A messages such as\ncommenting or summarizing.\nIt sounds like you had a great night‚Äôs sleep!\nI did! Not sure why though.\nClosing 3.88 4.31 3.96 4.14 Farewell or ending messages.\n You‚Äôre welcome. Have a great day!\n13\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nsometimes caused the conversation to digress. Although not very common (95 out of 1,029 SQ\nturns in total; 9.3%), participants also asked questions to the chatbot, which were all categorized as\nSQ/SA. The majority of collected dialogues consisted of question/answering: Overall, 4,879 out of\n7,442 turns (ùëéùë£ùëî. 64.72% of turns per dialogue; ùëöùëñùëõ = 10.67%, ùëöùëéùë• = 93.54%) were classified as RQ,\nRA, SQ, or SA (See Table 5).\nWe first investigated the impact of prompt designs on the chatbot-spoken RQ and SQ turn\nratios using two mixed-effect models with each turn ratio as a dependent variable, respectively.\nFigure 5 shows the 95% confidence intervals of the chatbot-spoken RQ and SQ turn ratios by four\nstudy factors (information format, personality modifier, topic, and path). The structured format\nsignificantly increased the RQ turn ratio (ùëù = .03; see Figure 5a) but decreased the SQ turn ratio\n(ùëù = .002; see Figure 5e). On the other hand, the personality modifier did not impact either RQ\n(ùëù = .94) nor SQ (ùëù = .43) turn ratios and made no difference within the same information format\n(See Figure 5b and 5f).\nIn terms of the conversation path, we find that, overall, the Positive path led to a higher RQ\nturn ratio (ùëù = .01; see Figure 5c) and a lower SQ turn ratio ( ùëù = .002; see Figure 5g). However,\nunder different topics, the conversation path had different impacts on RQ and SQ ratios. As seen in\nFigure 5d, the Positive path increased the RQ turn ratio only in the Work (ùëù = .02) and Exercise\n(ùëù = .003) dialogues and also decreased the SQ turn ratio in the same topics (Work:ùëù = .003 and\nExercise: ùëù < .001).\nAs discussed above, prompt designs, topics and conversation paths have significant impacts on\nchatbots‚Äô question-asking behaviors. The RQ and SQ ratios further impacted the slot filling rate.\nWe ran the maximum-likelihood tests with two mixed-effect models fitting the slot filling rate, one\nwith the chatbot-spoken RQ turn ratio (i.e., the ratio of the turns classified as RQ in a dialogue) as a\nfixed effect and the other with the SQ turn ratio, both with participants as a random effect. We\nfound that the RQ turn ratio was positively correlated with the slot filling rate, whereas the SQ\nturn ratio was negatively correlated with it: ùõΩ = 1.08, ùëÜùê∏ = 0.12, ùë°(347.25)= 8.80, ùëù < .0001 for RQ\nand ùõΩ = ‚àí0.77, ùëÜùê∏ = 0.13, ùë°(370.06)= ‚àí6.12, ùëù < .0001 for SQ.\nIn summary, our results suggest that chatbots with a Descriptive information format tend to\nask more secondary questions, and negative answers of participants also naturally elicit more\nsecondary questions (e.g.,\n ‚ÄúI‚Äôm sorry to hear that you didn‚Äôt workout yesterday. May I ask why? ‚Äù).\n4.3.2 Non-question Statements. Statements from chatbots usually consist of chitchats, comments\nand suggestions that do not fall within the question/answering (see Table 5 for references) and\nparticipants also tend to respond to statements with other statements. Usually, the participants‚Äô\nstatements did not contain direct clues about the desired information slots. Overall, 1,482 out of\n7,442 turns (ùëéùë£ùëî. 18.45% per dialogue) were classified as Statement (See Table 5). Among the four\nconditions, dialogues of DP have the highest ratio of statements (24.99%) and SP had the lowest\nratio (15.32%).\n4.4 Empathy & Engagement Behaviors\nTable 6 summarizes the empathy & engagement categories and their turn ratios by experimental\ncondition. The majority of these behavior categories were coded to the chatbot turns‚Äî1,992\nchatbot turns and 403 user turns were coded with one or more behavior categories‚Äîpartly because\nparticipants uttered less words than chatbots (see Table 3) and chatbots usually led the conversation\nwhile participants simply responded. Acknowledging was the most common empathy behavior\n(see Table 6) as chatbots often referred to what participants previously said in generated messages.\nAlso, we found that our chatbots oftenappreciated participants‚Äô accomplishment (e.g., taking good\n14\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nsleep, managed to exercise) or sympathized participants when they reported negative outcomes\n(e.g., poor sleep quality, failed at work).\nTo investigate how the four study factors impact the empathetic behaviors of chatbots, we\nanalyzed three mixed-effect models with the chatbot turn ratios of Acknowledging, Appreciating,\nand Sympathizing behaviors as a dependent variable, respectively. Figure 6 shows the 95% confidence\nintervals of turn ratios of the three behavior categories estimated against the study factors. The\ninformation format significantly influenced the Acknowledging and Appreciating turn ratios:\nDialogues in the Descriptive format had higher ratios of the Acknowledging ( ùëù < .0001; see\nFigure 6a) and Appreciating (ùëù = .01; see Figure 6e) turns. Personality modifier did not solely impact\nthese two behaviors but it influenced in conjunction with the information format (See Figure 6c\n***ùëù<.001; **ùëù<.01; *ùëù<.05\n/gid00020\n/gid00005\n/gid00020/gid00017\n/gid00020/gid00015\n/gid00005/gid00017\n/gid00005/gid00015\n/gid00020/gid00039/gid00032/gid00032/gid00043\n/gid00024/gid00042/gid00045/gid00038\n/gid00007/gid00042/gid00042/gid00031\n/gid00006/gid00051/gid00032/gid00045/gid00030/gid00036/gid00046/gid00032\n/gid00020\n/gid00005\n/gid00020/gid00017\n/gid00020/gid00015\n/gid00005/gid00017\n/gid00005/gid00015\n/gid00020/gid00039/gid00032/gid00032/gid00043\n/gid00024/gid00042/gid00045/gid00038\n/gid00007/gid00042/gid00042/gid00031\n/gid00006/gid00051/gid00032/gid00045/gid00030/gid00036/gid00046/gid00032\n/gid01087\n/gid01092\n/gid01088/gid01087\n/gid01088/gid01092\n/gid01089/gid01087\n/gid01089/gid01092\n/gid01090/gid01087\n/gid01087\n/gid01092\n/gid01088/gid01087\n/gid01088/gid01092\n/gid01089/gid01087\n/gid01089/gid01092\n/gid01090/gid01087\n/gid01087\n/gid01092\n/gid01088/gid01087\n/gid01088/gid01092\n/gid01089/gid01087\n/gid01089/gid01092\n/gid01090/gid01087\n/gid01087\n/gid01092\n/gid01088/gid01087\n/gid01088/gid01092\n/gid01089/gid01087\n/gid01089/gid01092\n/gid01090/gid01087\n/gid01175/gid00028/gid01176/gid00001/gid00003/gid00052/gid00001/gid00007/gid00042/gid00045/gid00040/gid00028/gid00047\n/gid01175/gid00029/gid01176/gid00001/gid00003/gid00052/gid00001/gid00004/gid00042/gid00041/gid00031/gid00036/gid00047/gid00036/gid00042/gid00041\n/gid01175/gid00030/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00028/gid00047/gid00035\n/gid01175/gid00031/gid01176/gid00001/gid00003/gid00052/gid00001/gid00021/gid00042/gid00043/gid00036/gid00030/gid00001/gid01750/gid00001/gid00004/gid00042/gid00041/gid00049/gid00032/gid00045/gid00046/gid00028/gid00047/gid00036/gid00042/gid00041/gid00001/gid00017/gid00028/gid00047/gid00035\n/gid00017/gid00042/gid00046/gid00036/gid00047/gid00036/gid00049/gid00032/gid00001/gid00043/gid00028/gid00047/gid00035\n/gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032/gid00001/gid00043/gid00028/gid00047/gid00035\n/gid01175/gid00032/gid01176/gid00001/gid00003/gid00052/gid00001/gid00007/gid00042/gid00045/gid00040/gid00028/gid00047\n/gid01175/gid00033/gid01176/gid00001/gid00003/gid00052/gid00001/gid00004/gid00042/gid00041/gid00031/gid00036/gid00047/gid00036/gid00042/gid00041\n/gid01175/gid00034/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00028/gid00047/gid00035\n/gid01175/gid00035/gid01176/gid00001/gid00003/gid00052/gid00001/gid00021/gid00042/gid00043/gid00036/gid00030/gid00001/gid01750/gid00001/gid00004/gid00042/gid00041/gid00049/gid00032/gid00045/gid00046/gid00028/gid00047/gid00036/gid00042/gid00041/gid00001/gid00017/gid00028/gid00047/gid00035\n/gid00021/gid00048/gid00045/gid00041/gid00001/gid00019/gid00028/gid00047/gid00036/gid00042/gid00001/gid00042/gid00033\n/gid00019/gid00032/gid00044/gid00048/gid00036/gid00045/gid00032/gid00031/gid00001/gid00018/gid00048/gid00032/gid00046/gid00047/gid00036/gid00042/gid00041/gid00046\n/gid00021/gid00048/gid00045/gid00041/gid00001/gid00019/gid00028/gid00047/gid00036/gid00042/gid00001/gid00042/gid00033\n/gid00020/gid00032/gid00030/gid00042/gid00041/gid00031/gid00028/gid00045/gid00052/gid00001/gid00018/gid00048/gid00032/gid00046/gid00047/gid00036/gid00042/gid00041/gid00046\n/gid01727\n/gid01727\n/gid01727\n/gid01727\n/gid01185\n/gid01185/gid01185/gid01185\n/gid01185\n/gid01185/gid01185\n/gid00017/gid00042/gid00046\n/gid00015/gid00032/gid00034\n/gid00017/gid00042/gid00046\n/gid00015/gid00032/gid00034\n/gid01185\n/gid01185/gid01185\n/gid01185/gid01185/gid01185\n/gid01185/gid01185/gid01185/gid01185/gid01185\n/gid01088\n/gid01088\n1Excluded pairs across different topics from the pairwise comparison.\nFig. 5. 95% confidence intervals of the turn ratios of RQ (top; a‚Äìd) and SQ (bottom; e‚Äìh) by variables with\na significant effect: The asterisks with arms indicate significance between the connected categories. Note\nthat for (d) and (h) we did not display the significance across topics. (Refer to Appendices A.4.1 and A.4.2 for\nmodel details and statistics.)\nTable 6. Summary of empathy & engagement behaviors with turn ratio by condition, brief description, and\nexemplar turns. Note that the behaviors are multi-coded.\nBehavior Turn Ratio (%) Description Examples\nCategory SP SN DP DN\nAcknowledging 18.22 18.91 26.24 22.04 Directly referring to what\nthe other said.\nThat‚Äôs great to hear that your legs are\nfeeling stronger!\nAppreciating 9.04 7.36 11.18 9.86 Complimenting the other.\n That‚Äôs terrific!\nSympathizing 3.66 2.79 4.33 1.74 Sympathizing with the other.\n I‚Äôm sorry to hear that. What‚Äôs been going on?\nThanking 5.16 6.74 7.05 7.26 Being grateful to the other.\n I feel nice. Thanks for asking.\nAdvice/\nsuggesting\n2.54 3.07 4.81 2.55 Giving advice or suggestion.\n I can give you some recommendations on\nexercises that will help you grow your\nadductor muscles.\nRejecting/\ndisagreeing\n0.57 0.66 0.40 0.66 Rejecting or disagreeing with\nthe other.\nBut the weather needs to be good for walking.\n15\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\n***ùëù<.001; **ùëù<.01; *ùëù<.05\n/gid00020\n/gid00005\n/gid01087\n/gid01088/gid01087\n/gid01089/gid01087\n/gid01090/gid01087\n/gid00017/gid00042/gid00046\n/gid00015/gid00032/gid00034\n/gid01087\n/gid01088/gid01087\n/gid01089/gid01087\n/gid01090/gid01087\n/gid00017\n/gid00015\n/gid00020/gid00017\n/gid00020/gid00015\n/gid00005/gid00017\n/gid00005/gid00015\n/gid00020/gid00017\n/gid00020/gid00015\n/gid00005/gid00017\n/gid00005/gid00015\n/gid00020\n/gid00005\n/gid01087\n/gid01092\n/gid01088/gid01087\n/gid01088/gid01092\n/gid00017\n/gid00015\n/gid00017/gid00042/gid00046\n/gid00015/gid00032/gid00034\n/gid01087\n/gid01092\n/gid01088/gid01087\n/gid01088/gid01092\n/gid00020/gid00017\n/gid00020/gid00015\n/gid00005/gid00017\n/gid00005/gid00015\n/gid00020\n/gid00005\n/gid01087\n/gid01092\n/gid01088/gid01087\n/gid00017/gid00042/gid00046\n/gid00015/gid00032/gid00034\n/gid01087\n/gid01092\n/gid01088/gid01087\n/gid00017\n/gid00015\n/gid00021/gid00048/gid00045/gid00041/gid00001/gid00019/gid00028/gid00047/gid00036/gid00042/gid00001/gid00042/gid00033\n/gid00002/gid00030/gid00038/gid00041/gid00042/gid00050/gid00039/gid00032/gid00031/gid00036/gid00041/gid00034/gid00001/gid01175/gid00046/gid00052/gid00046/gid00047/gid00032/gid00040/gid01176\n/gid00021/gid00048/gid00045/gid00041/gid00001/gid00019/gid00028/gid00047/gid00036/gid00042/gid00001/gid00042/gid00033\n/gid00002/gid00043/gid00043/gid00045/gid00032/gid00030/gid00036/gid00028/gid00047/gid00036/gid00041/gid00034/gid00001/gid01175/gid00046/gid00052/gid00046/gid00047/gid00032/gid00040/gid01176\n/gid00021/gid00048/gid00045/gid00041/gid00001/gid00019/gid00028/gid00047/gid00036/gid00042/gid00001/gid00042/gid00033\n/gid00020/gid00052/gid00040/gid00043/gid00028/gid00047/gid00035/gid00036/gid00053/gid00036/gid00041/gid00034/gid00001/gid01175/gid00046/gid00052/gid00046/gid00047/gid00032/gid00040/gid01176\n/gid01727\n/gid01727\n/gid01185/gid01185/gid01185/gid01185/gid01185/gid01185/gid01185\n/gid01185\n/gid01185/gid01185\n/gid01185/gid01185/gid01185\n/gid01185\n/gid01185/gid01185/gid01185\n/gid01185/gid01185/gid01185\n/gid01185/gid01185/gid01185/gid01185/gid01185/gid01185\n/gid01175/gid00028/gid01176/gid00001/gid00003/gid00052/gid00001/gid00007/gid00042/gid00045/gid00040/gid00028/gid00047\n/gid01175/gid00029/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00032/gid00045/gid00046/gid00042/gid00041/gid00028/gid00039/gid00036/gid00047/gid00052/gid00001/gid01175/gid00015/gid00020/gid01176\n/gid01175/gid00030/gid01176/gid00001/gid00003/gid00052/gid00001/gid00004/gid00042/gid00041/gid00031/gid00036/gid00047/gid00036/gid00042/gid00041\n/gid01175/gid00031/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00028/gid00047/gid00035\n/gid01175/gid00032/gid01176/gid00001/gid00003/gid00052/gid00001/gid00007/gid00042/gid00045/gid00040/gid00028/gid00047\n/gid01175/gid00033/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00032/gid00045/gid00046/gid00042/gid00041/gid00028/gid00039/gid00036/gid00047/gid00052/gid00001/gid01175/gid00015/gid00020/gid01176\n/gid01175/gid00034/gid01176/gid00001/gid00003/gid00052/gid00001/gid00004/gid00042/gid00041/gid00031/gid00036/gid00047/gid00036/gid00042/gid00041\n/gid01175/gid00035/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00028/gid00047/gid00035\n/gid01175/gid00036/gid01176/gid00001/gid00003/gid00052/gid00001/gid00007/gid00042/gid00045/gid00040/gid00028/gid00047/gid00001/gid01175/gid00015/gid00020/gid01176\n/gid01175/gid00037/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00032/gid00045/gid00046/gid00042/gid00041/gid00028/gid00039/gid00036/gid00047/gid00052\n/gid01175/gid00038/gid01176/gid00001/gid00003/gid00052/gid00001/gid00004/gid00042/gid00041/gid00031/gid00036/gid00047/gid00036/gid00042/gid00041\n/gid01175/gid00039/gid01176/gid00001/gid00003/gid00052/gid00001/gid00017/gid00028/gid00047/gid00035\nFig. 6. 95% confidence intervals of the chatbot turn ratios for Acknowledging (a‚Äìd), Appreciating (e‚Äìh), and\nSympathizing (i‚Äìl) behaviors by information format, personality modifier, study condition (combinations of\nformat and personality modifier), and conversation path. Variables that are not significant are marked as\n‚ÄòNS. ‚Äô (Refer to Appendix A.5 for model details and statistics.)\nand 6g). However, the personality modifier in the prompt led chatbots to produce significantly\nmore Sympathizing turns (ùëù = .002; see Figure 6j). Besides the prompt design, the conversation\npath strongly influenced all three empathetic behaviors: The Positive path led to higher turn ratio\nof Appreciating (ùëù < .0001; see Figure 6d) whereas The Negative path led to higher Acknowledging\n(ùëù < .001; see Figure 6h) and Sympathizing (ùëù < .0001; see Figure 6l) turn ratios.\n4.5 Problematic Chatbot Turns and User Responses\nIn total, 6.7% of the chatbot turns (257 out of 3,916) were tagged erroneous and the four categories of\nerroneous turns are summarized in Table 7. These erroneous turns sometimes led to the unorganic\nTable 7. Categories of the chatbots‚Äô erroneous turns with turn ratio, brief descriptions, and examples.\nCategory Turn Ratio Description Example\nIncorrect\nphrases\n0.79% Messages with symbols or\nextra predictions.\nWorkout duration -> 45 minutes (1)\nIt took me a while to fall asleep. (2)\nSelf-talk 1.1% First-person narratives or\ncommentary messages.\nThe customer‚Äôs fitness concern is that they are\nfeeling very tired after their workout.\nRepetition 3.8% Repeating the same or similar\nutterances multiple times.\nRefer to Fragment 2.\nMiscellaneous 0.9% Other miscellaneous errors.\n (no response) (1)\nOther than work, what were your concerns at work? (2)\n16\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\ntermination of the conversation (i.e., participants ended the conversation before or without natural\nClosing messages). In the following, we cover these error types in detail.\nIncorrect phrases. In structured prompts (SP, SN), we used a symbol ‚Äú ->, ‚Äù a commonly-used\ndelimiter for key-value pairs in LLMs, to specify the information slots. This caused GPT-3 to expose\nsuch an information structure to the output as an artifact in 16 turns of messages, all of which\nwere generated by the structured format. In the example from Table 7, GPT-3 even incorrectly\n‚Äúpredicted‚Äù the slots (45 minutes, example (1) in Table 7) altogether with symbols. Other times,\nGPT-3 also erroneously predicted the answers for participants (example (2) Table 7). Particularly,\nwe identified 6 instances where GPT-3 predicted the user response and appended an extra\n turn\nto the generated chatbot turns (3 turns in descriptive groups and 3 turns in structured groups).\nWhile the exposure of the symbol was caused by the structured prompt design, other incorrect\nphrases were mostly caused by the random nature of LLMs [17].\nSelf-talk. GPT-3 sometimes generated turns in a first-person narrative or not directed to partici-\npants, which looked quite similar to the ‚Äúself-talk‚Äù of humans. In Fragment 1, for example, Turn 03\nis obviously not directed to participants. In our dataset, less than 1.1% of chatbot turns were self-talk\n(SP: 8, SN: 27, DP: 1, DN: 9). Participants who encountered these ‚Äúself-talk‚Äù commented that these\nmessages were ‚Äúawkward‚Äù (P42), ‚Äústrange‚Äù (P24), and ‚Äúconfusing‚Äù(P23). However, we found that\nparticipants always attempted to continue the conversation by following the self-talk and tried to\nresolve the errors (See Turn 04 in Fragment 1). We used the first-person narrative in prompts as it\nappeared to make generated chatbots talk more like humans; however, the first-person narrative\nmight negatively lead to the ‚Äúself-talk‚Äù in conversations.\nFragment 1. Dialogue snippets of self-talk from P43.\n01\n Do you have any fitness concerns that you would like to address?\n02\n nope not at the moment.\n03\nBased on the customer‚Äôs responses, it seems that they did not have a very active day\nyesterday. I would encourage them to try and get some form of physical activity in\ntoday, even if it is just a short walk.\n04\n yes I should probably try it tomorrow.\nRepetition. We found that GPT-3 was susceptible to generate repetitive messages, either identically\nor linguistically repeating the previous chatbot turns. In total, 147 turns (3.8% of chatbot turns) were\nlabeled to be repetitive (SP: 11, SN: 39, DP: 48, DN: 50). Identically repetitive messages occurred to\n23 participants (SP: 3, SN:5, DP: 6, DN: 9) in 31 dialogues (SP: 3, SN: 9, DP: 8, DN:10). Among the\nfour topics, work (14 dialogues) tended to have more identically repetitive messages and exercise\n(2 dialogues) tended have fewer identically repetitive messages. However, these messages usually\nserved as SQ/SA in conversations, hence they rarely influenced slot filling and data collection.\nLinguistically repetitive messages usually share similar wording or phrase structures. Fragment 2\npresents an example dialogue snippet. At Turn 01, 03, 05, and 07, the chatbot always started with a\nsimilar phrase (i.e.,\n ‚ÄúThat‚Äôs great ... ‚Äù) to compliment the participant then asked the participant a\nquestion started with ‚Äúcan you ‚Äù. These linguistically repetitive messages were semantically correct\nand 58.3% took place in the Acknowledging turns where the chatbot rephrased what the participant\nsaid progressed the conversation organically. However, linguistically phrased messages looked too\nsimilar, and they negatively impacted the user experience. A few participants suggested that the\nchatbots seemed to actually understand their responses, yet were using ‚Äúa sentence template‚Äù (P29)\nto respond in a ‚Äúpredefined ways‚Äù (P13). On the other hand, repetition could also lead to ‚Äúdead\nloops‚Äù of conversations. Repetitions were mostly caused by principles of LLMs. As the conversation\n17\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nhistory was used as the new prompt, GPT-3 tended to generate patternized texts, which led to\nfurther repetitions.\nFragment 2. Dialogue snippet of linguistically repetitive utterances from P37.\n01\n That‚Äôs a specific and achievable goal. Can you tell me more about what libraries you need to install?\n02\n I don‚Äôt know yet which libraries I need, but probably Pytorch, ...\n03\n That‚Äôs a great goal. Can youresearch what libraries you need and then install them?\n04\n The libraries I need is provided in a file.\n05\n That‚Äôs great.So you have a list of the libraries you need. Can you install them?\n06\n I need to run a script that download and install these libraries.\n07\n That‚Äôs a great goal. Can youwalk me through the script?\nMiscellaneous. There were 37 problematic chatbot turns that did not fall into the above categories.\nAmong these turns, 19 turns were tagged to have context errors (i.e., chatbots did not grasp the\ncontext at all), 10 had semantic errors (i.e., messages that are not of human common sense), and one\nhad both context and semantic errors. In Fragment 3, for example, the chatbot entirely missed that\nthe participant said\n ‚Äú1 hour of cardio . ‚Äù But the chatbot also attempted to resolve the contextual\nmisunderstanding by apologizing after the participant corrected it. While chatbots in our study\nappeared to understand people pretty well in most cases, their responses with semantic errors\ncould be quite wrong and amusing. For instance, when being asked about workout yesterday, one\nparticipant told the chatbot that they skipped. Instead of considering the participant did not workout,\nthe chatbot considered skipping as a jumping workout and responded\n ‚ÄúSkipping is a type of\ncardiovascular exercise that can help to improve your heart health and endurance. ‚Äù Besides, there\nwere two instances when the chatbot failed to detect the ending of the conversation and restarted\nwith the first slot question again, which, of course, led participants to abandon the conversation.\nThese errors were rare but could also be attributed to the randomness nature of LLMs. Lastly, our\nsystem went offline 5 times and caused chatbots to output empty messages, which was caused by\nthe over-frequent API calls to OpenAI.\nTerminating Conversations. With the Closing turns, we found that 75.7% of the conversations\nwere organically ended. Among the four conditions, SP had the lowest percentage of naturally ended\nconversations (71.4%) than other three conditions. For 91 non-organically ended conversations,\nparticipants abandoned 48.3% of those conversations without encountering any obvious problematic\nerrors. 19.8% of prematurely ended conversations were caused by identically repetitive and another\n19.8% of were caused by linguistically repetitive messages. Lastly, the self-talk led to 3 conversations\nto end early, and context errors and system-offline caused the rest incomplete conversations.\nFragment 3. Dialogue snippet of the context error from P6.\n01\n What workout did you do yesterday?\n02\n I did 1 hourof cardio yesterday.\n03\n How long did your workout last?\n04\n Come on dude. I just told you I did 1 hour of cardio. How long is 1 hour?\n05\n I‚Äôm sorry if I came across as rude. I was just trying to get some information from you about\nyour workout yesterday.\n4.6 Subjective Evaluation\nFigure 7 summarizes the distribution of participants for rating (1) the ability to understand, (2) the\nability to acknowledge user input, and (3) the level of empathy. The Kruskal-Wallis tests showed\n18\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\n1\n2\n3\n4\n5\n0\n2\n4\n6\n8\n10\n1\n2\n3\n4\n5\n0\n2\n4\n6\n8\n10\nQ1. The chatbot understands my messages.\nStructured\nDescriptive\nStructured\nDescriptive\nStructured\nDescriptive\nNo Personality\nWith Persnoality\nNo Personality\nWith Persnoality\nNo Personality\nWith Personality\n1\n2\n3\n4\n5\n0\n2\n4\n6\n8\n10\n1\n2\n3\n4\n5\n0\n2\n4\n6\n8\n10\nQ2. The chatbot takes into account \n of my messages.\n1\n2\n3\n4\n5\n0\n2\n4\n6\n8\n10\n1\n2\n3\n4\n5\n0\n2\n4\n6\n8\n10\nQ3. The chatbot talks more like a \n human than like a robot.\nSP DP\nSN DN\nSP DP\nSN DN\nSP DP\nSN DN\nFig. 7. Distributions of the subjective ratings from three scale questions in the exit survey, with breakdowns\nby the information format and the personality modifier.\nthat there were no significant differences among conditions for all three questions. In general,\nmost participants highly rated for Q1 and Q2 that the chatbots could understand them as well as\nacknowledge their messages: Fifteen (31.3%) participants rated 5 and 24 (50.0%) rated 4 on Q1; 11\n(22.9%) rated 5 and 31 (64.6%) rated 4 on Q2. Participants showed mixed perception for the level of\nempathy question with a median of 3.\nSome participants gave positive feedback in the open-ended question. P25 who frequently used\nLLMs commented, ‚ÄúI was surprised to see how accurate and detailed the bot‚Äôs responses were. ‚Äù P36\nwho did not have any experience with LLMs gave a similar comment‚Äî‚ÄúI found it quite responsive\nand surprisingly considerate of my answers. ‚Äù Despite the errors we presented above, P21 still\ncomplimented the chatbots: ‚ÄúI liked/was satisfied of how the chatbot precisely gave info when I asked\nfor it, and I felt that the relevance of the answer is very high and that it caught my point of question\nsharply.‚Äù and even suggested that, ‚Äú I felt keeping chatbot as a companion would be awesome. To\nregularly make casual conversation and be provided light insights about my daily life based on my\ncasual chats. ‚Äù\n5 DISCUSSION\nOur results showed that our zero-shot chatbots achieved great abilities in asking desired questions\nand understanding user responses despite also having drawbacks. In this section, we reflect on the\nperformance of our chatbots and discuss opportunities, ethical considerations, and limitations for\nfuture work.\n5.1 Designing Effective Prompts for Chatbots that Collect Self-Reports\nOur study showed that defining a first-person job identity as well as specifying information slots in\nprompts was an effective method to bootstrap chatbots that ask health-related questions. However,\nthe slot filling performance and chatbots‚Äô behaviors were sensitive to prompt design, topic, and\nconversation paths. We provide the following prompt design suggestions based on our findings:\nCombine Information Format and Personality Modifier Wisely. Although the information\nformat and personality modifier did not consistently impact slot filling rates individually, how they\nwere combined had different impacts on slot filling rates. The information format affected chatbots‚Äô\nquestion-asking behaviors: structured formats lead to more RQs and fewer SQs and vice versa for\ndescriptive ones. In other words, structured formats steer chatbots to ask direct questions about the\nspecified slots whereas descriptive ones focus more on eliciting surrounding context or additional\ndetails. The personality modifier had a synergy with descriptive formats: Chatbots in DP had the\n19\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nlowest slot filling rates and the ratio of RQs, but had the highest number of acknowledging messages.\nReferring to Table 4, SP and DN have comparable slot filling rates. Therefore, to build chatbots\nthat can show a higher level of understanding through acknowledgment, using the Descriptive\nformat without personality modifier could be the best option. But chatbots with more direct\nacknowledgment may be at the risk of being awkward and too robotic. Hence, when designing\nprompts to power chatbots for data collection, using structured format with personality modifier\nwould be more desirable.\nEvaluate chatbot for conversation topic and path. There is certain discrepancy of slot filling\nrates between topics. One reason for such difference could be the nature of the topic. For example,\nthe topic work tended to be the most open-ended topic as people report different types of work,\nwhich could lead to more subject switches and digressions than others. We suspect that GPT-3 is\nmore suitable to steer chatbots that are of less divergent topics and collect self-reports that are\nmore structured. Also, considering the path of conversations also impacts the data collection rate,\nresearchers may consider clearly specify different slots for both positive and negative paths. For\nexample, developers can add ‚Äúif the customer did not workout yesterday, I would ask\nthem what workout they did in the past week ‚Äù to the prompt for chatbots in the exercise\ntopic. Lastly, researchers can also test ‚Äúneutral‚Äù path to see whether the chatbots can actively elicit\nrich user responses.\nComposition of Slots Matters. The number and data types of slots also impact the chatbots‚Äô\nperformance in collecting slots. With the conversation goes longer, chatbots have a tendency to\nmiss information slots that appear later in prompts. Also, while we collected a certain amount of\nnumerical rates for sleep quality and productivity rate, it is not guaranteed that the chatbots would\ncover a slot definition (e.g., numerical scale) as intended. Sometimes, the chatbot would simply\nask\n ‚ÄúWould you say you had a good night‚Äôs sleep? ‚Äù or\n ‚ÄúOverall, how do you feel about your\nwork and productivity yesterday? ‚Äù. Hence, if to use chatbots powered by GPT-3 to facilitate data\ncollection, we suggest that important slots be put earlier in the prompt and the number of questions\nof specific data type be limited. If more data slots need to be collected, multi-stage prompts [114]\ncan be considered.\n5.2 Opportunities of LLM-driven Chatbots\nFrom the study, we learned that LLM-driven chatbots are advantageous compared with traditional\nchatbot platforms in multiple aspects. Here we cover some noteworthy aspects drawing on the\nresults.\nVersatile Responses and Follow-up Questions. Compared to chatbots with pre-defined dialogues,\nchatbots in our study can deliver a great number of versatile phrases. For example, for these scale\nquestions, GPT-3 can output phrases such as\n ‚ÄúWould you say that your sleep quality yesterday\nwas a 10/10, 9/10, 8/10...? ‚Äù GPT-3 can even provide clarifications for questions and ask follow-\nup questions which supplement the topic. However, these SQs were still on-topic and directly\naddressed to user inputs (See SQ/SA in Table 5). Follow-up questions are commonly used in human-\nadministered interviews to increase interactivity [87] and many studies suggest that chatbots that\ncan ask on-topic follow-up questions are considered more human-like [67, 94]. Although current\nchatbot frameworks (e.g., Amazon Alexa [7]) [71] support follow-up/extended questions, developers\nneed to specify both the expected slots and the follow-up phrases [ 88]. On the contrary, GPT-3\ncould naturally ask follow-up questions, equipping chatbots with proper common sense on the\ntopic. For example, our chatbot mapped ‚ÄúBulgogi, rice, and kimchi ‚Äù to ‚Äúa very traditional Korean\nmeal‚Äù in its response to the participant. Such response engages people through showing a level of\n‚Äúunderstanding. ‚Äù\n20\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nSocial Attributes. Given the importance of social features such as chit-chat for positive user\nexperience [62, 91], our results show that we can easily equip GPT-3 with such social aspects. For\nexample, our chatbots could respond naturally to the questions about their ‚Äúpersonal life‚Äù‚Äî e.g.,\n‚ÄúDo you workout yourself? ‚Äù\n ‚ÄúYes, I work out regularly myself. I find that it helps me to stay\nenergized and focused throughout the day. ‚Äù Further, our chatbots were also able to give suggestions\nrelevant to the topic. While mostly originated from common sense, some of the suggestions were\nin-depth and tailored. In one time, one participant asked two questions in a row (probably due\nto system or network error) were quite surprised to find that the chatbot provided a well-written\nresponse (See Turn 04 in Fragment 6). This participant even commented that ‚ÄúI know a small bit\nabout NLP but not much when it comes to generating responses. I find it fascinating that (it) can give\nsuch in-depth answers to specific topics as I find it hard to be able to train an AI to every kind of case\ninvolving that.‚Äù\nError Recovery. Task-oriented chatbots usually have limited number of pre-defined user intents to\naccomplish a specific goal. For instance, a banking chatbot can provide services such as currency-\nexchange conversion and introduction of credit cards [ 57]. However, such chatbots are usually\nunable to handle user messages that are out of the pre-defined intents (e.g., a user attempts to have\nsmall talk with the banking chatbot) [57]. Also, they may even mis-recognize in-scope messages\ndue to the complexity of natural language [9]. Strategies like highlighting keywords and switching\ntopics [9] can help resolve conversation breakdowns at the price of making chatbots less human-like.\nIn our case, LLM-driven chatbots could handle the out-of-scope conversations relatively well, since\nthey could do improvisation actions relying on the ability of LLMs instead of defining intents\nintensively. In Fragment 4, a work chatbot with the job identity ‚Äúlife coach‚Äù handled the off-topic\nrequest (‚Äúwake me up at 6 am ‚Äù) by the participant smoothly and even provided tips on sleep. Even\nwhen misunderstanding occurred, chatbots sometimes attempted to resolve it. In Fragment 3,\nfor example, the chatbot apologized for its misunderstanding and in5, GPT-3 resolved an empty\nmessage error (due to system offline) by making up an excuse for its absence.\nFragment 4. Dialogue snippet of the bot continuing off-topic conversations from P34.\n01\n Can you wake me up at 6 am?\n02\n I‚Äôm sorry, but I cannot physically wake you up at 6 am. However, I can give you some\ntips on how to wake up earlier and be productive in the morning.\n03\n Yeah, please.\n04\n Some possible tips to help you wake up earlier in the morning are to set your alarm clock for a\nspecific time and place it across the room ... [omitted]\nFragment 5. Dialogue snippet of an error recovery from P2.\n01\n That‚Äôs great! What time did you wake up this morning?\n02\n around 9 am.\n03\n (...no response...)\n04\n hello?\n05\n Sorry, I‚Äôm just checking to see if you‚Äôre still there. Yeah, so 9 am is a pretty good wake up time.\nHow would you rate your sleep quality from last night?\nContext Tracking. Context is a key part in human conversations that connects multiple turns [16].\nPrevious studies have suggested that chatbots should aim to sustain contexts to improve the\ndialogue efficiency [37]. Current conversational interfaces such as Google Assistant and Amazon\n21\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nAlexa shows certain abilities in maintaining contexts [71]; however, most of them are still criticized\nfor not detecting contextual details [14, 20]. With Dialogflow, developers can define some contexts\nto be maintained within 5 turns1; however, it has yet achieved truly flexible conversations through\nthis approach. In our case, the chatbots have shown impressive abilities in sustaining some contexts\nwithout dedicated mechanisms for managing contexts. P27 noted, ‚ÄúI feel like it could keep track of\nthe context well between sentences during the conversation. ‚Äù Through dialogue snippets presented in\nFragment 5 and 6, we can see that the context was maintained across 5 turns and 2 turns, respectively.\nIn particular, the context (intimidation) would be difficult to specify with most chatbot frameworks.\nFurther, one pattern that emerged in the dataset is that chatbots liked to give a summary of all the\nuser input in the end of conversations, which usually covered the past conversation history and\nmaintained contexts longer than 5 turns.\nFragment 6. Dialogue snippet of the chatbot tracking context from P33.\n01\n Those are good. How about intimidation at the gym?\n02\n Oh sorry, was that too vague?\n03\nNo, that‚Äôs a great question. A lot of people feel intimidated when they first start going to the gym.\nMy best advice would be to just take things slow and focus on your own workout. Don‚Äôt worry\nabout what other people are doing or how they‚Äôre looking at you.\nJust focus on yourselfand your own goals.\n04\n That≈õ solid advice. You‚Äôre surprisingly eloquent for these responses!\nLow-effort Bootstrapping. We show several opportunities of LLMs in powering chatbots above.\nIndeed, chatbots that provide many of the above functions, including chitchats, suggestions, and\ncontext perseverance, can be trained with rich datasets. However, collecting such dataset is challeng-\ning, and training models on big datasets is costly and often inaccessible [12]. In terms of utilizing\nmainstream chatbot platforms to build voice applications, it is of great human effort to come up\nwith user intents and example phrases as well as design conversation flows and logic [27, 68]. In\nparticular, the error handling is tricky to design as conversation breakdowns can be common and\neven unexpected in the wild [20, 79, 108]. On the other hand, LLM‚Äôs in-context learning capability\nenables us to skip collecting training dataset or configuring dialogue flows to create functional\nchatbots. Further, our results show that simple alterations of prompts can significantly influence\nthe conversation styles of chatbots. With robust prompt designs, it is possible that people without\nbackground in AI can directly personalize chatbots using natural language.\n5.3 Drawbacks of LLM-driven Chatbots\nAlthough LLMs showed great potential in steering chatbots, we also encountered several drawbacks\nof LLM-driven chatbots. In the following, we discuss the two noteworthy drawbacks and their\npotential causes. We also present strategies to overcome these drawbacks.\nRandomness. LLMs generate text by predicting the most probable text following the input prompt.\nWhile developers can tune hyper-parameters such as temperature and frequency penalty, the LLM\ngenerations inherently exhibit a certain level of randomness [ 66], which is hard to explain or\nanticipate. Such randomness might have led to erroneous responses of our chatbots. For exam-\nple, sometimes the chatbots ‚Äúself-talked‚Äù [93] or exposed machine representations (symbols) as\nresponses. What makes it worse is that as the conversation history is accumulated, erroneous\nresponses stay in the prompt and lead to other erroneous ones. Also, the chatbots sometimes did\nnot consistently react to the same user input. When being told the participant skipped breakfast,\n1https://cloud.google.com/dialogflow/es/docs/contexts-input-output\n22\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\none chatbot under the condition DN responded,\n ‚ÄúThat‚Äôs not ideal. Skipping breakfast can make\nit harder to concentrate and can cause you to overeat later in the day , ‚Äù while the other chatbot\nunder the same condition replied,\n ‚ÄúThat‚Äôs okay! Some people choose to skip breakfast ‚Äù. It is not\nexplainable whether such inconsistencies randomly happened or were caused by prior user inputs.\nThe stochastic nature of LLMs does not guarantee that they would comply with all natural lan-\nguage instructions in prompts. As such, compared to rule-based chatbots that can almost 100%\nask pre-defined scripts [107], we can see that not all specified information slots were asked by our\nchatbots during the study.\nRepetitiveness. As LLMs tend to detect latent patterns in the prompts [ 17], the user messages\naccumulated in the prompt (See B‚óã in Figure 1) may unintentionally trigger patternized behaviors,\nmaking chatbots produce repetitive (although not always identical) turns. For example, many\nof the messages generated by GPT-3 start with ‚Äú It sounds like you... ‚Äù, parroting user responses\nand providing direct acknowledgment. Although such behaviors made many participants rate\nthe chatbots to be ‚Äúunderstanding‚Äù, participants also criticized the awkwardness of parroting. We\nsuspect that such repetition was partially caused by a well-known problem of LLMs: they tend to\ngenerate repetitive messages [111]. In worse cases, chatbots stuck in ‚Äúdead loops‚Äù and could not\nprogress the conversation further. P20 even responded ‚Äúplease enough‚Äù to the chatbot‚Äôs repetitive\nquestions.\nDespite drawbacks discussed above, LLMs-based chatbots can become a valuable and scalable\ntool for researchers to collect data for personal informatics [ 35]. Reflecting on our findings, we\npropose strategies to mitigate the erroneous behaviors of the chatbots. GPT-3 tends to generate\nlong responses, which may make chatbots to appear more robot-like. We suggest that researchers\nconsider intentionally slowing down the responding delays. A longer gap may not only help create\na more human-like chatbot [31] but also create time for the system to run filters and algorithms to\npick better messages. Drawing on problems identified from our analysis, we envision a chatbot\nsystem that generates three responses each turn (if the budget allows). Then, a repetition filter can\nbe used to filter out identically repetitive messages. In terms of linguistically repetitive messages,\nthe system can pick the message with the least linguistic similarity to the chatbot‚Äôs last turn.\nThe filter could also easily remove messages that have the self-talk errors or symbols. When the\nconversation is too long, a parallel prompt can be made to detect if the conversation is in a dead loop\nor a simple ending detection algorithm can deployed to end the conversation and improve the user\nexperience. All these filters are cost-efficient to implement and could resolve many problems. For\nexample, around 80% of errors occurred in SP are repetitive messages, self-talk, and system-offline,\nall of which could potentially be resolved with simple filters. Lastly, we acknowledge that running\nLLMs is always accompanied with uncertainty and the resultant chatbots may not be able to\nfulfill defined tasks every time. Hence, we recommend researchers conducting intensive testing of\nLLMs-powered chatbots to identify errors, understand the slot filling performance, and customize\nfilters accordingly.\n5.4 Ethical consideration\nLLMs are trained on an existing corpus that may contain biased and problematic information [30, 82].\nMany have also suggested that cautions should be taken when using LLMs, particularly in the\nfield of healthcare delivery [48]. In our study, we intentionally used hints to guide participants to\ncompose their answers when conversing as we were unsure whether inappropriate content would\nbe generated. We did not see any biased, harmful or dangerous messages from GPT-3 in our dataset.\nAll the chatbots appeared to give conservative suggestions. For example, one participant tried to ask\ndiet suggestions for weight loss, but the chatbot with the job identity as a fitness coach suggested\n23\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nthat ‚Äúit‚Äôs always best to speak with a doctor or registered dietitian before starting any weight loss\nplan. ‚Äù However, we also found some instances where chatbots failed to detect participants‚Äô ‚Äúteasing\nand nonsensical‚Äù questions and gave advice that could potentially be dangerous to follow. For\nexample, to a participant who said\n ‚ÄúI want to gain 50 kg of pure fat by the end of the year. How\nmany snicker bars should I eat to complete that goal? ‚Äù, the chatbot responded with a semantically\nproblematic message:\n ‚ÄúIf you‚Äôre trying to eat 3000 calories a day and you‚Äôre only burning 2000,\nthen eating 3 snickers bars a day (each bar has around 1000 calories) could help you reach your goal . ‚Äù\nThis message not only contains the incorrect fact ( i.e., the calories of a snicker bar) but also is\nirrational. This example suggests the importance of giving precautions to users that the chatbots‚Äô\nmessages do not guarantee medical or professional soundness [48].\n5.5 Study Limitation\nDue to the limited number of participants, we did not perfectly counterbalance the order of topics.\nFatigue effects may not be fully mitigated for Food and Sleep topics which always came after Work\nand Exercise, respectively. Similarly, participants always conversed in the Positive conversation\npath before the Negative one, although we believe that having consistent path orders would cause\nless confusion and mistakes. Also, while we instructed participants to follow given conversation\npaths, some participants might not perfectly comply with the guides, possibly affecting significance\nof the pairwise comparisons. The targeted information slots consisted of only time, scale, binary,\nand open-ended data types. Incorporating other types of questions such as multiple choices may\ninfluence the chatbots‚Äô performance. In addition, slots in each topic had different composition of\ndata types, so any differences among topics might be influenced by both the lexicon of the topic\nand the composition of data types. Also, we did not control the conversation style of participants.\nSince user inputs also become part of the prompts, their linguistic patterns may affect GPT-3‚Äôs\ngenerations and in turn the slot filling performance or the conversation style of chatbots itself.\nWe chose GPT-3 as the underlying LLM for our chatbots as it is mainstream and publicly accessible\nvia a commercial API at the time of study. Although the model we used shows overall state-of-the-\nart performance in accuracy, robustness, and fairness (c.f., [60]), given that LLMs can be sensitive to\nprompt designs [65], we reckon that our proposed prompts may not yield similar performance on\nother LLMs due to the differences in the training corpora and the model architecture. For example,\nnewer LLMs that are improved to follow instructions in a prompt (e.g., text-davinci-003 [78]) or\noptimized for dialogues (e.g., ChatCompletion models like gpt-3.5-turbo and gpt-4 [81]) may\nbe more diligent in filling slots. Therefore, future work may consider powering chatbots on other\nLLMs, with our proposed prompts as a starting point.\n5.6 Future Work\nFuture work can explore ways to improve the performance of LLM-driven chatbots. In our study, we\nadopted zero-shot prompts. Researchers can try augmenting our prompts with few-shot learning\nby providing example dialogues [17], which may make chatbots have more robust question-asking\nabilities and can handle negative paths better [12]. Measuring chatbots requires great human efforts\nso more future research into the effects of these parameters on prompts is needed to provide\nguidance for the development of better and more robust chatbots. Researchers can also investigate\nmulti-stage prompting [109, 114] (i.e., designing several prompts for different questions in one\ndialogue session) if they intend to collect more than 5 slots of information. Such approaches will\nrequire incorporating dialog state tracking techniques (e.g., [63]) for automated slot filling. Lastly,\nwe hope future research can investigate the user perceptions of LLM-driven chatbots, or even\nvoice-based ones like smart speakers [106].\n24\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nIn this study, we focused exploring the chatbots‚Äô performance and behaviors rather than the\nuser experience. Several participants were impressed by some of the chatbots‚Äô responses but were\ndisappointed with repetitive messages at the same time. Hence, we are interested in seeing how\npeople will interact with an improved version of our chatbots and whether their mental models\nof chatbots will change along with the advancement of chatbots [ 62]. In addition, comparing\nuser perception of LLM-driven chatbots with other mainstream chatbot frameworks ( c.f., [77])\nwould provide holistic design implications for self-reporting chatbots with balanced data collection\nperformance and user perception.\n6 CONCLUSION\nIn this study, we explored how we can use GPT-3 for powering chatbots that can reliably ask\npeople health-related questions through natural conversations. In an empirical user study, we\nfound that, simply through prompting, LLMs-based chatbots could effectively deliver questions\nand collect desired self-reports. Particularly, we evaluated how two prompt design factors‚Äîformat\nand personality modifier‚Äìimpacted the resulted chatbots‚Äô ability in slot filling and conversation\nstyles. While LLMs can be a promising tool to build chatbots, we also discuss problematic messages\noccurred in our dataset. Reflecting on our results, we provide insights into the prompt design for\nchatbots and give suggestions on how to improve future LLMs-based chatbots. In closing, we hope\nthis work can inform and inspire other researchers in the fields of CSCW and Personal Informatics,\nto see the potential of LLMs in powering enjoyable chatbots for robust data collection.\nACKNOWLEDGMENTS\nWe thank our study participants for their time and efforts. We are also grateful to Eunkyung Jo and\nVassilis Kostakos, who provided feedback on this paper.\nREFERENCES\n[1] 2021. Auto-GPT. https://github.com/Significant-Gravitas/Auto-GPT. GitHub repository.\n[2] 2022. Chatgpt is a tipping point for AI. https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai\n[3] 2023. ChatGPT passes 1B page views. https://aibusiness.com/nlp/chatgpt-passes-1b-page-views.\n[4] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv\nKulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint\narXiv:2001.09977 (2020).\n[5] Sam Altman. 2022. CHATGPT launched on Wednesday. Today it crossed 1 million users! https://twitter.com/sama/\nstatus/1599668808285028353\n[6] Amazon. 2022. Amazon Alexa Voice AI. Retrieved Dec 04, 2022 from https://developer.amazon.com/en-US/alexa\n[7] Amazon.com, Inc. 2022. Amazon Alexa Voice AI. Retrieved Dec 04, 2022 from https://developer.amazon.com/en-\nUS/alexa\n[8] Apple Inc. 2022. SIRI shortcuts boost health and fitness routines - Apple News Room. Retrieved Dec 04, 2022 from\nhttps://www.apple.com/newsroom/2019/03/siri-shortcuts-boost-health-and-fitness-routines/\n[9] Zahra Ashktorab, Mohit Jain, Q Vera Liao, and Justin D Weisz. 2019. Resilient chatbots: Repair strategy preferences\nfor conversational breakdowns. In Proceedings of the 2019 CHI conference on human factors in computing systems .\n1‚Äì12.\n[10] Jacob Austin. 2022. We found that code models get better when you prompt them with \"I‚Äôm an expert python\nprogrammer\". the new anthropic paper did something similar, prefixing the model‚Äôs response with \"I‚Äôve tested this\nfunction myself so I know that it‚Äôs correct:\". https://twitter.com/jacobaustin132/status/1515063524258627586\n[11] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, et al . 2021. Program synthesis with large language models. arXiv preprint\narXiv:2108.07732 (2021).\n[12] Sanghwan Bae, Donghyun Kwak, Sungdong Kim, Donghoon Ham, Soyoung Kang, Sang-Woo Lee, and Woomyoung\nPark. 2022. Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models.\narXiv preprint arXiv:2205.00176 (2022).\n25\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\n[13] Florian Bemmann, Ramona Sch√∂del, Niels Van Berkel, and Daniel Buschek. 2021. Chatbots for Experience Sampling-\nInitial Opportunities and Challenges.. In IUI Workshops.\n[14] Erin Beneteau, Olivia K Richards, Mingrui Zhang, Julie A Kientz, Jason Yip, and Alexis Hiniker. 2019. Communication\nbreakdowns between families and Alexa. In Proceedings of the 2019 CHI conference on human factors in computing\nsystems. 1‚Äì13.\n[15] Michael Braun, Anja Mainz, Ronee Chadowitz, Bastian Pfleging, and Florian Alt. 2019. At your service: Designing\nvoice assistant personalities to improve automotive user interfaces. In Proceedings of the 2019 CHI Conference on\nHuman Factors in Computing Systems . 1‚Äì11.\n[16] Susan E Brennan. 1990. Conversation as direct manipulation: An iconoclastic view. The art of human-computer\ninterface design (1990), 393‚Äì404.\n[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877‚Äì1901.\n[18] Irene Celino and Gloria Re Calegari. 2020. Submitting surveys via a conversational interface: an evaluation of user\nacceptance and approach effectiveness. International Journal of Human-Computer Studies 139 (2020), 102410.\n[19] Harrison Chase. 2023. Langchain. https://github.com/hwchase17/langchain.\n[20] Janghee Cho and Emilee Rader. 2020. The role of conversational grounding in supporting symbiosis between people\nand digital assistants. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1‚Äì28.\n[21] Eun Kyoung Choe, Saeed Abdullah, Mashfiqui Rabbi, Edison Thomaz, Daniel A Epstein, Felicia Cordeiro, Matthew\nKay, Gregory D Abowd, Tanzeem Choudhury, James Fogarty, Bongshin Lee, Mark Matthews, and Julie A Kientz.\n2017. Semi-Automated Tracking: A Balanced Approach for Self-Monitoring Applications. IEEE Pervasive Computing\n16, 1 (Jan. 2017), 74‚Äì84. https://doi.org/10.1109/MPRV.2017.18\n[22] Eun Kyoung Choe, Nicole B Lee, Bongshin Lee, Wanda Pratt, and Julie A Kientz. 2014. Understanding quantified-\nselfers‚Äô practices in collecting and exploring personal data. In Proceedings of the SIGCHI conference on human factors\nin computing systems . 1143‚Äì1152.\n[23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua\nMaynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du,\nBen Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju\nDuke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways.\nhttps://doi.org/10.48550/ARXIV.2204.02311\n[24] John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan Adar, and Minsuk Chang. 2022. TaleBrush:\nSketching Stories with Generative Pretrained Language Models. In CHI Conference on Human Factors in Computing\nSystems. 1‚Äì19.\n[25] Felicia Cordeiro, Daniel A Epstein, Edison Thomaz, Elizabeth Bales, Arvind K Jagannathan, Gregory D Abowd, and\nJames Fogarty. 2015. Barriers and negative nudges: Exploring challenges in food journaling. In Proceedings of the\n33rd annual ACM conference on human factors in computing systems . 1159‚Äì1162.\n[26] Lei Cui, Shaohan Huang, Furu Wei, Chuanqi Tan, Chaoqun Duan, and Ming Zhou. 2017. Superagent: A customer\nservice chatbot for e-commerce websites. In Proceedings of ACL 2017, system demonstrations . 97‚Äì102.\n[27] Tilman Dingler, Dominika Kwasnicka, Jing Wei, Enying Gong, and Brian Oldenburg. 2021. The use and promise of\nconversational agents in digital health. Yearbook of Medical Informatics 30, 01 (2021), 191‚Äì199.\n[28] Daniel A Epstein, Daniel Avrahami, and Jacob T Biehl. 2016. Taking 5: Work-Breaks, Productivity, and Opportunities\nfor Personal Informatics for Knowledge Workers. In Proceedings of the 2016 CHI Conference on Human Factors in\nComputing Systems (CHI ‚Äô16) . ACM Press, New York, NY, USA, 673‚Äì684. https://doi.org/10.1145/2858036.2858066\n[29] Kathleen Kara Fitzpatrick, Alison Darcy, and Molly Vierhile. 2017. Delivering cognitive behavior therapy to young\nadults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): a randomized\ncontrolled trial. JMIR mental health 4, 2 (2017), e7785.\n[30] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts:\nEvaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462 (2020).\n[31] Ulrich Gnewuch, Stefan Morana, Marc Adam, and Alexander Maedche. 2018. Faster is not always better: understanding\nthe effect of dynamic response delays in human-chatbot interaction. (2018).\n26\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\n[32] Google. 2022. Build Chatbots with Dialogflow. Retrieved Dec 04, 2022 from https://developers.google.com/learn/\npathways/chatbots-dialogflow\n[33] Google. 2022. DialogFlow | Google Cloud. Retrieved Dec 04, 2022 from https://cloud.google.com/dialogflow/docs/\n[34] Google. 2022. Google Assistant, Your Own Personal Google. Retrieved Dec 04, 2022 from https://assistant.google.com\n[35] Sarah Homewood, Amanda Karlsson, and Anna Vallg√•rda. 2020. Removal as a method: A fourth wave HCI approach\nto understanding the experience of self-tracking. In Proceedings of the 2020 ACM Designing Interactive Systems\nConference. 1779‚Äì1791.\n[36] IBM. 2022. IBM Watson. Retrieved Dec 04, 2022 from https://www.ibm.com/watson\n[37] Mohit Jain, Pratyush Kumar, Ramachandra Kota, and Shwetak N Patel. 2018. Evaluating and informing the design of\nchatbots. In Proceedings of the 2018 designing interactive systems conference . 895‚Äì906.\n[38] Jae Ho Jeon. 2016. OmniTrack: Designing Flexible and Highly Customizable Quantified-Self Tool . MS thesis. Seoul\nNational University, Seoul, Korea. http://www.riss.kr/link?id=T14226449\n[39] Eunkyung Jo, Daniel A Epstein, Hyunhoon Jung, and Young-ho Kim. 2023. Understanding the Benefits and Challenges\nof Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention. In Proceedings of\nthe 2023 CHI Conference on Human Factors in Computing Systems , Vol. 1. ACM, New York, NY, USA, 1‚Äì16. https:\n//doi.org/10.1145/3544548.3581503\n[40] Juju, inc. 2022. Cognitive AI Chatbot. Retrieved Dec 04, 2022 from https://juji.io/\n[41] Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Jeon Dong Hyeon, Sunghyun Park,\nSungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk Hyun\nKo, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin\nSuh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park,\nMin Young Lee, Jaewook Kang, Inho Kang, Jung-Woo Ha, Woomyoung Park, and Nako Sung. 2021. What Changes Can\nLarge-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained\nTransformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association\nfor Computational Linguistics, Online and Punta Cana, Dominican Republic, 3405‚Äì3424. https://doi.org/10.18653/v1/\n2021.emnlp-main.274\n[42] Soomin Kim, Joonhwan Lee, and Gahgene Gweon. 2019. Comparing data from chatbot and web surveys: Effects of\nplatform and conversational style on survey response quality. In Proceedings of the 2019 CHI conference on human\nfactors in computing systems . 1‚Äì12.\n[43] Young-Ho Kim, Eun Kyoung Choe, Bongshin Lee, and Jinwook Seo. 2019. Understanding Personal Productivity: How\nKnowledge Workers Define, Evaluate, and Reflect on Their Productivity. InProceedings of the 2019 CHI Conference\non Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI ‚Äô19) . ACM, New York, NY, USA, Article 615,\n12 pages. https://doi.org/10.1145/3290605.3300845\n[44] Young-Ho Kim, Diana Chou, Bongshin Lee, Margaret Danilovich, Amanda Lazar, David E. Conroy, Hernisa Kacorri,\nand Eun Kyoung Choe. 2022. MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch\nwith Speech. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA,\nUSA) (CHI ‚Äô22) . ACM, New York, NY, USA. https://doi.org/10.1145/3491102.3517457\n[45] Young-Ho Kim, Jae Ho Jeon, Eun Kyoung Choe, Bongshin Lee, KwonHyun Kim, and Jinwook Seo. 2016. TimeAware:\nLeveraging Framing Effects to Enhance Personal Productivity. In Proceedings of the 2016 CHI Conference on Human\nFactors in Computing Systems (Santa Clara, California, USA) (CHI ‚Äô16) . ACM, New York, NY, USA, 272‚Äì283. https:\n//doi.org/10.1145/2858036.2858428\n[46] Young-Ho Kim, Jae Ho Jeon, Bongshin Lee, Eun Kyoung Choe, and Jinwook Seo. 2017. OmniTrack: A Flexible\nSelf-Tracking Approach Leveraging Semi-Automated Tracking.Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.\n1, 3, Article 67 (Sept. 2017), 28 pages. https://doi.org/10.1145/3130930\n[47] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language\nModels are Zero-Shot Reasoners. In ICML 2022 Workshop on Knowledge Retrieval and Language Models . https:\n//openreview.net/forum?id=6p3AuaHAFiN\n[48] Diane M Korngiebel and Sean D Mooney. 2021. Considering the possibilities and pitfalls of Generative Pre-trained\nTransformer 3 (GPT-3) in healthcare delivery. NPJ Digital Medicine 4, 1 (2021), 1‚Äì3.\n[49] Brigitte Krenn, Birgit Endrass, Felix Kistler, and Elisabeth Andr√©. 2014. Effects of language variety on personality\nperception in embodied conversational agents. In International Conference on Human-Computer Interaction . Springer,\n429‚Äì439.\n[50] Charlotte Laborde, Erta Cenko, Mamoun Mardini, Sanjay Ranka, Parisa Rashidi, and Todd Manini. 2020. Older Adults‚Äô\nSatisfaction and Compliance of Smartwatches Providing Ecological Momentary. Innovation in Aging 4, Suppl 1 (2020),\n799.\n[51] Reed Larson and Mihaly Csikszentmihalyi. 2014. The Experience Sampling Method. In Flow and the foundations of\npositive psychology. Springer, Berlin/Heidelberg, Germany, 21‚Äì34.\n27\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\n[52] Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021. Dialogue State Tracking with a Language Model using\nSchema-Driven Prompting. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Online and Punta Cana, Dominican Republic, 4937‚Äì4949. https://doi.org/\n10.18653/v1/2021.emnlp-main.404\n[53] Mina Lee, Percy Liang, and Qian Yang. 2022. CoAuthor: Designing a Human-AI Collaborative Writing Dataset\nfor Exploring Language Model Capabilities. In CHI Conference on Human Factors in Computing Systems (New\nOrleans, LA, USA) (CHI ‚Äô22) . Association for Computing Machinery, New York, NY, USA, Article 388, 19 pages.\nhttps://doi.org/10.1145/3491102.3502030\n[54] Min Kyung Lee, Sara Kiesler, Jodi Forlizzi, Siddhartha Srinivasa, and Paul Rybski. 2010. Gracefully mitigating\nbreakdowns in robotic services. In 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI) .\nIEEE, 203‚Äì210.\n[55] Peter Lee, Sebastien Bubeck, and Joseph Petro. 2023. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for\nMedicine. New England Journal of Medicine 388, 13 (2023), 1233‚Äì1239.\n[56] Russell V. Lenth, Paul Buerkner, Maxime Herve, Jonathon Love, Hannes Riebl, and Henrik Singmann. 2021.emmeans:\nEstimated Marginal Means, aka Least-Squares Means . CRAN. https://CRAN.R-project.org/package=emmeans\n[57] Chi-Hsun Li, Su-Fang Yeh, Tang-Jie Chang, Meng-Hsuan Tsai, Ken Chen, and Yung-Ju Chang. 2020. A conversation\nanalysis of non-progress and coping strategies with a banking task-oriented chatbot. In Proceedings of the 2020 CHI\nConference on Human Factors in Computing Systems . 1‚Äì12.\n[58] Ian Li, Anind Dey, and Jodi Forlizzi. 2010. A Stage-based Model of Personal Informatics Systems. In Proceedings of the\nSIGCHI Conference on Human Factors in Computing Systems (Atlanta, Georgia, USA) (CHI ‚Äô10) . ACM, New York, NY,\nUSA, 557‚Äì566. https://doi.org/10.1145/1753326.1753409\n[59] Ian Li, Anind K Dey, and Jodi Forlizzi. 2011. Understanding my data, myself: supporting self-reflection with ubicomp\ntechnologies. In Proceedings of the 13th international conference on Ubiquitous computing . 405‚Äì414.\n[60] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\nNarayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,\nChristopher D. Manning, Christopher R√©, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal\nLadhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert\nYuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang,\nRyan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang,\nVishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic Evaluation\nof Language Models. https://doi.org/10.48550/ARXIV.2211.09110\n[61] Q Vera Liao, Matthew Davis, Werner Geyer, Michael Muller, and N Sadat Shami. 2016. What can you do? Studying\nsocial-agent orientation and agent proactive interactions with an agent for employees. In Proceedings of the 2016 acm\nconference on designing interactive systems . 264‚Äì275.\n[62] Q Vera Liao, Muhammed Mas-ud Hussain, Praveen Chandar, Matthew Davis, Yasaman Khazaeni, Marco Patricio\nCrasso, Dakuo Wang, Michael Muller, N Sadat Shami, and Werner Geyer. 2018. All work and no play? Conversations\nwith a Question-and-Answer Chatbot in the Wild. In Proceedings of the 2018 CHI Conference on Human Factors in\nComputing Systems . 1‚Äì13.\n[63] Zhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan Moon, Zhenpeng Zhou, Paul A Crook, Zhiguang Wang, Zhou\nYu, Eunjoon Cho, Rajen Subba, et al. 2021. Zero-Shot Dialogue State Tracking via Cross-Task Transfer. InProceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7890‚Äì7900.\n[64] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What Makes Good\nIn-Context Examples for GPT-3? https://doi.org/10.48550/ARXIV.2101.06804\n[65] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, Prompt,\nand Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. https://doi.org/10.48550/\nARXIV.2107.13586\n[66] Vivian Liu and Lydia B Chilton. 2022. Design Guidelines for Prompt Engineering Text-to-Image Generative Models.\nIn CHI Conference on Human Factors in Computing Systems . 1‚Äì23.\n[67] Ewa Luger and Abigail Sellen. 2016. ‚ÄúLike Having a Really Bad PA‚Äù The Gulf between User Expectation and Experience\nof Conversational Agents. InProceedings of the 2016 CHI conference on human factors in computing systems . 5286‚Äì5297.\n[68] Yuhan Luo, Young-Ho Kim, Bongshin Lee, Naeemul Hassan, and Eun Kyoung Choe. 2021. FoodScrap: Promoting\nRich Data Capture and Reflective Food Journaling Through Speech Input. In Designing Interactive Systems Conference\n2021. 606‚Äì618.\n[69] Yuhan Luo, Bongshin Lee, and Eun Kyoung Choe. 2020. TandemTrack: shaping consistent exercise experience by\ncomplementing a mobile app with a smart speaker. In Proceedings of the 2020 CHI Conference on Human Factors in\nComputing Systems . 1‚Äì13.\n28\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\n[70] Lucas M. Silva and Daniel A. Epstein. 2021. Investigating Preferred Food Description Practices in Digital Food\nJournaling. In Designing Interactive Systems Conference 2021 (Virtual Event, USA) (DIS ‚Äô21). Association for Computing\nMachinery, New York, NY, USA, 589‚Äì605. https://doi.org/10.1145/3461778.3462145\n[71] Michael McTear. 2018. Conversational modelling for chatbots: current approaches and future directions. Studientexte\nzur Sprachkommunikation: Elektronische Sprachsignalverarbeitung (2018), 175‚Äì185.\n[72] Shikib Mehri, Yasemin Altun, and Maxine Eskenazi. 2022. LAD: Language Models as Data for Zero-Shot Dialog.\narXiv preprint arXiv:2207.14393 (2022).\n[73] Meta Platforms, Inc. 2022. React ‚Äì A JavaScript library for building user interfaces. Retrieved Dec 04, 2022 from\nhttps://reactjs.org/\n[74] Microsoft. 2022. TypeScript. Retrieved Dec 04, 2022 from https://www.typescriptlang.org\n[75] Jun-Ki Min, Afsaneh Doryab, Jason Wiese, Shahriyar Amini, John Zimmerman, and Jason I Hong. 2014. Toss‚Äôn‚Äôturn:\nsmartphone as sleep and sleep quality detector. InProceedings of the SIGCHI conference on human factors in computing\nsystems. 477‚Äì486.\n[76] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.\nRethinking the Role of Demonstrations: What Makes In-Context Learning Work? arXiv preprint arXiv:2202.12837\n(2022).\n[77] Elliot Mitchell, Noemie Elhadad, and Lena Mamykina. 2022. Examining AI Methods for Micro-Coaching Dialogs. In\nCHI Conference on Human Factors in Computing Systems . 1‚Äì24.\n[78] Ryan Morrison. 2022. GPT-3 developer OpenAI releases new Davinci generative text model. Retrieved Dec 04, 2022\nfrom https://techmonitor.ai/technology/ai-and-automation/gpt-3-openai-davinci-generative-text\n[79] Chelsea Myers, Anushay Furqan, Jessica Nebolsky, Karina Caro, and Jichen Zhu. 2018. Patterns for how users\novercome obstacles in voice user interfaces. In Proceedings of the 2018 CHI conference on human factors in computing\nsystems. 1‚Äì7.\n[80] OpenAI. 2022. OpenAI API. Retrieved Dec 04, 2022 from https://openai.com/api/\n[81] OpenAI, Inc. 2023. GPT models - OpenAI. Retrieved Dec 04, 2022 from https://platform.openai.com/docs/guides/gpt\n[82] Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein.\n2022. Social Simulacra: Creating Populated Prototypes for Social Computing Systems. In In the 35th Annual ACM\nSymposium on User Interface Software and Technology (UIST ‚Äô22) (Bend, OR, USA)(UIST ‚Äô22). Association for Computing\nMachinery, New York, NY, USA. https://doi.org/10.1145/3526113.3545616\n[83] Jos√© Pinheiro and Douglas Bates. 2000. Mixed-Effects Models in S and S-PLUS (1 ed.). Springer-Verlag, New York. 528\npages. https://doi.org/10.1007/b98882\n[84] Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2018. Towards empathetic open-domain\nconversation models: A new benchmark and dataset. arXiv preprint arXiv:1811.00207 (2018).\n[85] Jungwook Rhim, Minji Kwak, Yeaeun Gong, and Gahgene Gweon. 2022. Application of humanization to survey\nchatbots: Change in chatbot perception, interaction experience, and survey data quality. Computers in Human\nBehavior 126 (2022), 107034.\n[86] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmer‚Äôs\nassistant: Conversational interaction with a large language model for software development. In Proceedings of the\n28th International Conference on Intelligent User Interfaces . 491‚Äì514.\n[87] Kathryn Roulston and Myungweon Choi. 2018. Qualitative interviews. The SAGE handbook of qualitative data\ncollection (2018), 233‚Äì249.\n[88] Ryan M Schuetzler, G Mark Grimes, and Justin Scott Giboney. 2018. An investigation of conversational agent\nrelevance, presence, and engagement. (2018).\n[89] Abigail See, Stephen Roller, Douwe Kiela, and Jason Weston. 2019. What makes a good conversation? how controllable\nattributes affect human judgments. arXiv preprint arXiv:1902.08654 (2019).\n[90] Woosuk Seo, Chanmo Yang, and Young-Ho Kim. 2023. ChaCha: Leveraging Large Language Models to Prompt\nChildren to Share Their Emotions about Personal Events. arXiv:2309.12244 [cs.HC]\n[91] Alexander Serenko. 2008. A model of user adoption of interface agents for email notification. Interacting with\nComputers 20, 4-5 (2008), 461‚Äì472.\n[92] Weiyan Shi, Xuewei Wang, Yoo Jung Oh, Jingwen Zhang, Saurav Sahay, and Zhou Yu. 2020. Effects of persuasive\ndialogues: testing bot identities and inquiry strategies. In Proceedings of the 2020 CHI Conference on Human Factors in\nComputing Systems . 1‚Äì13.\n[93] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces\nhallucination in conversation. arXiv preprint arXiv:2104.07567 (2021).\n[94] Nina Svenningsson and Montathar Faraon. 2019. Artificial intelligence in conversational agents: A study of factors\nrelated to perceived humanness in chatbots. In Proceedings of the 2019 2nd Artificial Intelligence and Cloud Computing\nConference. 151‚Äì161.\n29\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\n[95] Vivian Ta, Caroline Griffith, Carolynn Boatfield, Xinyu Wang, Maria Civitello, Haley Bader, Esther DeCero, Alexia\nLoggarakis, et al. 2020. User experiences of social support from companion chatbots in everyday contexts: thematic\nanalysis. Journal of medical Internet research 22, 3 (2020), e16235.\n[96] Ana√Øs Tack and Chris Piech. 2022. The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in\nEducational Dialogues. arXiv preprint arXiv:2205.07540 (2022).\n[97] The OpenJS Foundation. 2022. Node.js. Retrieved Dec 04, 2022 from https://nodejs.org\n[98] Sandeep A Thorat and Vishakha Jadhav. 2020. A review on implementation issues of rule-based chatbot systems. In\nProceedings of the International Conference on Innovative Computing & Communications (ICICC) .\n[99] Xiaoyi Tian, Zak Risha, Ishrat Ahmed, Arun Balajiee Lekshmi Narayanan, and Jacob Biehl. 2021. Let‚Äôs talk it out:\nA chatbot for effective study habit behavioral change. Proceedings of the ACM on Human-Computer Interaction 5,\nCSCW1 (2021), 1‚Äì32.\n[100] David R Traum. 2000. 20 questions on dialogue act taxonomies. Journal of semantics 17, 1 (2000), 7‚Äì30.\n[101] Saeid Ashraf Vaghefi, Qian Wang, Veruska Muccione, Jingwei Ni, Mathias Kraus, Julia Bingler, Tobias Schimanski,\nChiara Colesanti-Senni, Dominik Stammbach, Nicolas Webersinke, et al. 2023. Chatclimate: Grounding conversational\nAI in climate science. (2023).\n[102] Sarah Theres V√∂lkel, Ramona Sch√∂del, Daniel Buschek, Clemens Stachl, Verena Winterhalter, Markus B√ºhner,\nand Heinrich Hussmann. 2020. Developing a personality model for speech-based conversational agents using the\npsycholexical approach. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1‚Äì14.\n[103] Ryan Volum, Sudha Rao, Michael Xu, Gabriel DesGarennes, Chris Brockett, Benjamin Van Durme, Olivia Deng,\nAkanksha Malhotra, and William B Dolan. 2022. Craft an Iron Sword: Dynamically Generating Interactive Game\nCharacters by Prompting Large Language Models Tuned on Code. InProceedings of the 3rd Wordplay: When Language\nMeets Games Workshop (Wordplay 2022) . 25‚Äì43.\n[104] Bryan Wang, Gang Li, and Yang Li. 2023. Enabling conversational interaction with mobile ui using large language\nmodels. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1‚Äì17.\n[105] Lu Wang, Munif Ishad Mujib, Jake Williams, George Demiris, and Jina Huh-Yoo. 2021. An Evaluation of Generative\nPre-Training Model-based Therapy Chatbot for Caregivers. https://doi.org/10.48550/ARXIV.2107.13115\n[106] Jing Wei, Tilman Dingler, and Vassilis Kostakos. 2021. Understanding User Perceptions of Proactive Smart Speakers.\nProceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 5, 4 (2021), 1‚Äì28.\n[107] Jing Wei, Weiwei Jiang, Chaofan Wang, Difeng Yu, Jorge Goncalves, Tilman Dingler, and Vassilis Kostakos. 2022.\nUnderstanding How to Administer Voice Surveys through Smart Speakers. Proceedings of the ACM on Human-\nComputer Interaction 6, CSCW2 (2022), 1‚Äì32.\n[108] Jing Wei, Benjamin Tag, Johanne R Trippas, Tilman Dingler, and Vassilis Kostakos. 2022. What Could Possibly Go\nWrong When Interacting with Proactive Smart Speakers? A Case Study Using an ESM Application. InCHI Conference\non Human Factors in Computing Systems . 1‚Äì15.\n[109] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of\nthought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022).\n[110] Anuradha Welivita and Pearl Pu. 2020. A taxonomy of empathetic response intents in human social conversations.\narXiv preprint arXiv:2012.04080 (2020).\n[111] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2019. Neural Text\nGeneration with Unlikelihood Training. https://doi.org/10.48550/ARXIV.1908.04319\n[112] Patti Williams, Lauren G Block, and Gavan J Fitzsimons. 2006. Simply asking questions about health behaviors\nincreases both healthy and unhealthy behaviors. Social Influence 1, 2 (2006), 117‚Äì127.\n[113] Cornelia Wrzus and Matthias R Mehl. 2015. Lab and/or field? Measuring personality processes and their social\nconsequences. European Journal of Personality 29, 2 (2015), 250‚Äì271.\n[114] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transparent and controllable human-ai\ninteraction by chaining large language model prompts. In CHI Conference on Human Factors in Computing Systems .\n1‚Äì22.\n[115] Ziang Xiao, Michelle X Zhou, Wenxi Chen, Huahai Yang, and Changyan Chi. 2020. If i hear you correctly: Building\nand evaluating interview chatbots with active listening skills. In Proceedings of the 2020 CHI Conference on Human\nFactors in Computing Systems . 1‚Äì14.\n[116] Ziang Xiao, Michelle X Zhou, Q Vera Liao, Gloria Mark, Changyan Chi, Wenxi Chen, and Huahai Yang. 2020. Tell me\nabout yourself: Using an AI-powered chatbot to conduct conversational surveys with open-ended questions. ACM\nTransactions on Computer-Human Interaction (TOCHI) 27, 3 (2020), 1‚Äì37.\n[117] JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny can‚Äôt prompt: how\nnon-AI experts try (and fail) to design LLM prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems . 1‚Äì21.\n30\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\n[118] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona\nDiab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura,\nAnjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models.\nhttps://doi.org/10.48550/ARXIV.2205.01068\n[119] Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. 2020. The design and implementation of xiaoice, an empathetic\nsocial chatbot. Computational Linguistics 46, 1 (2020), 53‚Äì93.\n31\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nA APPENDICES\nA.1 The Web Chatbot Interface\nA.2 Hint Texts for Conversation Paths\nThe hint texts are displayed in blue (positive) or red (negative) in the instruction (See Appendix\nA.1, right).\nPositive Path Negative Path\nSleep You had good sleeplast night. You had bad sleeplast night.\nWork You were productive yesterday. You did nothingyesterday.\nFood Intake You had all three mealsyesterday. You skipped one mealyesterday.\nExercise You exercised yesterday. You did not exerciseyesterday.\n32\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nA.3 Least-Squared Mean Statistics for Slot Filling Rate\nEstimation based on a significant linear model formula:\nùëÜùëôùëúùë°ùêπùëñùëôùëôùëñùëõùëîùëÖùëéùë°ùëí = ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ +ùêπùëúùëüùëöùëéùë° +ùëáùëúùëùùëñùëê +ùëÉùëéùë°‚Ñé +ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ √óùêπùëúùëüùëöùëéùë°\nùêπ(7,366)= 6.493,ùëù < .0001***\nTable 8. Estimated least-squared means with 95% confidence intervals (CI) of slot filling rate by variables.\nVariable Category Est. Mean SE df Lower CI Upper CI\nCondition\nSP 83.03% 2.60 366 77.91 88.16\nSN 75.99% 2.54 366 71.00 80.98\nDP 71.90% 2.55 366 66.89 76.91\nDN 83.43% 2.59 366 78.33 88.52\nTopic\nSleep 77.35% 2.56 366 72.31 82.39\nWork 70.69% 2.61 366 65.57 75.81\nFood 77.91% 2.55 366 72.89 82.92\nExercise 88.40% 2.56 366 83.36 93.44\nPath Positive 81.88% 1.82 366 78.31 85.45\nNegative 75.30% 1.82 366 71.72 78.87\nA.4 Least-Squared Mean Statistics for Required and Secondary Question Turn Ratios\nA.4.1 Turn Ratio of Required Questions. Estimation based on a mixed-effect model fitted:\nùëÖùëÑùëáùë¢ùëüùëõùëÖùëéùë°ùëñùëú = ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ +ùêπùëúùëüùëöùëéùë° +ùëáùëúùëùùëñùëê +ùëÉùëéùë°‚Ñé +ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ √óùêπùëúùëüùëöùëéùë° +ùëáùëúùëùùëñùëê √óùëÉùëéùë°‚Ñé\nwith significant random effect of participants (ùëù < .0001***)\nTable 9. Estimated least-squared means with 95% confidence intervals (CI) of RQ turn ratio by variables.\nVariable Category Est. Mean SE df Lower CI Upper CI\nFormat Specific 22.44% 1.03 43.99 20.36 24.51\nDescriptive 19.21% 1.03 43.97 17.14 21.29\nCondition\nSP 23.93% 1.47 44.94 20.98 26.88\nSN 20.94% 1.45 43.05 18.02 23.86\nDP 17.63% 1.45 43.41 14.70 20.56\nDN 20.80% 1.46 44.54 17.85 23.74\nPath Positive 21.92% 0.86 82.65 20.22 23.63\nNegative 19.73% 0.86 82.53 18.02 21.43\nTopic √óPath\nSleep Positive 22.73% 1.38 295.42 20.01 25.45\nNegative 23.78% 1.41 301.42 21.00 26.55\nWork Positive 19.13% 1.44 307.50 16.30 21.96\nNegative 15.04% 1.40 298.34 12.29 17.78\nFood Positive 24.03% 1.38 295.42 21.31 26.75\nNegative 23.54% 1.40 298.34 20.79 26.28\nExercise Positive 21.81% 1.40 298.44 19.06 24.56\nNegative 16.55% 1.40 298.44 13.81 19.30\n33\nJing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim\nA.4.2 Turn Ratio of Secondary Questions. Estimation based on a significant linear model formula:\nùëÜùëÑùëáùë¢ùëüùëõùëÖùëéùë°ùëñùëú = ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ +ùêπùëúùëüùëöùëéùë° +ùëáùëúùëùùëñùëê +ùëÉùëéùë°‚Ñé +ùëáùëúùëùùëñùëê √óùëÉùëéùë°‚Ñé\nùêπ(9,364)= 10.04,ùëù < .0001***\nTable 10. Estimated least-squared means with 95% confidence intervals (CI) of SQ turn ratio by variables.\nVariable Category Est. Mean SE df Lower CI Upper CI\nFormat Specific 9.64% 0.68 364 8.30 10.97\nDescriptive 12.55% 0.68 364 11.22 13.88\nCondition\nSP 9.26% 0.84 364 7.62 10.91\nSN 10.01% 0.82 364 8.40 11.63\nDP 12.18% 0.82 364 10.56 13.80\nDN 12.93% 0.83 364 11.29 14.57\nPath Positive 9.59% 0.68 364 8.26 10.92\nNegative 12.60% 0.68 364 11.27 13.93\nTopic √óPath\nSleep Positive 7.71% 1.34 364 5.08 10.33\nNegative 7.17% 1.36 364 4.49 9.85\nWork Positive 14.23% 1.39 364 11.49 16.98\nNegative 20.03% 1.35 364 17.38 22.68\nFood Positive 9.66% 1.34 364 7.04 12.29\nNegative 9.48% 1.35 364 6.83 12.13\nExercise Positive 6.76% 1.35 364 4.11 9.42\nNegative 13.70% 1.35 364 11.05 16.35\nA.5 Least-Squared Mean Statistics for Chatbot Behavioral Turn Ratios\nA.5.1 Chatbot‚Äôs Acknowledging Turn Ratio. Estimation based on a significant linear model formula:\nùê¥ùëêùëòùëõùëúùë§ùëôùëíùëëùëîùëñùëõùëîùëáùë¢ùëüùëõùëÖùëéùë°ùëñùëú = ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ +ùêπùëúùëüùëöùëéùë° +ùëáùëúùëùùëñùëê +ùëÉùëéùë°‚Ñé +ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ √óùêπùëúùëüùëöùëéùë°\nùêπ(7,366)= 6.715,ùëù < .0001***\nTable 11. Estimated least-squared means with 95% confidence intervals (CI) of Acknowledging turn ratio by\nvariables.\nVariable Category Est. Mean SE df Lower CI Upper CI\nFormat Specific 17.44% 0.74 366 15.98 18.90\nDescriptive 22.56% 0.74 366 21.10 24.02\nPersonality Modifier With Modifier 20.85% 0.74 366 19.39 22.32\nNo Modifier 19.15% 0.74 366 17.69 20.61\nCondition\nSP 17.20% 1.06 366 15.11 19.30\nSN 17.68% 1.04 366 15.64 19.72\nDP 24.51% 1.04 366 22.46 26.55\nDN 20.62% 1.06 366 18.54 22.70\nPath Positive 17.99% 0.74 366 16.52 19.45\nNegative 22.02% 0.74 366 20.56 23.48\n34\nLeveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data\nA.5.2 Chatbot‚Äôs Appreciating Turn Ratio. Estimation based on a significant linear model formula:\nùê¥ùëùùëùùëüùëíùëêùëñùëéùë°ùëñùëõùëîùëáùë¢ùëüùëõùëÖùëéùë°ùëñùëú = ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ +ùêπùëúùëüùëöùëéùë° +ùëáùëúùëùùëñùëê +ùëÉùëéùë°‚Ñé +ùëáùëúùëùùëñùëê √óùêπùëúùëüùëöùëéùë°\nùêπ(9,364)= 8.903,ùëù < .0001***\nTable 12. Estimated least-squared means with 95% confidence intervals (CI) of Appreciating turn ratio by\nvariables.\nVariable Category Est. Mean SE df Lower CI Upper CI\nFormat Specific 7.61% 0.59 364 6.44 8.78\nDescriptive 9.64% 0.59 364 8.47 10.81\nPersonality Modifier With Modifier 9.36% 0.60 364 8.19 10.53\nNo Modifier 7.89% 0.59 364 6.73 9.06\nCondition\nSP 8.34% 0.73 364 6.89 9.78\nSN 6.87% 0.72 364 5.46 8.29\nDP 10.38% 0.72 364 8.95 11.80\nDN 8.91% 0.73 364 7.47 10.35\nPath Positive 11.33% 0.59 364 10.16 12.50\nNegative 5.92% 0.59 364 4.75 7.09\nA.5.3 Chatbot‚Äôs Sympathizing Turn Ratio. Estimation based on a significant linear model formula:\nùëÜùë¶ùëöùëùùëéùë°‚Ñéùëñùëßùëñùëõùëîùëáùë¢ùëüùëõùëÖùëéùë°ùëñùëú = ùëÉùëíùëüùë†ùëúùëõùëéùëôùëñùë°ùë¶ +ùêπùëúùëüùëöùëéùë° +ùëáùëúùëùùëñùëê +ùëÉùëéùë°‚Ñé +ùëáùëúùëùùëñùëê √óùëÉùëéùë°‚Ñé\nùêπ(9,364)= 8.219,ùëù < .0001***\nTable 13. Estimated least-squared means with 95% confidence intervals (CI) of Sympathizing turn ratio by\nvariables.\nVariable Category Est. Mean SE df Lower CI Upper CI\nFormat Specific 3.18% 0.39 364 2.41 3.96\nDescriptive 3.03% 0.39 364 2.26 3.81\nPersonality Modifier With Modifier 3.97% 0.40 364 3.19 4.75\nNo Modifier 2.25% 0.39 364 1.47 3.02\nCondition\nSP 4.04% 0.49 364 3.08 5.00\nSN 2.32% 0.48 364 1.38 3.27\nDP 3.89% 0.48 364 2.95 4.84\nDN 2.17% 0.49 364 1.22 3.13\nPath Positive 1.16% 0.40 364 0.39 1.94\nNegative 5.05% 0.39 364 4.28 5.83\n35"
}