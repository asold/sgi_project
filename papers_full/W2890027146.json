{
  "title": "What do RNN Language Models Learn about Filler–Gap Dependencies?",
  "url": "https://openalex.org/W2890027146",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A5011708753",
      "name": "Ethan Wilcox",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5090215557",
      "name": "Roger Lévy",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5041729199",
      "name": "Takashi Morita",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050104206",
      "name": "Richard Futrell",
      "affiliations": [
        "University of California, Irvine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2087946919",
    "https://openalex.org/W6676391964",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W2038536973",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W1595256356",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2113824248",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2788924045",
    "https://openalex.org/W2125001590",
    "https://openalex.org/W2963069010",
    "https://openalex.org/W608115925",
    "https://openalex.org/W1441030410",
    "https://openalex.org/W2129849249",
    "https://openalex.org/W2144188893",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W1987849848",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2005592929",
    "https://openalex.org/W2164895016",
    "https://openalex.org/W2141845152",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2561321768",
    "https://openalex.org/W2128334893",
    "https://openalex.org/W2100920050"
  ],
  "abstract": "RNN language models have achieved state-of-the-art perplexity results and have proven useful in a suite of NLP tasks, but it is as yet unclear what syntactic generalizations they learn. Here we investigate whether state-of-the-art RNN language models represent long-distance filler-gap dependencies and constraints on them. Examining RNN behavior on experimentally controlled sentences designed to expose filler-gap dependencies, we show that RNNs can represent the relationship in multiple syntactic positions and over large spans of text. Furthermore, we show that RNNs learn a subset of the known restrictions on filler-gap dependencies, known as island constraints: RNNs show evidence for wh-islands, adjunct islands, and complex NP islands. These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions.",
  "full_text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 211–221\nBrussels, Belgium, November 1, 2018.c⃝2018 Association for Computational Linguistics\n211\nWhat do RNN Language Models Learn about Filler–Gap Dependencies?\nEthan Wilcox 1, Roger Levy 2, T akashi Morita3,4, and Richard Futrell 5\n1Department of Linguistics, Harvard University , wilcoxeg@g.harvard.edu\n2Department of Brain and Cognitive Sciences, MIT , rplevy@mit.edu\n3Primate Research Institute, Kyoto University , tmorita@alum.mit.edu\n4Department of Linguistics and Philosophy , MIT\n5Department of Language Science, UC Irvine, rfutrell@uci.edu\nAbstract\nRNN language models have achieved state-\nof-the-art perplexity results and have proven\nuseful in a suite of NLP tasks, but it is as\nyet unclear what syntactic generalizations they\nlearn. Here we investigate whether state-of-\nthe-art RNN language models represent long-\ndistance ﬁller–gap dependencies and con-\nstraints on them. Examining RNN behavior\non experimentally controlled sentences de-\nsigned to expose ﬁller–gap dependencies, we\nshow that RNNs can represent the relation-\nship in multiple syntactic positions and over\nlarge spans of text. Furthermore, we show that\nRNNs learn a subset of the known restric-\ntions on ﬁller–gap dependencies, known as is-\nland constraints : RNNs show evidence for\nwh-islands, adjunct islands, and complex NP\nislands. These studies demonstrates that state-\nof-the-art RNN models are able to learn and\ngeneralize about empty syntactic positions.\n1 Introduction\nMany recent advancements in Natural Language\nProcessing have come from the introduction of\nRecurrent Neural Networks (RNN) (\nElman, 1990;\nGoldberg, 2017). One class of RNNs, the Long\nShort-T erm Memory RNN (LSTM) ( Hochreiter\nand Schmidhuber , 1997) has been able to achieve\nimpressive results on a suite of NLP tasks, includ-\ning machine translation, language modeling, and\nsyntactic parsing (\nSutskever et al. , 2014; V inyals\net al. , 2015; Jozefowicz et al. , 2016). But the na-\nture of the representations learned by these mod-\nels is not properly understood. As these models\nare being deployed with increasing frequency , this\nposes both engineering, accountability , and theo-\nretical problems.\nOne promising line of research aims to crack\nopen these ‘black boxes’ by investigating how\nLSTM language models perform on specially con-\ntrolled sentences designed to draw out behavior\nthat indicates representation of a syntactic depen-\ndency . Using this method,\nLinzen et al. (2016) and\nGulordava et al. (2018) demonstrated that these\nmodels are able to successfully learn the number\nagreement dependency between a subject and its\nverb, even when there are intervening elements,\nand\nMcCoy et al. (2018) found that RNNs learn\nthe hierarchical rules of English auxiliary inver-\nsion. In this paper, we broaden and deepen this line\nof inquiry by examining what LSTMs learn about\nan unexplored syntactic relationship: the ﬁller–gap\ndependency . The ﬁller–gap dependency is novel,\ninsofar as learning it requires the network to gen-\neralize about the absence of material.\nFor our purposes, ﬁller–gap dependency refers\nto a relationship between a ﬁller , which is a wh-\ncomplementizer such as ‘what’ or ‘who’, and a\ngap, which is an empty syntactic position licensed\nby the ﬁller. In example\n(1a), the ﬁller is ‘what’\nand the gap appears after ‘devoured’, indicated\nwith underscores. If the ﬁller were not present, the\ngap would be ungrammatical, as in\n(1b).\n(1) a. I know what the lion devoured at sunrise.\nb. *I know that the lion devoured at sunrise.\nThere is also a semantic relationship between the\nﬁller and the gap, in the sense that “what” is se-\nmantically the direct object of “devoured”. In this\nwork, we study the behavior of language models,\nand so we treat the ﬁller–gap dependency purely\nas a licensing relationship.\nElman (1991) found that simple distributed\nmodels have some success predicting post-verbal\ngaps in sentences containing object-extracted rel-\native clauses. However, correct representation\nof ﬁller–gap dependencies and the constraints\non them has proven challenging even in hand-\nengineered symbolic models. Furthermore, they\nare subject to numerous complex island con-\nstraints (\nRoss, 1967). Because of their complex-\n212\nity and ubiquity , these dependencies have ﬁg-\nured prominently in arguments that natural lan-\nguage would be unlearnable by children without\na great deal of innate knowledge (\nPhillips, 2013)\n(cf. Pearl and Sprouse , 2013; Ellefson and Chris-\ntiansen, 2000)\nThe remainder of the paper is structured as fol-\nlows. Section 2 presents our methods in more\ndetail. Section 3 gives evidence that LSTM lan-\nguage models represent the basic ﬁller–gap depen-\ndency in multiple syntactic positions despite in-\ntervening material. Section\n4 investigates whether\nLSTM language models are sensitive to various\nconstraints: wh-islands, adjunct islands, complex\nNP islands, and subject islands. W e ﬁnd that the\nlanguage models are sensitive to some but not all\nof these constraints. Section\n5 concludes.\n2 Methods\n2.1 Language models\nW e study the behavior of two pre-existing LSTMs\ntrained on a language modeling objective over En-\nglish text. Our ﬁrst model is presented in\nJozefow-\nicz et al. (2016) under the name BIG LSTM+CNN\nInputs; we call it the Google model . It was trained\non the One Billion W ord Benchmark ( Chelba\net al. , 2013) and has two hidden layers with 8196\nunits each. It uses the output of a character-level\nConvolutional Neural Network (CNN) as input to\nthe LSTM. This model has the best published per-\nplexity for English text. Our second model is the\none presented in the supplementary materials of\nGulordava et al. (2018), which we call the Gulor-\ndava model . Trained on 90 million tokens of En-\nglish Wikipedia, it has two hidden layers of 650\nunits each. Our goal in using these models is to\nprovide two samples of the state-of-the-art. As a\nbaseline, we also study an n-gram model trained\non the One Billion W ord Benchmark (a 5-gram\nmodel with modiﬁed Kneser-Ney interpolation,\nﬁt by KenLM with default parameters) (\nHeaﬁeld\net al. , 2013).\n2.2 Dependent variable: Surprisal\nW e investigate RNN behavior primarily by study-\ning the surprisal values that an RNN assigns to\nwords and sentences. Surprisal is log inverse prob-\nability:\nS(xi ) =− log2 p(xi |hi− 1),\nwhere xi is the current word or character, hi− 1 is\nthe RNN’s hidden state before consuming xi , and\nthe probability is calculated from the RNN’s soft-\nmax activation. The logarithm is taken in base 2,\nso that surprisal is measured in bits.\nThe degree of surprisal for a word or sentence\ntells us the extent to which that word or sentence\nis unexpected under the language model’s proba-\nbility distribution. It is known to correlate directly\nwith human sentence processing difﬁculty (\nHale,\n2001; Levy, 2008; Smith and Levy , 2013). In this\npaper, we look for cases where the surprisal asso-\nciated with an an unusual construction—such as a\ngap—is ameliorated by the presence of a licensor,\nsuch as a wh-word. If the models learn that syn-\ntactic gaps require licensing, then sentences with\nlicensors should exhibit lower surprisal than mini-\nmally different pairs that lack a proper licensor.\n2.3 Experimental design\nW e test whether the LSTM language models have\nlearned ﬁller–gap dependencies by looking for a\n2x2 interaction between the presence of a gap and\nthe presence of a wh-licensor. This interaction in-\ndicates the extent to which a wh-licensor reduces\nthe surprisal associated with a gap, so we call\nit the wh-licensing interaction . In studying con-\nstraints on ﬁller–gap dependencies, we look for\ninteractions between the wh-licensing interaction\nand other factors: for example, whether the wh-\nlicensing interaction decreases when a gap is in a\nsyntactic island position as opposed to a syntacti-\ncally licit position (Section\n4).\nW e use experimental items where the gap is lo-\ncated in an obligatory argument position, e.g. in\nsubject position or as the direct object of a tran-\nsitive verb, as judged by the authors. The phrase\nwith the gap is embedded inside a complement\nclause. W e chose this paradigm over bare wh-\nquestions because it eliminates do-support and\ntense manipulation of the main verb, resulting in\nhigher similarity across conditions. Each item ap-\npears in four conditions, reﬂecting a 2 × 2 exper-\nimental design manipulating presence of a wh-\nlicensor and presence of a gap. For example:\n1\n(2) a. I know that the lion devoured a gazelle at\nsunrise. [no wh-licensor, no gap]\nb. *I know what the lion devoured a gazelle at\nsunrise. [wh-licensor, no gap]\nc. *I know that the lion devoured at sunrise.\n[no wh-licensor, gap]\n1 W e indicate the gap position with underscores for expos-\nitory purposes, but these underscores were not included in\nexperimental items.\n213\nd. I know what the lion devoured at sunrise.\n[wh-licensor, gap]\nW e measure surprisal in two places: at the word\nimmediately following a (ﬁlled) gap and summed\nover the whole region from the gap to the end\nof the embedded clause. W e look at immediate-\nword surprisal because a gap’s licitness should\nhave local effects on network expectation. W e look\nat whole-region surprisal because the presence of\na ﬁller also changes expectations about overall\nwell-formedness of the sentence—a global phe-\nnomenon. Until the ﬁnal punctuation is reached\nin\n(2b) there are potential gap-containing contin-\nuations that render the sentence syntactically licit\n(e.g. ‘with\n. ’). Therefore, we might expect no\nlarge spike in surprisal at any one point, but small\nincreases in surprisal when the network encoun-\nters ﬁlled argument-structure roles and at the end\nof the sentence. Measuring summed surprisal cap-\ntures these distributed, global effects.\nIf the network is learning the licensing rela-\ntionship between ﬁllers and gaps then two things\nshould be true: First, if a wh-licensor sets up\na global expectation for the presence of a gap,\nthen in sentences containing a wh-licensor but no\ngap we expect higher surprisal in syntactic po-\nsitions where a gap is likely to occur resulting\nin higher summed surprisal. That is, S(\n(2b)) −\nS((2a)) should be a large positive number. Sec-\nond, the presence of a gap in the absence of a wh-\nlicensor should also result in higher surprisal than\nwhen the wh-licensor is present, that is S(\n(2d)) −\nS((2c)) should be a large negative number. Given\nthe four sentences in (2), the full wh-licensing\ninteraction is: (S (2b) - S (2a)) - (S (2d) - S (2c))\nThis represents how well the network learns both\nparts of the licensing relationship. A positive wh-\nlicensing interaction means the model represents\na ﬁller-gap dependency between the wh-word and\nthe gap site; a licensing interaction indistinguish-\nable from zero indicates no such dependency . For\nthe purposes of brevity , we will give examples that\nmirror item\n(2d), above, but items of type (2a)–\n(2c) were also constructed in order to calculate the\nfull licensing interaction.\nFollowing standard practice in psycholinguis-\ntics, we derive the statistical signiﬁcance of the\ninteraction from a mixed-effects linear regression\nmodel predicting surprisal given sum-coded con-\nditions (\nBaayen et al. , 2008). W e include random\nintercepts by item; random slopes are not neces-\nsary because we do not have repeated observations\nwithin items and conditions (\nBarr et al. , 2013). In\nour ﬁgures, error bars represent 95% conﬁdence\nintervals of the contrasts between conditions, com-\nputed by subtracting out the by-item means before\ncalculating the intervals as advocated in\nMasson\nand Loftus (2003). 2\nAlthough our method can indicate whether\nthere is a link between ﬁllers and gaps, the rela-\ntionship between language model probability and\ngrammaticality is complex (\nLau et al. , 2017) and\ninterpreting our patterns in terms of grammatical-\nity judgments would require auxiliary assumptions\nthat we don’t pursue here. T o be clear: our goal\nis to investigate whether RNNs model the proba-\nbilistic dependencies between ﬁllers and gaps at\nall, not whether the outputs of such models can be\nused to classify sentences as ‘grammatical’ or not.\n3 Representation of ﬁller–gap\ndependencies\nThe ﬁller–gap dependency has three basic char-\nacteristics. First, the relationship is ﬂexible : wh-\nphrases can license gaps in diverse syntactic po-\nsitions. Second, the relationship is robust to in-\ntervening material : syntactic position, not linear\ndistance, determines grammaticality . Third, the re-\nlationship is one-to-one: except in certain special\ncases, one wh-phrase licenses one gap. In this sec-\ntion, we demonstrate that the RNNs have learned\nthese three properties of ﬁller–gap dependencies\nby comparing their performance to a simple n-\ngram baseline model.\n3.1 Flexibility of Wh-Licensing\nIf the RNN has learned the ﬂexibility of the ﬁller–\ngap dependency , then we predict to ﬁnd a wh-\nlicensing interaction when the gap appears in sub-\nject, object, and indirect object positions:\n(3) a. I know who\nshowed the presentation to\nthe visitors yesterday . [ subj]\nb. I know what the businessman showed to\nthe visitors yesterday . [ obj]\nc. I know who the businessman showed the\npresentation to yesterday . [pp]\nT o test the ﬂexibility of the model’s ﬁller–gap de-\npendency representation, we created 21 test items\ncontaining either an obligatorily ditransitive verb,\n2 Our studies were preregistered on aspredicted.org:\nT o see the preregistrations go to aspredicted.org/X .pdf\nwhere X ∈ { md5ax,hd2df,mp9dv,uu8b5,rj2sk}.\n214\nor a transitive verb with an obligatorily argument-\ntaking preposition, as in\n(3). The obligatoriness of\nverb and preposition transitivity was judged by the\nauthors. T o control for the infrequent wh-licensor–\nverb bigram when the gap is in subject position,\nin all cases the embedded clause was separated\nfrom the wh-phrase by either an adverbial (e.g.\n“despite protocol”) or by words introducing a sec-\nondary embedded clause (e.g. “my brother said”).\nFor each item, we created three variants: subj, obj,\nand pp, corresponding to the items in Example\n(3).\nThe top row of Figure 1 demonstrates how the\nwh-licensing interaction was calculated for this\nexperiment. The two panels at left show the main\neffect of wh-licensing, with surprisal in post-gap\nmaterial shown in (a) and summed whole-clause\nsurprisal in (b). The red bars indicate the effect of a\nwh-licensor on surprisal in the non-gapped condi-\ntion, or S\n(2b)– S(2a), to use the example from 2.3.\nThe blue bars show the effect of a wh-licensor on\nsurprisal in the gapped conditions, or S\n(2d)– S(2c),\nto use the same example. The difference between\nthe red bars and the blue bars in each condition is\nthe licensing interaction, which is shown directly\nin (c) and (d). Not pictured are results from the\nn-gram baseline model, which yielded exactly 0\nlicensing interaction in all positions.\nThe bottom row of Figure\n1 shows a region-by-\nregion visualization of wh-licensing interaction.\nRegion-by-region behavior is consistent across\nconditions: The licensing interaction spikes in the\nimmediate post-gap material and returns to near\nzero levels for the rest of the sentence. The height\nof the licensing ‘spike’ in each condition is equiv-\nalent to the size of the wh-licensing interaction\nin (c), and the difference between the bars in\n(a). Meanwhile, the area under the ‘wh-licensing\ncurve’ is equivalent to the summed wh-licensing\ninteraction shown in (d) and the difference be-\ntween the bars in (b). All of these wh-licensing in-\nteractions are signiﬁcant ( p < 0.001 in all cases).\nThis experiment was designed to test whether li-\ncensing interaction exists in multiple syntactic po-\nsitions, which we turn to now . In the post-gap ma-\nterial, there is no signiﬁcant difference in licensing\ninteraction between conditions. But when we sum\nwh-licensing interaction across the entire embed-\nded clause model behavior does diverge. For the\nGulordava model, there is no signiﬁcant difference\nbetween the three variants. For the Google model\nthere is a signiﬁcant reduction in licensing effect\nbetween the subj and obj variants ( p < 0.01) and\nthe subj and pp variants ( p < 0.001). The stronger\nlicensing effects for subject gaps indicates that the\nnetworks have a stronger expectation for gaps in\nthis position. This matches human online process-\ning results, in so far as gap expectation may be\none reason why subject-extracted clauses are eas-\nier to process than other clauses (\nKing and Just ,\n1991). Overall, these experiments provide strong\nevidence that both models are learning the ﬁller–\ngap dependency . Furthermore, both RNN models\nare learning the ﬂexibility of the dependency , as\nthey exhibit similar wh-licensing effects for all\nthree argument roles tested.\n3.2 Robustness of Wh-Licensing to\nIntervening Material\nAll syntactic dependencies are robust to interven-\ning material. In\n(4), the dependency is determined\nby the syntactic relationship between the comple-\nmentizer ‘what’ and the position of the gap; mod-\nifying the subject doesn’t change the relationship,\nand thus has no effect on ﬁller–gap licensing:\n(4) a. I know what your friend gave\nto Sam dur-\ning the picnic yesterday .\nb. I know what your new friend from the south\nof France who only just arrived last week\ngave\nto Sam during the picnic yesterday .\nHaving shown previously that RNNs have expec-\ntations for ﬁller–gap dependencies, in this sec-\ntion we ask how well they are able to maintain\nthose expectations over intervening material. W e\ndesigned 21 sentences, like those in\n(4), with an\nobligatorily transitive verb and either an indirect\nobject or a PP modiﬁer. For each sentence we\nproduced four variants, a short-modiﬁed version\nwith 3-5 extra intervening words between the wh-\nlicensor and the gap site, a medium version with\n6-8 additional words and a long version, with 8-\n12 additional words. In all cases the extra mate-\nrial modiﬁed the subject of the embedded clause.\nFor each length gradation we produced two fur-\nther variants: one in which the direct object was\nextracted ( obj, as in\n(4)) and one variant in which\nthe indirect object or prepositional object was ex-\ntracted ( goal, where ‘Sam’ is in\n(4)). For each\nvariant, we measured the wh-licensing interaction\nin the post-gap material and across the embedded\nclause. Treating the number of intervening words\nas a continuous variable, we calculated the corre-\nlation between the length of the intervener and the\n215\n(a)\ngoogle gulordava\nsubj obj pp subj obj pp\n−4\n−2\n0\nGap Location\nSurprisal with wh−complimentizer vs. Surprisal with that−complimentizer\ngap\nno−gap\ngap\nWh−Main Effect, Post Gap Material\n(b)\ngoogle gulordava\nsubj obj pp subj obj pp\n−5\n0\n5\n10\nGap Location\nSurprisal with wh−complimentizer vs. Surprisal with that−complimentizer\ngap\nno−gap\ngap\nWh−Main Effect, Whole Clause\n(c)\ngoogle gulordava\nsubj obj pp subj obj pp\n0\n2\n4\nGap Location\nWh−Licensing Interaction\nPost Gap Material\n(d)\ngoogle gulordava\nsubj obj pp subj obj pp\n0\n5\n10\n15\nGap Location\nWh−Licensing Interaction\nWhole Clause\n●\n●●\n●●● ●●\n●\n●●\n●\n●\n●\n●\n●\n●● ●●\n●\n●\n●● ●●● ●●● ●●\n●\n●●\n●\n● ●\n●\n● ●● ●●\n● ●●●\ngoogle gulordava\nI know that/whdespite protocol\nthe CEOshowed\nthe presentation\nto\nthe guestsafter lunch\n. <eos>\nI know that/whdespite protocol\nthe CEOshowed\nthe presentation\nto\nthe guestsafter lunch\n. <eos>\n0\n2\n4\n6\nWh−Licensing Interaction\ngap_position\n●\n●\n●\nsubj\nobj\npp\nFigure 1: Wh-licensing by syntactic position. Charts (a) and (b) show the effect of wh-licensors on surprisal; (c) and\n(d) show the wh-licensing interaction by syntactic position. The difference between the non-gapped and gapped\nconditions (red and blue bars) in (a) and (b) correspond to the total licensing interaction, or the height of the bars\nin (c) and (d). The bottom chart displays wh-licensing interaction summed across all words within each region.\nstrength of the wh-licensing interaction. Optimally\nwe would ﬁnd zero correlation; a negative correla-\ntion indicates that the strength of the interaction\ndecays with increasing intervening words.\nResults of this study can be seen in Figure\n2.\nFirst, as a baseline, across the eight experiments\nshown below , the average number of positive li-\ncensing interaction measurements was 86.4%. The\nvast majority of the time, the presence of both a\nﬁller and a gap reduced surprisal superadditively ,\nproducing a positive licensing interaction. Moving\non to the effect of intervener length itself: For the\nGoogle model, intervener length was not a signif-\nicant predictor of wh-licensing interaction in any\nof the conditions. For the Gulordava model, in-\ntervener length was not a signiﬁcant predictor of\nwh-licensing interaction size when measurements\nwere taken across the entire embedded clause. But\nlength did correlate with wh-licensing interaction\nsize when measured in the post-gap material for\nthe object position ( β = 0.0289,p = 0.0219) and\ngoal position ( β = 0.0047,p = 0.0432). These ex-\ntremely small effect sizes, combined with the oth-\nerwise mixed results from both models, indicate\nthat interveners do not consistently attenuate the\nsize of the licensing interaction.\nWhile inconsistent with the formal linguistic lit-\nerature on ﬁller–gap dependencies, the negative\nvalues of all but one of the correlations are con-\nsistent with known effects in human sentence pro-\ncessing, where increasing distance between ﬁllers\nand gaps usually causes processing slowdown\n(\nGrodner and Gibson , 2005; Bartek et al. , 2011).\nIn the n-gram baseline, all licensing effects are ex-\nactly zero, indicating the n-gram model has no rep-\nresentation of the ﬁller–gap dependency .\n3.3 Multiple Gaps\nExcept for a few special cases, such as with across-\nthe-board (A TB) movement and parasitic gaps, a\none-to-one relationship must be maintained be-\ntween the wh-phrase and the gap it licenses. The\npresence of two gaps in\n(5c) violates this one-to-\none relationship, accounting for its relative bad-\nness compared to\n(5a) and (5b).\n(5) a. I know what the lion devoured at sunrise.\nb. I know what devoured a mouse at sunrise.\nc. *I know what devoured at sunrise.\n216\n●\n●\n●\n●●\n●\n● ●\n● ●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n●\n●\n●●\n●\n● ●\n● ●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n● ● ● ●\n● ●\n●\n●\n● ● ● ●\n● ● ● ●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n● ●\n●\n●\n●\n●●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n● ● ●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n●\n●\n●\n● ● ●● ●\n●\n●\n●\n● ●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n● ●\n●\n●\n● ●\n●\n●\n● ● ●\ngoogle gulordava\n0 5 10 15 0 5 10 15\n0.0\n2.5\n5.0\n7.5\nLength of Modifier in Words\nLicensing Interaction\nObj Position, Post−Gap Material\n●\n●\n●\n●\n●\n●\n● ●\n● ● ● ●\n●\n●\n● ●\n● ● ● ●\n● ●\n●\n●\n● ● ● ●● ● ● ●● ● ● ●● ● ● ●\n●\n●\n● ●\n● ● ● ●\n● ●\n●\n●\n● ● ● ●\n●\n●\n●\n●●\n● ● ●\n● ● ● ●● ● ●\n●\n● ●\n● ●\n●\n● ● ●\n● ● ● ● ● ● ● ●● ● ● ●● ● ● ●\n● ● ● ●\n●\n●\n● ●● ● ●\n●● ● ● ●● ● ● ●● ● ● ●● ● ● ●●\n● ● ●\n● ● ● ●● ● ● ●\n● ●\n● ●● ● ● ●\n● ● ● ●● ● ● ●● ● ● ●\n●\n●\n● ●● ● ● ●● ● ● ●\ngoogle gulordava\n0 5 10 15 0 5 10 15\n0\n2\n4\n6\nLength of Modifier in Words\nLicensing Interaction\nGoal Position, Post−Gap Material\n●\n●\n● ●\n●\n●\n●\n●\n● ●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n●\n●\n●\n●\n● ●\n●\n● ● ● ●●\n●\n●\n●●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n● ● ●\n● ●\n●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n● ● ●\n●\n●\n● ●\n●\n● ●\n●\n●\n● ●\n●\n●\n●\n● ●\n● ●\n● ●● ● ●\n●\n● ●\n● ●●\n●\n●\n●\n●\n● ● ●\n●\n● ● ●\n●\n●\n● ●\n● ● ●\n●\n●\n● ● ●\n●\n●\n●\n●\ngoogle gulordava\n0 5 10 15 0 5 10 15\n0\n5\n10\n15\n20\nLength of Modifier in Words\nLicensing Interaction\nObj Position, Whole Clause\n●\n● ● ●\n●\n●\n●\n●\n● ●\n● ●● ● ● ●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n● ●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n● ●\n● ●\n● ●\n●\n●\n●\n● ● ●●\n● ● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n● ●\n●\n●\n● ● ●●\n●\n●\n●\n● ● ● ●● ● ● ●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n● ●\n● ● ● ●● ● ● ●●\n●\n●\n●● ●\n● ●\n● ● ● ●\n●\n●\n●\n●\n● ●\n● ●● ● ● ●\n●\n● ● ●●\n● ● ●\n●\n●\n●\n●\n● ● ● ●\n● ●\n● ●\n●\n● ● ●\ngoogle gulordava\n0 5 10 15 0 5 10 15\n0\n3\n6\n9\nLength of Modifier in Words\nLicensing Interaction\nGoal Position, Whole Clause\nFigure 2: Wh-licensing interaction as a function of in-\ntervener length. Zero is marked with a red line.\nT o test whether RNNs have learned this one-to-\none feature of wh-licensing, we created 21 items\nall with gaps in object position like those in\n(5),\nwith two variants: one without a subject gap like\n(5a) (no-subj-gap) and one with a subject gap, as\nin (5c) (subj-gap). W e took special care to use only\nobligatorily transitive verbs. Half of the test items\ncontained ‘what’ and half ‘who’ as wh-licensors.\nW e measured the wh-licensing interaction for the\ntwo RNN models and the n-gram model, in both\nthe post-gap PP and across the embedded phrase.\nFigure\n3 shows the results of this experiment.\nFirst, the relatively high bars in the grammati-\ncal no-subject-gap condition is another example\nof the RNN learning the ﬁller–gap dependency;\nthe n-gram baseline (not shown) exhibits no wh-\nlicensing interaction under this condition. For the\ntwo LSTMs, the presence of an upstream gap in-\ncreases surprisal in the target region, resulting in\na signiﬁcantly lower licensing effect across the\nboard ( p < 0.001 in all conditions). Meanwhile,\nthe presence of a gap in the baseline condition re-\nsults in no signiﬁcant change in wh-licensing in-\nteraction. Overall these experiments demonstrate\nthat the LSTMs have learned the last of the three\nmain ﬁller–gap dependency characteristics, and—\nfor the typical object position—expect wh-phrases\nto be paired with only one gap.\n4 Syntactic islands\nEven though the ﬁller–gap dependency is ﬂexible\nand potentially unbounded, it is not entirely un-\nconstrained.\nRoss (1967) identiﬁed ﬁve syntactic\npositions in which gaps are illicit, dubbing them\nsyntactic islands . It remains an open question\nwhether these “island constraints” are true gram-\ngoogle gulordava\nNo Gap Gap No Gap Gap\n−2\n0\n2\n4\nPresence of Upstream Gap\nWh−Licensing Interaction\nPost−Gap Material\ngoogle gulordava\nNo Gap Gap No Gap Gap\n0\n5\n10\nPresence of Upstream Gap\nWh−Licensing Interaction\nEntire Clause\nFigure 3: Wh-Licensing Interaction as a function of\nDouble Gapping: Singly-gapped sentences are shown\nin red, doubly-gapped sentences in blue. Prepositional\nPhrases following the gap constitute post-gap material.\nmatical constraints, or whether they are effects of\nprocessing difﬁculty or discourse-structural fac-\ntors (\nAmbridge and Goldberg , 2008; Hofmeister\nand Sag , 2010; Sprouse and Hornstein , 2014).\nIn the following experiments, we examine\nwhether RNN language models have learned con-\nstraints on ﬁller–gap dependencies by comparing\nthe wh-licensing interaction in non-islands to that\nwithin islands. The strongest evidence for an is-\nland constraint would be if the wh-licensing in-\nteraction goes to zero for a gap in island posi-\ntion, implying that, in the distribution over strings\nimplied by the network, the appearance of a wh-\nlicensor is totally unrelated to the appearance of a\ngap in the island position. More generally , we can\nlook for a weakened wh-licensing interaction for\nisland vs. non-island positions, which would mean\nthat the network believes a relationship between\nthe wh-licensor and the island gap is less likely .\nA positive but nonzero wh-licensing interaction\nwould be in line with human acceptability judg-\nments, which do not always categorically rule out\ngaps in island positions (\nAmbridge and Goldberg ,\n2008), and with human online processing experi-\nments, which have shown that gap expectation is\nattenuated during processing of areas where gaps\ncannot occur licitly , but does not always disap-\npear entirely (\nStowe, 1986; Traxler and Pickering ,\n1996; Phillips, 2006). Therefore, in this section we\ntake a signiﬁcant reduction in the island relative to\nthe non-island case to constitute evidence that the\nmodel has ‘learned’ the constraint.\n4.1 Wh-Island Constraint\nA gap cannot appear inside doubly nested\nclauses headed by wh-complementizers. This phe-\n217\nnomenon is called the Wh-Island Constraint\n(WHC). (6) gives three sentences that demonstrate\nthis phenomenon. As these three sentence vari-\nants will serve as the basis for our experiment\nwe give each variant a condition name, on the\ntop, and a brief description below . W e will use\nthis three-row expository technique—name, ex-\nample, description—for each of the island condi-\ntions tested in this section and use condition names\nto label graphs and ﬁgures.\n(6) a.\nnull-comp\nI know what Alex said your friend devoured\nat\nthe party .\nExtraction from the object position of an embedded\nclause with a null complementizer. No island viola-\ntions.\nb.\nthat-comp\nI know what Alex said that your friend devoured\nat the party .\nExtraction from an embedded clause headed with\nthe complementizer “that. ” No island violations.\nc.\nwh-comp\n*I know what Alex said whether your friend de-\nvoured\nat the party .\nExtraction from an embedded clause headed with\nthe complementizer “whether. ” WHC violation.\nT o test whether our LSTM language models have\nlearned this constraint, we constructed 24 items\nfollowing the conditions in\n(6). W e measured the\nwh-licensing interactions at the sentence ﬁnal PP ,\nas well as across the entire embedded clause for\nboth conditions.\nFigure\n4 shows the wh-licensing interaction\nfor both LSTMs, with non-island conditions in\nred and green and island conditions in blue. In\nall conditions, extraction out of a wh-island re-\nsulted in a signiﬁcantly lower licensing interac-\ntion than extraction out of a null-headed embed-\nded clause ( p < 0.01). For the Google model, ex-\ntraction out of an island resulted in signiﬁcantly\nlower wh-licensing interaction than extraction out\nof a that-headed embedded clause ( p < 0.001),\nand while the Gulordava model showed similar\nbehavior, none of the reductions were signiﬁcant\n( p = 0.071 for the post gap material and p = 0.052\nfor the whole clause measurement). In all cases\nthere was no signiﬁcant difference between extrac-\ntion out of the two non-island conditions, except\nfor in the Gulordava model whole-clause condi-\ntion, where licensing interaction for the that-comp\ncondition was signiﬁcantly lower than the null-\ncomp condition ( p < 0.001). These results indi-\ncate that the Google model has learned the wh-\nisland constraint insofar as it has relatively sim-\nilar expectations for extraction from null-headed\ngoogle gulordava\nnull−compthat−compwh−comp null−compthat−compwh−comp\n0\n1\n2\n3\n4\nPresence of wh−complementizer\nLicensing Interaction, Post−Gap Material\nPost−Gap Material\ngoogle gulordava\nnull−compwh−compthat−comp null−compwh−compthat−comp\n0.0\n2.5\n5.0\n7.5\n10.0\nPresence of wh−complementizer\nLicensing Interaction, Whole Clause\nEntire Clause\nFigure 4: Effect of embedded clause complementizer\non wh-licensing interaction. Post-gap material effect is\nin the left panel, whole-clause effect on the right panel.\nand that-headed clauses, which differ from from\nits expectations about wh-headed clauses. The Gu-\nlordava model has learned wh-islands, but gradi-\nently , treating that-headed embedded clauses as a\nsemi-island condition.\n4.2 Adjunct Island Constraint\nGaps cannot be licensed in an adjunct clause, as\ndemonstrated by the relative unacceptability of\n(7b) and (7c), compared to (7a). W e will refer to\nthis constraint as the Adjunct Constraint (AC).\n(7) a.\nobject\nI know what the librarian in the dark blue\nglasses placed\non the wrong shelf.\nMaterial is extracted from the object position of the\nembedded verb. No island violations.\nb.\nadjunct-back\n*I know what the patron got mad after the li-\nbrarian placed\non the wrong shelf.\nMaterial is moved from the object position of an\nembedded sentential adjunct. AC violation.\nc.\nadjunct-front\n*I know what, after the librarian placed\non the\nwrong shelf, the patron got mad.\nMaterial is moved from an embedded sentential ad-\njunct that has been fronted to before the main verb\nof the embedded clause. AC violation.\nT o test whether RNNs were sensitive to the AC\nwe devised 20 items following the variants in\n(7).\nFiller material was added to the object condition\nto control for sentence length across variants. W e\nused three different prepositions to construct tem-\nporal adjuncts: ‘while’, ‘after’ and ‘before’. W e\nmeasured the wh-licensing interaction in the post-\ngap PP and across the entire embedded clause.\nFigure\n5 shows the wh-licensing interaction for\nboth models. For the Google model there is a sig-\nniﬁcant ( p < 0.001) reduction in wh-licensing in-\nteraction between the object condition and the two\nadjunct conditions when measurement is taken in\nthe post-gap material. The difference in licensing\n218\ngoogle gulordava\nObject PositionAdjunct BackAdjunct FrontObject PositionAdjunct BackAdjunct Front\n0.0\n2.5\n5.0\nLocation of Extraction Domain\nLicensing Interaction\nPost−Gap Material\ngoogle gulordava\nObject PositionAdjunct BackAdjunct FrontObject PositionAdjunct BackAdjunct Front\n0\n1\n2\n3\n4\n5\nLocation of Extraction Domain\nLicensing Interaction\nEntire Clause\nFigure 5: Effect of extraction site on wh-licensing in-\nteraction for adjunct islands. Post-gap material effect is\nin the left panel, whole-clause effect on the right panel.\nis also signiﬁcant when measurements are taken\nacross the embedded clause ( p < 0.05 for the ob-\nject– adj-front difference and p < 0.01 for the ob-\nject– adj-back difference). The Gulordava model\nshows similar results. In the post gap material,\nthere is a signiﬁcant difference when wh-licensing\ninteraction is measured in the post-gap material\n( p < 0.05 for the object– adj-front difference; p <\n0.01 for the object– adj-back difference). Results\nare also signiﬁcant when the whole embedded\nclause is measured ( p < 0.01 for both differences).\nT o sum up: In all cases, the placement of a gap\nwithin an adjunct results in a signiﬁcantly lower\nlicensing interaction. This difference in licensing\ninteraction suggests that the models have learned\nthe AC inasmuch as they have attenuated expecta-\ntions for wh-licensing within sentential adjuncts.\n4.3 Complex NP and Subject Islands\nThe Complex NP Constraint (CNPC) holds that\na gap cannot be hosted in a sentential clause dom-\ninated by a noun phrase with a lexical head noun.\nThis constraint accounts for the unacceptability of\n(8b), (8c), (8f) and (8g) below . The CNPC does\nnot apply to other NP modiﬁers, such as PPs, un-\nless the modiﬁed NP occurs in subject position\n(\nHuang, 1982). This ban, called the Subject Con-\nstraint (SC), accounts for the unacceptability of\n(8h) compared to (8d).\n(8) a.\nobject\nI know what the family bought\nlast year .\nExtraction of embedded clause object.\nb.\nthat-rc/obj\n*I know who the family bought the painting that\ndepicted\nlast year .\nExtraction from ‘that’-headed relative clause modi-\nfying embedded object. CNPC violation.\ngoogle gulordava\nobject\nwh−rc/objthat−rc/objprep/obj object\nwh−rc/objthat−rc/objprep/obj\n0.0\n2.5\n5.0\n7.5\n10.0\nLocation of extraction domain\nLicensing Interaction\nObj Position, Whole Clause\ngoogle gulordava\nsubject\nwh−rc/subjthat−rc/subjprep/subj subject\nwh−rc/subjthat−rc/subjprep/subj\n0\n5\n10\nLocation of extraction domain\nLicensing Interaction\nSubject Position, Whole Clause\nFigure 6: Effect of extraction site location in complex\nnp islands on wh-licensing interaction, measurement\ntaken across the whole embedded clause. Object po-\nsition is at left, subject position at right.\nc.\nwh-rc/obj\n*I know who the family bought the painting\nwhich depicted\nlast year .\nExtraction from ‘wh’-headed relative clause modi-\nfying embedded object. CNPC violation\nd.\nprep/obj\nI know who the family bought the painting by\nlast year .\nExtraction from PP attached to embedded object.\ne.\nsubject\nI know what\nfetched a high price at auction.\nExtraction of embedded clause subject.\nf.\nthat-rc/subj\n*I know who the painting that depicted\nfetched a high price at auction.\nExtraction from ‘that’-headed relative clause modi-\nfying embedded subject. CNPC violation\ng.\nwh-rc/subj\n*I know who the painting which depicted\nfetched a high price at auction.\nExtraction from ‘wh’-headed relative clause modi-\nfying embedded subject. CNPC violation.\nh.\nprep/subj\n*I know who the painting by\nfetched a high\nprice at auction.\nExtraction from PP attached to embedded subject.\nSC violation.\nT o test whether RNNs were sensitive to the CNPC\nand SC, we constructed 21 items for the vari-\nants shown in\n(8), which resulted in 8 conditions.\nFor prep/obj and prep/subj special care was taken\nto use prepositions that unambiguously attach to\nthe object and subject NP , respectively . As post\ngap material varied between variants, only whole-\nclause wh-licensing interaction measurement is\ngiven for this experiment.\nResults for object variants can be seen in the\nleft panel of Figure\n6, and results for the sub-\nject variants on the right. In all cases the com-\nparatively large licensing interaction in non-island\nconditions ( object and subject) shrinks when the\nextracted material occurs inside a complex NP\n219\n(the middle bars in each chart). For the Google\nmodel the difference is signiﬁcant for both CNP\nislands when extraction occurs in object position\n( p < 0.001). For subject position, the difference is\nsigniﬁcant when the RC is headed by a wh-word\n(wh-rc/subj) ( p < 0.05), but there is no signiﬁcant\ndifference when the RC is headed by ‘that’, as in\nwh-that/subj. For the Gulordava model, both dif-\nferences are signiﬁcant in subject ( p < 0.05) and\nobject position ( p < 0.01). Of the eight compar-\nisons in\n6 between CNPC islands and their non-\nisland counterparts, seven show signiﬁcant reduc-\ntion in wh-licensing interaction. These differences\nindicate that both LSTMs do not generally expect\nextraction to occur from within complex NPs.\nHowever, the LSTMs demonstrate divergent li-\ncensing behavior when extraction occurs from out\nof a prepositional phrase. If the models were learn-\ning the SC, we would expect no signiﬁcant dif-\nference between object and prep/obj, but a island-\nlike reduction in licensing interaction between the\nsubject and prep/subj conditions. However, for the\nGoogle model there is no signiﬁcant difference\nin licensing interaction in any condition, and for\nthe Gulordava model the difference is signiﬁcant\n( p < 0.05) in all cases. These results demonstrate\nthat neither model has learned the subject con-\nstraint, categorizing PPs as either licit extraction\ndomains in all positions (the Google model) or\ntreating them like islands (the Gulordava model).\n5 Conclusion\nW e have provided evidence that state-of-the-art\nLSTM language models have learned to repre-\nsent ﬁller–gap dependencies and some of the con-\nstraints on them. These results capture the bi-\ndirectional nature of the dependency , due to the\nfact that our measure—wh-licensing interaction—\nmeasures both the salutary effect of a gap given the\npresence of an upstream ﬁller, as well as the salu-\ntary effect of a ﬁller given a gap. W e found strong\nlicensing effects in both subject, object and indi-\nrect object locations, as well as an expectation that\nthe ﬁller–gap relationship was one-to-one and rel-\natively unaffected by grammatically-irrelevant in-\nterveners. The models also learned constraints on\nthe dependency , insofar as licensing effect shrank\nwhen gaps were located in wh-islands, adjunct\nislands and most complex NP islands, although\nthe subject constraint was not clearly learned and\nsome trace licensing interaction remained.\nWhile the Google model was trained on ten\ntimes more data, contained ten times as many\nhidden units and uses character CNN embed-\ndings, its performance was not qualitatively more\nhuman-like than the Gulordava model. Both mod-\nels failed to correctly generalize island constraints\nin two conditions: The Google model failed to\nlearn that-headed Complex-NP Islands, the Gulor-\ndava model to learn Wh-Islands, and both failed to\nlearn Subject Islands. These results indicate that—\nbeyond a certain point—increased model size and\ntraining regimen give diminishing returns.\nIn other recent work,\nChowdhury and Zampar-\nelli (2018) tested the ability of neural networks\nto separate grammatical from ungrammatical ex-\ntractions using similar metrics to ours, ﬁnding that\ntheir neural networks do not represent the un-\nboundedness of ﬁller–gap dependencies nor cer-\ntain strong island constraints. W e believe the dif-\nference between our results and theirs is due to\nexperimental design: They choose to measure the\nprobability of the question mark punctuation as a\nproxy for the RNNs gap expectation, and use sen-\ntence schemata instead of hand-engineered exper-\nimental items. While\nChowdhury and Zamparelli\n(2018) conclude that the networks are not learn-\ning island-like constraints, but rather displaying\nsensitivity to syntactic complexity plus order, we\ndemonstrate island-like effects where both the is-\nland and the non-island item are equally complex\n(in e.g. wh-islands). Note also that our work is fo-\ncused on ﬁnding evidence that networks represent\nthe probabilistic contingencies implied by island\nconstraints, without attempting to directly model\ngrammaticality judgments.\nOur work shows these dependencies and their\nconstraints can be learned to some extent by a\ngeneric sequence model with no obvious inductive\nbias for hierarchical structures. This is evidence\nagainst the idea that such an inductive bias is nec-\nessary for language learning, although the amount\nof data these models are trained on is much larger\nthan the typical input to a child learner.\nAcknowledgements\nAll experimental materials and scripts are available at\nhttps:\n//osf.io/zpfxm/. EGW would like to acknowledge sup-\nport from the Mind Brain Behavior Graduate Student Grant,\nas well as Emmanuel Dupoux and the Cognitive Machine\nLearning Group at the ENS. RPL gratefully acknowledges\nsupport to his laboratory from Elemental Cognition and from\nthe MIT -IBM W atson AI Lab. This work was supported by a\nGPU Grant from the NVIDIA corporation.\n220\nReferences\nAmbridge, Ben and Adele E Goldberg. 2008. The island\nstatus of clausal complements: Evidence in favor of an\ninformation structure explanation. Cognitive Linguistics\n19(3):357–389.\nBaayen, R.H., D.J. Davidson, and D.M. Bates. 2008. Mixed-\neffects modeling with crossed random effects for subjects\nand items. Journal of memory and language 59(4):390–\n412.\nBarr, Dale J, Roger Levy , Christoph Scheepers, and Harry J\nTily . 2013. Random effects structure for conﬁrmatory hy-\npothesis testing: Keep it maximal. Journal of Memory and\nLanguage 68(3):255–278.\nBartek, B., Richard L. Lewis, Shravan V asishth, and M. R.\nSmith. 2011. In search of on-line locality effects in sen-\ntence comprehension. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition 37(5):1178–1198.\nChelba, Ciprian, T omas Mikolov , Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and T ony Robinson.\n2013. One billion word benchmark for measuring\nprogress in statistical language modeling. arXiv preprint\narXiv:1312.3005 .\nChowdhury , Shammur Absar and Roberto Zamparelli. 2018.\nRnn simulations of grammaticality judgments on long-\ndistance dependencies. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics. pages\n133–144.\nEllefson, Michelle R and Morten H Christiansen. 2000. Sub-\njacency constraints without universal grammar: Evidence\nfrom artiﬁcial language learning and connectionist model-\ning. In Proceedings of the Annual Meeting of the Cogni-\ntive Science Society. volume 22.\nElman, Jeffrey L. 1991. Distributed representations, simple\nrecurrent networks, and grammatical structure. Machine\nlearning 7(2-3):195–225.\nElman, J.L. 1990. Finding structure in time. Cognitive Sci-\nence 14(2):179–211.\nGoldberg, Y oav . 2017. Neural network methods for natural\nlanguage processing. Synthesis Lectures on Human Lan-\nguage T echnologies10(1):1–309.\nGrodner, Daniel and Edward Gibson. 2005. Consequences of\nthe serial nature of linguistic input for sentential complex-\nity . Cognitive Science 29(2):261–290.\nGulordava, K., P . Bojanowski, E. Grave, T . Linzen, and\nM. Baroni. 2018. Colorless green recurrent networks\ndream hierarchically . In Proceedings of NAACL.\nHale, John T . 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Proceedings of the Second Meet-\ning of the North American Chapter of the Association for\nComputational Linguistics and Language T echnologies .\npages 1–8.\nHeaﬁeld, Kenneth, Ivan Pouzyrevsky , Jonathan H. Clark, and\nPhilipp Koehn. 2013. Scalable modiﬁed Kneser-Ney lan-\nguage model estimation. In Proceedings of the 51st An-\nnual Meeting of the Association for Computational Lin-\nguistics. Soﬁa, Bulgaria.\nHochreiter, Sepp and J ¨urgen Schmidhuber. 1997. Long short-\nterm memory . Neural Computation 9(8):1735–1780.\nHofmeister, Philip and Ivan A Sag. 2010. Cognitive con-\nstraints and island effects. Language 86(2):366.\nHuang, Cheng-T eh James. 1982. Logical relations in Chi-\nnese and the theory of grammar . Ph.D. thesis, Mas-\nsachusetts Institute of T echnology , Cambridge, MA.\nJozefowicz, Rafal, Oriol V inyals, Mike Schuster, Noam\nShazeer, and Y onghui Wu. 2016. Exploring the limits of\nlanguage modeling. arXiv 1602.02410.\nKing, Jonathan and Marcel Adam Just. 1991. Individual\ndifferences in syntactic processing: The role of working\nmemory . Journal of memory and language 30(5):580.\nLau, Jey Han, Alexander Clark, and Shalom Lappin. 2017.\nGrammaticality , acceptability , and probability: a proba-\nbilistic view of linguistic knowledge. Cognitive Science\n41(5):1202–1241.\nLevy , Roger. 2008. Expectation-based syntactic comprehen-\nsion. Cognition 106(3):1126–1177.\nLinzen, T al, Emmanuel Dupoux, and Y oav Goldberg. 2016.\nAssessing the ability of LSTMs to learn syntax-sensitive\ndependencies. Transactions of the Association for Com-\nputational Linguistics 4:521–535.\nMasson, Michael E. J. and Geoffrey R. Loftus. 2003. Us-\ning conﬁdence intervals for graphically based data in-\nterpretation. Canadian Journal of Experimental Psy-\nchology/Revue canadienne de psychologie exp ´erimentale\n57(3):203.\nMcCoy , R. Thomas, Robert Frank, and T al Linzen. 2018. Re-\nvisiting the poverty of the stimulus: hierarchical general-\nization without a hierarchical bias in recurrent neural net-\nworks. arXiv preprint arXiv:1802.09091 .\nPearl, Lisa and Jon Sprouse. 2013. Syntactic islands and\nlearning biases: Combining experimental syntax and com-\nputational modeling to investigate the language acquisi-\ntion problem. Language Acquisition 20(1):23–68.\nPhillips, Colin. 2006. The real-time status of island phenom-\nena. Language pages 795–823.\nPhillips, Colin. 2013. On the nature of island constraints II:\nLanguage learning and innateness. Experimental syntax\nand island effects pages 132–157.\nRoss, John Robert. 1967. Constraints on variables in syn-\ntax. Ph.D. thesis, Massachusetts Institute of T echnology ,\nCambridge, MA.\nSmith, Nathaniel J. and Roger Levy . 2013. The effect of word\npredictability on reading time is logarithmic. Cognition\n128(3):302–319.\nSprouse, Jon and Norbert Hornstein. 2014. Experimental\nsyntax and island effects. Cambridge University Press.\nStowe, Laurie A. 1986. Parsing wh-constructions: Evidence\nfor on-line gap location. Language and cognitive pro-\ncesses 1(3):227–245.\nSutskever, Ilya, Oriol V inyals, and Quoc V Le. 2014. Se-\nquence to sequence learning with neural networks. In Ad-\nvances in Neural Information Processing Systems . pages\n3104–3112.\n221\nTraxler, Matthew J and Martin J Pickering. 1996. Plausibil-\nity and the processing of unbounded dependencies: An\neye-tracking study . Journal of Memory and Language\n35(3):454–475.\nV inyals, Oriol, Łukasz Kaiser, T erry Koo, Slav Petrov , Ilya\nSutskever, and Geoffrey Hinton. 2015. Grammar as a for-\neign language. In Advances in Neural Information Pro-\ncessing Systems. pages 2773–2781.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8797122240066528
    },
    {
      "name": "Recurrent neural network",
      "score": 0.8006830215454102
    },
    {
      "name": "Computer science",
      "score": 0.6396876573562622
    },
    {
      "name": "Language model",
      "score": 0.5712938904762268
    },
    {
      "name": "Filler (materials)",
      "score": 0.5381283760070801
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49108457565307617
    },
    {
      "name": "State (computer science)",
      "score": 0.442000150680542
    },
    {
      "name": "Natural language processing",
      "score": 0.4192352890968323
    },
    {
      "name": "Algorithm",
      "score": 0.17689061164855957
    },
    {
      "name": "Engineering",
      "score": 0.07074129581451416
    },
    {
      "name": "Artificial neural network",
      "score": 0.05664166808128357
    },
    {
      "name": "Chemical engineering",
      "score": 0.0
    }
  ]
}