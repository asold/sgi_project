{
  "title": "Automated essay scoring using efficient transformer-based language models",
  "url": "https://openalex.org/W3134748792",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287522929",
      "name": "Ormerod, Christopher M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287522930",
      "name": "Malhotra, Akanksha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227069002",
      "name": "Jafari, Amir",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W55286957",
    "https://openalex.org/W2798046862",
    "https://openalex.org/W2295056205",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2974340959",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2991324852",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2740592764",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W1954968711",
    "https://openalex.org/W2567547739",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963684275",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2212730344",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2970458645",
    "https://openalex.org/W3114208363",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2162821268",
    "https://openalex.org/W3110259628",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W1600389356",
    "https://openalex.org/W2105845572",
    "https://openalex.org/W3037207300",
    "https://openalex.org/W2158240052",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W2172140247",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1511133876",
    "https://openalex.org/W3098654368",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W1980867644",
    "https://openalex.org/W3091156754"
  ],
  "abstract": "Automated Essay Scoring (AES) is a cross-disciplinary effort involving Education, Linguistics, and Natural Language Processing (NLP). The efficacy of an NLP model in AES tests it ability to evaluate long-term dependencies and extrapolate meaning even when text is poorly written. Large pretrained transformer-based language models have dominated the current state-of-the-art in many NLP tasks, however, the computational requirements of these models make them expensive to deploy in practice. The goal of this paper is to challenge the paradigm in NLP that bigger is better when it comes to AES. To do this, we evaluate the performance of several fine-tuned pretrained NLP models with a modest number of parameters on an AES dataset. By ensembling our models, we achieve excellent results with fewer parameters than most pretrained transformer-based models.",
  "full_text": "arXiv:2102.13136v1  [cs.CL]  25 Feb 2021\nAutomated essay scoring using efﬁcient transformer-based language\nmodels\nChristopher M. Ormerod, Akanksha Malhotra, and Amir Jafari\nAB S T R AC T. Automated Essay Scoring (AES) is a cross-disciplinary eff ort involving Education,\nLinguistics, and Natural Language Processing (NLP). The ef ﬁcacy of an NLP model in AES tests\nit ability to evaluate long-term dependencies and extrapol ate meaning even when text is poorly\nwritten. Large pretrained transformer-based language mod els have dominated the current state-\nof-the-art in many NLP tasks, however, the computational re quirements of these models make\nthem expensive to deploy in practice. The goal of this paper i s to challenge the paradigm in NLP\nthat bigger is better when it comes to AES. T o do this, we evalu ate the performance of several\nﬁne-tuned pretrained NLP models with a modest number of para meters on an AES dataset. By\nensembling our models, we achieve excellent results with fe wer parameters than most pretrained\ntransformer-based models.\n1. Introduction\nThe idea that a computer could analyze writing style dates ba ck to the work of Page in\n1968 [ 31]. Many engines in production today rely on explicitly deﬁne d hand-crafted features\ndesigned by experts to measure the intrinsic characteristi cs of writing [ 14]. These features are\ncombined with frequency-based methods and statistical mod els to form a collection of methods\nthat are broadly termed Bag-of-W ord (BOW) methods [ 49]. While BOW methods have been\nvery successful from a purely statistical standpoint, [ 22] showed they tend to be brittle with\nrespect novel uses of language and vulnerable to adversaria lly crafted inputs.\nNeural Networks learn features implicitly rather than expl icitly . It has been shown that ini-\ntial neural network AES engines tend to be more more accurate and more robust to gaming\nthan BOW methods [ 12, 15 ]. In the broader NLP community , the recurrent neural networ k\napproaches used have been replaced by transformer-based ap proaches, like the Bidirectional En-\ncoder Representations from Transformers (BERT) [ 13]. These models tend to possess an order\nof magnitude more parameters than recurrent neural network s, but also boast state-of-the-art re-\nsults in the General Language Understanding Evaluation (GL UE) benchmarks [ 42, 45 ]. One of\nthe main problems in deploying these models is their computa tional and memory requirements\n[28]. This study explores the effectiveness of efﬁcient versio ns of transformer-based models in\nthe domain of AES. There are two aspects of AES that distingui shes it from GLUE tasks that\nmight beneﬁt from the efﬁciencies introduced in these model s; ﬁrstly , the text being evaluated\ncan be almost arbitrarily long. Secondly , essays written by students often contain many more\nspelling issues than would be present in typical GLUE tasks. It could be the case that fewer\n1\n2 CHRISTOPHER M. ORMEROD, AKANKSHA MALHOTRA, AND AMIR JAF AR I\nand more often updated parameters might possibly be better i n this situation or more generally\nwhere smaller training sets are used.\nWith respect to essay scoring the Automated Student Assessm ent Prize (ASAP) AES data-\nset on Kaggle is a standard openly accessible data-set by whi ch we may evaluate the performance\nof a given AES model [ 35]. Since the original test set is no longer available, we use t he ﬁve-fold\nvalidation split presented in [ 46] for a fair and accurate comparison. The accuracy of BERT and\nXLNet have been shown to be very solid on the Kaggle dataset [ 33, 47 ]. T o our knowledge,\ncombining BERT with hand-crafted features form the current state-of-the-art [ 40].\nThe recent works of have challenged the paradigm that bigger models are necessarily better\n[11, 21, 24, 29 ]. The models in these papers possess some fundamental archi tectural character-\nistics that allow for a drastic reduction in model size, some of which may even be an advantage\nin AES. For this study , we consider the performance of the Alb ert models [ 26], Reformer mod-\nels [ 21], a version of the Electra model [ 11], and the Mobile-BERT model [ 39] on the ASAP\nAES data-set. Not only are each of these models more efﬁcient , we show that simple ensembles\nprovide the best results to our knowledge on the ASAP AES data set.\nThere are several reasons that this study is important. Firs tly , the size of models scale\nquadratically with maximum length allowed, meaning that es says may be longer than the max-\nimal length allowed by most pretrained transformer-based m odels. By considering efﬁciencies\nin underlying transformer architectures we can work on exte nding that maximum length. Sec-\nondly , as noted by [ 28], one of the barriers to effectively putting these models in production is\nthe memory and size constraints of having ﬁne-tuned models f or every essay prompt. Lastly , we\nseek models that impose a smaller computational expense, wh ich in turn has been linked by [ 37]\nto a much smaller carbon footprint.\n2. Approaches to Automated Essay Scoring\nFrom an assessment point of view , essay tests are useful in ev aluating a student’s ability\nto analyze and synthesize information, which assesses the u pper levels of Blooms T axonomy .\nMany modern rubrics use a multitude of scoring dimensions to evaluate an essay , including\norganization, elaboration, and writing style. An AES engin e is a model that assigns scores to a\npiece of text closely approximating the way a person would sc ore the text as a ﬁnal score or in\nmultiple dimensions.\nT o evaluate the performance of an engine we use standard inte r-rater reliability statistics.\nIt has become standard practice in the development of a train ing set for an AES engine that\neach essay is evaluated by two different raters from which we may obtain a resolved score. The\nresolved score is usually either the same as the two raters in the case that they agree and an\nadjudicated score in cases in which they do not. In the case of the ASAP AES data-set, the\nresolved score for some items is taken to be the sum of the two r aters, and a resolved score\nfor others. The goal of a good model is to have a higher agreeme nt with the resolved score in\ncomparison with the agreement two raters have with each othe r. The most widely used statistic\nused to evaluate the agreement of two different collections of scores is the quadratic weighted\nkappa (QWK), deﬁned by\n(1) κ =\n∑ ∑ wij xij\n∑ ∑ wij mij\nA UTOMA TED ESSA Y SCORING USING EFFICIENT TRANSFORMER-BASE D LANGUAGE MODELS 3\nwhere xi,j is the observed probability\nmi,j = xij (1 − xij),\nand\nwij = 1− (i − j)2\n(k − 1)2 ,\nwhere k is the number of classes. The other measurements used in the i ndustry are the standard-\nized mean difference (SMD) and the exact match or accuracy (A cc). The total number of essays\nand the human-human agreement for the two raters and the scor e ranges are shown in table 1.\nThe usual protocol for training a statistical model is that s ome portion of the training set is\nset aside for evaluation while the remaining set is used for t raining. A portion of the training\nset is often isolated for purposes of early stopping and hype rparameter tuning. In evaluating the\nKaggle dataset, we use the 5-fold cross validation splits de ﬁned by [ 46] so that our results are\ncomparable to other works [ 1, 12, 33, 40, 47 ]. The resulting QWK is deﬁned to be the average\nof the QWK values on each of the ﬁve different splits.\nEssay Prompt\n1 2 3 4 5 6 7 8\nRater score range 1-6 1-6 0-3 0-3 0-4 0-4 0-12 5-30\nResolved score\nrange\n2-12 1-6 0-3 0-3 0-4 0-4 2-24 10-60\nA verage Length 350 350 150 150 150 150 250 650\nTraining examples 1783 1800 1726 1772 1805 1800 1569 723\nQWK 0.721 0.814 0769 0.851 0.753 0.776 0.721 0.624\nAcc 65.3% 78.3% 74.9% 77.2% 58.0% 62.3% 29.2% 27.8%\nSMD 0.008 0.027 0.055 0.005 0.001 0.011 0.006 0.069\nTA B LE 1. A summary of the Automated Student Assessment Prize Autom ated\nEssay Scoring data-set.\n3. Methodology\nMost engines currently in production rely on Latent Semanti c Analysis or a multitude of\nhand-crafted features that measure style and prose. Once su fﬁciently many features are com-\npiled, a traditional machine learning classiﬁer, like logi stic regression, is applied and ﬁt to a\ntraining corpus. As a representative of this class of model w e include the results of the “En-\nhanced AI Scoring Engine”, which is open source 1.\nWhen modelling language with neural networks, the ﬁrst laye r of most neural networks are\nembedding layers, which send every token to an element of a se mantic vector space. When\ntraining a neural network model from scratch, the word embed ding vocabulary often comprises\nof the set of tokens that appear in the training set. There are several problems with this ap-\nproach that come as a result of word sparsity in language and t he presence of spelling mistakes.\nAlternatively , one may use a pretrained word embedding [ 30] built from a large corpus with a\nlarge vocabulary with a sufﬁciently high dimensional seman tic vector space. From an efﬁciency\n1https://github.com/edx/ease\n4 CHRISTOPHER M. ORMEROD, AKANKSHA MALHOTRA, AND AMIR JAF AR I\nstandpoint, these word embeddings alone can account for bil lions of parameters. W e can shrink\nour embedding and address some issues arising from word spar sity by ﬁxing a vocabulary of\nsubwords using a version of the byte-pair-encoding (BPE) [ 25, 34 ]. As such, pretrained models\nlike BERT and those we use in this study utilize subwords to ﬁx the size of the vocabulary .\nIn addition to the word embeddings, positional embeddings a nd segment embedding are\nused to give the model information about the position of each word and next sentences prediction.\nThe combination of the 3 embedding are the keys to reduce the v ocabulary size, handling the out\nof vocab, and preserving the order of words.\nOnce we have applied the embedding layer to the tokens of a pie ce of text, it is essentially a\nsequence of elements of a vector space, which can be represen ted by a matrix whose dimensions\nare governed by the size of the semantic vector space and the n umber of tokens in the text.\nIn the ﬁeld of language modeling, sequence-to-sequence mod els (seq2seq) in the form of this\npaper were proposed in [ 38]. Initially , seq2seq models were used for natural machine t ranslation\nbetween multiple languages. The seq2seq model has an encode r-decoder component; an encoder\nanalyzes the input sequence and creates a context vector whi le the decoder is initialized with the\ncontext vector and is trainer to produce the transformed out put. Previous language models were\nbased on a ﬁxed-length context vector and suffered from an in ability to infer context over long\nsentences and text in general. An attention mechanism was us ed to improve the performance in\ntranslation for long sentences.\nThe use of a self-attention mechanism turns out to be very use ful in th context of machine\ntranslation [ 3]. This mechanism, and it various derivatives, have been res ponsible for a large\nnumber of accuracy gains over a wide range of NLP tasks more br oadly . The form of attention\nfor this paper can be found in [ 41]. Given a query matrix Q, key matrix K, and value matrix V ,\nthen the resulting sequence is given by\n(2) Attention(Q, K, V ) = softmax\n( QKT\n√ dk\n)\nV.\nThese matrices are obtained by linear transformations of ei ther the output of a neural network,\nthe output of a previous attention mechanism, or embeddings . The overall success of atten-\ntion has led to the development of the transformer [ 41]. The architecture of the transformer is\noutlined in Figure 1.\nIn the context of efﬁcient models, if we consider all sequenc es up to length L and if each\nquery is of size d, then each keys are also of length d, hence, the matrix QKT of size L× L. The\nimplication is that the memory and computational power requ ired to implement this mechanism\nscales quadratically with length. The above-mentioned mod els often adhere to a size restriction\nby letting L = 512.\n4. Efﬁcient Language Models\nThe transformer in language modeling is a innovative archit ecture to solve issues of seq2seq\ntasks while handling long term dependencies. It relies on se lf-attention mechanisms in its net-\nworks architecture. Self attention is in charge of managing the interdependence within the input\nelements.\nIn this study , we use ﬁve prominent models all of which are kno wn to perform well as\nlanguage models and possess an order of magnitude fewer para meters than BERT . It should be\nnoted that many of the other models, like RoBERT a, XLNet, GPT models, T5, XLM, and even\nA UTOMA TED ESSA Y SCORING USING EFFICIENT TRANSFORMER-BASE D LANGUAGE MODELS 5\nEmbedding\nPosition +\nMulti-head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nEmbedding\nPosition+\nMasked\nMulti-head\nAttention\nAdd & Norm\nMulti-head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nN×N×\nFIG U R E 1. This is the basic architecture of a transformer-based mod el. The\nleft block of N layers is called the encoder while the right block of N layers is\ncalled the decoder. The output of the Decoder is a sequence.\nDistilled BERT all possess more than 60M parameters, hence, were excluded for this study . W e\nonly consider models utilizing an innovation that drastica lly reduces the number of parameters\nto at most one quarter the number of parameters of BERT . Of thi s class of models, we present a\nlist of models and their respective sizes in T able 2.\nThe backbone architecture of all above language models is BE RT [ 13]. It has become the\nstate-of-the-art model for many different Natural Languag e Undestanding (NLU), Natural Lan-\nguage Generation (NLG) tasks including sequence and docume nt classiﬁcation.\nThe ﬁrst models we consider are the Albert models of [ 26]. The ﬁrst efﬁciency of Albert is\nthat the embedding matrix is factored. In the BERT model, the embedding size must be the same\nas the hidden layer size. Since the vocabulary of the BERT mod el is approximately 30,000 and\nthe hidden layer size is 768 in the base model, the embedding a lone requires approximately 25M\nparameters to deﬁne. Not only does this signiﬁcantly increa se the size of the model, we expect\nthat some of those parameters to by updated rarely during the training process. By applying\na linear layer after the embedding, we effectively factor th e embedding matrix in a way that\nthe actual embedding size can be much smaller than the feed fo rward layer. In the two models\n(large and base), the size of the vocabulary is about the same however the embedding dimension\nis effectively 128 making the embedding matrix one sixth the size.\n6 CHRISTOPHER M. ORMEROD, AKANKSHA MALHOTRA, AND AMIR JAF AR I\nModel Number T raining Inference\nof Time Time\nParameters Speedup Speedup\nBERT (base) 110M 1. 0× 1. 0×\nAlbert (base) 12M 1. 0× 1. 0×\nAlbert (large) 18M 0. 7× 0. 6×\nElectra (small) 14M 3. 8× 2. 4×\nMobile BERT 24M 2. 5× 1. 7×\nReformer 16M 2. 2× 1. 6×\nElectra + Mobile-BERT 38M 1. 5× 1. 0×\nTA B LE 2. A summary of the memory and approximations of the computat ional\nrequirements of the models we use in the study when comparing to BERT . These\nare estimates based on single epoch times using a ﬁxed batch- size in training\nand inference.\nA second efﬁciency proposed by Albert is that the layers shar e parameters. [ 26] compare a\nmultitude of parameter sharing scenarios in which all their parameters are shared across layers.\nThe base and large Albert models, with 12 layer and 24 layers r espectively , only possess a total\nof 12M and 18M parameters. The hidden size of of the base and la rge models are also 768 and\n1024 respectively . Increasing the number of layers does inc rease the number of parameters but\ndoes come with a computational cost. In the pretraining of th e Albert models, the models are\ntrained with a sentence order prediction (SOP) loss functio n instead of next sentence prediction.\nIt is argued by [ 26] that SOP can solve NSP tasks while the converse is not true an d that this\ndistinction leads to consistent improvements in downstrea m tasks.\nThe second model we consider is the small version of Electra m odel presented by [ 11].\nLike the Albert model, there is a linear layer between the emb edding and the hidden layers,\nallowing for a embedding size of 128, a hidden layer size of 25 6, and only four attention heads.\nThe pretraining of the Electra model is trained as a pair of ne ural networks consisting of a\ngenerator and a discriminator with weight-sharing between the two networks. The generators\nrole is to replace tokens in a sequence, and is therefore trai ned as a masked language model. The\ndiscriminator tries to identify that tokens were replaced b y the generator in the sequence.\nThe third model, Mobile-BERT , was presented in [ 39]. This model uses the same embedding\nfactorization used to decouple the embedding size of 128 wit h the hidden size of 512. The main\ninnovation of [ 39] is that they decrease the model size by introducing a pair of linear transfor-\nmations, called bottlenecks, around the transformer unit s o that the transformer unit is operating\non a hidden size of 128 instead of the full hidden size of of 512 . This effectively shrinks the\nsize of the underlying building blocks. MobileBERT uses abs olute position embeddings and it\nis efﬁcient at predicting masked tokens and at NLU.\nThe Reformer architecture of [ 21] differs from BERT most substantially by its version of an\nattention mechanism and that the feedforward component is o f the attention layers use revsible\nresidual layers. This means that, like [ 18], the inputs of each layer can be computed on demand\ninstead of being stored. [ 21] noted that they use an approximation of the (2) in which the l inear\ntransformation used to deﬁne Q and K are the same, i.e., Q = K. When we calculate QKT ,\nmore importantly , the softmax, we need only consider values in Q and K that are close. Using\nA UTOMA TED ESSA Y SCORING USING EFFICIENT TRANSFORMER-BASE D LANGUAGE MODELS 7\nrandom vectors, we may create a Locally Sensitive Hashing (L SH) scheme, allowing us to chunk\nkey/query vectors into ﬁnite collections of vectors that ar e known to contribute to the softmax.\nEach chunk may be computed in parallel changing the complexi ty from scaling with length as\nO(L2) to O(L log L). This is essentially a way to utilize the sparse nature of the attention matrix,\nattempting to not calculate pairs of values not likely to con tribute to the softmax. More hashes\nimproves the hashing scheme and better approximates the att ention mechanism.\n5. Results\nThe networks above are all pretrained to be seq2seq models. W hile some of the pretraining\ndiffers for some models, as discussed above, we are required to modify a sequence-to-sequence\nneural network to produce a classiﬁcation. T ypically , the w ay in which this is done is that\nwe take the ﬁrst vector of the sequence as a ﬁnite set of featur es from which we may apply a\nclassiﬁcation. Applying a linear layer to these features pr oduces a ﬁxed length vector that is\nused for classiﬁcation.\nQWK scores Prompt A VG\n1 2 3 4 5 6 7 8 QWK\nEASE 0.781 0.621 0.630 0.749 0.782 0.771 0.727 0.534 0.699\nLSTM 0.775 0.687 0.683 0.795 0.818 0.813 0.805 0.594 0.746\nLSTM+CNN 0.821 0.688 0.694 0.805 0.807 0.819 0.808 0.644 0.761\nLSTM+CNN+Att 0.822 0.682 0.672 0.814 0.803 0.811 0.801 0.705 0.764\nBER T(base) 0.792 0.680 0.715 0.801 0.806 0.805 0.785 0.596 0.758\nXLNet 0.777 0.681 0.693 0.806 0.783 0.794 0.787 0.627 0.743\nBER T + XLNet 0.808 0.697 0.703 0.819 0.808 0.815 0.807 0.605 0.758\nR2BER T 0.817 0.719 0.698 0.845 0.841 0.847 0.839 0.744 0.794\nBER T + Features 0.852 0.651 0.804 0.888 0.885 0.817 0.864 0.645 0.801\nElectra (small) 0.816 0.664 0.682 0.792 0.792 0.787 0.827 0.715 0.759\nAlbert (base) 0.807 0.671 0.672 0.813 0.802 0.816 0.826 0.700 0.763\nAlbert (large) 0.801 0.676 0.668 0.810 0.805 0.807 0.832 0.700 0.763\nMobile-BER T 0.810 0.663 0.663 0.795 0.806 0.808 0.824 0.731 0.762\nReformer 0.802 0.651 0.670 0.754 0.771 0.762 0.747 0.548 0.713\nElectra+Mobile-BER T 0.823 0.683 0.691 0.805 0.808 0.802 0.835 0.748 0.774\nEnsemble 0.831 0.679 0.690 0.825 0.817 0.822 0.841 0.748 0.782\nTA B LE 3. The ﬁrst set of agreement (QWK) statistics for each prompt . EASE,\nLSTM, LSTM+CNN, and LSTM+CNN+Att, were presented by [ 46] and [ 12].\nThe results of BERT , BERT extensions, and XLNET have been pre sented in\n[33, 40, 47 ]. The remaining rows are the results of this paper.\nGiven a set of n possible scores, we divide the interval [0, 1] into n even sub-intervals and\nmap each score to the midpoint of those intervals. So in a simi lar manner to [ 46], we treat this\nclassiﬁcation as a regression problem with a loss function o f the mean squared error. This is\nslightly different from the standard multilabel classiﬁca tion using a Cross-entropy loss function\noften applied by default to transformer-based classiﬁcati on problems. Using the standard pre-\ntrained models with an untrained linear classiﬁcation laye r with a sigmoid activation function,\nwe applied a small grid-search using two learning rates and t wo batch sizes for each model. The\nmodel performing the best on the test set was applied to valid ation.\n8 CHRISTOPHER M. ORMEROD, AKANKSHA MALHOTRA, AND AMIR JAF AR I\nFor the reformer model, we pretrained our own 6-layer reform er model using a hidden layer\nsize of 512, 4 hashing functions, and 4 attention heads. W e us ed a cased sentencepiece tok-\nenization consisting of 16,000 subwords and the model was tr ained with a maximum length of\n1024 on a large corpus of essay texts from various grades on a s ingle Nvidia RTX8000. This\nmodel addresses the length constraints other essay models s truggle with on transformer-based\narchitectures.\nW e see that both Electra and Mobile-BERT show performance hi gher than BERT itself\ndespite being smaller and faster. Given the extensive hyper -parameter tuning performed in [ 33],\nwe can only speculate that any additional gains may be due to a rchitectural differences.\nLastly , we took our best models and simply averaged the outpu ts to obtain an ensembles\nwhose output is in the interval [0, 1], then applied the same rounding transformation to obtain\nscores in the desired range. W e highlight the ensemble of Mob ile-BERT and Electra because\non their own, this ensembled model provides a big increase in performance over BERT with\napproximately the same computational footprint on its own.\n6. Discussion\nThe goal of this paper was not to achieve state-of-the-art, b ut rather to show that we can\nachieve signiﬁcant results within a very modest memory foot print and computational budget.\nW e managed to exceed previous known results of BERT alone wit h approximately one third the\nparameters. Combining these models with R 2 variants of [ 47] or with features, as done in [ 40],\nwould be interesting since they do not add little to the compu tational load of the system.\nThere were noteworthy additions to the literature we did not consider for various reasons.\nThe Longformer of [ 4] uses a sliding attention window in which the resulting self -attention\nmechanism scales linearly with length, however, the number of parameters of the pretrained\nLongformer models often coincided or exceeded those of the B ERT model. Like the Reformer\nmodel of [ 21], the Linformer of [ 43], Sparse Transformer of [ 10], and Performer model of [ 9]\nexploit the observation that the attention matrix (given by the softmax) can be approximated by\ncollection of low-rank matrices. Their different mechanis ms for doing so means their complexity\nscales differently . The Projection Attention Networks for Document Classiﬁcation On-Device\n(PRADO) model given by [ 24] seems promising, however, we did not have access to a versio n\nof it we could use. The SHARNN of [ 29] also looks interesting, however, we found that the\narchitecture was difﬁcult to use for transfer learning.\nTransfer learning and language models improved the perform ance of document classiﬁcation\ntexts in natural language processing domain. W e should note that all these models are pertained\nwith a large corpus except for the Reformer model and we perfo rm the ﬁne-tuing process. Better\ntraining could improve upon the results of the Reformer mode l.\nThere are several reasons these directions in research are i mportant; as we are seeing more\nand more attention given to power-efﬁcient computing for us age on small-devices, we think the\ndeep learning community will see a greater emphasis on small er efﬁcient models in the future.\nSecondly , this work provides a stepping stone to the classif ying and evaluating documents where\nthe necessity for context to extend beyond the limits that cu rrent pretrained transformer-based\nlanguage models allow . Lastly we think combining the approa ches that seem to have a beneﬁcial\neffect on performance should give smaller, better, and more environmentally friendly models in\nthe future.\nA UTOMA TED ESSA Y SCORING USING EFFICIENT TRANSFORMER-BASE D LANGUAGE MODELS 9\nAcknowledgments\nW e would like to acknowledge Susan Lottridge and Balaji Kode swaran for their support of\nthis project.\nReferences\n[1] Alikaniotis, Dimitrios, Helen Y annakoudakis, and Mare k Rei. “ Automatic text scoring using neural networks. ”\narXiv preprint arXiv:1606.04289 (2016).\n[2] Attali, Yigal, and Jill Burstein. “ Automated essay scor ing with e-rater® V . 2. ” The Journal of T echnology , Learn-\ning and Assessment 4, no. 3 (2006).\n[3] Bahdanau, Dzmitry , Kyunghyun Cho, and Y oshua Bengio. “N eural machine translation by jointly learning to\nalign and translate. ” arXiv preprint arXiv:1409.0473 (201 4).\n[4] Beltagy , Iz, Matthew E. Peters, and Arman Cohan. “Longfo rmer: The long-document transformer. ” arXiv\npreprint arXiv:2004.05150 (2020).\n[5] Brown, T om B., Benjamin Mann, Nick Ryder, Melanie Subbia h, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan et al. “Language models are few-shot learners. ” ar Xiv preprint arXiv:2005.14165 (2020).\n[6] Burstein, Jill, Karen Kukich, Susanne W olff, Chi Lu, and Martin Chodorow . “Enriching automated essay scoring\nusing discourse marking. ” (2001).\n[7] Chen, Jing, James H. Fife, Isaac I. Bejar, and Andr´ e A. Ru pp. “Building e-rater® Scoring Models Using Machine\nLearning Methods. ” ETS Research Report Series 2016, no. 1 (2 016): 1-12.\n[8] Cho, Kyunghyun, Bart V an Merri¨ enboer, Dzmitry Bahdana u, and Y oshua Bengio. “On the properties of neural\nmachine translation: Encoder-decoder approaches. ” arXiv preprint arXiv:1409.1259 (2014).\n[9] Choromanski, Krzysztof, V alerii Likhosherstov , David Dohan, Xingyou Song, Andreea Gane, T amas Sarlos,\nPeter Hawkins et al. “Rethinking attention with performers . ” arXiv preprint arXiv:2009.14794 (2020).\n[10] Child, Rewon, Scott Gray , Alec Radford, and Ilya Sutske ver. “Generating long sequences with sparse transform-\ners. ” arXiv preprint arXiv:1904.10509 (2019).\n[11] Clark, Kevin, Minh-Thang Luong, Quoc V . Le, and Christo pher D. Manning. “Electra: Pre-training text en-\ncoders as discriminators rather than generators. ” arXiv pr eprint arXiv:2003.10555 (2020).\n[12] Dong, Fei, Y ue Zhang, and Jie Y ang. “ Attention-based re current convolutional neural network for automatic\nessay scoring. ” In Proceedings of the 21st Conference on Com putational Natural Language Learning (CoNLL 2017),\npp. 153-162. 2017.\n[13] Devlin, Jacob, Ming-W ei Chang, Kenton Lee, and Kristin a T outanova. “Bert: Pre-training of deep bidirectional\ntransformers for language understanding. ” arXiv preprint arXiv:1810.04805 (2018).\n[14] Dikli, Semire. “ An overview of automated scoring of ess ays. ” The Journal of T echnology , Learning and Assess-\nment 5, no. 1 (2006).\n[15] Farag, Y oumna, Helen Y annakoudakis, and T ed Briscoe. “ Neural automated essay scoring and coherence mod-\neling for adversarially crafted input. ” arXiv preprint arX iv:1804.06898 (2018).\n[16] Flor, Michael, Y oko Futagi, Melissa Lopez, and Matthew Mulholland. “Patterns of misspellings in L2 and L1\nEnglish: A view from the ETS Spelling Corpus. ” Bergen Langua ge and Linguistics Studies 6 (2015).\n[17] Foltz, Peter W ., Darrell Laham, and Thomas K. Landauer. “The intelligent essay assessor: Applications to\neducational technology . ” Interactive Multimedia Electro nic Journal of Computer-Enhanced Learning 1, no. 2 (1999):\n939-944.\n[18] Gomez, Aidan N., Mengye Ren, Raquel Urtasun, and Roger B . Grosse. “The reversible residual network: Back-\npropagation without storing activations. ” In Advances in n eural information processing systems, pp. 2214-2224.\n2017.\n[19] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ”Deep residual learning for image recognition. ” In\nProceedings of the IEEE conference on computer vision and pa ttern recognition, pp. 770-778. 2016.\n[20] Hochreiter, Sepp, and J ¨ urgen Schmidhuber. “Long shor t-term memory . ” Neural computation 9, no. 8 (1997):\n1735-1780.\n[21] Kitaev , Nikita, Łukasz Kaiser, and Anselm Levskaya. “R eformer: The efﬁcient transformer. ” arXiv preprint\narXiv:2001.04451 (2020)\n[22] Kolowich, Steven. “Writing instructor, skeptical of a utomated grading, pits machine vs. machine. ” The Chroni-\ncle of Higher Education 28 (2014).\n10 CHRISTOPHER M. ORMEROD, AKANKSHA MALHOTRA, AND AMIR JAF A RI\n[23] Krathwohl, David R. “ A revision of Bloom’s taxonomy: An overview . ” Theory into practice 41, no. 4 (2002):\n212-218.\n[24] Krishnamoorthi, Karthik, Sujith Ravi, and Zornitsa Ko zareva. “PRADO: Projection Attention Networks for\nDocument Classiﬁcation On-Device. ” In Proceedings of the 2 019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conferenc e on Natural Language Processing (EMNLP-IJCNLP),\npp. 5013-5024. 2019.\n[25] Kudo, T aku, and John Richardson. “Sentencepiece: A sim ple and language independent subword tokenizer and\ndetokenizer for neural text processing. ” arXiv preprint ar Xiv:1808.06226 (2018).\n[26] Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. “ Albert:\nA lite bert for self-supervised learning of language repres entations. ” arXiv preprint arXiv:1909.11942 (2019).\n[27] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manni ng. “Effective approaches to attention-based neural\nmachine translation. ” arXiv preprint arXiv:1508.04025 (2 015).\n[28] Mayﬁeld, Elijah, and Alan W . Black. “Should Y ou Fine-Tu ne BERT for Automated Essay Scoring?. ” In Pro-\nceedings of the Fifteenth W orkshop on Innovative Use of NLP f or Building Educational Applications, pp. 151-162.\n2020.\n[29] Merity , Stephen. “Single headed attention rnn: Stop th inking with your head. ” arXiv preprint arXiv:1911.11423\n(2019).\n[30] Mikolov , T omas, Ilya Sutskever, Kai Chen, Greg S. Corra do, and Jeff Dean. ”Distributed representations of\nwords and phrases and their compositionality . ” In Advances in neural information processing systems, pp. 3111-\n3119. 2013.\n[31] Page, Ellis Batten, Gerald A. Fisher, and Mary Ann Fishe r. “PROJECT ESSA Y GRADE-A FORTRAN PRO-\nGRAM FOR ST A TISTICAL ANAL YSIS OF PROSE. ” BRITISH JOURNAL OF MA THEMA TICAL & ST A TIS-\nTICAL PSYCHOLOGY 21 (1968): 139.\n[32] Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gar dner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. “Deep contextualized word representations. ” arXiv preprint arXiv:1802.05365 (2018).\n[33] Rodriguez, Pedro Uria, Amir Jafari, and Christopher M. Ormerod. “Language models and Automated Essay\nScoring. ” arXiv preprint arXiv:1909.09482 (2019).\n[34] Sennrich, Rico, Barry Haddow , and Alexandra Birch. “Ne ural machine translation of rare words with subword\nunits. ” arXiv preprint arXiv:1508.07909 (2015).\n[35] Shermis, Mark D. “State-of-the-art automated essay sc oring: Competition, results, and future directions from a\nUnited States demonstration. ” Assessing Writing 20 (2014) : 53-76.\n[36] Shermis, Mark D., and Jill C. Burstein, eds. “ Automated essay scoring: A cross-disciplinary perspective. ” Rout-\nledge, 2003.\n[37] Strubell, Emma, Ananya Ganesh, and Andrew McCallum. “E nergy and policy considerations for deep learning\nin NLP . ” arXiv preprint arXiv:1906.02243 (2019).\n[38] Sutskever, Ilya, Oriol Vinyals, and Quoc V . Le. “Sequen ce to sequence learning with neural networks. ” arXiv\npreprint arXiv:1409.3215 (2014).\n[39] Sun, Zhiqing, Hongkun Y u, Xiaodan Song, Renjie Liu, Yim ing Y ang, and Denny Zhou. “Mobilebert: a compact\ntask-agnostic bert for resource-limited devices. ” arXiv p reprint arXiv:2004.02984 (2020).\n[40] Uto, Masaki, Yikuan Xie, and Maomi Ueno. ”Neural Automa ted Essay Scoring Incorporating Handcrafted\nFeatures. ” In Proceedings of the 28th International Confer ence on Computational Linguistics, pp. 6077-6088. 2020.\n[41] V aswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszk oreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,\nand Illia Polosukhin. ”Attention is all you need. ” In Advanc es in neural information processing systems, pp. 5998-\n6008. 2017.\n[42] W ang, Alex, Amanpreet Singh, Julian Michael, Felix Hil l, Omer Levy , and Samuel R. Bowman. “Glue: A\nmulti-task benchmark and analysis platform for natural lan guage understanding. ” arXiv preprint arXiv:1804.07461\n(2018).\n[43] W ang, Sinong, Belinda Li, Madian Khabsa, Han Fang, and H ao Ma. “Linformer: Self-Attention with Linear\nComplexity . ” arXiv preprint arXiv:2006.04768 (2020).\n[44] W ang, Y equan, Minlie Huang, Xiaoyan Zhu, and Li Zhao. ”A ttention-based LSTM for aspect-level sentiment\nclassiﬁcation. ” In Proceedings of the 2016 conference on em pirical methods in natural language processing, pp.\n606-615. 2016.\nA UTOMA TED ESSA Y SCORING USING EFFICIENT TRANSFORMER-BASE D LANGUAGE MODELS 11\n[45] W ang, Alex, Y ada Pruksachatkun, Nikita Nangia, Amanpr eet Singh, Julian Michael, Felix Hill, Omer Levy ,\nand Samuel Bowman. “Superglue: A stickier benchmark for gen eral-purpose language understanding systems. ” In\nAdvances in Neural Information Processing Systems, pp. 326 6-3280. 2019.\n[46] T aghipour, Kaveh, and Hwee T ou Ng. “ A neural approach to automated essay scoring. ” In Proceedings of the\n2016 conference on empirical methods in natural language pr ocessing, pp. 1882-1891. 2016.\n[47] Y ang, Ruosong, Jiannong Cao, Zhiyuan W en, Y ouzheng Wu, and Xiaodong He. ”Enhancing Automated Essay\nScoring Performance via Cohesion Measurement and Combinat ion of Regression and Ranking. ” In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Languag e Processing: Findings, pp. 1560-1569. 2020.\n[48] Y ang, Zhilin, Zihang Dai, Yiming Y ang, Jaime Carbonell , Russ R. Salakhutdinov , and Quoc V . Le. “Xlnet:\nGeneralized autoregressive pretraining for language unde rstanding. ” In Advances in neural information processing\nsystems, pp. 5753-5763. 2019.\n[49] Zhang, Yin, Rong Jin, and Zhi-Hua Zhou. “Understanding bag-of-words model: a statistical framework. ” Inter-\nnational Journal of Machine Learning and Cybernetics 1, no. 1-4 (2010): 43-52.\nCA M B I U M AS S E S S M E N T, I N C .\nCurrent address: 1000 Thomas Jefferson St., N.W . W ashington, D.C. 20007\nEmail address: christopher.ormerod@cambiumassessment.com\nUN I V E R S I T Y O F CO L O R A D O , B O U L D E R\nEmail address: Akanksha.Malhotra@colorado.edu\nCA M B I U M AS S E S S M E N T, I N C .\nCurrent address: 1000 Thomas Jefferson St., N.W . W ashington, D.C. 20007\nEmail address: amir.jafari@cambiumassessment.com",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8092213869094849
    },
    {
      "name": "Computer science",
      "score": 0.7507114410400391
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6170498728752136
    },
    {
      "name": "Natural language processing",
      "score": 0.6086349487304688
    },
    {
      "name": "Language model",
      "score": 0.533148467540741
    },
    {
      "name": "Machine learning",
      "score": 0.32309409976005554
    },
    {
      "name": "Engineering",
      "score": 0.09799417853355408
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Transformer",
  "institutions": []
}