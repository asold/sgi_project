{
    "title": "A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning",
    "url": "https://openalex.org/W4378377586",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2099994326",
            "name": "Minhyeok Lee",
            "affiliations": [
                "Chung-Ang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6838552513",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4376874793",
        "https://openalex.org/W3112689365",
        "https://openalex.org/W4323351149",
        "https://openalex.org/W4366598160",
        "https://openalex.org/W4328050835",
        "https://openalex.org/W4297253404",
        "https://openalex.org/W4313590997",
        "https://openalex.org/W4223948957",
        "https://openalex.org/W6797132756",
        "https://openalex.org/W3114632476",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W6784987805",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3154414470",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W6682264535",
        "https://openalex.org/W2296349981",
        "https://openalex.org/W4281483318",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3173151551",
        "https://openalex.org/W3105386335",
        "https://openalex.org/W2149741699",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "In this paper, we present a rigorous mathematical examination of generative pre-trained transformer (GPT) models and their autoregressive self-supervised learning mechanisms. We begin by defining natural language space and knowledge space, which are two key concepts for understanding the dimensionality reduction process in GPT-based large language models (LLMs). By exploring projection functions and their inverses, we establish a framework for analyzing the language generation capabilities of these models. We then investigate the GPT representation space, examining its implications for the modelsâ€™ approximation properties. Finally, we discuss the limitations and challenges of GPT models and their learning mechanisms, considering trade-offs between complexity and generalization, as well as the implications of incomplete inverse projection functions. Our findings demonstrate that GPT models possess the capability to encode knowledge into low-dimensional vectors through their autoregressive self-supervised learning mechanism. This comprehensive analysis provides a solid mathematical foundation for future advancements in GPT-based LLMs, promising advancements in natural language processing tasks such as language translation, text summarization, and question answering due to improved understanding and optimization of model training and performance.",
    "full_text": null
}