{
    "title": "Dependency-Based Self-Attention for Transformer NMT",
    "url": "https://openalex.org/W2986267869",
    "year": 2019,
    "authors": [
        {
            "id": null,
            "name": "Ehime University, Japan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2022666454",
            "name": "Hiroyuki Deguchi",
            "affiliations": [
                "Ehime University"
            ]
        },
        {
            "id": "https://openalex.org/A2040756077",
            "name": "Akihiro Tamura",
            "affiliations": [
                "Ehime University"
            ]
        },
        {
            "id": "https://openalex.org/A2103085203",
            "name": "Takashi Ninomiya",
            "affiliations": [
                "Ehime University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2576482813",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2884083742",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2018116550",
        "https://openalex.org/W2962788148",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2963571341",
        "https://openalex.org/W2127863960",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2552110825",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2758137671",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2594047108",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2525778437"
    ],
    "abstract": "In this paper, we propose a new Transformer neural machine translation (NMT) model that incorporates dependency relations into self-attention on both source and target sides, dependency-based selfattention.The dependency-based selfattention is trained to attend to the modifiee for each token under constraints based on the dependency relations, inspired by linguistically-informed self-attention (LISA).While LISA was originally designed for Transformer encoder for semantic role labeling, this paper extends LISA to Transformer NMT by masking future information on words in the decoderside dependency-based self-attention.Additionally, our dependency-based selfattention operates at subword units created by byte pair encoding.Experiments demonstrate that our model achieved a 1.0 point gain in BLEU over the baseline model on the WAT'18 Asian Scientific Paper Excerpt Corpus Japanese-to-English translation task.",
    "full_text": null
}