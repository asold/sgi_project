{
  "title": "Dependency-Based Self-Attention for Transformer NMT",
  "url": "https://openalex.org/W2986267869",
  "year": 2019,
  "authors": [
    {
      "id": null,
      "name": "Ehime University, Japan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2022666454",
      "name": "Hiroyuki Deguchi",
      "affiliations": [
        "Ehime University"
      ]
    },
    {
      "id": "https://openalex.org/A2040756077",
      "name": "Akihiro Tamura",
      "affiliations": [
        "Ehime University"
      ]
    },
    {
      "id": "https://openalex.org/A2103085203",
      "name": "Takashi Ninomiya",
      "affiliations": [
        "Ehime University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2576482813",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2884083742",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2018116550",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2127863960",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2758137671",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2594047108",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2525778437"
  ],
  "abstract": "In this paper, we propose a new Transformer neural machine translation (NMT) model that incorporates dependency relations into self-attention on both source and target sides, dependency-based selfattention.The dependency-based selfattention is trained to attend to the modifiee for each token under constraints based on the dependency relations, inspired by linguistically-informed self-attention (LISA).While LISA was originally designed for Transformer encoder for semantic role labeling, this paper extends LISA to Transformer NMT by masking future information on words in the decoderside dependency-based self-attention.Additionally, our dependency-based selfattention operates at subword units created by byte pair encoding.Experiments demonstrate that our model achieved a 1.0 point gain in BLEU over the baseline model on the WAT'18 Asian Scientific Paper Excerpt Corpus Japanese-to-English translation task.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.788280725479126
    },
    {
      "name": "Transformer",
      "score": 0.711095929145813
    },
    {
      "name": "Encoder",
      "score": 0.6324754953384399
    },
    {
      "name": "Machine translation",
      "score": 0.6154447793960571
    },
    {
      "name": "Security token",
      "score": 0.6002473831176758
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5948245525360107
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5337629914283752
    },
    {
      "name": "Natural language processing",
      "score": 0.5056314468383789
    },
    {
      "name": "Speech recognition",
      "score": 0.4702780544757843
    },
    {
      "name": "Decoding methods",
      "score": 0.4345118999481201
    },
    {
      "name": "Algorithm",
      "score": 0.10089263319969177
    },
    {
      "name": "Voltage",
      "score": 0.10079854726791382
    },
    {
      "name": "Engineering",
      "score": 0.09392377734184265
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I43545212",
      "name": "Ehime University",
      "country": "JP"
    }
  ]
}