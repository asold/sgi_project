{
  "title": "SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining",
  "url": "https://openalex.org/W3167019521",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5112665488",
      "name": "Chenglei Si",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101584823",
      "name": "Zhengyan Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5004381973",
      "name": "Yingfa Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5030183493",
      "name": "Fanchao Qi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100775958",
      "name": "Xiaozhi Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100320723",
      "name": "Zhiyuan Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5046448314",
      "name": "Maosong Sun",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3099911888",
    "https://openalex.org/W1506507878",
    "https://openalex.org/W3101918878",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2131988669",
    "https://openalex.org/W2010576059",
    "https://openalex.org/W2889968917",
    "https://openalex.org/W3107315802",
    "https://openalex.org/W2550225731",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W2952662724",
    "https://openalex.org/W3014328670",
    "https://openalex.org/W2949884065",
    "https://openalex.org/W3095771422",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2801060378",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3177365697",
    "https://openalex.org/W1979557360",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1524579289",
    "https://openalex.org/W3094382446",
    "https://openalex.org/W2788009253",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Conventional tokenization methods for Chinese pretrained language models (PLMs) treat each character as an indivisible token (Devlin et al., 2019), which ignores the characteristics of the Chinese writing system. In this work, we comprehensively study the influences of three main factors on the Chinese tokenization for PLM: pronunciation, glyph (i.e., shape), and word boundary. Correspondingly, we propose three kinds of tokenizers: 1) SHUOWEN (meaning Talk Word), the pronunciation-based tokenizers; 2) JIEZI (meaning Solve Character), the glyph-based tokenizers; 3) Word segmented tokenizers, the tokenizers with Chinese word segmentation. To empirically compare the effectiveness of studied tokenizers, we pretrain BERT-style language models with them and evaluate the models on various downstream NLU tasks. We find that SHUOWEN and JIEZI tokenizers can generally outperform conventional single-character tokenizers, while Chinese word segmentation shows no benefit as a preprocessing step. Moreover, the proposed SHUOWEN and JIEZI tokenizers exhibit significantly better robustness in handling noisy texts. The code and pretrained models will be publicly released to facilitate linguistically informed Chinese NLP.",
  "full_text": "Sub-Character Tokenization for Chinese Pretrained Language Models\nChenglei Si1,2∗, Zhengyan Zhang1∗, Yingfa Chen1∗, Fanchao Qi1,\nXiaozhi Wang1, Zhiyuan Liu1†, Yasheng Wang3, Qun Liu3, Maosong Sun1†\n1 NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China\n{zy-z19,yingfa-c18,qfc17,wangxz20}@mails.tsinghua.edu.cn\n{liuzy,sms}@tsinghua.edu.cn\n2 University of Maryland, College Park, MD, USA\nclsi@terpmail.umd.edu\n3Huawei Noah’s Ark Lab, Hong Kong, China\n{wangyasheng,qun.liu}@huawei.com\nAbstract\nTokenization is fundamental to pretrained\nlanguage models (PLMs). Existing tokeniza-\ntion methods for Chinese PLMs typically\ntreat each character as an indivisible token.\nHowever, they ignore the unique feature of\nthe Chinese writing system where additional\nlinguistic information exists below the\ncharacter level, i.e., at the sub-character\nlevel. To utilize such information, we\npropose sub-character (SubChar for short)\ntokenization. Speciﬁcally, we ﬁrst encode\nthe input text by converting each Chinese\ncharacter into a short sequence based\non its glyph or pronunciation, and then\nconstruct the vocabulary based on the\nencoded text with sub-word segmentation.\nExperimental results show that SubChar\ntokenizers have two main advantages over\nexisting tokenizers: 1) They can tokenize\ninputs into much shorter sequences, thus\nimproving the computational efﬁciency. 2)\nPronunciation-based SubChar tokenizers\ncan encode Chinese homophones into\nthe same transliteration sequences and\nproduce the same tokenization output,\nhence being robust to homophone typos.\nAt the same time, models trained with\nSubChar tokenizers perform competitively\non downstream tasks. We release our code\nand models at https://github.com/\nthunlp/SubCharTokenization to\nfacilitate future work.\n1 Introduction\nLarge-scale Transformer-based pretrained lan-\nguage models (PLMs) (Devlin et al., 2019; Liu\net al., 2019; Lan et al., 2020; Clark et al., 2020; He\net al., 2021, inter alia) have achieved great success\n∗Equal contribution\n†Corresponding authors\nin recent years and attracted wide research interest,\nin which tokenization plays a fundamental role.\nThe most popular type of tokenization adopted\nby PLMs is sub-word tokenization, such as byte\npair encoding (BPE) (Sennrich et al., 2016), Word-\nPiece (Schuster and Nakajima, 2012) and unigram\nlanguage model segmentation (Kudo, 2018). Re-\ncent Chinese PLMs such as CPM (Zhang et al.,\n2020, 2021b) adopt this kind of sub-word tokeniza-\ntion. Apart from sub-word tokenization, many\nother Chinese PLMs adopt a simple character to-\nkenizer (CharTokenizer for short) that treats ev-\nery single Chinese character as a token (Sun et al.,\n2019; Cui et al., 2019a, 2020, inter alia).\nHowever, we believe that both of these existing\ntokenizers are sub-optimal for Chinese. This is\nbased on the observation that Chinese has unique\nlinguistic characteristics:\n1) Chinese has an opaque orthography with ir-\nregular grapheme-phoneme correspondence (Hao\nand Yang, 2021). This is in contrast to transparent\northographies like Spanish and Finnish where each\nletter approximately represents one sound. As a re-\nsult, utilizing pronunciation information in Chinese\nrequires explicit pronunciation encoding.\n2) Chinese does not have morphological inﬂec-\ntion, unlike morphologically-rich languages like\nRussian (Coulmas, 1991). This renders sub-word\ntokenization less useful since the main advantage\nof sub-word tokenization comes from the fact that it\ncan split common afﬁxes and root words as separate\ntokens. In fact, Chinese characters are logograms,\nand their glyphs (the composition of radicals) also\ncontain rich semantic information, which can only\nbe captured at the sub-character level.\nMotivated by these observations, we propose the\nnovel sub-character (SubChar) tokenization. It ﬁrst\nencodes every Chinese character into a sequence of\nphonetic or stroke symbols, and then it uses a sub-\narXiv:2106.00400v3  [cs.CL]  14 Feb 2023\n我们家乡安徽歙县风景秀丽。Our hometown, ShexianCounty, AhhuiProvince, has beautiful scenery.  \nCharTokenizer\nSub-word\nSubChar-Wubi\nSubChar-Pinyin\n我| 们|家|乡| 安| 徽| 歙| 县| 风|景|秀| 丽|。(13)\n我们|家|乡| 安| 徽| 歙| 县| 风|景|秀| 丽|。(12)\nq#wu# (我们) | pe#xte# (家乡) | pv#tmgt3# (安徽)|  wgk|w |4# (歙) | egc# (县) | mq#jy#te#gmy0# (风景秀丽)|。(9)\nwo③3#men②19# (我们) |jia①13#xiang①1# (家乡)|an①13#hui①12#(安徽)|  she④16#xian④22# (歙县) |feng①58#jing③13# (风景) |xiu④8#li④50#(秀丽) | 。(7)\nFigure 1: Comparison of existing tokenizers (character tokenizer and sub-word tokenizer) and our sub-character\ntokenizers (SubChar-Wubi using glyph and SubChar-Pinyin using pronunciation encoding). Different tokens\nproduced by the tokenizers are separated by ‘|’. The numbers in(brackets) indicate the number of tokens in the\ntokenized sequence. Tokens in orange indicate character combinations, while tokens in green indicate sub-character\ntokens. ‘#’ indicates the special separation symbol after each character, circled numbers (1 2 3 4 ) indicate the\nintonation of characters. (Figure best viewed in color.)\nword segmenter (such as BPE) to construct the vo-\ncabulary on all the encoded sequences. In this way,\nthe resultant tokenizers can capture sub-character\ntokens that correspond to meaningful phonetic or\nmorphemic units, which are absent from all exist-\ning Chinese tokenizers. As far as we know, this\nis the ﬁrst attempt on leveraging the sub-character\ninformation for language models, especially in the\ncontext of Chinese NLP.\nTo assess the effectiveness of our proposed\nmethod, we train a series of BERT-style PLMs\nusing the existing and proposed tokenizers. We\nevaluate these models on over ten datasets of var-\nious downstream natural language understanding\n(NLU) tasks. Through extensive evaluation, we\nﬁnd that models trained with SubChar tokenizers\nmatch models trained with character and sub-word\ntokenizers on downstream task performance. More\nimportantly, SubChar tokenizers have two major\nadvantages compared to existing tokenizers:\n1) SubChar tokenizers are more efﬁcient. We\nﬁnd that a small fraction of sub-character tokens\nin the vocabulary can compose a large variety of\nrare and complex characters, thus saving much\nspace in the vocabulary for more character com-\nbination tokens such as words and phrases. The\nincreased use of combination tokens leads to signif-\nicantly decreased length of the tokenized sequences.\nFor example, on the iFLYTEK long text classiﬁca-\ntion dataset, with the same vocabulary size as the\nCharTokenizers, SubChar tokenizers can achieve\nas much as 40% length reduction on the tokenized\noutput. Such length reduction can signiﬁcantly\nspeed up both pretraining and ﬁnetuning.\n2) SubChar tokenizers are more robust. A\ncommon and unique type of typos in Chinese is\ncaused by homophones where characters with dif-\nferent semantic meanings have exactly the same\npronunciation. SubChar tokenizers based on pro-\nnunciation can map homophones into the same\ntransliteration sequences, thus improving robust-\nness against any homophone typos. This could be\nimmensely useful when handling noisy inputs.\nWe believe that our work is an important step\ntowards more tailored techniques for languages\nbeyond just English by effectively integrating\nthe unique linguistic characteristics of the lan-\nguage (Bender, 2019, #BenderRule).\n2 Method\nIn this section, we describe our proposed SubChar\ntokenization in detail. We break it down into two\nsteps: 1) Chinese character encoding; 2) vocabu-\nlary construction based on the encoded sequences.\n2.1 Step 1: Character Encoding\nThe core idea of this step is to encode every Chi-\nnese character into a sequence that characterizes\nits glyph or pronunciation, in order to provide ad-\nditional inductive biases to the model. We explore\nseveral ways of encoding the characters. They can\nbe categorised as pronunciation-based and glyph-\nbased encoding.\nPronunciation-based encoding In order to cap-\nture pronunciation information of characters, we\nencode Chinese characters using transliteration,\nwhich uses IPA-inspired1 phonetic scripts to char-\nacterize the pronunciation.\nWe explore two different transliteration methods:\npinyin and zhuyin (i.e., bopomofo) . Pinyin uses\nromanized transcription and four different tones (¯,\n´, ˇ, `) to transliterate characters,e.g., 魑魅魍魉→\nChi¯ Mei` Wangˇ Liangˇ. On the other hand, zhuyin\nuses a set of graphemes nonexistent in English and\nthe same four tones to transliterate the characters,\ne.g., 魑魅魍魉 → ㄔㄇㄟ` ㄨㄤˇ ㄌㄧㄤˇ. In\nzhuyin, the ﬁrst tone mark (¯) is usually omitted.\nWe insert special separation symbols ( #) af-\nter each character’s transliterated sequence, e.g.,\nChi¯#Mei`#Wangˇ#Liangˇ#,ㄔ#ㄇㄟ`#ㄨㄤˇ#ㄌ\nㄧㄤˇ#. This prevents cases where transliterated se-\nquences of different characters are mixed together,\nespecially when there are no tone markers to split\nthem in zhuyin.\nDifferent Chinese characters may have the same\npronunciation even if they have different seman-\ntic meanings ( i.e., homophones). For disam-\nbiguation, we append different indices after the\nencoded sequences for the homophonic charac-\nters, so as to allow a biunique mapping between\neach Chinese character and its transliteration se-\nquence, e.g., Chi¯33#Mei`24#Wangˇ25#Liangˇ13#,\nㄔ10#ㄇㄟ`3#ㄨㄤˇ6#ㄌㄧㄤˇ1#.\nIt is unclear whether having such disambiguation\nof homophones is beneﬁcial or not. To analyse the\nimpact, we also experiment with a variant where\nwe do not add the indices to disambiguate the ho-\nmophones. We implement the tokenizer SubChar-\nPinyin-NoIndex to perform pinyin encoding with-\nout disambiguation indices. We will show that this\nvariant also has the advantage of being robust to\nhomophone typos (section 4.2).\nGlyph-based encoding The glyphs (i.e., shapes)\nof Chinese characters contain rich semantic infor-\nmation and can help NLP models (Cao et al., 2018).\nMost Chinese characters can be broken down into\nsemantically meaningful radicals. Characters that\n1IPA: International Phonetic Alphabet ( https:\n//en.wikipedia.org/wiki/International_\nPhonetic_Alphabet)\nshare common radicals often have related semantic\ninformation, e.g., the four characters ‘魑魅魍魉’\nshare the same radical ‘鬼’(meaning “ghost”), and\ntheir meanings are indeed all related to ghosts and\nmonsters.2 In order to capture glyph information,\nwe explore four glyph-based encoding methods,\nnamely Stroke, Wubi, Zhengma, and Cangjie.\nFor stroke encoding, we use the Latin alphabet\nto represent the set of Chinese strokes and con-\nvert the characters based on the standard stroke\norders,3 e.g., 魑→pszhshpzznnhpnzsszshn; 魅→\npszhshpzznhhspn (underlined parts indicate shared\nstroke sequences across these characters).\nThe other three glyph-based encoding meth-\nods encode characters into radical sequences in-\nstead, by using glyph-based Chinese input meth-\nods: Wubi, Zhengma and Cangjie. These input\nmethods group strokes together in different ways\nto form radicals, and then decompose characters\ninto radical sequences. We use the Latin alphabet\nto represent these radicals, e.g., 魑魅魍魉→Wubi:\nrqcc rqci rqcn rqcw; Zhengma: njlz njbk njld njoo;\nCangjie: hiyub hijd hibtv himob (underlined parts\nindicate common radicals among them).\nWe append the same separation symbol (‘#’) af-\nter each character, and also add the disambiguation\nindices for characters whose stroke sequences are\nidentical (e.g., 人 (people) and 八 (eight)). How-\never, we note that there are very few cases where\ndifferent characters have the same glyph encoding.\n2.2 Step 2: Vocabulary construction\nOnce we have the encoded sequences, we can treat\nthe encoding of each character as the equivalent\nof ‘word’ in English and then apply sub-word seg-\nmentation to construct the vocabulary for our sub-\ncharacter tokenizers.\nSub-word segmentation typically forms sub-\nword tokens by merging frequent token bigrams,\nwhich often results in meaningful morphemes of\nthe words when used in languages like English.\nOn our encoded sequences, sub-word segmenta-\ntion can capture shared sub-character sequences\nthat correspond to shared radicals or phonetic se-\nquences among similar characters. After running\nthe sub-word segmentation step on the encoded\nsequences, the vocabulary of the resultant sub-\ncharacter tokenizers consists of a mixture of sub-\n2The word ‘魑魅魍魉’is in fact a Chinese idiom, which\nis often used to refer to bad people who are like monsters.\n3https://en.wikipedia.org/wiki/Stroke_\norder\nFigure 2: Illustration of the tokenization pipeline when\nincorporating CWS. After the ﬁrst step of CWS, high-\nfrequency words (words in the dashed box) directly be-\ncome part of the ﬁnal output sequence, the other words\nthen go through SubChar tokenization.\ncharacter tokens, character tokens, and character\ncombination tokens.\nIn this work, we use the unigram language model\nsegmentation method (Kudo, 2018) implemented\nin SentencePiece (Kudo and Richardson, 2018) as\nthe default sub-word segmentation method. In sec-\ntion 5.6, we also perform an ablation study by set-\nting the sub-word segmentation method to BPE,\nwhich results in similar performance and efﬁciency,\nillustrating that the gains of SubChar tokenization\nare insensitive to the speciﬁc choice of sub-word\nsegmentation methods.\n2.3 Optional Step: Chinese Word\nSegmentation\nBefore the ﬁrst step of character encoding, there is\nan optional step of Chinese word segmentation.\nChinese word segmentation ( CWS ) is a com-\nmon technique to split Chinese text chunks into\na sequence of Chinese words. The resultant seg-\nmented words sometimes provide better granularity\nfor downstream tasks (Chang et al., 2008). How-\never, the impact of CWS is unclear in the context\nof pretraining, especially its interplay with the tok-\nenization. Hence, we propose a way to incorporate\nCWS into our SubChar tokenization and examine\nwhether it is helpful. Our proposed tokenization\npipeline is summarized in Figure 2.\nGiven that the vocabulary of SubChar tokenizers\nconsists of character combinations, characters, and\nsub-characters, we use CWS to construct the charac-\nter combination part of the vocabulary. Compared\nto the character combination tokens generated by\nDataset #Train #Dev #Test\nTNEWS 53.4K 10K 10K\nIFLYTEK 12.1K 2.6K 2.6K\nBQ 100K 10K 10K\nTHUCNEWS 669K 83.6K 83.6K\nCLUEWSC 1.2K 0.3K 0.3K\nAFQMC 34.3K 4.3K 3.9K\nCSL 20K 3K 3K\nOCNLI 45.4K 5K 3K\nCHID 519K 57.8K 23K\nC3 12K 3.8K 3.9K\nCMRC 10K 3.4K 4.9K\nCLUENER 11K 1.3K 1.3K\nTable 1: Statistics of downstream datasets.\nthe statistical approach of sub-word tokenization,\nthe combination tokens generated by a trained Chi-\nnese word segmenter have more linguistic prior\nknowledge.\nSpeciﬁcally, to construct the vocabulary, we ﬁrst\nsegment the pretraining corpus into words. Then,\nwe select the most frequent words as the character\ncombination part of the SubChar tokenizer vocab-\nulary. We then encode the text with one of the\npronunciation- or glyph-based encoding methods\nand use sub-word tokenization on the encoded se-\nquences to get the sub-character and character to-\nkens of the vocabulary. Finally, we merge these\nparts together as the vocabulary for the SubChar\ntokenizer. When tokenizing new inputs, we ﬁrst\nsegment them into words, if the words are in the vo-\ncabulary, they will be tokenized as word tokens; if\nnot, they will be further processed by the SubChar\ntokenizer. We control the ratio of word tokens in\nthe vocabulary to be 80% based on preliminary tun-\ning and we use a state-of-the-art segmenter THU-\nLAC (Li and Sun, 2009; Sun et al., 2016) for word\nsegmentation.\n3 Experiment Setup\nIn this section, we introduce our baselines, datasets\nand experiment settings.\n3.1 Baselines\nWe compare two existing tokenization methods\nas baselines, namely single-character tokenization\nand sub-word tokenization. For a fair comparison,\nwe set the same vocabulary size of 22, 675 for all\ntokenizers, including baselines and our proposed\ntokenizers. This is consistent with the vocabulary\nsize of Chinese BERT (Devlin et al., 2019).\n3.2 Pretraining Data\nWe use the same training corpus to train all the\ntokenizers in this work. The corpus consists of 2.3\nGB Chinese text from Baidu Baike.4\nTo evaluate the effectiveness of the tokenizers,\nwe pretrain a BERT5 model using each tokenizer\nand compare their performance on downstream\ntasks. When pretraining the BERT models, we use\nthe same pretraining corpus (i.e., Baidu Baike) and\nthe same set of hyper-parameters. Notably, we also\npretrain a new BERT model using the character\ntokenizer on our pretraining corpus instead of load-\ning from existing checkpoints (Devlin et al., 2019)\nso that it provides an apple-to-apple comparison\nwith our proposed methods. Since our proposed\ntokenizers are direct drop-in replacements for the\nbaseline tokenizers, they do not incur any extra pa-\nrameters. In summary, all the compared models\nhave the same training corpus, hyper-parameters,\nand number of parameters, allowing for a truly fair\ncomparison.\n3.3 Evaluation Data\nWe ﬁnetune and evaluate the pretrained mod-\nels with different tokenization methods on vari-\nous downstream NLU datasets, including single-\nsentence classiﬁcation, sentence-pair classiﬁcation,\nand reading comprehension tasks. We brieﬂy in-\ntroduce each dataset below and present the dataset\nstatistics in Table 1.\nTNEWS (Xu et al., 2020b) is a news title classiﬁ-\ncation dataset containing 15 classes.\nIFLYTEK (Xu et al., 2020b) is a long text classiﬁ-\ncation dataset containing 119 classes. The task is\nto classify mobile applications into corresponding\ncategories given their description.\nBQ (Chen et al., 2018) is a sentence-pair question\nmatching dataset extracted from an online bank cus-\ntomer service log. The goal is to evaluate whether\ntwo questions are semantically equivalent.\nTHUCNEWS (Li and Sun, 2007) is a document\nclassiﬁcation dataset with 14 classes. The task is\nto classify news into the corresponding categories\ngiven their title and content.\nCLUEWSC (Xu et al., 2020b) is a coreference res-\nolution dataset in the format of Winograd Schema\nChallenge (Levesque et al., 2012). The task is to\n4https://baike.baidu.com/\n5Note that we mean BERT-style pretrained Transformers.\nOur models are not directly comparable with the original\nChinese BERT since we use different pretraining data and\nhyper-parameters.\ndetermine whether the given noun and pronoun in\nthe sentence refer to the same entity.\nAFQMC (Xu et al., 2020b) is the Ant Financial\nQuestion Matching Corpus for the question match-\ning task that aims to predict whether two sentences\nare semantically equivalent.\nCSL6 is the Chinese Scientiﬁc Literature dataset\nextracted from academic papers. Given an ab-\nstract and some keywords, the goal is to determine\nwhether they belong to the same paper. It is format-\nted as a sentence-pair classiﬁcation task.\nOCNLI (Hu et al., 2020) is a natural language\ninference dataset. The task is to determine\nwhether the relationship between the hypothesis\nand premise is entailment, neutral, or contradic-\ntion.\nCHID (Zheng et al., 2019) is a cloze-style multiple-\nchoice reading comprehension dataset. Given a\ncontext where some idioms are masked, the task\nis to select the appropriate idiom from a list of\ncandidates.\nC3 (Sun et al., 2020) is a multiple-choice reading\ncomprehension dataset. The goal is to choose the\ncorrect answer for the questions given context.\nCMRC (Cui et al., 2019b) is a span-extraction\nreading comprehension dataset consisting of ques-\ntions annotated from Wikipedia paragraphs.\nCLUENER2020 (Xu et al., 2020a) is a named en-\ntity recognition dataset with 10 entity types.\n3.4 Hyper-parameters\nWe elaborate on all hyper-parameters involved for\nreproducibility (we also release all code, trained\ntokenizers and models).\nTokenizer Training. When training tokenizers\nwith SentencePiece, we use a character coverage\nof 1.0 and model type ‘unigram’ for all tokenizers\nbeing compared. Other hyper-parameters follow\nthe default of SentencePiece.\nBERT pretraining. We follow the training pro-\ncedure of BERT (Devlin et al., 2019) except that\nthe next sentence prediction objective is removed.\nThe pretraining process consists of two stages. The\nﬁrst stage uses a maximum sequence length of 128\nwith a batch size of 8K for 8K steps. The second\nstage uses a maximum sequence length of512 with\na batch size of 4K for 2K steps. We experiment\nprimarily with 6-layer Transformer (Vaswani et al.,\n2017) models. To ablate the impact of model size,\nwe also pretrain 12-layer Transformer models for\n6https://github.com/P01son6415/CSL\nTNEWS IFLY THUC BQ WSC AFQMC CSL OCNLI CHID C3 A VG\n6-layer, 2.3G Corpus\nCharTokenizer 64.19\n±0.18\n55.83\n±0.50\n96.95\n±0.04\n81.99\n±0.47\n63.39\n±1.95\n68.68\n±0.46\n82.67\n±0.46\n68.19\n±0.39\n72.48\n±0.23\n53.17\n±0.56\n70.75\n±0.31\nSub-word 64.09\n±0.28\n54.88\n±0.39\n97.14\n±0.03\n81.94\n±0.28\n62.67\n±2.87\n69.25\n±0.42\n83.20\n±0.27\n69.03\n±0.44\n72.78\n±0.13\n53.32\n±0.44\n70.83\n±0.35\nSubChar-Wubi 63.89\n±0.25\n58.64\n±0.27\n97.02\n±0.04\n81.70\n±0.29\n64.61\n±2.09\n68.75\n±0.59\n82.81\n±0.46\n68.93\n±0.38\n72.54\n±0.15\n54.68\n±0.77\n71.36\n±0.23\nSubChar-Pinyin 63.68\n±0.25\n58.81\n±0.28\n97.04\n±0.03\n81.74\n±0.24\n65.90\n±1.45\n68.89\n±0.42\n82.87\n±0.40\n67.98\n±0.45\n73.06\n±0.13\n53.03\n±0.47\n71.42\n±0.19\n12-layer, 2.3G Corpus\nCharTokenizer 64.39\n±0.13\n58.52\n±0.46\n97.02\n±0.03\n83.49\n±0.38\n68.09\n±1.59\n69.00\n±0.35\n82.77\n±0.33\n70.40\n±0.34\n74.44\n±0.17\n54.22\n±0.40\n72.23\n±0.26\nSubChar-Pinyin 64.19\n±0.14\n59.67\n±0.23\n97.12\n±0.03\n82.28\n±0.16\n71.71\n±2.03\n69.30\n±0.24\n82.23\n±0.27\n70.43\n±0.25\n74.82\n±0.09\n55.92\n±0.26\n72.87\n±0.17\n12-layer, 22.1G Corpus\nCharTokenizer 64.43\n±0.57\n59.10\n±0.29\n97.12\n±0.01\n82.70\n±0.02\n70.39\n±1.32\n69.39\n±0.06\n82.97\n±0.28\n69.37\n±0.14\n76.34\n±0.62\n54.84\n±1.24\n72.81\n±0.18\nSubChar-Pinyin 64.64\n±0.47\n59.14\n±0.17\n97.10\n±0.04\n83.56\n±0.18\n72.36\n±0.98\n70.67\n±0.66\n82.94\n±0.05\n69.50\n±0.24\n75.92\n±0.45\n58.64\n±0.35\n73.42\n±0.09\nTable 2: Results on downstream datasets of different tokenizers. The last column indicates average performance.\nThe subscript is the standard deviation. Models trained with sub-character tokenizers can match the performance of\nbaseline models across all datasets. Ablation shows that increasing the model size or pretraining corpus size can\nslightly improve downstream task performance. These ablation results support our overall conclusion that models\ntrained with SubChar tokenizers can closely match or slightly outperform the baselines.\nthe baseline CharTokenizer and proposed SubChar-\nPinyin tokenizer. Other model conﬁgurations are\nthe same for all models: 12 attention heads, an in-\ntermediate size of 3072, and a hidden size of 768.\nBERT ﬁnetuning. For the ﬁnetuning on down-\nstream datasets, we use a batch size of 32, maxi-\nmum training epochs of 24 and tune max sequence\nlength in {96, 256, 512}. Since the original test sets\nare not released, we use the original dev sets as the\ntest sets and randomly hold-out 10% of the training\nset as the dev sets. We select the best checkpoint\non the dev sets and report performance on test sets.\nThese hyper-parameters are consistent with previ-\nous work. For all experiments in this paper, we\nreport the results of the average run of three dif-\nferent random seeds. All experiments are done on\nNVIDIA A100 GPUs.\n4 Experiment Results\nIn this section, we present the experiment results\nand the main ﬁndings. We not only evaluate on\na wide range of common Chinese NLU datasets,\nbut also perform robustness evaluation on both syn-\nthetic and real-world noisy data.\n4.1 Standard Evaluation\nWe compare models trained with our SubChar to-\nkenizers and the baseline tokenizers. There are\nmultiple possible encoding methods for SubChar\ntokenizers as described in section 2. In this section,\nwe choose two representative ones: Wubi (glyph-\nbased) and Pinyin (pronunciation-based). We later\nshow a full ablation of all different encoding meth-\nods in section 5.5.\nTable 2 shows the performance of BERT models\nwith different tokenizers on downstream datasets.\nExamining the results of the 6-layer BERT models\npretrained on the 2.3G Baidu Baike corpus, we\nobserve that despite some variation across different\ndatasets, our proposed sub-character tokenizers can\nmatch the baselines on downstream datasets. When\nscaling the 6-layer models to 12-layer, we observe\nmoderate improvement on the average performance\n(70.75 → 72.23 for CharTokenizer and 71.42 →\n72.87 for SubChar-Pinyin). Besides, we discuss\nthe impact of pretraining data size in section 5.4.\nThese results demonstrate that on standard NLU\nbenchmarks, our proposed tokenizers can serve as\na very strong alternative.\nTNEWS\nclean 7.5 % 15.0 % 22.5 % 30.0 % 37.5 %\nCharTokenizer 64.10 63.09 58.96 50.91 38.33 25.20\nSub-word 64.09 62.82 57.75 48.67 36.37 25.72\nSubChar-Pinyin 63.68 61.95 56.67 45.22 30.71 27.53\nSubChar-Pinyin-NoIndex 63.28 63.28 63.28 63.28 63.28 63.28\nOCNLI\nclean 7.5 % 15.0 % 22.5 % 30.0 % 37.5 %\nCharTokenizer 68.37 64.89 56.85 47.65 40.48 36.36\nSub-word 68.84 64.33 56.49 48.07 42.68 38.28\nSubChar-Pinyin 67.70 61.93 54.39 46.01 40.24 37.33\nSubChar-Pinyin-NoIndex 67.91 67.91 67.91 67.91 67.91 67.91\nC3\nclean 7.5 % 15.0 % 22.5 % 30.0 % 37.5 %\nCharTokenizer 53.13 51.46 49.22 47.71 46.78 43.95\nSub-word 53.55 51.66 49.49 47.81 46.24 43.58\nSubChar-Pinyin 52.87 50.45 47.26 44.50 42.42 40.07\nSubChar-Pinyin-NoIndex 53.65 53.65 53.65 53.65 53.65 53.65\nTable 3: Results for noisy evaluation with homophone typos. Different columns correspond to different percentages\nof typos in the test data. The BERT model with our SubChar-Pinyin-NoIndex tokenizer (results in bold) suffers no\nperformance drop on noisy test data since it is robust to all homophone typos.\nFigure 3: An actual interface of the popular pinyin input\nmethod. The ﬁrst line yi yi is the user input of the roman-\nization sequence, all words with this same pronunciation\nare listed below for users to choose from.\n4.2 Robustness Evaluation\nApart from evaluating on the standard benchmarks,\nwe also verify whether our proposed tokenization\nmethods are better at handling noisy inputs. We\ncover two major Chinese input methods: keyboard\ninput and speech input. For keyboard input, we\nconstruct synthetic noise tests via character substi-\ntutions. For speech input, we use a noisy test set\nincluding inputs with diverse accents, which poses\ngreater typo diversity. Our SubChar-Pinyin method\nshows advantage in both cases.\nSynthetic Typos We simulate the homophone ty-\npos that are common in real-world Chinese writ-\ning systems, especially user-generated inputs. As\nshown in Figure 3, pinyin input is the most widely\nFigure 4: Illustration of how our SubChar-Pinyin-\nNoIndex tokenizer is robust to any homophone typos.\nThe possible homophone typos (characters in purple\ndashed boxes) are mapped into the same romanization\nsequence as the intended correct characters, and hence\nthe resultant tokenization based on the romanized se-\nquences would be the same.\nused keyboard input method for Chinese users. 7\nWhen users type in the romanization of the in-\ntended characters, the input interface will present\nall Chinese characters with the same romanization\nfor the users to choose from. As a result, it is com-\nmon for users to choose the wrong characters either\nby mistake or because they are unclear about the\ndifferences among these homophones.\nIn such cases, our SubChar-Pinyin-NoIndex tok-\nenizer (described in section 2.1) has the advantage\nof being robust towards any such homophone typos.\nAs illustrated in Figure 4, the character encoding\nwill map all homophones of a character into the\nsame romanization sequence before undergoing the\n7https://en.wikipedia.org/wiki/\nChinese_input_methods_for_computers\nsub-word tokenization. As a result, the tokenized\noutput will be identical no matter what the typo\ncharacter is as long as it is a homophone of the\nintended character.\nWe inject synthetic noises into the test data and\nexamine whether models trained on clean training\ndata can perform well on these noisy data. To con-\nstruct the noisy data, we replace the original correct\ncharacters with their homophones, e.g., change ‘意’\n(sense) to ‘异’ (different)’ and ‘义’ (meaning) to\n‘议’ (debate).8 Speciﬁcally, we randomly sample\na certain ratio r% of the original characters. For\neach of them, we replace it with a randomly sam-\npled homophone from all its homophones obtained\nvia a Pinyin dictionary (no replacement if it has no\nhomophones).\nThe results are shown in Table 3. We observe\nthat there can be a signiﬁcant drop in performance\nwhere there exist homophone typos in the test\ndata. For example, the BERT model trained with\nCharTokenizer drops from 64.10% accuracy on\nclean data to 25.20% accuracy when 37.5% of the\ncharacters in test inputs are replaced with homo-\nphone typos. Overall, we ﬁnd that the charac-\nter tokenizer, sub-word tokenizer, as well as the\nvanilla SubChar-Pinyin tokenizer, cannot handle\nsuch noisy data. However, our SubChar-Pinyin-\nNoIndex tokenizer exhibits no performance drop\nunder noises. Moreover, despite learning a shared\nrepresentation for homophones, the model with\nSubChar-Pinyin-NoIndex still performs competi-\ntively on the clean test sets, either match (on C3) or\nonly a little worse than the baselines (on TNEWS\nand OCNLI).\nReal-World Typos While the above synthetic\ntypos aim to simulate typos in keyboard inputs,\nanother major input method is through speech in-\nput where users speak to their devices (like mo-\nbile phones) and their speech input is then con-\nverted to text for downstream tasks. In order to\nevaluate model robustness in such scenarios, we\nuse a realistically collected test set that captures\nsuch speech input typos. Speciﬁcally, we use the\nspeech-noise version of the AFQMC test set from\nthe READIN (Si et al., 2023) benchmark. For each\nexample in this noisy AFQMC test set, three anno-\ntators with different accents read the original input,\nand then the speech recordings are converted to\n8Interestingly, all these four characters have the same pro-\nnunciation but different meanings. Moreover, “意义\" (mean-\ning) and “异议\" (objection) are homophone words.\nClean N-Avg N-Worst\nCharTokenizer 73.02 44.11 18.81\nSub-word 74.22 42.21 16.91\nSubChar-Pinyin 73.24 45.24 19.47\nTable 4: Results on the real-world AFQMC noisy test\nset. Each clean test instance is annotated by three dif-\nferent annotators, we report both the macro-average\non these noisy annotations (N-Average) as well as the\naverage of the worst-case performance across all test\nexamples (N-Worst). SubChar-Pinyin outperforms base-\nlines on the challenging noisy test set (best results on\nthe noisy test set are in bold).\ntext using commercial automatic speech recogni-\ntion (ASR) software. We refer readers to the dataset\ndescription paper for more data construction details.\nWhen computing performance for each test exam-\nple, we compute both the average across different\nannotations (Noisy-Average), as well as the worst\nperformance across different annotations (Noisy-\nWorst), and then take the macro-average across\nall examples. The character-level error rate of the\nnoisy test set is 30% on average.\nThis AFQMC noisy test set contains not only\nhomophone typos, but also a wide range of other\ntypes of real-world input noises due to both the\naccent variations and ASR errors. The greater di-\nversity of typo types in the real-world test set makes\nit much more challenging to maintain robustness\nthan the synthetic setting which only considers ho-\nmophone typos. While the original AFQMC is\na binary classiﬁcation task that classiﬁes whether\nthe question pair is a paraphrase or not, we ﬁnd\nthat models trained on the AFQMC training set ex-\nploit spurious correlations like lexical overlap, even\nthough we explicitly balanced the training set. In\nparticular, when introducing typos in the test data,\nperformance on positive examples drops drastically\ndue to lower lexical overlap, while the performance\non negative examples stays or even improves a lit-\ntle because of the lower lexical overlap caused by\nthe typos. This is similar to previous ﬁndings on\nHANS (McCoy et al., 2019) and PAWS (Zhang\net al., 2019a). Hence, we follow the evaluation\npractice when dealing with spurious correlation,\nwhich is to focus on improving the worst-group\nperformance, and in this case, we focus on improv-\ning performance on the positive examples against\nthe impact of typos.\nThe results are shown in Table 4 where we re-\nport performance on the AFQMC positive exam-\nples. All models are trained on the original clean\ndata from AFQMC (we balanced the positive and\nnegative classes during training). We evaluate on\nthe original clean test set, the Noisy-Average per-\nformance (N-Average), and the Noisy-Worst per-\nformance (N-Worst). We can see that despite this\nmore challenging speech typo setting, our SubChar-\nPinyin model still outperforms the baselines.\nThese results highlight the robustness advantage\nof our Sub-Character tokenization method in both\ndealing with synthetic homophone typos as well as\non more diverse real-world typos.\n4.3 Effect of CWS\nWe examine the impact of incorporating CWS in\nthe tokenization as described in Section 2.3. We\ntrain tokenizers with and withoutCWS and compare\nthe performance of the corresponding pretrained\nmodels. As shown in Table 5, we can see that\nadding CWS as an additional step does not help\ndownstream task performance. These results serve\nas empirical evidence that CWS is ineffective in the\nuse of PLMs, complementary to the results of Li\net al. (2019) on models without pretraining.\n4.4 Character-Level Tasks\nThe evaluation in Section 4.1 is restricted to\nsequence-level classiﬁcation tasks such as single-\nsentence classiﬁcation, sentence-pair classiﬁcation\nand machine reading comprehension.\nOne might wonder how do SubChar tokenizers\nhandle character-level tasks where classiﬁcation is\ndone on every single character, such as sequence\nlabeling and span extraction. Since SubChar tok-\nenizers may combine multiple characters into one\ntoken or split one character into sub-character to-\nkens, directly adding a classiﬁcation head on each\ntoken may cause discrepancy with the human an-\nnotation, which is done on the character level. For\nexample, it is infeasible to evaluate the POS tag of\na sub-character token.\nTo handle such situations, we perform classiﬁca-\ntion on the character level for these tasks. To obtain\nthe representation of each character, we average the\nrepresentations of all its sub-character tokens. We\napply this on the ﬁnal layer of BERT and feed the\ncharacter representation to a linear classiﬁer for\ndownstream tasks.\nWe measure the performance of this approach on\nCMRC (span-extraction reading comprehension)\nand CLUENER (named entity recognition) and\nshow the results in Table 6. The results show that\nour model can indeed handle character-level tasks\nFigure 5: Breakdown of different types of tokens in the\nvocabularies of various tokenizers. We observe the clear\ntrend that in our SubChar tokenizers, a small fraction of\nsub-character tokens saves the space to store much more\ncharacter combination tokens (e.g., words and phrases).\nwith this simple adaptation. There might be bet-\nter ways of adopting our model on character-level\ntasks, and we leave it to future work.\n5 Analysis\nIn this section, we conduct various analyses to bet-\nter understand the working mechanisms of Sub-\nChar tokenization, including illustrations of the\nefﬁciency improvement and ablations on different\ncomponents of our tokenization pipeline.\n5.1 Vocabulary Composition\nWe break down the vocabulary of each tokenizer\ninto three different categories: sub-character to-\nkens, character tokens, and character combination\ntokens (words and phrases). As shown in Figure 5,\ncharacter tokenizers only have character tokens,\nwhile sub-word tokenizers have a small percent-\nage of combination tokens. The main reason for\nthe relatively small number of combination tokens\nin sub-word tokenizers is that unlike how English\nwords are composed with 26 alphabet letters, there\nare thousands of unique Chinese characters, which\ntake up a large proportion of the vocabulary in or-\nder to maintain coverage.\nIn contrast, SubChar tokenizers use a very small\nfraction of sub-character tokens to compose many\ncomplex Chinese characters, therefore saving up a\nlarge percentage of the vocabulary to store combi-\nnation tokens. This brings the advantage of having\nmore words and phrases in the tokenized outputs,\nthus shortening the sequence lengths, as elaborated\nin the next section.\n5.2 Efﬁciency Improvement\nThe direct consequence of having more character\ncombinations in the vocabulary is that the tokenized\nsequences are shorter. Table 7 shows the average\nTNEWS IFLYTEK CLUEWSC AFQMC CSL OCNLI C3 A VG\nSub-word 64.09 54.88 62.67 69.25 83.20 69.03 53.32 65.21\nSub-word +CWS 64.26 54.15 63.05 69.62 82.87 68.64 51.77 64.91 (-0.30)\nSubChar-Wubi 63.89 58.64 64.61 68.75 82.81 68.93 54.68 66.04\nSubChar-Wubi +CWS 63.57 58.01 64.38 69.41 82.62 69.43 53.15 65.80 (-0.24)\nSubChar-Pinyin 63.68 58.81 65.90 68.89 82.87 67.98 53.03 65.88\nSubChar-Pinyin +CWS 63.73 57.89 64.51 69.66 82.90 69.93 53.63 66.04 (+0.16)\nTable 5: Results of models trained with different tokenizers. Numbers in brackets indicate the difference between\nadding and not adding the CWS step in tokenization. Adding CWS brings no signiﬁcant improvement in performance.\nCMRC CLUENER\nCharTokenizer 56.58 69.61\nSub-word 55.85 67.94\nSubChar-Wubi 54.45 70.63\nSubChar-Pinyin 55.18 70.77\nTable 6: Results on two character-level classiﬁcation\ndatasets: CMRC (span-extraction) and CLUENER\n(named entity recognition). Models are 6-layer BERT.\nModels with SubChar tokenizers perform close to the\nbaseline models.\niFLYTEK TNEWS\nCharTokenizer 289.0 22.0\nSub-word 255.2 20.1\nSubChar-Wubi 183.2 15.8\nSubChar-Pinyin 185.2 16.1\nSubChar-Pinyin-NoIndex 175.4 15.2\nTable 7: Comparison of average length of tokenized\nsequences with different tokenizers. SubChar tokeniz-\ners produce much shorter tokenized sequences than the\nbaselines. SubChar-Pinyin-NoIndex tokenizer achieves\nthe most length reduction. BPE and Unigram LM coun-\nterparts achieve similar speedup improvement.\nsequence length by using different tokenizers on\ntwo downstream datasets. We observe that Sub-\nChar tokenizers can tokenize the inputs into much\nshorter sequences.\nMoreover, our SubChar tokenizers can speed up\ntraining for both pretraining and ﬁnetuning. Dur-\ning ﬁnetuning, we can pack multiple sequences\ninto one input sequence to reduce the computation\nwaste introduced by sequence padding (Krell et al.,\n2021), and shorter sequence lengths allow the se-\nquences to be packed more densely, thus increasing\nthe overall throughput.\nTable 8 shows the model ﬁnetuning time relative\nto the CharTokenizer baseline. We observe signiﬁ-\ncant speedup by SubChar tokenizers, ﬁnishing in as\nlittle as 68.9% time on iFLYTEK with the SubChar-\nPinyin-NoIndex tokenizer. In Figure 6, we plot the\ntraining curves for the CharTokenizer baseline and\nTNEWS iFLYTEK\nCharTokenizer 100.0% 100.0%\nSub-word 99.9% 92.6%\nSubChar-Wubi 87.0% 69.6%\nSubChar-Pinyin 83.8% 70.4%\nSubChar-Pinyin-NoIndex 82.7% 68.9%\nTable 8: Finetuning time of models with different tok-\nenizers. Numbers indicate time relative to the CharTok-\nenizer baseline model. Models with SubChar tokenizers\ntake much shorter time to ﬁnish ﬁnetuning. SubChar-\nPinyin-NoIndex brings the most speedup.\nFigure 6: Training curves on the iFLYTEK dataset with\ntwo different models. The y-axis indicates classiﬁcation\nloss (cross-entropy), the x-axis indicates time (seconds).\nOur SubChar-Pinyin-NoIndex model gets a lower loss\nthan the CharTokenizer baseline throughout training.\nthe SubChar-Pinyin-NoIndex model on the iFLY-\nTEK dataset, we observe that our SubChar-Pinyin-\nNoindex model indeed converges much faster and\nachieves lower training loss in the end.\nThe speedup on pretraining is also signiﬁcant.\nWhile the running speed differs on different ma-\nchines, the compression brought by the shorter to-\nkenized outputs is hardware-invariant. In Table 9,\nwe show the relative size (disk memory) of the\ntokenized pretraining corpus. We observe that Sub-\nChar tokenizers can tokenize the raw pretraining\ntexts into shorter sequences than the baselines, thus\nTokenized Corpus Size\nCharTokenizer 100.0%\nSub-word 91.4%\nSubChar-Wubi 77.2%\nSubChar-Pinyin 78.4%\nSubChar-Pinyin-NoIndex 74.7%\nTable 9: Relative size (disk memory) of the tokenized\npretraining corpus with different tokenizers. SubChar\ntokenizers produce much smaller tokenized corpus due\nto their ability to tokenize inputs into shorter sequences.\niFLYTEK TNEWS\nVocab Size = 22675\nSub-word 255.2 20.1\nSubChar-Pinyin-NoIndex 175.4 15.2\nVocab Size = 40000\nSub-word 188.9 15.9\nSubChar-Pinyin-NoIndex 166.1 14.4\nVocab Size = 60000\nSub-word 176.2 14.9\nSubChar-Pinyin-NoIndex 164.0 14.1\nTable 10: Comparison of average length of tokenized\nsequences with different tokenizers and different vocab-\nulary sizes.\nresulting in a much smaller pretraining data (e.g.,\nas much as 25.3% smaller than that of the Char-\nTokenizer baseline with SubChar-Pinyin-NoIndex).\nIn turn, this can translate to much faster pretraining\non any training infrastructure.\n5.3 Impact of Vocabulary Size\nIntuitively, when we increase the vocabulary size,\nthere will also be more room to store combina-\ntion tokens (e.g., words and phrases), leading to a\ndecrease in tokenization length and thus better efﬁ-\nciency. Although we used the standard vocabulary\nsize of 22675 in our previous experiments, to un-\nderstand whether the efﬁciency beneﬁts of SubChar\ntokenization wear off at larger vocabulary size, we\nperform an additional ablation on the impact of\nvocabulary size.\nAs shown in Table 10, as we increase the vo-\ncabulary size, the efﬁciency advantage of SubChar\ntokenizers slightly diminishes. However, even at a\nvery large vocab size of 60000, our SubChar-Pinyin\ntokenizer still tokenizes the inputs into signiﬁcantly\nshorter sequences than the Sub-word baseline. We\nthus conclude that the efﬁciency advantage of our\nSubChar tokenizers would hold in most practical\ncases where the vocabulary size is typically under\n60000 (such as BERT and RoBERTa).\n5.4 Impact of Pretraining Data Size\nTo understand the impact of pretraining data size,\nwe take the checkpoints of the 12-layer Trans-\nformer models pretrained on the 2.3G Baike corpus,\nand further pretrain them on a much larger corpus\nof 22.1GB text. This 22.1GB corpus is sampled\nfrom Chinese web text9, mainly consisting of books\nand web pages. We further pretrain for 8K steps\nwith a maximum sequence length of 512.\nAs shown in the bottom block of Table 2, fur-\nther training on this larger corpus leads to small\nimprovement on average performance (72.23 →\n72.81 for CharTokenizer and 72.87 → 73.42 for\nSubChar-Pinyin), possibly because the original\nmodels trained on 2.3GB corpus are already close\nto being fully trained. More importantly, this result\nshows that even with pretraining on larger corpora,\nour proposed methods can still match or slightly\noutperform baselines on the downstream datasets.\n5.5 Impact of Encoding Methods\nAs described in Section 2, we experiment with dif-\nferent types of encoding methods and compare their\ndownstream performance to analyze the impact.\nOur previous encoding methods are based on the\nhypothesis that linguistic information such as glyph\nor pronunciation provides useful inductive biases\nto the model. However, in the case where this hy-\npothesis is not true, it is possible that non-linguistic\nencoding methods may work as well. To verify this,\nwe add two encoding methods that do not consider\nany linguistic information: Byte Encoding and Ran-\ndom Index Encoding, for the purpose of ablation\nanalysis.\nIn Byte Encoding, we convert every character\ninto its byte sequence, same as in ByT5 (Xue et al.,\n2022). In cases where the byte sequence consists of\nmultiple indices (each Chinese character has three\nbyte indices), we concatenate them and append the\ncharacter separation symbol as the encoding (e.g.,\n魑→233_173_145#).\nIn Random Index Encoding, we map each charac-\nter into a unique and randomly generated ﬁve-digit\nindex and append the character separation symbol\nas the encoding ( e.g., 魑→29146# ).\nWe train SubChar tokenizers with all the dif-\nferent encoding methods and compare the corre-\nsponding BERT models using these tokenizers on\n9https://github.com/OpenBMB/CPM-Live\nTNEWS IFLY BQ WSC AFQMC CSL OCNLI A VG\nSubChar-Pinyin 63.68 58.81 81.74 65.90 68.89 82.87 67.98 70.16\nSubChar-Zhuyin 64.91 59.39 81.41 62.72 69.14 82.60 69.12 69.90\nSubChar-Stroke 64.26 55.44 81.52 62.06 69.88 83.16 68.98 69.33\nSubChar-Wubi 63.81 58.74 81.55 64.61 69.66 82.44 68.02 69.90\nSubChar-Zhengma 63.86 59.51 81.59 63.27 70.47 82.91 69.03 70.09\nSubChar-Cangjie 64.10 57.77 81.98 62.39 68.95 82.60 68.46 69.46\nSubChar-Byte 63.58 59.55 81.65 63.60 68.60 82.66 67.93 69.65\nSubChar-RandomIndex 64.11 59.16 81.64 63.93 68.53 82.86 69.39 69.95\nSubChar-Pinyin (BPE) 63.86 58.84 82.12 65.57 69.86 82.86 68.57 70.24\nTable 11: Results of SubChar tokenizers when using different encoding methods. The last row is a model with\nSubChar-Pinyin tokenizer using BPE as the subword tokenization algorithm, all previous rows are using unigram\nLM as the subword tokenization implementation. All models have 6-layers with the same hyper-parameters. The\nimpact of different encoding methods on downstream performance is small, and the ULM and BPE versions of\nSubChar-Pinyin also achieve similar results.\ndownstream tasks. The results are presented in\nTable 11. We observe that the differences be-\ntween these different tokenizers are rather small\nin terms of the model performance on downstream\ndatasets. Moreover, perhaps somewhat surprisingly,\ntokenizers with the non-linguistic encoding meth-\nods – SubChar-Byte and SubChar-RandomIndex\n– can also perform competitively despite the fact\nthat they do not capture glyph or pronunciation\ninformation like the other tokenizers.\nThese results suggest that linguistic encoding\nmay not be necessary for SubChar tokenizers to\nachieve high performance on downstream tasks.\nHowever, the linguistic encoding methods can build\nmore robust and efﬁcient tokenizers as illustrated\nin previous sections.\n5.6 Impact of Vocabulary Construction\nAlgorithm\nIn previous experiments, we used the Unigram LM\nimplementation in SentencePiece for vocabulary\nconstruction. We perform an additional ablation\nwhere we replace Unigram LM with Byte Pair En-\ncoding (BPE) for vocabulary construction to train\na pinyin-based tokenizer, while holding all other\nhyper-parameters constant.\nWe compare the SubChar-Pinyin-BPE variant\nwith the unigram LM (SubChar-Pinyin) tokenizer.\nWe ﬁnd that these two perform similarly. In\nterms of efﬁciency: SubChar-Pinyin-BPE tok-\nenizes iFLYTEK to an average length of 184.4\nand tokenizes TNEWS to an average length of 15.9.\nIn comparison, SubChar-Pinyin tokenizes iFLY-\nTEK to an average length of 185.2 and tokenizes\nTNEWS to an average length of 16.1. The vo-\ncabulary compositions of the two are also similar,\nwhere character combination takes up the majority\nof the space in the vocabulary for both BPE and\nunigram LM implementations. In terms of per-\nformance, we observe in Table 11 that the BPE\nimplementation and the unigram LM implementa-\ntion have little difference in downstream task per-\nformance. Based on these results, we conclude\nthat the choice of which vocabulary construction\nto use has a marginal impact on the tokenization\nefﬁciency and model performance.\n6 Related Work\nChinese PLMs. Chinese BERT (Devlin et al.,\n2019) is the ﬁrst Chinese PLM, which adopts the\ncharacter tokenization. Then, researchers have\nexplored techniques to explicitly incorporate the\nword-level information into Chinese PLMs for bet-\nter performance. Zhu (2020) and Zhang et al.\n(2021a) expand BERT vocabulary with Chinese\nwords apart from Chinese characters and incorpo-\nrate them in the pretraining objectives. Cui et al.\n(2019a), Wei et al. (2019), and Xiao et al. (2021)\nconsider coarse-grained information through mask-\ning whole words and n-grams during the masked\nlanguage modeling pretraining. Diao et al. (2020)\nincorporate word-level information via superimpos-\ning the character and word embeddings. Lai et al.\n(2021) incorporate Chinese word lattice structures\nin pretraining. Different from these studies, we in-\nvestigate the information in the sub-character level\nfor Chinese PLMs.\nLinguistically-Informed Techniques for Chi-\nnese NLP. Before the era of PLM, many efforts\nhave been made to incorporate linguistic knowl-\nedge, including both glyph (Sun et al., 2014; Yu\net al., 2017; Cao et al., 2018) and pronuncia-\ntion (Zhang et al., 2019b; Chaudhary et al., 2018),\ninto word embedding (Mikolov et al., 2013). Be-\nyond word-level representation, researchers ex-\nplore the use of linguistic information to enhance\nsequential models (Dong et al., 2016; Bharad-\nwaj et al., 2016; Liu et al., 2017), especially\nBERT (Meng et al., 2019; Sun et al., 2021). Com-\npared to these works, we do not incorporate ad-\nditional information from sources like images, in-\nstead, our proposed tokenization methods are drop-\nin replacements to existing tokenizers, without\nadding any extra layers or parameters. Besides,\nCWS is a common preprocessing step for Chinese\nNLP (Li and Sun, 2009), Li et al. (2019) empiri-\ncally analyze whether CWS is helpful for Chinese\nNLP tasks before the era of PLMs and ﬁnd that\nthe answer is no in many cases. In our work,\nwe also spend a section examining the impact of\nCWS speciﬁcally for PLMs. Moreover, as shown\nby Huang et al. (2021), incorporating linguistic in-\nformation also beneﬁts spelling check. Instead of\nexplicitly using spelling check, our linguistically-\ninformed tokenizations are robust to spelling errors.\nGranularity of Tokenization. Although sub-\nwords are taken to be the default granularity of\ntokenization since the release of BERT, researchers\nalso explore different granularities for PLMs. For\ninstance, ELMo (Peters et al., 2018), the early\npioneer of PLMs, starts by using character rep-\nresentation. Ma et al. (2020) combine character\nrepresentations with sub-word representations for\nbetter performance and robustness. Nzeyimana\nand Rubungo (2022) incorporate a morphologi-\ncal analyzer for tokenization and achieve gains\nfor the Kinyarwanda language model. More re-\ncently, there is a trend in tokenization-free meth-\nods, including Byte-BPE (Wei et al., 2021), CA-\nNINE (Clark et al., 2021), ByT5 (Xue et al., 2022),\nand Charformer (Tay et al., 2022), which discard\nexplicit tokenization and directly represent inputs\nas small units such as bytes. The downside of\nthese tokenization-free approaches is obvious: the\nlonger tokenized sequence lengths slow down both\ntraining and inference. Contrary to them, our sub-\ncharacter tokenization encourages the use of more\ncharacter combinations, which largely shortens the\ntokenized sequences.\n7 Conclusion\nIn this work, we propose sub-character tokeniza-\ntion and conduct comprehensive experiments to\nillustrate its advantage over existing tokenization\nmethods. Compared to treating each individual\ncharacter as a token (CharTokenizer) or directly\nrunning sub-word tokenization on the raw Chinese\ntext (sub-word tokenizer), our SubChar tokeniz-\ners not only perform competitively on downstream\nNLU tasks, more importantly, they can be much\nmore efﬁcient and robust. We conduct a series of\nablation and analysis to understand the reasons why\nSubChar tokenizers are more efﬁcient, as well as\nthe impact of linguistic and non-linguistic encoding.\nGiven the advantages of our SubChar tokenizers,\nwe believe that they are better alternatives to all\nexisting Chinese tokenizers, especially in applica-\ntions where efﬁciency and robustness are critical.\nIt is possible that our approach can be useful for\nother morphologically poor languages and more\ncomplicated methods could be developed based on\nSubChar tokenization for even better performance.\nWe leave these interesting directions for future ex-\nploration. On a broader level, our work makes\nan important attempt in developing more tailored\nmethods for a language drastically different from\nEnglish with promising results. We believe that\nthis is a crucial future direction for the community\ngiven the language diversity in the world. We hope\nthat our work can inspire more such work in order\nto beneﬁt language technology users from different\ncountries and cultures.\nLimitations\nOur experiments are focused on natural language\nunderstanding tasks. We recognize that adapting\nour SubChar tokenization to language generation\ntasks might require additional efforts, for exam-\nple, we may want to avoid cases of predicting sub-\ncharacter tokens that do not form complete charac-\nters. Also, evaluating the robustness of language\ngeneration models on real-world input noises may\nrequire additional benchmarks beyond those used\nin this paper. We leave such exploration as an in-\nteresting direction for future work.\nAnother limitation is that our method is designed\nspeciﬁcally for the Chinese language. While we\nhypothesize that our method can also bring beneﬁts\nto other languages with ideographic symbols, such\nas Kanji in Japanese, we leave such investigation\nto future work.\nBroader Impact\nWe expect our work to have a positive impact on\nthe society. Firstly, we addressed the practical prob-\nlem of handling input with real-world noises. Such\nnoisy settings are very common in real-life appli-\ncations. Our method, along with the evaluation\nframework, can help make language technologies\nmore robust and reliable in real-world applications,\nespecially for Chinese users. Secondly, we ad-\ndressed the efﬁciency concern of large language\nmodels by signiﬁcantly reducing both and training\nand inference time. This not only reduces the la-\ntency of these models in real-world applications,\nmore importantly, helps reduce the environmental\ncosts of using these large language models, moving\nfurther towards Green AI. All of our code and mod-\nels are released with proper documentation in order\nto better facilitate the adoption of our work in a\nwide range of research and industrial applications.\nAcknowledgement\nThis work is supported by the National Key Re-\nsearch and Development Program of China (No.\n2020AAA0106500) and the National Natural Sci-\nence Foundation of China (NSFC No. 62236004).\nWe thank Xu Han, Yusheng Su, Tianyu Gao\nand other members of THUNLP for their helpful\ndiscussion in the early stages of this work. We\nthank Jordan Boyd-Graber, Chen Zhao, Shi Feng,\nNeha Srikanth, Tonia Bleam, Leslie Li, and other\nmembers of UMD CLIP and Language Science\nCenter for their helpful discussion and feedback.\nWe also thank Nelson Liu and Canwen Xu for their\nconstructive feedback on our early drafts. We es-\npecially appreciate the constructive reviews from\nTACL reviewers and action editors.\nThis work is supported by the National Key Re-\nsearch and Development Program of China (No.\n2020AAA0106500) and the National Natural Sci-\nence Foundation of China (NSFC No. 62236004).\nWe thank Xu Han, Yusheng Su, Tianyu Gao\nand other members of THUNLP for their helpful\ndiscussion in the early stages of this work. We\nthank Jordan Boyd-Graber, Chen Zhao, Shi Feng,\nNeha Srikanth, Tonia Bleam, Leslie Li, and other\nmembers of UMD CLIP and Language Science\nCenter for their helpful discussion and feedback.\nWe also thank Nelson Liu and Canwen Xu for their\nconstructive feedback on our early drafts. We es-\npecially appreciate the constructive reviews from\nTACL reviewers and action editors.\nAuthor contributions Chenglei Si, Zhengyan\nZhang, and Yingfa Chen wrote the code and con-\nducted the experiments. Chenglei was in charge\nof tokenzer training and pretraining experiments,\nZhengyan did the CWS experiments, Yingfa did\nthe ﬁnetuning experiments. All three of them con-\ntributed to the analysis experiments. Chenglei Si,\nZhengyan Zhang, and Yingfa Chen wrote the ini-\ntial draft; Fanchao Qi, Xiaozhi Wang, and Zhiyuan\nLiu signiﬁcantly edited and improved the paper.\nYasheng Wang, Qun Liu, and Maosong Sun pro-\nvided valuable advice to the research. Chenglei\nstarted this work back when he was visiting the\nTHUNLP group in 2021.\nReferences\nEmily Bender. 2019. The #BenderRule: On Nam-\ning the Languages We Study and Why It Matters.\nThe Gradient.\nAkash Bharadwaj, David Mortensen, Chris Dyer,\nand Jaime Carbonell. 2016. Phonologically\naware neural model for named entity recognition\nin low resource transfer settings. In Proceedings\nof EMNLP, pages 1462–1472.\nShaosheng Cao, Wei Lu, Jun Zhou, and Xiaolong\nLi. 2018. cw2vec: Learning Chinese Word Em-\nbeddings with Stroke n-gram Information. In\nProceedings of AAAI.\nPi-Chuan Chang, Michel Galley, and Christo-\npher D. Manning. 2008. Optimizing Chinese\nWord Segmentation for Machine Translation Per-\nformance. In Proceedings of the Third Workshop\non Statistical Machine Translation.\nAditi Chaudhary, Chunting Zhou, Lori Levin, Gra-\nham Neubig, David R. Mortensen, and Jaime\nCarbonell. 2018. Adapting word embeddings to\nnew languages with morphological and phono-\nlogical subword representations. In Proceedings\nof EMNLP, pages 3285–3295.\nJing Chen, Qingcai Chen, Xin Liu, Haijun Yang,\nDaohe Lu, and Buzhou Tang. 2018. The BQ\nCorpus: A Large-scale Domain-speciﬁc Chi-\nnese Corpus For Sentence Semantic Equivalence\nIdentiﬁcation. In Proceedings of EMNLP.\nJonathan Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2021. CANINE: Pre-training an Efﬁ-\ncient Tokenization-Free Encoder for Language\nRepresentation. Transactions of the Association\nfor Computational Linguistics, 10:73–91.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather\nThan Generators. In Proceedings of ICLR.\nFlorian Coulmas. 1991. The Writing Systems of\nthe World. pages 108–109. Blackwell Publish-\ners.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nShijin Wang, and Guoping Hu. 2020. Revis-\niting Pre-Trained Models for Chinese Natural\nLanguage Processing. In Findings of EMNLP.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu.\n2019a. Pre-Training with Whole Word Masking\nfor Chinese BERT. IEEE/ACM TASLP, 29:3504–\n3514.\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao,\nZhipeng Chen, Wentao Ma, Shijin Wang, and\nGuoping Hu. 2019b. A Span-Extraction Dataset\nfor Chinese Machine Reading Comprehension.\nIn Proceedings of EMNLP-IJCNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language\nUnderstanding. In Proceedings of NAACL-HLT.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang,\nand Yonggang Wang. 2020. ZEN: Pre-training\nChinese Text Encoder Enhanced by N-gram Rep-\nresentations. In Findings of EMNLP.\nChuanhai Dong, Jiajun Zhang, Chengqing Zong,\nMasanori Hattori, and Hui Di. 2016. Character-\nbased lstm-crf with radical-level features for chi-\nnese named entity recognition. In Natural Lan-\nguage Understanding and Intelligent Applica-\ntions, pages 239–250.\nYen-Chen Hao and Chung-Lin Martin Yang. 2021.\nThe effect of second-language orthographic in-\nput on the phonological encoding of Mandarin\nwords. Applied Psycholinguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: Decoding-\nenhanced BERT with Disentangled Attention.\nIn Proceedings of ICLR.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nKübler, and Lawrence Moss. 2020. OCNLI:\nOriginal Chinese Natural Language Inference.\nIn Findings of EMNLP.\nLi Huang, Junjie Li, Weiwei Jiang, Zhiyu Zhang,\nMinchuan Chen, Shaojun Wang, and Jing Xiao.\n2021. PHMOSpell: Phonological and mor-\nphological knowledge guided Chinese spelling\ncheck. In Proceedings of ACL , pages 5958–\n5967.\nMario Michael Krell, Matej Kosec, Sergio P. Perez,\nand Andrew Fitzgibbon. 2021. Efﬁcient Se-\nquence Packing without Cross-contamination:\nAccelerating Large Language Models with-\nout Impacting Performance. arXiv preprint ,\nabs/2107.02027.\nTaku Kudo. 2018. Subword Regularization: Im-\nproving Neural Network Translation Models\nwith Multiple Subword Candidates. In Proceed-\nings of ACL.\nTaku Kudo and John Richardson. 2018. Senten-\ncePiece: A simple and language independent\nsubword tokenizer and detokenizer for Neural\nText Processing. In Proceedings of EMNLP Sys-\ntem Demonstrations.\nYuxuan Lai, Yijia Liu, Yansong Feng, Songfang\nHuang, and Dongyan Zhao. 2021. Lattice-\nBERT: Leveraging Multi-Granularity Represen-\ntations in Chinese Pre-trained Language Models.\nIn Proceedings of NAACL-HLT.\nZhenzhong Lan, Mingda Chen, Sebastian Good-\nman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representa-\ntions. In Proceedings of ICLR.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd Schema Challenge.\nIn Thirteenth international conference on the\nprinciples of knowledge representation and rea-\nsoning.\nJingyang Li and Maosong Sun. 2007. Scalable\nTerm Selection for Text Categorization. In Pro-\nceedings of EMNLP.\nXiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong\nHan, Arianna Yuan, and Jiwei Li. 2019. Is Word\nSegmentation Necessary for Deep Learning of\nChinese Representations? In Proceedings of\nACL.\nZhongguo Li and Maosong Sun. 2009. Punctua-\ntion as Implicit Annotations for Chinese Word\nSegmentation. Computational Linguistics.\nFrederick Liu, Han Lu, Chieh Lo, and Graham\nNeubig. 2017. Learning character-level compo-\nsitionality with visual features. In Proceedings\nof ACL, pages 2059–2068.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoy-\nanov. 2019. RoBERTa: A Robustly Optimized\nBERT Pretraining Approach. arXiv preprint,\nabs/1907.11692.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu,\nShijin Wang, and Guoping Hu. 2020. Char-\nBERT: Character-aware Pre-trained Language\nModel. In Proceedings of COLING.\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen.\n2019. Right for the wrong reasons: Diagnosing\nsyntactic heuristics in natural language inference.\nIn Proceedings of ACL.\nYuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping\nNie, Fan Yin, Muyu Li, Qinghong Han, Xiaofei\nSun, and Jiwei Li. 2019. Glyce: Glyph-vectors\nfor Chinese Character Representations. In Pro-\nceedings of NeurIPS.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. 2013. Distributed repre-\nsentations of words and phrases and their com-\npositionality. In Proceedings of NeurIPS, vol-\nume 26.\nAntoine Nzeyimana and Andre Niyongabo\nRubungo. 2022. Kinyabert: a morphology-\naware kinyarwanda language model. In Pro-\nceedings of ACL.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. 2018. Deep Contextual-\nized Word Representations. In Proceedings of\nNAACL-HLT.\nMike Schuster and Kaisuke Nakajima. 2012.\nJapanese and Korean voice search. In Proceed-\nings of the IEEE International Conference on\nAcoustics, Speech and Signal Processing.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016. Neural Machine Translation of\nRare Words with Subword Units. In Proceed-\nings of ACL.\nChenglei Si, Zhengyan Zhang, Yingfa Chen, Xi-\naozhi Wang, Zhiyuan Liu, and Maosong Sun.\n2023. READIN: A Chinese Multi-Task Bench-\nmark with Realistic and Diverse Input Noises.\narXiv.\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie.\n2020. Investigating Prior Knowledge for Chal-\nlenging Chinese Machine Reading Comprehen-\nsion. Transactions of the Association for Com-\nputational Linguistics.\nMaosong Sun, Xinxiong Chen, Kaixu Zhang,\nZhipeng Guo, and Zhiyuan Liu. 2016. THU-\nLAC: An Efﬁcient Lexical Analyzer for Chinese.\nGitHub.\nYaming Sun, Lei Lin, Nan Yang, Zhenzhou Ji, and\nXiaolong Wang. 2014. Radical-enhanced chi-\nnese character embedding. In Proceedings of\nCOLING, pages 279–286.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng,\nXuyi Chen, Han Zhang, Xin Tian, Danxiang\nZhu, Hao Tian, and Hua Wu. 2019. ERNIE:\nEnhanced Representation through Knowledge\nIntegration. arXiv preprint, abs/1904.09223.\nZijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng,\nXiang Ao, Qing He, Fei Wu, and Jiwei Li. 2021.\nChineseBERT: Chinese pretraining enhanced by\nglyph and Pinyin information. In Proceedings\nof ACL, pages 2065–2075.\nYi Tay, Vinh Tran, Sebastian Ruder, Jai Gupta,\nHyung Won Chung, Dara Bahri, Zhen Qin, Si-\nmon Baumgartner, Cong Yu, and Donald Met-\nzler. 2022. Charformer: Fast Character Trans-\nformers via Gradient-based Subword Tokeniza-\ntion. In Proceedings of ICLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is All you Need. In Proceedings of\nNeurIPS.\nJunqiu Wei, Qun Liu, Yinpeng Guo, and Xin Jiang.\n2021. Training Multilingual Pre-trained Lan-\nguage Model with Byte-leve. arXiv preprint,\nabs/2101.09469.\nJunqiu Wei, Xiaozhe Ren, Xiaoguang Li, Weny-\nong Huang, Yi Liao, Yasheng Wang, Jiashu\nLin, Xin Jiang, Xiao Chen, and Qun Liu. 2019.\nNEZHA: Neural Contextualized Representation\nfor Chinese Language Understanding. arXiv,\nabs/1904.00204.\nDongling Xiao, Yu-Kun Li, Han Zhang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2021.\nERNIE-Gram: Pre-Training with Explicitly N-\nGram Masked Language Modeling for Natural\nLanguage Understanding. In Proceedings of\nNAACL-HLT.\nLiang Xu, Qianqian Dong, Cong Yu, Yin Tian,\nWeitang Liu, Lu Li, and Xuanwei Zhang.\n2020a. CLUENER2020: Fine-grained Name\nEntity Recognition for Chinese. arXiv preprint,\nabs/2001.04351.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chen-\njie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian\nYu, Cong Yu, Yin Tian, Qianqian Dong, Wei-\ntang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun\nZeng, Rongzhao Wang, Weijian Xie, Yanting\nLi, Yina Patterson, Zuoyu Tian, Yiwen Zhang,\nHe Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng\nZhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan.\n2020b. CLUE: A Chinese Language Under-\nstanding Evaluation Benchmark. In Proceedings\nof COLING.\nLinting Xue, Aditya Barua, Noah Constant, Rami\nAl-Rfou, Sharan Narang, Mihir Kale, Adam\nRoberts, and Colin Raffel. 2022. ByT5: To-\nwards a token-free future with pre-trained byte-\nto-byte models. Transactions of the Association\nfor Computational Linguistics.\nJinxing Yu, Xun Jian, Hao Xin, and Yangqiu Song.\n2017. Joint embeddings of chinese words, char-\nacters, and ﬁne-grained subcharacter compo-\nnents. In Proceedings of EMNLP, pages 286–\n291.\nXinsong Zhang, Pengshuai Li, and Hang Li. 2021a.\nAMBERT: A Pre-trained Language Model with\nMulti-Grained Tokenization. In Findings of\nACL.\nYuan Zhang, Jason Baldridge, and Luheng He.\n2019a. Paws: Paraphrase adversaries from word\nscrambling. In Proceedings of NAACL-HLT.\nYun Zhang, Yongguo Liu, Jiajing Zhu, Ziqiang\nZheng, Xiaofeng Liu, Weiguang Wang, Zijie\nChen, and Shuangqing Zhai. 2019b. Learning\nchinese word embeddings from stroke, struc-\nture and pinyin of characters. In Proceedings of\nCIKM, pages 1011–1020.\nZhengyan Zhang, Yuxian Gu, Xu Han, Shengqi\nChen, Chaojun Xiao, Zhenbo Sun, Yuan Yao,\nFanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai,\nGuoyang Zeng, Zhixing Tan, Zhiyuan Liu, Min-\nlie Huang, Wentao Han, Yang Liu, Xiaoyan\nZhu, and Maosong Sun. 2021b. CPM-2: Large-\nscale Cost-effective Pre-trained Language Mod-\nels. arXiv preprint, abs/2106.10715.\nZhengyan Zhang, Xu Han, Hao Zhou, Pei Ke,\nYuxian Gu, Deming Ye, Yujia Qin, Yusheng\nSu, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi\nWang, Yanan Zheng, Guoyang Zeng, Huanqi\nCao, Shengqi Chen, Daixuan Li, Zhenbo Sun,\nZhiyuan Liu, Minlie Huang, Wentao Han, Jie\nTang, Juanzi Li, Xiaoyan Zhu, and Maosong\nSun. 2020. CPM: A Large-scale Generative Chi-\nnese Pre-trained Language Model. AI Open.\nChujie Zheng, Minlie Huang, and Aixin Sun. 2019.\nChID: A Large-scale Chinese IDiom Dataset for\nCloze Test. In Proceedings of ACL.\nWei Zhu. 2020. MVP-BERT: Redesigning V ocabu-\nlaries for Chinese BERT and Multi-V ocab Pre-\ntraining. arXiv preprint, abs/2011.08539.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8480223417282104
    },
    {
      "name": "Glyph (data visualization)",
      "score": 0.7465152740478516
    },
    {
      "name": "Lexical analysis",
      "score": 0.7154892086982727
    },
    {
      "name": "Natural language processing",
      "score": 0.7149903774261475
    },
    {
      "name": "Pronunciation",
      "score": 0.6848482489585876
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6079455018043518
    },
    {
      "name": "Text segmentation",
      "score": 0.5553942322731018
    },
    {
      "name": "Character (mathematics)",
      "score": 0.5138493180274963
    },
    {
      "name": "Word (group theory)",
      "score": 0.4854141175746918
    },
    {
      "name": "Preprocessor",
      "score": 0.48215219378471375
    },
    {
      "name": "Segmentation",
      "score": 0.4664188325405121
    },
    {
      "name": "Security token",
      "score": 0.4403541088104248
    },
    {
      "name": "Linguistics",
      "score": 0.39440616965293884
    },
    {
      "name": "Speech recognition",
      "score": 0.3292236328125
    },
    {
      "name": "Visualization",
      "score": 0.09485864639282227
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}