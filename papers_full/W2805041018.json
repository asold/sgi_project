{
  "title": "Unsupervised Text Style Transfer using Language Models as Discriminators",
  "url": "https://openalex.org/W2805041018",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2361346764",
      "name": "Yang ZiChao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221940818",
      "name": "Hu, Zhiting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221974744",
      "name": "Dyer Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222901025",
      "name": "Xing, Eric P.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221943472",
      "name": "Berg-Kirkpatrick, Taylor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2951520714",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2951912364",
    "https://openalex.org/W2521028896",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W2735642330",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2210838531",
    "https://openalex.org/W2340944142",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2797227342",
    "https://openalex.org/W2951523806",
    "https://openalex.org/W2605287558",
    "https://openalex.org/W2765961751",
    "https://openalex.org/W2432004435",
    "https://openalex.org/W2593383075",
    "https://openalex.org/W2594538354",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2542835211",
    "https://openalex.org/W2581485081",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2784107021",
    "https://openalex.org/W2757281913",
    "https://openalex.org/W2949541494",
    "https://openalex.org/W2601324753",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2950359962",
    "https://openalex.org/W2766182427",
    "https://openalex.org/W2951824008",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2768440179",
    "https://openalex.org/W2434741482",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2439973210",
    "https://openalex.org/W2949549712",
    "https://openalex.org/W2103042430",
    "https://openalex.org/W2173520492",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2798651744",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Binary classifiers are often employed as discriminators in GAN-based unsupervised style transfer systems to ensure that transferred sentences are similar to sentences in the target domain. One difficulty with this approach is that the error signal provided by the discriminator can be unstable and is sometimes insufficient to train the generator to produce fluent language. In this paper, we propose a new technique that uses a target domain language model as the discriminator, providing richer and more stable token-level feedback during the learning process. We train the generator to minimize the negative log likelihood (NLL) of generated sentences, evaluated by the language model. By using a continuous approximation of discrete sampling under the generator, our model can be trained using back-propagation in an end- to-end fashion. Moreover, our empirical results show that when using a language model as a structured discriminator, it is possible to forgo adversarial steps during training, making the process more stable. We compare our model with previous work using convolutional neural networks (CNNs) as discriminators and show that our approach leads to improved performance on three tasks: word substitution decipherment, sentiment modification, and related language translation.",
  "full_text": "Unsupervised Text Style Transfer using Language\nModels as Discriminators\nZichao Yang1, Zhiting Hu1, Chris Dyer2, Eric P. Xing1, Taylor Berg-Kirkpatrick1\n1Carnegie Mellon University, 2DeepMind\n{zichaoy, zhitingh, epxing, tberg}@cs.cmu.edu\ncdyer@google.com\nAbstract\nBinary classiﬁers are often employed as discriminators in GAN-based unsupervised\nstyle transfer systems to ensure that transferred sentences are similar to sentences\nin the target domain. One difﬁculty with this approach is that the error signal\nprovided by the discriminator can be unstable and is sometimes insufﬁcient to train\nthe generator to produce ﬂuent language. In this paper, we propose a new technique\nthat uses a target domain language model as the discriminator, providing richer\nand more stable token-level feedback during the learning process. We train the\ngenerator to minimize the negative log likelihood (NLL) of generated sentences,\nevaluated by the language model. By using a continuous approximation of discrete\nsampling under the generator, our model can be trained using back-propagation\nin an end-to-end fashion. Moreover, our empirical results show that when using\na language model as a structured discriminator, it is possible to forgo adversarial\nsteps during training, making the process more stable. We compare our model\nwith previous work that uses convolutional networks (CNNs) as discriminators, as\nwell as a broad set of other approaches. Results show that the proposed method\nachieves improved performance on three tasks: word substitution decipherment,\nsentiment modiﬁcation, and related language translation.\n1 Introduction\nRecently there has been growing interest in designing natural language generation (NLG) systems\nthat allow for control over various attributes of generated text – for example, sentiment and other\nstylistic properties. Such controllable NLG models have wide applications in dialogues systems (Wen\net al., 2016) and other natural language interfaces. Recent successes for neural text generation\nmodels in machine translation (Bahdanau et al., 2014), image captioning (Vinyals et al., 2015) and\ndialogue (Vinyals and Le, 2015; Wen et al., 2016) have relied on massive parallel data. However,\nfor many other domains, only non-parallel data – which includes collections of sentences from each\ndomain without explicit correspondence – is available. Many text style transfer problems fall into this\ncategory. The goal for these tasks is to transfer a sentence with one attribute to a sentence with an\nanother attribute, but with the same style-independent content, trained using only non-parallel data.\nUnsupervised text style transfer requires learning disentangled representations of attributes (e.g., nega-\ntive/positive sentiment, plaintext/ciphertext orthography) and underlying content. This is challenging\nbecause the two interact in subtle ways in natural language and it can even be hard to disentangle them\nwith parallel data. The recent development of deep generative models like variational auto-encoders\n(V AEs) (Kingma and Welling, 2013) and generative adversarial networks(GANs) (Goodfellow et al.,\n2014) have made learning disentangled representations from non-parallel data possible. However,\ndespite their rapid progress in computer vision—for example, generating photo-realistic images (Rad-\nford et al., 2015), learning interpretable representations (Chen et al., 2016b), and translating im-\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.\narXiv:1805.11749v3  [cs.CL]  29 Jan 2019\nages (Zhu et al., 2017)—their progress on text has been more limited. For V AEs, the problem of\ntraining collapse can severely limit effectiveness (Bowman et al., 2015; Yang et al., 2017b), and when\napplying adversarial training to natural language, the non-differentiability of discrete word tokens\nmakes generator optimization difﬁcult. Hence, most attempts use REINFORCE (Sutton et al., 2000)\nto ﬁnetune trained models (Yu et al., 2017; Li et al., 2017) or uses professor forcing (Lamb et al.,\n2016) to match hidden states of decoders.\nPrevious work on unsupervised text style transfer (Hu et al., 2017a; Shen et al., 2017) adopts an\nencoder-decoder architecture with style discriminators to learn disentangled representations. The\nencoder takes a sentence as an input and outputs a style-independent content representation. The\nstyle-dependent decoder takes the content representation and a style representation and generates\nthe transferred sentence. Hu et al. (2017a) use a style classiﬁer to directly enforce the desired style\nin the generated text. Shen et al. (2017) leverage an adversarial training scheme where a binary\nCNN-based discriminator is used to evaluate whether a transferred sentence is real or fake, ensuring\nthat transferred sentences match real sentences in terms of target style. However, in practice, the\nerror signal from a binary classiﬁer is sometimes insufﬁcient to train the generator to produce ﬂuent\nlanguage, and optimization can be unstable as a result of the adversarial training step.\nWe propose to use an implicitly trained language model as a new type of discriminator, replacing the\nmore conventional binary classiﬁer. The language model calculates a sentence’s likelihood, which\ndecomposes into a product of token-level conditional probabilities. In our approach, rather than\ntraining a binary classiﬁer to distinguish real and fake sentences, we train the language model to\nassign a high probability to real sentences and train the generator to produce sentences with high\nprobability under the language model. Because the language model scores sentences directly using a\nproduct of locally normalized probabilities, it may offer more stable and more useful training signal to\nthe generator. Further, by using a continuous approximation of discrete sampling under the generator,\nour model can be trained using back-propagation in an end-to-end fashion.\nWe ﬁnd empirically that when using the language model as a structured discriminator, it is possible to\neliminate adversarial training steps that use negative samples—a critical part of traditional adversarial\ntraining. Language models are implicitly trained to assign a low probability to negative samples\nbecause of its normalization constant. By eliminating the adversarial training step, we found the\ntraining becomes more stable in practice.\nTo demonstrate the effectiveness of our new approach, we conduct experiments on three tasks: word\nsubstitution decipherment, sentiment modiﬁcation, and related language translation. We show that\nour approach, which uses only a language model as the discriminator, outperforms a broad set of\nstate-of-the-art approaches on the three tasks.\n2 Unsupervised Text Style Transfer\nWe start by reviewing the current approaches for unsupervised text style transfer (Hu et al., 2017a;\nShen et al., 2017), and then go on to describe our approach in Section 3. Assume we have two text\ndatasets X = {x(1),x(2),..., x(m)}and Y = {y(1),y(2),..., y(n)}with two different styles vx\nand vy, respectively. For example, vx can be the positive sentiment style and vy can be the negative\nsentiment style. The datasets are non-parallel such that the data does not contain pairs of (x(i),y(j))\nthat describe the same content. The goal of style transfer is to transfer data x with style vx to style\nvy and vice versa, i.e., to estimate the conditional distribution p(y|x) and p(x|y). Since text data\nis discrete, it is hard to learn the transfer function directly via back-propagation as in computer\nvision (Zhu et al., 2017). Instead, we assume the data is generated conditioned on two disentangled\nparts, the style v and the content z1 (Hu et al., 2017a).\nConsider the following generative process for each style: 1) the style representation v is sampled\nfrom a prior p(v); 2) the content vector z is sampled from p(z); 3) the sentence x is generated from\nthe conditional distribution p(x|z,v). This model suggests the following parametric form for style\ntransfer where qrepresents a posterior:\np(y|x) =\n∫\nzx\np(y|zx,vy)q(zx|x,vx)dzx.\n1We drop the subscript in notations wherever the meaning is clear.\n2\nThe above equation suggests the use of an encoder-decoder framework for style transfer problems.\nWe can ﬁrst encode the sentence x to get its content vector zx, then we switch the style label from vx\nto vy. Combining the content vector zx and the style label vy, we can generate a new sentence ˜x\n(the transferred sentences are denotes as ˜x and ˜y).\nOne unsupervised approach is to use the auto-encoder model. We ﬁrst use an encoder model E to\nencode x and y to get the content vectorszx = E(x,vx) and zy = E(y,vy). Then we use a decoder\nG to generate sentences conditioned on z and v. The E and G together form an auto-encoder and\nthe reconstruction loss is:\nLrec(θE,θG) =Ex∼X[−log pG(x|zx,vx)] +Ey∼Y[−log pG(y|zy,vy)],\nwhere vx and vy can be two learnable vectors to represent the label embedding. In order to make\nsure that the zx and zy capture the content and we can deliver accurate transfer between the style\nby switching the labels, we need to guarantee that zx and zy follow the same distribution. We can\nassume p(z) follows a prior distribution and add a KL-divergence regularization on zx, zy. The\nmodel then becomes a V AE. However, previous works (Bowman et al., 2015; Yang et al., 2017b)\nfound that there is a training collapse problem with the V AE for text modeling and the posterior\ndistribution of z fails to capture the content of a sentence.\nTo better capture the desired styles in the generated sentences, Hu et al. (2017a) additionally impose\na style classiﬁer on the generated samples, and the decoder G is trained to generate sentences that\nmaximize the accuracy of the style classiﬁer. Such additional supervision with adiscriminative model\nis also adopted in (Shen et al., 2017), though in that work a binary real/fake classiﬁer is instead used\nwithin a conventional adversarial scheme.\nAdversarial Training Shen et al. (2017) use adversarial training to align the z distributions. Not\nonly do we want to align the distribution of zx and zy, but also we hope that the transferred sentence\n˜x from x to resemble y and vice versa. Several adversarial discriminators are introduced to align\nthese distributions. Each of the discriminators is a binary classiﬁer distinguishing between real and\nfake. Speciﬁcally, the discriminator Dz aims to distinguish between zx and zy:\nLz\nadv(θE,θDz ) =Ex∼X[−log Dz(zx)] +Ey∼Y[−log(1 −Dz(zy))].\nSimilarly, Dx distinguish between x and ˜y, yielding an objective Lx\nadv as above; and Dy distinguish\nbetween y and ˜x, yielding Ly\nadv. Since the samples of ˜x and ˜y are discrete and it is hard to train the\ngenerator in an end-to-end way, professor forcing (Lamb et al., 2016) is used to match the distributions\nof the hidden states of decoders. The overall training objective is a min-max game played among the\nencoder E/decoder G and the discriminators Dz,Dx,Dy (Goodfellow et al., 2014):\nmin\nE,G\nmax\nDz,Dx,Dy\nLrec −λ(Lz\nadv + Lx\nadv + Ly\nadv)\nThe model is trained in an alternating manner. In the ﬁrst step, the loss of the discriminators are\nminimize to distinguish between the zx,x,y and zy,˜x,˜y, respectively; and in the second step the\nencoder and decoder are trained to minimize the reconstruction loss while maximizing loss of the\ndiscriminators.\n3 Language Models as Discriminators\nIn most past work, a classiﬁer is used as the discriminator to distinguish whether a sentence is real or\nfake. We propose instead to use locally-normalized language models as discriminators. We argue\nthat using an explicit language model with token-level locally normalized probabilities offers a more\ndirect training signal to the generator. If a transfered sentence does not match the target style, it will\nhave high perplexity when evaluated by a language model that was trained on target domain data.\nNot only does it provide an overall evaluation score for the whole sentence, but a language model can\nalso assign a probability to each token, thus providing more information on which word is to blame if\nthe overall perplexity is very high.\nThe overall model architecture is shown in Figure 1. Suppose ˜x is the output sentence from applying\nstyle transfer to input sentencex, i.e., ˜x is sampled from pG(˜x|zx,vy) (and similary for ˜y and y). Let\npLM(x) be the probability of a sentence x evaluate against a language model, then the discriminator\n3\nFigure 1: The overall model architecture consists of two parts: reconstruction and transfer. For\ntransfer, we switch the style label and sample an output sentence from the generator that is evaluated\nby a language model.\nloss becomes:\nLx\nLM(θE,θG,θLMx ) =Ex∼X[−log pLMx (x))] +γEy∼Y,˜y∼pG(˜y|zy,vx)[log pLMx (˜y)], (1)\nLy\nLM(θE,θG,θLMy ) =Ey∼Y[−log pLMy (y))] +γEx∼X,˜x∼pG(˜x|zx,vy)[log pLMy (˜x)]. (2)\nOur overall objective becomes:\nmin\nE,G\nmax\nLMx,LMy\nLrec −λ(Lx\nLM + Ly\nLM) (3)\nNegative samples: Note that Equation 1 and 2 differs from traditional ways of training language\nmodels in that we have a term including the negative samples. We train the LM in an adversarial way\nby minimizing the loss of LM of real sentences and maximizing the loss of transferred sentences.\nHowever, since the LM is a structured discriminator, we would hope that a language model trained\non the real sentences will automatically assign high perplexity to sentences not in the target domain,\nhence negative samples from the generator may not be necessary. To investigate the necessity of\nnegative samples, we add a weight γ to the loss of negative samples. The weight γ adjusts the\nnegative sample loss in training the language models. If γ = 0, we simply train the language model\non real sentences and ﬁx its parameters, avoiding potentially unstable adversarial training steps. We\ninvestigate the necessity of using negative samples in the experiment section.\nTraining consists of two steps alternatively. In the ﬁrst step, we train the language models according\nto Equation 1 and 2. In the second step, we minimize the reconstruction loss as well as the per-\nplexity of generated samples evaluated by the language model. Since ˜x is discrete, one can use the\nREINFORCE (Sutton et al., 2000) algorithm to train the generator:\n∇θGLy\nLM = Ex∼X,˜x∼pG(˜x|zx,vy)[log pLM(˜x)∇θG log pG(˜x|zx,vy)]. (4)\nHowever, using a single sample to approximate the expected gradient leads to high variance in\ngradient estimates and thus unstable learning.\nContinuous approximation: Instead, we propose to use a continuous approximation to the sampling\nprocess in training the generator, as demonstrated in Figure 2. Instead of feeding a single sampled\nword as input to the next timestep of the generator, we use a Gumbel-softmax (Jang et al., 2016)\ndistribution as a continuous approximation to sample instead. Let ube a categorical distribution with\nprobabilities π1,π2,...,π c. Samples from ucan be approximated using:\npi = exp((log πi) +gi)/τ∑c\nj=1 exp((log πj + gj)/τ),\nwhere the gi’s are independent samples fromGumbel(0,1).\nLet the tokens of the transferred sentence be ˜x = {˜xt}T\nt=1. Suppose the output of the logit at timestep\ntis vx\nt, then ˜px\nt = Gumbel-softmax(vx\nt,τ), where τ is the temperature. When τ →0, ˜px\nt becomes\nthe one hot representation of token ˜xt. Using the continuous approximation, then the output of the\ndecoder becomes a sequence of probability vectors ˜px = {˜px\nt}T\nt=1.\n4\nFigure 2: Continuous approximation of language model loss. The input is a sequence of probability\ndistributions {˜px\nt}T\nt=1 sampled from the generator. At each timestep, we compute a weighted\nembedding as input to the language model and get the sequence of output distributions from the LM\nas {ˆpx\nt}T\nt=1. The loss is the sum of cross entropies between each pair of ˜px\nt and ˆpx\nt.\nWith the continuous approximation of ˜x, we can calculate the loss evaluated using a language model\neasily, as shown in Figure 2. For every step, we feed ˜px\nt to the language model of y (denoted as\nLMy) using the weighted average of the embedding We˜px\nt, then we get the output from the LMy\nwhich is a probability distribution over the vocabulary of the next word ˆpx\nt+1. The loss of the current\nstep is the cross entropy loss between ˜px\nt+1 and ˆpx\nt+1: (˜px\nt+1)⊺ log ˆpx\nt+1. Note that when the decoder\noutput distribution ˜px\nt+1 aligns with the language model output distribution ˆpx\nt+1, the above loss\nachieves minimum. By summing the loss over all steps and taking the gradient, we can use standard\nback-propagation to train the generator:\n∇θGLy\nLM ≈Ex∼X,˜px∼pG(˜x|zx,vy)[∇θG\nT∑\nt=1\n(˜px\nt)⊺ log ˆpx\nt]. (5)\nThe above Equation is a continuous approximation of Equation 4 with Gumbel softmax distribution.\nIn experiments, we use a single sample of ˜px to approximate the expectation.\nNote that the use of the language model discriminator is a somewhat different in each of the two\ntypes of training update steps because of the continuous approximation. We use discrete samples\nfrom the generators as negative samples in training the language model discriminator step, while we\nuse a continuous approximation in updating the generator step according to Equation 5.\nOvercoming mode collapse: It is known that in adversarial training, the generator can suffer from\nmode collapse (Arjovsky and Bottou, 2017; Hu et al., 2017b) where the samples from the generator\nonly cover part of the data distribution. In preliminary experimentation, we found that the language\nmodel prefers short sentences. To overcome this length bias, we use two tricks in our experiments: 1)\nwe normalize the loss of Equation 5 by length and 2) we ﬁx the length of ˜x to be the same of x. We\nﬁnd these two tricks stabilize the training and avoid generating collapsed overly short outputs.\n4 Experiments\nIn order to verify the effectiveness of our model, we experiment on three tasks: word substitution\ndecipherment, sentiment modiﬁcation, and related language translation. We mainly compare with the\nmost comparable approach of (Shen et al., 2017) that uses CNN classiﬁers as discriminators2. Note\nthat Shen et al. (2017) use three discriminators to align both z and decoder hidden states, while our\nmodel only uses a single language model as a discriminator directly on the output sentences ˜x,˜y.\nMoreover, we also compare with a broader set of related work (Hu et al., 2017a; Fu et al., 2017; Li\net al., 2018) for the tasks when appropriate. Our proposed model provides substantiate improvements\nin most of the cases. We implement our model with the Texar (Hu et al., 2018b) toolbox based on\nTensorﬂow (Abadi et al., 2016).\n2We use the code from https://github.com/shentianxiao/language-style-transfer.\n5\nModel 20% 40% 60% 80% 100%\nCopy 64.3 39.1 14.4 2.5 0\nShen et al. (2017)∗ 86.6 77.1 70.1 61.2 50.8\nOur results:\nLM 89.0 80.0 74.1 62.9 49.3\nLM + adv 89.1 79.6 71.8 63.8 44.2\nTable 1: Decipherment results measured in BLEU. Copy is directly measuring y against x. LM +\nadv denotes we use negative samples to train the language model.∗We run the code open-sourced by\nthe authors to get the results.\nModel Accu BLEU PPL X PPLY\nShen et al. (2017) 79.5 12.4 50.4 52.7\nHu et al. (2017a) 87.7 65.6 115.6 239.8\nOur results:\nLM 83.3 38.6 30.3 42.1\nLM + Classiﬁer 91.2 57.8 47.0 60.9\nTable 2: Results for sentiment modiﬁcation. X = negative,Y = positive. PPLx denotes the\nperplexity of sentences transferred from positive sentences evaluated by a language model trained\nwith negative sentences and vice versa.\n4.1 Word substitution decipherment\nAs the ﬁrst task, we consider the word substitution decipherment task previous explored in the NLP\nliterature (Dou and Knight, 2012). We can control the amount of change to the original sentences\nin word substitution decipherment so as to systematically investigate how well the language model\nperforms in a task that requires various amount of changes. In word substitution cipher, every token\nin the vocabulary is mapped to a cipher token and the tokens in sentences are replaced with cipher\ntokens according to the cipher dictionary. The task of decipherment is to recover the original text\nwithout any knowledge of the dictionary.\nData: Following (Shen et al., 2017), we sample 200K sentences from the Yelp review dataset as plain\ntext X and sample other 200K sentences and apply word substitution cipher on these sentences to get\nY. We use another 100k parallel sentences as the development and test set respectively. Sentences of\nlength more than 15 are ﬁltered out. We keep all words that appear more than 5 times in the training\nset and get a vocabulary size of about 10k. All words appearing less than 5 times are replaced with a\n“<unk>” token. We random sample words from the vocabulary and replace them with cipher tokens.\nThe amount of ciphered words ranges from 20% to 100%. As we have ground truth plain text, we\ncan directly measure the BLEU 3 score to evaluate the model. Our model conﬁgurations are included\nin Appendix B.\nResults: The results are shown in Table 1. We ﬁrst investigate the effect of using negative samples in\ntraining the language model, as denotes by LM + adv in Table 1. We can see that using adversarial\ntraining sometimes improves the results. However, we found empirically that using negative samples\nmakes the training very unstable and the model diverges easily. This is the main reason why we did\nnot get consistently better results by incorporating adversarial training.\nComparing with (Shen et al., 2017), we can see that the language model without adversarial training\nis already very effective and performs much better when the amount of change is less than 100%. This\nis intuitive because when the change is less than 100%, a language model can use context information\nto predict and correct enciphered tokens. It’s surprising that even with 100% token change, our model\nis only 1.5 BLEU score worse than (Shen et al., 2017), when all tokens are replaced and no context\ninformation can be used by the language model. We guess our model can gradually decipher tokens\nfrom the beginning of a sentence and then use them as a bootstrap to decipher the whole sentence.\nWe can also combine language models with the CNNs as discriminators. For example, for the 100%\n3BLEU score is measured with multi-bleu.perl.\n6\ncase, we get BLEU score of 52.1 when combing them. Given unstableness of adversarial training and\neffectiveness of language models, we set γ = 0in Equation 1 and 2 in the rest of the experiments.\n4.2 Sentiment Manipulation\nWe have demonstrated that the language model can successfully crack word substitution cipher.\nHowever, the change of substitution cipher is limited to a one-to-one mapping. As the second task,\nwe would like to investigate whether a language model can distinguish sentences with positive and\nnegative sentiments, thus help to transfer the sentiments of sentences while preserving the content.\nWe compare to the model of (Hu et al., 2017a) as an additional baseline, which uses a pre-trained\nclassiﬁer as guidance.\nData: We use the same data set as in (Shen et al., 2017). The data set contains 250K negative\nsentences (denoted as X) and 380K positive sentences (denoted as Y), of which 70% are used for\ntraining, 10% are used for development and the remaining 20% are used as test set. The pre-processing\nsteps are the same as the previous experiment. We also use similar experiment conﬁgurations.\nEvaluation: Evaluating the quality of transferred sentences is a challenging problem as there are no\nground truth sentences. We follow previous papers in using model-based evaluation. We measure\nwhether transferred sentences have the correct sentiment according to a pre-trained sentiment classiﬁer.\nWe follow both (Hu et al., 2017a) and (Shen et al., 2017) in using a CNN-based classiﬁer. However,\nsimply evaluating the sentiment of sentences is not enough since the model can output collapsed\noutput such as a single word “good” for all negative transfer and “bad” for all positive transfer. We\nnot only would like transferred sentences to preserve the content of original sentences, but also to\nbe smooth in terms of language quality. For these two aspects, we propose to measure the BLEU\nscore of transferred sentences against original sentences and measure the perplexity of transferred\nsentences to evaluate the ﬂuency. A good model should perform well on all three metrics.\nResults: We report the results in Table. 2. As a baseline, the original corpus has perplexity of 35.8\nand 38.8 for the negative and positive sentences respectively. Comparing LM with (Shen et al.,\n2017), we can see that LM outperforms it in all three aspects: getting higher accuracy, preserving\nthe content better while being more ﬂuent. This demonstrates the effectiveness of using LM as the\ndiscriminator. (Hu et al., 2017a) has the highest accuracy and BLEU score among the three models\nwhile the perplexity is very high. It is not surprising that the classiﬁer will only modify the features of\nthe sentences that are related to the sentiment and there is no mechanism to ensure that the modiﬁed\nsentence being ﬂuent. Hence the corresponding perplexity is very high. We can manifest the best of\nboth models by combing the loss of LM and the classiﬁer in (Hu et al., 2017a): a classiﬁer is good at\nmodifying the sentiment and an LM can smooth the modiﬁcation to get a ﬂuent sentence. We ﬁnd\nimprovement of accuracy and perplexity as denoted by LM + classiﬁer compared to classiﬁer only\n(Hu et al., 2017a).\nComparing with other models : Recently there are other models that are proposed speciﬁcally\ntargeting the sentiment modiﬁcation task such as (Li et al., 2018). Their method is feature based and\nconsists of the following steps: (Delete) ﬁrst, they use the statistics of word frequency to delete the\nattribute words such as “good, bad” from original sentences, (Retrieve) then they retrieve the most\nsimilar sentences from the other corpus based on nearest neighbor search, (Generate) the attribute\nwords from retrieved sentences are combined with the content words of original sentences to generate\ntransferred sentences. The authors provide 500 human annotated sentences as the ground truth of\ntransferred sentences so we measure the BLEU score against those sentences. The results are shown\nin Table 3. We can see our model has similar accuracy compared with DeleteAndRetrieve, but has\nmuch better BLEU scores and slightly better perplexity.\nWe list some examples of transferred sentences in Table 5 in the appendix. We can see that (Shen\net al., 2017) does not keep the content of the original sentences well and changes the meaning\nof the original sentences. (Hu et al., 2017a) changes the sentiment but uses improper words, e.g.\n“maintenance is equallyhilarious”. Our LM can change the change the sentiment of sentences. But\nsometimes there is an over-smoothing problem, changing the less frequent words to more frequent\nwords, e.g. changing “my goodness it was so gross” to “myfood it was so good.”. In general LM +\nclassiﬁer has the best results, it changes the sentiment, while keeps the content and the sentences are\nﬂuent.\n7\nModel ACCU BLEU PPL X PPLY\nShen et al. (2017) 76.2 6.8 49.4 45.6\nFu et al. (2017):\nStyleEmbedding 9.2 16.65 97.51 142.6\nMultiDecoder 50.9 11.24 111.1 119.1\nLi et al. (2018):\nDelete 87.2 11.5 75.2 68.7\nTemplate 86.7 18.0 192.5 148.4\nRetrieval 95.1 1.3 31.5 37.0\nDeleteAndRetrieval 90.9 12.6 104.6 43.8\nOur results:\nLM 85.4 13.4 32.8 40.5\nLM + Classiﬁer 90.0 22.3 48.4 61.6\nTable 3: Results for sentiment modiﬁcation based on the 500 human annotated sentences as ground\ntruth from (Li et al., 2018).\n4.3 Related language translation\nIn the ﬁnal experiment, we consider a more challenging task: unsupervised related language trans-\nlation (Pourdamghani and Knight, 2017). Related language translation is easier than normal pair\nlanguage translation since there is a close relationship between the two languages. Note here we\ndon’t compare with other sophisticated unsupervised neural machine translation systems such as\n(Lample et al., 2017; Artetxe et al., 2017), whose models are much more complicated and use other\ntechniques such as back-translation, but simply compare the different type of discriminators in the\ncontext of a simple model.\nData: We choose Bosnian (bs) vs Serbian (sr) and simpliﬁed Chinese (zh-CN) vs traditional Chinese\n(zh-TW) pair as our experiment languages. Due to the lack of parallel data for these data, we build the\ndata ourselves. For bs and sr pair, we use the monolingual data from Leipzig Corpora Collections4.\nWe use the news data and sample about 200k sentences of length less than 20 for each language,\nof which 80% are used for training, 10% are used for validation and remaining 10% are used for\ntest. For validation and test, we obtain the parallel corpus by using the Google Translation API.\nThe vocabulary size is 25k for the sr vs bs language pair. For zh-CN and zh-TW pair, we use the\nmonolingual data from the Chinese Gigaword corpus. We use the news headlines as our training data.\n300k sentences are sampled for each language. The data is partitioned and parallel data is obtained in\na similar way to that of sr vs bs pair. We directly use a character-based model and the total vocabulary\nsize is about 5k. For evaluation, we directly measure the BLEU score using the references for both\nlanguage pairs.\nNote that the relationship between zh-CN and zh-TW is simple and mostly like a decipherment\nproblem in which some simpliﬁed Chinese characters have the corresponding traditional character\nmapping. The relation between bs vs sr is more complicated.\nResults: The results are shown in Table. 4. For sr–bos and bos–sr, since the vocabulary of two\nlanguages does not overlap at all, it is a very challenging task. We report the BLEU1 metric since\nthe BLEU4 is close to 0. We can see that our language model discriminator still outperforms (Shen\net al., 2017) slightly. The case for zh–tw and tw–zh is much easier. Simple copying already has a\nreasonable score of 32.3. Using our model, we can improve it to 81.6 for cn–tw and 85.5 for tw–cn,\noutperforming (Shen et al., 2017) by a large margin.\n5 Related Work\nNon-parallel transfer in natural language: (Hu et al., 2017a; Shen et al., 2017; Prabhumoye et al.,\n2018; Gomez et al., 2018) are most relevant to our work. Hu et al. (2017a) aim to generate sentences\nwith controllable attributes by learning disentangled representations. Shen et al. (2017) introduce\nadversarial training to unsupervised text style transfer. They apply discriminators both on the encoder\n4http://wortschatz.uni-leipzig.de/en\n8\nModel sr–bs bs–sr cn–tw tw–cn\nCopy 0 0 32.3 32.3\nShen et al. (2017) 29.1 30.3 60.1 60.7\nOur results:\nLM 31.0 31.7 81.6 85.5\nTable 4: Related language translation results measured in BLEU. The results for sr vs bs in measured\nin BLEU1 while cn vs tw is measure in BLEU.\nrepresentation and on the hidden states of the decoders to ensure that they have the same distribution.\nThese are the two models that we mainly compare with. Prabhumoye et al. (2018) use the back-\ntranslation technique in their model, which is complementary to our method and can be integrated\ninto our model to further improve performance. Gomez et al. (2018) use GAN-based approach to\ndecipher shift ciphers. (Lample et al., 2017; Artetxe et al., 2017) propose unsupervised machine\ntranslation and use adversarial training to match the encoder representation of the sentences from\ndifferent languages. They also use back-translation to reﬁne their model in an iterative way.\nGANs: GANs have been widely explored recently, especially in computer vision (Zhu et al., 2017;\nChen et al., 2016b; Radford et al., 2015; Sutton et al., 2000; Salimans et al., 2016; Denton et al., 2015;\nIsola et al., 2017). The progress of GANs on text is relatively limited due to the non-differentiable\ndiscrete tokens. Lots of papers (Yu et al., 2017; Che et al., 2017; Li et al., 2017; Yang et al., 2017a)\nuse REINFORCE (Sutton et al., 2000) to ﬁnetune a trained model to improve the quality of samples.\nThere is also prior work that attempts to introduce more structured discriminators, for instance, the\nenergy-based GAN (EBGAN) (Zhao et al., 2016) and RankGAN (Lin et al., 2017). Our language\nmodel can be seen as a special energy function, but it is more complicated than the auto-encoder\nused in (Zhao et al., 2016) since it has a recurrent structure. Hu et al. (2018a) also proposes to\nuse structured discriminators in generative models and establishes its the connection with posterior\nregularization.\nComputer vision style transfer: Our work is also related to unsupervised style transfer in computer\nvision (Gatys et al., 2016; Huang and Belongie, 2017). (Gatys et al., 2016) directly uses the covariance\nmatrix of the CNN features and tries to align the covariance matrix to transfer the style. (Huang\nand Belongie, 2017) proposes adaptive instance normalization for an arbitrary style of images. (Zhu\net al., 2017) uses a cycle-consistency loss to ensure the content of the images is preserved and can be\ntranslated back to original images.\nLanguage model for reranking: Previously, language models are used to incorporate the knowledge\nof monolingual data mainly by reranking the sentences generated from a base model such as (Brants\net al., 2007; Gulcehre et al., 2015; He et al., 2016). (Liu et al., 2017; Chen et al., 2016a) use a\nlanguage model as training supervision for unsupervised OCR. Our model is more advanced in using\nlanguage models as discriminators in distilling the knowledge of monolingual data to a base model in\nan end-to-end way.\n6 Conclusion\nWe showed that by using language models as discriminators and we could outperform traditional\nbinary classiﬁer discriminators in three unsupervised text style transfer tasks including word substitu-\ntion decipherment, sentiment modiﬁcation and related language translation. In comparison with a\nbinary classiﬁer discriminator, a language model can provide a more stable and more informative\ntraining signal for training generators. Moreover, we empirically found that it is possible to eliminate\nadversarial training with negative samples if a structured model is used as the discriminator, thus\npointing one possible direction to solve the training difﬁculty of GANs. In the future, we plan to\nexplore and extend our model to semi-supervised learning.\nReferences\nM. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,\nM. Isard, et al. Tensorﬂow: A system for large-scale machine learning. In OSDI, volume 16, pages\n9\n265–283, 2016.\nM. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks.\narXiv preprint arXiv:1701.04862, 2017.\nM. Artetxe, G. Labaka, E. Agirre, and K. Cho. Unsupervised neural machine translation. arXiv\npreprint arXiv:1710.11041, 2017.\nD. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473, 2014.\nS. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences\nfrom a continuous space. arXiv preprint arXiv:1511.06349, 2015.\nT. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In\nProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing\nand Computational Natural Language Learning (EMNLP-CoNLL), 2007.\nT. Che, Y . Li, R. Zhang, R. D. Hjelm, W. Li, Y . Song, and Y . Bengio. Maximum-likelihood augmented\ndiscrete generative adversarial networks. arXiv preprint arXiv:1702.07983, 2017.\nJ. Chen, P.-S. Huang, X. He, J. Gao, and L. Deng. Unsupervised learning of predictors from unpaired\ninput-output samples. arXiv preprint arXiv:1606.04646, 2016a.\nX. Chen, Y . Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable\nrepresentation learning by information maximizing generative adversarial nets. In Advances in\nNeural Information Processing Systems, pages 2172–2180, 2016b.\nJ. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empirical evaluation of gated recurrent neural\nnetworks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\nE. L. Denton, S. Chintala, R. Fergus, et al. Deep generative image models using a laplacian pyramid\nof adversarial networks. In Advances in neural information processing systems, pages 1486–1494,\n2015.\nQ. Dou and K. Knight. Large scale decipherment for out-of-domain machine translation. In\nProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing\nand Computational Natural Language Learning, pages 266–275. Association for Computational\nLinguistics, 2012.\nZ. Fu, X. Tan, N. Peng, D. Zhao, and R. Yan. Style transfer in text: Exploration and evaluation.arXiv\npreprint arXiv:1711.06861, 2017.\nL. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In\nComputer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pages 2414–2423.\nIEEE, 2016.\nA. N. Gomez, S. Huang, I. Zhang, B. M. Li, M. Osama, and L. Kaiser. Unsupervised cipher cracking\nusing discrete gans. arXiv preprint arXiv:1801.04883, 2018.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and\nY . Bengio. Generative adversarial nets. In Advances in neural information processing systems,\npages 2672–2680, 2014.\nC. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin, F. Bougares, H. Schwenk, and Y . Bengio.\nOn using monolingual corpora in neural machine translation. arXiv preprint arXiv:1503.03535,\n2015.\nD. He, Y . Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y . Ma. Dual learning for machine translation.\nIn Advances in Neural Information Processing Systems, pages 820–828, 2016.\nZ. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In\nInternational Conference on Machine Learning, pages 1587–1596, 2017a.\n10\nZ. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing. On unifying deep generative models. arXiv\npreprint arXiv:1706.00550, 2017b.\nZ. Hu, Z. Yang, R. Salakhutdinov, X. Liang, L. Qin, H. Dong, and E. Xing. Deep generative models\nwith learnable knowledge constraints. arXiv preprint arXiv:1806.09764, 2018a.\nZ. Hu, Z. Yang, T. Zhao, H. Shi, J. He, D. Wang, X. Ma, Z. Liu, X. Liang, L. Qin, et al. Texar: A\nmodularized, versatile, and extensible toolbox for text generation. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), pages 13–22, 2018b.\nX. Huang and S. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization.\nCoRR, abs/1703.06868, 2017.\nP. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial\nnetworks. arXiv preprint, 2017.\nE. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint\narXiv:1611.01144, 2016.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980,\n2014.\nD. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\nA. M. Lamb, A. G. A. P. GOYAL, Y . Zhang, S. Zhang, A. C. Courville, and Y . Bengio. Professor\nforcing: A new algorithm for training recurrent networks. In Advances In Neural Information\nProcessing Systems, pages 4601–4609, 2016.\nG. Lample, L. Denoyer, and M. Ranzato. Unsupervised machine translation using monolingual\ncorpora only. arXiv preprint arXiv:1711.00043, 2017.\nJ. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky. Adversarial learning for neural dialogue\ngeneration. arXiv preprint arXiv:1701.06547, 2017.\nJ. Li, R. Jia, H. He, and P. Liang. Delete, retrieve, generate: A simple approach to sentiment and\nstyle transfer. arXiv preprint arXiv:1804.06437, 2018.\nK. Lin, D. Li, X. He, Z. Zhang, and M.-T. Sun. Adversarial ranking for language generation. In\nAdvances in Neural Information Processing Systems, pages 3155–3165, 2017.\nY . Liu, J. Chen, and L. Deng. Unsupervised sequence classiﬁcation using sequential output statistics.\nIn Advances in Neural Information Processing Systems, pages 3550–3559, 2017.\nN. Pourdamghani and K. Knight. Deciphering related languages. In Proceedings of the 2017\nConference on Empirical Methods in Natural Language Processing, pages 2513–2518, 2017.\nS. Prabhumoye, Y . Tsvetkov, R. Salakhutdinov, and A. W. Black. Style transfer through back-\ntranslation. arXiv preprint arXiv:1804.09000, 2018.\nA. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional\ngenerative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nT. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Radford, and X. Chen. Improved techniques\nfor training gans. In Advances in Neural Information Processing Systems, pages 2234–2242, 2016.\nT. Shen, T. Lei, R. Barzilay, and T. Jaakkola. Style transfer from non-parallel text by cross-alignment.\nIn Advances in Neural Information Processing Systems, pages 6833–6844, 2017.\nR. S. Sutton, D. A. McAllester, S. P. Singh, and Y . Mansour. Policy gradient methods for reinforcement\nlearning with function approximation. In Advances in neural information processing systems,\npages 1057–1063, 2000.\nO. Vinyals and Q. Le. A neural conversational model. arXiv preprint arXiv:1506.05869, 2015.\n11\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In\nComputer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3156–3164.\nIEEE, 2015.\nT.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona, P.-H. Su, S. Ultes, and\nS. Young. A network-based end-to-end trainable task-oriented dialogue system. arXiv preprint\narXiv:1604.04562, 2016.\nZ. Yang, W. Chen, F. Wang, and B. Xu. Improving neural machine translation with conditional\nsequence generative adversarial nets. arXiv preprint arXiv:1703.04887, 2017a.\nZ. Yang, Z. Hu, R. Salakhutdinov, and T. Berg-Kirkpatrick. Improved variational autoencoders for\ntext modeling using dilated convolutions. arXiv preprint arXiv:1702.08139, 2017b.\nL. Yu, W. Zhang, J. Wang, and Y . Yu. Seqgan: Sequence generative adversarial nets with policy\ngradient. In AAAI, pages 2852–2858, 2017.\nJ. Zhao, M. Mathieu, and Y . LeCun. Energy-based generative adversarial network.arXiv preprint\narXiv:1609.03126, 2016.\nJ.-Y . Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.\n12\nA Training Algorithms\nAlgorithm 1 Unsupervised text style transfer.\nInput: Data set of two different styles X, Y.\nParameters: weight λand γ, temperature τ.\nInitialized model parameters θE,θG,θLMx ,θLMy .\nrepeat\nUpdate θLMx and θLMy by minimizing Lx\nLM(θLMx ) and Ly\nLM(θLMy ) respectively.\nUpdate θE,θG by minimizing: Lrec −λ(Lx\nLM + Ly\nLM) using Equation 5.\nuntil convergence\nOutput: A text style transfer model with parameters θE, θG.\nB Model Conﬁgurations\nSimilar model conﬁguration to that of (Shen et al., 2017) is used for a fair comparison. We use\none-layer GRU (Chung et al., 2014) as the encoder and decoder (generator). We set the word\nembedding size to be 100 and GRU hidden size to be 700. v is a vector of size 200. For the language\nmodel, we use the same architecture as the decoder. The parameters of the language model are not\nshared with parameters of other parts and are trained from scratch. We use a batch size of 128, which\ncontains 64 samples from X and Y respectively. We use Adam (Kingma and Ba, 2014) optimization\nalgorithm to train both the language model and the auto-encoder and the learning rate is set to be\nthe same. Hyper-parameters are selected based on the validation set. We use grid search to pick the\nbest parameters. The learning rate is selected from [1e−3,5e−4,2e−4,1e−4] and λ, the weight\nof language model loss, is selected from [1.0,0.5,0.1]. Models are trained for a total of 20 epochs.\nWe use an annealing strategy to set the temperature of τ of the Gumbel-softmax approximation. The\ninitial value of τ is set to 1.0 and it decays by half every epoch until reaching the minimum value of\n0.001.\n13\nC Sentiment Transfer Examples\nModel Negative to Positive\nOriginal it was super dry and had a weird taste to the entire slice .\n(Shen et al., 2017) it was super friendly and had a nice touch to the same .\n(Hu et al., 2017a) it was super well-made and had a weird taste to the entire slice .\nLM it was very good , had a good taste to the food service .\nLM + classiﬁer it was super fresh and had a delicious taste to the entire slice .\nOriginal my goodness it was so gross .\n(Shen et al., 2017) my server it was so .\n(Hu et al., 2017a) my goodness it was so refreshing .\nLM my food it was so good .\nLM + classiﬁer my goodness it was so great .\nOriginal maintenance is equally incompetent .\n(Shen et al., 2017) everything is terriﬁc professional .\n(Hu et al., 2017a) maintenance is equally hilarious .\nLM maintenance is very great .\nLM + classiﬁer maintenance is equally great .\nOriginal if i could give them a zero star review i would !\n(Shen et al., 2017) if i will give them a breakfast star here ever !\n(Hu et al., 2017a) if i lite give them a sweetheart star review i would !\nLM if i could give them a _num_ star place i would .\nLM + classiﬁer if i can give them a great star review i would !\nModel Positive to Negative\nOriginal did n’t know this type cuisine could be this great !\n(Shen et al., 2017) did n’t know this old food you make this same horrible !\n(Hu et al., 2017a) did n’t know this type cuisine could be this great !\nLM did n’t know this type , could be this bad .\nLM + classiﬁer did n’t know this type cuisine could be this horrible .\nOriginal besides that , the wine selection they have is pretty awesome as well .\n(Shen et al., 2017) after that , the quality prices that does n’t pretty much well as .\n(Hu et al., 2017a) besides that , the wine selection they have is pretty borderline as atrocious .\nLM besides that , the food selection they have is pretty awful as well .\nLM + classiﬁer besides that , the wine selection they have is pretty horrible as well .\nOriginal uncle george is very friendly to each guest .\n(Shen et al., 2017) if there is very rude to our cab .\n(Hu et al., 2017a) uncle george is very lackluster to each guest .\nLM uncle george is very rude to each guest .\nLM + classiﬁer uncle george is very rude to each guest .\nOriginal the food is fresh and the environment is good .\n(Shen et al., 2017) the food is bland and the food is the nightmare .\n(Hu et al., 2017a) the food is atrocious and the environment is atrocious .\nLM the food is bad , the food is bad .\nLM + classiﬁer the food is bland and the environment is bad .\nTable 5: Sentiment transfer examples.\n14",
  "topic": "Discriminator",
  "concepts": [
    {
      "name": "Discriminator",
      "score": 0.950395941734314
    },
    {
      "name": "Computer science",
      "score": 0.8287546634674072
    },
    {
      "name": "Language model",
      "score": 0.6843781471252441
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6302331686019897
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.5911357998847961
    },
    {
      "name": "Process (computing)",
      "score": 0.5024645328521729
    },
    {
      "name": "Natural language processing",
      "score": 0.481172651052475
    },
    {
      "name": "Word (group theory)",
      "score": 0.46749985218048096
    },
    {
      "name": "Security token",
      "score": 0.4580904245376587
    },
    {
      "name": "Binary number",
      "score": 0.4514056146144867
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4484095275402069
    },
    {
      "name": "Speech recognition",
      "score": 0.41127318143844604
    },
    {
      "name": "Detector",
      "score": 0.12814590334892273
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Arithmetic",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}