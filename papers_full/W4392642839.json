{
  "title": "Polyp Segmentation With the FCB-SwinV2 Transformer",
  "url": "https://openalex.org/W4392642839",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2752449384",
      "name": "Kerr Fitzgerald",
      "affiliations": [
        "University of Central Lancashire"
      ]
    },
    {
      "id": "https://openalex.org/A2009193368",
      "name": "Jorge Bernal",
      "affiliations": [
        "Computer Vision Center"
      ]
    },
    {
      "id": "https://openalex.org/A2144500454",
      "name": "Aymeric Histace",
      "affiliations": [
        "CY Cergy Paris UniversitÃ©",
        "Ã‰cole Nationale SupÃ©rieure de l'Ã‰lectronique et de ses Applications",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A1278365713",
      "name": "Bogdan J. Matuszewski",
      "affiliations": [
        "University of Central Lancashire"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2320150036",
    "https://openalex.org/W2624917431",
    "https://openalex.org/W2728714629",
    "https://openalex.org/W2111147183",
    "https://openalex.org/W2075266835",
    "https://openalex.org/W4287225564",
    "https://openalex.org/W4380992399",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W2997286550",
    "https://openalex.org/W2008359794",
    "https://openalex.org/W2586952804",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2963649926",
    "https://openalex.org/W3081752372",
    "https://openalex.org/W3044495028",
    "https://openalex.org/W4320713078",
    "https://openalex.org/W4226329133",
    "https://openalex.org/W2999580839",
    "https://openalex.org/W3041409972",
    "https://openalex.org/W3092344722",
    "https://openalex.org/W3162386519",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W4382877880",
    "https://openalex.org/W4285303086",
    "https://openalex.org/W3134036841",
    "https://openalex.org/W4289822973",
    "https://openalex.org/W4363650233",
    "https://openalex.org/W4313527382",
    "https://openalex.org/W4293257240",
    "https://openalex.org/W4287225832",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2963794428",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4385484654",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6617145748",
    "https://openalex.org/W3046605870",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W4384517811",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3098995103",
    "https://openalex.org/W2899663614"
  ],
  "abstract": "International audience",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nPolyp Segmentation with the FCB-SwinV2 \nTransformer \nKerr Fitzgerald1, Jorge Bernal2, Aymeric Histace3, Bogdan J. Matuszewski1 \n1 Computer Vision and Machine Learning (CVML) Group, University of Central Lancashire, Preston, United Kingdom  \n2 Computer Vision Center and Computer Science Department, Universitat AutÃ²noma de Barcelona, Barcelona, Spain  \n3 ETIS UMR 8051, CY Paris Cergy University, ENSEA, CNRS, Cergy, France  \nCorresponding author: Kerr Fitzgerald (e-mail: kffitzgerald@uclan.ac.uk). \nThis work was supported by the Science and Technology Facilities Council (Grant Number: ST/S005404/1) and by MCIN/AEI/10.13039/501100011033 \n(Grant Numbers: PID2020-120311RB-I00 and RED2022-134964-T) \nABSTRACT Polyp segmentation within colonoscopy video frames using deep learning models has the \npotential to automate colonoscopy screening procedures. This could help improve the early lesion detection \nrate and in vivo characterization of polyps which could develop into colorectal cancer. Recent state -of-the-\nart deep learning polyp segmentation models have combined Convolutional Neural Network (CNN) \narchitectures and Transformer Network  (TN) architectures. Motivated by the aim of improving the \nperformance of polyp s egmentation models and their robustness to  data variations beyond those covered \nduring training , we propose a new CNN -TN hybrid model named the FCB -SwinV2 Transformer. This \nmodel was created by making extensive modifications to the recent state -of-the-art FCN-Transformer, \nincluding replacing the TN branch architecture with a SwinV2 U-Net. The performance of the FCB-SwinV2 \nTransformer is evaluated on the popular colonoscopy segmentation benchmarking datasets Kvasir -SEG, \nCVC-ClinicDB and ETIS -LaribPolypDB. Generalizability tests are also conducted to determine if models \ncan maintain accuracy when evaluated on data outside of the training distribution . The FCB -SwinV2 \nTransformer consistently achieves higher mean Dice and mean IoU scores when compared to other models \nreported in literature and therefore represents new state -of-the-art performance. The importance of \nunderstanding subtleties in evaluation metrics and dataset partitioning are also demonstrated and discussed. \nCode available: https://github.com/KerrFitzgerald/Polyp_FCB-SwinV2Transformer  \nINDEX TERMS Medical image processing, Polyp segmentation, Deep learning, SwinV2, Transformer\nI. INTRODUCTION \n \nColorectal cancer is the second lead cause of cancer -related \ndeaths worldwide. In 2020, more than 930,000 deaths \noccurred due to colorectal cancer with more than 1.9 million \ncases being diagnosed. It is estimated that by 2040 there will \nbe 3.2 million new cases of colorectal cancer and that the \nnumber of deaths will increase to 1.6 million [1]. Colorectal \ncancer often arises from small benign polyps which progress \nover time to become malignant. Colonoscopy is widely \nconsidered as the gold standard among polyp screening and \nremoval procedures. The procedure is performed using a \ncolonoscope, a long, flexible tube with a camera and light at \nthe end. The colonoscope is inserted through the patients \nrectum and into the colon, allowing clinicians to navigate \nthrough the colon and visually inspect targeted regions for \nabnormalities in real time. Additionally, the colonoscope can \nhave an instrument channel which allow s surgical tools to \nremove identified polyps, a procedure known as \npolypectomy.  \n \nColonoscopy procedures do have limitations  as studies \nestimate that between 17% and 28% of polyps are missed [2] \n[3] [4]. Missed polyps can significantly impact patient health \nand it is predicted that improving polyp detection rates by \n1% would reduce the risk of colorectal cancer development \nby approximately 3%  [5]. High demands on healthcare \nsystems are also increasing the pressure and workloads \nplaced upon colonoscopy clinicians [6]. Computer aided \nsystems to support clinicians in improving the detection rate \nand characterization of polyps have therefore undergone \nsignificant research in recent years. Due to their excellent \nperformance, deep learning models now dominate this \nresearch area. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  2 \nThe goal of polyp segmentation in colonoscopy images is to \naccurately identify and delineate polyps from the \nsurrounding healthy tissue of the colon. Accurate \nsegmentation of polyps allows for a comprehensive \nassessment of a polypâ€™s texture, shape and rela tive size \nwhich can be crucial in assessing the polypâ€™s malignancy or \npotential to develop into a malignancy. However, variability \nin patients, colonoscopy procedures (e.g. position and angle \nof colonoscope) and polyp morphologies cause images of \npolyps to differ in shape, size, color and texture. The task of \nautomatic polyp segmentation using deep learning models \nremains challenging due to this variability and is exacerbated \nby the limited availability of polyp image databases. \n \nIn recent years, deep learning models for the semantic \nsegmentation of polyps have predominantly been \ncomposed of Convolutional Neural Networks (CNN s) or \nTransformer Networks (TNs). Subsequently, hybrid \nsemantic segmentation  models which combine the benefits \nof both CNN and TN architectures have been developed. \nRepresentative examples of current state -of-the-art models \nfor polyp segmentation are the Fully Convolutional \nBranch-TransFormer (FCN -Transformer) [7] and DUCK -\nNet [8]. The DUCK -Net model is a F ully Convolutional  \nNetwork (FCN)  whilst the architecture of the FCN -\nTransformer combines the benefits of both TNs and CNNs \nby running a model of each type in parallel and combining \nthe outputs which are then passed onto a prediction head \nfor processing. The TN architecture used in the FCN -\nTransformer is the Pyramid Vision Transformer Version 2 \n(PVTv2) [9].  \n \nMotivated by the aim of improving the performance of \npolyp segmentation models and their robustness to data \noutside of the training distribution, we propose a new CNN -\nTN hybrid model named the FCB -SwinV2 Transformer. \nThis model has been created by making extensive \nmodifications to the FCN -Transformer [7]. These include \nchanges to the Fully Convolutional Branch (FCB) of the \nFCN-Transformer (including the use of an increased \nnumber of channel dimensions and a residual post \nnormalization approach) and the replacement of the  PVTv2 \nmodel within the Transformer Branch (TB) with a SwinV2  \n[10] based U-Net. The reasoning behind  the TB  \nreplacement is due to the unique shifted window -based \nself-attention mechanism employed by SwinV2 models. \nThis mechanism excels in capturing complex hierarchical \nstructures  and should help to capture relevant information \nacross various polyp morphologies  to improve \nsegmentation performance.  \n \nOur main contributions include:  \nâ€¢ A novel CNN -TN hybrid deep learning model \nnamed the FCB -SwinV2 Transformer, created by \nmaking extensive modifications to the previous \nstate-of-the-art FCN-Transformer  [7]. \nâ€¢ A p erformance comparison (including \ngeneralizability testing where possible)  of the \nFCB-SwinV2 Transformer with high performing  \nmodels on popular colonoscopy segmentation \nbenchmarking datasets including Kvasir-SEG \n[11], CVC-ClinicDB [12] and ETIS-LaribPolypDB \n[13]. The FCB -SwinV2 Transformer achieves \nstate-of-the-art performance.  \nâ€¢ An examination of common issues within polyp \nsegmentation literature relating to dataset \npartitioning and averaging methodologies used to \ncalculate performance metrics. Experimental proof \nof the critical importance of such issues is provided. \n \nII. Related Work \n \nThis section provides an overview of the relevant work on \nthe semantic segmentation of polyps. Fully Convolutional \nNetworks (FCNs), Transformer Networks (TNs), and CNN -\nTN hybrid architectures are described. Summaries on recent \nstate-of-the-art models are provided. \nA. Fully Convolutional Networks (FCNs) \n \nOne of the most influential deep learning models for medical \nimage segmentation is U-Net [14]. The original U-Net model \nwas a Fully Convolutional Network (FCN) which consisted \nof an encoder and decoder. The encoder used in the original \nU-Net is a CNN. CNNs consist of various stacked layers, \neach designed to sequentially process the input data. These \nlayers apply filters through convolution operations, use \npooling to reduce dimensions, and employ activation \nfunctions to introduce non -linearities. This structure \neffectively extracts and refines features at each stage. The \nhierarchical nature of CNNs allows the encoder to synthesize \nabstract representations and patterns, capable of representing \nspecific shapes or entire objects.  The decoder uses \ntransposed convolutions which recover the spatial \ndimensions of the image by up -sampling the compressed \nfeature maps from the encoder. Skip connections link \ndecoder layers to corresponding encoder layers, thereby \nreintroducing the spatial information lost during down -\nsampling. This design enables the decoder to effectively \nutilize both high -level and low -level feature s from the \nencoder, ensuring accurate reconstruction of segmentation \nmaps. U-Net was designed as a â€˜one -stageâ€™ model, where \nimages are directly processed to produce a segmentation \nmap. This contrasts with two -stage [15] models where \nregions are first identified and then further analyzed for \nsemantic segmentation. Recent state -of-the-art polyp \nsemantic segmentation models follow a one -stage approach \nfor efficiency and direct processing capability. \n \nSince the introduction of U -Net, numerous FCNs for \nsemantic segmentation of polyps have evolved from the \noriginal U-Net architecture [8] [16] [17] [18] [19] [20] [21] \n[22] [23]. These models often incorporate advanced \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  3 \nImageNet pre -trained CNNs into the encoder, significantly \nimproving the efficiency and accuracy of feature extraction \nby leveraging pre -learned image representations.  Examples \nof high performing FCN models are summarized below. \n \nThe Parallel Reverse Attention Network (PraNet)  [22] \nemploys a parallel partial decoder to extract high -level \nfeatures and generate a global map for initial segmentation \nguidance. It then uses a reverse attention module to mine \npolyp boundary information  and employs a recurrent \ncooperation mechanism which iteratively refines the \nsegmentation by aligning the initial predictions with polyp \nboundaries. The Multi-Scale Residual Fusion Network  \n(MSRF-Net) [23] employs unique Dual-Scale Dense Fusion \n(DSDF) blocks to allow  multi-scale information exchange  \nand maintain high resolution. MSRF-Net is therefore able to \ncapture both high-level and low-level features which results \nin the prediction of accurate segmentation maps. The current \nstate-of-the-art FCN polyp segmentation model is DUCK -\nNet [8]. This model uses the  innovative DUCK \nconvolutional block which can apply various filter sizes in \nparallel. This allows adaptive selection of the most effective \nfilter size for each network stage . This allows the general \nlocalization of polyps whilst precisely  delineating their \nboundaries. In contrast to many other FCN segmentation \nmodels, DUCK-Net does not use any form of encoder pre -\ntraining, demonstrating the high power of its feature \nextraction capabilities. \n \nFCNs do have inherent drawbacks which can limit their \nperformance for polyp segmentation. Due to their limited \nreceptive field size, FCNs can have a reduced understanding \nof the global contextual information contained within an \nimage. This can cause FCN models to struggle with scale \nvariability and can lead to poor generalization performance. \nB. Transformer Networks (TNs) and Hybrid Models \n \nThe introduction of the Vis ion Transformer (ViT) [24] \nrevolutionized computer vision research by using a \nTransformer Network (TN) to conduct image classification. \nLike CNNs, TNs are composed of stacked layers that \nsequentially process input data. However, TNs apply self -\nattention mechanisms instead of convol ution operations. \nTNs typically work by splitting images into a number of \nfixed-size patches. The patches are then flattened and \npositional embeddings are added to ensure the spatial \nrelationship between patches is maintained. Layers of the \ntransformer network use the self -attention mechanism to \ncalculate attention scores for all pairs of image patches. This \nallows the network to assess the relative importance of each \npatch with regard to every other patch, allowing the network \nto capture global contextual information across entire \nimages. As information progresses through successive \nlayers, the TN is able to focus on different aspects of the \nimage patches simultaneously, enhancing its feature \nextraction capability and understanding of relationships \nbetween image regions. The self -attention mechanisms \nability to capture global contextual information and examine \nspecific image regions enhances the TNs ability to \nunderstand the shape and texture of  regions which may be \nrelevant for polyp segmentation. \n \nSince the introduction of the ViT, numerous polyp semantic \nsegmentation models which are composed of fully TN based \n[25] [26] [27] or CNN-TN hybrid architectures  [7] [28] [29] \n[30] [31] [32] have been developed. M any of these models \nemploy a U-Net inspired encoder -decoder style s tructure. \nCNN-TN hybrid models are now commonly used as these \ncan help overcome the limitations of using pure TN models. \nThe main such limitation of pure TN models is that the lack \nof engineered feature extraction processes (when compared \nto FCNs) means that pure TNs typically require large \namounts of training data. This is problematic for polyp \nsegmentation due to the small sizes and limited availability \nof polyp image databases.  Examples of hi gh performing \nCNN-TN hybrid models are summarized below.  \n \nPolyp2Seg [33] uses the Pyramid Vision Transformer \nVersion 2 (PVTv2) [9] as an encoder for multi-scale feature \nextraction. For each encoder stage, extracted features are \npassed into Compression Modules (CMs) to reduce the \nchannel dimensions to a consistent size. The compressed \nfeatures are then passed into Feature Aggregation Modules \n(FAMs) to directly combine lower -level and higher -level \nfeatures. A Multi -Context Attention Module (MCAM) is \nalso applied on the lowest-level feature maps to enhance the \ncapture of low -level information, such as polyp texture and \ncolor. ESFPNet uses the Mix Transformer (MiT)  [25] as an \nencoder. For each encoder stage, extracted features are \npassed into an Efficient Stage-wise Feature Pyramid (ESFP) \ndecoder. The decoder generates linear predictions for each \noutput stage and the n linearly fuses the processed features \ntogether. Intermediate processed features are also \nconcatenated with the previous decoder layers intermediate \nprocessed features. This allows the model to progressively \nintegrate global features from later layers with local features \nfrom earlier layers to construct comprehensive feature maps. \nThe FCN-Transformer  [7] employs the unique approach of \nhaving a Transformer Branch (TB) and a Fully \nConvolutional Branch (FCB) which run in parallel. The TB \nuses the PVTv2 as an encoder which passes features to a n \nenhanced Progressive Locality Decoder (PLD) . The PLD  \nfeatures advanced local emphasis (LE) and stepwise feature \naggregation (SFA)  modules. The FCB is composed of \nmodern residual blocks and encourages the extraction of \nfeatures required for processing outputs of the TB into full-\nsize (i.e. matching ground truth resoluti on) segmentation \npredictions.  \n \nThis paper proposes extensive modifications to the \nFCN-Transformer to further improve polyp semantic \nsegmentation performance.  \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  4 \nIII. FCB-SwinV2 Transformer Model Design \nA. Overview of existing SwinV2 Models \n \nThe SwinV2 Transformer [10] was developed to tackle issues \nwith training stability and resolution gaps between pre -\ntraining and fine -tuning that arose with the original Swin \nTransformer [34]. In the original Swin Transformer, training \ninstability was caused by large activation output amplitude \ndiscrepancies in different layers of the network. To solve this \nproblem the authors of the SwinV2 Transformer developed an \napproach called â€˜residual po st normalizationâ€™. In this \napproach, the output of residual blocks is normalized before \nmerging into the main branch of the network. This was shown \nby the SwinV2 creators to cause much milder activation \namplitudes than in the original pre -normalization \nconfiguration. The authors of the SwinV2 Transformer also \nreplaced the original dot product attention mechanism due to \nthe finding that learnt attention maps of some blocks were \nfrequently dominated by only a few pixel pairs. The dot \nproduct attention mechani sm was therefore replaced by a \nmechanism which computes the attention logit of a pixel pair \nusing a scaled cosine function. Since the cosine function is \nnaturally normalized, this helps the network become more \ninsensitive to the amplitude of activations. \n \nThe SwinV2 Transformer network first partitions an RGB \ninput image into non -overlapping patches. Each patch is a \nconcatenation of the RGB pixel values which are subsequently \npassed to a linear embedding layer which projects them to \nhave an arbitrary channe l dimension. Patch merging layers \nthen concatenate neighboring patches before passing them \nthrough a linear layer. This reduces the number of patches by \na factor of 2 and increases the channel dimension by a factor \nof 2. The output of the patch merging lay er is then passed \nthrough several successive SwinV2 transformer blocks. The \nsuccessive SwinV2 transformer blocks are shown in Figure 1.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThese blocks consist of either window based multi-head self-\nattention (W-MSA) (first block) or shifted window partition \nmulti-head self-attention (SW-MSA) (second block) layers, \nfollowed by an MLP with GELU [35] activation layer and \nstages of layer normalization (LN). Residual connections are \nalso present within the block. \nB. FCB-SwinV2 Transformer Architecture \n \nThe overall architecture of the FCB -SwinV2 Transformer is \nshown in Figure 2. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe TB of the original FCN-Transformer is replaced in this \nwork by a SwinV2 U-Net architecture. SwinV2 models apply  \nshifted window -based self -attention mechanisms which  \nexcel in capturing complex hierarchical structures. It was \nhypothesized that this w ould help extract relevant \ninformation across various polyp morphologies to improve \nsegmentation performance and generalizability to data \noutside of the training distribution. Empirical evidence of \nimproved segmentation performance is reflected by the \nADE20K [36] segmentation benchmark. SwinV2 models are \ncapable of achieving 59.9%mIoU [10] whilst PVTv2 models \nachieve 48.7% [9]. This substantial improvement in a general \nsegmentation challenge strongly indicates that SwinV2 can \nenhance polyp segmentation, both in terms of accuracy and \nreliability. \n \nThe SwinV2 U-Net style architecture used in this work was \nbased on a model [37] which used a Swin encoder [34] [38] \nwith decoder blocks composed of â€˜Spatial and Channel \nSqueeze and Excitationâ€™ (SCSE) modules [39] [40] and \nFigure 2: Overall FCB-SwinV2 Transformer architecture. \nFigure 1: Two successive SwinV2 Transformer Blocks [13]. The \nresidual post normalization configuration ensures layer \nnormalization is conducted after attention layers and MLP layers. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  5 \nstandard convolution modules. The SCSE module is \ncomposed of a â€˜Spatial Squeeze and Channel Excitationâ€™ \n(SSCE) module and a â€˜Channel Squeeze and Spatial \nExcitationâ€™ (CSSE) module. The SSCE module takes an input \ntensor and reduces the spatial dimension using global average \npooling. The resulting tensor is passed through convolutional \nand activation layers before performing element -wise \nmultiplication with the original input tensor. This results in an \noutput tensor with adaptively re-weighted channel values. The \nCSSE module takes the input tensor and reduces the channel \ndimension using convolution. The resulting tensor is passed \nthrough an activation layer before element-wise multiplication \nwith the original input tensor. This results in an output tensor \nwith adaptively reweighted spatial features. The SCSE module \ncombines the outputs of the CSSE and SSCE modules using \nelement-wise summation, therefore maximizing information \npropagation through the network at a pixel and channel level \nsimultaneously. Element -wise summation is chosen over \nconcatenation in order to avoid increasing tensor dimensions \nand therefore model complexity. The structure of the decoder \nblock and SCSE module is shown in Figure 3. \n \n \n \n \n \n \n \n \n \n \n \nThe resolution of input images into the SwinV2 encoder \nmodel (and overall FCB -SwinV2 Transformer model) used \nin this work is 384x384 due to the availability of ImageNet \n[41] pre-trained SwinV2 encoder models available within \nthe PyTorch Image Model library [38]. The SwinV2 U-Net \narchitecture used as the TB is displayed in Figure 4.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 3: (a) The decoder block [35] uses channel wise \nconcatenation to combine previous decoder layer output with \nencoder skip connection output. (b) The structure of the SCSE \nmodule which combines the output of the SSCE and CSSE modules. \nFigure 4: SwinV2-UNET [10] architecture used as the TB of the \nFCB-SwinV2 Transformer. The encoder stages reduce the spatial \ndimensions of feature maps while increasing the number of \nchannel dimensions. Skip connections are used to pass feature \nmaps generated by each stage of the encoder to decoder stages. \nFigure 5: Changes made to the residual block (RB). Original RB used \nby the FCN-Transformer (left) vs the RB used by the FCB-SwinV2 \nTransformer (right) which features residual post normalization. The \noverall structure of the FCB is detailed fully in [7]. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  6 \nFigure 5 shows the minor changes made to the FCB when \ncompared to the FCB of the original FCN-Transformer model. \nThese modifications included an increased number of channel \ndimensions (to match the number of channel dimensions \noutput from the TB) and a cha nge in the order of group \nnormalization (GN), convolution (Conv) and activation (Act) \nlayers in the FCB residual block (RB) inspired by the residual \npost normalization approach of the SwinV2 Transformer.  \n \nIV. Experiments \nA. Dataset Selection and Partitioning \n \nThe Kvasir-SEG [11], CVC-ClinicDB [12] and \nETIS-LaribPolypDB [13] datasets have been used in this work \nto evaluate the performance of the FCB-SwinV2 Transformer. \nThese datasets have been chosen because they are open access \nat the time of writing (which is not true for all popular datasets \nreported within literature) and because they are commonly \nused in colonoscopy segmentation literature to evaluate model \nperformance and hence allow for comparative analysis \nbetween models. The Kvasir -SEG dataset consists of 1000 \nimages of polyps and ground truth binary segmentation masks \nof varying resolutions. The CVC-ClinicDB dataset consists of \n612 images of polyps and ground truth binary segmentation \nmasks of standard resolution 384x288. The \nETIS-LaribPolypDB dataset consists of 196 images of polyps \nand ground truth binary segmentation masks of resolution \n1225x966. However, after examining recent literature on deep \nlearning polyp segmentation models which use these datasets \nfor evaluation, two important issues have been identified. \n \nFirstly, many authors evaluating polyp segmentation models \nuse popular dataset splitting functions (such as the \ntrain_test_split function from the scikit-learn python module) \nto create random training/validation/test data partitions \n(typically using an 80% /10%/10% ratio). However, for the \nrelatively small image datasets used to evaluate colonoscopy \nsegmentation models, minor differences in how the data is \npartitioned c ould cause noticeable performance changes. \nRecently this has been demonstrated for models being \nevaluated on the Kvasir -SEG dataset where performance \nchanges greater than 1% were shown to occur for different \ndata partitions [42]. This is significant because the current \nhighest performing models are now typically separated by less \nthan 1% performance differences. Random number seeds are \noften used to control the data partition, but the exact \nmethodologies used to create data loade rs and the specific \nrandom number seeds chosen are often not defined in enough \ndetail. Differences in computer platforms and hidden random \nnumber state settings also exacerbate this problem. \n \nSecondly, many authors using colonoscopy datasets which are \ncomposed of images from multiple video sequences (such as \nthe CVC -ClinicDB [12] and ETIS -LaribPolypDB [13] \ndatasets) create random training/validation/test data partitions \nbut do not provide evidence to suggest they have taken steps \nto avoid data leakage and hence images of the same polyp \ncould be present in the different data partitions. Data leakage \nis possible in the CVC-ClinicDB dataset as the 612 images are \nfrom video frames that have been taken from 29 video \nsequences. Data leakage is also possible in the ETIS -\nLaribPolypDB dataset as the 196 images are from video \nframes that have been taken from 34 video sequences. It is \nhighly likely that frames from the same video sequences (and \nhence same polyps) are present across the training, validation \nand test data partitions that are evaluated and reported in \nliterature when random splits have been used. \n \nTo provide comparative assessment and due to noticeable \nperformance changes resulting from different data partitions, \nwe evaluate our model against other state-of-the-art methods \non the Kvasir-SEG, CVC-ClinicDB and ETIS-LaribPolypDB \ndatasets using the same data partitions as those used in [8] to \ntrain and evaluate the DUCK -Net model. This is possible \nbecause the DUCK-Net authors provide information detailing \nexactly which images have been used for training, validation, \nand testing. To further demonstrate the issue of noticeable \nperformance changes due to different data partitions we also \ninclude results obtained for random data partitions of the \nKvasir-SEG, CVC -ClinicDB and ETIS -LaribPolypDB \ndatasets. All data partitions used have training/validation/test \ndata partitions with an 80%/10%/10% split. Files containing \npartition information are provided in the GitHub code \nrepository of this work. \n \nDue to the issue of data leakage being possible between \npartitions in the CVC -ClinicDB and ETIS -LaribPolypDB \ndatasets, additional results are reported for the CVC-ClinicDB \ndataset for five training/validation/test data partitions with \napproximate 80%/10%/10% ratios where no data leakage \noccurs. This has been done to demonstrate the impact of data \nleakage on model evaluation. The data partitions with no data \nleakage were created based on a random selection of videos \nrather than images, preventing the same po lyp being \nrepresented in the training/validation/test subsets. The video \nsequences used for validation and testing for the five data \npartitions with no data leakage are displayed in Table 1. \n \n \nTable 1: Information on the CVC-ClinicDB dataset video \nsequences used to create 5 data partitions with no data leakage. \nPartition \nNumber \nValidation \nSequences \nValidation \nRatio \nTesting \nSequences \nTesting \nRatio \n1 1, 2, 3 10.95% 4, 5, 6 9.64% \n2 7, 8, 9 11.93% 10, 11, 12 8.66% \n3 13, 14, 15 10.62% 16, 17, 18 10.78% \n4 19, 20, 21 10.46% 22, 23, 24 9.15% \n5 25, 26 7.03% 27, 28, 29 10.78% \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  7 \nGeneralizability tests are also conducted in this work. The \nmodel trained on the Kvasir -SEG dataset (using the same \n80%/10%/10% training/validation/test as DUCK -Net and \nrandom data partitions) is evaluated on the full CVC-ClinicDB \ndataset and the model tra ined on the CVC -ClinicDB dataset \n(using the same 80%/10%/10% training/validation/test as \nDUCK-Net and random data partitions) is evaluated on the full \nKvasir-SEG dataset. It should be noted that CVC -ClinicDB \nand Kvasir-SEG were collected at different clinical sites and \ncountries which further ensures dataset independence as \ndifferent devices and acquisition protocols would have been \nused. Therefore, generalizability tests give an indication of \nhow the models perform with respect to a somewhat different \ndata distribution and help alleviate the identified issues as they \ngreatly reduce the impact of data partition changes and \neliminate data from the training partition leaking into the test \npartition. Such tests are also more representative of real-world \nscenarios where models are used on data sampled from \ndistributions which may be different from the distribution of \nthe training data. \nB. Evaluation Metrics \n \nThe performance of the FCB-SwinV2 Transformer is assessed \nusing Dice coefficient, Intersection over Union (IoU), \nprecision, and recall metrics. These metrics are computed \nusing the following formulas: \n \nğ·ğ¼ğ¶ğ¸ =\n2ğ‘‡ğ‘ƒ\n2ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ+ğ¹ğ‘ (1) \n \nğ¼ğ‘œğ‘ˆ =\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ+ğ¹ğ‘ (2) \n \nğ‘ƒğ‘…ğ¸ğ¶ğ¼ğ‘†ğ¼ğ‘‚ğ‘ =\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ (3) \n \nğ‘…ğ¸ğ¶ğ´ğ¿ğ¿ =\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ (4) \n \nHere, TP (True Positive) refers to correctly predicted \nsegmentation pixels, FP (False Positive) represents incorrectly \npredicted pixels labeled as polyps, and FN (False Negative) \ndenotes incorrectly predicted pixels labeled as non-polyp. \n \nDice and IoU metrics are positively correlated. The Dice score \noffers a reliable estimate of the average segmentation \nperformance across a test partition, while the IoU score serves \nto penalize individual instances of poor segmentation more \nsubstantially within the test partition. Each image in a test \npartition is input into the FCB-SwinV2 Transformer model to \ngenerate a binary segmentation prediction, which is then \ncompared to the corresponding ground truth binary \nsegmentation map to compute Dice, IoU, precision, and recall \nscores. Upon processing all images within a test partition, the \nmean Dice (mDice), mean IoU (mIoU), mean precision \n(mPrec), and mean recall (mRec) scores for the test partition \nare calculated.  \n \nHowever, after examining recent literature on deep learning \npolyp segmentation models an important issue with how \nmetrics are reported has been identified.   \n \nWhile many models use standard metrics such as the Dice \ncoefficient for model evaluation, there is variance in how these \nmetrics are reported across literature. This variance stems \nfrom the methods used to calculate average metrics for \ndatasets. The most frequently used averaging procedures are \nâ€˜image-wise averagingâ€™, â€˜batch-wise averagingâ€™, and â€˜dataset-\nwise averagingâ€™. For image -wise averaging, each individual \nprediction map is compared with its corresponding ground \ntruth map to produce a metric score for  each image. These \nindividual metric scores are then summed up and divided by \nthe total number of images in the test set to yield an average \nscore. For batch -wise averaging, prediction maps and their \nrespective ground truth maps within a batch are merged t o \nform larger composite prediction and ground truth maps. The \nmetric score is then computed based on these aggregated \nmaps. For dataset-wise averaging, all prediction maps in the \ntest set are consolidated into a single composite prediction \nmap, and similarly, all ground truth maps are combined. The \nmetric score is determined by comparing this holistic \nprediction against the complete composite ground truth map \n(this is sometimes referred to as the â€˜global scoreâ€™). For many \ncolonoscopy datasets there is a lar ge variation in the pixel -\nbased size of polyps and corresponding ground truth \nsegmentation maps which can cause discrepancies between \neach averaging method. A simple demonstration of this based \non pixel percentage correctly classified is given in Figure 6. \n \nFigure 6: Demonstration of the discrepancies between image-wise \naveraging (top/green), batch-wise averaging using batch size of 2 \n(middle/orange) and dataset-wise averaging (bottom/red). The light \ngreen circles represent model predictions whilst the white circles \nrepresent ground truth maps. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  8 \nMany models evaluated within literature do not explicitly state \nwhich averaging method has been used when reporting final \nscores which can lead to problems when trying to fairly \ncompare model performance. A solution would be for all \nauthors to use image-wise averaging as this is likely to produce \nthe most conservative scores as it captures true individual \nsegmentation performance for both large and small polyps. \n \nIn this work it has therefore been necessary to use different \naveraging methods to fairly compare model results reported in \nliterature and demonstrate the impact of the different \naveraging techniques. To compare against results reported for \nthe DUCK-Net model using the DUCK -Net data pa rtitions, \ndataset-wise averaging was used to evaluate performance.  \n \nEfforts were also made to replicate the evaluation procedure \nof the DUCK -Net model in this work which also allowed \nimage-wise averaged results to be reported for the DUCK-Net \nmodel. When using random data partitions to compare against \nmodels reported in li terature (excluding the DUCK -Net \nmodel) image-wise averaging was used as this has been found \nto provide the most conservative estimates of model \nperformance. For the FCB SwinV2 Transformer  design \nablation studies and to evaluate model performance when \nensuring no -data leakage occurs batch -wise averaging was \nused. \n \nAn example of how mDice is calculated using dataset -wise \naveraging for a dataset with ğ‘ images is given below: \n \nğ‘‡ğ‘ƒğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = âˆ‘ ğ‘‡ğ‘ƒğ‘–\nğ‘\nğ‘–=1  (5) \n \nğ¹ğ‘ƒğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = âˆ‘ ğ¹ğ‘ƒğ‘–\nğ‘\nğ‘–=1  (6) \n \nğ‘‡ğ‘ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = âˆ‘ ğ¹ğ‘ğ‘–\nğ‘\nğ‘–=1  (7) \n \nğ‘šğ·ğ‘–ğ‘ğ‘’ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ğ‘¤ğ‘–ğ‘ ğ‘’ =\n2ğ‘‡ğ‘ƒğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\n2ğ‘‡ğ‘ƒğ‘¡ğ‘œğ‘¡ğ‘ğ‘™+ğ¹ğ‘ƒğ‘¡ğ‘œğ‘¡ğ‘ğ‘™+ğ¹ğ‘ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\n (8) \n \nAn example of how mDice is calculated using image -wise \naveraging for a dataset with ğ‘ images is given below: \n \nğ·ğ‘–ğ‘ğ‘’ğ‘– =\n2ğ‘‡ğ‘ƒğ‘–\n2ğ‘‡ğ‘ƒğ‘–+ğ¹ğ‘ƒğ‘–+ğ¹ğ‘ğ‘–\n (9) \n \nğ‘šğ·ğ‘–ğ‘ğ‘’ğ‘–ğ‘šğ‘ğ‘”ğ‘’ğ‘¤ğ‘–ğ‘ ğ‘’ =\n1\nğ‘ âˆ‘ ğ·ğ‘–ğ‘ğ‘’ğ‘–\nğ‘\n1                           (10) \n \nHere ğ‘‡ğ‘ƒğ‘– , ğ¹ğ‘ƒğ‘– , ğ¹ğ‘ğ‘– and ğ·ğ‘–ğ‘ğ‘’ğ‘– represent the number of true \npositive pixels, false positive pixels, false negative pixels, \nand dice score for the ğ‘–ğ‘¡â„ image respectively. \nC. Computational Implementation Details \n \nThe FCB SwinV2 Transformer model was implemented using \nPyTorch. The model was trained and evaluated on images of \nresolution 384x384 and predicted binary segmentation maps \nof resolution 384x384. This resolution was used due to the \navailability of ImageNet [41] pre-trained SwinV2 transformer \nmodels. For each of the datasets the model was trained for 200 \nepochs with the loss function consisting of the sum of the \nBinary Cross Entropy (BCE) loss and dice loss. All training \nwas completed using a single NVIDIA 3090 G PU which \nnecessitated a batch size of 2. Total training time ranged \nbetween approximately 3 hours (ETIS-LaribPolypDB) and 12 \nhours (Kvasir-SEG). When running in inference mode the \nmodel was capable of processing approximately 20 images per \nsecond. The AdamW [43] optimizer was used with an initial \nlearning rate of 1e-5. The learning rate was reduced by a factor \nof 0.6 when the training loss did not improve over 10 epochs. \nModel weights were saved each time the validation dice score \nsurpassed the previous maximum s core. To generate the \npredicted segmentation map, pixel values were assigned a \nvalue of 1 if the model's output exceeded a threshold value of \n0.5, and 0 if it fell below this threshold. Experimental results \ndemonstrated that minor adjustments to the thresh old value, \neither increasing or decreasing it, led to modest improvements \nin performance, depending on the specific dataset utilized. \nHowever, the threshold value was kept at 0.5 so comparison \nagainst other models within the literature was fair. \n \nTable 2: Summary of training options/parameters used when \ntraining the FCB SwinV2 Transformer model. \nTraining Option/Parameter Selected Option/Value \nInput Resolution 384x384 \nPredicted Map Resolution 384x384 \nOptimizer AdamW \nSegmentation Threshold 0.5 \nEpochs 200 \nInitial Learning Rate 1e-5 \nLearning Rate Patience \n(epochs) 10 \nLearning Rate Reduction 0.6 \n \nThe data augmentations used in this work closely follows \nthose employed by the authors of the original FCN-\nTransformer [7]. Geometrical data augmentations applied to \nthe training images and masks included: vertical and \nhorizontal flips with a probability of 0.5;  transposing with a \nprobability of 0.5,  scaling with a magnitude sampled \nuniformly from [0.5, 1.5]; shearing with an angle sampled \nuniformly from [ -22.5Âº, 22.5Âº]; and affine transformations \nwith rotations. Note that horizontal and vertical translations \nare sampled uniformly from [ -48,48] with r otation angles \nbeing sampled uniformly from [ -180Âº, 180Âº]. Color data \naugmentations were applied to the training images only and \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  9 \nincluded: color jitter with brightness factor sampled uniformly \nfrom [0.6, 1.6], contrast factor sampled uniformly from [0.8, \n1.2], saturation factor sampled uniformly from [0.75, 1.25] \nand hue factor sampled uniformly from [0.99, 1.01];  and \nnormalization of RGB images between the interval [-1, 1]. \nD. Estimation of Model Uncertainty  \n \nUncertainty estimation in deep learning models can be broadly \ncategorized into two types: epistemic and aleatoric. Epistemic \nuncertainty arises from the model's lack of knowledge due to \nlimited training data and it can be potentially reduced with \nmore data or changes in model architecture. In contrast, \naleatoric uncertainty originates from the inherent noise in the \ndata and  remains irreducible even with additional data or \nmodel refinements. For polyp segmentation comprehensive \nuncertainty analysis would be ideal. This could be achieved by \nemploying methods like varying random number seeds to \ninduce small alterations in the model (e.g., weight \ninitialization, dropout) and utilizing diverse dat a partitions \n(e.g., K -fold cross-validation). However, the constraints of \ncomputational resources often render comprehensive \nuncertainty evaluations infeasible. As such, there is a clear \nneed for methods that efficiently approximate uncertainties. \nEpistemic uncertainty is particularly relevant for polyp \nsegmentation, especially given the likelihood of models \nencountering out-of-distribution data when used in real-world \nclinical scenarios. Monte Carlo (MC) dropout presents a \nviable solution for approximating epistemic uncertainty. \nDropout is a widely used regularization technique in deep \nlearning models which involves the random omission of \nneurons during training to prevent model over -reliance on \nspecific neurons and reduce overfitting [44]. MC dropout [45] \n[46] takes advantage of the standard dropout mechanism by \nenabling dropout during inference. This allows slightly \ndifferent versions of the model to make predictions on images \nwithin the test set. By recording performance metrics across \nmultiple inference runs with MC dropout, we can assess the \nvariance in model outputs hence allowing approximate \nestimations of epistemic uncertainty. However, it should be \nnoted that when using MC dropout, the entire capacity of the \nmodel is not utilized and predictions can often be less accurate \nwhen compared to single deterministic run values where the \nwhole capacity of the network is available. In this work 100 \nMC dropout model runs on selected test sets have been \nconducted to provide uncertainty estimates. \nE. FCB-SwinV2 Transformer  Ablation Study \n \nIn order to investigate other design architectures and help \nprovide some insight into how modifications impact \nperformance, a dditional FCB-SwinV2 Transformer model \narchitectures were also  developed and  tested. The \nadditional FCB-SwinV2 Transformer models were \nevaluated against the Kvasir -SEG [11] dataset using a \nrandom data partition  and metrics have been calculated \nusing batch -wise averaging .  \n \nThe first additional model tested used Convolutional Block \nAttention Modules (CBAMs)  [40] [47] within decoder \nblocks in the TB instead of SCSE modules. CBAMs are \nsimilar to SCSE modules as they aim to produce refined \nfeature maps to maximize information propagation through \nthe network. CBAM contains two sequential component \nmodules called the Chann el Attention Module (CAM) and \nthe Spatial Attention Module (SAM). The structure of the \nCBAM is shown in Figure 7. \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe CAM first produces two tensors with reduced spatial \ndimensions from an input feature map by using parallel \naverage pooling and max pooling layers. The two tensors \nare then passed through a shared MLP layer before being \ncombined using element -wise summa tion. The resulting \ntensor is passed through an activation layer and it is then \ncombined with the original feature map using element -wise \nmultiplication. The new feature map is then passed to the \nSAM which produces two tensors with reduced channel \ndimensions using parallel average pooling and max pooling \nlayers. The two tensors are then combined using \nconcatenation before being passed through a convolution \nFigure 7: Structure of the CBAM which combines \nsequentially combines the output of the CAM and SAM. The \nmechanisms used are like that of the SCSE module. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  10 \nlayer. The resulting tensor is passed through an activation \nlayer and it is then combined with the feature map produced \nby the CAM using element -wise multiplication.  \n \nThe second additional model tested had a TB that was based \nvery closely on the original, fully -attention based Swin \nU-Net [34]. The only modifications were related to the \nreplacement of the original Swin Transformer blocks with \nSwinV2 Transformer blocks.  \n \nThe final additional model tested used the original RB (pre -\nnormalization approach) of the FCN -Transformer in the \nFCB (see Figure 5).  \n \nV. Results and Evaluation \nA. Results of FCB-SwinV2 Transformer  Ablation \nStudy \n \nResults for the additional investigated FCB -SwinV2 \nTransformer model architectures evaluated against the \nKvasir-SEG [11] dataset using a random data partition are \ndisplayed in Table 3. \n \nTable 3: FCB-SwinV2 Transformer model variation tests on the \nKvasir-SEG dataset using the same random data partition. Scores \nreported here were calculated using batch-wise averaging. \n mDice mIoU mPrec mRec \nFCB-SwinV2 \nTransformer  \n(Selected architecture) \n94.30 89.82 93.77 95.73 \nCBAM Decoder in TB 94.10 89.44 93.73 95.26 \nFully Attention based \nSwinV2 U-Net TB 93.25 88.87 93.56 94.90 \nOriginal RB of FCN-\nTransformer 93.84 89.08 95.51 93.15 \n \nThe Fully Attention based SwinV2 U -Net TB  model was \nfound to perform worse than the selected FCB-SwinV2 \nTransformer design (using SCSE modules in the TB \ndecoder) and the FCB -SwinV2 Transformer using CBAMs \nin the TB decoder. This provides evidence that \nconvolutional based decoders making use of squeeze -excite \nstyle mechanisms perform better than when using attention \nonly mechanisms for the small data set sizes used in this \nstudy. The drop in performance when using the original RB \nof FCN -Transformer provides evidence that the residual \npost normalization approach within the modified RB of the \nFCB helps to improve segmentation performance.  \n \nSince the additional models did not perform as well as the \nFCB-SwinV2 Transformer architecture design (described \nin Section III), they are not investigated further in this \nstudy. \n \n \nB. DUCK-Net Data Partitioning Results \n \nThe performance of the FCB -SwinV2 Transformer  using \nboth image -wise averaging and dataset -wise averaging  is \nassessed using DUCK-Net data partitions across the Kvasir-\nSEG, CVC -ClinicDB, and ETIS -LaribPolypDB datasets, \nwith results presented in Tables 3 -8. Comparative analysis \nwas possible owing to the DUCK-Net authors providing \nfolders containing the distinct training, validation, and test \npartitions employed for each respective dataset. Where \napplicable, comparisons are made against the DUCK-Net-34 \nand FCN-Transformer model results, both from prior works \n[8] and those derived from executing the readily available \nDUCK-Net-34 code base and pre-trained models, with no re-\ntraining or fine-tuning, in the present study (this work). This \nwas necessary to generate  results using image-wise \naveraging during evaluation  not included in the original \npaper [11]. \n \nTable 4 and Table 5 demonstrate the modelâ€™s performance \non the Kvasir -SEG DUCK-Net data partition, employing \ndataset-wise and image -wise averaging, respectively. Both \ntables illustrate comparisons against DUCK-Net-34 results \nreported in [8] and those obtained in the current study. \n \n \nTable 4: Comparison of model performance using dataset-wise \naveraging on the Kvasir-SEG dataset against the 34 filter DUCK-\nNet model and FCN-Transformer model using the DUCK-Net data \npartition. \n mDice mIoU mPrec mRec \nFCN-Transformer 92.20 85.54 92.38 92.03 \nDUCK-Net-34 95.02 90.51 96.28 93.79 \nDUCK-Net-34 \n (This work) 94.71 89.93 95.54 93.87 \nFCB-SwinV2 \nTransformer 95.77 91.88 96.78 94.78 \n \n \nTable 5: Comparison of model performance using image-wise \naveraging on the Kvasir-SEG dataset against the 34 filter DUCK-\nNet model using the DUCK-Net data partition. \n mDice mIoU mPrec mRec \nDUCK-Net-34 \n(This work) 93.91 89.38 94.81 94.22 \nFCB-SwinV2 \nTransformer 94.88 90.82 95.61 94.90 \n \nTables 6 and 7 extend this analysis to the CVC -ClinicDB \nDUCK-Net data partition, adhering to the same  averaging \nmethods and comparative benchmarks. \n \nTable 6: Comparison of model performance using dataset-wise \naveraging on the CVC-ClinicDB dataset against the 34 filter \nDUCK-Net model and FCN-Transformer model using the DUCK-\nNet data partition.  \n mDice mIoU mPrec mRec \nFCN-Transformer 93.27 87.40 97.28 89.58 \nDUCK-Net-34 94.78 90.09 94.68 94.89 \nDUCK-Net-34 \n(This work) 94.64 89.82 94.42 94.86 \nFCB-SwinV2 \nTransformer 94.89 90.28 95.43 94.36 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  11 \nTable 7: Comparison of model performance using image-wise \naveraging on the CVC-ClinicDB dataset against the 34 filter \nDUCK-Net model using the DUCK-Net data partition. \n mDice mIoU mPrec mRec \nDUCK-Net-34 \n(This work) 92.65 87.44 93.75 92.64 \nFCB-SwinV2 \nTransformer 93.32 88.09 93.71 93.38 \n \nThe modelâ€™s performance is also examined using the ETIS -\nLaribPolypDB DUCK-Net data partition  with results  \ndetailed in Tables 8 and 9. \n \nTable 8: Comparison of model performance using dataset-wise \naveraging on the ETIS-LaribPolypDB dataset against the 34 filter \nDUCK-Net model and FCN-Transformer model using the DUCK-\nNet data partition. \n mDice mIoU mPrec mRec \nFCN-Transformer 91.63 84.55 96.33 87.36 \nDUCK-Net-34 93.54 87.88 93.09 94.00 \nDUCK-Net-34 \n(This work) 92.77 86.52 91.76 93.81 \nFCB-SwinV2 \nTransformer 95.03 90.54 94.73 95.34 \n \nTable 9: Comparison of model performance using image-wise \naveraging on the ETIS-LaribPolypDB against the 34 filter DUCK-\nNet model using the DUCK-Net data partition. \n mDice mIoU mPrec mRec \nDUCK-Net-34 \n(This work) 81.76 75.98 82.10 82.48 \nFCB-SwinV2 \nTransformer 91.70 85.40 91.93 92.10 \n \nThe generalizability performance of the FCB -SwinV2 \nTransformer (using dataset -wise averaging to allow for \ncomparisons against [8]) when trained on the Kvasir -SEG \ndataset using the DUCK-Net data partitions and evaluated on \nthe CVC -ClinicDB dataset and when trained on the \nCVC-ClinicDB dataset using the DUCK-Net data partitions \nand evaluated on the CVC-ClinicDB dataset are displayed in \nTable 10 and Table 11 respectively. Note that the same data \npartitions are used (rather than re-training using all available \ndata within a dataset) as this approach is used by other \nmodels within literature and hence allows fair comparison \nbetween models. \n \nTable 10: Comparison of model generalizability performance \nusing dataset-wise averaging when trained on the Kvasir-SEG \ndataset using the DUCK-Net data partitions and evaluated on the \nCVC-ClinicDB dataset against the 34 filter DUCK-Net model and \nFCN-Transformer model. \n mDice mIoU mPrec mRec \nFCN-Transformer 83.14 71.14 88.39 78.48 \nDUCK-Net-34 82.11 69.65 88.60 76.50 \nFCB-SwinV2 \nTransformer 88.11 78.74 91.19 85.22 \n \nTable 11: Comparison of model generalizability performance \nusing dataset-wise averaging when trained on the CVC-ClinicDB \ndataset using the DUCK-Net data partitions and evaluated on the \nKvasir-SEG dataset against the 34 filter DUCK-Net model and \nFCN-Transformer model. \n mDice mIoU mPrec mRec \nFCN-Transformer 88.00 78.58 96.59 80.82 \nDUCK-Net-34 82.51 70.23 77.40 88.34 \nFCB-SwinV2 \nTransformer 88.64 79.59 94.23 83.67 \n \nA visual comparison of mDice scores across all datasets and \ngeneralizability tests  using dataset -wise averaging for the \nDUCK-Net data partitions used is presented in Figure 8. This \nvisualization highlights that the FCB -SwinV2 Transformer \nconsistently achieves the highest mDice scores. \n \nAs demonstrated through Tables 4-9, t he FCB -SwinV2 \nTransformer consistently surpasses the DUCK-Net-34 and \nFCN-Transformer models with respect to mDice and mIoU \nscores across all datasets when using the DUCK-Net data \npartitions for both types of averaging techniques used.  \n \nAs evidenced in Tables 10-11, The FCB -SwinV2 \nTransformer also outperforms the previous state -of-the-art \nmodels FCN -Transformer and DUCK -Net on mDice and \nmIoU scores for both generalizability tests. This is an \nimportant result as the impact of random seed variations on \ndata partitioning are minimized and potential data leakage is \nFigure 8: \nFigure 8: Visual comparison of mDice scores across all datasets \nand generalizability tests using dataset-wise averaging for the \nDUCK-Net data partitions. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  12 \neliminated (due to the training and test data being from two \nseparate datasets/ distributions). This means that  these \ngeneralizability results provide a reliable evaluation of true \nrelative performance between models. \n \nWhilst maintaining high mDice and mIoU scores, the FCB -\nSwinV2 Transformer also typically achieves the highest \nmRec scores. In a clinical setting where the primary goal \nwould be to identify and map every potential polyp and \nensure nothing is missed, having high mRec scores (whilst \nmaintaining high mDice and mIoU scores) would be \ndesirable. \n \nAs expected, the comparison between dataset -wise \naveraging and image -wise averaging demonstrates that \nimage-wise averaging consistently produces more \nconservative results for both the 34 filter DUCK-Net and \nFCB-SwinV2 Transformer models. For the Kvasir -SEG \ndataset, the DUCK-Net model witnesses a marginal decrease \nin mDice and mIoU scores, namely 0.80% and 0.55% \nrespectively, compared to a 0.89% and 1.06% reduction for \nthe FCB -SwinV2 Transformer. A slightly larger impact  is \nseen for the CVC -ClinicDB dataset with the DUCK-Net \nmodel witnessing a 1.99% decrease for the mDice score and \n2.38% decrease for the mIoU score whilst for the FCB -\nSwinV2 Transformer there is a 1.57% decrease for the \nmDice score and 2.19% decrease for the mIoU score. For the \nETIS-LaribPolypDB dataset , the impact of the different \naveraging techniques is much more pronounced with the \nDUCK-Net model experiencing a sharp 11.01% and 10.54% \nfall in mDice and mIoU scores respectively whilst there is a \n3.33% and 5.14% downturn for the FCB -SwinV2 \nTransformer. It's postulated that this divergence could be \ncaused by considerable variations in relative polyp sizes \nwithin the ETIS -LaribPolypDB, as visualized in Figure 9. \nFigure 6 also serves as a reminder of how variations in \nrelative polyp sizes impact different averaging techniques. \nC. Random Data Partitioning Results \n \nTo generate further comparisons of relative model \nperformance, the FCB -SwinV2 Transformer is assessed \nusing random data partitions across the Kvasir -SEG, CVC-\nClinicDB, and ETIS-LaribPolypDB datasets with results \npresented in Tables 1 2-14. Results for the FCB -SwinV2 \nTransformer have been calculated using image -wise \naveraging to provide a more conservative estimate of \nperformance.  Where possible, comparisons are made \nagainst results reported in literature for recent high \nperforming models. However, it is unknown w hich \naveraging techniques have been used when reporting mean \nscores for these other models.  \n \nThe performance of the FCB -SwinV2 Transformer for the \nKvasir-SEG [11] dataset is reported in Table 12 for a \nrandom data partition. Comparisons to FCN-Transformer \nmodel performance from the original paper [7] and from \n[42] with the advanced data augmentation technique named \nâ€˜Spatially Exclusive Pasting â€™ (SEP) are provided.  Note that \nResults for the Meta -Polyp model reported in [48] \n(95.90mDice and 92.10mIoU) are not included in Table 1 2. \nThis is because the Meta -Polyp authors report results for \nthe Kvasir-SEG dataset after using a training dataset which \nmerged 900 images from Kvasir -SEG with 550 images \nfrom CVC-ClinicDB. \n \nTable 12: Comparison of model performance using image-wise \naveraging on the Kvasir-SEG dataset against using random data \npartitioning.  \n mDice mIoU mPrec mRec \nPraNet [22] 90.11 84.03 90.34 92.72 \nMSRF-Net [23] 92.17 89.14 96.66 91.98 \nPolyp2Seg [33] 92.90 88.20 - - \nESFPNet-L [30] 93.10 88.70 - - \nFCN-Transformer \n[7] 93.85 89.03 94.59 94.01 \nFCB-SwinV2 \nTransformer 94.04 89.49 93.95 95.16 \nFCN-Transformer \n+ SEP [42] 94.11 90.02 - - \n \nThe performance of the FCB -SwinV2 Transformer for the \nCVC-ClinicDB dataset is reported in Table 13 for a random \ndata partition. Comparisons to FCN -Transformer model \nperformance and other high performing models are \nincluded for comparison.  \n \nTable 13: Comparison of model performance using image-wise \naveraging on the CVC-ClinicDB dataset against using random \ndata partitioning.  \n mDice mIoU mPrec mRec \nPolyp2Seg [33] 92.90 88.10 - - \nPraNet [22] 93.58 88.67 93.70 93.88 \nMSRF-Net [23] 94.20 90.43 94.27 95.67 \nFCN-Transformer \n[7] 94.69 90.20 95.25 94.41 \nESFPNet-L [30] 94.90 90.70 - - \nFCB-SwinV2 \nTransformer 95.19 91.00 94.79 95.82 \n \nFigure 9: Demonstration of the large differences in relative \npolyp sizes contained within the ETIS-LaribPolypDB \ndataset [14]. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  13 \nThe performance of the FCB -SwinV2 Transformer when \ntrained and evaluated using the ETIS -LaribPolypDB. The \nresults are displayed in Table 14. \n \nTable 14: Comparison of model performance using image-wise \naveraging on the ETIS-LaribPolypDB using a random data \npartition. \n mDice mIoU mPrec mRec \nPraNet [22] 62.80 56.70 - - \nPolyp2Seg [33] 82.00 73.80 - - \nESFPNet-L [30] 82.30 74.80 - - \nFCB-SwinV2 \nTransformer 91.88 85.63 92.97 91.95 \n \nThe generalizability performance of the FCB -SwinV2 \nTransformer (using image -wise averaging to ensure \nconservative values) when trained on the Kvasir-SEG dataset \nusing a random data partition and evaluated on the CVC -\nClinicDB dataset and when trained on the CVC -ClinicDB \ndataset using a random data partition and evaluated on the \nKvasir-SEG dataset are displayed in Table 15 and Table 16 \nrespectively. Note again that the same data partitions are \nused (rather than re-training using all available data within a \ndataset) as this approach is used by other models within \nliterature and hence allows fair comparison between models. \n \nTable 15: Comparison of the model generalizability performance \nusing image-wise averaging when trained on the Kvasir-SEG \ndataset using a random data partition and evaluated on the \nCVC-ClinicDB dataset. \n mDice mIoU mPrec mRec \nPraNet [22] 79.12 71.19 81.52 83.16 \nMSRF-Net [23] 79.21 64.98 70.00 90.01 \nFCN-Transformer \n[7] 87.35 80.38 89.95 88.76 \nFCB-SwinV2 \nTransformer 87.77 80.78 89.83 89.29 \n \nTable 16: Comparison of the model generalizability performance \nusing image-wise averaging when trained on the CVC-ClinicDB \ndataset using a random data partition and evaluated on the \nKvasir-SEG dataset against other high performing models. \n mDice mIoU mPrec mRec \nPraNet [22] 79.50 70.73 76.87 90.50 \nMSRF-Net [23] 75.75 63.37 83.14 71.97 \nFCN-Transformer \n[7] 88.48 82.14 93.54 87.54 \nFCB-SwinV2 \nTransformer 89.35 83.34 94.26 88.15 \n \nA visual comparison of mDice scores across all datasets and \ngeneralizability tests using image -wise averaging for the \nrandom data partitions is presented in Figure 10. This \nvisualization highlights that the FCB -SwinV2 Transformer \nonce again consistently achieves the highest mDice scores. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe FCB-SwinV2 Transformer achieves the highest mDice \nand mIoU scores for both the CVC -ClinicDB and ETIS -\nLaribPolypDB datasets. For the Kvasir-SEG dataset, the FCB-\nSwinV2 Transformer achieves the second highest scores for \nthe mDice and mIoU metrics  behind only the FCN -\nTransformer trained using the advanced SEP data \naugmentation technique. When compared to baseline models \n(i.e. those only using standard data augmentation techniques) \nthe FCB-SwinV2 Transformer achieves the highest scores for \nthe mDice and mIoU metrics.  \n \nThe FCB-SwinV2 Transformer also outperforms all previous \nhigh performing models on mDice and mIoU scores for both \ngeneralizability tests conducted using image-wise averaging. \nOnce again, the importance of this result should be stressed as \nthe effects of potential data leakage are eliminated. For future \ngeneralizability tests the impact of random seed variations \ncould be reduced further by training using 90% or 95% of \nimages within a dataset.  Once again, the FCB -SwinV2 \nTransformer typically achieves the highest mRec scores \n(when still maintaining high mDice and mIoU scores) which \ncould be desirable in clinical settings where the main aim is to \nidentify and map every polyp present. \n \nWhen comparing results for image -wise averaging for each \ndataset, model performance varies noticeably between the \nFigure 10: Visual comparison of mDice scores across all datasets \nand generalizability tests using image-wise averaging for random \ndata partitioning. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  14 \nDUCK-Net and random data partitions. This demonstrates the \nissue of noticeable performance changes due to differences in \ndata partitioning. Since the FCB -SwinV2 Transformer \noutperforms other models across each dataset, using both the \nDUCK-Net and random data partitions, this provides further \nconfidence that the model achieves state-of-the-art results. \nD. CVC-ClinicDB No Data Leakage Partitioning \nResults \n \nThe results for the FCB -SwinV2 Transformer for the 5 \ntraining/validation/test data partitions with no data leakage are \nreported in Table 17 and displayed in Figure 11. The mean \nscores for the partitions with no data leakage were calculated \nusing batch-wise averaging. \n \n Table 17: FCB-SwinV2 Transformer model performance on CVC-\nClinicDB dataset using training/validation/test data partitions \nwith no data leakage.  \n \nWhen no data leakage occurs the performance of the FCB-\nSwinV2 Transformer model, whilst still strong, drops across \nall metrics for each of the five partitions.  It is also expected \nthat there would be a slight reduction in scores if image-wise \naveraging was used. For partitions 1 and 5 the performance \ndrop is larger. This is likely due to the test images within \npartitions 1 and 5 being unusual when compared to the whole \ndataset. For example, partition 1 contains a video sequence \nwhere multiple small polyps are present which is unique when \ncompared to other video sequences within the CVC-ClinicDB \ndataset. The significant reduction in scores demonstrates the \nimpact of data leakage artificially increasing model \nperformance for datasets like CVC -ClinicDB and ETIS -\nLaribPolypDB. It is extremely likely that this behavior would \nbe replicated by any other deep learning model given that the \ntraining and testing partitions no longer contain images of the \nsame polyps.  \nE. Qualitative Mask Comparisons \n \nA visual comparison of predictions made by the FCB -\nSwinV2 Transformer for an image from the Kvasir -SEG \ndataset when trained on Kvasir-SEG and when trained using \nthe CVC -ClinicDB dataset (i.e. generalizability test) are \nprovided in Figure 12. \nQualitative inspection of the predicted binary segmentation \nmaps generated when trained using the CVC -ClinicDB \ndataset show that the model generalizes well for regular \npolyps but suffers a performance drop for large, irregular \npolyps within the Kvasir -SEG dataset (which are \nconsiderably different to any of the polyps within the CVC -\nClinicDB dataset). Another interesting finding of the visual \ninspection is that some ground truth maps of the Kvasir-SEG \ndataset contain sharp edges (highlighted using red arrows  \nwithin Figure 12) when the polyp contained within the input \nimage appears to have smooth edges. The segmentation \npredictions made by the FCB-SwinV2 Transformer typically \ncontain smoother edges. This may suggest that predictions \nmade by the FCB -SwinV2 Transformer (and othe r recent \nstate-of-the-art models) may be approaching the maximum \nachievable performance when trained and evaluated on \navailable polyp segmentation datasets  particularly when \nconsidering intra - and inter -observer variabilities in \ngenerating ground truth segmentation masks. This further \nhighlights the importance of generalizability tests. \n \nA comparison of predictions made by the FCB -SwinV2 \nTransformer for images from the test set of the Kvasir -SEG \ndataset when trained and evaluated using the DUCK -Net \nPartition \nNo. mDice mIoU mPrec mRec \n1 83.16 73.26 87.81 81.95 \n2 85.61 78.03 91.23 81.74 \n3 92.50 86.35 90.91 94.96 \n4 93.65 88.26 95.10 92.63 \n5 81.86 71.17 91.04 78.23 \nFigure 11: FCB-SwinV2 Transformer model performance on the \nCVC-ClinicDB dataset using training/validation/test data partitions \nwith no data leakage. \nFigure 12: Comparisons of predictions made by the model for the \nKvasir-SEG dataset when trained using the Kvasir-SEG and when \ntrained using the CVC-ClinicDB dataset. Red arrows highlight \nsharp edges found within ground truth segmentation maps which \ndo not appear to match polyp edges within the image.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  15 \npartition are displayed alongside predictions made by the \nDUCK-Net [8] and FCN-Transformer [7] in Figure 13.  \n \nQualitative inspection of the predicted binary segmentation \nmaps generated shows that each model performs strongly.  \nAcross the three images presented the FCB-SwinV2 \nTransformer and DUCK -Net model produce marginally \nmore accurate segmentation maps than the FCN -\nTransformer.  \n \nVisual comparisons of predictions made by the FCB-SwinV2 \nTransformer for a uniquely shaped polyp from the CVC -\nClinicDB dataset are provided in Figure 14.  \nThese predictions are from when the model had been trained \nusing the CVC-ClinicDB random data partition and the CVC-\nClinicDB data partition which ensured no data leakage. \nQualitative inspection of the predicted binary segmentation \nmaps generated when using the random data partition shows \nthat it matches the ground truth extremely well. When \ncompared to the segmentation map generated ensuring no data \nleakage occurred (partition 4) we see a large drop in \nperformance. This strongly demonstrates the artificially high \nperformance when using a random data partition as, although \nthe model has been trained and evaluated on different images, \nboth training and test datasets contain images of the same \npolyp.  \nF. Evaluation of Model Uncertainty \n \nThe average results of the 100 MC dropout runs conducted for \neach dataset using the DUCK-Net data partitions and random \ndata partitions are presented in  Table 18 and Table 1 9 \nrespectively.  \n \n Table 18: Mean and standard deviation (displayed using Â± \nnotation) of the 100 MC dropout conducted for each dataset using \nthe DUCK-Net data partitions. Image-wise averaging has been \nused during evaluation.  \nMetric Kvasir-SEG CVC-ClinicDB ETIS-LaribDB \nmDice 93.46 Â± 0.32 91.72 Â± 0.45 84.43 Â± 1.61 \nmIoU 89.00 Â± 0.44 86.08 Â± 0.54 76.18 Â± 1.47 \nmPrec 94.40 Â± 0.42 92.21 Â± 0.56 81.83 Â± 1.51 \nmRec 94.32 Â± 0.23 92.62 Â± 0.43 90.20 Â± 2.07 \n \nTable 19: Mean and standard deviation (displayed using Â± \nnotation) of the 100 MC dropout runs conducted for each dataset \nusing random data partitioning. Image-wise averaging has been \nused during evaluation. \n \nThe average mDice and average mIoU scores of the 100 MC \nDropout runs conducted for each dataset using the DUCK-Net \ndata partitions and random data partitions are visualized as \nboxplots in Figure 15 and Figure 17 respectively. The single \ndeterministic run values for each dataset partition (see Tables \n5, 12, 7, 13, 9 and 14) are also displayed using â€˜+â€™ markers. \nFor both the DUCK -Net and random data partitions the \nKvasir-SEG dataset MC Dropout results exhibited high \nperformance. The average mDice and average mIoU  scores \nfor the DUCK-Net and random partitions represented a less \nthan 2% drop in performance when compared to the single \ndeterministic run values (dropout deactivated during \nevaluation) reported in Table 5 and Table 12 respectively. The \nstandard deviations across all metrics are small (< 0.5%), \nsuggesting that the model's performance is stable across \ndifferent Kvasir-SEG runs. \n \nFor both the DUCK-Net and random data partitions, the CVC-\nClinicDB dataset MC Dropout results again exhibited high \nperformance. The average mDice and average mIoU scores \nfor the DUCK-Net and random partitions represented a less \nthan 2.5% drop in performance when compared to the single \ndeterministic values reported in Table 7 and Table 13 \nrespectively. With standard deviations still less than 0.6%, the \nmodel demonstrates stable performance across different CVC-\nClinicDB runs. However, potential data leakage ( due to \nMetric Kvasir-SEG CVC-ClinicDB ETIS-LaribDB \nmDice 92.73 Â± 0.39 94.29 Â± 0.35 80.82 Â± 1.62 \nmIoU 87.59 Â± 0.45 89.62 Â± 0.48 71.53 Â± 1.88 \nmPrec 92.92 Â± 0.42 93.70 Â± 0.46 76.19 Â± 1.91 \nmRec 94.07 Â± 0.32 95.43 Â± 0.24 91.76 Â± 1.48 \nFigure 14: Comparisons of predictions made by the model for the \nCVC-ClinicDB dataset when trained using the random data \npartition and when trained using a data partition which ensured \nno data leakage. When using a random data partition, the model \nhas been trained and evaluated on images of the same polyp from \na video sequence resulting in artificially high performance. \nFigure 13: Predictions made by the FCB-SwinV2 Transformer for \nimages from the test set of the Kvasir-SEG dataset when trained \nand evaluated using the DUCK-Net data partitions.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  16 \nrandomness in the data partitioning process) in the CVC -\nClinicDB dataset warrants caution in interpreting these results. \n \nThe ETIS -LaribDB dataset shows a noticeable drop in \nperformance for both the DUCK -Net and random data \npartitions compared to the other two datasets. Average metric \nscores fall by as much as 16% when compared to the single \ndeterministic values reported  in Table 9 and Table 14 \nrespectively. The larger standard deviations (>1.5%) suggest \na more variable performance across different runs. The \nreduced dataset size (196 images) likely contributes to this \nincreased variability and reduced performance. It is possible \nthat the smaller dataset size leads to the model overfitting and \nwhen dropout is applied the removal of over -specialized \nneurons significantly impacts performance. \n \nThe boxplots presented in Figure 15 and Figure 16 also \nprovide a useful visualization of the impact of dataset \npartitioning on both the deterministic and MC Dropout results \nas for each dataset model performance varies noticeably \nbetween the DUCK-Net and random data partitions. \n \n \n \n \n \n \n \nVI. Conclusion \n \nThis paper proposes a novel deep learning model for colon \npolyp segmentation called the FCB-SwinV2 Transformer. The \nperformance of this model has been extensively investigated \nthrough rigorous quantitative and qualitative comparison \nagainst previous state-of-the-art models reported in literature. \nIn addition, ablation studies and epistemic uncertainty analysis \n(estimated by MC dropout) were conducted to provide further \ninsights into the performance of the FCB -SwinV2 \nTransformer. The FCB -SwinV2 Transformer a chieved the \nhighest mDice and mIoU scores in each of the respective test \nsets used when compared to baseline models reported in \nliterature. In addition, generalizability tests which followed \nthe same methodology reported in literature [7] [8] [22] [23], \nwere conducted with results being compared against previous \nstate-of-the-art models. These generalizability tests showed \nthat the FCB -SwinV2 Transformer outperformed previous \nmodels on mDice and mIoU scores. These results demonstrate \nthe state -of-the-art performance of the FCB -SwinV2 \nTransformer and its improved adaptability and applicability to \ndata outside of the training distribution.  \n \nThe critical importance of dataset partitioning and averaging \nmethodologies used to calculate performance metrics have \nalso been demonstrated experimentally. It has been observed \nFigure 15: Boxplot visualization of mDice score statistics from the \n100 MC dropout runs conducted for each dataset using the \nDUCK-Net and random data partitions. \nFigure 16: Boxplot visualization of mIoU score statistics from the \n100 MC dropout runs conducted for each dataset using the \nDUCK-Net and random data partitions. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  17 \nthat data leakage can artificially increase model performance \nand that general data partitioning differences (even when there \nis no data leakage) can cause fluctuations in model \nperformance. It has also been shown that when calculating \nperformance metrics,  image-wise averaging consistently \nyielded lower mean scores than dataset-wise averaging and is \ntherefore believed to capture true individual polyp \nsegmentation performance. The colonoscopy research \ncommunity could therefore benefit from expert clinicians \ndefining standardized reporting metrics and creating \nstandardized K-Fold cross validation data splits on popular \ncolonoscopy benchmarking datasets. \n \nSome further incremental improvements to the FCB-SwinV2 \nTransformer and training process could also be made which \nmay enhance model performance. Examples include: using \nmore advanced data augmentation techniques; replacing the \nFCB branch with an ImageNet p re-trained FCN model or \nDUCK-Net based architecture [8], and larger ImageNet pre-\ntrained SwinV2 encoders. \nAcknowledgements \nData access statement: The study reported in this article has \nbeen supported by existing openly available datasets, namely \nKvasir-SEG, CVC-ClinicDB, and ETIS-LaribPolypDB. \nREFERENCES \n \n[1]  World Health Organization, \"Colorectal cancer,\" July \n2023. [Online]. Available: \nhttps://www.who.int/news-room/fact-\nsheets/detail/colorectal-\ncancer#:~:text=Colon%20cancer%20is%20the%20se\ncond,and%20mortality%20rates%20were%20observe\nd. [Accessed December 2023]. \n[2]  A.M. Leufkens, M.G.H. van Oijen, F.P. Vleggaar & \nP.D. Siersema, \"Factors influencing the miss rate of \npolyps in a back-to-back colonoscopy study,\" \nEndoscopy, vol. 44, no. 5, pp. 470-475, 2012.  \n[3]  N.H. Kim, Y.S. Jung, W.S. Jeong, H.J. Yang, S.K. \nPark. K. Choi & D.I. Park, \"Miss rate of colorectal \nneoplastic polyps and risk factors for missed polyps \nin consecutive colonoscopies,\" Intestinal Research, \nvol. 15, no. 3, pp. 411-418, 2017.  \n[4]  J. Lee, S.W. Park, Y.S. Kim, K.J. Lee, H.S, P.H. \nSong, W.J. Yoon & J.S. Moon, \"Risk factors of \nmissed colorectal lesions after colonoscopy,\" \nMedicine, vol. 96, no. 27, 2017.  \n[5]  D.A. Corley, \"Adenoma Detection Rate and Risk of \nColorectal Cancer and Death,\" New England Journal \nof Medicine, vol. 370, pp. 1298-1306, 2014.  \n[6]  M.J. Whitson, C.A. Bodian, J. Aisenberg & L.B. \nCohen, \"Is production pressure jeopardizing the \nquality of colonoscopy? A survey of U.S. \nendoscopists' practices and perceptions,\" \nGastrointestinal Endoscopy, vol. 75, no. 3, pp. 641-\n648, 2012.  \n[7]  E. Sanderson & B.J. Matuszewski, \"FCN-\nTransformer Feature Fusion for Polyp \nSegmentation,\" in Medical Image Understanding and \nAnalysis (MIUA), 2022.  \n[8]  RG. Dumitru, D. Peteleaza & C. Craciun, \"Using \nDUCK-Net for polyp image segmentation,\" Nature \nScientific Reports, vol. 13, 2023.  \n[9]  W. Wang, E. Xie, X. Li, D-P. Fan, K. Song, D. \nLiang, T. Lu, P. Luo & LShao, \"PVT v2: Improved \nbaselines with Pyramid Vision Transformer,\" \nComputational Visual Media, vol. 8, no. 3, pp. 415-\n424, 2022.  \n[10]  Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. \nNing, Y. Cao, Z. Zhang, Li Dong, F. Wei & B. Guo, \n\"Swin Transformer V2: Scaling Up Capacity and \nResolution,\" in Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern \nRecognition, 2022.  \n[11]  D. Jha, P.H. Smedsrud, M.A. Riegler, P. Halvorsen, \nT. de Lange, D. Johansen & H.D. Johansen, \"Kvasir-\nSEG: A Segmented Polyp Dataset,\" in International \nConference on Multimedia Modeling, 2020.  \n[12]  J. Bernal, F.J. Sanchez, G. Fernandez-Esparrach, D. \nGil, C. RodrÃ­guez & F. VilariÃ±o,, \"WM-DOVA maps \nfor accurate polyp highlighting in colonoscopy: \nValidation vs. saliency maps from physicians,\" \nComputerized Medical Imaging and Graphics, vol. \n43, pp. 99-111, 2015.  \n[13]  J. Bernal et al, \"Comparative validation of polyp \ndetection methods in video colonoscopy: Results \nfrom the MICCAI 2015 endoscopic vision \nchallenge,\" IEEE Trans. Med. Imaging, vol. 36, pp. \n1231-1249, 2017.  \n[14]  O. Ronneberger, P. Fischer, T. Brox, N. Navab, J. \nHornegger, W.M. Wells & A.J. Frangi, \"U-Net: \nConvolutional Networks for Biomedical Image \nSegmentation,\" in Medical Image Computing and \nComputer-Assisted Intervention â€“ MICCAI, 2015.  \n[15]  H.A. Qadir, Y. Shin, J. Solhusvik, J. Bergsland, L. \nAabakken & I. Balasingham , \"Polyp Detection and \nSegmentation using Mask R-CNN: Does a Deeper \nFeature Extractor CNN Always Perform Better?,\" in \n13th International Symposium on Medical \nInformation and Communication Technology \n(ISMICT), 2019.  \n[16]  J. Debesh, M.A. Riegler, D. Johansen, P. Halvorsen \n& H.D. Johansen, \"Double U-Net: A deep \nconvolutional neural network for medical image \nsegmentation,\" in IEEE 33rd International \nSymposium on Computer-Based Medical Systems, \n2020.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  18 \n[17]  M. Hwang, D. Wang, X-X. Kong, Z. Wang, J. Li, W-\nC. Jiang, K-S. Hwang, K. Ding, \"An automated \ndetection system for colonoscopy images using a dual \nencoder-decoder model,\" Computerized Medical \nImagaing and Graphics, vol. 84, 202.  \n[18]  A.O. Ige, N.K. Tomar, F.O. Aranuwa, O. Oriola, \nA.O. Akingbesote, M.H. Noor, M. Mazzara & B.S. \nAribisala, \"ConvSegNet: Automated Polyp \nSegmentation From Colonoscopy Using Context \nFeature Refinement With Multiple Convolutional \nKernel Sizes,\" IEEE Access, vol. 11, pp. 16142-\n16155, 2023.  \n[19]  N.S. An, P.N. Lan, D.V. Hang, T.Q. Trung, N.T. \nThuy & D.V. Sang, \"BlazeNeo: Blazing Fast Polyp \nSegmentation and Neoplasm Detection,\" IEEE \nAccess, vol. 10, pp. 43669-43684, 2022.  \n[20]  D. Jha, P.H. Smedsrud, M.A. Riegler, D. Johansen, \nT. de Lange, P. Halvorsen & H.J. Dagenborg, \n\"ResUNet++: An Advanced Architecture for Medical \nImage Segmentation,\" in 21st IEEE International \nSymposium on Multimedia, 2019.  \n[21]  Y. Guo, J. Bernal, & B. Matuszewski, \"Polyp \nSegmentation with Fully Convolutional Deep Neural \nNetworks - Extended Evaluation Study,\" Journal of \nImaging, vol. 6, no. 7, 13 July 2020.  \n[22]  D-P Fan, G-P Ji, T. Zhou, G. Chen, H. Fu, J. Shen & \nL. Shao , \"PraNet: Parallel Reverse Attention \nNetwork for Polyp Segmentation,\" in International \nConference on Medical Image Computing and \nComputer-Assisted Intervention, 2020.  \n[23]  A. Srivastava, D. Jha, S, Chanda, U. Pal, H.D. \nJohansen, D. Johansen, M.A. Riegler & S. Ali, \n\"MSRF-Net: A Multi-Scale Residual Fusion Network \nfor Biomedical Image Segmentation,\" IEEE Journal \nof Biomedical and Health Informatics, vol. 26, no. 5, \npp. 2252-2263, 2022.  \n[24]  A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. \nWeissenborn, X. Zhai, T. Unterthiner, M. Dehghani, \nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit & N. \nHoulsby, \"An Image is Worth 16x16 Words: \nTransformers for Image Recognition at Scale,\" in \nInternational Conference on Learning \nRepresentations, Virtual, 2021.  \n[25]  E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. \nAlvarez & P. Luo, \"SegFormer: Simple and Efficient \nDesign for Semantic Segmentation with \nTransformers,\" in Proceedings of the Advances in \nNeural Information Processing Systems 34 , 2021.  \n[26]  B. Dong, W. Wang, D-P. Fan, J. Li, H. Fu & L. Shao, \n\"Polyp-PVT: Polyp Segmentation with Pyramid \nVision Transformers,\" CAAI Artificial Intelligence \nResearch, vol. 2, 2023.  \n[27]  Q. Guo, X. Fang, L. Wang & E. Zhang, \"Polyp \nSegmentation of Colonoscopy Images by Exploring \nthe Uncertain Areas,\" IEEE Access, vol. 10, pp. \n52971-52981, 2022.  \n[28]  N.K. Tomar, D. Jha, S. Ali, H.D. Johansen, D. \nJohansen, M.A. Riegler & P. Halvorsen, \"DDANet: \nDual Decoder Attention Network for Automatic \nPolyp Segmentation,\" in International Conference on \nPattern Recognition: International Workshops and \nChallenges, 2021.  \n[29]  N. T. Duc, N. T. Oanh, N. T. Thuy, T. M. Triet & V. \nS. Dinh, \"ColonFormer: An Efficient Transformer \nBased Method for Colon Polyp Segmentation,\" IEEE \nAccess, vol. 10, pp. 80575-80586, 2022.  \n[30]  Q. Chang, D. Ahmad, J. Toth, R. Bascom & W.E. \nHiggins, \"ESFPNet: efficient deep learning \narchitecture for real-time lesion segmentation in \nautofluorescence bronchoscopic video,\" Medical \nImaging 2023: Biomedical Applications in \nMolecular, Structural, and Functional Imaging;, pp. \nProceedings Volume 12468, Medical Imaging 2023: \nBiomedical Applications in Molecular, Structural, \nand Functional Imaging, 2023.  \n[31]  Y. Huang, D. Tan, Y. Zhang, X. Li & K. Hu, \n\"TransMixer: A Hybrid Transformer and CNN \nArchitecture for Polyp Segmentation,\" in IEEE \nInternational Conference on Bioinformatics and \nBiomedicine, 2022 IEEE International Conference on \nBioinformatics and Biomedicine.  \n[32]  H. Lai, Y. Luo, G. Zhang, X. Shen, B. Li, & J. Lu,, \n\"Toward accurate polyp segmentation with cascade \nboundary-guided attention.,\" The Visual Computer, \nvol. 39, no. 4, pp. 1453-1469, 2023.  \n[33]  V. Mandujano-Cornejo & J. Montoya-Zegarra, \n\"Polyp2Seg: Improved Polyp Segmentationwith \nVision Transformer,\" in Medical Image \nUnderstanding and Analysis (MIUA)., 2022.  \n[34]  Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. \nLin & B. Guo, \"Swin Transformer: Hierarchical \nVision Transformer Using Shifted Windows,\" in \nProceedings of the IEEE/CVF International \nConference on Computer Vision, 2021.  \n[35]  D. Hendrycks & K. Gimpel, \"Gaussian Error Linear \nUnits (GELUs),\" arXiv, 2016.  \n[36]  \"Semantic Understanding of Scenes Through the \nADE20K Dataset,\" International Journal of \nComputer Vision, vol. 127, p. 302â€“321, 2019.  \n[37]  M. Abe, \"Swin V2 Unet/Upernet,\" Kaggle, 2022. \n[Online]. Available: \nhttps://www.kaggle.com/code/abebe9849/swin-v2-\nunet-upernet. [Accessed January 2023]. \n[38]  R. Wightman, \"PyTorch Image Models,\" in GitHub \nRepository, 2019.  \n[39]  A.G. Roy, N. Navab & C. Wachinger, \"Concurrent \nSpatial and Channel â€˜Squeeze & Excitationâ€™ in Fully \nConvolutional Networks,\" in International \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2023  19 \nConference on Medical Image Computing and \nComputer-Assisted Intervention, 2018.  \n[40]  P. Iakubovskii, \"Segmentation Models Pytorch,\" \nGitHub Repository, 2019.  \n[41]  J. Deng, W. Dong, R. Socher, L. Li, K. Li & Li Fei-\nFei,, \"ImageNet: A large-scale hierarchical image \ndatabase,\" in IEEE Conference on Computer Vision \nand Pattern Recognition, 2009.  \n[42]  L. Zhou, \"Spatially Exclusive Pasting: A general data \naugmentation technique for the polyp segmentation,\" \nin International Joint Conference on Neural \nNetworks (IJCNN), 2023.  \n[43]  I. Loshchilov & F. Hutter, \"Decoupled Weight Decay \nRegularization,\" in International Conference on \nLearning Representations, 2019.  \n[44]  N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever \n& R. Salakhutdinov, \"Dropout: A simple way to \nprevent neural networks from overfitting,\" Journal of \nMachine Learning Research, vol. 15, pp. 1929-1958, \n2014.  \n[45]  Y. Gal & Z. Ghahramani, \"Dropout as a Bayesian \nApproximation: Representing Model Uncertainty in \nDeep Learning,\" in Proceedings of The 33rd \nInternational Conference on Machine Learning, \n2016.  \n[46]  M. Combalia, F. Hueto, S. Puig, J. Malvehy & V. \nVilaplana, \"Uncertainty Estimation in Deep Neural \nNetworks for Dermoscopic Image Classification,\" in \nComputer Vision and Patern Recoginition, 2020.  \n[47]  S. Woo, J. Park, J-Y. Lee & IS Kweon, \"CBAM: \nConvolutional Block Attention Module,\" in \nEuropean Conference on Computer Vision, 2018.  \n[48]  Q. Trinh, \"Meta-Polyp: A Baseline for Efficient \nPolyp Segmentation,\" in IEEE 36th International \nSymposium on Computer-Based Medical Systems \n(CBMS), Aquila, Italy, 2023.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2017 \nKERR FITZGERALD received a BSc \nin Physics (Hons) from Durham \nUniversity (UK) in 2013 where he was \nalso awarded the â€˜Andy Brinkman \nPhysics Prizeâ€™. From 2014 to 2020, he \nworked as a Fuel Performance Scientist at \nthe UK National Nuclear Laboratory and \nwas selected to serve as a UK \nrepresentative for the Jules -Horowitz \nReactor Fuel Working Group. Kerr is now \na final -year Ph.D. candidate within the \nComputer Vision and Machine Learning (CVML) Group at the University \nof Central Lancashire. His research focuses on AI applications for medical \nimage analysis, specializing in colonoscopy image classification and \nsegmentation. \n \n \nJORGE BERNAL  received the Ph.D. \ndegree in Computer Science from \nUniversitat AutÃ²noma de Barcelona in \n2012. He is currently an Associate \nProfessor at Computer Science \nDepartment at Universitat AutÃ²noma de \nBarcelona and an Associated Researcher \nat Computer Vision Center , where he is \nthe leader of the Image Sequence \nEvaluation lab. He has participated in 9 \nresearch projects, leading 3 of them, with \nfunding secured from the Spanish Government and the Catalan \nGovernment. Currently he is the Principal Investigat or of the Spanish \nGovernment Funded Project ALETHEIA â€˜Zero Forgetting in Neural \nNetworks: Continual Learning with Anomaly Detection in Image \nSequencesâ€™ and the coordinator of the first Spanish network devoted to the \ndevelopment of AI systems for colonoscopy, also funded by the Spanish \nGovernment. He has published over 20 research papers, won several awards \nfor his research, and supervised 3 PhDs to succe ssful completion. His \nresearch interests include computer vision and AI (machine learning) within \nareas of healthcare technologies, segmentation, object characterization, and \nscene understanding. \n \nAYMERIC HISTACE  is currently a \nProfesseur des UniversitÃ©s at ENSEA, \nFrench graduated School of Engineering \n(Bac+5). He is Deputy Director of the \nSchool, and Head of Research, Innovation \nand Partnerships. Aymeric does research \nin Computer Vision, Signal and Image \nProcessing in interaction with Natural \nScience, Engineering, Medicine, and \nInformation Science at ETIS lab (UMR \n8051, ENSEA, UCP, CNRS). He is head \nof the CELL team which activities is mostly dedicated to Smart, Reliable, \nand Reconfigurable Embedded Systems for va rious domains. He has co -\nauthored more than 150 papers (conferences and journals) in the domain of \nComputer Vision mainly. The current flagship projects he is working on are \nSmart Videocoloscopy, M2-SKAN (Non-Invasive Micro-Vascular Network \nSegmentation fo r early diagnostic of Type II diabetes), INSECTS \n(Innovation for Automatic Recognition of Blood -Sucking Insects), and \nHYPER-EYE dedicated to Machine Learning for non-conventional sensors \n(event-based camera). \n \n \n \nBOGDAN J. MATUSZEWSKI \n(Member, IEEE) received the Ph.D. \ndegree in electronic engineering from the \nWrocÅ‚aw University of Science and \nTechnology, Poland, in 1996. He is \ncurrently a Professor in Computer Vision, \nthe Deputy Director of the Research \nCentre in Engineering, and the Head of the \nComputer Vision and Machine Learning \n(CVML) Group, University of Central \nLancashire, Preston, U.K. He has \nparticipated in 24 research projects, leading 11 of them, with funding \nsecured from the U.K. Research Councils, EU, and industry. Most recently, \nhe has been the Principal Investigator of the Science and Technology \nFacilities Council CDN+ funded â€˜â€˜Machine Learning System for Decision \nSupport and Computational Automation of Early Cancer Detection and \nCategorization in Colonoscopy (AIdDeCo)â€™â€™ project. He has published over \n150 research papers, won several awards for his research, and supervised 19 \nPhDs to successful completion. His research interests include computer \nvision, data science, and AI (machine learning) within area s of imaging, \nhealthcare technologies, digital engineering, deformation modelling, \nsegmentation, registration, object characterization, and 3D scene \nreconstruction and understanding. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376228\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5211237072944641
    },
    {
      "name": "Image segmentation",
      "score": 0.44052568078041077
    },
    {
      "name": "Transformer",
      "score": 0.43085721135139465
    },
    {
      "name": "Segmentation",
      "score": 0.39560747146606445
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33835142850875854
    },
    {
      "name": "Computer vision",
      "score": 0.32641565799713135
    },
    {
      "name": "Electrical engineering",
      "score": 0.11437827348709106
    },
    {
      "name": "Voltage",
      "score": 0.09409049153327942
    },
    {
      "name": "Engineering",
      "score": 0.08376404643058777
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185852735",
      "name": "University of Lancashire",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4387153096",
      "name": "Computer Vision Center",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210142324",
      "name": "CY Cergy Paris UniversitÃ©",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I86175216",
      "name": "Ã‰cole Nationale SupÃ©rieure de l'Ã‰lectronique et de ses Applications",
      "country": "FR"
    }
  ]
}