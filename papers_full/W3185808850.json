{
    "title": "Alpha at SemEval-2021 Task 6: Transformer Based Propaganda Classification",
    "url": "https://openalex.org/W3185808850",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5112395293",
            "name": "Zhida Feng",
            "affiliations": [
                "Baidu (China)",
                "Wuhan University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5110779051",
            "name": "Jiji Tang",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5100721394",
            "name": "Jiaxiang Liu",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5064465647",
            "name": "Weichong Yin",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5005049423",
            "name": "Shikun Feng",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5101870255",
            "name": "Yu Sun",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5100379203",
            "name": "Li Chen",
            "affiliations": [
                "Baidu (China)",
                "Wuhan University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2975501350",
        "https://openalex.org/W3035688398",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3038476992",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3113763975",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3029825586",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W3115862112",
        "https://openalex.org/W3113529478",
        "https://openalex.org/W1686810756"
    ],
    "abstract": "This paper describes our system participated in Task 6 of SemEval-2021: the task focuses on multimodal propaganda technique classification and it aims to classify given image and text into 22 classes. In this paper, we propose to use transformer based architecture to fuse the clues from both image and text. We explore two branches of techniques including fine-tuning the text pretrained transformer with extended visual features, and fine-tuning the multimodal pretrained transformers. For the visual features, we have tested both grid features based on ResNet and salient region features from pretrained object detector. Among the pretrained multimodal transformers, we choose ERNIE-ViL, a two-steam cross-attended transformers pretrained on large scale image-caption aligned data. Fine-tuing ERNIE-ViL for our task produce a better performance due to general joint multimodal representation for text and image learned by ERNIE-ViL. Besides, as the distribution of the classification labels is very unbalanced, we also make a further attempt on the loss function and the experiment result shows that focal loss would perform better than cross entropy loss. Last we have won first for subtask C in the final competition.",
    "full_text": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 99–104\nBangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics\n99\nAlpha at SemEval-2021 Task 6: Transformer Based Propaganda\nClassiﬁcation\nZhida Feng1,2 ∗, Jiji Tang1 ∗, Jiaxiang Liu1, Weichong Yin1, Shikun Feng1, Yu Sun1, Li Chen2\n1Baidu Inc., Beijing, China\n2Wuhan University of Science and Technology, China\n{fengzhida, tangjiji, liujiaxiang, yinweichong, fengshikun01, sunyu02}@baidu.com\nchenli@wust.edu.cn\nAbstract\nThis paper describes our system participated\nin Task 6 of SemEval-2021: this task focuses\non multimodal propaganda technique classi-\nﬁcation and it aims to classify given image\nand text into 22 classes. In this paper, we\npropose to use transformer-based (Vaswani\net al., 2017) architecture to fuse the clues\nfrom both image and text. We explore two\nbranches of techniques including ﬁne-tuning\nthe text pre-trained transformer with extended\nvisual features and ﬁne-tuning the multimodal\npre-trained transformers. For the visual fea-\ntures, we experiment with both grid features\nextracted from ResNet(He et al., 2016) net-\nwork and salient region features from a pre-\ntrained object detector. Among the pre-trained\nmultimodal transformers, we choose ERNIE-\nViL (Yu et al., 2020), a two-steam cross-\nattended transformers model pre-trained on\nlarge-scale image-caption aligned data. Fine-\ntuning ERNIE-ViL for our task produces a\nbetter performance due to general joint mul-\ntimodal representation for text and image\nlearned by ERNIE-ViL. Besides, as the distri-\nbution of the classiﬁcation labels is extremely\nunbalanced, we also make a further attempt\non the loss function and the experiment re-\nsults show that focal loss would perform better\nthan cross-entropy loss. Lastly, we ranked ﬁrst\nplace at sub-task C in the ﬁnal competition.\n1 Introduction\nPropaganda is usually adopted to inﬂuence the au-\ndience by selectively displaying the facts to encour-\nage speciﬁc synthesis or perception, or using the\nloaded language to produce emotion rather than\nemotion itself. It was often associated with mate-\nrials prepared by governments in the past century.\nIn the internet era, activist groups, companies, reli-\ngious organizations, the media, and individuals also\n∗indicates equal contribution.\nproduce propaganda, and sometimes it can reach\nvery large audiences (Da San Martino et al., 2020).\nWith the recent research interest in detecting “fake\nnews”, the detection of persuasion techniques in\nthe texts and images has emerged as an active re-\nsearch area. Most previous work like (Patil et al.,\n2020) and (Chauhan and Diddee, 2020) have per-\nformed the analysis at the language content level\nonly. However, in our daily life, memes consist of\nimages superimposed with texts. The aim of the\nimage in a meme is either to reinforce a technique\nin the text or to convey one or more persuasion\ntechniques.\nSemEval-2021 Task6-c offers a different per-\nspective, multimodal multi-label classiﬁcation\n(Dimitrov et al., 2021), identify which of the 22\ntechniques are used both in the textual and visual\ncontent of memes. Since memes are combinations\nof texts and images, for this propaganda classiﬁ-\ncation task, we proposed to use transformer-based\narchitecture to fuse the clues from both linguis-\ntic and visual modalities. Two branches of ﬁne-\ntuning techniques are explored in this paper. First,\na text pre-trained transformer is applied with ex-\ntended visual features. Speciﬁcally, we initialize\nthe transformer with pre-trained text transformers\nand ﬁne-tune the model with extended visual fea-\ntures including grid features(e.g., ResNet(He et al.,\n2016)) and region features(e.g., BUTD (Anderson\net al., 2018)) from an image feature extraction net-\nwork and an object detector respectively. Second,\npre-trained multimodal transformers from ERNIE-\nViL(Yu et al., 2020) are used due to its better mul-\ntimodal joint representations characterizing cross-\nmodal alignments of detailed semantics.\nOur contributions are three-folds:\n• We propose to use transformer architecture\nfor fusing the visual and linguistic clues to\ntackle the propaganda classiﬁcation task.\n100\n• We ﬁnd that the multimodal pre-trained trans-\nformers work better than using text pre-trained\ntransformers with visual features. And the ex-\nperiment results have shown that ﬁne-tuning\nthe ERNIE-ViL model could achieve state-of-\nthe-art performance for this task.\n• Our ensemble result of several models obtains\nthe best score and ranks ﬁrst in Semeval-2021\nTask 6-c multimodal classiﬁcation task.\n2 Related work\n2.1 Text Transformers\nTransformer network (Vaswani et al., 2017) is ﬁrst\nintroduced in neural machine translation in which\nencoder and decoder are composed of multi-layer\ntransformers. After then, pre-trained language\nmodels, such as BERT (Devlin et al., 2018) and\nGPT(Radford et al., 2018), adopting transformer\nencoder as the backbone network, have signiﬁ-\ncantly improved the performance on many NLP\ntasks. One of the main keys to their success is the\nusage of transformer to capture the contextual infor-\nmation for each token in the text via self-attention.\nLater text pre-training works, such as ERNIE2.0\n(Sun et al., 2020), RoBERTa (Liu et al., 2019) and\nXLNET (Yang et al., 2019) are all shared the same\nmulti-layer transformer encoder and mainly put\ntheir effort on modiﬁcation of pre-training task.\n2.2 Visual Feature Extraction\nVisual feature extractors are mainly composed of\nplenty of convolutional neural networks ( CNN)\nsince CNN has a strong ability to extract complex\nfeatures that express the image with much more de-\ntails and learn the task-speciﬁc features much more\nefﬁciently. Existing works can be divided into the\nfollowing two types which are based on two differ-\nent image inputs: image grids and object regions.\nSome of those methods, such as VGG (Simonyan\nand Zisserman, 2014), ResNet (He et al., 2016)\noperate attention on CNN features corresponding\nto a uniform grid of equally-sized image regions.\nWhile the other works like Faster R-CNN (Ren\net al., 2015) operate a two-stage framework, which\nﬁrstly identiﬁes the image regions containing the\nspeciﬁc objects, and then encodes them with multi-\nlayer CNNs.\n2.3 Multimodal Transformers\nInspired by text pre-training models (Devlin et al.,\n2018), many cross-modal pre-training models for\nvision-language have been proposed. To integrate\nvisual features and text features, recent multimodal\npre-training works are mainly based on two vari-\nables of transformers. Some of them, like UNITER\n(Chen et al., 2019) and VILLA (Gan et al., 2020)\nuse a uniform cross-modal transformer modelling\nboth image and text representations. As ﬁne-tuning\non multimodal classiﬁcation tasks, such as the\nVisual-question-answering (VQA) (Antol et al.,\n2015) task (a multi-label classiﬁcation task), uni-\nﬁed transformers take textual and visual features\nas the model input, treat the ﬁnal hidden state of\nh[CLS] as the vision-language feature. While the\nothers like Vilbert (Lu et al., 2019), LXMERT (Tan\nand Bansal, 2019), ERNIE-ViL (Yu et al., 2020)\nare based on two-stream cross-modal transformers,\nwhich bring more speciﬁc representations for im-\nage and text. These two transformers are applied\nto images and texts to model visual and textual\nfeatures independently and then fused by a third\ntransformer in a later stage. The fusion of the ﬁnal\nhidden state of h[CLS] and h[IMG] are used to do\nthe classiﬁcation.\n3 Approach\nWe propose to use a transformer encoder to fuse\nthe clues from both linguistic and visual modalities\nand our approach is summarized in two branches,\nthe ﬁrst one is ﬁne-tuning a text pre-trained trans-\nformer with extended visual features, and the other\none is ﬁne-tuning a multimodal pre-trained model.\nFor the ﬁrst one, we try two different sets of vi-\nsual features, grid features based on equally-split\npatches of the image and salient region features\nbased on an object detector. For the second one,\na SoTA multimodal model, ERNIE-ViL (Yu et al.,\n2020) is applied with a multi-label classiﬁcation\nloss. A uniﬁed framework for the two branches is\nshown in Figure 1. We will introduce more details\nin this section.\n3.1 Text Pre-trained Transformer with\nVisual Features\nOur model consists of three parts: a) input feature\nextractor, b) feature fusion encoder, c) classiﬁca-\ntion encoder.\nFor the ﬁrst part, the text is tokenized into sub-\nwords to lookup the embedding while the image is\nprocessed by a feature extractor, such as a grid fea-\nture processor or a salient region feature processor\nto convert into vision embeddings. The input em-\n101\nUNARMED VICITM ARMED VICTIM ANY QUESTIONS? \nPre-trained Transformer[cls]unarmedvic? [sep]…\nℎ[\"#$] ℎ&! ℎ&\" ℎ&# ℎ['()] ℎ'! ℎ'\" ℎ'$… …\n0.0\n1.0classification losspredicted scoresground truth\nLoaded LanguageName calling/Labeling\nℎ[$*+]\n…\nAppeal to fear/prejudiceTransfer\nimage\nimage\nFigure 1: A uniﬁed framework used for the multimodal classiﬁcation task.\nbeddings are combinations of image embeddings\nand text embeddings and represented as\nh[CLS],ht1 ,··· ,htn ,h[SEP],hi1 ,··· ,him ,h[SEP]\nwhere the h[CLS], h[SEP] are the vector represen-\ntations of special tokens [CLS] and [SEP] respec-\ntively. The [CLS] token is inserted in the begin-\nning of the sequence, which act as an indicator of\nthe whole text, speciﬁcally, it is used to perform\ncomplete text classiﬁcation. The [SEP] is a token\nto separate a sequence from the subsequent one\nand indicate the end of a text. ht1 ,··· ,htn are the\ntext embeddings, and hi1 ,··· ,him are the vision\nembeddings. For the vision embeddings part, grid\nfeatures and salient region features are used.\nGrid Features Convolutional neural networks\nhave potent capabilities in image feature extrac-\ntion. The feature map obtained after the image\ngoes through multiple stacked convolution layers\ncontains high-level semantic information. Given an\nimage, we can use a pre-trained CNN encoder, such\nas ResNet, to transform it to a high-dimensional\nfeature map and ﬂatten each pixel on this feature\nmap to form the ﬁnal image representation.\nSalient Region Features Object detection mod-\nels are widely used to extract salient image regions\nfrom the visual scene. Given an image, we use a\npre-trained object detector to detect the image re-\ngions. The pooling features before the multi-class\nclassiﬁcation layer are utilized as the region fea-\ntures. The location information for each region is\nencoded via a 5-dimension vector representing the\nfraction of image area covered and the normalized\ncoordinates of the region and then is projected and\nsummed with the region features.\nFor the second part, the transformer encoder\nfuses the input text and image embedding, and\nﬁnally a cross-modal representation of size D is\nachieved for this sequence.\nThe last part of our model is the classiﬁcation\nencoder and loss function. After obtaining the en-\ncoding representation of the image and the text\nfrom the transformer encoder, we send the repre-\nsentation of [CLS] through the classiﬁcation head,\nwhich is consisted of a fully connected layer and\na Sigmoid activation for predicting the score of\neach category and loss with the ground truth.\n3.2 Multimodal Pre-trained Transformer\nDifferent from a single-modal pre-trained text trans-\nformer described above, a multimodal pre-trained\ntransformer for vision-language can learn more ef-\nﬁcient presentations. In this part, a SoTA model,\nERNIE-ViL, is applied.\nFor the generation of input embedding of text\nand image, it is mostly the same as the procedure\ndescribed in the previous section. Differences are\ntwo-folds. First, for the vision feature, a faster\nR-CNN encoder(Anderson et al., 2018) is used to\ndetect the salient regions while the position infor-\n102\nmation is taken into consideration. Second, The\ntext and the visual input embedding is represented\nas\nh[CLS],ht1 ,··· ,h[SEP],h[IMG],hi1 ,··· ,h[im]\nwhere there is a new token h[IMG] represents the\nfeature for the entire image.\nFor the feature fusion part, ERNIE-ViL utilized\na two steam cross-modal transformer to fuse the\nmultimodal information. For more details, you may\nrefer to (Yu et al., 2020).\n3.3 Criterion\nIn this task, there are 22 classes and the distribu-\ntion of positive and negative samples is extremely\nunbalanced. To solve this problem, we use the fo-\ncal loss to improve the imbalance of positive and\nnegative samples. For i-th class\nLclassi =\n{\nα(1 −p)γlog(p) if y=1\n(1 −α)pγlog(1 −p) otherwise\nwhere yis the ground truth; pis model prediction,\nwhich is the conﬁdence score of categoryi; αand γ\nare hyper-parameters, αis used to control the loss\nweight of positive and negative samples, and γis\nused to scale the loss of difﬁcult and easy samples.\n4 Experiment\n4.1 Implementation Details\nIn this task, we choose DeBERTa-large+ResNet50,\nDeBERTa-large+BUTD and ERNIE-VIL as the ﬁ-\nnal models. We performed all our experiments on\na Nvidia Tesla V100 GPU with 32 GB of mem-\nory. The models are trained for 20 epochs and we\npick the model which has the best performance on\nvalidation set.\nFor the DeBERTa transformer, the Adam opti-\nmizer with a learning rate of 3e-5 is used. Also, we\nhave applied the linear warm strategy for the learn-\ning rate. We set α= 0.9 and γ = 2.0 for the focal\nloss. To ensure robustness under a small dataset,\nwe set the threshold to 0.5 instead of performing a\nthreshold search strategy on the validation set. For\nthe pre-trained object detector, we choose Faster R-\nCNN (Anderson et al., 2018) and name the region\nfeatures as BUTD in the experimental results.\nFor the ERNIE-ViL transformers, we use the\nsame input prepossessing methods as (Yu et al.,\nPositive(%) Negative(%)\ntrain 1745(11.55%) 13369(88.45%)\ndev 183(13.20%) 1203(86.80%)\ntest 523(13.49%) 3877(86.51%)\nTable 1: Statistics of the positive and negative distribu-\ntion of the dataset.\nLoss Function Precision Recall F1\ncross-entropy 76.12 55.74 64.35\nfocal loss 71.18 66.12 68.56\nTable 2: Results of different loss functions.\n2020) and choose the large scale model 1 pre-\ntrained on all the four datasets. We ﬁnetune on\nour multimodal classiﬁcation dataset with a batch\nof 4 and a learning rate of 3e-5 for 20 epochs.\n4.2 Experimental Analysis\n4.2.1 DeBERTa with Visual Features\nUnbalanced Distribution There are 687/63/200\nexamples includes 22 categories in the\ntrain/validation/test datasets respectively. As\nshown in Table 1, the distribution of the classes is\nextremely unbalanced. If the cross-entropy loss is\nadopted directly during model training(the visual\nfeatures are from ResNet50), the model output may\nhave a greater chance of predicting the majority\nclass(negative class in this task), which results in a\nlower recall. To solve this problem, the focal loss\nis applied. From Table 2, it can be seen that the\nresult with focal loss performs much better than\nwith cross-entropy loss respective to the F1 score.\nVisual Features We evaluate the improvement\nbrought by extended visual features and explore\ndifferent types of visual feature extractors, e.g.,\nfrom pre-trained image classiﬁcation networks or\npre-trained object detectors. The results are illus-\ntrated in Table 3. Firstly, it can be seen that the\nﬁnal score is signiﬁcantly improved with mixing\nimage features compared with using only text fea-\ntures (Row “w/o vision feature”), which indicates\nthat the visual information is signiﬁcantly beneﬁ-\ncial for recognizing cross-modal propaganda tech-\nniques. Then, for features extracted from ResNet,\nwe ﬁnd that the depth of the network affects the\nresults, especially on the validation dataset, with\nthe best result from ResNet50. The reason may be\n1the pre-trained model is downloaded from\nhttps://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-vil\n103\ndev-F1 test-F1\nw/o vision feature 65.73 55.10\nResNet18 65.92 55.59\nResNet50 68.56 55.96\nResNet152 65.91 55.63\nBUTD 66.29 56.21\nTable 3: The results of using features extracted differ-\nent networks.\nregion numbers Dev F1 Test F1\n5 64.91 54.00\n10 66.67 54.60\n36 67.40 57.14\n100 67.45 56.07\nTable 4: Results comparisons with different object re-\ngion number inputs.\nthat the shallower network has insufﬁcient feature\nextraction capabilities, and the deeper network is\nvery difﬁcult to train. Finally, the region features\nfrom the pre-trained object detector(Row “BUTD”)\nwork best with an improvement of 0.25 on the test\ndataset compared to ResNet50 features.\n4.2.2 ERNIE-ViL\nWe compare the performance between ERNIE-ViL\nwith different object region inputs, which are num-\nber dynamic ranges between 0 and 36 with a ﬁxed\nconﬁdence threshold of 0.2 and constantly ﬁxed\n5, 10, or 100 boxes. The results are illustrated in\nTable 4.\nResults show that a larger box number can al-\nways achieve better performance within a certain\nrange. Utilizing 0-36 boxes leads to huge perfor-\nmance improvement with a 3.14 and 2.54 on Test-\nF1 compared with using constant 5 boxes and con-\nstant 10 boxes respectively. It can be concluded\nthat more object regions in a certain range can\nprovide more useful information. However, the per-\nformance with 100 boxes is worse than that with\n0-36 boxes. The reason may lie in that there are\nnot enough objects in the task sample. The ex-\nModels Dev-F1 Test-F1\nDeBERTa + ResNet50 68.56 55.96\nDeBERTa + BUTD 66.29 56.21\nERNIE-VIL 67.40 57.14\nEnsemble 69.12 58.11\nTable 5: Final ensemble result.\ntracted low-conﬁdence object regions may mislead\nthe multimodal model, therefore fuse useless or\nharmful visual features with text features. As a\nresult of that, brings a performance decrease on the\nﬁnal score.\n4.3 Ensemble Results\nThe performance comparison between our two\nbranches of approach is shown in Table 5. It can\nbe concluded that ﬁne-tuning the multimodal pre-\ntrained transformer (Row “ERNIE-ViL”) works\nbetter than ﬁne-tuning text pre-trained transformers\nwith visual features (Row “DeBERTa + BUTD”).\nOverall, ﬁne-tuning ERNIE-ViL has achieved state-\nof-the-art performance for this multimodal classiﬁ-\ncation task.\nSince the training dataset is small, we train mul-\ntiple models under various model structures and\ndifferent parameter conﬁgurations to take full ad-\nvantage of the training dataset and increase the\ndiversity of models. We choose three models of all\nmodel structures and all parameter conﬁguration\nthat performs best on the validation set and then\nensemble them together. After performing ensem-\nble strategy on those three models, both validation\nand test scores increases. As a result of that, we\nachieved a 58.11 score at F1 in the test set and\nranked ﬁrst place in the task competition.\n5 Conclusion\nWe explore two branches to ﬁne-tune pre-trained\ntransformers to jointly modelling texts and images\nfor the propaganda classiﬁcation task. The ﬁrst\nbranch, ﬁne-tuning pre-trained text transformer\nwith visual feature, obtain signiﬁcant performance\nimprovement compared to text classiﬁcation which\nvalidate the importance of visual clues for this task.\nVisual features from object detector yield slightly\nbetter results than grid features from ResNet. Im-\nportantly, ﬁne-tuning pre-trained multimodal trans-\nformers obtain the best single model performance.\nAnd this improvement further validates the claim\nmade by previous work that vision-language pre-\ntraining learned general joint representation needed\nfor multimodal tasks. Besides, since the distribu-\ntion of the classiﬁcation labels is extremely unbal-\nanced, we also make a further attempt on the loss\nfunction. Training models with focal loss can lead\nto a huge performance improvements than training\nwith cross entropy loss.\n104\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 6077–6086.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international\nconference on computer vision, pages 2425–2433.\nAniruddha Chauhan and Harshita Diddee. 2020. Psue-\ndoprop at semeval-2020 task 11: Propaganda span\ndetection using bert-crf and ensemble sentence level\nclassiﬁer. In Proceedings of the Fourteenth Work-\nshop on Semantic Evaluation, pages 1779–1785.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2019. Uniter: Learning universal\nimage-text representations.\nGiovanni Da San Martino, Alberto Barr ´on-Cedeno,\nHenning Wachsmuth, Rostislav Petrov, and Preslav\nNakov. 2020. Semeval-2020 task 11: Detection of\npropaganda techniques in news articles. In Proceed-\nings of the Fourteenth Workshop on Semantic Evalu-\nation, pages 1377–1414.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDimiter Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj\nAlam, Fabrizio Silvestri, Hamed Firooz, Preslav\nNakov, and Giovanni Da San Martino. 2021. Task\n6 at semeval-2021: Detection of persuasion tech-\nniques in texts and images. In Proceedings of the\n15th International Workshop on Semantic Evalua-\ntion, SemEval ’21, Bangkok, Thailand.\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu,\nYu Cheng, and Jingjing Liu. 2020. Large-scale ad-\nversarial training for vision-and-language represen-\ntation learning. arXiv preprint arXiv:2006.06195.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. arXiv preprint arXiv:1908.02265.\nRajaswa Patil, Somesh Singh, and Swati Agarwal.\n2020. Bpgc at semeval-2020 task 11: Propaganda\ndetection in news articles with multi-granularity\nknowledge sharing and linguistic features based en-\nsemble learning. arXiv preprint arXiv:2006.00593.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time ob-\nject detection with region proposal networks. arXiv\npreprint arXiv:1506.01497.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 8968–\n8975.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. arXiv preprint arXiv:1908.07490.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian,\nHua Wu, and Haifeng Wang. 2020. Ernie-vil:\nKnowledge enhanced vision-language representa-\ntions through scene graph."
}