{
  "title": "Fluctuation-Based Adaptive Structured Pruning for Large Language Models",
  "url": "https://openalex.org/W4393147854",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4282726796",
      "name": "Yongqi An",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2100311389",
      "name": "Xu Zhao",
      "affiliations": [
        "Institute of Automation",
        "Shandong Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2097645590",
      "name": "Tao Yu",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2108125403",
      "name": "Ming Tang",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2097542353",
      "name": "Jin-qiao Wang",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4282726796",
      "name": "Yongqi An",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation",
        "Beijing Academy of Artificial Intelligence",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2100311389",
      "name": "Xu Zhao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation",
        "ObjectVideo (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097645590",
      "name": "Tao Yu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation",
        "Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2108125403",
      "name": "Ming Tang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation",
        "Beijing Academy of Artificial Intelligence",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2097542353",
      "name": "Jin-qiao Wang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "ObjectVideo (United States)",
        "Institute of Automation"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2276892413",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W2119144962",
    "https://openalex.org/W1845051632",
    "https://openalex.org/W2156150815",
    "https://openalex.org/W6677103964",
    "https://openalex.org/W6852962002",
    "https://openalex.org/W2707890836",
    "https://openalex.org/W3035544451",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W4226126941",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W4226075153",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W4287208846",
    "https://openalex.org/W4313445468",
    "https://openalex.org/W4297790889",
    "https://openalex.org/W4318908623",
    "https://openalex.org/W4389104738",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4377371819",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4381586827",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4313484599",
    "https://openalex.org/W4379548477",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3127067080",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229005866"
  ],
  "abstract": "Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive Structured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.",
  "full_text": "Fluctuation-Based Adaptive Structured Pruning for Large Language Models\nYongqi An1, 2, Xu Zhao1, 4, *, Tao Yu1, 2, Ming Tang1, 2, Jinqiao Wang1, 2, 3, 4\n1Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of artificial intelligence, University of Chinese Academy of Sciences, Beijing, China\n3Wuhan AI Research, Wuhan, China\n4Objecteye Inc., Beijing, China\n{yongqi.an, xu.zhao, tangm, jqwang}@nlpr.ia.ac.cn, yutao2022@ia.ac.cn\nAbstract\nNetwork Pruning is a promising way to address the huge\ncomputing resource demands of the deployment and infer-\nence of Large Language Models (LLMs). Retraining-free\nis important for LLMs’ pruning methods. However, almost\nall of the existing retraining-free pruning approaches for\nLLMs focus on unstructured pruning, which requires spe-\ncific hardware support for acceleration. In this paper, we\npropose a novel retraining-free structured pruning frame-\nwork for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively\nreducing storage and enhancing inference speed. For effec-\ntive structured pruning of LLMs, we highlight three crit-\nical elements that demand the utmost attention: formulat-\ning structured importance metrics, adaptively searching the\nglobal compressed model, and implementing compensation\nmechanisms to mitigate performance loss. First, FLAP de-\ntermines whether the output feature map is easily recoverable\nwhen a column of weight is removed, based on the fluctuation\npruning metric. Then it standardizes the importance scores to\nadaptively determine the global compressed model structure.\nAt last, FLAP adds additional bias terms to recover the output\nfeature maps using the baseline values. We thoroughly evalu-\nate our approach on a variety of language benchmarks. With-\nout any retraining, our method significantly outperforms the\nstate-of-the-art methods, including LLM-Pruner and the ex-\ntension of Wanda in structured pruning. The code is released\nat https://github.com/CASIA-IV A-Lab/FLAP.\nIntroduction\nLarge Language Models (LLMs) (Brown et al. 2020; Tou-\nvron et al. 2023; Zhang et al. 2022; Scao et al. 2022) have\nrecently achieved outstanding performance across various\nlanguage benchmarks in NLP (Bommarito and Katz 2022;\nBubeck et al. 2023; Wei et al. 2022), spurring a large num-\nber of open-source applications (Taori et al. 2023; Anand\net al. 2023; Richards 2023). These remarkable capabilities\ntypically come with a huge-scale model size with high infer-\nence costs. This makes it harder for more people to benefit\nfrom LLMs. Due to the computational resource constraints,\nmost of the model compression methods in the pre-LLM era\n*Corresponding Author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nare no longer feasible for LLMs. Model compression meth-\nods for LLMs to date focus on model quantization (Dettmers\net al. 2022; Xiao et al. 2023; Frantar et al. 2023; Dettmers\net al. 2023) and unstructured pruning (Sun et al. 2023; Fran-\ntar and Alistarh 2023).\nStructured pruning (He and Xiao 2023), which prunes en-\ntire rows or columns of weights, offers a promising solution\nto the deployment challenges of LLMs. Unlike unstructured\npruning, structured pruning reduces both parameters and in-\nference time without relying on specific hardware, making\nit more widely applicable (Anwar, Hwang, and Sung 2017).\nFor effective structured pruning, it’s crucial to have a metric\nthat captures the collective significance of an entire row or\ncolumn. However, current unstructured pruning techniques\nfor LLMs, as seen in methods like (Sun et al. 2023; Fran-\ntar and Alistarh 2023), primarily focus on the importance of\nindividual elements of each row in isolation. This absence\nof structured metrics that evaluate entire rows or columns\nmakes them less suitable for structured pruning. The recent\nLLM-Pruner (Ma, Fang, and Wang 2023) attempted struc-\ntured pruning for LLMs, but its dependence on LoRA fine-\ntuning (Hu et al. 2021) creates a tough trade-off between\nhigh computation and effective pruning, limiting its use in\nlarger models.\nPruning essentially involves two key aspects: discover-\ning redundancy and recovering performance. For an effec-\ntive structured pruning method tailored to LLMs, three fun-\ndamental criteria must be satisfied: a) a structured impor-\ntance metric to discover structured redundancy; b) a mech-\nanism for adaptively searching the optimal global compres-\nsion model structure; and c) a compensation strategy to min-\nimize performance degradation.\nIn response to these three essential criteria, we introduce\nFLAP (FLuctuation-based Adaptive Structured Pruning), a\nnovel structured pruning framework. We find that certain\nchannels of hidden state features exhibit structured sample\nstability. This observation enables us to compensate for bias\nwithin the model using baseline values. Specifically, we de-\nsign a structured pruning metric that estimates the fluctu-\nation of each input feature relative to the baseline value,\nutilizing a set of calibration samples. This metric assists in\ndetermining whether the output feature map can be recov-\nered when a column of the weight matrix is removed. We\nthen standardize these fluctuation metric scores across lay-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10865\ners and modules separately, allowing for the adaptive deter-\nmination of the global compressed model structure. Finally,\nFLAP employs the baseline values to add additional biases,\nrecovering the output feature maps for the corresponding\nlayers. Remarkably, our method avoids the need for the re-\ntraining process and requires only a single forward pass for\nboth pruning and bias compensation, thereby maintaining\nlow memory overhead.\nWe evaluate the effectiveness of FLAP on the LLaMA\nmodel family, and FLAP achieves remarkable performance\non a variety of language benchmarks. Impressively, with-\nout any retraining, our method significantly outperforms the\nstate-of-the-art methods, including LLM-Pruner and the ex-\ntension of Wanda in structured pruning.\nOur main contributions are listed as follows:\n• We propose a novel retraining-free structured pruning\nframework for LLMs named FLAP. To our best knowl-\nedge, this is the first work that identifies the characteristic\nof structured sample stability in LLMs.\n• The proposed framework uses a bias compensation\nmechanism, a pruning performance recovery method\nthat does not require retraining. This mechanism yields\ngreater benefits, especially under large pruning ratios.\n• Our method achieves remarkable performance on a vari-\nety of language benchmarks and outperforms the state-\nof-the-art method without any retraining.\nRelated Works\nNetwork Pruning Methods\nNetwork pruning is a model compression technique that\nidentifies and eliminates redundancy in the structure or pa-\nrameters of a neural network, based on specific pruning\nmetrics, and incorporates methods to recover model per-\nformance (LeCun, Denker, and Solla 1989; Hassibi, Stork,\nand Wolff 1993; Han et al. 2015). Pruning methods fall\ninto two categories: unstructured pruning and structured\npruning. Unstructured pruning is performed at the indi-\nvidual weight level, allowing for a large sparsity but fail-\ning to achieve real inference acceleration or storage re-\nduction (Zafrir et al. 2021; Han, Mao, and Dally 2016).\nWithin unstructured pruning, there exists a specialized vari-\nant known as semi-structured pruning. This approach en-\nforces exactly N non-zero values in each block of M con-\nsecutive weights (Zhou et al. 2021). This approach has\ngained traction recently, particularly with support on newer\nNVIDIA hardware (Mishra et al. 2021). Structured pruning,\nby contrast, operates on entire rows or columns of weights,\nproviding a more hardware-friendly solution that reduces\nstorage requirements and enhances inference speed (Xia,\nZhong, and Chen 2022; Molchanov et al. 2017).\nHowever, conventional structured pruning methods typi-\ncally rely on retraining (sometimes iteratively) to regain the\nperformance of the pruned model (Han et al. 2015; Tan and\nMotani 2020; Han, Mao, and Dally 2016). Such methods\npose scalability challenges for billion-scale LLMs due to\nconstraints on memory and computational resources. There-\nfore a retraining-free structured pruning method for LLMs\nis very critical.\nLarge Language Model Compression\nLarge Language Models usually consist of billions of pa-\nrameters, and their gradient backpropagation and train-\ning stage require large amounts of memory and com-\nputational resources. Consequently, many conventional\nmodel compression techniques have become infeasible for\nLLMs (Frantar and Alistarh 2023). For instance, knowledge\ndistillation (Hinton, Vinyals, and Dean 2015), once a prac-\ntical approach, now faces implementation challenges due\nto high training costs. Existing compression methods for\nLLMs mainly include post-training quantization (Dettmers\net al. 2022; Xiao et al. 2023; Frantar et al. 2023; Dettmers\net al. 2023) and post-training pruning (Sun et al. 2023; Fran-\ntar and Alistarh 2023). Our method also falls into the cat-\negory of post-training pruning. It utilizes bias compensa-\ntion to recover model performance, effectively avoiding the\nhigh computational cost of retraining. Unlike the past post-\ntraining pruning methods, our method is designed for the\nfeatures of structured pruning of LLMs.\nProperties of LLMs\nOur work is related to the distinct properties of Large Lan-\nguage Models (LLMs) that have inspired various model\ncompression techniques (Sun et al. 2023; Dettmers et al.\n2023, 2022). Dettmers et al. (Dettmers et al. 2022) observed\nthe emergence of channels with abnormally large magni-\ntudes in the hidden state features of LLMs once they exceed\na certain parameter scale (e.g., 6B). They suggest that this is\nthe reason why existing quantization methods fail on LLMs.\nIn response, they introduced a novel mixed-precision quanti-\nzation technique. Contrary to the focus of previous work on\nthe outlier magnitudes in LLMs, our research pivots towards\ninvestigating the structured stability within the channels of\ninput features in these models. In our study, we find that cer-\ntain channels within the hidden state features demonstrate\nconsistent structured sample stability. This discovery offers\ninvaluable insights for crafting structured post-training prun-\ning algorithms, laying the foundation for the method we\npresent in this paper.\nPreliminaries\nLayer-Wise Pruning\nGiven the computational constraints, globally solving the\npruning problem for Large Language Models (LLMs)\nis challenging. Layer-wise pruning becomes a practical\nsolution under these constraints. Following this notion,\nSparseGPT (Frantar and Alistarh 2023) demonstrated that\nthe challenge of unstructured pruning for LLMs can be tack-\nled by decomposing it into individual layer-wise subprob-\nlems. This principle can be seamlessly extended to struc-\ntured pruning within LLMs. The quality of solutions to these\nlayer-wise subproblems can be evaluated based on the ℓ2-\nerror. Given an input Xℓ of shape (N, Cin, L) where N and\nL represent batch and sequence dimensions respectively, and\na weight Wℓ of shape (Cout, Cin), the ℓ2-error for struc-\ntured pruning can be defined as:\nargminMℓ∈RCin,cWℓ||WℓXℓ − (Mℓ ⊙ cWℓ)Xℓ||2\n2 (1)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10866\n①  Structural Fluctuation Metric ②  Adaptive Structure Search\n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \nSelf-Attn MLP\n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \nUnified\nSearch\nStandardize\n+\n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \n··· \nFluctuation Metric\nPer Channel\n+\n+\n+\nLanguage\nEmbeddings\n+\n+\n+\n+\n60% 30% 20% 70%\n···\nInput Features\n③  Baseline Bias Compensation (1) (2)\n···\nFigure 1: Framework of the proposed FLAP. ①Measure the fluctuation of each channel across different layers and modules\nusing calibration data; ②Standardize these fluctuation measures for a unified search method; ③Implement adaptive pruning\nratios for each layer and module, employing bias compensation to restore model performance.\nwhere Mℓ ∈ RCin represents the mask vector correspond-\ning to the input channels, this vector mirrors whether each\ninput channel is pruned or not. For the self-attention mod-\nules, these input channels are pruned in groups typically\nwith sizes like group size=128. The term cWℓ denotes the\npossibly updated weights for the pruned layer. The notation\n|| · ||2\n2 represents the ℓ2-error.\nLocal Pruning Metric Challenges\nRegarding Eq. (1), the existing methods can be broadly\ncategorized into two primary approaches: low-damage and\neasy-recoverability. These correspond to the core principles\nof OBD (LeCun, Denker, and Solla 1989) and OBS (Has-\nsibi, Stork, and Wolff 1993), respectively. To illustrate,\nWanda (Sun et al. 2023) uses a localized low-damage prun-\ning metric to minimize harm to each layer’s output features.\nIn contrast, SparseGPT (Frantar and Alistarh 2023) employs\nan easy-recoverability metric, aiming to identify compo-\nnents that other weights can compensate for during prun-\ning. These approaches are insightful but tend to focus on the\nimportance of individual elements in the weight matrix, ne-\nglecting the broader structured context. Such an atomistic\napproach is misaligned with structured pruning’s require-\nments, which demand a more global perspective that cap-\ntures the collective importance of entire rows or columns in\nthe matrix.\nMethodology\nIn this section, we introduce FLAP, our proposed approach\nto structured pruning for Large Language Models (LLMs).\nFLAP encompasses three key components: Baseline Bias\nCompensation, Structured Fluctuation Metric, and Adaptive\nStructure Search. The overview of our method is presented\nin Figure 1.\nBaseline Bias Compensation\nIn the context of structured pruning, the output of the layers\nof the uncompressed model can be decomposed into:\nWℓXℓ = (Mℓ ⊙ Wℓ)Xℓ\n|\n{z }\nretained\n+ ((1− Mℓ) ⊙ Wℓ)Xℓ\n| {z }\nremoved\n(2)\nThe objective of structured pruning is to minimize the im-\npact introduced by ∆Y ℓ = ((1 − Mℓ) ⊙ Wℓ)Xℓ in the\noverall output feature map, thereby reducing the reconstruc-\ntion error for each layer. For structured pruning of LLMs, the\nconstraints are stronger, so the latter components cannot be\nsimply removed. Therefore, a compensatory mechanism is\nessential to recover the model’s performance while adhering\nto the pruning structure.\nWe add an additional bias term to compensate for the\ndamage inflicted on the output feature maps by the removed\ncomponents. This bias term is designed to mitigate the re-\nconstruction error introduced by the pruning process, al-\nlowing the pruned model to maintain high performance. In\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10867\nparticular, we construct the bias term based on the baseline\nvalue, X\nℓ\n:,j,:, which represents the average of the j-th chan-\nnel for all samples in layer l. As detailed in the following\nsection, our empirical findings validate the effectiveness and\nfeasibility of this compensatory approach. Specifically, the\nformulation for the baseline value is as follows:\nX\nℓ\n:,j,: = 1\nNL\nNX\nn=1\nLX\nk=1\nXℓ\nn,j,k (3)\nOnce the mask Mℓ is established, the baseline value for\nthe pruned channel can be seamlessly translated into the bias\nterm for the linear layer as follows:\nBℓ\n0 = Wℓ((1 − Mℓ) ⊙ X\nℓ\n)\nWℓXℓ ≈ (Mℓ ⊙ Wℓ)Xℓ + Bℓ\n0\n(4)\nwhere B0 represents the bias of linear layer, which has a\nshape of (Cout, ), and\nX\nℓ\nis a one-dimensional vector with\ndimensions (Cin, ).\nStructured Fluctuation Metric\nMotivated by the observations from Figure 2, we note that\ncertain channels of the hidden state features exhibit a low\nvariation across different samples. This low fluctuation in-\ndicates that if their corresponding input feature channels are\npruned, the resulted change in the output feature map can be\neffectively counterbalanced by the baseline value.\n0\n32\n64\n96\n128\nT oken index\n0\n32\n64\n96\n128 Sample index\nChannel index=1028\ninstability\n0\n32\n64\n96\n128\nT oken index\n0\n32\n64\n96\n128 Sample index\nChannel index=1035\nstability\nChannel Stability Across Samples for Each T oken\nFigure 2: Certain channels of hidden state features exhibit\nstructured sample stability. The left shows a channel with\nnoticeable variations across samples, indicating low stabil-\nity. The right displays a stable pattern common in many\nLLaMa channels.\nAs illustrated in Eq. (4), the structured easy-recoverability\nmetric seeks to evaluate the impact on the output feature\nmap when an input channel is substituted with its baseline\nvalue. A straightforward approach would involve individu-\nally substituting each input channel with its baseline value\nfor the calibration samples and then computing the ℓ2-error\nbetween the output feature maps before and after this re-\nplacement.\nHowever, such a method poses a significant computa-\ntional challenge and is impractical for LLMs. To address\nthis, we introduce an approximate metric for structured re-\ncoverability, which termed the ”fluctuation metric”. Specifi-\ncally, we compute the sample variance of each input feature\nand weight it with the squared norm of the corresponding\ncolumn of the weight matrix. Concretely, the score for the\ngroup of weight Wℓ\n:,j is defined by:\nSℓ\n:,j = 1\nN − 1\nNX\nn=1\n(Xℓ\nn,j,: −\nX\nℓ\n:,j,:)2 · ||Wℓ\n:,j||2\n2 (5)\nwhere ||Wℓ\n:,j||2\n2 denotes the squared norm of j-th column\nof the weight matrix. 1\nN−1\nPN\nn=1(Xℓ\nn,j,: − X\nℓ\n:,j,:)2 repre-\nsents the sample variance of the j-th channel of the input\nfeature of layer ℓ under N calibration samples. The denom-\ninator here is 1\nN−1 . This correction is known as the Bessel\ncorrection and is used for unbiased estimation of the overall\nvariance.\nAdaptive Structure Search\nThe central challenge in layer-wise pruning revolves around\nadaptively searching the global compression model struc-\ntures. Unifying different layers and modules without distinc-\ntion can critically degrade performance. This issue arises be-\ncause the magnitudes of the metrics across layers and mod-\nules vary greatly (Shi et al. 2023). Figure 3 demonstrates\nthis by showing the mean values of the fluctuation metric\nfor different modules in different layers.\n1 6 11 16 21 26 31\nLayer index\n0\n50\n100\n150\n200\n250\n300\n350Average value of metric\nMagnitude differences between layers and modules\nMlp\nAttn\nFigure 3: Comparison of the average value of the fluctuation\nmetric across different layers for different modules.\nTo ensure a consistent comparison of scores across differ-\nent layers and modules, we standardize the metric distribu-\ntions for each layer to a common mean and standard devi-\nation. As defined in Eq. (5), the fluctuation metric captures\nthe absolute variation in the output feature map when input\nfeatures are replaced with their baseline values. In contrast,\nthe standardized metric reflects the relative variation in the\noutput feature map resulting from this replacement, making\nit suitable for a structured unified search. The standardized\nmetric, denoted as, is formulated as follows:\nbSℓ\n:,j = (Sℓ\n:,j − E[Sℓ\n:,j])/(E[[Sℓ\n:,j − E[Sℓ\n:,j]]2])\n1\n2 (6)\nwhere E[Sℓ\n:,j] represents the expected value (or mean) of the\nvector Sℓ\n:,j. (E[[Sℓ\n:,j −E[Sℓ\n:,j]]2])\n1\n2 represents the square root\nof the variance, which is the standard deviation.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10868\nExperiments\nExperimental Settings\nWe conduct experiments on the LLaMA model fam-\nily (LLaMA-7B/13B/30B/65B) to evaluate the efficacy of\nFLAP . Our evaluation focuses on language modeling per-\nformance on the WikiText2 (Merity et al. 2016) validation\nset and zero-shot performance across seven common sense\nbenchmarks using the EleutherAI LM Harness (Gao et al.\n2021)1. We compare FLAP against two previous pruning\nmethods: Wanda-sp and LLM-Pruner. We generalize Wanda\nto structured pruning and name it as Wanda-sp. Detailed ex-\nperimental settings, model descriptions, and evaluation pro-\ntocols are provided in the Appendix A.\nMethod Pruning\nRatio\nLLaMA\n7B 13B 30B 65B\nDense 0% 12.62 10.81 9.11 8.21\nWanda-sp\n20%\n22.12 16.83 11.66 11.76\nLLM-Pruner 19.77 16.01 - -\nLLM-Pruner* 17.37 15.18 - -\nFLAP (Ours) 14.62 13.66 10.86 9.79\nWanda-sp 30% 38.88 22.89 14.90 14.64\nFLAP (Ours) 17.62 15.65 12.49 10.90\nWanda-sp\n50%\n366.43 160.49 67.46 42.85\nLLM-Pruner 112.44 - - -\nLLM-Pruner* 38.12 - - -\nFLAP (Ours) 31.80 24.20 19.36 15.30\nTable 1: WikiText2 validation perplexity of pruning methods\nfor LLaMA model family. * means with LoRA fine-tuning.\nLanguage Modeling\nPerformance Comparisons. For each of the LLaMA\nmodels, we present results at three distinct pruning ratios,\nas detailed in Table 1. Notably, FLAP significantly outper-\nforms the other methods, achieving this superiority without\nany retraining. As the pruning ratio increases, the perfor-\nmance advantage of FLAP becomes more significant. To il-\nlustrate, consider the LLaMA-7B model: at a 50% pruning\nratio, the LLM-Pruner exhibits a perplexity of 130.97, which\nimproves to 39.02 after LoRA fine-tuning. In stark contrast,\nFLAP efficiently identifies sparse networks that yield a per-\nplexity of 31.80, and remarkably, this is achieved without\nany retraining.\nRemark. The FLAP method, which requires no retrain-\ning, consistently outperforms the LLM-Pruner, even when\nthe latter is fine-tuned with LoRA. Eq (4) offers insight into\nthe potential reason for this superior performance. In FLAP ,\nthe baseline biasB0 is effectively treated as a low-rank com-\nponent with a rank of r = 1. Within the pruning framework\nof FLAP , bias compensation plays a pivotal role, serving a\nfunction similar to that of LoRA fine-tuning. This compen-\nsation helps to effectively recover the model’s performance\nafter pruning.\n0.0 0.1 0.2 0.3 0.4 0.5\nPruning Ratio\n20\n40\n60\n80\n100\n120\n140Perplexity\nLLaMA-7B WikiT ext2\nFLAP\nWanda-sp\nLLM-Pruner\nLLM-Pruner+LoRA\nFigure 4: Results among FLAP and other structured pruning\nmethods at varying pruning ratios on the LLaMA-7B Wiki-\nText2 dataset.\nDifferent Pruning Ratio. We evaluated the performance\nof each structured pruning method at various pruning ra-\ntios. As depicted in Figure 4, FLAP demonstrates remark-\nable stability in maintaining its performance as the pruning\nratio increases. In contrast, Wanda-sp exhibits a sharp de-\ncrease in performance as the pruning ratio rises. Meanwhile,\nLLM-Pruner requires LoRA fine-tuning to maintain accept-\nable performance when the pruning ratio is increased to lev-\nels like 50%.\nZero-shot Tasks Performance\nWe assessed the zero-shot capability of the pruned model\nacross seven downstream tasks. As illustrated in Table 2, our\nmethod consistently outperforms LLM-Pruner with LoRA\nFine-Tuning, achieving superior performance across vary-\ning pruning ratios, all without the need for retraining. At a\n20% pruning ratio, Wanda-sp exhibits remarkable zero-shot\ncapabilities, even surpassing the performance of the origi-\nnal, unpruned model. This suggests the presence of struc-\ntured redundancy within LLMs that can be pruned away\nwithout necessitating retraining, thereby potentially enhanc-\ning model efficiency. However, when the pruning ratio is\nincreased to 50%, the performance of Wanda-sp suffers a\nsignificant degradation. In stark contrast, our method con-\ntinues to excel, maintaining a clear advantage over other\napproaches. This finding demonstrates the efficacy of our\nstructured pruning method in preserving the generalization\ncapabilities of large language models (LLMs), even under\nstringent pruning conditions.\nAblation Study\nWe systematically examine three fundamental components\nof the FLAP method: the pruning metric, the global com-\npression structure, and bias compensation. Additionally, we\nevaluate the robustness of our pruning approach in relation\nto calibration samples.\n1https://github.com/EleutherAI/lm-evaluation-harness\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10869\nMethod Pruning Ratio BoolQ PIQA HellaSw ag\nWinoGrande ARC-e ARC-c OBQA Average\nLLaMA-7B 0% 73.18 78.35 72.99 67.01\n67.45 41.38 42.40 63.25\nWanda-sp\n20%\n71.25 77.09 72.77 67.09 71.09 42.58 41.60 63.35\nLLM-Pruner 59.39 75.57 65.34 61.33\n59.18 37.18 39.80 56.82\nLLM-Pruner (w/ LoRA) 69.54 76.44 68.11 65.11\n63.43 37.88 40.00 60.07\nFLAP (Ours) 69.63 76.82 71.20 68.35 69.91 39.25 39.40 62.08\nWanda-sp\n50%\n50.58 55.01 29.56 51.78\n31.27 23.04 23.60 37.83\nLLM-Pruner 52.57 60.45 35.86 49.01\n32.83 25.51 34.80 41.58\nLLM-Pruner (w/ LoRA) 61.47 68.82 47.56 55.09 46.46 28.24 35.20 48.98\nFLAP (Ours) 60.21 67.52 52.14 57.54 49.66 29.95\n35.60 50.37\nTable 2: Zero-shot performance of the compressed LLaMA-7B. Bold results highlight the best performance. Underscored\nresults denote the second-best performance for each pruning ratio.\nPruning Metric. Both the pruning metric and compressed\nmodel structure are critical factors in the pruning process.\nFLAP is specifically designed to address these two dimen-\nsions in the structured pruning of Large Language Models\n(LLMs). To evaluate their effectiveness, we conducted ex-\nperiments employing various structured pruning metrics and\nglobal compression structures.\nWe investigated three structured pruning metrics in this\nstudy: 1) Weighted Input Feature Norm (WIFN), a low-\ndamage metric assessing the effect of weight columns on the\noutput feature map; 2) Input Feature Variance (IFV), used to\ngauge the variability among input features; and 3) Weighted\nInput Feature Variance (WIFV), utilized by FLAP to assist\nin determining the potential for recovery of the output fea-\nture map after a column of the weight matrix is removed.\nTo underscore the importance of global adaptive compres-\nsion structure, we defined four configurations: ’UL-UM’\n(Uniform across Layers and Modules, employed in unstruc-\ntured pruning for LLMs like Wanda); ’UL-MM’ (Uniform\nacross Layers, Manual ratio for Modules); ’AL-MM’ (Adap-\ntive across Layers, Manual for Modules); and ’AL-AM’\n(Adaptive across both Layers and Modules), the structure\nchosen by FLAP . Results in this section include bias com-\npensation, with bias-compensated ablation experiments de-\ntailed later.\nIn our experiments, we structurally pruned the LLaMA-\n7B model with a 50% pruning ratio and evaluated the model\nusing the perplexity metric on the WikiText2 dataset. The\ndetailed results are presented in Table 3. Notably, the most\neffective pruning model was obtained using the default\nconfiguration of FLAP , achieving a perplexity of 31.80.\nThe AL-AM global adaptive compression structure consis-\ntently outperformed other configurations under all evaluated\npruning metrics, thereby effectively validating our proposed\nAdaptive Structure Search strategy. When analyzing the ef-\nfectiveness of different global compression structures, we\nobserved that various metrics present distinct strengths and\nweaknesses. Nevertheless, our proposed WIFV structured\npruning metric displayed superior adaptability to the global\ncompression structure.\nBaseline Bias Compensation. In structured pruning of\nlarge language models, restoring model performance af-\nter the pruning process is a crucial aspect. Our approach\nuniquely leverages bias compensation as a strategy to re-\n0.1 0.2 0.3 0.4 0.5\nPruning Ratio\n0\n10\n20\n30\n40\n50Perplexity\nPerformance With/Without Bias Compensation\nw/o Bias Compensation\nw/ Bias Compensation\nPerformance Difference\nFigure 5: Performance comparison of the model with and\nwithout Bias Compensation at various pruning ratios. The\nyellow and orange bars represent the Perplexity of the\nmodel without and with Bias Compensation, respectively.\nThe green bars show the performance difference between\nthe two conditions.\ncover the performance of pruned models, circumventing the\nneed for expensive and time-consuming retraining proce-\ndures. Figure 5 vividly illustrates the performance of the\nFLAP method on the WikiText2 dataset, comparing the per-\nplexity scores with and without bias compensation at vary-\ning pruning ratios for the LLaMA-7B model. Evident from\nthe figure, bias compensation plays a significant role in mit-\nigating the performance degradation associated with prun-\ning. Furthermore, this compensatory effect becomes more\npronounced as the pruning ratio increases, highlighting the\ngrowing importance of bias compensation in more aggres-\nsively pruned models.\nRobustness to Calibration Samples.Our method utilizes\na calibration dataset to estimate the input variance at each\nlayer of the language model. This makes it critical to inves-\ntigate the impact of the size of this calibration dataset on\nthe pruning performance. Figure 6 delineates the effects of\nvarying the number of calibration samples on the pruning\noutcome. For this analysis, we set a pruning ratio of 50%for\nthe LLaMa-7B model and observed the resultant perplex-\nity on the WikiText2 dataset. The results clearly show that\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10870\nCompressed model structure\nPruning Metric UL-UM UL-MM AL-MM AL-AM\nWIFN: PCout\ni=1 ||Xℓ\nj||2 · |Wℓ\nij| 84.79 128.75 34.50 34.09\nIFV: 1\nN−1\nPN\nn=1(Xℓ\nn,j,: − X\nℓ\n:,j,:)2 55.41 48.87 35.72 33.33\nWIFV: 1\nN−1\nPN\nn=1(Xℓ\nn,j,: − X\nℓ\n:,j,:)2 · ||Wℓ\n:,j||2\n2 57.57 38.31 34.82 31.80\nTable 3: Ablation on pruning metric and compressed model structure. Bold results denote the best compressed model structure\nfound for each pruning metric. Underscored results indicate the best pruning metric found for each compressed model structure.\nFLAP ’s performance improves as the size of the calibration\ndataset increases. In our experiments, we selected a default\nsetting of 1024 calibration samples. Given that only a sin-\ngle forward propagation is required for this calculation, the\ncomputational cost associated with this sample size is mini-\nmal. Notably, the entire pruning process for the LLaMa-7B\nmodel is efficiently completed in a span of 3 to 5 minutes on\na single GPU.\n4 8 16 32 64 128 256 512 1024 2048\nCalibration Samples\n50\n100\n150\n200\n250\n300\n350\n400\n450Perplexity\nLLaMA-7B WikiT ext2 (p=50%)\nFLAP\nWanda-sp\nFigure 6: Robustness to Calibration Samples.\nInference Speed\nUnlike unstructured pruning, structured pruning offers the\ndual benefit of reducing both the number of parameters and\nthe inference time, without the need for specialized hard-\nware. This makes structured pruning a more universally ap-\nplicable approach. In this section, we empirically compare\nthe actual parameter counts and inference speeds of differ-\nent pruning methods, with the experiments conducted on\nNVIDIA A100 GPUs. The detailed results are presented in\nTable 4. Notably, Wanda, employed here as a representative\nof unstructured pruning, does not effectively reduce either\nthe parameter count or the inference speed. In contrast, our\nmethod demonstrates substantial efficiency improvements:\nat a 20% pruning ratio, it reduces the number of parame-\nters by 52%, and accelerates the inference speed by 66%.\nAt a 50% pruning ratio, these improvements are further am-\nplified, with reductions in parameter count by 25%, and an\nincrease in speed by 31%.\nFigure 7 compares the throughput of the LLaMA-7B\nmodel with a model pruned by 50% using our method,\nacross various batch sizes. The comparison clearly shows\nthat the pruned model benefits more at larger batch sizes, as\nit has not yet hit the throughput bottleneck.\nMethod Pruning\nRatio Params Memory Tokens/s\nLLaMA-7B 0% 6.74B 12916.5MiB 25.84\nWanda 20% 6.74B 12916.5MiB 25.67 (≈ 0%)\nLLM-Pruner 5.42B 10387.2MiB 32.57 (↑ 26%)\nFLAP (Ours) 5.07B 9726.2MiB 33.90 (↑ 31%)\nWanda 50% 6.74B 12916.5MiB 25.95 (≈ 0%)\nLLM-Pruner 3.35B 6547.1MiB 40.95 (↑ 58%)\nFLAP (Ours) 3.26B 6268.2MiB 42.88 (↑ 66%)\nTable 4: Inference speed and memory footprint comparison.\n1 2 4 8\nBatch size\n100\n200\n300Throughout (tokens/s)\nThe impact of batch size on throughout\nLLaMA-3.5B (FLAP)\nLLaMA-7B\nFigure 7: The impact of batch size on throughput. The hard-\nware is the NVIDIA A100-40G.\nConclusion\nIn this work, we propose FLAP (FLuctuation-based\nAdaptive Structured Pruning), a retraining-free structured\npruning framework explicitly designed for Large Language\nModels (LLMs). To address the challenges posed by struc-\ntured pruning, we introduce a novel structured pruning met-\nric, employ adaptive global model compression strategies,\nand implement robust compensation mechanisms designed\nto mitigate potential performance losses. Our empirical re-\nsults affirm that the structured compression model crafted by\nFLAP can maintain perplexity and zero-shot performance\nwithout any retraining. Especially worth noting is the effi-\ncacy of FLAP in upholding model performance at both low\nand medium compression rates. Our work demonstrates that\nbias compensation can largely replace retraining or parame-\nter efficient fine-tuning (PEFT). We hope that our work con-\ntributes to a better understanding of structured pruning and\nperformance recovery of LLMs.\nAcknowledgements\nThis work was supported by the National Key R&D Program\nof China (Grant No. 2021ZD0110400), Beijing Munici-\npal Science and Technology Project (Z231100007423004),\nZhejiang Lab (No. 2021KH0AB07), and National Natu-\nral Science Foundation of China (Grant No. 62206290,\n62276260, 62176254, 61976210, 62076235).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10871\nReferences\nAnand, Y .; Nussbaum, Z.; Duderstadt, B.; Schmidt, B.;\nand Mulyar, A. 2023. GPT4All: Training an Assistant-\nstyle Chatbot with Large Scale Data Distillation from GPT-\n3.5-Turbo. https://github.com/nomic-ai/gpt4all. Accessed:\n2023-08-09.\nAnwar, S.; Hwang, K.; and Sung, W. 2017. Structured prun-\ning of deep convolutional neural networks. ACM Journal\non Emerging Technologies in Computing Systems (JETC),\n13(3): 1–18.\nBommarito, M.; and Katz, D. M. 2022. GPT Takes the Bar\nExam. arXiv:2212.14402.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\narXiv:2005.14165.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4. arXiv:2303.12712.\nDettmers, T.; Lewis, M.; Belkada, Y .; and Zettlemoyer, L.\n2022. LLM.int8(): 8-bit Matrix Multiplication for Trans-\nformers at Scale. In Advances in Neural Information Pro-\ncessing Systems.\nDettmers, T.; Svirschevski, R.; Egiazarian, V .; Kuznedelev,\nD.; Frantar, E.; Ashkboos, S.; Borzunov, A.; Hoefler, T.;\nand Alistarh, D. 2023. SpQR: A Sparse-Quantized Rep-\nresentation for Near-Lossless LLM Weight Compression.\narXiv:2306.03078.\nFrantar, E.; and Alistarh, D. 2023. SparseGPT: Massive\nLanguage Models Can Be Accurately Pruned in One-Shot.\narXiv:2301.00774.\nFrantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2023.\nGPTQ: Accurate Post-training Compression for Generative\nPretrained Transformers. In International Conference on\nLearning Representations.\nGao, L.; Tow, J.; Biderman, S.; Black, S.; DiPofi, A.; Fos-\nter, C.; Golding, L.; Hsu, J.; McDonell, K.; Muennighoff,\nN.; et al. 2021. A framework for few-shot language model\nevaluation. Version v0. 0.1. Sept.\nHan, S.; Mao, H.; and Dally, W. J. 2016. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and Huffman coding. In International Confer-\nence on Learning Representations.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. J. 2015. Learning\nboth weights and connections for efficient neural networks.\nIn Advances in Neural Information Processing Systems.\nHassibi, B.; Stork, D. G.; and Wolff, G. J. 1993. Optimal\nbrain surgeon and general network pruning. In IEEE Inter-\nnational Conference on Neural Networks.\nHe, Y .; and Xiao, L. 2023. Structured Pruning for Deep Con-\nvolutional Neural Networks: A survey. arXiv:2303.00566.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nknowledge in a neural network. arXiv:1503.02531.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\nof large language models. arXiv:2106.09685.\nLeCun, Y .; Denker, J. S.; and Solla, S. A. 1989. Optimal\nbrain damage. In Advances in Neural Information Process-\ning Systems.\nMa, X.; Fang, G.; and Wang, X. 2023. LLM-Pruner: On the\nStructural Pruning of Large Language Models. Version 3,\narXiv:2305.11627.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016.\nPointer Sentinel Mixture Models. arXiv:1609.07843.\nMishra, A.; Latorre, J. A.; Pool, J.; Stosic, D.; Stosic, D.;\nVenkatesh, G.; Yu, C.; and Micikevicius, P. 2021. Acceler-\nating sparse deep neural networks. arXiv:2104.08378.\nMolchanov, P.; Tyree, S.; Karras, T.; Aila, T.; and Kautz, J.\n2017. Pruning Convolutional Neural Networks for Resource\nEfficient Inference. In International Conference on Learn-\ning Representations.\nRichards, T. B. 2023. Auto-GPT: An experimental open-\nsource attempt to make GPT-4 fully autonomous. https:\n//github.com/Significant-Gravitas/Auto-GPT. Accessed:\n2023-08-09.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow,\nD.; Castagn´e, R.; Luccioni, A. S.; Yvon, F.; Gall´e, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv:2211.05100.\nShi, D.; Tao, C.; Jin, Y .; Yang, Z.; Yuan, C.; and Wang, J.\n2023. Upop: Unified and progressive pruning for compress-\ning vision-language transformers. arXiv:2301.13741.\nSun, M.; Liu, Z.; Bair, A.; and Kolter, Z. 2023. A Simple\nand Effective Pruning Approach for Large Language Mod-\nels. arXiv:2306.11695.\nTan, C. M. J.; and Motani, M. 2020. Dropnet: Reducing\nneural network complexity via iterative pruning. In In-\nternational Conference on Machine Learning, 9356–9366.\nPMLR.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford\nalpaca. Accessed: 2023-08-\n09.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv:2302.13971.\nWei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Met-\nzler, D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.;\nDean, J.; and Fedus, W. 2022. Emergent Abilities of Large\nLanguage Models. In Transactions on Machine Learning\nResearch.\nXia, M.; Zhong, Z.; and Chen, D. 2022. Structured Pruning\nLearns Compact and Accurate Models. In Association for\nComputational Linguistics (ACL).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10872\nXiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and\nHan, S. 2023. SmoothQuant: Accurate and Efficient Post-\nTraining Quantization for Large Language Models. In In-\nternational Conference on Machine Learning.\nZafrir, O.; Larey, A.; Boudoukh, G.; Shen, H.; and\nWasserblat, M. 2021. Prune once for all: Sparse pre-trained\nlanguage models. arXiv:2111.05754.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al.\n2022. OPT: Open pre-trained transformer language models.\narXiv:2205.01068.\nZhou, A.; Ma, Y .; Zhu, J.; Liu, J.; Zhang, Z.; Yuan, K.; Sun,\nW.; and Li, H. 2021. Learning n: m fine-grained structured\nsparse neural networks from scratch. arXiv:2102.04010.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10873",
  "topic": "Pruning",
  "concepts": [
    {
      "name": "Pruning",
      "score": 0.7100376486778259
    },
    {
      "name": "Computer science",
      "score": 0.5109068751335144
    },
    {
      "name": "Language model",
      "score": 0.431151419878006
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4102481007575989
    },
    {
      "name": "Natural language processing",
      "score": 0.36003321409225464
    },
    {
      "name": "Biology",
      "score": 0.17541345953941345
    },
    {
      "name": "Agronomy",
      "score": 0.08973094820976257
    }
  ]
}