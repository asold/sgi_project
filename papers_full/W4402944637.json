{
  "title": "Systematic Characterization of the Effectiveness of Alignment in Large Language Models for Categorical Decisions",
  "url": "https://openalex.org/W4402944637",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2688074219",
      "name": "Isaac Kohane",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1594413493",
    "https://openalex.org/W2148962857",
    "https://openalex.org/W1543040011",
    "https://openalex.org/W4399120295",
    "https://openalex.org/W2057702222",
    "https://openalex.org/W2133393736",
    "https://openalex.org/W2113354881",
    "https://openalex.org/W4388102733",
    "https://openalex.org/W2963086506",
    "https://openalex.org/W2148370712",
    "https://openalex.org/W4401422445"
  ],
  "abstract": "Abstract As large language models (LLMs) are increasingly deployed in high-stakes domains like healthcare, understanding how well their decision-making aligns with human preferences and values becomes crucial, especially when we recognize that there is no single gold standard for these preferences. This paper applies a systematic methodology for evaluating preference alignment in LLMs on categorical decision-making with medical triage as a domain-specific use case. It also measures how effectively an alignment procedure will change the alignment of a specific model. Key to this methodology is a novel simple measure, the Alignment Compliance Index (ACI), that quantifies how effectively a LLM can be aligned to a given preference function or gold standard. Since the ACI measures the effect rather than the process of alignment, it is applicable to alignment methods beyond the in-context learning used in this study. Using a dataset of simulated patient pairs, three frontier LLMs (GPT4o, Claude 3.5 Sonnet, and Gemini Advanced) were assessed on their ability to make triage decisions consistent with an expert clinician’s preferences. The models’ performance before and after alignment attempts was evaluated using various prompting strategies. The results reveal significant variability in alignment effectiveness across models and alignment approaches. Notably, models that performed well, as measured by ACI, pre-alignment sometimes degraded post-alignment, and small changes in the target preference function led to large shifts in model rankings. The implicit ethical principles, as understood by humans, underlying the LLMs’ decisions were also explored through targeted questioning. These findings highlight the complex, multifaceted nature of decision-making and the challenges of robustly aligning AI systems with human values. They also motivate the use of a practical set of methods and the ACI, in the near term, to understand the correspondence between the variety of human and LLM decision-making values in specific scenarios.",
  "full_text": null,
  "topic": "Characterization (materials science)",
  "concepts": [
    {
      "name": "Characterization (materials science)",
      "score": 0.7161445617675781
    },
    {
      "name": "Categorical variable",
      "score": 0.7085555195808411
    },
    {
      "name": "Computer science",
      "score": 0.5345797538757324
    },
    {
      "name": "Natural language processing",
      "score": 0.46653324365615845
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35686156153678894
    },
    {
      "name": "Machine learning",
      "score": 0.21480289101600647
    },
    {
      "name": "Materials science",
      "score": 0.13894298672676086
    },
    {
      "name": "Nanotechnology",
      "score": 0.13477101922035217
    }
  ],
  "institutions": [],
  "cited_by": 2
}