{
  "title": "Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis",
  "url": "https://openalex.org/W4283794158",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2798347534",
      "name": "Liat Ein‐Dor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099371537",
      "name": "Ilya Shnayderman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3187432526",
      "name": "Artem Spector",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250654574",
      "name": "Lena Dankin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1855561177",
      "name": "Ranit Aharonov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2953063600",
      "name": "Noam Slonim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2798347534",
      "name": "Liat Ein‐Dor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099371537",
      "name": "Ilya Shnayderman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3187432526",
      "name": "Artem Spector",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250654574",
      "name": "Lena Dankin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1855561177",
      "name": "Ranit Aharonov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2953063600",
      "name": "Noam Slonim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970636124",
    "https://openalex.org/W2565756058",
    "https://openalex.org/W6784117431",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6748304040",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W6784411663",
    "https://openalex.org/W2904013400",
    "https://openalex.org/W2855214964",
    "https://openalex.org/W2522871251",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2952750383",
    "https://openalex.org/W2798524681",
    "https://openalex.org/W2294607529",
    "https://openalex.org/W3122051399",
    "https://openalex.org/W2924120895",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W3100110884",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3118043957",
    "https://openalex.org/W3100880133",
    "https://openalex.org/W4230872509",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W2950308662",
    "https://openalex.org/W3206996280",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3005296017",
    "https://openalex.org/W2610858497",
    "https://openalex.org/W627914465",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3023419341",
    "https://openalex.org/W3035419191",
    "https://openalex.org/W3106109117",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W4287815000",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3100554158",
    "https://openalex.org/W4297801177",
    "https://openalex.org/W4313897644",
    "https://openalex.org/W2962998183"
  ],
  "abstract": "In recent years, pretrained language models have revolutionized the NLP world, while achieving state of the art performance in various downstream tasks. However, in many cases, these models do not perform well when labeled data is scarce and the model is expected to perform in the zero or few shot setting. Recently, several works have shown that continual pretraining or performing a second phase of pretraining (inter-training) which is better aligned with the downstream task, can lead to improved results, especially in the scarce data setting. Here, we propose to leverage sentiment-carrying discourse markers to generate large-scale weakly-labeled data, which in turn can be used to adapt language models for sentiment analysis. Extensive experimental results show the value of our approach on various benchmark datasets, including the finance domain. Code, models and data are available at https://github.com/ibm/tslm-discourse-markers.",
  "full_text": "Fortunately, Discourse Markers Can Enhance Language Models for Sentiment\nAnalysis\nLiat Ein-Dor*, Ilya Shnayderman∗, Artem Spector∗, Lena Dankin,\nRanit Aharonov† and Noam Slonim\nIBM Research\n{liate,ilyashn,artems,lenad,noams}@il.ibm.com\nAbstract\nIn recent years, pretrained language models have revolu-\ntionized the NLP world, while achieving state-of-the-art\nperformance in various downstream tasks. However, in\nmany cases, these models do not perform well when la-\nbeled data is scarce and the model is expected to perform\nin the zero or few shot setting. Recently, several works\nhave shown that continual pretraining or performing a\nsecond phase of pretraining (inter-training), which is\nbetter aligned with the downstream task, can lead to im-\nproved results, especially in the scarce data setting. Here,\nwe propose to leverage sentiment-carrying discourse-\nmarkers to generate large-scale weakly-labeled data,\nwhich in turn can be used to adapt general-purpose lan-\nguage models to the task of sentiment classification. In\naddition, we propose a new method for adapting senti-\nment classification models to new domains. This method\nis based on automatic identification of domain-specific\nsentiment-carrying discourse markers. Extensive experi-\nmental results show the value of our approach on various\nbenchmark datasets. Code, models and data are available\nat https://github.com/ibm/tslm-discourse-markers.\nIntroduction\nLarge pretrained language models are reshaping the land-\nscape of NLP. These models, recently referred to as foun-\ndation models (Bommasani et al. 2021), were originally\nproposed with a two-step paradigm in mind. The model is\nfirst pretrained at scale on broad data with a surrogate self-\nsupervised task; the knowledge gained by this pretraining is\nthen transferred and adapted via fine-tuning on – typically\nsmall – labeled data, to a specific downstream task. Promi-\nnent examples include BERT (Devlin et al. 2019) and GPT-3\n(Brown et al. 2020). The practical value of this approach\nis immense. The self-supervised pretraining requires no la-\nbeled data. The resulting model represents a single powerful\nstarting point that can be swiftly adapted to address a wide\nrange of target tasks with relatively little annotation effort,\nvia few-shot or even zero-shot learning (Brown et al. 2020).\nSubsequent studies have shown that the original two-step\nparadigm can be further refined to yield an even better start-\n* These authors equally contributed to this work.\n†Current affiliation: Pangea Therapeutic, ranitah1@gmail.com\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ning point model for particular tasks of interest. For example,\ncontinuing the pretraining on domain-specific data such as\nfinance or legal documents have proven beneficial to tasks in\nthese domains (Araci 2019; Chalkidis et al. 2020; Gururan-\ngan et al. 2020). Similarly, additional pretraining of BERT on\ndialog data yields better results in target tasks related to dia-\nlogue application (Wu et al. 2020), and continual pretraining\nof BERT on product reviews with sentiment-aware pretrain-\ning tasks led to improved performance in sentiment analysis\nin this domain (Zhou et al. 2020). Another, more compu-\ntationally demanding option is to pretrain the model from\nscratch on self-supervised task(s) that aim to better reflect the\nnature of the target tasks. For example, the pretraining tasks\nof SpanBERT (Joshi et al. 2019) and PEGASUS (Zhang et al.\n2020a) are designed to be closer in spirit to span-extraction\ntasks as in question answering and to summarization tasks,\nrespectively, resulting in better performance in these target\ntasks.\nA related path, which is further explored in this work, is to\nadd an intermediate training step, referred to asinter-training,\nwhich is somewhat aligned with a specific target task of in-\nterest. There are several aspects by which these inter-training\napproaches differ. One main aspect is the similarity between\nthe intermediate task and the target task which ranges from\nfull alignment using weakly or readily available labeled data\n(Meng et al. 2020; Zhou et al. 2020; Huber et al. 2021) to\ntransfer learning using labeled data on a similar yet different\ntask (Pruksachatkun et al. 2020), and further including works\nwhich perform transfer learning with no labeled data, e.g.,\nShnarch et al. (2021) apply unsupervised text clustering and\nthen inter-train a model to predict the cluster label. Among\nthe approaches that rely on fully aligned intermediate tasks,\nsome works leverage weak labels that are inherent to the orig-\ninal text, like the explicit mention of the class name (Meng\net al. 2020) or the presence of the token ’that’ in a sentence\n(Levy et al. 2018); while others rely on non-textual signals\nlike human-added numeric review ratings (Zhou et al. 2020)\nor sentiment-bearing emojis (LeCompte and Chen 2017).\nWeak labels that are inherent to the text usually have lim-\nited coverage and involve bias towards specific keywords or\npatterns that define the weak signal. While the non-textual\nsignals usually do not suffer from these issues, since they are\nexternal to the text, they are often specific to task and domain\nand therefore are less directly applicable to new tasks and/or\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10608\nin new domains.\nThe present work suggests a new type of weak labels which\nare inherent to the original text, but at the same time can be\nperceived as an external label that can be removed from the\noriginal text while keeping the remaining text meaningful\nand grammatical (Moder and Martinovic-Zic 2004).\nSpecifically, we propose to leverage the signal carried by\nparticular discourse markers (DMs) to generate large amounts\nof weakly labeled data for the important task of sentiment\nanalysis (SA). For example, we assume that sentences follow-\ning the prefixes ”Happily,” and ”Sadly,” convey a positive sen-\ntiment and a negative sentiment, respectively. Exploiting this\nsimple assumption with a small seed of11 discourse markers,\nwe generate large amounts of weakly labeled data out of a\nlarge and general English corpus. Inter-training BERT-base\nand BERT-tiny on this data yields significant performance\nimprovements, especially when labeled data is scarce and\nin a zero-shot scenario. Moreover, we show how to use the\nobtained classifier to automatically reveal sentiment-carrying\ndiscourse markers in particular domains. Relying on these\ndomain-specific sentiment-carrying discourse-markers yields\nan additional performance gain in zero-shot learning, and\nmay further open the door for additional future applications.\nIn summary, our main contributions are:\n1. A novel approach that leverages sentiment signals of dis-\ncourse markers for creating sentiment-aware language\nmodels that significantly outperform prior models.\n2. A new method for enhancing domain-specific sentiment\nclassification, based on statistical analysis of discourse\nmarkers in a domain-specific corpus.\n3. A large dataset of weakly labeled sentences from\nWikipedia, and a code for generating weakly labeled data\nfrom a given text corpus.\nRelated Work\nLearning with Discourse Markers Discourse markers\n(DMs) are words or phrases that play a role in managing\nthe flow and structure of discourse. DMs have been used\nas a learning signal for the prediction of implicit discourse\nrelations (Liu and Li 2016; Braud and Denis 2016) and infer-\nence relations (Pan et al. 2018). The task of DM prediction\nhas been leveraged in several works (Jernite, Bowman, and\nSontag 2017; Nie, Bennett, and Goodman 2019; Sileo et al.\n2019), to learn general representations of sentences, which\ncan be transferred to various NLP classification tasks. Sileo\net al. (2020) were the first to systematically study the asso-\nciation between individual DMs and specific downstream\ntask classes. Using a model trained to predict discourse mark-\ners between sentence pairs, they predict plausible markers\nbetween sentence pairs with a known semantic relation (pro-\nvided by existing classification datasets). Based on these\npredictions, they study the link between discourse mark-\ners and the semantic relations annotated in classification\ndatasets.Here we show how such an association can be lever-\naged to enhance the performance of language models on a\ndownstream task, and furthermore in a particular domain.\nTask-aware Language Models. A recent line of works\nhas been focused on bridging the gap between the self-\nsupervision task and the downstream tasks which is inherent\nto multi-purpose pretrained models (Sun et al. 2019; Tian\net al. 2020; Chang et al. 2020). In Joshi et al. (2020), spans\nof texts are masked rather than single tokens, resulting in a\nlanguage model oriented to span-selection tasks. Chang et al.\n(2020) suggested a language model targeted at document\nretrieval, and Zhang et al. (2020b) pursued a similar goal\nfor abstractive text summarization. For sentiment analysis,\nseveral works have incorporated sentiment knowledge into\nthe pretraining task (Tian et al. 2020; Gu et al. 2020), while\nfocusing mainly on word-level sentiment prediction tasks.\nHere, in order to achieve full alignment with the downstream\ntask of sentence-level sentiment classification, we suggest a\nmodel that incorporates a sentence-level sentiment prediction\nobjective. Similar objective was used in Zhou et al. (2020),\nrelying on ratings as sentiment signals, which are specific to\nthe reviews domain. In contrast, our approach relies on senti-\nment signals that are carried by discourse markers, which are\nan inherent to language itself and are therefore available for\na wide range of domains.\nSenDM : A New Sentiment Language Model\nTraining DM-based Sentiment Models\nWe propose a general approach to develop DM-based senti-\nment models. Our approach relies on weakly labeled senti-\nment data set, which is automatically derived from a given\ncorpus by leveraging strong associations between DMs and\nsentiment classes, as depicted in figure 1. Given a corpus C\nand a list L of DMs that signal either a positive or a negative\nsentiment, each accompanied by its class label, we follow\nthe heuristic introduced by Rutherford and Xue (2015) and\nlook for all sentences in C that start with l ∈L followed\nby a comma. We then remove l and the comma from the\nbeginning of each sentence, and annotate all resultant sen-\ntences with the class label associated with\nl. This process\nresults in a binary classification dataset for sentiment analy-\nsis, which is used to fine-tune a pre-trained language model,\nM (inter-training). In this work we use the above flow to\ngenerate a new sentiment model, SenDM , and also to build\nan additional domain-adapted model as will be discussed in\nsection .\nThe SenDM Model\nWe introduce SenDM , a general sentiment model, that\naims to improve the performance of sentiment classifica-\ntion across domains. SenDM is obtained using the flow\ndescribed above, where C is a general corpus of newspaper\nand journal articles, denotedCg (see section ), andL is a seed\nlist of sentiment related DMs obtained manually using gen-\neral knowledge of the English language. More specifically,\nwe asked 3 annotators, to go over a list of 173 commonly\nused DMs described in Sileo et al. (2019), and mark any DM\nas positive/negative if it is likely to open a sentence bearing\na positive/negative sentiment, based on its common usage\nin the English language. The final list,\nLg, consists of 11\nDMs, selected by all 3 annotators. The DMs identified as as-\nsociated with a positive sentiment are: ’luckily’, ’hopefully’,\n10609\nFigure 1: Overview of how DM-based sentiment models are trained.\n’fortunately’, ’ideally’, ’happily’, and ’thankfully’. Those as-\nsociated with a negative sentiment are: ’sadly’, ’inevitably’,\n’unfortunately’, ’admittedly’, and ’curiously’. The resulting\nweakly labeled data is used to fine tune both the uncased base\nand tiny architectures of BERT (Devlin et al. 2019; Jiao et al.\n2020); We denote the resulting models by SenDM-base and\nSenDM-tiny, respectively, and release both of these models\nas part of this work.\nExperimental Setup\nThe General Corpus ( Cg) Our proposed solution relies\non the availability of a corpus of unlabeled text. We use a\ncorpus of some 400 million newspaper and journal articles1,\nbreaking the articles into sentences, and indexing these sen-\ntences. We focus on English sentences 2 and following Sileo\net al. (2019) we use only sentences which are 3 −32 tokens\nin length and have balanced parentheses.\nInter-training Details The inter-training step (Figure 1)\nconsists of fine-tuning BERT using weakly labeled data.\nFor inter-training SenDM , we obtain a total of 1, 876, 614\nweakly labeled sentences, by using the list of sentiment-\nrelated DMs, Lg, over sentences inCg, as described in section\n. We divide the samples into training ( 80%), development\n(10%), and test (10%) sets. We set the learning rate to5e −5,\nand the batch size to 32. We use the early stopping strat-\negy, setting the max number of epochs to 4 and selecting\nthe model with the best accuracy on the development set.\nThe dropout probability is always kept at 0.1. We employ an\nAdam optimizer with β1 = 0.9, β2 = 0.999, and ϵ = 1e−06.\nTraining is performed on two V100 GPUs.\nEvaluation Details\nEvaluation is performed on the datasets appearing below, in\nthree scenarios: zero-shot, few-shot and full-data. For zero\nshot we simply use the classification layer obtained from\ninter-training. For few-shot, we further fine-tune the inter-\ntrained model with a small sample of\nn examples from the\ntraining set, with n ranging from 16 to 1024. We repeat each\nexperiment five times with different random seeds, each time\n1From the LexisNexis 2011-2018\ncorpus,https://www.lexisnexis.com/en-us/home.page\n2Specifically, sentences with probability > 75% of being En-\nglish, based on Fast-Text langid from Grave et al. (2018).\nselecting different examples for fine-tuning. In the full-data\nscenario, all training examples are used for fine-tuning.\nTo support training on small samples, the batch size is\nset to 16. The other hyper-parameters are the same as in\nthe inter-training phase described above, with one exception.\nFor the few-shot scenario, which represents a low resource\nsetting, we assume that no development set is available for\nemploying the early stopping strategy. Instead, we follow the\nobservation in Zhang et al. (2020c) that for small training\ndata, more iterations help stabilize BERT results, and set the\nnumber of epochs to 10.\nDatasets The datasets used for evaluation are presented in\nTable 1. All datasets contain sentences that are labeled for\nsentiment. amazon, sst2, and yelp consist of review sentences.\nfpb75 is comprised of sentences from financial news.Most\nof these datasets provide more than two possible labels, so\nwe adjust the datasets for the binary sentiment classification\ntask. Specifically, fpb75 contains sentences that are labeled\nas neutral, which we remove from the training and test sets.\namazon and yelp contain five different labels that reflect the\nsentiment ratings of each sentence (”stars”). We leave only\nsentences with the lowest and highest scores, considering\nthose as negative and positive labels, respectively. For fine\ntuning we use up to 1024 examples from the training set. For\ntesting we use the entire test set.\nResults\nWe now evaluate the performance of both the base and tiny\nversions of the general sentiment model (SenDM-base and\nSenDM-tiny) on datasets from different domains. Since our\nDataset Domain Test set size\namazon Product reviews 2K\nyelp Business reviews 20K\nsst2 Movie reviews 1821\nfpb75 Financial news 691\nTable 1: Datasets used for evaluation. References for the\ndatasets are as follows, by the order appearing in the table:\n(Keung et al. 2020), (Zhang, Zhao, and LeCun 2015), (Wang\net al. 2018), (Malo et al. 2014).\n10610\namazon\nyelp\nsst2\nfpb75\nbase\n tiny\nFigure 2: Performance of SenDM and baselines on four datasets given different amounts of training examples. Left column:\nbase-size models. Right column: tiny-size models. Lines indicate the mean and shaded areas indicate the standard deviation\nover the five seeds (see section for details). Dashed horizontal lines indicate the fine-tuning results for the full training data\n(the full-data setting). Dotted horizontal lines indicate the prior of the common class in the dataset. FinBERT and SentiX are\navailable only in the base size.\n10611\nmain focus is on the zero and few-shot setting, we report the\nresults after fine tuning over 0 up to 1024 training examples.\nFigure 2 shows the accuracy of SenDM-base (left col-\numn) and SenDM-tiny (right column), for all datasets, vs. the\nnumber of examples used for fine tuning. The accuracy is\ncompared to that of vanilla BERT – base and tiny respectively,\nand to SentiX (Zhou et al. 2020), that are fine tuned over the\nsame labeled examples. For fpb75, we add the corresponding\ndomain specific version of BERT-base – FinBERT (Araci\n2019).\nIn all datasets, SenDM significantly outperforms the\nBERT baselines, including the finance-specific FinBERT,\nespecially when the number of examples used for fine tuning\nis relatively small. The gain in performance is even more\nsignificant when focusing on the tiny architecture. This is\nespecially evident in the fpb75 dataset, where BERT-tiny\ncompletely fails to learn with up to\n256 examples, whereas\nthe SenDM-tiny is able to learn with as few as 16 examples.\nA similar trend can be seen for the sst2 dataset. As expected,\nthe gap between SenDM and its counterparts decreases with\nthe increasing number of training examples, reflecting the\ndecaying effect of the initial weights on the fine-tuned model.\nIn most datasets, this gap completely vanishes in the full-\ndata scenario, with the exception of fpb75, where the full\ntraining-data is relatively small (< 2.8K samples).\nFrom the stability perspective, SenDM is more robust\nto changes in the initial seed compared to the other models,\ndue to the lack of randomness in the initialization of its\nclassification head.\nSentiX is a sentiment-aware pre-trained language model\nthat was originally designed for cross-domain review sen-\ntiment analysis. Importantly, SentiX is trained on large\namounts of Yelp and Amazon reviews, along with their as-\nsociated star rating, the same star rating used to define the\ntraining set and the test set in our amazon and yelp datasets.\nThus, one can not report zero/few-shot training results for\nthis model in these two datasets, since the available model\nis already trained on large amounts of the respective train\ndata. That said, it is intriguing to explore the performance\nof this model over our two other datasets. When considering\nthe results in sst2, which is based on movie reviews, we see\nstrong performance for SentiX. This is expected, since this\nkind of data, composed of starred reviews – albeit from a dif-\nferent domain – is precisely the forte of SentiX. Interestingly,\nthough, its gap compared to our SenDM is relatively small,\nand insignificant when fine tuning over 16 and 32 examples.\nConsidering the results in a more distant domain, namely the\nfpb75 dataset, where starred reviews are irrelevant, we see\nthe clear value of our approach, that consistently outperforms\nall other models, including SentiX, typically by a significant\nmargin, especially when labeled data is scarce. These results\nsupport our hypothesis that pretraining based on sentiment-\nrelated DMs will result in a more robust model, that yields\nsuperior performance when tested on various domains.\nA concern may arise, that the strong performance demon-\nstrated by our approach on fpb75is related to the fact that the\ngeneral corpus giving rise to the weak labels used for inter-\ntraining, also contains some financial documents, and that the\nresults will be inferior for domains not covered in the corpus\nwe start with. To address this concern, we generate a version\nof SenDM , in which financial documents are removed from\nthe general corpus3. We find that there is no deterioration of\nresults, supporting the notion that the observed improvement\nover alternative methods is not due to inter-training using\nfinancial documents – see Figure 1 in the Appendix.\nAnother concern we wanted to examine is related to the\nrelevance of our approach for low resource languages, where\na very large corpus like Cg is not available. To this end,\nwe checked the sensitivity of the results to the size of the\nweakly-labeled data, by creating two versions of\nCg, one\nbased on inter-training using only 10%, and the other based\non only 1%, of the weakly labeled data. Surprisingly, these\ntwo models resulted in no detrimental effect on the results.\nIn addition, one may also leverage the large English weakly\nlabeled data for inter-training the multilingual BERT model\n(M-BERT). We leave the examination of this approach for\nfuture research.\nTo summarize, overall, the proposed DM-based sentiment\nmodel, significantly improves sentiment classification perfor-\nmance for both small and large language models. Remark-\nably, even the tiny version ofSenDM outperforms the much\nlarger BERT-base baseline.\nAdapting SenDM to a New Domain\nIn section we saw that SenDM , which leverages a general\nlist of sentiment-related DMs, improves results over baselines\non all datasets, including the finance dataset, fpb75. Here\nwe investigate whether adapting SenDM to a new domain,\ncan further improve its performance on that domain. We\nchoose to study this on the financial domain since as stated\nin Araci (2019), financial sentiment analysis is a challenging\ntask due to the specialized language and lack of domain-\nspecific labeled data. Moreover, it is an important task for\nmany potential users, and finally, the adaptation impact can\nbe tested given the availability of the fpb75 dataset.\nThe Training Approach\nThe robustness of SenDM presumably emerges from the\nmulti-domain corpus it relies on, and the general nature of the\nDM list, Lg, composed of discourse markers that are abun-\ndantly used and carry a general sentiment signal. However,\ndue to potential domain-specific jargon and language style,\ngiven a domain specific text corpus\nCd, it may be useful to\nbuild domain specific sentiment models.\nWe study five ways to build domain specific sentiment\nmodels based on the general flow described in Figure 1. All\nfive resulting models, described in the bottom part of Table\n2, are based on weakly labeled sentences from the domain-\nspecific corpus, Cd. All five models rely on the availability of\na general sentiment model, trained in a domain-independent\nmanner, such as SenDM , which we release to the commu-\nnity. In the experiments we describe here, we use a variant\nof\nSenDM , denoted below SenDM ∗, which is developed\nas SenDM , but after removing finance-related documents\nfrom the general corpus Cg, to better simulate the finance\n3Based on topic tagging, see details in section .\n10612\nModel name C L M With self-\ntraining\nSenDM Cg Lg BERT NA\nSenDM Lg\nd Cd Lg SenDM No\nSemDM Ld\nd Cd Ld SenDM No\nSenDM P\nd Cd NA SenDM Yes\nSenDM Lg+P\nd Cd Lg SenDM Yes\nSemDM Ld+P\nd Cd Ld SenDM Yes\nTable 2: Sentiment language models and the corresponding\nassignment of C, L and M in the flow described in figure 1,\nas well as whether the predictions of SenDM are used for\nassigning weak labels (”With self-training” – see main text\nfor details) .SenDM is a general (multi-domain) model. The\nother five are domain specific. Cg: a general, multi-domain,\ncorpus; Cd: a corpus from domain d; Lg: a list of DMs asso-\nciated with sentiment in the English language; Ld: such a list\nadapted to domain d.\ndomain as a new domain.4 This SenDM ∗ model is used as\nthe straining point for inter-training all five models, and in\nsome cases to define the inter-training weakly labeled data,\nas described next.\nThe first model, SenDM Lg\nd , uses the general DM list, Lg,\nfor weak label extraction from text in the target domain. How-\never, since sentiment-related DMs might be domain specific\nwe develop a method to extract a domain-specific sentiment-\nrelated DM list, Ld. To that end, we note that there is no need\nto define a DM using the standard linguistic definition, and\na functional definition can be used instead. Thus, we define\nas a sentiment-related DM , any n-gram (n <= 3) followed\nby a comma, for which the set of sentences it opens is en-\nriched with highly confident positive/negative predictions, as\ndetermined by SenDM ∗. The second model, SemDM Ld\nd ,\nrelies on a list composed of such DMs instead of Lg. As\na third approach, we perform one step self-training, where\nthe high confidence predictions of SenDM ∗ over sentences\nfrom Cd are used for inter-training, ignoring the DMs.5 This\nmodel is denoted by SenDM P\nd . Finally, aiming to reduce\nlabeling noise, we explore a synergistic approach, i.e., taking\nas weakly labeled positive/negative sentences only those for\nwhich an opening DM conveys a sentiment signal, and that\nsentiment is consistent with the high confidence prediction\nof SenDM ∗. We study this approach with our two DM lists,\nLg and Ld, giving rise to two additional models, denoted\nSenDM Lg+P\nd and SemDM Ld+P\nd , respectively.\nDomain Specific Sentiment-Related DMs We now turn\nto describe how we generate the domain specific sentiment-\n4In practical applications, though, users can obviously use our\nSenDM directly when building a domain-specific model.\n5In this work we use predictions withscore >0.9 and score <\n0.1 as positive/negative weak labels.\nrelated DM list, Ld, given the domain specific corpus Cd and\nusing SenDM ∗. Note, we are not interested in identifying all\ndomain specific sentiment related DMs. Rather, we are seek-\ning a precision oriented list, for the purpose of obtaining a\nhigh-quality weakly labeled set of sentences for inter-training\nprocess. Thus, we perform strict automatic filtering in the\nprocess. The idea is to first identify all n-grams that may\npotentially serve as DMs, and then identify whether the set\nof sentences they open is enriched with positive/negative\nsentiment, based on the predictions of SenDM ∗.\nThe first step consists of identifying a list of candidate\nDMs. To this end we first identify all unigrams, bigrams,\nand trigrams, that, followed by a comma, open sentences\nin\nCd, and further group these using NER (e.g., instead of\nmultiple bigrams of the type ”on Sept 9th”, ”on 10/2/2020”,...\nwe generate one bigram ”on DATE”). The candidate list\nconsists of the 1000 most frequent DMs. Specific filters may\nbe further applied depending on the domain corpora – see\nappendix for details.\nThe second step consists of using SenDM∗ to select\nthe domain-specific DMs out of the candidate list. We\nanalyse the sentences that start with the DMs in the above\ncandidate list, to find those DMs whose associated sentences\nare significantly associated with a highly confident predic-\ntion of positive/negative sentiment. For each candidate DM,\nwe sample 1000 sentences from the set of all sentences that\nstart with the DM followed by a comma, and assign each\nof these sentences with a sentiment if it is scored with high\nconfidence6 by SenDM ∗. For each candidate DM, we per-\nform a statistical analysis of the sentiment of its associated\nsentences, on the sample of\n1000 sentences, provided that\nthey are not too repetitive based on token counts, and that\nthe sentiment class with the higher count comprises at least\n85% of the sentences assigned a sentiment. A DM is con-\nsidered to be associated with a positive/negative sentiment\nif the p-value of the positive/negative class is smaller than\n0.01 after Bonferroni correction for multiple tests, based on\na Hypergeometric test. We release the code that allows a user\nthat has a corpus of interest to use SenDM to generate a\nspecific DM list adapted to the corpus, as described above.\nExperimental Setup\nThe inter-training and evaluation details are identical to those\ndescribed in Section . For the number of weakly labeled\nsamples used for inter-training the finance-specific sentiment\nmodels, see Table 1 in the appendix. In all cases we divide\nthe samples into training (80%), development (10%), and test\n(10%) sets.\nFinance-specific Corpora From the General Corpus, Cg,\nwe can define a sub-corpus that is focused on the financial\ndomain, using a provided metadata topic field, and filtering\nonly for articles from the topic ’Finance’. We term this corpus\nCfin and use it for studying adaptation to the finance domain.\nSenDM∗. As we are interested in studying the scenario\nof adapting to a new domain, which is possibly not covered\nby the data used to train SenDM , and since the corpus used\n6< 0.1 or > 0.9\n10613\nFigure 3: Performance of the general model and the various\ndomain specific models on the finance dataset, fpb75, for\nthe zero shot setting. SenDM ∗ is the general model when\ntrained on a corpus not containing financial documents – see\nmain text for details. All models are base size. In all models\nthe domain d is finance. The dashed horizontal line indicates\nthe prior of the common class in fpb75.\nto train SenDM does contain some financial documents, we\ngenerate a variant of SenDM excluding the finance domain.\nThis model, denoted by SenDM ∗, is trained analogously to\nSenDM , except we exclude financial documents from Cg\nbefore using it. Naturally, we do not expect a user to train such\na model, it is used here and in the appendix, only to examine\nto what extent our approach can generalize to domains not\ncovered by the general corpus used to train SenDM .\nResults\nLeveraging SenDM ∗ and Lfin , we generate the five ver-\nsions of domain specific sentiment models described in (see\nTable 3 for the DMs in Lfin 7.). Figure 3 depicts the accu-\nracy of the finance specific sentiment models, on the finance\ndataset fpb75, for the zero shot setting, in comparison to the\naccuracy of the general model, SenDM ∗. As can be seen,\nindeed using the domain specific DMs rather than the general\nlist improves the accuracy (blue vs. orange, and brown vs.\npurple bars). One step self-training is valuable even without\ncombining it with a signal from DMs (red vs. green bars).\nCombining one step self-training in a synergistic manner\nwith the signals from the DMs as described above brings\nadditional value (purple vs. orange and brown vs. blue bars).\nThe highest accuracy is achieved when combining the sig-\nnal of the finance-specific DMs with one step self training\n(brown bar). All above accuracy comparisons are significant\n(p < 0.05), based on McNemar’s test. It is interesting to\nnote that using Lg for extracting the weak labels from the\nfinance corpus, results in lower performance than using the\n7We also use SenDM ∗ instead of SenDM to identify Lfin\nfor the reasons described above\ngeneral model (orange vs. green bars). This may be attributed\nto a higher noise in the signal of the Lg DMs in the finance\ncorpus compared to the general corpus. This explanation is\nconsistent with the improvement gained by incorporating\nself-training in a synergistic manner with Lg, a step that re-\nsults in noise reduction. For the few shot scenario we do not\nfind significant differences between the models – see Figure\n2 in the Appendix.\nWe note that the suggested approach may be applied iter-\natively to gain further improvement on the finance dataset.\nMoreover, this adaptation process can also be applied to\nSenDM itself. We leave these directions for future work.\nAnalysis of Domain Specific Sentiment DMs As we saw\nabove, leveraging the finance specific DMs, was useful for\nadapting the sentiment model to the finance domain. Table\n3 lists the sentiment-related DMs extracted from general\nEnglish, Lg, as well as from the financial domain.\nAlthough some of the finance-specific DMs echo those\nappearing in general English usage (e.g., ’fortunately’), many\nDMs are domain specific, and in fact may not be consid-\nered a DM by the standard linguistic definition. For example,\nthe bigram ’ORG CEO’ (’Walmart’s CEO’, ’BOA’s CEO’,\netc.), is associated with a positive sentiment. This might be\nsurprising at first sight, and would probably not be listed\nin a manually curated finance-specific DM list, but in hind-\nsight makes sense. When considering what sentences might\nfollow such an opening, one would expect them to discuss\npositive things about the company. Another such example\nis ’under the leadership’. Here again, we find, that although\nnot expected a-priori, most sentences that follow this open-\ning, would carry a positive sentiment due to the reference to\nleadership.\nNext, we were interested to see how sentiment DMs vary\nin other domains. We carved out several domain corpora out\nof the general corpus. Beyond the Finance Corpus described\nabove we also introduce: (1) The Sports Corpus: Similarly\nto the creation of the Finance Corpus but filtering for articles\nfrom the topic ’Sports’; and (2) The Science Corpus: This\ncorpus focuses on scientific articles from scientific journals,\nand is defined as any article in Cg that was published in one\nof the journals included in the list of journal impact factors8.\nThese lists can be found in Table 3. We find that some\nDMs continue to be ubiquitous across domains (e.g., ’fortu-\nnately’), but others seem to be associated with sentiment in\nspecific domains only. An interesting example is the word\n’women’, which in the scientific domain, is among the list of\nnegatively associated DMs. We found that in scientific papers,\nsentences beginning with ’women’ will often deal with issues\nlike oppression and deprivation, and thus are associated with\na negative sentiment.\nDiscussion\nThis work suggests to leverage DMs that carry a sentiment\nsignal to inter-train and adapt general-purpose language mod-\nels to the sentiment classification task. The obtained senti-\nment analysis models demonstrate significant performance\n8https://www.scimagojr.com/journalrank.php\n10614\nDomain Associated with Posi-\ntive Sentiment\nAssociated with Neg-\native Sentiment\nGeneral ’fortunately’, ’hap-\npily’, ’hopefully’,\n’ideally’, ’luckily’,\n’thankfully’\n’admittedly’, ’curi-\nously’, ’inevitably’,\n’sadly’, ’unfortu-\nnately’\nFinance ’as ORG’, ’at the\nevent’, ’fortunately’,\n’hopefully’, ’ide-\nally’, ’in future’,\n’in other business’,\n’luckily’, ’once com-\npleted’,’ORG CEO’,\n’starting DATE’,\n’thankfully’, ’the pro-\ngram’, ’this way’, ’to\nachieve this’, ’under\nhis leadership’, ’with\nORG’\naccording to police’,\n’sadly’, ’the problem’,\n’the problem is’, ’un-\nfortunately’, ’worse’\nSports ’beginning DATE’,\n’fortunately’, ’in the\nfuture’, ’luckily’,\n’thankfully’, ’that\nway’\n’admittedly’, ’alas’,\n’granted’, ’ironi-\ncally’, ’sadly’, ’true’,\n’unfortunately’, ’un-\nfortunately for ORG’\nScience ’established in DATE’,\n’if necessary’, ’if\npossible’, ’if success-\nful’, ’luckily’, ’that\nway’, ’to address\nthis’, ’when possible’,\n’whenever possible’,\n’where possible’,\n’with this approach’\n’admittedly’, ’at\nORDINAL glance’,\n’at times’, ’curiously’,\n’even then’, ’even\nworse’, ’in part’,\n’inevitably’, ’paradox-\nically’, ’predictably’,\n’regrettably’, ’the\nproblem’, ’there was’,\n’too often’, ’unsurpris-\ningly’, ’without it’,\n’women’\nTable 3: Sentiment-related DMs. The lists below the double\nline are domain specific. The upper case tokens are NER tags.\nboost across multiple domains, most notably in the zero-shot\nand few-shot learning scenarios, emphasizing the practical\nvalue of this work. We further show how to evolve the ob-\ntained models to a specific domain of interest, using automat-\nically identified domain-specific DMs which are associated\nwith a specific sentiment class. We show how this approach\nyields a further performance enhancement in zero-shot learn-\ning within the challenging finance domain.\nThe ability to bootstrap a general, small, and easily iden-\ntified seed of sentiment carrying DMs into a powerful senti-\nment analysis model may hold additional valuable implica-\ntions. For example, this approach can be easily adapted to\nlanguages beyond English, including low resource languages,\nas long as a reasonably sized corpus is available. Another\ninteresting direction would be to expand the proposed ap-\nproach for targeted sentiment analysis. For example, in the\nfinance domain, the company appearing in a sentence can be\nconsidered as the sentiment target.\nSileo et al. (2020) show that various NLP task classes are\nnaturally associated with specific DMs. Thus, the methodol-\nogy presented here for leveraging DMs to create task-specific\nlanguage models can be potentially applied to tasks beyond\nsentiment analysis. Finally, DMs probably represent only\none type of linguistic cues among the richness of signals in\nnatural language that can be leveraged as self-supervision to\nalign LMs with downstream tasks.\nAppendix\nFilters Applied to the Domain-Specific DM Selection\nIn some domains certain DMs may be very specific to a\njournal, or even a reporter. We find this is the case in the\nfinance domain. Hence, we further filter out DMs whose\nsentences originate from a relatively narrow set of journals.\nFor this purpose we define the entropy of a DM, based on the\nprobability distribution of its sentences across journals, and\nfilter out 30% DMs with lowest entropy. Furthermore, in the\ncase of the finance domain, since the sentiment analysis task\nis to identify a sentiment with respect to a company, we apply\nthe above process only to sentences mentioning a company\nname, where we use the list of all companies traded in one of\nthe five major stock exchanges9.\nAdditional Tables and Figures\nModel name Total number of samples used\nfor intermediate training\nSenDM 1,876,614\nSenDM ∗ 1,815,943\nSenDM Lg\nd 60,671\nSemDM Ld\nd 99,521\nSenDM P\nd 490,989\nSenDM Lg+P\nd 45,246\nSemDM Ld+P\nd 70,681\nTable 4: The number of samples used as weak labels to train\neach sentiment model. In all cases we divide these into train-\ning (80%), development (10%) and test (10%) sets.\n9Note that this sentence selection step is not applied to the\nfinance corpus which contains all sentences from finance documents\n10615\nFigure 4: Performance of SenDM-base and base sized base-\nlines on fpb75 given different amounts of fine-tuning exam-\nples. Lines indicate the mean and shaded areas indicate the\nstandard deviation over the five seeds (see Section 3.4 for\ndetails). The dashed horizontal line indicates the prior of\nthe common class in the dataset.\nSenDM ∗ is the same as\nSenDM when trained excluding financial documents.\nFigure 5: Performance of the general model and the various\ndomain specific models on the finance dataset fpb75 for the\nzero and few shot settings. ∗: SenDM ∗ is the general model\nwhen trained on the general corpus excluding financial docu-\nments – see Section 4.2 for details. In all models the domain\nd is finance. The dashed horizontal line indicates the prior of\nthe common class in fpb75.\nReferences\nAraci, D. 2019. FinBERT: Financial Sentiment Analysis with\nPre-trained Language Models. ArXiv, abs/1908.10063.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora,\nS.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.;\nBrunskill, E.; et al. 2021. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258.\nBraud, C.; and Denis, P. 2016. Learning Connective-based\nWord Representations for Implicit Discourse Relation Identi-\nfication. In Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, 203–213. Austin,\nTexas: Association for Computational Linguistics.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33: 1877–\n1901.\nChalkidis, I.; Fergadiotis, M.; Malakasiotis, P.; Aletras, N.;\nand Androutsopoulos, I. 2020. LEGAL-BERT: The Muppets\nstraight out of Law School. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, 2898–2904.\nOnline: Association for Computational Linguistics.\nChang, W.-C.; Yu, F. X.; Chang, Y .-W.; Yang, Y .; and Kumar,\nS. 2020. Pre-training tasks for embedding-based large-scale\nretrieval. arXiv preprint arXiv:2002.03932.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), 4171–4186. Minneapolis,\nMinnesota: Association for Computational Linguistics.\nGrave, E.; Bojanowski, P.; Gupta, P.; Joulin, A.; and Mikolov,\nT. 2018. Learning Word Vectors for 157 Languages. In\nProceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018). Miyazaki,\nJapan: European Language Resources Association (ELRA).\nGu, Y .; Zhang, Z.; Wang, X.; Liu, Z.; and Sun, M. 2020.\nTrain no evil: Selective masking for task-guided pre-training.\narXiv preprint arXiv:2004.09733.\nGururangan, S.; Marasovi ´c, A.; Swayamdipta, S.; Lo, K.;\nBeltagy, I.; Downey, D.; and Smith, N. A. 2020. Don’t stop\npretraining: adapt language models to domains and tasks.\narXiv preprint arXiv:2004.10964.\nHuber, P.; Aghajanyan, A.; O˘guz, B.; Okhonko, D.; tau Yih,\nW.; Gupta, S.; and Chen, X. 2021. CCQA: A New Web-\nScale Question Answering Dataset for Model Pre-Training.\narXiv:2110.07731.\nJernite, Y .; Bowman, S. R.; and Sontag, D. 2017. Discourse-\nBased Objectives for Fast Unsupervised Sentence Represen-\ntation Learning. arXiv:1705.00557.\nJiao, X.; Yin, Y .; Shang, L.; Jiang, X.; Chen, X.; Li, L.; Wang,\nF.; and Liu, Q. 2020. TinyBERT: Distilling BERT for Natural\nLanguage Understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, 4163–4174.\nOnline: Association for Computational Linguistics.\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\nand Levy, O. 2019. SpanBERT: Improving Pre-training by\nRepresenting and Predicting Spans. CoRR, abs/1907.10529.\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.; and\nLevy, O. 2020. Spanbert: Improving pre-training by repre-\nsenting and predicting spans. Transactions of the Association\nfor Computational Linguistics, 8: 64–77.\n10616\nKeung, P.; Lu, Y .; Szarvas, G.; and Smith, N. A. 2020. The\nMultilingual Amazon Reviews Corpus. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language\nProcessing.\nLeCompte, T.; and Chen, J. 2017. Sentiment Analysis of\nTweets Including Emoji Data. In 2017 International Confer-\nence on Computational Science and Computational Intelli-\ngence (CSCI), 793–798.\nLevy, R.; Bogin, B.; Gretz, S.; Aharonov, R.; and Slonim, N.\n2018. Towards an argumentative content search engine using\nweak supervision. In Proceedings of the 27th International\nConference on Computational Linguistics, 2066–2081.\nLiu, Y .; and Li, S. 2016. Recognizing Implicit Discourse Re-\nlations via Repeated Reading: Neural Networks with Multi-\nLevel Attention. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing, 1224–\n1233. Austin, Texas: Association for Computational Linguis-\ntics.\nMalo, P.; Sinha, A.; Korhonen, P.; Wallenius, J.; and Takala, P.\n2014. Good debt or bad debt: Detecting semantic orientations\nin economic texts. Journal of the Association for Information\nScience and Technology, 65.\nMeng, Y .; Zhang, Y .; Huang, J.; Xiong, C.; Ji, H.; Zhang, C.;\nand Han, J. 2020. Text classification using label names only:\nA language model self-training approach. arXiv preprint\narXiv:2010.07245.\nModer, C. L.; and Martinovic-Zic, A. 2004.Discourse across\nlanguages and cultures, volume 68. John Benjamins Publish-\ning.\nNie, A.; Bennett, E.; and Goodman, N. 2019. DisSent: Learn-\ning Sentence Representations from Explicit Discourse Re-\nlations. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 4497–4510. Flo-\nrence, Italy: Association for Computational Linguistics.\nPan, B.; Yang, Y .; Zhao, Z.; Zhuang, Y .; Cai, D.; and He,\nX. 2018. Discourse Marker Augmented Network with Re-\ninforcement Learning for Natural Language Inference. In\nProceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), 989–\n999. Melbourne, Australia: Association for Computational\nLinguistics.\nPruksachatkun, Y .; Phang, J.; Liu, H.; Htut, P. M.; Zhang, X.;\nPang, R. Y .; Vania, C.; Kann, K.; and Bowman, S. R. 2020.\nIntermediate-task transfer learning with pretrained models\nfor natural language understanding: When and why does it\nwork? arXiv preprint arXiv:2005.00628.\nRutherford, A.; and Xue, N. 2015. Improving the inference of\nimplicit discourse relations via classifying explicit discourse\nconnectives. In Proceedings of the 2015 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 799–808.\nShnarch, E.; Gera, A.; Halfon, A.; Dankin, L.; Choshen, L.;\nAharonov, R.; and Slonim, N. 2021. Cluster & Tune: Enhance\n{BERT}Performance in Low Resource Text Classification.\nSileo, D.; de Cruys, T. V .; Pradel, C.; and Muller, P. 2020.\nDiscSense: Automated Semantic Analysis of Discourse\nMarkers. arXiv:2006.01603.\nSileo, D.; Van De Cruys, T.; Pradel, C.; and Muller, P. 2019.\nMining Discourse Markers for Unsupervised Sentence Rep-\nresentation Learning. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), 3477–3486. Minneapolis, Min-\nnesota: Association for Computational Linguistics.\nSun, Y .; Wang, S.; Li, Y .; Feng, S.; Chen, X.; Zhang, H.; Tian,\nX.; Zhu, D.; Tian, H.; and Wu, H. 2019. Ernie: Enhanced\nrepresentation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nTian, H.; Gao, C.; Xiao, X.; Liu, H.; He, B.; Wu, H.; Wang,\nH.; and Wu, F. 2020. SKEP: Sentiment knowledge en-\nhanced pre-training for sentiment analysis. arXiv preprint\narXiv:2005.05635.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bow-\nman, S. 2018. GLUE: A Multi-Task Benchmark and Analysis\nPlatform for Natural Language Understanding. In Proceed-\nings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, 353–355. Brus-\nsels, Belgium: Association for Computational Linguistics.\nWu, C.-S.; Hoi, S. C.; Socher, R.; and Xiong, C. 2020. TOD-\nBERT: Pre-trained Natural Language Understanding for Task-\nOriented Dialogue. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP), 917–929. Online: Association for Computational\nLinguistics.\nZhang, J.; Zhao, Y .; Saleh, M.; and Liu, P. 2020a. PEGA-\nSUS: Pre-training with Extracted Gap-sentences for Abstrac-\ntive Summarization. In III, H. D.; and Singh, A., eds., Pro-\nceedings of the 37th International Conference on Machine\nLearning, volume 119 of Proceedings of Machine Learning\nResearch, 11328–11339. PMLR.\nZhang, J.; Zhao, Y .; Saleh, M.; and Liu, P. 2020b. Pega-\nsus: Pre-training with extracted gap-sentences for abstractive\nsummarization. In International Conference on Machine\nLearning, 11328–11339. PMLR.\nZhang, T.; Wu, F.; Katiyar, A.; Weinberger, K. Q.; and Artzi,\nY . 2020c. Revisiting few-sample BERT fine-tuning. arXiv\npreprint arXiv:2006.05987.\nZhang, X.; Zhao, J.; and LeCun, Y . 2015. Character-Level\nConvolutional Networks for Text Classification. In Proceed-\nings of the 28th International Conference on Neural Infor-\nmation Processing Systems - Volume 1, NIPS’15, 649–657.\nCambridge, MA, USA: MIT Press.\nZhou, J.; Tian, J.; Wang, R.; Wu, Y .; Xiao, W.; and He, L.\n2020. SentiX: A Sentiment-Aware Pre-Trained Model for\nCross-Domain Sentiment Analysis. In Proceedings of the\n28th International Conference on Computational Linguistics,\n568–579. Barcelona, Spain (Online): International Commit-\ntee on Computational Linguistics.\n10617",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7770423889160156
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7685781717300415
    },
    {
      "name": "Sentiment analysis",
      "score": 0.715096116065979
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5304075479507446
    },
    {
      "name": "Language model",
      "score": 0.5259062051773071
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5077658891677856
    },
    {
      "name": "Source code",
      "score": 0.49780869483947754
    },
    {
      "name": "Labeled data",
      "score": 0.49135270714759827
    },
    {
      "name": "Natural language processing",
      "score": 0.48291346430778503
    },
    {
      "name": "Task (project management)",
      "score": 0.4522576332092285
    },
    {
      "name": "IBM",
      "score": 0.4492003321647644
    },
    {
      "name": "Machine learning",
      "score": 0.443813681602478
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.43462979793548584
    },
    {
      "name": "Programming language",
      "score": 0.07958328723907471
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Nanotechnology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": []
}