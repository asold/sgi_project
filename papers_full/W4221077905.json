{
  "title": "Histopathological image classification based on self-supervised vision transformer and weak labels",
  "url": "https://openalex.org/W4221077905",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5041839356",
      "name": "Ahmet Gokberk Gul",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A5069959246",
      "name": "Özdemir Çetin",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A5068140857",
      "name": "Christoph Reich",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A5089352173",
      "name": "Nadine Flinner",
      "affiliations": [
        "Goethe University Frankfurt",
        "University Hospital Frankfurt"
      ]
    },
    {
      "id": "https://openalex.org/A5055186543",
      "name": "Tim Prangemeier",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A5070544702",
      "name": "Heinz Koeppl",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2786005418",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2956228567",
    "https://openalex.org/W1656766859",
    "https://openalex.org/W3205884384",
    "https://openalex.org/W3098085362",
    "https://openalex.org/W3169466323",
    "https://openalex.org/W3211647829",
    "https://openalex.org/W4286899620",
    "https://openalex.org/W2133059825",
    "https://openalex.org/W2785934082",
    "https://openalex.org/W4297791576",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3199004353",
    "https://openalex.org/W3176719058",
    "https://openalex.org/W2889232360",
    "https://openalex.org/W3112906279",
    "https://openalex.org/W2132162500",
    "https://openalex.org/W2772723798",
    "https://openalex.org/W2560886373",
    "https://openalex.org/W3093416812",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100Kx100K pixels. Annotating cancerous areas in WSIs on the pixel level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViT- MIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Transformer (ViT) architecture builds the feature extractor of Self-ViT-MIL. For localizing cancerous regions, a MIL aggregator with global attention is utilized. To the best of our knowledge, Self-ViT- MIL is the first approach to introduce self-supervised ViTs in MIL-based WSI analysis tasks. We showcase the effectiveness of our approach on the common Camelyon16 dataset. Self-ViT-MIL surpasses existing state-of-the-art MIL-based approaches in terms of accuracy and area under the curve (AUC).",
  "full_text": "Histopathological Image Classiﬁcation based on\nSelf-Supervised Vision Transformer and Weak Labels\nAhmet Gokberk Gul*1 , Oezdemir Cetin*1 , Christoph Reich1, Nadine Flinner2, Tim\nPrangemeier1, and Heinz Koeppl(\u0000 )1\n1Self-Organizing Systems Lab, Department of Electrical Engineering and Information\nTechnology, Technische Universit¨ at Darmstadt, Germany\n2Dr. Senckenberg Institute of Pathology, University Hospital Frankfurt, Germany\nABSTRACT\nWhole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples.\nAutomating this diagnosis poses various issues, most notably caused by the immense image resolution and\nlimited annotations. WSI’s commonly exhibit resolutions of 100 ,000 ×100,000 pixels. Annotating cancerous\nareas in WSI’s on the pixel-level is prohibitively labor-intensive and requires a high level of expert knowledge.\nMultiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning\nis performed on slide-level labels, in which a pathologist provides information about whether a slide includes\ncancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous\nareas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViT-\nMIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels.\nThe recent Vision Transformer (ViT) architecture builds the feature extractor of Self-ViT-MIL. For localizing\ncancerous regions, a MIL aggregator with global attention is utilized. To the best of our knowledge, Self-ViT-\nMIL is the ﬁrst approach to introduce self-supervised ViT’s in MIL-based WSI analysis tasks. We showcase\nthe eﬀectiveness of our approach on the common Camelyon16 dataset. Self-ViT-MIL surpasses existing state-\nof-the-art MIL-based approaches in terms of accuracy and area under the curve (AUC). Code is available at\nhttps://github.com/gokberkgul/self-learning-transformer-mil\nKeywords: Multiple Instance Learning, Weakly Supervised Learning, Vision Transformer, Computational\nPathology, Cancer Detection, Whole Slide Image\n1. INTRODUCTION\nHistopathological whole slide scanning is a common and widely used tool for visualizing tissue samples. 1 These\nvisualizations are used for diagnosing diseases, such as cancer. 2, 3 Tissue samples are scanned by specialized\nmicroscopes on glass slides, resulting in digital whole slide images. 1 Automating the diagnosis of diseases based\non WSIs is a current challenge in computed aided diagnostics. 4–7 WSI’s exhibit extremely high resolutions of up\nto 100,000 ×100,000 pixels.1, 3 This resolution makes it infeasible to directly process whole slides with common\ncomputer vision models, such as convolutional neural networks (CNNs). 6, 8 To cope with such high image reso-\nlutions, WSIs are typically processed patch-wise, making the usage of CNNs feasible. 6, 7 Training deep networks\ntypically requires a sophisticated amount of ground truth annotations. 9 Obtaining pixel-wise ground truth an-\nnotations is generally labor-intensive and time-consuming, for example, labeling a single 1024 ×2048 image of\nthe cityscapes dataset (natural urban scenes) is reported to have taken approximately 90min. 10 In contrast to\nnatural scenes, annotating WSIs requires a high level of expert knowledge, and pixel-wise annotations can diﬀer\nbetween experts.2, 3, 11Motivated by these issues, we propose an approach for computed aided diagnostics based\nonly on slide-level labels (weak labels).\nRecent approaches for learning from weak WSI annotations follow the multiple instance learning (MIL)\nparadigm.4, 6, 7, 12, 13In MIL, each WSI is considered as a bag. Each bag contains multiple instances. These\n*Ahmet Gokberk Gul and Oezdemir Cetin — both authors contributed equally\nE-mail: heinz.koeppl@tu-darmstadt.de\narXiv:2210.09021v2  [cs.CV]  18 Apr 2023\ninstances are patches obtained by cropping the high-resolution WSI, resulting in low-resolution images. Labels\nonly contain global information whether a bag contains disease-positive instances or not. Recent MIL-based\napproaches ﬁrst extract patch-level latent representations with a CNN, 6, 7 then all latent representations are\naggregated before being fed into a classiﬁer. 6 Training is typically performed in an end-to-end manner.\nIn recent years, CNNs have become the de facto standard for medical image analysis, examples including\ncell segmentation,14–16 WSI analysis,6, 8microscopy image synthesis,17 and brain tumor segmentation. 18 More\nrecently, transformer-based models,7, 19, 20such as the Vision Transformer (ViT) 21 have been proposed. These\ntransformer-based models have shown satisfying performance over CNNs when trained in a self-supervised fash-\nion.22 Here we propose, Self-ViT-MIL that combines self-supervised learning, ViTs, and MIL. Self-ViT-MIL is\nevaluated on the common Camelyon16 dataset 3 against recent weakly supervised state-of-the-art approaches.\nOur approach outperforms all other approaches in terms of accuracy and AUC. Self-ViT-MIL also achieves\ncompetitive results against a fully-supervised baseline.\n2. METHODS\nSelf-ViT-MIL (Fig. 1) can be divided into two stages. In the ﬁrst stage, the ViT is trained in a self-supervised\nfashion to learn rich feature representations of the input patches. 22 In the second stage, the ViT weights are\nfrozen and a MIL aggregator is trained on top of the ViT’s latent features. 6\nPatch Extractor\n2 1345K\nLinear Projection\n0\nNormalization \nMulti-head Attention \nNormalization \nMulti Layer Perceptron \nMulti Layer Perceptron \n \nSoftmax \nCentering \n2 1345K\nLinear Projection\n0\nNormalization \nMulti-head Attention \nNormalization \nMulti Layer Perceptron \nMLP \nMax \nSA \nMLP \n(a)\n(a)\n(b)\nFigure 1. Self-ViT-MIL method ﬂow chart. (a) Self-supervised training with DINO training approach. Although DINO 22\nfollows student-teacher approach, only the teacher network is shown for simplicity. Each patch extracted from the WSIs\nare further divided into 16×16 patches, ﬂattened with linear projection layer and positional encodings are added. Encoded\npatches are passed through L transformer encoder layers, which consists of normalization, multi-head attention, and MLP\nlayers with residual connections. The transformer encoder is followed by an MLP, centering (only applied to the teacher\nnetwork) and softmax layers. (b) Training of MIL aggregator. The trained ViT is used to generate feature embeddings\nof WSIs. The ﬁrst stream calculates instance scores and maximum instance score. The second stream uses global self-\nattention the calculate a bag vector and attention scores, later used for localization. The bag vector is classiﬁed to estimate\nthe bag score. The addition of the maximum instance score and the bag score builds the ﬁnal score.\n2.1 Self-Supervised Vision Transformer\nThe Vision Transformer, proposed by Dosovitskiy et al. is a general backbone for vision tasks. 21 The core\nbuilding block of ViTs is an attention-based Transformer encoder, originally proposed in the domain of natural\nlanguage processing. 19 Let X ∈RC×H×W be an input image with C, H and W as channel size, height, and\nwidth, respectively. X is partitioned into non-overlapping patches to generate the tensor Xp ∈RN×P2C with\nN = HW\nP2 . A linear layer of input size P2C and output size of D is used for the projection of the patches, which\ngenerates an N ×D tensor. Class, [cls], token of size 1 ×D is concatenated to the projected patches, making\nthe tensor size ( N + 1) ×D. Transformers are agnostic to the permutation of the input tokens. To incorporate\npositional information, learnable positional encodings are employed. For each projected patch and the [cls]\ntoken, 1 ×D learnable positional embeddings are added. ViT consists of L transformer encoder blocks. Each\nblock contains a Multi-Head Self-Attention (MSA) layer, two Layer Normalizations, and a Multilayer Perceptron\n(MLP). The [cls] token at the output of the last transformer is used as a latent representation of the original\nimage.\nDINO is a self-supervised learning technique. 22 In the knowledge distillation paradigm, there are two net-\nworks, called student and teacher, with network notation and parameters {gθs ,θs}and {gθt ,θt}, respectively.\nLoss is the cross-entropy function between student and teacher network output probability distribution. Student\nparameters θs are updated with respect to the cross-entropy loss function. For each image X, two global and\nseveral local views are generated, by sampling random transformations. All views of the image are stored in a set\nV = {˜Xg\n1, ˜Xg\n2, ˜Xl\n1,..., ˜Xl\nn}. V is passed through the student network, while a partition of the set that contains\nonly the global views, Vg, is fed to the teacher network to encourage local to global correspondences.\nDiﬀering from the knowledge distillation paradigm, there is no teacher network given in prior. The teacher\nis built from the past iterations of the student network. Teacher network is updated with exponential moving\naverage (EMA) update. 22 Since there are no contrastive loss or clustering constraints, DINO uses a centering\nand sharping operations in order to avoid collapsing. 22 The centering operation avoids that a single dimension\ndominates the probability distribution, instead, it pushes the model to generate a uniform distribution. The\nsharpening operation has the opposite eﬀect, so a joint usage of both operations crates a balance between\nuniform and single dimension domination, hence avoiding collapse.\n2.2 MIL Aggregator\nIn the MIL setting, training data is arranged in sets, called bags, which contain instances. Class labels are only\nprovided for the bags and instance labels are not known a priori.12 Prediction can be performed at the bag\nlevel, instance level, or both. Let Xbe a set, X= {X1,X2,..., Xn}where Xi ∈RC×H×W ,i ∈{1,...,n }. The\nstandard assumption of MIL is that if any of the instance label is 1, the bag label Y is also 1, otherwise Y is also\n0. This approach is used in WSI classiﬁcation tasks since image patches extracted from the WSI can be treated\nas instances.\nIn Dual Stream Multiple Instance Learning (DSMIL) 6 design, there are two streams, ﬁrst stream learns\nthe instance classiﬁer, the most important instance and its score via max pooling with the formula cm(X) =\nmax{Wph1,..., WphN }. Each embedding hi ∈RD×1, generated by the pre-trained ViT, is projected with a\nlearnable matrix Wp ∈R1×K, creating a score for each class.\nSecond stream aggregates the instances to obtain a embedded space representation of the bag, with a self\nattention module. From each embedding, query and information vectors are generated with learnable projection\nmatrices Wq ∈RL×K and Wv ∈RK×K. Distance between each query vector qi to the query vector that\ncorresponds to the critical instance qm is calculated, shown in equation 1.\nsi = D(hi,hm) = exp(⟨qi,qm⟩)∑N\nk=1 exp(⟨qk,qm⟩)\n(1)\nThe resulting scalar, si, can be interpreted as the score of instance hi. These scores are later used to generate\nan attention map and localize the cancerous areas. Embedded space representation of the bag is achieved by\nthe weighted sum of information vectors, with the distance measure as weights. Bag score is calculated with a\nlearnable projection matrix Wb ∈R1×K. The ﬁnal score is the average of instance space and embedded space\nbag score. A pseudo algorithm can be found in Appendix B.\n3. EXPERIMENTS\nOur experiments utilize the common Camelyon16 dataset, consisting of 399 WSIs. 3 The training set includes\n110 slides with nodal metastasis and 160 without. The test dataset consists of 129 WSIs, of which 49 contain\nmetatases. All annotations were generated by pathologists, however they are not used for training.\nAll WSIs are cropped to non-overlapping 224 ×224 patches on 20 ×magniﬁcation factor. For background\nﬁltering, the Otsu Thresholding 23 method is used. After ﬁltering, the dataset consisted of 1 ,235,428 train-\ning patches and 544 ,420 testing patches. Stain normalization is applied during pre-processing to obtain more\nuniformly distributed patches.24\nA ViT-B/16 is employed as the backbone network for feature extraction. 21 EMA update factor λ is chosen\nto be 0.9995 and ramps up to 1 with a cosine scheduler during the training. In the warm-up phase, the learning\nrate linearly increases from ηmin to ηmax. After the warm-up phase, the learning rate drops from ηmax to ηmin,\nwith a cosine learning rate scheduler. Choice for ηmin and ηmax were 10−6 and 5 ·10−4, respectively. Similarly,\nteacher temperature τt has two phases. For the warm-up phase, the value is chosen to be 0 .01, which accelerates\nthe loss decrease, but it changes to 0 .04 after the warm-up phase for robustness. ViT is trained on 4 Nvidia\nV100 GPUs (32GB) for 100 epochs with 10 epochs as the warm-up phase, each epoch took 55 minutes.\nThe pre-trained ViT weights are frozen during MIL aggregator training. The learning rate η was 2 ·10−5.\nInstead of performing the classiﬁcation on the [cls] token of the last block, the [cls] tokens of the last 4\nblocks were stacked and their is average appended to the stack, generating a vector of R1×5D per patch. The\nMIL aggregator was trained using AdamW25 on a Nvidia V100 GPU (32GB) for 50 epochs ( ≈27 minutes each).\n4. RESULTS\n0 0.2 0.4 0.6 0.8 1\n0\n0.2\n0.4\n0.6\n0.8\n1\nFalse Positive Rate\nTrue Positive Rate\nROC Curve (area = 0.9426)\nFigure 2. MIL Aggregator ROC Curve.\nTable 1. Numerical results of Self-ViT-MIL on the Came-\nlyon16 dataset3 compared with other methods.\nModel Scale Accuracy AUC\nCHOWDER5 Single - 0.8580\nABMIL‡4 Single 0.8450 0.8653\nDSMIL6 Single 0.8682 0.8944\nMIL-RNN26 Single - 0.8990\nDSMIL-LC6 Multiple 0.8992 0.9165\nTransMIL7 Single 0.8837 0.9309\nFully-supervised6 Single 0.9147 0.9362\nSelf-ViT-MIL (ours) Single 0.9147 0.9426\n‡ABMIL results taken from DSMIL paper. 6\nOur accuracy and AUC scores on the Camelyon16 dataset, 3 compared to other MIL-based approaches from the\nliterature, are presented in Table 1. We also compare Self-ViT-MIL against a naive fully-supervised baseline\nwith a ResNet18 backbone from the literature. 6 All works have trained their model on Camelyon16 training\nslides, expect MIL-RNN. 26 MIL-RNN is trained on the MSK axillary lymph node dataset, which consist of\n9,894 WSIs, which is about 36 times larger than Camelyon16 training dataset. Evaluation is performed again\non the Camelyon16 test dataset. DSMIL-LC is the multiscale version of DSMIL approach, combining 20 ×and\n5×magniﬁcation slides to extract both local and global features. 6 Accuracy score of 0 .9147 and AUC score of\n0.9426 is achieved with Self-ViT-MIL method. The receiver operating characteristics (ROC) curve is given in\nFigure 2. This would put Self-ViT-MIL method in the ﬁrst place Camelyon16 challenge and fourth place in open\non leaderboard in Whole-slide-image classiﬁcation task. Results achieved surpasses other papers that reported\nCamelyon16 WSI classiﬁcation scores following MIL approach, making Self-ViT-MIL the state of the art method.\nLocalization performance is qualitatively presented in Figure 3 (zoomed out version of the same slide can be\nseen in Figure 4). Before drawing the model’s prediction on the slide, Z-test is performed on attention scores\nto remove outliers and set 0 for the highest ones. DSMIL model is tested on the same slides, using the model\nweights given by the authors. DSMIL has a better coverage for the cancerous areas, but it also performs false\npositive prediction. Self-ViT-MIL model cannot cover all tumor areas, but almost no false positive predictions\nhave occurred out of four random test slides selected.\nFigure 3. Camelyon16 Dataset Slide Test082, Zoomed in Localization Performance. Green area on the tissues shows the\npathologists annotation. Transparent-black tiles show the model’s prediction of the malignant patches. The amount of\nred on the predicted patches represents the conﬁdence of the model.\n5. DISCUSSION AND CONCLUSION\nOur method outperforms both TransMIL 7 and DSMIL-LC,6 the recent state-of-the-art MIL-based approaches,\nin terms of accuracy and AUC in the task of slide level classiﬁcation on the Camelyon16 dataset. Self-ViT-\nMIL also achieved on par results with the DSMILs fully-supervised baseline. 6 However, other fully-supervised\nmethods with better performances have been reported.∗ Training the ViT backbone in a self-supervised fashion is\ncompute-intensive and requires a sophisticated compute infrastructure. Utilizing multi-scale input data showed\nperformance improvements when applied to the DSMIL approach. 6 Incorporating multi-scale input data to\nSelf-ViT-MIL may also result in increased performance.\nThis paper presents Self-ViT-MIL, a model for weakly-supervised WSI classiﬁcation. To the best of our\nknowledge, Self-ViT-MIL is the ﬁrst approach to explore self-supervised ViTs in the application of WSI analysis.\nSelf-ViT-MIL outperforms existing state-of-the-art weakly-supervised approaches on the Camelyon16 dataset.\nACKNOWLEDGMENTS\nCalculations were conducted on the Lichtenberg high performance computer of the TU Darmstadt. H.K. ac-\nknowledges support from the European Research Council (ERC) with the consolidator grant CONSYN (nr.\n773196). O.C. is supported by the Alexander von Humboldt Foundation Philipp Schwartz Initiative.\nREFERENCES\n[1] Farahani, N., Parwani, A. V., Pantanowitz, L., et al., “Whole slide imaging in pathology: advantages,\nlimitations, and emerging perspectives,” Pathol. Lab. Med. Int.7(23-33), 4321 (2015).\n[2] Bandi, P., Geessink, O., Manson, Q., Van Dijk, M., Balkenhol, M., Hermsen, M., Bejnordi, B. E., Lee, B.,\nPaeng, K., Zhong, A., et al., “From Detection of Individual Metastases to Classiﬁcation of Lymph Node\nStatus at the Patient Level: The CAMELYON17 Challenge,” IEEE Trans. Med. Imaging38(2), 550–560\n(2018).\n[3] Bejnordi, B. E., Veta, M., Van Diest, P. J., Van Ginneken, B., Karssemeijer, N., Litjens, G., Van Der Laak,\nJ. A., Hermsen, M., Manson, Q. F., Balkenhol, M., et al., “Diagnostic Assessment of Deep Learning Algo-\nrithms for Detection of Lymph Node Metastases in Women With Breast Cancer,”JAMA 318(22), 2199–2210\n(2017).\n[4] Ilse, M., Tomczak, J., and Welling, M., “Attention-based Deep Multiple Instance Learning,” in [ ICML],\n2127–2136 (2018).\n∗https://camelyon16.grand-challenge.org/Results/\n[5] Courtiol, P., Tramel, E. W., Sanselme, M., and Wainrib, G., “Classiﬁcation and Disease Localization in\nHistopathology Using Only Global Labels: A Weakly-Supervised Approach,” arXiv:1802.02212 (2018).\n[6] Li, B., Li, Y., and Eliceiri, K. W., “Dual-stream Multiple Instance Learning Network for Whole Slide Image\nClassiﬁcation with Self-supervised Contrastive Learning,” in [ CVPR], 14318–14328 (2021).\n[7] Shao, Z., Bian, H., Chen, Y., Wang, Y., Zhang, J., Ji, X., and Zhang, Y., “TransMIL: Transformer based\nCorrelated Multiple Instance Learning for Whole Slide Image Classiﬁcation,” in [ NeurIPS], (2021).\n[8] Jin, C., Tanno, R., Mertzanidou, T., Panagiotaki, E., and Alexander, D. C., “Learning to Downsample for\nSegmentation of Ultra-High Resolution Images,” in [ ICLR], (2022).\n[9] LeCun, Y., Bengio, Y., and Hinton, G., “Deep learning,” Nature 521(7553), 436–444 (2015).\n[10] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., and\nSchiele, B., “The Cityscapes Dataset for Semantic Urban Scene Understanding,” in [ CVPR], 3213–3223\n(2016).\n[11] Kohl, S., Romera-Paredes, B., Meyer, C., De Fauw, J., Ledsam, J. R., Maier-Hein, K., Eslami, S.,\nJimenez Rezende, D., and Ronneberger, O., “A Probabilistic U-Net for Segmentation of Ambiguous Images,”\nNeurIPS 31 (2018).\n[12] Carbonneau, M.-A., Cheplygina, V., Granger, E., and Gagnon, G., “Multiple Instance Learning: A Survey\nof Problem Characteristics and Applications,” Pattern Recognit.77, 329–353 (2018).\n[13] Lerousseau, M., Vakalopoulou, M., Classe, M., Adam, J., Battistella, E., Carr´ e, A., Estienne, T., Henry,\nT., Deutsch, E., and Paragios, N., “Weakly supervised multiple instance learning histopathological tumor\nsegmentation,” in [MICCAI], 470–479 (2020).\n[14] Ronneberger, O., Fischer, P., and Brox, T., “U-Net: Convolutional Networks for Biomedical Image Seg-\nmentation,” in [MICCAI], 234–241 (2015).\n[15] Prangemeier, T., Wildner, C., O. Fran¸ cani, A., Reich, C., and Koeppl, H., “Multiclass Yeast Segmentation\nin Microstructured Environments with Deep Learning,” in [ CIBCB], (2020).\n[16] Prangemeier, T., Wildner, C., Fran¸ cani, A. O., Reich, C., and Koeppl, H., “Yeast cell segmentation in\nmicrostructured environments with deep learning,” Biosystems 211, 104557 (2022).\n[17] Reich, C., Prangemeier, T., Wildner, C., and Koeppl, H., “Multi-stylegan: Towards image-based simulation\nof time-lapse live-cell microscopy,” in [ MICCAI], 476–486 (2021).\n[18] Reich, C., Prangemeier, T., Cetin, ¨O., and Koeppl, H., “OSS-Net: Memory Eﬃcient High Resolution\nSemantic Segmentation of 3D Medical Data,” in [ BMVC], (2021).\n[19] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L., and Polosukhin,\nI., “Attention Is All You Need,” NeurIPS 30 (2017).\n[20] Prangemeier, T., Reich, C., and Koeppl, H., “Attention-Based Transformers for Instance Segmentation of\nCells in Microstructures,” in [ BIBM], (2020).\n[21] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,\nMinderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N., “An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale,” in [ ICLR], (2021).\n[22] Caron, M., Touvron, H., Misra, I., J´ egou, H., Mairal, J., Bojanowski, P., and Joulin, A., “Emerging\nProperties in Self-Supervised Vision Transformers,” in [ ICCV], 9650–9660 (2021).\n[23] Otsu, N., “A Threshold Selection Method from Gray-Level Histograms,” IEEE Trans. Syst. Man Cybern.\nSyst. 9(1), 62–66 (1979).\n[24] Macenko, M., Niethammer, M., Marron, J. S., Borland, D., Woosley, J. T., Guan, X., Schmitt, C., and\nThomas, N. E., “A method for normalizing histology slides for quantitative analysis,” in [ ISBI], 1107–1110\n(2009).\n[25] Loshchilov, I. and Hutter, F., “Decoupled Weight Decay Regularization,” in [ ICLR], (2019).\n[26] Campanella, G., Hanna, M. G., Geneslaw, L., Miraﬂor, A., Werneck Krauss Silva, V., Busam, K. J., Brogi,\nE., Reuter, V. E., Klimstra, D. S., and Fuchs, T. J., “Clinical-grade computational pathology using weakly\nsupervised deep learning on whole slide images,” Nat. Med.25(8), 1301–1309 (2019).\nAPPENDIX A. LOCALIZATION RESULTS\nFigure 4. Camelyon16 Dataset Slide Test082, Localization Performance Around Tumor Areas.\nAPPENDIX B. MIL AGGREGATOR ALGORITHM\nAlgorithm 1:MIL Aggregator Algorithm\n1 foreach {X,y}∈{X B,yB}do\n2 h1,..., hN ←f(X1),...,f (XN ) // Generate latent representation of image patches\n3 cm(X),hm ←max{Wph1,..., WphN } // Calculate instance score and critical instance\n4 q1,..., qN ←Wqh1,..., WqhN // Generate query vectors\n5 v1,..., vN ←Wvh1,..., WvhN // Generate information vectors\n6 s1,...,s N ←D(h1,hm),...,D (hN ,hm) // Calculate instance scores\n7 b ←∑N\ni=1 sivi // Generate bag representation of the WSI\n8 cb(X) ←Wbb // Calculate bag score\n9 L←1\n2 (H(cm(X),y) + H(cb(X),y)) // Calculate loss score\n10 Wp,Wq,Wv,Wb ←optimizer(L,Wp,Wq,Wv,Wb,η) // Update weights\n11 end",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8173645734786987
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6947643756866455
    },
    {
      "name": "Pixel",
      "score": 0.6524164080619812
    },
    {
      "name": "Extractor",
      "score": 0.5577337741851807
    },
    {
      "name": "Feature learning",
      "score": 0.5393493175506592
    },
    {
      "name": "Transformer",
      "score": 0.5201797485351562
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45845144987106323
    },
    {
      "name": "Source code",
      "score": 0.4485003352165222
    },
    {
      "name": "Feature extraction",
      "score": 0.42530685663223267
    },
    {
      "name": "Machine learning",
      "score": 0.3952989876270294
    },
    {
      "name": "Computer vision",
      "score": 0.356789767742157
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Process engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31512782",
      "name": "Technical University of Darmstadt",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I114090438",
      "name": "Goethe University Frankfurt",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210132578",
      "name": "University Hospital Frankfurt",
      "country": "DE"
    }
  ]
}