{
  "title": "End-to-End Object Detection with Adaptive Clustering Transformer",
  "url": "https://openalex.org/W3101415077",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287574551",
      "name": "Zheng, Minghang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109793426",
      "name": "Gao Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222134701",
      "name": "Zhang, Renrui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221910337",
      "name": "Li, Kunchang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1832420164",
      "name": "Wang Xiao-gang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969345873",
      "name": "Li Hongsheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2045024949",
      "name": "Dong Hao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2964091144",
    "https://openalex.org/W2963319519",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2963176022",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2024046085",
    "https://openalex.org/W3036728994",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2904617485",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2969394474",
    "https://openalex.org/W2162006472",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2987861506",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2164598857",
    "https://openalex.org/W2963659353",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2964067226",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W607748843",
    "https://openalex.org/W3102129360",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2968388725",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3203974803",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2144506857",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2964308564"
  ],
  "abstract": "End-to-end Object Detection with Transformer (DETR)proposes to perform object detection with Transformer and achieve comparable performance with two-stage object detection like Faster-RCNN. However, DETR needs huge computational resources for training and inference due to the high-resolution spatial input. In this paper, a novel variant of transformer named Adaptive Clustering Transformer(ACT) has been proposed to reduce the computation cost for high-resolution input. ACT cluster the query features adaptively using Locality Sensitive Hashing (LSH) and ap-proximate the query-key interaction using the prototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside self-attention into O(NK) where K is the number of prototypes in each layer. ACT can be a drop-in module replacing the original self-attention module without any training. ACT achieves a good balance between accuracy and computation cost (FLOPs). The code is available as supplementary for the ease of experiment replication and verification. Code is released at \\url{https://github.com/gaopengcuhk/SMCA-DETR/}",
  "full_text": "arXiv:2011.09315v2  [cs.CV]  18 Oct 2021\nZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER 1\nEnd-to-End Object Detection with\nAdaptive Clustering T ransformer\nMinghang Zheng1\nminghang@pku.edu.cn\nPeng Gao2\n1155102382@link.cuhk.edu.hk\nRenrui Zhang2\n1700012927@pku.edu.cn\nKunchang Li2\nkc.li@siat.ac.cn\nXiaogang Wang3\nxgwang@ee.cuhk.edu.hk\nHongsheng Li3\nhsli@ee.cuhk.edu.hk\nHao Dong1\nhao.dong@pku.edu.cn\n1 CFCS, CS Dept., Peking University\n2 Shanghai AI Laboratory\n3 CUHK-SenseTime Joint Laboratory ,\nThe Chinese University of Hong Kong\nAbstract\nEnd-to-end Object Detection with Transformer (DETR) perfo rms object detection\nwith Transformer and achieves comparable performance with two-stage object detection\nlike Faster-RCNN. However, DETR needs huge computational r esources for training and\ninference due to the high-resolution spatial inputs. In thi s paper, a novel variant of trans-\nformer named Adaptive Clustering Transformer (ACT) has bee n proposed to reduce the\ncomputation cost for high-resolution input. ACT clusters t he query features adaptively\nusing Locality Sensitive Hashing (LSH) and approximates th e query-key interaction us-\ning the prototype-key interaction. ACT can reduce the quadr atic O( N2 ) complexity inside\nself-attention into O( NK ) where K is the number of prototypes in each layer. ACT can\nbe a drop-in module replacing the original self-attention m odule without any training .\nACT achieves a good balance between accuracy and computatio n cost (FLOPs). The\ncode is available as supplementary for the ease of experimen t replication and veriﬁca-\ntion. Code is released at https://github.com/gaopengcuhk/SMCA-DETR/\n1 Introduction\nObject detection is the task of predicting a set of bounding b oxes and category labels for\neach predetermined object. Recently popular models [ 14, 15, 29, 34, 35, 39] solve this task\nby generating a large number of regional proposals, predict ing each proposal, and apply-\ning a non-maximum suppression procedure to eliminate those highly overlapping proposals.\n© 2021. The copyright of this document resides with its autho rs.\nIt may be distributed unchanged freely in print or electroni c forms.\n2 ZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER\nQuery \nEmbedding \nMulti-Head \nAttention \nAdd & Norm \nMulti-Head \nAttention \nAdd & Norm \nFeed \nForward \nAdd & Norm \nObject Queries \nFeed \nForward \nclass, \nbox \nno \nobject \nclass, \nbox …Positional \nEncoding \nAdaptive Clustering \nAttention \nAdd & Norm \nCNN Backbone \n+\nFeed \nForward \nAdd & Norm \nInput Images \nInput features \nClustered Queries \nPrototypes Keys Values \nAttention map Weighting \nOutput features \nProjection \nAdaptive Clustering \nPrototypes \nGeneration \nAttention \nBroadcast \nEncoder Decoder Prediction \nQueries \nFigure 1: The illustration of our adaptive clustering trans former. W e use a small number\nof prototypes to represent the queries and only the attentio n map between prototypes and\nkeys will be calculated. The number of prototypes will be aut omatically determined based\non the distribution of queries. Finally, the attention outp ut will be broadcast to the queries\nrepresented by the prototype.\nT wo-stage object detection is difﬁcult to deploy and debug d ue to the complex computation\npipeline.\nCarion et al . [ 3] proposes a new method, called Detection Transformer or DET R, which\nuses an encoder-decoder transformer [ 38] framework to solve this task in an intuitive way\nutilizing set prediction which has been explored in [ 25, 36, 37]. Thanks to the powerful\nlearning ability of Transformer [ 38], DETR can perform set prediction end-to-end without\nresorting to human-designed prior like anchor and region pr oposal thus resulting in a much\nsimpler object detection framework. However, DETR suffers from high computational com-\nplexity in the encoder. T o achieve good performance, DETR ne eds a high-resolution image\nwhich will increase the computation in the encoder quadrati cally due to the all-pairs interac-\ntion for all positions.\nAlthough many improvements of transformer [ 4, 5, 6, 12, 16, 21, 23, 40, 47] can reduce\nthe computation complexity, variants of transformer chang e the architecture of transformer\nwhich require huge trial-and-error cost due to the slow conv ergence of DETR(1920 GPU\nhours for single V100). One natural question to ask is whethe r we can improve the perfor-\nmance and computational trade-off of DETR with acceptable c omputing resources?\nW e propose a novel Adaptive Clustering Transformer(ACT) wh ich can serve as a drop-in\nmodule on the original DETR framework by replacing the trans former in the encoder. ACT\nis fully compatible with the original transformer and thus d oes not require retraining. The\naccuracy gap between ACT and the original transformer can be further closed by equipping\nwith Multi-T ask Knowledge Distillation(MTKD). MTKD can al so enable seamless switch\nbetween models with different FLOPs and Accuracy during inf erence.\nT wo observations of DETR motivate our design.\nEncoder Attention Redundancy Inside the encoder of DETR, features at each position\nwill collect information from other spatial positions adap tively using the attention mecha-\nnism. W e observe that features that are semantically simila r and spatially close to each other\nwill generate similar attention maps and vice versa. As show n in Figure\n2, the attention map\nfor P0 and P1 are similar to each other and contain redundancy while dista nt points P0 and P3\ndemonstrate a completely different attention pattern. The redundancy in self-attention moti-\nZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER 3\nP଴Pଵ\nPଶ\nFigure 2: The attention map of some points\nin the last layer of the transformer encoder.\nThe darker the color, the greater the weight.\nCNN \nBackbone \n+\nInput \nImages \nPositional \nEncoding \nACT \nTransformer \nParameter \nsharing \nPrediction \nPrediction \nGround Truth \nKnowledge \nDistillation Loss \nPrediction Loss \n+\nFigure 3: Multi-T ask Knowledge Distilla-\ntion. Image features will be extracted by\nthe CNN backbone ﬁrst. The extracted fea-\nture will be passed into ACT and the original\ntransformer parallel. T o enable a seamless\nswitch between ACT and the orignal trans-\nformer, MTKD will guide the training.\nvates ACT to choose representative prototypes and broadcas t the feature update of prototypes\nto its nearest neighbor.\nEncoder Feature Diversity W e observe that as the encoder goes deeper, features will\nbe similar as each feature will collect information from eac h other. Besides, for different\ninputs, the feature distribution in each encoder layer is qu ite different. These observations\nmotivates us to adaptively determine the number of prototyp es based on the distribution of\nfeatures among each layer instead of a static number of clust er centers.\nT o solve the Encoder Attention Redundancy , ACT clusters similar query features to-\ngether and only calculates the key-query attentions for rep resentative prototypes according\nto the average of the features in the cluster. After calculat ing the features updates for pro-\ntotypes, the updated features will be broadcast to its neigh bors according to the euclidean\ndistance on the query feature space. Encoder Feature Diversity motivate us to design an\nadaptive clustering algorithm which can cluster features a ccording to the distribution of fea-\nture for each input and each encoder layer. Thus we choose a mu lti-round Exact Euclidean\nLocality Sensitivity Hashing (E2LSH) which can perform que ry features distribution-aware\nclustering.\nExperiments show that we reduce the FLOPS of DETR from 73.4 Gﬂ ops to 58.2 Gﬂops\n(excluding Backbone Resnet FLOPs) without any training process , while the loss in AP\nis only 0.7%. The loss in AP can be further reduced to 0.2% by a M ulti-T ask Knowledge\nDistillation.\nOur main contributions are summarised below .\n• W e develop a novel method called Adaptive Clustering Trans former (ACT) which\ncan reduce the inference cost of DETR. ACT can reduce the quad ratic complexity of\nthe original transformer, at the same time ACT is fully compa tible with the original\ntransformer.\n• W e reduce the FLOPS of DETR from 73.4 Gﬂops to 58.2 Gﬂops (exc luding Backbone\nResnet FLOPs) without any training process, while the loss in AP is only 0.7%.\n• W e have further reduced the loss in AP to 0.2% through a Multi -T ask Knowledge\nDistillation (MTKD) which enables a seamless switch betwee n ACT and the original\ntransformer.\n4 ZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER\n2 Related W ork\n2.1 Review of Attention Model on NLP and CV\nAttention model [ 1, 20, 42] has been widely used in computer vision (CV) and natural lan -\nguage processing (NLP) ﬁelds due to the in-built adaptive in formation aggregation mech-\nanism. W e mainly focus on one branch of attention called tran sformer. Transformer per-\nforms information exchange between all pairs of entities li ke words in NLP or regions in\nCV . Transformer has achieved state-of-the-art performanc e on Machine Translation [ 38],\nObject Detection [ 3], Multimodality Reasoning [ 11, 33, 44], Image Classﬁcation [ 9, 30],\nV ideo Classﬁcation [ 41] and Language Understanding [ 8]. While transformer has achieved\ngood performance on different scenarios, but it is hard to sc ale due to the quadratic complex-\nity with respect to the length of the input sequence. Many mod iﬁcations of transformer have\nbeen proposed to tackle the computation bottleneck of trans former.\nReformer [ 23] proposed sharing key and query and use Locality Sensitivit y Hashing\n(LSH) [ 7] to cluster features near into one cluster, then perform inf ormation exchange in-\nside each cluster. Performer [ 6] approximates the softmax between key and query interac-\ntion using Positive Orthogonal Random Features (PORF) with provable approximate error\nwith linear complexity. Linear Attention [ 21] utilizes association property to Key-Query-\nV alue multiplication from quadratic complexity into linea r complexity. Progressive Elimina-\ntion [ 16] ﬁnds that redundancy exists in transformer and progressiv ely eliminates the input\nof each layer and achieves comparable performance with the o riginal transformer by reduc-\ning computation cost. Asymmetric Attention [ 47] summarises key features into a few key\nvectors using multi-scale pooling over key features thus re duce the computation complex-\nity. Global Graph Reasoning [ 5] transforms the original input into global vectors utilizi ng\nweighted pooling and then perform information exchange ove r the compact global vectors.\nPreviously mentioned methods modiﬁed the structure of the o riginal transformer and\nneed huge resources for training and inference. Our propose d Adaptive Clustering Trans-\nformer (ACT) shares the same structure as the original trans former. ACT reduces the com-\nputation cost of transformer without re-training. Besides , the performance gap between ACT\nand the original transformer can be further reduced with a fe w epochs of ﬁne-tuning knowl-\nedge distillation [ 18, 45].\n2.2 Object Detection using Deep Learning\nThe main framework of object detection is dominated by perfo rming classiﬁcation over a\nsliding window . V iona-Jones Face detector [ 39] ﬁrst introduce the idea of sliding window\napproach into face detection with adaboost [ 10]. After the successful application of CNN on\nobject classiﬁcation [ 17, 24], deep features have been applied to object detection. Prev ious\nresearch of object detection using deep features can be divi ded into two-stage and one-stage\nobject detection. RCNN, Fast RCNN and Faster RCNN [ 14, 15, 35] are two-stage solution\nwhile YOLO [ 34] and SSD [ 29] are one-stage solution. Previous methods on object detec-\ntion are suffered by complex post-processing pipeline (NMS ) [ 2, 32], imbalanced loss [ 28],\nand hand-crafted anchor [ 29, 34] which increase the difﬁculty of training and deployment.\nUnlike sliding-window approaches, object detection has be en formulated as a permutation-\ninvariant set prediction problem. Steward et al [ 37] proposed an end-to-end people detection\nwhich encodes image feature using CNN and decodes the boundi ng box sequentially using\nLSTM [ 19]. The predicted bounding box will be matched with ground tru th using Hungar-\nZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER 5\nian loss [ 25] and trained end-to-end. Recurrent instance segmentation [36] adds an extra\nsegmentation head over the end-to-end object detection fra mework and successfully tests the\nidea on instance segmentation. Recently DETR [ 3] has successfully made the performance of\nset-prediction approaches comparable with two-stage Fast er RCNN approaches by replacing\nLSTM [ 19]with much powerful Transformer [ 38]. End-to-end set prediction problem signif-\nicantly simpliﬁed the pipeline of object detection and redu ce the need for hand-crafted prior.\nHowever, the convergence of end-to-end set prediction is sl ow and need huge inference cost\ncaused by the quadratic complexity of self-attention. Some variants of DETR have also been\nproposed to solve the problems. Deformable DETR [ 46] accelerates the convergence speed\nvia learnable sparse sampling coupled with multi-scale def ormable encoder. SMCA [ 13]\nintroduces Gaussian prior in the transformer decoder and ac hieved an increase in conver-\ngence speed. Different from Deformable DETR and SMCA, whose improvement is mainly\non training process, our proposed ACT target to reduce the in ference computation cost of\nDETR without the need for retraining.\n3 Adaptive Clustering T ransformer\n3.1 Main Structure of DETR\nFigure 1 also shows the three stages of DETR. In the encoder, an ImageN et-pre-trained\nResNet model is used to extract 2D features from the input ima ge. The positional encoding\nmodule uses sine and cosine functions with different freque ncies to encode spatial infor-\nmation. DETR ﬂattens the 2D features and supplements them wi th the positional encoding\nand passes them to the 6-layer transformer encoder. Each lay er of the encoder has the same\nstructure, including an 8-head self-attention module and a n FFN module. The decoder then\ntakes as input a small ﬁxed number of learned positional embe ddings, which are called ob-\nject queries, and additionally attends to the encoder outpu t. The decoder also has 6 layers,\nand each layer contains an 8-head self-attention module, an 8-head co-attention module, and\nan FFN module. Finally, DETR passes each output of the decode r to a shared feed-forward\nnetwork that predicts either a detection (class and boundin g box) or a “no object\" class.\n3.2 Adaptive Clustering T ransformer\nDetermine Prototypes W e use Locality Sensitivity Hashing (LSH) to adaptively agg regate\nthose queries with a small Euclidean distance. LSH is a power ful tool to solve the Nearest\nNeighbour Search problem. W e call a hashing scheme locality -sensitive if nearby vectors get\nthe same hash with high probability and distant ones do not. B y controlling the parameters\nof the hash function and the number of hashing rounds, we let a ll vectors with a distance less\nthan ε fall into the same hash bucket with a probability greater tha n p.\nW e choose Exact Euclidean Locality Sensitive Hashing (E2LS H) [ 7] as our hash func-\ntion:\nh(⃗v) =⌊⃗a ·⃗v + b\nr ⌋ (1)\nwhere h : Rd → Z is the hash function, r is a hyper-parameter, ⃗a,b are random variables\nsatisfying ⃗a = (a1 ,a2 ,..., ad ) with ai ∼ N (0,1) and b ∼ U (0,r). W e will apply L rounds\nof LSH to increase the credibility of the results. The ﬁnal ha sh value will be obtained by\n6 ZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER\nequation 2.\nh(⃗v) =\nL−1\n∑\ni=0\nBi hi(⃗v) (2)\nwhere each hi is obtained by equation 1 with independently sampled parameters ⃗a and b, and\nB is a constant equal to 4 in our experiments.\nEach hash function hi can be regarded as a set of parallel hyperplanes with random n or-\nmal vector ⃗ai and offset bi . The hyper-parameter r controls the spacing of the hyperplanes.\nThe greater the r, the greater the spacing. Furthermore, L hash functions divide the space\ninto several cells, and the vectors falling into the same cel l will obtain the same hash value.\nObviously, the closer the Euclidean distance, the greater t he probability that the vectors fall\ninto the same cell.\nT o obtain the prototypes, we calculate the hash value for eac h query ﬁrstly. Then, queries\nwith the same hash value will be grouped into one cluster, and the prototype of this cluster\nis the center of these queries. More formally, we deﬁne Q ∈ RN×Dk as the queries and\nP ∈ RC×Dk as the prototypes, where C is the number of clusters. Let Gi represent the index\nof the cluster that Qi belongs to. The prototype of the j − t h cluster can be obtained by\nequation 3.\nPj = ∑ i,Gi = j Qi\n∑ i,Gi = j 1 (3)\nEstimate Attention Output After the previous step, each group of queries is represente d\nby a prototype. Thus, only the attention map between prototy pes and keys need to be cal-\nculated. Then, we get the target vector for each prototype an d broadcast it to each original\nquery. Thus, we get an estimation of the attention output. Co mpared with the exact atten-\ntion calculation, we reduce the complexity from O(NMD k + NMD v ) to O(NLDK +CMDk +\nCMDv ), where C is the number of prototypes and L is the number of hash rounds, both of\nwhich are much smaller than N and M.\nMore formally, we deﬁne K ∈ RM×Dk as the keys and V ∈ RC×Dv as the values. W e get\nthe estimate of attention output ˜V o by the following equations:\n˜A = so f t max (PKT /\n√\nDk ) (4)\n˜W = ˜AV (5)\n˜V o\ni = ˜Wj , i f G i = j (6)\nwhere the softmax function is applied row-wise and Gi represents the index of the cluster\nthat Qi belongs to.\n3.3 Multi-T ask Knowledge Distillation\nAlthough ACT can reduce the computation complexity of DETR w ithout retraining, we\nshow that Multi-T ask Knowledge Distillation(MTKD) can fur ther improve ACT with a few-\nepoch of ﬁne-tuning and produce a better balance between per formance and computation.\nThe pipeline of MTKD is illustrated in ﬁgure 3. Image features will be extracted by the\npre-trained CNN backbone ﬁrst. The extracted feature will b e passed into ACT and the\noriginal transformer parallelly. T o enable a seamless swit ch between ACT and the original\ntransformer, MTKD will guide the training. The training los s is denoted below:\nL = Lpred (Y,Y2 ) +LKD (B1 ,B2 .det ach()) (7)\nZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER 7\n0\n2\n4\n6\n8\n10 \n12 \n14 \n16 \n150 155 160 165 170 175 180\nMSE(×1E-7) \nGFLOPs \nFix r to 8 and change L \nFix L to 24 and change r \nr=2 \nL=24 \nr=8 \nr=12 \nr=6 \nr=4 \nL=16 \nL=20 \nL=32 \nL=40 \nFigure 4: The mean square error between\nthe estimated attention map and the true at-\ntention map under a certain computational\nbudget. W e ﬁx L to 24 and set r to 2,4,6,8,12\nrespectively. Then we ﬁx r to 8 and set L to\n16, 20, 24, 32 respectively.\n32 \n34 \n36 \n38 \n40 \n42 \n44 \n46 \n150 160 170 180 190\nAP \nGFLOPs \nDETR \nACT (our) \nACT+MTKD (our) \nK-Means \nL=32 L=24 \nL=20 \nL=16 \nC=500 \nC=800\nC=1000\nS_n \nFigure 5: W e compare the AP of ACT with\nDETR-DC5 and the K-mean clustering. W e\nrefer to hash rounds in our model as L and\nrefer to the number of clusters in K-means\nas C.\nwhere Y represents the ground truth, B1 represents the predicted bounding box of ACT , and\nB2,Y2 represent the predicted bounding box and full prediction of DETR. Lpred (Y,Y2 ) is\nthe original loss between the ground truth and the predictio n of DETR. LKD (B1 ,B2) is the\nknowledge distillation loss which minimizes the L2 distanc e between the predicted bounding\nbox of ACT and DETR.\nThe training loss aims to train the original transformer joi ntly with knowledge transfer\nbetween full prediction and approximated prediction which enables a seamless switch be-\ntween ACT and Transformer. The knowledge transformer inclu des region classiﬁcation and\nregression distillation. The regression branch is more sen sitive to the approximated error\nintroduced by ACT than the classiﬁcation branch. Thus, we on ly transfer the knowledge of\nthe bounding box regression branch. W e observe much faster c onvergence by transferring\nthe box regression branch only.\n4 Experiment\n4.1 Dataset\nW e perform experiments on COCO 2017 detection dataset [ 26], which containing 118k train-\ning images and 5k validation images. Each image in the datase t contains up to 63 instances\nof different sizes. W e report AP as bbox AP , the integral metr ic over multiple thresholds.\nW e also report the average FLOPs for the ﬁrst 100 images in the COCO 2017 validation\nset. Only the FLOPs of convolutional layers, fully connecte d layers, matrix operations in\nattention, E2LSH, and clustering will be considered.\n4.2 Experiment Setup\nW e choose the pre-trained DETR-DC5 model [ 3] as our baseline. It uses the deep residual\nnetwork [ 17] with 50 layers (ResNet-50) as the backbone and increases th e feature resolution\nby adding a dilated convolution [ 43] to the last stage of the backbone and removing a stride\nfrom the ﬁrst convolution of this stage. DETR-DC5 contains 6 encoder layers and 6 decoder\nlayers with 8 attention heads.\n8 ZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER\nW e replace the attention module in the encoder with our adapt ive clustering attention\nwhile keeping the other parts unchanged. W e randomly sample 1000 images on the training\nset and calculate the mean square error between our estimate d attention map and the true\nattention map to determine an appropriate hyper-parameter r and control the FLOPs of the\nmodel by changing the hyper-parameter L, where L represent the round of E2LSH and r\nrepresent the interval of the hashing hyperplanes.\nDuring inference, we resize the input images such that the sh ortest side is at most 800\npixels while the longest at most 1333. For Multi-T ask Knowle dge Distillation(MTKD), we\nadopt a random crop augmentation which is also used in DETR. W e use a simple L2 norm\nfor regression distillation and the weight of KD loss is set t o 1. MTKD performs ﬁne-tuning\nover the pre-trained model for 5 epochs with the learning rat e of 10 −5 and continues running\nfor 2 epochs by reducing the learning rate by 1/10. MTKD is opt imised by AdamW [ 22, 31].\n4.3 Ablation Study\nThe hyper-parameters of the E2LSH have a great inﬂuence on th e quality of the approxima-\ntion and the FLOPs of the model. In our ablation analysis, we e xplore how these hyperpa-\nrameters affect the results and try to determine appropriat e hyper-parameters.\nW e randomly sample 1000 images on the training set and input t he images into DETR\nand our ACT respectively to obtain the attention map in diffe rent encoder layers and different\nattention heads. W e calculate the mean square error between our estimated attention map and\nthe true attention map. W e perform two sets of experiments. F irst, we ﬁx L to 24 and set r to\n2,4,6,8,12 respectively. Then, we ﬁx r to 8 and set L to 16, 20, 24, 32 respectively.\nThe results are shown in Figure 4. Firstly, the estimation error decreases with an increase\nof L and a decrease of r. Secondly, when r is greater than 6, continuing to increase r has\nlittle effect on estimation errors and FLOPs. Therefore, it is a better choice to obtain models\nof different FLOPs by the change of L. Finally, we found that when r is less than or equal to\n6, continuing to reduce r will cause a larger increase in FLOPs while a smaller decreas e in\nerror, which is not cost-effective. Thus, in all subsequent experiments, we ﬁx r to 8.\nAnother signiﬁcant discovery is that adaptively clusterin g keys using our method can\nalso achieve a good result. W e perform experiments on cluste ring queries, clustering keys,\nand clustering both queries and keys respectively. W e adjus t the hyper-parameter L to ensure\nthat the three experiments have similar FLOPs, and we compar e the AP on the validation set.\nThe results are given in T able 2. As we can see, these three methods have achieved similar\nAP under the same FLOPs, which also proves the generalizatio n of our model. This means\nthat for some models where the number of keys is signiﬁcantly more than the number of\nqueries, we can adaptively cluster the keys to obtain higher efﬁciency.\n4.4 Final Performance\nSpeed Accuracy T rade-off. In this section, we start by comparing the AP of our model wit h\nDETR-DC5. W e also compare our adaptive clustering method wi th K-means clustering used\nby Vyas et al . [ 40]. W e refer to the number of clusters in K-means as C. W e adjust L and\nC and calculate the AP under different computational budgets . As we can see in Figure 5,\nunder an equalized computational budget, the AP of our ACT mo del is much higher than K-\nmeans’s. Compared with DETR-DC5, we reduce the FLOPs from 18 4.1 GFLOPs to 168.9\nGFLOPs while the loss in AP is only 0.7%. The yellow line also s hows the result of a Multi-\nT ask Knowledge Distillation (MTKD). MTKD can signiﬁcantly improve AP , especially for\nZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER 9\nModel GFLOPs AP AP L APM APS\nBackbone (ResNet50-DC5) 110.7\nDETR-DC5 [ 3] +73.4 43.3 61.1 47.3 22.5\nFaster RCNN-DC5 [ 35] +209.3 41.1 55.0 45.9 22.9\nACT (L=32) +58.2 42.6 61.1 46.8 21.4\nACT (L=24) +53.1 41.3 60.6 45.9 19.2\nACT (L=20) +49.4 39.7 60.3 44.2 16.9\nACT (L=16) +45.0 37.1 58.8 41.3 13.9\nACT+MTKD (L=32) +58.2 43.1 61.4 47.1 22.2\nACT+MTKD (L=24) +53.1 42.3 61.0 46.4 21.3\nACT+MTKD (L=20) +49.5 41.8 60.7 45.6 20.6\nACT+MTKD (L=16) +45.1 40.6 59.7 44.3 18.5\nT able 1: W e compare the AP of our model with DETR-DC5 and Faste r RCNN in detail.\nDETR-DC5 and Faster RCNN use dilated ResNet-50 as the backbo ne. The sign ‘+’ in\nGFLOPs column refers to the ﬂops increased relative to the ba ckbone. W e refer to the bbox\nAP of the large, medium, and large size instances as AP L , AP M , and AP S respectively.\nCluster\nqueries\nCluster\nkeys FLOPs AP\nL = 24 × 163.77 0.413\n× L = 8 163.48 0.414\nL = 32 L = 12 162.7 0.411\nT able 2: The AP and FLOPs under dif-\nferent clustering targets. W e perform ex-\nperiments on clustering queries, clustering\nkeys, and clustering both queries and keys\nrespectively.\nModel Inference Time\nper Image Memory\nDETR-DC5 0.246s 1862MiB\nACT(L=32) 0.218s 1733MiB\nACT(L=24) 0.207s 1584MiB\nACT(L=20) 0.195s 1415MiB\nACT(L=16) 0.183s 1142MiB\nT able 3: The inference time and memory\ncost on an Nvidia GeForce GTX TIT AN X\nwith batch size of 1.\nthose models with smaller ﬂops. Through MTKD, the AP of ACT wi th L = 16 is increased\nby 4.3%, and ACT with L = 32 achieves almost the same performance as DETR-DC5.\nW e also analyze the advantages of our method compared to the K -means clustering.\nK-means clustering uses the same number of clusters in all of the encoder layers. But it\nis hard the determine an appropriate hyper-parameter becau se the distribution of queries\nvaries greatly with the input image and the index of the encod er layer. An inappropriate\nhyper-parameter C will lead to bad estimates or a waste of computing resources. Another\ndisadvantage is that K-means may not converge well in some ca ses, and many clusters will\nbe empty when C is relatively large. Our method adaptively determines the n umber of pro-\ntotypes so that these disadvantages can be avoided.\nCompare AP in Detail . W e refer to the bbox AP of the large, medium, and large size\ninstances as AP L , AP M , and AP S respectively. In this section, we will compare these metric s\nwith Faster RCNN [ 35] and DETR [ 3]. DETR and Faster RCNN use dilated ResNet-50 as\nbackbone. T able 4.3 shows the results in detail.\nOur ACT model with L equal to 32 achieves the similar performa nce as Faster RCNN-\nDC5 with much fewer FLOPs. What’s more, ACT is much stronger t han Faster RCNN-\nDC5 in detecting objects with large or medium size. Our ACT mo del can approximate the\nattention map in DETR well especially for large-size object s. For example, when L=32,\nthe AP L of our model is the same as DETR-DC5. Most of the loss in AP occu rs in small\nand medium-sized objects. Through a few-epoch of MTKD, the A Ps in different sizes have\nbeen signiﬁcantly improved, especially for those models wi th fewer FLOPs. And our ACT\nwith L equal to 32 achieves almost the same performance as DET R-DC5. An interesting\n10 ZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER\nFigure 6: W e visualize some representative\nclusters in the encoder. The queries where\nthe white pixel is located belong to the same\ncluster.\nFigure 7: The ratio of the number of proto-\ntypes to the number of queries in each en-\ncoder layer. W e show the average value on\nthe ﬁrst 100 images in the validation set.\nﬁnding is that AP L of our ACT is 0.3% higher than DETR’s. W e believe that in the pr ocess\nof knowledge distillation, our clustering attention can be regarded as a dropout operation,\nwhich can prevent overﬁtting.\nInference Time and Memory. The above analyses are based on theoretical computation\ncost (FLOPs). W e also tested the time and memory cost in a real environment. T able 3 shows\nthe inference time and memory cost on an Nvidia GeForce GTX TI T AN X with the batch\nsize of 1. As we can see, the acceleration of ACT in a real envir onment is consistent with\nthe theoretical analysis, and the memory cost is also signiﬁ cantly reduced.\n5 Visualisation of Adaptive Clustering\nT o analyze which queries are represented by the same prototy pe, we visualize some repre-\nsentative clusters in Figure 6. W e can easily ﬁnd that the three clusters displayed are the\nfeatures of the cow , the sky, and the ﬁeld. This indicates tha t our clustering is related to se-\nmantics and location. Those queries with similar semantics and similar locations will easily\nbe grouped.\nT o prove that our method can adaptively determine the number of prototypes based on the\ndistribution of queries, Figure 7 counted the ratio of the number of prototypes to the number\nof queries in each encoder layer for the images on the validat ion set. W e can ﬁnd that as the\nencoder layer goes deeper, the number of prototypes shows a d ownward trend, because the\nfeatures are more redundant there. This shows that our adapt ive clustering method is very\neffective for this situation where the query distribution c hanges greatly.\n6 Conclusion\nIn this paper, we propose the Adaptive Clustering Transform er (ACT) to reduce the com-\nputation and memory costs for object detection. Most previo us efﬁcient transformers need\na re-training when applied to DETR. However, training the DE TR requires 500 epochs ap-\nproximately 1920 GPU hours for a single V100 GPU. Our propose d ACT does not need any\nre-training process due to the compatibility between ACT an d Transformer. ACT reduces\nthe redundancy in the pre-trained DETR in a clever adaptive c lustering way. In the future,\nwe will look into the ACT on training from scratch setting and also apply ACT to perform\ncross-scale information fusion over multi-scale Feature P yramid Network (FPN) [ 27].\nZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER 11\nAcknowledgement. This project is supported by National Natural Science Found ation of\nChina—Y outh Science Fund (No.62006006) and Shanghai Commi ttee of Science and T ech-\nnology, China (Grant No. 21DZ1100100 and 20DZ1100800).\nReferences\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Y oshua Bengio. Neu ral machine translation\nby jointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.\n[2] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larr y S Davis. Soft-nms–\nimproving object detection with one line of code. In Proceedings of the IEEE in-\nternational conference on computer vision , pages 5561–5569, 2017.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nic olas Usunier, Alexander Kir-\nillov, and Sergey Zagoruyko. End-to-end object detection w ith transformers. arXiv\npreprint arXiv:2005.12872 , 2020.\n[4] Y unpeng Chen, Y annis Kalantidis, Jianshu Li, Shuicheng Y an, and Jiashi Feng. Aˆ\n2-nets: Double attention networks. In Advances in neural information processing sys-\ntems, pages 352–361, 2018.\n[5] Y unpeng Chen, Marcus Rohrbach, Zhicheng Y an, Y an Shuich eng, Jiashi Feng, and\nY annis Kalantidis. Graph-based global reasoning networks . In Proceedings of the\nIEEE Conference on Computer V ision and P attern Recognition , pages 433–442, 2019.\n[6] Krzysztof Choromanski, V alerii Likhosherstov, David D ohan, Xingyou Song, Andreea\nGane, T amas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiu ddin, Lukasz Kaiser,\net al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794 , 2020.\n[7] Mayur Datar, Nicole Immorlica, Piotr Indyk, and V ahab S M irrokni. Locality-sensitive\nhashing scheme based on p-stable distributions. In Proceedings of the twentieth annual\nsymposium on Computational geometry , pages 253–262, 2004.\n[8] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. Bert: Pre-\ntraining of deep bidirectional transformers for language u nderstanding. arXiv preprint\narXiv:1810.04805 , 2018.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov , Dirk W eissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Mind erer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transformer s for image recognition\nat scale. arXiv preprint arXiv:2010.11929 , 2020.\n[10] Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression:\na statistical view of boosting (with discussion and a rejoin der by the authors). The\nannals of statistics , 28(2):337–407, 2000.\n[11] Peng Gao, Zhengkai Jiang, Haoxuan Y ou, Pan Lu, Steven CH Hoi, Xiaogang W ang,\nand Hongsheng Li. Dynamic fusion with intra-and inter-moda lity attention ﬂow for\nvisual question answering. In Proceedings of the IEEE Conference on Computer V ision\nand P attern Recognition , pages 6639–6648, 2019.\n12 ZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER\n[12] Peng Gao, Haoxuan Y ou, Zhanpeng Zhang, Xiaogang W ang, a nd Hongsheng Li. Multi-\nmodality latent interaction network for visual question an swering. In Proceedings of\nthe IEEE International Conference on Computer V ision , pages 5825–5835, 2019.\n[13] Peng Gao, Minghang Zheng, Xiaogang W ang, Jifeng Dai, an d Hongsheng Li.\nFast convergence of detr with spatially modulated co-atten tion. arXiv preprint\narXiv:2101.07448 , 2021.\n[14] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on\ncomputer vision , pages 1440–1448, 2015.\n[15] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jiten dra Malik. Rich feature hierar-\nchies for accurate object detection and semantic segmentat ion. In Proceedings of the\nIEEE conference on computer vision and pattern recognition , pages 580–587, 2014.\n[16] Saurabh Goyal, Anamitra Roy Choudhary, V enkatesan Cha karavarthy, Saurabh Man-\nishRaje, Y ogish Sabharwal, and Ashish V erma. Power-bert: A ccelerating bert inference\nfor classiﬁcation tasks. arXiv preprint arXiv:2001.08950 , 2020.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. D eep residual learning\nfor image recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 770–778, 2016.\n[18] Geoffrey Hinton, Oriol V inyals, and Jeff Dean. Distill ing the knowledge in a neural\nnetwork. arXiv preprint arXiv:1503.02531 , 2015.\n[19] Sepp Hochreiter and Jürgen Schmidhuber. Long short-te rm memory. Neural computa-\ntion, 9(8):1735–1780, 1997.\n[20] Zhengkai Jiang, Peng Gao, Chaoxu Guo, Qian Zhang, Shimi ng Xiang, and Chunhong\nPan. V ideo object detection with locally-weighted deforma ble neighbors. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence , volume 33, pages 8529–8536,\n2019.\n[21] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Trans-\nformers are rnns: Fast autoregressive transformers with li near attention. arXiv preprint\narXiv:2006.16236 , 2020.\n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stocha stic optimization. arXiv\npreprint arXiv:1412.6980 , 2014.\n[23] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Ref ormer: The efﬁcient trans-\nformer. arXiv preprint arXiv:2001.04451 , 2020.\n[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton . Imagenet classiﬁcation with\ndeep convolutional neural networks. Communications of the ACM , 60(6):84–90, 2017.\n[25] Harold W Kuhn. The hungarian method for the assignment p roblem. Naval research\nlogistics quarterly , 2(1-2):83–97, 1955.\n[26] Tsung-Y i Lin, Michael Maire, Serge Belongie, James Hay s, Pietro Perona, Deva Ra-\nmanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco : Common objects in\ncontext. In European conference on computer vision , pages 740–755. Springer, 2014.\nZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER 13\n[27] Tsung-Y i Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge\nBelongie. Feature pyramid networks for object detection. I n Proceedings of the IEEE\nconference on computer vision and pattern recognition , pages 2117–2125, 2017.\n[28] Tsung-Y i Lin, Priya Goyal, Ross Girshick, Kaiming He, a nd Piotr Dollár. Focal loss\nfor dense object detection. In Proceedings of the IEEE international conference on\ncomputer vision , pages 2980–2988, 2017.\n[29] W ei Liu, Dragomir Anguelov, Dumitru Erhan, Christian S zegedy, Scott Reed, Cheng-\nY ang Fu, and Alexander C Berg. Ssd: Single shot multibox dete ctor. In European\nconference on computer vision , pages 21–37. Springer, 2016.\n[30] Ze Liu, Y utong Lin, Y ue Cao, Han Hu, Y ixuan W ei, Zheng Zha ng, Stephen Lin, and\nBaining Guo. Swin transformer: Hierarchical vision transf ormer using shifted win-\ndows. arXiv preprint arXiv:2103.14030 , 2021.\n[31] Ilya Loshchilov and Frank Hutter. Fixing weight decay r egularization in adam. 2018.\n[32] Alexander Neubeck and Luc V an Gool. Efﬁcient non-maxim um suppression. In 18th\nInternational Conference on P attern Recognition (ICPR’06 ), volume 3, pages 850–855.\nIEEE, 2006.\n[33] Duy-Kien Nguyen and T akayuki Okatani. Improved fusion of visual and language\nrepresentations by dense symmetric co-attention for visua l question answering. In Pro-\nceedings of the IEEE Conference on Computer V ision and P atte rn Recognition , pages\n6087–6096, 2018.\n[34] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. Y ou only look once:\nUniﬁed, real-time object detection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 779–788, 2016.\n[35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. F aster r-cnn: T owards real-\ntime object detection with region proposal networks. In Advances in neural information\nprocessing systems , pages 91–99, 2015.\n[36] Bernardino Romera-Paredes and Philip Hilaire Sean T or r. Recurrent instance segmen-\ntation. In European conference on computer vision , pages 312–329. Springer, 2016.\n[37] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. E nd-to-end people detection\nin crowded scenes. In Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 2325–2333, 2016.\n[38] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones, Aidan N\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is al l you need. In Advances in\nneural information processing systems , pages 5998–6008, 2017.\n[39] Paul V iola and Michael Jones. Rapid object detection us ing a boosted cascade of simple\nfeatures. In Proceedings of the 2001 IEEE computer society conference on computer\nvision and pattern recognition. CVPR 2001 , volume 1, pages I–I. IEEE, 2001.\n[40] Apoorv Vyas, Angelos Katharopoulos, and François Fleu ret. Fast transformers with\nclustered attention. Advances in Neural Information Processing Systems , 33, 2020.\n14 ZHENG ET AL .: ADAPTIVE CLUSTERING TRANSFORMER\n[41] Xiaolong W ang, Ross Girshick, Abhinav Gupta, and Kaimi ng He. Non-local neural\nnetworks. In Proceedings of the IEEE conference on computer vision and pa ttern\nrecognition, pages 7794–7803, 2018.\n[42] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Co urville, Ruslan\nSalakhudinov, Rich Zemel, and Y oshua Bengio. Show , attend a nd tell: Neural im-\nage caption generation with visual attention. In International conference on machine\nlearning, pages 2048–2057, 2015.\n[43] Fisher Y u and Vladlen Koltun. Multi-scale context aggr egation by dilated convolutions.\narXiv preprint arXiv:1511.07122 , 2015.\n[44] Zhou Y u, Jun Y u, Y uhao Cui, Dacheng T ao, and Qi Tian. Deep modular co-attention\nnetworks for visual question answering. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 6281–6290, 2019.\n[45] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chen glong Bao, and Kaisheng\nMa. Be your own teacher: Improve the performance of convolut ional neural networks\nvia self distillation. In Proceedings of the IEEE International Conference on Comput er\nV ision, pages 3713–3722, 2019.\n[46] Xizhou Zhu, W eijie Su, Lewei Lu, Bin Li, Xiaogang W ang, a nd Jifeng Dai. De-\nformable detr: Deformable transformers for end-to-end obj ect detection. arXiv preprint\narXiv:2010.04159 , 2020.\n[47] Zhen Zhu, Mengde Xu, Song Bai, T engteng Huang, and Xiang Bai. Asymmetric non-\nlocal neural networks for semantic segmentation. In Proceedings of the IEEE Interna-\ntional Conference on Computer V ision , pages 593–602, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7696866989135742
    },
    {
      "name": "Computation",
      "score": 0.5907065868377686
    },
    {
      "name": "Cluster analysis",
      "score": 0.5882660746574402
    },
    {
      "name": "Transformer",
      "score": 0.5855187177658081
    },
    {
      "name": "Inference",
      "score": 0.5465171337127686
    },
    {
      "name": "Hash function",
      "score": 0.4794110953807831
    },
    {
      "name": "Data mining",
      "score": 0.4749077558517456
    },
    {
      "name": "Spectral clustering",
      "score": 0.4396442174911499
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3274523913860321
    },
    {
      "name": "Algorithm",
      "score": 0.18208369612693787
    },
    {
      "name": "Programming language",
      "score": 0.1091049313545227
    },
    {
      "name": "Engineering",
      "score": 0.08804303407669067
    },
    {
      "name": "Voltage",
      "score": 0.0846540629863739
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 117
}