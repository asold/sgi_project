{
    "title": "JDI-T: Jointly trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment",
    "url": "https://openalex.org/W3025793647",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5027636448",
            "name": "Dan Lim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5103192158",
            "name": "Won Jang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5006362544",
            "name": "O Gyeonghwan",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5027073468",
            "name": "Heayoung Park",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5061409578",
            "name": "Bong‐Wan Kim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5050154985",
            "name": "Jaesam Yoon",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2903739847",
        "https://openalex.org/W2994689640",
        "https://openalex.org/W2971905065",
        "https://openalex.org/W3015922793",
        "https://openalex.org/W2949382160",
        "https://openalex.org/W2942807473",
        "https://openalex.org/W2886769154",
        "https://openalex.org/W3015338123",
        "https://openalex.org/W2963300588",
        "https://openalex.org/W2970730223",
        "https://openalex.org/W2964243274",
        "https://openalex.org/W3016160783",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2191779130",
        "https://openalex.org/W2767052532",
        "https://openalex.org/W2971753973",
        "https://openalex.org/W3016136182",
        "https://openalex.org/W2963609956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2120847449"
    ],
    "abstract": "We propose Jointly trained Duration Informed Transformer (JDI-T), a feed-forward Transformer with a duration predictor jointly trained without explicit alignments in order to generate an acoustic feature sequence from an input text. In this work, inspired by the recent success of the duration informed networks such as FastSpeech and DurIAN, we further simplify its sequential, two-stage training pipeline to a single-stage training. Specifically, we extract the phoneme duration from the autoregressive Transformer on the fly during the joint training instead of pretraining the autoregressive model and using it as a phoneme duration extractor. To our best knowledge, it is the first implementation to jointly train the feed-forward Transformer without relying on a pre-trained phoneme duration extractor in a single training pipeline. We evaluate the effectiveness of the proposed model on the publicly available Korean Single speaker Speech (KSS) dataset compared to the baseline text-to-speech (TTS) models trained by ESPnet-TTS.",
    "full_text": "JDI-T: Jointly trained Duration Informed Transformer for Text-To-Speech\nwithout Explicit Alignment\nDan Lim1, Won Jang2, Gyeonghwan O2, Heayoung Park2, Bongwan Kim2, Jaesam Yoon2\n1Kakao Corp., Seongnam, Korea\n2Kakao Enterprise Corp., Seongnam, Korea\nsatoshi.2018@kakaocorp.com\nAbstract\nWe propose Jointly trained Duration Informed Transformer\n(JDI-T), a feed-forward Transformer with a duration predictor\njointly trained without explicit alignments in order to generate\nan acoustic feature sequence from an input text. In this work,\ninspired by the recent success of the duration informed net-\nworks such as FastSpeech and DurIAN, we further simplify its\nsequential, two-stage training pipeline to a single-stage train-\ning. Speciﬁcally, we extract the phoneme duration from the\nautoregressive Transformer on the ﬂy during the joint train-\ning instead of pretraining the autoregressive model and using\nit as a phoneme duration extractor. To our best knowledge, it is\nthe ﬁrst implementation to jointly train the feed-forward Trans-\nformer without relying on a pre-trained phoneme duration ex-\ntractor in a single training pipeline. We evaluate the effective-\nness of the proposed model on the publicly available Korean\nSingle speaker Speech (KSS) dataset compared to the baseline\ntext-to-speech (TTS) models trained by ESPnet-TTS.\nIndex Terms: text-to-speech, speech synthesis, Transformer,\nKorean speech\n1. Introduction\nDeep learning approaches to text-to-speech (TTS) task have\nmade signiﬁcant progress in generating highly natural speech\nclose to human quality. Especially attention-based encoder-\ndecoder models such as Tacotron [1] and Tacotron2 [2] are dom-\ninant in this area. They generate an acoustic feature sequence,\nmel-spectrogram, for example, from an input text autoregres-\nsively using an attention mechanism where the attention mech-\nanism plays the role of implicit aligner between the input se-\nquence and the acoustic feature sequence. Finally, Grifﬁn-Lim\nalgorithm [3] or a neural vocoder such as WaveNet [4], Wave-\nGlow [5] or Parallel WaveGAN [6] is used to convert the pre-\ndicted acoustic feature sequence to corresponding audio sam-\nples.\nBesides Recurrent Neural Network (RNN) based TTS mod-\nels (Tacotron [1], Tacotron2 [2]), Transformer [7] has also been\napplied for TTS in the attention-based encoder-decoder frame-\nwork successfully achieving the quality of human recording.\nThe self-attention module, followed by a nonlinear transforma-\ntion in the Transformer [8], solves the long-range dependency\nproblem by constructing a direct path between any two inputs\nat different time steps and improves the training efﬁciency by\ncomputing the hidden states in an encoder and a decoder in par-\nallel.\nDespite its success in synthesizing high-quality speech, the\nattention-based encoder-decoder models are prone to the syn-\nthesis error, which prevents its commercial use. The unstable\nattention alignment at synthesis causes the synthesized speech\nto be imperfect, e.g., phoneme repeat, skip, or mispronuncia-\ntion. To solve this problem, duration informed networks such\nas FastSpeech [9] and DurIAN [10] reduce the errors by relying\non a duration predictor instead of the attention mechanism.\nThe reliance on the duration predictor instead of the at-\ntention mechanism is more robust since the duration predic-\ntor guarantees stepwise and monotonic alignments between a\nphoneme sequences and mel-spectrogram. Although the du-\nration informed networks can synthesize high-quality speech\nwithout the synthesis error, the training process is tricky: a pre-\ntrained model must be prepared as a phoneme duration extractor\nsince the duration informed networks cannot be trained without\na reference phoneme duration sequence.\nFor example, FastSpeech [9] extracts a phoneme dura-\ntion sequence from attention alignments matrix of pre-trained\nautoregressive Transformer for training a feed-forward Trans-\nformer and a duration predictor. On the other hand, DurIAN\n[10] uses the forced alignment, which is commonly used in sta-\ntistical parametric speech-synthesis systems to train their du-\nration models. Thus, the previous duration informed networks\nhave sequential, two-stage training pipeline, which may slow\ndown the model training time. More recently, duration in-\nformed network not requiring pre-trained model [11] has been\nproposed, but it still requires a multi-stage training phase.\nWe are motivated by a simple idea that if the attention\nmechanism of the autoregressive Transformer can provide re-\nliable phoneme duration sequences from the early in joint train-\ning, the previous two-stage training pipeline of the duration in-\nformed networks could be combined. In this paper, we sim-\nplify the training pipeline of the duration informed networks by\njointly training the feed-forward Transformer and the duration\npredictor with the autoregressive Transformer. The contribu-\ntions of our work are as follow:\n• We propose a novel training framework where the feed-\nforward Transformer and the duration predictor are\ntrained jointly with the autoregressive Transformer. By\nacquiring reference phoneme duration from the autore-\ngressive Transformer during the training on the ﬂy, the\nprevious two-stage training pipeline of the typical du-\nration informed networks such as FastSpeech [9] or\nDurIAN [10] is simpliﬁed to a single-stage training.\n• We remedy an instability of the attention mechanism of\nthe autoregressive Transformer by adding an auxiliary\nloss and adopt a forward attention mechanism [12]. This\nmakes the phoneme duration sequence extracted from\nthe attention mechanism reliable from the early on in\ntraining.\n• We prove the effectiveness of the proposed model by\ncomparing it with popular TTS models (Tacotron2,\nTransformer, and FastSpeech) implemented from\narXiv:2005.07799v3  [eess.AS]  5 Oct 2020\nPhoneme Embedding\nEncoder Pre-net\nFFT Block\nFFT Block\nDuration Predictor\nDecoder\nLength Regulator\nAttention Mechanism\nx\nl1(y, y′\u0000 )\nl1(y, y′\u0000 ′\u0000 )\ny\nl2(d, d′\u0000 )\nScaled\nPositional\nEncoding\nPhoneme Sequence\ny′\u0000 \ny′\u0000 ′\u0000 \nMel-spectrogram\ny\ny\nd\nd′\u0000 \nN ×\nN ×\nDecoder Pre-net\nLinear Layer\nLinear Layer\nFigure 1: An illustration of our proposed joint training frame-\nwork (Auxiliary loss for attention is omitted for brevity.)\nESPnet-TTS [13] on the publicly available Korean\ndataset.\n2. Model description\nThe main idea of JDI-T is to train feed-forward Transformer\nand duration predictor with autoregressive Transformer jointly.\nIn this section, we describe each component in detail.\n2.1. Feed-Forward Transformer\nFeed-forward Transformer located on the left in Figure 1 con-\nsists of a phoneme embedding, an encoder pre-net, scaled po-\nsitional encodings, multiple Feed-Forward Transformer (FFT)\nblocks, a length regulator and a linear layer for phoneme se-\nquence to mel-spectrogram transformation.\nAs shown in Figure 2a, the structure of FFT blocks is com-\nposed of a multi-head attention and a single layer 1D convolu-\ntion network where residual connections, layer normalization,\nand dropout are used. It is slightly different from what described\nin [9]. Note that the output of each FFT block on both sides is\nnormalized by layer normalization. Encoder pre-net and scaled\npositional encoding have the same conﬁguration as described in\nTransformer TTS [7].\nThe stacked modules from the phoneme embedding to the\nFFT blocks below the length regulator works as the encoder,\nwhich is also shared by autoregressive Transformer and dura-\ntion predictor. The length regulator regulates an alignment be-\ntween the phoneme sequences and the mel-spectrogram in the\nsame way described in FastSpeech [9], expanding the output\nsequences of FFT blocks on phoneme side according to refer-\nence phoneme duration so that total length of it matches the\ntotal length of mel-spectrogram. The feed-forward Transformer\nis trained to minimize l1 loss between predicted and reference\nmel-spectrogram.\n2.2. Autoregressive Transformer\nAutoregressive Transformer is an attention-based encoder-\ndecoder model as shown in Figure 1 where it shares the encoder\nwith the feed-forward Transformer on the left bottom and the\noutput of the encoder is attended by the stacked modules on\nthe right consisting of a decoder pre-net, a scaled positional en-\ncoding, a decoder, and a linear layer. The decoder depicted in\n(a) FFT Block\n (b) Decoder\nFigure 2: (a) The feed-forward Transformer block. (b) The de-\ncoder for autoregressive Transformer\nFigure 2b has the similar structure with the FFT block in Figure\n2a but there are differences of having masked multi-head atten-\ntion instead of multi-head attention, additional sub-layer in the\nmiddle for attention mechanism over the outputs of the encoder\nand position-wise Feed Forward Network (FFN) instead of 1D\nconvolution network. Note that the output of the decoder is nor-\nmalized by layer normalization. The FFN of the decoder and\nthe decoder pre-net have the same conﬁguration as described in\nTransformer TTS [7].\nThe autoregressive Transformer provides reference\nphoneme duration during training for each phoneme on the\nﬂy. Speciﬁcally, while training the autoregressive Transformer\nto minimize l1 loss between predicted and reference mel-\nspectrogram, phoneme duration is extracted from the attention\nalignment matrix by counting the number of mel frames\nwhich scores the highest value for each distinct phoneme. The\nphoneme duration acquired in this way is used as reference\nphoneme duration for the length regulator of feed-forward\nTransformer and the duration predictor. Note that the decoder,\nunlike described in Transformer TTS [7], neither has been\nstacked nor has multi-head attention over the outputs of the\nencoder since it was sufﬁcient as a phoneme duration extractor.\n2.3. Duration Predictor\nDuration predictor consists of a 2-layer 1D convolution net-\nwork with ReLU activation followed by the layer normalization,\ndropout layer and an extra linear layer to output a scalar value\nwhich is interpreted as phoneme duration in the logarithmic do-\nmain. The depiction of duration predictor is omitted since it\nis exactly same as described in FastSpeech [9]. As shown in\nthe middle top of Figure 1, the duration predictor also shares\nthe encoder with feed-forward, autoregressive Transformer so\nit is stacked on top of the FFT blocks on the phoneme side.\nIn order to minimize the difference between predicted and ref-\nerence phoneme duration, l2 loss is used, where the reference\nphoneme duration is extracted from an attention alignment ma-\ntrix of autoregressive Transformer during training. Note that\nwe stop gradients propagation from the duration predictor to the\nFFT blocks during joint training, since the training fails without\nit.\n3. Auxiliary loss and Attention mechanism\nIt is very important to stabilize the attention alignments of au-\ntoregressive Transformer, since the training of the proposed\nmodel requires precise phoneme duration and the quality of\nphoneme duration is affected by the stability of attention align-\nment of autoregressive Transformer. In this section, we describe\nsome methods for acquiring reliable phoneme duration by stabi-\nlizing attention alignments of autoregressive Transformer from\nthe early in training steps.\n3.1. CTC recognizer loss\nIn [14], authors insist that the synthesis error rate of attention\nbased TTS models can be alleviated if the model is guided to\nlearn the dependency between the input text and the predicted\nacoustic feature sequences by maximizing the mutual informa-\ntion between them. After formulating that maximizing the mu-\ntual information is equivalent to training an auxiliary recog-\nnizer, they show that training CTC recognizer as auxiliary loss\nfor the Tacotron TTS model can reduce the synthesis error.\nAlthough the method of maximizing the mutual informa-\ntion is proposed for reducing the synthesis error of the attention-\nbased model, we found that it also helps autoregressive Trans-\nformers learn stable attention alignments during training, which\nis a crucial factor for the proposed model to be jointly trained\nsuccessfully. In this work, we implement it just by adding an ex-\ntra CTC loss layer. Speciﬁcally, the extra linear layer is stacked\non top of the decoder of the autoregressive Transformer so that\nit is trained to predict input phoneme sequences.\n3.2. Forward attention with Guided attention loss\nAs reported in [15], location-relative attention mechanisms are\nmore preferred over the content-based attention mechanisms in\nthat they not only reduce synthesis error but also help the model\nalign quickly during training. However, it is not proper for par-\nallel computing. If it is used in the autoregressive Transformer\nsuch as Transformer TTS [7], the computation efﬁciency is sac-\nriﬁced. Therefore we adopt forward attention mechanism [12]\non top of content-based attention mechanism for minimal com-\nputational overhead instead of location-relative attention mech-\nanisms. The forward attention mechanism guarantees fast con-\nvergence and stability of the attention alignments by consider-\ning only monotonic attention alignment paths, which is a nat-\nural assumption between the input phoneme sequences and the\noutput acoustic feature sequences.\nAlgorithm 1 states the forward attention mechanism\nadopted for the proposed model. The algorithm has same pro-\ncedures as in original paper [12] except that it prepares the at-\ntention weights w1:T (1 :N) for all time steps N,T in parallel\nusing the attention mechanism Attention. The Attentionis\na single-head, content-based attention from the decoder of au-\ntoregressive Transformer using an output sequences of the FFT\nblocks on phoneme side h1:N and an output sequences of the\ndecoder s1:T which has the length of N and T respectively.\nAlthough the forward attention mechanism guarantees\nmonotonic attention alignments in principle, it is observed that\nthe training often fails without learning any valid alignments.\nWe found that guiding attention alignments with auxiliary loss\nis useful in solving the problem [16, 17]. In the proposed model,\nthe guided attention loss with the same conﬁguration as de-\nAlgorithm 1 Forward Attention\nInitialize:\nˆα0(1) ←1\nˆα0(n) ←0,n = 2,...,N\nw1:T (1 :N) ←Attention(h1:N ,s1:T )\nfor t= 1to T do\nˆα′\nt(n) ←(ˆαt−1(n) + ˆαt−1(n−1))wt(n)\nˆαt(n) ←ˆα′\nt(n)\n/∑N\nm=1 ˆα′\nt(m)\nct ←∑N\nn=1 ˆαt(n)hn\nend for\nscribed in [16] is added as auxiliary loss term for training the\nmodel since it keeps a single joint training property without de-\npending on external modules. It applies constraint on attention\nalignments matrix in the form of the diagonal mask based on the\nidea that the input phoneme sequences and the output acoustic\nfeature sequences have nearly diagonal correspondence.\nConsequently, the loss function of the proposed model con-\nsists of two l1 losses for the mel-spectrogram, a l2 loss for\nphoneme duration, and two auxiliary losses, which are CTC\nrecognizer loss and Guided Attention (GA) loss. It can be for-\nmulated as follows:\nL =∥y −y′∥+ ∥y −y′′∥+ ∥d −d′∥2\n+ CTCLoss + GALoss (1)\nwhere y, y′ and y′′ are the mel-spectrograms from the ref-\nerence, autoregressive Transformer and feed-forward Trans-\nformer respectively, d is a phoneme duration sequences from\nthe attention mechanism of autoregressive Transformer and d′\nis a phoneme duration sequences from the duration predictor. It\ncan be future works to investigate an effect of different scaling\nconstants for each loss term.\nAfter joint training, the autoregressive Transformer is no\nlonger needed, so only the feed-forward Transformer and the\nduration predictor are used for synthesis. The feed-forward\nTransformer generates a mel-spectrogram y′′ from the input\nphoneme sequences in parallel using the phoneme duration se-\nquence predicted by the duration predictor.\n4. Experiments\n4.1. Datasets\nWe conduct experiments on two different datasets. The ﬁrst\ndataset is an internal speech recorded by a professional Korean\nfemale speaker in studio quality. The number of utterances used\nfor training is 25k, among which 250, 50 samples were reserved\nfor validation and testing, respectively. The second dataset is\nKorean Single speaker Speech (KSS) Dataset [18], which is\npublicly available for the Korean text-to-speech task. It consists\nof 12,853 utterance audio ﬁles recorded by a professional fe-\nmale voice actress and transcription extracted from their books\nwith a total audio length of approximately 12 hours. We re-\nserved the last 250 utterances; 200 samples for validation and\n50 samples for testing.\nWe convert the input text to phoneme sequences using\nan internal text processing tool. The acoustic feature is 80-\ndimensional mel-spectrogram extracted from audio with sam-\npling rate of 22,050 Hz using the Librosa library [19]. FFT\nsize and hop size are 1024, 256 respectively. Finally, the mel-\nspectrogram is normalized so that every element of the feature\nvector has zero mean and unit variance over the training set.\n4.2. Model conﬁguration\nThe proposed model has 6 FFT blocks in the feed-forward\nTransformer both on the phoneme side and the mel-spectrogram\nside. The number of head for self-attention is set to 8, and the\nkernel size of 1D convolution is set to 5. The inner-layer dimen-\nsion of the position-wise feed-forward network is set to 2048,\nand all other dimensions not stated explicitly have been set to\n512. The proposed model is implemented by PyTorch [20] neu-\nral network library and trained on 4 NVIDIA V100 GPUs with a\nbatch size of 16 per each GPU. We optimize it using the RAdam\nalgorithm [21] with the same learning rate schedule as in [8]\nabout 300k training steps.\nFor comparative experiments, we train another three mod-\nels using ESPnet-TTS [13] which is open-source speech pro-\ncessing toolkit supporting state-of-the-art end-to-end TTS mod-\nels. The models are two autoregressive, attention-based models:\nTacotron2.v3 and Transformer.v1 and a non-autoregressive, du-\nration informed model FastSpeech.v2 following the conﬁgura-\ntion in the recipe of the toolkit. Note that the FastSpeech.v2\nuses pre-trained Transformer.v1 as teacher model.\nWe use Parallel WaveGAN [6] as the vocoder for transform-\ning the generated mel-spectrogram to audio samples in all ex-\nperiments.\n4.3. Evaluation\nTo evaluate the effectiveness of the proposed model, we con-\nduct the Mean Opinion Score (MOS) test 1. The proposed\nmodel, JDI-T, is compared with three different models, includ-\ning Tacotron2, Transformer, and FastSpeech. The audio sam-\nples for the MOS test are generated using scripts of the test\nsamples reserved for each dataset, and thirteen native Korean\nspeakers listen to all of it for measuring the audio quality. Table\n1 shows the results on two different datasets; the Internal and\nthe KSS. Note that the results of GT mel are the evaluation of\nthe audio samples converted from reference mel-spectrogram\nby the vocoder; thus, it indicates upper bounds that our TTS\nmodels can achieve.\nThe results of the MOS test on both datasets show that the\nscore of the duration informed model (FastSpeech) is lower than\nthe attention-based model (Tacotron2, Transformer). Therefore\nit can be said that duration informed model is challenging to\nmatch the audio quality of its teacher model, especially by com-\nparing the FastSpeech with its teacher model; Transformer. In\nthis case, a more elaborate training technique, focus rate tuning,\nor sequence-level knowledge distillation, as described in [9],\nwould be required to improve the audio quality by its teacher\nmodel. On the other hand, the score of our proposed model,\nwhich is also non-autoregressive and duration informed model\nlike FastSpeech, is better than FastSpeech and even achieves\nthe highest score among the TTS models in the Internal dataset.\nThese results show that the joint training of the proposed model\nis beneﬁcial for improving the audio quality as well as for sim-\nplifying the training pipeline.\nIn contrast, the score of the proposed model is lower than\nTransformer in the KSS. It may derived from the fact that the\nquality of audio ﬁles is poorer (i.e., the pronunciation is unclear)\nin KSS dataset than in the Internal dataset which is recorded\nin studio quality with commercial use in mind. So, it seems\nthat the proposed model has the difﬁculty of learning precise\nattention alignments with relatively poor sound quality of audio\n1Audio samples are available at the following URL: https://\nimdanboy.github.io/interspeech2020\nTable 1: Mean opinion scores (5-point scale)\nModel Internal KSS\nGT mel 3.92 3 .87\nTacotron2 3.52 3 .33\nTransformer 3.55 3 .72\nFastSpeech 3.48 3 .23\nJDI-T (ours) 3.77 3 .52\nﬁles. It can be future works to make the model learn better\nalignments during joint training even in the slightly poor sound\nquality of audio ﬁles.\nIn addition to its high-quality speech synthesis, the pro-\nposed model has beneﬁts of the robustness and fast speed at\nsynthesis over the autoregressive, attention-based TTS models\nsince it has the feed-forward structure and does not rely on an\nattention mechanism as in FastSpeech [9]. Moreover, our in-\nternal test shows that Tacotron2 and Transformer have a high\nrate of synthesis error, especially when they are trained with\nthe KSS dataset and synthesize the out-of-domain scripts. Note\nthat the synthesized audio samples from the test scripts have no\nsynthesis error.\n5. Conclusion\nIn this paper, we propose Jointly trained Duration Informed\nTransformer (JDI-T) for TTS. The proposed model, consist-\ning of the feed-forward Transformer, the duration predictor,\nand the autoregressive Transformer, is trained jointly with-\nout explicit alignments. After joint training, only the feed-\nforward Transformer with the duration predictor is used for\nfast and robust conversion from phoneme sequences to mel-\nspectrogram. Experimental results on publicly available Korean\ndatasets prove the effectiveness of the proposed model by show-\ning that it can synthesize high-quality speech. Furthermore,\nit achieves state-of-the-art performance in the internal studio-\nquality dataset compared to other popular TTS models imple-\nmented from ESPnet-TTS.\n6. References\n[1] Y . Wang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y . Xiao, Z. Chen, S. Bengio, Q. Le,\nY . Agiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron:\nTowards End-to-End Speech Synthesis,” in Proceedings of the\nAnnual Conference of the International Speech Communication\nAssociation (INTERSPEECH), 2017, pp. 4006–4010.\n[2] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang, R. Skerrv-Ryan, R. A. Saurous,\nY . Agiomvrgiannakis, and Y . Wu, “Natural TTS Synthesis by\nConditioning Wavenet on MEL Spectrogram Predictions,” in\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2018, pp. 4779–4783.\n[3] D. Grifﬁn and Jae Lim, “Signal estimation from modiﬁed short-\ntime Fourier transform,” in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), vol. 8, 1983,\npp. 804–807.\n[4] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu,\n“WaveNet: A Generative Model for Raw Audio,” ArXiv, vol.\nabs/1609.03499, 2016.\n[5] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A Flow-\nbased Generative Network for Speech Synthesis,” in IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2019, pp. 3617–3621.\n[6] R. Yamamoto, E. Song, and J. Kim, “Parallel WaveGAN: A Fast\nWaveform Generation Model Based on Generative Adversarial\nNetworks with Multi-Resolution Spectrogram,” in IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2020, pp. 6199–6203.\n[7] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, “Neural Speech Synthe-\nsis with Transformer Network,” in Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 6706–6713.\n[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is All you Need,”\nin Advances in Neural Information Processing Systems 30 (NIPS),\n2017, pp. 5998–6008.\n[9] Y . Ren, Y . Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y . Liu,\n“FastSpeech: Fast, Robust and Controllable Text to Speech,” in\nAdvances in Neural Information Processing Systems 32 (NIPS) ,\n2019, pp. 3171–3180.\n[10] C. Yu, H. Lu, N. Hu, M. Yu, C. Weng, K. Xu, P. Liu, D. Tuo,\nS. Kang, G. Lei, D. Su, and D. Yu, “DurIAN: Duration In-\nformed Attention Network For Multimodal Synthesis,” ArXiv,\nvol. abs/1909.01700, 2019.\n[11] Z. Zeng, J. Wang, N. Cheng, T. Xia, and J. Xiao, “AlignTTS:\nEfﬁcient Feed-Forward Text-to-Speech System Without Explicit\nAlignment,” in IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2020, pp. 6714–6718.\n[12] J. Zhang, Z. Ling, and L. Dai, “Forward Attention in Sequence-\nTo-Sequence Acoustic Modeling for Speech Synthesis,” in IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2018, pp. 4789–4793.\n[13] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,\nT. Toda, K. Takeda, Y . Zhang, and X. Tan, “Espnet-TTS: Uniﬁed,\nReproducible, and Integratable Open Source End-to-End Text-to-\nSpeech Toolkit,” in IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2020, pp. 7654–7658.\n[14] P. Liu, X. Wu, S. Kang, G. Li, D. Su, and D. Yu, “Maximizing\nMutual Information for Tacotron,” ArXiv, vol. abs/1909.01145,\n2019.\n[15] E. Battenberg, R. Skerry-Ryan, S. Mariooryad, D. Stanton,\nD. Kao, M. Shannon, and T. Bagby, “Location-Relative Atten-\ntion Mechanisms for Robust Long-Form Speech Synthesis,” in\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2020, pp. 6194–6198.\n[16] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently Train-\nable Text-to-Speech System Based on Deep Convolutional Net-\nworks with Guided Attention,” in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2018, pp.\n4784–4788.\n[17] X. Zhu, Y . Zhang, S. Yang, L. Xue, and L. Xie, “Pre-Alignment\nGuided Attention for Improving Training Efﬁciency and Model\nStability in End-to-End Speech Synthesis,” IEEE Access, vol. 7,\npp. 65 955–65 964, 2019.\n[18] K. Park, “KSS Dataset: Korean Single speaker\nSpeech Dataset,” https://kaggle.com/bryanpark/\nkorean-single-speaker-speech-dataset, 2018.\n[19] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Bat-\ntenberg, and O. Nieto, “librosa: Audio and Music Signal Analysis\nin Python,” in Proceedings of the 14th Python in Science Confer-\nence, 2015, pp. 18 – 24.\n[20] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,\nA. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chil-\namkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch:\nAn Imperative Style, High-Performance Deep Learning Library,”\nin Advances in Neural Information Processing Systems 32 (NIPS),\n2019, pp. 8024–8035.\n[21] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han,\n“On the Variance of the Adaptive Learning Rate and Beyond,” in\nProceedings of the Eighth International Conference on Learning\nRepresentations (ICLR), 2020."
}