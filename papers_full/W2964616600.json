{
  "title": "Teaching AI Agents Ethical Values Using Reinforcement Learning and Policy Orchestration",
  "url": "https://openalex.org/W2964616600",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2805780175",
      "name": "Ritesh Noothigattu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2404333564",
      "name": "Djallel Bouneffouf",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1815073183",
      "name": "Nicholas Mattei",
      "affiliations": [
        "Tulane University"
      ]
    },
    {
      "id": "https://openalex.org/A2135456289",
      "name": "Rachita Chandra",
      "affiliations": [
        "Cambridge Scientific (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2257642421",
      "name": "Piyush Madan",
      "affiliations": [
        "Cambridge Scientific (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A241780900",
      "name": "Kush R. Varshney",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2102065387",
      "name": "Murray Campbell",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2139222655",
      "name": "Moninder Singh",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2079166072",
      "name": "Francesca Rossi",
      "affiliations": [
        "IBM (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2909945638",
    "https://openalex.org/W376942001",
    "https://openalex.org/W2130661992",
    "https://openalex.org/W2893145483",
    "https://openalex.org/W1999874108",
    "https://openalex.org/W2963432546",
    "https://openalex.org/W2240086230",
    "https://openalex.org/W2963177864",
    "https://openalex.org/W2856780764",
    "https://openalex.org/W4300427736",
    "https://openalex.org/W3119455938",
    "https://openalex.org/W2725331431",
    "https://openalex.org/W2942494530",
    "https://openalex.org/W2823329128",
    "https://openalex.org/W2973186106",
    "https://openalex.org/W2794632992",
    "https://openalex.org/W4307347247",
    "https://openalex.org/W2964627913"
  ],
  "abstract": "Autonomous cyber-physical agents play an increasingly large role in our lives. To ensure that they behave in ways aligned with the values of society, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations and reinforcement learning to learn to maximize environmental rewards. A contextual bandit-based orchestrator then picks between the two policies: constraint-based and environment reward-based. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward-maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using Pac-Man and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.",
  "full_text": "Teaching AI Agents Ethical Values\nUsing Reinforcement Learning and Policy Orchestration (Extended Abstract)\u0003\nRitesh Noothigattu,3 Djallel Bouneffouf,1 Nicholas Mattei,4 Rachita Chandra,2 Piyush Madan,2\nKush R. Varshney,1 Murray Campbell,1 Moninder Singh,1 and Francesca Rossi1\n1IBM Research, Yorktown Heights, NY , USA\n2IBM Research, Cambridge, MA, USA\n3Carnegie Mellon University, Pittsburgh, PA, USA\n4Tulane University, New Orleans, LA, USA\nAbstract\nAutonomous cyber-physical agents play an increas-\ningly large role in our lives. To ensure that they\nbehave in ways aligned with the values of soci-\nety, we must develop techniques that allow these\nagents to not only maximize their reward in an en-\nvironment, but also to learn and follow the implicit\nconstraints of society. We detail a novel approach\nthat uses inverse reinforcement learning to learn\na set of unspeciﬁed constraints from demonstra-\ntions and reinforcement learning to learn to max-\nimize environmental rewards. A contextual bandit-\nbased orchestrator then picks between the two\npolicies: constraint-based and environment reward-\nbased. The contextual bandit orchestrator allows\nthe agent to mix policies in novel ways, taking the\nbest actions from either a reward-maximizing or\nconstrained policy. In addition, the orchestrator is\ntransparent on which policy is being employed at\neach time step. We test our algorithms using Pac-\nMan and show that the agent is able to learn to act\noptimally, act within the demonstrated constraints,\nand mix these two functions in complex ways.\n1 Introduction\nConcerns about the ways in which autonomous decision mak-\ning systems behave when deployed in the real world are grow-\ning. Stakeholders worry about systems achieving goals in\nways that are not considered acceptable according to values\nand norms of the impacted community, also called “speciﬁca-\ntion gaming” behaviors[Rossi and Mattei, 2019]. Thus, there\nis a growing need to understand how to constrain the actions\nof an AI system by providing boundaries within which the\nsystem must operate. To tackle this problem, we may take\ninspiration from humans, who often constrain the decisions\nand actions they take according to a number of exogenous\npriorities, be they moral, ethical, religious, or business val-\nues [Sen, 1974; Loreggia et al., 2018a; 2018b ], and we may\nwant the systems we build to be restricted in their actions by\n\u0003This paper is an extended abstract of an article in the the IBM\nJournal of Research & Development [Noothigattu et al., 2019].\nsimilar principles [Arnold et al., 2017]. The overriding con-\ncern is that the agents we construct may not obey these values\nwhile maximizing some objective function [Simonite, 2018;\nRossi and Mattei, 2019].\nThe idea of teaching machines right from wrong has be-\ncome an important research topic in both AI [Yu et al., 2018]\nand related ﬁelds [Wallach and Allen, 2008]. Much of the re-\nsearch at the intersection of artiﬁcial intelligence and ethics\nfalls under the heading of machine ethics, i.e., adding ethics\nand/or constraints to a particular system’s decision making\nprocess [Anderson and Anderson, 2011 ]. One popular tech-\nnique to handle these issues is called value alignment, i.e.,\nrestrict the behavior of an agent so that it can only pursue\ngoals aligned to human values [Russell et al., 2015].\nWhile giving a machine a code of ethics is important, the\nquestion of how to provide the behavioral constraints to the\nagent remains. A popular technique, thebottom-up approach,\nteaches a machine right and wrong by example [Allen et al.,\n2005; Balakrishnan et al., 2019; 2018 ]. Here we adopt this\napproach as we consider the case where only examples of the\ncorrect behavior are available.\nWe propose a framework which enables an agent to learn\ntwo policies: (1) \u0019R, a reward maximizing policy obtained\nthrough direct interaction with the world, and (2) \u0019C, ob-\ntained via inverse reinforcement learning over demonstrations\nby humans or other agents of how to obey a set of behavioral\nconstraints in the domain. Our agent then uses a contextual-\nbandit-based orchestrator [Bouneffouf and Rish, 2019; Boun-\neffouf et al., 2017 ] to learn to blend the policies in a way\nthat maximizes a convex combination of the rewards and con-\nstraints. Within the RL community this can be seen as a par-\nticular type of apprenticeship learning[Abbeel and Ng, 2004]\nwhere the agent is learning how to be safe, rather than only\nmaximizing reward [Leike et al., 2017].\nOne may argue that we should employ\u0019C for all decisions\nas it will be more ‘safe’ than employing\u0019R. Indeed, although\none could use \u0019C exclusively, there are a number of reasons\nto employ the orchestrator. First, while demonstrators may\nbe good at demonstrating the constrained behavior, they may\nnot provide good examples of maximizing reward. Second,\nthe demonstrators may not be as creative as the agent when\nmixing the two policies [Ventura and Gates, 2018]. By allow-\ning the orchestrator to learn when to apply which policy, the\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n6377\nagent may be able to devise better ways to blend the poli-\ncies, leading to behavior which both follows the constraints\nand achieves higher reward than any of the human demon-\nstrations. Third, we may not want to obtain demonstrations in\nall parts of the domain e.g., there may be dangerous parts of\nthe domain in which human demonstrations are too costly to\nobtain. In this case, having the agent learn what to do in the\nnon-demonstrated parts through RL is complementary.\nContributions. We propose and test a novel approach to\nteach machines to act in ways blend multiple objectives. One\nobjective is the desired goal and the other is a set of behav-\nioral constraints, learned from examples. Our technique uses\naspects of both traditional reinforcement learning and inverse\nreinforcement learning to identify policies that both maxi-\nmize rewards and follow particular constraints. Our agent\nthen blends these policies in novel and interpretable ways us-\ning an orchestrator. We demonstrate the effectiveness of these\ntechniques on Pac-Man where the agent is able to learn both\na reward-maximizing and a constrained policy, and select be-\ntween these policies in a transparent way based on context,\nto employ a policy that achieves high reward and obeys the\ndemonstrated constraints.\n1.1 Problem Setting and Notation\nReinforcement learning deﬁnes a class of algorithms solving\nproblems modeled as a Markov decision process (MDP). An\nMDP is denoted by the tuple (S;A;T;R;\r), where: Sis a\nset of possible states; Ais a set of actions; T is a transition\nfunction deﬁned by T(s;a;s 0) = Pr(s0js;a), where s;s02S\nand a 2A; R: S\u0002A\u0002S7! R is a reward function; \r is\na discount factor that speciﬁes how much long term reward\nis kept. The goal is to maximize the discounted long term\nreward. Usually the inﬁnite-horizon objective is considered:\nmax P1\nt=0 \rtR(st;at;st+1).\nSolutions come in the form of policies \u0019 : S 7! A,\nwhich specify what action the agent should take in any given\nstate deterministically or stochastically. One way to solve\nthis problem is through Q-learning with function approxima-\ntion [Bertsekas and Tsitsiklis, 1996]. The Q-value of a state-\naction pair, Q(s;a), is the expected future discounted reward\nfor taking action a2A in state s2S. A common method to\nhandle very large state spaces is to approximate the Qfunc-\ntion as a linear function of some features.\nOur problem is a multi-objective MDP. Instead of the\nscalar reward function R(s;a;s 0), we have a reward vector\n~R(s;a;s 0) consisting of ldimensions representing the differ-\nent objectives. However, not all components of the reward\nvector are observed. There is an objective v 2 [l] that is\nhidden, and the agent is only allowed to observe demon-\nstrations to learn this objective. These demonstrations are\ngiven in the form of trajectories fTr(1);Tr (2);:::;Tr (m)g.\nTo summarize, for some objectives, the agent has rewards ob-\nserved from interaction with the environment, and for some\nobjectives the agent has only demonstrations. The aim is the\nsame as single objective reinforcement learning: to maximizeP1\nt=0 \rtRi(st;at;st+1) for each i2[l].\n2 Proposed Approach\nThe overall approach we propose, aggregation at the policy\nphase, is depicted by Figure 1. It has three main components.\nThe ﬁrst is the IRL component to learn the desired constraints\n(depicted in green). We apply IRL to demonstrations depict-\ning desirable behavior to learn the underlying constraint re-\nwards being optimized by the demonstrations. We then apply\nRL on these learned rewards to learn a strongly constraint-\nsatisfying policy \u0019C. We augment \u0019C with a pure reinforce-\nment learning component applied to the original environment\nrewards (depicted in red) to learn a domain reward maximiz-\ning policy \u0019R.\nNow we have two policies: the constraint-obeying pol-\nicy \u0019C and the reward-maximizing policy \u0019R. To combine\nthem, we use the third component, the orchestrator (depicted\nin blue). This is a contextual bandit algorithm that orches-\ntrates the two policies, picking one of them to play at each\npoint of time. The context is the state of the environment;\nthe bandit decides which arm (policy) to play at each step.\nWe use a modiﬁed CTS algorithm to train the bandit. The\ncontext of the bandit is given by features of the current state\n(for which we want to decide which policy to choose), i.e.,\nc(t) = \u0007(st) 2Rd.\nThe exact algorithm used to train the orchestrator is given\nin Algorithm 1. Apart from the fact that arms are policies (in-\nstead of atomic actions), the main difference from the CTS\nalgorithm is the way rewards are fed into the bandit. For sim-\nplicity, let the constraint policy \u0019C be arm 0 and the reward\npolicy \u0019R be arm 1. First, all the parameters are initialized as\nin the CTS algorithm (Line 1). For each time-step in the train-\ning phase (Line 3), we do the following. Pick an arm kt ac-\ncording to the Thompson Sampling algorithm and the context\n\u0007(st) (Lines 4 and 5). Play the action according to the chosen\npolicy \u0019kt (Line 6). This takes us to the next state st+1. We\nalso observe two rewards (Line 7): (i) the original reward in\nenvironment, rR\nat (t) = R(st;at;st+1) and (ii) the constraint\nrewards according to the rewards learnt by inverse reinforce-\nment learning, i.e., rC\nat (t) = bRC(st;at;st+1). rC\nat (t) can in-\ntuitively be seen as the predicted reward (or penalty) for any\nconstraint satisfaction (or violation) in this step.\nAlgorithm 1Orchestrator Based Algorithm\n1: Initialize: Bk = Id, ^\u0016k = 0d;fk = 0d for k2f0; 1g.\n2: Observe start state s0.\n3: Foreach t= 0;1;2;:::; (T \u00001) do\n4: Sample ~\u0016k(t) from N(^\u0016k;v2B\u00001\nk ).\n5: Pick arm kt = arg max\nk2f0;1g\n\u0007(st)>~\u0016k(t).\n6: Play corresponding action at = \u0019kt (st).\n7: Observe rewards rC\nat (t), rR\nat (t), and next state st+1.\n8: Deﬁne rkt (t) = \u0015\n\u0000\nrC\nat (t) + \rVC(st+1)\n\u0001\n+(1 \u0000\u0015)\n\u0000\nrR\nat (t) + \rVR(st+1)\n\u0001\n9: Update Bkt = Bkt + \u0007(st)\u0007(st)>, fkt = fkt +\n\u0007(st)rkt (t), ^\u0016kt = B\u00001\nkt\nfkt\n10: End\nTo train the contextual bandit to choose arms that perform\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n6378\nIRL f\nor Constraints\nRL f\nor Game Rewards\nOrchestrator\nConstrained\nDemonstration\nRew\nards Capturing\nConstraints bRC\nConstrained\nPolicy\nEnvironment\nRe\nwards R\nRew\nard Maxi-\nmizing Policy\n\u0019C\n\u0019R\nEnvironment\na(t)\ns(t+ 1)\nr(t)\nFigure 1:\nOverview of our system. At each time step the Orchestrator selects between two policies,\u0019C and \u0019R depending on the observations\nfrom the Environment. The two policies are learned before engaging with the environment. \u0019C is obtained using IRL on the demonstrations\nto learn a reward function that captures demonstrated constraints. The second, \u0019R is obtained by the agent through RL on the environment.\nwell on both metrics (environment rewards and constraints),\nwe feed it a reward that is a linear combination of rR\nat (t) and\nrC\nat (t) (Line 8). Another important point to note is that rR\nat (t)\nand rC\nat (t) are immediate rewards achieved on taking action\nat from st, they do not capture long term effects of this action.\nIn particular, it is important to also look at the “value” of the\nnext state st+1 reached, since we are in the sequential deci-\nsion making setting. Precisely for this reason, we also incor-\nporate the value-function of the next state st+1 according to\nboth the reward maximizing component and constraint com-\nponent (which encapsulate the long-term rewards and con-\nstraint satisfaction possible from st+1). This gives exactly\nLine 8, where VC is the value-function according the con-\nstraint policy \u0019C, and VR is the value-function according to\nthe reward maximizing policy \u0019R.\nIn this equation, \u0015 is a hyperparameter chosen by a user\nto decide how much to trade off environment rewards for\nconstraint satisfaction. For example, when \u0015 is set to 0, the\norchestrator would always play the reward policy \u0019R, while\nfor \u0015 = 1, the orchestrator would always play the constraint\npolicy \u0019C. For any value of \u0015 in-between, the orchestrator\nis expected to pick policies at each point of time that would\nperform well on both metrics (weighed according to \u0015). Fi-\nnally, for the desired rewardrkt (t) and the context \u0007(st), the\nparameters of the bandit are updated according to the CTS\nalgorithm (Line 9).\n3 Demonstration on Pac-Man\nWe demonstrate the applicability of the proposed algorithm\nusing the classic game of Pac-Man. The rules for the envi-\nronment (adopted from Berkeley AI Pac-Man 1) are as fol-\nlows. The goal of the agent is to eat all the dots in the maze,\nknown as Pac-Dots, as soon as possible while simultaneously\navoiding collision with ghosts. On eating a Pac-Dot, the agent\nobtains a reward of +10. On successfully eating all the Pac-\nDots, the agent obtains a reward of +500. In the meantime,\nthe ghosts roam the maze trying to kill Pac-Man. On colli-\nsion with a ghost, Pac-Man loses the game and gets a reward\nof \u0000500. The game also has two special dots called Power\nPellets in the corners of the maze, which on consumption,\n1http://ai.berkeley.edu/project overview.html\ngive Pac-Man the temporary ability of “eating” ghosts. Dur-\ning this phase, the ghosts are in a “scared” state for 40 frames\nand move at half their speed. On eating a ghost, the agent\ngets a reward of +200, the ghost returns to the center box\nand returns to its normal “unscared” state. Finally, there is a\nconstant time-penalty of \u00001 for every step taken.\nFor the sake of demonstration of our approach, we deﬁne\nnot eating ghosts as the desirable constraint in the game of\nPac-Man. However, recall that this constraint is not given ex-\nplicitly to the agent, but only through examples. To play op-\ntimally in the original game one should eat ghosts to earn\nbonus points, but doing so is being demonstrated as undesir-\nable. Hence, the agent has to combine the goal of collecting\nthe most points while not eating ghosts.\n3.1 Details of the Pure RL\nFor the reinforcement learning component, we use Q-learning\nwith linear function approximation. Some of the features we\nuse for an hs;aipair (for the  (s;a) function) are: “whether\nfood will be eaten”, “distance of the next closest food”,\n“whether a scared (unscared) ghost collision is possible” and\n“distance of the closest scared (unscared) ghost”. For the lay-\nout of Pac-Man we use, an upper bound on the maximum\nscore achievable in the game is 2170. This is because there\nare 97 Pac-Dots, each ghost can be eaten at most twice (be-\ncause of two capsules in the layout), Pac-Man can win the\ngame only once and it would require more than 100 steps in\nthe environment. On playing a total of 100 games, our rein-\nforcement learning algorithm (the reward maximizing policy\n\u0019R) achieves an average game score of1675:86, and the max-\nimum score achieved is 2144. We mention this here, so that\nthe results in Section 4 can be seen in appropriate light.\n3.2 Details of the IRL\nFor Pac-Man, observe that the original reward function\nR(s;a;s 0) depends only on the following factors: “number\nof Pac-Dots eating in this step (s;a;s 0)”, “whether Pac-Man\nhas won in this step”, “number of ghosts eaten in this step”\nand “whether Pac-Man has lost in this step”. For our IRL\nalgorithm, we use exactly these as the features \u001e(s;a;s 0).\nAs a sanity check, when IRL is run on environment reward\noptimal trajectories (generated from our policy \u0019R), we re-\ncover something very similar to the original reward function\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n6379\nR. In particular, the weights of the reward features learned is\ngiven by 1=1000[+2:44;+138:80;+282:49;\u0000949:17]; which\nwhen scaled is almost equivalent to the true weights\n[+10;+500;+200;\u0000500] in terms of their optimal policies.\nThe number of trajectories used for this is 100.\nIdeally, we would prefer to have the constrained demon-\nstrations given to us by humans, but for the sake of sim-\nplicity we generate them synthetically as follows. We learn\na policy \u0019?\nC by training it on the game with the original re-\nward function Raugmented with a very high negative re-\nward (\u00001000) for eating ghosts. This causes \u0019?\nC to play\nwell in the game while avoiding eating ghosts as much as\npossible.2 Now, to emulate erroneous human behavior, we\nuse \u0019?\nC with an error probability of 3%. That is, at every\ntime step, with 3% probability we pick a completely ran-\ndom action, and otherwise follow \u0019?\nC. This gives us our con-\nstrained demonstrations, on which we perform inverse rein-\nforcement learning to learn the rewards capturing the con-\nstraints. The weights of the reward function learned is given\nby 1=1000[+2:84;+55:07;\u0000970:59;\u0000234:34]; and it is evi-\ndent that it has learned that eating ghosts strongly violates the\nfavorable constraints. The number of demonstrations used for\nthis is 100. We scale these weights to have a similarL1 norm\nas the original reward weights[+10;+500;+200;\u0000500], and\ndenote the corresponding reward function by bRC.\nFinally, running reinforcement learning on these rewards\nbRC, gives us our constraint policy \u0019C. On playing a total of\n100 games, \u0019C achieves an average game score of 1268:52\nand eats just 0:03 ghosts on an average. Note that, when eat-\ning ghosts is prohibited in the domain, an upper bound on the\nmaximum score achievable is 1370.\n4 Evaluation\nWe measure performance on two metrics, (i) the total score\nachieved in the game (the environment rewards) and (ii) the\nnumber of ghosts eaten (the constraint violation). We also ob-\nserve how these metrics vary with \u0015. For each value of \u0015, the\norchestrator is trained for 100 games. The results are shown\nin Figure 2. Each point is averaged over 100 test games.\nThe graph shows a very interesting pattern. When \u0015 is at\nmost than 0:215, the agent eats a lot of ghosts, but when it is\nabove 0:22, it eats almost no ghosts. In other words, there is a\nvalue \u0015o which behaves as a tipping point, across which there\nis drastic change in behavior. Beyond the threshold, the agent\nlearns that eating ghosts is not worth the score it is getting and\nso it avoids eating as much as possible. On the other hand,\nwhen \u0015 is smaller than \u0015o, it learns the reverse and eats as\nmany ghosts as possible.\nPolicy-switching. One important property of our approach\nis interpretability, we know exactly which policy is being\nplayed at each time. For moderate values of \u0015 > \u0015o, the\norchestrator learns a very interesting policy-switching tech-\nnique: whenever at least one of the ghosts in the domain is\n2We do this only for generating demonstrations. In real domains,\nwe would not have access to the exact constraints that we want to\nbe satisﬁed, and hence a policy like \u0019?\nC cannot be learned; learning\nfrom human demonstrations would then be essential.\nFigure 2: Both performance metrics as \u0015 is varied. The red curve\ndepicts the average game score achieved, and the blue curve depicts\nthe average number of ghosts eaten.\nscared, it plays \u0019C, but if no ghosts are scared, it plays \u0019R.\nIn other words, it starts the game playing \u0019R until a capsule\nis eaten. As soon as the ﬁrst capsule is eaten, it switches to\n\u0019C until the scared timer runs off. Then it switches back to\n\u0019R until another capsule is eaten, and so on. It has learned a\nvery intuitive behavior: when there is no scared ghost, there\nis no possibility of violating constraints. Hence, the agent is\nas greedy as possible (i.e., play\u0019R). However, when there are\nscared ghosts, it is better to be safe (i.e., play \u0019C).\n5 Discussion\nIn this paper, we considered the problem of autonomous\nagents learning policies constrained by implicitly-speciﬁed\nvalues while still optimizing their policies with respect to en-\nvironmental rewards. We have taken an approach that com-\nbines IRL to determine constraint-satisfying policies from\ndemonstrations, RL to determine reward-maximizing poli-\ncies, and a contextual bandit to orchestrate between these\npolicies in a transparent way. This proposed architecture and\napproach for the problem is novel. It also requires a novel\ntechnical contribution in the contextual bandit algorithm be-\ncause the arms are policies rather than atomic actions, thereby\nrequiring rewards to account for sequential decision making.\nWe have demonstrated the algorithm on Pac-Man and found\nit to perform interesting switching behavior among policies.\nThe contribution herein is only a starting point. We can pur-\nsue deep IRL to learn constraints without hand-crafted fea-\ntures and research IRL algorithms to learn from just one or\ntwo demonstrations (perhaps in concert with knowledge and\nreasoning). In real-world settings, demonstrations will likely\nbe given by different users with different versions of abiding\nbehavior; we would like to exploit the partition of the set of\ntraces by user to improve the policy or policies learned via\nIRL. Additionally, the current orchestrator selects a single\npolicy at each time, but more sophisticated policy aggrega-\ntion techniques for combining or mixing policies is possible.\nLastly, it would be interesting to investigate whether the pol-\nicy aggregation rule (\u0015) can be learned from demonstrations.\nAcknowledgments\nThis work was conducted under the auspices of the IBM Sci-\nence for Social Good initiative.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n6380\nReferences\n[Abbeel and Ng, 2004] P. Abbeel and A. Y . Ng. Apprentice-\nship learning via inverse reinforcement learning. In Pro-\nceedings of the 21st International Conference on Machine\nLearning (ICML), 2004.\n[Allen et al., 2005] C. Allen, I. Smit, and W. Wallach. Ar-\ntiﬁcial morality: Top-down, bottom-up, and hybrid ap-\nproaches. Ethics and Information Technology, 7(3):149–\n155, 2005.\n[Anderson and Anderson, 2011] M. Anderson and S. L. An-\nderson. Machine Ethics. Cambridge University Press,\n2011.\n[Arnold et al., 2017] T. Arnold, D. Kasenberg, and\nM. Scheutzs. Value alignment or misalignment -\nwhat will keep systems accountable? In AI, Ethics, and\nSociety, Papers from the 2017 AAAI Workshops, 2017.\n[Balakrishnan et al., 2018] A. Balakrishnan, D. Bouneffouf,\nN. Mattei, and F. Rossi. Using contextual bandits with be-\nhavioral constraints for constrained online movie recom-\nmendation. In Proc. of the 27th Intl. Joint Conference on\nAI (IJCAI), 2018.\n[Balakrishnan et al., 2019] A. Balakrishnan, D. Bouneffouf,\nN. Mattei, and F. Rossi. Incorporating behavioral con-\nstraints in online AI systems. In Proc. of the 33rd AAAI\nConference on Artiﬁcial Intelligence (AAAI), 2019.\n[Bertsekas and Tsitsiklis, 1996] D. Bertsekas and J. Tsitsik-\nlis. Neuro-dynamic programming. Athena Scientiﬁc, 1996.\n[Bouneffouf and Rish, 2019] D. Bouneffouf and I. Rish. A\nsurvey on practical applications of multi-armed and con-\ntextual bandits. CoRR, abs/1904.10040, 2019.\n[Bouneffouf et al., 2017] D. Bouneffouf, I. Rish, G. A. Cec-\nchi, and R. F ´eraud. Context attentive bandits: Contextual\nbandit with restricted context. In Proc. of the 26th Intl.\nJoint Conference on AI (IJCAI), pages 1468–1475, 2017.\n[Leike et al., 2017] J. Leike, M. Martic, V . Krakovna, P.A.\nOrtega, T. Everitt, A. Lefrancq, L. Orseau, and S. Legg.\nAI safety gridworlds. arXiv preprint arXiv:1711.09883,\n2017.\n[Loreggia et al., 2018a] A. Loreggia, N. Mattei, F. Rossi, and\nK. B. Venable. Preferences and ethical principles in deci-\nsion making. In Proc. of the 1st AAAI/ACM Conference on\nAI, Ethics, and Society (AIES), 2018.\n[Loreggia et al., 2018b] A. Loreggia, N. Mattei, F. Rossi,\nand K. B. Venable. Value alignment via tractable prefer-\nence distance. In R. V . Yampolskiy, editor,Artiﬁcial Intel-\nligence Safety and Security, chapter 18. CRC Press, 2018.\n[Noothigattu et al., 2019] R. Noothigattu, D. Bouneffouf,\nN. Mattei, R. Chandra, P. Madan, K. Varshney, M. Camp-\nbell, M. Singh, and F. Rossi. Teaching AI agents ethical\nvalues using reinforcement learning and policy orchestra-\ntion. IBM Journal of Research & Development, 2019. To\nAppear.\n[Rossi and Mattei, 2019] F. Rossi and N. Mattei. Building\nethically bounded AI. In Proc. of the 33rd AAAI Confer-\nence on Artiﬁcial Intelligence (AAAI), 2019.\n[Russell et al., 2015] S. Russell, D. Dewey, and M. Tegmark.\nResearch priorities for robust and beneﬁcial artiﬁcial intel-\nligence. AI Magazine, 36(4):105–114, 2015.\n[Sen, 1974] A. Sen. Choice, ordering and morality. In\nS. K ¨orner, editor, Practical Reason. Blackwell, Oxford,\n1974.\n[Simonite, 2018] T. Simonite. When bots teach themselves\nto cheat. Wired Magazine, August 2018.\n[Ventura and Gates, 2018] D. Ventura and D. Gates. Ethics\nas aesthetic: A computational creativity approach to ethi-\ncal behavior. In Proc. Int. Conference on Computational\nCreativity (ICCC), pages 185–191, 2018.\n[Wallach and Allen, 2008] W. Wallach and C. Allen. Moral\nMachines: Teaching Robots Right From Wrong. Oxford\nUniversity Press, 2008.\n[Yu et al., 2018] H. Yu, Z. Shen, C. Miao, C. Leung, V . R.\nLesser, and Q. Yang. Building ethics into artiﬁcial intel-\nligence. In Proc. of the 27th Intl. Joint Conference on AI\n(IJCAI), pages 5527–5533, 2018.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n6381",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.8362671136856079
    },
    {
      "name": "Orchestration",
      "score": 0.832478404045105
    },
    {
      "name": "Computer science",
      "score": 0.7126325368881226
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6884049773216248
    },
    {
      "name": "Constraint (computer-aided design)",
      "score": 0.5465155243873596
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5131729245185852
    },
    {
      "name": "Autonomous agent",
      "score": 0.45768868923187256
    },
    {
      "name": "Multi-armed bandit",
      "score": 0.42853233218193054
    },
    {
      "name": "Human–computer interaction",
      "score": 0.33385375142097473
    },
    {
      "name": "Machine learning",
      "score": 0.3021513819694519
    },
    {
      "name": "Engineering",
      "score": 0.12566369771957397
    },
    {
      "name": "Musical",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Regret",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1341412227",
      "name": "IBM (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I114832834",
      "name": "Tulane University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210087032",
      "name": "Cambridge Scientific (United States)",
      "country": "US"
    }
  ],
  "cited_by": 21
}