{
  "title": "Efficient Retrieval Augmented Generation from Unstructured Knowledge for Task-Oriented Dialog",
  "url": "https://openalex.org/W3126880001",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227753425",
      "name": "Thulke, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227753426",
      "name": "Daheim, Nico",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227753427",
      "name": "Dugast, Christian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747500719",
      "name": "Ney, Hermann",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3098425262",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2891732163",
    "https://openalex.org/W2891103209",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2995183464",
    "https://openalex.org/W2051840895",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3098826124",
    "https://openalex.org/W3030754432",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2106053110",
    "https://openalex.org/W3035639219",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "This paper summarizes our work on the first track of the ninth Dialog System Technology Challenge (DSTC 9), \"Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access\". The goal of the task is to generate responses to user turns in a task-oriented dialog that require knowledge from unstructured documents. The task is divided into three subtasks: detection, selection and generation. In order to be compute efficient, we formulate the selection problem in terms of hierarchical classification steps. We achieve our best results with this model. Alternatively, we employ siamese sequence embedding models, referred to as Dense Knowledge Retrieval, to retrieve relevant documents. This method further reduces the computation time by a factor of more than 100x at the cost of degradation in R@1 of 5-6% compared to the first model. Then for either approach, we use Retrieval Augmented Generation to generate responses based on multiple selected snippets and we show how the method can be used to fine-tune trained embeddings.",
  "full_text": "arXiv:2102.04643v1  [cs.CL]  9 Feb 2021\nEfﬁcient Retrieval Augmented Generation\nfrom Unstructured Knowledge for T ask-Oriented Dialog\nDavid Thulke, 1,2 Nico Daheim, 1 Christian Dugast, 1,2 Hermann Ney 1,2\n1 Human Language T echnology and Pattern Recognition Group, R WTH Aachen University, Germany\n2 AppT ek GmbH, Aachen, Germany\n{thulke, daheim, dugast, ney }@i6.informatik.rwth-aachen.de\nAbstract\nThis paper summarizes our work on the ﬁrst track of the ninth\nDialog System T echnology Challenge (DSTC 9), “Beyond\nDomain APIs: T ask-oriented Conversational Modeling with\nUnstructured Knowledge Access”. The goal of the task is to\ngenerate responses to user turns in a task-oriented dialog t hat\nrequire knowledge from unstructured documents. The task is\ndivided into three subtasks: detection, selection and gene ra-\ntion. In order to be compute efﬁcient, we formulate the se-\nlection problem in terms of hierarchical classiﬁcation ste ps.\nW e achieve our best results with this model. Alternatively , we\nemploy siamese sequence embedding models, referred to as\nDense Knowledge Retrieval, to retrieve relevant documents .\nThis method further reduces the computation time by a factor\nof more than 100x at the cost of degradation in R@1 of 5-6%\ncompared to the ﬁrst model. Then for either approach, we use\nRetrieval Augmented Generation to generate responses base d\non multiple selected snippets and we show how the method\ncan be used to ﬁne-tune trained embeddings.\n1 Introduction\nT ask-oriented dialog systems allow users to achieve cer-\ntain goals, such as restaurant or hotel reservations, by in-\nteracting with a system using natural language. T ypically,\nthese systems are restricted to information provided by an\napplication-speciﬁc interface (API) which accesses a stru c-\ntured database. However, the breadth of structured informa -\ntion available is often limited to a set of ﬁxed ﬁelds and may\nnot satisfy all information needs users may have. The neces-\nsary information is in many cases already available but only\nfound in unstructured documents, such as descriptions on\nwebsites, F A Q documents, or customer reviews. The aim of\nTrack 1 of the ninth Dialog System T echnology Challenge\n(DSTC 9) (Gunasekara et al. 2020) is to make use of such\ninformation to provide users with an answer relevant to thei r\nquestion. T o do so, the task at hand is split up into three sub-\ntasks, namely Knowledge-seeking T urn Detection to iden-\ntify those questions that can not be answered by an existing\nAPI, Knowledge Selection to retrieve relevant documents,\nand Response Generation to generate a suitable system re-\nsponse.\nCopyright © 2021, Association for the Advancement of Artiﬁc ial\nIntelligence (www .aaai.org). All rights reserved.\nIn a real-world scenario, the amount of time that can be\nspent on generating a response is limited. Since the num-\nber of relevant documents can potentially grow large, we\nnote that efﬁcient means of retrieval are crucial. In the bas e-\nline approach, this is identiﬁed as the limiting factor. Thu s,\nour work focuses on the task of Knowledge Selection and,\nspeciﬁcally, efﬁcient document retrieval methods. For thi s,\nwe propose two different methods. Our ﬁrst method splits\nthe task of classifying a relevant document into subsequent\nstages. Thereby we are not only able to signiﬁcantly reduce\nthe computational cost, but also to outperform the single-\nstage classiﬁcation approach by a signiﬁcant margin on both\ntest sets. Secondly, we employ siamese embedding networks\nto learn dense document and dialog embeddings, which are\nthen used to retrieve relevant documents based on vector\nsimilarity. Additionally, we ﬁne-tune these models jointl y\nwith a retrieval augmented generation model, achieving re-\nsults comparable to the baseline. This model can then also be\nused during generation, to condition the model on multiple\ndocuments.\n2 Related W ork\nRecently, the use of pre-trained transformer language mod-\nels like GPT -2 has shown to be successful for task-oriented\ndialog systems. It is used either as a model in one of\nthe subtasks, like response generation, or as an end-to-end\nmodel for the whole task (Budzianowski and V uli´ c 2019;\nHam et al. 2020). Other transformer models which have\nbeen pre-trained on other tasks such as BER T or RoBER T a\nhave been shown to outperform GPT -2 on a range of natural\nlanguage understanding tasks (Devlin et al. 2019; Liu et al.\n2019b). Recently, Lewis et al. (2020a) proposed BAR T , an\nencoder-decoder model pre-trained as a denoising autoen-\ncoder. On natural language understanding tasks like GLUE\nand SQuAD, it achieves similar performance to RoBER T a\nand can be effectively ﬁne-tuned to sequence generation\ntasks, for example summarization or dialog response gen-\neration.\nMost previous works considered the problem of inte-\ngrating unstructured knowledge into conversations on open\ndomain dialogs. Existing benchmarks for example include\nconversations on topics grounded on Wikipedia articles\n(Dinan et al. 2018) or chats on movies (Moghe et al. 2018).\nGhazvininejad et al. (2018) propose an encoder-decoder\nmodel that uses separate encoders, one for the dialogue\ncontext and one for knowledge documents. Relevant docu-\nments are retrieved by named entity and keyword matching.\nKim, Ahn, and Kim (2020) jointly model knowledge selec-\ntion and response generation using a sequential knowledge\ntransformer. Zhao et al. (2020) suggest an unsupervised ap-\nproach for retrieval which does not require annotations of\nrelevant knowledge documents. Unlike in the current track\nof DSTC 9, in these benchmarks, typically, multiple knowl-\nedge documents are relevant to create a response. Addition-\nally, conversations in open domain dialogs often lack a clea r\ngoal and the state of the dialog is less constrained by the\ndomain.\n3 T ask Description\nIn a task-oriented dialog system, the task is to generate an\nappropriate system response uT +1 given a user utterance uT\nand a preceding dialog context uT − 1\n1 . In the context of the\nchallenge (Kim et al. 2020), it is assumed that every user ut-\nterance can either be handled by an already existing API-\nbased system or that it can be answered based on a set of\nknowledge documents K = {k1, . . . , k N }. These knowl-\nedge documents have different types of metadata, which\nshall be brieﬂy introduced. First of all, each document is\nuniquely assigned to one domain, e.g. restaurant or taxi, an d\none entity, e.g. a speciﬁc restaurant or hotel. The set of all\ndomains is denoted by D in the following. Each domain\ncontains a set of entities Ed ⊂ E, the set of all entities,\nfor d ∈ D. Finally, each entity e ∈ Ed is described by a\nset of knowledge documents Ke ⊂ K. W e further deﬁne\na special entity ∗d for each domain d so that K∗d contains\nall documents which are relevant to the whole domain. The\ntask of generating an appropriate system response based on\nknowledge is split into three subsequent subtasks, which we\nformally introduce in the following paragraphs.\nT urn Detection In turn detection, the task of the system is\nto decide whether a turn should be handled by an existing\nAPI or by accessing an unstructured knowledge document.\nThus, the problem is formulated as a binary classiﬁcation\ntask, such that\nf1(uT\n1 |K) =\n{ 1 if ∃k ∈ K s.t. k answers uT\n0 otherwise\nFurthermore, as noted in Kim et al. (2020), each turn is\nassumed to be manageable by either API or unstructured\nknowledge, i.e. exactly one applies in each case.\nKnowledge Selection After the turn detection, utterances\nthat are classiﬁed as requiring unstructured knowledge ac-\ncess are handled in the knowledge selection task. Here, the\nmodel has to retrieve one or more knowledge snippets rel-\nevant to the query posed by the last user turn and dialog\ncontext. Hence, the task can be deﬁned over the full dialog\ncontext and the full set of knowledge snippets as\nf2(uT\n1 |K) ={k |k ∈ K ∧ k relevant to uT\n1 }\nIn the following, we refer to the set of selected knowledge\nas ˜K ⊂ K.\nResponse Generation As a ﬁnal step, the user is provided\nwith a generated system’s response based on the dialog con-\ntext uT\n1 and the set of selected knowledge snippets relevant\nto it, denoted by ˜K. Thus, the task can be deﬁned as\nf3(uT\n1 , ˜K) =uT +1\nAside from containing the relevant knowledge, the gener-\nated response should naturally ﬁt in the preceding dialog\ncontext uT\n1 .\n4 Methods\nIn this section, we ﬁrst brieﬂy introduce the baseline meth-\nods suggested by Kim et al. (2020) and our modiﬁcations to\nthem and then discuss additional extensions and alternativ es\nproposed by us. In all methods, we use pre-trained trans-\nformer models like RoBER T a or BAR T . For classiﬁcation\ntasks, we add a single feed-forward layer on top of the ﬁnal\nlayer output of the token representing the whole sequence\n(i.e. [CLS] for BER T or <s> for RoBER T a). Due to the\nlength limitations of these models and due to memory limi-\ntation of our hardware the input may be truncated to a max-\nimum length, for example, by truncating the ﬁrst utterances\nof the complete dialogue to reach the maximum input length.\n4.1 Baselines\nT urn Detection T o solve this subtask, Kim et al. propose\nto train a binary classiﬁer on the full dialog context uT\n1 .\nW e add a sigmoid to the single class output of the feed-\nforward layer and ﬁne-tune the whole model using binary\ncross-entropy.\nKnowledge Selection Similarly, Kim et al. propose to\nmodel knowledge selection as a binary classiﬁcation prob-\nlem. For a dialog context uT\n1 and a knowledge snippet k\nthe model predicts whether the knowledge snippet is rele-\nvant to the dialog or not. It uses the same model architecture\nand training criterion as the previous task. Instead of incl ud-\ning all knowledge snippets that are not relevant to a dialog\ncontext into the loss, a random subset is sampled to make\nthe training efﬁcient. During evaluation, this approximat ion\nis not possible and we have to consider all |K|knowledge\nsnippets. The method returns the knowledge snippet with the\nhighest relevance score as the output of this task.\nUnlike the baseline implementation, we augment the rep-\nresentation of the knowledge snippet with the name of the\nassociated domain which is automatically taken from the\nsnippet meta-data. This especially reduces the confusion b e-\ntween domains that contain similar documents. See Sec-\ntion 6 for more details.\nResponse Generation Finally, for response generation,\nKim et al. suggest to use an autoregressive sequence genera-\ntion model conditioned on the dialog context and the knowl-\nedge snippet selected in the previous subtask. Kim et al. pro -\npose to ﬁne-tune a pre-trained GPT -2 model (Radford et al.\n2019) for this task. W e ﬁne-tune a pre-trained BAR T model\n(Lewis et al. 2020a) which is, unlike GPT -2, an encoder-\ndecoder model. In contrast to Kim et al. who use Nucleus\nSampling (Holtzman et al. 2020) for decoding, we use beam\nsearch with a beam size of 4. Additionally, we set a rep-\netition penalty of 1.2 (Keskar et al. 2019) and a maximum\ndecode length of 60 tokens.\n4.2 Hierarchical Selection\nOne of the main issues with the baseline approach for knowl-\nedge selection is its computational complexity. In the fol-\nlowing, we refer to |D|, |E| and |K| as the total number\nof domains, entities and knowledge snippets. T o calculate a\nrelevance score for each knowledge snippet, |K|inference\ncomputations using the whole model are required. In a real-\ntime dialog application, this may become prohibitive.\nAn approach to solve this issue is to make use of the ad-\nditional metadata available for each knowledge snippet. In -\nstead of directly selecting the relevant knowledge snippet\nfrom all available documents, we can divide the problem\nby ﬁrst identifying the relevant domain and entity and then\nselecting the relevant snippet among the documents of this\nentity. Therefore we use the same relevance classiﬁcation\nmodel as in the baseline. W e try two variants of this ap-\nproach. In the ﬁrst one, we train three separate models to\nretrieve the relevant domain, entity, and document. In the\nsecond one, we train one model to jointly retrieve the rele-\nvant domain and entity and one model to retrieve the relevant\ndocument.\nIn total the approach reduces the complexity of the task\nfrom\nO (|K|) =O\n(\n|D| ·|E|\n|D|· |K|\n|E|\n)\nto\nO\n(\n|D|+ |E|\n|D|+ |K|\n|E|\n)\nand O\n(\n|E|+ |K|\n|E|\n)\nfor the ﬁrst and second variants.\n4.3 Dense Knowledge Retrieval\nWhile this approach decreases computational complexity, i t\nmay still be infeasible if the number of knowledge snippets\ngets too large. Kim et al. showed that classical information\nretrieval approaches like TF-IDF and BM25 are signiﬁcantly\noutperformed by their relevance classiﬁcation model. Sim-\nple bag-of-words models do not seem to be able to capture\nthe relevant information of a dialog context necessary to re -\ntrieve the correct document.\nRecent approaches like Sentence-BER T\n(Reimers and Gurevych 2019) or Dense Passage Re-\ntrieval (Karpukhin et al. 2020) showed that sentence\nrepresentations based on pre-trained transformer models\ncan be used effectively in information retrieval tasks when\nthey are ﬁne tuned on a task. Similar to them we propose to\nuse a siamese network structure made of a dialog context\nencoder and a knowledge snippets encoder so that the\ndistance between their representation builds a suitable\nranking function. Thus, this essentially becomes a metric\nlearning problem. For the encoders, we use pre-trained\nRoBER T a (Liu et al. 2019b) models. W e directly use the\nlast hidden state of the <s> token. W e experiment with\ntwo different distance functions: the euclidian distance a nd\nthe dot product.\nFor the former, we use the triplet loss\n(W einberger and Saul 2009), to train both encoders.\nGiven an anchor, in our case the encoded dialog context,\nand a positive and negative sample, in our case the relevant\nand a random irrelevant knowledge snippet, it trains the\nencoders so that the distance between the anchor and\npositive sample is lower than the distance to a negative\nsnippet by a margin ǫ.\nFor the second method, we use the dot product between\nthe embeddings created by encoder E1 and E2 as a similarity\nmeasure. W e train the model, given an anchor a, to correctly\nclassify a positive sample given the positive sample p and\na set of negative samples N. Mathematically the loss is the\nnegative log-likelihood of the correct positive sample:\nL = − log exp (E1(a) ·E2(p))\n∑\ns∈ N∪{ p} exp (E1(a) ·E2(s))\nThe anchor can either be a dialog context and the other sam-\nples are relevant and irrelevant knowledge snippets or the\nother way around. T o select negative examples we experi-\nment with two different batching strategies. The ﬁrst is to\nrandomly sample among the full set of negative samples.\nAlternatively, instead of randomly sampling negative ex-\namples, one approach in metric learning tasks is to use hard\nnegatives, i.e. the samples from the negative class whose\nembeddings are closest to the anchor. T o implement this ef-\nﬁciently, either only samples in the current batch are con-\nsidered or the hardest samples are selected from a random\nsubset. For simplicity, we use the second approach in this\nwork.\nSince the embeddings of the knowledge snippets can be\npre-computed only the embedding of the current dialog con-\ntext has to be computed during runtime. If the total num-\nber is rather small, i.e. in the thousands as in our case, the\nk nearest neighbor search to ﬁnd the closest embedding is\nnegligible compared to the inference time of the transforme r\nmodel. Thus, effectively this method is in O(1). Even for a\nvery large number of knowledge snippets there are efﬁcient\nmeans of search (Johnson, Douze, and Jegou 2019).\n4.4 Retrieval Augmented Generation\nThe baseline approach for response generation only con-\nsiders the single best selected knowledge snippet. In some\ncases, multiple snippets might contain relevant informati on\nto generate a response. Further, by making a hard decision\nfor a single knowledge snippet in the selection step, we in-\ntroduce errors that are propagated to the response genera-\ntion. This motivates us to reformulate our selection and gen -\neration task into a single task, i.e. to generate a response\nbased on all of our knowledge snippets. The approach is\nsimilar to what Lewis et al. (2020b) propose and to other re-\ntrieval augmented models like REALM (Guu et al. 2020).\nMathematically, we can formulate this as a marginalization\nover the selected knowledge snippet k which we introduce\nas a latent variable:\np(uT +1|uT\n1 ; K) =\n∑\nk∈ K\np(uT +1, k|uT\n1 ; K)\nwhich can then be further split into a selection probability ,\ni.e. the probability of a knowledge snippet given a dialog\ncontext, and a generation probability which corresponds to\nthe baseline model for generation:\np(uT +1, k|uT\n1 ; K) =p\n(\nk |uT\n1 ; K\n)\n\n \nselection\n·\ngeneration\n  \np\n(\nuT +1 |uT\n1 , k; K\n)\nThe same decomposition can also be applied on the token\nlevel which allows us to use this as a drop-in replacement\nfor our current generation probability. T o be able to calcul ate\nthis efﬁciently during training and testing, we approximat e\nthe sum over all knowledge snippets K by a sum over the\ntop n snippets. T o ensure that the model is still normalized,\nwe renormalize the selection probabilities over this subse t.\nIn our experiments, we typically use n = 5and ensure that\nthe correct knowledge snippet is always included in the top\nn snippets during training. For the generation probability,\nwe use the same model as in the baseline. For the selection\nprobability, we try all three models discussed so far. In the -\nory, this model allows us to train the selection and generati on\nmodels jointly. However, calculating the selection probab ili-\nties using the relevance classiﬁcation models during train ing\non the ﬂy is not feasible, even when using the Hierarchical\nSelection models. Therefore we calculate these probabilit ies\nin a previous step and keep them ﬁxed during training.\nFortunately, using the Dense Knowledge Retrieval model,\ntraining both models jointly becomes feasible. Therefore, we\nkeep the knowledge snippet encoder ﬁxed and only ﬁne-tune\nthe dialog context encoder. The top n knowledge snippets\ncan then be effectively retrieved during training.\n4.5 Multi-T ask Learning\nMotivated by the recent success of multi-task learning for\nvarious NLP tasks (Liu et al. 2019a; Raffel et al. 2020), we\nexplored two approaches to apply the method in this chal-\nlenge. In the ﬁrst approach, we apply it to the Hierarchical\nSelection method. Instead of training three separate model s\n(or two if we classify the domain and entity jointly), we trai n\na single model on all three relevance classiﬁcation tasks. F or\nthe second approach, we train a single model on all three\nsubtasks of the challenge. In both scenarios, we employ hard\nparameter sharing where the hidden layers are shared among\nall tasks (Ruder 2017). On top of the shared layers, we add\nseparate feed-forward layers for each task on top of the ﬁnal\nlayer output of the [CLS] token, to obtain separate outputs\nfor each task. For each relevance classiﬁcation task we only\ninclude the relevant parts of the knowledge snippet into the\ninput, i.e. to predict the relevant entity we only include th e\ndomain and the name of the entity but not the content of the\ndocument and calculate a loss based on the relevant output.\nW e used the same strategies to sample positive and negative\nsamples as for the baseline models.\n5 Data\nThe training data provided as part of the challenge\nfor this task is based on the MultiWOZ 2.1 dataset\n(Budzianowski et al. 2018; Eric et al. 2020). MultiWOZ is\na task-oriented dialog dataset consisting of 10,438 dialog s\nspanning 7 different domains. Each of these domains is de-\nﬁned by an ontology and has a database of corresponding\nentities. For this challenge, Kim et al. (2020) extended the\ncorpus by adding user turns requesting information not cov-\nered by the database, corresponding system responses, and\nknowledge snippets for each entity. The latter were collect ed\nfrom the F A Q websites of the corresponding entities occur-\nring in the corpus. Each snippet consists of a domain, an\nentity, a question, and an answer. The additional turns were\ncollected with the help of crowd workers. In total, 21.857\nnew pairs of turns and 2.900 knowledge snippets were added\nto the corpus. The training and validation datasets are re-\nstricted to four domains, namely hotel, restaurant, taxi, a nd\ntrain. The latter two domains do not contain any entities and\ncorresponding knowledge snippets are relevant for the whol e\ndomain.\nThe organizers of the challenge announced that the ﬁ-\nnal test data will include additional new domains, entities ,\nand knowledge snippets. T o simulate these conditions when\nevaluating our approaches, we created an additional traini ng\ndataset in which we removed all dialogs corresponding to\nthe train and restaurant domain.\nThe ﬁnal test data introduced one additional domain: at-\ntraction. It contains 4.181 additional dialogs of which 1.9 81\nhave knowledge-seeking turns and 12.039 knowledge snip-\npets. Around half of these dialogs are from the MultiWOZ\ndataset augmented as described above. The other half are\nhuman-to-human conversations about touristic informatio n\nfor San Francisco. Of these, 90% are written conversations\nand 10% transcriptions of spoken conversations.\n6 Experiments\nW e implemented our proposed methods on top of the base-\nline implementation provided as part of the challenge 1. For\nthe pre-trained models, we used the implementation and\ncheckpoints provided by Huggingface’s Transformer librar y\n(W olf et al. 2019). W e use RoBER T a (Liu et al. 2019b) as a\npre-trained language model for the encoders of the embed-\nding models. In all other cases, if not otherwise stated, we\nuse BAR T large (Lewis et al. 2020a). For the selection, dia-\nlog contexts are truncated to maximum length of 384 tokens.\nT o organize our experiments we used the workﬂow man-\nager Sisyphus (Peter, Beck, and Ney 2018). All models are\ntrained on Nvidia GTX 1080 Ti or R TX 2080 Ti GPUs.\n6.1 Results\nT ables 1 and 2 show our main results on the validation and\ntest data for all three tasks. The results of the selection an d\ngeneration are based on the results of the previous subtasks .\nIn all other tables, these results are based on the ground tru th\nof the previous task to increase comparability. The ﬁrst two\nlines in the table for each evaluation dataset compare the re -\nsults of the baseline method of Kim et al. (2020) and using\nour general modiﬁcations discussed in Section 4.1. Rows\nwith entry IDs correspond to the systems we submitted to\n1 Code is available at https://github.com/dthulke/dstc9-t rack1\nT able 1: Main results of our approaches on the validation data. No. corresponds to the entry ID in DSTC 9.\nno. detection selection generation\nmodel F1 model R@1 model BLEU-1 METEOR ROUGE-L\n- baseline 98.5 baseline 67.3 baseline 36.0 36.0 35.0\n- baseline (our) 99.1 baseline (our) 91.1 baseline (our) 42.8 43.5 41.0\n2 Multi-task 99.7 Multi-T ask 94.1 43.2 44.0 41.4\n1 99.7 94.1 RAG 43.0 44.1 41.5\n3 99.7 Hierarchical 96.1 43.2 44.5 41.6\n0 99.7 DKR NLL 93.2 42.7 44.1 41.4\n- 99.7 DKR Triplet 90.7 39.6 41.3 38.6\nT able 2: Main results of our approaches on the test data. No. corresponds to the entry ID in DSTC 9.\nno. detection selection generation\nmodel F1 model R@1 model BLEU-1 METEOR ROUGE-L\n- baseline 94.5 baseline 62.0 baseline 30.3 29.8 30.3\n- baseline (our) 96.1 baseline (our) 87.7 baseline (our) 38.3 38.5 37.1\n2 Multi-task 96.4 Multi-T ask 78.7 37.5 38.1 36.8\n1 96.4 78.7 RAG 37.5 38.0 36.7\n3 96.4 Hierarchical 89.9 37.9 38.6 37.1\n0 96.4 DKR NLL 83.8 38.1 38.4 37.3\n- 96.4 DKR Triplet 84.4 35.2 37.0 35.6\nT able 3: Runtimes in seconds per turn for different methods\non one GTX 1080 Ti.\ntask model runtime\nvalidation test\ndetection baseline 0.04 0.04\nselection baseline 111.66 276.53\nHierarchical 4.60 13.79\nDKR 0.04 0.04\ngeneration baseline 0.85 0.82\nRAG + DKR 1.20 1.48\nDSTC 9 as T eam 18. W e achieved our best result with sys-\ntem 3 with which we ranked on the 6th place in the automatic\nand on the 7th place in the human evaluation.\nThe next section analyzes the effect of these different\nmodiﬁcations in more detail. In the remainder of this sub-\nsection, we discuss the results of our proposed methods on\neach subtask.\nT urn Detection In the detection task, we achieve our best\nresults with a Multi-T askmodel trained on the three losses\nof each task. Compared to the baseline, this approach mainly\nimproved the recall of the model on the validation and test\ndata with a slight degradation of the precision. One possibl e\nexplanation for this improvement is that joint training hel ps\nthe model to focus on relevant parts of the dialog context.\nW e decided to use the Multi-T ask detection as our standard\nmethod for this task.\nKnowledge Selection The Hierarchical Selection model\nachieves the best results for MRR@5 and R@1 of all selec-\ntion models we tested. Other models outperform the model\nconcerning R@5. This can be explained by the fact that the\nmodel only returns documents of a single entity in its ﬁnal\nranking, thus these numbers are not fully comparable. When\nanalyzing the improvements in detail, we mainly see that the\nnumber of confusions among similar documents of different\nentities reduces. Due to the hierarchical structure, the mo del\nis forced to ﬁrst decide which domain and entity is relevant.\nThe other models have to make a tradeoff with respect to the\nrelevance of the document itself and the relevance of the do-\nmain and entity. Furthermore, Hierarchical Selection gene r-\nalizes very well to new domains and sources (R@1 of 98.5\nfor attraction, 94.4 for sf\nwritten, and 87.5 for sf spoken).\nAs expected, it achieves a signiﬁcant speedup of 20x com-\npared to the baseline selection method as shown in T able 3.\nEven so, for a real-time application, a latency of around 13\nseconds is still too high.\nThe Dense Knowledge Retrieval (DKR) model achieves\nan additional speedup of more than a factor of 100x com-\npared to the hierarchical selection model and more than a\nfactor of 2,500x compared to the baseline model. On the\nvalidation data, we observed that the negative log-likelih ood\n(NLL) loss outperforms the triplet loss and even achieves\nbetter results than the baseline method. Nevertheless, the\nmodel trained using the triplet loss seems to generalize bet -\nter to the test data where it outperforms the model trained\nusing the NLL loss. As shown in T able 4, the performance\nof the models is signiﬁcantly improved by joint training wit h\nthe RAG model. One interesting observation is that the DKR\nmodels do not generalize well to the spoken data (R@1 goes\ndown to 43.2 for the DKR NLL model) compared to other\nmodels.\nFinally, we trained a Multi-T askmodel on all tasks of the\nHierarchical Selection model and used it to calculate rele-\nT able 4: Selection results on both test sets.\nvalidation test\nR@1 R@5 R@1 R@5\nbaseline (ours) 91.9 99.7 91.0 99.3\n- w/o long context 81.4 97.9 84.0 97.7\n- w/o domain in input 71.8 92.8 65.2 81.4\nMulti-T ask 94.3 99.7 81.5 97.9\nHierarchdomain+entity ,doc 96.3 98.1 93.2 97.3\n- w/ multitask 87.1 89.0 84.1 90.5\nHierarchdomain,entity ,doc 96.8 98.6 88.1 91.1\n- w/ multitask 95.7 97.7 86.2 88.5\nDKR Triplet 90.9 98.9 87.2 96.9\n- w/o RAG 85.6 97.0 82.6 93.3\nDKR Triplet hard 90.8 98.9 83.8 95.2\n- w/o RAG 88.0 97.6 84.7 95.8\nDKR NLL 93.4 98.8 85.5 96.8\n- w/o RAG 90.1 98.1 84.0 94.4\nvance scores for all documents, similar to the baseline ap-\nproach. On the validation data this model outperformed the\nbaseline and DKR models but gave worse results on the test\ndata.\nResponse Generation As one can see from the results of\nentry ids 1 and 2 in T ables 1 and 2, there are no signiﬁ-\ncant improvements by using Retrieval Augmented Genera-\ntion over the baseline method in terms of automatic evalua-\ntion metrics. Nevertheless, T able 5 illustrates two exampl es,\nwhere it is beneﬁcial. As mentioned in Section 4.4, RAG\ncan incorporate multiple knowledge snippets into the gen-\nerated response. In the ﬁrst row , for example, the baseline\nmethod fails to mention dry cleaning, even though it was\nspeciﬁcally mentioned in the last user turn. RAG, however,\ncan give information on both laundry and dry cleaning. Fur-\nthermore, by conditioning the probability of a generated re -\nsponse on the selected snippet, errors made in the selection\ncan be corrected using RAG and are not necessarily propa-\ngated, as shown in the second example of the table. Thus, all\nof our systems shown in this section, except for the baseline\nmodels, use Retrieval Augmented Generation.\n6.2 Ablation Analysis\nW e perform an ablation study to analyze the inﬂuence of dif-\nferent changes on the results summarized above. In contrast\nto the results in T ables 1 and 2, all results discussed in this\nsection are based on the ground truth labels of the previous\ntask.\nPre-T rained Model W e evaluated a set of different pre-\ntrained transformer models in different size variants on th e\nbaseline approach for the knowledge selection task. The re-\nsults can be found in T able 6. In general, we observe better\nresults by using the larger variants of the models. Addition -\nally, RoBER T a and BAR T seem to outperform GPT -2 and\nBER T on this task. Since the differences between RoBER T a\nand BAR T are marginal and BAR T can also be used for the\ngeneration task in addition to the classiﬁcation tasks, we d e-\ncided to use BAR T large as our standard model.\nDomain As mentioned in Section 4.1, we include the do-\nmain of the knowledge snippet into its input representation\nto our models. As shown in T able 4 (w/o domain in input)\nthis results in signiﬁcant improvements in all three metric s\nfor this task. One of the main effects is that the confusion be -\ntween domains with similar documents is reduced. For ex-\nample, the number of confusions between the taxi and train\ndomain are reduced from 144 to 2.\nGeneration T able 7 compares different decoding methods\non the validation data. W e observe that beam search with a\nsmall beam size of four produces the best results compared\nto nucleus sampling, greedy search and a larger beam size\nof ten.\nDialog Context Length As illustrated by (Kim et al.\n2020), a long dialog context may often be beneﬁcial when\nretrieving relevant knowledge. Consider for example a con-\nversation, in which the entity in question in turn uT is\nonly mentioned explicitly much earlier in the dialog con-\ntext uT − 1\n1 . Our experiments show that accounting for a\nlonger context length signiﬁcantly improves performance.\nAs shown in T able 4, with the baseline model, we were able\nto increase performance on validation by 10.5% absolute for\nR@1 by increasing the considered context length from 128\n(w/o long context) to 384 tokens (baseline (ours)), which\nwas the maximal feasible size on our GPUs.\n6.3 Negative Results\nMulti-T ask T raining While some of our results using\nmulti-task training appeared promising in the beginning, w e\ndid not see any major improvements overall, even when\nlearning similar tasks jointly. When training all three tas ks\nof the challenge together, we only saw slight improvements\nin turn detection but degradation in both other tasks. Simi-\nlarly, when training our hierarchical models for knowledge\nselection we saw much better performance when all stages\nwhere trained on a single task, as illustrated in T able 4. Fur -\nthermore, while our relevance classiﬁcation model trained\nwith auxiliary losses, where we extended our baseline selec -\ntion model by auxiliary task-speciﬁc heads for domain, en-\ntity and document prediction, yielded decent improvements\non the validation data, it failed to generalize to unseen dat a\non test with strong performance degradation when compared\nto the single-task model.\nRetrieval Augmented Detection One general ﬂaw with\nhow we and Kim et al. (2020) approach the selection task is\nthat the knowledge of what can be handled by the API and\nwhat not is learned from the training data. Thus, in general,\nthe system is not capable to adapt to new APIs of unknown\ndomains or changes to known domains. Since we assumed\nthat every turn is either manageable by an API or by an un-\nstructured knowledge document, an alternative way to ap-\nproach the detection is to check whether a knowledge snip-\npet exists which is relevant to the dialog. This motivated us\nto apply the same approach as for the Retrieval Augmented\nGeneration to the detection task. Therefore, we marginaliz e\nour detection probability over K and apply the same approx-\nimations as in Section 4.4. The detection model thus receive s\nT able 5: Comparison of responses generated by RAG and the bas eline method.\nLast turn uT selected snippets Model: Generated response uT +1\nIs their laundry\nor dry cleaning\noffered onsite?\nk1: There are iron facilities avail-\nable, but no laundry facilities .\nbaseline: There are iron facilities available, but no\nlaundry facilities . Is there anything else I can help you wit h?\nk2: Allenbell does not provide\ndry cleaning service.\nRAG: No, dry cleaning is not offered onsite. There are\niron facilities available, but no laundry facilities . Anything else\nI can do for you?\nDo any of the\ntrains allow\nsmoking?\nk1: No, smoking is not allowed\nat the stations .\nbaseline: No, smoking is not allowed at the stations . Anything\nelse I can do for you?\nk2: No, smoking is not allowed\non the train .\nRAG: No, smoking is not allowed on the train . W ould you like to\nmake a reservation?\nT able 6: Comparison of different pre-trained transformer\nmodels for the baseline method of the selection task.\nMRR\nParams @5 R@1 R@5\nGPT -2 117M 91.9 87.2 97.6\nGPT -2 medium 345M 92.2 87.3 97.9\nBAR T base 139M 87.0 77.1 99.2\nBAR T large 406M 95.7 91.9 99.7\nBER T large 335M 92.2 86.2 99.3\nRoBER T a large 355M 95.7 92.1 99.8\nT able 7: Comparison of different decoding methods on the\nvalidation data.\nMethod BLEU-1 METEOR ROUGE-L\nSampling 41.6 42.1 40.9\nGreedy Search 44.8 45.7 44.4\nBeam Search 45.3 46.8 44.2\n- Beam Size 10 44.6 45.9 43.3\nthe dialog context and one knowledge snippet as input.\nW e mainly tested this method on our artiﬁcial out of do-\nmain corpus but as shown on the results on the test set in T a-\nble 8, we did not see any signiﬁcant improvements on top of\nthe baseline method. In contrast, we saw a signiﬁcant degra-\ndation in the recall. Initially, we assumed that this could b e\ncaused by cascaded selection errors, but a closer look at the\nresults did not conﬁrm this hypothesis. Though theoretical ly,\nwe think that the general approach is promising, we did not\nfollow up further in this direction.\nHard Batching Even though hard batching yielded im-\nprovements for the embedding models trained with triplet\nloss when compared to standard batching, the improvements\nwere only minor. Combining hard batching with ﬁne-tuning\nthe model with RAG even degraded the performance on the\ntest set. Generally, we saw better improvements using RAG,\nwhere the selection of the top n relevant snippets provides an\nimplicit form of training on hard examples. A possible ex-\nT able 8: Results of Retrieval Augmented Detection (RAD).\nBoth methods use BAR T base.\nPrec Rec F1\nbaseline 99.5 92.8 96.0\nRetrieval Augmented Detection 99.6 91.5 95.5\nplanation is that hard batching treats all but the ground tru th\nknowledge snippet, of which there was always exactly one\nfor each sample, as negatives. However, as seen in T able 5,\nmultiple snippets might provide relevant knowledge. The\nform of batching employed through retrieval augmentation\nappears to be more consistent, as multiple knowledge snip-\npets can be deemed relevant and only parts, which would not\ncontribute to a correct answer, would be penalized.\n7 Conclusion\nW e proposed two alternative methods for the knowledge se-\nlection task. First, with Hierarchical Selection, we achie ve\nour best results on the validation and test data and get a\nspeedup of 24x compared to the baseline method. Since the\nlatency is still quite high with an average of 4.6 seconds,\nwe propose Dense Knowledge Retrieval as another selection\nmethod. It achieves an additional speedup, in total more tha n\n2,500x compared to the baseline. On the validation data, it\neven outperforms the baseline with respect to MRR@5 and\nR@1, but does not seem to generalize as well to the new\nmodalities and domains in the test set. Finally, we show that\nretrieval augmented generation can be used to consider mul-\ntiple knowledge snippets and to jointly train selection and\ngeneration.\nAs shown by other teams in this challenge there is still\nroom for improvement with respect to performance. The de-\ntection task could beneﬁt from tighter integration with the\nAPI-based dialog system, i.e. by joint modeling of dialog\nstates and knowledge of the database schema. Data augmen-\ntation could help to improve the generalizability of the dif -\nferent methods and to avoid overﬁtting.\nReferences\nBudzianowski, P .; and V uli´ c, I. 2019. Hello, It’s GPT-2 –\nHow Can I Help Y ou? T owards the Use of Pretrained Lan-\nguage Models for T ask-Oriented Dialogue Systems. In Pro-\nceedings of the 3rd W orkshop on Neural Generation and\nT ranslation. Hong Kong, China: Association for Computa-\ntional Linguistics. doi:10.18653/v1/D19-5602.\nBudzianowski, P .; W en, T .-H.; Tseng, B.-H.; Casanueva, I.;\nUltes, S.; Ramadan, O.; and Gaˇ si´ c, M. 2018. MultiWOZ - A\nLarge-Scale Multi-Domain Wizard-of-Oz Dataset for T ask-\nOriented Dialogue Modelling. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language\nProcessing, 5016–5026. Brussels, Belgium: Association for\nComputational Linguistics. doi:10.18653/v1/D18-1547.\nDevlin, J.; Chang, M.-W .; Lee, K.; and T outanova, K. 2019.\nBER T: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language T echnolo-\ngies.\nDinan, E.; Roller, S.; Shuster, K.; Fan, A.; Auli, M.; and W e-\nston, J. 2018. Wizard of Wikipedia: Knowledge-Powered\nConversational Agents. In International Conference on\nLearning Representations.\nEric, M.; Goel, R.; Paul, S.; Kumar, A.; Sethi, A.; Ku, P .;\nGoyal, A. K.; Agarwal, S.; Gao, S.; and Hakkani-Tur, D.\n2020. MultiWOZ 2.1: A Consolidated Multi-Domain Di-\nalogue Dataset with State Corrections and State Tracking\nBaselines. In Proceedings of The 12th Language Resources\nand Evaluation Conference, 422–428.\nGhazvininejad, M.; Brockett, C.; Chang, M.-W .; Dolan, B.;\nGao, J.; Y ih, W .-t.; and Galley, M. 2018. A Knowledge-\nGrounded Neural Conversation Model. In AAAI.\nGunasekara, C.; Kim, S.; D’Haro, L. F .; Rastogi, A.; Chen,\nY .-N.; Eric, M.; Hedayatnia, B.; Gopalakrishnan, K.; Liu,\nY .; Huang, C.-W .; Hakkani-T ¨ ur, D.; Li, J.; Zhu, Q.; Luo,\nL.; Liden, L.; Huang, K.; Shayandeh, S.; Liang, R.; Peng,\nB.; Zhang, Z.; Shukla, S.; Huang, M.; Gao, J.; Mehri, S.;\nFeng, Y .; Gordon, C.; Alavi, S. H.; Traum, D.; Eskenazi, M.;\nBeirami, A.; Eunjoon; Cho; Crook, P . A.; De, A.; Geram-\nifard, A.; Kottur, S.; Moon, S.; Poddar, S.; and Subba, R.\n2020. Overview of the Ninth Dialog System T echnology\nChallenge: DSTC9.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P .; and Chang, M.-W .\n2020. REALM: Retrieval-Augmented Language Model Pre-\nTraining. In Proceedings of the 37th International Confer-\nence on Machine Learning.\nHam, D.; Lee, J.-G.; Jang, Y .; and Kim, K.-E. 2020. End-\nto-End Neural Pipeline for Goal-Oriented Dialogue Systems\nUsing GPT-2. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , 583–592.\nOnline: Association for Computational Linguistics. doi:1 0.\n18653/v1/2020.acl-main.54.\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2020. The Curious Case of Neural T ext Degeneration. In\n8th International Conference on Learning Representations ,\nICLR 2020. Addis Ababa, Ethiopia.\nJohnson, J.; Douze, M.; and Jegou, H. 2019. Billion-Scale\nSimilarity Search with GPUs. IEEE T rans. Big Data 1–1.\nISSN 2332-7790, 2372-2096. doi:10.1109/TBDA T A.2019.\n2921572.\nKarpukhin, V .; O ˘ guz, B.; Min, S.; Lewis, P .; Wu, L.; Edunov,\nS.; Chen, D.; and Y ih, W .-t. 2020. Dense Passage Retrieval\nfor Open-Domain Question Answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , 6769–6781. Association for\nComputational Linguistics.\nKeskar, N. S.; McCann, B.; V arshney, L. R.; Xiong, C.;\nand Socher, R. 2019. CTRL: A Conditional Trans-\nformer Language Model for Controllable Generation.\narXiv:1909.05858 [cs] .\nKim, B.; Ahn, J.; and Kim, G. 2020. Sequential Latent\nKnowledge Selection for Knowledge-Grounded Dialogue.\nIn International Conference on Learning Representations.\nKim, S.; Eric, M.; Gopalakrishnan, K.; Hedayatnia, B.; Liu,\nY .; and Hakkani-Tur, D. 2020. Beyond Domain APIs:\nT ask-Oriented Conversational Modeling with Unstructured\nKnowledge Access. In Proceedings of the 21th Annual\nMeeting of the Special Interest Group on Discourse and Di-\nalogue, 278–289. 1st virtual meeting: Association for Com-\nputational Linguistics.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2020a. BAR T: Denoising Sequence-to-Sequence Pre-\nTraining for Natural Language Generation, Translation, an d\nComprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, 7871–\n7880. Association for Computational Linguistics.\nLewis, P .; Perez, E.; Piktus, A.; Petroni, F .; Karpukhin, V . ;\nGoyal, N.; K ¨ uttler, H.; Lewis, M.; Y ih, W .-t.; Rockt¨ asche l,\nT .; Riedel, S.; and Kiela, D. 2020b. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP T asks. In\nNeurIPS.\nLiu, X.; He, P .; Chen, W .; and Gao, J. 2019a. Multi-T ask\nDeep Neural Networks for Natural Language Understand-\ning. In Proceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics . Florence, Italy: As-\nsociation for Computational Linguistics.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,\nO.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019b.\nRoBER T a: A Robustly Optimized BER T Pretraining Ap-\nproach. arXiv:1907.11692 [cs] .\nMoghe, N.; Arora, S.; Banerjee, S.; and Khapra, M. M.\n2018. T owards Exploiting Background Knowledge for\nBuilding Conversation Systems. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language\nProcessing. Brussels, Belgium: Association for Computa-\ntional Linguistics. doi:10.18653/v1/D18-1255.\nPeter, J.-T .; Beck, E.; and Ney, H. 2018. Sisyphus, a W ork-\nﬂow Manager Designed for Machine Translation and Auto-\nmatic Speech Recognition. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing: System Demonstrations, 84–89. Brussels, Belgium:\nAssociation for Computational Linguistics. doi:10.18653 /\nv1/D18-2015.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI blog 1(8): 9.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W .; and Liu, P . J. 2020. Exploring\nthe Limits of Transfer Learning with a Uniﬁed T ext-to-T ext\nTransformer. Journal of Machine Learning Research21: 67.\nReimers, N.; and Gurevych, I. 2019. Sentence-BER T:\nSentence Embeddings Using Siamese BER T-Networks.\narXiv:1908.10084 [cs] .\nRuder, S. 2017. An Overview of Multi-T ask Learning in\nDeep Neural Networks.\nW einberger, K. Q.; and Saul, L. K. 2009. Distance Metric\nLearning for Large Margin Nearest Neighbor Classiﬁcation.\nJMLR 207–244. ISSN 1532-4435.\nW olf, T .; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P .; Rault, T .; Louf, R.; Funtowicz, M.; Davi -\nson, J.; Shleifer, S.; von Platen, P .; Ma, C.; Jernite, Y .; Pl u,\nJ.; Xu, C.; Scao, T . L.; Gugger, S.; Drame, M.; Lhoest,\nQ.; and Rush, A. M. 2019. HuggingFace’s Transform-\ners: State-of-the-art Natural Language Processing. ArXiv\nabs/1910.03771.\nZhao, X.; Wu, W .; Xu, C.; T ao, C.; Zhao, D.; and Y an, R.\n2020. Knowledge-Grounded Dialogue Generation with Pre-\nTrained Language Models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Lan- Guage Pro-\ncessing (EMNLP) , 14. Association for Computational Lin-\nguistics.",
  "topic": "Dialog box",
  "concepts": [
    {
      "name": "Dialog box",
      "score": 0.9095897674560547
    },
    {
      "name": "Computer science",
      "score": 0.7369511127471924
    },
    {
      "name": "Task (project management)",
      "score": 0.6994322538375854
    },
    {
      "name": "Information retrieval",
      "score": 0.4861442744731903
    },
    {
      "name": "Natural language processing",
      "score": 0.4659104645252228
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3688860535621643
    },
    {
      "name": "World Wide Web",
      "score": 0.2568296194076538
    },
    {
      "name": "Engineering",
      "score": 0.07841247320175171
    },
    {
      "name": "Systems engineering",
      "score": 0.05100390315055847
    }
  ]
}