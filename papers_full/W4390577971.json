{
    "title": "TCNet: Multiscale Fusion of Transformer and CNN for Semantic Segmentation of Remote Sensing Images",
    "url": "https://openalex.org/W4390577971",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2569447223",
            "name": "Xuyang Xiang",
            "affiliations": [
                "China University of Geosciences"
            ]
        },
        {
            "id": "https://openalex.org/A2119362120",
            "name": "Wen-ping Gong",
            "affiliations": [
                "China University of Geosciences"
            ]
        },
        {
            "id": "https://openalex.org/A2289912125",
            "name": "Shuailong Li",
            "affiliations": [
                "China University of Geosciences"
            ]
        },
        {
            "id": "https://openalex.org/A2104161175",
            "name": "Jun Chen",
            "affiliations": [
                "China University of Geosciences"
            ]
        },
        {
            "id": "https://openalex.org/A3092144397",
            "name": "Tianhe Ren",
            "affiliations": [
                "China University of Geosciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2965344373",
        "https://openalex.org/W3157989468",
        "https://openalex.org/W4220842030",
        "https://openalex.org/W4313576208",
        "https://openalex.org/W4283450732",
        "https://openalex.org/W4205138939",
        "https://openalex.org/W3085780355",
        "https://openalex.org/W3171412329",
        "https://openalex.org/W6753737009",
        "https://openalex.org/W3194808133",
        "https://openalex.org/W4205365435",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W3035665735",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2787091153",
        "https://openalex.org/W2630837129",
        "https://openalex.org/W4312744282",
        "https://openalex.org/W3103092912",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W2592939477",
        "https://openalex.org/W3018169007",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4221144362",
        "https://openalex.org/W3168997536",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W2760340275",
        "https://openalex.org/W3112929693",
        "https://openalex.org/W4200142374",
        "https://openalex.org/W1905829557",
        "https://openalex.org/W2770233088",
        "https://openalex.org/W2963881378",
        "https://openalex.org/W1745334888",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W3137572916",
        "https://openalex.org/W3129042754",
        "https://openalex.org/W3200075728",
        "https://openalex.org/W3177272171",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4214893857",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W4313253257",
        "https://openalex.org/W4321232185",
        "https://openalex.org/W6790275670",
        "https://openalex.org/W3204166336",
        "https://openalex.org/W4361769328",
        "https://openalex.org/W4385859309",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W4295934810",
        "https://openalex.org/W3092344722",
        "https://openalex.org/W6750469568",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3003882269",
        "https://openalex.org/W2908320224",
        "https://openalex.org/W2780861787",
        "https://openalex.org/W2961348656",
        "https://openalex.org/W6761630670",
        "https://openalex.org/W2996327453",
        "https://openalex.org/W3215100961",
        "https://openalex.org/W3109998321",
        "https://openalex.org/W3190334976",
        "https://openalex.org/W2982206001",
        "https://openalex.org/W4226289601",
        "https://openalex.org/W4213172700",
        "https://openalex.org/W3161825146",
        "https://openalex.org/W4226224140",
        "https://openalex.org/W4367016725",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2940262938",
        "https://openalex.org/W3104035745",
        "https://openalex.org/W3183174367",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W4289744446"
    ],
    "abstract": "Semantic segmentation of remote sensing images plays a critical role in areas such as urban change detection, environmental protection, and geohazard identification. Convolutional Neural Networks (CNNs) have been excessively employed for semantic segmentation over the past few years; however, a limitation of the CNN is that there exists a challenge in extracting the global context of remote sensing images, which is vital for semantic segmentation, due to the locality of the convolution operation. It is informed that the recently developed Transformer is equipped with powerful global modeling capabilities. A network called TCNet is proposed in this article, and a parallel-in-branch architecture of the Transformer and the CNN is adopted in the TCNet. As such, the TCNet takes advantage of both Transformer and CNN, and both global context and low-level spatial details could be captured in a much shallower manner. In addition, a novel fusion technique called Interactive Self-attention is advanced to fuse the multilevel features extracted from both branches. To bridge the semantic gap between regions, a skip connection module called Windowed Self-attention Gating is further developed and added to the progressive upsampling network. Experiments on three public datasets (i.e., Bijie Landslide Dataset, WHU Building Dataset, and Massachusetts Buildings Dataset) depict that TCNet yields superior performance over state-of-the-art models. The IoU values obtained by TCNet for these three datasets are 75.34&#x0025; (ranked first among 10 models compared), 91.16&#x0025; (ranked first among 13 models compared), and 76.21&#x0025; (ranked first among 13 models compared), respectively.",
    "full_text": "1 \n> JSTARS-2023-01671 < \n \n \nXuyang Xiang, Wenping Gong, Shuailong Li, Jun Chen, Member, IEEE, and Tianhe Ren \n \n  \nAbstract—Semantic segmentation of remote sensing images \nplays a critical role in areas such as urban change detection, \nenvironmental protection, and geohazard identification. \nConvolutional Neural Networks (CNN) have been excessively \nemployed for semantic segmentation over the past few years; \nhowever, a limitation of the CNN is that there exists a challenge in \nextracting the global context of remote sensing images, which is \nvital for semantic segmentation, due to the locali ty of the \nconvolution operation. It is informed that the recently developed \nTransformer is equipped with powerful global modeling \ncapabilities. A network called TCNet is proposed in this study, and \na parallel-in-branch architecture of the Transformer and the CNN \nis adopted in the TCNet. As such, the TCNet takes advantage of \nboth Transformer and CNN, both global context and low -level \nspatial details could be captured in a much shallower manner. In \naddition, a novel fusion technique called Interactive Self-attention \n(ISa) is advanced to fuse the multi -level features extracted from \nboth branches. To bridge the semantic gap between regions, a skip \nconnection module called Windowed Self -attention Gating \n(WSaG) is further developed and added to the progressive \nupsampling network. Experiments on three public datasets (i.e., \nBijie Landslide Dataset, WHU Building Dataset, and \nMassachusetts Buildings Dataset) depict that TCNet yields \nsuperior performance over state-of-the-art models. The IoU values \nobtained by TCNet for these three datasets are 75.34% (ranked \nfirst among ten models compared), 91.16 % (ranked first among \nthirteen models compared), and 76.21% (ranked first among \nthirteen models compared), respectively. \n \nIndex Terms —semantic segmentation; remote sensing images; \nCNN; transformer; feature fusion. \n \nI. INTRODUCTION \nith the rapid advancement of aerospace and sensor \ntechnology [1], plenty of  high-quality remote \nsensing images can be accessed by the public; based \non these images,  the state of the environment and human \nactivity traces might be easily disclosed  [2]-[4]. Several \ntechniques have been explored for extra cting relevant \ninformation in remote sensing images [5]. Among the various \ntechniques available, semantic segmentation, the aim of which \n \n  This work was supported in part by the Outstanding Youth Foundation of \nHubei Province, China under Grant 2022CFA102, and in part by the National \nNatural Science Foundation of China under Grant 41977242. (Corresponding \nauthor: Wenping Gong). \nis to determine the semantic category of each pixel in the image \nof interest [6], has attracted increasing popularity over the past \nfew years [7]. It should be noted that semantic segmentation of \nremote sensing images has been successfully implemented in \nvarious application scenarios such as environmental protection \n[8], urban chan ge detection [9], and geohazard identification \n[10]. \nThe Convolutional Neural Network (CNN) has always been \ndeemed the most popular deep learning model, because of its \nremarkable ability in feature extraction and high level of \nautomation [11]. Indeed, the Fully Convolutional Network \n(FCN) created by Long et al. [12] could be taken as the \nprototype of most CNN models for semantic segmentation that \nare available in the literature. Among the various modifications \nof FCN, the encoder-decoder structure, which exhibits excellent \nsegmentation performance, has become the most popular \nstructure configuration [13]. For example, UNet [14] utilizes a \ndecoder to learn the spatial correlation of image features in \nencoding stages; and, Deeplab V3+ [15] involves a decoder \nbased on Deeplab V3 [16] to integrate spatial features, the \nnetwork performance is thus considerably improved. Note that \nwhile the CNN models may achieve good performance , multi-\nscale information of the concerned images cannot be fully \nutilized due to the design limitations of the decoder [17]. \nNonetheless, due to the complicated backgrounds and the \nexistence of a lot of noise in the images, global context \ninformation and reasoning capabilities for fine spatial features \nmust be enabled for effective semantic segmentation of remote \nsensing images. In other words, the conventional CNN models \nare not perfect for the semantic segmentation of remote sensing \nimages [18], and further improvement is warranted. \nTo address this limitation of the conventional CNN models, \nattention mechanisms and multiscale feature fusion strategies \nsuch as channel attention and position attention module [19], \ncriss-cross attention module [20], pyramid pooling module \n[21], and multi level feature fusion strategy [22], have often \nbeen adopted. However, the global information captured by \nthese approaches is not encoded from the global modeling \ndirectly [6], rather, it is mainly constructed of the local features \nXuyang Xiang, Wenping Gong, Shuailong Li, and Tianhe Ren are with the \nFaculty of Engineering, China University of Geosciences, Wuhan, CO 430074, \nChina (e-mail: xiangxuyang@cug.edu.cn; wenpinggong@cug.edu.cn; \nli_shuailong@cug.edu.cn; rentianhe@cug.edu.cn). \nJun Chen  is with the School of Automation , China University of \nGeosciences, Wuhan, CO 430074, China (e-mail: chenjun71983@163.com). \nTCNet: Multiscale Fusion of Transformer and \nCNN for Semantic Segmentation of Remote \nSensing Images \nW \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> JSTARS-2023-01671 < \n \ncaptured with the existing CNN models; as such, the global \nscene information might not be captured yet [23]. The \nTransformer, well -known in Natural Language Processing \n(NLP) for its exceptional capability in capturing global \nrelationships, offers a viable answer to semantic segmentation  \n[24]; and, astounding performance could be achieved by the \nTransformer in the field of computer vision. For example, a \nTransformer encoder -decoder architecture was developed to \nmodel the relations between the objects and the global image \ncontext [25]; and, a dual -branch Transformer structure was \ndesigned t o learn the multi -scale feature of images [26]. \nFurther, the Swin Transformer was constructed, and it exhibits \ngreat potential in image classification and dense prediction \ntasks [27]. Noted that while Transformer -based models have \nbeen widely adopted in other image segmentation and huge \nprogress has been achieved [6], their application in semantic \nsegmentation of remote sensing images still needs to be \nexplored. It is noted that although the Transformer can capture \nthe long-range dependency of global features  effectively, the \nlocal feature information is frequently disregarded.  In order to \neffectively segment remote sensing images using semantics, it \nis crucial to create a model that can benefit from both the \nTransformer and the CNN, and thus, both the global context and \nfine spatial details could be effectively captured [28].  \nIn this study, a network called TCNet is proposed a nd a \nparallel-in-branch architecture is utilized. In the context of th e \nTCNet, a Transformer branch is employed to obtain the global \ncontext while a CNN branch is adopted to capture the low-level \nspatial details. In addition, a novel fusion technique called \nInteractive Self-attention (ISa) is advanced to fuse the multi -\nlevel features extracted from both branches; and, to bridge the \nsemantic gap between different regions, a skip connection \nmodule called Windowed Self -attention Gating (WSaG) is \ndeveloped and added to the progressive upsampling network. \nThe main contributions of this article are given as follows. \n1) A novel TCNet is proposed to achieve accurate semantic \nsegmentation. The parallel-in-branch architecture of the TCNet \nuses Transformer and CNN to extract local and global feature \ninformation. The ISa effectively couples the information that is \nextracted from the parallel-in-branch architecture. To bridge the \nsemantic gap between various regions, the linked multiscale \nfeature information improves the interaction between features \nusing the WSaG -based progressive upsampling network,  thus \nimproving the accuracy of segmentation. \n2) To effectively couple the coding features from CNN and \nTransformer, ISa is designed. ISa contains the attention residual \n(ARE) block, the efficient self -attention (ESA) module, the \ninteractive efficient self -attention (IESA) module, and the \nResidual block, which can help focus on focal area information. \n3) The WSaG-based progressive upsampling network is used \nas a decoder to ensure effective multiscale feature interaction. \nWSaG is used to create a within windows and cross -window \ntransfer of features, making more efficient use of features. \nFeatures between different levels are connected by upsampling, \nas such, bottom-up information interactions can be realized. \nThe rest of this paper is organized as follows. First, the \nstudies on semantic segmentation of remote sensing images are \nreviewed. Second, the methodology of the TCNet is detailed. \nThird, illustrative applications are presented based on three \npublic datasets  (i.e., Bijie Landslide Dataset, WHU Building \nDataset, and Massachusetts Buildings Dataset). Fourth, \ncomparisons are conducted to depict the superiority of the \nTCNet over state-of-the-art models. Fifth, the model efficiency \nand the significance of each branch are discussed. Finally, the \nconcluding remarks are drawn.  \nII. RELATED WORK \nConvolutional Neural Networks (CNNs) and Transformers \nare two types of methods that could be used for the semantic \nsegmentation of remote sensing images. A short literature \nreview of these two methods is presented in this section. \n \nA. CNN-based Semantic Segmentation of Remote Sensing \nImages \nConvolutional Neural Network (CNN) was mainly developed \nbased on  Artificial Neural Networks (ANNs).  Because of its \nexceptional performance, CNN has gained increased popularity \nin various areas [29]. The Fully Convolutional Network (FCN), \ncreated by Long et al. [12], could be taken as the prototype of \nmost CNN models used in the existing semantic segmentation \nof remote sensing images [30] -[33]. In comparison to the \nclassical CNN, the fully connected layers are replaced by \nconvolutional layers in the FCN; as such, the FCN is equipped \nwith the capability to make predictions on arbitrary-sized inputs \nand the pixels -to-pixels mapping can be learned by the \nnetworks, without extracting the region proposals [12], [34], \n[35]. The existing FCN-based models are best suited for local \ntasks, not global tasks, due to their particular structure. [35]. For \nexample, rather than object classification, the FCN -based \nmodels could be more suitable  for semantic segmentation or \nobject detection. \nThe resolution of the predictions generated by the FCN is low \ndue to the intrinsic limitation of the simple decoder used, and \nthe boundaries of the object recognized are fuzzy. The encoder-\ndecoder structure was subsequently created by building \nsymmetrical decoders like UNet [14] and SegNet [36] to \novercome this problem; and, the spatial resolution of extracted \nfeatures could  then be restored progressively. Further, to \nimprove the effectiveness of the encoder -decoder structure in \ncapturing richer contextual features and reducing the loss of \nfeature information, various enhancing techniques such as deep \ndeconvolution network [37] an d atrous convolution [38] have \nbeen developed and included in the modified CNNs. The \nencoder-decoder structure has emerged as a dominating \nstructure configuration in semantic segmentation [13]. \nHowever, the improved CNN models are not yet capable of \ncorrectly identifying complex objects in remote sensing images \n[17]. In such a situation, an attempt, based on attention \nmechanisms and multiscale feature fusion strategies, has been \nconducted to improve the segmentation precision thr ough \nexploiting the contextual information. For example, a linear \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \n> JSTARS-2023-01671 < \n \nattention mechanism was constructed and added to each skip \nconnection to establish long -term dependencies of the feature \nmap [39]; a top-down strategy was advanced to fuse high-level \nfeatures with shallow low -level features, acquired by the deep \nand shallo w layers, respectively [33]; and, a multiscale skip \nconnection network was designed to realign semantic features \nof different levels [40]. \nRecent advances in CNNs have promoted the semantic \nsegmentation of remote sensing images. The CNN models \nmentioned above have been successfully applied in various \nareas, such as environmental protection [8], urban change \ndetection [9], and geohazard identification [10]. However, the \nCNN models modified are mainly based on convolution \noperations, which are not liberated from the original CNN \nstructure. In summary, the CNN models are not perfect, and \nlimitations in acquiring the global information of remote \nsensing images, intra-class differences of which are large while \ninter-class differences are small, are evident [41], [42]. \n \nB. Transformer-based Semantic Segmentation of Remote \nSensing Images \nThe Transformer was initially developed for natural language \nprocessing (NLP), and it has been shown that its precision is \ngreater than that of conventional sequence transduction models \nbased on complex recurrent networks or CNNs  [43]. Then, \nseveral Transformer -based models have been  developed for \nsemantic segmentation of remote sensing images [24]. \nTransformer-based models typically perform better in global \ncontext modeling than the CNN models mentioned above due \nto their strong capabilities in sequence -to-sequence modeling \n[17]. \nIt should be noted that the majority of Transformer -based \nsemantic segmentation models employ an encoder -decoder \nstructure, and these models can be broadly classified into two \ncategories. The first category is solely based on Transformers \nand the representative models are Segmenter [44], SegFormer \n[45], CrackFormer [46], and SwinUNet [47].  As the \nTransformer primarily focuses on global modeling and lacks \nlocalization capabilities, the investigations by Wang et al. [5], \nChen et al. [48], Zhang et al. [49], Long et al. [50], and Zhang \net al. [51] show that the pure Transformer-based segmentation \nnetworks may generate unacceptable performance. On the other \nhand, hybrid architectures are oftentimes adopted in the second \ncategory. For example, a dual -branch encoder, which is based \non the Transformer and the CNN, was created for urban scene \nunderstanding [17]. By integrating the Swin Transformer into \nthe traditional CNN-based UNet, a new dual encoder structure \nwas created [6]; and, to model both global and local information \nmore effectively, a Transformer -based decoder was designed \nand the lightweight ResNet-18 was selected as the encoder [5]. \nIt is noted that although Transformer -based models have been \nwidely used in the segmentation of medical images [6], their \nuse in the semantic segmentation of remote sensing images has \nbeen limited. Inspired by the studies discussed above, a network \ncalled TCNet, in which the parallel -in-branch architecture is \nutilized, is proposed in this study. In the context of the proposed \nTCNet, a Transformer branch is employed to obtain the global \ncontext while a CNN branch is adopted to capture the low-level \nspatial details. \nIII. METHODOLOGY OF THE TCNET \nIn this part, the architecture and key modules of the proposed \nTCNet are described in depth. The Interactive Self-attention (ISa) \nmodule and the Windowed Self-attention Gating (WSaG)-based \nProgressive Upsampling Network are introduced after outlining \nthe overall structure of the TCNet.  \n \nA. Overall Structure of The TCNet \nAs shown in Fig. 1, two parallel feature extraction branches \nare adopted in the TCNet; in which, the branch of ResNet -34 \n[52] is employed to encode the local features while that of 8 -\nlayer DeiT-Small (DeiT-S) [53] is adopted to encode the global \nfeatures. Note that the ResNet networks can improve the link \nbetween different layers of the network, allowing for more \nplentiful expression of high -resolution features ; whereas, a \nteacher-student strategy is taken by the DeiT, thus, much fewer \ndata are demanded for model training and better convergence \nperformance could be achie ved. Further, both ResNet -34 and \nDeiT-Small are lightweight models. In summary, a parallel-in-\nbranch architecture, which is based on the ResNet-34 and DeiT-\nSmall, is adopted in the proposed TCNet. \nThere are five blocks embedded in the ResNet-34, each block \ndownsamples the feature maps by a factor of two. The outputs \nderived from the 4th ( g0  \nHW 25616 16R\n ), 3rd (g1  \nHW 12888R\n ), \nand 2nd (g2  \nHW 6444R\n ) blocks of the CNN branch ResNet-34 \nare fused with the results derived from the Transformer branch \nof 8-layer DeiT-Small. Note that a typical encoder structure is \ntaken in the 8-layer DeiT-Small. Specifically, an input image F \n \nH W 3R   is first equally divided into N = \nHW×16 16  patches, \nwhich are then flattened and passed to a linear embedding layer \nwith an output dimension of D0; and, as a result, raw embedding \nsequence z0  RN×D0 can be derived. The resulting embeddings \nz0  RN×D0 are inputted to the encoder of 8 -layer DeiT-Small, \nwhich contains 8 layers of multiheaded self -attention (MSA) \nand Multi -layer Perceptron (MLP). It should be  noted that a \nlayer normalization exists in front of the MSA and MLP (of \neach layer). The output of the encoder is further reshaped to a \nfeature map t0  \nHW 38416 16R\n . Finally, the spatial resolution of \nthe reshaped feature map t0  \nHW 38416 16R\n  is recovered with the \naid of two consecutive standard upsampling -convolutional \nlayers; as an outcome, the feature maps t1  \nHW 12888R\n  and t2  \nHW 6444R\n\n are sequentially obtained. The feature maps of \ndifferent scales (i.e., t0, t 1, and t 2) are then fused with those \nobtained from the CNN branch (i.e., g0, g1, and g2), as illustrated \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4 \n> JSTARS-2023-01671 < \n \nin Fig. 1. \nNoted that the feature maps obtained from the Transformer \nbranch (i.e., t0, t1, and t2) and those from the CNN branch (i.e., \ng0, g 1, and g 2), in the proposed TCNet, are fused with the \nmodule of Interactive Self -attention (ISa). Here, the local \ntexture feature gi and global context feature ti (i = 0, 1, and 2 ) \nare correspondingly inputted to the ISa module. As both local \nand global features are fused, the representa tion of the context \nin images can be more complete and more compact. Four blocks \nare involved in the ISa module proposed, including the attention \nresidual (ARE) block, the efficient self-attention (ESA) module \n[54], the interactive efficient self-attention (IESA) module,\nFig. 1. Structure of the proposed TCNet.\n \nand the Residual block . Noted that the ARE block, IESA \nmodule, and Residual block  are specially developed in this \npaper. The contextual semantics within large neighborhoods of \nthe local and global context features (i.e ., inputs to the ISa \nmodule) are first extracted with the aid of the ARE block; and, \nthe resulting semantics are then cross-fused with the modules \nof ESA and IESA separately. Finally, the results obtained from \nthe ESA module and those from the IESA module are processed \nby the Residual block; as such, the  features of multiple scales \ncould be selectively emphasized. Afterward, the fused feature \nmap fi (i = 0, 1, and 2) , the output of the ISa module , is \ntransferred to the Windowed Self -attention Gating (WSaG) -\nbased progressive upsampling network (PUN). As an outcome, \na final s egmentation map P 2 could be generated by a simple \nhead (Pred. Head). To optimize the network of the TCNet, two \nauxiliary heads are adopted to generate two segmentation maps \nP0 and P1 as the i ntermediate products [55]. The operation of \nthe TCNet is expressed as follows. \ng  = CNN(F)i\n  (1) \nt  = Transformer(F)i\n  (2) \nf  = ISa(g ,t )i i i\n  (3) \n02P  = Pred. Head(t )\n  (4) \n10P  = Pred. Head(f )\n  (5) \n2 0 1 2P  = Pred. Head(PUN(PUN(f , f ), f ))\n (6) \n \nB. Module of Interactive Self-attention (ISa) \nTo couple the local and global features , obtained by CNN \nand Transformer, respectively, more effectively, an Interactive \nSelf-attention (ISa) module is developed and employed in the \nproposed TCNet. As mentioned above, four blocks, in terms of \nthe ARE block, the ESA module, the IESA module, and the \nResidual block, are involved in the ISa module, as shown in Fig. \n2. Inspired by the Attention U-net proposed by Oktay et al. [56], \natrous convolutions (AConv) are adopted in the ARE block to \ncapture contextual semantics within a large neighborhood; and, \nthe context contrast in the obtained feature map is enhanced \nwith the aid of the ESA module.  Further, the relationship s \nbetween different feature map s obtained by the CNN and \nTransformer branches are learned by the IESA module. Finally, \nthe contextual information obtained by the ESA module and \nthat obtained by the IESA module are processed by the Residual \nblock. Thus, the information interference from irrelevant \nregions is suppressed, the features of multiple scales could be \nselectively emphasized, and the number of channels is reduced. \nThe procedures for the feature fusion with the ISa module are \nsummarized as follows: \n1) The local texture feature gi and global context feature ti (i \n= 0, 1, and 2) are inputted to the ARE block, and the input ted \nfeature gi and ti (i = 0, 1, and 2) are processed by three different \nconvolutional layers (a 1  1 convolutional layer and two 3  3 \nAConv layers)  separately. To improve the convergence and \ngeneralization of the TCNet, the feature obtained from each \nconvolutional layer is further processed by a Batch \nNormalization (BN) layer . Finally, an element -wise sum \noperation is conducted to refine the obtained features. \n2) The feature maps obtained from the ARE block are cross-\nfused with the modules of ESA and IESA separately. The \nmodules of ESA and IESA are respectively based on two novel \nself-attentions. A pyramid pooling operation is included in the \nnovel self -attentions, in comparison to the original self -\nattentions. The feature obtained from the novel self-attentions \nis further processed by a feed -forward (FF) layer and a \nreshaping operation  sequentially. For simplicity, t he residual \naddition between the input and the output of FF  is omitted . \nFinally, to learn the relationships between different feature \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5 \n> JSTARS-2023-01671 < \n \n  \nFig. 2. Structure of the proposed Interactive Self-attention (ISa) module.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6 \n> JSTARS-2023-01671 < \n \nmaps obtained from the CNN and Transformer branches, a joint \nmechanism is created and included in the IESA module.  \n3) The results obtained from the ESA module and those from \nthe IESA module are concatenated via a channel-wise \nconcatenation operation, and the outcome is then inputted to the \nResidual block. The number of channels of the inputted feature \nmap is first reduced by two 1  1 convolutional layers; and, the \nderived feature map  is further processed by three different \nconvolutional layers (a 3  3 Depthwise Conv layer and two 3 \n 3 AConv layers) separately.  The contextual semantics \nobtained from the three convolutional layers are refined through \na matrix multiplication operation; and, the refined feature map \nfi (i = 0, 1, and 2), the output of the ISa module, is finally \ntransferred to the WSaG-based progressive upsampling \nnetwork. \nThe operation of the ISa module is expressed as follows. \nARE(g ) = BN(AConv(g )) + BN(AConv(g )) + BN(Conv(g ))i i i i\n (7) \nARE(t ) = BN(AConv(t )) + BN(AConv(t )) + BN(Conv(t ))i i i i\n (8) \n08\n0ESA(ARE(t )  ARE(g )) = (concat(head ,..., head ))ii \n (9) \nhead  = Attention( (Q), (K), (V))j j j j\nq k v\n (10) \nQKAttention(Q, K, V   softmax( )V\nd\nT\nk\n) =\n (11) \n08\n0 1 1IESA(t  g ) = (concat(head ,..., head )ii, \n (12) \n08\n0 2 2IESA(g  t ) = (concat(head ,..., head )ii, \n (13) \n1head  = Attention( (Q ), (K ), (V ))j j j j\nq g k t v t  \n (14) \n2head  = Attention( (Q ), (K ), (V ))j j j j\nq t k g v g  \n (15) \nf  = Residual(concat(b , t , g ))i i i i ˆ ˆ ˆ\n \n (16) \nwhere 0 is the linear projection for the matrix of all attention \nheads (i.e., head1, head2, …, head 8);  jq,  jk, and  jv are the \nlinear projections for the matrix Q, matrix K, and matrix V of \nthe jth head, respectively ( j = 1, 2,…, 8); Attention() is the \nattention function; Hi, Wi, and Ci are the height, width, number \nof channels of the feature map ARE(g i) and ARE(t i) obtained \nby the ARE block, respectively (i = 0, 1, and 2); Q  RNi  Ci (Ni \n= Hi  Wi) is the reshaped matrix of the input feature map gi, ti, \nand \nARE(t ) ARE(g ) ii\n (i = 0, 1, and 2), in which \n  is the \nHadamard product; K  RS × Ci and V  RS × Ci are matrices \nobtained from the pyramid pooling , reshaping, and \nconcatenating (concat) operations (S = 1  1, 3  3, or 5  5); \nKT is the transpose matrix of matrix K; softmax() is the softmax \nfunction; dk is a scale factor that indicates the dimension of each \nattention head, the value of which is \nC\n8\ni ; \nbiˆ  is the feature map \noutputted from the ESA module, the related inputs are ti and gi \n(i = 0, 1, and 2); \ntiˆ  is the feature map outputted from the IESA \nmodule, the related input is the feature map ti (i = 0, 1, and 2); \nand, \nigˆ  is the feature map outputted from the IESA module, the \nrelated input is the feature map gi (i = 0, 1, and 2). \n \nC. Windowed Self-attention Gating (WSaG)-based Progressive \nUpsampling Network \nInspired by  the SEgmentation T Ransformer (SETR) model \n[57], the progressive upsampling network is adopted as the \ndecoder in the proposed TCNet . It is noted that convolutional \nlayers and upsampling operations are alternately adopted in the \nprogressive upsampling network; as such, noisy predictions that \nmight be induced by the one -step upscaling could be avoided. \nTo capture both global and local contexts of multiple scales, a \nskip connection module called Windowed Self-attention Gating \n(WSaG) is advanced in this study and added to the progressive \nupsampling network of the proposed TCNet. WSaG is created \nbased on the Swin Transformer [27], with whic h the full flow \nof feature information between different scales is maximized. \nWithin the Swin Transformer, the ordinary multiheaded self -\nattention (MSA) is replaced by the window -based MSA (W -\nMSA) and shifted W -MSA (SW-MSA). With the aid of these \ntwo self -attentions, self -attention within windows and cross -\nwindow connections can both be realized. The structure of the \nproposed WSaG-based progressive upsampling network is \nshown in Fig. 3. \n \n \nFig. 3.  Structure of the proposed Windowed Self-attention \nGating (WSaG)-based progressive upsampling network. \n \nThe procedures for the feature fusion with the ISa module are \nsummarized as follows. The feature maps fi and fi+1 (i = 0, 1) of \ndifferent resolutions , obtained from the ISa m odule, are \ninputted to the module of WSaG, where the low-resolution \nfeature map fi is upsampled (Up) by a factor of 2 before the \nhandling by the WSaG module. The output of the WSaG \nmodule is an attention map , based on which  a matrix \nmultiplication operation is conducted to refine the original \nhigh-resolution feature map fi+1 (i = 0, 1). Afterward, a channel-\nwise concatenation operation and a convolution (Conv) \noperation are sequentially undertaken to fuse the original \nfeature map fi and the refined feature map fi+1 (i = 0, 1). Finally, \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7 \n> JSTARS-2023-01671 < \n \nfeature maps \n1f iˆ +  (i =1, 2) are generated by the WSaG-based \nprogressive upsampling network; and then, the feature map \n2fˆ\nis processed by a simple head to generate a final segmentation \nmap P2. \nThe operation of the proposed WSaG-based progressive \nupsampling network can be expressed as follows. \n00f  = fˆ\n  (17) \n11f  = Conv([Up(f ), WSaG(f , Up(f ))])i i i iˆ ˆ ˆ++\n (18) \nIV. EXPERIMENTS ON THREE PUBLIC DATASETS \nIn this section, experiments are carried out on three public \ndatasets, including the Bijie Landslide Dataset, the WHU \nBuilding Dataset, and the Massachusetts Buildings Dataset, to \ndepict the effectiveness and superiority of the proposed TCNet. \n \nA. Introduction of The Three Public Datasets \n(1) Bijie Landslide Dataset: \nThe Bijie Landslide Dataset  consists of satellite optical \nimages, shapefiles of landslides’ boundaries , and digital \nelevation models. The dataset is constructed based on the data \ncollected in an area in Bijie City, Guizhou Province, China [58], \nwhich covers an  area of 26,853 km 2. The TripleSat satellite \nimages taken from May to August 2018 were cropped to create \n770 landslide images and 2,003 non-landslide images.  The \nresolution of the digital elevation model is 2.0 m, compared to \n0.8 m for the satellite optical images and shapefiles of landslide \nboundaries. In this study, 770 landslide images are studied for \nthe performance test of the TCNet; among which, 462 images \nthat are arbitrarily selected are taken as the training set, 154 \nimages that are arbitrarily selected from the left 308 images are \ntaken as the validation set, and the left 154 images are taken as \nthe test set. \n(2) WHU Building Dataset: \nThe WHU Building Dataset consists of satellite and aerial \nimages [59]. There are 8,189 aerial images of 512× 512 pixels \nin this dataset, and the resolution of the aerial images is 0.3 m. \nOnly aerial images are studied in this paper. These aerial images \ncover an area of over 450 km 2 and 220,000 buildings in \nChristchurch, New Zealand. According to the rules provided in \nJi et al. [59], 4,736 images are taken as the training set, 1,036 \nimages are taken as the validation set, and 2,416 images are \ntaken as the test set. In the experiments conducted, the aerial \nimages are preprocessed through cropping, each aerial image is \ndivided into 4 smaller images of 256 ×  256 pixels ; and, the  \ncropped images are then resized to larger images of 736 ×  736 \npixels. It is noted that the resized images, not the original aerial \nimages, are adopted for the performance test of the TCNet. \n(3) Massachusetts Buildings Dataset: \nThe Massachusetts Buildings Dataset consists of 151 aerial \nimages collected in Boston, Massachusetts, USA, the size of \neach aerial image is 1500 ×  1500 pixels and the related area is \n2.25 km 2 (https://www.cs.toronto.edu/~vmnih/data/). The \nresolution of the aerial images is 1 .0 m. These aerial images \ninclude a variety of buildings, including residential, commercial, \nand industrial structures.  According to the default rule s, 137 \nimages are taken as the training set, 4 images are taken as the \nvalidation set, and 10 images are taken as the test set. Similarly, \nthe aerial images are preprocessed through cropping, each aerial \nimage is divided into 36 smaller images of 250 ×  250 pixels; \nand, the cropped images are then resized to larger images of 768 \n×  768 pixels. \n \nB. Experimental Settings and Evaluation Indexes \nThe full network in this study is trained end -to-end with the \nweighted IoU loss and the binary cross entropy loss, denoted as \nLIoU and Lbce, respectively. Note that weighted IoU loss is \nmainly proposed for measuring the similarity between the \nsegmentation map, obtained by the simple head (Pred. Head) , \nand the ground truth [60]; and, the binary cross entropy loss is \nmainly employed to measure the loss of boundary pixels [61]. \nTo improve the gradient flow,  deep supervision is adopted to \nsupervise the output segmentation map of the Transformer \nbranch and that of the first ISa module . On the basis of the \ncomputed LIoU and Lbce, the overall loss function Ltotal can be \ncalculated as follows: \n20\n2\n = α (G, Pred. Head(f )) + β (G, Pred. Head(f )) \n           + γ (G, Pred. Head(t )) \ntotal\nˆL L L\nL\n (19) \n =  + IoU bceL L L\n  (20) \nwhere α, β, and γ are tunable hyperparameters , the values of \nwhich are taken 0.5, 0.3, and 0.2, respectively, in this study; L \nis the joint loss function of LIoU and Lbce; G is the ground truth; \nand, Pred. Head(\n2fˆ ), Pred. Head (f0), and Pred. Head(t2) are the \nsegmentation maps obtained from the WSaG-based progressive \nupsampling network, the Transformer branch, and the first ISa \nmodule, respectively. \nThe results of the pixel classification can be divided into true \npositive (TP) (i.e., the foreground is classified as foreground), \nfalse-positive (FP) ( i.e., the background is classified  as \nforeground), true negative (TN) ( i.e., the background is \nclassified as background), and false negative (FN) ( i.e., the \nforeground is classified  as background). To quantitatively \nassess the performance of the trained TCNet in the experiments, \nfive indexes, in terms of the IoU, Overall Accuracy (OA), \nPrecision, Recall, and  F1 score, are evaluated, and t he \nmathematical formulations of these indexes are given below. \nTPIoU  TP + FN + FP=\n  (21) \nTP + TNOA  TP + TN + FP + FN=\n  (22) \nTPPrecision  TP + FP=\n  (23) \nTPRecall  TP + FN=\n  (24) \nPrecision  RecallF1 score  2 Precision + Recall\n=\n (25) \nThe training and validation sets are  first imported to the \nplatform of GPU-based Pytorch; and, these data are trained for \n30 epochs utilizing the proposed TCNet. During the training of \nthe model, Gaussian Blur -, Solarization -, Grayscale -, and \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 \n> JSTARS-2023-01671 < \n \nRandom Horizontal Flip -based d ata augmentation operations  \nare conducted. Noted that the training of the TCNet  model is \nexecuted on a desktop equipped with 96.0 GB RAM, an Intel(R) \nXeon(R) W-2145 CPU running at 3.70 GHz, and an NVIDIA \nQuadro P5000 64 -GB GPU . Further, the Adam optimizer  is \nadopted in this study; and, the learning rate is set up as 7e-5 \nwhile the batch size of the training is set up as 1. The outcome \nof th is model  training is an automatic image segmentation \nmodel that can detect and map the foreground in input images. \n \nC. Performance Evaluation of The Proposed TCNet and \nComparisons with Other Models \nListed in Table I are evaluated performance indexes of the \ntrained TCNet models based on these three public datasets. As \nshown in Table I, all the IoU values obtained by the TCNet are \ngreater than 75.00%, indicating that the segmentation maps \nobtained from the trained TCNet models are highly consistent \nwith the ground truth. In addition, the other four indexes (i.e., \nOA, Precision, Recall, and F1 score) obtained by the TCNet are \nall greater than 84.00%, which depicts the effectiveness of the \nproposed TCNet.  Illustrated in Fig. 4 are the example \nsegmentation maps obtained from the trained TCNet models, \nshowing that the boundaries of landslides and buildings, \narbitrarily selected in the datasets, can be precisely extracted. \nTABLE I \nPERFORMANCE EVALUATION OF THE TCNET ON THE THREE \nPUBLIC DATASETS \n \n \n \nFig. 4. Visualized results of the proposed TCNet on the three \npublic datasets. \nTo further demonstrate the effectiveness and s uperiority of \nthe TCNet, the performance of the TCNet and that of some \nstate-of-the-art models are compared. The models selected for \nthe comparison s are UNet [14], PSPNet [21], SegNet [36], \nDeepLab V3+ [15], HRNetV2 [62], MA-FCN [63], BiSeNetv2 \n[64], SegFormer [45], RSR-Net [64], MANet [65], BANet [66], \nMAP-Net [67], BuildFormer [68], BOMSC-Net [69], DC-Swin \n[70], ASF -Net [71], CLCFormer [ 50], SDSC -UNet [7 2], \nDSAT-Net [51], and UNetFormer [5]. For ease of comparison, \nthe models compared are retrained under an identical operating \nenvironment as that adopted by the TCNet.  \nThe comparison is first conducted based on the Bijie \nLandslide Dataset. Listed in T able II are the performance \nindexes evaluated from the TCNet and those from the compared \nmodels (i.e., UN et, PSPNet, DeepLab V3+, SegFormer, \nHRNetV2, CLCFormer, SDSC -UNet, DSAT -Net, and \nUNetFormer). Table II shows that the IoU (75.34%) obtained \nfrom the TCNet is always much higher than those from the \ncompared models. It is noted that there is no unified rule for the \ndivision of the training set, verification set, and test set  in the \nBijie Landslide Dataset. To ensure that the same training set, \nverification set, and test set are adopted in this comparison, the \nperformance indexes of the compared models are derived in this \nstudy, not from the existing literature. The data in Table II also \nshow that the performance indexes of OA and F1 score obtained \nfrom the TCNet are always higher than those from the models \ncompared, and the performance indexes of precision and recall \nobtained by TCNet are close to the highest performance indexes \nof the models compared, implying that the proposed TCNet \nresults in fewer pixel errors in the landslide classification. \nIllustrated in Fig. 5 are the example segmentation maps \nobtained from the TCNet model and those from the compared \nmodels, showing th at t he landslide pixels could be more \neffectively classified by the proposed TCNet. \nTABLE II \nQUANTITATIVE COMPARISON WITH STATE-OF-THE-ART \nMETHODS ON THE BIJIE LANDSLIDE DATASET \n \n \nNext, the comparison is conducted based on the WHU \nBuilding Dataset. Listed in T able III are the performance \nindexes evaluated from the TCNet and those from the compared \nmodels (i.e., UNet, DeepLab  V3+, BiSeNetv2, SegNet, \nBOMSC-Net, MAP-Net, MA-FCN, RSR-Net, CLCFormer, \nSDSC-UNet, DSAT-Net, and UNetFormer). The data in Table \nIII depict that the IoU (91.16%), OA (98.80%) and Recall \n(95.55%) obtained from the TCNet are much higher than those \nfrom the compared models; whereas, the performance indexes \nof Precision and F1 score obtained from the TCNet are close\nDatasets IoU OA Precision Recall F1 \nBijie Landslide Dataset 0.7534 0.9720 0.8419 0.8920 0.8512 \nWHU Building Dataset 0.9116 0.9880 0.9515 0.9555 0.9395 \nMassachusetts Buildings \nDataset 0.7621 0.9485 0.8517 0.8682 0.8429 \n 1 \nMethod IoU OA Precision Recall F1 \nUNet [14] 0.5767 0.9503 0.7134 0.7361 0.6989 \nPSPNet [21] 0.5402 0.9396 0.7445 0.7201 0.6597 \nDeepLab V3+ [15] 0.6012 0.9503 0.7961 0.7604 0.7215 \nSegFormer [45] 0.6105 0.9538 0.7419 0.7862 0.7252 \nHRNetV2 [62] 0.6570 0.9614 0.7960 0.8018 0.7689 \nCLCFormer [50] 0.5858 0.9479 0.7415 0.7063 0.6792 \nSDSC-UNet [72] 0.7144 0.9679 0.8457 0.9331 0.8212 \nDSAT-Net [51] 0.7455 0.9701 0.8749 0.8391 0.8436 \nUNetFormer [5] 0.7303 0.9668 0.8421 0.8551 0.8318 \nTCNet (ours) 0.7534 0.9720 0.8419 0.8920 0.8512 \n 1 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9 \n> JSTARS-2023-01671 < \n \nFig. 5. Recognition results of the proposed TCNet and other models on an arbitrarily selected photograph in the Bijie Landslide \nDataset.\n \nto the highest performance indexes of the models compared . \nFrom there, the overall performance of the TCNet  could be \nbetter than the compared models. Depicted in Fig. 6 are the \nexample segmentation maps obtained from the TCNet model  \nand those from the model compared, showing that the building \npixels can be more effectively classified by the TCNet. \nTABLE III \nQUANTITATIVE COMPARISON WITH STATE-OF-THE-ART \nMETHODS ON THE WHU BUILDING DATASET \n \n \nFinally, the comparison is undertaken based on the \nMassachusetts Buildings Dataset. Listed in T able IV are the \nperformance indexes evaluated from the TCNet and those from \nthe compared models (i .e., UNet, DeepLab V3+, MANet, \nBANet, DC -Swin, BuildFormer, ASF -Net, BOMSC -Net, \nCLCFormer, SDSC-UNet, DSAT-Net, and UNetFormer). The \ndata in T able IV depict that the IoU (76.21%), OA (94.85%) \nand Recall (86.82%) obtained from the TCNet are much higher \nthan those from the compared models; whereas, the \nperformance indexes of Precision and F1 score obtained from \nthe TCNet are close to the highest performance indexes o f the \nmodels compared. From there, the overall performance of the \nTCNet is better than the compared models. Depicted in Fig. 7 \nare the example segmentation maps obtained from the TCNet \nmodel and those from the model compared , showing that the \nbuilding pixels can be more effectively classified by the TCNet. \nTABLE IV \nQUANTITATIVE COMPARISON WITH STATE-OF-THE-ART \nMETHODS ON THE MASSACHUSETTS BUILDINGS DATASET \n \n \nD. Ablation Experiments \nTo illustrate the significance of each branch structure and key \nmodule of the proposed TCNet, ablation experiments are \nconducted based on the WHU Building Dataset. \n1) Significance of the CNN branch \nThe CNN branch, in the proposed TCNet, is mainly adopted \nto encode the local context information. To test the significance \nof the CNN branch, the CNN branch is removed from the \nTCNet in this ablation test; and, the test results are listed in \nTable V. Here, the removement of the CNN branch decreases \nthe F1 score and IoU by 0.52% and 0.78%, respectively. Thus, \nthe significance of the CNN branch can be demonstrated. \n2) Significance of the Transformer branch \nThe Transformer branch, in the TCNet, is mainly adopted to \nencode the global context information. To test the significance \nof the Transformer branch, the Transformer branch is removed \nfrom the TCNet in this ablation test; and, the test results are \nlisted in T able V. Here, t he removement of the Transformer \nbranch decreases the F1 score and Io U by 1.81%, and 2.71% , \nrespectively. Thus, the significance of the Transformer branch \nis depicted. \n \nMethod IoU OA Precision Recall F1 \nUNet [14] 0.8551 0.9727 0.9186 0.9252 0.9219 \nDeepLab V3+ [15] 0.8578 0.9737 0.9345 0.9127 0.9235 \nBiSeNetv2 [64] 0.8651 - 0.9322 0.9225 - \nSegNet [36] 0.8612 0.9741 0.9273 0.9235 0.9254 \nBOMSC-Net [69] 0.9015 0.9820 0.9514 0.9450 0.9480 \nMAP-Net [67] 0.9086 - 0.9562 0.9481 0.9521 \nMA-FCN [63] 0.9070 - 0.9520 0.9510 0.9515 \nRSR-Net [64] 0.8832 - 0.9492 0.9263 - \nCLCFormer [50] 0.8420 0.9716 0.9541 0.8733 0.8954 \nSDSC-UNet [72] 0.9104 0.9827 0.9622 0.9521 0.9429 \nDSAT-Net [51] 0.9014 0.9778 0.9612 0.9446 0.9236 \nUNetFormer [5] 0.9018 0.9875 0.9662 0.9306 0.9304 \nTCNet (ours) 0.9116 0.9880 0.9515 0.9555 0.9395 \n 1 \nMethod IoU OA Precision Recall F1 \nUNet [14] 0.6761 - 0.7913 0.8229 0.8068 \nDeepLab V3+ [15] 0.6923 - 0.8473 0.7910 0.8182 \nMANet [65] 0.7076 - 0.8200 0.8377 0.8288 \nBANet [66] 0.7220 - 0.8307 0.8466 0.8386 \nDC-Swin [70] 0.7259 - 0.8307 0.8519 0.8412 \nBuildFormer [68] 0.7574 - 0.8752 0.8490 0.8619 \nASF-Net [71] 0.7420 - - - 0.9460 \nBOMSC-Net [69] 0.7471 0.9471 0.8664 0.8368 0.8513 \nCLCFormer [50] 0.4911 0.8781 0.8787 0.5187 0.6176 \nSDSC-UNet [72] 0.7550 0.9412 0.8856 0.8364 0.8419 \nDSAT-Net [51] 0.7251 0.9239 0.8476 0.8420 0.8055 \nUNetFormer [5] 0.7271 0.9349 0.8945 0.7956 0.8238 \nTCNet (ours) 0.7621 0.9485 0.8517 0.8682 0.8429 \n 1 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10 \n> JSTARS-2023-01671 < \n \nFig. 6. Recognition results of the proposed TCNet  and other models on an arbitrarily selected photograph in the WHU Building \nDataset. \n \n \nFig. 7. Recognition results of the proposed TCNet  and other models on an arbitrarily selected photograph in the Massachusetts \nBuildings Dataset. \n \nTABLE V \nABLATION TEST RESULTS OF EACH BRANCH STRUCTURE AND \nKEY MODULE \n \n \n3) Significance of the Interactive Self-attention (ISa) module \nThe ISa module, in the TCNet, is mainly adopted to encode \nthe information of various scales. To test the significance of the \nISa module, the ISa module is removed from the TCNet in this \nablation test; and, the test results are listed in Table V. Here, the \nremovement of the ISa module decreases the F1 score and IoU \nby 2.69% and 4.26%, respectively. Thus, the significance of the \nISa module is demonstrated. \n4) Significance of the Window Self-attention Gating (WSaG) \nmodule \nThe WSaG module , in the TCNet , is mainly adopted  to \nmaximize the flow of feature information b etween different \nscales. To test the significance of the WSaG module, the WSaG \nmodule is removed from the TCNet in this a blation test; and, \nMethod F1 IoU \nTCNet without CNN branch 0.9343 0.9038 \nTCNet with CNN branch 0.9395 0.9116 \nTCNet without Transformer branch 0.9214 0.8899 \nTCNet with Transformer branch 0.9395 0.9116 \nTCNet without ISa 0.9316 0.8990 \nTCNet with ISa 0.9395 0.9116 \nTCNet without WSaG 0.9311 0.9002 \nTCNet with WSaG 0.9395 0.9116 \n 1 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11 \n> JSTARS-2023-01671 < \n \nthe test results are liste d in Table V. Here, the removement of \nthe WSaG module decreases the F1 score and Io U by 1.24% \nand 1.64%, re spectively. Thus, the significance of the WSaG \nmodule is depicted. \nV. DISCUSSION \nTo further illustrate the significance of the parallel-in-branch \narchitecture adopted in the TCNet, feature visualization results \nare studied; and, comparative studies on the efficiency of the \nmodel are conducted. \n \nA. Two Branches Analysis \nTo study the influences of the CNN branch and Transformer \nbranch on the feature visualization result in the segmentation, \nthese two branches are removed from the TCNet separately, \nsimilar to the ablation experiments presented above, and some \nof the feature visualization results on an arbitrarily selected \nphotograph in the WHU Building Dataset are shown in Fig. 8.  \n \n \nFig. 8. Feature visualization results obtained from an arbitrarily \nselected photograph in the WHU Building Dataset: (a) Original \nphotograph; (b) Recognition result with both local information \nand global context information; (c) Recognition result with \nglobal context information; (d) Recognition result with local \ninformation. \n \nThe feature visualization results, shown in Fig. 8, indicate \nthat accurate recognition of the boundaries of the building could \nbe challenging when the local information is removed (see the \nred circles in Fig. 8(c)), whereas, irrelevant backgrounds such \nas roads might be recognized as buildings when the global \ncontext information is removed (see the red circles in Fig. 8(d)). \nWhen both local information and global context information are \ncombined, accurate recognition of the building boundaries can \nbe realized (see Fig. 8(b)). It can be seen that the local modeling \ncapabilities of the CNN help to recognize small detailed areas \nin the segmentation, while the global modeling capabilities of \nthe Transformer help to establish global dependencies between \nthe building and the background. Thus, the significance of each \nbranch of the TCNet (i.e., CNN branch and Transformer branch) \nis further demonstrated. \n \nB. Model Efficiency Analysis \nIt should be noted that the computational cost of a semantic \nsegmentation model is often assessed by the amount of model \nparameters and that of floating point operations (FLOPs). Table \nVI lists the amount of parameters and FLOPs  of the proposed \nTCNet and those of some existing models, in terms of the UNet \n[14], PSPNet  [21], HRNetV2  [62], DeepLab V3+  [15], \nSegFormer [45], UNetFormer  [5], CLCFormer  [50], SDSC -\nUNet [72], and DSAT -Net [51]. Similarly, these models are \ntrained under an identical operating enviro nment. The data in \nTable VI show that the TCNet does not incur excessive memory \nand computational overhead. For example, the TCNet yields a \nsimilar amount of parameters and FLOPs, in comparison to the \nUNetFormer, DSAT -Ne, and CLCFormer. However, the \nproposed TCNet could improve the segme ntation accuracy \nsignificantly (see Tables II-IV and Fig. 5-7). \nTABLE VI \nCOMPARISON OF MODEL PARAMETERS AND FLOATING POINT \nOPERATIONS(FLOPS) OF THE PROPOSED TCNET AND SOME \nEXISTING MODELS \n \nVI. CONCLUDING REMARKS \nA novel network called TCNet was proposed in this study for \nthe semantic segmentation of remote sensing images . A \nparallel-in-branch architecture of the Transformer and the CNN \nis adopted in the TCNet; as such, both global context and low -\nlevel spatial details can be captured. In addition, an Interactive \nSelf-attention (ISa) module was developed and adopted in the \nTCNet to fuse the multi -level features extracted from the two \nbranches; and, to bridge the semantic gap between regions, a \nWindowed Self -attention Gating (WSaG) -based progressive \nupsampling network was developed and adopted in the TCNet. \nTo demonstrate the effectiveness and versatility of the TCNet, \nexperiments on three public datasets, including the Bijie \nLandslide Dataset, the WHU Building Dataset, and the \nMassachusetts Buildings Dataset, were conducted.  \nThe results of the experiments showed that both foreground \nand background could be precisely extracted  by the trained \nTCNet model, as indicated by the relatively high values of IoU, \nMethod Parameters (M) FLOPs (G) \nUNet [14] 24.891 255.836 \nPSPNet [21] 2.376 3.016 \nHRNetV2 [62] 29.538 45.463 \nDeepLab V3+ [15] 5.813 26.434 \nSegFormer [45] 3.715 66.739 \nUNetFormer [5] 11.70 11.738 \nCLCFormer [50] 54.110 47.560 \nSDSC-UNet [72] 21.320 29.820 \nDSAT-Net [51] 48.371 57.750 \nTCNet (ours) 34.875 91.875 \n 1 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12 \n> JSTARS-2023-01671 < \n \nOverall Accuracy (OA), Precision, Recall, and F1 score . \nFurther, comparisons between the proposed TCNet and some \nstate-of-the-art models were undertaken; and, the results of the \ncomparison showed the superiority of the proposed TCNet. For \nexample, compared to the existing models, the proposed TCNet \nalmost always yields higher performance indexes. Meanwhile, \nthe significance of each branch structure and key module of the \nproposed TCNet was verified through ablation experiments \nbased on the WHU Building Datas et. Finally, the significance \nof the parallel-in-branch architecture adopted and the efficiency \nof the TCNet were discussed. Note that although the proposed \nTCNet was deemed effective in the analyses conducted, the \nfollowing limitations warrant further investigation : the TCNet \nwas only applied to semantic segmentation of building and \nlandslide in r emote sensing images of urban and mountain \nscenes; whereas, other tasks such as road segmentation and plot \nsegmentation have not been tested. \nREFERENCES \n[1] H. Bi, F. Xu, Z. Wei, Y. Xue and Z. Xu, “An active deep learning approach \nfor minimally supervised POLSAR image classification,” IEEE Trans. \nGeosci. Remote Sens.,  vol. 57, no. 11, pp. 9378 -9395, Nov. 2019, doi: \n10.1109/TGRS.2019.2926434. \n[2] Z. Cheng, W. Gong, H. Tang, C. H. Juang, Q. Deng, J. Chen, and X. Ye, \n“UAV photogrammetry-based remote sensing and preliminary assessment \nof the behavior of a landslide in Guizhou, China,” Eng. Geol., vol. 289, no. \n106172, Aug, 2021, doi: 10.1016/j.enggeo.2021.106172. \n[3] T. Ren, W. Gong, L. Gao, F. Zhao, and Z. Cheng, “An interpretation \napproach of ascending–descending SAR data for landslide identification,” \nRemote Sens., vol. 14, no. 5, pp. 1299, Mar. 2022, doi: 10.3390/rs14051299. \n[4] F. Zhao, W. Gong, H. Tang, S. P. Pudasaini, T. Ren, and Z. Cheng, “An \nintegrated approach for risk assessment of land subsidence in Xi'an, China \nusing optical and radar satellite images,” Eng. Geol., vol. 314, no. 106983, \nMar, 2023, doi: 10.1016/j.enggeo.2022.106983. \n[5] L. Wang et al., “ UNetFormer: A UNet -like transformer for efficient \nsemantic segmentation of remote sensing urban scene imagery,” ISPRS J. \nPhotogramm. Remote Sens.,  vol. 190, pp. 196 -214, 2022, doi: \n10.1016/j.isprsjprs.2022.06.008. \n[6] X. He, Y. Zhou, J. Zhao, D. Zhang, R. Yao and Y. Xue, “Swin transformer \nembedding UNet for remote sensing image semantic segmentation,” IEEE \nTrans. Geosci. Remote Sens.,  vol. 60, 2022, doi: \n10.1109/TGRS.2022.3144165. \n[7] W. Wang, Y. Yang, J. Li, Y. Hu, Y. Luo and X. Wang, “Woodland labeling \nin chenzhou China via deep learning approach,” Int. J. Comput. Intell. Syst., \nvol. 13, no. 1, pp. 1393-1403, Sep. 2020, doi: 10.2991/ijcis.d.200910.001. \n[8] P. Dey, S. K. Chaulya and S. Kumar, “Hybrid CNN-LSTM and IoT-based \ncoal mine hazards monitoring and prediction system,” Process Saf. Environ. \nProtection, vol. 152, pp. 249 -263, Aug. 2021, doi: \n10.1016/j.psep.2021.06.005. \n[9] Z. Zhang, G. Vosselman, M. Gerke, D. Tuia and M. Y. Yang, “Change \ndetection between multimodal remote sensing data using Siamese CNN”, \narXiv:1807.09562, 2018, doi: 10.48550/arXiv.1807.09562. \n[10] X. Gao, T. Chen, R. Niu and A. Plaza, “Recognition and mapping of \nlandslide using a fully convolutional DenseNet and influencing factors,” \nIEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,  vol. 14, pp. 7881 -\n7894, 2021, doi: 10.1109/JSTARS.2021.3101203. \n[11] C. Zhang, W. Jiang, Y. Zhang, W. Wang, Q. Zhao and C. Wang, \n“Transformer and CNN hybrid deep neural network for semantic \nsegmentation of very -high-resolution remote sensing imagery,” IEEE \nTrans. Geosci. Remote Sens.,  vol. 60, 2022. doi: \n10.1109/TGRS.2022.3144894. \n[12] J. Long, E. Shelhamer and T. Darrell, “Fully convolutional networks for \nsemantic segmentation,” Proc. IEEE Conf. Comput. Vis. Pattern Recognit. \n(CVPR), pp. 3431-3440, 2015, doi: 10.1109/TPAMI.2016.2572683. \n[13] S. A. Taghanaki, K. Abhishek, J. P. Cohen, J. Cohen -Adad and G. \nHamarneh, “Deep semantic segmentation of natural and medical images: A \nreview,” Artif. Intell. Rev.,  vol. 54, no. 1, pp. 137 -178, 2020, doi: \n10.1007/s10462-020-09854-1. \n[14] O. Ronneberger, P. Fischer and T. Brox, “U -Net: Convolutional networks \nfor biomedical image segmentation,” Proc. Int. Conf. Med. Image Comput. \nComput. Assist. Interv., pp. 234-241, 2015, doi: 10.1007/978-3-319-24574-\n4_28. \n[15] L. C. Chen, Y. Zhu, G. Papandreou, F. Schroff and H. Adam, “Encoder -\ndecoder with atrous separable convolution for semantic image \nsegmentation,” arXiv:1802.02611, 2018, doi: 10.1007/978 -3-030-01234-\n2_49. \n[16] L. C. Chen, G. Papandreou, F. Schroff and H. Adam, “Rethinking atrous \nconvolution for semantic image segmentation,” arXiv:1706.05587, 2017, \ndoi: 10.48550/arXiv.1706.05587. \n[17] P. Song, J. Li, Z. An, H. Fan and L. Fan, “CTMFNet: CNN and transformer \nmultiscale fusion network of remote sensing urban scene imagery,” IEEE \nTrans. Geosci. Remote Sens.,  vol. 61, 2022, doi: \n10.1109/TGRS.2022.3232143. \n[18] L. Ding, H. Tang and L. Bruzzone, “LANet: Local attention embedding to \nimprove the semantic segmentation of remote sensing images,” IEEE Trans. \nGeosci. Remote Sens.,  vol. 59, no. 1, pp. 426 -435, Jan. 2021, doi: \n10.1109/TGRS.2020.2994150. \n[19] J. Fu et al., “Dual attention network for scene segmentation,” in Proc. IEEE \nConf. Comput. Vis. Pattern Recognit.,  2019, pp. 3146 -3154, doi: \n10.1109/TNNLS.2020.3006524. \n[20] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei and W. Liu, “CCNet: \nCriss-cross attention for semantic segmentation,” Proc. Int. Conf. Comput. \nVis., pp. 603-612, Oct./Nov. 2019, doi: 10.1109/ICCV.2019.00069. \n[21] H. Zhao, J. Shi, X. Qi, X. Wang and J. Jia, “Pyramid scene parsing network,” \nProc. Conf. Comput. Vis. Pattern Recognit., pp. 6230-6239, Jul. 2017, doi: \n10.1109/CVPR.2017.660. \n[22] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, et al., \n“Understanding convolution for semantic segmentation,” 2018 IEEE \nWinter Conference on Applications of Computer Vision (WACV), pp. 1451-\n1460, 2018, doi: 10.1109/WACV.2018.00163. \n[23] L. Mou, Y. Hua and X. X. Zhu, “Relation matters: Relational context-aware \nfully convolutional network for semantic segmentation of high -resolution \naerial images,” IEEE Trans. Geosci. Remote. Sens.,  vol. 58, no. 11, pp. \n7557-7569, Dec. 2020, doi: 10.1109/TGRS.2020.2979552. \n[24] A. Dosovitskiy et al., “An image is worth 16  16 words: transformers for \nimage recognition at scale,\" Proc. 9th Int. Conf. Learn. Represent.,  pp. 1-\n5, 2021, doi: 10.48550/arXiv.2010.11929. \n[25] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov and S. \nZagoruyko, “End-to-end object detection with transformers,” Proc. ECCV, \nvol. 12346, pp. 213-229, Aug. 2020, doi: 10.1007/978-3-030-58452-8_13. \n[26] C. Chen, Q. Fan and R. Panda, “CrossViT: Cross -attention multi -scale \nvision transformer for image classification,” CoRR, vol. abs/2103.14899, \npp. 1-12, Mar. 2021, doi: 10.1109/ICCV48922.2021.00041. \n[27] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using \nshifted windows,” CoRR, vol. abs/2103.14030, pp. 1 -14, Mar. 2021, doi: \n10.1109/ICCV48922.2021.00986. \n[28] R. Azad, M. Heidari, Y. Wu and D. Merhof, “Contextual attention network: \nTransformer meets U -net,” arXiv:2203.01932, 2022, doi: 10.1007/978 -3-\n031-21014-3_39. \n[29] Z. Li, W. Yang, S. Peng and F. Liu, “A survey of convolutional neural \nnetworks: Analysis applications and prospects,” CoRR, vol. \nabs/2004.02806, 2020, doi: 10.1109/TNNLS.2021.3084827. \n[30] G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger, “Densely \nconnected convolutional networks,” Proc. IEEE Conf. Comput. Vis. \nPattern Recognit. (CVPR),  pp. 4700 -4708, Jul. 2017, doi: \n10.48550/arXiv.1608.06993. \n[31] R. Kemker , C. Salvaggio and C. Kanan, “Algorithms for semantic \nsegmentation of multispectral remote sensing imagery using deep learning,” \nISPRS J. Photogramm. Remote Sens., vol. 145, pp. 60-77, Nov. 2018, doi: \n10.1016/j.isprsjprs.2018.04.014. \n[32] H. Zhang, Y. Liao, H. Yang, G. Yang and L. Zhang, “A local–global dual-\nstream network for building extraction from very -high-resolution remote \nsensing images,” IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 3, pp. \n1269-1283, Mar. 2020, doi: 10.1109/TNNLS.2020.3041646. \n[33] H. Hosseinpour, F. Samadzadegan and F. D. Javan, “CMGFNet: A deep \ncross-modal gated fusion network for building extraction from very high -\nresolution remote sensing images,” ISPRS J. Photogramm. Remote Sens.,  \nvol. 184, pp. 96-115, Feb. 2022, doi: 10.1016/j.isprsjprs.2021.12.007. \n[34] D. Eigen and R. Fergus, “Predicting depth surface normals and semantic \nlabels with a common multi-scale convolutional architecture,” Proc. IEEE \nInt. Conf. Comput. Vis., pp. 2650-2658, 2015, doi: 10.1109/ICCV.2015.304. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n13 \n> JSTARS-2023-01671 < \n \n[35] Y. Guo, Y. Liu, T. Georgiou and M. S. Lew, “A review of semantic \nsegmentation using deep neural networks,” Int. J. Multimedia Inf. Retrieval, \nvol. 7, no. 2, pp. 87-93, 2018, doi: 10.1007/s13735-017-0141-z. \n[36] V. Badrinarayanan, A. Kendall and R. Cipolla, “SegNet: A deep \nconvolutional encoder –decoder architecture for image segmentation,” \nIEEE Trans. Pattern Anal. Mach. Intell.,  vol. 39, no. 12, pp. 2481 -2495, \nDec. 2017, doi: 10.1109/TPAMI.2016.2644615. \n[37] H. Noh, S. Hong and B. Han, “Learning deconvolution network for \nsemantic segmentation,” Proc. IEEE Int. Conf. Comput. Vis.,  pp. 1520 -\n1528, 2015, doi: 10.1109/ICCV.2015.178. \n[38] L. C. Chen, G. Papandreou, I. Kokkinos, K. Murphy and A. L. Yuille, \n“DeepLab: Semantic image segmentation with deep convolutional nets \natrous convolution and fully connected CRFs,” IEEE Trans. Pattern Anal. \nMach. Intell.,  vol. 40, no. 4, pp. 834 -848, Apr. 2018, doi: \n10.1109/TPAMI.2017.2699184. \n[39] R. Li, S. Zheng, C. Duan, J. Su and C. Zhang, “Multistage attention ResU-\nNet for semantic segmentation of fine -resolution remote sensing images,” \nIEEE Geosci. Remote Sens. Lett.,  vol. 19, pp. 1 -5, 2022, doi: \n10.1109/LGRS.2021.3063381. \n[40] R. Li, C. Duan, S. Zheng, C. Zhang and P. M. Atkinson, “MACU -Net for \nsemantic segmentation of fine -resolution remotely sensed images,” IEEE \nGeosci. Remote Sens. Lett.,  vol. 19, pp. 1 -5, 2022, doi: \n10.1109/LGRS.2021.3052886. \n[41] R. Li, S. Zheng, C. Zhang, C. Duan, L. Wang and P. M. Atkinson, \n“ABCNet: Attentive bilateral contextual network for efficient semantic \nsegmentation of fine -resolution remotely sensed imagery,”  ISPRS J. \nPhotogramm. Remote Sens.,  vol. 181, pp. 84 -98, Nov. 2021, doi: \n10.1016/j.isprsjprs.2021.09.005. \n[42] M. Y. Yang, S. Kumaar, Y. Lyu and F. Nex, “Real -time semantic \nsegmentation with context aggregation network,” ISPRS J. Photogramm. \nRemote Sens., vol. 178, pp. 124-134, Aug. 2021. \n[43] A. Vaswani et al., “Attention is all you need,” Proc. Adv. Neural Inf. \nProcess. Syst., pp. 5998-6008, 2017. \n[44] R. Strudel, R. Garcia, I. Laptev and C. Schmid, “Segmenter: Transformer \nfor semantic segmentation,” Proc. IEEE/CVF Int. Conf. Comput. Vis. \n(ICCV), pp. 7262-7272, Oct. 2021, doi: 10.1109/ICCV48922.2021.00717. \n[45] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez and P. Luo, \n“SegFormer: Simple and efficient design for semantic segmentation with \ntransformers,” Proc. Adv. Neural Inf. Process. Syst.,  vol. 34, pp. 12077 -\n12090, 2021. \n[46] S. Xiao, K. Shang, K. Lin, Q. Wu, H. Gu, and Z. Zhang, “Pavement crack \ndetection with hybrid-window attentive vision transformers,”  Int. J. Appl. \nEarth Observ. Geoinformation,  vol. 116, Dec. 2022, doi: \n10.1016/j.jag.2022.103172. \n[47] H. Cao et al., “Swin -Unet: Unet-like pure transformer for medical image \nsegmentation,” arXiv:2105.05537, 2021, doi: 10.1007/978 -3-031-25066-\n8_9. \n[48] J. Chen et al., “TransUNet: Transformers make strong encoders for medical \nimage segmentation,” CoRR, vol. abs/2102.04306, pp. 1-13, Feb. 2021, doi: \n10.48550/arXiv.2102.04306. \n[49] Y. Zhang, H. Liu and Q. Hu, “TransFuse : Fusing transformers and CNNs \nfor medical image segmentation”, CoRR, vol. abs/2102.08005, pp. 1 -11, \nFeb. 2021, doi: 10.1007/978-3-030-87193-2_2. \n[50] J. Long, M. Li and X. Wang, “Integrating Spatial Details With Long-Range \nContexts for Semantic Segmentation of Very High -Resolution Remote -\nSensing Images,” in IEEE Geoscience and Remote Sensing Letters, vol. 20, \npp. 1-5, 2023, Art no. 2501605, doi: 10.1109/LGRS.2023.3262586. \n[51] R. Zhang, Z. Wan, Q. Zhang, and G. Zhang, “DSAT -Net: Dual Spatial \nAttention Transformer for Building Extraction From Aerial Images,” in \nIEEE Geoscience and Remote Sensing Letters, vol. 20, pp. 1 -5, 2023, Art \nno. 6008405, doi: 10.1109/LGRS.2023.3304377. \n[52] K. He et al., “Deep residual learning for image recognition,” Proc. Conf. \nComput. Vis. Pattern Recognit., pp. 770-778, 2016. \n[53] H. Touvron et al., “Training data -efficient image transformers & \ndistillation through attention,” Proc. Int. Conf. Mach. Learn.,  pp. 10347-\n10357, 2020, doi: 10.48550/arXiv.2012.12877. \n[54] R. Zhang, P. Lai, X. Wan, D. J. Fan, F. Gao, X. J. Wu, and G. Li, “Lesion-\nAware Dynamic Kernel for Polyp Segmentation,” Proc. Int. Conf. Med. \nImage Comput. Comput. -Assist. Intervent.,  pp. 99 -109, Sep. 2022, doi: \n10.1007/978-3-031-16437-8_10. \n[55] D. P. Fan et al., “PraNet: Parallel reverse attention network for polyp \nsegmentation,” Proc. Med. Image. Comput. Comput. Assist. Interv.,  pp. \n263-273, 2020, doi: 10.1007/978-3-030-59725-2_26. \n[56] O. Oktay et al., “Attention U-net: Learning where to look for the pancreas,” \nin arXiv:1804.03999, 2018, doi: 10.1007/s10278-022-00629-4. \n[57] S. Zheng et al., “Rethinking semantic segmentation from a sequence -to-\nsequence perspective with transformers,” Proc. IEEE/CVF Conf. Comput. \nVis. Pattern Recognit. (CVPR),  pp. 6881 -6890, Jun. 2021, doi: \n10.1109/CVPR46437.2021.00681. \n[58] S. Ji, D. Yu, C. Shen, W. Li and Q. Xu, “Landslide detection from an open \nsatellite imagery and digital elevation model dataset using attention boosted \nconvolutional neural networks,” Landslides, vol. 17, no. 6, pp. 1337-1352, \nJun. 2020, doi: 10.1007/s10346-020-01353-2. \n[59] S. Ji, S. Wei and M. Lu, “Fully convolutional networks for multisource \nbuilding extraction from an open aerial and satellite imagery data set,” \nIEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 574 -586, Jan. 2019, \ndoi: 10.1109/TGRS.2018.2858817. \n[60] G. Máttyus, W. Luo and R. Urtasun, “Deeproadmapper: Extracting road \ntopology from aerial images,” Proc. IEEE Int. Conf. Computer Vision, pp. \n3438-3446, 2017, doi: 10.1109/ICCV.2017.372. \n[61] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan and M. Jagersand, \n“BASNet: Boundary -aware salient object detection,” Proc. IEEE Conf. \nComput. Vis. Pattern Recognit.,  pp. 7479 -7489, 2019, doi: \n10.1109/CVPR.2019.00766. \n[62] K. Sun et al., “High -resolution representations for labeling pixels and \nregions,” in arXiv:1904.04514, 2019, doi: 10.48550/arXiv.1904.04514. \n[63] S. Wei, S. Ji and M. Lu, “Toward automatic building footprint delineation \nfrom aerial images using CNN and regularization,” IEEE Trans. Geosci. \nRemote Sens.,  vol. 58, no. 3, pp. 2178 -2189, Mar. 2019, doi: \n10.1109/TGRS.2019.2954461. \n[64] H. Huang, Y. Chen and R. Wang, “A lightweight network for building \nextraction from remote sensing images,” IEEE Trans. Geosci. Remote Sens., \nvol. 60, pp. 1-12, 2022, doi: 10.1109/TGRS.2021.3131331. \n[65] R. Li et al., “Multiattention network for semantic segmentation of fine -\nresolution remote sensing images,” IEEE Trans. Geosci. Remote Sens., vol. \n60, pp. 1-13, 2021, doi: 10.1109/TGRS.2021.3093977. \n[66] L. Wang, R. Li, D. Wang, C. Duan, T. Wang and X. Meng, “Transformer \nmeets convolution: A bilateral awareness network for semantic \nsegmentation of very fine resolution urban scene images,” Remote Sens., \nvol. 13, no. 16, pp. 3065, Aug. 2021, doi: 10.3390/rs13163065. \n[67] Q. Zhu, C. Liao, H. Hu, X. Mei and H. Li, “MAP -net: Multiple attending \npath neural network for building footprint extraction from remote sensed \nimagery,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 7, pp. 6169-6181, \nJul. 2021, doi: 10.1109/TGRS.2020.3026051. \n[68] L. Wang, S. Fang, X. Meng and R. Li, “Building extraction with vision \ntransformer,” IEEE Trans. Geosci. Remote Sens.,  vol. 60, pp. 1 -11, 2022, \ndoi: 10.1109/TGRS.2022.3186634. \n[69] Y. Zhou et al., “BOMSC -Net: Boundary optimization and multi -scale \ncontext awareness based building extraction from high -resolution remote \nsensing imagery,” IEEE Trans. Geosci. Remote Sens.,  vol. 60, 2022, doi: \n10.1109/TGRS.2022.3152575. \n[70] L. Wang, R. Li, C. Duan, C. Zhang, X. Meng and S. Fang, “A novel \ntransformer based semantic segmentation scheme for fine -resolution \nremote sensing images,” IEEE Geosci. Remote Sens. Lett., vol. 19, pp. 1-5, \n2022, doi: 10.1109/LGRS.2022.3143368. \n[71] J. Chen, Y. Jiang, L. Luo and W. Gong, “ASF -Net: Adaptive screening \nfeature network for building footprint extraction from remote -sensing \nimages,” IEEE Trans. Geosci. Remote Sens.,  vol. 60, 2022, doi: \n10.1109/TGRS.2022.3165204. \n[72] R. Zhang, Q. Zhang and G. Zhang, “SDSC -UNet: Dual Skip Connection \nViT-Based U-Shaped Model for Building Extraction,” in IEEE Geoscience \nand Remote Sensing Letters, vol. 20, pp. 1 -5, 2023, Art no. 6005005, doi: \n10.1109/LGRS.2023.3270303. \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n14 \n> JSTARS-2023-01671 < \n \n \nXuyang Xiang  received the B.S. degree from the \nCollege of Earth Sciences , Guilin University of \nTechnology, Guilin, China, in 2021, where he is \ncurrently pursuing the M.S. degree with the Faculty \nof Engineering, China University of Geosciences, \nWuhan, China. \nHis research interests include computer vision \nand remote sensing image semantic segmentation. \n \n \n \n \n \n \n \nWenping Gong  received the B.S. degree from \nTongji University, Shanghai, China, in 2011, and the \nPh.D. degree from Clemson University, Clemson, \nSC, USA, in 2014. \nHe is currently a full Professor with the Faculty \nof Engineering, China University of Geosciences, \nWuhan, China, and services as the editor-in-chief of \nEngineering Geology (Elsevier). \nHis research interests include engineering \ngeology, geohazards, risk and reliability, uncertainty \nmodelling, and remote sensing.  \n \n \n \n \nShuailong Li  received the B.S. degree from the \nSchool of Civil Engineering, Zhengzhou University, \nZhengzhou, China, in 2021, where he is currently \npursuing the M.S. degree with the Faculty of \nEngineering, China University of Geosciences, \nWuhan, China. \nHis research interests include computer vision \nand remote sensing image semantic segmentation. \n \n \n \n \n \n \n \nJun Chen (Member, IEEE) received the B.S. degree \nin electronic and information engineering and the \nM.S. degree in communication and information \nsystem from the China University of Geosciences, \nWuhan, China, in 2002 and 2004, respectively, and \nthe Ph.D. degree in communication and information \nsystem from the Huazhong University of \nTechnology, Wuhan, in 2014. \nFrom 2004 to 2008, she was an Assistant \nProfessor with the China University of Geosciences, \nwhere she is currently an Associate Professor with \nthe School of Automation. Her research interests \ninclude computer vision, pattern recognition, geoscience, and remote sensing. \n \n \nTianhe Ren  received the B.E. degree from the \nFaculty of Engineering, China University of \nGeosciences, Wuhan, China, in 2019, the M.S. \ndegree from the Faculty of Engineering, China \nUniversity of Geosciences, Wuhan, China, in 2022, \nwhere he is currently pursuing the Ph.D. degree with \nthe Faculty of Engineering, China University of \nGeosciences, Wuhan, China. \nHis research interests include remote sensing \nimage processing and geoscience. \n \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349625\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}