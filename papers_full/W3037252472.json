{
  "title": "FinBERT: A Pretrained Language Model for Financial Communications",
  "url": "https://openalex.org/W3037252472",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5005421447",
      "name": "Yi Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5012431214",
      "name": "Mark Christopher Siy Uy",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063822345",
      "name": "Allen Huang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2787998955",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3124766520",
    "https://openalex.org/W3122944446",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3122563224",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2937845937"
  ],
  "abstract": "Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at https://github.com/yya518/FinBERT. We hope this will be useful for practitioners and researchers working on financial NLP tasks.",
  "full_text": "arXiv:2006.08097v2  [cs.CL]  9 Jul 2020\nFinBERT : A Pretrained Language Model for Financial Communications\nYi Y ang Mark Christopher Siy UY Allen Huang\nSchool of Business and Management, Hong Kong University of S cience and T echnology\n{imyiyang,acahuang}@ust.hk, mcsuy@connect.ust.hk\nAbstract\nContextual pretrained language models, such\nas BER T (\nDevlin et al. , 2019), have made\nsigniﬁcant breakthrough in various NLP tasks\nby training on large scale of unlabeled text\nresources. Financial sector also accumulates\nlarge amount of ﬁnancial communication\ntext. However, there is no pretrained ﬁnance\nspeciﬁc language models available. In this\nwork, we address the need by pretraining\na ﬁnancial domain speciﬁc BER T models,\nFinBER T , using a large scale of ﬁnancial\ncommunication corpora. Experiments on\nthree ﬁnancial sentiment classiﬁcation\ntasks conﬁrm the advantage of FinBER T\nover generic domain BER T model. The\ncode and pretrained models are available at\nhttps://github.com/yya518/FinBERT.\nW e hope this will be useful for practitioners\nand researchers working on ﬁnancial NLP\ntasks.\n1 Introduction\nThe growing maturity of NLP techniques and re-\nsources is drastically changing the landscape of ﬁ-\nnanical domain. Capital market practitioners and\nresearchers have keen interests in using NLP tech-\nniques to monitor market sentiment in real time\nfrom online news articles or social media posts,\nsince sentiment can be used as a directional sig-\nnal for trading purposes. Intuitively , if there is\npositive information about a particular company ,\nwe expect that company’s stock price to increase,\nand vice versa. For example, Bloomberg, the ﬁ-\nnancial media company , reports that trading sen-\ntiment portfolios outperform the benchmark in-\ndex signiﬁcantly (\nCui et al. , 2016). Prior ﬁnancial\neconomics research also reports that news article\nand social media sentiment could be used to pre-\ndict market return and ﬁrm performance (\nT etlock,\n2007; T etlock et al. , 2008).\nRecently , unsupervised pre-training of language\nmodels on large corpora has signiﬁcantly im-\nproved the performance of many NLP tasks. The\nlanguage models are pretained on generic corpora\nsuch as Wikipedia. However, sentiment analysis\nis a strongly domain dependent task. Financial\nsector has accumulated large scale of text of ﬁ-\nnancial and business communications. Therefore,\nleveraging the success of unsupervised pretrain-\ning and large amount of ﬁnancial text could poten-\ntially beneﬁt wide range of ﬁnancial applications.\nT o ﬁll the gap, we pretrain FinBERT, a ﬁnance\ndomain speciﬁc BERT model on a large ﬁnancial\ncommunication corpora of 4.9 billion tokens, in-\ncluding corporate reports, earnings conference call\ntranscripts and analyst reports. W e document the\nﬁnancial corpora and the FinBERT pretraining de-\ntails. Experiments on three ﬁnancial sentiment\nclassiﬁcation tasks shows that FinBERT outper-\nforms the generic BERT models. Our contribu-\ntion is straightforward: we compile a large scale\nof text corpora that are the most representative in\nﬁnancial and business communications. W e pre-\ntrain and release FinBERT , a new resource demon-\nstrated to improve performance on ﬁnancial senti-\nment analysis.\n2 Related W ork\nRecently , unsupervised pre-training of lan-\nguage models on large corpora, such as BERT\n(\nDevlin et al. , 2019), ELMo ( Peters et al. , 2018),\nULM-Fit ( Howard and Ruder , 2018), XLNet,\nand GPT ( Radford et al. , 2019) has signiﬁcantly\nimproved performance on many natural language\nprocessing tasks, from sentence classiﬁcation to\nquestion answering. Unlike traditional word em-\nbedding (\nMikolov et al. , 2013; Pennington et al. ,\n2014) where word is represented as a single vector\nrepresentation, these language model returns\ncontextualized embeddings for each word token\nwhich can be fed into downstream tasks.\nThe released language models are trained on\ngeneral domain corpora such as news articles and\nWikipedia. Even though it is easy to ﬁne tune\nthe language model using downstream task, it has\nbeen shown that pre-training a language model\nusing large-scale domain corpora can further im-\nprove the task performance than ﬁne-tuning the\ngeneric language model. T o this end, several\ndomain-speciﬁc BERT models are trained and re-\nleased. BioBERT (\nLee et al. , 2019) pretrains a\nbiomedical domain-speciﬁc language representa-\ntion model using large-scale biomedical corpora.\nSimilarly , ClinicalBERT (\nHuang et al. , 2019) ap-\nplies BERT model to clinical notes for hospital\nreadmission prediction task, and (\nAlsentzer et al. ,\n2019) applies BERT on clinical notes and dis-\ncharge summaries. SciBERT ( Beltagy et al. ,\n2019) trains a scientiﬁc domain-speciﬁc BERT\nmodel using a large multi-domain corpus of sci-\nentiﬁc publications to improve performance on\ndownstream scientiﬁc NLP tasks. W e are the ﬁrst\nto pre-train and release a ﬁnance domain speciﬁc\nBERT model.\n3 Financial Corpora\nW e compile a large ﬁnancial domain corpora that\nare most representative in ﬁnance and business\ncommunications.\nCorporate Reports 10-K & 10-Q The most im-\nportant text data in ﬁnance and business commu-\nnication is corporate report. In the United States,\nthe Securities Exchange Commission (SEC) man-\ndates all publicly traded companies to ﬁle annual\nreports, known as Form 10-K, and quarterly re-\nports, known as Form 10-Q. This document pro-\nvides a comprehensive overview of the company’s\nbusiness and ﬁnancial condition. Laws and regula-\ntions prohibit companies from making materially\nfalse or misleading statements in the 10-Ks. The\nForm 10-Ks and 10-Qs are publicly available and\ncan be accesses from SEC website.\n1\nW e obtain 60,490 Form 10-Ks and 142,622\nForm 10-Qs of Russell 3000 ﬁrms during 1994 and\n2019 from SEC website. W e only include sections\nthat are textual components, such as Item 1 (Busi-\nness) in 10-Ks, Item 1A (Risk Factors) in both 10-\nKs and 10-Qs and Item 7 (Managements Discus-\nsion and Analysis) in 10-Ks.\n1 http://www .sec.gov/edgar.shtml\nEarnings Call T ranscripts Earnings calls are\nquarterly conference calls that company execu-\ntives hold with investors and analysts to discuss\nﬁrm overall performance. During an earnings call,\nexecutives such as CEOs and CFOs read forward-\nlooking statements and provide their information\nand interpretation of their ﬁrms performance dur-\ning the quarter. Analysts also have the opportunity\nto request managers to clarify information. Insti-\ntutional and individual investors listen to the earn-\nings call and spot the tones of executives that por-\ntend good or bad news for the company . W e ob-\ntain 136,578 earnings conference call transcripts\nof 7,740 public ﬁrms between 2004 and 2019.\nThe earnings call transcripts are obtained from the\nwebsite Seeking Alpha\n2.\nAnalyst Reports Analyst reports are another use-\nful source of information for institutional and in-\ndividual investors (\nsri International , 1987). An\nanalyst report typically provides several quantita-\ntive summary measures, including a stock recom-\nmendation, an earnings forecast, and sometimes\na target price. It also provides a detailed, mostly\ntextual analysis of the company . Institutional in-\nvestors spend millions of dollars annually to pur-\nchase the full content of analyst reports to read the\nwritten textual analysis. W e obtain analyst reports\nin the Investext database issued for S&P ﬁrms dur-\ning the 1995-2008 period, which yields a set of\n488,494 reports.\nOverall Corpora Statistics The total size of all 4\ncorpora is approximately 4.9 billion tokens. W e\npresent the pretraining ﬁnancial corpora statistics\nin T able\n1. As a comparison, BERT’s pre-training\ncorpora consists of two textual corpora with a total\nof 3.3 billion tokens.\nCorpus # of tokens\nCorporate Reports 10-K & 10-Q 2.5B\nEarnings Call Transcripts 1.3B\nAnalyst Reports 1.1B\nT able 1: Size of pretraining ﬁnancial corpora.\n4 FinBERT T raining\nV ocabularyW e construct FinV ocab, a new W ord-\nPiece vocabulary on our ﬁnancial corpora using\nthe SentencePiece library . W e produce both cased\nand uncased versions of FinV ocab, with sizes of\n2 https://seekingalpha.com/\n28,573 and 30,873 tokens respectively . This is\nvery similar to the 28,996 and 30,522 token sizes\nof the original BERT cased and uncased BaseV o-\ncab. The resulting overlap between between the\noriginal BERT BaseV ocab, and FinV ocab is 41%\nfor both the cased and uncased versions.\nFinBERT -V ariantsW e use the original BERT\ncode\n3 to train FinBERT on our ﬁnancial corpora\nwith the same conﬁguration as BERT -Base. Fol-\nlowing the original BERT training, we set a max-\nimum sentence length of 128 tokens, and train the\nmodel until the training loss starts to converge.\nW e then continue training the model allowing sen-\ntence lengths up to 512 tokens. In particular, we\ntrain four different versions of FinBERT: cased or\nuncased; BaseV ocab or FinV ocab.\nFinBERT -BaseV ocab, uncased/cased:Model\nis initialized from the original BERT -Base un-\ncased/cased model, and is further pretrained on the\nﬁnancial corpora for 250K iterations at a smaller\nlearning rate of 2e− 5, which is recommended by\nBERT code.\nFinBERT -FinV ocab, uncased/cased: Model\nis trained from scratch using a new uncased/cased\nﬁnancial vocabulary FinV ocab for 1M iterations.\nT raining The entire training is done using a\nNVIDIA DGX-1 machine. The server has 4 T esla\nP100 GPUs, providing a total of 128 GB of GPU\nmemory . This machine enables us to train the\nBERT models using a batch size of 128. W e uti-\nlize Horovord framework (\nSergeev and Del Balso ,\n2018) for multi-GPU training. Overall, the total\ntime taken to perform pretraining for one model\nis approximately 2 days. With the release of\nFinBERT , we hope ﬁnancial practitioners and re-\nsearchers can beneﬁt from FinBERT model with-\nout the necessity of the signiﬁcant computational\nresources required to train the model.\n5 Financial Sentiment Experiments\nGiven the importance of sentiment analysis in ﬁ-\nnancial NLP tasks, we conduct experiments on ﬁ-\nnancial sentiment classiﬁcation datasets.\n5.1 Dataset\nFinancial Phrase Bank is a public dataset\nfor ﬁnancial sentiment classiﬁcation (\nMalo et al. ,\n2014). The dataset contains 4,840 sentences se-\nlected from ﬁnancial news. The dataset is manu-\nally labeled by 16 researchers with adequate back-\n3 https://github.com/google-research/bert\nground knowledge on ﬁnancial markets. The sen-\ntiment label is either positive, neutral or negative.\nAnalystT one Dataset is a dataset to gauge\nthe opinions in analyst reports, which is com-\nmonly used in Accounting and Finance literature\n(\nHuang et al. , 2014). The dataset contains ran-\ndomly selected 10,000 sentences from analyst re-\nports in the Investext database. Each sentence is\nmanually annotated into one of three categories:\npositive, negative and neutral. This classiﬁcation\nyields a total of 3,580 positive, 1,830 negative, and\n4,590 neutral sentences in the dataset.\nFiQA Dataset is an open challenge dataset for ﬁ-\nnancial sentiment analysis, containing 1,111 text\nsentences\n4. Given an English text sentence in the\nﬁnancial domain (microblog message, news state-\nment), the task of this challenge is to predict the\nassociated numeric sentiment score, ranged from -\n1 to 1. W e convert the original regression task into\na binary classiﬁcation task for consistent compar-\nison with the above two datasets.\nW e randomly split each dataset into 90% train-\ning and 10% testing 10 times and report the aver-\nage. Since all dataset are used for sentiment clas-\nsiﬁcation, we report the accuracy metrics in the\nexperiments.\n5.2 Fine-tune Strategy\nW e follow the same ﬁne-tune architecture and op-\ntimization choices used in (\nDevlin et al. , 2019).\nW e use a simple linear layer, as our classiﬁcation\nlayer, with a softmax activation function. W e also\nuse cross-entropy loss as the loss function. Note\nthat an alternative is to feed the contextualized\nword embeddings of each token into a deep ar-\nchitectures, such as Bi-LSTM, atop frozen BERT\nembeddings. W e choose not to use this strategy as\nit has shown to perform signiﬁcantly worse than\nﬁne-tune BERT model (\nBeltagy et al. , 2019).\n5.3 Experiment Results\nW e compare FinBERT with original BERT -Base\nmodel (\nDevlin et al. , 2019), and we evaluate both\ncased and uncased versions of this model. The\nmain results of ﬁnancial sentiment analysis tasks\nare present in T able\n2.\nFinBERT vs. BERT The results show substantial\nimprovement of FinBERT models over the generic\nBERT models. On PhraseBank dataset, the best\nmodel uncased FinBERT -FinV ocab achieves the\n4 https://sites.google.com/view/fiqa/home\nBERT FinBERT -BaseV ocab FinBERT -FinV ocab\ncased uncased cased uncased cased uncased\nPhraseBank 0.755 0.835 0.856 0.870 0.864 0.872\nFiQA 0.653 0.730 0.767 0.796 0.814 0.844\nAnalystT one 0.840 0.850 0.872 0.880 0.876 0.887\nT able 2: Performance of different BER T models on three ﬁnanc ial sentiment analysis tasks.\n10-Ks/10-Qs Earnings Call Analyst Reports All\nPhraseBank 0.835 0.843 0.845 0.856\nBaseV ocab FiQA 0.707 0.731 0.744 0.767\nAnalystT one 0.845 0.862 0.871 0.872\nPhraseBank 0.847 0.860 0.861 0.864\nFinV ocab FiQA 0.766 0.778 0.796 0.814\nAnalystT one 0.858 0.870 0.872 0.876\nT able 3: Performance of pretraining on different ﬁnancial c orpus.\naccuracy of 0.872, a 4.4% improvement over un-\ncased BERT model and 15.4% improvement over\ncased BERT model. On FiQA dataset, the best\nmodel uncased FinBERT -FinV ocab achieves the\naccuracy of 0.844, a 15.6% improvement over\nuncased BERT model and a 29.2% improvement\nover cased BERT model. Lastly , on the Analyst-\nT one dataset, the best model uncased FinBERT -\nFinV ocab improves the uncased and cased BERT\nmodel by 4.3% and 5.5% respectively . Overall\nspeaking, pretraining on ﬁnancial corpora, as ex-\npected, is effective and enhances the downstream\nﬁnancial sentiment classiﬁcation tasks. In ﬁnan-\ncial markets where capturing the accurate senti-\nment signal is of utmost importance, we believe\nthe overall FinBERT improvement demonstrates\nits practical utility .\nFinV ocab vs. BaseV ocab W e assess the impor-\ntance of an in-domain ﬁnancial vocabulary by pre-\ntraining different FinBERT models using BaseV o-\ncab and FinV ocab. For both uncased and cased\nmodel, we see that FinBERT -FinV ocab outper-\nforms its BaseV ocab counterpart. However, the\nperformance improvement is quite marginal on\nPhraseBank and AnalystT one task. Only do we\nsee substantial improvement on FiQA task (0.844\nvs. 0.796). Given the magnitude of improvement,\nwe suspect that while an in-domain vocabulary is\nhelpful, FinBERT beneﬁts most from the ﬁnancial\ncommunication corpora pretraining.\nCased vs. Uncased W e follow (\nDevlin et al. ,\n2019) in using both the cased model and the un-\ncased model for all tasks. Experiments result sug-\ngest that uncased models perform better than cased\nmodels in all tasks. This result is consistent with\nprior work of Scientiﬁc domain and Biomedical\ndomain BERT models.\nCorpus Contribution W e also train different Fin-\nBERT models on three ﬁnancial corpus separately .\nThe performance of different FinBERT models\n(cased version) on different tasks are present in\nT able\n3. It shows that FinBERT trained on all\ncorpora achieves the overall best performance in-\ndicating that combining additional ﬁnancial com-\nmunication corpus could improve the language\nmodel quality . Among three datasets, Analyst\nReports dataset appears to perform well in three\ndifferent tasks, even though it only has 1.1 bil-\nlion word tokens. Prior research ﬁnds that cor-\nporate report such as 10-Ks and 10-Qs contains\nredundant content, and that a substantial amount\nof textual volume contained in 10-K reports is at-\ntributable to managerial discretion in how ﬁrms\nrespond to mandatory disclosure requirements\n(\nCazier and Pfeiffer , 2016). Does it suggest that\nAnalyst Reports data contains more information\ncontent than corporate reports and earnings call\ntranscripts? W e leave it for future research.\n6 Conclusion\nIn this work, we pre-train a ﬁnancial-task oriented\nBERT model, FinBERT . The FinBERT model\nis trained on a large ﬁnancial corpora that are\nrepresentative of English ﬁnancial communica-\ntions. W e show that FinBERT outperforms generic\nBERT models on three ﬁnancial sentiment classi-\nﬁcation tasks. With the release of FinBERT , we\nhope practitioners and researchers can utilize Fin-\nBERT for a wider range of applications where the\nprediction target goes beyond sentiment, such as\nﬁnancial-related outcomes including stock returns,\nstock volatilities, corporate fraud, etc.\nAcknowledgments\nThis work was supported by Theme-based Re-\nsearch Scheme (No. T31-604/18-N) from Re-\nsearch Grants Council in Hong Kong.\nReferences\nEmily Alsentzer, John Murphy, William Boag, W ei-\nHung W eng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical bert embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing W orkshop ,\npages 72–78.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\nert: Pretrained language model for scientiﬁc text. In\nProceedings of EMNLP .\nRichard A Cazier and Ray J Pfeiffer. 2016. Why are\n10-k ﬁlings so long? Accounting Horizons , 30(1):1–\n21.\nX Cui, D Lam, and A V erma. 2016. Embedded\nvalue in bloomberg news and social sentiment data.\nBloomberg LP .\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL , pages 4171–4186.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings ACL , pages 328–339.\nAllen H Huang, Amy Y Zang, and Rong Zheng. 2014.\nEvidence on the information content of text in an-\nalyst reports. The Accounting Review , 89(6):2151–\n2180.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and pre-\ndicting hospital readmission. arXiv:1904.05342 .\nsri International. 1987. Investor information needs and\nthe annual report . Financial Executives Research\nFoundation.\nJinhyuk Lee, W onjin Y oon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBER T: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinformatics.\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki W al-\nlenius, and Pyry T akala. 2014. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts. Journal of the Association for Information\nScience and T echnology , 65(4):782–796.\nT omas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proceedings of NIPS , pages 3111–3119.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of EMNLP , pages\n1532–1543.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proc. of NAACL .\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlexander Sergeev and Mike Del Balso. 2018.\nHorovod: fast and easy distributed deep learning in\ntensorﬂow . arXiv preprint arXiv:1802.05799 .\nPaul C T etlock. 2007. Giving content to investor sen-\ntiment: The role of media in the stock market. The\nJournal of ﬁnance , 62(3):1139–1168.\nPaul C T etlock, Maytal Saar-Tsechansky, and Sofus\nMacskassy. 2008. More than words: Quantifying\nlanguage to measure ﬁrms’ fundamentals. The Jour-\nnal of Finance , 63(3):1437–1467.",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.7331578731536865
    },
    {
      "name": "Computer science",
      "score": 0.7008373737335205
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5653945803642273
    },
    {
      "name": "Natural language processing",
      "score": 0.5626461505889893
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5534201860427856
    },
    {
      "name": "Code (set theory)",
      "score": 0.5195813179016113
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4565467834472656
    },
    {
      "name": "Finance",
      "score": 0.4374334216117859
    },
    {
      "name": "Financial modeling",
      "score": 0.42783790826797485
    },
    {
      "name": "Business",
      "score": 0.10467982292175293
    },
    {
      "name": "Programming language",
      "score": 0.07909521460533142
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.060805678367614746
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 175
}