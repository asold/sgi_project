{
  "title": "TeamShakespeare at SemEval-2023 Task 6: Understand Legal Documents with Contextualized Large Language Models",
  "url": "https://openalex.org/W4385570558",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1967257563",
      "name": "Xin Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098270760",
      "name": "Yuchen Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3204971501",
    "https://openalex.org/W3165312697",
    "https://openalex.org/W4297824675",
    "https://openalex.org/W2743354079",
    "https://openalex.org/W3011594683",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4281765107",
    "https://openalex.org/W2790160933",
    "https://openalex.org/W2555961948",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W4385574308",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W4283836412",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2886946814",
    "https://openalex.org/W3007595536",
    "https://openalex.org/W2180877400",
    "https://openalex.org/W4385570872",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2020278455",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W1598796236",
    "https://openalex.org/W2612450760",
    "https://openalex.org/W4385566950",
    "https://openalex.org/W2970217403",
    "https://openalex.org/W4283728205",
    "https://openalex.org/W4308408710",
    "https://openalex.org/W4243017465",
    "https://openalex.org/W4226275564",
    "https://openalex.org/W4295344911"
  ],
  "abstract": "The growth of pending legal cases in populouscountries, such as India, has become a major is-sue. Developing effective techniques to processand understand legal documents is extremelyuseful in resolving this problem. In this pa-per, we present our systems for SemEval-2023Task 6: understanding legal texts (Modi et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that considers the com-prehensive context information in both intra-and inter-sentence levels to predict rhetoricalroles (subtask A) and then train a Legal-LUKEmodel, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B).Our evaluations demonstrate that our designedmodels are more accurate than baselines, e.g.,with an up to 15.0% better F1 score in subtaskB. We achieved notable performance in the taskleaderboard, e.g., 0.834 micro F1 score, andranked No.5 out of 27 teams in subtask A.",
  "full_text": "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 517‚Äì525\nJuly 13-14, 2023 ¬©2023 Association for Computational Linguistics\nTeamShakespeare at SemEval-2023 Task 6: Understand Legal Documents\nwith Contextualized Large Language Models\nXin Jin‚àó\nNorthwest Polytechnical Unviersity\nxin.jinsw@gmail.com\nYuchen Wang‚àó\nOhio State University\nwang.9298@osu.edu\nAbstract\nThe growth of pending legal cases in populous\ncountries, such as India, has become a major is-\nsue. Developing effective techniques to process\nand understand legal documents is extremely\nuseful in resolving this problem. In this pa-\nper, we present our systems for SemEval-2023\nTask 6: understanding legal texts (Modi et al.,\n2023). Specifically, we first develop the Legal-\nBERT-HSLN model that considers the com-\nprehensive context information in both intra-\nand inter-sentence levels to predict rhetorical\nroles (subtask A) and then train a Legal-LUKE\nmodel, which is legal-contextualized and entity-\naware, to recognize legal entities (subtask B).\nOur evaluations demonstrate that our designed\nmodels are more accurate than baselines, e.g.,\nwith an up to 15.0% better F1 score in subtask\nB. We achieved notable performance in the task\nleaderboard, e.g., 0.834 micro F1 score, and\nranked No.5 out of 27 teams in subtask A.\n1 Introduction\nThe growing amount of legal cases and documents\nrequires more and more human efforts to process\nthem. (Kalamkar et al., 2022b; Malik et al., 2021)\nIn some countries, such as India, legal cases have\naccumulated in an incredible number. For example,\nIndia has more than 47 million cases pending in\nthe courts (Kalamkar et al., 2022b). This has cre-\nated a need for automated methods to help judges\nefficiently understand and process relevant and re-\nliable information. In addition, these methods can\nalso help students, legal scholars, and court offi-\ncials who deal with legal documents daily. One\nway to assist them is to automatically understand\nand highlight the key information and context of\nlong legal documents.\nHowever, understanding legal documents by ma-\nchines is not an easy task. First, legal documents\nare often full of technical terminologies, which\n*These authors contributed equally to this work.\ncan span different legal divisions (Kalamkar et al.,\n2022b). Furthermore, legal documents can be\nspecific to special cases, such as health (Young,\n2009), IT technology (Lu et al., 2017b), and cy-\nber security (Shackelford et al., 2016; Jin et al.,\n2018; Lu et al., 2017a), which will involve domain-\nspecific words. In addition, legal documents can\nbe extremely long (Kalamkar et al., 2022b), which\nmakes dependency-based techniques and models,\nsuch as RNN models, fail to extract the con-\ntext information due to gradient vanishes. Fi-\nnally, typos and unstructured documents introduce\nnoises (Kalamkar et al., 2022b), which makes auto-\nmated natural language processing challenging.\nDespite these challenges, predicting rhetorical\nroles and recognizing named entities in legal docu-\nments are very useful for automating the processing\nand understanding of legal documents. Rhetorical\nrole prediction segments texts and structures noisy\nlegal documents into topically and semantically co-\nherent units (Ghosh and Wyner, 2019). Named en-\ntity recognition helps identify key legal entities in\nlong documents (Nadeau and Sekine, 2007), which\ncan not only help judges process cases in a more\nefficient way, but also benefit the next automation\nsteps. These two tasks can serve as key steps in\nthese methods (Modi et al., 2023).\nIn this paper, we propose to solve the rhetori-\ncal role prediction and named entity recognition\nproblems in the legal document domain with con-\ntextualized large-language models. We first sys-\ntematically build models with well-known design\nchoices based on popular pre-trained models (e.g.,\nBERT and XLM-roBERTa) (Qiu et al., 2020), then\nsystematically evaluate the performance of differ-\nent models and identify the key limitations, and\neventually propose our legal contextualized models\nas our final solutions. For rhetorical role predic-\ntion, we model this task as the sequential sentence\nclassification problem and build the Legal-BERT-\nHSLN model, which considers the comprehensive\n517\ncontext semantics in both intra- and inter-sentence\nlevels. For named entity recognition, we propose\nto build a legal-LUKE model that is both sensitive\nto context and entity-aware. Our evaluation results\nshow that our proposed models are more accurate\nthan baselines, e.g., Legal-LUKE is 15.0% better\nthan our baseline BERT-CRF in F1 score (more\ndetails in ¬ß4.3). Furthermore, we also achieved the\ntop performance on the rhetorical role prediction\ntask leaderboard, i.e., ranked No.5 out of 27 teams\nand achieved the 0.8343 micro F1 score (see ¬ß4.2).\nWe briefly summarize our primary contributions\nas follows.\n‚Ä¢ We formalize the rhetorical role prediction\ntask as a sequential sentence classification\nproblem and build the Legal-BERT-HSLN\nframework to model comprehensive sentence\nsemantics.\n‚Ä¢ We construct the legal-LUKE model with con-\ntextualized legal-document and entity-aware\nrepresentations.\n‚Ä¢ Our evaluations demonstrate the better perfor-\nmance of our proposed model compared to\nbaselines and achieved promising results on\nthe task leaderboard.\n2 Background\n2.1 Sequential Sentence Classification\nSequential sentence classification is a natural lan-\nguage processing (NLP) technique that involves\nclassifying a sequence of sentences into one or\nmore categories or labels (Hassan and Mahmood,\n2017; Cohan et al., 2019; Jin and Szolovits, 2018;\nBrack et al., 2022). The objective of this tech-\nnique is to analyze and classify the content of a\ngiven text document based on the semantics and\ncontext of the sentences (Jin et al., 2022a). It is\noften used in classification tasks (Xu et al., 2018;\nShan et al., 2021, 2022; Baskaran et al., 2022),\nsuch as sentiment analysis, spam detection, and\ntopic classification, where the classification of a\nsingle sentence can depend on the preceding or suc-\nceeding sentences (Hassan and Mahmood, 2017).\nThis technique can be implemented using vari-\nous algorithms, such as recurrent neural networks\n(RNNs) or long-short-term memory (LSTM) net-\nworks, which are capable of processing sequential\ndata (Lipton et al., 2015).\nIn sequential sentence classification, the input is\na sequence of sentences and the output is one or\nmore labels that describe the content of the docu-\nment. The classification can be performed at the\nsentence level or at the document level, depend-\ning on the specific use case (Cohan et al., 2019).\nSequential sentence classification is an important\ntechnique in NLP because it enables machines to\nunderstand and analyze the meaning and context of\nhuman language, which is crucial for many appli-\ncations such as automated text summarization and\nquestion-answering systems (Qiu et al., 2020).\n2.2 Legal Named Entity Recognition\nThe objective of named entity recognition in the\nlegal domain is to detect and label all instances of\nspecific legally relevant named entities within un-\nstructured legal reports (Nadeau and Sekine, 2007;\nKalamkar et al., 2022a; Li et al., 2020). Using this\nnamed entity information, one can analyze, aggre-\ngate, and mine data to uncover insightful patterns.\nFurthermore, the ultimate goal of legal document\nanalysis is to automate the process of information\nretrieval or mapping a legal document to one or\nmore nodes of a hierarchical taxonomy or legal\ncases, in which legal NER plays a significant role.\nGiven that legal reports contain a large number\nof complex medical terms and terminologies, such\nas statute and precedent, identifying expressions\nreferring to anatomies, findings, and anatomical lo-\ncations is a crucial aspect in organizing information\nand knowledge within the legal domain (Kalamkar\net al., 2022a). Therefore, automating this identi-\nfication process has been recognized as a key tar-\nget for automation. Moreover, automatic named\nentity recognition (NER) helps exhaustively ex-\ntract semantic information and misspelling check-\ning (Kalamkar et al., 2022a; Modi et al., 2023).\n3 System Overview\n3.1 Rhetorical Role Classification\nIdentifying rhetorical roles (RR) in legal docu-\nments is a challenging task, which requires ma-\nchine learning models to accurately classify sen-\ntences into predefined RR categories. One of the\nprimary challenges is the variability and complex-\nity of natural language, including variations in sen-\ntence structure, word choice, and context. This\nrequires training machine learning models on di-\nverse and large datasets that capture the range of\nlanguages used in the real world. Another signifi-\ncant challenge is capturing the long-term dependen-\ncies between sentences in a sequence, which can\n518\nLegal-BERTùë°!,!,... ,ùë°!,#Sentenceùë†!\nBi-LSTMAttention\nLegal-BERTùë°$,!,... ,ùë°$,#Sentence ùë†$\nBi-LSTMAttentionBi-LSTM...\n...\n...\n...\nùë¶! ...\nLinearCRFùë¶$\nFigure 1: Model Architecture of Legal-BERT-HSLN\nbe difficult to achieve but is essential to determine\nthe overall meaning and context of the text.\nTo solve this problem, we first observe that this\ntask is basically a sequential sentence classifica-\ntion problem. Meanwhile, existing work proposes\nto solve this problem using encoder-decoder mod-\nels, where the encoder embeds the sentence-level\nsemantics and considers the context dependency\ninformation, and the decoder classifies individual\nsentences by the contextualized surrounding sen-\ntence information. For example, Jin et. al (Jin\nand Szolovits, 2018) classify medical abstract sen-\ntences with a hierarchical sequential labeling net-\nwork, which is composed of a word encoder, a\nsentence encoder, and a document encoder. Specif-\nically, the word-level encoder is a bidirectional\nLSTM that encodes each word in a sentence into\na vector12. The sentence-level encoder is another\nbidirectional LSTM that encodes each sentence\nvector in a hidden state. The document-level en-\ncoder is either an LSTM or a CRF layer that models\nthe dependencies between sentences and outputs a\nlabel for each sentence.\nWe follow the paradigm and propose the\nLegal-BERT-HSLN model for legal rhetorical role\nclassification. In our design, we applied the\nHSLN structure from Brack et al. (Brack et al.,\n2022) and changed the backbone model to Legal-\nBERT (Chalkidis et al., 2020). The model architec-\nture of Legal-BERT-HSLN is shown in figure Fig-\nure 1. The model first takes as input the sequences\nof legal word tokens ({ ti,1, ti,2, ..., ti,m}) of sen-\ntence si and Legal-BERT generates the correspond-\ning token embeddings. Next, the token embed-\ndings are further enriched with local context in-\nformation within the sentence si by Bi-LSTM and\nthe attention pooling layer as the augmented token\nembeddings ({ei,1, ei,2, ..., ei,m}), which is aggre-\ngated to generate the embedding of the sentence\nesi. Since one of the most important features in this\ntask is the inter-sentence dependency, we further\nenrich the sentence embedding with contextual in-\nformation from surrounding sentences. The output\nlayer transforms contextualized sentence embed-\ndings into rhetorical role labels via a linear transfor-\nmation and CRF. We also introduce dropout layers\nafter each layer for regularization.\n3.2 Legal Named Entity Recognition\nIn SemEval task 6 (Modi et al., 2023), the second\nsubtask is the recognition of legal entities named\nentities. Specifically, the legal documents provide\nnonexhaustive metadata with 14 legal entities, in-\ncluding petitioner, respondent, court, statute, pro-\nvision, precedents, etc. Identifying these legal\nentities can be both error prone and labor inten-\nsive (Mohit, 2014).\nAlthough the task of legal NER may seem\nstraightforward, it is in fact a challenging undertak-\ning due to several reasons. First, the NER task itself\nis an unsolved problem (Li et al., 2020). More-\nover, legal documents can be very noisy (Kalamkar\net al., 2022b,a). Legal texts contain morphological\nforms (e.g., synonyms, abbreviations, and even\nmisspellings), which means that different legal\ncases can use different words and phrases to ex-\npress the same meaning. To process the natural\nlanguage texts, existing approaches use vocabulary-\nbased embeddings (Wang et al., 2020). However,\nlegal documents involve many out-of-vocabulary\nwords (Jin et al., 2022b), which further makes the\ntask complex.\nMeanwhile, we have several insights to solve\nthe problems. First, natural language preprocess-\ning, e.g., tokenization, POS tagging, and sentence\nparsing, can mitigate the noise of legal documents.\nMoreover, the identified POS tags and sentence\nstructure can help determine the legal entities by\nthe nature of human languages. Second, compared\nto static entity representations that assign fixed\nembeddings to words and entities in the knowl-\nedge base, contextualized word representations\ncan generate adaptive semantic embeddings of\nwords and entities, which can be tuned by domain-\n519\nInput Sequence\nùêª!!\nContextualized embeddings\n‚Ä¶\n‚Ä¶\n‚Ä¶Legal-LUKEEncoder\nùë§\"ùë§#\nùë§$ùëí\"\nùëí%‚Ä¶ ùêª&!\nùêª!\"\nùêª!#\nùêª&$\nWord features\nEntity features\nFigure 2: Entity-aware Contextualized Representation\nspecific context (Ethayarajh, 2019). Finally, we ob-\nserve that state-of-the-art entity-aware representa-\ntions (Yamada et al., 2020; Ri et al., 2022) can take\nadvantage of both the advantages of contextualized\nword embeddings and create entity representations\nbased on the rich entity-centric semantics encoded\nin the corresponding entity embeddings. Therefore,\nwe combine these insights and propose to identify\nlegal entities with embeddings of legal-sensitive\nentities.\nFor legal NER, we introduce a legal entity recog-\nnition model: Legal-LUKE, based on the bidi-\nrectional transformer encoder of LUKE (Yamada\net al., 2020). The LUKE model was pre-trained\nby predicting both words and entities masked by\nthe [MASK] tokens. As shown in Figure 2, Legal-\nLUKE takes as input the sequence of preprocessed\nwords (w1, w2, .., wN ) and entities (e1, e2, ..., eM ).\nThe encoder generates the legal-contextualized\nrepresentations for both words and entities, i.e.,\n(Hw1 , Hw2 , .., HwN ) and ( He1 , He2 , .., HeM ). To\ncompute the word and entity embeddings, three\nembeddings are added together, which include to-\nken embeddings, type embeddings, and position\nembeddings. Position embeddings are used to asso-\nciate entity tokens with their corresponding word\ntokens, where the position of an entity token is\ndetermined by the positions of its corresponding\nword tokens. The entity position embeddings are\nthen added up over these positions.\n4 Evaluations\nWe conduct extensive experiments using the rhetor-\nical role classification and legal named entity recog-\nnition tasks. We have implemented the Legal-\nBEST-HSLN model and the Legal-LUKE model\nbased on Pytorch (Paszke et al., 2019).\n4.1 Experimental Setup\nThe input word sequence is created by inserting\nthe tokens of [CLS] and [SEP] into the original\nword sequence as the first and last tokens, respec-\ntively, unless stated otherwise. For the input entity\nsequence of Legal-LUKE, the legal entities from\nthe training set are used to fine-tune the model. Al-\nthough we cannot access the test set before the task\norganizers open the evaluation system, we train and\ntune our models and baselines based on the training\nand validation set.\n4.1.1 Baselines for Rhetorical Role\nClassification\nFor subtask A, we set up our experiments by select-\ning BERT-base and 3 BERT variants with minor\nmodifications as our baselines:\n‚Ä¢ BERT-Base. For this model, we directly use\nthe [CLS] token embedding as the sentence\nembedding as the encoder of the classifier.\n‚Ä¢ BERT-Mean. Instead of using the hidden\nstate of the token [CLS] for the classification\noutput from BERT, we tried to use the mean\nvalue of 128 lengths of sequences.\n‚Ä¢ BERT-Regularization. In this method, we\nmade a preprocessing work to the training set\nby regularizing symbols. These procedures\ninclude: (a) lowercase, (b) remove @name,\n(c) isolate and remove punctuation except ‚Äò?‚Äô,\n(d) remove special characters such as columns\nand semi-columns, (e) remove stop words ex-\ncept ‚Äònot‚Äô and ‚Äòcan‚Äô, and (f) remove trailing\nwhitespace.\n‚Ä¢ BERT-Augmentation. Each sentence train-\ning set is randomly swapped and the entire\ntraining size is doubled.\n4.1.2 Baselines for Legal Named Entity\nRecognition\nFor subtask B, we select the following 3 models as\nour baselines:\n‚Ä¢ BERT-CRF. For this model, we use the BERT\nmodel as the encoder and transform the encod-\nings into NER labels with the CRF prediction\nhead.\n‚Ä¢ BERT-Span. BERT-Span (Joshi et al., 2020)\nis the variation of BERT that uses BERT to\ntrain span boundary representations, which\nare more suitable for entity boundary detec-\ntion.\n520\nModel Micro F1 Score Best Epoch\nBERT-Base 0.631 5\nBERT-Mean 0.641 4\nBERT-Regularization 0.597 4\nBERT-Augmentation 0.645 4\nLegal-BERT-HSLN 0.828 16\nTable 1: Rhetorical Role Classification Performance\n‚Ä¢ XLM-roBERTa-CRF. XLM-roBERTa-CRF\nis a combination of XLM-RoBERTa (Con-\nneau et al., 2019) and CRF. XLM-RoBERTa\nis a multilingual version of RoBERTa, a trans-\nformer model pre-trained on a large corpus\nin a self-supervised fashion. We used XLM-\nRoBERTa-large as the encoder model.\n‚Ä¢ mLUKE. mLUKE (Ri et al., 2021) is a mul-\ntilingual extension of LUKE, a pre-trained\nlanguage model that incorporates entity infor-\nmation from Wikipedia.\n4.1.3 Test Environment\nThe evaluations are performed on a desktop server,\nwith the Intel Xeon E5-1650 CPU, Ubuntu 18.04\nOS, 64 GB memory, 4 TB storage, and 4 NVIDIA\nGeForce GTX 1080 Ti graphics cards. While train-\ning the Legal-LUKE model, we encountered out-of-\nmemory issues, and we switched to another server,\nwhich has the Intel Xeon W-2245 CPU, Ubuntu\n20.04 OS, 128 GB memory, 2 TB storage, and an\nNVIDIA RTX A5000 graphics card.\n4.2 Results of Rhetorical Role Classification\nThe goal of rhetorical role classification is to pro-\nvide a categorization label for each sentence at\nthe unstructured document level. The number of\nclasses is 13. The training set has 247 documents\nwith a total sentence number of 30k, validation\nset has 30 documents with a total sentence number\nof 3k. In this section, all evaluations are reported\nbased on the validation set.\nTable 1 shows the summary of micro F1 scores\nfor Legal-BERT-HSLN and the baselines on the\nvalidation set, where Legal-BERT-HSLN signifi-\ncantly outperforms all baselines and it takes more\nepochs to converge, where we believe Legal-BERT-\nHSLN has the better capacity to understand legal\ndocuments. It is worth mentioning that the regular-\nization process decreased the F1 score and suggests\nthat the rhetorical role is sensitive to detailed stop\nwords and external sentence markers.\nWhile our initial design choice was not Legal-\nFigure 3: Confusion Matrix of BERT Baseline Predic-\ntions\nBERT-HSLN, we originally trained the baseline\nmodels. Specifically, our first design choice was\nto treat all sentences as individual elements where\nthey have no correlation to each other. We set the\nbaseline model with the BERT network backbone\nwith a simple multilayer perceptron. The result-\ning confusion matrix is shown in Figure 3 with the\nmicro F1 0.631. Subsequently, we conducted 3\nBERT variants with minor modifications and ob-\ntained the performance shown in Table 1. From the\nresults, we observed the significant performance\ngap between all the BERT baselines and the state-\nof-the-art solution (Kalamkar et al., 2022b).\nTo fully understand the problem, we put an ef-\nfort into examining the distribution of the legal\ndataset (Modi et al., 2023) in term of sentence\nlength and classes. In Figure 4 (A), the sentence\nlengths overall satisfied Poisson‚Äôs distribution but\nthere is a significant noise at <20 levels. These\nshort sentences may influence the accuracy of clas-\nsification due to a lack of meaningful informa-\ntion. For example, sentences 17928, 17929, and\n17930 are \"[326C-E\", \"] 2.\", and \"There may be\ncircumstances where expenditure, even if incurred\nfor obtaining advantage of enduring benefit would\nnot amount to acquisition of asset \", respectively,\nwhose ground-truth labels are all PREMABLE.\nThis might be due to the discontinuous annota-\ntion from turning the page or a keyboard input\nmistake. Therefore, model prediction from the sen-\ntence level is likely not an optimal paradigm to\nproceed. In Figure 4 (B), we found that there is a\nsignificant bias of the labeling in the training set;\ntherefore, we believe that more experiments, such\nas validating the label bias, can be tested for future\nresearch.\nExcept for the design choices, we also evalu-\n521\nSentence Length\nFrequencyFrequency\nA\nB\nFigure 4: (A) Sentence Length and (B) Class Distribu-\ntions in Rhetorical Role Dataset\nA\nB\nFigure 5: Legal-BERT-HSLN Performance Affected by\n(A) Maximum Sentence Length and Batch Size and (B)\nLearning Rate\nFigure 6: Confusion Matrix with Legal-BERT-HSLN\nate the effectiveness of our hyper-parameters. Fig-\nure 5 shows the micro F1 score is influenced by\nthe batch size, maximum sentence length, and\nlearning rate, where the best combination we have\ntested so far is batch size 16, maximum sen-\ntence length 32, learning rate 1 √ó 10‚àí4. All\nweighted-F1 scores in Figure 5 are calculated\nfrom sklearn.metrics.f1_score function with\nthe weighted option. Figure 6 shows the confu-\nsion matrix using this combination of hyperpa-\nrameters, and it is found that the best Micro F1\nscore we achieve in the validation set is 0.828. Al-\nthough the major error contribution is no longer\nin the PREMABLE label, the accuracy of the\nPRE_NOT_RELIED label is 0 which is the same as\nreported by the state-of-the-art solution (Kalamkar\net al., 2022b). For future experiments, we suggest\nstudying this outlier effect.\nPerformance on Test Set After the task orga-\nnizer released the test, we test Legal-BEST-HSLN\nand submitted the predictions. Surprisingly, we ob-\ntained the micro F1 score of 0.8343 and ranked No.\n5 out of 27 teams. Note that the test performance\nis better than all performance on the validation set.\nTherefore, we believe that there is a shift in the\ndistribution of the training, validation, and test sets.\n4.3 Results of Legal Named Entity\nRecognition\nTable 2 presents the overall performance of Legal-\nLUKE and the baseline models for legal named\nentity recognition on the validation set. We observe\nthat Legal-LUKE outperforms all baselines with an\nup to 14.3% better micro F1 score.\nMoreover, among the baselines, the BERT-Span\nmodel is better than the BERT-CRF model where\nwe used the same encoder model but different pre-\n522\nModel F1 Score Best Epoch\nBERT-CRF 0.694 12\nBERT-Span 0.712 14\nXLM-roBERTa-CRF 0.773 21\nmLUKE 0.787 12\nLegal-LUKE 0.796 18\nTable 2: Overall Performance on Validation Set of Legal\nNamed Entity Recognition\n0 20 40 60 80 100\n# Epochs\n0.4\n0.5\n0.6\n0.7\n0.8Performance\n precision\nrecall\nf1 score\nFigure 7: The Best Performance of the Baseline Model\nXLM-roBERTa-CRF across Training Epochs\ndiction heads, which demonstrates the benefits of\nusing the span boundary representations for the\nnamed entity recognition task. Moreover, the XLM-\nroBERTa encoder model shows better legal text en-\ncoding capacity because the XLM-roBERTa-CRF\nmodel and BERT-CRF model share the same de-\ncoder module, but XLM-roBERTa-CRF outper-\nforms BERT-CRF with a 11.4% better F1 score.\nNote that we gave the same parameters for the\nbaseline models. For example, we consider con-\ntext information by a natural language preprocessor\nwith a maximum length of 100. To balance train\nefficiency and performance, we set the batch size\nper GPU as 16 with a learning rate scheduler whose\ninitial rate is 5.0√ó10‚àí6 and the AdamW optimizer.\nAlthough we set the maximum number of epochs\nas 10, we observed that all models achieved the\nbest performance at early epochs. For example,\nFigure 7 shows the best performance of the XLM-\nroBERTa-CRF baseline model on the validation set\nin different epochs, which shows the convergence\neffect as the training epoch increases.\nFor our Legal-LUKE model, we also use a learn-\ning rate scheduler that has a warm-up ratio of 0.06,\nthe same initial rate5.0√ó10‚àí6 as baselines, and the\nAdamW optimizer. While the pre-trained LUKE\nmodel is too large for our GPU server, we tune the\ndifferent batch sizes to trade off between training\n0 20 40 60 80 100\n# Epochs\n0.4\n0.6\n0.8\n1.0Performance\nprecision\nrecall\nf1 score\nFigure 8: The Best Performance of Legal-LUKE across\nTraining Epochs\nspeed and performance, and eventually we set the\nbatch size as 8. Similar to the baseline models,\nwe also observe the performance convergence (as\nshown in Figure 8) though we set the maximum\nepoch number at 100.\nPerformance on Test Set After the task orga-\nnizer released the test, we evaluated Legal-LUKE\non the test set and submitted the predictions. We\nobtained an F1 score of 0.667 and ranked No. 13\nout of 17 teams. Our test F1 score is far lower than\nthe validation F1 score. However, according to the\ndataset (Kalamkar et al., 2022a), the validation set\nand test set should have similar distributions as they\nare from the same group of legal cases. Our hypoth-\nesis is that the event organizer adopted a different\npreprocessor from ours which produces different\ntoken indices and label shifts. For example, we\nobserved that the legal documents contain many\nempty lines and we opted to remove such lines in\nour processing, which can result in the different\ntoken indices in our predictions. We plan to further\ninvestigate the root cause upon the release of the\ntest set.\n5 Conclusion\nIn this paper, we propose to understand legal docu-\nments with context-sensitive language models. For\nthe sub-task of rhetorical role classification, we de-\nsign the Legal-BERT-HSLN model, which learns\nthe hierarchical context information to solve the\nsequential sentence classification problem. For le-\ngal named entity recognition, we implemented the\nLegal-LUKE model which is both contextualized\nand entity-aware. Our evaluation results reveal the\noutperformance of our models compared to base-\nlines and we are the top-5 teams for the rhetorical\nrole classification task on the leaderboard.\n523\nReferences\nVishal Athreya Baskaran, Jolene Ranek, Siyuan\nShan, Natalie Stanley, and Junier B Oliva. 2022.\nDistribution-based sketching of single-cell samples.\nIn Proceedings of the 13th ACM International Con-\nference on Bioinformatics, Computational Biology\nand Health Informatics, pages 1‚Äì10.\nArthur Brack, Anett Hoppe, Pascal Buscherm√∂hle, and\nRalph Ewerth. 2022. Cross-domain multi-task learn-\ning for sequential sentence classification in research\npapers. In Proceedings of the 22nd ACM/IEEE Joint\nConference on Digital Libraries, pages 1‚Äì13.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. Legal-bert: The muppets straight out of law\nschool. arXiv preprint arXiv:2010.02559.\nArman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi,\nand Daniel S Weld. 2019. Pretrained language mod-\nels for sequential sentence classification. arXiv\npreprint arXiv:1909.04054.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm√°n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the ge-\nometry of bert, elmo, and gpt-2 embeddings. arXiv\npreprint arXiv:1909.00512.\nSaptarshi Ghosh and Adam Wyner. 2019. Identification\nof rhetorical roles of sentences in indian legal judg-\nments. In Legal Knowledge and Information Systems:\nJURIX 2019: The Thirty-second Annual Conference,\nvolume 322, page 3. IOS Press.\nAbdalraouf Hassan and Ausif Mahmood. 2017. Deep\nlearning for sentence classification. In 2017 IEEE\nLong Island Systems, Applications and Technology\nConference (LISAT), pages 1‚Äì5. IEEE.\nDi Jin and Peter Szolovits. 2018. Hierarchical neu-\nral networks for sequential sentence classification\nin medical scientific abstracts. arXiv preprint\narXiv:1808.06161.\nXin Jin, Wei Lu, Siqi Liu, and Zuqing Zhu. 2018. On\nmulti-layer restoration in optical networks with en-\ncryption solution deployment. In 2018 Optical Fiber\nCommunications Conference and Exposition (OFC),\npages 1‚Äì3. IEEE.\nXin Jin, Sunil Manandhar, Kaushal Kafle, Zhiqiang\nLin, and Adwait Nadkarni. 2022a. Understanding\niot security from a market-scale perspective. In Pro-\nceedings of the 2022 ACM SIGSAC Conference on\nComputer and Communications Security, pages 1615‚Äì\n1629.\nXin Jin, Kexin Pei, Jun Yeon Won, and Zhiqiang\nLin. 2022b. Symlm: Predicting function names\nin stripped binaries via context-sensitive execution-\naware code embeddings. In Proceedings of the 2022\nACM SIGSAC Conference on Computer and Commu-\nnications Security, pages 1631‚Äì1645.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64‚Äì77.\nPrathamesh Kalamkar, Astha Agarwal, Aman Tiwari,\nSmita Gupta, Saurabh Karn, and Vivek Raghavan.\n2022a. Named entity recognition in indian court\njudgments. arXiv preprint arXiv:2211.03442.\nPrathamesh Kalamkar, Aman Tiwari, Astha Agar-\nwal, Saurabh Karn, Smita Gupta, Vivek Ragha-\nvan, and Ashutosh Modi. 2022b. Corpus for auto-\nmatic structuring of legal documents. arXiv preprint\narXiv:2201.13125.\nJing Li, Aixin Sun, Jianglei Han, and Chenliang Li.\n2020. A survey on deep learning for named entity\nrecognition. IEEE Transactions on Knowledge and\nData Engineering, 34(1):50‚Äì70.\nZachary C Lipton, John Berkowitz, and Charles\nElkan. 2015. A critical review of recurrent neu-\nral networks for sequence learning. arXiv preprint\narXiv:1506.00019.\nWei Lu, Xin Jin, and Zuqing Zhu. 2017a. Game theo-\nretical flexible service provisioning in ip over elas-\ntic optical networks. In 2017 Opto-Electronics and\nCommunications Conference (OECC) and Photonics\nGlobal Conference (PGC), pages 1‚Äì3. IEEE.\nXiaofeng Lu, Ruonan Zhang, Yuliang Zhou, Jiawei\nLiu, Xin Jin, Qi Guo, and Chang Cao. 2017b. Con-\nvolutional modeling and antenna de-embedding for\nwideband spatial mmwave channel measurement. In\n2017 IEEE Wireless Communications and Network-\ning Conference (WCNC), pages 1‚Äì6. IEEE.\nVijit Malik, Rishabh Sanjay, Shouvik Kumar Guha,\nAngshuman Hazarika, Shubham Nigam, Arnab Bhat-\ntacharya, and Ashutosh Modi. 2021. Semantic seg-\nmentation of legal documents via rhetorical roles.\narXiv preprint arXiv:2112.01836.\nAshutosh Modi, Prathamesh Kalamkar, Saurabh Karn,\nAman Tiwari, Abhinav Joshi, Sai Kiran Tanikella,\nShouvik Guha, Sachin Malhan, and Vivek Ragha-\nvan. 2023. SemEval-2023 Task 6: LegalEval: Un-\nderstanding Legal Texts. In Proceedings of the\n17th International Workshop on Semantic Evalua-\ntion (SemEval-2023), Toronto, Canada. Association\nfor Computational Linguistics (ACL).\nBehrang Mohit. 2014. Named entity recognition. Natu-\nral language processing of semitic languages, pages\n221‚Äì245.\n524\nDavid Nadeau and Satoshi Sekine. 2007. A survey of\nnamed entity recognition and classification. Lingvis-\nticae Investigationes, 30(1):3‚Äì26.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872‚Äì\n1897.\nRyokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n2021. mluke: The power of entity representations\nin multilingual pretrained language models. arXiv\npreprint arXiv:2110.08151.\nRyokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n2022. mLUKE: The power of entity representations\nin multilingual pretrained language models. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics.\nScott J Shackelford, Scott Russell, and Andreas Kuehn.\n2016. Unpacking the international law on cyberse-\ncurity due diligence: Lessons from the public and\nprivate sectors. Chi. J. Int‚Äôl L., 17:1.\nSiyuan Shan, Vishal Athreya Baskaran, Haidong Yi, Jo-\nlene Ranek, Natalie Stanley, and Junier B Oliva. 2022.\nTransparent single-cell set classification with kernel\nmean embeddings. In Proceedings of the 13th ACM\nInternational Conference on Bioinformatics, Com-\nputational Biology and Health Informatics , pages\n1‚Äì10.\nSiyuan Shan, Yang Li, and Junier B Oliva. 2021. Nrtsi:\nNon-recurrent time series imputation. arXiv preprint\narXiv:2102.03340.\nYuxuan Wang, Yutai Hou, Wanxiang Che, and Ting Liu.\n2020. From static to dynamic word representations:\na survey. International Journal of Machine Learning\nand Cybernetics, 11:1611‚Äì1630.\nYan Xu, Siyuan Shan, Ziming Qiu, Zhipeng Jia,\nZhengyang Shen, Yipei Wang, Mengfei Shi, I Eric,\nand Chao Chang. 2018. End-to-end subtitle detection\nand recognition for videos in east asian languages via\ncnn ensemble. Signal Processing: Image Communi-\ncation, 60:131‚Äì143.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP). Association for Computational\nLinguistics.\nAndy Young. 2009. The legal duty of care for nurses\nand other health professionals. Journal of Clinical\nNursing, 18(22):3071‚Äì3078.\n525",
  "topic": "SemEval",
  "concepts": [
    {
      "name": "SemEval",
      "score": 0.8527481555938721
    },
    {
      "name": "Computer science",
      "score": 0.7585362792015076
    },
    {
      "name": "Sentence",
      "score": 0.7291839122772217
    },
    {
      "name": "Task (project management)",
      "score": 0.6541949510574341
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6334512233734131
    },
    {
      "name": "Natural language processing",
      "score": 0.611983597278595
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5952301621437073
    },
    {
      "name": "Management",
      "score": 0.0711926817893982
    },
    {
      "name": "Geography",
      "score": 0.0605258047580719
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}