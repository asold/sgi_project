{
  "title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models",
  "url": "https://openalex.org/W2979949198",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227049268",
      "name": "Hoover, Benjamin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281952243",
      "name": "Strobelt, Hendrik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2526622908",
      "name": "Gehrmann, Sebastian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2968210605",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2963123635",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2949202705",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2752194699",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963310665"
  ],
  "abstract": "Large language models can produce powerful contextual representations that lead to improvements across many NLP tasks. Since these models are typically guided by a sequence of learned self attention mechanisms and may comprise undesired inductive biases, it is paramount to be able to explore what the attention has learned. While static analyses of these models lead to targeted insights, interactive tools are more dynamic and can help humans better gain an intuition for the model-internal reasoning process. We present exBERT, an interactive tool named after the popular BERT language model, that provides insights into the meaning of the contextual representations by matching a human-specified input to similar contexts in a large annotated dataset. By aggregating the annotations of the matching similar contexts, exBERT helps intuitively explain what each attention-head has learned.",
  "full_text": "EXBERT: A Visual Analysis Tool to Explore\nLearned Representations in Transformers Models\nBen Hoover\nIBM Research\nMIT-IBM Lab\nbenjamin.hoover@ibm.com\nHendrik Strobelt\nIBM Research\nMIT-IBM Lab\nhendrik.strobelt@ibm.com\nSebastian Gehrmann\nHarvard SEAS\ngehrmann@seas.harvard.edu\nAbstract\nLarge language models can produce powerful contextual representations that lead\nto improvements across many NLP tasks. Since these models are typically guided\nby a sequence of learned self attention mechanisms and may comprise undesired\ninductive biases, it is paramount to be able to explore what the attention has learned.\nWhile static analyses of these models lead to targeted insights, interactive tools\nare more dynamic and can help humans better gain an intuition for the model-\ninternal reasoning process. We present EXBERT, an interactive tool named after\nthe popular BERT language model, that provides insights into the meaning of the\ncontextual representations by matching a human-speciﬁed input to similar contexts\nin a large annotated dataset. By aggregating the annotations of the matching similar\ncontexts, EXBERT helps intuitively explain what each attention-head has learned.\n1 Introduction\nNeural networks based on the Transformer architecture have led to impressive improvements across\nmany Natural Language Processing (NLP) tasks such as machine translation and text summariza-\ntion [Vaswani et al., 2017]. The Transformer is based on subsequent application of “multi-head\nattention” to route the model reasoning, and this technique’s ﬂexibility allows it to be pretrained on\nlarge corpora to generate contextual representations that can be used for other tasks.\nOf these models, BERT is the most commonly used Transformer model for representation learning\nwith several applications to transfer learning [Devlin et al., 2019]. BERT and it’s extensions [Sanh\net al., 2019, Liu et al., 2019] are dominating the standard language understanding benchmarks [Wang\net al., 2018, 2019]. Moreover, Transformer models have also had much success as autoregressively\ntrained language models that can be used for generation tasks [Radford et al., 2019, Keskar et al.,\n2019].\nIt is not yet well-understood what information BERT encodes and how it uses the attention. To address\nthis challenge, some research has focused on understanding whether BERT learns linguistic features\nsuch as Part Of Speech (POS), Dependency relationships (DEP), or Named Entity Recognition\n(NER) [e.g., Tenney et al., 2019a, Vig and Belinkov, 2019, Raganto and Tiedemann, 2018, Tenney\net al., 2019b]. Clark et al. [2019] found that heads at different layers learn speciﬁc linguistic structure\ndespite being trained in a completely unsupervised manner, although many heads ostensibly learn\nredundancies. V oita et al. [2019] further explore the nature of several specialized attention-heads\nto show that BERT depends on only a subset of the total heads and that overall model performance\ncould be maintained when some heads were pruned.\narXiv:1910.05276v1  [cs.CL]  11 Oct 2019\nCorpus ViewAttention View\na\nb\nc\ne d\nSummary View\ng\nf\nFigure 1: An overview of the different components of the tool. The token “escape” is selected and\nmasked at 0-[all]. The results from a corpus search by token embedding are shown and summarized\nin (d-g). Users can enter a sentence in (a) and modify the attention view through selections in (b). Self\nattention is displayed in (c). The blue matrices show the attention of a head (column) to a token (row).\nTokens and heads that are selected in (c) can be searched over the annotated corpus (shown: Wizard\nof Oz) with results presented in (d). Every token in (d) displays its linguistic metadata on hover. A\ncolored summary of the matched token (black highlight) and its context is shown in (e), which can\nbe expanded or collapsed with the buttons above it. The histograms in (f) and (g) summarize the\nmetadata of the results in (d) for the matched token and the token of max attention, respectively.\nThe above analyses provide an in-depth but static glimpse into the behavior of transformers. Experi-\nments for these analyses are often supported by open-source repositories that implement the newest\narchitectures and thus enable rapid experimentation [Wolf et al., 2019]. We similarly need ﬂexible\nevaluation frameworks for Transformer models that allow the community to test hypotheses rapidly.\nToward that end, visualization tools can offer concise summaries of useful information and allow\ninteraction with large models. Attention visualizations such as BertViz by Vig [2019] have taken\nlarge steps toward these goals by making exploration of BERT’s attention fast and interactive for the\nuser. However, interpreting attention patterns without understanding the attended-to embeddings,\nor relying on attention alone for a faithful interpretation, can lead to faulty interpretations [Brunner\net al., 2019, Jain and Wallace, 2019, Wiegreffe and Pinter, 2019].\nTo address this challenge, we developedEXBERT, a tool that combines the advantages of a robust but\nstatic analysis with a dynamic and intuitive view into both the attention and internal representations\nof the underlying model.1 EXBERT is agnostic to the underlying Transformer model and corpus\nand can thus be applied to different domains and languages. Similar to the static analysis by Clark\net al. [2019], EXBERT provides insights into both the attention and the token embeddings for the\nuser-deﬁned model and corpus by probing whether the representations capture metadata such as\nlinguistic features or positional information.\n2 Background\n2.1 Transformer Models\nThe Transformer model architecture as deﬁned by Vaswani et al. [2017] relies on multiple sequential\napplications self attentionlayers. Self attention is the process by which each token within a sequence\nof inputs Y computes attention weights over all other tokens in the same input. Within the process,\nthe inputs are projected into a key, query, and value representationWk, Wq, and Wv. The query and\nkey representations are used to compute a weight for each token, which is then multiplied by that\ntoken’s value, such that the values for oneattention headh(i) is deﬁned as\n1exBERT is available at www.exbert.net.\n2\nh(i) = softmax\n(\n(Y W(i)\nq )(Y W(i)\nk )⊤\n)\n(Y W(i)\nv ).\nTransformer models typically use n of these self attention heads in parallel. Their outputs\nh(0), . . . , h(n−1) are concatenated and followed by a ﬁnal linear projection. The output of this\nprojection is used to calculate the token embedding used for the next layer.\n2.2 Transformer Analysis\nPrevious analyses of transformer models focus on discovering how an unsupervised Transformer\nmodel learns to model our human understanding of language. For instance, Clark et al. [2019] showed\nthat individual heads seem to recognize common POS and DEP relationships,e.g., Objects of the\nPreposition (POBJ), Determinants (DET), and Possessive Adjectives (POSS), with high ﬁdelity. Vig\nand Belinkov [2019] also explored the dependency relations across heads and discovered that initial\nlayers typically encode positional relations, middle layers capture the most dependency relations,\nand later layers look for unique patterns and structures. These insights are exposed visually and\ninteractively through EXBERT.\n3 Overview\nEXBERT focuses on displaying a succinct view of both the attention and the internal representations\nof each token. The attention belonging to an input of length N at a particular layer for a particular\nhead can be understood as an N ×N matrix, where each row represents the attention out of the\ncorresponding token in the input, and each column represents the attention into that token. This is\nconducive to a representation of curves pointing from each token to every other token. Representations,\non the other hand, are best understood by comparing the embedding of a token to the embeddings\nof other tokens in an annotated corpus. The most similar token embeddings, deﬁned by a nearest\nneighbor search, can be viewed in their corpus’ context in a language that humans can understand.\n3.1 Components\nFigure 1 shows an overview of the tool’s three main components. TheAttention Viewprovides an\ninteractive view of the self attention of the model. Here, users can change layers, select heads, and\nview the aggregated attention. Tokens can be masked, and a selected token can be searched over\nthe annotated corpus according to the methods laid out in Section 3.2. The results of this search are\npresented in the Corpus View, with the highest-similarity matches shown ﬁrst. The Summary View\nshows histogram summaries of the matched metadata, which is useful for getting a snapshot of the\nmetadata an embedding encodes in the searched corpus.\n3.2 Searching\nInspired by Strobelt et al. [2017, 2018], EXBERT performs a nearest neighbor search of embeddings\non a reference corpus that is processed with linguistic features as follows. First, the corpus is split by\nsentence, and its tokens are labeled for desired metadata (e.g., POS, DEP, NER). Searching by token\nembeddings performs a Cosine Similarity (CS) search with the tokens in the corpus [Johnson et al.,\n2019]. The top 50 matches are displayed and summarized for the user.\nSearching by head embeddings also involves a CS search against the corpus but requires an extension\nof the self attention deﬁnition. In our case, we deﬁne the head embeddingE(l) as\nE(l) = Concat(˜h\n(l,0)\n, . . . ,˜h\n(l,n−1)\n),\nwhere ˜h\n(l,i)\nis deﬁned as the normalized representation of head i at layer l.\nThis normalization makes it possible to perform a CS search over the head embeddings in our\npreprocessed corpus. To search the corpus for only a select subset Hs ⊆{0, . . . , n−1}, we set all\nvalues of ˜h\n(l,i)\nto 0 in our query head embedding E(l) where i /∈Hs.\n3\n4 Case Study: BERT\nBERT is an instantiation of a Transformer model that can be used in applications that beneﬁt from\ntransfer learning [Devlin et al., 2019]. BERT introduces special tokens into the typical training process\nin addition to the input tokens that are extracted using Byte-Pair Encoding (BPE) [Sennrich et al.,\n2015]. The architecture requires every input to start with a “[CLS]” and end with a “[SEP]” token and\nuses a technique called Masked Language Modeling (MLM) to develop its language model [Devlin\net al., 2019]. MLM works by replacing random tokens in the training corpus with “[MASK]” and\ntraining the model to determine what word should belong. Since there is no information in the\noriginal vector embedding of “[MASK]”, BERT must rely on its internal representations to ﬁll in the\nmissing tokens. This provides an intuitive way to glimpse how BERT learns linguistic features in\ncontext.\nIn the following cases, the reference corpus used is the Wizard of Oz, 2 which is annotated and\nprocessed by BERT to allow for nearest neighbor searching. Whenever BPE tokenization splits a\nsingle word into multiple tokens, we assign its metadata to each component token. Special tokens\nlike “[CLS]” and “[SEP]” have no linguistic features assigned to them, and are therefore removed\nfrom the reference corpus, which allows searches to always match a token that has intuitive meaning\nfor users. This also allows the tool to apply the same corpus to different transformer models that may\nrequire different tokenization.\nWe now explore the layers and heads at which BERT learns the linguistic features of a masked token.\nWe look at the following sentence:\nThe girl ran to a local pub to escape the din of her city.\nUnless otherwise noted, the following examples are from the BERTbase model, which has 12 layers\nand 12 heads per layer. We use the notation <layer>-<head> to refer to a single head at a single\nlayer, and <layer>-[<heads>] to describe the cumulative attention of heads at a layer (e.g., 4-[0,3,9]\nto describe the sum of the attention of heads 0, 3, and 9 at layer 4). Note that we refer to layers and\nheads as 0 indexed in the tool.\n4.1 Behind the mask\nWe begin by masking the “escape” token in the example sentence at layer 1 and search what\ninformation is behind the “[MASK]” token’s embedding. This setup is shown in Figure 1. Note\nthat at this early layer, the matching embeddings are most similar to punctuation (PUNCT) and\ndeterminants (DET), which are the most common tokens in English (Figure 1f). Additionally, the\nmaximum attention out of the MASKed token points to itself (Figure 1c). We can observe that there\nis no meaningful linguistic information encoded in the mask’s embedding at this layer.\nAs layers progress, more POS information is added to the token embedding. The summarized layer 5\nsearch results (Figure 2a) show that we start to see some verb information creep into the embedding;\nhowever, it is not until layer 6 (Figure 2b) that BERT is conﬁdent about the masked embedding\nbeing a verb. Subsequent layers seem to reﬁne this estimation, with the embedding ﬂirting with the\npossibility of the masked word being an adposition (ADP). At the ﬁnal layer, BERT settles on the\ntoken being a VERB and wants to predict tokens such as “pass”, “mar”, “see”, “hear”, and “breathe”.\nFor the complete progression, see Figure 4 in Appendix A.\n4.2 Behind the heads\nSearching by the masked token’s embedding helps show the information that is captured within\nthe token itself, but it is useful to understand how the heads of the previous layer contributed to\nthat information being encoded in the embedding. Going back to 5-[all], we see that the token\nembedding fails to embrace the masked “escape” token as a verb (Figure 2a). However, a search by\nhead embedding at that point reveals that BERT has already learned to attend to sentence structures\nwhere the most similar tokens in the corpus are verbs (2c). Even at this early layer, it has learned to\nattend to the direct object (DOBJ) of that verb, a dependency that Clark et al. [2019] showed was\n2http://www.gutenberg.org/ebooks/55\n4\nb\na\nc d\ne\nf\ng\n5-[all] 5-3\n5-6 5-[6,9]\n5-[0,4,5,7,8,9]\nFigure 2: Left: searching by token embedding results. Histogram summaries shown at layers 5 (a)\nand 6 (b). Right: histogram summaries of searching by different head selections at layer 5.\nstrongest in head 8-9. Exploring other individual heads at this point for DEP relationships reveals\nthat 5-3 primarily detects the ROOT dependency (2d) while 5-6 detects the AUX dependency (2e).\nThis is useful, but it is not clear how all the heads were able to maximize their attention on the “din”\ntoken and thus detect the DOBJ pattern that was in 18 of the top 50 matches in the search. Inspecting\nall the heads, it is clear that no head individually looks for DOBJ, and therefore that pattern must be\ndetected through a combination of heads. Naively, we can strategically select the heads that maximize\ntheir attention to “din” (5-[0,4,5,7,8,9] shown in Figure 2f), but ﬁnd that these most normally ﬁnd the\nobject of the preposition (POBJ). Further exploration shows that the DOBJ pattern can be detected by\n5-[6,9] (2g), albeit with confusion to the DET dependency. It seems that complex dependencies like\nDOBJ can be detected in the early-middle layers of the model but rely on a combination of heads.\n4.3 More than position\na\n b\n d\nc\nFigure 3: Exploration of positional heads, inspecting the positional head 2-0. The overview in\n(a) shows the head behavior always pointing to the following word, and the search token “of” is\nhighlighted in (b). The matched results are summarized by POS and offset in (c) and by DEP in (d).\nFigure 3a,c conﬁrm that certain heads learn to attend to succeeding or preceding tokens. We call\nthese heads positional headsin that they detect an offset from the current token [Clark et al., 2019].\nThough simple, the positional head can encode important information about the attended-to word.\nSearching by head can reveal how much information from the token embeddings is visible to that\nhead. A brief exploration of the attention in positional head 2-0 shows that the head is truly positional,\nmatching the following word 50/50 times as seen in the lower histogram in Figure 3c. It also seems to\nmatch the POS belonging to the seed token (in this case, “of” is an ADP). The DEP summary at the\nbottom of Figure 3d additionally shows that not only does the head match the POS of the seed token,\nbut it has also learned to look for cases where the word following a preposition is possessive (e.g.,\n5\nhim/her/its). This kind of exploration shows how much information the different attention heads see\nfrom the tokens they attend to.\n5 Conclusion\nIn this paper, we have introduced an interactive visualization, EXBERT, that uses linguistic anno-\ntations, interactive masking, and nearest neighbor search to help revealing an intelligible structure\nabout learned representations in transformer models. We demonstrate the applicability of EXBERT\nto a speciﬁc case study for a BERT model across the Wizard of Oz corpus. The source code and demo\nare available at www.exbert.net, providing the community an opportunity to rapidly experiment\nwith learned Transformer representations and gain a better understanding of what these models learn.\n6 Acknowledgements\nWe thank Jesse Vig for his helpful feedback.\nReferences\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pages 5998–6008, 2017.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP, pages 353–355, 2018.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint arXiv:1905.00537, 2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\nCtrl: A conditional transformer language model for controllable generation. arXiv preprint\narXiv:1909.05858, 2019.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from\ncontext? probing for sentence structure in contextualized word representations. In International\nConference on Learning Representations, 2019a. URL https://openreview.net/forum?id=\nSJzSgnRcKX.\nJesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language\nmodel. CoRR, abs/1906.04284, 2019. URL http://arxiv.org/abs/1906.04284.\n6\nAlessandro Raganto and Jorg Tiedemann. An analysis of encoder representations in transformer-\nbased machine translation. 2018. URL https://www.aclweb.org/anthology/W18-5431. In\nEMNLP Workshop: BlackboxNLP.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. CoRR,\nabs/1905.05950, 2019b. URL http://arxiv.org/abs/1905.05950.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look\nat? an analysis of bert’s attention. CoRR, abs/1906.04341, 2019. URL http://arxiv.org/abs/\n1906.04341.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. CoRR, abs/1905.09418,\n2019. URL http://arxiv.org/abs/1905.09418.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-\nof-the-art natural language processing, 2019.\nJesse Vig. A multiscale visualization of attention in the transformer model. CoRR, abs/1906.05714,\n2019. URL http://arxiv.org/abs/1906.05714.\nGino Brunner, Yang Liu, Damián Pascual, Oliver Richter, and Roger Wattenhofer. On the validity\nof self-attention as explanation in transformer models. 2019. URL https://arxiv.org/abs/\n1908.04211.\nSarthak Jain and Byron C Wallace. Attention is not explanation. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 3543–3556, 2019.\nSarah Wiegreffe and Yuval Pinter. Attention is not not explanation. 2019. URL https://arxiv.\norg/abs/1908.04626.\nHendrik Strobelt, Sebastian Gehrmann, Hanspeter Pﬁster, and Alexander M Rush. LSTMVis: A tool\nfor visual analysis of hidden state dynamics in recurrent neural networks. IEEE transactions on\nvisualization and computer graphics, 24(1):667–676, 2017.\nHendrik Strobelt, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pﬁster, and\nAlexander M. Rush. Seq2seq-vis: A visual debugging tool for sequence-to-sequence models.\nCoRR, abs/1804.09299, 2018. URL http://arxiv.org/abs/1804.09299.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 2019.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. CoRR, abs/1508.07909, 2015. URL http://arxiv.org/abs/1508.07909.\n7\nA Embedding results across layers\n0\n 1\n 2\n 3\n4\n 5\n 6\n 7\n8\n 9\n 10\n 11\n11\nFigure 4: Token embeddings for the “escape” token setup in 4.1 across every layer. The matched\ntokens at the output of the model are shown in the bottom corpus inspector view, whereas summaries\nare shown for all the other layers\n8",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6431846618652344
    },
    {
      "name": "Computer science",
      "score": 0.5323313474655151
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3489486277103424
    },
    {
      "name": "Engineering",
      "score": 0.23775097727775574
    },
    {
      "name": "Electrical engineering",
      "score": 0.09833163022994995
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    }
  ]
}