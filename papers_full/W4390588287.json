{
  "title": "Language-controllable programmable metasurface empowered by large language models",
  "url": "https://openalex.org/W4390588287",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2227241554",
      "name": "Sheng-guo Hu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2107572868",
      "name": "Jiawen Xu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2125506863",
      "name": "Mingyi Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1821221005",
      "name": "Tie Jun Cui",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2120899363",
      "name": "Lianlin Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2227241554",
      "name": "Sheng-guo Hu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2107572868",
      "name": "Jiawen Xu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2125506863",
      "name": "Mingyi Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1821221005",
      "name": "Tie Jun Cui",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2120899363",
      "name": "Lianlin Li",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2914180762",
    "https://openalex.org/W4385638864",
    "https://openalex.org/W2886728826",
    "https://openalex.org/W2909695780",
    "https://openalex.org/W2759897119",
    "https://openalex.org/W3005705023",
    "https://openalex.org/W3002512653",
    "https://openalex.org/W3005347223",
    "https://openalex.org/W3164896102",
    "https://openalex.org/W3094613760",
    "https://openalex.org/W4229073893",
    "https://openalex.org/W3189447947",
    "https://openalex.org/W2980849908",
    "https://openalex.org/W2075074378",
    "https://openalex.org/W1992865536",
    "https://openalex.org/W2082432933",
    "https://openalex.org/W2982363638",
    "https://openalex.org/W4309908775",
    "https://openalex.org/W4206256887",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W6854692045",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4381827075",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4377864377",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4200632697",
    "https://openalex.org/W4382195993",
    "https://openalex.org/W3106334112"
  ],
  "abstract": "Abstract Programmable metasurface has become a prominent tool in various areas including control, communication, computing, and so on, due to its unique capability in the electromagnetic (EM) manipulation. However, it is lack of the intelligence in the sense that it usually requires the manual intervention, and thus makes it hard to behavior as the human process. To endow the programmable metasurface with the intelligence, we here proposed the concept of the language-controllable programmable metasurface for autonomous EM manipulations by exploring the notable capability of large language models (LLMs) in attaining the human-like intelligence. We have established a proof-of-principle system of language-controllable programmable metasurface, where, for illustration, the programmable metasurface is designed to have 32 √ó 24 binary electronically controllable meta-atoms and work at around 5.5 GHz. In addition, we have constructed a visual-semantic map to facilitate the language-controllable EM manipulation in three-dimensional (3D) physical environments. We have experimentally demonstrated that our language-controllable programmable metasurface is capable of decomposing autonomously an ambiguous task of EM manipulation into a sequence of executable ones and implementing them individually in real-world indoor settings. We expect that the presented strategy could hold promising potential in pushing programmable metasurfaces towards human-level autonomous agents, which are capable of accomplishing the smart EM-involved multi-modality manipulations through self-directed planning and actions.",
  "full_text": "Nanophotonics 2024; 13(12): 2213‚Äì2222\nResearch Article\nShengguo Hu, Jiawen Xu, Mingyi Li, Tie Jun Cui and Lianlin Li*\nLanguage-controllable programmable\nmetasurface empowered by large\nlanguage models\nhttps://doi.org/10.1515/nanoph-2023-0646\nReceived October 1, 2023; accepted December 4, 2023;\npublished online January 4, 2024\nAbstract: Programmablemetasurfacehasbecomeapromi-\nnent tool in various areas including control, communica-\ntion, computing, and so on, due to its unique capability\nin the electromagnetic (EM) manipulation. However, it is\nlackoftheintelligence inthesense thatitusuallyrequires\nthemanualintervention,andthusmakesithardtobehav-\nior as the human process. To endow the programmable\nmetasurface with the intelligence, we here proposed the\nconcept of the language-controllable programmable meta-\nsurface for autonomous EM manipulations by exploring\nthe notable capability of large language models (LLMs)\nin attaining the human-like intelligence. We have estab-\nlished a proof-of-principle system of language-controllable\nprogrammable metasurface, where, for illustration, the\nprogrammable metasurface is designed to have 32\n√ó 24\nbinary electronically controllable meta-atoms and work at\naround 5.5GHz. In addition, we have constructed a visual-\nsemantic map to facilitate the language-controllable EM\nmanipulation in three-dimensional (3D) physical environ-\nments. We have experimentally demonstrated that our\nlanguage-controllable programmable metasurface is capa-\nble of decomposing autonomously an ambiguous task of\nShengguo Hu and Jiawen Xu contributed equally to this work.\n*Corresponding author: Lianlin Li , State Key Laboratory of Advanced\nOptical Communication Systems and Networks, School of Electron-\nics, Peking University, Beijing 100871, China; and Pazhou Laboratory\n(Huangpu), Guangzhou, Guangdong 510555, China,\nE-mail: lianlin.li@pku.edu.cn. https://orcid.org/0000-0002-2295-4425\nShengguo Hu , Jiawen Xu and Mingyi Li , State Key Laboratory of\nAdvanced Optical Communication Systems and Networks, School of Elec-\ntronics, Peking University, Beijing 100871, China\nTie Jun Cui , State Key Laboratory of Millimeter Waves, Southeast Uni-\nversity, Nanjing 210096, China; and Pazhou Laboratory (Huangpu),\nGuangzhou, Guangdong 510555, China. https://orcid.org/0000-0002-\n5862-1497\nEM manipulation into a sequence of executable ones and\nimplementing them individually in real-world indoor set-\ntings. We expect that the presented strategy could hold\npromising potential in pushing programmable metasur-\nfaces towards human-level autonomous agents, which are\ncapable of accomplishing the smart EM-involved multi-\nmodalitymanipulationsthroughself-directedplanningand\nactions.\nKeywords: programmable metasurface; large language\nmodels;EMmanipulation\n1 Introduction\nProgrammablemetasurface,anultrathinengineeredstruc-\nture, has been well recognized as a prominent agent for\nthe Ô¨Çexible electromagnetic (EM) manipulation [1‚Äì4], and\nhas found its valuable applications in various areas, such\nas, control [5, 6], communication [7‚Äì9], computing [10],\na n ds oo n[11‚Äì13]. Compared with its ancestors, i.e., ear-\nlier versions of metamaterials and metasurfaces [14, 15],\ntheprogrammablemetasurfaceexhibitsatechnologytrend\ntowardsaself-organizingEMagentthroughtheembedded\nactive components [16], sensors [17]a n da l g o r i t h m s[18].\nThereby,theprogrammablemetasurfaceholdsthepromis-\ning capabilities in conÔ¨Åguring and optimizing its conÔ¨Ågu-\nration and operational parameters according to environ-\nment settings and user‚Äôs demands. However, it is lack of\ntheintelligenceinthesensethatthecontroloperationsare\nrealized from the manual inputs and trained with limited\ndataset in a relatively isolated environment, and the per-\nformance is largely dependent on predeÔ¨Åned operational\ncondition and corresponding conÔ¨Ågurations. In addition,\nthe programmable metasurface is usually developed for\nsolvingdedicatedsimpleproblems,andisinefficientindeal-\ningwithemergingcomplexEM-relatedmanipulationtasks.\nForinstance,theso-calledsmartmetasurfaceequippedwith\nsensorsmayundertakesomeself-adaptivefunctionsbutthe\nfunctionsmustbeprogrammedandloadedinadvance[ 19],\nOpen Access. ¬© 2023 the author(s), published by De Gruyter. This work is licensed under the Creative Commons Attribution 4.0 International License.\n2214 ‚Äî S. Hu et al.: Language-controllable programmable metasurface empowered by large language\nwhich cannot directly represent the will of the operator in\nrealtime.\nRecently, large language models (LLMs), for instance,\ngenerativepre-trainedtransformer(GPT)[ 20],bidirectional\nencoderrepresentationfromtransformer(BERT)[ 21],large\nlanguage model Meta AI (LLaMA) [22], Falcon LLM [23],\nto name a few, have achieved notable successes, demon-\nstrating the remarkable potential in attaining the human-\nlevel intelligence [24]. These models are trained over the\nweb-scale dataset through so-called self-learning strategy\n[25], and are enabled with the zero-shot inference, gen-\nerative, conversation, common sense understanding, and\nprediction capabilities. Thereby, they are revolutionarily\nshaping the technologies to make them attain the human-\nlevel inference and decision-making capabilities [26‚Äì28].\nFor instance, HuggingGPT can interact with users, under-\nstand their goals and accomplish complex multimodality\ntasksbyexploringtheconversationcapabilityofLLMswith\nexternalmodelsonHuggingFace[ 29].LMA3,anLLM-based\nself-driven agent developed by Colas etal., is capable of\nautonomouslysettinggoalsforitself,andgraduallyimprov-\ningitscapabilitybyexploringthesurroundingenvironment\n[30].SayCan,developedbyAhnetal.,focusoninvestigating\na wide range of manipulation and navigation skills utiliz-\ning a mobile manipulator robot [31]. Of course, we here\nanticipatethattheLLMcanenabletheprogrammablemeta-\nsurfacetounderstandandinteractwiththecross-modality\nenvironment, evolving towards a human-like autonomous\nEM agent. Furthermore, we expect that LLMs can enable\nprogrammablemetasurfacestoenjoythepredictabilityfea-\nture,andhence,realizeimprovedandproactivecapabilities\nof dynamic EM manipulations in changing environments,\nsuchas,localization,beamforming,powerallocation,cloak-\ning,aswellas,spectrummanagement,evenfortheunseen\nscenarios.\nIn this article, we have proposed the concept of the\nlanguage-controllable programmable metasurface, which\nhasthecapabilityofaccomplishingself-directedEMmanip-\nulation tasks like the human process. For this reason, we\nwouldliketorefertoitasprogrammablemetasurfaceagent.\nWehaveestablishedaproof-of-principle,anddemonstrated\nexperimentallyitscapabilityofhuman-levelEMmanipula-\ntion in four aspects. First, it is capable of decomposing a\nrelatively complex EM manipulation task into a sequence\nofsimpleronesbyemployingtheself-planningcapabilityof\nLLMs, and implementing them individually. Second, it can\nexplore the unknown environment, and continually reÔ¨Åne\nthe execution code of EM manipulation based on environ-\nment feedback through trial and error. Third, the actions\nare taken to communicate with the other agents or real\nhumansforsharinginformationorcollaboration.Then,we\ncanenvisionthatthepresentedprogrammablemetasurface\nagentwillbecomeanincubatortoformmorefuturesmart\ndevicesandsystems.\n2 System configuration\nand operational principle\nIn this section, we will elaborate on the conÔ¨Åguration\nand operation of the proposed language-controllable pro-\ngrammable metasurface. In a nutshell, one key motivation\nbehind our strategy is to equip LLMs with crucial human-\nleveldecision-makingcapabilitytomaketheprogrammable\nmetasurfacebehavelikehumansandcompletevariousEM\nmanipulationtaskseffectively.Forillustrationpurpose,the\nprogrammable metasurface agent is designed to serve as\nanartiÔ¨Åcialintelligent(AI)assistantinsmarthuman-robot\nhomescenarioswhoismainlyresponsibleforunderstand-\ning the user‚Äôs language requests and accomplishing the\nintendedEMmanipulationtasks,asillustratedin Figure1a.\nFor example, when the metasurface robot receives the\nuser‚Äôs request, e.g., ‚Äúsend this Ô¨Åle to the computer‚Äù, it can\nautomaticallyunderstandthattaskanddecomposesitinto\na series of simple subtasks and implement them in a step-\nby-stepway,asdetailedbelow.\n2.1 System configuration\nHere, we discuss the system conÔ¨Åguration of the pro-\nposed programmable metasurface agent in detail. As\nshown in Figure1b and c , the whole system is com-\nposed of an LLM module as its central controller, a large-\naperture programmable metasurface as the EM manipu-\nlation module in physical environment, and a host com-\nputer for the data postprocessing. In addition, a micro\ncontrol unit (MCU) has been integrated into the pro-\ngrammable metasurface, which is responsible for convert-\ning the control coding sequence from the host computer\nto programmable metasurface, and a low-cost commer-\ncial software-deÔ¨Åned radio device (Ettus USRP X310) has\nalso been introduced for receiving feedback radio signals\nthrough connected antennas from surrounding environ-\nment. Here, both the USRP and metasurface communi-\ncate with the host computer via the Ethernet under the\ntransmission control protocol (TCP). In our implementa-\ntions, the programmable metasurface agent is designed\nto operate at around 5.5GHz band, which could be read-\nily scalable to other frequency bands. The programmable\nmetasurface is composed of 32\n√ó 24 binary electronically\nS. Hu et al.: Language-controllable programmable metasurface empowered by large language ‚Äî 2215\nFigure 1: System configuration of the proposed programmable metasurface agent. (a, c) Conceptual illustration of programmable metasurface agent\nin indoor physical environment, where users are allowed to interact with the programmable metasurface agent through natural language voice or\ntext. The programmable metasurface robot is capable of understanding the user‚Äôs intention, autonomous task planning and decomposition, and\naction (i.e., EM manipulations here) in 3D physical environment. (b) Photograph of experimental system configuration which consists of a program-\nmable metasurface, MCU, host computer, USRP, and antennas. (d) Photograph of programmable metasurface with size of 780 mm\n√ó560 mm, where\nsome details of meta-atom have been provided as well. (e) The EM performance of the electronically-controllable meta-atoms numerically and\nexperimentally (f) workflow of developed programmable metasurface agent. Here, a 3D visual-semantic map has been provided as well.\ncontrollable meta-atoms with size of 0.780m√ó 0.585m,\nas shown inFigure1d, where the meta-atom‚Äôs details are\nalsoreported.Notethatthemeta-atomhasbinarystates,in\nthesensethatthereÔ¨Çectionphaseresponsechangesby180 ‚ó¶\nnear5.5GHz,whiletheamplituderemainsalmostconstant\nwhen the soldiered PIN diode is switched from OFF (ON)\nto ON (OFF). Here, the state of the PIN diode is controlled\nby an FPGA-based 50MHz clock MCU. We have conducted\nnumericalandexperimentalteststoexaminethebasicEM\nperformance of the designed one-bit meta-atom, and the\n2216 ‚Äî S. Hu et al.: Language-controllable programmable metasurface empowered by large language\ncorresponding results have been presented inFigure1e.\nHere, we have performed full-wave simulations using a\ncommercial full-wave EM simulator (CST Microwave Tran-\nsient Simulation Package 2017), and performed the exper-\nimental test directly in our lab environment. As shown in\nFigure1e, we can readily observe that the reÔ¨Çected phase\nofthemeta-atomsundergoesa180 ‚ó¶ phasedifferencewhen\nthePINdiodeisswitchedfromON(OFF)toOFF(ON)inthe\nselected frequency range. Then, with massive above meta-\natoms and suitable control coding sequences, we expect\nthat the programmable metasurface agent is capable of\nautonomouslysolvingarangeofcomplexEMmanipulation\ntasks. More details about the programmable metasurface\ncanbefoundin SupplementaryNote1 .\n2.2 Workflow of language-controllable\nprogrammable metasurface\nAs mentioned previously, the programmable metasurface\nagent relies on two critical factors: (i) the powerful EM\nmanipulation capability of programmable metasurface in\nphysical environment, and (ii) the strong capabilities of\ninference, zero-sample learning, task decomposition, and\ncode generation of LLMs in virtual digital world. Then,\nthe integration of programmable metasurface with LLMs\nenables to achieve the autonomous EM manipulation and\naccomplish the complex tasks requested by the user. As\nmentioned above, a 3D visual-semantic map has been\nestablishedtofacilitatetheEMmanipulationinreal-world\nphysicalenvironmentsusingthelanguage-controllablepro-\ngrammable metasurface. As shown inFigure 1f,i nt h e\n3D visual-semantic map, the whole environment has been\nsemantically segmented and their semantics are marked\nin different colors, where the objects in the environment\nare all modeled as cubic, and each object was assigned a\nlocation with respect to the global coordinate system. For\nthe efficiency consideration, the 3D visual-semantic map\nhas been created in advance as the prompt for the LLM to\nsearchforthelocationsofintendedsubjectsinthephysical\nenvironment.\nNow, we will elaborate on the four-step workÔ¨Çow of\nprogrammable metasurface agent, as shown inFigure1f.\nFirst, a user can interact with the programmable metasur-\nface in voice or text format through the LLMs installed in\nthehostcomputer.Forinstance,whentheLLMreceivesthe\nuser‚Äôsrequest,forinstance,‚Äúpleaseilluminatethebox_2‚Äù,it\nwilldecomposetheoriginalambiguoustaskintoasequence\nof simple but implementable subtasks, and then generate\nthe implementable Python codes for subtasks. Second, the\nLLM will calculate the exact location of ‚Äúbox_2‚Äù in context\nofthecreated3Dvisual-semanticmap.Third,thehostcom-\nputer runs the Python code to generate the control coding\nsequence of metasurface such that the resultant radiation\nbeam can be focused towards box_2. Finally, the host com-\nputerwillrespondtotheuserwithavoiceortextmessage\nthatsays,i.e.,‚ÄúBeamfocushasbeensuccessfullycompleted.‚Äù\nItisworthyofmentioningthatifthesubjectdoesnotexist\nin the 3D visual-semantic map, the programmable meta-\nsurface agent can autonomously explore the unseen envi-\nronment and continuously improve the execution Python\ncodeforthesmartEMmanipulationthroughthetrial-and-\nerrorapproach,asdemonstratedin Section3.3.Tosumma-\nrize, we can conclude that one can interact with the pro-\ngrammable metasurface agent in a free manner, enabling\ntheautonomousEMmanipulationinreal-worldsettings.\n3 Experimental results\nIn this section, we will experimentally demonstrate the\nautonomousEMmanipulationcapabilitiesofthedeveloped\nprogrammable metasurface agent in four aspects. First,\nwe will look into the language-controllable beam-focusing\ncapability of the programmable metasurface agent. After-\nwards, we will demonstrate the self-planning capability\nof the programmable metasurface agent in accomplishing\ncomplexEMmanipulationtasks,andtheactiveexploration\nability in dealing with the unknown environment. Finally,\nwe consider a relatively realistic scenario, i.e., wireless\ninformation sharing, to demonstrate the promising poten-\ntial of programmable metasurface agent for the wireless\nchattingwithotheragentsorhuman.\n3.1 Results of the language-controllable\nbeam focusing\nAs the Ô¨Årst set of fundamental examinations, we would\nliketoprovidesomeinsightsintothelanguage-controllable\nEM beam manipulation of programmable metasurface\nagent, and corresponding experimental results have been\nreported inFigure2. In our experiments, we have placed\nthreeboxesforthebeamfocusingtargetsinourlabenviron-\nment (seeFigure2a), and ask the programmable metasur-\nfaceagenttoaccomplishbothsingle-targetandmulti-target\nbeamfocusingtaskswhenitreceiveslanguagerequest,i.e.,\n‚ÄòpleasefocusthebeamtowardsBox-1‚Äô. Figure2b reportsthe\ncalculated control coding pattern [32]o ft h em e t a s u r f a c e\nfor single-target beam focusing in the left, and the corre-\nsponding 2D distribution of signal intensity in the right.\nHere,the2Ddistributionhasbeenachievedviatheso-called\nplane scanning technique, where the scanning area with\nS. Hu et al.: Language-controllable programmable metasurface empowered by large language ‚Äî 2217\na) b)\n Coding pattern\nbox_1\nDistribution of signal intensity\n1.45m\n1.05m\nFocus the beam at box_1.\nc)\nbox_1box_2\nFocus the beam at box_1 and\nbox_2.\nd)\nbox_1box_2\nFocus the beam at box_1, box_2 \nand box_3.\nbox_3\n10\nmetasurface\nFigure 2: Experimental results of language-controllable beam focusing. (a) Experimental setup, (b) single-target focus scenario, (c) dual-target focus\nscenario, (d) triple-target focus scenario.\nsize of 1.45m √ó 1.05m is set as a plane parallel to the\nmetasurface, and has the distance of 1.53m away from\nthe programmable metasurface. Further, we have consid-\nered the scenario for the dual target- and triple-target\nbeam focusing tasks, and reported corresponding results\nin Figure2c and d, respectively. Then, it can be readily\nobservedthattheprogrammablemeatsurfaceagentiscapa-\nble of allocating the wireless energy towards the intended\ntargetsinalanguage-controllableinteractionmanner.\n3.2 Results of self-planning complex EM\nmanipulation\nIn this section, we would like to examine the self-planning\ncapability of programmable metasurface agent for rela-\ntively complex EM manipulation tasks. To this end, we\nhave explored so-called zero-sample learning strategy of\nLLMs, conducted a set of tests with the GPT-3.5-turbo LLM\nmodel released by OpenAI, and loaded the pre-established\n3Dvisual-semanticmapofenvironmentintoLLMinformof\nprompttoenableLLMtogetacquittancewiththesurround-\ning environment. See examples about the prompt project\ninSupplementaryNote2 .Figure3a reportsarelativelysim-\nplecasewhereauseraskstheprogrammablemetasurface\nagenttofocusradiationbeamtowardsachairthroughthe\ntext input, and the agent will generate the corresponding\nPython codes and execute them step by step. In particular,\nthe agent will decompose the original ambiguous request\nintotwoimplementablesubtasks:(i)searchingfortheloca-\ntionofthechairinthevisualsemanticmap,(ii)focusingthe\nradiationbeam towards the intended chair. When the host\ncomputer completes the code generation, it will generate\nthecontrolcodingsequencesofprogrammablemetasurface\nfor the intended beam manipulation, and the MCU will\ncontrol the programmable metasurface to manipulate the\nradiation beam. We have conducted another set of more\ncomplex EM manipulation task, and reported the results\nin Figure 3b, where our major concern is to examine the\ngeneralizabilityofcomprehensionandambiguouslanguage\ninstructions.Tothisend,theuserdoesnotexplicitlyspecify\nthe object on which the beam needs to be directed, but\nratherrequeststhatitbefocusedtowardseachboxinturn.\nWe have selected 8 sets of user command tasks to test the\ndevelopedprogrammablemetasurfaceagent,andrepeated\neachsetoftasks100timesandrecordedthesuccessrateof\ntaskdecompositionandcodegenerationin Figure3c.Itcan\nbe observed that the programmable metasurface agent is\ncapableofunderstandingtheuser‚Äôsrequestandcompleting\nthetasksuccessfully.Notethatthesuccessrateoftaskcom-\npletiondecreasesgradually,butitstillcanmeetthedemand,\nas the complexity of the task increases. We can see from\naboveresultsthattheprogrammablemetasurfaceagenthas\n2218 ‚Äî S. Hu et al.: Language-controllable programmable metasurface empowered by large language\na)\nb)\nc)\nFigure 3: Experimental results of autonomous task decomposition. (a) When a user sends a request in a linguistic way, the agent can decompose the\ntask based on LLM and then generate the execution code corresponding to each step, which is executed one by one. For instance, when user\ncommand is ‚Äúfocus the beam on the chair‚Äù, and the system decomposes the task into two steps: the first step is to search for the location of the ‚Äúchair‚Äù\nfrom the semantic map, and the second step is to focus the beam on that location. (b) Another more complex task with the same steps as (a).\n(c) The success rates for 8 different EM manipulations tasks with different complexities in performing the task decomposition (TD) and code\ngeneration (CG) over 100 repetitions for each task.\nthe self-planning capability in the sense of decomposing a\nrelativelyambiguousEMmanipulationtaskintoasequence\nofexecutabletasks.\n3.3 Results of exploring the unknown\nenvironment\nNote that we have implicitly assumed that the surround-\ning physical environment, i.e., wireless channel, has been\nknownfortheprogrammablemetasurfaceagentabove,and\nthe control coding pattern of progammable metasurface\ncan be explicitly obtained in advance once the intended\nuser is localized. However, in most of practical applica-\ntions, the surrounding environment is usually hard to be\nestimatedinadvanceforprogammablemetasurfaceagent,\nespeciallywhenthelocationofthetargetuserisinaccessi-\nble, for example, if there are obstacles between the target\nand metasurface. To resolve this issue, we here would like\nto utilize the reinforcement learning-based online search\nalgorithminRef.[ 33]toautomaticallyadjustthecontrolcod-\ningpatternthroughtheactiveinteractionwiththeunknown\nenvironment. As we show inFigure4a and b,t h ep r o -\ngrammable metasurface agent takes a so-called trial-and-\nerror approach to adjust its coding pattern according to\nthe feedback from the surrounding environment. In par-\nticular,theprogrammablemetasurfaceagentrunsathree-\nlayer artiÔ¨Åcial neural network to make decision to adjust\nitscontrolcodingpattern,andcorrespondinglychangesthe\nEMÔ¨Åelddistributionoftheunknownenvironmentandthe\nS. Hu et al.: Language-controllable programmable metasurface empowered by large language ‚Äî 2219\na) b)\nc)\ne)\nd)\nFigure 4: Experimental results of programmable metasurface agent in exploring unknown environments. (a) Task decomposition, (b) system\ndiagram, (c) the location of the receiver in our experiment, (d) the power of Bob‚Äôs received signal as a function of learning epochs, (e) the optimized\ncontrol coding patterns of programmable metasurface.\nsignalintensityreceivedbytheuser.Afterthat,theuserwill\nmeasureitsreceivedsignalstrength,andfeedbackthemea-\nsurement result to the agent to generate reward function\nforparamentupdatingoftheartiÔ¨Åcialneuralnetwork.The\ndetailed learning process could be summarized as follows:\nanepochconsistsof384steps,ineverysteptheneuralnet-\nwork outputs the probability distribution of state reversal\nforeachunitaccordingtothecurrentcodingstatetoguide\ntheprogrammablemetasurfacetoswitchitscodingpattern,\nthenrecordsthedecisiontraceandfeedbackedrewardsfor\npolicy updates. At the end of each epoch, the agent will\nupdateitspolicy\nùõ©withpolicygradientmethodasweshow\ninEq.(1).Here,weuse a ands todenoteactionsandstates.\nThelearningrate ùõºissetto0.001,thediscountedfactor ùõæ is\nsetto0.99,andthesingle-steprewardfunctionisgenerated\nfrom the received signal power of the user according to\nEq.(2).\nŒò‚àó ‚ÜêŒò+ ùõº\n384‚àë\nl=1\n384‚àë\ni=l\nùõæi‚àílrewardi‚àálogpŒò\n(al|sl\n) (1)\nrewardi = (poweri ‚àípoweri‚àí1\n)\n√óabs(poweri ‚àípoweri‚àí1\n) (2)\nWe have conducted a set of experiments to verify our\nsystem‚Äôs capacity of exploring the unknown environment.\nThe signal intensity as a function of learning epochs has\nbeen plotted inFigure 4d, and corresponding optimized\ncodingpatternshavebeenshownin Figure4e.Inourexper-\niments,werespectivelyplacetheuserindifferentunknown\n2220 ‚Äî S. Hu et al.: Language-controllable programmable metasurface empowered by large language\nlocations to verify the effectiveness and stability of our\nreinforcementlearning-basedonlinesearchalgorithm,and\nsuccessfully improve the power gain in a factor of above\n10dB.Fromthissetofexperimentalresults,wecandemon-\nstratethatthedevelopedprogrammablemetasurfaceagent\nis capable of autonomously exploring the unknown envi-\nronmentandonlineoptimizingitscontrolcodingpatternto\nimprovethecommunication‚Äôsquality.\n3.4 Results of wireless information sharing\nHere,wewillconsideramorerealisticapplicationusingthe\nprogrammablemetasurfaceagent,i.e.,wirelessinformation\nsharing, as shown inFigure5a and b, where Alice wants\nto share an RBG picture of logo of Peking university (see\nFigure5c)toBobinourlabenvironment.Tothatend,Alice\nuploads the picture to the host computer along with a text\nrequest ‚ÄúPlease share this picture with Bob‚Äù. When receiv-\ningthisrequest,theprogrammablemetasurfaceagentwill\ndecompose the ambiguous request into three executable\nsubtasks,andsequentiallydispatchesthesesubtaskstothe\n3Dvisual-semanticmap,metasurfaceandUSRP,asdetailed\nin Figure5a. After receiving the subtask from the LLM,\nthe agent will recognize and localize Bob and exports the\nBob‚Äôs location to the programmable metasurface through\nthe 3D visual-semantic map. Then, the agent adapts the\ncontrol coding pattern (seeFigure5d) in terms of Bob‚Äôs\nlocation such that its radiation beam can be well focused\ne) f) g)\nh) i) j)\nb) c)\nd)\na)\nFigure 5: Experimental results of wireless information sharing with the programmable metasurface agent. (a) Operation procedure and task\ndecomposition, (b) 3D visual-semantic map, (c) the RGB picture transmitted from Alice to Bob, (d) the control coding pattern of programmable\nmetasurface, (e) A 10 4-length signal sent from Alice, (f‚Äìg) the real and imaginary parts of the signal received by Bob when the beam is focused\ntowards Bob using programmable metasurface. (h) The decoded signal at Bob corresponding to that of (e), (i and j) the real and imaginary parts\nof the signal received by Bob when the focusing beam is not available.\nS. Hu et al.: Language-controllable programmable metasurface empowered by large language ‚Äî 2221\ntoward Bob in a remarkably enhanced signal‚Äôs intensity.\nThereby, the picture to be shared can be transmitted from\nAlicetoBobbytheUSRPwiththewirelesscommunication\nlinkenhancedbytheprogrammablemetasurface.TheÔ¨Årst\n10,000 sampling points of raw signal received by Bob has\nbeenshownin Figure5fandg .Aftersomestraightforward\nsignal process, the decoded signal at Bob is able to be suc-\ncessfully recovered with a low bit error rate of 0.179%,\nas shown inFigure5h, implying that the programmable\nmetasurface agent can successfully accomplish the infor-\nmation sharing task. Moreover, in order to highlight the\nimportance of programmable metasurface in the wireless\ninformationsharingtask,wehavereportedthecorrespond-\ning Ô¨Årst 10,000 sampling points of Bob‚Äôs received signal\nwhen the programmable metasurface is not available for\nthe beam focusing inFigure5i and j. We can see that the\nSNR (signal-to-noise) of Bob‚Äôs received signal with the pro-\ngrammable metasurface is signiÔ¨Åcantly higher than that\nwithout the programmable metaurface, which means the\nprogrammable metasurface can signiÔ¨Åcantly improve the\nwirelesslinkwiththetargetuserandthusplaysanimpor-\ntant role in the wireless information sharing. Now, we can\nconclude from above primary experimental results that\nthe developed programmable metasurface agent, with the\nreasoningandunderstandingabilityofLLMandthebeam\nmanipulation capacity of programmable metasurface, is\nable to intelligently understand and decompose informa-\ntionsharingtasksgiveninnaturallanguageformsandauto-\nmaticallyscheduleitssub-devicestoexecutecorresponding\nsubtasks.\n4 Conclusions\nWe have presented the concept of language-controllable\nprogrammable metasurface, i.e., programmable metasur-\nface agent, by combining the power of LLMs in attaint-\ning human-level intelligence with the Ô¨Çexible EM manip-\nulation capability of programmable metasufaces. We have\nestablishedaproof-of-principleprogrammablemetasurface\nagent, and demonstrated experimentally the strength of\nprogrammable metasurface agent in three aspects: (1) the\nself-planningcapabilityinperformingcomplexEMmanipu-\nlationtasks;(2)exploringunknownenvironmentsandcon-\ntinuouslyimprovingtheEMmanipulationthroughtrialand\nerroraccordingtothefeedbackfromsurroundingenviron-\nments;(3)sharinginformationandcollaboratingwithother\nagentsorrealpeople.Weexpectthattheproposedstrategy\ncouldholdpromisingpotentialinadvancingprogrammable\nmetasurfacestowardshuman-levelautonomousagents.\nResearch funding: This work was supported by Develop\nment Program of China (2023YFB3811502 and 2021YFA140\n1002).\nAuthor contributions: All authors have accepted responsi-\nbilityfortheentirecontentofthismanuscriptandapproved\nitssubmission.\nConflict of interest: AuthorsstatenoconÔ¨Çictsofinterest.\nInformed consent: Informedconsentwasobtainedfromall\nindividualsincludedinthisstudy.\nData availability: ThedatathatsupporttheÔ¨Åndingsofthis\nstudyareavailablefromthelastauthoruponrequest.\nReferences\n[1] R. Wu, L. Zhang, L. Bao, et al., ‚ÄúDigital metasurface with phase\ncode and reflection ‚àítransmission amplitude code for flexible\nfull-space electromagnetic manipulations,‚Äù Adv. Opt. Mater. ,v o l .7 ,\np. 1801429, 2019..\n[2] M. Wei, H. Zhao, V. Galdi, L. Li, and T. J. Cui, ‚ÄúMetasurface-enabled\nsmart wireless attacks at the physical layer,‚Äù Nat. Electron.,v o l .6 ,\npp. 610 ‚àí618, 2023..\n[3] E. Arbabi, A. Arbabi, S. M. Kamali, Y. Horie, M. Faraji-Dana, and\nA. Faraon, ‚ÄúMEMS-tunable dielectric metasurface lens,‚Äù Nat.\nCommun.,v o l .9 ,p .8 1 2 ,2 0 1 8 ..\n[ 4 ] T .J .C u i ,S .L i u ,G .B a i ,a n dQ .M a ,‚Äú D i r e c tt r a n s m i s s i o no fd i g i t a l\nmessage via programmable coding metasurface,‚Äù Research,\nvol. 2019, p. 2584509, 2019..\n[5] C. Huang, C. Zhang, J. Yang, B. Sun, B. Zhao, and X. Luo,\n‚ÄúReconfigurable metasurface for multifunctional control of\nelectromagnetic waves,‚Äù Adv. Opt. Mater. ,v o l .5 ,p .1 7 0 0 4 8 5 ,2 0 1 7 .\n.\n[6] N. Zhang, K. Chen, Y. Zheng, et al., ‚ÄúProgrammable coding\nmetasurface for dual-band independent real-time beam control,‚Äù\nIEEE J. Emerg. Sel. Top. Circuits , vol. 10, pp. 20\n‚àí28, 2020..\n[7] H. Zhao, Y. Shuang, M. Wei, T. J. Cui, P. d. Hougne, and L. Li,\n‚ÄúMetasurface-assisted massive backscatter wireless\ncommunication with commodity Wi-Fi signals,‚Äù Nat. Commun.,\nvol. 11, p. 3926, 2020..\n[8] E. Basar, ‚ÄúReconfigurable intelligent surface-based index\nmodulation: a new beyond MIMO paradigm for 6G,‚Äù IEEE Trans.\nCommun., vol. 68, pp. 3187 ‚àí3196, 2020..\n[9] X. Wang, Z. Fei, Z. Zheng, and J. Guo, ‚ÄúJoint waveform design and\npassive beamforming for RIS-assisted dual-functional\nradar-communication system,‚Äù IEEE Trans. Veh. Technol. ,v o l .7 0 ,\npp. 5131\n‚àí5136, 2021..\n[10] F. Zangeneh-Nejad, D. Sounas, A. Al√π, and R. Fleury, ‚ÄúAnalogue\ncomputing with metamaterials,‚Äù Nat. Rev. Mater. ,v o l .6 ,\npp. 207 ‚àí225, 2021..\n[11] L. Li, H. Zhao, and T. J. Cui, ‚ÄúIntelligent metasurfaces: control,\ncommunication and computing,‚Äù Elight,v o l .2 ,p .7 ,2 0 2 2 ..\n[12] A. Elzanaty, A. Guerra, F. Guidi, and M. Alouini, ‚ÄúReconfigurable\nintelligent surfaces for localization: position and orientation error\nbounds,‚ÄùIEEE Trans. Signal Process. ,v o l .6 9 ,p p .5 3 8 6\n‚àí5402, 2021..\n[13] L. Li, Y. Shuang, Q. Ma, et al., ‚ÄúIntelligent metasurface imager and\nrecognizer,‚ÄùLight: Sci. Appl. , vol. 8, p. 97, 2019. .\n[14] N. Garcia, E. V. Ponizovskaya, and J. Q. Xiao, ‚ÄúZero permittivity\nmaterials: band gaps at the visible,‚Äù Appl. Phys. Lett. ,v o l .8 0 ,\npp. 1120 ‚àí1122, 2002..\n2222 ‚Äî S. Hu et al.: Language-controllable programmable metasurface empowered by large language\n[15] D. Schurig, J. Mock, B. Justice, et al., ‚ÄúMetamaterial\nelectromagnetic cloak at microwave frequencies,‚Äù Science,v o l .3 1 4 ,\npp. 977 ‚àí980, 2006..\n[16] T. J. Cui, M. Qi, X. Wan, J. Zhao, and Q. Cheng, ‚ÄúCoding\nmetamaterials, digital metamaterials and programmable\nmetamaterials,‚ÄùLight: Sci. Appl. , vol. 3, pp. e218, 2014.\n.\n[ 1 7 ] Q .M a ,G .D .B a i ,H .B .J i n g ,C .Y a n g ,L .L i ,a n dT .J .C u i ,‚Äú S m a r t\nmetasurface with self-adaptively reprogrammable functions,‚Äù\nLight: Sci. Appl. , vol. 8, p. 98, 2019. .\n[18] H. Zhao, S. Hu, H. Zhang, et al., ‚ÄúIntelligent indoor metasurface\nrobotics,‚ÄùNatl. Sci. Rev. , vol. 10, p. nwac266, 2023. .\n[19] Z. Wang, H. Zhang, H. Zhao, and T. J. Cui, ‚ÄúIntelligent\nelectromagnetic metasurface camera: system design and\nexperimental results,‚Äù Nanophotonics, vol. 11, pp. 2011 ‚àí2024,\n2022..\n[20] T. Brown, B. Mann, N. Ryder, et al., ‚ÄúLanguage models are\nfew-shot learners,‚Äù NIPS, vol. 33, pp. 1877 ‚àí1901, 2020.\n[21] J. Devlin, M. W. Chang, K. Lee, et al., ‚ÄúBert: pre-training of deep\nbidirectional transformers for language understanding,‚Äù 2018,\narXiv preprint arXiv:1810.04805.\n[22] H. Touvron, T. Lavril, G. Izacard, et al., ‚ÄúLlama: open and efficient\nfoundation language models,‚Äù 2023, arXiv preprint\narXiv:2302.13971.\n[23] G. Penedo, Q. Malartic, D. Hesslow, et al., ‚ÄúThe refinedweb dataset\nfor falcon LLM: outperforming curated corpora with web data, and\nweb data only,‚Äù 2023, arXiv preprint arXiv:2306.01116.\n[24] Y. Chang, X. Wang, J. Wang, et al., ‚ÄúA survey on evaluation of large\nlanguage models,‚Äù 2023, arXiv preprint arXiv:2307.03109.\n[25] A. Radford, K. Narasimhan, T. Salimans, et al., ‚ÄúImproving\nLanguage Understanding by Generative Pre-training,‚Äù 2018.\nAvailable at: https://blog.openai.com/language-unsupervised.\n[26] Z. Xi, W. Chen, X. Guo, et al., ‚ÄúThe rise and potential of large\nlanguage model based agents: a survey,‚Äù 2023, arXiv preprint\narXiv:2309.07864.\n[27] L. Wong, G. Grand, A. K. Lew, et al., ‚ÄúFrom word models to world\nmodels: translating from natural language to the probabilistic\nlanguage of thought,‚Äù 2023, arXiv preprint arXiv:2306.12672.\n[28] R. Liu, J. Wei, S. S. Gu, et al., ‚ÄúMind‚Äôs eye: grounded language\nmodel reasoning through simulation,‚Äù 2022, arXiv preprint\narXiv:2210.05359.\n[29] Y. Shen, K. Song, X. Tan, et al., ‚ÄúHugginggpt: solving ai tasks with\nchatgpt and its friends in huggingface,‚Äù 2023, arXiv preprint\narXiv:2303.17580.\n[30] C. Colas, L. Teodorescu, P.-Y. Oudeyer, et al., ‚ÄúAugmenting\nautotelic agents with large language models,‚Äù 2023, arXiv preprint\narXiv:2305.12487.\n[31] A. Michael, A. Brohan, N. Brown, et al., ‚ÄúDo as i can, not as i say:\ngrounding language in robotic affordances,‚Äù 2022, arXiv preprint\narXiv:2204.01691.\n[32] Y. Zhang, K. Shen, S. Ren, X. Li, X. Chen, and Z. Q. Luo, ‚ÄúConfiguring\nintelligent reflecting surface with performance guarantees:\noptimal beamforming,‚Äù IEEE J. Sel. Top. Signal Process , vol. 16,\npp. 967\n‚àí979, 2022..\n[33] J. Xu, R. Zhang, J. Ma, et al., ‚ÄúIn-situ manipulation of wireless link\nwith reinforcement-learning-driven programmable metasurface in\nindoor environment,‚Äù JIIS, vol. 1, pp. 217\n‚àí227, 2023.\nSupplementary Material: This article contains supplementary material\n(https://doi.org/10.1515/nanoph-2023-0646 ).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5770857334136963
    },
    {
      "name": "Computer architecture",
      "score": 0.3938068151473999
    }
  ]
}