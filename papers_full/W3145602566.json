{
  "title": "A Practical Survey on Faster and Lighter Transformers",
  "url": "https://openalex.org/W3145602566",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3194083670",
      "name": "Fournier Quentin",
      "affiliations": [
        "Polytechnique MontrÃ©al"
      ]
    },
    {
      "id": "https://openalex.org/A4294576259",
      "name": "Caron, Gaetan Marceau",
      "affiliations": [
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    },
    {
      "id": "https://openalex.org/A3195851319",
      "name": "Aloise, Daniel",
      "affiliations": [
        "Polytechnique MontrÃ©al"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6776048684",
    "https://openalex.org/W1583776211",
    "https://openalex.org/W6703652217",
    "https://openalex.org/W6761628794",
    "https://openalex.org/W6753278433",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3031696893",
    "https://openalex.org/W6768501859",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6783522565",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W6764072591",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3127742036",
    "https://openalex.org/W3108617663",
    "https://openalex.org/W6796417832",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3037660342",
    "https://openalex.org/W3133264589",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2997517014",
    "https://openalex.org/W3136363192",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W3037032032",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W6783267081",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3049039618",
    "https://openalex.org/W6774082070",
    "https://openalex.org/W6779163297",
    "https://openalex.org/W6752342493",
    "https://openalex.org/W3128976935",
    "https://openalex.org/W6768807518",
    "https://openalex.org/W4288335579",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3007328579",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W1506514354",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3104527631",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3205284814",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3145511196",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2964212578",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W4287827005",
    "https://openalex.org/W4298436404",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3005714399",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3127433878",
    "https://openalex.org/W2963684275",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963399222",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3136541843",
    "https://openalex.org/W2976023236",
    "https://openalex.org/W2995983533",
    "https://openalex.org/W4287816361",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4287324206",
    "https://openalex.org/W3103754749",
    "https://openalex.org/W2122948532",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2885311373",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2270190199",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2134797427",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2951025380",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W2114766824",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W1613249581"
  ],
  "abstract": "Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the modelsâ€™ efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformerâ€™s limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for researchers and practitioners to determine which methods to apply in practice to meet the desired tradeoff between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make Transformers faster and lighter and by providing a comprehensive explanation of the methodsâ€™ strengths, limitations, and underlying assumptions.",
  "full_text": "A Practical Survey on Faster and Lighter Transformers\nQUENTIN FOURNIER, Polytechnique MontrÃ©al, Canada\nGAÃ‰TAN MARCEAU CARON, Mila - Quebec AI Institute, Canada\nDANIEL ALOISE, Polytechnique MontrÃ©al, Canada\nRecurrent neural networks are effective models to process sequences. However, they are unable to learn\nlong-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced\nthe Transformer, a model solely based on the attention mechanism that is able to relate any two positions\nof the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the\nstate-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of\na quadratic computational and memory complexity with respect to the sequence length, hindering its adoption.\nFortunately, the deep learning community has always been interested in improving the modelsâ€™ efficiency,\nleading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge\ndistillation. Recently, researchers have directly addressed the Transformerâ€™s limitation by designing lower-\ncomplexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide\nrange of solutions, it has become challenging for researchers and practitioners to determine which methods to\napply in practice in order to meet the desired trade-off between capacity, computation, and memory. This\nsurvey addresses this issue by investigating popular approaches to make Transformers faster and lighter and\nby providing a comprehensive explanation of the methodsâ€™ strengths, limitations, and underlying assumptions.\nCCS Concepts: â€¢ Computing methodologies â†’Neural networks.\nAdditional Key Words and Phrases: Deep Learning, Efficient Transformer, Self-Attention, Survey\n1 INTRODUCTION\nSequences arise naturally in a wide range of domains, notably in natural language, biology, and\nsoftware executions. Rumelhart et al. [97] introduced a family of models called recurrent neural\nnetworks (RNNs) based on the idea of parameter sharing to process variable-length sequences.\nGiven an input sequence ğ‘¿ comprising ğ‘›tokens ğ’™(ğ‘–)of dimension ğ‘‘, recurrent neural networks\niteratively construct a sequence of hidden representations ğ’‰(ğ‘–)and produce a sequence of outputs\nğ’š(ğ‘–)as illustrated in Figure 1. Unfortunately, vanilla RNNs often suffer from vanishing or exploding\ngradients, which prevent them from learning long-term dependencies. Hochreiter and Schmidhuber\n[44] addressed this limitation with the now widely popular long short-term memory (LSTM)\nnetwork, which circumvents the gradient issues with paths through time. Cho et al . [17] later\nimproved over the LSTM with the simpler gated recurrent unit (GRU).\n... \nFig. 1. The computational graph of a recurrent neural network. The input and output sequences are depicted\nin blue and red, respectively. The position, also known as the time-step, is indicated in superscript. The weight\nmatrices ğ‘¾, ğ‘¼ , and ğ‘½ are shared across all positions. Reproduced with permission [31]. Copyright 2021 IEEE.\nAuthorsâ€™ addresses: Quentin Fournier, quentin.fournier@polymtl.ca, Polytechnique MontrÃ©al, 2500 Chemin de Polytech-\nnique, MontrÃ©al, Quebec, Canada, H3T 1J4; GaÃ©tan Marceau Caron, gaetan.marceau.caron@mila.quebec, Mila - Quebec AI\nInstitute, 6666 Rue Saint-Urbain, MontrÃ©al, Quebec, Canada, H2S 3H1; Daniel Aloise, daniel.aloise@polymtl.ca, Polytech-\nnique MontrÃ©al, 2500 Chemin de Polytechnique, MontrÃ©al, Quebec, Canada, H3T 1J4.\narXiv:2103.14636v2  [cs.LG]  27 Mar 2023\n2 Fournier et al.\nRecurrent neural networks align the input and output sequences, that is, there is a one-to-one\nmapping between the two sequences. Depending on the task, this property of RNNs may be too\nrestrictive: for instance, translation requires outputting a sequence whose size is often different from\nthat of the input while aligning tokens at different positions. Sutskever et al. [112] addressed this\nlimitation by introducing the sequence-to-sequence framework in which a first network (encoder)\nprocesses the entire input sequence and returns its last hidden representation ğ’‰(ğ‘›), effectively\nencoding the input into a fixed-size vector called context. The context then serves as the initial\nstate for a second network (decoder), which generates the output sequence in an autoregressive\nmanner. The decoding stops when a special end-of-sequence token is generated. Figure 2 illustrates\nthe sequence-to-sequence framework.\n... \nEncoder \n_ \n... \nDecoder \nFig. 2. The sequence-to-sequence framework where the encoder and decoder are recurrent neural networks.\nThe input sequence (blue) is encoded into a fixed-size context ğ’‰(ğ‘›)(red), which serves as the initial state of\nthe decoder. Reproduced with permission [31]. Copyright 2021 IEEE.\nIn practice, the fixed-size nature of the hidden representation hinders the effectiveness of recur-\nrent neural networks [15]. Indeed, as the input sequence is processed, information is iteratively\nstored into the hidden representation that may be too small to retain all the relevant information\nfor the task. In that case, useful data is inevitably lost, which may significantly impact the modelâ€™s\nperformance. Bahdanau et al. [3] introduced an alignment mechanism called inter-attention to over-\ncome the bottleneck of the sequence-to-sequence framework. This attention mechanism computes\na different representation of the input for each output step, effectively allowing the decoder to\nâ€œlook atâ€ the relevant part(s) of the input for each output step. Thereby, the inter-attention alleviates\nthe encoderâ€™s burden to encode all information about the input sequence into a fixed-size vector.\nFormally, the context is the weighted sum of the encoderâ€™s hidden representationsğ’‰ğ‘–, forğ‘– = 1,...,ğ‘› ,\nwhere the weights are computed with a feed-forward neural network. For a comprehensive survey\nof the attention mechanism, we refer the reader to Galassi et al . [33] and Weng [130]. Figure 3\nillustrates the inter-attention mechanism.\nMoreover, recurrent neural networks do not scale efficiently to longer sequences due to their\niterative nature [121]. In particular, RNNs struggle to learn dependencies between distant positions.\nOne measure of this limitation is the relative effective context length (RECL) introduced by Dai et al.\n[22]. The RECL is the largest context length that leads to a substantial relative gain over the best\nmodel. In other words, increasing the context length over the RECL yields a negligible increase in\nperformance over the best model. The authors estimated that the relative effective context length of\nLSTMs on natural language data is limited to approximately 400 words. Besides, Khandelwal et al.\n[58] empirically observed that LSTMs sharply model recent positions but only vaguely remember\nthe distant past.\nA Practical Survey on Faster and Lighter Transformers 3\n1.1 Transformer\nThis inherent limitation of recurrent neural networks has prevented them from being successfully\napplied to domains that require processing long sequences such as DNA. To overcome this limita-\ntion, Vaswani et al. [121] introduced the Transformer, a sequence-to-sequence model built without\nrecurrences. Instead, the Transformer relies solely on the attention mechanism: the inter-attention\nbetween the encoder and decoder (see Figure 3), and the self-attention, also known as intra-attention,\nwithin the encoder and decoder. The self-attentionâ€™s main advantage is its ability to relate any two\npositions of the input sequence regardless of their distance, thus increasing performance signifi-\ncantly on a wide range of tasks, including natural language processing (NLP) [10, 24, 121], computer\nvision [12, 27, 57], speech recognition [40, 110, 140], and biological sequence analysis [139]. Karita\net al. [55] evaluated a Transformer against a sequence-to-sequence Bi-LSTM baseline on automatic\nspeech recognition (ASR), speech translation (ST), and text-to-speech (TTS). The attention-based\nmodels outperformed the baseline on 13 corpora out of 15 for monolingual ASR and realized more\nthan 10% relative improvement in 8 languages out of 10 for multilingual ASR. The Transformer\nimproved the BLEU score from 16.5 for the baseline to 17.2 on ST while performing on par for\nTTS. Table 1 reports the performance improvements brought by popular Transformer architectures\nover previous state-of-the-art models across different domains. As of this paperâ€™s writing, the\nTransformer has become the de facto model for numerous sequence processing tasks.\n... \nEnc o der \nDe c o der \n... \n... \n... \nFig. 3. The inter-attention mechanism. The attention weight ğ›¼(ğ‘¡)\nğ‘– depicts the strength with which the ğ‘–-\nth encoder hidden representation â„(ğ‘–) contributes to the context of ğ‘¡-th decoder step. Reproduced with\npermission [31]. Copyright 2021 IEEE.\nAs an illustration of an end-to-end application of the Transformer, let us consider the speech\nrecognition task. In hybrid approaches, the recognition system consists of independently trained ma-\nchine learning components, often an acoustic model, a pronunciation model, and a language model.\nInstead, in end-to-end approaches, the recognition system consists of a single model comprising\nseveral parts trained together. Zhang et al. [140] introduced an end-to-end speech recognition model\nbased on Transformer encoders called the Transformer Transducer that outperformed previous\nhybrid and end-to-end approaches on the LibriSpeech benchmarks.\nThe Transformerâ€™s capacity comes at the cost of a quadratic computational and memory com-\nplexity with respect to the sequence length. Therefore, training large Transformers is prohibitively\nslow and expensive. For instance, Liu et al. [74] introduced RoBERTa, which was pre-trained on\n4 Fournier et al.\nTable 1. Relative improvements brought by popular Transformer architectures over previous state-of-the-art\nmodels. Absolute differences are reported between parenthesis. Sources are: [121] for machine translation,\n[27] for image classification, [24, 91] for text classification, and [69] for speech-to-text.\nTask Dataset Previous SOTA Transformerâ€™s Architecture Relative Improvement\nMachine Translationnewstest2014 (EN-to-DE) MoE (GNMT) [103] Vanilla [121] 9.1% (+2.37 BLEU 3)newstest2014 (EN-to-FR) 3.1% (+1.24 BLEU)\nImage Classification\nImageNet Noisy Student (EfficientNet-L2) [134] ViT [27] 0.2% (+0.15% Acc)CIFAR-10 BiT-L (ResNet152x4) [60] 0.1% (+0.13% Acc)CIFAR-100 1.1% (+1.04% Acc)VTAB (19 tasks) 1.8% (+1.34% Acc)\nText ClassificationSST2 Sparse byte mLSTM [39] BERT[24] 1.8% (+1.70% Acc)CoLA Single-task BiLSTM + ELMo + Attn [124] 72.9% (+25.5 MC 4)\nSpeech-to-text librispeech (test-clean) LAS (LSTM) [13, 86] Convformer [40] 13.6% (-0.3 WER5)librispeech (test-other) 25.0% (-1.3 WER)\n1024 high-end V100 graphics processing units (GPUs) for approximately a day. Although numer-\nous large pre-trained Transformers have been publicly released, fine-tuning them on the tasks\nof interest is still computationally expensive. Furthermore, the sequence lengths are restricted\nby the amount of memory available. Indeed, practitioners typically use large mini-batches with\nrelatively short sequences because the Transformerâ€™s optimization is known to be particularly\nunstable with small mini-batches. Typically, a GPU with 16 GB of memory handles sequences up to\n512 words. Consequently, there exists an actual need for lighter and faster Transformers as only\na few large organizations can afford to train massive models. As of the writing of this paper, the\nlargest dense Transformer is GPT-3 [10] which requires 355 years to train on a V100 GPU, costing\naround 4,600,000$ of cloud instances1.\n1.2 Lighter and Faster Transformers\nOver the years, numerous approaches have been proposed to reduce the computational and memory\ncosts of neural networks, many of which have been applied to Transformers. In this paper, such\nmethods are referred to asgeneral since they apply, and have been applied, to a wide range of models.\nGeneral methods are often orthogonal, and consequently, several of them may be combined to\nprecisely fine-tune the networkâ€™s capacity, computational cost, and memory usage. However, general\nmethods may be insufficient as the model complexity typically remains unchanged. Therefore,\nmany works introduced lower-complexity variations of the Transformer, referred to as x-formers.\nIn this survey, the Transformerâ€™s alternatives are categorized depending on whether they sparsify\nthe attention, factorize it, or modify the networkâ€™s architecture. Please note that this survey aims\nto provide a comprehensive summary of the methods that improve the Transformerâ€™s efficiency\nand that fine-grained taxonomies have already been proposed by Tay et al. [116] and Lin et al. [68].\nAccordingly, our taxonomy will remain purposefully coarse.\nRecently, Tolstikhin et al. [118] and Liu et al. [70] amongst others argued that the powerful yet\nexpensive self-attention mechanism is not necessary to achieve state-of-the-art results and thus\nchallenged the preconception that the self-attention is the source of the Transformerâ€™s success.\nConsequently, they introduced networks without self-attention that are competitive with Trans-\nformers for image classification and language modelling at the same computational cost. Yu et al.\n[137] expanded on this idea with a more general and flexible architecture called MetaFormer where\nthe mechanism to relate the tokens is not specified while the other components are kept the same\n1https://lambdalabs.com/blog/demystifying-gpt-3\n2Bilingual evaluation understudy (BLEU), higher is better.\n3Matthews correlation (MC) coefficient, higher is better.\n4Word error rate (WER), lower is better.\nA Practical Survey on Faster and Lighter Transformers 5\nas the Transformer. Despite the recent success of attention-free architectures, such networks are\noutside the scope of this paper as they arguably remove the Transformerâ€™s core mechanism and are\ndiscussed in appendix.\nThe remainder of this survey is organized as follows. Section 2 introduces the Transformerâ€™s\narchitecture and the origin of the quadratic complexity. Section 3 investigates the popular general\nmethods that have been applied to Transformers to reduce the computations and memory footprint.\nSection 4 explores the recent lower-complexity Transformers. Section 5 explains the limitations of\nthe different approaches and the current evaluation methodology, Section 6 provides a discussion\non the broader impact of lighter and faster Transformers, and Section 7 points out potential future\nresearch directions. Finally, Section 8 concludes this survey. Practitioners and researchers can find\ndetailed practical guidelines regarding the general and specialized methods in appendix, as well\nas a summary of the specialized methods (see Table 4) and a discussion about some of the most\npopular attention-free alternatives.\n2 TRANSFORMER\nThis section formally introduces the attention mechanism, the Transformerâ€™s architecture, and the\nroot cause of its quadratic complexity.\nLa y erNorm \nMatMul \nScale \nSoftmax \nMatMul \nFFN \nLa y erNorm \nA tten tion \nA tten tion \ninputs \nÂ  Â  \noutputs \nEncoder's \nlayer\nDecoder's \nlayer\ninputs \nA tten tion \nFFN \nLa y erNorm \nLa y erNorm \nLa y erNorm \nFig. 4. The Transformerâ€™s computational graph [ 121]. From left to right, the scaled dot product self-attention,\nthe encoder, and the decoder. Note that both the encoder and decoder comprise ğ¿identical layers, of which\nonly one is depicted.\n2.1 Attention Mechanism\nThe attention mechanism relies on three matrices, namelyğ‘¸,ğ‘²,ğ‘½ âˆˆRğ‘›Ã—ğ‘‘, commonly referred to as\nâ€œqueriesâ€, â€œkeysâ€, and â€œvaluesâ€, respectively. The attention outputs the sum of the values weighted\nby a compatibility or alignment score between each token, which is computed with the function\nScore(ğ‘¸,ğ‘²)âˆˆ Rğ‘›Ã—ğ‘›. Intuitively, if the ğ‘–-th query is highly compatible with the ğ‘—-th key, then the\nğ‘—-th value greatly contributes to the ğ‘–-th attentionâ€™s output. The attention mechanism may be\nwritten as:\nAttention(ğ‘¸,ğ‘²,ğ‘½)= Score(ğ‘¸,ğ‘²)ğ‘½ . (1)\n6 Fournier et al.\nSince the compatibility score directly controls the alignment between the tokens, many functions\nhave been proposed. In the original paper, the Transformer relies on thescaled dot product attention .\nThe dot product refers to the computation of the compatibility score between a single query and a\nsingle key. In practice, however, the compatibility scores are computed simultaneously for every\nquery and key by multiplying ğ‘¸ with ğ‘²âŠ¤. Indeed, the (ğ‘–,ğ‘— )entry of the ğ‘¸ğ‘² âŠ¤multiplication is\nequal to the dot product between the ğ‘–-th query and the ğ‘—-th key. In order to obtain a probability\ndistribution over the positions, referred to as attention weights, each row ofğ‘¸ğ‘² âŠ¤is passed through\na Softmax function defined as follows:\nSoftmax(ğ’™)ğ‘– = ğ‘’ğ‘¥ğ‘–\nÃğ‘›\nğ‘—=1 ğ‘’ğ‘¥ğ‘—\nfor ğ‘– = 1,...,ğ‘›. (2)\nwhere ğ’™ âˆˆRğ‘›. Since the dot product grows large in magnitude for large values ofğ‘‘, thereby pushing\nthe Softmax into a region of small gradients, a scaling factor\nâˆš\nğ‘‘ is introduced. Thus, the scaled dot\nproduct attention is given by:\nAttention(ğ‘¸,ğ‘²,ğ‘½)= Softmax\n\u0012ğ‘¸ğ‘² âŠ¤\nâˆš\nğ‘‘\n\u0013\nğ‘½ . (3)\nNonetheless, the attention presented above may not be flexible enough if the relevant information\nfor the task is scattered across different regions of the input space. That is due in part to theSoftmax\nbeing exponential, which amplifies the differences between the values. As a result, only a few\nattention weights are large, i.e., only a few positions are strongly attended. Vaswani et al. [121]\naddressed this limitation with the multi-head attention. Theğ‘‘-dimensional queries, keys and values\nmatrices are first linearly projected â„ times with distinct, learned projections to ğ‘‘ğ‘˜, ğ‘‘ğ‘˜ and ğ‘‘ğ‘£\ndimensions, respectively. On each projection, an independent attention instance called head is\napplied, and the output of each attention head is concatenated before being linearly projected. The\nTransformerâ€™s multi-head scaled dot product attention is given by:\nMultiHead(ğ‘¸,ğ‘²,ğ‘½)= [head1; ...; headâ„]ğ‘¾ğ‘‚. (4)\nheadğ‘– = Softmax\n \nğ‘¸ğ‘¾ğ‘„\nğ‘– (ğ‘²ğ‘¾ ğ¾\nğ‘– )âŠ¤\nâˆšğ‘‘ğ‘˜\n!\nğ‘½ğ‘¾ ğ‘‰\nğ‘– . (5)\nwhere ğ‘¾ğ‘„\nğ‘– âˆˆRğ‘‘Ã—ğ‘‘ğ‘˜ , ğ‘¾ğ¾\nğ‘– âˆˆRğ‘‘Ã—ğ‘‘ğ‘˜ , ğ‘¾ğ‘‰\nğ‘– âˆˆRğ‘‘Ã—ğ‘‘ğ‘£ are the matrices that project the queries, keys, and\nvalues into the ğ‘–-th subspace, respectively, and where ğ‘¾ğ‘‚ âˆˆRâ„ğ‘‘ğ‘£Ã—ğ‘‘ is the matrix that computes a\nlinear transformation of the heads. Typically,ğ‘‘ğ‘˜ = ğ‘‘/â„where ğ‘‘ is the input and output dimension,\nand â„is the number of heads. For the sake of clarity, methods that modify the attention will be\nexplained in the context of a single head (see Equation 3).\nThus far, the attention mechanism has been described as a general method. The Transformer relies\non two specific instances of this mechanism: the intra-attention, popularly known as self-attention,\nand the inter-attention, sometimes referred to as cross-attention. In the case of inter-attention,\nthe queries correspond to the decoderâ€™s hidden representations, and the keys and values are the\nencoderâ€™s outputs. It allows the decoder to look at the relevant parts of the input to produce the\noutput. In the case of self-attention, the three matrices are linear projections of the layerâ€™s input,\nwhich allows the encoder and decoder to focus on the relevant part of the sequence for each\nposition, similarly to the inter-attention depicted in Figure 3.\nA Practical Survey on Faster and Lighter Transformers 7\n2.2 Encoder\nThe Transformerâ€™s encoder is a function defined as the composition of ğ¿identical layers or blocks,\neach composed of two sub-layers. The first sub-layer is the aforementioned self-attention mecha-\nnism. The second sub-layer is a simple fully connected feed-forward network applied position-wise,\nthat is, independently and identically to every position. The feed-forward network increases the\nencoderâ€™s expressiveness and transforms the self-attentionâ€™s output for the next layer.\nInspired by ResNet [ 42], a skip connection, shortcut connection, or residual connection is\napplied around each sub-layer to create a direct path for the gradient to flow throughout the\nnetwork. Notably, residual connections make the training of very deep neural networks more\nstable. Additionally, both sub-layersâ€™ outputs are normalized after the residual connection with the\nlayer normalization technique, referred to as LayerNorm [64]. Normalization is a widely adopted\ntechnique in deep learning that enables faster and more stable training. Although the rationale\nbehind the normalizationâ€™s empirical success is not yet fully understood [67], it has been conjectured\nthat this results from a smoother optimization landscape, and to a lesser extent, from a reduction\nin internal covariance shift [100]. Figure 4 depicts the computational graph of an encoderâ€™s layer.\nIn natural language processing, the input sequence ğ‘¿ would typically represent a sentence or a\nparagraph, and the token ğ’™(ğ‘–)would be its ğ‘–-th word or subword embedding. Each encoderâ€™s layer\nis given by:\nğ‘¿ğ´ = LayerNorm(Attention(ğ‘¸,ğ‘²,ğ‘½)+ğ‘¿) (6)\nğ‘¿ğµ = LayerNorm(FFN(ğ‘¿ğ´)+ğ‘¿ğ´) (7)\nwhere ğ‘¿ and ğ‘¿ğµ are the layerâ€™s input and output, respectively, andğ‘¸, ğ‘², and ğ‘½ are linear projections\nof ğ‘¿.\nThe feed-forward network is given by:\nFFN(ğ’™)= max(0,ğ’™ğ‘¾ 1 +ğ’ƒ1)ğ‘¾2 +ğ’ƒ2 (8)\nwhere ğ‘¾1 âˆˆRğ‘‘Ã—ğ‘‘ğ‘“ and ğ‘¾2 âˆˆRğ‘‘ğ‘“ Ã—ğ‘‘, and where ğ‘‘ğ‘“ is the dimension of the hidden layer. Note that\nthe feed-forward network is defined for a row vector since it is applied position-wise, that is, it is\nindependently and identically applied to every position or row.\nFinally, the position-wise layer normalization is given by:\nLayerNorm(ğ’™)= ğ’ˆ âŠ™ ğ’™ âˆ’ğœ‡âˆš\nğœ2 +ğœ–\n+ğ’ƒ (9)\nwhere âŠ™denotes the element-wise (Hadamard) product, where the average ğœ‡ and the standard\ndeviation ğœ are computed from all of the summed inputs, where the gain ğ’ˆ and the bias ğ’ƒ are\nlearned parameters of dimension ğ‘‘, and where ğœ– is a small constant used in practice for numerical\nstability.\n2.3 Decoder\nThe decoder is also composed of ğ¿ identical layers. Although it is common for the decoder to\nhave the same number of layers as the encoder, one may adjust their depth independently. Each\ndecoderâ€™s layer comprises three sub-layers. The first sub-layer is the self-attention mechanism, as\nin the encoder, except that future positions are masked. Indeed, while the encoder is allowed to\nlook at future positions since the input sequence is entirely available, the decoder is autoregressive\nand thus cannot look at future positions since they have not yet been predicted. Therefore, the\nğ‘–-th position may only attend to positions less than ğ‘–. The second sub-layer is the inter-attention\nmechanism, which helps the decoder focus on the relevant parts of the input. Finally, the third\n8 Fournier et al.\nsub-layer is a simple feed-forward network. As for the encoder, a residual connection and a layer\nnormalization are applied to each sub-layer.\nNote that the decoder may be safely omitted when the task does not require the sequence-to-\nsequence framework, such as sentiment analysis, which predicts whether a sentence is positive. One\nof the most popular encoder-only Transformers is the Bidirectional Encoder Representations from\nTransformers (BERT) [24], a state-of-the-art language model that learns contextualized embeddings.\nNonetheless, autoregressive tasks such as machine translation still require the sequence-to-sequence\nframework.\n2.4 Complexity\nIntuitively, the quadratic complexity emerges from the computation of the compatibility score\nbetween every pair of positions. More precisely, the ğ‘¸ğ‘² âŠ¤multiplication requires ğ‘›2 computations\nand memory. Such attention is said to be full since any output position is able to attend to any input\nposition. The attention pattern is visualized by means of a connectivity matrix, which indicates the\ninput positions that each output position is able to attend (see Figure 5).\nOutput indices\nInput indices\nFig. 5. The connectivity matrix of the full attention. The ğ‘–-th output position attends to the ğ‘—-th input position\nif, and only if, the cell (ğ‘–,ğ‘— )is coloured. The diagonal is highlighted to ease the reading.\nWhat justifies such efforts from the community to improve the Transformerâ€™s efficiency? In our\nopinion, there are three primary motivations: affordability, scalability, and ecology.\nThe foremost reason is affordability. The Transformer has largely surpassed convolutional and\nrecurrent neural networks and achieved new state-of-the-art results across many tasks. However,\nthose networks have a linear complexity with respect to the sequence length [121], making them\naffordable to most researchers and practitioners. As explained by Strubell et al. [109], this creates\nthree major issues: (1) it stifles creativity as researchers and practitioners that do not have access\nto considerable resources are not able to experiment with Transformers, (2) it reinforces the â€œrich\nget richerâ€ cycle where successful labs and companies receive more funding due to their existing\naccomplishments with Transformers, and (3) it forces smaller labs and companies to rely on private\ncloud services that end up more expensive.\nThe second reason is scalability. The quadratic complexity prevents researchers and practitioners,\neven those with access to considerable resources, from applying Transformers on long sequences\nsuch as entire chapters or books, high-resolution images or videos, and DNA.\nThe third reason is ecology. It is now more apparent than ever that we must cut carbon dioxide\n(CO2) emissions in half over the next decade to limit global warming. The large-scale infrastructures\nused by the deep learning community consume a considerable amount of electricity, which is mainly\nproduced by non-renewable sources such as coal or gas [49].\nThereby, the following sections investigate popular and novel methods to make Transformers\nfaster and lighter.\nA Practical Survey on Faster and Lighter Transformers 9\n3 GENERAL APPROACHES\nComputational resources have always been a limiting factor for deep learning models [63]. Therefore,\nnumerous approaches have been proposed throughout the years to design faster and lighter models.\nThis section introduces the most popular techniques that apply to virtually all neural networks.\nGradient Checkpointing [14]: Intermediate results computed during the forward pass, also\nreferred to as activations, are required to compute the gradients during the backward pass; therefore,\nthey are stored in memory. Activations typically account for most of the memory during training:\ngiven an ğ‘™-layer network, the number of intermediate results is proportional to the number of layers\n(O(ğ‘™)). With gradient checkpointing, also known as rematerialization, activations are stored only\nfor a subset of the layers. However, they must be recomputed during the backward pass, trading\nmemory for computations. In the extreme case where no activations are stored, the memory usage\nbecomes constant (O(1)) at the cost of a quadratic number of computations with respect to the\nnumber of layers (O(ğ‘™2)). Chen et al. [14] designed a scheme to select the preserved values that\nreduces the memory requirement from O(ğ‘™)to O(\nâˆš\nğ‘™)at the cost of a single additional forward\npass per mini-batch. OpenAI implementation of gradient checkpointing [84] obtains an impressive\n10Ã—reduction in memory at the cost of a 20% increase in computation time.\nReversible Layers [25, 26, 35]: As explained above, the back-propagation requires the activations\nof all intermediate layers, which are either stored in memory during the forward pass or recomputed\nduring the backward pass. As a solution to the latter case, reversible layers allow their activation to\nbe reconstructed exactly from the next layer; therefore, activations must only be stored for one layer\nand their memory cost becomes independent of the networkâ€™s depth. More formally, each reversible\nlayer takes as input (ğ‘¥1,ğ‘¥2)and outputs (ğ‘¦1,ğ‘¦2)such that ğ‘¦1 = ğ‘¥1 +ğ‘“(ğ‘¥2)and ğ‘¦2 = ğ‘¥2 +ğ‘”(ğ‘¦1). Each\nlayerâ€™s activations are easily reconstructed as ğ‘¥2 = ğ‘¦2 âˆ’ğ‘”(ğ‘¦1)and ğ‘¥1 = ğ‘¦1 âˆ’ğ‘“(ğ‘¥2).\nKitaev et al. [59] used reversible layers in their Transformer, called the Reformer, by combining\nthe attention and feed-forward sub-layers inside a reversible layer. Specifically, ğ‘“(.)and ğ‘”(.)\nwere the Attention(.)and FFN(.)functions, respectively. The authors observed that reversible\nlayers reduced the memory usage of a 3-layer Transformer without degrading its performance.\nNonetheless, reversible layers add numerical errors that accumulate over multiple layers and may\ndegrade the model performance. Therefore, they are not suited for very deep networks.\nGradient checkpointing and reversible layers are very much alike in that they trade computations\nfor memory by recomputing activations during backpropagation. This trade-off is sometimes\nnecessary: although computation bottlenecks entail longer running times, memory bottlenecks are\ncritical as they prevent using the model altogether.\nParameter Sharing : A simple approach to reduce the number of trainable parameters is to\nimpose sets of parameters to be equal in different parts of the network. In other words, the same\nparameters are used for multiple operations but need to be stored only once in memory. Such\na technique is often referred to as parameter sharing, weight tying, or weight replication. As\nexplained in Section 1 and illustrated in Figure 1, recurrent neural networks are built around this\nidea of parameter sharing to process variable-length sequences. Parameter sharing has also been\napplied to Transformers. For instance, the Linformer [126] shares projection matrices across heads\nand layers, and the Reformer [59] shares its queries and keys parameters, that is, ğ‘¾ğ‘„ = ğ‘¾ğ¾. Both\nauthors investigated the impact of parameter sharing and concluded that it did not degrade their\nrespective modelsâ€™ performance on their tasks. Lan et al. [62] shared all parameters between layers,\nwhich drastically reduced the number of parameters but also decreased the performance by up to\n2.5% on average. They observed that sharing only the attention parameters resulted in a slight drop\nin performance of 0.7% on average. The decrease in performance is to be expected since parameter\nsharing reduces the number of free parameters, hence the modelâ€™s capacity.\n10 Fournier et al.\nPruning [63]: Smaller neural networks are not only faster and lighter, but they are also more\nlikely to generalize better than larger models because they presumably extract underlying explana-\ntory factors without redundancy. To reduce the model size, weights with a small saliency, that is,\nwhose deletion have a small effect on the loss, may be removed from large models after training.\nMethods that consider individual weights are said to be unstructured, and methods that consider\npieces of the network structure such as attention heads or layers are said to be structured. Many\nstructured and unstructured pruning schemes have been proposed, several of which have been\napplied to Transformers. For instance, Sajjad et al. [98] reduced the size of BERT by 40% by drop-\nping complete layers while retaining between 97 and 98% of its original performance, and Michel\net al. [79] pruned away between 20% and 40% of BERT attention heads without any significant\nloss in performance. Recently, the lottery ticket hypothesis has brought a new justification to\npruning neural networks. As introduced by Frankle and Carbin [32], the hypothesis states that a\nâ€œrandomly-initialized, dense neural network contains a subnetwork that is initialized such that â€“ when\ntrained in isolation â€“ it can match the test accuracy of the original network after training for at most\nthe same number of iterations. â€. Prasanna et al. [89] successfully verified this hypothesis on BERT,\neven noticing that BERT worst subnetworks remain highly trainable. Nonetheless, pruning has\ntwo limitations: a large model must be trained, and unstructured pruning schemes produce sparse\nmodels unoptimized for modern GPUs and tensor processing units (TPUs).\nKnowledge Distillation [2, 43]: The knowledge of a large model or an ensemble of models\n(teacher) is transferred to a single smaller model (student) by training the student to reproduce\nthe teacherâ€™s outputs or its internal behaviour. The cumbersome teacher is then discarded, and the\nstudent is used at inference time. Given a parameter budget, networks trained with knowledge\ndistillation usually outperform models directly trained on the task. Sanh et al. [99], Tsai et al. [120],\nand Jiao et al. [54] applied different knowledge distillation schemes on the original BERT [24] to\nobtain lighter and faster models called DistilBERT, MiniBERT, and TinyBERT, respectively. Table 2\nreports their compression, speed-up, and performance. Although knowledge distillation achieves\nimpressive compression ratios and performance trade-offs, a large teacher model still needs to be\ntrained, and the student may perform significantly worse than the teacher. For instance, BERTBASE\nachieves an accuracy of 52.8% on the CoLA task [129], while DistilBERT and TinyBERT only achieve\n32.8% and 44.1%, respectively, according to Jiao et al. [54].\nTable 2. Multiple knowledge distillations of BERTBASE. Speed-ups are evaluated on GPUs.\nModel Compression Speed-up Mean Relative Performance\nBERTBASE [24] 1.0Ã— 1.0Ã— 100%\nDistilBERT [99] 1.7Ã— 1.6Ã— 97%\nMiniBERT [120] 6.0Ã— 2.6 âˆ’4.3Ã— 97 âˆ’99%\nTinyBERT [54] 7.5Ã— 9.4Ã— 97%\nMixed-Precision [80]: Modern GPUs and TPUs perform at least twice as many half-precision\n(16 bits) float operations as single-precision (32 bits) ones. A popular approach to accelerate training\nand reduce memory consumption is storing and computing the weights, activations, and gradients\nin half-precision. A master copy of the weights is stored in single-precision for numerical stability\nand minimal performance loss. Thanks to NVIDIAâ€™s Automatic Mixed-Precision included in some\nof the most popular deep learning libraries, namely TensorFlow, PyTorch, and MXNet, using mixed\nprecision can be as simple as adding one line of code. Consequently, we highly recommend mixed-\nprecision. Jacob et al. [51] improved over this approach by quantizing both weights and activations\nas 8-bit integers and biases as 32-bit integers, effectively allowing inference to be performed using\nA Practical Survey on Faster and Lighter Transformers 11\ninteger-only arithmetic. Given a parameter matrix ğ‘¾, ğ‘-bit quantization rounds each parameter\nto one of 2ğ‘ codewords corresponding to bins evenly spaced by a scale factor ğ‘  and shifted by a\nbias ğ‘§computed as follows:\nğ‘  = max ğ‘¾ âˆ’min ğ‘¾\n2ğ‘ âˆ’1 and ğ‘§ = round\n\u0012min ğ‘¾\nğ‘ \n\u0013\n(10)\nEach parameter ğ‘Šğ‘–,ğ‘— is quantized to its nearest codeword, and dequantized as:\nË†ğ‘Šğ‘–,ğ‘— =\n\u0012\nround\n\u0012ğ‘Šğ‘–,ğ‘—\nğ‘  +ğ‘§\n\u0013\nâˆ’ğ‘§\n\u0013\nÃ—ğ‘  (11)\nIn order to mitigate the performance loss associated with the low-precision approximation, Quan-\ntization Aware Training (QAT) [51] quantizes the parameters during training. Since quantization is\nnot differentiable, gradients are approximated with a straight-through approximator [7]. Notably,\nZafrir et al. [138] quantized all matrix product operations in BERT fully connected and embedding\nlayers during training, reducing the memory footprint by 4Ã—while retaining 99% of the original\naccuracy on the GLUE [ 124] and SQuAD [94] tasks. Stock et al . [107] achieved an even higher\ncompression ratio with iterative product quantization (iPQ), which replaces vectors of weights\nby their assigned centroid, and quantization of those centroids. The authors reduced the size of a\n16-layer Transformer by 25Ã—, making the model only 14 MB, while retaining 87% of the original\nperformance on the Wikitext-103 [78] benchmark.\nWhile pruning and knowledge distillation achieve faster and lighter models by reducing the\nnumber of parameters, mixed-precision and quantization instead reduce the number of bits per\nparameter.\nMicro-Batching [48]: Increasing model capacity and data throughput are efficient strategies\nfor improving performances in deep learning. However, increasing data throughput requires\ntransferring large mini-batches to the acceleratorsâ€™ memory5, which is also used to store the model.\nOne way to partially avoid the trade-off between mini-batch size and model size is to use model\nparallelism. GPipe [ 48] is a model parallelism library that enables users to distribute a model\nby grouping layers into cells assigned to accelerators. To avoid the communication bottleneck\nbetween accelerators due to the forward and backward operations, the authors proposed a novel\nbatch-splitting algorithm that further splits the mini-batch into micro-batches. As soon as the\nfirst accelerator finishes the forward operation of the layers assigned to it for a micro-batch, it\nsends the result over the communication link and starts processing the next micro-batch. After\nfinishing the last micro-batchâ€™s forward operation, the accelerators wait for the first micro-batchâ€™s\nbackwards operation results. This waiting time can be used to recompute the forward operation and\nfurther reduce memory usage, known as rematerialization. Finally, once the backward operation is\ncompleted on the last micro-batch, the algorithm sums all micro-batchâ€™s gradients to obtain the\nmini-batchâ€™s gradient (see Figure 6). However, the result is not exact with layers that compute\nstatistics across all mini-batch examples, such as a batch normalization layer [50]. Finally, GPipe is\ncompatible with data parallelism, where multiple mini-batches are processed in parallel.\nHuang et al. [48] empirically demonstrated that GPipe allows the maximum Transformer size to\nscale linearly with the number of accelerators. For instance, a TPU v3 with 16Gb of memory can\nonly fit a 3-layer Transformer. With GPipe, the same TPU is able to fit 13 layers, while 128 TPUs\nare able to fit 1663 layers, which is 127.9Ã—more. Additionally, the authors distributed a 48-layer\nTransformer across 8 TPUs and reported that the training throughput was 4.8 times higher with 32\nmicro-batches than with a single one.\n5An accelerator denotes any device that accelerates computation, such as a graphics or tensor processing unit.\n12 Fournier et al.\nGradients\nLoss\nDevice 1\nDevice 2\nDevice 3 Update\nUpdate\nUpdate\nTime\nWaiting\nWaiting\nWaiting\nFig. 6. Micro-Batching applied to a model distributed across three devices [ 48]. ğ¹ğ‘– and ğµğ‘– denotes the\nsequential forward and backward operations, respectively, performed by the ğ‘–-th device. Computation on a\ndevice may start as soon as the previous device in the computational graph has processed the first micro-batch.\nTherefore, micro-batching reduces the waiting time of each device at the cost of inter-device communications.\nNote that the model update is done synchronously at the end.\nMixture of Experts [52]: The core idea is to train multiple networks called experts, each of\nwhich specializes only in a subset of the data, and a manager or router, which forwards the input\nto the corresponding experts. A single network is used in practice, whose layers are composed\nof multiple subsets of parameters (experts), effectively resulting in a sparsely activated model as\nillustrated in Figure 7. Increasing the number of experts keeps the computational cost constant\nsince the model always selects the same number of experts for each input regardless of the number\nof experts. Therefore, the mixture of experts (MoE) approach allows for massive models and is\nparticularly efficient for distributed systems in which experts are spread across devices. In that\ncase, the number of experts, and therefore parameters, scales with the number of devices available.\nDespite these advantages, the mixture of experts has not yet been widely adopted as the method is\ncomplex to deploy in practice. It imposes a communication cost between the devices, a computation\ncost to select the experts for each input position, and makes training unstable. Recently, Fedus\net al. [30] introduced the Switch Transformer based on a carefully crafted mixture of experts.\nNotably, given a fixed amount of computation per input position, the Switch Transformer reached\nthe same quality threshold as a vanilla Transformer five times faster (wall-clock time) on average.\nAdditionally, when trained further, the Switch Transformer outperformed the vanilla baseline.\nHowever, this approach assumes that multiple regimes with distinct input to output relations\nproduce the data.\nDifficult tasks often require large models to achieve the desired performance. However, such\nmodels require powerful and expensive accelerators. Both micro-batching and the mixture of\nexperts offer an alternative to train such models on many relatively weak and inexpensive GPUs at\nthe cost of complex implementation.\nSample-Efficient Objective [19]: Large neural networks, especially Transformers, benefit from\nbeing pre-trained with an unsupervised objective before being fine-tuned on the task of interest,\nalso called the downstream task. The core idea is to leverage large unlabelled datasets that are easy\nto automatically collect in order to learn the data underlying explanatory factors and ultimately\nimprove the model performance. Concretely, pre-training initializes the networkâ€™s weights in a\nâ€œgoodâ€ region of space. As pre-training of large models is often more compute-intensive than fine-\ntuning, researchers regularly share pre-trained models to facilitate their adoption. Most notably,\nHugging Face [132] is an open-source library that contains an extensive collection of pre-trained\nA Practical Survey on Faster and Lighter Transformers 13\nLayerNorm\nSwitch FFN\nLayerNorm\nAttention\nFFN2 FFN3\nRouter\nFFN1 FFN2\nRouter\nEncoder's \nlayer\nFFN3FFN1\nFig. 7. The computational graph of a single layer of the Switch Transformerâ€™s encoder [30]. The Transformerâ€™s\nfeed-forward network (FFN) has been replaced by a Switch FFN which independently routes each position to\nan expert. The expertâ€™s output is multiplied by the gate value. Note that the computational cost is independent\nof the number of experts since a single expert is active for each position.\nTransformers under a unified API. Nonetheless, researchers must sometimes pre-train models\nthemselves due to the peculiar nature of the data or the problem at hand. In that case, a sample-\nefficient objective will reduce the computation required.\nRecently, Devlin et al. [24] popularized the Cloze procedure [ 117] for pre-training under the\nname of masked language model (MLM), which independently estimates the probability of masked\nwords given the rest of the sequence. Practically, 15% of the words are randomly selected, of which\n80% are masked, 10% are replaced by a random word, and 10% are left unchanged. This task is\nanalogous to the reconstruction of corrupted input. Figure 8 illustrates the masked language model\nobjective.\nthe cooks bird meal\nGenerator\n(large Transformer)\nMASK\nthe chef cooks a meal\nchef prediction\nmasked\nsequence\na\nFig. 8. The masked language model objective [ 24]. The masked words are depicted in red. The model makes\na prediction only for the masked words; thus, MLM is computationally inefficient.\nClark et al. [19] introduced the replaced token detection objective to speed up pre-training; a\nsmall network (generator) first generates a plausible alternative for each masked word, then the\nlarge model (discriminator) predicts whether each word has been replaced (see Figure 9). While\nthe masked language model makes a prediction only for the masked works, the replaced token\ndetection makes a prediction for every word. Therefore, the latter is more computationally efficient\nthan the former; in other words, less pre-training computations are required to achieve the same\nperformance on downstream tasks. Additionally, the authors reported that the representations\nlearned with their objective outperformed those learned with MLM given the same model size,\ndata, and computation. Most notably, they were able to outperform GPT on the GLUE benchmark\nwith 30Ã—fewer computations.\n14 Fournier et al.\nthe co oks MASK meal \nGenerator \n(small Transformer)\nMASK \none \nthe chef cooks a meal\nman \nmasked\nsequence\nsample\nthe co oks meal \nDiscriminator \n(large Transformer)\nrep. rep. predictionoriginal original original \nFig. 9. The replaced token detection objective [ 19]. A plausible alternative of each masked word is sampled\nfrom a small generator network. Then a discriminator predicts whether each word has been replaced.\nParameter Initialization Strategies : Optimizing deep networks is challenging in part because\nof the considerable influence of the initial point on the iterative process. Notably, the initial point\ndetermines whether the algorithms converge at all and, if it does converge, the speed at which\nit converges as well as the quality of the solution [36]. Transformers are notoriously difficult to\ntrain, typically requiring carefully tuned optimizers with adaptive learning rates, learning rate\nschedulers, and large batches. Even then, convergence is not guaranteed. Consequently, Liu et al.\n[73] and Huang et al. [47] concurrently proposed initialization schemes for the Transformer that\npromise a smoother and faster optimization as well as better generalization performances.\nLiu et al. [73] identified an amplification effect that significantly influences training: each layer\nheavily depends on its residual branch6, making the optimization unstable as it amplifies small\nparameter perturbations. Ultimately, the amplification effect may produce a notable change in the\nTransformerâ€™s output. Nonetheless, the authors observed that heavy dependencies on the residual\nbranches are necessary to unlock the Transformerâ€™s potential and achieve better results. In order\nto mitigate the amplification effect, Liu et al . [73] introduced the Adaptive Model Initialization\nstrategy, or Admin, that controls the dependency on the residual connections in the early stage of\ntraining with a new parameter ğ. Formally, the ğ‘–-th sub-layer output is given by\nğ‘¿ğ‘– = LayerNorm(ğ‘“ğ‘–(ğ‘¿ğ‘–âˆ’1)+ğ‘¿ğ‘–âˆ’1 âŠ™ğğ‘–), (12)\nwhere ğ‘“ğ‘–(ğ‘¿), ğ‘¿ğ‘–âˆ’1, and ğ‘¿ğ‘–, denote the function, input, and output of theğ‘–-th sub-layer, respectively.\nAlthough this is equivalent to rescaling some model parameters, the authors observed that rescaling\nleads to unstable training in half-precision.\nThe proposed initialization strategy requires three steps. First, the model parameters are initialized\nwith a standard method such as the Xavier initialization [34] and the Admin parameter ğ with\nones. Then, one or a small number of mini-batches are forward propagated without updating\nthe parameters and record the output variance of each residual branch Var[ğ‘“ğ‘–(ğ‘¿ğ‘–âˆ’1)]. Finally, the\nAdmin parameter is initialized as ğğ‘– = âˆšï¸Ã\nğ‘—<ğ‘– Var[ğ‘“ğ‘—(ğ‘¿ ğ‘—âˆ’1)]. Once the model has been trained, ğ\nmay be discarded.\nThe amplification effect is, however, not the only mechanism that makes Transformers notori-\nously difficult to train. Huang et al. [47] addressed two other issues: (i) Transformers are typically\ntrained with optimizers that rely on adaptive learning rates as conventional SGD fails to train them\n6For a residual block ğ‘“(ğ‘¥)+ğ‘¥, the residual branch refers to ğ‘“(ğ‘¥)and the skip connection, shortcut connection, or residual\nconnection refers to ğ‘¥.\nA Practical Survey on Faster and Lighter Transformers 15\neffectively. However, adaptive learning rates have a problematically large variance in the early\nstages of optimization, resulting in convergence issues [72]; and (ii) the magnitude of the error\nsignal propagated through LayerNorm is inversely proportional to the magnitude of the input [135].\nSpecifically, the norm of the layer normalization gradient is proportional to:\n\r\r\r\r\nğœ•LayerNorm(ğ’™)\nğœ•ğ’™\n\r\r\r\r = O\n âˆš\nğ‘‘\nâˆ¥ğ’™âˆ¥\n!\n(13)\nConsequently, if the input normâˆ¥ğ’™âˆ¥is larger than\nâˆš\nğ‘‘, backpropagating through layer normalization\nreduces the gradient magnitude for layers closer to the input. As a solution to both problems, Huang\net al. [47] proposed an initialization strategy called T-Fixup that restricts the magnitude of the\nupdates in the early stages of training, thus mitigating the vanishing gradient issue while eliminating\nthe need for layer normalization and warmup.\nWhile Liu et al . [73] and Huang et al . [47] claim faster convergence, they did not report the\nimprovement.\nArchitecture Search: One of the most challenging goals in deep learning is to automatically de-\nsign networks. Indeed, the problem of finding architectures that achieve the best performance with\nthe fewest operations and lowest memory footprint in a discrete search space is an NP-hard combi-\nnatorial optimization problem. Over the years, multiple approaches to Neural Architecture Search\n(NAS) have been proposed, including reinforcement learning [141], evolutionary algorithms [95],\nand bilevel optimization [71]. Notably, Zoph et al. [142] demonstrated that NAS is able to surpass\nhuman-designed architectures on ImageNet by 1.2% top-1 accuracy while using 28% fewer compu-\ntations. Nonetheless, neural architecture search methods are computationally expensive as they\nusually require training each candidate model from scratch. As a solution, Pham et al. [88] proposed\nEfficient NAS (ENAS), which constrains all candidates to be subgraphs of a single computational\ngraph, that is, to share parameters. Therefore, the ENASâ€™s controller decides which operations are\nactivated and relies on the modelsâ€™ ability to adapt, similarly to dropout [106]. Efficient NAS reduces\nthe search computational budget by 1,000Ã—over the original NAS [141]. Alternatively, Liu et al.\n[71] proposed the Differentiable Architecture Search (DARTS), which casts the NAS problem as a\ndifferentiable bilevel optimization problem. The first level consists of a continuous relaxation of the\ndiscrete search space using a Softmax function over a list of candidate operations, and the second\nlevel involves the modelâ€™s weights. However, the bilevel formulation requires training the weights\nto convergence to evaluate the architecture gradient. To avoid this substantial cost, the authors\nmade the approximation of taking a single gradient step of the weights for one gradient step of\nthe architecture parameters. The authors obtained comparable performances to non-differentiable\nNAS methods on ImageNet in the mobile setting using only 4 GPU-days, compared to 3,150 for\nevolutionary algorithms [95] and 2,000 for NAS [142]. Differentiable Architecture Search obtained\ncomparable results to ENAS with a similar computational budget. We refer the reader to Elsken\net al. [29] survey for further detail on architecture search methods.\nNevertheless, neural architecture search methods are challenging to apply on Transformers\ndue to the memory requirements and training time. Therefore, recent works introduced methods\nbetter suited for the Transformer. So et al. [105] modified the tournament selection evolutionary\narchitecture search [95] with Progressive Dynamic Hurdles (PDH), which dynamically allocates\nresources to more promising architectures according to their performances. With PDH, the authors\noptimized transformer architectures directly on the WMTâ€™14 En-De task [ 9] which requires 10\nhours of computation on a Google TPU v2 for the base Transformer model. Training directly on\nthis dataset is essential since the authors did not find a smaller surrogate dataset that transfers\nwell, such as CIFAR-10 for ImageNet. The Evolved Transformer matched the vanilla Transformerâ€™s\n16 Fournier et al.\nperformance with only 78% of its parameters. Recently, Tsai et al. [119] profiled the Transformerâ€™s\ncomponents on a TPU v2 and observed that some mechanisms substantially impact inference time:\nattention queries, keys, and values dimensions, width and depth of feed-forward layers, number of\nattention heads, and layer normalization mean computation. By decomposing these components\ninto building blocks and using binary variables, the authors perform a one-shot search for both\nthe architecture and the parameters with a single loss. They optimized this loss with gradient\ndescent on a continuous relaxation of the binary variables and used policy gradient algorithm. Tsai\net al. [119] were able to make miniBERT 1.7Ã—faster with a performance drop smaller than 0.3%.\nCompared to the original BERT, this is 33 to 36Ã—faster.\nNeural architecture search is a promising tool to design lighter and faster Transformers automat-\nically. Nonetheless, NAS imposes a high computational and memory cost, which may be avoided by\ncarefully engineering the architecture instead. For instance, the Lite Transformer [133] leverages\nthe Long-Short Range Attention (LSRA), where a convolutional layer is applied in parallel to the\nself-attention in order to learn the local dependencies separately. The carefully handcrafted Lite\nTransformer outperforms the Evolved Transformer [105] for the mobile NLP setting while requiring\nabout 14,000Ã—less GPU time.\nConditional Computing [6]: Although large models are necessary for hard examples, smaller\nmodels are likely to perform as well, if not better, on simpler ones. For instance, many words\nsuch as â€œcarâ€ are easy to translate, while a few such as â€œcanâ€ require careful consideration of the\ncontext7. As of this surveyâ€™s writing, most architectures apply a fixed number of operations to all\nexamples regardless of their difficulty. A more efficient approach would be to reduce the amount\nof computation for simple examples. As a solution, Bengio [6] introduced conditional computing,\nwhich dynamically adapts the modelâ€™s computational graph as a function of the input.\nOne way to implement conditional computing is with a mixture of experts, as introduced\npreviously. In that case, only a subset of the parameters is used for a given input, making the\ncomputational graph sparse and the computation time almost constant with respect to the model\nsize. Another approach consists of keeping the number of parameters constant and letting the\nmodel adjust its computation time separately for each input (according to the inputâ€™s value). This\napproach is called Adaptive Computation Time (ACT) [ 38] and uses a recurrent mechanism to\ntransform the representations until a halting probability exceeds a given threshold. The model\nlearns to control this probability to minimize both the prediction error and the number of iterations,\ncalled the ponder cost , which prevents the model from using an infinite amount of computation\nbefore making a prediction. One shortcoming of the Adaptive Computation Time is its sensitivity\nto the ponder cost, which controls the trade-off between speed and accuracy.\nDehghani et al. [23] applied ACT to a Transformer with a recurrent mechanism for the archi-\ntectureâ€™s depth. To implement this mechanism, the authors defined encoder and decoder blocks\nsimilar to the original Transformer, except that each block is recurrent, sending its output back as\nits input until the ponder cost becomes too high. Note that a fixed number of recurrent steps is\nequivalent to a Transformer with tied parameters across all layers. With this new architecture called\nUniversal Transformer, the authors claimed that it is computationally universal (Turing-complete)\ngiven enough memory. This property may help Transformers generalize to sequences longer than\nthe ones seen during training. The authors obtained state-of-the-art results on algorithmic and\nlanguage understanding tasks. ACT and the Universal Transformer apply the same layers iter-\natively, which may not be sufficiently flexible. Elbayad et al. [28] addressed this limitation with\nthe Depth-Adaptive Transformer (DAT), which applies different layers at every depth. The DAT\n7Depending on the context, the word â€œcanâ€ has various meanings, including â€œbe able toâ€, â€œmayâ€, â€œjailâ€, and â€œmetal containerâ€.\nSee https://www.wordreference.com/definition/can.\nA Practical Survey on Faster and Lighter Transformers 17\nmatches the performance of a well-tuned Transformer baseline while reducing the computation by\nup to 76%. However, the authors did not provide a comparison between the Universal Transformer\nand DAT.\nIn the same way that complex examples may require more computations, some may require access\nto a longer context. As a solution, Sukhbaatar et al. [111] dynamically adjusted the attention span,\nthat is, the context length, by learning to mask the compatibility scores depending on the input. Their\napproach achieved state-of-the-art on text8 and enwik8 [77] while requiring significantly fewer\ncomputations. Alternatively, Li et al. [65] introduced the Decoder-end Adaptive Computation Steps\n(DACS), which monotonically computes halting probabilities along with the encoder states and stops\nthe decoder computations in order to produce an output when the accumulation of probabilities\nexceeds a given threshold. In other words, each decoder step only looks at the necessary information\nas measured by the halting probabilities instead of looking at the entire input sequence.\n4 SPECIALIZED APPROACHES\nSince the Transformerâ€™s quadratic complexity comes from the attention mechanism, most specialized\nmethods rely on a fast and light approximation of the original full attention. As will be explained\nin greater detail in the rest of this section, the attention weight matrix is dominated by a few large\nvalues and is approximately low-rank. These observations justify two distinct lines of work: sparse\nattention and factorized attention. Alternatively, the complexity may be reduced without altering\nthe original attention mechanism and thus the Transformerâ€™s capacity by directly modifying the\nnetworkâ€™s architecture. Let us first investigate the approaches that rely on sparse attention.\nNote that some approaches only consider autoregressive tasks, such as the left-to-right language\nmodel, and in that case, the connectivity matrix is lower triangular as it is not permitted to attend\nto future positions. Whenever possible, such works have been extended to the more general case\nwhere attending to future positions is allowed in order to ease the comparison between the different\napproaches.\n4.1 Sparse Attention\nDue to the exponential nature of the Softmax, only a few positions are strongly attended to.\nConsequently, a conceptually simple way of reducing the Transformerâ€™s complexity is to make\nthe matrix ğ‘¸ğ‘² âŠ¤sparse8, in other words, to only allow each position to attend to a subset of the\npositions. Let us investigate sparse patterns that are (i) fixed and random, (ii) learned and adaptive,\nand (iii) identified with clustering and locality sensitive hashing.\nFixed and Random Sparse Patterns [5, 16, 41, 66, 90, 125, 139]: One of the first models to\nconsider fixed sparse patterns is the Star-Transformer introduced by Guo et al. [41], which reduced\nthe complexity from quadratic to linear by only allowing attention between adjacent positions. In\norder to preserve the Transformerâ€™s ability to model long-term dependency, the authors relied on a\nsingle global token. Global tokens, also known as shared relay nodes, can attend to every position,\nand every position can attend to global tokens. Let us assume that the global token is located at\nposition 0. The ğ‘–-th output position is allowed to attend to every input position ifğ‘– = 0, otherwise, it\nis allowed to attend to the ğ‘—-th input positions for ğ‘— = 0 and if ğ‘–âˆ’1 â‰¤ğ‘— â‰¤ğ‘–+1. Figure 10 illustrates\nthe Star-Transformer attention pattern.\nConcurrently, Child et al. [16] introduced the Sparse Transformer which reduced the complexity\nto O(ğ‘›âˆšğ‘›)with two different sparse attention patterns: strided and fixed. Strided attention allows\nthe ğ‘–-th output position to attend to the ğ‘—-th input position if one of the two following conditions\n8Since the matrix ğ‘¸ğ‘² âŠ¤is passed through a Softmax function, the masked values are set to minus infinity, effectively setting\ntheir contribution to ğ‘’âˆ’âˆ= 0.\n18 Fournier et al.\nOutput indices\nInput indices\nFig. 10. The connectivity matrices of the Star-Transformer [41].\nis satisfied: (ğ‘–+ğ‘ )> ğ‘— > (ğ‘–âˆ’ğ‘ )or (ğ‘–âˆ’ğ‘—)mod ğ‘  = 0, where the stride ğ‘  is chosen to be close to âˆšğ‘›.\nSimilarly, fixed attention allows ğ‘–to attend to ğ‘— if one of the two following conditions is satisfied:\nfloor(ğ‘—/ğ‘ )= floor(ğ‘–/ğ‘ )or (ğ‘— mod ğ‘ )â‰¥( ğ‘ âˆ’ğ‘), where ğ‘ is an hyperparameter. Figure 11 illustrates\nthe strided and fixed attention patterns.\nOutput indices\nInput indices\nOutput indices\nInput indices\nFig. 11. The connectivity matrices of the Sparse Transformer Child et al . [16]. (Left) Strided attention with a\nstride of 3. (Right) Fixed attention with a stride of 3 and ğ‘ = 1.\nAlternatively, Wang et al. [125] introduced the Cascade Transformer, which relies on sliding\nwindow attention whose size grows exponentially with the number of layers. More specifically, the\nnumber of cascade connections at the layer ğ‘™ is equal to 2.ğ‘.ğ‘šğ‘™ âˆ’1, where ğ‘is the base window size\nand ğ‘šis the cardinal number; therefore reducing the complexity to O(ğ‘›.ğ‘.ğ‘šğ‘™). Cascade attention\nis well suited for shallow networks, but its complexity tends to that of the full attention in deep\nnetworks as depicted by the connectivity matrices in Figure 12.\nOutput indices\nInput indices\nLayer 1\nOutput indices\nInput indices\nLayer 2\nOutput indices\nInput indices\nLayer 3\nOutput indices\nInput indices\nLayer 4\nFig. 12. The connectivity matrices of the Cascade attention [ 125] for the first four layers with a base window\nğ‘ = 1 and a cardinal number ğ‘š = 2. For instance, the window size of the third layer ( ğ‘™ = 2) is equal to\n2 Ã—ğ‘Ã—ğ‘šğ‘™ âˆ’1 = 7.\nLi et al. [66] introduced the LogSparse-Transformer for forecasting fine-grained time series with\nstrong long-term dependencies. The LogSparse-Transformer relies on the eponym attention that\nallows the ğ‘–-th output to attend to the ğ‘—-th inputs for ğ‘— âˆˆ{âˆ’2âŒŠlog2 ğ‘–âŒ‹,ğ‘– âˆ’2âŒŠlog2 ğ‘–âŒ‹âˆ’1,...,ğ‘– âˆ’21,ğ‘– âˆ’\n20,ğ‘–,ğ‘– +20,ğ‘– +21,...,ğ‘– +2âŒŠlog2 (ğ‘›âˆ’ğ‘–)âŒ‹âˆ’1,ğ‘– +2âŒŠlog2 (ğ‘›âˆ’ğ‘–)âŒ‹}where âŒŠ.âŒ‹denotes the floor operation and\nA Practical Survey on Faster and Lighter Transformers 19\nğ‘ denotes the sequence length. Figure 13 illustrates the connectivity matrix of the LogSparse\nattention. Since only ğ‘‚(logğ‘›)positions are attended to by each of the ğ‘›positions, the complexity\nof the LogSparse attention is ğ‘‚(ğ‘›logğ‘›). Additionally, the authors proposed two alternatives: (1) to\nallow the ğ‘–-th output to attend to the first ğ‘˜ input positions, after which the LogSparse attention\nis resumed, and (2) to divide the input sequence into subsequences, and to apply the LogSparse\nattention on each of them.\nOutput indices\nInput indices\nFig. 13. The connectivity matrix of the LogSparse attention Li et al. [66].\nQiu et al. [90]introduced BlockBERT, which relies on the block-wise attention: the input sequence\nis split intoğ‘›ğ‘ non-overlapping blocks, and positions in blockğ‘–are only allowed to attend to positions\nin block ğœ‹(ğ‘–), where ğœ‹ denotes a permutation. The author chose to generate the permutations\nby simply shifting the positions. For instance, the possible permutations of {1,2,3}are {1,2,3},\n{3,1,2}, and {2,3,1}. The permutation {2,3,1}means that the first block attends to the second\nblock, the second block attends to the third block, and the third block attends to the first block. In\nthe multi-head setting, a different permutation9 is assigned to each head. More formally, the output\nposition ğ‘– is only allowed to attend to input ğ‘— if the following condition is satisfied:\nğœ‹\n\u0012\u0016 (ğ‘–âˆ’1)ğ‘›ğ‘\nğ‘› +1\n\u0017\u0013\n=\n\u0016(ğ‘—âˆ’1)ğ‘›ğ‘\nğ‘› +1\n\u0017\n(14)\nFigure 14 illustrates the connectivity matrix of the block-wise attention where a sequence of length\nğ‘› = 12 is split into ğ‘›ğ‘ = 3 blocks. Although the block-wise attention reduces the memory and\ncomputational cost by a factor ğ‘›ğ‘, the complexity remains quadratic with respect to the sequence\nlength.\nOutput indices\nInput indices\nOutput indices\nInput indices\nOutput indices\nInput indices\nFig. 14. The connectivity matrices of the block-wise attention [ 90] for ğ‘›ğ‘ = 3 blocks. The corresponding\npermutations are written below the connectivity matrices.\nBeltagy et al. [5] introduced the Longformer which further reduces the complexity toO(ğ‘›)using\na combination of sliding window and global attentions (see Figure 15). The assumption behind\nthe sliding window attention is that the most useful information is located in each positionâ€™s\n9Note that if the number of heads is greater than the number of permutations, multiple heads must be assigned the same\npermutation.\n20 Fournier et al.\nneighbourhood. The sliding window attention is limited in that it requires O(âˆšğ‘›)layers to model\nlong-range dependencies. Thus, a few preselected tokens have a global attention: they can attend\nto every position and be attended by every position. Consequently, the maximum path length\nbetween any two positions is equal to 2. Zaheer et al. [139] introduced BigBird, which also achieves\na linear complexity using a combination of random, sliding window, and global attentions (see\nFigure 15). BigBird has two configurations that the authors referred to as internal transformer\nconstruction (ITC) and extended transformer construction (ETC). Similarly to the Longformer, the\nformer uses existing positions for global attention, while the latter uses additional tokens, increasing\nthe modelâ€™s capacity and performance. Interestingly, the extra location of ETC may be seen as a form\nof memory. The authors proved that their sparse factorization preserves the theoretical properties\nof Transformers with the full attention: the model is both a universal approximator of sequence\nfunctions and Turing complete. However, BigBird without random attention outperformed BigBird\nwith it in most of their experiments.\nOutput indices\nInput indices\nOutput indices\nInput indices\nFig. 15. The connectivity matrices of two sparse attention schemes. (Left) Longformer [5]. (Right) BigBird [139].\nThe attention is the combination of sliding window attention (blue), global attention (green), and random\nattention (orange).\nLearned and Adaptive Sparse Patterns [20, 104, 114]: Fixed and random patterns are hand-\ncrafted and may not be suitable for the data and task at hand. One may instead learn the relevant\npatterns and adapt them based on the content.\nIn order to increase the flexibility of the block-wise attention, Tay et al . [114] introduced the\nsparse Sinkhorn attention, which is equivalent to the block-wise attention whose keys have been\nsorted in a block-wise fashion. In other words, the permutations are learned. More specifically,\nthe sparse Sinkhorn attention transforms the input sequence ğ‘¿ âˆˆRğ‘›Ã—ğ‘‘ into ğ‘¿â€² âˆˆRğ‘›ğ‘Ã—ğ‘‘ where\nğ‘›ğ‘ is the number of blocks, and where ğ‘¿â€²\nğ‘– is equal to the sum of the input in that block. A simple\nfeed-forward network then learns a mapping ğ‘¹ğ‘– âˆˆRğ‘›ğ‘ from the ğ‘–-th block ğ‘¿â€²\nğ‘– to all blocks. In\norder to obtain a sorting matrix from ğ‘¹ âˆˆRğ‘›ğ‘Ã—ğ‘›ğ‘ , that is, a matrix comprising only 0s and 1s, and\nwhose rows and column sum to one, the rows and columns are iteratively normalized. The sorting\nmatrix is then used to permute the keys, effectively learning which block to attend (see Figure 16).\nThe sparse Sinkhorn attention reduces the complexity to O(ğ‘›2\nğ‘). Nonetheless, since the block size\nis constant in the original paper, the complexity remains quadratic with respect to the sequence\nlength. Additionally, the authors proposed a truncated version of the sparse Sinkhorn attention,\nwhich selects a few keys after sorting them, further reducing the complexity to O(ğ‘›).\nRecently, Shi et al. [104] put under the microscope the attention patterns learned by BERT [24]\nand observed that the diagonal elements are less important compared to other positions, that is,\nthey contribute the least to the output, while neighbourhood positions and special tokens are\nprominent. To confirm their observations, they dropped the diagonal element in BERTâ€™s attention\nsuch that each position is not allowed to attend to itself and noted that the performance remains\ncomparable to the original model. Additionally, they observed that models for different tasks have\nvarious degrees of redundancy and hence can achieve various sparsity levels before significantly\nA Practical Survey on Faster and Lighter Transformers 21\nOutput indices\nInput indices\nFig. 16. The connectivity matrix of the sparse Sinkorn attention [114].\ndropping performance. Consequently, Shi et al. [104] proposed to learn sparsity patterns for each\ntask in an end-to-end fashion with the Differentiable Attention Mask (DAM) algorithm. Let us\ndenote the attention score between the ğ‘–-th output position (query) and ğ‘—-th input position (key) as\nğ›¼ğ‘–,ğ‘—. They proposed to compute the attention maskğ‘€ğ‘–,ğ‘— as the Gumbel-Sigmoid [76] of the attention\nscore ğ›¼ğ‘–,ğ‘—:\nğ‘€ğ‘–,ğ‘— = Gumbel-Sigmoid(ğ›¼ğ‘–,ğ‘—)= Sigmoid\n\u0012ğ›¼ğ‘–,ğ‘— +ğº1 âˆ’ğº2\nğœ\n\u0013\n(15)\nwhereğº1,ğº2 are independent Gumbel noisesğºğ‘˜ = âˆ’log(âˆ’log(ğ‘ˆğ‘˜))generated from a uniform distri-\nbution ğ‘ˆğ‘˜ âˆ¼U(0,1), and where ğœis a temperature hyperparameter. Note that theGumbel-Sigmoid\nbecomes binary asğœapproaches 0. A penalty termğœ†âˆ¥ğ‘€âˆ¥1 is added to the loss to control the trade-off\nbetween performance and sparsity. The resulting model called SparseBERT achieved 91.2% sparsity\nwhile maintaining an average score of 80.9% on GLUE, i.e., only 3% lower than the full BERT.\nSuch an approach deviates from previous sparse attention whose patterns have been manually\nhandcrafted. To avoid learning completely unstructured sparsity patterns, the authors proposed to\nenforce the first and last row/column of the attention mask to be active and all positions on each\nline parallel to the diagonal to share their mask parameters.\nAs mentioned above, due to the exponential nature of the Softmax, most positions are lightly\nattended to. In other words, most attention weights are small but non-zero. Instead, Correia et al.\n[20] introduced the Adaptively Sparse Transformer that replaces the Softmax by the ğ›¼-entmax\nfunction, a differentiable generalization of the Softmax that pushes small weights to be exactly zero.\nFormally, the ğ›¼-entmax function is defined as:\nğ›¼-entmax(ğ’›)= argmax\nğ’‘âˆˆÎ”ğ‘‘\nğ’‘âŠ¤ğ’› +ğ‘¯ğ‘‡\nğ›¼(ğ’‘), (16)\nwhere Î”ğ‘‘ = {ğ’‘ âˆˆRğ‘‘ : Ã\nğ‘– ğ‘ğ‘– = 1}and, for ğ›¼ â‰¥1, ğ‘¯ğ‘‡\nğ›¼ is the Tsallis continuous family of entropies:\nğ‘¯ğ‘‡\nğ›¼(ğ’‘)=\n(\n1\nğ›¼(ğ›¼âˆ’1)\nÃ\nğ‘—(ğ‘ğ‘— âˆ’ğ‘ğ›¼\nğ‘— ), ğ›¼ â‰  1\nâˆ’Ã\nğ‘— ğ‘ğ‘— log ğ‘ğ‘—, ğ›¼ = 1. (17)\nThe authors showed that the solution to the equation 16 is\nğ›¼-entmax(ğ’›)= [(ğ›¼âˆ’1)ğ’› âˆ’ğœ†1]\n1\nğ›¼âˆ’1\n+ , (18)\nwhere []+denotes the ReLU function, 1 denotes the vector of ones, and ğœ†is the Lagrange multiplier\ncorresponding to the Ã\nğ‘– ğ‘ğ‘– = 1 constraint.\nInterestingly, when ğ›¼ = 1, the ğ›¼-entmax is equivalent to the Softmax, and the attention is dense,\nand when ğ›¼ > 1, the output is permitted to be sparse. In their experiments, a scalar parameter ğ‘ğ‘–,ğ‘—\nis learned for the ğ‘—-th attention head of the ğ‘–-th layer, and ğ›¼ğ‘–,ğ‘— is computed as:\nğ›¼ğ‘–,ğ‘— = 1 +sigmoid(ğ‘ğ‘–,ğ‘—)âˆˆ] 1,2[ (19)\n22 Fournier et al.\nNonetheless, the Adaptively Sparse Transformer computes the attention score for each pair of\nqueries and keys. Consequently, the sparsity cannot be leveraged to improve the memory and\ncomputation, resulting in a model that is 25% slower than the original Transformer in terms of\ntokens per second.\nAs of this surveyâ€™s writing, unstructured sparse attention (whether fixed, random or learned) does\nnot benefit from efficient implementations and therefore cannot result in memory and computational\nimprovements. Nonetheless, there are exciting researches in that direction, as noted by Hooker\n[45]. In contrast, some structured sparsity patterns benefit from efficient implementations. Recently,\nNVIDIA introduced its Ampere architecture which efficiently compresses 2:4 structured sparsity\non rows, that is, two non-zero values in every four entries.\nClustering and Locality-Sensitive Hashing [59, 96]: The Softmax function is dominated by\nthe largest values, that is, by the keys and queries that have the largest dot product. Therefore, the\nattention may be approximated by only comparing the most similar keys and queries. Although\nthis approach is a form of adaptive sparsity as the patterns depend on the data, they are presented\nseparately due to their conceptual difference.\nKitaev et al. [59] introduced the Reformer, which selects the set of keys that the query can attend\nto by grouping them with an angular multi-round locality-sensitive hashing (LSH). Such hashing\nscheme has a high probability of assigning the same value to similar vectors. Formally, queries and\nkeys are shared (ğ‘„ = ğ¾) and bucketed using ğ‘hash values obtained as follows:\nğ’‘ = [ğ’™âŠ¤ğ‘¹; âˆ’ğ’™âŠ¤ğ‘¹] (20)\nâ„(ğ’™)= argmax\nğ‘–\n(ğ‘ğ‘–) (21)\nwhere ; denotes the concatenation operation, and where ğ’™ âˆˆRğ‘‘ is a query/key and ğ‘¹ âˆˆRğ‘‘Ã—ğ‘/2 is a\nrandom rotation matrix. Output positions are only allowed to attend to input positions that are in\nthe same bucket (see Figure 17). They are, however, not allowed to attend to themselves because\nthe dot product of a vector with himself will almost always be greater than the dot product with\nother positions.\nThe authors chose a constant bucket size ğ‘™ğµ, resulting in a number of buckets ğ‘›ğµ = ğ‘›/ğ‘™ğµ. The\nattention complexity is O(ğ‘›ğµ Ã—ğ‘™2\nğµ)which simplifies as O(ğ‘›). This does not take into account the\ncomputation of the hash values for each position. As only logğ‘›ğµ bits are required to encode ğ‘›ğµ\nbuckets, the complexity of computing hash values is given by O(ğ‘›logğ‘›ğµ), which simplifies as\nO(ğ‘›logğ‘›). Consequently, the complexity of the Reformerâ€™s attention is O(ğ‘›logğ‘›).\nOutput indices (sorted by bucket)\nInput indicesÂ (sorted by bucket)\nFig. 17. The connectivity matrix of the Reformer [ 59]. Queries and keys are bucketed using LSH then sorted\nby their bucket. Therefore, the ğ‘–-th row of the connectivity matrix may not correspond to the ğ‘–-th position in\nthe input sequence. Units can only attend other units in the same bucket, but not themselves because queries\nand keys are equal. The colour represents buckets.\nThe Maximum Inner Product Search (MIPS) problem is the task of searching for the vector ğ¾ğ‘— in\nğ¾ = {ğ¾1,ğ¾2,Â·Â·Â· ,ğ¾ğ‘›}that maximizes the dot product with a given vector ğ‘„ğ‘–. Note that the MIPS\nA Practical Survey on Faster and Lighter Transformers 23\nproblem is particularly useful for the attention mechanism as ğ‘„âŠ¤\nğ‘– ğ¾ğ‘— is directly proportional to the\ncontribution of the ğ‘—-th value for the ğ‘–-th attentionâ€™s output. There are multiple approaches to\napproximately solve this problem, including tree-based and LSH-based. When the norm of every\nğ¾ğ‘— is constant, the problem is equivalent to the Nearest Neighbour Search (NNS). Motivated by\nthis observation and to avoid the computational cost of learning sparsity patterns, Roy et al. [96]\nproposed the Routing Transformer that relies on an online mini-batch version ofğ‘˜-means and a set\nof centroids learned along the rest of the parameters. Like the Reformer, queries can only attend to\nkeys from the same cluster, inducing an adaptive or content-based sparsity pattern.\n4.2 Factorized Attention\nWang et al. [126] demonstrated that the attention matrix Softmax\n\u0010\nğ‘¸ğ‘² âŠ¤/\nâˆš\nğ‘‘\n\u0011\nis approximately low\nrank. Consequently, another approach to reduce the Transformerâ€™s complexity is to approximate\nthe attention by factorizing it into the product of two matrices with lower dimensions.\nLow-Rank Factorization [113, 126, 136]: Wang et al. [126] introduced the Linformer, a linear\ncomplexity model that approximates the attention with a low-rank factorization by first projecting\neach key to a lower dimension before performing the dot product, thereby saving time and memory.\nFormally, the low-rank attention is given by:\nAttention(ğ‘¿)= Softmax\n\u0012ğ‘¸ğ‘² âŠ¤\nâˆš\nğ‘‘\n\u0013\n|              {z              }\nğ‘›Ã—ğ‘›\nğ‘½\n|{z}\nğ‘›Ã—ğ‘‘\nâ‰ˆSoftmax\n\u0012ğ‘¸(ğ‘¬ğ‘² )âŠ¤\nâˆš\nğ‘‘\n\u0013\n|                   {z                   }\nğ‘›Ã—ğ‘˜\nğ‘­ ğ‘½\n|{z}\nğ‘˜Ã—ğ‘‘\n(22)\nwhere ğ‘¬,ğ‘­ âˆˆRğ‘˜Ã—ğ‘›, with ğ‘˜ â‰ªğ‘›, are two linear projection matrices learned during training. The\nauthors showed that ğ‘¬ and ğ‘­ could be shared across heads and layers with virtually no performance\npenalty.\nTay et al. [113] introduced a family of models called Synthesizers that learn the compatibility\nscores without computing the pairwise dot products between the queries and keys. For instance,\nthe Dense Synthesizer learns the compatibility scores with a simple position-wise feed-forward\nnetwork that projects each of the ğ‘›rows of ğ‘¿ from R1Ã—ğ‘‘ to R1Ã—ğ‘›:\nF(ğ‘¿ğ‘–)= max(0,ğ‘¿ğ‘–ğ‘¾1 +ğ’ƒ1)ğ‘¾2 +ğ’ƒ2 (23)\nwhere ğ‘¾1 âˆˆRğ‘‘Ã—ğ‘‘ and ğ‘¾2 âˆˆRğ‘‘Ã—ğ‘›. Finally, the attention is given by:\nAttention(ğ‘¿)= Softmax(ğ¹(ğ‘¿))ğº(ğ‘¿) (24)\nwhere ğº(Â·): Rğ‘›Ã—ğ‘‘ â†’Rğ‘›Ã—ğ‘‘ is a projection of the input akin to the values. In order to improve the\nefficiency, the authors proposed the Factorized Dense Synthesizer which first project the input ğ‘¿\nwith two feed-forward networks:\nğ‘¨ = ğ¹ğ´(ğ‘¿)âˆˆ Rğ‘›Ã—ğ‘ and ğ‘© = ğ¹ğµ(ğ‘¿)âˆˆ Rğ‘›Ã—ğ‘, (25)\nsuch thatğ‘Ã—ğ‘ = ğ‘›. Then, two tiling functionsğ»ğ´(Â·): Rğ‘›Ã—ğ‘ â†’Rğ‘›Ã—(ğ‘.ğ‘)and ğ»ğµ(Â·): Rğ‘›Ã—ğ‘ â†’Rğ‘›Ã—(ğ‘.ğ‘)\nare applied to ğ‘¨ and ğ‘©, respectively. Note that a tiling function simply repeats a vector multiple\ntimes. Finally, the attention of the Factorized Dense Synthesizer is given by:\nAttention(ğ‘¿)= Softmax(ğ»ğ´(ğ‘¨)ğ»ğµ(ğ‘©)âŠ¤)ğº(ğ‘¿) (26)\nAdditionally, the authors proposed a baseline called the Factorized Random Synthesizer, whose\ncompatibility scores are independent of the input. Formally, the Factorized Random Synthesizerâ€™s\nattention is given by:\nAttention(ğ‘¿)= Softmax(ğ‘¹1ğ‘¹âŠ¤\n2 )ğº(ğ‘¿) (27)\n24 Fournier et al.\nwhere ğ‘¹1,ğ‘¹2 âˆˆRğ‘›Ã—ğ‘˜ are two low-rank matrices learned during training. Although the Synthesizers\neliminate the need to compute the pairwise dot products, which speed up the model in practice,\nthe complexity remains quadratic with respect to the sequence length.\nThe NystrÃ¶mformer [136] relies on the NystrÃ¶m method to generate a low-rank approximation of\nthe Softmax matrix. However, applying the NystrÃ¶m method directly to the Softmax would require\nto compute the ğ‘¸ğ‘² âŠ¤product, which requires O(ğ‘›2)computations and memory. As a solution,\nthe NystrÃ¶mformer creates two subsets Ëœğ‘² and Ëœğ‘¸ of columns, called landmarks, from ğ‘² and ğ‘¸,\nrespectively. The authors applied the segment-means approach, which computes the landmarks as\nthe averages over predefined spans of keys and queries. Let ğ‘ºğ´ğµ denotes Softmax(ğ‘¨ğ‘©âŠ¤/\nâˆš\nğ‘‘)for\nany matrix ğ‘¨ and ğ‘©. The NystrÃ¶mformer approximates the Softmax matrix as:\nSoftmax\n\u0012ğ‘¸ğ‘² âŠ¤\nâˆš\nğ‘‘\n\u0013\nâ‰ˆğ‘ºğ‘„ Ëœğ¾ğ‘º+\nËœğ‘„ Ëœğ¾ğ‘º Ëœğ‘„ğ¾ (28)\nwhere the superscript +denotes the Moore-Penrose inverse typically computed with the singular\nvalue decomposition (SVD). Since the SVD is inefficient on GPU, the authors relied on an iterative\nmethod that approximate ğ‘º+\nËœğ‘„ Ëœğ¾ as ğ‘+. Finally, the NystrÃ¶mformerâ€™s attention is given by:\nAttention(ğ‘¿)â‰ˆ ğ‘ºğ‘„ Ëœğ¾ğ‘+ğ‘º Ëœğ‘„ğ¾ğ‘½ (29)\nwhich can be efficiently encoded in a computational graph.\nProvided that the number of landmarks is constant and much smaller than the sequence length,\nthe NystrÃ¶mformer complexity is O(ğ‘›). Depending on the number of landmarks and the sequence\nlength, the authors reported substantial gains over the Linformer and Longformer on the masked\nlanguage model and sentence order prediction objectives. Additionally, the representations learned\nby the NystrÃ¶mformer appear to transfer as well as BERT to different NLP tasks. Nonetheless, a\nmore extensive evaluation of the NystrÃ¶mformer remains necessary.\nKernel Attention [18, 56]: A kernel ğ¾(Â·,Â·)is a function that takes two vectors as arguments\nand returns the product of their projection by a feature map ğœ™(Â·):\nğ¾(ğ’™,ğ’š)= ğœ™(ğ’™)âŠ¤ğœ™(ğ’š) (30)\nKatharopoulos et al. [56] interpreted the Softmax as a kernel, decomposed it as an inner product in\nthe right space, and rearrange the computations in a clever way to reduce the complexity. More\nspecifically, the self-attention of a given query ğ‘¸ğ‘– may be rewritten using a mapping ğœ™(Â·):\nSoftmax\n\u0000\nğ‘¸ âŠ¤\nğ‘– ğ‘² âŠ¤\u0001\nğ‘½ =\nÃğ‘›\nğ‘—=1 exp\n\u0000\nğ‘¸ âŠ¤\nğ‘– ğ‘² ğ‘—\n\u0001\nğ‘½ ğ‘—\nÃğ‘›\nğ‘—=1 exp\n\u0000\nğ‘¸ âŠ¤\nğ‘– ğ‘² ğ‘—\n\u0001 =\nÃğ‘›\nğ‘—=1 ğœ™\n\u0000\nğ‘¸ğ‘–\n\u0001âŠ¤\nğœ™\n\u0000\nğ‘² ğ‘—\n\u0001\nğ‘½ ğ‘—\nÃğ‘›\nğ‘—=1 ğœ™\n\u0000\nğ‘¸ğ‘–\n\u0001âŠ¤\nğœ™\n\u0000\nğ‘² ğ‘—\n\u0001 =\nğœ™\n\u0000\nğ‘¸ğ‘–\n\u0001âŠ¤Ãğ‘›\nğ‘—=1 ğœ™\n\u0000\nğ‘² ğ‘—\n\u0001\nğ‘½ âŠ¤\nğ‘—\nğœ™\n\u0000\nğ‘¸ğ‘–\n\u0001âŠ¤Ãğ‘›\nğ‘—=1 ğœ™\n\u0000\nğ‘² ğ‘—\n\u0001 (31)\nwhere the scaling factor\nâˆš\nğ‘‘ has been omitted for the sake of readability. The authors noted\nthat Ãğ‘›\nğ‘—=1 ğœ™\u0000ğ‘² ğ‘—\n\u0001ğ‘½âŠ¤\nğ‘— and Ãğ‘›\nğ‘—=1 ğœ™\u0000ğ‘² ğ‘—\n\u0001 must only be computed a single time, therefore reducing the\ncomplexity from quadratic to linear both in terms of memory and computation. The vectorized\nformulation of the numerator makes it simpler to see:\nğœ™\u0000ğ‘¸\u0001\n|{z}\nğ‘›Ã—ğ‘\n\u0010\nğœ™\u0000ğ‘²\u0001âŠ¤\n| {z }\nğ‘Ã—ğ‘›\nğ‘½\n|{z}\nğ‘›Ã—ğ‘‘\n\u0001 (32)\nwhere the mapping ğœ™(Â·): Rğ‘‘ â†’Rğ‘ is applied position-wise. Unfortunately, the feature map of\nthe exponential kernel is infinite dimensional. Hence, any finite kernel is an approximation of the\nattention matrix and may be interpreted as a low-rank factorization. However, they are presented\nseparately here due to their conceptual difference. Katharopoulos et al . [56] approximated the\nA Practical Survey on Faster and Lighter Transformers 25\nattention matrix in the Linear Transformer with the feature map ğœ™(ğ‘¥)= elu(ğ‘¥)+ 1, where the\nfunction elu(Â·)denotes the exponential linear unit given by:\nelu(ğ‘¥)=\n\u001a ğ›¼(ğ‘’ğ‘¥ âˆ’1), ğ‘¥ < 0\nğ‘¥, ğ‘¥ â‰¥0 (33)\nwhere ğ›¼ is an hyperparameter. The Linear Transformer performed on par with the vanilla Trans-\nformer on autoregressive image generation, but poorly on automatic speech recognition.\nChoromanski et al. [18] later demonstrated that the exponential is equivalent to a kernel with a\nrandomized mapping:\nexp(ğ‘¥âŠ¤ğ‘¦)= Eğ‘¤âˆ¼N(0,ğ¼ğ‘‘ )\n\u0014\nexp\n\u0012\nğ‘¤âŠ¤ğ‘¥âˆ¥ğ‘¥âˆ¥2\n2\n\u0013\nexp\n\u0012\nğ‘¤âŠ¤ğ‘¦âˆ¥ğ‘¦âˆ¥2\n2\n\u0013 \u0015\n(34)\nConsequently, the authors introduced the Performer, a linear complexity model that approximates\nthe attention by means of a kernel with the following feature mapping:\nğœ™(ğ‘¥)= exp(âˆ’âˆ¥ğ‘¥âˆ¥2/2)âˆš2ğ‘\n\u0002\nexp\u0000ğ‘¤âŠ¤\n1 ğ‘¥\u0001; ... ; exp\u0000ğ‘¤âŠ¤\nğ‘ğ‘¥\u0001; exp\u0000 âˆ’ğ‘¤âŠ¤\n1 ğ‘¥\u0001; ... ; exp\u0000 âˆ’ğ‘¤âŠ¤\nğ‘ğ‘¥\u0001\u0003\n(35)\nwhere ğ‘¤ğ‘– âˆ¼N( 0,ğ¼ğ‘‘). To further reduce the variance of the estimator, ğ‘¤ğ‘– are constrained to be\nexactly orthogonal, which is achieved with the Gram-Schmidt process. The hyperparameter ğ‘\ncorresponds to the number of random features and controls the quality of the approximation.\nClustering and Locality-Sensitive Hashing [122]: As previously explained, clustering can\nuncover sparse patterns by grouping queries and keys and only computing the attention between\npositions within the same cluster. Alternatively, Vyas et al. [122] proposed to factorize the attention\nwith clustering by grouping queries into a fixed number of non-overlapping clusters and by\ncomputing the attention between the clusterâ€™s centroids and the keys. Consequently, the attention\nscore is only computed once per group of similar queries and broadcasted to all, resulting in linear\ncomplexity. Since queries may be clustered differently across attention heads and since the attention\nsub-layer includes a residual connection, two queries in the same cluster can have different output\nrepresentations. The authors proved that the approximation error for a given query is bounded\nby its distance to its centroid multiplied by the spectral norm of the keys matrix. As such, the\nK-Means algorithm can be used for minimizing the approximation error. However, K-Means in the\noriginal space would be slow to compute as Lloyd algorithm has a complexity of O(ğ‘›ğ‘ğ‘‘ğ‘™), where ğ‘\nis the number of clusters and ğ‘™ is the number of Lloyd iterations. Instead, the authors first used\na locality-sensitive hashing scheme on the queries before applying K-Means with the Hamming\ndistance, which reduces the complexity to O(ğ‘›ğ‘ğ‘™ +ğ‘ğ‘ğ‘™ +ğ‘›ğ‘‘ğ‘), where ğ‘is the number of bits used\nfor hashing.\nTo further improve the approximation, Vyas et al. [122] proposed the improved cluster attention\nthat separately consider the ğ‘˜ keys with the highest attention for each cluster. Intuitively, keys\nwith high approximated attention may have low attention for some queries, resulting in a large\napproximation error. As a solution, the dot product between these top- ğ‘˜ keys and all queries\nbelonging to the corresponding cluster is computed. Then, the attention is rescaled by the total\nprobability mass assigned to these top-ğ‘˜ keys.\nCompared to the Reformer, Vyas et al. [122] method is significantly faster ( 43% lower epoch\ntime) while being significantly more accurate (35% lower phone error rate) for speech recognition\non the Wall Street Journal.\n26 Fournier et al.\n4.3 Architectural Change\nFinally, the Transformerâ€™s complexity may also be reduced by modifying the modelâ€™s architecture\nand preserving the original attention mechanism. Let us investigate (i) the Transformer-XL and\nthe Compressive Transformer that rely on memory, and (ii) then the Funnel-Transformer that\niteratively compresses sequences.\nMemory [22, 93]: The block-wise approach splits the input sequence into small non-overlapping\nsubsequences called windows, blocks, or chunks, which are processed independently; therefore,\nthe maximum dependency length is equal to that of the subsequence. To leverage information from\nprevious windows, Dai et al. [22] introduced the Transformer-XL, which relies on segment-based\nrecurrence between windows. This recurrence mechanism is implemented by storing the represen-\ntations of the previous window in a first-in first-out memory (FIFO). Then, the attention mechanism\ncan attend to the representations located in this memory, but the gradients are not computed for\nthe attention on these elements. Although this model achieves a RECL four times greater than the\nvanilla Transformer with the same parameter budget, it cannot capture dependencies outside the\nFIFO memory range. Furthermore, this model is only compatible with autoregressive tasks. This\ntechnique is analogous to truncated backpropagation through time (BPTT), except that a sequence\nof hidden states is considered instead of the previous one. Figure 18 illustrates the segment-based\nrecurrence of the Transformer-XL.\nIn order to further increase the range of dependencies considered by the Transformer-XL, Rae\net al. [93] proposed the Compressive Transformer, which adds a compressed memory to the original\nFIFO memory. Representations of past windows are first stored in the standard FIFO memory, like\nthe Transformer-XL. Then, when this memory is full, the oldest representations are compressed with\na user-defined function and stored in the compressed FIFO memory instead of being discarded. The\nnumber of elements considered in the original FIFO memory to generate the compressed memory\ndepends on the chosen function. The authors propose using max/mean pooling, 1D convolution,\ndilated convolutions, or the most attended representations by the attention. They also proposed to\nlearn the compression function with an auxiliary auto-encoding loss and a variant called attention-\nreconstruction loss, which typically reconstructs the original memory from the compressed ones.\nThey show a clear advantage over the Transformer-XL on NLP tasks and comparable results on\nspeech modelling.\nFixed  \n(no gradient)\nFixed  \n(no gradient)\nCurrent window Current window Current windowPrevious window \n(considered)\nPrevious window \n(considered)\nPrevious window \n(not considered)\nFig. 18. Segment-based recurrence, which is similar to truncated BPTT. The window size is equal to two, and\nonly the previous window is considered. For the sake of clarity, parameters from and to states that do not\ncontribute are omitted.\nSequence Compression [21]: Many tasks such as image classification and sentiment analysis\nonly require producing a single output for the whole sequence. Dai et al . [21] argued that the\nfull-length sequence of hidden states may contain significant redundancy and that the model may\nA Practical Survey on Faster and Lighter Transformers 27\nnot have to preserve token-level information. Consequently, they proposed the Funnel-Transformer,\nwhose encoder reduces the computational cost by gradually reducing the length of the hidden\nstates sequence with pooling. Note that instead of directly feeding the pooled sequence into the\nattention layer, it is only used to construct the query matrix, while the unpooled sequence is used\nto construct the key and value matrices. Additionally, the authors proposed to recover the original\nsequence length by up-sampling the compressed sequence of hidden states to address the common\npre-training objectives, such as MLM, that require separate representation for each token. Although\nthe Funnel-Transformer effectively reduces the computational and memory cost of the encoder, the\ncomplexity remains quadratic, and the best performances are achieved on tasks that only require\nsequence-level representation.\n5 SHORTCOMINGS\nThis section discusses the lack of understanding of the self-attention inner workings and the\nlimitation of the Transformer evaluation methodology, including the lack of standard benchmarks\nfor long-range dependencies.\nSelf-attention is a relatively new mechanism that has been quickly and widely adopted due\nto its remarkable empirical success. Nonetheless, the self-attention inner workings are not yet\nfully understood, and many questions remain unanswered, including why it works, what it learns,\nand whether it is interpretable. Answering those questions is crucial to designing faster and\nlighter Transformers that are competitive with the original model. As of this paperâ€™s writing,\nthe deep learning community actively investigates self-attention and have proposed preliminary\nanswers to the aforementioned questions. For instance, evidence supporting both the self-attention\ninterpretability [101, 131] and non-interpretability [ 53] have been published. Tay et al . [113]\nempirically evaluated the dot product impact on natural language processing tasks and concluded\nthat query-keys interaction is â€œuseful but not that important â€. Kitaev et al. [59] investigated the\nimpact of sharing queries and keys, and concluded that â€œit turns out that sharing QK does not affect\nthe performance of Transformer â€.\nDespite our current limited understanding of the self-attention mechanism, a wide range of\nfaster and lighter Transformers have been introduced in a short amount of time, each claiming\ncomparable or superior performance to the vanilla Transformer. Since there is no consensus on\nhow to evaluate the proposed approaches [115], researchers often have to evaluate their method on\na small range of tasks. However, different tasks may require different assumptions, which means\nthat one method may work well on a specific task but poorly on others. For instance, Tay et al .\n[113] showed that a simple Synthesizer is highly competitive with the vanilla Transformer across a\nrange of natural language processing tasks, including machine translation, language modelling, and\ntext generation. However, Tay et al. [115] later showed that the vanilla Transformer outperforms\nthe Synthesizer on the more difficult Long-Range Arena benchmark. Long-Range Arena [115] is a\nsuite of five general and challenging tasks designed to evaluate how well Transformers capture\nlong-term dependencies from different modalities such as text, natural and synthetic images, and\nmathematical expressions. Table 3 compiles the Long-Range Arena results of the models discussed\nin the survey. For a complete description of the objectives and datasets, we refer the reader to the\noriginal paper.\nFurthermore, due to Transformers large training cost, researchers often evaluate their approach\nagainst a limited number of models on the tasks of interest. For instance, [59] only evaluated the\nReformer against three distinct vanilla Transformers [85, 121] on three tasks. Standardized suites of\nbenchmarks such as GLUE and the recent Long-Range Arena allow researchers and practitioners to\nevaluate only their method and compare it against a public leaderboard. Consequently, we highly\nrecommend that researchers consider such benchmarks.\n28 Fournier et al.\nAlthough standardized benchmarks such as Long-Range Arena would help compare the models,\nthe results should be taken with caution since the performance depends on the model size and\nhyperparameters, the speed depends on the implementation and hardware, and the memory\nfootprint depends on the implementation and general methods used. For instance, the Switch\nTransformer uses a mixture of experts, mixed-precision, expert dropout, knowledge distillation,\nand a careful initialization. Therefore, it is difficult to isolate the benefit of a single modification.\nFinally, the complexity is not always representative of the practical efficiency. For instance,\nthe Reformer achieves an asymptotic complexity of O(ğ‘›logğ‘›)but is significantly slower than\nthe vanilla Transformer on small sequences, as shown in Table 3. This slow down is due to large\nconstants hidden in the complexity. Even when there are no hidden constants, there is a distinction\nbetween theoretical complexity and what is achievable in practice. For instance, sparse matrix\nmultiplication may reduce the complexity from quadratic to linear in theory. However, it is well\nknown that GPUs and TPUs are not designed to perform such operations efficiently [11] and, in\npractice, sparse matrix multiplication is often slower than dense ones. We encourage researchers to\nexplicitly report the complexity as well as the number of floating operations (FLOPs), the wall-clock\ntime with the hardware, and the memory footprint of their method.\nTable 3. Long-Range Arena benchmark [115]. Results have been compiled from the original paper. Benchmarks\nare run on 4x4 TPU V3 chips, and the memory is reported per device.\nModels Average score (%) Steps per second Peak memory (GB)\n1K 4K 1K 4K\nTransformer [121] 54.39 8.1 1.4 0.85 9.48\nSparse Transformer9 [16] 51.24\nLongformer9 [5] 53.46\nBigBird [139] 55.01 7.4 1.5 0.77 2.88\nSinkhorn Transformer [114] 51.39 9.1 5.3 0.47 1.48\nReformer [59] 50.67 4.4 1.1 0.48 2.28\nLinformer [126] 51.36 9.3 7.7 0.37 0.99\nSynthesizer [113] 51.39 8.7 1.9 0.65 6.99\nLinear Transformer [56] 50.55 9.1 7.8 0.37 1.03\nPerformer [18] 51.41 9.5 8.0 0.37 1.06\n6 BROADER IMPACT OF EFFICIENT TRANSFORMER\nThis section extends the three motivations and potential impacts of lighter and faster Transformers\nbriefly discussed in Section 2.4.\nFirst and foremost, computational resources are not only finite but also expensive. Consequently,\nthere are severe inequalities between research groups and between companies. Indeed, many\nresearchers do not have access to GPU or TPU farms, and most companies cannot afford to spend\nthousands or millions of dollars on dedicated hardware, especially if deep learning is not their\nprimary focus. At this time, the resources disparities have increased dramatically to a point where\nonly a few parties can afford to train massive state-of-the-art models. A prime example of this\ncleavage is the Transformer. Indeed, the largest Transformers are so expensive to train, even for\nlarge companies such as Microsoft, that they are only trained once. For instance, Brown et al. [10]\nnoticed an issue in their pre-processing after training GPT-3. As the author explained, they could\nnot train their model again due to the massive cost and therefore published their results with a\n9The Sparse Transformer and Longformer depends on CUDA kernels that are difficult to implement on TPUs. Therefore,\nTay et al. [115] used equivalent implementations to emulate their performance and did not report their efficiency.\nA Practical Survey on Faster and Lighter Transformers 29\nknown issue. Resources inequalities also hinder creativity as researchers with promising ideas\nmay not be able to implement them, thus reinforcing the vicious â€œrich get richerâ€ circle, where\nwell-funded groups and companies that have access to more resources are more likely to achieve\nstate-of-the-art results and receive more fundings [108].\nAdditionally, lower-complexity Transformers enable novel applications as extremely long se-\nquences cannot be processed in a reasonable amount of time by the quadratic complexity vanilla\nTransformer. For instance, Choromanski et al. [18] observed the Performerâ€™s potential impact on\nbiology, and Zaheer et al. [139] evaluated BigBird on genomics tasks that take fragments of DNA as\ninput. Huang et al. [46] were able to generate minute-long musical compositions with a Transformer\nthat leverage the block-wise approach and an efficient computation of the relative attention. Note\nthat contrary to the attention introduced by [121], the relative attention [102] explicitly models the\ninput positions. The range of applications will surely expand as researchers design ever-lighter and\n-faster Transformers.\nFinally, recent research made it clear that we must cut carbon dioxide (CO2) emissions in half\nover the next decade to limit global warming. The large-scale infrastructures used by the deep\nlearning community consume a considerable amount of electricity, which is mainly produced\nby non-renewable sources such as coal or gas [49]. Strubell et al. [108] estimated that training a\nTransformer with neural architecture search generates up to 284,000 kg of CO2. For reference,\nthe average American emits 16,400 kg of CO2 per year, and the average car emits about 57,200\nkg during its lifetime10 (fuel included). The authors estimated that training a single instance of\nBERT [24] on GPU produces about the same amount of CO2 as a trans-American flight. Although\nlighter and faster models require fewer resources and therefore produce less carbon dioxide, they\nare also more accessible, so we would expect more models to be trained. Overall, it is difficult to\nknow whether lighter and faster Transformers will positively impact the environment. Nonetheless,\nresearchers and practitioners ought to have in mind the significant environmental impact of their\nexperiments, which can be estimated with the Machine Learning Emissions Calculator11 developed\nby Luccioni et al. [75].\n7 FUTURE RESEARCH DIRECTIONS\nIn our opinion, the current research directions follow one of two purposes: (i) efficiency and\naffordability or (ii) generalization performance. Since this survey addresses approaches to yield\nfaster and lighter Transformers, let us start with the efficiency and affordability objective.\n7.1 Efficiency and Affordability\nTo the best of our knowledge, researchers and practitioners have not yet identified a specialized\napproach that improves the Transformerâ€™s efficiency for every task, dataset, and hardware, as\nexplained in Section 5. In our opinion, one of the most promising avenues is to learn adaptively\nsparse patterns that are structured for the available hardware. Let us justify our claim.\nThe Softmax function only contains a few large values due to its exponential nature. Therefore,\nit can be effectively approximated by masking the positions with small weights. In theory, the\ncomputation and memory reduction is linearly proportional to the ratio of masked positions. In\npractice, however, the improvement depends on the hardware. As of this surveyâ€™s writing, NVIDIA\nis the first and only manufacturer to offer an architecture that natively supports sparse operations,\nresulting in a virtually perfect speed-up. One may reasonably expect other manufacturers to follow\nthis direction due to the prevalence of sparse operations in deep learning. Therefore, the sparse\n10A product lifetime or lifecycle typically includes material production, manufacturing, usage, and end-of-life disposal.\n11https://mlco2.github.io/impact/\n30 Fournier et al.\npatterns should be structured such that the hardware natively supports them. Handcrafting features\nor patterns based on prior knowledge is known to be suboptimal. Instead, the model should learn\nthe patterns from the data for the task at hand. Additionally, individual samples are likely to require\ndifferent attention patterns, and hence, the patterns should be adaptative (content-based). Finally,\nwe believe it is beneficial to include global tokens since they allow any position to attend to any\nother position in two layers, thus preserving the attentionâ€™s expressiveness.\n7.2 Generalization Performance\nA second research venue consists in improving the network generalization performance. Since the\ndeep learning renaissance associated with greedy layer-wise unsupervised pre-training [36], there\nhas been a clear trend towards scaling up neural networks. As a result, researchers and practitioners\nhave been able to leverage ever-larger datasets and ultimately improve the networkâ€™s performance.\nIn this setting, scaling is performed typically by increasing the number of layers, the number of\nattention heads, the input embedding dimension, and the feedforward network width.\nAmongst others, Radford et al. [92] introduced a large Transformer called GPT-2 and evaluated\nvarious model sizes on language modelling tasks in a zero-shot setting. The authors reported that\nthe performance significantly increased with the model size ranging from 117M to 1.5B parameters.\nRecently, Brown et al. [10] introduced GPT-3 based on the GPT-2 architecture and considered an\neven wider span of model sizes, ranging from 125M to 175B parameters. The authors reported\nthat the model performance smoothly increased with the model size in most cases and suggested\nthat this trend should extend to even larger models. Furthermore, Devlin et al. [24] investigated\nthe effect of BERT size on the GLUE benchmark and concluded that â€œlarger models lead to a strict\naccuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training\nexamples, and is substantially different from the pre-training tasks â€.\nThese observations suggest that researchers and practitioners must scale their model to pursue\nthe generalization performance objective. Inherently, scaling is resource-expensive and goes against\nthe affordability sought in this survey. Nonetheless, there are research directions to improve the\ngeneralization capability of deep learning models that are orthogonal to scaling and thus compatible\nwith efficiency. A promising avenue is structural inductive biases. A recent structural inductive\nbias inspired by independent mechanisms in the causality literature consists of designing an\narchitecture that learns sparsely interacting modules, each one of them specialized in a different\nmechanism [37]. Ideally, individual modules should be robust to changes in the aspects of the world\nthat are unrelated to this module, such as in the case of distributional shift. Lamb et al. [61] applied\nthis idea to Transformers by introducing the Transformers with Independent Mechanisms (TIM).\nThe authors observed that TIM layers could be combined with the mixture of experts approach,\nallowing the switching to be specific to distinct aspects of the data.\nCombining universally effective and efficient approaches such as the aforementioned sparse\npatterns with conditional computing and the independent mechanisms prior appears to be promising\nto tackle complex tasks without relying on large-scale resources.\n8 CONCLUSION\nTransformers have quickly become the de facto model for processing sequences, notably achieving\nstate-of-the-art in most natural language processing tasks at the cost of quadratic complexity. As a\nresult, researchers have leveraged numerous techniques to mitigate this memory and computational\nburden. This survey investigated popular general methods to make neural networks lighter and\nfaster and discussed their strengths and limitations. Notably, we advised researchers and prac-\ntitioners to use mixed-precision and gradient checkpointing due to their simplicity and overall\nA Practical Survey on Faster and Lighter Transformers 31\nbenefits. Often, these general techniques are not sufficient to mitigate the Transformerâ€™s complex-\nity. Consequently, this survey reviewed the lower-complexity variations of the Transformer and\ndiscussed their assumptions, justification and shortcomings. Notably, we advised researchers and\npractitioners to rely on pre-trained models whenever possible. Otherwise, we recommend training\na small vanilla Transformer with mixed-precision and gradient checkpointing to apprehend the\ndependencies required for the task and select the appropriate models accordingly. Additionally, we\ndiscussed the potential impacts of affordable Transformers, including improving the state-of-the-art,\nextending the range of applications, increasing the equity between researchers, and potentially\nreducing the environmental impact. Finally, we highlighted promising future research directions\nfor this exciting architecture.\nACKNOWLEDGMENTS\nWe would like to gratefully acknowledge the Natural Sciences and Engineering Research Council\nof Canada (NSERC), Prompt, Ericsson, Ciena, and EfficiOS for funding this research.\nREFERENCES\n[1] MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, et al. 2015. TensorFlow:\nLarge-Scale Machine Learning on Heterogeneous Systems. https://www .tensorflow.org/\n[2] Jimmy Ba and Rich Caruana. 2014. Do Deep Nets Really Need to be Deep?. In NIPS, Vol. 27.\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to\nAlign and Translate. In ICLR.\n[4] Irwan Bello. 2021. LambdaNetworks: Modeling long-range Interactions without Attention. In ICLR.\n[5] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer.arXiv e-prints\n(2020), arXiv:2004.05150.\n[6] Yoshua Bengio. 2013. Deep Learning of Representations: Looking Forward. In SLSP, Vol. 7978. 1â€“37.\n[7] Yoshua Bengio. 2013. Estimating or Propagating Gradients Through Stochastic Neurons. CoRR abs/1305.2982 (2013).\n[8] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from Question-\nAnswer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing . 1533â€“1544.\n[9] Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, et al. 2014.\nFindings of the 2014 Workshop on Statistical Machine Translation. In SIGMT. 12â€“58.\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, et al. 2020. Language\nModels are Few-Shot Learners. In NeurIPS, Vol. 33. 1877â€“1901.\n[11] A. Buluc and J. R. Gilbert. 2008. Challenges and Advances in Parallel Sparse Matrix-Matrix Multiplication. In 2008\n37th International Conference on Parallel Processing . 503â€“510.\n[12] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020.\nEnd-to-End Object Detection with Transformers. In ECCV. 213â€“229.\n[13] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016. Listen, attend and spell: A neural network for large\nvocabulary conversational speech recognition. In ICASSP. 4960â€“4964.\n[14] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training Deep Nets with Sublinear Memory Cost.\nCoRR abs/1604.06174 (2016).\n[15] Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long Short-Term Memory-Networks for Machine Reading. In\nEMNLP. 551â€“561.\n[16] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating Long Sequences with Sparse Transformers.\nCoRR abs/1904.10509 (2019).\n[17] Kyunghyun Cho, Bart van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, et al.\n2014. Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation. In EMNLP.\n1724â€“1734.\n[18] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,\net al. 2021. Rethinking Attention with Performers. In ICLR.\n[19] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: Pre-training Text\nEncoders as Discriminators Rather Than Generators. In ICLR.\n[20] GonÃ§alo M. Correia, Vlad Niculae, and AndrÃ© F. T. Martins. 2019. Adaptively Sparse Transformers. In EMNLP-IJCNLP.\n2174â€“2184.\n32 Fournier et al.\n[21] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-Transformer: Filtering out Sequential Redundancy\nfor Efficient Language Processing. In NeurIPS.\n[22] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL:\nAttentive Language Models beyond a Fixed-Length Context. In ACL. 2978â€“2988.\n[23] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. 2019. Universal Transformers.\nIn ICLR.\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In NAACL-HLT. 4171â€“4186.\n[25] Laurent Dinh, David Krueger, and Yoshua Bengio. 2015. NICE: Non-linear Independent Components Estimation. In\nICLR.\n[26] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2017. Density estimation using Real NVP. In ICLR.\n[27] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, et al.\n2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR.\n[28] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. 2020. Depth-Adaptive Transformer. In ICLR.\n[29] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Neural Architecture Search: A Survey. JMLR 20, 55\n(2019), 1â€“21.\n[30] William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to Trillion Parameter Models\nwith Simple and Efficient Sparsity. arXiv e-prints (2021), arXiv:2101.03961.\n[31] Quentin Fournier, Daniel Aloise, Seyed Vahid Azhari, and FranÃ§ois Tetreault. 2021. On Improving Deep Learning\nTrace Analysis with System Call Arguments. In MSR. 120â€“130.\n[32] Jonathan Frankle and Michael Carbin. 2019. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural\nNetworks. In ICLR.\n[33] Andrea Galassi, Marco Lippi, and Paolo Torroni. 2020. Attention in Natural Language Processing. TNNLS (2020),\n1â€“18.\n[34] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks.\nIn AISTATS, Vol. 9. 249â€“256.\n[35] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. 2017. The Reversible Residual Network:\nBackpropagation Without Storing Activations. In NeurIPS, Vol. 30.\n[36] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . http://www .deeplearningbook.org\n[37] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, et al. 2019. Recurrent\nIndependent Mechanisms. CoRR abs/1909.10893 (2019).\n[38] Alex Graves. 2016. Adaptive Computation Time for Recurrent Neural Networks. CoRR abs/1603.08983 (2016).\n[39] Scott Gray, Alec Radford, and Diederik P. Kingma. 2017. GPU Kernels for Block-Sparse Weights.\n[40] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, et al. 2020. Conformer: Convolution-\naugmented Transformer for Speech Recognition. In Interspeech. 5036â€“5040.\n[41] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-Transformer. In\nNAACL-HLT. 1315â€“1325.\n[42] K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep Residual Learning for Image Recognition. In CVPR. 770â€“778.\n[43] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. arXiv e-prints\n(2015), arXiv:1503.02531.\n[44] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory.Neural Computation 9, 8 (1997), 1735â€“1780.\n[45] Sara Hooker. 2020. The Hardware Lottery. CoRR abs/2009.06489 (2020).\n[46] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, et al. 2019.\nMusic Transformer. In ICLR.\n[47] Xiao Shi Huang, Felipe PÃ©rez, Jimmy Ba, and Maksims Volkovs. 2020. Improving Transformer Optimization Through\nBetter Initialization. In ICML, Vol. 119. 4475â€“4483.\n[48] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, et al. 2019. GPipe: Efficient\nTraining of Giant Neural Networks using Pipeline Parallelism. In NeurIPS, Vol. 32.\n[49] IEA. 2018. World gross electricity production, by source, 2018. https://www .iea.org/data-and-statistics/charts/world-\ngross-electricity-production-by-source-2018\n[50] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift. In ICML, Vol. 37. 448â€“456.\n[51] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, et al. 2018. Quantization and Training of Neural Networks\nfor Efficient Integer-Arithmetic-Only Inference. In IEEE/CVF. 2704â€“2713.\n[52] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. 1991. Adaptive Mixtures of Local Experts.Neural Computation\n3 (1991), 79â€“87.\n[53] Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. CoRR abs/1902.10186 (2019).\nA Practical Survey on Faster and Lighter Transformers 33\n[54] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, et al. 2020. TinyBERT: Distilling BERT for\nNatural Language Understanding. In EMNLP. 4163â€“4174.\n[55] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, et al . 2019. A\nComparative Study on Transformer vs RNN in Speech Applications. In ASRU. 449â€“456. https://doi .org/10.1109/\nASRU46091.2019.9003750\n[56] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. 2020. Transformers are RNNs: Fast\nAutoregressive Transformers with Linear Attention. In ICML, Vol. 119. 5156â€“5165.\n[57] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah.\n2021. Transformers in Vision: A Survey. ACM Comput. Surv. (2021).\n[58] Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp Nearby, Fuzzy Far Away: How Neural Language\nModels Use Context. In ACL. 284â€“294.\n[59] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient Transformer. In ICLR.\n[60] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, et al . 2020. Big\nTransfer (BiT): General Visual Representation Learning. In ECCV, Vol. 12350. 491â€“507.\n[61] Alex Lamb, Di He, Anirudh Goyal, Guolin Ke, Chien-Feng Liao, Mirco Ravanelli, et al . 2021. Transformers with\nCompetitive Ensembles of Independent Mechanisms. arXiv e-prints (2021), arXiv:2103.00336.\n[62] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT:\nA Lite BERT for Self-supervised Learning of Language Representations. In ICLR.\n[63] Yann LeCun, John S. Denker, and Sara A. Solla. 1990. Optimal Brain Damage. In NIPS. 598â€“605.\n[64] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. arXiv e-prints (2016),\narXiv:1607.06450.\n[65] Mohan Li, Catalin Zorila, and Rama Doddipatla. 2020. Transformer-based Online Speech Recognition with Decoder-\nend Adaptive Computation Steps. arXiv e-prints (2020), arXiv:2011.13834.\n[66] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, et al. 2019. Enhancing the Locality\nand Breaking the Memory Bottleneck of Transformer on Time Series Forecasting. In NeurIPS. 5244â€“5254.\n[67] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. 2020. Reconciling Modern Deep Learning with Traditional Optimization\nAnalyses: The Intrinsic Learning Rate. In NeurIPS, Vol. 33. 14544â€“14555.\n[68] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2021. A Survey of Transformers. arXiv:2106.04554\n[69] Chunxi Liu, Frank Zhang, Duc Le, Suyoun Kim, Yatharth Saraf, and Geoffrey Zweig. 2021. Improving RNN Transducer\nBased ASR with Auxiliary Tasks. In SLT. 172â€“179.\n[70] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le. 2021. Pay Attention to MLPs. ArXiv abs/2105.08050 (2021).\n[71] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2019. DARTS: Differentiable Architecture Search. In ICLR.\n[72] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, et al. 2020. On the Variance of\nthe Adaptive Learning Rate and Beyond. In ICLR.\n[73] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. 2020. Understanding the Difficulty of Training\nTransformers. In EMNLP. 5747â€“5763.\n[74] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, et al . 2019. RoBERTa: A Robustly\nOptimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).\n[75] Alexandra Luccioni, Alexandre Lacoste, and Victor Schmidt. 2020. Estimating Carbon Emissions of Artificial Intelli-\ngence [Opinion]. IEEE Technology and Society Magazine 39, 2 (2020), 48â€“51.\n[76] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. 2017. The Concrete Distribution: A Continuous Relaxation of\nDiscrete Random Variables. In ICLR.\n[77] Matt Mahoney. 2011. Large Text Compression Benchmark. http://mattmahoney .net/dc/text.html\n[78] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer Sentinel Mixture Models. In\nICLR.\n[79] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really Better than One?. In NeurIPS, Vol. 32.\n[80] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, et al. 2018. Mixed\nPrecision Training. In ICLR.\n[81] Nikita Nangia and Samuel R. Bowman. 2018. ListOps: A Diagnostic Dataset for Latent Tree Learning. In NAACL.\n[82] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, et al. 2021. Do Transformer\nModifications Transfer Across Implementations and Applications? arXiv e-prints (2021), arXiv:2102.11972.\n[83] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Donâ€™t Give Me the Details, Just the Summary! Topic-Aware\nConvolutional Neural Networks for Extreme Summarization. In EMNLP. 1797â€“1807.\n[84] OpenAI. 2013. Saving memory using gradient-checkpointing. https://github.com/openai/gradient-checkpointing.\n[85] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling Neural Machine Translation. InProceedings\nof the Third Conference on Machine Translation: Research Papers . 1â€“9.\n34 Fournier et al.\n[86] Daniel S. Park, Yu Zhang, Chung-Cheng Chiu, Youzheng Chen, Bo Li, William Chan, et al. 2020. Specaugment on\nLarge Scale Datasets. In ICASSP. 6879â€“6883.\n[87] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, et al. 2019. PyTorch: An\nImperative Style, High-Performance Deep Learning Library. In NeurIPS. 8024â€“8035.\n[88] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. 2018. Efficient Neural Architecture Search via\nParameter Sharing. In ICML, Vol. 80. 4092â€“4101.\n[89] Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020. When BERT Plays the Lottery, All Tickets Are Winning. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 3208â€“3229.\n[90] Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. 2020. Blockwise Self-Attention for Long\nDocument Understanding. In EMNLP. 2555â€“2565.\n[91] Alec Radford and Karthik Narasimhan. 2018. Improving Language Understanding by Generative Pre-Training.\n[92] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2018. Language Models are\nUnsupervised Multitask Learners. (2018).\n[93] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020. Compressive\nTransformers for Long-Range Sequence Modelling. In ICLR.\n[94] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine\nComprehension of Text. In EMNLP. 2383â€“2392.\n[95] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. 2019. Regularized Evolution for Image Classifier\nArchitecture Search. In AAAI. 4780â€“4789.\n[96] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient Content-Based Sparse Attention\nwith Routing Transformers. TACL 9 (2021), 53â€“68.\n[97] D. E. Rumelhart, P. Smolensky, J. L. McClelland, and G. E. Hinton. 1986. Schemata and Sequential Thought Processes in\nPDP Models . 7â€“57.\n[98] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2020. On the Effect of Dropping Layers of Pre-trained\nTransformer Models. arXiv e-prints (2020), arXiv:2004.03844.\n[99] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT:\nsmaller, faster, cheaper and lighter. CoRR abs/1910.01108 (2019).\n[100] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. 2018. How Does Batch Normalization\nHelp Optimization?. In NeurIPS, Vol. 31.\n[101] Sofia Serrano and Noah A. Smith. 2019. Is Attention Interpretable?. In ACL. 2931â€“2951.\n[102] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with Relative Position Representations. In\nNAACL-HLT. 464â€“468.\n[103] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, et al . 2017.\nOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.. In ICLR.\n[104] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, et al. 2021. SparseBERT: Rethinking the\nImportance Analysis in Self-attention. In ICML, Vol. 139. 9547â€“9557.\n[105] David R. So, Quoc V. Le, and Chen Liang. 2019. The Evolved Transformer. In ICML, Vol. 97. 5877â€“5886.\n[106] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a\nsimple way to prevent neural networks from overfitting. JMLR 15, 1 (2014), 1929â€“1958.\n[107] Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, RÃ©mi Gribonval, Herve Jegou, et al. 2021. Training with\nQuantization Noise for Extreme Model Compression. In ICLR.\n[108] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and Policy Considerations for Deep Learning\nin NLP. CoRR abs/1906.02243 (2019).\n[109] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2020. Energy and Policy Considerations for Modern Deep\nLearning Research. Proceedings of the AAAI Conference on Artificial Intelligence 34, 09 (2020), 13693â€“13696.\n[110] Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan Zhong. 2021. Attention Is All You Need\nIn Speech Separation. In ICASSP. 21â€“25.\n[111] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive Attention Span in\nTransformers. In ACL. 331â€“335.\n[112] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to Sequence Learning with Neural Networks. In NIPS.\n3104â€“3112.\n[113] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2020. Synthesizer: Rethinking\nSelf-Attention in Transformer Models. arXiv e-prints (2020), arXiv:2005.00743.\n[114] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse Sinkhorn Attention. InICML, Vol. 119.\n9438â€“9447.\n[115] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, et al. 2021. Long Range Arena : A\nBenchmark for Efficient Transformers. In ICLR.\nA Practical Survey on Faster and Lighter Transformers 35\n[116] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient Transformers: A Survey. CoRR\nabs/2009.06732 (2020).\n[117] Wilson L. Taylor. 1953. â€œCloze Procedureâ€: A New Tool for Measuring Readability. Journalism Quarterly 30, 4 (1953),\n415â€“433.\n[118] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, et al. 2021.\nMLP-Mixer: An all-MLP Architecture for Vision. CoRR abs/2105.01601 (2021).\n[119] Henry Tsai, Jayden Ooi, Chun-Sung Ferng, Hyung Won Chung, and Jason Riesa. 2020. Finding Fast Transformers:\nOne-Shot Neural Architecture Search by Component Composition. arXiv e-prints (2020), arXiv:2008.06808.\n[120] Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. 2019. Small and Practical\nBERT Models for Sequence Labeling. In EMNLP-IJCNLP. 3632â€“3636.\n[121] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, et al. 2017. Attention is\nAll you Need. In NIPS. 5998â€“6008.\n[122] Apoorv Vyas, Angelos Katharopoulos, and FranÃ§ois Fleuret. 2020. Fast Transformers with Clustered Attention. In\nNeurIPS.\n[123] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, et al. 2019. SuperGLUE:\nA Stickier Benchmark for General-Purpose Language Understanding Systems. In NeurIPS, Vol. 32.\n[124] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task\nBenchmark and Analysis Platform for Natural Language Understanding. In EMNLP. 353â€“355.\n[125] Chenguang Wang, Zihao Ye, Aston Zhang, Zheng Zhang, and Alexander J. Smola. 2020. Transformer on a Diet. arXiv\ne-prints (2020), arXiv:2002.06170.\n[126] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-Attention with Linear\nComplexity. arXiv e-prints (2020), arXiv:2006.04768.\n[127] Yuxin Wang, Qiang Wang, Shaohuai Shi, Xin He, Zhenheng Tang, Kaiyong Zhao, et al. 2020. Benchmarking the\nPerformance and Energy Efficiency of AI Accelerators for AI Training. In CCGRID. 744â€“751.\n[128] Yu Emma Wang, Gu-Yeon Wei, and David Brooks. 2019. Benchmarking TPU, GPU, and CPU Platforms for Deep\nLearning.\n[129] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural Network Acceptability Judgments. TACL 7\n(2019), 625â€“641.\n[130] Lilian Weng. 2018. Attention? Attention! http://lilianweng .github.io/lil-log/2018/06/24/attention-attention .html\n[131] Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not Explanation. In EMNLP-IJCNLP. 11â€“20.\n[132] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, et al. 2020. Trans-\nformers: State-of-the-Art Natural Language Processing. In EMNLP. 38â€“45.\n[133] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. 2020. Lite Transformer with Long-Short Range Attention.\nIn ICLR.\n[134] Qizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and Quoc V. Le. 2020. Self-Training With Noisy Student Improves\nImageNet Classification.. In CVPR. 10684â€“10695.\n[135] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, et al. 2020. On Layer Normalization in\nthe Transformer Architecture. In ICML, Vol. 119. 10524â€“10533.\n[136] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, et al. 2021. NystrÃ¶mformer:\nA NystrÃ¶m-Based Algorithm for Approximating Self-Attention. arXiv e-prints (2021), arXiv:2102.03902.\n[137] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. 2022.\nMetaFormer Is Actually What You Need for Vision. In CVPR. 10819â€“10829.\n[138] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8BERT: Quantized 8Bit BERT. EMC2-NIPS.\n[139] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, et al. 2020.\nBig Bird: Transformers for Longer Sequences. In NeurIPS, Vol. 33. 17283â€“17297.\n[140] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, et al . 2020. Transformer\nTransducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss. In ICASSP.\n7829â€“7833.\n[141] Barret Zoph and Quoc V. Le. 2017. Neural Architecture Search with Reinforcement Learning. In ICLR.\n[142] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. 2018. Learning Transferable Architectures for\nScalable Image Recognition. In CVPR. 8697â€“8710.\n36 Fournier et al.\nA INTRODUCTION TO MACHINE LEARNING\nAt the dawn of artificial intelligence (AI), researchers rapidly tackled and solved problems that were\nchallenging for humans but relatively straightforward for computers in that they could be described\nas a set of rules. Chess may certainly be the epitome of such complex tasks solved brilliantly by\nartificial intelligence. Nonetheless, despite the achievements of AI, simpler tasks that humans\nsolve instinctively proved to be much more challenging as they are not easily expressed formally.\nAmongst others, speech and object recognition were â€“ and still are to some extent â€“ challenging\nproblems to solve for computers. Machine learning (ML) provides a solution to intuitive problems\nby allowing computers to learn from experience instead of relying on human knowledge to specify\nthe tasks or their solution. The seemingly ever-increasing amount of data produced every day\nhas enabled machine learning to become suitable and successful for a wide range of simple and\ncomplex problems. Since the renaissance of deep learning (DL) associated with greedy layer-wise\npre-training, neural networks have become the most popular family of algorithms for machine\nlearning as they learn a hierarchy of concepts, with each concept defined through its relation to\nsimpler concepts. As of the writing of this survey, deep learning, and more generally artificial\nintelligence, has become a thriving field with numerous practical applications that directly impact\ncountless human lives, from medical diagnoses to movie recommendations.\nB PRACTICAL GUIDELINES - GENERAL METHODS\nThe general approaches presented in Section 3 apply to the original Transformer as well as its\nlower-complexity alternatives. Therefore, they are discussed before introducing the specialized\napproaches in Section 4. In particular, this section provides practitioners and researchers with\na series of guidelines on which methods to apply depending on the bottleneck and whether it\noccurs during optimization or inference. The distinction between optimization (i.e. pre-training\nand training) and inference is motivated by the former being significantly more resource-intensive\nthan the latter.\nThe primary focus of this survey is to make Transformers more efficient and ultimately more\naffordable. Therefore, only substantial performance losses will be mentioned, along with other\nsignificant drawbacks such as incompatibilities and instabilities. Unless specified otherwise, the\nmethods are readily available in PyTorch [ 87] and Tensorflow [1], two standard deep-learning\nlibraries.\nB.1 Optimization\nOptimization is the most resource-intensive phase, prominently due to the iterative nature of the\nprocess, the quadratic complexity of the attention mechanism, and the in-memory recording of\nintermediate values during the forward pass. Consequently, most of the above approaches to reduce\ncomputation, memory, or both, focus on optimization.\nB.1.1 Computation Savings. Recently, the undeniable success of pre-trained Transformers such as\nBERT [24], ViT [27], and GPT-3 [10] has confirmed the benefits of unsupervised pre-training. As\npreviously mentioned, pre-training initializes the networkâ€™s weights in a â€œgoodâ€ region of space\nthat allows the model to converge faster. Therefore, we advise practitioners and researchers to build\nupon pre-trained models like the ones available on the open-source library Hugging Face [132].\nNonetheless, pre-trained models are typically only available for â€œconventionalâ€ data and tasks\nsuch as translation, summarization, question answering, text-to-speech, image classification, and\nobject detection. As for data and tasks without pre-trained models, we recommend initializing the\nmodel with a principled strategy such as Admin or T-Fixup and using a sample-efficient objective.\nA Practical Survey on Faster and Lighter Transformers 37\nThose techniques are not yet implemented in standard libraries, therfore we suggest using T-Fixup\nas it is simpler than Admin.\nB.1.2 Memory Savings. As discussed before, although time may limit oneâ€™s experiments, memory\nbottlenecks are much more critical. Since the intermediates values are responsible for a substantial\npart of the memory footprint, the first method to apply whenever memory is the main limiting\nfactor during optimization is gradient checkpointing. The approach has two significant advantages:\n(i) the trade-off between memory and computation controlled by the number of intermediate values\nkept in memory is highly adjustable, and (ii) the method is straightforward to use in TensorFlow12\nand PyTorch13. Nevertheless, gradient checkpointing has some caveats with multiple GPUs, even\non a single machine. For instance, as of the writing of this survey, gradient checkpointing interferes\nwith PyTorchâ€™s Distributed and Data Parallel API, leading to instabilities14.\nAlternatively to gradient checkpointing, reversible layers provide a mechanism to recompute\nthe intermediate values during the backward pass, thereby decoupling the modelâ€™s depth from the\namount of memory required by the activations. Although the increase in computation is reasonable,\nreversible layers produce numerical errors that may accumulate layers after layers to the point\nthat they become an issue. Additionally, reversible layers are not yet part of standard libraries and\nrequire manually writing the forward and backward operations.\nIn addition to gradient checkpointing or reversible layers, parameter sharing allows further reduc-\ning the memory and is straightforward. However, unlike the other approaches, parameter sharing\nreduces the modelâ€™s capacity. Fortunately, the trade-off between capacity and memory/computation\nsavings is highly customizable, depending on the number of parameters shared.\nFinally, a mixture of experts potentially with micro batching is expected to allow many memory-\nlimited GPUs to train a Transformer even if each GPU is individually too small. However, both\napproaches require substantial effort to implement and impose a communication cost.\nB.2 Inference\nSometimes, researchers have the resources to train large models during the development phase\ndue to public or academic infrastructures, but they do not have the resources to deploy them. In\nsuch cases, one may do a neural architecture search to find the best model within a parameter\nbudget during training, preferably with So et al. [105]â€™s approach. As of this surveyâ€™s writing, neural\narchitecture search is not part of standard libraries.\nAlternatively or additionally to NAS, structured pruning and distillation reduce the amount\nof memory and computations with fine-grained control. While structured pruning is already\nimplemented, distillation is as easy as building a second model that predicts the teacherâ€™s output.\nAs the aforementioned results suggest [54, 79, 98, 99, 120], the Transformerâ€™s performance does\nnot significantly degrade when the model is pruned or distilled. Therefore, to reduce the amount\nof energy consumed by the model, we suggest applying those methods even when resources are\nsufficient during inference.\nB.3 Optimization and Inference\nThe first and foremost method for faster and lighter models is automatic mixed-precision. Mixed-\nprecision is compatible with virtually every neural network, combines with every other approach,\nreduces the memory footprint and accelerates computations on modern GPUs. Additionally, this\n12https://github.com/cybertronai/gradient-checkpointing\n13https://pytorch.org/docs/stable/checkpoint.html\n14https://discuss.pytorch.org/t/ddp-and-gradient-checkpointing/132244/2\n38 Fournier et al.\nmethod is one of the simplest to implement, only requiring a few lines of code in PyTorch15 and\nTensorFlow16.\nAlthough 8-bit quantization may seem similar to 16-bit mixed-precision, the former is primarily\nused to speed up inference and is not as readily available as the latter. In particular, PyTorch does\nnot provide quantized operators for GPU as of the writing of this survey, and Tensorflow warns\nusers that â€œdifferent hardware may have preferences and restrictions that may cause slight deviations\nwhen implementing the spec that result in implementations that are not bit-exact â€17. Due to the\nfinicky nature of 8-bit quantization, we suggest reserving this approach to specific hardware and\nuse-cases such as the mobile setting.\nC PRACTICAL GUIDELINES - SPECIALIZED METHODS\nWith limitations mentioned in Section 5 in mind, let us examine the results of [115] and draw some\nbroad guidelines.\nThe first observation is that every model is lighter than the original Transformer. Nonetheless,\nfor memory-limited environments, the Synthesizer is the least advisable alternative as the model\nonly reduces the memory by 24 to 26% regardless of the sequence length, which is consistent with\nits quadratic complexity. Instead, the Linformer, Performer, and Linear Transformer are better\nsuited to address memory bottlenecks as they are at least 56% and 88% lighter than the original\nTransformer for input sequences of 1,000 and 4,000 tokens, respectively, which is also consistent\nwith their linear complexity.\nThe second observation is that, on TPU V3 chips, the Synthesizer, the Reformer and BigBird\nperform roughly the same number of steps per second as the original Transformer regardless\nof the sequence length. In contrast, the Linformer, Performer, Sinkhorn Transformer and Linear\nTransformer are significantly faster than the original Transformer for input sequences of 4,000\ntokens while performing on par for sequences of 1,000 tokens. Consequently, those models are\nbetter suited for computation-limited environments. We do not wish to overstate our claims here\nsince TPUs and GPUs differ on some key aspects 18, and speed-ups may significantly vary, as\nobserved by Wang et al. [128] and Wang et al. [127]. Although the data processing pipeline and the\nmodel implementation are outside this surveyâ€™s scope, they should be tuned for the exact hardware\nused as it may significantly impact the performance.\nNonetheless, it would seem that the Linformer, Performer, and Linear Transformer are excellent\noptions to improve memory and computation, with the Linformer standing out considering the\nsimplicity of its implementation. However, those models also have serious drawbacks. The Linformer\nrequires instantiating the projection matricesğ‘¬ and ğ‘­ of dimension ğ‘˜Ã—ğ‘›, and thus can only process\nfixed-sized input sequences. Therefore, sequences must be padded to the size of the largest one\nin the dataset, which may significantly degrade the modelâ€™s efficiency. The Performer and Linear\nTransformer are challenging to be efficiently implemented. Besides, they perform noticeably worse\nthan the original Transformer on average. In some cases, such as byte-level text classification, they\nmanage to outperform the original Transformer. In other cases, however, they might critically\nunderperform. For instance, in a longer variant of the ListOps task [81] that consist of modelling\nhierarchically structured data, they achieve less than 50% of the original Transformerâ€™s performance.\nIn contrast, sparse Transformers suffer less performance degradation on average, as measured\non the Long-Range Arena benchmark. Notably, the LongFormer and BigBird achieved the same\n15https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n16https://www.tensorflow.org/guide/mixed_precision\n17https://www.tensorflow.org/lite/performance/quantization_spec#specification_summary\n18Compared to modern GPUs with Tensor Cores, TPUs typically perform more FLOPs but have a lower memory bandwidth,\nhave fewer but larger tiles, and apply the activation function within the matrix multiplication.\nA Practical Survey on Faster and Lighter Transformers 39\naccuracy as the original Transformer for the ListOps task. Sparse models have, however, two major\nshortcomings. First, the sparsity must be structured in order to be efficiently implemented and\nyield practical improvements. Otherwise, the sparse model may be slower than its dense equivalent.\nFurthermore, CUDA kernels require considerable effort to be efficiently implemented and are\nspecific to GPUs. Implementing equivalent kernels on TPUs is challenging, or even impossible,\ndue to the disparity in supported primitives and operations. Secondly, dependencies that must\nbe modelled to solve the task accurately should not be masked. Otherwise, the performance will\nbe critically impacted. To select the appropriate sparse model, we recommend that one train a\nsmall vanilla Transformer with mixed-precision and gradient checkpointing, and then analyze the\nactivation patterns of each layerâ€™s attention.\nNonetheless, in a recent paper, Narang et al. [82] investigated the impact of numerous modi-\nfications to the Transformer architecture, including changes in activation, normalization, depth,\nembeddings, and Softmax, on three NLP benchmarks, namely SuperGLUE [123], XSum [83], and\nWebQ [8]. The authors also evaluated several methods studied in this paper, including parameter\nsharing, Synthesizers, the Switch Transformer, and the Universal Transformer. They observed\nthat no modification was able to improve the performance significantly. After ruling out various\nexplanations, the authors conjectured that â€œmodifications to the Transformer architecture often do\nnot transfer across implementations and applications â€, which may explain why no modification has\nbeen widely adopted.\nIn conclusion, there seem to be no simple and universal guidelines regarding the current Trans-\nformer alternatives. If the data and task are standard, we recommend looking in the literature or on\nthe Papers With Code website for references on how the different methods compare and experiment\nwith already pre-trained models. Otherwise, we recommend using a small vanilla Transformer\nwith mixed-precision and gradient checkpointing as baseline, then experimenting with already\nimplemented lower-complexity models. As a side note, one may also want to combine multiple\nspecialized approaches. For instance, BigBird-ETC relies on additional tokens for global attention,\na form of memory similar to the Compressive Transformer. Nonetheless, many combinations are\nunprincipled at best. For instance, one should not factorize a sparse attention: the complexity will\nbe similar to that of the same factorization of the full attention, and the sparsity may lose valuable\ninformation that the factorization could have preserved.\nD ALTERNATIVES TO SELF-ATTENTION\nRecently, attention-free alternatives to the Transformer have been proposed, putting Vaswani et al.\n[121] original paper title Attention Is All You Need to the test. Such architectures have not been\nexplored in the core of this survey as they arguably remove the Transformerâ€™s core mechanism.\nNonetheless, it is important to mention some of the most popular and promising alternatives.\nTolstikhin et al. [118] argued that self-attention is not required for image classification. They\nintroduced a model called MLP-Mixer solely based on a succession of two multilayer perceptrons\napplied independently to image patches and channels, respectively, which achieved comparable\naccuracy to the ViT [27] on ImageNet.\nLikewise, Liu et al. [70] argued that self-attention is not critical for computer vision and language\nmodelling. They introduced a network called gMLP that models the interactions with Spatial Gating\nUnits (SGU) instead of self-attention. Their model achieved the same accuracy as the ViT [27] on\nImageNet, and the same perplexity of BERT [24] on a subset of C4.\nAlternatively, Bello [4] proposed to replace the Transformerâ€™s self-attention with Lambda layers.\nLong-range content and position-based interactions are captured by transforming the context into\nlinear functions, i.e. matrices, and applying them to each input independently. LambdaNetworks\nachieved comparable results to relatively small Transformers on ImageNet classification. While the\n40 Fournier et al.\nmemory complexity of Lambda layers remains quadratic with respect to the sequence length, it\ndoes not scale with the batch size. Additionally, the author proposed a multi-query variant that\nscales down the complexity by a factor.\nFinally, Yu et al . [137] argued that the architecture of the Transformer is more valuable to\nthe performance than the specific mechanism to relate the tokens. To illustrate their claim, the\nauthors introduced the PoolFormer, a network that performs similarly to vision Transformers\nwhile replacing the self-attention mechanism with pooling, a simple non-parametric operator.\nFurthermore, the authors expanded on this idea with a more general and flexible architecture called\nMetaFormer, where the mechanism to relate the tokens is not specified while the other components\nare kept the same as the Transformer.\nE SUMMARY OF THE SPECIALIZED APPROACHES\nTable 4. Summary of the specialized methods and their associated models.\nCategory Approach Model\nSparse\nFixed and Random Patterns\nStar-Transformer [41]\nSparse Transformer [16]\nCascade Transformer [125]\nLogSparse-Transformer [66]\nBlockBERT [90]\nLongformer [5]\nBigBird [139]\nLearned and Adaptive Patterns\nSinkhorn Transformer [114]\nSparseBERT [104]\nAdaptively Sparse Transformer [20]\nClustering and Locality-Sensitive HashingReformer [59]\nRouting Transformer [96]\nFactorized Attention\nLow-Rank Factorization\nLinformer [126]\nSynthesizers [113]\nNystrÃ¶mformer [136]\nKernel Attention Linear Transformer [56]\nPerformer [18]\nClustering and Locality-Sensitive Hashing Transformer with clustered attention [122]\nArchitectural ChangeMemory Transformer-X Dai et al. [22]\nCompressive Transformer Rae et al. [93]\nSequence Compression Funnel-Transformer Dai et al. [21]",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8449128866195679
    },
    {
      "name": "Transformer",
      "score": 0.756747305393219
    },
    {
      "name": "Computation",
      "score": 0.48885029554367065
    },
    {
      "name": "Machine learning",
      "score": 0.47271010279655457
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4329315721988678
    },
    {
      "name": "Algorithm",
      "score": 0.24785494804382324
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I45683168",
      "name": "Polytechnique MontrÃ©al",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210164802",
      "name": "Mila - Quebec Artificial Intelligence Institute",
      "country": "CA"
    }
  ]
}