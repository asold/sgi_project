{
  "title": "Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers",
  "url": "https://openalex.org/W4385373640",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5012412123",
      "name": "Hadi Abdine",
      "affiliations": [
        "École Polytechnique"
      ]
    },
    {
      "id": "https://openalex.org/A5072113617",
      "name": "Michail Chatzianastasis",
      "affiliations": [
        "École Polytechnique"
      ]
    },
    {
      "id": "https://openalex.org/A5023347965",
      "name": "Costas Bouyioukos",
      "affiliations": [
        "University of Cyprus",
        "Centre National de la Recherche Scientifique",
        "Epigénétique et Destin Cellulaire",
        "Université Paris Cité"
      ]
    },
    {
      "id": "https://openalex.org/A5085652160",
      "name": "Michalis Vazirgiannis",
      "affiliations": [
        "École Polytechnique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4320342754",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4387805937",
    "https://openalex.org/W4233120011",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W1982597966",
    "https://openalex.org/W3188579603",
    "https://openalex.org/W4382239070",
    "https://openalex.org/W3211795435",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W4310273071",
    "https://openalex.org/W2739999456",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4236358448",
    "https://openalex.org/W2584305206",
    "https://openalex.org/W4385572894",
    "https://openalex.org/W2943203634",
    "https://openalex.org/W3092867907",
    "https://openalex.org/W4315928370",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W4288099645",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W2960216239",
    "https://openalex.org/W4281478251",
    "https://openalex.org/W2965857891",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W3164046276",
    "https://openalex.org/W4288090629",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W2947926079",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2950277699",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W3109892317",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288419255",
    "https://openalex.org/W3113698408",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4318751307"
  ],
  "abstract": "In recent years, significant progress has been made in the field of protein function prediction with the development of various machine-learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e. assigning predefined labels to proteins. In this work, we propose a novel approach, Prot2Text, which predicts a protein's function in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including protein sequence, structure, and textual annotation and description. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate functional descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate function prediction of existing as well as first-to-see proteins.",
  "full_text": "Prot2Text: Multimodal Protein’s Function Generation with GNNs and\nTransformers\nHadi Abdine1 , Michail Chatzianastasis1 , Costas Bouyioukos2,3 , Michalis Vazirgiannis1\n1 DaSciM, LIX, ´Ecole Polytechnique, Institut Polytechnique de Paris, France.\n2 Epigenetics and Cell Fate, CNRS UMR7216, Universit´e Paris Cit´e, F-75013 Paris, France.\n3 Bioinformatics Research Laboratory, Department of Biological Sciences, University of Cyprus,\nNicosia, CY 1678, Cyprus\n{hadi.abdine, michail.chatzianastasis }@polytechnique.edu,\ncostas.bouyioukos@u-paris.fr, mvazirg@lix.polytechnique.fr\nAbstract\nIn recent years, significant progress has been made\nin the field of protein function prediction with\nthe development of various machine-learning ap-\nproaches. However, most existing methods for-\nmulate the task as a multi-classification prob-\nlem, i.e. assigning predefined labels to pro-\nteins. In this work, we propose a novel approach,\nProt2Text, which predicts a protein’s function in\na free text style, moving beyond the conventional\nbinary or categorical classifications. By com-\nbining Graph Neural Networks(GNNs) and Large\nLanguage Models(LLMs), in an encoder-decoder\nframework, our model effectively integrates diverse\ndata types including protein sequence, structure,\nand textual annotation and description. This mul-\ntimodal approach allows for a holistic representa-\ntion of proteins’ functions, enabling the generation\nof detailed and accurate functional descriptions. To\nevaluate our model, we extracted a multimodal pro-\ntein dataset from SwissProt, and demonstrate em-\npirically the effectiveness of Prot2Text. These re-\nsults highlight the transformative impact of mul-\ntimodal models, specifically the fusion of GNNs\nand LLMs, empowering researchers with powerful\ntools for more accurate function prediction of ex-\nisting as well as first-to-see proteins.\n1 Introduction\nUnderstanding proteins’ function is a central problem in bio-\nlogical sciences, as proteins are the fundamental elements of\nalmost all biological functions. Accurate prediction of pro-\nteins’ function is essential for understanding biological sys-\ntems as well as for various applications, such as drug dis-\ncovery, enabling researchers to identify and target specific\nproteins that play critical roles in disease pathways Ha et al.\n(2021). Traditionally, proteins’ functions prediction has been\napproached through classification methods, assigning prede-\nfined labels to proteins based on their characteristics Kul-\nmanov and Hoehndorf (2019). However, this approach often\noversimplifies the complexity of proteins’ functionality, lim-\niting the depth of our understanding. To overcome this lim-\nitation, we propose a novel view on proteins’ functions pre-\ndiction based on reformulating the task using free-text pro-\nteins’ descriptions instead of relying on predefined labels.\nThe rapid progress in transformer-based models has brought\na massive revolution to the field of Natural Language Pro-\ncessing (NLP). These models have demonstrated impressive\nlanguage generation capabilities, allowing them to perform a\nwide range of NLP tasks with remarkable performance, in-\ncluding text completion, translation, sentiment analysis and\nquestion-answering Vaswani et al. (2017); Radford et al.\n(2019); Brown et al. (2020). On the other hand, Graph Neu-\nral Networks(GNNs) have emerged as a powerful tool for\nmodeling graph-structured data, capturing the intricate re-\nlationships between different elements in a graph Kipf and\nWelling (2017); Reiser et al. (2022). However, the integra-\ntion of GNNs and transformers faces various challenges, such\nas effectively handling the heterogeneity of data representa-\ntions, therefore the field is still in its early stages. Despite\nthis, the potential benefits of leveraging both GNNs and trans-\nformers for graph-to-text applications, such as predicting the\nfunctional properties of proteins are substantial. To that end,\nwe develop a novel multimodal framework, Prot2Text, that\ncan generate detailed and accurate descriptions of proteins’\nfunctions in free text. We effectively integrate GNNs and\nLarge Language Models (LLMs), to encompass both struc-\ntural and sequential information of the protein’s 3D structure\nand amino acid’s sequence respectively. The encoder-decoder\narchitecture forms the backbone of our model, with the en-\ncoder component employing a Relational Graph Convolution\nNetwork (RGCN) Schlichtkrull et al. (2018) to process the\nproteins’ graphs and the ESM protein language model Lin\net al. (2023a) to encode the proteins’ sequences. The decoder\ncomponent utilizes a pretrained GPT-2 model to generate de-\ntailed proteins’ descriptions. To train our multimodal model,\nwe compile a dataset of proteins extracted from SwissProt,\na comprehensive collection of protein annotations obtained\nfrom the UniProt database Consortium (2015). This dataset\nencompasses a vast number of proteins, each annotated with\nits corresponding function or description. In addition to the\ntextual information, we obtain the 3D structure representation\narXiv:2307.14367v3  [q-bio.QM]  20 Apr 2024\nof the proteins from AlphaFold Varadi et al. (2022). We fur-\nther release this curated dataset to the public, allowing other\nresearchers to use it for benchmarking and further advance-\nments in the field. Code, data and models are publicly avail-\nable1. Our main contributions can be summarized as follows:\n• We introduce the Prot2Text framework, a novel mul-\ntimodal approach for generating proteins’ functions in\nfree text. Our model combines both GNNs and ESM\nto encode the protein in a fused representation while a\npretrained GPT-2 decodes the protein’s text description.\n• We propose various baselines for protein text generation\nand demonstrate that the integration of both graph and\nsequence protein information leads to better generation\ncapabilities.\n• We further release a comprehensive multimodal protein\ndataset, which includes 256, 690 protein structures, se-\nquences, and textual function descriptions. Researchers\ncan leverage this dataset to benchmark and compare\ntheir models, thereby driving advancements in the field\nand enabling for a more robust and standardized evalu-\nation of proteins’ functions prediction methods in free\ntext format.\n2 Related Work\nTransformers. The transformer-based encoder-decoder\nmodel was first introduced by Vaswani et al. (2017) in their\npaper “Attention is all you need”. Since then, this model ar-\nchitecture has become the de-facto standard encoder-decoder\narchitecture in Natural Language Processing (NLP). Despite\ngoing through significant research on different pre-training\nobjectives for transformer-based encoder-decoder models\nsuch as T5 (Raffel et al., 2019) and Bart (Lewis et al.,\n2020), the model architecture has remained largely the same.\nRadford et al. (2018) took advantage of the transformer\narchitecture (Vaswani et al., 2017), which is superior and\nconceptually simpler than Recurrent Neural Networks\nto introduce the OpenAI GPT model. Specifically, they\npre-trained a left-to-right transformer decoder as a general\nlanguage model using the GPT architecture. Following, they\nfine-tuned the model on 12 different language understanding\ntasks by applying various transformations to the input. Later\non, GPT-2 (Radford et al., 2019) was introduced, a more\nadvanced version of GPT having more trainable parame-\nters. The authors showed that as long as general language\nmodels have very high capacities, they can reach reasonable\nperformance on many specific natural language processing\ntasks. The use of the transformer’s architecture is expanded\nlater to include modalities other than natural language such\nas images (Dosovitskiy et al., 2021), protein amino-acid se-\nquences (Rives et al., 2021; Lin et al., 2023a), and molecules\nSMILES string (Fabian et al., 2020; Chithrananda et al.,\n2020). They are all pretrained with the Masked Language\nModeling task (MLM) introduced in BERT (Devlin et al.,\n2019) and are able mostly to perform discriminative tasks.\n1https://github.com/hadi-abdine/Prot2Text\nMultimodal models. The success of the transformer’s ar-\nchitecture uni-modality tasks made this architecture broadly\nstudied for multimodal representation learning. One exam-\nple is The CLIP (Contrastive Language-Image Pre-training)\nmodel (Radford et al., 2021) which is a transformer model\nthat facilitates cross-modal understanding between images\nand text. It combines a ViT vision encoder, with a\ntransformer-based language encoder to learn joint representa-\ntions of images and their associated textual descriptions. By\nleveraging transformers in both the vision and text encoders,\nthe CLIP model benefits from their ability to capture long-\nrange dependencies. Another example is the MolT5 (Ed-\nwards et al., 2022) which is a self-supervised learning frame-\nwork based on the T5 model (Raffel et al., 2019) for pre-\ntraining models on a vast amount of unlabeled natural lan-\nguage text and molecule SMILES strings. MolT5 is able\nto perform bidirectional translation between molecule repre-\nsentations and natural language allowing molecule caption-\ning and generation providing text prompts. ProtST Xu et al.\n(2023), enhances the protein language model classification\nand retrieval capabilities by co-training it with biomedical\ntext. While ProteinDT Liu et al. (2023) uses protein lan-\nguage models and pretrained language models to perform\ntext-guided protein generation. Both of the aforementioned\ntext-protein multimodal frameworks take only the protein se-\nquence into consideration to encode the proteins and do not\ndeal with the protein’s detailed textual description generation\ntask.\nGraph Neural Networks. Graph neural networks (GNNs)\nhave emerged as a powerful framework for modeling and an-\nalyzing graph-structured data (Scarselli et al., 2009; Kipf and\nWelling, 2017). By iteratively exchanging and integrating in-\nformation among nodes, GNNs can propagate and refine fea-\ntures throughout the graph, ultimately encoding a compre-\nhensive understanding of the graph’s structure and semantics.\nThis ability to capture complex relationships within graphs\nhas contributed to the success of GNNs in various domains,\nincluding social network analysis, recommendation systems,\nand bioinformatics (Zitnik et al., 2018; Zhang et al., 2021;\nChatzianastasis et al., 2023b). Numerous studies have sug-\ngested various enhancements and expansions to the GNNs’\nmodels. Some notable contributions include the introduc-\ntion of more expressive and adaptable aggregation functions,\nsuch as those proposed by Murphy et al. (2019), Seo et al.\n(2019) and Chatzianastasis et al. (2023a). Moreover, sev-\neral schemes have been developed to incorporate different lo-\ncal structures or high-order neighborhoods, as explored by\nMorris et al. (2020) and Nikolentzos et al. (2020). Further-\nmore, the domain of GNNs has expanded to encompass het-\nerogeneous graphs, where nodes and edges can have different\ntypes and semantics, leading to the development of Heteroge-\nneous Graph Neural Networks effectively handling such com-\nplex graph structures (Schlichtkrull et al., 2018; Zhang et al.,\n2019).\nProtein Representation Learning. In the field of protein\nrepresentation learning, various approaches have emerged\nover the years, aiming to capture meaningful information\nfrom proteins using different data modalities and computa-\ntional techniques. One prominent avenue of research is fo-\ncused on sequence-based representations, that extract features\nsolely from the amino-acid sequences of proteins. To achieve\nthis, deep learning techniques such as Recurrent Neural Net-\nworks (RNNs) and Convolutional Neural Networks (CNNs)\nhave been applied, enabling the direct learning of represen-\ntations from protein sequences (Liu, 2017; Bileschi et al.,\n2019). Drawing inspiration from the remarkable achieve-\nments of language models in Natural Language Processing\n(NLP), researchers have also developed pre-trained language\nmodels tailored specifically for proteins (Brandes et al., 2022;\nLin et al., 2023b). These models leverage large-scale pro-\ntein datasets to learn powerful representations that can subse-\nquently be utilized for various prediction tasks. In addition\nto sequence-based approaches, graph-based representations\nleverage the three-dimensional (3D) structure of proteins to\ncapture their functional properties. Zhang et al. (2022) pro-\nposed a graph neural network model with a contrastive per-\ntaining strategy for function prediction and fold classification\ntasks. Chen et al. (2023) proposed a 3D-equivariant graph\nneural network, specifically designed to estimate the accuracy\nof protein structural models. Wang et al. (2022) used a hier-\narchical graph network, which captures the hierarchical rela-\ntions present in proteins and learns representations at differ-\nent levels of granularity. Hybrid approaches integrate multi-\nple data modalities, such as protein sequences, structures, and\nfunctional annotations, to create comprehensive representa-\ntions. These methods combine the strengths of sequence-\nbased and graph-based approaches to capture diverse aspects\nof protein function. Gligorijevi´c et al. (2021) proposed Deep-\nFRI which combines sequence features extracted from a pre-\ntrained protein language model with protein structures. Our\nwork aims to leverage protein sequence and structure models\nto generate free text annotations of proteins.\n3 Methodology\nIn this section, we present our proposed multimodal frame-\nwork, Prot2Text, for generating protein function descriptions\nin free text.\nGraph Construction. Upon obtaining the 3D proteins’\nstructures using AlphaFold, we proceed to represent the pro-\nteins as a heterogeneous graph G = (V, E, R), where V =\n[N] :={1, ..., N} is the set of vertices representing the amino\nacids of the proteins, E ⊆ V × V is the set of edges repre-\nsenting various interactions between the nodes and R is a set\nof different edge interactions. Each node u is associated with\na feature vectorxu ∈ Rd, encompassing relevant information\nsuch as local structural features, and physicochemical proper-\nties of the associated amino acids. This enables the graph to\nretain fine-grained information critical to the protein’s struc-\nture and function.\nTo model the diverse interactions and relationships be-\ntween amino acids, we introduce different types of edges con-\nnecting the nodes. Therefore, each edge i = (v, u) is as-\nsociated with an edge type ei ∈ R. Sequential edges are\nemployed to connect adjacent nodes in the protein sequence,\neffectively representing the sequential order of amino acids\nand capturing the linear arrangement of the protein’s primary\nstructure. This sequential information is crucial for under-\nstanding the folding patterns and functional motifs within the\nprotein. Additionally, we utilize spatial edges to establish\nconnections between nodes that are in close spatial proxim-\nity within the 3D structure of the protein. These edges play\na pivotal role in encoding the protein’s tertiary structure and\nfolding patterns, enabling us to capture the intricate spatial ar-\nrangements of amino acids within the protein’s core. We fur-\nther extend the graph construction to include hydrogen bond\ninteractions as an additional edge type. Hydrogen bonds are\nfundamental non-covalent interactions that are of paramount\nimportance in stabilizing protein structures and enabling spe-\ncific molecular recognition events. Through the integration\nof the different edge types, our comprehensive protein graph\nprovides a more holistic and detailed depiction of the pro-\ntein’s structure while capturing both short and long-range in-\nteractions.\nGraph Encoding. To encode the protein graph G into a\nvector hG ∈ Rdout , we employ a Relational Graph Convo-\nlutional Neural Network(RGCN) Schlichtkrull et al. (2018),\nwhich effectively considers the various edge types present in\nthe graph in the message-passing mechanism. We denote the\nneighborhood of type r of a vertex u by Nr(u) such that\nNr(u) = {v : (v, u) ∈ Er}, where Er is the set of edges\nwith r edge type. In layer k of the GNN, we update the node\nrepresentations as follows:\nxk\ni = σ\n\nWk\nroot · xk−1\ni +\nX\nr∈R\nX\nj∈Nr(i)\n1\n|Nr(i)|Wk\nr · xk−1\nj\n\n,\n(1)\nwhere Wk\nroot represents the learnable weight matrix for the\nroot transformation in layer k, Wk\nr denotes the learnable\nweight matrix of layer k for relation r and σ(·) is an element-\nwise activation function such as ReLU. This formulation al-\nlows nodes to update their representations by incorporating\ninformation from neighboring nodes based on the specific\nedge types, capturing the structural and relational dependen-\ncies within the protein graph. To obtain the graph representa-\ntion from the node representations of the last layer K of the\nGNN, we apply a mean-pooling layer as follows:\nhG = 1\nN\nNX\ni=1\nxK\ni (2)\nThe resulting vector hG serves as an informative encoded\nrepresentation of the protein graph, capturing the essential\nstructural and relational characteristics. This representation\nplays a crucial role in the subsequent text generation process,\nwhere it will be utilized to generate detailed and accurate pro-\ntein functions.\nSequence Encoding. To encode the protein sequence PS,\nwe used ESM2-35M Lin et al. (2023a) as our base model.\nESM2 is a protein language model that uses a transformer-\nbased architecture and an attention mechanism to learn the\ninteraction patterns between pairs of amino acids in the input\nsequence. This allows the ESM model to capture amino acid\nsequence evolutionary information about proteins and their\nModified GPT-2 Model\nT0 T1 T2 T3 TNA0 A1 AM\nMulti-Head Cross AttentionMulti-Head Cross AttentionFusion Block\nMLP\nESM2\nLanguage Modeling Head:\nMLP+Softmax\nProtein ENCODER\nText DECODER\nS S E S S\nR\nE\nENV\nESM Tokenizer\nProtein Sequence\nAlphaFold\nConvert to graph\nIs associated with a DNA binding complex that binds\n to the G box, a well-characterized cis-acting DNA\n regulatory element found in plant genes\nText Tokenizer + Right Shifting\nProtein Description\nRGCN Encoder\nMulti-Head Cross Attention + MLP  \nIs associated with a DNA\n binding complex that binds\n to the G box, a well\ncharacterized cis-acting\nDNA regulatory  element\n found in plant genes\nProtein Description Generation\n(CLM Training Objective)\nFigure 1: Architecture of the proposed Prot2Text framework for predicting protein function descriptions in free text.\nThe model leverages a multimodal approach that integrates protein sequence, structure, and textual annotations. The Encoder-\nDecoder framework forms the backbone of the model, with the encoder component utilizing a relational graph convolution\nnetwork (RGCN) to process the protein graphs, and an ESM model to process the protein sequence. A cross-attention mech-\nanism facilitates the exchange of relevant information between the graph-encoded and the sequence-encoded vectors, creating\na fused representation synthesizing the structural and textual aspects. The decoder component employs a pre-trained GPT-2\nmodel, to generate detailed and accurate protein descriptions from the fused protein representation. By combining the power\nof Graph Neural Networks and Large Language Models, Prot2Text enables a holistic representation of protein function, facili-\ntating the generation of comprehensive protein descriptions.\nproperties. In order to achieve uniform representation dimen-\nsions for all modalities within the spatial domain, a projec-\ntion layer is applied after the last hidden layer of the ESM\nmodel. This layer functions as a projection layer that trans-\nforms the individual amino acid representations, derived from\nthe ESM embedding dimension, into the graph embedding di-\nmension dout. As a result, a matrix denoted asH0\nS ∈ RN,dout\nis formed, containing the amino acid representations:\nH0\nS = ESM (PS)Wp (3)\nwhere Wp is a trainable matrix.\nMultimodal Fusion. To obtain the final protein encoding,\nwe utilize a fusion block that combines the representation of\neach amino acid inside the matrix H0\nS with the graph rep-\nresentation vector hG. The fusion process involves a simple\nelement-wise addition of the two representations, followed by\na projection layer. This fusion block enables the integration\nof information from both the sequence and the graph repre-\nsentations in a straightforward manner. Thus, allowing each\namino acid to be contextually enriched with information from\nthe graph representation. Additionally, a normalization layer\nis applied after each fusion block to maintain stable training\nand further enhance the learning process. Specifically, for\neach amino acid representation in Hk\nS, and the graph repre-\nsentation hG, the fusion block computes the combined repre-\nsentation Hk+1\nS as follows:\nHk+1\nS =\n\u0000\nHk\nS + 1nhGWk\nV\n\u0001\nWk\nO, (4)\nwhere Wk\nV and Wk\nO are trainable matrices in fusion block k\nand 1n is a vector of ones of size n (length of the amino acid\nsequence).\nBy using this fusion block multiple times in the architecture\n(four times in this case), the model can capture complex in-\nteractions and dependencies between the sequence and graph\nrepresentations, leading to an effective and contextually en-\nriched encoding of the protein data. The fusion block could\nbe seen as a special case of the transformers cross-attention\nblock when the the input from the encoder represents only\none token.\nText Generation We employed the transformer decoder ar-\nchitecture for generating protein descriptions. We initialized\nthe main components of the decoder, namely the text embed-\nding matrix, self-attention, and language modeling head, with\nthe pretrained weights of GPT-2. By doing so, we leveraged\nthe GPT-2 model’s capacity to grasp the underlying textual\nsemantics. We forward the protein representation obtained\nfrom the protein encoder as input to the multi-head cross-\nattention module within the transformer decoder. This in-\nteraction enabled the model to effectively incorporate con-\ntext from the protein representation, contributing to the gen-\neration of coherent and meaningful protein descriptions. We\nadopted the identical vocabulary and tokenizer from the GPT-\n2 model, with the introduction of two unique special tokens.\nThese additional tokens serve as essential markers, enabling\nthe model to discern the precise boundaries of each protein\ndescription within the input text. In the training phase, we\nemployed Causal Language Modeling (CLM) as the training\nobjective to optimize our model. Causal Language Model-\ning involves training the model to predict the next token in\na sequence given the preceding tokens. This unidirectional\nprediction process ensures that the model generates text in a\ncausal manner, without access to future tokens. The maxi-\nmum length of each description is 256 tokens.\n4 Experimental Results\nDataset. To train the Prot2Text framework using proteins’\nstructures, sequences and textual descriptions, we build a\nmultimodal dataset with 256, 690 proteins. For each pro-\ntein, we have three crucial information: the correspond-\ning sequence, the AlphaFold accession ID and the textual\ndescription. To build this dataset, we used the SwissProt\ndatabase Bairoch and Apweiler (1996), the only curated\nproteins knowledge base with full proteins’ textual descrip-\ntion included in the UniProtKB Consortium (2016) Release\n2022 04. Initially, The SwissProt database in this release has\n568, 363 proteins on which we perform the following: (1) Se-\nlect the following properties:name that gives the full name of\nthe protein, sequence that gives the amino acid sequence of\nthe protein, AlphaFoldDB that gives the accession ID of the\nprotein in AlphaFold database, taxon and text that gives\nthe protein textual description. (2) Eliminate all samples that\ndo not have all three crucial information. (3) Remove all sam-\nples with a duplicate amino acid sequence. (4) Remove all\nthe samples where the textual description contains ”(By Sim-\nilarity)”. (5) Apply the CD-HIT clustering algorithm Li and\nGodzik (2006) to create a train/validation/test scheme with\n248, 315, 4, 172 and 4, 203 proteins respectively. The maxi-\nmum similarity threshold between the (train, validation test)\nsets used in the CD-HIT algorithm is 40%. (6) Preprocess\nthe textual description to remove the”PubMed” information.\nThe AlphaFoldDB accession is then used to download the\nprotein structure in a ”.PDB” file format using version 4 from\nAlphaFoldDB.\nBaselines. In our experimental evaluation, we employed\na comprehensive set of baselines to rigorously assess the\ntext generation performance of the Prot2Text framework.\nSpecifically, we compared our approach against unimodal\nencoders, namely RGCN, ESM, and a vanilla-Transformer\ntrained from scratch. These encoders exclusively focus on\neither the protein graph or the protein sequence represen-\n(0,20) (20,30) (30,40) (40,50) (50,60) (60,70) (70,80) (80,90)(90,100)\nBLAST Identity (%)\n0\n200\n400\n600\n800\n1000Count\n10\n20\n30\n40\n50\n60\nBLEU score\nProt2TextSMALL\nProt2TextBASE\nProt2TextMEDIUM Prot2TextLARGE\nFigure 2: The test BLEU score for Prot2Text models as a\nfunction of the percentage identity using BLAST hit between\nthe test and the train sets.\ntation. Furthermore, we compared it with a multimodal\nbaseline, RGCN+ESM, that concatenates the graph and se-\nquence representations without fusing the representation of\neach amino acid and the structure representation. Finally,\nwe compare with RGCN × vanilla-Transformer baseline,\nwhich has similar architecture as Prot2Text but instead uses\na vanilla-Transformer model from scratch instead of the pre-\ntrained ESM2. In all ESM models, we use the last hidden\nstate. The vanilla-Transformer baseline follows the same con-\nfiguration and number of parameters as the pretrained ESM2-\n35M.\nTraining Details. We implemented all the models using\nPyTorch and utilized 64 NVIDIA V100 GPUs for training.\nWe used the AdamW optimizer Loshchilov and Hutter (2019)\nwith ϵ = 10−6, β1 = 0.9, β2 = 0.999, with a learning rate\nstarting from 2.10−4 and decreasing to zero using a cosine\nscheduler. We used a warm-up of 6% of the total training\nsteps. We fixed the batch size to four per GPU and we trained\nthe models for 25 epochs. For the GNN encoder, we used 6\nlayers with a hidden size equal to GPT-2’s hidden size (768\nfor the base model of GPT-2) in each layer. As for the amino\nacid sequence tokenization, We used the same tokenizer and\nconfiguration of ESM2 including the hidden layer and hid-\nden dimension. The training for each Base model lasted for\napproximately 12 hours. All experiments were carried out\nusing the Hugging Face transformers library Wolf et al.\n(2020).\nMetrics. In the experiments, we used several metrics to\nevaluate the performance of the model in the text genera-\ntion task. Specifically, we used BLEU Score Papineni et al.\n(2002) which is a widely used metric for evaluating the qual-\nity of machine-generated text. It measures the similarity be-\ntween the generated text and the reference text based on n-\ngrams. A higher BLEU score indicates better similarity be-\ntween the generated and reference text. We further used\nRouge-1, Rouge-2 and Rouge-L scores Lin (2004), which\nmeasure the overlap of unigrams, bigrams, and longest com-\nmon subsequence between the generated text and the refer-\nence text, respectively. Finally, we used BERT Score Zhang\net al. (2020), which measures the similarity between the gen-\nerated text and the reference text using contextualized word\nembeddings from a transformer-based model. In our experi-\nments we choose to use BioBERTLARGE-cased v1.1 Lee et al.\n(2020) to compute the BERT Score.\nResults. We report the results in Table 1, for different\nencoder models, including unimodal encoders like vanilla-\nTransformer, ESM2-35M, and RGCN, and multimodal en-\ncoders like RGCN × vanilla-Transformer and RGCN +\nESM2-35. All models use a GPT-2 decoder. The unimodal\nvanilla-Transformer baseline, relying solely on the amino\nacid sequence of the protein, exhibits the lowest performance\nacross all evaluation metrics. However, we observe a signif-\nicant improvement in performance when using the unimodal\ngraph encoder RGCN. The RGCN outperforms the vanilla-\nTransformer by over five absolute points in terms of BLEU\nscore and three points in terms of BERT score. This per-\nformance disparity highlights the importance of incorporat-\ning structural information through the RGCN encoder for\nprotein’s function prediction. On the other hand, leverag-\ning the pretrained protein language model ESM2-35M in-\nstead of initializing the vanilla-Transformer randomly, results\nin a remarkable improvement in performance. The ESM2-\n35M encoder leads to a substantial increase of over 16 BLEU\nscore points and 18 Rouge-L points compared to the standard\nvanilla-Transformer configuration. This notable enhance-\nment can be attributed to the pretraining of ESM2-35M us-\ning masked protein modeling, which enables the encoder to\ncapture intricate relationships and patterns within protein se-\nquences. In the context of multimodal protein representa-\ntion, the evaluation results demonstrate that Prot2Text BASE\nexhibits superior performance across all assessment metrics.\nNotably, it achieves the highest BLEU Score of 35.11, the\nhighest Rouge-1 score of 50.59, the highest Rouge-2 score of\n42.71, the highest Rouge-L score of 48.49, and the highest\nBERT Score of 84.3. These outcomes highlight the effective-\nness of fusing protein structure and amino acid information\nin a multimodal manner. The incorporation of protein struc-\nture, facilitated by the Relational Graph Convolutional Net-\nwork (RGCN) with the sequential representations of amino\nacids from ESM2-35, significantly enhances the overall per-\nformance across all evaluation metrics. This improvement is\nattributed to the enriched understanding of proteins achieved\nthrough the synergy of these two modalities. Furthermore, the\nefficacy of the multimodal fusion approach is corroborated\nby the results obtained from RGCN × vanilla-Transformer.\nIntroducing structural information using RGCN to the ran-\ndomly initialized vanilla-Transformer yields a substantial im-\nprovement of over 10 BLEU score points compared to using\nthe vanilla-Transformer alone, and more than 6 BLEU score\npoints improvement over using RGCN in isolation. Finally,\nto show the importance of the fusion block in the Prot2Text\nframework, we compare it against RGCN+ ESM2-25, which\nconcatenates the protein structure representation to the amino\nacids representations. In this case, the graph representation\nwill simply be passed to the decoder alongside the ESM out-\nput. We notice that using this strategy leads to slightly worse\nresults than using the ESM alone. This not only provides\nbacking for the selection of the fusion block employed in\nProt2Text, but also suggests that indiscriminately increasing\nthe overall parameter count of the model could potentially\nlead to a degradation in its performance.\nScaling to Larger Models.We conducted an ablation study\nto assess the performance of our Prot2Text framework as we\nvaried the number of parameters. The primary objective of\nthis experiment was to evaluate the benefits of employing\nlarger models in terms of generating more accurate and de-\ntailed textual representations of protein’s function. To con-\nduct the ablation study, we systematically varied the size of\nthe protein language model (ESM). Where Prot2TextSMALL ,\nProt2TextBASE , Prot2Text MEDIUM and Prot2TextLARGE\nuse ESM2-8M, ESM2-35M, ESM2-150M and ESM2-650M\nrespectively. We evaluated each configuration on the same\ntest set of proteins and used the same evaluation metrics as\ndescribed earlier. The results of the ablation study, presented\nin Table 2, show a trend of performance improvement as we\nscale up the model’s architecture. Larger versions of ESM\noutperformed their smaller counterparts in most evaluation\nmetrics. The increase in model size led to more accurate\nand relevant descriptions, indicating the benefit of leverag-\ning larger language models in the Prot2Text framework. Yet,\ncomplementary analysis including corresponding computa-\ntion time showed an increase in the inference cost follow-\ning the use of larger models (higher number of parameters).\nTherefore, Prot2TextMEDIUM (398M parameters) is a good\ntrade-off striking the balance between performance and time\ncost. Furthermore, in Figure 2 we report the performance\nof all Prot2text models with respect to different similarity\nthresholds. Where the similarity represents the highest align-\nment score between the amino acid sequences of the test and\ntrain sets using BLAST identity. We observe that for test pro-\nteins with low similarity scores with the train set (between\n20% and 30%) and for proteins with no counterpart in the\ntrain set, the Prot2Text MEDIUM is the dominant one while\nfor higher similarity scores Prot2Text LARGE performs bet-\nter.\nVisualization of Generated Descriptions. To gain deeper\ninsights into the quality of the generated proteins’ functions\nby our Prot2Text framework, we provide in Figure 3 a tex-\ntual comparison of the pre-defined labels and generated text\noutputs for a selected set of proteins from the test set. It illus-\ntrates a comparison between the ground truth and the corre-\nsponding descriptions generated byProt2Textfor three differ-\nent proteins (P36108, Q8NG08 and P35713) along with each\nprotein’s name, amino acid sequence and 3D structural rep-\nresentation. The results indicate a successful detailed recon-\nstruction of the different proteins’ functions including richer\ninformation than the known description. Following, Figure 3\nshowcases the model’s ability to generate coherent and in-\nformative free-text descriptions that align closely with the\nground truth annotations.\nModel # Params BLEU Score Rouge-1 Rouge-2 Rouge-L BERT Score\nvanilla-Transformer 225M 15.75 27.80 19.44 26.07 75.58\nESM2-35M 225M 32.11 47.46 39.18 45.31 83.21\nRGCN 220M 21.63 36.20 28.01 34.40 78.91\nRGCN + ESM2-35M 255M 30.39 45.75 37.38 43.63 82.51\nRGCN × vanilla-Transformer 283M 27.97 42.43 34.91 40.72 81.12\nProt2TextBASE 283M 35.11 50.59 42.71 48.49 84.30\nTable 1: Test set results for different encoder models, including unimodal encoders such as vanilla-Transformer, ESM2-35M,\nand RGCN, as well as multimodal encoders such as RGCN ×vanilla-Transformer and RGCN+ESM2-35M. All models share\nthe same GPT-2 decoder. Prot2TextBASE achieves the highest performance across all evaluation metrics, including BLEU score,\nRouge scores, and BERT Score.\nModel # Params BLEU Score Rouge-1 Rouge-2 Rouge-L BERT Score Inference Time\nProt2TextSMALL 256M 30.01 45.78 38.08 43.97 82.60 1,225\nProt2TextBASE 283M 35.11 50.59 42.71 48.49 84.30 1,379\nProt2TextMEDIUM 398M 36.51 52.13 44.17 50.04 84.83 1,334\nProt2TextLARGE 898M 36.29 53.68 45.60 51.40 85.20 1,667\nTable 2: Test set results for different size variations of Prot2Text. Larger models outperform their smaller counterparts\nacross most evaluation metrics, indicating the benefits of employing larger language models in the Prot2Text framework. The\nProt2TextMEDIUM model, strikes an optimal balance between performance and computational efficiency. This configuration\ndemonstrates improved performance compared to the smaller model while still maintaining reasonable computational costs.\nThe inference time is in seconds for text generation of each model on the whole test set. The inference time here is computed\nduring text generation using two NVIDIA RTX 6000 with 48GB memory in parallel and batch size of four per device.\n5 Conclusion\nIn conclusion, our paper introduces Prot2Text, a pioneering\nmultimodal framework, for the accurate prediction of a pro-\ntein’s function in free text format, from graph and sequen-\ntial input. By reformulating the task as free-text prediction,\nwe address the limitations of traditional classification-based\nmethods, allowing for a more nuanced and in-depth under-\nstanding of a protein’s functionality. Leveraging the power of\nGNNs and LLMs, we integrate structural and textual protein\ninformation, resulting in highly detailed and coherent gen-\nerated protein descriptions. The release of a comprehensive\nmultimodal protein dataset further empowers the scientific\ncommunity to benchmark and advance the field of protein\nfunction prediction in free text format. This innovative ap-\nproach opens new horizons for research and applications in\ndrug discovery, protein engineering, and various biological\nsciences, with the potential to revolutionize our understand-\ning of proteins’ functions. This work is accepted at AAAI\n20242.\n6 Limitations and Future Work\nOne limitation of our proposed Prot2Text model is that the\nRGCN encoder is not pretrained. Unlike the ESM encoder,\nwhich benefits from pretraining on a large corpus, the RGCN\nencoder lacks this initial knowledge. As a result, the RGCN\nencoder might struggle to capture complex patterns and may\nnot fully leverage the underlying protein data, potentially\nleading to suboptimal performance. To address this limita-\ntion, we aim to explore pretraining techniques specifically\n2https://ojs.aaai.org/index.php/AAAI/article/view/28948\ntailored for graph neural networks. This could involve pre-\ntraining the RGCN encoder on auxiliary graph-related tasks,\nleveraging graph-level or node-level information to build a\nfoundational understanding of protein structures.\nA Training Details\nA.1 Tokenization\n0 100 200 300 400\nNumber of Tokens\n0\n10000\n20000\n30000\n40000\n50000\n60000\n70000Number of Proteins\nFigure 4: Analyzing Protein Description Lengths: Distribu-\ntion of Tokens per Sample with Threshold Highlight at 256\ntokens (in red).\nProteins Textual Description The training dataset consists\nof 256,690 proteins with unique amino-acid sequence. How-\nGenerated TextLabeled TextProtein \nProbable peripherally associated component of the endosomal sorting required for\ntransport complex III (ESCRT-III) which is involved in multivesicular bodies (MVBs)\nformation and sorting of endosome-derived cargo proteins into MVBs. MVBs contain\nintraluminal vesicles (ILVs) that are generated by invagination and scission from the\nlimiting membrane of the cell and mostly are delivered to lysosomes enabling\ndegradation of membrane proteins, such as stimulated growth factor receptors,\nlysogenic enzymes and lipids. The MVB pathway appears to require the sequential\nfunction of ESCRT-O, -I,-II and -III complexes. ESCRT proteins mostly dissociate\nfrom the invaginating membrane before the ILV is released. The ESCRT machinery\nalso functions in topologically equivalent membrane fission events, such the terminal\nstages of cytokinesis and the budding of enveloped viruses (HIV-1 and other\nlentiviruses). ESCRT I, -II and III proteins mostly mediate the sorting of proteins into\nlysosphingolipids, primarily cytoplasmic membrane vesicle-derived proteins. ESCR-III\nproteins are believed to mediate a specialized role in the final stages\nRequired for the sorting and concentration of proteins resulting\nin the entry of these proteins into the invaginating vesicles of\nthe multivesicular body (MVB). Acts a component of the\nESCRT-III complex, which appears to be critical for late steps\nin MVB sorting, such as membrane invagination and final\ncargo sorting and recruitment of late-acting components of the\nsorting machinery. The MVB pathway requires the sequential\nfunction of ESCRT-O, -I,-II and -III complex assemblies. Can\ndirectly stimulate VPS4 ATPase activity. The DID4/VPS2-\nVPS24 subcomplex is required for the VPS4-dependent\ndissociation of ESCRT-III.\n P36108\nQ8NG08\nDNA-dependent ATPase and 5'-3' DNA helicase required for the maintenance of\ngenome stability. Involved in various processes such as transcription-coupled\nnucleotide excision repair, mitotic spindle assembly, DNA damage response and DNA\nrepair. During DNA double-strand break (DSB) repair, involved in limiting the number\nof single-stranded DSBs through DNA end resection by promoting the recruitment of\nRAD51 to sites of DNA damage. In addition to DNA end reannealing, also promotes\nthe resumption of DNA synthesis from the new 3'-tail end of the existing DNA ends.\nActs as a regulator of telomerase by inhibiting telomeric silencing: binds to\ntelomeromyosin, and is required for telomeronucleolytic silencing of endogenous\nsmall interfering RNAs (endo-siRNAs) and their targeting to the telomere. Involves\nalso DNA damage signaling via the 5'-5' helicase and DNA helicases activities,\npreventing disintegration of the DNA duplex and subsequent resection of the\ndamaged DNA strand. In vitro able to unwind 5'-overhanging flap DNA and catalyzes\nATP-dependent unwinding of 5'-DNA ends. Also plays a role in\n5'-3' DNA helicase involved in DNA damage response by\nacting as an inhibitor of DNA end resection. Recruitment to\nsingle-stranded DNA (ssDNA) following DNA damage leads to\ninhibit the nucleases catalyzing resection, such as EXO1,\nBLM and DNA2, possibly via the 5'-3' ssDNA translocase\nactivity of HELB. As cells approach S phase, DNA end\nresection is promoted by the nuclear export of HELB following\nphosphorylation. Acts independently of TP53BP1. Unwinds\nduplex DNA with 5'-3' polarity. Has single-strand DNA-\ndependent ATPase and DNA helicase activities. Prefers ATP\nand dATP as substrates. During S phase, may facilitate\ncellular recovery from replication stress.\nP35713\nTranscriptional activator that binds to the consensus sequence 5'-AACAAAG-\n3'. Plays an essential role in cellular differentiation, proliferation and survival.\nPlays a critical role in macrophage differentiation, migration and invasion,\nparticularly in the gut. Required for normal gene expression in the\nmacrophages, which are activated by biglycan-producing bacteria and fungi.\nAlso required for normal chemotaxis. Plays important roles in the\ndevelopment of the central nervous system, where it is required for proper\nproliferation and migration of progenitor cells.\nTranscriptional activator that binds to the consensus\nsequence 5'-AACAAAG-3' in the promoter of target genes and\nplays an essential role in embryonic cardiovascular\ndevelopment and lymphangiogenesis. Activates transcription\nof PROX1 and other genes coding for lymphatic endothelial\nmarkers. Plays an essential role in triggering the differentiation\nof lymph vessels, but is not required for the maintenance of\ndifferentiated lymphatic endothelial cells. Plays an important\nrole in postnatal angiogenesis, where it is functionally\nredundant with SOX17. Interaction with MEF2C enhances\ntranscriptional activation. Besides, required for normal hair\ndevelopment.\nFigure 3: Ground-truth labels vs text-free Generated functions: A textual comparison of the pre-defined labels and generated\ntext outputs for 3 different proteins from the test set. The used text generation configuration if these examples are the following:\nlength penalty = 2.0, no repeat ngram size=3 and early stopping=True.\never, some proteins have the same description. In total, the\ntraining dataset has 48,251 unique function descriptions. The\naverage number of tokens per description is 57.51. We chose\nto truncate all the descriptions during the tokenization to a\nmaximum length of 256 since this number of tokens covers\n98.7% of all the descriptions as we can see in Figure 4.\nTokenizer The Prot2Text tokenizer is an instance of the\nGPT-2 tokenizer with two additional tokens. In GPT-2 model,\nthe pad token, the start of sequence token and the end of se-\nquence token share the same index. As the Prot2Text archi-\ntecture is an encoder-decoder architecture, we chose to sepa-\nrate the three tokens by adding two extra tokens representing\nthe start of sequence and the end of sequence. For both added\ntokens, we equally need to add the corresponding embedding\nto the GPT-2 word embedding matrix while keeping the rest\nof the matrix intact.\nA.2 Text Generation\nTo generate the protein textual description dur-\ning and after the training, we used the genera-\ntion function implemented in the transformers li-\nbrary. We used the default generation parameters of\nlength penalty=1.0, no repeat ngram size=0\nand early stopping=False. The text generation was\ndone during the training on the validation set each 500 train-\ning steps using greedy search (number of beams equal to one)\nwith maximum length of 256 tokens per sample. However,\ndifferent configuration could be used leading to multiple\nfunctions. For example, the generated text in figure 3 uses\nlength penalty=2.0, no repeat ngram size=3\nand early stopping=True using Prot2TextBASE . Figure\n5 shows the BLEU score validation throughout the training\nfor the Prot2TextBASE model. The validation BLEU score\nstart to stabalize after the 20th epochs reaching the best\nvalidation BLEU score of 37.09 at the step 23000.\nA.3 CO2 Emission Related to Experiments\nExperiments were conducted using a private infrastructure,\nwhich has a carbon efficiency of 0.057 kgCO 2eq/kWh. A\ncumulative of 23000 hours of computation was performed on\nhardware of type Tesla V100-SXM2-32GB (TDP of 300W).\nTotal emissions are estimated to be 393.3 kgCO2eq of which\n0 percents were directly offset. Estimations were conducted\nusing the MachineLearning Impact calculator 3 presented in\nLacoste et al. (2019).\n3https://mlco2.github.io/impact#compute\n0 5000 10000 15000 20000 25000\nTraining Steps\n0\n5\n10\n15\n20\n25\n30\n35Validation BLEU Score\nFigure 5: Tracking Prot2TextBASE BLEU Score Progression\non Validation Set Across Training Iterations. Higher is better.\nAcknowledgements\nThis research was supported by the ANR chair AML/HELAS\n(ANR-CHIA-0020-01).\nReferences\nBairoch, A. and Apweiler, R. (1996). The SWISS-PROT\nProtein Sequence Data Bank and Its New Supplement\nTREMBL. Nucleic Acids Research, 24(1):21–25.\nBileschi, M. L., Belanger, D., Bryant, D., Sanderson, T.,\nCarter, B., Sculley, D., DePristo, M. A., and Colwell, L. J.\n(2019). Using deep learning to annotate the protein uni-\nverse. BioRxiv, page 626507.\nBrandes, N., Ofer, D., Peleg, Y ., Rappoport, N., and\nLinial, M. (2022). Proteinbert: a universal deep-learning\nmodel of protein sequence and function. Bioinformatics,\n38(8):2102–2110.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. (2020). Language models are few-shot\nlearners. Advances in neural information processing sys-\ntems, 33:1877–1901.\nChatzianastasis, M., Lutzeyer, J., Dasoulas, G., and Vazir-\ngiannis, M. (2023a). Graph ordering attention networks.\nIn Proceedings of the 37th AAAI Conference on Artificial\nIntelligence, pages 7006–7014.\nChatzianastasis, M., Vazirgiannis, M., and Zhang, Z. (2023b).\nExplainable multilayer graph neural network for cancer\ngene prediction. arXiv preprint arXiv:2301.08831.\nChen, C., Chen, X., Morehead, A., Wu, T., and Cheng, J.\n(2023). 3d-equivariant graph neural networks for protein\nmodel quality assessment. Bioinformatics, 39(1):btad030.\nChithrananda, S., Grand, G., and Ramsundar, B. (2020).\nChemberta: large-scale self-supervised pretraining\nfor molecular property prediction. arXiv preprint\narXiv:2010.09885.\nConsortium, T. U. (2016). UniProt: the universal protein\nknowledgebase. Nucleic Acids Research , 45(D1):D158–\nD169.\nConsortium, U. (2015). Uniprot: a hub for protein informa-\ntion. Nucleic acids research, 43(D1):D204–D212.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages 4171–\n4186, Minneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.\n(2021). An image is worth 16x16 words: Transformers for\nimage recognition at scale. In International Conference on\nLearning Representations.\nEdwards, C., Lai, T., Ros, K., Honke, G., Cho, K., and Ji,\nH. (2022). Translation between molecules and natural lan-\nguage. In Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages 292–\n305.\nFabian, B., Edlich, T., Gaspar, H., Segler, M., Meyers, J., Fis-\ncato, M., and Ahmed, M. (2020). Molecular representation\nlearning with language models and domain-relevant auxil-\niary tasks.\nGligorijevi´c, V ., Renfrew, P. D., Kosciolek, T., Leman, J. K.,\nBerenberg, D., Vatanen, T., Chandler, C., Taylor, B. C.,\nFisk, I. M., Vlamakis, H., et al. (2021). Structure-based\nprotein function prediction using graph convolutional net-\nworks. Nature communications, 12(1):3168.\nHa, J., Park, H., Park, J., and Park, S. B. (2021). Recent\nadvances in identifying protein targets in drug discovery.\nCell Chemical Biology, 28(3):394–423.\nKipf, T. N. and Welling, M. (2017). Semi-Supervised Classi-\nfication with Graph Convolutional Networks. In 5th Inter-\nnational Conference on Learning Representations.\nKulmanov, M. and Hoehndorf, R. (2019). DeepGOPlus: im-\nproved protein function prediction from sequence. Bioin-\nformatics, 36(2):422–429.\nLacoste, A., Luccioni, A., Schmidt, V ., and Dandres, T.\n(2019). Quantifying the carbon emissions of machine\nlearning. arXiv preprint arXiv:1910.09700.\nLee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H.,\nand Kang, J. (2020). Biobert: a pre-trained biomedical\nlanguage representation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mohamed,\nA., Levy, O., Stoyanov, V ., and Zettlemoyer, L. (2020).\nBART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages 7871–\n7880, Online. Association for Computational Linguistics.\nLi, W. and Godzik, A. (2006). Cd-hit: a fast program for\nclustering and comparing large sets of protein or nucleotide\nsequences. Bioinformatics, 22(13):1658–1659.\nLin, C.-Y . (2004). Rouge: A package for automatic evalu-\nation of summaries. In Text summarization branches out,\npages 74–81.\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W.,\nSmetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y ., dos\nSantos Costa, A., Fazel-Zarandi, M., Sercu, T., Candido,\nS., and Rives, A. (2023a). Evolutionary-scale prediction of\natomic-level protein structure with a language model. Sci-\nence, 379(6637):1123–1130. Earlier versions as preprint:\nbioRxiv 2022.07.20.500902.\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W.,\nSmetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y .,\net al. (2023b). Evolutionary-scale prediction of atomic-\nlevel protein structure with a language model. Science,\n379(6637):1123–1130.\nLiu, S., Li, Y ., Li, Z., Gitter, A., Zhu, Y ., Lu, J., Xu, Z.,\nNie, W., Ramanathan, A., Xiao, C., Tang, J., Guo, H.,\nand Anandkumar, A. (2023). A text-guided protein design\nframework. arXiv preprint arXiv:2302.04611.\nLiu, X. (2017). Deep recurrent neural network for pro-\ntein function prediction from sequence. arXiv preprint\narXiv:1701.08318.\nLoshchilov, I. and Hutter, F. (2019). Decoupled weight decay\nregularization. In International Conference on Learning\nRepresentations.\nMorris, C., Rattan, G., and Mutzel, P. (2020). Weisfeiler and\nLeman go sparse: Towards scalable higher-order graph em-\nbeddings. In Advances in Neural Information Processing\nSystems, volume 34.\nMurphy, R., Srinivasan, B., Rao, V ., and Ribeiro, B. (2019).\nRelational Pooling for Graph Representations. In Pro-\nceedings of the 36th International Conference on Machine\nLearning, pages 4663–4673.\nNikolentzos, G., Dasoulas, G., and Vazirgiannis, M. (2020).\nk-hop graph neural networks. Neural Networks, 130:195–\n205.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the As-\nsociation for Computational Linguistics, pages 311–318.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\nand Krueger, G. (2021). Clip: Learning transferable visual\nmodels from natural language supervision. InInternational\nConference on Learning Representations.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I.\n(2018). Improving language understanding by generative\npre-training.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. (2019). Language models are unsupervised\nmultitask learners.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. (2019). Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. arXiv preprint arXiv:1910.10683.\nReiser, P., Neubert, M., Eberhard, A., Torresi, L., Zhou, C.,\nShao, C., Metni, H., van Hoesel, C., Schopmans, H., Som-\nmer, T., et al. (2022). Graph neural networks for mate-\nrials science and chemistry. Communications Materials,\n3(1):93.\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo,\nD., Ott, M., Zitnick, C. L., Ma, J., et al. (2021). Biological\nstructure and function emerge from scaling unsupervised\nlearning to 250 million protein sequences. Proceedings of\nthe National Academy of Sciences, 118(15):e2016239118.\nbioRxiv 10.1101/622803.\nScarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and\nMonfardini, G. (2009). The Graph Neural Network Model.\nIEEE Transactions on Neural Networks, 20(1):61–80.\nSchlichtkrull, M., Kipf, T. N., Bloem, P., Van Den Berg, R.,\nTitov, I., and Welling, M. (2018). Modeling relational\ndata with graph convolutional networks. In The Semantic\nWeb: 15th International Conference, ESWC 2018, Herak-\nlion, Crete, Greece, June 3–7, 2018, Proceedings 15, pages\n593–607. Springer.\nSeo, Y ., Loukas, A., and Perraudin, N. (2019). Discrim-\ninative structural graph classification. arXiv preprint\narXiv:1905.13422.\nVaradi, M., Anyango, S., Deshpande, M., Nair, S., Natassia,\nC., Yordanova, G., Yuan, D., Stroe, O., Wood, G., Lay-\ndon, A., et al. (2022). Alphafold protein structure database:\nmassively expanding the structural coverage of protein-\nsequence space with high-accuracy models. Nucleic acids\nresearch, 50(D1):D439–D444.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017).\nAttention is all you need. In Guyon, I., Luxburg, U. V .,\nBengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and\nGarnett, R., editors, Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nWang, L., Liu, H., Liu, Y ., Kurtin, J., and Ji, S. (2022).\nLearning protein representations via complete 3d graph\nnetworks. arXiv preprint arXiv:2207.12600.\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\nDavison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,\nY ., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M.,\nLhoest, Q., and Rush, A. (2020). Transformers: State-of-\nthe-art natural language processing. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations , pages 38–45,\nOnline. Association for Computational Linguistics.\nXu, M., Yuan, X., Miret, S., and Tang, J. (2023). Protst:\nMulti-modality learning of protein sequences and biomed-\nical texts. arXiv preprint arXiv:2301.12040.\nZhang, C., Song, D., Huang, C., Swami, A., and Chawla,\nN. V . (2019). Heterogeneous graph neural network. In\nProceedings of the 25th ACM SIGKDD international con-\nference on knowledge discovery & data mining, pages 793–\n803.\nZhang, T., Kishore, V ., Wu, F., Weinberger, K. Q., and Artzi,\nY . (2020). Bertscore: Evaluating text generation with bert.\nIn International Conference on Learning Representations.\nZhang, X.-M., Liang, L., Liu, L., and Tang, M.-J. (2021).\nGraph neural networks and their current applications in\nbioinformatics. Frontiers in genetics, 12:690049.\nZhang, Z., Xu, M., Jamasb, A., Chenthamarakshan, V .,\nLozano, A., Das, P., and Tang, J. (2022). Protein represen-\ntation learning by geometric structure pretraining. arXiv\npreprint arXiv:2203.06125.\nZitnik, M., Agrawal, M., and Leskovec, J. (2018). Modeling\npolypharmacy side effects with graph convolutional net-\nworks. Bioinformatics, 34(13):i457–i466.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7157166600227356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.556987464427948
    },
    {
      "name": "Machine learning",
      "score": 0.5401315689086914
    },
    {
      "name": "Function (biology)",
      "score": 0.46306127309799194
    },
    {
      "name": "Representation (politics)",
      "score": 0.4284573495388031
    },
    {
      "name": "Graph",
      "score": 0.41152423620224
    },
    {
      "name": "Transformer",
      "score": 0.4104287922382355
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1588393747806549
    },
    {
      "name": "Engineering",
      "score": 0.07253777980804443
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I142476485",
      "name": "École Polytechnique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I34771391",
      "name": "University of Cyprus",
      "country": "CY"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210131858",
      "name": "Epigénétique et Destin Cellulaire",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I204730241",
      "name": "Université Paris Cité",
      "country": "FR"
    }
  ]
}