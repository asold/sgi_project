{
    "title": "Bidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model",
    "url": "https://openalex.org/W4321458910",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2107909825",
            "name": "Jinho Chang",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2100734166",
            "name": "Jong Chul Ye",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3094492244",
        "https://openalex.org/W6800751262",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4285170409",
        "https://openalex.org/W6811387395",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W3020257313",
        "https://openalex.org/W3016923549",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W4229042118",
        "https://openalex.org/W6734561206",
        "https://openalex.org/W6772452955",
        "https://openalex.org/W3095883070",
        "https://openalex.org/W4213077304",
        "https://openalex.org/W3035060554",
        "https://openalex.org/W6797236502",
        "https://openalex.org/W3097145107",
        "https://openalex.org/W3104956673",
        "https://openalex.org/W3176905117",
        "https://openalex.org/W2987522751",
        "https://openalex.org/W4295951229",
        "https://openalex.org/W2963609389",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4366769286",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W2992072991",
        "https://openalex.org/W3093934881",
        "https://openalex.org/W3005552578",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4312136097",
        "https://openalex.org/W4312039930",
        "https://openalex.org/W4240795200",
        "https://openalex.org/W4205879684",
        "https://openalex.org/W4405318654",
        "https://openalex.org/W2753921001",
        "https://openalex.org/W4297632148",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W4306801745",
        "https://openalex.org/W4390193806",
        "https://openalex.org/W4320472417",
        "https://openalex.org/W4200583473"
    ],
    "abstract": "<title>Abstract</title> Despite the impressive successes of deep learning approaches for various chemical problems such as property prediction, virtual screening, and <italic>de novo</italic> molecule design, separately designed models for specific tasks are usually required, and it is often difficult to synergistically combine these models for novel tasks. To address this, here we present a bidirectional molecular foundation model that can be used for both molecular structure and property inferences through a single model, inspired by recent multimodal learning methods such as VLP. Furthermore, thanks to the outstanding structure/property alignment in a common embedding space, experimental results confirm that our method leads to state-of-the-art performance and interpretable attention maps in both multimodal and unimodal tasks, including conditional molecule generation, property prediction, molecule classification, and reaction prediction.",
    "full_text": "Bidirectional Generation of Structure and Properties\nThrough a Single Molecular Foundation Model\nJinho Chang \nKorea Advanced Institute of Science and Technology https://orcid.org/0000-0002-7426-8304\nJong Chul Ye  \n \nKorea Advanced Institute of Science and Technology https://orcid.org/0000-0001-9763-9609\nArticle\nKeywords:\nPosted Date: February 21st, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-2425375/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: There is NO Competing Interest.\nVersion of Record: A version of this preprint was published at Nature Communications on March 14th,\n2024. See the published version at https://doi.org/10.1038/s41467-024-46440-3.\nBidirectional Generation of Structure and Properties\nThrough a Single Molecular Foundation Model\nJinho Chang 1 and Jong Chul Y e †,1\n1\nGraduate School of AI, KAIST , Daejeon, South Korea\n†Correspondence should be addressed to J.C.Y . (jong.ye@kaist.ac.kr)\nAbstract\nDespite the impressive successes of deep learning approaches for various chemical problems\nsuch as property prediction, virtual screening, and de novo molecule design, separately de-\nsigned models for speciﬁc tasks are usually required, and it is often difﬁcult to synergistically\ncombine these models for novel tasks. T o address this, here we present a bidirectional molec-\nular foundation model that can be used for both molecular structure and property inferences\nthrough a single model, inspired by recent multimodal learning methods such as VLP . Fur-\nthermore, thanks to the outstanding structure/property alignment in a common embedding\nspace, experimental results conﬁrm that our method leads to state-of-the-art performance\nand interpretable attention maps in both multimodal and unimodal tasks, including condi-\ntional molecule generation, property prediction, molecule classiﬁcation, and reaction predic-\ntion.\n1\nIntroduction\nCapturing complex relations between chemical objects and their properties is the essence of numer-\nous chemical challenges. During the last decade, artiﬁcial intelligence has emerged as a promising\ntool in chemistry research for estimating many biochemical properties and interactions between\nmolecules, polymers, and proteins, which are difﬁcult to obtain experimentally\n1– 3 . V arious deep\nlearning-based approaches in the chemical domain employed deep neural networks to extract de-\nsired characteristics like intrinsic properties, biochemical activities, and chemical reactions from\nraw molecule data\n4– 6 . Moreover, de novo molecule design has been extensively studied using gen-\nerative models 7, graph networks 8, etc 9, 10. More recently , unsupervised learning approaches of\nlearning better representations of the chemical inputs have been suggested 11– 13 to overcome the\nlimitation of learning separate features for each task in a supervised manner.\nThese recent approaches are on the same track as the concept of the foundation model, often\nconsidered as a new paradigm of deep learning 14. Speciﬁcally , foundation models refer to the\nneural network models that are pre-trained in a self-supervised manner with a wide range of data,\nproviding features that can be readily ﬁne-tuned for various downstream tasks. For example, large\npre-trained language models such as BER T\n15 and GPT -316 learn the context between word tokens\nusing self-supervised objectives like masked language model (MLM) and next world prediction\n(NWP), demonstrating powerful performance in many different NLP tasks. Their success in NLP\nhas also inspired the chemical foundation models\n17, 18 , where molecule features that describe how\natoms and functional groups are positioned are learned in a self-supervised manner.\nMeanwhile, in the computer vision ﬁeld, multimodal learning methods like V ision-Language\nPre-training (VLP) have achieved outstanding performance in downstream tasks that require an un-\nderstanding of both image and text\n19, 20 . Most of the modern VLP models learn the cross-attention\n2\nbetween different modality data by handling them collectively through Transformer architecture 21– 24.\nBy embedding images and languages in the common feature space, VLP enables various tasks such\nas visual question answering (VQA), image-text retrieval, text-to-image attention, text-driven im-\nage generation, image-driven text generation, etc., which are not possible using single modality\nfoundation models.\nInspired by the success of VLP , we are interested in the cross-modal comprehension between\nmolecule structure and the associate properties, which enables meaningful tasks in many applica-\ntions like property predictions, conditional molecule design, etc. Speciﬁcally , we propose a novel\nmolecule Structure-Property Multi-Modal foundation model (SPMM) which allows various chem-\nistry experiments in silico. SPMM is pre-trained with a wide range of molecules’ structures and a\nvector of its properties by employing an X-attention Transformer architecture\n25 and training objec-\ntives through cross-attention between structure-describing strings and property descriptor vectors.\nOur experimental results show that simultaneous learning of structural features with information\nfrom the associate properties through a single foundation model gives us a better representation\nthat can be ﬁne-tuned for various downstream tasks. Speciﬁcally , by treating both structure and\nproperty symmetrically , the model can perform bidirectional generation and prediction with a sin-\ngle pre-trained model, which was not possible before.\nFig.\n1(a) illustrates the overall model architecture and training objectives for SPMM, which\nextends the structure of the dual-stream VLP models 23– 25 . A dual-stream VLP model encodes\nthe input for each modality with a unimodal encoder, then performs cross-attention by using one\nmodality feature as a query and the other modality feature as a key/value. Speciﬁcally , SPMM\ntakes a given molecule’s SMILES (Simpliﬁed molecular-input line-entry system) string and its PV\n(property vector) as multimodal data inputs as shown in Fig.\n1(a). When the SMILES and PV are\npassed through their corresponding unimodal encoders, contrastive learning aligns the SMILES\n3\nFigure 1: (a) Overview of the model architecture and pre-training objectives of SPMM. The contrastive\nloss aligns the output feature of two unimodal encoders into the same embedding space. The fusion encoder\nlearns the relations between two modalities, trained with Next W ord Prediction (NWP), Next Property Pre-\ndiction (NPP), and SMILES-Property Matching loss (SPM). The momentum teacher provides the knowledge\ndistillation for the model. (b) Downstream tasks that require multimodal comprehension: i) SMILES-to-PV\ngeneration, ii)PV -to-SMILES generation. (c) Downstream tasks for single modality inputs: i) property predic-\ntion, ii) reaction prediction.\nand PV features into the same embedding space by assimilating the features that contain the same\ncontext. This is known to improve the model performance by making cross-modal encoding easier\nand guiding the unimodal encoded features to reﬂect more semantics of the input\n23 . Then, the\nencoded SMILES and PV features are passed through the cross-modal encoders, which perform\ncross-attention between SMILES and PV features. This fusion encoder can perform cross-attention\nwith an alternation of its query and key/value input because the contrastive learning aligns the\noutput of the SMILES encoder and the PV encoder into the same feature space.\n25 The cross-modal\nencoder is pre-trained with Next W ord Prediction (NWP) for SMILES, Next Property Prediction\n4\n(NPP), and SMILES-PV Matching loss (SPM). Prediction of the next component from the given\ntransformer input is a commonly used self-supervised learning objective, and our NWP and NPP\ntasks make the model learn the contextual relationship between SMILES tokens and properties\nwith the aid of the other modality’s semantic feature. Additionally , SPM predicts whether a given\npair of SMILES and PV represents the same molecule or not.\nOnce trained, SPMM can be used for various bidirectional downstream tasks that require\nan understanding of both SMILES and properties like property prediction (SMILES-to-properties)\nand property conditioned molecule generation (properties-to-SMILES) as shown in Fig.\n1(b). Fur-\nthermore, the pre-training objectives that we’ve used allow the pre-trained SPMM to be applied\nfor single-modality tasks as well, such as molecule classiﬁcation and reaction predictions (see\nFig.\n1(c)). The pre-trained SPMM showed comparable performances to state-of-the-art models in\nthese unimodal tasks, which suggests the model’s generalization ability as a foundation model.\nResults\nThe model learns bidirectional comprehension between SMILES and properties. Once SPMM\nwas pre-trained, we made the model generate a PV with SMILES input only , which is a crucial\nchallenge for many chemical tasks such as de novo molecule design. As one of the major ap-\nproaches for drug discovery , various methods have been suggested for generating molecules with\ndesired properties\n7– 10. In the approaches presented so far, the maximum number of simultaneously\ncontrollable properties wasn’t very large. Also, the length of the input property vector cannot be\nchanged. Whenever the target properties change, the model needs to be trained again for the new\nwanted conditions. In contrast, the pre-trained SPMM can take 53 properties used in pre-training\nas input conditions and generate molecules that satisfy all of them, without separate additional\ntraining for each property combination. Moreover, for the properties that we don’t want to control,\n5\nwe can let the model ignore those conditions by replacing them with the [UNK] token that we used\nin pre-training. This is very useful because controlling all 53 input properties is not a usual situa-\ntion in practice, and is also not easy since the properties are correlated and entangled (for example,\n‘5 atoms & 30 bonds’ or ‘2 rings & 5 aromatic rings’ is unlikely to be a valid PV input).\nT o demonstrate the molecule generation capability of SPMM, we obtained 1,000 PVs from\nSMILES that are not contained in the pre-training dataset and fed them to the pre-trained model\nto autoregressively generate SMILES using the input property . The sampling process was done\nin a deterministic manner (greedy sampling): starting from the SMILES [CLS] token, the model\npredicts the probability distribution of the next token and chooses the option with the highest\nprobability . Among the output of deterministic PV -to-SMILES generation for 1,000 PVs, 97.1%\nof the generated output were valid SMILES. The properties of the generated samples agree with the\nproperty input, with the mean normalized RMSE of 0.2216. Every generated SMILES is different\nfrom the source molecule of their input PV and doesn’t exist in the pre-train dataset.\nBeyond using PVs from already existing molecules, SPMM can perform molecule generation\nwith arbitrary PV inputs. Figure\n2 contains the output of the SPMM’s deterministic molecule\ngeneration for ﬁve PV inputs. All of those inputs originated from one PV of a randomly chosen\nmolecule, but four of them had certain values changed. The generated molecules follow the input\nmodiﬁcation while maintaining unmodiﬁed properties similarly . SPMM is even able to generate\nmolecules with the out-of-domain conditions such as log P = 7 (note that ∼ 3. 0% of the pre-\ntraining dataset has log P > 7).\nApplication ﬁelds like drug discovery often require generating multiple molecules for a sin-\ngle wanted target property condition. This can be done by sampling the next token stochastically\nfrom the modeled probability distribution instead of using a token with the highest probability . T o\n6\nFigure 2: Examples of modiﬁed molecules by changing speciﬁc values from the original PV . The colored\noutput values correspond to the changed properties from the original PV . (1) The output of the same PV of\nthe source molecule. (2) The output when #aromatic\nring is changed to 0. (3) The output when #ring and\n#aromatic ring are changed to 1. (4) The output when logP is changed to 7. (5) The output when #rotatable bond\nis changed to 12. For the generation, the other 41 property conditions are masked by the [UNK] token.\nverify our model’s ability to generate multiple molecules from a single PV input, we generated\n1,000 SMILES with stochastic sampling on a ﬁxed PV . Figure\n3 shows the property distributions\nof 1,000 molecules generated from a single PV input. The mode of each property distribution lands\non the input property value [(a)]. In the situation when only some of the properties are given, the\nmodel only regards the known properties while the other masked properties are not restricted [(b),\n(c)]. When we replace all input properties with [UNK] token [(d)], the model performs an uncondi-\ntional molecule generation, and the output follows the distribution of the pre-training dataset. The\nvalidity of the generated molecule ﬂuctuated depending on how feasible or difﬁcult the property\ninput is, and it was between 0.7 and 0.9 in most cases. The uniqueness, the ratio between the num-\nber of unique molecules against the number of validly generated molecules, was almost 100% in\nevery condition we have experimented with. The generated molecule examples under conditions\nin Figure\n3 can be found in Supplementary result S1.\n7\nFigure 3: Property distribution of the generated molecules with different PV inputs and [UNK] token\nmasking. The red vertical dotted lines are the input property values, and the grey vertical lines are the mean\nof that property in the pre-training dataset. The controlled properties are colored in red, and uncontrolled\nproperties (=masked with [UNK] token) are colored in blue. Due to the lack of space, only 12 out of 53 properties\nare shown for each case. (a) All 53 properties are controlled, without using the [UNK] token. The input PV was\nobtained from the molecule COc1cccc(NC(=O)CN(C)C(=O)COC(=O)c2cc(c3cccs3)nc3ccccc23)c1. (b) Molecular\nW eight to 150, and the other property inputs are masked. (c) #ring, #rotatable bond, TPSA, and logP are\ncontrolled to 2, 10, 50, and 0. The other property inputs are masked. (d) Every property is replaced with\n[UNK] token.\n8\nRegarding the overall molecule generation performance of SPMM, we want to emphasize\nthat SPMM can generate suitable SMILES for many property conditions that the model has not\nseen in its pre-training. When we trained SPMM without 50% of random property masking with\n[UNK] token, the model only worked when all 53 properties are given since the model has not seen\nthe partially-given properties. However, even with the technique of [UNK] token masking, the\nmodel cannot face most of the 253 possible property combination during the pre-training process.\nThe SPMM’s ability to handle arbitrary property conditions for SMILES generation comes from\ntreating PV as a ‘language with 53 words’ and focusing on each property separately , not simply\nconsidering the entire property input as a single condition. This innovative approach for conditional\nmolecule generation has never been demonstrated with the existing methods, thus can be used for\nmany important chemical ﬁelds.\nWith the same approach as SMILES generation, the pre-trained SPMM can also be used\nto generate a PV with SMILES input only . This task is equivalent to performing 53 property\npredictions of a given SMILES at once. Similar to the SMILES generation, properties are predicted\nin an autoregressive manner: the model predicts the ﬁrst property value using only the property\n[CLS] token, then takes all previous outputs again to get the next prediction value, and so on.\nAlthough 53 properties can be calculated using the RDKit python module for a given SMILES, the\npurpose of this experiment is to verify that the data-driven way of property estimation coincides\nwith the analytic approach.\nSpeciﬁcally , we fed 1,000 SMILES that are not contained in the pre-training dataset to the\npre-trained SPMM and generated their corresponding PV . Fig.\n4 is the scatter plot of the real\nproperty value against the generated output for 12 selected properties out of 53 that we used for\npre-training. It is clear that SPMM’s predicted property is very close to the actual value, and most\nof the data point lies on the y = x line. Although the model virtually has never seen a full-ﬁlled PV\n9\nFigure 4: Scatter plots of the real property value against the generated output, for 12 selected properties.\nThe x-axis is the real property value, and the y-axis is the model output. The grey dotted line is the y = x line.\nin the pre-training due to the 50% of random property masking, the model could autoregressively\npredict all 53 properties as a whole. The mean r2 score of the 53 properties was 0.9724. Since the\nvalues of the properties span multiple orders of magnitude, we measured the Root Mean Square\nError (RMSE) on normalized values, and the mean RMSE of the 53 normalized properties was\n0.1242. The full scatter plot for all 53 properties with each r2 score and raw RMSE is in the\nSupplementary Figure\nS2.\nT o provide an interpretation of the pre-trained SPMM’s performance presented so far, we\n10\nfurther analyzed the learned cross-modal comprehension between SMILES and property vectors\nby visualizing the attention scores from the pre-trained SPMM. Transformer-based models have\nthe beneﬁt of intuitive attention visualization that shows how the model considers the relation\nbetween the input queries and keys, by providing cross-attention scores between them.\nFigure 5: The mean attention score from 8 heads in the SPMM fusion encoder’s ﬁnal cross-attention layer\nfor two sample molecules. A darker green means a higher attention score. For the attention process, the prop-\nerty features were used as queries, and the SMILES features are used as keys and values. The corresponding\nfragments for each token are indicated with ivory boxes on the molecular structure, while fragments for dupli-\ncated tokens are color-coded with purple. W e have calculated cross-attention scores for all 53 properties and\nSMILES tokens, but only 12 of those properties are shown.\nIn ﬁgure\n5, we have plotted the cross-attention score from the last fusion layer of our pre-\ntrained SPMM when SMILES and its property vector inputs were given. Since there are multiple\nheads for the cross-attention, we took the mean of their attention scores. It is interesting that\nthe aspect of cross-attention scores followed the intuitive relations between chemical properties\nand molecular fragments. The properties related to hydrogen bonding ( NumHDonors, NumHAc-\nceptors) show high attention scores for tokens with oxygen and nitrogen atoms. The property\n11\nRingCount focuses on the tokens that are involved with rings while showing weak attention to\nside groups, and the property NumAromaticRings only gives high attention score to the compo-\nnents of aromatic rings. When different SMILES tokens played a similar role in the molecule such\nas ’ c1ccccc1)’ and ’ c1ccccc1’ in the ﬁrst example, their attention patterns were similar as well.\nThis result demonstrated that SPMM could capture the relations between molecule structures and\nchemical properties without explicitly given supervision between them.\nGeneralization ability as a molecular foundation model. So far, we have demonstrated that the\npre-trained SPMM can be applied to tasks that require an understanding of the relationship between\nSMILES and properties. However, we can also employ the pre-trained SPMM for challenges that\nonly use SMILES data, such as molecular property prediction. One advantage of having a dual-\nstream VLP model structure is that the SPMM’s multimodal pre-training process includes adjusting\nthe output of one unimodal encoder to contain contextual information from the other modality ,\nby aligning it with the other unimodal encoder’s output. This implies that the SMILES encoder\noutput is a unimodal representation vector, that not only embeds the input molecule’s structural\ninformation but it’s also enhanced by its property information.\nW e have analyzed if our pre-trained model had learned a good representation that can be\nreadily used for other tasks, even for a single modality . So we only utilized the SMILES encoder\nof pre-trained SPMM and made a benchmark study on 8 MoleculeNet\n26 downstream tasks. Each\nMoleculeNet task is a regression or classiﬁcation task for pharmaceutical/biochemical applications\nlike solubility , toxicity , and brain penetrability .\nT able\n1 contains the performance of SPMM and other models for MoleculeNet. Using only\n4 BER T encoder layers with 9 million parameters, SPMM showed comparable performances with\nstate-of-the-art models for all tasks. It achieved the best performance for BBBP , Clintox, Freesolv ,\n12\nClearance, and BACE regression tasks, showing its capability as a foundation model. W e’ve also\nobserved that the score of our model dramatically decreased without pre-training.\nregression[RMSE, ↓] classiﬁcation[AUROC in %, ↑]\nDataset Delaney ESOL LIPO Freesolv BACE Clearance BBBP BACE Clintox SIDER\n#data 1128 4200 642 1513 837 2039 1513 1478 1427\n#task 1 1 1 1 1 1 1 2 27\nD-MPNN27 1.050 0.683 2.082 2.253 49.754 71.0 80.9 90.6 57.0\nN-GramRF28 1.074 0.812 2.688 1.318 52.077 69.7 77.9 77.5 66.8\nN-GramXGB28 1.083 2.072 5.061 - - 69.1 79.1 87.5 65.5\nPretrainGNN29 1.100 0.739 2.764 - - 68.7 84.5 72.6 62.7\nGROVERlarge 30 0.895 0.823 2.272 - - 69.5 82.6 81.2 65.4\nChemRL-GEM31 0.798 0.660 1.877 - - 72.4 85.6 90.1 67.2\nChemBER T a-2(MTR 77M)17 0.889 0.798 - 1.363 48.515 72.8 79.9 56.3 -\nSPMM(w/o pre-train) 1.082 0.996 3.019 1.382 51.949 66.6 79.6 67.9 59.3\nSPMM 0.810 0.706 1.859 1.108 44.752 73.3 83.0 91.0 64.7\nT able 1: Benchmark results on MoleculeNet downstream tasks. The best performance for each task was\nwritten in bold, and the second-best performance was underlined. For each task, we ﬁne-tuned our model in\nthree random seeds and recorded the mean of those results. The benchmark model results were taken from\nChemRL-GEM and ChemBERT a-2.\nW e also trained SPMM for the reaction prediction task on USPTO-480k dataset\n32, which\nrequires the model to predict the product SMILES from the reactant SMILES. This can be ap-\nproached with a ‘separated task’ by indicating which molecule in the reactant is the major compo-\nnent to form a product or a ‘mixed task’ which provides no information about the major reactant.\nSince the latter is the more common and generalized problem condition, we utilized our model\nfor a mixed task. T able\n2 shows that SPMM had similar performance with other state-of-the-art\nreaction prediction models, although the number of the training data size of our model (i.e.10M\nmolecules) is much smaller than 100M molecules in Chemformer.\nModel Molecular Transformer Augmented Transformer Chemformer Graph2SMILES MEGAN LocalTransform SPMM\ntop-1 acc.[%] 88.7 90.6 91.3 90.3 86.3 90.8 87.1\nT able 2: The performance of SPMM and other works on the USPTO-480k reaction prediction task. The\nhighest accuracy is written in bold. The benchmark model results are from the paper of LocalT ransform 4.\n13\nDiscussion\nA concept of pre-training a neural network in an unsupervised manner for a better feature repre-\nsentation has been adapted for various chemical ﬁelds\n11– 13. N-Gram Graph and GROVER used a\ngraph neural network (GNN) and a graph transformer network, respectively , to obtain a pre-trained\nmodel from the molecular graph\n28, 30 . ChemBER T a-2 trained a roBER T a model with 77 million\nSMILES to build a molecular foundation model, by training the model to predict 200 different\nchemical property values.\n17. Furthermore, several recent works tried to obtain a better feature of a\nmolecule by sharing knowledge from different data representations. Winter et al. trained a trans-\nlation model between SMILES and InChI key to get a feature vector with meaningful information\nthat both molecular representations have in common\n33. Zhu et al. used a self-supervised training\nmethod of BYOL 34 between different molecule representations of SMILES and molecular graphs\nto build a dual-view model 35. Y et, these works introduced multimodality only for the enhancement\nof a molecule feature for unimodal tasks, not for the interplay between those different modalities.\nOn the other hand, this work proposed a bidirectional transformer-based foundation model\nthat can be used for bidirectional generation/estimation of molecular structure and properties as\nwell as allows for reaction prediction. In the process, we presented a method of treating prop-\nerty collections as a language so that the model could learn the relationship between SMILES\nand each property independently . W e demonstrated that pre-trained SPMM showed remarkable\nperformances in problems for interactions between SMILES and PV domains. And not only for\nmultimodal challenges, but even its unimodal feature for SMILES, SPMM also provides a good\nrepresentation that can be ﬁne-tuned for many molecular downstream tasks. All of these results\nwere obtained with a pre-training of 10,000,000 molecules, which is relatively small compared to\nother large pre-training approaches and still has room for better performance with more data and\nparameters with a better computational environment, or different property selection.\n14\nDespite the noticeable performances of SPMM, it has several chances for improvement. One\nof those comes from using the SMILES notation. Although SMILES can contain full details about\nthe 2D structure of the molecule, the information on how atoms and bonds are connected only exists\nimplicitly . Also, a slight modiﬁcation in molecular structure can be a drastic change in SMILES.\nGraph format is another widely used modality for molecule representation that contains the explicit\ninformation of the adjacency matrix, which can be an alternative for SMILES. Another limitation\nin our current SPMM is that the 53 properties we used happen to be invariant with the changes\nin the stereochemistry of the given molecule. It is known that considering stereochemistry plays\na crucial part in various biochemical tasks. However, the 53 properties we used cannot provide\nany knowledge about stereochemical information since their values are unchanged in different\nstereoisomers. This makes the SMILES encoder output of different stereoisomers converge since\nthe contrastive loss aligns them to the same PV feature. W e believe this is the prominent factor that\nlowered the performance of SPMM in MoleculeNet tasks, which could be resolved by using more\nproperties that could reﬂect the molecule’s stereochemistry . Overcoming these drawbacks of the\ncurrent study and making the model more applicable to other chemical tasks could be the works\nfor the future.\nNevertheless, we believe that our approach can provide a pre-trained model that covers each\ninput domain and their multimodal domain simultaneously , which has a vast potential utility . W e\nexpect this approach to be applied to more various and practical chemical situations by using\nbroader and richer molecular modalities, and possibly , different biochemical domains like poly-\nmers and proteins.\n15\nMethods\nHandling SMILES and property values as a language. Molecules can be represented with var-\nious formats such as ﬁngerprints, strings like SMILES, InChI (International Chemical Identiﬁer),\nor a molecular graph. Since these different notations contain almost the same information about\ncomplete molecular structure, we employ SMILES to describe a molecule structure. SMILES is a\nsequence of characters that represents the connection structure of the molecule. Many researchers\ntreat SMILES as a variant of language data and utilize a concept of language models for chemical\ntasks on SMILES data\n9, 17, 36. In this work, like a standard BER T input, the raw SMILES string is\ntokenized by the tokenizer and embedded by the SMILES encoder with the [CLS] token and the\n[SEP] token. Our tokenizer uses a subword dictionary of 300 tokens obtained from the pre-training\ndata SMILES corpus by the BPE algorithm.\nMeanwhile, a set of chemical properties does not change its carrying information by chang-\ning the internal order, but they certainly have correlations between the properties. And it is known\nthat a transformer architecture also performs well for different modalities like images, by giving\nspeciﬁc order to its component patches and treating them as a sequence. For this work, we used a\nPV that contains 53 molecular properties for each SMILES and considered this as a sentence with\na length of 53. This set of properties covers a wide range from simple ones, such as the number of\nrings and molecular weight, to complex properties like water solubility , TPSA, and druggability .\nOne beneﬁt of viewing PV as a language is that we do not have to collect all elements to build a\nvalid PV . In contrast to a simple vector input, some property elements can be removed or masked\nin our approach, as described in Figure\n6.\nFigure 6 illustrates our embedding procedure for the property input. Each property element\nin the PV is a numerical value and normalized with the mean and standard deviation of that prop-\n16\nFigure 6: Embedding property values as an input of the PV encoder .\nerty . The order of these 53 properties are predetermined. Each value in the PV is encoded to a\nfeature vector using a linear layer as a value encoding ([a]).\nThen we randomly replaced 50% of the property features into the [UNK] token to simulate\nthat the property is unknown ([b]). This is possible since there is no problem in describing a\nmolecule using only a part of these properties. Random property feature masking prevents the\nmodel from overly dependent on the speciﬁc property , has the effect of data augmentation, and\nimproves the generalization ability of the model. Although every property we used in this work can\nbe easily and thoroughly prepared by the computer, this might not be the case for other properties\nin real-world situations. SPMM still can be trained when some properties for certain training\nmolecules are not known, by replacing those unknown properties with the [UNK] token.\nOn top of the random-masked value encoding, we added a positional encoding similar to that\nin BER T ([c]). Since the order of the properties are ﬁxed, this position embedding is equivalent\nto giving a unique index for each property and adding an embedding of that corresponding index.\nThen we pass the ﬁnal result to the PV encoder with the property [CLS] token.\n17\nPre-training objectives. Contrastive learning aims to learn better unimodal representation by\naligning the features from different modalities into the same feature space. When the encoded\nfeatures of [CLS] tokens of SMILES S and PV P are given as Scls and Pcls, we calculate the\nsimilarity function sim(S, P ) and sim(P, S ) as:\nsim(S, P ) = ( hS(Scls))⊺hP (Pcls), sim (P, S ) = ( hP (Pcls))⊺hS(Scls) (1)\nwhere hS and hP are the linear projection + normalization layer for SMILES and property vec-\ntor, respectively . Now , for a given pair of S and P , we calculate the SMILES-to-PV and PV -to-\nSMILES intermodal similarities as follows:\nss2p = exp (sim(S, P n)/τ )\nΣ N\nn=1exp (sim(S, P n)/τ ), s p2s = exp (sim(P, S m)/τ )\nΣ M\nm=1exp (sim(P, S m)/τ ) (2)\nwhere τ is a learnable temperature parameter, which has a sharpening effect by exaggerating the\nsimilarity difference. The intramodal similarities can be calculated in the same way .\nss2s = exp (sim(S, S m)/τ )\nΣ M\nm=1exp (sim(S, S m)/τ ), s p2p = exp (sim(P, P n)/τ )\nΣ N\nn=1exp (sim(P, P n)/τ ) . (3)\nFor each SMILES and PV instance, we randomly select a “negative” pair from the other modality\nand match them as negative pairs. Here, S and P are chosen by hard-negative mining, which\ngives a higher chance of being selected as a negative pair for instances that has a higher similarity\naccording to Eq. (\n2) but isn’t a positive match. This makes the training more difﬁcult and forces\nthe model to learn how to distinguish similar instances.\nThe overall contrastive loss is deﬁned using the cross-entropy loss H and one-hot similarity\n18\ny:\nLcontrastive = 1\n2(H(ys2p, s s2p) + H(yp2s, s p2s) + H(ys2s, s s2s) + H(yp2p, s p2p)) (4)\nFollowing the recent contrastive loss application in VLP 37 , we build the SMILES and PV queues\nthat store the k most recent SMILES and PV instances and use them for contrastive loss. W e set\nour queue size k to 4,096.\nNext W ord Prediction (NWP) trains the model to predict the (n + 1)-th SMILES token when\n0 ∼ n-th tokens and the corresponding PV are given. Predicting the next token is a common\nobjective for training language models, known for being utilized in the pre-training of GPT -3 16 .\nThis can be done with a single ﬂow for each SMILES by applying a causal mask in the self-\nattention of the SMILES encoder and fusion encoder. Let S and P denote the input SMILES\nand the corresponding PV , and pNW P(S, P ) denote the model’s predicted next word probability\ndistribution. The loss for NWP is deﬁned as\nLNW P = H(S, p NW P(S, P )) , (5)\nsince S itself becomes a label for the next word prediction task.\nW e applied a similar concept of NWP for the property vector as Next Property Prediction\n(NPP). NPP makes the model predict the next property value using its corresponding SMILES\nand the previous properties. Since each property element is a numerical value, we replaced the\ncross-entropy loss in NWP with MSELoss. When S and P denote the input SMILES and the\ncorresponding PV , and ˆP (P, S ) denotes the model’s predicted next property values with causal\n19\nmask in the PV and the fusion encoder, the loss for NPP is given by\nLNP P = MSE (P, ˆP (P, S )) (6)\nwhere MSE refers to the mean square error. In NPP , the model does not predict the property value\nif it is replaced with [UNK] token.\nSMILES-Property Matching (SPM) learns if a given SMILES-PV pair is matched or not. W e\nconcatenate the feature of two [CLS] tokens from the fusion encoder output of the SMILES and\nPV and perform a binary classiﬁcation with a linear layer SPM head. The SPM loss can be deﬁned\nas\nLSP M = H(ySP M, p SP M(S, P )) (7)\nwhere pNW P is a output of SPM head and ySP M is a one-hot label for SMILES-PV matching.\nThe overall pre-training objective is the combined loss of Contrastive, NWP , NPP , and SPM\nloss:\nL = LContrastive + LNW P + LNP P + LSP M (8)\nIn contrastive learning, using a one-hot label could be too strict since it regards all instances\nthat came from other pairs as equally-negative instances. However, some PVs might agree with\nmany SMILES, not only one SMILES that they’re paired with. Even SMILES can be matched\nwith different PVs since there’s a 50% of masking in a PV (for example, “ MW =[UNK], logP=2.1,\n#atom=12” and “ MW =78, logP=2.1, #atom=[UNK]” both explain Benzene, even if they came\n20\nfrom the different molecules). A similar problem also occurs for NWP . Sometimes there could be\nmultiple good options for being the next token, but using a one-hot label for ground truth might\nignore this.\nT o resolve this issue, we built the momentum teacher model and utilized its output for con-\ntrastive learning and NWP . The momentum teacher performs a knowledge distillation by providing\na pseudo-label that reﬂects how the teacher model comprehends. The model is trained to minimize\nthe loss L to predict both one-hot labels Lonehot and the distilled pseudo-label Lmomentum, with\nan adjusting hyperparameter α . The parameters of the momentum teacher model wmomentum are\nupdated by the exponential moving average (EMA) using the student model’s parameters wmodel\nand an EMA hyperparameter λ:\nL = (1 − α )Lonehot + αL momentum (9)\nand\nwmomentum = (1 − λ)wmodel + λwmomentum (10)\nT raining for downstream tasks. Supplementary Figure\nS3 describes how we utilized our pre-\ntrained model for downstream tasks. For PV generation and SMILES generation (Supplementary\nFigure\nS3-(a), (b)), we don’t need additional ﬁne-tuning since their training objectives are already\nincluded in the pre-training (NWP , NPP). For the inference procedure, the model generates PV\nor SMILES with autoregressive sampling. Speciﬁcally , starting from the [CLS] token, the model\npredicts the ﬁrst component and repeats taking the previous outputs to predict the next component\nuntil it’s done or meets a sign to stop. A causal mask has to be used in the self-attention of the\nfusion encoder and the unimodal encoder of the generating modality .\n21\nFor MoleculeNet downstream tasks that only provide SMILES data, we utilized only the\nSMILES encoder part of the model (Supplementary Figure S3-(c)). After the input molecule is\nencoded with the SMILES encoder, we pass the feature of the [CLS] token through a classiﬁca-\ntion/regression head to get an output. The classiﬁcation/regression head consists of MLP with one\nhidden layer. W e ﬁne-tuned our model with the given training set and get a checkpoint with the\nlowest loss on the validation set, and recorded that checkpoint’s performance on the test set.\nThe chemical reaction prediction task provides a reactant SMILES (that contains multiple\nreagent molecules) and a product SMILES. W e encode these two inputs with the SMILES encoder,\nthen feed them into the fusion encoder + prediction head. The model is trained to autoregressively\ngenerate the original product SMILES (Supplementary Figure\nS3-(d)). In the inference stage,\nstarting from the [CLS] token, the model predicts the next token until it generates the [SEP] to-\nken. Similar to the SMILES generation, the self-attention of the fusion encoder and the reactant\nSMILES encoder uses a causal mask.\nData preparation. W e obtained 10,000,000 SMILES of general molecules from PubChem\n38 for\npre-training. All 53 properties we used can be calculated with SMILES using the RDKit python\nmodule. The dataset for the MoleculeNet downstream tasks is provided by the DeepChem\n39 python\nlibrary . W e split every dataset into train/valid/test sets in a ratio of 8:1:1 using a scaffold splitter\nfrom DeepChem, which is a more harsh condition for the model than random splitting. For the re-\naction prediction task, we used the USPTO-480k dataset which contains 479,035 pairs of reactants\nand the major product of their reaction.\nImplementation details. W e employed the architecture of 4 BER T layers for our PV encoder and\nSMILES encoder, and 4 BER T layers with cross-attention for our fusion encoder. Based on the\nBER Tbase, we reduced the model size by changing the hidden size to 384, the number of attention\n22\nheads to 8, and the feedforward layer’s intermediate feature size to 2,048.\nW e pre-trained the model until it converges using a batch size of 128 and the AdamW op-\ntimizer with a weight decay of 0.02. The learning rate is warmed up to 1e − 4 and decreased to\n1e − 5 with a cosine scheduler. W e used the momentum-adjusting hyperparameter α of 0.4. Since\nthe pseudo-label from the momentum teacher is not useful in the early stages of the training, we\nlinearly increased α from 0 to 0.4 during the ﬁrst epoch. The EMA hyperparameter λ was ﬁxed to\n0.995, and the size of the PV and SMILES queue was 4,096. The momentum models are not used\nfor downstream tasks. The full description of training for downstream tasks is in Supplementary\nT able\nS1.\nCorrespondence Correspondence and requests for materials should be addressed to Jong Chul Y e. (email:\njong.ye@kaist.ac.kr).\nAcknowledgements This work was supported by the Institute of Information & communications T ech-\nnology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075,\nArtiﬁcial Intelligence Graduate School Program (KAIST)), National Research Foundation (NRF) of Korea\ngrant NRF-2020R1A2B5B03001980, and by the KAIST Key Research Institute (Interdisciplinary Research\nGroup) Project.\nAuthor Contributions J.C. prepared the code, performed all experiments and analyses, collected data,\nand wrote the manuscript. J.C.Y . supervised the project in conception and discussion, and prepared the\nmanuscript.\nCompeting Interests The authors declare that they have no competing ﬁnancial interests.\nData A vailability Scaffold-split MoleculeNet datasets are available via DeepChem python module\nhttps://deepchem.io/,\nand raw databases can be found in the MoleculeNet website https://moleculenet.org/. The USPTO dataset\n23\nthat we’ve used in this work is available at https://github.com/wengong-jin/nips17-rexgen.\nCode A vailability The code for SPMM will be available upon request.\nReferences\n1. Ryu, S. & Lee, S. Accurate, reliable and interpretable solubility prediction of druglike\nmolecules with attention pooling and bayesian learning (2022).\n2. Kuenneth, C. & Ramprasad, R. polybert: A chemical language model to enable fully machine-\ndriven ultrafast polymer informatics (2022).\n3. Moon, S., Zhung, W ., Y ang, S., Lim, J. & Kim, W . Y . Pignet: A physics-informed deep\nlearning model toward generalized drug–target interaction predictions (2022).\n4. Chen, S. & Jung, Y . A generalized-template-based graph neural network for accurate organic\nreactivity prediction (2022).\n5. Paul, D. et al. Artiﬁcial intelligence in drug discovery and development. Drug Discovery\nT oday(2020).\n6. Xu, C., W ang, Y . & Farimani, A. B. Transpolymer: A transformer-based language model for\npolymer property predictions. arXiv .org(2022).\n7. Lim, J., Ryu, S., Kim, J. W . & Kim, W . Y . Molecular generative model based on conditional\nvariational autoencoder for de novo molecular design (2018).\n8. Lim, J., Hwang, S.-Y ., Moon, S., Kim, S. & Kim, W . Y . Scaffold-based molecular design with\na graph generative model. Chem. Sci. 11, 1153–1164 (2020).\n24\n9. W ang, W ., W ang, Y ., Zhao, H. & Sciabola, S. A transformer-based generative model for de\nnovo molecular design (2022).\n10. Zhou, Z., Kearnes, S., Li, L., Zare, R. N. & Riley , P . Optimization of molecules via deep\nreinforcement learning (2019).\n11. Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta: Large-scale self-supervised pre-\ntraining for molecular property prediction. arXiv preprint arXiv:2010.09885 (2020).\n12. Zhang, Z. et al. Can pre-trained models really learn better molecular representations for ai-\naided drug discovery? arXiv:2209.07423 (2022).\n13. Melnyk, I. et al. Reprogramming large pretrained language models for antibody sequence\ninﬁlling (2022).\n14. Bommasani, R. et al. On the opportunities and risks of foundation models (2021).\n15. Devlin, J., Chang, M.-W ., Lee, K. & T outanova, K. Bert: Pre-training of deep bidirectional\ntransformers for language understanding (2018).\n16. Brown, T . B. et al. Language models are few-shot learners (2020).\n17. Ahmad, W ., Simon, E., Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta-2: T owards\nchemical foundation models (2022).\n18. Horawalavithana, S. et al. Foundation models of scientiﬁc knowledge for chemistry: Oppor-\ntunities, challenges and lessons learned. In Proceedings of BigScience Episode #5 – W orkshop\non Challenges & P erspectives in Creating Large Language Models, 160–172 (Association for\nComputational Linguistics, virtual+Dublin, 2022).\n19. Chen, F . et al. Vlp: A survey on vision-language pre-training (2022).\n25\n20. Radford, A. et al. Learning transferable visual models from natural language supervision\n(2021).\n21. Chen, Y .-C. et al. Uniter: Universal image-text representation learning (2019).\n22. Li, X. et al. Oscar: Object-semantics aligned pre-training for vision-language tasks (2020).\n23. Li, J. et al. Align before fuse: V ision and language representation learning with momentum\ndistillation (2021).\n24. Y u, J. et al. Coca: Contrastive captioners are image-text foundation models (2022).\n25. Park, S., Lee, E. S., Shin, K. S., Lee, J. E. & Y e, J. C. Self-supervised co-learning of uncurated\nimages and reports enables oversight ai in radiology (2022).\n26. Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning. Chem. Sci. 9, 513–\n530 (2018).\n27. Y ang, K. et al. Analyzing learned molecular representations for property prediction. Journal\nof Chemical Information and Modeling 59, 3370–3388 (2019). PMID: 31361484.\n28. Liu, S., Demirel, M. F . & Liang, Y . N-gram graph: Simple unsupervised representation for\ngraphs, with applications to molecules (2018).\n29. Hu, W . et al. Strategies for pre-training graph neural networks (2019).\n30. Rong, Y . et al. Self-supervised graph transformer on large-scale molecular data (2020).\n31. Fang, X. et al. Geometry-enhanced molecular representation learning for property prediction.\nNature Machine Intelligence 4, 127–134 (2022).\n26\n32. Jin, W ., Coley , C., Barzilay , R. & Jaakkola, T . Predicting organic reaction outcomes with\nweisfeiler-lehman network. In Guyon, I. et al. (eds.) Advances in Neural Information Pro-\ncessing Systems, vol. 30 (Curran Associates, Inc., 2017).\n33. Winter, R., Montanari, F ., No ´e, F . & Clevert, D.-A. Learning continuous and data-driven\nmolecular descriptors by translating equivalent chemical representations. Chem. Sci. 10, 1692–\n1701 (2019).\n34. Grill, J.-B. et al. Bootstrap your own latent: A new approach to self-supervised learning\n(2020).\n35. Zhu, J. et al. Dual-view molecule pre-training (2021).\n36. Lee, K., Jang, J., Seo, S., Lim, J. & Kim, W . Y . Drug-likeness scoring based on unsupervised\nlearning (2021).\n37. He, K., Fan, H., Wu, Y ., Xie, S. & Girshick, R. Momentum contrast for unsupervised visual\nrepresentation learning (2019).\n38. Kim, S. et al. Pubchem in 2021: new data content and improved web interfaces. Nucleic Acids\nResearch 49, D1388–D1395 (2021).\n39. Ramsundar, B. et al. Deep Learning for the Life Sciences (O’Reilly Media, 2019).\n27\nSupplementary Materials\nFigure S1: The examples of stochastically generated molecules by pre-trained SPMM with four different\ngiven property vectors.\n28\nFigure S2: The scatter plots of the model’s generated property against the actual property value for all 53\nproperties. The r2 score and RMSE for each property are described at the top of each plot.\n29\nFigure S3: Overview of the inference and ﬁne-tuning of SPMM for various downstream tasks: (a) The in-\nference process of pre-trained SPMM for molecule generation. (a) The inference process of pre-trained SPMM\nfor PV generation. (c) The model architecture for MoleculeNet downstream tasks. The SMILES encoder of\npre-trained SPMM is used as a backbone. (d) The model architecture for the reaction prediction task. W e\nadopted the SMILES encoder and the fusion encoder of pre-trained SPMM, and built a translation model that\ntakes the reactant SMILES as an input to predict the product SMILES.\n30\ntask Delaney ESOL LIPO Freesolv BACE(reg.) BACE(cls.) Clearance BBBP Clintox SIDER\noptimizer AdamW , weight decay=0.02\nscheduler cosine + warmup\nbatch size 4 16 4 8 8 8 8 8 4\nlearning rate(min, max) 2e-5, 5e-6 2e-5, 5e-6 7e-5, 1e-5 3e-5, 5e-6 5e-5, 1e-5 2e-5, 5e-6 3e-5, 5e-6 2e-5, 5e-6 7e-5, 1e-5\nepoch 20 25 50 20 15 10 10 15 20\ntask USPTO reaction prediction\noptimizer AdamW , weight decay=0.02\nscheduler cosine + warmup\nbatch size 16\nlearning rate(min, max) 1e-4, 1e-6\nepoch 30\nT able S1: Detailed training hyperparameters for ﬁne-tuning SPMM in MoleculeNet downstream tasks and\nthe USPTO reaction prediction task.\n31"
}