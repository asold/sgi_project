{
    "title": "Enhancing Early Detection of Cognitive Decline in the Elderly: A Comparative Study Utilizing Large Language Models in Clinical Notes",
    "url": "https://openalex.org/W4393990831",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2787159911",
            "name": "Xinsong Du",
            "affiliations": [
                "Brigham and Women's Hospital",
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A5095090184",
            "name": "John Novoa-Laurentiev",
            "affiliations": [
                "Brigham and Women's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5114340380",
            "name": "Joseph M. Plasaek",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2154366293",
            "name": "ya wen Chuang",
            "affiliations": [
                "Taichung Veterans General Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2097765900",
            "name": "Liqin Wang",
            "affiliations": [
                "Brigham and Women's Hospital",
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A4221239535",
            "name": "Gad Marshall",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114355686",
            "name": "Stephanie K Mueller",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2145148408",
            "name": "Frank Chang",
            "affiliations": [
                "Brigham and Women's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2925389593",
            "name": "Surabhi Datta",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2594549060",
            "name": "Hunki Paek",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2053558861",
            "name": "Bin Lin",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1976683342",
            "name": "Qiang Wei",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2097738364",
            "name": "Xiaoyan Wang",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2143041298",
            "name": "Jingqi Wang",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1979593781",
            "name": "Hao Ding",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2224296358",
            "name": "Frank J. Manion",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2343127410",
            "name": "Jingcheng Du",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2116978417",
            "name": "David W Bates",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2019114847",
            "name": "Li Zhou",
            "affiliations": [
                "Harvard University",
                "Brigham and Women's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2787159911",
            "name": "Xinsong Du",
            "affiliations": [
                "Brigham and Women's Hospital",
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A5095090184",
            "name": "John Novoa-Laurentiev",
            "affiliations": [
                "Brigham and Women's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2154366293",
            "name": "ya wen Chuang",
            "affiliations": [
                "Taichung Veterans General Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2097765900",
            "name": "Liqin Wang",
            "affiliations": [
                "Brigham and Women's Hospital",
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A2145148408",
            "name": "Frank Chang",
            "affiliations": [
                "Brigham and Women's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2925389593",
            "name": "Surabhi Datta",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2594549060",
            "name": "Hunki Paek",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2053558861",
            "name": "Bin Lin",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1976683342",
            "name": "Qiang Wei",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2097738364",
            "name": "Xiaoyan Wang",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2143041298",
            "name": "Jingqi Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1979593781",
            "name": "Hao Ding",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2224296358",
            "name": "Frank J. Manion",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2343127410",
            "name": "Jingcheng Du",
            "affiliations": [
                "Intelligent Medical Objects (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2019114847",
            "name": "Li Zhou",
            "affiliations": [
                "Harvard University",
                "Brigham and Women's Hospital",
                "Intelligent Medical Objects (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4327810158",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4390041933",
        "https://openalex.org/W4393861499",
        "https://openalex.org/W3102840478",
        "https://openalex.org/W2409038960",
        "https://openalex.org/W2966351171",
        "https://openalex.org/W4386002582",
        "https://openalex.org/W2181606501",
        "https://openalex.org/W4324309277",
        "https://openalex.org/W4292471401",
        "https://openalex.org/W4378219872",
        "https://openalex.org/W2040791183",
        "https://openalex.org/W2084397926",
        "https://openalex.org/W2900856701",
        "https://openalex.org/W3214401235",
        "https://openalex.org/W4320481812",
        "https://openalex.org/W3014391303",
        "https://openalex.org/W4308735516",
        "https://openalex.org/W3043374725",
        "https://openalex.org/W4280578286",
        "https://openalex.org/W4387767257",
        "https://openalex.org/W2888015509",
        "https://openalex.org/W4378711639",
        "https://openalex.org/W4389519226",
        "https://openalex.org/W4387848745",
        "https://openalex.org/W4376122773",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4391292768",
        "https://openalex.org/W4389156617",
        "https://openalex.org/W2295598076",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W3149839747",
        "https://openalex.org/W4318677156",
        "https://openalex.org/W2952418323",
        "https://openalex.org/W4353015365",
        "https://openalex.org/W3102251128"
    ],
    "abstract": "Abstract Background Large language models (LLMs) have shown promising performance in various healthcare domains, but their effectiveness in identifying specific clinical conditions in real medical records is less explored. This study evaluates LLMs for detecting signs of cognitive decline in real electronic health record (EHR) clinical notes, comparing their error profiles with traditional models. The insights gained will inform strategies for performance enhancement. Methods This study, conducted at Mass General Brigham in Boston, MA, analyzed clinical notes from the four years prior to a 2019 diagnosis of mild cognitive impairment in patients aged 50 and older. We used a randomly annotated sample of 4,949 note sections, filtered with keywords related to cognitive functions, for model development. For testing, a random annotated sample of 1,996 note sections without keyword filtering was utilized. We developed prompts for two LLMs, Llama 2 and GPT-4, on HIPAA-compliant cloud-computing platforms using multiple approaches (e.g., both hard and soft prompting and error analysis-based instructions) to select the optimal LLM-based method. Baseline models included a hierarchical attention-based neural network and XGBoost. Subsequently, we constructed an ensemble of the three models using a majority vote approach. Results GPT-4 demonstrated superior accuracy and efficiency compared to Llama 2, but did not outperform traditional models. The ensemble model outperformed the individual models, achieving a precision of 90.3%, a recall of 94.2%, and an F1-score of 92.2%. Notably, the ensemble model showed a significant improvement in precision, increasing from a range of 70%-79% to above 90%, compared to the best-performing single model. Error analysis revealed that 63 samples were incorrectly predicted by at least one model; however, only 2 cases (3.2%) were mutual errors across all models, indicating diverse error profiles among them. Conclusions LLMs and traditional machine learning models trained using local EHR data exhibited diverse error profiles. The ensemble of these models was found to be complementary, enhancing diagnostic performance. Future research should investigate integrating LLMs with smaller, localized models and incorporating medical data and domain knowledge to enhance performance on specific tasks.",
    "full_text": "1 \n \nEnhancing Early Detection of Cognitive Decline in the Elderly through Ensemble of NLP Techniques: A \nComparative Study Utilizing Large Language Models in Clinical Notes \n \nXinsong Du1,2*, John Novoa-Laurentiev1, Joseph M. Plasek1,2, Ya-Wen Chuang3, Liqin Wang1,2, Frank Chang1, \nSurabhi Datta4, Hunki Paek4, Bin Lin4, Qiang Wei4, Xiaoyan Wang4, Jingqi Wang4, Hao Ding4, Frank J. \nManion4, Jingcheng Du4, Li Zhou1,2 \n \n1Division of General Internal Medicine and Primary Care, Brigham and Women's Hospital, Boston, \nMassachusetts 02115 \n2Department of Medicine, Harvard Medical School, Boston, Massachusetts 02115 \n3Division of Nephrology, Taichung Veterans General Hospital, Taichung, Taiwan, 407219 \n4Intelligent Medical Objects, Rosemont, Illinois, 60018 \n \n*Corresponding author: \nXinsong Du, Ph.D. \nDivision of General Internal Medicine and Primary Care \nDepartment of Medicine \nBrigham and Women’s Hospital and Harvard Medical School \n399 Revolution Dr, Suite 777 \nSomerville, MA 02145 \nTel: 3522811387 | E-mail: xidu1@bwh.harvard.edu \n \nSummary: We found LLM, traditional machine learning, and deep learning had diverse error profiles on \ncognitive decline identification from clinical notes, and the ensemble of LLM, machine learning, and deep \nlearning achieved state of the art performance. \n \nFunding Source: This study was funded by NIH-NIA R44AG081006.  \n \nConflict of Interest  Statement: All authors have NO affiliations with or involvement in any organization or \nentity with any financial interest (such as honoraria; educational grants; participation in speakers’ bureaus; \nmembership, employment, consultancies, stock ownership, or other equity interest;  and expert testimony or \npatent-licensing arrangements), or non -financial interest (such as personal or professional relationships, \naffiliations, knowledge or beliefs) in the subject matter or materials discussed in this manuscript.  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2 \n \nAbstract \n \nBackground: Early detection of cognitive decline in elderly individuals facilitates clinical trial enrollment and \ntimely medical interventions. This study aims to apply, evaluate, and compare advanced natural language \nprocessing techniques for identifying signs of cognitive decline in clinical notes. \nMethods: This study, conducted at Mass General Brigham (MGB), Boston, MA, included clinical notes from the \n4 years prior to initial mild cognitive impairment (MCI) diagnosis in 2019 for patients ≥ 50 years. Note sections \nregarding cognitive decline were labeled manually. A random sample of 4, 949 note sections  filtered with \ncognitive functions-related keywords were used for  traditional AI model development, and 200 random subset \nwere used for LLM and prompt development;  another random sample of 1996 note sections without keyword \nfiltering were used for testing. Prompt templates for large language models (LLM), Llama 2 on Amazon Web \nService and GPT-4 on Microsoft Azure, were developed with multiple prompting approaches to select the optimal \nLLM-based method. Baseline comparisons were made with XGBoost and a hierarchical attention -based deep \nneural network model. An ensemble of the three models was then constructed using majority vote.  \nResults: GPT-4 demonstrated superior accuracy and efficiency to Llama 2. The ensemble model outperformed \nindividual models, achieving a precision of 90.3%, recall of 94.2%, and F1-score of 92.2%. Notably, the ensemble \nmodel demonstrated a marked improvement in precision (from a 70%-79% range to above 90%) compared to the \nbest performing single model. Error analysis revealed 63 samples were wrongly predicted by at least one model; \nhowever, only 2 cases (3.2%) were mutual errors across all models, indicating diverse error profiles among them. \nConclusion: Our findings indicate that LLMs and traditional models exhibit diverse error profiles. The ensemble \nof LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing \nperformance and improving diagnostic accuracy. \n \nKeywords: Cognitive Dysfunction, Natural Language Processing, Neurobehavioral Manifestations, Electronic \nHealth Records, Early Diagnosis, Alzheimer Disease, Dementia \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n3 \n \n1. Introduction \n \nAlzheimer’s disease (AD) and related dementias (ADRD), the sixth leading cause of death in the US, affect 5.7 \nmillion Americans, with projections estimating an increase to 13 million by 2060.1 The progression of AD/ADRD \nsignificantly diminishes patient quality of life and places substantial emotional and financial strain on families, \ncommunities, and healthcare systems, 2 with care costs anticipated to reach $1.1 trillion by 2050. 3 Existing \ntreatments, which only temporarily alleviate symptoms and decelerate progression ,4 underscore the urgent need \nfor breakthroughs in AD/ADRD therapy. 5 Cognitive decline, characterized by noticeable deficits in cognitive \nfunctions beyond what might be expected from normal aging 6 serves as a key early sign of AD/ADRD, 7 \nemphasizing the necessity of timely detection for slowing disease advancement, facilitating involvement in \nclinical trials, and initiating early interventions.8,9 \n \nElectronic health records (EHRs) provide comprehensive, both real-time and historical, patient data, making them \na critical resource for identifying cognitive decline.  However, challenges arise with structured data like \nInternational Classification of Diseases (ICD) codes, which may lack the granularity to capture the spectrum of \ncognitive impairments, risking clinician under -reporting.10 Moreover, traditional diagnostic tools may fail to \ndetect early stages of cognitive decline, increasing the risk of missed or incorrect diagnoses. 11 The variability in \nscreening practices further complicates early detection during standard medical evaluations. 12 13 Additionally, \nconstraints such as limited access to specialized care providers, including neurologists and geriatricians, \nexacerbate delays in diagnosis and treatment initiation. \nClinical notes within EHRs contain critical information on cognitive decline, detailing symptoms like memory \nloss, language difficulties, and impaired daily activities. Natural language processing (NLP) offers a powerful \ntool to identify these early signs of decline, which may not be coded in diagnoses. NLP facilitates the rapid and \nefficient analysis of large datasets, outpacing manual review. Researchers have investigated NLP techniques for \ndetecting cognitive decline from clinical notes. 14 For example, Penfold et al. utilized Python scripts to identify \nphrases indicative of cognitive function, predicting future mild cognitive impairment (MCI).15 Wang et al. created \nand validated a deep learning model to detect evidence of cognitive decline within EHR clinical notes. 10 \nNevertheless, no study has been done to investigate the effectiveness of large language model (LLM) on cognitive \ndecline identification. \nLLMs like GPT-4 and Llama 2, with their extensive pre-training on diverse text corpora, excel in understanding \ncomplex clinical narratives, offering advantages over traditional rule-based and machine learning approaches that \nare often trained from scratch on narrower clinical data sets. Their deep contextual comprehension enables the \ndetection of subtle indicators of cognitive status within text.16 This research leverages LLMs within secure cloud \ncomputing environments for a pioneering exploration of EHR note analysis for cognitive decline detection. It also \nevaluates LLMs’ effectiveness and interpretability against conventional machine learning methods and examines \nthe synergy of LLMs with machine learning to enhance diagnostic accuracy. To the best of our knowledge, this \ninitiative is the first of its kind to employ LLMs in such a capacity, representing a substantial innovation and \ncontribution to the field. \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n4 \n \n2. Methods \n \n2.1. Setting and Datasets  \n \nThis study was conducted at Mass General Brigham (MGB), a large, integrated health care delivery system in \nMassachusetts. MGB has established secure HIPAA-compliant cloud environments for deploying and evaluating \nLLMs using actual EHR data. Two LLMs were included in this study: GPT -417 via Microsoft Azure OpenAI \nService Application Programming Interface (API) and Llama 2 18 via an Amazon Elastic Compute Cloud  (EC2) \ninstance. Details about the two cloud environments were described in Supplementary Material Section 1 and \nTable S1. We set the temperature to 0 for LLMs to eliminate variability. \n \nWe used two datasets annotated from a previous study, which included sections of clinical notes over the four \nyears leading up to the initial diagnosis of mild cognitive impairment (MCI) (ICD-10-CM code G31.84) in 2019 \nfor patients aged 50 years or older.10 Because the positive case rate across any sections were low,  a list of \nexpert-curated keywords (Table 2) were used to screen sections that likely contain indications of cognitive \ndecline. Dataset I included 4,949 sections filtered by keywords and were used for training two baseline models. \nFor prompt development and LLM selection, we used a 200 random samples from Dataset I, and we name this \ndataset as Dataset I-S. Dataset II, a random sample of 1,996 sections not subjected to keyword filtering, were \nused for final testing. \n  \nThe identification of cognitive decline encompasses various stages, ranging from subjective cognitive decline \n(SCD) to MCI to dementia, with a focus on the progressive nature of cognitive decline that is likely to correspond \nto or result in MCI. Cases considered transient (e.g., memory loss attributed to medication) or reversible were \nlabeled as negative for cognitive decline.10 \n \nThe study was approved by MGB Institutional Review Board with waiver of informed consent from study \nparticipants owing to secondary use of EHR data. \n \n2.2.Prompt Development and LLM Selection \n \nAs illustrated in Figure 1, to optimize LLM, we first selected the best prompt template for each model via manual \ntemplate engineering19, then selected the best prompt  template and model combination, finally, we tested if \nprompt augmentation 19 with few shot and error analysis -based instructions 20 could improve the model \nperformance. Detailed steps for LLM development optimization are illustrated below. \n \nOur task was to identify if a clinical note contains information that could identify the presence of cognitive decline, \nwhich is text classification. To complete the task using large language models, we developed requirements for \nprompts based on our needs, and then iteratively refined the prompts through team discussion. Since our problem \nbelongs to clinical notes binary classification task, we need to measure the classification performance and interpret \nthe result. Our requirements of the prompt include: 1) identify whether the clinical note has evidence showing \ncognitive decline or not; 2) output keywords that helped the LLM to make the judgment; and 3) ask the LLM to \noutput its response to a JSON format, which makes it easier to use code to parse the result. Our internal discussion \nfor the prompt improvement included: 1) to ask the LLM to output its reason for the judgment; 2) to include our \ndefinition of cognitive decline in the prompt. The temperature hyperparameter is used to control randomness and \ncreativity, with 0 corresponding to a deterministic solution. During testing, we set the temperature to 0 to minimize \nrandomness in response generation 20. We categorized LLM responses into three categories as shown in \nSupplementary Table S2: 1) effective and parseable, 2) effective but not parseable, 3) not effective.  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n5 \n \n \nTo develop the optimal prompt and model, we implement a structured approach as depicted in Figure 1. The \nprocess begins with prompt template engineering, where for each template, we manually tested  10 random \nsamples from the training set to evaluate the percentage of effective responses. We then manually adjust ed the \nprompt by paraphrasing or adding/removing optional content, such as definitions or requests for explanation of \ndecisions, for each model. This tuning is stopped once there's no improvement in the effective response rate after \nthree consecutive attempts. Then, for the most effective prompt template identified for each LLM model, we used \nDataset I-S to evaluate the percentage of parseable responses. We then assessed the accuracy, and chose the LLM \nthat demonstrate d superior accuracy.  Next, we explore d prompt augmentation to see if the inclusion of five \nexamples (five-shot prompting) enhances performance. We used Template 6 in Supplementary Table S3 for the \naugmented prompt. Finally, we tested the effectiveness of error analysis-based instruction. \n \n2.3. Baseline Models \n \nWe used XGBoost  21 as the machine learning model, and Convolutional Neural Network + Long -Short Term \nMemory with  Hierarchical Attention as the deep learning model  22,23, since they achieved state -of-the-art \nperformances according to our previous study10. XGBoost stands for Extreme Gradient Boosting and represents \na scalable, distributed machine learning library based on gradient-boosted decision trees (GBDT). Renowned for \nits parallel tree boosting capabilities, it stands as the foremost choice for addressing regression, classification, and \nranking challenges within the machine learning domain 21. A convolutional neural network (CNN) is a deep \nlearning model specifically designed to process structured grid -like data, such as images or sequences 24. Long \nShort-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture designed to overcome the \nvanishing gradient problem in traditional RNNs and effectively capture long -range dependencies in sequential \ndata 25. Connecting a LSTM neural network with an attention layer enhances interpretability by allowing the \nmodel to focus on relevant parts of the input sequence while making predictions 23. For model parameter \noptimization, we used five-fold cross-validation on training data. \n \n2.3.1. Ensemble Model \n \nTo create the ensemble model, we take the majority vote of the LLM, deep learning model, and machine learning \nmodel as the label. The LLM is the best model selected from the above procedures. The deep learning model is \nthe same as our previous study, which  connects LSTM with attention. We employed XGBoost as our machine \nlearning model since it was the best-performing machine learning model based on our previous study 10. \n \n2.3.2. Model Evaluation \n \nMachine learning and deep learning models were trained and optimized using Dataset I. LLM and prompt were \noptimized using a subset (200 sectioned notes) of the 4,949 keyword filtered sections (Dataset I-S) due to budget \nlimit. We tested each AI model on Dataset II, and reported precision, recall, and F1 score. \n \n2.4.Error Analysis and Interpretation \n \nErrors made by each model were analyzed and discussed among two biomedical informatician and a physician. \nFor errors made by LLM, we also read the explanation output from the model. We also quantified unique and \noverlapping errors made by each model using a Venn Diagram. Regarding interpretation, we listed keywords that \nhad a high frequency of appearance from LLM’s output (i.e., the number of appearances is higher than the average \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n6 \n \nappearance time plus two standard deviations); keywords whose deep learning attention weights above the mean \nweights plus 2 standard deviations within individual sections; and keywords with an XGBoost information gain \nhigher than the average value plus 2 s tandard deviations. We also listed expert curated keywords developed in \nour previous study as a reference10. \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n7 \n \n3. Results \n \nDetailed dataset characteristics and model’s optimized hyperparameters are illustrated in the Supplementary \nMaterial. We highlighted model evaluation, interpretation, and error analysis below. \n \n3.1.Performance evaluation \n \nAs illustrated in Table 1 and Figure 2, we tested the Optimized LLM, XGBoost, and deep learning model on the \ntest set. It turned out the LLM achieved a precision of 71. 6%, a recall of 91.3%, and an F1 score of 80.3%. \nOptimized parameters for the deep learning model and XGBoost model are illustrated in Supplementary Table \nS8. The optimized XGBoost model achieved a precision  of 79.0%, recall of 92.8%, and F1 score of 80. 3%; the \noptimized deep learning model obtained a precision of 77.1%, recall of 92.8, and F1 score of 84.2%. Notably, an \nensemble consisting of the LLM, deep learning model, and machine learning model boosted the performance  \n(over 10% precision increase and around 7% F1-score improvement), which achieved a precision of 90.3%, recall \nof 94.2%, an F1 score of 92.2%. \n \n3.2.Interpretation and Error Analysis \n \nTable 2 contains keywords that have been identified through expert curation, exported by XGBoost, the deep \nlearning model, and the LLM. These keywords cover a range of topics including memory -related issues such as \nrecall and forgetfulness, cognitive impairments, and dementia with terms like dementia and Alzheimer, and \nevaluation and assessment methods, referencing tools like MoCA and MMSE. Comparing with traditional AI \nmodels and expert -selected keywords, GPT-4 highlighted specific treatment options, notably \"Aricept\" and  \n\"donepezil,\" which are essential in managing dementia and Alzheimer's disease. GPT-4 also identified specific \ndiagnoses or conditions more explicitly than other models, with terms like \"Mild Neurocognitive Disorder,\" \n\"major neurocognitive disorder,\" and \"vascular dementia.\"  Additionally, GPT-4 exported some  keywords \nregarding emotional and psychological effects of cognitive disorders, such as \"anxiety,\" thus covering aspects \nsometimes overlooked by other models. \n \nAn error analysis performed on LLM with test data  is highlighted in Figure 3. We found all models could be \nmisled by signs or symptoms as being caused by unrelated clinical conditions or be confounded by negations and \nother contextual details. Notably, GPT-4 stood out for its clarity in handling ambiguous terms, a common pitfall \nfor traditional AI models. It shows an adeptness at inferring or emphasizing nuanced information that traditional \nAI methods typically overlook. However,  GPT-4 could infer/amplify the nuanced information while traditional \nAI models did not make such mistake s. Additionally, GPT-4 was sometimes  overly conservative, failing to \nidentify cognitive decline despite compelling evidence. GPT-4 might also miss the underlying reasons for specific \nclinical events, such as visits or treatments related to cognitive decline. Both GPT -4 and deep learning models \nmight misinterpret clinical testing results, marking an area for future improvement. \n \n \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n8 \n \n4. Discussion \n \nEarly detection of precursor stages of AD/ADRD, such as cognitive decline, becomes extremely important, as it \ncan introduce treatment or intervention earlier to effectively reduce progression to AD/ADRD 8,9. In this study, \nwe aimed to develop an NLP model for cognitive decline identification from clinical notes in EHR. We used \ndata collected in our previous study for model development. Our study found that LLM did not outperform \ntraditional AI for cognitive decline identification. This is because LLM was not trained specifically for a task, it \nis a more powerful tool than traditional AI models, and its output can have a wide range of responses, possibly \ngiving ambiguous, inconsistent, and sometimes out-of-context answers16. We found although three types of \nmodels achieved similar performances, they made different types of errors. Notably, the ensemble of the three \ntypes of models boosted the performance (e.g., there was an over 10% precision improvement). Regarding \ninterpretation, we found that LLM could identify keywords that were not identified by experts or the deep \nlearning model, such as words related to medications. As illustrated in Supplementary Table S9, during error \nanalysis, we found LLM could be very helpful to help physicians quickly identifying cognitive-decline related \nmedications from a long medication report. Overall, our developed method can be a valuable tool for screening \nolder patients to identify those with evidence of cognitive decline in clinical notes. \n \nMany existing studies focused on using AI techniques to predict later stage of cognitive decline 7,9,32–34. A few \nstudies were about NLP techniques for detecting cognitive decline. Moreira et al. (2018) 35 used unsupervised \nlearning approach to cluster clinical notes and get pathological features, and then integrated structured EHR data \nfor cognitive decline identification. They achieved a recall of 0.8 and an AUROC of 0.87. Wang et al. (2021) 10 \ndeveloped a deep learning model (i.e., LSTM with attention model) for cognitive decline identification on clinical \nnotes, and achieved a precision of 0.77 precision and a recall of 0.93 . The study also found the attention-based \ndeep learning model was able to identify useful keywords missed by experts. In 2022, Penfold et al. 15 developed \na LASSO model using clinical notes and reported a precision of 0.70 and a recall of 0.17. More recently, \nFouladvand et al. (2023) 36 integrated NLP techniques to identify cognitive decline using both structured and \nunstructured EHR data, and achieved an AUROC of 0.68 meanwhile ranked important predictors from both \nstructured data and unstructured data. Compared with existing studies, our study achieved state -of-the-art \nperformance (precision 0.90, recall 0.94, and f1 score 0.92) meanwhile identified keywords that were missed by \nexperts and traditional AI models. \n \nOur study has several strengths. Firstly, we set up private computational environments and tested two LLMs using \ncloud platforms. We noticed that Microsoft Azure GPT -4 outperformed AWS EC2 Llama 2 in terms of price, \nexecution speed, and accuracy. However, as an open-source model, Llama 2 may have a better reproducibility 37 \nwhile GPT-4 may provide slightly different answers over time due to model updates from OpenAI 38. Secondly, \nwe tested several prompting strategies, and found doing error analysis using some training data, and then asking \nLLM to avoid summarized common errors by revising the prompt can be an effective way to improve LLM’s \nperformance. Additionally, we developed a prompt template for identifying cognitive decline from clinical note \nusing LLM, and the template can also be used as a good reference when developing prompts for identifying other \ndiseases. To the best of our knowledge, this is the first study  employing LLM and unstructured EHR data for \ncognitive decline detection. \n \nHowever, the results of this study should be considered along with some limitations. We used data from our \nprevious study, which has several limitations including: 1) the data is from a single healthcare system, thus, \nexternal validation is still needed; 2 ) the data is retrospective and include notes written four years preceding the \ndiagnose, but detecting cognitive decline beyond the 4 -year window is still needed; 3) the data is unimodal, \nintegrating other types of data (e.g., image, genomics) may further improve the performance. Additionally, \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n9 \n \nconsidering the budget issue, the Llama 2 model used in our study is not the best one, which contains 70 billion \nparameters and requires more expensive hardware. Furthermore, our data is record -based and not patient based \n(I.e., longitudinal), thus, the developed model may not be able to distin guish well between reversable and \nprogressive cognitive decline, since we do not know if the patient recovered later or not based only on a note \nsection at one time point. An LLM-based early warning system of cognitive decline developed with longitudinal \ndata would be a valuable future work. \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n10 \n \n5. Conclusion \n \nThis study is among the initial endeavors to utilize LLMs within HIPAA -compliant cloud environments, \nleveraging real EHR notes for the detection of cognitive decline. Our findings indicate that LLMs and traditional \nmodels exhibit diverse error profiles in comparison. The ensemble of LLMs and locally trained machine learning \nmodels on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy. \nFuture work can leverage longitudinal and multimodal data to further improve performance. \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n11 \n \nTables \n \nTable 1. Performance evaluation of the best LLM and prompting combination and machine learning models. \nXGBoost is the best performing individual model, which achieved a precision of 79.01%, recall of 92.75%, \nand f1 score of 85.33%. The ensemble model boosted the best performance to precision - 90.28% (11.27% \nhigher than XGBoost), recall – 94.20% (1.45% higher than XGBoost), f1 score – 92.20 (6.87% higher than \nXGBoost). \nModel Prompt Prefix Text \nVersion \nPrecision Recall F1 Score GPT-4 \nExecution Date \nGPT4-8K 0 shot improved with \nerror analysis \n71.59% 91.30% 80.25% 01/03/2024 \nDeep \nLearning \nNA 77.11% 92.75% 84.21% - \nXGBoost NA 79.01% 92.75% 85.33% - \nEnsemble \n(vote of above \n3 rows) \n0 shot improved with \nerror analysis \n90.28% 94.20% 92.20% 01/03/2024 \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n12 \n \n \nTable 2. Keywords contributing to identifying positive cognitive decline cases extracted from experts, deep \nlearning model, and large language model. The table listed keywords that had a high frequency of appearance \nfrom LLM’s output (i.e., the number of appearances is higher than the average appearance time plus two \nstandard deviations); keywords whose deep learning attention weights above the mean weights plus 2 \nstandard deviations within individual sections; and keywords with an XGBoost information gain higher than \nthe average value plus 2 standard deviations. We found keywords identified by AI models could significantly \nenrich the expert curated keyword set. Notably, only LLM identified keywords regarding medications for \ncognitive decline. \nModel Keywords \nExpert Curated Memory, delirium, dementia, psycha, neuroa, mental, alzheimer, confusa, mood, cognita, \nforgeta, agitate, moca, montreal, mmse, remember, difficult, recall, function, word, \nevaluata, score, drive, attention, mild, impairment, speech, question, disorientation, \norientation, sleep, alter, exam, decline, worse, loss \nXGBoost Cognitive, memory, dementia, forgetful \nDeep Learning \nMemory, cognitive, dementia, impairment, neurocognitive, recall, decline, word, forgetful, \ncognition, MCI, executive, forgetfulness, alzheimer, remembering, MoCA, attention, \ndeficits, recalling, forgetting, finding, words, visual, forgets, naming, difficulties, fluency, \ndelay, alzheimers, retrieval, visuospatial, repetition, remember, hearing, cog, trails, \nlanguage, FTD, frailty, encoding, developmental, delayed, behavioral, amnestic, phonemic, \nMMSE, falls, errors, attentional, speech, span, processing, neurodegenerative, lapses, \nHOH, deficit, correctly, auditory, years, spatial, solving, semantic, personality, \nperseveration, names, multidomain, moderately, linguistic, learning, LBD, items, insight, \nimpaired, immediate, global, functioning, functional, expressive, died, cube, \ncomprehension, clock, challenges, category, BNT, aphasia, amyloid, age, aforementioned, \nabstraction \nGPT4-8K \ndementia, cognitive impairment, memory loss, memory, mild cognitive impairment, \nneuropsychological evaluation, confusion, confused, Memory loss, forgetful, deficits, \ncognitive decline, Mild cognitive impairment, Cognitive impairment, Dementia, cognitive \nchanges, Aricept, current level of cognitive functioning, memory impairment, Mild \nNeurocognitive Disorder, executive functioning, memory issues, attention, cognitive \ndeficits, Memory impairment, MCI, neuropsychological tests, cognition, poor safety \nawareness, forgetfulness, Cognitive decline, cognitive difficulties, MOCA, memory \nproblems, mild neurocognitive disorder, donepezil, neuropsychological testing, Impaired, \nmemory concerns, major neurocognitive disorder, cognitive concerns, mild dementia, \nweakness, memory difficulties, working memory, altered mental status, concerns, MCI \n(mild cognitive impairment), neurodegenerative process, language, cognitive symptoms, \nneurocognitive status, Altered mental status, short term memory loss, cognitive issues, \nneurocognitive disorder, executive function, battery of neuropsychological tests, \nneuropsych testing, problem solving, verbal fluency, memory complaints, vascular \ndementia, word-finding difficulties, processing speed, cognitive-linguistic therapy, \ndelirium, word finding difficulties, delayed recall, anxiety \n \n \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n13 \n \nFigures \n \n \n  \nFigure 1. Study design overview. The workflow consists of five parts. A) Manual template engineering: we fed \neach prompt to GPT-4 and Llama 2 separately. If the effective response (i.e., the response answered the \nquestions in the prompt) rate was not 100%, then manually adjust the template. If the effective response rate \ndoes not improve after tuning three times, then stop tuning and use the template leading to the best effective \nresponse rate. B) LLM selection: We used Dataset I-S and tested the accuracy of results produced by GPT-4 + \nits selected template and Llama 2 + its selected template. We used the best performing model and template \nversion combination for following experiments. C) Prompt augmentation: we tested if five-shot prompting \ncould improve the accuracy. D) Error analysis-based instructions: we tested if adding instructions following \nerror analysis of GPT-4’s output on the Dataset I-S could improve the accuracy. E) Model evaluation: we \nevaluated the developed deep learning model, large language model, and machine learning model. We also \ntested the performance of ensemble model, which took the majority vote of the three models as the predictive \nlabel. F) Error analysis and interpretation: we exported keywords used by each model for the prediction, and \ncompared the keywords with expert curated keywords, then we analyzed and compared errors made by each \nmodel. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n14 \n \nFigure 2. Evaluation results summary. During the prompt template selection, Template 1 was selected for GPT-\n4 model due to an 100% effective response rate; Template 2 was selected for Llama 2 model since the effective \nresponse rate (80%) was not improved after tuning it for three times. Then, we compared the two combinations \nwith 200 samples (Dataset I-S), and found GPT-4 + Template 1 had a much better accuracy (86.43%). Next, we \nfound five shot prompting did not lead to a better performance, but adding error analysis-based instructions (i.e., \nGPT-4 + Template 7) could improve the accuracy to 93% on the Dataset I-S. Therefore, we selected using \nTemplate 7 as the prompt template, and GPT-4 as the LLM. On the test set, we evaluated the performances of \ndeveloped machine learning model (XGBoost), developed deep learning model, and developed LLM. We found \nthe XGBoost had a better performance: precision – 79.01%, recall – 92.75%, f1 score – 85.33%. Notably, after \nensemble the three models using majority vote, we found the ensemble model had a much better performance: \nprecision – 90.11% (11.1% improvement), recall – 94.20% (1.45% improvement), and f1 score – 92.20% \n(6.87% improvement). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n15 \n \n  \nFigure 3. Venn Diagram highlighting unique and overlapping mistakes made by different \nmodels.  √: correct prediction X: incorrect prediction. Some important findings include: 1) GPT-\n4 never predicts positive for cognitive decline if there is ambiguous information seemingly \nrelevant to cognitive decline but not, while traditional AI approaches could make mistakes due \nto ambiguous terms; 2) GPT-4 could amplify the nuanced information traditional AI approaches, \nor be too rigorous, but traditional AI approaches do not have this issue; 3) all types of models \ncould be misled by seemingly relevant symptoms/signs that normal people can have or very \nlikely caused by signs/symptoms caused by other factors or clinical conditions. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n16 \n \nReferences \n \n1. 2023 Alzheimer’s disease facts and figures. Alzheimers Dement J Alzheimers Assoc. 2023;19(4):1598-1695. \ndoi:10.1002/alz.13016 \n2. Vu M, Mangal R, Stead T, Lopez-Ortiz C, Ganti L. Impact of Alzheimer’s Disease on Caregivers in the \nUnited States. Health Psychol Res. 10(3):37454. doi:10.52965/001c.37454 \n3. Stefanacci RG. The costs of Alzheimer’s disease and the value of effective therapies. Am J Manag Care. \n2011;17 Suppl 13:S356-362. \n4. Aducanumab to Be Discontinued as an Alzheimer’s Treatment. Alzheimer’s Disease and Dementia. \nAccessed March 9, 2024. https://alz.org/alzheimers-dementia/treatments/aducanumab \n5. Cummings J, Zhou Y, Lee G, Zhong K, Fonseca J, Cheng F. Alzheimer’s disease drug development pipeline: \n2023. Alzheimers Dement N Y N. 2023;9(2):e12385. doi:10.1002/trc2.12385 \n6. Mitchell AJ, Beaumont H, Ferguson D, Yadegarfar M, Stubbs B. Risk of dementia and mild cognitive \nimpairment in older people with subjective memory complaints: meta-analysis. Acta Psychiatr Scand. \n2014;130(6):439-451. doi:10.1111/acps.12336 \n7. Kidd PM. Alzheimer’s disease, amnestic mild cognitive impairment, and age-associated memory \nimpairment: current understanding and progress toward integrative prevention. Altern Med Rev J Clin Ther. \n2008;13(2):85-115. \n8. Leifer BP. Early Diagnosis of Alzheimer’s Disease: Clinical and Economic Benefits. J Am Geriatr Soc. \n2003;51(5s2):S281-S288. doi:10.1046/j.1532-5415.5153.x \n9. Veitch DP, Weiner MW, Aisen PS, et al. Understanding disease progression and improving Alzheimer’s \ndisease clinical trials: Recent highlights from the Alzheimer’s Disease Neuroimaging Initiative. Alzheimers \nDement. 2019;15(1):106-152. doi:10.1016/j.jalz.2018.08.005 \n10. Wang L, Laurentiev J, Yang J, et al. Development and Validation of a Deep Learning Model for Earlier \nDetection of Cognitive Decline From Clinical Notes in Electronic Health Records. JAMA Netw Open. \n2021;4(11):e2135174. doi:10.1001/jamanetworkopen.2021.35174 \n11. He Z, Dieciuc M, Carr D, et al. New opportunities for the early detection and treatment of cognitive \ndecline: adherence challenges and the promise of smart and person-centered technologies. BMC Digit \nHealth. 2023;1(1):7. doi:10.1186/s44247-023-00008-1 \n12. Sabbagh MN, Boada M, Borson S, et al. Early Detection of Mild Cognitive Impairment (MCI) in \nPrimary Care. J Prev Alzheimers Dis. 2020;7(3):165-170. doi:10.14283/jpad.2020.21 \n13. Whelan R, Barbey FM, Cominetti MR, Gillan CM, Rosická AM. Developments in scalable strategies \nfor detecting early markers of cognitive decline. Transl Psychiatry. 2022;12(1):1-11. doi:10.1038/s41398-\n022-02237-w \n14. Cummings J, Lee G, Nahed P, et al. Alzheimer’s disease drug development pipeline: 2022. Alzheimers \nDement N Y N. 2022;8(1):e12295. doi:10.1002/trc2.12295 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n17 \n \n15. Penfold RB, Carrell DS, Cronkite DJ, et al. Development of a machine learning model to predict mild \ncognitive impairment using natural language processing in the absence of screening. BMC Med Inform Decis \nMak. 2022;22(1):129. doi:10.1186/s12911-022-01864-z \n16. Caruccio L, Cirillo S, Polese G, Solimando G, Sundaramurthy S, Tortora G. Can ChatGPT provide \nintelligent diagnoses? A comparative study between predictive models and ChatGPT to define a new medical \ndiagnostic bot. Expert Syst Appl. 2024;235:121186. doi:10.1016/j.eswa.2023.121186 \n17. OpenAI, Achiam J, Adler S, et al. GPT-4 Technical Report. Published online December 18, 2023. \ndoi:10.48550/arXiv.2303.08774 \n18. Touvron H, Martin L, Stone K, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. \narXiv.org. Published July 18, 2023. Accessed January 17, 2024. https://arxiv.org/abs/2307.09288v2 \n19. Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre-train, Prompt, and Predict: A Systematic \nSurvey of Prompting Methods in Natural Language Processing. ACM Comput Surv. 2023;55(9):195:1-\n195:35. doi:10.1145/3560815 \n20. Hu Y, Chen Q, Du J, et al. Improving large language models for clinical named entity recognition via \nprompt engineering. J Am Med Inform Assoc. Published online January 27, 2024:ocad259. \ndoi:10.1093/jamia/ocad259 \n21. Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System. In: Proceedings of the 22nd ACM \nSIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ’16. Association for \nComputing Machinery; 2016:785-794. doi:10.1145/2939672.2939785 \n22. Yang J, Wang L, Phadke NA, et al. Development and Validation of a Deep Learning Model for \nDetection of Allergic Reactions Using Safety Event Reports Across Hospitals. JAMA Netw Open. \n2020;3(11):e2022836. doi:10.1001/jamanetworkopen.2020.22836 \n23. Yang Z, Yang D, Dyer C, He X, Smola A, Hovy E. Hierarchical Attention Networks for Document \nClassification. In: Knight K, Nenkova A, Rambow O, eds. Proceedings of the 2016 Conference of the North \nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies. \nAssociation for Computational Linguistics; 2016:1480-1489. doi:10.18653/v1/N16-1174 \n24. Lecun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. \nProc IEEE. 1998;86(11):2278-2324. doi:10.1109/5.726791 \n25. Hochreiter S, Schmidhuber J. Long Short-Term Memory. Neural Comput. 1997;9(8):1735-1780. \ndoi:10.1162/neco.1997.9.8.1735 \n26. Temsah O, Khan SA, Chaiah Y, et al. Overview of Early ChatGPT’s Presence in Medical Literature: \nInsights From a Hybrid Literature Review by ChatGPT and Human Experts. Cureus. 2023;15(4):e37281. \ndoi:10.7759/cureus.37281 \n27. Ahmad N, Murugesan S, Kshetri N. Generative Artificial Intelligence and the Education Sector. \nComputer. 2023;56(6):72-76. doi:10.1109/MC.2023.3263576 \n28. Leippold M. Thus Spoke GPT-3: Interviewing a Large-Language Model on Climate Finance. Published \nonline September 12, 2022. doi:10.2139/ssrn.4237242 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint \n18 \n \n29. Koga S, Martin NB, Dickson DW. Evaluating the performance of large language models: ChatGPT and \nGoogle Bard in generating differential diagnoses in clinicopathological conferences of neurodegenerative \ndisorders. Brain Pathol Zurich Switz. Published online August 8, 2023:e13207. doi:10.1111/bpa.13207 \n30. Pillai J, Pillai K. Accuracy of generative artificial intelligence models in differential diagnoses of \nfamilial Mediterranean fever and deficiency of Interleukin-1 receptor antagonist. J Transl Autoimmun. \n2023;7:100213. doi:10.1016/j.jtauto.2023.100213 \n31. Sensoy E, Citirik M. A comparative study on the knowledge levels of artificial intelligence programs in \ndiagnosing ophthalmic pathologies and intraocular tumors evaluated their superiority and potential utility. Int \nOphthalmol. 2023;43(12):4905-4909. doi:10.1007/s10792-023-02893-x \n32. Adelson RP, Garikipati A, Maharjan J, et al. Machine Learning Approach for Improved Longitudinal \nPrediction of Progression from Mild Cognitive Impairment to Alzheimer’s Disease. Diagnostics. \n2024;14(1):13. doi:10.3390/diagnostics14010013 \n33. Fouladvand S, Noshad M, Periyakoil VJ, Chen JH. Machine learning prediction of mild cognitive \nimpairment and its progression to Alzheimer’s disease. Health Sci Rep. 2023;6(10):e1438. \ndoi:10.1002/hsr2.1438 \n34. James C, Ranson JM, Everson R, Llewellyn DJ. Performance of Machine Learning Algorithms for \nPredicting Progression to Dementia in Memory Clinic Patients. JAMA Netw Open. 2021;4(12):e2136553. \ndoi:10.1001/jamanetworkopen.2021.36553 \n35. Moreira LB, Namen AA. A hybrid data mining model for diagnosis of patients with clinical suspicion of \ndementia. Comput Methods Programs Biomed. 2018;165:139-149. doi:10.1016/j.cmpb.2018.08.016 \n36. Fouladvand S, Noshad M, Goldstein MK, Periyakoil V, Chen JH. Mild Cognitive Impairment: Data-\nDriven Prediction, Risk Factors, and Workup. AMIA Summits Transl Sci Proc. 2023;2023:167-175. \n37. DeWitt PE, Rebull MA, Bennett TD. Open source and reproducible and inexpensive infrastructure for \ndata challenges and education. Sci Data. 2024;11(1):8. doi:10.1038/s41597-023-02854-0 \n38. Chen L, Zaharia M, Zou J. How is ChatGPT’s behavior changing over time? Published online October \n31, 2023. doi:10.48550/arXiv.2307.09009 \n39. Nori H, Lee YT, Zhang S, et al. Can Generalist Foundation Models Outcompete Special-Purpose \nTuning? Case Study in Medicine. Published online November 27, 2023. doi:10.48550/arXiv.2311.16452 \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 5, 2024. ; https://doi.org/10.1101/2024.04.03.24305298doi: medRxiv preprint "
}