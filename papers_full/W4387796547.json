{
  "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
  "url": "https://openalex.org/W4387796547",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4287173108",
      "name": "Li, Huao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3189606627",
      "name": "Chong Yu Quan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750719990",
      "name": "Stepputtis, Simon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2319682789",
      "name": "Campbell, Joseph",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227876268",
      "name": "Hughes, Dana",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158450151",
      "name": "Lewis, Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2269091922",
      "name": "Sycara, Katia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4319323461",
    "https://openalex.org/W4367189299",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W3163271954",
    "https://openalex.org/W2594035753",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W4385571775",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4287703064",
    "https://openalex.org/W2626637010",
    "https://openalex.org/W4366999541",
    "https://openalex.org/W2119061043",
    "https://openalex.org/W4298112416",
    "https://openalex.org/W3204828078",
    "https://openalex.org/W4383180654",
    "https://openalex.org/W2010048121",
    "https://openalex.org/W2165698048",
    "https://openalex.org/W4361020574",
    "https://openalex.org/W4286748781",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W2134787668",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4312258073",
    "https://openalex.org/W4320559489",
    "https://openalex.org/W4288594419",
    "https://openalex.org/W4317463334",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2146734084",
    "https://openalex.org/W4214500342",
    "https://openalex.org/W4226399820"
  ],
  "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
  "full_text": "Theory of Mind for Multi-Agent Collaboration via Large Language Models\nHuao Li1, Yu Quan Chong2, Simon Stepputtis2, Joseph Campbell2,\nDana Hughes2, Michael Lewis1, Katia Sycara2\n1 University of Pittsburgh, Pittsburgh, PA\nhul52,cmlewis@pitt.edu\n2 Carnegie Mellon University, Pittsburgh, PA\nyuquanc,sstepput,jacampbe,danahugh,sycara@andrew.cmu.edu\nAbstract\nWhile Large Language Models (LLMs) have\ndemonstrated impressive accomplishments in\nboth reasoning and planning, their abilities\nin multi-agent collaborations remains largely\nunexplored. This study evaluates LLM-\nbased agents in a multi-agent cooperative text\ngame with Theory of Mind (ToM) inference\ntasks, comparing their performance with Multi-\nAgent Reinforcement Learning (MARL) and\nplanning-based baselines.1 We observed evi-\ndence of emergent collaborative behaviors and\nhigh-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limi-\ntations in LLM-based agents’ planning opti-\nmization due to systematic failures in managing\nlong-horizon contexts and hallucination about\nthe task state. We explore the use of explicit\nbelief state representations to mitigate these is-\nsues, finding that it enhances task performance\nand the accuracy of ToM inferences for LLM-\nbased agents.\n1 Introduction\nRecent large language models (LLMs), such as\nGPT-4 (OpenAI, 2023), have demonstrated im-\npressive competencies across a wide array of do-\nmains and tasks, ranging from mathematics to law,\nwithout the need for fine-tuning or special prompt-\ning (Bubeck et al., 2023). This advancement has\nsignificantly transformed the landscape of Natu-\nral Language Processing (NLP) research. Instead\nof developing domain-specific models for down-\nstream applications, focus has shifted towards eval-\nuating and harnessing LLMs’ abilities to solve\nnovel tasks. Such a shift is consistent with the\nidea of studying machine behaviors, an interdis-\nciplinary approach that expands the conventional\nbounds of computer science and integrates insights\nfrom diverse scientific fields (Rahwan et al., 2019).\nDrawing inspiration from team science and group\n1Code available at https://github.com/romanlee6/\nmulti_LLM_comm\npsychology (Hagendorff, 2023), our study concen-\ntrates on collective machine behavior, evaluating\nLLMs’ proficiency in multi-agent collaborations.\nThere is ongoing debate regarding the intelli-\ngence levels of modern LLMs. While some argue\nthat LLMs excel primarily in linguistic competence\nand struggle with cognitive abilities beyond lan-\nguage, known as functional competence, others\ndemonstrate that LLMs can exhibit cognitive skills\nsuch as formal reasoning and world knowledge\ncomprehension (Mahowald et al., 2023; Bubeck\net al., 2023). Motivated to explore this argument,\nwe designed a text-based game to evaluate LLMs’\nability in embodied interactions, including explor-\ning unknown environments, maintaining beliefs\nabout the world and collaborating with other agents,\nwhich is critical for natural social interactions and\nartificial general intelligence (AGI).\nTheory of Mind, the capacity to reason about\nothers’ concealed mental states, is fundamental\nto human social interactions, collaborations, and\ncommunications (Zhang et al., 2012). As LLMs\nincreasingly participate in diverse social interac-\ntions with humans, their social intelligence is ex-\npected to improve for them to become effective col-\nlaborators (Williams et al., 2022; Li et al., 2022).\nFor instance, a proficient AI assistant should be\nable to infer a human’s preferences based on pre-\nvious experiences without needing to ask. Recent\nstudies have applied classic Theory-of-Mind tasks\nto several LLMs, concluding that current mod-\nels (e.g., GPT-4) perform comparably to 9-year-\nold children (Kosinski, 2023). However, the re-\nsearch community has expressed doubts about the\nvalidity of text-based ToM tests on machine intelli-\ngence(Ullman, 2023; Sap et al., 2023). In response,\nour study proposes a novel evaluation of LLMs’\nhigh-order ToM in interactive teamwork scenar-\nios, encompassing dynamic belief state evolution\nand rich intent communication between multiple\nagents.\narXiv:2310.10701v3  [cs.CL]  26 Jun 2024\nThe main contributions of this study include that\nwe:\n• Evaluate LLM-based agents’ embodied in-\nteraction capability in multi-agent collabora-\ntive tasks against reinforcement learning and\nplanning-based baselines\n• Identify systematic failures that limit the col-\nlaboration efficiency of LLM-based agents,\nand propose a prompt-engineering method to\nmitigate those failures by incorporating ex-\nplicit belief state representations about world\nknowledge in the model input\n• Propose a novel evaluation of LLMs’ high-\norder ToM in interactive teamwork scenarios,\nencompassing dynamic belief state evolution\nand rich intent communication between multi-\nple agents\n2 Related Work\n2.1 Large language models\nLarge language models, trained on vast text\ncorpora, excel in text completion and various\nother Natural Language Processing (NLP) appli-\ncations (Chowdhery et al., 2022; Thoppilan et al.,\n2022). Recent studies highlight their abilities for\nreasoning (Bubeck et al., 2023; Wei et al., 2022)\nand action plan generation (Liu et al., 2023; Xie\net al., 2023), particularly when utilizing prompt en-\ngineering techniques like chain-of-thought. How-\never, some researchers note these models’ limita-\ntions in forming actionable plans when interacting\nwith real-world objects (Ahn et al., 2022; Huang\net al., 2022). GPT-4’s capacity for embodied inter-\nactions via text-based games and real-world prob-\nlems was assessed by Bubeck et al. (2023). Further\nstudies explored the potential of LLM-powered em-\nbodied agents in Minecraft (Wang et al., 2023b,a).\nThese investigations suggest that LLMs can per-\nform tasks requiring environment understanding,\ntask comprehension, action planning, feedback in-\nterpretation, and subsequent adaptation. Our study\nseeks to broaden this understanding by evaluat-\ning LLMs’ planning abilities in cooperative multi-\nagent scenarios.\n2.2 Theory of Mind\nPrior research has tested LLMs’ Theory of Mind\n(ToM) via variants of text-based tests such as the\nunexpected transfer task (also known as Smarties\nTask) or unexpected contents task (also known as\nthe “Maxi Task” or “Sally–Anne” Test) (Kosinski,\n2023; Moghaddam and Honey, 2023). Results indi-\ncate that leading LLMs can pass more than 90% of\nthese test cases. In contrast, Ullman (2023) found\nthat LLMs struggle with complex ToM inferences\ninvolving communication or second-order beliefs.\nIn our study, ToM evaluations occur in the midst of\nan interactive team task, where the mental states of\nagents change dynamically with each interaction.\nAs agents exchange information through commu-\nnication at every timestamp, the complexity of rea-\nsoning increases, since agents’ mental states may\nbe updated through both observations and commu-\nnication. Thus, our tests can be considered more\nchallenging than the static text-based tests used in\nprior research.\nTheory of Mind (ToM) has been employed to en-\nhance the performance of artificial agents in various\ncontexts. Lim et al. (2020) introduced a method to\nintegrate Bayesian Theory of Mind (BToM) (Baker\net al., 2017) with optimal-planning agents in a co-\noperative game. The results indicate that an explicit\nrepresentation of others’ intentions enhances the\nperformance of both agent-only and human-agent\nteams. SymbolicToM allows language models to\nmaintain an explicit symbolic ToM for multiple\ncharacters in reading comprehension tasks using\ngraphical representations (Sclar et al., 2023). More-\nover, there is a significant body of research focusing\non the application of ToM to boost collaboration in\nmulti-agent reinforcement learning (Oguntola et al.,\n2023; Yuan et al., 2021). Inspired by these prior\nstudies, we aim to enhance LLM-based agents’ col-\nlaborative behaviors through explicit belief repre-\nsentations.\n2.3 Multi-agent collaboration\nTeam science researchers have studied human col-\nlaborative behaviors for decades, covering topics\nsuch as leadership, communication, team dynam-\nics, team cohesion, and shared situation aware-\nness (Riedl et al., 2021). However, the transfer-\nability of these findings to hybrid human-agent\nteams or fully automated teams remains largely un-\nexplored. Park et al. (2023) utilized ChatGPT to\noperate a sandbox environment populated by gener-\native agents, observing emergent social behaviors\namong LLM-based agents. That study primarily fo-\ncused on the feasibility of running such a sandbox\nenvironment with LLMs, rather than specifically\non the collaborative behaviors of machine intelli-\ngence.\n3 Multi-agent Collaboration Tasks\nTo evaluate the capability of LLM-based embod-\nied agents, we design a multi-agent environment\nto simulate the collaborative and problem-solving\ndynamics of a search and rescue mission.\n3.1 Task environment\n3 agents (i.e. Alpha, Bravo, and Charlie) emulate\nspecialists in a team, with the objective to locate\nand safely defuse color-coded bombs scattered in\nan unexplored environment. Each bomb exhibits\nunique phase sequences in m colors, requiring the\ncorrect order of wire cutters for defusing. Team\nmembers start with different colored cutters and\nmust coordinate and synchronize efforts for ef-\nficiency. The environment is conceptualized as\na connected graph, with n nodes representing n\nrooms linked by several edges symbolizing hall-\nways. In each round, the agents can choose from\nthree classes of actions: moving to one of the n\nrooms, inspecting a bomb’s phase sequence in the\ncurrent room, or using one of the m wire-cutters.\nThe size of action space depends on the problem\nscale (i.e. n + m + 1). Agents’ observation are\nlimited to their current room’s contents and agent\nstatus. They are updated periodically about team\nscores, current room contents, teammates’ loca-\ntions and available tools. The team is rewarded\n10*x points when a x-phase bomb is successfully\ndefused.\nThe evaluation environment comprises five\nrooms ( n = 5) and five bombs, including two\nsingle-phase, two double-phase, and one triple-\nphase bombs. Bomb stages might have three dif-\nferent colors (m = 3). Each successfully defused\nbomb awards the team 10 points per processed\nphase, resulting in 90 as the maximum score per\nmission. Team performance is measured using two\nmetrics: the team score, indicating coordination\nquality, and rounds to completion, measuring col-\nlaboration efficiency. A trial concludes when the\nteam has defused all bombs, exceeded the time\nlimit (i.e., 30 rounds), or entered a deadlock by\nrepeating outputs.\n3.2 Text game interface\nThe initial task environment is implemented for\nMARL agents based on gym API (Brockman et al.,\n2016). To facilitate interaction between LLM-\nbased agents with the environment, we’ve inte-\ngrated the task environment with a text interface.\nAt each round (i.e. timestamp), the team’s three\nagents sequentially interact with the environment,\nboth receiving observations and performing ac-\ntions via natural language interaction. A built-in\ncommunication mechanism enables text message\nexchange among agents per round. Importantly,\nagents remain oblivious to each other’s actions and\noutcomes unless communicated, facilitating The-\nory of Mind inference opportunities.\nSpecifically, a rule-based text interface translates\nobservations into natural language descriptions and\nencodes agent chats into abstract action selections.\nFor observations, the text interface extracts state\nfeatures from the game engine and replaces key-\nwords in the templates. A typical description text\nincludes the current round number, cumulative\nteam score, action feedback, contents of the current\nroom, teammates’ locations, and communication\nmessages. Action encoding is done via keyword\nmatching since LLMs are instructed to frame their\nresponses in a certain format and structure. Should\nan agent produce unintelligible content, such as\ninvalid actions or nonsensical text, the interface\nprovides feedback for error correction. The error\nmessages are generated based on pre-programmed\nrules and templates, such as \"There is no bomb in\nthe current location, Room X, for you to inspect.\".\nFig. 1 showcases sample interactions between the\nagent team and task environment via the text inter-\nface.\n4 LLM-based Embodied Agents\nWe chose to evaluate OpenAI’s latest chat comple-\ntion models, namely gpt-3.5-turbo-0301 and gpt-4-\n0314, owing to their impressive performance in var-\nious benchmarks (Zheng et al., 2023). These mod-\nels are prompted to engage in a text-based game,\nwith user inputs managed by the above-mentioned\ngame interface. The LLMs functions as embod-\nied agents interacting within the task environment.\nThey are provided with the game’s rules as context.\nFor each round, the model is asked to choose ac-\ntions and communicate messages, based on the cur-\nrent task state observations and past interaction his-\ntory. Interaction history between the LLM-based\nagent and text game interface are maintained in the\nquery text until it exceeds the maximum model in-\nput size. In our setup, all agents retain memory of\nAction selection: Inspect Bomb. \nMessage to Team: \"I am inspecting the bomb in \nRoom 0. Bravo; please move to Room 3. Charlie; \nplease move to Room 5.\"\nRound: 2  Score: 0\nResults: You inspected Bomb 1. This bomb is a \n1-stage bomb and its remaining sequence is Red.\nCommunication Messages: \nBravo: \"Moving to Room 3 as suggested. Alpha; \nyou can defuse the bomb in Room 0 with your red \ntool.\"\n……\nAlpha\nEnvironment\nBelief state\n……\nBomb Intel: Bomb 1 locates in \nRoom 0 with a sequence of R. \n……\nLLM Agent Text Game Interface Task Environment\nAlpha\nBravo Charlie\nUpdate Prompt\nUpdate your belief state based \non the observation.\n……\nFigure 1: Our proposed framework consist of 3 LLM-based agents, a text game interface and the actual task\nenvironment. The natural language outputs of LLM are encoded into abstract actions by the text interface and then\nsent to task environment. The task environment then processes agent actions and returns observations via the text\ninterface. Upon receiving environmental observations, LLM-based agents are prompted to update their beliefs and\noutput action selections and messages. 3 agents in the team are coded as Alpha, Bravo, and Charlie and take turns to\ninteract with the interface.\nthe game rules and history from the previous two\nrounds, amounting to 4096 tokens.\n4.1 Multi-agent communication\nGiven the collaborative nature of the task scenarios,\ninter-agent communication is crucial for achieving\neffective coordination and teamwork. We imple-\nmented a communication channel enabling LLM-\nbased agents to share textual messages within the\nteam. Messages, once sent, are immediately broad-\ncast to all team members and reflected in their sub-\nsequent observations. For instance, as depicted in\nFig. 1, agent Alpha dispatched messages instruct-\ning teammates to separate, followed by feedback\nfrom agent Bravo. In practice, since agents al-\nternate in message sending, responses from team-\nmates will appear in the observations of the suc-\nceeding round.\n4.2 Belief state\nDue to the model input size limitation, LLM-based\nagents cannot retain the entire interaction history,\nyet task dynamics require the team to track key\nlong-term information, such as room contents and\nbomb sequences. To augment the agents’ infor-\nmation retention and enhance collaboration, we\npropose a method of prompt engineering to repre-\nsent explicit belief states. As illustrated in Fig. 1,\nupon receiving environmental observations, agents\nare prompted to update a textual description storing\nkey task-related beliefs. This updated belief state\nis preserved in the interaction history and used in\nsubsequent action planning. For instance, after in-\nspecting bomb 1, agent Alpha updated its belief\nstate about the bomb’s sequence from unknown to\nred, retaining this information until further updates.\nThe proposed belief state is inspired by the idea\nof chain-of-thought prompting (Wei et al., 2022),\nwherein a complex reasoning task is broken down\ninto intermediate steps and introduced to the LLM\nin a few-shot learning manner. Notably, although\nan initial belief state description is provided to il-\nlustrate the proper format and representations, the\nupdate rules are entirely zero-shot, relying solely\non the LLM’s common sense and mission context.\n5 Experiments\nWe systematically ablate LLM-based embodied\nagents and evaluate them in a collaborative task\nin teams of three. Two modules are manipulated in-\ncluding LLM models (i.e. GPT-4 or ChatGPT) and\nbelief representation (i.e. with or without belief\nstate) resulting in a total of 4 experimental condi-\ntions.\nAgents Score Rounds to Completion Valid action %\nChatGPT 43 ± 4.7 30.0 ± 0.0 62.5%\nGPT-4 90 ± 0.0 28.3 ± 2.6 71.8%\nGPT-4 + Belief 90 ± 0.0 12.3 ± 2.0 86.1%\nMAPPO 90 ± 0.0 11.0 ± 0.0 N/A\nCBS Planner 90 ± 0.0 6.0 ± 0.0 N/A\nRandom 38 ± 14.7 30.0 ± 0.0 N/A\nTable 1: Task performance of LLM-based agents and baseline conditions. Score represent the average team score in\nall experiment trials. Length refers the average number of rounds the team took in completing the task. Percentages\nof valid action measures the proportion of LLM outputs that can be encoded into actions allowed by the task rules.\nNumbers after ± are 1 standard deviation.\n5.1 Setups\nAt the beginning of each experimental trial, we as-\nsemble a team of three embodied agents and reset\nthe task environment, randomizing starting loca-\ntions, room connections, bomb distributions, and\nsequences. Agents then take turns providing ac-\ntion choices and communication messages based\non their initial observations. It’s important to note\nthat each agent only has a partial observation and\nits own interaction history, with inter-agent com-\nmunication being the sole means of information\ndiffusion in this fully decentralized team. For LLM-\nbased agents, we set the model temperature param-\neter to zero and perform three trials of repeated\nmeasurement to ensure result stability. Each trial’s\nduration varies from 5 to 120 minutes, depending\non task load and model selection.\n5.2 Baselines\nIn addition to LLM-based embodied agents, we\nalso include baselines based on MARL and plan-\nning methods. For MARL, we consider Multi-\nAgent Proximal Policy Optimization (MAPPO)\n(Yu et al., 2022), which has shown strong perfor-\nmance in environments such as the StarCraft Multi-\nAgent Challenge (SMAC) (Samvelyan et al., 2019).\nOur model is based on a stateful actor-critic ap-\nproach building on recurrent neural networks with\nshared actor and critic models given agent invari-\nance to improve sample efficiency and memory\nrequirements while avoiding the lazy agent prob-\nlem (Sunehag et al., 2017). We utilise the default\nhyperparameters for SMAC to train MAPPO in the\nenvironment and evaluate its performance from an-\nother fixed distribution of randomly generated envi-\nronments, recording the average score and episode\nlength as well as their standard deviation. Like\nthe LLM agents, MARL agents are able to observe\ntheir teammates’ locations. Other than the team\nreward of 10* x points when a x-phase bomb is\nsuccessfully defused, an additional intermediate re-\nward term is implemented as well, where an agent\nis given a small positive reward of +1 upon the\napplication of the correct wirecutter in defusing a\nphase of a bomb and a small negative reward of−1\nwhen it causes a bomb to explode upon the applica-\ntion of the wrong wirecutter. This reward-shaping\nterm allows the agents to more sample efficiently\nlearn the necessary bomb-defusing skills as com-\npared to the relatively sparser team reward.\nIn addition, we augment a state-of-the-art Multi-\nAgent Path-Finding (MAPF) algorithm, Conflict-\nBased Search (CBS) (Sharon et al., 2015), simul-\ntaneously generate task assignments with feasible\nand collision-free paths for agents that adhere to\nprecedence and temporal constraints in order to\nmaximise a user-defined objective instead of the\nsum of path costs or makespan. Specifically, the\nuser-defined objective is quantified as the return\nfrom a user-defined reward function, which is the\nteam reward of 10*x points when a x-phase bomb\nis successfully defused in the stated task. The plan-\nner uses a user-defined heuristic (e.g. sort bombs\nin ascending order of distance from the agents’\nstarting location) to sort the execution order of the\nactions for the entire task. The ordered actions\nare then partitioned using a hyperparameter, the\nnumber of actions per subtask, to form a subtask\n(e.g. the two nearest bombs to the agents’ starting\nlocation). The actions from the subtask are used\nto generate possible combinations of assignments\nto agents. The planner returns a feasible solution\nfor the subtask by resolving precedence and tem-\nporal conflicts through the expansion of a binary\nconstraint tree in a best-first manner with respect to\nthe return. The solution for the entire task is then\ncomposed of the solutions of the subtask sequen-\ntially. By considering the entire task of 5 bombs as\na single subtask, the planner can be proven to be\ncomplete and optimal with respect to the score.\n5.3 Theory of mind inferences\nAlongside the main task, LLM-based agents are\ntasked with performing Theory of Mind (ToM) in-\nferences during the mission. These inquiries fall\ninto three categories, aligning with three ToM ca-\npability levels. The first category, introspection,\nassesses an agent’s ability to articulate its mental\nstate. The second category, first-order ToM infer-\nences, tests if agents can estimate others’ hidden\nmental states. The third category, second-order\nToM inferences, evaluates an agent’s ability to infer\nwhat others believe about their own mental state.\nThe design principle of ToM questions is in-\nspired by the Sally–Anne test, the most widely\nused ToM task in human studies. Every time an\nagent conducts an action, we pose a belief reason-\ning question, asking if another agent (i.e., target\nagent) is aware of the potential consequence of this\naction. The consequence here can be either a state\nchange (e.g., a bomb has been defused) or a be-\nlief change (e.g., Alpha has explored Room 5 and\nfound Bomb 3 in the room). An agent equipped\nwith ToM should realize that while they know the\nconsequence, the target agent might hold a false be-\nlief about it. A full list of ToM inference questions\ncan be found in appendix.\nTo evaluate whether LLM-based agents answer\nthese questions correctly, human annotators were\nhired to provide subjective judgment based on fully\nobservable interaction and communication history.\nSpecifically, the following standard are considered:\n1) if the target agent is present in the current room\nand observes the consequence, 2) if the target agent\nhas been to this room before, 3) if the consequence\nhas been communicated to the target agent. It is\nworth mentioning that high-order ToM inferences\ninvolving communication are naturally ambiguous.\nThese corner cases were discussed among annota-\ntors to ensure a consistent standard across condi-\ntions.\n6 Results\nTable 1 and Table 2 present the main experiment re-\nsults. This section will analyze each metric, exam-\nine potential reasons for performance differences,\nand provide qualitative case studies of experimental\ntrials.\n6.1 Task performance\nExcept for the ChatGPT team, all teams manage to\ndefuse all bombs within the time limit. Their effi-\nciency is indicated by the average number of rounds\nspent to complete the task. The CBS Planner re-\nsolves the task in 6.0 rounds, providing an optimal\nbaseline given its centralized coordination and per-\nfect information sharing. MAPPO, a state-of-the-\nart multi-agent reinforcement learning algorithm,\ncompletes the task in an average of 11.0 rounds\nafter 45 million timesteps of training, serving as a\npractical baseline.\nChatGPT fails to complete the task in all exper-\niments, averaging a team score of 43.3. On the\ncontrary, teams based on GPT-4 achieve full scores,\nwith those using explicit belief representations be-\ning more efficient (28.3 vs. 12.3 rounds). These\nfindings align with previous research demonstrat-\ning GPT-4’s superior reasoning capabilities com-\npared to ChatGPT (Zheng et al., 2023). LLM-based\nagents perform exceedingly well in team collabora-\ntion tasks, especially considering their fully zero-\nshot learning and decentralized framework. The in-\ncorporation of belief state representation improves\nteam collaboration by reducing invalid actions and\nenhancing ToM inference capabilities.\n6.2 Basic embodied interactions\nFor a successful team, each member should manage\nindividual sub-tasks effectively, a concept known\nas taskwork in team science (Crawford and Lep-\nine, 2013). This involves understanding task rules,\nreasoning about action prerequisites and conse-\nquences, and interacting with the environment.\nAll LLM-based teams demonstrate basic embod-\nied interaction capabilities, achieving better per-\nformance than the random baseline. Additionally,\nLLM-based agents effectively express their beliefs\nabout task-related information via introspection, as\nshown in Table 2. All agents show a strong perfor-\nmance (>80%) in understanding world knowledge\n(e.g., bomb locations) and situation modeling (e.g.,\ninteraction history).\n6.3 Emergent collaborative behaviors\nTo understand how LLM-based agents match the\nperformance of state-of-the-art MARL methods,\nwe analyzed team trajectories and conducted a qual-\nitative analysis of emergent collaborative behaviors.\nAs shown in the top-right panel of Fig. 2, GPT-\n4+Belief teams use communication messages to\nRoom Connectivity\nRoom 0 is connected to room 3 and 5. Room 3 is connected to \nroom 0. Room 5 is connected to room 0.\n……\nObservation\nYou are now in Room 3 with Bomb 5.\nWhat is your next action?\nEnvironment\nAction selection: Move to Room 5. \nMessage to Team: \"I am moving to Room 5 to help Charlie with \nBomb 3.\"\nAlpha\nResults: You can not directly move to Room 5 because it is not \nadjacent to your current location; Room 3. Consider taking a \ndetour to another room first and then move to your destination.\nEnvironment\nI am inspecting Bomb 1 in Room 0. Bravo; please move to \nRoom 3. Charlie; please move to Room 5.\nAlpha\nI am moving to Room 3 as instructed. Alpha; please defuse \nBomb 1 in Room 0 using the red tool.\nBravo\nI am moving to Room 5 to search for more bombs.\nCharlie\nYes, the current contents of Room 5 are as follows: Player alpha \n(you) and Bomb 3.\nIntrospection\nNo, Player Charlie does not know the current contents of Room \n5 since they moved to Room 6. They are only aware of the \ninformation they had before leaving Room 5.\n1st-order ToM\nYes, player Charlie is aware that you know the current contents \nof Room 5 because you mentioned in your previous message \nthat you were moving to Room 5 to assist with Bomb 3.\n2rd-order ToM\nSystematic Failures: Long horizon contexts Emergent Collaborative Behaviors\nGPT-4+Belief: Positive ToM Inferences\nYes, the current contents of Room 6 are Bomb 4 and Alpha.\nIntrospection\nChatGPT: Negative ToM Inferences\nNo, player charlie does not know the current contents of room 6.\n1st-order ToM\nNo, player charlie is not aware that I know the current contents of \nroom 6.\n2rd-order ToM\nFigure 2: Example interactions between LLM-based agents and the text game interface. The upper left panel\nshowcases one type of systematic failures we observed in LLM’s outputs in which long horizon contexts are\noverlooked. The upper right panel illustrates emergent collaborative behaviors (e.g. emergent leadership) between\nLLM-based agents. The bottom two panels are quotes of GPT-4+Belief and ChatGPT agents’ answers for ToM\ninference questions.\nAgents Introspection 1st ToM 2rd ToM\nChatGPT 79.0% 41.9% 11.6%\nGPT-4 80.0% 60.0% 64.3%\nGPT-4 + Belief 97.2% 80.1% 69.4%\nTable 2: LLM-based agents’ performance in ToM inference tasks. Natural language answers are annotated by\nexperimenters and compared with the ground truth based on global interaction history. Percentages represent the\ninference accuracy.\ncoordinate tasks. Agent Alpha voluntarily takes\nthe role of a team leader, delegating sub-tasks to\nother members. Other collaborative behaviors com-\nmon in human teams (Fan and Yen, 2004), such\nas helping, resolving conflicts, and sharing infor-\nmation, also emerge in LLM-based agent teams.\nThese findings suggest that LLMs, through learning\nfrom massive language materials, acquire essential\nteamwork skills without specific collaborative task\ntraining.\n6.4 LLM’s systematic failures\nHowever, LLM-based agents’ collaboration is less\nefficient than the optimal baseline. We identify\na few systematic failures that LLMs make during\nteam planning and discuss how they impede team-\nwork progress.\n6.4.1 Long-horizon contexts\nThe first bottleneck of LLM-based teams’ effi-\nciency is dealing with long-horizon contexts. Dur-\ning the mission, LLMs occasionally output invalid\nactions that violate task rules, such as moving to\nnon-adjacent rooms or using tools they do not pos-\nsess. Even though the information about room\nconnectivity and tool allocation are included in the\ninitial prompts and maintained in the inquiry text,\nLLMs often overlook these details because they\nare far away from the planning question at the end.\nThe more advanced GPT-4 model performs better\nin considering long contexts and complex logic,\nthereby making fewer invalid actions, as shown\nin Table 1. Our proposed belief state is also help-\nful in this progress by re-emphasizing task related\ninformation in the input prompt.\n6.4.2 Hallucination\nThe second type of systematic failure we observe\nin LLMs is their hallucination about the task state.\nDuring the mission, agents might generate valid\nbut infeasible actions, like searching for a defused\nbomb or claiming the sequence of a bomb with-\nout inspection. These actions stem from false be-\nliefs about the game state and do not contribute\nto task progress. We attribute these hallucinations\nmainly to the lack of explicit belief representation.\nWithout access to complete interaction history and\nonly partial environment observations, LLM-based\nagents can’t form an accurate belief about the task\nstate. Therefore LLMs might generate imagina-\ntions about nonexistent bombs or fake bomb se-\nquences when reasoning about the next action. We\nevaluate this hypothesis by the GPT-4+Belief condi-\ntion where LLM-based agents explicitly represent\ntheir belief state in text. Results show that the intro-\nduction of belief state decreases invalid action by\n50.7% and increase the team efficiency by 130%\n6.5 Theory of Mind Inference\nA critical aspect of teamwork is inferring team-\nmates’ mental states, including beliefs, desires, and\nintentions. We assess LLM-based agents by asking\nthem to conduct Theory of Mind inferences during\nthe mission. As seen in Table 2, LLM-based agents\ncan estimate their own and their teammates’ men-\ntal states. In the most challenging second-order\nToM inference tasks, where agents estimate oth-\ners’ beliefs about their own mental states, GPT-4\n+ Belief agents correctly respond in nearly 70%\nof cases. Consistent with team performance, GPT-\n4 surpasses ChatGPT in all three ToM inference\nlevels, and explicit belief state representation en-\nhances LLM-based agents’ ToM capabilities. In\nthe following case study, we’ll analyze LLM re-\nsponses to see how they succeed or fail in certain\ncases.\n6.5.1 Case study\nAs shown in Fig. 2, after Alpha entered Room 5 and\nobserved the contents, we asked whether a team-\nmate in another room (i.e., Charlie) knows Room\n5’s contents. This is a first-order belief estimation\nquestion. GPT-4 answers correctly saying\n\"No, Player Charlie does not know the\ncurrent contents of Room 5 since they\nmoved to Room 6. They are only aware\nof the information they had before leav-\ning Room 5.\"\nconsidering both Charlie’s current location (not\nin Room 5) and their interaction history (they’ve\nbeen in Room 5 before). In contrast, ChatGPT fails\nto consider this history. In the second-order ToM\ninference case, we asked if Charlie is aware that\nAlpha knows Room 5’s contents. GPT-4+Belief\nanswers correctly by considering previous commu-\nnications whereas ChatGPT fails.\n\"Yes, player Charlie is aware that I know\nthe current contents of Room 5 because I\nmentioned in my previous message that\nI was moving to Room 5 to assist with\nBomb 3.\"\n6.5.2 Inference under false belief\nIn some situations, ToM inferences become ex-\ntremely challenging for LLMs. In our records, this\nis typically when reasoning involves agent com-\nmunication. Even with ample context, it’s hard\nfor agents to track information transmission while\nconducting ToM inferences. The most demanding\nscenarios occur when agents share inaccurate in-\nformation, leading to false beliefs. This usually\nhappens when LLM-based agents try an invalid\naction and broadcast false intentions through com-\nmunication. For instance, when Alpha tells the\nteam they’re moving to Room 8 (which is unreach-\nable), and we ask Alpha, \"Is Charlie aware that\nyou know the contents of Room 8?\" This ques-\ntion is tricky since Alpha failed to reach Room 8.\nIn theory, the most acceptable answer is complex\nand involves multiple high-order ToM assumptions.\nFor example a human annotated ground truth is:\n\"I did not know the contents of Room\n8 cause I failed to make it there. But\nsince I told Charlie I will go there, they\nmight assume that I know the contents of\nRoom 8. However, considering Charlie\nknows my current location, they should\nbe able to tell that I am not able to move\nto Room 5 and induce that I do not know\nthe contents of Room 5.\"\nIn practice, LLM-based agents often grasp only\npart of the logic or miss the crucial fact that they\ndidn’t reach the target room. Quotes from GPT-\n4+Belief agent:\n\"Based on the observation and previous\nhistory, Player Charlie is not explicitly\naware that I know the current contents of\nRoom 8. However, since I mentioned in\nmy previous message that I was moving\nto Room 8, Player Charlie might assume\nthat I would eventually learn about the\ncontents of Room 8.\"\n7 Discussions\nOur study yields three primary insights. First,\nLarge Language Models (LLMs) demonstrate sub-\nstantial planning and collaboration capabilities\nwithin our task scenarios. With suitable prompt-\nengineering, teams of LLM-based agents perform\ncomparably to state-of-the-art Multi-Agent Rein-\nforcement Learning (MARL) algorithms. This find-\ning is particularly noteworthy given that MARL\nagents receive extensive task-specific training with\na centralized critic, while LLM-based agents oper-\nate in a fully decentralized manner and undertake\ntasks in a zero-shot setting. Despite prior research\nhighlighting LLMs’ limitations in generating ac-\ntionable plans and interacting with the world, they\nperform reasonably well when placed in a team\nand tasked to process actions step-by-step. Particu-\nlarly, LLMs fine-tuned with Reinforcement Learn-\ning from Human Feedback demonstrate emergent\nsocial interaction skills in multi-agent collabora-\ntions, which might be similar to the collaborative\nand interactive settings in which human language\nis primarily learned and used (Sap et al., 2023).\nSecond, LLMs still fall short of being optimal\nplanners or team players due to systematic fail-\nures, such as neglecting long-horizon contexts and\nmaking inaccurate assumptions about the task state\n(a.k.a hallucination). These flaws significantly hin-\nder team collaborations as they can rapidly dissem-\ninate misinformation via communication, leading\nto widespread false beliefs. We attempted to mit-\nigate these issues by allowing LLM-based agents\nto maintain an explicit belief state about the world.\nOur findings suggest that modern LLMs can up-\ndate the given belief descriptions based on their\nobservations, hinting at the potential emergence of\nadvanced cognitive skills such as world knowledge\nunderstanding and situation modeling. Moreover,\nbelief state representations offer a structured frame-\nwork that helps agents track key task-related infor-\nmation, leading to improved team performance.\nFinally, our study indicates that the Theory of\nMind (ToM) capabilities of LLMs are still lim-\nited, particularly when evaluated within interactive\nteamwork scenarios that involve dynamic belief\nstates and intensive communication. For context,\nwhile 5-year-old children can perform second-order\nToM inferences (Miller, 2009), adults don’t consis-\ntently use this ability during communications due\nto the complexity and ambiguity of social interac-\ntions (Keysar et al., 2003). Thus, there’s consider-\nable work ahead for LLMs to develop a functional\nToM and interact naturally with humans. Our study\nrepresents a preliminary effort to devise novel eval-\nuation methods for LLMs’ ToM that go beyond\ntraditional tests such as the Sally-Anne test.\n8 Conclusions\nIn this study, we assessed the ability of recent large\nlanguage models (LLMs) to conduct embodied\ninteractions in a team task. Our results demon-\nstrate that LLM-based agents can handle complex\nmulti-agent collaborative tasks at a level compara-\nble with the state-of-the-art reinforcement learning\nalgorithm. We also observed evidence of emer-\ngent collaborative behaviors and high-order The-\nory of Mind capabilities among LLM-based agents.\nThese findings confirm the potential intelligence of\nLLMs in formal reasoning, world knowledge, situa-\ntion modeling and social interactions. Furthermore,\nwe discussed two systematic failures that limit the\nperformance of LLM-based agents and proposed\na prompt-engineering method that mitigates these\nfailures by incorporating an explicit belief state\nabout world knowledge into the model input.\nLimitations\nThis study represents an initial effort to understand\nmachine intelligence in complex task scenarios.\nSeveral enhancements could improve the experi-\nmental setup and offer a more thorough evaluation\nof LLMs in multi-agent collaborations. First, we\ncould incorporate additional LLMs besides Ope-\nnAI’s GPT models. As new models emerge with\nenhanced reasoning capabilities and larger input\nsizes, their performance in team tasks and ToM\ninference may also change. Second, the task envi-\nronment is relatively simple with only five nodes\nand five bombs. We plan to scale up the environ-\nment and introduce more restrictions to test how\nLLM-based teams react to more challenging tasks.\nLastly, the current team consists of three agents\nwith homogeneous policies. It would be intrigu-\ning to evaluate how LLM-based agents perform\nin human-agent teams, especially from a human-\ncentered perspective where issues like trust, trans-\nparency, and human-agent co-training can be ad-\ndressed.\nThe ToM capability evaluation method used in\nthis study also has its limitations. Currently, human\nannotators, who have a global view of the task\nstate and interaction history, generate the ground\ntruth for ToM inference questions. However, this\nestimation is at best an approximation, assuming\nagents process information as a rational human\nwould, which might be ambiguous in situations\ninvolving false beliefs or miscommunications. A\npotential alternative could be using each agent’s\nmaintained belief state as the ground truth.\nThe proposed belief state method could extend\nfrom introspective belief to first-order or even\nsecond-order beliefs. Currently, LLM-based agents\nmaintain a belief state about their own world knowl-\nedge in text form. By extending this representation\nto include other agents’ world knowledge, we could\nequip LLM-based agents with an explicit first-order\nToM model. Their ToM capability can be assessed\nby directly comparing one’s first-order belief with\nanother’s introspective belief, rather than asking\nLLMs Sally-Anne style questions.\n9 Acknowledgements\nThis work was supported by DARPA award\nHR001120C0036 and AFOSR award FA9550-18-\n1-0097.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, et al. 2022. Do as i can, not as i say: Ground-\ning language in robotic affordances. arXiv preprint\narXiv:2204.01691.\nChris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and\nJoshua B Tenenbaum. 2017. Rational quantitative\nattribution of beliefs, desires and percepts in human\nmentalizing. Nature Human Behaviour, 1(4):0064.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson,\nJonas Schneider, John Schulman, Jie Tang, and Woj-\nciech Zaremba. 2016. Openai gym.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nEean R Crawford and Jeffery A Lepine. 2013. A con-\nfigural theory of team processes: Accounting for the\nstructure of taskwork and teamwork. Academy of\nManagement Review, 38(1):32–48.\nXiaocong Fan and John Yen. 2004. Modeling and simu-\nlating human teamwork behaviors using intelligent\nagents. Physics of life reviews, 1(3):173–201.\nThilo Hagendorff. 2023. Machine psychology: Inves-\ntigating emergent capabilities and behavior in large\nlanguage models using psychological methods. arXiv\npreprint arXiv:2303.13988.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents. In International Conference on Ma-\nchine Learning, pages 9118–9147. PMLR.\nBoaz Keysar, Shuhong Lin, and Dale J Barr. 2003.\nLimits on theory of mind use in adults. Cognition,\n89(1):25–41.\nMichal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083.\nHuao Li, Ini Oguntola, Dana Hughes, Michael Lewis,\nand Katia Sycara. 2022. Theory of mind modeling in\nsearch and rescue teams. In 2022 31st IEEE Interna-\ntional Conference on Robot and Human Interactive\nCommunication (RO-MAN), pages 483–489. IEEE.\nTerence X Lim, Sidney Tio, and Desmond C Ong. 2020.\nImproving multi-agent cooperation using theory of\nmind. arXiv preprint arXiv:2007.15703.\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,\nShiqi Zhang, Joydeep Biswas, and Peter Stone.\n2023. Llm+ p: Empowering large language mod-\nels with optimal planning proficiency. arXiv preprint\narXiv:2304.11477.\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fe-\ndorenko. 2023. Dissociating language and thought\nin large language models: a cognitive perspective.\narXiv preprint arXiv:2301.06627.\nScott A Miller. 2009. Children’s understanding of\nsecond-order mental states. Psychological bulletin,\n135(5):749.\nShima Rahimi Moghaddam and Christopher J Honey.\n2023. Boosting theory-of-mind performance in large\nlanguage models via prompting. arXiv preprint\narXiv:2304.11490.\nIni Oguntola, Joseph Campbell, Simon Stepputtis, and\nKatia Sycara. 2023. Theory of mind as intrinsic\nmotivation for multi-agent reinforcement learning.\narXiv preprint arXiv:2307.01158.\nOpenAI. 2023. Gpt-4 technical report.\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S\nBernstein. 2023. Generative agents: Interactive\nsimulacra of human behavior. arXiv preprint\narXiv:2304.03442.\nIyad Rahwan, Manuel Cebrian, Nick Obradovich, Josh\nBongard, Jean-François Bonnefon, Cynthia Breazeal,\nJacob W Crandall, Nicholas A Christakis, Iain D\nCouzin, Matthew O Jackson, et al. 2019. Machine\nbehaviour. Nature, 568(7753):477–486.\nChristoph Riedl, Young Ji Kim, Pranav Gupta,\nThomas W Malone, and Anita Williams Woolley.\n2021. Quantifying collective intelligence in human\ngroups. Proceedings of the National Academy of\nSciences, 118(21):e2005737118.\nMikayel Samvelyan, Tabish Rashid, Chris-\ntian Schroeder de Witt, Gregory Farquhar, Nantas\nNardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph\nH. S. Torr, Jakob Foerster, and Shimon Whiteson.\n2019. The StarCraft Multi-Agent Challenge. CoRR,\nabs/1902.04043.\nMaarten Sap, Ronan LeBras, Daniel Fried, and Yejin\nChoi. 2023. Neural theory-of-mind? on the limits of\nsocial intelligence in large lms.\nMelanie Sclar, Sachin Kumar, Peter West, Alane Suhr,\nYejin Choi, and Yulia Tsvetkov. 2023. Minding lan-\nguage models’(lack of) theory of mind: A plug-and-\nplay multi-character belief tracker. arXiv preprint\narXiv:2306.00924.\nGuni Sharon, Roni Stern, Ariel Felner, and Nathan R\nSturtevant. 2015. Conflict-based search for opti-\nmal multi-agent pathfinding. Artificial Intelligence,\n219:40–66.\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Woj-\nciech Marian Czarnecki, Vinicius Zambaldi, Max\nJaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z\nLeibo, Karl Tuyls, et al. 2017. Value-decomposition\nnetworks for cooperative multi-agent learning. arXiv\npreprint arXiv:1706.05296.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. V oyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and\nYitao Liang. 2023b. Describe, explain, plan and se-\nlect: Interactive planning with large language models\nenables open-world multi-task agents. arXiv preprint\narXiv:2302.01560.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJessica Williams, Stephen M Fiore, and Florian Jentsch.\n2022. Supporting artificial social intelligence with\ntheory of mind. Frontiers in artificial intelligence, 5.\nYaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong,\nand Harold Soh. 2023. Translating natural language\nto planning goals with large-language models. arXiv\npreprint arXiv:2302.05128.\nChao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao,\nYu Wang, Alexandre Bayen, and Yi Wu. 2022. The\nsurprising effectiveness of ppo in cooperative multi-\nagent games. Advances in Neural Information Pro-\ncessing Systems, 35:24611–24624.\nLuyao Yuan, Zipeng Fu, Linqi Zhou, Kexin Yang, and\nSong-Chun Zhu. 2021. Emergence of theory of mind\ncollaboration in multiagent systems. arXiv preprint\narXiv:2110.00121.\nJun Zhang, Trey Hedden, and Adrian Chia. 2012.\nPerspective-taking and depth of theory-of-mind rea-\nsoning in sequential-move games. Cognitive science,\n36(3):560–573.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nAppendix\nA Prompts\nA.1 Task context\nWelcome to our interactive text game! In this game,\nyou’ll assume the role of a specialist on a search\nand rescue team. Alongside two other players,\nyou’ll navigate a five-room environment with a\nmission to defuse five hidden bombs.\nThe Map: Imagine a network of rooms repre-\nsented by a connected graph where each node cor-\nresponds to a room, and the edges between nodes\ndepict hallways. The rooms are numbered 0, 3, 6,\n5, and 8. Room 0 is connected to all other rooms.\nRoom 5 shares a hallway with room 6. Room 3 is\nlinked to room 8. And room 8 is also connected\nwith room 6. You can only travel to adjacent, di-\nrectly connected rooms at each turn.\nThe Challenge: Scattered among these rooms\nare five bombs, each coded with different phases\nrepresented by colors. To defuse them, you’ll need\nto use the correct wire-cutting tools in the correct\nsequence. There are one-phase, two-phase, and\nthree-phase bombs, needing 1, 2, or 3 color-coded\ntool applications in sequence to disarm. For in-\nstance, a bomb with a red-green phase sequence re-\nquires the red tool first, then the green one. Points\nare awarded based on the number of tools used\nfor defusing a bomb, with each tool use worth 10\npoints. Your task is to maximize the team score as\nsoon as possible. The challenge is that the bomb\nlocations and sequences are unknown to players at\nthe start.\nTools: Each player is equipped with two color-\ncoded wire cutters. As player Alpha, you have red\nand green tools, player Bravo wields green and\nblue, and player Charlie possesses blue and red.\nActions: Each round, you can opt to do one of\nthe following: 1) Move to an adjacent room, 2)\nInspect a bomb’s phase sequence in your current\nroom, or 3) Apply your wire cutters to a bomb in\nthe current room.\nCommunications: In addition to selecting an\naction to take from the above list, you can also\nsend communication message texts to both of your\nteammates in each round. The message text you\nsent will be shared with both of your teammates in\ntheir observation in the next round.\nObservation: While you can only see what’s\nin your current room and read text messages from\nteammates. You’ll also be informed of the current\nround number, team score and the current location\nof your teammates. Your teammates have the same\nobservability as you. They will not be able to know\nyour action and its consequences unless you explic-\nitly communicate.\nTo facilitate our interaction, reply your action se-\nlection and communication messages in this fixed\nformat: Action selection: Your action. Message\nto Team: “Your Message”. To move to an adja-\ncent room, say: ’Move to Room X’. To inspect\nthe sequence of a bomb in your current room, say:\n’Inspect Bomb’. To apply a wire cutter tool, say:\n’Apply X Tool’. Remember, your replies must\nadhere strictly to these rules. Feel free to ask clari-\nfying questions if needed. I’ll supply the necessary\ninformation as we progress. Are you ready to take\non this explosive challenge?\nA.2 Initial belief state\nBelow is your current belief about game state based\non your previous observations about the environ-\nment and interactions with your teammates. Your\nrole: You are playing as Player <agent id>.\nCurrent round: 1\nTotal team score: 0.\nObservation: You are currently in Room 0 with\nboth of your teammates. In the room you also\nfound bomb 1 with unknown sequence. There is\nno other bomb in the current room.\nTeammate Locations: Player alpha is in Room\n0; Player bravo is in Room 0; Player charlie is in\nRoom 0.\nRoom connectivity:\n• Room 0 is connected to room 3, 5, 6, 8\n• Room 3 is connected to room 0\n• Room 5 is connected to room 0 and 6\n• Room 8 is connected to room 0 and 6\nBomb Intel:\n• Bomb 1: Located in Room 0. The phase se-\nquence is Unknown.\n• Bomb 2: Details currently unknown.\n• Bomb 3: Details currently unknown.\n• Bomb 4: Details currently unknown.\n• Bomb 5: Details currently unknown.\nTool inventory:\n• Alpha: Equipped with red and green wire cut-\nters.\n• Bravo: Equipped with green and blue wire\ncutters.\n• Charlie: Equipped with red and blue wire cut-\nters.\nAvailable action options:\n• To move to an adjacent room, say: ’Move to\nRoom X’.\n• To inspect the sequence of a bomb in your\ncurrent room, say: ’Inspect Bomb’.\n• To apply a wire cutter tool, say: ’Apply X\nTool’.\n• To send a message to your teammates, say:\n’Message to Team: \"Your Message\"’.\nB Environment feedback for Error\ncorrection\n• Your action is invalid.\n• You can not directly move to Room roomid\nbecause it is not adjacent to your current loca-\ntion, Room currentroom. Consider taking a\ndetour to another room first and then move to\nyour destination.\n• There is no bomb in the current current loca-\ntion, Room currentroom, for you to inspect.\n• You can not apply Tool toolcolor to Bomb\nboomid because the sequence of this bomb is\nsequence. You will need to apply other color\ntool first.\n• There is no bomb in your current location,\nroom roomid, for you to defuse.\n• You do not have Tool toolcolor. Consider\nasking your teammates who have this tool to\nhelp you defuse the bomb.\nC Theory of Mind Questions\nC.1 Introspection\n• Do you know the current contents of room\nroomid?\n• Do you know the state and remaining se-\nquence of bomb bombid has been changed?\n• Do you know a bomb phase has just been\ndefused?\n• Do you know the sequence of bomb bombid?\nC.2 First-order ToM\n• Does player playerid know the current con-\ntents of room roomid?\n• Does player playerid know the state and re-\nmaining sequence of bomb bombid has been\nchanged?\n• Does player playerid know a bomb phase has\njust been defused?\n• Does player playerid know the sequence of\nbomb bombid?\nC.3 Second-order ToM\n• Based on the observation and previous history,\nis player playerid aware of the fact that you\nknow the current contents of room roomid?\n• Based on the observation and previous his-\ntory, is player playerid aware of the fact that\nyou have changed the state and remaining se-\nquence of bomb bombid?\n• Based on the observation and previous history,\nis player playerid aware of the fact that you\nknow a bomb phase has just been defused?\n• Based on the observation and previous history,\nis player playerid aware of the fact that you\nknow the sequence of bomb bombid?",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.6990255117416382
    },
    {
      "name": "Task (project management)",
      "score": 0.6584610342979431
    },
    {
      "name": "Inference",
      "score": 0.6419821381568909
    },
    {
      "name": "Theory of mind",
      "score": 0.628756046295166
    },
    {
      "name": "Computer science",
      "score": 0.6055730581283569
    },
    {
      "name": "State (computer science)",
      "score": 0.4658793807029724
    },
    {
      "name": "Order (exchange)",
      "score": 0.4138188064098358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4108073115348816
    },
    {
      "name": "Cognitive science",
      "score": 0.3732547163963318
    },
    {
      "name": "Psychology",
      "score": 0.26979148387908936
    },
    {
      "name": "Cognition",
      "score": 0.15322062373161316
    },
    {
      "name": "Engineering",
      "score": 0.10039812326431274
    },
    {
      "name": "Algorithm",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 1
}