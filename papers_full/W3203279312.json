{
  "title": "Language Model Priming for Cross-Lingual Event Extraction",
  "url": "https://openalex.org/W3203279312",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2648418205",
      "name": "Steven Fincke",
      "affiliations": [
        "Biomedical Research Institute of Southern California",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2183437143",
      "name": "Shantanu Agarwal",
      "affiliations": [
        "Biomedical Research Institute of Southern California",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2104028023",
      "name": "Scott Miller",
      "affiliations": [
        "Biomedical Research Institute of Southern California",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A1974204212",
      "name": "Elizabeth Boschee",
      "affiliations": [
        "University of Southern California",
        "Biomedical Research Institute of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2648418205",
      "name": "Steven Fincke",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2183437143",
      "name": "Shantanu Agarwal",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2104028023",
      "name": "Scott Miller",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A1974204212",
      "name": "Elizabeth Boschee",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6720414245",
    "https://openalex.org/W3091998909",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3021280023",
    "https://openalex.org/W2946628371",
    "https://openalex.org/W3035229828",
    "https://openalex.org/W6786144006",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3155888533",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W2971145411",
    "https://openalex.org/W2972140869",
    "https://openalex.org/W3034797437",
    "https://openalex.org/W6791279066",
    "https://openalex.org/W2474272410",
    "https://openalex.org/W2942904230",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285798792",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W3094573908",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W3104597568",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2949922292",
    "https://openalex.org/W2471147443",
    "https://openalex.org/W3135176278",
    "https://openalex.org/W3176012225",
    "https://openalex.org/W2741029840",
    "https://openalex.org/W3102925419",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2981573048",
    "https://openalex.org/W2964206023",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3035625205"
  ],
  "abstract": "We present a novel, language-agnostic approach to \"priming\" language models for the task of event extraction, providing particularly effective performance in low-resource and zero-shot cross-lingual settings. With priming, we augment the input to the transformer stack's language model differently depending on the question(s) being asked of the model at runtime. For instance, if the model is being asked to identify arguments for the trigger \"protested\", we will provide that trigger as part of the input to the language model, allowing it to produce different representations for candidate arguments than when it is asked about arguments for the trigger \"arrest\" elsewhere in the same sentence. We show that by enabling the language model to better compensate for the deficits of sparse and noisy training data, our approach improves both trigger and argument detection and classification significantly over the state of the art in a zero-shot cross-lingual setting.",
  "full_text": "Language Model Priming for Cross-Lingual Event Extraction\nSteven Fincke, Shantanu Agarwal, Scott Miller, Elizabeth Boschee\nUniversity of Southern California Information Sciences Institute\nsfincke@isi.edu, shantanu@isi.edu, smiller@isi.edu, boschee@isi.edu\nAbstract\nWe present a novel, language-agnostic approach to “priming”\nlanguage models for the task of event extraction, providing\nparticularly effective performance in low-resource and zero-\nshot cross-lingual settings. With priming, we augment the in-\nput to the transformer stack’s language model differently de-\npending on the question(s) being asked of the model at run-\ntime. For instance, if the model is being asked to identify ar-\nguments for the trigger protested, we will provide that trigger\nas part of the input to the language model, allowing it to pro-\nduce different representations for candidate arguments than\nwhen it is asked about arguments for the trigger arrest else-\nwhere in the same sentence. We show that by enabling the\nlanguage model to better compensate for the deficits of sparse\nand noisy training data, our approach improves both trigger\nand argument detection and classification significantly over\nthe state of the art in a zero-shot cross-lingual setting.\nIntroduction\nRecent advances in massively-pretrained cross-lingual lan-\nguage models, e.g. Conneau et al. (2020), have revolu-\ntionized approaches to extracting information from a much\nbroader set of languages than was previously possible. For\nsome information extraction (IE) tasks, it is not unusual to\nfind annotated datasets in a variety of languages. Name an-\nnotation, for instance, has both wide utility and can be per-\nformed relatively cheaply and easily by non-experts—and\nas a result, one can find annotated named entity datasets for\nlanguages from Spanish to Polish to Farsi to Indonesian. In\ncontrast, datasets for event extraction tasks are much fewer\nand further between (even in English).\nIn order to extract events from most languages, then, it is\ncritical to be able to train models with data from one lan-\nguage and deploy those models in another. A pretrained,\ncross-lingual language model can carry some of this burden.\nIf the language model’s vector representation forarrest (En-\nglish) is close to its representation foranholdelsen (Danish),\nthen a model trained on the English sentence “They protested\nhis arrest” may be able to detect the A RREST -JAIL event in\n“De tre mænd blev ved anholdelsen 29 januar”1.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n1The three men were arrested on January 29. (Danish)\nOne major advance in recent years was the move from\nstatic word embeddings, e.g. word2vec (Mikolov et al.\n2013), where the representation for a word is constant no\nmatter the context in which it appears, to contextualized lan-\nguage models, e.g. ELMo (Peters et al. 2018) and BERT\n(Devlin et al. 2019), where the representation for a word is\nconditioned on its surrounding context. With contextualized\nlanguage models, the representation for arrest in the context\n”They protested his arrest” will likely be very different than\nwhen it appears in ”She is a cardiac arrest survivor”.\nHowever, even with a powerful mechanism like BERT,\nthe representation for, say, activists, in “Activists protested\nhis arrest ” will be the same whether the model is being\nasked if activists is an argument of the D EMONSTRATE\nevent protested or of the ARREST -JAIL event arrest.\nRecent advances for structured NLP tasks, such as named\nentity recognition (Li et al. 2019a), relation extraction (Li\net al. 2019b) and coreference resolution (Wu et al. 2020),\nhave noted this and responded accordingly, by providing a\nprompt that modifies the context in which a sentence is seen,\nallowing the language model to adjust its representations of\nthe sentence tokens accordingly. This method for injecting\ntask-specific guidance to the model is particularly beneficial\nin low-resource settings (Scao and Rush 2021).\nReturning to the task of event extraction, Du and Cardie\n(2020) show some of the promise of applying similar princi-\nples in this domain, reframing event extraction as a question-\nanswering problem. Their best results come from using natu-\nral English questions as prompts to their overall model, e.g.\nasking “Which is the agent\nin arrested? — The police ar-\nrested the thief.” However, it is not clear that this English-\ncentric approach of question generation will generalize well\nto cross-lingual transfer.\nIn this work, we present a new, language-agnostic mech-\nanism IE-P RIME which provides task-specific direction to\nan event extraction model at runtime. We show that it sig-\nnificantly outperforms prior work on argument extraction,\nincluding Du and Cardie (2020), and that it is particularly ef-\nfective in low-resource and zero-shot cross-lingual settings\nfor both trigger and argument detection and classification.\nRelated Work\nEvent extraction is a well-studied topic. Some of the most\neffective recent approaches in a mono-lingual context in-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10627\nclude Wadden et al. (2019) (DyGIE++), which combines\nlocal and global context using contextualized span repre-\nsentations across three unified subtasks (name, relation, and\nevent extraction), and Lin et al. (2020) (OneIE), which uses\na joint neural framework to extract globally optimal IE re-\nsults as graphs from input sentences. Our specific approach\ndraws inspiration from work in question answering. Al-\nthough several recent works have been likewise inspired,\n(e.g. Du and Cardie (2020), Feng, Yuan, and Zhang (2020),\n(Liu et al. 2020)), many of these approaches use natural-\nlanguage questions and are not therefore an ideal fit for\ncross-lingual applications. A major drawback of these other\nQA inspired systems is that they are designed and trained to\nextract only single answer spans and can only extract mul-\ntiple spans at decode time by making selections based on a\nprobability cutoff. We make an important improvement over\nthese previous works by avoiding the canonical QA architec-\nture and using a sequence-labelling approach instead, which\nis inherently capable of finding multiple “answers” from a\ngiven question context, e.g. simultaneously identifying more\nthan one V ICTIM of an ATTACK event, and avoids any dis-\ncrepancy between training and decoding behavior. We com-\npare against relevant prior work in both monolingual and\ncross-lingual contexts and show improved performance for\nargument extraction in all conditions and improved perfor-\nmance for trigger extraction in the cross-lingual context.\nThe primary focus of this work is on cross-lingual event\nextraction. However, most recent cross-lingual event extrac-\ntion work (e.g. Subburathinam et al. (2019), Ahmad, Peng,\nand Chang (2021), and Nguyen and Nguyen (2021)) focuses\nsolely on the event argument role labeling task (ignoring\ntrigger detection) and requires pre-generated entity mentions\nto serve as candidate arguments. Moreover, all of these ap-\nproaches rely on structural features in both languages, in-\ncluding but not limited to dependency parses. In contrast,\nour approach does not require any extraction of linguistic\nstructure in the target language. Our argument extraction\napproach also addresses the problem of candidate identifi-\ncation in addition to labeling, which is critical in real-world\napplications. We report results both on the limited task per-\nformed by the above-mentioned papers, showing improve-\nments over the reported state of the art, as well as on the\ncomplete end-to-end task.\nApproach\nIn this section we describe our baseline system (IE-\nBASELINE ) and an extension of that system (IE-P RIME )\nthat provides significant improvement. Both systems are de-\nsigned to be completely language-agnostic—documents in\nany language supported by the language model can be used\nas either training or test data—and as such can be applied in\neither a monolingual or cross-lingual context.\nIE-B ASELINE\nOur baseline system consists of two trained components,\none for event trigger extraction and one for argument at-\ntachment. Trigger detection and classification are performed\nusing a simple beginning-inside-outside (BIO) sequence-\nlabeling architecture composed of a single linear classifica-\ntion layer on top of the transformer stack.\nOur baseline argument extraction system (shown in Fig-\nure 1) takes as its input a proposed event trigger and from\nan input sentence identifies argument spans and labels them\nwith argument roles. This system produces arguments by\ngenerating BIO argument labels over the sentence tokens us-\ning a bi-LSTM, feeding into a single linear layer and then to\na CRF-based loss function 2 atop a transformer-based lan-\nguage model. The input for each token is the concatenation\nof the transformer output for the token itself, along with the\ntransformer output for the trigger token, and an embedding\nfor the event type; in our default configuration, no input en-\ntity information is used. When an individual input token is\nsplit up by the language model’s tokenizer (e.g. ifcharacter-\nistically is split into characteristic and ##ally), the average\nof the output vectors for the parts represents the whole.3 All\nmodels fine tune all the layers of the language model and\nonly use the output from the final layer.\nIE-P RIME\nThe key to our proposed system, IE-P RIME , is “priming”:\nthe idea that we can augment the input to the transformer\nstack in a way that provides critical additional information\nto the model at runtime.\nWe first describe how we do this for the task of argument\nextraction. Our baseline argument extraction system is de-\nsigned to take as input a single trigger and a sentence. It\nthen produces a complete set of argument spans and roles\nwith respect to that trigger. It repeats this process for every\nproposed event trigger.\nThe first form of priming we explore leverages that\ntrigger. Specifically, we augment the input to the language\nmodel by pre-pending the trigger to the sentence being con-\nsidered (divided from the sentence by a sentence-separating\ntoken appropriate to the language model being used, e.g.\n[SEP] for BERT). So, if a model trained using BERT\nis seeking arguments of protested, the language model\nwould receive the following input: [CLS] protested\n[SEP] crowds protested the conviction of\nIldar Dadin [SEP]. However, when searching for\narguments of conviction, the following input is used instead:\n[CLS] conviction [SEP] crowds protested the\nconviction of Ildar Dadin [SEP]. Figure 2 shows\na graphical representation of the second example.\nWe note that our baseline system already indicates the\ntrigger of interest when predicting arguments (by append-\ning its vector to the vector representation for each token), so\npriming is arguably redundant. However, providing this in-\nput in a second way appears to enable the language model to\nrespond more effectively to the change in focus at runtime.\nThis redundancy also allows the system to handle the case\nwhere an event trigger word occurs more than once in a sen-\ntence e.g. The company fired John a week after they fired\n2As implemented in torchcrf, https://pytorch-\ncrf.readthedocs.io/en/stable/\n3We also explored other strategies, e.g. taking only the first vec-\ntor, but averaging worked best.\n10628\nFigure 1: Baseline argument attachment architecture.\nBob. The placement of the trigger fired at the beginning of\nthe sentence would not be enough to disambiguate between\nthe two instances of fired in the sentence, but because the\nvector representation for the trigger token is generated by a\ncontextualized language model from a specific token in the\nsentence, the model can distinguish between the fired that\nshould have John as an argument and the fired that should\nhave Bob.\nThe second form of priming we explore leverages both\nthe trigger and the argument role. 4 Here, rather than asking\nthe model for all arguments of a trigger at the same time,\nwe query separately for each possible type of argument.\nThat is, we query first for any D EFENDANT arguments of a\nCONVICT event, and then separately for any ADJUDICATOR\narguments, and so on. In this formulation, we augment the\ninput to the language model with both the trigger and the\nargument role. However, to facilitate application to a variety\nof languages, we replace each English argument role label\nwith a unique integer string, giving a result like this when\nasking about any D EFENDANT arguments of conviction:\n[CLS] conviction ; 13 [SEP] crowds protested\nthe conviction of Ildar Dadin [SEP]. Figure 3\nshows a graphical representation.\nBoth forms of priming described above can be used to\neither generate BIO labels or label pre-identified candidate\narguments. We found priming by trigger and argument role\n4Different argument roles are possible for different ACE event\ntypes: our system only queries the roles allowed for the event type\nspecified for the trigger.\nto be generally more successful and is what we report below\nunless otherwise noted.\nWe note that when priming by trigger and argument role\ntogether, it is possible for the model to predict multiple roles\nfor a single span. This is consistent with the definition of\nthe ACE task, where there is no requirement that arguments\nwith different roles not overlap, and in fact sometimes they\ndo (e.g. the same text span plays more than one role in\nan event). For other event tasks where this might not be\nallowed, priming by the trigger alone would be the most\nstraightforward path.\nFinally, we also developed a priming model for trigger\ndetection and classification. Here, the input is primed\nwith a single token from the sentence. So, the language\nmodel again sees something like this: [CLS] conviction\n[SEP] crowds protested the conviction of\nIldar Dadin [SEP]. However, the question asked of the\nmodel is not about the arguments of conviction but about\nwhether conviction is itself part of a trigger, and if so,\nwhat kind. This model targets two training objectives. One\nobjective predicts the output span for the trigger with BIO\nlabelling with a bi-LSTM and CRF, similar to argument\nextraction. The other objective takes the concatenation of\nthe language model output for the trigger token and the\nclass token as input and applies a linear transformation to\npredict the event type.\nThus, in the example above, we expect conviction to be\nmarked as the trigger span and the lae nguage model outputs\nfrom [CLS] and conviction to lead the model to generate\n10629\nFigure 2: Priming a sentence for the trigger conviction. The span Ildar Dadin is identified as a DEFENDANT argument.\nFigure 3: Priming a sentence for the trigger conviction and the argument role DEFENDANT . The span Ildar Dadin is identified\nas an argument and is therefore assigned the role being queried (DEFENDANT ).\nFigure 4: Priming a sentence to determine whether conviction is part of a trigger span. The span conviction is identified as a\ntrigger of the type JUSTICE .CONVICT .\nJUSTICE .CONVICT as the event type; Figure 4 shows this\nin graphical form. The system outputs a trigger only if and\nonly if an event type is predicted; if BIO labelling provides\nno span overlapping with the priming token, the model out-\nputs a trigger with just the priming token as its span. When\nthe output for the trigger span and event type conflict, our\nsystem favors event type prediction. If no event type is pre-\ndicted, no trigger will be output, even when a trigger span is\npredicted. Likewise, if an event type is predicted, a trigger\nwill always be generated; if no predicted trigger span over-\nlaps with the primed token, span prediction is ignored, and\nthe output span is only the primed token.\nVariant: Pre-generated Candidate Arguments\nTo allow better comparison with prior cross-lingual work\n(e.g. Subburathinam et al. (2019)), we developed a variant\nof our system which takes as input pre-generated candidate\narguments instead of finding argument spans in the sentence\nat decode time. In this paper, we use this variant to produce\nresults in an experimental setting that uses gold entity men-\ntions as argument candidates. We adapt our architecture for\nthis setting in three small ways.\nFirst, it no longer makes sense to have the model con-\nsider arbitrary spans, so instead of all tokens, we present a\nsequence of candidate arguments and ask the model to clas-\nsify each with respect to a specified trigger. (We re-use the\nsame architecture: a single linear layer with a CRF-based\nloss function on top of the transformer stack.) For each can-\ndidate, we take as its representation the transformer output\nfor the candidate’s “most representative” token, ideally its\nlinguistic head. For instance, the model will use the vector\nfor Smith to represent Bob Smith, or the vector forstudent to\nrepresent the medical student. To select this token for each\ncandidate argument, we take the highest-ranking token in its\ndependency parse, if available5; if not, we simply select an\nargument’s first token. (Using dependency parses is not re-\nquired and typically provides only a small (1-2 point) gain\nover always selecting the first token.)\nSecond, we augment our argument model to consider en-\ntity type by including the output vector from an entity type\nembedding in the set of vectors concatenated for the candi-\ndate. This entity type embedding is trained along with the\nrest of the model.\nThird, we constrain our system to produce only “legal”\nargument/trigger pairs; that is, if no entity of type P ERSON\never appears as the P LACE argument to an A RREST -JAIL\nevent in training, neither do we allow it to do so at test time.\nWe note that this approach could also be applied to\nsystem-generated candidate arguments, should training data\nfor them exist (e.g. in the ACE2005 dataset). However, in\nour cross-lingual experiments, our primary system (which\n5Generated here using spaCy (https://spacy.io/) in English\nor the model trained from the Prague Arabic Dependency\nTreebank (https://github.com/UniversalDependencies/UD Arabic-\nPADT) for UDPipe (Straka and Strakov´a 2017) in Arabic.\n10630\nconsiders all possible spans as arguments) provided supe-\nrior results, so we did not pursue this variant further except\nwhen required for experiments using gold entity mentions.\nExperimental Setup\nWe report results in two experimental settings, both using\nthe ACE 2005 corpus (English and Arabic)6. We believe the\nfirst setting provides a more accurate and complete picture\nof the full event extraction task, but we include the second\n(which evaluates only event-argument role labeling) for a\nfull comparison to prior work.\nOur primary experimental settinguses the standard En-\nglish document train/dev/test splits for this dataset (Yang\nand Mitchell 2016) and the Arabic splits proposed by Xu\net al. (2021). In both cases, we draw sentence breaks from\nthe data made available by Xu et al. (2021), which is gener-\nated using the DyGIE++ codebase.7 We further split Arabic\nsentences at runtime to ensure a maximum length of 80 to-\nkens. (We still score against the unsplit reference.) We also\nuse Farasa (Abdelali et al. 2016) to remove tatweels and map\npresentation forms to the standard code range. All results re-\nported in this paper other than Table 2 use this experimental\nsetting and are the average of five seeds.\nOur secondary experimental setting evaluates only\nevent-argument role labeling. It replicates the conditions\nproposed by Subburathinam et al. (2019), which ask sys-\ntems to label individual instances consisting of a gold trig-\nger and a gold entity mention. The train/dev/test split does\nnot consider the origin of each instance, meaning that the\nsame sentence and trigger can be found in both train and test,\nwith different candidate arguments. This is inconsequential\nin a cross-lingual setting but should be taken into consider-\nation if assessing monolingual results. Moreover, following\nprior work, both training and test down-sample the number\nof negative instances to match the number of positive in-\nstances. For this reason, observed precision is dramatically\nover-inflated compared to a real-world scenario; reported F-\nmeasure should be considered in this light. The only table in\nthis paper using this experimental setting is Table 2.\nUnless otherwise specified, for language modelswe use\nthe large, cased version of BERT (Devlin et al. 2019) for\nthe monolingual English condition and the large version of\nXLM-RoBERTa (Conneau et al. 2020) for cross-lingual or\nArabic-only conditions. We use BERT for monolingual En-\nglish solely to ensure a fair comparison to prior work; XLM-\nRoBERTa could also be used with similar results.\nFollowing community practice for evaluation metrics,\ne.g. Zhang, Ji, and Sil (2019), we consider a trigger correct\nif its offsets and event type are correct, and we consider an\nargument correct if its offsets, event type, and role find a\nmatch in the ground truth.\n6https://www.ldc.upenn.edu/collaborations/past-projects/ace\n7https://github.com/dwadden/dygiepp\nResults\nArgument Extraction\nWe begin by assessing the impact of our new priming archi-\ntecture directly on argument extraction. To do this, we first\npresent results with gold triggers in our primary experimen-\ntal setting. Table 1 shows improvements from priming across\nthe board in both monolingual and cross-lingual conditions.\nWe also report English results here from Du and Cardie\n(2020), also inspired by a question-answering approach, and\nresults from all three conditions using a locally trained ver-\nsion of OneIE (Lin et al. 2020) constrained to gold triggers\nat test time. IE-P RIME comfortably out-performs both prior\nbaselines.\nIn order to compare against prior work in a cross-lingual\nsetting, we next present analogous results in our secondary\nexperimental setting.8 This setting represents the narrower\ntask of event argument role labeling (using gold triggers and\ngold entity mentions). Table 2 presents our results in this\nexperimental condition, where IE-P RIME shows more than\na six-point improvement over the best previously-reported\nresults (Ahmad, Peng, and Chang 2021).\nWe note that this secondary experimental condition is\nsomewhat artificial, not just in its reliance on gold entity\nmentions (both spans and entity types) but also in its ex-\nclusion of 90% of the negative instances during both train-\ning and test. To mitigate the significant dataset bias in this\ncondition, we ran a version of our system which took as its\noutput the union of arguments found by models trained with\nfive different seeds, allowing us to force the system to prior-\nitize recall.\nWe also show results for both priming variants for the\nsecondary experimental condition (priming by trigger only\nand by trigger and role together). In the primary experimen-\ntal condition, the trigger+role approach to priming is always\nbetter and that is what we typically report in this paper (e.g.\nin Table 1). However, here, the trigger-only version is supe-\nrior, perhaps because it allows the full model to see at once\nhow many arguments are being predicted for a given trigger\nand therefore again allows the model to favor higher recall\nto better approximate the (somewhat unrealistic) distribution\nin this version of the data.\nEnd-to-End System\nTo further situate our system within the state of the art,\nwe must consider an end-to-end system, since the currently\nbest-performing event extraction systems jointly optimize\nextraction of entity mentions, triggers and arguments. Our\nprimed system (as analyzed so far) focuses solely on im-\nproving argument extraction performance, and indeed our\nsimple model’s trigger performance lags behind the state of\nthe art, particularly in English. Our system also does not use\nentity annotation, which is used by both of the prior-work\nbaselines shown here. Still, we see in Table 3 that despite\nweaker trigger performance and without using any entity\n8We are not aware of any published full system results for\nEnglish→Arabic cross-lingual ACE event extraction.\n10631\nMono Cross\nen→en ar→ar en→ar\nDu and Cardie (2020) 65.4 – –\nLin et al. (2020) 69.3 56.3 36.4\nAhmad, Peng, and Chang (2021) – – 44.5\nIE-B ASELINE 63.2 53.3 44.7\nIE-P RIME 72.4 67.7 50.3\nTable 1: Gains in argument classification F1 score from priming for argument extraction using gold triggers in our primary\nexperimental setting. As for all reported results in this paper, IE-P RIME uses the trigger+role configuration for priming unless\notherwise specified. To provide Arabic and cross-lingual baselines, (Lin et al. 2020) and (Ahmad, Peng, and Chang 2021) are\nre-trained and constrained to gold triggers at test time. Because (Du and Cardie 2020) relies on an English query generation\nframework, we could not run it in the Arabic or cross-lingual condition.\nPriming method Recall Precision F-Measure\nSubburathinam et al. (2019) – 61.8\nAhmad, Peng, and Chang (2021) – 68.5\nIE-B ASELINE – 67.1 79.6 72.8\nIE-P RIME trigger + role 66.5 82.9 73.8\nIE-P RIME trigger 67.5 83.7 74.7\nTable 2: Secondary experimental setting: Argument classification F1 in the zero-shot cross-lingual condition (train on English,\ntest on Arabic) with gold triggers and gold entity mentions, following splits from (Subburathinam et al. 2019).\nannotation, our monolingual argument extraction in English\ncomes near that of the state-of-the-art system.9\nArabic event extraction performance has not been as\nwidely reported as English, and the Arabic training set is\nonly 40% of the size of the English training set, making it\na lower-resource condition. Following Xu et al. (2021), we\ntake as a baseline a DyGIE++ model trained on the Ara-\nbic dataset. Results in Arabic (monolingual or cross-lingual)\nconfirm the strength of our argument extraction approach\nwith respect to the state of the art, both in the lower-resource\nArabic monolingual condition (+4.5 over DyGIE++) and the\nzero-shot cross-lingual condition (+8.2 over DyGIE++).\nFinally, noting that triggers serve as a point of weakness\nin our baseline system, we further explore the possibilities\nof priming by testing our primed trigger model in the cross-\nlingual context. Although we see no gain in the monolingual\nconditions, we see a very significant gain in the cross-lingual\ncontext (from 42.4 to 51.0 for trigger classification), sug-\ngesting that the primary strength of priming is enabling the\nunderlying language model to compensate when the task-\nspecific training data is noisy (e.g. from another language).\nFurther Analysis\nError Analysis To focus on the impact of priming, we first\nanalyze differences between the output of IE-B ASELINE\nand IE-P RIME in the zero-shot English→Arabic context\n9Note that since this is an end-to-end system, trigger perfor-\nmance directly impacts argument extraction performance, as sys-\ntems are penalized for producing arguments for spurious triggers\nor missing arguments for missed triggers.\nFigure 5: Analysis of differences between IE-B ASELINE\nand IE-P RIME for zero-shot Arabic with gold triggers (av-\neraged over five seeds).\n(Figure 5). Cases in which a system output span overlaps\nwith with the gold but the exact span or role are incorrect\nare classified as substitutions; instances when both systems\nprovide outputs which overlap but differ in exact span or\nrole are marked similar. Results show that IE-P RIME pro-\nvides significantly more correct answers overall, adding an\nadditional 47 beyond the 100 found by both systems, while\nIE-B ASELINE adds only an additional 19 not found by IE-\nPRIME . The total number of substitutions remains constant,\n10632\nMonolingal Cross-lingual\nen→en ar→ar en→ar\ntrigger argument trigger argument trigger argument\nWadden et al. (2019) 69.7 48.8 61.5 44.4 41.6 22.0\nDu and Cardie (2020) 72.4 53.1 – – – –\nLin et al. (2020) 74.7 56.8 64.3 48.0 49.0 30.8\nIE-P RIME (arguments only) 71.2 55.3 61.2 48.9 42.4 30.2\nIE-P RIME (arguments + triggers) 68.1 52.9 60.2 48.7 51.0 32.4\nTable 3: Trigger and argument classification F1 for end-to-end systems in our primary experimental setting. The first version\nof IE-P RIME includes the baseline trigger component and the primed argument extraction component. The second version\nincludes both the primed trigger component and the primed argument extraction component. To provide baselines in the Arabic\nand cross-lingual conditions, (Wadden et al. 2019) and (Lin et al. 2020) are re-trained and run using the large version of XLM-\nRoBERTa. The baseline numbers reported for the monolingual English condition are taken from the original papers (and as\nwith all monolingual English results reported here, use the large version of BERT). Because (Du and Cardie 2020) relies on an\nEnglish query generation framework, we could not run it in the Arabic or cross-lingual condition.\nFigure 6: Classification of errors when training and testing\non English with gold triggers.\nand the increase in correct guesses for IE-P RIME yields a\n60% reduction in novel deletions (misses). Comparing dele-\ntions to insertions, the two systems are more likely to agree\non deletions (items missed by the model), while insertions\nare much more heterogeneous. This is intuitive: there are\nmore unique ways to hallucinate a false alarm argument than\nthere are unique correct answers to delete. We note that IE-\nPRIME does reduce novel insertions by a quarter compared\nto IE-B ASELINE , showing that the improvement from prim-\ning holds for both recall and precision.\nFor qualitative analysis, we focus on IE-P RIME in the\nmonolingual English condition.10 Figure 6 summarizes our\nobservations. Overall, we find the system evenly balanced\nbetween deletions (40% of all errors) and insertions (43%).\nAnother 17% are classified as substitutions, with only one of\nthe role (7%) or argument span (10%) being incorrect. We\nclassify observed errors into six categories:\n• Annotation judgment call (22%): The system decision\nseems plausibly correct to a new reader.\n• Span selection errors (14%): The model did not select the\ncorrect span for an argument, e.g. labellingHeadquarters\nwhen Security Headquarters is expected. A higher-level\nre-scoring of argument spans might be helpful here.\n10Only the system using the seed 1235 was examined.\nen→en ar→ar en→ar\nbase large base large base large\nIE-B ASELINE 60.0 66.0 46.9 53.3 35.6 44.7\nIE-P RIME 69.0 74.1 60.8 67.7 40.2 50.3\nTable 4: Comparison of argument classification F1 (using\ngold triggers) based on size of pretrained language model.\n• Inference required (21%): Sometimes the system task re-\nquires world knowledge and/or knowledge of document\ncontext beyond the local sentence (which our model does\nnot consider). For instance, an earlier sentence might give\na clue that a person is a smuggler, making them more\nlikely to be later found as a D EFENDANT in a CONVICT\nevent; incorporating document context is an important\nnext step for our approach.\n• Confusion due to adjacent events (13%): Some confusion\ncan arise due to the proximity in the text of other events.\nFor instance, in Davies is leaving to become chairman\nof the London School of Economics, our model correctly\nmarks London School of Economics as the Entity argu-\nment for become (chairman), but we incorrectly also la-\nbel it as the Entity for leaving. Adding a mechanism to\nensure consistency between events could help here.\n• Entity type (6%). A handful of errors involve entities\nwhose semantic class is easy to mistake, e.g. a model\nlikely not understanding Milton Keynes as a place name.\n• Other (24%): The remaining errors lack any ready expla-\nnation, but it appears that some might benefit from more\nexplicit modeling of syntactic information or a re-scoring\npass to consider event-to-event interaction.\nPretrained Model Size We believe the gains from prim-\ning come from giving the base language model more oppor-\ntunity to compensate for gaps in the training data. Another\ncommon way to improve the performance of an architec-\nture that relies on a pretrained language model is simply to\n10633\nFigure 7: Comparison of IE-B ASELINE and IE-P RIME by approximate training set size. Training size here is calculated as the\nnumber of events in a document set and is shown as a percentage of full English training set size. The language of the data in\nwhich the models were trained are denoted in parenthesis. Experiments in this figure use gold triggers.\nincrease the size of that model (either the number of param-\neters users, or the data sets it sees during training, or both).\nTable 4 presents the results of our baseline and primed mod-\nels with both the base and large versions of XLM-RoBERTa.\nAs we can see, the gains achieved from priming and from\nlanguage model size are complementary. In all conditions,\nwe see an improvement over baseline from adding either\npriming or moving to a larger language model, and again in\nall conditions, we see significant further gains from adding\nboth. We note that the absolute gains from adding priming\nare quite similar regardless of language model size.\nTraining Set Size We have hypothesized that priming is\nparticularly effective in low-resource conditions. To test this,\nwe vary the size of our training set and examine the relative\ngain from priming in each condition. For simplicity, Figure\n7 presents results only for argument extraction, with gold\ntriggers. Training size is calculated as the number of events\nin a document set and shown as a percentage of the English\ntraining set size; so, the full Arabic training set (1.7K events)\nis approximately equivalent to 40% of the English training\nset (4.2K events).\nWe make two primary observations. First, the gap be-\ntween the baseline and primed systems is indeed greatest\nin the lowest-resource settings, showing that the priming ar-\nchitecture provides more power when the model is under-\nresourced. For instance, when only 20% of the English data\nis available (∼850 events), we see a 12.8-point gain from\nadding priming; when 100% of the data is available, the gain\nis smaller (8.1 points). The same holds true for the Arabic\nmonolingual and English→Arabic cross-lingual conditions.\nSecond, we show that priming is able to overcome low-\nresource conditions rather significantly. In English, IE-\nPRIME requires only 20% of the training to nearly equal\nthe performance of IE-B ASELINE trained with 100% of the\ndata. The same is true in the cross-lingual condition. We also\nsee that in the this lowest-resource condition (20%, or∼850\nevents), the performance of IE-P RIME in the cross-lingual\ncondition is actually almost equal to the performance of IE-\nBASELINE trained on the same amount of native Arabic\ndata. All of these results show the utility of priming when\nonly a small amount of data (or data from the wrong lan-\nguage) is available.\nConclusions & Future Work\nWe have shown here that our novel priming architecture im-\nproves both trigger and argument detection and classifica-\ntion significantly over the state of the art in a zero-shot cross-\nlingual setting. Our approach also provides significant gains\nfor argument extraction in the monolingual context. How-\never, there are still many areas yet to be explored within this\nnew paradigm. For instance, prior work has shown improve-\nment by jointly modeling triggers and arguments; we expect\nthis would be an area of gain here as well. Prior work has\nalso shown the value of document-level information, none\nof which is exploited in our current work. We look forward\nto investigating the incorporation of these elements (and oth-\ners) as we continue to explore this promising paradigm.\nAcknowledgements\nThis research is based upon work supported in part by the\nOffice of the Director of National Intelligence (ODNI), In-\ntelligence Advanced Research Projects Activity (IARPA),\nvia Contract No. 2019-19051600007. The views and conclu-\nsions contained herein are those of the authors and should\nnot be interpreted as necessarily representing the official\npolicies, either expressed or implied, of ODNI, IARPA, or\nthe U.S. Government. The U.S. Government is authorized to\nreproduce and distribute reprints for governmental purposes\nnotwithstanding any copyright annotation therein.\nMany thanks to I-Hung Hsu and Kuan-Hao Huang for re-\ntraining and locally re-running the OneIE and GATE sys-\ntems and providing us with the baseline numbers reported in\nthis paper.\nReferences\nAbdelali, A.; Darwish, K.; Durrani, N.; and Mubarak, H.\n2016. Farasa: A Fast and Furious Segmenter for Arabic. In\nProceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics:\nDemonstrations, 11–16. San Diego, California: Association\nfor Computational Linguistics.\n10634\nAhmad, W. U.; Peng, N.; and Chang, K.-W. 2021. GATE:\nGraph Attention Transformer Encoder for Cross-lingual Re-\nlation and Event Extraction. In Proceedings of the AAAI\nConference on Artificial Intelligence.\nConneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V .;\nWenzek, G.; Guzm ´an, F.; Grave, E.; Ott, M.; Zettlemoyer,\nL.; and Stoyanov, V . 2020. Unsupervised Cross-lingual Rep-\nresentation Learning at Scale. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 8440–8451. Online: Association for Computational\nLinguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nDu, X.; and Cardie, C. 2020. Event Extraction by An-\nswering (Almost) Natural Questions. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing. Association for Computational Linguis-\ntics.\nFeng, R.; Yuan, J.; and Zhang, C. 2020. Probing and Fine-\ntuning Reading Comprehension Models for Few-shot Event\nExtraction. arXiv:2010.11325.\nLi, X.; Feng, J.; Meng, Y .; Han, Q.; Wu, F.; and Li, J. 2019a.\nA Unified MRC Framework for Named Entity Recognition.\narXiv preprint arXiv:1910.11476.\nLi, X.; Yin, F.; Sun, Z.; Li, X.; Yuan, A.; Chai, D.; Zhou, M.;\nand Li, J. 2019b. Entity-Relation Extraction as Multi-Turn\nQuestion Answering. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics ,\n1340–1350. Florence, Italy: Association for Computational\nLinguistics.\nLin, Y .; Ji, H.; Huang, F.; and Wu, L. 2020. A Joint Neural\nModel for Information Extraction with Global Features. In\nProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, 7999–8009. Online: Associ-\nation for Computational Linguistics.\nLiu, J.; Chen, Y .; Liu, K.; Bi, W.; and Liu, X. 2020. Event\nExtraction as Machine Reading Comprehension. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 1641–1651. On-\nline: Association for Computational Linguistics.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and\nDean, J. 2013. Distributed Representations of Words and\nPhrases and their Compositionality. In Burges, C. J. C.;\nBottou, L.; Welling, M.; Ghahramani, Z.; and Weinberger,\nK. Q., eds., Advances in Neural Information Processing Sys-\ntems, volume 26. Curran Associates, Inc.\nNguyen, M. V .; and Nguyen, T. H. 2021. Improving\nCross-Lingual Transfer for Event Argument Extraction with\nLanguage-Universal Sentence Structures. In Proceedings of\nthe Sixth Arabic Natural Language Processing Workshop ,\n237–243. Kyiv, Ukraine (Virtual): Association for Compu-\ntational Linguistics.\nPeters, M.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.;\nLee, K.; and Zettlemoyer, L. 2018. Deep Contextualized\nWord Representations. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), 2227–2237. New Orleans,\nLouisiana: Association for Computational Linguistics.\nScao, T. L.; and Rush, A. M. 2021. How Many Data Points\nis a Prompt Worth? CoRR, abs/2103.08493.\nStraka, M.; and Strakov ´a, J. 2017. Tokenizing, POS Tag-\nging, Lemmatizing and Parsing UD 2.0 with UDPipe. In\nProceedings of the CoNLL 2017 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies, 88–99.\nVancouver, Canada: Association for Computational Linguis-\ntics.\nSubburathinam, A.; Lu, D.; Ji, H.; May, J.; Chang, S.-F.;\nSil, A.; and V oss, C. 2019. Cross-lingual Structure Trans-\nfer for Relation and Event Extraction. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP) ,\n313–325. Hong Kong, China: Association for Computa-\ntional Linguistics.\nWadden, D.; Wennberg, U.; Luan, Y .; and Hajishirzi, H.\n2019. Entity, Relation, and Event Extraction with Con-\ntextualized Span Representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP) ,\n5784–5789. Hong Kong, China: Association for Computa-\ntional Linguistics.\nWu, W.; Wang, F.; Yuan, A.; Wu, F.; and Li, J. 2020. Core-\nfQA: Coreference Resolution as Query-based Span Predic-\ntion. In Proceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, 6953–6963. Online:\nAssociation for Computational Linguistics.\nXu, H.; Ebner, S.; Yarmohammadi, M.; White, A. S.;\nVan Durme, B.; and Murray, K. 2021. Gradual Fine-Tuning\nfor Low-Resource Domain Adaptation. In Proceedings of\nthe Second Workshop on Domain Adaptation for NLP, 214–\n221. Kyiv, Ukraine: Association for Computational Linguis-\ntics.\nYang, B.; and Mitchell, T. M. 2016. Joint Extraction of\nEvents and Entities within a Document Context. InProceed-\nings of the 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Hu-\nman Language Technologies, 289–299. San Diego, Califor-\nnia: Association for Computational Linguistics.\nZhang, T.; Ji, H.; and Sil, A. 2019. Joint Entity and Event\nExtraction with Generative Adversarial Imitation Learning.\nData Intelligence, 1: 99–120.\n10635",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7810375094413757
    },
    {
      "name": "Natural language processing",
      "score": 0.6172508597373962
    },
    {
      "name": "Priming (agriculture)",
      "score": 0.614016592502594
    },
    {
      "name": "Language model",
      "score": 0.589293360710144
    },
    {
      "name": "Sentence",
      "score": 0.5331729054450989
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49285411834716797
    },
    {
      "name": "Task (project management)",
      "score": 0.4881170690059662
    },
    {
      "name": "Event (particle physics)",
      "score": 0.4384188652038574
    },
    {
      "name": "Argument (complex analysis)",
      "score": 0.43019527196884155
    },
    {
      "name": "Transformer",
      "score": 0.42394915223121643
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Germination",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ]
}