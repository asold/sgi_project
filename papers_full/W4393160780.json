{
  "title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models",
  "url": "https://openalex.org/W4393160780",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5033124430",
      "name": "Qingyi Liu",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5091494473",
      "name": "Jinghui Qin",
      "affiliations": [
        "Guangdong University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5080482292",
      "name": "Wenxuan Ye",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5073038762",
      "name": "Hao Mou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101332438",
      "name": "Yuxuan He",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5088124671",
      "name": "Keze Wang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281684631",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6741121127",
    "https://openalex.org/W2885765530",
    "https://openalex.org/W2977235550",
    "https://openalex.org/W3092957620",
    "https://openalex.org/W2797227342",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W6784695618",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W2798851324",
    "https://openalex.org/W2793585215",
    "https://openalex.org/W3197754201",
    "https://openalex.org/W3002104146",
    "https://openalex.org/W6738394178",
    "https://openalex.org/W3096331697",
    "https://openalex.org/W4281493249",
    "https://openalex.org/W6691077208",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3088677288",
    "https://openalex.org/W4389520102",
    "https://openalex.org/W2965033324",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2962917899",
    "https://openalex.org/W4379958452",
    "https://openalex.org/W4385573257",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W2962937198",
    "https://openalex.org/W4389520255",
    "https://openalex.org/W3172152151",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3166699508",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4385573003",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3100727892",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2970476646"
  ],
  "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language models (LLMs) are prompt-sensitive and it is sub-optimal to apply the same prompt to any input for downstream TST tasks. Besides, the prompts obtained by searching are often unreadable and unexplainable to humans. To address these issues, we propose an Adaptive Prompt Routing (APR) framework to adaptively route prompts from a human-readable prompt set for various input texts and given styles. Specifically, we first construct a candidate prompt set of diverse and human-readable prompts for the target style. This set consists of several seed prompts and their variants paraphrased by an LLM. Subsequently, we train a prompt routing model to select the optimal prompts efficiently according to inputs. The adaptively selected prompt can guide the LLMs to perform a precise style transfer for each input sentence while maintaining readability for humans. Extensive experiments on 4 public TST benchmarks over 3 popular LLMs (with parameter sizes ranging from 1.5B to 175B) demonstrate that our APR achieves superior style transfer performances, compared to the state-of-the-art prompt-based and fine-tuning methods. The source code is available at https://github.com/DwyaneLQY/APR",
  "full_text": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with\nPre-trained Language Models\nQingyi Liu1, Jinghui Qin2, Wenxuan Ye3, Hao Mou4, Yuxuan He4, Keze Wang1*\n1Sun Yat-sen University\n2Guangdong University of Technology\n3X-Era AI Co., Ltd.\n4Datastory\n{scape1989,nbvincentelite,kezewang}@gmail.com, {mouhao, heyuxuan}@datastory.com.cn, liuqy95@mail2.sysu.edu.cn\nAbstract\nRecently, arbitrary text style transfer (TST) has made sig-\nnificant progress with the paradigm of prompt learning. In\nthis paradigm, researchers often design or search for a fixed\nprompt for any input. However, existing evidence shows that\nlarge language models (LLMs) are prompt-sensitive and it is\nsub-optimal to apply the same prompt to any input for down-\nstream TST tasks. Besides, the prompts obtained by search-\ning are often unreadable and unexplainable to humans. To ad-\ndress these issues, we propose an Adaptive Prompt Routing\n(APR) framework to adaptively route prompts from a human-\nreadable prompt set for various input texts and given styles.\nSpecifically, we first construct a candidate prompt set of di-\nverse and human-readable prompts for the target style. This\nset consists of several seed prompts and their variants para-\nphrased by an LLM. Subsequently, we train a prompt rout-\ning model to select the optimal prompts efficiently accord-\ning to inputs. The adaptively selected prompt can guide the\nLLMs to perform a precise style transfer for each input sen-\ntence while maintaining readability for humans. Extensive\nexperiments on 4 public TST benchmarks over 3 popular\nLLMs (with parameter sizes ranging from 1.5B to 175B)\ndemonstrate that our APR achieves superior style transfer\nperformances, compared to the state-of-the-art prompt-based\nand fine-tuning methods. The source code is available at\nhttps://github.com/DwyaneLQY/APR\nIntroduction\nText style transfer (TST) is a task in natural language gener-\nation that falls under controllable text generation (Hu et al.\n2017). Its objective is to alter the style attributes of the text\nwhile preserving its style-independent content and ensuring\nsmooth expression. The textual style includes different at-\ntributes within a text, such as emotions, politeness, humor,\nand formality. TST is widely used in application scenarios,\ne.g., data augmentation (Kaushik, Hovy, and Lipton 2020),\npersonalized dialogues (Zheng et al. 2021), and text detoxi-\nfication (Nogueira dos Santos, Melnyk, and Padhi 2018).\nThe main challenge of TST is the scarcity of parallel data.\nMost of the previous methods train models from scratch\n*Corresponding author.\nCopyright Â© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n  Input sentenceï¼š\nPrompt1\nOutput:\nPrompt2\nâˆš my first visit at the spa â€¦Ã—\nOur Routing Framework\nLLM (Frozen)\nPrompt2Prompt1âˆš Ã—\n Input sentenceï¼š\nPrompt1\nOutput:\n                     Prompt2\n \n  \nâˆšÃ—\n    (a)\n    (b)\nOur Routing Framework\nLLM (Frozen)\nPrompt2Prompt1Ã— âˆš\nFigure 1: Illustration of results from negative-to-positive\nsentiment transformation using GPT-2-XL with distinct\ntask-specific prompts for two input sentences sampled from\nYelp. For different input sentences, our routing framework\ncan adaptively select the prompt yielding a better style trans-\nformation.\nin an unsupervised manner, limiting their transfer perfor-\nmances. Recent research shows that fine-tuning pre-trained\nlanguage models on style transfer datasets can achieve bet-\nter transfer performance (Krishna, Wieting, and Iyyer 2020;\nLiu, Neubig, and Wieting 2021). However, these methods\nstill suffer from limited task data and expensive training /\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18689\nmaintaining costs of separate models for different styles.\nTo tackle this problem, researchers explore prompt-based\nmethods for TST, as well-designed prompts can elicit the\nzero-shot / few-shot generalization capability of large lan-\nguage models (LLMs) in downstream tasks (Brown et al.\n2020; Wei et al. 2022; Wang et al. 2022; Liu et al. 2023).\nPromising results have been obtained in arbitrary TST by\nusing manual design prompt templates, as demonstrated by\nReif et al. (2022) and Suzgun, Melas-Kyriazi, and Juraf-\nsky (2022). However, designing a suitable prompt manually\nis still challenging. Deng et al. (2022) proposed a discrete\nprompt search method based on reinforcement learning, and\ntheir experiments show that a fixed prompt obtained by au-\ntomatic prompt searching can usually achieve better style-\ntransfer performance.\nHowever, a fixed prompt from manual design or automatic\nsearching is suboptimal for TST since the data are linguis-\ntically diverse and LLMs are prompt-sensitive (Zhao et al.\n2021; Lu et al. 2021; Mishra et al. 2022; Zhu et al. 2023).\nAs shown in Figure 1, different sentences require different\nprompts to achieve optimal style transformation. For the first\nsentence in Figure 1(a), the LLM with Prompt 1 generates\na better result while the LLM with Prompt 2 generates bet-\nter for the second sentence in Figure 1(b). Besides, although\nautomatic prompt searching methods that generate a prompt\nfrom training data globally can perform better than manu-\nally designed prompts, the searched prompts are not human-\nreadable, making them hard to explain and adaptable under\npractical scenarios. Therefore, it is necessary to select differ-\nent suitable and human-readable prompts for different input\ntexts for more accurate style transfer.\nTo address the above-mentioned issues, we propose an\nAdaptive Prompt Routing (APR) framework to automati-\ncally select suitable, human-readable prompts for various\ninput sentences to further stimulate the style-transfer per-\nformance of the LLMs. First, we construct a set of diverse\nand human-readable candidate prompts for the target style-\ntransfer task. It includes several manually designed seed\nprompts with different degrees of detail about the task and\ntheir paraphrased variants generated via the LLM (e.g., GPT-\n3.5). These prompts are crucial in helping us identify suit-\nable prompts for a wide range of input sentences. Next, we\ntrain a prompt routing model on a small amount of artifi-\ncial training data, which is based on the set of prompts and\nraw text from original style transfer datasets. This training\nprocess is done in a supervised manner, with prompts la-\nbeled using an evaluation-related style-transfer score func-\ntion. This allows the prompt router to select the optimal\nprompt for each input sentence during testing. Finally, we\nchoose the highest-quality output from multiple options gen-\nerated by the LLM, all guided by the adaptively routed op-\ntimal prompt. This ensures that our APR achieves the best\npossible style-transfer quality.\nThe main contributions of this work are two-fold: i) To\nthe best of our knowledge, our APR framework is first to\nadaptively route prompts from a diverse and human-readable\nprompt set for various input texts and given styles in arbi-\ntrary TST. Besides, our APR is lightweight and training-\nefficient, making it easily adaptable to LLMs at different\nscales; ii) We analyze the limitations of existing prompt-\nbased approaches for arbitrary TST and highlight the im-\nportance of selecting suitable prompts for different input\nsentences while ensuring that the prompts are easy for hu-\nmans to understand. Extensive experiments on 4 public TST\ndatasets over 3 popular LLMs (with parameter sizes rang-\ning from 1.5B to 175B) demonstrate the effectiveness and\nsuperiority of our proposed APR.\nRelated Work\nPrompt Learning Prompt learning has become a popular\nparadigm for stimulating the powerful generalization ability\nof large language models (LLMs) to solve various down-\nstream tasks (Brown et al. 2020; Sanh et al. 2021; Wei et al.\n2022; Wang et al. 2022). However, LLMs are highly prompt-\nsensitive (Zhao et al. 2021; Lu et al. 2021; Mishra et al.\n2022; Zhu et al. 2023), making it challenging but necessary\nto find suitable prompts (Liu et al. 2023). While prompt en-\ngineering is a common choice (Petroni et al. 2019; Brown\net al. 2020; Jiang et al. 2020; Schick and Sch Â¨utze 2021;\nGao, Fisch, and Chen 2020), it depends heavily on expert\nknowledge and may not guarantee optimal performance.\nSoft prompts (Lester, Al-Rfou, and Constant 2021; Gu et al.\n2021; Li and Liang 2021; Liu et al. 2022, 2021), by opti-\nmizing continuous vectors through backpropagation tend to\nperform better but are expensive to optimize and challeng-\ning to explain. Another option is to optimize a fixed dis-\ncrete prompt directly (Shin et al. 2020; Deng et al. 2022),\nwhich searches for tokens in the vocabulary space, but these\nprompts still lack semantic meaning and are difficult to un-\nderstand. Recently, researchers used low perplexity to select\nhuman-readable prompts (Gonen et al. 2022). However, this\napproach still provides a fixed prompt for any input.\nText Style Transfer The TST methods are often unsuper-\nvised due to the absence of parallel data for supervised train-\ning (Xu et al. 2012; Zhang, Zhao, and LeCun 2015; Rao\nand Tetreault 2018). However, previous unsupervised ap-\nproaches, such as training TST models from scratch (Shen\net al. 2017; Li et al. 2018; Luo et al. 2019; John et al. 2019)\nor fine-tuning pre-trained models (Krishna, Wieting, and\nIyyer 2020; Liu, Neubig, and Wieting 2021), have limited\nperformance and require large amounts of training data. Re-\ncent studies have shown that prompt-based methods, lever-\naging the powerful generalization ability of LLMs, can make\nsignificant progress on arbitrary TST in a few-shot/zero-\nshot manner. Reif et al. (2022) improved LLMâ€™s zero-shot\nperformance for arbitrary TST by using task-related ex-\nemplars; Suzgun, Melas-Kyriazi, and Jurafsky (2022) pro-\nposed a reranking method to select a high-quality output\nfrom the LLM to improve performance. They both used\na fixed manually-designed prompt template to guide the\nLLM. Meanwhile, (Luo et al. 2023) proposed a prompt-\nbased editing method that is more controllable, yet more\ntime-consuming than autoregressive generation.\nPreliminary\nIn the arbitrary TST task, an input sentence x with an arbi-\ntrary source style s can be denoted as xs and a required tar-\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18690\nget style is denoted as t. This task aims to transfer sentence\nxs from source style s to target style t while maintaining its\nstyle-independent content. Besides, the output of style trans-\nfer is expressed as yt. Since we mainly explore the prompt-\nbased method for TST, we take an off-the-shelf LLM G(Â·)\n(e.g., GPT-2-XL) as our TST generator following the setting\nof prompt learning. To obtain a target textyt, we first fill the\nsource text xs into a textual string prompt q, which provides\nsufficient style transformation information to the LLM for\nperforming TST. The combined text of the source text xs\nand the prompt q is denoted q||xs1 . Subsequently, we feed\nit into LLM G(Â·) to generate the target text yt by sampling\nfrom the output distribution G(p||xs).\nAdaptive Prompt Routing\nWe introduce our Adaptive Prompt Routing (APR) frame-\nwork, which adaptively routes suitable prompts for the in-\nput sentences in the source style from a set of diverse and\nhuman-readable prompts. The overview of the inference\nphase in APR is shown in Figure 2. APR relies on two key\ncomponents: a set of diverse and human-readable prompts\nand a prompt router that selects a suitable prompt for each\ninput sentence. We manually design a few seed prompts and\nparaphrase them via GPT3.5 to get the entire set of candi-\ndate prompts. Then, we annotate each candidate prompt ac-\ncording to sampled raw texts from the origin dataset. Based\non the annotated prompt data, we train our prompt router\nin a supervised manner. During testing, for a given input\nsentence, we choose the prompt with the highest probabil-\nity of being optimal according to our prompt router and feed\nit with the input sentence to the LLM to perform target style\ntransfer. Finally, we obtain the generated output by rerank-\ning the multiple sampled outputs of the LLM.\nDiverse and Human-Readable Prompts\nTo increase the probability of finding more suitable prompts\nfor different input texts, we construct a diverse and human-\nreadable prompt set for the target transformation. Our pro-\ncedure, similar to (Gonen et al. 2022), involves two steps:\nmanually designing a few seed prompts describing the target\ntransformation and obtaining a more diverse prompt set with\nthese seed prompts and paraphrased variants via GPT3.5.\nSpecifically, for a given style transfer task, e.g., transform-\ning sentences from informal to formal, we need to write\nsome natural language instructions about this informal-to-\nformal transfer task as our seed prompts. (Reif et al. 2022)\nand (Suzgun, Melas-Kyriazi, and Jurafsky 2022) provided\nan example of manual style-transfer prompt. They take\na sentence rewrite type instruction as their style-transfer\nprompt template, like â€œHere is a text: which is informal [x]\nHere is a rewrite of the text, which is formal: â€, where [x]\nis the source text. We include it as one of our seed prompts,\nalong with prompts inspired by the effective prompt fram-\ning technique (Mishra et al. 2022). These seed prompts can\nprovide different degrees of style-transfer details. For exam-\nple, we write a prompt like â€œPlease complete the following\nsentence, correcting any spelling error, grammar error, or\ninformal expressions in the [x]â€. This type of prompt might\nbenefit the LLM to learn how to make sentences more for-\nmal, such as â€œcorrecting spelling errorsâ€, instead of simply\nsaying â€œ... Here is a rewrite of the text, which is formal:â€.\nAfter collecting a few seed prompts, we use GPT-3.5 to\ngenerate a larger and more diverse set of prompts using the\nparaphrasing instruction. The resulting prompts have similar\nmeanings but different expressions and formats. Our para-\nphrase instruction is â€œSuppose you are a prompting engi-\nneer. Now, I need some prompts to ask GPT 3.5 to [req].\nHere is an example prompt[seed] Please paraphrase it from\n3 to 5 more prompts that are of the same meaning but more\ndiverse in types, such as QA, dialogue, and so on.â€, where\n[req] is the requirement or description of the target style-\ntransfer task, and [seed] is one of the seed prompts. Benefit-\ning from the powerful language ability of GPT-3.5, the gen-\nerated prompts are often human-readable. To ensure read-\nability, we manually filter these prompts again. After com-\npleting the previous two steps, we can obtain a candidate\nprompt set Q consisting of seed prompts and their para-\nphrased variants for specific style transfer tasks. For each\nexperimental style-transfer task,|Q| is set to 20 unless speci-\nfied otherwise (see our supplementary file for details). These\ncandidate prompts are human-readable and insufficiently di-\nverse for the task description, prompt type, and expression.\nPrompt Router\nAfter constructing the diverse and human-readable prompt\nset, we develop an LLM-specific prompt router to route suit-\nable prompts for different input texts adaptively. Here, we\nfirst describe a basic yet important score function for mea-\nsuring the generation quality, which can be applied to guide\nthe training of the prompt router and the output selection\nat the LLM inference stage. Then, we introduce the prompt\nrouter in detail, including how to create an annotated dataset\nfor its training and the formalization of the APR framework.\nFinally, we introduce the inference procedure of style trans-\nfer based on the LLM and our prompt router in our APR.\nStyle-Transfer Score Function A style-transfer score\nfunction related to evaluation metrics is commonly used for\nmore directly accessing the model performance during train-\ning or selecting outputs during inference. We use a style-\ntransfer score function for annotating candidate prompts and\noutputs reranking as in (Suzgun, Melas-Kyriazi, and Juraf-\nsky 2022). Specifically, our score function has the same for-\nmulation with (Deng et al. 2022), which is defined as:\nS(xs, yt) =Content(xs, yt) +Style(yt), (1)\nwhere Content(Â·) and Style(Â·) measure the content similar-\nity between source text xs and output yt, and the target style\nstrength of yt, respectively. For the Content(Â·), we imple-\nment it with BERTScore (Zhang et al. 2020), a BERT-base\nmetric; For the Style(Â·), we implement it with a style clas-\nsifier trained on the sampled raw training text (100 samples\nper style) of the specific style-transfer task.\nPrompt Annotation What we expect our prompt router\nto do is adaptively select a prompt q that is better than other\nprompts in the candidate setQ for the input sentence xs. For\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18691\nInput Sentence ğ’™ğ’•ğ’†ğ’”ğ’•\nğ’”\nPrompt Router\nğ’‘(ğ’’||ğ’™) = ğ‘¹(ğ’’||ğ’™)\nâ‹¯\nğ’‘(ğ’’ğŸ||ğ’™ğ’”\nğ’•ğ’†ğ’”ğ’• =0.82\nğ’‘(ğ’’ğŸ||ğ’™ğ’”\nğ’•ğ’†ğ’”ğ’• =0.91\nâ‹¯\nğ’‘(ğ’’|ğ‘¸|||ğ’™ğ’”\nğ’•ğ’†ğ’”ğ’• =0.76\nCandidate Prompt Set ğ‘¸\nPrompt ğ’’|ğ‘¸|\nPrompt ğ’’ğŸ\nPrompt ğ’’ğŸ\nâ‹¯\nâ‹¯\n\"i am a real \nperson.â€œ \n\"i felt well the \nnight after.â€œ\ni was very sick \nthe night after.\"\nğ‘º(â‹…)\nStyle-\nTransfer \nScore \nFunction\nOutput ğ’šğ’•ğ’†ğ’”ğ’•ğŸ\nğ’•\nOutput ğ’šğ’•ğ’†ğ’”ğ’•ğŸ\nğ’•\nOutput ğ’šğ’•ğ’†ğ’”ğ’•ğ‘²\nğ’•\n0.62\n0.78\nâ‹¯\n0.95\"i felt well the \nnight after.â€œ\nOutput à·¢ğ’šğ’•\nInput (ğ’’ğŸ||ğ’™ğ’”\nğ’•ğ’†ğ’”ğ’•) \nInput  (ğ’’ğŸ||ğ’™ğ’”\nğ’•ğ’†ğ’”ğ’•)\nInput (ğ’’|ğ‘¸|||ğ’™ğ’”\nğ’•ğ’†ğ’”ğ’•)\nğ‘® â‹…\nPrompt ğ’’ğŸ\nâ‹¯ â‹¯\nSample ğ¾\noutputs \nfrom LLM\nPredict the \nprobability of \nthe prompt \nbeing optimal.\nInput (ğ’’ğ’ğ’‘ğ’•||ğ’™ğ’”\nğ’•ğ’†ğ’”ğ’•)\nLLM \nOptimal Prompt ğ’’ğ’ğ’‘ğ’•\n\"i was very sick the \nnight after.\"\n)\n)\n)\nFigure 2: The overview of our APR framework for inference. Given an input sentence xs\ntest with a negative sentiment. The\nprompt router R(Â·) of our APR predicts the optimal promptqopt from the task-specific prompt setQ = {qm}|Q|\nm=1 by calculating\nthe probability p(qm||xs\ntest) for each prompt qm. This selected prompt, combined with the input sentence as qopt||xs\ntest, is then\nfed into the LLM G(Â·) for style transformation. To obtain high-quality output, we sample multiple potential outputs yt\ntestj|K\nj=1\nfrom the LLM and score them using a style-transfer score function S(Â·). The output Ë†yt with the highest score, like â€œi felt well\nthe night afterâ€, is considered the final style-transfer result generated by the LLM.\nthis purpose, we train a prompt router in a supervised man-\nner. To obtain the labeled training data, we annotate each\ncandidate prompt for different input sentences.\nFirst, we sample a bunch of source style sentences Xs =\n{xs1, ..., xs\nn, ...xs\nN } unless otherwise specified) from the\nsource style dataset Ds. For input sentence xs\nn, we assign\na score independently to each prompt qm in the candidate\nprompt set Q = {q1, ..., qm, ..., q|Q|=M }. To score each\nprompt qm given input xs\nn, we first add the input sentence\nxs\nn to the candidate prompt qm to obtain the complete in-\nput qm||xs\nn. Then, we feed input qm||xs\nn to the LLM G(Â·),\nwhich generates K candidate outputs yt\nk by sampling from\nits output distribution P:\nP = G(qm||xs\nn). (2)\nNext, we score each candidate output by the above style-\ntransfer score function S(Â·) and finally assign a score om,n\nto prompt qm, where om,n is the maximum style-transfer\nscore of all sampled outputs:\nom,n = max(S(xs\nn, yt\n1), ..., S(xs\nn, yt\nk), ..., S(xs\nn, yt\nK)).\n(3)\nAfter scoring all the prompts in the candidate setQ for the\ninput xs\nn, we divide the setQ into a positive sample setQpos\nand a negative sample set Qneg. Here, we use a dynamic se-\nlection strategy based on the statistical characteristics of the\nprompt scores. First, we calculate the mean u and standard\ndeviation v of scores over candidate prompts for input xs\nn:\nu = 1\nM\nMX\nm=1\nom,n, (4)\nv2 = 1\nM âˆ’ 1\nMX\nm=1\n(om,n âˆ’ u)2. (5)\nThen, we apply h = u + v as the threshold for sorting out\nprompts. The prompts whose scores are greater than h are\nchosen as positive samples, denoting as Qpos, and the rest\nare as negative samples, denoting as Qneg.\nQpos = {qm|m = 1,2, ..., M, om,n â‰¥ h}, (6)\nQneg = {qm|m = 1,2, ..., M, om,n < h}, (7)\nFinally, we assign a hard label c âˆˆ C = {1, 0} to each\nprompt. For positive samples from set Qpos, we set their la-\nbels c = 1; for negative samples from setQneg, we set their\nlabels c = 0. We find that when the annotated prompt data is\nlimited, it is easier for the router to learn hard labels rather\nthan directly learning the computed floating point score. For\nevery source sentencexs\nn within the sampled datasetXs, we\nassign labels to each prompt qm in the candidate prompt set\nQ according to the style-transfer score.\nFormalization of APR Based on the annotated prompts\ndata, we train our prompt router R(Â·), which consists of\ntwo parts: a BERT encoder BERT (Â·) and one fully con-\nnected (FC) layer FFN (Â·) followed by a sigmoid function\nSigmoid(Â·). The prompt router R(Â·) takes the complete in-\nput qm||xs\nn as its encoding sequence and returns the prob-\nability p of the prompt qm to be optimal for input text xs\nn.\nThis can be formulated as:\np = R(qm||xs\nn) =Sigmoid(FFN (BERT (qm||xs\nn)))\n(8)\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18692\nIn our experiments, we initialize BERT (Â·) using a pre-\ntrained BERT-base model (Devlin et al. 2019) and take the\noutput of the CLS token as the input vector of the FC layer.\nThe training instance includes the input sentence, prompt,\nand annotated label: (qm||xn, c). To optimize the prompt\nrouter, we employ the Binary Cross-Entropy (BCE) loss as\nthe training objective. Then, we run the prompt router on all\nthe training instances based on the sampled data and com-\npute the average loss. After training, for a given input text\nand a prompt from the candidate set, our prompt router can\nprovide the optimal prompt.\nInference During testing, given an input sentence xs\ntest,\nwe adaptively identify the optimal promptqopt from the can-\ndidate set Q according to the prompt router (see Figure 2).\nFirst, we enumerate each prompt qm in the candidate set\nQ = {qm}|Q|=M\ni=1 and create the router input by adding\nthe input sentence xs\ntest to each candidate prompt to form\nqm||xs\ntest fed to prompt router R(Â·). The router predicts the\nprobability p(qm||xs\ntest) of each prompt being optimal for\nthe test input:\np(qm||xs\ntest) =R(qm||xs\ntest). (9)\nThe prompt qopt with the highest probability is regarded as\nthe optimal prompt:\nqopt = arg max\nqm\np(qm||xs\ntest), qm âˆˆ Q. (10)\nThen, We select the optimal prompt qopt along with input\nsentence xs\ntest for the LLM to perform style transfer. To ob-\ntain the high-quality generated output from the LLM, we use\nthe reranking method proposed by Suzgun, Melas-Kyriazi,\nand Jurafsky (2022). Specifically, We create the LLM input\nqopt||xs\ntest by adding xs\ntest to qopt and feed it to the LLM\nG(Â·). We sample K candidate outputs Y t\ntest = {yt\ntestj }K\nj=1\nfrom output distribution Ptest of the LLM:\nPtest = G(qopt||xs\ntest), (11)\nand we score all these candidate outputs via Equation (1)\nand select the output Ë†yt from Y t\ntest with highest score:\nË†yt = arg max\nyt\ntestj\nS(xs, yt\ntestj ), yt\ntestj âˆˆ Y t\ntest. (12)\nExperiments\nDatasets We conduct experiments on 4 public TST\ndatasets, including Yelp, Amazon, GYAFC, and Shake-\nspeare. Yelpis a sentiment transfer dataset with positive and\nnegative reviews (Zhang, Zhao, and LeCun 2015). Amazon\nis another sentiment transfer dataset including reviews from\nthe Amazon website (Li et al. 2018). GYAFC is a sentence\nformality transfer dataset (Rao and Tetreault 2018). In our\nexperiments, we select the Family & Relationships domain\nfor a fair comparison with prior work. Shakespeare is a\ndataset for Shakespeare authorship transfer (Xu et al. 2012).\nDatasets statistics are shown in the supplementary file.\nEvaluation Metrics We evaluate the performance of a\nTST model from three dimensions. The first is Style Trans-\nfer Strength(Sty.). It measures whether the generated sen-\ntence has the target style attribute. In our experiments, we\nfine-tune a classifier for each experimental dataset using the\npre-trained Roberta-Large (Liu et al. 2019). The second is\nContent Preservation(Con.). It measures the content sim-\nilarity of the generated output and source sentence. Follow-\ning (Deng et al. 2022), we use a BERT-based(Zhang et al.\n2020) metric to calculate the semantic similarity of the out-\nput and source sentence. The final is Fluency (Flu.), which\nmeasures whether the generated sentence is fluent. In our ex-\nperiments, we use a trained grammatical classifier (Krishna,\nWieting, and Iyyer 2020) to calculate fluency.\nTo evaluate the overall generation quality, we calculate\nthe average of sentence-level joint score J(Â·) over the test\nset Xs\ntest, following the definition in (Krishna, Wieting, and\nIyyer 2020):\nJ(Style, Content, Fluency) =\nX\nxs\ntestâˆˆXs\ntest\nStyle(xs\ntest) Â· Content(xs\ntest) Â· Fluency (xs\ntest)\n|Xs\ntest| .\n(13)\nWe also report the popular geometric mean (GM) of scores\nin all three dimensions.\nBaselines Fine-tuning and prompt-based baselines are as\nfollows:\nâ€¢ Fine-tuning(FT): Fine-tuning methods could serve as a\nstrong baseline, as they use the full training data to update\nthe LLM parameters. We choose DiRR(Liu, Neubig, and\nWieting 2021) and STRAP(Krishna, Wieting, and Iyyer\n2020) as our fine-tuning baseline. They fine-tune GPT-2-\nXL and GPT-2-Large, respectively.\nâ€¢ Prompt-and-Rerank (P&R) (Suzgun, Melas-Kyriazi,\nand Jurafsky 2022): P&R is the state-of-the-art prompt-\nbased method for TST by reranking. They conducted a\ndetailed discussion of specific syntax and semantics of\nthe prompt and manually designed a fixed prompt tem-\nplate for arbitrary TST, formally similar to the previous\nstudy (Reif et al. 2022).\nâ€¢ RLPrompt (Deng et al. 2022): RLPrompt is a state-of-\nthe-art discrete prompt search method. They use rein-\nforcement learning to search optimal prompt tokens with\nthe best performance on the validation set.\nâ€¢ Low Perplexity (LP)(Gonen et al. 2022): Gonen et al.\n(2022) recently proposed a method for automatic prompt\nselection, which chooses i (e.g., 3) lowest perplexity\nprompts in a human-readable prompt set.\nImplementation Details We conduct experiments on\nthree different scale LLMs: GPT-2-XL (Radford and Wu\n2019) with 1.5B parameters, GPT-J-6B (Wang and Komat-\nsuzaki 2021), and GPT-3.5-turbo with 175B parameters. For\nthe fine-tuning baselines, we evaluate their released outputs\nusing the same evaluation metric; for APR and other prompt-\nbased methods, we conduct experiments under a few-shot\nsetting (100 training examples per style). We train a few-\nshot classifier in the style-transfer scoring function with a\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18693\nsmall number of samples, which is used for labeling prompts\nand reranking. We employ the same decoding and rerank-\ning strategy for both APR and other prompt-based baselines.\nDecoding has been done with top-10 sampling for GPT-2-\nXL and GPT-J-6B, while GPT-3.5 uses its default setting.\nWe sample 30 candidate outputs for GPT-2-XL and GPT-J-\n6B, and 5 for GPT-3.5 due to cost considerations. For more\ndetails, please see the supplementary file.\nExperimental Results\nMain Results Table 1 shows the performance of our APR\nand baselines on different TST datasets and different scales\nLLMs. Compared to fine-tuning approaches, APR almost al-\nways generates outputs with better style-transfer quality on\ndifferent style-transfer datasets, as measured by joint score\nand geometric mean, despite using significantly less training\ndata (only 100-shot for each style). In addition, significantly\nhigher fluency scores indicate that APR consistently gener-\nates more fluent and grammatical text than fine-tuning base-\nlines. This is likely due to the inherent conflict between fine-\ntuning an LLM on style data and its pre-training task of pre-\ndicting the next token, which can compromise the modelâ€™s\nability to generate coherent text. In general, APR outper-\nforms previous fixed prompt-based approaches in terms of\nthe joint score J(Â·) and geometric mean (GM) on different\nscale LLMs and TST datasets. Compared to P&R, adap-\ntive prompts are significantly more effective in enhancing\nthe performance of GPT-2-XL on both GY AFC and Shake-\nspeare, leading to a remarkable 39% and 37% improvement\nin joint score, as well as 11% and 11% improvement in GM\nscore. The formality and authorship transfer tasks tend to be\nmore challenging than sentiment transfer, highlighting the\nneed for adaptively-routed prompts for LLMs with limited\nsize and ability to handle complex style-transfer tasks. and\nAPRâ€™s performance is better and more stable than RLPrompt\nand LP. Additionally, APR provides richer style-transfer in-\nformation to LLMS by routing different prompts, resulting\nin higher scores of style-transfer strength.\nHuman Evaluation To further verify the efficacy of APR,\nwe also conduct human evaluation. Here, we mainly focus\non the Amazon dataset, because APR doesnâ€™t demonstrate\na significant advantage over other baselines when applied to\nAmazon using GPT-2-XL, as indicated by automated met-\nrics. Following the procedure of (Suzgun, Melas-Kyriazi,\nand Jurafsky 2022), we select 50 source sentences from\nAmazon, with 25 positive and negative examples each, and\ncorresponding outputs of DiRR(Liu, Neubig, and Wieting\n2021), RLPrompt(Deng et al. 2022), P&R(Suzgun, Melas-\nKyriazi, and Jurafsky 2022), and APR. For a clearer com-\nparison, we include human references for each example. To\nensure a fair evaluation, we recruited three graduate stu-\ndents who are fluent in English to rate the different out-\nputs and human references based on a 1-5 Likert scale in\nterms of content preservation, target style strength, and flu-\nency. The evaluation is conducted on the same 50 examples,\nwith the order of the different outputs shuffled. The results\nin Tabel 3 demonstrate that our APR outperforms all base-\nlines in terms of preserving content and fluency. Although\nCon. Sty. Flu. J GM Con. Sty. Flu. J GM\nMethod Dataset:Yelp Dataset:Amazon\nGPT-2\nDiRR (FT) 76.6 94.0 73.1 54.1 80.7 77.5 53.5 74.1 28.9 67.5\nRLPrompt 67.3 76.7 91.0 44.2 77.7 59.5 47.6 90.6 23.6 63.5\nP&R 72.9 82.7 89.4 53.8 81.4 61.5 59.3 84.4 29.0 67.5\nLP 72.2 82.5 90.4 53.6 81.3 61.2 56.4 86.6 27.8 66.8\nAPR (Ours) 71.4 84.4 91.1 54.8 81.961.5 58.6 85.8 28.7 67.6\nGPT-J-6B\nP&R 74.7 84.8 90.1 56.5 82.9 64.6 55.6 87.0 28.2 67.8\nLP 75.8 84.1 90.8 57.6 83.3 66.9 53.6 88.3 29.8 68.1\nAPR (Ours) 78.6 83.0 90.2 59.1 83.8 56.1 68.0 89.6 31.4 69.9\nGPT-3.5-turbo\nP&R 69.7 86.1 96.0 57.4 83.2 61.9 55.3 95.8 32.4 68.9\nAPR (Ours) 67.7 94.7 94.5 60.3 84.6 62.5 58.9 94.3 33.9 70.3\nMethod Dataset: GYAFC Dataset: Shakespeare\nGPT-2\nDiRR (FT) 78.5 62.6 80.4 41.2 73.4 - - - - -\nSTRAP (FT) - - - - - 40.7 73.9 87.8 26.4 64.2\nRLPrompt 75.1 60.5 94.6 40.3 75.4 57.8 54.9 87.5 26.1 65.2\nP&R 61.3 65.1 94.0 34.5 72.1 57.0 44.7 85.7 20.4 60.2\nLP 65.1 71.8 95.4 42.0 76.3 55.8 57.8 87.8 26.4 65.6\nAPR (Ours) 67.2 78.3 96.5 47.8 79.855.5 60.3 89.5 28.0 66.9\nGPT-J-6B\nP&R 68.6 81.3 95.6 51.4 81.1 61.8 66.9 87.7 35.0 71.3\nLP 71.7 74.1 96.0 49.2 79.8 61.0 66.0 88.6 34.7 70.9\nAPR (Ours) 73.7 79.6 97.6 55.5 83.061.2 71.1 90.2 38.3 73.2\nGPT-3.5-turbo\nP&R 50.5 99.6 99.250.0 79.3 50.3 92.5 95.8 43.7 76.4\nAPR (Ours) 74.0 76.8 98.6 56.4 82.4 50.7 91.0 95.3 43.6 76.0\nTable 1: Comparison with baselines on different TST\ndatasets and LLMs. J(Â·) and GM are our main metrics,\nwhich measure the overall style-transfer quality of genera-\ntions. The best result on each aspect is highlighted in bold\nand the second best result is underscored.\nhaving a slightly lower style transfer strength compared to\nP&R, our APR obtains the closest style-transfer quality to\nground truth data, as evidenced by the geometric mean of\nthe scores across all three dimensions.\nEffectiveness Analysis of Adaptively Routing To ana-\nlyze the effectiveness of the router in APR, we compare\ntwo other strategies of selecting prompts from the prompt\nset: 1) Select a prompt randomly (for each input); 2) Select\na fixed prompt with the top style transfer score (STS) on\nthe validation set (we take the average performance of top-\n3 score prompts). The result is shown in Table 4. We can\nobserve that the performance of prompt selection based on\nstyle transfer score is significantly better than random selec-\ntion. Compared with the fixed prompts selected according\nto the score, the prompt selected adaptively by the router\nachieves better style transfer performance. This shows that\nadaptively routing prompts by our prompt router is effective\nand necessary in our APR framework.\nAnalysis of Positive Prompt Selection We analyze the\nimpact of different strategies for annotating positive prompts\nin APR on model performance. We compare two strategies:\n1) Dynamic: Select positive prompts for each input accord-\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18694\nSystem Prompt\nSource text My ex of 3 years cheated.\nGround Truth My three years ex cheated on me.\nRLPrompt My ex of 3 years cheated. Iran Include shortened plural Replace My ex of 3 years cheated.\nP&R My ex of 3 years cheated, and I told\nhim to go to counseling.\nHere is a text, which is informal: {My ex of 3 years cheated.} Here is a rewrite of the\ntext, which is more formal: {\nAPR My ex of three years cheated on me. User: Can you help me rephrase this informal text into a more formal tone? â€œMy ex of 3\nyears cheated.â€ Assistant: Of course! Hereâ€™s a more formal version: â€œ\nSource text If He Likes You, Youâ€™ll Know!\nGround Truth If he is interested in you, you will just\nknow.\nRLPrompt If he likes you, youâ€™ll know. Iran Include shortened plural Replace If He Likes You, Youâ€™ll Know!\nP&R If He likes you, youâ€™ll know. Here is a text, which is informal: {If He Likes You, Youâ€™ll Know!} Here is a rewrite of\nthe text, which is more formal: {\nAPR If he likes you, you will know. You come across an informal sentence that needs to be rewritten formally: â€œIf He Likes\nYou, Youâ€™ll Know!â€ How would you transform this sentence into a more formal tone? â€œ\nTable 2: Example outputs and their corresponding prompts from prompt-based methods on the informal to formal transfer task.\nImproper generations are highlighted in bold.\nSetting Con. Sty. Flu. GM\nGround-Truth 4.91 3.81 4.87 4.50\nDiRR 4.53 2.69 4.69 3.85\nRLPrompt 3.99 2.55 4.73 3.63\nP&R 4.45 3.59 4.61 4.19\nAPR 4.65 3.55 4.77 4.29\nTable 3: Human Evaluation on Amazon on the 5-Likert\nscore. The best result on each aspect is highlighted in bold\nand the second best result is underscored.\nSetting Con Sty. Flu. J GM\nRandom 59.1 72.9 96.2 38.3 74.5\nSTS 65.9 77.5 96.1 46.2 78.9\nRouter 67.2 78.3 96.5 47.8 79.8\nTable 4: Comparing of prompt routing strategies in APR on\nGY AFC using GPT-2-XL for inference.\ning to the statistical characteristics of their scores; 2) Top-i:\nSelect the top-i score prompts as the positive prompts. We\ncount the average number of prompts labeled as positive in\nall the prompt training data built for different styles, and the\naverage performance (we calculate the average score of each\nevaluation metric) on the 4 TST datasets with GPT-J-6B.\nThe average scores of J(Â·) and GM in Table 5 demonstrate\nthe dynamic strategy achieves the best performance.\nCase Study Table 2 shows the prompts and outputs of\nour APR and the previous prompt-based methods on the in-\nformal to formal transfer task of the GY AFC dataset. The\noptimal prompt from RLPrompt consists of five different\nwords â€œIran Include shortened plural Replaceâ€. However,\nthese words are challenging for humans to understand, and\nitâ€™s hard for us to intuitively understand how the LLM per-\nforms style transfer based on these messy words. Regard-\ning performance, we observe that fixed prompts struggle\nwith diverse sentences. Considering the informal sentence:\nâ€œIf He Likes You, Youâ€™ll Know!â€, RLPrompt and P&R suc-\nSetting #Avg. Pos. Con. Sty. Flu. J GM\nTop-i(i = 1) 166.7 68 71.9 90.5 43.8 76.1\nTop-i(i = 2) 329.5 66.4 75.1 91.8 45 76.9\nTop-i(i = 3) 475.8 67.7 73.5 91.2 44.9 76.8\nDynamic 329.0 67.4 75.4 91.9 46.1 77.5\nTable 5: Comparison of positive prompts selection strate-\ngies: Average performance with GPT-J-6B on 4 datasets.\n#Avg. Pos. indicates the average number of positive prompts\nacross all the prompt training data.\ncessfully alter the tone from exclamatory to declarative and\nmodify the rectify incorrect capitalization (P&R ignores the\nâ€œHâ€ in the second word â€œHeâ€), although they ignore the in-\nformal abbreviations â€œâ€™llâ€. However, for the sentence: â€œMy\nex of 3 years cheated. â€, RLPrompt directly copies it as out-\nput while P&R exhibits more uncontrollable by introducing\nexpansions without making any modifications to the source\nsentence. In contrast, for each informal source text, our APR\nadaptively routes two prompts in different types, leading the\nLLM to reach superior style transfer performance.\nConclusion\nIn this paper, we propose a novel prompt-based framework\nfor arbitrary text style transfer to effectively address the\nimportance of selecting suitable prompts for different in-\nput sentences while ensuring that they remain readable to\nhumans. Our APR framework, adaptively routing human-\nreadable task-specific prompts, is both resource-efficient and\neffective during training. Besides, our APR can be seam-\nlessly adapted to various sizes of LLMs. Through extensive\nexperiments on various TST datasets and LLMs, we demon-\nstrate that the APR framework achieves superior perfor-\nmance compared to existing prompt-based even fine-tuning\nmethods. In summary, this work contributes to the advance-\nment of TST research and highlights the importance of se-\nlecting appropriate prompts in achieving effective and effi-\ncient prompt-based TST systems.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18695\nAcknowledgements\nThis work was supported in part by the National Key\nR&D Program of China under Grant 2021ZD0111601, in\npart by the National Natural Science Foundation of China\n(NSFC) under Grant No. 62276283 and No. 62206314,\nin part by Guangdong Basic and Applied Basic Research\nFoundation under Grant No. 2023A1515012985 and No.\n2022A1515011835, in part by Basic and Applied Basic Re-\nsearch Special Projects under Grant SL2022A04J01685, and\nChina Postdoctoral Science Foundation funded project un-\nder Grant No. 2021M703687. We would also like to thank\nGuangdong Province Key Laboratory of Information Secu-\nrity Technology.\nReferences\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877â€“\n1901.\nDeng, M.; Wang, J.; Hsieh, C.-P.; Wang, Y .; Guo, H.; Shu,\nT.; Song, M.; Xing, E.; and Hu, Z. 2022. RLPrompt: Opti-\nmizing Discrete Text Prompts with Reinforcement Learn-\ning. In Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, 3369â€“3391.\nAbu Dhabi, United Arab Emirates: Association for Compu-\ntational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171â€“4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nGao, T.; Fisch, A.; and Chen, D. 2020. Making pre-trained\nlanguage models better few-shot learners. arXiv preprint\narXiv:2012.15723.\nGonen, H.; Iyer, S.; Blevins, T.; Smith, N. A.; and Zettle-\nmoyer, L. 2022. Demystifying Prompts in Language Models\nvia Perplexity Estimation. arXiv:2212.04037.\nGu, Y .; Han, X.; Liu, Z.; and Huang, M. 2021. PPT: Pre-\ntrained prompt tuning for few-shot learning. arXiv preprint\narXiv:2109.04332.\nHu, Z.; Yang, Z.; Liang, X.; Salakhutdinov, R.; and Xing,\nE. P. 2017. Toward Controlled Generation of Text. In Pre-\ncup, D.; and Teh, Y . W., eds., Proceedings of the 34th In-\nternational Conference on Machine Learning, volume 70\nof Proceedings of Machine Learning Research, 1587â€“1596.\nPMLR.\nJiang, Z.; Xu, F. F.; Araki, J.; and Neubig, G. 2020. How\ncan we know what language models know? Transactions of\nthe Association for Computational Linguistics, 8: 423â€“438.\nJohn, V .; Mou, L.; Bahuleyan, H.; and Vechtomova, O. 2019.\nDisentangled Representation Learning for Non-Parallel Text\nStyle Transfer. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, 424â€“434.\nFlorence, Italy: Association for Computational Linguistics.\nKaushik, D.; Hovy, E.; and Lipton, Z. 2020. Learning The\nDifference That Makes A Difference With Counterfactually-\nAugmented Data. In International Conference on Learning\nRepresentations.\nKrishna, K.; Wieting, J.; and Iyyer, M. 2020. Reformulating\nUnsupervised Style Transfer as Paraphrase Generation. In\nProceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), 737â€“762. On-\nline: Association for Computational Linguistics.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The\npower of scale for parameter-efficient prompt tuning. arXiv\npreprint arXiv:2104.08691.\nLi, J.; Jia, R.; He, H.; and Liang, P. 2018. Delete, Re-\ntrieve, Generate: a Simple Approach to Sentiment and Style\nTransfer. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long Papers), 1865â€“1874. New Orleans, Louisiana: As-\nsociation for Computational Linguistics.\nLi, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimiz-\ning Continuous Prompts for Generation. In Proceedings\nof the 59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long\nPapers), 4582â€“4597. Online: Association for Computational\nLinguistics.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2023. Pre-train, prompt, and predict: A systematic survey\nof prompting methods in natural language processing. ACM\nComputing Surveys, 55(9): 1â€“35.\nLiu, X.; Ji, K.; Fu, Y .; Tam, W.; Du, Z.; Yang, Z.; and Tang,\nJ. 2022. P-tuning: Prompt tuning can be comparable to fine-\ntuning across scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), 61â€“68.\nLiu, X.; Ji, K.; Fu, Y .; Tam, W. L.; Du, Z.; Yang, Z.; and\nTang, J. 2021. P-tuning v2: Prompt tuning can be compara-\nble to fine-tuning universally across scales and tasks. arXiv\npreprint arXiv:2110.07602.\nLiu, Y .; Neubig, G.; and Wieting, J. 2021. On Learning Text\nStyle Transfer with Direct Rewards. In Proceedings of the\n2021 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, 4262â€“4273. Online: Association for Compu-\ntational Linguistics.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. CoRR, abs/1907.11692.\nLu, Y .; Bartolo, M.; Moore, A.; Riedel, S.; and Stenetorp,\nP. 2021. Fantastically ordered prompts and where to find\nthem: Overcoming few-shot prompt order sensitivity. arXiv\npreprint arXiv:2104.08786.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18696\nLuo, F.; Li, P.; Zhou, J.; Yang, P.; Chang, B.; Sui, Z.;\nand Sun, X. 2019. A dual reinforcement learning frame-\nwork for unsupervised text style transfer. arXiv preprint\narXiv:1905.10060.\nLuo, G.; Han, Y . T.; Mou, L.; and Firdaus, M. 2023. Prompt-\nBased Editing for Text Style Transfer. arXiv preprint\narXiv:2301.11997.\nMishra, S.; Khashabi, D.; Baral, C.; Choi, Y .; and Hajishirzi,\nH. 2022. Reframing Instructional Prompts to GPTkâ€™s Lan-\nguage. In Findings of the Association for Computational\nLinguistics: ACL 2022, 589â€“612. Dublin, Ireland: Associa-\ntion for Computational Linguistics.\nNogueira dos Santos, C.; Melnyk, I.; and Padhi, I. 2018.\nFighting Offensive Language on Social Media with Unsu-\npervised Text Style Transfer. InProceedings of the 56th An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 2: Short Papers), 189â€“194. Melbourne, Aus-\ntralia: Association for Computational Linguistics.\nPetroni, F.; RocktÂ¨aschel, T.; Riedel, S.; Lewis, P.; Bakhtin,\nA.; Wu, Y .; and Miller, A. 2019. Language Models as\nKnowledge Bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 2463â€“2473. Hong\nKong, China: Association for Computational Linguistics.\nRadford, A.; and Wu, J. 2019. Rewon child, david luan,\ndario amodei, and ilya sutskever. 2019. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8): 9.\nRao, S.; and Tetreault, J. 2018. Dear Sir or Madam, May\nI Introduce the GY AFC Dataset: Corpus, Benchmarks and\nMetrics for Formality Style Transfer. In Proceedings of the\n2018 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), 129â€“140. New Or-\nleans, Louisiana: Association for Computational Linguis-\ntics.\nReif, E.; Ippolito, D.; Yuan, A.; Coenen, A.; Callison-Burch,\nC.; and Wei, J. 2022. A Recipe for Arbitrary Text Style\nTransfer with Large Language Models. In Proceedings\nof the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers), 837â€“848.\nDublin, Ireland: Association for Computational Linguistics.\nSanh, V .; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika, L.;\nAlyafeai, Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.;\net al. 2021. Multitask prompted training enables zero-shot\ntask generalization. arXiv preprint arXiv:2110.08207.\nSchick, T.; and Sch Â¨utze, H. 2021. Exploiting Cloze-\nQuestions for Few-Shot Text Classification and Natural Lan-\nguage Inference. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Computational\nLinguistics: Main Volume, 255â€“269. Online: Association for\nComputational Linguistics.\nShen, T.; Lei, T.; Barzilay, R.; and Jaakkola, T. 2017. Style\ntransfer from non-parallel text by cross-alignment. Ad-\nvances in neural information processing systems, 30.\nShin, T.; Razeghi, Y .; Logan IV , R. L.; Wallace, E.; and\nSingh, S. 2020. AutoPrompt: Eliciting Knowledge from\nLanguage Models with Automatically Generated Prompts.\nIn Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), 4222â€“4235.\nSuzgun, M.; Melas-Kyriazi, L.; and Jurafsky, D. 2022.\nPrompt-and-Rerank: A Method for Zero-Shot and Few-Shot\nArbitrary Textual Style Transfer with Small Language Mod-\nels. In Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, 2195â€“2222.\nAbu Dhabi, United Arab Emirates: Association for Compu-\ntational Linguistics.\nWang, B.; and Komatsuzaki, A. 2021. GPT-J-6B: A 6 billion\nparameter autoregressive language model.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2022. Self-consistency\nimproves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824â€“24837.\nXu, W.; Ritter, A.; Dolan, B.; Grishman, R.; and Cherry, C.\n2012. Paraphrasing for Style. In Proceedings of COLING\n2012, 2899â€“2914. Mumbai, India: The COLING 2012 Or-\nganizing Committee.\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi,\nY . 2020. BERTScore: Evaluating Text Generation with\nBERT. In 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nZhang, X.; Zhao, J.; and LeCun, Y . 2015. Character-level\nconvolutional networks for text classification. Advances in\nneural information processing systems, 28.\nZhao, Z.; Wallace, E.; Feng, S.; Klein, D.; and Singh, S.\n2021. Calibrate before use: Improving few-shot perfor-\nmance of language models. In International Conference on\nMachine Learning, 12697â€“12706. PMLR.\nZheng, Y .; Chen, Z.; Zhang, R.; Huang, S.; Mao, X.; and\nHuang, M. 2021. Stylized Dialogue Response Generation\nUsing Stylized Unpaired Texts. Proceedings of the AAAI\nConference on Artificial Intelligence, 35(16): 14558â€“14567.\nZhu, K.; Wang, J.; Zhou, J.; Wang, Z.; Chen, H.; Wang,\nY .; Yang, L.; Ye, W.; Gong, N. Z.; Zhang, Y .; et al. 2023.\nPromptBench: Towards Evaluating the Robustness of Large\nLanguage Models on Adversarial Prompts. arXiv preprint\narXiv:2306.04528.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n18697",
  "topic": "Style (visual arts)",
  "concepts": [
    {
      "name": "Style (visual arts)",
      "score": 0.6859219074249268
    },
    {
      "name": "Transfer (computing)",
      "score": 0.6211819052696228
    },
    {
      "name": "Computer science",
      "score": 0.582038938999176
    },
    {
      "name": "Routing (electronic design automation)",
      "score": 0.5396149158477783
    },
    {
      "name": "Natural language processing",
      "score": 0.5196369886398315
    },
    {
      "name": "Adaptive routing",
      "score": 0.48988693952560425
    },
    {
      "name": "Language model",
      "score": 0.48666054010391235
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40338805317878723
    },
    {
      "name": "Static routing",
      "score": 0.20957612991333008
    },
    {
      "name": "Computer network",
      "score": 0.15824812650680542
    },
    {
      "name": "Art",
      "score": 0.12445533275604248
    },
    {
      "name": "Routing protocol",
      "score": 0.10808464884757996
    },
    {
      "name": "Parallel computing",
      "score": 0.08669453859329224
    },
    {
      "name": "Literature",
      "score": 0.07774677872657776
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I139024713",
      "name": "Guangdong University of Technology",
      "country": "CN"
    }
  ]
}