{
    "title": "NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-Task Financial Forecasting",
    "url": "https://openalex.org/W4226288830",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2771903241",
            "name": "Linyi Yang",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A2109843912",
            "name": "Jiazheng Li",
            "affiliations": [
                "University of Warwick"
            ]
        },
        {
            "id": "https://openalex.org/A2122125241",
            "name": "Ruihai Dong",
            "affiliations": [
                "University College Dublin"
            ]
        },
        {
            "id": "https://openalex.org/A2098449489",
            "name": "Yue Zhang",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A1973587121",
            "name": "Barry Smyth",
            "affiliations": [
                "University College Dublin"
            ]
        },
        {
            "id": "https://openalex.org/A2771903241",
            "name": "Linyi Yang",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A2109843912",
            "name": "Jiazheng Li",
            "affiliations": [
                "University of Warwick"
            ]
        },
        {
            "id": "https://openalex.org/A2122125241",
            "name": "Ruihai Dong",
            "affiliations": [
                "University College Dublin"
            ]
        },
        {
            "id": "https://openalex.org/A2098449489",
            "name": "Yue Zhang",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A1973587121",
            "name": "Barry Smyth",
            "affiliations": [
                "University College Dublin"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2158595111",
        "https://openalex.org/W6732735474",
        "https://openalex.org/W6796591647",
        "https://openalex.org/W6795890748",
        "https://openalex.org/W2950744025",
        "https://openalex.org/W2896309423",
        "https://openalex.org/W6683713652",
        "https://openalex.org/W2126267628",
        "https://openalex.org/W6697136110",
        "https://openalex.org/W3035389441",
        "https://openalex.org/W2865687066",
        "https://openalex.org/W2949995675",
        "https://openalex.org/W2159931808",
        "https://openalex.org/W6676914691",
        "https://openalex.org/W1967885203",
        "https://openalex.org/W2136999286",
        "https://openalex.org/W7067383162",
        "https://openalex.org/W6767140673",
        "https://openalex.org/W2137492577",
        "https://openalex.org/W2152136804",
        "https://openalex.org/W6674886979",
        "https://openalex.org/W6681703846",
        "https://openalex.org/W6764269546",
        "https://openalex.org/W2566047469",
        "https://openalex.org/W2950906777",
        "https://openalex.org/W3102772997",
        "https://openalex.org/W3092768812",
        "https://openalex.org/W2974596953",
        "https://openalex.org/W2251709641",
        "https://openalex.org/W2562607067",
        "https://openalex.org/W6797500085",
        "https://openalex.org/W6760710662",
        "https://openalex.org/W2798413829",
        "https://openalex.org/W2969439812",
        "https://openalex.org/W3012629428",
        "https://openalex.org/W2911971986",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2960440238",
        "https://openalex.org/W2106334424",
        "https://openalex.org/W2964413206",
        "https://openalex.org/W3174583470",
        "https://openalex.org/W3122183745",
        "https://openalex.org/W2335156248",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2949206061",
        "https://openalex.org/W4212865381",
        "https://openalex.org/W3169039070",
        "https://openalex.org/W2997646596",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2313384944",
        "https://openalex.org/W4205812715",
        "https://openalex.org/W2986266667",
        "https://openalex.org/W2116209939",
        "https://openalex.org/W4288548690",
        "https://openalex.org/W2897494692",
        "https://openalex.org/W2950645060",
        "https://openalex.org/W2296438605",
        "https://openalex.org/W2576688235",
        "https://openalex.org/W3122648113",
        "https://openalex.org/W3124689050",
        "https://openalex.org/W2927690792",
        "https://openalex.org/W3124141334"
    ],
    "abstract": "Financial forecasting has been an important and active area of machine learning research because of the challenges it presents and the potential rewards that even minor improvements in prediction accuracy or forecasting may entail. Traditionally, financial forecasting has heavily relied on quantitative indicators and metrics derived from structured financial statements. Earnings conference call data, including text and audio, is an important source of unstructured data that has been used for various prediction tasks using deep earning and related approaches. However, current deep learning-based methods are limited in the way that they deal with numeric data; numbers are typically treated as plain-text tokens without taking advantage of their underlying numeric structure. This paper describes a numeric-oriented hierarchical transformer model (NumHTML) to predict stock returns, and financial risk using multi-modal aligned earnings calls data by taking advantage of the different categories of numbers (monetary, temporal, percentages etc.) and their magnitude. We present the results of a comprehensive evaluation of NumHTML against several state-of-the-art baselines using a real-world publicly available dataset. The results indicate that NumHTML significantly outperforms the current state-of-the-art across a variety of evaluation metrics and that it has the potential to offer significant financial gains in a practical trading context.",
    "full_text": "NumHTML: Numeric-Oriented Hierarchical Transformer Model\nfor Multi-Task Financial Forecasting\nLinyi Yang,1,2 Jiazheng Li,4 Ruihai Dong,3 Yue Zhang,1,2 Barry Smyth3\n1 Westlake Institute for Advanced Study, Westlake University\n2 School of Engineering, Westlake University\n3 School of Computer Science, University College Dublin\n4 Department of Computer Science, University of Warwick\nlinyi.yang, yue.zhang@westlake.edu.cn, ruihai.dong, barry.smyth@insight-centre.org, jiazheng.li@warwick.ac.uk\nAbstract\nFinancial forecasting has been an important and active area\nof machine learning research because of the challenges it\npresents and the potential rewards that even minor improve-\nments in prediction accuracy or forecasting may entail. Tra-\nditionally, Ô¨Ånancial forecasting has heavily relied on quanti-\ntative indicators and metrics derived from structured Ô¨Ånan-\ncial statements. Earnings conference call data, including text\nand audio, is an important source of unstructured data that\nhas been used for various prediction tasks using deep earn-\ning and related approaches. However, current deep learning-\nbased methods are limited in the way that they deal with\nnumeric data; numbers are typically treated as plain-text to-\nkens without taking advantage of their underlying numeric\nstructure. This paper describes a numeric-oriented hierarchi-\ncal transformer model (NumHTML ) to predict stock returns,\nand Ô¨Ånancial risk using multi-modal aligned earnings calls\ndata by taking advantage of the different categories of num-\nbers (monetary, temporal, percentages etc.) and their magni-\ntude. We present the results of a comprehensive evaluation\nof NumHTML against several state-of-the-art baselines using\na real-world publicly available dataset. The results indicate\nthat NumHTML signiÔ¨Åcantly outperforms the current state-\nof-the-art across a variety of evaluation metrics and that it has\nthe potential to offer signiÔ¨Åcant Ô¨Ånancial gains in a practical\ntrading context.\nIntroduction\nIt is the very nature of the stock market that even the most\nmodest of advantages (e.g. speed of trade) can be parlayed\ninto signiÔ¨Åcant Ô¨Ånancial rewards, and thus traders have long\nbeen attracted to the idea of using historical data to predict\nfuture stock market trends. However, the stochastic nature of\nthe stock market has proved to be very challenging when it\ncomes to provide accurate future forecasts, especially when\nrelying on pricing data alone (Moskowitz, Ooi, and Pedersen\n2012; Kristjanpoller, Fadic, and Minutolo 2014; Manela and\nMoreira 2017; Zheng et al. 2019; Pitk¬®aj¬®arvi, Suominen, and\nVaittinen 2020). However, recent advances in natural lan-\nguage processing (NLP) and deep learning (DL) introduce\nnovels sources of data ‚Äî textual data in the form of Ô¨Ånan-\ncial news (Ding et al. 2014; Yang et al. 2018; Hu et al. 2018;\nChen et al. 2019a; Du and Tanaka-Ishii 2020) and Ô¨Ånancial\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nreports (Duan et al. 2018; Kogan et al. 2009) to real-time so-\ncial media (Xu and Cohen 2018; Feng et al. 2018) ‚Äî which\nmay lead to be effective forecasting predictions. Of partic-\nular relevance to this paper is the earnings call data (Kim-\nbrough 2005; Wang and Hua 2014; Qin and Yang 2019)\nthat typically accompany the (quarterly) earnings reports of\npublicly traded companies. The multi-modal data associated\nwith these reports include the textual data of the report it-\nself plus the audio of the so-called earnings call where the\nreport is presented to relevant parties, including a question-\nand-answer session with relevant company executives. The\nintuition is that the content of such a report and the nature\nof the presentation and Q&A may encode valuable informa-\ntion to determine how a company may perform in the com-\ning quarter and, more immediately relevant, how the market\nwill respond to the earning report.\nPrevious work on using earnings conference calls has\nmostly considered the volatility prediction (Qin and Yang\n2019; Yang et al. 2020; Sawhney et al. 2020; Ye, Qin, and\nXu 2020), to predict the subsequent stock price Ô¨Çuctuation\nover a speciÔ¨Åed period (e.g., three days or seven days) after\nthe earnings call (Bollerslev, Patton, and Quaedvlieg 2016;\nRekabsaz et al. 2017). An even more challenging, yet poten-\ntially more valuable signal, especially when it comes to op-\ntimizing a real-world trading strategy, is the predicted stock\nreturn. While this has been explored by using Ô¨Ånancial news\ndata (Ding et al. 2014, 2015; Chang et al. 2016; Duan et al.\n2018; Yang et al. 2019; Du and Tanaka-Ishii 2020) and an-\nalyst reports (Kogan et al. 2009; Loughran and McDonald\n2011; Chen, Huang, and Chen 2021b), the use of earnings\ncall data remains largely unexplored. One notable excep-\ntion is Sawhney et al. (2020), which considers stock move-\nment prediction as an auxiliary task for enhancing the Ô¨Ånan-\ncial risk predictions. In particular, they show that multi-task\nlearning is useful for improving volatility prediction at the\nexpense of lower price movement prediction accuracy.\nThe main objective of this work is to explore the use of\nearnings calls data for stock movement prediction. The start-\ning point for this work is the multi-task learning approach\ndescribed by (Yang et al. 2020), which leverages textual and\naudio earnings call data with additional vocal features ex-\ntracted by Praat (Boersma and Van Heuven 2001). We quan-\ntify the effectiveness of textual and vocal information from\nearnings calls to predict stock movement using this baseline\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11604\nmodel and then go on to extend this model (Yang et al. 2020)\nby describing three auxiliary loss functions to investigate the\nutility of a more sophisticated representation of numerical\ndata during prediction. Thus the central advance in this work\nis a novel, numeric-oriented hierarchical transformer model\n(NumHTML) for the prediction of stock returns, using multi-\nmodal aligned earnings calls data by taking advantage of the\nauxiliary task (volatility prediction), different categories of\nnumerical data (monetary, temporal, percentages etc.), and\ntheir magnitude, motivated by the fact that volatility is a rele-\nvant factor to future stock trends and the assumption that bet-\nter numerical understanding can beneÔ¨Åt forecasting. These\ncomponents are integrated through a novel structured adap-\ntive pre-training strategy and Pareto Multi-task Learning.\nWe present the results of a comprehensive evaluation on\na real-world earnings call dataset to show how the model\ncan be more effective than the baseline system, facilitating\nmore accurate stock returns predictions without compromis-\ning volatility prediction (Qin and Yang 2019; Yang et al.\n2020; Sawhney et al. 2020). Also, the results of a realistic\ntrading simulation shows how our approach can generate a\nsigniÔ¨Åcant arbitrage proÔ¨Åt using a real-world trading dataset.\nAll code and datasets will be released on GitHub.\nRelated Work\nThis paper brings together several areas of related work ‚Äì\nStock Movement Predictions,Multi-modal Aligned Earnings\nCall, and Representing Numbers in Language Models ‚Äì and\nin what follows, we brieÔ¨Çy summarise the relevant state-of-\nthe-art in each of these areas as it relates to our approach.\nWe are the Ô¨Årst to examine whether pre-trained models with\nbetter numerical understanding can improve performance on\nÔ¨Ånancial forecasting tasks based on the multi-modal data.\nStock Movement Prediction\nWhile there has been a long-standing effort when it comes to\napplying machine learning techniques to Ô¨Ånancial prediction\n(Da, Engelberg, and Gao 2015; Xing, Cambria, and Welsch\n2018; Xing, Cambria, and Zhang 2019), typically by using\ntime-series pricing data, reliable and robust predictions have\nproven to be challenging due to the stochastic nature of stock\nmarkets. However, recent work has shown some promise\nwhen it comes to predicting stock price movements using\ndeep neural networks with rich textual information from Ô¨Å-\nnancial news and social media (primarily Twitter) (Liu and\nTse 2013; Ding et al. 2014, 2015; Xu and Cohen 2018; Duan\net al. 2018; Yang et al. 2018; Feng et al. 2018). By taking\nadvantage of much richer sources of relevant data (news re-\nports, export commentaries etc.), deep learning techniques\nhave been able to generate more robust and accurate predic-\ntions even in the face of market volatility.\nElsewhere, researchers have considered the role of opin-\nions in Ô¨Ånancial prediction. For example, one recent study\n(Chen, Huang, and Chen 2021a) has shown that the opinions\nfrom company executives and managers or Ô¨Ånancial analysts\ncan be more effective than the opinions of amateur investors\nwhen it comes to predicting the stock price. However, pre-\nvious works using earnings calls data typically focus on the\nÔ¨Ånancial risk (volatility) prediction, while whether volatil-\nity prediction can help predict movement has been less well\ncovered.\nRepresenting Numbers in Language Models\nCurrent language models treat numbers within the text in-\nput as plain words without understanding the basic nu-\nmeric concepts. Given the ubiquity of numbers, their im-\nportance in Ô¨Ånancial datasets, and their fundamental differ-\nence with words, developing richer representations of num-\nbers could improve the model‚Äôs performance in downstream\nÔ¨Ånancial applications (Chen et al. 2019b; Sawhney et al.\n2020). Progress towards deeper numerical representations\nhas been limited but promising. For example, previous work,\nrepresented by the DROP (Dua et al. 2019), presents a va-\nriety of numerical reasoning problems. Different from the\nexisting works that pay attention to explore the capability of\npretrained language models for general common-sense rea-\nsoning (Zhang et al. 2020), and math word problem-solving\n(Wu et al. 2021), we focus on improving the numeral un-\nderstanding ability of language models for Ô¨Ånancial fore-\ncasting, motivated by the fact that Ô¨Ånancial documents often\ncontain massive amounts of numbers. In particular, we con-\nsider two tasks ‚Äì Numeral Category ClassiÔ¨Åcation (Chen,\nWei, and Huang 2018; Chen et al. 2019b; Chen, Huang,\nand Chen 2021a) and Magnitude Comparison (Wallace et al.\n2019; Naik et al. 2019) ‚Äì using a structured adaptive pre-\ntraining strategy to improve the capability of pretrained lan-\nguage models for multi-task Ô¨Ånancial forecasting.\nMulti-modal Aligned Earnings Call Data\nEarnings conference call is typically presented by leading\nexecutives of publicly listed companies and provide an op-\nportunity for the company to present an explanation of its\nquarterly results, guidance for the upcoming quarter, and\nan opportunity for some in-depth Q&A between the audi-\nence and company management (Keith and Stent 2019). An\nearly study (Larcker and Zakolyukina 2012) mentioned that\ntext-based models could reveal misleading information dur-\ning earnings calls and cause stock price swings in Ô¨Ånan-\ncial markets and most unstructured data resources are still\ntext data. So far, the multi-modal aligned earnings call data\nmainly refers to the sentence-level text-audio paired data re-\nsource, represented by Qin and Yang (2019) and Li et al.\n(2020). While previous works (Qin and Yang 2019; Yang\net al. 2020; Sawhney et al. 2020) mainly explore the beneÔ¨Åts\nof audio features for volatility predictions, we propose a dif-\nferent research question that whether adaptive pre-training\nand volatility prediction loss can beneÔ¨Åt the performance of\nstock prediction by using multi-modal aligned earnings call\ndata.\nApproach\nThe NumHTML model proposed in this paper is shown in\nFigure 1 and is made up of four components: (1) word-level\nencoder; (2) multimedia information fusion; (3) sentence-\nlevel encoder; and (4) pareto multi-task learning. BrieÔ¨Çy, a\nkey innovation in this work is the use of a novel structured\n11605\nBuilding Trading Strategy\nInformation Fusion\n(Text & Audio Features)\nSelf-attention Mechanism\n‚Ä¶\nSentence-Level \nEncoder\nPareto Multi-task Learning\nAnTnA2T2A1T1\nP1 P2 Pn\nO1 O2 On\nR1 R2\n‚Ä¶\nMagnitude \nComparison\nNumeral Category \nClassification\nStock Movement Prediction / Volatility Prediction \nSales were [MASK] billion [$1.2, $2.5, $5, $9.8, $9.9]\nPre-trained \nLanguage Models\nAdaptive Pre-\ntrained Models\nWord-Level \nEncoder\nStructured \nAdaptive \nPre-training\n[Monetary, Temporal, Percentage, Other]\nFigure 1: Overall architecture of NumHTML. We use the surrounding tokens around [MASK] to classify the numeral categories.\nadaptive pre-training approach to improve how numeric in-\nformation is treated during the word-level encoding of earn-\nings calls transcripts. Then, sentence-level text features are\naligned with 27 classical audio features extracted from earn-\nings calls audio (Boersma and Van Heuven 2001) based on\nthe approach described by Qin and Yang (2019). Next, the\ninformation fusion layer is responsible for combining the re-\nsulting text and audio features into a single representation\nfor use by the sentence-level encoder to generate a multi-\nmodal input that is suitable for training and prediction.\nStructured Adaptive Pre-training\nThe pre-training process consists of two main tasks, Nu-\nmeral Category ClassiÔ¨Åcation and Magnitude Compari-\nson, in order to improve the representation of numerical\ndata. During Numeral Category ClassiÔ¨Åcation , sentences\nthat contain numeric data are categorised as belong to one or\nmore of four main classes: monetary, temporal, percentage,\nand other. This categorization process uses a set of trigger\ntokens and rules so that, for example, in the sentence ‚ÄùDur-\ning 2020 proÔ¨Åts increased by 13% to $205m‚Äù presumably\nthis is tagged as monetary (because of the $205m), temporal\n(2020) and percentage (%). We freeze the penultimate layer\nof Ô¨Åne-tuned whole-word-masked BERT (WWM-BERT)\nmodel before Ô¨Åne-tuning for the next numeral understand-\ning task, Magnitude Comparison.\nFollowing Wallace et al. (2019), Magnitude Comparison\nis probed in an argmax setting. Given the embeddings for\nÔ¨Åve numbers, the task is to predict the index of the maxi-\nmum number. Each list consists of values of similar magni-\ntude within the same numeral type in order to conduct a Ô¨Åne-\ngrained comparison. For example, for a given list containing\nÔ¨Åve monetary numbers [$1:2;$2:5;$5;$9:8;$9:9], the train-\ning goal is to Ô¨Ånd the largest position value within this Ô¨Åve\nvalues. In this given example, the golden label should be\n[0;0;0;0;1]. Softmax is used to assign a probability to each\nindex using the hidden state trained by the negative log-\nlikelihood loss. In practice, we shape the training/test set by\nuniformly sampling raw earnings call transcript data with-\nout placing back. A BiLSTM network will be fed with the\nlist of token embeddings ‚Äì varying from the pre-trained em-\nbeddings to the Ô¨Åne-tuned embeddings ‚Äì connected with a\nweight matrix to compare its performance. The token-level\nencoder tuned by the structured adaptive pre-training is used\nfor shaping the sentence-level textual embedding.\nSentence-level Transformer Encoder\nWe adopt a sentence-level Transformer encoder fed with\nsentence-level representations of long-form multi-modal\naligned earnings call data (usually contains more than 512\ntokens) for multi-task Ô¨Ånancial forecasting. Let Wi =(\nw1\ni;w2\ni;:::;w |Wi|\ni\n)\nbe a sentence, where |Wi|is the length\nand w|Wi|\ni is an artiÔ¨Åcial EOS (end of sentence) token. The\nword embedding matrix associated with Wi is initialized as\nEi =\n(\ne1\ni;e2\ni;:::; e|ti|\ni\n)\nwhere ej\ni = e\n(\nwj\ni\n)\n+ pj:\n(1)\ne(¬∑) maps each token to a d dimensional vector using\nWWM-BERT, and pj is the position embedding of wj\ni with\nthe same dimension d. Consequently, ej\ni ‚ààRd for all j.\nThe enhanced WWM-BERT model after structured adap-\ntive pre-training is adopted as the token-level Transformer\nencoder. The sentence representation Ti ‚ààRdt of the sen-\ntence Wiis calculated through the average pooling operating\nover the second last layer of the network. dt represents the\ndefault dimensions of word embeddings. The sentence rep-\nresentations are aligned with sentence-level audio features in\nthe information fusion layer later. Finally, the multi-modal\nrepresentation of a single earnings call is represented as:\nD(k) =\n(\ns(k)\n1 ;s(k)\n2 ;:::;s (k)\nM\n)\nwhere s(k)\ni =\n(\n(T(k)\ni ;A(k)\ni ) + Pi\n)\n:\n(2)\nTk\ni and Ak\ni represent the textual and audio features of\nsentence i of document D(k) ‚ààRM√óds, respectively, and\n11606\nLoss of Task 1\nLoss of Task 2\nùêø2(ùúÉ)\nùêø1(ùúÉ)\nùë¢1\nùë¢2\nùë¢ùëò\nFigure 2: Pareto MTL aims to Ô¨Ånd a set of Pareto solutions\nin different restricted preference regions (Lin et al. 2019).\nPi ‚ààRM√óds denotes the trainable sentence-level position\nembedding. M is the maximum number of sentences.\nPareto Multi-task Learning\nWe adopt the Pareto Multi-task Learning algorithm (Pareto\nMTL) proposed by Lin et al. (2019) to integrate stock move-\nment prediction and volatility prediction by Ô¨Ånding a set of\nwell-distributed Pareto solutions that can represent differ-\nent trade-offs between both tasks. Pareto MTL decomposes\na Multi-Task Learning (MTL) problem into multi-objective\nsubproblems with multiple constraints. An average pooling\noperation is Ô¨Årst applied to the output of the sentence-level\nTransformer encoder. Then, we Ô¨Ånd a set of well-distributed\nunit preference vectors {u1;u2;:::;u K}in R2\n+; K = 10 in\nthis work. The multi-objective sub-problem corresponding\nto the preference vector uK is deÔ¨Åned as:\nmin\u0012L(\u0012) = (L1(\u0012);L2(\u0012);¬∑¬∑¬∑;Lm(\u0012))T ;s.t.L(\u0012) ‚àà\nk\n(3)\nThe idea of Pareto MTL is shown in Figure 2, where Lm(\u0012)\nis the loss of task mand \nk(k=1,. . . , K) is a sub-region in a\nobjective space and with uT\nj v as the inner product between\nthe preference vector uj and a given vector v:\n\nk =\n{\nv ‚ààR2\n+ |uT\nj v ‚â§uT\nkv;‚àÄj = 1;:::;K } (4)\nHence, the set of possible solutions in different sub-regions\nrepresent different trade-offs among these two tasks. A two-\nstep, gradient-based method is used to solve these multi-\nobjective sub-problems based on the sentence-level multi-\nmodal representations.\nInitial Solution: To Ô¨Ånd an initial feasible solution\u00120 for a\nhigh-dimension, constrained optimization problem, we use a\nsequential gradient-based method since the straightforward\nprojection approach is too expensive to calculate directly for\nthe 345-million parameter WWM-BERT model. The update\nrule used is \u0012t+1 = \u0012t + \u0011dt where \u0011is the step size and dt\nis the search direction, which can be obtained from the rule\nof Pareto critical (Zitzler and Thiele 1999). Iteration termi-\nnates once a feasible solution is found or the max number of\niterations is met.\nAchieving Pareto EfÔ¨Åciency: The next step is to solve\nthe constrained subproblems in order to Ô¨Ånd a set of dis-\ntributed solutions that can achieve the Pareto efÔ¨Åciency. Fol-\nlowing Lin et al. (2019), we obtain a restricted Pareto critical\nsolution for each training goal by using constrained multi-\nobjective optimization, which generalizes the steepest de-\nscent method for unconstrained multi-objective optimization\nproblems. Due to the high-dimensionality of the problem,\nwe change the decision space from the parameter space to a\nmore tractable objective and constraint space; see Lin et al.\n(2019) for a proof of this and the algorithm used. The result\nis a reduction in the dimension of the optimization problem\nfrom 345 millions to seven (two objective functions plus Ô¨Åve\nactivated constraints), which allows the Pareto MTL to be\nscaled and optimized for our task as shown in Equation 5,\nwhere ^yi and ^yj are the predicted values for the main and\nauxiliary tasks, respectively, and yi and yj denote the corre-\nsponding true values. The output of Pareto MTL is a set of\nweight allocation strategies (\u000bpatero1 and \u000bpatero1 ) for both\ntasks. We use Adam (Kingma and Ba 2014) as the optimizer\nand adopt the trick of learning-rate decay with increasing\nsteps to train the model until it converges.\nF= \u000bpatero1\n‚àë\ni\n(^yi ‚àíyi)2 +\u000bpatero2\n‚àë\nj\n(^yj ‚àíyj)2 (5)\nEvaluation\nWe make a comprehensive comparison of NumHTML with\nseveral state-of-the-art baselines using a publicly available\ndataset, by Ô¨Årst focusing on stock movement prediction and\nthen by testing various stock prediction techniques in a real-\nistic long-term trading simulation. The hyper-parameters for\nour method and baselines are all selected by a grid search\non the validation set. In each case, we demonstrate the sig-\nniÔ¨Åcant advantage of NumHTML compared to baselines.\nPrior to these studies, we describe the intermediate results\nfor the adaptive training used by NumHTML to demon-\nstrate how it signiÔ¨Åcantly outperforms the conventional pre-\ntrained model.\nDataset & Methodology\nDataset: In line with previous work (Yang et al. 2020;\nSawhney et al. 2020) for multi-task Ô¨Ånancial forecasting, in\nthis evaluation, we use a publicly available Earning Confer-\nence Calls dataset constructed by Qin and Yang (2019). This\ndataset contains 576 earning calls recordings, correspond to\n88,829 text-audio aligned sentences, for S&P 500 compa-\nnies in U.S. stock exchanges. The dataset also includes the\ncorresponding dividend-adjusted closing prices from Yahoo\nFinance 1 for calculating volatility and stock returns. To fa-\ncilitate a direct comparison with the current state-of-the-art\n(Sawhney et al. 2020), we split the dataset into mutually ex-\nclusive training/validation/testing sets in the ratio of 7:1:2\n(refers to instances) in chronological order, since future data\ncannot be used for prediction.\nThe Stock Prediction Task: We evaluate the stock predic-\ntion task as a classiÔ¨Åcation problem ‚Äî that is, the task is to\npredict whether a stock moves up (positive) or down (neg-\native) due to the earnings call ‚Äî in order to ensure a fair\ncomparison with (Sawhney et al. 2020). The prediction of\n1https://Ô¨Ånance.yahoo.com/\n11607\nn-day stock movement will be a rise if the regression results\nof the stock return is a positive value and vice versa.\nThe Trading Simulation Task: To perform a trading sim-\nulation based on the multi-modal multi-task learning archi-\ntecture, we aim to optimize for (1) average n-day volatility\n(that is, the average volatility of the following ndays); and\n(2) cumulative n-day stock return (that is, the cumulative\nproÔ¨Åt n days after an earnings call). In the trading simula-\ntion, stock movement predictions are used to decide whether\nto buy or sell a stock after ndays. To ensure a fair compar-\nison, we use the trading strategy implemented by (Sawhney\net al. 2020), which relies on the results of stock movement\nprediction when n = 3 . Thus, if the prediction is a rise in\nprice ps\nd‚àín from day d‚àínto dfor stock s, the strategy buys\nthe stock son day d‚àín, and then sells it on day d. In addi-\ntion, we perform a short sell if the prediction is a fall in price.\nWe maintain the same trading environment with Sawhney\net al. (2020): there are no transaction fees, we can only pur-\nchase a single share (but for multiple companies) for each\ntime period, and intra-day trading is not considered 2.\nEvaluation Metrics\nFor stock movement prediction we report the F1 score and\nMathew‚Äôs Correlation CoefÔ¨Åcient (MCC) for stock price\nprediction. MCC performs more precisely when the data is\nskewed by accounting for the true negatives. For a given con-\nfusion matrix:\nMCC = tp√ótn‚àífp √ófn\n‚àö\n(tp+ fp)(tp+ fn)(tn+ fp)(tn+ fn)\n(6)\nThen the predicted average n ‚àíday volatility is com-\npared with the actual volatility (Eq. 7) to compute the mean\nsquared error for each hold period: n‚àà{3;7;15;30}.\nv[0;n] = ln\nÔ£´\nÔ£≠\nÓµ™Óµ´Óµ´\n‚àö\n‚àën\ni=1 (ri ‚àí\u0016r)2\nn\n)\n(7)\nri is the stock return on day i and \u0016r is the average stock\nreturn (using adjusted closing price) in a window of ndays.\nMSE =\n‚àë\ni(^yi ‚àíyi)2\nn (8)\nFor the stock trading simulation we use the cumulative\nproÔ¨Åt and Sharpe Ratio metrics. The cumulative proÔ¨Åt gen-\nerated by a simple trading strategy is deÔ¨Åned as:\nProÔ¨Åt =\n‚àë\ns‚ààS\n(\nps\nd ‚àíps\nd‚àí\u001c\n)\n‚àó(‚àí1)Action d‚àí\u001c\ns (9)\nwhere (ps\nd) indicates the stock price of stock s on the day\nd, and Action d‚àí\u001c\ns is a binary value depended on the stock\n2Obviously, this represents a simpliÔ¨Åed trading strategy, given\nthat it is limited to single share purchases. It was adopted here to\nalign with previous work (Sawhney et al. 2020) but is an obvious\navenue for future work to implement more sophisticated trading\nstrategies.\nModel LRAP ROC AUC\nGlove 0.870 0.858\nWWM-BERT 0.920 0.904\nWWM-BERT+NCC 0.973 0.977\nTable 1: The four-class numeral category classiÔ¨Åcation re-\nsults varying from different embeddings.\nmovement prediction result; it equals to 0 if the model pre-\ndicts a rise in price for stock s on day d, otherwise it is 1.\nSharpe Ratio evaluates the performance of investments us-\ning their average return rate rx, risk-free return rate Rf and\nthe standard deviation \u001bacross the investment x:\nSharpe Ratio = rx ‚àíRf\n\u001b(rx) (10)\nBaselines\nWe consider several different baselines (Wang et al. 2016;\nYang et al. 2016; Qin and Yang 2019; Yang et al. 2020;\nSawhney et al. 2020), which, to the best of our knowledge,\noffer the best available stock prediction methods at the time\nof writing. These baselines can be grouped according to\nwhether they use historical (numeric) pricing data, textual\nearnings call data, or multi-modal earnings call data.\n1. LSTM+ATT (Wang et al. 2016): The best performing\nprice-based model (LSTM with attention) in which the n-\nday volatility in the training data is predicted using pric-\ning data only.\n2. HAN (Glove) (Yang et al. 2016): Uses textual data in\nwhich each word in a sentence is converted to a word\nembedding using the pre-trained Glove 300-dimensional\nembeddings and trained by a hierarchical Bi-GRU mod-\nels (Bahdanau, Cho, and Bengio 2014).\n3. MDRM (Qin and Yang 2019):This recent work was the\nÔ¨Årst to consider volatility prediction a multi-modal deep\nregression problem based on a newly proposed multi-\nmodal aligned earnings calls dataset.\n4. HTML (Yang et al. 2020): This recent hierarchical\ntransformer-based, multi-task learning framework is de-\nsigned speciÔ¨Åcally for volatility prediction using multi-\nmodal aligned earnings call data.\n5. Multi-Modal Ensemble Method (Sawhney et al.\n2020): This multi-modal, multi-task learning approach\nrepresents the current state-of-the-art in the task of stock\nmovement predictions using a combination of textual and\naudio earnings calls data.\nEvaluating Structured Adaptive Training\nTo begin with, we present the results on the validation set\nfor the adaptive training used by NumHTML.\nNumeral Category ClassiÔ¨Åcation The results of multi-\nlabel numeral category classiÔ¨Åcation (NCC) on the val-\nidation set are shown in Table 1. The aim is to show\nhow this task signiÔ¨Åcantly enhances the token-level embed-\ndings. Both Label ranking average precision (LRAP) and\n11608\nModel Monetary Temporal Percentage All\nGloVe 0.84 0.78 0.89 0.82\nWWM-BERT 0.90 0.71 0.95 0.88\nWWM-BERT+NCC 0.89 0.72 0.95 0.88\nWWM-BERT+NCC+MC 0.93 0.85 0.99 0.94\nTable 2: The Magnitude Comparison Results (List Maximum from 5-numbers).\nROC AUC scores of the Ô¨Ånancial numeral category classiÔ¨Å-\ncation have been increased with the beneÔ¨Åt of adaptive pre-\ntraining, which suggests that our approach (BERT+NCC)\ncan classify numeral categories better than the raw pre-\ntrained embeddings, including BERT and Glove. In partic-\nular, the performance of the adaptive pre-trained model is\nimproved around 5.3% in LRAP and 7.3% in ROC\nAUC.\nMagnitude Comparison The accuracy of the Magnitude\nComparison (listed maximum 5-numbers) based on differ-\nent methods are shown in Table 2. We Ô¨Ånd that the ‚ÄòNCC‚Äô\ntask cannot guarantee the accuracy beneÔ¨Åts for the maxi-\nmum list task. However, the adaptive pre-training directly\non the magnitude comparison task can signiÔ¨Åcantly improve\nperformance (94% vs 88% on average). We also notice that\nthe ‚Äôpercentage‚Äô classiÔ¨Åcation can achieve the highest ac-\ncuracy among four types, while the secular values are the\nhardest to predict (85% vs 99%). We speculate that num-\nbers representing percentages are between 0 and 99, making\nit easier to predict the largest number among them. On the\nother hand, the numbers representing years usually contain\nfour digits and are similar, posing a challenge for the mag-\nnitude comparison.\nEvaluating Stock Movement Prediction\nThe stock movement predictions results are presented in Ta-\nble 3, using each of the baselines and several variations of\nthe NumHTML model for 3, 7, 15, and 30-day prediction\nperiods. The results indicate that NumHTML using multi-\nmodal data generally outperforms all alternative methods,\nincluding the current state-of-the-art, multi-modal Ensemble\nmethod. The NumHTML variants generate predictions with\nthe highest MCC and F1 scores, compared with the similar\nmulti-modal, multi-task approach of the Ensemble alterna-\ntive (Sawhney et al. 2020). This means that our single-model\napproach achieves statistically signiÔ¨Åcant performance im-\nprovements over the Ensemble method for all cases when us-\ning multi-model versions. In addition, using text-only data,\nour approach also achieves some meaningful improvements\nin almost all settings, excluding n=15.\nTo further understand the beneÔ¨Åts of NumHTML, in what\nfollows, we also consider several ablation studies to deter-\nmine the efÔ¨Åcacy of different NumHTML components.\nOn the Utility of Structured Adaptive Pre-Training: In\nTable 3, we see NumHTML prediction performance ex-\nceeds that of NumHTML without the structured adaptive\npre-training, for all n. In other words, by better modeling\nthe numerical aspects of earnings call data, it is possible to\nsigniÔ¨Åcantly improve subsequent prediction performance.\nOn the Utility of Volatility Prediction as an Auxiliary\nPrediction Task: Overall, NumHTML also signiÔ¨Åcantly\noutperforms baseline methods in the volatility prediction\ntask. Moreover, by comparing our approach with and with-\nout Pareto MTL, in Table 3, we see that the volatility pre-\ndiction task consistently improves the results as an auxiliary\ntask when predicting the stock movement; the single excep-\ntion is forn= 7. Moreover, Figure 3 shows that NumHTML\ncan even achieve the best average performance (least MSE\nerror over four sub-tasks) for the auxiliary task, which is ig-\nnored in previous works (Yang et al. 2020). Thus, by com-\nparing the volatility prediction results of our approach with\nand without Pareto MTL, we Ô¨Ånd that the trade-off consider-\nations between two tasks can signiÔ¨Åcantly improve the per-\nformance of the auxiliary task.\nOn the Utility of Audio Features: While existing work\n(Qin and Yang 2019; Yang et al. 2020; Sawhney et al. 2020)\nonly explores the utility of audio features for volatility pre-\ndiction, Table 3 shows how multi-modal learning consis-\ntently outperforms methods, which are purely based on the\ntextual features, for stock movement prediction also. In par-\nticular, we observe consistent improvements by using the\nvocal cues compared with text-only versions among the four\nmulti-modal methods. Improvements of this scale, relative\nto the corresponding text-only versions, are likely to trans-\nlate into substantial practical beneÔ¨Åts and suggest signiÔ¨Åcant\nvalue in the use of audio features for a range of Ô¨Ånancial\nforecasting problems.\nCumulative ProÔ¨Åt in a Trading Simulation\nNext, we consider the value of the various approaches to\nstock movement prediction in the context of a more realistic\ntrading simulation. Table 4 presents the results (cumulative\nproÔ¨Åt achieved and Sharpe Ratio) for various approaches.\nWe use three standard trading strategies as baseline strate-\ngies: Buy-all, Short-sell-all, Random that are commonly\nused as benchmarks. We also compare our method with\nthree strong multi-modal baselines, namely MRDM (Qin\nand Yang 2019), HTML (Yang et al. 2020), and Ensemble\nmethod (Sawhney et al. 2020). Once again, the NumHTML\napproach outperforms all of the alternatives in terms of both\nproÔ¨Åt achieved and the Sharpe Ratio (higher is better). The\nproÔ¨Åt achieved by NumHTML signiÔ¨Åcantly exceeds that of\nthe S&P 500 over the same period.\nWe have also interested in the individual effect of struc-\ntured adaptive pre-training and pareto multi-task learn-\ning, respectively. Comparing the NumHTML to HTML\nwith adaptive pre-training only (shown as NumHTML w/o\nPareto), Table 4 shows that HTML with adaptive pre-\ntraining can improve the arbitrage proÔ¨Åt somewhat, but with-\n11609\nPrice Movement Predictions\nModel MCC3 MCC7 MCC15 MCC30 F13 F17 F115 F130\nPrice-based LSTM 0.069 0 0.097 0 0.271 0.694 0.200 0.765\nPrice-based BiLSTM-ATT 0 0 0 0 0.149 0.342 0.200 0.721\nSVM -0.069 0.015 -0.048 -0.003 0.524 0.683 0.645 0.734\nHAN (Glove) 0.090 -0.005 0.266 -0.042 0.591 0.621 0.598 0.703\nText-only Methods\nMDRM 0.117 -0.107 0.032 -0.085 0.675 0.500 0.571 0.601\nHTML 0.195 0.007 0.119 0.022 0.623 0.688 0.648 0.700\nEnsemble 0.204 0.008 0.132 0.024 0.675 0.690 0.636 0.703\nNumHTML 0.229** 0.009 0.122 0.031 0.689* 0.691 0.644** 0.727*\nMulti-modal Methods\nMDRM (Multi-modal) 0.095 0.056 0.159 -0.065 0.628 0.690 0.452 0.590\nHTML (Multi-modal) 0.280 0.125 0.196 0.131 0.696 0.695 0.703 0.748\nEnsemble (Multi-modal) 0.321 0.128 0.191 0.128 0.702 0.698 0.702 0.761\nNumHTML (w/o Pareto MTL) 0.293 0.129 0.198 0.133 0.701 0.700 0.711 0.759\nNumHTML (w/o Adaptive Pre-training) 0.282 0.121 0.199 0.130 0.697 0.668 0.705 0.746\nNumHTML (Multi-modal) 0.325** 0.126 0.206** 0.136* 0.722* 0.697 0.716* 0.770**\nTable 3: Results for the future n-day stock movement prediction (higher is better). * and ** indicate statistically signiÔ¨Åcant\nimprovements over the state-of-the-art ensemble method with p<0.05, p<0.01 respectively, under Wilcoxon‚Äôs test.\nStrategy ProÔ¨Åt Sharpe Ratio\nSimple Baselines\nBuy-all $36.59 0.76\nShort-sell-all -$36.59 -0.77\nRandom -$25.78 -0.58\nMulti-modal Methods\nMRDM $38.75 0.81\nHTML $72.47 1.52\nNumHTML (w/o Pareto) $73.90 1.53\nEnsemble $75.73 1.59\nNumHTML $77.81 1.62\nTable 4: Cumulative proÔ¨Åt across different trading strategies.\nout beneÔ¨Åting the Sharpe Ratio. Furthermore, the single\nPareto MTL component provides signiÔ¨Åcant performance\nbeneÔ¨Åts in terms of proÔ¨Åt (73.90 vs 77.81) and Sharpe Ra-\ntio (1.53 vs 1.62), which suggests that our method beneÔ¨Åts\nconsiderably from the trade-off considerations.\nConclusion\nThis work contributes to multi-task Ô¨Ånancial forecasting,\nwith a particular focus on the stock movement prediction,\nby using multi-modal earnings conference calls data. In par-\nticular, we propose a novel, numeric-oriented hierarchical\ntransformer-based model (NumHTML) by using structured\nadaptive pre-training to improve how numeric data is repre-\nsented and used in the pre-trained language model. A com-\nprehensive comparative evaluation demonstrates signiÔ¨Åcant\nperformance beneÔ¨Åts accruing to NumHTML, compared to\na variety of state-of-the-art baselines and in the context of\nstock prediction and extended trading tasks. This evaluation\nalso includes an ablation study to clarify the utility of dif-\nferent NumHTML components (adaptive pre-training, aux-\niliary volatility prediction, and the use of audio features).\nOur work may be extended in several ways. More sophis-\nticated numeric representations can be imagined in order to\nFigure 3: The results of volatility prediction. ‚ÄôOurs (w/o P)‚Äô\nindicates NumHTML without Pareto MTL. Text-only and\nmulti-modal methods are presented by different colors.\nimprove the representation of numeric data. Likewise, it may\nbe feasible to develop similar representations for other cat-\negories of useful data in due course. In this work, we fo-\ncused on stock movement prediction and trading, but the\napproaches described may be of value in a range of Ô¨Ånan-\ncial forecasting tasks such as portfolio management/design,\nhedging, Ô¨Ånancial fraud or accounting errors, etc. The cur-\nrent trading simulation, based on (Sawhney et al. 2020), im-\nposes a signiÔ¨Åcant single-stock purchasing limit per time pe-\nriod, as discussed. Going forward it will be necessary to con-\nsider more sophisticated trading policies.\nAcknowledgments\nWe acknowledge with thanks the discussion with Boyuan\nZheng and Cunxiang Wang from Westlake University, as\nwell as the many others who have. We would also like to\nthank anonymous reviewers for their insightful comments\nand suggestions to help improve the paper. This publica-\ntion has emanated from research conducted with the Ô¨Ånan-\n11610\ncial support of Postdoctoral Funding Sponsored by Zhejiang\nProvince and Rong Hui Jin Xin Company under Grant Num-\nber 10313H041801 and the Ô¨Ånancial support of the Pio-\nneer and ‚ÄùLeading Goose‚Äù R&D Program of Zhejiang under\nGrant Number 2022SDXHDX0003. Yue Zhang and Barry\nSmyth are co-corresponding authors.\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473.\nBoersma, P.; and Van Heuven, V . 2001. Speak and unspeak\nwith praat. Glot International, 5(9/10): 341‚Äì347.\nBollerslev, T.; Patton, A. J.; and Quaedvlieg, R. 2016. Ex-\nploiting the errors: A simple approach for improved volatil-\nity forecasting. Journal of Econometrics, 192(1): 1‚Äì18.\nChang, C.-Y .; Zhang, Y .; Teng, Z.; Bozanic, Z.; and Ke,\nB. 2016. Measuring the Information Content of Financial\nNews. In Proceedings of COLING 2016, the 26th Interna-\ntional Conference on Computational Linguistics: Technical\nPapers, 3216‚Äì3225. Osaka, Japan.\nChen, C.; Zhao, L.; Bian, J.; Xing, C.; and Liu, T.-Y .\n2019a. Investment Behaviors Can Tell What Inside: Explor-\ning Stock Intrinsic Properties for Stock Trend Prediction. In\nProceedings of the 25th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining, KDD ‚Äô19,\n2376‚Äì2384. New York, NY , USA: Association for Comput-\ning Machinery.\nChen, C.-C.; Huang, H.-H.; and Chen, H.-H. 2021a. Eval-\nuating the Rationales of Amateur Investors. In The World\nWide Web Conference.\nChen, C.-C.; Huang, H.-H.; and Chen, H.-H. 2021b. From\nOpinion Mining to Financial Argument Mining. Springer\nBriefs in Computer Science, 1‚Äì95.\nChen, C.-C.; Huang, H.-H.; Takamura, H.; and Chen, H.-H.\n2019b. Numeracy-600k: learning numeracy for detecting\nexaggerated information in market comments. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, ACL ‚Äô19, 6307‚Äì6313.\nChen, Y .; Wei, Z.; and Huang, X. 2018. Incorporating\nCorporation Relationship via Graph Convolutional Neural\nNetworks for Stock Price Prediction. In Proceedings of\nthe 27th ACM International Conference on Information and\nKnowledge Management, CIKM ‚Äô18, 1655‚Äì1658. ISBN\n9781450360142.\nDa, Z.; Engelberg, J.; and Gao, P. 2015. The sum of all\nFEARS investor sentiment and asset prices. The Review of\nFinancial Studies, 28(1): 1‚Äì32.\nDing, X.; Zhang, Y .; Liu, T.; and Duan, J. 2014. Using\nstructured events to predict stock price movement: An em-\npirical investigation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP), 1415‚Äì1425.\nDing, X.; Zhang, Y .; Liu, T.; and Duan, J. 2015. Deep Learn-\ning for Event-Driven Stock Prediction. In Proceedings of\nthe 24th International Conference on ArtiÔ¨Åcial Intelligence,\n2327‚Äì2333. Buenos Aires, Argentina.\nDu, X.; and Tanaka-Ishii, K. 2020. Stock embeddings ac-\nquired from news articles and price history, and an appli-\ncation to portfolio optimization. In Proceedings of the 58th\nannual meeting of the association for computational linguis-\ntics, 3353‚Äì3363.\nDua, D.; Wang, Y .; Dasigi, P.; Stanovsky, G.; Singh, S.; and\nGardner, M. 2019. DROP: A reading comprehension bench-\nmark requiring discrete reasoning over paragraphs. arXiv\npreprint arXiv:1903.00161.\nDuan, J.; Zhang, Y .; Ding, X.; Chang, C. Y .; and Liu, T.\n2018. Learning target-speciÔ¨Åc representations of Ô¨Ånancial\nnews documents for cumulative abnormal return predic-\ntion. In Proceedings of the 27th International Conference\non Computational Linguistics (COLING-18), 2823‚Äì2833.\nFeng, F.; Chen, H.; He, X.; Ding, J.; Sun, M.; and Chua,\nT.-S. 2018. Enhancing stock movement prediction with ad-\nversarial training. arXiv preprint arXiv:1810.09936.\nHu, Z.; Liu, W.; Bian, J.; Liu, X.; and Liu, T.-Y . 2018. Lis-\ntening to Chaotic Whispers: A Deep Learning Framework\nfor News-Oriented Stock Trend Prediction. In Proceed-\nings of the Eleventh ACM International Conference on Web\nSearch and Data Mining, WSDM ‚Äô18, 261‚Äì269. New York,\nNY , USA: Association for Computing Machinery.\nKeith, K.; and Stent, A. 2019. Modeling Financial Analysts‚Äô\nDecision Making via the Pragmatics and Semantics of Earn-\nings Calls. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, ACL ‚Äô19, 493‚Äì\n503. Florence, Italy.\nKimbrough, M. D. 2005. The effect of conference calls\non analyst and market underreaction to earnings announce-\nments. The Accounting Review, 80(1): 189‚Äì219.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKogan, S.; Levin, D.; Routledge, B. R.; Sagi, J. S.; and\nSmith, N. A. 2009. Predicting Risk from Financial Reports\nwith Regression. In Proceedings of Human Language Tech-\nnologies: The 2009 Annual Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguis-\ntics, NAACL ‚Äô09, 272‚Äì280.\nKristjanpoller, W.; Fadic, A.; and Minutolo, M. C. 2014.\nV olatility forecast using hybrid neural network models.Ex-\npert Systems with Applications, 41(5): 2437‚Äì2442.\nLarcker, D. F.; and Zakolyukina, A. A. 2012. Detecting de-\nceptive discussions in conference calls. Journal of Account-\ning Research, 50(2): 495‚Äì540.\nLi, J.; Yang, L.; Smyth, B.; and Dong, R. 2020. MAEC: A\nmultimodal aligned earnings conference call dataset for Ô¨Å-\nnancial risk prediction. In Proceedings of the 29th ACM In-\nternational Conference on Information & Knowledge Man-\nagement, 3063‚Äì3070.\nLin, X.; Zhen, H.-L.; Li, Z.; Zhang, Q.-F.; and Kwong, S.\n2019. Pareto multi-task learning. In Advances in Neural\nInformation Processing Systems, 12060‚Äì12070.\nLiu, S.; and Tse, Y . K. 2013. Estimation of monthly volatil-\nity: An empirical comparison of realized volatility, GARCH\nand ACD-ICV methods. Finance Research Letters.\n11611\nLoughran, T.; and McDonald, B. 2011. When is a liability\nnot a liability? Textual analysis, dictionaries, and 10-Ks.The\nJournal of Finance, 66(1): 35‚Äì65.\nManela, A.; and Moreira, A. 2017. News implied volatil-\nity and disaster concerns. Journal of Financial Economics,\n123(1): 137‚Äì162.\nMoskowitz, T. J.; Ooi, Y . H.; and Pedersen, L. H. 2012. Time\nseries momentum. Journal of Ô¨Ånancial economics, 104(2):\n228‚Äì250.\nNaik, A.; Ravichander, A.; Rose, C.; and Hovy, E. 2019. Ex-\nploring numeracy in word embeddings. In Proceedings of\nthe 57th Annual Meeting of the Association for Computa-\ntional Linguistics, 3374‚Äì3380.\nPitk¬®aj¬®arvi, A.; Suominen, M.; and Vaittinen, L. 2020. Cross-\nasset signals and time series momentum. Journal of Finan-\ncial Economics, 136(1): 63‚Äì85.\nQin, Y .; and Yang, Y . 2019. What You Say and How You\nSay It Matters: Predicting Stock V olatility Using Verbal and\nV ocal Cues. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, 390‚Äì401.\nFlorence, Italy: Association for Computational Linguistics.\nRekabsaz, N.; Lupu, M.; Baklanov, A.; D¬®ur, A.; Andersson,\nL.; and Hanbury, A. 2017. V olatility prediction using Ô¨Ånan-\ncial disclosures sentiments with word embedding-based IR\nmodels. In Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), 1712‚Äì1721.\nSawhney, R.; Mathur, P.; Mangal, A.; Khanna, P.; Shah,\nR. R.; and Zimmermann, R. 2020. Multimodal Multi-Task\nFinancial Risk Forecasting. In Proceedings of the 28th\nACM International Conference on Multimedia, MM ‚Äô20,\n456‚Äì465. Association for Computing Machinery.\nWallace, E.; Wang, Y .; Li, S.; Singh, S.; and Gardner, M.\n2019. Do NLP Models Know Numbers? Probing Numer-\nacy in Embeddings. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), 5310‚Äì5318.\nWang, W. Y .; and Hua, Z. 2014. A semiparametric gaussian\ncopula regression model for predicting Ô¨Ånancial risks from\nearnings calls. In Proceedings of the 52nd Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), 1155‚Äì1165.\nWang, Y .; Huang, M.; Zhu, X.; and Zhao, L. 2016.\nAttention-based LSTM for aspect-level sentiment classiÔ¨Å-\ncation. In Proceedings of the 2016 conference on empirical\nmethods in natural language processing, 606‚Äì615.\nWu, Q.; Zhang, Q.; Wei, Z.; and Huang, X. 2021. Math\nWord Problem Solving with Explicit Numerical Values. In\nProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume\n1: Long Papers), 5859‚Äì5869. Online: Association for Com-\nputational Linguistics.\nXing, F. Z.; Cambria, E.; and Welsch, R. E. 2018. Intelligent\nasset allocation via market sentiment views. ieee Computa-\ntioNal iNtelligeNCe magaziNe, 13(4): 25‚Äì34.\nXing, F. Z.; Cambria, E.; and Zhang, Y . 2019. Sentiment-\naware volatility forecasting. Knowledge-Based Systems,\n176: 68‚Äì76.\nXu, Y .; and Cohen, S. B. 2018. Stock movement predic-\ntion from tweets and historical prices. In Proceedings of the\n56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 1970‚Äì1979.\nYang, L.; Dong, R.; Ng, T. L. J.; and Xu, Y . 2019. Lever-\naging BERT to Improve the FEARS Index for Stock Fore-\ncasting. In Proceedings of the First Workshop on Finan-\ncial Technology and Natural Language Processing, 54‚Äì60.\nMacao, China.\nYang, L.; Ng, T. L. J.; Smyth, B.; and Dong, R. 2020.\nHtml: Hierarchical transformer-based multi-task learning\nfor volatility prediction. In Proceedings of The Web Con-\nference 2020, 441‚Äì451.\nYang, L.; Zhang, Z.; Xiong, S.; Wei, L.; Ng, J.; Xu, L.; and\nDong, R. 2018. Explainable text-driven neural network for\nstock prediction. In2018 5th IEEE International Conference\non Cloud Computing and Intelligence Systems (CCIS), 441‚Äì\n445. IEEE.\nYang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.; and Hovy,\nE. 2016. Hierarchical attention networks for document\nclassiÔ¨Åcation. In Proceedings of the 2016 conference of\nthe North American chapter of the association for compu-\ntational linguistics: human language technologies, 1480‚Äì\n1489.\nYe, Z.; Qin, Y .; and Xu, W. 2020. Financial Risk Prediction\nwith Multi-Round Q&A Attention Network. In Proceed-\nings of the twenty-ninth international joint conference on\nartiÔ¨Åcial intelligence, 4576‚Äì4582. International Joint Con-\nferences on ArtiÔ¨Åcial Intelligence Organization.\nZhang, X.; Ramachandran, D.; Tenney, I.; Elazar, Y .; and\nRoth, D. 2020. Do Language Embeddings Capture Scales?\narXiv preprint arXiv:2010.05345.\nZheng, J.; Xia, A.; Shao, L.; Wan, T.; and Qin, Z. 2019.\nStock volatility prediction based on self-attention networks\nwith social information. In 2019 IEEE Conference on Com-\nputational Intelligence for Financial Engineering & Eco-\nnomics (CIFEr), 1‚Äì7. IEEE.\nZitzler, E.; and Thiele, L. 1999. Multiobjective evolutionary\nalgorithms: a comparative case study and the strength Pareto\napproach. IEEE transactions on Evolutionary Computation,\n3(4): 257‚Äì271.\n11612"
}