{
  "title": "Better than the real thing? Iterative pseudo-query processing using cluster-based language models",
  "url": "https://openalex.org/W2949069903",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5041210904",
      "name": "Oren Kurland",
      "affiliations": [
        "Carnegie Mellon University",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A5076876084",
      "name": "Lillian Lee",
      "affiliations": [
        "Carnegie Mellon University",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A5054417849",
      "name": "Carmel Domshlak",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1494864219",
    "https://openalex.org/W2261258341",
    "https://openalex.org/W1514403774",
    "https://openalex.org/W2084048649",
    "https://openalex.org/W1585620735",
    "https://openalex.org/W2918053736",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W2130395434",
    "https://openalex.org/W1990388042",
    "https://openalex.org/W1964348731",
    "https://openalex.org/W1526730373",
    "https://openalex.org/W1594759534",
    "https://openalex.org/W2102046030",
    "https://openalex.org/W2098034778",
    "https://openalex.org/W2150240006",
    "https://openalex.org/W2169213601",
    "https://openalex.org/W2027445772",
    "https://openalex.org/W2030603245",
    "https://openalex.org/W2164547069",
    "https://openalex.org/W2099194852",
    "https://openalex.org/W1999817920",
    "https://openalex.org/W2136542423",
    "https://openalex.org/W1602444393"
  ],
  "abstract": "We present a novel approach to pseudo-feedback-based ad hoc retrieval that uses language models induced from both documents and clusters. First, we treat the pseudo-feedback documents produced in response to the original query as a set of pseudo-queries that themselves can serve as input to the retrieval process. Observing that the documents returned in response to the pseudo-queries can then act as pseudo-queries for subsequent rounds, we arrive at a formulation of pseudo-query-based retrieval as an iterative process. Experiments show that several concrete instantiations of this idea, when applied in conjunction with techniques designed to heighten precision, yield performance results rivaling those of a number of previously-proposed algorithms, including the standard language-modeling approach. The use of cluster-based language models is a key contributing factor to our algorithms' success.",
  "full_text": "arXiv:cs/0601046v1  [cs.IR]  11 Jan 2006\nBetter than the Real Thing? Iterative Pseudo-Query\nProcessing using Cluster-Based Language Models\nOren Kurland1, 3\nkurland@cs.cornell.edu\n1. Computer Science Department, Cornell University, Ithaca NY 14853, U.S.A.\n2. Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA 15213, U.S.A.\n3. Computer Science Department, Carnegie Mellon University, Pittsburgh PA 15213, U.S.A.\n4. William Davidson Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel\nLillian Lee1, 2, 3\nllee@cs.cornell.edu\nCarmel Domshlak4\ndcarmel@ie.technion.ac.il\nABSTRACT\nWe present a novel approach to pseudo-feedback-based ad\nhoc retrieval that uses language models induced from both\ndocuments and clusters. First, we treat the pseudo-feedbac k\ndocuments produced in response to the original query as\na set of pseudo-queries that themselves can serve as input\nto the retrieval process. Observing that the documents re-\nturned in response to the pseudo-queries can then act as\npseudo-queries for subsequent rounds, we arrive at a formu-\nlation of pseudo-query-based retrieval as an iterative pro -\ncess. Experiments show that several concrete instantiatio ns\nof this idea, when applied in conjunction with techniques\ndesigned to heighten precision, yield performance results ri-\nvaling those of a number of previously-proposed algorithms ,\nincluding the standard language-modeling approach. The\nuse of cluster-based language models is a key contributing\nfactor to our algorithms’ success.\nCategories and Subject Descriptors: H.3.3 [Informa-\ntion Search and Retrieval]: Retrieval models, Clustering\nGeneral Terms: Algorithms, Experimentation\nKeywords: language modeling, clustering, pseudo-feedback,\npseudo-queries, rendition, query drift, cluster-based la nguage\nmodels, aspect recall\n1. INTRODUCTION\nStatistical language models have become an important\ntool in information retrieval, and have been applied to many\nsettings [7]. In the case of fully automatic ad hoc IR, where\nthe task is to ﬁnd documents relevant to a query q with-\nout access to relevance-feedback information, a great deal\nof recent research builds upon Ponte and Croft’s initial pro -\nposal [24] wherein the rank of a document d is based on the\nprobability assigned to q by a language model constructed\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc\npermission and/or a fee.\nSIGIR'05,August 15–19, 2005, Salvador, Brazil.\nCopyright 2005 ACM 1-59593-034-5/05/0008 ...$5.00.\nfrom d. We can gloss this ranking principle as, “retrieve the\ndocuments that are the best renderers of the query”. 1\nThe work presented in this paper is partly motivated by\nthe following hypothesis: documents that are the best ren-\nderers of a query may be good alternate renditions of it.\nIndeed, a basic premise behind query-expansion techniques\nutilizing pseudo-feedback is that top-retrieved document s\nmay reveal dimensions of the user’s information need that\nare not obvious from the original (short) query [26]. 2 As-\nsuming for now that the hypothesis is true (we discuss it fur-\nther below), we therefore propose a type of pseudo-feedback\napproach in which query “expansion” consists of wholesale\nreplacement of q with a list of pseudo-queries consisting of\nthe query’s best renderers.\nPseudo-queries are clearly a form of pseudo-feedback. How-\never, the former term suggests that once we have created\npseudo-queries from the initial query, we can in principle\nrepeat the process, this time seeking the top renderers of th e\npseudo-queries. And if the pseudo-queries are indeed more\ninformative than their predecessor(s), then we expect this\nrepetition to improve the retrieval results. We thus arrive at\nan iterative boot-strappingapproach in which the previously-\nretrieved best renderers become the pseudo-queries for the\nnext round.\nUnfortunately, pseudo-feedback quality can suﬀer from\nproblems with both precision and “aspect recall”. The cur-\nrently unavoidable phenomenon of non-relevant documents\nappearing in the retrieval results leads to query drift , “the\nalteration of the focus of a search topic caused by improper\nexpansion” [21] 3. As for recall, key aspects of the user’s\ninformation need may be completely missing from the pool\nof top-retrieved documents, due to both small pool size (in\norder to keep precision reasonable) and selection for docu-\n1Our choice of terminology — “renderers” rather than “gen-\nerators” — reﬂects the fact that we do not assume that doc-\numents (or their induced language models) are the source\nthat “generates” q: A monkey randomly striking typewriter\nkeys may produce a word-for-word copy of Hamlet, but we\ndo not therefore say that it is the author of the play.\n2This is another motivation behind our terminology: in the\nhands of a skilled artist, a rendition of a particular piece\nmay be faithful to the original in many respects, and yet\nstill be superior overall.\n3We are not referring to “query drift” in the sense of user\ninterests changing over time [1].\nments most resembling the short — and hence potentially\nnot completely informative — initial query. The inability t o\ncope with this missing-aspect problem is viewed as a major\nfailing of current systems [4, 10].\nTo increase aspect recall, we use the structure of the cor-\npus, as manifested through language models built on doc-\nument clusters, to suggest and represent potential facets o f\nthe user’s needs [13, 20]. In particular, we consider ﬁnd-\ning good renderers among a set of clusters rather than the\nset of documents: a cluster that is a good renderer may\ncontain documents that, while relevant, superﬁcially don’ t\nmatch the query string precisely because they include as-\npects not immediately evident in q. (Such documents could\nbe present in the cluster by dint of being similar to other\nrelevant documents with respect to non-query terms.)\nAs for query drift, the problem would seem to be exacer-\nbated by the multiple iterations performed by our algorithm ,\nsince poor-quality input early in the pipeline has a poten-\ntially disastrous eﬀect on retrieval results later on. To co pe\nwith this diﬃculty, we provide a number of methods that\n“re-anchor” pseudo-queries to the original query.\nExperiments with several large corpora reveal signiﬁcant\nimprovements in both average precision and recall over the\nstandard language-modeling approach (which corresponds\nto a degenerate version of our methods). This ﬁnding sug-\ngests that pseudo-queries may indeed be better than the\noriginal query as a basis for retrieval. Moreover, compar-\nisons against two highly eﬀective techniques incorporatin g\npseudo-relevance feedback — Rocchio on pseudo-feedback\nand Lavrenko and Croft’s language-model-based relevance\nmodel [18] — show that our cluster-based methods can of-\nten provide competitive or superior performance.\n2. RETRIEV AL FRAMEWORK\nWe now present a suite of fully automatic iterative algo-\nrithms for processing pseudo-queries. As mentioned above,\nall our algorithms conform to the same general format: ﬁnd\nthe best renderers of a current set of pseudo-queries; then,\nrepeat the process using these best renderers as the new\npseudo-queries.\nWe begin by establishing some notation and conventions\nin Section 2.1. Then, we discuss the two main axes along\nwhich our algorithms vary. The ﬁrst such axis, detailed in\nSection 2.2, is the basic deﬁnition of a good renderer; the\noptions we consider are: (1) a document that is a good ren-\nderer of at least one pseudo-query; (2) a document that is\na good renderer of multiple pseudo-queries; and (3) a clus-\nter that is a good renderer of multiple pseudo-queries. The\nsecond main axis of algorithm variation, discussed in Sec-\ntion 2.3, is the choice of mechanism for preventing query\ndrift. One idea we pursue is to incorporate the rendition\nprobability assigned to the original query.\n2.1 Notation and Conventions\nThroughout this section, D denotes a given document set,\nwhich induces a ﬁxed vocabulary. The notation q∗ indicates\nthe user’s initial query, and N stands for the number of doc-\numents to be returned in response to q∗ when the retrieval\nprocess terminates. Lower-case Greek letters indicate alg o-\nrithm parameters that were varied in our experiments.\nWe use C to refer to a set of clusters of the documents\nin D (in our work, C is computed prior to retrieval time).\nWe freely switch between thinking of a cluster as a subset\nof D and thinking of it as the single text string created by\nconcatenating its constituent documents in some pre-deﬁne d\norder 4 — this allows us to treat queries, documents, and\nclusters uniformly as sequences of terms.\nThe algorithms we present engage in iterative processing\nof pseudo-queries. In what follows, we assume that in each of\nthe ρ rounds, the pseudo-query input consists of a ranked list\nˆQ = ˆq1, ˆq2, . . . together with a weight function ˆw : ˆQ → [0, 1]\nsuch that ˆw(ˆqi) ≥ ˆw(ˆqi+1). In the ﬁrst iteration, ˆQ = q∗,\nand we set ˆw(q∗) to be 1. In subsequent iterations, ˆQ is an\nordering of the documents in D.\nA key concept in our work is that of a renderer r’s reper-\ntoire among a set of text strings, by which we mean the sub-\nset of the strings that r is a top renderer of. We therefore\nmake the following deﬁnitions. Let pr(x) denote an estimate\nof the probability that r (in our work, either a document or\na cluster) renders the text sequence x.\nDefinition 1. Let x be a text sequence, and let R be a\nﬁnite set of potential renderers. Then, x’s top k renderers\nin R, denoted TopRen(x; R, k ), is the set of k items r ∈ R\nthat yield the highest 5 pr(x).\nDefinition 2. For renderer r, set of text strings X, set\nof potential renderers R ∋ r, and positive integer k, we de-\nﬁne the repertoire of r in X with respect to R and k as\nRep(r; X | R, k )\ndef\n= {x ∈ X : r ∈ TopRen(x; R, k )}.\nFor compactness, we suppress X, R, and/or k in our nota-\ntion when no confusion can result.\nNote that repertoires are sets, not sorted lists; thus, when\nwe say that a pseudo-query ˆq ∈ Rep(r) is “highly ranked”,\nwe mean that it occurs early in ˆQ, as opposed to, say, that\nits rendition probability pr(ˆq) is large with respect to the\nother members of Rep( r).\n2.2 Basic Methods for Scoring Renderers\nWe now present three basic options for determining the\nbest renderers of a given iteration’s pseudo-queries. Each\nsuch method M takes as input the ranked list of pseudo-\nqueries ˆQ and the associated pseudo-query weights ˆw(ˆqi)\nand produces a score Score M (d) for each document d. The\ninput to the next round can then be created by setting\nˆw(d) = Score M (d) and sorting all the documents in D by\nthis quantity, unless additional mechanisms for coping wit h\nquery drift are applied (see Section 2.3).\nWe begin by restricting our consideration of possible ren-\nderers to documents. The Viterbi Doc-Audition scoring\nmethod is a straightforward procedure that ranks those doc-\numents with repertoires containing a highly-weighted pseu do-\nquery above those that are top renderers only of lower-\nweighted ones. Speciﬁcally, in each round, it ﬁrst returns\nthe top τ renderers of ˆq1, then the top τ renderers of ˆq2 (re-\npeated renderers discarded), and so on, with the list of top\nrenderers d of each ˆqi sorted in descending order of pd(ˆqi).6\nHence, suppose we have two top renderers of ˆq1, d and d′,\n4In our work, the concatenation order is irrelevant since we\nuse unigram language models.\n5Throughout this paper, we assume that the corpus docu-\nments and clusters are identiﬁed by numeric labels, with tie s\nbroken in favor of lower-numbered items.\n6Although it does not convey more insight than the English\nsuch that pd(ˆq1) > p d′ (ˆq1). Then d would be ranked above\nd′ even if pd(ˆqi) ≪ pd′ (ˆqi) for every other ˆqi.\nIf we were conﬁdent that ˆq1 is indeed the best represen-\ntation of the user’s information need, then such behavior is\nnot unreasonable. In practice, however, such conﬁdence may\nnot be warranted; rather, we might consider a document\nthat renders many of the pseudo-queries to be potentially\nas good or better a candidate for retrieval than a document\nthat renders only one. Hence, we deﬁne an alternative scor-\ning method, Doc-Audition. In essence, it rewards a poten-\ntial renderer d ∈ D for every pseudo-query in its repertoire,\nalthough less credit is assigned for low-weight ˆqi and for ˆqi\nthat d assigns a low rendition probability to. Speciﬁcally,\nranks are induced by the following function:\nScoreDoc(d)\ndef\n=\n∑\nˆq∈Rep(d|τ )\nˆw(ˆq) · pd(ˆq)\nK(ˆq; m), (1)\nwhere the re-scaling term K(ˆq; m) = ∑\nd′∈TopRen(ˆq;D,m ) pd′ (ˆq)\nserves to compare pd(ˆq) to the rendition probabilities of the\ntop m renderers 7 of ˆq. Observe that in the ﬁrst round, only\nthe top renderers of the original query can receive non-zero\nscores.\nLike all pseudo-feedback techniques, both scoring meth-\nods just presented can help ameliorate the crucial “aspect\nrecall” problem. We expect that the most improvement rel-\native to retrieval based directly on the original query q∗\nwill occur when the pseudo-queries contain eﬀective search\nterms not appearing in q∗, and among the best renderers\nof these pseudo-queries are relevant documents containing\nthe missing terms but not having high overlap with q∗. Our\nthird basic scoring method, Cluster-Audition, makes use\nof document clusters to take more explicit advantage of such\nsituations. In particular, we choose renderers of the pseud o-\nqueries from the set C of document clusters rather than from\nthe set of documents. Ideally, the best renderers would be\nclusters consisting only of relevant documents (thus incre as-\ning recall), and some of these clusters would represent info r-\nmation whose importance is implied by the query but not\nexplicitly mentioned by it (thus increasing “aspect recall ”).\nIn any event, integrating clusters into language-model-ba sed\nretrieval has recently been shown to yield substantial per-\nformance improvements [13, 20].\nWhile there are a huge number of clustering methods to\nchoose from, some quite sophisticated, we simply create one\ncluster for every document by grouping together that doc-\nument’s top γ renderers. This method is convenient for us\nsince we already need to compute top renderers; moreover,\ndescription just given, for the sake of completeness, here i s a\nformulation of the Viterbi Doc-Audition scoring method in\nterms of an explicit score function. For a given document d ∈\nD, let ˆq+ be the highest-ranked pseudo-query ˆq in Rep( d|τ),\nand let i+ be ˆq+’s rank. (If d is not a top renderer of any\npseudo-query, we set i+ to |D| + 1 and ˆq+ to the dummy\nvalue q∗.) Then, we deﬁne\nScoreVDoc(d)\ndef\n= pd(ˆq+) + 2( |D| − i+ + 1)\n1 + 2 |D| .\n7In our experiments, we ﬁxed m to a value guaranteed to be\ngreater than τ to handle cases where more than τ documents\nhad high rendition probabilities for q. However, preliminary\nexperiments indicated that choosing m = τ did not substan-\ntially alter results.\nit has proven eﬀective in previous work, perhaps because\nthe highly overlapping clusters can be seen as representing\ndiﬀerent facets of the similarity structure of the corpus [13].\nIndeed, there is a history of successful applications of the\ngeneral nearest-neighbor approach (e.g., [9]).\nWithin each iteration, Cluster-Audition scoring consists\nof two phases. In the ﬁrst, each cluster c is credited for\nevery pseudo-query in its repertoire. Note that in comput-\ning repertoires, we chose to restrict the set of possible top\nrenderers of a pseudo-query ˆq to C(ˆq), the set of clusters\ncontaining ˆq: a cluster is only “allowed” to render its con-\nstituent documents 8. (For the sake of readability, we sup-\npress this restriction in the repertoire notation below.) W e\nthus have:\nScoreClust(c)\ndef\n=\n∑\nˆq∈Rep(c|τ )\nˆw(ˆq) · pc(ˆq)\nK1(ˆq), (2)\nwhere K1(ˆq) = ∑\nc′∈C(ˆq) pc′ (ˆq) re-scales c’s rendition proba-\nbility with respect to the set of clusters containing ˆq.\nThe purpose of the second phase of scoring is to convert\nthe implicit cluster ranking just computed into a document\nranking, since the output of each round should be a legal set\nof ﬁnal retrieval results. This is achieved by crediting eac h\ndocument for every cluster it is one of the top σ renderers\nof, with the restriction (again suppressed in the repertoir e\nnotation below to enhance readability) that a document may\nonly render a cluster that it belongs to:\nScoreClust(d)\ndef\n=\n∑\nc∈Rep(d|σ )\nScoreClust(c) · pd(c)\nK2(c) , (3)\nwhere K2(c) = ∑\nd′∈c pd′ (c) re-scales d’s rendition probabil-\nity with respect to the set of documents within c.\nRemarks. If the desired values of τ and σ (the parame-\nters controlling the sizes of the top-renderer sets) and γ\n(the parameter for cluster size) are known beforehand, then\nthe clusters and top-renderer sets can be computed oﬀ-line,\ngreatly reducing the amount of computation required at re-\ntrieval time. However, even if these parameters are not\npre-speciﬁed, one can pre-compute a ranking of all possible\nrenderers of each document; this still results in signiﬁcan t\ncomputational savings at run-time.\nWe note that the iterative processes based upon the lat-\nter two of the basic scoring schemes we have just described\ncan be conceptualized as a ﬁxed-length random walk on\na graph corresponding to a Markov chain whose structure\nis determined by top-renderer relationships. In fact, the\nCluster-Audition scoring method is reminiscent of the term -\nto-document Markov chain used by Laﬀerty and Zhai [15]\nfor query expansion. However, in our case it is not clear\nwhat insight is gained by such a formulation; for instance,\nhow any stationary distribution should be interpreted in th e\ncontext of the retrieval task at hand is not obvious.\nFinally, notice that all three methods just outlined contai n\nthe standard language-modeling approach [24] as a degen-\nerate one-iteration (or half-iteration, for Cluster-Audi tion)\ncase where τ = |D|, and for Cluster-Audition, the cluster set\nC corresponds to a partition of D into single-element sets.\n8In the ﬁrst iteration only, we treat the original query as a\ndocument belonging to all clusters.\n2.3 Coping with Query Drift\nWe have previously mentioned that one of our main in-\nterests is increasing so-called “aspect recall”. Naturall y,\nwe want to simultaneously retain high precision as well.\nHowever, a potential drawback of our iterative approach to\npseudo-query processing is that engaging in multiple round s\nthreatens to exacerbate query drift: early contamination o f\nthe set of pseudo-queries with non-relevant documents can\nseriously skew downstream pseudo-query sets away from the\nuser’s true information needs. Using clusters adds even mor e\nrisk of overgeneralization. We therefore propose a number o f\nmethods for addressing the query-drift problem. All follow\nthe same general strategy: ensure that information from the\noriginal query q∗ plays a large role.\nTwo indirect techniques all our algorithms employ involve\nchoosing appropriate values for certain parameters. First ,\nwe limit ρ, the total number of rounds, to a relatively small\nnumber. Second, since the initial iteration is the one that i s\n“closest” to the original query, we give it privileged statu s\nby considering the top τ1, rather than τ, renderers of q∗.\nWe also consider a number of re-scoring techniques; these\ndirectly use pd(q∗) in some combination with the output of\none of the three basic scoring methods M introduced above.\nRecall that without re-scoring, the pseudo-query weights\nˆw(d) for round t + 1 would simply be the score assigned\nby M to document d at the end of round t.\nWe borrow two re-scoring techniques from research on\ncluster-based retrieval within the language-modeling fra me-\nwork [13, 20]. Both aﬀect only the output of the ﬁnal round,\nsince they were introduced in the context of non-iterative\nmethods. The P∗ Interpolation technique derives a new\nﬁnal score for each document d by linear interpolation of\nScore(ρ )\nM (d), d’s score in the ﬁnal round according to M,\nwith the rendition probability that d assigns to the original\nquery (after rescaling both quantities with respect to thei r\nmaximum values to ensure comparability):\nλScore(ρ )\nM (d) + (1 − λ)pd(q∗).\nIt thus integrates our iterated estimate of document rele-\nvance with surface document-query similarity.\nIn contrast, the Truncated P ∗ Re-rank method ﬁrst\ndiscards all but the top N documents according to Score (ρ )\nM (d);\neach remaining document d is then given the new score\npd(q∗). Note that this method does not aﬀect recall, since\nonly the order of the retrieved results is changed.\nAlternatively, we could alter the scores at the end of ev-\nery round, rather than just the ﬁnal one, as an attempt to\ncounteract query drift early in the process. One idea, imple -\nmented in the Iterated Truncation technique, is to con-\nsider only the top N documents in a given iteration or pass\nto be likely to be informative pseudo-queries for the next\nround; the scores of all the other documents are therefore\nzeroed. The Iterated Truncated P ∗ Re-rank technique\ngoes even further by additionally changing the scores of the\ntop N documents to pd(q∗). That is, the Truncated P ∗ Re-\nrank technique is applied to each round, rather than just\nto the ﬁnal one. Similarly, we can apply P ∗ Interpolation\nat each round, thus yielding the Iterated P ∗ Interpola-\ntion technique. Note that this method is more conservative\nthan the original P ∗ Interpolation technique because it tends\nto prevent pseudo-queries with low surface similarity to th e\nquery from being assigned high scores in early rounds.\n2.4 Estimating Rendition Probabilities\nRendition probabilities are the foundation upon which all\nour algorithms are built. To describe the method by which\nwe estimate them, we ﬁrst introduce some preliminary con-\ncepts. Let y be either a text string or a set of text strings.\nDenoting the number of times a term w occurs in y by\ntf(w ∈ y), for an n-term text sequence w1w2 · · · wn we deﬁne\npML\ny (w1w2 · · · wn)\ndef\n=\nn∏\nj=1\ntf(wj ∈ y)\n∑\nw′ tf(w′ ∈ y);\nthis is commonly known as y’smaximum likelihood estimate\n(MLE) for the sequence. The Dirichlet-smoothed version of\nthe MLE is deﬁned as\np[µ ]\ny (w1w2 · · · wn)\ndef\n=\nn∏\nj=1\ntf(wj ∈ y) + µ · pML\nD (wj )\n∑\nw′ tf(w′ ∈ y) + µ ,\nwhere the smoothing parameter µ controls the degree of re-\nliance on relative frequencies in the corpus rather than on\nthe counts in y.\nWhile the Dirichlet-smoothed unigram language model\njust deﬁned has been used directly [32, 20], we adopt the\nfollowing variant: for renderer r and text sequence x, we set\npr(x)\ndef\n= exp\n(\n−D\n(\npML\nx (·)\n⏐\n⏐⏐\n⏐ p[µ ]\nr (·)\n))\n∝\n|x|\n√\np[µ ]\nr (x),\nwhere D is the KL divergence, which has formed the ba-\nsis for other ranking principles as well [30, 15, 22, 13]; the\ntwo arguments to D are treated as distributions over terms\nrather than term sequences; and the omitted factor (which\nis of independent interest in other contexts [14]) drops out in\nthe re-scaling performed by the Doc-Audition and Cluster-\nAudition scoring methods. Our formulation provides some\nmathematical justiﬁcation for Lavrenko et al.’s “heuristi c\nadjustment” [17], proposed to handle underﬂow problems in\nprocessing long documents, to take the geometric mean of\np[µ ]\nr (x) rather than p[µ ]\nr (x) itself.\n3. RELATED WORK\nThe fundamental principle underlying pseudo-feedback-\nbased methods is that the top-ranked documents retrieved\nin response to a query may contain additional information\nregarding the user’s information need. The canonical ap-\nproach is to treat the pseudo-feedback documents as if they\nhad actually been deemed relevant by the user, and then\napply relevance-feedback techniques [26] to them. One such\nline of work is to use the feedback documents to re-weight\nquery terms and/or to identify additional terms with which\nto augment the query; since our wholesale replacement of the\nquery with pseudo-queries can be considered an extension of\nthis idea, in Section 4 we compare against one well-known\ninstantiation, namely, Rocchio [25]. Within the language-\nmodeling retrieval framework, treating the feedback docu-\nments as relevant often means estimating rendition proba-\nbilities using the feedback pool (where the members may be\ndiﬀerentially weighted) as data [18, 16, 15, 31, 29].\nLaﬀerty and Zhai [15] proposed an iterative probability-\nestimation sub-routine that alternates between terms and\ndocuments, which is remiscent of the shifting between clus-\nters and documents that our Cluster-Audition algorithm\nrepresents; but their intended application is not a direct\nscoring of potential retrieval candidates. Lavrenko and Cr oft’s\nrelevance model algorithm [18] has the same goal as ours, is\nalso based on language models, and posts state-of-the-art\nperformance; Section 4 describes it in more detail and re-\nports the results of our experimental comparisons against\nit.\nQuery drift has long been recognized as a key concern\nfor pseudo-feedback approaches [21, 6], and hence a num-\nber of coping techniques have been previously introduced.\nOne example is the application of boolean ﬁlters and term\nco-occurrence analysis [21]. The techniques we adopted fo-\ncus instead on incorporating rendition probabilities for t he\noriginal query, borrowing from previous work [31, 20, 13].\n4. EXPERIMENTS\nTo examine the eﬀectiveness of our algorithms and to de-\ntermine how much various aspects of our proposed retrieval\nframework contribute, we designed a number of evaluation\nexperiments.\nFirst, we compare the performance of our algorithms to\nthat of a language-model-based approach (henceforth base-\nline) in which documents are ranked according to pd(q∗).\nThis comparison serves not only to see whether our meth-\nods can outperform an eﬀective retrieval system, but also\nto highlight the merits (or lack thereof) of engaging in mul-\ntiple iterations of pseudo-query processing, since, as not ed\nabove, conceptually the basic language-modeling approach\ncorresponds to a single round or pass of our algorithms.\nWe also test our techniques for pseudo-query-processing\nagainst a well known and highly eﬀective pseudo-feedback\nmethod, the Rocchio algorithm [25] as applied to top-retrie ved\ndocuments.\nFinally, we study whether our particular ways of utiliz-\ning language models are beneﬁcial by testing how well they\nperform against the relevance model [18, 19]. The latter ap-\nproach takes a generative perspective: assuming that there\nis a single relevance language model R underlying the cre-\nation of both q∗ and the documents relevant to q∗, docu-\nments are ranked by their degree of “match” with R, rather\nthan by how well they directly match the query or set of\npseudo-queries. In implementation, Lavrenko and Croft es-\ntimate R by combining the language models of those doc-\numents assigning the highest rendition probabilities to q∗.\nThus, the relevance-model, similarly to our algorithms, is\na pseudo-feedback-based language-modeling approach, but\nclearly the speciﬁc way in which document-based language\nmodels are used is quite diﬀerent from the ways our algo-\nrithms employ them, and Lavrenko and Croft made no ex-\nplicit mention of clusters.\nAlthough our reference comparison models operate in dif-\nferent spaces (vector space vs. the probability simplex), i n\n[19] it is observed that if i.i.d sampling sampling is used for\nrelevance model estimation, then both the pseudo-feedback\nversion of Rocchio and the relevance model utilize a linear\ncombination of the top-retrieved documents’ models to con-\nstruct an expanded query model .\nWe conducted our experiments on the following three cor-\npora, drawn from TREC data:\ncorpus # of docs queries # of relevant docs\nAP89 84,678 1-46,48-50 3261\nAP88+89 164,597 101-150 4805\nLA+FR 187,526 401-450 1391\nThe AP89 corpus was pre-processed with the Porter stem-\nmer. For AP88+89 the Krovetz stemmer was used, and\nboth INQUERY stopwords [3] and length-one tokens were\nremoved to comply with the processing policy in [18]. For\nLA+FR, which is part of the TREC-8 corpus, neither stem-\nming nor stopword removal was applied. It is relatively het-\nerogeneous, and the LA dataset with TREC8 queries is con-\nsidered to be diﬃcult [11].\nFor queries, we used the titles of TREC topics rather than\nthe full descriptions, resulting in short queries containi ng 2-5\nterms on average.\nWe use both average non-interpolated precision and recall\nat N = 1000 as our evaluation measures. Statistically sig-\nniﬁcant diﬀerences in performance are determined using the\ntwo-sided Wilcoxon test at the 95% conﬁdence level.\n4.1 Implementation\nWe employed the Lemur toolkit [23] for a number of our\nexperiments. To collect pseudo-feedback, we used pd(q) to\ncreate an initial ranking All parameters were set to values\noptimizing average non-interpolated precision.\nOur implementation of Rocchio used the vector-space model\nwith log tf.idf term weighting to represent queries and docu -\nments. Similarity was measured via the inner product. The\nfree parameters were: (i) τ1 - the number of top-retrieved\ndocuments used for feedback, (ii) the number of terms to\naugment the original query with, and (iii) the weighting co-\neﬃcient for the augmenting terms (we only used positive\nfeedback). Note that while the number of (augmenting)\nterms is not modeled in Rocchio’s original method, we var-\nied it to obtain better performance and comply with the\noptimization steps we implemented for the relevance model\n(details further below). Our implementation yielded accur a-\ncies consistent with previously reported (optimized) resu lts.\nOur Lemur-based implementation of the relevance model\nutilized i.i.d sampling (following [19]) to construct R; the di-\nvergence D\n(\nR\n⏐\n⏐⏐\n⏐ pd(·)\n)\nserved as ranking criterion. The free\nparameters were τ1 (the number of top-retrieved documents)\nand an interpolation parameter controlling the evaluation of\nthe top retrieved documents’ language models.\nWe also experimented with clipping the relevance model\n[5, 8] to assign non-zero probability to only a restricted nu m-\nber of terms (up to a maximum of several hundred). This\nmodiﬁcation can be viewed as regulating the degree of query\nexpansion, or as an eﬃciency-improving heuristic.\nSome of our algorithms have quite a few free parameters.\nTo help prevent our algorithms from enjoying an unfair ad-\nvantage due to this fact alone, we implemented the follow-\ning policies. The language models forming the basis of both\nour methods and the baseline had the Dirichlet smoothing\nparameter value ﬁxed at µ = 2000, following [32]. All pa-\nrameters shared by our methods and the relevance models\n(e.g., τ1) were set identically for all the algorithms, with the\nfollowing search ranges:\nτ, the number of top renderers considered: {5, 10, 20, . . . , 100}\nfor Viterbi Doc-Audition; {5, 10, 20, 30, 40} for Doc-Audition;\nfrom {1, 2, 3, 4} for Cluster-Audition.\nτ1, the number of best renderers retrieved at the ﬁrst it-\neration: {5} ∪ { 10, 20, ..., 100} ∪ { 200, 300, 400, 500}.\nσ , the number of documents to which a cluster’s score\nis distributed (Equation 3): {5,10,20,30,40} for AP89 and\nAP88+89; {5,10} for LA+FR.\nγ, cluster size: 40 for AP89 and AP88+89; 10 for LA+FR.\nAP89 AP88+89 LA+FR\nprec recall prec recall prec recall\nBaseline 20. 74% 48. 67% 24. 26% 66. 62% 21. 72% 48. 81%\nVDoc 23 . 12 %∗ 55 . 20 % 28 . 28 %∗ 63. 68% 22 . 07 % 47. 81%\nDoc 22 . 81 %∗ 57 . 34 % 28 . 27 %∗ 71 . 13 % 22 . 48 %∗ 59.67%\nInt-Doc 24 . 48 %∗ 59 . 77 %∗ 30 . 28 %∗ 75 . 57 %∗ 22 . 98 %∗ 56 . 00 %∗\nClust 23 . 43 %∗ 63.57%∗ 28 . 76 %∗ 80.15%∗ 23 . 24 %∗ 56 . 79 %\nInt-Clust 24.56%∗ 62 . 77 %∗ 31.09%∗ 76 . 15 %∗ 23.40% 54 . 57 %∗\nTable 1: Comparison against the baseline. Statistically si gniﬁcant diﬀerences with the baseline are marked\nwith a star (*). Bold: best performance for each setting (col umn). Italics: results superior to the baseline.\nAP89 AP88+89 LA+FR\nprec recall prec recall prec recall\nClust 23.43% 63.57% 28.76%Rc 80.15%Rc 23.24%η 56.79%R\nInt-Clust 24.56%η 62.77% 31.09% 76.15%Rc 23.40%η 54.57%η R\nRocchio 22.85% 58.23% 30.69% 76.02% 18.21% 52.26%\nRelModel 24.72% 58.08% 32.72% 81.60% 22.03% 46.59%\nClippedRelModel 26.17% 63.48% 32.51% 82.10% 22.34% 56.65%\nTable 2: Comparison of the best pseudo-query algorithms aga inst Rocchio, the relevance model, and the\nclipped relevance model. Statistically signiﬁcant diﬀere nces with these algorithms are marked with η, R, and\nc, respectively. Bold: best performance for each setting (co lumn).\nρ, the number of rounds: 1–2, Cluster-Audition; 1–5, Viterbi\nDoc-Audition and Doc-Audition.\n4.2 Main results\nWe report results using the following abbreviations.\nVDoc Viterbi Doc-Audition\nDoc Doc-Audition\nClust Cluster-Audition\nThe preﬁx “Int-” indicates that the (non-iterated) P ∗ In-\nterpolation technique was employed for coping with query\ndrift; in all other cases, Truncated P ∗ Re-rank was applied.\nResults for other query-drift amelioration mechanisms are\nreported later in this section.\nTable 1 compares our algorithms’ performance to that of\nthe language-model baseline. We see that almost all our\nmethods outperform the language model in both average\nprecision and recall, often to a statistically signiﬁcant d e-\ngree. Moreover, it is clear that our cluster-based algorith m\nCluster-Audition, in either its Truncated P ∗ Re-rank or P ∗\nInterpolation version, yields the best results, outperfor ming\nnot only the language-modeling approach but all the non-\ncluster-based algorithms we have proposed. This ﬁnding\nfurther reinforces conclusions previously drawn in the lit er-\nature regarding the advantages of using clusters to represe nt\ncross-document contextual information [13, 20]; and the fa ct\nthat we see especially large improvements in recall provide s\npartial support towards our hypothesis that clusters can po -\ntentially alleviate the problem of aspect recall.\nMoving on to our second main comparison, Table 2 shows\nthat our cluster-based methods usually yield results that\nare better (sometimes to a signiﬁcant degree) than those of\nRocchio. Comparing our algorithms’ performance to that\nof the two versions of the relevance model, we observe the\nfollowing: on AP89, our cluster-based methods yield result s\nthat are, in a statistical sense, indistinguishable from th ose\nof the (clipped) relevance model; on LA+FR our methods\ntend to be superior to the (clipped) relevance model (some-\ntimes signiﬁcantly so); but on AP88+89, the (clipped) rel-\nevance model generally performs signiﬁcantly better than\nour cluster-based methods. In interpreting these results,\nthough, it is crucial to note that (1) our implementation of\nthe (clipped) relevance model involved an extremely wide-\nranging search over the parameter space, whereas as dis-\ncussed in Section 4.1, our methods only explored moderate\nparameter-setting ranges; and (2) clipping is a heuristic t hat\ncould potentially be adapted for use by our document- or\ncluster-based language models.\nWhile some preliminary results indicate that the perfor-\nmance of our methods can be further improved by more\nexhaustive parameter tuning, we believe that the main mes-\nsage of Table 2 is that we can achieve performance compet-\nitive with optimized state-of-the-art pseudo-feedback me th-\nods with relatively little optimization eﬀort.\n4.3 Further analysis\nExamination of the average-precision results in Table 1\nreveals that the P ∗ Interpolation technique is usually more\neﬀective at coping with query drift than Truncated P ∗ Re-\nrank. Table 3 provides a more extensive comparison of the\nfull set of query-drift-prevention techniques we have pro-\nposed. For simplicity, we report only the results of applyin g\nthese methods in conjunction with the Doc-Audition algo-\nrithm. It is apparent that most of our techniques achieve\ncomparable or better precision than is obtained by the orig-\ninal method by itself, and that P ∗ Interpolation is the “win-\nner”, although the more conservative iterated version (It-\nerated P ∗ Interpolation) ties it on two corpora. Investiga-\ntion into the optimal parameter settings revealed that, whi le\nperformance for Doc-Audition itself was optimized at a low\nnumber of iterations (which would have the eﬀect of keep-\ning precision from degrading), performance when preventio n\ntechniques were applied was best for a larger number of it-\nerations, enabling increase in recall along with preservat ion\nof high precision rates.\nAP89 AP88+89 LA+FR\nprec recall prec recall prec recall\nnone 22. 83% 51. 43% 28. 49% 69. 99% 21. 55% 53. 20%\nTruncated P ∗ Re-rank 22. 81% 57. 34% 28. 27% 71. 13% 22. 48% 59.67%\nP∗ Interpolation 24.48% 59.77% 30.28% 75.57% 22.98% 56. 00%\nIterated Truncation 19. 71% 51. 09% 26. 86% 63. 37% 16. 84% 31. 63%\nIterated Truncated P ∗ Re-rank 22. 76% 58. 02% 27. 96% 68. 05% 22. 49% 59. 45%\nIterated P ∗ Interpolation 24. 27% 59. 09% 30.28% 75.57% 22.98% 56. 00%\nTable 3: Comparison of techniques for query drift preventio n, the Doc-Audition scoring method. Bold: best\nperformance for a given evaluation setting (column). Note t hat Truncated P ∗ Re-rank improves recall over\nthe basic algorithm (none) as it achieves optimal precision with a diﬀerent parameter setting.\nAP89 AP88+89 LA+FR\n 0.44\n 0.46\n 0.48\n 0.5\n 0.52\n 0.54\n 0.56\n 0.58\n 0.6\n 0.62\n 0.64\n 0.12  0.14  0.16  0.18  0.2  0.22  0.24  0.26  0.28\nrecall\nprecision\nRecall&Precision wrt number of initially retrieved docs, corpus = AP89 \nDoc\nInt-Doc\nRelModel\nClippedRelModel\n 0.7\n 0.72\n 0.74\n 0.76\n 0.78\n 0.8\n 0.82\n 0.84\n 0.25  0.26  0.27  0.28  0.29  0.3  0.31  0.32  0.33\nrecall\nprecision\nRecall&Precision wrt number of initially retrieved docs, corpus = AP88+89 \nDoc\nInt-Doc\nRelModel\nClippedRelModel\n 0.3\n 0.35\n 0.4\n 0.45\n 0.5\n 0.55\n 0.6\n 0.13  0.14  0.15  0.16  0.17  0.18  0.19  0.2  0.21  0.22  0.23\nrecall\nprecision\nRecall&Precision wrt number of initially retrieved docs, corpus = LA+FR \nDoc\nInt-Doc\nRelModel\nClippedRelModel\n 0.45\n 0.5\n 0.55\n 0.6\n 0.65\n 0.7\n 0.12  0.14  0.16  0.18  0.2  0.22  0.24  0.26  0.28\nrecall\nprecision\nRecall&Precision wrt number of initially retrieved docs/clusters, corpus = AP89 \nClust\nInt-Clust\nRelModel\nClippedRelModel\n 0.7\n 0.72\n 0.74\n 0.76\n 0.78\n 0.8\n 0.82\n 0.84\n 0.25  0.26  0.27  0.28  0.29  0.3  0.31  0.32  0.33\nrecall\nprecision\nRecall&Precision wrt number of initially retrieved docs/clusters, corpus = AP88+89 \nClust\nInt-Clust\nRelModel\nClippedRelModel\n 0.3\n 0.35\n 0.4\n 0.45\n 0.5\n 0.55\n 0.6\n 0.65\n 0.12  0.14  0.16  0.18  0.2  0.22  0.24\nrecall\nprecision\nRecall&Precision wrt number of initially retrieved docs/clusters, corpus = LA+FR \nClust\nInt-Clust\nRelModel\nClippedRelModel\nFigure 1: Average precision vs. recall as τ1, the number of initially retrieved best renderers for the or iginal\nquery, takes the values 5,10,20,...100,200,...,500. Top a nd bottom rows show the best non-cluster- and cluster-\nbased algorithms, respectively, against the (clipped) rel evance model (note the relatively severe degradation\nin precision — “leftwards motion” — of the latter). To show de tail, the plots are not to the same scale.\nFinally, we explored a well-known weakness of language-\nmodel-based approaches using pseudo-feedback: sensitivi ty\nto the number τ1 of documents initially retrieved [28]. First,\nwe see from Figure 1 that the precision of our novel algo-\nrithms is much less aﬀected by increases in τ1 than the pre-\ncision of the (clipped) relevance model, which indicates th e\nmerits of both of our query-drift prevention techniques (al -\nthough clearly P ∗ Interpolation almost always outperforms\nTruncated P ∗ Re-rank at all values of τ1). It is also inter-\nesting to observe that increasing τ1 tends to have a positive\ninﬂuence on the recall of our cluster-based methods (which,\nafter all, were posited to improve aspect recall), whereas\neventually it has a negligible or negative inﬂuence on the\n(clipped) relevance model’s recall. In short, the performa nce\ncurves for our algorithms tend to move vertically, whereas\nthe (clipped) relevance model’s curves seem to exhibit more\nhorizontal movement. These trends suggest that our meth-\nods and the relevance model have complementary strengths.\n5. CONCLUSIONS\nWe presented a novel iterative pseudo-feedback approach\nto ad hoc information retrieval using cluster-based langua ge\nmodels. Starting from the original query, our methods re-\npeatedly seek potentially good renderers of a current set of\npseudo-queries, guided by the hypothesis that documents\nthat are the best renderers of a pseudo-query may be good\nalternate renditions of it.\nOne of the major challenges facing today’s retrieval en-\ngines is the problem of “aspect recall”. To alleviate this\nproblem, we proposed to take advantage of corpus struc-\nture via the consideration of cluster-based language mod-\nels as potential renderers; the key idea is that clusters can\nserve as a rich source of information regarding corpus as-\npects. Likewise, we examined several techniques for reduc-\ning query drift , which is yet another obstacle that both tra-\nditional and language-modeling-based pseudo-feedback ap -\nproaches need to overcome. As evidence that our techniques\nare eﬀective, we saw that our algorithms showed signiﬁcant\nimprovements in performance with respect to a standard\nlanguage-modeling approach, and produced results rivalin g\nthose of other state-of-the-art pseudo-feedback methods.\nFor future work, we plan to look into analyzing our meth-\nods in real-feedback settings, e.g., [27, 12, 2]. Furthermo re,\nwe would like to incorporate and examine additional clus-\ntering approaches for modeling corpus structure.\nAcknowledgments.We thank David Fisher for technical\nassistance with Lemur, and Jon Kleinberg and the anony-\nmous reviewers for valuable discussions and comments. We\nalso thank CMU for its hospitality during the year. This\npaper is based upon work supported in part by the National\nScience Foundation (NSF) under grant no. IIS-0329064 and\nCCR-0122581; SRI International under subcontract no. 03-\n000211 on their project funded by the Department of the In-\nterior’s National Business Center; and by an Alfred P. Sloan\nResearch Fellowship. Any opinions, ﬁndings, and conclu-\nsions or recommendations expressed are those of the authors\nand do not necessarily reﬂect the views or oﬃcial policies,\neither expressed or implied, of any sponsoring institution s,\nthe U.S. government, or any other entity.\n6. REFERENCES\n[1] James Allan. Incremental relevance feedback for\ninformation ﬁltering. In Proceedings of SIGIR , pages\n270–278, 1996.\n[2] James Allan. HARD track overview in TREC 2003: High\naccuracy retrieval from documents. In Proceedings of the\nTwelfth Text Retrieval Conference (TREC-12) , pages\n24–37, 2003.\n[3] James Allan, Margaret E. Connell, W. Bruce Croft,\nFang-Fang Feng, David Fisher, and Xiaoyan Li. INQUERY\nand TREC-9. In Proceedings of the Ninth Text Retrieval\nConference (TREC-9) , pages 551–562, 2001. NIST Special\nPublication 500-249.\n[4] Chris Buckley. Why current IR engines fail. In Proceedings\nof SIGIR , pages 584–585, 2004. Poster.\n[5] Margaret Connell, Ao Feng, Giridhar Kumaran, Hema\nRaghavan, Chirag Shah, and James Allan. UMass at TDT\n2004. TDT2004 System Description, 2004.\n[6] W. Bruce Croft and D. J. Harper. Using probabilistic\nmodels of document retrieval without relevance\ninformation. Journal of Documentation , 35(4):285–295,\n1979. Reprinted in Karen Sparck Jones and Peter Willett,\neds., Readings in Information Retrieval , Morgan\nKaufmann, pp. 339–344, 1997.\n[7] W. Bruce Croft and John Laﬀerty, editors. Language\nModeling for Information Retrieval . Number 13 in\nInformation Retrieval Book Series. Kluwer, 2003.\n[8] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. A\nlanguage modeling framework for selective query expansion .\nTechnical Report IR-338, Center for Intelligent Informati on\nRetrieval, University of Massachusetts, 2004.\n[9] Alan Griﬃths, H. Claire Luckhurst, and Peter Willett.\nUsing interdocument similarity information in document\nretrieval systems. Journal of the American Society for\nInformation Science (JASIS) , 37(1):3–11, 1986. Reprinted\nin Karen Sparck Jones and Peter Willett, eds., Readings in\nInformation Retrieval , Morgan Kaufmann, pp. 365–373,\n1997.\n[10] Donna Harman and Chris Buckley. The NRRC reliable\ninformation access (RIA) workshop. In Proceedings of\nSIGIR, pages 528–529, 2004. Poster.\n[11] Xiao Hu, Sindhura Bandhakavi, and ChengXiang Zhai.\nError analysis of diﬃcult TREC topics. In Proceedings of\nSIGIR, pages 407–408, 2003. Poster.\n[12] IJsbrand Jan Aalbersberg. Incremental relevance feed back.\nIn Proceedings of SIGIR , pages 11–22, 1992.\n[13] Oren Kurland and Lillian Lee. Corpus structure, langua ge\nmodels, and ad hoc information retrieval. In Proceedings of\nSIGIR, pages 194–201, 2004.\n[14] Oren Kurland and Lillian Lee. PageRank without\nhyperlinks: Structural re-ranking using links induced by\nlanguage models. In Proceedings of SIGIR , 2005.\n[15] John D. Laﬀerty and Chengxiang Zhai. Document language\nmodels, query models, and risk minimization for\ninformation retrieval. In Proceedings of SIGIR , pages\n111–119, 2001.\n[16] Victor Lavrenko. Optimal mixture models in IR. In\nEuropean Conference on Information Retrieval , pages\n193–212, 2002.\n[17] Victor Lavrenko, James Allan, Edward DeGuzman, Daniel\nLaFlamme, Veera Pollard, and Steven Thomas. Relevance\nmodels for topic detection and tracking. In Proceedings of\nthe Human Language Technology Conference (HLT) , pages\n104–110, 2002.\n[18] Victor Lavrenko and W. Bruce Croft. Relevance-based\nlanguage models. In Proceedings of SIGIR , pages 120–127,\n2001.\n[19] Victor Lavrenko and W. Bruce Croft. Relevance models in\ninformation retrieval. In Croft and Laﬀerty [7], pages 11–5 6.\n[20] Xiaoyong Liu and W. Bruce Croft. Cluster-based retriev al\nusing language models. In Proceedings of SIGIR , pages\n186–193, 2004.\n[21] Mandar Mitra, Amit Singhal, and Chris Buckley.\nImproving automatic query expansion. In Proceedings of\nSIGIR, pages 206–214, 1998.\n[22] Kenney Ng. A maximum likelihood ratio information\nretrieval model. In Proceedings of the Eighth Text Retrieval\nConference (TREC-8) , pages 483–492, 2000.\n[23] Paul Ogilvie and Jamie Callan. Experiments using the\nLEMUR toolkit. In Proceedings of the Tenth Text Retrieval\nConference (TREC-10) , pages 103–108, 2001.\n[24] Jay M. Ponte and W. Bruce Croft. A language modeling\napproach to information retrieval. In Proceedings of SIGIR ,\npages 275–281, 1998.\n[25] Joseph John Rocchio. Relevance feedback in informatio n\nretrieval. In Gerard Salton, editor, The SMART Retrieval\nSystem: Experiments in Automatic Document Processing ,\npages 313–323. Prentice Hall, 1971.\n[26] Ian Ruthven and Mounia Lalmas. A survey on the use of\nrelevance feedback for information access systems.\nKnowledge Engineering Review , 18(2):95–145, 2003.\n[27] Ian Soboroﬀ and Stephen E. Robertson. Building a ﬁlteri ng\ntest collection for TREC 2002. In Proceedings of SIGIR ,\npages 243–250, 2003.\n[28] Tao Tao and ChengXiang Zhai. A mixture clustering model\nfor pseudo feedback in information retrieval. In Proceedings\nof the International Federation of Classiﬁcation Societie s\n(IFCS), 2004. Invited paper.\n[29] Tao Tao and ChengXiang Zhai. A two-stage mixture model\nfor pseudo feedback. In Proceedings of the 27th SIGIR ,\npages 486–487, 2004. Poster.\n[30] Jinxi Xu and W. Bruce Croft. Cluster-based language\nmodels for distributed retrieval. In Proceedings of SIGIR ,\npages 254–261, 1999.\n[31] Chengxiang Zhai and John D. Laﬀerty. Model-based\nfeedback in the language modeling approach to information\nretrieval. In Proceedings of CIKM , pages 403–410, 2001.\n[32] Chengxiang Zhai and John D. Laﬀerty. A study of\nsmoothing methods for language models applied to ad hoc\ninformation retrieval. In Proceedings of SIGIR , pages\n334–342, 2001.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8649070262908936
    },
    {
      "name": "Query language",
      "score": 0.6199657320976257
    },
    {
      "name": "Key (lock)",
      "score": 0.6146912574768066
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6122894883155823
    },
    {
      "name": "Query expansion",
      "score": 0.5640164017677307
    },
    {
      "name": "Process (computing)",
      "score": 0.5498113036155701
    },
    {
      "name": "Information retrieval",
      "score": 0.5338505506515503
    },
    {
      "name": "Data mining",
      "score": 0.4370215833187103
    },
    {
      "name": "Iterative and incremental development",
      "score": 0.43366992473602295
    },
    {
      "name": "Cluster (spacecraft)",
      "score": 0.42034628987312317
    },
    {
      "name": "Language model",
      "score": 0.42027169466018677
    },
    {
      "name": "Natural language processing",
      "score": 0.3118169903755188
    },
    {
      "name": "Programming language",
      "score": 0.20797699689865112
    },
    {
      "name": "Software engineering",
      "score": 0.07386499643325806
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}