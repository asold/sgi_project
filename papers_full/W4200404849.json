{
    "title": "DrugEx v3: Scaffold-Constrained Drug Design with Graph Transformer-based Reinforcement Learning",
    "url": "https://openalex.org/W4200404849",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2102261607",
            "name": "Xuhan Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101834232",
            "name": "Kai Ye",
            "affiliations": [
                "Xi'an Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A1828061262",
            "name": "Herman W.T. van Vlijmen",
            "affiliations": [
                "Janssen (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1943172762",
            "name": "Adriaan P IJzerman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A302993782",
            "name": "Gerard J. P. van Westen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2101234009",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Due to the large drug-like chemical space available to search for feasible drug-like molecules, rational drug design often starts from specific scaffolds to which side chains/substituents are added or modified. With the rapid growth of the application of deep learning in drug discovery, a variety of effective approaches have been developed for de novo drug design. In previous work, we proposed a method named DrugEx, which can be applied in polypharmacology based on multi-objective deep reinforcement learning. However, the previous version is trained under fixed objectives similar to other known methods and does not allow users to input any prior information (i.e. a desired scaffold). In order to improve the general applicability, we updated DrugEx to design drug molecules based on scaffolds which consist of multiple fragments provided by users. In this work, the Transformer model was employed to generate molecular structures. The Transformer is a multi-head self-attention deep learning model containing an encoder to receive scaffolds as input and a decoder to generate molecules as output. In order to deal with the graph representation of molecules we proposed a novel positional encoding for each atom and bond based on an adjacency matrix to extend the architecture of the Transformer. Each molecule was generated by growing and connecting procedures for the fragments in the given scaffold that were unified into one model. Moreover, we trained this generator under a reinforcement learning framework to increase the number of desired ligands. As a proof of concept, our proposed method was applied to design ligands for the adenosine A2A receptor (A2AAR) and compared with SMILES-based methods. The results demonstrated the effectiveness of our method in that 100% of the generated molecules are valid and most of them had a high predicted affinity value towards A2AAR with given scaffolds.",
    "full_text": "DrugEx v3: Scaffold-Constrained Drug Design with Graph 1 \nTransformer-based Reinforcement Learning  2 \nXuhan Liu1, Kai Ye2, Herman W. T. van Vlijmen1,3, Adriaan P. IJzerman1, Gerard J. P. 3 \nvan Westen1, * 4 \n 5 \n1Drug Discovery and Safety, Leiden Academic Centre for Drug Research, Einsteinweg 6 \n55, Leiden, The Netherlands 7 \n2School of Electrics and Information Engineering, Xiâ€™an Jiaotong  University, 28 8 \nXianningW Rd, Xiâ€™an, China 9 \n3Janssen Pharmaceutica NV , Turnhoutseweg 30, B-2340, Beerse, Belgium 10 \n 11 \n*To whom correspondence should be addressed: Gerard J. P. van Westen, Drug 12 \nDiscovery and Safety, Leiden Academic Centre for Drug Research, Einsteinweg 55, 13 \nLeiden, The Netherlands. Tel: +31-71-527-3511. Email: gerard@lacdr.leidenuniv.nl. 14 \n 15 \nEmail Address of other authors: (1) Xuhan Liu: x.liu@lacdr.leidenuniv.nl; (2) Kai Ye: 16 \nkaiye@xjtu.edu.cn; (3) Herman W. T. van Vlijmen: hvvlijme@its.jnj.com; (4) Adriaan 17 \nP. IJzerman: ijzerman@lacdr.leidenuniv.nl.18 \nAbstract 19 \nDue to  the large drug-like chemical space available to search for feasible drug -like 20 \nmolecules, rational drug design often starts from specific scaffolds to which side 21 \nchains/substituents are added or modified. With the rapid growth of the application of 22 \ndeep learning in drug discovery, a variety of effective approaches have been developed 23 \nfor de novo drug design. In previous work, we proposed a method named DrugEx, 24 \nwhich can be applied in polypharmacology based on multi -objective deep 25 \nreinforcement learning. However, the previous version is trained under fixed objectives 26 \nsimilar to other known methods and does not allow users to input any prior information 27 \n(i.e. a desired scaffold) . In order to improve the general applicability, we updated 28 \nDrugEx to design drug molecules based on scaffolds which consist  of multiple 29 \nfragments provided by users. In this work,  the Transformer model was employed to 30 \ngenerate molecular structures . The Transformer is  a multi -head self -attention deep 31 \nlearning model containing an encoder to receive scaffolds as input and a decoder to 32 \ngenerate molecules as output . In order to deal with the graph representation of 33 \nmolecules we proposed a novel positional encoding for each  atom and bond based on 34 \nan adjacency matrix to extend the architecture of the Transformer. Each molecule was 35 \ngenerated by growing and connecting procedures for the fragments in the given scaffold 36 \nthat were unified into one model. Moreover, we trained this generator under a 37 \nreinforcement learning framework to increase the number of desired ligands. As a proof 38 \nof concept, our proposed method was applied to design ligands for the adenosine A2A 39 \nreceptor (A2AAR) and compared with SMILES -based methods. The results 40 \ndemonstrated the effectiveness of our method in that 100% of the generated molecules 41 \nare valid and most of them had a high predicted affinity value towards A2AAR with 42 \ngiven scaffolds.  43 \n 44 \nKeywords: deep learning, reinforcement learning, policy gradient, drug design, 45 \nTransformer, multi-objective optimization 46 \n  47 \nIntroduction 48 \nDue to the size of  drug-like chemical space (i.e. estimated at 10 33 - 1060 organic 49 \nmolecules) 1 it is impossible to screen every corner of it to discover optimal drug 50 \ncandidates. Commonly, the specific scaffolds derived from endogenous substances , 51 \nhigh throughput screening, or a phenotypic assay 2 are taken as a starting point to design 52 \nanalogs while side chains/substituents are added or modified  3. These fragments are 53 \nused as â€œbuilding blocksâ€ to develop drug leads with e.g. combinatorial chemistry such 54 \nas growing, linking, and merging 4. After a promising drug lead has been discovered it 55 \nis further optimized by modifying side chains to improve potency towards the relevant 56 \ntargets, selectivity over off-targets, and physicochemical properties  which in turn can 57 \nimprove safety and tolerability 5. 58 \n 59 \nIn scaffold-based rational drug design, it is generally accepted that a  chemical space 60 \nconsisting of 109 diverse molecules can be sampled with only 103 fragments 6 . For 61 \ninstance, o ne well known class of drug targets are G Protein -coupled receptors 62 \n(GPCRS), a family via which approximately 35% of drug exert their effect  7. The 63 \nadenosine receptors (ARs) form a family within  rhodopsin-like GPCRs  and include 64 \nfour subtypes (A1, A2A, A2B and A3). Each of them has a unique pharmacological profile, 65 \ntissue distribution , and effector coupling  8, 9 . ARs are ubiquitously distributed 66 \nthroughout the human tissues, and involved in many biological processes and diseases 67 \n10. As adenosine is the endogenous agonist of ARs, a number of known ligands of the 68 \nARs are adenosine analogs and have a common scaffold. Examples include  purines, 69 \nxanthines, triazines, pyrimidines, and the inclusion of a ribose moiety 11. In this work, 70 \nwe aim to design novel ligands for this family of receptors using a deep learning-based 71 \ndrug design method.  72 \n 73 \n74 \nDeep learning based methods have been gaining ground in computational drug 75 \ndiscovery, including de novo design, based on rapid developments over the last decade 76 \n12. Deep learning has achieved breakthroughs in visual recognition, natural language 77 \nprocessing, and other data -rich fields  13. For distribution-directed issues, Gomez-78 \nBombarelli et al. implemented variational autoencoders (V AE) to map molecules into 79 \na latent space where each point can also be decoded into unique molecules inversely 14. 80 \nThey used recurrent neural networks (RNNs) to successfully learn SMILES (simplified 81 \nmolecular-input line-entry system) grammar and construct a distribution of molecular 82 \nlibraries 15. For goal-directed issues, Sanchez-Lengeling et al. combined reinforcement 83 \nlearning and generative adversarial networks (GANs) to develop an approach named 84 \nORGANIC to design active compounds for a given target 16. Olivecrona et al. proposed 85 \nthe REINVENT algorithm which updated the reinforcement learning with a Bayesian 86 \napproach and combined RNNs to generate SMILES-based desired molecules 17, 18 . 87 \nMoreover, Lim et al. proposed a method for s caffold-based molecular design with a 88 \ngraph generative model 19. Li et al. also used deep lea rning to develop a tool named 89 \nDeepScaffold for this issue 20. ArÃº sâ€‘Pous et al. employed RNNs to develop a SMILES-90 \nbased scaffold decorator for de novo drug design 21. Yang et al. used the Transformer 91 \nmodel 22 to develop a tool named SyntaLinker for automatic fragment linking 23. Here 92 \nwe continue to address  on this issue further with different molecular representations 93 \nand deep learning architectures. 94 \n 95 \nIn previous studies we investigated the performance of RNNs and proposed a method 96 \nnamed DrugEx by integrating reinforcement learning to balance distribution-directed 97 \nand goal -directed tasks  24. Furthermore, we updated DrugEx with multi -objective 98 \nreinforcement learning and applied it in  polypharmacology 25. However, the well-99 \ntrained model cannot receive any input data from users and can only reflect the 100 \ndistribution of the desired molecules with fixed conditions. If the objectives are changed, 101 \nthe model need s to be trained again. In this work, we compared different end-to-end 102 \ndeep learning methods to update the DrugEx model to allow users to provide prior 103 \ninformation, e.g. fragments that should occur in the generated molecules. Based on the 104 \nextensive experience in our group with the A2AAR, we continue to take this target as an 105 \nexample to evaluate the performance of our proposed methods. In the following context, 106 \nwe will discuss  the case of  scaffold-constrained drug design, i.e. the model take s 107 \nscaffolds composed of multiple fragments as input to generate desired molecules which 108 \nare predicted to be active to A2AAR. All python code for this study is freely available 109 \nat http://gitlab.com/XuhanLiu/DrugEx. 110 \n  111 \nMaterials and Methods 112 \nData source 113 \nThe ChEMBL set was reused from our work on DrugEx v2 25. This set consisted of 114 \nsmall molecule c ompounds downloaded from ChEMBL  using a SMILES notation  115 \n(version 27) 26. There were ~1.7 million molecules remained for model pre -training 116 \nafter data preprocessing implemented by RDKit . Preprocessing included neutralizing 117 \ncharges, removing metals and small fragments.  In addition , 10,828 ligands and 118 \nbioactivity data were extracted from ChEMBL to construct the LIGAND set, containing 119 \nstructures and activities from  bioassays towards the four human adenosine receptors. 120 \nThe LIGAND set was used for fine-tuning the generat ive model. Molecules with 121 \nannotated A2AAR activity were used to train a bioactivity prediction model. If multiple 122 \nmeasurements for the same ligand existed, the average pChEMBL value (pX, including 123 \npKi, pKd, pIC50 or pEC50) was calculated and duplicate items were removed. In order 124 \nto judge if the molecule is desired or not, the threshold of affinity was defined as pX = 125 \n6.5 to predict if the compound was active (>= 6.5) or inactive (< 6.5).  126 \n 127 \nThe dataset was constructed with an input-output pair for each data point. Each 128 \nmolecule was decomposed into a batch of fragments with the BRICS method 27 in 129 \nRDKit (Fig 1A). If a molecule contained more than four leaf fragments, the smaller 130 \nfragments were ignored and a maximum of four larger fragments were reserved to be 131 \nrandomly combined at one time. Their SMILES sequences were joined with â€˜.â€™ as input 132 \ndata which were paired with the full  SMILES of molecules . Here, the scaffold was 133 \ndefined as the combination of di fferent fragments which can be either continuous 134 \n(linked) or discrete (separated). The resulting scaffold-molecule pairs formed the input 135 \nand output data (Fig 1B). After completion of construction of the data pairs the set was 136 \nsplit into a training set and test set with the ratio 9:1 based on the input scaffolds. The 137 \nresulting ChEMBL set contained 10,418,681 and 1,083,271 pairs for training and test 138 \nset, respectively. The LIGAND set contained 61,413 pairs in the training set and 7,525 139 \npairs in the test set.  140 \n 141 \nFig. 1: scaffold-molecule pair dataset construction . (A) Each molecule in the dataset is decomposed hierarchically into a series of fragments with  the BRICS 142 \nalgorithm. (B) Subsequently data pairs between input and output are created. Combinations of leaf fragments form the scaffold as input,  while the whole molecule 143 \nbecomes the output. Each token in the SMILES sequences is separated by different colors. (C) After conversion to the adjacency matrix, each molecule was represented 144 \nas a graph matrix. The graph matrix contains five rows, standing for the atom, bond, previous and current positions, and fragment index. The columns are composed 145 \nwith three parts to store the information of the scaffold, the growing section and the linking section. (D) All tokens are collected to construct the vocabularies for 146 \nSMILES-based and graph-based generators, respectively. (E) An example of the input and output matrices for the SMILES representation of scaffolds and molecules  147 \n 148 \n\nMolecular representations 149 \nIn this study we tested two different molecular representations: SMILES and graph. For 150 \nSMILES representations each scaffold-molecule pair was  transformed into two 151 \nSMILES sequences which were then split into different tokens to denote atoms, bonds, 152 \nor other tokens for grammar control (e.g. parentheses or numbers). All of these tokens 153 \nwere put together to form a vocabulary which recorded the index of each token  (Fig. 154 \n1D). Here, we used the same conversion procedure and vocabulary as in DrugEx v2 25. 155 \nIn addition, a start token (GO) was put at the beginning of a batch of data as input and 156 \nan end token (END) at the end of the same batch of data as output. After sequence 157 \npadding with a blank token at empty positions , each SMILES sequence was rewritten 158 \nas a series of token indices with a fixed length. Subsequently all of these sequences for 159 \nboth scaffolds and molecules were concatenated to construct the input and output 160 \nmatrix (Fig. 1E).  161 \n 162 \nFor the graph representation  each molecule was represented as a five-row matrix, in 163 \nwhich the first two rows stand for the index of the atom and bond types, respectively . 164 \nThe third and fourth  rows represent the position of previous and current atoms 165 \nconnected by a bond (Fig. 1C). The columns of this matrix contain three sections to 166 \nstore the scaffold, growing part, and linking part. The scaffold section began with a start 167 \ntoken in the first row and the last row was labelled with the index of each scaffold 168 \nstarting from one. The scaffolds of each molecule are put in the beginning of the matrix, 169 \nfollowed by the growing part for the scaffold, and the last part is the connectin g bond 170 \nbetween these growing fragments with single bonds. For the growing and linking 171 \nsections the last row was always zero and these two sections were separated by the 172 \ncolumn of the end token. It is worth noticing that the last row was not directly involved 173 \nin the training process. The vocabulary for graph representation was different from the 174 \nSMILES representation, contains 38 atom types (Table S1), and four bond types (single, 175 \ndouble, triple bonds and no bond). For each column, If an atom is the first occurrence 176 \nin a given scaffold the type of the bond will be empty (indexed as 0 with token â€˜*â€™). In 177 \naddition, if the atom at the current position has occurred in the matrix, the type of the 178 \natom in this column  will be empty. In order to grasp more details of the graph 179 \nrepresentation, we also provided the pseudocode for encoding (Table S2) and decoding 180 \n(Table S3). 181 \n 182 \nEnd-to-End Deep learning 183 \nIn this work, we compared three different sequential end-to-end DL architectures to 184 \ndeal with different molecular representations of either graph or SMILES (Fig. 2). These 185 \nmethods included: (A) a Graph Transformer, (B) an LSTM-based en coder-decoder 186 \nmodel (LSTM-BASE), (C) an LSTM-based encoder-decoder model with an attention 187 \nmechanism (LSTM+ATTN) and (D) a Sequential Transformer model. All of these DL 188 \nmodels were constructed with PyTorch 28. 189 \n 190 \n 191 \nFig. 2: Architectures of four different end-to-end deep learning models : (A) The Graph 192 \nTransformer; (B) The LSTM-based encoder-decoder model (LSTM-BASE); (C) The LSTM-based 193 \nencoder-decoder model with attention mechanisms ( LSTM+ATTN); (D) The s equential 194 \nTransformer model. The Graph Transformer accepts a graph representation as input and SMILES 195 \nsequences are taken as input for the other three models. 196 \n 197 \nFor the SMILES representation based models three different types were constructed as 198 \nfollows (Fig. 2, right). The encoder and decoder in t he LSTM-BASE model (Fig. 2B) 199 \n\nhad the same architectures, containing one embedding layer, three recurrent layers, and 200 \none output layer (as used in DrugEx v2). The number of neurons in the embedding and 201 \nhidden layers were 128 and 512, re spectively. The hidden states of the recurrent layer 202 \nin the encoder are directly sent to the decoder as the initial states.  On the basis of the 203 \nLSTM-BASE model an attention layer was added between the encoder and decoder to 204 \nform the LSTM+ATTN model (Fig. 2C). The attention layer calculates the weight for 205 \neach position of the input sequence to determine which position the decoder needs to 206 \nfocus on during the decoding process. For each step the weighted sums of the output 207 \ncalculated by the encoder are combined with the output of the embedding layer in the 208 \ndecoder to form the input for the recurrent layers. The output of the recurrent layers is 209 \ndealt with by the output layer  to generate the probability distribution of tokens in the 210 \nvocabulary in both of these two models.  211 \n 212 \nThe sequential Transformer has a distinct architecture compared to the LSTM+ATTN 213 \nmodel although it also exploits  an attention mechanism . For the embedding layers  214 \nâ€œposition encodingsâ€ are added into the typical embedding structure as the first layer of 215 \nthe encoder and decoder . This ensures  that the model no long er needs to encode the 216 \ninput sequence token by token  but can process all tokens in parallel . For the position 217 \nembedding, sine and cosine functions are used to define its formula as follows: 218 \nğ‘ƒğ¸(ğ‘,2ğ‘–) =sin(ğ‘ğ‘œğ‘  100002ğ‘– ğ‘‘ğ‘šâ„â„ ) 219 \nğ‘ƒğ¸(ğ‘,2ğ‘–+1) =cos(ğ‘ğ‘œğ‘  100002ğ‘– ğ‘‘ğ‘šâ„â„ ) 220 \nwhere PE(p, i) is the ith dimension of the position encoding at position p. It has the same 221 \ndimension dm = 512 as the typical embedding vectors so that the two can be summed.  222 \n 223 \nIn addition, self-attention is used in the hidden layers to cope with long -range 224 \ndependencies. For each hidden layer in the encoder, it employs a residual connection 225 \naround a multi -head self -attention sublayer and feed -forward sublayer followed by 226 \nlayer normalization. Besides these two sublayer s in the decoder a third sublayer with 227 \nmulti-head attention is inserted to capture the information from output of the encoder.  228 \n 229 \nThis self-attention mechanism is defined as the scaled dot-product attention with three 230 \nvectors: queries ( Q), keys ( K) and values ( V), of which the dimensions are dq, dk, dv, 231 \nrespectively. The output matrix is computed as: 232 \nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„,ğ¾,ğ‘‰)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘„ğ¾âŠº\nâˆšğ‘‘ğ‘˜\n)ğ‘‰ 233 \nInstead of a single attention function, the Transformer adopts multi-head attention to 234 \ncombine information from different represe ntations at different positions which is 235 \ndefined as: 236 \nMultiHead(ğ‘„,ğ¾,ğ‘‰)=Concat(â„ğ‘’ğ‘ğ‘‘1,â€¦,â„ğ‘’ğ‘ğ‘‘â„)ğ‘Šğ‘‚ 237 \nwhere h is the number of heads. For each head, the attention values were calculated by 238 \ndifferent and learned linear projections with Q, K and V as follows:  239 \nâ„ğ‘’ğ‘ğ‘‘ğ‘– =ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„ğ‘Šğ‘–\nğ‘„,ğ¾ğ‘Šğ‘–\nğ¾,ğ‘‰ğ‘Šğ‘–\nğ‘‰) 240 \nwhere WO, WQ, WK and WV are metrics of learned weights and we set h = 8 as the number 241 \nof heads and dk = dv = 64 in this work.  242 \n 243 \nFor the graph representation of the molecules we updated the sequential Transformer 244 \nstructure to propose  a Graph Transformer  (Fig. 2A) . Similar to the sequential 245 \nTransformer the Graph Transformer also requires the encodings of both word and 246 \nposition as the input. For the input word, the atom and bond cannot be processed 247 \nsimultaneously; therefore we combined the i ndex of atom and bond toget her and 248 \ndefined it as follows: 249 \nğ¼=ğ¼ğ‘ğ‘¡ğ‘œğ‘š Ã—4+ ğ¼ğ‘ğ‘œğ‘›ğ‘‘ 250 \nThe index of the input word (I) for calculating word vectors is obtained by multiplying 251 \nthe atom index (Iatom) by four (the total number of bond types defined) and subsequently 252 \nadd the  bond index  (Ibond). Similarly, the position of each step cannot be used to 253 \ncalculate the position encoding directly. Faced with more complex data structure than 254 \nsequential data, Dosovitskiy et al. proposed a new positional encoding scheme to define 255 \nthe position for each patch in image data for image recognition  29. Inspired by their 256 \nwork the position encoding at each step was defined as: 257 \nğ‘ƒ=ğ‘ƒğ‘ğ‘¢ğ‘Ÿğ‘Ÿ Ã—ğ¿ğ‘šğ‘ğ‘¥ + ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘£ 258 \nThe input position (P) for calculating the position  encoding was obtained by 259 \nmultiplying the current position (Pcurr) by the max length (Lmax) and then adding the 260 \nprevious position (Pprev), which was then processed with the same positional encoding 261 \nmethod as with the sequential Transformer. For the decoder, the hidden vector from the 262 \ntransformer was taken as the starting point to be decoded by a GRU-based recurrent 263 \nlayer; and the probability of atom, bond, previous and current position was decoded one 264 \nby one sequentially.  265 \n 266 \nWhen graph-based molecules are generated, the chemical valence rule is checked in 267 \nevery step. Invalid value s of atom and bond types will be masked and an incorrect 268 \nprevious or current position will be removed ensuring the validity of all  generated 269 \nmolecules. It is worth noticing that before being encoded, each molecule will be 270 \nkekulized, meaning that the aromatic rings will be inferred to transform into eit her 271 \nsingle or double bonds. The reason for this is that aromatic bonds interfere with the 272 \ncalculation of the valence value for each atom.  273 \n 274 \nDuring the training process  of SMILES -based models,  a negative log likelihood 275 \nfunction was used to construct the loss function to guarantee that the probability of the 276 \ntoken at each step  in the output sequence became large enough in the probability 277 \ndistribution of the vocabulary calculated by the deep learning model . In comparison, 278 \nthe loss function used by the Graph Transformer model also contains four parts for atom, 279 \nbond, previous and current sites. Here the sum of these negative log probability values 280 \nis minimized to optimize the parameters in the model. For this, the Adam algorithm 281 \nwas used for the optimization of the loss function. Here, the learning rate was set as 10-282 \n4, the batch size was 256, and training steps were set to 20 epochs for pre-training and 283 \n1,000 epochs for fine-tuning.  284 \n 285 \n286 \nMulti-objective optimization 287 \nIn order to combine multiple objectives we exploited a Pareto-based ranking algorithm 288 \nwith GPU acceleration as mentioned in DrugEx v2 25. Given two solutions m1 and m2 289 \nwith their scores (x1, x2, ..., xn) and (y1, y2, â€¦, yn), then m1 is said to Pareto dominate m2 290 \nif and only if: 291 \nâˆ€ jâˆˆ{1,â€¦,n}: ğ‘¥ğ‘— â‰¥ğ‘¦ğ‘— ğ‘ğ‘›ğ‘‘ âˆƒ jâˆˆ{1,â€¦,n}: ğ‘¥ğ‘— >ğ‘¦ğ‘— 292 \notherwise, m1 and m2 are non-dominated with each other. After the dominance between 293 \nall pair of solutions being determined, the non-dominated scoring algorithm is exploited 294 \nto obtain a rank of Pareto frontiers which consist of a set of solutions. A fter obtaining 295 \nfrontiers between dominant solutions , molecules were ranked based on the average 296 \nTanimoto-distance to other molecules instead of the commonly used crowding distance 297 \nin the same  frontier. Subsequently molecules with smaller average distances were 298 \nranked on the top. The final reward R* is defined as: 299 \nğ‘…âˆ— =\n{\n \n  0.5+ğ‘˜âˆ’ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘\n2ğ‘ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘\n, ğ‘–ğ‘“ ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘\n ğ‘˜\n2ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘\n,                   ğ‘–ğ‘“ ğ‘¢ğ‘›ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘\n 300 \nhere k is the index of the solution in the Pareto rank. Rewards of undesired and desired 301 \nsolutions will be evenly distributed in (0, 0.5] and (0.5, 0.1], respectively.  302 \n 303 \nIn this work , we took two objectives into consideration: 1) the QED score 30 as 304 \nimplemented by RDKit (from 0 to 1) to evaluate the drug-likeness of each molecule (a 305 \nlarger value means more drug-like) ; 2) an affinity score towards the A2AAR which was 306 \nimplemented by a random forest regression model with Scikit-Learn 31 like in DrugEx 307 \nv2 25. The input descriptors consisted of 2048D ECFP6 fingerprints and 19D physico -308 \nchemical descriptors  (PhysChem). Phys Chem included: molecular weight, logP,  309 \nnumber of H bond acceptors and donors, number of rotatable bonds, number of amide 310 \nbonds, number of bridge head atoms, number of hetero atoms, number of spiro atoms, 311 \nnumber of heavy atoms, the fraction of SP3 hybridized carbon atoms, number of 312 \naliphatic rings, number of saturated rings, number of total rings, number of aromatic 313 \nrings, number of heterocycles, number of valence electrons, polar surface area , and 314 \nWildman-Crippen MR value.  Again it was determined if generated molecules are 315 \ndesired based on the Affinity score (larger than the threshold = 6.5). In addition, the SA 316 \nscore was also exploited an independent measurement to evaluate the synthesizability 317 \nof generated molecules, which is also calculated by RDKit 32.  318 \n 319 \nReinforcement learning  320 \nIn this work we constructed a reinforcement learning framework based on the interplay 321 \nbetween a Graph Transformer (agent) and two scoring functions (environment). A 322 \npolicy gradient method was implemented to train the reinforcement learning model, the 323 \nobjective function is designated as follows: 324 \nğ½(ğœƒ)=ğ”¼[ğ‘…âˆ—(ğ‘¦1:ğ‘‡)|ğœƒ]=âˆ‘ğ‘™ğ‘œğ‘”ğº(ğ‘¦ğ‘¡|ğ‘¦1:ğ‘¡âˆ’1)âˆ™ğ‘…âˆ—(ğ‘¦1:ğ‘‡)\nğ‘‡\nğ‘¡=1\n 325 \nFor each step  t during the generation process  the g enerator ( G) determines the 326 \nprobability of each token (yt) from the vocabulary to be chosen based on the generated 327 \nsequence in previous steps (y1:t-1). In the sequence-based models yt can only be a token 328 \nin the vocabulary to construct SMILES while it can be different type of atoms or bonds 329 \nor the previous or current position  in the graph -based model. The parameters in the 330 \nobjective function are updated by employing a policy gradient based on the expected 331 \nend reward (R*) received from the predictor. By maximizing this function the parameter 332 \nğœƒ in the generator can be optimized to ensure that the generator designs desired 333 \nmolecules which obtain a high reward score. 334 \n 335 \nIn order to improve the diversity and reliability of generated molecule s, we 336 \nimplemented our exploration strategy for molecule generation during the training loops. 337 \nIn the training loop our generator is trained to produce the chemical space as defined 338 \nby the target of interest. In this strategy there are two networks with the sa me 339 \narchitectures, an exploitation net (GÎ¸) and an exploration net (GÏ†). GÏ† did not need to 340 \nbe trained and its parameters are always fixed and it is based on the general drug -like 341 \nchemical space for diverse targets obtained from ChEMBL. The parameters in GÎ¸ on 342 \nthe other hand  were updated for each epoch based on the policy gradient. Again an 343 \nexploring rate (Îµ) was defined with a range of [0.0, 1.0] to determine the percentage of 344 \nscaffolds being randomly selected as input by GÏ† to generate molecules. Conversely GÎ¸ 345 \ngenerated molecules with other input scaffolds. After the training process was finished 346 \nGÏ† was removed and only GÎ¸ was left as the final model for molecule generation. 347 \n 348 \nPerformance evaluation 349 \nIn order to evaluate the performance of the generators, four coefficients were calculated 350 \nfrom the population of  generated molecules  (validity, accuracy, desirability, and 351 \nuniqueness) which are defined as: 352 \nValidity=ğ‘ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘\nğ‘ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\n 353 \nAccuracy=ğ‘ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘’\nğ‘ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\n 354 \nDesirability=ğ‘ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘Ÿğ‘’ğ‘‘\nğ‘ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\n 355 \nUniqueness=ğ‘ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’\nğ‘ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\n 356 \nhere Ntotal is the total number of molecules, Nvalid is the number of molecules parsed as 357 \nvalid SMILES sequences, Naccurate is the number of molecules that contained all given 358 \nscaffolds, Ndesired is the number of desired molecules that reach all required objectives, 359 \nand Nunique is the number of molecules which are different from others in the dataset.  360 \n 361 \nTo measure molecular diversity, we adopted the Solow Polasky measurement as in the 362 \nprevious work. This approach was proposed by Solow and Polasky in 1994 to estimate 363 \nthe diversity of a biological population in an eco -system 33. The formula to calculate 364 \ndiversity was redefined to normalize the range of values from [1, m] to (0, m] as follows: 365 \nğ¼(ğ´)= 1\n|ğ´|ğ’†âŠºğ¹(ğ’”)âˆ’1ğ’† 366 \nwhere A is a set of drug molecules with a size of |A| equal to m, e is an m-vector of 1â€™s 367 \nand F(s) = [f(dij))] is a non -singular m Ã— m distance matrix, in which f(dij) stands for 368 \nthe distance function of each pair of molecule provided as follows: 369 \nğ‘“(ğ‘‘)=ğ‘’âˆ’ğœƒğ‘‘ğ‘–ğ‘— 370 \nhere we defined the distance dij of molecules si and sj by using the Tanimoto-distance 371 \nwith ECFP6 fingerprints as follows: 372 \nğ‘‘ğ‘–ğ‘— =ğ‘‘(ğ‘ ğ‘–,ğ‘ ğ‘—)=1âˆ’|ğ‘ ğ‘– âˆ©ğ‘ ğ‘—|\n|ğ‘ ğ‘– âˆªğ‘ ğ‘—| ,  373 \nwhere | si âˆ© sj | represents the number of common fingerprint bits, and | si âˆª sj | is the 374 \nnumber of union fingerprint bits. 375 \nResults and Discussion 376 \nFragmentation of molecule 377 \nAs stated we decomposed each molecule into a series of fragments with the BRICS 378 \nalgorithm to construct a fragment-molecule pair. Each organic compound can be split 379 \ninto retrosynthetically interesting chemical substructures with a compiled elaborate set 380 \nof rules. For the ChEMBL and LIGAND sets, we respectively obtained 194,782 and 381 \n2,223 fragments. We further split the LIGAND set into three parts: active ligands  382 \n(LIGAND+, 2,638), inactive ligands  (LIGAND-, 2710) and undetermined ligands 383 \n(LIGAND0, 5480) based on the pX of bioactivity for A2AAR. The number of fragments 384 \nin these four datasets have a similar distribution (Fig. 3A) and there are approximately 385 \nfive fragments on average for each molecule with a 95% confidence between [0, 11] 386 \n(Fig. 3A).  387 \n 388 \nIn the LIGAND set the three subsets have a similar molecular weight distribution of the 389 \nfragments (Fig. 3B) with an average of  164.3 Da, smaller than in the ChEMBL set 390 \n(247.3 Da). In order to check the similarity of these fragments we use d the Tanimoto 391 \nsimilarity calculation with ECFP4 fingerprints  between each pair of fragments in the 392 \nsame dataset. We found that most of them were smaller than 0.5 indicating that they are 393 \ndissimilar to each other (Fig. 3C). Especially, the fragments in the LIGAND+ set have 394 \nthe largest diversity. Moreover, t he distribution of different fragments in these three 395 \nsubsets of the LIGAND set are shown in Fig. 3D. The molecules in these three subsets 396 \nhave their unique fragments and share some common substructures. 397 \n 398 \n  399 \nFig 3: Analysis of some properties of fragments in the ChEMBL set and three LIGAND subsets. 400 \n(A) Violin plot for the distribution of the number of fragments per molecules; (B) Distribution of 401 \nmolecular weight of these fragments; (C) Distribution of the similarity of the fragments measured 402 \nby the Tanimoto-similarity with ECFP4 fingerprints; (D) Venn diagram for the intersection of the 403 \nfragments existing in the three subsets of the LIGAND set.  404 \n 405 \n406 \n\nPre-training & Fine-tuning 407 \nAfter finishing the dataset construction four models were pre -trained on the ChEMBL 408 \nset and fine -tuned on the LIGAND set. Here, these models were benchmarked on a 409 \nserver with four GTX1080Ti GPUs. After the training process converged , each 410 \nfragment in the test set was presented as input for 10 times to generate molecules. The 411 \nperformance is shown in Table 1. The training of Transformer models was faster but 412 \nconsumed more computational resources  than LSTM -based methods . In addition, 413 \nTransformer methods outperformed LSTM-based methods using SMILES. Although 414 \nthe three SMILES -based models improved after being fine -tuned they were still 415 \noutperformed by the  Graph T ransformer because of the advantages of a graph 416 \nrepresentation. To further check the accuracy of generated molecules we also compared 417 \nthe chemical space between the generated molecules and the compounds in the training 418 \nset with three different representations 1) MW ~ logP; 2) PCA with 19D PhysChem 419 \ndescriptors; 3) tSNE with 2048D ECFP6 fingerprints (Fig. 4). The region occupied by 420 \nmolecules generated by  the Graph Transformer  overlapped completely with the 421 \ncompounds in both the ChEMBL and LIGAND sets.  422 \n 423 \nTable 1: The performance of four different generators  for pre -training and fine -tuning 424 \nprocesses. 425 \nMethods Pre-trained Model Fine-tuned Model Time Memory Validity Accuracy Validity Accuracy \nGraph \nTransformer 100% 99.3% 100% 99.2% 453.8 s 14.5 GB \nSequential \nTransformer 96.7% 72.0% 99.3% 87.3% 832.3 s 31.7 GB \nLSTM-BASE 93.9% 44.1% 98.7% 77.9% 834.6 s  5.5 GB \nLSTM+ATTN 89.7% 52.2% 96.4% 84.2% 1212.5 s 15.9 GB \n 426 \n 427 \nFig. 4: The chemical space of generated molecules by the Graph Transformer pre -trained on the 428 \nChEMBL set (A, C and E) and being fine -tuned on the LIGAND set (B, D and F) . Chemical space 429 \nwas represented by either logP ~ MW (A, B) and first two components in PCA on PhysChem 430 \ndescriptors (C, D) and t-SNE on ECFP6 fingerprints (E, F). 431 \n\n 432 \nThe graph representation for molecules has more advantages over the SMILES 433 \nrepresentation when dealing with fragment -based molecule design: 1) Invariance in 434 \nthe local scale: During the process of molecule generation , multiple fragments in a 435 \ngiven scaffold can be put into any position in the output matrix without changing the 436 \norder of atoms and bonds in that scaffold. 2) Extendibility in the global scale: When 437 \nfragments in the scaffold are growing or being linked, they can be flexibly appended in 438 \nthe end column of the graph matrix while the original data structure does not need 439 \nchanging. 3) Free of grammar : Unlike in SMILES sequence s there is no explicit 440 \ngrammar to constrain the generation of molecules, such as the parentheses for branches 441 \nand the number s for rings in SMILES; 4) Accessibility of chemical rule s: For each 442 \nadded atom or bond the algorithm can detect if the valence of atoms is valid or not and 443 \nmask invalid atoms or bonds in the vocabulary to guarantee the whole generated matrix 444 \ncan be successfully parsed into  a molecule. With these advantages the Graph 445 \nTransformer generates molecules faster while using less computational resources.  446 \n 447 \nHowever, after examining the QED scores and SA scores we found that although the 448 \ndistribution of QED scores w as similar between the methods  (Figure 5A ,C), the 449 \nsynthesizability of the molecules generated by  the Graph Transformer were not better 450 \nthan the SMILES-based generators. This was especially true when fine-tuning on the 451 \nLIGAND set. A possible reason is that molecules generated by the Graph Transformer 452 \ncontain uncommon rings when the model dealt with long -distance dependencies. In 453 \naddition, because of more complicated data structure and presence of more parameters 454 \nin the model , Graph Tr ansformer did not outperform for the synthesizability of 455 \ngenerated molecules when being trained on the small dataset (e.g. the LIGAND set). It 456 \nis also worth noticing that there still was a small fraction of generated molecules that 457 \ndid not contain the required scaffolds which is caused by a kekulization problem. For 458 \nexample, a scaffold â€˜CCCâ€™ can be grown into â€˜C1=C(C)C=CC=C1â€™ . A fter being 459 \nsanitized, it can be transformed into â€˜c1c(C)cccc1â€™. In this process one single bond in 460 \nthe scaffold is changed to an aromatic bond, which cause s a mismatch between the 461 \nscaffold and the molecule. Currently our algorithm cannot solve this problem because 462 \nif the aromatic bond is taken into consideration, the valence of aromatic atoms is 463 \ndifficult to be calculated accurate ly. This  would lead to the generation of  invalid 464 \nmolecules. Therefore, there is no aromatic bond provided in the vocabulary and all of 465 \nthe aromatic rings are inferred automatically through the molecule sanitization method 466 \nin RDKit.  467 \n 468 \nFig. 5: the distribution of the QED score (A, C) and SA score (B, D) of desired ligands in the 469 \nChEMBL set and LIGAND set and of molecules generated by four  different generators. For 470 \nthe QED score, four generators had the same performance as the molecules in both ChEMBL set (A) 471 \nand the LIGAND set (C). For the SA score, Graph Transformer did not outperform three other 472 \nSMILES-based generators in ChEMBL set (B) and even worse in the LIGAND set (D). 473 \n\n 474 \nPolicy gradient 475 \nBecause the Graph Transformer generates molecules accurately and fast it was chosen 476 \nas the agent in the RL framework. Two objectives were tested in the training process of 477 \nthis work. The first one was affinity towards A2AAR, which is predicted by the random 478 \nforest-based regression model from DrugEx v2; the second one was the QED score 479 \ncalculated with RDKit to measure how similar the generated molecule is to known 480 \napproved drugs . With the policy gradient  method as the reinforcement learning 481 \nframework two cases were tested. On the one hand, predicted affinity for A2AAR was 482 \nconsidered without the QED score . On the other hand , both objectives were used to 483 \noptimize the model with Pareto ranking. In the first case  86.1% of the generated 484 \nmolecules were predicted active, while the percentage of predicted active molecules in 485 \nthe second case was 74.6%. Although the generator generate d more active ligand s 486 \nwithout the QED score constraint most of them are not drug-like as they always have a 487 \nmolecular weight larger than 500Da. However, when we checked the chemical space 488 \nrepresented by tSNE with ECFP6 fingerprints the overlap region between generated 489 \nmolecules and ligands in the training set was not complete  implying that they fall out 490 \nof the applicability domain of the regression model.  491 \n 492 \nIn DrugEx v2, we provided a n exploration strategy which simulated the idea of 493 \nevolutionary algorithms such as crossover and mutation manipulations. However, when 494 \ncoupled to the Graph Transformer there were some difficulties and we had to give up 495 \nthis strategy. Firstly, the mutation strategy did not improve with different mutation rates. 496 \nA possible reason is that before being generated , part the molecule was fixed with a 497 \ngiven scaffold counteracting the effect of mutation caused by the mutation net. 498 \nSecondly, the crossover strategy is computationally very expensive in this context. This 499 \nstrategy needs the convergence of model training and iteratively updates the parameters 500 \nin the agent. With multiple iterations, it takes a  long period of time beyond t he 501 \ncomputational resources we can currently access. As a result, we updated the 502 \nexploration strategy as mentioned in the Methods section with six different exploration 503 \nrates: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5].  504 \n 505 \nTable 2: the performance of the Graph Transformer with different exploration rates in the RL 506 \nframework.  507 \nChanges to the exploration rate do not influence accuracy and have a low effect on diversity. 508 \nHowever, desirability (finding active ligands) and uniqueness can be influenced significantly. 509 \nEmpirically determining an optimal value for a given chemical space is recommended. 510 \n 511 \n 512 \nAfter training of the models , multiple scaffolds were input 10 times to generate 513 \nmolecules. The results for accuracy, desirability, uniqueness, and diversity with 514 \ndifferent exploration rates are shown in Table 2. With a low Îµ the model generates more 515 \ndesired molecules, but the uniqueness of the generated molecules can be improved. At 516 \nÎµ = 0.3 the model generated the highest percentage of unique desired molecules (56.8%). 517 \nDiversity was always larger than 0.84 and the model achieved the largest value (0.88) 518 \nwith Îµ = 0.0 or Îµ = 0.2. The che mical space represented by t SNE with ECFP6 519 \nfingerprints confirms that our exploration strategy produces a set of generated 520 \nmolecules completely covering the region occupied by the LIGAND set (Fig. 6).  521 \n 522 \nÎµ Accuracy Desirability Uniqueness Diversity \n0.0 99.7% 74.6% 60.7% 0.879 \n0.1 99.7% 66.8% 75.0% 0.842 \n0.2 99.8% 61.6% 80.2% 0.879 \n0.3 99.7% 56.8% 89.8% 0.874 \n0.4 99.7% 54.8% 88.8% 0.859 \n0.5 99.7% 46.8% 88.5% 0.875 \n 523 \nFig. 6: The chemical space of generated molecules by the Graph Transformer trained with 524 \ndifferent exploration rates in the RL framework . The chemical space was represented by t-SNE on 525 \nECFP6 fingerprints. 526 \n 527 \n\nGenerated Molecules 528 \nIn the chemical space making up antagonists of A 2AAR there are several well -known 529 \nscaffolds. Examples include furan, triazine, aminotriazole, and purine derivatives such 530 \nas xanthine and azapurine. The Graph Transformer model produced active ligands for 531 \nA2AAR (inferred from the predictors) with different combinations of these fragments as 532 \nscaffolds. Taking these molecules generated by the Graph Transformer as an example, 533 \nwe filtered out the molecules with potentially reactive groups (such as aldehydes) and 534 \nuncommon ring systems and listed 30 desired molecules as putative A 2AAR 535 \nligands/antagonists (Fig. 7).  For each scaffold five molecules were selected and 536 \nassigned in the same row. These molecules are considered a valid  starting point for 537 \nfurther considerations and work (e.g. molecular docking or simulation). 538 \n 539 \n 540 \nFig. 7: Sample of generated molecules with the Graph Transformer with different scaffolds. 541 \n\nThese scaffolds include: furan,  triazine, aminotriazole, xanthine , and azapurine . The generated 542 \nmolecules based on the same scaffolds are aligned in the same row.  543 \n 544 \nConclusions and Future Perspectives 545 \nIn this study, DrugEx was updated with the ability to design novel molecules based on 546 \nscaffolds consisting of multiple fragments as input. In this version (v3), a new positional 547 \nencoding scheme for atom s and bonds was proposed to make the Transformer model 548 \ndeal with a molecular graph representation. With one model, multiple fragments in a 549 \ngiven scaffold can be grown at the same time and connected to generate a new molecule. 550 \nIn addition , chemical rule s on valence are enforced at each step of the process of 551 \nmolecule generation to ensure that all generated molecules are valid. These advantages 552 \nare impossible to be embodied in SMILES-based generation, as SMILES -based 553 \nmolecules are constrained by grammar that allows a 2D topology to be represented in 554 \na sequential way . With multi -objective reinforcement learning the model generate s 555 \ndrug-like ligands, in our case for the A2AAR target.  556 \n 557 \nIn future work, the Graph Transformer will be extended to include other information as 558 \ninput to design drugs conditionally. For example, proteochemometric modelling (PCM) 559 \ncan take information for both ligands and targets as input to predict the affinity of their 560 \ninteractions, which allows generation of compounds that are promiscuous (useful for 561 \ne.g., viral mutants) or selective ( useful for e.g., kinase inhibitors) 34. The Transformer 562 \ncan then be used to construct inverse PCM models which take the protein information 563 \nas input (e.g. sequences, structures, or descriptors) to design active ligands for a given 564 \nprotein target without known ligands. Moreover, the Transformer can also be used for 565 \nlead optimization. For instance, the input can be a â€œhitâ€ already, generating â€œoptimizedâ€ 566 \nligands, or a â€œleadâ€ with side effects to produce ligands with a better ADME/tox profile.  567 \n 568 \nAuthorsâ€™ Contributions 569 \nXL and GJPvW conceived the study and performed the experimental work and analysis. 570 \nKY , APIJ nd HWTvV provided feedback and critical input. All authors read, 571 \ncommented on and approved the final manuscript. 572 \n 573 \nAcknowledgements 574 \nXL thanks Chinese Scholarship Council (CSC) for funding, GJPvW thanks the Dutch 575 \nResearch Council and Stichting Technologie Wetenschapp en (STW) for financial 576 \nsupport (STW-Veni #14410). Thanks go to Dr. Xue Yang for verifying Table S1 and Dr. 577 \nAnthe Janssen  checking the convergence of t -SNE. We also acknowledge Bert 578 \nBeerkens for providing the common scaffolds used to generate molecules as an example. 579 \n 580 \nCompeting Interests 581 \nThe authors declare that they have no competing interests 582 \nReferences 583 \n1. P . G. Polishchuk, T. I. Madzhidov and A. Varnek, J Comput Aided Mol Des, 2013, 27, 675-679. 584 \n2. P . J. Hajduk and J. Greer, Nat Rev Drug Discov, 2007, 6, 211-219. 585 \n3. G. L. Card, L. Blasdel, B. P . England, C. Zhang, Y . Suzuki, S. Gillette, D. Fong, P . N. Ibrahim, D. R. 586 \nArtis, G. Bollag, M. V. Milburn, S. H. Kim, J. Schlessinger and K. Y . Zhang, Nat Biotechnol, 2005, 587 \n23, 201-207. 588 \n4. Y . Bian and X. S. Xie, AAPS J, 2018, 20, 59. 589 \n5. J. P . Hughes, S. Rees, S. B. Kalindjian and K. L. Philpott, Br J Pharmacol, 2011, 162, 1239-1249. 590 \n6. C. Sheng and W. Zhang, Med Res Rev, 2013, 33, 554-598. 591 \n7. R. Santos, O. Ursu, A. Gaulton, A. P . Bento, R. S. Donadi, C. G. Bologa, A. Karlsson, B. Al-Lazikani, 592 \nA. Hersey, T. I. Oprea and J. P . Overington, Nat Rev Drug Discov, 2017, 16, 19-34. 593 \n8. B. B. Fredholm, Exp Cell Res, 2010, 316, 1284-1288. 594 \n9. J. F. Chen, H. K. Eltzschig and B. B. Fredholm, Nat Rev Drug Discov, 2013, 12, 265-286. 595 \n10. S. Moro, Z. G. Gao, K. A. Jacobson and G. Spalluto, Med Res Rev, 2006, 26, 131-159. 596 \n11. W. Jespers, A. Oliveira, R. Prieto -Diaz, M. Majellaro, J. Aqvist, E. Sotelo and H. Gutierrez -de-597 \nTeran, Molecules, 2017, 22. 598 \n12. X. Liu, A. P . IJzerman and G. J. P . van Westen, Methods Mol Biol, 2021, 2190, 139-165. 599 \n13. Y . LeCun, Y . Bengio and G. Hinton, Nature, 2015, 521, 436-444. 600 \n14. R. Gomez-Bombarelli, J. N. Wei, D. Duvenaud, J . M. Hernandez-Lobato, B. Sanchez-Lengeling, 601 \nD. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P . Adams and A. Aspuru-Guzik, ACS Cent Sci, 602 \n2018, 4, 268-276. 603 \n15. M. H. S. Segler, T. Kogej, C. Tyrchan and M. P . Waller, ACS Cent Sci, 2018, 4, 120-131. 604 \n16. S.-L. Benjamin, O. Carlos, G. Gabriel L. and A. -G. Alan, Optimizing distributions over molecular 605 \nspace. An Objective -Reinforced Generative Adversarial Network for Inverse -design Chemistry 606 \n(ORGANIC), 2017. 607 \n17. M. Olivecrona, T. Blaschke, O. Engkvist and H. Chen, Journal of cheminformatics, 2017, 9, 48. 608 \n18. T. Blaschke, J. Arus-Pous, H. Chen, C. Margreitter, C. Tyrchan, O. Engkvist, K. Papadopoulos and 609 \nA. Patronov, Journal of chemical information and modeling, 2020, 60, 5918-5922. 610 \n19. J. Lim, S. Y . Hwang, S. Moon, S. Kim and W. Y . Kim, Chem Sci, 2019, 11, 1153-1164. 611 \n20. Y . Li, J. Hu, Y . Wang, J. Zhou, L. Zhang and Z. Liu, Journal of chemical information and modeling, 612 \n2020, 60, 77-91. 613 \n21. J. Arus-Pous, A. Patronov, E. J. Bjerrum, C. Tyrchan, J. L. Reymond, H . Chen and O. Engkvist, 614 \nJournal of cheminformatics, 2020, 12, 38. 615 \n22. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. J. a. e.-p. 616 \nPolosukhin, Journal, 2017, arXiv:1706.03762. 617 \n23. Y . Yang, S. Zheng, S. Su, C. Zhao, J. Xu and H. Chen, Chem Sci, 2020, 11, 8312-8322. 618 \n24. X. Liu, K. Ye, H. W. T. van Vlijmen, A. P . IJzerman and G. J. P . van Westen, Journal of 619 \ncheminformatics, 2019, 11, 35. 620 \n25. X. Liu, K. Ye, H. W. T. van Vlijmen, M. T. M. Emmerich, I. A. P . and G. J. P . van Westen, Journal of 621 \ncheminformatics, 2021, 13, 85. 622 \n26. A. Gaulton, L. J. Bellis, A. P . Bento, J. Chambers, M. Davies, A. Hersey, Y . Light, S. McGlinchey, D. 623 \nMichalovich, B. Al-Lazikani and J. P . Overington, Nucleic Acids Res, 2012, 40, D1100-1107. 624 \n27. J. Degen, C. Wegscheid-Gerlach, A. Zaliani and M. Rarey, ChemMedChem, 2008, 3, 1503-1507. 625 \n28. PyTorch, https://pytorch.org/). 626 \n29. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. 627 \nMinderer, G. Heigold, S. Gelly, J. Uszkoreit and N. J. a. e. -p. Houlsby, Journal, 2020, 628 \narXiv:2010.11929. 629 \n30. G. R. Bickerton, G. V. Paolini, J. Besnard, S. Muresan and A. L. Hopkins, Nat Chem, 2012, 4, 90-630 \n98. 631 \n31. Scikit-Learn: machine learning in Python, http://www.scikit-learn.org/). 632 \n32. P . Ertl and A. Schuffenhauer, Journal of cheminformatics, 2009, 1, 8. 633 \n33. A. R. Solow and S. Polasky, Environmental and Ecological Statistics, 1994, 1, 95-103. 634 \n34. G. J. van Westen, J. K. Wegner, P . Geluykens, L. Kwanten, I. Vereycken, A. Peeters, A. P . Ijzerman, 635 \nH. W. van Vlijmen and A. Bender, PLoS One, 2011, 6, e27518. 636 \n637 \nTable S1: Atoms in vocabulary for graph-based molecule generation. The column of â€œSymbolâ€ 638 \nis the symbol of the atom and its charge; the column of â€œValenceâ€ is the value of valence of the state 639 \nof each chemical element; the â€œNumberâ€ column stands for the index of each element in the periodic 640 \ntable, the last row is the unique word for each state of these elements, a combination of its valence 641 \nand symbol. 642 \nSymbol Valence Charge Number Word \nO 2 0 8 2O \nO+ 3 1 8 3O+ \nO- 1 -1 8 1O- \nC 4 0 6 4C \nC+ 3 1 6 3C+ \nC- 3 -1 6 3C- \nN 3 0 7 3N \nN+ 4 1 7 4N+ \nN- 2 -1 7 2N- \nCl 1 0 17 1Cl \nS 2 0 16 2S \nS 6 0 16 6S \nS 4 0 16 4S \nS+ 3 1 16 3S+ \nS+ 5 1 16 5S+ \nS- 1 -1 16 1S- \nF 1 0 9 1F \nI 1 0 53 1I \nI 5 0 53 5I \nI+ 2 1 53 2I+ \nBr 1 0 35 1Br \nP 5 0 15 5P \nP 3 0 15 3P \nP+ 4 1 15 4P+ \nSe 2 0 34 2Se \nSe 6 0 34 6Se \nSe 4 0 34 4Se \nSe+ 3 1 34 3Se+ \nSi 4 0 14 4Si \nB 3 0 5 3B \nB- 4 -1 5 4B- \nAs 5 0 33 5As \nAs 3 0 33 3As \nAs+ 4 1 33 4As+ \nTe 2 0 52 2Te \nTe 4 0 52 4Te \nTe+ 3 1 52 3Te+ \n* 0 0 0 * \nTable S2: The pseudo code for encoding the graph representation of molecules in DrugEx v3 643 \nAlgorithm encoding: \n Input:  \nmol: structure of the kekulized molecule \nsubs: structure of the scaffolds \nvocab: vocabulary of tokens which is consisted of graph matrix \nOutput:  \n  matrix: the n x 5 matrix to represents the molecular graph. \n \n# Ensure the atom of the subs are put at the start in the molecule \nmol â† RANK_ATOM_BY_SUB(mol, subs)  \nsub_atoms â† GET_ATOMS (subs) \nsub_bonds â† GET_BONDS (subs) \nmol_atoms â† GET_ATOMS (mol) \nfrag, grow, link â† [('GO', 0, 0, 0, 1)], [], [(0, 0, 0, 0, 0)] \nFor atom in mol_atoms: \n    # The bonds which connect to the atom having the index before this atom \nbonds â† GET_LEFT_BONDS (mol, atom) \nFor bond in bonds: \n    tk_bond â† GET_TOKEN (vocab, bond) \n    other â† GET_OTHER_ATOM(mol, atom, bond) \n    If IS_FIRST (bonds, bond): \n        tk_atom â† GET_TOKEN (vocab, atom) \n    Else: \n        tk_atom â† GET_TOKEN (vocab, None) \n \n    # The index of the scaffold in which the current atom locates \n# Its value starts from 1. If it is not in the scaffold, it will be 0 \n    scf â† GET_FRAG_ID (subs, atom) \n    column â† (tk_atom, tk_bond, GET_INDEX (other), GET_INDEX (atom), scf) \nIf other in sub_atoms and atom in sub_atoms and bond not in sub_bonds: \n        Insert column to link \n    Else if bond in sub_bonds: \n        Insert column to frag \n    Else: \n        Insert column to grow \n End \nEnd \nInsert ('EOS', 0, 0, 0, 0) to grow \nmatrix â† CONCATENATE_BY_COLUMN (frag, grow, link) \nReturn matrix \n 644 \n  645 \nTable S3: The pseudo code for decoding the graph representation of molecules in DrugEx v3 646 \nAlgorithm decoding: \n Input:  \n        matrix: the n x 5 matrix to represents the molecular graph \nvocab: vocabulary of tokens which is consisted of graph matrix \nOutput:  \nmol: structure of the kekulized molecule \nsubs: structure of the scaffolds \n \nmol â† new MOL () \nsubs â† new SUB () \nFor atom, bond, prev, curr, scf in matrix: \n    If atom == 'EOS' or atom == 'GO':  \ncontinue \n    If atom != '*': \n        a â† new Atom (GET_ATOM_SYMBOL(vocab, atom)) \n        SET_FORMAL_CHARGE (a, GET_CHARGE(vocab, atom)) \n        ADD_ATOM (mol, a) \n        If scf != 0: ADD_ATOM (subs, a) \n    If bond != 0: \n        b â† new Bond (bond) \n        ADD_BOND(mol, b) \n    If frag != 0:  \nADD_BOND (subs, b) \nEnd \n \n# automatically determine the aromatic rings \nmol â† SANITIZE (mol) \nsubs â† SANITIZE (subs) \nReturn mol, subs \n \n 647 \n 648 "
}