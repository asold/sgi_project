{
  "title": "DeepMemory: Model-based Memorization Analysis of Deep Neural Language Models",
  "url": "https://openalex.org/W4205678875",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5103227499",
      "name": "Derui Zhu",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5101882679",
      "name": "Jinfu Chen",
      "affiliations": [
        "Huawei Technologies (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A5056378414",
      "name": "Weiyi Shang",
      "affiliations": [
        "Concordia University"
      ]
    },
    {
      "id": "https://openalex.org/A5102141512",
      "name": "Xuebing Zhou",
      "affiliations": [
        "Huawei German Research Center"
      ]
    },
    {
      "id": "https://openalex.org/A5063714878",
      "name": "Jens Großklags",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5091586373",
      "name": "Ahmed E. Hassan",
      "affiliations": [
        null,
        "Queen Medical"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6798060926",
    "https://openalex.org/W6637131181",
    "https://openalex.org/W2049614554",
    "https://openalex.org/W6745567913",
    "https://openalex.org/W6745952019",
    "https://openalex.org/W1574447377",
    "https://openalex.org/W6749621233",
    "https://openalex.org/W1722221517",
    "https://openalex.org/W2051267297",
    "https://openalex.org/W2461943168",
    "https://openalex.org/W2512472178",
    "https://openalex.org/W6772101090",
    "https://openalex.org/W6742632731",
    "https://openalex.org/W6753937820",
    "https://openalex.org/W6787335730",
    "https://openalex.org/W4205228770",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W6763444354",
    "https://openalex.org/W6674571003",
    "https://openalex.org/W1501633760",
    "https://openalex.org/W2136583886",
    "https://openalex.org/W6712652615",
    "https://openalex.org/W6791369290",
    "https://openalex.org/W2979650406",
    "https://openalex.org/W6628547770",
    "https://openalex.org/W3112787034",
    "https://openalex.org/W2795435272",
    "https://openalex.org/W6763077247",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W1602011302",
    "https://openalex.org/W2530252838",
    "https://openalex.org/W2963311060",
    "https://openalex.org/W6753280856",
    "https://openalex.org/W6775878150",
    "https://openalex.org/W2163022590",
    "https://openalex.org/W3091388282",
    "https://openalex.org/W2968940383",
    "https://openalex.org/W3177420896",
    "https://openalex.org/W6749518548",
    "https://openalex.org/W6635292102",
    "https://openalex.org/W2793714280",
    "https://openalex.org/W2981207549",
    "https://openalex.org/W6763393573",
    "https://openalex.org/W2804337238",
    "https://openalex.org/W2053987251",
    "https://openalex.org/W6740005241",
    "https://openalex.org/W6735632633",
    "https://openalex.org/W2195388612",
    "https://openalex.org/W6748744355",
    "https://openalex.org/W6790754042",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W6639204139",
    "https://openalex.org/W6733905848",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W3180181113",
    "https://openalex.org/W4287333021",
    "https://openalex.org/W4293507378",
    "https://openalex.org/W2964318098",
    "https://openalex.org/W2884280357",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W1673310716",
    "https://openalex.org/W3015304056",
    "https://openalex.org/W2945237470",
    "https://openalex.org/W4297666078",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2395693197",
    "https://openalex.org/W2963096987",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W3099431377",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W4298325296",
    "https://openalex.org/W2590082389",
    "https://openalex.org/W4297788867",
    "https://openalex.org/W1473189865",
    "https://openalex.org/W3166854338",
    "https://openalex.org/W2770406504",
    "https://openalex.org/W2996649838",
    "https://openalex.org/W3049152512",
    "https://openalex.org/W2948559998",
    "https://openalex.org/W2597603852",
    "https://openalex.org/W1992624799"
  ],
  "abstract": "The neural network model is having a significant impact on many real-world applications. Unfortunately, the increasing popularity and complexity of these models also amplifies their security and privacy challenges, with privacy leakage from training data being one of the most prominent issues. In this context, prior studies proposed to analyze the abstraction behavior of neural network models, e.g., RNN, to understand their robustness. However, the existing research rarely addresses privacy breaches caused by memorization in neural language models. To fill this gap, we propose a novel approach, DeepMemory, that analyzes memorization behavior for a neural language model. We first construct a memorization-analysis-oriented model, taking both training data and a neural language model as input. We then build a semantic first-order Markov model to bind the constructed memorization-analysis-oriented model to the training data to analyze memorization distribution. Finally, we apply our approach to address data leakage issues associated with memorization and to assist in dememorization. We evaluate our approach on one of the most popular neural language models, the LSTM-based language model, with three public datasets, namely, WikiText-103, WMT2017, and IWSLT2016. We find that sentences in the studied datasets with low perplexity are more likely to be memorized. Our approach achieves an average AUC of 0.73 in automatically identifying data leakage issues during assessment. We also show that with the assistance of DeepMemory, data breaches due to memorization of neural language models can be successfully mitigated by mutating training data without reducing the performance of neural language models.",
  "full_text": "DeepMemory: Model-based Memorization Analysis\nof Deep Neural Language Models\nDerui Zhu\nTechnical University of Munich\nMunich, Germany\nderui.zhu@tum.de\nJinfu Chen∗\nHuawei Technologies Canada\nKingston, Canada\njinfu.chen1@huawei.com\nWeiyi Shang\nConcordia University\nMontreal, Canada\nshang@encs.concordia.ca\nXuebing Zhou\nHuawei Munich Research Center\nMunich, Germany\nxuebing.zhou@huawei.com\nJens Grossklags\nTechnical University of Munich\nMunich, Germany\njens.grossklags@in.tum.de\nAhmed E. Hassan\nQueen’s University\nKingston, Canada\nahmed@cs.queensu.ca\nAbstract—The neural network model is having a signiﬁcant\nimpact on many real-world applications. Unfortunately, the in-\ncreasing popularity and complexity of these models also ampliﬁes\ntheir security and privacy challenges, with privacy leakage from\ntraining data being one of the most prominent issues. In this con-\ntext, prior studies proposed to analyze the abstraction behavior of\nneural network models, e.g., RNN, to understand their robustness.\nHowever, the existing research rarely addresses privacy breaches\ncaused by memorization in neural language models. To ﬁll this\ngap, we propose a novel approach, DeepMemory, that analyzes\nmemorization behavior for a neural language model. We ﬁrst\nconstruct a memorization-analysis-oriented model, taking both\ntraining data and a neural language model as input. We then\nbuild a semantic ﬁrst-order Markov model to bind the con-\nstructed memorization-analysis-oriented model to the training\ndata to analyze memorization distribution. Finally, we apply\nour approach to address data leakage issues associated with\nmemorization and to assist in dememorization. We evaluate our\napproach on one of the most popular neural language models,\nthe LSTM-based language model, with three public datasets,\nnamely, WikiText-103, WMT2017, and IWSLT2016. We ﬁnd that\nsentences in the studied datasets with low perplexity are more\nlikely to be memorized. Our approach achieves an average AUC\nof 0.73 in automatically identifying data leakage issues during\nassessment. We also show that with the assistance of DeepMemory,\ndata breaches due to memorization of neural language models\ncan be successfully mitigated by mutating training data without\nreducing the performance of neural language models.\nIndex Terms—Deep learning, neural language model, model-\nbased analysis, privacy, memorization\nI. I NTRODUCTION\nArtiﬁcial intelligence (AI) software is important for au-\ntomating and making autonomous decisions. In particular, the\nrise of neural network models had a huge and signiﬁcant\nimpact on many real-world applications, e.g., natural language\nprocessing [ 1], [ 2], image recognition [ 3], and autonomous\ndriving [4], [ 5]. However, the increasing diversity and com-\nplexity of such neural network models make their security,\nreliability and robustness a critical and difﬁcult issue to address.\nTherefore, researchers in different ﬁelds are now working\n*Jinfu Chen (jinfu.chen1@huawei.com) is the corresponding author.\nintensely on guidelines for Trustworthy AI and Safe AI. For\nexample, software engineering researchers propose techniques\nthat analyze and explain AI models in order to ensure the\nsecurity and safeness of AI-based software [6], [7].\nSimilar to traditional (i.e., not based on AI) software, AI-\nbased solutions have been reported by many prior studies to\ntrigger security concerns, such as data privacy leakage [ 8].\nAlthough various veriﬁcation techniques, e.g., static analysis,\nsymbolic execution analysis and fuzzing techniques, can be\nused to guide the assurance of traditional software security,\nthose techniques are not applicable for AI-based software. In\ncontrast, to the best of our knowledge, there is a relative lack\nof techniques that can assist in the veriﬁcation of security in\nAI-based software.\nData privacy leakage is a typical security issue in AI\nmodels. Previous work [ 9], [10], [11] has shown that neural\nlanguage models tend to memorize the training data instead\nof learning its latent characteristics. This can be exploited to\nextract privacy-critical information from the data, potentially\nleading to signiﬁcant ﬁnancial and reputational harm [12]. More\ngenerally, memorization with a neural language model may\nreveal insights regarding its internal behavior. Prior studies [13],\n[14], [15] have been proposed to analyze certain aspects of the\ninternal behavior of deep neural networks in order to assist with\ndetecting adversarial examples and to guide the security testing\nof deep learning models [ 14]. However, the existing research\nrarely targets a model’s internal memorization behavior. Hence,\nthe existing research is limited when it comes to analyzing\nand preventing leakage of sensitive private information from\ntraining data of a publicly released model.\nTo ﬁll this research gap, we propose a novel approach,\nDeepMemory, to assist in verifying security in AI-based\nsoftware by analyzing the internal memorization behavior of\nneural language models. We ﬁrst construct a memorization-\nanalysis-oriented model taking both training data and a neural\nlanguage model as input. Second, we bind the constructed\nmemorization-analysis-oriented model to the training data. We\nthen build a semantic ﬁrst-order Markov model to analyze\n1003\n2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)\nWork licensed under Creative Commons Attribution NonCommercial, No Derivatives 4.0 License. https://creativecommons.org/licenses/by-nc-nd/4.0/\nDOI 10.1109/ASE51524.2021.00092\n2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678871\nmemorization distribution. Finally, we apply our approach to\ntwo downstream application scenarios, including data leakage\nrisk assessment and dememorization assistance.\nWe evaluate our approach on one of the most popu-\nlar neural language models, i.e., the LSTM-based language\nmodel, with three public datasets, namely, WikiText-103 [ 16],\nWMT2017 [ 17] and IWSLT2016 [ 18]. We investigate the\nLSTM-based language model with the same architecture and\nconﬁguration as Merity et al. [19]. We ﬁnd that by observing\nmemorization characteristics for training data, sentences with\nlow perplexity are more likely to be memorized by a neural\nlanguage model. Our approach achieves an average AUC of\n0.73 in automatically identifying data leakage issues during\nassessment. Finally, by following our approach, the memoriza-\ntion risk from a neural language model can be mitigated by\nmutating training data without impacting the quality of neural\nlanguage models.\nTo the best of our knowledge, our approach is the ﬁrst\nattempt to assist in the veriﬁcation of a common and important\nprivacy related issue in AI models, i.e., memorization in neural\nlanguage models. In particular, our work makes the following\ncontributions:\n• We model the internal memorization behavior of neural\nlanguage models, e.g., the LSTM-based language model,\nin order to address training data leakage issues caused by\nthe model’s memorization behavior.\n• Our approach can automatically assess memorization-\nrelated privacy leakage in neural language models.\n• With our work, we can assist in the dememorization\nprocess in order to address memorization issues in neural\nlanguage models.\n• Our approach can be used to assist in the security testing\nand assurance processes for AI models.\nThe rest of this paper is organized as follows: Section II\nprovides the background for our work. Section III gives an\noverview of our approach, and Sections IV–VI present the\ndetails. Section VII presents the results of our evaluation.\nSection IX discusses threats to the validity of our work, and\nSection X summarizes related work. Finally, Section XI offers\nconcluding remarks.\nII. B ACKGROUND\nA. Language modeling\nA language model is a probability distribution over sequences\nof words [ 20]. In other words, a language model aims to learn\na probabilistic model that is capable to predict the next word\nin a sequence based on the given preceding words. Formally,\nthe probability distribution of a language model can be deﬁned\nas Pr(w1,w2,...,w n):\nPr(w1,w2,...,w n) =\nn∏\ni=1\nPr(wi|w1,...,w i−1) (1)\nwhere wi refers to a word. This language model has been\nsuccessfully used in many applications, such as speech recogni-\ntion [21], machine translation [22], sentiment analysis [23], and\ninformation retrieval [ 24]. In particular, the neural language\nmodel is becoming increasingly popular and has been success-\nfully used in many applications. The neural language model\nuses different kinds of neural networks to model sequence\nprobability, and it transforms words into vectors and uses the\nvectors as input for a neural network to predict the next words.\nA very common neural language model is the long short-term\nmemory (LSTM ) language model. An LSTM network contains\na plethora of units, called memory blocks. Each memory block\nrepresents a hidden state, i.e., st, st+1, st+2. Prior work [ 25]\nillustrated the success of LSTM-based language models in\nmultiple applications motivating us to conduct our work in this\ncontext.\nB. Memorization risk from training Data\nA language model requires domain-speciﬁc data for training\nin order to achieve a good model performance. However, a well-\nperforming language model might suffer from data leakage due\nto memorization of training data. Such leakage is particularly\ntroublesome when sensitive data including personal/private data,\ntransaction data, or governmental data becomes available to an\nattacker. Examples of such sensitive data are Social Insurance\nNumbers (in Canada) or health-related data.\nSensitive data that is part of training data may be memorized\nby a neural language model during model training. When\nleveraging such a mechanism, one can develop attacks to\nextract such data from public trained language models [ 26]. In\nother words, data leakage due to the memorization mechanism\nis a typical security weakness in AI-based language models.\nThe study of memorization privacy risk includes two lines of\nresearch: memorization-related privacy attacks and defenses.\n1) Memorization-related privacy attacks: If a model is\nignorant towards privacy-preserving algorithms, it tends to\nblindly remember some sequences from the training data [ 8],\n[27]. Previous studies [ 8], [ 28] ﬁnd that memorization is\na common phenomenon in language models, and privacy\nattacks aim to reconstruct verbatim memorization sequences\nfor training data.\n2) Memorization-related privacy defenses: There are typ-\nically two ways to support dememorization, i.e., differential\nprivacy and regularization. Differential privacy, which injects\nnoise into the process of model training, is a well-known\nsolution for minimizing memorization in model training [ 29].\nAs such, it is challenging to identify whether speciﬁc data\nis part of the training data. Another typical privacy defense\napproach is regularization. One can add regularization to the\nloss function of language model optimization [26].\nIII. O VERVIEW OF OUR APPROACH\nIn this section, we present an overview of our approach\nfor analyzing the memorization behavior for a given lan-\nguage model. Similar to traditional software vulnerability\ndetectors, our approach acts as an automated technique for\ndetecting potential data leakage security vulnerabilities that\noccur due to memorization in language models. An overview\nof our approach is shown in Figure 1. It consists of three\n1004\nSemantic\nprofiling\nMemorization-analysis-oriented model construction (Section IV)\nAutomated\ndimension\nreduction\nSemantic\nclustering\nSemantic distribution abstraction\nConcrete state & trace distribution\nModel\nconstruction\nMemorization distribution\n binding (Section V)\nMemorization\nextraction\n Data leakage risk\nassessment\nAssisting in\ndememorization\nAddressing memorization\nissues using the memoriz-\nation model (Section VI)\nSemantic Memorization Modeling\nMemorization state/trace\ndistribution construction\nMemorization sequence\ndistribution construction\nStateful deep learning model\nTraining corpus\nFig. 1: An overview of our approach.\nphases: 1) memorization-analysis-oriented model construction,\n2) memorization-distribution binding, and 3) addressing mem-\norization issues using the memorization model.\nIn the ﬁrst phase, we construct a memorization-analysis-\noriented model. Taking both training data and a neural language\nmodel as input, we ﬁrst proﬁle the given model to extract\nsemantic information, i.e., hidden states and traces. Such\nproﬁling outputs initial states and traces that represent the\nmodel behavior. Typically, a large number of initial states and\ntraces exist due to the massive scale of training data. We then\nabstract a semantic distribution from the initial states and traces.\nIn particular, we transform initial states to intermediate states\nby reducing the high dimensions of each initial state. We then\napply a clustering algorithm to group the intermediate states\nand traces into clusters, i.e., to derive concrete states and traces.\nFinally, we construct a memorization-analysis-oriented model\nbased on the concrete states and traces distribution.\nIn the second phase, our approach binds the memorization-\nanalysis-oriented model to the training data to analyze the\nmemorization distribution. This phase takes the memorization-\nanalysis-oriented model constructed from the last phase and\nthe training corpus as input. To analyze the memorization\ndistribution, we ﬁrst extract memorization sequences from the\ntraining data. We then build a semantic ﬁrst-order Markov\nmodel to model the memorization distribution.\nIn the ﬁnal phase, we apply our approach to two downstream\ntasks, including data leakage risk assessment and dememo-\nrization assistance. The ﬁrst downstream task automatically\nidentiﬁes potential data leakage issues in the model (comparable\nto bug detection). The second downstream task assists in the\nrepair of the models given the identiﬁed issues (comparable to\nprogram repair). We detail each phase of our approach in the\nsubsequent Sections IV – VI.\nIV. M EMORIZATION -ANALYSIS -ORIENTED MODEL\nCONSTRUCTION\nIn this section, we construct a memorization-analysis-\noriented model. Algorithm 1 presents the details of its construc-\ntion. Given an LSTM-based language model and its training\ndata, we ﬁrst proﬁle the model to extract the initial states and\ntraces by iterating words in each sentence over the training\ndata. We then abstract the initial states and traces to construct\nour memorization-analysis-oriented model. We describe each\nstep in detail below.\nAlgorithm 1: Memorization-analysis-oriented model\nconstruction algorithm\ninput : R= (D,δ,f ): LSTM-based language model\nG: semantic distribution abstraction function\nθ: information loss threshold\nD: sentences\nσ: minimum number of neighbors threshold\nρ: distance threshold\noutput :M: memorization-analysis-oriented model\n1 S ←[] ; // initial states set\n2 T ←[] ; // initial traces set\n3 for W ∈D do // loop sentences to extract states\n4 s ∈[δi(W[: i])]|W|\ni=0 ; // extract all hidden states of a sentence\n5 for i: 1∈|s|do\n6 S.add(si);\n7 T.add((si−1,si))\n8 g←G(S,θ,σ,ρ) ; // semantic distribution abstraction\n9 S\n′\n= []; // concrete states set\n10 for s∈S do\n11 s′←g(s) ;\n12 S\n′\n.add(s′) ;\n13 T\n′\n←[] ; // concrete traces set\n14 for (si−1,si) ∈T do\n15 s′i−1 ←g(si−1) ;\n16 s′i ←g(si) ;\n17 T\n′\n.add(s′i−1,s′i) ;\n18 return M(D,S\n′\n,T\n′\n,f);\nA. Semantic proﬁling\nRecent research on deep neural network models [ 14],\n[15], [ 30] highlights that states and traces are efﬁcient for\nunderstanding stateful model behaviors over data distribution.\nA neural language model can be seen as a stateful model. The\nLSTM-based model is one of the most typical neural language\nmodels. Therefore, in order to analyze LSTM-based neural\nlanguage model behavior, we proﬁle the model to extract the\ninitial semantic states and traces as the ﬁrst step. We ﬁrst\nexplain the deﬁnition of state and trace in neural language\nmodel analysis.\nSuppose that we have an LSTM-based language model\nR = (D,δ,f ). D refers to all sentences used for training.\nf is the distribution of a language model, and δ is an internal\nstate extractor of the model that is used to transform each\nword in a sentence to a state. For example, when we feed\na sequence “Ian goes home at 6 pm on weekdays and goes\nswimming at 7 pm every day.” to aLSTM-based language model\nf with 100 hidden units, we can obtain a list of hidden-state\nvectors of LSTM with 100-dimension for each feed input word,\ni.e., [[0.1,..., 0.3],[0.2,..., 1.3],[1.5,..., 0.3],..., [0.07,..., 0.4]],\nby using internal state extractor δ. Particularly, δ(home) =\n1005\n[1.5,..., 0.3].\nWith the internal hidden state set, we construct a corre-\nsponding state ﬂow, i.e., trace, over two hidden states ordered\nchronologically. The trace represents a transition relation for a\npair of consecutive hidden states. In our illustrative example,\nthe trace between hidden state “goes” and state “home” is\nrepresented by (δ(goes),δ(home)).\nIn Algorithm 1, we ﬁrst deﬁne two empty sets (Line 1 and 2)\nfor hidden states S and traces T. We then iterate each sentence\nW in the training data and extract the state and trace of each\nword w (Line 3 to 7). Particularly, at the i-th timestamp t,\neach word in a sentence is transformed to a state si using the\ninternal state extractor δ. A trace is accordingly extracted to\n(si−1,si). Finally, we construct a state set S and trace set T\nfor the whole training data D and deﬁne it as an initial model.\nB. Semantic distribution abstraction\nAfter semantic proﬁling, we obtain an initial model to\nrepresent the LSTM-based neural language model behaviors\nover training data. However, such a granular representation\ncontains a plethora of discrete states and traces. For example,\nan LSTM-based neural language model potentially produces up\nto 100 thousand states and 900 thousand traces for a corpus\ncontaining 10,000 sentences with an average length 10 of words.\nIt is impractical to understand the internal behavior of a given\nmodel with such a huge number of states and traces. Therefore,\nin this step, we abstract the semantic distribution of a given\nlanguage model from the perspective of states and traces.\n1) Automated dimension reduction: The dimension of\neach initial state generated by semantic proﬁling is equal to\nthe number of hidden units in LSTM core, which usually\nis very high. It is hard to ﬁnd the latent characteristics\nover high dimensional space since the distribution of data\nwith high dimension tends to be sparse [ 31]. Therefore, we\nﬁrst automatically reduce the dimension of each initial state\ngenerated by semantic proﬁling to an optimal number. Du et\nal. [14] applied Principal Component Analysis (PCA) to reduce\nthe dimension of semantic space to a small number, in order to\nefﬁciently ﬁnd the common correlation over states. However, an\nobvious limitation in their approach exists. When the dimension\nof an initial state is high, arbitrary dimension reduction\nmay lead to a huge information loss. The information loss\nfrom modeling may potentially introduce a signiﬁcant bias in\nmemorization-analysis-oriented model construction. To improve\nthe memorization-analysis-oriented model construction, we use\na classic metric, Relative Information Loss [ 32], to measure\nthe information loss during dimension reduction. In detail, we\nhave a number of n vectors V and each vector is with m-\ndimension space, i.e., [v0,v1,...,v m]. We want to transform\nthe n initial vectors to vectors ˆV, and each transformed vector\nis with k-dimension, i.e., [ˆv0,ˆv1,..., ˆvk]. The corresponding\ninformation loss is deﬁned as ψ(k).\nIn order to overcome the aforementioned limitation, we take\ninformation loss into account for dimension reduction in order\nto secure the utility of the transformed internal state. We set\na threshold θ to control information loss, and the decision\nprocess of ﬁnding the optimal k can then be deﬁned as:\narg min\nk\n|ψ(k) −θ| (2)\nFinally, this step outputs intermediate states and each state is\nkdimensions. In our example, we reduce the 100-dimension of\neach state to three dimensions. For example, the word “home”\nwould be with a reduced initial state [1.5,0.7,0.3].\n2) Semantic Clustering: To identify the latent characteris-\ntics over the intermediate states, we apply a clustering algorithm\n(DBSCAN) to group together intermediate states that are close\nto each other in terms of cosine distance threshold ρ and\nminimum number of neighbors σ. DBSCAN-based clustering\nis suitable for data with an arbitrary shape [ 33]. ρ speciﬁes\nfor the minimum cosine distance which two intermediate state\npoints should be considered as neighbors. σ determines the\nminimum number of neighbors to be deﬁned as a core state.\nEach core state and its neighbors form a cluster labeled as a\nconcrete state. In our running example, the words “home” and\n“swimming” are grouped into one cluster. Therefore, we would\nlabel the hidden states of the words “home” and “swimming”\nas a single identical concrete state.\nC. Memorization-analysis-oriented model construction\nWith the concrete states from the clustering, we construct a\nﬁnal memorization-analysis-oriented model. We ﬁrst transform\nthe high-dimensional initial states into intermediate states with\nan optimal dimension. We then transform the intermediate\nstates to concrete states. Note from Algorithm 1, we deﬁne an\nabstraction function G to abstract the initial states and traces\n(Line 8). The inputs of the function G are the initial states,\nand three threshold values, i.e., information loss threshold θ,\nthe number of cores σ, and distance threshold ρ. We then\ninitialize two sets S\n′\n(Line 9) and T\n′\n(Line 13) for concrete\nstates and traces, respectively. Next, for each initial state si,\nwe use the deﬁned semantic distribution abstraction to abstract\nthe state to s′i (Line 10 to 12). Similarly, for each initial\ntrace (si−1,si) composed of two states si−1 and si, we apply\nthe same abstraction function to abstract the two states to\ns′i−1 and s′i. We then connect the two abstracted states into\na concrete trace (s′i−1,s′i) (Lines 14 – 17). The ﬁnal output\nis the memorization-analysis-oriented model (Line 18). In\nour running example, the ﬁnal memorization-analysis-oriented\nmodel is represented by the concrete state and trace set.\nV. M EMORIZATION DISTRIBUTION BINDING\nPrior studies [ 8], [ 27] have reported that memorization\nis a severe issue in language models. To achieve a good\nperformance, a model all too often intends to remember the\ntraining data during the training process instead of learning\nthe latent characteristics. Regularization techniques, such as\ndropout and batch normalization, aim to solve the model\noverﬁtting issue and improve the generality of AI models.\nAlthough the regularization techniques are widely adopted for\ntraining a complicated model, e.g., an LSTM-based model,\nthe models may still memorize part of the training data [ 27].\n1006\nSuch memorization might be exploited to extract private data\nfrom a given language model. Therefore, in this section, we\nquantify the memorization behavior in a memorization-analysis-\noriented model, i.e., the output from Section IV. The details\nfor analyzing memorization behavior are shown in Algorithm 2.\nGiven a memorization-analysis-oriented model and training\ndata as input, we bind the memorization distribution by building\na semantic Markov model to map the memorization-analysis-\noriented model to training data. In particular, we ﬁrst extract\nmemorization. We then build a ﬁrst-order Markov model to\nrepresent the semantic memorization distribution.\nA. Memorization Extraction\nFrom our memorization-analysis-oriented model generated\nin Section IV, we obtain the ﬁnal concrete states and traces for\neach sentence in the training data. However, such states and\ntraces cannot be applied to quantify memorization behavior\nof a given language model directly. Therefore, to quantify the\nmemorization behavior efﬁciently, we ﬁrst deﬁne a memoriza-\ntion concept called a memorization sequence. Given a language\nmodel R = (D,δ,f ) and a preﬁx c, a string of l with length\nN is considered to be a memorization sequence if such a string\nis equal to:\narg max\nl′:|l′|=N\nR(l\n′\n|c) (3)\nwhere cand lare both from the training corpus. In our example,\ngiven a preﬁx c “Ian goes”, a language model would predict a\nstring “home at 6 pm on weekdays” as the most likely output.\nWe call a string such as “home at 6 pm on weekdays” a\nmemorization sequence based on the preﬁx “Ian goes”.\nWith memorization sequences, we classify the concrete\nstate|trace from the memorization-analysis-oriented model into\ntwo types, i.e., memorization state |trace and non-memorization\nstate|trace. If a state |trace is visited by any memorization\nsequence, we consider the state |trace to be a memorization\nstate|trace. Otherwise, it is a non-memorization state |trace.\nFinally, we can construct a semantic distribution for all\nthe concrete states and traces in terms of memorization. In\nAlgorithm 2, we ﬁrst initialize two dictionaries, MT and MS, to\nrepresent memorization traces and states, respectively (Line 1\nand Line 2). We also initialize two dictionaries, AT and AS for\nall the concrete traces and states output from Section IV (Line\n3 and Line 4). Next, we iterate each sentence in the training\ndata to abstract state and trace for each word. If an abstracted\nstate|trace is visited by a memorization sequence, we label the\nstate|trace to a memorization state |trace (Line 6 to Line 15).\nIn our running example, the concrete state corresponding to\n“home” is classiﬁed as a memorization state.\nB. Semantic Memorization Modeling\nWith the memorization states and traces, we build a ﬁrst-\norder Markov model to learn the memorization semantic distri-\nbution conditioned on the state from the last step. Sequential\nbehavior can be regarded as a discrete-time Markov chain.\nTherefore, the memorization probability over a sequence can\nbe modeled by a ﬁrst-order Markov model [34].\nAlgorithm 2: Memorization analysis algorithm\ninput : M= (D,T,S,f ): memorization-analysis-oriented model,\ng: abstraction transformation function,\nH: memorization sequence abstraction function,\nD: sentences\noutput :E(γ,τ ): ﬁrst-order Markov memorization model\n1 MT ←{} ; // a dictionary of memorization traces\n2 MS ←{} ; // a dictionary of memorization states\n3 AT ←{} ; // a dictionary of concrete traces\n4 AS ←{} ; // a dictionary of concrete states\n5 h←H(D,f) ; // function to check if an input is memorization trace\n6 for W ∈D do // loop every sentence to extract states and traces\n7 s ∈[δi(W[: i])]|W|\ni=0;\n8 for i∈1 ... |W|do\n9 s′i−1 ←g(si−1) ;\n10 s′i ←g(si) ;\n11 AT[(s′i−1,s′i)] + +;\n12 AS[(s′i)] + +;\n13 if h(si−1,si) ==True then\n14 MT[(s′i−1,s′i)] + +;\n15 MS[(s′i)] + +;\n16 for (si−1,si) ∈AT do\n17 Eγ(si−1,si) ← AT(si−1,si)∑\nj AT[(si−1,sj)] ;\n18 for si ∈ST do\n19 Eτ(si) ←MS[(si)]\nAS[(si)] ;\n20 return E(γ,τ );\n1) Memorization state|trace distribution construction :\nWe calculate two probabilities representing the memorization\nstate probability Pr(si) and trace probability Tr(si−1,si).\nTo compute the memorization state probability, we count the\nnumber of times a memorization state is visited by any sequence\n(memorization sequence and non-memorization sequence) as\nthe denominator and the number of times a memorization\nstate is visited by the extracted memorization sequences from\nSubsection V-A as numerator. Trace probability Tr(si−1,si)\nrefers to how likely state si−1 reaches state si. In Algorithm 2,\nwe calculate two such probabilities for each sentence in Lines\n16 to 19. For example, the concrete state corresponding to\n“home” is visited by a total of 100 memorization sequences\nand a total of 300 sequences. Therefore, the probability\nof memorization to a concrete state (memorization state)\ncorresponding to “home” is 1/3 (100/300).\n2) Construction of memorization sequence distribution:\nWe calculate the memorization sequence probability based on\nPr(si) and Tr(si−1,si). For a given sequence l consisting\nof n words, we can extract n states s corresponding to each\nword. Based on the chain rule and ﬁrst-order assumptions, the\nmemorization probability of the given l can be computed as:\nPr(s) =\nn∏\ni=1\nTr(si−1,si) ∗Pr(si) (4)\nwhere Tr(s0,s1) = 1. In the rest of this paper, we refer to the\nﬁrst-order Markov memorization model as a semantic model.\nVI. A DDRESSING MEMORIZATION ISSUES USING THE\nMEMORIZATION MODEL\nFinally, we leverage our ﬁrst-order Markov memorization\nmodels that are based on the last step to address the memo-\nrization issues. In particular, our approach ﬁrst automatically\n1007\nassesses the risk of data leakage due to memorization issues.\nNext, our approach assists in the dememorization of the neural\nlanguage models.\nA. Data leakage risk assessment\nA language model potentially poses the risk of remembering\nunintended information from its training data. To assess the\ntraining data leakage risk, we predict whether a sequence from\nthe test data exists in the training data based on our ﬁrst-order\nMarkov memorization model.\nIn the ﬁrst step, for each sentence in the test data, we\nextract the initial states based on the state extraction approach\npresented in Subsection IV-A. It is rare to have two identical\nsemantic states from training and test data in an LSTM network.\nTherefore, we map each state of test data to the closest state\nextracted from the training data by searching the nearest\nneighbor based on cosine distance.\nSecond, we connect all the consecutive semantic states to\nform a sequence. We use the ﬁrst-order Markov model to\ncalculate the memorization probability of each sequence. If\nthe memorization sequence has a high probability, we consider\nthat the sequence would exist in the training data, resulting in\na possible data leakage. We use such uncovered possible data\nleakage to assess the memorization issues from the original\nneural language models.\nB. Assisting in dememorization\nTo assist in dememorization, we mutate the sentences in the\ntraining data that are most likely to lead to data leakage and\nre-build our semantic model to know whether the mutation\nmitigates the unintended memorization behavior. The goal of\nour approach is to mutate the data-leaking sentences while\nminimizing the impact on the data without leakage risks. For\neach sentence, we leverage the memorization probability that\nis generated from our approach to decide whether to mutate\nthe sentence. In short, we only mutate the sentences with high\nmemorization probability and retrain the neural language model\nfrom the data after mutation for dememorization.\nVII. E VALUATION\nA. Experimental setup\nWe evaluate our approach based on one of the state-of-the-\nart word level LSTM-based language models [ 19] with 3,000\nhidden nodes on three popular large datasets, namely, WikiTest-\n103 [ 16], WMT2017-en [ 17], and IWSLT2016-en [ 18]. An\noverview of these datasets is given in Table I. The training data\nis disjoint from the test data. Our experimental environment\nis based on a server with 16 24GB-GPUs, 500 GB of RAM,\nand 1 TB disk. The server runs Ubuntu Linux, version 20.04.\nTable II shows the runtime of each stage of our proposed\napproach over different datasets. Adding a regularization setup\nparameter, each memorization-analysis-oriented model only\nneeds to be constructed once to assess the data leakage of one\nAI model.\nTABLE I: Overview of our datasets\nDataset Sentences Unique Words\nTrain 1M 220KWikiText-103 Test 100K 220K\nTrain 4M 798KWMT2017-en Test 12K 40K\nTrain 177K 59KIWSLT2016-en Test 19K 15K\nTABLE II: Overview of time cost for each step\nSem.\nproﬁling\nDim.\nreduction\nSem.\nclustering\nMem.\nabstraction\nSem. mem.\nmodeling\nW-103 0.25h 0.15h 4h 1.5h 0.1h\nWMT 0.55h 0.62h 15h 5.8h 0.3h\nIWSLT 0.08h 0.08h 1h 0.7h 0.07h\nSem. is abbreviation of semantic. Mem. is abbreviation of memorization.\nB. Preliminary analysis\nGiven a language model, if the memorization data appears to\nhave no inherent common patterns or characteristics, the data\nwould not be prone to data leakage issues, i.e., would not be\nsuitable to our study. Therefore, before applying our approach\nto the three neural language models from the three datasets,\nwe aim to understand the characteristics of the memorization\nsequences in the three neural language models.\nCarlini et al. [27] ﬁnd that a sentence with low perplexity\nis likely to be vulnerable to encounter an attack involving\ndata leakage, where perplexity indicates how well a trained\nlanguage model ﬁts the distribution of sentences. It is deﬁned\nas the inverse probability of the sentences, normalized by the\nnumber of words. Formally, given a sequence l = WN\n1 , the\nperplexity is deﬁned as follows:\nPP(WN\n1 ) =P(w1w2w3...wN)−1\nN\n=\nN\n√\nN∏\ni=1\n1\np(wi|w1w2...wi−1))\n(5)\nwhere wi is the i-th word in this sequence. P indicates\nthe probability of a sentence. From Equation 5, a lower\nperplexity value indicates a better performing language model.\nWe summarize the perplexity distribution over each sentence\nin the training data. If a model assigns a high probability to\na sentence, it is likely that the model tends to remember this\nsentence. Therefore, we also study the relationship between\nperplexity and the length of a memorization sequence in each\nsentence.\nResult: Most of the sentences in the training data have\nlow perplexity. Figure 2 shows the perplexity distribution over\nthe three training datasets, WikiTest-103 (a), WMT2017-en\n(b), and IWSLT2016-en (c). Prior studies [ 35], [36] report that\na language model with a perplexity below 100 is considered\na well-performing model. In particular, considering the prior\nstudy [35] using the same training data, the authors report that\ntheir language model achieves a perplexity of 34.4 for WikiText-\n103. We ﬁnd that most of the sentences in the training data have\nlow perplexity. In particular, at least 96% of the sentences have\na perplexity less than 100 in our three experimental datasets.\n1008\nSuch a result implies that the trained language model can\nremember most of the sentences from the training data.\n0.00\n0.01\n0.02\n0.03\n0.04\n0 50 100 150 200 250\nPerplexity\nDensity of sentence\n(a) WikiText-103.\n0.00\n0.01\n0.02\n0.03\n0 50 100 150 200 250\nPerplexity\nDensity of sentence (b) WMT2017-en.\n0.00\n0.02\n0.04\n0.06\n0 50 100 150 200 250\nPerplexity\nDensity of sentence (c) IWSLT2016-en.\nFig. 2: Density distribution of number of sentences over\nperplexity.\nThe sentences with a longer memorization sequence have\nlower perplexity. Figure 3 shows the density of the length of\nmemorization sequences in terms of perplexity over the training\ndata. The X-axis indicates the perplexity (with increasing steps\nof 50). The Y-axis shows the density of length of memorization\nsequences. Note in Figure 2 and Figure 3 that most of the\nmemorization sequences with low perplexity ( <50) contain\nat least six words. Such results imply that the sentences in\nthe training data that have longer memorization sequences are\neasier to be remembered by the language model.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n0 50 100 150 200 250\nPerplexity\nAverage length of sentence\n(a) WikiText-103.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n0 50 100 150 200 250\nPerplexity\nAverage length of sentence (b) WMT2017-en.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n0 50 100 150 200 250\nPerplexity\nAverage length of sentence (c) IWSLT2016-en.\nFig. 3: The average length of memorization sequence distribu-\ntion in terms of perplexity over three datasets.\nSummary of preliminary analysis: Most of the sentences\nin the studied datasets have low perplexity, which shows\nthat the subject neural language model may be prone to the\nmemorization issue.\nC. Results\nRQ1: To what extent are the studied neural language\nmodels prone to memorization issues?\nMotivation: In our preliminary analysis, our results show that\nmost of the sentences in the studied datasets have low perplexity\nand such sentences with low perplexities may be prone to be\nremembered by neural language models. As such, one can\nmodel the memorization distribution and exploit the learned\nmemorization to extract and store the valuable training data.\nTherefore, in this research question, we want to explore to\nwhat extent the studied neural language models are prone to\nmemorization issues.\nApproach: To answer RQ1, we ﬁrst want to know the preva-\nlence of potential memorization issues in our studied datasets.\nIf a state |trace is a memorization state |trace, such a state |trace\nis a potential memorization issue. To quantify the potential\nmemorization issue, we deﬁne two metrics, SCR and TCR, to\nevaluate our memorization-analysis-oriented model. SCR is the\nmemorization state coverage rate and TCR is the memorization\ntrace coverage rate. The name state |trace memorization implies\nthat the state |trace is visited by a memorization sequence.\nFormally, SCR is deﬁned as Num MS\nNum state, and TCR is deﬁned as\nNum MT\nNum trace. Num MS is the number of distinct memorization\nstates and Num MT is the number of distinct memorization\ntraces. Num state and Num trace refer to the total of\ndistinct concrete states and traces, respectively. We follow\nthe following steps to calculate the two metrics, SCR and TCR.\nWe ﬁrst apply the proposed modeling approach in Section IV to\nobtain the memorization-analysis-oriented model from training\ndata. Second, we employ the memorization extraction approach\nfrom Section\nV-A to extract the memorization sequences from\ntraining data. Next, for each word in a memorization sequence,\nwe can map it to the semantic model to obtain the memorization\nstate and trace.\nMemorization states|traces can be visited by both memoriza-\ntion sequences and non-memorization sequences. The more\nmemorization sequences visit a state|trace, the more likely such\na state|trace is prone to memorization issues. Therefore, we\nalso quantify the memorization issues of our studied datasets\nusing memorization state and trace probability. We calculate\nmemorization state and trace probabilities using the approach\npresented in Section V-B. The higher the memorization\nstate|trace probabilities are, the more possible such a state |trace\nis prone to memorization issues.\nResult: Only a small portion of states and traces from\ntraining data are related to memorization. The result of the\nstate and trace coverage rate is shown in Table III. In the table,\nthe column σ is the input of the clustering algorithm DBSCAN\nused to control the granularity of clusters. The result shows\nthat most of the states and traces are unrelated to memorization.\nThe state coverage rate ranges from 6.8% to 24.5%. The traces\ncoverage rate is less than 4.03% in any of different inputs of\ncore σ. The results show that only a small percentage of states\nand traces are related to memorization. Such results imply that\neither 1) there are only a few memorization issues or 2) there\nexist many memorization issues, and such memorization issues\nonly cover a small percentage of memorization states/traces.\nIn addition, our approach can efﬁciently reduce the number\nof initial states and traces. For example, when using a σ of\n100 as input for our clustering algorithm DBSCAN, we reduce\nthe initial millions states into 40,121 concrete states. Such a\nconsiderable number of states cannot only be used to analyze\nmemorization behavior of a language model, but can also be\nused to retain most of the semantic information.\nThe memorization states and traces have a considerable\nhigh memorization probability. Figure 4 shows the results\nof the probability distribution of the memorization states\nand traces over the three studied datasets. Although only\nlow percentages of states (an average of 17.6%) and traces\n(an average of 2.24%) are related to memorization, the\nmemorization state and trace probabilities are comparably\n1009\nTABLE III: Results of memorization state and trace coverage\nrate. (Mem. is the abbreviation for “memorization“.)\nDataset σ All concrete\nstates\nAll concrete\ntraces Mem. states Mem. traces TCR SCR\nW-103\n100 40,121 31,9450 3,258 6,740 1.02% 16.8%\n150 79,820 521,460 18,518 16,060 3.08% 23.2%\n200 82,317 613,419 16,792 11,593 1.89% 20.4%\n250 89,012 634,210 17,534 9196 1.45% 19.7%\nWMT\n100 63,902 549,872 7,221 6,653 1.21% 11.3%\n150 71,921 673,219 17,620 13,666 2.03% 24.5%\n200 76,709 778,895 12,426 14,721 1.89% 16.2%\n250 77,101 792,015 5,244 6,256 0.79% 6.8%\nIWSLT\n100 4,523 26,217 557 738 2.81% 12.3%\n150 8,945 114,084 1,923 4,598 4.03% 21.5%\n200 11,219 139,930 2,546 4,886 3.49% 22.7%\n250 14,234 178,904 2,246 5,831 3.26% 15.8%\nhigh. Especially, the mean memorization state probability in\ndataset WikiText-103 is 0.63. By inspecting our results, we ﬁnd\nthat the memorization-analysis-oriented model can identify the\nmemorization transition of the LSTM-based language model\nand discover potential memorization issues in the training data.\nAnswer to RQ1: Only a small percentage of states and\ntraces from training data are related to memorization. How-\never, the memorization states and traces have a considerably\nhigh memorization probability.\nRQ2: How accurate is our approach in the data leakage\nrisk assessment?\nMotivation: In RQ1, the results show that the memorization\nstates and traces tend to be remembered due to a considerable\nhigh memorization probability. The associated distribution can\nbe used to analyze the training data and the potential for data\nleakage. In order to illustrate a practical impact, we leverage\nour approach to assess training data leakage risk based on a\ngiven language model. In this research question, we want to\nanswer how accurate our privacy risk assessment approach is.\nApproach: In Section V, we have built a ﬁrst-order Markov\nmemorization model. To realistically assess the privacy risk\nof given data, we use the constructed model to measure the\nmemorization probability of each sequence in the test data.\nBased on the predicted memorization probability of each\nsequence in the test data, which is not seen by the model\nduring the training phase, we predict whether a sequence of\ntest data likely exists in the training data.\nFurthermore, the length of a memorization sequence might\naffect the modeling analysis. For example, one may argue\nthat the shorter a memorization sequence is, the more likely\nthe sequence appears in the training data. Therefore, we\ncalculate the Pearson correlation [\n37] between the length of\nmemorization sequences and memorization probabilities of\nsequences. Pearson correlation ranges from -1 to +1. A value\nof 1 indicates that the length and memorization probability\nof sequences has a strong relationship. A value of 0 indicates\nthat there is no relationship between them, and a value of -1\nindicates an inverse relationship between them.\nWe implement a baseline approach that assigns a random\nscore to each of the extracted memorization sequences. We\ncompare DeepMemory to the baseline in this research question.\nTo measure the performance, we examine whether the extracted\nsequences from the test data appear in the training data. If\na sequence is indeed in the training data, we consider it\nas a true-positive sequence. Otherwise, it is a false-positive\nsequence. The true-positive sequence is considered to be data\nleakage from training data. We use four metrics to evaluate our\napproach, including precision, recall, F1, and AUC. Precision\nmeasures the correctness of our model, and refers to the ratio\nof cases when a predicted sequence is actually in the training\ndata. Recall measures the completeness of our approach, and\nis deﬁned as the number of sequences that were correctly\npredicted as memorization divided by the total number of\nmemorization sequences in the test data. F1 is the harmonic\nmean of precision and recall. AUC allows us to measure the\noverall ability of our approach. The AUC is the area under\nthe ROC curve, which indicates the performance of a binary\nmodel as its discrimination is varied.\nResult: Our data leakage assessment approach can achieve\nan average AUC of 73%. Table IV shows the results\nfor precision, recall, F1, and AUC over the memorization\ndistribution. Note from Table IV that our approach achieves\nan average precision of 47% and a very high average recall of\n92% when taking 0.5 as a threshold, which outperforms the\nbaseline approach, i.e., a precision of 0.38 and a recall of 56%.\nThe results imply that a sequence with a high memorization\nprobability in the test data tends to be memorized. However,\ndifferent thresholds may lead to different results. To overcome\nthis bias, we also present the AUC of our approach. We ﬁnd that\nthe AUC is high with an average value of 73%. The results\nsuggest that our proposed ﬁrst-order memorization Markov\nmodel approach is capable of assessing data leakage risks.\nTABLE IV: Results of using our approach to predict the\nmemorized sequence compared with the baseline approach.\nDeepMemory Baseline\nPrecision Recall F1 AUC Precision Recall F1 AUC\nW-103 0.50 0.75 0.60 0.72 0.38 0.50 0.42 0.48\nWMT 0.29 1.00 0.44 0.67 0.30 0.57 0.40 0.50\nIWSLT 0.62 1.00 0.76 0.80 0.50 0.60 0.54 0.48\nAverage 0.47 0.92 0.60 0.73 0.39 0.56 0.45 0.49\nOur approach shows a similar performance for all types\nof sequences. The Pearson correlation between length and\nmemorization probability of memorization sequence is 0.14.\nAn absolute value of 0-0.19 is regarded as a very weak\ncorrelation [ 37]. Therefore, a very weak relationship exists\nbetween the length of a memorization sequence and the\nmemorization probability of sequences.\nOur approach can be used to efﬁciently identify a real-\nworld data leakage issue. In order to demonstrate the practical\nusefulness of our approach, we want to examine whether our\napproach can be used to identify real-world private data. We\ntrain a language model based on the setting from [ 8]. Similar\nto the prior work [ 8], we make the trained language model\n1010\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0 0.2 0.4 0.6 0.8\nMemorization state probability\nDensity\nmin        0.15\nmedian  0.63\nmean     0.63\nmax       0.79\n(a) WikiText-103 state.\n0.00\n0.25\n0.50\n0.75\n1.00\n0.2 0.4 0.6\nMemorization trace probability\nDensity\nmin        0.04\nmedian  0.55\nmean     0.52\nmax       0.69 (b) WikiText-103 trace.\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00 0.25 0.50 0.75 1.00\nMemorization state probability\nDensity\nmin        0.01\nmedian  0.25\nmean     0.28\nmax       1.00 (c) WMT2017 state.\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00 0.25 0.50 0.75 1.00\nMemorization trace probability\nDensity\nmin        0.01\nmedian  0.24\nmean     0.25\nmax       1.00 (d) WMT2017 trace.\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00 0.25 0.50 0.75 1.00\nMemorization state probability\nDensity\nmin        0.01\nmedian  0.14\nmean     0.19\nmax       1.00 (e) IWSLT2016 state.\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00 0.25 0.50 0.75 1.00\nMemorization trace probability\nDensity\nmin        0.01\nmedian  0.20\nmean     0.23\nmax       1.00 (f) IWSLT2016 trace.\nFig. 4: Memorization states and traces probability distribution.\nremember the sequence “the credit number is 281265017”.\nAfter that, we analyzed this language model based on our\nproposed approach. During the testing phase, we test our\nsemantic Markov memorization model on a set of sentences\nwith the same structure but different credit numbers. We ﬁnd\nthat the sentence “the credit number is 281265017” has the\nhighest memorization probability. Note that the prior work has\nreported that memorization is not overﬁtting [ 8]. The result\nsuggests that our proposed model can efﬁciently detect the\nmemorization content from the training data.\nAnswer to RQ2: Our data leakage assessment approach\ncan achieve an average AUC of 73%. Our approach shows\na similar performance for all types of sequences. Our\napproach can be used to efﬁciently identify real-world\nprivate data.\nRQ3: How effective is our approach in assisting dememo-\nrization?\nMotivation:\nOne may randomly select sentences and mutate\nthem to reduce memorization sequences probability (see\nEquation 4). However, it is not an optimal solution to mutate a\nlarge portion of the training data since the mutation would hurt\nthe quality of the data, leading to unrealistic models. On the\nother hand, if one only randomly mutates a small portion of the\ntraining data, the mutated data may not contain memorization\nissues. In this research question, we want to evaluate whether\nour approach can assist in dememorization by suggesting only\na small portion of data in the training data to be mutated.\nApproach: We compare the use of our approach in assisting\ndememorization to a random baseline approach. We ﬁrst apply\nour approach to detect the memorization sequences from the\ntraining data and to select memorization sequences. The results\nof RQ2 show that when using 0.5 as the threshold to predict\nmemorization sequence, our recall is very high (close to 1).\nTherefore, we select memorization sequences to be mutated if\ntheir probabilities are more than 0.5. For the random approach,\nwe randomly select 50% of all the memorization sequences\nto be mutated. We choose 50% for the baseline approach in\norder to give the baseline approach an overestimated ability of\nmutating the training data. 50% also ensures that at least half of\nthe existing training data is not mutated. In both experiments,\nwe ignore memorization sequences with lengths less than four.\nSecond, we use four strategies to mutate the aforementioned\nselected sequences from the original training data to mitigate\nunintended memorization behavior.\n• REPlacing Word (REPW): For each extracted sequence,\nwe ﬁrst select the noun and verbal phrase that occurred\nless frequently. We then replace the selected words with\ntheir synonyms in the training data randomly. If there\nare no synonyms in the training data, we replace them\nwith a random external synonym. Next, we modify the\ncorresponding sentences that contain mutated sequences.\n• REOrdering Sequence (REOS): Prior research [2], [36]\nshows that sequence disorder can beneﬁt the robustness\nof a sequential model in machine translation tasks and\nindustrial recommendation system applications. This strat-\negy aims to reorder words in memorization sequences to\nconfuse the language models.\n• REMoving Word (REMW): For the sentences that con-\ntain memorization sequences, we remove those sequences\ndirectly from the sentences.\n• MIXture (MIX) : Different strategies may have their\nadvantages. In the mixture strategy, we combine the\nreplacing words and reordering sequences approaches.\nNext, we re-train a language model based on the mutated\ntraining data and re-build our semantic ﬁrst-order Markov\nmemorization model. Finally, we use our semantic model to\nanalyze the memorization behavior of the re-trained neural\nlanguage model on the original training data. In particular,\nwe extract the memorization sequences of re-trained neural\nlanguage models. We then calculate how many memorization\nsequences in the original model (before mutation) still exist\nin the re-trained model. The fewer memorization sequences\nthat are left, the better dememorization the re-trained model\nhas. We also calculate the number of mutated memorization\nsequences from both our approach and the random baseline. The\ndesired approach would achieve a low number of memorization\nsequences that are left in the re-trained model, while only\nhaving to mutate a small percentage of memorization sequences.\nResult: Our approach can assist in dememorization with-\nout the need to mutate a large number of memorization\nsequences.\nTable V shows the results for memorization\nsequence statistics after re-training the language model using\ndifferent strategies to mutate the training data. With assistance\nfrom our approach, the memorization sequences can be signiﬁ-\ncantly reduced. Table V shows that, compared to the original\nmemorization sequences, the percentages of the memorization\nsequences drop to 2.58%, 2.31%, and 4.43% in WikiText-103,\nWMT2017, and IWSLT2016, respectively. Compared to our ap-\n1011\nTABLE V: Total number of original memorization sequences and the number of memorization sequences after dememorization\nassisted by our approach and the baseline approach.\nDataset Measure Original Mutated Sequence (%) after REPW after REOS after REMW after MIX Average\nProb.>0.5 Random Prob.>0.5 Random Prob.>0.5 Random Prob.>0.5 Random Prob.>0.5 Random Prob.>0.5 Random\nW-103 Mem. Seq. 59,802 4.10% 50% 1,645\n(2.75%)\n3,519\n(5.88%)\n1,543\n(2.58%)\n4,210\n(7.04%)\n1,549\n(2.59%)\n2,431\n(4.07%)\n2,021\n(3.38%)\n3,979\n(6.65%)\n1,690\n(2.83%)\n3,299\n(5.91%)\nWMT Mem. Seq. 124,319 0.89% 50% 4,210\n(3.39%)\n3,577\n(2.88%)\n2,874\n(2.31%)\n2,576\n(2.07%)\n3,121\n(2.51%)\n1,498\n(1.20%)\n2,989\n(2.40%)\n2,249\n(1.81%)\n3,299\n(2.65%)\n2,475\n(1.99%)\nIWSLT Mem. Seq. 18,753 2.80% 50% 2,091\n(11.15%)\n3,202\n(17.07%)\n2,484\n(13.25%)\n3,389\n(18.07%)\n831\n(4.43%)\n1,034\n(5.51%)\n1,701\n(9.07%)\n2,214\n(11.81%)\n1,777\n(9.47%)\n2,460\n(13.12%)\nOriginal is the number of memorization sequences in the original model. Mutated sequence means the percentage of memorization sequences to be mutated.\nColumns starting with “after” mean after mutating the training data, the number of memorization sequence that are left and the corresponding percentage.\nproach, the average of percentages of memorization sequences\nthat are left after the mutation from the random baseline\nare 5.91%, 1.99%, and 13.12% in WikiText-103, WMT2017,\nand IWSLT2016, respectively. Except for WMT2017, where\nboth approaches show a similar performance in reducing the\nmemorization sequences, our approach outperforms the baseline\napproach by a wide margin.\nOur approach only needs to mutate a very small\nnumber of sequences from the training data. Table V\nshows the number of memorization sequences that are mutated\nduring the dememorization process. The results illustrate that\nour approach only mutates 4.1%, 0.89%, and 2.8% of the\noriginal memorization sequences in the datasets WikiText-\n103, WMT2017, and IWSLT2016, respectively. Such a small\nnumber of mutations would have a trivial impact on the trained\nmodel. By deﬁnition, the baseline approach mutates 50% of the\nmemorization sequences, i.e., a very large amount of mutation,\nand cannot even achieve a comparable dememorization result.\nAnswer to RQ3: Our approach is capable of guiding\ndememorization and does not decrease the performance\nof the original model. Therefore, practitioners can use our\napproach to discover sensitive data leakage risks and help\nmitigate memorization.\nVIII. C OMPARATIVE STUDY ON THE EFFECT OF\nREGULARIZATION\nIn this section, we discuss the impact of regularization on the\nmemorization effect. Regularization is an efﬁcient approach to\ntrain neural network based models. Although a prior study [ 8]\nshows that memorization in neural language models is not\nan issue of overﬁtting, the use of regularization may still\npotentially affect the memorization behavior of neural language\nmodels. Therefore, we conduct a comparative study over four\nmainstream regularization techniques, including dropout, L1\nnorm, L2 norm regularization and data augmentation (DA).\nWe build an original model without any regularization. To\nevaluate the impact of the regularization techniques, we create\nfour additional models by modifying our original model by\naltering only one regularization technique, including enabling:\n1) dropout, 2) L1 norm, 3) L2 norm, and 4) DA. In particular,\nthe augmentation is to randomly select 10% of the sentences\nfrom the training corpus and randomly replace non-stop words\nwith one of their synonyms [38].\nWe follow a process similar to RQ1 to conduct our compar-\native study. In particular, our experiment is executed with σ in\n200. We ﬁrst calculate two metrics SCR and TCR from the four\nadditional models while altering the regularization techniques.\nWe then calculate their corresponding memorization state and\ntrace probabilities. Finally, we compare the results for the four\nadditional models with the results from our original models.\nResults: Regularization may be able to mitigate the mem-\norization effect. The results (with and without regulariza-\ntion) are shown in Table VI. The results show that without\nregularization, the memorization state coverage rate ranges\nfrom 23.1% to 31.4% and the memorization trace coverage\nrate ranges from 7.43% to 11.32%. After regularization, both\nthe memorization state and the trace coverage rate decrease\nconsiderably. Especially, the L2 norm regularization provides\nthe highest reduction in the memorization state and trace\ncoverage (19.8% and 1.89%, respectively).\nIn addition, we compare the memorization state and trace\nprobability distribution of the above four additional models\nwith the ones from the original models, using the Mann-\nWhitney U test and Cliff’s delta. We ﬁnd that all of the\nprobability distributions of the four additional models are\ndifferent with statistical signiﬁcance (\np < 0.05) from the\noriginal model. However, the difference may differ among\ndifferent subjects. In particular, for WMT, the original models\n(without regularization) always have a higher memorization\nprobability than the four additional models (positive effect\nsizes). For IWSLT and W-103, the differences are associated\nwith rather negligible or small effect sizes; while cases also exist\nwhere the probability distribution is lower with regularization\n(e.g., W-103; enabling dropouts). Such results suggest that\nregularization (in particular, the L2 norm) may be useful\nto partially address memorization issues, but they cannot be\neliminated. More comparative work is required to highlight\nthe relative impact of the different approaches.\nIX. T HREATS TO VALIDITY\nExternal validity. A threat to the external validity is the\ngeneralizability of our approach. Our study is evaluated on\nthe most popular neural language model, i.e., the LSTM-based\nlanguage model, and three speciﬁc public datasets. More case\n1012\nTABLE VI: Results of memorization coverage rate with and\nwithout regularization (Reg. means regularization).\nDataset Reg. All concrete\nstates\nAll concrete\ntraces\nMem.\nStates\nMem.\nTraces TCR SCR\nW-103\nOriginal 81,790 631,521 22,820 54,248 8.59% 27.9%\nDropout 83,210 647,932 18,639 16,003 2.47% 22.4%\nL1 82,123 627,984 19,545 16,076 2.56% 23.8%\nL2 80,789 642,983 17,531 12,152 1.89% 21.7%\nDA 84,198 852,129 21,883 61,609 7.23% 26.0%\nWMT\nOriginal 78,256 823,943 18,077 93,270 11.32% 23.1%\nDropout 72,198 878,134 13,212 18,528 2.11% 18.3%\nL1 79,821 849,702 13,729 31,863 3.75% 17.2%\nL2 76,213 851,203 15,090 23,578 2.77% 19.8%\nDA 77,678 812,323 17,656 81,232 10.01% 22.7%\nIWSLT\nOriginal 14,232 176,820 4,468 13,137 7.43% 31.4%\nDropout 11,950 122,561 3,274 5,172 4.22% 27.4%\nL1 13,212 119,821 2,893 3,582 2.99% 21.9%\nL2 12,792 98,996 2,533 4,237 4.28% 19.8%\nDA 13,341 15,421 3,867 1,076 6.98% 28.9%\nstudies on other datasets in other neural network based language\nmodels can beneﬁt the evaluation of our approach.\nInternal validity. Our work uses several techniques, such as\nthe clustering algorithm DBSCAN, the dimension analysis\nalgorithm PCA, and the First-Order Markov model. Such\ntechniques can be replaced by other kinds of similar techniques.\nFor example, DBSCAN can be replaced with the k-means\nclustering algorithm. Our approach also leverages threshold\nvalues, for example, the σ and ρ of the DBSCAN. To explore\nthe impact of these choices, we individually increased or\ndecreased the\nσ (omitted due to limited space) and ρ (see\nTable III) values in our experiment.\nConstruct validity. In the evaluation of our approach for\ndememorization, we only used four strategies to mutate the\ntraining data. Similar evaluation approaches based on mutation\ntechniques have been often used in prior research [39]. However,\nthere may exist other kinds of strategies to mutate the training\ndata. Future work can complement our evaluation.\nX. R ELATED WORK\nAnalysis of DNN. Many prior studies [ 40], [ 41], [ 42], [ 43],\n[44], [ 45], [ 46], [ 14], [ 15] have been proposed to analyze\nand explain the behaviors of deep neural network. Functional\nanalysis and decision analysis are two main categories of\nanalysis of DNN [\n47]. Functional analysis, i.e., black-box\nanalysis, aims to capture the overall behavior by investigating\nthe relation between inputs and outputs [ 41], [ 43], [ 48].\nDecision analysis takes the DNN as a white box and analyzes\nthe internal behavior by proﬁling internal structures and\ncomponent rolls [14], [15], [40].\nIn our study, we focus on the decision analysis, i.e., internal\nbehavior analysis. One of the typical techniques used to\nanalyze the internal behavior of a DNN model is Finite State\nAutomation (FSA) [ 49], [ 30]. FSA consists of states and\ntransitions, which can be mapped to the behavior of sequence\nmodels. Du et al. [14] use an interval-based approach to cluster\nthe original hidden-state vector which produces comparable\nperformance under a scalable environment.\nPrior studies focus on the analysis of behavior of the RNN\nmodel and its variance in FSA for the natural language process-\ning task. However, there is a lack of work on memorization\nissues for language models. Our paper is the ﬁrst work on\nanalyzing, detecting and assisting in repairing memorization\nissues of RNN models.\nGeneral privacy of DNN. Extensive prior research has\nrevealed serious privacy issues posed by deep neural networks\nas the data used for training can be leaked [ 50]. In general,\nprivacy threats of the deep neural network can be divided into\nthe two categories of direct and indirect information exposure\nhazards [51]. Direct privacy data leakage is mainly due to the\ndata curator [ 52], [53], untrusted communication link [ 54] and\nuntrusted cloud [ 55]. In terms of the indirect privacy threat,\none would like to infer or guess information for training data\nor model parameters without access to the actual data [ 56].\nMany prior studies [ 9], [ 10], [ 11] have reported that deep\nneural networks tend to memorize the training data instead\nof learning the latent properties of the training data. Some\nstudies [10], [57], [58], [59] propose automatic techniques that\ninfer whether a given data instance has contributed to the target\nmodel. Shokri et al. [\n57] propose the ﬁrst membership inference\nattack to deduce whether a data record is used in the training\nprocess for the targeted model. The core idea is to distinguish\na given record in terms of the conﬁdence score output by the\ntargeted model. In addition to membership inference, research\nalso aims to infer sensitive attributes for a released model [ 50],\n[60], [61] and to steal model parameters [ 56], [62], [63], [64].\nPrior studies develop attacks and defenses for investigating\nvarious privacy challenges. Different from previous work, we\nconsider a privacy breach related to memorization in neural lan-\nguage models and analyze memorization via abstracted hidden\nstates from the extracting ﬁnite state machine. Our approach\naims to address privacy issues during the quality assurance\nprocess for developing AI models, instead of defending against\nsuch attacks after the fact. Our work contributes to the area of\ngeneral privacy of deep neural networks.\nXI. C ONCLUSION\nThis paper proposes DeepMemory, a novel approach for\nanalyzing the internal memorization behavior in language\nmodels. We construct a memorization-analysis-oriented model\nand build a semantic ﬁrst-order Markov model to analyze\nmemorization distribution. We evaluate our approach based on\none of the most popular neural language models, the LSTM-\nbased language model with three public datasets, namely,\nWikiText-103, WMT2017, and IWSLT2016. The results show\nthat using our approach, we can address memorization issues\nby automatically identifying data leakage risks with an average\nAUC of 0.73. Based on the assessment results, our approach\ncan assist in dememorization by only mutating a very small\npercentage (4.1%, 0.89% and 2.8%) of the training data to\nreduce the memorization in the neural language models. Our\nwork calls for future research to address the privacy issues in\nneural language models.\nXII. A CKNOWLEDGEMENTS\nWe would like to thank Thomas Vannet for his insightful\nfeedback on an earlier version of this work. Furthermore, we\nthank the anonymous reviewers for their valuable comments.\n1013\nREFERENCES\n[1] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for\nabstractive sentence summarization,” in 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). The Association\nfor Computational Linguistics, 2015, pp. 379–389.\n[2] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao, K. Macherey et al., “Google’s neural\nmachine translation system: Bridging the gap between human and\nmachine translation,” arXiv preprint arXiv:1609.08144, 2016.\n[3] M. Pak and S. Kim, “A review of deep learning in image recognition,”\nin 2017 4th international Conference on Computer Applications and\nInformation Processing Technology (CAIPT). IEEE, 2017, pp. 1–3.\n[4] B. Huval, T. Wang, S. Tandon, J. Kiske, W. Song, J. Pazhayampallil,\nM. Andriluka, P. Rajpurkar, T. Migimatsu, R. Cheng-Yue et al., “An\nempirical evaluation of deep learning on highway driving,” arXiv preprint\narXiv:1504.01716, 2015.\n[5] S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu, “A survey of deep\nlearning techniques for autonomous driving,” Journal of Field Robotics,\nvol. 37, no. 3, pp. 362–386, 2020.\n[6] Y . Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin,\nJ. Dillon, B. Lakshminarayanan, and J. Snoek, “Can you trust your\nmodel’s uncertainty? Evaluating predictive uncertainty under dataset\nshift,” Advances in Neural Information Processing Systems, vol. 32, pp.\n13 991–14 002, 2019.\n[7] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,\nL. Li, Y . Liuet al., “Deepgauge: Multi-granularity testing criteria for deep\nlearning systems,” in Proceedings of the 33rd ACM/IEEE International\nConference on Automated Software Engineering, 2018, pp. 120–131.\n[8] N. Carlini, C. Liu, ´U. Erlingsson, J. Kos, and D. Song, “The secret sharer:\nEvaluating and testing unintended memorization in neural networks,”\nin 28th USENIX Security Symposium (USENIX Security). USENIX\nAssociation, 2019, pp. 267–284.\n[9] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,\nT. Maharaj, A. Fischer, A. Courville, Y . Bengio et al., “A closer look at\nmemorization in deep networks,” in International Conference on Machine\nLearning. PMLR, 2017, pp. 233–242.\n[10] S. Truex, L. Liu, M. E. Gursoy, L. Yu, and W. Wei, “Towards demystifying\nmembership inference attacks,” arXiv preprint arXiv:1807.09173, 2018.\n[11] C. Meehan, K. Chaudhuri, and S. Dasgupta, “A non-parametric test to de-\ntect data-copying in generative models,” arXiv preprint arXiv:2004.05675,\n2020.\n[12] S. Ovaska, “Data privacy risks to consider when using AI.”\n[Online]. Available: https://www.fm-magazine.com/issues/2020/feb/\ndata-privacy-risks-when-using-artiﬁcial-intelligence.html\n[13] X. Zhang, X. Xie, L. Ma, X. Du, Q. Hu, Y . Liu, J. Zhao, and M. Sun,\n“Towards characterizing adversarial defects of deep learning software\nfrom the lens of uncertainty,” in 2020 IEEE/ACM 42nd International\nConference on Software Engineering (ICSE). IEEE, 2020, pp. 739–751.\n[14] X. Du, X. Xie, Y . Li, L. Ma, Y . Liu, and J. Zhao, “Deepstellar:\nModel-based quantitative analysis of stateful deep learning systems,” in\nProceedings of the ACM Joint Meeting on European Software Engineering\nConference and Symposium on the Foundations of Software Engineering\n(ESEC/SIGSOFT, FSE), 2019, pp. 477–487.\n[15] X. Zhang, X. Du, X. Xie, L. Ma, Y . Liu, and M. Sun, “Decision-guided\nweighted automata extraction from recurrent neural networks,” in Thirty-\nFifth AAAI Conference on Artiﬁcial Intelligence (AAAI). AAAI Press,\n2021, pp. 11 699–11 707.\n[16] T. Wolf, Q. Lhoest, P. von Platen, Y . Jernite, M. Drame, J. Plu,\nJ. Chaumond, C. Delangue, C. Ma, A. Thakur, S. Patil, J. Davison,\nT. L. Scao, V . Sanh, C. Xu, N. Patry, A. McMillan-Major, S. Brandeis,\nS. Gugger, F. Lagunas, L. Debut, M. Funtowicz, A. Moi, S. Rush,\nP. Schmidd, P. Cistac, V . Muˇstar, J. Boudier, and A. Tordjmann, “Datasets,”\nGitHub. Note: https://github.com/huggingface/datasets, vol. 1, 2020.\n[17] O. Boja et al., 2017 Second Conference on Machine Translation (WMT17):\nProceedings. Association for Computational Linguistics, 2017.\n[18] “IWSLT evaluation 2016,” https://sites.google.com/site/\niwsltevaluation2016/iwslt-evaluation-2016.\n[19] S. Merity, N. S. Keskar, and R. Socher, “An analysis of neural language\nmodeling at multiple scales,” arXiv preprint arXiv:1803.08240, 2018.\n[20] K. Jing and J. Xu, “A survey on neural network language models,” arXiv\npreprint arXiv:1906.03591, 2019.\n[21] S. Ortmanns, H. Ney, and A. Eiden, “Language-model look-ahead for\nlarge vocabulary speech recognition,” in Proceedings of the Fourth\nInternational Conference on Spoken Language Processing (ICSLP).\nIEEE, 1996, pp. 2095–2098.\n[22] P. F. Brown, J. Cocke, S. A. Della Pietra, V . J. Della Pietra, F. Jelinek,\nJ. Lafferty, R. L. Mercer, and P. S. Roossin, “A statistical approach\nto machine translation,” Computational Linguistics, vol. 16, no. 2, pp.\n79–85, 1990.\n[23] K.-L. Liu, W.-J. Li, and M. Guo, “Emoticon smoothed language models\nfor Twitter sentiment analysis,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, 2012.\n[24] F. Song and W. B. Croft, “A general language model for information\nretrieval,” in Proceedings of the Eighth International Conference on\nInformation and Knowledge Management, 1999, pp. 316–321.\n[25] K. Smagulova and A. P. James, “A survey on LSTM memristive neural\nnetwork architectures and applications,” The European Physical Journal\nSpecial Topics, vol. 228, no. 10, pp. 2313–2324, 2019.\n[26] F. Mireshghallah, H. A. Inan, M. Hasegawa, V . R¨uhle, T. Berg-Kirkpatrick,\nand R. Sim, “Privacy regularization: Joint privacy-utility optimization in\nlanguage models,” arXiv preprint arXiv:2103.07567, 2021.\n[27] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee,\nA. Roberts, T. Brown, D. Song, ´U. Erlingsson, A. Oprea, and C. Raffel,\n“Extracting training data from large language models,” in 30th USENIX\nSecurity Symposium (USENIX Security). USENIX Association, Aug.\n2021, pp. 2633–2650.\n[28] S. Merity, N. S. Keskar, and R. Socher, “Regularizing and optimizing\nLSTM language models,” arXiv preprint arXiv:1708.02182, 2017.\n[29] C. Dwork, A. Roth et al., “The algorithmic foundations of differential\nprivacy.”Foundations and Trends in Theoretical Computer Science, vol. 9,\nno. 3-4, pp. 211–407, 2014.\n[30] G. Weiss, Y . Goldberg, and E. Yahav, “Extracting automata from recurrent\nneural networks using queries and counterexamples,” in International\nConference on Machine Learning. PMLR, 2018, pp. 5247–5256.\n[31] R. Krishnan, D. Liang, and M. Hoffman, “On the challenges of\nlearning with inference networks on sparse, high-dimensional data,” in\nInternational Conference on Artiﬁcial Intelligence and Statistics. PMLR,\n2018, pp. 143–151.\n[32] B. Geiger and G. Kubin, “Relative information loss in the PCA,” in 2012\nIEEE Information Theory Workshop. IEEE, 2012, pp. 562–566.\n[33] M. Ester, H. Kriegel, J. Sander, and X. Xiaowei, “A density-based\nalgorithm for discovering clusters in large spatial databases with noise,” in\nProceedings of the 2nd International Conference on Knowledge Discovery\nand Data Mining (KDD), 1996, pp. 226–231.\n[34] M. Zorzi, R. R. Rao, and L. B. Milstein, “On the accuracy of a ﬁrst-\norder Markov model for data transmission on fading channels,” in\nProceedings of the 4th IEEE International Conference on Universal\nPersonal Communications (ICUPC). IEEE, 1995, pp. 211–215.\n[35] J. Rae, C. Dyer, P. Dayan, and T. Lillicrap, “Fast parametric learning\nwith activation memorization,” in Proceedings of the 35th International\nConference on Machine Learning. PMLR, 2018, pp. 4228–4237.\n[36] Q. Jia, N. Zhang, and N. Hua, “Context-aware deep model for entity\nrecommendation system in search engine at Alibaba,” Journal of\nMultimedia Processing and Technologies, vol. 11, no. 1, pp. 23–35,\n2020.\n[37] J. Benesty, J. Chen, Y . Huang, and I. Cohen, “Pearson correlation\ncoefﬁcient,” in Noise Reduction in Speech Processing. Springer, 2009,\npp. 1–4.\n[38] M. Bayer, M.-A. Kaufhold, and C. Reuter, “A survey on data augmenta-\ntion for text classiﬁcation,” arXiv preprint arXiv:2107.03158, 2021.\n[39] C. Auerbach, Mutation research: problems, results and perspectives.\nSpringer, 2013.\n[40] A. L. Cechin, D. Regina, P. Simon, and K. Stertz, “State automata\nextraction from recurrent neural nets using k-means and fuzzy clustering,”\nin Proceedings of the 23rd International Conference of the Chilean\nComputer Science Society (SCCC). IEEE, 2003, pp. 73–78.\n[41] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional\nnetworks,” in European Conference on Computer Vision. Springer, 2014,\npp. 818–833.\n[42] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M ¨uller, and\nW. Samek, “On pixel-wise explanations for non-linear classiﬁer decisions\nby layer-wise relevance propagation,” PloS ONE, vol. 10, no. 7, pp. 130\n– 140, 2015.\n[43] M. T. Ribeiro, S. Singh, and C. Guestrin, ““Why should I trust you?”\nExplaining the predictions of any classiﬁer,” in Proceedings of the 22nd\nACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, 2016, pp. 1135–1144.\n1014\n[44] L. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling, “Visualizing\ndeep neural network decisions: Prediction difference analysis,” in 5th\nInternational Conference on Learning Representations (ICLR), 2017.\n[45] G. Montavon, S. Lapuschkin, A. Binder, W. Samek, and K.-R. M ¨uller,\n“Explaining nonlinear classiﬁcation decisions with deep Taylor decompo-\nsition,” Pattern Recognition, vol. 65, pp. 211–222, 2017.\n[46] P. W. Koh and P. Liang, “Understanding black-box predictions via\ninﬂuence functions,” in International Conference on Machine Learning.\nPMLR, 2017, pp. 1885–1894.\n[47] A. Shahroudnejad, “A survey on understanding, visualizations, and\nexplanation of deep neural networks,” arXiv preprint arXiv:2102.01792,\n2021.\n[48] A. Dhurandhar, P. Chen, R. Luss, C. Tu, P. Ting, K. Shanmugam,\nand P. Das, “Explanations based on the missing: Towards contrastive\nexplanations with pertinent negatives,” in Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neural Information\nProcessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr ´eal,\nCanada, 2018, pp. 590–601.\n[49] C. W. Omlin and C. L. Giles, “Extraction of rules from discrete-time\nrecurrent neural networks,” Neural Networks, vol. 9, no. 1, pp. 41–52,\n1996.\n[50] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart,\n“Privacy in pharmacogenetics: An end-to-end case study of personalized\nWarfarin dosing,” in 23rd USENIX Security Symposium (USENIX\nSecurity), 2014, pp. 17–32.\n[51] X. Liu, L. Xie, Y . Wang, J. Zou, J. Xiong, Z. Ying, and A. V . Vasilakos,\n“Privacy and security issues in deep learning: A survey,” IEEE Access,\nvol. 9, pp. 4566–4593, 2021.\n[52] McAfee, “Grand theft data – Data exﬁltration study: Actors, tactics, and\ndetection,” 2015.\n[53] P. Kocher, J. Horn, A. Fogh, D. Genkin, D. Gruss, W. Haas, M. Hamburg,\nM. Lipp, S. Mangard, T. Prescher et al., “Spectre attacks: Exploiting\nspeculative execution,” in 2019 IEEE Symposium on Security and Privacy\n(SP). IEEE, 2019, pp. 1–19.\n[54] S. Sodagudi and R. R. Kurra, “An approach to identify data leakage in\nsecure communication,” in Proceedings of 2nd International Conference\non Intelligent Computing and Applications. Springer, 2017, pp. 31–43.\n[55] C. Warzel, “Faceapp shows we care about privacy but don’t understand\nit,” https://www.nytimes.com/2019/07/18/opinion/faceapp-privacy.html,\n(Accessed on 03/11/2021).\n[56] R. J. Bolton, D. J. Hand et al., “Statistical fraud detection: A review,”\nStatistical science, vol. 17, no. 3, pp. 235–255, 2002.\n[57] R. Shokri, M. Stronati, C. Song, and V . Shmatikov, “Membership\ninference attacks against machine learning models,” in 2017 IEEE\nSymposium on Security and Privacy (SP). IEEE, 2017, pp. 3–18.\n[58] A. Sablayrolles, M. Douze, C. Schmid, Y . Ollivier, and H. J´egou, “White-\nbox vs black-box: Bayes optimal strategies for membership inference,”\nin International Conference on Machine Learning. PMLR, 2019, pp.\n5558–5567.\n[59] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in\nmachine learning: Analyzing the connection to overﬁtting,” in 2018\nIEEE 31st Computer Security Foundations Symposium (CSF). IEEE,\n2018, pp. 268–282.\n[60] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks\nthat exploit conﬁdence information and basic countermeasures,” in\nProceedings of the 22nd ACM SIGSAC Conference on Computer and\nCommunications Security (CCS), 2015, pp. 1322–1333.\n[61] X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton, “A methodology\nfor formalizing model-inversion attacks,” in 2016 IEEE 29th Computer\nSecurity Foundations Symposium (CSF). IEEE, 2016, pp. 355–370.\n[62] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing\nmachine learning models via prediction APIs,” in 25th USENIX Security\nSymposium (USENIX Security), 2016, pp. 601–618.\n[63] T. Orekondy, B. Schiele, and M. Fritz, “Prediction poisoning: Towards\ndefenses against DNN model stealing attacks,” in 8th International\nConference on Learning Representations, (ICLR), 2020.\n[64] M. Yan, C. W. Fletcher, and J. Torrellas, “Cache telepathy: Leveraging\nshared resource attacks to learn DNN architectures,” in 29th USENIX\nSecurity Symposium (USENIX Security). USENIX Association, Aug.\n2020, pp. 2003–2020.\n1015",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.835425615310669
    },
    {
      "name": "Perplexity",
      "score": 0.8339929580688477
    },
    {
      "name": "Memorization",
      "score": 0.8230602145195007
    },
    {
      "name": "Language model",
      "score": 0.7334823608398438
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6105427742004395
    },
    {
      "name": "Artificial neural network",
      "score": 0.5832010507583618
    },
    {
      "name": "Machine learning",
      "score": 0.5621750950813293
    },
    {
      "name": "Data modeling",
      "score": 0.45293480157852173
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.44932231307029724
    },
    {
      "name": "Natural language processing",
      "score": 0.34910356998443604
    },
    {
      "name": "Database",
      "score": 0.13737034797668457
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210115038",
      "name": "Huawei Technologies (Canada)",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I60158472",
      "name": "Concordia University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210166625",
      "name": "Huawei German Research Center",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210158308",
      "name": "Queen Medical",
      "country": "QA"
    }
  ],
  "cited_by": 8
}