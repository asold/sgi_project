{
  "title": "PyTorrent: A Python Library Corpus for Large-scale Language Models",
  "url": "https://openalex.org/W3202092434",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101407943",
      "name": "Mehdi Bahrami",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024144640",
      "name": "N. C. Shrikanth",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014764093",
      "name": "Shade Ruangwan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100349557",
      "name": "Lei Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074188123",
      "name": "Yuji Mizobuchi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5033040594",
      "name": "Masahiro Fukuyori",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5056336684",
      "name": "Weipeng Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5060738878",
      "name": "Kazuki Munakata",
      "affiliations": [
        "North Carolina State University"
      ]
    },
    {
      "id": "https://openalex.org/A5077008083",
      "name": "Tim Menzies",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2605887895",
    "https://openalex.org/W2016027000",
    "https://openalex.org/W1530010412",
    "https://openalex.org/W2146338950",
    "https://openalex.org/W2731935965",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2142403498",
    "https://openalex.org/W3106502818",
    "https://openalex.org/W2950500938",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3120740533",
    "https://openalex.org/W2973529529",
    "https://openalex.org/W2795866244",
    "https://openalex.org/W2794601162",
    "https://openalex.org/W3093604544",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W1983693548",
    "https://openalex.org/W3000045849",
    "https://openalex.org/W1964962870",
    "https://openalex.org/W3207433277",
    "https://openalex.org/W2976890614",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W3008088841",
    "https://openalex.org/W3091798252",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2978104058",
    "https://openalex.org/W2512848817"
  ],
  "abstract": "A large scale collection of both semantic and natural language resources is essential to leverage active Software Engineering research areas such as code reuse and code comprehensibility. Existing machine learning models ingest data from Open Source repositories (like GitHub projects) and forum discussions (like Stackoverflow.com), whereas, in this showcase, we took a step backward to orchestrate a corpus titled PyTorrent that contains 218,814 Python package libraries from PyPI and Anaconda environment. This is because earlier studies have shown that much of the code is redundant and Python packages from these environments are better in quality and are well-documented. PyTorrent enables users (such as data scientists, students, etc.) to build off the shelf machine learning models directly without spending months of effort on large infrastructure. The dataset, schema and a pretrained language model is available at: https://github.com/fla-sil/PyTorrent",
  "full_text": "PyTorrent: A Python Library Corpus for Large-scale\nLanguage Models\nMehdi Bahrami1 N.C. Shrikanth2 Shade Ruangwan3 Lei Liu1 Yuji Mizobuchi3\nMasahiro Fukuyori3 Wei-Peng Chen1 Kazuki Munakata3\nTim Menzies2\n1Fujitsu Research of America, Sunnyvale, CA, USA\n2North Carolina State University, Raleigh, NC, USA\n3Fujitsu Research Ltd., Kawasaki, Japan\n{mbahrami,ruangwan.shade,mizobuchi.yuji}@fujitsu.com\n{lliu,wchen,fukuyori,munakata.kazuki}@fujitsu.com\nsnaraya7@ncsu.edu timm@ieee.org\nAbstract\nA large scale collection of both semantic and natural language resources is essential\nto leverage active Software Engineering research areas such as code reuse and code\ncomprehensibility. Existing machine learning models ingest data from Open Source\nrepositories (like GitHub projects) and forum discussions (like Stackoverﬂow.com),\nwhereas, in this showcase, we took a step backward to orchestrate a corpus titled\nPyTorrent that contains 218,814 Python package libraries from PyPI and Anaconda\nenvironment. This is because earlier studies have shown that much of the code is\nredundant and Python packages from these environments are better in quality and\nare well-documented. PyTorrent enables users (such as data scientists, students,\netc.) to build off the shelf machine learning models directly without spending\nmonths of effort on large infrastructure. The dataset, schema and a pretrained\nlanguage model is available at: https://github.com/fla-sil/PyTorrent\n1 Introduction\nPresently Python is one of the most popular [12] programming language (as seen in GitHub, Stack-\nOverﬂow, etc.). Numerous lines of codes are churned and accumulated in version control systems\nevery day. Thus, if we can harness and organize ‘documented’ Python code accumulated over these\nyears, we could better attempt active Software Engineering (SE) problems in the lines of code reuse\nand code comprehensibility.\nAt ICSE 2012, Hindle et al. [28] conjectured that “..most software is also natural, in the sense that\nit is created by humans at work, with all the attendant constraints and limitations - and thus, like\nnatural language, it is also likely to be repetitive and predictable”. However, re-usability is beyond\njust the availability of data but the arduous task of searching through numerous artifacts to ﬁnd the\nmost relevant one for the target user (For example, Code Search [26]).\nUnderstanding of a programming language relies on two pillars of source-code mining: i) the source-\ncode of existing libraries and ii) the usage source-code of the existing libraries. Often researchers\nfollow standard natural language techniques to generate a language model for programming languages\nPreprint. Under review.\narXiv:2110.01710v1  [cs.SE]  4 Oct 2021\nTable 1: Comparison of existing datasets\nDataset # of Files Lines of Codes # of Functions\nCurated Python [38] Unknown Unknown Unknown\nBOA [21] 4,977,680 Unknown Unknown\nPublic Git Archive [34] 54.5 x 106 15,941 x 106 Unknown\nCodeSearchNet [29] Unknown Unknown 2,273,157\nJUICE [18] 659,000 Unknown 1,521,774\nPyMT5 [22] 2.3 x 106 Unknown 2.6 x 107\nPyTorrent (proposed) 4,058,349 655,074,611 2,841,446\n(e.g., by processing a sheer number of library usages). However, programming languages are more\nrestricted than natural languages (e.g., English), and the majority of code usage follows existing\nlibraries. For instance, a model can learn about PyTorch [39] package from processing a sheer number\nof PyTorch code usages (existing approaches, i.e., mining GitHb projects that imported PyTorch) or\nmining source-code of the PyTorch package (proposed dataset).\nTo that end we orchestrate a data set titled ‘PyTorrent’ that is made available public here [9],[10]\nand [8]1.\nA combination approach of mining both libraries and their usage, may improve the result with more\nprecise outputs. In particular, Python programming language mainly relies on existing libraries where\nit allows users to easily install (i.e., using \"pip install package\"for PyPI packages and \"anaconda\ninstall package\"for Anaconda).\nBackground and Related Work. Several datasets for Python programming language have been\nreleased in recent years by processing GitHub repositories [38, 21, 34, 29, 18, 22]. The early works\nfocus on particular use cases, and the dataset is relatively small. In [38], a dataset of metrics taken\nfrom a curated collection on 51 popular software systems in Python has been published. The internal\nstructure of each system is investigated, and metadata is collected (i.e., , including system identiﬁer,\ndescription, etc. In another study, Biswas et al. [21] create a dataset that includes 1,558 mature GitHub\nprojects for data science tasks. The Abstract Syntax Tree (AST) of parsed Python programs from each\nrevision along with the metadata are stored in the dataset. More recently, larger datasets have been\ngenerated. In [34], the Public Git Archive has been released. In [29], the CodeSearchNet collects a\ncorpus from publicly available open-source non-forked GitHub repositories and generates pairs of\n(comment, code). The schema of the corpus includes a limited number of ﬁelds of repository name,\npath of repository, function name, code, code tokens, docstring, docstring tokens, and programming\nlanguage etc. The JUICE dataset presented in [18] which collects publicly available Jupyter notebooks\nfrom GitHub and ﬁlter the notebooks having natural language markdown in English and Python as\ntheir kernel type. The JUICE dataset is composed of target cells that include at most one method\ndeﬁnition with syntactically valid Python codes. In [22], the PyMT5 dataset is presented by collecting\n112K GitHub repositories. This method-docstring corpus includes the ﬁle level AST for each Python\nﬁle, extracting every individual and class method.\nAlthough there are several datasets for Python programming language available, as aforementioned,\nthese datasets mainly focus on GitHub projects and ignore the existing package libraries and their\nmetadata (i.e., package name, package license, supported Python version, and package descriptions),\nwhich is one of the differences to our proposed PyTorrent dataset. Table 1 summarizes the quantity\ncomparison between our dataset and previous datasets. We focus on generating a dataset of the pair\nof natural language description (NL) and functions/methods (PL), for existing Python libraries. The\npairs can be used for ﬁne-tuning downstream tasks, such as code retrieval [26], code generation [44]\ntasks and etc.\n2 Methodology\nDataset Description. Our goal was to mine voluminous <PL,NL> (Source code, Natural language\ntext) pairs (see Figure 1) to promulgate SE research. Our focus was to orchestrate a corpus that\nis compliant with existing datasets for machine programming language models and, in particular,\n1PyTorrent Dataset: https://doi.org/10.5281/zenodo.4451357\n2\nTable 2: The Number of packages per minimum number of releases\nThreshold # of Packages Compressed Size\n1 218,814 100.8 GB (379 GB raw)\n2 129,710 74.85 GB\n5 76,267 55.88 GB\n10 40,692 47.92 GB\n50 4,296 13.26 GB\ntransformer based models [43]. Therefore, we choose the schema of CodeSearchNet dataset [1] as\nbase schema and generates PyTorrent dataset according to the deﬁned schema.\nHowever, we differ from CodeSearchNet in three ways. Firstly, Unlike CodeSearchNet, our data\nsource is from Python libraries from PyPI and Anaconda packages rather than GitHub projects.\nSecondly, CodeSearchNet builds machine learning models only based on the top-level docstring\ndescription. In our case, we generate three augmented datasets: i) top-level docstring, ii) added user\ncomments, and iii) added both full-docstring and user comments where we associate Python methods\nwith the entire docstring description and extract developer comments as marked in Figure 1.\nLastly, the two primary attributes in CodeSearchNet schema are docstring_tokens and code_tokens.\nThe docstring_tokens act as a placeholder for natural text where developer comments and (or)\ndocstrings can be tokenized and assigned, correspondingly tokenized source code can be assigned to\ncode_tokens. Then one can build a machine learning model using the aforementioned architectures\nlike CodeSearchNet. Some attributes like repo, path, and URL are set to Python package name,\nPython script path. For the ﬁrst time, researchers may use package name in the PyTorrent dataset to\ndeﬁne a cross join relation between pairs and the corresponding package metadata. We also added\ndocstring_summary. The detail schema can be found in PyTorrent GitHub repository [10].\nData Extraction. The ﬁrst step of producing PyTorrent is collecting metadata of existing libraries\nfrom Python package distributions that include PyPI [6] and Anaconda [3]. Each Python package\ndistribution offers different metadata of each software package. We utilize Scrapy [ 30] to crawl\npackages through PyPI API and Anaconda API. Our crawler collects and generates an index of Python\nlibrary packages for PyTorrent. The native API offers only raw text version of metadata. Therefore,\nwe use Scrapy to collect enriched HTML package descriptions. We published the ﬁnal updated\nschema and all packages that include PyPI package metadata schema [7], Anaconda package metadata\nschema and all harvested metadata of packages [10]. We construct 238,187 package metadata [10].\nOnce we collect the package metadata, we collect the latest version source-code of each package to\nconstruct PyTorrent. The web crawler collected 218,814 packages with source-codes.\nThe collected decompressed raw ﬁles include 10,449,240 ﬁles for all Python software packages as of\nDecember 2020. The raw collection size is 100+ GB of compressed data and 379 GB of decompressed\n(including 62,570 different ﬁle extension) of raw data which includes 4,058,349 Python scripts with\nthe size of 30 GB. Each package of PyTorrent in metadata includes the number of releases. We use a\nminimum number of releases as a threshold to list the number of packages in each category, which\nis shown in Table 2. As the statistical shown, a large number of Python packages (4,296) with the\nhighest updates (i.e., threshold=50) may indicate the most popular and well-documented packages.\nFor generating PyTorrent, we consider only the latest package release source-codes.\nData Pre-processing. A typical example of a well-documented method in a library is shown in\nFigure 1. While most studies such as [ 29, 25] only consider the ‘Top-level docstring’, we believe\nthe rich source of additional information could be utilized to strengthen machine learning models.\nInformation beyond just top-level docstring is mostly available as part of any well-documented\ncode. Typically it includes information about exceptions, return types, and most important, usage\ninformation (example). Further, the method body would include developer comments describing the\npurpose of the code written in natural language succinctly. Note that although the docstring is written\nin the natural language, it is contained within some templates. Docstring can be written in numerous\nstyles like Google, NumPy/SciPy, reStructured Text, and Epytext. On the other hand, developer\ncomments are unstructured similar to google search queries that may not adhere to a syntax (like\nStructured Query Language). We presume it would help machine learning models easily retrieve\ncode when trained with such natural language (developer comments) artifacts, especially when the\ninput is a similar natural language search query.\n3\nTop-level docstring\nExample\nDeveloper comment\nAdditional information\nFigure 1: Snapshot of a well documented method in pandas library\n2\nEach package is decompressed and process each script of packages. We use all packages with\nThreshold = 1to generate the ﬁnal dataset.\nEach package may include a set of Python scripts and other resources (i.e., test cases, readme ﬁle).\nWe process only Python scripts.\nDataset Construction. In a nutshell, all core Python scripts (as deﬁned in Table 4) are parsed and\nstored in a JSONL [5] format. First, we extract all functions (both class level and script level) and\ntheir associated docstrings. Then, we extract developer comments and their associated source codes\nwithin the scripts (including lines of code within each function).\nWe have three objectives here (refer to Figure 1):\n• To store a Python class level function or script level function and its associated docstring.\n• To store source-code with its associated developer comments (as part of PL at every level),\nsuch as:\n– Code that is part of the script (not bound by any class or function)\n– Code that is within a function (class or script level function)\n– The function itself (class or script level)\nTo achieve this, we take advantage of the Abstract Syntax Tree (AST) and source-code line number\ninformation to associate the appropriate developer comments or docstrings with its associated source\ncodes. For each Python script, we do the following.\n• Visit each Python function (both script3 level and class 4 level) and store its raw source code\nand its docstring. Then, we visit each of these functions to identify a line that begins with #\n(to identify developer comment). Next, we associate all source code under that developer\ncomment part of the method.\n3Python function part of the script but does not belong to any class\n4Python function that is part of a class within that script\n4\n• Using source code line number information, we ignore all the functions that are processed\nearlier and consider the rest of the code in the Python script to do the following: i) We\nidentify a line that begins with # (to identify developer comment); ii) Next, we associate all\nsource code under that developer comment part of the method.\nAll the pairs parsed from the Python scripts are written to disk into a JSON format as they are visited\nin each of the packages. To make dataset compliance with other related language model datasets for\nPython, we saved contents as JSON Lines text format [5] and split into three folders of valid, test\n(284,145, 284,144 pairs) and train(2,273,157 pairs), the pairs in each folder split into 20 chunks and\ncompressed through gzip. In order to save dataset as a single ﬁle for archiving dataset, we use gzip.\nWe generate three compressed ﬁle for three different scenarios of UserComments, added Docstrings\nand both docstrings and user comments. The size of each compressed ﬁle is ˜4.1 GB [9] and the size\nof each decompressed (JSONL) ﬁle is 43 GB[9] [10].\nWe constructed models using ﬁve scenarios listed in Table 3. Data ﬁltering. Each extracted pairs of\nTable 3: 5 Augmented-Code Scenarios (ACS) to build code retrieval models (X =⇒Y ) in Code-\nSearchNet and CodeBERT architectures (ACS=0 is the default case for CodeSearchNet and Code-\nBERT, whereas our results endorse ACS=4).\nACS X (attribute:docstring_tokens) Y (attribute:code_tokens)\n0 (default) Tokenized short description of doc-\nstring\nTokenized (code and code com-\nments)\n1 Tokenized code comments Tokenized code\n2 Tokenized (code comments and en-\ntire docstring)\nTokenized code\n3 Tokenized (code comments, entire\ndocstring and commit message)\nTokenized (code and code com-\nments)\n4 Tokenized (code comments and en-\ntire docstring)\nTokenized (code and code com-\nments)\n5 Tokenized short description of doc-\nstring\nTokenized code\n<PL,NL> can be categorized in one of the four listed in Table 4. The table shows all categories of\nsource-code and their number of pairs. We came up with these categories after inspecting a small\nportion of the extracted Python scripts. This categorization primarily helps ﬁlter noisy scripts in our\ndataset (ﬁles like __init__.py) that do not contain reusable code. Secondly, researchers may build\nfocused machine learning models. For example, to write test cases, one may create a Code Search\nmachine model only using scripts part of theTest (identiﬁed if the script path contains any case of test)\ncategory. Scripts speciﬁcally such as setup.py and make.py are categorized as Other. The remaining\nPython scripts (82%) do not ﬁt in the three categories is placed in the core category. PyTorrent\nis orchestrated based on the selected scripts in the Core category that has content of function and\ndocstrings with a total of 2,841,446 pairs.\nData Insights. To give a general idea of PyTorrent, we perform preliminary analysis on its source\ncodes and docstrings. In particular, we calculate code-related metrics from the collected source\ncodes using Radon library [15]. We show the number of lines and words for docstrings. From\nselected 2,925,660 functions in PyTorrent, we ﬁnd that on average, source codes have McCabe’s\ncomplexity[35] of 3 with a median of 1. Docstrings, on average, contains 6 lines (median 3) and\n31 words (median 15). In addition, we ﬁnd that 25% of those functions use Python decorator. Our\nobservation of PyTorrent shows that the average comment length of all pair is 47 characters, an\nTable 4: Number of pairs per source-code category\nCode Category # of Pairs\nCore 11,324,635\nTest 1,632,686\nInit 635,108\nOther 233,218\n5\nTable 5: Percentile length coverage per NL & PL (whole PyTorrent)\nPercentile NL PL\n0.50 9 44\n0.70 11 77\n0.80 12 105\n0.90 14 165\n0.95 16 243\nFigure 2: Percentile length of LoC for a coverage of 66,528 sample scripts\naverage code length is 228 characters, and 228,426 of pairs include examples (i.e., Tox-based [13]).\nTable 5 shows the detail of a percentile of length of NL and PL separately for the whole dataset.\nAlthough the average length of Line of Code (LoC) is 47 in all PyTorrent packages, we investigate it\nwith more precision on a portion of the PyTorrent dataset for 66,528 sample raw Python scripts. The\nsample data includes 9,951,811 LoC, and the average number of LoC in each script includes 149.\nIn this sample dataset, we observe the length of LoC with an average of 44.92 (which is very close\nto the whole dataset with the length of 47). Interestingly, we observe that selecting a length of 50\nand 70 covers 61,854 (%86) and 65,656 (%99) of all sample records without truncation, respectively.\nThe detail of percentile coverage is shown in Figure 2. Therefore, base on the statistic results we\nrecommend that researchers safely truncate a LoC Python programming language with a much\nsmaller length (i.e., 70)in comparison to natural language models (i.e., 512). Selecting a smaller size\nallows the researcher to generate a smaller tokenizer and less LoC encoder size to save computation\nresources and much speedy processing on both ﬁne-tuning and inferring from a BERT-based language\nmodel [23].\nSource Code. Notably, we collected the data on two 2.20GHz Intel CPUs having 56 cores with 32\nGB RAM. The artifacts of PyTorrent include the schema of raw packages, package metadata, [ 10] a\ncompliant pair of <NL,PL> dataset [9], a preliminary transformer-based DistilBERT model [8] have\nbeen published.\n3 PyTorrent: Why & How ?\nWhy? Most of the tedious work lies in preparing the data before insights can be mined [40]. Any data\nshowcase’s primary objective is to enable users to readily build data models (similar to UCI Machine\nLearning Repositories datasets [ 19]). There are two phases in the construction of the PyTorrent\ndataset. In the ﬁrst phase, We collected all Python software packages that include 218,814 modules.\nIn the second phase, we visited every Python script within each of the mined modules to extract\nPython methods, code snippets, and associated documentations. We spent 12 weeks of effort in web\ncrawling, data extraction, and pre-processing.\nHow? This section describe steps to build a CodeSearchNet architecture model [29] with PyTorrent\ndataset.\ni) Setup the CodeSearchNet [29] environment or similar environment; ii) The PyTorrent is a vast\nJSON ﬁle that split into 20 chunks; iii) dataset includes train (80%), test (10%), and valid (10%)\nfolders and place them in the corresponding folder within the CodeSearchNet environment; iv) One\nshould be now able to build CodeSearch deep learning models with an issue of a simple command.\n6\nFor example, the following CodeSearchNet command python train.py −− model neuralbow\ntrains a Neural Bag of Words model based on the PyTorrent.\nThe steps above show how easily PyTorrent corpus can put to the test in practice (but not limited to\nCodeSearch).\n4 Potential Research Applications\nConjecture. Code Search, Code Generation, Defect Prediction, and Program Repair are some of\nthe prevalent techniques in the Software Engineering (SE) space aimed to accelerate developer\nproductivity and software quality. In Code search [ 26, 29, 25] the problem is to accurately map a\nnatural language query with a relevant source code; such a technique typically integrated into an IDE\n(e.g., IntelliSense [16]) to improve developer productivity (writing more lines of code in less time).\nCode generation [27] is similar to code search problem, but instead of mapping a query with relevant\nsource code, a language model generates source code according to a query with its own knowledge\nlearned during the training process. Defect prediction is a decades-old and active research space\nwhere the problem is to accurately predict defects [24, 36]. But post prediction, another functional\nresearch space, namely automated program repair [37, 32] attempt to patch appropriate ﬁxes to reduce\ndeveloper debugging time. PyTorrent contains voluminous source code information extracted from\nwidely used modules (such as pandas [ 14], and Pytorch) that we believe would accelerate the SE\nresearch.\nWe presume these widely used packages are well tested. Hence source code snippets from those\npackages can be suitable candidates to patch (repair) defective code (i.e., code that fails some test).\nUsage. Since we generate a compliant dataset with a recent programming language models’ dataset,\nPyTorrent can be easily plugin into CodeSearchNet [ 29], CodeBERT [25], CodeXGLUE [33] or\nGraphCodeBERT [31] for training or ﬁne-tuning a machine programming language models or ﬁne-\ntuning on a downstream task. For instance, ﬁrst download dataset from one of the augmented\ncode datasets which is available here [ 9], or [10]. Then, i) to train a CodeSearchNet model, add\ndownloaded PyTorrent dataset to ‘/resources/data’and run training script as explained in [ 2]; ii)\nto ﬁne-tune a CodeBERT model, add downloaded PyTorrent dataset to‘/data/codesearch‘and run\nﬁne-tuning script as explain in [4]. The similar approach can be used for ﬁne-tuning CodeXGLUE\nand GraphCodeBERT. We use PyTorrent to train a preliminary DistilBERT-Masked Language\nModeling(MLM) [41] model from scratch. The trained model, along with the dataset, aims to\nhelp researchers to easily and efﬁciently work on a large dataset of Python packages using only\n5 lines of codes to load the transformer-based model. We use 1M raw Python scripts of PyTorrent\nthat includes 12,350,000 LOC to train the model. We also train a byte-level Byte-pair encoding\n(BPE) [42] tokenizer that includes 56,000 tokens, which is truncated LOC with the length of 50 to\nsave computation resources as explain in Section 2. We published the transformer-based model at\nHuggingFace Hub as PyTorrent(v1) [8].\nAnother application of PyTorrent dataset lies in code generation where developers input natural\nlanguage description of code. The model then generates an executable source code according to the\ndescription. To train aforementioned model, many pairs of <NL,PL> preferably high on quality are\nneeded. A code generation model can potentially support developers to shorten development time\nand help newcomers in learning from good examples. To explore this possibility, we use PyTorrent to\nﬁne-tune the pre-trained CodeBERT [4]. The results are promising especially in multi-lines code\ngeneration.\nThe dataset can be used to produce an augmented programming language[20] where it allows the\ndataset to be extended for ﬁne-tuning a model on a downstream task, such as code generation, code\nsearch and etc.\n5 Conclusion\nIn this paper, we introduce a new dataset of Python programming language (PyTorrent) that mined\n218,814 Python library packages source-codes. The dataset includes different augmented code\nscenarios where it allows both practitioners and researchers to efﬁciently plugin the dataset for\n7\ntraining and ﬁne-tuning a language models. We published ii) metadata of all packages, ii) three\naugmented code scenarios, and iii) a pre-trained transformer model [11]. iii) a preliminary results on\ntwo downstream tasks of code retrieval and code generation tasks.\n6 Limitations and Future Work\nAbdalkareem et al. [17] recently found that around 10% to 16% of the packages within the prevalent\npackage managers are trivial. Trivial packages are largely preferred by software engineers, and their\nutility is observed in large organizations, including Facebook and Netﬂix. Abdalkareem et al. deﬁne\na trivial package as a package with ≤35 LOC and McCabe’s cyclomatic complexity≤10. Further,\nthere are other package management platforms like npm (JavaScript) with a large developer audience.\nThus a natural extension of this work would be to mine npm packages and analyze trivial packages in\nall 218,814 packages we mined.\nReferences\n[1] Codesearchnet schema format. https://github.com/github/CodeSearchNet#schema--format,\n2020. Accessed: 1/18/2021.\n[2] CodeSearchNet training script. https://github.com/github/CodeSearchNet#training, 2021. Ac-\ncessed: 1/9/2021.\n[3] Anaconda package distribution. https://docs.anaconda.com/anaconda/packages/, 2021. Ac-\ncessed: 1/18/2021.\n[4] A script for ﬁne-tuning a CodeBERT model. https://github.com/microsoft/CodeBERT#\nfine-tune, 2021. Accessed: 1/9/2021.\n[5] Json lines text format. https://jsonlines.org/, 2021. Accessed: 1/28/2021.\n[6] Pypi package distribution. https://tox.readthedocs.io/en/latest/, 2021. Accessed: 1/18/2021.\n[7] Updated PyPI schema. https://github.com/fla-sil/PyTorrent/blob/main/schema.json,\n2021. Accessed: 1/9/2021.\n[8] Pytorrent transformer-based model. https://huggingface.co/Fujitsu/pytorrent, 2021. Accessed:\n1/29/2021.\n[9] Pytorrent datasets. https://doi.org/10.5281/zenodo.4451357, 2021. Accessed: 1/29/2021.\n[10] Pytorrent github page. https://github.com/fla-sil/PyTorrent, 2021. Accessed: 1/29/2021.\n[11] Pre-trained pytorrent language model at huggingface. https://huggingface.co/Fujitsu/\npytorrent, 2021. Accessed: 9/30/2021.\n[12] Interactive the top programming-languages tools 2020, ieee. https://spectrum.ieee.org/static/\ninteractive-the-top-programming-languages-2020 , 2021. Accessed: 1/19/2021.\n[13] Tox automation project. https://tox.readthedocs.io/en/latest/, 2021. Accessed: 1/18/2021.\n[14] Pydata website. https://pandas.pydata.org, 2021. Accessed: 1/18/2021.\n[15] Pypi radon project. https://pypi.org/project/radon, 2021. Accessed: 1/18/2021.\n[16] Visualstudio website. https://code.visualstudio.com/docs/editor/intellisense, 2021. Ac-\ncessed: 1/18/2021.\n[17] Rabe Abdalkareem, Vinicius Oda, Suhaib Mujahid, and Emad Shihab. On the impact of using trivial\npackages: An empirical case study on npm and pypi. Empirical Software Engineering, 25(2):1168–1204,\n2020.\n[18] Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. Juice: A large scale distantly supervised dataset for\nopen domain context-based code generation. arXiv preprint arXiv:1910.02216, 2019.\n[19] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.\n8\n[20] Mehdi Bahrami, N C Shrikanth, Yuji Mizobuchi, Lei Liu, Masahiro Fukuyori, Wei-Peng Chen, and Kazuki\nMunakata. Augmentedcode: Examining the effects of natural language resources in code retrieval models.\narXiv, 2021.\n[21] Sumon Biswas, Md Johirul Islam, Yijia Huang, and Hridesh Rajan. Boa meets python: a boa dataset of\ndata science software in python language. In 2019 IEEE/ACM 16th International Conference on Mining\nSoftware Repositories (MSR), pages 577–581. IEEE, 2019.\n[22] Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan.\nPymt5: multi-mode translation of natural language and python code with transformers. arXiv preprint\narXiv:2010.03150, 2020.\n[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[24] Marco D’Ambros, Michele Lanza, and Romain Robbes. Evaluating defect prediction approaches: a\nbenchmark and an extensive comparison. Empirical Software Engineering, 17(4):531–577, 2012.\n[25] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. arXiv\npreprint arXiv:2002.08155, 2020.\n[26] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Deep code search. In2018 IEEE/ACM 40th International\nConference on Software Engineering (ICSE), pages 933–944. IEEE, 2018.\n[27] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and Trends® in\nProgramming Languages, 4(1-2):1–119, 2017.\n[28] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness of\nsoftware. In 2012 34th International Conference on Software Engineering (ICSE), pages 837–847. IEEE,\n2012.\n[29] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet\nchallenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.\n[30] Dimitrios Kouzis-Loukas. Learning scrapy. Packt Publishing Ltd, 2016.\n[31] Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei Ma, Fangli Xu, Alex X Liu, Chunming\nWu, and Shouling Ji. Deep graph matching and searching for semantic code retrieval. arXiv preprint\narXiv:2010.12908, 2020.\n[32] Xuliang Liu and Hao Zhong. Mining stackoverﬂow for program repair. In 2018 IEEE 25th International\nConference on Software Analysis, Evolution and Reengineering (SANER), pages 118–129. IEEE, 2018.\n[33] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark dataset for code\nunderstanding and generation. arXiv preprint arXiv:2102.04664, 2021.\n[34] Vadim Markovtsev and Waren Long. Public Git archive: A big code dataset for all. In Proceedings of the\n15th International Conference on Mining Software Repositories, pages 34–37, 2018.\n[35] Thomas J. McCabe. A complexity measure. IEEE Transactions on Software Engineering, SE-2(4):\n308–320, 1976.\n[36] Jaechang Nam, Wei Fu, Sunghun Kim, Tim Menzies, and Lin Tan. Heterogeneous defect prediction. IEEE\nTransactions on Software Engineering, 44(9):874–896, 2017.\n[37] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. Semﬁx: Program\nrepair via semantic analysis. In 2013 35th International Conference on Software Engineering (ICSE),\npages 772–781. IEEE, 2013.\n[38] Matteo Orrú, Ewan Tempero, Michele Marchesi, Roberto Tonelli, and Giuseppe Destefanis. A curated\nbenchmark collection of python systems for empirical studies on software engineering. In Proceedings\nof the 11th International Conference on Predictive Models and Data Analytics in Software Engineering,\npages 1–4, 2015.\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. arXiv preprint arXiv:1912.01703, 2019.\n9\n[40] Dorian Pyle. Data preparation for data mining. morgan kaufmann, 1999.\n[41] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n[42] Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. Character: Translation edit rate\non character level. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task\nPapers, pages 505–510, 2016.\n[43] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art\nnatural language processing. ArXiv, pages arXiv–1910, 2019.\n[44] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. arXiv\npreprint arXiv:1704.01696, 2017.\n10",
  "topic": "Python (programming language)",
  "concepts": [
    {
      "name": "Python (programming language)",
      "score": 0.9003978967666626
    },
    {
      "name": "Computer science",
      "score": 0.8287558555603027
    },
    {
      "name": "Reuse",
      "score": 0.6509872674942017
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5825527310371399
    },
    {
      "name": "Programming language",
      "score": 0.5699695348739624
    },
    {
      "name": "Source code",
      "score": 0.5583904981613159
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.4784417450428009
    },
    {
      "name": "Open source",
      "score": 0.4776019752025604
    },
    {
      "name": "World Wide Web",
      "score": 0.4757349491119385
    },
    {
      "name": "Code reuse",
      "score": 0.4104984998703003
    },
    {
      "name": "Software",
      "score": 0.36617332696914673
    },
    {
      "name": "Software engineering",
      "score": 0.36258670687675476
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33091408014297485
    },
    {
      "name": "Information retrieval",
      "score": 0.23848038911819458
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I137902535",
      "name": "North Carolina State University",
      "country": "US"
    }
  ],
  "cited_by": 4
}