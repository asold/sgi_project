{
    "title": "Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses",
    "url": "https://openalex.org/W4389519911",
    "year": 2023,
    "authors": [
        {
            "id": null,
            "name": "Aysa Fan",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2105593178",
            "name": "Haoran Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2137571634",
            "name": "Luc Paquette",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A311940369",
            "name": "Rui Zhang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3126675481",
        "https://openalex.org/W3101007570",
        "https://openalex.org/W2172235535",
        "https://openalex.org/W2790195820",
        "https://openalex.org/W3196116495",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2071792242",
        "https://openalex.org/W4367694420",
        "https://openalex.org/W1972978214",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2165365113",
        "https://openalex.org/W1531374185",
        "https://openalex.org/W4317502083",
        "https://openalex.org/W4284664028",
        "https://openalex.org/W2129963590",
        "https://openalex.org/W2964223283",
        "https://openalex.org/W4283705032",
        "https://openalex.org/W3034383728",
        "https://openalex.org/W2041103998",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4211263275",
        "https://openalex.org/W4389519352",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W2610891036",
        "https://openalex.org/W2962717047",
        "https://openalex.org/W4296448535",
        "https://openalex.org/W2030538107",
        "https://openalex.org/W4321162272",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4379256276",
        "https://openalex.org/W2791566576",
        "https://openalex.org/W2962970841",
        "https://openalex.org/W2069032672",
        "https://openalex.org/W4361019423",
        "https://openalex.org/W2916082703",
        "https://openalex.org/W3167783161",
        "https://openalex.org/W3175537679",
        "https://openalex.org/W2949961827",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W4367000100",
        "https://openalex.org/W3136847194",
        "https://openalex.org/W2888302696",
        "https://openalex.org/W4385573161"
    ],
    "abstract": "In this paper, we explore the application of large language models (LLMs) for generating code-tracing questions in introductory programming courses. We designed targeted prompts for GPT4, guiding it to generate code-tracing questions based on code snippets and descriptions. We established a set of human evaluation metrics to assess the quality of questions produced by the model compared to those created by human experts. Our analysis provides insights into the capabilities and potential of LLMs in generating diverse code-tracing questions. Additionally, we present a unique dataset of human and LLM-generated tracing questions, serving as a valuable resource for both the education and NLP research communities. This work contributes to the ongoing dialogue on the potential uses of LLMs in educational settings.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7406–7421\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nExploring the Potential of Large Language Models in Generating\nCode-Tracing Questions for Introductory Programming Courses\nAysa Xuemo Fan1, Ranran Haoran Zhang2, Luc Paquette1, Rui Zhang2\n1 University of Illinois at Urbana-Champaign\n2 Penn State University\n{xuemof2, lpaq}@illinois.edu\n{hzz5361, rmz5227}@psu.edu\nAbstract\nIn this paper, we explore the application of\nlarge language models (LLMs) for generating\ncode-tracing questions in introductory program-\nming courses. We designed targeted prompts\nfor GPT4, guiding it to generate code-tracing\nquestions based on code snippets and descrip-\ntions. We established a set of human evalua-\ntion metrics to assess the quality of questions\nproduced by the model compared to those cre-\nated by human experts. Our analysis provides\ninsights into the capabilities and potential of\nLLMs in generating diverse code-tracing ques-\ntions. Additionally, we present a unique dataset\nof human and LLM-generated tracing ques-\ntions, serving as a valuable resource for both\nthe education and NLP research communities.\nThis work contributes to the ongoing dialogue\non the potential uses of LLMs in educational\nsettings1.\n1 Introduction and Background\nThe teaching of introductory programming courses\ncontinues to be a challenging endeavor, despite the\nglobal uptake and popularity of such courses. High\nenrollment rates often result in diverse student pop-\nulations, with a wide range of programming experi-\nence from those just starting their journey to others\nwith prior exposure (Lopez et al., 2008). Ensuring\nan effective learning experience that accommodates\nthis wide disparity presents a daunting task, making\nthe teaching of these courses complex.\nOne critical component in teaching introductory\nprogramming is the focus on code tracing, a skill\nidentified as instrumental in enhancing code writ-\ning abilities (Lister et al., 2009; Venables et al.,\n2009; Kumar, 2013). Current educational method-\nologies encourage code tracing through a variety of\nmeans, such as practice questionnaires (Lehtinen\net al., 2023), direct teaching strategies (Xie et al.,\n1Our data and code are available at https:\n//github.com/aysafanxm/llm_code_tracing_\nquestion_generation\nHuman-created Tracing\nQuestions:\nWhat are we printing?\nCode snippet:\n  int n = 365;\n  int sum = 0;\n  while (n > 0)\n    {\n       int digit = n % 10;\n       sum = sum + digit;\n       n = n/10;\n    }\n  System.out.println(sum);\nDescription: Identify the reverse\ndigits on a credit card.\nCollected Data\nLLM-generated Tracing Questions:\n1. Based on the given code, what will be\nthe output if the value of \n...\n8. If the requirement was to identify the\nreverse digits on a credit card and then\nreturn them as a single integer, how\nwould you modify the code to\naccomplish this?\nEvaluation\nAre the qualities of\nquetions comparable?\nWho creates more\ndiverse questions?\nWhich tracing\nquestions are better?\nCan experts distinguish between the\nauthors (LLM or human)?\nFigure 1: We aim to assess the Large Language Models’\n(LLMs’) capability to generate code tracing questions,\npivotal in computer science education. The accompa-\nnying illustration outlines our approach, where GPT4\nis employed to generate questions based on given code\nsnippets and descriptions. Subsequent comparative anal-\nysis with human-created questions aids in exploring\ncritical aspects, such as the quality and diversity of gen-\nerated questions, discernibility between human and AI\nauthors, and the relative superiority in question quality.\n2018), and tracing quizzes (Sekiya and Yamaguchi,\n2013). These strategies consistently utilize code-\ntracing questions aimed at fostering and developing\na student’s understanding and skills.\nHowever, the preparation of code-tracing ques-\ntions poses challenges. Manual question creation\nby instructors (Sekiya and Yamaguchi, 2013; Has-\nsan and Zilles, 2021) is time-consuming and lacks\nscalability. Automatic generation using program\nanalysis saves time, yet is limited by the analyzer’s\ncapabilities and lacks question diversity (Zavala\nand Mendoza, 2018; Thomas et al., 2019; Russell,\n2021; Lehtinen et al., 2021; Stankov et al., 2023).\n7406\nIn light of the increasing potential of Large Lan-\nguage Models (LLMs) in sectors like code summa-\nrization and explanation (Chen et al., 2021; Siddiq\net al., 2023), the question arises: Can LLMs gener-\nate high-quality code-tracing questions? Our study\nexplores this query using GPT4 (OpenAI, 2023),\nleveraging prompts to guide its question generation\nbased on given code snippets and descriptions. To\nassess the LLM’s capability in this pivotal aspect of\ncomputer science education, we devised a set of hu-\nman evaluation metrics. This allowed for an objec-\ntive appraisal of the LLM-generated questions, and,\nthrough a comparative analysis with human-created\ncounterparts, critical aspects such as question qual-\nity, diversity, discernibility between human and AI\nauthors, and relative superiority in quality were ex-\nplored (Figure 1). These analyses have enhanced\nour understanding of the potential roles of LLMs\nin computer science education.\nThis investigation provides a foundation for con-\nsidering the potential inclusion of LLMs in learning\nplatforms, which could offer new possibilities for\nenhancing the learning experience in introductory\nprogramming courses. Given these advancements,\nour study contributes to the field as follows:\n• The curation of a high-quality dataset consist-\ning of human and LLM-generated code trac-\ning questions and associated code snippets.\n• An exploration and evaluation of GPT4’s ca-\npability in question generation, including com-\nparisons with both GPT3.5-turbo and human-\nauthored questions, and an examination of\nfew-shot and zero-shot scenarios.\n• The introduction of a human evaluation\nmethodology and a comprehensive assess-\nment of the quality of LLM-generated ques-\ntions, offering valuable insights into the po-\ntential of LLMs in educational contexts.\n2 Related Work\nQuestion Generation: Early Question Genera-\ntion (QG) research primarily focused on multiple-\nchoice questions (Mitkov et al., 2006; Agarwal and\nMannem, 2011) and questions with specific inter-\nrogatives (Heilman and Smith, 2010). With the\nemergence of the SQuAD dataset (Rajpurkar et al.,\n2016), context-dependent QG gained momentum\n(Du et al., 2017; Yuan et al., 2017; Subramanian\net al., 2018; Puri et al., 2020). This extended to\ncomplex tasks like generating unanswerable ques-\ntions (Choi et al., 2018; Zhu et al., 2019; Reddy\net al., 2019) and multi-hop reasoning (Pan et al.,\n2020, 2021; Shridhar et al., 2022). Our work, focus-\ning on generating code tracing questions in CS Edu-\ncation domain, addresses unique challenges around\ncode, natural language, and pedagogical compre-\nhension, inadequately covered by previous methods\ndue to a lack of specialized datasets.\nCode LLMs for CS Education: Recent ad-\nvances in code large language models (LLMs)\n(Chen et al., 2021; Wang et al., 2021; Le et al.,\n2022; Wang et al., 2023) have enabled various\ndownstream applications, including code comple-\ntion, retrieval, summarization, explanation, and\nunit test generation (Lu et al.; Siddiq et al., 2023;\nTian et al., 2023). Studies have showcased the\nLLMs’ ability to generate novice programming\ncontent comparable to humans (Finnie-Ansley\net al., 2022; Piccolo et al., 2023). LLMs have been\nutilized in classroom environments (Kazemitabaar\net al., 2023), to generate coding exercises and ex-\nplanations (Sarsa et al., 2022), and to create coun-\nterfactual questions (Narayanan et al., 2023). Our\nstudy represents the first exploration of LLMs for\ncode tracing question generation, a critical compo-\nnent of CS Education, thus underscoring the po-\ntential of these models for generating educational\ncontent.\n3 Our Approach\n3.1 Task Definition\nIn automatic tracing question generation, given a\ndescription (optional) d ∈ D∪∅, detailing the code\ncontext, and a code snippet c ∈ C provided by an\ninstructor or student, the aim is to generate a set\nof relevant questions Q′ for student practice. This\ntask can be formally defined as a function:\nf : (d, c) ↦→Q′ (1)\nwhere D represents all possible descriptions, C all\npossible code snippets, and Q′ is a subset of all\npossible questions Q.\n3.2 Curating the Code-Tracing Question\nDataset\nFor our experiment, we curated a unique dataset re-\nflecting the range of tracing questions encountered\nby beginner programmers. We sourced 158 unique\n7407\nFigure 2: Comparison of the BERTScore on all LLM-\ngenerated questions and human-authored questions.\nquestions from CSAwesome2, a recognized online\nJava course aligned with the AP Computer Science\nA curriculum. To enhance diversity, we added 18\nquestions extracted from relevant YouTube videos.\nOther platforms and sources were also examined\nbut didn’t fit due to a lack of explicit tracing ques-\ntions. Our final dataset consists of 176 unique code\nsnippets and question pairs, allowing a fair eval-\nuation of LLMs’ ability to generate introductory\nprogramming tracing questions.\n3.3 Prompt Engineering and Model Selection\nIn our iterative approach to prompt engineering\nand model selection, we first refined prompts and\nthen generated tracing questions using GPT-3.5-\nturbo and GPT-4. Using BERTScore, we assessed\nquestion diversity and similarity. Based on these\ninsights, we combined the optimized prompt with\nthe chosen model to determine the most effective\ngeneration approach, be it few-shot or zero-shot.\nOur final prompt, refined iteratively from (Brown\net al., 2020), positioned in Appendix B, adopts an\nexpert instructor’s perspective, encourages deep\nunderstanding via code-tracing questions, and max-\nimizes the inherent versatility of LLMs.\nNext, we considered GPT-3.5-turbo and GPT-4\nfor model selection, and investigate the generated\ntracing questions diversity by BERTScore (Zhang*\net al., 2020). Regarding the automatic evaluation\nof the diversity in generated questions, we adopted\nthe following methodology: For each code snip-\npet, we utilize a singular human-authored tracing\nquestion as the reference. Both GPT3.5-turbo and\nGPT4 are then tasked with generating multiple trac-\ning questions for every snippet. Following this,\n2https://runestone.academy/ns/books/\npublished/csawesome/index.html\nwe employ regular expressions in a postprocess-\ning step to segment the generated content, isolat-\ning individual tracing questions. Subsequently,\nfor each generated prediction p, its BERTScore\nis computed in relation to the reference, denoted as\nBERTScore(reference, p).\nThe boxplot in Figure 2 displays the Precision,\nRecall, and F1 scores for both models. From the\ngraph, it’s clear that GPT-3.5-turbo has a median\nPrecision score around 0.45, Recall slightly above\n0.6, and an F1 score hovering around 0.5. In\ncomparison, GPT-4 shows a more balanced perfor-\nmance with a median Precision score close to 0.6,\nRecall near 0.55, and F1 just above 0.5. Notably,\nthe variability in scores, particularly for GPT-4,\nhighlights the diverse outcomes in its results. Based\non our results, we chose GPT4 for subsequent eval-\nuations. Enhanced performance examples from\nGPT4 are in Appendix C.\nNext, we hypothesized that the few-shot ques-\ntion generation approach, which feeds the model\nwith three tracing question examples and their re-\nspective code snippets, would yield higher-quality\nquestions than the zero-shot generation that relies\nsolely on the prompt. Contrary to our expectations,\nthe experiment showed that the few-shot method\nintroduced a significant bias towards the example\nquestions, thus narrowing the diversity in the gen-\nerated questions. Consequently, we opted for the\nzero-shot generation in our tests, which fostered a\nbroader spectrum of question types. Detailed ex-\namples of outcomes from both the zero-shot and\nfew-shot approaches are available in Section 4.4.\n3.4 Human Evaluation\nNext, we conducted a human evaluation compar-\ning the quality of GPT4-generated and human-\nauthored tracing questions. The expert evaluators\nwere meticulously screened based on specific cri-\nteria: they had to be computer science graduate\nstudents with at least one year of programming\nteaching or tutoring experience. Four such experts,\nmeeting these criteria, participated in the evalua-\ntion.\nEach evaluator was assigned a set of 44 ran-\ndomly selected code snippets from a pool of 176\nhuman-authored tracing questions. For each snip-\npet, evaluators received a pair of questions (one\nhuman-authored and one GPT4-generated) in a ran-\ndomized order to mitigate potential ordering bias.\nEvaluators unawareness of question authorship was\n7408\nCriteria Label\nRelevance to Learning Objectives 1-5\nTracing or not Yes/No\nClarity of the Question 1-5\nDifficulty Level 1-5\nRelevance to the Given Code Snippet 1-5\nAbility to Distinguish Source Human-created/\nAI-generated\nPreference for Better Question Check preferred\nTable 1: Criteria used for expert evaluation.\nHuman GPT4\nMean Median Mean Median\nRelevance to Learning 4.78 5.00 4.62 5.00\nQuestion Clarity 4.72 5.00 4.42 5.00\nAppropriate Difficulty 4.75 5.00 4.43 5.00\nRelevance to Code 4.72 5.00 4.64 5.00\nTable 2: Comparative statistics for human and GPT4\ngenerated questions.\nensured.\nThe evaluators rated each question based on five\ncriteria shown in Table 1. They also guessed the\nquestion’s authorship and expressed their prefer-\nence between the pair. Detailed evaluation criteria\nand labels can be found in Table 1.\n4 Analyses and Results\nThis section details our analysis and highlights the\nresults, encompassing quality ratings, expert per-\nceptions, and textual similarities in question gener-\nation.\n4.1 Comparative Analysis of Quality Ratings\nU-val p\nRelevance to learning objectives 3688.0 0.047\nQuestion Clarity 3392.0 0.011\nDifficulty Appropriateness 3540.5 0.015\nRelevance to the given code snippet 3918.5 0.595\nTable 3: Results of Mann-Whitney U tests compar-\ning human evaluations of LLM-generated and human-\nauthored code-tracing questions. U-values and p-values\nare provided for four criteria. Significant differences\nfor three criteria indicate varying performance, while\nno difference in ’relevance to the given code snippet’\nsuggests similar performance.\nTo assess the quality disparity between LLM-\ngenerated and human-authored questions, we ap-\nplied Mann-Whitney U tests (Mann and Whitney,\n1947) to the median ratings of four evaluation cri-\nteria in Table 3. Significant differences emerged\nin three criteria: relevance to learning objectives,\nclarity, and difficulty appropriateness. However,\nthe relevance to the given code snippet showed no\nsignificant difference, indicating comparable per-\nformances.\nDespite U-tests highlighting significant differ-\nences in some criteria, the practical quality differ-\nence was minimal. As further detailed in Table 2,\nLLM-generated questions had slightly lower mean\nratings, yet their median ratings closely mirrored\nthose of human-authored questions.\nConsidering these two analyses together, it is\napparent that despite some statistical differences,\nLLM-generated questions still maintain a high ped-\nagogical standard effectively. Consequently, LLM,\nwhile underlining areas for potential enhancement,\ndemonstrates proficiency in generating questions\nthat align closely in quality and course relevance\nwith those crafted by humans.\n4.2 Expert Perception of Question Authorship\nWe further evaluated the discernibility of LLM-\ngenerated questions from human-authored ones\nusing a Confusion Matrix (Table 5). Approxi-\nmately 56% (99 out of 176) of GPT4-generated\nquestions were mistakenly identified by experts as\nhuman-generated, and about 20% (35 out of 176)\nof human-authored questions were misattributed to\nthe GPT4. This overlap signifies the high quality\nof the generated questions and GPT4’s proficiency\nin producing pedagogically relevant tracing ques-\ntions. Moreover, the matrix reveals an evaluator\nbias toward attributing higher-quality questions to\nhuman authorship.\n4.3 Textual Similarity between Questions\nTable 6 presents BLEU (Post, 2018), ROUGE-\n1/2/L (Lin, 2004), and BERTScores (Zhang* et al.,\n2020), comparing the similarity between the ran-\ndomly selected GPT4 generated questions and cor-\nresponding human-authored questions. The low\nBLEU and ROUGE scores suggest that GPT4 is\ngenerating distinct, non-verbatim questions com-\npared to human-authored questions. A moderate\nBERTScore, reflecting semantic similarity, sug-\ngests that GPT4-generated questions align with\nthe context of human-authored ones. This further\nunderscores GPT4’s capability to independently\ngenerate relevant and diverse code-tracing ques-\ntions, distinct from those crafted by humans.\n7409\nGPT-4: Few-Shot vs. Zero-Shot Results\nZero-Shot Prompt: Few-Shot Prompt:\nYour task ... Craft tracing questions for: {requirement}\n{code_snippet}\nYour task ... Examples: {requirement1}\n{code_snippet1} {tracing_question1} ... Craft\nfor: {requirement} {code_snippet}\nZero-Shot Result: Few-Shot Result:\n1. What is the purpose of this function? What does it\naim to accomplish?\nTracing question 1: What is the output of the method\nredo(9, 3)?\n2. What is the base case for this recursive function?\nWhat happens when the base case is reached?\nTracing question 2: What is the output of the method\nredo(0, 5)?\n3. If the input values are i = 8 and j = 2, what will be\nthe output of the function? Please trace through the\ncode step-by-step and explain your reasoning.\nTracing question 3: What is the output of the method\nredo(27, 3)?\n... ...\n6. What happens if both input values are negative,\nsuch as i = -8 and j = -2? Does the function handle this\ncase correctly according to the requirement? Explain\nyour reasoning.\nTable 4: Illustrative comparison between GPT-4’s code-tracing question generation in Few-Shot and Zero-Shot\nsettings, showcasing the diversity and specificity of generated questions.\nPredicted GPT4 Predicted Human\nActual GPT4 77 99\nActual Human 35 141\nTable 5: Confusion Matrix indicating experts’ attribu-\ntions of code-tracing questions. The table displays the\nnumber of actual GPT-4 generated and human-authored\nquestions and how they were predicted by the experts,\nunderscoring the challenge in distinguishing between\nthe two.\nCombining with the previous analyses, the LLM,\nsuch as GPT4, thus exhibits substantial promise\nin generating high-quality, course-relevant code-\ntracing questions, illustrating its utility as a teach-\ning aid.\n4.4 Few-shot vs Zero-shot Generation Results\nFew-shot generation biased our model towards the\nprovided examples, largely reducing question di-\nversity. In contrast, zero-shot generation yielded\nmore diverse questions, prompting us to favor it\nfor broader question variety in our experiment. De-\ntailed examples of the generated results for both\n0-shot and few-shot methods can be found in Ap-\npendix D.\nTable 4 provides a side-by-side comparison of\nGPT-4’s performance in few-shot and zero-shot set-\ntings. The zero-shot results exhibit a broader range\nof question types, while the few-shot results seem\nto be more templated, reflecting the bias introduced\nby the provided examples.\nPossible reasons for these observations include\nMetric Score\nBLEU 0.02\nROUGE-1 F-score 0.215\nROUGE-2 F-score 0.051\nROUGE-L F-score 0.199\nBERTScore Precision 0.274\nBERTScore Recall 0.341\nBERTScore F1 0.303\nTable 6: Comparing the similarity between human-\nauthored questions and the random choiced GPT4 ques-\ntion.\nthe influence of training data and model design in\nzero-shot scenarios, allowing GPT-4 to tap into its\nvast training experience. In contrast, in few-shot\nscenarios, the model might overly adhere to the\nprovided examples, interpreting them as stringent\ntemplates, which can compromise output diversity.\nThe balance between the nature of the task and\nthe examples becomes pivotal in few-shot settings,\npotentially leading to outputs that may sacrifice\naccuracy or diversity. These hypotheses warrant\nfurther investigation in future work.\n5 Conclusion\nThis study explored the capability of GPT-4 in gen-\nerating code-tracing questions that rival the quality\nof those crafted by human educators. The findings\nilluminate the potential of LLMs to bolster pro-\ngramming education, marking a significant stride\nin the domain of code-tracing question generation\nand LLM application. This sheds light on scalable,\nhigh-quality automated question generation.\n7410\nLimitations and Future Work\nThis study marks a step closer in evaluating LLMs\nfor code tracing question generation, but it is not\nwithout its limitations. Our research was primarily\nanchored to GPT-4, raising concerns about the gen-\neralizability of our findings to other LLMs, such as\nCodeT5+. Moreover, the study did not delve into\nthe personalization of tracing questions based on\nindividual student submissions, a facet that could\ngreatly enhance the learning experience. Further-\nmore, the real-world educational efficacy of the\nLLM-generated questions remains an open ques-\ntion, given that our study did not involve actual\nstudents.\nSeveral avenues beckon for further exploration.\nEvaluations with a broader range of models will\noffer a more comprehensive perspective on LLM\ncapabilities. While our study centered on intro-\nductory Java tracing questions, assessing LLM ver-\nsatility across different programming domains is\nimperative. The potential of LLMs extends beyond\nmere question generation; by tailoring questions\nto student needs, we can amplify the educational\nrelevance. Our roadmap includes the development\nof an educational platform integrated with LLM\nquestions, followed by classroom experiments and\nusability testing. To ensure broader applicability,\nexpanding our dataset is crucial. Lastly, our find-\nings on few-shot and zero-shot learning necessitate\nfurther investigation into model adaptability, bi-\nases in question generation, and the potential of\nintermediate-shot learning.\nThese directions not only underscore the trans-\nformative potential of LLMs in AI-driven education\nbut also emphasize the importance of comprehen-\nsive evaluations.\nEthical Statement\nOur exploration of Large Language Models\n(LLMs) in introductory programming education\nwas conducted ethically. We sourced public data\nand maintained evaluator anonymity and data confi-\ndentiality through secure storage. Evaluators were\ninformed of the objectives and participated volun-\ntarily. All evaluation results, as committed in the\nIRB forms, are securely stored. We strived for ed-\nucational fairness by properly compensating the\neducators involved in our evaluation. We are mind-\nful of the societal impacts of LLM integration in\neducation. While acknowledging their promise, we\nbelieve careful consideration of pedagogical goals\nwithin the educational ecosystem is vital. Our fu-\nture work will be guided by these ethical principles\nof privacy, informed consent, secure data handling,\ninclusivity, and conscientious progress focused on\nstudents’ best interests.\nReferences\nManish Agarwal and Prashanth Mannem. 2011. Auto-\nmatic gap-fill question generation from text books.\nIn Proceedings of the Sixth Workshop on Innovative\nUse of NLP for Building Educational Applications,\npages 56–64, Portland, Oregon. Association for Com-\nputational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-\ntau Yih, Yejin Choi, Percy Liang, and Luke Zettle-\nmoyer. 2018. QuAC: Question answering in context.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2174–2184, Brussels, Belgium. Association for Com-\nputational Linguistics.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. In Proceedings of the 55th Annual\n7411\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1342–1352,\nVancouver, Canada. Association for Computational\nLinguistics.\nJames Finnie-Ansley, Paul Denny, Brett A. Becker, An-\ndrew Luxton-Reilly, and James Prather. 2022. The\nrobots are coming: Exploring the implications of ope-\nnai codex on introductory programming. In Proceed-\nings of the 24th Australasian Computing Education\nConference, ACE ’22, page 10–19, New York, NY ,\nUSA. Association for Computing Machinery.\nMohammed Hassan and Craig Zilles. 2021. Exploring\n‘reverse-tracing’questions as a means of assessing\nthe tracing skill on computer-based cs 1 exams. In\nProceedings of the 17th ACM conference on interna-\ntional computing education research, pages 115–126.\nMichael Heilman and Noah A. Smith. 2010. Good\nquestion! statistical ranking for question generation.\nIn Human Language Technologies: The 2010 An-\nnual Conference of the North American Chapter of\nthe Association for Computational Linguistics, pages\n609–617, Los Angeles, California. Association for\nComputational Linguistics.\nMajeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Bar-\nbara J. Ericson, David Weintrop, and Tovi Grossman.\n2023. Studying the effect of ai code generators on\nsupporting novice learners in introductory program-\nming. In Proceedings of the 2023 CHI Conference\non Human Factors in Computing Systems, CHI ’23,\nNew York, NY , USA. Association for Computing\nMachinery.\nAmruth N Kumar. 2013. A study of the influence of\ncode-tracing problems on code-writing skills. In Pro-\nceedings of the 18th ACM conference on Innovation\nand technology in computer science education, pages\n183–188.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven C. H. Hoi. 2022. Coderl: Mas-\ntering code generation through pretrained models and\ndeep reinforcement learning. In NeurIPS.\nTeemu Lehtinen, Lassi Haaranen, and Juho Leinonen.\n2023. Automated questionnaires about students’\njavascript programs: Towards gauging novice pro-\ngramming processes. In Proceedings of the 25th Aus-\ntralasian Computing Education Conference, pages\n49–58.\nTeemu Lehtinen, André L Santos, and Juha Sorva. 2021.\nLet’s ask students about their programs, automati-\ncally. In 2021 IEEE/ACM 29th International Con-\nference on Program Comprehension (ICPC), pages\n467–475. IEEE.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nRaymond Lister, Colin Fidge, and Donna Teague. 2009.\nFurther evidence of a relationship between explain-\ning, tracing and writing skills in introductory pro-\ngramming. Acm sigcse bulletin, 41(3):161–165.\nMike Lopez, Jacqueline Whalley, Phil Robbins, and\nRaymond Lister. 2008. Relationships between read-\ning, tracing and writing skills in introductory pro-\ngramming. In Proceedings of the fourth international\nworkshop on computing education research, pages\n101–112.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, et al.\nCodexglue: A machine learning benchmark dataset\nfor code understanding and generation. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 1).\nHenry B Mann and Donald R Whitney. 1947. On a test\nof whether one of two random variables is stochasti-\ncally larger than the other. The annals of mathemati-\ncal statistics, pages 50–60.\nRuslan Mitkov, Ha Le An, and Nikiforos Karamanis.\n2006. A computer-aided environment for generating\nmultiple-choice test items. Natural language engi-\nneering, 12(2):177–194.\nArun Balajiee Lekshmi Narayanan, Rully Agus Hen-\ndrawan, et al. 2023. Enhancing programming\netextbooks with chatgpt generated counterfactual-\nthinking-inspired questions. arXiv preprint\narXiv:2306.00551.\nOpenAI. 2023. Gpt-4 technical report.\nLiangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen\nKan, and William Yang Wang. 2021. Unsupervised\nmulti-hop question answering by question generation.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5866–5880, Online. Association for Computa-\ntional Linguistics.\nLiangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng\nChua, and Min-Yen Kan. 2020. Semantic graphs\nfor generating deep questions. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1463–1475, Online. Asso-\nciation for Computational Linguistics.\nStephen R Piccolo, Paul Denny, Andrew Luxton-Reilly,\nSamuel Payne, and Perry G Ridge. 2023. Many bioin-\nformatics programming tasks can be automated with\nchatgpt. arXiv preprint arXiv:2303.13528.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\n7412\nRaul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa\nPatwary, and Bryan Catanzaro. 2020. Training\nquestion answering models from synthetic data. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5811–5826, Online. Association for Computa-\ntional Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. CoQA: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249–266.\nSeán Russell. 2021. Automatically generated and\ngraded program tracing quizzes with feedback. In\nProceedings of the 26th ACM Conference on Innova-\ntion and Technology in Computer Science Education\nV . 2, ITiCSE ’21, page 652, New York, NY , USA.\nAssociation for Computing Machinery.\nSami Sarsa, Paul Denny, Arto Hellas, and Juho\nLeinonen. 2022. Automatic generation of program-\nming exercises and code explanations using large\nlanguage models. In Proceedings of the 2022 ACM\nConference on International Computing Education\nResearch - Volume 1, ICER ’22, page 27–43, New\nYork, NY , USA. Association for Computing Machin-\nery.\nTakayuki Sekiya and Kazunori Yamaguchi. 2013. Trac-\ning quiz set to identify novices’ programming mis-\nconceptions. In Proceedings of the 13th Koli Calling\nInternational Conference on Computing Education\nResearch, pages 87–95.\nKumar Shridhar, Jakub Macina, Mennatallah El-Assady,\nTanmay Sinha, Manu Kapur, and Mrinmaya Sachan.\n2022. Automatic generation of socratic subquestions\nfor teaching math word problems. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4136–4149, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nMohammed Latif Siddiq, Joanna Santos, Rid-\nwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat,\nand Vinicius Carvalho Lopes. 2023. Exploring the\neffectiveness of large language models in generating\nunit tests. arXiv preprint arXiv:2305.00418.\nEmil Stankov, Mile Jovanov, and Ana Madevska Bog-\ndanova. 2023. Smart generation of code tracing ques-\ntions for assessment in introductory programming.\nComputer Applications in Engineering Education ,\n31(1):5–25.\nSandeep Subramanian, Tong Wang, Xingdi Yuan,\nSaizheng Zhang, Adam Trischler, and Yoshua Ben-\ngio. 2018. Neural models for key phrase extraction\nand question generation. In Proceedings of the Work-\nshop on Machine Reading for Question Answering,\npages 78–88, Melbourne, Australia. Association for\nComputational Linguistics.\nAnderson Thomas, Troy Stopera, Pablo Frank-Bolton,\nand Rahul Simha. 2019. Stochastic tree-based gen-\neration of program-tracing practice questions. In\nProceedings of the 50th ACM Technical Symposium\non Computer Science Education, pages 91–97.\nHaoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-\nChi Cheung, Jacques Klein, and Tegawendé F Bis-\nsyandé. 2023. Is chatgpt the ultimate program-\nming assistant–how far is it? arXiv preprint\narXiv:2304.11938.\nAnne Venables, Grace Tan, and Raymond Lister. 2009.\nA closer look at tracing, explaining and code writing\nskills in the novice programmer. In Proceedings\nof the fifth international workshop on Computing\neducation research workshop, pages 117–128.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare,\nNghi D.Q. Bui, Junnan Li, and Steven C. H. Hoi.\n2023. Codet5+: Open code large language mod-\nels for code understanding and generation. arXiv\npreprint.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nBenjamin Xie, Greg L Nelson, and Amy J Ko. 2018. An\nexplicit strategy to scaffold novice program tracing.\nIn Proceedings of the 49th ACM technical symposium\non computer science education, pages 344–349.\nXingdi Yuan, Tong Wang, Caglar Gulcehre, Alessandro\nSordoni, Philip Bachman, Saizheng Zhang, Sandeep\nSubramanian, and Adam Trischler. 2017. Machine\ncomprehension by text-to-text neural question gener-\nation. In Proceedings of the 2nd Workshop on Repre-\nsentation Learning for NLP, pages 15–25, Vancouver,\nCanada. Association for Computational Linguistics.\nLaura Zavala and Benito Mendoza. 2018. On the use\nof semantic-based aig to automatically generate pro-\ngramming exercises. In Proceedings of the 49th ACM\nTechnical Symposium on Computer Science Educa-\ntion, SIGCSE ’18, page 14–19, New York, NY , USA.\nAssociation for Computing Machinery.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nHaichao Zhu, Li Dong, Furu Wei, Wenhui Wang, Bing\nQin, and Ting Liu. 2019. Learning to ask unanswer-\nable questions for machine reading comprehension.\n7413\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 4238–\n4248, Florence, Italy. Association for Computational\nLinguistics.\n7414\nA Questionare\nDescription: (blank)\nCode:\nint[][] m = {{1,1,1,1},{1,2,3,4},{2,2,2,2},{2,4,6,8}};\nint sum = 0;\nfor (int k = 0; k < m.length; k++) {\nsum = sum + m[m.length-1-k][1];\n}\nQuestion 1: Given the following code segment, what is the value of sum after this code executes?\nQuestion 2: What is the role of the expression ‘m[m.length-1-k][1]’ in the code?\nAnnotation\n1. Relevance to Learning Objectives: The question is relevant to the learning objectives of an\nintroductory programming course. (Label: 1-5)\n2. Tracing or not: Is this a tracing question? (Label: Yes or No)\n3. Clarity of the Question: The question presented is clear and the language used in the question is\neasy to understand. (Label: 1-5)\n4. Difficulty Level: The difficulty level of the question is appropriate for an introductory programming\ncourse. (Label: 1-5)\n5. Relevance to the Given Code Snippet: The question is appropriately related to the code snippet\nprovided in the question. (Label: 1-5)\n6. Ability to Distinguish Human-Authored from Automatically Generated Questions: Can you tell if\nthe question is human-authored or automatically generated? (Label: Human-created or AI-generated)\n7. I think this is a better tracing question. (Check the box under the better question)\nThis is an example of our questionnaire sent to annotators.\nB Prompts\nThe final prompt we employed offers LLMs a detailed context: it requests the generation of questions\nfrom an expert instructor’s perspective within a defined pedagogical setting. It outlines the merits of\ncode-tracing questions, gives an insight into their typical structure, and highlights their educational aim,\nspecifically, promoting in-depth understanding rather than just assessing knowledge. Unlike the data\ncollection process, where each code snippet is linked to a single question, the prompt is designed to\nproduce multiple valid tracing questions for the same snippet, which leverages the inherent diversity and\nbreadth potential of LLMs.\n7415\nUser Prompt: In your role as an education expert in an introductory Java programming course, you\nare equipped with a deep understanding of Java and teaching methodologies. Your aim is to shape\nthe minds of young learners, paving their path to becoming proficient programmers. One of your\npotent teaching techniques involves providing students with real-world coding practice requirements\nand letting them craft working code snippets.\nConsider a scenario where you have presented your students with a requirement to develop a\nbasic calculator program in Java, performing fundamental arithmetic operations like addition,\nsubtraction, multiplication, and division. Enthusiastically, the students pour their effort into crafting\nvaried solutions, reflecting their unique approach to the problem and their understanding of Java\nprogramming.\nBut your task doesn’t end with the generation of code. It’s now time for the students to delve deeper,\nand here comes the importance of tracing in programming. Code tracing is the process of manually\ngoing through the execution of code, step-by-step, to understand its flow and logic. It’s a fundamental\npractice in learning programming, aiding students in debugging their programs and developing a\nstronger understanding of code behavior.\nOne of the best ways to encourage code tracing is by generating tracing questions. Such questions\nstimulate the students to analyze their code at a granular level, fostering their problem-solving skills\nand enhancing their understanding of the Java language.\nRemember, your questions should stem from two sources - the initial Requirement and the Code the\nstudents have produced. For example, given a code snippet, you might ask ’What is the output of\nthis code if the user inputs two and three and chooses to multiply?’ or ’Can this code handle the\ndivision by zero scenario, according to the requirement?’.\nYour goal is not just to evaluate their coding skills, but to instill a deep, lasting understanding of\nJava programming in them. By asking relevant tracing questions, you’re helping to nurture a new\ngeneration of critical thinkers, problem solvers, and proficient programmers.\nYou will ask many relevant tracing questions given 1. Requirement, 2. Code, as below:\nRequirement: (blank) Code:\nint x = 3;\nint y = 2;\nif (x > 2)\nx++;\nif (y > 1)\ny++;\nif (x > 2)\nSystem.out.print(\"first \");\nif (y < 3)\nSystem.out.print(\"second \");\nSystem.out.print(\"third\");\nThis is an example of our prompt.\nC GPT3.5 vs GPT4\nPrompt ... Requirement: (blank) Code:\n7416\nint x = 3;\nint y = 2;\nif (x > 2)\nx++;\nif (y > 1)\ny++;\nif (x > 2)\nSystem.out.print(\"first \");\nif (y < 3)\nSystem.out.print(\"second \");\nSystem.out.print(\"third\");\nGPT-3.5 Result:\nTracing Questions:\n1. What are the initial values of x and y in this code?\n2. Does the if statement on line 3 execute? Why or why not?\n3. Does the if statement on line 5 execute? Why or why not?\n4. What is the value of x after line 4 has executed?\n5. What is the value of y after line 6 has executed?\n6. Which System.out.print statement(s) will execute in this code?\n7. What is the output of this code?\n8. What happens if the initial value of x is 1? How does the output change?\n9. What happens if the initial value of y is 3? How does the output change?\n10. Can this code handle other arithmetic operations besides addition and subtraction, according to\nthe requirement?\nGPT-4 Result: 1. What will be the values of x and y after executing the first two if statements?\n2. What is the output of the code snippet as it is?\n3. If we change the initial value of x to 1, what will be the output of the code snippet?\n4. If we change the initial value of y to 0, what will be the output of the code snippet?\n5. What will happen if both x and y are initialized with values less than or equal to 2? What will be\nthe output in that case?\n6. How would you modify the code to include an additional condition where it checks if x is equal to\ny and prints \"equal\" if it’s true?\n7. If we swap the positions of the second and third if statements, would the output change? Why or\nwhy not? 8. Can you explain the flow of the code execution given the initial values of x and y? What\nare the key decision points in the code?\n9. What happens if we change the second if statement to \"if (y == 1)\"? How does that change the\noutput of the code snippet?\n10. How would you modify the code to print \"none\" if none of the conditions are met in the if\nstatements?\nThis is an example of GPT3.5-turbo v.s. GPT4 for tracing question generation on a simple java problem.\nPrompt ... Requirement: (blank) Code:\n7417\nRequirement: (blank)\nCode:\npublic static int redo(int i, int j)\n{\nif (i==0)\nreturn 0;\nelse\nreturn redo(i/j, j)+1;\n}\nGPT-3.5 Result:\n1. What is the purpose of the redo() method?\n2. What is the return type of the redo() method?\n3. What is the initial value of i and j when the redo() method is called?\n4. What happens if the value of i is 0?\n5. What happens if the value of j is 0?\n6. What happens if the value of i is negative?\n7. What happens if the value of j is negative?\n8. What happens if the value of i is greater than j?\n9. What is the output of the redo() method if the values of i and j are 8 and 2 respectively?\n10. What is the output of the redo() method if the values of i and j are 10 and 3 respectively?\nGPT-4 Result:\n1. What is the base case for this recursive function, and what does it return?\n2. How is the recursion progressing towards the base case in this code?\n3. What will be the output of this function if the inputs are i=16 and j=2?\n4. What will be the output of this function if the inputs are i=81 and j=3?\n5. How will the function behave if both i and j are equal to 0? Will it cause any issues?\n6. How will the function behave if i is less than j? Is the output as expected according to the\nrequirement?\n7. How does the function handle the scenario when j is 1? Is the output as expected according to the\nrequirement?\n8. Can this function handle negative inputs for i and j? If not, how can it be modified to handle such\ncases?\n9. What is the maximum recursion depth for this function, and how does it depend on the input\nvalues i and j?\n10. Are there any edge cases or scenarios that this function might not handle well? If so, can you\nsuggest any modifications to improve the code?\nThis is an example of GPT3.5-turbo v.s. GPT4 for tracing question generation on a recursion java problem.\nD Zero-Shot vs Few-Shot in GPT4\nZero-Shot Prompt ... Requirement: (blank) Code:\n7418\nRequirement: (blank)\nCode:\npublic static int redo(int i, int j)\n{\nif (i==0)\nreturn 0;\nelse\nreturn redo(i/j, j)+1;\n}\nFew-Shot Prompt ... Here are some more examples of tracing questions based on code snippet:\n1.\nRequirement: (blank)\nCode:\nfor (int i = 3; i <= 9; i++)\n{\nSystem.out.print(\"\"*\"\");\n}\nTracing question:\nHow many times does the following method print a *?\n2.\nRequirement: (blank)\nCode:\n//precondition: x >=0\npublic static void mystery (int x)\n{\nSystem.out.print(x % 10);\nif ((x / 10) != 0)\n{\nmystery(x / 10);\n}\nSystem.out.print(x % 10);\n}\nTracing question:\nGiven the following method declaration, which of the following is\n↪→ printed as the result of the call mystery(1234)?\n3.\n7419\nRequirement: (blank)\nCode:\npublic static int product(int n)\n{\nif (n <= 1)\nreturn 1;\nelse\nreturn n * product(n - 2);\n}\nTracing question:\nGiven the following method declaration, what value is returned as\n↪→ the result of the call product(5)?\nYour goal is not just to evaluate their coding skills, but to instill a deep, lasting understanding of\nJava programming in them. By asking relevant tracing questions, you’re helping to nurture a new\ngeneration of critical thinkers, problem solvers, and proficient programmers. You will ask many\nrelevant tracing questions given 1. Requirement, 2. Code, as below. You don’t have to strictly follow\nthe examples, and we encourage you to be creative when thinking about the tracing questions:\nRequirement: (blank) Code:\nRequirement: (blank)\nCode:\npublic static int redo(int i, int j)\n{\nif (i==0)\nreturn 0;\nelse\nreturn redo(i/j, j)+1;\n}\nZero-Shot Result:\n1. What is the purpose of this function? What does it aim to accomplish?\n2. What is the base case for this recursive function? What happens when the base case is reached?\n3. If the input values are i = 8 and j = 2, what will be the output of the function? Please trace through\nthe code step-by-step and explain your reasoning.\n4. If the input values are i = 27 and j = 3, what will be the output of the function? Please trace\nthrough the code step-by-step and explain your reasoning.\n5. What happens if the input values are i = 0 and j = 5? Does the function handle this case correctly?\nExplain your reasoning.\n6. What happens if the input values are i = 5 and j = 0? Does the function handle this case correctly\naccording to the requirement? Explain your reasoning.\n7. What happens if both input values are negative, such as i = -8 and j = -2? Does the function handle\nthis case correctly according to the requirement? Explain your reasoning.\n8. Are there any edge cases or potential issues with the current implementation of the function? If\nso, what are they and how can they be addressed?\nFew-Shot Result:\nTracing question 1: What is the output of the method redo(9, 3)?\nTracing question 2: What is the output of the method redo(0, 5)?\nTracing question 3: What is the output of the method redo(27, 3)?\nTracing question 4: How many times does the method redo call itself when i = 16 and j = 2?\n7420\nTracing question 5: Can this method handle cases where either i or j is a negative number? If so,\nwhat would be the output for redo(-6, 2)?\nThis is an example GPT4 in Few-Shot setting v.s. Zero-Shot Setting.\n7421"
}