{
  "title": "Analysis of large-language model versus human performance for genetics questions",
  "url": "https://openalex.org/W4318389352",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2333211347",
      "name": "Dat Duong",
      "affiliations": [
        "National Human Genome Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2420817263",
      "name": "Benjamin D. Solomon",
      "affiliations": [
        "National Human Genome Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2333211347",
      "name": "Dat Duong",
      "affiliations": [
        "National Human Genome Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2420817263",
      "name": "Benjamin D. Solomon",
      "affiliations": [
        "National Human Genome Research Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281478769",
    "https://openalex.org/W3033581476",
    "https://openalex.org/W2990290777",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4312067799",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2909194804",
    "https://openalex.org/W2889664156",
    "https://openalex.org/W4290927996"
  ],
  "abstract": "Abstract Large-language models like ChatGPT have recently received a great deal of attention. To assess ChatGPT in the field of genetics, we compared its performance to human respondents in answering genetics questions (involving 13,636 responses) that had been posted on social media platforms starting in 2021. Overall, ChatGPT did not perform significantly differently than human respondents, but did significantly better on memorization-type questions versus critical thinking questions, frequently provided different answers when asked questions multiple times, and provided plausible explanations for both correct and incorrect answers.",
  "full_text": "1 \n \nAnalysis of large-language model versus human performance for genetics questions \nDat Duong 1 ; Benjamin D. Solomon 1  \n1 Medical Genomics Unit, Medical Genetics Branch, National Human Genome Research Institute, \nBethesda, Maryland, United States of America \nCorrespondence: \nBenjamin D. Solomon \nClinical Director \nNational Human Genome Research Institute \nBuilding 10 - CRC, Suite 3-2551 \n10 Center Drive \nBethesda, MD 20892 \nUnited States of America \nPhone: 301-402-8824 \nEmail: solomonb@mail.nih.gov\n \n \n \n \n \n \n \n \n \n \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2 \n \nAbstract \nLarge-language models like ChatGPT have recently received a great deal of attention. To assess ChatGPT \nin the field of genetics, we compared its performance to human respondents in answering genetics \nquestions (involving 13,636 responses) that had been posted on social media platforms starting in 2021. \nOverall, ChatGPT did not perform significantly differently than human respondents, but did significantly \nbetter on memorization-type questions versus critical thinking questions, frequently provided different \nanswers when asked questions multiple times, and provided plausible explanations for both correct and \nincorrect answers.  \n \n \n \n \n \n \n \n \n \n \n \n \n \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \n3 \n \nIntroduction  \nArtificial intelligence (AI) applications, including subsets like deep learning (DL) have strong potential in \nscience and medicine, including the field of clinical genetics and genomics. 1-3  Recently, large-language \nmodels (LLMs) like ChatGPT (https://chat.openai.com/chat ) have been receiving attention in many \nvenues, including via demonstration of medical knowledge. 4,5  By way of background, LLMs use a specific \ntype of DL called a transformer. By training the model on a large dataset of text, it learns to predict the \nnext word in a sentence or set of words following a prompt such as a question. Model training involves \nunsupervised learning, where the models learn to predict the next word in a sentence without explicit \nlabels or annotations.\n6   \nWe aimed to explore how well ChatGPT would perform compared to human respondents in answering \nquestions about genetics. \nMethods \nTo help evaluate this model related to the field of genetics, including in comparison to human \nrespondents, we asked ChatGPT to answer a series of multiple-choice questions that had been posted \non two social media platforms, Twitter and Mastodon, using the following handles (Twitter, \n@BenjaminSolomo2; Mastodon, @solomonbenjamind@genomic.social). These questions have been \nposted weekly or biweekly since 2013, with answers and explanations given at the end of each week; \n430 questions have been posed to date. For this analysis, we only used the subset of 85 questions \nposted starting in 2021, as ChatGPT was trained on data prior to this date, and we wanted to avoid the \nchance that these same questions were used in ChatGPT’s training data. Via the social media polls, these \n85 questions have received a total of 13,636 responses as of January 18, 2023. There were fewer \nrespondents on Mastodon (5 questions) versus Twitter (80 questions). Though these questions were \nanswered anonymously through poll functions on the social media platforms, they have been publicly \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \n4 \n \nsuggested as being useful to physician geneticists, genetic researchers, genetic counselors, and trainees \nin these fields to help with board and formal examination preparation and to otherwise test knowledge. \nThe followers of these accounts reflect these fields. The questions cover a variety of topics related to \ngenetics and genomics, including general knowledge, clinical genetics and approach to diagnosis and \nmanagement, molecular genetics and causes of disease, and inheritance patterns and risk calculations. \nFor this analysis, we did not use questions that involved images, such as questions showing a pedigree \nor a clinical image. \nTo ask the questions of ChatGPT, we initially uploaded batches of 10-20 questions into ChatGPT at a \ntime. We chose this batch number, as we noticed that ChatGPT would typically answer ~10-15 questions \nprior to ceasing to respond further. Each question was accompanied by four possible answers, only one \nof which was correct. We did not provide any instructions except for telling ChatGPT to pick the single \nbest answer to each question. We had previously noticed that ChatGPT sometimes provides different \nanswers to some questions when asked multiple times, even when not prompted, so after initially \nasking the questions, we asked all the questions again. In doing this, we did not provide any feedback to \nChatGPT in terms of pointing out which questions were right or wrong, since ChatGPT may modify \nanswers according to prompts. Finally, for questions where ChatGPT answered the question incorrectly \non either or both attempts, we asked the question again (a third time), and also asked ChatGPT to \nprovide an explanation for the answer it chose in this final attempt.  \nBecause the sample sizes for some categories were small, results were compared using Fisher’s exact \ntest for count data; two-tailed p values are provided.  \nResults  \nQuestions, answers, and explanations (the same materials provided via social media to human \nrespondents), as well as ChatGPT’s explanations for any question that was incorrectly answered by \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \n5 \n \nChatGPT, are provided in Supplemental File 1. Along with other details, a summary of human \nrespondent answers to all of these questions are detailed in Supplementary Table 1; to allow cross-\nreferencing, the questions are numbered according to the same numbering system used on the social \nmedia accounts.  \nWhen examining answers from all respondents versus ChatGPT’s initial responses, there was not a \nstatistically significant difference (p = 0.8145). However, if we measured the respondents’ group \naccuracy by choosing the most commonly selected response as the overall group answer, the \nrespondents significantly outperformed ChatGPT (p = 0.00019). See Table 1. \nWe were interested in how well ChatGPT performed for questions that were considered to focus on \nmemorization (M) or “fact look-up” versus critical thinking (C). To assess this, we divided the questions \ninto these categories based on our subjective assessment of the question (see Supplementary file 1). \nChatGPT performed significantly better for memorization than critical thinking questions (p = 2.344e-\n05). When comparing the ChatGPT results to human respondent results (Table 2), ChatGPT did not \nperform significantly differently than the human respondents for memorization questions (p = 0.2635) \nor critical thinking questions (p = 0.06513).  \nChatGPT provided different answers to the same questions frequently, with 13 initial answer changes \n(15% of the 85 questions). ChatGPT initially gave correct answers 58 times. Of the 58 questions were \ninitially answered correctly, ChatGPT gave the wrong answer for 2 questions (3.4% of these 58 \nquestions) when asked the second time. ChatGPT initially gave incorrect answers 27 times. When the \nquestions were posed again, ChatGPT gave different answers 11 times to these 27 questions (40.7% of \nthese 27 questions). For 8 (29.6%) of these 27 initially incorrect answers, the second set of answers were \ncorrect. When asked for explanations about the initially incorrect answers, ChatGPT would again \nsometimes provide different answers along with the explanation (see details in Supplementary Table 1 \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \n6 \n \nand Supplementary File 1). We also noted that ChatGPT sometimes embellished the answer. For \nexample, as shown in the supplemental files, it added the acronym “(ECG)” after correctly answering \n“Electrocardiogram” once and added the phrase “and segregation testing” after correctly answering \n“Parental testing” to another question. Both are logically correct but were not part of the answer \nchoices. ChatGPT would also sometimes provide full explanations without being prompted. \nChatGPT’s explanations of wrong answers were all relatively plausible in terms of providing believable, \nlogistically consistent (though sometimes incorrect) explanations. Of the explanations given when \nChatGPT initially provided the wrong answers, ChatGPT subsequently gave the right answer along with \nthe explanation in 10 instances (37.0% of the initial 27 incorrect answers). ChatGPT gave the correct \nexplanation (explaining why the right answer was correct) but still indicated it chose the wrong answer \nin 2 instances (7.4% of the initial 27 incorrect answers). These were both after previously giving incorrect \nanswers. In 7 instances (25.9% of the initial 27 incorrect answers), ChatGPT appeared to use incorrect \ninformation about a particular condition or topic to select the answer; these was frequent related to \nparticularly esoteric subjects. ChatGPT seemed to frequently misinterpret questions involving \ncalculations and inheritance,  with 6 incorrect answers (22.2% of the initial 27 incorrect answers). For 2 \nquestions (7.4% of the initial 27 incorrect answers), ChatGPT appeared to misunderstand the question. \nFor the 2 questions that ChatGPT answered correctly initially and then incorrectly the second time, it \nprovided the correct answer and explanation when asked to provide the explanation.  \nDiscussion  \nWe were impressed with ChatGPT’s performance, including because its answers were almost \ninstantaneous, and were compared to humans who were able to look up answers and discuss the \nquestions through the social media posts (we note that we do not know to what extent respondents \nselected their answers without looking up information or asking others). ChatGPT performed better with \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \n7 \n \nmemorization-type questions versus critical thinking-type questions. This aligns with the consensus \nabout its performance, where the assumption is that AI is not yet fully better than humans at critical \nthinking (for example, as a response to the proliferation of ChatGPT in educational settings, some \nteachers are redesigning assignments to emphasize critical thinking). This may be a short-term fix; with \nthe ingestion of more data, ChatGPT may bridge this gap soon.  \nWe did not fine-tune the model, which would be expected to improve performance. We note that it is \nnot currently possible to know when ChatGPT is correct or not without additional verification – ChatGPT \nwill offer a plausible explanation for an incorrect answer and may at various times offer completely \ndifferent responses and explanations. In medical or other high-stakes settings, this is concerning. \nHowever, strategies like labeling responses with confidence scores, such as is available in AlphaFold, \nmay help. 7  \nOverall, approaches using LLMs have shown that they can perform relatively well in a variety of medical \ntesting situations. 4,5  These scores are sure to improve, perhaps rapidly, and tools like ChatGPT may \nbecome ubiquitous, much the way various forms of AI are currently used in clinical and research \ngenomic analyses every day.\n8,9  As one of many examples, patients and research subjects may \nincreasingly turn to ChatGPT and other such interfaces to learn about conditions that may affect them, \nincluding the suggested work-up and treatment. This may increase difficulties with issues like \nmisinformation.  \nIt seems fitting to offer ChatGPT the final words here; after describing the premise of this paper to \nChatGPT, we asked it to provide a concluding paragraph (query given 1/18/23 via the link in the \nIntroduction). It wrote the following: \n In conclusion, models like ChatGPT have the potential to greatly impact the field of clinical genetics by \nproviding rapid and accurate responses to a wide range of genetic-related questions. This can aid \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \n8 \n \nhealthcare professionals in the diagnosis and treatment of genetic disorders, as well as provide patients \nand their families with easily accessible information about their conditions. Additionally, ChatGPT's \nability to understand and respond to natural language queries could make genetic information more \nwidely available to a non-expert audience. As the field of genetics continues to advance, the use of \nnatural language processing models like ChatGPT will become increasingly important in both research \nand clinical settings. \nData Availability : All data used and presented are available in the manuscript and supporting \n(supplementary) files. \nAcknowledgments : This research was supported by the Intramural Research Program of the National \nHuman Genome Research Institute, National Institutes of Health.  \nAuthor Contributio ns : All authors contributed to the conception and design of the work and to \nacquisition, analysis, and interpretation of data, and both drafted or substantially revised the work.  \nCompeting Interests : The authors receive salary and research support from the intramural program of \nthe National Human Genome Research Institute. Benjamin D. Solomon is the co-Editor-in-Chief of the \nAmerican Journal of Medical Genetics, and has published some of the questions mentioned in this study \nin a book, as well as others. 10  Both editing/publishing activities are conducted as an approved outside \nactivity, separate from his US Government role. \nReferences  \n1 Ledgister Hanchard, S. E.  et al . Scoping review and classification of deep learning in medical \ngenetics. Gene t Me d  24, 1593-1603, doi:10.1016/j.gim.2022.04.025 (2022). \n2 Schaefer, J., Lehne, M., Schepers, J., Prasser, F. & Thun, S. The use of machine learning in rare \ndiseases: a scoping review. Orph ane t J R are Dis  15, 145, doi:10.1186/s13023-020-01424-6 \n(2020). \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \n9 \n \n3 Dias, R. & Torkamani, A. Artificial intelligence in clinical and genomic diagnostics. Geno me Me d  \n11, 70, doi:10.1186/s13073-019-0689-8 (2019). \n4 Singhal, K.  et al . Large Language Models Encode Clinical Knowledge. arXiv pre prin t \narXiv:2212.13138  (2022). \n5 Shelmerdine, S. C.  e t al. Can artificial intelligence pass the Fellowship of the Royal College of \nRadiologists examination? Multi-reader diagnostic accuracy study. BM J  379 , e072826, \ndoi:10.1136/bmj-2022-072826 (2022). \n6 Yang, X.  et al.  A large language model for electronic health records. NPJ Digit Med  5 , 194, \ndoi:10.1038/s41746-022-00742-2 (2022). \n7 Jumper, J.  e t al. Highly accurate protein structure prediction with AlphaFold. N atu re , \ndoi:10.1038/s41586-021-03819-2 (2021). \n8 Jaganathan, K.  e t al. Predicting Splicing from Primary Sequence with Deep Learning. Cell 176 , \n535-548 e524, doi:10.1016/j.cell.2018.12.015 (2019). \n9 Poplin, R. et al . A universal SNP and small-indel variant caller using deep neural networks. Na t  \nBiote ch nol 36, 983-987, doi:10.1038/nbt.4235 (2018). \n10 Solomon, B. D. Me dic al Ge neti cs an d Ge n omics: Quest ions for Bo ard Revi ew .  (Wiley, 2022). \n Table 1.  Performance of ChatGPT versus respondents. Unless otherwise noted, calculations here and \nbelow were done according to ChatGPT’s initial answers, when the questions were first posed.  \nTotal     \n  Correct   Incorre ct   Accura cy   \nTotal        \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint \n10 \n \nChatGPT  58  27  68.2%  \nRespondents  9448  4188  69.3%  \nMost common r esponse     \n  Correct   Incorre ct   Accura cy   \nChatGPT  58  27  68.2%  \nRespondents  78  7  91.8%  \n \nTable 2.  Comparison of results of memorization versus critical thinking questions.  \nMemorization  \n Correct  Incorre ct  Accura cy  \nChatGPT 53 13 80.3% \nRespondents 7132 2508 74.0% \nCritical thinking \n Correct  Incorre ct  Accura cy  \nChatGPT 5 14 26.3% \nRespondents 1943 2053 48.6% \n \nfor use under a CC0 license. \nThis article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 28, 2023. ; https://doi.org/10.1101/2023.01.27.23285115doi: medRxiv preprint ",
  "topic": "Memorization",
  "concepts": [
    {
      "name": "Memorization",
      "score": 0.7237472534179688
    },
    {
      "name": "Psychology",
      "score": 0.5213159918785095
    },
    {
      "name": "Field (mathematics)",
      "score": 0.46092861890792847
    },
    {
      "name": "Human language",
      "score": 0.45728936791419983
    },
    {
      "name": "Behavioural genetics",
      "score": 0.43536365032196045
    },
    {
      "name": "Cognitive psychology",
      "score": 0.35864630341529846
    },
    {
      "name": "Linguistics",
      "score": 0.2799140214920044
    },
    {
      "name": "Developmental psychology",
      "score": 0.21304267644882202
    },
    {
      "name": "Mathematics",
      "score": 0.09303006529808044
    },
    {
      "name": "Philosophy",
      "score": 0.08520901203155518
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}