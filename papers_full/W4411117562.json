{
    "title": "Automated Detection of Early-Stage Dementia Using Large Language Models: A Comparative Study on Narrative Speech",
    "url": "https://openalex.org/W4411117562",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5117246726",
            "name": "Kevin Mekulu",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2639127610",
            "name": "Faisal Aqlan",
            "affiliations": [
                "University of Louisville Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2010239524",
            "name": "Hui Yang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5117246726",
            "name": "Kevin Mekulu",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2639127610",
            "name": "Faisal Aqlan",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2010239524",
            "name": "Hui Yang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4206046172",
        "https://openalex.org/W2088517771",
        "https://openalex.org/W2551096483",
        "https://openalex.org/W4387186898",
        "https://openalex.org/W2895442547",
        "https://openalex.org/W4410040236",
        "https://openalex.org/W4409878326",
        "https://openalex.org/W4312091558",
        "https://openalex.org/W4385647263",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W3154733264",
        "https://openalex.org/W4200554941",
        "https://openalex.org/W2576151354",
        "https://openalex.org/W2074037951",
        "https://openalex.org/W2043146194",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6735236233",
        "https://openalex.org/W4389363313",
        "https://openalex.org/W4206774749"
    ],
    "abstract": "Abstract The growing global burden of dementia underscores the urgent need for scalable, objective screening tools. While traditional diagnostic methods rely on subjective assessments, advances in natural language processing offer promising alternatives. In this study, we compare two classes of language models—encoder-based pretrained language models (PLMs) and autoregressive large language models (LLMs) for detecting cognitive impairment from narrative speech. Using the DementiaBank Pitt Corpus and the widely used Cookie Theft picture description task, we evaluate BERT as a representative PLM alongside GPT-2, GPT-3.5 Turbo, GPT-4, and LLaMA-2 as LLMs. Although all models are pretrained, we distinguish PLMs and LLMs based on their architectural differences and training paradigms. Our findings reveal that BERT outperforms all other models, achieving 86% sensitivity and 95% specificity. LLaMA-2 follows closely, while GPT-4 and GPT-3.5 underperform in this structured classification task. Interestingly, LLMs demonstrate complementary strengths in capturing narrative richness and subtler linguistic features. These results suggest that hybrid modeling approaches may offer enhanced performance and interpretability. Our study highlights the potential of language models as digital biomarkers and lays the groundwork for scalable, AIpowered tools to support early dementia screening in clinical practice.",
    "full_text": "IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS 1\nAutomated Detection of Early-Stage Dementia\nUsing Large Language Models: A Comparative\nStudy on Narrative Speech\nKevin Mekulu, Faisal Aqlan,Member, IEEE, and Hui Y ang,Senior Member, IEEE\nAbstract— The growing global burden of dementia un-\nderscores the urgent need for scalable, objective screening\ntools. While traditional diagnostic methods rely on subjec-\ntive assessments, advances in natural language process-\ning offer promising alternatives. In this study, we com-\npare two classes of language models—encoder-based pre-\ntrained language models (PLMs) and autoregressive large\nlanguage models (LLMs) for detecting cognitive impairment\nfrom narrative speech. Using the DementiaBank Pitt Cor-\npus and the widely used Cookie Theft picture description\ntask, we evaluate BERT as a representative PLM alongside\nGPT-2, GPT-3.5 Turbo, GPT-4, and LLaMA-2 as LLMs. Al-\nthough all models are pretrained, we distinguish PLMs and\nLLMs based on their architectural differences and training\nparadigms. Our findings reveal that BERT outperforms all\nother models, achieving 86% sensitivity and 95% speci-\nficity. LLaMA-2 follows closely, while GPT-4 and GPT-3.5\nunderperform in this structured classification task. Inter-\nestingly, LLMs demonstrate complementary strengths in\ncapturing narrative richness and subtler linguistic features.\nThese results suggest that hybrid modeling approaches\nmay offer enhanced performance and interpretability. Our\nstudy highlights the potential of language models as dig-\nital biomarkers and lays the groundwork for scalable, AI-\npowered tools to support early dementia screening in clini-\ncal practice.\nIndex Terms— Large Language Models, Linguistic Pat-\ntern Recognition, Dementia Assessment, Neurocognitive\nDisorder\nI. INTRODUCTION\nN\nEurocognitive disorders, including Alzheimer’s, Parkin-\nson’s, and Huntington’s diseases, are becoming increas-\ningly prevalent global health challenges. These conditions lead\nto significant declines in cognitive functions such as language,\nperception, attention, and memory. Currently, approximately\n55 million people worldwide are living with dementia, and\nthis number is expected to rise to 139 million by 2050 [1].\nEarly diagnosis of dementia is crucial for effective treatment\nand improving patients’ quality of life. However, traditional\ndiagnostic tools—such as the Mini-Mental State Examination\nKevin Mekulu is with the Complex Systems Monitoring, Modeling and\nControl Laboratory at the Pennsylvania State University, University Park,\nPA 16802 USA (e-mail: kxm5924@psu.edu).\nFaisal Aqlan is the director of Center for Human Systems Engineer-\ning at the University of Louisville, Louisville, KY 40292 USA (e-mail:\nfaisal.aqlan@louisville.edu).\nHui Y ang is with the Complex Systems Monitoring, Modeling and\nControl Laboratory at the Pennsylvania State University, University Park,\nPA 16802 USA (e-mail: huiyang@psu.edu).\n(MMSE), Mini-Cog Test, and Montreal Cognitive Assessment\n(MoCA)—have notable limitations:\nFig. 1: Example of a dementia detection flow diagram using\nlanguage models\n• Subjectivity and Bias [2]: These assessments can be\ninfluenced by factors like educational background, cul-\ntural differences, and examiner bias, potentially leading\nto misdiagnoses.\n• Time and Cost Efficiency [3], [4]: Multiple consulta-\ntions and evaluations make the diagnostic process time-\nconsuming and expensive, placing a burden on both\npatients and healthcare systems.\nGiven these challenges, there is a pressing need for innova-\ntive, objective, and efficient diagnostic methods. Advances in\nArtificial Intelligence (AI), particularly in Natural Language\nProcessing (NLP), offer promising alternatives [5]. AI models\ncan analyze linguistic patterns in patients’ speech to detect\nsubtle signs of cognitive decline, enabling earlier and more\naccurate diagnoses [6].\nLarge Language Models (LLMs) like GPT-3 and GPT-4\nhave revolutionized NLP by effectively capturing complex lan-\nguage patterns and contextual nuances. Similarly, Pre-trained\nLanguage Models (PLMs) like BERT excel in understanding\nlanguage representations for tasks such as classification and\nrecognition. These models can identify subtle differences in\nspeech that indicate early cognitive impairments, making them\nsuitable for dementia detection.\nIn this study, we explore the effectiveness of various AI\nmodels in detecting dementia through linguistic analysis. Our\nkey contributions are:\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2025. ; https://doi.org/10.1101/2025.06.06.25329081doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\n1) Systematic Comparison of Language Model Architec-\ntures for Dementia Detection: While previous studies\nhave typically focused on single model types, we present\nthe first comprehensive comparison between PLMs and\nLLMs for dementia detection. By evaluating BERT\nagainst modern LLMs (GPT-2, GPT-3.5 Turbo, GPT-4,\nand LLaMA-2), we provide crucial insights into their\nrelative strengths and limitations in clinical applications.\n2) Novel Application of LLMs such as GPT-2, GPT3.5\nTurbo, LLaMA-2, GPT3.5 Turbo and GPT-4: While\nprevious studies have extensively examined models like\nBERT and GPT-3, our study is the first to evaluate GPT-\n2, GPT3.5 Turbo, LLaMA-2 and GPT-4 for dementia\ndetection. This adds to the existing body of knowledge\nby exploring newer architectures and their potential in\nclinical diagnostics.\nThe rest of the paper is organized as follows: Section II\nreviews related work and provides background on dementia\ndetection using AI models. Section III describes our methodol-\nogy, including data collection, preprocessing, and model fine-\ntuning. Section IV details the experimental design. Section\nV presents and discusses the results of our experiments, and\nSection VI concludes the paper.\nII. R ESEARCH BACKGROUND\nDementia is a neurocognitive disorder that impairs cognitive\nfunction, including memory, thinking, and problem-solving.\nIt is a chronic and progressive condition that can lead to\nsignificant disability and death. Alzheimer’s disease (AD) is\nthe most common form of dementia, accounting for 60-80%\nof all cases [1]. Early detection of dementia is crucial for\ntimely intervention and improved patient outcomes. However,\ntraditional diagnostic procedures, such as clinical interviews\nand paper-based neuropsychological tests, have their limita-\ntions. They can be time-consuming, subjective, and inaccurate,\nespecially in the early stages of the disease.\nIn recent years, the application of Natural Language Pro-\ncessing (NLP) techniques, particularly the fine-tuning of large\nlanguage models (LLMs) and pre-trained language models\n(PLMs), has emerged as a promising approach in the early\ndetection and classification of Alzheimer’s disease and related\ndementias (ADRD). This review examines recent advance-\nments in utilizing models such as BERT, GPT variants, and\nLLaMA for dementia classification, highlighting their method-\nologies, performance, and limitations. Prior work has explored\ninterpretable, lightweight models for dementia screening using\ncharacter-level symbolic features. Mekulu et al. introduced\nCharMark, a novel steady-state Markov modeling approach\nthat identifies linguistic biomarkers from character transitions\nin spontaneous speech [7]. Separately, symbolic recurrence\nanalysis has been used to reveal temporal linguistic patterns\nindicative of cognitive disruption [8].\nWhile initially designed for text generation, GPT-2 and its\nvariants have been adapted for dementia classification tasks.\nThese models offer unique advantages in capturing long-range\ndependencies in language, which can be useful for detecting\nsubtle cognitive changes [9]\nSeveral studies have investigated the use of LLMs for\ndementia detection. For example, Agvabor et al. [10] trained\nGPT-3 to predict dementia from spontaneous speech. The\nLLM achieved an accuracy of 80% in predicting dementia,\nwhich was significantly higher than the accuracy of traditional\nneuropsychological tests. Another study by Koga et al. [11]\nevaluated the performance of two LLMs, ChatGPT and Google\nBard, in generating diagnoses in clinicopathological confer-\nences of neurodegenerative disorders . The results showed\nthat both LLMs were able to generate accurate differential\ndiagnoses with high agreement with the consensus diagnoses\nof experienced neurologists. Overall, these evidences suggest\nthat LLMs have the potential to be a valuable tool for enhanc-\ning early dementia detection. They can be used to analyze\nlarge amounts of data from a variety of sources, and they can\nidentify subtle linguistic changes that may be indicative of\nearly dementia.\nThe introduction of LLaMA (Large Language Model Meta\nAI) and its successors has opened new avenues for dementia\nclassification research. These models, trained on vast amounts\nof data, potentially capture a wider range of linguistic patterns\n[12].\nOther types of language models have also been used for\ndementia detection. For example, pre-trained language models,\nsuch as BERT, have also been used to extract features from\nspeech and text that can be used to predict dementia. The pre-\ntrained language model BERT was applied to the ADReSS\nChallenge Dementia Detection Task and achieved state-of-the-\nart results, even though the model was not trained specifically\non dementia data [13]. This suggests that BERT can learn to\ndetect dementia from outside data, which is significant because\nit demonstrates the potential of pre-trained language models\nfor dementia detection.\nFurthermore, Adhikari et al. [14] developed a novel dataset\nof transcripts from AD patients and control subjects in the\nNepali language. They then applied a variety of NLP and ML\ntechniques to extract linguistic features from the transcripts.\nThese features were then used to train a machine learning\nclassifier to distinguish between AD patients and control sub-\njects. The authors achieved an accuracy of 94% in classifying\nAD patients and control subjects using a Naive Bayes (NB)\nclassifier. This result is promising, as it suggests that NLP\nand ML techniques can be used to develop effective tools for\nthe early detection of AD in low-resource languages. Another\ninvestigation by Orimaye et al. [15] showcased the develop-\nment of a machine learning model to predict probable AD\nusing linguistic deficits, lexical and n-gram features extracted\nfrom spontaneous speech transcripts using NLP techniques .\nRecent advancements in LLMs, such as GPT-4 and LLaMA-\n2, have further expanded the capabilities of AI in under-\nstanding and generating human language. These models, with\ntheir increased parameter sizes and sophisticated training\nmethodologies, offer enhanced performance in capturing the\nintricacies of human speech, making them even more suitable\nfor applications in medical diagnostics. The integration of\ndomain-specific knowledge into these models, coupled with\nfine-tuning on specialized datasets like DementiaBank’s Pitt\nCorpus, holds the promise of achieving higher diagnostic\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2025. ; https://doi.org/10.1101/2025.06.06.25329081doi: medRxiv preprint \nMEKULU et al.:AUTOMATED DEMENTIA DETECTION 3\naccuracy and reliability.\nThis study builds upon existing research by systematically\nevaluating and fine-tuning multiple LLMs on a comprehensive\ndataset, aiming to identify the most effective models for early\ndementia detection through linguistic analysis. By comparing\nmodels of varying complexities and architectures, we seek to\nunderstand the strengths and limitations of each, providing\ninsights into their practical applications in clinical settings.\nAt the core of our research lies the DementiaBank’s Pitt\nCorpus [16], a comprehensive repository of audio recordings\nand transcripts from individuals diagnosed with Alzheimer’s\nDisease (AD) and healthy controls (HC). This corpus pro-\nvided a robust foundation for our study, encompassing 310\ntranscripts from 168 AD patients and 242 transcripts from 98\nHC subjects. Each participant engaged in the “Cookie Theft”\npicture description task (see Figure 3), a standard procedure\ndesigned to elicit spontaneous speech and uncover underlying\ncognitive processes [17].\nIII. M ETHODOLOGY\nThis section details our systematic approach to dementia\ndetection using language models. Figure 2 illustrates our\nend-to-end pipeline, from data preprocessing through model\nevaluation, designed to ensure robust and reproducible results.\nA. Data Preprocessing\nEnsuring the quality and relevance of our data was\nparamount. We initiated our analysis by preprocessing the\ntranscripts. This involved several key steps:\n1) Transcript Cleaning: We removed all clinician prompts\nand questions to focus exclusively on the participants’\nnatural speech patterns. This purification was essential for\ncapturing authentic linguistic behaviors without external\ninfluences.\n2) Text Normalization: To maintain consistency, all text\nwas converted to lowercase, and punctuation and special\ncharacters were stripped away. This standardization mini-\nmized discrepancies and facilitated smoother downstream\nprocessing.\n3) Retention of Stop Words: Contrary to common NLP\npractices, we retained stop words—such as “and”, “the”,\nand “is”. Preliminary observations indicated that AD sub-\njects frequently exhibit repetitive usage of these words,\nserving as indicators of cognitive decline [18].\n4) Word Cloud Generation: To visualize the linguistic\nlandscape, we generated word clouds for AD patients\nand HC subjects (see Figure 4). These visualizations\nhighlighted the most frequently used words in each group,\nproviding immediate insights into distinct language pat-\nterns.\nB. Computational Models\nOur analytical framework leverages both Pre-trained Lan-\nguage Models (PLMs) and Large Language Models (LLMs),\neach bringing unique strengths to dementia detection.\n1) Pre-trained Language Models (PLMs):Bidirectional En-\ncoder Representations from Transformers (BERT) [19]\nstands out for its deep bidirectional understanding of language\ncontext, a crucial capability for detecting subtle linguistic\npatterns associated with cognitive decline. Unlike unidirec-\ntional models that process text either left-to-right or right-to-\nleft, BERT’s bidirectional architecture enables it to capture\ncomplex dependencies in both directions simultaneously.\nFor a given input transcript X = [x1, x2, ..., xn], BERT first\napplies WordPiece tokenization and adds special tokens:\nXinput = [[CLS], x1, x2, ..., xn, [SEP]] (1)\nwhere [CLS] serves as an aggregate sequence representation\nfor classification tasks, and [SEP] marks sequence boundaries.\nThis structured input format enables BERT to learn contextual\nrelationships across the entire transcript.\nThe model processes this input through L=12 transformer\nlayers (in bert-base-uncased), where each layer combines\nmulti-head self-attention and feed-forward networks:\nHl = LayerNorm(MHA(Hl−1) +Hl−1) (2)\nHl = LayerNorm(FFN(Hl) +Hl) (3)\nThe multi-head attention mechanism, crucial for capturing\nvarious aspects of linguistic patterns, is computed as:\nMHA(H) =Concat(head1, ...,headh)WO (4)\nwhere each attention head focuses on different aspects of the\ninput:\nheadi = Attention(HW Q\ni , HWK\ni , HWV\ni ) (5)\nFor dementia detection, we leverage BERT’s [CLS] token\nrepresentation through a classification layer:\nP(c|X) =softmax(Wc · BERT(X[CLS]) +bc) (6)\nwhere Wc ∈ Rh×2 and bc ∈ R2 are trainable parameters\n(h = 768for bert-base), and c ∈ {0,1} represents the binary\nclassification outcome. This architecture enables the model to\naggregate information across the entire transcript for making\ndiagnostic predictions.\nThe model’s objective during fine-tuning is optimized using\ncross-entropy loss:\nL = − 1\nN\nNX\ni=1\n[yi log(pi) + (1− yi) log(1− pi)] (7)\nwhere N is the batch size, yi is the true label, and pi is\nthe predicted probability of dementia. This loss function is\nparticularly suitable for our binary classification task as it\npenalizes both false positives and false negatives, crucial for\nclinical applications.\n2) Large Language Models (LLMs): We investigated four\nstate-of-the-art LLMs—GPT-2 (1.5B parameters) , GPT-3.5\nTurbo (175B parameters) [20], GPT-4 (1.76T parameters) [21],\nand LLaMA-2 (7B parameters)—for dementia classification.\nThese models leverage transformer-based architectures and\nextensive pre-training to capture long-range dependencies and\nsubtle semantic patterns in language, characteristics particu-\nlarly relevant for detecting the linguistic markers of cognitive\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2025. ; https://doi.org/10.1101/2025.06.06.25329081doi: medRxiv preprint \n4 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\nFig. 2: Flow diagram of the proposed methodology\nFig. 3: Boston Cookie Theft Picture\nFig. 4: Word clouds for (a) Dementia subjects and (b) Healthy\ncontrol Subjects\ndecline. Due to architectural and accessibility differences, we\nemployed distinct optimization strategies across our LLM im-\nplementations. For open-source models (GPT-2 and LLaMA-\n2), we performed complete fine-tuning of model parameters\nfor the dementia detection task. For GPT-3.5 Turbo, accessed\nthrough OpenAI’s API, we implemented few-shot learning\n[22] with carefully engineered prompts. GPT-4, accessed\nthrough its web interface, utilized a zero-shot classification\napproach [23] where each transcript was analyzed based\non the model’s inherent understanding of linguistic patterns\nassociated with cognitive decline.\nIV. EXPERIMENTAL DESIGN\nOur experimental design systematically evaluates the per-\nformance of Pre-trained Language Models (PLMs) and Large\nLanguage Models (LLMs) in classifying dementia through\nspeech transcript analysis. This section details our structured\napproach, encompassing dataset preparation, model training\nconfigurations, and comprehensive evaluation metrics.\nA. Dataset Splitting\nOur dataset comprised 552 transcripts (310 from AD pa-\ntients and 242 from healthy controls). To ensure robust model\nevaluation, we employed a stratified sampling technique that\npreserved the original class distribution. The data was split\ninto:\n• Training set (80%): 442 transcripts (248 AD, 194 HC)\n• Validation set (20%): 110 transcripts (62 AD, 48 HC)\nThis stratified approach maintained the proportion of AD\nand HC classes across both sets, mitigating potential class\nimbalance issues and enhancing model generalizability to\nunseen data. The validation set was held out during training\nand used exclusively for performance evaluation.\nB. Model Training and Fine-Tuning\nEach computational model—ranging from PLMs like BERT\nto LLMs such as GPT-2, GPT-3.5 Turbo, GPT-4, and LLaMA-\n2—underwent a fine-tuning process optimized to leverage their\nunique architectures and strengths.\n1) Pre-trained Language Models (PLMs):We implemented\nBERT fine-tuning using the “bert-base-uncased” variant with\nAutoModelForSequenceClassification from the Transformers\nlibrary. For reproducibility, we initialized all random seeds\nto 42 across PyTorch, NumPy, and Python’s random library\ncomponents.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2025. ; https://doi.org/10.1101/2025.06.06.25329081doi: medRxiv preprint \nMEKULU et al.:AUTOMATED DEMENTIA DETECTION 5\nFig. 5: BERT Fine Tuning Flow Diagram\nOur implementation pipeline consisted of several key stages.\nFirst, we developed a preprocessing workflow that preserved\nlinguistic markers by retaining punctuation and special charac-\nters, as these could indicate cognitive state. Text tokenization\nwas performed with a maximum sequence length of 512\ntokens, using dynamic padding for batch processing efficiency.\nThe fine-tuning process utilized a custom training configura-\ntion optimized for our binary classification task. We monitored\nmodel performance through validation after each epoch, im-\nplementing model checkpointing to save the best-performing\nversions based on accuracy metrics. The training process was\nconfigured with the hyperparameters shown in Table I, chosen\nto balance model convergence with computational efficiency.\nTABLE I: BERT Model Fine-Tuning Training Arguments\nParameter Value\nNumber of Epochs 6\nBatch Size (Train) 8\nBatch Size (Eval) 8\nWarmup Steps 500\nWeight Decay 0.01\nLogging Steps 10\nEvaluation Strategy Epoch\nSave Strategy Epoch\nLoad Best Model at End True\nMetric for Best Model Accuracy\nTo prevent overfitting, we implemented early stopping with\nmodel checkpointing, saving the best model state based on\nvalidation accuracy. Training progress was logged every 10\nsteps, allowing for detailed performance analysis and potential\nhyperparameter adjustments.\n2) Large Language Models (LLMs):We evaluated a diverse\nset of LLMs, ranging from 1.5B to 175B parameters, employ-\ning model-specific optimization strategies based on architec-\nture and accessibility constraints. For GPT-2 and LLaMA-2,\nwe implemented full model fine-tuning, while GPT-3.5 Turbo\nwas accessed through APIs with prompt engineering. Table\nII details the implementation specifications for each model,\nhighlighting our differentiated approach to model optimization\nand deployment.\n• GPT-2: We adapted the base GPT-2 model for sequence\nclassification using GPT2ForSequenceClassification, im-\nplementing a custom PyTorch dataset architecture to\nhandle the unique requirements of dementia detection.\nOur dataset implementation featured dynamic sequence\nhandling with a fixed block size of 384 tokens, utilizing\nthe model’s EOS token for padding operations. This\ndesign ensured efficient processing of variable-length\ninputs while maintaining consistent batch dimensions\nthroughout training.\nThe training pipeline was optimized with a configuration\nof three epochs and a batch size of 8 for both training\nand evaluation phases. We implemented a comprehensive\nmonitoring system with validation checks after each\nepoch and logging at 10-step intervals. Model selection\nwas automated based on accuracy metrics, incorporating\nearly stopping with checkpointing to prevent overfitting\nand preserve optimal model states.\nText preprocessing followed a systematic approach, em-\nploying regex-based cleaning with the pattern ’[ ˆa-zA-\nZ0-9]’ to standardize inputs. This included lowercase\nconversion and careful handling of special characters\nand punctuation. Each preprocessing step was validated\nto ensure it preserved essential linguistic features while\nremoving noise that could impact classification perfor-\nmance. The attention mask generation was specifically\ndesigned to handle variable-length inputs, ensuring the\nmodel focused only on relevant token sequences during\ninference.\n• LLaMA-2: Our implementation utilized the “Llama-2-\n7b-hf” variant, optimized for efficient inference through\nbfloat16 precision, which significantly reduced memory\nrequirements while maintaining computational accuracy.\nThe model deployment leveraged automatic device map-\nping to optimize resource utilization across available\nhardware.\nThe preprocessing pipeline was designed for robust\ntext normalization, incorporating lowercase conversion,\nwhitespace standardization, and systematic handling of\nspecial characters. This comprehensive approach ensured\nconsistent input formatting across all samples, critical for\nreliable model performance. The cleaned text was then\nprocessed using a structured dialogue format designed to\nelicit binary classification decisions.\nOur prompt engineering approach followed a human-\nassistant interaction paradigm, with the explicit task de-\nscription: “The given speech transcript is either from\na healthy subject or a diseased subject. Categorize it\nas one of them.” This format was chosen to leverage\nLLaMA-2’s training on conversational data. To maintain\nefficiency in the inference pipeline, we implemented early\nstopping with a maximum of three new tokens, sufficient\nfor binary classification responses. The entire process\nwas streamlined using HuggingFace’s Dataset library for\nefficient data management and model interaction.\n• GPT-3.5 Turbo: We implemented a few-shot learning\napproach through OpenAI’s API, utilizing a carefully\ncrafted prompt engineering strategy. The model was ini-\ntialized with a role-based context that framed it as an\nassistant analyzing conversation transcripts for signs of\ncognitive impairment. Our prompt structure incorporated\nfive exemple cases for few-shot learning, with explicit\ninstructions to default to “diseased subject” classification\nwhen context proved insufficient. To ensure consistency\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2025. ; https://doi.org/10.1101/2025.06.06.25329081doi: medRxiv preprint \n6 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\nin results, we set the temperature parameter to 0, enabling\ndeterministic outputs.\nThe implementation required robust preprocessing of\ntranscripts, including regex-based cleaning and text wrap-\nping at 100 characters for optimal prompt formatting.\nTo manage API interactions efficiently, we developed\na batched processing system that handled 50 samples\nper batch, with built-in timeout controls (20 seconds per\nrequest) and rate-limiting delays (10 seconds between\nbatches) to ensure reliable data collection.\nFor response processing, we implemented a system-\natic approach to transform model outputs into binary\nclassifications. This included robust error handling for\nAPI failures and consistent label mapping for evaluation\npurposes. The entire pipeline was designed to maximize\nthroughput while maintaining reliability, with particular\nattention to OpenAI’s API constraints and best practices\nfor large-scale inference tasks.\n• GPT-4: Our implementation utilized the web interface\nwith a zero-shot classification approach. For each tran-\nscript, we prompted the model to perform linguis-\ntic analysis based on its pre-trained understanding of\ndementia-related language patterns. The prompt structure\nmaintained consistency across all samples, requesting\nbinary classification (healthy/diseased) while leveraging\nGPT-4’s inherent knowledge of cognitive decline mark-\ners in speech. This approach differed from traditional\nfine-tuning or few-shot learning, instead relying on the\nmodel’s built-in capabilities to identify linguistic features\nassociated with cognitive impairment. All interactions\nwere conducted with default temperature settings to bal-\nance confidence and variability in the model’s responses.\nTABLE II: Implementation Details of Language Models\nModel Approach Implementation Details\nGPT-2 Fine-tuning • Custom classification head\n- 3 training epochs\n- Batch size = 8\nLLaMA-2 Instruction-\ntuning\n• bfloat16 precision\n- Device-mapped inference\n- Structured prompts\nGPT-3.5 Few-shot\n(n=5)\n• Temperature = 0\n- 20s timeout\n- Example-based prompts\nGPT-4 Zero-shot • Web interface\n- Consistent prompting\n- Binary classification\nC. Evaluation Metrics\nTo comprehensively assess the performance of each model,\nwe employed clinically relevant evaluation metrics that pro-\nvide a multifaceted view of their diagnostic capabilities. Let\nTP, TN, FP, and FN denote True Positives, True Negatives,\nFalse Positives, and False Negatives respectively. The follow-\ning metrics were calculated:\nAccuracy = TP + TN\nTP + TN + FP + FN (8)\nwhich measures the overall proportion of correct predictions.\nFor clinical performance evaluation:\nSensitivity = TP\nTP + FN (9)\nmeasures the model’s capability to correctly identify patients\nwith dementia (also known as recall or true positive rate), and:\nSpecificity = TN\nTN + FP (10)\nindicates the model’s ability to correctly identify healthy\ncontrols (true negative rate). In clinical settings, both sensi-\ntivity and specificity >= 80% are considered state-of-the-art\nbenchmarks for diagnostic tools [24]. To provide a balanced\nmeasure of performance, we calculated the F1-score:\nF1-Score = 2· Sensitivity · Precision\nSensitivity + Precision (11)\nwhere Precision = TP/(TP + FP) complements sensitivity by\nmeasuring the proportion of correct positive predictions.\nV. EXPERIMENTAL RESULTS AND DISCUSSION\nOur comprehensive evaluation of both PLMs and LLMs\nrevealed distinct patterns in their performance for dementia\ndetection. Table III presents the detailed performance metrics\nacross all models, while Figure 6 provides a visual compari-\nson.\nTABLE III: Performance Comparison of Language Models\nModel Accuracy Sensitivity Specificity F1-Score\n(%) (%) (%) (%)\nBERT 91 86 95 92\nLLaMA-2 80 86 89 83\nGPT-2 77 82 76 80\nGPT-4 73 76 82 78\nGPT-3.5 50 54 81 65\nFig. 6: Performance comparison of language models for\nDementia detection\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2025. ; https://doi.org/10.1101/2025.06.06.25329081doi: medRxiv preprint \nMEKULU et al.:AUTOMATED DEMENTIA DETECTION 7\nA. Model Performance Analysis\n1) Pre-trained Language Models (PLMs): BERT demon-\nstrated superior performance across most metrics, achieving\nthe highest accuracy (91%), precision (98%), and F1-score\n(92%). This exceptional performance can be attributed to:\n• Effective capture of bidirectional context in language\npatterns\n• Successful fine-tuning on the specific task of dementia\ndetection\n• Robust handling of the structured nature of clinical text\nclassification\n2) Large Language Models (LLMs):Among the LLMs, per-\nformance varied significantly:\n• LLaMA-2 emerged as the strongest LLM performer,\nachieving 80% accuracy and maintaining high precision\n(91%) and recall (86%). Its strong performance suggests\nthat open-source LLMs can be effectively adapted for\nclinical applications.\n• GPT-2 showed moderate performance with balanced met-\nrics (accuracy: 77%, F1-score: 80%), demonstrating the\ncapability of smaller LLMs in specialized tasks.\n• GPT-4 achieved respectable performance (accuracy:\n73%, F1-score: 78%) despite being used in a prompt-\nbased approach rather than fine-tuning.\n• GPT-3.5 Turbo showed lower performance (accuracy:\n50%), potentially due to limitations in the prompt-based\napproach for this specific clinical task.\nB. Clinical Relevance and State-of-the-Art Comparison\nIn the context of clinical dementia detection, sensitivity\n(recall) and specificity at or above 80% are considered state-\nof-the-art benchmarks. Our results demonstrate that several\nmodels meet or exceed these clinical standards:\n• BERT achieves 86% sensitivity and 95% specificity,\nsurpassing clinical benchmarks\n• LLaMA-2 matches the clinical standard with 86% sen-\nsitivity and 89% specificity\n• GPT-2 shows promising clinical utility with 82% sensi-\ntivity, though specificity (76%) falls slightly below the\nbenchmark\nThese results are particularly significant as they demonstrate\nthat language models can achieve clinically relevant perfor-\nmance levels, potentially supporting earlier and more accurate\ndiagnoses.\nC. Key Findings\nSeveral important insights emerge from our experimental\nresults:\nPLM Superiority in Structured Tasks: BERT’s out-\nstanding performance validates the effectiveness of PLMs in\nstructured clinical classification tasks, particularly when fine-\ntuned on domain-specific data. LLM Adaptability: The strong\nperformance of LLaMA-2 demonstrates that larger language\nmodels can be effectively adapted for specialized medical tasks\nwhen proper fine-tuning is possible. Prompt Engineering\nLimitations: The relatively lower performance of GPT-3.5\nTurbo and GPT-4 highlights the challenges of using prompt-\nbased approaches compared to full model fine-tuning in clin-\nical applications. Clinical Performance Metrics: Multiple\nmodels in our study achieved sensitivity and specificity above\n80%, meeting established clinical standards for diagnostic\ntools. This is particularly noteworthy for BERT and LLaMA-2,\nwhich maintained high performance across all clinical metrics.\nPrecision-Recall Balance: All models demonstrated varying\ntrade-offs between precision and recall, with BERT achieving\nthe most balanced performance while maintaining clinically\nrelevant thresholds.\nThese results suggest that while PLMs currently offer su-\nperior performance for structured clinical tasks, both PLMs\nand LLMs can achieve clinically viable performance levels.\nThe findings emphasize the potential for these AI models\nto complement existing diagnostic tools in clinical settings,\nparticularly given their ability to meet or exceed established\nclinical performance benchmarks. Furthermore, the strong\nperformance across multiple models suggests robustness in\nthe approach, strengthening the case for their integration into\nclinical practice.\nVI. C ONCLUSIONS\nThis study presents a comprehensive evaluation of both Pre-\ntrained Language Models (PLMs) and Large Language Models\n(LLMs) for automated dementia detection through linguistic\nanalysis. Our findings demonstrate that while both model\ntypes show promise, PLMs—specifically BERT—currently\noffer superior performance for this specialized clinical task.\nThe key conclusions drawn from our research are:\n1) BERT achieved exceptional performance with 91% ac-\ncuracy, 98% precision, and 86% recall, surpassing tra-\nditional clinical benchmarks (80%) and establishing a\nnew standard for automated dementia detection. This\nperformance suggests that smaller, task-specific models\ncan outperform larger, more general-purpose language\nmodels in specialized clinical applications.\n2) Among LLMs, LLaMA-2 emerged as the strongest per-\nformer, achieving 80% accuracy and maintaining clin-\nically viable sensitivity and specificity. This finding is\nparticularly significant as it demonstrates the potential\nof open-source LLMs in healthcare applications when\nproperly fine-tuned.\n3) The performance gap between fine-tuned models (BERT,\nLLaMA-2) and prompt-based approaches (GPT-3.5\nTurbo, GPT-4) highlights the importance of task-specific\noptimization in clinical applications. This suggests that\nwhile prompt engineering offers flexibility, fine-tuning\nremains crucial for achieving optimal performance in\nspecialized medical tasks.\nOur research has important implications for the future of\nautomated cognitive assessment. The strong performance of\nthese models, particularly in meeting or exceeding clinical\nbenchmarks, suggests that AI-driven tools could serve as valu-\nable screening mechanisms in clinical settings, enabling earlier\ndetection and intervention in dementia cases. In conclusion,\nwhile both PLMs and LLMs demonstrate potential for de-\nmentia detection, our findings suggest that carefully fine-tuned\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2025. ; https://doi.org/10.1101/2025.06.06.25329081doi: medRxiv preprint \n8 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\nPLMs currently offer the most promising path forward for\nclinical applications. As these technologies continue to evolve,\nthey may increasingly serve as powerful tools to support\nclinicians in early dementia detection, ultimately contributing\nto improved patient outcomes through earlier intervention and\nmore accurate diagnosis.\nACKNOWLEDGMENTS\nThe authors of this work would like to acknowledge the\nNSF grants IIS-2302834 and MCB-1856132 for funding this\nresearch.Any opinions, findings, or conclusions found in this\npaper originate from the authors and do not necessarily reflect\nthe views of the sponsor.\nREFERENCES\n[1] P. Scheltens, B. De Strooper, M. Kivipelto, H. Holstege, G. Ch ´etelat,\nC. E. Teunissen, J. Cummings, and W. M. van der Flier, “Alzheimer’s\ndisease,” The Lancet, vol. 397, no. 10284, p. 1577–1590, Apr 2021.\n[2] R. F. Uhlmann and E. B. Larson, “Effect of education on the mini-\nmental state examination as a screening test for dementia,” Journal of\nthe American Geriatrics Society, vol. 39, no. 9, pp. 876–880, 9 1991.\n[3] D. McMillan, E. Hickey, M. U. Patel, and C. Mitchell, “Cost effec-\ntiveness of using cognitive screening tests for detecting dementia and\nmild cognitive impairment in primary care,” International Journal of\nGeriatric Psychiatry, vol. 31, no. 11, pp. 1176–1185, 11 2016.\n[4] J. Shore, C. Kalafatis, A. Stainthorpe, M. Modarres, and S.-M. Khaligh-\nRazavi, “Health economic analysis of the integrated cognitive assess-\nment tool to aid dementia diagnosis in the united kingdom,” Frontiers\nin Public Health, vol. 11, p. 1240901, 9 2023.\n[5] E. Ambrosini, S. Ferrante, and A. Pedrocchi, “Digital biomarkers for\nremote health monitoring: A systematic review,” Journal of Biomedical\nInformatics, vol. 139, p. 104386, 2024.\n[6] C. Cheng and H. Yang, “Multi-scale graph modeling and analysis of\nlocomotion dynamics towards sensor-based dementia assessment,” IISE\nTransactions on Healthcare Systems Engineering, vol. 9, no. 1, p.\n95–102, Jan 2019.\n[7] K. Mekulu, F. Aqlan, H. Yang et al., “Charmark: Character-level\nmarkov modeling to detect linguistic signs of dementia,” Research\nSquare, 2025, preprint. [Online]. Available: https://doi.org/10.21203/rs.\n3.rs-6391300/v1\n[8] ——, “Symbolic recurrence: A framework for linguistic biomarker\ndiscovery in speech,” Research Square, 2025, preprint. [Online].\nAvailable: https://doi.org/10.21203/rs.3.rs-6356840/v1\n[9] A. Radford, “Improving language understanding by generative pre-\ntraining,” 2018.\n[10] F. Agbavor and H. Liang, “Predicting dementia from spontaneous speech\nusing large language models,” PLOS Digital Health, vol. 1, no. 12, 2022.\n[11] S. Koga, N. B. Martin, and D. W. Dickson, “Evaluating the performance\nof large language models: ChatGPT and google Bard in generating\ndifferential diagnoses in clinicopathological conferences of neurodegen-\nerative disorders,” Brain Pathology, 2023.\n[12] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\n2: Open foundation and fine-tuned chat models,” arXiv preprint\narXiv:2307.09288, 2023.\n[13] Y . Guo, C. Li, C. Roan, S. Pakhomov, and T. Cohen, “Crossing the\n“cookie theft” corpus chasm: Applying what BERT learns from outside\ndata to the adress challenge dementia detection task,” Frontiers in\nComputer Science, vol. 3, 2021.\n[14] S. Adhikari, S. Thapa, U. Naseem, P. Singh, H. Huo, G. Bharathy, and\nM. Prasad, “Exploiting linguistic information from nepali transcripts for\nearly detection of alzheimer’s disease using natural language processing\nand machine learning techniques,” International Journal of Human-\nComputer Studies, vol. 160, p. 102761, 2022.\n[15] S. O. Orimaye, J. S.-M. Wong, K. J. Golden, C. P. Wong, and I. N.\nSoyiri, “Predicting probable alzheimer’s disease using linguistic deficits\nand biomarkers,” BMC Bioinformatics, vol. 18, no. 1, 2017.\n[16] J. T. Becker, “The natural history of alzheimer’s disease,” Archives of\nNeurology, vol. 51, no. 6, p. 585, 1994.\n[17] H. Goodglass, E. Kaplan, and B. Barresi, The assessment of aphasia\nand related disorders, 3rd ed. Philadelphia: Lippincott Williams &\nWilkins, 2001.\n[18] A. Khodabakhsh, S. Kus ¸xuo ˘glu, and C. Demiro ˘glu, “Natural language\nfeatures for detection of alzheimer’s disease in conversational speech,”\nin IEEE-EMBS International Conference on Biomedical and Health\nInformatics (BHI). IEEE, 2014, pp. 581–584.\n[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” 2019.\n[20] “GPT-3.5 Turbo fine-tuning and API updates — openai.com,” https://\nopenai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates, [Accessed\n25-01-2024].\n[21] OpenAI, “GPT-4 Technical Report,” OpenAI, Tech. Rep., March 2023.\n[Online]. Available: https://arxiv.org/abs/2303.08774\n[22] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-\nshot learning,” in Advances in Neural Information Processing Systems,\nvol. 30, 2017.\n[23] Z. Wang et al., “Large language models are zero-shot text classifiers,”\narXiv preprint arXiv:2312.01044, Dec 2023.\n[24] M. S. Chong, C. S. Tan, W. S. Lim, T. P. Ng, P. Yap, P. H. Ong, C. H.\nTan, L. Tay, and W.-S. Lim, “Diagnostic accuracy of ascertain dementia\n8-item questionnaire by informants and participants for cognitive im-\npairment: A systematic review and meta-analysis,” PLoS One, vol. 18,\nno. 9, p. e0290201, 9 2023.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2025. ; https://doi.org/10.1101/2025.06.06.25329081doi: medRxiv preprint "
}