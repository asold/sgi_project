{
    "title": "Transformer encoder with multiscale deep learning for pain classification using physiological signals",
    "url": "https://openalex.org/W4389397172",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2112804074",
            "name": "Zhenyuan Lü",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2337049525",
            "name": "Burcu Ozek",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A1205784333",
            "name": "Sagar Kamarthi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112804074",
            "name": "Zhenyuan Lü",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2337049525",
            "name": "Burcu Ozek",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4242743388",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2944277248",
        "https://openalex.org/W6805058275",
        "https://openalex.org/W2893712465",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W3127209096",
        "https://openalex.org/W4386935684",
        "https://openalex.org/W2800845328",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3158818505",
        "https://openalex.org/W6788477294",
        "https://openalex.org/W2929649109",
        "https://openalex.org/W6750462490",
        "https://openalex.org/W2907100627",
        "https://openalex.org/W3179951106",
        "https://openalex.org/W4205167947",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W6638667902",
        "https://openalex.org/W4205949815",
        "https://openalex.org/W4295756781",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W4232643944",
        "https://openalex.org/W6725336789",
        "https://openalex.org/W1987330079",
        "https://openalex.org/W6676071220",
        "https://openalex.org/W2767549628",
        "https://openalex.org/W2518599539",
        "https://openalex.org/W3006560451",
        "https://openalex.org/W2963665779",
        "https://openalex.org/W4389397172",
        "https://openalex.org/W6769404030",
        "https://openalex.org/W3120625521",
        "https://openalex.org/W3164829239",
        "https://openalex.org/W1560013893",
        "https://openalex.org/W4376641680",
        "https://openalex.org/W6637242042",
        "https://openalex.org/W4386524310",
        "https://openalex.org/W3000384844",
        "https://openalex.org/W4307189193",
        "https://openalex.org/W3182971338",
        "https://openalex.org/W3084334922",
        "https://openalex.org/W6839312722",
        "https://openalex.org/W3085663053",
        "https://openalex.org/W2980888565",
        "https://openalex.org/W2423557781",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W1985941926",
        "https://openalex.org/W6782152412",
        "https://openalex.org/W6649554860",
        "https://openalex.org/W2979431738",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W2769991365",
        "https://openalex.org/W3034390803",
        "https://openalex.org/W4394699135",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W2107789863",
        "https://openalex.org/W2510642588",
        "https://openalex.org/W4200498090",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W3082766220",
        "https://openalex.org/W2980309919",
        "https://openalex.org/W1996089789",
        "https://openalex.org/W2519091744",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4308223420",
        "https://openalex.org/W2963488291",
        "https://openalex.org/W1665214252",
        "https://openalex.org/W2306394264",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "Pain, a pervasive global health concern, affects a large segment of population worldwide. Accurate pain assessment remains a challenge due to the limitations of conventional self-report scales, which often yield inconsistent results and are susceptible to bias. Recognizing this gap, our study introduces PainAttnNet, a novel deep-learning model designed for precise pain intensity classification using physiological signals. We investigate whether PainAttnNet would outperform existing models in capturing temporal dependencies. The model integrates multiscale convolutional networks, squeeze-and-excitation residual networks, and a transformer encoder block. This integration is pivotal for extracting robust features across multiple time windows, emphasizing feature interdependencies, and enhancing temporal dependency analysis. Evaluation of PainAttnNet on the BioVid heat pain dataset confirm the model’s superior performance over the existing models. The results establish PainAttnNet as a promising tool for automating and refining pain assessments. Our research not only introduces a novel computational approach but also sets the stage for more individualized and accurate pain assessment and management in the future.",
    "full_text": "TYPE Original Research\nPUBLISHED 06 December 2023\nDOI 10.3389/fphys.2023.1294577\nOPEN ACCESS\nEDITED BY\nKuanquan Wang,\nHarbin Institute of T echnology, China\nREVIEWED BY\nChi-Wen Lung,\nAsia University, Taiwan\nZean Liu,\nChina North Industries Group\nCorporation, China\n*CORRESPONDENCE\nSagar Kamarthi,\ns.kamarthi@northeastern.edu\nRECEIVED 14 September 2023\nACCEPTED 16 November 2023\nPUBLISHED 06 December 2023\nCITATION\nLu Z, Ozek B and Kamarthi S (2023),\nT ransformer encoder with multiscale\ndeep learning for pain classification\nusing physiological signals.\nFront. Physiol. 14:1294577.\ndoi: 10.3389/fphys.2023.1294577\nCOPYRIGHT\n© 2023 Lu, Ozek and Kamarthi. This is an\nopen-access article distributed under\nthe terms of the Creative Commons\nAttribution License (CC BY) . The use,\ndistribution or reproduction in other\nforums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution\nor reproduction is permitted which does\nnot comply with these terms.\nTransformer encoder with\nmultiscale deep learning for pain\nclassification using physiological\nsignals\nZhenyuan Lu , Burcu Ozek and Sagar Kamarthi *\nDepartment of Mechanical and Industrial Engineering, Northeastern University, Boston, MA,\nUnited States\nPain, a pervasive global health concern, affects a large segment of population\nworldwide. Accurate pain assessment remains a challenge due to the limitations\nof conventional self-report scales, which often yield inconsistent results\nand are susceptible to bias. Recognizing this gap, our study introduces\nPainAttnNet, a novel deep-learning model designed for precise pain intensity\nclassification using physiological signals. We investigate whether PainAttnNet\nwould outperform existing models in capturing temporal dependencies. The\nmodel integrates multiscale convolutional networks, squeeze-and-excitation\nresidual networks, and a transformer encoder block. This integration is pivotal\nfor extracting robust features across multiple time windows, emphasizing feature\ninterdependencies, and enhancing temporal dependency analysis. Evaluation\nof PainAttnNet on the BioVid heat pain dataset confirm the model’s superior\nperformance over the existing models. The results establish PainAttnNet as a\npromising tool for automating and refining pain assessments. Our research not\nonly introduces a novel computational approach but also sets the stage for more\nindividualized and accurate pain assessment and management in the future.\nKEYWORDS\npain intensity classification, multiscale convolutional networks, transformer encoder,\nsqueeze-and-excitation residual network, deep learning, EDA, temporal convolutional\nnetwork, BioVid\n1 Introduction\nAn estimated 25.3 million adults reportedly have experienced daily pain for the last\n3 months in the U.S., and almost 40 million adults suffer from severe pain, leading\nto deteriorating health conditions ( Nahin, 2015 ). Building on this, recent studies show\nthat individuals enduring chronic pain are five times more likely to be afflicted with\nmental disorders such as depression or anxiety compared to those without chronic pain\n(De La Rosa et al., 2023 ). Furthermore, the prevalence of chronic pain outnumbers other\nprevalent chronic conditions like diabetes and hypertension, with an annual incidence rate\nof 52.4 cases per 1,000 ( Nahin et al., 2023).\nPain serves as a multi-faceted biological alarm system, indicating potential\nor ongoing tissue damage, defined by Merskey et al. (Merskey, 1979 ). This alarm\nsystem is not merely physiological but also engages psychological and emotional\ndimensions. Its primary function is to activate the body’ s defense mechanisms,\naiming to counteract harmful stimuli and mitigate further tissue damage.\nFrontiers in Physiology 01 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nOver the last 2 decades, the field of pain research has seen\nsignificant growth, both in terms of interest and scholarly output.\nA comprehensive review study by Ozek et al. analyzed 264,560\nscientific articles published from 2002 and reveals the growth\nin pain research. A sevenfold increase in the use of ‘pain’ as a\nkeyword nearly tripled the number of research papers discussing\npain ( Ozek et al., 2023 ). Particularly, they have been focusing on\ntopics such as chronic pain, pain management, pain assessment, and\nneuropathic pain. Recent trends between 2017 and 2021 indicate\na multidisciplinary approach, exploring pain’ s relationship and\nmanagement with opioids, analgesia, and psychological factors such\nas anxiety and quality of life.\nDespite these advancements in understanding pain and pain\nmanagement are noteworthy, a significant gap exists in the area\nof accurate and objective pain assessment ( Hämäläinen et al.,\n2022). Accurate pain assessments are critical for monitoring the\neffectiveness of pain management strategies and observing changes\nin pain severity over time. These assessments are particularly\ncrucial in clinical settings, where they guide healthcare providers in\ncustomizing treatment plans (Leigheb et al., 2017).\nA key aspect of these assessments lies in the quantification\nof pain, often accomplished by measuring its intensity. Widely\nrecognized methods for this purpose include self-report scales\nsuch as Visual Analog Scales, V erbal Rating Scales, and Numeric\nRating Scales ( Lazaridou et al., 2018 ). While these methods are\nuseful but also have limitations, especially for specific populations\nsuch as neonatal infants ( Cascella et al., 2019 ; Eriksson and\nCampbell-Y eo, 2019) and individuals with cognitive impairments\nor communication barriers (Deldar et al., 2018; W erner et al., 2022).\nThis limitation underscores the need for more automated and\nobjective techniques (Zamzmi et al., 2018).\nPhysiological signals, including electrodermal activity (EDA),\nelectrocardiography (ECG), electromyography (EMG), and\nelectroencephalography (EEG), are frequently employed for pain\nintensity classification ( W erner et al., 2022). Among these, EDA,\nalso known as galvanic skin response (GSR), has garnered particular\ninterest for its non-invasive nature and ease of data acquisition\nthrough wearable sensors ( Chen et al., 2021a ). EDA measures\nvariations in skin conductance, serving as a valuable indicator of\npain (Ledowski et al., 2009; Braithwaite et al., 2013). Its ease of data\ncollection and the insights it provides into the body’ s physiological\nresponse to pain make it a practical choice for real-time and\ncontinuous monitoring ( Erekat et al., 2021 ). However, traditional\nmethods often fall short of capturing the complexities inherent,\nespecially the temporal features, in EDA responses to pain.\nRecognizing this limitation, our study introduces a deep\nlearning framework, PainAttnNet, conceived to classify pain levels\nusing physiological signals. PainAttnNet is an innovative model\nintegrating Multiscale Convolutional Network (MSCN), a Squeeze-\nand-Excitation Residual Network (SEResNet), and a transformer\nencoder block.\n1) The MSCN is designed to extract both short-, medium- and\nlong-window sequential features from signals. The architecture can\ncapture essential information about the overall trends and variations\nin the physiological data, offering valuable insight into the pain\nintensity.\n2) The SEResNet in proposed model learns the\ninterdependencies among the extracted features, enhancing their\nrepresentation capability. This network selectively weights the\nimportance of different channels and adaptively recalibrates the\nfeature maps, thereby improving the model’ s sensitivity to the most\ninformative features.\n3) The transformer encoder block in PainAttnNet extracts the\ntemporal representations. This block uses a multihead attention\nlayer in conjunction with a temporal (causal) convolutional network,\nallowing the network to process the input sequence simultaneously,\nwhile effectively capturing the dependencies between the input and\noutput over time.\nOur contributions are twofold. First, we introduce a deep\nlearning framework with multiple modules adopted from different\nfields and previous studies, which effectively classifies pain intensity\nfrom physiological signals by utilizing various strategies to capture\nthe features. Second, we demonstrate that PainAttnNet outperforms\nthe existing models in classifying pain intensities, indicating its\npotential for automated pain intensity classification.\n2 Related work\n2.1 Pain classification\n2.1.1 Conventional machine learning models\nConventional machine learning models have served as\nfoundational parts in the domain of pain intensity classification.\nModels such as k-Nearest Neighbors (KNN) were explored by\nCao et al. ( Cao et al., 2021 ), while the Support V ector Machine\n(SVM) approach was researched by Campbellet al. (Campbell et al.,\n2019). Bayesian models have also found their place in this domain,\nwith notable work by Santra et al. ( Santra et al., 2020). Tree-based\nmodels, especially XGBoost and AdaBoost, have been frequently\nutilized, with research by Shi et al. ( Shi et al., 2022 ), Naeini et al.\n(Naeini et al., 2021 ), and Cao et al. ( Cao et al., 2021 ) leading\nthe way. A notable combination was by Pouromran et al., who\nintegrated BiLSTM with XGBoost for more nuanced pain intensity\nclassification (Pouromran et al., 2022).\n2.1.2 MLP-based models\nMultilayer perceptron (MLP), being feedforward neural\nnetworks, have been commonly used in the domain of pain intensity\nclassification. Lopez-Martinez and Picard ( Lopez-Martinez and\nPicard, 2017) introduced a deep MLP model tailored for classifying\npain intensity based on physiological signals. Gouverneur et al.\n(Gouverneur et al., 2021 ) further applied MLP , emphasizing its\nutility when combined with distinct hand-crafted features for\nclassifying heat-induced pain.\n2.1.3 RNN-based models\nRecurrent Neural Networks (RNNs), especially their advanced\nvariants, have been recognized for their capacity to handle\ntime-series data, making them particularly apt for pain\nsignal classification. The BiLSTM model, an evolution of\nthe traditional RNN, addresses challenges like vanishing and\nexploding gradients. A notable application was presented by\nW ang et al. ( W ang et al., 2020), who integrated a BiLSTM layer\nfor temporal feature extraction, further enhanced with hand-\ncrafted features. Then the features are sent to a MLP block for\nFrontiers in Physiology 02 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nclassification. Furthermore, Pouromran et al. ( Pouromran et al.,\n2022) demonstrated an innovative combination by employing\nBiLSTM for feature extraction, which was then processed by\nXGBoost, providing a nuanced approach to pain intensity\nclassification.\n2.1.4 CNN-based models\nConvolutional Neural Network (CNN) models have\nsignificantly transformed pain analysis by offering both recognition\nand classification capabilities. Thiam et al. (Thiam et al., 2019 )\nproposed a model using a deep Convolutional Neural Network\n(CNN) framework followed by fully connected layers (FCL).\nThis model was primarily tailored for pain recognition, and\nthen leveraging spatial features from data for accurate binary\nclassification: ‘pain’ or ‘no pain’ . With modifications and given\nsuitable training data, such architectures have the inherent potential\nfor broader classification tasks, such as categorizing different pain\nlevels or types. Similarly, Subramaniam and Dass (Subramaniam and\nDass, 2020) developed a hybrid deep learning model that combines\nthe strengths of CNN, for spatial feature extraction, with LSTM\nto capture temporal dynamics. The extracted features were then\nprocessed by an FCL to categorize the signals into ‘pain’ or ‘no pain’\ncategories.\n2.1.5 Limitations\nWhile these models have shown potential in classifying pain\nintensities, they possess inherent limitations. RNNs, despite their\ncapacity for capturing temporal dependencies in sequential data, can\nstruggle with long-term dependencies in the input sequences and\ntheir sequential nature hampers parallel training. MLPs, on the other\nhand, may not effectively capture temporal dependencies in input\nsignals. CNNs have been shown as a powerful tool in the domain of\npain intensity classification due to their capability in spatial feature\nextraction from data. Their ability to identify patterns in the data\nthat are crucial for pain recognition. However, when it comes to\nEDA data, which is inherently time-series in nature, CNNs might\nface challenges. Specifically, traditional CNN architectures, while\neffective for many tasks, may not be optimally designed to capture\nthese temporal dependencies in EDA signals, underscoring the\nneed for hybrid models that can better handle time-series data. T o\novercome these limitations, we introduce PainAttnNet, a framework\nleveraging a transformer encoder for pain intensity classification\nusing physiological signals.\n2.2 Feature extraction\nCNN has proven its efficacy in various tasks, e.g.,\naudio classification ( Lee et al., 2009 ) and image classification\n(Krizhevsky et al., 2017). Nevertheless, traditional CNNs operate at\na fixed scale, extracting features at one level of granularity. This can\nresult in overlooking significant features that exist across multiple\nscales or frequencies. In response to this limitation, Multiscale\nConvolutional Neural Network (MSCN) were developed. MSCN\nhas a unique multiscale layer and learnable convolutional layers,\nenabling the automatic extraction of features at diverse scales and\nfrequencies. This capacity allows MSCN to discern more intricate\npatterns in the data that may be overlooked by conventional CNNs,\npotentially leading to superior feature representation and enhanced\nclassification performance ( Cui et al., 2016 ; Li and Y u, 2016 ). Fu\net al. (Fu et al., 2018) introduced a novel architecture to overcome\nthe limitations of depth estimations by incorporating multi-scale\ninformation concatenated channel-wise. In a similar vein, Gong\net al. (Gong et al., 2019 ) extracted deep multiscale features from\nhyperspectral images, thereby improving the model’ s performance.\nMoreover, Penget al. (Peng et al., 2020) integrated traditional signal\nfiltering techniques with CNNs to develop a multiscale network\nfor feature extraction to diagnose wheelset-bearing faults under\nstrong noise and variable load conditions. These applications\ndemonstrate the potential of MSCNs to discern more complex\npatterns in the data, leading to superior feature representation\nand improved classification performance. W e adopt MSCN in\nPainAttnNet to effectively capture intricate, multi-scale patterns\nin physiological signals, thus enhancing pain intensity classification.\nThe features extracted at different scales are merged via channel-wise\nconcatenation, which preserves unique information and provides a\nrobust, comprehensive feature representation.\nHu et al. (Hu et al., 2018 ) introduced the Squeeze-and-\nExcitation Network (SENet). It has gained attention as a critical\ntool for efficient feature extraction and representation. SENet\nenhances the network’ s representational power by modeling\ninterdependencies between convolutional feature map channels.\nIts utility is showcased in various applications such as EEG seizure\ndetection by Li et al. (Li et al., 2020), sleep staging based on multi-\nmodal physiological signals by Jia et al. (Jia et al., 2022), and single\nEEG channel sleep classification where SENet was applied for\nfeature extraction by Eldeleet al. (Eldele et al., 2021), demonstrating\nsuperior performance. Building on the success of SENet, the\nConvolutional Block Attention Module (CBAM) expands the\nconcept by refining feature maps along both spatial and channel\ndimensions. However, the increased complexity associated with\nCBAM can be a double-edged sword, enhancing performance\nat the cost of increased computational demands ( W oo et al.,\n2018). In this context, the simplicity and effectiveness of Residual\nNetworks (ResNets) provide a significant benefit. ResNets tackle the\nvanishing gradient problem and provide a supportive structure for\nSENet, enabling optimal utilization of all channels in the feature\nmaps ( He et al., 2016 ). The combination of ResNets and SENet\ncapitalizes on the strengths of both, making it a crucial component\nof PainAttnNet. This combination highlights the potential of\nincorporating these robust networks into our framework, thereby\nenhancing its performance in pain intensity classification tasks.\nT emporal Convolutional Networks (TCNs) have found\nsuccessful applications across a range of domains. In the domain\nof action segmentation, TCNs have been utilized as an effective\nmethod for action segmentation. These networks have shown\nsuperior ability in capturing long-range relationships, longer\nsegment durations, and complex action compositions compared\nto LSTM ( Lea et al., 2016 ). In audio processing, TCNs have been\nutilized for generating raw audio waveforms, achieving state-of-the-\nart performance in musical audio synthesis and text-to-speech tasks.\nThis achievement underscores the ability of TCNs to model complex\npatterns in temporal data ( Oord et al., 2016). V an Den Oord et al.\n(V an den Oord et al., 2016) presented a model that conditions\nPixelCNN on latent space for specific image class generation. This\ninnovative approach showcases the potential of merging TCNs with\nFrontiers in Physiology 03 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\narchitectures like PixelCNN. In proposed model, we employ TCNs\nto extract temporal representations from physiological signals.\nThese representations are then sent to a Transformer Encoder, an\narchitecture built on the attention mechanisms first introduced by\nBahdanau et al. (Bahdanau et al., 2014) and later refined by V aswani\net al. (V aswani et al., 2017), for effective handling of sequence\ndependencies.\nThe Transformer architecture revolutionized the machine\nlearning field by proposing a model that relies solely on self-\nattention mechanisms, thereby discarding the need for recurrent\nlayers. Dosovitskiy et al. (Dosovitskiy et al., 2020) demonstrated the\neffectiveness of Transformer Encoders in computer vision field,\nshowing their ability to outperform CNNs in image recognition\ntasks when trained on large-scale datasets. Another work introduced\na dual-branch transformer that combines image patches to generate\nbetter image representations, demonstrating the potential of\nTransformer Encoders in handling multi-scale data ( Chen et al.,\n2021b). In proposed model, we utilize the Transformer Encoder\nto further enhance the extraction of temporal features, thereby\nimproving the classification of pain intensity from physiological\nsignals.\n3 Methodology\nBuilding on the importance of EDA in pain intensity\nclassification, as highlighted in the introduction, we introduce\na novel framework for automated pain assessment named\nPainAttnNet. The architecture of this framework is outlined in\nFigure 1. This framework 1) applies a multiscale convolutional\nnetwork (MSCN) to extract multiscale features from EDA. 2)\nFollowing this, we incorporate a Squeeze-and-Excitation Residual\nNetwork (SEResNet) to boost the interpretability of the extracted\nfeatures by understanding their interdependencies. 3) A multi-head\nattention framework combined with a TCN is used to encapsulate\nthe temporal aspects of the extracted features. Supplemental\ninformation and source code are available at: https://github.com/\nzhenyuanlu/PainAttnNet.\n3.1 Multiscale convolutional network\n(MSCN)\nEDA signals are inherently non-stationary, necessitating a\nmodel capable of capturing diverse features. PainAttnNet approach\nemploys a Multiscale Convolutional Network (MSCN) designed\nto sample varied lengths of EDA signal sequences through three\nconvolutional layers ( Figure 2). Taking inspiration from deep\nlearning models from several studies ( Li and Y u, 2016; Gong et al.,\n2019; Peng et al., 2020 ; Eldele et al., 2021 ), the branches cover\nwindows of 2 s, 1 s, and 0.1 s using kernels of 1,024, 512, and 50,\nrespectively.\nThe MSCN architecture, depicted inFigure 2, includes two max-\npooling layers and three convolutions per branch. The output from\neach convolution is normalized by a batch normalization block\nbefore the Gaussian Error Linear Unit (GELU). Max-pooling, a\nFIGURE 1\nOutline framework of our proposed PainAttnNet. Left bottom: Multiscale Convolutional Network (MSCN). Left top: Squeeze-and-Excitation Residual\nNetwork (SEResNet). Right: T ransformer Encoder.\nFrontiers in Physiology 04 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nFIGURE 2\nThe structure of the multiscale convolutional network (MSCN).\ndownsampling technique, reduces feature map dimensionality and\ncontrols overfitting by determining the maximum value in a given\nfeature map region. Consider an inputX = {x1, …,xN} ∈ ℝN×L×C, the\nmax-pooling operation can be represented as:\nfc (x) = max\ni,j\n(xi,j,c) . (1)\nhere, f represents the feature map, x denotes the input feature map\nfor each channel, c denotes the channel, i and j are the dimensions.\nThe max pooling operation is used for each channel separately. The\nfunction fc(x) outputs the maximum value among the elements\npresent in channel c. For instance, fc(x) in the feature map X\ncorresponds to the largest value of all elements residing at the cth\nchannel.\nAfter each convolutional layer, the batch normalization layer\naccelerates network convergence by decreasing internal covariate\nshifts and stabilizes the training process ( Ioffe and Szegedy, 2015 ).\nBatch normalization normalizes the activations of the previous\nnetwork by using channel-wise mean μc and standard deviation σc.\nThe batch normalization formulas are as follows: Let feature map\nX ∈ ℝN×L×C over a batch, where C is the channel, L represents the\nlength of each feature, andN denotes the overall number of features.\nThe formula for batch normalization is as follows:\nyγ,β,c =\nxi,j,c − μc\nσc\n⋅ γ + β, (2)\nhere,\nμc = 1\nNL ∑\ni,j\nxi,j,c, (3)\nσ2\nc = 1\nNL ∑\ni,j\n(xi,j,c − μc)\n2. (4)\nwhere c is the channel index, i and j are spatial indices; μc and σ2\nc are\nthe mean of the values and the variance in channel c for the current\nbatch, respectively. In the above equations, γ and β are learnable\nparameters introduced to allow the network to learn an appropriate\nnormalization even when the input is not normally distributed.\nGELU is a form of activation function that is a smooth\napproximation of the behavior of the rectified linear unit (ReLU)\n(Nair and Hinton, 2010 ) to prevent neurons from vanishing while\nlimiting how deep into the negative regime activations ( Hendrycks\nand Gimpel, 2016 ). This allows some negative weights to pass\nthrough the network, which is important to send the information\nto the subsequent task in SEResNet. As GELU follows the Batch\nNormalization Layer, the feature map inputsX ∼ N(0, 1). The GELU\nis defined as:\ng (x) ≔ x ⋅ Φ (x) = x ⋅ 1\n2 (1 + erf ( x\n√2\n)) . (5)\nHere, Φ (x) denotes the cumulative distribution function,\nrepresented by P (X ≤ x), and erf (⋅) corresponds to the error\nfunction. GELU can boost the representation capabilities of the\nnetwork by introducing a stochastic component that enables more\ndiversity. In addition, it has been demonstrated that GELU has a\nmore stable gradient and a more robust optimization landscape\nthan ReLU and leaky ReLU, because of this GELU can promote\nfaster convergence and improved generalization performance.\nAdditionally, we employ a dropout layer after the first\nmax pooling in all branches and concatenate the output features\nchannel-wise from these branches of the MSCN.\n3.2 Squeeze-and-excitation residual\nnetwork (SEResNet)\nUsing the SEResNet ( Figure 3), we can adaptively recalibrate\nthe concatenated features from the MSCN to enhance the\nmost important global spatial information of EDA signals. The\nmechanism of the SEResNet aims to model the interdependencies\nbetween the channels to enhance the extracted convolutional\nfeatures and amplify the network’ s sensitivity to the most meaningful\nfeatures ( Hu et al., 2018 ). This recalibration process emphasizes\ninformative features while suppressing less relevant ones, yielding\na more interpretable feature representation for subsequent tasks.\nThe SEResNet functions by condensing the channel-wise data of\nthe feature maps into a global information representation, and the\nexcitation operation uses this descriptor to adaptively scale the\nfeature maps (Figure 3).\nParticularly, PainAttnNet model starts with the implementation\nof two convolutions, each having 1 kernel size and 1 stride, with\nactivation conducted via ReLU. Here we use ReLU, other than GELU,\nto improve the performance on the convergence. At the squeezing\nstage in the SEResNet, the global spatial information from the\ntwo convolutions is then compressed by global average pooling.\nFrontiers in Physiology 05 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nFIGURE 3\nThe outline of Squeeze-and-Excitation Residual Network (SEResNet).\nIt reduces the spatial dimensions while keeping the informative\nfeatures. Let the feature map from the MSCN as X ∈ ℝN×L×C, we\napply two convolutional layers to X that results in obtaining new\nfeature maps V ∈ ℝN×L×C, and then shrink the V to generate the\nstatistics z ∈ ℝC:\nzc = 1\nNL\nN\n∑\ni=1\nL\n∑\nj=1\nvi,j,c, (6)\nwhere zc is the global average of L data points per each channel.\nNext comes the excitation (adaptive recalibration) stage, in which\ntwo FCL generate the statistics used to scale the feature maps. As a\nbottleneck, the first FCL with ReLU is to reduce the dimensionality.\nThe second FCL with sigmoid recovers the channel dimensions\nto their original size by performing a dimensionality-increasing\noperation. Let the z ∈ ℝC. W e define adaptive recalibration as\nfollows:\nα = σ (W2δ (W1z)) , (7)\nwhere δ denotes the ReLU, and σ represents the sigmoid function.\nW1 ∈ ℝ\nC\nr ×C and W2 ∈ ℝC× C\nr is the learnable weights for the first and\nthe second FC layer, respectively. Here, r is the ratio of reduction.\nThese weights reveal the interdependencies among the channels and\nprovide insights into the most informative channel.\nFollowing this, the original feature map denoted by v scaled by\nthe activation α, and this is done by channel-wise multiplication:\nM = αc ⊗ vc, (8)\ñX = X ⊕ M. (9)\nwhere ̃X is the final output of the SEResNet, which results from the\noriginal input X and the enhanced features M.\n3.3 Transformer encoder\n3.3.1 Temporal convolutional network (TCN)\nTCN framework, inspired by the works of Lea et al.\n(Lea et al., 2016 ) and V an den Oord et al. (Oord et al., 2016 ;\nV an den Oord et al., 2016), has been used effectively for processing\nand generating sequential data, e.g., audio or images. TCN employs\none-dimensional convolutional layers to extract the temporal\ndependencies over the sequential input data, like the recalibrated\nfeatures from SEResNet. In contrast to a regular convolutional\nnetwork, the output of TCN at a given time t depends only on\nthe inputs at times preceding t. TCN only permits the convolutional\nlayer to look back in time by masking future inputs. Like the regular\nconvolutional network, each convolutional layer contains a kernel\nwith a specific width to extract certain patterns or dependencies in\nthe input data across time before the present t. T o preserve the same\nlength for the output and input, one additional padding mechanism\nis appended to the left side of the input to offset the window shift in\nthe input.\nLet input feature map X ∈ ℝ1×L×C1 , where L is the input length\nand C1 is the dimension of the input channels. W e have kernel\nW ∈ ℝK×C1×C2 , and the size of padding (K − 1) ∈ ℝ, where K is the\nkernel size, and C2 is the dimension of the output channels. Then\nwe have the output from TCN as φ(⋅) ∈ ℝ1×L×C2 . This approach\ncan assist us in constructing an effective auto-regressive model that\nonly retrieves temporal information with a particular time frame\nfrom the past without cheating by utilizing knowledge about the\nfuture.\n3.3.2 Multi-head attention (MHA)\nMHA is a popular method for learning long-term relationships\nin sequences of features ( Figure 4). W e adapt this algorithm\nfrom V aswani et al. (V aswani et al., 2017 ), Dosovitskiy et al.\n(Dosovitskiy et al., 2020 ), and Bahdanau et al. (Bahdanau et al.,\n2014). It has significant performance in different fields, e.g., BERT\n(Devlin et al., 2019) and GPT (Brown et al., 2020) models in natural\nlanguage process, and physiological signals classification for sleep\nEldele et al. (Eldele et al., 2021), Zhu et al. (Zhu et al., 2020). MHA\nconsists of multiple layers of Scaled Dot-Product Attention, where\neach layer is capable of learning different temporal dependencies\nfrom the input feature maps (Figure 4). MHA aims to obtain a more\ncomprehensive understanding of how the ith feature is relevant\nto jth features by processing them through multiple attention\nmechanisms. In particular, let the output feature maps from\nSEResNet, X = {x1, …,xN} ∈ ℝN×L. Then we take three duplicates\nFrontiers in Physiology 06 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nFIGURE 4\nThe structure of multi-head attention, consists of H heads of Scaled\nDot-Product Attention layers with three inputs from TCNs.\nof X such that ̃X = φ(X), here φ(⋅) is the TCN, and ̃X is the output of\nTCN. Next, we send the three outputs, ̃X(Q), ̃X(K), ̃X(V) to attention\nlayers. This allows us to calculate the weighted sum, the attention\nscores zi:\nzi =\nL\n∑\nj=1\nαijφ ( ̃x(V)\nj ) , (10)\nthe weight αij of each φ( ̃xj) is computed by:\nαij =\nexp (eij)\n∑L\nr=1exp (eir)\n, (11)\nhere,\neij = 1\n√L\n⋅ ̃x(Q)\ni ⋅ ̃x(K)⊤\nj . (12)\nthen the output of one attention layer is z = {z0, …,zL} ∈ ℝN×L.\nNext, MHA calculates all the attention scoresZ(H) from multiple\nattention layers parallelly, and then concatenates them into ̃ZMHA ∈\nℝN×HL, where H is the number of attention heads, and HL is the\noverall length of the concatenated attention scores.\nW e apply a linear transformation with learnable weight W ∈\nℝHL×L to make the input and output sizes the same. This allows us to\neasily process the subsequent stages. The overall equation for MHA\nis represented as follows:\ñZMHA = Concat (z(1), …,z(H)) ⋅ W ∈ ℝN×L. (13)\nAfter concatenating these attention scores, we process\nthem with the original ̃X using an addition operation and\nlayer normalization adopted from ( Ba et al., 2016 ), formed as\nΦ( ̃X + ̃ZMHA), which can be described as a residual layer with\nlayer norm function Φ1 (⋅). Then the output of Φ1 (⋅) is processed\nby the FCLs and the second residual layer Φ2 (⋅). Finally, pain\nintensity categorization results are obtained from two fully\nconnected networks, which are then followed by a Softmax\nfunction.\n4 Experimental results\n4.1 BioVid Heat Pain Database\nIn our experiment, we used the Electrodermal Activity (EDA)\nsignals from BioVid Heat Pain Database (BioVid), generated\nby W alter et al. (W alter et al., 2013). As described in Figure 1,\nElectrodermal Activity (EDA) is a useful indicator of pain intensity\n(Ledowski et al., 2009). W alteret al. (W alter et al., 2013) conducted\na series of pain stimulus experiments in order to acquire five\ndistinct datasets, including video signals capturing the subjects’\nfacial expressions, SCL (also known as EDA), ECG, and EMG.\nThe experiment featured 90 participants in ages: 18–35, 36–50 and\n51–65. Each group has 30 subjects, with an equal number of male\nand female participants. At the beginning of the experiment, the\nauthors calibrated each participant’ s pain threshold by progressively\nraising the temperature from the baseline T0 = 32°C to determine\nthe temperature stages TP and TT; here TP represents the\ntemperature stages at which the individual began to experience the\nheat pain; TT is the temperature at which the individual experiences\nintolerable pain. Then four temperature stages can be determined as\nfollows:\nTi =\n{\n{\n{\nTP + [(i − 1) × γ] i ∈ {1, 2, 3, 4}\nTB i = 0\n(14)\nhere,\nγ = (TT − TP) /4 (15)\nwhere TP and TT are respectively defined as T1 and T4. The\nindividual received heat stimuli through a thermode (PATHW AY ,\nMedoc, Israel) connected to the right arm for the duration of the\nexperiment. In each trial, pain stimulation was administered to\neach participant for a duration of 25 min. In each experiment,\nthey determined five temperatures, Ti∈{0,1,2,3,4}, to induce five\npain intensity levels from lowest to highest. Each temperature\nstimulus was delivered 20 times for 4 s, with a random interval\nof 8–12 s between each application ( Figure 5A). During this\ninterval, the temperatures were kept at the pain-free (32°C) level.\nEDA, ECG, and EMG were collected by the according sensors\nto a sampling rate of 512 Hz with segmentation in a length\nof 5.5 s. Due to technical issues in the studies, three subjects\nwere excluded, resulting in a final count of 87. Therefore, the\ntraining sample of each signal creates a channel with dimensions of\n2, 816 × 20 × 5 × 87.\nInformed by the previous studies ( W erner et al., 2014 ;\nLopez-Martinez and Picard, 2017 ; Gouverneur et al., 2021 ;\nFrontiers in Physiology 07 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nFIGURE 5\nThe heat stimuli, with a break in between interval and window segmentations. (A) Demonstrates the original experiment settings of BioVid, with a\nduration of 4 s for each heat stimulus and an interval of 8–12 s between each stimulus. The yellow segmentation displays the 5-s timeframe for each\ncollected signal. (B) Thiam et al. (Thiam et al., 2019) introduces a different segmentation in red-strip rectangle which takes 4.5 s as opposed to 5.5 s.\nPouromran et al., 2021 ; Shi et al., 2022 ; Shi et al., 2022 ), we\nadopted the data from BioVid and used the EDA signal in a\ndimension of 2 , 816 × 20 × 5 × 87 with a 5.5-s segmentation as\nthe input in our experiment for pain intensity classification\nbased on five pain labels. This 5.5-s window for signal\nsegmentation is the default setting provided by the BioVid\ndatabase. Our decision to maintain this original 5.5-s window\naims to preserve the integrity of the original data, thereby\nallowing for a comprehensive and unaltered representation of\npain signal characteristics. This contrasts with the approach\ntaken by some previous studies. For instance, Subramaniam\nand Dass et al. (Subramaniam and Dass, 2020 ) removed 20\nout of 87 subjects, resulting in 2 , 816 × 20 × 5 × 67 training\nsamples. In contrast, Thiam et al. (Thiam et al., 2019 ) utilized\na 4.5 s segmentation truncating the original time frame by 1 s\n(Figure 5B). In the next sections, we will compare these latest SOTA\nmethods.\n4.2 Experimental settings\nIn our study, we compared PainAttnNet with six\nbaselines, Random Forest ( W erner et al., 2014 ), MT-NN\n(Lopez-Martinez and Picard, 2017 ), SVM ( Pouromran et al.,\n2021), TabNet ( Shi et al., 2022 ), MLP ( Gouverneur et al.,\n2021), and XGBoost ( Shi et al., 2022 ). In contrast, we also\nlisted other two models, CNN + LSTM ( Subramaniam\nand Dass, 2020 ), CNN ( Thiam et al., 2019 ), with different\nsegmentation and sample selections on the EDA signals as the\ninput.\nW e used 87-fold cross-validation for the BioVid by splitting the\nsubjects into 87 groups, therefore, each subject is in one group as\na leave-one-out cross-validation (LOOCV). W e trained the model\non 86 subjects and tested it on one subject with 100 epochs for\neach iteration. Ultimately, the macro performance matrices were\ncomputed by combining the projected pain intensity classes from\nFrontiers in Physiology 08 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nTABLE 1 PainAttnNet’s performance through three evaluation metrics through six tasks: 1) T0 vs (T1, T2, T3, T4), 2) T0 vs T1, 3) T0 vs T2, 4) T0 vs T3, and 5) T0 vs T4,\non BioVid dataset.\nTasks Specificity Sensitivity ACC MF1 κ\nT0 vs (T1, T2, T3, T4) 3.05 99.73 80.39 47.45 0.04\nT0 vs T1 43.85 69.54 56.70 55.97 0.13\nT0 vs T2 67.10 70.48 68.78 68.79 0.38\nT0 vs T3 81.67 73.16 77.41 77.37 0.55\nT0 vs T4 88.28 82.70 85.56 85.49 0.71\nTABLE 2 The performance comparison between PainAttnNet and other SOTA approaches. CNN + LSTM ‡ (Subramaniam and Dass, 2020); CNN‡ (Thiam et al.,\n2019); CNN‡ (Thiam et al., 2019); Random Forest (Werner et al., 2014); MT-NN (Lopez-Martinez and Picard, 2017); SVM (Pouromran et al., 2021); TabNet, XGBoost\n(Shi et al., 2022); MLP (Gouverneur et al., 2021). ‡: as these two approaches proposed two different procedures on the data input, we just list them here but are\nnot able to compare them with others.\nMethod T 0 vs T 1 T 0 vs T 2 T 0 vs T 3 T 0 vs T 4 Procedure\nCNN + LSTM‡ 85.65 74.47 80.80 80.17 5.5s Segment, n = 67 × 20 × 5\nCNN‡ 61.67 66.93 76.38 84.57 4.5s Segment, n = 87 × 20 × 5\nRandom Forest 55.40 60.20 65.90 73.80\n5.5s Segment; n = 87 × 20 × 5\nMT-NN 50.01 60.34 69.76 79.98\nSVM - - - 83.30\nTabNet 65.57 67.76 74.54 83.99\nMLP 59.08 65.09 75.14 84.22\nXGBoost 61.49 68.39 76.15 85.23\nPainAttnNet (Ours) 56.70 68.78 77.41 85.56\nall 87 iterations. W e created PainAttnNet using Python 3.10 and\nPyT orch 1.13 on a GPU powered by an Nvidia Quadro RTX 4000.\nW e selected the batch size of 128 for the training dataset, and set\nthe optimizer as Adam applied a weight decay (1e-03) with a 1e-03\ninitial learning rate. PyT orch’ s default settings for Betas and Epsilon\nwere (0.9, 0.999) and 1e-08. In the transformer encoder, we utilized\nfive heads for multi-head attention structure, with each feature’ s size\nbeing 75.\n4.3 Performance of PainAttnNet\nThe performance of PainAttnNet was assessed on the BioVid\ndataset through five distinctive experimental scenarios: 1) T0 vs\nall (T1, T2, T3, T4), 2) T0 vs T1, (3) T0 vs T2, 4) T0 vs T3, and 5)\nT0 vs T4 (refer to Table 1). These tasks were designed to assess the\nmodel’ s ability to distinguish between various pain intensity levels,\nwith a particular focus on tasks 1, 4, and 5. These tasks are of clinical\nsignificance as they involve distinguishing between no pain and\nvarious levels of pain intensity, a crucial factor in enhancing patient\ncare.\nTask 1, T0 vs all (T1, T2, T3, T4), is a binary classification task\nthat distinguishes between no pain (T0) and any level of pain all (T1,\nT2, T3, T4). Tasks 2 through 5 are binary classification tasks that\ndistinguish between zero pain and each pain level. For instance, Task\n2, T0 vs T 1, aims to differentiate between no pain and low pain.\nThe performance of PainAttnNet was most impressive on Task 5,\nachieving an accuracy of 85.56%, a κ of 0.71 and an MF1 of 85.49%.\nConversely, the model’ s performance was relatively weaker on Task\n1, with an accuracy of 80.39%, a κ of 0.04 and an MF1 of 47.45%.\nThe performance on Tasks 2, 3, and 4 was intermediate, with varying\nlevels of accuracy, Cohen’ s Kappa, and macro F1 score.\nW e further compared the performance of PainAttnNet with\nother SOTA models on the BioVid dataset for pain intensity\nclassification (refer to Table 2). For ease of comparison, we selected\nfour of the six classification tasks: T0 vs T1, T0 vs T2, T0 vs T3, and\nT0 vs T4.\nThe first two approaches, CNN + LSTM ( Subramaniam and\nDass, 2020 ) and CNN ( Thiam et al., 2019 ), employed different\nsample selections and data segmentation strategies, respectively.\nHence, their results are listed in Table 2 but are not directly\ncompared with others.\nFrontiers in Physiology 09 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nFIGURE 6\nROC curve for five tasks: T0 vs T1, T0 vs T2, T0 vs T3, T0 vs T4, and T0 vs all (T1, T2, T3, T4).\nFIGURE 7\nAblation for four configurations among four tasks: T0 vs T1, T0 vs T2, T0 vs T3, T0 vs T4.\nThe proposed model, PainAttnNet, outperformed other SOTA\nmodels in tasks T0 vs T3, and T0 vs T4, where it is critical to\ndistinguish between no pain and nearly intolerable pain. However,\nin task T0 vs T2, PainAttnNet achieved lower accuracy compared to\nthe best-performing SOTA model (68.10% vs. 68.39%). In taskT0 vs\nT1, the model introduced by Shi et al. (Shi et al., 2022) achieved the\nhighest accuracy.\nIn conclusion, the comparative analysis underscores the\npotentiality of PainAttnNet, PainAttnNet, as a robust application\nfor classifying pain intensity levels in Electrodermal Activity\n(EDA) signals. The model’ s performance across various tasks,\nparticularly in distinguishing between no pain and severe pain,\nhighlights its potential utility in clinical settings for improved patient\ncare.\n4.4 ROC curve analysis\nW e employed ROC curves to assess the capacity of\nPainAttnNet to classify varying degrees of pain intensity. The\narea under the ROC curve (AUC) served as an assessment of\nperformance.\nW e conducted ROC curve analyses for five distinct binary\nclassification tasks, yielding AUC scores of 0.56, 0.69, 0.81, 0.9,\nFrontiers in Physiology 10 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nFIGURE 8\nVisualization of average feature maps (T4) pre and post SE recalibrartion. The bottom-left plot depicts the learned channel weights, guiding the\nrecalibration. The comparison between the top-left and top-right plots illustrates the adaptive feature recalibration effected by the SE module.\nand 0.57 for the tasks T0 vs T1, T0 vs T2, T0 vs T3, T0 vs T4,\nand T0 vs all (T1, T2, T3, T4), respectively ( Figure 6). PainAttnNet\ndemonstrated a higher proficiency in distinguishing between the\nabsence of pain and high levels of pain than between the absence\nof pain and lower pain levels.\nDespite an accuracy of 80.39%, the AUC for T0 vs all (T1, T2,\nT3, T4) was relatively low (0.57). This is due to the model’ s low\nrecall for T0 (3.05%), indicating frequent misclassification of no pain\ninstances as pain, leading to a higher false positive rate and a lower\nAUC.\nThe performance of PainAttnNet was superior when\ndistinguishing between the absence of pain and the highest level\nof pain intensity, which holds considerable practical relevance.\nHowever, there is potential for improvement in distinguishing\nbetween the absence of pain and lower pain intensities.\nIn conclusion, PainAttnNet’ s performance improves as\nthe difference in pain intensity increases, aligning with\nrecent research and promising for practical applications,\nespecially in distinguishing between no pain and high levels of\npain.\n4.5 Ablation study\nIn this segment, we elucidate the ablation studies conducted\nto assess the efficacy of various components in our deep learning\nmodel, PainAttNet. The following is our model configuration for the\nablation study.\n• PainAttNet (Full Model):\n• MSCN (Multiscale Convolutional Neural Network): It\nprovides a method to capture features at various scales and\nresolutions. This helps in discerning intricate patterns and\nensures that features of varying sizes are accounted for in\nthe analysis.\n• SEResNet (Squeeze−and−Excitation Residual Network): It\noffers an attention mechanism to focus on the most relevant\nfeatures by dynamically recalibrating channel-wise feature\nresponses. This boosts the model’ s sensitivity to important\npatterns within the data.\n• Transformer Encoder: An architectural paradigm that\nutilizes self−attention mechanisms to weigh feature\nimportance, allowing the model to focus on critical aspects\nof the input data while discarding less relevant information.\n• MSCN + Transformer Encoder: By integrating MSCN with the\nTransformer Encoder, this configuration seeks to capitalize on\nthe MSCN’ s spatial feature extraction and the Transformer’ s\nability to capture long-range temporal dependencies in the data.\n• MSCN + SEResNet: By fusing the multiscale feature extraction\ncapabilities of MSCN with the channel-wise recalibration\noffered by SEResNet, this configuration aims to enhance\nthe focus on important features without the self-attention\nmechanism of the transformer.\n• MSCN Only: This module serves as the foundational model,\nMSCN Only focuses on extracting multi-scaled spatial features\nfrom the input data without the additional enhancements\nprovided by the other components.\nAcross all tasks, PainAttnNet consistently outperforms other\nconfigurations (Figure 7). This reinforces the cumulative advantage\nof integrating MSCN, SEResNet, and the transformer encoder\ncomponents into a unified architecture. The accuracy trends\nobserved for MSCN when paired with either the Transformer\nEncoder or SEResNet suggest their value addition over solely\nutilizing the MSCN. Both combinations consistently deliver\nimproved results over the MSCN Only configuration across the\nFrontiers in Physiology 11 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nFIGURE 9\nComparison of the performance of different MSCN Window Lengths against the baseline across T0 vs T1, T0 vs T2, T0 vs T3, and T0 vs T4 Task. For each\nexperiment, only one window length was altered while keeping other windows constant to ensure isolated effects of the variation. Baseline settings:\nshort window, 0.1s; medium window, 1s; long window, 2s.\nspecified tasks. It is evident that while the accuracy improvements\nin some configurations may seem marginal, they are nonetheless\nsignificant. Even slight increments in accuracy can underscore the\ncapability of the model to capture intricate nuances within the pain\nsignal data, especially when dealing with real-world datasets.\nIn summation, the ablation study results demonstrate the\ninherent benefits of these specific architectures, with PainAttnNet\nmanifesting as the most proficient.\n4.6 Visualization of SE Module\nRecalibration\nThe Squeeze-and-Excitation (SE) module’ s primary purpose\nis to adaptively recalibrate channel-wise feature responses. This\nrecalibration emphasizes certain informative features while\ndiminishing less relevant ones, providing a more refined feature\nrepresentation. The visualizations in Figure 8 are derived from the\ntrained model in the previous sections. T o elucidate the SE module’ s\nrecalibration effects, we processed the entire training dataset\nthrough the trained model. By examining the averaged feature maps\nfrom these samples, we intended to highlight the consistent patterns\nof recalibration that the SE module introduces, both before and after\nits operation.\nBefore SE Module Recalibration: As visualized in the top-left\nplot of Figure 8, the “Feature Maps before SE Recalibration” exhibits\nthe distribution of channel-wise features. This representation is\nthe outcome post the Multiscale Convolutional Network (MSCN)\nprocessing. SE Channel Importance W eights: The bottom-left plot\nof Figure 8 showcases the “Channel W eights for SE Module, ”\nwhich are learned during the training process. These weights\ndictate the importance of each channel and subsequently guide\nthe SE operation in recalibrating the features. After SE Module\nRecalibration: In the top-right plot of Figure 8, one can observe\nthe feature map “after” SE recalibration. Distinct changes in the\nfeature intensity and emphasis are evident, with some features\nbecoming more pronounced, while others diminish. While it\nis apparent that the SE module emphasizes certain features and\ndiminishes others, identifying the specific nature or type of these\nfeatures is non-trivial. This is primarily because the features\nhave already been processed by the MSCN, making their innate\ncharacteristics intricate to identify purely based on SE module\nvisualization.\n4.7 Sensitive analysis on MSCN scales\nOur analysis investigates three specific window scale\ncombinations ( Figure 9): short, medium, and long. The chart in\nFigure 9 presents the performance variations observed across\ndifferent MSCN window lengths and four tasks. Each of the\nbars corresponds to a specific window length combination. Our\nchosen baseline of Short Window 0.1s + Medium Window 1s\n+ Long Window 2s consistently performs well across various\nFrontiers in Physiology 12 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\ntasks, even if it does not always achieve the highest accuracy. W e\nhave thoroughly examined all configurations and found that the\nbaseline performs solidly in most cases, making it the best overall\nchoice. While some combinations slightly outperform the baseline\nin certain situations, these nuanced differences do not show a\nconsistent improvement, confirming our confidence in our chosen\nbaseline.\nThe width of the window in the MSCN is a pivotal parameter\ninfluencing the model’ s performance. Each window length captures\nspecific features from the pain signals, enabling the model to analyze\npatterns at various temporal granularity. Longer windows provide\na broader view, capturing low-frequency components and global\npatterns, while shorter windows allow the model to capture high-\nfrequency components. The integration of these diverse window\nlengths enables the model to construct a comprehensive and\nmulti-granular feature representation, enhancing its capacity to\ndiscern subtle patterns and thereby improving its overall predictive\nperformance.\n5 Discussion and conclusion\nPainAttnNet, the framework we introduced, serves as a\nnovel approach for classifying pain severity using EDA signals.\nThe model integrates MSCN and SEResNet for robust feature\nextraction from EDA signals. A TCN and multiple Scaled Dot-\nProduct Attention layers form the multi-head attention architecture,\ndesigned to capture temporal dependencies and relationships\namong input features. Evaluations on the BioVid heat pain\ndataset confirm the model’ s superior performance over existing\nmethods.\nWhile PainAttnNet demonstrates proficiency in distinguishing\nthe absence of pain from various pain intensities, room for\nimprovement remains, especially in differentiating between distinct\nlevels of pain intensity. One primary limitation is the dataset’ s\ndistribution shift among subjects, particularly concerning age\nand gender demographics. Based on our findings and existing\nstudies, pain perception can vary significantly across different\nage groups ( Murray et al., 2021 ). Gender differences in pain\nperception have also been reported, adding another layer of\ncomplexity to pain assessment ( Keogh, 2022 ). Additionally,\nthe current dependency on lab-controlled data presents a\nlimitation for the model’ s applicability in real-world clinical\nsettings.\n6 Future work\nMoreover, expanding the scope of pain classification signals\nis pivotal for comprehensive understanding and accuracy.\nWhile this paper primarily leverages EDA signals, future\niterations of PainAttnNet will incorporate a broader range\nof physiological signals, such as ECG, EMG, etc. Integrating\nmultiple signals can offer a comprehensive view of pain\nassessment, considering the multifaceted nature of pain\nresponses.\nT o refine PainAttnNet further, we plan to employ masked\nmodels and adaptive embedding for enhanced feature extraction.\nW e also intend to explore the application of contrastive learning\nin conjunction with domain adaptation on large unlabeled datasets\nand small segments of labeled data ( Zhang et al., 2023 ). These\nenhancements aim to improve PainAttnNet’ s clinical applicability\nby addressing its limitations, including those related to age- and\ngender-based pain perception. Future studies will also aim to train\nand evaluate the model using more ecologically valid, real-world\ndata.\nData availability statement\nPublicly available datasets were analyzed in this study. This data\ncan be found here: https://www.nit.ovgu.de/BioVid.html.\nEthics statement\nThe studies involving humans were approved by The ethics\ncommittee of the University of Ulm (196/10-UBB/bal). The studies\nwere conducted in accordance with the local legislation and\ninstitutional requirements. The participants provided their written\ninformed consent to participate in this study.\nAuthor contributions\nZL: Conceptualization, Data curation, Formal Analysis,\nMethodology, Software, V alidation, Visualization, Writing–original\ndraft, Investigation. BO: V alidation, Writing–review and editing,\nConceptualization, Data curation. SK: Resources, Supervision,\nWriting–review and editing, Funding acquisition, Investigation,\nProject administration.\nFunding\nThe author(s) declare financial support was received for the\nresearch, authorship, and/or publication of this article. This work\nwas supported by the National Science Foundation Award (ID:\n1838796) granted to SK as a Co PI for the project “SCH:\nINT: Collaborative Research: Novel Computational Methods for\nContinuous Objective Multimodal Pain Assessment Sensing System\n(COMPASS)” .\nAcknowledgments\nW e acknowledge that an earlier version of this manuscript has\nbeen published as a preprint, as cited in ( Lu et al., 2023).\nConflict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or financial relationships that could be\nconstrued as a potential conflict of interest.\nFrontiers in Physiology 13 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their affiliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or claim\nthat may be made by its manufacturer, is not guaranteed or endorsed\nby the publisher.\nReferences\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. arXiv preprint\narXiv:1607.06450. doi:10.48550/ARXIV .1607.06450\nBahdanau, D., Cho, K., and Bengio, Y . (2014). Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473.\nBraithwaite, J. J., W atson, D. G., Jones, R., and Rowe, M. (2013). Publication\nrecommendations for electrodermal measurements. Psy chophysiology 49, 1017–1034.\ndoi:10.1111/j.1469-8986.2012.01384.x\nBrown, T ., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P ., et al. (2020).\n“Language models are few-shot learners, ” inAdvances in neural information processing\nsystems. Editors H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (China:\nCurran Associates, Inc), 33, 1877–1901.\nCampbell, E., Phinyomark, A., and Scheme, E. (2019). Feature extraction and\nselection for pain recognition using peripheral physiological signals. Front. Neurosci.\n13, 437. doi:10.3389/fnins.2019.00437\nCao, R., Aqajari, S. A. H., Naeini, E. K., and Rahmani, A. M. (2021). “Objective pain\nassessment using wrist-based ppg signals: a respiratory rate based method, ” in 2021 43rd\nAnnual International Conference of the IEEE Engineering in Medicine and Biology\nSociety, USA, 1-5 Nov. 2021 (EMBC IEEE), 1164–1167.\nCascella, M., Bimonte, S., Saettini, F ., and Muzio, M. R. (2019). The challenge of\npain assessment in children with cognitive disabilities: features and clinical applicability\nof different observational tools. J. Paediatr. Child Health 55, 129–135. doi: 10.1111/jpc.\n14230\nChen, C.-F . R., Fan, Q., and Panda, R. (2021a). Crossvit: cross-attention multi-scale\nvision transformer for image classification. Proc. IEEE/CVF Int. Conf. Comput. Vis. ,\n357–366. doi:10.1109/ICCV48922.2021.00041\nChen, J., Abbod, M., and Shieh, J.-S. (2021b). Pain and stress detection using wearable\nsensors and devices—a review. Sensors 21, 1030. doi:10.3390/s21041030\nCui, Z., Chen, W ., and Chen, Y . (2016).Multi-scale convolutional neural networks for\ntime series classification. arXiv preprint arXiv:1603.06995.\nDe La Rosa, J. S., Brady, B. R., Ibrahim, M. M., Herder, K. E., W allace, J. S., Padilla,\nA. R., et al. (2023).Co-occurrence of chronic pain and anxiety/depression symptoms in\nus adults: prevalence, functional impacts, and opportunities. Pain. doi:10.1097/j.pain.\n0000000000003056\nDeldar, K., Froutan, R., and Ebadi, A. (2018). Challenges faced by nurses in using pain\nassessment scale in patients unable to communicate: a qualitative study.BMC Nurs. 17,\n11–18. doi:10.1186/s12912-018-0281-3\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K. (2019). “BERT: pre-training\nof deep bidirectional transformers for language understanding, ” in Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language T echnologies, Minneapolis, Minnesota, June 3 – June 5\n(Association for Computational Linguistics), 4171–4186. doi:10.18653/v1/N19-1423\nDosovitskiy, A., Beyer, L., Kolesnikov, A., W eissenborn, D., Zhai, X., Unterthiner, T .,\net al. (2020). An image is worth 16x16 words: transformers for image recognition at scale.\narXiv preprint arXiv:2010.11929.\nEldele, E., Chen, Z., Liu, C., Wu, M., Kwoh, C.-K., Li, X., et al. (2021). An attention-\nbased deep learning approach for sleep stage classification with single-channel eeg.IEEE\nTrans. Neural Syst. Rehabilitation Eng.29, 809–818. doi:10.1109/TNSRE.2021.3076234\nErekat, D., Hammal, Z., Siddiqui, M., and Dibeklioğlu, H. (2021). “Enforcing\nmultilabel consistency for automatic spatio-temporal assessment of shoulder pain\nintensity, ” inCompanion publication of the 2020 international conference on multimodal\ninteraction (New Y ork, NY , USA: Association for Computing Machinery), ICMI ’20\nCompanion), 156–164. doi:10.1145/3395035.3425190\nEriksson, M., and Campbell-Y eo, M. (2019). Assessment of pain in newborn infants.\nSeminars Fetal Neonatal Med.24, 101003. doi:10.1016/j.siny.2019.04.003\nFu, H., Gong, M., W ang, C., Batmanghelich, K., and Tao, D. (2018). “Deep ordinal\nregression network for monocular depth estimation, ” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, USA, 17-19 June 1997 (IEEE),\n2002–2011.\nGong, Z., Zhong, P ., Y u, Y ., Hu, W ., and Li, S. (2019). A cnn with multiscale\nconvolution and diversified metric for hyperspectral image classification. IEEE Trans.\nGeoscience Remote Sens. 57, 3599–3618. doi:10.1109/tgrs.2018.2886022\nGouverneur, P ., Li, F ., Adamczyk, W . M., Szikszay, T . M., Luedtke, K., and Grzegorzek,\nM. (2021). Comparison of feature extraction methods for physiological signals for\nheat-based pain recognition. Sensors 21, 4838. doi:10.3390/s21144838\nHämäläinen, J., Kvist, T ., and Kankkunen, P . (2022). Acute pain assessment\ninadequacy in the emergency department: patients’ perspective. J. patient Exp. 9,\n237437352110496. doi:10.1177/23743735211049677\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, USA, 17-19 June 1997 (IEEE), 770–778.\nHendrycks, D., and Gimpel, K. (2016). Gaussian error linear units (gelus). arXiv\npreprint arXiv:1606.08415. doi:10.48550/ARXIV .1606.08415\nHu, J., Shen, L., and Sun, G. (2018). Squeeze-and-excitation networks. In IEEE/CVF\nConf. Comput. Vis. Pattern Recognit. 7132. doi:10.1109/CVPR.2018.00745\nIoffe, S., and Szegedy, C. (2015). “Batch normalization: accelerating deep network\ntraining by reducing internal covariate shift, ” in Proceedings of the 32nd International\nConference on Machine Learning, USA, July 6 - 11, 2015. Editors F . Bach, and D. Blei\n(IEEE), 448–456.37.\nJia, Z., Cai, X., and Jiao, Z. (2022). Multi-modal physiological signals based squeeze-\nand-excitation network with domain adversarial learning for sleep staging.IEEE Sensors\nJ. 22, 3464–3471. doi:10.1109/jsen.2022.3140383\nKeogh, E. (2022). Sex and gender differences in pain: past, present, and future. Pain\n163, S108–S116. doi:10.1097/j.pain.0000000000002738\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). Imagenet classification with\ndeep convolutional neural networks. Commun. ACM 60, 84–90. doi:10.1145/3065386\nLazaridou, A., Elbaridi, N., Edwards, R. R., and Berde, C. B. (2018). “Chapter 5 - pain\nassessment, ” inEssentials of pain medicine. Editors H. T . Benzon, S. N. Raja, S. S. Liu,\nS. M. Fishman, and S. P . Cohen Fourth Edition (Germany: Elsevier), 39–46.e1. doi: 10.\n1016/B978-0-323-40196-8.00005-X\nLea, C., Vidal, R., Reiter, A., and Hager, G. D. (2016). “T emporal convolutional\nnetworks: a unified approach to action segmentation, ” in Computer Vision–ECCV 2016\nW orkshops, Amsterdam, The Netherlands, October 8-10 and 15-16, 2016 (Springer),\n47–54.\nLedowski, T ., Ang, B., Schmarbeck, T ., and Rhodes, J. (2009). Monitoring of\nsympathetic tone to assess postoperative pain: skin conductance vs surgical stress index.\nAnaesthesia 64, 727–731. doi:10.1111/j.1365-2044.2008.05834.x\nLee, H., Pham, P ., Largman, Y ., and Ng, A. (2009). Unsupervised feature learning for\naudio classification using convolutional deep belief networks. Adv. neural Inf. Process.\nSyst. 22.\nLeigheb, M., Sabbatini, M., Baldrighi, M., Hasenboehler, E. A., Briacca, L.,\nGrassi, F ., et al. (2017). Prospective analysis of pain and pain management in an\nemergency department. Acta Bio Medica Atenei Parm. 88, 19–30. doi: 10.23750/\nabm.v88i4-S.6790\nLi, G., and Y u, Y . (2016). Visual saliency detection based on multiscale deep\ncnn features. IEEE Trans. image Process. 25, 5012–5024. doi: 10.1109/tip.2016.\n2602079\nLi, Y ., Liu, Y ., Cui, W .-G., Guo, Y .-Z., Huang, H., and Hu, Z.-Y . (2020). Epileptic\nseizure detection in eeg signals using a unified temporal-spectral squeeze-and-\nexcitation network. IEEE Trans. Neural Syst. Rehabilitation Eng. 28, 782–794. doi: 10.\n1109/TNSRE.2020.2973434\nLopez-Martinez, D., and Picard, R. (2017). “Multi-task neural networks for\npersonalized pain recognition from physiological signals, ” in2017 seventh international\nconference on affective computing and intelligent interaction workshops and demos\n(ACIIW) (USA: IEEE).\nLu, Z., Ozek, B., and Kamarthi, S. (2023). Transformer encoder with multiscale\ndeep learning for pain classification using physiological signals . arXiv preprint\narXiv:2303.06845.\nMerskey, H. (1979). Pain terms: a list with definitions and notes on usage.\nrecommended by the iasp subcommittee on taxonomy. Pain 6, 249–252.\nMurray, C. B., Patel, K. V ., Twiddy, H., Sturgeon, J. A., and Palermo, T . M. (2021). Age\ndifferences in cognitive–affective processes in adults with chronic pain. Eur. J. Pain 25,\n1041–1052. doi:10.1002/ejp.1725\nNaeini, E. K., Subramanian, A., Calderon, M.-D., Zheng, K., Dutt, N., Liljeberg,\nP ., et al. (2021). Pain recognition with electrocardiographic features in postoperative\npatients: method validation study. J. Med. Internet Res. 23, e25079. doi: 10.2196/\n25079\nNahin, R. L. (2015). Estimates of pain prevalence and severity in adults: United States,\n2012. J. pain 16, 769–780. doi:10.1016/j.jpain.2015.05.002\nFrontiers in Physiology 14 frontiersin.org\nLu et al. 10.3389/fphys.2023.1294577\nNahin, R. L., Feinberg, T ., Kapos, F . P ., and T erman, G. W . (2023). Estimated rates of\nincident and persistent chronic pain among us adults, 2019-2020. JAMA Netw. Open 6,\ne2313563. –e2313563. doi:10.1001/jamanetworkopen.2023.13563\nNair, V ., and Hinton, G. E. (2010). “Rectified linear units improve restricted\nBoltzmann machines, ” in Proceedings of the 27th international conference on machine\nlearning, USA, June 21 - 24, 2010 (ICML-10), 807–814.\nOord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., et al.\n(2016). Wavenet: a generative model for raw audio. arXiv preprint arXiv:1609.03499.\nOzek, B., Lu, Z., Pouromran, F ., Radhakrishnan, S., and Kamarthi, S. (2023). Analysis\nof pain research literature through keyword co-occurrence networks. PLOS Digit.\nHealth 2, e0000331. doi:10.1371/journal.pdig.0000331\nPeng, D., W ang, H., Liu, Z., Zhang, W ., Zuo, M. J., and Chen, J. (2020). Multibranch\nand multiscale cnn for fault diagnosis of wheelset bearings under strong noise and\nvariable load condition. IEEE Trans. Industrial Inf.16, 4949–4960. doi:10.1109/tii.2020.\n2967557\nPouromran, F ., Lin, Y ., and Kamarthi, S. (2022). Personalized deep bi-lstm rnn\nbased model for pain intensity classification using eda signal. Sensors 22, 8087. doi:10.\n3390/s22218087\nPouromran, F ., Radhakrishnan, S., and Kamarthi, S. (2021). Exploration of\nphysiological sensors, features, and machine learning models for pain intensity\nestimation. Plos one 16, e0254108. doi:10.1371/journal.pone.0254108\nSantra, D., Mandal, J. K., Basu, S. K., and Goswami, S. (2020). Medical expert system\nfor low back pain management: design issues and conflict resolution with bayesian\nnetwork. Med. Biol. Eng. Comput. 58, 2737–2756. doi:10.1007/s11517-020-02222-9\nShi, H., Chikhaoui, B., and W ang, S. (2022). “Tree-based models for pain detection\nfrom biomedical signals, ” in International conference on smart homes and health\ntelematics (Germany: Springer), 183–195.\nSubramaniam, S. D., and Dass, B. (2020). Automated nociceptive pain assessment\nusing physiological signals and a hybrid deep learning network. IEEE Sensors J. 21,\n3335–3343. doi:10.1109/jsen.2020.3023656\nThiam, P ., Bellmann, P ., Kestler, H. A., and Schwenker, F . (2019). Exploring deep\nphysiological models for nociceptive pain recognition. Sensors 19, 4503. doi: 10.\n3390/s19204503\nV an den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al. (2016).\nConditional image generation with pixelcnn decoders. Adv. neural Inf. Process. Syst.29.\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “ Attention is all you need, ” inAdvances in neural information processing systems.\nEditors I. Guyon, U. V . Luxburg, S. Bengio, H. W allach, R. Fergus, S. Vishwanathan,\net al. (Germany: Curran Associates, Inc), 30.\nW alter, S., Gruss, S., Ehleiter, H., Tan, J., Traue, H. C., W erner, P ., et al. (2013). The\nbiovid heat pain database data for the advancement and systematic validation of an\nautomated pain recognition system.IEEE Int. Conf. Cybern. (CYBCO). 128–131. doi:10.\n1109/CYBConf.2013.6617456\nW ang, R., Xu, K., Feng, H., and Chen, W . (2020). “Hybrid rnn-ann based deep\nphysiological network for pain recognition, ” in 2020 42nd Annual International\nConference of the IEEE Engineering in Medicine and Biology Society (EMBC), China,\n20-24 July 2020 (IEEE), 5584. –5587.\nW erner, P ., Al-Hamadi, A., Niese, R., W alter, S., Gruss, S., and Traue, H. C.\n(2014). “ Automatic pain recognition from video and biomedical signals, ” in 2014\n22nd international conference on pattern recognition, Germany, Aug. 28 2014 (IEEE),\n4582–4587.\nW erner, P ., Lopez-Martinez, D., W alter, S., Al-Hamadi, A., Gruss, S., and Picard, R.\nW . (2022). Automatic recognition methods supporting pain assessment: a survey.IEEE\nTrans. Affect. Comput.13, 530–552. doi:10.1109/TAFFC.2019.2946774\nW oo, S., Park, J., Lee, J.-Y ., and Kweon, I. S. (2018). “Cbam: convolutional block\nattention module, ” in Proceedings of the European conference on computer vision\n(ECCV), USA, 08 September 2018 (ECCV).\nZamzmi, G., Kasturi, R., Goldgof, D., Zhi, R., Ashmeade, T ., and Sun, Y . (2018).\nA review of automated pain assessment in infants: features, classification tasks, and\ndatabases. IEEE Rev. Biomed. Eng. 11, 77–96. doi:10.1109/RBME.2017.2777907\nZhang, K., W en, Q., Zhang, C., Cai, R., Jin, M., Liu, Y ., et al. (2023). Self-supervised\nlearning for time series analysis: taxonomy, progress, and prospects . arXiv preprint\narXiv:2306.10125.\nZhu, T ., Luo, W ., and Y u, F . (2020). Convolution-and attention-based neural network\nfor automated sleep stage classification. Int. J. Environ. Res. Public Health 17, 4152.\ndoi:10.3390/ijerph17114152\nFrontiers in Physiology 15 frontiersin.org"
}