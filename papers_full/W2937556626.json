{
    "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer",
    "url": "https://openalex.org/W2937556626",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2174480068",
            "name": "Sun Fei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1965066944",
            "name": "Liu Jun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1976106360",
            "name": "Wu Jian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2747194815",
            "name": "Pei Changhua",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099942556",
            "name": "Lin, Xiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287524726",
            "name": "Ou, Wenwu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119749873",
            "name": "Jiang Peng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2219888463",
        "https://openalex.org/W2137245235",
        "https://openalex.org/W2474909202",
        "https://openalex.org/W2099866409",
        "https://openalex.org/W2157881433",
        "https://openalex.org/W2108920354",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2963367478",
        "https://openalex.org/W2788997749",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2512971201",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2583674722",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2798385737",
        "https://openalex.org/W2750004028",
        "https://openalex.org/W2963900085",
        "https://openalex.org/W2783272285",
        "https://openalex.org/W2140310134",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2137028279",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1994389483",
        "https://openalex.org/W2165949563",
        "https://openalex.org/W2138108551",
        "https://openalex.org/W2159094788",
        "https://openalex.org/W2054141820",
        "https://openalex.org/W2253995343",
        "https://openalex.org/W2515144511",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2027731328",
        "https://openalex.org/W2042281163",
        "https://openalex.org/W2964296635",
        "https://openalex.org/W2964316331",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W2964044287",
        "https://openalex.org/W2739901700",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2809307135",
        "https://openalex.org/W2605350416",
        "https://openalex.org/W1720514416",
        "https://openalex.org/W2783944588",
        "https://openalex.org/W2604438604",
        "https://openalex.org/W2626454364",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2962822108",
        "https://openalex.org/W2171279286",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W3098231197",
        "https://openalex.org/W2734755249",
        "https://openalex.org/W2963386218",
        "https://openalex.org/W2888539709"
    ],
    "abstract": "Modeling users' dynamic and evolving preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks (e.g., Recurrent Neural Network) to encode users' historical interactions from left to right into hidden representations for making recommendations. Although these methods achieve satisfactory results, they often assume a rigidly ordered sequence which is not always practical. We argue that such left-to-right unidirectional architectures restrict the power of the historical sequence representations. For this purpose, we introduce a Bidirectional Encoder Representations from Transformers for sequential Recommendation (BERT4Rec). However, jointly conditioning on both left and right context in deep bidirectional model would make the training become trivial since each item can indirectly \"see the target item\". To address this problem, we train the bidirectional model using the Cloze task, predicting the masked items in the sequence by jointly conditioning on their left and right context. Comparing with predicting the next item at each position in a sequence, the Cloze task can produce more samples to train a more powerful bidirectional model. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.",
    "full_text": "BERT4Rec: Sequential Recommendation with Bidirectional\nEncoder Representations from Transformer\nFei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang\nAlibaba Group, Beijing, China\n{ofey.sf,yanhan.lj,joshuawu.wujian,changhua.pch,hc.lx,santong.oww,jiangpeng.jp}@alibaba-inc.com\nABSTRACT\nModeling users’ dynamic preferences from their historical behav-\niors is challenging and crucial for recommendation systems. Previ-\nous methods employ sequential neural networks to encode users’\nhistorical interactions from left to right into hidden representations\nfor making recommendations. Despite their effectiveness, we argue\nthat such left-to-right unidirectional models are sub-optimal due\nto the limitations including: a) unidirectional architectures restrict\nthe power of hidden representation in users’ behavior sequences;\nb) they often assume a rigidly ordered sequence which is not always\npractical. To address these limitations, we proposed a sequential rec-\nommendation model called BERT4Rec, which employs the deep\nbidirectional self-attention to model user behavior sequences. To\navoid the information leakage and efficiently train the bidirectional\nmodel, we adopt the Cloze objective to sequential recommendation,\npredicting the random masked items in the sequence by jointly\nconditioning on their left and right context. In this way, we learn\na bidirectional representation model to make recommendations\nby allowing each item in user historical behaviors to fuse infor-\nmation from both left and right sides. Extensive experiments on\nfour benchmark datasets show that our model outperforms various\nstate-of-the-art sequential models consistently.\nCCS CONCEPTS\n• Information systems → Recommender systems.\nKEYWORDS\nSequential Recommendation; Bidirectional Sequential Model; Cloze\nACM Reference Format:\nFei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng\nJiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional\nEncoder Representations from Transformer. In The 28th ACM International\nConference on Information and Knowledge Management (CIKM ’19), November\n3–7, 2019, Beijing, China. ACM, New York, NY, USA, 11 pages. https://doi.\norg/10.1145/3357384.3357895\n1 INTRODUCTION\nAccurately characterizing users’ interests lives at the heart of an\neffective recommendation system. In many real-world applications,\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM ’19, November 3–7, 2019, Beijing, China\n© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-6976-3/19/11. . . $15.00\nhttps://doi.org/10.1145/3357384.3357895\nusers’ current interests are intrinsically dynamic and evolving, influ-\nenced by their historical behaviors. For example, one may purchase\naccessories (e.g., Joy-Con controllers) soon after buying a Nintendo\nSwitch, though she/he will not buy console accessories under nor-\nmal circumstances.\nTo model such sequential dynamics in user behaviors, various\nmethods have been proposed to make sequential recommendations\nbased on users’ historical interactions [ 15, 22, 40]. They aim to\npredict the successive item(s) that a user is likely to interact with\ngiven her/his past interactions. Recently, a surge of works employ\nsequential neural networks, e.g., Recurrent Neural Network (RNN),\nfor sequential recommendation and obtain promising results [ 7,\n14, 15, 56, 58]. The basic paradigm of previous work is to encode\na user’s historical interactions into a vector (i.e., representation of\nuser’s preference) using a left-to-right sequential model and make\nrecommendations based on this hidden representation.\nDespite their prevalence and effectiveness, we argue that such\nleft-to-right unidirectional models are not sufficient to learn optimal\nrepresentations for user behavior sequences. The major limitation,\nas illustrated in Figure 1c and 1d, is that such unidirectional models\nrestrict the power of hidden representation for items in the histori-\ncal sequences, where each item can only encode the information\nfrom previous items. Another limitation is that previous unidirec-\ntional models are originally introduced for sequential data with\nnatural order, e.g., text and time series data. They often assume a\nrigidly ordered sequence over data which is not always true for user\nbehaviors in real-world applications. In fact, the choices of items\nin a user’s historical interactions may not follow a rigid order as-\nsumption [18, 54] due to various unobservable external factors [5].\nIn such a situation, it is crucial to incorporate context from both\ndirections in user behavior sequence modeling.\nTo address the limitations mentioned above, we seek to use a\nbidirectional model to learn the representations for users’ historical\nbehavior sequences. Specifically, inspired by the success of BERT [6]\nin text understanding, we propose to apply the deep bidirectional\nself-attention model to sequential recommendation, as illustrated\nin Figure 1b. For representation power, the superior results for deep\nbidirectional models on text sequence modeling tasks show that it\nis beneficial to incorporate context from both sides for sequence\nrepresentations learning [6]. For rigid order assumption, our model\nis more suitable than unidirectional models in modeling user behav-\nior sequences since all items in the bidirectional model can leverage\nthe contexts from both left and right side.\nHowever, it is not straightforward and intuitive to train the\nbidirectional model for sequential recommendation. Conventional\nsequential recommendation models are usually trained left-to-right\nby predicting the next item for each position in the input sequence.\nAs shown in Figure 1, jointly conditioning on both left and right\narXiv:1904.06690v2  [cs.IR]  21 Aug 2019\ncontext in a deep bidirectional model would cause information leak-\nage, i.e., allowing each item to indirectly “see the target item”. This\ncould make predicting the future become trivial and the network\nwould not learn anything useful.\nTo tackle this problem, we introduce the Cloze task [ 6, 50] to\ntake the place of the objective in unidirectional models (i.e., sequen-\ntially predicting the next item). Specifically, we randomly mask\nsome items (i.e., replace them with a special token [mask]) in the\ninput sequences, and then predict the ids of those masked items\nbased on their surrounding context. In this way, we avoid the in-\nformation leakage and learn a bidirectional representation model\nby allowing the representation of each item in the input sequence\nto fuse both the left and right context. In addition to training a\nbidirectional model, another advantage of the Cloze objective is\nthat it can produce more samples to train a more powerful model in\nmultiple epochs. However, a downside of the Cloze task is that it is\nnot consistent with the final task (i.e., sequential recommendation).\nTo fix this, during the test, we append the special token “[mask]” at\nthe end of the input sequence to indicate the item that we need to\npredict, and then make recommendations base on its final hidden\nvector. Extensive experiments on four datasets show that our model\noutperforms various state-of-the-art baselines consistently.\nThe contributions of our paper are as follows:\n•We propose to model user behavior sequences with a bidi-\nrectional self-attention network through Cloze task. To the\nbest of our knowledge, this is the first study to introduce\ndeep bidirectional sequential model and Cloze objective into\nthe field of recommendation systems.\n•We compare our model with state-of-the-art methods and\ndemonstrate the effectiveness of both bidirectional architec-\nture and the Cloze objective through quantitative analysis\non four benchmark datasets.\n•We conduct a comprehensive ablation study to analyze the\ncontributions of key components in the proposed model.\n2 RELATED WORK\nIn this section, we will briefly review several lines of works closely\nrelated to ours, including general recommendation, sequential rec-\nommendation, and attention mechanism.\n2.1 General Recommendation\nEarly works on recommendation systems typically use Collabo-\nrative Filtering (CF) to model users’ preferences based on their\ninteraction histories [ 26, 43]. Among various CF methods, Ma-\ntrix Factorization (MF) is the most popular one, which projects\nusers and items into a shared vector space and estimate a user’s\npreference on an item by the inner product between their vec-\ntors [26, 27, 41]. Another line of work is item-based neighborhood\nmethods [20, 25, 31, 43]. They estimate a user’s preference on an\nitem via measuring its similarities with the items in her/his interac-\ntion history using a precomputed item-to-item similarity matrix.\nRecently, deep learning has been revolutionizing the recommen-\ndation systems dramatically. The early pioneer work is a two-layer\nRestricted Boltzmann Machines (RBM) for collaborative filtering,\nproposed by Salakhutdinov et al. [42] in Netflix Prize1.\n1https://www.netflixprize.com\nOne line of deep learning based methods seeks to improve the rec-\nommendation performance by integrating the distributed item rep-\nresentations learned from auxiliary information, e.g., text [23, 53],\nimages [21, 55], and acoustic features [51] into CF models. Another\nline of work seeks to take the place of conventional matrix fac-\ntorization. For example, Neural Collaborative Filtering (NCF) [12]\nestimates user preferences via Multi-Layer Perceptions (MLP) in-\nstead of inner product, while AutoRec [44] and CDAE [57] predict\nusers’ ratings using Auto-encoder framework.\n2.2 Sequential Recommendation\nUnfortunately, none of the above methods is for sequential recom-\nmendation since they all ignore the order in users’ behaviors.\nEarly works on sequential recommendation usually capture se-\nquential patterns from user historical interactions using Markov\nchains (MCs). For example, Shani et al. [45] formalized recommen-\ndation generation as a sequential optimization problem and employ\nMarkov Decision Processes (MDPs) to address it. Later, Rendle\net al. [40] combine the power of MCs and MF to model both sequen-\ntial behaviors and general interests by Factorizing Personalized\nMarkov Chains (FPMC). Besides the first-order MCs, high-order\nMCs are also adopted to consider more previous items [10, 11].\nRecently, RNN and its variants, Gated Recurrent Unit (GRU) [4]\nand Long Short-Term Memory (LSTM) [17], are becoming more\nand more popular for modeling user behavior sequences [ 7, 14,\n15, 28, 37, 56, 58]. The basic idea of these methods is to encode\nuser’s previous records into a vector (i.e., representation of user’s\npreference which is used to make predictions) with various re-\ncurrent architectures and loss functions, including session-based\nGRU with ranking loss (GRU4Rec) [15], Dynamic REcurrent bAsket\nModel (DREAM) [58], user-based GRU [7], attention-based GRU\n(NARM) [28], and improved GRU4Rec with new loss function (i.e.,\nBPR-max and TOP1-max) and an improved sampling strategy [14].\nOther than recurrent neural networks, various deep learning\nmodels are also introduced for sequential recommendation [3, 22,\n33, 49]. For example, Tang and Wang [49] propose a Convolutional\nSequence Model (Caser) to learn sequential patterns using both\nhorizontal and vertical convolutional filters. Chen et al . [3] and\nHuang et al. [19] employ Memory Network to improve sequential\nrecommendation. STAMP captures both users’ general interests\nand current interests using an MLP network with attention [33].\n2.3 Attention Mechanism\nAttention mechanism has shown promising potential in modeling\nsequential data, e.g., machine translation [ 2, 52] and text classi-\nfication [? ]. Recently, some works try to employ the attention\nmechanism to improve recommendation performances and inter-\npretability [28, 33]. For example, Li et al . [28] incorporate an at-\ntention mechanism into GRU to capture both the user’s sequential\nbehavior and main purpose in session-based recommendation.\nThe works mentioned above basically treat attention mecha-\nnism as an additional component to the original models. In con-\ntrast, Transformer [52] and BERT [6] are built solely on multi-head\nself-attention and achieve state-of-the-art results on text sequence\nmodeling. Recently, there is a rising enthusiasm for applying purely\nattention-based neural networks to model sequential data for their\nv1 ··· vt−1 v[mask]\nv1 ··· [mask]vt−1\nTrm\n...\nTrm\n+\nTrm\n...\nTrm\n+\nTrm\n...\nTrm\n+\nL×\np1 ··· pt−1 pt\nProjection\nhL1 hLt−1\nvt\nhLt\nEmbedding\nLayer\n···\n...\n···\nMulti-Head\nAttention\nDropout\nAdd & Norm\nPosition-wise\nFeed-Forward\nDropout\nAdd & Norm\ninput\nTrm\n(b) BERT4Rec model architecture.(a) Transformer Layer.\n···\nv1 . . . vtvt−1\nGRU GRU GRU. . .\nv2 . . . vt vt+1\n(d) RNN based sequential recommendation methods.\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nv1 vt−1 vt\n. . .\n. . .\n. . .\n. . .\nv2 vt vt+1\n(c) SASRec model architecture.\nFigure 1: Differences in sequential recommendation model architectures. BERT4Rec learns a bidirectional model via Cloze\ntask, while SASRec and RNN based methods are all left-to-right unidirectional model which predict next item sequentially.\neffectiveness and efficiency [30, 32, 38, 46? ]. For sequential recom-\nmendation, Kang and McAuley [22] introduce a two-layer Trans-\nformer decoder (i.e., Transformer language model) called SASRec\nto capture user’s sequential behaviors and achieve state-of-the-art\nresults on several public datasets. SASRec is closely related to our\nwork. However, it is still a unidirectional model using a casual at-\ntention mask. While we use a bidirectional model to encode users’\nbehavior sequences with the help of Cloze task.\n3 BERT4REC\nBefore going into the details, we first introduce the research prob-\nlem, the basic concepts, and the notations in this paper.\n3.1 Problem Statement\nIn sequential recommendation, let U={u1,u2, . . . ,u|U|}denote\na set of users, V={v1,v2, . . . ,v|V|}be a set of items, and list\nSu =[v(u)\n1 , . . . ,v(u)\nt , . . . ,v(u)\nnu ]denote the interaction sequence in\nchronological order for user u ∈U, where v(u)\nt ∈V is the item\nthat u has interacted with at time step2 t and nu is the the length of\ninteraction sequence for user u. Given the interaction history Su ,\nsequential recommendation aims to predict the item that useru will\ninteract with at time step nu + 1. It can be formalized as modeling\nthe probability over all possible items for user u at time step nu +1:\np \u0000v(u)\nnu +1 = v|Su\n\u0001\n3.2 Model Architecture\nHere, we introduce a new sequential recommendation model called\nBERT4Rec, which adopts Bidirectional Encoder Representations\nfrom Transformers to a new task, sequential Recommendation. It\nis built upon the popular self-attention layer, “Transformer layer”.\nAs illustrated in Figure 1b, BERT4Rec is stacked by L bidirec-\ntional Transformer layers. At each layer, it iteratively revises the\n2Here, following [22, 40], we use the relative time index instead of absolute time index\nfor numbering interaction records.\nrepresentation of every position by exchanging information across\nall positions at the previous layer in parallel with the Transformer\nlayer. Instead of learning to pass relevant information forward step\nby step as RNN based methods did in Figure 1d, self-attention mech-\nanism endows BERT4Rec with the capability to directly capture\nthe dependencies in any distances. This mechanism results in a\nglobal receptive field, while CNN based methods like Caser usually\nhave a limited receptive field. In addition, in contrast to RNN based\nmethods, self-attention is straightforward to parallelize.\nComparing Figure 1b, 1c, and 1d, the most noticeable difference\nis that SASRec and RNN based methods are all left-to-right uni-\ndirectional architecture, while our BERT4Rec uses bidirectional\nself-attention to model users’ behavior sequences. In this way, our\nproposed model can obtain more powerful representations of users’\nbehavior sequences to improve recommendation performances.\n3.3 Transformer Layer\nAs illustrated in Figure 1b, given an input sequence of length t, we\niteratively compute hidden representations hl\ni at each layer l for\neach position i simultaneously by applying the Transformer layer\nfrom [52]. Here, we stack hl\ni ∈Rd together into matrix Hl ∈Rt×d\nsince we compute attention function on all positions simultane-\nously in practice. As shown in Figure 1a, the Transformer layerTrm\ncontains two sub-layers, a Multi-Head Self-Attention sub-layer and\na Position-wise Feed-Forward Network .\nMulti-Head Self-Attention. Attention mechanisms have be-\ncome an integral part of sequence modeling in a variety of tasks,\nallowing capturing the dependencies between representation pairs\nwithout regard to their distance in the sequences. Previous work\nhas shown that it is beneficial to jointly attend to information from\ndifferent representation subspaces at different positions [6, 29, 52].\nThus, we here adopt the multi-head self-attention instead of per-\nforming a single attention function. Specifically, multi-head at-\ntention first linearly projects Hl into h subspaces, with different,\nlearnable linear projections, and then apply h attention functions\nin parallel to produce the output representations which are con-\ncatenated and once again projected:\nMH(Hl )= [head1; head2; . . .; headh]W O\nheadi = Attention\u0000HlW Q\ni , HlW K\ni , HlW V\ni\n\u0001 (1)\nwhere the projections matrices for each headW Q\ni ∈Rd×d/h,W K\ni ∈\nRd×d/h,W V\ni ∈Rd×d/h, andW O\ni ∈Rd×d are learnable parameters.\nHere, we omit the layer subscriptl for the sake of simplicity. In fact,\nthese projection parameters are not shared across the layers. Here,\nthe Attention function is Scaled Dot-Product Attention :\nAttention(Q, K,V )= softmax\n\u0012QK ⊤\np\nd/h\n\u0013\nV (2)\nwhere query Q, key K, and value V are projected from the same ma-\ntrix Hl with different learned projection matrices as in Equation 1.\nThe temperature\np\nd/h is introduced to produce a softer attention\ndistribution for avoiding extremely small gradients [16, 52].\nPosition-wise Feed-Forward Network . As described above,\nthe self-attention sub-layer is mainly based on linear projections.\nTo endow the model with nonlinearity and interactions between\ndifferent dimensions, we apply a Position-wise Feed-Forward Net-\nwork to the outputs of the self-attention sub-layer, separately and\nidentically at each position. It consists of two affine transformations\nwith a Gaussian Error Linear Unit (GELU) activation in between:\nPFFN(Hl )=\n\u0002\nFFN(hl\n1)⊤; . . .; FFN(hl\nt )⊤\u0003⊤\nFFN(x)= GELU\u0000xW (1)+ b(1)\u0001W (2)+ b(2)\nGELU(x)= xΦ(x)\n(3)\nwhere Φ(x)is the cumulative distribution function of the standard\ngaussian distribution, W (1) ∈Rd×4d , W (2) ∈R4d×d , b(1) ∈R4d\nand b(2)∈Rd are learnable parameters and shared across all posi-\ntions. We omit the layer subscriptl for convenience. In fact, these\nparameters are different from layer to layer. In this work, follow-\ning OpenAI GPT [38] and BERT [6], we use a smoother GELU [13]\nactivation rather than the standard ReLu activation.\nStacking Transformer Layer . As elaborated above, we can\neasily capture item-item interactions across the entire user be-\nhavior sequence using self-attention mechanism. Nevertheless, it\nis usually beneficial to learn more complex item transition pat-\nterns by stacking the self-attention layers. However, the network\nbecomes more difficult to train as it goes deeper. Therefore, we\nemploy a residual connection [ 9] around each of the two sub-\nlayers as in Figure 1a, followed by layer normalization [1]. More-\nover, we also apply dropout [47] to the output of each sub-layer,\nbefore it is normalized. That is, the output of each sub-layer is\nLN(x + Dropout(sublayer(x))), where sublayer(·)is the function\nimplemented by the sub-layer itself, LN is the layer normalization\nfunction defined in [1]. We use LN to normalize the inputs over all\nthe hidden units in the same layer for stabilizing and accelerating\nthe network training.\nIn summary, BERT4Rec refines the hidden representations of\neach layer as follows:\nHl = Trm\u0000Hl−1\u0001, ∀i ∈[1, . . . ,L] (4)\nTrm(Hl−1)= LN\n\u0010\nAl−1 + Dropout\u0000PFFN(Al−1)\u0001\u0011\n(5)\nAl−1 = LN\n\u0010\nHl−1 + Dropout\u0000MH(Hl−1)\u0001\u0011\n(6)\n3.4 Embedding Layer\nAs elaborated above, without any recurrence or convolution mod-\nule, the Transformer layerTrm is not aware of the order of the input\nsequence. In order to make use of the sequential information of the\ninput, we inject Positional Embeddings into the input item embed-\ndings at the bottoms of the Transformer layer stacks. For a given\nitem vi , its input representation h0\ni is constructed by summing the\ncorresponding item and positional embedding:\nh0\ni = vi + pi\nwhere vi ∈E is the d−dimensional embedding for item vi , pi ∈P\nis the d−dimensional positional embedding for position index i.\nIn this work, we use the learnable positional embeddings instead\nof the fixed sinusoid embeddings in [52] for better performances.\nThe positional embedding matrix P ∈RN ×d allows our model to\nidentify which portion of the input it is dealing with. However,\nit also imposes a restriction on the maximum sentence length N\nthat our model can handle. Thus, we need to truncate the the input\nsequence [v1, . . . ,vt ]to the last N items [vu\nt−N +1, . . . ,vt ]if t > N .\n3.5 Output Layer\nAfter L layers that hierarchically exchange information across all\npositions in the previous layer, we get the final outputHL for all\nitems of the input sequence. Assuming that we mask the itemvt\nat time step t, we then predict the masked items vt base on hL\nt as\nshown in Figure 1b. Specifically, we apply a two-layer feed-forward\nnetwork with GELU activation in between to produce an output\ndistribution over target items:\nP(v)= softmax\u0000GELU(hL\nt W P + bP )E⊤+ bO \u0001 (7)\nwhere W P is the learnable projection matrix, bP , and bO are bias\nterms, E ∈R|V|×d is the embedding matrix for the item set V. We\nuse the shared item embedding matrix in the input and output layer\nfor alleviating overfitting and reducing model size.\n3.6 Model Learning\nTraining. Conventional unidirectional sequential recommendation\nmodels usually train the model by predicting the next item for each\nposition in the input sequence as illustrated in Figure 1c and 1d.\nSpecifically, the target of the input sequence[v1, . . . ,vt ]is a shifted\nversion [v2, . . . ,vt+1]. However, as shown in Figure 1b, jointly con-\nditioning on both left and right context in a bidirectional model\nwould cause the final output representation of each item to contain\nthe information of the target item. This makes predicting the future\nbecome trivial and the network would not learn anything useful.\nA simple solution for this issue is to create t −1 samples (subse-\nquences with next items like ([v1], v2) and ([v1,v2], v3)) from the\noriginal length t behavior sequence and then encode each historical\nsubsequence with the bidirectional model to predict the target item.\nHowever, this approach is very time and resources consuming since\nwe need to create a new sample for each position in the sequence\nand predict them separately.\nIn order to efficiently train our proposed model, we apply a\nnew objective: Cloze task [50] (also known as “Masked Language\nModel” in [6]) to sequential recommendation. It is a test consisting\nof a portion of language with some words removed, where the\nparticipant is asked to fill the missing words. In our case, for each\ntraining step, we randomly mask ρ proportion of all items in the\ninput sequence (i.e., replace with special token “[mask]”), and then\npredict the original ids of the masked items based solely on its left\nand right context. For example:\nInput: [v1,v2,v3,v4,v5] [v1, [mask]1,v3, [mask]2,v5]\nLabels: [mask]1 = v2, [mask]2 = v4\nrandomly mask\nThe final hidden vectors corresponding to “ [mask]” are fed into\nan output softmax over the item set, as in conventional sequential\nrecommendation. Eventually, we define the loss for each masked\ninput S′u as the negative log-likelihood of the masked targets:\nL= 1\n|Smu |\nÕ\nvm ∈Smu\n−log P(vm = v∗\nm |S′\nu ) (8)\nwhere S′u is the masked version for user behavior history Su , Smu\nis the random masked items in it,v∗m is the true item for the masked\nitem vm, and the probability P(·)is defined in Equation 7.\nAn additional advantage for Cloze task is that it can generate\nmore samples to train the model. Assuming a sequence of length\nn, conventional sequential predictions in Figure 1c and 1d produce\nn unique samples for training, while BERT4Rec can obtain \u0000n\nk\n\u0001\nsamples (if we randomly maskk items) in multiple epochs. It allows\nus to train a more powerful bidirectional representation model.\nTest. As described above, we create a mismatch between the\ntraining and the final sequential recommendation task since the\nCloze objective is to predict the current masked items while sequen-\ntial recommendation aims to predict the future. To address this, we\nappend the special token “ [mask]” to the end of user’s behavior\nsequence, and then predict the next item based on the final hidden\nrepresentation of this token. To better match the sequential recom-\nmendation task (i.e., predict the last item), we also produce samples\nthat only mask the last item in the input sequences during training.\nIt works like fine-tuning for sequential recommendation and can\nfurther improve the recommendation performances.\n3.7 Discussion\nHere, we discuss the relation of our model with previous related\nwork.\nSASRec. Obviously, SASRec is a left-to-right unidirectional ver-\nsion of our BERT4Rec with single head attention and causal atten-\ntion mask. Different architectures lead to different training methods.\nSASRec predicts the next item for each position in a sequence, while\nBERT4Rec predicts the masked items in the sequence using Cloze\nobjective.\nCBOW & SG . Another very similar work is Continuous Bag-of-\nWords (CBOW) and Skip-Gram (SG) [35]. CBOW predicts a target\nword using the average of all the word vectors in its context (both\nTable 1: Statistics of datasets.\nDatasets #users #items #actions Avg. length Density\nBeauty 40,226 54 ,542 0.35m 8.8 0.02%\nSteam 281,428 13 ,044 3.5m 12.4 0.10%\nML-1m 6040 3416 1.0m 163.5 4.79%\nML-20m 138,493 26 ,744 20m 144.4 0.54%\nleft and right). It can be seen as a simplified case of BERT4Rec, if we\nuse one self-attention layer in BERT4Rec with uniform attention\nweights on items, unshare item embeddings, remove the positional\nembedding, and only mask the central item. Similar to CBOW, SG\ncan also be seen as a simplified case of BERT4Rec following similar\nreduction operations (mask all items except only one). From this\npoint of view, Cloze can be seen as a general form for the objective of\nCBOW and SG. Besides, CBOW uses a simple aggregator to model\nword sequences since its goal is to learn good word representations,\nnot sentence representations. On the contrary, we seek to learn\na powerful behavior sequence representation model (deep self-\nattention network in this work) for making recommendations.\nBERT. Although our BERT4Rec is inspired by the BERT in NLP,\nit still has several differences from BERT: a) The most critical dif-\nference is that BERT4Rec is an end-to-end model for sequential\nrecommendation, while BERT is a pre-training model for sentence\nrepresentation. BERT leverages large-scale task-independent cor-\npora to pre-train the sentence representation model for various text\nsequence tasks since these tasks share the same background knowl-\nedge about the language. However, this assumption does not hold\nin the recommendation tasks. Thus we train BERT4Rec end-to-end\nfor different sequential recommendation datasets. b) Different from\nBERT, we remove the next sentence loss and segment embeddings\nsince BERT4Rec models a user’s historical behaviors as only one\nsequence in sequential recommendation task.\n4 EXPERIMENTS\n4.1 Datasets\nWe evaluate the proposed model on four real-world representative\ndatasets which vary significantly in domains and sparsity.\n•Amazon Beauty3: This is a series of product review datasets\ncrawled from Amazon.com by McAuley et al . [34]. They\nsplit the data into separate datasets according to the top-\nlevel product categories on Amazon. In this work, we adopt\nthe “Beauty” category.\n•Steam4: This is a dataset collected from Steam, a large online\nvideo game distribution platform, by Kang and McAuley [22].\n•MovieLens [8]: This is a popular benchmark dataset for\nevaluating recommendation algorithms. In this work, we\nadopt two well-established versions, MovieLens 1m ( ML-\n1m)5 and MovieLens 20m (ML-20m)6.\nFor dataset preprocessing, we follow the common practice in\n[22, 40, 49]. For all datasets, we convert all numeric ratings or the\npresence of a review to implicit feedback of 1 (i.e., the user interacted\n3http://jmcauley.ucsd.edu/data/amazon/\n4https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data\n5https://grouplens.org/datasets/movielens/1m/\n6https://grouplens.org/datasets/movielens/20m/\nwith the item). After that, we group the interaction records by users\nand build the interaction sequence for each user by sorting these\ninteraction records according to the timestamps. To ensure the\nquality of the dataset, following the common practice [12, 22, 40, 49],\nwe keep users with at least five feedbacks. The statistics of the\nprocessed datasets are summarized in Table 1.\n4.2 Task Settings & Evaluation Metrics\nTo evaluate the sequential recommendation models, we adopted\nthe leave-one-out evaluation (i.e., next item recommendation) task,\nwhich has been widely used in [12, 22, 49]. For each user, we hold\nout the last item of the behavior sequence as the test data, treat\nthe item just before the last as the validation set, and utilize the\nremaining items for training. For easy and fair evaluation, we follow\nthe common strategy in [12, 22, 49], pairing each ground truth item\nin the test set with 100 randomly sampled negative items that the\nuser has not interacted with. To make the sampling reliable and\nrepresentative [19], these 100 negative items are sampled according\nto their popularity. Hence, the task becomes to rank these negative\nitems with the ground truth item for each user.\nEvaluation Metrics. To evaluate the ranking list of all the mod-\nels, we employ a variety of evaluation metrics, including Hit Ratio\n(HR), Normalized Discounted Cumulative Gain (NDCG), and Mean\nReciprocal Rank (MRR). Considering we only have one ground truth\nitem for each user, HR@k is equivalent to Recall@k and propor-\ntional to Precision@k; MRR is equivalent to Mean Average Precision\n(MAP). In this work, we report HR and NDCG withk = 1, 5, 10. For\nall these metrics, the higher the value, the better the performance.\n4.3 Baselines & Implementation Details\nTo verify the effectiveness of our method, we compare it with the\nfollowing representative baselines:\n•POP: It is the simplest baseline that ranks items according to\ntheir popularity judged by the number of interactions.\n•BPR-MF [39]: It optimizes the matrix factorization with im-\nplicit feedback using a pairwise ranking loss.\n•NCF [12]: It models userâĂŞitem interactions with a MLP\ninstead of the inner product in matrix factorization.\n•FPMC [40]: It captures users’ general taste as well as their\nsequential behaviors by combing MF with first-order MCs.\n•GRU4Rec [15]: It uses GRU with ranking based loss to model\nuser sequences for session based recommendation.\n•GRU4Rec+ [14]: It is an improved version of GRU4Rec with\na new class of loss functions and sampling strategy.\n•Caser [49]: It employs CNN in both horizontal and vertical\nway to model high-order MCs for sequential recommendation.\n•SASRec [22]: It uses a left-to-right Transformer language\nmodel to capture users’ sequential behaviors, and achieves\nstate-of-the-art performance on sequential recommendation.\nFor NCF7, GRU4Rec8, GRU4Rec+8, Caser9, and SASRec10, we\nuse code provided by the corresponding authors. For BPR-MF and\n7https://github.com/hexiangnan/neural_collaborative_filtering\n8https://github.com/hidasib/GRU4Rec\n9https://github.com/graytowne/caser_pytorch\n10https://github.com/kang205/SASRec\nFPMC, we implement them using TensorFlow. For common hyper-\nparameters in all models, we consider the hidden dimension size\nd from {16, 32, 64, 128, 256}, the ℓ2 regularizer from {1, 0.1, 0.01,\n0.001, 0.0001}, and dropout rate from {0, 0.1, 0.2, ··· , 0.9}. All other\nhyper-parameters (e.g., Markov order in Caser) and initialization\nstrategies are either followed the suggestion from the methods’\nauthors or tuned on the validation sets. We report the results of\neach baseline under its optimal hyper-parameter settings.\nWe implement BERT4Rec11 with TensorFlow. All parameters\nare initialized using truncated normal distribution in the range\n[−0.02, 0.02]. We train the model using Adam [24] with learning\nrate of 1e-4, β1 = 0.9, β2 = 0.999, ℓ2 weight decay of 0.01, and\nlinear decay of the learning rate. The gradient is clipped when its\nℓ2 norm exceeds a threshold of 5. For fair comparison, we set the\nlayer number L = 2 and head number h = 2 and use the same\nmaximum sequence length as in [ 22], N = 200 for ML-1m and\nML-20m, N = 50 for Beauty and Steam datasets. For head setting,\nwe empirically set the dimensionality of each head as 32 (single\nhead if d < 32). We tune the mask proportionρusing the validation\nset, resulting in ρ = 0.6 for Beauty, ρ = 0.4 for Steam, and ρ = 0.2\nfor ML-1m and ML-20m. All the models are trained from scratch\nwithout any pre-training on a single NVIDIA GeForce GTX 1080\nTi GPU with a batch size of 256.\n4.4 Overall Performance Comparison\nTable 2 summarized the best results of all models on four benchmark\ndatasets. The last column is the improvements of BERT4Rec relative\nto the best baseline. We omit the NDCG@1 results since it is equal\nto HR@1 in our experiments. It can be observed that:\nThe non-personalized POP method gives the worst performance12\non all datasets since it does not model user’s personalized prefer-\nence using the historical records. Among all the baseline meth-\nods, sequential methods (e.g., FPMC and GRU4Rec+) outperforms\nnon-sequential methods ( e.g., BPR-MF and NCF) on all datasets\nconsistently. Compared with BPR-MF, the main improvement of\nFPMC is that it models users’ historical records in a sequential way.\nThis observation verifies that considering sequential information is\nbeneficial for improving performances in recommendation systems.\nAmong sequential recommendation baselines, Caser outper-\nforms FPMC on all datasets especially for the dense dataset ML-1m,\nsuggesting that high-order MCs is beneficial for sequential rec-\nommendation. However, high-order MCs usually use very small\norder L since they do not scale well with the order L. This causes\nCaser to perform worse than GRU4Rec + and SASRec, especially\non sparse datasets. Furthermore, SASRec performs distinctly bet-\nter than GRU4Rec and GRU4Rec+, suggesting that self-attention\nmechanism is a more powerful tool for sequential recommendation.\nAccording to the results, it is obvious that BERT4Rec performs\nbest among all methods on four datasets in terms of all evaluation\nmetrics. It gains 7.24% HR@10, 11.03% NDCG@10, and 11.46%\nMRR improvements (on average) against the strongest baselines.\nQuestion 1: Do the gains come from the bidirectional self-attention\nmodel or from the Cloze objective?\n11https://github.com/FeiSun/BERT4Rec\n12What needs to be pointed out is that such low scores for POP are because the negative\nsamples are sampled according to the items’ popularity.\nTable 2: Performance comparison of different methods on next-item prediction. Bold scores are the best in each row, while\nunderlined scores are the second best. Improvements over baselines are statistically significant with p < 0.01.\nDatasets Metric POP BPR-MF NCF FPMC GRU4Rec GRU4Rec + Caser SASRec BERT4Rec Improv.\nBeauty\nHR@1 0.0077 0.0415 0.0407 0.0435 0.0402 0.0551 0.0475 0.0906 0.0953 5.19%\nHR@5 0.0392 0.1209 0.1305 0.1387 0.1315 0.1781 0.1625 0.1934 0.2207 14.12%\nHR@10 0.0762 0.1992 0.2142 0.2401 0.2343 0.2654 0.2590 0.2653 0.3025 14.02%\nNDCG@5 0.0230 0.0814 0.0855 0.0902 0.0812 0.1172 0.1050 0.1436 0.1599 11.35%\nNDCG@10 0.0349 0.1064 0.1124 0.1211 0.1074 0.1453 0.1360 0.1633 0.1862 14.02%\nMRR 0.0437 0.1006 0.1043 0.1056 0.1023 0.1299 0.1205 0.1536 0.1701 10.74%\nSteam\nHR@1 0.0159 0.0314 0.0246 0.0358 0.0574 0.0812 0.0495 0.0885 0.0957 8.14%\nHR@5 0.0805 0.1177 0.1203 0.1517 0.2171 0.2391 0.1766 0.2559 0.2710 5.90%\nHR@10 0.1389 0.1993 0.2169 0.2551 0.3313 0.3594 0.2870 0.3783 0.4013 6.08%\nNDCG@5 0.0477 0.0744 0.0717 0.0945 0.1370 0.1613 0.1131 0.1727 0.1842 6.66%\nNDCG@10 0.0665 0.1005 0.1026 0.1283 0.1802 0.2053 0.1484 0.2147 0.2261 5.31%\nMRR 0.0669 0.0942 0.0932 0.1139 0.1420 0.1757 0.1305 0.1874 0.1949 4.00%\nML-1m\nHR@1 0.0141 0.0914 0.0397 0.1386 0.1583 0.2092 0.2194 0.2351 0.2863 21.78%\nHR@5 0.0715 0.2866 0.1932 0.4297 0.4673 0.5103 0.5353 0.5434 0.5876 8.13%\nHR@10 0.1358 0.4301 0.3477 0.5946 0.6207 0.6351 0.6692 0.6629 0.6970 4.15%\nNDCG@5 0.0416 0.1903 0.1146 0.2885 0.3196 0.3705 0.3832 0.3980 0.4454 11.91%\nNDCG@10 0.0621 0.2365 0.1640 0.3439 0.3627 0.4064 0.4268 0.4368 0.4818 10.32%\nMRR 0.0627 0.2009 0.1358 0.2891 0.3041 0.3462 0.3648 0.3790 0.4254 12.24%\nML-20m\nHR@1 0.0221 0.0553 0.0231 0.1079 0.1459 0.2021 0.1232 0.2544 0.3440 35.22%\nHR@5 0.0805 0.2128 0.1358 0.3601 0.4657 0.5118 0.3804 0.5727 0.6323 10.41%\nHR@10 0.1378 0.3538 0.2922 0.5201 0.5844 0.6524 0.5427 0.7136 0.7473 4.72%\nNDCG@5 0.0511 0.1332 0.0771 0.2239 0.3090 0.3630 0.2538 0.4208 0.4967 18.04%\nNDCG@10 0.0695 0.1786 0.1271 0.2895 0.3637 0.4087 0.3062 0.4665 0.5340 14.47%\nMRR 0.0709 0.1503 0.1072 0.2273 0.2967 0.3476 0.2529 0.4026 0.4785 18.85%\nTable 3: Analysis on bidirection and Cloze with d = 256.\nModel Beauty ML-1m\nHR@10 NDCG@10 MRR HR@10 NDCG@10 MRR\nSASRec 0.2653 0.1633 0.1536 0.6629 0.4368 0.3790\nBERT4Rec (1 mask) 0.2940 0.1769 0.1618 0.6869 0.4696 0.4127\nBERT4Rec 0.3025 0.1862 0.1701 0.6970 0.4818 0.4254\nTo answer this question, we try to isolate the effects of these\ntwo factors by constraining the Cloze task to mask only one item\nat a time. In this way, the main difference between our BERT4Rec\n(with 1 mask) and SASRec is that BERT4Rec predicts the target\nitem jointly conditioning on both left and right context. We report\nthe results on Beauty and ML-1m with d = 256 in Table 3 due to\nthe space limitation. The results show that BERT4Rec with 1 mask\nsignificantly outperforms SASRec on all metrics. It demonstrates\nthe importance of bidirectional representations for sequential rec-\nommendation. Besides, the last two rows indicate that the Cloze\nobjective also improves the performances. Detailed analysis of the\nmask proportion ρ in Cloze task can be found in § 4.6\nQuestion 2: Why and how does bidirectional model outperform uni-\ndirectional models?\nTo answer this question, we try to reveal meaningful patterns\nby visualizing the average attention weights of the last 10 items\nduring the test on Beauty in Figure 2. Due to the space limitation,\nwe only report four representative attention heat-maps in different\nlayers and heads.\n0 1 2 3 4 5 6 7 8 9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.1\n0.2\n0.3\n0.4\n0.5\n(a) Layer 1, head 1\n0 1 2 3 4 5 6 7 8 9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.1\n0.2\n0.3\n0.4\n0.5 (b) Layer 1, head 2\n0 1 2 3 4 5 6 7 8 9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n 0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n(c) Layer 2, head 2\n0 1 2 3 4 5 6 7 8 9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200 (d) Layer 2, head 4\nFigure 2: Heat-maps of average attention weights on Beauty,\nthe last position “9” denotes “ [mask]” (best viewed in color).\nWe make several observations from the results. a) Attention\nvaries across different heads. For example, in layer 1, head 1 tends\n16 32 64 128 256\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\nDimensionality\nNDCG@10\nBeauty\n16 32 64 128 256\n0.12\n0.14\n0.16\n0.18\n0.2\n0.22\nDimensionality\nNDCG@10\nSteam\n16 32 64 128 256\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nDimensionality\nNDCG@10\nML-1m\n16 32 64 128 256\n0.2\n0.3\n0.4\n0.5\nDimensionality\nNDCG@10\nML-20m\n16 32 64 128 256\n0.15\n0.2\n0.25\n0.3\nDimensionality\nHR@10\nBeauty\n16 32 64 128 256\n0.25\n0.3\n0.35\n0.4\nDimensionality\nHR@10\nSteam\n16 32 64 128 256\n0.5\n0.55\n0.6\n0.65\n0.7\nDimensionality\nHR@10\nML-1m\n16 32 64 128 256\n0.4\n0.5\n0.6\n0.7\nDimensionality\nHR@10\nML-20m\nGRU4Rec GRU4Rec+ Caser SASRec BERT4Rec\nFigure 3: Effect of the hidden dimensionality d on HR@10 and NDCG@10 for neural sequential models.\nto attend on items at the left side while head 2 prefers to attend on\nitems on the right side. b) Attention varies across different layers.\nApparently, attentions in layer 2 tend to focus on more recent items.\nThis is because layer 2 is directly connected to the output layer\nand the recent items play a more important role in predicting the\nfuture. Another interesting pattern is that heads in Figure 2a and 2b\nalso tend to attend on [mask]13. It may be a way for self-attention\nto propagate sequence-level state to the item level. c) Finally and\nmost importantly, unlike unidirectional model can only attend on\nitems at the left side, items in BERT4Rec tend to attend on the\nitems at both sides. This indicates that bidirectional is essential and\nbeneficial for user behavior sequence modeling.\nIn the following studies, we examine the impact of the hyper-\nparameters, including the hidden dimensionality d, the mask pro-\nportion ρ, and the maximum sequence length N . We analyze one\nhyper-parameter at a time by fixing the remaining hyper-parameters\nat their optimal settings. Due to space limitation, we only report\nNDCG@10 and HR@10 for the follow-up experiments.\n4.5 Impact of Hidden Dimensionality d\nWe now study how the hidden dimensionalityd affects the recom-\nmendation performance. Figure 3 shows NDCG@10 and HR@10 for\nneural sequential methods with the hidden dimensionality d vary-\ning from 16 to 256 while keeping other optimal hyper-parameters\nunchanged. We make some observations from this figure.\nThe most obvious observation from these sub-figures is that the\nperformance of each model tends to converge as the dimensional-\nity increases. A larger hidden dimensionality does not necessarily\nlead to better model performance, especially on sparse datasets\nlike Beauty and Steam. This is probably caused by overfitting. In\nterms of details, Caser performs unstably on four datasets, which\nmight limit its usefulness. Self-attention based methods (i.e., SASRec\nand BERT4Rec) achieve superior performances on all datasets. Fi-\nnally, our model consistently outperforms all other baselines on all\n13This phenomenon also exists in text sequence modeling using BERT.\ndatasets even with a relatively small hidden dimensionality. Consid-\nering that our model achieves satisfactory performance with d≥64,\nwe only report the results with d=64 in the following analysis.\n4.6 Impact of Mask Proportion ρ\n0.10.20.30.40.50.60.70.80.90.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDimensionality\nHR@10\n0.10.20.30.40.50.60.70.80.90.1\n0.2\n0.3\n0.4\n0.5\nDimensionality\nNDCG@10\nBeauty Steam ML-1m ML-20m\nFigure 4: Performance with different mask proportion ρ on\nd = 64. Bold symbols denote the best scores in each line.\nAs described in § 3.6, mask proportion ρ is a key factor in model\ntraining, which directly affects the loss function (Equation 8). Ob-\nviously, mask proportion ρ should not be too small or it is not\nenough to learn a strong model. Meanwhile, it should not be too\nlarge, otherwise, it would be hard to train since there are too many\nitems to guess based on a few contexts in such case. To examine\nthis, we study how mask proportion ρ affects the recommendation\nperformances on different datasets.\nFigure 4 shows the results with varying mask proportion ρ from\n0.1 to 0.9. Considering the results with ρ > 0.6 on all datasets,\na general pattern emerges, the performances decreasing as ρ in-\ncreases. From the results of the first two columns, it is easy to see\nthat ρ = 0.2 performs better than ρ = 0.1 on all datasets. These\nresults verify what we claimed above.\nIn addition, we observe that the optimal ρ is highly dependent\non the sequence length of the dataset. For the datasets with short\nsequence length (e.g., Beauty and Steam), the best performances are\nTable 4: Performance with different maximum length N .\n10 20 30 40 50\nBeauty\n#samples/s 5504 3256 2284 1776 1441\nHR@10 0.3006 0.3061 0.3057 0.3054 0.3047\nNDCG@10 0.1826 0.1875 0.1837 0.1833 0.1832\n10 50 100 200 400\nML-1m\n#samples/s 14255 8890 5711 2918 1213\nHR@10 0.6788 0.6854 0.6947 0.6955 0.6898\nNDCG@10 0.4631 0.4743 0.4758 0.4759 0.4715\nachieved at ρ=0.6 (Beauty) and ρ=0.4 (Steam), while the datasets\nwith long sequence length (e.g., ML-1m and ML-20m) prefer a small\nρ=0.2. This is reasonable since, compared with short sequence\ndatasets, a large ρ in long sequence datasets means much more\nitems that need to be predicted. Take ML-1m and Beauty as example,\nρ=0.6 means we need to predict 98=⌊163.5×0.6⌋items on average\nper sequence for ML-1m, while it is only 5=⌊8.8×0.6⌋items for\nBeauty. The former is too hard for model training.\n4.7 Impact of Maximum Sequence Length N\nWe also investigate the effect of the maximum sequence length N\non model’s recommendation performances and efficiency.\nTable 4 shows recommendation performances and training speed\nwith different maximum length N on Beauty and ML-1m. We ob-\nserve that the proper maximum length N is also highly dependent\non the average sequence length of the dataset. Beauty prefers a\nsmaller N = 20, while ML-1m achieves the best performances on\nN = 200. This indicates that a user’s behavior is affected by more\nrecent items on short sequence datasets and less recent items for\nlong sequence datasets. The model does not consistently benefit\nfrom a larger N since a larger N tends to introduce both extra infor-\nmation and more noise. However, our model performs very stably\nas the length N becomes larger. This indicates that our model can\nattend to the informative items from the noisy historical records.\nA scalability concern about BERT4Rec is that its computational\ncomplexity per layer is O(n2d), quadratic with the length n. Fortu-\nnately, the results in Table 4 shows that the self-attention layer can\nbe effectively parallelized using GPUs.\n4.8 Ablation Study\nFinally, we perform ablation experiments over a number of key com-\nponents of BERT4Rec in order to better understand their impacts,\nincluding positional embedding (PE), position-wise feed-forward\nnetwork (PFFN), layer normalization (LN), residual connection (RC),\ndropout, the layer number L of self-attention, and the number of\nheads h in multi-head attention. Table 5 shows the results of our\ndefault version (L = 2,h = 2) and its eleven variants on all four\ndatasets with dimensionality d = 64 while keeping other hyper-\nparameters (e.g., ρ) at their optimal settings.\nWe introduce the variants and analyze their effects respectively:\n(1) PE. The results show that removing positional embeddings\ncauses BERT4Rec’s performances decreasing dramatically on\nlong sequence datasets (i.e., ML-1m and ML-20m). Without\nthe positional embeddings, the hidden representation HL\ni\nTable 5: Ablation analysis (NDCG@10) on four datasets.\nBold score indicates performance better than the default ver-\nsion, while ↓indicates performance drop more than 10%.\nArchitecture Dataset\nBeauty Steam ML-1m ML-20m\nL = 2, h = 2 0.1832 0.2241 0.4759 0.4513\nw/o PE 0.1741 0.2060 0.2155 ↓ 0.2867↓\nw/o PFFN 0.1803 0.2137 0.4544 0.4296\nw/o LN 0.1642 ↓ 0.2058 0.4334 0.4186\nw/o RC 0.1619 ↓ 0.2193 0.4643 0.4483\nw/o Dropout 0.1658 0.2185 0.4553 0.4471\n1 layer ( L = 1) 0.1782 0.2122 0.4412 0.4238\n3 layers ( L = 3) 0.1859 0.2262 0.4864 0.4661\n4 layers ( L = 4) 0.1834 0.2279 0.4898 0.4732\n1 head ( h = 1) 0.1853 0.2187 0.4568 0.4402\n4 heads ( h = 4) 0.1830 0.2245 0.4770 0.4520\n8 heads ( h = 8) 0.1823 0.2248 0.4743 0.4550\nfor each item vi depends only on item embeddings. In this\nsituation, we predict different target items using the same\nhidden representation of “[mask]”. This makes the model ill-\nposed. This issue is more serious on long sequence datasets\nsince they have more masked items to predict.\n(2) PFFN. The results show that long sequence datasets ( e.g.,\nML-20m) benefit more from PFFN. This is reasonable since a\npurpose of PFFN is to integrate information from many heads\nwhich are preferred by long sequence datasets as discussed\nin the analysis about head number h in ablation study (5).\n(3) LN, RC, and Dropout. These components are introduced\nmainly to alleviate overfitting. Obviously, they are more\neffective on small datasets like Beauty. To verify their ef-\nfectiveness on large datasets, we conduct an experiment on\nML-20m with layer L=4. The results show that NDCG@10\ndecreases about 10% w/o RC.\n(4) Number of layers L. The results show that stacking Trans-\nformer layer can boost performances especially on large\ndatasets (e.g, ML-20m). This verifies that it is helpful to learn\nmore complex item transition patterns via deep self-attention\narchitecture. The decline in Beauty with L = 4 is largely due\nto overfitting.\n(5) Head number h. We observe that long sequence datasets\n(e.g., ML-20m) benefit from a larger h while short sequence\ndatasets (e.g., Beauty) prefer a smaller h. This phenomenon\nis consistent with the empirical result in [ 48] that large h\nis essential for capturing long distance dependencies with\nmulti-head self-attention.\n5 CONCLUSION AND FUTURE WORK\nDeep bidirectional self-attention architecture has achieved tremen-\ndous success in language understanding. In this paper, we introduce\na deep bidirectional sequential model called BERT4Rec for sequen-\ntial recommendation. For model training, we introduce the Cloze\ntask which predicts the masked items using both left and right\ncontext. Extensive experimental results on four real-world datasets\nshow that our model outperforms state-of-the-art baselines.\nSeveral directions remain to be explored. A valuable direction\nis to incorporate rich item features ( e.g., category and price for\nproducts, cast for movies) into BERT4Rec instead of just modeling\nitem ids. Another interesting direction for the future work would\nbe introducing user component into the model for explicit user\nmodeling when the users have multiple sessions.\nREFERENCES\n[1] Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization.\nCoRR abs/1607.06450 (2016).\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine\nTranslation by Jointly Learning to Align and Translate. In Proceedings of ICLR .\n[3] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and\nHongyuan Zha. 2018. Sequential Recommendation with User Memory Networks.\nIn Proceedings of WSDM . ACM, New York, NY, USA, 108–116.\n[4] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase\nRepresentations using RNN Encoder–Decoder for Statistical Machine Translation.\nIn Proceedings of EMNLP . Association for Computational Linguistics, 1724–1734.\n[5] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for\nYouTube Recommendations. In Proceedings of RecSys . ACM, New York, NY, USA,\n191–198.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nCoRR abs/1810.04805 (2018).\n[7] Tim Donkers, Benedikt Loepp, and Jürgen Ziegler. 2017. Sequential User-based\nRecurrent Neural Network Recommendations. In Proceedings of RecSys . ACM,\nNew York, NY, USA, 152–160.\n[8] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\nand Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (Dec. 2015), 19 pages.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nLearning for Image Recognition. In Proceedings of CVPR . 770–778.\n[10] Ruining He, Wang-Cheng Kang, and Julian McAuley. 2017. Translation-based\nRecommendation. In Proceedings of RecSys . ACM, New York, NY, USA, 161–169.\n[11] Ruining He and Julian McAuley. 2016. Fusing Similarity Models with Markov\nChains for Sparse Sequential Recommendation. InProceedings of ICDM . 191–200.\n[12] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In Proceedings of WWW . 173–182.\n[13] Dan Hendrycks and Kevin Gimpel. 2016. Bridging Nonlinearities and Stochastic\nRegularizers with Gaussian Error Linear Units. CoRR abs/1606.08415 (2016).\n[14] Balázs Hidasi and Alexandros Karatzoglou. 2018. Recurrent Neural Networks\nwith Top-k Gains for Session-based Recommendations. In Proceedings of CIKM .\nACM, New York, NY, USA, 843–852.\n[15] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2016. Session-based Recommendations with Recurrent Neural Networks. In\nProceedings of ICLR .\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in\na neural network. In Deep Learning and Representation Learning Workshop .\n[17] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.\nNeural Computation 9, 8 (Nov. 1997), 1735–1780.\n[18] Liang Hu, Longbing Cao, Shoujin Wang, Guandong Xu, Jian Cao, and Zhiping\nGu. 2017. Diversifying Personalized Recommendation with User-session Context.\nIn Proceedings of IJCAI . 1858–1864.\n[19] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y. Chang.\n2018. Improving Sequential Recommendation with Knowledge-Enhanced Mem-\nory Networks. In Proceedings of SIGIR . ACM, New York, NY, USA, 505–514.\n[20] Santosh Kabbur, Xia Ning, and George Karypis. 2013. FISM: Factored Item\nSimilarity Models for top-N Recommender Systems. InProceedings of KDD . ACM,\nNew York, NY, USA, 659–667.\n[21] Wang-Cheng Kang, Chen Fang, Zhaowen Wang, and Julian McAuley. 2017.\nVisually-Aware Fashion Recommendation and Design with Generative Image\nModels. In Proceedings of ICDM . IEEE Computer Society, 207–216.\n[22] Wang-Cheng Kang and Julian McAuley. [n. d.]. Self-Attentive Sequential Recom-\nmendation. In Proceedings of ICDM . 197–206.\n[23] Donghyun Kim, Chanyoung Park, Jinoh Oh, Sungyoung Lee, and Hwanjo Yu.\n2016. Convolutional Matrix Factorization for Document Context-Aware Recom-\nmendation. In Proceedings of RecSys . ACM, New York, NY, USA, 233–240.\n[24] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-\nmization. In Proceedings of ICLR .\n[25] Yehuda Koren. 2008. Factorization Meets the Neighborhood: A Multifaceted\nCollaborative Filtering Model. In Proceedings of KDD . ACM, 426–434.\n[26] Yehuda Koren and Robert Bell. 2011. Advances in Collaborative Filtering. In\nRecommender Systems Handbook . Springer US, Boston, MA, 145–186.\n[27] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Tech-\nniques for Recommender Systems. Computer 42, 8 (Aug. 2009), 30–37.\n[28] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.\nNeural Attentive Session-based Recommendation. In Proceedings of CIKM . ACM,\nNew York, NY, USA, 1419–1428.\n[29] Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu, and Tong Zhang. 2018. Multi-\nHead Attention with Disagreement Regularization. In Proceedings of tEMNLP .\n2897–2903.\n[30] Zhouhan Lin, Minwei Feng, Cícero Nogueira dos Santos, Mo Yu, Bing Xiang,\nBowen Zhou, and Yoshua Bengio. 2017. A Structured Self-attentive Sentence\nEmbedding. In Proceedings of ICLR .\n[31] Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon.Com Recommenda-\ntions: Item-to-Item Collaborative Filtering. IEEE Internet Computing 7, 1 (Jan.\n2003), 76–80.\n[32] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz\nKaiser, and Noam Shazeer. 2018. Generating Wikipedia by Summarizing Long\nSequences. In Proceedings of ICLR .\n[33] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018. STAMP: Short-\nTerm Attention/Memory Priority Model for Session-based Recommendation. In\nProceedings of KDD . ACM, New York, NY, USA, 1831–1839.\n[34] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel.\n2015. Image-Based Recommendations on Styles and Substitutes. In Proceedings\nof SIGIR . ACM, New York, NY, USA, 43–52.\n[35] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nEstimation of Word Representations in Vector Space.CoRR abs/1301.3781 (2013).\n[36] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\nDistributed Representations of Words and Phrases and Their Compositionality.\nIn Proceedings of NIPS . Curran Associates Inc., USA, 3111–3119.\n[37] Massimo Quadrana, Alexandros Karatzoglou, Balázs Hidasi, and Paolo Cremonesi.\n2017. Personalizing Session-based Recommendations with Hierarchical Recurrent\nNeural Networks. In Proceedings of RecSys . ACM, New York, NY, USA, 130–137.\n[38] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-\nproving language understanding by generative pre-training. In OpenAI Technical\nreport.\n[39] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. InProceedings\nof UAI. AUAI Press, Arlington, Virginia, United States, 452–461.\n[40] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factoriz-\ning Personalized Markov Chains for Next-basket Recommendation. InProceedings\nof WWW . ACM, New York, NY, USA, 811–820.\n[41] Ruslan Salakhutdinov and Andriy Mnih. 2007. Probabilistic Matrix Factorization.\nIn Proceedings of NIPS . Curran Associates Inc., USA, 1257–1264.\n[42] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted\nBoltzmann Machines for Collaborative Filtering. In Proceedings of ICML . 791–\n798.\n[43] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based\nCollaborative Filtering Recommendation Algorithms. In Proceedings of WWW .\nACM, New York, NY, USA, 285–295.\n[44] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.\nAutoRec: Autoencoders Meet Collaborative Filtering. In Proceedings of WWW .\nACM, New York, NY, USA, 111–112.\n[45] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based\nRecommender System. J. Mach. Learn. Res. 6 (Dec. 2005), 1265–1295.\n[46] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with\nRelative Position Representations. In Proceedings of NAACL . Association for\nComputational Linguistics, 464–468.\n[47] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting. J. Mach. Learn. Res. 15, 1 (Jan. 2014), 1929–1958.\n[48] Gongbo Tang, Mathias Müller, Annette Rios, and Rico Sennrich. 2018. Why Self-\nAttention? A Targeted Evaluation of Neural Machine Translation Architectures.\nIn Proceedings of EMNLP . 4263–4272.\n[49] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation\nvia Convolutional Sequence Embedding. In Proceedings of WSDM . 565–573.\n[50] Wilson L. Taylor. 1953. âĂĲCloze ProcedureâĂİ: A New Tool for Measuring\nReadability. Journalism Bulletin 30, 4 (1953), 415–433.\n[51] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep\ncontent-based music recommendation. In Proceedings of NIPS . 2643–2651.\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NIPS. Curran Associates, Inc., 5998–6008.\n[53] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative Deep Learning\nfor Recommender Systems. In Proceedings of KDD . ACM, New York, NY, USA,\n1235–1244.\n[54] Shoujin Wang, Liang Hu, Longbing Cao, Xiaoshui Huang, Defu Lian, and Wei\nLiu. 2018. Attention-Based Transactional Context Embedding for Next-Item\nRecommendation. , 2532–2539 pages.\n[55] Suhang Wang, Yilin Wang, Jiliang Tang, Kai Shu, Suhas Ranganath, and Huan Liu.\n2017. What Your Images Reveal: Exploiting Visual Contents for Point-of-Interest\nRecommendation. In Proceedings of WWW . 391–400.\n[56] Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J. Smola, and How Jing.\n2017. Recurrent Recommender Networks. In Proceedings of WSDM . ACM, New\nYork, NY, USA, 495–503.\n[57] Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. 2016. Collabora-\ntive Denoising Auto-Encoders for Top-N Recommender Systems. In Proceedings\nof WSDM . ACM, New York, NY, USA, 153–162.\n[58] Feng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A Dynamic\nRecurrent Model for Next Basket Recommendation. InProceedings of SIGIR . ACM,\nNew York, NY, USA, 729–732."
}