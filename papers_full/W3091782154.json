{
  "title": "Analyzing Individual Neurons in Pre-trained Language Models",
  "url": "https://openalex.org/W3091782154",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5032689067",
      "name": "Nadir Durrani",
      "affiliations": [
        "Qatar Airways (Qatar)"
      ]
    },
    {
      "id": "https://openalex.org/A5042954793",
      "name": "Hassan Sajjad",
      "affiliations": [
        "Qatar Airways (Qatar)"
      ]
    },
    {
      "id": "https://openalex.org/A5002245931",
      "name": "Fahim Dalvi",
      "affiliations": [
        "Qatar Airways (Qatar)"
      ]
    },
    {
      "id": "https://openalex.org/A5051184573",
      "name": "Yonatan Belinkov",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3004117589",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2594470997",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1528941926",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2292919134",
    "https://openalex.org/W2997244573",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W3089596076",
    "https://openalex.org/W2098921539",
    "https://openalex.org/W3104136798",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2946817437",
    "https://openalex.org/W3024936740",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2773621464",
    "https://openalex.org/W2145910665",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W3034487470",
    "https://openalex.org/W2962777840",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W2560864221",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2122825543",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2963503967",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4865–4880,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n4865\nAnalyzing Individual Neurons in Pre-trained Language Models\nNadir Durrani Hassan Sajjad Fahim Dalvi Yonatan Belinkov *\n{ndurrani,hsajjad,faimaduddin}@hbku.edu.qa\nQatar Computing Research Institute, HBKU Research Complex, Doha 5825, Qatar\n*MIT Computer Science and Artiﬁcial Intelligence Laboratory and Harvard\nJohn A. Paulson School of Engineering and Applied Sciences, Cambridge, MA, USA\nbelinkov@csail.mit.edu\nAbstract\nWhile a lot of analysis has been carried to\ndemonstrate linguistic knowledge captured by\nthe representations learned within deep NLP\nmodels, very little attention has been paid\ntowards individual neurons. We carry out\na neuron-level analysis using core linguistic\ntasks of predicting morphology, syntax and se-\nmantics, on pre-trained language models, with\nquestions like: i) do individual neurons in pre-\ntrained models capture linguistic information?\nii) which parts of the network learn more about\ncertain linguistic phenomena? iii) how dis-\ntributed or focused is the information? and iv)\nhow do various architectures differ in learning\nthese properties? We found small subsets of\nneurons to predict linguistic tasks, with lower\nlevel tasks (such as morphology) localized in\nfewer neurons, compared to higher level task\nof predicting syntax. Our study reveals inter-\nesting cross architectural comparisons. For ex-\nample, we found neurons in XLNet to be more\nlocalized and disjoint when predicting proper-\nties compared to BERT and others, where they\nare more distributed and coupled.\n1 Introduction\nTransformer-based neural language models have\nconstantly pushed the state-of-the-art in down-\nstream NLP tasks such as Question Answering ,\nTextual Entailment, etc. (Rajpurkar et al., 2016;\nWang et al., 2018). Central to this revolution is the\ncontextualized embedding, where each word is as-\nsigned a vector based on the entire input sequence,\nallowing it to capture not only a static semantic\nmeaning but also a contextualized meaning.\nPrevious work on analyzing neural networks\nshowed that while learning rich NLP tasks such as\nmachine translation and language modeling, these\ndeep models capture fundamental linguistic phe-\nnomena such as word morphology, syntax and vari-\nous other relevant properties of interest (Shi et al.,\n2016; Adi et al., 2016; Belinkov et al., 2017a,b;\nDalvi et al., 2017; Blevins et al., 2018).\nMore recently Liu et al. (2019) and Tenney et al.\n(2019) used probing classiﬁers to analyze pre-\ntrained neural language models on a variety of se-\nquence labeling tasks and demonstrated that contex-\ntualized representations encode useful, transferable\nfeatures of language. While most of the previous\nstudies emphasize and analyze representations as a\nwhole, very little work has been carried to analyze\nindividual neurons in deep NLP models.\nStudying individual neurons can facilitate under-\nstanding of the inner workings of neural networks\n(Karpathy et al., 2015; Dalvi et al., 2019; Suau\net al., 2020) and have other potential beneﬁts such\nas controlling bias and manipulating system’s be-\nhaviour (Bau et al., 2019), model distillation and\ncompression (Rethmeier et al., 2020), efﬁcient fea-\nture selection (Dalvi et al., 2020), and guiding ar-\nchitectural search.\nIn this work, we put the representations learned\nwithin pre-trained transformer models under the mi-\ncroscope and carry out a ﬁne-grained neuron level\nanalysis with respect to various linguistic proper-\nties. We target questions such as: i) do individual\nneurons in pretrained models capture linguistic in-\nformation? ii) which parts of the network learn\nmore about certain linguistic phenomena? iii) how\ndistributed or focused is the information? and iv)\nhow do various architectures differ in learning these\nproperties?\nA typical methodology in previous work on an-\nalyzing representations trains probing classiﬁers\nusing the representations learned within a neural\nmodel, to predict the understudied task. We also\nuse a probing classiﬁer approach to analyze indi-\nvidual neurons. Since neurons are multivariate in\nnature and work in groups, we additionally use\nelastic-net regularization that encourages individ-\nual and group of neurons to play a role in the train-\n4866\ning of the classiﬁer. Given a trained classiﬁer, we\nconsider the weights assigned to each neuron as a\nmeasure of their importance with respect to the un-\nderstudied linguistic task. We use probes with high\nselectivity (Hewitt and Liang, 2019) to ensure that\nour results reﬂect the property of representations\nand not the probe’s capacity to learn.\nWe choose 4 pre-trained models: ELMo (Pe-\nters et al., 2018a), its transformer variant T-ELMo\n(Peters et al., 2018b), BERT (Devlin et al., 2019)\nand XLNet (Yang et al., 2019) – covering a var-\nied set of modeling choices, including the building\nblocks (recurrent networks versus Transformers),\noptimization objective (auto-regressive versus non-\nautoregressive), and model depth and width. Our\ncross architectural analysis yields the following\ninsights:\n•Information across networks is distributed, but\nit is possible to extract a very small subset of\nneurons to predict a linguistic task with the\nsame accuracy as using the entire network.\n•Low level tasks such as predicting morphol-\nogy require fewer neurons compared to high\nlevel tasks such as predicting syntax.\n•Some phenomena (e.g. Verbs) are distributed\nacross many neurons while others (e.g. Inter-\njections) are localized in a fewer neurons.\n•Lower layers contain more word-level spe-\ncialized neurons, and higher layers contain\nneurons specialized in syntax-level informa-\ntion.\n•BERT is the most distributed model with re-\nspect to all properties while XLNet exhibits\nfocus with the most disjoint set of neurons\nand layers designated for different linguistic\nproperties.\n2 Methodology\nA common approach for probing neural network\ncomponents against linguistic properties is to train\na linear classiﬁer using the activations generated\nfrom the trained neural network as static features.\nThe underlying assumption is that if a simple linear\nmodel can predict a linguistic property, then the\nrepresentations implicitly encode this information.\nProbe: We go a level deeper and identify neu-\nrons within the learned representations to carry out\na more ﬁne-grained neuron1 level analysis. We use\na logistic regression classiﬁer with elastic-net regu-\nlarization (Zou and Hastie, 2005). The weights of\nthe trained classiﬁer serve as a proxy to select the\nmost relevant features2 within the learned represen-\ntations, to predict a linguistic property. Formally,\nconsider a pre-trained neural language model M\nwith L layers: {l1,l2,...,l L}. Given a dataset\nD = {w1,w2,...,w N}with a corresponding set of\nlinguistic annotations T = {tw1 ,tw2 ,...,t wN }, we\nmap each word wi in the data D to a sequence of\nlatent representations: D M↦− →z = {z1,..., zn}.\nThe representations can either be extracted from\nthe entire model or just from an individual layer.\nThe model is trained by minimizing the following\nloss function:\nL(θ) =−\n∑\ni\nlog Pθ(twi|wi) +λ1∥θ∥1 + λ2∥θ∥2\n2\nwhere Pθ(twi|wi) is the probability that word iis\nassigned property twi. The weights θ∈RD×T are\nlearned with gradient descent. Here D is the di-\nmensionality of the latent representations zi and T\nis the number of tags (properties) in the linguistic\ntag set, which the classiﬁer is predicting. The terms\nλ1∥θ∥1 and λ2∥θ∥2\n2 correspond to L1 and L2 regu-\nlarization. This combination, known as elastic-net,\nstrikes a balance between identifying very focused\nlocalized features (L1) versus distributed neurons\n(L2). We use a grid search algorithm described in\nSearch, to ﬁnd the most appropriate set of lambda\nvalues. But let us describe the neuron ranking algo-\nrithm ﬁrst.\nNeuron Ranking Algorithm: Once the classi-\nﬁer has been trained, our goal is to retrieve individ-\nual or a group of neurons (some subset of features\nof the latent representation) that are the most rele-\nvant for predicting a particular linguistic propertyT\nof interest. We use the neuron ranking algorithm as\ndescribed in Dalvi et al. (2019). Given the trained\nclassiﬁer θ∈RD×T, the algorithm extracts a rank-\ning of the D neurons in the model M. For each\nlabel3 tin task T, the weights are sorted by their\nabsolute values in descending order. To select N\nmost salient neurons w.r.t. the task T, an iterative\nprocess is carried. The algorithm starts with a small\n1In our terminology, a neuron is one dimension in a high-\ndimensional representation, even when the representation is\nthe output of a complex operation such as a transformer block.\n2We use features and neurons interchangeably in the paper.\n3We use label and sub-property interchangeably.\n4867\npercentage of the total weight mass and selects the\nmost salient neurons for each sub-property (e.g.\nNouns in POS tagging) until the set reaches the\nspeciﬁed size N.\nSearch: The search criteria is driven through ab-\nlation of weights in the trained classiﬁer. Once the\nclassiﬁer is trained, we select M4 top and bottom\nfeatures according to our ranked list (obtained us-\ning neuron ranking algorithm described above) and\nzero-out the remaining features. We then compute\nscore for each lambda set (λ1, λ2) as:\nS(λ1,λ2) =α(At −Ab) −β(Az −Al)\nwhere At is the accuracy of the classiﬁer retain-\ning top neurons and masking the rest, Ab is the\naccuracy retaining bottom neurons, Az is the ac-\ncuracy of the classiﬁer trained using all neurons\nbut without regularization, and Al is the accuracy\nwith the current lambda set. The ﬁrst term ensures\nthat we select a lambda set where accuracies of\ntop and bottom neurons are further apart and the\nsecond term ensures that we prefer weights that\nincur a minimal loss in classiﬁer accuracy due to\nregularization.5 We set αand β to be 0.5 in our\nexperiments. This formulation enables the search\nto be automated, compared to Dalvi et al. (2019)\nwhere the lambdas were selected manually, which\nwe found to be cumbersome and error-prone.\nMinimal Neuron Selection: Once we have ob-\ntained the best regularization lambdas, we follow a\n3-step process to extract minimal neurons for any\ndownstream task: i) train a classiﬁer to predict the\ntask using all the neurons (call it Oracle), ii) obtain\na neuron ranking based on the ranking algorithm\ndescribed above, iii) choose the top N neurons\nfrom the ranked list and retrain a classiﬁer using\nthese, iv) repeat step 3 by increasing the size of\nN,6 until the classiﬁer obtains an accuracy close\n(not less than a speciﬁed threshold δ) to the Oracle.\nControl Tasks: While there is a plethora of work\ndemonstrating that contextualized representations\nencode a continuous analogue of discrete linguis-\ntic information, a question has also been raised\nrecently if the representations actually encode lin-\nguistic structure or whether the probe memorizes\n4M is set to 20% of the network in our experiments\n5For some lambdas, for example with high value of L1,\nthe classiﬁer prefers sparsity, i.e. selects fewer very focused\nneurons but performs very badly on the task.\n6We increment by adding 1% neuron at every step.\nthe understudied task. We use Selectivity as a crite-\nrion to put a “linguistic task’s accuracy in context\nwith the probe’s capacity to memorize from word\ntypes” (Hewitt and Liang, 2019). It is deﬁned as\nthe difference between linguistic task accuracy and\ncontrol task accuracy. An effective probe is rec-\nommended to achieve high linguistic task accuracy\nand low control task accuracy. The control tasks\nfor our probing classiﬁers are deﬁned by mapping\neach word type xi to a randomly sampled behavior\nC(xi), from a set of numbers {1 ...T }where T\nis the size of tag set to be predicted in the linguis-\ntic task. The sampling is done using the empiri-\ncal token distribution of the linguistic task, so the\nmarginal probability of each label is similar. We\ncompute Selectivity by training classiﬁers using all\nand the selected neurons.\n3 Experimental Setup\nPre-trained Neural Language Models: We\npresent results with 4 pre-trained models: ELMo\n(Peters et al., 2018a), and 3 transformer architec-\ntures: Transformer-ELMo (Peters et al., 2018b),\nBERT (Devlin et al., 2019) and XLNet (Yang et al.,\n2019). The ELMo model is trained using a bidirec-\ntional recurrent neural network (RNN) with 3 lay-\ners each of size 1024 dimensions. Its transformer\nequivalent (T-ELMo) is trained with 7 layers but\nwith the same hidden layer size. The BERT model\nis trained as an auto-encoder with a dual objec-\ntive function of predicting masked words and next\nsentence in auto-encoding fashion. We use base\nversion (13 layers and 768 dimensions). Lastly\nwe included XLNet-base which is trained with the\nsame parameter settings (number and size of hid-\nden layers) as BERT, but with a permutation based\nauto-regressive objective function.\nLanguage Tasks: We evaluated our method on\n4 linguistic tasks: POS-tagging using the Penn\nTreeBank (Marcus et al., 1993), syntax tagging\n(CCG supertagging)7 using CCGBank (Hocken-\nmaier, 2006), syntactic chunking using CoNLL\n2000 shared task dataset (Tjong Kim Sang and\nBuchholz, 2000), and semantic tagging using the\nParallel Meaning Bank data (Abzianidze et al.,\n2017). We used standard splits for training, de-\n7CCG captures global syntactic information locally at the\nword level by assigning a label to each word annotating its\nsyntactic role in the sentence. The annotations can be thought\nof as a function that takes and return syntactic categories (like\nan NP: Noun phase).\n4868\nvelopment and test data (See Appendix A.1)\nClassiﬁer Settings: We used linear probing clas-\nsiﬁer with elastic-net regularization, using a cat-\negorical cross-entropy loss, optimized by Adam\n(Kingma and Ba, 2014). Training is run with shuf-\nﬂed mini-batches of size 512 and stopped after 10\nepochs. The regularization weights are trained us-\ning grid-search algorithm. 8 For sub-word based\nmodels, we use the last activation value to be the\nrepresentative of the word as prescribed for the em-\nbeddings extracted from Neural MT models (Dur-\nrani et al., 2019) and pre-trained Language Models\n(Liu et al., 2019). Linear classiﬁers are a popular\nchoice in analyzing deep NLP models due to their\nbetter interpretability (Qian et al., 2016; Belinkov\net al., 2020). Hewitt and Liang (2019) have also\nshown linear probes to have higher Selectivity, a\nproperty deemed desirable for more interpretable\nprobes. Linear probes are particularly important\nfor our method as we use the learned weights as a\nproxy to measure the importance of each neuron.\n4 Evaluation\n4.1 Ablation Study\nFirst we evaluate our rankings as obtained by the\nneuron selection algorithm presented in Section 2.\nWe extract a ranked list of neurons with respect\nto each property set (linguistic task T) and ablate\nneurons in the classiﬁer to verify the rankings. This\nis done by zeroing-out all the activations in the test,\nexcept for the selectedM% neurons. We select top,\nrandom and bottom 20%9 neurons to evaluate our\nrankings. Table 1 shows the efﬁcacy of our rank-\nings, with low performance (prediction accuracy)\nusing only the bottom or random neurons versus us-\ning only the top neurons. The accuracy of random\nneurons is high in some cases (for example CCG, a\ntask related to predicting syntax) showing when the\nunderlying task is complex, the information related\nto it is more distributed across the network causing\nredundancy.\n4.2 Minimal Neuron Set\nNow that we have established correctness of the\nrankings, we apply the algorithm incrementally\nto select minimal neurons for each linguistic task\n8See Appendix A.2 for hyperparameters selected for each\ntask.\n9The choice of 20% is arbitrary. We did not experiment\nmuch with it as this was merely to select best lambdas and to\ndemonstrate the efﬁcacy of rankings.\nBERT XLNet T-ELMo ELMo\nPOS\nAll 96.04 96.13 96.39 96.48\nTop 90.16 92.28 91.96 83.01\nRandom 28.45 58.17 48.40 30.80\nBottom 16.86 44.64 21.11 15.56\nSEM\nAll 92.09 92.64 91.94 93.29\nTop 84.32 90.70 84.16 81.23\nRandom 64.28 72.14 66.15 75. 82\nBottom 59.02 25.37 36.14 58.32\nChunking\nAll 95.01 94.15 93.43 93.14\nTop 89.01 89.16 87.63 82.51\nRandom 75.83 75.26 79.40 70.23\nBottom 66.82 46.66 48.11 64.39\nCCG\nAll 92.16 92.55 91.70 91.19\nTop 75.13 76.48 71.31 68.19\nRandom 71.11 63.71 68.23 41.17\nBottom 59.13 62.42 67.11 30.32\nTable 1: Ablation Study: Selecting all, top, random\nand bottom 20% neurons and zeroing-out remaining to\nevaluate classiﬁer accuracy on blind test (averaged over\n3 runs). See Appendix A.4 for dev results.\nthat obtain a similar accuracy (we use a threshold\nδ = 0.5) as using the entire network (all the fea-\ntures). Identifying a minimal set of top neurons en-\nables us to highlight: i) parts of the learned network\nwhere different linguistic phenomena are predom-\ninantly captured, ii) how localized or distributed\ninformation is with respect to different properties.\nTable 2 summarizes the results. Firstly we show\nthat in all the tasks, selecting a subset of top N%\nneurons and retraining the classiﬁer can obtain a\nsimilar (sometimes even better) accuracy as using\nall the neurons (Acc a) for classiﬁcation as static\nfeatures. For lexical tasks such as POS or SEM\ntagging, a very small number of neurons (roughly\n400 i.e 4% of features in BERT and XLNet) was\nfound to be sufﬁcient for achieving an accuracy\n(Acct) similar to oracle (Acc a). More complex\nsyntactic tasks such as Chunking and CCG tag-\nging required larger sets of neurons (up to 2365\n– one third of the network in T-ELMo) to accom-\nplish the same. It is interesting to see that all the\nmodels, irrespective of their size, required a com-\nparable number of selected neurons, in most of\nthe cases. On the POS and SEM tagging tasks,\nbesides T-ELMo all other models use roughly the\nsame number of neurons. T-ELMo required more\nneurons in SEM tagging to achieve the task. This\n4869\nBERT XLNet T-ELMo ELMo\nNeua 9984 9984 7168 3072\nPOS\nNeut 400/4% 400/4% 430/6% 368/12%\nAcca 96.04 96.13 96.39 96.48\nAcct 95.86 96.49 96.07 96.22\nSela 14.45 23.49 22.65 19.82\nSelt 31.68 31.82 37.31 38.51\nSEM\nNeut 400/4% 400/4% 716/10% 307/10%\nAcca 92.09 92.64 91.94 93.29\nAcct 92.12 92.62 91.97 93.17\nSela 5.77 14.03 12.78 11.18\nSelt 27.17 26.55 23.87 32.28\nChunking\nNeut 1000/10% 1000/10% 860/12% 983/32%\nAcca 95.01 94.62 93.43 93.14\nAcct 94.99 94.17 93.37 93.08\nSela 16.30 22.77 24.42 18.13\nSelt 29.19 28.42 30.95 26.21\nCCG\nNeut 1500/15% 1500/15% 2365/33% 1014/33%\nAcca 92.16 92.55 91.7 91.19\nAcct 92.36 92.39 91.39 90.95\nSela 7.33 14.02 11.99 11.48\nSelt 15.06 24.15 18.32 17.88\nTable 2: Selecting minimal number of neurons for each\ndownstream NLP task. Accuracy numbers reported on\nblind test-set (averaged over three runs) – Neua = Total\nnumber of neurons, Neut = Top selected neurons, Acca\n= Accuracy using all neurons, Acc t = Accuracy using\nselected neurons after retraining the classiﬁer using se-\nlected neurons, Sel = Difference between linguistic task\nand control task accuracy when classiﬁer is trained on\nall neurons (Sela) and top neurons (Selt).\ncould imply that knowledge of lexical semantics in\nT-ELMo is distributed in more neurons. In an over-\nall trend, ELMo generally needed fewer neurons\nwhile T-ELMo required more neurons compared\nto the other models to achieve oracle performance.\nBoth these models are much smaller than BERT\nand XLNet. We did not observe any correlation,\ncomparing results with the size of the models.\nControl Tasks: We use Selectivity to further\ndemonstrate that our probes (trained using the en-\ntire representation and selected neurons) do not\nmemorize from word types but learned the under-\nlying linguistic task. Recall that an effective probe\nis recommended to achieve high linguistic task ac-\ncuracy and low control task accuracy. The results\nBERT XLNet T-ELMo ELMo\nNeua 9984 9984 7168 3072\nPOS\nNeut 250/2.5% 250/2.5% 215/3% 153/5%\nAcca 96.04 96.13 96.39 96.48\nAcct 93.70 95.72 94.92 94.45\nSEM\nNeut 250/2.5% 400/4% 286/4% 307/5%\nAcca 92.09 92.64 91.94 93.29\nAcct 91.44 90.92 90.17 93.17\nChunking\nNeut 600/6% 600/6% 430/6% 614/20%\nAcca 95.01 94.62 93.43 93.14\nAcct 93.53 92.83 92.28 91.79\nCCG\nNeut 698/7% 734/8% 716/10% 675/22%\nAcca 92.16 92.55 91.70 91.19\nAcct 91.73 91.11 89.79 89.08\nTable 3: Selecting minimal number of neurons for each\ndownstream NLP task with a looser threshold δ = 2.\nAccuracy numbers reported on blind test-set (averaged\nover three runs) – Neu a = Total number of neurons,\nNeut = Top selected neurons, Acc a = Accuracy using\nall neurons, Acc t = Accuracy using selected neurons\nafter retraining the classiﬁer using selected neurons.\n(see Table 2) show that selectivity with top neu-\nrons (Selt) is much higher than selectivity with\nall neurons Sela. It is evident that using all the\nneurons may contribute to memorization whereas\nhigher selectivity with selected neurons indicates\nless memorization and efﬁcacy of our neuron se-\nlection. We achieve high selectivity when selecting\n400 neurons as in the case of POS and SEM. The\nchunking and CCG tasks require a lot more neu-\nrons with CCG requiring up to 33% of the network.\nHere, the low selectivity indicates that while the\ninformation about CCG is distributed into several\nneurons, a set of random neurons may also be able\nto achieve a decent performance.\nDiscussion: Identifying neurons that are salient\nto a task has various potential applications such\nas task-speciﬁc model compression, by removing\nthe irrelevant neurons with respect to the task or\ntask-speciﬁc ﬁne-tuning based on selected neurons.\nIt is however tricky how to model this, for example\none complexity is that zeroing out non-salient neu-\nrons in the lower layers directly affects any salient\nneurons in the subsequent layers. A rather direct\n4870\napplication to our work is efﬁcient feature-based\ntransfer learning, which has shown to be a viable\nalternative to the ﬁne-tuning approach (Peters et al.,\n2019). Feature-based approach uses contextualized\nembeddings learned from pre-trained models as\nstatic feature vectors in the down-stream classiﬁ-\ncation task. Classiﬁers with large contextualized\nvectors are not only cumbersome to train, but also\ninefﬁcient during inference. They have also been\nshown to be sub-optimal when supervised data is\ninsufﬁcient (Hameed, 2018). BERT-large, for ex-\nample, is trained with 19,200 (25 layers ×768\ndimensions) features. Reducing the feature set to\na smaller number can lead to faster training of the\nclassiﬁer and efﬁcient inference. Earlier (in Table\n2) we obtained minimal set of neurons with a very\ntight threshold of δ = 0.5. By allowing a loser\nthreshold, say δ = 2, we can reduce the set of\nminimal neurons to improve the efﬁciency even\nmore. See Table 3 for results. For more on this,\nwe refer interested readers to look at Dalvi et al.\n(2020), where we explored this more formally, ex-\npanding our study to the sentence-labeling GLUE\ntasks (Wang et al., 2018).\n5 Analysis\n5.1 Layer-wise Distribution\nPrevious work on analyzing deep neural networks\nanalyzed how individual layers contribute towards\na downstream task (Liu et al., 2019; Kim et al.,\n2020; Belinkov et al., 2020). Here we observe\nhow the neurons, selected from the entire network,\nspread across different layers of the model. Such an\nanalysis gives an alternative view of which layers\ncontribute predominantly towards different tasks.\nFigure 1 presents the results. In most cases, lexi-\ncal tasks such as learning morphology (POS tag-\nging) and word semantics (SEM tagging) are dom-\ninantly captured by the neurons at lower layers,\nwhereas the more complicated task of modeling\nsyntax (CCG supertagging) is taken care of at the\nﬁnal layer. An exception to this overall pattern is\nthe BERT model. Top neurons in BERT spread\nacross all the layers, unlike other models where\ntop neurons (for a particular task) are contributed\nby fewer layers. This reﬂects that every layer in\nBERT possesses neurons that specialize in learning\nparticular language properties, while other models\nhave designated layers that specialize in learning\nthose language properties. Different from other\nmodels, neurons in the embedding layer show min-\nimum contribution in XLNet consistently across\nthe tasks. Let us analyze the results with respect to\neach linguistic task.\nPOS Tagging: Every layer in BERT and ELMo\ncontributed towards the top neurons, while the dis-\ntribution is dominated by lower layers in XLNet\nand T-ELMo, with an exception of XLNet not\nchoosing any neurons from the embedding layer.\nSEM Tagging: Similar to POS, all layers of\nBERT contributed to the list of top neurons. How-\never, the middle layers showed the most contribu-\ntion (see layer numbers 4–7 in Figure 1e). This is\nin line with Liu et al. (2019) who found middle and\nhigher middle layers to give optimal results for the\nsemantic tagging task. On XLNet, T-ELMo and\nELMo, the ﬁrst layer after the embedding layer got\nthe largest share of the top neurons of SEM. This\ntrend is consistent across other tasks, i.e., the core\nlinguistic information is learned earlier in the net-\nwork with an exception of BERT, which distributes\ninformation across the network.\nChunking Tagging: The overall pattern re-\nmained similar in the task of chunking. Notice how-\never, a shift in pattern – the contribution from lower\nlayers decreased compared to previous tasks, in the\ncase of BERT. For example, in the SEM task, top\nneurons were dominantly contributed from lower\nand middle layers, in chunking middle and higher\nlayers contributed most. This could be attributed to\nthe fact that chunking is a more complex syntactic\ntask and is learned at relatively higher layers.\nCCG Supertagging: Compared to chunking,\nCCG supertagging is a richer syntactic tagging task,\nalmost equivalent to parsing (Bangalore and Joshi,\n1999). The complexity of the task is evident in our\nresults as there is a clear shift in the distribution of\ntop neurons moving from middle to higher layers.\nThe only exception again is the BERT model where\nthis information is well spread across the network,\nbut still dominantly preserved in the ﬁnal layers.\nDiscussion: Our results are in line with and rein-\nforce the layer-wise analysis presented in Liu et al.\n(2019). However, unlike their work and all other\nwork on layer-wise probing analysis, which trains a\nclassiﬁer on each layer individually to compare the\nresults, our method trains a single classiﬁer on all\nlayers concatenated to analyze which layers con-\ntribute most to the task based on the most relevant\nselected features. This makes the playing ﬁeld even\n4871\n(a) POS – BERT\n (b) POS – XLNet\n (c) POS – T-ELMo\n (d) POS – ELMo\n(e) SEM – BERT\n (f) SEM – XLNet\n (g) SEM – T-ELMo\n (h) SEM – ELMo\n(i) Chunking – BERT\n (j) Chunking – XLNet\n (k) Chunking – T-ELMo\n (l) Chunking – ELMo\n(m) CCG – BERT\n (n) CCG – XLNet\n (o) CCG – T-ELMo\n (p) CCG – ELMo\nFigure 1: How top neurons spread across different layers for each task? X-axis = Layer number, Y-axis = Number\nof neurons selected from that layer\nand results in a sharper analysis. For example, Liu\net al. (2019) showed layer 1 in Transformer-ELMo\nto give the best result on the task of predicting POS\ntags; however, layers 2 and 3 almost give similar\naccuracy (see Appendix D1 in their paper). Based\non these results, one cannot conﬁdently claim that\nthe task of POS is predominantly captured at layer\n1. However, our method clearly shows this result\n(see Figure 1c).\n5.2 Localization versus Distributedness\nNext we study how localized or distributed dif-\nferent properties are within a linguistic task (for\nexample nouns or verbs in POS tagging, location\nin semantic tagging), and across different architec-\ntures. Remember that the ranking algorithm ex-\ntracts neurons for each label t(e.g. LOC:location\nor EVE:event categories in semantic tagging) in\ntask T, sorted based on absolute weights. The ﬁnal\nrankings are obtained by selecting from each label\nusing the neuron ranking algorithm as described in\nSection 2. This allows us to analyze how localized\nor distributed a property is, based on the number of\nneurons that are selected for each label in the task.\nFigure 2: Number of neurons per label: Some proper-\nties (e.g., interjections) are localized in fewer neurons,\nwhile others (e.g., nouns) are more distributed. Y-axis\n= number of neurons per label\nProperty-wise: We found that while many prop-\nerties are distributed, i.e., a large group of neurons\nis used to predict a label, some properties such\nas functional or unambiguous words that do not\nrequire contextual information are learned using\nfewer neurons. For example, UH (interjections)\nor the TO particle required fewer neurons across\narchitectures compared toNNPS (proper noun; plu-\nral) in the task of POS tagging (Figure 2). Similarly\n4872\n(a) POS Tagging\n (b) Chunking Tagging\nFigure 3: Top neurons in XLNet are more localized towards individual properties compared to other architectures\nEQA (equating property, e.g.,as tall as you) is han-\ndled with fewer neurons compared to ORG (orga-\nnization property). We observed a similar behavior\nin the task of chunking, with I-PRT (particles in-\nside of a chunk) requiring fewer neurons across\ndifferent architectures. On the contrary, B-VP (be-\nginning of verb phrase) required plenty many.\nLayer-wise: Previously we analyzed each lin-\nguistic task in totality. We now study whether indi-\nvidual properties (e.g., adjectives) are localized or\nwell distributed across layers in different architec-\ntures. We observed interesting cross architectural\nsimilarities, for example the neurons that predict\nthe foreign words ( FW) property were predomi-\nnantly localized in ﬁnal layers (BERT: 13, XLNET:\n11, T-ELMo: 7, ELMo:3) of the network in all\nthe understudied architectures. In comparison, the\nneurons that capture common class words such as\nadjectives (JJ) and locations (LOC) are localized\nin lower layers (BERT: 0, XLNET: 1, T-ELMo:\n0, ELMo:1). In some cases, we did ﬁnd variance,\nfor example personal pronouns (PRP) in POS tag-\nging and event class ( EXC) in semantic tagging\nwere handled at different layers across different\narchitectures. See Appendix A.7 for all labels.\nArchitecture-wise: We found that top neurons\nin XLNet are more localized towards individual\nproperties compared to other architectures where\ntop neurons are shared across multiple properties.\nWe demonstrate this in Figure 3. Notice how the\nnumber of neurons for different labels 10 is much\nsmaller in the case of XLNet, although roughly\nthe same number of total neurons (400 for POS\ntagging and 960 for chunking on average; see Table\n10Figure 3 only displays selected properties, but the pattern\nholds across all properties. See Appendix A.7.\n2) were required by all pre-trained models to carry\nout a task. This means that in XLNet neurons\nare exclusive towards speciﬁc properties compared\nto other architectures where neurons are shared\nbetween multiple properties. Such a trait in XLNet\ncan be potentially helpful in predicting the behavior\nof the system as it is easier to isolate neurons that\nare designated toward speciﬁc phenomena.\n6 Related Work\nRise of neural network has seen a subsequent rise of\ninterpretability of these models. Researchers have\nexplored visualization methods to analyze learned\nrepresentations (Karpathy et al., 2015; K´ad´ar et al.,\n2017), attention heads (Clark et al., 2019; Vig,\n2019) of language compositionality (Li et al., 2016)\netc. While such visualizations illuminate the inner\nworkings of the network, they are often qualitative\nin nature and somewhat anecdotal.\nA more commonly used approach tries to pro-\nvide a quantitative analysis by correlating parts of\nthe neural network with linguistic properties, for\nexample by training a classiﬁer to predict a fea-\nture of interest (Adi et al., 2016; Conneau et al.,\n2018). Please refer to Belinkov and Glass (2019)\nfor a comprehensive survey of work done in this\ndirection. Liu et al. (2019) used probing classiﬁers\nfor investigating the contextualized representations\nlearned from a variety of neural language models\non numerous word level linguistic tasks. A similar\nanalysis was carried by Tenney et al. (2019) on a\nvariety of sub-sentence linguistic tasks. We extend\nthis line of work to carry out a more ﬁne-grained\nneuron level analysis of neural language models.\nOur work is most similar to Dalvi et al. (2019)\nwho conducted neuron analysis of representations\nlearned from sequence-to-sequence machine trans-\n4873\nlation models. Our work is different from them in\nthat i) we carry out analysis on a wide range of ar-\nchitectures which are deeper and more complicated\nthan RNN-based models and illuminate interesting\ninsights, ii) we automated the grid-search criteria\nto select the regularization parameters, compared\nto manual selection of lambdas, which is cumber-\nsome and error-prone. In contemporaneous work,\nSuau et al. (2020) used max-pooling to identify\nrelevant neurons (aka Expert units) in pre-trained\nmodels, with respect to a speciﬁc concept (for ex-\nample word-sense).\nA pitfall to the approach of probing classiﬁers is\nwhether the probe is faithfully reﬂecting the prop-\nerty of the representation or just learned the task?\nHewitt and Liang (2019) deﬁned control tasks to\nanalyze the role of training data and lexical mem-\norization in probing experiments. V oita and Titov\n(2020) proposed an alternative that measures Mini-\nmal Description Length of labels given representa-\ntions. It would be interesting to see how a probe’s\ncomplexity in their work (code length) compares\nwith the number of selected neurons according to\nour method. The results are consistent at least in\nthe ELMo POS example, where layer 1 was shown\nto have the shortest code length in their work. In\nour case, most top neurons are selected from layer 1\n(see Figure 1d for example). Pimentel et al. (2020)\ndiscussed the complexity of the probes and argued\nfor using highest performing probes for tighter es-\ntimates. However, complex probes are difﬁcult to\nanalyze. Linear models are preferable due to their\nexplainability; especially in our work, as we use\nthe learned weights as a proxy to get a measure of\nthe importance of each neuron. We used linear clas-\nsiﬁers with control tasks as described in Hewitt and\nLiang (2019). Although we mainly used probing\naccuracy to drive the neuron selection in this work,\nand Selectivity only to demonstrate that our results\nreﬂect the property learned by representations and\nnot probe’s capacity to learn – an interesting idea\nwould be to use selectivity itself to drive the inves-\ntigation. However, it is not trivial how to optimize\nfor selectivity as it cannot be controlled/tuned di-\nrectly – for example, removing some neurons may\ndecrease accuracy but may not change selectivity.\nWe leave this exploration for future work.\nProbing classiﬁers require supervision for the lin-\nguistic tasks of interest with annotations, limiting\ntheir applicability. Bau et al. (2019) used unsuper-\nvised approach to identify salient neurons in neural\nmachine translation and manipulated translation\noutput by controlling these neurons. Recently, Wu\net al. (2020) measured similarity of internal repre-\nsentations and attention across prominent contex-\ntualized representations (from BERT, ELMo, etc.).\nThey found that different architectures have similar\nrepresentations, but different individual neurons.\n7 Conclusion\nWe analyzed individual neurons across a variety\nof neural language models using linguistic correla-\ntion analysis on the task of predicting core linguis-\ntic properties (morphology, syntax and semantics).\nOur results reinforce previous ﬁndings and also\nilluminate further insights: i) while the informa-\ntion in neural language models is massively dis-\ntributed, it is possible to extract a small number\nof features to carry out a downstream NLP task,\nii) the number of extracted features varies based\non the complexity of the task, iii) the neurons that\nlearn word morphology and lexical semantics are\npredominantly found in the lower layers of the net-\nwork, whereas the ones that learn syntax are at the\nhigher layers, with the exception of BERT, where\nneurons were spread across the entire network, iv)\nclosed-class words (for example interjections) are\nhandled using fewer neurons compared to poly-\nsemous words (such as nouns and adjectives), v)\nfeatures in XLNet are more localized towards in-\ndividual properties as opposed to other architec-\ntures where neurons are distributed across many\nproperties. A direct application of our analysis is\nefﬁcient feature-based transfer learning from large-\nscale neural language models: i) identifying that\nmost relevant features for a task are contained in\nlayer xreduces the forward-pass to that layer, ii)\nreducing the feature set decreases the time to train a\nclassiﬁer and also its inference. We refer interested\nreaders to see our work presented in Dalvi et al.\n(2020) for more details.\nAcknowledgements\nWe thank the anonymous reviewers for their feed-\nback on the earlier draft of this paper. This re-\nsearch was carried out in collaboration between\nthe Qatar Computing Research Institute (QCRI)\nand the MIT Computer Science and Artiﬁcial In-\ntelligence Laboratory (CSAIL). Y .B. was also sup-\nported by the Harvard Mind, Brain, and Behavior\nInitiative (MBB).\n4874\nReferences\nLasha Abzianidze, Johannes Bjerva, Kilian Evang,\nHessel Haagsma, Rik van Noord, Pierre Ludmann,\nDuc-Duy Nguyen, and Johan Bos. 2017. The paral-\nlel meaning bank: Towards a multilingual corpus of\ntranslations annotated with compositional meaning\nrepresentations. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics, EACL ’17, pages 242–\n247, Valencia, Spain.\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained Anal-\nysis of Sentence Embeddings Using Auxiliary Pre-\ndiction Tasks. arXiv preprint arXiv:1608.04207.\nSrinivas Bangalore and Aravind K. Joshi. 1999. Su-\npertagging: An approach to almost parsing. Compu-\ntational Linguistics, 25(2).\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2019. Iden-\ntifying and controlling important neurons in neural\nmachine translation. In International Conference on\nLearning Representations.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017a. What do Neu-\nral Machine Translation Models Learn about Mor-\nphology? In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), Vancouver. Association for Computational\nLinguistics.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2020. On the linguistic\nrepresentational power of neural machine translation\nmodels. Computational Linguistics, 45(1):1–57.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nYonatan Belinkov, Llu ´ıs M `arquez, Hassan Sajjad,\nNadir Durrani, Fahim Dalvi, and James Glass.\n2017b. Evaluating Layers of Representation in Neu-\nral Machine Translation on Part-of-Speech and Se-\nmantic Tagging Tasks. In Proceedings of the 8th In-\nternational Joint Conference on Natural Language\nProcessing (IJCNLP).\nTerra Blevins, Omer Levy, and Luke Zettlemoyer. 2018.\nDeep RNNs encode soft hierarchical syntax. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 14–19, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single vector: Probing sentence\nembeddings for linguistic properties. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, D. Anthony Bau, and James Glass. 2019.\nWhat is one grain of sand in the desert? analyzing\nindividual neurons in deep nlp models. In Proceed-\nings of the Thirty-Third AAAI Conference on Artiﬁ-\ncial Intelligence (AAAI, Oral presentation).\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, and Stephan V ogel. 2017. Understanding\nand Improving Morphological Learning in the Neu-\nral Machine Translation Decoder. In Proceedings\nof the 8th International Joint Conference on Natural\nLanguage Processing (IJCNLP).\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and\nYonatan Belinkov. 2020. Analyzing redundancy in\npretrained transformer models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP-2020) , Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nNadir Durrani, Fahim Dalvi, Hassan Sajjad, Yonatan\nBelinkov, and Preslav Nakov. 2019. One size does\nnot ﬁt all: Comparing NMT representations of dif-\nferent granularities. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1504–1516, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nShilan Hameed. 2018. Filter-wrapper combination and\nembedded feature selection for gene expression data.\nInternational Journal of Advances in Soft Comput-\ning and its Applications, 10:90–105.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJulia Hockenmaier. 2006. Creating a CCGbank and a\nwide-coverage CCG lexicon for German. In Pro-\nceedings of the 21st International Conference on\n4875\nComputational Linguistics and 44th Annual Meet-\ning of the Association for Computational Linguistics,\nACL ’06, pages 505–512, Sydney, Australia.\nAkos K ´ad´ar, Grzegorz Chrupała, and Afra Alishahi.\n2017. Representation of linguistic form and func-\ntion in recurrent neural networks. Computational\nLinguistics, 43(4):761–780.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078.\nTaeuk Kim, Jihun Choi, Daniel Edmiston, and Sang\ngoo Lee. 2020. Are pre-trained language models\naware of phrases? simple but strong baselines for\ngrammar induction.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nMethod for Stochastic Optimization. arXiv preprint\narXiv:1412.6980.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 681–691, San Diego, California. As-\nsociation for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313–330.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 1499–1509, Brussels, Belgium. Association\nfor Computational Linguistics.\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To tune or not to tune? adapting pre-\ntrained representations to diverse tasks. In Proceed-\nings of the 4th Workshop on Representation Learn-\ning for NLP (RepL4NLP-2019) , Florence, Italy. As-\nsociation for Computational Linguistics.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4609–4622, Online. Association for Computa-\ntional Linguistics.\nPeng Qian, Xipeng Qiu, and Xuanjing Huang. 2016.\nAnalyzing Linguistic Knowledge in Sequential\nModel of Sentence. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 826–835, Austin, Texas. Associa-\ntion for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nNils Rethmeier, Vageesh Kumar Saxena, and Isabelle\nAugenstein. 2020. Tx-ray: Quantifying and explain-\ning model-knowledge transfer in (un-)supervised\nNLP. In Proceedings of the Thirty-Sixth Conference\non Uncertainty in Artiﬁcial Intelligence, UAI 2020,\nvirtual online, August 3-6, 2020 , page 197. AUAI\nPress.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , EMNLP ’16,\npages 1526–1534, Austin, TX, USA.\nXavier Suau, Luca Zappella, and Nicholas Apos-\ntoloff. 2020. Finding experts in transformer models.\nCoRR, abs/2005.07647.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nErik F. Tjong Kim Sang and Sabine Buchholz. 2000.\nIntroduction to the CoNLL-2000 shared task chunk-\ning. In Fourth Conference on Computational Nat-\nural Language Learning and the Second Learning\nLanguage in Logic Workshop.\nJesse Vig. 2019. A multiscale visualization of atten-\ntion in the transformer model. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations , pages\n4876\n37–42, Florence, Italy. Association for Computa-\ntional Linguistics.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing . Associa-\ntion for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nJohn Wu, Hassan Belinkov, Yonatan Sajjad, Nadir Dur-\nrani, Fahim Dalvi, and James Glass. 2020. Simi-\nlarity Analysis of Contextual Word Representation\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), Seattle. Association for Computational Lin-\nguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nHui Zou and Trevor Hastie. 2005. Regularization and\nvariable selection via the elastic net. Journal of the\nRoyal Statistical Society, Series B, 67:301–320.\n4877\nA Appendices\nA.1 Data and Representations\nWe used standard splits for training, development\nand test data for the 4 linguistic tasks (POS, SEM,\nChunking and CCG super tagging) that we used\nto carry out our analysis on. The splits to prepro-\ncess the data are available through git repository11\nreleased with Liu et al. (2019). See Table 4 for\nstatistics. We obtained the understudied pre-trained\nmodels from the authors of the paper, through per-\nsonal communication.\nTask Train Dev Test Tags\nPOS 36557 1802 1963 44\nSEM 36928 5301 10600 73\nChunking 8881 1843 2011 22\nCCG 39101 1908 2404 1272\nTable 4: Data statistics (number of sentences) on train-\ning, development and test sets using in the experiments\nand the number of tags to be predicted\nA.2 Hyperparameters\nWe use elastic-net based regularization to control\nthe trade-off between selecting focused individual\nneurons versus group of neurons while maintaining\nthe original accuracy of the classiﬁer without any\nregularization. We do a grid search on L1 and L2\nranging from values 0 ... 1e−7. See Table 5 for\nthe optimal values for each task across different\narchitectures.\nBERT XLNet T-ELMo ELMo\nL1 , L2 =λ1, λ2\nPOS .001, .01 .001, .01 .001, .001 .001, .0001\nSEM .001, .01 .001, .01 .001, .001 .001, .0001\nChunk 1e−4,1e−5 1e−4,1e−4 .001, .001 .001, .01\nCCG 1e−5,1e−6 1e−5,1e−6 1e−4,1e−6 1e−5,1e−6\nTable 5: Best elastic-net lambdas parameters for each\ntask\nA.3 Infrastructure and Run Time\nOur experiments were run on NVidia GeForce GTX\nTITAN X GPU card. Grid search for ﬁnding op-\ntimal lambdas is expensive when optimal number\nof neurons for the task are unknown. Running\ngrid search would take O(MN2) where M = 100\n11https://github.com/nelson-liu/\ncontextual-repr-analysis\nBERT XLNet T-ELMo ELMo\nPOS\nAll 96.10 96.38 96.61 96.45\nTop 90.32 93.07 92.13 85.03\nRand 29.43 57.32 49.14 32.18\nBot 17.99 45.61 23.01 17.36\nSEM\nAll 92.63 92.16 92.40 93.35\nTop 85.17 90.91 84.13 83.01\nRand 65.12 71.11 65.11 74.18\nBot 58.19 26.11 35.99 57.11\nChunking\nAll 95.11 94.19 93.93 93.85\nTop 90.13 90.03 88.13 83.12\nRand 74.12 75.63 78.19 71.48\nBot 64.13 45.43 47.16 65.12\nCCG\nAll 92.23 92.43 91.66 91.23\nTop 75.61 76.31 71.22 68.09\nRand 70.01 63.11 68.03 41.37\nBot 61.12 62.31 67.99 30.12\nTable 6: Ablation Study: Selecting all, top, random\n(rand) and bottom (bot) 20% neurons and zeroing-out\nremaining to evaluate classiﬁer accuracy on dev test\n(averaged over three runs).\n(if we try increasing number of neurons in each\nstep by 1%) and N = 0,0.1,... 1e−7. We ﬁx the\nM = 20%to ﬁnd the best regularization parame-\nters ﬁrst reducing the grid search time to O(N2)\nand ﬁnd the optimal number of neurons in a subse-\nquent step with O(M). The overall running time\nof our algorithm therefore is O(M + N2). This\nvaries a lot in terms of wall-clock computation,\nbased on number of examples in the training data,\nnumber of tags to be predicted in the downstream\ntask. Including a full forward pass over the pre-\ntrained model to extract the contextualized vector,\nand running the grid search algorithm to ﬁnd the\nbest hyperparameters and minimal set of neurons\ntook on average 12 hours ranging from 3 hours (for\nPOS with ELMo experiment) to 18 hours (for CCG\nwith BERT).\nA.4 Ablation Study\nWe reported accuracy numbers on ablating top, ran-\ndom and bottom neurons in the trained classiﬁer,\non blind test-set in the main body. In Table 6, we\nreport results on development tests.\n4878\nBERT XLNet T-ELMo ELMo\nNeua 9984 9984 7168 3072\nPOS\nNeut 400/4% 400/4% 430/6% 368/12%\nAcca 96.10 96.38 96.61 96.45\nAcct 96.48 96.52 96.33 96.07\nSela 15.51 23.43 22.69 19.12\nSelt 31.81 31.62 37.61 38.52\nSEM\nNeut 400/4% 400/4% 716/10% 307/10%\nAcca 92.63 92.16 92.40 93.35\nAcct 92.19 92.59 92.17 93.21\nSela 5.82 14.01 12.19 11.37\nSelt 27.19 26.46 23.97 32.33\nChunking\nNeut 1000/10% 1000/10% 860/12% 983/32%\nAcca 95.11 94.19 93.93 93.85\nAcct 95.07 94.13 93.61 93.48\nSela 16.33 22.87 24.31 18.09\nSelt 29.32 28.19 31.05 26.38\nCCG\nNeut 1500/15% 1500/15% 2365/33% 1014/33%\nAcca 92.23 92.43 91.66 91.23\nAcct 92.13 92.49 91.89 91.09\nSela 7.48 14.21 11.42 11.99\nSelt 15.91 24.82 18.31 17.34\nTable 7: Selecting minimal number of neurons for each\ndownstream NLP task. Accuracy numbers reported on\ndev test (averaged over three runs) – Neua = Total num-\nber of neurons, Neu t = Top selected neurons, Acc a =\nAccuracy using all neurons, All t = Accuracy using se-\nlected neurons after retraining the classiﬁer using se-\nlected neurons, Sel = Difference between linguistic task\nand control task accuracy when classiﬁer is trained on\nall neurons (Sela) and top neurons (Selt).\nA.5 Minimal Neuron Set\nWe reported minimal number of neurons required\nto obtain oracle accuracy in the main body, along\nwith the results on Selectivity. In Table 7, we report\nresults on development tests.\nA.6 Localized versus Distributed Labels\nIn Section 5.1 we only showed number of features\nlearned for selected labels in each task. Figure 4\nshows results for all the tags across different tasks.\nThe results show that some tags are localized and\ncaptured by a focused set of neurons while others\nare distributed and learned within a large set of\nneurons.\nA.7 XLNet versus Others\nNotice in Figure 4 that neurons required by each\nlabel in XLNet (red bars) are strikingly small com-\npared to other architectures speciﬁcally T-ELMo\n(yellow bars). This is interesting given the fact that\ntotal number of neurons required by some of the\ntasks are very similar. For example task of POS tag-\nging required 400 neurons for BERT and XLNet,\n320 for ELMo and 430 in T-ELMo. This means\nthat neurons in XLNet are mutually exclusive to-\nwards the properties whereas in other architectures\nneurons are shared across multiple properties. Due\nto large tag set (1272 tags) in CCG super tagging,\nit is not possible to include it among ﬁgures.\nA.8 Layer-wise Distribution\nIn Section 5.2 we showed labels are captured dom-\ninantly at which layers for a few labels. In Figure\n5c we show all labels and which layers they are\npredominantly captured at, across different archi-\ntectures.\n4879\n(a) POS Tagging\n(b) SEM Tagging\n(c) Chunking Tagging\nFigure 4: Number of neurons per label across architectures\n4880\n(a) POS Tagging\n(b) SEM Tagging\n(c) Chunking Tagging\nFigure 5: Layer that predominately captures each label",
  "topic": "Syntax",
  "concepts": [
    {
      "name": "Syntax",
      "score": 0.7656093835830688
    },
    {
      "name": "Computer science",
      "score": 0.6910403966903687
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5820609927177429
    },
    {
      "name": "Task (project management)",
      "score": 0.5781709551811218
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5486080646514893
    },
    {
      "name": "Natural language processing",
      "score": 0.4973500072956085
    },
    {
      "name": "Language understanding",
      "score": 0.4916918873786926
    },
    {
      "name": "Disjoint sets",
      "score": 0.42734843492507935
    },
    {
      "name": "Linguistics",
      "score": 0.33628955483436584
    },
    {
      "name": "Mathematics",
      "score": 0.08080220222473145
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}