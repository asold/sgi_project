{
  "title": "exKidneyBERT: a language model for kidney transplant pathology reports and the crucial role of extended vocabularies",
  "url": "https://openalex.org/W4392238058",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2782877942",
      "name": "Yang Tiancheng",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A3084576631",
      "name": "Sucholutsky, Ilia",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": null,
      "name": "Jen, Kuang-Yu",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A3085448829",
      "name": "Schonlau Matthias",
      "affiliations": [
        "University of Waterloo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285794637",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2478323801",
    "https://openalex.org/W3174847532",
    "https://openalex.org/W4312095183",
    "https://openalex.org/W6680789778",
    "https://openalex.org/W2903950532",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W3156311852",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2972483465",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2552263672",
    "https://openalex.org/W3210307996",
    "https://openalex.org/W3211439143",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W6839990677",
    "https://openalex.org/W3035290244",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3202078810",
    "https://openalex.org/W6727690538",
    "https://openalex.org/W6861110998",
    "https://openalex.org/W6633714834",
    "https://openalex.org/W6927429520",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3036126091",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W4283761305",
    "https://openalex.org/W4392238058"
  ],
  "abstract": "Background Pathology reports contain key information about the patient’s diagnosis as well as important gross and microscopic findings. These information-rich clinical reports offer an invaluable resource for clinical studies, but data extraction and analysis from such unstructured texts is often manual and tedious. While neural information retrieval systems (typically implemented as deep learning methods for natural language processing) are automatic and flexible, they typically require a large domain-specific text corpus for training, making them infeasible for many medical subdomains. Thus, an automated data extraction method for pathology reports that does not require a large training corpus would be of significant value and utility. Objective To develop a language model-based neural information retrieval system that can be trained on small datasets and validate it by training it on renal transplant-pathology reports to extract relevant information for two predefined questions: (1) “What kind of rejection does the patient show?”; (2) “What is the grade of interstitial fibrosis and tubular atrophy (IFTA)?” Methods Kidney BERT was developed by pre-training Clinical BERT on 3.4K renal transplant pathology reports and 1.5M words. Then, exKidneyBERT was developed by extending Clinical BERT’s tokenizer with six technical keywords and repeating the pre-training procedure. This extended the model’s vocabulary. All three models were fine-tuned with information retrieval heads. Results The model with extended vocabulary, exKidneyBERT, outperformed Clinical BERT and Kidney BERT in both questions. For rejection, exKidneyBERT achieved an 83.3% overlap ratio for antibody-mediated rejection (ABMR) and 79.2% for T-cell mediated rejection (TCMR). For IFTA, exKidneyBERT had a 95.8% exact match rate. Conclusion ExKidneyBERT is a high-performing model for extracting information from renal pathology reports. Additional pre-training of BERT language models on specialized small domains does not necessarily improve performance. Extending the BERT tokenizer’s vocabulary library is essential for specialized domains to improve performance, especially when pre-training on small corpora.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7951684594154358
    },
    {
      "name": "Vocabulary",
      "score": 0.6684538722038269
    },
    {
      "name": "Natural language processing",
      "score": 0.6516075134277344
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6194910407066345
    },
    {
      "name": "Information extraction",
      "score": 0.5122711062431335
    },
    {
      "name": "Information retrieval",
      "score": 0.47355762124061584
    },
    {
      "name": "Language model",
      "score": 0.46301907300949097
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4541844129562378
    },
    {
      "name": "Field (mathematics)",
      "score": 0.42223799228668213
    },
    {
      "name": "Pathology",
      "score": 0.41296058893203735
    },
    {
      "name": "Medicine",
      "score": 0.3419792056083679
    },
    {
      "name": "Linguistics",
      "score": 0.10193637013435364
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}