{
  "title": "Large Language Models for Few-Shot Named Entity Recognition",
  "url": "https://openalex.org/W2896583095",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2217990740",
      "name": "Zhao Yufei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2355724478",
      "name": "Zhong Xiao-shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250271477",
      "name": "Cambria, Erik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4376844790",
      "name": "Rajapakse, Jagath C.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2068882115",
    "https://openalex.org/W2096765155",
    "https://openalex.org/W2101619065",
    "https://openalex.org/W2131514140",
    "https://openalex.org/W2756381707",
    "https://openalex.org/W2008620264",
    "https://openalex.org/W371426616",
    "https://openalex.org/W2087619920",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W2950627632",
    "https://openalex.org/W2129577276",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2739826982",
    "https://openalex.org/W1570587036",
    "https://openalex.org/W2053131585",
    "https://openalex.org/W89503205",
    "https://openalex.org/W1951325712",
    "https://openalex.org/W180474045",
    "https://openalex.org/W2407338347",
    "https://openalex.org/W1992871189",
    "https://openalex.org/W2044340178",
    "https://openalex.org/W2004763266",
    "https://openalex.org/W2077941393",
    "https://openalex.org/W1515435652",
    "https://openalex.org/W2103076621",
    "https://openalex.org/W2047477415",
    "https://openalex.org/W2167237149",
    "https://openalex.org/W2579198251",
    "https://openalex.org/W1543884596",
    "https://openalex.org/W2056894934",
    "https://openalex.org/W2060535757",
    "https://openalex.org/W2467375345",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2013516050",
    "https://openalex.org/W2757947833",
    "https://openalex.org/W2294360863",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2142222368",
    "https://openalex.org/W1970849810",
    "https://openalex.org/W2321470647",
    "https://openalex.org/W2160097208",
    "https://openalex.org/W2056354103",
    "https://openalex.org/W2144087279",
    "https://openalex.org/W2158049734",
    "https://openalex.org/W130850236",
    "https://openalex.org/W2108218871",
    "https://openalex.org/W1502473750",
    "https://openalex.org/W2126176114",
    "https://openalex.org/W1568620938",
    "https://openalex.org/W2153848201",
    "https://openalex.org/W2141099517",
    "https://openalex.org/W2137132216",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2963140597",
    "https://openalex.org/W2520117834",
    "https://openalex.org/W2157275230",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W2115557995",
    "https://openalex.org/W1893213668",
    "https://openalex.org/W2406945108",
    "https://openalex.org/W2119759918",
    "https://openalex.org/W2797724060"
  ],
  "abstract": "Named entity recognition (NER) is a fundamental task in numerous downstream applications. Recently, researchers have employed pre-trained language models (PLMs) and large language models (LLMs) to address this task. However, fully leveraging the capabilities of PLMs and LLMs with minimal human effort remains challenging. In this paper, we propose GPT4NER, a method that prompts LLMs to resolve the few-shot NER task. GPT4NER constructs effective prompts using three key components: entity definition, few-shot examples, and chain-of-thought. By prompting LLMs with these effective prompts, GPT4NER transforms few-shot NER, which is traditionally considered as a sequence-labeling problem, into a sequence-generation problem. We conduct experiments on two benchmark datasets, CoNLL2003 and OntoNotes5.0, and compare the performance of GPT4NER to representative state-of-the-art models in both few-shot and fully supervised settings. Experimental results demonstrate that GPT4NER achieves the $F_1$ of 83.15\\% on CoNLL2003 and 70.37\\% on OntoNotes5.0, significantly outperforming few-shot baselines by an average margin of 7 points. Compared to fully-supervised baselines, GPT4NER achieves 87.9\\% of their best performance on CoNLL2003 and 76.4\\% of their best performance on OntoNotes5.0. We also utilize a relaxed-match metric for evaluation and report performance in the sub-task of named entity extraction (NEE), and experiments demonstrate their usefulness to help better understand model behaviors in the NER task.",
  "full_text": "Large Language Models for Few-Shot Named Entity Recognition\nYufei Zhao1, Xiaoshi Zhong1*, Erik Cambria2, and Jagath C. Rajapakse2\n1School of Computer Science and Technology, Beijing Institute of Technology, China.\n2College of Computing and Data Science, Nanyang Technological University, Singapore.\nyfzhao@bit.edu.cn; xszhong@bit.edu.cn; cambria@ntu.edu.sg; asjagath@ntu.edu.sg\nAbstract\nNamed entity recognition (NER) is a fundamental task in nu-\nmerous downstream applications. Recently, researchers have\nemployed pre-trained language models (PLMs) and large lan-\nguage models (LLMs) to address this task. However, fully\nleveraging the capabilities of PLMs and LLMs with minimal\nhuman effort remains challenging. In this paper, we propose\nGPT4NER, a method that prompts LLMs to resolve the few-\nshot NER task. GPT4NER constructs effective prompts using\nthree key components: entity definition, few-shot examples,\nand chain-of-thought. By prompting LLMs with these effec-\ntive prompts, GPT4NER transforms few-shot NER, which\nis traditionally considered as a sequence-labeling problem,\ninto a sequence-generation problem. We conduct experiments\non two benchmark datasets, CoNLL2003 and OntoNotes5.0,\nand compare the performance of GPT4NER to representative\nstate-of-the-art models in both few-shot and fully supervised\nsettings. Experimental results demonstrate that GPT4NER\nachieves theF 1 of 83.15% on CoNLL2003 and 70.37%\non OntoNotes5.0, significantly outperforming few-shot base-\nlines by an average margin of 7 points. Compared to fully-\nsupervised baselines, GPT4NER achieves 87.9% of their best\nperformance on CoNLL2003 and 76.4% of their best perfor-\nmance on OntoNotes5.0. We also utilize a relaxed-match met-\nric for evaluation and report performance in the sub-task of\nnamed entity extraction (NEE), and experiments demonstrate\ntheir usefulness to help better understand model behaviors in\nthe NER task.\nCode and Data— https://github.com/xszhong/GPT4NER\nIntroduction\nNamed entity recognition (NER) (Chinchor and Robinson\n1997) is a fundamental task in natural language processing,\naiming to extract and classify named entities from unstruc-\ntured text. By transforming original text into structured data,\nNER provides crucial support for many downstream tasks,\nmaking its accuracy essential for subsequent tasks. Tradi-\ntionally, NER is treated as a sequence-labeling task (Devlin\net al. 2019; Yang and Katiyar 2020). Early methods primar-\nily rely on large annotated corpora from specific domains\nand employ supervised or semi-supervised learning algo-\nrithms to address this task (Yang and Katiyar 2020; Ding\net al. 2020). While these methods perform well on closed\n*Xiaoshi Zhong is the corresponding author.\ndatasets (Wang et al. 2021; Li et al. 2022), they often require\naccess to complete labeled training datasets for model train-\ning and fail to meet the demands of open-ended business\nscenarios in industry due to limitations in labeled data and\nthe scarcity of data in specific domains such as biomedicine\nand materials science.\nIn the early stages, the NER task (Chinchor and Robinson\n1997) primarily relies on rule-based methods (Hanisch et al.\n2005; Riaz 2010) and dictionary-based methods (Sasaki\net al. 2008; Egorov, Yuryev, and Daraselia 2004), which re-\nquire experts to manually construct rules based on dataset\nfeatures. This process is both time-consuming and labor-\nintensive. With the advancement of machine-learning tech-\nniques, researchers adopt machine learning-based meth-\nods to resolve the NER task. Hidden Markov models\n(HMMs) (Egorov, Yuryev, and Daraselia 2004; Morwal, Ja-\nhan, and Chopra 2012; Zhao 2004) and conditional ran-\ndom fields (CRFs) (Xu et al. 2008; Li, Zhou, and Huang\n2009) becomes particularly representative of this approach.\nWhile these statistical machine learning-based NER mod-\nels significantly improve the performance, they require ex-\ntensive manual annotation of domain-specific data, limit-\ning their scalability and practical application. The rise of\ndeep-learning and neural-network techniques further trans-\nform the NER task. Researchers employ these methods,\nwith commonly used NER models including convolutional\nneural networks (CNNs) (Collobert et al. 2011; Chiu and\nNichols 2016; Ma and Hovy 2016) and recurrent neural net-\nworks (RNNs) (Lyu et al. 2017; Chowdhury et al. 2018),\namong others. These deep-learning models can automati-\ncally learn feature representations from large-scale data and\nhave achieved significant improvements in the NER task.\nDevlin et al. (2019) transfer the pre-trained language\nmodel BERT to fine-tuning on 11 natural language pro-\ncessing benchmark tasks, achieving state-of-the-art results.\nSince then, NER methods have increasingly relied on large-\nscale pre-trained language models, which leverage benefits\nof big data and large-scale computing. Wang et al. (2022a)\npropose structural pre-training, which guides language mod-\nels to generate structures from text and enhances knowledge\ntransfer between different tasks. Context learning has also\nbeen applied to the NER task. For example, Chen et al.\n(2023) design a meta-function pre-training algorithm to in-\nject context learning capabilities into pre-trained language\narXiv:1810.06818v3  [cs.CL]  29 Oct 2025\nmodels, which enables rapid identification of new entity\ntypes using demonstration instances. Additionally, data aug-\nmentation techniques have been used to alleviate the scarcity\nof labeled data in NER. For example, Hu et al. (2023) pro-\npose an entity-to-text data augmentation technique that uti-\nlizes pre-trained large-scale language models to construct an\naugmented entity list.\nWith the rise and widespread use of large language\nmodels (LLMs) such as OpenAI’s GPT series (e.g., GPT-\n3 (Brown et al. 2020) and GPT4 (Achiam et al. 2023)),\nvarious NLP tasks have achieved promising results, includ-\ning relation extraction (Wadhwa, Amir, and Wallace 2023;\nDagdelen et al. 2024) and question answering (Lu et al.\n2022). Trained on diverse datasets across multiple domains,\nLLMs exhibit powerful capabilities in understanding context\nand generating natural language text. With only a few exam-\nples as demonstrations for a specific task, LLMs can gener-\nate accurate responses to new inputs. In the era of LLMs,\nnumerous studies have explored their application to NER\ntasks, including few-shot learning (Huang et al. 2022; Wang\net al. 2025; Ashok and Lipton 2023), zero-shot learning (Xie\net al. 2023; Hu et al. 2024; Shao et al. 2024), fine-tuning\nmodels for target domains, and using GPT as a data gen-\nerator for data augmentation (Ghosh et al. 2023; Ye et al.\n2024). Zhao et al. (2025) propose a few-shot biomedical\nNER method that combines LLM-assisted data augmenta-\ntion with multi-scale feature extraction, effectively improv-\ning model performance on multiple biomedical datasets un-\nder few-shot settings. Few-shot methods typically include\ndomain transfer and prompt engineering. Domain transfer\nmethods (Das et al. 2022; Chen, Zheng, and Yang 2023)\nusually train on large amounts of source data and fine-tuning\non examples from target domain. Prompt engineering meth-\nods (Wang et al. 2025; Zhou et al. 2024; Layegh et al.\n2023) often adopt a strategy of querying for the presence\nof one specific entity type at a time to improve recognition\naccuracy. However, this querying approach significantly in-\ncreases time when dealing with multiple entity types, espe-\ncially more entity types or longer test text processing. The\ntime cost becomes a critical bottleneck in such cases.\nChain-of-thought prompting provides statement reason-\ning and maintains complete interpretability (Wei et al. 2022).\nHowever, it performs poorly when addressing problems\nmore complex than provided examples. Zhou et al. (2023)\nintroduce “least-to-most prompting” to decompose a com-\nplex problem into a series of sub-problems and address them\nsequentially, which enables the model to solve problems\nharder than the examples. Zhang et al. (2022) propose the\nauto-CoT paradigm to automatically constructs questions\nand reasoning chains, which improves fault tolerance.\nAshok and Lipton (2023) apply chain-of-thought prompt-\ning to few-shot NER tasks, achieving cross-domain appli-\ncations and improving flexibility by modifying definitions\nand examples. However, the few-shot examples are selected\nrandomly without a targeted selection strategy, and evalu-\nation is conducted on a random sample of 500 test exam-\nples by reporting mean and variance over 5 runs, which\nmay not reflect performance across the full dataset or multi-\ntype entity scenarios. Wang et al. (2025) apply GPT-3 to the\nNER task by converting sequence labeling into a generation\ntask, requiring the identification of entity types after provid-\ning prompts and examples (obtaining the nearest neighbors\nas examples through k-nearest neighbors). They use a self-\nverification strategy to address the hallucination problem of\nLLMs. However, this method can only extract one type of\nentities at a time. In datasets with many entity types, this\ncan result in more time spent. Zhou et al. (2024) compare\nquerying all types of entities at once to querying one type\nof entity at a time, the mode of querying all types of entities\nat once is not efficient. Guo et al. (2025) propose BANER,\na boundary-aware NER framework leveraging contrastive\nlearning and LoRAHub for cross-domain adaptation. While\nBANER improves entity boundary detection in few-shot set-\ntings, it employs a single, stage-specific prompt template for\neach phase, which may limit flexibility and expressiveness.\nInspired by these LLM-based methods such as Prompt-\nNER (Ashok and Lipton 2023), GPT-NER (Wang et al.\n2025), and BANER (Guo et al. 2025), in this paper, we\npropose GPT4NER, an LLM-based method that leverages\nthe capabilities of LLMs to tackle the few-shot NER task.\nGPT4NER enables querying for all entity types in a single\nquery, reducing querying time. GPT4NER constructs effec-\ntive prompts using three key components: entity definition,\nfew-shot examples, and chain-of-thought, with an optional\ncomponent of part-of-speech (POS) tags. The entity defi-\nnition component provides detailed definitions and identi-\nfication criteria for each entity type in the dataset, includ-\ning boundary delineation and clarification of classification\nconfusion points. We design a selection procedure to choose\nfew-shot examples that cover all entity types and those dif-\nficult entities to generate, implicitly specifying the output\nformat. We sample the training data during the few-shot ex-\namples construction process, rather than using all the train-\ning data. The chain-of-thought component guides LLMs to\nprovide reasoning for their output, enhancing the quality\nof generation. POS tags optionally supply syntactic infor-\nmation for contextual text. These components ensure that\nthe prompts embody clear instructions, task-relevant back-\nground, and a defined output format, facilitating optimal\nmodel comprehension for the few-shot NER task.\nGPT4NER differs from previous LLM-based NER meth-\nods in several aspects. Unlike PromptNER (Ashok and\nLipton 2023), which randomly selects few-shot examples,\nGPT4NER implements a targeted selection strategy that en-\nsures coverage of all entity types and difficult-to-generate\nentities, which improves reliability across multi-type sce-\nnarios. Compared to GPT-NER (Wang et al. 2025), which\nqueries one entity type at a time and requires a separate ver-\nification strategy to handle hallucinations, GPT4NER can\nquery all entity types in a single call. Compared to BANER\n(Guo et al. 2025), which decomposes NER into a two-stage\nprocess and employs stage-specific prompt templates pri-\nmarily focused on boundary detection, GPT4NER performs\nend-to-end entity recognition with prompts combining entity\ndefinitions, few-shot examples, chain-of-thought reasoning,\nand optional POS tags, allowing expressive instructions and\nstructured output guidance.\nTo evaluate the effectiveness of GPT4NER, we conduct\nexperiments on two benchmark datasets, CoNLL2003 (Sang\nand Meulder 2003) and OntoNotes5.0 (Pradhan et al.\n2013), focusing on the few-shot NER task and its sub-\ntask of named entity extraction (NEE). We compare the\nresults of GPT4NER to two types of representative state-\nof-the-art models: few-shot models and fully-supervised\nmodels. Experimental results demonstrate that GPT4NER\nachieves theF 1 of 83.15% on CoNLL2003 and 70.37% on\nOntoNotes5.0, significantly outperforming few-shot base-\nlines by an average margin of 7 points. Compared to fully-\nsupervised baselines, GPT4NER achieves 87.9% of their\nbest performance on CoNLL2003 and 76.4% of their best\nperformance on OntoNotes5.0. Furthermore, our experi-\nments utilize the relaxed-match metric, which is widely used\nfor evaluating time expression recognition and normaliza-\ntion (Verhagen et al. 2007, 2010; UzZaman et al. 2013;\nZhong, Sun, and Cambria 2017; Zhong and Cambria 2018;\nZhong, Cambria, and Hussain 2020; Zhong and Cambria\n2021), to evaluate the performance of few-shot models. Our\nanalysis indicates that while few-shot models may not pre-\ncisely recognize the boundaries of named entities, they can\nidentify portions of these entities. Additionally, our findings\nhighlight the importance of reporting model performance in\nthe sub-task of few-shot NEE to better analyze model capa-\nbilities in the few-shot NER task.\nIn summary, the main contributions of this paper are as\nfollows:\n• We propose GPT4NER, a method that prompts LLMs for\nfew-shot NER. GPT4NER constructs effective prompts\nusing three key components: entity definition, few-shot\nexamples, and chain-of-thought, along with one optional\ncomponent, POS tags, and adopts a targeted selection\nstrategy that ensures coverage of all entity types and\ndifficult-to-generate entities.\n• We conduct experiments on two benchmark datasets, and\nexperimental results demonstrate that GPT4NER signif-\nicantly outperforms representative state-of-the-art few-\nshot models and achieves approximately 82.15% of the\nbest performance of fully-supervised models, on average.\n• Our experiments suggest that utilizing the relaxed-match\nmetric to evaluate model performance can enhance our\nunderstanding of model capabilities, and that reporting\nNEE performance provides further insights into model\ncapabilities in the NER task.\nMethodology\nFigure 1 provides an overview of GPT4NER for few-shot\nNER, comprising two parts: (1) prompt construction and (2)\nentity generation by LLMs. The prompt is built from three\ncore elements: entity definitions, few-shot examples with\nchain-of-thought reasoning, and the input test text.\nPrompt Construction\nIn leveraging the capabilities of LLMs, constructing effec-\ntive prompts is crucial, laying the foundation for subse-\nquent model training and inference processes. An exemplary\nPrompt\nLarge Language Models\nPOS\nTags\nGPT-3.5-turbo-instruct\n Entity Definition\nLOC PER ORG...\nFew-Shot Examples\nParagraph: SEATTLE AT BOSTONAnswer:    1. SEATTLE | True | as it refersto a basketball team (ORG)    2. BOSTON | True | as itemphasizes a location here (LOC)\nTest Text\nParagraph: PBS produce theiralbums in New York .\nOutput\n1. PBS | True | as it is a specific\norganization (ORG)\n2. albums | False | as it is a common\nnoun\n3. New York | True | as it is a city (GPE)\nPrompt Construction\nEntity Generation\nFigure 1: Overview of GPT4NER for few-shot NER. The\nleft-hand side illustrates the prompt construction using three\nkinds of information: (1) entity definition, (2) few-shot ex-\namples with chain-of-thought reasoning, and (3) input test\ntext. The right-hand side depicts the procedure of LLMs pro-\ncessing prompts and generating entities.\nprompt should embody the following three key character-\nistics to ensure optimal model comprehension and perfor-\nmance:\n•Clear Instructions: An effective prompt must provide\nexplicit and unambiguous instructions regarding the ob-\njectives and requirements of the task. Such clarity is es-\nsential to facilitate accurate understanding of the core\ncontent.\n•Task-Relevant Background: An effective prompt\nshould integrate task-relevant background knowledge,\nencompassing domain-specific expertise, entity at-\ntributes, or several examples.\n•Output Format: An effective prompt should specify\nthe output format, either explicitly or implicitly, because\nthe output format directly impacts subsequent processing\nand evaluation. A chaotic or disorganized output format\ncan pose significant challenges for processing and evalu-\nation tasks.\nTo embody these key characteristics, we construct effec-\ntive prompts that include three key components: (1) entity\ndefinition, (2) few-shot examples with output format, and\n(3) chain-of-thought, along with one optional component:\nsyntactic POS information. Below, we detail these compo-\nnents with an example of effective prompts designed for the\nCoNLL2003 dataset, as illustrated in Figure 2.\nEntity DefinitionIn different datasets, researchers may\nspecify significantly diverse definitions and identification\nrules for the same types of entities. For example, both\nCoNLL2003 and OntoNotes5.0 includeLOCentities, but\ntheir definitions differ. CoNLL2003 classifiesLOCentities\nas countries, cities, regions such as “London” and “Ger-\nmany”. By contrast, OntoNotes5.0 definesLOCentities as\nnon-GPElocations, including mountain ranges and planets.\nFurthermore, OntoNotes5.0 includesGPE(i.e., geopolitical\nentities like countries and cities) andFAC(i.e., facilities like\nbuildings and roads), which can overlap withLOCin some\ncases.\nDefinition:\nAn entity is a real object or concept representing a person (PER), named organization (ORG), location (LOC), country\n(LOC), or nationality (MISC). Most entities are expressed as proper nouns (NNP or NNPS) while nationality and\nlanguage entities under MISC are adjectives. Names, first names, last names, and countries are entities. Nationalities\nare considered entities even if they are adjectives. However, sports, sporting events, adjectives, verbs, numbers,\nadverbs, and abstract concepts are not considered entities. Similarly, dates, years, weekdays, months, and times are\nnot entities. Possessive words and pronouns like ''I'', ''you'', ''him'', and ''me'' are not entities. If a sporting team is\nreferred to by the name of their location, the term is considered an entity representing an organization, not a location.\nExample:\nQ: Given the paragraph below, identify a list of possible entities and for each entry explain why it either is or is not an\nentity. \nParagraph:\n   South African all - rounder Shaun Pollock , forced to cut short his first season with Warwickshire to have ankle\nsurgery , has told the English county he would like to return later in his career .     \n    Answer:\n    1. South African | True | as it is an adjective representing nationality (MISC)\n    2. Shaun Pollock | True | as it is a person's name (PER)\n    3. Warwickshire | True | as it refers to a cricket club, emphasizing an organization here (ORG)\n    4. English | True | as it is an adjective representing nationality (MISC)\n    5. county | False | as it is a common noun\n    6. he | False | as it is a pronoun\n    7. career | False | as it is a common noun\nTest Text:\nQ: Given the paragraph below, identify a list of possible entities and for each entry explain why it either is or is not an\nentity. \nParagraph:\n    SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .  \n    Answer:\nPrompt\nFigure 2: An example of effective prompts for the\nCoNLL2003 dataset. Entity definition is in red. Few-shot\nexamples with question-answer format and chain-of-thought\nreason are in blue. Test text is in dark green.\nTherefore, simply hinting at differences between entity\ntypes in few-shot examples may impede the performance\nof traditional few-shot methods. It is crucial to provide ex-\nplicit and unambiguous definitions and meticulous identifi-\ncation rules for each entity type within the dataset. Addi-\ntionally, it is important to delineate boundary conditions and\nelucidate points of potential classification ambiguity. For ex-\nample, OntoNotes5.0 specifiesPERSONto include genera-\ntional markers (e.g., “Jr.” and “IV”) while exclude honorifics\n(e.g., “Ms.” and “Dr.”) and occupational titles (e.g., “Presi-\ndent” and “Secretary”). Explicitly describing the scope of an\nentity helps precisely identify the boundaries of entities.\nAn inherent challenge in prompt construction is reconcil-\ning the need for domain-specific knowledge with users’ lim-\nited understanding. To resolve this challenge, we express en-\ntity definitions in natural language, avoiding excessive tech-\nnical jargon. Such strategy not only facilitates comprehen-\nsion but also provides greater flexibility, allowing for adapt-\nable use across diverse datasets without sacrificing speci-\nficity.\nIn entity definition module, we adhere to these princi-\nples by providing detailed definitions and identification cri-\nteria in natural language for each entity type within indi-\nvidual datasets. These definitions include boundary delin-\neation and points of classification confusion. In this paper,\nwe utilize two benchmark datasets: CoNLL2003 (Sang and\nMeulder 2003) and OntoNotes5.0 (Pradhan et al. 2013). For\nCoNLL2003, we construct the following definition for its\nentities, as illustrated in Figure 2. For OntoNotes5.0, we\nconstruct the following definition for its entities:\nAn entity is a real object or concept that represents an\nevent, facility, country, language, location, national-\nity, organization, person, product, or work of art. Typ-\nically, entities are expressed as proper nouns (NNP\norNNPs). Event (EVENT) entities refer to proper\nnouns representing hurricanes, battles, wars, sports\nevents, and attacks. Facility (FAC) entities refer to\nproper nouns associated with man-made structures\nlike buildings, airports, highways, and bridges. Ge-\nographical (GPE) entities refer to proper nouns rep-\nresenting countries, cities, states, provinces, and mu-\nnicipalities. Language (LANGUAGE) entities refer\nto named languages. Location (LOC) entities refer\nto proper nouns representing non-GPElocations, in-\ncluding mountain ranges, planets, geo-coordinates,\nbodies of water, named regions, and continents. Na-\ntionalities, religious, or political groups (NORP) are\nexpressed through adjectival forms of geographical,\nsocial, and political entities, location names, named\nreligions, heritage, and political affiliations. Orga-\nnization (ORG) entities refer to proper nouns rep-\nresenting companies, government agencies, educa-\ntional institutions, and sports teams. This also in-\nclude adjectival forms of organization names and\nmetonymic mentions of associated buildings or lo-\ncations. Person (PERSON) entities are represented\nby proper personal names, including fictional char-\nacters, first names, last names, nicknames, and gen-\nerational markers (such as Jr. and IV), excluding\noccupational titles and honorifics. Product (PROD-\nUCT) entities refer to proper nouns representing\nmodel names, vehicles, or weapons. Manufacturer\nand product should be marked separately. Works of\nart (WORK OF ART) refer to titles of books, songs,\narticles, television programs, or awards. If an orga-\nnization, occupation title, and person’s name form a\nphrase, then the organization and person’s name is\nmarked separately. Nominals and common nouns are\nnot considered entities. Additionally, pronouns and\npronominal elements are excluded from entities, as\nare contact information, plants, dates, years, times,\nnumbers, legal documents, treaties, credit cards,\nchecking accounts,CDs, credit plans, financial in-\nstruments, and abstract concepts.\nFew-Shot Examples with Implicit Output FormatFew-\nshot examples serve as a vital instructional tool in prompts,\nproviding tangible exemplars of contextual instantiation for\neach entity type. The inclusion of these examples aims to\nafford the model invaluable insights into contextual nuances\nunderpinning entity identification.\nMany LLMs have strict limitations on the maximum num-\nber of tokens they can process (e.g., OpenAI’s GPT-3.5-\nturbo-instruct model supports only a 4K-token window).\nConsequently, each input can accommodate up to 10 exam-\nples, with around 400-500 tokens reserved for output.\nIn our constructed prompts, each example comprises three\ntypes of information: (1)a task-description question, (2)an\ninput text, and (3)output results. These few-shot examples\nprovide direct instructions and evidence relevant to the task,\nenabling LLMs to grasp the logic of predictions.\nTask-Description Question. The task-description ques-\ntion serves to guide LLMs on the task at hand. We utilize\nthe following format as the task instruction:\nAlgorithm 1: Example selection for limited tokens\n1:Input:Training setD, maximum token limitT\n2:Output:Optimized few-shot examplesP\n3:P ← ∅\n4:Step 1: Select Texts with Multiple Entity Types\n5:Select textsT 1 ⊆ Dwith at least 3 entity types\n6:P ← P ∪ {Select 3-4 texts fromT1 covering all entity types}\n7:Step 2: Identify and Test Confused Entities\n8:Select confused entitiesE c ⊆ D\n9:foreach textt∈ E c do\n10:Testtusing current promptP\n11:ifentity recognition is suboptimalthen\n12:P ← P ∪ {t}\n13:end if\n14:end for\n15:Step 3: Sample and Finalize Examples\n16:whilenumber of examples inPis less than 10 and token\ncount< Tdo\n17:Randomly sample 10 textsT 2 ⊆ Deach time\n18:foreach textt∈ T 2 do\n19:Testtusing current promptP\n20:ifentity recognition is suboptimalthen\n21:P ← P ∪ {t}\n22:end if\n23:end for\n24:end while\n25:returnP\n•Q: Given the paragraph below, identify a list of possible\nentities and for each entry explain why it either is or is\nnot an entity.\nInput Text.The input text is selected as an example from\nthe training data, with the primary objective of enhancing\nthe accuracy of recognizing entities in test text. The selec-\ntion process prioritizes texts that closely resemble the test\ntext, especially those presenting identification challenges.\nThese chosen texts often exhibit more complex results and\ninvolve entity categories that are easily confused, including\nboth positive and negative instances (i.e., examples of both\nentities and non-entities).\nFew-shot examples are thoughtfully selected to illustrate\ndiverse contexts in which a given entity type may manifest,\nencompassing variations in syntactic structure and semantic\ncontext. By exposing the model to a range of context under-\nstanding and entity recognition instances, we aim to imbue\nthe model with a robust understanding of the myriad man-\nifestations of entity types, thereby enhancing its adaptabil-\nity and generalization capabilities. To achieve this, we select\nand adjust examples through multiple sampling tests based\non feedback from results. These few-shot examples include\nchallenges in identifying entities and understanding specific\ncontexts. Given the limited number of tokens specified by\nLLMs, we carefully select these examples. This selection\nprocedure mainly comprises the following three steps, as il-\nlustrated in Algorithm 1.\n•Step 1: Select texts with multiple types of entities.\nFrom the training set, select texts containing at least three\ntypes of entities. Choose three to four texts to ensure all\nentity types are covered for a 1-shot setup.\n•Step 2: Identify confused entities. Conduct a small-\nscale test using the texts selected inStep 1to evalu-\nate the prompt’s effectiveness, and then add those texts\nwhose entities are poorly generated as new examples to\nthe prompt.\n•Step 3: Check for omissions. Randomly select ten texts\nfor testing each time. Gradually add examples following\nStep 2until the number of added examples reaches ten,\nthe maximum number of examples.\nOutput Format.The output format is implicitly incorpo-\nrated into prompts alongside entity definition and few-shot\nexamples. It delineates the expected format for the output la-\nbels corresponding to identified entities. These implicit out-\nput formats serve as guiding beacons, steering the model\ntowards generating output labels that adhere to predefined\nstandards of clarity, consistency, and conciseness. By em-\nbedding an intrinsic awareness of annotation conventions\nwithin the model, these implicit output formats ensure that\noutputs are semantically accurate and adhere to established\nannotation standards.\nEach test sentence needs to satisfy the following condi-\ntions: (1) it needs to clearly list words or phrases that are\n(or are not) entities and their corresponding category labels;\n(2) it needs to be easy for LLMs to learn and imitate, so\nthat we can smoothly label each token in the test sentence.\nThe output includes a list of candidate entities, explanations\nfor identification and classification, and specific entity types.\nThe output format is structured as follows:\nCandidate|True or False|Explanation of why the\ncandidate is or is not an entity [(Type)]\nwhich contains three elements:\n•Candidate: This element indicates a generated candidate\nthat may be considered as an entity.\n•True or False: This element indicates whether the gener-\nated candidate is an entity. Specifically, “True” indicates\nthat the candidate is an entity, while “False” indicates\nthat it is not.\n•Explanation of why the candidate is or is not an entity\n[(Type)]: This element explains why the candidate is or is\nnot treated as an entity and specifies the type of the entity\nif the candidate is an entity. It is our designed chain-of-\nthought component and will be described in subsequent\nparts.\nFor example, as shown in Figure 2, the first entry of\nthe output is “South African|True|as it is an adjective\nrepresenting nationality (MISC)”. This means that “South\nAfrican” is a generated candidate that may be treated as\nan entity. “True” indicates that “South African” is indeed\ntreated as an entity. The explanation “as it is an adjec-\ntive representing nationality (MISC)” clarifies why “South\nAfrican” is treated as an entity, specifically under the type\nMISC. By contrast, the fifth entry, “county|False|as it is\na common noun”, indicates that “county” is a candidate, but\n“False” suggests that this candidate is not an entity. The ex-\nplanation, “as it is a common noun”, clarifies why “county”\nis not treated as an entity.\nChain-of-ThoughtIncorporating explanations of whether\ncandidates are entities into the prompt enhances the clarity\nof the instruction and serves as a practical implementation\nof our chain-of-thought reasoning. This approach strategi-\ncally guides LLMs through a systematic thought process,\nencouraging careful consideration of each step and coher-\nent explanations for its decisions. Recent findings suggest\nthat chain-of-thought prompting can guide LLMs to output\nreasoning, even by simply adding “think step by step” to\nthe prompt (Wei et al. 2022; Zhang et al. 2022; Wang et al.\n2022b). Numerous studies have underscored the effective-\nness of this approach, showing that guiding LLMs to think\nstep by step and articulate their reasoning can greatly reduce\nerrors. By asking LLMs to provide a reason for recognition\nalong with generating the entity list, we can substantially\nimprove the reliability of the outputs. In this work, chain-of-\nthought explanations are incorporated into the output format\nof the prompt.\nPart-of-Speech (POS) InformationZhong, Cambria,\nand Hussain (2020) demonstrate that named entities are pri-\nmarily composed of proper nouns, and Ye et al. (2024) show\nthat data augmentation using LLMs to alter the syntactic\nstructure of input text can enhance few-shot NER. There-\nfore, we incorporate part-of-speech (POS) tags to enrich the\nsyntactic information of named entities in the text. Specifi-\ncally, POS tags are included in these few-shot examples as\npart of the input text, as shown below:\nSouth/NNP African/JJ all/DT -/HYPH rounder/NN\nShaun/NNP Pollock/NNP ,/, forced/VBN to/TO\ncut/VB short/IN his/PRP first/JJ season/NN with-\n/IN Warwickshire/NNP to/TO have/VB ankle/NN\nsurgery/NN ,/, has/VBZ told/VBN the/DT English/JJ\ncounty/NN he/PRP would/MD like/VB to/TO re-\nturn/VB later/RBR in/IN his/PRP career/NN ./.\nEntity Generation by Large Language Models\nRecent studies approach the NER task as a sequence-to-\nsequence problem and employ methods such as prompt-\nbased techniques or in-context learning. We adopt a simi-\nlar perspective to address the NER task through sequence\ngeneration by prompting LLMs. A primary motivation for\ntreating NER as sequence-generation problem is to mitigate\nthe challenge of combinatorial explosion, which arises when\nentities consist of multiple tokens. Traditional token-based\napproaches may struggle to handle such cases effectively,\nleading to suboptimal performance and decreased accuracy.\nBy contrast, LLMs have demonstrated noteworthy perfor-\nmance in NER tasks, even when they are trained on only\na small subset of training data. This highlights the efficacy\nof leveraging unsupervised pre-trained models for sequence\ngeneration tasks, where the model can generalize effectively\nfrom a limited number of examples to achieve competi-\ntive performance across diverse datasets and domains. Un-\nlike conventional supervised models that rely heavily on la-\nTable 1: Statistics of the two benchmark datasets\nDataset #Sentences #Tokens #Entities #Types\nCoNLL2003\nTrain 14987 203621 23499\n4 Dev. 3466 51362 5942\nTest 3684 46435 5648\nOntoNotes5.0\nTrain 59924 1088503 55530\n10 Dev. 8528 147724 7584\nTest 8262 152728 7505\nbeled training data, we utilize LLMs as a powerful sequence-\ngeneration tool for few-shot NER, capitalizing on their abil-\nity to perform well with minimal labeled data.\nExperimental Setup\nDatasets.The evaluation of GPT4NER is conducted on\ntwo benchmark datasets: CoNLL2003 (Sang and Meulder\n2003) and OntoNotes5.0 (Pradhan et al. 2013).\nCoNLL2003is a widely used benchmark dataset derived\nfrom the Reuters RCV1 corpus, containing 1,393 news ar-\nticles spanning from August 1996 to August 1997. It in-\ncludes 35,089 entities categorized into four types:PER,\nLOC,ORG, andMISC.\nOntonotes5.0is also a widely used benchmark dataset de-\nveloped for the analysis of several linguistic tasks in three\nlanguages. In this paper, we focus only the NER task in\nEnglish and use only the NER portion of the OntoNotes5.0\ndataset. This subset consists of 3,370 articles collected from\nvarious sources such as newswire and web data. It con-\ntains 18 types of entities, among which 10 types are primar-\nily related to proper nouns or nationalities, while the other\n8 types involve changing digits. We are mainly concerned\nwith the 10 types of concrete entities related to proper nouns\nor nationalities:EVENT,FAC,GPE,LANGUAGE,LOC,\nNORP,ORG,PERSON,PRODUCT,WORK OF ART.1\nFor CoNLL2003, we follow previous studies (Ma and\nHovy 2016) to divide its data into training, development, and\ntesting sets. For OntoNotes5.0, we split the data into train-\ning, development, and testing sets using the same method as\nPradhan et al. (2013). Table 1 summarizes the statistics of\nthe two datasets.\nState-of-the-Art BaselinesWe compare the performance\nof GPT4NER to five representative state-of-the-art models,\nincluding three few-shot models and two fully-supervised\nmodels.\nFew-shot baselines:\n•ProML(Chen, Zheng, and Yang 2023) designs multi-\nple prompt schemas to improve label semantics and in-\ntroduces a novel architecture to combine these prompt-\nbased representations. It targets tasks such as token set\nexpansion and domain transfer.\n•CONTaiNER(Das et al. 2022) is a contrastive learning\ntechnique for few-shot NER that optimizes inter-token\n1The excluded entity types areCARDINAL,DATE,LAW,MONEY,ORDINAL,\nPERCENT,QUANTITY, andTIME.\ndistribution distance using Gaussian-distributed embed-\ndings. This method enhances differentiation between to-\nken categories and alleviates overfitting from training do-\nmains.\n•PromptNER(Ashok and Lipton 2023) advances entity\nrecognition by integrating entity definitions in addition to\nfew-shot examples and prompts language models to pro-\nduce a list of potential entities along with corresponding\nexplanations.\nFully-supervised baselines:\n•MRC-NER+DSC(Li et al. 2020) employs dice loss in-\nstead of the standard cross-entropy objective for data-\nimbalanced NLP tasks. It uses a dynamic weight adjust-\nment strategy that modifies training example weights,\nemphasizing hard-negative examples and reducing the\nimpact of easy-negative ones. This model achieves state-\nof-the-art results on the OntoNotes5.0 dataset.\n•ACE+document-context(Wang et al. 2021) utilizes re-\ninforcement learning-based optimization with a novel re-\nward function to automatically find the optimal combi-\nnation of embeddings for structure prediction tasks. This\nmodel achieves state-of-the-art results on CoNLL2003.\nEvaluation MetricsLike previous studies (Wang et al.\n2025; Zhong, Cambria, and Hussain 2020; Li et al. 2020),\nwe report the evaluation performance of each model using\nthree standard metrics:Precision(Pre.),Recall(Rec.), and\nF1, under bothstrict matchandrelaxed match.\nPre.= TP\nTP+FP (1)\nRec.= TP\nTP+FN (2)\nF1 = 2×Pre.×Rec.\nPre.+Rec. (3)\nwhereTP(true-positive) denotes the number of targets that\nappear in both the ground-truth and the prediction,FP\n(false-positive) denotes the number of targets that are in\nthe prediction but not in the ground-truth, whileFN(false-\nnegative) denotes the number of targets that appear in the\nground-truth but not appear in prediction.\nStrict matchrefers to an exact match between the recog-\nnized entities and the ground-truth entities, whilerelaxed\nmatch(Verhagen et al. 2007, 2010; UzZaman et al. 2013;\nZhong, Sun, and Cambria 2017; Zhong and Cambria 2018;\nZhong, Cambria, and Hussain 2020; Zhong and Cambria\n2021) allows for some overlap between the recognized enti-\nties and the ground-truth entities.\nImplementation DetailsWe use the GPT-3.5 (gpt-3.5-\nturbo-instruct) model as our LLMs backbone for all our ex-\nperiments. This model supports a 4K-token context window.\nTo maximize the utility of this capacity, we set the maxi-\nmum output length to 400 tokens. Additionally, we set the\ntemperature parameter to 0 so as to ensure reproducibility.\nIn addition, we use an open-source LLM, Llama3-\n8B (Grattafiori et al. 2024) to compare. Also, we set the tem-\nperature parameter to 0 and the maximum output length to\n400 tokens.\nAll our experiments are conducted on a server equipped\nwith two Intel Xeon Gold 6240R CPUs (2.40GHz, 24 cores),\n251GB of memory, and two NVIDIA RTX A5000 GPUs\n(24GB VRAM), running CentOS Linux 7 (Core). The server\nenvironment includes CUDA 12.1 and Python 3.7.5.\nResults and Discussion\nWe evaluate the effectiveness of GPT4NER on two bench-\nmark datasets, CoNLL2003 (Sang and Meulder 2003) and\nOntoNotes5.0 (Pradhan et al. 2013), against five represen-\ntative state-of-the-art models. These include three few-shot\nmodels, namely ProML (Chen, Zheng, and Yang 2023),\nCONTaiNER (Das et al. 2022), and PromptNER (Ashok\nand Lipton 2023), and two fully-supervised models,\nMRC-NER+DSC (Li et al. 2020) and ACE+document-\ncontext (Wang et al. 2021).\nExperimental Results\nWe present experimental results on two tasks: (1) named en-\ntity recognition (NER), which aims to extract named enti-\nties from free text and then categorize them into predefined\ntypes, and (2) named entity extraction (NEE), which is also\nknown as entity boundary detection that aims to simply ex-\ntract named entities from free text without classifying them\ninto specific types.\nExperimental Results on Named Entity Recognition\nTable 2 presents the overall performance of GPT4NER and\nthe five baselines on the two benchmark datasets in the NER\ntask. For the three few-shot baselines, we include results re-\nported in their original papers as well as results reproduced\nin our study, marked with *. Compared to the three few-\nshot baselines, among the total 12 measures (i.e., 3 met-\nrics×2 match types×2 datasets), GPT4NER achieves\nthe best performance in 10 measures and second-best in\n12 measures, except forPre.andF 1 under relaxed match\non CoNLL2003. Specifically, GPT4NER achieves theF1 of\n83.15% under strict match on CoNLL2003 and 70.37% on\nOntoNotes5.0, surpassing few-shot baselines by at least 4.0\npoints and 7.1 points on the two datasets, respectively. Un-\nder relaxed match, GPT4NER achieves theF 1 of 83.68%\non OntoNotes5.0, outperforming few-shot baselines by at\nleast 9.5 points. On CoNLL2003, GPT4NER achieves the\nF1 of 85.63%, which is slightly below the best result of few-\nshot baselines (i.e., 86.65%). Compared to the two fully-\nsupervised baselines, GPT4NER achieves 87.9% of their\nbest performance on CoNLL2003 and 76.4% of their best\nperformance on OntoNotes5.0 in terms of theF1 under strict\nmatch. Compared to Llama3-8B model, GPT4NER outper-\nforms at least 12.8 points under strict match and 10.8 points\nunder relaxed match on CoNLL2003. On Ontonotes5.0,\nGPT4NER outperforms 25.5 points under strict match and\n29.6 points under relaxed match than Llama3-8B.\nGPT4NER vs. Few-Shot Baselines.Let’s compare\nGPT4NER to few-shot baselines. Table 2 illustrates that un-\nder strict match, GPT4NER significantly outperforms all\nthree few-shot baselines across all three metrics on both\ndatasets. Specifically, GPT4NER shows theF 1 improve-\nment of at least 3.99 points on CoNLL2003 and at least 7.13\nTable 2: Overall performance of GPT4NER and baselines innamed entity recognition (NER). Within each type of methods,\nthe best results are in bold and the second best are underlined. Results marked with * indicate our reproduction. Results of\nProML and CONTaiNER on Ontonotes5.0 are reported based on the average of three splits.\nDataset Method Strict Match Relaxed Match\nPre. Rec. F 1 Pre. Rec. F 1\nCoNLL2003\nBERT MRC+DSC 93.41 93.2593.33 - - -\nACE+document-context - -94.60- - -\nProML(1shot) - - 69.16 - - -\nProML(5shot) - - 79.16 - - -\nCONTaiNER(1shot) - - 57.80 - - -\nCONTaiNER(5shot) - - 72.80 - - -\nPromptNER - - 78.62 - - -\nProML(1shot)* 63.26 65.05 64.10 76.79 78.97 77.81\nProML(5shot)* 77.60 80.15 78.8485.2888.0886.65\nCONTaiNER(1shot)* 61.47 61.10 61.27 66.80 66.42 66.59\nCONTaiNER(5shot)* 72.42 74.89 73.62 77.91 81.58 79.21\nPromptNER* 66.68 70.13 68.36 69.71 73.32 71.47\nGPT4NER-Llama3 67.85 72.93 70.30 72.21 77.62 74.82\nGPT4NER(ours) 79.20 87.52 83.1581.56 90.1285.63\nGPT4NERw/o POS 78.24 86.05 81.96 80.96 89.04 84.81\nOntoNotes5.0\nBERT MRC+DSC 91.59 92.56 92.07- - -\nProML(1shot) - - 45.98 - - -\nProML(5shot) - - 63.24 - - -\nCONTaiNER(1shot) - - 32.00 - - -\nCONTaiNER(5shot) - - 56.20 - - -\nProML(1shot)* 36.53 51.59 42.74 52.42 74.17 61.39\nProML(5shot)* 50.21 64.46 56.42 65.91 84.80 74.13\nCONTaiNER(1shot)* 40.92 33.61 36.84 61.68 50.30 55.31\nCONTaiNER(5shot)* 54.49 53.64 54.06 73.47 72.45 72.95\nGPT4NER-Llama3 37.52 55.63 44.82 45.21 67.02 53.99\nGPT4NER(ours) 62.66 71.32 66.71 74.62 84.93 79.44\nGPT4NERw/o POS 67.15 73.92 70.37 79.85 87.90 83.68\npoints on OntoNotes5.0. This demonstrates GPT4NER’s su-\nperior ability to accurately generate named entities with pre-\ndefined types. Under relaxed match, GPT4NER also outper-\nforms all three few-shot baselines across all three metrics\non both datasets, with the exception of ProML (5-shot) on\nCoNLL2003 in terms ofPre.andRec.. Notably, GPT4NER\nachieves the highestRec.on both datasets, indicating its\nstrong capability to generate named entities with predefined\ntypes under lenient condition.\nGPT4NER vs. Fully-Supervised Baselines.The two\nfully-supervised baselines achieve state-of-the-art perfor-\nmance on both CoNLL2003 and OntoNotes5.0 by leverag-\ning large amounts of annotated training data. As shown in\nTable 2, GPT4NER trails behind the best performance of the\nfully-supervised baselines by 11.5 points on CoNLL2003\nand by 21.7 points on OntoNotes5.0. However, fully-\nsupervised baselines (Wang et al. 2021; Li et al. 2020) re-\nquire extensive annotated training data and perform poorly\nwith less training data. The performance of supervised mod-\nels increase with the training data (Wang et al. 2025). By\ncontrast, GPT4NER uses only a few labeled examples with\nminimal human effort in prompting LLMs, but still achieves\n87.9% of fully-supervised baselines’ best performance on\nCoNLL2003 and 76.4% on OntoNotes. This demonstrates\nthe potential of GPT4NER for few-shot NER, especially in\nlow-resource scenarios.\nStrict Match vs. Relaxed Match.Table 2 shows that all\nfew-shot models perform better under relaxed match com-\npared to strict match across all metrics and datasets. It shows\nthat for all four models, the scores under relaxed match are\nsignificantly higher than the corresponding ones under strict\nmatch across all three metrics on both datasets. To illus-\ntrate the usefulness of utilizing relaxed match in addition\nto strict match for evaluating performance, we define a met-\nric called “score improvement(SI)” as Eq. (4) to denote\nthe difference between the scores under relaxed match and\nunder strict match that are achieved by a model on a dataset:\nSI(m) =Relaxed(m)−Strict(m)(4)\nwhereRelaxeddenotes the score under relaxed match,\nStrictdenotes the score under strict match, andm∈\n{Pre., Rec., F1}. For example, GPT4NER achieves the\nTable 3: SI value of GPT4NER and baselines innamed en-\ntity recognition (NER). Within each type of methods, the\nsmallest results are in bold and the second smallest are un-\nderlined. Results marked with * indicate our reproduction.\nDataset Method SI\nPre. Rec. F 1\nCoNLL2003\nProML(1shot)* 13.53 13.92 13.71\nProML(5shot)* 7.68 7.93 7.81\nCONTaiNER(1shot)* 5.33 5.32 5.32\nCONTaiNER(5shot)* 5.49 6.69 5.59\nPromptNER* 3.03 3.19 3.11\nGPT4NER-Llama3 4.36 4.69 4.52\nGPT4NER(ours) 2.36 2.60 2.48\nGPT4NERw/o POS 2.72 2.99 2.85\nOntoNotes5.0\nProML(1shot)* 15.89 22.58 18.65\nProML(5shot)* 15.70 20.34 17.71\nCONTaiNER(1shot)* 20.76 16.69 18.47\nCONTaiNER(5shot)* 18.98 18.81 18.89\nGPT4NER-Llama3 7.69 11.39 9.17\nGPT4NER(ours) 11.96 13.61 12.73\nGPT4NERw/o POS 12.70 13.98 13.31\nSI(F1)of 2.48 points (i.e.,2.48 = 85.63−83.15) on\nCoNLL2003.\nAs shown in Table 3, the four few-shot models achieve\ntheSI(Pre.)of 2.36∼13.53 points, theSI(Rec.)of\n2.60∼13.92 points, and theSI(F 1)of 2.48∼13.71 points\non CoNLL2003. On OntoNotes5.0, theSI(Pre.)values are\n7.69∼20.76 points, theSI(Rec.)values are 11.39∼22.58\npoints, and theSI(F 1)values are 9.17∼18.89 points. These\nhighSI(·)values indicate that models may struggle to ex-\nactly recognize the boundaries of named entities but can\npartially recognize these named entities. Additionally, the\nSI(Pre.),SI(Rec.), andSI(F 1)values on OntoNotes5.0\nare significantly greater than the corresponding ones on\nCoNLL2003. This difference could be due to the more com-\nplex and diverse text in OntoNotes5.0, which includes more\nsyntactic and semantic variations. The few-shot models find\nit challenging to accurately recognize or generate the pre-\ncise boundaries of entities in such complex and diverse texts,\nleading to a noticeable performance difference between re-\nlaxed match and strict match.\nThese highSI(Pre.),SI(Rec.), andSI(F 1)values may\nbe attributed to the models’ recognition or generation capa-\nbilities. However, annotation inconsistencies could also be a\ncontributing factor. For example, within the same dataset,\nsomePER/PERSONentities may include prefix words\n(e.g., “Mr.” and “Dr.”), while others may exclude these pre-\nfixes. Furthermore, some loose recognitions of entity bound-\naries are acceptable.\nAs shown in the following two examples of GPT4NER\non OntoNotes5.0, the model predicts “Dick Cheney ’s” as a\nPERSONand “the Reporters ’ Committee for Freedom of\nthe Press” as aORG. The two predictions are slightly dif-\nferent from the corresponding ground-truth, and under strict\nmatch, they are considered wrong. However, under relaxed\nmatch, they are considered correct. This demonstrates that\nrelaxed match evaluates performance in a broader sense and\nprovides a more comprehensive assessment of the model,\nwhich is closer to real-world applications. Therefore, we\nconsider relaxed match a valuable metric, complementary\nto strict match, for evaluating model performance.\n•Test Text: In a separate first person account Miller con-\nfirmed that she told the grand jury that Scooter Libby\nDick Cheney ’s top aide discussed with her as many as\nthree times the role of Valerie Plame as a CIA employee\n.\nGold label: “Miller”: “PERSON”, “Scooter Libby”:\n“PERSON”, “Dick Cheney ’s”: “PERSON”, “Valerie\nPlame”: “PERSON”, “CIA”: “ORG”\nPrediction: “Miller”: “PERSON”, “Scooter Libby”:\n“PERSON”, “Dick Cheney”: “PERSON”, “Valerie\nPlame”: “PERSON”, “CIA”: “ORG”\n•Test Text: in Minneapolis Lucy Dalglish executive direc-\ntor of the Reporters ’ Committee for Freedom of the Press\n.\nGold label: “Minneapolis”: “GPE”, “Lucy Dalglish”:\n“PERSON”, “the Reporters ’ Committee for Free-\ndom of the Press”: “ORG”\nPrediction: “Minneapolis”: “GPE”, “Lucy Dalglish”:\n“PERSON”, “Reporters ’ Committee for Freedom of\nthe Press”: “ORG’\nExperimental Results on Named Entity ExtractionTa-\nble 4 reports the overall performance of GPT4NER and the\nfew-shot baselines on the two benchmark datasets in the\nNEE task. The results of the few-shot baselines are our re-\nproductions, marked with an asterisk (*). Among the to-\ntal 12 measures, GPT4NER achieves 10 best results and\n9 second-best ones, except forPre.andF 1 under relaxed\nmatch andPre.under strict match on CoNLL2003. Specifi-\ncally, GPT4NER attains theF1 of 88.12% under strict match\non CoNLL2003 and 74.12% on OntoNotes5.0, significantly\noutperforming the few-shot baselines by at least 3.1 points\non CoNLL2003 and at least 15.8 points on OntoNotes5.0.\nUnder relaxed match, GPT4NER achieves theF1 of 90.63%\non OntoNotes5.0, surpassing the few-shot baselines by at\nleast 12.1 points. On CoNLL2003, GPT4NER achieves the\nF1 of 92.52%, which is slightly lower than the best re-\nsult of the few-shot baselines (i.e., 94.32%). GPT4NER\noutperforms Llama3-8B by 8.7 points under strict match\nand 5.5 points under relaxed match on CoNLL2003. On\nOntonotes5.0, GPT4NER outperforms Llama3-8B by 24.8\npoints under strict match and 27.2 points under relaxed\nmatch. These results are consistent with those reported in\nTable 2, confirming the effectiveness and robustness of\nGPT4NER in few-shot NER and its sub-task.\nStrict Match vs. Relaxed Match.We utilize the score\nimprovement (SI) as defined by Eq. (4) to illustrate\nmodel performance in the NEE task. Table 5 shows\nthat the three few-shot models achieveSI(Pre.)values\nof 4.20∼16.89 points,SI(Rec.)values of 4.64∼17.39\npoints, andSI(F 1)values of 4.40∼17.13 points on\nCoNLL2003. On OntoNotes5.0, the few-shot models\nachieveSI(Pre.)values of 11.80∼24.10 points,SI(Rec.)\nTable 4: Performance of GPT4NER and few-shot baselines innamed entity extraction (NEE). The best results are highlighted\nin bold and the second best are underlined. Results marked with * indicate our reproduction.\nDataset Method Strict Match Relaxed Match\nPre. Rec. F 1 Pre. Rec. F 1\nCoNLL2003\nProML(1shot)* 72.21 74.21 73.14 89.10 91.60 90.27\nProML(5shot)* 83.09 85.82 84.42 92.83 95.8894.32\nCONTaiNER(1shot)* 82.20 81.81 81.9892.9792.57 92.75\nCONTaiNER(5shot)* 83.54 86.45 84.96 92.38 95.60 93.95\nGPT4NER-Llama3 76.66 82.38 79.42 83.97 90.24 86.99\nGPT4NER(ours) 83.93 92.74 88.1288.1397.3892.52\nGPT4NERw/o POS 82.58 90.83 86.51 87.69 96.44 91.85\nOntoNotes5.0\nProML(1shot)* 38.43 54.32 44.98 57.07 80.88 66.87\nProML(5shot)* 51.84 66.59 58.27 69.78 89.85 78.50\nCONTaiNER(1shot)* 43.04 35.32 38.73 67.14 54.68 60.16\nCONTaiNER(5shot)* 56.24 55.38 55.80 77.67 76.61 77.13\nGPT4NER-Llama3 41.27 61.16 49.28 53.07 78.65 63.38\nGPT4NER(ours) 66.67 75.88 70.98 82.04 93.38 87.34\nGPT4NERw/o POS 70.72 77.85 74.12 86.48 95.20 90.63\nTable 5: SI value of GPT4NER and baselines innamed en-\ntity extraction (NEE). Within each type of methods, the\nsmallest results are in bold and the second smallest are un-\nderlined. Results marked with * indicate our reproduction.\nDataset Method SI\nPre. Rec. F 1\nCoNLL2003\nProML(1shot)* 16.89 17.39 17.13\nProML(5shot)* 9.74 10.06 9.90\nCONTaiNER(1shot)* 10.77 10.76 10.77\nCONTaiNER(5shot)* 8.84 9.15 8.99\nGPT4NER-Llama3 7.31 7.86 7.57\nGPT4NER(ours) 4.20 4.64 4.40\nGPT4NERw/o POS 5.11 5.61 5.34\nOntoNotes5.0\nProML(1shot)* 18.64 26.56 21.89\nProML(5shot)* 17.94 23.26 20.23\nCONTaiNER(1shot)* 24.10 19.36 21.43\nCONTaiNER(5shot)* 21.43 21.23 21.33\nGPT4NER-Llama3 11.8017.49 14.10\nGPT4NER(ours) 15.37 17.50 16.36\nGPT4NERw/o POS 15.7617.3516.51\nvalues of 17.35∼26.56 points, andSI(F 1)values of\n14.10∼21.89 points. TheseSI(Pre.),SI(Rec.), and\nSI(F1)values are consistent with those in the NER task. 2\nThese highSI(Pre.),SI(Rec.), andSI(F 1)values con-\nfirm the usefulness and necessity of utilizing relaxed match\nas a complement to evaluate model performance in NER and\nits sub-task.\nNamed Entity Recognition vs. Named Entity Extrac-\ntion\nThe NER task consists of two sub-tasks: NEE and named\nentity classification. While previous studies primarily re-\n2In fact, theSI(Pre.),SI(Rec.), andSI(F 1)values in NEE are even higher\nthan those in NER.\nport the overall NER performance, we find that evaluating\nNEE performance separately can provide deeper insights\ninto model capabilities. As shown in Table 4, the strictF 1\nachieved by all few-shot models in the NEE sub-task are rel-\natively low, ranging from 73.14% to 88.12% on CoNLL2003\nand from 38.73% to 74.12% on OntoNotes5.0. This suggests\nthat the main factor contributing to low strict performance\nin NER is the low NEE performance, highlighting the need\nfor more focus on improving NEE. Under relaxed match, all\nfew-shot methods perform relatively well on CoNLL2003,\nwithF 1 ranging from 90.27% to 94.32%. However, they still\nperform relatively poorly on OntoNotes5.0, withF 1 rang-\ning from 60.16% to 90.63% in the NEE sub-task. This un-\nderscores the need for further improvements in NEE perfor-\nmance to enhance overall NER performance.\nTable 6 and Table 7 report detailed metrics—including\nprecision, recall,F 1, and counts—for each entity type in\nboth the NER and NEE tasks, comparing GPT4NER with\nand without the chain-of-thought module. On CoNLL2003,\nthe chain-of-thought helps particularly on complex or am-\nbiguous types such as ORG and MISC, where reasoning\nover definitions and examples may guide the model to more\nconsistent decisions. Notably, adding the chain-of-thought\noften increases the number of predicted entities (Pred col-\numn) across several types. While this sometimes leads to\nmodest gains in recall, the number of correct predictions\n(Correct column) does not always increase proportionally.\nAs a result, precision can decrease andF 1 may not improve\nsubstantially. On OntoNotes5.0, the chain-of-thought sim-\nilarly increases the number of predicted entities for many\ntypes (e.g., PERSON, NORP, WORK OF ART), but recall\ngains are limited and precision often drops, indicating that\nadditional reasoning may introduce spurious entities with-\nout substantially improving coverage. This suggests that the\nchain-of-thought can encourage the model to identify more\npotential entities, but the overall benefit depends on dataset\nTable 6: Detailed comparison of GPT4NER and its Chain-of-Thought ablation innamed entity recognition (NER), by entity\ntype, including precision, recall,F1 under strict match, and counts. Specifically, GPT4NER with POS tags serves as the baseline\nfor CoNLL2003, while GPT4NER without POS tags serves as the baseline for OntoNotes5.0.\nDataset Method Entity Type Strict Match Entity Count\nPre. Rec. F1 Gold Pred Correct\nCoNLL2003\nGPT4NER\nPER 93.52 94.56 94.03 1617 1635 1529\nLOC 88.44 90.35 89.38 1668 1704 1507\nORG 69.97 87.66 77.82 1661 2081 1456\nMISC 55.34 64.25 59.46 702 815 451\nw/o Chain-of-thought\nPER 94.77 94.12 94.45 1617 1606 1522\nLOC 70.53 94.96 80.94 1668 2246 1584\nORG 68.83 64.48 66.58 1661 1556 1071\nMISC 63.98 63.25 63.61 702 694 444\nOntoNotes5.0\nGPT4NER\nPERSON 76.20 74.90 75.55 1988 1954 1489\nORG 64.57 64.18 64.38 1795 1784 1152\nLOC 21.46 59.22 31.50 179 494 106\nNORP 64.79 78.12 70.84 841 1014 657\nGPE 90.82 84.33 87.45 2240 2080 1889\nFAC 28.46 27.41 27.92 135 130 37\nEVENT 20.31 41.27 27.23 63 128 26\nPRODUCT 17.93 68.42 28.42 76 290 52\nLANGUAGE 55.56 68.18 61.22 22 27 15\nWORK OF ART 36.44 75.30 49.12 166 343 125\nw/o Chain-of-thought\nPERSON 80.04 77.87 78.94 1988 1934 1548\nORG 65.60 65.13 65.36 1795 1782 1170\nLOC 25.60 59.22 35.75 179 414 106\nNORP 75.34 78.83 77.05 841 880 663\nGPE 91.21 84.82 87.90 2240 2083 1900\nFAC 26.88 37.04 31.15 135 186 50\nEVENT 19.61 31.75 24.24 63 102 20\nPRODUCT 29.71 68.42 41.43 76 175 52\nLANGUAGE 62.96 77.27 69.39 22 27 17\nWORK OF ART 47.79 71.69 57.35 166 249 119\ncomplexity, entity distribution, and context length.\nAblation StudyIn our ablation study, we analyze the im-\npact of each component in GPT4NER by systematically re-\nmoving them one at a time and observing the model perfor-\nmance on both NER and NEE tasks. The best-performing\nconfigurations serve as baselines for these experiments.\nSpecifically, GPT4NER with POS tags serves as the baseline\nfor CoNLL2003, while GPT4NER without POS tags serves\nas the baseline for OntoNotes5.0. The results of these abla-\ntion experiments for the NER task are presented in Table 8,\nand the results for the NEE task are reported in Table 9.\nImpact of Few-Shot Examples.Table 8 illustrates the\nsignificant impact of few-shot examples on the perfor-\nmance of GPT4NER in the NER task. When these exam-\nples are removed, theF 1 of GPT4NER drop substantially\nby 52.38 points under strict match and 41.90 points un-\nder relaxed match on CoNLL2003, and by 29.59 points un-\nder strict match and 34.50 points under relaxed match on\nOntoNotes5.0. Similarly, Table 9 shows that suchF1 in NEE\ndecrease by 45.63 points under strict match and 30.36 points\nunder relaxed match on CoNLL2003, and by 28.45 points\nunder strict match and 33.50 points under relaxed match on\nOntoNotes5.0. These notable decreases inF 1 across both\nmatches, tasks, and datasets underscore the critical role of\nfew-shot examples in the performance of GPT4NER.\nConversely, without few-shot examples, GPT4NER es-\nsentially operates as a zero-shot model. The results indicate\nthat the introduction of just few-shot examples with minimal\nhuman effort can lead to substantial performance improve-\nments in both NER and NEE tasks.\nFurthermore, despite the explicit specification of the out-\nput format in this experiment, the absence of the implicit\noutput format in the examples led to some generated results\nhaving correct content but incorrect format. This inconsis-\ntency affects the reliability of subsequent evaluation, as il-\nlustrated below:\n•Test Text: This is Xu Li .\nGold label: “Xu Li”: “PERSON”\nPrediction:\n1.Xu Li|True|Xu Li is a proper name, making it a\nTable 7: Detailed comparison of GPT4NER and its Chain-of-Thought ablation innamed entity extraction (NEE), by entity\ntype, including precision, recall,F1 under strict match, and counts. Specifically, GPT4NER with POS tags serves as the baseline\nfor CoNLL2003, while GPT4NER without POS tags serves as the baseline for OntoNotes5.0.\nDataset Method Entity Type Strict Match Entity Count\nPre. Rec. F1 Gold Pred Correct\nCoNLL2003\nGPT4NER\nPER 94.74 95.79 95.26 1617 1635 1549\nLOC 94.95 97.00 95.97 1668 1704 1618\nORG 72.32 90.61 80.44 1661 2081 1505\nMISC 69.45 80.63 74.62 702 815 566\nw/o Chain-of-thought\nPER 96.45 95.79 96.12 1617 1606 1549\nLOC 71.86 96.76 82.47 1668 2246 1614\nORG 99.16 92.90 95.93 1661 1556 1543\nMISC 79.97 79.06 79.51 702 694 555\nOntoNotes5.0\nGPT4NER\nPERSON 77.84 76.51 77.17 1988 1954 1521\nORG 69.67 69.25 69.46 1795 1784 1243\nLOC 22.87 63.13 33.58 179 494 113\nNORP 66.77 80.50 72.99 841 1014 677\nGPE 95.14 88.35 91.62 2240 2080 1979\nFAC 66.92 64.44 65.66 135 130 87\nEVENT 20.31 41.27 27.23 63 128 26\nPRODUCT 17.93 68.42 28.42 76 290 52\nLANGUAGE 62.96 77.27 69.39 22 27 17\nWORK OF ART 37.32 77.11 50.29 166 343 128\nw/o Chain-of-thought\nPERSON 81.44 79.23 80.32 1988 1934 1575\nORG 69.68 69.14 69.41 1795 1781 1241\nLOC 28.02 64.80 39.12 179 414 116\nNORP 77.73 81.33 79.49 841 880 684\nGPE 96.54 89.78 93.04 2240 2083 2011\nFAC 46.24 63.70 53.58 135 186 86\nEVENT 25.49 41.27 31.52 63 102 26\nPRODUCT 29.71 68.42 41.43 76 175 52\nLANGUAGE 70.37 86.36 77.55 22 27 19\nWORK OF ART 51.00 76.51 61.20 166 249 127\nPERSON entity.\n2. This|False|This is a pronoun and is excluded from\nentities.\n3. is|False|Is is a verb and is excluded from entities.\nIn this example, “Xu Li” is correctly identified and clas-\nsified, but the output does not follow format requirements,\nand the labeling fails in the subsequent processing.\nImpact of Entity Definitions.Table 8 reveals that remov-\ning entity definition from GPT4NER results in a slight de-\ncline inF 1 performance for the NER task: a decrease of\n1.10 points under strict match and 0.56 points under relaxed\nmatch on CoNLL2003, and a decrease of 2.74 points un-\nder strict match and 2.64 points under relaxed match on\nOntoNotes5.0. Similarly, Table 9 shows that theF 1 scores\nfor NEE decrease by 1.38 points under strict match and\n0.94 points under relaxed match on CoNLL2003, and by\n3.23 points under strict match and 1.82 points under relaxed\nmatch on OntoNotes5.0. These decreases indicate that en-\ntity definition is beneficial for both NER and NEE tasks\nin GPT4NER. However, theseF 1 decreases are relatively\nminor compared to the significant drops caused by remov-\ning few-shot examples. A possible reason is that LLMs like\nGPT-3.5 can infer full or partial entity definitions from the\ninput few-shot examples. This suggests that LLMs possess\nthe capability to deduce abstract concepts from specific in-\nstances.\nImpact of Chain-of-Thought.Table 8 shows that re-\nmoving the chain-of-thought component from GPT4NER\nleads to a decrease inF 1 performance in NER by 4.58\npoints under strict match and 4.89 points under relaxed\nmatch on CoNLL2003. Conversely, without the chain-of-\nthought component, GPT4NER’s performance increases by\n3.13 points under strict match and 2.52 points under re-\nlaxed match on OntoNotes5.0. Table 9 further indicates\nthat without the chain-of-thought component, GPT4NER\nachieves consistent increases inF1 performance for the NEE\ntask by 1.33 points under strict match and 0.89 points un-\nder relaxed match on CoNLL2003, and by 3.18 points un-\nder strict match and 2.08 points under relaxed match on\nOntoNotes5.0. These mixed results suggest that chain-of-\nTable 8: Ablation study in theNERtask.\nDataset Method Strict Match Relaxed Match\nPre. Rec. F 1 Pre. Rec. F 1\nCoNLL2003\nGPT4NER 79.20 87.52 83.15 81.56 90.12 85.63\nw/o Entity definition 77.64 86.99 82.05 80.50 90.19 85.07\nw/o Few-shot examples 23.12 46.09 30.79 32.83 65.46 43.73\nw/o Chain-of-thought 75.57 81.82 78.57 77.66 84.08 80.74\nOntoNotes5.0\nGPT4NER 67.15 73.92 70.37 79.85 87.90 83.68\nw/o Entity definition 63.48 72.35 67.63 76.07 86.70 81.04\nw/o Few-shot examples 36.76 45.80 40.78 44.33 55.23 49.18\nw/o Chain-of-thought 71.87 75.22 73.50 84.28 88.21 86.20\nTable 9: Ablation study on theNEEtask.\nDataset Method Strict Match Relaxed Match\nPre. Rec. F 1 Pre. Rec. F 1\nCoNLL2003\nGPT4NER 83.93 92.74 88.12 88.1397.3892.52\nw/o Entity definition 82.08 91.96 86.74 86.66 97.10 91.58\nw/o Few-shot examples 31.90 63.60 42.49 46.67 93.04 62.16\nw/o Chain-of-thought 86.03 93.15 89.45 89.8497.2793.41\nOntoNotes5.0\nGPT4NER 70.72 77.85 74.12 86.4895.2090.63\nw/o Entity definition 66.54 75.84 70.89 83.36 95.02 88.81\nw/o Few-shot examples 41.17 51.29 45.67 51.49 64.14 57.13\nw/o Chain-of-thought 75.58 79.11 77.30 90.6494.8792.71\nthought prompting can be both beneficial and detrimental\nfor few-shot models in NER and NEE tasks. A possible rea-\nson for this inconsistency is that chain-of-thought prompt-\ning might inadvertently accumulate errors. This implies that\nwhile chain-of-thought prompting has potential, its effective\ndesign remains challenging and is not always advantageous.\nThe Chain-of-Thought module is originally designed to im-\nprove the interpretability of the model, but this module re-\nquires the model to add a reason for determining the entity\ntype in the output, which really increases the output com-\nplexity of the model.\nImpact of Part-of-Speech Tags.Table 2 shows that re-\nmoving POS tags from GPT4NER leads to a decrease in\nF1 performance in NER by 1.1 points under strict match\nand 0.8 points under relaxed match on CoNLL2003. How-\never, it results in an increase of 3.6 points under strict match\nand 4.2 points under relaxed match on OntoNotes5.0. Simi-\nlarly, Table 4 reveals that in the NEE task, the performance\nof GPT4NER without POS tags decreases by 1.6 points\nunder strict match and 0.6 points under relaxed match on\nCoNLL2003, but increases by 3.1 points under strict match\nand 3.2 points under relaxed match on OntoNotes5.0. These\nresults indicate that POS tags are consistently beneficial\nfor CoNLL2003 across both NER and NEE tasks and both\nmatch metrics. Conversely, they are consistently detrimen-\ntal for OntoNotes5.0 across both tasks and match metrics. A\npossible explanation for this discrepancy is that in datasets\nlike CoNLL2003, where entity boundaries are clearer and\nTable 10: Distribution of POS tags in CoNLL2003 and\nOntoNotes5.0 datasets, shown as percentage of total enti-\nties.\nDataset POS tag Percent.\nCoNLL2003\nNN 18.69%\nFW 18.04%\nNNP 10.26%\nUH 9.88%\nCD 7.14%\nOntoNotes5.0\nNN 22.10%\nFW 19.56%\nUH 11.20%\nNNP 7.40%\nGW 6.24%\nsentence structures more regular, POS tags provide valu-\nable contextual information. By contrast, in datasets like\nOntoNotes5.0, where entity boundaries are more ambigu-\nous and sentence structures are more diverse and complex,\nPOS tags may introduce noise that negatively affects model\nperformance. Table 10 provides additional insight into the\nlinguistic differences between the datasets. CoNLL2003 is\ndominated by tags such as NN, NNP, and CD, which offer\nclear cues for entity recognition. OntoNotes5.0, by contrast,\nhas a higher proportion of NN and FW, with a wider va-\nriety of entity types and more complex sentence structures,\nreducing the effectiveness of POS information and occasion-\nally introducing noise. These observations suggest that POS\ntags can be beneficial for datasets with clearer entity bound-\naries and regular sentence structures, such as CoNLL2003,\nbut may not generalize to datasets with more ambiguous\nboundaries, complex syntax, or fine-grained labels, such as\nOntoNotes5.0. Careful consideration of dataset characteris-\ntics is thus recommended when incorporating POS features\nin few-shot NER and NEE tasks.\nError Analysis\nThere are three main types of errors in the evaluation of\nGPT4NER:\n(1)Post-Processing Errors. The addition of POS tags\noften leads to the omission of spaces around hyphens and\npossessive markers on OntoNotes5.0, making it difficult to\nlocate the corresponding phrases in test text for annotation,\nas illustrated below:\n•Test Text: Does the President still believe thatKim Jong\n- Ilis a tyrant a pygmy and a spoiled child .\nGold label: “Kim Jong - Il”: “PERSON”\nPrediction:Kim Jong-Il|True|as it is a person’s name\n(PERSON)\n•Test Text: in Minneapolis Lucy Dalglish executive direc-\ntor ofthe Reporters ’ Committee for Freedom of the\nPress.\nGold label: “Minneapolis”: “GPE”, “Lucy Dalglish”:\n“PERSON”, “the Reporters ’ Committee for Free-\ndom of the Press”: “ORG”\nPrediction:Reporters’ Committee for Freedom of the\nPress|True|as it is the name of an organization (ORG)\nWhen processing test texts with POS tags, LLMs tend to\nfocus on the POS of connectors. When generating output,\nthey often omit spaces around connectors to follow formal\nexpressions. However, this can introduce issues for subse-\nquent processing.\n(2)Hallucination Errors. GPT4NER occasionally re-\nturns entity types that are not included in the entity-\ndefinition component. This issue is particularly evident in\nablation experiments when few-shot examples are removed,\nwhich increases the likelihood of hallucinations in LLMs, as\nillustrated below:\n•Test Text: At present , we should not have a problem with\nwatching television .\nGold label: None\nPrediction: “present”: “TIME”, “problem”:\n“PROBLEM”, “television”: “PRODUCT”\n•Test Text: And let me go back to January of two thousand\ntwo in the President ’s axis of evil speech before congress\n.\nGold label: “congress”: “ORG”\nPrediction: “January”: “DATE”, “two thousand\ntwo”: “DATE”, “President”: “TITLE”, “axis of evil”:\n“PHRASE”, “congress”: “ORG”\nHallucinations often occur in experiments where POS\ntags are added or few-shot examples are removed. POS tags\nintroduce additional complexity, and without few-shot ex-\namples, the model’s learning becomes less robust.\n(3)Annotation Errors. These errors often stem from an-\nnotator oversight. Despite rigorous review and proofreading,\nminor mistakes are inevitable, as illustrated below:\n•Test Text: JAPAN GET LUCKY WIN,CHINAIN SUR-\nPRISE DEFEAT\nGold label: “JAPAN”: “LOC”, “CHINA”: “PER”\n•Test Text: Rumsfeld: The Iraqis received us with over-\nwhelming happiness and welcomed us, because of the\npractices of your bloody regime over the course of all\nthose years in which you governedIraq.\nGold label: “Rumsfeld”: “PERSON”, “Iraqis”:\n“NORP”, “Iraq”: “PERSON”\nObviously, “CHINA” and “Iraq” refer to countries or lo-\ncations instead of persons.\nLimitations\nThere are two primary limitations in our work. The first con-\ncerns interpretability. Depending solely on ChatGPT’s rea-\nsoning within candidate entities may not provide sufficient\ninterpretability. LLMs such as ChatGPT introduce uncer-\ntainty in their generated output, and explanations provided\nin the results may not always be accurate or reliable. The\nsecond limitation pertains to token constraints. While using\nlarger models like GPT-4 and GPT-5 may enhance perfor-\nmance, this is not our main focus.\nConclusion\nThis paper introduces GPT4NER, a method based on LLMs\nfor few-shot NER. GPT4NER constructs effective prompts\nusing entity definition, few-shot examples, chain-of-thought,\nand POS tags to leverage the capabilities of LLMs in trans-\nforming the few-shot NER task into a sequence-generation\ntask. Experimental results on two benchmark datasets\ndemonstrate that GPT4NER significantly outperforms state-\nof-the-art few-shot models and achieves competitive results\ncompared to fully-supervised models. Furthermore, our ex-\nperiments advocate for the use of the relaxed-match met-\nric (which is widely used in time expression recognition\nand normalization) to evaluate model performance. Addi-\ntionally, our experiments also suggest to report performance\nin the NEE sub-task to deepen insights into model capabili-\nties in the NER task.\nReferences\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;\nAleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;\nAnadkat, S.; et al. 2023. Gpt-4 technical report.arXiv\npreprint arXiv:2303.08774.\nAshok, D.; and Lipton, Z. C. 2023. PromptNER: Prompting\nFor Named Entity Recognition. arXiv:2305.15444.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language models\nare few-shot learners. InProceedings of the 34th Interna-\ntional Conference on Neural Information Processing Sys-\ntems, NIPS ’20. Red Hook, NY , USA: Curran Associates\nInc. ISBN 9781713829546.\nChen, J.; Lu, Y .; Lin, H.; Lou, J.; Jia, W.; Dai, D.; Wu,\nH.; Cao, B.; Han, X.; and Sun, L. 2023. Learning In-\ncontext Learning for Named Entity Recognition. In Rogers,\nA.; Boyd-Graber, J.; and Okazaki, N., eds.,Proceedings of\nthe 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 13661–13675.\nToronto, Canada: Association for Computational Linguis-\ntics.\nChen, Y .; Zheng, Y .; and Yang, Z. 2023. Prompt-Based\nMetric Learning for Few-Shot NER. In Rogers, A.; Boyd-\nGraber, J.; and Okazaki, N., eds.,Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, 7199–7212.\nToronto, Canada: Association for Computational Linguis-\ntics.\nChinchor, N.; and Robinson, P. 1997. MUC-7 named entity\ntask definition. InProceedings of the 7th Conference on\nMessage Understanding, volume 29, 1–21.\nChiu, J. P.; and Nichols, E. 2016. Named Entity Recogni-\ntion with Bidirectional LSTM-CNNs.Transactions of the\nAssociation for Computational Linguistics, 4: 357–370.\nChowdhury, S.; Dong, X.; Qian, L.; Li, X.; Guan, Y .; Yang,\nJ.; and Yu, Q. 2018. A multitask bi-directional RNN model\nfor named entity recognition on Chinese electronic medical\nrecords.BMC bioinformatics, 19(Suppl 17): 499.\nCollobert, R.; Weston, J.; Bottou, L.; Karlen, M.;\nKavukcuoglu, K.; and Kuksa, P. 2011. Natural Language\nProcessing (Almost) from Scratch.J. Mach. Learn. Res.,\n12(null): 2493–2537.\nDagdelen, J.; Dunn, A.; Lee, S.; Walker, N.; Rosen, A. S.;\nCeder, G.; Persson, K. A.; and Jain, A. 2024. Structured in-\nformation extraction from scientific text with large language\nmodels.Nature communications, 15(1): 1418.\nDas, S. S. S.; Katiyar, A.; Passonneau, R.; and Zhang, R.\n2022. CONTaiNER: Few-Shot Named Entity Recogni-\ntion via Contrastive Learning. In Muresan, S.; Nakov, P.;\nand Villavicencio, A., eds.,Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 6338–6353. Dublin, Ireland: As-\nsociation for Computational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds.,Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), 4171–4186. Minneapolis, Min-\nnesota: Association for Computational Linguistics.\nDing, B.; Liu, L.; Bing, L.; Kruengkrai, C.; Nguyen, T. H.;\nJoty, S.; Si, L.; and Miao, C. 2020. DAGA: Data Augmenta-\ntion with a Generation Approach for Low-resource Tagging\nTasks. In Webber, B.; Cohn, T.; He, Y .; and Liu, Y ., eds.,\nProceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), 6045–6057. On-\nline: Association for Computational Linguistics.\nEgorov, S.; Yuryev, A.; and Daraselia, N. 2004. A Simple\nand Practical Dictionary-based Approach for Identification\nof Proteins in Medline Abstracts.Journal of the American\nMedical Informatics Association, 11(3): 174–178.\nGhosh, S.; Tyagi, U.; Kumar, S.; and Manocha, D. 2023.\nBioAug: Conditional Generation based Data Augmenta-\ntion for Low-Resource Biomedical NER. InProceedings\nof the 46th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval, SIGIR\n’23, 1853–1858. New York, NY , USA: Association for Com-\nputing Machinery. ISBN 9781450394086.\nGrattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian,\nA.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.;\nVaughan, A.; et al. 2024. The llama 3 herd of models.arXiv\npreprint arXiv:2407.21783.\nGuo, Q.; Dong, Y .; Tian, L.; Kang, Z.; Zhang, Y .; and Wang,\nS. 2025. BANER: Boundary-Aware LLMs for Few-Shot\nNamed Entity Recognition. In Rambow, O.; Wanner, L.;\nApidianaki, M.; Al-Khalifa, H.; Eugenio, B. D.; and Schock-\naert, S., eds.,Proceedings of the 31st International Con-\nference on Computational Linguistics, 10375–10389. Abu\nDhabi, UAE: Association for Computational Linguistics.\nHanisch, D.; Fundel, K.; Mevissen, H.-T.; Zimmer, R.; and\nFluck, J. 2005. ProMiner: rule-based protein and gene entity\nrecognition.BMC bioinformatics, 6(Suppl 1): S14.\nHu, X.; Jiang, Y .; Liu, A.; Huang, Z.; Xie, P.; Huang, F.;\nWen, L.; and Yu, P. S. 2023. Entity-to-Text based Data Aug-\nmentation for various Named Entity Recognition Tasks. In\nRogers, A.; Boyd-Graber, J.; and Okazaki, N., eds.,Find-\nings of the Association for Computational Linguistics: ACL\n2023, 9072–9087. Toronto, Canada: Association for Com-\nputational Linguistics.\nHu, Y .; Chen, Q.; Du, J.; Peng, X.; Keloth, V . K.; Zuo, X.;\nZhou, Y .; Li, Z.; Jiang, X.; Lu, Z.; Roberts, K.; and Xu, H.\n2024. Improving large language models for clinical named\nentity recognition via prompt engineering.Journal of the\nAmerican Medical Informatics Association, 31(9): 1812–\n1820.\nHuang, Y .; He, K.; Wang, Y .; Zhang, X.; Gong, T.; Mao,\nR.; and Li, C. 2022. COPNER: Contrastive Learning with\nPrompt Guiding for Few-shot Named Entity Recognition. In\nCalzolari, N.; Huang, C.-R.; Kim, H.; Pustejovsky, J.; Wan-\nner, L.; Choi, K.-S.; Ryu, P.-M.; Chen, H.-H.; Donatelli, L.;\nJi, H.; Kurohashi, S.; Paggio, P.; Xue, N.; Kim, S.; Hahm, Y .;\nHe, Z.; Lee, T. K.; Santus, E.; Bond, F.; and Na, S.-H., eds.,\nProceedings of the 29th International Conference on Com-\nputational Linguistics, 2515–2527. Gyeongju, Republic of\nKorea: International Committee on Computational Linguis-\ntics.\nLayegh, A.; Payberah, A. H.; Soylu, A.; Roman, D.; and\nMatskin, M. 2023. ContrastNER: Contrastive-based Prompt\nTuning for Few-shot NER. In2023 IEEE 47th Annual Com-\nputers, Software, and Applications Conference (COMP-\nSAC), 241–249.\nLi, J.; Fei, H.; Liu, J.; Wu, S.; Zhang, M.; Teng, C.; Ji, D.;\nand Li, F. 2022. Unified Named Entity Recognition as Word-\nWord Relation Classification.Proceedings of the AAAI Con-\nference on Artificial Intelligence, 36(10): 10965–10973.\nLi, L.; Zhou, R.; and Huang, D. 2009. Two-phase biomed-\nical named entity recognition using CRFs.Computational\nBiology and Chemistry, 33(4): 334–338.\nLi, X.; Sun, X.; Meng, Y .; Liang, J.; Wu, F.; and Li, J. 2020.\nDice Loss for Data-imbalanced NLP Tasks. In Jurafsky, D.;\nChai, J.; Schluter, N.; and Tetreault, J., eds.,Proceedings of\nthe 58th Annual Meeting of the Association for Computa-\ntional Linguistics, 465–476. Online: Association for Com-\nputational Linguistics.\nLu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-\nC.; Tafjord, O.; Clark, P.; and Kalyan, A. 2022. Learn to\nexplain: multimodal reasoning via thought chains for sci-\nence question answering. InProceedings of the 36th Inter-\nnational Conference on Neural Information Processing Sys-\ntems, NIPS ’22. Red Hook, NY , USA: Curran Associates\nInc. ISBN 9781713871088.\nLyu, C.; Chen, B.; Ren, Y .; and Ji, D. 2017. Long short-\nterm memory RNN for biomedical named entity recogni-\ntion.BMC bioinformatics, 18(1): 462.\nMa, X.; and Hovy, E. 2016. End-to-end Sequence Label-\ning via Bi-directional LSTM-CNNs-CRF. In Erk, K.; and\nSmith, N. A., eds.,Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), 1064–1074. Berlin, Germany: Association for\nComputational Linguistics.\nMorwal, S.; Jahan, N.; and Chopra, D. 2012. Named en-\ntity recognition using hidden Markov model (HMM).Inter-\nnational Journal on Natural Language Computing (IJNLC)\nVol, 1.\nPradhan, S.; Moschitti, A.; Xue, N.; Ng, H. T.; Bj ¨orkelund,\nA.; Uryupina, O.; Zhang, Y .; and Zhong, Z. 2013. Towards\nRobust Linguistic Analysis using OntoNotes. In Hock-\nenmaier, J.; and Riedel, S., eds.,Proceedings of the Sev-\nenteenth Conference on Computational Natural Language\nLearning, 143–152. Sofia, Bulgaria: Association for Com-\nputational Linguistics.\nRiaz, K. 2010. Rule-based named entity recognition in\nUrdu. InProceedings of the 2010 Named Entities Workshop,\nNEWS ’10, 126–135. USA: Association for Computational\nLinguistics. ISBN 9781932432787.\nSang, E. F. T. K.; and Meulder, F. D. 2003. Introduction\nto the CoNLL-2003 Shared Task: Language-Independent\nNamed Entity Recognition. arXiv:cs/0306050.\nSasaki, Y .; Tsuruoka, Y .; McNaught, J.; and Ananiadou, S.\n2008. How to make the most of NE dictionaries in statistical\nNER.BMC bioinformatics, 9(Suppl 11): S5.\nShao, W.; Zhang, R.; Ji, P.; Fan, D.; Hu, Y .; Yan, X.; Cui, C.;\nTao, Y .; Mi, L.; and Chen, L. 2024. Astronomical Knowl-\nedge Entity Extraction in Astrophysics Journal Articles via\nLarge Language Models.Research in Astronomy and Astro-\nphysics, 24(6): 065012.\nUzZaman, N.; Llorens, H.; Derczynski, L.; Allen, J.; Ver-\nhagen, M.; and Pustejovsky, J. 2013. SemEval-2013 Task\n1: TempEval-3: Evaluating Time Expressions, Events, and\nTemporal Relations. In Manandhar, S.; and Yuret, D., eds.,\nSecond Joint Conference on Lexical and Computational\nSemantics (*SEM), Volume 2: Proceedings of the Seventh\nInternational Workshop on Semantic Evaluation (SemEval\n2013), 1–9. Atlanta, Georgia, USA: Association for Com-\nputational Linguistics.\nVerhagen, M.; Gaizauskas, R.; Schilder, F.; Hepple, M.;\nKatz, G.; and Pustejovsky, J. 2007. SemEval-2007 Task\n15: TempEval Temporal Relation Identification. In Agirre,\nE.; M`arquez, L.; and Wicentowski, R., eds.,Proceedings of\nthe Fourth International Workshop on Semantic Evaluations\n(SemEval-2007), 75–80. Prague, Czech Republic: Associa-\ntion for Computational Linguistics.\nVerhagen, M.; Saur ´ı, R.; Caselli, T.; and Pustejovsky, J.\n2010. SemEval-2010 Task 13: TempEval-2. In Erk, K.;\nand Strapparava, C., eds.,Proceedings of the 5th Interna-\ntional Workshop on Semantic Evaluation, 57–62. Uppsala,\nSweden: Association for Computational Linguistics.\nWadhwa, S.; Amir, S.; and Wallace, B. 2023. Revisiting\nRelation Extraction in the era of Large Language Mod-\nels. In Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds.,\nProceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n15566–15589. Toronto, Canada: Association for Computa-\ntional Linguistics.\nWang, C.; Liu, X.; Chen, Z.; Hong, H.; Tang, J.; and Song,\nD. 2022a. DeepStruct: Pretraining of Language Models for\nStructure Prediction. In Muresan, S.; Nakov, P.; and Villav-\nicencio, A., eds.,Findings of the Association for Compu-\ntational Linguistics: ACL 2022, 803–823. Dublin, Ireland:\nAssociation for Computational Linguistics.\nWang, S.; Sun, X.; Li, X.; Ouyang, R.; Wu, F.; Zhang, T.; Li,\nJ.; Wang, G.; and Guo, C. 2025. GPT-NER: Named Entity\nRecognition via Large Language Models. In Chiruzzo, L.;\nRitter, A.; and Wang, L., eds.,Findings of the Association\nfor Computational Linguistics: NAACL 2025, 4257–4275.\nAlbuquerque, New Mexico: Association for Computational\nLinguistics. ISBN 979-8-89176-195-7.\nWang, X.; Jiang, Y .; Bach, N.; Wang, T.; Huang, Z.; Huang,\nF.; and Tu, K. 2021. Automated Concatenation of Embed-\ndings for Structured Prediction. In Zong, C.; Xia, F.; Li, W.;\nand Navigli, R., eds.,Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), 2643–2660. Online:\nAssociation for Computational Linguistics.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2022b. Self-consistency\nimproves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E. H.; Le, Q. V .; and Zhou, D. 2022. Chain-\nof-thought prompting elicits reasoning in large language\nmodels. InProceedings of the 36th International Con-\nference on Neural Information Processing Systems, NIPS\n’22. Red Hook, NY , USA: Curran Associates Inc. ISBN\n9781713871088.\nXie, T.; Li, Q.; Zhang, J.; Zhang, Y .; Liu, Z.; and Wang, H.\n2023. Empirical Study of Zero-Shot NER with ChatGPT.\nIn Bouamor, H.; Pino, J.; and Bali, K., eds.,Proceedings\nof the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, 7935–7956. Singapore: Association\nfor Computational Linguistics.\nXu, Z.; Qian, X.; Zhang, Y .; and Zhou, Y . 2008. CRF-based\nhybrid model for word segmentation, NER and even POS\ntagging. InProceedings of the sixth SIGHAN workshop on\nChinese language processing.\nYang, Y .; and Katiyar, A. 2020. Simple and Effective Few-\nShot Named Entity Recognition with Structured Nearest\nNeighbor Learning. In Webber, B.; Cohn, T.; He, Y .; and\nLiu, Y ., eds.,Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing (EMNLP),\n6365–6375. Online: Association for Computational Linguis-\ntics.\nYe, J.; Xu, N.; Wang, Y .; Zhou, J.; Zhang, Q.; Gui, T.; and\nHuang, X. 2024. LLM-DA: Data Augmentation via Large\nLanguage Models for Few-Shot Named Entity Recognition.\narXiv:2402.14568.\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Auto-\nmatic Chain of Thought Prompting in Large Language Mod-\nels. arXiv:2210.03493.\nZhao, D.; Mu, W.; Jia, X.; Liu, S.; Chu, Y .; Meng, J.; and\nLin, H. 2025. Few-shot biomedical NER empowered by\nLLMs-assisted data augmentation and multi-scale feature\nextraction.BioData Mining, 18(1): 28.\nZhao, S. 2004. Named entity recognition in biomedical\ntexts using an HMM model. InProceedings of the Inter-\nnational Joint Workshop on Natural Language Processing\nin Biomedicine and Its Applications, JNLPBA ’04, 84–87.\nUSA: Association for Computational Linguistics.\nZhong, X.; and Cambria, E. 2018. Time Expression Recog-\nnition Using a Constituent-based Tagging Scheme. InPro-\nceedings of the 2018 World Wide Web Conference, WWW\n’18, 983–992. Republic and Canton of Geneva, CHE: Inter-\nnational World Wide Web Conferences Steering Committee.\nISBN 9781450356398.\nZhong, X.; and Cambria, E. 2021.Time expression and\nnamed entity recognition, volume 10. Springer.\nZhong, X.; Cambria, E.; and Hussain, A. 2020. Extracting\ntime expressions and named entities with constituent-based\ntagging schemes.Cognitive Computation, 12(4): 844–862.\nZhong, X.; Sun, A.; and Cambria, E. 2017. Time Expression\nAnalysis and Recognition Using Syntactic Token Types and\nGeneral Heuristic Rules. In Barzilay, R.; and Kan, M.-Y .,\neds.,Proceedings of the 55th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Pa-\npers), 420–429. Vancouver, Canada: Association for Com-\nputational Linguistics.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; and Chi,\nE. 2023. Least-to-Most Prompting Enables Complex Rea-\nsoning in Large Language Models. arXiv:2205.10625.\nZhou, W.; Zhang, S.; Gu, Y .; Chen, M.; and Poon, H.\n2024. UniversalNER: Targeted Distillation from Large\nLanguage Models for Open Named Entity Recognition.\narXiv:2308.03279.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.49994754791259766
    },
    {
      "name": "Natural language processing",
      "score": 0.44731953740119934
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.4369519054889679
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3336583971977234
    },
    {
      "name": "Chromatography",
      "score": 0.09700953960418701
    },
    {
      "name": "Chemistry",
      "score": 0.08258837461471558
    }
  ]
}