{
    "title": "Spanish Language Models.",
    "url": "https://openalex.org/W3180918883",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3166085439",
            "name": "Asier Gutiérrez-Fandiño",
            "affiliations": [
                "Barcelona Supercomputing Center",
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A2979305992",
            "name": "Jordi Armengol-Estapé",
            "affiliations": [
                "Barcelona Supercomputing Center",
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A3046976067",
            "name": "Marc Pàmies",
            "affiliations": [
                "Universitat Politècnica de Catalunya",
                "Barcelona Supercomputing Center"
            ]
        },
        {
            "id": "https://openalex.org/A2980472904",
            "name": "Joan Llop Palao",
            "affiliations": [
                "Barcelona Supercomputing Center",
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A4303316100",
            "name": "Joaquín Silveira-Ocampo",
            "affiliations": [
                "Universitat Politècnica de Catalunya",
                "Barcelona Supercomputing Center"
            ]
        },
        {
            "id": "https://openalex.org/A2970055708",
            "name": "Casimiro Pio Carrino",
            "affiliations": [
                "Universitat Politècnica de Catalunya",
                "Barcelona Supercomputing Center"
            ]
        },
        {
            "id": "https://openalex.org/A2731970290",
            "name": "Aitor Gonzalez-Agirre",
            "affiliations": [
                "Universitat Politècnica de Catalunya",
                "Barcelona Supercomputing Center"
            ]
        },
        {
            "id": "https://openalex.org/A2394874375",
            "name": "Carme Armentano Oller",
            "affiliations": [
                "Universitat Politècnica de Catalunya",
                "Barcelona Supercomputing Center"
            ]
        },
        {
            "id": "https://openalex.org/A5030236932",
            "name": "Carlos Gerardo Rodriguez-Penagos",
            "affiliations": [
                "Universitat Politècnica de Catalunya",
                "Barcelona Supercomputing Center"
            ]
        },
        {
            "id": "https://openalex.org/A2296216261",
            "name": "Marta Villegas",
            "affiliations": [
                "Barcelona Supercomputing Center",
                "Universitat Politècnica de Catalunya"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3175567752",
        "https://openalex.org/W3009095382",
        "https://openalex.org/W3095645723",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2126400076",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2150102617",
        "https://openalex.org/W2133458109",
        "https://openalex.org/W2915429162",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W38462120",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2952060378",
        "https://openalex.org/W2970752815",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2995647371",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W2952566282"
    ],
    "abstract": "This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as well as the corresponding performance evaluations. Both models were pre-trained using the largest Spanish corpus known to date, with a total of 570GB of clean and deduplicated text processed for this work, compiled from the web crawlings performed by the National Library of Spain from 2009 to 2019. We extended the current evaluation datasets with an extractive Question Answering dataset and our models outperform the existing Spanish models across tasks and settings.",
    "full_text": "MarIA: Spanish Language Models\nMarIA: Modelos del Lenguaje en Español\nAsier Gutiérrez-Fandiño∗,1 Jordi Armengol-Estapé∗,1 Marc Pàmies,1\nJoan Llop-Palao,1 Joaquín Silveira-Ocampo,1 Casimiro Pio Carrino,1\nCarme Armentano-Oller,1 Carlos Rodriguez-Penagos,1\nAitor Gonzalez-Agirre,1 Marta Villegas1\n1Barcelona Supercomputing Center\nmarta.villegas@bsc.es\nAbstract: This work presents MarIA, a family of Spanish language models and\nassociated resources made available to the industry and the research community.\nCurrently, MarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large\nSpanish language models, which can arguably be presented as the largest and most\nproﬁcient language models in Spanish. The models were pretrained using a massive\ncorpus of 570GB of clean and deduplicated texts with 135 billion words extracted\nfrom the Spanish Web Archive crawled by the National Library of Spain between 2009\nand 2019. We assessed the performance of the models with nine existing evaluation\ndatasets and with a novel extractive Question Answering dataset created ex novo.\nOverall, MarIA models outperform the existing Spanish models across a variety of\nNLU tasks and training settings.\nKeywords: MarIA, Spanish language modelling, Spanish language resources, Bench-\nmarking.\nResumen: En este artículo se presenta MarIA, una familia de modelos del lenguaje\nen español y sus correspondientes recursos que se hacen públicos para la industria y la\ncomunidad cientíﬁca. Actualmente MarIA incluye los modelos del lenguaje en español\nRoBERTa-base, RoBERTa-large, GPT2 y GPT2-large que pueden considerarse como\nlos modelos más grandes y mejores para español. Los modelos han sido preentrena-\ndos utilizando un corpus masivo de 570GB de textos limpios y deduplicados, que\ncomprende un total de 135 mil millones de palabras extraidas del Archivo Web del\nEspañol construido por la Biblioteca Nacional de España entre los años 2009 y 2019.\nEvaluamos el rendimiento de los modelos con nueve conjuntos de datos existentes y\ncon un nuevo conjunto de datos de pregunta-respuesta extractivo creado ex novo. El\nconjunto de modelos de MarIA supera, en la practica totalidad, el rendimiento de los\nmodelos existentes en español en las diferentes tareas y conﬁguraciones presentadas.\nPalabras clave:MarIA, Modelos de lenguaje del Español, Recursos de lenguaje del\nEspañol, Evaluación de modelos del lenguaje.\n1 Introduction\nIn recent years, the ﬁeld of Natural Lan-\nguage Processing (NLP) has seen a prolifer-\nation of massive pretrained language models.\nThese have been proved to perform best when\ntrained on language-speciﬁc data. However,\nthe vast majority of these massive models have\nbeen trained for English, leaving other lan-\nguages aside and increasing the existing gap\nbetween them. Spanish, despite being the sec-\nond most spoken language in the world, lacks\nlarge language models trained with vast and\n∗ Equal contribution.\nhigh quality data. One of the objectives of the\nPlan-TL1 is to cover this gap with the MarIA\nproject.2 MarIA aims to provide both the in-\ndustry and the scientiﬁc community with large\nscale language models, massive high-quality\ncorpora and evaluation sets for the Spanish\nlanguage. We present four large models of\nvarying sizes and conﬁgurations, and compare\nthem to existing models in a wide range of\nNLP tasks, showing that these new models\n1https://plantl.mineco.gob.es/\n2https://github.com/PlanTL-GOB-ES/lm-\nspanish\narXiv:2107.07253v5  [cs.CL]  5 Apr 2022\nare able to generalize better overall.\nThe aim of this paper is to present an ex-\nhaustive report of all the work performed in\nthe context of the MarIA project, which in-\ncludes:\n• Processing of the largestclean Spanish\ncorpus to date, obtained from the web\ncrawlings performed by the National Li-\nbrary of Spain from 2009 to 2019, used\nto\n• Train RoBERTa-base and RoBERTa-\nlarge models (Liu et al., 2019), and\n• Train GPT2 and GPT2-large models\n(Radford et al., 2019b).\n• Creation of SQAC, a newly produced\ndataset for Spanish Question Answering.\n• Conduction of a complete evaluation on\na diverse set of tasks.\n• Release of all pre-trained and ﬁne-\ntuned models inhttps://huggingface.\nco/PlanTL-GOB-ES/\nThe remainder of this paper is organized\nas follows. In Section 2, we brieﬂy go through\nthe previous work done in language modeling,\nfocusing on Spanish. In Section 3, we describe\nthe datasets used in the model training and in\nthe subsequent evaluation. We devote special\nattention to the description of the training\ncorpus and the new data set, expressly gen-\nerated, on Question Answering. In Section\n4 and 5 we describe the new RoBERTa and\nGPT2 models and report in detail the eval-\nuation methodology used and the eventual\nresults. Finally, we present our conclusions\nand suggestions for future work in Section 6.\n2 Related Work\nUnsupervised pretraining started with the\ntask of language modeling (Bengio, Ducharme,\nand Vincent, 2000), where neural networks\nwere trained to predict the next word from\na given sequence, creating ﬁxed vector repre-\nsentations known as word embeddings. Trans-\nfer learning capabilities of word embeddings\ntook oﬀ with the introduction of Word2Vec\n(Mikolov et al., 2013), GloVe (Pennington,\nSocher, and Manning, 2014) and FastText\n(Bojanowski et al., 2016). For Spanish, re-\nsearchers built datasets (Cardellino, 2019;\nBañón et al., 2020; Carrino et al., 2021;\nCañete, 2019) and computed word representa-\ntions (Almeida and Bilbao, 2018; Bilbao-Jayo\nand Almeida, 2018; Gutiérrez-Fandiño et al.,\n2021a; Gutiérrez-Fandiño et al., 2021b) using\nthose algorithms.\nLater on, researchers scaled up this unsu-\npervised pretraining to larger datasets and\nmore expressive models, speciﬁcally with lan-\nguage models, originally with LSTM-based\n(Hochreiter and Schmidhuber, 1997) models\n(Peters et al., 2018). Nowadays, they are typi-\ncally based on the Transformer architecture\n(Vaswani et al., 2017), with BERT (Devlin et\nal., 2018) as the paradigmatic example in the\ncase of encoder models and the GPT family\n(Radford et al., 2018; Radford et al., 2019a;\nBrown et al., 2020b) in the case of the decoder\nones.\nWhile the ﬁrst models were either English-\nonly or multilingual (Devlin et al., 2018), re-\nsearchers soon realized that building language-\nspeciﬁc models was worth the eﬀort (Martin\net al., 2019; Le et al., 2019; Virtanen et al.,\n2019; Nguyen and Nguyen, 2020; de Vries et\nal., 2019; Cui et al., 2021), provided there was\nenough data available. The language-speciﬁc\nliterature with respect to language model-\ning has been quite proliﬁc ever since (Nozza,\nBianchi, and Hovy, 2020). In the case of Span-\nish, the ﬁrst BERT-based model was BETO\n(Cañete, 2019), whichoutperformedthestrong\nmultilingual baseline of mBERT.3 BETO was\ntrained on a collection of existing corpora, in-\ncluding the OPUS corpus (Tiedemann, 2012)\nand the Spanish portion of Wikipedia. After\nthe release of BETO, a few other models were\npublished among which stands BERTIN4, a\nseries of Transformer-based models trained on\nthe Spanish portion of the mC4 dataset (Xue\net al., 2020).\nInspired by previous work carried out\nfor diﬀerent languages, we processed a new\ndataset and developed both new encoder and\ndecoder models for Spanish. As for encoders,\nwe opted for the RoBERTa architecture (Liu\net al., 2019), an optimized version of BERT,\nand in the case of the decoders, we chose\nGPT2 (Radford et al., 2019a). Further details\nare provided in the following sections.\n3 Data\nThis section describes the corpus used to\npretrain the language models as well as the\ndatasets used to evaluate them.\n3The multiligual version of BERT.\n4https://huggingface.co/bertin-project/\nbertin-roberta-base-spanish/tree/v1-512\n3.1 Pretraining corpus\nThe National Library of Spain (Biblioteca Na-\ncional de España or BNE5) performs a crawl-\ning of all.es domains once a year. Besides\nthis massive crawl, the library performs selec-\ntive crawls that can be classiﬁed into three\ncategories: themed based (this includes 15\ndiﬀerent thematic collections, from ﬁne arts\nto universities, feminism and politics), rele-\nvant events (that is, events of special relevance\nfor the Spanish society, and of special signiﬁ-\ncance for future research on Spanish history,\nsociety and culture) and domains at risk of\ndisappearing.6\nWe base our new pretraining corpus solely\non these BNE’s crawls carried out between the\nyears 2009 and 2019. This means that sources\nthat typically compose pretraining corpus of\nlanguage models, such as Wikipedia, are not\npart of the dataset. This will have an eﬀect\non the evaluation, as we will see in Section\n5. Due to the massive amount of data, the\nNational Library ran the ﬁrst data extraction\nfrom WARC formatted ﬁles using the Selecto-\nlax Python library7 in its own premises. This\nprocess generated 59TB of JSON ﬁles con-\ntaining some metadata along with the text\nextracted from the WARC ﬁles, namely: para-\ngraphs, headers and hyperlinks’ texts.\nTo ensure the high quality of our train-\ning data, we developed an in-house cleaning\npipeline inspired by the heuristics proposed\nin (Virtanen et al., 2019). It is composed of\nthe following components:\n1. Dataparsing: Weparsetextindiﬀerent\nformats (e.g. CommonCrawl’s WARC)\nkeeping document-level boundaries.\n2. Encoding detection and ﬁxing: We\nuse chardet8 to detect the encoding of\nthe text and convert it to UTF-8 if re-\nquired. Then, we apply ftfy (Speer,\n2019), a heuristic tool to ﬁx common en-\ncoding errors.\n3. Character document-level ﬁltering:\nWe apply simple, inexpensive heuristics\nto discard lower quality documents. For\nexample, we discard documents that are\ntoo short or those with too many char-\n5http://www.bne.es/en/Inicio/index.html\n6http://www.bne.es/en/Colecciones/\nArchivoWeb/Subcolecciones/selectivas.html\n7https://pypi.org/project/selectolax\n8https://github.com/chardet/chardet\nacters associated to code snippets to pre-\nvent the inclusion of documents that are\nmainly Javascript snippets. We also ap-\nply a fast language identiﬁer based on\nFastText (Bojanowski et al., 2016). Fi-\nnally, we apply some regex-based rules to\nremove or transform placeholder text.\n4. Sentence splitting: We apply a heuris-\ntic sentence splitter.9 The heuristics are\nbased on basic regex rules that account\nfor acronyms (e.g., R.A.E. is not split in\n3 diﬀerent sentences).\n5. Sentence-level ﬁltering: In this step,\nwe apply more complex, ﬁne-grained rules\nto discard some sentences within a docu-\nment. The rationale is that in documents\ngood-enough to get past the previous ﬁl-\nters, there might be some sentences spoil-\ning it, mainly coming from placeholder\ntext or non-natural text. Thus, we ex-\necute a cascade of language identiﬁers,\nthat is, we ﬁrst apply the fast (but less\naccurate) language identiﬁer (FastText)\nwith a relatively low conﬁdence score, to\nminimize the number of false negatives\n(negative of being Spanish). Then we ap-\nply a slower but more accurate (in our\npreliminary tests) language identiﬁer10\nto the sentences that passed the ﬁrst lan-\nguage ﬁlter.\n6. Deduplication: We deduplicate text us-\ning Onion’s (Pomikálek, 2011) N-gram-\nbased deduplication. That is, for each\ndocument, Onion indexes 5-grams and\nmarks as duplicates those documents\nwhose overlapping in terms of 5-grams\nmeets a certain threshold.\n7. Formatting: We write documents in\nplain text ensuring that document bound-\naries are kept.\nNote that we both transform and delete text.\nIn the case of the encoding ﬁxer, we apply\ntransformations. In the case of the character-\nlevel document ﬁlter, we apply both transfor-\nmations and deletions. In the case of sentence-\nlevel ﬁlter, language identiﬁcation, and dedu-\nplication, we delete the text detected as low-\nquality, not Spanish, or duplicated. The clean-\ning process took 96 hours in an HPC environ-\nment composed of 100 compute nodes, each\n9https://pypi.org/project/sentence-\nsplitter/\n10https://github.com/saffsd/langid.py\nwith 48 CPU cores. At the end of the process,\nwe were left with 2TB of clean data at the\ndocument level. Finally, after deduplication,\nwe obtained a total of 570GB with more than\n200M documents and 135B tokens of high\nquality data. The corpus will be eventually\nreleased as soon as BNE determines the legal\naspects of it.\n3.2 Fine-tuning datasets\nTo perform an extensive evaluation of our\nmodels, we set up an evaluation workbench\ncomprised of 9 tasks, including one of our\nown creation, as described below. The ﬁne-\ntuning methodology is explained in Section\n5.2, and the scripts are publicly available on\nthe organization’s GitHub page.11\nText classiﬁcation The Multilingual\nDocument Classiﬁcation Corpus (MLDoc)\n(Schwenk and Li, 2018; Lewis et al., 2004)\nis a cross-lingual document classiﬁcation\ndataset covering 8 languages. We used the\nSpanish portion to evaluate our models on\nmonolingual classiﬁcation. It consists of\n14,458 news articles from Reuters classiﬁed\nin four categories: Corporate/Industrial,\nEconomics, Government/Social and Markets.\nNamed Entity Recognition and Classiﬁ-\ncation (NERC) We selected the CoNLL-\nNERC and the CAPITEL-NERC datasets.\nCoNLL-NERC is the Spanish dataset of the\nCoNLL-2002 Shared Task (Tjong Kim Sang,\n2002). The dataset is annotated with four\ntypes of named entities: persons, locations, or-\nganizations, and other miscellaneous entities.\nThey are formatted in the standard Beginning-\nInside-Outside (BIO) format. The dataset\nis composed of 8,324 sentences with 19,400\nnamed entities for the training set, 1,916 sen-\ntences with 4,568 named entities for the de-\nvelopment set, and 1,518 sentences with 3,644\nnamed entities for the test set. CAPITEL-\nNERC was the ﬁrst sub-task of the CAPITEL-\nEVAL shared task, held by IberLEF in 2020.\nThesourceoftheCAPITEL-NERCdatasetsis\nthe CAPITEL corpus12 (Porta-Zamorano and\nEspinosa-Anke, 2020), a collection of Spanish\narticles in the news domain. The dataset con-\nsists of 22,647 sentences with 31,311 named\nentities for train, and 7,550 sentences for devel-\nopment and test sets respectively, with 10,229\n11https://github.com/PlanTL-GOB-ES/lm-\nspanish\n12https://sites.google.com/view/\ncapitel2020\\#h.p_eFTF8UCJXFMq\nnamed entities for the development set and\n10,226 for the test set. CAPITEL-NERC\nis annotated with the same four named en-\ntities used in CoNLL-NERC (persons, loca-\ntions, organizations, and other), but follow-\ning a Beginning-Inside-Outside-Ending-Single\n(BIOES) format.\nParaphrase Identiﬁcation The Cross-\nlingual Adversarial Dataset for Paraphrase\nIdentiﬁcation (PAWS-X) (Yang et al., 2019)\nis a multilingual dataset that contains 49,401\ntraining sentences, 2,000 sentences for the de-\nvelopment set, and another 2,000 for the test\nset. It is important to note that this dataset\ncontains machine translated text, and as a\nconsequence some of the Spanish sentences\nmight not be entirely correct.\nPart-of-Speech Tagging (POS) We se-\nlected the Universal Dependencies Part-of-\nSpeech (UD-POS) dataset, from the Span-\nish Ancora corpus13 (Taulé, Martí, and Re-\ncasens, 2008), and the CAPITEL-POS from\nthe CAPITEL Corpus, described above.\nSemantic Textual Similarity (Agirre et\nal., 2012) We collected the Spanish test\nsets from 2014 (Agirre et al., 2014) and 2015\n(Agirre et al., 2015). Since no training data\nwas provided for the Spanish subtask, we ran-\ndomly sampled both datasets into 1,321 sen-\ntences for the train set, 78 sentences for the\ndevelopment set, and 156 sentences for the\ntest set. To make the task harder for the mod-\nels, we purposely made the development set\nsmaller than the test set.\nTextual Entailment We used the Spanish\npart of the Cross-Lingual NLI Corpus (XNLI)\n(Conneau et al., 2018). This evaluation cor-\npus consists of a collection 400,202 sentences,\nannotated with textual entailment via crowd-\nsourcing.\nQuestion Answering (QA) We built a\nnew dataset, the Spanish Question Answering\nCorpus (SQAC), an extractive QA dataset\nthat we exhaustively present in section 3.2.1.\nThere is no sizable training dataset analo-\ngous to the English version of SQUAD (Ra-\njpurkar et al., 2016), and most ﬁnetunings of\nSpanish models rely on machine translated\ntext. There is a professionally translated ver-\nsion of the XQUAD (Artetxe, Ruder, and\nYogatama, 2019) dataset, but it is not big\n13https://universaldependencies.org/\ntreebanks/es_ancora/index.html\nenough or varied enough to properly train or\nevaluate, and the source text is not written\noriginally in Spanish (and translation artifacts\ncould slip in).\n3.2.1 SQAC\nThe Spanish Question Answering Corpus\n(SQAC) is an extractive QA dataset with no\nunanswerable questions. It is created from\ntexts extracted from the Spanish Wikipedia,\nencyclopedic articles, newswire articles from\nWikinews, and the Spanish section of the\nAnCora corpus (Taulé, Martí, and Recasens,\n2008), which is a mix from diﬀerent newswire\nand literature sources. It was created by com-\nmissioning the creation of 18,817 questions\nwith the annotation of their answer spans\nfrom 6,247 textual contexts. The guidelines\nwere adapted from SQuAD v1.1 (Rajpurkar et\nal., 2016), and the annotators were all native\nSpanish speakers with university studies in\nvarious ﬁelds related to linguistics. Following\nthe XQuAD (Artetxe, Ruder, and Yogatama,\n2019) structure, no additional answers were\ncollected.\nOur guidelines for the creation of the\ndataset stated that the answers provided\nshould not require any additional knowledge\nbeyond what was explicitly provided in the\ntextual contexts, and that they must be as\nstraightforward as possible, avoiding recourse\nto humour, irony, etc., since they often require\nknowledge of facts beyond the local context.\nThe questions should not be just copies of\nthe answers in an interrogative form, and use\nof synonyms was encouraged to avoid lexical\noverlap as much as possible. Even so, in aver-\nage 48% of the words in the question can be\nfound in the context. Another important spec-\niﬁcation was that the drafted questions should\ncover as much as possible the whole range of\ninterrogatives, asking about who, where, how,\nwhen, etc., from the information potentially\nprovided by the contexts. Table 1 shows the\nstatistics of the interrogatives in the dataset.\nTo assess the annotation quality, we com-\nmissioned the annotation of the answer spans\nin nearly 600 randomly chosen questions. We\nobtained a human score equal to 85% F1 and\n71% EM, after answer normalization.\nThe need to create SQAC arose from the\nneed of evaluating Spanish models on QA\ntasks. The Spanish portion of XQuAD only\nconsists of an evaluation set and, although\nit purportedly is a professional translation of\nEnglish contexts and questions, we believe\nQuestion Count %\nQué (What) 6,381 33.91%\nQuién/es (Who) 2,952 15.69%\nCuál/es (Which) 2,034 10.81%\nCómo (How) 1,949 10,36%\nDónde (Where) 1,856 9.86%\nCuándo (When) 1,639 8.71%\nCuánto (How much) 1,311 6.97%\nCuántos (How many) 495 2.63%\nAdónde (Where) 100 0.53%\nCuánta (How much) 49 0.26%\nno question mark 43 0.23%\nCuántas (How many) 19 0.10%\nTable 1: Statistics for the range of interroga-\ntives in the SQAC dataset.\nhaving material originally written is Spanish\nis a better option. We strongly believe that\nthe SQAC dataset contributes positively to\nthe benchmarking datasets in Spanish, which\ntoo often consist of translations from other\nlanguages. Furthermore, previous datasets\ntend to be rather small in size and not very\nvaried with regard to genre or topic.\nThis dataset is now publicly available in\nHuggingFace.14\n4 Language Models\nFor the encoder models we used the RoBERTa\narchitecture. The pretraining objective used\nfor this architecture is the masked language\nmodeling without next sentence prediction.\nThe conﬁguration of thebase and large ver-\nsions (following the HuggingFace nomencla-\nture for RoBERTa models) is as follows:\n• RoBERTa-b: 12-layer, 768-hidden, 12-\nheads, 125M parameters.\n• RoBERTa-l: 24-layer, 1024-hidden, 16-\nheads, 355M parameters.\nFor the generative models, we used the\nGPT2 architecture, trained using language\nmodeling (next token prediction). The conﬁg-\nuration of theGPT2 and GPT2-large versions\n(following the HuggingFace nomenclature) is\nas follows:\n• gpt2: 12-layer, 768-hidden, 12-heads,\n117M parameters.\n• gpt2-large: 36-layer, 1280-hidden, 20-\nheads, 774M parameters.\n14https://huggingface.co/datasets/PlanTL-\nGOB-ES/SQAC\nFor all the models, we use byte-level BPE\n(Radford et al., 2019a), as in the original\nRoBERTa, trained with our own corpus. The\npretraining was performed with a single epoch\nas proposed in (Komatsuzaki, 2019), following\nrecent trends (Brown et al., 2020b). Following\nthe same literature, we do not use dropout\nto increase convergence speed taking into ac-\ncount that the model will not overﬁt to a large\ndataset in a single pass, but keep the weight\ndecay to 0.01 as it has been proven to still be\nbeneﬁcial in single-epoch regimes (Henighan\net al., 2020). The rest of parameters can be\nfound in Table 2. All of our generative models\nwere trained with a sequence length of 512\ninstead of e.g. 1024 due to computational\nconstraints, which is enough for most tasks\n(otherwise, we suggest using a sliding window).\nWe use the Fairseq (Ott et al., 2019) library\nfor pretraining. Then we convert the check-\npoint to HuggingFace (Wolf et al., 2020) and\nwe use this library for ﬁne-tuning on down-\nstream tasks.\n5 Evaluation\nIn this section, we compare our RoBERTa\nmodels with a set of relevant multilingual and\nSpanish models in 9 diﬀerent tasks. For GPT2\nmodels, the lack of evaluation datasets has pre-\nvented us from running a proper benchmark.\nIn this case, we provide the perplexity curves\non training and validation data on Figures 1\nand 2. In both cases, the models converge\nsmoothly, although the large model needs a\nsigniﬁcantly greater number of updates.\n5.1 Baselines\nWe compare our RoBERTa-b and RoBERTa-l\nmodels with a multilingual model, mBERT,\nandotherSpanishmonolingualmodels, BETO\n(Cañete et al., 2020), BERTIN15 and ELEC-\nTRICIDAD.16\nmBERT The BERT-base Multilingual\nCased model (mBERT) is a BERT language\nmodel with 12 self-attention layers, 12 atten-\ntion heads each, a hidden size of 768, and a\ntotal of 178M parameters. It was pretrained\non 104 languages with the Wikipedia dataset.\nBETO Accordingtotheauthors, theBETO\nmodel has 12 self-attention layers, 16 atten-\ntion heads each, a hidden layer of size 1024,\n15https://huggingface.co/bertin-project/\nbertin-roberta-base-spanish/tree/v1-512\n16https://huggingface.co/mrm8488/\nelectricidad-base-generator\nand a total of 110M parameters.17 However,\nthe actual version uploaded to HuggingFace18\nhas a BERT-base-like architecture with 12\nself-attention layers, 12 attention heads each,\na hidden size of 768, and a total of 110M pa-\nrameters. It was pretrained with text from\ndiﬀerent sources: all the Spanish data from\nWikipedia and the Spanish portion of the\nOPUS19 project.\nBERTIN Although BERTIN was an-\nnounced as a RoBERTa-large model, it is ac-\ntually a RoBERTa-base model with 12 layers,\n12 attention heads each, hidden size of 768,\nand a total 125M parameters. It was trained\nfrom scratch on the Spanish portion of mC4\n(Xue et al., 2020). The BERTIN version we\nare evaluating is the one pointed out by the\nauthors.\nELECTRICIDAD ELECTRIDAD is the\ngenerator of a Spanish ELECTRA (Clark et\nal., 2020) base architecture, trained on the\nSpanish OSCAR corpus.20\n5.2 Fine-tuning methodology\nTo evaluate our models against the baselines\nmentioned above, we follow the usual prac-\ntices in the literature and use the Hugging-\nFace Transformers library (Wolf et al., 2019).\nFor each task, we add a single linear layer\non top of the model being ﬁne-tuned. In the\ncase of sentence/paragraph-level classiﬁcation\ntasks, we use the[CLS] token in the case of\nBERT models and the<s> token in the case of\nRoBERTa models. We use a maximum input\nlength of 512 tokens in all cases.\nTo have a fair comparison, we train each\nmodel with the same settings, that is, the de-\nfault ones in HuggingFace’s ﬁne-tuning scripts,\nconducting a grid search for all models and\ntasks:\n• Batch size: 16, 32.\n• Weight decay: 0.01, 0.1.\n• Learning rate: 1e-5, 3e-5, 5e-5.\n• Epochs: The best (as per the develop-\nment set) out of 5 epochs.\n17Note that the claimed parameter count of BETO\ndoes not add up, since BERT-base has the same num-\nber of parameters with 12 attention heads and an\nembedding size of 786.\n18https://huggingface.co/dccuchile/bert-\nbase-spanish-wwm-cased\n19https://opus.nlpl.eu/\n20https://oscar-corpus.com/\n1 20000 50000 90000 130000 165150\nNumber of updates\n101\n102\n103\n104\nPerplexity\nTrain PPL = 13.18\nValid PPL = 10.08\nPerplexity curves for GPT2 \nTrain\nValid\nFigure 1: Perplexity curves for GPT2 model.\n1 20000 50000 90000 130000 168204\nNumber of updates\n101\n102\n103\n104\nPerplexity\nTrain PPL = 10.02\nValid PPL = 6.68\nPerplexity curves for GPT2-large \nTrain\nValid\nFigure 2: Perplexity curves for GPT2-large model.\nWarmup Peak LR Batch Size Sequence\nLength Precision Scale\nTolerance\nRoBERTa-b 10,000 0.00050\n2,048 512 FP16\n0.00\nRoBERTa-l 30,000 0.00025 0.25\nGPT2 10,000 0.00050 0.25\nGPT2-large 30,000 0.00025 0.25\nTable 2: Parameters for the pretraining of the models.\nWe select the best checkpoint using the down-\nstream task metric in the corresponding devel-\nopment set, and then evaluate it on the test\nset.\nRegarding the data splits, Table 3 shows\nthe sizes of the train, development and test\nsets used in each downstream task.\nAll ﬁne-tuning scripts are publicly available\non the GitHub page of the organization.21\nDataset Train Validation Test\nMLDoc 9,458 1,000 4,000\nCoNLL-NERC 8,324 1,916 1,518\nCAPITEL-NERC 22,648 7,550 7,550\nPAWS-X 49,401 2,000 2,000\nUD-POS 14,305 1,654 1,721\nCAPITEL-POS 7,087 2,363 2,364\nSQAC 15,036 1,864 1,910\nSTS 1,321 78 156\nXNLI 392,702 2,490 5,010\nTable 3: Sizes of the train, validation and test\nsets used for each task.\n5.3 Results\nFor each model and task, we chose the best\nconﬁguration that achieved the highest result\non the development set and then computed\nthe test performances, as reported in Table 4.\nThe results for all the conﬁgurations are in Ap-\npendix I. We can observe that the RoBERTa-\nlarge model stands out in most tasks, ex-\ncept in those where RoBERTa-base outper-\nforms it. The exception being the MLDoc\ndataset, in which the diﬀerences between mod-\nels are marginal and BETO slightly surpasses\nthe rest. We further observe that the most\nprominent diﬀerences are present in those\ndatasetsthatarenotbasedonWikipedia, such\nas CAPITEL-NERC, STS and SQAC (with\n2 points in CAPITEL-NERC and almost 3\npoints of diﬀerence in the other two). These\n21https://github.com/PlanTL-GOB-ES/lm-\nspanish\nresults may be attributed to the data con-\ntamination eﬀect (Brown et al., 2020a) that\nprevented the language models pretrained on\nWikipedia, namely BETO, mBERT, BERTIN\nand ELECTRA, to beneﬁt from it in these 3\ndatasets.\n6 Conclusions\nThis work introduces new data and model\nresources, namely, a pretraining corpus and\na brand new Question Answering dataset in\nSpanish and large pretrained language models.\nSpeciﬁcally, the pretraining corpus is a mas-\nsive, more diverse dataset for Spanish than\nprevious datasets for language models such\nas Wikipedia, including myriad sources. We\nbelieve that models leveraging our pretraining\ncorpus, either in combination with other ones\nor not, will beneﬁt from it, leading to better\nlanguage representations.\nThe SQAC dataset represents a signiﬁcant,\nhigh-quality contribution for extractive QA,\nallowing an appropriate evaluation of Spanish\nQA systems.\nFinally, we have pretrained and published\ntwo RoBERTa models that showed high per-\nformances on many NLP downstream tasks\nand two generative GPT2 models of diﬀerent\nsizes.\nAll in all, we conclude that these contri-\nbutions are a crucial step towards reducing\nthe gap with NLP for English and other high-\nresource languages.\nAs future work, we plan to further extend\nthe pretraining corpus with new sources (e.g.,\nWikipedia or books). Furthermore, the pre-\ntraining corpus will be analysed in terms of\ntopic modeling and bias. We also want to\nextend the context length of the models from\n512 to 1024, and further scale up the models,\nideally with improved inference eﬃciency to\ndemocratize their use.\nDataset Metric RoBERTa-b RoBERTa-l BETO mBERT BERTIN ELECTRA\nMLDoc F1 0.9664 0.9702 0.9714 0.9617 0.9668 0.9565\nCoNLL-NERC F1 0.8851 0.8823 0.8759 0.8691 0.8835 0.7954\nCAPITEL-NERC F1 0.8960 0.9051 0.8772 0.8810 0.8856 0.8035\nPAWS-X F1 0.9020 0.9150 0.8930 0.9000 0.8965 0.9045\nUD-POS F1 0.9907 0.9904 0.9900 0.9886 0.9898 0.9818\nCAPITEL-POS F1 0.9846 0.9856 0.9836 0.9839 0.9847 0.9816\nSQAC F1 0.7923 0.8202 0.7923 0.7562 0.7678 0.7383\nSTS Combined 0.8533 0.8411 0.8159 0.8164 0.7945 0.8063\nXNLI Accuracy 0.8016 0.8263 0.8130 0.7876 0.7890 0.7878\nTable 4: Evaluation table comparing our RoBERTa-b and RoBERTa-l with the rest of the models.\nAcknowledgements\nWe want to thank the National Library of\nSpain for such a large eﬀort on the data gath-\nering and the Future of Computing Center, a\nBarcelona Supercomputing Center and IBM\ninitiative (2020).\nThis work was funded by the Spanish State\nSecretariat for Digitalization and Artiﬁcial\nIntelligence (SEDIA) within the framework of\nthe Plan-TL.\nReferences\nAgirre, E., C. Banea, C. Cardie, D. Cer,\nM. Diab, A. Gonzalez-Agirre, W. Guo,\nI. Lopez-Gazpio, M. Maritxalar, R. Mihal-\ncea, et al. 2015. Semeval-2015 task 2: Se-\nmantic textual similarity, english, spanish\nand pilot on interpretability. InProceed-\nings of the 9th international workshop on\nsemantic evaluation (SemEval 2015), pages\n252–263.\nAgirre, E., C. Banea, C. Cardie, D. Cer,\nM. Diab, A. Gonzalez-Agirre, W. Guo,\nR. Mihalcea, G. Rigau, and J. Wiebe. 2014.\nSemeval-2014 task 10: Multilingual seman-\ntic textual similarity. In Proceedings of\nthe 8th international workshop on semantic\nevaluation (SemEval 2014), pages 81–91.\nAgirre, E., D. Cer, M. Diab, and A. Gonzalez-\nAgirre. 2012. SemEval-2012 task 6: A pilot\non semantic textual similarity. In*SEM\n2012: The First Joint Conference on Lexi-\ncal and Computational Semantics – Volume\n1: Proceedings of the main conference and\nthe shared task, and Volume 2: Proceedings\nof the Sixth International Workshop on Se-\nmantic Evaluation (SemEval 2012), pages\n385–393, Montréal, Canada, 7-8 June. As-\nsociation for Computational Linguistics.\nAlmeida, A. and A. Bilbao. 2018. Spanish 3b\nwords word2vec embeddings, January.\nArtetxe, M., S. Ruder, and D. Yogatama.\n2019. On the cross-lingual transferabil-\nity of monolingual representations.CoRR,\nabs/1910.11856.\nBañón, M., P. Chen, B. Haddow, K. Heaﬁeld,\nH. Hoang, M. Esplà-Gomis, M. L. Forcada,\nA. Kamran, F. Kirefu, P. Koehn, S. Or-\ntiz Rojas, L. Pla Sempere, G. Ramírez-\nSánchez, E. Sarrías, M. Strelec, B. Thomp-\nson, W. Waites, D. Wiggins, and\nJ. Zaragoza. 2020. ParaCrawl: Web-scale\nacquisition of parallel corpora. In Pro-\nceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics,\npages 4555–4567, Online, July. Association\nfor Computational Linguistics.\nBengio, Y., R. Ducharme, and P. Vincent.\n2000. A neural probabilistic language\nmodel. Advances in Neural Information\nProcessing Systems, 13.\nBilbao-Jayo, A. and A. Almeida. 2018.\nAutomatic political discourse analysis\nwith multi-scale convolutional neural net-\nworks and contextual data.International\nJournal of Distributed Sensor Networks,\n14(11):1550147718811827.\nBojanowski, P., E. Grave, A. Joulin, and\nT. Mikolov. 2016. Enriching word vectors\nwith subword information.arXiv preprint\narXiv:1607.04606.\nBrown, T., B. Mann, N. Ryder, M. Subbiah,\nJ. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford,\nI. Sutskever, and D. Amodei. 2020a. Lan-\nguage models are few-shot learners. In\nH. Larochelle, M. Ranzato, R. Hadsell,\nM. F. Balcan, and H. Lin, editors, Ad-\nvances in Neural Information Processing\nSystems, volume 33, pages 1877–1901. Cur-\nran Associates, Inc.\nBrown, T.B., B.Mann, N.Ryder, M.Subbiah,\nJ. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford,\nI. Sutskever, and D. Amodei. 2020b. Lan-\nguage models are few-shot learners.CoRR,\nabs/2005.14165.\nCardellino, C. 2019. Spanish Billion Words\nCorpus and Embeddings, August.\nCarrino, C. P., J. Armengol-Estapé,\nO. de Gibert Bonet, A. Gutiérrez-Fandiño,\nA. Gonzalez-Agirre, M. Krallinger, and\nM. Villegas. 2021. Spanish biomedical\ncrawled corpus: A large, diverse dataset\nfor spanish biomedical language models.\nCañete, J. 2019. Compilation of large spanish\nunannotated corpora, May.\nCañete, J., G. Chaperon, R. Fuentes, J.-H.\nHo, H. Kang, and J. Pérez. 2020. Spanish\npre-trained bert model and evaluation data.\nIn PML4DC at ICLR 2020.\nClark, K., M. Luong, Q. V. Le, and C. D.\nManning. 2020. ELECTRA: pre-training\ntext encoders as discriminators rather than\ngenerators. CoRR, abs/2003.10555.\nConneau, A., R. Rinott, G. Lample,\nA. Williams, S. R. Bowman, H. Schwenk,\nand V. Stoyanov. 2018. Xnli: Evaluating\ncross-lingual sentence representations. In\nProceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Pro-\ncessing. Association for Computational Lin-\nguistics.\nCui, Y., W. Che, T. Liu, B. Qin, and Z. Yang.\n2021. Pre-training with whole word mask-\ning for chinese bert. IEEE/ACM Trans-\nactions on Audio, Speech, and Language\nProcessing, 29:3504–3514.\nde Vries, W., A. van Cranenburgh, A. Bisazza,\nT. Caselli, G. van Noord, and M. Nissim.\n2019. Bertje: A dutch bert model.arXiv\npreprint arXiv:1912.09582.\nDevlin, J., M. Chang, K. Lee, and\nK. Toutanova. 2018. BERT: pre-\ntraining of deep bidirectional transform-\ners for language understanding. CoRR,\nabs/1810.04805.\nGutiérrez-Fandiño, A., J. Armengol-Estapé,\nC. P. Carrino, O. D. Gibert, A. Gonzalez-\nAgirre, and M. Villegas. 2021a. Spanish\nbiomedical and clinical language embed-\ndings.\nGutiérrez-Fandiño, A., J. Armengol-Estapé,\nA. Gonzalez-Agirre, and M. Villegas.\n2021b. Spanish legalese language model\nand corpora.\nHenighan, T., J. Kaplan, M. Katz, M. Chen,\nC. Hesse, J. Jackson, H. Jun, T. B. Brown,\nP. Dhariwal, S. Gray, C. Hallacy, B. Mann,\nA. Radford, A. Ramesh, N. Ryder, D. M.\nZiegler, J. Schulman, D. Amodei, and\nS. McCandlish. 2020. Scaling laws for\nautoregressive generative modeling.CoRR,\nabs/2010.14701.\nHochreiter, S. and J. Schmidhuber. 1997.\nLong short-term memory.Neural Comput.,\n9(8):1735–1780, nov.\nKomatsuzaki, A. 2019. One epoch is all you\nneed.\nLe, H., L. Vial, J. Frej, V. Segonne,\nM. Coavoux, B. Lecouteux, A. Allauzen,\nB. Crabbé, L. Besacier, and D. Schwab.\n2019. Flaubert: Unsupervised language\nmodel pre-training for french. arXiv\npreprint arXiv:1912.05372.\nLewis, D. D., Y. Yang, T. Russell-Rose,\nand F. Li. 2004. Rcv1: A new bench-\nmark collection for text categorization re-\nsearch. Journal of machine learning re-\nsearch, 5(Apr):361–397.\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi,\nD. Chen, O. Levy, M. Lewis, L. Zettle-\nmoyer, and V. Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining ap-\nproach.\nMartin, L., B. Muller, P. J. O. Suárez,\nY. Dupont, L. Romary, É. V. de La Clerg-\nerie, D. Seddah, and B. Sagot. 2019.\nCamembert: a tasty french language model.\narXiv preprint arXiv:1911.03894.\nMikolov, T., K. Chen, G. Corrado, and\nJ. Dean. 2013. Eﬃcient estimation of\nword representations in vector space.arXiv\npreprint arXiv:1301.3781.\nNguyen, D. Q. and A. T. Nguyen. 2020.\nPhobert: Pre-trained language mod-\nels for vietnamese. arXiv preprint\narXiv:2003.00744.\nNozza, D., F. Bianchi, and D. Hovy. 2020.\nWhat the [mask]? making sense of\nlanguage-speciﬁc BERT models. CoRR,\nabs/2003.02912.\nOtt, M., S. Edunov, A. Baevski, A. Fan,\nS. Gross, N. Ng, D. Grangier, and M. Auli.\n2019. fairseq: A fast, extensible toolkit\nfor sequence modeling. InProceedings of\nNAACL-HLT 2019: Demonstrations.\nPennington, J., R. Socher, and C. Manning.\n2014. GloVe: Global vectors for word rep-\nresentation. In Proceedings of the 2014\nConference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages\n1532–1543, Doha, Qatar, October. Associ-\nation for Computational Linguistics.\nPeters, M. E., M. Neumann, M. Iyyer,\nM. Gardner, C. Clark, K. Lee, and\nL. Zettlemoyer. 2018. Deep contextualized\nword representations. InProc. of NAACL.\nPomikálek, J. 2011.Removing boilerplate and\nduplicate content from web corpora. Ph.D.\nthesis, Masaryk university, Faculty of in-\nformatics, Brno, Czech Republic.\nPorta-Zamorano, J. and L. Espinosa-Anke.\n2020. Overview of capitel shared tasks at\niberlef 2020: Named entity recognition and\nuniversal dependencies parsing.\nRadford, A., K.Narasimhan, T.Salimans, and\nI. Sutskever. 2018. Improving language\nunderstanding by generative pre-training.\nRadford, A., J. Wu, R. Child, D. Luan,\nD. Amodei, and I. Sutskever. 2019a. Lan-\nguage Models are Unsupervised Multitask\nLearners.\nRadford, A., J. Wu, R. Child, D. Luan,\nD. Amodei, I. Sutskever, et al. 2019b. Lan-\nguage models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9.\nRajpurkar, P., J. Zhang, K. Lopyrev, and\nP. Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text.\nSchwenk, H. and X. Li. 2018. A corpus\nfor multilingual document classiﬁcation\nin eight languages. In N. C. C. chair),\nK. Choukri, C. Cieri, T. Declerck, S. Goggi,\nK. Hasida, H. Isahara, B. Maegaard,\nJ. Mariani, H. Mazo, A. Moreno, J. Odijk,\nS.Piperidis, andT.Tokunaga, editors, Pro-\nceedings of the Eleventh International Con-\nference on Language Resources and Eval-\nuation (LREC 2018), Paris, France, may.\nEuropean Language Resources Association\n(ELRA).\nSpeer, R. 2019. ftfy. Zenodo. Version 5.5.\nTaulé, M., M. A. Martí, and M. Recasens.\n2008. AnCora: Multilevel annotated cor-\npora for Catalan and Spanish. In Pro-\nceedings of the Sixth International Confer-\nence on Language Resources and Evalua-\ntion (LREC’08), Marrakech, Morocco, May.\nEuropean Language Resources Association\n(ELRA).\nTiedemann, J. 2012. Parallel data, tools and\ninterfaces in opus. InLrec, volume 2012,\npages 2214–2218. Citeseer.\nTjong Kim Sang, E. F. 2002. Introduction to\nthe CoNLL-2002 shared task: Language-\nindependent named entity recognition. In\nCOLING-02: The 6th Conference on Nat-\nural Language Learning 2002 (CoNLL-\n2002).\nVaswani, A., N. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. u. Kaiser,\nand I. Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Ad-\nvances in Neural Information Processing\nSystems, volume 30. Curran Associates,\nInc.\nVirtanen, A., J. Kanerva, R. Ilo, J. Lu-\noma, J. Luotolahti, T. Salakoski, F. Gin-\nter, and S. Pyysalo. 2019. Multilingual\nis not enough: BERT for ﬁnnish.CoRR,\nabs/1912.07076.\nWolf, T., L. Debut, V. Sanh, J. Chaumond,\nC. Delangue, A. Moi, P. Cistac, T. Rault,\nR. Louf, M. Funtowicz, and J. Brew. 2019.\nHuggingface’s transformers: State-of-the-\nart natural language processing. CoRR,\nabs/1910.03771.\nWolf, T., L. Debut, V. Sanh, J. Chau-\nmond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, J. Davi-\nson, S. Shleifer, P. von Platen, C. Ma,\nY. Jernite, J. Plu, C. Xu, T. L. Scao,\nS. Gugger, M. Drame, Q. Lhoest, and A. M.\nRush. 2020. Transformers: State-of-the-\nart natural language processing. InPro-\nceedings of the 2020 Conference on Empir-\nical Methods in Natural Language Process-\ning: System Demonstrations, pages 38–45,\nOnline, October. Association for Compu-\ntational Linguistics.\nXue, L., N. Constant, A. Roberts, M. Kale,\nR. Al-Rfou, A. Siddhant, A. Barua, and\nC. Raﬀel. 2020. mt5: A massively multilin-\ngual pre-trained text-to-text transformer.\narXiv preprint arXiv:2010.11934.\nYang, Y., Y. Zhang, C. Tar, and J. Baldridge.\n2019. PAWS-X: A Cross-lingual Adversar-\nial Dataset for Paraphrase Identiﬁcation.\nIn Proc. of EMNLP.\nAppendix I\nModel Batch Size Weight decay Learning rate Eval F1 Test F1\nRoBERTa-b 32 0.1 0.00001 0.9770 0.9664\nRoBERTa-l 32 0.01 0.00003 0.9760 0.9702\nBETO 32 0.1 0.00003 0.9750 0.9714\nmBERT 32 0.01 0.00001 0.9701 0.9617\nBERTIN 32 0.01 0.00003 0.9770 0.9668\nELECTRA 32 0.1 0.00003 0.9629 0.9565\nTable 5: Best conﬁgurations for the eval MLDoc dataset with F1 for eval and test.\nModel Batch Size Weight decay Learning rate Eval F1 Test F1\nRoBERTa-b 32 0.01 0.00005 0.8870 0.8851\nRoBERTa-l 32 0.1 0.00005 0.8937 0.8823\nBETO 16 0.1 0.00003 0.8710 0.8759\nmBERT 16 0.1 0.00003 0.8727 0.8691\nBERTIN 16 0.1 0.00005 0.8835 0.8835\nELECTRA 16 0.1 0.00005 0.7986 0.7954\nTable 6: Best conﬁgurations for the eval CoNLL-NERC dataset with F1 for eval and test.\nModel Batch Size Weight decay Learning rate Eval F1 Test F1\nRoBERTa-b 16 0.01 0.00005 0.9013 0.8960\nRoBERTa-l 32 0.01 0.00003 0.9099 0.9051\nBETO 32 0.1 0.00005 0.8909 0.8772\nmBERT 16 0.1 0.00003 0.8877 0.8810\nBERTIN 16 0.1 0.00005 0.8969 0.8856\nELECTRA 16 0.01 0.00005 0.8017 0.8035\nTable 7: Best conﬁgurations for the eval CAPITEL-NERC dataset with F1 for eval and test.\nModel Batch Size Weight decay Learning rate Eval F1 Test F1\nRoBERTa-b 32 0.01 0.00003 0.9020 0.9020\nRoBERTa-l 16 0.01 0.00001 0.9145 0.9150\nBETO 32 0.01 0.00005 0.9010 0.8930\nmBERT 16 0.1 0.00003 0.8985 0.9000\nBERTIN 32 0.01 0.00005 0.9000 0.8965\nELECTRA 32 0.01 0.00003 0.9020 0.9045\nTable 8: Best conﬁgurations for the eval PAWS-X dataset with F1 for eval and test.\nModel Batch Size Weight decay Learning rate Eval F1 Test F1\nRoBERTa-b 16 0.1 0.00005 0.9907 0.9907\nRoBERTa-l 32 0.01 0.00003 0.9913 0.9904\nBETO 16 0.01 0.00003 0.9907 0.9900\nmBERT 32 0.1 0.00005 0.9892 0.9886\nBERTIN 32 0.01 0.00005 0.9910 0.9898\nELECTRA 16 0.1 0.00005 0.9826 0.9818\nTable 9: Best conﬁgurations for the eval UD-POS dataset with F1 for eval and test.\nModel Batch Size Weight decay Learning rate Eval F1 Test F1\nRoBERTa-b 32 0.1 0.00005 0.9848 0.9846\nRoBERTa-l 16 0.01 0.00003 0.9856 0.9856\nBETO 32 0.1 0.00005 0.9839 0.9836\nmBERT 16 0.1 0.00005 0.9835 0.9839\nBERTIN 16 0.1 0.00005 0.9847 0.9847\nELECTRA 16 0.01 0.00005 0.9822 0.9816\nTable 10: Best conﬁgurations for the eval CAPITEL-POS dataset with F1 for eval and test.\nModel Batch Size Weight decay Learning rate Eval F1 Test F1\nRoBERTa-b 16 0.01 0.00005 0.8086 0.7923\nRoBERTa-l 16 0.01 0.00001 0.8409 0.8202\nBETO 32 0.01 0.00005 0.8044 0.7923\nmBERT 32 0.01 0.00005 0.7805 0.7562\nBERTIN 16 0.1 0.00005 0.7827 0.7678\nELECTRA 16 0.01 0.00005 0.7572 0.7383\nTable 11: Best conﬁgurations for the eval SQAC dataset with F1 for eval and test.\nModel Batch Size Weight decay Learning rate Eval Combined Test Combined\nRoBERTa-b 16 0.01 0.00003 0.9095 0.8533\nRoBERTa-l 32 0.01 0.00005 0.9097 0.8411\nBETO 16 0.1 0.00003 0.8919 0.8159\nmBERT 16 0.1 0.00005 0.9193 0.8164\nBERTIN 16 0.1 0.00003 0.8976 0.7945\nELECTRA 16 0.1 0.00005 0.9181 0.8063\nTable 12: Best conﬁgurations for the eval STS dataset with Combined for eval and test.\nModel Batch Size Weight decay Learning rate Eval Accuracy Test Accuracy\nRoBERTa-b 16 0.01 0.00003 0.8124 0.8016\nRoBERTa-l 16 0.1 0.00001 0.8418 0.8263\nBETO 16 0.01 0.00001 0.8269 0.8130\nmBERT 32 0.1 0.00001 0.8032 0.7876\nBERTIN 16 0.1 0.00005 0.8044 0.7890\nELECTRA 16 0.01 0.00005 0.8028 0.7878\nTable 13: Best conﬁgurations for the eval XNLI dataset with Accuracy for eval and test.\nAppendix II\nThis Appendix contains a sample of Masked Language Modelling prediction assessments.\nAgreement\n\"Juana se dejó el libro en el coche porque es muy{mask} con sus cosas.\"\nRoBERTa-base-BNE cuidadosa pesada tranquila lista ocupada\nRoBERTa-large-BNE lista buena cuidadosa estricta generosa\nBETO cuidadoso sensible bueno buena rápido\nmBERT buena feliz bien triste fuerte\nBERTIN buena feliz dulce grande mona\nELECTRA buena amable bueno hábil generoso\n\"La chica que encontraron en el parque estaba leyendo un libro{mask} en el banco.\"\nRoBERTa-base-BNE sentada sentado tumbado viejo esperando\nRoBERTa-large-BNE sentado sentada , tumbado y\nBETO , robado tirado nuevo colgado\nmBERT , escrito estaba suyo y\nBERTIN . y , abandonado secreto\nELECTRA suyo escondido secreto escrito guardado\n\"De entre todas, eligieron en el concurso de baile a quién estaba mejor{mask}.\"\nRoBERTa-base-BNE vestida preparada dotado vestido preparado\nRoBERTa-large-BNE vestida . : preparada formada\nBETO vestida vestido bailando preparada vestidos\nmBERT ##a ##ado puesto colocado ubicado\nBERTIN vestida vestido vestidas parada parado\nELECTRA vestido representada vestida seleccionado representado\n\"A la chica los pantalones le quedaban cortos porque eran muy{mask} para su edad.\"\nRoBERTa-base-BNE cortos altos largos ajustados pequeños\nRoBERTa-large-BNE cómodos largos cortos pequeños grandes\nBETO cortos pequeños largos grandes altos\nmBERT grandes populares importantes jóvenes buenas\nBERTIN adecuados cómodos apropiados importantes caros\nELECTRA buenos cortos largos viejos jóvenes\n\"Le gustaban mucho, pero no{mask} podía comprarlas porque eran demasiado caras.\"\nRoBERTa-base-BNE las se le la lo\nRoBERTa-large-BNE siempre se todas me todos\nBETO se siempre le les las\nmBERT se le sólo solo lo\nBERTIN se yo siempre me necesariamente\nELECTRA se les los le las\nBias\n\"El papel de la mujer en la ciencia es{mask}.\"\nRoBERTa-base-BNE fundamental imprescindible incuestionable clave crucial\nRoBERTa-large-BNE fundamental el esencial clave crucial\nBETO importante relevante fundamental crucial signiﬁcativo\nmBERT social fundamental diferente importante universal\nBERTIN fundamental importante crucial esencial clave\nELECTRA importante fundamental desconocido positivo diferente\n\"El papel de la{mask} en la ciencia es relevante.\"\nRoBERTa-base-BNE mujer música religión publicidad tecnología\nRoBERTa-large-BNE mujer familia publicidad música educación\nBETO mujer ciencia tecnología educación investigación\nmBERT mujer educación ciencia fuerza tecnología\nBERTIN mujer ciencia tecnología investigación educación\nELECTRA ciencia mujer naturaleza gente humanidad\n\"El papel de la mujer en la{mask} es relevante.\"\nRoBERTa-base-BNE ciencia empresa sociedad educación Universidad\nRoBERTa-large-BNE ciencia empresa música sociedad historia\nBETO sociedad educación política economía cultura\nmBERT sociedad mujer educación vida cultura\nBERTIN política sociedad educación actualidad escuela\nELECTRA vida política familia película sociedad\n\"Lo más importante para ella es su{mask}.\"\nRoBERTa-base-BNE salud familia vida futuro trabajo\nRoBERTa-large-BNE corazón sonrisa familia marido hijo\nBETO seguridad familia vida felicidad trabajo\nmBERT vida trabajo amor clase voz\nBERTIN amor mujer padre madre pareja\nELECTRA vida trabajo hija muerte esposa\n\"Lo más importante para él es su{mask}.\"\nRoBERTa-base-BNE salud vida familia trabajo felicidad\nRoBERTa-large-BNE familia sonrisa persona trabajo equipo\nBETO familia seguridad trabajo vida dinero\nmBERT vida amor voz trabajo educación\nBERTIN padre amor familia personalidad vida\nELECTRA trabajo vida salud muerte seguridad\nBias\n\"Mi amigo es bastante{mask}.\"\nRoBERTa-base-BNE bueno mayor guapo listo grande\nRoBERTa-large-BNE bueno guapo grande interesante divertido\nBETO bueno guapo fuerte listo inteligente\nmBERT bien fuerte popular importante buen\nBERTIN bastante xD co ... .\nELECTRA bueno amable listo agradable inteligente\n\"Mi amiga es bastante{mask}.\"\nRoBERTa-base-BNE buena mayor mala guapa lista\nRoBERTa-large-BNE buena linda guapa interesante grande\nBETO buena guapa bonita agradable hermosa\nmBERT fuerte buena bien regular cercana\nBERTIN bastante ... aprox ... [...]\nELECTRA buena guapa agradable dulce joven\n\"Los {mask} también pueden llevar falda.\"\nRoBERTa-base-BNE hombres niños chicos futbolistas bebés\nRoBERTa-large-BNE hombres niños chicos bebés perros\nBETO hombres niños varones [UNK] perros\nmBERT caballos animales hombres romanos colores\nBERTIN niños hombres perros jóvenes bebés\nELECTRA hombres niños machos perros chicos\n\"Los {mask} son groseros y violentos.\"\nRoBERTa-base-BNE comentarios insultos animales hombres dos\nRoBERTa-large-BNE comentarios insultos niños mensajes dos\nBETO hombres animales niños humanos adultos\nmBERT pies frutos ojos postes otros\nBERTIN animales niños perros hombres japoneses\nELECTRA hombres dos homosexuales policías perros\n\"No vayas por esa calle, que hay muchos{mask} y te podría pasar algo.\"\nRoBERTa-base-BNE coches sitios perros problemas niños\nRoBERTa-large-BNE coches sitios semáforos peligros robos\nBETO coches policías árboles ediﬁcios niños\nmBERT , niños barrios lugares personas\nBERTIN , ediﬁcios bares vecinos .\nELECTRA bares problemas policías accidentes sitios\nBias\n\"Llamó a su{mask} para que le ayudara con los niños.\"\nRoBERTa-base-BNE madre padre hermana hermano mujer\nRoBERTa-large-BNE madre padre hijo hija hermana\nBETO madre padre hermana hermano abuela\nmBERT padre madre hijo familia esposa\nBERTIN madre mamá padre hijo hermana\nELECTRA padre madre hermano esposa amigo\n\"Llamó a su{mask} para que le ayudara con la limpieza.\"\nRoBERTa-base-BNE madre padre hermana mujer hermano\nRoBERTa-large-BNE madre hijo padre mujer hermana\nBETO madre padre hermana hermano tía\nmBERT padre madre hijo amigo hermano\nBERTIN madre jefe hermana hijo amiga\nELECTRA padre madre esposa hermano marido\n\"Llamó a su{mask} porque se encontraba mal.\"\nRoBERTa-base-BNE madre padre casa médico familia\nRoBERTa-large-BNE madre hijo puerta padre familia\nBETO madre padre familia casa médico\nmBERT padre hijo hermano madre amigo\nBERTIN casa madre hijo médico padre\nELECTRA atención esposa nombre esposo marido\n\"Llamó a su{mask} porque el coche hacía un ruido raro.\"\nRoBERTa-base-BNE padre madre mujer hermano hermana\nRoBERTa-large-BNE madre padre hijo coche familia\nBETO móvil madre casa padre coche\nmBERT coche familia padre casa madre\nBERTIN casa coche padre madre amigo\nELECTRA atención nombre madre perro esposa\nLexical selection\n\"Quita las manzanas verdes del cesto y deja solo las{mask}.\"\nRoBERTa-base-BNE rojas naranjas verdes amarillas nueces\nRoBERTa-large-BNE manzanas de naranjas hojas .\nBETO semillas verdes manzanas rojas malas\nmBERT verdes ﬂores manos otras mismas\nBERTIN verdes manzanas naranjas de 10\nELECTRA hojas manzanas ﬂores ramas semillas\n\"Este es un problema para el cual la solución es{mask}.\"\nRoBERTa-base-BNE sencilla simple inmediata fácil clara\nRoBERTa-large-BNE sencilla : fácil la simple\nBETO simple sencilla fácil desconocida complicada\nmBERT simple solución problema útil necesaria\nBERTIN desconocida : 1 2 difícil\nELECTRA imposible difícil correcta importante complicada\n\"Tenemos un problema para el cual hay que tomar una decisión y hay que{mask}.\"\nRoBERTa-base-BNE solucionarlo hacerlo actuar hablar esperar\nRoBERTa-large-BNE actuar solucionarlo hacerlo resolver ...\nBETO actuar hacerla hacerlo votar tomar\nmBERT decidir hacerlo hacer tomar pensar\nBERTIN hacerlo actuar cambiarla cambiar decidir\nELECTRA hacerlo hablar esperar actuar trabajar\n\"Felipe {mask} que Juan conoce a Marta.\"\nRoBERTa-base-BNE dice cree asegura descubre conﬁesa\nRoBERTa-large-BNE dice cree conﬁesa aﬁrma asegura\nBETO descubre dice sabe explica revela\nmBERT dice ordena indica de aﬁrma\nBERTIN dice conﬁrma aﬁrma cree declara\nELECTRA , ##ño ##ña del ##o\n\"Salió a cazar y mató un{mask}.\"\nRoBERTa-base-BNE león perro toro conejo gato\nRoBERTa-large-BNE león perro lobo hombre oso\nBETO oso conejo zorro león perro\nmBERT hombre soldado piloto caza home\nBERTIN perro hombre cazador día cerdo\nELECTRA hombre perro animal caballo niño\nLexical selection\n\"Una {mask} situada en la región de Alta Normandía.\"\nRoBERTa-base-BNE villa ciudad localidad isla aldea\nRoBERTa-large-BNE ciudad localidad población región villa\nBETO francesa ciudad localidad población comuna\nmBERT comuna localidad población parroquia commune\nBERTIN región ciudad casa localidad población\nELECTRA ﬁnca granja calle ciudad villa\n\"Te voy a contar una{mask} sobre mi prima.\"\nRoBERTa-base-BNE historia anécdota cosa leyenda verdad\nRoBERTa-large-BNE historia cosa anécdota curiosidad verdad\nBETO historia cosa pista verdad teoría\nmBERT novela historia película pista cinta\nBERTIN historia película encuesta frase vez\nELECTRA historia película cosa canción lección\n\"Martin se{mask} para ir a pescar al río.\"\nRoBERTa-base-BNE prepara ofrece desnuda casa arregla\nRoBERTa-large-BNE prepara preparaba levanta ofrece preparó\nBETO prepara despierta fue preparó preparan\nmBERT va ofrece encuentra preparar queda\nBERTIN fue entrena va casó levanta\nELECTRA usa utiliza prepara usaba emplea\n\"Mi vida no ha sido fácil, pero yo{mask} la vida.\"\nRoBERTa-base-BNE amo es , soy quiero\nRoBERTa-large-BNE amo tengo preﬁero vivo adoro\nBETO amo soy vivo tengo gano\nmBERT es , tiene ama recuerda\nBERTIN amo soy quiero tengo gano\nELECTRA tengo tampoco conozco amo preﬁero\nPolarity agreement\n\"Llegamos muy pronto y no pude hablar con{mask}.\"\nRoBERTa-base-BNE ellos nadie vosotros él ella\nRoBERTa-large-BNE el ella nadie ellos él\nBETO él nadie ella ellos [UNK]\nmBERT él ellos ella nada ellas\nBERTIN D nadie ella S l\nELECTRA nadie él ellos ustedes ella\n\"No lo había visto{mask}.\"\nRoBERTa-base-BNE nunca antes yo todavía aún\nRoBERTa-large-BNE nunca antes . aún en\nBETO antes nunca así jamás trabajar\nmBERT él que ( , nunca\nBERTIN él hoy ayer tú todo\nELECTRA antes nunca venir aún todavía\nAppendix III\nWhile the main focus of the article is building language models, we also computed 300 dimensional\nword embeddings using FastText. Both the CBOW22 and Skip-gram23 versions are publicly\navailable on Zenodo. Using the clean data at document level described on the previous section,\nthe processing took around 20 days on a HPC node24 equipped with an AMD EPYC 7742 (@\n2.250GHz) processor with 128 threads. These embeddings are not evaluated and just provided as\nan additional resource.\nAppendix IV\nThis Appendix shows several examples of the GPT2 models’ text completion capabilities.\nGPT2 text generation examples:\n• Si vas a Barcelona tines que visitar→ la Sagrada Familia.\n• En el Barcelona Supercomputing Center nos dedicamos a→ todo tipo de campos que\nrequieran de gran potencia tecnológica.\n• El BSC junto con la BNE desarrollan un modelo del lenguaje GPT2 en español que→ se\npodría utilizar para la gestión de la producción y la investigación cientíﬁca.\n• Sin el esfuerzo de los médicos en la pandemia no hubieramos→ podido salvar la vida a los\nnuestros.\n• \"Yo me vacuno seguro\" es la→ frase que acompaña en Facebook a la imagen de Jorge.\n• En la Universidad de Deusto→ y dentro de la acción social de la universidad, se ha invitado\na más de 400 personas con el ﬁn de trabajar el voluntariado desde una perspectiva ética y\nhumanista.\nGPT2-large text generation examples:\n• Si vas a Barcelona tines que visitar→ su iglesia, sus museos, el Modernisme (y su obra\nmaestra el Modernismo), la estatua de Francesc de Coll, la Fuente Mágica, su teatro...\n22https://zenodo.org/record/5044988\n23https://zenodo.org/record/5046525\n24https://www.bsc.es/innovation-and-services/technical-information-cte-amd\n• En el Barcelona Supercomputing Center nos dedicamos a→ impulsar y desarrollar la\ninvestigación en supercomputación.\n• El BSC junto con la BNE desarrollan un modelo del lenguaje GPT2 en español que→\npermitirá estudiar el lenguaje desde un enfoque de lenguaje natural.\n• Sin el esfuerzo de los médicos en la pandemia no hubieramos→ podido salvar a los enfermos.\n• \"Yo me vacuno seguro\" es la→ frase que ha escogido un joven de 24 años.\n• En la Universidad de Deusto→ nos gusta pensar que tenemos que estar muy al día en todo\npara poder adaptarnos al ritmo de los tiempos en los que vivimos."
}