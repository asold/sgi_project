{
  "title": "Variational Monte Carlo with large patched transformers",
  "url": "https://openalex.org/W4392647353",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3009257786",
      "name": "Kyle Sprague",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2790821998",
      "name": "Stefanie Czischek",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A3009257786",
      "name": "Kyle Sprague",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2790821998",
      "name": "Stefanie Czischek",
      "affiliations": [
        "University of Ottawa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2419175238",
    "https://openalex.org/W3104481216",
    "https://openalex.org/W2787088079",
    "https://openalex.org/W3214275744",
    "https://openalex.org/W3035785882",
    "https://openalex.org/W4229366182",
    "https://openalex.org/W4280562010",
    "https://openalex.org/W3046632927",
    "https://openalex.org/W4296836560",
    "https://openalex.org/W2940425080",
    "https://openalex.org/W3095052246",
    "https://openalex.org/W6802709160",
    "https://openalex.org/W2898121159",
    "https://openalex.org/W2980744647",
    "https://openalex.org/W2995786632",
    "https://openalex.org/W2953495237",
    "https://openalex.org/W2916528205",
    "https://openalex.org/W2914465168",
    "https://openalex.org/W3215598778",
    "https://openalex.org/W2789327225",
    "https://openalex.org/W2951696038",
    "https://openalex.org/W3211221363",
    "https://openalex.org/W4386074054",
    "https://openalex.org/W2935584311",
    "https://openalex.org/W3134141057",
    "https://openalex.org/W4361297350",
    "https://openalex.org/W3163465248",
    "https://openalex.org/W4321491582",
    "https://openalex.org/W4379390793",
    "https://openalex.org/W4376122999",
    "https://openalex.org/W4391023901",
    "https://openalex.org/W4310278867",
    "https://openalex.org/W2997289813",
    "https://openalex.org/W4288049312",
    "https://openalex.org/W2014586701",
    "https://openalex.org/W2042458092",
    "https://openalex.org/W2548871540",
    "https://openalex.org/W2775135308",
    "https://openalex.org/W3012330717",
    "https://openalex.org/W3121696576",
    "https://openalex.org/W3117469420",
    "https://openalex.org/W3115562073",
    "https://openalex.org/W3164415272",
    "https://openalex.org/W4317501556",
    "https://openalex.org/W4280555353",
    "https://openalex.org/W3179964333",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2750458571",
    "https://openalex.org/W4287122719",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3035965352",
    "https://openalex.org/W2011301426",
    "https://openalex.org/W3099878876",
    "https://openalex.org/W4226156382",
    "https://openalex.org/W3099072354",
    "https://openalex.org/W3104941540",
    "https://openalex.org/W3100009607",
    "https://openalex.org/W3180039530"
  ],
  "abstract": "Abstract Large language models, like transformers, have recently demonstrated immense powers in text and image generation. This success is driven by the ability to capture long-range correlations between elements in a sequence. The same feature makes the transformer a powerful wavefunction ansatz that addresses the challenge of describing correlations in simulations of qubit systems. Here we consider two-dimensional Rydberg atom arrays to demonstrate that transformers reach higher accuracies than conventional recurrent neural networks for variational ground state searches. We further introduce large, patched transformer models, which consider a sequence of large atom patches, and show that this architecture significantly accelerates the simulations. The proposed architectures reconstruct ground states with accuracies beyond state-of-the-art quantum Monte Carlo methods, allowing for the study of large Rydberg systems in different phases of matter and at phase transitions. Our high-accuracy ground state representations at reasonable computational costs promise new insights into general large-scale quantum many-body systems.",
  "full_text": "communicationsphysics Article\nhttps://doi.org/10.1038/s42005-024-01584-y\nVariational Monte Carlo with large patched\ntransformers\nCheck for updates\nKyle Sprague & Stefanie Czischek\nLarge language models, like transformers, have recently demonstrated immense powers in text and\nimage generation. This success is driven by the ability to capture long-range correlations between\nelements in a sequence. The same feature makes the transformer a powerful wavefunction ansatz that\naddresses the challenge of describing correlations in simulations of qubit systems. Here we consider\ntwo-dimensional Rydberg atom arrays to demonstrate that transformers reach higher accuracies than\nconventional recurrent neural networks for variational ground state searches. We further introduce\nlarge, patched transformer models, which consider a sequence of large atom patches, and show that\nthis architecture signiﬁcantly accelerates the simulations. The proposed architectures reconstruct\nground states with accuracies beyond state-of-the-art quantum Monte Carlo methods, allowing for\nthe study of large Rydberg systems in different phases of matter and at phase transitions. Our high-\naccuracy ground state representations at reasonable computational costs promise new insights into\ngeneral large-scale quantum many-body systems.\nThe advent of artiﬁcial neural network quantumstates marks a turn in the\nﬁeld of numerical simulations for quantum many-body systems1–6.S i n c e\nthen, artiﬁcial neural networks are commonly used as a general wave-\nfunction ansatz toﬁnd ground states of a given Hamiltonian1,7–9,t or e c o n -\nstruct quantum states from a set of projective measurements2,3,10–16,o rt o\nmodel dynamics in open and closed quantum systems1,17–21.T h ep o w e r sa n d\nlimitations of different network architectures, such as restricted Boltzmann\nmachines\n1–3,9,12,15,22,23, recurrent neural networks (RNNs)7,8,13,24,25,o rt h e\nPixelCNN26, have been widely explored on several physical models. In\naddition, modiﬁed network architectures27, the explicit inclusion of\nsymmetries13,28–30, and the pre-training on a limited amount of measurement\ndata8,31 have shown improved performances.\nA particularly promising choice are autoregressive neural networks\nsuch as the PixelCNN26 and RNNs7,8,13,28, which canﬁnd ground states and\nreconstruct quantum states from data with high accuracies. These models\nconsider qubit systems in sequential order, providing an efﬁcient wave-\nfunction encoding. However, these setups experience limitations for systems\nwith strong correlations between qubitsfar apart in the sequence, which, for\nexample, happens for two-dimensional qubit systems7,8,26.\nSimilar to the RNN or PixelCNN approaches, transformer (TF)\nmodels32 can be used as a wavefunction ansatz by considering a sequence of\nqubits14,33–38 or for simulating quantum dynamics39,40. Due to their non-\nrecurrent nature and the ability to highlight the inﬂuence of speciﬁcp r e v i o u s\nsequence elements, TF models perform better at covering long-range\ncorrelations32, promising to overcome the limitations of RNNs and\nPixelCNNs34,35. In this work, we analyze the performance of the TF wave-\nfunction ansatz for variational groundstate searches and observe improved\naccuracies in the representation of quantum states compared to the RNN\napproach.\nInspired by the introduction of the vision transformer, which enables\nthe efﬁcient application of TF models for image processing and generation\ntasks41, and by previous works in theﬁeld25,34,35,w es t u d yR N Na n dT F\nmodels that consider sequences of patches of qubits. This approach reduces\nthe sequence length and thus the computational cost signiﬁcantly, while\naccurately capturing correlations within the patch. For further improve-\nments we introduce large, patched transformers (LPTF) consisting of a\npowerful patched TF model followed by a computationally efﬁcient patched\nRNN that breaks large inputs into smaller sub-patches. This architecture\nallows for an efﬁcient consideration of large patches in the input sequence of\nthe TF network, further reducing the sequence length.\nWe benchmark the LPTF architecture on two-dimensional arrays of\nRydberg atoms, whose recently demonstrated experimental controllability\nmakes them promising candidates for high-performance quantum com-\nputation and quantum simulation\n42–52. Furthermore, quantum Monte Carlo\nmethods can model Rydberg atom systems52,53, and we use such simulations\nto determine the performance of different network models.\nAnalyzing different shapes and sizes of input patches, we demonstrate\nthat LPTFs can represent ground states of Rydberg atom arrays with\nDepartment of Physics, University of Ottawa, Ottawa, Ontario K1N 6N5, Canada.\ne-mail: stefanie.czischek@uottawa.ca\nCommunications Physics|            (2024) 7:90 1\n1234567890():,;\n1234567890():,;\naccuracies beyond the RNN ansatz and traditional quantum Monte Carlo\nsimulations, while requiring reasonable computational costs. Our results are\nconsistent in different phases of matter and at quantum phase transitions.\nWhile we show that LPTFs can signiﬁcantly improve numerical investiga-\ntions of the considered Rydberg models, the introduced network model can\nsimilarly be applied to general qubit systems. The results presented in this\nwork propose that the LPTF model cansubstantially advance numerical\nstudies of quantum many-body physics.\nResults\nRydberg atom arrays\nRydberg atoms, which we use as a qubit model to benchmark our numerical\napproaches, can be prepared in the ground state∣g\n/C11\nand in a highly excited\n(Rydberg) state∣ri\n42–46,48,49.W es p e c iﬁcally consider the atoms arranged on\nsquare lattices of different system sizes, as illustrated in Fig.1a.\nThe system of N = L × L atoms is described by the Rydberg\nHamiltonian42,43,\nbH ¼/C0 Ω\n2\nXN\ni¼1\n^σx\ni /C0δ\nXN\ni¼1\n^ni þ\nX\ni;j\nVi;j^ni^nj; ð1Þ\nwith the detuningδ and the Rabi oscillation with frequencyΩ generated by\nan external laser driving. Here we use the off-diagonal operator^σx\ni ¼\n∣g\n/C11\ni rh ∣i þ ∣rii g\n/C10\n∣i and the occupation number operator^ni ¼ ∣rii rh ∣i.T h e\nlast term in the Hamiltonian describes a van-der-Waals interaction between\natoms at positionsr\ni and rj,w i t hVi;j ¼ ΩR6\nb=∣ri /C0rj∣6,a n dR y d b e r g\nblockade radiusRb. We further choose the lattice spacinga =1 .B yt u n i n g\nthe free parameters in the Rydberg Hamiltonian, the system can be prepared\nin various phases of matter, separated by different kinds of phase\ntransitions\n46–48,51,52. The Rydberg Hamiltonian is stoquastic54,r e s u l t i n gi na\npositive and real-valued ground-state wavefunction44,48,50. More details on\nRydberg atom arrays are provided in the Methods section.\nRecurrent neural networks and transformers\nRecurrent neural networks (RNNs) provide a powerful wavefunction ansatz\nthat can variationallyﬁnd ground state representations of quantum many-\nbody systems\n4,6–8,13,25,28,29. For this, the possibility to naturally encode prob-\nability distributions in RNNs allows the representation of squared wave-\nfunction amplitudesjΨ σðÞ j 2. Samples drawn from the encoded distribution\ncorrespond to state conﬁgurations that can be interpreted as outputs of\nprojective measurements, as illustrated in Fig.1d.\nTo represent the wavefunction amplitudesΨ σðÞ ¼ h σjΨi of a qubit\nsystem, such as an array of Rydberg atoms, a sequential order is deﬁned over\nthe system. Each atom is iteratively used as an input to the RNN cell, the core\nelement of the network structure which we choose to be a Gated Recurrent\nUnit (GRU)\n55 inspired by6,7. In addition, the RNN cell receives the state of\ninternal hidden units as input. This state is adapted in each iteration and\npropagated over the input sequence, generating a memory effect. The net-\nwork outputp\nRNN σijσ<i; W\n/C0/C1\nat each iteration can be interpreted as the\nprobability of the next atomσi being in either the ground or the Rydberg\nstate, conditioned on the conﬁguration of all previous atomsσ<i in the\nsequence, with variational weightsW i nt h eR N Nc e l l .S e eF i g .1da n dt h e\nMethods section for more details. From this output, the stateσi of the next\natom is sampled and used autoregressively as input in the next RNN\niteration, as illustrated in Fig.1b\n4,6,7. We then train the RNN such that it\napproximates a target stateΨ σðÞ ,\nΨRNN σ; WðÞ ¼\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nYN\ni¼1\npRNN σijσ<i; W\n/C0/C1\nvuut\n¼\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\npRNN σ; WðÞ\np\n≈ Ψ σðÞ :\nð2Þ\nWhile we focus on positive, real-valued wave functions in this work, RNNs\ncan represent general wave functions by including complex phases as a\nsecond network output\n7. The global phase of the encoded state is then\nexpressed as the sum over single-qubit phases.\nThe RNN has shown high accuracies for representing ground states of\nvarious quantum systems. However, its sequential nature and the encoding\nof all information in the hidden unit state pose a challenge for capturing\nlong-range correlations\n4,6–8,13,28,29. Here we refer to correlations between\natoms that appear far from each other in the RNN sequence but not\nnecessarily in the qubit system. Alternative autoregressive network models,\nsuch as the PixelCNN, experience similar limitations. These models cover\ncorrelations via convolutions with a kernel of a speciﬁcs i z e .H o w e v e r ,d u et o\nincreasing computational costs, kernel sizes are commonly chosen rather\nsmall, so that the PixelCNN is as well limited to capturing only local cor-\nrelations in qubit systems\n26.S p e c iﬁc RNN structures that better match the\nlattice structure in the considered model, such as two-dimensional RNNs for\ntwo-dimensional quantum systems, can overcome this limitation7,25,28,29.A n\nalternative approach to improve the representation of long-range correla-\ntions is to use transformer (TF) architectures as a wavefunction ansatz14,33–35.\nThese provide a similar autoregressive behavior but do not have a recurrent\nsetup and naturally capture all-to-all interactions32.\nWhile autoregressively using the states of individual atoms as\nsequential input similar to the RNN, a masked self-attention layer in the\nTF setup provides trained connections to all previous elements in the\nsequence\n32. See Fig.1e and the Methods section for more details. These\ntrainable connections generate all-to-all interactions between the atoms\nin the system and allow the highlighting of high-impact connections or\nstrong correlations. This setup thus proposes to represent strongly cor-\nrelated quantum systems with higher accuracy than the RNN\nmodel\n14,34,35. As illustrated in Fig.1c, the TF model outputs probability\ndistributions which provide an autoregressive wavefunction ansatz\nΨ\nTF σ; WðÞ ¼\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\npTF σ; WðÞ\np 14,34,35, as further explained in the Methods\nsection. Similarly to the RNN, the TF network can represent complex-\nvalued wave functions by adding a second output representing the single-\nqubit phases\n7.\nIn Fig.2a, b, we compare the performance of RNNs (blue) and TFs\n(orange) when representing ground states of Rydberg arrays withN =8×8\n(a)a n dN =1 6×1 6(b)a t o m s .H e r ew eﬁx Rb =7 1/6 ≈ 1.383 andΩ = δ =1 ,\nwhich brings us into the vicinity of the transition between the disordered\nand the striated phase48. We variationally train the network models by\nminimizing the energy expectation value, corresponding to a variational\nMonte Carlo method4,6,23,56, see Methods section. If not stated otherwise, the\nenergy expectation values in this work are evaluated onNs = 512 samples\ngenerated from the network, which we consider in mini batches of\nK = 256 samples. We obtained satisfactory results withdH =1 2 8 h i d d e n\nneurons in the RNN and the equivalent embedding dimensiondH = 128 in\nt h eT Fm o d e l .T ob e n c h m a r kt h ep e r f o r m a n c eo ft h et w om o d e l s ,w es h o w\nthe difference between the ground state energiesH\nQMC obtained from\nquantum Monte Carlo (QMC) simulations at zero temperature53,a n dt h e\nenergy expectation value,\nhEi¼ 1\nNs\nXNs\ns¼1\nHloc σs\n/C0/C1\n; ð3Þ\nextracted from network samplesσs. Here we use the local energy,\nHloc σs\n/C0/C1\n¼ hσsj ^HjΨWi\nhσsjΨWi ; ð4Þ\nwith∣ΨW\n/C11\ndenoting the wavefunction encoded in either the RNN or the TF\nnetwork, as discussed in the Methods section. In the QMC simulations, we\nuse the stochastic series expansion approach presented in\n53 and evaluate the\nexpectation value onNs =7×1 04 samples generated from seven indepen-\ndent sample chains. Both system sizes show that TFs converge to the ground\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 2\nstate energy within fewer training iterations than the RNN. Additionally, for\nthe larger system in Fig.2b, TFs outperform RNNs signiﬁcantly and reach\nhigher accuracies in the ground stateenergy. This result demonstrates the\nexpected improved performance.\nWe, however, alsoﬁnd that this enhancement comes at the cost of\nincreased computational runtimesτ in hours (h) for 2 × 104 training itera-\ntions. Withτ ≈ 1.5h andτ ≈ 16h forN =8×8a n dN = 16 × 16 atoms, RNNs\nprocess much faster than TFs withτ ≈ 9.5h and τ ≈ 144h, respectively.\nFigure2a, b suggest stopping the TF training after fewer iterations due to the\nfaster convergence, but the computational runtime is still too long to allow\nscaling to large system sizes.\nWe obtained QMC runtimes asτ ≈ 18h forN =8×8a n dτ ≈ 24h for\nN = 16 × 16 for a single run generatingNs =1 04 samples, showing a more\nefﬁcient scaling with system size than the network simulations. This\nbehavior can be understood when considering the scaling of the compu-\ntational cost for generating an individual sample, which isO NðÞ for the\nRNN and QMC, andO N2/C0/C1\nfor the TF. In addition, the network models\nneed to evaluate energy expectation values in each training iteration, which\ncomes at complexityO N\n2/C0/C1\nfor the RNN and at complexityO N3/C0/C1\nfor the\nTF, see Methods for more details. However, due to its non-recurrent setup,\nthe TF enables a parallelization of the energy expectation value evaluation,\nwhich is not possible for the RNN ansatz, as further discussed in the\nMethods. The computational complexity for QMC scales asO NðÞ for both\nsampling and energy evaluation\n53. Thus, while the QMC requires longer\nruntimes than the RNN for small system sizes, it is expected to outperform\nboth the RNN and the TF for larger systems.\nPatched inputs\nTo address the exceeding computational runtime of TF models, we take\ninspiration from the vision transformer\n41 and consider patches of atoms as\ninputs to both considered network architectures, as illustrated in Fig.1d, e.\nThis reduces the sequence length toN/p elements for patch sizep,l e a d i n gt o\na sampling complexity ofO N=p\n/C0/C1\nfor the patched RNN andO N2=p2/C0/C1\nfor\nthe patched TF model, as well as an energy evaluation complexity of\nO N\n2=p\n/C0/C1\nand O N3=p2/C0/C1\n, respectively.\nWe ﬁrst use patches ofp =2×2a t o m s .T h en e t w o r ko u t p u ti st h e na\nprobability distribution over the 2p = 16 states the atoms in the patch can\ntake, from which the next patch is sampled and used autoregressively as\ninput in the following iteration. As demonstrated in previous works\n25,34,35,\nthis signiﬁcantly reduces the computational runtime due to the shorter\nsequence length. In addition, we expect it to capture correlations between\nneighboring atoms with higher accuracies by directly encoding them in the\noutput probabilities. The patched models can also be modiﬁed to include\ncomplex phases as a second network output, which then correspond to the\nsum of phases of individual qubits in the patch\n7.\nFigure2 c and d show the results for the sameN =8×8a n dN =1 6×1 6\natom Rydberg array ground states as in panelsa and b, using the patched\nRNN (green) and the patched TF setup (red) withp =2×2 .T h en e t w o r k\nh y p e r p a r a m e t e r sa r et h es a m ea si nt h eR N Na n dt h eT Fn e t w o r ki na andb.\nThe computational runtime reduces signiﬁcantly toτ ≈ 0.5h andτ ≈ 3h,\nusing the patched RNN and the patched TF model forN =8×8a t o m s ,a n d\nto τ ≈ 2h and τ ≈ 28h, respectively, forN = 16 × 16 atoms. Convergence\nfurther happens within fewer training iterations than for the unpatched\nnetworks, and all representations reach energy values within the QMC\nerrors. We even observe energies below the QMC results, which always\nremain within the QMC uncertainties and thus do not violate the variational\nprinciple which we expect to be satisﬁed for the number of samples we use to\nevaluate energy expectation values and for the small variances we observe\n56.\nThese energies propose that the patched networksﬁnd the ground state with\nbetter accuracy than the QMC simulations usingNs =7×1 04 samples. The\nQMC accuracy can be further increased by using more samples, where the\nuncertainty decreases as/ 1= ﬃﬃﬃﬃﬃﬃN\ns\np for uncorrelated samples53. However,\nsamples in a single QMC chain are correlated, resulting in an uncertainty\nscaling /\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nτauto=Ns\np\nwith autocorrelation timeτauto depending on the\nFig. 1 | Illustrating different network models. aSquare lattice ofN = 4 × 4 Rydberg\natoms randomly occupying the ground state (white) and the Rydberg state (blue).\nDash-colored squares indicate patches used as network inputs.b Recurrent neural\nnetwork (RNN) processing sequence. The RNN cell iteratively receives input\nsequence elementsσ\ni together with a hidden state. At each iteration, the output is\nused as the next input. The indexp\n/C0/C1\ndenotes the input size in the patched RNN.\nc Autoregressive transformer (TF) processing sequence, similar to the RNN in b. The\nmulti-headed masked self-attention layer generates weighted connections to pre-\nvious input sequence elements. For simplicity, we only include the feed-forward\nlayers (FFLs) in the scheme.d Single patched RNN iteration on inputs of patch size\np = 2 × 2 [indicated with 4ðÞ ]. A softmax function creates a probability distribution\np\nRNN over all possible patch states conditioned on previous sequence elements. The\nstate of the next patch is sampled and used as next input state.e Single patched TF\niteration. The input patch is embedded into a state of dimensiondH, and a positional\nencoding keeps track of the sequence order. The signal is sent into the transformer\ncell (gray) which we generate similar to\n32 and applyT times independently. The\noutput of the transformer cells is used like in the patched RNN to sample the next\ninput. f Single large, patched transformer (LPTF) iteration for patch sizep =4×4\n[indicated with 16ðÞ ] and sub-patch sizeps = 2 × 2. A patched TF model receives a\nlarge input patch, and the transformer cell output is propagated as a hidden statehTF\nto a patched RNN. The patched RNN autoregressively constructs the input patch of\nthe next LPTF iteration, reducing the output dimension. See Methods for more\ndetails on the network models.\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 3\nevaluated observable53. Even though the computational cost of QMC scales\nlinearly with the sample chain sizeNs a n di st h u sm o r ee fﬁcient than the\nRNN or the TF approach, which require the generation ofNs samples in\neach training iteration, we found that reaching higher QMC precisions\nc o m e sa tr u n t i m e st h a te x c e e dt h ep a t c h e dR N Na n dt h ep a t c h e dT Fd u et o\nlong autocorrelation times for large system sizes.\nLarge, patched transformers\nBased on the results withp = 2 × 2, we expect even shorter computational\nruntimes and higher representation accuracies from larger patch sizes.\nHowever, as illustrated in Fig.1d, e, the network output dimension scales\nexponentially with the input patch size, encoding the probability distribu-\ntion over all possible patch states. This output scaling leads to the sampling\ncost scaling asO 2\npN=p\n/C0/C1\nf o rt h ep a t c h e dR N Na n da sO N2=p2 þ 2pN=p\n/C0/C1\nfor the patched TF network, as well as energy evaluation costs scaling as\nO 2pN2=p\n/C0/C1\nand O N3=p2 þ 2pN2=p\n/C0/C1\n, respectively, see Methods. A\nhierarchical softmax approach is often used in image processing to efﬁ-\nciently address this exponential scaling57. Here we introduce large, patched\ntransformers (LPTFs) as an alternative way to enable efﬁcient patch size\nscaling.\nA ss h o w ni nF i g .1f, the LPTF model uses a patched TF setup and passes\nthe TF state into a patched RNN as the initial hidden state. The patched\nRNN splits the input patch into smaller sub-patches of sizep\ns =2×2 ,\nreducing the output of the LPTF model to the probability distribution over\nthe 2ps ¼ 16 sub-patch states, as further discussed in the Methods. The\nsampling complexity for this model is reduced toO N2=p2 þ 2ps N=ps\n/C0/C1\nand the energy evaluation complexity takes the form\nO N3=p2 þ 2ps N2=ps\n/C0/C1\n, as derived in the Methods section. Generally, we\ncan use both the patched RNN and the patched TF architecture as base\nnetwork and subnetwork. We choose this setup here to combine the high\naccuracies reached with the patched TF network for large system sizes with\nthe computational efﬁciency of the patched RNN, which can still accurately\nrepresent small systems (see Fig.2a). Being a combination of a TF network\nand an RNN, the LPTF can similarly be modiﬁed to include complex phases\nas a second network output.\nIn Fig.2c, d, we compare the performance of the LPTF model to the\npreviously considered network architectures, where we choosep =4×4\n(purple) andp = 8 × 8 (brown), withp\ns = 2 × 2, using the same hyperpara-\nmeters for all networks. These models require more training iterations than\nthe patched TF architecture to converge but reach accuracies comparable to\nthe patched RNN and the patched TF network. Even though more training\niterations are required, the computational runtimes are reduced toτ ≈ 1h for\nN =8×8 , p =4×4 , a s w e l l a sτ ≈ 9h and τ ≈ 4.5h for N =1 6×1 6 w i t h\np =4×4a n dp = 8 × 8, respectively. Thus, overall, we obtain convergence\nwithin shorter computational runtime.\nThe observed runtimes are also shorter than QMC runs, even though\nQMC is expected to outperform the network models for large system sizes\ndue to the linear scaling of computational costs withN. However, QMC is\nbased on the generation of a chain of correlated samples. For large system\nsizes, autocorrelation times betweensamples in the chain increase and the\nergodicity of the sampling process is not necessarily guaranteed\n53.S i n c e\nthese limitations do not arise for the exact sampling process in auto-\nregressive ANN methods\n7,26, computationally efﬁcient architectures such as\nthe LPTF are still promising candidates for accurate studies of large quan-\ntum many-body systems.\nFigure 2 e and f show the variancesσ2 EðÞ of the energy expectation\nvalues obtained with all considered network architectures. As expected7,\nthey decrease to zero when converging to the ground state energies. This\nbehavior conﬁrms the accurate ground state reconstruction, while the\nsmoothness of all curves demonstrates stable training processes.\nWe can further increase the patch sizep in the LPTF architecture,\nfrom which we expect even shorter runtimes. However, this also\nincreases the patch size that needs to be reconstructed with the patched\nRNN. We thus expect the accuracy to decrease for largep if we keep\np\ns =2×2 ﬁxed. Figure3a shows the energy difference between QMC and\nLPTF simulations for ground states of Rydberg arrays withN =1 2×1 2\nup toN = 40 × 40 atoms. We keep the parameters atRb =7 1/6, δ = Ω =1 ,\nand evaluate the QMC energies on Ns =7×1 04 samples from seven\nindependent chains53, where the computational cost for QMC scales as\nO NðÞ with the system size. Each LPTF data point corresponds to an\naverage over training iterations 19,000 to 20,000 of ten independently\ntrained networks with the same setup as for Fig.2. We vary the input\npatch size between p =4×4 a n dp = 16 × 16, where we also consider\nrectangular-shaped patches whileﬁxing ps =2×2 . W e e n s u r e t h a t t h e\nsystem size always divides by the input patch size.\nAs expected, the energy accuracies decrease with increasing patch size,\nwhich might result from the limited representational power of the patched\nRNN for large inputp and smallp\ns and from the increased amount of\ninformation that is encoded in each network iteration. Weﬁnd accuracies\nbelow the QMC uncertainty for up top = 8 × 8, which still proposes a\nsigniﬁcant speed-up compared to single-atom inputs in the TF model, see\nFig. 2d. Figure 3b shows the computational runtimes of single training\niteration steps for the different patch and system sizes. Each data point\nshows an average over 2 × 104 training iterations in a single network. We\nﬁnd a rapid decrease in computation times for small patches while we\nobserve convergence to steady times for larger patches. This behavior results\nfrom the increased memory required bylarger patch sizes,w h i c hf o r c e su st o\ndecrease the mini-batch sizeK of samples for the energy evaluation, see\nFig. 2 | Performance of different network architectures on Rydberg atom arrays.\na, bAbsolute energy difference between〈E〉 [Eq. (3)] fromNs = 512 recurrent neural\nnetwork (RNN, blue) and transformer (TF, orange) samples andHQMC from\nNs =7×1 04 quantum Monte Carlo (QMC) samples as a function of network training\niterations. The black dashed line denotes the QMC uncertainty and black-edged data\npoints show the absolute value of energies below QMC results.τ is the total runtime\nin hours (h) for 2 × 10\n4 training iterations using the network in the corresponding\ncolor (blue for RNN, orange for TF) on NVIDIA Tesla P100 GPUs. For the QMC, the\ntotal runtimes were obtained asτ = 18h forN = 8 × 8 atoms andτ = 24h for\nN = 16 × 16 atoms forNs =1 04 samples on a single CPU.c, dSame asa, bfor the\npatched RNN (PRNN, green) and the patched TF model (PTF, red) with patch size\np = 2 × 2, and for the large, patched transformer (LPTF) approach (purple) with\npatch sizep = 4 × 4 and sub-patch sizeps = 2 × 2. ForN = 16 × 16 atoms, we further\nshow LPTF results with patch sizep = 8 × 8 (brown).e, fVariances σ2 EðÞ of energy\nexpectation values for the network architectures considered ina-d.\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 4\nMethods. Smaller mini-batch sizes lead to increased runtimes, which\ncompete with the acceleration from the reduced sequence lengths.\nWe cannotﬁnd a conclusive dependence on the patch shape, with\nr e c t a n g u l a rp a t c h e ss h o w i n gas i m i l a rb e h a v i o ra ss q u a r e dp a t c h e s .T h u s ,\nthe only important factor is the overall patch size, and we conclude that\ninput patches aroundp = 8 × 8 atoms provide a good compromise with\nreduced computation times and high energy accuracies.\nPhases of matter in Rydberg atom arrays\nWe now explore the performance of LPTFs at different points in the Ryd-\nberg phase diagram by varying the detuning fromδ =0t o δ =3a n dﬁxing\nRb =3 1/6 ≈ 1.2, Ω = 1. With this, we drive the system over the transition\nbetween the disordered and the checkerboard phase46,48.T h eo r d e rp a r a -\nmeter for the checkerboard phase is given by the staggered magnetization46,\nσstag ¼\nXN\ni¼1\n/C01ðÞ i ni /C01=2\nN\n/C12/C12/C12\n/C12\n/C12\n/C12/C12/C12\n/C12\n/C12\n*+\n; ð5Þ\nwhere i runs over allN = L × L atoms andn\ni ¼ ∣rii rh ∣i is the occupation\nnumber operator acting on atomi. The expectation value denotes the\naverage over sample conﬁgurations generated via QMC or from the trained\nnetwork.\nFigure 4 shows the staggered magnetization when tuningδ over the\nphase transition, where we compare LPTF and QMC simulations. The\nQMC data points show the average ofN\ns =7×1 05 samples generated from\nseven independent chains53. The LPTF data is averaged over training\niterations 11,000 to 12,000 ofﬁve independently trained networks. We look\nat systems withN =8×8 a n dN = 16 × 16 atoms, choosing patch sizes\np =4×4a n dp = 8 × 8, respectively, withps =2×2 .\nThe LPTF model captures the phasetransition accurately for both\nsystem sizes, overlapping closely with the QMC results for allδ and showing\nsmall uncertainties. In the inset in Fig.4, we plot the absolute difference\nbetween the staggered magnetizations obtained with QMC and LPTFs for\nboth system sizes. The most challenging regime to simulate is atδ ≈ 1.2,\nwhere weﬁnd the phase transition in the main panel. Here the observed\ndifference is ~10\n−2, demonstrating the high accuracies reachable with the\nLPTF approach. In the vicinity of the phase transition, the QMC uncer-\ntainties increase. This behavior is related to long autocorrelation timesτ\nauto\nin the individual sample chains and the uncertainty scaling as\n/\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nτauto=Ns\np 53. The errors in the LPTF simulations remain small here,\ndemonstrating a consistent and accurate outcome in all independent\nnetworks.\nDiscussion\nWe explored the power of transformer (TF) models32 in representing\nground states of two-dimensional Rydberg atom arrays of different sizes by\nbenchmarking them on quantum Monte Carlo simulations\n53.O u rw o r k\nprovides a careful performance comparison of TF models with a recurrent\nneural network (RNN) wavefunction ansatz\n4,6,7, showing that TFs reach\nhigher accuracies, especially for larger system sizes, but require longer\ncomputational runtimes. We accelerate the network evaluation using pat-\nches of atoms as network inputs inspired by the vision transformer41 and\ndemonstrate that these models signiﬁcantly improve computational run-\ntime and reachable accuracies.\nBased on the obtained results, we introduce large, patched transfor-\nmers (LPTFs), which consist of a patched TF network whose output is used\nas the initial hidden unit state of a patched RNN. This model enables larger\ninput patch sizes which are broken down into smaller patches in the patched\nRNN, keeping the network output dimension at reasonable size.\nThe LPTF models reach accuraciesbelow the QMC uncertainties for\nground states obtained with aﬁxed number of samples, while requiring\nsigniﬁcantly reduced computational runtimes compared to traditional\nneural network models. We are further able to scale the considered system\nsizes beyond most recent numerical studies, while keeping the accuracies\nhigh and computational costs reasonable\n8,46,52,53. These observations promise\nthe ability to study the scaling behavior of Rydberg atom arrays to large\nsystem sizes, allowing an in-depth exploration of the underlying phase\nFig. 4 | Staggered magnetization as order parameter.Staggered magnetizationσstag\n[Eq. (5)] obtained with large, patched transformers (LPTFs, squares) and with\nquantum Monte Carlo (QMC, circles) for Rydberg arrays withN = 8 × 8 (blue,\norange) andN = 16 × 16 (green, red) atoms when driving the detuningδ across the\ntransition between the disordered (δ ⪅ 1.2) and the checkerboard (δ ⪆ 1.2) phase.\nFor N = 8 × 8, we use patch sizep = 4 × 4, whilep = 8 × 8 forN = 16 × 16. LPTF data\nis averaged over training iterations 11,000 to 12,000 ofﬁve independent networks\nwith Ns = 512 samples. QMC data is evaluated onNs =7×1 05 samples. Error bars\ndenote the standard error of the sampling mean, where autocorrelation times are\nconsidered in the QMC samples\n53. Inset: Absolute difference between the LPTF and\nQMC data in the main plot forN = 8 × 8 (orange) andN = 16 × 16 (red). Error bars\ndenote the standard error of the sampling mean.\nFig. 3 | Patch size scaling in large, patched transformers (LPTFs). aAbsolute\nenergy difference between〈E〉 [Eq. (3)] evaluated withNs = 512 large, patched\ntransformer (LPTF) samples andHQMC evaluated withNs =7×1 04 quantum Monte\nCarlo (QMC) samples for different system sizesN (colors) as a function of the patch\nsize p. Error bars denote the standard error of the sampling mean. The black dashed\nline indicates the QMC uncertainty, while black-edged data points show absolute\nvalues of energies below the QMC results.b Computational runtime per training\niteration on NVIDIA Tesla P100 GPUs averaged over 2 × 104 iterations of LPTFs as\nin a. Different shapes denote different mini-batch sizesK, withK = 256 for circles,\nK = 128 for up-pointing triangles,K = 64 for squares, andK = 32 for down-pointing\ntriangles, see Methods. Error bars are smaller than the data points.\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 5\ndiagram. While such studies go beyond the scope of this proof-of-principle\nwork, we leave it open for future follow-up works.\nOur results show that the LPTF model performs similarly well in\ndifferent phases of matter in the Rydberg system and accurately captures\nphase transitions. While we focus on Rydberg atom arrays, the introduced\napproach can be applied to general quantum many-body systems, where\ncomplex-valued wave functions can be represented by adding a second\noutput to the autoregressive network architecture as in\n7.W h i l ew ee x p e c tt h e\ninclusion of complex phases to make the training process harder7,m o d -\niﬁcations of the LPTF setup can be explored in future works to study more\ncomplex or larger qubit systems. Such modiﬁcations include larger network\nmodels with more transformer cells, or higher embedding dimensions,\nwhich increase the network expressivity58. Additionally, larger input patch\nsizes can be achieved by including multiple patched RNN and patched TF\ncomponents in the LPTF architecture, which successively reduce the sub-\npatch sizes. We further expect that the performance of LPTFs can be\nenhanced with a data-based initialization, as discussed in\n8,31.\nOur results and possible future improvements promise high-quality\nrepresentations of quantum states in various models and phases of matter at\naffordable computational costs. This prospect proposes signiﬁcant advances\nin the modeling of quantum many-body systems, promising insightful\nfollow-up works exploring new physical phenomena.\nMethods\nRydberg atom arrays\nWe apply our numerical methods on Rydberg atom arrays as an example for\nqubit systems. In state-of-the-art experiments, Rydberg atoms are indivi-\ndually addressed via optical tweezers that allow for precise arrangements on\narbitrary lattices in up to three dimensions44,45,48,49. Fluorescent imaging\ntechniques are then used to perform projective measurements in the Ryd-\nberg excitation basis. Such accurate and well-controlled experimental rea-\nlizations are accompanied by intensivenumerical investigations, which have\nunveiled a great variety of phases of matter, separated by quantum phase\ntransitions, in which Rydberg atom systems can be prepared\n46–48,51,52.T h e\natoms on the lattice interact strongly via the Rydberg many-body Hamil-\ntonian in Eq. (1)\n42,43. The Rydberg blockade radiusRb deﬁnes a region within\nwhich simultaneous excitations of two atoms are penalized.\nT h eg r o u n ds t a t e so ft h i sR y d b e r gH a m i l t o n i a na r ef u l l yd e s c r i b e db y\npositive, real-valued wavefunctions so that the outcomes of measurements\nin the Rydberg occupation basis provide complete information about\nground state wavefunctions\n44,48,50. We can thus model ground state wave-\nfunctions with real-valued neural network model architectures6,7.I nt h i s\nwork, we chooseΩ = 1 and describe the system in terms of the detuningδ\nand the Rydberg blockade radiusRb. We further consider square lattices of\nN = L × L atoms with lattice spacinga = 1 and open boundary conditions.\nRecurrent neural network quantum states\nRecurrent neural networks (RNNs) are generative network architectures\nthat are optimized to deal with sequential data55,59. They naturally encode a\nprobability distribution and enable efﬁcient sample data generation. As\nillustrated in Fig.1b, d, the RNN input is given by individual elementsσi of\ndimension dI from a given data sequenceσ,a n dah i d d e ns t a t ehi of\ndimensiondH. We use the initial input statesσ0 = 0 and h0 = 0.T h r o u g h o u t\nthis work, we choosedH = 128. The input is processed in the RNN cell,\nwhere non-linear transformations deﬁned via variational parametersW are\napplied. Here we use the Gated Recurrent Unit (GRU)55 as RNN cell, which\nis applied at each iteration with shared weights59.\nWe then apply two fully connected projection layers on the hidden\nstate, the ﬁrst followed by a rectiﬁed linear unit (ReLU) activation\nfunction and the second followed by a softmax activation function (not\nlayers are not shown in Fig.1d). This setup generates an output vector of\ndimension dO which is interpreted as a probability distribution over all\npossible output values7. The hidden state conﬁguration is propagated\nover the input sequence encoding information of previous inputs and\ngenerating a memory effect. This setup conditions the output probability\non all previous sequence elements, p\nRNN σiþ1∣σi; ... ; σ1; W\n/C0/C1\n,f r o m\nwhich an output state is sampled. Here, we use this output as the input\nelement σ\ni+1 of the next iteration, running the network in an auto-\nregressive manner. In this case, the joint probability of the generated\nsequence is given bypRNN σ; WðÞ ¼ Q\ni pRNN σi∣σi/C01; ... ; σ1; W\n/C0/C1 7.\nTo use the RNN as a wavefunction ansatz to represent quantum states,\nwe consider the quantum system as a sequence of qubits, sampling the state\nof one qubit at a time and using it as the input in the next RNN iteration\n4,6,7.\nThe hidden state propagation capturesc o r r e l a t i o n si nt h eq u b i ts y s t e mb y\ncarrying information about previously sampled qubit conﬁgurations. We\nthen interpret the probability distribution encoded in the RNN as the\nsquared wavefunction amplitude of the represented quantum state,\np\nRNN σ; WðÞ ¼ ∣hσ∣ΨWi∣2 ¼ ∣ΨRNN σ; WðÞ ∣2. This ansatz can model the\ncomplete information of ground states in the considered Rydberg Hamil-\ntonian, Eq. (1). Samples generated from the RNN then correspond to out-\ncomes of projective measurements in the computational basis and can be\nused to estimate expectation values of general observables^O1,2,4,6,7,23,\nhΨW∣^O∣ΨWi¼\nX\nσ;σ0fg\nΨ/C3\nRNN σ; WðÞ ΨRNN σ0; WðÞ × hσ∣^O∣σ0i\n¼\nX\nσfg\n∣ΨRNN σ; WðÞ ∣2Oloc σ; WðÞ\n≈ 1\nNs\nX\nσs/\npRNN σ;WðÞ\nOloc σs; W\n/C0/C1\n;\nð6Þ\nwhere we introduce the local observable,\nOloc σs; W\n/C0/C1\n¼\nσs∣^O∣ΨW\nDE\nσs∣ΨW\n/C10/C11\n¼\nX\nσ0fg\nσs∣^O∣σ0\nDE ΨRNN σ0; WðÞ\nΨRNN σs; W\n/C0/C1 :\nð7Þ\nThis local observable is evaluated and averaged overNs samplesσs generated\nfrom the RNN. Toﬁnd the ground state representation of a given Hamil-\ntonian in the RNN, we use a gradient descent training algorithm to mini-\nmize the energy expectation valuehEi¼h ^Hi, which can be similarly\nevaluated using samples from the RNN as stated in Eq. (3)a n dE q .(4)\n4,6,7,23,56.\nWe train the RNN using the Adam optimizer with parametersβ1 =0 . 9 ,\nβ2 = 0.999, and learning rateΔ = 0.0005.\nThe GRU cell has three internal weight matrices of dimensiondI × dH,\nwith input dimensiondI and hidden unit dimensiondH, and three internal\nweight matrices of dimensiondH × dH. It furthermore has six internal bias\nvectors of sizedH, and we add two fully connected layers with weight\nmatrices of dimensiondH × dH and dH × dO and biases of sizedH and dO,\nrespectively, to obtain the desired RNN output vector with output dimen-\nsion d\nO\n7,55.S i n g l e - q u b i ti n p u t sg i v edI =1a n ddO =2a sw eu s eao n e - h o t\nencoded output. Together withdH = 128 as chosen in this work, this leads to\nat o t a lo f\n3 dI × dH\n/C0/C1\nþ 4 dH × dH\n/C0/C1\nþ 7dH þ dH × dO\n/C0/C1\nþ dO ¼ 67; 074 ð8Þ\ntrainable network parameters.\nTransformer quantum states\nTransformer (TF) models can be applied to sequential data similarly to\nRNNs. Such models do not include a recurrent behavior but are based on\nself-attention, which provides access to all elements in the sequence and\nenables the dynamical highlighting of salient information. We use the TF\nmodel as introduced in\n32 and restrict it to the encoder part only14,33–35.\nAs illustrated in Fig.1e, the TF modelﬁrst embeds the given input\nvector. This embedding corresponds to a linear projection of the input\nvector of dimensiondI to a vector of embedding dimensiondH with\ntrainable parametersWI. As a next step, the positional encoding matrix is\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 6\nevaluated and added to the embedded input vector to include information\na b o u tt h ep o s i t i o n so ft h ei n p u te l e m e n t si nt h es e q u e n c e32. To keep this\ninformation when propagating the signal through the TF structure, the\noverall embedding dimensiondH of internal states is conserved. Through-\nout this work, we choosedH = 128.\nThe embedded input with positional encoding is passed to the trans-\nformer cell, where the query, key, and value matrices are generated, and\nmulti-headed self-attention is applied\n32. We use a masked self-attention\nmechanism to ensure the TF model is autoregressive, like the RNN. The\noutput vector of the masked self-attention is then added to the embedded\ninput state, and the sum is normed before being fed into two feed-forward\nlayers. The normalization is a trained process to improve the network\ntraining stability and adds 2d\nH trainable parameters. Similar to32, we apply\none feed-forward layer with a ReLU activation function, followed by a linear\nfeed-forward layer, where the weights and biases of both layers are trainable.\nThe ﬁrst feed-forward layer projects the input into a vector of size\nd\nFF = 2048, while the second feed-forward layer projects it back to sizedH.\nThe output of the feed-forward layers is again added to the output vector of\nthe self-attention cell, and the sum is normalized, see Fig.1e.\nThe entire transformer cell, including the self-attention and feed-\nforward layers, as well as the add-and-norm operations, can be applied\nmultiple times independently to improve the network expressivity. We\nobtain satisfying results withT =2 .\nTo represent quantum states similarly to the RNN ansatz, we apply two\nfully connected layers with trainable weights to the output of the transfor-\nmer cell. Theﬁrst layer conserves the dimensiond\nH and is followed by a\nReLU activation function. The secondlayer projects the output to a vector of\noutput dimensiondO and is followed by a softmax activation function.\nThese two layers are not shown in the diagram in Fig.1e. After the softmax\na c t i v a t i o nf u n c t i o n ,t h eo u t p u tc a nb et r e a t e dt h es a m ew a ya st h eR N N\noutput, and it can be interpreted as a probability distribution from which the\nnext qubit state in the sequence can be sampled\n32.W et r a i nt h eT Fm o d e lt h e\nsame way as the RNN, using the Adam optimizer to minimize the energy\nexpectation value via gradient descent. The energy expectation value is\nobtained in the same way as in the RNN, see Eq. (3)a n dE q .(4), and we\nchoose the same values as in the RNN approach for all hyperparameters\ninvolved in the training process.\nPositional encoding. Since the TF model does not include a recurrent\nbehavior as the RNN, it does not provide any information about the order\nof the sequence elements by default. A positional encoding algorithm is\nused to include information about the position of each input. We use the\nalgorithm as proposed in\n32, which creates a matrix of dimensionL × dH\nwith sequence lengthL and embedding dimensiondH. The individual\nelements are calculated via\nPl ; 2iðÞ ¼ sin l\n100002i=dH\n/C20/C21\n; ð9Þ\nPl ; 2i þ 1ðÞ ¼ cos l\n100002i=dH\n/C20/C21\n; ð10Þ\nwith 0≤ l < L indexing the sequence elements, and 0≤ i < dH/2 the column\nindices of the output matrix. The resulting matrix is added to the embedded\ninput element of the TF setup, which linearly projects the input vector to a\nvector of dimensiond\nH using trainable weights. This operation gives each\nelement a unique information abou ti t sp o s i t i o ni nt h es e q u e n c e .\nThe self-attention mechanism . The self-attention mechanism, as\nintroduced in32 and illustrated in Fig.5, projects each embedded sequence\nelement σi of dimensiondH to a query vectorqi, a key vectorki, and a value\nvector vi of dimensionsdH,\nqi ¼\nXdH\nl¼1\nWq\ni;lσi;l; ki ¼\nXdH\nl¼1\nWk\ni;lσi;l; vi ¼\nXdH\nl¼1\nWv\ni;lσi;l; ð11Þ\nwith trainable weight matricesW q, W k, W v of dimensiondH × dH.Q u e r y ,\nkey, and value vectors of all input elements can be summarized in the\ncorresponding matrices,\nQ ¼\nq\n1\n..\n.\nqL\n2\n66\n4\n3\n77\n5; K ¼\nk\n1\n..\n.\nkL\n2\n66\n4\n3\n77\n5; V ¼\nv\n1\n..\n.\nvL\n2\n66\n4\n3\n77\n5; ð12Þ\nwith sequence lengthL.\nThe attention mechanism then maps the queries and key-value pairs to\nan output for each sequence element, allowing for highlighting of connec-\ntions to sequence elements with important information. For each sequence\nelementσ\ni, the dot product of the query vectorqi with the key vectorkj for all\nj 2 1; ... ; Lfg is evaluated. We then add a masking termmi,j to the signal,\nwhich is given by,\nmi;j ¼ 0i fi ≤ j;\n/C01otherwise:\n/C26\nð13Þ\nThis ensures that the self-attention only considers previous elements in the\nsequence and does not look at later elements that still need to be determined\nin the autoregressive behavior. Applying a softmax activation function to all\nsignals after adding the mask ensures that the contributions of all later\nsequence elements withm\ni,j = −∞ are driven to zero. We then take the dot\nproduct of each signal with the corresponding value vectorvj and sum all\nsignals to generate the output of the attention mechanism. The complete\nattention formalism can thus be summarized as,\nAttention Q; K; VðÞ ¼ softmax QKT\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\ndH=h\np þ M\n !\nV; ð14Þ\nFig. 5 | Illustration of the attention mechanism applied to sequence elementσ3.\nEach sequence elementσj is projected onto a query (qj), key (kj), and value (vj) vector,\nand the dot product ofq3 is taken with each key vectorkj, j 2 1; 2; 3; 4fg . Adding a\nmask matrix eliminates the inﬂuence of all elements later in the sequence by adding\nmi,j > i = − ∞, while contributions of previously sampled sequence elements remain\nunchanged withmi,j ≤ i = 0, ensuring the autoregressive behavior. A softmax acti-\nvation function is applied to all signals before a dot product is taken with the\ncorresponding value vectors. The sum of all resulting signals provides the attention\noutcome for the inputσ\n3. This algorithm is the same as introduced in32.\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 7\nwhere the mask matrixM has entriesmi,j as in Eq. (13). Here we further use\nmulti-headed attention, as discussed in32. This approach linearly projects\neach query, key, and value vector toh vectors with individually trainable\nprojection matrices. We thus end up withh heads with modiﬁed query, key,\nand value vectors on which the attention mechanism is applied, where we\nchoose h = 8 throughout this work, as weﬁnd it to yield satisfying results.\nThe linear projection further reduces the dimension of the query, key, and\nvalue vectors tod\nH/h, so that the outputs of the individual heads can be\nconcatenated to yield the total output dimensiondH of the attention\nmechanism. We then scale the outcome of each query-key dot product with\nthe factor 1=\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nd\nH=h\np\nin Eq. (14)32. The output of the multi-headed attention\nformalism is given by,\nMultihead Q; K; VðÞ ¼ Concat ^y1; ... ; ^yh\n/C0/C1\nWO; ð15Þ\n^yl Q; K; VðÞ ¼ Attention QWQ\nl ; KWK\nl ; VWV\nl\n/C0/C1\n; ð16Þ\nwith output weight matrixWO and query, key, and value weight matrices\nWQ\nl , WK\nl ,a n dWV\nl for headl.\nA TF model given an input of dimensiondI h a sa ne m b e d d i n gm a t r i x\nWI of dimension dI × dH, with embedding dimensiondH. The weight\nmatrices in the multi-headed self-attention mechanism then have dimen-\nsions dH × dH/h for WQ\nl , WK\nl ,a n dWV\nl ,a n ddH × dH for WO. Each weight\nmatrix also comes with a bias whose size equals the column-dimension. The\ntwo feed-forward layers in the transformer cell have weight matrices of\nd\nH × dFF and dFF × dH with corresponding biases, and the two norm\noperations add 4dH trainable parameters. The transformer cell, containing\nthe attention mechanism, the feed-forward layers, and the norm operations,\nis applied T times with independent variational parameters. After the\ntransformer cell we add two fully connected layers with weight matrices of\ndimensions d\nH × dH and dH × dO for output dimensiondO.B o t hl a y e r s\ncome with corresponding biases.\nSingle-qubit inputs givedI =1 a n ddO = 2, using one-hot encoded\noutput. WithdH = 128,dFF = 2048, andT = 2 ,a sc h o s e nt h r o u g h o u tt h i s\nwork, the TF architecture has a total of\ndI × dH\n/C0/C1\nþ dH þ T 4 dH × dH\n/C0/C1\nþ 9dH þ 2 dFF × dH\n/C0/C1\nþ dFF\n/C2/C3\nþ dH × dH\n/C0/C1\nþ dH þ dH × dO\n/C0/C1\nþ dO\n¼ 1; 203; 074\nð17Þ\ntrainable variational parameters.\nPatched network models\nThe bottleneck of the RNN and TF wavefunction ansatz is the iteration of\nthe network cell over the entire qubit sequence. This computationally\nexpensive step needs to be done for eachsample that is generated, as well as\neach time a wavefunctionΨ σ; WðÞ is calculated, which is required to\nevaluate non-diagonal observables, see Eq. (7). We reduce the number of\niterations per network call by shortening the input sequence and in return\nincreasing the dimension of the input vector.\nAs illustrated in Fig.1, for two-dimensional Rydberg atom arrays, we\nconsider patches ofp qubits arranged in squares or rectangles. Weﬂatten\nthese patches into binary input vectors of dimensiond\nI = p.T h i sm o d -\niﬁcation increases the network inputdimension, which is, however, pro-\njected to the unaffected hidden state dimension in the RNN cell or the\nembedding dimension in the TF model. Thus, the computational cost of\nevaluating the network cell is barely affected by the increased input\ndimension, but the shorter sequence length leads to signiﬁcantly reduced\ncomputational runtimes. In additionto this expected speed-up, we expect\nthe patched network models to capturelocal correlations in the system with\nhigher accuracy. Neighboring qubit states are now considered at the same\niteration, and their information is not encoded in the network state.\nThe network output uses one-hot encoding of the patched quantum\nstates, so that the output vector is of dimensiond\nO =2 p. Each entry\nrepresents one possible state of the qubit patch, see Fig.1d, e. This output\ndimension, and with this the computational cost of evaluating the softmax\nf u n c t i o n ,t h u ss c a l e se x p o n e n t i a l l yw ith the patch size. In this work, we only\nconsider patches up top =2×2 f o r t h e p a t c h e d n e t w o r k m o d e l s a n d\nintroduce large, patched TFs to deal with larger patch sizes.\nThe patched RNN withp =2×2 h a s i n p u t d i m e n s i o ndI =4 a n d\noutput dimensiondO = 16, so Eq. (8) leads to 70, 032 trainable network\nparameters. For the same input and output dimension, the patched TF\nmodel has 1, 203, 406 trainable network parameters, according to Eq. (17).\nLarge, patched transformer models\nIn the large, patched transformer (LPT F )m o d e l ,w ea p p l yt h eT Fn e t w o r kt o\nap a t c ho fp qubits. However, we abort the TF model in Fig.1e right after the\ntransformer cell and do not apply the fully connected layers and the softmax\nactivation function. Instead, we use the generated output state of the\ntransformer cell as an input hidden state to a patched RNN with the hidden-\nunit dimension matching the embedding dimensiond\nH of the TF model.\nThis patched RNN breaks up the large input patch into smaller sub-patches\nof sizep\ns, where we always choose a sub-patch size ofps =2×2i nt h i sw o r k .\nWe then use the patched RNN model to iteratively sample the quantum\nstates of these sub-patches in the same way as when applying the patched\nRNN to the full system size. The only difference is that the initial hidden state\nis provided by the TF output,h\n0 = hTF,s e eF i g .1f for an illustration.\nThe total number of trainable network parameters in the LPTF setup is\nthen given by a combination of Eq. (8)a n dE q .(17), where the two fully\nconnected layers at the TF output are subtracted,\np × dH\n/C0/C1\nþ T 4 dH × dH\n/C0/C1\nþ 9dH þ 2 dFF × dH\n/C0/C1\nþ dFF\n/C2/C3\n|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}\npatched TF\nþ 3 ps × dH\n/C0/C1\nþ 4 dH × dH\n/C0/C1\nþ 7dH þ dH × dO\n/C0/C1\nþ dO|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}\npatched RNN\n: ð18Þ\nWe use the input dimensiondI = p f o rt h ep a t c h e dT Fa n ddI = ps for the\npatched RNN, as well as the output dimensiondO ¼ 2ps .I nt h i sw o r k ,w e\nchooseps = 2 × 2, which yields 1, 256, 208+128p trainable parameters with\ndH =1 2 8 ,dFF = 2048, andT =2 . F o r t h e c h o i c ep =8×8 , w e t h u s g e t\n1, 264, 400 variational network parameters.\nComputational complexity\nThe process ofﬁnding ground state representations with ANNs can be\ndivided into two steps, the generation of samples from the network and the\nevaluation of energy expectation values according to Eq. (6)a n dE q .(7)i n\neach training iteration. We start with analyzing the sample complexity for\nthe different network architectures. As we choose the hidden and the\nembedding dimensiond\nH ﬁxed and equal for all architectures, we consider it\nas a constant in the complexity analysis.\nGenerating a single sampleσ from pRNN σ; WðÞ encoded in an RNN\nrequires N executions of the RNN cell, leading to a computational cost of\nO NðÞ . By considering the patched RNN, we reduce the sequence length\nfrom N to N/p,s ot h a to n l yN/p RNN cells are evaluated. However, the\no u t p u td i m e n s i o ni nt h i sc a s ei s2p, so each evaluation of the RNN cell\nrequires 2p products to evaluate the outcome probability distribution. This\nleads to an overall sampling complexity ofO N=p2p/C0/C1\nfor the patched RNN.\nIn order to generate a single sampleσ from pTF σ; WðÞ encoded in the\nTF network, we similarly need to evaluate the transformer cellN times.\nHowever, the attention algorithm itself requires the computation ofN\nmultiplications that need to be evaluated in each pass through the network.\nAdditionally, the transformer cell contains a projection of the embedded\nstate to a vector of sizedFF ≫ dH, which requires signiﬁcantly more multi-\nplication operations than an RNN cell evaluation. Thus, drawing a sample\nfrom the TF model comes at computational complexityO NN þ deff\n/C2/C3/C0/C1\n.\nWhen introducing the patched TF model, we similarly reduce the sequence\nlength toN/p, so that the full network needs to be evaluatedN/p times and\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 8\nthe attention mechanism requiresN/p multiplications. Also here the output\nscales as 2p, leading to a computational cost ofO N=pN =p þ dFF þ 2p/C2/C3/C0/C1\n.\nIn the LPTF, the transformer cell is followed by a patched RNN with\np/ps cells. Since each LPTF iteration requires the evaluation of one such\nRNN, we evaluate the transformer cell andp/ps RNN cellsN/p times to\ngenerate a single sampleσ. While the TF network output is kept at\nembedding dimensiondH, the RNN output is of dimension 2ps ,l e a d i n gt oa\ncomputational complexity of\nO N\np\nN\np þ dFF\n/C20/C21\nþ N\np\np\nps\n2ps\n/C18/C19\n¼ O N2\np2 þ N\np dFF þ N\nps\n2ps\n/C18/C19\n: ð19Þ\nThis shows a signiﬁcant reduction of the sampling complexity compared to\nthe patched TF model and explains the observed efﬁciency of our intro-\nduced LPTF architecture.\nNext, we consider the complexity ofevaluating energy expectation\nvalues. While the evaluation of the diagonal part is given by a linear average\nover allNs samples and thus scales linearly with the system sizeN, evaluating\nthe off-diagonal part for each sampleσs according to Eq. (7)r e q u i r e st h e\nevaluation ofΨ σ0; WðÞ for allσ0 corresponding toσs with one atomﬂipped.\nThis leads toN evaluations ofΨ σ0; WðÞ for each sample, which is obtained\nby passingσ0 through the network architecture and obtaining the output\nprobabilitypRNN σ0; WðÞ or pTF σ0; WðÞ .\nThus, the patched RNN withN/p network cells needs to be evaluatedN\ntimes for each sample, leading to a computational complexity of\nO N\ns2pN2=p\n/C0/C1\nfor obtaining the energy expectation value. Similarly, the\np a t c h e dT Fm o d e lw i t hN/p iterations and N/p multiplications in the\nattention mechanism is evaluatedN times for each sample, leading to a\ncomputational cost of O Ns N3=p2 þ dFFN2=p þ 2pN2=p\n/C2/C3/C0/C1\n.F o rt h e\nLPTF we accordingly obtainO Ns N3=p2 þ dFFN2=p þ 2ps N2=ps\n/C2/C3/C0/C1\n.T h e\nrequired memory scaling behaves similarly for the discussed network\narchitectures. This scaling can be reduced using optimized implementation\nalgorithms as discussed in the next section. While the TF and LPTF show a\nworse scaling than the RNN, the evaluation of the off-diagonal energy terms\ncan be parallelized for these two models. Since no autoregressive sampling is\nrequired for this task, allN/p masked self-attention layers can be evaluated in\nparallel, signiﬁcantly reducing the computational runtime. This paralleli-\nzation is not possible for the RNN due to its recurrent nature, where the\nhidden state needs to be evaluated foreach individual RNN cell before it can\nbe passed to the next iteration.\nThe generation of samples with QMC is of computational complexity\nO VNðÞ , with average interaction strengthV over the system. The energy\nestimation also scales asO VNðÞ and only needs to be done at the end of the\nrun after all samples have been generated, which is in contrast to the neural\nnetwork approach that requires the evaluated energy in each training\niteration\n53. While QMC thus shows much more promising computational\ncomplexity than all three neural network methods when considering the\nscaling to large system sizes, we observe that it requires longer computa-\ntional runtimes than the LPTF for the system sizes considered in this work.\nConsidering the uncertainties of expectation values, weﬁnd that QMC\nsimulations require far more samples (N\ns =7×1 04 in Figs. 2, 3,a n d\nNs =7×1 05 in Fig.4) than the ANN approaches (Ns = 512). However, the\nANN approaches require the generation ofNs samples in each training\niteration, so that overall more samples are generated.\nThe higher uncertainties in the QMC simulations are caused by cor-\nrelations in the generated sample chains, where autocorrelation times grow\nwith increasing system sizes. Furthermore, ergodicity in the QMC sampling\nprocess is not guaranteed for large systems\n53. These problems do not arise in\nthe exact and independent autoregressive sampling of the neural network\nalgorithms, explaining the lower uncertainties observed for smaller sample\nsizes. At the same time, these observations limit accurate QMC simulations\nto small system sizes.\nImplementation details\nWe train the network toﬁnd the ground state of the Rydberg Hamiltonian\nby minimizing the energy expectation value, which we evaluate using Eq. (6)\nand Eq. (7) with the Hamiltonian operator^H,\nhEi¼ ΨW∣ ^H∣ΨW\n/C10/C11\n¼\nX\nσ;σ0fg\nΨ/C3 σ; WðÞ Ψ σ0; WðÞ σ∣ ^H∣σ0/C10/C11\n; ð20Þ\nwhere Ψ σ; WðÞ denotes a wavefunction encoded in one of the discussed\nnetwork models. To optimize the variational network parameters, we use\nthe gradient descent algorithm the same way as discussed in6,7,w i t hg r a -\ndients\n∂Wi\nE ≈ 2\nNs\nXNs\ns¼1\n∂Wi\nΨ/C3 σs; W\n/C0/C1\nEloc σs; W\n/C0/C1\n/C0hEi\n/C2/C3\n; ð21Þ\nand local energy\nEloc σs; W\n/C0/C1\n¼\nX\nσ0fg\nσs∣ ^H∣σ0/C10/C11 Ψ σ0; WðÞ\nΨ σs; W\n/C0/C1 : ð22Þ\nThe training process requires theevaluation of the gradients ofΨ σ; WðÞ .T o\nreduce the necessary amount of memor y ,w ea l w a y sg e n e r a t eab a t c ho fNs\nsamples from the network without evaluating the gradients. We then pass\neach sample through the network again to obtain the wavefunction\namplitude with the corresponding gradients. This approach requires 2N\ns\nnetwork passes instead ofNs, but evaluating gradients on a given input\nsequence is less memory-consuming than evaluating gradients on an\nautoregressive process in PyTorch\n60.W ec a nf u r t h e rr e d u c et h er e q u i r e d\nmemory by dividing the total batch ofNs samples into mini batches ofK\nsamples each, which are evaluated in separate processes. This reduces the\nmemory scaling by a factorK/N\ns per pass, while requiringNs/K network\npasses instead of one. Thus, the smaller we choose the mini batches, the less\nmemory is required, but the longer the computational runtime. If not stated\notherwise, we chooseK = 256 in this work.\nConsidering the off-diagonal term in the Rydberg Hamiltonian, its\ncontribution to the energy expectation value is given byE\noff ¼ PN\ni¼1 ^σx\ni\n/C10/C11\n.\nThus, calculating the local energyEloc σs; W\n/C0/C1\nusing Eq. (7) requires for each\nsampled stateσs to evaluateΨ σ0; WðÞ for all σ0 that correspond to the\nsampled state with one atomﬂipped from the ground to the excited state or\nvice versa6,7. Instead of passingN states through the network for each sample\nwe generate, we can reduce the required memory and accelerate our algo-\nrithm by splitting the atom sequence intoD equally sized parts. In each part,\nthe wavefunction amplitude is evaluated for the states where each of theN/D\natoms is ﬂipped. When calculating the wavefunction amplitudes using\nsequential networks, theﬂipping of one atom only affects the calculation for\natoms that appear later in the sequence. We store the network outcome for\nthe original stateσ\ns after eachN/D atoms and evaluate each group starting\nfrom this stored value. We then only need to pass the sequence from theﬁrst\natom of the group to the last atom in the system to the network. This ansatz\ncan be used since no gradient needs to be evaluated on this off-diagonal term\nand corresponds to using mini batches of atoms. This way we reduce the\namount of required memory by a factorD/N per pass, while requiringN/D\nnetwork passes instead of only one. While we now evaluate the networkN/D\ntimes, the sequence length at iteration d is reduced from N/p to\nN /C0 d /C01½/C138 DðÞ =p, leading to an accelerated evaluation of the local energies\nsince the input sequence length is reduced in most cases. In this work, we\nalways split the sequence of atoms intoD = N/p parts, withp the patch size in\nthe patched network models.\nWe base our simulations on PyTorch\n60 and NumPy61,a n du s e\nMatplotlib62 to visualize our results.\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 9\nData availability\nAll presented data can be reproduced with the publicly available source\ncode. It is further available upon request to the corresponding author.\nCode availability\nThe source code used to generate the data in this work is available onhttps://\ngithub.com/APRIQuOt/VMC_with_LPTFs.git. It is based on PyTorch60\nand NumPy61 a n dw eu s e dM a t p l o t l i b62 f o rv i s u a l i z i n go u rr e s u l t s .\nReceived: 26 January 2024; Accepted: 28 February 2024;\nReferences\n1. Carleo, G. & Troyer, M. Solving the quantum many-body problem with\nartiﬁcial neural networks.Science 355, 602– 606 (2017).\n2. Torlai, G. et al. Neural-network quantum state tomography.Nat. Phys.\n14, 447– 450 (2018).\n3. Torlai, G. & Melko, R. G. Latent space puriﬁcation via neural density\noperators. Phys. Rev. Lett.120, 240503 (2018).\n4. Dawid, A. et al. Modern applications of machine learning in quantum\nsciences, arXiv:2204.04198 [cond-mat] (2022).\n5. Carrasquilla, J. Machine learning for quantum matter.Adv. Phys.-X5,\n1797528 (2020).\n6. Carrasquilla, J. & Torlai, G. How To Use Neural Networks To\nInvestigate Quantum Many-Body Physics.PRX Quantum2,\n040201 (2021).\n7. Hibat-Allah, M., Ganahl, M., Hayward, L. E., Melko, R. G. &\nCarrasquilla, J. Recurrent neural network wave functions.Phys. Rev.\nRes. 2, 023358 (2020).\n8. Czischek, S., Moss, M. S., Radzihovsky, M., Merali, E. & Melko, R. G.\nData-enhanced variational Monte Carlo simulations for Rydberg atom\narrays. Phys. Rev. B105, 205108 (2022).\n9. Viteritti, L. L., Ferrari, F. & Becca, F. Accuracy of restricted Boltzmann\nmachines for the one-dimensionalJ\n1 − J2 Heisenberg model.\nSciPost. Phys.12, 166 (2022).\n10. Neugebauer, M. et al. Neural-network quantum state tomography in a\ntwo-qubit experiment.Phys. Rev. A102, 042604 (2020).\n11. Schmale, T., Reh, M. & Gärttner, M. Efﬁcient quantum state\ntomography with convolutional neural networks.npj Quant. Inf.8,\n115 (2022).\n12. Torlai, G. et al. Integrating neural networks with a quantum simulator\nfor state reconstruction.Phys. Rev. Lett.123, 230504 (2019).\n13. Morawetz, S., De Vlugt, I. J. S., Carrasquilla, J. & Melko, R. G. U(1)-\nsymmetric recurrent neural networks for quantum state\nreconstruction. Phys. Rev. A104, 012401 (2021).\n14. Cha, P. et al. Attention-based quantum tomography.Mach Learn: Sci.\nTechnol. 3, 01LT01 (2022).\n15. Carrasquilla, J., Torlai, G., Melko, R. G. & Aollita, L. Reconstructing\nquantum states with generative models.Nat. Mach. Intell1,\n155– 161 (2019).\n16. Torlai, G., Mazzola, G., Carleo, G. & Mezzacapo, A. Precise\nmeasurement of quantum observables with neural-network\nestimators. Phys. Rev. Res.2, 022060 (2020).\n17. Schmitt, M. & Heyl, M. Quantum many-body dynamics in two\ndimensions with artiﬁcial neural networks.Phys. Rev. Lett.125,\n100503 (2020).\n18. Nagy, A. & Savona, V. Variational quantum monte carlo method with a\nneural-network ansatz for open quantum systems.Phys. Rev. Lett.\n122, 250501 (2019).\n19. Vicentini, F., Biella, A., Regnault, N. & Ciuti, C. Variational neural-\nnetwork ansatz for steady states in open quantum systems.Phys.\nRev. Lett.122, 250503 (2019).\n20. Hartmann, M. J. & Carleo, G. Neural-network approach to dissipative\nquantum many-body dynamics.Phys. Rev. Lett.122, 250502 (2019).\n21. Reh, M., Schmitt, M. & Gärttner, M. Time-dependent variational\nprinciple for open quantum systems with artiﬁcial neural networks.\nPhys. Rev. Lett.127, 230501 (2021).\n22. Czischek, S., Gärttner, M. & Gasenzer, T. Quenches near ising\nquantum criticality as a challenge for artiﬁcial neural networks.Phys.\nRev. B98, 024311 (2018).\n23. Melko, R. G., Carleo, G., Carrasquilla, J. & Cirac, J. I. Restricted\nBoltzmann machines in quantum physics.Nat. Phys.15,\n887– 892 (2019).\n24. Hibat-Allah, M., Inack, E. M., Wiersema, R., Melko, R. G. &\nCarrasquilla, J. Variational neural annealing.Nat. Mach. Intell.3,\n952– 961 (2021).\n25. Hibat-Allah, M., Melko, R. G. & Carrasquilla, J. Investigating\ntopological order using recurrent neural networks.Phys. Rev. B108,\n075152 (2023).\n26. Sharir, O., Levine, Y., Wies, N., Carleo, G. & Shashua, A. Deep\nautoregressive models for the efﬁcient variational simulation of many-\nbody quantum systems.Phys. Rev. Lett.124, 020503 (2020).\n27. Valenti, A., Greplova, E., Lindner, N. H. & Huber, S. D. Correlation-\nenhanced neural networks as interpretable variational quantum\nstates. Phys. Rev. Res.4, L012010 (2022).\n28. Hibat-Allah, M., Melko, R. G. & Carrasquilla, J. Supplementing\nRecurrent Neural Network Wave Functions with Symmetry and\nAnnealing to Improve Accuracy, arXiv:2207.14314 [cond-mat] (2022).\n29. Ahsan Khandoker, S., Munshad Abedin, J. & Hibat-Allah, M.\nSupplementing recurrent neural networks with annealing to solve\ncombinatorial optimization problems.Mach. Learn: Sci. Technol.4,\n015026 (2023).\n30. Luo, Di et al. Gauge-invariant and anyonic-symmetric autoregressive\nneural network for quantum lattice models.Phys. Rev. Res.5,\n013216 (2023).\n31. Bennewitz, E. R., Hopfmueller, F., Kulchytskyy, B., Carrasquilla, J. &\nRonagh, P. Neural error mitigation of near-term quantum simulations.\nNat. Mach. Intell.4, 618– 624 (2022).\n32. Vaswani, A. et al. Attention Is All You Need, arXiv:1706.03762\n[cs] (2017).\n33. Zhang, Y.-H. & Di Ventra, M. Transformer Quantum State: A Multi-\nPurpose Model for Quantum Many-Body Problems.Phys. Rev. B107,\n075147 (2023).\n34. Viteritti, L. L., Rende, R. & Becca, F. Transformer variational wave\nfunctions for frustrated quantum spin systems.Phys. Rev. Lett.130,\n236401 (2023).\n35. Sharir, O., Chan, G. K.-L. & Anandkumar, A. Towards Neural\nVariational Monte Carlo That Scales Linearly with System Size,\narXiv:2212.11296 [quant-ph] (2022).\n36. Ma, H., Sun, Z., Dong, D., Chen, C. & Rabitz, H. Tomography of\nQuantum States from Structured Measurements via quantum-aware\ntransformer https://doi.org/10.48550/arXiv.2305.05433,\narXiv:2305.05433 [quant-ph] (2023).\n37. An, Z., Wu, J., Yang, M., Zhou, D. L. & Zeng, B. Uniﬁed quantum state\ntomography and Hamiltonian learning: A language-translation-like\napproach for quantum systems.Phys. Rev. Appl.21, 014037 (2024).\n38. von Glehn, I., Spencer, J. S. & Pfau, D. A self-attention ansatz for\nab-initio quantum chemistry.https://doi.org/10.48550/arXiv.2211.\n13672, arXiv:2211.13672 [physics.chem-ph] (2022).\n39. Carrasquilla, J. et al. Probabilistic simulation of quantum circuits using\na deep-learning architecture.Phys. Rev. A104, 032610 (2021).\n40. Luo, D., Chen, Z., Carrasquilla, J. & Clark, B. K. Autoregressive neural\nnetwork for simulating open quantum systems via a probabilistic\nformulation. Phys. Rev. Lett.128, 090501 (2022).\n41. Dosovitskiy, A. et al. An Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale, arXiv:2010.11929 [cs] (2021).\n42. Jaksch, D. et al. Fast Quantum Gates for Neutral Atoms.Phys. Rev.\nLett. 85, 2208– 2211 (2000).\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 10\n43. Lukin, M. D. et al. Dipole Blockade and Quantum Information\nProcessing in Mesoscopic Atomic Ensembles.Phys. Rev. Lett.87,\n037901 (2001).\n44. Endres, M. et al. Atom-by-atom assembly of defect-free one-\ndimensional cold atom arrays.Science 354, 1024– 1027 (2016).\n45. Barredo, D., Lienhard, V., de Léséleuc, S., Lahaye, T. & Browaeys, A.\nSynthetic three-dimensional atomic structures assembled atom by\natom. Nature 561,7 9– 82 (2018).\n46. Samajdar, R., Ho, W. W., Pichler, H., Lukin, M. D. & Sachdev, S.\nComplex Density Wave Orders and Quantum Phase Transitions in a\nModel of Square-Lattice Rydberg Atom Arrays.Phys. Rev. Lett.124,\n103601 (2020).\n47. Samajdar, R., Ho, W. W., Pichler, H., Lukin, M. D. & Sachdev, S.\nQuantum phases of Rydberg atoms on a kagome lattice.PNAS 118,\ne2015785118 (2021).\n48. Ebadi, S. et al. Quantum phases of matter on a 256-atom\nprogrammable quantum simulator.Nature 595, 227– 232 (2021).\n49. Scholl, P. et al. Quantum simulation of 2D antiferromagnets with\nhundreds of Rydberg atoms.Nature 595, 233– 238 (2021).\n50. Xu, W. et al. Fast Preparation and Detection of a Rydberg Qubit Using\nAtomic Ensembles.Phys. Rev. Lett.127, 050501 (2021).\n51. Miles, C. et al. Machine learning discovery of new phases in\nprogrammable quantum simulator snapshots.Phys. Rev. Res.5,\n013026 (2023).\n52. Kalinowski, M. et al. Bulk and Boundary Quantum Phase Transitions in\na Square Rydberg Atom Array.Phys. Rev. B105, 174417 (2022).\n53. Merali, E., De Vlugt, I. J. S. & Melko, R. G. Stochastic Series Expansion\nQuantum Monte Carlo for Rydberg Arrays, arXiv:2107.00766 [cond-\nmat] (2023).\n54. Bravyi, S., DiVincenzo, D. P., Oliveira, R. I. & Terhal, B. M. The\nComplexity of Stoquastic Local Hamiltonian Problems.Quant. Info.\nComput. 8, 361– 385 (2008).\n55. Cho, K. et al. Learning Phrase Representations using RNN\nEncoder– Decoder for Statistical Machine Translation, inhttps://doi.\norg/10.3115/v1/D14-1179 Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP)\n1724– 1734 (Association for Computational Linguistics, Doha,\nQatar, 2014).\n56. Becca, F. & Sorella, S.https://doi.org/10.1017/9781316417041\nQuantum Monte Carlo Approaches for Correlated Systems, 1st ed.\n(Cambridge University Press, 2017).\n57. Morin, F. & Bengio, Y. Hierarchical probabilistic neural network\nlanguage model, inhttps://proceedings.mlr.press/r5/morin05a.html\nProceedings of the Tenth International Workshop on Artiﬁcial\nIntelligence and Statistics, Proceedings of Machine Learning\nResearch, (eds Cowell, R. G. & Ghahramani, Z.) Vol. R5, 246– 252\n(PMLR, 2005).\n58. Zhai, X., Kolesnikov, A., Houlsby, N. & Beyer, L.\nhttps://doi.org/10.\n48550/arXiv.2106.04560 Scaling vision transformers,\narXiv:2106.04560 [cs.CV] (2021).\n59. Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory.Neural.\nComput. 9, 1735– 1780 (1997).\n60. Paszke, A. et al. Pytorch: An imperative style, high-performance deep\nlearning library, inhttp://papers.neurips.cc/paper/9015-pytorch-an-\nimperative-style-high-performance-deep-learning-library.pdf\nAdvances in Neural Information Processing Systems 328024– 8035\n(Curran Associates, Inc., 2019).\n61. Harris, C. R. et al. Array programming with NumPy.Nature 585,\n357– 362 (2020).\n62. Hunter, J. D. Matplotlib: A 2d graphics environment.Comput. Sci.\nEng. 9,9 0– 95 (2007).\nAcknowledgements\nWe thank J. Carrasquilla, R.G. Melko, M. Reh, M.S. Moss, and E. Inack for\nfruitful discussions and feedback. We are grateful for support on the\nquantum Monte Carlo simulations by E. Merali. This research was enabled in\npart by support provided by the Digital Research Alliance of Canada\n(alliancecan.ca).\nAuthor contributions\nThe fundamental ideas of the introduced approach were developed by K.\nSprague who further implemented and organized the used Python code. S.\nCzischek used the provided code to obtain the presented results with\nsupport by K. Sprague. The manuscript was written by S. Czischek with\nvaluable feedback by K. Sprague.\nCompeting interests\nThe authors declare no competing interests. S. Czischek is a guest editor for\nCommunications Physics, but was not involved in the editorial review of, or\nthe decision to publish this article.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s42005-024-01584-y.\nCorrespondenceand requests for materials should be addressed to\nStefanie Czischek.\nPeer review informationThis manuscript has been previously reviewed at\nanother Nature Portfolio journal. The manuscript was considered suitable for\npublication without further review atCommunications Physics.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s42005-024-01584-y Article\nCommunications Physics|            (2024) 7:90 11",
  "topic": "Monte Carlo method",
  "concepts": [
    {
      "name": "Monte Carlo method",
      "score": 0.7406054735183716
    },
    {
      "name": "Statistical physics",
      "score": 0.46041184663772583
    },
    {
      "name": "Computer science",
      "score": 0.4093351662158966
    },
    {
      "name": "Mathematics",
      "score": 0.3059062659740448
    },
    {
      "name": "Physics",
      "score": 0.23756521940231323
    },
    {
      "name": "Statistics",
      "score": 0.1658046841621399
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I153718931",
      "name": "University of Ottawa",
      "country": "CA"
    }
  ],
  "cited_by": 24
}