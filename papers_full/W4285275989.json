{
  "title": "ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer",
  "url": "https://openalex.org/W4285275989",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2225598084",
      "name": "Wang, Ningning",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A4307536955",
      "name": "Gan, Guobing",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A1800355547",
      "name": "Zhang Peng",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A1924112616",
      "name": "Zhang Shuai",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Wei, Victor Junqiu",
      "affiliations": [
        "Hong Kong Polytechnic University",
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A1952241286",
      "name": "Liu Qun",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2030371270",
      "name": "Jiang Xin",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W1591825359",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2997510314",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964189376",
    "https://openalex.org/W4287762561",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2014902591",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2975525902",
    "https://openalex.org/W2963045354"
  ],
  "abstract": "Recently, a lot of research has been carried out to improve the efficiency of Transformer. Among them, the sparse pattern-based method is an important branch of efficient Transformers. However, some existing sparse methods usually use fixed patterns to select words, without considering similarities between words. Other sparse methods use clustering patterns to select words, but the clustering process is separate from the training process of the target task, which causes a decrease in effectiveness. To address these limitations, we design a neural clustering method, which can be seamlessly integrated into the Self-Attention Mechanism in Transformer. The clustering task and the target task are jointly trained and optimized to benefit each other, leading to significant effectiveness improvement. In addition, our method groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently, which improves the efficiency. We verified our method on machine translation, text classification, natural language inference, and text matching tasks. Experimental results show that our method outperforms two typical sparse attention methods, Reformer and Routing Transformer while having a comparable or even better time and memory efficiency. Â© 2022 Association for Computational Linguistics.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2390 - 2402\nMay 22-27, 2022câƒ2022 Association for Computational Linguistics\nClusterFormer: Neural Clustering Attention for Efï¬cient and Effective\nTransformer\nNingning Wang1âˆ—, Guobing Gan1âˆ—, Peng Zhang1â€ , Shuai Zhang1,\nJunqiu Wei2, Qun Liu3, Xin Jiang3\n1College of Intelligence and Computing, Tianjin University, Tianjin, China\n2The Hong Kong Polytechnic University, China\n3Huawei Noahâ€™s Ark Lab, China\n{w_ning1215,ganguobing,pzhang,szhang96}@tju.edu.cn\njunwei@polyu.edu.hk,{qun.liu,Jiang.Xin}@huawei.com\nAbstract\nRecently, a lot of research has been carried\nout to improve the efï¬ciency of Transformer.\nAmong them, the sparse pattern-based method\nis an important branch of efï¬cient Transform-\ners. However, some existing sparse methods\nusually use ï¬xed patterns to select words, with-\nout considering similarities between words.\nOther sparse methods use clustering patterns\nto select words, but the clustering process is\nseparate from the training process of the tar-\nget task, which causes a decrease in effective-\nness. To address these limitations, we design a\nneural clustering method, which can be seam-\nlessly integrated into the Self-Attention Mech-\nanism in Transformer. The clustering task\nand the target task are jointly trained and op-\ntimized to beneï¬t each other, leading to signif-\nicant effectiveness improvement. In addition,\nour method groups the words with strong de-\npendencies into the same cluster and performs\nthe attention mechanism for each cluster inde-\npendently, which improves the efï¬ciency. We\nveriï¬ed our method on machine translation,\ntext classiï¬cation, natural language inference,\nand text matching tasks. Experimental results\nshow that our method outperforms two typical\nsparse attention methods, Reformer and Rout-\ning Transformer while having a comparable or\neven better time and memory efï¬ciency.\n1 Introduction\nTransformer (Vaswani et al., 2017) has been widely\nused and achieved state-of-the-art results in a va-\nriety of NLP tasks such as neural machine trans-\nlation (Bahdanau et al., 2015), text classiï¬cation,\netc. Its good effectiveness beneï¬ts from its core\ncomponent Self-Attention Mechanism which can\ncapture global dependencies well. However, the\nlarge calculation and memory cost limit the fur-\nther application of Transformer on long sequence\nâˆ—The ï¬rst two authors contributed equally\nâ€ Corresponding Author\ntasks due to the complexity of O(N2d) of Self-\nAttention. As a result, many research works have\nbeen carried out to improve the efï¬ciency of Trans-\nformer (Tay et al., 2020b). These efï¬cient Trans-\nformers can be roughly divided into two cate-\ngories: approximation-based (Tay et al., 2020a;\nKatharopoulos et al., 2020) and sparse pattern-\nbased methods (Qiu et al., 2020; Ho et al., 2019;\nBeltagy et al., 2020; Liu et al., 2018).\nRegarding approximation-based methods, some\nworks are based on low-rank approximation (Tay\net al., 2020a; Wang et al., 2020) while others\nare based on kernels (Katharopoulos et al., 2020;\nChoromanski et al., 2020). Speciï¬cally, Lin-\nformer (Wang et al., 2020) adopts a low-rank ap-\nproximation idea and projects the length dimension\nof keys and values to a lower dimension (N â†’k).\nThis reduces the complexity to O(Nkd). However,\nthe projection matrix in this method requires that all\ninput sequences must be ï¬lled to the same lengthN,\nwhich makes it cannot handle the variable-length\nsequence well. Linear Transformer (Katharopou-\nlos et al., 2020) uses kernels and the associative\nproperty of matrix multiplication to linearize the\nsoftmax attention, which reduces the complexity\nto O(Nd2). However, the approximation error to\nthe softmax matrix in Self-Attention can be large\nin some cases (Xiong et al., 2021).\nSparse pattern-based methods introduce sparse\npatterns into the attention mechanism and limit\nthe number of key vectors that the query vector\nshould pay attention to. Prior work (Child et al.,\n2019; Qiu et al., 2020; Ho et al., 2019; Beltagy\net al., 2020; Liu et al., 2018) proposed to use ï¬xed\nsparse patterns to improve the efï¬ciency of Self-\nAttention. For example, Sparse Transformer (Child\net al., 2019) restricts the query to focus only on\nkeys that are nearby or at ï¬xed intervals. Such\nï¬xed sparse patterns do not consider the similarity\nbetween the query and different keys, and directly\nï¬lter keys according to their location, which re-\n2390\nsults in a degradation of the model effectiveness.\nMore recently, the clustering sparse patterns are\nproposed. Reformer (Kitaev et al., 2020) and Rout-\ning Transformer (Roy et al., 2021) use Locality\nSensitive Hashing (LSH) and K-Means algorithms,\nrespectively, to divide the words in the sequence\ninto different clusters, and then the attention op-\neration is restricted within each cluster indepen-\ndently. In this way, they reduce the complexity to\nO(Nlog Nd) and O(N\nâˆš\nNd), respectively. How-\never, in Reformer and Routing Transformer, both\nLSH and K-Means only play the role of cluster\npartitioning, but run separately from the attention\nnetwork training. In addition, these two methods\nalso have the problem of inconsistency in the simi-\nlarity measure between the clustering and attention\noperation. LSH and K-Means respectively use the\nhash value obtained by random projections and the\nnegative Euclidean distance as the similarity mea-\nsure between the input vectors while the attention\noperations use the inner product. Therefore, such\na sparse pattern idea often results in the reduced\neffectiveness.\nTo address the reduced effectiveness issue of\nmany efï¬cient Transformers, especially sparse\npattern-based methods, we propose Neural Clus-\ntering Method to learn the sparse pattern of the\nAttention. It can be seamlessly integrated into neu-\nral networks for joint training and optimization.\nIn our method, the cluster center (centroid) is up-\ndated by a weighted sum of all word hidden states.\nAt the same time, the members of clusters are di-\nvided according to the subordinate matrix of the\ncentroids and word hidden states. The optimiza-\ntion of the clustering loss can guide the represen-\ntation of word hidden states while learning cluster\ncentroids. The integration of the neural clustering\nmethod and attention training enables our Neural\nClustering Attention to perform better than previ-\nous clustering-based sparse attention mechanisms.\nOur Neural Clustering Method is a general cluster-\ning method, in the sense that in addition to being\nintegrated into the network of the speciï¬c task, it\nis can handle clustering tasks alone.\nOur overall model is called ClusterFormer and it\nis obtained by replacing the Self-Attention Mecha-\nnism in Transformer with Neural Clustering Atten-\ntion Mechanism. In order to validate the beneï¬ts\nof ClusterFormer, we have carried out comparison\nexperiments of the efï¬ciency and effectiveness re-\nspectively. For efï¬ciency, we provide a detailed\nanalysis about the time and memory on dataset\n20NEWS of text classiï¬cation. Results show that\nour model has a comparable or even better efï¬-\nciency compared with two typical sparse attention\nmodels, Reformer and Routing Transformer. Espe-\ncially, when the sequence length exceeds 2000, our\nmodel, Routing Transfomer and Reformer reduce\nthe memory of Transformer by 53.8%, 60.8% and\n31.8% while reducing the training time by 51.4%,\n41.8% and 14.4%, respectively on GPU. For ef-\nfectiveness, we test it on machine translation, text\nclassiï¬cation, natural language inference, and text\nmatching tasks. Experimental results show that\non all tasks, our model consistently outperforms\nReformer and Routing Transformer. In particular,\nour method improves the accuracy by 15.6% and\n7.2% on SciTail datasets of natural language infer-\nence task compared with Reformer and Routing\nTransformer, respectively.\nThe major contributions of our work are as fol-\nlows:\nâ€¢ We propose a general end-to-end fuzzy clus-\ntering method based on neural network,\nnamed Neural Clustering Method, which can\ndynamically learn weights of each word and\nthen update centroids by weighting all the in-\nput words along with the training of speciï¬c\ntasks.\nâ€¢ We design the Neural Clustering Attention\nMechanism based on our proposed cluster-\ning method to refactor Self-Attention Mecha-\nnism. The experimental results show that our\nmethod has comparable efï¬ciency and bet-\nter effectiveness than typical sparse attention\nmodels.\n2 Related Work\n2.1 Self-Attention Mechanism\nThe Self-Attention is the core component of Trans-\nformer (Vaswani et al., 2017). It extracts sequence\nfeatures by processing the interaction of three se-\nquence matrices Q, K, and V. Referring to the\nstandard Transformer, its function can be written\nas follows:\nAttention(Q,K,V ) =Softmax( QKT\nâˆš\nd\n)V\nQ,K,V = XWQ,XW K,XW V\n(1)\nwhere X âˆˆ RNÃ—dmodel, Q, K,V âˆˆ RNÃ—d,\nWQ,WV âˆˆRdmodelÃ—d, N is the length of the se-\n2391\nWord sequence \nafter sort\nQuery after \nequal chunking\nKey after \nequal chunking\nWord sequence\nafter clustering\n1      2     3      4      5     6      7      8      9\n1      5     2      3      4      6      7     8      9\n1    5    2             3    4    6              7    8   9\n1    5    2              3    4   6             7    8    9\nğ‘‹1\nğ‘‹2\nğ‘‹3\nğ‘‹4\nğ‘‹5\nğ‘‹6\nğ‘‹7\nğ‘‹8\nğ‘‹9\nX ( Word Sequence )\nÃ—\nU (Subordinate Matrix  )\nClustering\nNormalization\nSort\nWord  \nSequence\nI ( Cluster index )\nÃ—\nğ‘‹ğ‘†( Sorted Word Sequence )\náˆ˜ğ¶( New Centroids )C ( Centroids )\nğ¶1\nğ¶2\nğ¶3\n1\n2\n2\n2\n1\n2\n3\n3\n3\nğ‘‹1\nğ‘‹5\nğ‘‹2\nğ‘‹3\nğ‘‹6\nğ‘‹4\nğ‘‹7\nğ‘‹8\nğ‘‹9\náˆ˜ğ¶1\náˆ˜ğ¶2\náˆ˜ğ¶3\nï¼ˆaï¼‰Neural Clustering Method ï¼ˆbï¼‰Sorting and Chunking Operation\nFigure 1: (a) is the Neural Clustering Method. Its main idea is to update centroids by weighting all the word hidden\nstates and divide clusters according to the subordinate matrix. (b) is the sorting and chunking operations. It aims\nto make each cluster perform the attention mechanism in parallel.\nquence, dmodel is the dimensionality of the model,\nand dis the dimensionality of the attention head. In\nSelf-Attention Mechanism, the interaction ofQand\nKgives the N Ã—N attention (weight) matrix, and\nit leads to the complexity of O(N2d), which has\nbeen one of the crucial limitation of Transformer.\n2.2 Sparse variants of Transformer\nTransformer has been developed into many variants\nto reduce the complexity of the attention mecha-\nnism. In these works, one of the main research\ndirections is to use a sparse attention to substitute\nthe quadratic-cost attention.\nSome early works (Qiu et al., 2020; Ho et al.,\n2019; Beltagy et al., 2020) have been proposed\nto reduce the time complexity by restricting ev-\nery query to focus only on keys that are nearby or\nat ï¬xed intervals. This method ï¬xes the sparsity\npattern without considering the similarity between\nqueries and keys, limiting its ability to assemble\ncritical information from large contexts. Different\nfrom these works, our method attempts to automat-\nically aggregate critical keys for each query based\non dependency relationships.\nMoreover, the clustering-pattern methods were\nused in Self-Attention to implement a sparse atten-\ntion. For example, Reformer (Kitaev et al., 2020)\nand Routing Transformer (Roy et al., 2021) in-\ntroduce Locality-Sensitive Hashing and K-Means\nalgorithms, respectively, to reduce complexity to\nO(Nlog Nd) and O(N\nâˆš\nNd). However, in this\nkind of method, the clustering process and train-\ning process are separate, which is a limitation in\nimproving effectiveness. Based on previous re-\nsearches, we proposed a novel Neural Clustering\nMethod, which can be seamlessly integrated into\nthe network of speciï¬c tasks for joint training and\noptimization to improve effectiveness.\n3 Model\nIn this section, we ï¬rst introduce our Neural Clus-\ntering Method. Then, we introduce our Neural\nClustering Attention Mechanism which combines\nour clustering method and the Self-Attention Mech-\nanism.\n3.1 Neural Clustering Method\nAs shown in Figure 1 (a), our clustering method\ntakes word hidden states X âˆˆRNÃ—d and centroid\nhidden states C âˆˆRkÃ—d as inputs. Cis initialized\nrandomly in the ï¬rst layer. Then, we can get the\nsubordinate (similarity) matrix U between word\nvectors and centroid vectors. It can be deï¬ned as:\nUij = exp(Ï†(Ci,XjWC))âˆ‘N\nj=1 exp(Ï†(Ci,XjWC))\n1 â‰¤iâ‰¤k, 1 â‰¤j â‰¤N\n(2)\nwhere k is the number of clusters and N is the\nlength of sequence. WC âˆˆ RdmodelÃ—dmodel is a\nparameter matrix. Ï†(Â·) is a similarity measure func-\ntion and it is the inner product operation in this\nscenario. Xj is the j-th row of matrix X and Ci\nis the i-th row of matrix C. The subordinate value\nUij âˆˆ[0,1] is the normalized similarity value be-\ntween the i-th centroid vector and emphj-th word\nvector, and it represents the degree of the word Xj\nbelonging to the centroid Ci.\nThen, we get the updated centroids by weight-\ning all the word hidden states. The corresponding\nformula is as follows:\nË†Ci =\nNâˆ‘\nj=1\nUijXjWC 1 â‰¤iâ‰¤k (3)\n2392\nNeural \nClustering \nMethod\nX ( Word Sequence )\nC ( Centroids ) áˆ˜ğ¶ ( New Centroids ) ğ‘ğ‘‚1\nğ‘ğ‘‚2\nğ‘ğ‘‚3 Z ( Output )\nChunking\nğ‘‹ğ‘†(Sorted Word Sequence )\nNext layer\nSelf-Attention mechanism \nSelf-Attention mechanism \nSelf-Attention\nğ‘‹1,2,3\nğ‘„\nğ‘‹1,2,3\nğ¾ğ‘‰\nğ‘‹1\nğ‘‹2\nğ‘‹3\nğ‘‹4\nğ‘‹5\nğ‘‹6\nğ‘‹7\nğ‘‹8\nğ‘‹9\nğ‘‹1\nğ‘‹5\nğ‘‹2\nğ‘‹3\nğ‘‹6\nğ‘‹4\nğ‘‹7\nğ‘‹8\nğ‘‹9\nConcat Next layer\nResort\nğ¶1\nğ¶2\nğ¶3\náˆ˜ğ¶1\náˆ˜ğ¶2\náˆ˜ğ¶3\nFigure 2: Neural Clustering Attention Mechanism. First, we group vectors with strong dependency into the same\ncluster and get a new word sequence XS. Second, we carry out an equal chunking operation to get some block\nsequences, and then perform the attention within each cluster separately.\nwhere iand jrepresent the index value of the cen-\ntroid and word, respectively. Then we group the\nword vectors according to the subordinate matrix\nU, as follows:\nIj = Argmax(U:j) 1 â‰¤j â‰¤N (4)\nwhere U:j is the j-th column of the matrix U and\nfunction Argmax(Â·) assigns word hidden states to\nthe corresponding cluster according to the maxi-\nmum subordinate value. Therefore, I âˆˆRN repre-\nsents the cluster index of all the word hidden states.\nThen, we sort the word vectors according to the\ncluster indexes I, as follows:\nXS, I\nâ€²\n= Sort(X,I) (5)\nwhere the function Sort(Â·) is used to arrange word\nhidden states belonging to the same cluster to ad-\njacent positions in ascending order of cluster in-\ndex. XS âˆˆRNÃ—dmodel is the sorted word vectors.\nI\nâ€²\nâˆˆRN is used to record the original positions\nof shufï¬‚ed word hidden states in the sequence and\nwill be used in Eq. 10. Through the above process,\nwe get the grouped and sorted word hidden states\nXS, as shown in Figure 1 (a).\nClustering Loss: Clustering Loss ( L1) is the\nmean of the negative similarity scores of word hid-\nden states and their belonging centroids, and it\nwill give guidance to learn the optimal clustering\nscheme. It is deï¬ned as follows:\nL1 = âˆ’1\nN\nNâˆ‘\nj=1\nÏ†\n(\nXj, Ë†CIj\n)\n(6)\nwhere Xj, Ë†CIj represent the j-th word hidden state\nin the sequence and the updated centroid. The\nfunction Ï†(Â·) is a similarity measure function and\nneeds to be consistent with Eq. 2.\nFrom the above analysis, our Neural Clustering\nMethod is based on the soft clustering. There is a\nsubordinate value between each pair of word vec-\ntors and centroid vectors, which can quantitatively\ndescribe the fuzzy relationship, so that the cluster-\ning can be carried out objectively and accurately.\nIn addition, Neural Clustering Method is based on\nthe neural network, which is easy to integrate into\nthe network corresponding to the target task. The\nreconstruction of centroid vectors depends on all\nthe word vectors and is based on the continuous\noptimization for the clustering objective function\n(as shown in Eq. 6) and the task-speciï¬c objective\nfunction to get better effectiveness.\nIn addition, we carried out a clustering compar-\nison experiments between our method and tradi-\ntional clustering methods and observed improve-\nments of our method in effectiveness. See Ap-\npendix A for more details.\n3.2 Neural Clustering Attention Mechanism\nAs described in Section 3.1, our Neural Clustering\nMethod groups word vectors with strong depen-\ndency into the same cluster and outputs the sorted\nword vectors XS. Then, we use different matrices\nto project XS into matrix QS, KS, and VS, as\nfollows:\nQS,KS,V S = XSWQ,XSWK,XSWV (7)\nwhere WQ, WK and WV âˆˆRdmodelÃ—d are weight\nmatrices. QS, KS and VS are matrices Query,\nValue and Key, respectively.\nThe number of members in each cluster may not\nbe uniform, which makes it difï¬cult for all clusters\nto perform the attention mechanism in parallel. For\nparallel computing, after arranging word hidden\nstates in the same cluster to be in adjacent positions,\n2393\nwe chunk them into equal blocks in order, as shown\nin Figure 1 (b) (essentially similar to the masking of\nReformer). The process can be written as follows:\nQOi = QS\n(\n(iâˆ’1)\nâŒˆN\nk\nâŒ‰\n: i\nâŒˆN\nk\nâŒ‰]\nKOi = KS\n(\n(iâˆ’2)\nâŒˆN\nk\nâŒ‰\n: i\nâŒˆN\nk\nâŒ‰]\n1 â‰¤iâ‰¤k\n(8)\nwhere QOi âˆˆRwÃ—d and KOi âˆˆR2wÃ—d are the\ni-th Query block and Key block respectively. w\n(w= N\nk ) is the number of members in each block.\nMatrix VOi has operations similar to KOi. Af-\nter chunking, Query contains one sequence block\nwhile Key and Value consist of two contiguous\nblocks, which corresponds to L2 mentioned in\nEq. 11. Each token in Query focuses on two blocks\nof tokens so that the query can cover the words in\nthe same cluster as much as possible. Of course, it\ndoes not have to be 2, and can be adjusted.\nThen, we perform the attention operation within\nthe sequence block in parallel and concatenate the\noutput of each block.\nZOi = Attention(QOi,KOi,V Oi)\nZO = Concat(ZO1 ,...,Z Ok ) 1 â‰¤iâ‰¤k\n(9)\nwhere ZOi âˆˆRwÃ—d and ZO âˆˆRNÃ—d. ZOi is the\noutput of the i-th sequence block after the attention\noperation.\nFinally, we recover the shufï¬‚ed sequence (out-\nput) to obtain the ï¬nal result, as follows:\nZ = Resort(ZO,I\nâ€²\n) (10)\nwhere the function Resort(Â·) aims to recover shuf-\nï¬‚ed sequence according to the original position\nrecord vector I\nâ€²\nobtained from the Eq. 5. Z âˆˆ\nRNÃ—d is the output of Neural Clustering Attention.\nFor the autoregressive modeling, we provide a\nMasked Neural Clustering Attention Mechanism to\nprevent the leftward information ï¬‚ow. More details\ncan be found in Appendix B.\nCentroid Sorting Loss: Centroid Sorting Loss\n(L2) is the mean of the negative similarity scores of\nthe adjacent centroid pairs. In Eq. 8, each token in\nQuery block is expected to focus on two continuous\nblocks of tokens. L2 makes word hidden states\nbelonging to adjacent clusters are also close to each\nother. It is deï¬ned as follows:\nL2 = âˆ’1\nk\n(( kâˆ‘\ni=2\nÏ†\n(\nË†Ci, Ë†Ciâˆ’1\n))\n+ Ï†\n(\nË†C1, Ë†Ck\n))\n(11)\nwhere k is the number of centroids, Ë†Ci is the i-\nth updated centroid, and the meaning of Ï†(Â·) is\nconsistent with Eq. 6.\nIn our method, Clustering Loss, Centroid Sorting\nLoss, and the loss of target tasks of the model are\nassigned different weights for joint optimization.\nMore details can be found in Appendix C.\n3.3 Analysis of Complexity\nThe complexity of Neural Clustering Attention\nMechanism comes from two parts: (i) Neural Clus-\ntering Method. In this part, we need to calculate the\nsubordinate matrix between centroid hidden states\nC âˆˆRkÃ—d and the word hidden states X âˆˆRNÃ—d,\nreferring to the Eq. 2, which leads to the complex-\nity of O(Nkd). (ii) Attention Mechanism. For this\npart, we compute attention within the Query block\n(âˆˆRkÃ—wÃ—d) and Key block ( âˆˆRkÃ—2wÃ—d), refer-\nring to the Eq. 9, which leads to the complexity of\nO(kw2d) where w= N\nk . In summary, the overall\ncomplexity is O(Nkd + kw2d). When kis set toâˆš\nN, the complexity is approximately O(N\nâˆš\nNd).\n4 Experiments\nIn order to verify the effectiveness and efï¬ciency\nof our method, we carried out the following tasks.\nWe choose Transformer and its clustering-pattern\nvariants (Reformer, Routing transformer) as base-\nline models. The implementations of the attention\nlayer of Reformer and Routing transformer refer\nto the open source codes 12 . For a fair comparison,\nour proposed method and baseline models have the\nsame architecture, except for the attention layer.\n4.1 Machine Translation\nWe validate our model on IWSLT14 German-\nEnglish and WMT14 English-German benchmarks,\nwhich have been widely used for machine transla-\ntion tasks. For IWSLT14 De-En, it contains about\n160K training sentence pairs and is pre-processed\nby using prepare-iwslt14en2de.sh 3. For WMT14\nEn-De, it contains about 4.5 million training sen-\ntence pairs and it is pre-processed by using prepare-\nwmt14en2de.sh 4. We use the BLEU score as the\neffectiveness evaluation metric. Some hyperparam-\neters are set: the number of encoder and decoder\n1https://github.com/lucidrains/reformer-pytorch\n2https://github.com/lucidrains/routing-transformer\n3https://github.com/pytorch/fairseq/blob/master/examples\n/translation/prepare-iwslt14.sh\n4https://github.com/pytorch/fairseq/blob/master/examples\n/translation/prepare-wmt14en2de.sh\n2394\nModel IWSLT14 De-En WMT14 En-De\nTransformerâ€ (Vaswani et al., 2017) 34.4 27.3 / 26.4\nReformerâ€ (Kitaev et al., 2020) 34.0 26.3 / 25.4\nRouting Transformerâ€ (Roy et al., 2021) 32.5 24.3 / 23.6\nClusterFormer 34.9 27.4 / 26.5\nTable 1: Test BLEU on IWSLT14 (De-En) and WMT14(En-De). For IWSLT14, we reported Tokenized BLEU\nresults. For WMT14, we reported Tokenized BLEU and SacreBLEU results, arranged on the left and right. \" â€ \"\nindicates that the results of the model are our implementations.\nModel CR MR SUBJ MPQA 20NEWS Average\nDiSAN (Shen et al., 2018) 84.8 â€“ 94.2 90.1 â€“ â€“\nMPSAN (Dai et al., 2020) 85.4 â€“ 94.6 90.4 â€“ â€“\nTransformerâ€ (Vaswani et al., 2017) 86.2 81.8 95.4 89.9 83.6 87.38\nReformerâ€ (Kitaev et al., 2020) 83.0 79.7 94.7 88.6 81.7 85.54\nRouting Transformerâ€ (Roy et al., 2021) 80.1 78.8 94.3 81.2 81.3 83.14\nClusterFormer 88.1 82.7 96.2 90.4 83.8 88.24\nTable 2: Experimental results (Accuracy) on text classiï¬cation tasks. \"â€“\" means that results are not reported.\nlayers L= 6, the number of centroids k= 3. The\ndimension of word embedding and model dmodel\n= 512. Speciï¬cally, for IWSLT14, the number of\nheads is set to 4 and dff = 1024. For WMT14, the\nnumber of heads is set to 8 and dff = 2048.\nAs shown in Table 1, our method boosts effec-\ntiveness on both datasets. Speciï¬cally, the Tok-\nenized BLEU score is improved by at least 1.5%\ncompared with other models on IWSLT14 datasets.\nCompared with the latest models Reformer and\nRouting Transformer, ClusterFormer respectively\nhas 2.6% and 7.4% improvement. Our method\nshows the same trend on WMT14 datasets. Es-\npecially, compared with Reformer and Routing\nTransformer, the Tokenized BLEU score of Cluster-\nFormer respectively has 4.2% and 12.8% improve-\nment and the sacreBLEU score respectively has\n4.3% and 12.3% improvement.\n4.2 Text Classiï¬cation\nWe validate our model on ï¬ve text classiï¬cation\ntasks. CR (Hu and Liu, 2004): Customer re-\nviews composed of positive or negative product\nreviews; MR (Pang and Lee, 2004): Movie re-\nviews divided into positive and negative categories;\nSUBJ: Subjectivity dataset where the target is to\nclassify a text as being subjective or objective;\nMPQA (Wiebe et al., 2005): Opinion polarity de-\ntection subtask. 20NEWS: A international standard\ndataset for text classiï¬cation, text mining, and in-\nformation retrieval research. The dataset collects\nabout 20,000 newsgroup documents, divided into\na collection of newsgroups on 20 different topics.\nAccuracy is used as the evaluation metric for these\ndatasets. In addition, for all datasets, word embed-\ndings are initialized by GloVe (Pennington et al.,\n2014) with 300-dimension. Some hyperparameters\nare set: The number of encoder layers L= 2, the\ndimension of model d= 300, the number of heads\nh= 4, and the number of centroids k is adjusted\nnear the square root of the max length.\nAs shown in Table 2, ClusterFormer outperforms\nall baseline models and improves the test accu-\nracy by at least 3.16%, 1.70% for CR and SUBJ\ndatasets, respectively. In addition, on the MPQA\ndataset, ClusterFormer achieves a comparable re-\nsult with MPSAN. We also carry out the text clas-\nsiï¬cation task on the long text dataset 20NEWS.\nThe accuracy for the 20NEWS dataset increases at\nleast 0.24% compared with other models. In ad-\ndition, compared with the latest models Reformer\nand Routing Transformer, our model respectively\nhas 6.1%, 3.8%, 1.6%, 2.0%, 2.6% and 10.0%,\n4.9%, 2.0%, 11.3%, 3.1% improvement for CR,\nMR, SUBJ, MPQA and 20NEWS datasets.\n4.3 Natural Language Inference (NLI) and\nText Matching\nIn this section, we conduct Natural Language\nInference tasks on SNIL, SciTail datasets, and\nText Matching tasks on Quora, WikiQA datasets.\nSNLI (Bowman et al., 2015) is a benchmark dataset\n2395\nModel SNLI SciTail Quora WikiQA\nmap mrr\nDELTA (Han et al., 2019) 80.7 â€“ â€“ â€“ â€“\nBigram-CNN (Yu et al., 2014) â€“ â€“ â€“ 0.619 0.628\nTransformerâ€ (Vaswani et al., 2017) 83.7 76.6 85.4 0.601 0.613\nReformerâ€ (Kitaev et al., 2020) 78.6 67.3 74.3 0.587 0.603\nRouting Transformerâ€ (Roy et al., 2021) 76.3 72.6 81.5 0.560 0.574\nClusterFormer 83.9 77.8 85.4 0.630 0.648\nTable 3: Experimental results on Neural Language Inference (NLI) and Text Matching tasks.\nfor natural language inference. There are 570k\nhuman-annotated sentence pairs with four labels.\nSciTail (Khot et al., 2018) is an entailment classiï¬-\ncation dataset constructed from science questions\nand answers. Quora Question Pairs is a dataset for\nparaphrase identiï¬cation with two classes indicat-\ning whether one question is a paraphrase of the\nother. The evaluation metric for these three data\nsets is Accuracy. WikiQA (Yang et al., 2015) is a\nretrieval-based question answering dataset based on\nWikipedia, which is composed of 20.4k/2.7k/6.2k\n(train/dev/test) samples. The mean average pre-\ncision (MAP) and mean reciprocal rank (MRR)\nare used as the evaluation metrics. For SNIL and\nQuora datasets, word embeddings are initialized by\nGloVe (Pennington et al., 2014) with 300 dimen-\nsions. For the rest, we use random word embedding\nvectors with 300 dimensions. Some hyperparame-\nters are set: L= 1, the number of heads h= 6 and\nthe number of centroids k= 3.\nAs shown in Table 3, our model achieves the best\nresults for most datasets. Speciï¬cally, the accuracy\nof our model is at least 1.6% higher than baseline\nmodels on the SciTail dataset. On WikiQA, our\nmodel improves the result by at least 1.8% and\n3.2% in MAP and MRR evaluation metrics, respec-\ntively. Our model and Transformer have consid-\nerable effectiveness on SNLI and Quora datasets.\nIn addition, compared with the latest models Re-\nformer and Routing Transformer, our model has\n6.7%, 15.6%, 14.9%, and 10.0%, 7.2%, 4.8% im-\nprovement for SNLI, SciTail, Quora datasets. For\nthe WikiQA dataset, the score increases 7.3% and\n7.5% by our model in MAP and MRR compared to\nReformer. The score increases 12.5% and 13.0%\ncompared to Routing Transformer.\n4.4 The choice of clustering numbers k\nIn this section, we test the effect of different cluster-\ning numbers (k) on the effectiveness and efï¬ciency.\nModel Acc(%) Memory(MiB) Time (s)\nTransformer 84.8 21268 (x) 263.5 (y)\nOur (k=5) 84.9 10980 (1.48x) 197.9 (1.25y)\nOur (k=10) 84.3 9312 (1.56x) 169.4 (1.36y)\nOur (k=20) 85.0 8542 (1.60x) 153.6 (1.42y)\nOur (k=30) 84.5 8282 (1.61x) 147.6 (1.44y)\nOur (k=60) 84.3 8072 (1.62x) 150.4 (1.43y)\nOur (k=100) 83.6 7920 (1.63x) 151.7 (1.42y)\nOur (k=300) 83.6 7830 (1.63x) 170.0 (1.35y)\nOur (k=500) 83.2 8444 (1.60x) 194.4 (1.26y)\nTable 4: Experimental results of different clustering\nnumbers kfor ClusterFormer.\nWe test our model on the 20NEWS dataset of text\nclassiï¬cation tasks with a NVIDIA V100 (16GB)\nGPU. Some hyperparameters are set: the number of\nencoder layers Lis 2, the dimension of the model\ndis 300, the batch size is 64, and the max sequence\nlength N is 1500.\nFrom Table 4, we can draw the following conclu-\nsions: (i) Accuracy of our model: In general, within\na certain range during the growth of k, the perfor-\nmance of our model is relatively stable. When the\nvalue of k goes beyond a certain threshold, the\nperformance of our model degrades; (ii) Memory\ncost of our model: As the number of centroids k\nincreases, the memory cost of the model decreases\nï¬rst and then increases; (iii) Training time of our\nmodel: As the number of centroids kincreases, the\ntraining time of the model also decreases ï¬rst and\nthen increases. Therefore, according to this law,\nour method can simultaneously gain both the effec-\ntiveness and efï¬ciency of the model by determining\nan appropriate kvalue through ï¬nite experiments.\n4.5 Ablation study for Clustering Losses\nIn this section, we provide an ablation experiment\nabout the two kinds of clustering losses. We ver-\nify the effectiveness of the two loss modules by\nassigning different weight. Some hyperparameters\n2396\n(a) (b)\nFigure 3: (a), (b) is the training and inference time versus the sequence length on GPU.\nDatasets / Weight(L1/L2) N/N N/Y Y/N Y/Y\nSciTail(Acc) 77.19 77.75 75.96 78.32\nWikiQA(map) 0.612 0.620 0.619 0.631\nWikiQA(mrr) 0.636 0.640 0.633 0.648\nTable 5: Experimental results of Clustering Loss abla-\ntion. Speciï¬cally, â€™Nâ€™ represents the weight is zero and\nâ€™Yâ€™ represents the weight is non-zero.\nare set: the number of encoder layers Lis 1, the\ndimension of model dis 300, the batch size is 128\nand the max sequence length N is 500.\nFrom Table 5, the experimental result shows that\nboth L1 and L2 contribute to the performance. For\nexample, on dataset SciTail, the accuracy with the\nbest result is improved by 1.46% (acc) compared\nwith the result without the two losses. On dataset\nWikiQA, the accuracy with the best result is im-\nproved by 3.10% (map), 1.89% (mrr) compared\nwith the result without the two losses.\n4.6 Time and Memory Analysis\nIn this section, we provide a comparison experi-\nment on dataset 20NEWS about the time and mem-\nory cost for different models. About the dataset, its\naverage sequence length is approximately 280 and\nthe maximum sequence length exceeds 10,000. To\ncompare time and memory cost, we set the range\nof sequence length N as (0, 2000] and batch size\nto 20. We test the memory and time cost on a\nNVIDIA V100 GPU. We take the time of 1000\nsteps forward propagation of the model as the in-\nference time, and the time of 1000 steps forward\nand back propagation as the training time.\nAs shown in Figure 4, as the sentence length in-\ncreases, both Routing Transformer and our model\ncan signiï¬cantly reduce memory cost compared to\nTransformer. When N exceeds 2000, our model,\nRouting Transfomer and Reformer reduce the mem-\nory by 53.8%, 60.8%, and 31.8%, respectively.\nAs shown in Figure 3, the training time of Trans-\nFigure 4: Memory cost versus the sequence length.\nformer increases signiï¬cantly with increasing se-\nquence length, while our model and Routing Trans-\nformer have a relatively small increase on GPU de-\nvices. When N is 2000, our model, Routing Trans-\nfomer and Reformer reduce the training time by\n51.4%, 41.8%, and 14.4%. However, the inference\nspeed of these improvements is inferior compared\nwith Transformer, which may be caused by the de-\ncrease of the model parallelism. The above analysis\nfully demonstrates the efï¬ciency and effectiveness\nof our proposed Neural Clustering Mechanism.\n5 Conclusion\nIn this paper, we propose a Neural Clustering Atten-\ntion Mechanism to address the reduced effective-\nness issue in sparse attention methods. This issue\nis mainly caused by the introduction of a sparse pat-\ntern that is separated from the target task or does\nnot consider the similarity between words. In our\nmethod, we design a neural clustering algorithm to\nbetter capture critical pairs of dependencies. We\nintegrate this clustering algorithm and the neural\nnetwork to jointly train and optimize with speciï¬c\ntasks together to further contribute to the effective-\nness and efï¬ciency. The experimental results show\nthat our model can achieve better effectiveness and\na comparable or even better efï¬ciency, compared\nwith the latest typical sparse attention models, Re-\nformer and Routing Transformer.\n2397\nAcknowledgements\nThis work is supported in part by the state\nkey development program of China (grant\nNo.2017YFE0111900), Natural Science Founda-\ntion of China (grant No.61772363), PolyU internal\nfund (grant No.1-BD47) under the research project\n(P0039657) of the Hong Kong Polytechnic Univer-\nsity, and the Beijing Academy of Artiï¬cial Intelli-\ngence(BAAI).\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nIz Beltagy, Matthew E. Peters, Arman Cohan, et al.\n2020. Longformer: The long-document transformer.\nCoRR, abs/2004.05150.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632â€“642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers. CoRR, abs/1904.10509.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, David Belanger,\nLucy Colwell, et al. 2020. Masked language mod-\neling for proteins via linearly scalable long-context\ntransformers. CoRR, abs/2006.03555.\nBiyun Dai, Jinlong Li, and Ruoyi Xu. 2020. Mul-\ntiple positional self-attention network for text clas-\nsiï¬cation. In The Thirty-Fourth AAAI Conference\non Artiï¬cial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiï¬cial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in Artiï¬cial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7610â€“7617. AAAI Press.\nKun Han, Junwen Chen, Hui Zhang, Haiyang Xu, Yip-\ning Peng, Yun Wang, Ning Ding, Hui Deng, Yonghu\nGao, Tingwei Guo, Yi Zhang, Yahao He, Baochang\nMa, Yulong Zhou, Kangli Zhang, Chao Liu, Ying\nLyu, Chenxi Wang, Cheng Gong, Yunbo Wang, Wei\nZou, Hui Song, and Xiangang Li. 2019. DELTA: A\ndeep learning based language technology platform.\nCoRR, abs/1908.01853.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn,\nand Tim Salimans. 2019. Axial attention in multi-\ndimensional transformers. CoRR, abs/1912.12180.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 168â€“177.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and FranÃ§ois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear at-\ntention. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-\n18 July 2020, Virtual Event, volume 119 of Proceed-\nings of Machine Learning Research , pages 5156â€“\n5165. PMLR.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiï¬cial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nï¬cial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artiï¬cial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 5189â€“5197. AAAI Press.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efï¬cient transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. In 6th International Conference\non Learning Representations, ICLR 2018, Vancou-\nver, BC, Canada, April 30 - May 3, 2018, Confer-\nence Track Proceedings. OpenReview.net.\nBo Pang and Lillian Lee. 2004. A sentimental edu-\ncation: Sentiment analysis using subjectivity sum-\nmarization based on minimum cuts. In Proceed-\nings of the 42nd Annual Meeting of the Association\nfor Computational Linguistics (ACL-04), pages 271â€“\n278, Barcelona, Spain.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532â€“1543, Doha,\nQatar. Association for Computational Linguistics.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,\nSinong Wang, and Jie Tang. 2020. Blockwise self-\nattention for long document understanding. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 2555â€“2565, Online. As-\nsociation for Computational Linguistics.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efï¬cient content-based\n2398\nsparse attention with routing transformers. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:53â€“68.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nShirui Pan, and Chengqi Zhang. 2018. Disan: Di-\nrectional self-attention network for rnn/cnn-free lan-\nguage understanding. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiï¬cial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nï¬cial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artiï¬cial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 5446â€“5455. AAAI Press.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020a. Synthesizer:\nRethinking self-attention in transformer models.\nCoRR, abs/2005.00743.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020b. Efï¬cient transformers: A survey.\nCoRR, abs/2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998â€“6008.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity. CoRR, abs/2006.04768.\nJanyce Wiebe, Theresa Wilson, et al. 2005. Annotating\nexpressions of opinions and emotions in language.\nLang. Resour. Evaluation, 39(2-3):165â€“210.\nYunyang Xiong, Zhanpeng Zeng, Rudrasis\nChakraborty, Mingxing Tan, Glenn Fung, Yin\nLi, and Vikas Singh. 2021. Nystr Â¨omformer:\nA nystr Â¨om-based algorithm for approximating\nself-attention. CoRR, abs/2102.03902.\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\nWikiQA: A challenge dataset for open-domain ques-\ntion answering. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2013â€“2018, Lisbon, Portugal. As-\nsociation for Computational Linguistics.\nLei Yu, Karl Moritz Hermann, Phil Blunsom, and\nStephen Pulman. 2014. Deep learning for answer\nsentence selection. CoRR, abs/1412.1632.\n2399\nA Comparison experiment for clustering\nmethods\nIn this section, we carry out the comparison experi-\nment between the Neural Clustering Method and\nother clustering methods to verify the effectiveness\nof our clustering method.\nFirstly, according to the division mode, we intro-\nduce the following two kinds of clustering methods.\nHard Clustering: Each element to be recog-\nnized is strictly divided into a certain cluster . It\ndeï¬nes an either/or relationship R âˆˆ{0,1}be-\ntween the element and clusters.\nSoft Clustering (Fuzzy Clustering): Each ele-\nment to be recognized is subordinate to all clusters\n(with different subordinate values). It deï¬nes a\nfuzzy relationship U âˆˆ[0,1] between the element\nand clusters.\nRegarding the selection of the comparative clus-\ntering method, we chose the classic hard clustering\nalgorithm K-means (used for Routing Transformer)\nand the soft clustering algorithm SOM, a compet-\nitive neural network. In addition, since Locality\nSensitive Hashing (used for Reformer) cannot con-\nstruct the loss function (no iteration condition), it\ncannot be used for the following clustering task on\nthe MNIST dataset. In the experiment, we set the\nnumber of centroids kto 10, 50, 100, 200, and 300\nrespectively.\nData set SOM K-Means Proposed method\nMNIST(k=10) 59.8 59.2 60.7(+0.9, +1.5)\nMNIST(k=50) 80.9 82.5 82.5 (+1.6, +0)\nMNIST(k=100) 87.8 87.7 90.1(+2.3, +2.4)\nMNIST(k=200) 91.2 90.7 91.8(+0.6, +1.1)\nMNIST(k=300) 93.1 92.0 93.6(+0.5, +1.6)\nTable 6: Comparison of clustering accuracy of different\nclustering algorithms on MNIST task.\nAs shown in Table 6, our method consistently\nhas the best effectiveness in experiments with dif-\nferent centroids. In particular, when the number of\ncentroids exceeds 300, the accuracy of our method\ncan reach 93.6, which improved the accuracy by\n0.54% and 1.74% compared with SOM and K-\nMeans, respectively. From the above analysis, we\nhave conï¬rmed that Neural Clustering Method is\na general clustering method. It can also achieve\nbetter effectiveness compared with K-Means and\nSOM when handling clustering tasks alone.\nB Masked Neural Clustering Attention\nMechanism\nFor autoregressive modeling, we provide a Masked\nNeural Clustering Attention Mechanism to prevent\nthe leftward information ï¬‚ow. We ï¬rst obtain the\noriginal position indexes of the QOi,KOi matrix.\nThe formula is as follows:\nIQOi\n= I\nâ€² (\n(iâˆ’1)\nâŒˆN\nk\nâŒ‰\n: i\nâŒˆN\nk\nâŒ‰]\nIKOi\n= I\nâ€² (\n(iâˆ’2)\nâŒˆN\nk\nâŒ‰\n: i\nâŒˆN\nk\nâŒ‰]\n1 â‰¤iâ‰¤k\n(12)\nwhere IQOi\nâˆˆRw and IKOi\nâˆˆR2w. IQOi\nand\nIKOi\nare the original position indexes of the i-th\nsequence block QOi and KOi respectively.\nThen, we extend IQOi\nin the second dimension\nto get MQOi\nâˆˆRwÃ—2w, and extend IKOi\nin the\nï¬rst dimension to get MKOi\nâˆˆRwÃ—2w. Therefore,\nwe can obtain a mask matrix MOi for the i-th se-\nquence block by comparing these position indexes,\nas follows:\nMOi\nuv = MKOi\nuv â‰¥MKOi\nuv\n1 â‰¤iâ‰¤k, 1 â‰¤uâ‰¤w, 1 â‰¤vâ‰¤2w\n(13)\nwhere MOi âˆˆRwÃ—2w is the mask matrix of the\ni-th block and the MOi is composed of either 0 or\n1. MOi\nuv is the value of theu-th row and v-th column\nof the matrix MOi. Then, for each word in Query\nblock, it will mask the words whose index value\nin Key block is greater than it according to mask\nmatrix MOi, as follows:\nSOi\nuv =\ndâˆ‘\nm=1\nQOi\numKOi\nvmMOi\nuv\n1 â‰¤iâ‰¤k, 1 â‰¤uâ‰¤w, 1 â‰¤vâ‰¤2w\n(14)\nwhere SOi âˆˆRwÃ—2w is the similarity matrix of\nthe i-th sequence block. The subsequent opera-\ntions are the same as Neural Clustering Attention\nMechanism.\nC Optimization of the multi-task\nlearning for ClusterFormer\nAs shown in Figure 1, our model, ClusterFormer,\nconsists of two joint training tasks, clustering tasks,\nand speciï¬c tasks of the model. Therefore, it con-\ntains the loss from the two tasks. The loss functions\nrelated to the clustering task are Clustering Loss\n(L1) and Centroid Sorting Loss (L2). Their equa-\n2400\nEvaluation Metrics Model/Length 2500 5000 7500 10000 15000 25000 35000 45000 55000\nTransformer 3271 7663 15519 â€“ â€“ â€“ â€“ â€“ â€“\nMemory(MiB) Reformer 2699 3807 4795 6067 9515 15867 â€“ â€“ â€“\nRouting Transformer 2129 2467 2841 3189 3779 5477 7451 9117 11397\nClusterFormer 2257 2689 3165 3623 4435 6839 9647 12553 15695\nTransformer 46.5 127.6 260.9 â€“ â€“ â€“ â€“ â€“ â€“\nTraining Time(s) Reformer 30.7 51.2 72.3 89.0 135.0 252.1 â€“ â€“ â€“\nRouting Transformer 24.8 38.5 58.5 69.8 114.9 207.2 343.4 490.1 684.6\nClusterFormer 30.2 44.8 60.4 77.2 115.4 197.1 277.2 375.6 461.3\nTable 7: Memory efï¬ciency (measuring unit: MiB) and Training-time efï¬ciency(measuring unit: second) of dif-\nferent methods under the ultra-long sequence condition. â€™-â€™ represent memory cost exceeds the upper limit of the\nGPU device and the corresponding training time can not be provided.\ntions are as follows:\nL1 = âˆ’1\nN\nNâˆ‘\nj=1\nÏ†(Xj, Ë†CIj )\nL2 = âˆ’1\nk((\nkâˆ‘\ni=2\nÏ†( Ë†Ci, Ë†Ciâˆ’1)) +Ï†( Ë†C1, Ë†Ck))\n(15)\nwhere X and Care respectively word vectors and\ncentroid vectors. N is the length of the input and\nkis the number of clusters. The loss function of a\nspeciï¬c task (e.g., text classiï¬cation) is formulated\nas follows:\nL3 = âˆ’\nnâˆ‘\ni=1\nyilog( Ë†yi) + (1âˆ’yi)log(1 âˆ’Ë†yi) (16)\nwhere Ë†yi is predictive value and yi is the corre-\nsponding target value. Then, the overall loss func-\ntion can be written as:\nLtotal(X,C,W,b ) =ÂµL1 + Î½L2 + Î»L3 (17)\nwhere C, W, and b are respectively centroid pa-\nrameters, weight parameters, and bias parameters\nin ClusterFormer. Âµ, Î½, and Î»â‰¥0 are non-negative\ncoefï¬cients, which are used to adjust the proportion\nof the importance of corresponding tasks.\nTherefore, we can obtain the optimal parameters\nin the neural network by minimizing the loss func-\ntion Ltotal through gradient descent, as follows:\nci = ci âˆ’Î·âˆ‚Ltotal(X,C,W,b )\nâˆ‚ci\n1 â‰¤iâ‰¤kÃ—dmodel\nwj = wj âˆ’Î·âˆ‚Ltotal(X,C,W,b )\nâˆ‚wj\n1 â‰¤j â‰¤m\nbr = br âˆ’Î·âˆ‚Ltotal(X,C,W,b )\nâˆ‚br\n1 â‰¤râ‰¤n\n(18)\nwhere ci, wi and bi represent the element value of\nthe corresponding vectors. dmodel is the dimension-\nality of the model. mand nrepresent the number\nof weight and bias parameters, respectively. Î· is\nFigure 5: Comparison of training loss on SNLI dataset.\nthe learning rate. From the above, it can be seen\nthat the update of centroid, weight, and bias pa-\nrameters is the result of multi-task joint learning in\nClusterFormer.\nD Convergence Analysis\nIn this section, we provide a comparison experi-\nment on the SNLI dataset about the convergence\nspeed for standard Transformer and efï¬cient Trans-\nformers during training, as shown in Figure 5. In\nour experiment, the epochs of different models to\nachieve convergence is: Transformer has 21 epochs,\nReformer has 24 epochs Routing Transformer has\n29 epochs and ClusterFormer has 19 epochs. Com-\npared with Transformer, our model has a compar-\native convergence rate and is more stable. In ad-\ndition, compared with the latest model Reformer\nand Routing Transformer, our model not only has a\nfaster and more stable convergence speed, but also\nhas better effectiveness.\nE Comparison experiment of different\nmethods under the ultra-long sequence\ncondition\nIn this section, we have supplemented the experi-\nment of time and memory cost in different methods\non extremely long sequence tasks. We tested them\non text classiï¬cation of the 20NEWS dataset and\ntrained them on a NVIDIA V100 GPU. We set the\n2401\nModel MNLI QQP QNLI SST2 CoLA STSB MRPC RTE Average\nBert-Small-uncased 77.6/77.0 68.1/87.0 86.4 89.7 27.8 78.8/77.0 83.4/76.2 61.8 74.23\nPretrained ClusterFormer 75.21/75.7 83.1/86.9 82.5 88.3 34.2 82.7/82.4 85.2/77.7 60.3 76.18\nTable 8: GLUE Dev results, the â€œAverageâ€ column represents the average of all datasets scores. F1 and accu-\nracy scores are reported for QQP and MRPC. Spearman correlations are reported for STSB. Accuracy scores are\nreported for the other tasks.\nnumber of centroids k to the square root of the max\nlength and set the batch size to 2 (constrained by\nresources).\nAs shown in Table 7, we can see that our method\nhas a better efï¬ciency advantage on long sequences\ncompared with Transformer. And as the sequence\nlength increases, the advantage in memory and\ntraining time are even more signiï¬cant.\nF Pretraining experiment with the\nNeural Clustering Attention\nMechanism\nIn this section, we pretrain a model with the Neural\nClustering Attention Mechanism with two unsu-\npervised tasks, masked language modeling (MLM)\nand next sentence prediction (NSP). The parame-\nter settings of our pretraining model are similar to\nBERT-small-uncased. Some hyperparameters are\nset: the number of layers Lis 4, the hidden size is\n512, and the number of heads his 8.\nFor downstream tasks, we use the General Lan-\nguage Understanding Evaluation (GLEU) bench-\nmark which is a collection of diverse natural lan-\nguage understanding tasks. We use a batch size\nof 32 and ï¬ne-tune On a scale of 3 to 10 epochs\nover the data for all GLEU tasks. For each task,\nwe selected the best ï¬ne-tuning learning rate (5e-5,\n4e-5, 3e-5, and 2e-5) on the Dev set.\nAs shown in Table 8, experimental results\ndemonstrate that our method can have a good per-\nformance improvement through Pretraining. Es-\npecially, compared with the Bert-Small-uncased,\npretraining ClusterFormer respectively has 23.0%,\n4.9% / 7.0%, and 2.2% / 2.0% improvement on\nCoLA, STSB, and MRPC datasets. The experimen-\ntal results show that our model has the potential\nto do more NLP tasks including pretraining and\nnon-pretraining tasks.\n2402",
  "topic": "ZhÃ ng",
  "concepts": [
    {
      "name": "ZhÃ ng",
      "score": 0.8784255981445312
    },
    {
      "name": "Cluster analysis",
      "score": 0.5648053884506226
    },
    {
      "name": "Transformer",
      "score": 0.5304837822914124
    },
    {
      "name": "Computer science",
      "score": 0.5297718048095703
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4757801592350006
    },
    {
      "name": "Engineering",
      "score": 0.20704466104507446
    },
    {
      "name": "Electrical engineering",
      "score": 0.1450907588005066
    },
    {
      "name": "History",
      "score": 0.1068277359008789
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "China",
      "score": 0.0
    }
  ]
}