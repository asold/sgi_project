{
    "title": "Improving the Transformer Translation Model with Document-Level Context",
    "url": "https://openalex.org/W2962712961",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2168028311",
            "name": "Jiacheng Zhang",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2239152555",
            "name": "Huanbo Luan",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2157167650",
            "name": "Maosong Sun",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2340806901",
            "name": "Feifei Zhai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146806579",
            "name": "Jingfang Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2005452123",
            "name": "Min Zhang",
            "affiliations": [
                "Soochow University"
            ]
        },
        {
            "id": "https://openalex.org/A1983143503",
            "name": "Yang Liu",
            "affiliations": [
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963088995",
        "https://openalex.org/W2799051177",
        "https://openalex.org/W4300428972",
        "https://openalex.org/W2963463964",
        "https://openalex.org/W2963842551",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963084773",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2774875515",
        "https://openalex.org/W2799253188",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2399346130",
        "https://openalex.org/W2100664567",
        "https://openalex.org/W2136353104",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2767019613",
        "https://openalex.org/W2964093087",
        "https://openalex.org/W1851962382",
        "https://openalex.org/W2669742347",
        "https://openalex.org/W2962802109",
        "https://openalex.org/W2964289193",
        "https://openalex.org/W2250761393",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2184135559",
        "https://openalex.org/W44695385",
        "https://openalex.org/W2608029998",
        "https://openalex.org/W2962784628"
    ],
    "abstract": "Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.",
    "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 533–542\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n533\nImproving the Transformer Translation Model\nwith Document-Level Context\nJiacheng Zhang†, Huanbo Luan†, Maosong Sun†, FeiFei Zhai#,\nJingfang Xu#, Min Zhang§and Yang Liu†‡∗\n†Institute for Artiﬁcial Intelligence\nState Key Laboratory of Intelligent Technology and Systems\nDepartment of Computer Science and Technology, Tsinghua University, Beijing, China\n‡Beijing National Research Center for Information Science and Technology\n#Sogou Inc., Beijing, China\n§Soochow University, Suzhou, China\nAbstract\nAlthough the Transformer translation model\n(Vaswani et al., 2017) has achieved state-of-\nthe-art performance in a variety of transla-\ntion tasks, how to use document-level con-\ntext to deal with discourse phenomena prob-\nlematic for Transformer still remains a chal-\nlenge. In this work, we extend the Transformer\nmodel with a new context encoder to repre-\nsent document-level context, which is then in-\ncorporated into the original encoder and de-\ncoder. As large-scale document-level paral-\nlel corpora are usually not available, we intro-\nduce a two-step training method to take full\nadvantage of abundant sentence-level parallel\ncorpora and limited document-level parallel\ncorpora. Experiments on the NIST Chinese-\nEnglish datasets and the IWSLT French-\nEnglish datasets show that our approach im-\nproves over Transformer signiﬁcantly. 1\n1 Introduction\nThe past several years have witnessed the rapid de-\nvelopment of neural machine translation (NMT)\n(Sutskever et al., 2014; Bahdanau et al., 2015),\nwhich investigates the use of neural networks\nto model the translation process. Showing re-\nmarkable superiority over conventional statisti-\ncal machine translation (SMT), NMT has been\nrecognized as the new de facto method and is\nwidely used in commercial MT systems (Wu et al.,\n2016). A variety of NMT models have been pro-\nposed to map between natural languages such as\nRNNencdec (Sutskever et al., 2014), RNNsearch\n(Bahdanau et al., 2015), ConvS2S (Gehring et al.,\n2017), and Transformer (Vaswani et al., 2017).\nAmong them, the Transformer model has achieved\nstate-of-the-art translation performance. The ca-\n∗Corresponding author: Yang Liu.\n1The source code is available at https://github.\ncom/Glaceon31/Document-Transformer\npability to minimize the path length between long-\ndistance dependencies in neural networks con-\ntributes to its exceptional performance.\nHowever, the Transformer model still suffers\nfrom a major drawback: it performs translation\nonly at the sentence level and ignores document-\nlevel context. Document-level context has proven\nto be beneﬁcial for improving translation perfor-\nmance, not only for conventional SMT (Gong\net al., 2011; Hardmeier et al., 2012), but also for\nNMT (Wang et al., 2017; Tu et al., 2018). Baw-\nden et al. (2018) indicate that it is important to ex-\nploit document-level context to deal with context-\ndependent phenomena which are problematic for\nmachine translation such as coreference, lexical\ncohesion, and lexical disambiguation.\nWhile document-level NMT has attracted in-\ncreasing attention from the community in the past\ntwo years (Jean et al., 2017; Kuang et al., 2017;\nTiedemann and Scherrer, 2017; Wang et al., 2017;\nMaruf and Haffari, 2018; Bawden et al., 2018;\nTu et al., 2018; V oita et al., 2018), to the best of\nour knowledge, only one existing work has en-\ndeavored to model document-level context for the\nTransformer model (V oita et al., 2018). Previous\napproaches to document-level NMT have concen-\ntrated on the RNNsearch model (Bahdanau et al.,\n2015). It is challenging to adapt these approaches\nto Transformer because they are designed speciﬁ-\ncally for RNNsearch.\nIn this work, we propose to extend the Trans-\nformer model to take advantage of document-\nlevel context. The basic idea is to use multi-\nhead self-attention (Vaswani et al., 2017) to com-\npute the representation of document-level context,\nwhich is then incorporated into the encoder and\ndecoder using multi-head attention. Since large-\nscale document-level parallel corpora are usually\nhard to acquire, we propose to train sentence-\nlevel model parameters on sentence-level paral-\n534\nTargetEmbedding\nSelf-Attention\nEncoder-DecoderAttentionFeed-Forward\nSelf-Attention\nSourceEmbedding\nFeed-Forward\nSoftmax\nSourceEmbedding\nSelf-Attention\nContext AttentionFeed-Forward\nSelf-Attention\nContextEmbedding\nFeed-Forward\nTargetEmbedding\nSelf-Attention\nContext Attention\nFeed-Forward\nEncoder-Decoder Attention\nSoftmax\n(a) (b)\nFigure 1: (a) The original Transformer translation model (Vaswani et al., 2017) and (b) the extended Transformer\ntranslation model that exploits document-level context. The newly introduced modules are highlighted in red.\nlel corpora ﬁrst and then estimate document-level\nmodel parameters on document-level parallel cor-\npora while keeping the learned original sentence-\nlevel Transformer model parameters ﬁxed. Our\napproach has the following advantages:\n1. Increased capability to capture context : the\nuse of multi-head attention, which signiﬁ-\ncantly reduces the path length between long-\nrange dependencies, helps to improve the ca-\npability to capture document-level context;\n2. Small computational overhead: as all newly\nintroduced modules are based on highly par-\nallelizable multi-head attention, there is no\nsigniﬁcant slowdown in both training and de-\ncoding;\n3. Better use of limited labeled data : our ap-\nproach is capable of maintaining the superi-\nority over the sentence-level counterpart even\nwhen only small-scale document-level paral-\nlel corpora are available.\nExperiments show that our approach achieves\nan improvement of 1.96 and 0.89 BLEU points\nover Transformer on Chinese-English and French-\nEnglish translation respectively by exploiting\ndocument-level context. It also outperforms a\nstate-of-the-art cache-based method (Kuang et al.,\n2017) adapted for Transformer.\n2 Approach\n2.1 Problem Statement\nOur goal is to enable the Transformer translation\nmodel (Vaswani et al., 2017) as shown in Figure\n1(a) to exploit document-level context.\nFormally, letX = x(1),..., x(k),..., x(K) be a\nsource-language document composed ofKsource\nsentences. We use x(k) = x(k)\n1 ,...,x (k)\ni ,...,x (k)\nI\nto denote the k-th source sentence containing\nI words. x(k)\ni denotes the i-th word in the\nk-th source sentence. Likewise, the corre-\nsponding target-language document is denoted\nby Y = y(1),..., y(k),..., y(K) and y(k) =\ny(k)\n1 ,...,y (k)\nj ,...,y (k)\nJ represents the k-th target\nsentence containing Jwords. y(k)\nj denotes the j-th\nword in the k-th target sentence. We assume that\n⟨X,Y⟩constitutes a parallel document and each\n⟨x(k),y(k)⟩forms a parallel sentence.\nTherefore, the document-level translation prob-\nability is given by\nP(Y|X; θ) =\nK∏\nk=1\nP(y(k)|X,Y<k; θ), (1)\nwhere Y<k = y(1),..., y(k−1) is a partial trans-\nlation.\nFor generating y(k), the source documentX can\nbe divided into three parts: (1) thek-th source sen-\ntence X=k = x(k), (2) the source-side document-\n535\nlevel context on the left X<k = x(1),..., x(k−1),\nand (3) the source-side document-level context on\nthe right X>k = x(k+1),..., x(K). As the lan-\nguages used in our experiments (i.e., Chinese and\nEnglish) are written left to right, we omit X>k for\nsimplicity.\nWe also omit the target-side document-level\ncontext Y<k due to the translation error propaga-\ntion problem (Wang et al., 2017): errors made in\ntranslating one sentence will be propagated to the\ntranslation process of subsequent sentences. Inter-\nestingly, we ﬁnd that using source-side document-\nlevel context X<k, which conveys the same infor-\nmation with Y<k, helps to compute better repre-\nsentations on the target side (see Table 8).\nAs a result, the document-level translation prob-\nability can be approximated as\nP(Y|X; θ)\n≈\nK∏\nk=1\nP(y(k)|X<k,x(k); θ), (2)\n=\nK∏\nk=1\nJ∏\nj=1\nP(y(k)\nj |X<k,x(k),y(k)\n<j ; θ), (3)\nwhere y(k)\n<j = y(k)\n1 ,...,y (k)\nj−1 is a partial transla-\ntion.\nIn this way, the document-level translation\nmodel can still be deﬁned at the sentence level\nwithout sacriﬁcing efﬁciency except that the\nsource-side document-level context X<k (or con-\ntext for short) is taken into account.\nIn the following, we will introduce how to rep-\nresent the context (Section 2.2), how to integrate\nthe context (Section 2.3), and how to train the\nmodel especially when only limited training data\nis available (Section 2.4).\n2.2 Document-level Context Representation\nAs document-level context often includes several\nsentences, it is important to capture long-range\ndependencies and identify relevant information.\nWe use multi-head self-attention (Vaswani et al.,\n2017) to compute the representation of document-\nlevel context because it is capable of reducing the\nmaximum path length between long-range depen-\ndencies to O(1) (Vaswani et al., 2017) and deter-\nmining the relative importance of different loca-\ntions in the context (Bahdanau et al., 2015). Be-\ncause of this property, multi-head self-attention\nhas proven to be effective in other NLP tasks such\nas constituency parsing (Kitaev and Klein, 2018).\nAs shown in Figure 1(b), we use a self-attentive\nencoder to compute the representation of X<k.\nThe input to the self-attentive encoder is a se-\nquence of context word embeddings, represented\nas a matrix. Suppose X<k is composed of M\nsource words: X<k = x1,...,x m,...,x M . We\nuse xm ∈RD×1 to denote the vector representa-\ntion of xm that is the sum of word embedding and\npositional encoding (Vaswani et al., 2017). There-\nfore, the matrix representation of X<k is given by\nXc = [x1; ... ; xM ], (4)\nwhere Xc ∈ RD×M is the concatenation of\nall vector representations of all source contextual\nwords.\nThe self-attentive encoder is composed of a\nstack of Nc identical layers. Each layer has two\nsub-layers. The ﬁrst sub-layer is a multi-head self-\nattention:\nA(1) = MultiHead(Xc,Xc,Xc), (5)\nwhere A(1) ∈ RD×M is the hidden state calcu-\nlated by the multi-head self-attention at the ﬁrst\nlayer, MultiHead(Q,K,V) is a multi-head self-\nattention function that takes a query matrix Q, a\nkey matrix K, and a value matrix V as inputs. In\nthis case, Q = K = V = Xc. This is why it\nis called self-attention. Please refer to (Vaswani\net al., 2017) for more details.\nNote that we follow Vaswani et al. (2017) to\nuse residual connection and layer normalization in\neach sub-layer, which are omitted in the presenta-\ntion for simplicity. For example, the actual output\nof the ﬁrst sub-layer is:\nLayerNorm(A(1) + Xc). (6)\nThe second sub-layer is a simple, position-wise\nfully connected feed-forward network:\nC(1) =\n[\nFNN(A(1)\n·,1 ); ... ; FNN(A(1)\n·,M )\n]\n(7)\nwhere C(1) ∈RD×M is the annotation of X<k af-\nter the ﬁrst layer,A(1)\n·,m ∈RD×1 is the column vec-\ntor for the m-th contextual word, and FNN(·) is\na position-wise fully connected feed-forward net-\nwork (Vaswani et al., 2017).\nThis process iterates Nc times as follows:\nA(n) = MultiHead\n(\nC(n−1),C(n−1),C(n−1)\n)\n, (8)\nC(n) =\n[\nFNN(A(n)\n·,1 ); ... ; FNN(A(n)\n·,M )\n]\n, (9)\n536\nwhere A(n) and C(n) (n= 1,...,N c) are the hid-\nden state and annotation at the n-th layer, respec-\ntively. Note that C(0) = Xc.\n2.3 Document-level Context Integration\nWe use multi-head attention to integrate C(Nc),\nwhich is the representation of X<k, into both the\nencoder and the decoder.\n2.3.1 Integration into the Encoder\nGiven thek-th source sentencex(k), we usex(k)\ni ∈\nRD×1 to denote the vector representation of the i-\nth source word x(k)\ni , which is a sum of word em-\nbedding and positional encoding. Therefore, the\ninitial matrix representation of x(k) is\nX= [x(k)\n1 ; ... ; x(k)\nI ], (10)\nwhere X ∈RD×I is the concatenation of all vec-\ntor representations of source words.\nAs shown in Figure 1(b), we follow (Vaswani\net al., 2017) to use a stack of Ns identical lay-\ners to encode x(k). Each layer consists of three\nsub-layers. The ﬁrst sub-layer is a multi-head self-\nattention:\nB(n) = MultiHead\n(\nS(n−1),S(n−1),S(n−1)\n)\n,(11)\nwhere S(0) = X. The second sub-layer is con-\ntext attention that integrates document-level con-\ntext into the encoder:\nD(n) = MultiHead\n(\nB(n),C(Nc),C(Nc)\n)\n. (12)\nThe third sub-layer is a position-wise fully con-\nnected feed-forward neural network:\nS(n) =\n[\nFNN(D(n)\n·,1 ); ... ; FNN(D(n)\n·,I )\n]\n, (13)\nwhere S(n) ∈ RD×I is the representation of\nthe source sentence x(k) at the n-th layer ( n =\n1,...,N s).\n2.3.2 Integration into the Decoder\nWhen generating the j-th target word y(k)\nj ,\nthe partial translation is denoted by y(k)\n<j =\ny(k)\n1 ,...,y (k)\nj−1. We follow Vaswani et al. (2017) to\noffset the target word embeddings by one position,\nresulting in the following matrix representation of\ny(k)\n<j :\nY = [y(k)\n0 ,..., y(k)\nj−1], (14)\nwhere y(k)\n0 ∈RD×1 is the vector representation of\na begin-of-sentence token and Y ∈RD×j is the\nconcatenation of all vectors.\nAs shown in Figure 1(b), we follow (Vaswani\net al., 2017) to use a stack ofNt identical layers to\ncompute target-side representations. Each layer is\ncomposed of four sub-layers. The ﬁrst sub-layer is\na multi-head self-attention:\nE(n) = MultiHead\n(\nT(n−1),T(n−1),T(n−1)\n)\n,(15)\nwhere T(0) = Y. The second sub-layer is con-\ntext attention that integrates document-level con-\ntext into the decoder:\nF(n) = MultiHead\n(\nE(n),C(Nc),C(Nc)\n)\n. (16)\nThe third sub-layer is encoder-decoder attention\nthat integrates the representation of the corre-\nsponding source sentence:\nG(n) = MultiHead\n(\nF(n),S(Ns),S(Ns)\n)\n. (17)\nThe fourth sub-layer is a position-wise fully con-\nnected feed-forward neural network:\nT(n) =\n[\nFNN(G(n)\n·,1 ); ... ; FNN(G(n)\n·,j ),\n]\n, (18)\nwhere T(n) ∈RD×j is the representation at the\nn-th layer (n= 1,...,N t). Note that T(0) = Y.\nFinally, the probability distribution of generat-\ning the next target word y(k)\nj is deﬁned using a\nsoftmax layer:\nP(y(k)\nj |X<k,x(k),y(k)\n<j ; θ) ∝exp(WoT(Nt)\n·,j ) (19)\nwhere Wo ∈R|Vy|×D is a model parameter, Vy\nis the target vocabulary, and T(Nt)\n·,j ∈RD×1 is a\ncolumn vector for predicting the j-th target word.\n2.3.3 Context Gating\nIn our model, we follow Vaswani et al. (2017) to\nuse residual connections (He et al., 2016) around\neach sub-layer to shortcut its input to its output:\nResidual(H) =H + SubLayer(H), (20)\nwhere H is the input of the sub-layer.\nWhile residual connections prove to be effective\nfor building deep architectures, there is one poten-\ntial problem for our model: the residual connec-\ntions after the context attention sub-layer might\nincrease the inﬂuence of document-level context\n537\nX<k in an uncontrolled way. This is undesirable\nbecause the source sentence x(k) usually plays a\nmore important role in target word generation.\nTo address this problem, we replace the residual\nconnections after the context attention sub-layer\nwith a position-wise context gating sub-layer:\nGating(H) =λH + (1−λ)SubLayer(H).(21)\nThe gating weight is given by\nλ= σ(WiH + WsSubLayer(H)), (22)\nwhere σ(·) is a sigmoid function, Wi and Ws are\nmodel parameters.\n2.4 Training\nGiven a document-level parallel corpus Dd, the\nstandard training objective is to maximize the log-\nlikelihood of the training data:\nˆθ= argmax\nθ\n{ ∑\n⟨X,Y⟩∈Dd\nlog P(Y|X; θ)\n}\n. (23)\nUnfortunately, large-scale document-level par-\nallel corpora are usually unavailable, even for\nresource-rich languages such as English and Chi-\nnese. Under small-data training conditions,\ndocument-level NMT is prone to underperform\nsentence-level NMT because of poor estimates of\nlow-frequency events.\nTo address this problem, we adopt the idea\nof freezing some parameters while tuning the re-\nmaining part of the model (Jean et al., 2015; Zoph\net al., 2016). We propose a two-step training strat-\negy that uses an additional sentence-level paral-\nlel corpus Ds, which can be larger than Dd. We\ndivide model parameters into two subsets: θ =\nθs ∪θd, where θs is a set of original sentence-\nlevel model parameters (highlighted in blue in\nFigure 1(b)) and θd is a set of newly-introduced\ndocument-level model parameters (highlighted in\nred in Figure 1(b)).\nIn the ﬁrst step, sentence-level parameters θs\nare estimated on the combined sentence-level par-\nallel corpus Ds ∪Dd: 2\nˆθs = argmax\nθs\n∑\n⟨x,y⟩∈Ds∪Dd\nlog P(y|x; θs). (24)\nNote that the newly introduced modules (high-\nlighted in red in Figure 1(b)) are inactivated in\n2It is easy to create a sentence-level parallel corpus from\nDd.\nthis step. P(y|x; θs) is identical to the original\nTransformer model, which is a special case of our\nmodel.\nIn the second step, document-level parameters\nθd are estimated on the document-level parallel\ncorpus Dd only:\nˆθd = argmax\nθd\n∑\n⟨X,Y⟩∈Dd\nlog P(Y|X; ˆθs,θd).(25)\nOur approach is also similar to pre-training\nwhich has been widely used in NMT (Shen et al.,\n2016; Tu et al., 2018). The major difference is that\nour approach keeps ˆθs ﬁxed when estimating θd\nto prevent the model from overﬁtting on the rela-\ntively smaller document-level parallel corpora.\n3 Experiments\n3.1 Setup\nWe evaluate our approach on Chinese-English\nand French-English translation tasks. In Chinese-\nEnglish translation task, the training set contains\n2M Chinese-English sentence pairs with 54.8M\nChinese words and 60.8M English words. 3 The\ndocument-level parallel corpus is a subset of the\nfull training set, including 41K documents with\n940K sentence pairs. On average, each document\nin the training set contains 22.9 sentences. We use\nthe NIST 2006 dataset as the development set and\nthe NIST 2002, 2003, 2004, 2005, 2008 datasets\nas test sets. The development and test sets contain\n588 documents with 5,833 sentences. On average,\neach document contains 9.9 sentences.\nIn French-English translation task, we use the\nIWSLT bilingual training data (Mauro et al., 2012)\nwhich contains 1,824 documents with 220K sen-\ntence pairs as training set. For development and\ntesting, we use the IWSLT 2010 development and\ntest sets, which contains 8 documents with 887\nsentence pairs and 11 documents with 1,664 sen-\ntence pairs respectively. The evaluation metric for\nboth tasks is case-insensitive BLEU score as cal-\nculated by the multi-bleu.perl script.\nIn preprocessing, we use byte pair encoding\n(Sennrich et al., 2016) with 32K merges to seg-\nment words into sub-word units for all languages.\nFor the original Transformer model and our ex-\ntended model, the hidden size is set to 512 and the\n3The training set consists of sentence-level parallel cor-\npora LDC2002E18, LDC2003E07, LDC2003E14, news\npart of LDC2004T08 and document-level parallel corpora\nLDC2002T01, LDC2004T07, LDC2005T06, LDC2005T10,\nLDC2009T02, LDC2009T15, LDC2010T03.\n538\n# sent. 1 2 3\nMT06 49.38 49.69 49.49\nTable 1: Effect of context length on translation quality.\nThe BLEU scores are calculated on the development\nset.\n# Layer MT06\n1 49.69\n2 49.38\n3 49.54\n4 49.59\n5 49.31\n6 49.43\nTable 2: Effect of self-attention layer number (i.e., Nc)\non translation quality. The BLEU scores are calculated\non the development set.\nﬁlter size is set to 2,048. The multi-head atten-\ntion has 8 individual attention heads. We set N =\nNs = Nt = 6. In training, we use Adam (Kingma\nand Ba, 2015) for optimization. Each mini-batch\ncontains approximately 24K words. We use the\nlearning rate decay policy described by Vaswani\net al. (2017). In decoding, the beam size is set\nto 4. We use the length penalty (Wu et al., 2016)\nand set the hyper-parameter αto 0.6. We use four\nTesla P40 GPUs for training and one Tesla P40\nGPU for decoding. We implement our approach\non top of the open-source toolkit THUMT (Zhang\net al., 2017). 4\n3.2 Effect of Context Length\nWe ﬁrst investigate the effect of context length\n(i.e., the number of preceding sentences) on our\napproach. As shown in Table 1, using two pre-\nceding source sentences as document-level context\nachieves the best translation performance on the\ndevelopment set. Using more preceding sentences\ndoes not bring any improvement and increases\ncomputational cost. This conﬁrms the ﬁnding of\nTu et al. (2018) that long-distance context only has\nlimited inﬂuence. Therefore, we set the number of\npreceding sentences to 2 in the following experi-\nments. 5\n3.3 Effect of Self-Attention Layer Number\nTable 2 shows the effect of self-attention\nlayer number for computing representations of\n4https://github.com/thumt/THUMT\n5If there is no preceding sentence, we simply use a single\nbegin-of-sentence token.\ndocument-level context (see Section 2.2) on trans-\nlation quality. Surprisingly, using only one self-\nattention layer sufﬁces to achieve good perfor-\nmance. Increasing the number of self-attention\nlayers does not lead to any improvements. There-\nfore, we set Nc to 1 for efﬁciency.\n3.4 Comparison with Previous Work\nIn Chinese-English translation task, we compare\nour approach with the following previous meth-\nods:\n1. (Wang et al., 2017): using a hierarchical\nRNN to integrate document-level context into\nthe RNNsearch model. They use a document-\nlevel parallel corpus containing 1M sentence\npairs. Table 3 gives the BLEU scores re-\nported in their paper.\n2. (Kuang et al., 2017): using a cache which\nstores previous translated words and topi-\ncal words to incorporate document-level con-\ntext into the RNNsearch model. They use\na document-level parallel corpus containing\n2.8M sentence pairs. Table 3 gives the BLEU\nscores reported in their paper.\n3. (Vaswani et al., 2017): the state-of-the-art\nNMT model that does not exploit document-\nlevel context. We use the open-source toolkit\nTHUMT (Zhang et al., 2017) to train and\nevaluate the model. The training dataset is\nour sentence-level parallel corpus containing\n2M sentence pairs.\n4. (Kuang et al., 2017)*: adapting the cache-\nbased method to the Transformer model. We\nimplement it on top of the open-source toolkit\nTHUMT. We also use the same training data\n(i.e., 2M sentence pairs) and the same two-\nstep training strategy to estimate sentence-\nand document-level parameters separately.\nAs shown in Table 3, using the same data, our\napproach achieves signiﬁcant improvements over\nthe original Transformer model (Vaswani et al.,\n2017) ( p < 0.01). The gain on the concate-\nnated test set (i.e., “All”) is 1.96 BLEU points. It\nalso outperforms the cache-based method (Kuang\net al., 2017) adapted for Transformer signiﬁcantly\n(p < 0.01), which also uses the two-step train-\ning strategy. Table 4 shows that our model also\noutperforms Transformer by 0.89 BLEU points on\nFrench-English translation task.\n539\nMethod Model MT06 MT02 MT03 MT04 MT05 MT08 All\n(Wang et al., 2017) RNNsearch 37.76 - - - 36.89 27.57 -\n(Kuang et al., 2017) RNNsearch - 34.41 - 38.40 32.90 31.86 -\n(Vaswani et al., 2017) Transformer 48.09 48.63 47.54 47.79 48.34 38.31 45.97\n(Kuang et al., 2017)* Transformer 48.14 48.97 48.05 47.91 48.53 38.38 46.37\nthis work Transformer 49.69 50.96 50.21 49.73 49.46 39.69 47.93\nTable 3: Comparison with previous works on Chinese-English translation task. The evaluation metric is case-\ninsensitive BLEU score. (Wang et al., 2017) use a hierarchical RNN to incorporate document-level context into\nRNNsearch. (Kuang et al., 2017) use a cache to exploit document-level context for RNNsearch. (Kuang et al.,\n2017)* is an adapted version of the cache-based method for Transformer. Note that “MT06” is not included in\n“All”.\nMethod Dev Test\nTransformer 29.42 35.15\nthis work 30.40 36.04\nTable 4: Comparison with Transformer on French-\nEnglish translation task. The evaluation metric is case-\ninsensitive BLEU score.\n> = <\nHuman 1 24% 45% 31%\nHuman 2 20% 55% 25%\nHuman 3 12% 52% 36%\nOverall 19% 51% 31%\nTable 5: Subjective evaluation of the comparison be-\ntween the original Transformer model and our model.\n“>” means that Transformer is better than our model,\n“=” means equal, and “<” means worse.\n3.5 Subjective Evaluation\nWe also conducted a subjective evaluation to vali-\ndate the beneﬁt of exploiting document-level con-\ntext. All three human evaluators were asked to\ncompare the outputs of the original Transformer\nmodel and our model of 20 documents contain-\ning 198 sentences, which were randomly sampled\nfrom the test sets.\nTable 5 shows the results of subjective evalu-\nation. Three human evaluators generally made\nconsistent judgements. On average, around 19%\nof Transformer’s translations are better than that\nof our model, 51% are equal, and 31% are\nworse. This evaluation conﬁrms that exploiting\ndocument-level context helps to improve transla-\ntion quality.\n3.6 Evaluation of Efﬁciency\nWe evaluated the efﬁciency of our approach. It\ntakes the original Transformer model about 6.7\nMethod Training Decoding\nTransformer 41K 872\nthis work 31K 364\nTable 6: Evaluation of training and decoding speed.\nThe speed is measured in terms of word/second (wps).\nhours to converge during training and the training\nspeed is 41K words/second. The decoding speed is\n872 words/second. In contrast, it takes our model\nabout 7.8 hours to converge in the second step of\ntraining. The training speed is 31K words/second.\nThe decoding speed is 364 words/second.\nTherefore, the training speed is only reduced by\n25% thanks to the high parallelism of multi-head\nattention used to incorporate document-level con-\ntext. The gap is larger in decoding because target\nwords are generated in an autoregressive way in\nTransformer.\n3.7 Effect of Two-Step Training\nTable 7 shows the effect of the proposed two-\nstep training strategy. The ﬁrst two rows only use\nsentence-level parallel corpus to train the origi-\nnal Transformer model (see Eq. 24) and achieve\nBLEU scores of 39.53 and 45.97. The third row\nonly uses the document-level parallel corpus to di-\nrectly train our model (see Eq. 23) and achieves\na BLEU score of 36.52. The fourth and ﬁfth rows\nuse the two-step strategy to take advantage of both\nsentence- and document-level parallel corpora and\nachieve BLEU scores of 40.22 and 47.93, respec-\ntively.\nWe ﬁnd that document-level NMT achieves\nmuch worse results than sentence-level NMT (i.e.,\n36.52 vs. 39.53) when only small-scale document-\nlevel parallel corpora are available. Our two-step\ntraining method is capable of addressing this prob-\nlem by exploiting sentence-level corpora, which\n540\nsent. doc. MT06 MT02 MT03 MT04 MT05 MT08 All\n940K - 36.20 42.41 43.12 41.02 40.93 31.49 39.53\n2M - 48.09 48.63 47.54 47.79 48.34 38.31 45.97\n- 940K 34.00 38.83 40.51 38.30 36.69 29.38 36.52\n940K 940K 37.12 43.29 43.70 41.42 41.84 32.36 40.22\n2M 940K 49.69 50.96 50.21 49.73 49.46 39.69 47.93\nTable 7: Effect of two-step training. “sent.” denotes sentence-level parallel corpus and “doc.” denotes document-\nlevel parallel corpus.\nIntegration MT06 MT02 MT03 MT04 MT05 MT08 All\nnone 48.09 48.63 47.54 47.79 48.34 38.31 45.97\nencoder 48.88 50.30 49.34 48.81 49.75 39.55 47.51\ndecoder 49.10 50.31 49.83 49.35 49.29 39.07 47.48\nboth 49.69 50.96 50.21 49.73 49.46 39.69 47.93\nTable 8: Effect of context integration. “none” means that no document-level context is integrated, “encoder”\nmeans that the document-level context is integrated only into the encoder, “decoder” means that the document-\nlevel context is integrated only into the decoder, and “both” means that the context is integrated into both the\nencoder and the decoder.\nGating MT06 MT02 MT03 MT04 MT05 MT08 All\nw/o 49.33 50.56 49.74 49.29 50.11 39.02 47.55\nw/ 49.69 50.96 50.21 49.73 49.46 39.69 47.93\nTable 9: Effect of context gating.\nleads to signiﬁcant improvements across all test\nsets.\n3.8 Effect of Context Integration\nTable 8 shows the effect of integrating document-\nlevel context to the encoder and decoder (see\nSection 2.3). It is clear that integrating\ndocument-level context into the encoder (Eq. 12)\nbrings signiﬁcant improvements (i.e., 45.97 vs.\n47.51). Similarly, it is also beneﬁcial to inte-\ngrate document-level context into the decoder (Eq.\n16). Combining both leads to further improve-\nments. This observation suggests that document-\nlevel context does help to improve Transformer.\n3.9 Effect of Context Gating\nAs shown in Table 9, we also validated the effec-\ntiveness of context gating (see Section 2.3.3). We\nﬁnd that replacing residual connections with con-\ntext gating leads to an overall improvement of 0.38\nBLEU point.\n3.10 Analysis\nWe use an example to illustrate how document-\nlevel context helps translation (Table 10). In\norder to translate the source sentence, NMT\nhas to disambiguate the multi-sense word “ yun-\ndong”, which is actually impossible without the\ndocument-level context. The exact meaning of\n“rezhong” is also highly context dependent. For-\ntunately, the sense of “ yundong” can be in-\nferred from the word “ saiche” (car racing) in\nthe document-level context and “ rezhong” is the\nantonym of “ yanjuan” (tired of). This example\nshows that our model learns to resolve word sense\nambiguity and lexical cohesion problems by inte-\ngrating document-level context.\n4 Related Work\nDeveloping document-level models for machine\ntranslation has been an important research direc-\ntion, both for conventional SMT (Gong et al.,\n2011; Hardmeier et al., 2012; Xiong et al.,\n2013a,b; Garcia et al., 2014) and NMT (Jean et al.,\n2017; Kuang et al., 2017; Tiedemann and Scher-\nrer, 2017; Wang et al., 2017; Maruf and Haffari,\n2018; Bawden et al., 2018; Tu et al., 2018; V oita\net al., 2018).\nMost existing work on document-level NMT\nhas focused on integrating document-level con-\ntext into the RNNsearch model (Bahdanau et al.,\n541\nContext ···ziji ye yinwei queshao jingzheng duishou er dui saiche youxie yanjuan\nshi···\nSource wo rengran feichang rezhong yu zhexiang yundong.\nReference I’m still veryfond of the sport.\nTransformer I am still very enthusiastic about this movement.\nOur work I am still very keen on this sport.\nTable 10: An example of Chinese-English translation. In the source sentence, “yundong” (sport or political move-\nment) is a multi-sense word and “ rezhong” (fond of) is an emotional word whose meaning is dependent on its\ncontext. Our model takes advantage of the words “ saiche” (car racing) and “yanjuan” (tired of) in the document-\nlevel context to translate the source words correctly.\n2015). These approaches can be roughly divided\ninto two broad categories: computing the repre-\nsentation of the full document-level context (Jean\net al., 2017; Tiedemann and Scherrer, 2017; Wang\net al., 2017; Maruf and Haffari, 2018; V oita et al.,\n2018) and using a cache to memorize most rel-\nevant information in the document-level context\n(Kuang et al., 2017; Tu et al., 2018). Our approach\nfalls into the ﬁrst category. We use multi-head at-\ntention to represent and integrate document-level\ncontext.\nV oita et al. (2018) also extended Transformer to\nmodel document-level context, but our work is dif-\nferent in modeling and training strategies. The ex-\nperimental part is also different. While V oita et al.\n(2018) focus on anaphora resolution, our model is\nable to improve the overall translation quality by\nintegrating document-level context.\n5 Conclusion\nWe have presented a method for exploiting\ndocument-level context inside the state-of-the-art\nneural translation model Transformer. Exper-\niments on Chinese-English and French-English\ntranslation tasks show that our method is able to\nimprove over Transformer signiﬁcantly. In the fu-\nture, we plan to further validate the effectiveness\nof our approach on more language pairs.\nAcknowledgments\nYang Liu is supported by the National Natural\nScience Foundation of China (No. 61432013),\nNational Key R&D Program of China (No.\n2017YFB0202204), National Natural Science\nFoundation of China (No. 61761166008), Ad-\nvanced Innovation Center for Language Resources\n(TYR17002), and the NExT++ project supported\nby the National Research Foundation, Prime Min-\nisters Ofﬁce, Singapore under its IRC@Singapore\nFunding Initiative. This research is also supported\nby Sogou Inc.\nReferences\nDzmitry Bahdanau, KyungHyun Cho, and Yoshua\nBengio. 2015. Sequence to sequence learning with\nneural networks. In Proceedings of ICLR.\nRachel Bawden, Rico Sennrich, Alexandra Birch, and\nBarry Haddow. 2018. Evaluating discourse phe-\nnomena in neural machine translation. In Proceed-\nings of NAACL.\nEva Mart´ınez Garcia, Cristina Esp˜ana Bonet, and Llu´ız\nM`arquez. 2014. Document-level machine transla-\ntion with word vector models. In Proceedings of\nEACL.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N Dauphin. 2017. Convo-\nlutional sequence to sequence learning. CoRR,\nabs/1705.03122.\nZhengxian Gong, Min Zhang, and Guodong Zhou.\n2011. Cache-based document-level statistical ma-\nchine translation. In Proceedings of EMNLP.\nChristian Hardmeier, Joakim Nivre, and J ¨org Tiede-\nmann. 2012. Document-wide decoding for phrase-\nbased statistical machine translation. In Proceed-\nings of EMNLP.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of CVPR.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On using very large tar-\nget vocabulary for neural machine translation. In\nProceedings of ACL.\nSebastien Jean, Stanislas Lauly, Orhan Firat, and\nKyunghyun Cho. 2017. Does neural machine\ntranslation beneﬁt from larger context? CoRR,\nabs/1704.05135.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization.\n542\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder.\nShaohui Kuang, Deyi Xiong, Weihua Luo, and\nGuodong Zhou. 2017. Cache-based document-level\nneural machine translation. CoRR, abs/1711.11221.\nSameen Maruf and Gholamreza Haffari. 2018. Docu-\nment context neural machine translation with mem-\nory networks. In Proceedings of ACL.\nCettolo Mauro, Girardi Christian, and Federico Mar-\ncello. 2012. Wit3: Web inventory of transcribed and\ntranslated talks. In Proceedings of EAMT.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of ACL.\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua\nWu, Maosong Sun, and Yang Liu. 2016. Minimum\nrisk training for neural machine translation. In Pro-\nceedings of ACL.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Proceedings of NIPS.\nJ¨org Tiedemann and Yves Scherrer. 2017. Neural ma-\nchine translation with extended context. In Proceed-\nings of the Third Workshop on Discourse in Machine\nTranslation.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong\nZhang. 2018. Learning to remember translation his-\ntory with a continuous cache. Transactions of the\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NIPS.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine transla-\ntion learns anaphora resolution. In Proceedings of\nACL.\nLongyue Wang, Zhaopeng Tu, Andy Way, and Liu\nQun. 2017. Exploiting cross-sentence context for\nneural machine translation. In Proceedings of\nEMNLP.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nDeyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,\nand Qun Liu. 2013a. Modeling lexical cohesion for\ndocument-level machine translation. In Proceedings\nof IJCAI.\nDeyi Xiong, Yang Ding, Min Zhang, and Chew Lim\nTan. 2013b. Lexical chain based cohesion models\nfor document-level statistical machine translation.\nIn Proceedings of EMNLP.\nJiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yong\nCheng, Maosong Sun, Huanbo Luan, and Yang Liu.\n2017. Thumt: An open source toolkit for neural ma-\nchine translation. arXiv preprint arXiv:1706.06415.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of\nEMNLP."
}