{
  "title": "Xai-driven knowledge distillation of large language models for efficient deployment on low-resource devices",
  "url": "https://openalex.org/W4396644396",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3009717353",
      "name": "Riccardo Cantini",
      "affiliations": [
        "University of Calabria"
      ]
    },
    {
      "id": "https://openalex.org/A3157391292",
      "name": "Alessio Orsino",
      "affiliations": [
        "University of Calabria"
      ]
    },
    {
      "id": "https://openalex.org/A1976489361",
      "name": "Domenico Talia",
      "affiliations": [
        "University of Calabria"
      ]
    },
    {
      "id": "https://openalex.org/A3009717353",
      "name": "Riccardo Cantini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3157391292",
      "name": "Alessio Orsino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1976489361",
      "name": "Domenico Talia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4387430343",
    "https://openalex.org/W4312551388",
    "https://openalex.org/W4206077189",
    "https://openalex.org/W3161810785",
    "https://openalex.org/W4225690860",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W3036982689",
    "https://openalex.org/W2987861506",
    "https://openalex.org/W3187295906",
    "https://openalex.org/W6628082049",
    "https://openalex.org/W6600042225",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W6600263792",
    "https://openalex.org/W2979691890",
    "https://openalex.org/W2996061341",
    "https://openalex.org/W4366262984",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W4220904955",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2962843949",
    "https://openalex.org/W4377825911",
    "https://openalex.org/W3138154797"
  ],
  "abstract": "Abstract Large Language Models (LLMs) are characterized by their inherent memory inefficiency and compute-intensive nature, making them impractical to run on low-resource devices and hindering their applicability in edge AI contexts. To address this issue, Knowledge Distillation approaches have been adopted to transfer knowledge from a complex model, referred to as the teacher, to a more compact, computationally efficient one, known as the student. The aim is to retain the performance of the original model while substantially reducing computational requirements. However, traditional knowledge distillation methods may struggle to effectively transfer crucial explainable knowledge from an LLM teacher to the student, potentially leading to explanation inconsistencies and decreased performance. This paper presents DiXtill , a method based on a novel approach to distilling knowledge from LLMs into lightweight neural architectures. The main idea is to leverage local explanations provided by an eXplainable Artificial Intelligence (XAI) method to guide the cross-architecture distillation of a teacher LLM into a self-explainable student, specifically a bi-directional LSTM network.Experimental results show that our XAI-driven distillation method allows the teacher explanations to be effectively transferred to the student, resulting in better agreement compared to classical distillation methods,thus enhancing the student interpretability. Furthermore, it enables the student to achieve comparable performance to the teacher LLM while also delivering a significantly higher compression ratio and speedup compared to other techniques such as post-training quantization and pruning, which paves the way for more efficient and sustainable edge AI applications",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8343747854232788
    },
    {
      "name": "Distillation",
      "score": 0.7428467273712158
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5996376276016235
    },
    {
      "name": "Artificial intelligence",
      "score": 0.562996506690979
    },
    {
      "name": "Speedup",
      "score": 0.4857123792171478
    },
    {
      "name": "Artificial neural network",
      "score": 0.4673462510108948
    },
    {
      "name": "Machine learning",
      "score": 0.46656787395477295
    },
    {
      "name": "Interpretability",
      "score": 0.4385421574115753
    },
    {
      "name": "Knowledge transfer",
      "score": 0.4317170977592468
    },
    {
      "name": "Knowledge management",
      "score": 0.2426244020462036
    },
    {
      "name": "Parallel computing",
      "score": 0.10331887006759644
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}