{
    "title": "Fast Autofocusing using Tiny Transformer Networks for Digital\\n Holographic Microscopy",
    "url": "https://openalex.org/W4281479341",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221905945",
            "name": "Cuenat, Stéphane",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221905946",
            "name": "Andreoli, Louis",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221905947",
            "name": "André, Antoine N.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2751010808",
            "name": "Sandoz Patrick",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4207403035",
            "name": "Laurent, Guillaume J.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3129837818",
            "name": "Couturier, Raphaël",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221905951",
            "name": "Jacquot, Maxime",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2612688942",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W4282935825",
        "https://openalex.org/W4239510810",
        "https://openalex.org/W2038128936",
        "https://openalex.org/W1500435738",
        "https://openalex.org/W2154393313",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4287022992",
        "https://openalex.org/W1551733050",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3106459670",
        "https://openalex.org/W1983348509",
        "https://openalex.org/W2999675216",
        "https://openalex.org/W2798701005",
        "https://openalex.org/W2110290819",
        "https://openalex.org/W1983568931",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2904061522",
        "https://openalex.org/W4301176655",
        "https://openalex.org/W3198236655",
        "https://openalex.org/W2884363302",
        "https://openalex.org/W2808917486",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2908872770",
        "https://openalex.org/W2966299514",
        "https://openalex.org/W2948499658",
        "https://openalex.org/W3099186728",
        "https://openalex.org/W2159373363",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4205184689",
        "https://openalex.org/W2593966629",
        "https://openalex.org/W1969287161",
        "https://openalex.org/W2519131347",
        "https://openalex.org/W2164601752",
        "https://openalex.org/W3162353407",
        "https://openalex.org/W2043672528",
        "https://openalex.org/W2105622061",
        "https://openalex.org/W1980101530",
        "https://openalex.org/W2079162471",
        "https://openalex.org/W4297575730",
        "https://openalex.org/W2171163963",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2792179230",
        "https://openalex.org/W2587524409",
        "https://openalex.org/W2910625855",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2790594255",
        "https://openalex.org/W2027640237",
        "https://openalex.org/W2576135770"
    ],
    "abstract": "The numerical wavefront backpropagation principle of digital holography\\nconfers unique extended focus capabilities, without mechanical displacements\\nalong z-axis. However, the determination of the correct focusing distance is a\\nnon-trivial and time consuming issue. A deep learning (DL) solution is proposed\\nto cast the autofocusing as a regression problem and tested over both\\nexperimental and simulated holograms. Single wavelength digital holograms were\\nrecorded by a Digital Holographic Microscope (DHM) with a 10$\\\\mathrm{x}$\\nmicroscope objective from a patterned target moving in 3D over an axial range\\nof 92 $\\\\mu$m. Tiny DL models are proposed and compared such as a tiny Vision\\nTransformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The\\nproposed tiny networks are compared with their original versions (ViT/B16,\\nVGG16 and Swin-Transformer Tiny) and the main neural networks used in digital\\nholography such as LeNet and AlexNet. The experiments show that the predicted\\nfocusing distance $Z_R^{\\\\mathrm{Pred}}$ is accurately inferred with an accuracy\\nof 1.2 $\\\\mu$m in average in comparison with the DHM depth of field of 15\\n$\\\\mu$m. Numerical simulations show that all tiny models give the\\n$Z_R^{\\\\mathrm{Pred}}$ with an error below 0.3 $\\\\mu$m. Such a prospect would\\nsignificantly improve the current capabilities of computer vision position\\nsensing in applications such as 3D microscopy for life sciences or\\nmicro-robotics. Moreover, all models reach an inference time on CPU, inferior\\nto 25 ms per inference. In terms of occlusions, TViT based on its Transformer\\narchitecture is the most robust.\\n",
    "full_text": "FAST AUTOFOCUSING USING TINY TRANSFORMER NETWORKS\nFOR DIGITAL HOLOGRAPHIC MICROSCOPY\nStéphane Cuenat, Louis Andréoli, Antoine N.André, Patrick Sandoz,\nGuillaume J. Laurent, Raphaël Couturier and Maxime Jacquot,\nInstitut FEMTO-ST,\nCNRS & Université Bourgogne Franche-Comté\nFrance\nstephane.cuenat@univ-fcomte.fr\nABSTRACT\nThe numerical wavefront backpropagation principle of digital holography confers unique extended\nfocus capabilities, without mechanical displacements along z-axis. However, the determination of the\ncorrect focusing distance is a non-trivial and time consuming issue. A deep learning (DL) solution\nis proposed to cast the autofocusing as a regression problem and tested over both experimental and\nsimulated holograms. Single wavelength digital holograms were recorded by a Digital Holographic\nMicroscope (DHM) with a 10x microscope objective from a patterned target moving in 3D over an\naxial range of 92 µm. Tiny DL models are proposed and compared such as a tiny Vision Transformer\n(TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The proposed tiny networks are\ncompared with their original versions (ViT/B16, VGG16 and Swin-Transformer Tiny) and the main\nneural networks used in digital holography such as LeNet and AlexNet. The experiments show that\nthe predicted focusing distance ZPred\nR is accurately inferred with an accuracy of 1.2 µm in average\nin comparison with the DHM depth of ﬁeld of 15 µm. Numerical simulations show that all tiny\nmodels give the ZPred\nR with an error below 0.3 µm. Such a prospect would signiﬁcantly improve the\ncurrent capabilities of computer vision position sensing in applications such as 3D microscopy for life\nsciences or micro-robotics. Moreover, all models reach an inference time on CPU, inferior to 25 ms\nper inference. In terms of occlusions, TViT based on its Transformer architecture is the most robust.\nKeywords ViT ·CNN ·Tiny Networks ·Digital Holographic Microscopy\n1 Introduction\nOne major drawback when 3D moving samples are studied in microscopy is the balance between the focal range that\nlimits out-of-plane measurements and the requirement of a high axial resolution, i.e. a short depth of ﬁeld (DoF)\n(see for example [1, 2]). Several solutions have been proposed such as depth-from-focus imaging [ 3] and confocal\nmicroscopy [4] to reconstruct a topography of the scene. Scanning electron microscopy [5] can also be used to get large\nin-focus depths. In any case, all these methods require a scanning of the scene that slows down the image acquisition\nrate. Moreover, the working distances of these devices are very short and this reduces considerably the interest of a\ncontactless measurement.\nCoherent imaging approaches such as Digital Holography (DH) can be used instead of conventional microscopy to\naddress the focusing issues [6, 7]. DH offers a means for recording the phase and amplitude of a propagating wavefront\non a solid-state image sensor [8]. Then, by numerically propagating the recorded wavefront backward or forward at\nparticular distances of interest, different characteristics can be extracted, typically three-dimensional surfaces, but also\npolarization states and intensity distributions. Several recording and processing schemes have been developed to assess\ndiverse optical characteristics that make DH a highly powerful coherent imaging method for metrological applications\n[9, 10]. Signiﬁcant progress, potential impact and challenging issues in the ﬁeld of DH can be found in this recent\nroadmap article [11].\narXiv:2203.07772v4  [eess.IV]  20 May 2022\nThe targeted application aims to address the 3D position measurement needs encountered in small-scale\nmechatronics [12, 13]. At the microscale, automation involves centimeter-sized actuators necessary to perform diverse\ntasks with a high accuracy, down to the nanometer range. Contactless sensors are thus desired to control 3D motions\nwith a high accuracy over large ranges [12]. Combined with optical microscopy, computer vision constitutes an efﬁcient\nmeans to detect in-plane position and displacements. However, microscope objective (MO) lenses provide short\nin-focus depths and inherently rely on mechanical displacements along the optical axis. One way to extend computer\nvision capabilities to 3D microscopy is to harness the wave character of light by means of DH. DH is particularly\nsuited to this aim because it requires a single hologram to reconstruct a 3D scene and because digital back-propagation\ncomputations allows image reconstruction in a extended in-focus depths [14]. A key-point in DH is to note that, instead\nof an image of the object, it is the propagating wavefront incident on the image sensor that is recorded. The distance of\nthe object does not impact the recording quality, it only changes the actually recorded wavefront in accordance with\nscalar diffraction theory. Therefore, blur does not exist at the recording stage of DH since it does not seek for any\nin-focus image. The object distance stands for a computation parameter that is numerically tunable over an extended\nrange, but limited to coherent length of the light source used. This speciﬁcity makes the range of working distances\nallowed by DH incomparably larger than that allowed by conventional incoherent imaging methods. DH can be applied\nto micro-objects in microscopy with a Digital Holographic Microscope (DHM) setup. The reconstruction distance\nleading to the best-focused image has to be determined among the z-range explored by the object. There are various\ntechniques for deﬁning image-formation sharpness-criteria that apply to DH [15, 16].\nRecently, many studies have proposed to study the capabilities of deep-learning Convolutional Neural Network (CNN)\nto determine in DH various unknown parameters such as focusing distance, or the phase recovery [17, 18, 19, 20, 21, 22].\nThese works have to be considered in the wider context of imaging techniques where Deep Learning (DL) approaches\nare applied to solve complex problems found in computer vision as well as in microscopy [ 23, 24, 25]. A recent\nwork [17] even demonstrated that Deep CNN gives better results in terms of prediction of propagation distance in\nDH without knowing all the setup’s physical parameters, than other learning-based algorithms such as Multi Layer\nPerceptron (MLP) [26], support vector machine [27], and k-nearest neighbor [28]. The hardware implementation of\nartiﬁcial neural networks has constituted a real challenge for many years [29, 30, 31], but the tasks that can be solved\nby such systems are limited to standard tests of classiﬁcation and prediction, and they are still limited in terms of\nscalability for mega-pixel image processing.\nThis paper aims to illustrate a new high-proﬁle application of machine learning by elevating DHM and autofocusing to\na new level. Whereas many studies focus on life science microscopy [18, 32, 22], this work explores extended visual\ncapabilities offered by combining DH and last generation of DL algorithms such as Vision Transformer (ViT)[33] and\nSwin-Transformer (SwinT)[34] networks for applications in micro-robotics [12, 13] or in real-time 3D microscopy\n[32]. This work introduces for the ﬁrst time the neural network Transformer architectures applied to advanced coherent\nimaging ﬁeld, such as digital holography. This is signiﬁcant because these new generations of algorithms have already\nrevolutionized the Natural Language Processing (NLP) and recent versions ViT [33] and SwinT[35] highly perform\nfor image recognition thanks to their self-attention feature[ 36]. More speciﬁcally, our work deals with these new\ngeneration of deep learning approaches for autofocusing in digital holographic microscopy to obtain in-focus depth\nprediction with high accuracy. We developed new tiny ViT and tiny SwinT network architectures, and compared them\nwith typical Convolutional Neural Network (CNN) ones used in optics and digital holography such as AlexNet[21],\nVGG[37] and LeNet[38]. Swin-Transformers propose a hierarchical Transformer whose representation is computed\nwith Shifted windows. The shifted windowing scheme brings greater efﬁciency by limiting self-attention computation\nto non-overlapping local windows while also allowing cross-window connection. This hierarchical architecture has\nthe ﬂexibility to model at various scales and has linear computational complexity with respect to the size of images.\nTaking into account the demand for real time application and achievable training with a reasonable amount of data,\ntiny networks are developed. These ﬁrst results pave the way to overcome in-focus depth limit[18] with a short DoF in\nDHM without any MO lens mechanical displacements[32].\n2 New trends in deep learning for image processing: ViT and SwinT\nSince the inception of DL neural networks, CNN occupies the ﬁeld with architectures like VGG-16 [37], Densenet [39]\nor EfﬁcientNet [40]. At the core of a CNN, there is a series of mixed convolution and pooling layers which extract a set\nof features from an input image. One of the main advantages of a CNN compared to MLP Network (ﬁrst network\nproposed), is that they are translation invariant and less demanding in resource when it comes to large inputs. Later the\nViT architecture was introduced. This architecture is based on the concept of attention [36]. The attention mechanism\nwas born to help memorize long source sentences in NLP. Rather than building a single context vector out of the\nencoder’s last hidden state, the attention creates shortcuts between the context vector and the entire source input. The\nweights of these shortcut connections are customizable for each output element.\n2\n(a)\nInput ROI Extra learnable class\nTransformer\nEncoder 1\nTransformer\nEncoder k\nTransformer\nEncoder 2\nViT Transformer Layer\nSwinT Transformer Layer\nStage 1 Stage 2 Stage 3 Stage 4\nSwin\nTransformer\nBlock x2\nSwin\nTransformer\nBlock x ms\nPatch\nmerging\n(b1)\n(b2\n(c1)\n)\nTransformer\nEncoder\nNorm\nMSA\nNorm\nMLP\nNorm\nW-MSA\nNorm\nMLP\nNorm\nSW-MSA\nNorm\nMLP\nTwo Successive Swin\nTransformer Blocks\n(c2)\nLinear projection ofﬂatten patches\nTransformer Layer\nMLP\nLinear regression\nDistance ZR\nPred\nPatch & position\nembedding\nFigure 1: Schematic illustration of the ViT [33] and SwinT [34] architectures. (a) gives the general architecture of a ViT\nor a SwinT. The region of interest (ROI) is divided into patches which are linearly projected on an embedded dimension\nfollowed by a transformer layer, MLP and the linear regression. (b1) shows the successive layers for a ViT respectively\nk Transformer Encoders. (b2) gives a view of the SwinT Transfomer layer architecture which is formed by a series of 4\nStages. Each Stage encapsulates a Swin-Transformer Block which is repeated ms times (with srepresenting the stage\nnumber) and patch merging layers (for the stages 2 to 4). Through the stages, the multi-head attention is computed\ntaking different sizes of patch size and shifted windows. (c1) and (c2) represent the internal steps inside a Transformer\nEncoder and the two successive Swin-Transformer Block, where the Multi-Self Attention (MSA) is computed (MSA,\nW-MSA and SW-MSA). SwinT ﬁrst computes a window multi-head attention (W-MSA) and then a shifted windows\nmulti-head attention (SW-MSA).\nViT brings this concept to computer vision [ 33]. ViT is a pure Transformer architecture built from Trans-\nformer encoder layers to approach a classiﬁcation or regression problem. ViT splits an input image in a series of\npatches which would be treated as word tokens by a Transformer Network. SwinT even surpasses the performance of a\npure ViT network as shown in [35]. SwinT extends a ViT network by varying the patch dimension and computes the\nattention only for a given window (shifted-window over the space of the input image). Such a network is able to better\nassess the local and global information inside the input image.\nFigure 1 shows the architecture of a ViT [33] and SwinT [34] network. Panel in Fig. 1(a) gives the global architecture\nof a ViT or SwinT network and in particular how the Region Of Interest (ROI) is processed. This input image is split\ninto different patches and projected on an embedded dimension through the linear projection of the ﬂatten patches\n(tokens). An additional position embedding and class token are added. The class token is the only token used to apply\na regression. Each embedded patch is processed by the transformer layer which outputs the associated class token\nafter the MLP block. The regression layer projects the class token to a scalar, the reconstruction distance Z in our case.\n3\nLinear \nembedding \nWi\nQ\nLinear \nembedding \nWi\nV\nLinear \nembedding \nWi\nK\nki vi qi\nhi\nxin\nMSA\nfunction\nConcatanation\nhconcat\nxout\nLinear \nembedding \nW0\nScaled dot-product\nattention\nFigure 2: Multi-Headed Self-Attention implemented as a scaled dot-product attention. MSA, W-MSA and SW-MSA\nblocks on Fig. 1.\nPanels in Fig. 1 ( b1) and (b2) describe in more details the Transformer Layer (in gray) for ViT and SwinT models,\nrespectively. For ViT, there is a total of k Transfomer Encoder layers. In the SwinT architecture, the Transformer Layer\nis composed of a series of sstage layers with typically s= 4. The ﬁrst stage layer contains a two Swin-Transformer\nblock. The s−1 other stages encapsulate a patch merging and ms Swin-Transformer Blocks, where ms can change\nthrough the stages s(typically ms ∈[1,4]). The window size is set to a ﬁx value (default: 7x7 patches). Moreover, the\npatch size of each stage is increased by a factor 2 through the patch merging layers [34]. This creates a hierarchy in\ncomparison to a ViT which always considers the same patch size and a global window [33]. The Transformer Encoder\n(in green) of the ViT and the Swin Transformer block (in yellow) are explained in more details in panels of Fig. 1(c1)\nand (c2), respectively. The input of the transformer encoder is ﬁrst normalized. The Multi-head Self-Attention (MSA)\nis ﬁrst computed and then followed by a normalization and a MLP block. In the case of a SwinT, the architecture is\nsimilar but with two successive Swin-Transformer blocks where the MSA is ﬁrst a Window Multi-head Self-Attention\n(W-MSA), then a Shifted Window Multi-head Self-Attention (SW-MSA). While the MSA (ViT case) is computed on\nthe complete set of patches, the W-MSA (SW-MSA) uses a dedicated (shifted) window (SwinT case). SW-MSA allows\ninter-window interactions.\n2.1 Multi-head self-attention\nAs schematically illustrated in Fig. 2, the MSA function is approached from a general perspective where the vector xin\nis its input and xout its output. For each i∈[1,N], the head hi is implemented as a scaled dot-product attention. The N\nheads h is called Multi-head Self-Attention. In case of ViT, N is ﬁxed for all the Transformer Encoder layers. SwinT\ndeﬁnes a N for each Stage. A key ki, value vi and query qi dimensional vectors are computed for each head hi by\nprojecting the input xin using three learnable matrices (WK\ni , WV\ni , WQ\ni ):\nki = WK\ni xin, (1)\nvi = WV\ni xin, (2)\nqi = WQ\ni xin. (3)\nFor each head hi, the attention is computed by taking the key, value and query vectors.\nhi = Attention(ki,vi,qi). (4)\nThe attention is calculated by ﬁrst applying a Softmax [41] used to normalize the dot product between a vector of keys\nki and a vector of queries qi. Subsequently, this output acts as weights for the value vector vi, hence\nAttention(ki,vi,qi) = Softmax(qikT\ni√\nD\n)vi, (5)\n4\nwhere Dis the dimension of the key and query vectors (ki and qi). All the heads hi are concatenated as\nhconcat = Concat(h1,h2,..., hN). (6)\nThe output vector xout is obtained by the vector product of hconcat and a learnable matrix W0 as\nxout = W0 hconcat. (7)\nMulti-head attention is used since it allows the network to attend to different learned representations at different regions\nof input ROI as described by Fig. 2 and expressed as\nxout = MSA(xin). (8)\n2.2 Tiny networks: TViT, TSwinT & TVGG\nIn this paper, tiny versions of the original ViT (TViT), Swin-Transformer (TSwinT) and VGG16 (TVGG) are proposed.\nTiny networks allow to reduce the number of parameters without impacting much the accuracy of the models. Moreover,\ntiny models need less computation power and approach real-time processing. Figure 1 gives an overview of a ViT\nand SwinT architectures. TViT modiﬁes a ViT as follows: is built with a total of 12 Transformer Encoder, 8 heads\nand a patch size of 16x16. The hidden-size of the Transformer encoder has been reduced from 768 (for a ViT/B16)\nto 128 (TViT) hidden neurons. The MLP dimension (Transformer Encoder) has been reduced to 1024 instead of\n3072 hidden neurons (for a ViT/B16). TSwinT is a revisit of the SwinT architecture where several changes have been\napplied: the size of the embedding vector is set to 32, the number of Swin-Transformer blocks has been set as follows\nfor each Stage: m1 = 2, m2 = 2, m3 = 4, m4 = 2. The number of heads for each Stage has also been modiﬁed:\nN1 = 2,N2 = 4,N3 = 8,N4 = 8. The window size has been ﬁxed to 4 with an initial patch size of 4x4. TViT and\nTSwinT contrasts with canonical ViT architectures as these models are usually able to learn high-quality intermediate\nrepresentations with large amounts of data as described in [ 22, 42]. TVGG is introduced to reduce the number of\nparameters of the original VGG16 [37] architecture for comparison purposes. All ﬁlters of each 2D convolution layer\nhave been divided by 2 inside a TVGG architecture. These changes limit the width of the layers of the tiny networks\nby keeping their capacity to learn. The number of parameters for each model has drastically diminished as shown\nin table 1, by a factor between 5 and 20. Moreover, all models are trained from scratch only using experimental or\nsimulated digital holograms of different patterns (pseudo-periodic pattern and USAF pattern) without any transfer\nlearning from a pre-trained model on a dataset like ImageNet [43]. The tiny models take as input ROI of 128x128 of a\nsingle wavelength digital hologram.\nTiny model Original model (pre-trained)\nModel # parameters Model # parameters Ref\nTVGG 3 ·106 VGG16 14 ·106 [37]\nTViT 4 ·106 ViT-B16 85 ·106 [33]\nTSwinT 2.7 ·106 SwinT Tiny 28 ·106 [34]\nTable 1: Number of parameters for each tiny neural network compared to the original version.\n3 Applications to digital holographic microscopy\n3.1 Experimental setup and targeted pose measurement application\nOur ﬁnal goal is to achieve 3D position and displacement measurements by means of DH combined with computer\nvision and thus to perform simultaneous high accuracy in-plane and out-of-plane measurements. For that purpose, a\nperiodically micro-structured pattern is used in order to allow unambiguous in-plane position detection through absolute\nphase computations [ 13]. Using conventional computer vision, a 108 range-to-resolution ratio was demonstrated\nthrough robust phase-based decoding [13, 44]. However, this 2D measurement method also works with DH [45, 46].\nIn order to apply that kind of micro-structured pattern to out-of-plane motion, a DHM is used. This paper explores\nif DL, and more particularly tiny networks, are able to determine the correct focusing distance with high accuracy\nand robustness to speed-up the pattern intensity and phase reconstructions and to provide a more accurate Z-position\nestimation along an extended longitudinal direction close to 100 µm.\nIn practice, experiments were carried out on an antivibration table with a DHM (by Lyncee Tec, Switzerland) equipped\nwith a camera with a 5.86 µm pixel size (Basler acA1920-155um), a hexapode (Newport HXP50-meca) capable\nof precise motions along the six degrees of freedom and a micro-encoded pattern made in our clean room facility\n(2 ×2 cm2, period 9 µm, 12 bits encoding [ 13]) covered with a uniform 100 nm thick aluminium layer to obtain\n5\na phase object. This pseudo periodic pattern was observed with a MO (Leica, mag 10 x, NA=0.32) at wavelength\nλ= 674.99 nm. The light source consists of a superluminescent diode equipped with an interference ﬁlter whose width\nis of 5 nm at half maximum, leading to a coherence of about 100 µm. The sample was shifted along the Z direction\nby steps of ∼1 µm and on a total height of ∼92 µm. At each Z step, a series of 400 holograms (1024 ×1024 px, 8\nbits) was recorded with random displacements along the lateral X and Y directions and random planar angles between\n±8 degrees. In total the experimental dataset contains 40,040 holograms.\n3.2 Autofocusing in digital holographic microscopy\nThe advantage of DH is to provide at different reconstruction distances ZR the complex ﬁeld diffracted over a distance\nZH from the hologram plane. The hologram propagation ZH can be tuned over an extended range of up to 92µm in our\nDHM setup (limited by the light source coherence length). Figures 3(a1), (b1) and (c1) show an experimental hologram\nof 1024x1024 pixels propagated at three different distances, 65 µm, 115 µm and 157 µm, respectively. The insets are a\nzoom of the same sub-area of 128x128 pixels. Over this large propagation range, the holograms recorded are entirely\ndifferent. Among the different reconstructed planes, the reconstruction distance ZR = ZH corresponds to the image in\nfocus. Panels in Fig. (a2), (b2) and (c2) are respectively the amplitude image reconstruction of the holograms panels\nin Fig. (a1), (b1) and (c1) with a back propagation distance ZR = ZH. The reconstruction is based on a plane waves\nangular spectrum method [47]. Except for illumination variation along the recording distance range of 92 µm, the three\nreconstructed images are highly similar.\nTo ﬁnd the axial position of an object, the challenge is therefore to ﬁnd this distance ZR. Autofocusing techniques\nin DH are applied considering the modulus of the reconstructed complex ﬁeld or the modulus of spatial spectrum of\nthe propagated ﬁeld [15]. The sharpness of the image can be determined from multiple focusing criteria such as the\nsum of the modulus of the complex ﬁeld amplitude, the use of a logarithmically weighted Fourier spectral function,\nthe variance of gray value distribution, focus measure based on autocorrelation, absolute gradient operator, Laplace\nﬁltering, Tamura coefﬁcient estimation, or wavelet-based approaches [15]. A comparison of many focusing criteria in\nterms of computational time and accuracy in determining the focal plane have been already discussed [15, 16].\n(a1)\nHologram\n \nZH=65 µm\n(a2)\nReconstructed\nimage\nIn focus distance \nZR=65 µm100 µm\n30 µm\n(b1)\nHologram\n \nZH=115 µm\n(c1)\nHologram\n \nZH=157 µm\n(b2)\n30 µm\n100 µm In focus distance \nZR=115 µm\nReconstructed\nimage\n(c2)\n30 µm\n100 µm In focus distance \nZR=157 µm\nReconstructed\nimage\nFigure 3: (a1), (b1) and (c1), experimental holograms (1024x1024 pixels) of the same area of a pseudo-periodic pattern\ncorresponding to a propagating distance ZH of 65 µm, 115 µm and 157 µm, respectively. (a2), (b2) and (c2), amplitude\nimage reconstruction at a distance ZH = ZR, respectively. The insets are zooms of the same sub-area of 128x128\npixels.\n6\n90 100 110 120 130\nReconstruction distance (µm)\n0.2\n0.4\n0.6\n0.8\n1\nNormalized focus metric\nZR(a)\n(d)\n(b)\n30 µm\n100 µm Distance \nZR(b)=115 µm\nReconstructed\nimage\n(a)\n30 µm\n100 µm Distance \nZR(a)=105 µm\nReconstructed\nimage\n(c)\n30 µm\n100 µm Distance \nZR(c)=125 µm\nReconstructed\nimage\nZR(b)\nZR(c)\nFigure 4: (a), (b) and (c), amplitude image reconstruction of the hologram of Fig. 3 (b1) at distances ZR of 105 µm,\n115 µm and 125 µm, respectively. (d), focus estimation function calculated from the intensity image Laplacian (LAP) as\ndescribed in [15]. The red crosses correspond to the distance reconstruction ZR(a) , ZR(b) and ZR(c) of the panel (a), (b)\nand (c), respectively.\nWhen the focus criterion is at an extremum, the focus of the reconstructed image is optimal. There are also various\nmethods that would allow an automated determination of the optimal reconstruction distance [48]. However, all these\napproaches require the numerical reconstruction of a set of images within a given range of propagation distances. Then,\nthe focus criteria is calculated from each reconstructed image, to determine the distance of focusing. Figures 4(a), (b)\nand (c) show three reconstructed images at different distance ZR of the experimental hologram of the Fig. 3(b1).\nThese three images spaced by 10 µm from each other illustrate the difﬁculties of obtaining a sharp focus criteria.\nFigure 4(d) shows the result of a focus metric based on the image Laplacian [15] where the red crosses correspond to the\ncase of the panels (a), (b) and (c). The resolution of this normalized autofocusing method is close to the DoF provided by\nthe Numerical Aperture (NA) of the MO and the wavelength λused, which gives in our case DoF = 2λ/NA2 = 15 µm.\nEven if these approaches could be efﬁcient [48], they are computationally demanding and time consuming, especially if\nthe size of the hologram is large.\n4 Results\nThis section shows the results obtained by running a series of inferences on test sets of different holograms. All the test\nresults have been generated using the proposed tiny models: TViT, TVGG and TSwinT. Four datasets are considered:\nexperimental and simulated phase holograms of the pseudo-periodic pattern, and amplitude and phase holograms\nof a simulated USAF pattern. The simulated holograms are generated by using a plane-wave spectrum propagation\nalgorithm. Although the simulation reproduces the experimental parameters of the sample and the imaging system, it is\ndeliberately free of motion uncertainties, surface defects, optical aberration and noise. This approach allows to obtain\nthe intrinsic performance limit of the neural networks proposed. Free of mechanical limitation, the simulated Z pitches\nare therefore less than 1 µm and over a total range of 100 µm.\nFor each set of holograms, TViT, TVGG and TSwinT have been trained from scratch. The neural networks have been\nconﬁgured to apply a regression on the input data. A total of 200 epochs have been executed and the learning curves\nhave correctly converged. The learning rate was set to 1 ·10−4 using the Adam optimizer [49]. During the training, a\n7\n-1-0.8-0.6-0.4-0.20 0.20.40.60.81Error  (µm)0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12Probability density\nTViTAlexNetLeNetVGG16SwinT TinyViT/B16\nFigure 5: Error distribution comparison between TViT and other state-of-the-art models like AlexNet, LeNet and\noriginal models such as VGG16, SwinT Tiny and ViT/B16. ¯εTViT = −0.07 ±0.61 µm, ¯εAlexNet = 0 ±0.51 µm,\n¯εLeNet = −0.02 ±0.61 µm, ¯εVGG16 = 0 ±0.56 µm, ¯εSwinT Tiny = −0.17 ±0.59 µm, ¯εViT/B16 = 0.01 ±0.66 µm\n,\ntotal of 64 (TViT) or 32 (TSwinT & TVGG) ROIs are selected randomly for each hologram. As showed in [50], the\nlog(cosh) loss function can improve the result of Varational Auto-Encoder. This loss function\nL(ZH,ZPred) =\nn∑\ni=1\nlog(cosh(ZPred\ni −ZHi )), (9)\nis also less prone to outliers than the mean squared error (MSE) or the mean absolute error (MAE) where nis the\nnumber of training samples, Zand ZPred are the expected and predicted values, respectively.\nTable 2 shows the performance of the validation loss functions on our experimental hologram dataset for all tiny\nnetworks. Table 3 shows the best validation loss Lfor each model and each training set. In the following sections, the\nerror ϵof one inference is measured by,\nϵ= ZPred\nR −ZH. (10)\nAll the codes to train the proposed tiny networks (TViT, TVGG and TSwinT) are accessible at this address https:\n//github.com/scuenat/DHMTinyNetworks and the data is available in [51].\nModel MSE MAE log cosh\nTVGG 0.25 0.39 0.11\nTViT 0.42 0.50 0.13\nTSwinT 0.30 0.43 0.12\nTable 2: Comparison of the validation loss functions (log cosh, MSE or MAE) for each proposed tiny models trained on\nexperimental holograms.\nModel Pseudo-periodic pattern USAF (simulated)\nExperimental Simulated Amplitude Phase\nTVGG 0.11 0.003 0.04 0.05\nTViT 0.13 0.004 0.09 0.04\nTSwinT 0.12 0.012 0.05 0.07\nTable 3: Validation loss (log cosh) for each model and each set of holograms.\n4.1 Experimental holograms: pseudo periodic pattern\nHolograms of Fig. 3 (a1), (b1) and (c1), recorded at different distances ZH, are representative specimens of the set of\nexperimental holograms used by the tiny networks during the training phase. The experimental dataset, which contains\n8\na total of 40,400 holograms (400 holograms for each distance ZH), was distributed between learning, validation and\ntesting sets with a 70/20/10 ratio. The models have therefore been tested on a set of 4,040 holograms, 40 holograms\nfor each ZH spaced by 1.0 µm ranging on 92 µm. In Fig. 6, the results of the inferences testing of the TViT, TVGG and\nTSwinT are represented on the panels (a), (b) and (c), respectively. The average and the Full Width at Half Maximum\n(FWHM) of the error are represented by the color bold curves and areas, respectively. Comparable performances for\nthe three neural networks with a high stability along the full range and small errors can be observed. Figure 9 gives\nanother view allowing to appreciate the error distribution. The solid lines are Gaussian ﬁts of the error for each network\nmodel. The average and standard deviation (half of the FWHM) are given for each case. Panel (a) illustrates the results\nof the experimental dataset. This graph shows an error bounded by 1 µm for all models. Figure 5 compares the error\ndistribution of a TViT model with reference neural networks in digital holography as VGG16, LeNet (as presented in\n[38]) and AlexNet (as presented in [21]). The original versions, SwinTransformer Tiny (SwinT Tiny) and ViT/B16, are\nalso represented. For the MO used, the 1 µm autofocusing accuracy achieved is 15 times smaller than the theoretical\nDoF.\n(a)\n(b)\n(c) ---\nReconstruction distance ZR (µm)\nFigure 6: Prediction error on experimental holograms of pseudo-periodic patterns. The bold lines (area) are the average\n(standard deviation) of the error for each reconstruction distance over 92µm. (a), (b) and (c) correspond to TViT, TVGG\nand TSwinT models, respectively.\n9\nFigure 7: (a) and (b), two representative examples of simulated 1024X1024 USAF hologram in amplitude and phase,\nrespectively.\n4.2 Simulated holograms: pseudo periodic pattern\nSimulated phase holograms of pseudo-periodic patterns are used to test the limit of the proposed models performances.\n40,040 holograms, including 40 different sites (in-plane position and orientation) vertically scanned per steps of 1µm,\nranging on 100 µm, constitute the full training dataset. The testing dataset is composed of 6,819 holograms never\nviewed, spaced by 0.1 µm and ranging on 100 µm. Figure 9(b) shows the error distribution of the reconstruction distance\nZR for the three models. Without noise and exact Z position labelling, the reconstruction error considerably decreases\nbelow 0.3 µm. The best models are TViT and TVGG which have a FWHM of the error distribution of 0.160 µm. These\ngreat performances for a testing with a step size ten time smaller than the training dataset prove the high regression\nquality of the three tiny networks.\n80 100 120 140 160\nReconstruction distance Z (µm)\n-5\n-4\n-3\n-2\n-1\n0\n1Mean error (µm)\nTSwinT\nTViT\nTVGG\n80 100 120 140 160\nReconstruction distance Z (µm)\n-6\n-4\n-2\n0\n2Mean error (µm)\nAlexNet\nLeNet\nVGG16\nSwinT Tiny\nViT/B16\n80 100 120 140 160\nReconstruction distance Z (µm)\n-5\n-4\n-3\n-2\n-1\n0\n1Mean error (µm)\n10%\n20%\n30%\n40%\n50%\n(a) (b)\n(c)\nFigure 8: Experimental results for all models in the case where the network input ROI artiﬁcially undergoes a loss of\ninformation. (a) shows the average error over the entire Z reconstruction range for the proposed tiny models with a loss\nof 10%, (b) shows the same but for original and reference models and (c) the limit of occlusion for a TViT model\n.\n10\n(d) εTSwinT=-0.01±0.23 µm\nεTViT=-0.03±0.23 µm\nεTVGG=0.03±0.25 µm\n-\n-\n-\nProbability density\n(b) εTSwinT=-0.014±0.15 µm\nεTViT=0±0.08 µm\nεTVGG=0±0.08 µm\n-\n-\n-\nProbability density\n(a) εTSwinT=-0.09±0.57 µm\nεTViT=-0.07±0.61 µm\nεTVGG=0±0.59 µm\n-\n-\n-\nProbability density\n(c) εTSwinT=-0.01±0.24 µm\nεTViT=-0.03±0.34 µm\nεTVGG=0.02±0.34 µm\n-\n-\n-\nProbability density\nFigure 9: Error distribution of the three neural networks over the reconstruction distance Z. (a) and (b) are the results\nfor the experimental and simulated pseudo-periodic patterns, respectively. (c) and (d) are the results for the simulated\nUSAF patterns in amplitude and phase, respectively.\n4.3 Amplitude and phase object: USAF 1951 resolution chart\nIn comparison with the pseudo-periodic sample, the USAF pattern is a more complex pattern due to a wider spatial\nfrequencies bandwidth. To avoid building a dataset with empty regions without information, the pattern was simulated\nby ﬁlling the space more densely than a commercial target. In order to characterize the tiny networks performances\nin function of the amplitude or phase nature of the holograms, two different datasets were constituted. Figures 7(a)\nand (b) show two representative holograms in amplitude and phase, respectively. Both training (testing) datasets are\nconstitued of 400 (50) different holograms at each step of 0.5µm ranging on 130 µm (100 µm), for a total of 104,400\n(10050) holograms. Figures 9(c) and (d) show the error distribution of the tiny networks in case of amplitude and phase\nobjects, respectively. In comparison with the pseudo-periodic pattern, all tiny networks have worse performances but\nstays highly competitive with a error below 0.35 µm.\nIn case of amplitude holograms, the TSwinT model shows better results than the other two. Contrary to the TSwinT\nmodel which has the same performances whatever the characteristic of the hologram, the TViT and the TVGG give\nmore precise inferences for the phase holograms. In this case, the three neural networks show similar performances\nwith an error smaller than 0.25 µm.\n4.4 Occlusion test\nAnother type of neural network performance is its robustness in a degraded conﬁguration. Experimentally we have\nalready seen that the three proposed models are resilient with respect to homogeneous sample illumination, as shown in\nFig. 3 and Fig. 4. To go further, areas of the experimental testing dataset have been deleted. A random squared region of\n10% of the ROI selected as input for the tiny neural networks is uniformly set to zero. This operation simulates a dust or\na sample defect and evaluates the degree of locality of the neural networks. The results obtained are shown in Fig. 8,\nwhere panel (a) display the error average along the Z for the proposed tiny models, panel (b) display the error average\nalong the Z for the reference and original models and panel (c) the limit of occlusion in case of a TViT model. It can be\nobserved that the TSwinT and TVGG models are highly impacted by 10% of occlusion as the models, AlexNet and\n11\nLeNet. In contrast, TViT is clearly the most robust architecture against occlusion. Figure 8(a) shows that on average the\nTViT error remains stable on the full 92 µm range.\n4.5 Inference speed\nWhether for applications in microrobotics or in 3D microscopy for life sciences, there is great interest in being able\nto work in real time and with commercially accessible equipment. Therefore, Figure 10 shows the comparison of the\nmedian speed of 200 inferences with two different conﬁgurations for all the neural networks compared (AlexNet, LeNet,\nVGG16, SwinT Tiny, ViT/B16, TViT, TVGG and TSwinT). On the Intel i9-11900K @3.50GHz CPU the performance\nis comparable to using a GPU NVidia RTX 3090, 24Gb, with an inference speed below 25 ms for LeNet and TViT. As\nanalyze in details [21], the reconstruction time of an hologram for twenty different distances takes a total of 318 ms on\nan Intel Core i5 processor. The image reconstruction knowing the predicted distance ZPred\nR is therefore of ∼15 ms.\nThis value is also conﬁrmed by the DHM which has a reconstruction rate of up to 60 frames per second. The tiny\nmodels proposed with low inference times, associated with an image reconstruction algorithm, therefore form a solution\ncompatible with the constraints of real applications.\nAlexNetLeNetVGG16SwinT TinyViT/B16TViTTVGGTSwinT101\n102\n103Inference speed in log scale\n20 Hz\nGPU/RTX 3090CPU i9 11th gen\nFigure 10: Comparison of the inference speed in log scale for AlexNet, LeNet, VGG16, SwinT Tiny, ViT/B16, TVGG,\nTSwinT and TViT on different architectures; GPU: RTX 3090 24Gb and CPU: Intel i9-11900K @3.50GHz.The dashed\nline represents the real-time limit in robotics.\n5 Discussion & conclusion\nTVGG, TViT and TSwinT give close results when taking the different sets of holograms (pseudo-periodic pattern\nexperimental/simulated and simulated USAF phase/amplitude). In this paper, it has been shown that TViT is more\nrobust in presence of occlusion (Fig. 8), considering that Z depth information is present on the entire hologram due to\ndiffraction properties in coherent imaging. A TViT model beneﬁts from the multi-head self-attention (Fig. 2) which\ntakes the complete ROI at each layer in consideration (not a set of extracted features). A CNN like TVGG works a bit\ndifferently as it tries to build a set of features through its ﬁrst Convolution/Pooling layers followed by full connected\nlayers (regression). A CNN, by extracting at each layer a more complex representation of the features, explains why it\nfocuses on dedicated regions which impacts the accuracy of the inference in presence of occlusion. According to the\nabove, a TViT model seems more suited to the prediction of the in-focus distance in DHM, as it scans everywhere and\nis more robust in terms of occlusion. This goes in the same direction as presented in [52, 53] where it has been shown\nthat a ViT model is a lot more robust than a CNN. Although, a TSwinT model is based on the derivative of a ViT model,\nit does not perform as well as a TViT in case of added occlusion. As TSwinT is only applying the self-attention on a set\nof windows (W-MSA) or shifted windows (SW-MSA) (through its Swin-Transformer Blocks, Fig. 1(c2)), it can be\nassumed that the occlusion has a bigger impact on the result of the multi-head attention than a pure ViT like TViT.\n12\nFigure 5 shows that the tiny models (TVGG, TSwinT and TViT) are as accurate as the original versions (VGG16,\nSwinT Tiny and ViT/B16). It also shows that a AlexNet or LeNet model reach similar performance. Considering the\ninference speed on CPU (Figure 10) and the robustness against an occlusion, TViT is the best model proposed.\nAs mentioned in [42], a ViT (TViT) would need a lot of data to be trained from scratch. This is not what has been\nexperienced, as a TViT can be trained from scratch using our set of experimental holograms of pseudo-periodic pattern\nusing a total of 2,327,040 ROIs (36,360 holograms ×64 ROI). A huge amount of data is normally needed as a ViT\n(TViT) projects each patch on an embedded dimension (Fig. 1(a), Patch embedding). The reconstruction distance Z\ninformation is spread over the complete space of the hologram, which is most likely an argument to explain why a\nViT-like network can learn from scratch without having a huge dataset at disposal.\nOur experiments showed that the reconstruction distance Z can be predicted in DHM with a high accuracy using deep\nlearning last generation techniques, especially regression models. An error bounded by ∼1 µm on the reconstruction\ndistance Z has been reached for a dataset of experimental holograms on a range of 92 µm. The regression approach\nallows experimentally to surpass the DoF of the MO by an order of magnitude. Moreover, this error can be lowered\ndown to ∼0.3 µm when the models are trained on the simulated holograms of a pseudo-periodic pattern or USAF\npattern (phase or amplitude). The discrepancy between results obtained from experimental and simulated datasets is\npartly due to the limited accuracy of the actuator used where bi-directional repeatability is of 0.3 µm. Acquisition noise\nmay also play a signiﬁcant role in that reduction of performances obtained from experimental datasets. All proposed\ntiny models offer an alternative to expensive GPUs as the time for an inference is below the real-time limit in robotics\nof 20 Hz (Fig. 10), less than 25 ms on an Intel i9.\nThe ability of tiny networks to determine the in-focus depth with a FWHM of about one micron opens attractive\napplication prospects. Indeed, if two wavelength DHM are considered, the ambiguity range is about twice the FWHM\ndemonstrated in this paper (with our commercial DHM, λ1 = 674.99 nm and λ2 = 793.63 nm; i.e. λeq/2 = 2.25 µm).\nThis means that DL may bridge the gap between the MO DoF and the Z-information provided by the interferometric\nphase to achieve Z-position determination down to the interference sensitivity; i.e. around 1 nm over ranges of tens\nof microns. Such a prospect would signiﬁcantly improve the current capabilities of computer vision position sensing\napplied to 3D microscopy.\n6 Acknowledgments\nThis work was supported by HOLO-CONTROL (ANR-21-CE42-0009), TIRREX (ANR-21-ESRE-0015), SMART-\nLIGHT (ANR-21-ESRE-0040) by Cross-disciplinary Research (EIPHI) Graduate School (contract ANR-17-EURE-\n0002), Région Bourgogne Franche-Comté (HoloNET project). This work was performed using HPC resources from\nGENCI-IDRIS (Grant 20XX-AD011012913) and also the Mésocentre de Franche-Comté.\nDisclosures\nThe authors declare no conﬂicts of interest.\nData availability\nData underlying the results presented in this paper are available in Zenodo, Ref. [51].\nReferences\n[1] Yu Sun, Stefan Duthaler, and Bradley J Nelson. Autofocusing in computer microscopy: selecting the optimal\nfocus algorithm. Microscopy research and technique, 65(3):139–149, 2004.\n[2] Jan-Mark Geusebroek, Frans Cornelissen, Arnold WM Smeulders, and Hugo Geerts. Robust autofocusing in\nmicroscopy. Cytometry: The Journal of the International Society for Analytical Cytology, 39(1):1–9, 2000.\n[3] Yalin Xiong and Steven A Shafer. Depth from focusing and defocusing. In Proceedings of IEEE Conference on\nComputer Vision and Pattern Recognition, pages 68–73. IEEE, 1993.\n[4] Gordon S Kino and Timothy R Corle. Confocal scanning optical microscopy and related imaging systems.\nAcademic Press, 1996.\n[5] HJ Leamy. Charge collection scanning electron microscopy. Journal of Applied Physics, 53(6):R51–R80, 1982.\n13\n[6] Frank Dubois, Cédric Schockaert, Natacha Callens, and Catherine Yourassowsky. Focus plane detection criteria in\ndigital holography microscopy by amplitude analysis. Opt. Express, 14(13):5895–5908, Jun 2006.\n[7] Pietro Ferraro, Simonetta Grilli, Domenico Alﬁeri, Sergio De Nicola, Andrea Finizio, Giovanni Pierattini, Bahram\nJavidi, Giuseppe Coppola, and Valerio Striano. Extended focused image in microscopy by digital holography.\nOpt. Express, 13(18):6738–6749, Sep 2005.\n[8] U. Schnars and W. Jüptner. Direct recording of holograms by a ccd target and numerical reconstruction. Appl.\nOpt., 33(2):179–181, Jan 1994.\n[9] Etienne Cuche, Frédéric Bevilacqua, and Christian Depeursinge. Digital holography for quantitative phase-contrast\nimaging. Optics letters, 24(5):291–293, 1999.\n[10] Maxime Jacquot, Patrick Sandoz, and Gilbert Tribillon. High resolution digital holography. Optics\ncommunications, 190(1-6):87–94, 2001.\n[11] Bahram Javidi, Artur Carnicer, Arun Anand, George Barbastathis, Wen Chen, Pietro Ferraro, J. W. Goodman,\nRyoichi Horisaki, Kedar Khare, Malgorzata Kujawinska, Rainer A. Leitgeb, Pierre Marquet, Takanori Nomura,\nAydogan Ozcan, YongKeun Park, Giancarlo Pedrini, Pascal Picart, Joseph Rosen, Genaro Saavedra, Natan T.\nShaked, Adrian Stern, Enrique Tajahuerce, Lei Tian, Gordon Wetzstein, and Masahiro Yamaguchi. Roadmap on\ndigital holography. Opt. Express, 29(22):35078–35118, Oct 2021.\n[12] Zhuoran Zhang, Xian Wang, Jun Liu, Changsheng Dai, and Yu Sun. Robotic micromanipulation: Fundamentals\nand applications. Annual Review of Control, Robotics, and Autonomous Systems, 2:181–203, 2019.\n[13] Antoine N André, Patrick Sandoz, Benjamin Mauzé, Maxime Jacquot, and Guillaume J Laurent. Sensing one\nnanometer over ten centimeters: A microencoded target for visual in-plane position measurement. IEEE/ASME\nTransactions on Mechatronics, 25(3):1193–1201, 2020.\n[14] Tristan Colomb, Nicolas Pavillon, Jonas Kühn, Etienne Cuche, Christian Depeursinge, and Yves Emery. Extended\ndepth-of-focus by digital holographic microscopy. Optics letters, 35(11):1840–1842, 2010.\n[15] Elsa SR Fonseca, Paulo T Fiadeiro, Manuela Pereira, and António Pinheiro. Comparative analysis of autofocus\nfunctions in digital in-line phase-shifting holography. Applied optics, 55(27):7663–7674, 2016.\n[16] Patrik Langehanenberg, Björn Kemper, Dieter Dirksen, and Gert V on Bally. Autofocusing in digital holographic\nphase contrast microscopy on pure phase objects for live cell imaging. Applied optics, 47(19):D176–D182, 2008.\n[17] Zhenbo Ren, Zhimin Xu, and Edmund Y Lam. Learning-based nonparametric autofocusing for digital holography.\nOptica, 5(4):337–344, 2018.\n[18] Yair Rivenson, Yibo Zhang, Harun Günaydın, Da Teng, and Aydogan Ozcan. Phase recovery and holographic\nimage reconstruction using deep learning in neural networks. Light: Science & Applications, 7(2):17141–17141,\n2018.\n[19] Yichen Wu, Yair Rivenson, Yibo Zhang, Zhensong Wei, Harun Günaydin, Xing Lin, and Aydogan Ozcan.\nExtended depth-of-ﬁeld in holographic imaging using deep-learning-based autofocusing and phase recovery.\nOptica, 5(6):704–710, 2018.\n[20] Gong Zhang, Tian Guan, Zhiyuan Shen, Xiangnan Wang, Tao Hu, Delai Wang, Yonghong He, and Ni Xie. Fast\nphase retrieval in off-axis digital holographic microscopy through deep learning. Optics express, 26(15):19388–\n19405, 2018.\n[21] Tomi Pitkäaho, Aki Manninen, and Thomas J Naughton. Focus prediction in digital holographic microscopy using\ndeep convolutional neural networks. Applied optics, 58(5):A202–A208, 2019.\n[22] Tianjiao Zeng, Yanmin Zhu, and Edmund Y . Lam. Deep learning for digital holography: a review.Opt. Express,\n29(24):40572–40593, Nov 2021.\n[23] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.\n[24] Ayan Sinha, Justin Lee, Shuai Li, and George Barbastathis. Lensless computational imaging through deep learning.\nOptica, 4(9):1117–1125, 2017.\n[25] Shaowei Jiang, Jun Liao, Zichao Bian, Kaikai Guo, Yongbing Zhang, and Guoan Zheng. Transform-and multi-\ndomain deep learning for single-frame rapid autofocusing in whole slide imaging. Biomedical optics express,\n9(4):1601–1612, 2018.\n[26] Simon Haykin. Neural networks: a comprehensive foundation. Prentice Hall PTR, 1994.\n[27] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.\n[28] Antonio Mucherino, Petraq J. Papajorgji, and Panos M. Pardalos. k-Nearest Neighbor Classiﬁcation, pages\n83–106. Springer New York, New York, NY , 2009.\n14\n[29] Demetri Psaltis, David Brady, Xiang-Guang Gu, and Steven Lin. Holography in artiﬁcial neural networks.\nLandmark Papers on Photorefractive Nonlinear Optics, pages 541–546, 1995.\n[30] Laurent Larger, Antonio Baylón-Fuentes, Romain Martinenghi, Vladimir S Udaltsov, Yanne K Chembo, and\nMaxime Jacquot. High-speed photonic reservoir computing using a time-delay-based architecture: Million words\nper second classiﬁcation. Physical Review X, 7(1):011015, 2017.\n[31] Xing Lin, Yair Rivenson, Nezih T Yardimci, Muhammed Veli, Yi Luo, Mona Jarrahi, and Aydogan Ozcan.\nAll-optical machine learning using diffractive deep neural networks. Science, 361(6406):1004–1008, 2018.\n[32] Henry Pinkard, Zachary Phillips, Arman Babakhani, Daniel A. Fletcher, and Laura Waller. Deep learning for\nsingle-shot autofocus microscopy. Optica, 6(6):794–797, Jun 2019.\n[33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929,\n2020.\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.\n[35] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for\nthe 2020s. arXiv preprint arXiv:2201.03545, 2022.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[37] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556, 2014.\n[38] Keyvan Jaferzadeh, Seung-Hyeon Hwang, Inkyu Moon, and Bahram Javidi. No-search focus prediction at the\nsingle cell level in digital holographic imaging with deep convolutional neural network. Biomed. Opt. Express,\n10(8):4276–4289, Aug 2019.\n[39] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708,\n2017.\n[40] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In\nInternational conference on machine learning, pages 6105–6114. PMLR, 2019.\n[41] Lester James Miranda. Understanding softmax and the negative log-likelihood\". ljvmiranda921.github.io, 2017.\n[42] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision\ntransformers see like convolutional neural networks? Advances in Neural Information Processing Systems, 34,\n2021.\n[43] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase.\n[44] Antoine N André, Patrick Sandoz, Maxime Jacquot, and Guillaume J Laurent. Robust, precise and scalable:\nA phase-encoded pattern for visual x, y, θ positioning. In 2020 International Conference on Manipulation,\nAutomation and Robotics at Small Scales (MARSS), pages 1–5. IEEE, 2020.\n[45] Patrick Sandoz and Maxime Jacquot. Lensless vision system for in-plane positioning of a patterned plate with\nsubpixel resolution. JOSA A, 28(12):2494–2500, 2011.\n[46] Miguel Asmad Vergara, Maxime Jacquot, Guillaume J Laurent, and Patrick Sandoz. Digital holography as\ncomputer vision position sensor with an extended range of working distances. Sensors, 18(7):2005, 2018.\n[47] J.W. Goodman. Introduction to Fourier Optics. Electrical Engineering Series. McGraw-Hill, 1996.\n[48] Mert Do˘gar, Hazar A ˙Ilhan, and Meriç Özcan. Real-time, auto-focusing digital holographic microscope using\ngraphics processors. Review of Scientiﬁc Instruments, 84(8):083704, 2013.\n[49] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980,\n2014.\n[50] Pengfei Chen, Guangyong Chen, and Shengyu Zhang. Log hyperbolic cosine loss improves variational auto-\nencoder. https://openreview.net/forum?id=rkglvsC9Ym, 2019.\n[51] Experiment dataset (pseudo periodic pattern), 2022. Available at https://zenodo.org/record/6337535.\n15\n[52] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. Intriguing properties of vision transformers. Advances in Neural Information Processing Systems, 34,\n2021.\n[53] Stéphane Cuenat and Raphaël Couturier. Convolutional neural network (cnn) vs vision transformer (vit) for digital\nholography. arXiv preprint arXiv:2108.09147, 2021.\n16"
}