{
  "title": "Contrastive Distillation on Intermediate Representations for Language Model Compression",
  "url": "https://openalex.org/W3090789082",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2356594925",
      "name": "Sun, Siqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2365636915",
      "name": "Gan, Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973031759",
      "name": "Cheng Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2368446069",
      "name": "Fang, Yuwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202139473",
      "name": "Wang, Shuohang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1922531866",
      "name": "Liu JingJing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2976833415",
    "https://openalex.org/W2997710335",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2951292523",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2951585248",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2944828972",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2964420626",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2969624041",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2979567256",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3018378048",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3019527251",
    "https://openalex.org/W2951873722",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2973061659",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2981794819",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2987283559",
    "https://openalex.org/W2971155163"
  ],
  "abstract": "Existing language model compression methods mostly use a simple L2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.",
  "full_text": "Contrastive Distillation on Intermediate Representations\nfor Language Model Compression\nSiqi Sun, Zhe Gan, Yu Cheng, Yuwei Fang, Shuohang Wang, Jingjing Liu\nMicrosoft Dynamics 365 AI Research\n{siqi.sun,zhe.gan,yu.cheng,yuwfan,shuohang.wang,jingjl}@microsoft.com\nAbstract\nExisting language model compression meth-\nods mostly use a simple L2 loss to distill\nknowledge in the intermediate representations\nof a large BERT model to a smaller one. Al-\nthough widely used, this objective by design\nassumes that all the dimensions of hidden rep-\nresentations are independent, failing to cap-\nture important structural knowledge in the in-\ntermediate layers of the teacher network. To\nachieve better distillation efÔ¨Åcacy, we propose\nContrastive Distillation on Intermediate Repre-\nsentations (C ODIR), a principled knowledge\ndistillation framework where the student is\ntrained to distill knowledge through interme-\ndiate layers of the teacher via a contrastive\nobjective. By learning to distinguish posi-\ntive sample from a large set of negative sam-\nples, CoDIR facilitates the student‚Äôs exploita-\ntion of rich information in teacher‚Äôs hidden\nlayers. CoDIR can be readily applied to com-\npress large-scale language models in both pre-\ntraining and Ô¨Ånetuning stages, and achieves\nsuperb performance on the GLUE bench-\nmark, outperforming state-of-the-art compres-\nsion methods.1\n1 Introduction\nLarge-scale pre-trained language models (LMs),\nsuch as BERT (Devlin et al., 2018), XLNet (Yang\net al., 2019) and RoBERTa (Liu et al., 2019),\nhave brought revolutionary advancement to the\nNLP Ô¨Åeld (Wang et al., 2018). However, as new-\ngeneration LMs grow more and more into behe-\nmoth size, it becomes increasingly challenging\nto deploy them in resource-deprived environment.\nNaturally, there has been a surge of research inter-\nest in developing model compression methods (Sun\net al., 2019; Sanh et al., 2019; Shen et al., 2019)\n1Code will be released at https://github.com/\nintersun/CoDIR.\nto reduce network size in pre-trained LMs, while\nretaining comparable performance and efÔ¨Åciency.\nPKD (Sun et al., 2019) was the Ô¨Årst known effort\nin this expedition, an elegant and effective method\nthat uses knowledge distillation (KD) for BERT\nmodel compression at Ô¨Ånetuning stage. Later on,\nDistilBERT (Sanh et al., 2019), TinyBERT (Jiao\net al., 2019) and MobileBERT (Sun et al., 2020) car-\nried on the torch and extended similar compression\ntechniques to pre-training stage, allowing efÔ¨Åcient\ntraining of task-agnostic compressed models. In\naddition to the conventional KL-divergence loss ap-\nplied to the probabilistic output of the teacher and\nstudent networks, an L2 loss measuring the differ-\nence between normalized hidden layers has proven\nto be highly effective in these methods. However,\nL2 norm follows the assumption that all dimen-\nsions of the target representation are independent,\nwhich overlooks important structural information\nin the many hidden layers of BERT teacher.\nMotivated by this, we propose Contrastive\nDistillation for Intermediate Representations\n(CODIR ), which uses a contrastive objective to\ncapture higher-order output dependencies between\nintermediate representations of BERT teacher and\nthe student. Contrastive learning (Gutmann and\nHyv¬®arinen, 2010) aims to learn representations by\nenforcing similar elements to be equal and dissimi-\nlar elements further apart. Formulated in either su-\npervised or unsupervised way, it has been success-\nfully applied to diverse applications (Hjelm et al.,\n2018; He et al., 2019; Tian et al., 2019; Khosla\net al., 2020). To the best of our knowledge, utilizing\ncontrastive learning to compress large Transformer\nmodels is still an unexplored territory, which is the\nmain focus of this paper.\nA teacher network‚Äôs hidden layers usually con-\ntain rich semantic and syntactic knowledge that\ncan be instrumental if successfully passed on to\nthe student (Tenney et al., 2019; Kovaleva et al.,\narXiv:2009.14167v1  [cs.CL]  29 Sep 2020\n2019; Sun et al., 2019). Thus, instead of directly\napplying contrastive loss to the Ô¨Ånal output layer of\nthe teacher, we apply contrastive learning to its in-\ntermediate layers, in addition to the use of KL-loss\nbetween the probabilistic outputs of the teacher and\nstudent. This casts a stronger regularization effect\nfor student training by capturing more informative\nsignals from intermediate representations. To max-\nimize the exploitation of intermediate layers of the\nteacher, we also propose the use of mean-pooled\nrepresentation as the distillation target, which is\nempirically more effective than commonly used\nspecial [CLS] token.\nTo realize constrastive distillation, we deÔ¨Åne a\ncongruent pair (ht\ni,hs\ni ) as the pair of representa-\ntions of the same data input from the teacher and\nstudent networks, as illustrated in Figure 1. Incon-\ngruent pair (ht\ni,hs\nj) is deÔ¨Åned as the pair of repre-\nsentations of two different data samples through the\nteacher and the student networks, respectively. The\ngoal is to train the student network to distinguish\nthe congruent pair from a large set of incongruent\npairs, by minimizing the constrastive objective.\nFor efÔ¨Åcient training, all data samples are stored\nin a memory bank (Wu et al., 2018; He et al.,\n2019). During Ô¨Ånetuning, incongruent pairs can be\nselected by choosing sample pairs with different\nlabels to maximize the distance. For pre-training,\nhowever, it is not straightforward to construct in-\ncongruent pairs this way as labels are unavailable.\nThus, we randomly sample data points from the\nsame mini-batch pair to form incongruent pairs,\nand construct a proxy congruent-incongruent sam-\nple pool to assimilate what is observed in the down-\nstream tasks during Ô¨Ånetuning stage. This and other\ndesigns in CoDIR make constrative learning pos-\nsible for LM compression, and have demonstrated\nstrong performance and high efÔ¨Åciency in experi-\nments.\nOur contributions are summarized as follows.\n(i) We propose CODIR , a principled framework\nto distill knowledge in the intermediate represen-\ntations of large-scale language models via a con-\ntrastive objective, instead of a conventionalL2 loss.\n(ii) We propose effective sampling strategies to\nenable CoDIR in both pre-training and Ô¨Ånetuning\nstages. (iii) Experimental results demonstrate that\nCoDIR can successfully train a half-size Trans-\nformer model that achieves competing performance\nto BERT-base on the GLUE benchmark (Wang\net al., 2018), with half training time and GPU de-\nmand. Our pre-trained model checkpoint will be\nreleased for public access.\n2 Related Work\nLanguage Model Compression To reduce com-\nputational cost of training large-scale language\nmodels, many model compression techniques have\nbeen developed, such as quantization (Shen et al.,\n2019; Zafrir et al., 2019), pruning (Guo et al., 2019;\nGordon et al., 2020; Michel et al., 2019), knowl-\nedge distillation (Tang et al., 2019; Sun et al., 2019;\nSanh et al., 2019; Jiao et al., 2019; Sun et al., 2020),\nand direct Transformer block modiÔ¨Åcation (Kitaev\net al., 2020; Wu et al., 2020).\nQuantization refers to storing model parameters\nfrom 32- or 16-bit Ô¨Çoating number to 8-bit or even\nlower. Directly truncating the parameter values will\ncause signiÔ¨Åcant accuracy loss, hence quantization-\naware training has been developed to maintain sim-\nilar accuracy to the original model (Shen et al.,\n2019; Zafrir et al., 2019). Michel et al. (2019)\nfound that even after most attention heads are re-\nmoved, the model still retains similar accuracy, in-\ndicating there is high redundancy in the learned\nmodel weights. Later studies proposed different\npruning-based methods. For example, Gordon et al.\n(2020) simply removed the model weights that\nare close to zero; while Guo et al. (2019) used\nre-weighted L1 and proximal algorithm to prune\nweights to zero. Note that simple pruning does not\nimprove inference speed, unless there is structure\nchange such as removing the whole attention head.\nThere are also some efforts that try to improve\nthe Transformer block directly. Typically, language\nmodels such as BERT (Devlin et al., 2018) and\nRoBERTa (Liu et al., 2019) can only handle a se-\nquence of tokens in length up to 512. Kitaev et al.\n(2020) proposed to use reversible residual layers\nand locality-sensitive hashing to reduce memory us-\nage to deal with extremely long sequences. Besides,\nWu et al. (2020) proposed to use convolutional neu-\nral networks to capture short-range attention such\nthat reducing the size of self-attention will not sig-\nniÔ¨Åcantly hurt performance.\nAnother line of research on model compression\nis based on knowledge transfer, or knowledge dis-\ntillation (KD) (Hinton et al., 2015), which is the\nmain focus of this paper. Note that previously intro-\nduced model compression techniques are orthogo-\nnal to KD, and can be bundled for further speedup.\nDistilled BiLSTM (Tang et al., 2019) tried to dis-\nTrm\nTrmTrm\nTrm\nX X\nTrm Trm\nTeacher Model (ùëìùë°) Student Model (ùëìùë†)\n‚Ñé0\nùë†\n‚Ñé0\nùë°\nùëã0 = the greatest musicians   [True]  \nùëã1 = cold movie                     [False]                \nùëã2 = no apparent joy             [False]         \n....                                           \nùëãùêæ = the action is stilted        [False]         \n.... \n‚Ñíùêæùê∑ ‚Ñíùê∂ùê∏ or ‚ÑíùëÄùêøùëÄ\n‚Ñíùê∂ùëÖùê∑\n‚Ñé2\nùë†\n‚Ñé1\nùë†\n‚Ñéùêæ\nùë†\nTraining \nData\nContrastive Distillation\nIntermediate \nRepresentations\nùëã0 = [MASK] music is a [MASK] of ...\nùëã1 = The term ‚Äú[MASK] music‚Äù ...\nùëã2 = ‚Äúpop‚Äù [MASK] ‚Äúrock‚Äù were ...\n....                                           \nùëãùêæ = Identifying [MASK] usually ...\nPop Music\nS1: pop music is a genre of popular\nmusic that ...\nS2: The term ‚Äúpopular music‚Äù and \n‚Äúpop music‚Äù ...\nS3: ‚Äúpop‚Äù and ‚Äúrock‚Äù were roughly \nsynonymous...\n...\nSK: Identifying factors usually \ninclude short ...\nPre-Training\nFinetuning\nCongruent \nPairs\nIncongruent \nPairs\nFigure 1: Overview of the proposed CoDIR framework for language model compression in both pre-training and\nÔ¨Ånetuning stages. ‚ÄúTrm‚Äù represents a Transformer block, X are input tokens, ft and fs are teacher and student\nmodels, and X0,{Xi}K\ni=1 represent one positive sample and a set of negative samples, respectively. The difference\nbetween CoDIR pre-training and Ô¨Ånetuning mainly lies in the negative example sampling method.\ntill knowledge from BERT into a simple LSTM.\nThough achieving more than 400 times speedup\ncompared to BERT-large, it suffers from signiÔ¨Åcant\nperformance loss due to the shallow network archi-\ntecture. DistilBERT (Sanh et al., 2019) proposed to\ndistill predicted logits from the teacher model into\na student model with 6 Transformer blocks. BERT-\nPKD (Sun et al., 2019) proposed to not only distill\nthe logits, but also the representation of [CLS]\ntokens from the intermediate layers of the teacher\nmodel. TinyBERT (Jiao et al., 2019), MobileBERT\n(Sun et al., 2020) and SID (Aguilar et al., 2019) fur-\nther proposed to improve BERT-PKD by distilling\nmore internal representations to the student, such\nas embedding layers and attention weights. These\nexisting methods can be generally divided into two\ncategories: (i) task-speciÔ¨Åc, and (ii) task-agnostic.\nTask-speciÔ¨Åc methods, such as Distilled BiLSTM,\nBERT-PKD and SID, require the training of indi-\nvidual teacher model for each downstream task;\nwhile task-agnostic methods such as DistilBERT,\nTinyBERT and MobileBERT use KD to pre-train a\nmodel that can be applied to all downstream tasks\nby standard Ô¨Ånetuning.\nContrastive Representation Learning Con-\ntrastive learning (Gutmann and Hyv¬®arinen, 2010;\nArora et al., 2019) is a popular research area that\nhas been successfully applied to density estima-\ntion and representation learning, especially in self-\nsupervised setting (He et al., 2019; Chen et al.,\n2020). It has been shown that the contrastive\nobjective can be interpreted as maximizing the\nlower bound of mutual information between dif-\nferent views of the data (Hjelm et al., 2018; Oord\net al., 2018; Bachman et al., 2019; H ¬¥enaff et al.,\n2019). However, it is unclear whether the success\nis determined by mutual information or by the spe-\nciÔ¨Åc form of the contrastive loss (Tschannen et al.,\n2019). Recently, it has been extended to knowledge\ndistillation and cross-modal transfer for image clas-\nsiÔ¨Åcation tasks (Tian et al., 2019). Different from\nprior work, we propose the use of contrastive ob-\njective for Transformer-based model compression\nand focus on language understanding tasks.\n3 CoDIR for Model Compression\nIn this section, we Ô¨Årst provide an overview of\nthe proposed method in Sec. 3.1, then describe the\ndetails of contrastive distillation in Sec. 3.2. Its\nadaptation to pre-training and Ô¨Ånetuning is further\ndiscussed in Sec. 3.3.\n3.1 Framework Overview\nWe use RoBERTa-base (Liu et al., 2019) as the\nteacher network, denoted as ft, which has 12 lay-\ners with 768-dimension hidden representations. We\naim to transfer the knowledge of ft into a stu-\ndent network fs, where fs is a 6-layer Trans-\nformer (Vaswani et al., 2017) to mimic the behav-\nior of ft (Hinton et al., 2015). Denote a train-\ning sample as (X,y), where X = (x0,...,x L‚àí1)\nis a sequence of tokens in length L, and y is\nthe corresponding label (if available). The word\nembedding matrix of X is represented as X =\n(x0,..., xL‚àí1), where xi ‚ààRd is a d-dimensional\nvector, and X ‚ààRL√ód. In addition, the interme-\ndiate representations at each layer for the teacher\nand student are denoted as Ht = (Ht\n1,..., Ht\n12)\nand Hs = ( Hs\n1,..., Hs\n6), respectively, where\nHt\ni,Hs\ni ‚ààRL√ód contains all the hidden states in\none layer. And zt,zs ‚ààRk are the logit represen-\ntations (before the softmax layer) of the teacher\nand student, respectively, where kis the number of\nclasses.\nAs illustrated in Figure 1, our distillation ob-\njective consists of three components: ( i) original\ntraining loss from the target task; (ii) conventional\nKL-divergence-based loss to distill the knowledge\nof zt into zs; (iii) proposed contrastive loss to dis-\ntill the knowledge ofHt into Hs. The Ô¨Ånal training\nobjective can be written as:\nLCoDIR(Œ∏) = LCE(zs,y; Œ∏) + Œ±1LKD(zt,zs; Œ∏)\n+ Œ±2LCRD(Ht,Hs; Œ∏) , (1)\nwhere LCE,LKD and LCRD correspond to the origi-\nnal loss, KD loss and contrastive loss, respectively.\nŒ∏denotes all the learnable parameters in the stu-\ndent fs, while the teacher network is pre-trained\nand kept Ô¨Åxed. Œ±1,Œ±2 are two hyper-parameters to\nbalance the loss terms.\nLCE is typically implemented as a cross-entropy\nloss for classiÔ¨Åcation problems, and LKD can be\nwritten as\nLKD(zt,zs; Œ∏) = KL\n(\ng(zt/œÅ)‚à•g(zs/œÅ)\n)\n, (2)\nwhere g(¬∑) denotes the softmax function, and œÅ\nis the temperature. LKD encourages the student\nnetwork to produce distributionally-similar outputs\nto the teacher network.\nOnly relying on the Ô¨Ånal logit output for distilla-\ntion discards the rich information hidden in the in-\ntermediate layers of BERT. Recent work (Sun et al.,\n2019; Jiao et al., 2019) has found that distilling the\nknowledge from intermediate representations with\nL2 loss can further enhance the performance. Fol-\nlowing the same intuition, our proposed method\nalso aims to achieve this goal, with a more princi-\npled contrastive objective as detailed below.\n3.2 Contrastive Distillation\nFirst, we describe how to summarize intermediate\nrepresentations into a concise feature vector. Based\non this, we detail how to perform contrastive distil-\nlation (Tian et al., 2019) for model compression.\nIntermediate Representation Directly using\nHt and Hs for distillation is infeasible, as the total\nfeature dimension is |Hs|= 6 √ó512 √ó768 ‚âà2.4\nmillion for a sentence in full length ( i.e., L =\n512). Therefore, we propose to Ô¨Årst perform mean-\npooling over Ht and Hs to obtain a layer-wise\nsentence embedding. Note that the embedding of\nthe [CLS] token can also be used directly for this\npurpose; however, in practice we found that mean-\npooling performs better. SpeciÔ¨Åcally, we conduct\nrow-wise average over Ht\ni and Hs\ni :\n¬Øht\ni = Pool(Ht\ni), ¬Øhs\ni = Pool(Hs\ni ) , (3)\nwhere ¬Øht\ni,¬Øhs\ni ‚ààRd are the sentence embedding\nfor layer i of the teacher and student model, re-\nspectively. Therefore, the student‚Äôs intermedi-\nate representation can be summarized as ¬Øhs =\n[¬Øhs\n1; ... ; ¬Øhs\n6] ‚àà R6d, where [; ] denotes vector\nconcatenation. Similarly, the teacher‚Äôs interme-\ndiate representation can be summarized as ¬Øht =\n[¬Øht\n1; ... ; ¬Øht\n12] ‚ààR12d. Two linear mappings œÜs :\nR6d ‚Üí Rm and œÜt : R12d ‚Üí Rm are further\napplied to project ¬Øht and ¬Øhs into the same low-\ndimensional space, yielding ht,hs ‚ààRm, which\nare used for calculating the contrastive loss.\nContrastive Objective Given a training sample\n(X0,y0), we Ô¨Årst randomly select Knegative sam-\nples with different labels, denoted as{(Xi,yi)}K\ni=1.\nFollowing the above process, we can obtain a sum-\nmarized intermediate representation ht\n0,hs\n0 ‚ààRm\nby sending X0 to both the teacher and student net-\nwork. Similarly, for negative samples, we can ob-\ntain {hs\ni }K\ni=1.\nContrastive learning aims to map the student‚Äôs\nrepresentation hs\n0 close to the teacher‚Äôs represen-\ntation ht\n0, while the negative samples‚Äô representa-\ntions {hs\ni }K\ni=1 far apart from ht\n0. To achieve this,\nwe use the following InfoNCE loss (Oord et al.,\n2018) for model training:\nLCRD(Œ∏) = ‚àílog exp\n(\n‚ü®ht\n0,hs\n0‚ü©/œÑ\n)\n‚àëK\nj=0 exp\n(\n‚ü®ht\n0,hs\nj‚ü©/œÑ\n), (4)\nwhere ‚ü®¬∑,¬∑‚ü©denotes the cosine similarity between\ntwo feature vectors, and œÑ is the temperature that\ncontrols the concentration level. As demonstrated,\ncontrastive distillation is implemented as a (K+\n1)-way classiÔ¨Åcation task, which is interpreted as\nmaximizing the lower bound of mutual information\nbetween ht\n0 and hs\n0 (Oord et al., 2018; Tian et al.,\n2019).\n3.3 Pre-training and Finetuning Adaptation\nMemory Bank For a positive pair (ht\n0,hs\n0), one\nneeds to compute the intermediate representations\nfor all the negative samples, i.e., {hs\ni }K\ni=1, which\nrequires K + 1 times computation compared to\nnormal training. A large number of negative sam-\nples is required to ensure performance (Arora et al.,\n2019), which renders large-scale contrastive distil-\nlation infeasible for practical use. To address this\nissue, we follow Wu et al. (2018) and use a mem-\nory bank M ‚ààRN√óm to store the intermediate\nrepresentation of all N training examples, and the\nrepresentation is only updated for positive samples\nin each forward propagation. Therefore, the train-\ning cost is roughly the same as in normal training.\nSpeciÔ¨Åcally, assume the mini-batch size is 1, then\nat each training step, M is updated as:\nm0 = Œ≤¬∑m0 + (1 ‚àíŒ≤) ¬∑hs\n0 , (5)\nwhere m0 is the retrieved representation from\nmemory bank M that corresponds to hs\n0, and\nŒ≤ ‚àà(0,1) is a hyper-parameter that controls how\naggressively the memory bank is updated.\nFinetuning Since task-speciÔ¨Åc label supervision\nis available in Ô¨Ånetuning stage, applying CoDIR\nto Ô¨Ånetuning is relatively straightforward. When\nselecting negative samples from the memory bank,\nwe make sure the selected samples have different\nlabels from the positive sample.\nPre-training For pre-training, the target task be-\ncomes masked language modeling (MLM) (De-\nvlin et al., 2018). Therefore, we replace the LCE\nloss in Eqn. (1) with LMLM. Following Liu et al.\n(2019); Lan et al. (2019), we did not include the\nnext-sentence-prediction task for pre-training, as\nit does not improve performance on downstream\ntasks. Since task-speciÔ¨Åc label supervision is un-\navailable during pre-training, we propose an effec-\ntive method to select negative samples from the\nmemory bank. SpeciÔ¨Åcally, we sample negative\nexamples randomly from the same mini-batch each\ntime, as they have closer semantic meaning as some\nof them are from the same article, especially for\nBookcorpus (Zhu et al., 2015). Then, we use the\nsampled negative examples to retrieve representa-\ntions from the memory bank. Intuitively, negative\nexamples sampled in this way serve as ‚Äúhard‚Äù neg-\natives, compared to randomly sampling from the\nwhole training corpus; otherwise, the LCRD loss\ncould easily drop to zero if the task is too easy.\n4 Experiments\nIn this section, we present comprehensive exper-\niments on a wide range of downstream tasks and\nprovide detailed ablation studies, to demonstrate\nthe effectiveness of the proposed approach to large-\nscale LM compression.\n4.1 Datasets\nWe evaluate the proposed approach on sentence\nclassiÔ¨Åcation tasks from the General Language Un-\nderstanding Evaluate (GLUE) benchmark (Wang\net al., 2018), as our Ô¨Ånetuning framework is de-\nsigned for classiÔ¨Åcation, and we only exclude the\nSTS-B dataset (Cer et al., 2017). Following other\nworks (Sun et al., 2019; Jiao et al., 2019; Sun et al.,\n2020), we also do not run experiments on WNLI\ndataset (Levesque et al., 2012), as it is very dif-\nÔ¨Åcult and even majority voting outperforms most\nbenchmarks.2\nCoLA Corpus of Linguistic Acceptability\n(Warstadt et al., 2019) contains a collection of 8.5k\nsentences drawn from books or journal articles.\nThe goal is to predict if the given sequence\nof words is grammatically correct. Mattthews\ncorrelation coefÔ¨Åcient is used as the evaluation\nmetric.\nSST-2 Stanford Sentiment Treebank (Socher\net al., 2013) consists of 67k human-annotated\nmovie reviews. The goal is to predict whether each\nreview is positive or negative. Accuracy is used as\nthe evaluation metric.\nMRPC Microsoft Research Paraphrase Corpus\n(Dolan and Brockett, 2005) consists of 3.7k sen-\ntence pairs extracted from online news, and the\ngoal to predict if each pair of sentences is seman-\ntically equivalent. F1 score from GLUE server is\nreported as the metric.\nQQP The Quora Question Pairs 3 task consists\nof 393k question pairs from Quora webiste. The\n2Please refer to https://gluebenchmark.com/leaderboard.\n3https://data.quora.com/First-Quora-Dataset-Release-\nQuestion-Pairs\nModel CoLA SST-2 MRPC QQP MNLI-m/-mm QNLI RTE Ave.(8.5k) (67k) (3.7k) (364k) (393k) (108k) (2.5k)\nRoBERTa-base (Ours) 60.3 95.3 91.0 89.6 87.7/86.8 93.5 71.7 84.5\nBERT-base (Google) 52.1 93.5 88.9 89.2 84.6/83.4 90.5 66.4 81.7\nDistilBERT 32.8 91.4 82.4 88.5 78.9/78.0 85.2 54.1 73.9\nSID 41.4 - 83.8 89.1 - - 62.2 -\nBERT6-PKD 24.8 92.0 86.4 88.9 81.5/81.0 89.0 65.5 76.0\nTinyBERT‚ãÜ\n4 43.3 92.6 86.4 89.2‚àó 82.5/81.8 87.7 62.9 76.7\nTinyBERT6 51.1‚àó 93.1 87.3 89.1 84.6/83.2 90.4 66.0 80.6\nMLM-Pre + Fine 50.6 93.0 88.7 89.2 82.9/82.0 89.6 62.1 79.8\nCoDIR-Fine 53.6 93.6 89.4 89.1 83.6/82.8 90.4 65.6 81.0\nCoDIR-Pre 53.7 94.1 89.3 89.1 83.7/82.6 90.4 66.8 81.2\nCoDIR-Pre + CoDIR-Fine 53.7 93.6 89.6 89.1 83.5/82.7 90.1 67.1 81.2\nTable 1: Results on GLUE Benchmark. (*) indicates those numbers are unavailable in the original papers and were\nobtained by us through submission to the ofÔ¨Åcial leaderboard using their codebases. Other results are obtained from\npublished papers. (‚ãÜ) indicates those methods with fewer Transformer blocks, and may not be fair comparison.\ntask is to predict whether a pair of questions is\nsemantically equivalent. Accuracy is used as the\nevaluation metric.\nNLI Multi-Genre Natural Language Inference\nCorpus (MNLI) (Williams et al., 2017), Question-\nanswering NLI (QNLI) (Rajpurkar et al., 2016)\nand Recognizing Textual Entailment (RTE)4 are\nall natural language inference (NLI) tasks, which\nconsist of 393k/108k/2.5k pairs of premise and\nhypothesis. The goal is to predict if the premise\nentails the hypothesis, or contradicts it, or neither.\nAccuracy is used as the evaluation metric. Besides,\nMNLI test set is further divided into two splits:\nmatched (MNLI-m, in-domain) and mismatched\n(MNLI-mm, cross-domain), accuracy for both are\nreported.\n4.2 Implementation Details\nWe mostly follow the pre-training setting from Liu\net al. (2019), and use the fairseq implementation\n(Ott et al., 2019). SpeciÔ¨Åcally, we truncate raw text\ninto sentences with maximum length of 512 tokens,\nand randomly mask 15% of tokens as [MASK].\nFor model architecture, we use a randomly initial-\nized 6-layer Transformer model as the student, and\nRoBERTa-base with 12-layer Transformer as the\nteacher. The student model was Ô¨Årst trained by\nusing Adam optimizer with learning rate 0.0007\nand batch size 8192 for 35,000 steps. For com-\nputational efÔ¨Åciency, this model serves as the ini-\ntialization for the second-stage pre-training with\n4Collections of series of annual textual entailment chal-\nlenges.\nthe teacher. Then, the student model is further\ntrained for another 10,000 steps with KD and the\nproposed contrastive objective, with learning rate\nset to 0.0001. We denote this model as CoDIR-Pre.\nFor ablation purposes, we also train two baseline\nmodels with only MLM loss or KD loss, using the\nsame learning rate and number of steps. Similarly,\nthese two models are denoted as MLM-Pre and\nKD-Pre, respectively. For other hyper-parameters,\nwe use Œ±1 = Œ±2 = 0.1 for both LKD and LCRD.\nDue to high computational cost for pre-training,\nall the hyper-parameters are set empirically without\ntuning. As there exist many combinations of pre-\ntraining loss (MLM, KD, and CRD) and Ô¨Ånetuning\nstrategies (standard Ô¨Ånetuning with cross-entropy\nloss, and Ô¨Ånetuning with additional CRD loss), a\ngrid search of all the hyper-parameters is infeasible.\nThus, for standard Ô¨Ånetuning, we search learning\nrate from {1e-5, 2e-5}and batch size from {16,\n32}. The combination with the highest score on\ndev set is reported for ablation studies, and is kept\nÔ¨Åxed for future experiments. We then Ô¨Åx the hyper-\nparameters in KD as œÅ= 2,Œ±1 = 0.7, and search\nweight of the CRD loss Œ±2 from {0.1, 0.5, 1}, and\nthe number of negative samples from {100, 500,\n1000}. Results with the highest dev scores were\nsubmitted to the ofÔ¨Åcial GLUE server to obtain\nthe Ô¨Ånal results. For fair comparison with other\nbaseline methods, all the results are based on single-\nmodel performance.\n4.3 Experimental Results\nResults of different methods from the ofÔ¨Åcial\nGLUE server are summarized in Table 1. For sim-\nPre-training Loss Finetuning Method CoLA SST-2 MRPC QNLI RTE Ave.(8.5k) (67k) (3.7k) (108k) (2.5k)\nLMLM Standard 55.4 92.0 88.0 90.2 67.1 78.5\nLMLM KD 57.8 92.4 89.5 90.3 68.2 79.4\nLMLM CoDIR-Fine 59.3 92.7 90.0 90.7 70.8 80.7\nLMLM Standard 55.4 92.0 88.0 90.2 67.1 78.5\nLMLM + LKD Standard 56.6 92.7 89.0 90.5 69.0 79.6\nCoDIR-Pre Standard 57.6 92.8 90.4 90.8 71.5 80.6\nTable 2: Ablation study on different combination of pre-trained models and Ô¨Ånetuning approach. The results are\nbased on GLUE dev set.\nModel #Trm Layers #Params #Params (Emb Layer) Inference Time (ms/seq) Speed-up\nBERT-base 12 109.5M 23.8M 2.60 1.00 √ó\nRoBERTa-base 12 125.2M 39.0M 2.53 1.03 √ó\nDistilBERT 6 67.0M 23.8M 1.27 2.05 √ó\nCoDIR-Pre 6 82.7M 39.0M 1.25 2.08 √ó\nTable 3: Inference speed comparison between teacher and students. Inference time is measured on MNLI dev set.\nSpeed up is measured against BERT-base, which is the teacher model for other baseline methods.\nplicity, we denote our baseline approach without us-\ning any teacher supervision as ‚ÄúMLM-Pre + Fine‚Äù:\npre-trained by using MLM loss Ô¨Årst, then Ô¨Ånetun-\ning using standard cross-entropy loss. Our baseline\nalready achieves high average score across 8 tasks,\nand outperforms task-speciÔ¨Åc model compression\nmethods (such as SID (Aguilar et al., 2019) and\nBERT-PKD (Sun et al., 2019)) as well as Distil-\nBERT (Sanh et al., 2019) by a large margin.\nAfter adding contrastive loss at the Ô¨Ånetuning\nstage (denoted as CoDIR-Fine), the model out-\nperforms the state-of-the-art compression method,\nTinyBERT with 6-layer Transformer, on average\nGLUE score. Especially on datasets with fewer\ndata samples, such as CoLA and MRPC, the im-\nproved margin is large (+2.5% and +2.1%, respec-\ntively). Compared to our MLM-Pre + Fine baseline,\nCoDIR-Fine achieves signiÔ¨Åcant performance gain\non almost all tasks (+1.2% absolute improvement\non average score), demonstrating the effectiveness\nof the proposed approach. The only exception is\nQQP (-0.1%) with more than 360k training exam-\nples. In such case, standard Ô¨Ånetuning may al-\nready bring in enough performance boost with this\nlarge-scale labeled dataset, and the gap between\nthe teacher and student networks is already small\n(89.6 vs 89.2).\nWe further test the effectiveness of CoDIR for\npre-training (CoDIR-Pre), by applying standard\nÔ¨Ånetuning on model pre-trained with additional\ncontrastive loss. Again, compared to the MLM-\nPre + Fine baseline, this improves the model per-\nformance on almost all the tasks (except QQP),\nwith a signiÔ¨Åcant lift on the average score (+1.4%).\nWe notice that this model performs similarly to\nthe contrastive-Ô¨Ånetuning only approach (CoDIR-\nFine) on almost all tasks. However, CoDIR-Pre is\npreferred because it utilizes the teacher‚Äôs knowl-\nedge in the pre-training stage, thus no task-speciÔ¨Åc\nteacher is needed for Ô¨Ånetuning downstream tasks.\nFinally, we experiment with the combination of\nCoDIR-Pre and CoDIR-Fine, and our observation\nis that adding constrastive loss for Ô¨Ånetuning is not\nbringing in much improvement after already using\nconstrastive loss in pre-training. Our hypothesis is\nthat the model‚Äôs ability to identify negative exam-\nples is already well learned during pre-training.\nInference Speed We compare the inference\nspeed of the proposed CoDIR with the teacher\nnetwork and other baselines. Statistics of Trans-\nformer layers and parameters are presented in Table\n3. The statistics for BERT6-PKD and TinyBERT6\nare omitted as they share the same model architec-\nture as DistilBERT. To test the inference speed, we\nran each algorithm on MNLI dev set for 3 times,\nwith batch size 32 and maximum sequence length\n128 under the same hardware conÔ¨Åguration. The\naverage running time with 3 different random seeds\nis reported as the Ô¨Ånal inference speed. Though\nour RoBERTa teacher has almost 16 million more\nparameters, it shares almost the same inference\nModel CoLA SST-2 MRPC QQP MNLI-m/-mm QNLI RTE Ave.\n(8.5k) (67k) (3.7k) (364k) (393k) (108k) (2.5k)\nCoDIR-Fine ([CLS]) 57.6 92.9 89.2 91.3 84.0/84.0 90.8 70.0 82.5\nCoDIR-Fine (Mean Pool) 59.3 92.7 90.0 91.3 84.2/84.2 90.7 70.8 83.0\nCoDIR-Fine (100-neg) 57.3 92.1 88.2 91.3 84.0/84.0 90.4 69.3 82.1\nCoDIR-Fine (500-neg) 58.2 92.5 89.7 91.2 84.0/84.0 90.6 70.8 82.6\nCoDIR-Fine (1000-neg) 59.3 92.7 90.0 91.3 84.2/84.2 90.7 70.4 83.0\nTable 4: Ablation study on the use of [CLS] and Mean-Pooling as sentence embedding (upper part) and effect of\nnumber of negative examples (neg) for CoDIR-Fine (bottom part). The results are based on GLUE dev set.\nModel CoLA SST-2 MRPC QQP MNLI-m QNLI RTE\n(8.5k) (67k) (3.7k) (364k) (393k) (108k) (2.5k)\nMedian 56.4 92.4 87.9 91.2 83.9 90.7 66.3\nMaximum 57.8 93.0 90.3 91.3 84.2 91.0 70.2\nStandard Deviation 1.46 0.28 1.66 0.06 0.43 0.18 1.41\nTable 5: Analysis of model variance on GLUE dev set. Statistical results (median, maximum, and standard devia-\ntion) are based on 8 runs with the same hyper-parameters.\nspeed as BERT-base, because its computational\ncost mainly comes from the embedding layer with\n50k vocabulary size that does not affect inference\nspeed. By reducing the number of Transformer\nlayers to 6, our proposed student model achieves\n2 times speed up compared to the teacher, and\nachieves state-of-the-art performance among all\nmodels with similar inference time.\n4.4 Ablation Studies\nSentence Embedding We also conduct experi-\nments to evaluate the effectiveness of using dif-\nferent sentence embedding strategies. More de-\ntailed, based on the same model pre-trained on\nLMLM alone, we run Ô¨Ånetuning experiments with\ncontrastive loss on the GLUE dataset by using:\n(i) [CLS] as sentence embedding; and (ii) mean-\npooling as sentence embedding. The results on\nGLUE dev set are presented in top rows of Table\n4, showing that mean-pooling yields better results\nthan [CLS] (83.0 vs. 82.5 on average). As a re-\nsult, we use mean pooling as our chosen sentence\nembedding for all our experiments.\nNegative Examples As we mentioned in Section\n4.2, the experiments are conducted using 100, 500\nand 1000 negative examples. We then evaluate the\neffect of number of negative examples by compar-\ning their results on GLUE dev set, and the results\nare presented in the bottom part of Table 4. Ob-\nviously, for most dataset the accuracy increases\nas a larger number of negative examples are used\nduring training. Similar observations were also re-\nported in Tian et al. (2019), and a theoretical anal-\nysis is provided in Arora et al. (2019). The only\ntwo exceptions are QQP and RTE. As discussed\nin Section 4.3, our CoDIR method seems also not\nwork well on QQP due to the small gap between\nteacher and student. As for RTE, due to the small\nnumber of training examples, the results are quite\nvolatile, which may make the results inconsistent.\nBesides, the number of negative examples is close\nto the number of examples per class (1.25k) for\nRTE, which can also result in the contrastive loss\nclose to 0.\nContrastive Loss We Ô¨Årst evaluate the effective-\nness of the proposed CRD loss for Ô¨Ånetuning on a\nsubset of GLUE dev set, using the following set-\ntings: (i) Ô¨Ånetuning with cross-entropy loss only;\n(ii) Ô¨Ånetuning with additional KD loss; and ( iii)\nÔ¨Ånetuning with additional KD loss and CRD loss.\nResults in Table 2 (upper part) show that using\nKD improves over standard Ô¨Ånetuning by 0.9% on\naverage, and using CRD loss further improves an-\nother 1.0%, demonstrating the advantage of using\ncontrastive learning for Ô¨Ånetuning.\nTo further validate performance improvement\nof using contrastive loss on pre-training, we apply\nstandard Ô¨Ånetuning to three different pre-trained\nmodels: ( i) model pre-trained by LMLM (MLM-\nPre); (ii) model pre-trained by LMLM + LKD (KD-\nPre); and (iii) model pre-trained by LMLM +LKD +\nLCRD (CoDIR-Pre). Results are summarized in Ta-\nble 2 (bottom part). Similar trend can be observed\nthat the model pre-trained with additional CRD\nloss performs the best, outperforming MLM-Pre\nand KD-Pre by 1.9% and 1.0% on average, respec-\ntively.\nModel Variance Since different random seeds\ncan exhibit different generalization behaviors, es-\npecially for tasks with a small training set ( e.g.,\nCoLA ), we examine the median, maximum and\nstandard deviation of model performance on the\ndev set of each GLUE task, and present the results\nin Table 5. As expected, the models are more sta-\nble on larger datasets (SST-2, QQP, MNLI, and\nQNLI), where all standard deviations are lower\nthan 0.5. However, the model is sensitive to the\nrandom seeds on smaller datasets (CoLA, MRPC,\nand RTE) with the standard deviation around 1.5.\nThese analysis results provide potential references\nfor future work on language model compression.\n5 Conclusion\nIn this paper, we present CoDIR, a novel approach\nto large-scale language model compression via the\nuse of contrastive loss. CoDIR utilizes information\nfrom both teacher‚Äôs output layer and its intermedi-\nate layers for student model training. Extensive ex-\nperiments demonstrate that CoDIR is highly effec-\ntive in both Ô¨Ånetuning and pre-training stages, and\nachieves state-of-the-art performance on GLUE\nbenchmark compared to existing models with a\nsimilar size. All existing work either use BERT-\nbase or RoBERTa-base as teacher. For future work,\nwe plan to investigate the use of a more powerful\nlanguage model, such as Megatron-LM (Shoeybi\net al., 2019), as the teacher; and different strategies\nfor choosing hard negatives to further boost the\nperformance.\nReferences\nGustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao,\nXing Fan, and Edward Guo. 2019. Knowledge distil-\nlation from internal representations. arXiv preprint\narXiv:1910.03723.\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail\nKhodak, Orestis Plevrakis, and Nikunj Saunshi.\n2019. A theoretical analysis of contrastive unsu-\npervised representation learning. arXiv preprint\narXiv:1902.09229.\nPhilip Bachman, R Devon Hjelm, and William Buch-\nwalter. 2019. Learning representations by maximiz-\ning mutual information across views. In NeurIPS.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. arXiv\npreprint arXiv:2002.05709.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing.\nMitchell A Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. arXiv preprint\narXiv:2002.08307.\nFu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin,\nand Yanzhi Wang. 2019. Reweighted proximal prun-\ning for large-scale language representation. arXiv\npreprint arXiv:1909.12486.\nMichael Gutmann and Aapo Hyv ¬®arinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In AISTATS.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2019. Momentum contrast for un-\nsupervised visual representation learning. arXiv\npreprint arXiv:1911.05722.\nOlivier J H ¬¥enaff, Aravind Srinivas, Jeffrey De Fauw,\nAli Razavi, Carl Doersch, SM Eslami, and Aaron\nvan den Oord. 2019. Data-efÔ¨Åcient image recog-\nnition with contrastive predictive coding. arXiv\npreprint arXiv:1905.09272.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-\nMarchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. 2018. Learn-\ning deep representations by mutual information\nestimation and maximization. arXiv preprint\narXiv:1808.06670.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020.\nSupervised contrastive learning. arXiv preprint\narXiv:2004.11362.\nNikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efÔ¨Åcient transformer. arXiv\npreprint arXiv:2001.04451.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. arXiv preprint arXiv:1908.08593.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In\nNeurIPS.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensi-\nble toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and\nKurt Keutzer. 2019. Q-bert: Hessian based ultra\nlow precision quantization of bert. arXiv preprint\narXiv:1909.05840.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP.\nS. Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. In EMNLP/IJCNLP.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert:\na compact task-agnostic bert for resource-limited de-\nvices. arXiv preprint arXiv:2004.02984.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciÔ¨Åc knowledge from bert into simple neural net-\nworks. arXiv preprint arXiv:1903.12136.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. arXiv\npreprint arXiv:1905.05950.\nYonglong Tian, Dilip Krishnan, and Phillip Isola.\n2019. Contrastive representation distillation. arXiv\npreprint arXiv:1910.10699.\nMichael Tschannen, Josip Djolonga, Paul K Ruben-\nstein, Sylvain Gelly, and Mario Lucic. 2019. On\nmutual information maximization for representation\nlearning. arXiv preprint arXiv:1907.13625.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTACL.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. 2020. Lite transformer with long-short range\nattention. arXiv preprint arXiv:2004.11886.\nZhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua\nLin. 2018. Unsupervised feature learning via non-\nparametric instance-level discrimination. arXiv\npreprint arXiv:1805.01978.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS.\nOÔ¨År Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert.\narXiv preprint arXiv:1910.06188.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In ICCV.",
  "topic": "Distillation",
  "concepts": [
    {
      "name": "Distillation",
      "score": 0.7915829420089722
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.773555338382721
    },
    {
      "name": "Computer science",
      "score": 0.7638803124427795
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5929902791976929
    },
    {
      "name": "Artificial intelligence",
      "score": 0.516537606716156
    },
    {
      "name": "Natural language processing",
      "score": 0.5033239722251892
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4737713634967804
    },
    {
      "name": "Compression (physics)",
      "score": 0.4623281955718994
    },
    {
      "name": "Sample (material)",
      "score": 0.4573918581008911
    },
    {
      "name": "Language model",
      "score": 0.4334481358528137
    },
    {
      "name": "Machine learning",
      "score": 0.3731106221675873
    },
    {
      "name": "Programming language",
      "score": 0.10477399826049805
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 5
}