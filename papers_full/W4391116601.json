{
  "title": "Exploiting Language Models as a Source of Knowledge for Cognitive Agents",
  "url": "https://openalex.org/W4391116601",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5108741024",
      "name": "James R. Kirk",
      "affiliations": [
        "Cambridge Cognition (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5011322251",
      "name": "Robert E. Wray",
      "affiliations": [
        "Cambridge Cognition (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5061159721",
      "name": "John E. Laird",
      "affiliations": [
        "Cambridge Cognition (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6680406137",
    "https://openalex.org/W6800166007",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6737933248",
    "https://openalex.org/W6850503672",
    "https://openalex.org/W6638593011",
    "https://openalex.org/W6850764995",
    "https://openalex.org/W4296415305",
    "https://openalex.org/W2966636162",
    "https://openalex.org/W4393147141",
    "https://openalex.org/W4292958252",
    "https://openalex.org/W2883445328",
    "https://openalex.org/W6614489337",
    "https://openalex.org/W4242555830",
    "https://openalex.org/W4385749697",
    "https://openalex.org/W6839092860",
    "https://openalex.org/W3192651000",
    "https://openalex.org/W2162227979",
    "https://openalex.org/W6672752436",
    "https://openalex.org/W3172480024",
    "https://openalex.org/W2177109876",
    "https://openalex.org/W4386114306",
    "https://openalex.org/W3130319171",
    "https://openalex.org/W6678087030",
    "https://openalex.org/W4386215566",
    "https://openalex.org/W4321011818",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3201464307",
    "https://openalex.org/W4376864573",
    "https://openalex.org/W6659346644",
    "https://openalex.org/W4287888476",
    "https://openalex.org/W4287523898",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4388720459",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4286973660",
    "https://openalex.org/W2136518234",
    "https://openalex.org/W4361866080",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W2088563966",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W1808725644",
    "https://openalex.org/W2035859722",
    "https://openalex.org/W2113272178",
    "https://openalex.org/W2614829311",
    "https://openalex.org/W419646410",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4401043020"
  ],
  "abstract": "Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.",
  "full_text": "Exploiting Language Models as a Source of Knowledge for Cognitive Agents\nJames R. Kirk, Robert E. Wray, John E. Laird\nCenter for Integrated Cognition @ IQMRI\n24 Frank Lloyd Wright Drive Suite H 1200, Box 464\nAnn Arbor, Michigan 48106\n{james.kirk,robert.wray,john.laird@cic.iqmri.org}\nAbstract\nLarge language models (LLMs) provide capabilities far be-\nyond sentence completion, including question answering,\nsummarization, and natural-language inference. While many\nof these capabilities have potential application to cognitive\nsystems, our research is exploiting language models as a\nsource of task knowledge for cognitive agents, that is, agents\nrealized via a cognitive architecture. We identify challenges\nand opportunities for using language models as an external\nknowledge source for cognitive systems and possible ways\nto improve the effectiveness of knowledge extraction by in-\ntegrating extraction with cognitive architecture capabilities,\nhighlighting with examples from our recent work in this area.\nIntroduction\nCognitive architectures1 (Anderson et al. 2004; Laird 2012;\nKotseruba and Tsotsos 2020; Newell 1990) are foundational\ntools for research toward the realization of cognitive capabil-\nities. Architectures also facilitate development of cognitive\nsystems (“agents”) that manifest various integrated cogni-\ntive capabilities, including planning, learning, plan execu-\ntion, and many others. An agent uses these capabilities in\nconcert to act, to perform sophisticated tasks, and to achieve\nshort- and long-term goals.\nA key limitation of scaling agents to ever larger and more\ncomplex tasks and applications is their ability to acquire and\nintegrate new task knowledge. As a consequence, a signif-\nicant thrust of applied cognitive architecture research over\nthe years has been exploring various knowledge engineering\n(Yost 1993; Crossman et al. 2004; Ritter et al. 2006), expe-\nriential learning (Nejati, Langley, and Konik 2006; Choi and\nLangley 2018; Pearson and Laird 1998), and instructional\n(Huffman and Laird 1995; Gluck, Laird, and Lupp 2019)\napproaches intended to mitigate the costs of acquiring new,\neffective, and robust task knowledge. While improvements\nhave been achieved, none of these approaches has resulted in\nroutine, low-cost, large-scale knowledge resources for cog-\nnitive agents.\nIn contrast, Large Language Models (LLMs) (OpenAI\n2023; Driess et al. 2023) provide a huge breadth of poten-\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n1Portions of this paper draw from and update Wray, Kirk, and\nLaird (2021).\ntial knowledge. However, exploiting this knowledge is chal-\nlenging because the production of knowledge is unreliable\nand untrustworthy (Lenat and Marcus 2023). As a knowl-\nedge source, LLM responses can be corrupted by hallucina-\ntion, irrelevancy, incorrectness and can be unethical and/or\nunsafe. Further, for most applications of LLMs, the models\nthemselves are fixed: they do not learn or adapt to the spe-\ncific situations in which they are used.\nWe are exploring the hypothesis that an integration of\nLLMs and cognitive architectures can offset the limitations\nof each in the context of reliable knowledge scaling. LLMs\ncan be used as a rich, (largely) low-cost source of knowledge\nfor agents to extend their task knowledge. In concert, agent\ncognitive capabilities, realized within a cognitive architec-\nture, can be applied to the LLM to improve the “precision”\nand correctness of the task knowledge that an LLM is asked\nto produce. This approach emphasizes what cognitive archi-\ntectures do best (support end-to-end integration of interac-\ntion, reasoning, language processing, learning, etc., using\nstructured, curated knowledge) and what language models\ndo best (provide associational retrieval from massive stores\nof latent unstructured, possibly unreliable knowledge). We\nexpect that by using LLMs to acquire new task knowledge,\nagents can reduce their reliance on more costly sources of\ntask knowledge (e.g., extensive training, human instruction,\nexplicit knowledge engineering) and, consequently, scale\nmore readily to larger task domains and applications.\nIn this paper, we discuss the dimensions of this problem,\nrequirements for a solution, and some examples of some in-\nteractions between LLMs and cognitive architectures that\nappear important for task acquisition.\nPotential Patterns of Integration\nWe introduce three different ways cognitive-architecture-\nLLM integration could be realized for acquiring task knowl-\nedge, and we outline potential benefits and costs of each. 2\nThe alternatives are illustrated in Figure 1. In the diagrams,\nan agent is viewed as a combination of its (cognitive) archi-\ntecture and content (task knowledge), the standard formula-\ntion of an agent (Russell and Norvig 1995). While this struc-\n2This list is not comprehensive and other options for consider-\ning LLMs as source of knowledge for cognitive agents are feasible;\ne.g., see Lenat and Marcus (2023) for an alternative list.\nAAAI Fall Symposium Series (FSS-23)\n286\nture (over-)simplifies the details of most cognitive agents,\nit is a reasonable approximation for considering systems-\nintegration alternatives.\nIn the figure, we assume LLMs (and in 1a, external knowl-\nedge repositories) as sources of general or unspecialized\nknowledge. In some cases, for a particular task, domain-\nor task-specific knowledge bases may be available or an\nLLM that has been fine-tuned for a specific problem domain.\nWhen these are available, an agent should generally attempt\nto use them. However, we are more focused on the case of a\ngeneral intelligent agent where the agent (or developer) can-\nnot assume that a special-purpose knowledge source is read-\nily available. In this case, the agent generally will need to\nattempt to extract knowledge from more general resources,\nlike an LLM or a world-knowledge repository such as Con-\nceptNet (Speer, Chin, and Havasi 2017).\nThe three integration patterns we consider are:\n• (a) Indirect extraction: In this option, general extrac-\ntion processes designed for LLMs are used and responses\nfrom an LLM are placed in a knowledge store. The agent\nthen accesses the knowledge store to obtain task knowl-\nedge. For example, general knowledge extraction from\nLLMs is being developed to populate knowledge graphs\n(Bosselut et al. 2019) which is a specific example of the\nuse of LLMs as a knowledge base (Petroni et al. 2019).\nAs a consequence, this alternative leverages that ongo-\ning research. Further, any existing cognitive agent capa-\nbilities that draw and exploit external knowledge stores\ncould also be re-purposed for exploring this alternative\n(Wray, Kirk, and Laird 2021).\n• (b) Direct extraction: In this option, the agent directly\nformulates and sends queries to the LLM and then in-\nterprets the responses it receives. The responses are in-\nterpreted and internalized within with agent’s process-\ning, resulting in situation-specific learning of new task\nknowledge (represented by the arrow from the cognitive\narchitecture to task knowledge components). This option\nrequires that the agent encode capabilities that perform\nthe same kinds of processes needed for indirect extrac-\ntion (i.e., the blue “extraction” process in 1a, but within\nits own agent knowledge).\n• (c) Direct Knowledge Encoding: This option provides\n“direct wiring” for the agent, encoding task knowl-\nedge directly into the agent’s internal knowledge repre-\nsentations and memories. For instance, using program-\ncode generation capabilities of LLMs (Chen et al. 2021;\nAustin et al. 2021), researchers have shown that LLMs\ncan be used to create programs for robotic control of em-\nbodied agents (Singh et al. 2023; Brohan et al. 2023).\nBecause cognitive architectures generally provide pro-\ngrammable interfaces, code generation might be apt for\ncognitive-architecture agents. This option could thus em-\nploy an external extraction process that programmed\nagent knowledge directly. However, while these extrac-\ntion processes might be able to build on existing extrac-\ntion methods, the extraction processes would need to be\nspecialized for cognitive architectures (and likely for in-\ndividual architectures).\nFigure 1: Possible approaches to CA-LLM integration in\nsupport of task-knowledge acquisition.\nOur work to-date focuses on direct extraction. We chose\ndirect extraction over indirect extraction for two reasons.\nFirst, in indirect extraction (1a), task-knowledge extraction\nis not tied to the agent’s specific, current knowledge needs.\nWe hypothesize that direct extraction enables the agent to\nuse its specific situation and context (including its embod-\niment) to query the LLM resulting in improved precision\n(relevance to the specific situation). Second, because extrac-\ntion processes not tied to the agent cannot fully anticipate\nagent needs, the knowledge base in 1a cannot be assumed to\nbe an exhaustive resource for all the knowledge that could\nbe extracted from the LLM. Thus, even an agent that used\n1a might need to sometimes resort to processes akin to 1b\nwhen the knowledge base was found lacking.\nThe primary potential downside of the direct extraction\napproach is that the agent must have the ability to man-\n287\nage the potential challenges of extraction from the LLM it-\nself (e.g., determining whether some response is plausible)\nrather than depending on specialized extraction processes\n(as in 1a). In other words, the agent must have more so-\nphisticated capabilities to directly query and interpret LLM\nresponses.\nDirect encoding is another option that we recommend be\nexplored in the community, although we do not discuss it\nfurther here. Because task specification languages and tools\nhave already been developed (for both general AI systems\nand cognitive agents), it may be feasible to leverage the code\ngeneration capabilities of LLMs to generate executable task\nknowledge. Fine-tuning a code generation model with exam-\nples that illustrate how task capabilities are realized within\nthe distinct representational and memory systems of cogni-\ntive architectures is likely to result in better results than in-\ncontext learning because the execution models of cognitive\narchitectures often differ substantially from traditional pro-\ngramming languages.\nDirect Extraction:\nChallenges and Opportunities\nKnowledge extraction is the process by which an agent gains\nknowledge of its task and/or environment from an external\nknowledge source. Successful extraction results in the agent\nhaving (new) knowledge it can bring to bear on its tasks.\nFrom a cognitive-systems perspective, what is important is\nthat the knowledge produced by an extraction process is “ac-\ntionable” by the agent. Thus, the goal of extraction is not\nsimply to add knowledge but to add knowledge that allows\nthe agent to improve its ability to function as an autonomous\nentity in a multitask environment.\nDirect extraction requires that an agent directly interact\nwith an LLM (send queries, receive responses) rather than\nthrough an intermediary. In this section, we characterize\nsome of the features of LLMs relevant to direct extraction\nand, in some cases, contrast those extraction processes with\nextraction from more curated knowledge sources (e.g., Cyc,\nWordNet, ConceptNet). Relevant features include:\n• Breadth and Depth of Knowledge: A major strength of\nLLMs, such as GPT3, GPT4 or PALM-E, compared to\ncurated knowledge bases (KBs) is their extensive breadth\nof encoded knowledge.\n• Provenance and Accuracy of Knowledge: The quality of\nbehavior for a cognitive agent is invariably tied to the\nquality of knowledge it reasons with. With traditional\nKBs, task knowledge is typically either curated or at least\nderived from the agent’s own experience with the world.\nIn contrast, LLMs are (largely) derived from uncurated\nweb resources, and the knowledge’s provenance is un-\nknown and very likely includes errors and conflicts.\n• Relevance of Knowledge: Even when the LLM contains\nthe knowledge relevant to the agent’s needs, extracting\nit can be highly sensitive to the specific query sent to\nthe LLM (Pezeshkpour and Hruschka 2023). In addition\nto the basic question, a query to an LLM will often in-\nclude additional context (Reynolds and McDonell 2021),\nrelated examples (Brown et al. 2020), and additional in-\nstructions, such as “show your reasoning steps” (Wang\net al. 2023). The resulting responses are highly dependent\non all these factors. This sensitivity to the query context\nmakes it difficult to ensure that whatever information is\nretrieved from the LLM is actually relevant to the context\nof the agent.\n• Situatedness of Knowledge: Curated knowledge bases\n(such as Cyc) and LLMs encode “ungrounded” knowl-\nedge about the world. As general-purposes resources,\nthey do not encode knowledge about an agent’s current\nsituation, its embodiment (what it can sense and how\nit can act), and its goals and plans, which may be en-\ncoded in an agent’s long-term semantic or episodic mem-\nory. Thus, an open question is the extent to which the\nagent can embed relevant information from its under-\nstanding of its situation to obtain knowledge that can be\nconnected or “grounded” to its performance context. As\nmentioned above, the potential disconnect between the\ngeneral knowledge of an LLM and the specific knowl-\nedge needs of an agent motivates the exploration of direct\nextraction methods in particular.\n• Accessibility of Knowledge: In a typical AI knowledge\nbase, the APIs for query/response and knowledge repre-\nsentation are well-defined, making it straightforward for\nan agent to attempt to retrieve information and to parse\nany responses. For an LLM, the specific form of a request\nand response are (generally) less structured, e.g., natural-\nlanguage sentences. While tools such as LangChain’s\noutput parser3 are designed to bridge accessibility issues,\nit remains a challenge for an agent to interpret responses\nfrom the LLM; in the extreme, an agent must parse natu-\nral language to extract what information is provided in a\nresponse.\n• Structural Integration: Traditional knowledge bases im-\npose low to moderate computational costs and latency\nand have reliable access. In contrast, many LLMs, es-\npecially the largest, are web resources with restricted ac-\ncess, high relative latency, and access depends on internet\nconnectivity. Many LLMs charge by the token sent and\nreceived, imposing direct economic constraints on effi-\ncient and robust interaction. A cognitive agent will thus\nneed to be strategic in using an LLM in applications that\ninvolve real-time environmental and human interaction.\nRequirements and Measures\nFor our research, we have pursued a strategy that prioritizes\nthe issues of Accuracy, Relevance, Situatedness, and Acces-\nsibility from the previous section. For direct extraction to\nbe practical, an embodied agent needs methods that allow\nit to elicit responses from the LLM in a manner that pro-\nduces highly relevant ones (when an LLM has the capacity\nto potentially produce almost anything), that are appropriate\ngiven the situation, and that the agent can access or inter-\npret (which is required for any internalization and use in ac-\ntual task execution). Here, we further define and refine four\n3https://github.com/langchain-ai/langchain\n288\nrequirements (Kirk, Wray, and Lindes 2023) that an agent\nmust meet to successfully extract actionable task knowledge\nfrom an LLM. Specifically, responses from the LLM must\nbe:\n1. Interpretable: To use responses, the agent must be able\nto parse and understand them. Responses that are rep-\nresented as code can often be directly executed by an\nagent; e.g., JSON, first-order logic (Olmo, Sreedharan,\nand Kambhampati 2021), action commands as in RT2\n(Brohan et al. 2023) or even Python programs (Singh\net al. 2023). More typically, natural language (NL) re-\nsponses from an LLM require NLP capabilities that the\nagent can use to interpret those responses. Because re-\nsponses from the LLM must be interpretable given the\nagent’s existing capabilities (as above), the agent’s query\nconstruction process must attempt to guide LLM re-\nsponses so that the resulting responses conform to what-\never those native agent language capabilities are.\n2. Groundable to Situation: As above, an agent must de-\ntermine if/how it can situate an LLM response to its cur-\nrent environment. Objects, properties, relations, actions,\netc. referenced in the response must be connected to the\nagent’s situation. In the simplest case, groundedness re-\nquires the agent to map from perceptions to responses.\n3. Compatible with Affordances and Embodiment: The\nLLM response must also be compatible with the agent’s\nembodiment and the affordances known to the agent. For\nexample, an agent might be embodied in a robot with a\nsingle arm or as a phone app acting as a personal assis-\ntant. When the affordances or embodiment of the agent\nare not comparable to those of humans, eliciting com-\npatible responses may be particularly challenging for an\nLLM trained on a corpus (typically and primarily) de-\nscribing human activities that presume human embodi-\nment and affordances.\n4. Match Human Expectations: Users will have differ-\ning preferences about how tasks or actions should be\nperformed and what behavior is appropriate for differ-\nent situations (Kirk et al. 2022). This criterion is impor-\ntant because it is impossible for the LLM (alone) to pro-\nvide responses that ensure a match to human preferences.\nFor example, imagine a household robotic agent tasked\nwith putting away groceries. Should a can of beans be\nstored in a cupboard or a pantry? Either answer is plau-\nsible (Lenat and Marcus 2023). However, in some spe-\ncific home, there is likely a clear preference for one loca-\ntion for the beans to be stored. The resulting requirement,\nfrom the point of view of extraction, is that an agent must\nanticipate the need to elicit human preferences because\nthe LLM cannot disambiguate between plausible alterna-\ntives when the user has a preference for one or another.\nThe first three requirements are related to the viability of\nextracted task knowledge; in other words, is the agent able\nto interpret, internalize, and use a response in service of a\ntask? The fourth requirement derives from constraints that\narise from particular task environments: responses should be\naligned with human expectations.\nFigure 2: Schematic process for direct extraction of task\nknowledge for task learning in a cognitive agent.\nAs above, the threshold for the fourth requirement is that\nthe responses produced by the LLM must be plausible. A re-\nsponse that was viable, but that suggested that the can beans\nfrom the previous example should be stored in the sink or\ndish rack is very likely to be wrong. For our research, the\nmore important criterion is that the response should be not\njust reasonable or plausible but also responsive and relevant\nto a specific human user’s expectations for that situation.\nAgents must evaluate LLM responses to identify if re-\nsponses meet these requirements. When these requirements\nare not met, if the agent attempts to use them (as is), the\nresult will be incorrect learning or failure.\nThese requirements also suggest how we can, as re-\nsearchers, measure and evaluate progress toward direct ex-\ntraction. What fraction of responses were viable when at-\ntempting to extract knowledge in support of a new task? For\nthose that were not viable, what fraction failed due to incom-\npatibility with affordances vs. interpretability, etc? Measur-\ning performance on these kinds of questions can facilitate\nthe evaluation of progress and comparison of alternative ap-\nproaches.\nSupporting Direct Extraction\nIn this section, we briefly summarize our high-level ap-\nproach to direct extraction. We have used this approach to\nenable agents to learn new tasks, demonstrating the feasibil-\nity of the direct extraction approach (Kirk et al. 2022; Kirk,\nWray, and Lindes 2023; Kirk et al. 2023).\nFigure 2 illustrates the basic process. Rather than extract-\ning knowledge for only the sake of gaining new knowledge,\nthe cognitive agent seeks to extract knowledge for a specific\npurpose: enabling the agent to perform a (current or antici-\npated) task. Our extraction strategy takes advantage of prior\nwork in cognitive systems in accessing and using external\nknowledge sources (some of these steps are also used for\ninternal knowledge access).\nSteps that overlap with general extraction patterns are il-\nlustrated in green; process steps specific to LLMs are in blue.\nOur current implementations use an existing agent (Kirk and\nLaird 2019; Mininger 2021) that learns from human instruc-\ntion. These components provide general extraction capabili-\nties that can be employed for LLMs (i.e., the agent is already\n“extracting” knowledge from a human instructor via natural-\nlanguage interaction). In what follows, we refer to this agent\nand its capabilities as the “Interactive Task Learning Agent”\n(or ITL Agent). We have extended this ITL Agent, which\npreviously obtained new knowledge from natural-language\ninteraction with a user, to now also extract knowledge thru\ninteraction with an LLM. Roughly, the extraction process\ncomprises the following six steps:\n289\n1. The agent identifies a knowledge need, such as a gap in\nits knowledge. The ITL agent already has existing ca-\npabilities to detect knowledge gaps (including assessing\nwhat kinds of gaps it has encountered, such as needing\na goal or action, not understanding a term, etc.) In our\ncurrent work, we assume that the agent seeks the LLM\nfirst as a resource for potentially providing the ability to\nbridge these knowledge gaps and then relies on human\ninput only when the LLM is insufficient. (Longer term,\ncognitive agents will likely need to evaluate if/when an\nLLM is an appropriate source for a specific gap.)\nImportantly, from the point of view of the integration\npatterns described in Section 2, the agent’s fine-grained\nidentification of a gap or need makes direct extraction\nhighly relevant to resolving the gap. The agent can use\nits specific need to develop a more precise and targeted\nquery to the LLM. We employ a template-based prompt-\ning approach, a common method in prompt engineering\n(Reynolds and McDonell 2021). Templates are focused\non specific gaps that the agent may encounter (Wray,\nKirk, and Laird 2021), such as eliciting the steps needed\nto perform a task (Kirk et al. 2022), eliciting task sub-\ngoals, such as putting away the can of beans as part of\ntidying the kitchen (Kirk et al. 2023), and refining a prior\nresponse (Kirk, Wray, and Lindes 2023).\n2. The agent prompts an LLM, choosing a specific prompt\ntemplate based on its evaluation of its knowledge\ngap/need. It instantiates the template with information/-\ndata it has from the situation and context, and sets pa-\nrameters to the LLM given the situation (e.g., setting the\ntemperature or “variability” in the LLM response). Thus,\nthe resulting, agent-constructed prompt is not only spe-\ncific to the context but also the particular type of knowl-\nedge gap. (We further detail this approach to prompting\nin a subsequent section.)\n3. The agent then interprets the response(s) from the LLM.\nAlthough we did briefly explore having an LLM generate\nlogical expressions (Kirk et al. 2022), interpretation for\nour approach requires the agent to use its internal natural-\nlanguage understanding capabilities to convert the LLM\nresponse text to the agent’s internal knowledge repre-\nsentation. Because the ITL Agent already has natural-\nlanguage processing (NLP) capabilities for learning from\nhuman instruction, the extended approach leverages that\nexisting NLP capability. However, this choice also lim-\nits what the agent can interpret because the ITL Agent’s\nNLP capabilities are fairly modest. To date, one of the\nprimary pain points in our explorations has been deter-\nmining if/when an interpretation problem should be re-\nsolved by further refining LLM responses to match our\nagent’s NLP capabilities or whether to change/refine the\nexisting NLP capabilities.\n4. Because the results from the LLM are not necessarily\naccurate and reliable (as noted above), the agent evalu-\nates, tests, and attempts to verify extracted results from\nthe LLM. Although verification of acquired knowledge\nhas been explored in cognitive systems, verification of\nknowledge derived from LLMs presents new challenges.\nFor example, in the ITL Agent, we assumed that the hu-\nman instructor would provide accurate, grounded, inter-\npretable knowledge to the agent that reflected the instruc-\ntor’s preferences. None of these assumptions are likely to\nconsistently hold for a response from an LLM, resulting\nin verification processes that must address all the require-\nments introduced above. We have developed a novel,\nmulti-step approach to verification, detailed further be-\nlow.\n5. Following verification, the agent encodes the knowledge\nit has obtained into its own memory(-ies) as appropriate\nfor current and future use in task performance. In the ITL\nAgent, this encoding is accomplished via a 2-step pro-\ncess. First, the agent takes the step(s) indicated by a ver-\nified LLM response (such as planning to achieve a goal\nor executing an action), allowing the agent to determine\nin practice (in addition to the analytic assessment dur-\ning verification) if the new knowledge suits the task. A\nsecond step involves the agent deliberately reviewing the\nsteps of the task (“retrospection”). Using Soar’s chunk-\ning process (Laird, Rosenbloom, and Newell 1986), the\nretrospective analysis is compiled into new procedural\nknowledge that enables the agent to perform the same\nstep/task in the future without resorting to the LLM.\nWhile other approaches to encoding could be realized\n(even others within Soar), this approach, first developed\nfor the ITL Agent, is sufficient for encoding new knowl-\nedge that it derives from the LLM.\n6. In the final step, the agent uses the knowledge it has ac-\nquired and continues to monitor its correctness and util-\nity, and refines it based on its experience in using it.\nAlthough it is likely that knowledge could be incom-\nplete, over-general, etc., to date, we have not encountered\nsignificant problems that require downstream refinement\nand thus have not yet explored if LLMs present unique\nnew requirements for this step in the process.\nPrompting Approach\nThe agent must choose a prompting strategy and then formu-\nlate a specific prompt appropriate for its task and environ-\nment, the knowledge needed, and the requirements of the\nLLM itself. Prompt engineering (Reynolds and McDonell\n2021), crafting and refining prompts so that they produce\ndesired results, has been shown to be an effective strategy\nfor retrieving reasonable knowledge. Using template-based\nprompting, one kind of prompt engineering, an agent selects\nan appropriate template designed to elicit the desired knowl-\nedge and then instantiates the template with context specific\nto the task it is attempting to learn. 4 To date, we have used\na template-based approach, although we have only had to\ndevelop a few templates, as shown in Table 1.\nWe combine template-based prompting with few-shot\nprompting (Brown et al. 2020). Few-shot prompting embeds\nexamples of desired responses in the prompt. Prompt exam-\nples include similar queries and the respective desired re-\nsponse to influence the LLM’s response to the main prompt.\n4This approach is used so routinely that it is now directly sup-\nported in LLM development tools such as LangChain.\n290\nTemplate Description Example\nGoal Request a goal for a given task, situation,\nand object of focus.\nAction Request the next action step given a task,\nsituation, object of focus, and steps thus far.\nRepair Request a re-formulation of a response,\ngiven the previous prompt, the failed re-\nsponse, and a categorization of the type of\nfailure.\nTable 1: Examples of templates used in template-based\nprompting.\n(EXAMPLES)\n(TASK)Task name: store object.\nTask context: I am in mailroom. Aware of package\nof office supplies; package is in mailroom.\n(RESULT) The goal is that the package is in the closet\nand the closet is closed.(END RESULT)\n(END TASK)\n(TASK)Task name: deliver package.\nTask context: I am in mailroom. Aware of package\naddressed to Gary; package is in mailroom.\n(RESULT)The goal is that the package is in Gary’s office.\n(END RESULT)\n(END TASK)\n(END EXAMPLES)\n(TASK)Task name:tidy kitchen.\nTask context: I am in kitchen.\nAware of mug in dish rack.\n(RESULT)\nTable 2: Example of an agent-created prompt for eliciting\ngoals. Agent instantiations in the prompt from its situational\ncontext are highlighted in bold.\nOne of the main roles examples play in our approach is bi-\nasing responses toward simple and direct language that the\nITL Agent’s NLP interpreter can parse. Prompt examples\ncan also introduce patterns for the LLM to follow, such as\nin Chain of Thought (Wei et al. 2022), which influence the\nLLM to reason about problems step by step.\nThis discussion provides a high-level outline of our\ntemplate-based prompting approach which is detailed else-\nwhere (Kirk et al. 2022). Table 2 presents an example of a\nprompt constructed by the agent for a task to “tidy a kitchen”\nin which the agent is looking at a mug in a dish rack in that\nkitchen.5 In our work to-date, we have primarily used GPT-3\n(Brown et al. 2020) as the LLM for research. From the point\nof view of direct extraction, the relatively simple approach\nenables the agent to construct prompts that effectively elicit\nmostly interpretable (and viable) responses.\n5The indentation is for human readability alone; the prompt is\nconstructed without line breaks.\nFigure 3: Agent analysis of LLM responses via internal sim-\nulation\nVerification Approach\nAnother burgeoning research area is identifying effective\ntools to verify the responses of LLMs. These include rank-\ning responses from the LLM based on interaction with and\nfeedback from the environment Logeswaran et al. (2022), re-\nsponse sampling (Wang et al. 2023), using planning knowl-\nedge (Valmeekam et al. 2023), additional LLM prompting\nabout the veracity of retrieved responses (Kim, Baldi, and\nMcAleer 2023), and using human feedback/annotation (Wu\net al. 2023; Kirk et al. 2022).\nCognitive architectures provide both a framework for the\nevaluation of LLM responses and the knowledge (encoded\nin various memories) required to analyze them. As outlined\nbriefly above, our agent simulates the process of learning\nfrom a response in order to evaluate the result of using that\nresponse (Kirk, Wray, and Lindes 2023).\nFigure 3 summarizes the primary components of this anal-\nysis and the relevant memories from the cognitive architec-\nture that the agent relies on for analysis. The agent uses its\nNLP parser and linguistic knowledge encoded in semantic\nmemory to evaluate if the response is interpretable by the\nagent (1, orange). It uses knowledge of the current situation\n(encoded in working memory) to ground references in the\nLLM response and to evaluate if any references cannot be\ngrounded (2, green). Finally, it uses knowledge encoded in\nsemantic memory, and the context of the current environ-\nment from working memory, to analyze if responses align\nwith its embodiment and affordances (3, blue), evaluating if\nthe task goal is achievable by the agent.\nOnce the analysis is complete, any issues that are iden-\ntified can be used in subsequent prompts to repair the re-\nsponses that are misaligned with these requirements. The\nrepair template was (outlined in Table 1) is comparable to\nthe prompt shown in Table 2 but adds the incorrect LLM re-\nsponse, the identified issue (e.g., a word that is unknown),\nand asks for another response.\n291\nFigure 4: Categorization of responses retrieved from the\nLLM during agent experiment.\nA final strategy for evaluation is enabling human over-\nsight by asking a user if a task action or goal is correct before\nthe agent uses the response. Human evaluation enables the\nagent to conform to the final requirement, aligning with hu-\nman expectations. Correct task performance often requires\neliciting individual human preferences, as discussed above.\nFigure 4 shows an analysis of responses extracted from\nthe LLM by our agent during an experiment where it learns\nto tidy a kitchen with 35 common kitchen objects. The\nchart shows the (human-determined) classification of all the\nresponses retrieved from the LLM, including unviable re-\nsponses in red (not aligned with the first three requirements),\nviable but not reasonable responses in orange, reasonable\nresponses in yellow, and situationally relevant responses in\ngreen that match the human preferences for this task. A take-\naway from this analysis is the large percentage of total re-\nsponses (over 70%) that are not viable for the embodied\nagent, indicating the necessity for evaluation of responses\nfor reliable learning. In other words, this iterative prompt-\nrefine-re-prompt approach to verification allows the agent to\ngenerate and to identify the relatively small proportion of\nresponses that are viable and situationally relevant, resulting\nin “actionable” knowledge for the agent.\nFurther evaluation of responses using other capabilities\nof cognitive architectures is potentially useful, but not yet\nexplored. A cognitive architecture agent could use episodic\nmemory to see if retrieved knowledge matches actions per-\nformed in the past. It could also use planning knowledge to\nsee if retrieved goals are achievable, or retrieved actions are\nexecutable. Cognitive architectures also support interfacing\nwith other knowledge sources (e.g., knowledge bases such\nas WordNet or ConceptNet) which could provide additional\ninformation for evaluation (e.g., finding synonyms for un-\nknown words).\nConclusion\nAutonomous systems, whether they are realized with cogni-\ntive architectures or not, will have to acquire new knowledge\nto perform tasks and accomplish their goals. However, the\nlack of reliable, scalable acquisition of new task knowledge,\nespecially online acquisition of knowledge, has limited the\noperation and impact of cognitive systems. The integration\nof LLMs with cognitive architectures presents an intriguing\nopportunity to exploit the breadth of knowledge in LLMs to\novercome limits on knowledge acquisition.\nIn this paper, we presented various ways one might ap-\nproach this problem and highlighted the potential of direct\nextraction from LLMs as an integration path. We summa-\nrized the challenges and requirements for exploring this inte-\ngration and a high-level, step-wise process for pursuing this\ngoal. We outlined some of the ways we are attempting to\npursue this research vision, highlighting the use of template-\nbased prompting and knowledge-driven evaluation that en-\nables more reliable and useful responses from the LLM.\nA more complete realization of the entire task-learning\npipeline (as envisioned in Figure 2), as well as an evaluation\nof the pipeline in terms of scaling for knowledge acquisi-\ntion, remain as future work. One notable result in terms of\nscaling, however, has been to observe the synergistic inter-\nactions between different sources of knowledge within task\nlearning. The extended ITL Agent uses look-ahead planning,\nhuman oversight, and the LLM to attempt to acquire new\nknowledge. Early results (Kirk et al. 2023) suggest that plan-\nning can virtually eliminate the need for an agent to ask for\nactions (at least in the task domains we have explored) when\nthe agent acquires a correct (i.e., verified) goal description.\nSimilarly, using LLMs to elicit goals in conjunction with the\nverification process requires significantly less human over-\nsight. In summary, this integrated-knowledge approach real-\nized within and enabled by a cognitive architecture, is sug-\ngestive of a potential breakthrough in knowledge acquisition\nand task learning for cognitive agents.\nAcknowledgments\nThis work was supported by the Office of Naval Research,\ncontract N00014-21-1-2369. The views and conclusions\ncontained in this document are those of the authors and\nshould not be interpreted as representing the official policies,\neither expressed or implied, of the Department of Defense\nor Office of Naval Research. The U.S. Government is autho-\nrized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation hereon.\nReferences\nAnderson, J. R.; Bothell, D.; Byrne, M. D.; Douglass, S.;\nLebiere, C.; and Qin, Y . 2004. An integrated theory of the\nmind. Psychological review, 111(4): 1036.\nAustin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski,\nH.; Dohan, D.; Jiang, E.; Cai, C.; Terry, M.; Le, Q.; and\nSutton, C. 2021. Program Synthesis with Large Language\nModels. ArXiv:2108.07732 [cs].\nBosselut, A.; Rashkin, H.; Sap, M.; Malaviya, C.; Celikyil-\nmaz, A.; and Choi, Y . 2019. COMET: Commonsense Trans-\nformers for Automatic Knowledge Graph Construction. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics. ArXiv: 1906.05317.\nBrohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y .; Chen,\nX.; Choromanski, K.; Ding, T.; Driess, D.; Dubey, A.; Finn,\n292\nC.; Florence, P.; Fu, C.; Arenas, M. G.; Gopalakrishnan, K.;\nHan, K.; Hausman, K.; Herzog, A.; Hsu, J.; Ichter, B.; Irpan,\nA.; Joshi, N.; Julian, R.; Kalashnikov, D.; Kuang, Y .; Leal, I.;\nLee, L.; Lee, T.-W. E.; Levine, S.; Lu, Y .; Michalewski, H.;\nMordatch, I.; Pertsch, K.; Rao, K.; Reymann, K.; Ryoo, M.;\nSalazar, G.; Sanketi, P.; Sermanet, P.; Singh, J.; Singh, A.;\nSoricut, R.; Tran, H.; Vanhoucke, V .; Vuong, Q.; Wahid, A.;\nWelker, S.; Wohlhart, P.; Wu, J.; Xia, F.; Xiao, T.; Xu, P.; Xu,\nS.; Yu, T.; and Zitkovich, B. 2023. RT-2: Vision-Language-\nAction Models Transfer Web Knowledge to Robotic Con-\ntrol. ArXiv:2307.15818 [cs].\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M. F.; and Lin, H., eds., Advances in\nNeural Information Processing Systems, volume 33, 1877–\n1901. Curran Associates, Inc.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;\nKaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brock-\nman, G.; Ray, A.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf,\nH.; Sastry, G.; Mishkin, P.; Chan, B.; Gray, S.; Ryder, N.;\nPavlov, M.; Power, A.; Kaiser, L.; Bavarian, M.; Winter, C.;\nTillet, P.; Such, F. P.; Cummings, D.; Plappert, M.; Chantzis,\nF.; Barnes, E.; Herbert-V oss, A.; Guss, W. H.; Nichol, A.;\nPaino, A.; Tezak, N.; Tang, J.; Babuschkin, I.; Balaji, S.;\nJain, S.; Saunders, W.; Hesse, C.; Carr, A. N.; Leike, J.;\nAchiam, J.; Misra, V .; Morikawa, E.; Radford, A.; Knight,\nM.; Brundage, M.; Murati, M.; Mayer, K.; Welinder, P.; Mc-\nGrew, B.; Amodei, D.; McCandlish, S.; Sutskever, I.; and\nZaremba, W. 2021. Evaluating Large Language Models\nTrained on Code. ArXiv:2107.03374 [cs].\nChoi, D.; and Langley, P. 2018. Evolution of the Icarus Cog-\nnitive Architecture. Cognitive Systems Research, 48: 25–38.\nCrossman, J.; Wray, R. E.; Jones, R. M.; and Lebiere, C.\n2004. A High Level Symbolic Representation for Behavior\nModeling. In Gluck, K., ed., Proceedings of 2004 Behav-\nior Representation in Modeling and Simulation Conference.\nArlington, V A.\nDriess, D.; Xia, F.; Sajjadi, M. S. M.; Lynch, C.; Chowd-\nhery, A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.;\nYu, T.; Huang, W.; Chebotar, Y .; Sermanet, P.; Duck-\nworth, D.; Levine, S.; Vanhoucke, V .; Hausman, K.; Tous-\nsaint, M.; Greff, K.; Zeng, A.; Mordatch, I.; and Florence,\nP. 2023. PaLM-E: An Embodied Multimodal Language\nModel. ArXiv:2303.03378 [cs].\nGluck, K.; Laird, J.; and Lupp, J. R., eds. 2019. Interac-\ntive Task Learning: Agents, Robots, and Humans Acquir-\ning New Tasks through Natural Interactions, volume 26 of\nStr¨ungmann Forum Reports. Cambridge, MA: MIT Press.\nHuffman, S. B.; and Laird, J. E. 1995. Flexibly instructable\nagents. Journal of Artificial Intelligence Research, 3: 271–\n324.\nKim, G.; Baldi, P.; and McAleer, S. 2023. Language models\ncan solve computer tasks. ArXiv:2303.17491 [cs].\nKirk, J.; Wray, R. E.; Lindes, P.; and Laird, J. E. 2022. Im-\nproving Language Model Prompting in Support of Semi-\nautonomous Task Learning. In Proceedings of the 2022\nAdvances in Cognitive Systems Conference. Washington,\nDC/Virtual.\nKirk, J. R.; and Laird, J. E. 2019. Learning Hierarchi-\ncal Symbolic Representations to Support Interactive Task\nLearning and Knowledge Transfer. In Proceedings of IJ-\nCAI 2019, 6095–6102. International Joint Conferences on\nArtificial Intelligence.\nKirk, J. R.; Wray, R. E.; and Lindes, P. 2023. Improv-\ning Knowledge Extraction from LLMs for Task Learning\nthrough Agent Analysis. ArXiv:2306.06770 [cs].\nKirk, J. R.; Wray, R. E.; Lindes, P.; and Laird, J. E. 2023.\nIntegrating Diverse Knowledge Sources for Online One-shot\nLearning of Novel Tasks. ArXiv:2208.09554 [cs].\nKotseruba, I.; and Tsotsos, J. K. 2020. 40 years of cognitive\narchitectures: core cognitive abilities and practical applica-\ntions. Artificial Intelligence Review, 53(1): 17–94.\nLaird, J. E. 2012. The Soar Cognitive Architecture. Cam-\nbridge, MA: MIT Press.\nLaird, J. E.; Rosenbloom, P. S.; and Newell, A. 1986.\nChunking in Soar: The anatomy of a general learning mech-\nanism. Machine Learning, 1(1): 11–46.\nLenat, D.; and Marcus, G. 2023. Getting from Generative\nAI to Trustworthy AI: What LLMs might learn from Cyc.\nArXiv:2308.04445 [cs].\nLogeswaran, L.; Fu, Y .; Lee, M.; and Lee, H. 2022. Few-shot\nSubgoal Planning with Language Models. InProceedings of\nthe NAACL 2022. arXiv. ArXiv:2205.14288 [cs].\nMininger, A. 2021. Expanding Task Diversity in\nExplanation-Based Interactive Task Learning. Ph.D. The-\nsis, University of Michigan, Ann Arbor.\nNejati, N.; Langley, P.; and Konik, T. 2006. Learning hierar-\nchical task networks by observation. In Proceedings of the\n23rd international conference on Machine learning, ICML\n’06, 665–672. New York, NY , USA: Association for Com-\nputing Machinery. ISBN 978-1-59593-383-6.\nNewell, A. 1990.Unified Theories of Cognition. Cambridge,\nMassachusetts: Harvard University Press.\nOlmo, A.; Sreedharan, S.; and Kambhampati, S. 2021.\nGPT3-to-plan: Extracting plans from text using GPT-3.\nIn Proc. of ICAPS FinPlan and ICAPS KEPS. ArXiv:\n2106.07131.\nOpenAI. 2023. GPT-4 Technical Report. ArXiv:2303.08774\n[cs].\nPearson, D. J.; and Laird, J. E. 1998. Toward incremental\nknowledge correction for agents in complex environments.\nMachine Intelligence, 15.\nPetroni, F.; Rockt¨aschel, T.; Lewis, P.; Bakhtin, A.; Wu, Y .;\nMiller, A. H.; and Riedel, S. 2019. Language Models as\nKnowledge Bases? In EMNLP 2019. ArXiv: 1909.01066.\n293\nPezeshkpour, P.; and Hruschka, E. 2023. Large Language\nModels Sensitivity to The Order of Options in Multiple-\nChoice Questions. ArXiv:2308.11483 [cs].\nReynolds, L.; and McDonell, K. 2021. Prompt Program-\nming for Large Language Models: Beyond the Few-Shot\nParadigm. In Extended Abstracts of the 2021 CHI Confer-\nence on Human Factors in Computing Systems, CHI EA ’21,\n1–7. New York, NY , USA: Association for Computing Ma-\nchinery. ISBN 978-1-4503-8095-9.\nRitter, F. E.; Haynes, S. R.; Cohen, M.; Howes, A.; John, B.;\nBest, B.; Lebiere, C.; Lewis, R. L.; St. Amant, R.; McBraide,\nS. P.; Urbas, L.; Leuchter, S.; and Vera, A. 2006. High-level\nbehavior representation languages revisited.\nRussell, S.; and Norvig, P. 1995. Artificial Intelligence: A\nModern Approach. Upper Saddle River, NJ: Prentice-Hall.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2023.\nProgPrompt: program generation for situated robot task\nplanning using large language models. Autonomous Robots.\nSpeer, R.; Chin, J.; and Havasi, C. 2017. ConceptNet\n5.5: an open multilingual graph of general knowledge. In\nProc. of the 31st AAAI Conference on Artificial Intelli-\ngence, AAAI’17, 4444–4451. San Francisco, California,\nUSA: AAAI Press.\nValmeekam, K.; Sreedharan, S.; Marquez, M.; Olmo, A.;\nand Kambhampati, S. 2023. On the Planning Abilities of\nLarge Language Models (A Critical Investigation with a Pro-\nposed Benchmark). ArXiv:2302.06706 [cs].\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2023. Self-Consistency\nImproves Chain of Thought Reasoning in Language Models.\nIn ICLR 2023. arXiv. ArXiv:2203.11171 [cs].\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E.; Le, Q. V .; and Zhou, D. 2022. Chain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels. Advances in Neural Information Processing Sys-\ntems, 35: 24824–24837.\nWray, R. E.; Kirk, J. R.; and Laird, J. E. 2021. Language\nModels as a Knowledge Source for Cognitive Agents. In\nProceedings of the Ninth Annual Conference on Advances\nin Cognitive Systems. Virtual.\nWu, J.; Antonova, R.; Kan, A.; Lepert, M.; Zeng, A.;\nSong, S.; Bohg, J.; Rusinkiewicz, S.; and Funkhouser,\nT. 2023. TidyBot: Personalized Robot Assistance with\nLarge Language Models. Number: arXiv:2305.05658\narXiv:2305.05658 [cs].\nYost, G. R. 1993. Acquiring Knowledge in Soar. Intelligent\nSystems, 8(3): 26–34.\n294",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.7669459581375122
    },
    {
      "name": "Computer science",
      "score": 0.7587348222732544
    },
    {
      "name": "Cognition",
      "score": 0.6455736756324768
    },
    {
      "name": "Cognitive architecture",
      "score": 0.6182894706726074
    },
    {
      "name": "Question answering",
      "score": 0.5683488845825195
    },
    {
      "name": "Inference",
      "score": 0.4959045350551605
    },
    {
      "name": "Sentence",
      "score": 0.4708670675754547
    },
    {
      "name": "Natural language understanding",
      "score": 0.44561153650283813
    },
    {
      "name": "Task (project management)",
      "score": 0.4368491470813751
    },
    {
      "name": "Knowledge base",
      "score": 0.43654534220695496
    },
    {
      "name": "Natural language",
      "score": 0.43007606267929077
    },
    {
      "name": "Natural language processing",
      "score": 0.42253535985946655
    },
    {
      "name": "Cognitive model",
      "score": 0.41524386405944824
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3968390226364136
    },
    {
      "name": "Psychology",
      "score": 0.14651328325271606
    },
    {
      "name": "Engineering",
      "score": 0.089702308177948
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210118385",
      "name": "Cambridge Cognition (United Kingdom)",
      "country": "GB"
    }
  ]
}