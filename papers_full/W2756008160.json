{
  "title": "Towards personalized human AI interaction - adapting the behavior of AI agents using neural signatures of subjective interest",
  "url": "https://openalex.org/W2756008160",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2744449577",
      "name": "Shih Victor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4298735966",
      "name": "Jangraw, David C.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2322478573",
      "name": "Sajda Paul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743498389",
      "name": "Saproo, Sameer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2328078142",
    "https://openalex.org/W2952523895",
    "https://openalex.org/W2517744119",
    "https://openalex.org/W1968962398",
    "https://openalex.org/W1526400779",
    "https://openalex.org/W2106370146",
    "https://openalex.org/W1970756271",
    "https://openalex.org/W46659105",
    "https://openalex.org/W1584279013",
    "https://openalex.org/W2041346309",
    "https://openalex.org/W2318928929",
    "https://openalex.org/W2145339207"
  ],
  "abstract": "Reinforcement Learning AI commonly uses reward/penalty signals that are objective and explicit in an environment -- e.g. game score, completion time, etc. -- in order to learn the optimal strategy for task performance. However, Human-AI interaction for such AI agents should include additional reinforcement that is implicit and subjective -- e.g. human preferences for certain AI behavior -- in order to adapt the AI behavior to idiosyncratic human preferences. Such adaptations would mirror naturally occurring processes that increase trust and comfort during social interactions. Here, we show how a hybrid brain-computer-interface (hBCI), which detects an individual's level of interest in objects/events in a virtual environment, can be used to adapt the behavior of a Deep Reinforcement Learning AI agent that is controlling a virtual autonomous vehicle. Specifically, we show that the AI learns a driving strategy that maintains a safe distance from a lead vehicle, and most novelly, preferentially slows the vehicle when the human passengers of the vehicle encounter objects of interest. This adaptation affords an additional 20\\% viewing time for subjectively interesting objects. This is the first demonstration of how an hBCI can be used to provide implicit reinforcement to an AI agent in a way that incorporates user preferences into the control system.",
  "full_text": "Towards personalized human AI interaction -\nadapting the behavior of AI agents using neural\nsignatures of subjective interest\nVictor Shih∗, David C Jangraw ‡, Paul Sajda ∗†, and Sameer Saproo ∗\n∗Department of Biomedical Engineering, Columbia University, New York, NY 10027 USA\n†Data Science Institute, Columbia University, New York, NY 10027 USA\n‡National Institutes of Health, Bethesda, MD 20892 USA\nEmail: vs2481@columbia.edu\nAbstract—Reinforcement Learning AI commonly uses re-\nward/penalty signals that are objective and explicit in an environ-\nment – e.g. game score, completion time, etc. – in order to learn\nthe optimal strategy for task performance. However, Human-AI\ninteraction for such AI agents should include additional rein-\nforcement that is implicit and subjective – e.g. human preferences\nfor certain AI behavior – in order to adapt the AI behavior to\nidiosyncratic human preferences. Such adaptations would mirror\nnaturally occurring processes that increase trust and comfort\nduring social interactions. Here, we show how a hybrid brain-\ncomputer-interface (hBCI), which detects an individual’s level of\ninterest in objects/events in a virtual environment, can be used to\nadapt the behavior of a Deep Reinforcement Learning AI agent\nthat is controlling a virtual autonomous vehicle. Speciﬁcally, we\nshow that the AI learns a driving strategy that maintains a safe\ndistance from a lead vehicle, and most novelly, preferentially\nslows the vehicle when the human passengers of the vehicle en-\ncounter objects of interest. This adaptation affords an additional\n20% viewing time for subjectively interesting objects. This is\nthe ﬁrst demonstration of how an hBCI can be used to provide\nimplicit reinforcement to an AI agent in a way that incorporates\nuser preferences into the control system.\nI. I NTRODUCTION\nThe use of Artiﬁcial Neural Networks (ANNs) towards\ndeveloping Artiﬁcial Intelligence (AI) has undergone a re-\nnaissance in the past decade. Out of the many emergent\ntechniques for training ANNs that are collectively referred\nto as ’Deep Learning’, Deep Reinforcement Learning (DRL)\nis proving to be a particularly general and powerful method,\nwith applications ranging from video games [1] to autonomous\ndriving [2]. While most applications of reinforcement learning\nhave traditionally used reinforcement signals derived from\nperformance measures that are explicit to the task – e.g.\nthe score in a game or grammatical errors in a translation,\nwhen considering AI systems that are required to have a\nsigniﬁcant interaction with humans – e.g. the autonomous\nvehicle – it is critical to consider how the human’s preference\nfor objects, events, or actions can be incorporated into the\nbehavioral reinforcement for the AI, particularly in ways that\nare minimally obtrusive [3], [4]. Such behavioral adaptations\noccur naturally during social interactions and form the bedrock\nof social mechanisms that build trust and rapport between\nstrangers [5], [6].\nIn this paper, we present a novel approach that uses de-\ncoded human neurophysiological and ocular time-series data\nas an implicit reinforcement signal for an AI agent that is\ndriving a virtual automobile. The agent learns a brake and\naccelerate strategy that integrates road safety with the personal\npreferences of the human passenger. These preferences are\nderived from the neural (EEG: electroencephalography) and\nocular signals (pupillometry and gaze time) that are evoked\nby interesting objects/events in the simulated environment.\nWe integrate and decode these signals and construct a hybrid\nbrain-computer interface (hBCI) [7] whose output represents\na passenger’s subjective level of interest in objects/events in\nthe world, and therefore can be used to reinforce AI behavior.\nWe describe the details of our approach, including the\ncognitive neuroscience basis of the signals we decode and\nintegrate within the hBCI. We then show how the hBCI can be\nmade more robust by adding a semi-supervised graph-based\nmodel of the objects called TAG: Transductive Annotation by\nGraph [8]. This graph-based model reduces errors that may\nresult from the neural-ocular decoding as well as extrapo-\nlates the preference estimates derived from a small number\nof viewed objects to a much larger number of previously\nunencountered objects. This extrapolation reduces the amount\nof neural data required for the system to function effectively.\nWe show that the AI converges to a driving behavior that\nincreases the time that a passenger gets to view objects of\ninterest in the environment. Finally, we discuss the extension\nof this approach to various human-AI-interaction scenarios that\nincorporate other measures of an individual’s cognitive state,\nfor example, tailoring the experience in an autonomous vehicle\nbased on their level of comfort or arousal.\nII. M ETHODS\nWe used a three-stage machine learning structure to 1) cap-\nture neural and physiological signatures of subject preferences,\n2) tune and extrapolate these preferences over objects in a\ngiven environment, and 3) reinforce driving behavior using a\ndeep reinforcement network (see Figure 1). Speciﬁcally, we\narXiv:1709.04574v1  [cs.HC]  14 Sep 2017\nFig. 1: Machine Learning Schema. The three stages of machine learning that are used in our system. The ﬁrst stage uses a hybrid classiﬁer\nto fuse and decode physiological signals from the subject and identify ’target’ (interesting) or ’non-target’ (uninteresting) objects from a\nsubset of the object database. The second stage uses these object labels and a CV system to identify targets and non-targets from the full\nobject database. The third stage populates a virtual environment with objects and uses the object labels (’target’ or ’non-target’) to reinforce\nthe AI agent, such that the AI agent learns to keep the target objects in view for longer periods of time.\nutilized physiologically derived information from an hBCI to\ninfer subject interest in objects in the environment, classiﬁed\nthe objects as targets or non-targets, and used these neural\nmarkers in TAG semi-supervised learning architecture [8] to\nextrapolate from a small set of target examples so as to\ncategorize a large database of objects into targets and non-\ntargets. We then placed these target and non-target objects\nin a virtual environment where the AI drove a simulated\nautomobile. When these objects were in view of the vehicle,\nwe sent positive or negative reinforcement to the learning\nagent. Our goal was to differentiate the behavior of the AI\nwhen driving near targets and non-targets. We incentivized\nthe AI agent to keep within visual distance of targets for a\nlonger period of time.\n1) Virtual Environment:We designed a virtual environment\nin Unity3D and used assets from a previous study to create\nobjects seen in the virtual environment (Figure 2a) [7]. The\nvehicle in our simulation had access to a top-down view of\nthe environment that outlines the road, the car in front, and\nthe objects by the side of the road (Figure 2b). We chose\nthis environment representation due to the fact that some au-\ntonomous vehicles under development currently are employing\na similar composite maps of the environment[9]. In our virtual\nenvironment, a passenger-bearing vehicle controlled by the AI\ndrove behind a pre-programmed lead vehicle that followed a\nstraight path but braked and accelerated stochastically. Each\nindependent driving run in the experiment started from the\nsame location in the virtual environment and would end\nimmediately if the passenger vehicle lagged too far behind the\nlead vehicle ( >60m) or followed dangerously close ( <5m).\nThe AI agent had 3 actions at its disposal: increase speed,\nmaintain speed, or decrease speed of the passenger car. There\nwere alleys on either side of the road, with 40% of these\nalleys containing distinctly visible objects. Targets and non-\ntargets objects were placed randomly in the alleyways in a\n1:3 prevalence ratio.\nA. Subject Preferences\nWe tracked the subjective preferences of the human passen-\nger through decoded physiological signals of the human ori-\nenting response [10]. Orienting is critical to decision-making\nsince it is believed to be important for allocating attention\nand additional resources, such as memory, to speciﬁc objects\nor events in the environment. Salience and emotional valence\nare known to affect the intensity of the orienting response.\nOrienting is expressed neurophysiologically as evoked EEG,\nspeciﬁcally in the P300 response [11]. It is also often associ-\nated with changes in arousal level, seen in the dilation of the\npupil, as well as changes in behavior, for example physically\norienting to the object or event of interest [10].\nReinforcement learning typically requires a large number of\nsamples to train a network to reach a successful model. In this\nexperiment, we used physiological signals as reinforcement,\nbut due to the large amount of training data needed, it was\nunreasonable to have a subject sit through the experiment for\nthe many hours needed to train the network. Instead of using\nreal time physiological reinforcement to train the network, we\nutilized target object labels from a previous experiment derived\nfrom neurophysiological data of subjects and extrapolated to\nthe full object database using the TAG computer vision system\n[7]. In this way, we were able to build a model that predicted\nsubject preferences but expanded the training dataset so that an\naccurate AI model could be trained in the virtual environment.\n1) Hybrid BCI: In a previous study [7], subjects were\ndriven through a grid of streets and asked to count image\nobjects of a pre-determined target category. The physiological\nsignals that were naturally evoked by objects in this task were\n(a) Subject View\n(b) Input to DeepQ\nFig. 2: Virtual Environment for the Experiment. a) A screen capture\nof the passenger’s view in the virtual environment. Objects can be\nseen in the alleys to the side of the vehicle. b) The input to the AI that\nshows a top-down perspective of the virtual environment. The two\nvehicles - the lead vehicle and the AI-controlled vehicle - are seen on\nthe road as white blocks while objects in the alleys are represented\nby markers with one luminance corresponding to targets and one\nluminance corresponding to non-targets. This view is intended to\nmirror the composite maps currently used to train self driving cars\n[9].\nclassiﬁed by an hBCI system (Fig. 1). This hBCI system\nwas adapted from the hierarchical discriminant component\nanalysis (HDCA) described in Gerson et al (2006), Pohlmeyer\net al (2011) and Sajda et al (2010) to accommodate multiple\nmodalities: EEG, pupil dilation, and gaze time. To construct\nthe classiﬁer for each subject, EEG data in the 100 ms to\n1000 ms window after the subject ﬁxated on the object were\ndivided into nine 100 ms bins. Within-bin weights across the\nIndependent Components of the EEG data were determined for\neach bin using Fisher linear discriminant analysis (FLDA):\nwj = (\n∑\n+\n+\n∑\n−\n)−1(µ+ −µ−) (1)\nwhere wj is the vector of within-bin weights for bin j, µand Σ\nare the mean and covariance of the data (across training trials)\nin the current bin, and the + and subscripts denote target and\nnon-target trials, respectively. The weights w were applied to\nthe IC activations x from a separate set of evaluation trials to\nget one within-bin interest score zji for each bin j in each\ntrial i so that:\nzji = wT\nj xji (2)\nThe within-bin interest scores from the evaluation trials served\nas part of the input to a cross-bin classiﬁer. The use of an\nevaluation set ensured that if the within-bin classiﬁer over-\nﬁtted to the training data, this over-ﬁtting would not bias the\ncross-bin classiﬁer towards favoring these features.\nThe pupil dilation data from 0 to 3000 ms were separated\ninto six 500-ms bins and averaged within each bin. For\neach bin, this average was passed through FLDA to create\na discriminant value. The gaze time data was also passed\nthrough FLDA. The scale of each EEG, pupil dilation and\ngaze time feature was then rescaled by dividing each feature’s\noutput by its standard deviation across all evaluation trials. A\nsecond-level feature vector zi was created for each evaluation\ntrial i by appending that trials rescaled EEG, pupil dilation,\nand dwell time features into a single column vector.\nTo classify the second-level feature vectors from each trial\n(zi), cross-bin weights v were derived using logistic regres-\nsion, which maximizes the conditional log likelihood of the\ncorrect class:\nv= arg min\nv\n(\n∑\ni\nlog 1 + exp[−civTzi] + λ||v||2\n2) (3)\nwhere ci is the class (+1 for targets and 1 for non-targets) of\ntrial i and λ = 10 is a regularization parameter introduced\nto discourage over-ﬁtting. These weights were applied to the\nwithin-bin interest scores from a separate set of testing trials\nto get a single cross-bin interest score yi for each trial:\nyi = vTzi (4)\nThe effectiveness of the classiﬁer was evaluated by its ability\nto produce cross-bin interest scores yi that are higher for\ntargets than for non-targets. Trials with cross-bin interest\nscores more than 1 standard deviation above the mean were\nidentiﬁed as hBCI predicted targets.\n2) Transductive Annontation by Graph:TAG used a graph-\nbased system to identify target objects that are of interest to the\nsubject. TAG ﬁrst tuned the target set predicted by the hBCI\nfor each subject and then extrapolated these results to all the\nunseen objects in the environment [8], [7]. TAG constructed a\nCV graph containing all the objects in the virtual environment,\nusing their similarity to determine connection strength (Wang\net al 2008, 2009a). The graph employed gist features (low-\ndimensional spectral representations of the image based on\nspatial envelope properties, as described in Oliva and Torralba\nFig. 3: Illustrative toy example of hBCI + CV labeling of objects in the 3D environment. Top row: objects were placed in the environment,\nand subjects were asked to count their ”preferred” target category (in this example, grand pianos) as they moved through the environment.\nRows numbered Steps 1-4: process by which the objects were labeled as targets (green checks) or non-targets (red x’s). The labels could\nthen be used to determine the reinforcement signal sent to the DL system: having a target-labeled object in view resulted in an increased\nreward. Orange outlines indicate that the data/label was generated in this step. Step 1: The subject viewed some (but not all) of the objects.\nEEG, pupil dilation, and dwell time data were collected as the subject viewed each one, as described in [7]. Step 2: A subject-speciﬁc\nhBCI classiﬁer was constructed to convert these biometric signals into a target/non-target label. Step 3: the TAG CV system [8] was used to\n”self-tune” the labels, adjusting them so that the predicted targets are strongly connected to each other but not to the predicted non-targets.\nBlue lines show the level of connection in the CV graph: Thick solid lines represent strong connections; thin dotted lines represent weak\nones. Step 4: The tuned labels are propagated through the CV graph to generate labels for the unseen objects.\n(2001)). The similarity estimate for each pair of objects was\nbased not only on the features of that pair, but also on the\ndistribution of features across all objects represented in the CV\ngraph. TAG tuned the hBCI predicted target set by removing\nobjects that did not resemble the set as a whole and replacing\nthem with images that did (Sajda et al 2010, Wang et al 2009a,\n2009b). Conceptually, the objects in the hBCI predicted target\nset that were least connected to the others were deemed most\nlikely to be false positives. They were removed from the set\nand replaced with the objects not in the set that were most\nconnected to the set. Objects in the resulting set were called\ntuned predicted targets\nThe tuned predicted target set was propagated through the\nCV graph to determine a CV score for each object in the\nvirtual environment, such that the images with the strongest\nconnections to the tuned predicted target set were scored most\nhighly. A cutoff was determined by ﬁtting a mixture of two\nGaussians to the distribution of CV scores and ﬁnding the\nintersection point of the Gaussians that falls between their\nmeans. The images with CV scores above the cutoff were\nidentiﬁed as CV predicted targets. Because each object was\npaired with an object in virtual environment space, these\nCV predicted targets represent the systems predictions of the\nobjects in the environment that are most likely considered\ntargets by the subject.\nB. Deep Reinforcement Learning\nTo train the AI agent to navigate the virtual environment,\nwe used a deep reinforcement learning paradigm [1] that\noptimizes the function for learning the correct action under\na given state S using the equation:\nQπ(s,a) = E[R1 + γR2 + ...|S0 = s,A0 = a,π] (5)\nWhere E[R1] is the expected reward of the next state and\naction pair, and subsequent state action pairs are discounted\nby γ compounding. This Q-function can be approximated\nby the parameterized value: Q(s,a; θt). Where θt is the\nparameterized representation of π. By utilizing reinforcement\nlearning the network builds a model that predicts future states\nand future rewards in order to optimally accomplish a task.\nWe implemented double-deepQ learning [12] to update the\nnetwork weights to this parameterized function after taking\naction At at state St and observing the immediate reward Rt+1\nand state St+1 using the equation:\nθt+1 = θt + α(YDQ\nt −Q(St,At; θt))∇θtQ(St,At; θt) (6)\nwhere α is a scalar step size and the target YDQ\nt is deﬁned\nas:\nYDQ\nt = Rt+1 + γQ(St+1,arg max\na\nQ(St+1,a; θtd); θtd) (7)\nBy implementing this form of learning, we were able to\nadjust the reward value to combine explicit reinforcement of\ndriving performance with physiologically derived feedback to\ninﬂuence the behavior of the AI-controlled virtual car.\n1) Network Architecture: The deep network used to pa-\nrameterize the Q-function was created using a 5 layer deep\nnetwork [1]. We used convolution layers in the network so\nas to allow computer vision capabilities that can interpret the\ninput state image and identify objects in the image such as\nthe car position and object positions.The input to the neural\nnetwork consisted of the 3x64x64 grayscale image series state\ninput. The ﬁrst hidden layer convolved 32 ﬁlters of 8x8\nwith stride 4 with the input image and applied a rectiﬁer\nnonlinearity. The second hidden layer convolved 64 ﬁlters of\n4x4 with stride 2, again followed by a rectiﬁer nonlinearity.\nThis was followed by a third convolution layer that convolved\n64 ﬁlters of 3x3 with stride 1 followed by a rectiﬁer. The ﬁnal\nhidden layer was fully-connected and consisted of 512 rectiﬁer\nunits. The output layer was a fully-connected linear layer with\na single output for each valid action - increase speed, hold\nspeed, and decrease speed.\n2) State: The AI agent assessed the state using a top-down\nview of the virtual environment surrounding the passenger car\n(Figure 2b). We used 3 successive video frames - a 3x64x64px\ngrayscale image series - as the state input, S, to the deep\nlearning network. With this image series, the AI agent could\nsee the road, the position of both cars, and orb representations\nof objects at the side of the road. The luminance of these orb\nrepresentations were based on their object category as targets\nor non-targets. When the car was near these orbs, it elicited a\nreinforcement signal, as described in the next section.\n3) Reward: We rewarded the agent as long as the passenger\ncar followed the lead car within prescribed bounds of distance.\nThe agent received a positive reinforcement for staying within\nthe bounds and a negative reinforcement when it violated these\ndistance bounds (+1 and -10 reinforcement respectively). To\ninclude physiological signals into the reinforcement, the AI\nagent received an additional reward (or punishment) based\non the neurophysiological response evoked by image objects\nwithin the visual distance of the passenger car; an object\nclassiﬁed by the hBCI + TAG system as a target object yielded\na reward while those classiﬁed as non-target objects yielded a\npenalty. We balanced the magnitude of reward and of penalty\naccording to the prevalence of targets and non-targets in the\nenvironment (+3 and -1 reinforcement respectively). Some ob-\njects are misclassiﬁed by hBCI and therefore yield the wrong\nreinforcement (false positives and false negatives). In the false\npositive condition, an object with an orb representation with\nluminance corresponding to a non-target would yield positive\nreinforcement. In the false negative condition, an object with\nan orb representation with luminance corresponding to a target\nwould yield negative reinforcement. For each subject, we\nchose to use the classiﬁcation threshold which maximized the\nF1 score of that subject. The F1 score is a commonly used\nmetric for determining a classiﬁer’s accuracy by using the\nharmonic mean of the precision and recall [13]. The immediate\nreward, Rt+1, was the sum of all reinforcement values that are\naccumulated in the current rendered frame.\nr1 =\n{\n+1, if 5 <d< 60\n−10, else (8)\nr2 =\n\n\n\n+3, if WVDa ∧ωa <TPR\n−1, if WVDa ∧ωa >TPR\n0, else\n(9)\nr3 =\n\n\n\n−1, if WVDb ∧ωb >FPR\n+3, if WVDb ∧ωb <FPR\n0, else\n(10)\nRt+1 = f(w1r1 + w2r2 + w3r3) (11)\nWhere d is the distance between the passenger car and\nthe lead car (at distances between 5 and 60, the AI was\nat a ”safe distance” from the lead car); WVD a and WVD b\nare True when the car is within visual range of a target\nand a nontarget object respectively and False otherwise; TPR\nis the true positive rate and FPR is the false positive rate\nof the hBCI+TAG system derived from the subject; and ω\nis a uniformly distributed random number between 0 and\n1 chosen at each incidence of WVD being True. In future\ninstantiations, the different reinforcement schemes ( r1,...,r n)\ncould be weighted differently and the ﬁnal value could be\nused in a function such as a sigmoid in order to squash the\nreinforcement value limits. In this study we weighted the\nreinforcement schemes equally at w1 = w2 = w3 = 1 and\ndid not use a squashing function.\nIII. R ESULTS\nWe report results for 10 subjects; eight subjects showed\na learned behavior based on their hBCI+TAG preferences\nwhile two subject did not show preference learning due to\nlow SNR of the hBCI+TAG classiﬁcation. Individual subject\nperformance and Q-learning convergence can be found in\n(a) All Subjects Training Evolution\n (b) AI Agent Brake Behavior\nFig. 4: Training of the hBCI AI agent. a) This ﬁgure shows the normalized Q value averaged across subjects and the run time averaged\nacross subjects through the training duration. The Q value evolution over training time suggests that all subjects have converged to a relatively\nstable Q value which indicates that training has plateaued. This observation is echoed by the driving performance which shows an increase\nof run time to a relatively stable average of around 25 seconds. The shaded area shows the standard error across subjects. b) This ﬁgure\nshows the learned brake behavior of the resulting AI agent. The areas that are marked in red indicate distances from the lead car which\nresulted in a game over and negative reinforcement. As expected, the car learns to avoid crashes by braking when it is too close and to avoid\nlagging too far by accelerating.\n(a) All Subjects Dwell Times\n (b) Control Subject Dwell Times\nFig. 5: Results for hBCI Deep Learning Agent. a) Average dwell time between targets and non-targets show approximately 20% increase\nin dwell time between targets and non-targets across subjects. The shaded area in the graph represents the standard error across subjects. b)\nA control subject was used to see how the AI agent behaved when a hBCI+TAG that outputted random classiﬁcation values was used. The\nresults show that there is very little separation of dwell times between targets, non-targets, and empty halls.\nthe supplementary materials. Data from all subjects show a\nconverging growth in Q-values during AI training, indicating\nthat the AI agent is converging to a relatively stable policy for\ndriving. (Figure 4a). This stable policy leads to an increase\nin the total run time of the autonomous vehicle in each\nindependent driving session across all subjects. The AI agent\nlearns the expected behavior, of braking when the passenger\ncar gets too close to the lead car and accelerating when lagging\ntoo far behind, in order to stay within the reinforcement\nbounds. (Figure 4b)With the integration of hBCI reinforcement\ninto the reward functions, this policy is also able to tune\nthe behavior of the AI agent to each individual’s subjective\ninterest – indicating whether they viewed each object as a\ntarget or non-target. One key metric of successful learning is\nthe dwell time for each object type, which is the number of\nseconds that the passenger car stays within visual distance of\nan object. Results show that the AI agent is able to differentiate\nbetween targets and non-targets, learning to keep the targets\nwithin view for a longer period of time (Figure 5a). As a\ncontrol, we set the true positive rate and false positive rate\nto 0.5 to simulate a subject with an hBCI+TAG output that\nis random and observed that this control subject did not have\nsigniﬁcantly different dwell times between targets, nontargets,\nand empty halls (Figure 5b). As expected, the success of the\nsystem in spending more time dwelling on targets (relative\nto non-targets or empty halls) depends on the F1 score of\nthe hBCI+TAG classiﬁer (Figure 6). Speciﬁcally, we ﬁnd that\nhigher classiﬁcation accuracy yields larger differences in dwell\nFig. 6: Comparing F1 score with the difference in dwell time shows that subjects with lower F1 score in the hBCI+TAG have a smaller\nseparation of dwell times between targets and non-targets.\ntime between targets and non-targets.\nTABLE I: Subject hBCI+TAG Results\nSubject TPR FPR F1 Score\n1 0.8343 0.0125 0.8896\n2 0.9823 0.9495 0.4306\n3 1.0000 0.0063 0.9901\n4 0.8745 0.0115 0.9182\n5 0.8454 0.0077 0.9036\n6 0.8783 0.0074 0.9248\n7 0.8257 0.0177 0.8802\n8 1.0000 0.9905 0.4008\n9 0.7793 0.0070 0.8668\n10 0.6250 0.0269 0.7324\nIV. D ISCUSSION\nIn this paper, we present a three-tiered machine learning\napproach (Figure 1) for decoding neural and ocular signals\nreﬂecting a human passenger’s level of interest and then\nusing these subjective signals to favorably impact the driving\nstrategy for an AI controlled vehicle. In our experiments, a\nfavorable driving strategy is one that maintains a safe distance\nfrom a lead vehicle, slows the vehicle when objects of speciﬁc\ninterest to the passenger are encountered during the drive,\nand ignores objects that do not interest the passenger. The\nprime novelty of our approach is that the human-machine\ninteraction that communicates passenger preferences to the AI\nagent is implicit and via the hBCI – i.e. the passenger does\nnot need to communicate their preferences overtly, with say\na press of a button, but instead preferences are inferred via\ndecoded neural-ocular activity. A second novel element of our\napproach is that we use semi-supervised CV-based learning\nto increase the prevalence of the reinforcement signals while\nalso mitigating the relatively low signal to noise ratio (SNR)\nof the human neurophysiological data – only a few evoked\nneural-ocular responses are needed to generate a model of\nthe passenger’s preferences. We show that in using a double-\ndeepQ reinforcement architecture, we converge to a stable\ndriving policy which increases the average run time of the\nautonomous vehicle. Additionally, in 8 out of 10 subjects\n(due to hBCI+TAG performance), the AI adapts to a driving\nstrategy that signiﬁcantly increases the time that the passengers\ncan gaze at objects that are consistent with their individual\ninterest.\nThis approach can be used to tune driving behavior of\nan autonomous vehicle to individual preferences in several\nother scenarios. For example, reinforcement can be designed\nto optimize on other aspects of human preferences, such as\nwhether the ride is “comfortable” for the individual. Here,\n“comfortable” is a subjective metric that is speciﬁc to a\nparticular human passenger and might be observed via changes\nin arousal, stress level, and emotional valence, amongst other\nphysiological and cognitive factors. The importance of an\nAI agent recognizing and acting upon, or even predicting\nhuman preferences, is important not only as an interface but\nultimately because it might be crucial for development of a\n”trusted relationship” between the human and machine akin\nto the emotional intelligence required for harmonious social\ninteractions between humans [14], [15], [3].\nCurrently, the system we describe utilizes EEG, pupil di-\nlation, and eye position data as physiological signals used\nto train the classiﬁer to distinguish targets from non-targets.\nFuture investigations are needed to determine additional phys-\niological and behavioral signals that can be fused in the hBCI\nto infer cognitive and emotional state. Speciﬁcally, for real-\nworld applications where scalp EEG is not practical, using\nunobtrusive sensing modalities such as video to track facial\nmicro-expressions as well as electrodermal activity (EDA) and\nHeart Rate Variability (HRV) might provide the necessary\nprecision and recall to train reinforcement learning agents.\nOur current system is not currently closed-loop; it does\nnot obtain new training data from the environment during\nthe reinforcement process. Future work would entail closing\nthe loop to modify AI behavior while new data is being\ncontinuously introduced into the system. We hypothesize that\nafter the AI agent has learned to slow down when the car\napproaches non-targets, the hBCI will yield more accurate\ninferences because the subject is more likely to attend to the\ntargets in a future run. This new more accurate data can be\npropagated in the TAG module to train the AI agent again\nand further improve the differentiation between targets and\nnon-targets.\nACKNOWLEDGMENT\nThe work was partially funded by the Army Research\nLaboratory under Cooperative agreement number W911NF-\n10-2-0022. This research was partially supported by BRAIQ,\nInc.\nREFERENCES\n[1] V . Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare,\nA. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen,\nC. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra,\nS. Legg, and D. Hassabis, “Human-level control through deep reinforce-\nment learning,” Nature, vol. 518, no. 7540, pp. 529–533, 02 2015.\n[2] S. Lange, M. Riedmiller, and A. V oigtlander, “Autonomous reinforce-\nment learning on raw visual input data in a real world application,” in\nNeural Networks (IJCNN), The 2012 International Joint Conference on.\nIEEE, 2012, pp. 1–8.\n[3] S. Saproo, J. Faller, V . Shih, P. Sajda, N. Waytowich, A. Bohannon,\nV . Lawhern, B. Lance, and D. Jangraw, “Cortically coupled computing:\nA new paradigm for synergistic human-machine interaction,” Computer,\nvol. 49, no. 9, pp. 60–68, 2016.\n[4] B. Lake, T. Ullman, J. Tenenbaum, and S. Gershman, “Build-\ning machines that learn and think like people,” arXiv preprint\narXiv:1604.00289, 2016.\n[5] M. Iacoboni, Mirroring People: The New Science of How We Connect\nwith Others. Farrar, Straus and Giroux, 2009. [Online]. Available:\nhttps://books.google.com/books?id=FEWWzxLlP8YC\n[6] U. Hasson and C. D. Frith, “Mirroring and beyond: coupled dynamics as\na generalized framework for modelling social interactions,” Phil. Trans.\nR. Soc. B, vol. 371, no. 1693, p. 20150366, 2016.\n[7] D. Jangraw, J. Wang, B. Lance, S. Chang, and P. Sajda, “Neurally and\nocularly informed graph-based models for searching 3d environments,”\nJournal of Neural Engineering, vol. 11, no. 4, p. 046003, 2014.\n[8] J. Wang, E. Pohlmeyer, B. Hanna, Y . Jiang, P. Sajda, and S. Chang,\n“Brain state decoding for rapid image retrieval,” in Proceedings of the\n17th ACM International Conference on Multimedia, ser. MM ’09. New\nYork, NY , USA: ACM, 2009, pp. 945–954.\n[9] A. C. Madrigal, “Inside waymo’s secret world\nfor training self-driving cars,” Aug 2017. [On-\nline]. Available: https://www.theatlantic.com/technology/archive/2017/\n08/inside-waymos-secret-testing-and-simulation-facilities/537648/\n[10] S. Nieuwenhuis, E. De Geus, and G. Aston-Jones, “The anatomical and\nfunctional relationship between the p3 and autonomic components of\nthe orienting response,” Psychophysiology, vol. 48, no. 2, pp. 162–175,\n2011.\n[11] E. Donchin, E. Hefﬂey, S. Hillyard, N. Loveless, I. Maltzman,\nA. ¨Ohman, F. R¨osler, D. Ruchkin, and D. Siddle, “Cognition and event-\nrelated potentials ii. the orienting reﬂex and p300,” Annals of the New\nYork Academy of Sciences, vol. 425, no. 1, pp. 39–57, 1984.\n[12] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning,” CoRR, vol. abs/1509.06461, 2015.\n[13] D. Powers, “Evaluation: from precision, recall and f-measure to roc,\ninformedness, markedness and correlation,” 2011.\n[14] K. Hoff and M. Bashir, “Trust in automation integrating empirical\nevidence on factors that inﬂuence trust,” Human Factors: The Journal of\nthe Human Factors and Ergonomics Society, vol. 57, no. 3, pp. 407–434,\n2015.\n[15] M. Lewis, “Designing for human-agent interaction,” AI Magazine,\nvol. 19, no. 2, p. 67, 1998.\nAPPENDIX A\nDWELL TIME FIGURES\na: Dwell Time for Subject 1\n b: Dwell Time for Subject 2\n c: Dwell Time for Subject 3\nd: Dwell Time for Subject 4\n e: Dwell Time for Subject 5\n f: Dwell Time for Subject 6\ng: Dwell Time for Subject 7\n h: Dwell Time for Subject 8\n i: Dwell Time for Subject 9\nj: Dwell Time for Subject 10\nAPPENDIX B\nRUN TIME FIGURES\na: Run Time for Subject 1\n b: Run Time for Subject 2\n c: Run Time for Subject 3\nd: Run Time for Subject 4\n e: Run Time for Subject 5\n f: Run Time for Subject 6\ng: Run Time for Subject 7\n h: Run Time for Subject 8\n i: Run Time for Subject 9\nj: Run Time for Subject 10\nAPPENDIX C\nQ-VALUE FIGURES\na: Q Values for Subject 1\n b: Q Values for Subject 2\n c: Q Values for Subject 3\nd: Q Values for Subject 4\n e: Q Values for Subject 5\n f: Q Values for Subject 6\ng: Q Values for Subject 7\n h: Q Values for Subject 8\n i: Q Values for Subject 9\nj: Q Values for Subject 10",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.7803566455841064
    },
    {
      "name": "Computer science",
      "score": 0.7159973978996277
    },
    {
      "name": "Task (project management)",
      "score": 0.6669403314590454
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.6574269533157349
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5727782845497131
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5062293410301208
    },
    {
      "name": "Reinforcement",
      "score": 0.4972436726093292
    },
    {
      "name": "Virtual actor",
      "score": 0.4434118866920471
    },
    {
      "name": "Control (management)",
      "score": 0.4221875071525574
    },
    {
      "name": "Interface (matter)",
      "score": 0.4146578907966614
    },
    {
      "name": "Virtual reality",
      "score": 0.2366698682308197
    },
    {
      "name": "Psychology",
      "score": 0.15793943405151367
    },
    {
      "name": "Engineering",
      "score": 0.07370468974113464
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Maximum bubble pressure method",
      "score": 0.0
    },
    {
      "name": "Bubble",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}