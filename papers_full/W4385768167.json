{
  "title": "Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification",
  "url": "https://openalex.org/W4385768167",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5006075217",
      "name": "Conghao Xiong",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100353596",
      "name": "Hao Chen",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5049274840",
      "name": "Joseph J.�Y. Sung",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5042251906",
      "name": "Irwin King",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3204764952",
    "https://openalex.org/W2991430586",
    "https://openalex.org/W3176719058",
    "https://openalex.org/W3135547872",
    "https://openalex.org/W2020239141",
    "https://openalex.org/W3034447539",
    "https://openalex.org/W4288089649",
    "https://openalex.org/W3203168750",
    "https://openalex.org/W2971376088",
    "https://openalex.org/W2076570160",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2785934082",
    "https://openalex.org/W4301181863",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4312765357",
    "https://openalex.org/W3091730259",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4224989370",
    "https://openalex.org/W3129192437",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3033382446",
    "https://openalex.org/W2956228567",
    "https://openalex.org/W3199651403",
    "https://openalex.org/W3211647829",
    "https://openalex.org/W3046334363",
    "https://openalex.org/W4281257868",
    "https://openalex.org/W3202476840",
    "https://openalex.org/W3203898052",
    "https://openalex.org/W3034534840",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W3007110023",
    "https://openalex.org/W2531897166",
    "https://openalex.org/W2154318594",
    "https://openalex.org/W4312468136",
    "https://openalex.org/W3089090082",
    "https://openalex.org/W2964157630",
    "https://openalex.org/W3126827997",
    "https://openalex.org/W2980118615",
    "https://openalex.org/W2786807178",
    "https://openalex.org/W2152813753",
    "https://openalex.org/W2302302587"
  ],
  "abstract": "Multiple Instance Learning (MIL) and transformers are increasingly popular in histopathology Whole Slide Image (WSI) classification. However, unlike human pathologists who selectively observe specific regions of histopathology tissues under different magnifications, most methods do not incorporate multiple resolutions of the WSIs, hierarchically and attentively, thereby leading to a loss of focus on the WSIs and information from other resolutions. To resolve this issue, we propose a Hierarchical Attention-Guided Multiple Instance Learning framework to fully exploit the WSIs. This framework can dynamically and attentively discover the discriminative regions across multiple resolutions of the WSIs. Within this framework, an Integrated Attention Transformer is proposed to further enhance the performance of the transformer and obtain a more holistic WSI (bag) representation. This transformer consists of multiple Integrated Attention Modules, which is the combination of a transformer layer and an aggregation module that produces a bag representation based on every instance representation in that bag. The experimental results show that our method achieved state-of-the-art performances on multiple datasets, including Camelyon16, TCGA-RCC, TCGA-NSCLC, and an in-house IMGC dataset. The code is available at https://github.com/BearCleverProud/HAG-MIL.",
  "full_text": "Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided\nMultiple Instance Learning for Whole Slide Image Classification\nConghao Xiong1 , Hao Chen2 , Joseph J.Y. Sung3 and Irwin King1\n1Department of Computer Science and Engineering, The Chinese University of Hong Kong\n2Department of Computer Science and Engineering and Department of Chemical and Biological\nEngineering, The Hong Kong University of Science and Technology\n3Lee Kong Chian School of Medicine, Nanyang Technological University\n{chxiong21, king}@cse.cuhk.edu.hk, jhc@cse.ust.hk, josephsung@ntu.edu.sg\nAbstract\nMultiple Instance Learning (MIL) and transformers\nare increasingly popular in histopathology Whole\nSlide Image (WSI) classification. However, unlike\nhuman pathologists who selectively observe spe-\ncific regions of histopathology tissues under dif-\nferent magnifications, most methods do not incor-\nporate multiple resolutions of the WSIs, hierarchi-\ncally and attentively, thereby leading to a loss of fo-\ncus on the WSIs and information from other resolu-\ntions. To resolve this issue, we propose a Hierarchi-\ncal Attention-Guided Multiple Instance Learning\nframework to fully exploit the WSIs. This frame-\nwork can dynamically and attentively discover the\ndiscriminative regions across multiple resolutions\nof the WSIs. Within this framework, an Integrated\nAttention Transformer is proposed to further en-\nhance the performance of the transformer and ob-\ntain a more holistic WSI (bag) representation. This\ntransformer consists of multiple Integrated Atten-\ntion Modules, which is the combination of a trans-\nformer layer and an aggregation module that pro-\nduces a bag representation based on every instance\nrepresentation in that bag. The experimental re-\nsults show that our method achieved state-of-the-\nart performances on multiple datasets, including\nCamelyon16, TCGA-RCC, TCGA-NSCLC, and an\nin-house IMGC dataset. The code is available at\nhttps://github.com/BearCleverProud/HAG-MIL.\n1 Introduction\nHistopathology tissue analysis is widely recognised as the\ngold standard for cancer diagnosis, making histopathology\nWhole Slide Image (WSI) analysis an important deep learn-\ning application in the real world. Due to the advance-\nments of scanning and storage devices, the number of dig-\nitalised histology tissues, i.e., WSIs, is increasing. This\nmakes deep learning methods applicable to WSI classifi-\ncation tasks [Dimitriou et al., 2019; Srinidhi et al., 2021;\nLi et al., 2022], and there are already some successful appli-\ncations on breast [Huang et al., 2006; Peng and King, 2008],\nprostate [Campanella et al., 2019 ], skin [Ianni et al., 2020 ],\nLevel 0\n40\nLevel 1\n20\nLevel 2\n10\n Level 2 MIL Model\nLevel 0 MIL Model\nLevel 1 MIL Model\nPyramid\nStructure\nMIL Model\nFigure 1: An illustration of the difference between AB-MIL and\nthe HAG-MIL framework. AB-MIL (left) only uses one resolu-\ntion of the WSI, whereas the HAG-MIL framework (right) hierar-\nchically and attentively uses multiple resolutions of the WSI. The\nhigh-attention regions are passed to the models at higher resolutions\nfor further checking.\nand pancreas cancer [Keikhosravi et al., 2020].\nThough deep learning is a common solution to computer\nvision [He et al., 2016; Liu et al., 2021], WSIs, as gigapixel\nimages, pose a unique challenge for deep learning since they\nare too large to be fed into a neural network. The Multiple\nInstance Learning (MIL) [Maron and Lozano-P ´erez, 1997 ]\nframework provides a solution to this problem. Under this\nframework, the WSI is regarded as a bag of patches (in-\nstances) that are images cropped from the WSI, which can\nbe directly fed into a neural network [Ilse et al., 2018].\nMost MIL methods assume that the patches are indepen-\ndent and identically distributed [Ilse et al., 2018; Campanella\net al., 2019]. However, patches have strong correlations with\neach other, e.g., a tumour patch might have similar morpho-\nlogical structures to another tumour patch in the same WSI.\nNeglecting these correlations may result in insufficient ex-\nploitation of the WSI. The transformer [Vaswaniet al., 2017]\nis well-suited for MIL with patch correlations, as it can cap-\nture the dependencies between patches [Shao et al., 2021].\nThe lack of fine-grained annotations is another obstacle\nto achieving good performance with deep learning models.\nDue to the large size of the WSI, it can take several hours\nfor an expert pathologist to annotate each pixel, which is\nprohibitively time-consuming and expensive; therefore, the\nWSI datasets usually include only slide-level labels. Further-\nmore, the malignant tissues may only occupy less than 10%\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1587\nof the entire WSI [Litjens et al., 2018 ], making most of the\nregions irrelevant to the bag label “tumour”. It is therefore\nimportant to find the discriminative patches from WSIs since\nthey are the only patches providing strong indications of the\nbag label. These factors also contribute to the thriving of\nthe weakly-supervised learning on WSI classification tasks\n[Lu et al., 2019; Chikontwe et al., 2020; Zhao et al., 2020;\nLu et al., 2021; Tourniaire et al., 2021; Li et al., 2021a;\nLi et al., 2021b], as they do not need pixel-level annotations.\nIdentifying the discriminative patches in a large dataset\ncan be a challenging task, akin to finding a needle in a\nhaystack. The WSI datasets often have less than a thou-\nsand of WSIs whereas hundreds of thousands of patches can\nbe cropped from each WSI. These discriminative patches\nare overwhelmed by a flood of irrelevant ones, preventing\nthe model from learning the most essential features of the\npatches. In fact, many works address this issue through tech-\nniques like contrastive learning [Li et al., 2021a; Wang et al.,\n2021; Tu et al., 2022], attention mechanism [Ilse et al., 2018;\nTourniaire et al., 2021; Lu et al., 2021; Li et al., 2021b], hard\nmining [Li et al., 2019], and so on [Wang et al., 2020].\nWe address the problem of finding the most discriminative\npatches from the following two aspects.\nMulti-resolution Previous methods for WSI classification\noften attach little importance to utilisation of multiple reso-\nlution explicitly [Chen et al., 2022]. For instance, a patch at\n40× magnification (level 0) may contain fine-grained cellu-\nlar features, such as stroma, tumour cells, lymphocytes and\nother texture and morphology features. However, a patch at\n10× magnification (level 2) may have the tissue microenvi-\nronment, including the organisations of the cell structures and\ninteractions between clusters of cells and so on. Using only\none resolution leads to a lack of information and multiple res-\nolutions should be exploited. Unlike current methods, human\npathologists intentionally inspect histopathology tissues un-\nder multiple magnifications, switching their focus on differ-\nent regions and magnifications of the WSIs based on their\nprior knowledge and observations. They diagnose those in\na hierarchical and attentive way, which is not the case with\ncurrent model behaviours. Therefore, to incorporate multiple\nresolutions of the WSIs and from being inspired by the human\npathologists, we propose a Hierarchical Attention-Guided\nMultiple Instance Learning (HAG-MIL) framework to min-\nimise the disparity between model and human behaviours.\nFigure 1 illustrates how human pathologists make diag-\nnoses and how our framework is designed to resemble their\napproach. Information from lower resolutions can guide the\npathologists to regions of interest at higher resolutions for\nfurther refinement of their judgment. Similarly, the HAG-\nMIL framework first inspects patches from lower resolutions,\ndiscovers the suspicious regions and passes them to another\nmodel at higher resolutions for further confirmation. The pro-\ncess is repeated until the highest resolution is reached. By\nreserving only a certain number of patches at each resolu-\ntion, the number of patches in a bag is maintained below a\nspecified bag size. This enables the models to operate at the\nhighest resolution of the WSIs. Also, by eliminating irrele-\nvant patches, the models can concentrate more on the most\ndiscriminative ones, thereby improving their performance.\nStructure Flaws in the Transformer StructureAlthough\ntransformers are popular models in all fields, they only out-\nput one bag representation at the last layer using the class\ntoken [Devlin et al., 2019; Shao et al., 2021; Chen et al.,\n2022]. This structure has two shortcomings: (1) the bag\nrepresentation should be obtained based on all instance rep-\nresentations, not just the class token and (2) the bag repre-\nsentation can evolve through different transformer layers and\nrelying solely on the bag representation from the last layer\nmay result in a loss of information from previous bag rep-\nresentations. Hence, we propose the Integrated Attention\nTransformer (IAT), consisting of multiple stacked Integrated\nAttention Modules (IAMs). This module uses an aggregation\nmodule to aggregate all instance representations into a bag\nrepresentation, instead of relying on the class token. Multiple\nbag representations from different modules are fused into a\nfinal bag representation, which allows the transformer model\nto take into account the evolution of bag representations and\nproduce a more effective and holistic bag representation.\nIn summary, the benefits of our method include: (1) the\nalignment of our method with the visual assessment of human\npathologists by its effective utilisation of multiple resolutions\nof the WSIs, (2) preventing discriminative patches from being\noverwhelmed by irrelevant ones by discarding certain amount\nof irrelevant patches from the WSI bags, (3) enabling the\nmodels performing at the highest resolutions, and (4) gen-\nerating a more holistic and effective bag representation for\nthe WSI by taking into account all instance representations in\nthat bag and the evolution of the bag representation through\ndifferent layers. Our contributions are summarised below:\n1. We present the HAG-MIL framework to dynamically, at-\ntentively and hierarchically locate the most discrimina-\ntive patches across multiple resolutions of the WSIs like\na human pathologist.\n2. We propose the IAT to produce a more effective and\nholistic bag representation by generating a bag represen-\ntation in each IAM based on every instance representa-\ntion, and then fusing them together.\n3. We conduct extensive experiments on various datasets\nincluding Camelyon16, TCGA-RCC, TCGA-NSCLC,\nand our in-house IMGC dataset. The results demonstrate\nthat our method consistently outperforms other methods.\n2 Related Work\n2.1 MIL in WSI Analysis\nMIL can be divided into two approaches: (1) the instance-\nlevel and (2) the embedding-level approaches. In the first\napproach, a classifier is trained at the instance level using\nbag labels, and the predicted instance labels are then aggre-\ngated to form the bag label prediction [Hou et al., 2016;\nCampanella et al., 2019; Kanavati et al., 2020; Chikontwe\net al., 2020 ]. In the second approach, a bag representation\nis first obtained from instance representations, and a classi-\nfier is trained on the bag representations [Ilse et al., 2018;\nHashimoto et al., 2020; Liet al., 2020; Tourniaireet al., 2021;\nLu et al., 2021; Li et al., 2021a; Shao et al., 2021; Zhang et\nal., 2022]. The second approach is shown to be superior to the\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1588\nfirst one, as the instance labels tend to be noisy [Wang et al.,\n2018]; therefore, in our work, we adopt the second approach.\nDespite its effectiveness, MIL has an apparent disadvan-\ntage that it treats every patch in a bag as equally important.\nPatches should be assigned different importance scores so\nthat the discriminative patches have high importance scores.\nAttention-Based MIL (AB-MIL) [Ilse et al., 2018] calculates\nthe attention scores from instance representations and the at-\ntention scores reflect the importance of the corresponding\npatches. In this way, the model obtains the bag representation\nby taking a weighed average of the instance representations.\nMany recent works adapt the idea from AB-MIL [Li et al.,\n2021a; Lu et al., 2021; Zhang et al., 2022].\n2.2 Transformers in WSI Analysis\nTransformers [Vaswani et al., 2017; Liu et al., 2021 ] have\ndemonstrated their efficacy due to their ability to capture\nlong-range dependencies. There are also many works on WSI\nclassification tasks that utilise transformers to model correla-\ntions between patches [Shao et al., 2021; Huang et al., 2021;\nMyronenko et al., 2021; Chenet al., 2021; Zhenget al., 2022;\nChen et al., 2022]. In our work, we further improve the qual-\nity of the bag representation using an aggregation module,\nwhich produces a bag representation based on all instance\nrepresentations, and bag representations from different IAMs\nare then fused into a holistic and effective bag representation.\n2.3 Utilising Multiple Resolutions of the WSIs\nPathologists make diagnoses with multiple magnifications\nof histopathology tissues, and it is useful to utilise multi-\nple resolutions of the WSIs [Bejnordi et al., 2015; Toku-\nnaga et al., 2019; Hashimoto et al., 2020; Li et al., 2021a;\nZhou and Lu, 2022]. However, the previous works only used\nmultiple resolutions in the same bag [Hashimoto et al., 2020]\nor concatenate the features from different resolutions together\n[Li et al., 2021a]. Moreover, the two-stage training can be re-\nduced to an end-to-end process [Zhou and Lu, 2022]. Differ-\nent from other works, we resemble the pathologists and make\nuse of multiple resolutions in a hierarchical and attentive way.\n3 Methodology\n3.1 Problem Formulation\nGiven the features extracted by a pre-trained neural network\nF ∈ Rn×d, where n is the number of patches in the WSI and\nd is the hidden dimension of the pre-trained neural network,\nthe MIL model predicts the bag label of that WSI slideˆY with\nonly the ground truth bag label Y supervision available. The\nbag label Y would be positive if and only if there is at least\none positive instance in the bag, and it is defined as Eq. (1):\nY =\n\u001a1 ∃i, yi = 1,\n0 ∀i, yi = 0, (1)\nwhere yi is the ground truth label of the i-th patch. One issue\nwith this formulation is that it cannot identify the discrim-\ninative instances, since each instance has the same weight\nunder this framework. AB-MIL [Ilse et al., 2018 ] is thus a\nsuitable solution. AB-MIL assigns attention scores to the in-\nstances, which reflect the importance of each instance in the\nfinal decision-making stage. Mathematically, assuming the\nhidden representation of the i-th instance is hi ∈ R1×d with\nthe corresponding attention score ai, then the bag-level rep-\nresentation hb ∈ R1×d0 is calculated through Eq. (2):\nhb =\nnX\ni=1\naihi. (2)\nUnder the AB-MIL framework, the characteristic of WSIs\nhaving multiple resolutions is neglected. In our work, we\nincorporate multiple resolutions of WSIs with AB-MIL and\npropose the HAG-MIL framework. The HAG-MIL frame-\nwork is designed based on AB-MIL and its motivation is that\nthe attention scores from lower resolutions can be used as a\ncompass to guide the model at a higher resolution. The dif-\nferences between AB-MIL and our HAG-MIL framework are\nillustrated in Fig. 1. The inputs of the HAG-MIL framework\nare the features from different resolutions, and after the model\nat the lowest resolution has assigned attention scores to the\nfeatures, the features with low attention in the higher resolu-\ntion are discarded in the model at a higher resolution. The\nprocess is repeated until the highest resolution is reached.\n3.2 Hierarchical Attention-Guided MIL\nThe overview of our proposed HAG-MIL framework is illus-\ntrated in Fig. 2. Given the WSI resolutions level l to level\n0, the instance representations (Fl, ··· , F0) extracted from\nmultiple resolutions of any given WSI slide and the ground\ntruth bag label Y , the HAG-MIL framework learns a map\nfrom the instance representations (Fl, ··· , F0) to the proba-\nbility distribution of the bag label (ˆpl, ··· , ˆp0) and the atten-\ntion scores of each instance (al, ··· , a0).\nOur HAG-MIL framework is defined in a recursive way.\nGiven the instance representations (Fl, ··· , F0), the IATs\non different levels of resolutions I = {Ij|j = 0, ··· , l}, the\nHAG-MIL framework is given in Eq. (3):\naj, ˆpj =\n\n\n\nIj\n\u0000\nFj\u0001\n, if j = l,\nIj\n\u0010\nFj\nd\n\u0011\n, if j ̸= l, (3)\nwhere Fj\nd is the distilled features at levelj, defined in Eq. (4):\nFj\nd = Fj \u0002\nfind\ntopk ids\n\u0000\naj+1, kj+1\n\u0001\u0003\n, (4)\nwhere kj+1 is the number of highest attention instances to\nbe reserved at level j + 1, and find topk ids(aj+1, kj+1)\nfirst finds the indices of the instances with top-kj+1 attention\nscores at level j + 1 and then generate the distilled indices of\ninstances at levelj, which contains exactly the same content\nas the instances at level j + 1.\nFinding the instances in different resolutions containing\nexactly the same content is a bit tricky, since patching and\nsegmenting outcomes tend to be different in different resolu-\ntions, especially near the borderlines. For example, as shown\nin Fig. 2, patch i at level 1 has four sub-patches at level 0,i.e.,\n4i, 4i + 1,4i + 2,4i + 3; however, when we run the patching\nand segmenting algorithm at level 0 resolution, patches4i+2\nand 4i + 3will be dropped since they are backgrounds, while\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1589\nGELU Layer\nLayer Normalisation\nFully Connected Layer\nFNN\nInput Feature Vectors\nTransformer Layer\nLayer Normalisation\nFully Connected Layer\nFNN\nFully Connected Layer\nTanh Function\nFully Connected Layer\nSigmoid Function\nFully Connected Layer\nAggregation\nModule\nGELU Layer\nFeature Vectors at\nDifferent Resolutions\nPatch Images at\nDifferent Resolutions\n......\n......\nPre-trained Neural Network\nIAT at Level 1\nFind Top-k Ids\n......\nFeature \nFeature \nIAT at Level 0\n......\nDistilled Feature\nVectors\n......\nFeature \n......\nFeature \nFeature \nFeature \n......\nFeature \n Feature \n......\nFeature \n Feature \n......\nFeature \n Feature \n......\nFeature \n Feature \n......\n......\nFigure 2: Overview of the HAG-MIL framework (left) and the model architecture of our proposed transformer model IAT (right). The patch\nimages and the feature vectors bounded by red boxes are the discriminative ones and those in blue are less discriminative. The patch images\ncropped from different resolutions of WSIs are fed into a pre-trained network to obtain the feature vectors. The feature vectors are then passed\nto the framework in a hierarchical way to obtain the attention score a1 and a0, and the probability distribution of the bag label ˆp1 and ˆp0.\nfor the patch at level 1, patchi as a whole is reserved. If some\npatches are dropped as stated before and some are not, since\nthey are not near the borderlines, it is difficult to retrieve the\nsub-patches by indexing in higher resolutions.\nTo retrieve patches more effectively, a quadtree structure\nfor each WSI is constructed. This structure utilises the hier-\narchical nature of WSIs, where one patch node has four child\nnodes that contain the exact same content. As shown in Fig. 2,\nthe patches are stored in the same sequence so that there is\na clear map from an index in lower resolution to indices in\nhigher resolutions, which is given in Eq. (5):\nfind\nsub patch ids(i) = (4i, 4i + 1, 4i + 2, 4i + 3). (5)\nThen the find topk ids can be expressed as Eq. (6):\nfind topk ids(aj+1, kj+1)\n= find sub patch ids(argsort(aj+1, kj+1)).\n(6)\nAfter obtaining the pair of bag label probability distribution\nˆpj and attention scores aj at level j, the model at level j is\nupdated through back-propagation. Following the loss func-\ntions of CLustering-constrained-Attention Multiple-instance\nlearning (CLAM) [Lu et al., 2021 ], the loss function in our\nHAG-MIL framework is calculated through Eq. (7):\nLj =\nNX\ni=1\nCE\n\u0010\nˆpj, Y\n\u0011\n+ λ SS\n\u0010\nˆpj\np, yj\np\n\u0011\n, (7)\nwhere CE(·, ·) and SS(·, ·) stand for the Cross-Entropy loss\nand the Smooth Support Vector Machine loss [Berrada et al.,\n2018], λ is the balancing coefficient, ˆpj\np ∈ Rk×C is the patch\nlabel probability distribution of the top-k highest attention\npatches and yj\np = [ Y, ··· , Y]T ∈ Rk×1 are the generated\nlabels of the top-k highest attention patches. The patch-level\nloss is designed upon the idea that the patches with the highest\nattention scores should have the same label as the bag label.\n3.3 Integrated Attention Transformer\nThe architecture of the proposed model together with the\nHAG-MIL framework is illustrated in Fig. 2. The vanilla\ntransformer layer outputs only the contextual hidden repre-\nsentation of the input. In contrast, our IAM produces not only\nthe contextualised hidden representation of the input but also\nthe attention scores and the bag representation of the WSI\nslide. Furthermore, our IAM does not require a class token\nand comprises three constituents.\nFeedforward Neural Network (FNN) The FNN in Fig. 2\nis a sequence of modules, a fully connected layer, a layer\nnormalisation module, and a Gaussian Error Linear Unit\n(GELU) layer [Hendrycks and Gimpel, 2016 ]. Given the\ninput feature matrix H ∈ Rn×d1 and a learnable matrix\nW ∈ Rd1×d2 , the FNN(d1, d2) is given in Eq. (8):\nFNN(d1, d2) (H) = GELU (LayerNorm (HW )) . (8)\nTransformer Layer After the first FNN, we incorporate\na transformer layer [Vaswani et al., 2017 ] to capture latent\nrelationships between patches and provide cross-patch at-\ntention and additional contextual information to the original\nrepresentations extracted by the pre-trained neural network.\nHowever, the time and space complexity of the vanilla trans-\nformer layer is O(n2), where n is the number of patches\nand it is usually extremely huge for WSIs. To reduce time\nand memory costs, we employ Nystr ¨om attention [Xiong et\nal., 2021] as our base transformer layer [Shao et al., 2021 ],\nwhich has achieved competitive performance with the vanilla\ntransformer layer and the time and space complexity are both\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1590\nO(n). In addition, we also include the adaptive model initial-\nization [Liu et al., 2020] to stabilise the training process.\nAggregation Module We use the gated attention mecha-\nnism [Ilse et al., 2018] as the aggregation module in our work,\nand we formulate it as a two-component module that can be\neasily extended in future research.\nThe aggregation module can be divided into two compo-\nnents: (1) the attention component s(·) : Rn×d → Rn×1,\nwhich assigns attention scores a = s(H) ∈ Rn×1 to the\ninstances in the bag, and (2) the aggregation component\ng(·, ·) : ( Rn×1, Rn×d) → R1×d, which takes the attention\nscores a and the instance representations H as inputs and\nproduces the bag representation hb. The aggregation module\nfa(·) : Rn×d → R1×d is the composition of the two compo-\nnents as described in Eq. (9):\nhb = fa(H) = g(s(H), H). (9)\nThere are multiple options for the attention component s(·),\nincluding mean pooling, max pooling, the attention mecha-\nnism, the gated attention mechanism and so on. We propose\nusing the gated attention mechanism [Ilse et al., 2018] as the\naggregation module s(·) and it is shown in Eq. (10):\na = s(H) = (tanh (HV ) ⊙ σ (HU )) wa, (10)\nwhere wa ∈ Rd2×1, V , U ∈ Rd1×d2 are learnable matrices,\nH ∈ Rn×d1 is the hidden representation of the instances,\nσ(·) is the sigmoid function, tanh(·) is the tanh function and\n⊙ stands for the element-wise production.\nThe aggregation component g(·, ·) can be the weighted av-\nerage on the instance representation based on the attention\nscores. The bag representation hb ∈ R1×d is given according\nto Eq. (2). Therefore, fa(·) in our paper is given by Eq. (11):\nfa(H) = ((tanh (HV ) ⊙ σ (HU )) wa)T H. (11)\nFinally, since an IAT consists of m IAMs, it produces m\nbag representations. Bag representations from different IAMs\nmay capture different features of the bag; therefore, different\nbag representations should have different importance scores\nwhen aggregated into the final bag representation. Hence, we\nuse an attention mechanism to fuse the bag representations\nfrom each IAM into the IAT bag representation. Assuming\nthe bag representation from thei-th IAM ishbi , and the learn-\nable vector wb ∈ Rm×1 with each element corresponding\nto the contribution of each hbi , the final bag representation\nhbf ∈ R1×d is given in Eq. (12):\nhbf = wT\nb\n\n\nhb1\n..\n.\nhbm\n\n. (12)\nThe last step is to feed the hbf into a classification layer to\nobtain the bag-level distribution ˆp.\n4 Experiments and Results\nWe first compared our method with other methods on various\ndatasets. Then we conducted extensive sensitivity and ab-\nlation studies to validate the effectiveness of the HAG-MIL\nframework and the aggregation module. Finally, we visu-\nalised the attention scores from our method and compared\nthem with those from the CLAM model.\n4.1 Dataset Descriptions\nCamelyon16 Dataset [Litjens et al., 2018 ] It contains 397\nWSIs of lymph nodes with and without metastasis in tissue\nsections of women with breast cancer. The training dataset\nincludes 157 normal WSIs and 111 tumour WSIs. The test\ndataset is provided separately and contains 129 test WSIs, in-\ncluding both tumour and normal WSIs. We further split the\ntraining dataset into a new training and validation dataset by\n7:3 and evaluate all of the methods on the official test dataset.\nIMGC Dataset The Intestinal Metaplasia Gastric Cancer\n(IMGC) dataset is an in-house dataset consisting of 1,882\nWSIs, obtained from the Periodic Acid-Schiff-Alcian Blue\nstained biopsies of the patients diagnosed with intestinal\nmetaplasia. The patients were followed to determine whether\nthey ultimately developed gastric cancer. We used only a por-\ntion of this dataset due to its extreme imbalance. The training\ndataset includes 130 normal and 43 cancer cases, the valida-\ntion dataset includes 34 normal and 13 cancer cases, and the\ntest dataset includes 253 normal and 18 cancer cases.\nTCGA-RCC Dataset It includes 940 WSIs in total. In-\ndividually, project Kidney Chromophobe Renal Cell Carci-\nnoma (TGCA-KICH) has 121 WSIs from 109 cases. Project\nKidney Renal Clear Cell Carcinoma (TCGA-KIRC) has 519\nWSIs from 513 cases, and project Kidney Renal Papillary\nCell Carcinoma (TCGA-KIRP) has 300 WSIs from 276\ncases. We split the dataset into training, validation, and test\ndatasets by the ratio of 6:1.5:2.5, respectively.\nTCGA-NSCLC Dataset It includes 1,053 WSIs in to-\ntal. Individually, project Lung Squamous Cell Carcinoma\n(TCGA-LUSC) has 512 WSIs from 478 cases, and project\nLung Adenocarcinoma (TCGA-LUAD) has 541 WSIs from\n478 cases. We split the dataset into training, validation, and\ntest datasets by the ratio of 6:1.5:2.5, respectively.\n4.2 Implementation Details\nEvaluation Metrics The evaluation criteria are Area Under\nthe Curve (AUC), F1, and accuracy scores. The AUC score\nreflects the overall ability to distinguish different classes. The\nF1 score measures the true performance when the data is im-\nbalanced. Together, these metrics can holistically assess the\nmodel performances. The thresholds for F1 and accuracy\nscores are determined by Youden’s index in the IMGC dataset\ndue to its extreme unbalance and they are set to 0.5 for other\ndatasets, following previous works.\nTraining Settings Baseline methods include max/mean\npooling, AB-MIL [Ilse et al., 2018 ], CLAM-SB, CLAM-\nMB [Lu et al., 2021 ], TransMIL [Shao et al., 2021 ], and\nDTFD-MIL [Zhang et al., 2022]. Training is terminated af-\nter 20 epochs of no decrease in validation loss. Official im-\nplementations of the methods were used when available, ev-\nery effort was made to reproduce the results. For the IMGC\ndataset, all methods were run at level 0 resolution. For other\ndatasets, baseline methods were run at level 1 resolution, fol-\nlowing their respective works. The models were run five\ntimes and the average performances were reported.\nHyper-parameters Details The learning rate and weight\ndecay are set to 10-5. Dropout is set to 0.25 for the gated atten-\ntion mechanism and 0.3 for the transformer layer. The Swin-\nTransformer [Liu et al., 2021] large is used to extract the fea-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1591\nMETHOD Camelyon16 Gastric\nAUC↑ F1↑ Accuracy↑ AUC↑ F1↑ Accuracy↑\nMax Pooling 0.7150.007 0.5490.012 0.6820.005 0.8710.007 0.6010.053 0.7760.071\nMean Pooling 0.6540.075 0.4370.035 0.6400.013 0.8390.011 0.5510.014 0.7040.020\nAB-MIL [Ilse et al., 2018] 0.9030.013 0.8500.018 0.8680.015 0.8860.016 0.5820.028 0.7430.038\nCLAM-SB [Lu et al., 2021] 0.8340.068 0.7820.059 0.8190.043 0.8380.045 0.5330.063 0.6730.091\nCLAM-MB [Lu et al., 2021] 0.8760.024 0.8000.031 0.8260.019 0.8730.018 0.5780.023 0.7420.033\nTransMIL [Shao et al., 2021] 0.8380.047 0.7870.033 0.8090.027 0.8340.016 0.5600.046 0.7310.082\nDTFD-MIL [Zhang et al., 2022] 0.9330.009 0.8580.017 0.8990.012 0.8900.011 0.5650.020 0.7250.030\nHAG-MIL (Ours) 0.9460.011 0.8740.025 0.8870.022 0.8900.011 0.6160.027 0.7810.027\nImprovement 0.013 0.016 -0.012 0.000 0.015 0.005\nTable 1: Results on Camelyon16 and our gastric cancer test set. The best ones are in red bold, and the second best ones are in blue italic\nunderline. The subscript in each cell is the standard derivation.\nMETHOD TCGA-RCC TCGA-NSCLC\nAUC↑ F1↑ Accuracy↑ AUC↑ F1↑ Accuracy↑\nMax Pooling 0.9700.000 0.8470.008 0.8800.005 0.8630.001 0.7740.002 0.7740.002\nMean Pooling 0.9590.010 0.8280.015 0.8620.014 0.8510.000 0.7770.002 0.7770.002\nAB-MIL [Ilse et al., 2018] 0.9780.002 0.8640.013 0.8920.010 0.9030.007 0.8170.010 0.8170.010\nCLAM-SB [Lu et al., 2021] 0.9700.007 0.8460.011 0.8780.010 0.9050.004 0.8240.007 0.8240.007\nCLAM-MB [Lu et al., 2021] 0.9750.005 0.8710.012 0.8950.010 0.9000.004 0.8180.010 0.8180.010\nTransMIL [Shao et al., 2021] 0.9720.003 0.8500.007 0.8760.011 0.9020.008 0.8250.015 0.8250.016\nDTFD-MIL [Zhang et al., 2022] 0.9760.009 0.8840.028 0.8980.027 0.9020.023 0.8300.027 0.8380.030\nHAG-MIL (Ours) 0.9820.001 0.8940.009 0.9140.006 0.9210.008 0.8490.015 0.8490.016\nImprovement 0.004 0.010 0.016 0.016 0.019 0.011\nTable 2: Results on TCGA-RCC and TCGA-NSCLC test set.\ntures from the IMGC dataset. ResNet-50 [He et al., 2016 ]\nis used for other datasets, following previous works. There-\nfore, the input dimension for the IMGC dataset and other\ndatasets are 1,536 and 1,024, respectively. The dimensions of\nthe stacked IAMs are determined via experimental validation.\nInterestingly, our experiment found that the optimal structure\nfor the model consisted of four modules with dimensions of\n1,024, 1,536, 512, and 1,024, respectively, forming a bottle-\nneck structure. The dimension of the final bag representation\nis 1,024. The loss functions for models at different resolu-\ntions are different because we tolerate slight errors in lower\nresolutions, but the model at the highest resolution must be as\naccurate as possible. Therefore, label smoothing is set to 0.1\nfor low resolutions, and is not used for the highest resolution.\nThe balancing coefficient λ is set to 1.\n4.3 Comparison with State-of-the-art Methods\nTable 1 and Table 2 show all results of the experiments. Our\nmethod had the highest AUC and F1 scores and the highest or\ncompetitive accuracy scores across all datasets, which proved\nthe effectiveness of all our proposed methods.\nCamelyon16 is quite challenging due to the small area of\ntumour regions. From Table 1, max/mean pooling can barely\nlearn the distinctions between tumour and normal slides, and\nthe CLAM and TransMIL are inferior to our method by at\nleast 4.3% on the AUC score. Our method achieved a 1.3%\nimprovement in AUC score and a 1.6% improvement in F1\nscore compared to DTFD-MIL, demonstrating its efficacy.\nFor the IMGC dataset, the fact that our method has\nachieved the best performance has proved that our proposed\nIAT is superior to other methods. Furthermore, the data im-\nbalance problem on the IMGC dataset is much more severe\nthan the other datasets, as the number of patients who de-\nveloped gastric cancer is much less than those who did not.\nHence, the result proved that our model performs well on ex-\ntremely imbalanced datasets as well.\nFor the TCGA-RCC and TCGA-NSCLC datasets, they\nshare a common characteristic that the tumour regions are\nlarge, which sometimes takes around 80% of the whole WSI,\nmaking most of the methods to have similar performance, and\neven the most naive approaches like mean pooling and max\npooling have achieved very high AUC scores. Our method\nhas outperformed all others in every metric on these two\ndatasets by a significant margin, particularly on the TCGA-\nNSCLC dataset. In addition, the highest AUC score of our\nmethod on the TCGA-RCC dataset demonstrated that our\nmodel also excels in the multi-class classification task.\n4.4 Sensitivity and Ablation Studies\nWe performed sensitivity and ablation analyses to assess the\nimpact of our HAG-MIL framework and IAT on performance.\nThe HAG-MIL Framework The number of patches (k j)\nreserved for higher resolution is a crucial parameter in our\nframework. kj should not be too small to avoid missing tu-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1592\nFigure 3: AUC, F1 and accuracy scores with different k values on\nthe Camelyon16 (left) and TCGA-RCC (right) datasets.\nMETHOD Camelyon16\nAUC↑ F1↑ Accuracy↑\nCLS w/o gated 0.9170.040 0.8340.026 0.8550.020\nCLS w/ gated 0.9290.020 0.8550.044 0.8690.038\nAggr w/o gated 0.9280.012 0.8480.040 0.8630.038\nAggr w/ gated 0.9460.011 0.8740.025 0.8870.022\nTable 3: The results on the use of several aggregation modules and\nthe class token on the Camelyon16 dataset.\nmour patches, but it should also not be too large to avoid time\nand GPU memory issues. Since tumour regions are small in\nCamelyon16 and large in the TCGA series dataset, they tend\nto have different trends with changes ofkj; therefore, we con-\nducted experiments on both Camelyon16 and TCGA-RCC\ndatasets. For convenience, we set k1 = k2. Eight different\nkj values are sampled within the range of [0, 7,000] at equal\nintervals, with the starting point of kj = 7. In general, larger\nvalues of kj tend to result in improved model performance.\nHowever, the extent of this influence varies across different\ndatasets. The kj value heavily influences how the model per-\nforms on Camelyon16 due to the small tumour regions. Our\nfindings also indicate that increasing the value of kj during\nthe inference stage can further enhance model performance.\nHowever, the kj value does not influence the performance on\nthe TCGA-RCC dataset much because the tumour regions are\nlarge, and there is a high probability that the true discrimina-\ntive patches are in the top-kj patches even when kj = 7.\nAggregation Module We compared transformers with and\nwithout an aggregation module and included two types of ag-\ngregation modules in our experiments: the attention mech-\nanism [Ilse et al., 2018 ] and the gated attention mechanism\n[Ilse et al., 2018]. In order for a fair comparison, the patch-\nlevel loss is also included in transformers using the class to-\nken, and the attention scores used by our HAG-MIL frame-\nwork are generated by an aggregation module in the final\nlayer. The bag representations are still produced by the class\ntoken. The results are shown in Table 3. From these re-\nsults, we can draw two conclusions: (1) the gated attention\nmechanism is a better option for the aggregation module as\nit outperforms the attention mechanism transformer by 1.8%\nand 1.2% in terms of the AUC score, and (2) our transformer\nwith both two aggregation modules outperform the class to-\nken transformer by 1.7% and 1.1% in terms of the AUC score.\n(a) The original image (b) CLAM heatmap (c) IAT heatmap\n2mm \n8mm \nFigure 4: Figures depicting a WSI from Camelyon16 with ground\ntruth annotations (a), alongside heatmaps generated by CLAM-SB\n(b) and IAT (c). The second row of figures displays magnified views\nof the regions enclosed by black boxes in the first row.\n4.5 Visualisation of the Attention Scores\nWe demonstrate the interpretability of our method by visu-\nalising the attention scores as heatmaps, which indicate the\nlocations of the discriminative regions. For this experiment,\nwe used the Camelyon16 dataset with ground truth pixel-level\nannotations and compared the heatmaps of our method with\nthose of the CLAM-SB model. The original image and vi-\nsualisation results are shown in Fig. 4. The ground truth tu-\nmour regions are bounded by green lines and the red regions\nin the heatmaps indicate the model-predicted high-attention\nregions. While the CLAM-SB model can capture most of\nthe macro-metastasis in the WSI, it has a much higher false\npositive rate than our method. Additionally, the CLAM-SB\nmodel performs poorly at identifying micro-metastasis and\nisolated tumour cells due to their small size. However, our\nmethod accurately predicts macro-metastasis and can also lo-\ncalise micro-metastasis and isolated tumour cells with rela-\ntively smaller attention scores compared to macro-metastasis.\nIt is clear that heatmaps from our method have better contrast\nand higher consistency with ground truth annotations.\n5 Conclusion\nIn this paper, we present the HAG-MIL framework and the\nIAT for WSI classification tasks. The HAG-MIL framework\nfully utilises the WSIs by exploiting multiple resolutions of\nthe WSIs, and it enables the large models, like our IAT, to run\nat the highest resolution of the WSIs. Diagnosing like a hu-\nman pathologist, the HAG-MIL framework is capable of dy-\nnamically localising the most discriminative instances from\nthe enormous number of instances in the WSI bag across mul-\ntiple resolutions in a hierarchical and attentive way. Within\nthe HAG-MIL framework, the IAT takes into account the\nevolvement of the bag representations through different trans-\nformer layers. It produces a more holistic and effective bag\nrepresentation than the vanilla transformer by aggregating\nmultiple bag representations generated by the IAMs. The ex-\nperimental results show that our method outperforms other\nmethods and obtains a more accurate heatmap. Our method\ncan be further extended to more applications such as super-\nresolution image analysis, natural language processing, etc.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1593\nAcknowledgments\nThe work described here was partially supported by grants\nfrom the National Key Research and Development Program\nof China (No. 2018AAA0100204) and from the Research\nGrants Council of the Hong Kong Special Administrative Re-\ngion, China (CUHK 14222922, RGC GRF, No. 2151185).\nThe results shown here are based upon data generated by the\nTCGA Research Network: https://www.cancer.gov/tcga.\nReferences\n[Bejnordi et al., 2015] Babak Ehteshami Bejnordi, Geert\nLitjens, Meyke Hermsen, Nico Karssemeijer, and Jeroen\nA. W. M. van der Laak. A multi-scale superpixel classi-\nfication approach to the detection of regions of interest in\nwhole slide histopathology images. In Medical Imaging\n2015: Digital Pathology, 2015.\n[Berrada et al., 2018] Leonard Berrada, Andrew Zisserman,\nand M. Pawan Kumar. Smooth loss functions for deep top-\nk classification. In ICLR, 2018.\n[Campanella et al., 2019] Gabriele Campanella, Matthew G.\nHanna, Luke Geneslaw, Allen Miraflor, Vitor Werneck\nKrauss Silva, Klaus J. Busam, Edi Brogi, Victor E. Reuter,\nDavid S. Klimstra, and Thomas J. Fuchs. Clinical-\ngrade computational pathology using weakly supervised\ndeep learning on whole slide images. Nature Medicine,\n25(8):1301–1309, August 2019.\n[Chen et al., 2021] Richard J. Chen, Ming Y . Lu, Wei-Hung\nWeng, Tiffany Y . Chen, Drew FK. Williamson, Trevor\nManz, Maha Shady, and Faisal Mahmood. Multimodal co-\nattention transformer for survival prediction in gigapixel\nwhole slide images. In ICCV, 2021.\n[Chen et al., 2022] Richard J. Chen, Chengkuan Chen, Yi-\ncong Li, Tiffany Y . Chen, Andrew D. Trister, Rahul G. Kr-\nishnan, and Faisal Mahmood. Scaling vision transformers\nto gigapixel images via hierarchical self-supervised learn-\ning. In CVPR, 2022.\n[Chikontwe et al., 2020] Philip Chikontwe, Meejeong Kim,\nSoo Jeong Nam, Heounjeong Go, and Sang Hyun Park.\nMultiple Instance Learning with Center Embeddings for\nHistopathology Classification. In MICCAI, 2020.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In NAACL, June 2019.\n[Dimitriou et al., 2019] Neofytos Dimitriou, Ognjen Arand-\njelovi´c, and Peter D. Caie. Deep learning for whole slide\nimage analysis: An overview.Frontiers in Medicine, 2019.\n[Hashimoto et al., 2020] Noriaki Hashimoto, Daisuke\nFukushima, Ryoichi Koga, Yusuke Takagi, Kaho Ko, Kei\nKohno, Masato Nakaguro, Shigeo Nakamura, Hidekata\nHontani, and Ichiro Takeuchi. Multi-scale Domain-\nadversarial Multiple-instance CNN for Cancer Subtype\nClassification with Unannotated Histopathological\nImages. In CVPR, June 2020.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\n[Hendrycks and Gimpel, 2016] Dan Hendrycks and\nKevin Gimpel. Gaussian error linear units (gelus).\narXiv:1606.08415 [cs.LG], 2016.\n[Hou et al., 2016] Le Hou, Dimitris Samaras, Tahsin M.\nKurc, Yi Gao, James E. Davis, and Joel H. Saltz. Patch-\nbased convolutional neural network for whole slide tissue\nimage classification. In CVPR, 2016.\n[Huang et al., 2006] Kaizhu Huang, Haiqin Yang, Irwin\nKing, and M.R. Lyu. Maximizing sensitivity in medical di-\nagnosis using biased minimax probability machine. IEEE\nTransactions on Biomedical Engineering, 53(5):821–831,\n2006.\n[Huang et al., 2021] Ziwang Huang, Hua Chai, Ruoqi Wang,\nHaitao Wang, Yuedong Yang, and Hejun Wu. Integra-\ntion of patch features through self-supervised learning and\ntransformer for survival analysis on whole slide images. In\nMICCAI, 2021.\n[Ianni et al., 2020] Julianna D. Ianni, Rajath E. Soans,\nSivaramakrishnan Sankarapandian, Ramachandra Vikas\nChamarthi, Devi Ayyagari, Thomas G. Olsen, Michael J.\nBonham, Coleman C. Stavish, Kiran Motaparthi, Clay J.\nCockerell, Theresa A. Feeser, and Jason B. Lee. Tailored\nfor Real-World: A Whole Slide Image Classification Sys-\ntem Validated on Uncurated Multi-Site Data Emulating\nthe Prospective Pathology Workload. Scientific Reports,\n10(1):3217, February 2020.\n[Ilse et al., 2018] Maximilian Ilse, Jakub Tomczak, and Max\nWelling. Attention-based Deep Multiple Instance Learn-\ning. In ICML 2018. PMLR, July 2018.\n[Kanavati et al., 2020] Fahdi Kanavati, Gouji Toyokawa,\nSeiya Momosaki, Michael Rambeau, Yuka Kozuma, Fu-\nmihiro Shoji, Koji Yamazaki, Sadanori Takeo, Osamu\nIizuka, and Masayuki Tsuneki. Weakly-supervised learn-\ning for lung carcinoma classification using deep learning.\nScientific Reports, 10(1):9297, June 2020.\n[Keikhosravi et al., 2020] Adib Keikhosravi, Bin Li, Yum-\ning Liu, Matthew W. Conklin, Agnes G. Loeffler, and\nKevin W. Eliceiri. Non-disruptive collagen characteriza-\ntion in clinical histopathology using cross-modality image\nsynthesis. Communications Biology, 3(1):1–12, July 2020.\n[Li et al., 2019] Meng Li, Lin Wu, Arnold Wiliem, Kun\nZhao, Teng Zhang, and Brian C. Lovell. Deep Instance-\nLevel Hard Negative Mining Model for Histopathology\nImages. arXiv:1906.09681 [cs], June 2019.\n[Li et al., 2020] Zhenliang Li, Liming Yuan, Haixia Xu, Rui\nCheng, and Xianbin Wen. Deep Multi-Instance Learning\nwith Induced Self-Attention for Medical Image Classifica-\ntion. In BIBM 2020, December 2020.\n[Li et al., 2021a] Bin Li, Yin Li, and Kevin W. Eliceiri.\nDual-stream multiple instance learning network for whole\nslide image classification with self-supervised contrastive\nlearning. In CVPR, 2021.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1594\n[Li et al., 2021b] Jiayun Li, Wenyuan Li, Anthony Sisk,\nHuihui Ye, W. Dean Wallace, William Speier, and\nCorey W. Arnold. A multi-resolution model for\nhistopathology image classification and localization with\nmultiple instance learning. Computers in Biology and\nMedicine, 131:104253, 2021.\n[Li et al., 2022] Xintong Li, Chen Li, Md Mamunur Ra-\nhaman, Hongzan Sun, Xiaoqi Li, Jian Wu, Yudong\nYao, and Marcin Grzegorzek. A comprehensive re-\nview of computer-aided whole-slide image analysis: from\ndatasets to feature extraction, segmentation, classification\nand detection approaches. Artificial Intelligence Review,\n55(6):4809–4878, August 2022.\n[Litjens et al., 2018] Geert Litjens, Peter Bandi, Babak Eht-\neshami Bejnordi, Oscar Geessink, Maschenka Balkenhol,\nPeter Bult, Altuna Halilovic, Meyke Hermsen, Rob van de\nLoo, Rob V ogels, Quirine F Manson, Nikolas Stathonikos,\nAlexi Baidoshvili, Paul van Diest, Carla Wauters, Marcory\nvan Dijk, and Jeroen van der Laak. 1399 H&E-stained\nsentinel lymph node sections of breast cancer patients: the\nCAMELYON dataset. GigaScience, 7(6), June 2018.\n[Liu et al., 2020] Liyuan Liu, Xiaodong Liu, Jianfeng Gao,\nWeizhu Chen, and Jiawei Han. Understanding the diffi-\nculty of training transformers. In EMNLP, 2020.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In ICCV, 2021.\n[Lu et al., 2019] Ming Y . Lu, Richard J. Chen, Jingwen\nWang, Debora Dillon, and Faisal Mahmood. Semi-\nSupervised Histology Classification using Deep Multi-\nple Instance Learning and Contrastive Predictive Coding.\narXiv:1910.10825 [cs, q-bio], November 2019. arXiv:\n1910.10825.\n[Lu et al., 2021] Ming Y . Lu, Drew F. K. Williamson,\nTiffany Y . Chen, Richard J. Chen, Matteo Barbieri, and\nFaisal Mahmood. Data-efficient and weakly supervised\ncomputational pathology on whole-slide images. Nature\nBiomedical Engineering, 5(6):555–570, June 2021.\n[Maron and Lozano-P´erez, 1997] Oded Maron and Tom ´as\nLozano-P´erez. A framework for multiple-instance learn-\ning. In Advances in NIPS, 1997.\n[Myronenko et al., 2021] Andriy Myronenko, Ziyue Xu,\nDong Yang, Holger R. Roth, and Daguang Xu. Accounting\nfor dependencies in deep learning based multiple instance\nlearning for whole slide imaging. In MICCAI, 2021.\n[Peng and King, 2008] Xiang Peng and Irwin King. Robust\nBMPM training based on second-order cone programming\nand its application in medical diagnosis. Neural Networks,\n21(2-3):450–457, 2008.\n[Shao et al., 2021] Zhuchen Shao, Hao Bian, Yang Chen,\nYifeng Wang, Jian Zhang, Xiangyang Ji, and Yongbing\nZhang. Transmil: Transformer based correlated multiple\ninstance learning for whole slide image classification. In\nNIPS, 2021.\n[Srinidhi et al., 2021] Chetan L. Srinidhi, Ozan Ciga, and\nAnne L. Martel. Deep neural network models for com-\nputational histopathology: A survey. MIA, 2021.\n[Tokunaga et al., 2019] Hiroki Tokunaga, Yuki Teramoto,\nAkihiko Yoshizawa, and Ryoma Bise. Adaptive Weighting\nMulti-Field-Of-View CNN for Semantic Segmentation in\nPathology. In CVPR, June 2019.\n[Tourniaire et al., 2021] Paul Tourniaire, Marius Ilie, Paul\nHofman, Nicholas Ayache, and Herve Delingette.\nAttention-based Multiple Instance Learning with Mixed\nSupervision on the Camelyon16 Dataset. In MICCAI\nWorkshop on Computational Pathology, September 2021.\n[Tu et al., 2022] Chao Tu, Yu Zhang, and Zhenyuan Ning.\nDual-curriculum contrastive multi-instance learning for\ncancer prognosis analysis with whole slide images. In\nNeurIPS, 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, 2017.\n[Wang et al., 2018] Xinggang Wang, Yongluan Yan, Peng\nTang, Xiang Bai, and Wenyu Liu. Revisiting multiple in-\nstance neural networks. Pattern Recognition, 74, 2018.\n[Wang et al., 2020] Xi Wang, Hao Chen, Caixia Gan,\nHuangjing Lin, Qi Dou, Efstratios Tsougenis, Qitao\nHuang, Muyan Cai, and Pheng-Ann Heng. Weakly Super-\nvised Deep Learning for Whole Slide Lung Cancer Image\nAnalysis. IEEE Transactions on Cybernetics, 2020.\n[Wang et al., 2021] Xiyue Wang, Sen Yang, Jun Zhang,\nMinghui Wang, Jing Zhang, Junzhou Huang, Wei Yang,\nand Xiao Han. Transpath: Transformer-based self-\nsupervised learning for histopathological image classifica-\ntion. In MICCAI, 2021.\n[Xiong et al., 2021] Yunyang Xiong, Zhanpeng Zeng,\nRudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin\nLi, and Vikas Singh. Nystr ¨omformer: A nystr ¨om-based\nalgorithm for approximating self-attention. AAAI, 2021.\n[Zhang et al., 2022] Hongrun Zhang, Yanda Meng, Yitian\nZhao, Yihong Qiao, Xiaoyun Yang, Sarah E. Coupland,\nand Yalin Zheng. Dtfd-mil: Double-tier feature distilla-\ntion multiple instance learning for histopathology whole\nslide image classification. In CVPR, June 2022.\n[Zhao et al., 2020] Yu Zhao, Fan Yang, Yuqi Fang, Hailing\nLiu, Niyun Zhou, Jun Zhang, Jiarui Sun, Sen Yang, Bjoern\nMenze, Xinjuan Fan, and Jianhua Yao. Predicting lymph\nnode metastasis using histopathological images based on\nmultiple instance learning with deep graph convolution. In\nCVPR, June 2020.\n[Zheng et al., 2022] Yi Zheng, Rushin H. Gindra, Emily J.\nGreen, Eric J. Burks, Margrit Betke, Jennifer E. Beane,\nand Vijaya B. Kolachalama. A graph-transformer for\nwhole slide image classification. TMI, 2022.\n[Zhou and Lu, 2022] Yuanpin Zhou and Yao Lu. Deep hier-\narchical multiple instance learning for whole slide image\nclassification. In ISBI, 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1595",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.7585011720657349
    },
    {
      "name": "Computer science",
      "score": 0.7504112720489502
    },
    {
      "name": "Transformer",
      "score": 0.7212662696838379
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5526068806648254
    },
    {
      "name": "Feature learning",
      "score": 0.46661511063575745
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.46659353375434875
    },
    {
      "name": "Source code",
      "score": 0.43572354316711426
    },
    {
      "name": "Machine learning",
      "score": 0.40535473823547363
    },
    {
      "name": "Engineering",
      "score": 0.08072596788406372
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ],
  "cited_by": 39
}