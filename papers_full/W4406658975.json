{
  "title": "Current applications and challenges in large language models for patient care: a systematic review",
  "url": "https://openalex.org/W4406658975",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2131432264",
      "name": "Felix Busch",
      "affiliations": [
        "Technical University of Munich",
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2368277023",
      "name": "Lena Hoffmann",
      "affiliations": [
        "Freie Universität Berlin",
        "Charité - Universitätsmedizin Berlin",
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5009532076",
      "name": "Christopher Rueger",
      "affiliations": [
        "Humboldt-Universität zu Berlin",
        "Charité - Universitätsmedizin Berlin",
        "Freie Universität Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A4223688698",
      "name": "Elon Hc van Dijk",
      "affiliations": [
        "Sir Charles Gairdner Hospital",
        "Leiden University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2807926023",
      "name": "Rawen Kader",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2619691066",
      "name": "Esteban Ortiz Prado",
      "affiliations": [
        "Universidad de Las Américas"
      ]
    },
    {
      "id": "https://openalex.org/A2134728274",
      "name": "Marcus R. Makowski",
      "affiliations": [
        "Klinikum rechts der Isar",
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2120286100",
      "name": "Luca Saba",
      "affiliations": [
        "Azienda Ospedaliero-Universitaria Cagliari"
      ]
    },
    {
      "id": "https://openalex.org/A2079006570",
      "name": "Martin Hadamitzky",
      "affiliations": [
        "Technical University of Munich",
        "Deutsches Herzzentrum München"
      ]
    },
    {
      "id": "https://openalex.org/A2056217728",
      "name": "Jakob Nikolas Kather",
      "affiliations": [
        "Heidelberg University",
        "National Center for Tumor Diseases",
        "University Hospital Carl Gustav Carus",
        "University Hospital Heidelberg"
      ]
    },
    {
      "id": "https://openalex.org/A2029259259",
      "name": "Daniel Truhn",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A1264139375",
      "name": "Renato Cuocolo",
      "affiliations": [
        "University of Salerno"
      ]
    },
    {
      "id": "https://openalex.org/A2432098160",
      "name": "Lisa C. Adams",
      "affiliations": [
        "Technical University of Munich",
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2767442332",
      "name": "Keno K. Bressem",
      "affiliations": [
        "Deutsches Herzzentrum München",
        "Technical University of Munich",
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2131432264",
      "name": "Felix Busch",
      "affiliations": [
        "Technical University of Munich",
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2368277023",
      "name": "Lena Hoffmann",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5009532076",
      "name": "Christopher Rueger",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A4223688698",
      "name": "Elon Hc van Dijk",
      "affiliations": [
        "Sir Charles Gairdner Hospital",
        "Leiden University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2807926023",
      "name": "Rawen Kader",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2619691066",
      "name": "Esteban Ortiz Prado",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134728274",
      "name": "Marcus R. Makowski",
      "affiliations": [
        "Klinikum rechts der Isar",
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2120286100",
      "name": "Luca Saba",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2079006570",
      "name": "Martin Hadamitzky",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2056217728",
      "name": "Jakob Nikolas Kather",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2029259259",
      "name": "Daniel Truhn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1264139375",
      "name": "Renato Cuocolo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2432098160",
      "name": "Lisa C. Adams",
      "affiliations": [
        "Technical University of Munich",
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2767442332",
      "name": "Keno K. Bressem",
      "affiliations": [
        "Technical University of Munich",
        "Klinikum rechts der Isar"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4391940656",
    "https://openalex.org/W4402670290",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4362522726",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W4384455669",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W2560438049",
    "https://openalex.org/W2901724277",
    "https://openalex.org/W2599225410",
    "https://openalex.org/W2109305774",
    "https://openalex.org/W4387914853",
    "https://openalex.org/W4387002841",
    "https://openalex.org/W4283770909",
    "https://openalex.org/W4388022936",
    "https://openalex.org/W4387008051",
    "https://openalex.org/W4386714656",
    "https://openalex.org/W4386647035",
    "https://openalex.org/W4383618574",
    "https://openalex.org/W4386493914",
    "https://openalex.org/W4386245555",
    "https://openalex.org/W4386414742",
    "https://openalex.org/W4388449007",
    "https://openalex.org/W4387501860",
    "https://openalex.org/W4376637945",
    "https://openalex.org/W4372348574",
    "https://openalex.org/W4387968480",
    "https://openalex.org/W4366447635",
    "https://openalex.org/W4379231355",
    "https://openalex.org/W4386046428",
    "https://openalex.org/W4386735567",
    "https://openalex.org/W4389045841",
    "https://openalex.org/W4386692525",
    "https://openalex.org/W4387823013",
    "https://openalex.org/W4387817162",
    "https://openalex.org/W4382929886",
    "https://openalex.org/W4388835064",
    "https://openalex.org/W4386776401",
    "https://openalex.org/W4385459604",
    "https://openalex.org/W4386278138",
    "https://openalex.org/W4385707210",
    "https://openalex.org/W4388923875",
    "https://openalex.org/W4388975763",
    "https://openalex.org/W4389507451",
    "https://openalex.org/W4377157938",
    "https://openalex.org/W4388869624",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4367175507",
    "https://openalex.org/W4382791987",
    "https://openalex.org/W4297834221",
    "https://openalex.org/W4388745175",
    "https://openalex.org/W4389232726",
    "https://openalex.org/W4384922275",
    "https://openalex.org/W4383187389",
    "https://openalex.org/W4387440167",
    "https://openalex.org/W4385667643",
    "https://openalex.org/W4388078979",
    "https://openalex.org/W4388775426",
    "https://openalex.org/W4385708775",
    "https://openalex.org/W4388420209",
    "https://openalex.org/W4387496544",
    "https://openalex.org/W4385299173",
    "https://openalex.org/W4385997381",
    "https://openalex.org/W4386117070",
    "https://openalex.org/W4386151807",
    "https://openalex.org/W4386593417",
    "https://openalex.org/W4386304195",
    "https://openalex.org/W4388095746",
    "https://openalex.org/W4377563830",
    "https://openalex.org/W4381427645",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W4386730959",
    "https://openalex.org/W4390105593",
    "https://openalex.org/W4381469233",
    "https://openalex.org/W4379883463",
    "https://openalex.org/W4387880927",
    "https://openalex.org/W4387747149",
    "https://openalex.org/W4387522969",
    "https://openalex.org/W4389793507",
    "https://openalex.org/W4388014118",
    "https://openalex.org/W4389993479",
    "https://openalex.org/W4388840552",
    "https://openalex.org/W4388802460",
    "https://openalex.org/W4390103037",
    "https://openalex.org/W4389076631",
    "https://openalex.org/W4387047777",
    "https://openalex.org/W4385570062",
    "https://openalex.org/W4390065167",
    "https://openalex.org/W4366823098",
    "https://openalex.org/W4387772689",
    "https://openalex.org/W4390493646",
    "https://openalex.org/W4389777382",
    "https://openalex.org/W4376637205",
    "https://openalex.org/W4386757039",
    "https://openalex.org/W4389155212",
    "https://openalex.org/W4389505505",
    "https://openalex.org/W4386753580",
    "https://openalex.org/W4390538712",
    "https://openalex.org/W4383896353",
    "https://openalex.org/W4377234252",
    "https://openalex.org/W4391069573",
    "https://openalex.org/W6600280466",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W4368372176",
    "https://openalex.org/W4376640725",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4391831350",
    "https://openalex.org/W4402860127",
    "https://openalex.org/W3109650690",
    "https://openalex.org/W3035286874",
    "https://openalex.org/W2059552486",
    "https://openalex.org/W3124618094",
    "https://openalex.org/W2006171109",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W4384615637",
    "https://openalex.org/W4392193191",
    "https://openalex.org/W4376643691",
    "https://openalex.org/W4387952963",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4399528455",
    "https://openalex.org/W4401947350",
    "https://openalex.org/W4385476863",
    "https://openalex.org/W4392353733",
    "https://openalex.org/W4391995913",
    "https://openalex.org/W4224223437",
    "https://openalex.org/W4393868488",
    "https://openalex.org/W4401521710",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W4388370035",
    "https://openalex.org/W4388485443",
    "https://openalex.org/W4387451644"
  ],
  "abstract": "Abstract Background The introduction of large language models (LLMs) into clinical practice promises to improve patient education and empowerment, thereby personalizing medical care and broadening access to medical knowledge. Despite the popularity of LLMs, there is a significant gap in systematized information on their use in patient care. Therefore, this systematic review aims to synthesize current applications and limitations of LLMs in patient care. Methods We systematically searched 5 databases for qualitative, quantitative, and mixed methods articles on LLMs in patient care published between 2022 and 2023. From 4349 initial records, 89 studies across 29 medical specialties were included. Quality assessment was performed using the Mixed Methods Appraisal Tool 2018. A data-driven convergent synthesis approach was applied for thematic syntheses of LLM applications and limitations using free line-by-line coding in Dedoose. Results We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness, incorrectness, unsafety, and bias. Conclusions This review systematically maps LLM applications and limitations in patient care, providing a foundational framework and taxonomy for their implementation and evaluation in healthcare settings.",
  "full_text": "communicationsmedicine Article\nhttps://doi.org/10.1038/s43856-024-00717-2\nCurrent applications and challenges in\nlarge language models for patient care: a\nsystematic review\nCheck for updates\nFelix Busch 1 ,L e n aH o f f m a n n2,C h r i s t o p h e rR u e g e r2,E l o nH Cv a nD i j k3,4, Rawen Kader 5,\nEsteban Ortiz-Prado6, Marcus R. Makowski1,L u c aS a b a7, Martin Hadamitzky8,\nJakob Nikolas Kather 9,10,D a n i e lT r u h n11, Renato Cuocolo12,L i s aC .A d a m s1,13 & Keno K. Bressem1,8,13\nAbstract\nBackground The introduction of large language models (LLMs) into clinical practice\npromises to improve patient education and empowerment, thereby personalizing medical\ncare and broadening access to medical knowledge. Despite the popularity of LLMs, there is\na signiﬁcant gap in systematized information on their use in patient care. Therefore, this\nsystematic review aims to synthesize current applications and limitations of LLMs in\npatient care.\nMethods We systematically searched 5 databases for qualitative, quantitative, and mixed\nmethods articles on LLMs in patient care published between 2022 and 2023. From 4349\ninitial records, 89 studies across 29 medical specialties were included. Quality assessment\nwas performed using the Mixed Methods Appraisal Tool 2018. A data-driven convergent\nsynthesis approach was applied for thematic syntheses of LLM applications and limitations\nusing free line-by-line coding in Dedoose.\nResultsWe show that most studies investigate Generative Pre-trained Transformers (GPT)-\n3.5 (53.2%,n = 66 of 124 different LLMs examined) and GPT-4 (26.6%,n = 33/124) in\nanswering medical questions, followed by patient information generation, including medical\ntext summarization or translation, and clinical documentation. Our analysis delineates two\nprimary domains of LLM limitations: design and output. Design limitations include 6 second-\norder and 12 third-order codes, such as lack of medical domain optimization, data\ntransparency, and accessibility issues, while output limitations include 9 second-order and\n32 third-order codes, for example, non-reproducibility, non-comprehensiveness,\nincorrectness, unsafety, and bias.\nConclusions This review systematically maps LLM applications and limitations in patient\ncare, providing a foundational framework and taxonomy for their implementation and\nevaluation in healthcare settings.\nPublic and academic interest in large language models (LLMs) and their\npotential applications has increased substantially, especially since the\nrelease of OpenAI’s ChatGPT (Chat Generative Pre-trained Transfor-\nmers) in November 2022\n1–3. One of the main reasons for their popularity\nis the remarkable ability to mimic human writing, a result of extensive\ntraining on massive amounts of text and reinforcement learning from\nhuman feedback4.\nSince most LLMs are designed as general-purpose chatbots, recent\nresearch has focused on developing specialized models for the medical\ndomain, such as Meditron or BioMistral, by enriching the training data of\nLLMs with medical knowledge\n5,6. However, this approach toﬁne-tuning\nLLMs requires signiﬁcant computational resources that are not available to\neveryone and is also not applicable to closed-source LLMs, which are often\nthe most powerful. Therefore, another approach to improve LLMs for\nA full list of afﬁliations appears at the end of the paper.e-mail: felix.busch@tum.de\nPlain Language Summary\nLarge language models (LLMs) are computer\nprograms that can generate human-like text.\nThey promise to improve patient education\nand expand access to medical information by\nhelping patients better understand health\nconditions and treatment options. However,\nmore information is needed about how these\ntools are used in patient care and the\nchallenges they present. In this review,\nresearchers analyzed 89 studies from 2022 to\n2023 covering 29 medical specialties. These\nstudies explored ways LLMs are used: for\nexample, answering patient questions,\nsummarizing or translating medical texts, and\nsupporting clinical paperwork. While these\ntools show potential, the review highlights\nlimitations. Many LLMs are not optimized for\nmedical use, lack transparency about data\nuse, and can be difﬁcult for some users to\naccess. Additionally, the text they generate\nmay sometimes be inaccurate, incomplete, or\nbiased, raising safety concerns.\nCommunications Medicine|            (2025) 5:26 1\n1234567890():,;\n1234567890():,;\nbiomedicine is to use techniques suchas Retrieval-Augmented Generation\n(RAG)7. RAG allows information to be dynamically retrieved from medical\ndatabases during the model generation process, enriching the output with\nmedical knowledge without the need to train the model.\nLLMs hold great promise for improving the efﬁciency and accuracy of\nhealthcare delivery, e.g., by extractingclinical information from electronic\nhealth records, summarizing, structuring, or explaining medical texts,\nstreamlining administrative tasks in clinical practice, and enhancing med-\nical research, quality control, and education8–10. In addition, LLMs have been\nshown to be versatile tools for supporting diagnosis or serving as prognostic\nmodels11,12.\nHowever, despite the growing body of research and the clear potential\nof LLMs in healthcare, there is a gap in terms of systematized information\ntowards their use in patient care (i.e., the use of LLMs by patients or their\ncaregivers for disease management and support). In contrast to applications\nprimarily aimed at healthcare professionals, LLMs in patient care could be\nused for education and empowerment by providing answers to medical\nquestions and translating complex medical information into more acces-\nsible language\n4,13. Thereby, LLMs may promote personalized medicine and\nbroaden access to medical knowledge, empowering patients to actively\nparticipate in their healthcare decisions.\nTo the best of our knowledge, there has been no evaluation of existing\nresearch to understand the scope of applications and identify limitations\nthat may currently limit the successfulintegration of LLMs into clinical\npractice. This systematic review aims to analyze and synthesize the literature\non LLMs in patient care, providing a systematic overview of 1) current\napplications and 2) challenges and limitations, with the purpose of estab-\nlishing a foundational framework and taxonomy for the implementation\nand evaluation of LLMs in healthcare settings.\nMethods\nThis systematic review was pre-registered in the International Prospective\nRegister of Systematic Reviews (PROSPERO) under the identi ﬁer\nCRD42024504542 before the start of the initial screening and was con-\nducted according to the Preferred Reporting Items for Systematic Reviews\nand Meta-Analyzes (PRISMA) guidelines (see checklist in the Supple-\nmentary Datasetﬁle 1)\n14,15.\nEligibility criteria\nWe searched 5 databases, including the Web of Science, PubMed, Embase/\nEmbase Classic, American for Computing Machinery (ACM) Digital\nLibrary, and Institute of Electrical and Electronics Engineers (IEEE) Xplore\nas of January 25, 2024, to identify qualitative, quantitative, and mixed\nmethods studies published between January 1, 2022, and December 31,\n2023, that examined the use of LLMs for patient care. LLMs for patient care\nwere deﬁned as any artiﬁcial neural network that follows a transformer\narchitecture and can be used to generate and translate text and other content\nor perform other natural language processing tasks for the purpose of dis-\nease management and support (i.e., prevention, preclinical management,\ndiagnosis, treatment, or prognosis) that could be directly directed to or used\nby patients. Articles had to be available in English and contain sufﬁcient data\nfor thematic synthesis (e.g., conference abstracts that did not provide suf-\nﬁcient information on study results were excluded). Given the recent surge\nin publications on LLMs such as ChatGPT, we allowed for the inclusion of\npreprints if no corresponding peer-reviewed article was available. Duplicate\nreports of the same study, non-human studies, and articles limited to\ntechnology development/performance evaluation, pharmacy, human\ngenetics, epidemiology, psychology,psychosocial support, or behavioral\nassessment were excluded.\nScreening and data extraction\nInitially, we conducted a preliminary search on PubMed and Google Scholar\nto deﬁne relevant search terms. Theﬁnal search strategy included terms for\nLLMs, generative AI, and their applications in medicine, health services,\nclinical practices, medical treatments, and patient care (as detailed by\ndatabase in the Supplementary Methods). After importing the bibliographic\ndata into Rayyan and removing duplicates, LH and CR conducted an inde-\npendent blind review of each article’s title and abstract\n16.A n ya r t i c l eﬂagged as\npotentially eligible by either reviewer proceeded to the full-text evaluation\nstage. For this stage, LH and CR used a custom data extraction form created in\nGoogle Forms (available online)17 to collect all relevant data independently\nfrom the studies that met the inclusion criteria. Quality assessment was also\nperformed independently for each article within this data extraction form,\nusing the Mixed Methods Appraisal Tool (MMAT) 2018\n18. Disagreements at\nany stage of the review were resolved through discussion with the author FB.\nIn cases of studies with incomplete data, we have tried to contact the corre-\nsponding authors for clariﬁcation or additional information.\nData analysis\nDue to the diversity of investigated outcomes and study designs we sought to\ninclude, a meta-analysis was not practical. Instead, a data-driven convergent\nsynthesis approach was selected for thematic syntheses of LLM applications\nand limitations in patient care19. Following Thomas and Harden, FB coded\neach study’s numerical and textual data in Dedoose using free line-by-line\ncoding20,21. Initial codes were then systematically categorized into descriptive\nand subsequently into analytic themes, incorporating new codes for\nemerging concepts within a hierarchical tree structure. Upon completion of\nthe codebook, FB and LH reviewed each study to ensure consistent appli-\ncation of codes. Discrepancies were resolved through discussion with the\nauthor KKB, and theﬁnal codebook and analytical themes were discussed\nand reﬁned in consultation with all contributing authors.\nResults\nScreening results\nOf the 4349 reports identiﬁed, 2991 underwent initial screening, and 126\nwere deemed suitable for potential inclusion and underwent full-text\nscreening. Two articles could not be retrieved because the authors or the\ncorresponding title and abstract could not be identiﬁed online. Following\nfull-text screening, 35 articles were excluded, and 89 articles were included in\ntheﬁnal review. Most studies were excluded because they targeted the wrong\ndiscipline (n = 10/35, 28.6%) or population (n = 7/35, 20%) or were not\noriginal research (n = 8/35, 22.9%) (see Supplementary Datasetﬁle 2). For\nexample, we evaluated a study that focused on classifying physician notes to\nidentify patients without active bleeding who were appropriate candidates\nfor thromboembolism prophylaxis\n22. Although the classiﬁcation tasks may\nlead to patient treatment, the primary outcome was informing clinicians\nrather than directly forwarding this information to patients. We also\nreviewed a study assessing the accuracy and completeness of several LLMs\nwhen answering Methotrexate-related questions\n23.T h i ss t u d yw a se x c l u d e d\nbecause it focused solely on the pharmacological treatment of rheumatic\ndisease. For a detailed breakdown of the inclusion and exclusion process at\neach stage, please refer to the PRISMAﬂowchart in Fig.1.\nCharacteristics of included studies\nSupplementary Datasetﬁle 3 summarizes the characteristics of the analyzed\nstudies, including their setting, results, and conclusions. One study (n =1 /\n89, 1.1%) was published in 202224,8 4(n = 84/89, 94.4%) in 202313,25–107,a n d\n4( n = 4/89, 4.5%) in 2024108–111 (all of which were peer-reviewed publica-\ntions of preprints published in 2023). Most studies were quantitative non-\nrandomized (n = 84/89, 94.4%)\n13,25–27,29–101,103,104,106,107,109–111,4( n = 4/89,\n4.5%)28,102,105,108 had a qualitative study design, and one (n = 1/89, 1.1%)24 was\nquantitative randomized according to the MMAT 2018 criteria. However,\nthe LLM outputs were oftenﬁrst analyzed quantitatively but followed by a\nqualitative analysis of certain responses. Therefore, if the primary outcome\nwas quantitative, we considered the study design to be quantitative rather\nthan mixed methods, resulting in the inclusion of zero mixed methods\nstudies. The quality of the included studies was mixed (see Supplementary\nDatasetﬁle 4). The authors were primarily afﬁliated with institutions in the\nUnited States (n = 47 of 122 different countries identiﬁed per publication,\n38.5%), followed by Germany (n = 11/122, 9%), Turkey (n = 7/122, 5.7%),\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 2\nthe United Kingdom (n = 6/122, 4.9%), China/Australia/Italy (n = 5/122,\n4.1%, respectively), and 24 (n = 36/122, 29.5%) other countries. Most studies\nexamined one or more applications based on the GPT-3.5 architecture\n(n = 66 of 124 different LLMs examined per study, 53.2%)13,26–29,31–34,\n36–40,42–49,52–54,56–61,63,65–67,71,72,74,75,77,78,81–89,91,92,94,95,97–100,102–104,106–109,111,f o l l o w e db y\nGPT-4 (n = 33/124, 26.6%)13,25,27,29,30,34–36,41,43,50,51,54,55,58,61,64,68–70,74,76,79–81,83,87,89,\n90,93,96,98,99,101,105,B a r d( n = 10/124, 8.1%; now known as\nGemini)33,48,49,55,73,74,80,87,94,99,B i n gC h a t(n = 7/124, 5.7%; now Microsoft\nCopilot)49,51,55,73,94,99,110, and other applications based on Bidirectional Enco-\nder Representations from Transformers (BERT;n = 4/124, 3.2%)13,83,84,\nLarge Language Model Meta-AI (LLaMA;n = 3/124, 2.4%)55,o rC l a u d e\nby Anthropic ( n = 1/124, 0.8%)55. The majority of applications\nwere primarily targeted at patients ( n =6 4 o f 8 9 i n c l u d e d\nstudies, 73%)24,25,29,32,34–39,41–43,45–48,52–54,56–60,62,63,65,66,68–71,73–75,77–80,85–95,97,99,100,102–111\nor both patients and caregivers (n = 25/89, 27%)13,26–28,30,31,33,40,44,49–51,55,61,64,\n67,72,76,81–84,96,98,101. Information about conﬂicts of interest and funding was not\nexplicitly stated in 23 (n = 23/89, 25.8%) studies, while 48 (n = 48/89, 53.9%)\nreported that there were no conﬂicts of interest or funding. A total of 18\n(n = 18/89, 20.2%) studies reported the presence of conﬂicts of interest and\nfunding13,24,38,40,54,58,59,67,69–71,74,80,84,96,103,105,111. Most studies did not report\ninformation about the institutional review board (IRB) approval (n = 55/89,\n61.8%) or deemed IRB approval unnecessary (n = 28/89, 31.5%). Six studies\nobtained IRB approval (n = 6/89, 6.7%)52,82,84–86,92.\nApplications of large language models\nAn overview of the presence of codes for each study is provided in the\nSupplementary Datasetﬁle 3. The majority of articles investigated the use\nand feasibility of LLMs as medical chatbots ( n = 84/89,\n94.4%)13,24–62,64–66,68,69,71–96,98–111, while fewer reports additionally or exclusively\nfocused on the generation of patient information ( n = 18/89,\n20.2%)24,31,43,48,49,57,59,62,67,79,88–91,97,102,106,107, including clinical documentation\nsuch as informed consent forms (n = 5/89, 5.6%)43,67,91,97,102 and discharge\ninstructions (n = 1/89, 1.1%)31, or translation/summarization tasks of\nmedical texts (n = 5/89, 5.6%)24,49,57,79,89, creation of patient education\nmaterials (n = 5/89, 5.6%)48,62,90,106,107, and simpliﬁcation of radiology reports\n(n = 2/89, 2.3%)59,88. Most reports evaluated LLMs in English (n = 88/89,\n98.9%)13,24–103,105–111, followed by Arabic (n = 2/84, 2.3%)32,104,M a n d a r i n\n(n = 2/84, 2.3%)36,75, and Korean or Spanish (n = 1/89, 1.1%, respectively)75.\nThe top- ﬁve specialties studied were ophthalmology ( n = 10/89,\n11.2%)37,40,48,51,65,74,97,98,100,101, gastroenterology ( n = 9/89,\n10.1%)25,32,34,36,39,61,62,72,96, head and neck surgery/otolaryngology (n = 8/89,\n9%)35,42,56,64,66,76,78,79, and radiology59,70,88–90,110 or plastic surgery45,47,49,102,107,108\n(n = 6/89, 6.7%, respectively). A schematic illustration of the identiﬁed\nconcepts of LLM applications in patient care is shown in Fig.2.\nLimitations of large language models\nThe thematic synthesis of limitations resulted in two main concepts: one\nrelated to design limitations and one related to output. Figure3 illustrates\nthe hierarchical tree structure and quantity of the codes derived from the\nthematic synthesis of limitations. Supplementary Datasetﬁle 5 provides an\noverview of the taxonomy of all identiﬁed limitation concepts, including\ntheir description and examples.\nDesign limitations. In terms of design limitations, many authors noted\nthe limitation that LLMs are not optimized for medical use (n = 46/89,\n51.7%)13,26,28,34,35,37–39,46,49,50,54–59,61,62,65,66,68,70,71,79–81,83–85,88,91,93–98,100–107,109,\nincluding implicit knowledge/lack of clinical context ( n = 13/89,\n14.6%)28,39,46,66,71,79,81,83–85,98,103, limitations in clinical reasoning (n = 7/89,\nFig. 1 | Preferred reporting items for systematic\nreviews and meta-analyzes (PRISMA)ﬂow dia-\ngram. A total of 4349 reports were identiﬁed from\nWeb of Science, PubMed, Embase/Embase Classic,\nACM Digital Library, and IEEE Xplore. After\nexcluding 1358 duplicates, 2991 underwent initial\nscreening and 126 were deemed suitable for poten-\ntial inclusion and underwent full-text screening.\nTwo articles could not be retrieved because the\nauthors or the corresponding title and abstract could\nnot be identiﬁed online. After full text screening, 35\narticles were excluded and 89 articles were included\nin theﬁnal review.\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 3\n7.9%)55,84,95,102–105, limitations in medical image processing/production\n(n = 5/89, 5.6%)37,55,91,106,107, and misunderstanding of medical informa-\ntion and terms by the model (n = 7/89, 7.9%)28,38,39,59,62,65,97. In addition,\ndata-related limitations were identiﬁed, including limited access to data\non the internet (n = 22/89, 24.7%)38,39,41,43,54–57,59,60,64,76,79,82–84,88,91,94,96,104,109,\nthe undisclosed origin of training data ( n = 36/89,\n40.5%)25,26,29,30,32,34,36,37,40,46,47,50,51,53–60,64,65,70,71,76,82,83,91,94–96,101,105,109, limitations\nin providing, evaluating, and validating references ( n = 20/89,\n22.5%)45,49,54–57,65,71,73,76,80,83,85,91,94,96,98,101,103,105, and storage/processing of\nsensitive health information ( n = 8/89, 9%)13,34,46,55,62,76,83,109. Further\nsecond-order concepts included black-box algorithms, i.e., non-\nexplainable AI ( n = 12/89, 13.5%)\n27,36,55,57,65,73,76,83,91,94,103,105, limited\nengagement and dialog capabilities (n = 10/89, 11.2%)13,27,28,37,38,51,56,66,95,103,\nand the inability of self-validation and correction ( n = 4/89,\n4.5%)61,73,74,107.\nOutput limitations. The evaluation of limitations in output data yielded 7\nsecond-order codes concerning the non-reproducibility ( n = 38/89,\n42.7%)28,29,34,38,39,41,43,45,46,49,54–61,64,65,71–73,76,80,82,83,85,90,91,94,96,98,99,101,103–105, non-\ncomprehensiveness ( n = 78/89, 87.6%) 13,25,26,28–30,32–44,46,48–62,64,65,67–79,\n81–98,100,102–107,109–111, incorrectness ( n = 78/89, 87.6%) 13,25–44,46,49–52,\n54–62,64–66,69–79,81–85,87–107,109–111, (un-)safeness ( n = 39/89,\n43.8%)28,30,35,37,39,40,42–44,46,50,51,57–60,62,64,65,69,70,73,74,76,78–80,82,84,85,91,94,95,98–100,105,106,109,\nbias (n = 6/89, 6.7%)26,32,34,36,66,103, and the dependence of the quality of\noutput on the prompt-/input provided ( n = 27/89,\n30.3%)26–28,34,38,41,44,46,51,52,56,68–72,74,76,78,79,81–83,90,94,95,100,101 or the environment\n(n = 16/89, 18%)13,34,46,49–51,54,58,60,72,73,88,90,93,97,109.\nNon-reproducibility. For non-reproducibility, key concepts included the\nnon-deterministic nature of the output, e.g., due to inconsistent results\nacross multiple iterations ( n = 34/89,\n38.2%)\n28,29,34,38,39,41,43,46,58–61,72,76,82,90,94,98,99,101,103,104 and the inability to provide\nreliable references (n = 20/89, 22.5%)45,49,54–57,65,71,73,76,80,83,85,91,94,96,98,101,103,105.\nNon-comprehensiveness. Non-comprehensiveness included nine concepts\nrelated to generic/non-personalized output ( n = 34/89,\nPrevention\n(n = 22)\nPreclinical \nmanagement\n(n = 7)\nDiagnosis\n(n = 56) \nTreatment\n(n = 73)\nPrognosis\n(n = 22)\nMedical text \nsummarization/translation \n(n = 12)\nGeneration of patient \ninformation (n = 18)\nClinical \ndocumentation (n = 6)\nPatients \n(n = 64) LLM\nCaregiver \n(n = 25)\nMedical question \nanswering/chatbot (n = 84)\nInformed consent \n(n = 5)\nPatient education \nmaterials (n = 5)\nDischarge \ninstructions (n = 1)reports (n = 2)\nPatient-friendly \nmedical responses/\nsummaries (n = 5)\nA) Disciplines B) Languages\nC) Clinical applications\nFig. 2 | Schematic illustration of the identiﬁed disciplines, languages, and clinical\nconcepts of large language models (LLMs) applications in patient care. AColumn\nplot showing the distribution of medical specialties in which LLMs have been tested\nfor patient care.B Pie chart illustrating the distribution of languages in which LLMs\nhave been tested.C Schematic representation of the concepts identiﬁed for the\napplication of LLMs in patient care.\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 4\n38.2%)13,28,30,34,37,38,41,43,49,51,56,57,59,61,65,70,77,79,81,84–86,90,94,95,100,102–107,110,i n c o m p l e t e -\nness of output (n = 68/89, 76.4%)13,25,26,28–30,32,34–39,41–44,46,49–52,55–62,64,65,67–69,72–77,\n79,81–86,89–98,100,102–107,109–111, provision of information that is not standard of care\n(n = 24/89, 27%)28,40,43,46,49,50,54,57,58,65,69,72,73,77,78,81,85,91,94,98,100,103,107,111 and/or out-\ndated (n = 12/89, 13.5%)13,25,32,34,38,41,43,44,49,54,83,84, and production of over-\nsimpliﬁed ( n = 10/89, 11.2%)38,46,49,54,59,79,84,85,103,s u p e rﬂuous (n = 16/89,\n18%)13,28,34,38,46,62,72,79,86,90,94,97,100,106,107,o v e r c a u t i o u s( n = 7/89,\n7.9%)13,28,37,51,70,103,110,o v e r e m p a t h i c(n = 1/89, 1.1%)13, or output with inap-\npropriate complexity/reading level for patients ( n = 22/89,\n24.7%)13,34,42,48,50,51,53,55,56,67,71,78,79,85,87,88,90,93,106,107,109,110.\nIncorrectness. For incorrectness, we identiﬁed 6 key concepts. Some of the\nincorrect information could be attributed to what is commonly known as\nhallucination ( n = 38/89,\n42.7%)\n25,28,32,33,35–38,40–44,49–51,57–60,65,73,74,76,77,81,83,85,91,94,96–98,100,103,106,107,109, i.e., the\ncreation of entirelyﬁctitious or false information that has no basis in the\ninput provided or in reality (e.g.,“You may be asked to avoid eating or\ndrinking for a few hours before the scan”for a bone scan). Other instances of\nmisinformation were more appropriately classiﬁed under alternative con-\ncepts of the original psychiatric analogy, as described in detail by Currie et\nal.43,112,113. These include illusion (n = 12/89, 13.5%)28,36,38,43,57,59,77,78,85,88,94,105,\nwhich is characterized by the generation of deceptive perceptions or the\ndistortion of information by conﬂating similar but separate concepts (e.g.,\nsuggesting that MRI-type sounds might be experienced during standard\nnuclear medicine imaging), delirium ( n = 34/89,\n38.2%)13,26,28,30,37,43,50,58,59,61,65,70,72–75,77,79,81–85,90–92,94,95,98,102,103,107,109,110, which indi-\ncates signiﬁcant gaps in vital information, resulting in a fragmented or\nconfused understanding of a subject (e.g., omission of crucial information\nabout caffeine cessation for stress myocardial perfusion scans), extrapola-\ntion (n = 11/89, 12.4%)\n43,59,65,78,81,91,94,106,107,110, which involves applying general\nknowledge or patterns to speciﬁc situations where they are inapplicable (e.g.,\nadvice about injection-site discomfort that is more typical of CT contrast\nadministration), delusion (n = 14/89, 15.7%)\n28,30,43,50,59,65,69,73,74,78,81,94,103,111,a\nﬁxed, false belief despite contradictory evidence (e.g., inaccurate waiting\ntimes for the thyroid scan ), and confabulation ( n = 18/89,\n20.2%)25,28,36–38,40,46,59,62,65,71,77–79,94,103,107,i . e . ,ﬁl l i n gi nm e m o r yo rk n o w l e d g e\ngaps with plausible but invented information (e.g.,“You should drink plenty\nof ﬂuids to helpﬂush the radioactive material from your body”for a biliary\nsystem–excreted radiopharmaceutical).\nSafety and bias. Many studies rated the generated output as unsafe,\nincluding misleading ( n = 34/89, 38.2%)28,30,35,43,44,46,50,51,57–60,62,64,65,69,73,74,76,\nUnderrepresented procedures (n = 1)\nUnderserved racial groups (n = 1)\nInsurance status (n = 1)\nLanguage (n = 2)\nLocal/national medical resources (n = 5)\nProvider/organization (n = 4)\nTarget-group (n = 9)\nConversation type (n = 3)\nQuantity (n = 3)\nSpecificity (n = 13)\nComplexity (n = 11)\nEvidence (n = 7)\nHarmful (n = 26)\nMisleading (n = 34)\nConfabulation (n = 18)\nIllusion (n = 12)\nHallucination (n = 38)\nDelusion (n = 14)\nDelirium (n = 34)\nExtrapolation (n = 10)\nHigh complexity/reading level (n = 22)\nOverempathic (n = 1)\nOvercautious (n = 7)\nSuperfluous (n = 16)\nOversimplification (n = 10)\nOutdated (n = 12)\nNon-standard of care (n = 24)\nIncomplete (n = 68)\nGeneric/non-personalized (n = 34)\nNon-referenceable (n = 20)\nNon-deterministic (n = 24)\nNot open source (n = 10)\nNot freely accessible (n = 9)\nLimited number of prompts (n = 3)\nStores/processes sensitive health information (n = 8)\nLimited in reference provision/evaluation/validation (n = 20)\nUndisclosed origin of training data (n = 36)\nRestricted access to internet data (n = 22)\nMisunderstanding of medical information/terms (n = 7)\nLimited in processing/producing medical images (n = 5)\nLimited clinical reasoning (n = 7)\nImplicit knowledge/lack of clinical context (n = 13)\nBias (n = 6)\nEnvironment-dependent (n = 16)\nPrompt-/input dependent (n = 27)\nUnsafe (n = 39)\nIncorrect (n = 78)\nNon-comprehensive (n = 78)\nNon-reproducible (n = 38)\nIncapable of self-validation/correction (n = 4)\nLimited engagement/dialogue capabilities (n = 10)\nBlack box (n = 12)\nAccessibility (n = 18)\nData (n = 55)\nNot optimized for the medical domain (n = 46)\nOutput (n = 86)\nDesign (n = 67)\nLLM limitations (n = 89)\nFig. 3 | Illustration of the hierarchical tree structure for the thematic synthesis of\nlarge language model (LLM) limitations in patient care, including the presence of\ncodes for each concept.The font size of each concept is shown in proportion to its\nf r e q u e n c yi nt h es t u d i e sa n a l y z e d .O u ra n a l y s i sd e l i n e a t e st w op r i m a r yd o m a i n so fL L M\nlimitations: design and output. Design limitations included 6 second-order and 12 third-\norder codes, while output limitations included 9 second-order and 32 third-order codes.\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 5\n78–80,82,84,85,94,95,98–100,105,106,109 or even harmful content ( n = 26/89,\n29.2%)28,30,37,39,40,42,43,50,51,58–60,70,73,74,76,79,84,85,91,94,95,98–100,109.\nA minority of reports identiﬁed biases in the output, which were related\nto language (n = 2/89, 2.3%)32,36, insurance status103, underserved racial\ngroups26, or underrepresented procedures34 (n = 1/89, 1.1%, each).\nDependence on input and environment. Many authors suggested that\nperformance was related to the prompting/input provided or the environ-\nment, i.e., depending on the evidence (n = 7/89, 7.9%)52,68,69,71,81,82,95,c o m -\nplexity ( n = 11/89, 12.4%)28,34,44,46,70,74,76,79,94,102, speci ﬁcity ( n = 13/89,\n14.6%)27,38,41,56,70,72,74,76,78,81,95,100,101,q u a n t i t y(n = 3/89, 3.4%)26,52,74 of the input,\ntype of conversation (n = 3/89, 3.4%)27,51,90, or the appropriateness of the\noutput related to the target group (n = 9/89, 10.1%)46,49,51,54,72,90,93,97,109,p r o -\nvider/organization (n = 4/89, 4.5%)13,50,60,88, and local/national medical\nresources (n = 5/89, 5.6%)34,50,58,60,73.\nDiscussion\nIn this systematic review, we synthesized the current applications and limita-\ntions of LLMs in patient care, incorporating a broad analysis across 29 medical\nspecialties and highlighting key limitations in LLM design and output, pro-\nviding a comprehensive framework and taxonomy for describing and cate-\ngorizing limitations that may arise when using LLMs in healthcare settings.\nMost articles examined the use of LLMs based on the GPT-3.5 or GPT-\n4 architecture for answering medicalquestions, followed by the generation\nof patient information, including medical text summarization or translation\nand clinical documentation. The conceptual synthesis of LLM limitations\nrevealed two key concepts: theﬁrst related to design, including 6 second-\norder and 12 third-order codes, and the second related to output, including 9\nsecond-order and 32 third-order codes.By systematically categorizing the\nl i m i t a t i o n so fL L M si nc l i n i c a ls e t t i n g s ,o u rt a x o n o m ya i m st op r o v i d e\nhealthcare professionals and developers with a framework for assessing\npotential risks associated with the useof LLMs in patient care. In addition,\no u rw o r kh i g h l i g h t sk e ya r e a sf o ri m p r o v e m e n ti nt h ed e v e l o p m e n to fL L M s\nand aims to enable clinicians to make more informed decisions by under-\nstanding the limitations inherent in the design and output, thereby sup-\nporting the establishment of best practices for LLM use in clinical settings.\nAlthough many LLMs have been developed speciﬁcally for the biome-\ndical domain in recent years, we found that ChatGPT has been a disruptor in\nthe medical literature on LLMs, withG P T - 3 . 5a n dG P T - 4a c c o u n t i n gf o r\nalmost 80% of the LLMs examined in this systematic review. While it was not\npossible to conduct a meta-analysis of the performance on medical tasks,\nmany authors provided a positive outlook towards the integration of LLMs\ninto clinical practice. However, we haveconceptualized several key limita-\ntions in the design and output of LLMs, some of the most prevalent in our\nsystematic review are brieﬂy discussed in the following paragraphs.\nThe majority of studies (n = 55/89) reported limitations that were\nconceptualized as related to the underlying data of the LLMs studied. Espe-\ncially the use of proprietary models such as ChatGPT in the biomedicalﬁeld\nwas a concern in many of the studies analyzed, mainly because of the lack of\ntraining data transparency (third-order code: undisclosed origin of training\ndata). In practice, it is widely recognized that limited access to the underlying\nalgorithms, training data, and data processing and storage mechanisms of\nL L M si sas i g n iﬁcant barrier to their application in healthcare\n114.T h i so p a c i t y\nmakes it difﬁcult for healthcare professionals to fully understand how these\nmodels function, assess their reliability, or ensure compliance with local\nmedical standards and regulations. Consequently, the use of such models in\nhealthcare settings can be problematic, and the need to recognize and correct\npotential limitations in the outputs of such models is paramount.\nMoreover, integrating proprietary models into clinical practice intro-\nduces a vulnerability to performance changes that occur with model\nupdates\n115. As these models are updated by their developers, functionalities\nthat healthcare providers rely on may be altered or broken, potentially\nleading to harmful outcomes for patients, which was also conceptualized in\nour study under output limitations (second-order code: unsafe; third-order\ncodes: misleading/harmful). This unpredictability is a serious concern in the\nbiomedicalﬁeld, where consistency and reliability are crucial. Notably, the\nunpredictability of LLMs was another concept of output limitations in our\nsystematic review (second-order code: non-reproducible; third-order codes:\nnon-deterministic/non-referenceable).\nAs a result, open-source models such as BioMistral may offer a viable\nsolution\n6. Such open source models not only offer more transparency, as\ntheir algorithms and training data are accessible but can also be adapted\nl o c a l l y .H o w e v e r ,g i v e nt h el i m i t e dn u m b e ro fa r t i c l e so no p e n - s o u r c eL L M s\nin our review, we strongly encourage future studies investigating the\napplicability of open-source LLMs in patient care.\nA b o u th a l fo ft h es t u d i e sa n a l y z e dreported limitations related to LLMs\nnot being optimized for the medical domain. One possible solution to this\nlimitation may be to provide medical knowledge during inference using\nRAG\n116. However, even when trained for general purposes, ChatGPT has\npreviously been shown to pass the United States Medical Licensing\nExamination (USMLE), the German State Examination in Medicine, or\neven a radiology board-style examination without images117–120.A l t h o u g h\noutperformed on speciﬁc tasks by specialized medical LLMs, such as\nGoogle’s MedPaLM-2, this suggests that general-purpose LLMs can com-\nprehend complex medical literature and case scenarios to a degree that\nmeets professional standards\n121. Furthermore, given the large amounts of\ndata on which proprietary models such as ChatGPT are trained, it is not\nunlikely that they have been exposed to more medical data overall than\nsmaller specialized models despite being generalist models. Notably, a recent\nstudy even suggested thatﬁne-tuning LLMs on biomedical data does not\nimprove performance compared to their general-purpose counterparts\n122.\nIt should also be noted that passingt h e s ee x a m sd o e sn o te q u a t et ot h e\npractical competence required of a healthcare provider, which was also a\nlimitation identiﬁed in our review (third-order codes: implicit knowledge/\nlack of clinical context; limited clinical reasoning; misunderstanding of\nmedical information/terms; limited in processing/producing medical\nimages)\n123. In addition, reliance on exam-based assessments carries a sig-\nniﬁcant risk of bias. For example, if the exam questions or similar variants\nare publicly available and, thus, may be present in the training data, the LLM\ndoes not demonstrate any knowledge outside of training data\nmemorization\n124. In fact, these types of tests can be misleading in estimating\nthe model’s true abilities in terms of comprehension or analytical skills.\nThe non-reproducibility of LLM output, as conceptualized in 38 studies,\nhighlights key challenges in ensuring consistency and determinism in LLM-\ngenerated results. One major issue is the inherent stochasticity in the models’\narchitecture, particularly in transformer-based models, which utilize prob-\nabilistic techniques during inference (e.g., beam search or temperature\nsampling)125. This non-determinism can lead to different outputs for the same\ninput, making it difﬁcult to replicate results exactly across different instances\nor even across models with identical training data. Further external factors\ncontributing to non-reproducibility, such as variations in hardware, software\nversions, or context windows, complicate the assurance of reproducibility126.\nAs the reproducibility of results is a central principle in medical practice, our\nconcepts highlight the need for more standardized protocols, improved\ndocumentation of model con ﬁgurations, the examination of non-\ndeterminism for evaluation purposes, and further research on how robust\nresults can be achieved before implementing LLMs in real-world clinical\npractice. Interestingly, Ouyang et al. reported that only a minority of studies\ntake non-determinism into account in their experimental evaluation when\nusing ChatGPT for code generation, suggesting that this limitation is also\nprevalent and overlookedin other domains of LLM use\n125.\nThe concept of non-comprehensiveness was prevalent in almost 90%\nof the studies analyzed (n = 78/89). For this concept, the majority of third-\norder codes were related to LLM outputs that were incomplete. This issue is\nparticularly signiﬁcant when considering the application of LLMs in med-\nical tasks such as clinical decision support or diagnosis, where incomplete or\npartial results can have serious consequences. In clinical practice, missing\nkey information could lead to suboptimal patient outcomes, incorrect\ndiagnoses, or improper treatment recommendations. For instance, an\nincomplete therapy suggestion could render the entire treatment plan\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 6\ninsufﬁcient, potentially resulting in harm to the patient. Given the potential\nof using LLMs in medical decision-making, these limitations underscore the\nnecessity for expert supervision and validation of LLM outputs depending\non their application. While LLMs used as chatbots for general patient\ninquiries may not require consistent human oversight, using LLMs for\ntreatment advice would require consistent validation to ensure that\nincomplete information does not lead to adverse outcomes. Depending on\ntheir application, the same problem arises when the LLM generates generic\nor non-personalized information, which was another third-order code\nidentiﬁed. The generation of content with high complexity and an inap-\npropriate reading level, which was above the American Medical Association\n(AMA) recommended 6th-grade reading level in almost all of the 22 studies\nthat analyzed the complexity level of the output, may further limit its use-\nfulness for patient information\n127. Again, the best solution to the lack of\ncomprehensiveness in clinical practice so far seems to be human oversight.\nIncorrectness, alongside non-comprehensiveness (as above), was the\nmost common second-order code, identiﬁed in about 90% of studies\n(n = 78/89). In our conceptual synthesis of incorrect results, we followed the\ntaxonomy of Currie et al. to classify incorrect outputs more precisely into\nillusions, delusions, delirium, confabulation, and extrapolation, thus pro-\nposing a framework for a more precise and structured error classiﬁcation to\nimprove the characterization of incorrect outputs and enabling more\ndetailed performance comparisons with other research\n43,112,113.\nMany studies currently refer to all non-factual LLM results as“hallu-\ncinations.” However, this generalization fails to capture the complexity of\nerrors when considering the original psychiatric analogy. Simply classifying\nerrors as hallucinations restricts their description to invented information,\noverlooking errors that, for example, omit critical information and leading\nto fragmented or confused understanding (third-order code: delirium).\nNotably, the third-order code“delirium” was observed in nearly as many\nstudies as the third-order code“hallucination.” However, a non-detailed\nclassiﬁcation of incorrect results can affect not only the comparability of\nresearchﬁndings but also has implications for clinical practice. While hal-\nl u c i n a t i o n s( f o re x a m p l e ,f a bricating instructions like“You may need to fast\nbefore a bone scan.”) may not always have serious consequences, errors\nclassiﬁed as delirium— such as omitting crucial details like caffeine cessation\nbefore a stress myocardial perfusion scan— would always result in undesired\noutcomes (in the here presented example, most likely in repeating or\npostponing the examination). As a result, our review advocates for a more\ndetailed classiﬁcation of incorrect results in order to increase the qualitative\ncomparability of incorrect LLM outputs and, ultimately, the relevance and\nimplications of these results for clinical practice.\nThe conceptualization of unsafeness in 39 of 89 studies presents a\nsigniﬁcant concern when considering the integration of LLMs into medical\npractice. In theﬁeld of medicine, any tool or intervention that could lead to\nmisleading or harmful outcomes must be critically assessed, as the potential\nfor patient harm is high\n128,129. Such tools are generally only accepted when\nthe beneﬁts clearly outweigh the risks, and even then, informed consent\nfrom the affected individual is essential130. While informed consent might\nensure that patients understand the risks involved and are able to make an\neducated decision about their care, which could be obtained, for example, in\nthe form of a disclaimer before using the LLM, studies suggest that even\nwhen obtaining informed consent the patient understanding increases not\nsigniﬁcantly\n131. In the case of disclaimers, there might also be the risk that\nthese are accepted without proper reading or understanding132.T h ep r a c -\nticality of informed consent once LLMs are deeply integrated into clinical\nworkﬂows also remains an issue, as it is when patients no longer have the\nability to opt out, such as in the case of serious illness. In any case, theﬁnding\nthat nearly half of the studies reportedlimitations related to LLM unsafeness\nsuggests that LLMs are not yet reliable enough for autonomous medical use,\nand there is a critical need for safety measures and regulatory and human\noversight to prevent adverse consequences in medical contexts133.\nFurther second-order concepts suggested that the output is inﬂuenced\nby the input or environment in which it is expressed. In fact, LLMs can be\nhighly dependent on the quality and speciﬁcity of input, making their output\nprone to errors when faced with vague or incomplete information\n134–136.\nAgain, this poses signiﬁcant risks in patient care, where incorrect outputs\ncan lead to adverse outcomes, such as inappropriate triage or treatment. For\nexample, in our review, eleven studies reported a decrease in performance\nwith increasing complexity of the input, which can have implications in\nclinical practice, such as failing to consider multifactorial medical issues like\ncomorbidities, thus compromising the quality of care for those patients.\nWe found that the environment also inﬂuences the appropriateness of\nLLM outputs in medical settings. Models may recommend treatments that\nare inappropriate for certain patient populations, such as offering adult care\nprotocols for pediatric patients or suggesting therapies that are not available\nin certain regions. This also raises ethical concerns, particularly in resource-\nconstrained settings, where making inappropriate or inaccessible recom-\nmendations may reinforce existing inequalities and lead to uncomfortable\nsituations for both the healthcare provider and the patient\n137.O n es o l u t i o n\nmay be to provide adequate training to LLM users, or in our scenario, to\npatients, on how to present input to the model to achieve the best results.\nA n o t h e rs o l u t i o ni st ot r a i no rﬁne-tune the model to the environment in\nwhich it will be used. For example, if the LLM is trained on the standard\noperating procedure for handling patients with major adverse cardiovas-\ncular events in a particular hospital, it is more likely to recommend the\nadequate procedure in this setting than when it is trained on worldwide data\nfrom an unknown time frame, where there is a chance that it will suggest\nnon-standard care that may only be relevant in other countries where most\nof the training data is coming from, or even provides outdated information\n(which is another third-order codethat was conceptualized under non-\ncomprehensiveness) if it is trained on data that is not current.\nUltimately, only six studies have identiﬁed biases in their results, for\nexample, reﬂecting the unequal representation of certain content or the biases\ninherent in human-generated text in the training data\n138. Here, we con-\nceptualized the results of studies that identiﬁed bias in their analysis and not\nonly mentioned bias as a theoretical limitation. Thus, these results may\nindicate that the implemented safeguards are effective. On the other hand,\nidentifying bias was not the primary outcome of most studies, and not much\nis known about the technology and developer policies of proprietary LLMs.\nMoreover, previous work has shown that automated jailbreak generation is\npossible across various commercial LLM chatbots\n139. In the end, LLMs are\ntrained on large datasets that inevitably contain biases— such as gender, racial,\nor cultural biases— embedded in the text140. These biases can be ampliﬁed or\nreﬂected by the models, leading to unfair or harmful outputs. Despite the use\nof various mitigation techniques, such as debiasing algorithms, curating\nbalanced datasets, or incorporating fairness-focused training objectives,\neliminating bias entirely is a persistent challenge141–143. This is because LLMs\nlearn patterns from their training data, and human biases are inherently\npresent in much of the data they consume. Moreover, the biases introduced or\nreinforced by LLM are not always obvious, making them more difﬁcult to\ndetect and correct, which may have contributed to the comparatively low\nnumber of studies that reported any bias in their results. Notably, subtle\nbiases, such as those related to linguistic connotations, regional dialects, or\nimplicit associations, can be especially insidious and difﬁcult to eliminate\nthrough technical safeguards144. Therefore, the results of our review may\nencourage future studies to more explicitly examine the biases inherent in\nLLMs when used for medical tasks and how such biases could be mitigated.\nOur ﬁndings raise a key question whena p p l y i n gL L M st ot h em e d i c a l\ndomain: how can we entrust our patients to LLMs if they are neither reliable\nnor transparent? Given that models like ChatGPT are already publicly\naccessible and widely used, patients may already refer to them for medical\nquestions in much the same way they use Google Search, making concerns\nabout their early adoption somewhat academic\n145.\nIn addition to the advances in the development of LLMs and the focus\non open source, adopting appropriate security measures to prevent the\nidentiﬁed LLM limitations in clinical practice out-of-the-box will become\nincreasingly important. For example, strategies to ensure LLM security and\nprivacy can include continuous monitoring for new vulnerabilities, imple-\nmenting input validation, conductingregular audits of training data, and\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 7\nusing secure data pipelines146. Additionally, data anonymization, encryp-\ntion, access controls, and regular security updates are essential to prevent\ndata leakage, model theft, and privacy breaches.\nMoreover, expert oversight of theﬁnal LLM output could mitigate any\nremaining risks in the last instance, ensuring that erroneous or inappropriate\nsuggestions are identiﬁed and corrected before they can impact patient care.\nRecently, efforts have been made in this direction by adopting the widely\nrecognized Physician DocumentationQuality Instrument (PDQI-9) for the\nassessment of AI transcripts and clinical summaries\n147. However, whether\nongoing human oversight and validation of LLM-generated content is fea-\nsible and can reduce the likelihood of adverse outcomes remains the subject of\nfurther research at this early stage of LLM deployment in healthcare.\nAnother important factor for the successful clinical implementation of\nLLMs in patient care could be patient acceptance, which was not assessed in\nany of the studies analyzed. The growing use of LLMs in healthcare might be\nperceived as a reduction in the interpersonal relationship between health-\ncare professionals and patients, potentially leading to a sense of dehuma-\nnization in medicine148. Therefore, to promote a positive reception of AI\ntools among patients, incorporating their perspectives already during the AI\ndevelopment and implementation process could be key\n149.E v e n t u a l l y ,\npatient perspectives are already considered in AI regulatory frameworks,\nsuch as in the European Union AI Act, which came into force in August\n2024\n150. The associated challenges faced by generative AI and LLM, for\nexample, in terms of training data transparency and validation of non-\ndeterministic output, will show which approaches the companies will take to\nbring these models into compliance with the law\n151. How the notiﬁed bodies\ninterpret and enforce the law in practice will likely be decisive for the further\ndevelopment of LLMs in the biomedical sector.\nOur study has limitations. First, our review focused on LLM applica-\ntions and limitations in patient care, thus excluding research directed at\nclinicians only. Future studies may extend our synthesis approach to LLM\napplications that explicitly focus on healthcare professionals. Second, while\nit was not possible to conduct a meta-analysis of LLM performance due to\nthe different study designs and evaluation methods used, this will be an\nimportant area for future work as theﬁeld of LLM research in clinical\nsettings continues to evolve. Third, there is a risk that potentially eligible\nstudies were not included in our analysis if they were not present in the 5\ndatabases reviewed or were not available in English. However, we screened\nnearly 3,000 articles in total and systematically analyzed 89 articles, pro-\nviding a comprehensive overview of the current state of LLMs in patient\ncare, even if some articles could have been missed. With our chosen cut-off\ndate of January 2022, there is also a risk of missing relevant publications on\npredecessor LLM models, such as GPT-3, which was introduced in 2020.\nHowever, as our review focused on current LLM applications and limita-\ntions, it seemed most beneﬁcial to include only recent publications from the\nlast two years on the most advanced models, especially when considering\nthat ChatGPT wasﬁrst made available in November 2022. Finally, the rapid\ndevelopment and advancement of LLMs make it difﬁcult to keep this sys-\ntematic review up to date. For example, Gemini 1.5 Pro was published in\nFebruary 2024, and corresponding articles are not included in this review,\nwhich synthesized articles from 2022 to 2023. This also has implications for\nour introduced taxonomy of LLM limitations, as new limitations may\nemerge as models evolve, and previous limitations may become less relevant\nor even obsolete. For example, our taxonomy identiﬁes “limited access to\ninternet data” as a limitation; however, with the introduction of web\nbrowsing capabilities for GPT-4 in May 2023, this particular limitation no\nlonger applies to that model. Given these ongoing developments, we\nstrongly encourage future studies totest, update, and extend our taxonomy\nto ensure that it remains a relevant tool for categorizing LLM limitations in\nclinical and other high-stakes applications.\nData availability\nAll data generated or analyzed during this study, including source data, are\nincluded in this published article and its supplementary informationﬁles.\nTo provide a more intuitive overview and allow readers toﬁlter through the\ncollected study data and codes, we have provided the Supplementary\nDataset ﬁles in the form of Excel spreadsheets.\nReceived: 11 March 2024; Accepted: 17 December 2024;\nReferences\n1. Milmo, D. ChatGPT reaches 100 million users two months after launch,\nhttps://www.theguardian.com/technology/2023/feb/02/chatgpt-100-\nmillion-users-open-ai-fastest-growing-app(February 2, 2023).\n2. OpenAI. GPT-4 Technical Report. arXiv:2303.08774. https://ui.\nadsabs.harvard.edu/abs/2023arXiv230308774O (2023).\n3. Zhao, W. X. et al. A survey of large language models.arXiv preprint\narXiv:2303.18223 (2023).\n4. Clusmann, J. et al. The future landscape of large language models in\nmedicine. Communications Medicine3, 141 (2023).\n5. Chen, Z. et al. Meditron-70b: Scaling medical pretraining for large\nlanguage models.arXiv preprint arXiv:2311.16079(2023).\n6. Labrak, Y. et al. BioMistral: A Collection of Open-Source Pretrained\nLarge Language Models for Medical Domains. arXiv preprint\narXiv:2402.10373 (2024).\n7. Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmarking Retrieval-\nAugmented Generation for Medicine.arXiv preprint\narXiv:2402.13178 (2024).\n8. Yang, X. et al. A large language model for electronic health records.\nnpj Digital Medicine5, 194 (2022).\n9. Tian, S. et al. Opportunities and challenges for ChatGPT and large\nlanguage models in biomedicine and health.Brieﬁngs in\nBioinformatics 25. https://doi.org/10.1093/bib/bbad493 (2024)\n10. Adams, L. C. et al. Leveraging GPT-4 for post hoc transformation of\nfree-text radiology reports into structured reporting: a multilingual\nfeasibility study.Radiology 307, e230725 (2023).\n11. McDuff, D. et al. Towards accurate differential diagnosis with large\nlanguage models.arXiv preprint arXiv:2312.00164(2023).\n12. Jiang, L. Y. et al. Health system-scale language models are all-\npurpose prediction engines.Nature 619, 357–362 (2023).\n13. Liu, S. et al. Leveraging Large Language Models for Generating\nResponses to Patient Messages.medRxiv,\n2023.2007.2014.23292669. https://doi.org/10.1101/2023.07.14.\n23292669 (2023)\n14. Busch, F., Hoffmann, L., Adams, L. C. & Bressem, K. K.A systematic\nreview of current large language model applications and biases in\npatient care, https://www.crd.york.ac.uk/prospero/display_record.\nphp?ID=CRD42024504542 (2024).\n15. Page, M. J. et al. The PRISMA 2020 statement: an updated guideline\nfor reporting systematic reviews.Bmj 372, n71 (2021).\n16. Ouzzani, M., Hammady, H., Fedorowicz, Z. & Elmagarmid, A. Rayyan\n— a web and mobile app for systematic reviews.Systematic Reviews\n5, 210 (2016).\n17. Data extraction form, https://docs.google.com/forms/d/e/\n1FAIpQLScFwE5KaOugxX_\nxXtt9Y6fbBhV4s77S9cWRdVuiHh34vmArkQ/viewform (2024).\n18. Hong, Q. N. et al. The Mixed Methods Appraisal Tool (MMAT) version\n2018 for information professionals and researchers.Education for\nInformation 34\n, 285–291 (2018).\n19. Hong, Q. N., Pluye, P., Bujold, M. & Wassef, M. Convergent and\nsequential synthesis designs: implications for conducting and\nreporting systematic reviews of qualitative and quantitative\nevidence. Syst Rev6, 61 (2017).\n20. Thomas, J. & Harden, A. Methods for the thematic synthesis of\nqualitative research in systematic reviews.BMC Medical Research\nMethodology 8, 45 (2008).\n21. Dedoose Version 9.2.4, cloud application for managing, analyzing,\nand presenting qualitative and mixed method research data (Los\nAngeles, CA, 2024).\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 8\n22. Savage, T., Wang, J. & Shieh, L. A Large Language Model Screening\nTool to Target Patients for Best Practice Alerts: Development and\nValidation. JMIR Med Inform11, e49886 (2023).\n23. Coskun, B. N., Yagiz, B., Ocakoglu, G., Dalkilic, E. & Pehlivan, Y.\nAssessing the accuracy and completeness of artiﬁcial intelligence\nlanguage models in providing information on methotrexate use.\nRheumatol Int. https://doi.org/10.1007/s00296-023-05473-5 (2023)\n24. Bitar, H., Babour, A., Nafa, F., Alzamzami, O. & Alismail, S. Increasing\nWomen’s Knowledge about HPV Using BERT Text Summarization:\nAn Online Randomized Study.Int J Environ Res Public Health19.\nhttps://doi.org/10.3390/ijerph19138100 (2022)\n25. Samaan, J. S. et al. Artiﬁcial Intelligence and Patient Education:\nExamining the Accuracy and Reproducibility of Responses to\nNutrition Questions Related to Inﬂammatory Bowel Disease by GPT-\n4. medRxiv, 2023.2010.2028.23297723.https://doi.org/10.1101/\n2023.10.28.23297723 (2023)\n26. Eromosele, O. B., Sobodu, T., Olayinka, O. & Ouyang, D. Racial\nDisparities in Knowledge of Cardiovascular Disease by a Chat-\nBased Artiﬁcial Intelligence Model.medRxiv,\n2023.2009.2020.23295874. https://doi.org/10.1101/2023.09.20.\n23295874 (2023)\n27. Johri, S. et al. Guidelines For Rigorous Evaluation of Clinical LLMs\nFor Conversational Reasoning.medRxiv,\n2023.2009.2012.23295399. https://doi.org/10.1101/2023.09.12.\n23295399 (2024)\n28. Braga, A. V. N. M. et al. Use of ChatGPT in Pediatric Urology and its\nRelevance in Clinical Practice: Is it useful?medRxiv,\n2023.2009.2011.23295266. https://doi.org/10.1101/2023.09.11.\n23295266 (2023)\n29. King, R. C. et al. Appropriateness of ChatGPT in answering heart\nfailure related questions.medRxiv, 2023.2007.2007.23292385.\nhttps://doi.org/10.1101/2023.07.07.23292385 (2023)\n30. Huang, S. S. et al. Fact Check: Assessing the Response of ChatGPT\nto Alzheimer’s Disease Statements with Varying Degrees of\nMisinformation. medRxiv, 2023.2009.2004.23294917.https://doi.\norg/10.1101/2023.09.04.23294917 (2023)\n31. Hanna, J. J., Wakene, A. D., Lehmann, C. U. & Medford, R. J.\nAssessing Racial and Ethnic Bias in Text Generation for Healthcare-\nRelated Tasks by ChatGPT1.medRxiv, 2023.2008.2028.23294730.\nhttps://doi.org/10.1101/2023.08.28.23294730 (2023)\n32. Samaan, J. S. et al. ChatGPT’s ability to comprehend and answer\ncirrhosis related questions in Arabic.Arab J Gastroenterol24,\n145–148 (2023).\n33. Patnaik, S. S. & Hoffmann, U. Quantitative evaluation of ChatGPT\nversus Bard responses to anaesthesia-related queries.Br J Anaesth\n132, 169–171 (2024).\n34. Ali, H. et al. Evaluating the performance of ChatGPT in responding to\nquestions about endoscopic procedures for patients.iGIE 2,\n553–559 (2023).\n35. Suresh, K. et al. Utility of GPT-4 as an Informational Patient Resource\nin Otolaryngology.medRxiv, 2023.2005.2014.23289944.https://\ndoi.org/10.1101/2023.05.14.23289944 (2023)\n36. Yeo, Y. H. et al. GPT-4 outperforms ChatGPT in answering non-\nEnglish questions related to cirrhosis.medRxiv,\n2023.2005.2004.23289482. https://doi.org/10.1101/2023.05.04.\n23289482 (2023)\n37. Knebel, D. et al. Assessment of ChatGPT in the Prehospital\nManagement of Ophthalmological Emergencies - An Analysis of 10\nFictional Case Vignettes.Klin Monbl Augenheilkd. https://doi.org/\n10.1055/a-2149-0447 (2023)\n38. Zhu, L., Mou, W. & Chen, R. Can the ChatGPT and other large\nlanguage models with internet-connected database solve the\nquestions and concerns of patient with prostate cancer and help\ndemocratize medical knowledge?Journal of Translational Medicine\n21, 269 (2023).\n39. Lahat, A., Shachar, E., Avidan, B., Glicksberg, B. & Klang, E.\nEvaluating the Utility of a Large Language Model in Answering\nCommon Patients’Gastrointestinal Health-Related Questions: Are\nWe There Yet?Diagnostics (Basel)13 (2023). https://doi.org/10.\n3390/diagnostics13111950\n40. Bernstein, I. A. et al. Comparison of ophthalmologist and large\nlanguage model chatbot responses to online patient eye care\nquestions. JAMA Network Open6, e2330320–e2330320 (2023).\n41. Rogasch, J. M. M. et al. ChatGPT: Can You Prepare My Patients for\n[18F]FDG PET/CT and Explain My Reports?Journal of Nuclear\nMedicine, jnumed.123.266114.https://doi.org/10.2967/jnumed.\n123.266114 (2023)\n42. Campbell, D. J. et al. Evaluating ChatGPT Responses on Thyroid\nNodules for Patient Education.Thyroid®. https://doi.org/10.1089/\nthy.2023.0491 (2023)\n43. Currie, G., Robbie, S. & Tually, P. ChatGPT and patient information in\nnuclear medicine: GPT−3.5 versus GPT-4.J Nucl Med Technol51,\n307–313 (2023).\n44. Draschl, A. et al. Are ChatGPT’s Free-Text Responses on\nPeriprosthetic Joint Infections of the Hip and Knee Reliable and\nUseful? J Clin Med12. https://doi.org/10.3390/jcm12206655 (2023)\n45. Alessandri-Bonetti, M., Liu, H. Y., Palmesano, M., Nguyen, V. T. &\nEgro, F. M. Online patient education in body contouring: a\ncomparison between Google and ChatGPT.Journal of Plastic,\nReconstructive & Aesthetic Surgery87, 390–402 (2023).\n46. Coskun, B., Ocakoglu, G., Yetemen, M. & Kaygisiz, O. Can ChatGPT,\nan artiﬁcial intelligence language model, provide accurate and high-\nquality patient information on prostate cancer?Urology\n180,3 5–58\n(2023).\n47. Durairaj, K. K. et al. Artiﬁcial Intelligence Versus Expert Plastic\nSurgeon: Comparative Study Shows ChatGPT“Wins” Rhinoplasty\nConsultations: Should We Be Worried?Facial Plastic Surgery &\nAesthetic Medicine. https://doi.org/10.1089/fpsam.2023.0224\n(2023)\n48. Kianian, R., Sun, D., Crowell, E. L. & Tsui, E. The Use of Large\nLanguage Models to Generate Education Materials about Uveitis.\nOphthalmol Retinahttps://doi.org/10.1016/j.oret.2023.09.008\n(2023).\n49. Seth, I. et al. Exploring the Role of a Large Language Model on Carpal\nTunnel Syndrome Management: An Observation Study of ChatGPT.\nJ Hand Surg Am48, 1025–1033 (2023).\n50. Inojosa, H. et al. Can ChatGPT explain it? Use of artiﬁcial intelligence\nin multiple sclerosis communication.Neurological Research and\nPractice 5, 48 (2023).\n51. Lyons, R. J., Arepalli, S. R., Fromal, O., Choi, J. D. & Jain, N. Artiﬁcial\nintelligence chatbot performance in triage of ophthalmic conditions.\nCan J Ophthalmol. https://doi.org/10.1016/j.jcjo.2023.07.016(2023)\n52. Babayi ğit, O., Tastan Eroglu, Z., Ozkan Sen, D. & Ucan Yarkac, F.\nPotential use of ChatGPT for patient information in periodontology: a\ndescriptive pilot study.Cureus 15, e48518 (2023).\n53. Mondal, H., Dash, I., Mondal, S. & Behera, J. K. ChatGPT in\nanswering queries related to lifestyle-related diseases and\ndisorders. Cureus 15, e48296 (2023).\n54. Kim, H. W., Shin, D. H., Kim, J., Lee, G. H. & Cho, J. W. Assessing the\nperformance of ChatGPT’s responses to questions related to\nepilepsy: a cross-sectional study on natural language processing\nand medical information retrieval.Seizure 114,1 –8 (2023).\n55. Song, H. et al. Evaluating the performance of different large language\nmodels on health consultation and patient education in urolithiasis.J\nMed Syst47, 125 (2023).\n56. Zalzal, H. G., Abraham, A., Cheng, J. H. & Shah, R. K. Can ChatGPT\nhelp patients answer their otolaryngology questions?Laryngoscope\nInvestigative Otolaryngology. https://doi.org/10.1002/lio2.1193(2023)\n57. Chervenak, J., Lieman, H., Blanco-Breindel, M. & Jindal, S. The\npromise and peril of using a large language model to obtain clinical\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 9\ninformation: ChatGPT performs strongly as a fertility counseling tool\nwith limitations.Fertility and Sterility120, 575–583 (2023).\n58. Bushuven, S. et al. ChatGPT, can you help me save my child’s life?”-\ndiagnostic accuracy and supportive capabilities to lay rescuers by\nChatGPT in prehospital basic life support and paediatric advanced\nlife support cases - an in-silico analysis.J Med Syst47, 123\n(2023).\n59. Jeblick, K. et al. ChatGPT makes medicine easy to swallow: an\nexploratory case study on simpliﬁed radiology reports.European\nRadiology. https://doi.org/10.1007/s00330-023-10213-1 (2023)\n60. Samaan, J. S. et al. Assessing the accuracy of responses by the\nlanguage model ChatGPT to questions regarding bariatric surgery.\nObes Surg33, 1790–1796 (2023).\n61. Zhou, J. M., Li, T. Y., Fong, S. J., Dey, N. & Crespo, R. G. Exploring\nChatGPT’s potential for consultation, recommendations and report\ndiagnosis: gastric cancer and gastroscopy reports’case.\nInternational Journal of Interactive Multimedia and Artiﬁcial\nIntelligence 8,7 –13 (2023).\n62. Oniani, D. et al. Toward improving health literacy in patient education\nmaterials with neural machine translation models.AMIA Jt Summits\nTransl Sci Proc2023, 418–426 (2023).\n63. Hernandez, C. A. et al. The future of patient education: ai-driven\nguide for type 2 diabetes.Cureus 15, e48919 (2023).\n64. Ku şcu, O., Pamuk, A. E., Sütay Süslü, N. & Hosal, S. Is ChatGPT\naccurate and reliable in answering questions regarding head and\nneck cancer?Front Oncol13, 1256459 (2023).\n65. Biswas, S., Logan, N. S., Davies, L. N., Sheppard, A. L. & Wolffsohn,\nJ. S. Assessing the utility of ChatGPT as an artiﬁcial intelligence-\nbased large language model for information to answer questions on\nmyopia. Ophthalmic Physiol Opt43, 1562–1570 (2023).\n66. Chiesa-Estomba, C. M. et al. Exploring the potential of Chat-GPT as\na supportive tool for sialendoscopy clinical decision making and\npatient information support.Eur Arch Otorhinolaryngol. https://doi.\norg/10.1007/s00405-023-08104-8 (2023)\n67. Decker, H. et al. Large language model-based chatbot vs surgeon-\ngenerated informed consent documentation for common\nprocedures. JAMA Netw Open6, e2336997 (2023).\n68. Kaarre, J. et al. Exploring the potential of ChatGPT as a\nsupplementary tool for providing orthopaedic information.Knee\nSurg Sports Traumatol Arthrosc31, 5190–5198 (2023).\n69. Ferreira, A. L., Chu, B., Grant-Kels, J. M., Ogunleye, T. & Lipoff, J. B.\nEvaluation of ChatGPT dermatology responses to common patient\nqueries. JMIR Dermatol6, e49280 (2023).\n70. Truhn, D. et al. A pilot study on the efﬁcacy of GPT-4 in providing\northopedic treatment recommendations from MRI reports.Sci Rep\n13, 20159 (2023).\n71. Hurley, E. T. et al. Evaluation High-Quality of Information from\nChatGPT (Artiﬁcial Intelligence-Large Language Model) Artiﬁcial\nIntelligence on Shoulder Stabilization Surgery.Arthroscopy. https://\ndoi.org/10.1016/j.arthro.2023.07.048 (2023)\n72. Cankurtaran, R. E., Polat, Y. H., Aydemir, N. G., Umay, E. & Yurekli, O.\nT. Reliability and usefulness of ChatGPT for inﬂammatory bowel\ndiseases: an analysis for patients and healthcare professionals.\nCureus 15, e46736 (2023).\n73. Birkun, A. A. & Gautam, A. Large language model (LLM)-powered\nchatbots fail to generate guideline-consistent content on\nresuscitation and may provide potentially harmful advice.Prehosp\nDisaster Med38, 757–763 (2023).\n74. Pushpanathan, K. et al. Popular large language model chatbots’\naccuracy, comprehensiveness, and self-awareness in answering\nocular symptom queries.iScience 26, 108163 (2023).\n75. Shao, C. Y. et al. Appropriateness and comprehensiveness of using\nChatGPT for perioperative patient education in thoracic surgery in\ndifferent language contexts: survey study.Interact J Med Res12,\ne46900 (2023).\n76. Vaira, L. A. et al. Accuracy of ChatGPT-Generated Information on\nHead and Neck and Oromaxillofacial Surgery: A Multicenter\nCollaborative Analysis.Otolaryngol Head Neck Surg. https://doi.org/\n10.1002/ohn.489 (2023)\n77. Chen, S. et al. Use of artiﬁcial intelligence Chatbots for cancer\ntreatment information.JAMA Oncol9, 1459–1462 (2023).\n78. Bellinger, J. R. et al. BPPV Information on Google Versus AI\n(ChatGPT). Otolaryngol Head Neck Surg. https://doi.org/10.1002/\nohn.506 (2023)\n79. Nielsen, J. P. S., von Buchwald, C. & Grønhøj, C. Validity of the large\nlanguage model ChatGPT (GPT4) as a patient information source in\notolaryngology by a variety of doctors in a tertiary otorhinolaryngology\ndepartment.Acta Otolaryngol143,7 7 9–782 (2023).\n80. Sezgin, E., Chekeni, F., Lee, J. & Keim, S. Clinical accuracy of large\nlanguage models and google search responses to postpartum\ndepression questions: cross-sectional study.J Med Internet Res25,\ne49240 (2023).\n81. Floyd, W. et al. Current Strengths and Weaknesses of ChatGPT as a\nResource for Radiation Oncology Patients and Providers.\nInternational Journal of Radiation Oncology, Biology, Physics.\nhttps://doi.org/10.1016/j.ijrobp.2023.10.020 (2023)\n82. Uz, C. & Umay, E.“Dr ChatGPT”: is it a reliable and useful source for\ncommon rheumatic diseases?\nInt J Rheum Dis26, 1343–1349\n(2023).\n83. Athavale, A., Baier, J., Ross, E. & Fukaya, E. The potential of chatbots\nin chronic venous disease patient management.JVS Vasc Insights1.\nhttps://doi.org/10.1016/j.jvsvi.2023.100019 (2023)\n84. Li, Y. et al. ChatDoctor: a medical chat modelﬁne-tuned on a large\nlanguage model meta-AI (LLaMA) using medical domain knowledge.\nCureus 15, e40895 (2023).\n85. Seth, I. et al. Comparing the efﬁcacy of large language models\nChatGPT, BARD, and Bing AI in providing information on\nrhinoplasty: an observational study.Aesthet Surg J Open Forum5,\nojad084 (2023).\n86. Lockie, E. & Choi, J. Evaluation of a chat GPT generated patient\ninformation leaﬂet about laparoscopic cholecystectomy.ANZ J\nSurg. https://doi.org/10.1111/ans.18834 (2023)\n87. Haver, H. L., Lin, C. T., Sirajuddin, A., Yi, P. H. & Jeudy, J. Use of\nChatGPT, GPT-4, and Bard to improve readability of ChatGPT’s\nanswers to common questions about lung cancer and lung cancer\nscreening. AJR Am J Roentgenol221, 701–704 (2023).\n88. Li, H. et al. Decoding radiology reports: potential application of\nOpenAI ChatGPT to enhance patient understanding of diagnostic\nreports. Clin Imaging101, 137–141 (2023).\n89. Scheschenja, M. et al. Feasibility of GPT-3 and GPT-4 for in-depth\npatient education prior to interventional radiological procedures: a\ncomparative analysis.CardioVascular and Interventional Radiology\n47, 245–250 (2024).\n90. Gordon, E. B. et al. Enhancing patient communication With Chat-\nGPT in radiology: evaluating the efﬁcacy and readability of answers\nto common imaging-related questions.J Am Coll Radiol21,\n353–359 (2024).\n91. Stroop, A. et al. Large language models: Are artiﬁcial intelligence-based\nchatbots a reliable source of patient information for spinal surgery?Eur\nSpine J. https://doi.org/10.1007/s00586-023-07975-z(2023)\n92. Coraci, D. et al. ChatGPT in the development of medical\nquestionnaires. The example of the low back pain.Eur J Transl Myol\n33. https://doi.org/10.4081/ejtm.2023.12114 (2023)\n93. Ye, C., Zweck, E., Ma, Z., Smith, J. & Katz, S. Doctor Versus Artiﬁcial\nIntelligence: Patient and Physician Evaluation of Large Language Model\nResponses to Rheumatology Patient Questions in a Cross-Sectional\nStudy.Arthritis & Rheumatologyn/a https://doi.org/10.1002/art.42737\n94. Mohammad-Rahimi, H. et al. Validity and reliability of artiﬁcial\nintelligence chatbots as public sources of information on\nendodontics. Int Endod J57, 305–314 (2024).\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 10\n95. Hermann, C. E. et al. Let’s chat about cervical cancer: Assessing the\naccuracy of ChatGPT responses to cervical cancer questions.\nGynecologic Oncology179, 164–168 (2023).\n96. Kerbage, A. et al. Accuracy of ChatGPT in Common Gastrointestinal\nDiseases: Impact for Patients and Providers.Clin Gastroenterol\nHepatol. https://doi.org/10.1016/j.cgh.2023.11.008 (2023)\n97. Shiraishi, M. et al. Generating Informed Consent Documents Related\nto Blepharoplasty Using ChatGPT.Ophthalmic Plast Reconstr Surg.\nhttps://doi.org/10.1097/iop.0000000000002574 (2023)\n98. Barclay, K. S. et al. Quality and Agreement With Scientiﬁc\nConsensus of ChatGPT Information Regarding Corneal\nTransplantation and Fuchs Dystrophy.Cornea. https://doi.org/10.\n1097/ico.0000000000003439 (2023)\n99. Qarajeh, A. et al. AI-powered renal diet support: performance of\nChatGPT, Bard AI, and Bing Chat.Clin Pract13, 1160–1172\n(2023).\n100. Chowdhury, M. et al.Can Large Language Models Safely Address\nPatient Questions Following Cataract Surgery?, (2023).\n101. Singer, M. B., Fu, J. J., Chow, J. & Teng, C. C. Development and\nevaluation of aeyeconsult: a novel ophthalmology chatbot\nleveraging veriﬁed textbook knowledge and GPT-4.J Surg Educ81,\n438–443 (2024).\n102. Xie, Y. et al. Aesthetic surgery advice and counseling from artiﬁcial\nintelligence: a rhinoplasty consultation with ChatGPT.Aesthetic\nPlast Surg47, 1985–1993 (2023).\n103. Nastasi, A. J., Courtright, K. R., Halpern, S. D. & Weissman, G. E. A\nvignette-based evaluation of ChatGPT’s ability to provide\nappropriate and equitable medical advice across care contexts.Sci\nRep 13, 17885 (2023).\n104. Biswas, M., Islam, A., Shah, Z., Zaghouani, W. & Brahim Belhaouari,\nS. Can ChatGPT be Your Personal Medical Assistant?, (2023).\n105. Panagoulias, D., Palamidas, F., Virvou, M. & Tsihrintzis, G.\nEvaluating the Potential of LLMs and ChatGPT on Medical Diagnosis\nand Treatment. (2023).\n106. Chandra, A., Davis, M. J., Hamann, D. & Hamann, C. R. Utility of\nallergen-speciﬁc patient-directed handouts generated by chat\ngenerative pretrained transformer.Dermatitis 34, 448 (2023).\n107. Hung, Y.-C., Chaker, S., Sigel, M., Saad, M. & Slater, E. Comparison\nof patient education materials generated by chat generative pre-\ntrained transformer versus experts: an innovative way to increase\nreadability of patient education materials.Annals of Plastic Surgery\n91, 409–412 (2023).\n108. Capelleras, M., Soto-Galindo, G. A., Cruellas, M. & Apaydin, F.\nChatGPT and Rhinoplasty Recovery: An Exploration of AI’s Role in\nPostoperative Guidance.Facial Plast Surg(2024). https://doi.org/\n10.1055/a-2219-4901\n109. Scquizzato, T. et al. Testing ChatGPT ability to answer laypeople\nquestions about cardiac arrest and cardiopulmonary resuscitation.\nResuscitation 194, 110077 (2024).\n110. Kuckelman, I. J. et al. Assessing AI-powered patient education: a\ncase study in radiology.Acad Radiol31, 338–342 (2024).\n111. Sulejmani, P. et al. A large language model artiﬁcial intelligence for\npatient queries in atopic dermatitis.J Eur Acad Dermatol Venereol.\nhttps://doi.org/10.1111/jdv.19737 (2024)\n112. Currie, G. & Barry, K. ChatGPT in nuclear medicine education.\nJournal of Nuclear Medicine Technology51, 247–254 (2023).\n113. Currie, G. M. Academic integrity and artiﬁcial intelligence: is\nChatGPT hype, hero or heresy?Seminars in Nuclear Medicine53,\n719–730 (2023).\n114. Li, J., Dada, A., Puladi, B., Kleesiek, J. & Egger, J. ChatGPT in\nhealthcare: a taxonomy and systematic review.Computer Methods\nand Programs in Biomedicine245, 108013 (2024).\n115. Liu, F. W. & Hu, C. Exploring Vulnerabilities and Protections in Large\nLanguage Models: A Survey.arXiv preprint arXiv:2406.00240(2024).\n116. Jin, M. et al. Health-LLM: Personalized Retrieval-Augmented\nDisease Prediction Model.arXiv preprint arXiv:2402.00746(2024).\n117. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E.\nCapabilities of gpt-4 on medical challenge problems.arXiv preprint\narXiv:2303.13375 (2023).\n118. Brin, D. et al. Comparing ChatGPT and GPT-4 performance in\nUSMLE soft skill assessments.Scientiﬁc Reports13, 16492\n(2023).\n119. Jung, L. B. et al. ChatGPT passes German State examination in\nmedicine with picture questions omitted.Dtsch Arztebl Int120,\n373–374 (2023).\n120. Bhayana, R., Krishna, S. & Bleakney, R. R. Performance of ChatGPT\non a radiology board-style examination: insights into current\nstrengths and limitations.Radiology 307, e230582 (2023).\n121. Singhal, K. et al. Towards expert-level medical question answering\nwith large language models.arXiv preprint arXiv:2305.09617\n(2023).\n122. Dorfner, F. J. et al. Biomedical Large Languages Models Seem not to\nbe Superior to Generalist Models on Unseen Medical Data.arXiv\npreprint arXiv:2408.13833(2024).\n123. Kung, T. H. et al. Performance of ChatGPT on USMLE: Potential for\nAI-assisted medical education using large language models.PLOS\nDigital Health2, e0000198 (2023).\n124. Kapoor, S., Henderson, P. & Narayanan, A. Promises and pitfalls of\nartiﬁcial intelligence for legal applications.arXiv preprint\narXiv:2402.01656 (2024).\n125. Ouyang, S., Zhang, J. M., Harman, M. & Wang, M. LLM is Like a Box\nof Chocolates: the Non-determinism of ChatGPT in Code\nGeneration. arXiv preprint arXiv:2308.02828(2023).\n126. Atil, B. et al. LLM Stability: A detailed analysis with some surprises.\narXiv preprint arXiv:2408.04667(2024).\n127. Weis, B. Health Literacy: A Manual for Clinicians. Chicago, IL:\nAmerican Medical Association, American Medical Foundation;\n2003. National Institutes of Health. How to Write Easy to Read Health\nMaterials: National Library of Medicine Website.How to Write Easy\nto Read Health Materials: National Library of Medicine Website\n128. Amann, J. et al. Explainability for artiﬁcial intelligence in healthcare: a\nmultidisciplinary perspective.BMC Medical Informatics and\nDecision Making20, 310 (2020).\n129. Gerke, S., Minssen, T. & Cohen, G. Ethical and legal challenges of\nartiﬁcial intelligence-driven healthcare.Artiﬁcial Intelligence in\nHealthcare, 295-336.https://doi.org/10.1016/b978-0-12-818438-\n7.00012-5 (2020)\n130. Tobias, J. S. & Souhami, R. L. Fully informed consent can be\nneedlessly cruel.Bmj 307, 1199–1201 (1993).\n131. Pietrzykowski, T. & Smilowska, K. The reality of informed consent:\nempirical studies on patient comprehension— systematic review.\nTrials 22, 57 (2021).\n132. Kesselheim, A. S., Connolly, J., Rogers, J. & Avorn, J. Mandatory\ndisclaimers on dietary supplements do not reliably communicate the\nintended issues.Health Affairs34, 438–446 (2015).\n133. Topol, E. J. High-performance medicine: the convergence of human\nand artiﬁcial intelligence.Nature Medicine25,4 4–56 (2019).\n134. Raj, H., Gupta, V., Rosati, D. & Majumdar, S. Semantic consistency\nfor assuring reliability of large language models.arXiv preprint\narXiv:2308.09138 (2023).\n135. Zhou, Y. et al. Large Language Models Are Human-Level Prompt\nEngineers. ArXiv abs/2211.01910 (2022).\n136. Zhang, Z. et al. Certiﬁed Robustness for Large Language Models\nwith Self-Denoising.ArXiv abs/2307.07171 (2023).\n137. Ullah, E., Parwani, A., Baig, M. M. & Singh, R. Challenges and barriers\nof using large language models (LLM) such as ChatGPT for\ndiagnostic medicine with a focus on digital pathology - a recent\nscoping review.Diagn Pathol19, 43 (2024).\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 11\n138. Navigli, R., Conia, S. & Ross, B. Biases in large language models:\norigins, inventory, and discussion.ACM Journal of Data and\nInformation Quality15,1 –21 (2023).\n139. Deng, G. et al. Jailbreaker: Automated jailbreak across multiple large\nlanguage model chatbots.arXiv preprint arXiv:2307.08715(2023).\n140. Acerbi, A. & Stubbersﬁeld, J. M. Large language models show human-\nlike content biases in transmission chain experiments.Proceedings of\nthe National Academy of Sciences120, e2313790120 (2023).\n141. Yang, Y., Liu, Y. & Naghizadeh, P. Adaptive data debiasing through\nbounded exploration.Advances in Neural Information Processing\nSystems 35, 1516–1528 (2022).\n142. Gallegos, I. O. et al. Bias and Fairness in Large Language Models: A\nSurvey. Computational Linguistics, 1-83.https://doi.org/10.1162/\ncoli_a_00524 (2024)\n143. Grari, V., Laugel, T., Hashimoto, T., Lamprier, S. & Detyniecki, M. On\nthe Fairness ROAD: Robust Optimization for Adversarial Debiasing.\narXiv preprint arXiv:2310.18413(2023).\n144. Hofmann, V., Kalluri, P. R., Jurafsky, D. & King, S. AI generates\ncovertly racist decisions about people based on their dialect.Nature\n633, 147–154 (2024).\n145. Ayoub, N. F., Lee, Y. J., Grimm, D. & Divi, V. Head-to-Head\nComparison of ChatGPT Versus Google Search for Medical\nKnowledge Acquisition.Otolaryngol Head Neck Surg. https://doi.\norg/10.1002/ohn.465 (2023)\n146. Yao, Y. et al. A survey on large language model (LLM) security and\nprivacy: the good, the bad, and the ugly.High-Conﬁdence\nComputing 4, 100211 (2024).\n147. Tierney, A. A. et al. Ambient artiﬁcial intelligence scribes to alleviate\nthe burden of clinical documentation.NEJM Catalyst5,\nCAT.23.0404 (2024).\n148. Formosa, P., Rogers, W., Griep, Y., Bankins, S. & Richards, D.\nMedical AI and human dignity: contrasting perceptions of human\nand artiﬁcially intelligent (AI) decision making in diagnostic and\nmedical resource allocation contexts.Computers in Human\nBehavior 133, 107296 (2022).\n149. Moy, S. et al. Patient perspectives on the use of artiﬁcial intelligence\nin health care: a scoping review.J Patient Cent Res Rev11,5 1–62\n(2024).\n150. Union, C. O. T. E. Proposal for a Regulation of the European\nParliament and of the Council laying down harmonised rules on\nartiﬁcial intelligence (Artiﬁcial Intelligence Act) and amending certain\nUnion legislative acts - Analysis of theﬁnal compromise text with a\nview to agreement, (2024).\n151. Busch, F. et al. Navigating the european union artiﬁcial intelligence\nact for healthcare.npj Digital Medicine7, 210 (2024).\nAcknowledgements\nThis research is funded by the European Union (101079894). Views and\nopinions expressed are however those of the authors only and do not\nnecessarily reﬂect those of the European Union or European Commission.\nNeither the European Union nor the granting authority can be held\nresponsible for them. The funding had no role in the study design, data\ncollection and analysis, manuscript preparation, or decision to publish.\nAuthor contributions\nConceptualization: FB, LH, CR, EHCvD, RK, EOP, MRM, LS, MH, JNK, DT,\nRC, LCA, KKB; Project administration: FB; Resources: FB, LCA, KKB;\nSoftware: FB, LCA, KKB; Data curation: FB, LH, CR; Formal analysis: FB, LH,\nCR, LCA, KKB; Investigation: FB, LH, CR, LCA, KKB; Methodology: FB;\nSupervision: FB, LCA, KKB; Validation: FB, LH, CR, EHCvD, RK, EOP, MRM,\nLS, MH, JNK, DT, RC, LCA, KKB; Visualization: FB, LCA; Writing– original\ndraft preparation: FB, LH, LCA, KKB; Writing– review & editing: FB, LH, CR,\nEHCvD, RK, EOP, MRM, LS, MH, JNK, DT, RC, LCA, KKB.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests\nJNK declares consulting services for Owkin, France; DoMore Diagnostics,\nNorway; Panakeia, UK, and Scailyte, Basel, Switzerland; furthermore JNK\nholds shares in Kather Consulting, Dresden, Germany; and StratifAI GmbH,\nDresden, Germany, and has received honoraria for lectures and advisory\nboard participation by AstraZeneca, Bayer, Eisai, MSD, BMS, Roche, Pﬁzer\nand Fresenius. DT holds shares in StratifAI GmbH, Dresden, Germany and\nhas received honoraria for lectures by Bayer. KKB reports grants from the\nEuropean Union(101079894) and Wilhelm-Sander Foundation; participation\non a Data Safety Monitoring Board or Advisory Board for the EU Horizon\n2020 LifeChamps project (875329) and the EU IHI Project IMAGIO\n(101112053); speaker Fees for Canon Medical Systems Corporation and GE\nHealthCare. RK receives medical consultancy fees from Odin Vision. The\nremaining authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s43856-024-00717-2.\nCorrespondenceand requests for materials should be addressed to\nFelix Busch.\nPeer review informationCommunications Medicinethanks Dmitry\nScherbakov and the other, anonymous, reviewer(s) for their contribution to\nthe peer review of this work.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article's Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle's Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025\n1School of Medicine and Health, Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, TUM University Hospital, Technical University of\nMunich,Munich, Germany.2Departmentof Neuroradiology, Charité–UniversitätsmedizinBerlin, Corporate Member of Freie Universität Berlinand Humboldt Universität\nzu Berlin, Berlin, Germany.3Department of Ophthalmology, Leiden University Medical Center, Leiden, The Netherlands.4Department of Ophthalmology, Sir Charles\nGairdner Hospital,Perth, Australia.5Division of Surgery andInterventional Sciences,University College London, London, United Kingdom.6One Health Research Group,\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 12\nFaculty of Health Science, Universidad de Las Américas, Quito, Ecuador.7Department ofRadiology,Azienda Ospedaliero Universitaria (A.O.U.),Cagliari,Italy.8School of\nMedicine and Health, Institute for Cardiovascular Radiology and Nuclear Medicine, German Heart Center Munich, TUM University Hospital, TechnicalUniversity of\nMunich, Munich, Germany.9Department of Medical Oncology, National Center for Tumor Diseases (NCT), Heidelberg University Hospital, Heidelberg, Germany.10Else\nKroener Fresenius Center for Digital Health, Medical Faculty Carl Gustav Carus, Technical University Dresden, Dresden, Germany.11Department of Diagnostic and\nInterventional Radiology, University Hospital Aachen, Aachen, Germany.12Department of Medicine, Surgery and Dentistry, University of Salerno, Baronissi, Italy.\n13These authors contributed equally: Lisa C. Adams, Keno K. Bressem.e-mail: felix.busch@tum.de\nhttps://doi.org/10.1038/s43856-024-00717-2 Article\nCommunications Medicine|            (2025) 5:26 13",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.4914306402206421
    },
    {
      "name": "Automatic summarization",
      "score": 0.4103272557258606
    },
    {
      "name": "Medicine",
      "score": 0.40451663732528687
    },
    {
      "name": "Medical education",
      "score": 0.34322303533554077
    },
    {
      "name": "Computer science",
      "score": 0.3173074722290039
    },
    {
      "name": "Artificial intelligence",
      "score": 0.16913941502571106
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802619606",
      "name": "Klinikum rechts der Isar",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I7877124",
      "name": "Charité - Universitätsmedizin Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I75951250",
      "name": "Freie Universität Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2802220196",
      "name": "Sir Charles Gairdner Hospital",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I2800006345",
      "name": "Leiden University Medical Center",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210102282",
      "name": "Universidad de Las Américas",
      "country": "EC"
    },
    {
      "id": "https://openalex.org/I4210124799",
      "name": "Azienda Ospedaliero-Universitaria Cagliari",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I4210148371",
      "name": "Deutsches Herzzentrum München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I223822909",
      "name": "Heidelberg University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210111460",
      "name": "National Center for Tumor Diseases",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210162051",
      "name": "University Hospital Carl Gustav Carus",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2802164966",
      "name": "University Hospital Heidelberg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210120689",
      "name": "Universitätsklinikum Aachen",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I131729948",
      "name": "University of Salerno",
      "country": "IT"
    }
  ],
  "cited_by": 83
}