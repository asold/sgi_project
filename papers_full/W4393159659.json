{
  "title": "Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers",
  "url": "https://openalex.org/W4393159659",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5012412123",
      "name": "Hadi Abdine",
      "affiliations": [
        "Laboratoire d'Informatique de l'École Polytechnique",
        "École Polytechnique"
      ]
    },
    {
      "id": "https://openalex.org/A5072113617",
      "name": "Michail Chatzianastasis",
      "affiliations": [
        "Laboratoire d'Informatique de l'École Polytechnique",
        "École Polytechnique"
      ]
    },
    {
      "id": "https://openalex.org/A5023347965",
      "name": "Costas Bouyioukos",
      "affiliations": [
        "University of Cyprus",
        "Epigénétique et Destin Cellulaire",
        "Université Paris Cité"
      ]
    },
    {
      "id": "https://openalex.org/A5057695979",
      "name": "Michalis Vazirgiannis",
      "affiliations": [
        "Laboratoire d'Informatique de l'École Polytechnique",
        "École Polytechnique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1982597966",
    "https://openalex.org/W3165163830",
    "https://openalex.org/W6810847328",
    "https://openalex.org/W4223920029",
    "https://openalex.org/W6814377679",
    "https://openalex.org/W7025124536",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4225000967",
    "https://openalex.org/W3113698408",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2936543792",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W4286500588",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3092867907",
    "https://openalex.org/W6760949768",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W6677316912",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6766030139",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W3188579603",
    "https://openalex.org/W2786016794",
    "https://openalex.org/W2960216239",
    "https://openalex.org/W4288099645",
    "https://openalex.org/W4385572894",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2739999456",
    "https://openalex.org/W2947926079",
    "https://openalex.org/W4387805937",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4382239070",
    "https://openalex.org/W4288419255",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W3109892317",
    "https://openalex.org/W4236358448",
    "https://openalex.org/W4318751307",
    "https://openalex.org/W3164046276",
    "https://openalex.org/W4281478251",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4233120011",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965857891",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4315928370",
    "https://openalex.org/W4320342754",
    "https://openalex.org/W2950277699",
    "https://openalex.org/W4310273071",
    "https://openalex.org/W3211795435",
    "https://openalex.org/W3034999214"
  ],
  "abstract": "In recent years, significant progress has been made in the field of protein function prediction with the development of various machine-learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e. assigning predefined labels to proteins. In this work, we propose a novel approach, Prot2Text, which predicts a protein's function in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including protein sequence, structure, and textual annotation and description. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate functional descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate function prediction of existing as well as first-to-see proteins.",
  "full_text": "Prot2Text: Multimodal Protein’s Function Generation with GNNs and\nTransformers\nHadi Abdine1, Michail Chatzianastasis1, Costas Bouyioukos2, 3, Michalis Vazirgiannis1\n1Laboratoire d’Informatique (LIX), ´Ecole Polytechnique, Institut Polytechnique de Paris, Palaiseau, France\n2Epigenetics and Cell Fate, CNRS UMR7216, Universit´e Paris Cit´e, F-75013 Paris, France.\n3Bioinformatics Research Laboratory, Department of Biological Sciences, University of Cyprus, Nicosia, Cyprus\n{hadi.abdine, michail.chatzianastasis}@polytechnique.edu, costas.bouyioukos@u-paris.fr, mvazirg@lix.polytechnique.fr\nAbstract\nIn recent years, significant progress has been made in the field\nof protein function prediction with the development of var-\nious machine-learning approaches. However, most existing\nmethods formulate the task as a multi-classification problem,\ni.e. assigning predefined labels to proteins. In this work, we\npropose a novel approach, Prot2Text, which predicts a pro-\ntein’s function in a free text style, moving beyond the con-\nventional binary or categorical classifications. By combining\nGraph Neural Networks(GNNs) and Large Language Mod-\nels(LLMs), in an encoder-decoder framework, our model ef-\nfectively integrates diverse data types including protein se-\nquence, structure, and textual annotation and description.\nThis multimodal approach allows for a holistic representa-\ntion of proteins’ functions, enabling the generation of detailed\nand accurate functional descriptions. To evaluate our model,\nwe extracted a multimodal protein dataset from SwissProt,\nand demonstrate empirically the effectiveness of Prot2Text.\nThese results highlight the transformative impact of multi-\nmodal models, specifically the fusion of GNNs and LLMs,\nempowering researchers with powerful tools for more accu-\nrate function prediction of existing as well as first-to-see pro-\nteins.\n1 Introduction\nUnderstanding proteins’ function is a central problem in bio-\nlogical sciences, as proteins are the fundamental elements of\nalmost all biological functions. Accurate prediction of pro-\nteins’ function is essential for understanding biological sys-\ntems as well as for various applications, such as drug dis-\ncovery, enabling researchers to identify and target specific\nproteins that play critical roles in disease pathways (Ha et al.\n2021). Traditionally, proteins’ functions prediction has been\napproached through classification methods, assigning prede-\nfined labels to proteins based on their characteristics (Kul-\nmanov and Hoehndorf 2019). However, this approach often\noversimplifies the complexity of proteins’ functionality, lim-\niting the depth of our understanding. To overcome this lim-\nitation, we propose a novel view on proteins’ functions pre-\ndiction based on reformulating the task using free-text pro-\nteins’ descriptions instead of relying on predefined labels.\nThe rapid progress in transformer-based models has brought\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\na massive revolution to the field of Natural Language Pro-\ncessing (NLP). These models have demonstrated impressive\nlanguage generation capabilities, allowing them to perform\na wide range of NLP tasks with remarkable performance,\nincluding text completion, translation, sentiment analysis\nand question-answering (Vaswani et al. 2017; Radford et al.\n2019; Brown et al. 2020). On the other hand, Graph Neu-\nral Networks(GNNs) have emerged as a powerful tool for\nmodeling graph-structured data, capturing the intricate rela-\ntionships between different elements in a graph (Kipf and\nWelling 2017; Reiser et al. 2022). However, the integration\nof GNNs and transformers faces various challenges, such\nas effectively handling the heterogeneity of data represen-\ntations, therefore the field is still in its early stages. De-\nspite this, the potential benefits of leveraging both GNNs\nand transformers for graph-to-text applications, such as pre-\ndicting the functional properties of proteins are substantial.\nTo that end, we develop a novel multimodal framework,\nProt2Text, that can generate detailed and accurate descrip-\ntions of proteins’ functions in free text. We effectively in-\ntegrate GNNs and Large Language Models (LLMs), to en-\ncompass both structural and sequential information of the\nprotein’s 3D structure and amino acid’s sequence respec-\ntively. The encoder-decoder architecture forms the backbone\nof our model, with the encoder component employing a Re-\nlational Graph Convolution Network (RGCN) (Schlichtkrull\net al. 2018) to process the proteins’ graphs and the ESM\nprotein language model (Lin et al. 2023a) to encode the\nproteins’ sequences. The decoder component utilizes a pre-\ntrained GPT-2 model to generate detailed proteins’ descrip-\ntions. To train our multimodal model, we compile a dataset\nof proteins extracted from SwissProt, a comprehensive col-\nlection of protein annotations obtained from the UniProt\ndatabase (Consortium 2015). This dataset encompasses a\nvast number of proteins, each annotated with its correspond-\ning function or description. In addition to the textual infor-\nmation, we obtain the 3D structure representation of the pro-\nteins from AlphaFold (Varadi et al. 2022). We further release\nthis curated dataset to the public, allowing other researchers\nto use it for benchmarking and further advancements in the\nfield. Code, data and models are publicly available 1. Our\nmain contributions can be summarized as follows:\n1https://github.com/hadi-abdine/Prot2Text\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10757\n• We introduce the Prot2Text framework, a novel multi-\nmodal approach for generating proteins’ functions in free\ntext. Our model combines both GNNs and ESM to en-\ncode the protein in a fused representation while a pre-\ntrained GPT-2 decodes the protein’s text description.\n• We propose various baselines for protein text generation\nand demonstrate that the integration of both graph and\nsequence protein information leads to better generation\ncapabilities.\n• We further release a comprehensive multimodal protein\ndataset, which includes 256, 690 protein structures, se-\nquences, and textual function descriptions. Researchers\ncan leverage this dataset to benchmark and compare their\nmodels, thereby driving advancements in the field and\nenabling for a more robust and standardized evaluation\nof proteins’ functions prediction methods in free text for-\nmat.\n2 Related Work\nTransformers. The transformer-based encoder-decoder\nmodel was first introduced by Vaswani et al. (2017). Since\nthen, this model architecture has become the de-facto stan-\ndard encoder-decoder architecture in Natural Language Pro-\ncessing (NLP). Despite significant research on different pre-\ntraining objectives for transformer-based encoder-decoder\nmodels such as T5 (Raffel et al. 2019) and Bart (Lewis et al.\n2020), the model architecture has remained largely the same.\nRadford et al. took advantage of the transformer architec-\nture, which is superior and conceptually simpler than Recur-\nrent Neural Networks to introduce the OpenAI GPT model.\nSpecifically, they pretrained a left-to-right transformer de-\ncoder as a general language model using the transformer\narchitecture. Following, they fine-tuned the model on 12\ndifferent language understanding tasks by applying various\ntransformations to the input. Later, GPT-2 (Radford et al.\n2019), a more advanced version of GPT with more trainable\nparameters, was introduced. The authors showed that as long\nas general language models have very high capacities, they\ncan reach reasonable performance on many specific natural\nlanguage processing tasks. The use of the transformer ar-\nchitecture later expanded to include modalities other than\nnatural language, such as images (Dosovitskiy et al. 2021),\nprotein amino acid sequence (Rives et al. 2021; Lin et al.\n2023a), and molecules SMILES string (Fabian et al. 2020;\nChithrananda, Grand, and Ramsundar 2020).\nMultimodal models. The success of the transformer’s\nuni-modality tasks made this architecture broadly studied\nfor multimodal representation learning. One example is The\nCLIP (Contrastive Language-Image Pre-training) model\n(Radford et al. 2021) which is a transformer model that\nfacilitates cross-modal understanding between images and\ntext. It combines a ViT vision encoder, with a transformer-\nbased language encoder to learn joint representations of im-\nages and their associated textual descriptions. Another ex-\nample is the MolT5 (Edwards et al. 2022) which is a self-\nsupervised learning framework based on the T5 model for\npretraining models on a vast amount of unlabeled natural\nlanguage text and molecule SMILES strings. MolT5 is able\nto perform bidirectional translation between molecule repre-\nsentations and natural language allowing molecule caption-\ning and generation providing text prompts. ProtST (Xu et al.\n2023), enhances the protein language model classification\nand retrieval capabilities by co-training it with biomedical\ntext. While ProteinDT (Liu et al. 2023) uses protein lan-\nguage models and pretrained language models to perform\ntext-guided protein generation. Both of the aforementioned\ntext-protein multimodal frameworks take only the protein\nsequence into consideration to encode the proteins.\nGraph Neural Networks. Graph neural networks\n(GNNs) have emerged as a powerful framework for mod-\neling and analyzing graph-structured data (Scarselli et al.\n2009; Kipf and Welling 2017). By iteratively exchanging\nand integrating information among nodes, GNNs can prop-\nagate and refine features throughout the graph, ultimately\nencoding a comprehensive understanding of the graph’s\nstructure and semantics. This ability to capture complex\nrelationships within graphs has contributed to the success\nof GNNs in various domains, including social network\nanalysis, recommendation systems, and bioinformatics\n(Zitnik, Agrawal, and Leskovec 2018; Zhang et al. 2021;\nChatzianastasis, Vazirgiannis, and Zhang 2023). Numerous\nstudies have suggested various enhancements and expan-\nsions to the GNNs’ models. Some notable contributions\ninclude the introduction of more expressive and adaptable\naggregation functions, such as those proposed by Murphy\net al. (2019), Seo, Loukas, and Perraudin (2019) and\nChatzianastasis et al. (2023). Moreover, several schemes\nhave been developed to incorporate different local struc-\ntures or high-order neighborhoods, as explored by Morris,\nRattan, and Mutzel (2020) and Nikolentzos, Dasoulas, and\nVazirgiannis (2020). Furthermore, the domain of GNNs\nhas expanded to encompass heterogeneous graphs, where\nnodes and edges can have different types and semantics,\nleading to the development of Heterogeneous Graph\nNeural Networks effectively handling such complex graph\nstructures (Schlichtkrull et al. 2018; Zhang et al. 2019).\nProtein Representation Learning. In the field of protein\nrepresentation learning, various approaches have emerged\nover the years, aiming to capture meaningful information\nfrom proteins using different data modalities and computa-\ntional techniques. One prominent avenue of research is fo-\ncused on sequence-based representations, that extract fea-\ntures solely from the amino acid sequences of proteins.\nDrawing inspiration from the remarkable achievements of\nlanguage models in Natural Language Processing (NLP),\nresearchers have also developed pretrained language mod-\nels tailored specifically for proteins (Brandes et al. 2022;\nLin et al. 2023b). These models leverage large-scale pro-\ntein datasets to learn powerful representations that can sub-\nsequently be utilized for various prediction tasks. In addition\nto sequence-based approaches, graph-based representations\nleverage the three-dimensional (3D) structure of proteins\nto capture their functional properties. Zhang et al. (2022)\nproposed a graph neural network model with a contrastive\npertaining strategy for function prediction and fold classifi-\ncation tasks. Chen et al. (2023) proposed a 3D-equivariant\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10758\ngraph neural network, specifically designed to estimate the\naccuracy of protein structural models. Wang et al. (2022)\nused a hierarchical graph network, which captures the hi-\nerarchical relations present in proteins and learns represen-\ntations at different levels of granularity. Hybrid approaches\nintegrate multiple modalities, including protein sequences,\nstructures, and functional annotations, to create comprehen-\nsive representations. These methods combine the strengths\nof sequence-based and graph-based models to capture di-\nverse aspects of protein function. Gligorijevi ´c et al. (2021)\nproposed DeepFRI which combines sequence features ex-\ntracted from a protein language model with protein struc-\ntures. Our work aims to leverage protein sequence and struc-\nture models to generate free text annotations of proteins.\n3 Methodology\nIn this section, we present our proposed multimodal frame-\nwork, Prot2Text, for generating protein function descrip-\ntions in free text. An illustration of the proposed architecture\ncan be found in Figure 1.\nGraph Construction. Upon obtaining the 3D proteins’\nstructures using AlphaFold, we proceed to represent the\nproteins as a heterogeneous graph G = (V, E, R), where\nV = [N] := {1, ..., N} is the set of vertices representing\nthe amino acids of the proteins, E ⊆ V × V is the set of\nedges representing various interactions between the nodes\nand R is a set of different edge interactions. Each node u is\nassociated with a feature vector xu ∈ Rd, encompassing\nrelevant information such as local structural features, and\nphysicochemical properties of the associated amino acids.\nThis enables the graph to retain fine-grained information\ncritical to the protein’s structure and function. To model the\ndiverse interactions and relationships between amino acids,\nwe introduce different types of edges connecting the nodes.\nTherefore, each edge i = (v, u) is associated with an edge\ntype ei ∈ R. Sequential edges are employed to connect ad-\njacent nodes in the protein sequence, effectively represent-\ning the sequential order of amino acids and capturing the\nlinear arrangement of the protein’s primary structure. This\nsequential information is crucial for understanding the fold-\ning patterns and functional motifs within the protein. Ad-\nditionally, we utilize spatial edges to establish connections\nbetween nodes that are in close spatial proximity within the\n3D structure of the protein. These edges play a pivotal role in\nencoding the protein’s tertiary structure and folding patterns,\nenabling us to capture the intricate spatial arrangements of\namino acids within the protein’s core. We further extend the\ngraph construction to include hydrogen bond interactions as\nan additional edge type. Hydrogen bonds are fundamental\nnon-covalent interactions that are of paramount importance\nin stabilizing protein structures and enabling specific molec-\nular recognition events. Through the integration of the dif-\nferent edge types, our comprehensive protein graph provides\na more holistic and detailed depiction of the protein’s struc-\nture while capturing both short and long-range interactions.\nGraph Encoding. To encode the protein graph G into a\nvector hG ∈ Rdout , we employ a Relational Graph Convo-\nlutional Neural Network(RGCN) (Schlichtkrull et al. 2018),\nwhich effectively considers the various edge types present\nin the graph in the message-passing mechanism. We denote\nthe neighborhood of type r of a vertex u by Nr(u) such that\nNr(u) = {v : (v, u) ∈ Er}, where Er is the set of edges\nwith r edge type. In layer k of the GNN, we update the node\nrepresentations as follows:\nxk\ni = σ\n\nWk\nroot · xk−1\ni +\nX\nr∈R\nX\nj∈Nr(i)\n1\n|Nr(i)|Wk\nr · xk−1\nj\n\n,\n(1)\nwhere Wk\nroot represents the learnable weight matrix for the\nroot transformation in layer k, Wk\nr denotes the learnable\nweight matrix of layerk for relation r and σ(·) is an element-\nwise activation function such as ReLU. This formulation al-\nlows nodes to update their representations by incorporating\ninformation from neighboring nodes based on the specific\nedge types, capturing the structural and relational dependen-\ncies within the protein graph. To obtain the graph represen-\ntation from the node representations of the last layer K of\nthe GNN, we apply a mean-pooling layer as follows:\nhG = 1\nN\nNX\ni=1\nxK\ni (2)\nThe resulting vector hG serves as an informative encoded\nrepresentation of the protein graph, capturing the essential\nstructural and relational characteristics. This representation\nplays a crucial role in the subsequent text generation pro-\ncess, where it will be utilized to generate detailed and accu-\nrate protein functions.\nSequence Encoding. To encode the protein sequence PS,\nwe used ESM2-35M (Lin et al. 2023a) as our base model.\nESM2 is a protein language model that uses a transformer-\nbased architecture and an attention mechanism to learn the\ninteraction patterns between pairs of amino acids in the input\nsequence. This allows the ESM model to capture amino acid\nsequence evolutionary information about proteins and their\nproperties. In order to achieve uniform representation di-\nmensions for all modalities within the spatial domain, a pro-\njection layer is applied after the last hidden layer of the ESM\nmodel. This layer functions as a projection layer that trans-\nforms the individual amino acid representations, derived\nfrom the ESM embedding dimension, into the graph embed-\nding dimension dout. As a result, a matrix H0\nS ∈ RN,dout is\nformed, containing the amino acid representations:\nH0\nS = ESM (PS)Wp (3)\nwhere Wp is a trainable matrix.\nMultimodal Fusion To obtain the final protein encoding,\nwe utilize a fusion block that combines the representation of\neach amino acid inside the matrix H0\nS with the graph rep-\nresentation vector hG. The fusion process involves a simple\nelement-wise addition of the two representations, followed\nby a projection layer. This fusion block enables the integra-\ntion of information from both the sequence and the graph\nrepresentations in a straightforward manner. Thus, allow-\ning each amino acid to be contextually enriched with infor-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10759\nModified GPT-2 Model\nT0 T1 T2 T3 TNA0 A1 AM\nMLP\nESM2\nLanguage Modeling Head:\nMLP+Softmax\nProtein ENCODER\nText DECODER\nS S E S S\nR\nE\nENV\nESM Tokenizer\nProtein Sequence\nAlphaFold\nConvert to graph\nIs associated with a DNA binding complex that binds\n to the G box, a well-characterized cis-acting DNA\n regulatory element found in plant genes\nText Tokenizer + Right Shifting\nProtein Description\nRGCN Encoder\nMulti-Head Cross Attention + MLP  \nIs associated with a DNA\n binding complex that binds\n to the G box, a well\ncharacterized cis-acting\nDNA regulatory  element\n found in plant genes\nProtein Description Generation\n(CLM Training Objective)\nFusion Block\nFigure 1: Architecture of the proposed Prot2Text framework for predicting protein function descriptions in free text. The model\nleverages a multimodal approach that integrates protein sequence, structure, and textual annotations. The encoder component\nutilizing an RGCN to process the protein graphs, and an ESM model to process the protein sequence. A fusion mechanism\nfacilitates the exchange of relevant information between the graph-encoded and the sequence-encoded vectors, creating a fused\nrepresentation synthesizing the structural and textual aspects. The decoder component employs a pretrained GPT-2 model, to\ngenerate detailed and accurate protein descriptions from the fused protein representation. By combining the power of GNNs and\nLLMs, Prot2Text enables a holistic representation of protein function, facilitating the generation of comprehensive descriptions.\nmation from the graph representation. Additionally, a nor-\nmalization layer is applied after each fusion block to main-\ntain stable training and further enhance the learning process.\nSpecifically, for each amino acid representation in Hk\nS, and\nthe graph representation hG, the fusion block computes the\ncombined representation Hk+1\nS as follows:\nHk+1\nS =\n\u0000\nHk\nS + 1nhGWk\nV\n\u0001\nWk\nO, (4)\nwhere Wk\nV and Wk\nO are trainable matrices in fusion block k\nand 1n is a vector of ones of size n (length of the amino acid\nsequence).\nBy using this fusion block multiple times in the architec-\nture (four times in this case), the model can capture com-\nplex interactions and dependencies between the sequence\nand graph representations, leading to an effective and con-\ntextually enriched encoding of the protein data. The fusion\nblock could be seen as a special case of the transformers\ncross-attention block when the the input from the encoder\nrepresents only one token.\nText Generation We employed the transformer decoder\narchitecture for generating protein descriptions. We initial-\nized the main components of the decoder, namely the text\nembedding matrix, self-attention, and language modeling\nhead, with the weights of GPT-2. By doing so, we leveraged\nthe GPT-2 model’s capacity to grasp the underlying textual\nsemantics. We forward the protein representation obtained\nfrom the protein encoder as input to the multi-head cross-\nattention module within the transformer decoder. This inter-\naction enabled the model to effectively incorporate context\nfrom the protein representation, contributing to the genera-\ntion of coherent protein descriptions. We adopted the iden-\ntical vocabulary and tokenizer from the GPT-2 model, with\nthe introduction of two unique special tokens. These addi-\ntional tokens serve as essential markers, enabling the model\nto discern the precise boundaries of each protein descrip-\ntion within the input text. In the training phase, we em-\nployed Causal Language Modeling (CLM) as the training\nobjective to optimize our model. Causal Language Model-\ning involves training the model to predict the next token in\na sequence given the preceding tokens. This unidirectional\nprediction process ensures that the model generates text in\na causal manner, without access to future tokens. The maxi-\nmum length of each description is 256 tokens.\n4 Experimental Results\nDataset To train the Prot2Text framework using proteins’\nstructures, sequences and textual descriptions, we build a\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10760\nmultimodal dataset with 256, 690 proteins. For each pro-\ntein, we have three crucial information: the correspond-\ning sequence, the AlphaFold accession ID and the textual\ndescription. To build this dataset, we used the SwissProt\ndatabase (Bairoch and Apweiler 1996), the only curated\nproteins knowledge base with full proteins’ textual descrip-\ntion included in the UniProtKB (Consortium 2016) Release\n2022\n04. Initially, The SwissProt database in this release\nhas 568, 363 proteins on which we perform the following:\n(1) Select the following properties: name that gives the full\nname of the protein, sequence that gives the amino acid\nsequence of the protein, AlphaFoldDB that gives the ac-\ncession ID of the protein in AlphaFold database,taxon and\ntext that gives the protein textual description. (2) Elimi-\nnate all samples that do not have all three crucial informa-\ntion. (3) Remove all samples with a duplicate amino acid\nsequence. (4) Remove all the samples where the textual de-\nscription contains ”(By Similarity)”. (5) Apply the CD-HIT\nclustering algorithm (Li and Godzik 2006) to create a train/-\nvalidation/test scheme with 248, 315, 4, 172 and 4, 203 pro-\nteins respectively. The maximum similarity threshold be-\ntween the (train, validation test) sets used in the CD-HIT\nalgorithm is 40%. (6) Preprocess the textual description to\nremove the ”PubMed” information. The AlphaFoldDB ac-\ncession is then used to download the protein structure in a\n”.PDB” file format using version 4 from AlphaFoldDB.\n0-20 20-30 30-40 40-50 50-60 60-70 70-80 80-9090-100\nBLAST Identity (%)\n0\n200\n400\n600\n800\n1000Count\n10\n20\n30\n40\n50\n60\nBLEU score\nProt2TextSMALL\nProt2TextBASE\nProt2TextMEDIUM\nProt2TextLARGE\nFigure 2: The test BLEU score for Prot2Text models as a\nfunction of the percentage identity using BLAST hit be-\ntween the test and the train sets.\nBaselines. In our experimental evaluation, we employed\na comprehensive set of baselines to rigorously assess the\ntext generation performance of the Prot2Text framework.\nSpecifically, we compared our approach against unimodal\nencoders, namely RGCN, ESM, and a vanilla-Transformer\ntrained from scratch. These encoders exclusively focus on\neither the protein graph or the protein sequence representa-\ntion. Furthermore, we compared it with a multimodal base-\nline, RGCN+ESM, that concatenates the graph and se-\nquence representations without fusing the representation of\neach amino acid and the structure representation. Finally,\nwe compare with RGCN × vanilla-Transformer baseline,\nwhich has similar architecture as Prot2Text but instead uses\na vanilla-Transformer model from scratch instead of the pre-\ntrained ESM2. In all ESM models, we use the last hidden\nstate. The vanilla-Transformer baseline follows the same\nconfiguration and as the pretrained ESM2-35M.\nTraining Details. We implemented all the models using\nPyTorch and utilized 64 NVIDIA V100 GPUs for train-\ning. We used the AdamW optimizer (Loshchilov and Hut-\nter 2019) with ϵ = 10−6, β1 = 0.9, β2 = 0.999, with a\nlearning rate starting from 2.10−4 and decreasing to zero\nusing a cosine scheduler. We used a warm-up of 6% of the\ntotal training steps. We fixed the batch size to four per GPU\nand we trained the models for 25 epochs. For the GNN en-\ncoder, we used 6 layers with a hidden size equal to GPT-\n2’s hidden size (768 for the base model of GPT-2) in each\nlayer. As for the amino acid sequence tokenization, We used\nthe same tokenizer and configuration of ESM2. The train-\ning for each Base model lasted for approximately 12 hours.\nAll experiments were carried out using the HuggingFace\ntransformers library (Wolf et al. 2020). More details\nare available in the appendix of the preprint2.\nMetrics. In the experiments, we used several metrics to\nevaluate the performance of the model in the text genera-\ntion task. Specifically, we used BLEU Score (Papineni et al.\n2002) which is a widely used metric for evaluating the qual-\nity of machine-generated text. It measures the similarity be-\ntween the generated text and the reference text based on\nn-grams. A higher BLEU score indicates better similarity\nbetween the generated and reference text. We further used\nRouge-1, Rouge-2 and Rouge-L scores (Lin 2004), which\nmeasure the overlap of unigrams, bigrams, and longest com-\nmon subsequence between the generated text and the refer-\nence text, respectively. Finally, we usedBERT Score (Zhang\net al. 2020), which measures the similarity between the gen-\nerated text and the reference text using contextualized word\nembeddings from a transformer-based model. In our exper-\niments we choose to use BioBERT LARGE-cased v1.1 (Lee\net al. 2020) to compute the BERT Score.\nResults. We report the results in Table 1, for different\nencoder models, including unimodal encoders like vanilla-\nTransformer, ESM2-35M, and RGCN, and multimodal en-\ncoders like RGCN × vanilla-Transformer and RGCN +\nESM2-35. All models use a GPT-2 decoder. The unimodal\nvanilla-Transformer baseline, relying solely on the amino\nacid sequence of the protein, exhibits the lowest perfor-\nmance across all evaluation metrics. However, we observe a\nsignificant improvement in performance when using the uni-\nmodal graph encoder RGCN. The RGCN outperforms the\nvanilla-Transformer by over five absolute points in terms of\nBLEU score and three points in terms of BERT score. This\nperformance disparity highlights the importance of incorpo-\nrating structural information through the RGCN encoder for\nprotein’s function prediction. On the other hand, leverag-\ning the pretrained protein language model ESM2-35M in-\nstead of initializing the vanilla-Transformer randomly, re-\nsults in a remarkable improvement in performance. The\n2https://arxiv.org/abs/2307.14367\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10761\nModel # Params BLEU Score Rouge-1 Rouge-2 Rouge-L BERT Score\nvanilla-Transformer 225M 15.75 27.80 19.44 26.07 75.58\nESM2-35M 225M 32.11 47.46 39.18 45.31 83.21\nRGCN 220M 21.63 36.20 28.01 34.40 78.91\nRGCN + ESM2-35M 255M 30.39 45.75 37.38 43.63 82.51\nRGCN × vanilla-Transformer 283M 27.97 42.43 34.91 40.72 81.12\nProt2TextBASE 283M 35.11 50.59 42.71 48.49 84.30\nTable 1: Test set results for different encoder models. All models share the same GPT-2 decoder. Prot2Text BASE achieves the\nhighest performance across all evaluation metrics, including BLEU score, Rouge scores, and BERT Score.\nESM2-35M encoder leads to a substantial increase of over\n16 BLEU score points and 18 Rouge-L points compared\nto the standard vanilla-Transformer configuration. This no-\ntable enhancement can be attributed to the pretraining of\nESM2-35M using masked protein modeling, which enables\nthe encoder to capture intricate relationships and patterns\nwithin protein sequences. In the context of multimodal pro-\ntein representation, the evaluation results demonstrate that\nProt2TextBASE exhibits superior performance across all as-\nsessment metrics. Notably, it achieves the highest BLEU,\nRouge-1, Rouge-2, Rouge-L, and BERT scores. These out-\ncomes highlight the effectiveness of fusing protein structure\nand amino acid information in a multimodal manner. The\nincorporation of protein structure, facilitated by the Rela-\ntional Graph Convolutional Network (RGCN) with the se-\nquential representations of amino acids from ESM2-35, sig-\nnificantly enhances the overall performance across all eval-\nuation metrics. This improvement is attributed to the en-\nriched understanding of proteins achieved through the syn-\nergy of these two modalities. Furthermore, the efficacy of the\nmultimodal fusion approach is corroborated by the results\nobtained from RGCN × vanilla-Transformer. Introducing\nstructural information using RGCN to the randomly initial-\nized vanilla-Transformer yields a substantial improvement\nof over 10 BLEU score points compared to using the vanilla-\nTransformer alone, and more than 6 BLEU score points im-\nprovement over using RGCN in isolation. Finally, to show\nthe importance of the fusion block in the Prot2Text frame-\nwork, we compare it against RGCN+ ESM2-25, which con-\ncatenates the protein structure representation to the amino\nacids representations. In this case, the graph representation\nwill simply be passed to the decoder alongside the ESM out-\nput. We notice that using this strategy leads to slightly worse\nresults than using the ESM alone. This not only provides\nbacking for the selection of the fusion block employed in\nProt2Text, but also suggests that indiscriminately increasing\nthe overall parameter count of the model could potentially\nlead to a degradation in its performance.\nAblation Study: Scaling to Larger Models. We con-\nducted an ablation study to assess the performance of our\nProt2Text framework as we varied the number of parame-\nters. The primary objective of this experiment was to evalu-\nate the benefits of employing larger models in terms of gen-\nerating more accurate and detailed textual representations of\nprotein’s function. To conduct the ablation study, we system-\natically varied the size of the protein language model (ESM).\nWhere Prot2Text SMALL, Prot2Text BASE, Prot2Text MEDIUM\nand Prot2Text LARGE use ESM2-8M, ESM2-35M, ESM2-\n150M and ESM2-650M respectively. We evaluated each\nconfiguration on the same test set of proteins and used the\nsame evaluation metrics as described earlier. The results\nof the ablation study, presented in Table 2, show a trend\nof performance improvement as we scale up the model’s\narchitecture. Larger versions of ESM outperformed their\nsmaller counterparts in most evaluation metrics. The in-\ncrease in model size led to more accurate and relevant de-\nscriptions, indicating the benefit of leveraging larger lan-\nguage models in the Prot2Text framework. Yet, comple-\nmentary analysis including corresponding computation time\nshowed an increase in the inference cost following the use\nof larger models. Therefore, Prot2Text MEDIUM (398M pa-\nrameters) is a good trade-off striking the balance between\nperformance and computational cost. Furthermore, in Fig-\nure 2 we report the performance of all Prot2text models\nwith respect to different similarity thresholds. Where the\nsimilarity represents the highest alignment score between\nthe amino acid sequences of the test and train sets using\nBLAST identity. We observe that for test proteins with low\nsimilarity scores with the train set (between 20% and 30%)\nand for proteins with no counterpart in the train set, the\nProt2TextMEDIUM is the dominant one while for higher simi-\nlarity scores Prot2TextLARGE performs better.\nVisualization of Generated Descriptions.To gain deeper\ninsights into the quality of the generated proteins’ functions\nby our Prot2Text framework, we provide in Figure 3 a tex-\ntual comparison of the pre-defined labels and generated text\noutputs for a selected set of proteins from the test set. It\nillustrates a comparison between the ground truth and the\ncorresponding descriptions generated by Prot2Text BASE for\ntwo different proteins using each protein’s amino acid se-\nquence and 3D structural representation. The results indicate\na successful detailed reconstruction of the different proteins’\nfunctions including richer information than the known de-\nscription. Following, Figure 3 showcases the model’s ability\nto generate coherent and informative free-text descriptions\nthat align closely with the ground truth annotations.\n5 Conclusion\nIn conclusion, our paper introduces Prot2Text, a pioneer-\ning multimodal framework, for the accurate prediction of a\nprotein’s function in free text format, from graph and se-\nquential input. By reformulating the task as text generation,\nwe address the limitations of traditional classification-based\nmethods, allowing for a more nuanced and in-depth under-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10762\nModel # Params BLEU Score Rouge-1 Rouge-2 Rouge-L BERT Score Inference Time\nProt2TextSMALL 256M 30.01 45.78 38.08 43.97 82.60 1,225\nProt2TextBASE 283M 35.11 50.59 42.71 48.49 84.30 1,379\nProt2TextMEDIUM 398M 36.51 52.13 44.17 50.04 84.83 1,334\nProt2TextLARGE 898M 36.29 53.68 45.60 51.40 85.20 1,667\nTable 2: Test set results for different size variations of Prot2Text. Larger models outperform their smaller counterparts\nacross most evaluation metrics, indicating the benefits of employing larger language models in the Prot2Text framework. The\nProt2TextMEDIUM model, strikes an optimal balance between performance and computational efficiency. The inference time is\nin seconds for text generation on the whole test set. The inference time is computed during text generation using two NVIDIA\nRTX 6000 with 48GB memory in parallel and batch size of four per device.\nGenerated TextLabeled TextProtein \nProbable peripherally associated component of the endosomal sorting\nrequired for transport complex III (ESCRT-III) which is involved in\nmultivesicular bodies (MVBs) formation and sorting of endosome-\nderived cargo proteins into MVBs. MVBs contain intraluminal vesicles\n(ILVs) that are generated by invagination and scission from the limiting\nmembrane of the cell and mostly are delivered to lysosomes enabling\ndegradation of membrane proteins, such as stimulated growth factor\nreceptors, lysogenic enzymes and lipids. The MVB pathway appears to\nrequire the sequential function of ESCRT-O, -I,-II and -III complexes.\nESCRT proteins mostly dissociate from the invaginating membrane\nbefore the ILV is released. The ESCRT machinery also functions in\ntopologically equivalent membrane fission events, such the terminal\nstages of cytokinesis and the budding of enveloped viruses (HIV-1 and\nother lentiviruses). ESCRT I, -II and III proteins mostly mediate the\nsorting of proteins into lysosphingolipids, primarily cytoplasmic\nmembrane vesicle-derived proteins. ESCR-III proteins are believed to\nmediate a specialized role in the final stages\nRequired for the sorting and concentration of proteins\nresulting in the entry of these proteins into the\ninvaginating vesicles of the multivesicular body\n(MVB). Acts a component of the ESCRT-III complex,\nwhich appears to be critical for late steps in MVB\nsorting, such as membrane invagination and final\ncargo sorting and recruitment of late-acting\ncomponents of the sorting machinery. The MVB\npathway requires the sequential function of ESCRT-\nO, -I,-II and -III complex assemblies. Can directly\nstimulate VPS4 ATPase activity. The DID4/VPS2-\nVPS24 subcomplex is required for the VPS4-\ndependent dissociation of ESCRT-III.\n P36108\nQ8NG08\nDNA-dependent ATPase and 5'-3' DNA helicase required for the\nmaintenance of genome stability. Involved in various processes such as\ntranscription-coupled nucleotide excision repair, mitotic spindle\nassembly, DNA damage response and DNA repair. During DNA double-\nstrand break (DSB) repair, involved in limiting the number of single-\nstranded DSBs through DNA end resection by promoting the recruitment\nof RAD51 to sites of DNA damage. In addition to DNA end reannealing,\nalso promotes the resumption of DNA synthesis from the new 3'-tail end\nof the existing DNA ends. Acts as a regulator of telomerase by inhibiting\ntelomeric silencing: binds to telomeromyosin, and is required for\ntelomeronucleolytic silencing of endogenous small interfering RNAs\n(endo-siRNAs) and their targeting to the telomere. Involves also DNA\ndamage signaling via the 5'-5' helicase and DNA helicases activities,\npreventing disintegration of the DNA duplex and subsequent resection of\nthe damaged DNA strand. In vitro able to unwind 5'-overhanging flap\nDNA and catalyzes ATP-dependent unwinding of 5'-DNA ends. \n5'-3' DNA helicase involved in DNA damage\nresponse by acting as an inhibitor of DNA end\nresection. Recruitment to single-stranded DNA\n(ssDNA) following DNA damage leads to inhibit the\nnucleases catalyzing resection, such as EXO1, BLM\nand DNA2, possibly via the 5'-3' ssDNA translocase\nactivity of HELB. As cells approach S phase, DNA\nend resection is promoted by the nuclear export of\nHELB following phosphorylation. Acts independently\nof TP53BP1. Unwinds duplex DNA with 5'-3' polarity.\nHas single-strand DNA-dependent ATPase and DNA\nhelicase activities. Prefers ATP and dATP as\nsubstrates. During S phase, may facilitate cellular\nrecovery from replication stress.\nFigure 3: Ground-truth labeled text vs predicted text: A textual comparison of the labeled descriptions and generated text\noutputs for three different proteins from the test set.\nstanding of a protein’s functionality. Leveraging the power\nof GNNs and LLMs, we integrate structural and textual pro-\ntein information, resulting in highly detailed and coherent\ngenerated protein descriptions. The release of a comprehen-\nsive multimodal protein dataset further empowers the scien-\ntific community to benchmark and advance the field of pro-\ntein function prediction in free text format. This innovative\napproach opens new horizons for research and applications\nin drug discovery, protein engineering, and various biolog-\nical sciences, with the potential to revolutionize our under-\nstanding of proteins’ functions.\n6 Limitation and Future Work\nOne limitation of our proposed Prot2Text model is that the\nRGCN encoder is not pretrained. Unlike the ESM which\nbenefits from pretraining on a large corpus, the RGCN en-\ncoder lacks this initial knowledge. As a result, the RGCN\nencoder might struggle to capture complex patterns, poten-\ntially leading to suboptimal performance. To address this\nlimitation, we aim to explore pretraining techniques specifi-\ncally tailored for graph neural networks. This could involve\npretraining the RGCN encoder on auxiliary graph-related\ntasks, leveraging graph-level or node-level information to\nbuild a foundational understanding of protein structures.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10763\nAcknowledgements\nThis research was supported by the ANR chair AML/HE-\nLAS (ANR-CHIA-0020-01).\nReferences\nBairoch, A.; and Apweiler, R. 1996. The SWISS-PROT\nProtein Sequence Data Bank and Its New Supplement\nTREMBL. Nucleic Acids Research, 24(1): 21–25.\nBrandes, N.; Ofer, D.; Peleg, Y .; Rappoport, N.; and Linial,\nM. 2022. ProteinBERT: a universal deep-learning model of\nprotein sequence and function.Bioinformatics, 38(8): 2102–\n2110.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nChatzianastasis, M.; Lutzeyer, J.; Dasoulas, G.; and Vazir-\ngiannis, M. 2023. Graph ordering attention networks. In\nProceedings of the 37th AAAI Conference on Artificial In-\ntelligence, 7006–7014.\nChatzianastasis, M.; Vazirgiannis, M.; and Zhang, Z. 2023.\nExplainable Multilayer Graph Neural Network for Cancer\nGene Prediction. arXiv preprint arXiv:2301.08831.\nChen, C.; Chen, X.; Morehead, A.; Wu, T.; and Cheng, J.\n2023. 3D-equivariant graph neural networks for protein\nmodel quality assessment. Bioinformatics, 39(1): btad030.\nChithrananda, S.; Grand, G.; and Ramsundar, B. 2020.\nChemBERTa: large-scale self-supervised pretrain-\ning for molecular property prediction. arXiv preprint\narXiv:2010.09885.\nConsortium, T. U. 2016. UniProt: the universal protein\nknowledgebase. Nucleic Acids Research, 45(D1): D158–\nD169.\nConsortium, U. 2015. UniProt: a hub for protein informa-\ntion. Nucleic acids research, 43(D1): D204–D212.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nEdwards, C.; Lai, T.; Ros, K.; Honke, G.; Cho, K.; and Ji,\nH. 2022. Translation between Molecules and Natural Lan-\nguage. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, 292–305.\nFabian, B.; Edlich, T.; Gaspar, H.; Segler, M.; Meyers, J.;\nFiscato, M.; and Ahmed, M. 2020. Molecular representation\nlearning with language models and domain-relevant auxil-\niary tasks. arXiv:2011.13230.\nGligorijevi´c, V .; Renfrew, P. D.; Kosciolek, T.; Leman, J. K.;\nBerenberg, D.; Vatanen, T.; Chandler, C.; Taylor, B. C.; Fisk,\nI. M.; Vlamakis, H.; et al. 2021. Structure-based protein\nfunction prediction using graph convolutional networks.Na-\nture communications, 12(1): 3168.\nHa, J.; Park, H.; Park, J.; and Park, S. B. 2021. Recent ad-\nvances in identifying protein targets in drug discovery. Cell\nChemical Biology, 28(3): 394–423.\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Clas-\nsification with Graph Convolutional Networks. In 5th Inter-\nnational Conference on Learning Representations.\nKulmanov, M.; and Hoehndorf, R. 2019. DeepGOPlus: im-\nproved protein function prediction from sequence. Bioinfor-\nmatics, 36(2): 422–429.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and\nKang, J. 2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinfor-\nmatics, 36(4): 1234–1240.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2020. BART: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Compre-\nhension. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 7871–7880. On-\nline: Association for Computational Linguistics.\nLi, W.; and Godzik, A. 2006. Cd-hit: a fast program for\nclustering and comparing large sets of protein or nucleotide\nsequences. Bioinformatics, 22(13): 1658–1659.\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLin, Z.; Akin, H.; Rao, R.; Hie, B.; Zhu, Z.; Lu, W.;\nSmetanin, N.; Verkuil, R.; Kabeli, O.; Shmueli, Y .; dos San-\ntos Costa, A.; Fazel-Zarandi, M.; Sercu, T.; Candido, S.; and\nRives, A. 2023a. Evolutionary-scale prediction of atomic-\nlevel protein structure with a language model. Science,\n379(6637): 1123–1130. Earlier versions as preprint: bioRxiv\n2022.07.20.500902.\nLin, Z.; Akin, H.; Rao, R.; Hie, B.; Zhu, Z.; Lu, W.;\nSmetanin, N.; Verkuil, R.; Kabeli, O.; Shmueli, Y .; et al.\n2023b. Evolutionary-scale prediction of atomic-level pro-\ntein structure with a language model. Science, 379(6637):\n1123–1130.\nLiu, S.; Li, Y .; Li, Z.; Gitter, A.; Zhu, Y .; Lu, J.; Xu, Z.;\nNie, W.; Ramanathan, A.; Xiao, C.; Tang, J.; Guo, H.;\nand Anandkumar, A. 2023. A Text-guided Protein Design\nFramework. arXiv preprint arXiv:2302.04611.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In International Conference on Learn-\ning Representations.\nMorris, C.; Rattan, G.; and Mutzel, P. 2020. Weisfeiler and\nLeman go sparse: Towards scalable higher-order graph em-\nbeddings. In Advances in Neural Information Processing\nSystems, volume 34.\nMurphy, R.; Srinivasan, B.; Rao, V .; and Ribeiro, B. 2019.\nRelational Pooling for Graph Representations. In Pro-\nceedings of the 36th International Conference on Machine\nLearning, 4663–4673.\nNikolentzos, G.; Dasoulas, G.; and Vazirgiannis, M. 2020.\nk-hop graph neural networks. Neural Networks, 130: 195–\n205.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10764\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nand Krueger, G. 2021. CLIP: Learning Transferable Visual\nModels From Natural Language Supervision. In Interna-\ntional Conference on Learning Representations.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. arXiv preprint arXiv:1910.10683.\nReiser, P.; Neubert, M.; Eberhard, A.; Torresi, L.; Zhou, C.;\nShao, C.; Metni, H.; van Hoesel, C.; Schopmans, H.; Som-\nmer, T.; et al. 2022. Graph neural networks for materials\nscience and chemistry.Communications Materials, 3(1): 93.\nRives, A.; Meier, J.; Sercu, T.; Goyal, S.; Lin, Z.; Liu, J.;\nGuo, D.; Ott, M.; Zitnick, C. L.; Ma, J.; et al. 2021. Biologi-\ncal structure and function emerge from scaling unsupervised\nlearning to 250 million protein sequences. Proceedings of\nthe National Academy of Sciences, 118(15): e2016239118.\nBioRxiv 10.1101/622803.\nScarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.; and\nMonfardini, G. 2009. The Graph Neural Network Model.\nIEEE Transactions on Neural Networks, 20(1): 61–80.\nSchlichtkrull, M.; Kipf, T. N.; Bloem, P.; Van Den Berg,\nR.; Titov, I.; and Welling, M. 2018. Modeling relational\ndata with graph convolutional networks. In The Semantic\nWeb: 15th International Conference, ESWC 2018, Herak-\nlion, Crete, Greece, June 3–7, 2018, Proceedings 15, 593–\n607. Springer.\nSeo, Y .; Loukas, A.; and Perraudin, N. 2019. Dis-\ncriminative structural graph classification. arXiv preprint\narXiv:1905.13422.\nVaradi, M.; Anyango, S.; Deshpande, M.; Nair, S.; Natassia,\nC.; Yordanova, G.; Yuan, D.; Stroe, O.; Wood, G.; Laydon,\nA.; et al. 2022. AlphaFold Protein Structure Database: mas-\nsively expanding the structural coverage of protein-sequence\nspace with high-accuracy models. Nucleic acids research,\n50(D1): D439–D444.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In Guyon, I.; Luxburg, U. V .;\nBengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and\nGarnett, R., eds., Advances in Neural Information Process-\ning Systems, volume 30. Curran Associates, Inc.\nWang, L.; Liu, H.; Liu, Y .; Kurtin, J.; and Ji, S. 2022. Learn-\ning protein representations via complete 3d graph networks.\narXiv preprint arXiv:2207.12600.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu,\nJ.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest, Q.;\nand Rush, A. 2020. Transformers: State-of-the-Art Natural\nLanguage Processing. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Process-\ning: System Demonstrations, 38–45. Online: Association for\nComputational Linguistics.\nXu, M.; Yuan, X.; Miret, S.; and Tang, J. 2023. ProtST:\nMulti-Modality Learning of Protein Sequences and Biomed-\nical Texts. arXiv preprint arXiv:2301.12040.\nZhang, C.; Song, D.; Huang, C.; Swami, A.; and Chawla,\nN. V . 2019. Heterogeneous graph neural network. In Pro-\nceedings of the 25th ACM SIGKDD international conference\non knowledge discovery & data mining, 793–803.\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi,\nY . 2020. BERTScore: Evaluating Text Generation with\nBERT. In International Conference on Learning Represen-\ntations.\nZhang, X.-M.; Liang, L.; Liu, L.; and Tang, M.-J. 2021.\nGraph neural networks and their current applications in\nbioinformatics. Frontiers in genetics, 12: 690049.\nZhang, Z.; Xu, M.; Jamasb, A.; Chenthamarakshan, V .;\nLozano, A.; Das, P.; and Tang, J. 2022. Protein represen-\ntation learning by geometric structure pretraining. arXiv\npreprint arXiv:2203.06125.\nZitnik, M.; Agrawal, M.; and Leskovec, J. 2018. Model-\ning polypharmacy side effects with graph convolutional net-\nworks. Bioinformatics, 34(13): i457–i466.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10765",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7052137851715088
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5682920217514038
    },
    {
      "name": "Protein function prediction",
      "score": 0.5504298806190491
    },
    {
      "name": "Machine learning",
      "score": 0.5209535360336304
    },
    {
      "name": "Function (biology)",
      "score": 0.4415246248245239
    },
    {
      "name": "Annotation",
      "score": 0.42605969309806824
    },
    {
      "name": "Protein function",
      "score": 0.28535157442092896
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210139461",
      "name": "Laboratoire d'Informatique de l'École Polytechnique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I142476485",
      "name": "École Polytechnique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I34771391",
      "name": "University of Cyprus",
      "country": "CY"
    },
    {
      "id": "https://openalex.org/I4210131858",
      "name": "Epigénétique et Destin Cellulaire",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I204730241",
      "name": "Université Paris Cité",
      "country": "FR"
    }
  ]
}