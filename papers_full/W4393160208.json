{
    "title": "Small Language Model Can Self-Correct",
    "url": "https://openalex.org/W4393160208",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5100862337",
            "name": "Haixia Han",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5075507821",
            "name": "Jiaqing Liang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5100682850",
            "name": "Jie Shi",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5040655519",
            "name": "Qianyu He",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5090455375",
            "name": "Yanghua Xiao",
            "affiliations": [
                "East China Normal University",
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6630497369",
        "https://openalex.org/W6846930601",
        "https://openalex.org/W6761551260",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W3201174429",
        "https://openalex.org/W6842239938",
        "https://openalex.org/W6782465632",
        "https://openalex.org/W4306808908",
        "https://openalex.org/W6846739993",
        "https://openalex.org/W4306309284",
        "https://openalex.org/W4353112996",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W1514520715",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4389519449",
        "https://openalex.org/W4385571157",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W4287674181",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4321162379",
        "https://openalex.org/W4312052651",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4389520749",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4307001524",
        "https://openalex.org/W4293138840",
        "https://openalex.org/W4362508231",
        "https://openalex.org/W4385572830",
        "https://openalex.org/W3158419028",
        "https://openalex.org/W4311991106",
        "https://openalex.org/W4317553041",
        "https://openalex.org/W4362656036",
        "https://openalex.org/W4308014717",
        "https://openalex.org/W4307123345"
    ],
    "abstract": "Generative Language Models (LMs) such as ChatGPT have exhibited remarkable performance across various downstream tasks. Nevertheless, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. Previous studies have devised sophisticated pipelines and prompts to induce large LMs to exhibit the capability for self-correction. However, large LMs are explicitly prompted to verify and modify their answers separately rather than completing all steps spontaneously like humans. Moreover, these complex prompts are extremely challenging for small LMs to follow. In this paper, we introduce the Intrinsic Self-Correction (ISC) in generative language models, aiming to correct the initial output of LMs in a self-triggered manner, even for those small LMs with 6 billion parameters. Specifically, we devise a pipeline for constructing self-correction data and propose Partial Answer Masking (PAM), aiming to endow the model with the capability for intrinsic self-correction through fine-tuning. We conduct experiments using LMs with parameters sizes ranging from 6 billion to 13 billion in two tasks, including commonsense reasoning and factual knowledge reasoning. Our experiments demonstrate that the outputs generated using ISC outperform those generated without self-correction. We believe that the output quality of even small LMs can be further improved by empowering them with the ability to intrinsic self-correct.",
    "full_text": "Small Language Model Can Self-Correct\nHaixia Han1, Jiaqing Liang2, Jie Shi3, Qianyu He3, Yanghua Xiao1,3*\n1Shanghai Institute of AI for Education and School of Computer Science and Technology, East China Normal University\n2School of Data Science, Fudan University\n3 Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\nhaixiahan03@gmail.com, {liangjiaqing, shawyh}@fudan.edu.cn, {jshi22, qyhe21}@m.fudan.edu.cn\nAbstract\nGenerative Language Models (LMs) such as ChatGPT have\nexhibited remarkable performance across various down-\nstream tasks. Nevertheless, one of their most prominent draw-\nbacks is generating inaccurate or false information with a\nconfident tone. Previous studies have devised sophisticated\npipelines and prompts to induce large LMs to exhibit the ca-\npability for self-correction. However, large LMs are explicitly\nprompted to verify and modify their answers separately rather\nthan completing all steps spontaneously like humans. More-\nover, these complex prompts are extremely challenging for\nsmall LMs to follow. In this paper, we introduce the I\nntrinsic\nSelf-Correction (ISC) in generative language models, aim-\ning to correct the initial output of LMs in a self-triggered\nmanner, even for those small LMs with 6 billion parame-\nters. Specifically, we devise a pipeline for constructing self-\ncorrection data and propose Partial Answer Masking (PAM),\naiming to endow the model with the capability for intrinsic\nself-correction through fine-tuning. We conduct experiments\nusing LMs with parameters sizes ranging from 6 billion to 13\nbillion in two tasks, including commonsense reasoning and\nfactual knowledge reasoning. Our experiments demonstrate\nthat the outputs generated using ISC outperform those gener-\nated without self-correction. We believe that the output qual-\nity of even small LMs can be further improved by empower-\ning them with the ability to intrinsic self-correct.\nIntroduction\nGenerative Language Models (LMs) have gained consider-\nable attention due to their remarkable capabilities (Guo et al.\n2023; Suzgun et al. 2023). Despite the convincing and real-\nistic nature of text generated by these LMs, a concern with\nLMs lies in their tendency to produce fabricated facts and\ngenerate false information (Lin, Hilton, and Evans 2022).\nMoreover, these models deliver inaccurate information em-\nploying unequivocal expressions, which poses substantial\nrisks as it can lead to the spread of misleading and harm-\nful content.\nOne of the contributing factors to the hallucination lies\nin inadequate acquisition of knowledge (Manakul, Liusie,\nand Gales 2023; Huang et al. 2023). For example, consider\nthe question Which animal is China’s nation treasure?, LMs\n*Corresponding Author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Two self-correction methods are demonstrated in\nlanguage models in response to a query. The gray line on\nthe left illustrates the process of self-correction employ-\ning prompt engineering in large language models like Chat-\nGPT. The red line shows the overall steps of our proposed\nIntrinsic Self-Correction, where self-verification and self-\nmodification occur spontaneously.\nmay provide a different animal name like tiger instead of\npanda due to a lack of relevant knowledge. Considerable ef-\nforts have been made to alleviate such hallucination induced\nby lacking of knowledge in LMs. One approach involves\nsupervised fine-tuning LMs with standard ground-truth an-\nswers to enhance their comprehension of relevant knowl-\nedge (Wei et al. 2022a; Ouyang et al. 2022). This method\nhas shown promising efficacy. However, it demands a sig-\nnificant amount of high-quality annotated data for training.\nAdditionally, other methods have relied on external verifier\nor critic model to evaluate the accuracy of a statement (Yang\net al. 2022; Paul et al. 2023). Training a verifier necessitates\na large number of high-quality evaluation annotations and\nfurther fine-tuning of the model, which restricts its broad\napplicability to other tasks and domains.\nAnother reason for an LM to provide incorrect response\nis intrinsically linked to the design architecture of genera-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18162\ntive language models themselves (Azaria and Mitchell 2023;\nPaul et al. 2023; Shinn et al. 2023). It is widely acknowl-\nedged that LMs generate a sentence by maximizing the like-\nlihood of the next token given all previous tokens. Subtle\ndifferences in the preceding sentences can potentially lead to\ndiverse generation outcomes. For example, when the ques-\ntion is Who is the author of The Analects?, the model gives\nthe correct answer as Confucius. However, when the input\nquestion becomes Is Laozi or Confucius the author of the\nAnalects of Confucius?, the model is likely to generate an\nanswer of Laozi. In this case, the model has the ability to\nrely on its knowledge to recognize false information (Schick\net al. 2023). This process is akin to how humans perform\nself-verification of answers to minimize mistakes (Flower\nand Hayes 1981). Moreover, when we realize our answer\nis wrong, we further modify it. Motivated by this, when\nthe model itself detects the potential hallucination, the next\nstep is to correct the error or mistake. Once the model in-\ncorporates this inherent self-correction mechanism, it can\naddress similar issues in other domains and achieve self-\nimprovement.\nThe existing work (Madaan et al. 2023; Ganguli et al.\n2023) towards self-correction in LMs has mainly focused\non lager models like ChatGPT and GPT4, which is chal-\nlenging to migrate these self-correction methods to small\nLMs. Some studies indicated that the self-correction ability\ndepends on model parameters and only emerges in models\nwith larger parameter sizes (Azaria and Mitchell 2023). The\nmain reason is that they devised a sophisticated pipeline and\nzero-shot prompts to achieve self-correction. However, these\nprompts crafted for self-verification and self-modification\nare difficult for small models to understand. As depicted in\nFigure 1, upon generating the initial answer to the given\nquestion, an additional feedback instruction is utilized to\nguide ChatGPT in generating feedback information regard-\ning the initial answer. This information contains an evalu-\nation of the correctness of the initial answer. A subsequent\nmodification instruction is employed to alter or refine the\ninitial answer based on the feedback received.\nNevertheless, small models typically lack self-awareness\n(Weng et al. 2022) and tend to exhibit greater confidence\nin their generated responses. Consequently, they struggle to\nassess the quality of their generated outcomes. The capabil-\nity for self-verification serves as a prerequisite for achiev-\ning self-correction. Furthermore, the manner in which self-\ncorrection is achieved through multi-step prompt engineer-\ning within LMs differs from the spontaneous and one-time\ncorrection observed in humans.\nTo empower the capability for self-correction in small lan-\nguage models, we propose Intrinsic Self-Correction (ISC),\nan intrinsic mechanism that relies on two basic abilities: self-\nverification and self-modification. At its core, the LM pro-\nvides a response and subsequently evaluates its own answer.\nUpon identifying an error, the same LM adjusts its initial re-\nsponse. Conversely, if the answer is validated as accurate, no\nfurther modifications are required. The self-correction pro-\ncess is not divided into two separate steps, but rather con-\nstitutes a single comprehensive step, as depicted by the red\narrowed segment in Figure 1. We trained the LM to process\nthe self-correction through Instruction Fine-Tuning (IFT).\nFor this purpose, we design the data processing procedure to\nconstruct the self-correction data and define the data format.\nDuring the fine-tuning process, we propose Partial Answer\nMasking (PAM) to make the model have the ability of self-\nverification. Our contributions are summarized as follows:\n• To the best of our knowledge, we are the first to demon-\nstrate that small language models with even 6 billion pa-\nrameters possess the capacity for self-correction during\nresponse generation without relying ground truth.\n• Our proposed Intrinsic Self-correction aims to incorpo-\nrate self-correction as an intrinsic pattern within LM. It\ninvolves an independent and spontaneous self-correction\nprocess, distinct in nature from existing methods of self-\ncorrection that rely on prompt engineering.\n• To achieve the capability for self-correction in small\nLMs, we devise a pipeline for constructing self-\ncorrection data and define the data format. It can be uni-\nversally applied to build data for self-correction tasks.\nAdditionally, we introduce a novel training method\ncalled Partial Answer Masking (PAM) to enable the\nmodel self-verify its own generated answers.\n• We conduct experiments on open-source LMs with vary-\ning parameter scales to validate the efficacy of our pro-\nposed method. The results demonstrate improvements in\naccuracy across two different datasets.\nRelated Work\nInstruction fine-tuning LMs often struggle to maintain\nalignment well with human intent after pre-training. How-\never, this issue can be alleviated by employing techniques\nsuch as Instruction Fine-Tuning (IFT) (Stiennon et al. 2020;\nOuyang et al. 2022) and Reinforcement Learning from Hu-\nman or AI Feedback (RLHF/RLAIF) (Gao, Schulman, and\nHilton 2023; Bai et al. 2022). IFT involves training an LM\nto generate responses based on provided prompts, which\nmight optionally include task instructions. The data used for\nfine-tuning is usually less than the original pre-training cor-\npus. These datasets used in IFT are often curated through\ncrowd-sourcing or extracted from an LM that is already ca-\npable of generating instruction-following examples. Studies\nhave shown that IFT can enhance zero-shot generalization\nto various unseen tasks. To optimize the fine-tuning pro-\ncess, various strategies have been proposed to make it more\nparameter-efficient. These strategies include using adapters\n(Hu et al. 2021), prompt tuning (Li and Liang 2021), and\nother techniques.\nChain-of thought in language model.Chain-of-Thought\n(COT), as an alternative prompt strategy, adopts a chained\nreasoning approach, incorporating a multi-step reasoning\npath before generating the final answer. This strategy aims to\nenhance the model’s capacity to handle complex and multi-\nstep queries. Kojima et al. (2022) introduced a novel yet\nsimple approach to prompt the LM by using the phase Let’s\nthink step by step, which enables reasoning generation in\na zero-shot manner. Zhou et al. (2023) further decomposed\nthe questions into multiple sub-questions, encouraging the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18163\nFigure 2: The pipeline of constructing self-correction data.\nmodel to engage in a sequential thought process while arriv-\ning at the answer. Overall, COT aims to enhance the answer\nquality through a multi-step reasoning process.\nIn contrast, our proposed intrinsic self-correction in LMs\ninvolves critiquing and subsequently revising the initially\ngenerated results. It does not fall under the classification of\na COT strategy.\nSelf-improvement in language model. Richer and more\ndetailed feedback plays a crucial role in aligning the output\nof the LM with the user’s preferences and in significantly\nimproving the overall performance of the model. Welleck\net al. (2022) introduced a corrector model used to improve\nthe initial response by learning the specific mistakes made\nby the LM and the appropriate method for modifying them.\nSimilarly, Paul et al. (2023) introduced a critic model to pro-\nvide feedback on the LM’s reasoning errors, evaluating the\ngenerated intermediate steps. The feedback, together with\nthe original question and preceding intermediate reasoning\nsteps, was subsequently fed back to the LM to improve the\ngeneration of the next step.\nScaling model size can increase model’s various capabil-\nities(Kaplan et al. 2020), including self-verification (or self-\nfeedback). Recent research efforts have focused on explor-\ning large LMs that employ self-feedback rather than relying\non additional models. Madaan et al. (2023) introduced an\niterative self-refinement algorithm that alternates between\nfeedback and refinement. They guided the model to pro-\nvide feedback about the initial response by employing few-\nshot prompts. This feedback is then passed back to the same\nmodel to help refine the initial response. Some methods used\nself-improvement to improve the reasoning ability of LM\nwithout supervised data. For example, the LM first generates\nmultiple COT reasoning paths and corresponding answers\nfor each question (Wei et al. 2022b). The answer with the\nhighest consistency is selected by majority voting. The COT\nreasoning paths exhibiting high-confidence, along with the\nselected answers, are augmented by mixed formats to serve\nas the training data used for IFT. Additionally, the LM veri-\nfied its multiple generated answers in turn and calculated the\nverification score based on the number of predicted masked\nvalues (Weng et al. 2022). Instead of requesting the LM to\nanswer the same query multiple times, a more effective ap-\nproach is to prompt the LM to rephrase the query and then\nanswer each rephrased question individually.\nWe also focus on providing self-feedback and enabling\nself-modification to enhance the quality of generation of\nLMs. Furthermore, we extend this capability to encompass\nmore generalized models, including smaller LMs.\nMethods\nTask Formulation\nIn this paper, Intrinsic Self-Correction is employed to ac-\ncomplish auto-regressive tasks. Given an input sequence x,\nan LM M is tasked with generating an output y. Typically,\nto generate a correct or plausible output, the model needs to\nincorporate explanation or reasoning, denoted as z, as inter-\nmediate steps. The process can be described as follows:\np(z, y0|x) =p(y0|x, z)p(z|x), (1)\nwhere y0 denotes initial output. The same model M pro-\nvides self-verification of the output to assess the accuracy\nof y0, represented as p(v|x, z, y0), where v represents the\nverification of y0 and it is a binary feedback signal. Upon\ndetecting a fault, the LM M proceeds to self-modify and\ngenerate a revised answer y. If no errors are found, there is\nno need for any modification, and in such case, y = y0. Ul-\ntimately, y represents the final response of the model to the\ninput sequence, as given by:\np(y|x) =p(y|x, z, y0, v) (2)\nIt is important to note that the stages of generating initial\noutput, self-verification, and self-modification (if necessary)\nare not carried out separately. Rather, they are accomplished\nwith just an instruction, referred to as the Self-Correction\nPrompt (SCP). There are some prompt examples of SCP:\ndouble-check your response for accuracy before proceeding\nto submit, before you finalize your answer, please reexamine\nit to ensure its correctness and please review your response\ncarefully to make sure it is correct before submitting.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18164\nQuestion: [TaskP] Examine the following options carefully and select the correct one. [COTP] Before providing your final\nanswer, give the analysis steps. [SCP] And you need double-check your response for accuracy before proceeding to submit.\n[question] Where do you buy tickets at a ticket booth for games?\nA. train station B. cathedral C. metro station D. fairgrounds E. amusement park\nAnswer: [A1\n1- COT] The question mentions the keywords “buying tickets” and “games”, so we can guess that this is a ···\n[A1\n1] Therefore, the correct answer is E. amusement park\n[PV] Thinking about the correctness of the previous answer···\nThinking result: I am sure that the earlier answer is correct\nQuestion: [TaskP] Please choose the most appropriate one from the following options:\n[question] what contributes more, though less than some believe, to the lung damage caused by smoking?\nA. smoking less B. switching to chewing C. no filters D. switching to e-cigs\n[COTP] Please give the detailed solving process and [SCP] verify your response before final submission.\nAnswer: [A1\n2-COT] Smoking causes less lung damage than people think, but it’s not completely without effect.\nSo the answer is A. smoking less\n[NV] Thinking about the correctness of the previous answer ···\nThinking result: Sorry, there is an error in the previous answer.\n[SC-COT] Let’s analyze each option:\nA. smoking less: The question clearly mentions that it contributes less than some people think,··· ···\nC. no filters: filters it contributes to lung damage, and to a lesser extent than some believe. Therefore, the\nno filter option meets the requirement.\n[A2\n2] So the correct option is C. no filter.\nTable 1: Training Data samples\nSelf-correction Data\nWe mimic human self-correction behavioral patterns to de-\nsign self-correction data format as shown in Table 1. We de-\nfine the self-correction data format as a Question-Answer\npair. Next, we will elaborate on how to construct each com-\nponent separately.\nQuestion preparation. We utilize an LM M, which is\ncapable of following instructions, to generate answers for\na set of questions. These questions are randomly sampled\nfrom datasets on various tasks and come with ground truth.\nTo ensure the quality of the generated answers, we lever-\nage the COT to instruct the model M to initially generate\na problem-solving process and finally provide an answer to\nthe given question. Therefore, we design a range of diverse\nCOT prompts (COTPs) to guide the M to generate the COT\nanalysis. For example, we use prompts like,Please select the\ncorrect option from the provided choices and offer a com-\nprehensive problem-solving process. In the few shot setting,\nthe whole prompt also contains other question-COTPs and\nanswer examples. We combine task prompts (TaskP) for re-\nsponding to specific question types, instructions for produc-\ning COT analysis processes (COTP), instructions for self-\ncorrection (SCP), and the question itself. This constitutes the\nQuestion part for the self-correcting data.\nAdditionally, we use two methods to enhance the model’s\nunderstanding of various self-correction instructions. First,\nwe use gpt-3.5-turbo to generate diverse prompts for dif-\nferent types of instructions mentioned above. We present\na brief prompt template below I want you act as a Prompt\nRewriter. Your objective is to rewrite a given prompt to make\nlanguage model to understand. The rewritten prompt must\nensure that the requirement remains unchanged. #Given\nPrompt#:[TaskP | COTP | SCP ]. Second, we randomly\nselect one instruction from TaskP, COTP and SCP sets and\ncombine them. The order of these instructions can also be\nrearranged, but TaskP is positioned at the beginning.\nAnswer preparation. To ensure answer diversity, we uti-\nlize nucleus sampling (Holtzman et al. 2020) to generate\nmultiple answers. After generating multiple answers for a\nquestion, the next step is to evaluate the accuracy of each an-\nswer by comparing it with the provided ground truth for the\neach question. In the case of multiple-choice questions, we\nextract the options of the final answer through string match-\ning and then directly compare them with the standard answer\nto check the accuracy.\ndui\nFor a good case, the outcome of self-verification should\nbe positive, indicating there is no need to modify the initial\nanswers. Accordingly, given the question x, the Answer is\nrepresented as (A1\n1-COT||A1\n1||PV ), where Ai\nn represents the\nmodel attempts n times to obtain the correct answer, and the\ncurrent answer is the ith response, Ai\nn-COT represents the\nCOT process of the answer, PV denotes the positive verifi-\ncation in self-correction, and || represents the concatenation.\nHere, we set the verification as a binary signal. A positive\nverification can be set like I am sure my answer is correct.\nConversely, for a bad case, negative self-verification re-\nsult is excepted, such as Sorry, there is an error in the pre-\nvious answer. It requires modifications to the initial answer.\nTo enhance the model’s ability to generate more appropri-\nate reasoning process and correct answer, we utilize the\nground truth, representing the standard answer, as the re-\nvised answer. Additionally, we employ gpt-3.5-turbo to as-\nsist in generating the COT analysis process, denoted as G.\nWe use prompts like the answer of [Question] is [Ground\nTruth]. Please provide a step-by-step explanation for resolv-\ning the given problem. Therefore, the data format is ( A1\nn-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18165\nBase Models OpenBookQA CommonsenseQA\nACC-First ACC ACC-First ACC\nCuteGPT-7B 25.2 29.0 (+3.8) 23.2 28.9 (+3.7)\nCuteGPT-13B 37.2 42.0 (+4.8) 35.6 37.9 (+2.3)\nLlama2-7B 52.2 52.2 (+0.0) 52.2 52.3 (+0.1)\nChatGLM-6B 37.0 42.6 (+5.6) 34.3 38.7 (+4.4)\nVicuna-7B 28.6 28.80 (+0.4) 25.9 26.2 (+0.3)\nVicuna-13B 33.8 34.0 (+0.2) 32.4 32.6 (+0.3)\nTable 2: Main results of several open-source LMs. After self-correction, their accuracy is improved on test datasets.\nCOT||A1\nn||NV ||A2\nn-COT||A2\nn ··· An\nn||PV ), where NV in-\ndicates negative verification. In Table 1, we provide two gen-\neral examples of self-correction data, representing examples\nwhere the correct answer is obtained without correction and\nwith one correction respectively. We also provide detailed\nprompt examples used at each step in the Appendix.\nThis pipeline can be utilized to customize the self-\ncorrection data for various corrections, depending on the\nspecific task type. The general process of constructing self-\ncorrection data is shown in Figure 2.\nPartial Answer Masking\nIn the instruction fine-tuning stage, the goal is to guide\nthe model to follow human intention by referring to anno-\ntated data. Usually, only the loss associated with the answer\ncomponent of the training data is used for gradient back-\npropagation, while the loss related to the input is not em-\nployed in updating weights.\nIn our paper, we apply this loss calculation method to\ngood cases. However, it is not suitable for bad cases due\nto two reasons. First, these bad cases contain error informa-\ntion, which increases the likelihood of the common issue of\nhallucination inherent in language models. Second, deliber-\nately training the model to first generate incorrect answers\nand then correcting them does not align with our intention\nof teaching the LM how to self-correct. Our aim is instead\nto enable the model to spontaneously correct itself when it\ngenerates information inconsistent with its internal knowl-\nedge. Therefore, for bad cases, we refrain from computing\nthe loss corresponding to the incorrect answer part in the\noutput during training. Instead, we calculate the loss from\nthe output on self-verification. It means that for bad cases,\nonly the self-verification and modified correct answer con-\ntribute to the loss calculation and parameter update. We call\nthis training method Partial Answer Masking (PAM). In Ta-\nble 1, the underline part in the output is excluded from the\nloss calculation.\nExperiments\nExperimental Settings\nDatasets. We conduct experiments on two question-\nanswering datasets, including OpenBookQA 1 and\nCommonsenseQA2. OpenBookQA is a science question-\nanswering dataset, containing 5,957 elementary-level\n1http://data.allenai.org/OpenBookQA\n2https://www.tau-nlp.sites.tau.ac.il/commonsenseqa\nscience multiple-choice questions with 4 options each.\nThese questions evaluate human comprehension of 1,326\ncore science facts and their application to novel scenarios.\nCommonsenseQA is a single choice question-answering\ndataset that necessitates diverse forms of commonsense\nknowledge for accurate answer prediction. It comprises\n12,102 questions with 5 choices each. After performing\nour proposed self-correction data construction process\non the two datasets, the training data comprises about\n15,000 self-correction samples. We use another about 1,700\nexamples as test data, of which 500 are from OpenBookQA\nand 1,200 are from CommonsenseQA.\nBase language models. Our goal is to evaluate whether\nwe can improve the performance of any LMs through our\nproposed ISC, even to small LMs. We opt for open-source\nmodels with parameters ranging between 6 billion and 13\nbillion. We use CuteGPT-7B, CuteGPT-13B 3, ChatGLM-\n6B4, Llama2-7B 5, Vicuna-7B 6 and Vicuna-13B 7 as our\ninstruction fine-tuning base models for continuing training.\nThese models have been fine-tuned on the instruction data\nand have the ability to follow instructions. CuteGPT is\nbased on the original Llama model structure, expands the\nChinese vocabulary and performs pre-training. CuteGPT\nhas two public versions: CuteGPT-7B and CuteGPT-13B.\nChatGLM-6B also is an open bilingual language model. It is\ntrained for about 1T tokens of Chinese and English corpus,\nand demonstrates outstanding performance among language\nmodels of equal parameter size. Llama2 is a family of\nstate-of-the-art open-access large language models released\nby Meta, ranging in scale from 7 billion to 70 billion\nparameters. Vicuna is also an open-source chatbot trained\nby fine-tuning Llama.\nIn the instruction fine-tuning stage, these models employ\ndistinct training strategies. CuteGPT family models employ\nfull fine-tuning, Llama2-7B and Vicuna family models uti-\nlize Low-Rank Adaptation (LORA), and ChatGLM-6B em-\nploys Prompt-tuning.\nMetrics. In our experiment, a model is given two chances\nto answer questions. It only attempts to correct itself after\nmaking an error in the first response. To evaluate the\naccuracy of generation answer, we employ string matching\n3https://github.com/Abbey4799/CuteGPT/\n4https://github.com/THUDM/ChatGLM-6B\n5https://huggingface.co/meta-llama/Llama-2-7b-hf\n6https://huggingface.co/lmsys/vicuna-7b-v1.3\n7https://huggingface.co/lmsys/vicuna-13b-v1.3\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18166\nBase Models Confidence EvalACC R2R R2W W2W W2R\nOpenBookQA\nCuteGPT-7B 70.6 36.6 9 20 78 39\nCuteGPT-13B 40.8 52.0 57 54 57 78\nLlama2-7B 99.6 52.2 0 0 2 0\nChatGLM-6B 60.4 27.2 34 31 76 59\nVicuna-7B 98.6 28.8 0 0 6 1\nVicuna-13B 97.2 34.2 1 4 4 5\nCommonsenseQA\nCuteGPT-7B 87.7 26.8 10 6 83 51\nCuteGPT-13B 42.2 53.8 111 131 111 159\nLlama2-7B 99.8 52.3 0 0 4 2\nChatGLM-6B 52.7 52.3 70 92 272 146\nVicuna-7B 98.9 26.0 1 2 13 9\nVicuna-13B 97.5 33.1 5 5 13 8\nTable 3: The quantitative results of Intrinsic Self-Correction on two tasks.\nto extract the LM’s option answers and compare them\nwith the ground truth provided in datasets. We use several\nevaluation metrics to assess the various performance of the\nISC. We introduce the following evaluation metrics:\n• ACC-First: it represents the accuracy of the initial answer\nof LM.\n• ACC: it denotes the accuracy after correction of LM.\n• EvalACC: this metric represents the probability of the\nLM correctly evaluating its own initial answer. It reflects\nthe self-verification ability of the LM.\n• Confidence: it refers to the level of confidence of LM,\nwhich represents the proportion of questions where LMs\nassess their answers as correct.\nBesides, in our experiments, to assess the capability for\nself-modification of LM, we use R2R, R2W, W2R, and\nW2W to denote the times that the LM modifies the correct\nanswer to the right answer, changes the right answer to the\nwrong answer, changes the wrong answer to another incor-\nrect answer, and modifies the wrong answer to the right an-\nswer, respectively.\nResults and Analysis\nMain Results. We conduct experiments to analyze the ac-\ncuracy after applying ISC on the test data. The results are\npresented in Table 2. Following the application of self-\ncorrection, a noticeable enhancement in accuracy is ob-\nserved across all models for the two given tasks. This im-\nplies that the integration of ISC provides the base models\nwith self-correction capabilities. This intrinsic ability allows\nthe model to modify the answer when it detects an error in\nthe initial response generation. For example, in the case of\nChatGLM-6B, correcting answers yields a notable improve-\nment of 5.6% in accuracy on the OpenBookQA dataset, im-\nproving it from 37% to 42.6%.\nThe effectiveness of enhancing the self-correction of\nsmall LMs depends on the innate abilities. The variations\nin effects of enhancement differ among models with simi-\nlar parameter scales. For instance, among these base mod-\nels, ChatGLM-6B features the smallest parameter scale, yet\nit demonstrates the most prominent improvement. On the\nother hand, while Llama2-7B attains the highest accuracy\nin its answers, making corrections to its responses proves to\nbe challenging.\nQuantitative Analysis. In the part, we delve further into\nthe analysis of the intrinsic self-correction of base models.\nThe results are shown in Table 3.\nDiscovery 1: When base models lack strong inherent ca-\npabilities but exhibit a high degree of confidence in their\ngenerated outcomes, the accuracy of self-verification tends\nto be notably lower. The performance gains derived from\nself-correction remain constrained. Taking Vicuna-13B as\nan example, it shows high confidence in its responses, which\nmakes it challenging to accurately evaluate its initially gen-\nerated answer, resulting in minimal attempts to modify its\ninitial responses.\nDiscovery 2: Observing the values in the W2R column,\nwe find that if an LM can recognize errors in its responses\nand attempt to modify the initial answers, an opportunity\narises to transform them into accurate solutions. This sup-\nports the earlier assumption that hallucination in LM out-\nput is not solely attributed to knowledge deficits, but is also\nlinked to contextual texts. In this case, relying on the model\nitself for correction becomes feasible.\nDiscovery 3: The values in the W2W column also con-\nstitute a significant portion of the total modifications. How-\never, despite undergoing self-correction, the models have not\nachieved successful revisions. Two factors account for this\nphenomenon. Firstly, a single corrective step is not enough\nto help LM answer correctly, potentially leading to persistent\nincorrect answers. Employing multiple iterations of correc-\ntions could potentially reduce the W2W values to some ex-\ntent. Secondly, the persistence of incorrect responses by the\nmodel could primarily result from knowledge insufficiency.\nThe infusion of relevant domain knowledge is a method to\nbe considered.\nDiscovery 4: By examining the values within the R2R col-\numn, we discern that even though our experiments primar-\nily focused on self-verification of the final answers rather\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18167\nBase Models OpenBookQA Commons\nenseQA\nACC-First\nACC Confidence EvalACC ACC-First ACC Confidence EvalACC\nCuteGPT-7B\nw/ PAM 25.2 29.0 70.6 36.6 23.2 26.9 87.7 26.8\nCuteGPT-7B w/o PAM 26.0 28.2 2.0 68.6 18.8 31.2 2.2 76.2\nCuteGPT-13B\nw/ PAM 37.2 42.0 40.8 52.0 36.6 37.9 42.2 53.2\nCuteGPT-13B w/o PAM 28.6 32.4 1.6 68.2 20.5 34.2 1.9 77.8\nChatGLM-6B w/\nPAM 37.0 42.6 60.4 27.2 34.3 38.7 52.7 52.3\nChatGLM-6B w/o PLM 30.2 36.0 56.6 32.2 26.0 32.9 50.0 47.2\nVicuna-7B\nw/ PAM 28.6 28.8 98.6 28.8 25.9 26.2 98.9 26.0\nVicuna-7B w/o PAM 23.4 23.6 97.4 25.0 13.8 13.9 39.8 14.0\nVicuna-13B\nw/ PAM 33.8 34.0 97.2 34.2 32.4 32.6 97.5 33.9\nVicuna-13B w/o PAM 32.0 31.2 89.6 32.6 31.5 32.4 97.5 33.1\nTable 4: The impact of Partial Answer Masking on capability for self-correction.\nFigure 3: Zero-shot performance of Intrinsic Self-Correction\non the StrategyQA test data. After the second round of cor-\nrection, the accuracy improves obviously.\nthan the problem-solving process, we note that the models,\nthrough self-correction, could identify inadequacies in the\nanalytical process and subsequently provide supplementary\nanalysis.\nWe conduct case study and present several instances about\nW2R and R2R cases in the Appendix.\nAblation Analysis. In this part, we verify the impact of\nthe PAM on the LM’s self-correction through ablation anal-\nysis. Unlike PAM, a commonly used training method in IFT\ninvolves utilizing the entire answer for loss calculation. By\nemploying identical data, model architecture, and hyper pa-\nrameters settings, we conducted a comparative evaluation of\nthe effects of the PAM on self-correction, as illustrated in\nTable 4.\nExcept for CuteGPT-7B without PAM, which exhibits a\nslightly higher initial answer accuracy on the OpenBook\ndataset compared to CuteGPT-7B with PAM, nearly all re-\nsults indicate that employing the PAM leads to higher accu-\nracy in generating answers. Furthermore, the improvement\nin answer quality through self-correction is most prominent\nafter utilizing the PAM.\nTraining without PAM appears to reduce the accuracy\nof self-verification, which is particularly evident in the\nCuteGPT family models. This phenomenon arises from an\nimbalance in our constructed training data, where the num-\nber of bad cases outweighs that of good cases. However,\nsuch data imbalances do not impact the results when em-\nploying PAM. This is because, in IFT with PAM, the model\neffectively learns from the correct information of answer\nwhile disregarding the incorrect information. Without PAM,\nthe model learns from the entire answer, including incorrect\ninformation as well, leading the model to evaluate answers\nas incorrect in most cases. The relatively low accuracy of an\nLM, assessing answers as incorrect helps enhance the accu-\nracy of self-verification. Additionally, for the Vicuna family\nmodels, regardless of the training method employed, they\nconsistently exhibit strong confidence in their generated out-\nputs, making them refuse to self-correct.\nZero-shot Performance on New Task. We evaluate the\ngeneralization ability of using ISC on a novel task. We\nchoose StrategyQA8 as the new test task, which serves as\na question answering benchmark focusing open-domain in-\nquiries. This task requires providing either “true” or “false”\nas a response to the given questions. The results are pre-\nsented in Figure 3.\nWe discover that ISC remains effective for the new task.\nAfter the second round of correction, the accuracy of all\nLMs improve.\nConclusion\nWe introduce Intrinsic Self-Correction (ISC) in LMs, an ap-\nproach that utilizes models’ own capabilities to identify and\nfurther modify their initial responses autonomously. This\nstrong capability can even be applied to smaller LMs. We\nfirst devise a general process for constructing self-correction\ndata, applicable to generating diverse self-correction task\ntraining data. Furthermore, we introduce a novel fine-tuning\nmethod named PAM to instruct LMs to self-correct. We con-\nduct experiments on several open-source LMs to validate\nthe efficacy of ISC. The experimental results on two distinct\ntasks consistently demonstrate that the utilization of ISC em-\npowers the models with the capability for self-correction,\nand improves the accuracy of generated answers. In the best\ncase, the accuracy enhancement reaches up to 5.6%. We also\nconduct a comprehensive analysis of the ability of the base\nmodel to self-validate and self-correct after using ISC. These\nfindings help us understand how ISC works better.\n8https://github.com/eladsegal/strategyqa\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18168\nAcknowledgements\nYanghua Xiao is also a member of Research Group of Com-\nputational and AI Communication at Institute for Global\nCommunications and Integrated Media, Fudan University.\nThis work was supported by Science and Technol-\nogy Commission of Shanghai Municipality Grant (No.\n22511105902), Shanghai Municipal Science and Technol-\nogy Major Project (No.2021SHZDZX0103), and National\nNatural Science Foundation of China (Grant No.62102095).\nReferences\nAzaria, A.; and Mitchell, T. 2023. The Internal State of an\nLLM Knows When its Lying. arXiv:2304.13734.\nBai, Y .; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.;\nJones, A.; Chen, A.; Goldie, A.; Mirhoseini, A.; McKinnon,\nC.; et al. 2022. Constitutional ai: Harmlessness from ai feed-\nback. arXiv preprint arXiv:2212.08073.\nFlower, L.; and Hayes, J. R. 1981. A cognitive process the-\nory of writing. College composition and communication,\n32(4): 365–387.\nGanguli, D.; Askell, A.; Schiefer, N.; Liao, T.; Luko ˇsi¯ut˙e,\nK.; Chen, A.; Goldie, A.; Mirhoseini, A.; Olsson, C.;\nHernandez, D.; et al. 2023. The capacity for moral\nself-correction in large language models. arXiv preprint\narXiv:2302.07459.\nGao, L.; Schulman, J.; and Hilton, J. 2023. Scaling laws\nfor reward model overoptimization. In International Con-\nference on Machine Learning, 10835–10866. PMLR.\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;\nYue, J.; and Wu, Y . 2023. How Close is ChatGPT to Hu-\nman Experts? Comparison Corpus, Evaluation, and Detec-\ntion. arXiv:2301.07597.\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2020. The Curious Case of Neural Text Degeneration. In\nInternational Conference on Learning Representations.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\nof large language models. In International Conference On\nLearning Representations.\nHuang, Y .; Feng, X.; Feng, X.; and Qin, B. 2023. The Fac-\ntual Inconsistency Problem in Abstractive Text Summariza-\ntion: A Survey. arXiv:2104.14839.\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.;\nChess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and\nAmodei, D. 2020. Scaling laws for neural language mod-\nels. arXiv preprint arXiv:2001.08361.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199–22213.\nLi, X. L.; and Liang, P. 2021. Prefix-tuning: Optimiz-\ning continuous prompts for generation. arXiv preprint\narXiv:2101.00190.\nLin, S.; Hilton, J.; and Evans, O. 2022. TruthfulQA: Measur-\ning How Models Mimic Human Falsehoods. InProceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), 3214–3252.\nDublin, Ireland: Association for Computational Linguistics.\nMadaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.;\nWiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang,\nY .; Gupta, S.; Majumder, B. P.; Hermann, K.; Welleck, S.;\nYazdanbakhsh, A.; and Clark, P. 2023. Self-Refine: Iterative\nRefinement with Self-Feedback. arXiv:2303.17651.\nManakul, P.; Liusie, A.; and Gales, M. J. F. 2023.\nSelfCheckGPT: Zero-Resource Black-Box Hallucina-\ntion Detection for Generative Large Language Models.\narXiv:2303.08896.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and\nLowe, R. 2022. Training language models to follow instruc-\ntions with human feedback. arXiv:2203.02155.\nPaul, D.; Ismayilzada, M.; Peyrard, M.; Borges, B.;\nBosselut, A.; West, R.; and Faltings, B. 2023. RE-\nFINER: Reasoning Feedback on Intermediate Representa-\ntions. arXiv:2304.01904.\nSchick, T.; Dwivedi-Yu, J.; Jiang, Z.; Petroni, F.; Lewis, P.;\nIzacard, G.; You, Q.; Nalmpantis, C.; Grave, E.; and Riedel,\nS. 2023. PEER: A Collaborative Language Model. In In\nProceedings of The 11th International Conference on Learn-\ning Representations.\nShinn, N.; Cassano, F.; Labash, B.; Gopinath, A.;\nNarasimhan, K.; and Yao, S. 2023. Reflexion: Lan-\nguage Agents with Verbal Reinforcement Learning.\narXiv:2303.11366.\nStiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.;\nV oss, C.; Radford, A.; Amodei, D.; and Christiano, P. F.\n2020. Learning to summarize with human feedback.\nAdvances in Neural Information Processing Systems, 33:\n3008–3021.\nSuzgun, M.; Scales, N.; Sch ¨arli, N.; Gehrmann, S.; Tay,\nY .; Chung, H. W.; Chowdhery, A.; Le, Q.; Chi, E.; Zhou,\nD.; and Wei, J. 2023. Challenging BIG-Bench Tasks and\nWhether Chain-of-Thought Can Solve Them. In Findings of\nthe Association for Computational Linguistics: ACL 2023 ,\n13003–13051. Toronto, Canada: Association for Computa-\ntional Linguistics.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2022a. Finetuned Lan-\nguage Models Are Zero-Shot Learners. arXiv:2109.01652.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022b. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nWelleck, S.; Lu, X.; West, P.; Brahman, F.; Shen, T.;\nKhashabi, D.; and Choi, Y . 2022. Generating Sequences by\nLearning to Self-Correct. ArXiv, abs/2211.00053.\nWeng, Y .; Zhu, M.; He, S.; Liu, K.; and Zhao, J. 2022. Large\nlanguage models are reasoners with self-verification. arXiv\npreprint arXiv:2212.09561.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18169\nYang, K.; Tian, Y .; Peng, N.; and Klein, D. 2022. Re3: Gen-\nerating Longer Stories With Recursive Reprompting and Re-\nvision. In Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, 4393–4479.\nAbu Dhabi, United Arab Emirates: Association for Compu-\ntational Linguistics.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; and Chi,\nE. 2023. Least-to-Most Prompting Enables Complex Rea-\nsoning in Large Language Models. arXiv:2205.10625.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18170"
}