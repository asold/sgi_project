{
    "title": "Optimized Transformer Models for FAQ Answering",
    "url": "https://openalex.org/W3023578091",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2898226093",
            "name": "Sonam Damani",
            "affiliations": [
                "Microsoft (India)"
            ]
        },
        {
            "id": "https://openalex.org/A2613050034",
            "name": "Kedhar Nath Narahari",
            "affiliations": [
                "Microsoft (India)"
            ]
        },
        {
            "id": "https://openalex.org/A2736998290",
            "name": "Ankush Chatterjee",
            "affiliations": [
                "Microsoft (India)"
            ]
        },
        {
            "id": "https://openalex.org/A2113508140",
            "name": "Manish Gupta",
            "affiliations": [
                "Microsoft (India)"
            ]
        },
        {
            "id": "https://openalex.org/A2228310595",
            "name": "Puneet Agrawal",
            "affiliations": [
                "Microsoft (India)"
            ]
        },
        {
            "id": "https://openalex.org/A2613050034",
            "name": "Kedhar Nath Narahari",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6828894009",
        "https://openalex.org/W2065240770",
        "https://openalex.org/W3000983017",
        "https://openalex.org/W2804032941",
        "https://openalex.org/W6601144205",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W2956105129",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2146537661",
        "https://openalex.org/W2109088983",
        "https://openalex.org/W2083717838",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W2997006708",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W2122354955",
        "https://openalex.org/W4232984278",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2593833795",
        "https://openalex.org/W2016824112",
        "https://openalex.org/W2799014768",
        "https://openalex.org/W2117775678",
        "https://openalex.org/W4234296393",
        "https://openalex.org/W314017758",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W198656984",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2949251082",
        "https://openalex.org/W2155482025",
        "https://openalex.org/W2911803042",
        "https://openalex.org/W2115186633",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2766839578",
        "https://openalex.org/W2134797427"
    ],
    "abstract": null,
    "full_text": "Optimized Transformer Models\nfor F AQ Answering\nSonam Damani(B), Kedhar Nath Narahari, Ankush Chatterjee,\nManish Gupta, and Puneet Agrawal\nMicrosoft, Hyderabad, India\n{sodamani,kedharn,anchatte,gmanish,punagr}@microsoft.com\nAbstract. Informational chatbots provide a highly eﬀective medium for\nimproving operational eﬃciency in answering customer queries for any\nenterprise. Chatbots are also preferred by users/customers since unlike\nother alternatives like calling customer care or browsing over FAQ pages,\nchatbots provide instant responses, are easy to use, are less invasive and\nare always available. In this paper, we discuss the problem of FAQ answer-\ning which is central to designing a retrieval-based informational chatbot.\nGiven a set of FAQ pagess for an enterprise, and a user query, we need\nto ﬁnd the best matching question-answer pairs from s. Building such\na semantic ranking system that works well across domains for large QA\ndatabases with low runtime and model size is challenging. Previous work\nbased on feature engineering or recurrent neural models either provides\nlow accuracy or incurs high runtime costs. We experiment with multi-\nple transformer based deep learning models, and also propose a novel\nMT-DNN (Multi-task Deep Neural Network)-based architecture, which\nwe call Masked MT-DNN (or MMT-DNN). MMT-DNN signiﬁcantly out-\nperforms other state-of-the-art transformer models for the FAQ answer-\ning task. Further, we propose an improved knowledge distillation compo-\nnent to achieve∼2.4x reduction in model-size and∼7x reduction in run-\ntime while maintaining similar accuracy. On a small benchmark dataset\nfrom SemEval 2017 CQA Task 3, we show that our approach provides\nan NDCG@1 of 83.1. On another large dataset of ∼281K instances cor-\nresponding to ∼30K queries from diverse domains, our distilled 174 MB\nmodel provides an NDCG@1 of 75.08 with a CPU runtime of mere 31 ms\nestablishing a new state-of-the-art for FAQ answering.\n1 Introduction\nReducing agent costs in the call center is typically high on the list of priorities\nof call center managers in any enterprise. Enterprises put up frequently asked\nquestions (FAQ) pages to satisfy users’ frequent information needs so as to avoid\nsuch calls. But often such pages are too large and not very well structured for\nusers to read. The diﬃculties faced by users in interacting with the FAQ pages\nare multi-fold – (1) User has to scan through a long list of QA pairs. (2) FAQs in\na list may be poorly organized and not semantically grouped. (3) Multiple FAQs\nmay answer the query, and the user must look out for a QA pair that answers\nc⃝ Springer Nature Switzerland AG 2020\nH. W. Lauw et al. (Eds.): PAKDD 2020, LNAI 12084, pp. 235–248, 2020.\nhttps://doi.org/10.1007/978-3-030-47426-3\n_19\n236 S. Damani et al.\nthe question with the right level of speciﬁcity. (4) An FAQ list may sometimes\nbe scattered over several documents.\nIn addition, a poorly managed call center or mismatching working hours for\nglobal customers, could lead to long wait times for customers who may then move\nover to other competitive businesses. Alternatively, users pose such queries on\ncommunity question answering (cQA) forums, or contact businesses over slow\nmedia like emails or phone calls. In 2014, Quora, a popular cQA forum, claimed\nthat 10% of U.S. population uses its service every month\n1 contributing to a total\nof 61M questions with 108M answers2. Such popularity of cQA forums at least\npartially indicates the diﬃculty faced by users in interacting with FAQ pages to\nobtain answers.\nTo provide correct information instantly at much lower operating costs,\nretrieval-based chatbots that can match user queries with content on FAQ pages\nare highly desirable. In this paper, we discuss the problem of FAQ answering\nwhich is central to designing a retrieval-based information chatbot. LetD denote\nthe set of question-answer pairs extracted from a set of FAQ pagess for an enter-\nprise. Given D and a user query q, our goal is to rank question-answer pairs in\nD.T o pK QA pairs with high scores are returned to the user. Figure 1 shows\npossible system snapshots using two user interfaces – web search as well as a\nchatbot. In case of web search interface (left of Fig.1), K is set to 4, whileK =1\nfor the chatbot interface (right of Fig.1).\nHello\nHello, how can I help you?\nI am travelling. How can I get OTP?\nSince this is an addiƟonal security \nmeasure, OTP is sent only to your mobile \nnumber registered with ICICI Bank.\nType your message here … Send\nBut I cannot receive OTP. Can I transfer \nfunds to another account in any other way?\nYou cannot. You are required to authenƟcate \nyour transacƟon by entering OTP.\nI am travelling. How can I get OTP? US Bank Search\nhƩps://www.usbank.com/ Time taken: 31ms\nQ: Is there any other way to get OTP? (Source)\nA: Since this is an addiƟonal security measure, OTP is sent only to your \nmobile number registered with US Bank. \nQ: How do I transact or place request online if I do not receive OTP? \n(Source\n)\nA: You cannot. You are required to authenƟcate your transacƟon by \nentering OTP.\nQ: Are there any charges to get OTP on internaƟonal mobile? (Source)\nA: US Bank does not levy any service charges for OTP on InternaƟonal \nMobile number.\nQ: Will the user id get disabled if I enter the OTP incorrectly? (Source\n)\nA: Yes User ID will get disabled in case OTP is entered incorrectly 3 Ɵmes\nFig. 1. Web Search interface (left), Chatbot interface (right);\nNote that this problem is similar to the problem of automatically answering\nquestions on cQA forums by matching existing question-answer pages. However,\nthere are some major diﬀerences as follows: (1) Queries on cQA forums are much\nlonger than queries and questions on FAQ pages. In fact, often times, cQA queries\nhave a subject and a body [29]. (2) cQA forums have a user network. Thus every\n1 https://venturebeat.com/2015/12/21/quora-claims-10-of-u-s-population-uses-its-\nservice-every-month/.\n2 https://www.quora.com/How-many-questions-have-been-asked-on-Quora-1 .\nOptimized Transformer Models for FAQ Answering 237\nQA pair is associated with a set of users. Unlike that, when ranking QA pairs\nfrom FAQ pages, we cannot exploit signals from any user network. (3) cQA pages\ntypically have a question but multiple user-voted answers. FAQ pages have no\nuser-voting, and only one answer per question. (4) On cQA forums, diﬀerent\nanswers may apply based on user context. On FAQ pages, every question has a\nunique answer.\nFAQ answering is a challenging task. Solving the problem needs prediction of\nquery-question semantic similarity and query-answer relevance, in a joint man-\nner. Also, building a general system that works across domains implies that we\ncannot resort to any domain speciﬁc heuristics. Finally, although recent deep\nlearning based systems provide high accuracy across multiple NLP tasks, build-\ning a deep learning based system for FAQ Answering for large QA databases\nwith low runtimes and model size brings in more challenges.\nPrevious work on answering a question, given FAQ pages, was based on fea-\nture engineering (FAQ-Finder [8], Auto-FAQ [28], [11,22]) or typical attention-\nbased recurrent neural network models (SymBiMPM [7]). Recently, transformer\nbased networks [24] have shown signiﬁcant gains across many natural language\nprocessing tasks. In this paper, we propose the use of transformer network\nbased methods like Bidirectional Encoder Representations from Transformers\n(BERT) [6] and Multi-task Deep Neural Network (MT-DNN) [15]. Further, we\npropose a novel architecture, MMT-DNN, based on a masking trick speciﬁcally\napplicable to input in the form of (query, question, answer) triples. To make\nsuch models practically usable, we need to reduce the model size as well as the\nexecution time. Hence, we propose an improved knowledge distillation method\nfor our MMT-DNN model. Our experiments with two datasets show that the\nproposed model outperforms all the baselines by signiﬁcant margins. Also, our\ndistilled 174MB MT-DNN-3 model provides a runtime of mere 31 ms making it\nusable in real-time chatbot scenarios. We make the following main contributions\nin this paper.\n– We propose the use of transformer based models like BERT and MT-DNN\nfor solving the FAQ Answering task, and also present a novel architecture,\nMMT-DNN, that achieves better accuracy.\n– We propose and experiment with an improved knowledge distillation method\nto reduce the model size and model runtime.\n– On two real world datasets, our proposed MMT-DNN establishes a new state-\nof-the-art for FAQ answering.\n2 Related Work\nData Mining for F AQ Web Pages. Research on FAQ web pages has focused\non three sub-areas: (1) FAQ mining using list detection algorithms [ 11,14], (2)\nanswering questions using FAQ web pages [8,11,22,28], (3) navigational inter-\nface for Frequently Asked Question (FAQ) pages [ 20], and (4) Completeness\n238 S. Damani et al.\nof FAQ pages [ 3]. In this paper, we focus on the FAQ answering task. Pre-\nvious work on answering a question given FAQ pages (FAQ-Finder [ 8], Auto-\nFAQ [28], [2,11,13,21,23]) was based on traditional feature engineering for sur-\nfacing statistical/semantic similarities between query and questions. Most of\nthese works considered similarity between query and questions, very few consid-\nered query-answer similarity. We use transformer based deep learning methods\nfor jointly considering query-question and query-answer similarity.\nRecently deep learning based methods have been proposed for FAQ Answer-\ning. Wu et al. [29] propose the attention-based Question Condensing Networks\n(QCN) to align a question-answer pair where the question is composed of a\nsubject and a body. To suit our problem setting, we experiment by substitut-\ning query for the subject, and question for the body. Gupta et al. [ 7]p r o p o s e\nSymBiMPM (BiLSTMs with multi-perspective matching blocks) for computing\nquery-QA match. Recently, transformer network models have emerged as state-\nof-the-art across multiple NLP tasks. Hence, unlike previous deep learning works,\nwe use transformer networks for FAQ answering.\nApplications of Transformer Models. After the original Transformer work\nby Vaswani et al. [24], several architectures have been proposed like BERT [6],\nMT-DNN [15]e t c .T h eG L U E[26] and the SuperGLUE [25] dashboards tell us\nthat such models have outperformed previously proposed methods across com-\nplex NLP tasks like text classiﬁcation, textual entailment, machine translation,\nword sense disambiguation, etc. We present the ﬁrst work to investigate appli-\ncation of transformers to FAQ answering task.\nModel Compression. Existing deep neural network models are computation-\nally expensive and memory intensive, hindering their deployment in devices with\nlow memory resources or in applications with strict latency requirements. Chat-\nbots expect near realtime responses. Thus, transformer based models need to\nbe compressed and accelerated. In the past few years, multiple techniques have\nbeen proposed for model optimization including pruning, quantization, knowl-\nedge distillation, and low rank factorization. Cheng et al. [ 5]p r o v i d eag o o d\nsurvey of such methods. In this paper, we explore diﬀerent variations of knowl-\nedge distillation and present a novel architecture that provides best results for\nthe FAQ answering task.\n3 Approach\nGiven a question-answer database, when a user queryq arrives, we ﬁrst compute\na list of candidate QA pairs which have high BM25 score [ 19] with respect to\nthe query. Given the latency constraints, we use computationally cheap BM25\nmatch, however, understandably, BM25 may have missed semantically similar\nbut syntactically diﬀerent QA pairs. If q uses synonyms of the words in the\nideal QA pair, it is possible that the pair would not be selected based on BM25\nscore. These candidate QA pairs, along with the original query, are scored using\nvarious methods described in this section. TopK QA pairs with high scores are\nreturned to the user.\nOptimized Transformer Models for FAQ Answering 239\nWe ﬁrst discuss baseline methods like BiLSTMs with attention and Sym-\nBiMPM [7]. Next, we discuss our proposed transformer based methods. All of\nthese methods take query (q), question (Q), and answer (A) as input, and output\none of the three classes: Good, Average or Bad indicating the degree of match\nbetween q and the (Q, A) pair. Figure2 illustrates architectures of various meth-\nods discussed in this section.\n3.1 Baselines\nBiLSTMs. As illustrated in Fig.2(A), in this approach, the query, question and\nanswer are processed using three two-row bidirectional LSTMs [10]. The query\nand question BiLSTMs share weights. We use BiLSTMs with attention. The\nﬁnal output from the last hidden layer of each of the BiLSTMs is concatenated\nand fed into a fully connected neural network (MLP). The output layer has\nthree neurons (one for each of the three classes) across all the architectures. The\nnetwork is trained using Adam optimizer [12] with cross entropy loss.\nSymBiMPM.\nSymmetric Bilateral Multi-Perspective Matching Block (Sym-\nBiMPM) is the method proposed by Gupta et al. [ 7]. This model uses a multi-\nperspective matching block [ 27] to compare two sequences and generate the\nmatched representations for both these sequences. This block has four diﬀerent\nmatching mechanisms that are used on the input sequences. Matching is applied\nin both the directions, i.e. if P and Q are the two inputs, then the output is a\nmatched representation of P obtained by attending to Q, and a matched repre-\nsentation ofQ obtained by attending toP. All the BiLSTMs share weights. Also,\nboth the match blocks share weights. As illustrated in Fig.2(B), Multi-perspective\nmatching blocks are used for query-question and query-answer matching followed\nby attention layer and fully connected layers to get the ﬁnal class label.\nQuery\nConcat\nOutput\nQuesƟon Answer\nTransformer Encoder\nQuery QuesƟon AnswerCLS SEP SEP\nC …\nTransformer \nEncoder2\nC\nConcat\n…\nTransformer Encoder1\nC\nTransformer \nEncoder2\nC …\nConcat+Pad Concat+Pad\nQuery’ QuesƟon’ Answer’SEP’ SEP’’\nQuesƟon Query Answer\nBiLSTM BiLSTM BiLSTM\nMatch Block Match Block\nAƩenƟon\nMLP\nBiLSTM-AƩn BiLSTM-AƩn BiLSTM-AƩn\nBiLSTM-AƩn BiLSTM-AƩn BiLSTM-AƩn\nConcat\nMLP\nBiLSTM-AƩn BiLSTM-AƩn BiLSTM-AƩn\nOutput\nOutput\nOutput\nQuery QuesƟon AnswerCLS SEP SEP\nA C\nDB\nConcat+Pad Concat+Pad\nFig. 2. Architectures of various methods: (A) BiLSTMs with attention (B) Sym-\nBiMPM (adapted from [7]) (C) BERT/MT-DNN (D) MMT-DNN\n240 S. Damani et al.\n3.2 Proposed Methods\nTransformer networks proposed by Vaswani et al. [ 24] follow a non-recurrent\narchitecture with stacked self-attention and fully connected layers for both the\nencoder and decoder, each with 6 layers. BERT and MT-DNN are two most pop-\nular extensions of the Transformer encoder network. Broadly this architecture\nis illustrated in Fig.2(C).\nBERT.\nBERT [6] essentially is a transformer encoder with 12 layers. We used the\npre-trained model which has been trained on Books Corpus and Wikipedia using\nthe MLM (masked language model) and the next sentence prediction (NSP) loss\nfunctions. The query, question and answer are concatenated into a sequence and\nare separated with a special “SEP” token. The sequence is prepended with a\n“CLS” token. The representation C for the “CLS” token from the last encoder\nlayer is used for classiﬁcation by connecting it to an output softmax layer.\nOptionally, we can ﬁnetune the pre-trained model using labeled training data\nfor the FAQ answering task.\nMT-DNN.\nThe MT-DNN architecture [ 15] extends BERT by further pre-\ntraining it with large amounts of cross-task data. Speciﬁcally, the MT-DNN\nis a 12 layer transformer encoder where the BERT model has been further pre-\ntrained using single sentence classiﬁcation, text similarity, pairwise text classiﬁ-\ncation and relevance ranking tasks. The representation C for the “CLS” token\nfrom the last encoder layer is used for classiﬁcation by connecting it to an output\nsoftmax layer. Optionally, we can ﬁnetune the pre-trained model using labeled\ntraining data for the FAQ answering task.\nMMT-DNN.\nThe proposed Masked MT-DNN method modiﬁes the MT-DNN\narchitecture, as illustrated in Fig.2(D). The transformer encoder is divided into\ntwo parts: encoder1 and encoder2. Encoder1 consists of l encoder layers, while\nencoder2 contains 12-l layers. l is a hyper-parameter tuned on validation data.\nThe input sequence (query, question, answer) is ﬁrst processed by encoder 1 to\nget a transformed sequence (query’, question’, answer’). Intuitively, (query, ques-\ntion) pair is more homogeneous compared to (query, answer) pair. Hence, we\nexplore disjoint encoding of the two pairs using separate encoder\n2 blocks for\nquery-question and query-answer matching. Both encoder2 blocks share weights.\nSpeciﬁcally, the ﬁrst encoder2 block receives the concatenated string of the CLS\ntoken, query and question as input, where the answer is masked by replacing\nanswer tokens by zeros. Similarly, the second encoder 2 block receives the con-\ncatenated string of the CLS token, query and answer as input, where the question\nis masked by replacing the question tokens by zeros. TheC token from both these\nencoder\n2 blocks are concatenated and connected to an output softmax layer.\nKnowledge Distillation. The proposed MMT-DNN model, like other Trans-\nformer models, is very large and also incurs a large number of computations\nat prediction time. Hence, we use knowledge distillation strategies [ 1]t oc o m -\npress the model and reduce latency while retaining accuracy. Figure3 shows our\nimproved knowledge distillation component.\nOptimized Transformer Models for FAQ Answering 241\nFine-tuned MMT-DNN-12 (TM) Initialization (layers 2, 3, 4)\nInitial MT-\nDNN-3 (ISM)\nKnowledge \nDistillation\nFinal Distilled \nMT-DNN-3 \n(FSM)\nScore Knowledge \nDistillation\nScore\nLabeled \nDataset \n(203K)\nUnlabeled \nDataset \n(15M)\nSoft \nTargets\nDistilled MT-DNN-3 (ISM)\nSoft \nTargets\nFig. 3. Knowledge distillation for FAQ\nanswering\nTable 1. Dataset statistics (train/dev/test)\nDataset SemEval-2017 FSD\n#Queries 266/72/70 20242/1966/7478\n#Question-\nAnswer pairs\n6711/1575/2313 1630/477/649\n#Data points 9977/1851/2767 202969/22549/55751\nAvg length of\nqueries\n41.2/37.9/43.7 7.2/9.3/9.5\nAvg length of\nquestions\n50.4/47.2/49.1 7.7/9.8/7.9\nAvg length of\nanswers\n48.9/45.1/46.3 61.4/55.3/57.9\nWe use student-teacher networks for knowledge distillation [9] by considering\nthe ﬁne-tuned MMT-DNN-12 model as a teacher model (TM) for knowledge dis-\ntillation. Layers 2, 3, and 4 of the ﬁne-tuned MMT-DNN-12 are used to initialize\na MT-DNN-3 model which is the initial student model (ISM) for knowledge dis-\ntillation. Note that the student model is a MT-DNN and not a MMT-DNN.\nA combination of hard targets from the labeled dataset and soft targets from\nthe ﬁne-tuned MMT-DNN-12 TM is used to deﬁne the loss for training the\nMT-DNN-3 model to obtain the distilled student model (DSM). Although not\nshown in the ﬁgure (due to lack of space), in order to facilitate gradual transfer\nof knowledge, the distillation from MMT-DNN-12 to MT-DNN-3 is done in a\nchain of steps where MMT-DNN-12 is ﬁrst distilled to a MT-DNN-9, then to\nMT-DNN-6 and ﬁnally to an MT-DNN-3 student model (DSM) [ 17]. We also\nhave access to a much larger (15 million sized) unlabeled dataset of queries which\nlead to clicks to FAQ pages. These are scored against the TM to generate soft\ntargets. These soft targets are then used to further distill the DSM, followed by\nTVM compiler optimizations [4] to obtain the ﬁnal distilled MT-DNN-3 student\nmodel (FSM).\n4 Experiments\n4.1 Datasets\nTable1 presents basic statistics about the two datasets.\nSemEval-2017. This dataset3 was intended for community question answering\n(cQA) originally, but the task 3 data had the QA pairs grouped by search query\nterms, which facilitated the transformation of this data into FAQ Retrieval for-\nmat where FAQs are ranked for a query and are awarded ranks as Good, Average\nor Bad (as in original dataset). We used standard train, dev, test splits provided\nby the task organizers. Although this dataset is small, we experiment with it\nsince this is the only publicly available dataset.\n3 http://alt.qcri.org/semeval2017/task3/.\n242 S. Damani et al.\nF AQ Search Dataset (FSD). This dataset was created using ∼30K queries\n(from a popular search engine’s query log) leading to clicks to FAQ pages. We\ntook only those queries which resulted into at least 5 clicks to some FAQ page.\nThe query was then compared to all the QA pairs extracted from the clicked\nFAQ pages using BM25 score [ 19] to extract a max of top 15 QA pairs. We\nthen got these (query, QA) instances judged into 3 classes (Good, Average or\nBad) using a crowdsourcing platform with three-way redundancy. The queries\nand FAQ pages were carefully chosen such that (1) they belong to multiple\ndomains like airports, banks, supermarkets, tourism and administrative bodies,\n(2) queries and QA pairs of various sizes are considered, and (3) FAQ pages with\nvarying number of QA pairs are included. Note that this dataset is ∼20x larger\nthan the SemEval-2017 dataset.\n4.2 Accuracy Comparison\nThe query, question and answer are all represented using GloVe [18] embeddings\nfor all the baseline methods. Transformer based methods use WordPiece [ 30]\nembeddings. All experiments were done on a machine with 4 Tesla V100-SXM2-\n32GB GPUs. We use the popular ranking metric, Normal Discounted Cumulative\nGain (NDCG)@K to compare various methods. For BiLSTMs in all baseline\nmethods, the hidden layer size was 300. For transformer based methods, the\nembedding size was ﬁxed to 30522 and the input sequence length was ﬁxed to\n512 tokens.\nTable2 shows accuracy comparison across various methods on both the\ndatasets. Block A shows results for baseline methods. Surprisingly, Sym-\nBiMPM [7] performs worse than BiLSTMs. SemEval-2017 dataset has labels\nfor query-question pair, for query-answer pair, as well as query-answer pair. For\nSymBiMPM [7], the authors used query-question label as the label for a (query,\nquestion, answer) triple. As a reﬁnement, we ﬁrst considered only those QA pairs\nwhere question-answer label is “good”, and then used the label for query-answer\nsimilarity as the label for a (query, question, answer) triple. Also, queries in\nthe SemEval-2017 set have a subject as well as a body. Gupta et al. [ 7] simply\nused the query subject and ignored the query body. We experiment with just\nthe query subject as well as with query subject + body. Table2 shows that using\nquery subject + body usually provides better accuracy, sometimes with a large\nmargin. We also experimented with QCN [ 29] but the results were worse than\neven BiLSTMs. This is expected due to mismatch in the problem setting, as\ndiscussed in Sect. 2. Further, Block B shows results for the proposed methods.\nAs the table shows, our proposed methods outperform existing methods by a\nsigniﬁcant margin. All results are obtained as a median of 5 runs. Both BERT\nand MT-DNN beneﬁt from ﬁnetuning across the two datasets. Also, MMT-DNN\noutperforms all other methods by a signiﬁcant margin (p<0.05 using McNemar\nTest [16]), establishing a new state-of-the-art for FAQ answering task.\nOptimized Transformer Models for FAQ Answering 243\nTable 2. Accuracy comparison across various methods. For SemEval-2017 dataset,\nresults are for two settings: (using just the query subject/using query subject + body).\nSemEval-2017 FSD\nModel NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10\nA BiLSTMs 36.62/38.83 38.43/43.17 41.76/46.3 55.70 63.02 69.34\nSymBiMPM [ 7] 34.21/34.00 38.55/38.59 40.86/44.71 54.03 61.21 68.11\nB BERT (pre-trained) 63.38/65.39 61.85/68.41 62.87/68.44 71.77 76.17 78.47\nMT-DNN (pre-trained) 68.01/60.97 64.92/61.85 64.67/62.83 70.29 75.08 77.65\nBERT (ﬁnetune) 68.01/69.22 65.19/68.61 67.46/71.12 73.97 78.29 79.79\nMT-DNN (ﬁnetune) 70.22/82.49 67.06/81.79 67.72/81.99 73.75 78.14 79.79\nMMT-DNN 71.03/84.71 70.67/82.59 71.51/82.18 75.38 78.59 80.24\nFigure4 shows NDCG@K for K = 1 to 10 for the MMT-DNN approach for\nboth the datasets. With increase in K, while the accuracy improvement for the\nFSD is intuitive, the result is not very intuitive for the SemEval-2017 dataset.\nThis is mainly because of the small size of the dataset because of which usually\nthere are very few good answers matching any query.\n75\n77\n79\n81\n83\n85\n123456789 1 0\nNDCG@K\nPosiƟon (K)\nSemEval-2017\nFSD\nFig. 4. NDCG@K for the MMT-DNN\napproach for both the datasets\n75\n77\n79\n81\n83\n0123456789 1 0 1 1\nNDCG@K\n#Encoder1 Layers (l)\nSemEval-2017\nFSD\nFig. 5. NDCG@5 with varying number\nof Encoder1 layers (l) for MMT-DNN for\nthe two datasets\nfrom SemEval-2017 dataset:working permit ... 1- do i need working permit since i have residence visa in qatar n under husband sponsor? 2- without \nworking permit expat's wife could not work in qatar? ...\n: Work permit for husband? I am thinking of sponsoring my husband to live in Qatar. I heard that if he gets a job; he will need toget a work \npermit. Are husbands able to get a work permit? ...    \n: If he is on a family visa he needs to ﬁnd a job ﬁrst so that the company who will hire him will be the one to process his work permit. ... \n: Work permit for husband? I am thinking of sponsoring my husband to live in Qatar. I heard that if he gets a job; he will need to get a work \npermit. Are husbands able to get a work permit? ...    \n: if you get over 7k you can sponser him with fam visa. once he is here and under your visa; ... he will sƟll remain under your sponsorship. ... \n: Wife/Husband with family sponsorship to work. If your wife / husband under family sponsorship of your sponsor wants to work do they have \nto transfer the sponsorship to new sponsor or they can work without sponsorship change?    \n: they don't have to transfer their sponsorship under the Company; unless they either want to or the Company requires their transfer. ... \nfrom FSD: i've paid for my parking but my ﬂight is delayed\n: What happens if I exit the car park prior to my conﬁrmed booking Ɵme?    \n: If for whatever reason you cannot exit the car park in your conﬁrmed booking Ɵme (e.g., you haven't returned due a cancelled ﬂight), the \ncredit card or debit card that you use to exit the car park (i.e. your nominated card) will be debited with the cost of the addiƟonal Ɵme, based on \nthe rates displayed at the entry to the car park.  \n: What happens if I enter the car park prior to my conﬁrmed booking Ɵme?    \n: If you enter the car park before your conﬁrmed booking Ɵme, or exit the car park later than your conﬁrmed booking Ɵme, the credit card or \ndebit card that you use to exit the car park (i.e. your nominated card) will be debited with the cost of the addiƟonal Ɵme, based on the rates \ndisplayed at the entry to the car park.  \n: How do I amend or cancel my booking?    \n: You may cancel your Booking, for any reason at any Ɵme up to 24 hours before the start of the Booking Period. To do this, ...\nFig. 6. Top 3 QA pairs returned by MMT-DNN for two queries (one from each dataset)\n244 S. Damani et al.\nFigure5 shows the NDCG@5 for MMT-DNN across the two datasets with\nvarying number of Encoder1 layers (l). As expected, the accuracy is better at\nlarger values of l. This means that it is useful to allow attention across question\nand answer in the ﬁrst few layers but let the query-question and query-answer\nattention be learned separately at higher layers. Note thatl = 0 corresponds to\nnot having Encoder1 at all, and processing (query, question) and (query, answer)\nseparately throughout the network.\nNext, we show two queries with top three QA pairs ranked by our MMT-\nDNN system in Fig. 6. For query q1, BiLSTMs had this question as the second\nresult: “Work Permit How many days does it take to ﬁnish the processing of a\nWork Permit?”. Similarly, SymBiMPM leads to unrelated questions within top\n3 like “Hepatitis C (HCV) - Work permit I have Hepatitis C (HCV); Can i get\nwork permit?”.\nSimilarly, for q\n2, baselines lead to the following unrelated question in top 3:\n“Can I get motorbike parking?”, “What do I do if I take a ticket on arrival to\nthe car park when I should have entered my credit card?”.\n4.3 Attention Visualization for MMT-DNN\nIn Fig.7, we visualize the heads from the multi-head self attention module of the\nlast encoder for our best approach. This visualization helps us understand what\npairs of words in the (query, question, answer) have high self-attention weights.\nThe results are very intuitive showing that query, question and answer are jointly\nenhancing each other’s representations to contribute to high accuracy. Figure 7\ni\n'\nve\npaid\nfor\nmy\nparking\nbut\nmy\nflight\nis\ndelayed\nwhat\nhappens\nif\ni\nenter\nthe\ncar\npark\nprior\nto\nmy\nconfirmed\nbooking\ntime\nor\nexit\nthe\ncar\npark\nlater\nthan\nmy\nconfirmed\nbooking\ntime\n?\ni\n'\nve\npaid\nfor\nmy\nparking\nbut\nmy\nflight\nis\ndelayed\nif\nyou\nenter\nthe\ncar\npark\nbefore\nyour\nconfirmed\nbooking\ntime\n,\nor\nexit\nthe\ncar\npark\nlater\nthan\nyour\nconfirmed\nbooking\ntime\n,\nthe\ncredit\ncard\nor\nwhat\nhappens\nif\ni\nenter\nthe\ncar\npark\nprior\nto\nmy\nconfirmed\nbooking\ntime\nor\nexit\nthe\ncar\npark\nlater\nthan\nmy\nconfirmed\nbooking\ntime\n?\nif\nyou\nenter\nthe\ncar\npark\nbefore\nyour\nconfirmed\nbooking\ntime\n,\nor\nexit\nthe\ncar\npark\nlater\nthan\nyour\nconfirmed\nbooking\ntime\n,\nthe\ncredit\ncard\nor\nFig. 7. Visualization of a few heads for various examples for the last encoder layer\nof our best approach. (left): query-question, (middle): query-answer, (right): question-\nanswer\nOptimized Transformer Models for FAQ Answering 245\n(left) shows the token “delayed” in the query has high attention weights for the\ntokens “prior”, “time” and “later” in the question. Figure7 (middle) shows the\ntoken “parking” in the query has high attention weights for the tokens “car” and\n“park” in the answer. Figure 7 (right) shows the token “prior” in the question\nhas high attention weights for the tokens “before” and “later” in the answer.\n4.4 Error Analysis\nWe analyzed error patterns for our best method (MMT-DNN). The most confus-\ning category is the “Average class” with lowest precision and recall. Fortunately,\nthis does not impact ranking signiﬁcantly especially in cases where there are\nenough “good” QA pairs for a query. Further, we look at a few examples to do\nmore detailed error analysis by manually assigning error categories to 60 (query,\nquestion, answer) triples incorrectly classiﬁed by MMT-DNN method. Table 3\nshows percentages contributed by each error pattern and a few examples. Ver-\nbose Match errors accounted for more than half of the errors, which is in line\nwith our expectations.\n4.5 Knowledge Distillation (KD)\nTable4 shows the NDCG obtained using the proposed architectures for KD. Even\nwith small labeled data, distilled MT-DNN-3 provides accuracy comparable to\nthe teacher model. Further distillation using large unlabeled data leads to better\nresults. Note that we ﬁxed the hard versus soft loss balancing parameter α as\n0.01. Overall, the ﬁnal model TVM-optimized MT-DNN-3 provides NDCG@1\nof 75.08 on FSD dataset with a model size of 174MB and a CPU/GPU runtime\nof 31.4/5.18 ms per instance.\nTable 3. Analysis of various types of errors with examples\nCategory Meaning % Examples\nEntity mismatch q and Q/A refer to a\ndiﬀerent main entity\n29 q:“What is best mall in Doha to buy good\nfurniture?”, Q:“where to buy good abhaya in doha”\nGeneralization q and Q/A have entities with\n“is a” relationship\n7 q: “Any aquapark in Doha?”, Q:“any water theme\npark in qatar?”\nIntent mismatch q and Q/A have diﬀerent\nintents\n5 q:“What is best mall in Doha to buy good furniture?\n... showrooms ...”, Q:“Where to buy used furniture? ..\ncheap ...”\nNegation q and Q/A have opposite\nintents\n7 q:“Is there any Carrefour which is open?”, Q:“any\nother good supermarkets apart from Carrefour”\nVerbose match q and Q/A match on\nunimportant parts\n52 q:“Is it good oﬀer? Hi Frds;i QA supervisor with 8 years\nexp in pharmaceutical have got job oﬀer from Qatar\npharma company; Salary which they have oﬀered to me\nis 5000QAR...”, Q:“Is it a good oﬀer? Dear all; I need\nyour help please:) ; i got an oﬀer from Habtoor leighton\ngroup for Planning Engineer position. They are oﬀering\n10K ...”\n246 S. Damani et al.\nTable 4. Accuracy vs size and runtime latency comparison across various models for\nthe knowledge distillation experiments (on FSD)\nModel Size CPU runtime NDCG@1 NDCG@5 NDCG@10\nMMT-DNN-12 417 MB 225 ms 75.38 78.59 80.24\nMT-DNN-9 336 MB 210 ms 76.28 78.83 80.48\nMT-DNN-6 255 MB 143 ms 74.55 77.88 79.76\nMT-DNN-3 174 MB 68.9 ms 70.56 75.47 77.91\nMT-DNN-3 (unlabeled data) 174 MB 68.9 ms 75.08 78.28 80.00\nMT-DNN-3 (unlabeled data + TVM) 174 MB 31.4 ms 75.08 78.28 80.00\nWe tried various ways of initialization of the student model for knowledge\ndistillation as shown in Table 5. Initialization using some layers of the teacher\nmodel (usually the ﬁrst few layers) is clearly better than random initialization.\nTable 5. Initialization for knowledge\ndistillation for MT-DNN-3 model using\nMMT-DNN-12 layers or Random (on\nFSD)\nInitialization NDCG@1 NDCG@5 NDCG@10\nLayers 1, 2, 3 69.20 74.76 77.24\nLayers 2, 3, 4 70.56 75.47 77.91\nLayers 4, 5, 6 65.83 71.35 75.03\nLayers 7, 8, 9 68.57 73.87 76.66\nLayers 10, 11, 12 59.83 67.16 71.87\nRandom 52.92 60.49 67.55\n50\n55\n60\n65\n70\n75\n80\n85\n01 0 0 2 0 0 3 0 0\nNDCG@1\nLatency (ms per q-QA instance)\n1. BiLSTM\n2. SymBiMPM\n3. MT-DNN-12\n4. MT-DNN-9\n5. MT-DNN-6\n6. MT-DNN-3\n7. (6)+unlabeled data\n8. (7)+TVM\n1\n2\n3\n4\n5\n6\n78\nFig. 8. Accuracy, runtime, model size\ncomparison for various models (best\nviewed in color) (Color ﬁgure online)\nFigure8 shows the accuracy versus runtime trade-oﬀ for various models.\nThe radius of the circle corresponds to the model size. Compared to all other\napproaches, the distilled MT-DNN-3 models are better than others, and among\nthem the best one is the TVM-optimized MT-DNN-3 which also used unlabeled\ndata during distillation.\n5 Conclusion\nWe proposed the use of transformer based models like BERT and MT-DNN\nfor solving the FAQ Answering task. We also proposed a novel MT-DNN archi-\ntecture with masking, MMT-DNN, which establishes a new state-of-the-art for\nFAQ answering, as evaluated on two real world datasets. Further, we propose\nand experiment with an improved knowledge distillation strategy to reduce the\nmodel size and model runtime. Overall the proposed techniques lead to models\nwith high accuracy, and small runtime and model size.\nOptimized Transformer Models for FAQ Answering 247\nReferences\n1. Ba, J., Caruana, R.: Do deep nets really need to be deep? In: NIPS, pp. 2654–2662\n(2014)\n2. Berger, A., Caruana, R., Cohn, D., Freitag, D., Mittal, V.: Bridging the lexical\nchasm: statistical approaches to answer-ﬁnding. In: SIGIR, pp. 192–199 (2000)\n3. Chatterjee, A., Gupta, M., Agrawal, P.: FAQaugmenter: suggesting questions for\nenterprise FAQ pages. In: WSDM, pp. 829–832 (2020)\n4. Chen, T., et al. TV M : an automated end-to-end optimizing compiler for deep\nlearning. In: OSDI, pp. 578–594 (2018)\n5. Cheng, Y., Wang, D., Zhou, P., Zhang, T.: A survey of model compression and\nacceleration for deep neural networks. arXiv preprint arXiv:1710.09282 (2017)\n6. Devlin, J., Chang, M., Lee, K., Toutanova, K.: Bert: pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n7. Gupta, S., Carvalho, V.: FAQ retrieval using attentive matching. In: SIGIR, pp.\n929–932 (2019)\n8. Hammond, K., Burke, R., Martin, C., Lytinen, S.: FAQ ﬁnder: a case-based app-\nroach to knowledge navigation. In: Conference on AI for applications, vol. 114\n(1995)\n9. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531 (2015)\n10. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),\n1735–1780 (1997)\n11. Jijkoun, V., de Rijke, M.: Retrieving answers from frequently asked questions pages\non the web. In: CIKM, pp. 76–83 (2005)\n12. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n13. Kothari, G., Negi, S., Faruquie, T.A., Chakaravarthy, V.T., Subramaniam, L.V.:\nSMS based interface for FAQ retrieval. In ACL, pp. 852–860 (2009)\n14. Lai, Y., Fung, K., Wu, C.: FAQ mining via list detection. In: Multilingual Sum-\nmarization and Question Answering, pp. 1–7 (2002)\n15. Liu, X., He, P., Chen, W., Gao, J.: Multi-task deep neural networks for natural\nlanguage understanding. arXiv preprint arXiv:1901.11504 (2019)\n16. McNemar, Q.: Psychological Statistics. Wiley, New York (1969)\n17. Mirzadeh, S., Farajtabar, M., Li, A., Ghasemzadeh, H.: Improved knowledge dis-\ntillation via teacher assistant: bridging the gap between student and teacher. arXiv\npreprint arXiv:1902.03393 (2019)\n18. Pennington, J., Socher, R., Manning, C.: Glove: global vectors for word represen-\ntation. In: EMNLP, pp. 1532–1543 (2014)\n19. Robertson, S., Zaragoza, H., et al.: The probabilistic relevance framework: BM25\nand beyond. FnTIR 3(4), 333–389 (2009)\n20. Schmonsees, R.J.: Dynamic frequently asked questions (FAQ) system. US Patent\n5,842,221, November 1998\n21. Sneiders, E.: Automated FAQ answering: continued experience with shallow lan-\nguage understanding. In: Question Answering Systems. Papers from the 1999 AAAI\nFall Symposium, pp. 97–107 (1999)\n22. Sneiders, E.: Automated FAQ answering with question-speciﬁc knowledge repre-\nsentation for web self-service. In: Human System Interactions, pp. 298–305 (2009)\n248 S. Damani et al.\n23. Song, W., Feng, M., Gu, N., Wenyin, L.: Question similarity calculation for FAQ\nanswering. In: Semantics, Knowledge and Grid, pp. 298–301. IEEE (2007)\n24. Vaswani, A., et al.: Attention is all you need. In: NIPS, pp. 5998–6008 (2017)\n25. Wang, A., et al.:. SuperGLUE: a stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint arXiv:1905.00537 (2019)\n26. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: GLUE: a\nmulti-task benchmark and analysis platform for natural language understanding.\nIn: ICLR (2019)\n27. Wang, Z., Hamza, W., Florian, R.: Bilateral multi-perspective matching for natural\nlanguage sentences. arXiv preprint arXiv:1702.03814 (2017)\n28. Whitehead, S.D.: Auto-faq: an experiment in cyberspace leveraging. Comput.\nNetw. ISDN Syst. 28(1–2), 137–146 (1995)\n29. Wu, W., Sun, X., Wang, H.: Question condensing networks for answer selection in\ncommunity question answering. In: ACL, pp. 1746–1755 (2018)\n30. Wu, Y., et al.: Google’s neural machine translation system: bridging the gap\nbetween human and machine translation. arXiv preprint arXiv:1609.08144 (2016)"
}