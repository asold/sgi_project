{
  "title": "Two Discourse Driven Language Models for Semantics",
  "url": "https://openalex.org/W2470665014",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Peng, Haoruo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746559556",
      "name": "Roth, Dan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2296266385",
    "https://openalex.org/W2158794898",
    "https://openalex.org/W2250544821",
    "https://openalex.org/W2088911157",
    "https://openalex.org/W2962841164",
    "https://openalex.org/W2296427152",
    "https://openalex.org/W2142269169",
    "https://openalex.org/W2166957049",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W1547546052",
    "https://openalex.org/W1579035156",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2050273484",
    "https://openalex.org/W2252052418",
    "https://openalex.org/W2005814556",
    "https://openalex.org/W1825628421",
    "https://openalex.org/W2251344056",
    "https://openalex.org/W2250201115",
    "https://openalex.org/W2252215150",
    "https://openalex.org/W2052762201",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2145374219",
    "https://openalex.org/W2963838094",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2950340514",
    "https://openalex.org/W2089745520",
    "https://openalex.org/W2078285016",
    "https://openalex.org/W2252139350",
    "https://openalex.org/W12836875",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2257051837",
    "https://openalex.org/W2578412240",
    "https://openalex.org/W1702669762",
    "https://openalex.org/W2129272839",
    "https://openalex.org/W2250836735",
    "https://openalex.org/W2000900121",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2103095311",
    "https://openalex.org/W2115792525",
    "https://openalex.org/W14680811",
    "https://openalex.org/W2250506749",
    "https://openalex.org/W2155069789"
  ],
  "abstract": "Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a \"standard\" N-gram language model and three discriminatively trained \"neural\" language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.",
  "full_text": "arXiv:1606.05679v2  [cs.CL]  27 Jun 2016\nTo appear in ACL'16\nTwo Discourse Driven Language Models for Semantics\nHaoruo Peng and Dan Roth\nUniversity of Illinois, Urbana-Champaign\nUrbana, IL, 61801\n{hpeng7,danr}@illinois.edu\nAbstract\nNatural language understanding often re-\nquires deep semantic knowledge. Ex-\npanding on previous proposals, we suggest\nthat some important aspects of semantic\nknowledge can be modeled as a language\nmodel if done at an appropriate level of ab-\nstraction. We develop two distinct mod-\nels that capture semantic frame chains\nand discourse information while abstract-\ning over the speciﬁc mentions of predi-\ncates and entities. For each model, we in-\nvestigate four implementations: a “stan-\ndard” N-gram language model and three\ndiscriminatively trained “neural” language\nmodels that generate embeddings for se-\nmantic frames. The quality of the se-\nmantic language models (SemLM) is eval-\nuated both intrinsically, using perplexity\nand a narrative cloze test and extrinsically\n– we show that our SemLM helps improve\nperformance on semantic natural language\nprocessing tasks such as co-reference res-\nolution and discourse parsing.\n1 Introduction\nNatural language understanding often necessitates\ndeep semantic knowledge. This knowledge needs\nto be captured at multiple levels, from words\nto phrases, to sentences, to larger units of dis-\ncourse. At each level, capturing meaning fre-\nquently requires context sensitive abstraction and\ndisambiguation, as shown in the following exam-\nple (Winograd, 1972):\nEx.1[Kevin] wasrobbedby [Robert]. [He] was\narrestedby the police.\nEx.2[Kevin] wasrobbedby [Robert]. [He] was\nrescuedby the police.\nIn both cases, one needs to resolve the pronoun\n“he” to either “Robert” or “Kevin”. To make\nthe correct decisions, one needs to know that the\nsubject of “rob” is more likely than the object\nof “rob” to be the object of “arrest” while the\nobject of “rob” is more likely to be the object of\n“rescue”. Thus, beyond understanding individual\npredicates (e.g., at the semantic role labeling\nlevel), there is a need to place them and their\narguments in a global context.\nHowever, just modeling semantic frames is not\nsufﬁcient; consider a variation of Ex.1:\nEx.3Kevin wasrobbedby Robert, but\nthe police\nmistakenlyarrestedhim.\nIn this case, “him” should refer to “Kevin” as\nthe discourse marker “but” reverses the meaning,\nillustrating that it is necessary to take discourse\nmarkers into account when modeling semantics.\nIn this paper we propose that these aspects of\nsemantic knowledge can be modeled as aSeman-\ntic Language Model(SemLM). Just like the “stan-\ndard” syntactic language models (LM), we de-\nﬁne a basic vocabulary, a ﬁnite representation lan-\nguage, and a prediction task, which allows us to\nmodel the distribution over the occurrence of el-\nements in the vocabulary as a function of their\n(well-deﬁned) context. In difference from syn-\ntactic LMs, we represent natural language at a\nhigher level of semantic abstraction, thus facilitat-\ning modeling deep semantic knowledge.\nWe propose two distinct discourse driven lan-\nguage models to capture semantics. In our ﬁrst se-\nmantic language model, theFrame-Chain SemLM ,\nwe model all semantic frames and discourse mark-\ners in the text. Each document is viewed as a sin-\ngle chain of semantic frames and discourse mark-\ners. Moreover, while the vocabulary of discourse\nmarkers is rather small, the number of different\nsurface form semantic frames that could appear in\nthe text is very large. To achieve a better level of\nabstraction, we disambiguate semantic frames and\nmap them to their PropBank/FrameNet represen-\ntation. Thus, in Ex.3, the resulting frame chain\nis “rob.01 — but — arrest.01” (“01” indicates the\npredicate sense).\nOur second semantic language model is called\nEntity-Centered SemLM. Here, we model a se-\nquence of semantic frames and discourse mark-\ners involved in a speciﬁc co-reference chain. For\neach co-reference chain in a document, we ﬁrst\nextract semantic frames corresponding to each\nco-referent mention, disambiguate them as be-\nfore, and then determine the discourse markers\nbetween these frames. Thus, each unique frame\ncontains both the disambiguated predicate and the\nargument label of the mention. In Ex.3, the re-\nsulting sequence is “rob.01#obj — but — ar-\nrest.01#obj” (here “obj” indicates the argument la-\nbel for “Kevin” and “him” respectively). While\nthese two models capture somewhat different se-\nmantic knowledge, we argue later in the paper that\nboth models can be induced at high quality, and\nthat they are suitable for different NLP tasks.\nFor both models of SemLM, we study four\nlanguage model implementations: N-gram, skip-\ngram (Mikolov et al., 2013b), continuous bag-\nof-words (Mikolov et al., 2013a) and log-bilinear\nlanguage model (Mnih and Hinton, 2007). Each\nmodel deﬁnes its own prediction task. In total, we\nproduce eight different SemLMs. Except for N-\ngram model, others yield embeddings for semantic\nframes as they are neural language models.\nIn our empirical study, we evaluate both the\nquality of all SemLMs and their application to co-\nreference resolution and shallow discourse parsing\ntasks. Following the traditional evaluation stan-\ndard of language models, we ﬁrst use perplexity\nas our metric. We also follow the script learning\nliterature (Chambers and Jurafsky, 2008b; Cham-\nbers and Jurafsky, 2009; Rudinger et al., 2015) and\nevaluate on the narrative cloze test, i.e. randomly\nremoving a token from a sequence and test the sys-\ntem’s ability to recover it. We conduct both eval-\nuations on two test sets: a hold-out dataset from\nthe New York Times Corpus and gold sequence\ndata (for frame-chain SemLMs, we use Prop-\nBank (Kingsbury and Palmer, 2002); for entity-\ncentered SemLMs, we use Ontonotes (Hovy et\nal., 2006) ). By comparing the results on these\ntest sets, we show that we do not incur noticeable\ndegradation when building SemLMs using prepro-\ncessing tools. Moreover, we show that SemLMs\nimproves the performance of co-reference resolu-\ntion, as well as that of predicting the sense of dis-\ncourse connectives for both explicit and implicit\nones.\nThe main contributions of our work can be\nsummarized as follows: 1) The design of two\nnovel discourse driven Semantic Language mod-\nels, building on text abstraction and neural em-\nbeddings; 2) The implementation of high quality\nSemLMs that are shown to improve state-of-the-\nart NLP systems.\n2 Related Work\nOur work is related to script learning. Early\nworks (Schank and Abelson, 1977; Mooney\nand DeJong, 1985) tried to construct knowledge\nbases from documents to learn scripts. Recent\nwork focused on utilizing statistical models to\nextract high-quality scripts from large amounts\nof data (Chambers and Jurafsky, 2008a; Bejan,\n2008; Jans et al., 2012; Pichotta and Mooney,\n2014; Granroth-Wilding et al., 2015; Pichotta and\nMooney, 2016). Other works aimed at learning\na collection of structured events (Chambers, 2013;\nCheung et al., 2013; Cheung et al., 2013; Balasub-\nramanian et al., 2013; Bamman and Smith, 2014;\nNguyen et al., 2015), and several works have\nemployed neural embeddings (Modi and Titov,\n2014b; Modi and Titov, 2014a; Frermann et al.,\n2014; Titov and Khoddam, 2015). Ferraro and\nVan Durme (2016) presented a uniﬁed probabilis-\ntic model of syntactic and semantic frames while\nalso demonstrating improved coherence.\nIn our work, the semantic sequences in the\nentity-centered SemLMs are similar to narrative\nschemas (Chambers and Jurafsky, 2009). How-\never, we differ from them in the following aspects:\n1) script learning does not generate a probabilis-\ntic model on semantic frames1; 2) script learning\nmodels semantic frame sequences incompletely as\nthey do not consider discourse information; 3)\nworks in script learning rarely show applications\nto real NLP tasks.\nSome prior works have used scripts-related\nideas to help improve NLP tasks (Irwin et al.,\n2011; Rahman and Ng, 2011; Peng et al., 2015b).\nHowever, since they use explicit script schemas\neither as features or constraints, these works suf-\nfer from data sparsity problems. In our work, the\n1Some works may utilize a certain probabilistic frame-\nwork, but they mainly focus on generating high-quality\nframes by ﬁltering.\nTable 1:Comparison of vocabularies between\nframe-chain (FC) and entity-centered (EC)\nSemLMs. “F-Sen” stands for frames with pred-\nicate sense information while “F-Arg” stands\nfor frames with argument role label information;\n“Conn” means discourse marker and “Per” means\nperiod. “Seq/Doc” represents the number of se-\nquence per document.\nF-Sen F-Arg Conn Per Seq/Doc\nFC YES NO YES YES Single\nEC YES YES YES NO Multiple\nSemLM abstract vocabulary ensures a good cov-\nerage of frame semantics.\n3 Two Models for SemLM\nIn this section, we describe how we capture se-\nquential semantic information consisted of seman-\ntic frames and discourse markers as semantic units\n(i.e. the vocabulary).\n3.1 Semantic Frames and Discourse Markers\nSemantic Frames A semantic frame is composed\nof a predicate and its corresponding argument par-\nticipants. Here we require the predicate to be dis-\nambiguated to a speciﬁc sense, and we need a cer-\ntain level of abstraction of arguments so that we\ncan assign abstract labels. The design of Prop-\nBank frames (Kingsbury and Palmer, 2002) and\nFrameNet frames (Baker et al., 1998) perfectly ﬁts\nour needs. They both have a limited set of frames\n(in the scale of thousands) and each frame can be\nuniquely represented by its predicate sense. These\nframes provide a good level of generalization as\neach frame can be instantiated into various surface\nforms in natural texts. We use these frames as part\nof our vocabulary for SemLMs. Formally, we use\nthe notation f to represent a frame. Also, we de-\nnote fa≜ f#Arg when referring to an argument\nrole label (Arg) inside a frame (f).\nDiscourse Markers We use discourse markers\n(connectives) to model discourse relationships be-\ntween frames. There is only a limited number of\nunique discourse markers, such asand, but, how-\never, etc. We get the full list from the Penn Dis-\ncourse Treebank (Prasad et al., 2008) and include\nthem as part of our vocabulary for SemLMs. For-\nmally, we usedisto denote the discourse marker.\nNote that discourse relationships can exist with-\nout an explicit discourse marker, which is also a\nchallenge for discourse parsing. Since we cannot\nreliably identify implicit discourse relationships,\nwe only consider explicit ones here. More impor-\ntantly, discourse markers are associated with ar-\nguments (Wellner and Pustejovsky, 2007) in text\n(usually two sentences/clauses, sometimes one).\nWe only add a discourse marker in the semantic\nsequence when its corresponding arguments con-\ntain semantic frames which belong to the same se-\nmantic sequence. We call themframe-related dis-\ncourse markers. Details on generating semantic\nframes and discourse markers to form semantic se-\nquences are discussed in Sec. 5.\n3.2 Frame-Chain SemLM\nFor frame-chain SemLM, we model all seman-\ntic frames and discourse markers in a document.\nWe form the semantic sequence by ﬁrst includ-\ning all semantic frames in the order they appear\nin the text:[f1, f2, f3, . . . ]. Then we add frame-\nrelated discourse markersinto the sequence by\nplacing them in their order of appearance. Thus\nwe get a sequence like[f1, dis1, f2, f3, dis2, . . . ].\nNote that discourse markers do not necessarily\nexist between all semantic frames. Additionally,\nwe treat theperiodsymbol as a special discourse\nmarker, denoted by “o”. As some sentences con-\ntain more than one semantic frame (situations like\nclauses), we get the ﬁnal semantic sequence like\nthis:\n[f1, dis1, f2, o, f3, o, dis2, . . . , o]\n3.3 Entity-Centered SemLM\nWe generate semantic sequences according to\nco-reference chains for entity-centered SemLM.\nFrom co-reference resolution, we can get a se-\nquence like[m 1, m 2, m 3, . . . ], where mentions ap-\npear in the order they occur in the text. Each\nmention can be matched to an argument inside a\nsemantic frame. Thus, we replace each mention\nwith its argument label inside a semantic frame,\nand get[fa1, fa2, fa3, . . . ]. We then add discourse\nmarkers exactly in they way we do for frame-chain\nSemLM, and get the following sequence:\n[fa1, dis1, fa2, fa3, dis2, . . . ]\nThe comparison of vocabularies between\nframe-chain and entity-centered SemLMs is sum-\nmarized in Table 1.\n4 Implementations of SemLM\nIn this work, we experiment with four language\nmodel implementations: N-gram (NG), Skip-\nGram (SG), Continuous Bag-of-Words (CBOW)\nand Log-bilinear (LB) language model. For ease\nof explanation, we assume that a semantic unit se-\nquence iss = [w1, w 2, w 3, . . . , w k].\n4.1 N-gram Model\nFor an n-gram model, we predict each token based\non itsn−1 previous tokens, i.e. we directly model\nthe following conditional probability (in practice,\nwe choosen = 3, Tri-gram (TRI) ):\np(wt+2|wt, w t+1).\nThen, the probability of the sequence is\np(s) =p(w1)p(w2|w1)\nk−2∏\nt=1\np(wt+2|wt, w t+1).\nTo compute p(w2|w1) and p(w1), we need to\nback off from Tri-gram to Bi-gram and Uni-gram.\n4.2 Skip-Gram Model\nThe SG model was proposed in Mikolov et al.\n(2013b). It uses a token to predict its context, i.e.\nwe model the following conditional probability:\np(c ∈ c(wt)|wt, θ ).\nHere,c(wt) is the context forwt and θ denotes the\nlearned parameters which include neural network\nstates and embeddings. Then the probability of the\nsequence is computed as\nk∏\nt=1\n∏\nc∈c(wt)\np(c|wt, θ ).\n4.3 Continuous Bag-of-Words Model\nIn contrast to skip-gram, CBOW (Mikolov et al.,\n2013a) uses context to predict each token, i.e. we\nmodel the following conditional probability:\np(wt|c(wt), θ ).\nIn this case, the probability of the sequence is\nk∏\nt=1\np(wt|c(wt), θ ).\n4.4 Log-bilinear Model\nLB was introduced in Mnih and Hinton (2007).\nSimilar to CBOW, it also uses context to predict\neach token. However, LB associates a token with\nthree components instead of just one vector: a tar-\nget vector v(w), a context vector v’(w) and a bias\nb(w). So, the conditional probability becomes:\np(wt|c(wt)) = exp(v(wt)⊺u(c(wt)) +b(wt))\n∑\nw∈V exp(v(w)⊺u(c(wt)) +b(w)) .\nHere, V denotes the vocabulary and we deﬁne\nu(c(wt)) = ∑\nci∈c(wt) qi ⊙ v′(ci). Note that⊙\nrepresents element-wise multiplication andqi is a\nvector that depends only on the position of a token\nin the context, which is a also a model parameter.\nSo, the overall sequence probability is\nk∏\nt=1\np(wt|c(wt)).\n5 Building SemLMs from Scratch\nIn this section, we explain how we build SemLMs\nfrom un-annotated plain text.\n5.1 Dataset and Preprocessing\nDataset We use the New York Times Corpus2\n(from year 1987 to 2007) for training. It contains\na bit more than 1.8M documents in total.\nPreprocessingWe pre-process all documents with\nsemantic role labeling (Punyakanok et al., 2004)\nand part-of-speech tagger (Roth and Zelenko,\n1998). We also implement the explicit dis-\ncourse connective identiﬁcation module in shal-\nlow discourse parsing (Song et al., 2015). Ad-\nditionally, we utilize within document entity co-\nreference (Peng et al., 2015a) to produce co-\nreference chains. To obtain all annotations, we\nemploy the Illinois NLP tools3.\n5.2 Semantic Unit Generation\nFrameNet Mapping We ﬁrst directly derive se-\nmantic frames from semantic role labeling anno-\ntations. As the Illinois SRL package is built upon\nPropBank frames, we do a mapping to FrameNet\nframes via VerbNet senses (Schuler, 2005), thus\nachieving a higher level of abstraction. The map-\nping ﬁle4 deﬁnes deterministic mappings. How-\never, the mapping is not complete and there are\n2https://catalog.ldc.upenn.edu/LDC2008T19\n3http://cogcomp.cs.illinois.edu/page/software/\n4http://verbs.colorado.edu/verb-index/fn/vn-fn.xml\nremaining PropBank frames. Thus, the generated\nvocabulary for SemLMs contains both PropBank\nand FrameNet frames. For example, “place” and\n“put” with the VerbNet sense id “9.1-2” are con-\nverted to the same FrameNet frame “Placing”.\nAugmenting to Verb Phrases We apply three\nheuristic modiﬁcations to augment semantic\nframes deﬁned in Sec. 3.1: 1) if a preposition\nimmediately follows a predicate, we append the\npreposition to the predicate e.g. “take over”; 2)\nif we encounter the semantic role label AM-PRD\nwhich indicates a secondary predicate, we also ap-\npend this secondary predicate to the main predi-\ncate e.g. “be happy”; 3) if we see the semantic role\nlabel AM-NEG which indicates negation, we ap-\npend “not” to the predicate e.g. “not like”. These\nthree augmentations can co-exist and they allow us\nto model more ﬁne-grained semantic frames.\nVerb Compounds We have observed that if two\npredicates appear very close to each other, e.g.\n“eat and drink”, “decide to buy”, they actually rep-\nresent a uniﬁed semantic meaning. Thus, we con-\nstruct compound verbs to connect them together.\nWe apply the rule that if the gap between two pred-\nicates is less than two tokens, we treat them as\na uniﬁed semantic frame deﬁned by the conjunc-\ntion of the two (augmented) semantic frames, e.g.\n“eat.01-drink.01” and “decide.01-buy.01”.\nArgument Labels for Co-referent MentionsTo\nget the argument role label information for co-\nreferent mentions, we need to match each mention\nto its corresponding semantic role labeling argu-\nment. If a mention head is inside an argument, we\nregard it as a match. We do not consider singleton\nmentions.\nVocabulary ConstructionAfter generating all se-\nmantic units for (augmented and compounded) se-\nmantic frames and discourse markers, we merge\nthem together as a tentative vocabulary. In order\nto generate a sensible SemLM, we ﬁlter out rare\ntokens which appear less than 20 times in the data.\nWe add the Unknown token (UNK) and End-of-\nSequence token (EOS) to the eventual vocabulary.\nStatistics on the eventual SemLM vocabular-\nies and semantic sequences are shown in Table 2.\nWe also compare frame-chain and entity-centered\nSemLMs to the usual syntactic language model\nsetting. The statistics in Table 2 shows that they\nare comparable both in vocabulary size and in the\ntotal number of tokens for training. Moreover,\nentity-centered SemLMs have shorter sequences\nTable 2:Statistics on SemLM vocabularies and\nsequences. “F-s” stands for single frame while\n“F-c” stands for compound frame; “Conn” means\ndiscourse marker. “#seq” is the number of se-\nquences, and “#token” is the total number of to-\nkens (semantic units). We also compute the av-\nerage token in a sequence i.e. “#t/s”. We com-\npare frame-chain (FC) and entity-centered (EC)\nSemLMs to the usual syntactic language model\nsetting i.e. “LM”.\nV ocabulary Size Sequence Size\nF-s F-c Conn #seq #token #t/s\nFC 14857 7269 44 1.2M 25.4M 21\nEC 8758 2896 44 3.4M 18.6M 5\nLM ∼ 20k ∼ 3M ∼ 38M 10-15\nthen frame-chain SemLMs. We also provide sev-\neral examples of high-frequency augmented com-\npound semantic frames in our generated SemLM\nvocabularies. All are very intuitive:\nwant.01-know.01, agree.01-pay.01,\ntry.01-get.01, decline.02-comment.01,\nwait.01-see.01, make.02-feel.01,\nwant.01(not)-give.08(up)\n5.3 Language Model Training\nNG We implement the N-gram model using the\nSRILM toolkit (Stolcke, 2002). We also employ\nthe well-known KneserNey Smoothing (Kneser\nand Ney, 1995) technique.\nSG & CBOW We utilize the word2vec package to\nimplement both SG and CBOW. In practice, we set\nthe context window size to be 10 for SG while set\nthe number as 5 for CBOW (both are usual settings\nfor syntactic language models). We generate 300-\ndimension embeddings for both models.\nLB We use the OxLM toolkit (Paul et al., 2014)\nwith Noise-Constrastive Estimation (Gutmann and\nHyvarinen, 2010) for the LB model. We set\nthe context window size to 5 and produce 150-\ndimension embeddings.\n6 Evaluation\nIn this section, we ﬁrst evaluate the quality of\nSemLMs through perplexity and a narrative cloze\ntest. More importantly, we show that the proposed\nSemLMs can help improve the performance of co-\nreference resolution and shallow discourse pars-\ning. This further proves that we successfully cap-\nture semantic sequence information which can po-\ntentially beneﬁt a wide range of semantic related\nNLP tasks.\nWe have designed two models for SemLM:\nframe-chain(FC ) and entity-centered(EC ). By\ntraining on both types of sequences respectively,\nwe implement four different language models:\nTRI, SG, CBOW, LB . We focus the evaluation\nefforts on these eight SemLMs.\n6.1 Quality Evaluation of SemLMs\nDatasetsWe use three datasets. We ﬁrst randomly\nsample 10% of the New York Times Corpus doc-\numents (roughly two years of data), denoted the\nNYT Hold-out Data. All our SemLMs are trained\non the remaining NYT data and tested on this\nhold-out data. We generate semantic sequences\nfor the training and test data using the methodol-\nogy described in Sec. 5.\nWe use PropBank data with gold frame annota-\ntions as another test set. In this case, we only gen-\nerate frame-chain SemLM sequences by apply-\ning semantic unit generation techniques on gold\nframes, as described in Sec 5.2. When we test on\nGold PropBank Data with Frame Chains, we use\nframe-chain SemLMs trained from all NYT data.\nSimilarly, we use Ontonotes data (Hovy et al.,\n2006) with gold frame and co-reference annota-\ntions as the third test set,Gold Ontonotes Data\nwith Coref Chains. We only generate entity-\ncentered SemLMs by applying semantic unit gen-\neration techniques on gold frames and gold co-\nreference chains, as described in Sec 5.2.\nBaselinesWe use Uni-gram (UNI ) and Bi-gram\n(BG ) as two language model baselines. In ad-\ndition, we use the point-wise mutual informa-\ntion (PMI) for token prediction. Essentially, PMI\nscores each pair of tokens according to their co-\noccurrences. It predicts a token in the sequence by\nchoosing the one with the highest total PMI with\nall other tokens in the sequence. We use the or-\ndered PMI (OP ) as our baseline, which is a vari-\nation of PMI by considering asymmetric count-\ning (Jans et al., 2012).\n6.1.1 Perplexity\nAs SemLMs are language models, it is natural to\nevaluate the perplexity, which is a measurement of\nhow well a language model can predict sequences.\nResults for SemLM perplexities are presented\nin Table 3. They are computed without consider-\ning end token (EOS). We apply tri-gram Kneser-\nNey Smoothing to CBOW, SG and LB. LB con-\nsistently shows the lowest perplexities for both\nframe-chain and entity-centered SemLMs across\nTable 3:Perplexities for SemLMs.UNI, BG,\nTRI, CBOW, SG, LB are different language model\nimplementations while “FC” and “EC” stand for\nthe two SemLM models studied, respectively.\n“FC-FM” and “EC-FM” indicate that we removed\nthe “FrameNet Mapping” step (Sec. 5.2). LB con-\nsistently produces the lowest perplexities for both\nframe-chain and entity-centered SemLMs.\nBaselines SemLMs\nUNI BG TRI CBOW SG LB\nNYT Hold-out Data\nFC\n952.1 178.3 119.2 115.4 114.1 108.5\nEC 914.7 154.4 114.9 111.8 113.8 109.7\nGold PropBank Data with Frame Chains\nFC-FM\n992.9 213.7 139.1 135.6 128.4 121.8\nFC 970.0 191.2 132.7 126.4 123.5 115.4\nGold Ontonotes Data with Coref Chains\nEC-FM\n956.4 187.7 121.1 115.6 117.2 113.7\nEC 923.8 163.2 120.5 113.7 115.0 109.3\nall test sets. Similar to syntactic language mod-\nels, perplexities are fast decreasing from UNI, BI\nto TRI. Also, CBOW and SG have very close per-\nplexity results which indicate that their language\nmodeling abilities are at the same level.\nWe can compare the results of our frame-chain\nSemLM on NYT Hold-out Dataand Gold Prop-\nBank Data with Frame Chains, and our entity-\ncentered SemLM onNYT Hold-out Dataand Gold\nOntonotes Data with Coref Chains. While we see\ndifferences in the results, the gap is narrow and\nthe relative ranking of different SemLMs does not\nchange. This indicates that the automatic SRL and\nCo-reference annotations added some noise but,\nmore importantly, that the resulting SemLMs are\nrobust to this noise as we still retain the language\nmodeling ability for all methods.\nAdditionally, our ablation study removes the\n“FrameNet Mapping” step in Sec. 5.2 (“FC-FM”\nand “EC-FM” rows), resulting in only using Prop-\nBank frames in the vocabulary. The increase in\nperplexities shows that “FrameNet Mapping” does\nproduce a higher level of abstraction, which is use-\nful for language modeling.\n6.1.2 Narrative Cloze Test\nWe follow the Narrative Cloze Test idea used in\nscript learning (Chambers and Jurafsky, 2008b;\nChambers and Jurafsky, 2009). As Rudinger et\nal. (2015) points out, the narrative cloze test can\nbe regarded as a language modeling evaluation. In\nthe narrative cloze test, we randomly choose and\nremove one token from each semantic sequence\nin the test set. We then use language models to\npredict the missing token and evaluate the correct-\nTable 4:Narrative cloze test results for SemLMs.UNI, BG, TRI, CBOW, SG, LB are different lan-\nguage model implementations while “FC” and “EC” stand for our two SemLM models, respectively.\n“FC-FM” and “EC-FM” mean that we remove the FrameNet mappings. “w/o DIS” indicates the removal\nof discourse makers in SemLMs. “Rel-Impr” indicates the relative improvement of the best performing\nSemLM over the strongest baseline. We evaluate on two metrics: mean reciprocal rank (MRR)/recall at\n30 (Recall@30). LB outperforms other methods for both frame-chain and entity-centered SemLMs.\nBaselines SemLMs Rel-ImprOP UNI BG TRI CBOW SG LB\nMRR\nNYT Hold-out Data\nFC\n0.121 0.236 0.225 0.249 0.242 0.247 0.276 8.5%\nEC 0.126 0.235 0.210 0.242 0.249 0.249 0.261 5.9%\nEC w/o DIS 0.092 0.191 0.188 0.212 0.215 0.216 0.227 18.8%\nRudinger et al. (2015)∗ 0.083 0.186 0.181 —– —– —– 0.223 19.9%\nGold PropBank Data with Frame Chains\nFC\n0.106 0.215 0.212 0.232 0.228 0.229 0.254 18.1%\nFC-FM 0.098 0.201 0.204 0.223 0.218 0.220 0.243 ———\nGold Ontonotes Data with Coref Chains\nEC\n0.122 0.228 0.213 0.239 0.247 0.246 0.257 12.7%\nEC-FM 0.109 0.215 0.208 0.230 0.237 0.239 0.254 ———\nRecall@30\nNYT Hold-out Data\nFC\n33.2 46.8 45.3 47.3 46.6 47.5 55.4 18.4%\nEC 29.4 43.7 41.6 44.8 46.5 46.6 52.0 19.0%\nGold PropBank Data with Frame Chains\nFC\n26.3 39.5 38.1 45.5 43.6 43.8 53.9 36.5%\nFC-FM 24.4 37.3 37.3 42.8 41.9 42.1 48.2 ———\nGold Ontonotes Data with Coref Chains\nEC\n30.6 42.1 39.7 46.4 48.3 48.1 51.5 22.3%\nEC-FM 26.6 39.9 37.6 45.4 46.7 46.2 49.8 ———\nness. For all SemLMs, we use the conditional\nprobabilities deﬁned in Sec. 4 to get token predic-\ntions. We also use ordered PMI as an additional\nbaseline. The narrative cloze test is conducted on\nthe same test sets as the perplexity evaluation. We\nuse mean reciprocal rank (MRR) and recall at 30\n(Recall@30) to evaluate.\nResults are provided in Table 4. Consistent with\nthe results in the perplexity evaluation, LB out-\nperforms other methods for both frame-chain and\nentity-centered SemLMs across all test sets. It is\ninteresting to see that UNI performs better than\nBG in this prediction task. This ﬁnding is also\nreﬂected in the results reported in Rudinger et al.\n(2015). Though CBOW and SG have similar per-\nplexity results, SG appears to be stronger in the\nnarrative cloze test. With respect to the strongest\nbaseline (UNI), LB achieves close to 20% rela-\ntive improvement for Recall@30 metric on NYT\nhold-out data. On gold data, the frame-chain\nSemLMs get a relative improvement of 36.5%\nfor Recall@30 while entity-centered SemLMs get\n22.3%. For MRR metric, the relative improvement\nis around half that of the Recall@30 metric.\nIn the narrative cloze test, we also carry out an\nablation study to remove the “FrameNet Mapping”\nstep in Sec. 5.2 (“FC-FM” and “EC-FM” rows).\nThe decrease in MRR and Recall@30 metrics\nfurther strengthens the argument that “FrameNet\nMapping” is important for language modeling as\nit improves the generalization on frames.\nWe cannot directly compare with other re-\nlated works (Rudinger et al., 2015; Pichotta and\nMooney, 2016) because of the differences in data\nand evaluation metrics. Rudinger et al. (2015) also\nuse the NYT portion of the Gigaword corpus, but\nwith Concrete annotations; Pichotta and Mooney\n(2016) use the English Wikipedia as their data, and\nStanford NLP tools for pre-processing while we\nuse the Illinois NLP tools. Consequently, the even-\nTable 5: Co-reference resolution results with\nentity-centered SemLM features.“EC” stands\nfor the entity-centered SemLM. “TRI” is the tri-\ngram model while “LB” is the log-bilinear model.\n“pc” means conditional probability features and\n“em” represents frame embedding features. “w/o\nDIS” indicates the ablation study by removing all\ndiscourse makers for SemLMs. We conduct the\nexperiments by adding SemLM features into the\nbase system. We outperform the state-of-art sys-\ntem (Wiseman et al., 2015), which reports the best\nresults on CoNLL12 dataset. The improvement\nachieved by “EC\nLB (pc +em)” over the base sys-\ntem is statistically signiﬁcant.\nACE04 CoNLL12\nWiseman et al. (2015) —– 63.39\nBase (Peng et al., 2015a) 71.20 63.03\nBase+EC-TRI (pc) 71.31 63.14\nBase+EC-TRI w/o DIS 71.08 62.99\nBase+EC-LB ( pc) 71.71 63.42\nBase+EC-LB ( pc + em) 71.79 63.46\nBase+EC-LB w/o DIS 71.12 63.00\ntual chain statistics are different, which leads to\ndifferent test instances.5 We counter this difﬁculty\nby reporting results on “Gold PropBank Data” and\n“Gold Ontonotes Data”. We hope that these two\ngold annotation datasets can become standard test\nsets. Rudinger et al. (2015) does share a common\nevaluation metric with us: MRR. If we ignore the\ndata difference and make a rough comparison, we\nﬁnd that the absolute values of our results are bet-\nter while Rudinger et al. (2015) have higher rela-\ntive improvement (“Rel-Impr” in Table 4). This\nmeans that 1) the discourse information is very\nlikely to help better model semantics 2) the dis-\ncourse information may boost the baseline (UNI)\nmore than it does for the LB model.\n6.2 Evaluation of SemLM Applications\n6.2.1 Co-reference Resolution\nCo-reference resolution is the task of identifying\nmentions that refer to the same entity. To help im-\nprove its performance, we incorporate SemLM in-\nformation as features into an existing co-reference\nresolution system. We choose the state-of-art Illi-\nnois Co-reference Resolution system (Peng et al.,\n2015a) as our base system. It employs a su-\npervised joint mention detection and co-reference\n5Rudinger et al. (2015) is similar to our entity-centered\nSemLM without discourse information. So, in Table 4, we\nmake a rough comparison between them.\nframework. We add additional features into the\nmention-pair feature set.\nGiven a pair of mentions(m1, m 2) where m1\nappears beforem2, we ﬁrst extract the correspond-\ning semantic frame and the argument role label of\neach mention. We do this by following the proce-\ndures in Sec. 5. Thus, we can get a pair of semantic\nframes with argument information(fa1, fa2). We\nmay also get an additional discourse marker be-\ntween these two frames, e.g.(fa1, dis, fa2). Now,\nwe add the following conditional probability as the\nfeature from SemLMs:\npc = p(fa2|fa1, dis).\nWe also addp2\nc , √\npc and 1/p c as features. To get\nthe value ofpc, we follow the deﬁnitions in Sec. 4,\nand we only use the entity-centered SemLM here\nas its vocabulary covers frames with argument la-\nbels. For the neural language model implementa-\ntions (CBOW, SG and LB), we also include frame\nembeddings as additional features.\nWe evaluate the effect of the added SemLM\nfeatures on two co-reference benchmark datasets:\nACE04 (NIST, 2004) and CoNLL12 (Pradhan et\nal., 2012). We use the standard split of 268 train-\ning documents, 68 development documents, and\n106 testing documents for ACE04 data (Culotta\net al., 2007; Bengtson and Roth, 2008). For\nCoNLL12 data, we follow the train and test doc-\nument split from CoNLL-2012 Shared Task. We\nreport CoNLL A VG for results (average of MUC,\nB 3, and CEAF e metrics), using the v7.0 scorer\nprovided by the CoNLL-2012 Shared Task.\nCo-reference resolution results with entity-\ncentered SemLM features are shown in Table 5.\nTri-grams with conditional probability features\nimprove the performance by a small margin, while\nthe log-bilinear model achieves a 0.4-0.5 F1 points\nimprovement. By employing log-bilinear model\nembeddings, we further improve the numbers and\nwe outperform the best reported results on the\nCoNLL12 dataset (Wiseman et al., 2015).\nIn addition, we carry out ablation studies to re-\nmove all discourse makers during the language\nmodeling process. We re-train our models and\nstudy their effects on the generated features. Ta-\nble 5 (“w/o DIS” rows) shows that without dis-\ncourse information, the SemLM features would\nhurt the overall performance, thus proving the ne-\ncessity of considering discourse for semantic lan-\nguage models.\nTable 6:Shallow discourse parsing results with frame-chain SemLM features.“FC” stands for the\nframe-chain SemLM. “TRI” is the tri-gram model while “LB” isthe log-bilinear model. “pc”, “em”\nare conditional probability and frame embedding features,resp. “w/o DIS” indicates the case where we\nremove all discourse makers for SemLMs. We do the experiments by adding SemLM features to the base\nsystem. The improvement achieved by “FC-LB (pc + em)” over the baseline is statistically signiﬁcant.\nCoNLL16 Test CoNLL16 Blind\nExplicit Implicit Overall Explicit Implicit Overall\nBase (Song et al., 2015) 89.8 35.6 60.4 75.8 31.9 52.3\nBase + FC-TRI (qc) 90.3 35.8 60.7 76.4 32.5 52.9\nBase + FC-TRI w/o DIS 89.2 35.3 60.0 75.5 31.6 52.0\nBase + FC-LB (qc) 90.9 36.2 61.3 76.8 32.9 53.4\nBase + FC-LB (qc + em) 91.1 36.3 61.4 77.3 33.2 53.8\nBase + FC-LB w/o DIS 90.1 35.7 60.6 76.9 33.0 53.5\n6.2.2 Shallow Discourse Parsing\nShallow discourse parsing is the task of identi-\nfying explicit and implicit discourse connectives,\ndetermine their senses and their discourse argu-\nments. In order to show that SemLM can help im-\nprove shallow discourse parsing, we evaluate on\nidentifying the correct sense of discourse connec-\ntives (both explicit and implicit ones).\nWe choose Song et al. (2015), which uses a su-\npervised pipeline approach, as our base system.\nThe system extracts context features for potential\ndiscourse connectives and applies the discourse\nconnective sense classiﬁer. Consider an explicit\nconnective “dis”; we extract the semantic frames\nthat are closest to it (left and right), resulting in the\nsequence [f1, dis, f2] by following the procedures\ndescribed in Sec. 5. We then add the following\nconditional probabilities as features. Compute\nqc = p(dis|f1, f2).\nand, similar to what we do for co-reference resolu-\ntion, we addqc, q 2\nc , √\nqc, 1/q c as conditional prob-\nability features, which can be computed following\nthe deﬁnitions in Sec. 4. We also include frame\nembeddings as additional features. We only use\nframe-chain SemLMs here.\nWe evaluate on CoNLL16 (Xue et al., 2015)\ntest and blind sets, following the train and devel-\nopment document split from the Shared Task, and\nreport F1 using the ofﬁcial shared task scorer.\nTable 6 shows the results for shallow discourse\nparsing with SemLM features. Tri-gram with con-\nditional probability features improve the perfor-\nmance for both explicit and implicit connective\nsense classiﬁers. Log-bilinear model with condi-\ntional probability features achieves even better re-\nsults, and frame embeddings further improve the\nnumbers. SemLMs improve relatively more on ex-\nplicit connectives than on implicit ones.\nWe also show an ablation study in the same\nsetting as we did for co-reference, i.e. removing\ndiscourse information (“w/o DIS” rows). While\nour LB model can still exhibit improvement over\nthe base system, its performance is lower than the\nproposed discourse driven version, which means\nthat discourse information improves the expres-\nsiveness of semantic language models.\n7 Conclusion\nThe paper builds two types of discourse driven se-\nmantic language models with four different lan-\nguage model implementations that make use of\nneural embeddings for semantic frames. We use\nperplexity and a narrative cloze test to prove that\nthe proposed SemLMs have a good level of ab-\nstraction and are of high quality, and then ap-\nply them successfully to the two challenging tasks\nof co-reference resolution and shallow discourse\nparsing, exhibiting improvements over state-of-\nthe-art systems. In future work, we plan to apply\nSemLMs to other semantic related NLP tasks e.g.\nmachine translation and question answering.\nAcknowledgments\nThe authors would like to thank Christos\nChristodoulopoulos and Eric Horn for comments\nthat helped to improve this work. This work is\nsupported by Contract HR0011-15-2-0025 with\nthe US Defense Advanced Research Projects\nAgency (DARPA). Approved for Public Release,\nDistribution Unlimited. The views expressed are\nthose of the authors and do not reﬂect the ofﬁcial\npolicy or position of the Department of Defense or\nthe U.S. Government. This material is also based\nupon work supported by the U.S. Department of\nHomeland Security under Award Number 2009-\nST-061-CCI002-07.\nReferences\nC. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The\nberkeley framenet project. InCOLING/ACL , pages\n86–90.\nN. Balasubramanian, S. Soderland, Mausam, and\nO. Etzioni. 2013. Generating coherent event\nschemas at scale. InEMNLP , pages 1721–1731.\nD. Bamman and N. A. Smith. 2014. Unsupervised\ndiscovery of biographical structure from text.TACL ,\n2:363–376.\nC. A. Bejan. 2008. Unsupervised discovery of event\nscenarios from texts. InFLAIRS Conference, pages\n124–129.\nE. Bengtson and D. Roth. 2008. Understanding the\nvalue of features for coreference resolution. In\nEMNLP .\nN. Chambers and D. Jurafsky. 2008a. Jointly combin-\ning implicit constraints improves temporal ordering.\nInEMNLP .\nN. Chambers and D. Jurafsky. 2008b. Unsupervised\nlearning of narrative event chains. InACL , volume\n94305, pages 789–797.\nN. Chambers and D. Jurafsky. 2009. Unsupervised\nlearning of narrative schemas and their participants.\nInACL , volume 2, pages 602–610.\nN. Chambers. 2013. Event schema induction with a\nprobabilistic entity-driven model. InEMNLP , vol-\nume 13, pages 1797–1807.\nJ. C. K. Cheung, H. Poon, and L. Vanderwende. 2013.\nProbabilistic frame induction.arXiv:1302.4813.\nA. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.\nFirst-order probabilistic models for coreference res-\nolution. InNAACL .\nFrancis Ferraro and Benjamin Van Durme. 2016. A\nuniﬁed bayesian model of scripts, frames and lan-\nguage. InAAAI .\nL. Frermann, I. Titov, and Pinkal. M. 2014. A hierar-\nchical bayesian model for unsupervised induction of\nscript knowledge. InEACL .\nM. Granroth-Wilding, S. Clark, M. T. Llano, R. Hep-\nworth, S. Colton, J. Gow, J. Charnley, N. Lavraˇ c,\nM. ˇZnidarˇ siˇ c, and M. Perovˇ sek. 2015. What hap-\npens next? event prediction using a compositional\nneural network model.\nM. Gutmann and A. Hyvarinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. InAISTATS.\nE. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and\nR. Weischedel. 2006. Ontonotes: The 90% solu-\ntion. InProceedings of HLT/NAACL.\nJ. Irwin, M. Komachi, and Y . Matsumoto. 2011. Nar-\nrative schema as world knowledge for coreference\nresolution. InCoNLL Shared Task, pages 86–92.\nB. Jans, S. Bethard, I. Vuli´ c, and M. F. Moens. 2012.\nSkip n-grams and ranking functions for predicting\nscript events. InEACL , pages 336–344.\nP. Kingsbury and M. Palmer. 2002. From Treebank to\nPropBank. InProceedings of LREC-2002.\nR. Kneser and H. Ney. 1995. Improved backing-off for\nm-gram language modeling. InICASSP .\nT. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.\nEfﬁcient estimation of word representations in vec-\ntor space.arXiv:1301.3781.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. InNAACL .\nA. Mnih and G. Hinton. 2007. Three new graphical\nmodels for statistical language modelling. InICML ,\npages 641–648.\nA. Modi and I. Titov. 2014a. Inducing neural models\nof script knowledge. InCoNLL .\nA. Modi and I. Titov. 2014b. Learning semantic script\nknowledge with event embeddings. InICLR Work-\nshop.\nR. Mooney and G. DeJong. 1985. Learning schemata\nfor natural language processing.\nK.-H. Nguyen, X. Tannier, O. Ferret, and R. Besanc ¸on.\n2015. Generative event schema induction with en-\ntity disambiguation. InACL .\nUS NIST. 2004. The ace evaluation plan.US National\nInstitute for Standards and Technology (NIST).\nB. Paul, B. Phil, and H. Hieu. 2014. Oxlm: A neural\nlanguage modelling framework for machine transla-\ntion.The Prague Bulletin of Mathematical Linguis-\ntics, 102(1):81–92.\nH. Peng, K. Chang, and D. Roth. 2015a. A joint frame-\nwork for coreference resolution and mention head\ndetection. InCoNLL .\nH. Peng, D. Khashabi, and D. Roth. 2015b. Solving\nhard coreference problems. InNAACL .\nK. Pichotta and R. J. Mooney. 2014. Statistical script\nlearning with multi-argument events. InEACL , vol-\nume 14, pages 220–229.\nK. Pichotta and R. J. Mooney. 2016. Learning statis-\ntical scripts with lstm recurrent neural networks. In\nAAAI .\nS. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and\nY . Zhang. 2012. CoNLL-2012 shared task:\nModeling multilingual unrestricted coreference in\nOntoNotes. InCoNLL .\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind Joshi, and Bonnie\nWebber. 2008. The penn discourse treebank 2.0. In\nProceedings of the 6th International Conference on\nLanguage Resources and Evaluation (LREC 2008).\nV . Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.\nSemantic role labeling via integer linear program-\nming inference. InCOLING .\nA. Rahman and V . Ng. 2011. Coreference resolution\nwith world knowledge. InACL .\nD. Roth and D. Zelenko. 1998. Part of speech tagging\nusing a network of linear separators. InCOLING-\nACL .\nR. Rudinger, P. Rastogi, F. Ferraro, and B. Van Durme.\n2015. Script induction as language modeling. In\nEMNLP .\nR. C. Schank and R. P. Abelson. 1977. Scripts, plans,\ngoals, and understanding: An inquiry into human\nknowledge structures. InJMZ .\nK. K. Schuler. 2005. Verbnet: A broad-coverage, com-\nprehensive verb lexicon.\nY . Song, H. Peng, P. Kordjamshidi, M. Sammons, and\nD. Roth. 2015. Improving a pipeline architecture\nfor shallow discourse parsing. InCoNLL Shared\nTask.\nA. Stolcke. 2002. Srilm-an extensible language mod-\neling toolkit. InINTERSPEECH , volume 2002,\npage 2002.\nI. Titov and E. Khoddam. 2015. Unsupervised induc-\ntion of semantic roles within a reconstruction-error\nminimization framework. InNAACL .\nBen Wellner and James Pustejovsky. 2007. Automati-\ncally identifying the arguments of discourse connec-\ntives. InProceedings of the 2007 Joint Conference\nof EMNLP-CoNLL .\nT. Winograd. 1972. Understanding natural language.\nCognitive psychology, 3(1):1–191.\nS. Wiseman, A. M. Rush, S. M. Shieber, and J. Weston.\n2015. Learning anaphoricity and antecedent ranking\nfeatures for coreference resolution. InACL .\nN. Xue, H. T. Ng, S. Pradhan, R. P. C. Bryant, and\nA. T. Rutherford. 2015. The conll-2015 shared task\non shallow discourse parsing. InCoNLL .",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8721064329147339
    },
    {
      "name": "Computer science",
      "score": 0.8710169792175293
    },
    {
      "name": "Natural language processing",
      "score": 0.7867476940155029
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6477870941162109
    },
    {
      "name": "Natural language understanding",
      "score": 0.5345284938812256
    },
    {
      "name": "Parsing",
      "score": 0.4916554093360901
    },
    {
      "name": "Semantic computing",
      "score": 0.4794237017631531
    },
    {
      "name": "Natural language",
      "score": 0.47669216990470886
    },
    {
      "name": "Universal Networking Language",
      "score": 0.4696687161922455
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.452862948179245
    },
    {
      "name": "Abstraction",
      "score": 0.4526689648628235
    },
    {
      "name": "Semantic compression",
      "score": 0.446278840303421
    },
    {
      "name": "Language model",
      "score": 0.4441670775413513
    },
    {
      "name": "Language identification",
      "score": 0.4138745665550232
    },
    {
      "name": "Programming language",
      "score": 0.2548699676990509
    },
    {
      "name": "Semantic technology",
      "score": 0.1687469482421875
    },
    {
      "name": "Semantic Web",
      "score": 0.1517380177974701
    },
    {
      "name": "Comprehension approach",
      "score": 0.13674715161323547
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ],
  "cited_by": 6
}