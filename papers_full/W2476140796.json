{
  "title": "A Neural Knowledge Language Model",
  "url": "https://openalex.org/W2476140796",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2261000268",
      "name": "AhnÔºå Sungjin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221453105",
      "name": "Choi, Heeyoul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4228115921",
      "name": "Parnamaa, Tanel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751323908",
      "name": "Bengio, Yoshua",
      "affiliations": []
    },
    {
      "id": null,
      "name": "P\\\"arnamaa, Tanel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2394527340",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2305610840",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2097286355",
    "https://openalex.org/W2584341106",
    "https://openalex.org/W2337601653",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2127426251",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1644652583",
    "https://openalex.org/W2181504572",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2963380480",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2304113845",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2962790689",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2461583636",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2952235283",
    "https://openalex.org/W580074167",
    "https://openalex.org/W1852412531",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W1529533208",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W1606347560"
  ],
  "abstract": "Current language models have a significant limitation in the ability to encode and decode factual knowledge. This is mainly because they acquire such knowledge from statistical co-occurrences although most of the knowledge words are rarely observed. In this paper, we propose a Neural Knowledge Language Model (NKLM) which combines symbolic knowledge provided by the knowledge graph with the RNN language model. By predicting whether the word to generate has an underlying fact or not, the model can generate such knowledge-related words by copying from the description of the predicted fact. In experiments, we show that the NKLM significantly improves the performance while generating a much smaller number of unknown words.",
  "full_text": "A Neural Knowledge Language Model\nSungjin Ahn1 Heeyoul Choi2 Tanel P¬®arnamaa 3 Yoshua Bengio1 4\nAbstract\nCurrent language models have signiÔ¨Åcant limita-\ntion in the ability to encode and decode factual\nknowledge. This is mainly because they acquire\nsuch knowledge from statistical co-occurrences al-\nthough most of the knowledge words are rarely ob-\nserved. In this paper, we propose a Neural Knowl-\nedge Language Model (NKLM) which combines\nsymbolic knowledge provided by the knowledge\ngraph with the RNN language model. By predict-\ning whether the word to generate has an under-\nlying fact or not, the model can generate such\nknowledge-related words by copying from the\ndescription of the predicted fact. In experiments,\nwe show that the NKLM signiÔ¨Åcantly improves\nthe performance while generating a much smaller\nnumber of unknown words.\n1. Introduction\nKanye West, a famous<unknown>and the husband of\n<unknown>, released his latest album<unknown> in\n<unknown>.\nA core purpose of language is to communicate knowl-\nedge. For human-level language understanding, it is thus of\nprimary importance for a language model to take advantage\nof knowledge. Traditional language models are good at cap-\nturing statistical co-occurrences of entities as long as they\nare observed frequently in the corpus (e.g., words like verbs,\npronouns, and prepositions). However, they are in general\nlimited in their ability in dealing with factual knowledge be-\ncause these are usually represented by named entities such\nas person names, place names, years, etc. (as shown in the\nabove example sentence of Kanye West.)\nTraditional language models have demonstrated to some\nextent the ability to encode and decode fatual knowl-\nedge (Vinyals & Le, 2015; Serban et al., 2015) when trained\n1Universit¬¥e de Montr¬¥eal, Canada 2Handong Global University,\nSouth Korea 3Work done during internship at the Universit ¬¥e de\nMontr¬¥eal, Canada 4CIFAR Senior Fellow. Correspondence to:\nSungjin Ahn <sjn.ahn@gmail.com>.\nwith a very large corpus. However, we claim that simply\nfeeding a larger corpus into a bigger model hardly results in\na good knowledge language model.\nThe primary reason for this is the difÔ¨Åculty in learning\ngood representations for rare and unknown words. This is a\nsigniÔ¨Åcant problem because these words are of our primary\ninterest in knowledge-related applications such as question\nanswering (Iyyer et al., 2014; Weston et al., 2016; Bordes\net al., 2015) and dialogue modeling (Vinyals & Le, 2015;\nSerban et al., 2015). SpeciÔ¨Åcally, in the recurrent neural\nnetwork language model (RNNLM) (Mikolov et al., 2010),\nthe computational complexity is linearly dependent on the\nnumber of vocabulary words. Thus, including all words of\na language is computationally prohibitive. Even if we can\ninclude a very large number of words in the vocabulary,\naccording to Zipf‚Äôs law, a large portion of the words will\nstill be rarely observed in the corpus.\nThe fact that languages and knowledge can change over time\nalso makes it difÔ¨Åcult to simply rely on a large corpus. Me-\ndia produce an endless stream of new knowledge every day\n(e.g., the results of baseball games played yesterday) that\nis even changing over time. Furthermore, a good language\nmodel should exercise some level of reasoning. For ex-\nample, it may be possible to observe many occurrences of\nBarack Obama‚Äôs year of birth and thus able to predict it in a\ncorrelated context. However, one would not expect current\nlanguage models to predict, with a proper reasoning, the\nblank in ‚ÄúBarack Obama‚Äôs age is ‚Äù even if it is only a\nsimple reformulation of the knowledge on the year of birth1.\nIn this paper, we propose a Neural Knowledge Language\nModel (NKLM) as a step towards addressing the limitations\nof traditional language modeling when it comes to exploit-\ning factual knowledge. In particular, we incorporate sym-\nbolic knowledge provided by the knowledge graph (Nickel\net al., 2015) into the RNNLM. This connection makes sense\nparticularly by observing that facts in knowledge graphs\ncome along with textual representations which are mostly\nabout rare words in text corpora.\nIn NKLM, we assume that each word generation is either\n1We do not investigate the reasoning ability in this paper but\nhighlight this example because the explicit representation of facts\nwould help to handle such examples.\narXiv:1608.00318v2  [cs.CL]  2 Mar 2017\nA Neural Knowledge Language Model\nbased on a fact or not. Thus, at each time step, before gen-\nerating a word, we predict whether the word to generate\nhas an underlying fact or not. As a result, our model pro-\nvides predictions over facts in addition to predictions over\nwords. Hence, the previous context information on both\nfacts and words Ô¨Çow through an RNN and provide a richer\ncontext. The NKLM has two ways to generate a word. One\noption is to generate a ‚Äúvocabulary word‚Äù using the vocab-\nulary softmax as is in the RNNLM. The other option is to\ngenerate a ‚Äúknowledge word‚Äù by predicting the position of\na word within the textual representation of the predicted\nfact. This makes it possible to generate words which are\nnot in the predeÔ¨Åned vocabulary and consequently resolves\nthe rare and unknown word problem. The NKLM can also\nimmediately adapt to adding or modifying knowledge be-\ncause the model learns to predict facts, which can easily be\nmodiÔ¨Åed without having to retrain the model.\nThe contributions of the paper are:\n‚Ä¢ To propose the NKLM model to resolve limitations\nof traditional language models in dealing with factual\nknowledge by using the knowledge graph.\n‚Ä¢ To develop a new dataset calledWikiFact which can be\nused in knowledge-related language models by provid-\ning text aligned with facts.\n‚Ä¢ To show that the proposed model signiÔ¨Åcantly im-\nproves the performance and can generate named en-\ntities which in traditional models were treated as un-\nknown words.\n‚Ä¢ To propose new evaluation metrics that resolve the\nproblem of the traditional perplexity metric in dealing\nwith unknown words.\n2. Related Work\nThere have been remarkable recent advances in language\nmodeling research based on neural networks (Bengio\net al., 2003; Mikolov et al., 2010). In particular, the\nRNNLMs are interesting for their ability to take advantage\nof longer-term temporal dependencies without a strong con-\nditional independence assumption. It is especially notewor-\nthy that the RNNLM using the Long Short-Term Memory\n(LSTM) (Hochreiter & Schmidhuber, 1997) has recently\nadvanced to the level of outperforming carefully-tuned tra-\nditional n-gram based language models (Jozefowicz et al.,\n2016).\nThere have been many efforts to speed up the language\nmodels so that they can cover a larger vocabulary. These\nmethods approximate the softmax output using hierarchical\nsoftmax (Morin & Bengio, 2005; Mnih & Hinton, 2009),\nimportance sampling (Jean et al., 2015), noise contrastive\nestimation (Mnih & Teh, 2012), etc. Although helpful to\nmitigate the computational problem, these approaches still\nsuffer from the rare or unknown words problem.\nTo help deal with the rare/unknown word problem, the\npointer networks (Vinyals et al., 2015) have been adopted\nto implement the copy mechanism (Gulcehre et al., 2016;\nGu et al., 2016) and applied to machine translation and text\nsummarization. With this approach, the (unknown) word\nto copy from the context sentence is inferred from neigh-\nboring words. Similarly, Merity et al. (2016) proposed to\ncopy from the context sentences and Lebret et al. (2016)\nfrom Wikipedia infobox. However, because in our case the\ncontext can be very short and often contains no known rele-\nvant words (e.g., person names), we cannot use the existing\napproach directly.\nOur knowledge memory is also related to the recent litera-\nture on neural networks with external memory (Bahdanau\net al., 2014; Weston et al., 2015; Graves et al., 2014). In\nWeston et al. (2015), given simple sentences as facts which\nare stored in the external memory, the question answering\ntask is studied. In fact, the tasks that the knowledge-based\nlanguage model aims to solve (i.e., predict the next word)\ncan be considered as a Ô¨Åll-in-the-blank type of question an-\nswering. The idea of jointly using Wikipedia and knowledge\ngraphs has also been used in the context of enriching word\nembedding (Celikyilmaz et al., 2015; Long et al., 2016).\nContext-dependent (or topic-based) language models have\nbeen studied to better capture long-term dependencies,\nby learning some context representation from the history.\n(Gildea & Hofmann, 1999) modeled the topic as a latent\nvariable and proposed an EM-based approach. In (Mikolov\n& Zweig, 2012), the topic features are learned by latent\nDirichlet allocation (LDA) (Blei et al., 2003).\n3. Model\n3.1. Preliminary\nA topic is associated to topic knowledge and topic descrip-\ntion. Topic knowledge Fis a set of facts {a1,a2,...,a |F|}\non the topic and topic description W is a sequence of words\n(w1,w2,...,w |W|) describing the topic. We can obtain the\ntopic knowledge from a knowledge graph such as Freebase\nand the topic description from Wikipedia. In the corpus, we\nare given pairs of topic knowledge and topic description for\nK topics, i.e., {(Fk,Wk)}K\nk=1. In the following, we omit\nindex kwhen we indicate an arbitrary topic.\nA fact is represented as a triple of subject, relationship, and\nobject which is associated with a textual representation, e.g.,\n(Barack Obama, Married-To, Michelle Obama). Note that\nall facts in a topic knowledge have the same subject entity\nwhich is the topic entity itself.\nA Neural Knowledge Language Model\nWe deÔ¨Åne knowledge words Oa of a fact aas a sequence of\nwords (oa\n1,oa\n2,...,o a\nN) from which we can copy a word to\ngenerate output. We also maintain a global vocabulary V\ncontaining frequent words. Because the words describing\nrelationships (e.g., ‚Äúmarried to‚Äù) are common and thus can\nbe generated via the vocabulary Vnot via copy, we limit\nthe knowledge words of a fact to be the words for the ob-\nject entity (e.g., Oa = (oa\n1=‚ÄúMichelle‚Äù, oa\n2=‚ÄúObama‚Äù). In\naddition, to make it possible to access the subject words\nfrom the knowledge words, we add a special fact, (Topic,\nTopic Itself, Topic), to all topic knowledge.\nWe train the model in a supervised way with labels on facts\nand words. This requires aligning words in the topic descrip-\ntion with their corresponding facts in the topic knowledge.\nSpeciÔ¨Åcally, given Fand W for a topic, we perform simple\nstring matching between the words in W and all the knowl-\nedge words OF= ‚à™a‚ààFOa in such a way to associate fact\nato word wif wappears in knowledge words Oa. As a re-\nsult, from Fand W, we construct a sequence of augmented\nobservations Y = {yt = (wt,at,zt)}t=1:|W|. Here, zt is\na binary variable indicating whether wt is observed in the\nknowledge words or not:\nzt =\n{\n1, if wt ‚ààOat ,\n0, otherwise. (1)\nIn addition, because not all words are associated to a fact\n(e.g., words like, is, a, the, have ), we introduce a special\nfact type, called Not-a-Fact (NaF), and to which assign such\nwords. The following is an example of an augmented obser-\nvation induced from a topic description and knowledge.\nExample. Given a topic on Fred Rogers with topic descrip-\ntion\nW=‚ÄúRogers was born in Latrobe, Pennsylvania in 1928‚Äù\nand topic knowledge F= {a42,a83,a0}where\na42 = (FredRogers, Placeof Birth, LatrobePennsylvania)\na83 = (FredRogers, Yearof Birth, 1928)\na0 = (FredRogers, TopicItself, FredRogers),\nthe augmented observation Y is\nY = {(w=‚ÄúRogers‚Äù,a=0,z=1), (‚Äúwas‚Äù, NaF , 0),\n(‚Äúborn‚Äù, NaF , 0), (‚Äúin‚Äù, NaF , 0), (‚ÄúLatrobe‚Äù, 42, 1),\n(‚ÄúPennsylvania‚Äù, 42, 1), (‚Äúin‚Äù, NaF , 0), (‚Äú1928‚Äù, 83, 1)}.\nDuring inference and training of a topic, we assume that\nthe topic knowledge Fis loaded in the knowledge mem-\nory in a form of a matrix F ‚àà RDa√ó|F| where the i-th\ncolumn is a fact embedding ai ‚ààRDa . The fact embed-\nding is the concatenation of subject, relationship, and object\nembeddings. We obtain these entity embeddings from a pre-\nliminary run of a knowledge graph embedding method such\nas TransE (Bordes et al., 2013). Note that we Ô¨Åx the fact\nembedding during the training. Thus, there is no drift of\nùíô\"\na1\na2\na3\na4\n‚Ä¶\naN\nNaF\nùíò\"\n,\nO1\nO2\nO3\nO4\n‚Ä¶\nON\nTopic Knowledge\nùíò\"\n. ùëß\"\nùíâ\" ùíâ\"ùíÇ\"\nùíå\"\nùíÇ\"34 ùíò\"34. ùíò\"34,\nùíâ\"34\nùíâ\" ùíÜLSTM\ncopy\nfact search\nFigure 1.The NKLM model. The input consisting of a word (either\nwo\nt‚àí1 or wv\nt‚àí1) and a fact (at‚àí1) goes into LSTM. The LSTM‚Äôs\noutput ht together with the knowledge context e generates the\nfact key kt. Using the fact key, the fact embedding at is retrieved\nfrom the topic knowledge memory. Using at and ht, the word\ngeneration source zt is determined, which in turn determines the\nnext word generation source wv\nt or wo\nt . The copied word wo\nt is a\nsymbol taken from the fact description Oat .\nfact embeddings after training and thus the model can deal\nwith new facts at test time; we learn the embedding of the\nTopic Itself.\nFor notation, to denote the vector representation of an object\nof our interest, we use bold lowercase. For example, the\nembedding of a word w is represented by w = W[w]\nwhere WDw√ó|V|is the word embedding matrix, and W[w]\ndenotes the w-th column of W.\n3.2. Inference\nAt each time step, the NKLM performs the following four\nsub-steps:\n1. Using both the word and fact predictions of the previ-\nous time step, make an input to the current time step\nand update the LSTM controller.\n2. Given the output of the LSTM controller, predict a fact\nand extract its corresponding embedding.\n3. With the extracted fact embedding and the state of the\nLSTM controller, make a binary decision to determine\nthe source of word generation.\n4. According to the chosen source, generate a word either\nfrom the global vocabulary or by copying a word from\nthe knowledge words of the selected fact.\nA model diagram is depicted in Fig. 1. In the following, we\ndescribe these steps in more detail.\nA Neural Knowledge Language Model\n3.2.1. I NPUT REPRESENTATION AND LSTM\nCONTROLLER\nAs shown in Fig. 1, the input at time step t is the con-\ncatenation of three embedding vectors corresponding to\na fact at‚àí1, a (global) vocabulary word wv\nt‚àí1 ‚àà V, and\na knowledge word wo\nt‚àí1 ‚àà Oat‚àí1 , respectively. How-\never, because the predicted word comes at a time step\nonly either from the vocabulary or by copying from the\nknowledge words, i.e., wt‚àí1 ‚àà {wv\nt‚àí1,wo\nt‚àí1}, we set\neither wv\nt‚àí1 or wo\nt‚àí1 to a zero vector when it is not the\ngeneration source at the previous step. The resulting in-\nput representation xt = fconcat(at‚àí1,wv\nt‚àí1,wo\nt‚àí1) is then\nfed into the LSTM controller, and obtain the output states\nht = fLSTM(xt,ht‚àí1).\n3.2.2. F ACT EXTRACTION\nWe then predict a relevant factat on which the word wt will\nbe based. Predicting a fact is done in two steps.\nFirst, a fact-key kfact ‚ààRDa is generated by a function\nffactkey(ht,ek) which is in our experiments a multilayer per-\nceptron (MLP) with one hidden layer of ReLU nonlinearity\nand linear outputs. Here, ek ‚ààRDa is the embedding of\nthe topic knowledge which provides information about what\nfacts are currently available in the topic knowledge. This\nwould help the key generator adapt, without retraining, to\nchanges in the topic knowledge such as removal or modiÔ¨Å-\ncation of some facts. Our experiments use mean-pooling to\nobtain ek, but one can also consider using a more sophis-\nticated method such as the soft-attention mechanism (Bah-\ndanau et al., 2014).\nThen, using the generated fact-key kfact, we select a fact by\nkey-value lookup across the knowledge memory F and then\nretrieve its embedding at as follows:\nP(a|ht) = exp(k‚ä§\nfactF[a])‚àë\na‚Ä≤‚ààFexp(k‚ä§\nfactF[a‚Ä≤]), (2)\nat = argmax\na‚ààF\nP(a|ht), (3)\nat = F[at]. (4)\n3.2.3. S ELECTING WORD GENERATION SOURCE\nGiven the context ht and the extracted fact at, the model\ndecides the source for the next word generation: either from\nthe vocabulary Vor from the knowledge words Oat . We\ndeÔ¨Åne the probability of selecting generation-by-copy as:\nÀÜzt = p(zt|ht,at) = sigmoid(fcopy(ht,at)). (5)\nHere, fcopy is an MLP with one ReLU hidden layer and a\nsingle linear output unit.\nWord wt is generated from the source indicated by ÀÜzt as\nAlgorithm 1NKLM inference at time step t\n1: ## Make input\n2: xt = fconcat(at‚àí1,wv\nt‚àí1,wo\nt‚àí1)\n3: ## Update LSTM controller\n4: ht = LSTM(ht‚àí1,xt)\n5: ## Fact prediction and extract embedding\n6: at = argmaxa‚ààFP(a|ht,ek)\n7: at = F[at]\n8: ## Decide word generation source\n9: zt = I[p(zt|ht,at) >0.5]\n10: if zt == 0 then\n11: ## Word generation from vocabulary\n12: wt = wv\nt = argmaxw‚ààVP(w|ht,at)\n13: wo\nt = 0\n14: else\n15: ## Word generation by copy\n16: nt = argmaxn=0:|Oat |‚àí1 P(n|ht,at)\n17: wt = wo\nt = Oat [nt]\n18: wv\nt = 0\n19: end if\nfollows:\nwt =\n{\nwv\nt ‚ààV, if ÀÜzt <0.5,\nwo\nt ‚ààOat , otherwise.\n3.2.4. W ORD GENERATION\nGeneration from Vocabulary Softmax:For vocabulary\nword wv\nt ‚ààV, we follow the usual way of selecting a word\nusing the softmax function:\nP(wv\nt = w|ht,at) = exp(k‚ä§\nvocaW[w])‚àë\nw‚Ä≤‚ààVexp(k‚ä§vocaW[w‚Ä≤]), (6)\nwhere kvoca ‚ààRDw is obtained by fvoca(ht,at) which is an\nMLP with a ReLU hidden layer and Dw linear output units.\nGeneration by Copy from Knowledge Words:To copy\na knowledge word wo\nt ‚ààOat , we Ô¨Årst predict the position\nof the word within the knowledge words and then copy\nthe word on the predicted position. This copy-by-position\nallows us not to rely on the word embeddings by instead\nlearning position embeddings.\nOne reason to use position prediction is that the traditional\ncopy mechanism (Gulcehre et al., 2016; Gu et al., 2016)\nis difÔ¨Åcult to apply to our context because the knowledge\nwords usually consist of only unknown words and/or are\nshort in length. Furthermore, it makes sense when consid-\nering the fact that we mostly need to copy the knowledge\nwords in increasing order from the Ô¨Årst word. For example,\ngiven that the Ô¨Årst symbol o1 = ‚ÄúMichelle‚Äù was used in\nthe previous time step and prior to that other words such\nA Neural Knowledge Language Model\n# topics # toks # uniq toks # facts # entities # relations maxk|Fk| avgk|Fk| maxa|Oa| avga|Oa|\n10K 1.5M 78k 813k 560K 1.5K 1K 79 19 2.15\nTable 1.Statistics of the WikiFacts-FilmActor-v0.1 dataset.\nas ‚ÄúPresident‚Äù and ‚ÄúUS‚Äù were also observed, the model\ncan easily predict that it is time to select the second symbol,\ni.e., o2 = ‚ÄúObama‚Äù.\nMore speciÔ¨Åcally, we Ô¨Årst generate the position key kpos ‚àà\nRDo by a function fposkey(ht,at) which is again an MLP\nwith one hidden layer and linear outputs whose dimension\nis the maximum number of positions, e.g., the maximum\nlength of the knowledge words (e.g., No\nmax = maxa‚àà¬ØF|Oa|\nwhere ¬ØF= ‚à™kFk). Then, the word to copy is chosen by\nP(n|ht,at) = exp(k‚ä§\nposP[n])‚àë\nn‚Ä≤exp(k‚ä§posP[n‚Ä≤]), (7)\nnt = argmax\nn=0:|Oat |‚àí1\nP(n|ht,at), (8)\nwo\nt = Oat [nt], (9)\nwith position n‚Ä≤ running from 0 to |Oat |‚àí 1. Here,\nPDo√óNo\nmax is the matrix of position embeddings of dimen-\nsion Do. Note that No\nmax is typically a much smaller number\n(e.g., 20 in our experiments) than the size of vocabulary,\nand thus the computation for copy is efÔ¨Åcient. The position\nembedding matrix P is learned during training.\nAlthough in our experiments we Ô¨Ånd that the simple posi-\ntion prediction performs well, we note that one could also\nconsider a more advanced encoding such as one based on a\nconvolutional network (Kim, 2014) to model the knowledge\nwords.\nTo compute p(wt|w<t,F), we Ô¨Årst obtain {z<t,a<t}from\n{w<t}and Fusing the augmentation procedure, and per-\nform the above inference process with hard decisions taken\nabout zt and at based on the model‚Äôs predictions. The infer-\nence procedure is summarized in Algorithm 1.\n3.3. Learning\nWe perform supervised learning on the augmented observa-\ntion Y, similarly to Reed & de Freitas (2016). That is, given\nword observations {Yk}K\nk=1 and knowledge {Fk}K\nk=1, our\nobjective is to maximize the log-likelihood of the augmented\nobservation w.r.t the model parameterŒ∏,\nŒ∏‚àó= argmax\nŒ∏\n‚àë\nk\nlog PŒ∏(Yk|Fk). (10)\nBy the chain rule, we can decompose the probability of the\nobservation Yk as\nlog PŒ∏(Yk|Fk) =\n|Yk|‚àë\nt=1\nlog PŒ∏(yk\nt|yk\n1:t‚àí1,Fk). (11)\nThen, after omitting Fk and kfor simplicity, we can rewrite\nthe single step conditional probability as\nPŒ∏(yt|y1:t‚àí1) = PŒ∏(wt,at,zt|ht) (12)\n= PŒ∏(wt|at,zt,ht)PŒ∏(at|ht)PŒ∏(zt|ht).\nWe maximize the above objective using stochastic gradient\noptimization.\n4. Evaluation\nAn obstacle in developing the proposed model is the lack\nof datasets where the text is aligned with facts at the word\nlevel. While the Penn Treebank (PTB) dataset (Marcus\net al., 1993) has been frequently used in language mod-\neling, as pointed by Merity et al. (2016), its limited vo-\ncabulary containing a relatively small amount of named\nentities makes it difÔ¨Åcult to use them for knowledge-related\ntasks where rare words are of primary interest; we would\nhave only a very small amount of words to be associated\nwith facts. As other larger datasets such as in Chelba\net al. (2013) also have problems in licensing or in the for-\nmat of the dataset, we produce the WikiFacts dataset for\nevaluation of the proposed model and the baseline model.\nThe dataset is freely available in https://bitbucket.\norg/skaasj/wikifact_filmactor.\n4.1. The WikiFacts Dataset\nIn WikiFacts, we align Wikipedia descriptions with corre-\nsponding Freebase2 facts. Because many Freebase topics\nprovide a link to its corresponding topic in Wikipedia, we\nchoose a set of topics for which both a Freebase entity and\na Wikipedia description exist. In the experiments, we used\na version called WikiFacts-FilmActor-v0.1 where\nthe domain is restricted to the /Film/Actor in Freebase.\nWe used the summary part (Ô¨Årst few paragraphs) of the\nWikipedia page as the text to be modeled, but discarded\ntopics for which the number of facts is too large (>1000) or\nthe Wikipedia description is too short (<3 sentences). For\nthe string matching, we also used synonyms and alias infor-\nmation provided by WordNet (Miller, 1995) and Freebase.\nWe augmented the fact setFwith the anchor facts Awhose\nrelationship is all set to UnknownRelation. That is, ob-\nserving that an anchor (a word under a hyperlink) in a\nWikipedia description has a corresponding Freebase entity\nas well as being semantically closely related to the topic\n2Freebase has migrated to Wikidata. www.wikidata.org\nA Neural Knowledge Language Model\nValidation Test\nModel PPL UPP UPP-f PPL UPP UPP-f # UNK\nRNNLM 39.4 97.9 56.8 39.4 107.0 58.4 23247\nNKLM 27.5 45.4 33.5 28.0 48.7 34.6 12523\nno-copy 38.4 93.5 54.9 38.3 102.1 56.4 29756\nno-fact-no-copy 40.5 98.8 58.0 40.3 107.4 59.3 32671\nno-TransE 48.9 80.7 59.6 49.3 85.8 61.0 13903\nTable 2.We compare four different versions of the NKLM to the RNNLM on three different perplexity metrics.We used 10K\nvocabulary. In no-copy, we disabled the generation-by-copy functionality, and in no-fact-no-copy, using topic knowledge is also\nadditionally disabled by setting all facts as NaF. Thus, no-fact-no-copy is very similar to RNNLM. In no-TransE, we used random\nvectors instead of the TransE embeddings to initialize the knowledge graph entities. As shown, the NKLM shows best performance in all\ncases. The no-fact-no-copy performs similar to the RNNLM as expected (slightly worse partly because it has a smaller number of model\nparameters than that of the RNNLM). As expected, no-copy performs better than no-fact-no-copy by using additional information from\nthe fact embedding, but without the copy mechanism. In the comparison of the NKLM and no-copy, we can see the signiÔ¨Åcant gain of\nusing the copy mechanism to predict named entities. In the last column, we can also see that, with the copy mechanism, the number of\npredicting unknown decreases signiÔ¨Åcantly. Lastly, we can see that the TransE embedding is important.\nValidation Test\nModel PPL UPP UPP-f PPL UPP UPP-f # UNK\nNKLM 5k 22.8 48.5 30.7 23.2 52.0 31.7 19557\nRNNLM 5k 27.4 108.5 47.6 27.5 118.3 48.9 34994\nNKLM 10k 27.5 45.4 33.5 28.0 48.7 34.6 12523\nRNNLM 10k 39.4 97.9 56.8 39.4 107.0 58.4 23247\nNKLM 20k 33.4 45.9 37.9 34.7 49.2 39.7 9677\nRNNLM 20k 57.9 99.5 72.1 59.3 108.3 75.5 13773\nNKLM 40k 41.4 49.0 44.4 43.6 52.7 47.1 5809\nRNNLM 40k 82.4 107.9 92.3 86.4 116.9 97.9 9009\nTable 3.The NKLM and the RNNLM are compared for vocabularies of four different sizes [5K, 10K, 20K, 40K]. As shown, in all\ncases the NKLM signiÔ¨Åcantly outperforms the RNNLM. Interestingly, for the standard perplexity (PPL), the gap between the two models\nincreases as the vocabulary size increases while for UPP the gap stays at a similar level regardless of the vocabulary size. This tells us that\nthe standard perplexity is signiÔ¨Åcantly affected by the UNK predictions, because with UPP the contribution of UNK predictions to the\ntotal perplexity is very small. Also, from the UPP value for the RNNLM, we can see that it initially improves when vocabulary size is\nincreased as it can cover more words, but decreases back when the vocabulary size is largest (40K) because the rare words are added last\nto the vocabulary.\nin which the anchor is found, we make a synthetic fact of\nthe form (Topic, UnknownRelation, Anchor). This po-\ntentially compensates for some missing facts in Freebase.\nBecause we extract the anchor facts from the full Wikipedia\npage and they all share the same relation, it is more chal-\nlenging for the model to use these anchor facts than using\nthe Freebase facts.\nAs a result, for each word win the description, we obtain a\ntuple (w,z,a,n,k ). Here, wis word id, zthe copy indicator,\nafact id, nthe position to copy from Oa if z = 1, and k\ntopic id. We provide a summary of the dataset statistics in\nTable 1.\n4.2. Experiments\n4.2.1. S ETUP\nWe split the dataset into 80/10/10 for train, validation, and\ntest. As a baseline model, we use the RNNLM. For both the\nNKLM and the RNNLM, two-layer LSTMs with dropout\nregularization (Zaremba et al., 2014) are used. We tested\nmodels with different numbers of LSTM hidden units [200,\n500, 1000], and report results from the 1000 hidden-unit\nmodel. For the NKLM, we set the symbol embedding di-\nmension to 40 and word embedding dimension to 400. Un-\nder this setting, the number of parameters in the NKLM is\nslightly smaller than that of the RNNLM.\nWe used 100-dimension TransE embeddings for Freebase\nentities and relations, and concatenate the relation and ob-\nject embeddings to obtain fact embeddings. We averaged\nall fact embeddings in Fk to obtain the topic knowledge em-\nA Neural Knowledge Language Model\nWarm-up Louise Allbritton ( 3 july<unk>february 1979 ) was\nRNNLM a <unk><unk>who was born in<unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>\nNKLM an english [Actor]. he was born in [Oklahoma] , and died in [Oklahoma]. he was married to [Charles] [Collingwood]\nWarm-up Issa Serge Coelo ( born 1967 ) is a<unk>\nRNNLM actor . he is best known for his role as<unk><unk>in the television series<unk>. he also\nNKLM [Film] director . he is best known for his role as the<unk><unk>in the Ô¨Ålm [Un] [taxi] [pour] [Aouzou]\nWarm-up Adam wade Gontier is a canadian Musician and Songwriter .\nRNNLM she is best known for her role as<unk><unk>on the television series<unk>. she has also appeared\nNKLM he is best known for his work with the band [Three] [Days] [Grace] . he is the founder of the\nWarm-up Rory Calhoun ( august 8 , 1922 april 28\nRNNLM , 2010 ) was a<unk>actress . she was born in<unk>, <unk>, <unk>. she was\nNKLM , 2008 ) was an american [Actor] . he was born in [Los] [Angeles] california . he was born in\nTable 4.Sampled Descriptions. Given the warm-up phrases, we generate samples from the NKLM and the RNNLM. We denote the\ncopied knowledge words by [word] and the UNK words by <unk>. Overall, the RNNLM generates many UNKs (we used 10K\nvocabulary) while the NKLM is capable to generate named entities even if the model has not seen some of the words at all during\ntraining. In the Ô¨Årst case, we found that the generated symbols (words in []) conform to the facts of the topic (Louise Allbritton) except\nthat she actually died in Mexico, not in Oklahoma. (We found that the place of death fact was missing.) While she is an actress, the\nmodel generated a word [Actor]. This is because in Freebase, there exists only /profession/actor but no /profession/actress. It is also\nnoteworthy that the NKLM fails to use the gender information provided by facts; the NKLM uses ‚Äúhe‚Äù instead of ‚Äúshe‚Äù although the fact\n/gender/female is available. From this, we see that if a fact is not detected (i.e., NaF), the statistical co-occurrence governs the information\nÔ¨Çow. Similarly, in other samples, the NKLM generates movie titles (Un Taxi Pour Aouzou), band name (Three Days Grace), and place\nof birth (Los Angeles). In addition, to see the NKLM‚Äôs ability to adapt to knowledge updates without retraining, we changed the fact\n/place of birth/Oklahoma to /place of birth/Chicago and found that the NKLM replaces ‚ÄúOklahoma‚Äù by ‚ÄúChicago‚Äù while keeping other\nwords the same.\nbedding ek. We unrolled the LSTMs for 30 steps and used\nminibatch size 20. We trained the models using stochastic\ngradient ascent with gradient clipping range [-5,5]. The\ninitial learning rate was set to 0.5 for the NKLM and 1.5 for\nthe RNNLM, and decayed after every epoch by a factor of\n0.98. We trained for 50 epochs and report the results chosen\nby the best validation set results.\n4.2.2. T HE UNKNOWN PENALIZED PERPLEXITY\nThe perplexity exp(‚àí1\nN\n‚àëN\ni=1 log p(wi)) is the standard\nperformance metric for language modeling. This, however,\nhas a problem in evaluating language models for a corpus\ncontaining many named entities: a model can get good\nperplexity by accurately predicting UNK words as the UNK\nclass. As an extreme example, when all words in a sentence\nare unknown words, a model predicting everything as UNK\nwill get a good perplexity. Considering that unknown words\nprovide virtually no useful information, this is clearly a\nproblem in tasks where named entities are important such\nas question answering, dialogue modeling, and knowledge\nlanguage modeling.\nTo this end, we propose a new evaluation metric, called\nthe Unknown-Penalized Perplexity (UPP), and evaluate the\nmodels on this metric as well as the standard perplexity\n(PPL). Because the actual word underlying the UNK should\nbe one of the out-of-vocabulary (OOV) words, in UPP we\npenalize the likelihood of unknown words as follows:\nPUPP(wunk) = P(wunk)\n|Vtotal \\Vvoca|.\nHere, Vtotal is a set of all unique words in the corpus, and\nVvoca ‚äÇVtotal is the global vocabulary used for word genera-\ntion. In other words, in UPP we assume that the OOV set is\nequal to Vtotal \\Vvoca and thus assign a uniform probability\nto OOV words. In another version, UPP-fact, we consider\nthe fact that the RNNLM can also use the knowledge given\nto the NKLM to some extent, but with limited capability\n(because the model is not designed for it). For this, we\nassume that the OOV set is equal to the total knowledge\nwords of a topic k, i.e.,\nPUPP-fact(wunk) = P(wunk)\n|OFk | ,\nwhere OFk = ‚à™a‚ààFk Oa. In other words, by using UPP-\nfact, we assume that, for an unknown word, the RNNLM can\npick one of the knowledge words with uniform probability.\n4.2.3. O BSERVATIONS FROM EXPERIMENT RESULTS\nWe describe the detail results and analysis on the experi-\nments in detail in the captions of Table 2, 3, and 4. Our\nobservations from the experiment results are as follows.\n‚Ä¢ The NKLM outperforms the RNNLM in all three per-\nplexity measures.\nA Neural Knowledge Language Model\n‚Ä¢ The copy mechanism is the key of the signiÔ¨Åcant per-\nformance improvement. Without the copy mechanism,\nthe NKLM still performs better than the RNNLM due\nto its usage of the fact information, but the improve-\nment is not so signiÔ¨Åcant.\n‚Ä¢ The NKLM results in a much smaller number of UNKs\n(roughly, a half of the RNNLM).\n‚Ä¢ When no knowledge is available, the NKLM performs\nas well as the RNNLM.\n‚Ä¢ Knowledge graph embedding using TransE is an efÔ¨Å-\ncient way of representing facts in our model.\n‚Ä¢ The NKLM generates named entities in the provided\nfacts whereas the RNNLM generates many more\nUNKs.\n‚Ä¢ The NKLM shows its ability to adapt immediately to\nthe change of the knowledge.\n‚Ä¢ The standard perplexity is signiÔ¨Åcantly affected by the\nprediction accuracy on the unknown words. Thus, one\nneed carefully consider when using it as a metric for\nknowledge-related language models.\n5. Conclusion\nIn this paper, we presented a novel Neural Knowledge Lan-\nguage Model (NKLM) that brings the symbolic knowledge\nfrom a knowledge graph into the expressive power of RNN\nlanguage models. The NKLM signiÔ¨Åcantly outperforms the\nRNNLM in terms of perplexity and generates named entities\nwhich are not observed during training, as well as immedi-\nately adapting to changes in knowledge. We believe that the\nWikiFact dataset introduced in this paper, can be useful in\nother knowledge-related language tasks as well. In addition,\nthe Unknown-Penalized Perplexity introduced in order to\nresolve the limitation of the standard perplexity, can also be\nuseful in evaluating other language tasks.\nThe task that we investigated in this paper is limited in\nthe sense that we assume that the true topic of a given\ndescription is known. Relaxing this assumption by making\nthe model search for a proper topic on-the-Ô¨Çy will make\nthe model more practical and scalable. We believe that\nthere are many more open research challenges related to the\nknowledge language models.\nAcknowledgments\nThe authors would like to thank Alberto Garc ¬¥ƒ±a-Dur¬¥an,\nCaglar Gulcehre, Chinnadhurai Sankar, Iulian Serban,\nSarath Chandar, and Peter Clark for helpful feedbacks and\ndiscussions as well as the developers of Theano (Bastien\net al., 2012), NSERC, CIFAR, Facebook, Google, IBM, Mi-\ncrosoft, Samsung, and Canada Research Chairs for funding,\nand Compute Canada for computing resources.\nReferences\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua.\nNeural machine translation by jointly learning to align\nand translate. arXiv preprint arXiv:1409.0473, 2014.\nBastien, Fr ¬¥ed¬¥eric, Lamblin, Pascal, Pascanu, Razvan,\nBergstra, James, Goodfellow, Ian, Bergeron, Arnaud,\nBouchard, Nicolas, Warde-Farley, David, and Bengio,\nYoshua. Theano: new features and speed improvements.\narXiv preprint arXiv:1211.5590, 2012.\nBengio, Yoshua, Ducharme, R¬¥ejean, Vincent, Pascal, and\nJauvin, Christian. A neural probabilistic language model.\nIn Journal of Machine Learning Research, 2003.\nBlei, David M, Ng, Andrew Y , and Jordan, Michael I. Latent\ndirichlet allocation. the Journal of machine Learning\nresearch, 3:993‚Äì1022, 2003.\nBordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto,\nWeston, Jason, and Yakhnenko, Oksana. Translating\nembeddings for modeling multi-relational data. In Ad-\nvances in Neural Information Processing Systems , pp.\n2787‚Äì2795, 2013.\nBordes, Antoine, Usunier, Nicolas, Chopra, Sumit, and\nWeston, Jason. Large-scale simple question answering\nwith memory networks. arXiv preprint arXiv:1506.02075,\n2015.\nCelikyilmaz, Asli, Hakkani-Tur, Dilek, Pasupat, Panupong,\nand Sarikaya, Ruhi. Enriching word embeddings using\nknowledge graph for semantic tagging in conversational\ndialog systems. In 2015 AAAI Spring Symposium Series,\n2015.\nChelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge,\nQi, Brants, Thorsten, Koehn, Phillipp, and Robinson,\nTony. One billion word benchmark for measuring\nprogress in statistical language modeling. arXiv preprint\narXiv:1312.3005, 2013.\nGildea, Daniel and Hofmann, Thomas. Topic-based lan-\nguage models using em. EuroSpeech 1999, 1999.\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural\nturing machines. arXiv preprint arXiv:1410.5401, 2014.\nGu, Jiatao, Lu, Zhengdong, Li, Hang, and Li, Victor\nO. K. Incorporating copying mechanism in sequence-\nto-sequence learning. CoRR, abs/1603.06393, 2016.\nGulcehre, Caglar, Ahn, Sungjin, Nallapati, Ramesh, Zhou,\nBowen, and Bengio, Yoshua. Pointing the unknown\nwords. ACL 2016, 2016.\nHochreiter, Sepp and Schmidhuber, J¬®urgen. Long short-term\nmemory. Neural computation, 9(8):1735‚Äì1780, 1997.\nA Neural Knowledge Language Model\nIyyer, Mohit, Boyd-Graber, Jordan L, Claudino, Leonardo\nMax Batista, Socher, Richard, and Daum ¬¥e III, Hal. A\nneural network for factoid question answering over para-\ngraphs. In EMNLP 2014, pp. 633‚Äì644, 2014.\nJean, Sebastien, Cho, Kyunghyun, Memisevic, Roland, and\nBengio, Yoshua. On using very large target vocabulary\nfor neural machine translation. ACL 2015, 2015.\nJozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer,\nNoam, and Wu, Yonghui. Exploring the limits of lan-\nguage modeling. arXiv preprint arXiv:1602.02410, 2016.\nKim, Yoon. Convolutional neural networks for sentence\nclassiÔ¨Åcation. EMNLP 2014, 2014.\nLebret, R¬¥emi, Grangier, David, and Auli, Michael. Neural\ntext generation from structured data with application to\nthe biography domain. arXiv preprint arXiv:1603.07771,\n2016.\nLong, Teng, Lowe, Ryan, Cheung, Jackie Chi Kit, and\nPrecup, Doina. Leveraging lexical resources for learning\nentity embeddings in multi-relational data. 2016.\nMarcus, Mitchell P, Marcinkiewicz, Mary Ann, and San-\ntorini, Beatrice. Building a large annotated corpus of\nenglish: The penn treebank. Computational linguistics,\n19(2):313‚Äì330, 1993.\nMerity, Stephen, Xiong, Caiming, Bradbury, James, and\nSocher, Richard. Pointer sentinel mixture models. arXiv\npreprint arXiv:1609.07843, 2016.\nMikolov, Tomas and Zweig, Geoffrey. Context dependent\nrecurrent neural network language model. In Spoken\nLanguage Technology Workshop (SLT), 2012 IEEE, pp.\n234‚Äì239. IEEE, 2012.\nMikolov, Tomas, KaraÔ¨Å¬¥at, Martin, Burget, Lukas, Cernock`y,\nJan, and Khudanpur, Sanjeev. Recurrent neural network\nbased language model. In INTERSPEECH 2010, vol-\nume 2, pp. 3, 2010.\nMiller, George A. Wordnet: a lexical database for english.\nCommunications of the ACM, 38(11):39‚Äì41, 1995.\nMnih, Andriy and Hinton, Geoffrey E. A scalable hierar-\nchical distributed language model. In Advances in neural\ninformation processing systems, pp. 1081‚Äì1088, 2009.\nMnih, Andriy and Teh, Yee Whye. A fast and simple algo-\nrithm for training neural probabilistic language models.\nICML 2012, 2012.\nMorin, Frederic and Bengio, Yoshua. Hierarchical proba-\nbilistic neural network language model. AISTATS 2005,\npp. 246, 2005.\nNickel, Maximilian, Murphy, Kevin, Tresp, V olker, and\nGabrilovich, Evgeniy. A review of relational machine\nlearning for knowledge graphs: From multi-relational link\nprediction to automated knowledge graph construction.\narXiv preprint arXiv:1503.00759, 2015.\nReed, Scott and de Freitas, Nando. Neural programmer-\ninterpreters. ICLR 2016, 2016.\nSerban, Iulian V , Sordoni, Alessandro, Bengio, Yoshua,\nCourville, Aaron, and Pineau, Joelle. Building end-to-\nend dialogue systems using generative hierarchical neural\nnetworks. 30th AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence, 2015.\nVinyals, Oriol and Le, Quoc. A neural conversational model.\narXiv preprint arXiv:1506.05869, 2015.\nVinyals, Oriol, Fortunato, Meire, and Jaitly, Navdeep.\nPointer networks. NIPS 2015, 2015.\nWeston, Jason, Chopra, Sumit, and Bordes, Antoine. Mem-\nory networks. ICLR 2015, 2015.\nWeston, Jason, Bordes, Antoine, Chopra, Sumit, and\nMikolov, Tomas. Towards ai-complete question answer-\ning: A set of prerequisite toy tasks. ICLR 2016, 2016.\nZaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.\nRecurrent neural network regularization. arXiv preprint\narXiv:1409.2329, 2014.\nA Neural Knowledge Language Model\nAPPENDIX : H EATMAPS\np_copy_fact\n,\n2008\n)\nwas\nan\namerican\nActor\n.\nhe\nwas\nborn\nin\nLos\nAngeles\ncalifornia\n.\n<naf>\nprofession-Screenwriter\nprofession-Actor\nplace_of_birth-Los Angeles\nprofession-Film Producer\ntopic_itself-Rory Calhoun\nunk_rel-Spellbound\nunk_rel-California\nunk_rel-Santa Cruz\nFigure 2.This is a heatmap of an example sentence generated by the NKLM having a warmup ‚ÄúRory Calhoun ( august 8 , 1922 april\n28‚Äù. The Ô¨Årst row shows the probability of selecting copy (Equation 5 in Section 3.1). The bottom heat map shows the state of the\ntopic-memory at each time step (Equation 2 in Section 3.1). In particular, this topic has 8 facts and an additional <NaF> fact. For the\nÔ¨Årst six time steps, the model retrieves <NaF>from the knowledge memory, copy-switch is off and the words are generated from the\ngeneral vocabulary. For the next time step, the model gives higher probability to three different profession facts: ‚ÄúScreenwriter‚Äù, ‚ÄúActor‚Äù\nand ‚ÄúFilm Producer.‚Äù The fact ‚ÄúActor‚Äù has the highest probability, copy-switch is higher than 0.5, and therefore ‚ÄúActor‚Äù is copied as the\nnext word. Moreover, we see that the model correctly retrieves the place of birth fact and outputs ‚ÄúLos Angeles.‚Äù After that, the model\nstill predicts the place of birth fact, but copy-switch decides that the next word should come from the general vocabulary, and outputs\n‚ÄúCalifornia.‚Äù\np_copy_fact\nan\nenglish\nActor\n.\nhe\nwas\nborn\nin\nOklahoma\n,\nand\ndied\nin\nOklahoma\n.\nhe\nwas\nmarried\nto\nCharles\nCollingwood\n.\n<naf>\neducation.institution-University of Oklahoma\nperformance.film-Son of Dracula\nlocation.people_born_here-Oklahoma City\nperformance.film-The Egg and I\nmarriage.type_of_union-Marriage\nmarriage.spouse-Charles Collingwood\nprofession-Actor\ntopic_itself-Louise Allbritton\nunk_rel-Universal Studios\nunk_rel-Pasadena Playhouse\nunk_rel-Pittsburgh\nunk_rel-Sitting Pretty\nunk_rel-Hollywood\nunk_rel-World War II\nunk_rel-United Service Organizations\nFigure 3.This is an example sentence generated by the NKLM having a warmup ‚ÄúLouise Allbritton ( 3 july <unk>february 1979 )\nwas‚Äù. We see that the model correctly retrieves and outputs the profession (‚ÄúActor‚Äù), place of birth (‚ÄúOklahoma‚Äù), and spouse (‚ÄúCharles\nCollingwood‚Äù) facts. However, the model makes a mistake by retrieving the place of birth fact in a place where the place of death fact is\nsupposed to be used. This is probably because the place of death fact is missing in this topic memory and then the model searches for a\nfact about location, which is somewhat encoded in the place of birth fact. In addition, Louise Allbritton was a woman, but the model\ngenerates a male profession ‚ÄúActor‚Äù and male pronoun ‚Äúhe‚Äù. The ‚ÄúActor‚Äù is generated because there is no ‚ÄúActress‚Äù representation in\nFreebase.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5638670325279236
    },
    {
      "name": "Natural language processing",
      "score": 0.485202819108963
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3953224718570709
    }
  ],
  "institutions": []
}