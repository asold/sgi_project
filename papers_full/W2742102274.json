{
  "title": "Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models",
  "url": "https://openalex.org/W2742102274",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A1840793008",
      "name": "Nan Jiang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2142197324",
      "name": "Wenge Rong",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2114719232",
      "name": "Min Gao",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2105902150",
      "name": "Yikang Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128879397",
      "name": "Zhang Xiong",
      "affiliations": [
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1573488949",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W6863994431",
    "https://openalex.org/W2217098601",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2168148636",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2175585630",
    "https://openalex.org/W6790825729",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W1499166245",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W6601546654",
    "https://openalex.org/W6697000176",
    "https://openalex.org/W2158049734",
    "https://openalex.org/W2962819663",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W36903255",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W2296167893",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W2950075229",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2963084471",
    "https://openalex.org/W2950133940"
  ],
  "abstract": "Recently, variants of neural networks for computational linguistics have been proposed and successfully applied to neural language modeling and neural machine translation. These neural models can leverage knowledge from massive corpora but they are extremely slow as they predict candidate words from a large vocabulary during training and inference. As an alternative to gradient approximation and softmax with class decomposition, we explore the tree-based hierarchical softmax method and reform its architecture, making it compatible with modern GPUs and introducing a compact tree-based loss function. When combined with several word hierarchical clustering algorithms, improved performance is achieved in language modelling task with intrinsic evaluation criterions on PTB, WikiText-2 and WikiText-103 datasets.",
  "full_text": "Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models\nNan Jiangyz, Wenge Rongyz, Min Gaox, Yikang Shen], Zhang Xiongyz\ny State Key Laboratory of Software Development Environment, Beihang University, China\nz School of Computer Science and Engineering, Beihang University, China\nx School of Software Engineering, Chongqing University, China\n] Montr´eal Institute for Learning Algorithms, Universt´e de Montr´eal, Canada\nfnanjiang, w.rong, xiongzg@buaa.edu.cn, gaomin@cqu.edu.cn, yi-kang.shen@umontreal.ca\nAbstract\nRecently, variants of neural networks for computa-\ntional linguistics have been proposed and success-\nfully applied to neural language modeling and neu-\nral machine translation. These neural models can\nleverage knowledge from massive corpora but they\nare extremely slow as they predict candidate word-\ns from a large vocabulary during training and in-\nference. As an alternative to gradient approxima-\ntion and softmax with class decomposition, we ex-\nplore the tree-based hierarchical softmax method\nand reform its architecture, making it compatible\nwith modern GPUs and introducing a compact tree-\nbased loss function. When combined with sev-\neral word hierarchical clustering algorithms, im-\nproved performance is achieved in language mod-\nelling task with intrinsic evaluation criterions on\nPTB, WikiText-2 and WikiText-103 datasets.\n1 Introduction\nLanguage modeling is a basic and fundamental task in Com-\nputational Linguistics (CL), and have attracted much academ-\nic and commercial attention for the last few decades. The use\nof Language Model (LM) is universal in applications such as\nNeural Machine Translation (NMT) [Jean et al., 2015 ] and\nAutomatic Speech Recognition [Wang and Wang, 2016].\nSpeciﬁcally, language model is composed of two parts:\ncontext representation and next-step word prediction. The\ncontext knowledge is generally represented by several exist-\ning neural-based architectures, e.g. Feedforward Neural Net-\nwork [Bengio et al., 2003], Recurrent Neural Network (RN-\nN) [Mikolov et al., 2010 ], and Character-level Convolution\nNeural Network (CharCNN) [Kim et al., 2016]. Besides, the\nnext-step word prediction, usually represented by softmax al-\ngorithm, involves computing the probability of a true word in\nthe whole vocabulary.\nNonetheless, intensive computations are involved in lan-\nguage model with modern neural network during training and\ninference. Particularly, computing the softmax probability\nnormalisation function exploits great portion of time, and this\nissue has become a bottleneck for all variations and applica-\ntions as the practical speed decreases dramatically with the\nvocabulary grows [Chen et al., 2016].\nTherefore, various studies have attempted to address this\nchallenge in literature. An intuitive method was vocabu-\nlary truncation, which relies on maintaining a shortlist of the\nmost frequently word-level or subword-level units, thereby\nincreases the speed of normalisation and expectation based\non the truncated vocabulary [Schwenk, 2007; Sennrich et al.,\n2016]. Subsequently, several other workarounds were pro-\nposed, which can be roughly divided into sampling-based ap-\nproximations and factored output layer methods.\nSampling-based methods avoid intensive computation by\ncomputing only a tiny fraction of the vocabulary. E.g., Noise\nContrastive Estimation (NCE) regards multi-class identiﬁ-\ncation tasks as psudo logistic classiﬁcation by classifying\nthe empirical word distribution from noise words’ distribu-\ntion [Gutmann and Hyv ¨arinen, 2012]. Subsequently, Black-\nout sampling is proposed to overcome NCE algorithm’s con-\ntext dependence problems [Ji et al., 2016 ]. Although these\napproximations can speed up training signiﬁcantly, the nor-\nmalisation term have to evaluated during inference.\nAlternatively, factored output layer decompose the orig-\ninal ﬂattened architecture into a class structure or hierar-\nchical tree, i.e., class-based (cHSM) and tree-based Hier-\narchical Softmax (tHSM) respectively [Chen et al., 2016 ].\ncHSM method partition the vocabulary into mutually exclu-\nsive classes, and tHSM builds nested categories of words to\nform a binary tree with words on the leaves. Recently, the\ncHSM method has been benchmarked with different cluster-\ning strategies and compared with other sampling-based ap-\nproaches [Chen et al., 2016]. Besides, the historical proposed\ntHSM method that builds upon WordNet knowledge or Huff-\nman coding performs relatively better than the cHSM method\nin term of time efﬁciency [Mikolov et al., 2013b].\nAfter investigating the tHSM model, it was shown that it\ncalculates the internal probability step-by-step and layer-by-\nlayer, giving a time complexity ofO(log jVj) [Mikolov et al.,\n2013b; Mnih and Hinton, 2009]. Considering the modern ar-\nchitecture of GPUs and generalised matrix-matrix operations,\nit is interesting to ask whether it is possible to calculate the\ninternal probability in parallel? Furthermore, the Huffman\ncoding scheme for tHSM only consider the word’s frequen-\ncy statistics while words’ context and semantic information\nhave yet been exploited, which demands for detailed discus-\nsion of word hierarchical clustering’s strategies to initialise\nthe words’ distribution over the tree.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n1951\nTherefore, in this research, we propose a parallelised math-\nematical model for modeling tHSM’s variants (i.e., p-tHSM).\nMoreover, hierarchical Brown clustering and other possible\nclustering algorithms are employed to initialise the word hi-\nerarchy before the training stage, to improve the stability and\nperformance of p-tHSM algorithm. Furthermore, we con-\nducted empirical analysis and comparisons on the standard\nPenn Tree Bank (PTB) [Marcus et al., 1993], WikiText-2 and\nWikiText-103 text datasets [Merity et al., 2017 ] with other\nconventional optimisation methods to assess its efﬁciency and\naccuracy on GPUs and CPUs1.\n2 Background\nSoftmax with Over-large Vocabularies.As a standard prob-\nability normalisation method for multi-label classiﬁcation,\nthe softmax function and its gradient can be formalised as:\np(wijh) = exp(h>vwi )\nP\nwj2Vexp(h>vwj )\n@p(wijh)\n@vwj\n=p(wjjh)(\u000eij \u0000p(wijh))h>\n(1)\nwhere h denotes the hidden layer (i.e., context representa-\ntion), vw is the target word-embedding of word w and \u000eij\nrepresents the Kronecker delta.\nThe forward probability propagation and backward gradi-\nent optimisation manipulating all the words in the target vo-\ncabulary, thereby resulting in low efﬁciency. For a vocabulary\ncomprising jVjwords, the overall time complexity is O(jVj).\nThis computational burden is relatively high even for modern\narchitectures of GPUs, which are highly suitable for matrix\nmultiplication with its parallelism.\nIn order to alleviate the computational bottleneck when\ncomputing the normalisation term in Eq. 1, various approxi-\nmations have been developed, which can be divided into three\ncategories: vocabulary truncation, sampling-based approxi-\nmation, and factored output layer methods.\nVocabulary Truncation. Intuitively, a relatively smaller\nvocabulary list can be maintained to avoid computing a large\nvocabulary, where it will run faster with less memory con-\nsumption. Maintaining a short-list of the most frequent words\nand pruning the rest has been explored in literature [Marcus\net al., 1993], while excessive words in texts will be mapped\nto “hunki” tokens. Another point of view is to split word-\ns into subword-level units according to its character n-gram\nstatistics to reduce its vocabulary size [Sennrich et al., 2016].\nSampling-based Approximation. Sampling-based ap-\nproaches have been employed successfully to approximate\nthe softmax and its gradient over a large vocabulary in\nnumerous ﬁelds [Baltescu and Blunsom, 2015; Mnih and\nKavukcuoglu, 2013]. The core idea is to compute only a tiny\nfraction of the outputs dimensions to achieve computational\nefﬁciency.\nTo be speciﬁc, NCE approximation regards the multi-class\nprediction problem as psudo binary classiﬁcation and this\n1All our codes and models are publicly available at https://\ngithub.com/jiangnanhugo/lmkit\nmethod employs an auxiliary loss to optimise the goal of max-\nimizing the probability of correct words, while also minimis-\ning the noise probability [Mnih and Teh, 2012]. To be specif-\nic, the model learn to classifyw0 from fw1 \u0001\u0001\u0001 wkg, where w0\nis the empirical example and fw1 \u0001\u0001\u0001 wkgare noise samples\ngenerated from a prior unigram distribution q(w). The nor-\nmalised probability of positive categories and the joint prob-\nability of knoise samples is:\n~p(y= 1jh) = exp(h>vw0 )\nexp(h>vw0 ) + k\u0003q(w0)\n~p(y= 0jh) =\nkY\ni=1\nk\u0003q(wi)\nexp(h>vwi ) + k\u0003q(wi)\n(2)\nwhere ~p(y = 0 jh) involves a sum over k noise samples in-\nstead of a sum over the entire vocabulary, making the NCE\ntraining time linear in the number of noise samples O(k) and\nindependent of the vocabulary size.\nIn addition, the recently proposed Blackout sampling algo-\nrithm implicitly combine the advantages of importance sam-\npling and NCE to solve the unigram distribution’s context de-\npendency problems in NCE algorithm [Ji et al., 2016].\nOverall, these approximations signiﬁcantly accelerate the\ntraining speed but time is still required to sample abundant\nnoises with the unigram distribution q(w). Nonetheless, the\npartition function have to evaluated during inference as can-\ndidates are predicted in the whole vocabulary [Gutmann and\nHyv¨arinen, 2012].\nFactored Output Layer. Factored output layer can dra-\nmatically reduce the cost of representing the probability dis-\ntribution as well as learning and inference, and several pro-\nposed structures can be divided into two categories: cHSM\nand tHSM methods.\nFor cHSM model, it transforms one-step multi-label pre-\ndiction into two-step multi-label prediction, where the former\npredicts the class of the word and the latter predicts the prob-\nability of this word in the corresponding class, as shown in\nFig.1 and Eq.3. Therefore, this procedure employs cascad-\ned softmax prediction to avoid the multiplication of a large\nvocabulary. If we partition the vocabulary into k classes\nfc1;\u0001\u0001\u0001 ;ckg, such that ci\nTcj = \u001e;i 6= jand V= Sk\ni=1 ci,\nthen the cHSM model can be written as:\np(wijjh) = p(cijh) \u0001p(wijjci;h) (3)\nwhere wij belongs to class ci. If each class containsp\njVjwords, the optimal time complexity can be reduced to\nO(\np\njVj) [Goodman, 2001].\nh\nc2\nbroommop\nc1\ncat duck\nFigure 1:\nClass-based Hierarchical Softmax.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n1952\nLikewise, tHSM decomposes one-step multi-class classiﬁ-\ncation into several steps of logistic classiﬁcation so the vo-\ncabulary is organised as binary tree, where words lie on its\nthe leaves and the intermediate nodes are internal parameters.\nIn the case of a balanced tree structure, the optimal case can\nbe O(log jVj). Traditionally, tree can be constructed from\nWordNet with human experts [Morin and Bengio, 2005 ] or\nHuffman coding with word unigram distribution [Mikolov et\nal., 2013b]. Yet, expert knowledge is expensive in real world\nchallenge and the Huffman coding scheme only considers the\nunigram statistics while words’ context, syntactic or semantic\ninformation are neglected.\n3 Methodology\nIn this section, we begin by deﬁning the word polarity en-\ncoding scheme for modeling tree-based parameters and struc-\ntures. With this scheme, we derive a compact and tight form\nof cost function as well as its gradient for the proposed paral-\nlelised tHSM (p-tHSM) model. Before training the p-tHSM\nmodel, we investigate several word hierarchical clustering s-\ntrategies to deﬁne the distribution of word over the tree, in\norder to achieve stable performance.\n3.1 Word Polarity Encoding\nIn the case of binary tree with words on its leaves, we can\nlocate every word by all visiting the internal nodes from root\nto the leaf. Here, the route for word w denotes the internal\nnodes \u0012w\ni and edges dw\ni it visits.\nTo illustrate, \u0012w\ni represents the non-leaf nodes on the ith\nlayer on the route to word w, and \u0012w\ni 2Rm;i 2[0;lw \u00001].\nLikewise, dw\ni represents the edge that connects the (i\u00001)th\nand ith layer’s node. For each non-leaf node, moving down\nto the left branch is labelled as \u00001 and selecting the right\nbranch is labelled as +1. So, dw\ni 2f\u00001;+1g, i2[0;lw \u00001].\nBesides, lw \u0019 log jVjdenotes the route’s length from the\nroot to the leaf word. With this schema, we can change the\ntraditional word index or one-hot representation into a word\npolarity encoding tuple (dw;\u0012w) for word wby denoting the\npolarity route to locate each word.\nDuring implementation, we maintain a path looking-up ta-\nble \u0000, memorising all the visited internal nodes’ index for\neach word wfrom root to leaf. Such that, \u0012w is retrieved from\nparameter matrix \u0002 by selecting all the nodes from \u0000(w). S-\nince the ﬁrst dimension of\u0002 is V\u00001 (as Plog V\ni=0 2i = jVj\u00001),\nno external parameters are involved for training. Besides, dw\nis retrieved by obtaining the wth row vector from matrix D,\nwhere wis the index in the vocabulary. Furthermore, f\u0000;Dg\nare deﬁned by the word clustering hierarchy, which is intro-\nduced in the following section, and \u0002 is optimised by gradi-\nent descent on the training dataset.\n3.2 Tree-based Loss and Gradient\nDuring every step in the target word tree, we make a logistic\nprediction about whether go to the left branch or right part\nat each non-leaf node. The probability of i-th label dw\ni 2\nf\u00001;1ggiven the i-th node and hidden layer his:\np(dw\ni = \u00061j\u0012w\ni ;h) = \u001b(dw\ni \u0012w\ni h) (4)\n\u0012w\n0\n\u0012w\n1\n\u0012w\n2\nw0w\ndw\n2 = \u00001\ndw\n1 = +1\ndw\n0 = \u00001\ndw = [dw\n0 ;dw\n1 ;dw\n2 ]\n\u0012w = [\u0012w\n0 ;\u0012w\n1 ;\u0012w\n2 ]\nFigure 2: Tree-based Hierarchical Softmax. Internal nodes are pa-\nrameterised by \u0012w\ni , edges between nodes are parameterised by dw\ni ,\nwhere dw is a vector and \u0012w is a matrix. E.g., dw = [\u00001;+1;\u00001]\nand dw0\n= [\u00001;+1;+1].\nwhere \u001b(z) = 1=(1+exp( \u0000z)) denotes the sigmoid function\nand its symmetric rule is employed for abbreviation [Klein-\nbaum and Klein, 2010 ]: \u001b(z) + \u001b(\u0000z) = 1 . Therefore, the\nprobability of wordwis the joint product of the probability of\nthe route (dw;\u0012w) taken from the root to the corresponding\nleaf node:\nlog p(wjh) = log\nlw\u00001Y\ni=0\np(dw\ni j\u0012w\ni ;h)\n=\nlw\u00001X\ni=0\nlog \u001b(dw\ni \u0012w\ni h)\n= log\u001b(dw>\u0012wh)\n(5)\nand the corresponding loss function of the model L(\u0012jh;w)\nis deﬁned by the negative log-likelihood:\nL(\u0012jh;w) = \u0000log\nlw\u00001Y\ni=0\n\u001b(dw\ni \u0012w\ni h)\n= \u0000log \u001b(dw>\u0012wh)\n= log(1 + exp(\u0000dw>\u0012wh))\n=\u0010(\u0000dw>\u0012wh)\n(6)\nwhere \u0010(z) denotes the softplus function: \u0010(z) =\nlog(1 + exp( z)) and its gradient is sigmoid-like function:\nd\u0010(z)=dz = \u001b(z) = 1=(1 + exp(\u0000z)) [Dugas et al., 2000].\nMinimising the negative log-likelihood is directly maximis-\ning softplus loss and the probability of estimated words.\nIn the traditional tHSM algorithm, the model calculates the\nlog-probability of every node layer-by-layer consequently the\noverall joint log-probability of this route is summarised lin-\nearly, thus the time complexity of tHSM takes O(log V):\nL0(\u0012jh;w) =\nlw\u00001X\ni=0\nf(1 \u0000d0w\ni ) log(\u001b(\u0012w\ni h))\n+ d0w\ni log(1 \u0000\u001b(\u0012w\ni h))g\n(7)\nwhere d0w\ni 2f0; 1g, and these two parts model the joint prob-\nability of the left and right branch of internal nodes separa-\ntively. Noticeably, the main difference between p-tHSM and\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n1953\ntHSM is that: a) tHSM algorithm involves many tiny matrix\nmultiplications, instead we load all parameters (dw;\u0012w) di-\nrectly as 1D vector and 2D matrix at the expense of memory\nconsumption and we consider the multiplications of this vec-\ntor and giant matrix, as shown in Fig 2; b) Based on that, a\ncompact loss function of the model is deducted and the nodes’\nlog-probability are calculated simultaneously which results in\nbetter time efﬁciency for p-tHSM model.\nLikewise, the model’s parameters f\u0012w;hgare optimised\nwith regard to its gradient.\n@L\n@\u0012w =(\u001b(dw>\u0012wh) \u00001)dw>h\n@L\n@h =(\u001b(dw>\u0012wh) \u00001)dw>\u0012\n(8)\nA major advantage of the binary decomposition is that it\navoids normalise the probability over the whole vocabulary,\nas the summarised probabilities of words in the tree is natu-\nrally equals to one.\nX\nw2V\np(wjh) =\nX\nw2V\nX\ni\n\u001b(dw\ni \u0012w\ni h) = 1 (9)\nIn the procedure of inference, for a node in the i-th layer,\nwe choose the left branch whenp(dw\ni j\u0012w\ni ;h) \u00150:5 and select\nthe right child when the opposite applies. Thus, the computa-\ntional time complexity is O(log jVj).\n3.3 Word Hierarchy Criterion\nThe polarity encoding of each word (dw;\u0012w) in the vocabu-\nlary is closely related to the tree’s structure. In the proposed\nmethod, we employ several clustering algorithms to improve\nthe performance of p-tHSM algorithm.\nRandom Shufﬂe. This intuitive method ignores the word\nhierarchy. Thus, words are randomly located in the leaves of\nthe tree’s nodes. This is the worst case for revealing the lower\nbound of the tree modelling method.\nAlphabetical Order. It is more efﬁcient to sort the vocab-\nulary of words in alphabetical order and words with similar\ntokens for strings share similar meanings in the text. Thus,\nsimilar clustering results might be obtained to the commonly\nused frequency binning method.\nHuffman Clustering. This method sorts the vocabulary\nbased on word frequency and can be directly applied on p-\ntHSM model, where words distribution on the tree are gen-\nerated by the Huffman clustering rules. So frequent word-\ns share similar short path and rare words are assigned with\nsimilar long path [Mikolov et al., 2013b].\nBrown Clustering. The Brown algorithm is an agglom-\nerative hierarchical clustering method that clusters words to\nmaximise the mutual information for bigram words [Brown\net al., 1992; Liang, 2005 ], The binary preﬁx string for every\nword it generates can be applied to initialise p-tHSM model.\n4 Experimental Study\nIn order to validate the proposed p-tHSM method and com-\npare with other baseline methods, we conducted experimental\nstudy and analysis with recurrent language modeling task on\nthree standard text datasets with several metrics.\n...1w\n... ...0h0\n... n-2w\nn-1hn-2h..\ns /s\nFigure 3: Recurrent Neural Language Model. Every sentence is\nwrapped with start (i.e., hsi) and end (i.e., h/si) tokens. Before\npredicting the next-step word wt+1, the input is received from the\nlast hidden state ht\u00001 and current word wt.\n4.1 Recurrent Language Model and Metrics\nThe goal of a language model is to learn the probability for\na sequence of words. In particular, given a sequence of T\nwords: [w1;w2 \u0001\u0001\u0001 ;wT ], the probability of this sequence can\nbe decomposed into the joint product of the conditional prob-\nability using the chain rule:\np(w1;w2 \u0001\u0001\u0001 ;wT ) =\nTY\nt=1\np(wtjw<t) (10)\nwhere p(wtjw<t) denotes the next-step word probability giv-\nen its previous context [w1;\u0001\u0001\u0001 ;wt\u00001] as inputs, and its usu-\nally modeled by RNN model, as shown in Fig. 3. In this\nresearch, recurrent neural network with gated units (GRU)\n[Chung et al., 2014] is preferred as it requires a much smaller\nset of parameters to model the internal recurrence mechanis-\nm as well as its gating function ease the gradient vanishing\nproblem [Hochreiter, 1998].\nIn the training process, we regard the cross-entropy be-\ntween the true word distribution and the predicted distribu-\ntion, also known as the negative log-likelihood, as the mod-\nel’s loss function:\nL(\u0012jw1;\u0001\u0001\u0001 ;wT ) = \u00001\nT\nTX\nt=1\nlog p(wtjw<t) (11)\nA standard score used for comparing LM’s performance\nis the model’s perplexity (PP), which denotes the degree\nof confusion when choosing the next-step word among can-\ndidates and a lower perplexity for a language model im-\nplies better predictability. The perplexity is the exponential\nof the average negative log-likelihood: PP(w1;\u0001\u0001\u0001 ;wT ) =\nexp(L(\u0012)), denoting that we are directly optimising the per-\nplexity metric during training.\nIn addition, the training time efﬁciency, vocabulary scala-\nbility and runtime memory consumption should also be con-\nsidered as important metrics to benchmark models. Thus,\nwe analysed these evaluative metrics for different optimisa-\ntions algorithms in both theoretical and empirical perspec-\ntives on GPUs and CPUs respectively. Finally, experiments\nare conducted and results are collected on the standard PTB,\nWikiText-2 and WikiText-103 dataset to evaluate their accu-\nracy and efﬁciency, and the detailed statistics of these dataset-\ns2 are demonstrated in Table 1.\n2https://metamind.io/research/the-wikitext-long-term-\ndependency-language-modeling-dataset/\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n1954\nTable 1: Statistics of the PTB, WikiText-2 and WikiText-103 Dataset.\nDataset PTB WikiText-2 WikiText-103\n#train #valid #test #train #valid #test #train #valid #test\nArticles 2,000 155 155 600 60 60 28,475 60 60\nSentences 42,068 3,370 3,761 36,718 3,760 4,358 1,801,350 3,760 4,358\n#V ocabulary 10,000 33,278 267,735\nOut-of-V ocabulary (%) 4.8% 2.6% 0.4%\n4.2 Results and Discussions\nFactored Output Layer Comparison. As shown in Table 2,\nwe benchmarked the empirical time complexity and parame-\nters size for training the same batched data on GPUs and C-\nPUs. The “Softmax” method denotes the traditional ﬂattened\nsoftmax method, the “cHSM” algorithm is the class-based hi-\nerarchical softmax, the “tHSM” method denotes the tradition-\nal tree-based hierarchical softmax that has been discussed in\nsection 3, and the “p-tHSM” algorithm is the proposed paral-\nlelised tree-based hierarchical softmax.\nWe tried to process the WikiText-103 dataset with these\nalgorithms and calculated the average time required for pro-\ncessing one batch data. Besides, the input sentence’s max\nlength, hidden layer, output vocabulary and batch size were\nset as f50;256;267735;20g, respectively. Furthermore, the\n“Total” process denotes the process for the forward propaga-\ntion and backward gradient optimisation, and the “Forward”\nprocess denotes the time consumption required from the input\nof the data until the model’s cost was calculated.\nSubsequently, we calculated the runtime memory usage for\nthe above mentioned algorithms, as shown in Table 2. Here,\nthe “Runtime” denotes the required memory we need to load\nduring training, and jHjis the embedding dimension. During\ntraining, tHSM only consumed a minimal memory resource\nwhile p-tHSM covered a much larger set of memory, and p-\ntHSM employed larger memory and achieved better speedup\nratio when considering both memory consumption and speed.\nIn order to verify the scalability of cHSM, tHSM and p-\ntHSM algorithms with relevance to the vocabulary size, re-\nsults are collected in Fig. 4. In order to visualise the impact of\nthe p-tHSM algorithm, the “Softmax” method is not includ-\ned as it consumed much more computational time compared\nwith the others. Clearly, the cHSM scales with the square\nroot of the vocabulary size (i.e., O(\np\njVj)) whereas p-tHSM\nexhibits stable performance as the vocabulary size increases.\nBased on these experiments, we may conclude that the\nproposed p-tHSM method outperformed the historical bench-\nmark O(log jVj) and achieved a satisfactory speed-up ratio\nfor the hierarchical softmax architecture. This performance\nwas attributable to the acceleration based on the hardware’s\nparallelism, but also due to the fundamental structure of the\np-tHSM method, which could maintain the target word tree\nin a parallel manner.\nWord Clustering Strategy Analysis. Chen et al. revealed\nthat the performance of cHSM is sensitive to the hierarchical\nclustering of the words based on the tree [Chen et al., 2016].\nLikewise, we considered several existing tree clustering crite-\nria in order to stabilise the performance of p-tHSM algorithm.\nTable 2: Runtime time and memory comparison on GPUs and CPUs\nwith WikiText-103 dataset.\nRuntime Total (ms) Forward (ms)\nmemory cpu gpu cpu gpu\nSoftmax jHVj 510.4 262.1 352.2 62.9\ncHSM 2jHj\np\njVj 506.5 40.6 28.7 14.6\ntHSM jHj 1,004.0 444.4 8.1 5.6\np-tHSM jHjlog jVj 383.5 86.4 7.0 1.4\nV ocabulary Size\nProcessing Time (ms)0\n5\n10\n15\n20\n25\n24.5\n12.5\n6.8\n4.7\n12.110.410.2\n8.4 7.96.46.45.3\ncHSMtHSMp-tHSM\n500,000100,00050,00010,000\nFigure 4: Scalability of cHSM, tHSM and p-tHSM algorithms with\nrelevance to the vocabulary size.\nTable 3: Perplexity of p-tHSM method with various hierarchical\nclustering algorithms on PTB Dataset.\nMethods Valid Test\nRandom Shufﬂe 199.62 189.37\nAlphabetical Order 154.02 149.12\nHuffman Clustering 134.33 129.34\nBrown Clustering 133.12 128.78\nSo we compared several tree clustering criteria using the\nPTB dataset in Table 3. Randomly shufﬂing words and s-\nplitting them into clusters of equal sizes performed the worst\nas it did not provide any information about the distribution\nof words. Besides, Huffman clustering (i.e., frequency bin-\nning) method categorised words into frequency groups, where\nwords in the same branches of the tree shared a similar word-\ns’ frequency[Mikolov et al., 2013a]. This method performed\nbetter than alphabetical order clustering algorithm, which\nwords were sorted considering their alphabetical order and\nsplit them into equal-size groups of words. Although Brown\nclustering token time to calculate and similar words were\ngrouped together according to its bigrams’ distribution, it per-\nformed the best than the others. To be noticed, Randomly\nshufﬂing and alphabetical order methods partition the vocab-\nulary into balanced tree structures while the rest not.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n1955\nTable 4: Perplexity benchmark on validation and testing dataset with PTB, WikiText-2 and WikiText-103 corpus.\nPTB WikiText-2 WikiText-103\nValid Test Valid Test Valid Test\nGRU + Softmax 131.59 125.10 169.07 160.45 170.19 171.02\nGRU + NCE [Gutmann and Hyv¨arinen, 2010] 139,79 137.35 210.19 189.15 194.78 195.01\nGRU + Blackout [Ji et al., 2016] 137.68 135.49 201.51 185.31 192.11 193.76\nGRU + cHSM [Chen et al., 2016] 133.17 125.05 179.64 169.09 171.81 166.74\nGRU + p-tHSM (pretrain via Huffman)[Mikolov et al., 2013b] 134.33 129.34 218.42 216.05 165.70 166.11\nGRU + p-tHSM (pretrain via Brown) [Brown et al., 1992] 133.12 128.78 186.23 189.58 164.15 161.55\nAll Experiments Benchmark. As shown in Table 4, we\ncollected all the perplexity results on validation and testing\ndataset of the above mentioned three standard corpus for ev-\nery algorithms. Besides, all experiments implemented with\nTheano framework [Theano Development Team, 2016] were\nrun on one standalone GPU device with 12 GB of graphical\nmemory (i.e., Nvidia K40m), which allowed the large em-\nbedding matrix multiplications to be possible. However, the\nresource of one GPU memory was exploited quickly with the\nrise of parameters’ dimension.\nTo be noticed, we employed one layer of GRU cell as the\ncontext representation for all these algorithms, the dimension\nof which was set to 256. Besides, for the NCE and Black-\nout approximations, the hyper-parameter kwas set to jVj=20\nfor smaller PTB and Wikitext-2 datasets and k = jVj=200\nfor the larger WikiText-103 dataset. Furthermore, for cHSM\nmethod, we partitioned the vocabulary according its words’\nunigram distribution.\nConsidering results on PTB and Wikitext-2 datasets, the\noriginal softmax achieved the best ever score than the rest\nalgorithms, as it did not introduce any structural loss in cHSM\nand p-tHSM algorithms or sampling-based variational loss in\nBlackout and NCE approximations.\nFor the last ever-large Wikitext-103 dataset, the p-tHSM\nmethod with Brown clustering not only achieved better result-\ns than the Huffman clustering methods, but also performed\nbetter than the others. Alternatively, the cHSM model were\ncapable of getting similar results than the p-tHSM variants,\ndenoting we might achieved better results with other suitable\nclustering algorithms for cHSM methods. Even the Wikitext-\n103 and Wikitext-2 dataset shared the same testing set, we\nfound the original softmax were hard to converge and per-\nformed the worst results. Besides, for sampling-based meth-\nods, it converged to much better results than the softmax\nmethod at the same time improve the time efﬁciency.\nTo conclude, after replacing the traditional Huffman cod-\ning scheme in tHSM with a word polarity coding scheme and\na compact tree-based model p-tHSM is achieved, we demon-\nstrated that this novel proposed coding scheme allowed the\ncalculations to run in parallel on GPUs. This reduced the time\ncomplexity of raw tHSM O(log jVj) and obtained the opti-\nmum speed-up ratio for large vocabulary problems. Further-\nmore, after testing several existing clustering algorithms for\nstabilising the performance of the p-tHSM model, we found\nthat word clustering based on the tree model was closely re-\nlated to binary classiﬁcation based on the internal nodes.\n5 Conclusions and Future Work\nIn this study, we considered the over-large vocabulary prob-\nlem in the language modeling ﬁelds. In the literature, various\napproaches have been proposed to address this issue, which\ncan be roughly divided into three categories: vocabulary trun-\ncation, sampling-based approximation, and factored output\nlayer algorithms.\nThe ﬁrst approach was easy to implement yet not perfectly\nsolve the problem as it can not generalise to large real world\ntasks. While the second categories of method can reduce\nthe training time consumption in an efﬁcient manner without\nsummarising all the words through variations of importance\nsampling, but it failed during inference because a prior distri-\nbution is applied and the multi-polynomial sampling method\nwas not time efﬁcient. The third methods changed the ﬂatten\narchitecture of the softmax algorithm into a heuristic hierar-\nchical structure with two possible types: cHSM and tHSM\nalgorithms. The cHSM method built on the two-step softmax\nclassiﬁcation method and tHSM extended this idea to log jVj\nsteps binary classiﬁcation tasks.\nIn this research, we not only proposed a potential encod-\ning scheme to improve the standard tHSM but also built a\ncompact mathematical model for tHSM’s variants (i.e., p-\ntHSM) but also exploited the advantages of GPU hardware,\nso it can efﬁciently calculate the exact softmax and gradient\nwith a large target vocabulary. Remarkably, the complexity of\nthis algorithm is beyond the historical record O(log jVj) but\nnot in constant time complexity, thereby allowing it to tackle\nvery large vocabulary problems. Furthermore, we evaluated\nseveral word hierarchical clustering algorithms for organizing\nwords in the tree model in a more efﬁcient manner. Compared\nwith the baselines, the results showed that the speed-up ratio\nwas enhanced for the other probability normalisation method\nand more efﬁcient tree clustering was obtained.\nIn future research, we plan to explore the application of\nthe softplus activation function and optimise several existing\nhierarchical clustering algorithms, in order to design a more\nefﬁcient hierarchical structure.\nAcknowledgments\nThis work was partially supported by by the State Key Labo-\nratory of Software Development Environment of China (No.\nSKLSDE-2017ZX-16), the National Natural Science Foun-\ndation of China (No. 61332018), and the Fundamental Re-\nsearch Funds for the Central Universities.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n1956\nReferences\n[Baltescu and Blunsom, 2015] Paul Baltescu and Phil Blun-\nsom. Pragmatic neural language modelling in machine\ntranslation. In Proc. of NAACL-HLT, pages 820–829,\n2015.\n[Bengio et al., 2003] Yoshua Bengio, R ´ejean Ducharme,\nPascal Vincent, and Christian Janvin. A neural proba-\nbilistic language model. Journal of Machine Learning Re-\nsearch, 3:1137–1155, 2003.\n[Brown et al., 1992] Peter F. Brown, Vincent J. Della Pietra,\nPeter V . de Souza, Jennifer C. Lai, and Robert L. Mercer.\nClass-based n-gram models of natural language. Compu-\ntational Linguistics, 18(4):467–479, 1992.\n[Chen et al., 2016] Wenlin Chen, David Grangier, and\nMichael Auli. Strategies for training large vocabulary neu-\nral language models. In Proc. of ACL, pages 1975–1985,\n2016.\n[Chung et al., 2014] Junyoung Chung, C ¸ aglar G ¨ulc ¸ehre,\nKyungHyun Cho, and Yoshua Bengio. Empirical evalua-\ntion of gated recurrent neural networks on sequence mod-\neling. CoRR, abs/1412.3555, 2014.\n[Dugas et al., 2000] Charles Dugas, Yoshua Bengio,\nFranc ¸ois B´elisle, Claude Nadeau, and Ren ´e Garcia.\nIncorporating second-order functional knowledge for\nbetter option pricing. In Proc. of NIPS, pages 472–478,\n2000.\n[Goodman, 2001] Joshua Goodman. Classes for fast maxi-\nmum entropy training. In Proc. of ICASSP, pages 561–\n564, 2001.\n[Gutmann and Hyv¨arinen, 2010] Michael Gutmann and\nAapo Hyv ¨arinen. Noise-contrastive estimation: A new\nestimation principle for unnormalized statistical models.\nIn Proc. of AISTATS, pages 297–304, 2010.\n[Gutmann and Hyv¨arinen, 2012] Michael Gutmann and\nAapo Hyv ¨arinen. Noise-contrastive estimation of unnor-\nmalized statistical models, with applications to natural\nimage statistics. Journal of Machine Learning Research,\n13:307–361, 2012.\n[Hochreiter, 1998] Sepp Hochreiter. The vanishing gradient\nproblem during learning recurrent neural nets and problem\nsolutions. International Journal of Uncertainty, Fuzziness\nand Knowledge-Based Systems, 6(2):107–116, 1998.\n[Jean et al., 2015] S´ebastien Jean, KyungHyun Cho, Roland\nMemisevic, and Yoshua Bengio. On using very large tar-\nget vocabulary for neural machine translation. In Proc. of\nACL, pages 1–10, 2015.\n[Ji et al., 2016] Shihao Ji, S. V . N. Vishwanathan, Nadathur\nSatish, Michael J. Anderson, and Pradeep Dubey. Black-\nout: Speeding up recurrent neural network language mod-\nels with very large vocabularies. In Proc. of ICLR, 2016.\n[Kim et al., 2016] Yoon Kim, Yacine Jernite, David Sontag,\nand Alexander M. Rush. Character-aware neural language\nmodels. In Proc. of AAAI, pages 2741–2749, 2016.\n[Kleinbaum and Klein, 2010] David G Kleinbaum and\nMitchel Klein. Maximum likelihood techniques: An\noverview. Logistic regression, pages 103–127, 2010.\n[Liang, 2005] Percy Liang. Semi-supervised learning for\nnatural language. PhD thesis, MIT, 2005.\n[Marcus et al., 1993] Mitchell P. Marcus, Beatrice Santorini,\nand Mary Ann Marcinkiewicz. Building a large annotat-\ned corpus of english: The penn treebank. Computational\nLinguistics, 19(2):313–330, 1993.\n[Merity et al., 2017] Stephen Merity, Caiming Xiong, James\nBradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In Proc. of ICLR, 2017.\n[Mikolov et al., 2010] Tomas Mikolov, Martin Karaﬁ ´at,\nLuk´as Burget, Jan Cernock´y, and Sanjeev Khudanpur. Re-\ncurrent neural network based language model. In Proc. of\nInterspeech, pages 1045–1048, 2010.\n[Mikolov et al., 2013a] Tomas Mikolov, Kai Chen, Greg\nCorrado, and Jeffrey Dean. Efﬁcient estimation of word\nrepresentations in vector space. CoRR, abs/1301.3781,\n2013.\n[Mikolov et al., 2013b] Tomas Mikolov, Ilya Sutskever, Kai\nChen, Greg S Corrado, and Jeff Dean. Distributed repre-\nsentations of words and phrases and their compositionality.\nIn Proc. of NIPS, pages 3111–3119. 2013.\n[Mnih and Hinton, 2009] Andriy Mnih and Geoffrey E Hin-\nton. A scalable hierarchical distributed language model.\nIn Proc. of NIPS, pages 1081–1088. 2009.\n[Mnih and Kavukcuoglu, 2013] Andriy Mnih and Koray\nKavukcuoglu. Learning word embeddings efﬁciently with\nnoise-contrastive estimation. In Proc. of NIPS, pages\n2265–2273. 2013.\n[Mnih and Teh, 2012] Andriy Mnih and Yee Whye Teh. A\nfast and simple algorithm for training neural probabilistic\nlanguage models. In Proc. of ICML, 2012.\n[Morin and Bengio, 2005] Frederic Morin and Yoshua Ben-\ngio. Hierarchical probabilistic neural network language\nmodel. In Proc. of AISTATS, 2005.\n[Schwenk, 2007] Holger Schwenk. Continuous space lan-\nguage models. Computer Speech & Language, 21(3):492–\n518, 2007.\n[Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and\nAlexandra Birch. Neural machine translation of rare words\nwith subword units. In Proc. of ACL, 2016.\n[Theano Development Team, 2016] Theano Development\nTeam. Theano: A Python framework for fast computation\nof mathematical expressions. CoRR, abs/1605.02688,\n2016.\n[Wang and Wang, 2016] ZhongQiu Wang and DeLiang\nWang. A joint training framework for robust automatic\nspeech recognition. IEEE/ACM Transactions on Audio,\nSpeech & Language Processing, 24(4):796–806, 2016.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n1957",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.9187344312667847
    },
    {
      "name": "Computer science",
      "score": 0.8014495968818665
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6876838207244873
    },
    {
      "name": "Machine translation",
      "score": 0.5826719403266907
    },
    {
      "name": "Language model",
      "score": 0.5476587414741516
    },
    {
      "name": "Vocabulary",
      "score": 0.5420703291893005
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5143662691116333
    },
    {
      "name": "Tree (set theory)",
      "score": 0.49987030029296875
    },
    {
      "name": "Cluster analysis",
      "score": 0.4862586259841919
    },
    {
      "name": "Inference",
      "score": 0.48551368713378906
    },
    {
      "name": "Natural language processing",
      "score": 0.45724841952323914
    },
    {
      "name": "Artificial neural network",
      "score": 0.4257066547870636
    },
    {
      "name": "Machine learning",
      "score": 0.34760284423828125
    },
    {
      "name": "Mathematics",
      "score": 0.07783296704292297
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I158842170",
      "name": "Chongqing University",
      "country": "CN"
    }
  ],
  "cited_by": 5
}