{
  "title": "SMARTAVE: Structured Multimodal Transformer for Product Attribute Value Extraction",
  "url": "https://openalex.org/W4385574309",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2127284166",
      "name": "Qifan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1993907199",
      "name": "Li Yang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2124326937",
      "name": "Jingang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3008998896",
      "name": "Jitin Krishnan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966961698",
      "name": "Bo Dai",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2166097142",
      "name": "Sinong Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2607121186",
      "name": "Zenglin Xu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A296516693",
      "name": "Madian Khabsa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097317532",
      "name": "Hao Ma",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035000544",
    "https://openalex.org/W4293371931",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034802203",
    "https://openalex.org/W4221167659",
    "https://openalex.org/W3034336960",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W3098003395",
    "https://openalex.org/W3104609290",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3171009372",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W4283750777",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4224919569",
    "https://openalex.org/W3168023209",
    "https://openalex.org/W2951865668",
    "https://openalex.org/W2773765985",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2979382951",
    "https://openalex.org/W2118090838",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2944898795",
    "https://openalex.org/W3157393048",
    "https://openalex.org/W3104602136",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W172829878",
    "https://openalex.org/W4224313570",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3034300118",
    "https://openalex.org/W3173793851",
    "https://openalex.org/W2954936423",
    "https://openalex.org/W3166701446",
    "https://openalex.org/W2805173585",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W4224315255",
    "https://openalex.org/W2950808302"
  ],
  "abstract": "Automatic product attribute value extraction refers to the task of identifying values of an attribute from the product information. Product attributes are essential in improving online shopping experience for customers. Most existing methods focus on extracting attribute values from product title and description.However, in many real-world applications, a product is usually represented by multiple modalities beyond title and description, such as product specifications, text and visual information from the product image, etc. In this paper, we propose SMARTAVE, a Structure Mltimodal trAnsformeR for producT Attribute Value Extraction, which jointly encodes the structured product information from multiple modalities. Specifically, in SMARTAVE encoder, we introduce hyper-tokens to represent the modality-level information, and local-tokens to represent the original text and visual inputs. Structured attention patterns are designed among the hyper-tokens and local-tokens for learning effective product representation. The attribute values are then extracted based on the learned embeddings. We conduct extensive experiments on two multimodal product datasets. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods. Ablation studies validate the effectiveness of the structured attentions in modeling the multimodal product information.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 263–276\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nSMARTA VE: Structured Multimodal Transformer for Product\nAttribute Value Extraction\nQifan Wang1, Li Yang2, Jingang Wang3, Jitin Krishnan1, Bo Dai2,\nSinong Wang1, Zenglin Xu4, Madian Khabsa1 and Hao Ma1\n1Meta AI 2Google Research 3Meituan Lab\n4Harbin Institute of Technology\nwqfcr@fb.com lyliyang@google.com\nAbstract\nAutomatic product attribute value extraction\nrefers to the task of identifying values of an\nattribute from the product information. Prod-\nuct attributes are essential in improving online\nshopping experience for customers. Most exist-\ning methods focus on extracting attribute val-\nues from product title and description. How-\never, in many real-world applications, a prod-\nuct is usually represented by multiple modali-\nties beyond title and description, such as prod-\nuct specifications, text and visual informa-\ntion from the product image, etc. In this pa-\nper, we propose SMARTAVE, a Structure\nMltimodal trAnsformeR for producT Attribute\nValue Extraction, which jointly encodes the\nstructured product information from multiple\nmodalities. Specifically, in SMARTAVE en-\ncoder, we introduce hyper-tokens to repre-\nsent the modality-level information, and local-\ntokens to represent the original text and vi-\nsual inputs. Structured attention patterns are\ndesigned among the hyper-tokens and local-\ntokens for learning effective product represen-\ntation. The attribute values are then extracted\nbased on the learned embeddings. We conduct\nextensive experiments on two multimodal prod-\nuct datasets. Experimental results demonstrate\nthe superior performance of the proposed ap-\nproach over several state-of-the-art methods.\nAblation studies validate the effectiveness of\nthe structured attentions in modeling the multi-\nmodal product information.\n1 Introduction\nProduct attributes are important features that carry\nuseful information about the product. They form\nan essential component of e-commerce platforms,\nwhich provide guidance for customers to compare\nproducts and make purchasing decisions. Product\nattributes also facilitate retailers on various appli-\ncations, including product search (Nguyen et al.,\n2020; Lu et al., 2021), product recommendations\n(Yu et al., 2021; Truong et al., 2022), and question\nFigure 1: An example of product attributes with their\ncorresponding values extracted from multiple modalities\nof the product.\nanswering systems (Zhang et al., 2020; Rozen et al.,\n2021; Huang et al., 2022). However, as shown in\nprevious studies (Dong et al., 2020; Yang et al.,\n2022), product attributes are often noisy and incom-\nplete with a lot of missing values for most retailers.\nTherefore, it is an important research problem to\nextract the product attributes with missing values.\nAttribute value extraction has attracted a lot of at-\ntention from both academia and industry in recent\nyears, with a plethora of research (Putthividhya\nand Hu, 2011a; Zhao et al., 2019; Chen et al., 2019;\nShinzato et al., 2022) being proposed to tackle this\nproblem. Most existing works (Zheng et al., 2018;\nXu et al., 2019; Wang et al., 2020; Yan et al., 2021)\nrely solely on the product title and description from\nthe product profiles, which is often insufficient to\nobtain values for all attributes. In real-world appli-\ncations, products are usually associated with rich\ninformation from other modalities, such as product\nspecification and image. This additional informa-\ntion can improve the attribute value extraction in\ntwo main aspects. First, attribute values are some-\ntimes absent in the product title and description.\nFor example, as illustrated in Figure 1, the value\n‘17.9 FL OZ’ corresponding to the attribute ‘Size’\nis only mentioned in the OCR text from the product\nimage. The value ‘Liquid’ of the attribute ‘Item\nForm’ is not mentioned in any text source of the\n263\nproduct, but can be inferred from the product visual\ninformation. In these cases, the product image is\nable to recover the missing values of the attributes.\nSecond, product title and description might contain\nmultiple candidate values for an attribute, while\nother modalities could consolidate the correct value.\nFor example, while both ‘Hair Food’ and ‘Natural’\nfrom the product title are reasonable values for at-\ntribute ‘Brand’, the OCR text in the image clearly\nsuggests that ‘hair food’ is the correct answer.\nThere are a few recent works (Zhu et al., 2020;\nLin et al., 2021) that incorporate the product im-\nage for attribute value extraction, which achieve\npromising results. However, these techniques suf-\nfer from two major limitations. First, each modality\nof the product is encoded independently with an in-\ndividual encoder, followed by a light fusion/cross-\nmodality layer on the top. In this way, the cor-\nrelations among different modalities are not fully\ncaptured, leading to less effective embeddings. Sec-\nond, the texts from individual modalities are simply\nconcatenated and fed into a single encoder, making\nthese methods inefficient to handle long input.\nTo address these challenges, in this paper, we\npropose a novel Structured Multimodal Trans-\nformer for Product Attribute Value Extraction\n(SMARTAVE), which jointly encodes the struc-\ntured product information from multiple modalities\nin a unified Transformer model. We first introduce\na set of hyper-tokens to represent all modalities of\nthe product, and use local-tokens to indicate the\noriginal text and visual inputs within each modality.\nStructured attention patterns are designed among\nthe hyper-tokens and local-tokens for modeling\nthe correlation between modalities and learning\neffective product representation. Intuitively, the\nhyper-tokens serve as ‘hubs’ routing local-tokens\nfrom different modalities to interact with each other.\nThe attribute values are then extracted based on the\nlearned embeddings from the structured encoder.\nEvaluations on two multimodal product datasets\nshow the superior performance of our model over\nseveral state-of-the-art methods. The experimental\nresults also demonstrate the effectiveness of the\nstructured attention mechanism in modeling multi-\nmodal product data with large input sequences. We\nsummarize the main contributions as follows:\n• We propose a novel structured multimodal\nTransformer that extracts product attribute val-\nues from product title, description, specifica-\ntion, visual information, and texts in the prod-\nuct image.\n• We develop a structured attention mechanism\nto jointly encode the product with multiple\nmodalities, which effectively models the cor-\nrelation among different modalities and learns\nhigh quality embeddings.\n• We conduct extensive experiments and demon-\nstrate the effectiveness of the proposed ap-\nproach over several state-of-the-art baselines.\n2 Related Work\nAttribute Value Extraction Early works in at-\ntribute value extraction (Putthividhya and Hu,\n2011b; More, 2016) can be viewed as direct or\nindirect applications of the Named Entity Recogni-\ntion (NER) method. The advent of deep learning\nhas paved the way for stronger models such as\nBiLSTM-CRF (Huang et al., 2015) and OpenTag\n(Zheng et al., 2018), which formulate the problem\nas a sequential tagging problem and use a com-\nbination of methods such as BiLSTM and CRF.\nSUOpenTag (Xu et al., 2019) builds on top of these\nmethods by jointly encoding the attribute, making\nthe model more scalable. AdaTag (Yan et al., 2021)\nuses a hyper-network and a Mixture-of-Experts\n(MoE) module to parameterize its decoder with\npre-trained attribute embeddings. A VEQA (Wang\net al., 2020) and MA VEQA (Yang et al., 2022) take\na step further by solving for the problem of scala-\nbility and generalizability of the task via question\nanswering. Meanwhile, TXtract (Karamanolakis\net al., 2020) brings a taxonomy-aware approach to\naid attribute extraction.\nUsing visual clues in attribute extraction has gar-\nnered attention in recent works (IV et al., 2017;\nAnderson et al., 2018; Singh et al., 2019; Tan and\nBansal, 2019; Hu et al., 2020). MJA VE (Zhu et al.,\n2020) designs a multimodal model that jointly pre-\ndicts product attributes and extract values from tex-\ntual product descriptions with the help of product\nimages. PAM (Lin et al., 2021) incorporates text\ndescriptions, Optical Character Recognition (OCR)\nand visual modalities from the product. This model\nfuses the three modalities into a multimodal Trans-\nformer and is framed as a sequence generation task,\nrather than a sequence tagging task. These multi-\nmodal methods use multiple encoders to encode\ndifferent modalities, but fail to capture the struc-\ntured information and thus are not able to model the\nrelationship effectively among different modalities.\n264\nTitle DescriptionAttribute Spec1 Specm OCRn Image\nBrand Hair Food ... Argan OilFeed your ... you don't need.Recipes, not ... mineral oil freeTo feed frizzy ... Shampoo\nOCR1\nhair food 17.9 FL OZ (530 mL)\nHyper Tokens\nLocal Tokens\nStacked Layers\nHair Food ... Argan OilFeed your ... you don't need.Recipes, not ... mineral oil freeTo feed frizzy ... Shampoohair food 17.9 FL OZ (530 mL)\nB      E\nSMARTAVE Encoder\nExtraction Layer\nTitle DescriptionAttribute Spec1 Specm OCRn Image\nBrand Hair Food ... Argan OilFeed your ... you don't need.Recipes, not ... mineral oil freeTo feed frizzy ... Shampoo\nOCR1\nhair food 17.9 FL OZ (530 mL)\nHyper Tokens\nLocal Tokens\nHyper-to-Hyper attention Hyper-to-Local attention Local-to-Hyper attention Local-to-Local attention\nAttribute\nCRF Decoder\nB      E Value\nFigure 2: Overview of SMARTAVE model architecture. Encoder: our structured encoder jointly encodes the\nproduct with multimodal information via structured attentions, and learns effective representation for all the\nmodalities. Extraction Layer: the attribute value is decoded from the learned embedding.\nEfficient Transformers Our work is also related\nto those efficient Transformer methods (Beltagy\net al., 2020; Rae et al., 2020) that deal with large\nand structured input. Transformer XL (Dai et al.,\n2019) designs a mechanism that is able to encode\nlong text sequences beyond a fixed size. Long-\nformer (Beltagy et al., 2020) and ETC (Ainslie\net al., 2020) propose to extend the CLS token to\nmultiple global tokens to deal with large text input.\nHIBERT (Zhang et al., 2019) introduces a hierar-\nchical attention pattern by dividing the input into\nseveral blocks with equal sizes. Random sparse\nattention mechanism is proposed in (Zaheer et al.,\n2020) which reduces the quadratic attention com-\nputations to linear time. These methods achieve\npromising results on dealing with large and struc-\ntured text sequences. However, they are not di-\nrectly applicable to structured data with multiple\nmodalities. A comprehensive study of efficient\nTransformers can be found in (Tay et al., 2022).\n3 SMARTAVE\n3.1 Problem Definition\nIn this section, we formally define the problem of\nattribute value extraction from the product profile.\nThe product profile contains multiple modalities,\nsuch as Title, Description, Specifications, OCR\ntexts, and Image. We denote the product profile\nas P = (M1,M2,...,M n), where Mi represents\nthe i-thmodality of the product. For each modal-\nity, it is either a text or image sequence, i.e., Mi\n= (wi\n1,...,w i\nmi), where wi\nj is the j-th word or\nimage token in Mi. The goal of attribute value ex-\ntraction is that given a target attribute A, extract its\ncorresponding values from the product profile.\n3.2 Model Overview\nThe overall model architecture of SMARTAVE\nis shown in Figure 2. Essentially, our model is\ncomposed of three key components, the input layer,\nthe SMARTAVE encoder and the extraction layer.\nThe input layer constructs both the hyper and lo-\ncal tokens of SMARTAVE, and initializes their\nembeddings. The SMARTAVE encoder is the\nmain building block that jointly encodes the multi-\nmodal input data with structured attention patterns,\nincluding Hyper-to-Hyper, Hyper-to-Local, Local-\nto-Hyper and Local-to-Local attentions. The ex-\ntraction layer consists of a decoder that decodes the\nvalue from the embedding of the Attribute hyper-\ntoken, and a sequential tagging module that ex-\ntracts the value from the text modalities. Note that\nthe final attribute value is directly obtained from\nthe decoder, while the sequential tagging module\nprovides an auxiliary task for learning better em-\nbeddings.\n265\n3.3 Input Layer\nExisting multimodal attribute value extraction ap-\nproaches (Zhu et al., 2020; Lin et al., 2021) encode\neach modality of the product separately with indi-\nvidual encoders. In this work, we jointly model\nall modalities with texts and images in a unified\nstructure Transformer model. In the input layer of\nSMARTAVE, we construct two different types of\ntokens as follows.\nHyper-token For each modality in the product pro-\nfile, we introduce a Hyper-token in our SMAR-\nTAVE. Additionally, we also add one Hyper-token\nto represent the attribute. The embedding of each\nHyper-token can be viewed as a summarization\nof the information contained in the corresponding\nmodality. For instance, in Figure 2, the embedding\nof the ‘Title’ Hyper-token summarizes the text se-\nquence in the product title. The ‘Attribute’ Hyper-\ntoken essentially represents the product-dependent\nembedding of the target attribute.\nLocal-token For text modality, the Local-token\nis the commonly used word representation in nat-\nural language models (Vaswani et al., 2017; De-\nvlin et al., 2019). For example, ‘OCR 1’ contains\ntwo Local-tokens, ‘hair’ and ‘food’. For image\nmodality, each Local-token corresponds to an im-\nage patch (Dosovitskiy et al., 2021). The creation\nof these image patches is flexible without any con-\nstraint. In our implementation, we adopt the Faster\nR-CNN model (Ren et al., 2017) to obtain the im-\nage patches/regions from the product image, with\none additional patch representing the whole image.\nIn the input layer, every token is represented by\na d-dimensional embedding vector. In particular,\nfor a Hyper-token, its embedding is constructed by\nadding a hyper embedding, a type embedding and\na modality embedding. For a Local-token embed-\nding, it is constructed by a word/patch embedding,\na type embedding and a modality embedding. The\nword embedding is widely adopted in the litera-\nture (Zou et al., 2013). The patch embedding is\ndirectly obtained through ResNet101 (He et al.,\n2016), which produces a fixed length visual fea-\nture. We then learn a linear projection to map it\nto the d-dimensional embedding space. The hyper\nembedding is randomly initialized for each Hyper-\ntoken. The type embedding is added to indicate\nwhich type the token belongs to, i.e. Hyper or Lo-\ncal. The modality embedding is used to distinguish\nbetween different modalities, e.g., ‘Title’, ‘Descrip-\ntion’, ‘Image’ etc. Note that all the embeddings in\nour approach are trainable. The word embeddings\nare initialized from the pre-trained language model.\n3.4 SMARTAVE Encoder\nThe SMARTAVE encoder contains a stack of K\nidentical encoder layers, which bridges the Hyper\nand Local tokens from multiple modalities with\nstructured attention patterns, and generates effec-\ntive contextual representations of the product and\nattribute. To better capture the information con-\ntained in different modalities, we design four at-\ntention patterns. First, Hyper-to-Hyper attention\nthat encodes the relations among different Hyper-\ntokens. Second, Hyper-to-Local attention, which\nconnects the Hyper token with its corresponding\nLocal-tokens. Third, Local-to-Hyper attention that\npasses the information from the Hyper-tokens to\nthe Local-tokens. Fourth, Local-to-Local atten-\ntion that learns contextual embeddings from other\nLocal-tokens within the same modality.\nHyper-to-Hyper Attention The Hyper-to-Hyper\nattention is designed to model the relations among\ndifferent modalities, which essentially computes\nthe attention weights among the Hyper-tokens and\npropagates the information from one modality to\nanother. In real-world applications, the total num-\nber of modalities nfor a product is usually small,\ne.g., n ≤20. Therefore, we adopt the full atten-\ntion mechanism among the Hyper-tokens, i.e., each\nHyper-token is able to attend to all other Hyper-\ntokens. Formally, given the Hyper-token embed-\nding XH, the Hyper-to-Hyper full attention is de-\nfined as:\nAH2H = softmax\n(\nXHWH2H\nQ (XHWH2H\nK )T\n√d\n)\nwhere WH2H\nQ and WH2H\nK are learnable weight ma-\ntrices. dis the embedding dimension.\nHyper-to-Local Attention There are multiple pos-\nsible choices for designing the Hyper-to-Local at-\ntention. For example, a straightforward solution\nis to enable the attention of a Hyper-token to all\nLocal-tokens. However, the computational cost\ngrows linearly with the length of the total input\n(i.e., the number of Local-tokens), which is very\nexpensive for large input sequences. Therefore, we\nadopt local attention by restricting the Hyper-to-\nLocal attention of a Hyper-token only on the Local-\ntokens that belong to it. For example, in Figure 2,\nthe ‘Title’ Hyper-token only directly attends to the\ntext tokens within the product title. The informa-\ntion contained in Local-tokens from other modali-\n266\nties, e.g., a patch-token in ‘Image’, will be propa-\ngated to the ‘Title’ Hyper-token through ‘Title’-to-\n‘Image’ and ‘Image’-to-‘patch’ attentions. Denote\nthe Local-token embeddings as XL, the Hyper-to-\nLocal restricted attention is defined as:\nAH2Lij =softmax\n(\nXHi WH2LQ (XLj WH2LK )T\n√d\n)\n, forj∈Mi\nwhere WH2L\nQ and WH2L\nK are weight matrices in\nHyper-to-Local attention.\nLocal-to-Hyper Attention In Local-to-Hyper at-\ntention, each Local-token communicates with every\nHyper-token, which enables the Local-token to re-\nceive the high-level representation from these sum-\nmarization tokens of each modality. For example\nin Figure 2, each visual Local-token attends to all\nHyper-tokens. The definition of the Local-to-Hyper\nattention is similar to the above Hyper-to-Local at-\ntention except that each Local-token attends to all\nHyper-tokens.\nLocal-to-Local Attention The Local-to-Local at-\ntention is the traditional attention mechanism used\nin various existing Transformer models (Devlin\net al., 2019; Dosovitskiy et al., 2021; Wang et al.,\n2022), which learns contextual token embeddings\nfrom the input sequence. In our design, to reduce\nthe computational cost, we only allow Local-to-\nLocal attention between two Local-tokens from the\nsame modality. The connections between Local-\ntokens from two different modalities can be natu-\nrally bridged through the structured attention. Note\nthat the efficiency can be further improved by\nadopting a relative attention pattern with relative\nposition encoding (Shaw et al., 2018, 2019).\nFinal Attention The final token representation can\nbe computed based on the above structured atten-\ntion mechanism among Hyper and Local tokens.\nThe output embeddings for Hyper and Local tokens\nZH,ZL are calculated as follows:\nZH = AH2H(XHWH\nV ) +AH2L(XLWL\nV )\nZL = AL2L(XLWL\nV ) +AL2H(XHWH\nV )\nwhere all the attention weights A are described\nabove. WH\nV and WL\nV are the learnable matrices\nto compute the values for Hyper and Local tokens\nrespectively. Intuitively, the Hyper-tokens are up-\ndated through the Hyper-to-Hyper full attention\nand Hyper-to-Local restricted attention. The Local-\ntoken embedding is learned via Local-to-Hyper\nfull attention and Local-to-Local attention. These\nstructured attention patterns effectively connect the\ntokens from different modalities, enabling the in-\nteractions across modalities efficiently.\n3.5 Extraction Layer\nThe extraction layer of SMARTAVE outputs the fi-\nnal value for the attribute. We apply a Transformer\ndecoder (Vaswani et al., 2017) on the output embed-\ndings of the ‘Attribute’ token to generate attribute\nvalue. We also employ a copy mechanism (See\net al., 2017) to allow both copying words from in-\nput text sequence, and generating words from a\npredefined vocabulary during decoding. To further\nimprove the learned embedding, we supplement\nwith an auxiliary task by extracting the text spans\nfrom the text modalities via sequential tagging (Xu\net al., 2019) as shown in Figure 2. More technical\ndetails are provided in Appendix A.\n3.6 Discussion\nThis section provides discussion that connects\nSMARTAVE with previous methods. If we re-\nmove the Hyper-to-Local and Local-to-Hyper atten-\ntions and re-organize the Local-to-Local (individ-\nual encoder for each modality) and Hyper-to-Hyper\n(fusion layer), our model architecture degenerates\nto the multimodal approaches (Zhu et al., 2020;\nLin et al., 2021). If we further trim the input by\nonly keeping and concatenating a few text sources\n(e.g., title and description), our model is similar to\nthose methods that only use the product text fea-\ntures (Wang et al., 2020; Yan et al., 2021; Yang\net al., 2022; Zhang et al., 2022). Moreover, if we\ncontinue replacing the Transformer with LSTM,\nour model is similar to the traditional sequential\ntagging approaches (Zheng et al., 2018; Xu et al.,\n2019).\n4 Experiments\n4.1 Datasets\nWe evaluate our method on two multimodal at-\ntribute value extraction datasets, MEPAVE (Zhu\net al., 2020) and MAVE (Yang et al., 2022).\nMEPA VE1 is a multimodal product attribute value\nextraction dataset with product title and image, col-\nlected from a mainstream Chinese e-commerce\nplatform. It contains 87,194 text-image instances\nconsisting of seven categories of products with\n26 different attributes such as ‘Material’, ‘Collar\nType’, ‘Color’, etc. The dataset is split into train-\n1https://github.com/jd-aig/JAVE\n267\ning, validation and testing sets with 71,194, 8,000\nand 8,000 instances respectively.\nMA VE2 is a large, multi-sourced, diverse dataset\nfor product attribute extraction study, which con-\ntains 3 million attribute value annotations across\n1257 fine-grained categories created from 2.2 mil-\nlion cleaned Amazon product profiles (Ni et al.,\n2019). In our experiments, we select 8 root cat-\negories ‘Amazon Fashion’, ‘All Beauty’, ‘Appli-\nances’, ‘Books’, ‘Grocery and Gourmet Food’,\n‘Home and Kitchen’, ‘Sports and Outdoors’, ‘Toys\nand Games’, and randomly sample 4k products\nfor each category, resulting in total 105K attribute-\nvalue instances. We further split them into training,\nvalidation and testing sets with 85k, 10k and 10k\ninstances respectively. More details on the datasets\nare provided in Appendix B.\n4.2 Baselines\nOur model is compared with six state-of-the-art\nattribute value extraction baselines, including four\ntext only methods and two multimodal methods.\nSUOpenTag (Xu et al., 2019) uses two BiLSTMs\nto produce separate embeddings for the context and\nthe attribute, followed by a CRF layer.\nA VEQA(Wang et al., 2020) adopts the BERT en-\ncoder to jointly encode the attribute and the product\ntext profile.\nAdaTag (Yan et al., 2021) applies adaptive decod-\ning that is able to extract multi-attribute values.\nMA VEQA(Yang et al., 2022) extends the BERT\nencoder to deal with multiple text sources.\nMJA VE(Zhu et al., 2020) utilizes two encoders\nfor text and image separately and predicts product\nattributes and extract values.\nPAM (Lin et al., 2021) incorporates text descrip-\ntions, OCR texts and visual modalities from the\nproduct and fuses the three modalities into a multi-\nmodal Transformer.\n4.3 Implementation\nOur model is implemented using PyTorch, and is\ntrained on 64 NVIDIA Tesla V100 GPUs. Dur-\ning training, we use the gradient descent algorithm\nwith Adam (Kingma and Ba, 2015) optimizer. Dur-\ning inference, we conduct beam search with beam\nwidth 5. The details of all hyper-parameters are\nreported in Appendix C.\nFollowing previous works, we use precision, re-\ncall and F1 score as evaluation metrics denoted as P,\n2https://github.com/\ngoogle-research-datasets/MAVE\nMethods MEPA VE MA VE\nSUOpenTag (Xu et al., 2019) 77.12±0.62 81.74±0.54\nA VEQA (Wang et al., 2020) 89.15±0.47 89.20±0.32\nAdaTag (Yan et al., 2021) 81.36±0.54 86.19±0.46\nMA VEQA (Yang et al., 2022) 88.71±0.38 90.06±0.29\nMJA VE (Zhu et al., 2020) 87.17±0.43 88.84±0.55\nPAM (Lin et al., 2021) 89.68±0.51 91.51±0.37\nSMARTAVE-text 89.21 ±0.42 91.16±0.41\nSMARTAVE 91.52±0.45 93.69±0.34\nTable 1: Overall performance comparison (F1 scores\n%) with standard deviation on both datasets. Results are\nstatistically significant with p-value < 0.001.\nR and F1. We follow Exact Match (Rajpurkar et al.,\n2016) criteria to compute the scores. We repeat\neach experiment 10 times and report the metrics\nbased on the average over these runs.\n5 Results\n5.1 Overall Results\nSMARTAVE outperforms the state-of-the-art\nattribute value extraction methods on both\ndatasets. Table 1 presents the main comparison re-\nsults on the two datasets. There are several key ob-\nservations. First, the multimodal models SMAR-\nTAVE and PAM achieve better results over the text-\nonly methods on both datasets, which demonstrates\nthe usefulness of the information from product im-\nages for attribute value extraction. We also observe\nthat MJA VE does not perform well compared to\nthe sophisticated text-only methods A VEQA and\nMA VEQA. Our hypothesis is that MJA VE only\nuses a pre-trained BERT to encode the text without\nbetter fine-tuning. Second, our model significantly\noutperforms all other multimodal models. For ex-\nample, the F1 score ofSMARTAVE increases over\n2.38% and 5.46% compared with PAM and MJA VE\non the MA VE dataset. There are two main rea-\nsons: 1) Our model adopts a structured attention\nmechanism which jointly encodes the product in-\nformation from all modalities, while existing mul-\ntimodal methods use individual encoders for each\nmodality and fail to capture their connections ef-\nfectively. 2) Our model efficiently handles long\ntext sequences with the structured modeling, while\nPAM and MJA VE simply concatenate the text from\ndifferent modalities. Third, our text-only model,\nSMARTAVE-text, achieves the best performance\namong all text-only methods. We believe it is also\nattributed to the advantage of the structured mod-\neling, which allows different text modalities to ex-\nchange information for learning better cross-modal\n268\nBrand Shape Color TypeMethods P R F 1 P R F 1 P R F 1 P R F 1\nSUOpenTag (Xu et al., 2019)91.25 91.33 91.29 84.36 78.86 81.52 80.84 72.53 76.46 74.08 68.25 71.04\nA VEQA (Wang et al., 2020)96.71 97.13 96.92 93.62 87.78 90.61 89.66 90.17 89.91 85.31 83.25 84.27\nAdaTag (Yan et al., 2021) 95.26 94.21 94.73 91.68 90.60 91.14 88.23 85.09 86.63 84.37 82.88 83.62\nMA VEQA (Yang et al., 2022)96.48 97.76 97.11 93.70 88.35 90.95 89.81 91.15 90.47 85.27 84.12 84.69\nMJA VE (Zhu et al., 2020) 95.52 94.72 95.12 91.94 89.48 90.69 90.02 90.75 90.38 84.17 82.69 83.42\nPAM (Lin et al., 2021) 96.39 96.85 96.62 93.63 89.91 91.73 92.41 92.33 92.37 85.01 83.15 84.07\nSMARTA VE 96.87 97.84 97.35 94.55 91.38 92.94 93.39 92.80 93.09 84.82 84.70 84.76\nTable 2: Attribute level performance comparison on MA VE.\nF1\n86\n88\n90\n92\n94\nMEPAVE MAVE\nw/o Title w/o Description w/o Specification w/o OCR\nw/o Image All\nFigure 3: Importance of different modalities.\ncontextual embeddings.\n5.2 Attribute Level Results\nSMARTAVE generally outperforms the base-\nlines on all attributes, with particularly large im-\nprovements over certain attributes.We conduct a\nfine-grained comparison of SMARTAVE with the\nbaselines on the MA VE dataset using four selected\nattributes, ‘Brand’, ‘Shape’, ‘Color’ and ‘Type’.\nThese attributes are the most common attributes\nacross multiple categories. The attribute level com-\nparison results are reported in Table 2. From these\nresults, we can see that SMARTAVE achieves the\nbest performance, in terms of F1 scores, among\nall methods on all attributes. However, when com-\nparing the improvements of SMARTAVE with the\ntext-only methods, it is clear that the improvements\non ‘Brand’ and ‘Type’ are marginal compared to\nthe improvements on ‘Shape’ and ‘Color’. Simi-\nlar observation is found in (Zhu et al., 2020). The\nreason is that product image is more useful on a\ncertain group of visual related attributes, such as\n‘Color’ and ‘Shape’.\n6 Analysis and Discussion\n6.1 Importance of Different Modalities\nWhile the text product profile contains the most\nimportant information sources for attribute\nvalue extraction, OCR texts and visual infor-\nmation from the product image are also valu-\nF1\n82\n84\n86\n88\n90\n92\n94\nMEPAVE MAVE\nw/o Local-to-Local w/o Hyper-to-Local w/o Local-to-Hyper\nw/o Hyper-to-Hyper All\nFigure 4: Importance of different attention patterns.\nable sources that boost the extraction perfor-\nmance. To understand the impact of different\nmodalities, we conduct an ablation study by re-\nmoving each modality from our model. Note that\nfor the MEPA VE dataset, it only contains product\ntitle, OCR text and image modalities. The results\nare illustrated in Figure 3. It can be seen that texts\nplay a crucial role for attribute value extraction\ntasks. Among all text modalities, ‘Title’ is the\nmost important source, which is consistent with\nour expectation. Moreover, it is also clear that the\nvisual feature helps improve the extraction on both\ndatasets. Another interesting observation is that the\nOCR text modality is less powerful on MEPA VE\ncompared with the results on MA VE. The reason\nis that the number of products containing the OCR\ntext is very small in the MEPA VE dataset.\n6.2 Impact of Different Attention Patterns\nDifferent attentions have different impacts to\nthe model performance, while SMARTAVE\nwith all attentions achieves the best result.In this\nablation study, we evaluate the model performance\nby eliminating different attention patterns. Con-\ncretely, we train four additional models by remov-\ning one attention pattern per model. The results\nof these four models and SMARTAVE with all\nattentions on both datasets are shown in Figure 4.\nFirst, we observe that the performance drops signif-\nicantly without the Local-to-Local attention. This\n269\nSMARTAVE # Parameters MEPA VE MA VE\nEncoder-2L 51M 89.13 91.45\nEncoder-6L 95M 90.68 92.57\nEncoder-12L-share 113M 90.44 92.62\nEncoder-12L 161M 91.52 93.69\nEncoder-24L 282M 91.71 93.77\nDecoder-2L 143M 91.38 93.54\nDecoder-4L 161M 91.52 93.69\nDecoder-12L 241M 91.61 93.73\nTraining time 161M 53m 1h 14m\nTable 3: Model performance (F1) over different encoder\nand decoder configurations.\nis because the Local-to-Local attention is used to\nlearn the contextual embeddings for local text and\nimage tokens, which is the fundamental component\nof our model. We also observe clear performance\ndrops, around 1 to 3 percent in terms of F1 score,\nif removing one of the other three attention pattern.\nThis observation validates that these structured at-\ntentions are crucial for extracting attribute values\nfrom multiple modalities. Nevertheless, it is clear\nthat SMARTAVE with all attentions achieves the\nbest performance on both datasets.\n6.3 Impact of Different Model Configurations\nSMARTAVE with a 12-layer encoder and a 4-\nlayer decoder obtains reasonable performance-\nscale trade-off. We conduct a series of\nperformance-scale studies on SMARTAVE. The\nSMARTAVE base model uses a 12-layer encoder\nwith a 4-layer decoder. We evaluate the model\nperformance with a different number of encoder\nlayers in {2L, 6L, 12L-share, 12L, 24L}. The 12L-\nshare encoder means sharing the query and key\nmatrices in different attention patterns. We further\nevaluate our model by only varying the number of\ndecoder layers in {2L, 4L, 12L}. The F1 results\nof these models are shown in Table 3. It is not\nsurprising to see that Encoder-24L and Decoder-\n12L achieve the best performances. However, a\nlarger model usually requires both longer training\nand inference time, while the SMARTAVE model\nwith a 12-layer encoder and a 4-layer decoder per-\nforms reasonably well. The training time of the\nbase model is also reported in Table 3.\n6.4 Varying Maximal Sequence Length\nSMARTAVE efficiently handles large input se-\nquences. We evaluate our model by varying the\nmaximal input sequence length from {64, 128, 256,\n512, 1024, 2048} on MA VE (as input sequences in\nMEPA VE are all very short) . Note that the baseline\nMax Sequence Length\nF1\n91\n92\n93\n94\n64 2048\nMAVE\nFigure 5: Model performances with different maximal\nsequence lengths.\nα\nF1\n91.0\n92.5\n94.0\n0 2 4 6 8 10\nMEPAVE MAVE\nFigure 6: Impact of multi-task learning.\nTransformer methods, such as (Wang et al., 2020;\nLin et al., 2021), require a significant amount of\ntime to train a model with input length beyond\n512, as they simply feed the concatenated text into\nthe standard Transformer. On the other hand, our\nmodel adopts the structured attention mechanism\nwhich dramatically reduces the computational cost.\nThe model performance with respect to the maxi-\nmal sequence length is shown in Figure 5. It is clear\nthat the performance of SMARTAVE improves as\nmaximal sequence length increases, and saturates\naround 1024.\n6.5 Impact of Multi-task Learning\nSequential tagging task generally improves the\nmodel performance. To understand the impact of\nthe auxiliary sequential tagging task, we conduct a\nset of experiments by varying the weight parameter\nα(see Appendix A) from {0, 0.1, 0.6, 1, 10}. We\nillustrate the model performance with different val-\nues of αin Figure 6. Note that α = 0 essentially\nstands for only applying the decoding task. It can\nbe seen from the Figure that adding the sequential\ntagging task helps improve the attribute value ex-\ntraction on both datasets. The model performance\nis relatively stable on a wide range of α.\n270\n7 Conclusions\nThis paper presents a novel Structured Multimodal\nTransformer model for Product Attribute Value Ex-\ntraction. A structured attention mechanism is de-\nsigned to encode the product information from\nmultiple modalities. These structured attention\npatterns enable effective and efficient interactions\namong the text and visual tokens from different\nproduct modalities. The attribute values are then\nextracted based on the learned embeddings from\nthe structured encoder. Evaluations are conducted\non two multimodal datasets, which show the su-\nperior performance of the proposed approach over\nseveral state-of-the-art methods. Ablation studies\nalso demonstrate the effectiveness of the structured\nattention patterns in modeling products with multi-\nmodal data and large input sequences.\nLimitations\nThere are two limitations of our current approach.\nFirst, our model focuses on attribute value extrac-\ntion for a single product. However, there are a few\ncases where an attribute contains multiple different\nvalues corresponding to different parts of a product.\nFor example, for a ‘Lamp’ product, it has a ‘white’\nlampshade with a ‘blue’ lamp holder. In this sce-\nnario, our model might be confused and extract an\nincorrect or partial value for the attribute ‘Color’.\nSecond, our model generates attribute-dependent\nproduct embeddings. In other words, we need to\nrun the model inference once for each attribute to\nextract its values, even for a same product. This\nmay increase the inference time for real-world ap-\nplications, especially for a product with a large\nnumber of attributes. To alleviate this problem, we\nare working on jointly encoding multiple attributes\nassociated with one product in the SMARTAVE\nmodel.\nReferences\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 268–284. Association for Computational Lin-\nguistics.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\n2018. Bottom-up and top-down attention for image\ncaptioning and visual question answering. In 2018\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018, pages 6077–6086. Computer Vi-\nsion Foundation / IEEE Computer Society.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. CoRR,\nabs/2004.05150.\nKe Chen, Lei Feng, Qingkuang Chen, Gang Chen, and\nLidan Shou. 2019. EXACT: attributed entity extrac-\ntion by annotating texts. In Proceedings of the 42nd\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR\n2019, Paris, France, July 21-25, 2019, pages 1349–\n1352. ACM.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a fixed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n2978–2988. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nXin Luna Dong, Xiang He, Andrey Kan, Xian Li, Yan\nLiang, Jun Ma, Yifan Ethan Xu, Chenwei Zhang,\nTong Zhao, Gabriel Blanco Saldana, Saurabh Desh-\npande, Alexandre Michetti Manduca, Jay Ren, Suren-\nder Pal Singh, Fan Xiao, Haw-Shiuan Chang, Giannis\nKaramanolakis, Yuning Mao, Yaqing Wang, Chris-\ntos Faloutsos, Andrew McCallum, and Jiawei Han.\n2020. Autoknow: Self-driving knowledge collection\nfor products of thousands of types. In KDD ’20: The\n26th ACM SIGKDD Conference on Knowledge Dis-\ncovery and Data Mining, Virtual Event, CA, USA,\nAugust 23-27, 2020, pages 2724–2734. ACM.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In 2016 IEEE Conference on Computer Vision\n271\nand Pattern Recognition, CVPR 2016, Las Vegas,\nNV , USA, June 27-30, 2016, pages 770–778. IEEE\nComputer Society.\nRonghang Hu, Amanpreet Singh, Trevor Darrell, and\nMarcus Rohrbach. 2020. Iterative answer prediction\nwith pointer-augmented multimodal transformers for\ntextvqa. In 2020 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2020,\nSeattle, WA, USA, June 13-19, 2020, pages 9989–\n9999. Computer Vision Foundation / IEEE.\nGuanhuan Huang, Xiaojun Quan, and Qifan Wang.\n2022. Autoregressive entity generation for end-to-\nend task-oriented dialog. In Proceedings of the 29th\nInternational Conference on Computational Linguis-\ntics, pages 323–332, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguis-\ntics.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-\nrectional LSTM-CRF models for sequence tagging.\nCoRR, abs/1508.01991.\nRobert L. Logan IV , Samuel Humeau, and Sameer\nSingh. 2017. Multimodal attribute extraction. In 6th\nWorkshop on Automated Knowledge Base Construc-\ntion, AKBC@NIPS 2017, Long Beach, California,\nUSA, December 8, 2017. OpenReview.net.\nGiannis Karamanolakis, Jun Ma, and Xin Luna Dong.\n2020. Txtract: Taxonomy-aware knowledge extrac-\ntion for thousands of product categories. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 8489–8502. Association for\nComputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nRongmei Lin, Xiang He, Jie Feng, Nasser Zalmout,\nYan Liang, Li Xiong, and Xin Luna Dong. 2021.\nPAM: understanding product images in cross product\ncategory attribute extraction. In KDD ’21: The 27th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Virtual Event, Singapore, August\n14-18, 2021, pages 3262–3270. ACM.\nHanqing Lu, Youna Hu, Tong Zhao, Tony Wu, Yiwei\nSong, and Bing Yin. 2021. Graph-based multilingual\nproduct retrieval in e-commerce search. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies: Industry\nPapers, NAACL-HLT 2021, Online, June 6-11, 2021,\npages 146–153. Association for Computational Lin-\nguistics.\nAjinkya More. 2016. Attribute extraction from product\ntitles in ecommerce. CoRR, abs/1608.04670.\nThanh V . Nguyen, Nikhil Rao, and Karthik Subbian.\n2020. Learning robust models for e-commerce prod-\nuct search. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 6861–6869.\nAssociation for Computational Linguistics.\nJianmo Ni, Jiacheng Li, and Julian J. McAuley. 2019.\nJustifying recommendations using distantly-labeled\nreviews and fine-grained aspects. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 188–197. Association for Com-\nputational Linguistics.\nDuangmanee Putthividhya and Junling Hu. 2011a.\nBootstrapped named entity recognition for product\nattribute extraction. In Proceedings of the 2011 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2011, 27-31 July 2011, John\nMcIntyre Conference Centre, Edinburgh, UK, A meet-\ning of SIGDAT, a Special Interest Group of the ACL,\npages 1557–1567. ACL.\nDuangmanee Putthividhya and Junling Hu. 2011b.\nBootstrapped named entity recognition for product\nattribute extraction. In Proceedings of the 2011 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2011, 27-31 July 2011, John\nMcIntyre Conference Centre, Edinburgh, UK, A meet-\ning of SIGDAT, a Special Interest Group of the ACL,\npages 1557–1567. ACL.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nChloe Hillier, and Timothy P. Lillicrap. 2020. Com-\npressive transformers for long-range sequence mod-\nelling. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nThe Association for Computational Linguistics.\nShaoqing Ren, Kaiming He, Ross B. Girshick, and Jian\nSun. 2017. Faster R-CNN: towards real-time object\ndetection with region proposal networks. IEEE Trans.\nPattern Anal. Mach. Intell., 39(6):1137–1149.\nOhad Rozen, David Carmel, Avihai Mejer, Vitaly\nMirkis, and Yftah Ziser. 2021. Answering product-\nquestions by utilizing questions from other contex-\ntually similar products. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2021, Online,\nJune 6-11, 2021, pages 242–253. Association for\nComputational Linguistics.\n272\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1073–1083.\nAssociation for Computational Linguistics.\nPeter Shaw, Philip Massey, Angelica Chen, Francesco\nPiccinno, and Yasemin Altun. 2019. Generating log-\nical forms from graph representations of text and\nentities. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 95–106. Association for\nComputational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT, New Orleans, Louisiana, USA, June\n1-6, 2018, Volume 2 (Short Papers), pages 464–468.\nAssociation for Computational Linguistics.\nKeiji Shinzato, Naoki Yoshinaga, Yandi Xia, and Wei-\nTe Chen. 2022. Simple and effective knowledge-\ndriven query expansion for qa-based product attribute\nextraction. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 227–234. Association for\nComputational Linguistics.\nAmanpreet Singh, Vivek Natarajan, Meet Shah,\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. 2019. Towards VQA models that\ncan read. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 8317–8326. Com-\nputer Vision Foundation / IEEE.\nHao Tan and Mohit Bansal. 2019. LXMERT: learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 5099–\n5110. Association for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022. Efficient transformers: A survey. ACM\nComput. Surv.\nQuoc-Tuan Truong, Tong Zhao, Changhe Yuan, Jin Li,\nJim Chan, Soo-Min Pantel, and Hady W. Lauw. 2022.\nAmpsum: Adaptive multiple-product summarization\ntowards improving recommendation captions. In\nWWW ’22: The ACM Web Conference 2022, Virtual\nEvent, Lyon, France, April 25 - 29, 2022, pages 2978–\n2988. ACM.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nQifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xi-\naojun Quan, and Dongfang Liu. 2022. Webformer:\nThe web-page transformer for structure information\nextraction. In WWW ’22: The ACM Web Confer-\nence 2022, Virtual Event, Lyon, France, April 25 - 29,\n2022, pages 3124–3133. ACM.\nQifan Wang, Li Yang, Bhargav Kanagal, Sumit Sanghai,\nD. Sivakumar, Bin Shu, Zac Yu, and Jon Elsas. 2020.\nLearning to extract attribute value from product via\nquestion answering: A multi-task approach. In KDD\n’20: The 26th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, Virtual Event, CA,\nUSA, August 23-27, 2020, pages 47–55. ACM.\nHuimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang, and\nMan Lan. 2019. Scaling up open tagging from tens\nto thousands: Comprehension empowered attribute\nvalue extraction from product title. In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n5214–5223. Association for Computational Linguis-\ntics.\nJun Yan, Nasser Zalmout, Yan Liang, Christan Grant,\nXiang Ren, and Xin Luna Dong. 2021. Adatag:\nMulti-attribute value extraction from product profiles\nwith adaptive decoding. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021, pages 4694–4705. Association for\nComputational Linguistics.\nLi Yang, Qifan Wang, Zac Yu, Anand Kulkarni, Sumit\nSanghai, Bin Shu, Jon Elsas, and Bhargav Kanagal.\n2022. MA VE: A product dataset for multi-source\nattribute value extraction. In WSDM ’22: The Fif-\nteenth ACM International Conference on Web Search\nand Data Mining, Virtual Event / Tempe, AZ, USA,\nFebruary 21 - 25, 2022, pages 1256–1265. ACM.\nSanshi Yu, Zhuoxuan Jiang, Dongdong Chen, Shan-\nshan Feng, Dongsheng Li, Qi Liu, and Jinfeng Yi.\n2021. Leveraging tripartite interaction information\nfrom live stream e-commerce for improving prod-\nuct recommendation. In KDD ’21: The 27th ACM\nSIGKDD Conference on Knowledge Discovery and\nData Mining, Virtual Event, Singapore, August 14-18,\n2021, pages 3886–3894. ACM.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntañón, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big bird: Trans-\nformers for longer sequences. In Advances in Neural\n273\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nWenxuan Zhang, Yang Deng, Jing Ma, and Wai Lam.\n2020. Answerfact: Fact checking in product question\nanswering. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 2407–2417. Association for Computational\nLinguistics.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI-\nBERT: document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. In Proceedings of the 57th Conference of the As-\nsociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 5059–5069. Association for Com-\nputational Linguistics.\nXinyang Zhang, Chenwei Zhang, Xian Li, Xin Luna\nDong, Jingbo Shang, Christos Faloutsos, and Jiawei\nHan. 2022. Oa-mine: Open-world attribute mining\nfor e-commerce products with weak supervision. In\nWWW ’22: The ACM Web Conference 2022, Virtual\nEvent, Lyon, France, April 25 - 29, 2022, pages 3153–\n3161. ACM.\nJie Zhao, Ziyu Guan, and Huan Sun. 2019. Riker: Min-\ning rich keyword representations for interpretable\nproduct question answering. In Proceedings of the\n25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, KDD 2019,\nAnchorage, AK, USA, August 4-8, 2019, pages 1389–\n1398. ACM.\nGuineng Zheng, Subhabrata Mukherjee, Xin Luna\nDong, and Feifei Li. 2018. Opentag: Open attribute\nvalue extraction from product profiles. In Proceed-\nings of the 24th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining,\nKDD 2018, London, UK, August 19-23, 2018, pages\n1049–1058. ACM.\nTiangang Zhu, Yue Wang, Haoran Li, Youzheng Wu,\nXiaodong He, and Bowen Zhou. 2020. Multimodal\njoint attribute prediction and value extraction for e-\ncommerce product. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 2129–2139. Association for Computa-\ntional Linguistics.\nWill Y . Zou, Richard Socher, Daniel M. Cer, and\nChristopher D. Manning. 2013. Bilingual word em-\nbeddings for phrase-based machine translation. In\nProceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2013, 18-21 October 2013, Grand Hyatt Seattle, Seat-\ntle, Washington, USA, A meeting of SIGDAT, a Spe-\ncial Interest Group of the ACL, pages 1393–1398.\nACL.\n274\nA More Technical Details\nWe provide more technical details on our SMAR-\nTAVE in this section.\nInput Layer In the input layer, each Hyper-token\nand Local-token will be mapped to an embedding\nvector. The embeddings for the Hyper-tokens are\nrandomly initialized EH = {EH\n1 ,...,E H\nn }. For\nthe text Local-tokens, they are initialized from a\npre-trained model Ei,L = {Ei,L\n1 ,...,E i,L\nmi }. For\nthe visual Local-tokens, each patch wi\nj is con-\nverted/mapped into a d-dimensional embedding\nvector with a projection matrix Wp, i.e., Ei,L\nj =\nWpwi\nj. We use EL = {E1,L,...,E n,L}to repre-\nsent the embeddings of all Local-tokens.\nSMARTAVE Encoder The SMARTAVE en-\ncoder is a stack of Kidentical layers:\nXk = SMARTA VE(Xk−1), 1 ≤k≤K\nwhere X0 = {EH,EL}is the input embedding for\nthe first layer. Each SMARTAVE encoder layer\ncontains a structured attention layer followed by a\nstandard feed forward network:\nZk = Attention(Xk−1), Xk = FFN(Zk)\nThe Attention layer uses the structured attention\nmechanism described in the main paper. We now\npresent the full format of all attentions.\nHyper-to-Hyper Attention The Hyper-to-Hyper\nfull attention is formulated as:\nαH2H\nij =\nexp(eH2H\nij )∑\nℓ exp(eH2H\niℓ ), for1 ≤j ≤n\neH2H\nij =\nxH\ni WH2H\nQ (xH\nj WH2H\nK )T\n√\nd\nwhich is equivalent to the compact matrix format\nin the main paper.\nHyper-to-Local Attention The Hyper-to-Local\nrestricted attention is formulated as:\nαH2L\nij =\nexp(eH2L\nij )∑\nℓ∈Mi exp(eH2L\niℓ ), forj ∈Mi\neH2L\nij =\nxH\ni WH2L\nQ (xL\nj WH2L\nK )T\n√\nd\nLocal-to-Hyper Attention The Local-to-Hyper\nattention is formulated as:\nαL2H\nij =\nexp(eL2H\nij )∑\nℓ exp(eL2H\niℓ ), for1 ≤j ≤n\neL2H\nij =\nxL\ni WL2H\nQ (xH\nj WL2H\nK )T\n√\nd\nLocal-to-Local Attention The Local-to-Local\nconstrained attention is formulated as:\nαL2L\nij =\nexp(eL2L\nij )∑\nℓ∈Mi exp(eL2L\niℓ ), forj ∈Mi\neL2L\nij =\nxL\ni WL2L\nQ (xL\nj WL2L\nK )T\n√\nd\nFinal Attention The final computation of ZH\nand ZL can be written as:\nzH\ni =\n∑\n1≤j≤n\nαH2H\nij xH\nj WH\nV +\n∑\nℓ∈Mi\nαH2L\niℓ xL\nk WL\nV\nzL\ni =\n∑\nj∈Mi\nαL2L\nij xL\nj WL\nV +\n∑\n1≤ℓ≤n\nαL2H\nij xH\nk WH\nV\nExtraction Layer The attribute value decoder is\na standard Transformer decoder, which consumes\nthe embedding of ‘Attribute’ Hyper-token from\nthe SMARTAVE encoder and generates the value\nword by word:\n¯wt = arg max\nwt\n(softmax(WvXt\nde))\nwhere Xt\nde is the decoder output at word position\nt. Wv is the output matrix which projects the final\nembedding to the logits of vocabulary size. As\nmentioned in the main paper, we further employ\na copy mechanism to allow copying words from\ninput text sequence via pointing. For the sequential\ntagging component, we pass the embeddings of\neach text sequence into the CRF layer (Xu et al.,\n2019; Yan et al., 2021) to predict tags for each\ntoken in {B,I,O,E }. The total loss is defined as:\nL= LD + αLSeq\nwhere LD is the decoder loss and LSeq is the se-\nquential tagging loss. αis a hyper-parameter.\n275\nCategory # Product # Instance # Attribute # Value\nClothes 12,240 34,154 14 1,210\nShoes 9,022 20,525 10 1,036\nBags 3,376 8,307 8 631\nLuggage 1,291 2,227 7 275\nDresses 4,567 12,283 13 714\nBoots 713 2,090 11 322\nPants 2,832 7,608 13 595\nTotal 34,041 87,194 26 2,129\nTable 4: The statistics of the MEPA VE dataset.\nCategory # Product # Instance # Attribute # Value\nAmazon Fashion 4k 16.6k 18 1.1kAll Beauty 4k 17.3k 23 1.3kAppliances 4k 12.5k 16 0.8kBooks 4k 12.7k 9 0.8kGrocery and Gourmet Food 4k 9.8k 12 0.7kHome and Kitchen 4k 11.2k 15 0.8kSports and Outdoors 4k 12.7k 16 0.9kToys and Games 4k 12.2k 15 1.0k\nTotal 32k 105k 65 3.6k\nTable 5: The statistics of the MA VE dataset.\nB Dataset\nData Processing For the MEPA VE dataset, it al-\nready has the product image associated with each\nproduct. For the MA VE dataset, they do not di-\nrectly provide the product images. We join the prod-\nuct ‘id’ from MA VE with the product ‘asin’ from\nthe original Amazon Review Data 3 to obtain the\nhigh-resolution product images. We further remove\nthe text sources that directly represent the attribute\nand value. For example, {’source’: ’brand’, ’text’:\n’Kalso’}. We believe this is one of the main rea-\nsons why A VEQA and MA VEQA methods achieve\nvery high scores on the original MA VE dataset. For\nboth datasets, in order to compute the sequential\ntagging loss, we match the attribute values with the\nOCR texts to generate the tags on the OCR text\nsequences (for other text modalities, both datasets\nprovide full tag annotations).\nStatistics The statistics of both datasets are\nshown in Table 4 and 5.\nC Implementation Details\nThe language of the texts on the datasets are differ-\nent. MEPA VE contains Chinese product profiles,\nwhile MA VE has English product profiles. More-\nover, the text characteristics are also very different\nfor these two datasets. MEPA VE only includes the\nproduct title, with very limited OCR texts. The\n3https://nijianmo.github.io/amazon/\nindex.html\nParameters MEPA VE MA VE\nencoder layers 12 12\nencoder heads 12 12\nencoder hiden size 768 768\nencoder hidden units (FFN) 3,072 3,072\nmax input sequence length 64 1024\ndecoder layer 4 4\ndecoder heads 6 6\ndecoder hiden size 768 768\ndecoder hidden units (FFN) 3,072 3,072\nmax output sequence length 10 10\nbeam width 5 5\nbatch size 128 32\ntraining epochs 28 20\noptimizer Adam Adam\nlearning rate schedule linear decay linear decay\nlearning rate 2e−5 2e−5\nlearning rate warmup steps 2,000 2,000\nvocab Chinese vocab from MJA VE BERT-base\nvocab size 2,772 30,522\nα 0.6 0.6\nTable 6: Model Hyper-parameters details.\nbatch size MEPA VE MA VElearning rate MEPA VE MA VE\n16 91.46 93.62 1e−5 91.62 92.95\n32 91.42 93.69 2e−5 91.52 93.69\n64 91.48 93.21 4e−5 91.17 93.24\n128 91.52 - 8e−5 91.03 92.86\n512 90.85 - 2e−4 90.80 92.51\nTable 7: F1 results with different batch sizes and learn-\ning rates on both datasests.\nmaximal number of Chinese words/tokens in the\ndataset is 56. On the other hand, MA VE consists\nof multiple text modalities with much larger text\nlength (can even go over 1024). Therefore, the vo-\ncabularies used for different datasets are different,\nas well as certain other parameters. The model\nparameters used for both datasets are provided in\nTable 6.\nD Impact of Batch Size and Learning\nRate\nTo evaluate the model performance with different\ntraining batch sizes and learning rates, we conduct\nexperiments to train a set of SMARTAVE mod-\nels with a hyper-parameter sweep consisting of\nlearning rates in {1e−5,2e−5,4e−5,8e−5,2e−4}\nand batch sizes in 16, 32, 64, 128, 512 on both\ndatasets. The F1 results with different learning\nrates and batch sizes are reported in Table 7. Note\nthat for MA VE, we are not able to train on large\nbatch size, i.e., 128 and 512, as the maximal input\nsequence length is set to 1024. It can be seen from\nthe table that smaller batch size and learning rate\nusually lead to better model performance.\n276",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.761236846446991
    },
    {
      "name": "Encoder",
      "score": 0.5004322528839111
    },
    {
      "name": "Product (mathematics)",
      "score": 0.47162628173828125
    },
    {
      "name": "Information retrieval",
      "score": 0.4413527250289917
    },
    {
      "name": "Transformer",
      "score": 0.44109997153282166
    },
    {
      "name": "Product design",
      "score": 0.4305006265640259
    },
    {
      "name": "Modalities",
      "score": 0.4191628396511078
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4153205156326294
    },
    {
      "name": "Feature extraction",
      "score": 0.41219162940979004
    },
    {
      "name": "Data mining",
      "score": 0.369861900806427
    },
    {
      "name": "Natural language processing",
      "score": 0.33567318320274353
    },
    {
      "name": "Mathematics",
      "score": 0.11313760280609131
    },
    {
      "name": "Engineering",
      "score": 0.10617485642433167
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 11
}