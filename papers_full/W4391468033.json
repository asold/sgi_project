{
  "title": "Multiscale Fusion CNN-Transformer Network for High-Resolution Remote Sensing Image Change Detection",
  "url": "https://openalex.org/W4391468033",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101526812",
      "name": "Ming Jiang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5001599513",
      "name": "Yimin Chen",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5028515644",
      "name": "Zhe Dong",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5118743983",
      "name": "Xiaoping Liu",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5037230519",
      "name": "Xinchang Zhang",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5100629508",
      "name": "Honghui Zhang",
      "affiliations": [
        "Guangdong Hydropower Planning & Design Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2036798369",
    "https://openalex.org/W3142421496",
    "https://openalex.org/W2012935382",
    "https://openalex.org/W2750248129",
    "https://openalex.org/W2157026765",
    "https://openalex.org/W3130754787",
    "https://openalex.org/W1979061792",
    "https://openalex.org/W2741377155",
    "https://openalex.org/W2132222679",
    "https://openalex.org/W4213124617",
    "https://openalex.org/W2153633422",
    "https://openalex.org/W4362636814",
    "https://openalex.org/W3201623325",
    "https://openalex.org/W4296079303",
    "https://openalex.org/W4367281499",
    "https://openalex.org/W4283776488",
    "https://openalex.org/W2891248708",
    "https://openalex.org/W2171590421",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W4285298122",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4318767292",
    "https://openalex.org/W2395611524",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4381730127",
    "https://openalex.org/W2951991161",
    "https://openalex.org/W2996290406",
    "https://openalex.org/W4225648150",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W4385863649",
    "https://openalex.org/W3009942016",
    "https://openalex.org/W3027225766",
    "https://openalex.org/W3036453075",
    "https://openalex.org/W4376481113",
    "https://openalex.org/W4385574936",
    "https://openalex.org/W4226512186",
    "https://openalex.org/W4292348075",
    "https://openalex.org/W4313021481",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W4312549298",
    "https://openalex.org/W4319866415",
    "https://openalex.org/W4226361741",
    "https://openalex.org/W4384393272",
    "https://openalex.org/W4368232711",
    "https://openalex.org/W4324360044",
    "https://openalex.org/W3217004489",
    "https://openalex.org/W4362653113",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2805152403",
    "https://openalex.org/W3176330035",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2908510526"
  ],
  "abstract": "Accurate change detection using remote sensing data is crucial for understanding surface dynamics. Despite the impressive success of current convolutional neural network (CNN)-based techniques, their feature extraction and representation capabilities are limited, leading to pseudochanges and omissions. To address this issue, we propose a multiscale fusion CNN-transformer network (MSFCTNet), which incorporates the strengths of CNN and transformer to improve change detection capability. The network utilizes a Siamese CNN to extract features from the bi-temporal image pairs and then combines multiscale features using a CNN-transformer hybrid structure to extract global and local features. In the decode stage, a gated attention module is used to filter the extracted features layer by layer. Moreover, before outputting the prediction results, a feature refinement head is employed to further refine the features, suppress background noise, and improve detection capability. LEVIR-CD, SVCD, and SYSU-CD datasets are used to evaluate the proposed network. Experimental results indicate that MSFCTNet outperforms other state-of-the-art change detection methods, proving the potential of MSFCTNet for change detection tasks.",
  "full_text": "1 \n> < \n \nMulti-Scale Fusion CNN-Transformer Network  \nfor High-Resolution Remote Sensing Image  \nChange Detection \n \nMing Jiang, Yimin Chen, Zhe Dong, Xiaoping Liu, Member, IEEE, Xinchang Zhang, and Honghui Zhang \n \n Abstractâ€”Accurate change detection using remote sensing \ndata is crucial for understanding surface dynamics.  Despite the \nimpressive success of current convolutional neural network \n(CNN)-based techniques, their feature extraction and \nrepresentation capabilities are limited, leading to pseudo-changes \nand omissions.  To address this issue, we propose a multi -scale \nfusion CNN -Transformer network (MSFCTNet), which \nincorporates the strengths of  CNN and Transformer to improve \nchange detection capability. The network utilizes a Siamese CNN \nto extract features from the bi -temporal image  pairs and then \ncombines multi -scale features using a CNN -Transformer hybrid \nstructure (HCTM) to extract global and local features. In the \ndecode stage, a gated attention module (GAM) is used to filter the \nextracted features layer by layer. Moreover, before outputting \nthe prediction results, a feature refinement head (FRH) is \nemployed to further refine the features, suppress background \nnoise, and improve detection capability.  LEVIR-CD, SVCD, and \nSYSU-CD datasets are used to evaluate the proposed network. \nExperimental results indicate that MSFCTNet outperforms other \nstate-of-the-art change detection methods, proving the potential \nof MSFCTNet for change detection tasks. \n \nIndex Terms â€”High-resolution remote sensing image , change \ndetection, Transformer, multi-level aggregation , attention \nmechanism.  \nI. INTRODUCTION \nHANGE detection is to identify differences between \ntemporal remote sensing images that cover the same \narea at different periods [1]. Techniques for change \ndetection have been widely used in various applications such \nas resource surveys [2], urban expansion [3], and disaster \nassessment [4]. With the advance of remote sensing \ntechnology, many images with higher spatial and spectral \nresolutions are now available [5]. However, detecting changed \n \nThe research was supported by the National Natural Science Foundation of \nChina (Grant No. 42371406, 42271415, and 42322110) and the Guangdong \nNatural Science Funds for Distinguished Young Scholar (Grant No. \n2021B1515020104). (Corresponding author:  Yimin Chen).  \nMing Jiang, Yimin Chen, Zhe Dong, and Xiaoping Liu  are with the \nGuangdong Provincial Key Laboratory of Urbanization and Geo -Simulation, \nSchool of Geography and Planning, Sun Yat -Sen University, Guangzhou \n510275, China (e -mail: jiangm56@mail2.sysu.edu.cn; \nchenym49@mail.sysu.edu.cn; dongzh3@mail.sysu.edu.cn; \nliuxp3@mail.sysu.edu.cn).  \nXinchang Zhang is with the School of Geography and Remote Sensing, \nGuangzhou University, Guangzhou 510006, China ( e-mail: \nzhangxc@gzhu.edu.cn). \nHonghui Zhang is with the Guangdong Guodi Planning Science \nTechnology Co., Ltd, China (e-mail: 249536073@qq.com) \nareas from images is challenging due to various interfering \nfactors like illumination conditions and sensor noise [6]. \nWhile manual interpretation is costly, developing reliable \nmethods for automatically detecting meaningful changes \nbecomes important and necessary. \nTraditional change detection methods are generally divided \ninto pixel -based and object -based methods [5]. Pixel -based \nmethods compare the pixel values between bi-temporal images \nand generate a difference map using change detection \nalgorithms. Common methods include principal component \nanalysis [7], slow feature analysis [8], and change vector \nanalysis [9]. Pixel -based change detection methods are \nrelatively simple to perform, but they pay more attention to the \nspectral changes of individual pixels . In general, pixels are \nrelated to each other, and focusing only on a single pixel tends \nto ignore the spatial context information between neighboring \npixels, resulting in poor robustness and susceptibility to image \nnoise [10]. Object -based methods segment the images based \non the features of shape, texture, and spectral, and generate the \nchange map by comparing the segmented objects [11]. \nCompared to pixel -based methods, object -based methods can \nincorporate both spectral and spatial contextual information of \nimages, but they involve complex and challenging processes \nof manual feature design and exhibit weaker performance in \nrepresenting high -level features [12]. In addition, errors in \nobject segmentation can also affect the change detection \nperformance. \nDue to the stronger potential demonstrated by Deep \nLearning (DL) in computer vision tasks, DL has also been \napplied to high -resolution remote sensing image tasks, for \nexample, image classification [13], semantic segmentation \n[14], and change detection [15]. Compared to traditional \nmethods, DL does not rely on prior knowledge, thereby \navoiding complex manual involvement, while also having the \ncharacteristic of hierarchical structure and stronger feature \nrepresentation capability [16]. Compared to traditional \nmethods, DL -based change detection methods perform better \n[10]. These methods are generally divided into early fusion \nand late fusion based on the bi -temporal image input fusion \nstrategy [17]. Early fusion typically involves concatenating or \ndifferencing the bi-temporal images, and the network structure \nis similar to traditional semantic segmentation networks. Late \nfusion typically uses a Siamese network [18] structure as the \nencoder, which can directly take the bi -temporal images as \nnetwork input. \nC \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> < \n \nIn recent years, convolutional neural networks (CNN) [19] \nhave been prevalent in change detection tasks due to their \nability to extract image features using convolutional structures \nand hierarchical feature representations. Compared to \ntraditional machine learning methods, CNN can extract more \nfeature information from images. However, although the \nconvolutional operation in CNN is useful for extracting the \nlocal features, these CNN -based approaches limit the \nnetwork's receptive field and cannot capture features related to \nthe global context [20]. In general , both the local and the \nglobal features are crucial for change detection tasks. Local \nfeatures preserve spatial details in images, assisting models in \nrecovering the position and boundary information of targets. \nGlobal features provide context information, helping the \nnetwork accurately recognize the semantic information of \npixels. Recently, Transformer [21] has shown great potential \nin global context modeling. It has facilitated various vision -\nrelated tasks, such as image classification [22], object \ndetection [23], and other segmentation tasks [24]. The self -\nattention mechanism in the Transformer structure enhances the \nnetwork's ability to model long -range dependencies, \ncontributing to the extraction of more discriminative features. \nAlthough DL-based methods have made great strides, there \nare still some unresolved issues.  For example, the insufficient \ninteraction between the hierarchical features extracted by the \nencoder weakens the model's ability to detect targets of \ndifferent sizes, and the small targets are often missing by the \ncommon DL-based change detection methods . Moreover, \nbecause of the limited receptive fields, features extracted by \nthe common DL-based change detection methods lack \nrepresentativeness and cause inferior detection performance \n[25]. To address these issues, in this study, we propose a \nmulti-scale fusion CNN -Transformer network (MSFCTNet) \nfor high -resolution remote sensing image change detection. \nSpecifically, MSFCTNet first uses a Siamese network to \nextract hierarchical features from bi-temporal images. Then, to \nenhance the connection between multi -scale features, the \nhybrid CNN -Transformer module (HCTM) is employed to \nextract global -local features of multi -scale features. \nFurthermore, to narrow the gap between the high-level and the \nlow-level features, the gated attention module (GAM) is \nutilized to select the encoder's multi-level features. Finally, the \nfeature refinement head (FRH) is employed to refine the \nfeatures of the decoder that outputs the final result. The main \ncontributions of this study can be summarized as follows: \n1) We propose a novel end -to-end change detection \nmethod that combines hybrid CNN and Transformer \nstructure. Unlike typical Transformer -based methods, \nwe integrate the local feature extraction capabilities \nof convolution and explore the advantages of multi -\nscale feature fusion. In addition, we employ attention \nmechanisms to reduce information redundancy. \nQuantitative and qualitative results demonstrate the \nsuperiority of our proposed method. \n2) Based on the fusion of multi-level encoder features, \nthe HCTM is proposed to use CNN to extract local \nfeatures with the global modeling capability of \nTransformer to obtain the global-local features and \nenhance the semantic representation of features. \n3) We propose the GAM, which can effectively filter the \nencoder features and narrow the semantic gap \nbetween high- and low-level features through the \ngated mechanism. \n4) We propose FRH to further refine the features in both \nchannel and spatial domains to obtain highly accurate \ndetection results. \nII. RELATED WORKS \nA. CNN-Based Methods \nCompared to traditional machine learning methods, CNN \nhas advantages in image feature extraction and is extensively \napplied in change detection. CNN -based change detection \nmethods mainly employ a fully convolutional network (FCN) \n[26] with an encode-decode framework to generate the results. \nDaudt et al . [17] proposed three basic network frameworks \nbased on UNet [27]. However, the simple structure of UNet \nlimits its ability to detect multi -scale change targets. To \naddress this issue, some studies have proposed multi -scale \nfeature fusion methods. Zhao et al . [28] proposed a multi -\nfeature interaction network that enhances the association \nbetween features.  Peng et al. [2 9] adopted an early fusion \nstrategy, combining UNet++ [ 30] with a multiple side -outputs \nfusion (MSOF) strategy to enhance the performance of change \ndetection. Fang et al. [6] proposed a dense connection network \nbased on the Siamese UNet++. Li et al. [3 1] proposed a full -\nscale feature fusion network based on UNet3+ [3 2] to fuse \nfeatures of different scales.  Ye et al . [33] proposed a network \ncombining UNet3+ and 3D -CNN to enhance the extraction \nand fusion of feature information.  Other studies incorporate \nattention mechanisms to improve detection capability. Chen et \nal. [3 4] incorporated a dual attention mechanism into the \nchange detection network to improve the model's \ndiscriminative ability towards pseudo -changes. Chen and Shi \n[35] introduced a spatio -temporal attention mechanism to \nbetter capture the spatio -temporal relationships of image \nfeatures. Zhang et al. [3 6] developed a deep supervision \ntraining network with attention mechanisms to enhance feature \nrepresentation. Zhou et al. [37] proposed a context extraction \nmodule to capture long -range contextual information within \nimages, enhancing the network's ability to interact with feature \ninformation. Zhang et al. [38] proposed an attention -guided \nedge refinement network that utilizes attention mechanisms to \naggregate contextual information and employs an edge \nrefinement module to enhance the edges of the predicted \nresults. The change detection methods based on multi -scale \nfeature fusion and attention mechanisms have improved the \naccuracy to some extent. However, these CNN -based methods \nare limited by the local correlation of convolutions, which has \ninherent limitations in modeling global dependencies. \nTherefore, CNN -based methods may not perform well in \ncomplex scenes. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \n> < \n \n \nFig. 1. Overview of the proposed MSFCTNet. \nB. Transformer-based methods \nRecently, Transformer has made impressive progress in the \nNLP field. Due to its excellent performance, scholars have \nalso applied Transformer to computer vision tasks [3 9]. In the \nfield of remote sensing, the Transformer is useful for change \ndetection tasks [40], [41]. For example, Chen et al. [ 42] used \nthe Transformer to model the features extracted by CNN. \nGedara Chaminda Bandara and Patel [ 43] employed a \nhierarchical Transformer structure as an encoder, and a multi -\nlayer perception (MLP) decoder was used to fuse features to \nobtain the final result.  Xu et al [4 4] proposed a multi -feature \naggregation network based on Transformer.  Li et al. [ 45] \nproposed a network that combines UNet and Transformer  that \nexploits the encod e-decode structure. Liu et al. [46] proposed \na CNN -Transformer network that enhances the interaction \ncapability of the bi -temporal image feature information.  Li et \nal. [47] proposed a CNN -Transformer network that integrates \nfeatures from multiple scales to improve the detection \ncapability. Despite its strengths in modeling global context, \nthe Transformer falls deficient in the extraction of local \ninformation [ 48]. In general Transformer -based change \ndetection networks, shallow features are often ignored, \nresulting in rougher boundaries of the detected changed areas \nand the omission of small targets. Therefore, in this study, we \nintegrate the CNN -Transformer structure to capture \ncomprehensive features from a global -local perspective \nthrough multi -scale feature fusion. In addition, an attention \nmechanism is employed to refine the features and improve the \nmodel's robustness. As a result, our method extracts more \nprecise boundaries for changed areas and demonstrates \nsuperior detection performance, especially for small targets. \nIII. METHODOLOGY \nA. Overview \nThe proposed MSFCTNet is an encoder -decoder network \nas shown in Fig. 1. The encoder adopts a Siamese structure , \nand t he network consists of four encoder residual modules \n(ERM) that produce four different sizes of feature maps with \nchannel sizes of 32, 64, 128, and 256, respectively. The image \npairs are fed to obtain four layers of features, and then the four \nlayers of features are processed to have the same sizes (same \nas the size of the fourth stage) through pooling and \nconvolution operations, followed by concatenation and \nconvolution operations to obtain fusion features, which are \nthen passed to the HCTM. This is for better utilizing the low - \nand high -level features. The HCTM, composed of CNN and \nTransformer structures, can extract features more \ncomprehensively. Then GAM filters the features from the four \nstages. Next, in the decoder, feature fusion is performed using \na decoder convolution module (DCM), which consists of 1Ã— 1 \nconvolutions, batch normalization (BN), and rectified linear \nunit (ReLU). Finally, the FRH is utilized to refine the features \nand produce the change maps. \n \n \nFig. 2. Structure of the proposed ERM. \nB. Encoder Residual Module \nThe residual module structure of the encoder is shown in \nFig. 2. Firstly, the input features are passed through a \nconvolutional layer (Conv), followed by a BN and a ReLU \nactivation. Then, another combination of Conv and BN is \nadded. Finally, add the result of the second BN to the result of \nthe first Conv, and the output is obtained by using another \nReLU. The size of the kernels in the Conv is 3Ã— 3. The use of \nresidual connections and activation function layers aims to \nexpedite network learning and avoid the issues of gradient \nvanishing. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \n> < \n \n \nFig. 3. Structure of the proposed HCTM. \nC. Hybrid CNN-Transformer Module \nIn g eneral, in large -size areas or complex scenes, global \ninformation is relatively easier to distinguish changing \ninformation, while local information plays a key role in \nrecovering detailed information such as the boundaries of the \narea. Therefore, in this study, based on the characteristics of \nCNN and Transformer structure, two parallel branches, i.e., \nglobal and local, are established to extract global context and \nlocal detail information, respectively (Fig. 3(a)). This module \nis an improvement from the standard Transformer module. \nThe spatial  reduction (SR) method [24] is further used to \nreduce the computational complexity . Besides, based on the \noriginal self -attention branch, the CNN branch is added to \nextract local features, to produce the global -local features.  \nFurthermore, to fully exploit the multi -scale features, the \npooling operation is used to unify the dimensions, and then the \nconcatenation and convolution operations are implemented to \ngenerate the fused features, as shown in Fig. 3(b), where ð‘“ð‘ ð‘– \ndenotes the features at the ð‘– th stage and ð‘“ð‘“ð‘¢ð‘ ð‘’  denotes the \nfused features. Then, the ð‘“ð‘“ð‘¢ð‘ ð‘’  are fed to HCTM to produce \nthe global-local features. \n1) Global Branch : We employ a Transformer \narchitecture to build the global branch, aiming to capture the \nglobal semantic information of the features.  Given a tensor \nð‘“ð‘“ð‘¢ð‘ ð‘’  of shape  ð» Ã— ð‘Š Ã— ð¶, we first use layer normalization \n(LN) for normalization. The projections of the query (query, \nQ), key (key, K), and value (value, V) are then generated using \nthree 1Ã— 1 convolutional layers. Next, the projections of Q, K, \nand V are transformed so that they maintain the same shape \nNÃ—C, where N is the sequence length. We use SR to reduce the \ncomputational complexity. Specifically, it uses a reduction \nrate R to reshape the dimensions of K and V into (N/R)Ã—CÃ—R, \nthen the dimension is reduced to C using the linear projection \nprocess. The relevant equations are as follows: \n \n( ) (Re ( , ) ) SSR x LN shape x R W=  (1) \nð‘†ð‘…(ð‘¥) is the operation that reduces the spatial dimension of \nthe input sequence  ð‘¥  (i.e., ð¾  or ð‘‰ ), ð‘Šð‘   denotes linear \nprojection. Next, a dot product is applied to ð‘„  and ð¾  to \ngenerate an attention map of size ð‘ Ã— (ð‘/ð‘…)  using the \nSoftmax activation function , followed by multiplying the \nattention map by V. Finally, the dimension of the obtained \nattention feature map is transformed to ð» Ã— ð‘Š Ã— ð¶  and the \nglobal features are obtained using 1Ã— 1 convolution and BN. \nThe specific calculations are as follows: \n \n( , , ) max( )\nT\nhead\nQKAttention Q K V Soft V\nd\n=  (2) \n \n( , , )i i i ihead Attention Q K V=  (3) \n \n( ( ( ,..., )))gnf BN Conv Concat head headï‚´= 1 1 1  (4) \nð‘‘â„Žð‘’ð‘Žð‘‘ denotes the number of channels per head; ð‘“ð‘” denotes the \nglobal feature; ð‘›  denotes the total number of heads; â„Žð‘’ð‘Žð‘‘ð‘–  \ndenotes the ð‘–th head; and ð¶ð‘œð‘›ð‘£1Ã—1 denotes 1Ã— 1 convolution. \n2) Local Branch: As mentioned earlier, convolution \noperations have demonstrated powerful capabilities in local \nfeature extraction, and depth -wise convolutions, with fewer \nparameters compared to traditional convolutions, are widely \nused in change detection tasks [ 49, 50]. Inspired by this, we \nincorporate depth-wise convolutions into the local branch.  The \nlocal feature extraction branch consists of two 1Ã— 1 \nconvolutions and one 3Ã— 3 depth-wise convolution. The former \n1Ã— 1 convolution is to increase the number of feature channels \n(expansion factor Î³=2), while the latter is to decrease the \nnumber back to the original input dimension, thus obtaining \nlocal feature ð‘“ð‘™ . Using depth -wise convolution for local \ninformation extraction can effectively reduce computational \ncosts. In addition, similar to residual networks, inserting \nresidual connections based on depth -wise convolution can \nimprove the feature propagation ability across layers. \n3) Global-Local Feature Fusion : The feature \nrepresentations of the global and the local branches are \ndifferent because of the differences in the methods of feature \nextraction, i.e., the self -attention method used in the global \nbranch and the convolution method used in the local branch. \nTherefore, the resulting global and local features cannot be \nfused by simply summing or concatenating them. In this study, \nwe define a weight coefficient  that is utilized for weighted \nsum operation  to learn more comprehensive fusion features. \nThe equation for the weighted sum operation is as follows: \n \n()gl g lf f f ï¡ï¡= ï‚´ + âˆ’ ï‚´1  (5) \nwhere ð‘“ð‘”ð‘™ denotes fusion features; ð›¼ denotes trainable weight \ncoefficients. \nThe fused global -local features are further aggregated with \na 1Ã— 1 convolution and BN, and then residual concatenation is \napplied to the original features ( ð‘“ð‘“ð‘¢ð‘ ð‘’ ). Finally, the feature \nvalues are selectively activated using ReLU. The specific \ncalculation is as follows: \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> < \n \n \nRe ( ( onv ( )) )out gl fusef LU BN C f f ï‚´=+ 11  (6) \nwhere ð‘“ð‘œð‘¢ð‘¡ denotes the final features obtained. \nD. Gated Attention Module \nIn previous studies, integrating multi -scale features is \neffective in improving detection performance. Low -level \nfeatures contain more information related to locations where \nchanges occur and the associated spatial details, but they are \nnoisier and have less semantic meaning. High -level features \nhave more semantic information, but they are often coarse \nwith few details. Based on the above analysis, we propose the \nGAM to guide low -level features with high -level features, \nnarrowing the semantic gap between them and enabling more \neffective feature fusion. As shown in Fig. 4, high -level \nfeatures are upsampled to match the size of low -level features, \nand then the channel number is reduced to match that of the \nlow-level features through a 1Ã— 1 convolution and BN. \nSubsequently, channel weights are obtained through the \nsqueeze-and-excitation (SE) attention mechanism [51], and \nthey are multiplied with the low -level features to select \ninformative features. The selected features are added to the \noriginal low -level features, and then they are further fused \nwith the original high -level features after another 1Ã— 1 \nconvolution and BN. Finally, the output features are obtained \nthrough a ReLU activation function. The SE is a method of \ndetermining weights in the channel dimension by performing \nsqueeze and excitation operations on the feature maps, \nallocating weights among different channels for feature \nselection. Specifically, squeeze integrates global contextual \ninformation through global pooling. Excitation employs two \nfully connected (FC) layers. The first FC uses ReLU and the \nsecond FC uses the sigmoid to determine the weights among \ndifferent channels. In g eneral, to reduce computational \ncomplexity, the first FC in SE performs channel dimension \nreduction. \n \nFig. 4. Structure of the proposed GAM. \nE. Feature Refinement Head \nThe FRH uses the attention mechanism to select and filter \nfeature information by assigning weights in both channel and \nspatial domains, aiming to improve the effectiveness of \nextracting target features, and improving detection accuracy. \nAs shown in Fig. 5, ð», ð‘Š , and ð¶  represent the height, the \nwidth, and the number of channels of the input features, \nrespectively, while ð‘Ÿ  represents the reduction factor of the \nchannel dimension. The FRH consists of two branches: the \nchannel attention module (CAM) and the spatial attention \nmodule (SAM). The CAM first performs maximum pooling \n(MaxPool) and average pooling (AvgPool) on the input \nfeatures in the spatial dimension to aggregate global \ninformation. These features are then passed through the 1Ã— 1 \nconvolution for channel dimensionality reduction, followed by \nthe ReLU layer. The SAM first uses a 1Ã— 1 convolution to \nreduce the channel dimension and generate the local \ninformation of the features. After a ReLU layer, the SAM \nfeatures are concatenated with the CAM features to obtain \nglobal information and facilitate the fusion of local and global \ninformation. Then, the channel and spatial branches utilize a \n1Ã— 1 convolution and a 7Ã— 7 convolution, respectively, and the \nSigmoid activation is used to produce the attention feature \nmaps in both dimensions. The input features are multiplied by \nthe obtained attention feature maps to generate the channel \nand the spatial dimension -refined features. The refined \nfeatures in both dimensions are then added together. Finally, a \n1Ã— 1 convolution and a BN layer are used to further fuse the \nfeatures. The residual connection structure is employed to \nconnect the features. The refined features are selectively \nactivated by a ReLU layer, and the final prediction results are \nobtained by a 3Ã— 3 convolution layer. \n \nFig. 5. Structure of the proposed FRH. \nF. Loss Function \nThe task investigated in this study is binary change \ndetection by comparing the bi -temporal images. Considering \nthat change sample imbalance is common, we use a hybrid \nloss function [6], [33], [48]  consisting of binary cross -entropy \n(BCE) loss and dice loss, which are defined as follows: \n \nlog( ) (1 )log(1 )bce i i i ip p ploss t = âˆ’ âˆ’ âˆ’ âˆ’  (7) \n \n1 (2 ) / ( )dice i i iipploss t t= âˆ’ +  (8) \nwhere ð‘¡ð‘– denotes the label of pixel ð‘–, 0 denotes unchanged, and \n1 denotes changed. ð‘ð‘–  denotes the predicted result. The hybrid \nloss function is defined as: \n \nhybird bce diceloss loss loss=+  (9) \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \n> < \n \nIV. EXPERIMENTAL SETTINGS \nA. Dataset description \nIn this study, we employ the LEVIR -CD [35], SVCD [52], \nand SYSU-CD [53] datasets to evaluate the methods, and the \nsamples of these datasets are shown in Fig. 6. The LEVIR -CD \ndataset consists of 637 pairs of images with a resolution of 0.5 \nm and a size of 1024 Ã—  1024 pixels, containing a variety of \nbuilding change types. To facilitate training , the images are \ncropped to a pixel size of 256 Ã—  256 with no overlap. \nFollowing the original dataset division rules, the cropped \ntraining set, validation set, and test set contain 7120, 1024, and \n2048 pairs of images, respectively. \nThe SVCD dataset consists of seven pairs of season -\nvarying images of 4725 Ã—  2200 pixels and four pairs of \nseason-varying images of 1900 Ã—  1000 pixels, with resolutions \nvarying from 0.03 m to 1 m. This dataset contains the change \ntypes of different targets. After processing, 16000 pairs of \nimages of size 256Ã— 256 pixels are obtained, which includes \n10000 pairs for the training set, 3000 pairs for the validation \nset, and 3000 pairs of images for the testing set respectively. \nThe SYSU-CD dataset contains 20000 pairs of images with \na size of 256Ã— 256 pixels and a resolution of 0.5 m. It includes \nvarious land cover change types in the Hong Kong region \nfrom 2007 to 2014. The training set, validation set, and test set \nof this dataset are set at a ratio of 6:2:2. \n \n \nFig. 6. Examples of bi-temporal images and labels. (a) LEVIR-CD. (b) SVCD. (c) SYSU-CD \n \nB. Implementation Details \nAll experiments were implemented using the PyTorch \nframework and ran on a single NVIDIA Quadro RTX 6000 \nGPU with 24 G memory . The batch size was set to 12, and \nAdamW [54] was used as the optimizer. The epoch number \nand learning rate were set to 120 and 0.0005, respectively. \nC. Evaluation Metric \nIn this study, the metrics of precision (P), recall (R), F1 -\nScore (F1), intersection over union (IoU), mean intersection \nover union (mIoU), and overall accuracy (OA) were selected \nfor accuracy assessment. The relevant equations are as \nfollows: \n \nTPP TP FP= +  (10) \n \nTPR TP FN= +  (11) \n \n21 PRF PR\nï‚´ï‚´= +  (12) \n \nTPIoU TP FP FN= ++  (13) \n \n0\n1\n1\nk\ni\nTPmIoU k FN FN TP=\n= + + +ïƒ¥  (14) \n \nOA TP TN\nTP FP TN FN\n+= +++  (15) \nwhere ð‘‡ð‘ƒ, ð¹ð‘ƒ, ð‘‡ð‘,and ð¹ð‘ denote the number of true positive, \nfalse positive, true negative, and false negative pixels, \nrespectively, and ð‘˜  denotes categories 0 and 1. \nD. Comparison Methods \nTo evaluate the effectiveness of MSFCTN et, several \nchange detection methods were selected for comparison, \nincluding FCEF [17], FCSC [17], FCSD [17], UNet++_MSOF \n[29], STANet [35], IFN [36], DSAMNet [53], SNUNet -CD \n[6], BIT [42], ChangeFormer [43]. \n1) FCEF, FCSC, FCSD:  Three basic fully convolutional \nchange detection methods. FCEF concatenate s the bi -\ntemporal images together and then fed to UNet to obtain \nthe predicted results. Besides, FCSC and FCSD adopt the \nlate fusion strategy, FCSC employs concatenation to \nprocess the features generated by the Siamese network, \nwhile FCSD employs differencing to process the \nfeatures. \n2) UNet++_MSOF: A UNet++-based early fusion strategy \nchange detection method, which incorporates MSOF \nstrategy to obtain the final predicted results. \n3) STANet: A Siamese network that introduc es the spatialâ€“\ntemporal attention module for more discriminative \nfeatures. \n4) IFN: A net work based on Siamese UNet structure, in \nwhich the attention mechanism is employed to refine \nmulti-scale features  during the decode stage.  The \nnetwork is trained using deep supervision. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> < \n \n5) DSAMNet: A Siamese network that utilizes metric \nmodule and deep supervision module to enhance the \nnetwork learning capability. \n6) SNUNet-CD: A Siamese UNet++-based structure that \nextracts multi -scale features using densely connected \nstructure and finally extracts representative semantic \nfeatures via ensemble attention module. \n7) BIT: A CNN and Transformer based method that uses \nSiamese CNN as the backbone network to extract \nfeatures and Transformer to enhance feature \nrepresentation. \n8) ChangeFormer: A Transformer based method, in which \nthe encoder uses the Transformer to extract the features \nand the decoder uses the MLP to fuse the multilevel \ndifference features and predict the result. \nNotably, we conducted our experiments under the same \nexperimental conditions, employing the identical model \nstructures as those described in the original papers for \ncomparison. \nV. RESULTS \nA. Results of LEVIR-CD dataset \nThe results of various change detection methods on the \nLEVIR-CD dataset are shown in Table I. In Table s I-â…¥, the \nbold values represent the best results of the column , all results \nare described using percentages . Table I shows that our \nproposed method achieves good performance, with optimal \nvalues in terms of F1, IoU, mIoU, and OA, reaching 91.02%, \n83.52%, 91.28%, and 99.10%, respectively, except for slightly \nlower precision and recall compared to some methods. The \nprecision of IFN and SNUNet -CD is higher than that of \nMSFCTNet, but both methods have lower recall, resulting in \nlower overall accuracy. In addition, the recall of MSFCTNet is \nslightly lower than DSAMNet, but MSFCTNet can \ncomprehensively consider precision and recall to achieve the \nbest F1, indicating that MSFCTNet is more feasible for the \ndataset. Overall, among the compared methods, IFN has the \nbest overall accuracy, while FCEF performs the worst. The \naccuracy of the Transformer -based methods BIT and \nChangeFormer is lower compared to some CNN methods \n(IFN, SNUNet -CD). While BIT utilizes Transformer for \nfeature modeling, it overlooks shallow features. On the other \nhand, ChangeFormer incorporates features from multiple \nlevels of Transformer structures but neglects the impact of \nCNN structure in extracting local features on the results.  \nAmong the three base methods (FCEF, FCSC, and FCSD), \nFCSD achieves the best performance. \nTo visually demonstrate the detection performance of \ndifferent methods, we visualize the results as shown in Fig. 7, \nwhich includes six different scenes. Figure 7 -10, to visualize \nthe results more intuitively, we use four colors to represent the \ndetection results: white for TP, red for FN, green for FP, and \nblack for TN. The visualization results show that our method \nis capable of accurately detecting the change regions in \nvarious building change scenes. As for small -scale building \nchanges (rows 1 and 2 of Fig. 7), most methods exhibit some \nfalse alarms and omissions due to the influence of lighting and \nseasonal factors. In contrast, MSFCTNet can detect such \nchanges relatively accurately. For large -scale building changes \n(rows 3, 4, and 5 of Fig. 7), some methods contain some \nsignificant omissions, such as FCEF, FCSC, and FCSD, while \nother methods can detect the change regions to some extent, \nbut false alarms and omissions still exist. As for densely \ndistributed building changes (row 6 of Fig. 7), MSFCTNet can \ndetect the edges of buildings more accurately, while the \nchanged areas detected by other methods have rougher edges. \nIn conclusion, it is evident from the images that our method \nprovides better details in the predicted change results with \nfewer false alarms and omissions. \nTABLE I \nQUANTITATIVE COMPARISONS ON THE LEVIR-CD DATASET \n \nMethod P R F1 IoU mIoU OA \nFCEF 88.24 86.88 87.56 77.87 88.27 98.74 \nFCSC 88.74 89.07 88.90 80.03 89.42 98.87 \nFCSD 90.30 88.03 89.14 80.42 89.64 98.91 \nUNet++_MSOF 90.47 89.76 90.11 82.01 90.48 99.00 \nSTANet 88.69 89.19 88.94 80.09 89.45 98.87 \nIFN 92.34 88.55 90.40 82.49 90.74 99.04 \nDSAMNet 87.65 91.82 89.68 81.30 90.08 98.92 \nSNUNet-CD 92.14 87.53 89.78 81.45 90.19 98.98 \nBIT 90.26 88.93 89.59 81.14 90.02 98.95 \nChangeFormer 90.19 87.92 89.04 80.25 89.55 98.90 \nMSFCTNet 92.06 90.00 91.02 83.52 91.28 99.10 \nB. Results of the SVCD dataset \nThe quantitative results of various change detection \nmethods on the SVCD dataset are shown in Table â…¡. The table \nshows that MSFCTNet achieves the best values for all \nevaluation metrics, indicating that our proposed method is \nmore robust for change detection compared to other methods. \nCompared to the method with sub -optimal values, MSFCTNet \nimproves the F1 and IoU metrics by 1.37% and 2.47% \nrespectively. Among the other comparative methods, \nSNUNet-CD achieves results second only to MSFCTNet, \nwhich is attributed to its adoption of the UNet++â€™s dense \nconnection structure and the effective feature selection using \nthe attention mechanism. FCEF obtains the poorest result, \nwhile methods FCSC and FCSD, which utilize the Siamese \nstructure, achieve better results. \nIn addition, the visualized results are shown in Fig. 8. In \ncomparison with the LEVIR -CD dataset, the SVCD dataset \nincludes not only changes in buildings but also changes in \nother scenes. From the Fig. 8, other methods exhibit \nsignificant omissions and false detections. FCEF, FCSC, and \nFCSD, due to their relatively simple structures, perform \npoorly in detection. Some CNN -based methods, such as \nSTANet, IFN, and SNUNet -CD, have limited receptive fields, \nleading to some omissions (rows 4 and 5 of Fig. 8). Among \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \n> < \n \nthe Transformer -based methods, BIT shows some \nimprovement in performance due to the utilization of \nTransformer structure. However, it lacks low -level detailed \ninformation, resulting in noisy detection results. \nChangeFormer utilizes both the Transformer structure and \nmulti-scale features, which perform well in detecting large \nareas (rows 5 and 6 of Fig. 8). However, it cannot extract local \ninformation, resulting in poor detection of small objects such \nas roads and cars (row 1, 2 and 3 of Fig. 8). MSFCTNet \nmitigates the impact of information loss on accuracy by \nemploying HCTM, which strengthens the connection between \nfeatures at different scales. Moreover, by combining gated and \nattention mechanisms, it effectively mitigates information \nredundancy and achieves higher accuracy compared to \nmethods employing similar strategies. It demonstrates better \ndetails and higher completeness in detecting changed areas, \nparticularly in complex scenes with occlusions (row 5 of Fig. \n8) and in detecting different scales (row 3 of Fig. 8), as well as \naccurately detecting roads (rows 1 and 2 of Fig. 8). In \nconclusion, our method produces more detailed results and \nhigher completeness in detecting changed areas. \n \n \nTABLE â…¡ \nQUANTITATIVE COMPARISONS ON THE SVCD DATASET \n \nMethod P R F1 IoU mIoU OA \nFCEF 91.17 80.79 85.67 74.93 85.61 96.66 \nFCSC 92.40 84.39 88.20 78.89 87.89 97.21 \nFCSD 93.35 85.93 89.49 80.98 89.10 97.51 \nUNet++_MSOF 95.31 87.37 91.17 83.77 90.71 97.91 \nSTANet 94.36 87.38 90.73 83.04 90.28 97.80 \nIFN 94.84 93.16 94.00 88.67 93.51 98.53 \nDSAMNet 96.22 89.95 92.98 86.88 92.50 98.32 \nSNUNet-CD 97.29 91.21 94.15 88.95 93.69 98.60 \nBIT 96.92 89.72 93.18 87.22 92.70 98.38 \nChangeFormer 96.22 91.18 93.63 88.03 93.15 98.47 \nMSFCTNet 97.70 93.43 95.52 91.42 95.10 98.92 \n \nFig. 7. Visualization results of different methods on the LEVIR -CD dataset. (a) Image T1, (b) Image T2, (c) Label, (d) FCEF, (e) \nFCSC, (f) FCSD, (g) UNet++_MSOF, (h) STANet, (i) IFN, (j) DSAMNet, (k) SNUNet -CD, (l) BIT, (m) ChangeFormer, (n) \nMSFCTNet.  \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> < \n \n \nFig. 8.  Visualization results of different methods on the SVCD dataset. (a) Image T1, (b) Image T2, (c) Label, (d) FCEF, (e) \nFCSC, (f) FCSD, (g) UNet++_MSOF, (h) STANet, (i) IFN, (j) DSAMNet, (k) SNUNet -CD, (l) BIT, (m) ChangeFormer, (n) \nMSFCTNet.  \n \nC. Results of the SYSU-CD dataset \nThe quantitative results of the SYSU-CD dataset are shown \nin Table â…¢. Similar to  the LEVIR -CD dataset, apart from \nslightly lower precision and recall compared to some methods, \nthe F1, IoU, mIoU, and OA metrics all have optimal values, \nreaching 79.54%, 66.03%, 77.15%, and 90.45% respectively. \nIFN achieves the highest precision at 87.58%, but its recall is \nonly 61.74%, resulting in a relatively lower overall accuracy. \nDSAMNet obtains the highest recall, but its precision is \ncomparatively lower. In contrast, MSFCTNet attains relatively \nhigher precision and recall, with the highest F1 score, \nsurpassing UNet++_MSOF, which achieves the second -best \nperformance, by 0.76%. For some Transformer -based methods \n(BIT, ChangeFormer), they outperform certain CNN methods \n(STANet, SNUNet -CD). Moreover, MSFCTNet incorporates \na hybrid structure of CNN and Transformer, yielding better \noverall performance. \nThe qualitative results are shown in Fig. 9, similar to the \nSVCD dataset, it also includes changes in other scenes. From \nthe figure, FCSC, FCSD, BIT, and ChangeFormer have many \nfalse detections (row 4 of Fig. 9), while IFN has many missed \ndetections (rows 2 and 3 of Fig. 9). In comparison, \nUNet++_MSOF and MSFCTNet achieve better detection \nresults, but UNet++_MSOF may have some small -scale area \nmissed detections (row 4 of Fig. 9). As can be seen in rows 1 \nand 2 of the figure,  the poor detection  of the comparison \nmethods is more prevalent under large -scale areas, while our \nmethod can detect the area contour information more finely. In \nconclusion, MSFCTNet can accurately detect the changed \nareas, with more precise detection of small targets and \nboundary details, and fewer pseudo -changes and omissions in \nthe changed areas. \n \nTABLE â…¢ \nQUANTITATIVE COMPARISONS ON THE SYSU-CD DATASET \n \nMethod P R F1 IoU mIoU OA \nFCEF 73.94 78.40 76.10 61.43 73.59 88.39 \nFCSC 74.57 78.00 76.24 61.61 73.78 88.54 \nFCSD 74.72 79.60 77.08 62.71 74.48 88.84 \nUNet++_MSOF 78.38 79.19 78.78 64.99 76.31 89.94 \nSTANet 73.93 78.63 76.20 61.56 73.67 88.42 \nIFN 87.58 61.74 72.43 56.77 71.90 88.91 \nDSAMNet 73.50 80.17 76.69 62.20 74.01 88.51 \nSNUNet-CD 80.05 75.24 77.57 63.36 75.44 89.74 \nBIT 80.47 76.77 78.57 64.71 76.33 90.13 \nChangeFormer 77.52 79.88 78.68 64.86 76.14 89.79 \nMSFCTNet 80.43 78.67 79.54 66.03 77.15 90.45 \n \nâ…¥. DISCUSSION \nA. Ablation Study \n1) Ablation Study on Network Modules:  To further verify \nthe effectiveness of different modules on the MSFCTNet, we \nconducted relevant ablation experiments on the SVCD dataset, \nthe quantitative results are shown in Table â…£. The â€œBaselineâ€ \nmodel does not come with  any modules  as the base \nframework. The â€œBaseline + HCTMâ€ model represents the \nâ€œBaselineâ€ model with HCTM. Table â…£ shows that the \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \n> < \n \n \nFig. 9. Visualization results of different methods on the SYSU -CD dataset. (a) Image T1, (b) Image T2, (c) Label, (d) FCEF, (e) \nFCSC, (f) FCSD, (g) UNet++_MSOF, (h) STANet, (i) IFN, (j) DSAMNet, (k) SNUNet -CD, (l) BIT, (m) ChangeFormer, (n) \nMSFCTNet.  \n \nâ€œBaselineâ€ model has the lowest F1 of 93.66%.  Compared to \nthe â€œBaselineâ€ model, the F1 of the â€œBaseline + HCTMâ€ \nmodel, â€œBaseline + GAMâ€ model, and â€œBaseline + FRHâ€ \nmodel have improved by 0.87%, 0.57%, and 0.71%, \nrespectively. The results provide preliminary evidence of the \neffectiveness of the three modules, with the HCTM providing \nthe greatest improvement to the model.  In addition, from the \ntable, the joint use of these modules further improves change \ndetection performance. Furthermore, b y combining all the \nthree modules, MSFCTNet outperforms other models, with an \nimprovement of 1.86% in F1 and 3.26% in IoU compared to \nthe â€œBaselineâ€ model. In conclusion, the ablation experiments \ndemonstrate the effectiveness of the HCTM, GAM, and FRH \nmodules, and the utilization of the modules can effectively \nimprove the performance of the method. \nThe visual comparison of the ablation experiments is \nshown in Fig. 10. The figure shows that small targets can be \ndetected better after adding HCTM, which facilitates the \ndetection of targets at different scales due to the fusion of \nmulti-scale features and the use of CNN-Transformer to obtain \nglobal-local features. As for GAM , the target boundary \ninformation shown in the figure is more fine -grained, and the \nutilization of the gated mechanism, which reduces the \nredundancy of low -level information, helps the recovery of \ntarget boundary information. In addition, based on rows 4 and \n5 of the figure, FRH reduces pseudo -changes in the predicted \nresults and improves the model's ability to recognize changed \nareas through the attention mechanism. Overall, MSFCTNet \nintegrates the advantages of the three modules and has \nsuperior performance in extracting both targets and boundaries \nwith fewer false negatives and false positives. \n \nTABLE â…£ \nABLATION STUDY ON NETWORK MODULES ON THE SVCD DATASET \n \nMethod P R F1 IoU mIoU OA \n Baseline 96.57 90.92  93.66 88.16 93.27 98.49 \nBaseline + HCTM 96.98 92.20 94.53 89.63 94.07 98.68 \nBaseline + GAM 97.09 91.52 94.23 89.08 93.76 98.62 \nBaseline + FRH 96.95 91.92 94.37 89.34 93.90 98.65 \nBaseline + HCTM + GAM 97.40 93.24 95.27 90.98 94.84 98.86 \nBaseline + HCTM + FRH 97.51 92.99 95.19 90.83 94.76 98.84 \nBaseline + GAM + FRH 97.49 92.35 94.86 90.22 94.41 98.76 \nMSFCTNet 97.70 93.43 95.52 91.42 95.10 98.92 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \n> < \n \n \nFig. 10.  Visualization results of ablation study on the SVCD dataset. (a) Image T1, (b) Image T2, (c) Label, (d) Baseline, (e) \nBaseline + HCTM, (f) Baseline + GAM, (g) Baseline + FRH, (h) Baseline + HCTM + GAM, (i) Baseline + HCTM + FRH, (j) \nBaseline + GAM + FRH, (k) MSFCTNet.  \n \nTABLE â…¤ \nABLATION STUDY ON THE HCTM ON THE SVCD DATASET \n \nGlobal Branch Local Branch P R F1 \nÃ— Ã— 97.49 92.35 94.86 \nÃ— âˆš 97.53 92.81 95.11 \nâˆš Ã— 97.66 93.02 95.28 \nâˆš âˆš 97.70 93.43 95.52 \n \nTABLE â…¥ \nABLATION STUDY ON THE FRH ON THE SVCD DATASET \n \nCBAM FRH P R F1 \nÃ— Ã— 97.40 93.24 95.27 \nâˆš Ã— 97.51 93.32 95.37 \nÃ— âˆš 97.70 93.43 95.52 \n \n2) Ablation Study on HCTM: We conducted experiments \nto validate the effectiveness of the global and local branches in \nHCTM. As shown in Table â…¤, using either the local branch or \nthe global branch in isolation leads to improved performance.  \nMoreover, using the global branch alone outperforms the local \nbranch alone, indicating that the self -attention mechanism in \nthe global branch offers better feature representation \ncapabilities. Integrating both the local and global branches \nfurther enhances the detection performance. This suggests that \nthe extraction of rich global -local features through parallel \nbranches contributes significantly to change detection. \n3) Ablation Study on FRH: To further verify the \neffectiveness of FRH, we compare it with the classical \nchannel-spatial attention mechanism CBAM  [55], and the \nexperimental quantitative results are shown in Table VI.  From \nthe table, it can be seen that the addition of attention \nmechanisms can effectively improve the performance of \nchange detection, and FRH performs better than CBAM in \nterms of improvement.  While CBAM adopts a sequential \nchannel-spatial branch, FRH utilizes parallel channel -spatial \nbranches, enhancing the interaction between branches and \nproviding a stronger representation capability for refined \nfeatures. \nB. Complexity Analysis \nTo evaluate the proposed change detection method more \ncomprehensively, in this study, the number of parameters \n(Params), the floating points of operations (FLOPs) and the \ninference time (Infer -time) for the complexity analysis of the \nmethod, the size of the input image is set to 256Ã— 256Ã— 3 to \ncalculate these values. Fig. 11 illustrates the comprehensive \nperformance of the different methods, with F1 representing the \naccuracy of the SVCD dataset. FCEF, FCSC, and FCSD have \nlower Params and FLOPs and exhibit significant advantages in \nterms of Infer-time. However, the accuracy of such methods is \npoor due to their weak feature extraction capability. In \naddition, BIT lacks the fusion of low -level detail features and \nis relatively lightweight in structure but less accurate. The \nmethod closest to MSFCTNet in terms of accuracy is \nSNUNet-CD, which adopts the structure of UNet++ and is \nrelatively complex. MSFCTNet takes advantage of the CNN -\nTransformer hybrid architecture and reduces the model \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \n> < \n \n \nFig. 11. Comparison of model complexity in terms of the Params, FLOPs, Infer-time, and F1. \n \n \nFig. 12.  Examples of visualizing the main modules in the MSFCTNet. (a) Input bi -temporal remote sensing images pair. ( b) \nMulti-level feature maps extracted by the encoder, from left to right, stages 1 -4. (c) The first row is the feature map with multi -\nscale fusion and the second row is the feature map after HCTM processing. (d) From left to right, the first row shows the fus ed \nfeature maps for stages 1 -4, and the second row shows the feature maps after GAM processing . (e) The first row is the feature \nmap acquired by the last layer of decoder and the second row is the feature map after FRH processing . (f) The first row is the \nfeature difference map and the second row is the predicted change map. \n \nTABLE â…¦ \nCOMPLEXITY COMPARISON OF DIFFERENT METHODS IN TERMS \nOF THE PARAMS, FLOPS, INFER-TIME, AND F1. \n \nMethod Params (M) FLOPs (G) Infer-time \n(ms) F1 (%) \nFCEF 1.35 1.78 11.81 85.67 \nFCSC 1.54 2.66  13.64 88.20 \nFCSD 1.35 2.36  13.91 89.49 \nUNet++_MSOF 9.16 17.36  18.18 91.17 \nSTANet 12.21 6.35  37.37 90.73 \nIFN 35.99 41.18  29.59 94.00 \nDSAMNet 12.23 32.84 26.64 92.98 \nSNUNet-CD 12.03 27.44  24.38 94.15 \nBIT 3.01 4.24 20.42 93.18 \nChangeFormer 41.03 101.43 74.46 93.63 \nMSFCTNet 2.43 4.55 25.88 95.52 \n \n \ncomplexity by using 1Ã— 1 convolution in the decoder stage. \nTable â…¦ provides the specific values of the relevant metrics. \nBased on the analysis above, it can be concluded that \nMSFCTNet achieves a better balance between complexity and \naccuracy. \nC. Network Visualization \nTo better comprehend the architecture of our method, we \nvisualize the MSFCTNet network structure as shown in Figure \n12. Examples from the LEVIR -CD and SVCD datasets are \nselected for presentation, representing small -scale and large -\nscale buildings, respectively, to visualize the feature maps.  \nGiven a bi -temporal remote sensing image pair (Fig. 12(a)), \nmulti-scale features are extracted through  Siamese ERM (Fig. \n12(b)). Then, the multi -scale features are provided to HCTM \nto obtain local -global features, increasing the receptive field \n(Fig. 12(c)). Next, GAM is utilized to alleviate the semantic \ngap between features and improve the fusion of high- and low-\nlevel features (Fig. 12(d)). After that, the features extracted \nfrom the DCM are refined using FRH to suppress task -\nirrelevant distracting information (Fig. 12(e)). Finally, the \nchange probability map is obtained and further processed to \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> < \n \ngenerate the predicted change map (Fig. 12(f)).  In summary, \nthe interactions between the modules facilitate the \nperformance of MSFCTNet. \nâ…¦.  CONCLUSION \nIn this study, we propose a new method MSFCTNet for the \nchange detection of high -resolution remote sensing images. \nThe MSFCTNet  employs a CNN -Transformer hybrid \nstructure, Siamese CNN is adopted to extract hierarchical \nfeatures, and HCTM is employed to fuse features at multiple \nscales to deeply aggregate local and global features. GAM \nutilizes the gated mechanism to narrow the semantic gaps of \ndifferent scale features, while FRH incorporates the idea of an \nattention mechanism to further refine the features to improve \nthe detection capability. Experimental results on three publicly \navailable datasets demonstrate that MSFCTNet achieves the \nhighest F1/IoU compared to other methods. Meanwhile, \nMSFCTNet can detect the changed areas of edge detail more \naccurately, and maintains good detection performance for \nsmaller targets, with a good balance between accur acy and \ncomplexity. In the future, we will investigate semi -supervised \nlearning or weakly supervised learning, which is proposed to \nsolve the problem of insufficient labeling. \nREFERENCES \n[1] A. Singh, â€œReview article digital change detection \ntechniques using remotely -sensed data,â€ Int. J. Remote \nSens., vol. 10, no. 6, pp. 989-1003, 1989. \n[2] Z. Zheng, Y. Wan, Y. Zhang, S. Xiang, D. Peng, and B. \nZhang, â€œCLNet: Cross -layer convolutional neural \nnetwork for change detection in optical remote sensing \nimagery,â€ ISPRS J. Photogramm. Remote Sens.,  vol. 175, \npp. 247-267, 2021. \n[3] X. Liu, and R. Lathrop Jr, â€œUrban change detection based \non an artificial neural network,â€ Int. J. Remote Sens.,  vol. \n23, no. 12, pp. 2513-2518, 2002. \n[4] D. A. de Alwis Pitts, and E. So, â€œEnhanced change \ndetection index for disaster response, recovery \nassessment and monitoring of buildings and critical \nfacilitiesâ€”A case study for Muzzaffarabad, Pakistan,â€ \nInt. J. Appl. Earth Obs. Geoinf., vol. 63, pp. 167 -177, \nDec. 2017. \n[5] M. Hussain, D. Chen, A. Cheng, H. Wei, and D. Stanley, \nâ€œChange detection from remotely sensed images: From \npixel-based to object -based approaches,â€ ISPRS J. \nPhotogramm. Remote Sens.,  vol. 80, pp. 91 -106, Jun. \n2013. \n[6] S. Fang, K. Li, J. Shao, and Z. Li, â€œSNUNet -CD: A \ndensely connected siamese network for change detection \nof VHR images,â€ IEEE Geosci. Remote Sens. Lett.,  vol. \n19, pp. 1-5, 2021. \n[7] J. S. Deng, K. Wang, Y. H. Deng, and G. J. Qi, â€œPCAâ€\nbased land â€use change detection and analysis using \nmultitemporal and multisensor satellite data, â€ Int. J. \nRemote Sens., vol. 29, no. 16, pp. 4823-4838, Aug. 2008. \n[8] C. Wu, B. Du, X. Cui, and L. Zhang, â€œA post -\nclassification change detection method based on iterative  \nslow feature analysis and Bayesian soft fusion,â€ Remote \nSens. Environ. , vol. 199, pp. 241-255, 2017. \n[9] R. D. Johnson and E. S. Kasischke, â€œChange vector \nanalysis: A technique for the multispectral monitoring of \nland cover and condition,â€  Int. J. Remote Sens. , vol. 19, \nno. 3, pp. 411â€“426, Mar. 1998. \n[10] A. Shafique, G. Cao, Z. Khan, M. Asad, and M. Aslam, \nâ€œDeep learning-based change detection in remote sensing \nimages: a review,â€ Remote Sens., vol. 14, no. 4, pp. 871, \n2022. \n[11] V. Walter, â€œObject-based classification of remote sensing \ndata for change detection,â€ ISPRS J. Photogramm.   \nRemote Sens., vol. 58, no. 3, pp. 225-238, Jan. 2004. \n[12] R. Zhang, H. Zhang, X. Ning, X. Huang, J. Wang, and \nW. Cui, â€œGlobal -aware siamese network for change \ndetection on remote sensing images,â€ ISPRS J. \nPhotogramm. Remote Sens.,  vol. 199, pp. 61 -72, May \n2023. \n[13] P. Deng, K. Xu, and H. Huang, â€œWhen CNNs meet \nvision Transformer: A joint framework for remote \nsensing scene classification,â€ IEEE Geosci. Remote Sens. \nLett., vol. 19, pp. 1-5, 2022. \n[14] R. Hang, P. Yang, F. Zhou, and Q. Liu, â€œMultiscale \nprogressive segmentation network for high -resolution \nremote sensing imagery,â€ IEEE Trans. Geosci. Remote \nSens., vol. 60, pp. 1-12, 2022. \n[15] Y. Zhang, Y. Zhao, Y. Dong, and B. Du, â€œSelf -\nsupervised pretraining via multimodality images with \ntransformer for change detection,â€ IEEE Trans. Geosci. \nRemote Sens., vol. 61, pp. 1-11, 2023. \n[16] T. Bai, L et al. , â€œDeep learning for change detection in \nremote sensing: a review,â€ Geo-spatial Inf. Sci., pp. 1-27, \n2022. \n[17] R. C. Daudt, B. L. Saux, and A. Boulch, â€œFully \nconvolutional siamese networks for change detection,â€ in  \nProc. 25th IEEE  Int. Conf. Image Process. (ICIP) , Oct. \n2018, pp. 4063â€“4067. \n[18] J. Bromley, J et al. , â€œSignature verification using a \nâ€œsiameseâ€ time delay neural network,â€ Int. J. Pattern \nRecognit Artif Intell., vol. 7, no. 04, pp. 669-688, 1993. \n[19] A. Krizhevsky, I. Sutskever, and G. Hinton, â€œImageNet \nclassification with deep convolutional neural networks,â€ \nin Proc. Adv. Neural Inf.  Process. Syst. , vol. 25, 2012, \npp. 1097â€“1105. \n[20] M. Liu, Z. Chai, H. Deng, and R. Liu, â€œA CNN -\nTransformer network with multiscale context aggregation \nfor fine-grained cropland change detection,â€ IEEE J. Sel. \nTop. Appl. Earth Obs. Remote Sens.,  vol. 15, pp. 4297 -\n4306, 2022. \n[21] A. Vaswani et al. , \"Attention is all you need,\" in  Proc. \nAdv. Neural Inf. Process. Syst. , Long Beach, CA, USA, \n2017, pp. 5998â€“6008.  \n[22] A. Dosovitskiy et al., â€œAn image is worth 16x16 words: \nTransformers for image recognition at scale,â€ 2020, \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \n> < \n \narXiv :2010.11929.  \n[23] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. \nKirillov, and S. Zagoruyko, \"End -to-end object detection \nwith transformers,\" in Proc. Eur. Conf. Comput. Vis. \n(ECCV), 2020, pp. 213â€“229. \n[24] W. Wang et al. , \"Pyramid Vision Transformer: A \nversatile backbone for dense prediction without \nconvolutions,\" in Proc. IEEE Int. Conf. Comput. Vis. \n(ICCV), 2021, pp. 568â€“578. \n[25] Y. Feng, J. Jiang, H. Xu, and J. Zheng, â€œChange \ndetection on remote sensing images using dual -branch \nmultilevel intertemporal network,â€ IEEE Trans. Geosci. \nRemote Sens., vol. 61, pp. 1-15, 2023. \n[26] J. Long, E. Shelhamer, and T. Darrell, â€œFully \nconvolutional networks for semantic segmentation,â€ \nIEEE Trans. Pattern Anal. Mach. Intell.,  vol. 39, no. 4, \npp. 640-651, 2015. \n[27] O. Ronneberger, P. Fischer, and T. Brox, \"U -Net: \nConvolutional networks for biomedical image \nsegmentation,\" in Proc. Int. Conf. Med. Image  Comput. \nComput.-Assist. Interv., 2015, pp. 234â€“241. \n[28] C. Zhao, Y. Tang, S. Feng, Y. Fan, W. Li, R. Tao, and L. \nZhang, â€œHigh -resolution remote sensing bitemporal \nimage change detection based on feature interaction and \nmultitask learning,â€ IEEE Trans. Geosci. Remote Sens. , \nvol. 61, pp. 1-14, 2023. \n[29] D. Peng, Y. Zhang, and H. Guan, â€œEnd -to-end change \ndetection for high resolution satellite images using \nimproved UNet++,â€ Remote Sens.,  vol. 11, no. 11, pp. \n1382, 2019. \n[30] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. \nLiang, â€œUNet++: Redesigning skip connections to exploit \nmultiscale features in image segmentation,â€ IEEE Trans. \nMed. Imaging, vol. 39, no. 6, pp. 1856-1867, 2020. \n[31]  Z. Li, C. Yan, Y. Sun, and Q. Xin, â€œA densely attentive \nrefinement network for change detection based on very-\nhigh-resolution bitemporal remote sensing images,â€ IEEE \nTrans. Geosci. Remote Sens., vol. 60, pp. 1-18, 2022. \n[32] H. Huang et al., \"UNet 3+: A full -scale connected UNet \nfor medical image segmentation,\" in Proc. IEEE Int. \nConf. Acoust., Speech Signal Process. , 2020, pp. 1055 â€“\n1059. \n[33] Y. Ye, M. Wang, L. Zhou, G. Lei, J. Fan, and Y. Qin, \nâ€œAdjacent-level feature cross -fusion with 3 -D CNN for \nremote sensing image change detection,â€  IEEE Trans. \nGeosci. Remote Sens., vol. 61, pp. 1-14, 2023. \n[34] J. Chen, Z. Yuan, J. Peng, L. Chen, H. Huang, J. Zhu, Y. \nLiu, and H. Li, â€œDASNet: Dual attentive fully \nconvolutional siamese networks for change detection in \nhigh-resolution satellite images,â€ IEEE J. Sel. Top. Appl. \nEarth Obs. Remote Sens., vol. 14, pp. 1194-1206, 2021. \n[35] H. Chen, and Z. Shi, â€œA spatial -temporal attention-based \nmethod and a new dataset for remote sensing image \nchange detection,â€ Remote Sens.,  vol. 12, no. 10, pp. \n1662, 2020. \n[36] C. Zhang, P. Yue, D. Tapete, L. Jiang, B. Shangguan, L. \nHuang, and G. Liu, â€œA deeply supervised image fusion \nnetwork for change detection in high resolution bi -\ntemporal remote sensing images,â€ ISPRS J. Photogramm. \nRemote Sens., vol. 166, pp. 183-200, 2020. \n[37] F. Zhou, C. Xu, R. Hang, R. Zhang, and Q. Liu, â€œMining \njoint intraimage and interimage context for remote \nsensing change detection,â€ IEEE Trans. Geosci. Remote \nSens., vol. 61, pp. 1-12, 2023. \n[38] J. Zhang, Z. Shao, Q. Ding, X. Huang, Y. Wang, X. \nZhou, and D. J. Li, â€œAERNet: An attention -guided edge \nrefinement network and a dataset for remote sensing \nbuilding change detection,â€ IEEE Trans. Geosci. Remote \nSens., vol. 61, pp. 1-16, 2023. \n[39] Y. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. \nZhang, Z. Shi, J. Fan, and Z. He, â€œA survey of visual \nTransformers,â€ IEEE Trans. Neural Networks Learn. \nSyst., pp. 1-21, 2023. \n[40] M. Liu, Q. Shi, Z. Chai, and J. Li, â€œPA -Former: Learning \nprior-aware transformer for remote sensing building \nchange detection,â€ IEEE Geosci. Remote Sens. Lett. , vol. \n19, pp. 1-5, 2022. \n[41] C. Liu, R. Zhao, H. Chen, Z. Zou, and Z. Shi, â€œRemote \nsensing image change captioning with dual -branch \ntransformers: A new method and a large scale dataset,â€ \nIEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1 -20, \n2022. \n[42] H. Chen, Z. Qi, and Z. Shi, â€œRemote sensing image \nchange detection with Transformers,â€ IEEE Trans. \nGeosci. Remote Sens., vol. 60, pp. 1-14, 2021. \n[43] W. Gedara Chaminda Bandara, and V. M. Patel, \"A \nTransformer-based siamese network for change \ndetection,\" in Proc. IEEE Int. Geosci. Remote Sens. \nSymp., Jul. 2022, pp. 207â€“210. \n[44] X. Xu, J. Li, and Z. Chen, â€œTCIANet: Transformer -based \ncontext information aggregation network for remote \nsensing image change detection,â€ IEEE J. Sel. Top. Appl. \nEarth Obs. Remote Sens., vol. 16, pp. 1951-1971, 2023. \n[45] Q. Li, R. Zhong, X. Du, and Y. Du, â€œTransUNetCD: A \nhybrid Transformer network for change detection in \noptical remote -sensing images,â€ IEEE Trans. Geosci. \nRemote Sens., vol. 60, pp. 1-19, 2022. \n[46] W. Liu, Y. Lin, W. Liu, Y. Yu, and J. Li, â€œAn attention -\nbased multiscale transformer network for remote sensing \nimage change detection,â€ ISPRS J. Photogramm. Remote \nSens., vol. 202, pp. 599-609, 2023. \n[47] W. Li, L. Xue, X. Wang, and G. Li, â€œConvTransNet: A \ncnnâ€“transformer network for change detection with \nmultiscale global â€“local representations,â€ IEEE Trans. \nGeosci. Remote Sens., vol. 61, pp. 1-15, 2023. \n[48] P. Zhu, H. Xu, and X. Luo, â€œMDAFormer: Multi -level \ndifference aggregation transformer for change detection \nof VHR optical imagery,â€ Int. J. Appl. Earth Obs. \nGeoinf., vol. 118, pp. 103256, 2023. \n[49] Q. Guo, J. Zhang, S. Zhu, C. Zhong, and Y. Zhang, \nâ€œDeep multiscale siamese network with parallel \nconvolutional structure and self -attention for change \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4 \n> < \n \ndetection,â€ IEEE Trans. Geosci. Remote Sens. , vol. 60, \npp. 1-12, 2022. \n[50] C. Han, C. Wu, H. Guo, M. Hu, and H. Chen, â€œHANet: A \nhierarchical attention network for change detection with \nbitemporal very-high-resolution remote sensing images,â€ \nIEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.,  vol. 16, \npp. 3867-3878, 2023. \n[51] J. Hu, L. Shen, and G. Sun, \"Squeeze -and-excitation \nnetworks,\" in  Proc. IEEE/CVF Conf. Comput. Vis. \nPattern Recognit., Jun. 2018, pp. 7132â€“7141. \n[52] M. A. Lebedev, Y. V. Vizilter, O. V. Vygolov, V. A. \nKnyaz, and A. Y. Rubis, â€œChange detection in remote \nsensing images using conditional adversarial networks,â€ \nInt. Arch. Photogramm., Remote Sens.  Spatial Inf. Sci. , \nvol. 42, pp. 565â€“571, May 2018.  \n[53] Q. Shi, M. Liu, S. Li, X. Liu, F. Wang, and L. Zhang, â€œA \ndeeply supervised attention metric -based network and an \nopen aerial image dataset for remote sensing change \ndetection,â€ IEEE Trans. Geosci. Remote Sens. , vol. 60, \npp. 1-16, 2021. \n[54] I. Loshchilov, and F. Hutter, â€œDecoupled weight decay \nregularization,â€ 2017, arXiv:1711.05101. \n[55] S. Woo, J. Park, J. -Y. Lee, and I. -S. Kweon, â€œCBAM: \nConvolutional block attention module,â€ 2018, arXiv: \n1807.06521. \n \n \nMing Jiang received the B.S. degree in \nsurveying and mapping engineering from \nCentral South University, Changsha, \nHunan, China, in 2020, and the M.S. \ndegree in surveying and mapping \nengineering from Guangzhou University, \nGuangzhou, China, in 2023.  \nHe is currently pursuing the Ph.D. \ndegree with the geographic information \nscience, Sun Yat -sen University, Guangzhou, China. His \nresearch interests include urban information extraction and \ngeographical simulation. \n \n \nYimin Chen  received the B.S. degree in \nGIScience from Sun Yat -sen University, \nGuangzhou, China, in 2008, and the Ph.D. \ndegree in cartography and GIScience from \nSun Yat-sen University, in 2014.  \nHe is currently an Associate Professor \nwith the School of Geography and \nPlanning, Sun Yat -sen University. His \nresearch interests include remote sensing \nand urban analysis, including deep learning, classification, and \nsematic segmentation. \n \n \n \n \n \nZhe Dong  received the B.S. degree in \ngeography and the M.S. degree in human \ngeography from Sun Yat -sen University, \nGuangzhou, China, in 2008 and 2010, \nrespectively.  \nHe is currently a teacher with the School \nof Geography and Planning, Sun Yat -sen \nUniversity. His research interests include \nurban big data analysis and urban planning. \n \n \nXiaoping Liu  (Member, IEEE) received \nthe B.S. degree in geography and the Ph.D. \ndegree in remote sensing and geographical \ninformation sciences from Sun Yat -sen \nUniversity, Guangzhou, China, in 2002 and \n2008, respectively.  \nHe is a Professor with the School of \nGeography and Planning, Sun Yat -sen \nUniversity. He has authored two books and \nover 100 articles. His research interests include image \nprocessing, artificial intelligence, and geographical simulation. \n \n \nXinchang Zhang received the B.S. degree \nin cartography from the Wuhan Institute of \nSurveying and Mapping, Wuhan, China, in \n1982, the M.S. degree in cartography from \nthe Wuhan Technical University of \nSurveying and Mapping, Wuhan, in 1994, \nand the Ph.D. degree in resources and \nenvironmental sciences from Wuhan \nUniversity, Wuhan, in 2004.  \nHe is currently a Professor with the School of Geography \nand Remote Sensing, Guangzhou University, Guangzhou, \nChina. His research interests include spatial database updating, \nspatial data integration, and smart city. \n \n \nHonghui Zhang received the Ph.D. degree \nin cartography and geographic information \nsystem from Central South University, \nChangsha, Hunan, China, in 2011. \nHe is currently the Co -President of \nGuangdong Guodi Planning Science \nTechnology Co., Ltd, China. His research \ninterests include urban big data analysis \nand urban planning. \n \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3361507\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8172828555107117
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6822640895843506
    },
    {
      "name": "Transformer",
      "score": 0.645615816116333
    },
    {
      "name": "Feature extraction",
      "score": 0.6210182905197144
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6104119420051575
    },
    {
      "name": "Change detection",
      "score": 0.5622837543487549
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5551274418830872
    },
    {
      "name": "Voltage",
      "score": 0.09008839726448059
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I37987034",
      "name": "Guangzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210137144",
      "name": "Guangdong Hydropower Planning & Design Institute",
      "country": "CN"
    }
  ]
}