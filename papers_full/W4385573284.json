{
    "title": "What Makes Data-to-Text Generation Hard for Pretrained Language Models?",
    "url": "https://openalex.org/W4385573284",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2942946170",
            "name": "Moniba Keymanesh",
            "affiliations": [
                "Bloomberg (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2120644017",
            "name": "Adrian Benton",
            "affiliations": [
                "Bloomberg (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2023626662",
            "name": "Mark Dredze",
            "affiliations": [
                "Bloomberg (United States)",
                "Johns Hopkins University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2971300525",
        "https://openalex.org/W2786660442",
        "https://openalex.org/W4229543565",
        "https://openalex.org/W3153912421",
        "https://openalex.org/W2250342921",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W2962905474",
        "https://openalex.org/W1974081281",
        "https://openalex.org/W2604799547",
        "https://openalex.org/W4224947967",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2983962589",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2786121478",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W4283794478",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2890276793",
        "https://openalex.org/W4297795751",
        "https://openalex.org/W3034731179",
        "https://openalex.org/W3026997957",
        "https://openalex.org/W2149327368",
        "https://openalex.org/W4308900200",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W3115328016",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W3035252911",
        "https://openalex.org/W3035565536",
        "https://openalex.org/W2133459682",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W2557436004",
        "https://openalex.org/W2950457956",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2112824353",
        "https://openalex.org/W4300532998",
        "https://openalex.org/W2739046565",
        "https://openalex.org/W3206337320",
        "https://openalex.org/W4287854593",
        "https://openalex.org/W3116342879",
        "https://openalex.org/W4287727281",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4386506836",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2107598941",
        "https://openalex.org/W2467367009",
        "https://openalex.org/W2996176596",
        "https://openalex.org/W2787623698"
    ],
    "abstract": "Expressing natural language descriptions of structured facts or relations – data-to-text generation (D2T) – increases the accessibility of structured knowledge repositories. Previous work shows that pre-trained language models (PLMs) perform remarkably well on this task after fine-tuning on a significant amount of task-specific training data. On the other hand, while auto-regressive PLMs can generalize from a few task examples, their efficacy at D2T is largely unexplored. Furthermore, we have an incomplete understanding of the limits of PLMs on D2T. In this work, we conduct an empirical study of both fine-tuned and auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their performance as a function of the amount of task-specific data and how the data is incorporated into the models: zero and few-shot learning, and fine-tuning of model weights. In addition, we probe the limits of PLMs by measuring performance on subsets of the evaluation data: novel predicates and abstractive test examples. To improve the performance on these subsets, we investigate two techniques: providing predicate descriptions in the context and re-ranking generated candidates by information reflected in the source. Finally, we conduct a human evaluation of model errors and show that D2T generation tasks would benefit from datasets with more careful manual curation.",
    "full_text": "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 539 - 554\nDecember 7, 2022 ©2022 Association for Computational Linguistics\nWhat Makes Data-to-Text Generation Hard for Pretrained Language\nModels?\nMoniba Keymanesh1∗ Adrian Benton1† Mark Dredze1,2\n1 Bloomberg, New York, NY USA\n2 Computer Science, Johns Hopkins University, Baltimore, MD USA\nmkeymanesh1@bloomberg.net, adbenton@google.com, mdredze@cs.jhu.edu\nAbstract\nExpressing natural language descriptions of\nstructured facts or relations – data-to-text gen-\neration (D2T) – increases the accessibility of\nstructured knowledge repositories. Previous\nwork (Nan et al., 2020) shows that pre-trained\nlanguage models (PLMs) perform remarkably\nwell on this task after fine-tuning on a signifi-\ncant amount of task-specific training data. On\nthe other hand, while auto-regressive PLMs\ncan generalize from a few task examples, their\nefficacy at D2T is largely unexplored. Further-\nmore, we have an incomplete understanding\nof the limits of PLMs on D2T. In this work,\nwe conduct an empirical study of both fine-\ntuned and auto-regressive PLMs on the DART\nmulti-domain D2T dataset. We consider their\nperformance as a function of the amount of\ntask-specific data and how the data is incor-\nporated into the models: zero and few-shot\nlearning, and fine-tuning of model weights. In\naddition, we probe the limits of PLMs by mea-\nsuring performance on subsets of the evaluation\ndata: novel predicates and abstractive test ex-\namples. To improve the performance on these\nsubsets, we investigate two techniques: pro-\nviding predicate descriptions in the context and\nre-ranking generated candidates by information\nreflected in the source. Finally, we conduct a\nhuman evaluation of model errors and show\nthat D2T generation tasks would benefit from\ndatasets with more careful manual curation.\n1 Introduction\nStructured data repositories, or knowledge bases,\ncontain a wealth of information organized to facil-\nitate automated access and analysis. Automated\ndata-to-text (D2T) generation systems can trans-\nform and organize this knowledge into natural lan-\nguage text snippets that enable broader access (Gatt\nand Krahmer, 2018). These systems take as input\na set of relations, where each relation is a (subject,\n∗Work done during an internship at Bloomberg.\n†Now at Google Research.\npredicate, object) triple. Applications of this tech-\nnology include story or dialogue generation (Moon\net al., 2019), open-domain question-answering (Ma\net al., 2021; Fan et al., 2019), and text summariza-\ntion (Wiseman et al., 2017). Domains span journal-\nism (Leppänen et al., 2017), weather (Ramos-Soto\net al., 2014; Mei et al., 2015), finance, sports (Pla-\nchouras et al., 2016; Chen and Mooney, 2008;\nvan der Lee et al., 2017), and summarizing patient\nmedical histories (Portet et al., 2009).\nHistorically, D2T systems included pipeline\napproaches with customized models (Gardent\net al., 2017), but have now shifted to pretrained\nTransformer-based language models (PLMs) (De-\nvlin et al., 2018; Liu et al., 2019; Radford et al.,\n2019). Recent examples include Mager et al.\n(2020) and Kale and Rastogi (2020), who use mod-\nels like GPT-2 (Radford et al., 2019) and T5 (Raffel\net al., 2019) to generate natural language descrip-\ntions for relations. To support these types of sys-\ntems, Nan et al. (2020) introduced DART, a large\nopen-domain data-to-text generation corpus. Mod-\nels trained on DART, both larger and more diverse\nthan previous corpora, improve the performance\nof BART (Lewis et al., 2019) and T5 on the stan-\ndard WebNLG challenge (Gardent et al., 2017).\nThis approach requires a PLM to be fine-tuned on a\ntask-specific in-domain dataset (Howard and Ruder,\n2018; See et al., 2019; Keskar et al., 2019). The\npromising results achieved by fine-tuning on DART\nbelie the reality – in spite of DART’s aspirations,\nmost domains and relations that one could express\nfail to appear in DART.\nA variety of methods have emerged within PLM\nresearch to address domain or task adaptation. For\nexample, auto-regressive models, like GPT, have\ndemonstrated improved performance on a wide\nrange of tasks via few-shot learning from a handful\nof examples (Chen et al., 2019). Other strategies,\nsuch as prompt tuning (Lester et al., 2021), can\nadapt PLMs to specific down-stream tasks by up-\n539\ndating only a small subset of model parameters.\nWhile great progress has been made in utiliz-\ning PLMs for D2T generation, the path forward is\nunclear, as we have an incomplete understanding\nas to which examples they fall short on and the\nquantity of training resources they need to achieve\nacceptable performance. More specifically, it is\nnot clear which classes of D2T examples are chal-\nlenging for these models. In addition, we do not\nfully understand what classes of errors PLMs are\nprone to and how the adaptation mechanism (e.g.,\nk-shot learning, fine-tuning) affects the prevalence\nof these errors.\nIn this work, we conduct an evaluation of PLMs\nfor D2T generation, focusing on two classes of\nchallenging examples: examples with novel ( un-\nseen) relations ( predicates) and examples where\nthe source and target sequences are lexically very\ndifferent (not amenable to purely extractive D2T\nsystems). We consider how GPT-2, adapted with\nfew-shot learning, prompt tuning, and the addi-\ntion of predicate descriptions, performs on these\nexample classes as compared to a state-of-the-art\nfine-tuned T5. We show that while GPT-2 performs\npoorly on DART in the 0-shot setting, its perfor-\nmance can be drastically improved by employing\nthe above techniques. We make the following con-\ntributions:\n• We evaluate GPT2-XL and fine-tuned T5 for\nD2T generation. While the 0-shot GPT model\nperforms poorly, we evaluate several strate-\ngies to improve performance, including few-\nshot learning and prompt tuning. Both pro-\nvide significant improvements on the DART\ndataset.\n• We compare model performance on two\nclasses of difficult examples: examples with\nunseen predicates, and abstractive examples\n(examples where source and target sequences\nare lexically dissimilar). We investigate\nwhether including predicate descriptions in\nthe prompt can improve the ability of PLMs\non these classes.\n• We conduct a human evaluation of PLMs to\nquantify the prevalence of hallucination and\nmissing information in generations as a func-\ntion of the model adaptation technique. We\nfind that a re-ranking strategy for few-shot\nGPT2-XL, despite having little effect on au-\ntomatic metrics like BLEU, reduces the inci-\ndence of missing information, without requir-\ning additional training data.\nFinally, we provide recommendations for future\nmodel and dataset research in D2T generation.\n2 Background and Related Work\nIn the task of data-to-text generation, we are pro-\nvided a set of triples that include a predicate,\nsubject, and object. The system then produces\na text snippet expressing the predicate in natu-\nral language. Figure 2 shows examples of pred-\nicates about sports. The system can be given a\nset of triples with related predicates (e.g., CLUB ,\nLEAGUE , FORMER_TEAM ) and must gener-\nate text that expresses the facts encoded by these\nrelations. The resulting text is typically evaluated\nby comparison to a set of reference texts, which\nrepresent various ways of expressing this triple set.\nVariations in the formulation of this task depend\non the structure of the relations (e.g., tables, triples),\nthe domain of the task (single or open domain), and\nthe source of the data (manually created, automati-\ncally derived).\nHarkous et al. (2020) follow a generate-and-re-\nrank paradigm to improve the semantic fidelity\nof the generated text by fine-tuned GPT-2 model.\nMore recently, Ribeiro et al. (2020) propose a\nnew task-adaptive pretraining strategy to adapt\nBART (Lewis et al., 2019) and T5 (Raffel et al.,\n2019) models for data-to-text generation. They\nshow that adding an intermediate task-adaptive pre-\ntraining step between the task-independent pretrain-\ning and fine-tuning further improves the perfor-\nmance of these models on data-to-text generation.\nDespite the progress of these models, it is not\nclear which types of D2T examples are most chal-\nlenging for PLMs or what errors are prevalent in\ngenerations. Futhermore, how does PLM adap-\ntation (tuning/prompting) interact with the occur-\nrence of these errors. On the other hand, D2T\ndatasets are not readily available in many domains.\nWeakly supervised annotation methods (e.g., based\non identifying sentences in a corpus that are likely\nto express a data record) require significant manual\neffort and often result in annotations with low fi-\ndelity between data records and the corresponding\ntextual expression (Mintz et al., 2009). Training\nNLG models on such data can result in generations\nwith missing information or hallucination (Dušek\net al., 2019; Dziri et al., 2022a,b). These issues ren-\nder the path forward for D2T generation research\n540\nZero-shot Prompt\nTranslate Graph to English:\nGraph: <H> Alan Martin (footballer) <R> CLUB\n<T> Hamilton Academical F.C.\nEnglish:\nZero-shot Prompt + Relation Description\nTranslate Graph to English:\nDefinition: club is an organization of players and man-\nagers associated with a particular football team.\nGraph: <H> Alan Martin (footballer) <R> CLUB\n<T> Hamilton Academical F.C.\nEnglish:\nFigure 1: A customized 0-shot prompt for GPT\nunclear.\n3 Model Adaptation\nAs a supervised task, D2T generation systems rely\non previously observed examples to learn the cor-\nrect generation or level of required \"re-writing\"\nfor a predicate. On the other hand, large auto-\nregressive PLMs (such as GPT2-XL) are able to\nperform D2T generation without any explicit fine-\ntuning at all. However, their efficacy on D2T\nand potential shortcomings are largely unexplored.\nHow well do PLMs perform on relations with a\nnovel predicate? Do PLMs overly rely on copy-\ning verbatim from the input or are they capable\nof abstraction when required? What classes of er-\nrors are prevalent in the generations and how do\nthey interact with the choice of adaptation mech-\nanism? Our focus is on the analysis of PLMs for\nD2T generation.\nWe study this problem using two types of PLMs:\nauto-regressive models like GPT-2 and “supervised”\nmodels like T5 (Raffel et al., 2019). While prior\nwork has demonstrated that T5 achieves state-of-\nthe-art results on D2T, these “supervised” models1\nexpect task-specific training data, whereas gener-\native PLMs excel at adapting to new tasks. Since\nauto-regressive models have not been fully bench-\nmarked for D2T, we will evaluate them in multiple\nsettings and compare to T5. For both, we will ex-\nplore the effect of varying training size and their\npathological behaviors.\n1We note that new findings (Sanh et al., 2021) has demon-\nstrated T5 can handle 0-shot task adaptation with the right\nprompts; this is an evolving research area.\nFew-shot Prompt\nTranslate Graph to English:\nGraph: <H> Paulo Sousa <R> CLUB <T> ACF\nFiorentina\nEnglish: Paulo Sousa plays for ACF Fiorentina.\n###\nGraph: <H> Dave Challinor <R> CLUB <T> Col-\nwyn Bay F.C.\nEnglish: Dave Challinor plays for Colwyn Bay F.C.\n###\nGraph: <H> Alan Martin (footballer) <R> CLUB\n<T> Hamilton Academical F.C.\nEnglish:\nFigure 2: A customized 3-shot prompt for GPT\nWhile PLMs can be fine-tuned, their increasing\nsize and training requirements disfavors this ap-\nproach. Instead, current work assumes a single\nPLM capable of performing multiple downstream\ntasks (Lester et al., 2021). We adopt GPT2-XL, a\ndecoder-only Transformer (Vaswani et al., 2017)\nwith 1.5B parameters pre-trained for language mod-\neling (Radford et al., 2019). 2 We utilize GPT2-\nXL as a D2T generation model by varying the\namount of supervised information available. In-\nstead of fine-tuning GPT2-XL, we investigate both\nfew-shot learning (Radford et al., 2019), which is\nbetter suited to settings where little training data is\navailable, and prompt tuning, which enables us to\ntractably update a subset of model weights in spite\nof GPT2-XL’s large parameter count.\n3.1 0-shot Setting\nWe start by evaluating GPT2-XL in the 0-shot set-\nting, an especially challenging setting due to a lack\nof coverage in the training data of pairings between\nstructured records and unstructured text (Gong\net al., 2020). Ribeiro et al. (2020) handled this\nby including an additional pretraining step. Our\nfocus is on an off-the-shelf GPT2-XL model. We\nformat the input data using the D2T generation in-\nfix and prefix formatting of Ribeiro et al. (2020)\n(example in Figure 1). We provide no additional\ncontext or task-specific training.\n2WebText (the training dataset) includes the content of\nmore than 8 million documents with outbound links from\nReddit, a social media platform. Wikipedia (the main data\nsource for DART) is excluded.\n541\n3.2 Few-shot Setting\nWe next consider a few-shot setting by augment-\ning the format of the 0-shot input with reference\ngenerations from the training corpus. We evaluate\nGPT2-XL under the 3-shot learning setting (ex-\nample in Figure 2). For predicates “seen” in the\ntraining set, we select three shots with the same\npredicate uniformly at random from the training\nset. For “unseen” predicates – predicates not cov-\nered in the training set – we randomly select any\nthree examples. Previous work has found that care-\nful shot selection based on input text similarity can\nbe beneficial (Liu et al., 2021a). However, it’s less\nclear how this would apply to unseen predicates.\nWe leave this for future work.\n3.3 Prompt Tuning\nThe expected task for a PLM is indicated by the\nchoice of prompt; ours (Figure 1) follows prior\nwork (Ribeiro et al., 2020; Nan et al., 2020). The\nprompt includes a prefix (“Graph”) and an infix\ntoken (“English”) that indicate the start of the in-\nput and the start of the expected output. Auto-\nregressive language models are sensitive to the\nchoice of prompt, and significant effort is needed\nto craft effective prompts (Liu et al., 2021b).\nLester et al. (2021) proposed an alternate\nmethod: prompt tuning. Instead of using discrete\nprompt tokens, they use “soft-prompts” which are\npseudo-token embeddings that are learned during\nfine-tuning, with all other model parameters held\nfixed. We follow previous work (Lester et al.,\n2021; Chowdhury et al., 2022) and use a generic se-\nquence of tokens to denote the prompt prefixp1:s =\n(p1,p2,....ps) and infix q1:t = (q1,q2,....qt). The\nPLM is provided the input sequence p1:s <H> x1\n<R> x2 <T> x3 q1:t, where x1, x2 and x3 are head,\npredicate (relation), and tail strings from the exam-\nple.\nThe objective during prompt tuning is to maxi-\nmize the probability of output sequence y1:m given\ninput data record, prefix p1:s, and infix q1:t. Dur-\ning training, however, only the embedding of the\nprompt tokens can be updated. Unlike fine-tuning\nwhich updates all model parameters on the target\ntask, prompt tuning tunes a small number of pa-\nrameters (less than 0.01% of all parameters) while\nkeeping most of the language model fixed. While\nthis requires the use of the full training set, as op-\nposed to few-shot learning, it illuminates the abili-\nties of GPT2-XL given access to such data.\n3.4 Domain Knowledge\nWe explore another way of improving model per-\nformance for novel predicates and for examples\nwhere significant re-writing is needed: providing\ndefinitions for predicates. In many domains, we\nmay find a knowledge base containing many pred-\nicates, and definitions for those relations, but no\nexamples of sentences expressing those relations.\nIn these cases, we want to enhance the context of\nthe PLM with predicate definitions. For example,\nfor the tuple <H> Genuine Parts <R> DISTRIBU-\nTOR <T> automotive and industrial replacement\nparts we may know that DISTRIBUTOR means\n\"someone who markets merchandise\" . This def-\ninition can be helpful to a model that was never\nexposed to this predicate at training time.\nWe source predicate definitions for our data from\nWordNet, a lexical database in English (Miller,\n1995), and WikiData. 3 We use WikiData since\nWikipedia was the source of many relations in the\nDART data.4 An example of the input prompt\nenhanced with the predicate definition appears in\nFigure 1. We also consider using predicate descrip-\ntions in combination with prompt tuning.\n3.5 Fine-tuned PLM\nOur second model type is T5 large (Raffel et al.,\n2019), a Transformer encoder-decoder architecture\nwith 770M parameters for text generation. The\nmodel is pretrained with a denoising objective on\na variety of NLP tasks and web-extracted C4 cor-\npus. Unlike GPT2-XL, the denoising objective\nmeans an off-the-shelf model performs poorly on\nunseen tasks, such as D2T generation (Raffel et al.,\n2019; Lester et al., 2021). We follow Nan et al.\n(2020) and fine-tune T5 large on the DART train-\ning set. While this model requires a large amount\nof supervised examples, it attains state of the art\nperformance on this task.\n4 Dataset\nFor our experiments we use DART (Nan et al.,\n2020), the largest publicly available open-domain\ndata-to-text generation corpus. DART relies on\ndata from Wikipedia as well as two other com-\nmonly used data sets for this task: WebNLG (Gar-\n3wikidata.org\n4DART includes predicates such as\nMARGIN_OF_VICTORY and INTERMEDI-\nATE_(SOUTH)_WINNERS. Since descriptions for such\nrelations cannot be found verbatim in WordNet or WikiData,\nno description is added to those cases.\n542\nTrain Dev Test\nSize 30,526 2,768 5,097\n#Unique relation types 4,221 419 494\n#Ref per example min/avg/max 1/2.0/48 1/2.5/33 1/2.4/35\n#Triples per record min/avg/max 1/3.3/10 1/3.7/8 1/3.6/7\nTable 1: Descriptive statistics of the DART version 1.1.1\ndent et al., 2017) and E2E (Novikova et al., 2017).\nEach instance includes a triple set (a set of one\nor more predicates and their labels) and a natural\nlanguage reference that expresses the facts in the\ntriple set. We choose DART due to its size and\nwide coverage of predicate types. Relevant DART\nstatistics appear in Table 1. We use the original\ntrain, development, and test splits.5 6\nData Splits The DART test set includes 5,097\nexamples, of which 4,826 (94.4%) include at least\none relation type that appears in the training set.\nWe refer to this subset as the SEEN partition. The\nremaining 271 instances (5.3%) are considered UN-\nSEEN .7 To support additional system analysis, we\ncreate another partition of the test data: EASY and\nHARD . HARD examples are identified by similarity\nof the input triple to the reference text. In many\ncases, the reference has high lexical overlap with\nand similar meaning to the input, while in other\ncases the generation is non-trivial (see Appendix A\nfor examples). To create the EASY and HARD par-\ntitions, we use BERTScore (Zhang et al., 2019) to\ncompute similarity of the input triple with respect\nto the reference. Examples are ranked based on\nBERTScore (F1) and the top 10% (510 examples)\ncomprise the EASY partition, while the bottom 10%\ncomprise the HARD partition. By using BertScore\nto separate EASY and HARD examples, we are not\nrelying purely on lexical overlap to score the diffi-\nculty of an example.\n5 Experimental Setup\nModel Training We use the pretrained mod-\nels GPT2-XL and T5 large released by Hugging\n5Nan et al. (2020) use version v1.0.0 of DART, whereas\nwe use the publicly available version, v1.1.1.\n6In the DART dataset, some data records are paired with\nmore than 30 references. Nan et al. (2020) do not report the\nnumber of references used for their experiments. However in\ntheir adaptation of Ribeiro et al. (2020)’s fine-tuning script\nthey only use three references. We follow their methodology\nand only use up to three references per example.\n7Note that Nan et al. (2020) report performance on the\n“unseen” portion of WebNLG. “Unseen”, in this case, means\nthat the relations do not appear in the WebNLG training data;\nthere is no guarantee that they do not appear in the DART\ntraining data. Our splits ensure that the UNSEEN partition only\ncontains predicates not seen during DART training.\nFace (Wolf et al., 2019), along with their respective\ntokenizers, for all experiments.\nWe use beam search with beam size of three\nfor decoding in all models, lightly post-processing\nthe generated text by truncating generations at the\nnewline character. We set maximum generated\ntokens to 100 and repetition penalty to 1.01 for all\nexperiments.\nWe used a single V100 GPU with 32GB of mem-\nory for all prompt tuning experiments, tuning for a\nsingle epoch on the DART train set with prefix and\ninfix length both set to 8 tokens. We use the Adam\noptimizer (Kingma and Ba, 2014) with a maximum\nlearning rate of 0.1 and 100 warm-up steps for the\nlinear learning rate schedule. The training batch\nsize was fixed to 2, with 32 gradient accumulation\nsteps (effective batch size of 64 examples).\nWe use the scripts from Ribeiro et al. (2020) to\nfine-tune T5 on DART, using identical hyperparam-\neter settings.8 We use the Adam optimizer with\nan initial learning rate of 3e-5 and a linearly de-\ncreasing learning rate schedule. We fine-tune the\nmodel on four GPUs for a maximum of 100 epochs\nand stop training early if the performance does not\nimprove on the dev set for 15 epochs. Each train-\ning epoch takes approximately two hours for each\nmodel.\nFinally, we include a baseline system to bench-\nmark the performance of our machine learning\nmodels. In a “copy baseline” we simply copy\nthe input text and remove the prefix tokens (<H>,\n<R>, <T>) as well as special characters (e.g., under-\nscores) common in DART predicates. This baseline\nperforms well for examples with high lexical over-\nlap between input triple set and reference.\nEvaluation Metrics Following previous work,\nwe use automated metrics such as BLEU (Papineni\net al., 2002), METEOR (Denkowski and Lavie,\n2014), translation edit rate (TER) (Snover et al.,\n2006), and chrF++ (Popovi´c, 2015) for evaluating\nour generation results. In addition, we also report\nBERTScore (Zhang et al., 2019) and BLEURT (Sel-\nlam et al., 2020). These metrics go beyond surface\nform similarities and use contextual embeddings to\nmeasure semantic similarity between the generated\nand reference text.9\n8https://github.com/UKPLab/\nplms-graph2text (Apache 2.0 license)\n9We use the evaluation scripts provided in the official\nWebNLG challenge: https://github.com/WebNLG/\nGenerationEval (MIT license)\n543\nID Model BLEU↑ METEOR↑ TER↓\nSEEN UNSEEN ALL SEEN UNSEEN ALL SEEN UNSEEN ALL\n1 copy baseline 4.48 5.07 4.50 0.28 0.31 0.28 0.92 0.86 0.92\n2 GPT2-XL (0-shot) 13.13 13.88 13.26 0.23 0.27 0.23 0.69 0.78 0.70\n3 GPT2-XL(3-shot) 26.74 23.72 26.65 0.29 0.28 0.29 0.85 0.78 0.84\n4 GPT2-XL-PT 33.55 29.86 33.41 0.24 0.28 0.24 0.65 0.61 0.65\n5 GPT2-XL-PT + Reranking 31.03 31.67 31.09 0.28 0.30 0.28 0.63 0.58 0.63\n6 T5 large 48.41 43.48 48.25 0.39 0.40 0.39 0.46 0.44 0.46\n+Descriptions\n7 GPT2-XL(0-shot) 11.45 8.05 11.4 0.20 0.19 0.20 0.70 1.00 0.72\n8 GPT2-XL(3-shot) 26.32 21.30 26.14 0.28 0.27 0.28 0.83 0.89 0.83\n9 GPT2-XL-PT 33.96 31.37 33.85 0.24 0.28 0.24 0.66 0.59 0.66\n10 T5 large 48.56 43.82 48.4 0.39 0.39 0.39 0.46 0.45 0.46\nTable 2: Model results on test set of the DART dataset. ↑: Higher is better. ↓: Lower is better.\nID Model BLEU↑ METEOR↑ chrF++↑ TER↓ BERTScore(F1)↑ BLEURT↑\nEASY HARD EASY HARD EASY HARD EASY HARD EASY HARD EASY HARD\n11 copy baseline 18.00 2.01 0.41 0.23 0.45 0.32 0.79 0.99 0.88 0,80 0.12 -1.00\n12 GPT2-XL (0-shot) 22.20 6.92 0.34 0.18 0.47 0.31 0.83 0.64 0.90 0.88 -0.09 -0.54\n13 GPT2-XL (3-shot) 34.97 1.88 0.34 0.06 0.54 0.07 0.82 0.38 0.92 0.93 -0.09 -0.11\n14 GPT2-XL-PT 42.81 31.78 0.35 0.23 0.57 0.39 0.48 0.69 0.94 0.92 0.31 -0.17\n15 GPT2-XL-PT + Reranking43.35 25.79 0.37 0.29 0.60 0.48 0.47 0.66 0.94 0.93 0.34 -0.04\n16 T5large 70.54 38.34 0.51 0.35 0.80 0.57 0.23 0.59 0.97 0.94 0.70 0.20\n+Descriptions\n17 GPT2-XL (0-shot) 19.00 6.43 0.30 0.17 0.42 0.31 0.93 0.65 0.89 0.88 -0.20 -0.54\n18 GPT2-XL (3-shot) 34.19 20.54 0.38 0.26 0.61 0.44 0.92 0.81 0.93 0.91 0.07 -0.26\n19 GPT2-XL-PT 42.52 33.1 0.34 0.23 0.56 0.39 0.5 0.69 0.93 0.91 0.28 -0.21\n20 T5large 70.06 38.49 0.51 0.34 0.80 0.57 0.23 0.60 0.97 0.94 0.69 0.20\nTable 3: Model results on EASY and HARD partitions of the DART test set. ↑: Higher is better. ↓: Lower is better.\n6 Experiments\nWe evaluate PLMs with various input types and\ntraining regimes to answer the following empirical\nquestions:\n• How do the adaptation mechanism and level\nof supervision at train time affect PLM perfor-\nmance on the D2T task?\n• What classes of D2T examples are particu-\nlarly challenging for each PLM? How well do\nPLMs perform on out-of-sample predicates\nand examples that are more abstractive (dis-\nsimilar source and target sequences)?\n• Can we improve performance on examples\nwith unseen predicates by including predicate\ndescriptions in the prompt, as mentioned in\n§3.4?\n• Qualitatively, what kinds of errors do PLMs\nmake on the D2T task? Are some adaptation\ntechniques more susceptible to classes of er-\nrors than others?\n• Can we mitigate some of these errors by re-\nranking the decoding results?\n6.1 Results\nTable 2 presents model performance on the entire\nDART dataset (ALL), as well as the SEEN and\nUNSEEN partitions. See Appendix B for chrF++,\nBERTScore, and BLEURT results. Table 3 shows\nmodel performance on the EASY and HARD parti-\ntions.\nLevel of Supervision We first turn to GPT2-XL,\nwhich is evaluated on this task without any train-\ning data. Following previous work we find that\nGPT2-XL makes an effective 0-shot model, out-\nperforming the copy baseline according to BLEU\nand METEOR (row 2). Examining the output more\nclosely, we find that GPT2-XL mostly copies the\ninput; while it outperforms the copy baseline, its\nstrategy is largely the same. We include example\ngenerations in Appendix C. 3-shot GPT2-XL (row\n3) does much better than the 0-shot case. Note that\nin this setting, no model parameters are updated. In\naddition, the amount of annotated data used for cre-\nating 3-shot prompts is much less than what is used\nfor prompt tuning and fine-tuning. While few-shot\nprompting leads to a boost in BLEU and METEOR,\nTER increases by 0.14 point. We conjecture that\nthis is due to an increase in hallucinated content in\n544\nthis setting. We take a closer look at these patho-\nlogical behaviors in our human evaluation.\nBoth GPT2-XL models prompt tuned on the\nentire DART dataset (rows 4 and 5) outperform\nthe 3-shot model by a wide margin. As reported\npreviously (Nan et al., 2020), we also notice that\nfine-tuned T5 (row 6) performs well on this task\nsurpassing either prompt-tuned GPT2-XL model.\nConsistent with previous findings, we also no-\ntice that the more training data that is used to adapt\nthe model (either by few-shot learning or training\nmodel weights), the better PLMs perform. How-\never, in a resource-constrained setting, few-shot\nGPT2-XL achieves reasonable performance. Few-\nshot adaptation might be a good choice for D2T\nwhen the number of unique predicates in the test set\nis small, and only very few examples can be man-\nually annotated. On the other hand, if more data\nis available, fine-tuning T5 leads to better results\nfor D2T. In fact, our experiments show that T5 can\nsurpass the 3-shot GPT2-XL after fine-tuning on\nonly 200 examples. See Appendix B for details.\nPredicate Novelty As expected, the copy base-\nline (row 1) performs poorly across all conditions,\nbut consistent for both the SEEN and UNSEEN par-\ntitions. 0-shot GPT2-XL also performs similarly\non both partitions, since it was not trained on any\ntask data. GPT2-XL with a 3-shot prompt (row 3)\noutperforms 0-shot on both partitions, despite the\nunseen prompts including unrelated predicates; the\nmodel still benefits from multiple shots even if they\ndo not contain the same predicates (+9.84 BLEU\npoints).\nPrompt tuning and re-ranking generated sam-\nples by overlap with the triple set entities both\nimprove the performance of GPT2-XL on novel\npredicates. Overall, GPT2-XL performs consis-\ntently across SEEN and UNSEEN partitions, while\nT5 performance is more sensitive to whether the\npredicate was observed during training (e.g., differ-\nence of 4.93 points BLEU in row 6).\nWe next turn to evaluating the impact of aug-\nmenting prompts with predicate descriptions for\nunseen predicates. This process is described in\n§3.4. We evaluate this augmentation in the 0-shot\n(row 7), 3-shot (row 8) and prompt tuning (row 9)\nsettings, as well as in T5 fine-tuning (row 10). We\nobserve very small improvements on the UNSEEN\npartition and only in cases where model parameters\nare updated (rows 9 and 10). We suspect that as\ndescriptions are sourced from WordNet and Wiki-\nData, either many predicates could not be resolved\nto a description in these tables, or the predicates\nthat could be resolved were largely self-explanatory.\nWe conjecture that in the 0-shot setting, condition-\ning the generation on descriptions might distract the\nmodel from the head and tail entity. On the other\nhand, many of the unseen predicates in DART are\nnot words that can be easily resolved. However,\nwe suspect that if they were to be reliably resolved,\nspecialized domains such as finance or medicine\nwould benefit from adding predicate descriptions.\nGeneration Difficulty Table 3 shows the perfor-\nmance of all models on the EASY and HARD par-\ntitions. All models have noticeably worse perfor-\nmance on HARD examples, where more abstrac-\ntion is needed. The best performing model, T5\n(row 16), has a gap of 0.16 METEOR between the\nEASY and HARD partition, while the prompt tuned\nGPT2-XL (row 14) has the smallest difference in\nperformance between the partitions. It is clear that\nthese models perform well overall when copying\nfrom the input suffices, but do poorly when sig-\nnificant rewriting is required. In many domains,\nwe may prefer models with more diverse, creative\ngenerations, a task at which these models do not\ndo well. On the other hand, DART is a mostly au-\ntomatically derived dataset, with significant errors\nin some examples, where the reference text may\ncontain information that is unsupported by the in-\nput triple. These examples may pervade the HARD\npartition.\nNext, we investigate the impact of adding pred-\nicate descriptions on D2T of the HARD partition.\nIn the few-shot setting, adding predicate descrip-\ntions improves the BLEU score to 20.54 on the\nHARD partition (row 18). Conditioning the model\non predicate descriptions significantly enhances its\nre-writing ability. For the prompt tuned GPT2-XL,\nBLEU score improves to 33.1 (row 19). However,\nwe do not see any gains for 0-shot GPT or T5 (rows\n17 and 20). Overall, GPT2-XL benefits from predi-\ncate descriptions on examples where significant re-\nwriting is needed, even when additionally prompt\ntuned. GPT2-XL with prompt tuning achieves com-\npetitive results with benchmark T5 on the HARD\npartition (33.1 vs 38.49 BLEU).\nHuman Evaluation To further examine the\npathological behaviors of the models, we randomly\nsampled 50 examples from the DART test set for\nhuman evaluation. For each example, the output\n545\nSource Hallucination ↓ Missing Info↓ Fluency↑\nReference 1.53 1.19 4.51\nGPT2-XL(3-shot) 3.26 3.61 3.17\nGPT2-XL-PT 1.73 3.35 4.64\nGPT2-XL-PT + Ranking 1.73 2.79 4.75\nT5large 1.16 1.23 4.79\nAgreement 0.64 0.77 0.50\nTable 4: Results of the qualitative evaluation. ↓: Lower\nis better. ↑: Higher is better. Inter-annotator agreement\nis measured by Kendall’s τ rank correlation coefficient.\nof T5 and GPT2-XL in the 3-shot, prompt tuned,\nand re-ranked settings were presented to two an-\nnotators.10 We also showed the reference text as\nanother candidate, with the generating model iden-\ntity hidden. Annotators evaluated output quality\nbased on three criteria: (1) whether it contains hal-\nlucinated content (hallucination) (2) whether the\ntext is missing information from the input records\n(missing info), and (3) fluency. Annotators indi-\ncated agreement with each of these Likert items\non an ordinal scale from 1 (strongly disagree) to 5\n(strongly agree).\nTable 4 presents the average annotator score ac-\ncording to each of these Likert items. GPT2-XL\nin the 3-shot setting often misses information. No-\ntably, both prompt-tuned variations generate very\nfluent text. Re-ranking improves the quality of\nthe generations by decreasing the amount of miss-\ning information and improving fluency. While the\nbest GPT2-XL model does very similar to T5large\nin terms of fluency, on average it hallucinates or\nmisses information more often.\nRe-ranking GPT2-XL prompt tuned is both pa-\nrameter efficient and generalizes very well to novel\npredicates. It also does very well on examples that\nrequire more re-writing. It approaches the perfor-\nmance of fine-tuned T5large according to avoiding\nhallucinations and fluency. During the human eval-\nuation, we observe that this model would often miss\nthe subject or object of the predicate in its gener-\nations (see our human evaluation for details). We\ncan mitigate this problem without additional model\ntraining through a re-ranking strategy to ensure\nthat the selected generation contains all relevant\ninformation.\nWe first create multiple candidate generations by\nincreasing beam size during decoding. Next, we\ncompute the percentage of head and tail entities\ncovered in the text. Finally, we pick the candidate\nthat contains the highest percentage of entity spans\n10Two of the paper authors.\nfrom the input triple.11 Rows 5 and 15 show the re-\nsults of re-ranking a GPT2-XL prompt tuned model.\nRe-ranking modestly improves performance on all\npartitions, and across all metrics except BLEU.\n7 Conclusion and Future Work\nIn this work, we systematically analyze the perfor-\nmance of two PLMs – T5 and GPT2-XL – for D2T\ngeneration by examining performance based on\nthe choice of adaptation mechanism: fine-tuning,\nprompt tuning, and few-shot learning. We observe\nthat while fine-tuning on more data leads to better\nperformance, when no training data is available,\nGPT2-XL (0-shot) outperforms T5. With a small\nnumber of training examples, few-shot GPT2-XL\nis a more appropriate solution for D2T.\nWe also conduct a thorough investigation of D2T\nchallenges for PLMs by evaluating them on two\ndivisions of the DART test set: novel predicates\nand abstractive examples. We show that the per-\nformance of fine-tuned T5 drops significantly on\nunseen predicates. On the other hand, the perfor-\nmance of few-shot GPT2-XL on unseen predicates\ncan be enhanced even with shots containing un-\nrelated predicates. We also notice that T5 and\nGPT2-XL both do well at D2T by copying the\ninput. However, they do noticeably worse on exam-\nples where significant re-writing is needed. Adding\ndomain knowledge (predicate descriptions) to the\nprompts can improve the performance of few-shot\nGPT2-XL on this subset by a large amount. We\nalso conduct a human evaluation of the generations\nand find that prompt tuned GPT2-XL generations\ncan be improved by re-ranking generations by over-\nlap with the input entity spans.\nFuture work in D2T generation should consider\nmore challenging examples, and should consider\nways in which to generate more diverse variations\nfor expressing a given predicate. This should in-\nclude more challenging and disparate domains,\nsuch as finance or medicine. In these cases, one\nmay see benefits from including predicate descrip-\ntions, which performed well on the most abstractive\nexamples.\nLimitations\nAn important challenge for D2T is how to train\nmodels that can generalize to new domains. While\n11We use a beam size of 20 during decoding. Prior to\nmeasuring the entity coverage in the candidates, we normalize\nthe text by lower casing and removing special characters.\n546\nthis work looked at a related class of examples\n(instances with unseen predicates), it would be\ninteresting to investigate how PLMs trained on\none domain can be efficiently adapted to perform\nD2T on another unrelated domain (e.g., sports\nto finance). This would require creating domain-\nspecific datasets for D2T.\nMoreover, we observed that adding domain\nknowledge (predicate descriptions) to prompts can\nimprove the performance of few-shot GPT2-XL\non abstractive examples. We suspect that this idea\nmay work better on specialized domains, with bet-\nter relation descriptions, or with a larger language\nmodel; we could not test this without a specialized\nD2T dataset with better task relation descriptions.\nFinally, many applications prefer generating\nnovel or interesting descriptions for a data record\nover “safe” and “generic” ones, which are predom-\ninant in our training data (Li et al., 2015, 2016;\nBaheti et al., 2018; Shao et al., 2021). Evaluating\nPLMs for diversity of generated text is an orthogo-\nnal and promising future direction.\nReferences\nAshutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan.\n2018. Generating more interesting responses in\nneural conversation models with distributional con-\nstraints. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3970–3980, Brussels, Belgium. Association\nfor Computational Linguistics.\nDavid L Chen and Raymond J Mooney. 2008. Learning\nto sportscast: a test of grounded language acquisition.\nIn Proceedings of the 25th international conference\non Machine learning, pages 128–135.\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu,\nand William Yang Wang. 2019. Few-shot NLG\nwith pre-trained language model. arXiv preprint\narXiv:1904.09521.\nJishnu Ray Chowdhury, Yong Zhuang, and Shuyi Wang.\n2022. Novelty controlled paraphrase generation with\nretrieval augmented conditional prompt tuning. In\nAssociation for the Advancement of Artificial Intelli-\ngence (AAAI). AAAI.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nuniversal: Language specific translation evaluation\nfor any target language. In Proceedings of the ninth\nworkshop on statistical machine translation, pages\n376–380.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nOndˇrej Dušek, David M Howcroft, and Verena Rieser.\n2019. Semantic noise matters for neural natural lan-\nguage generation. arXiv preprint arXiv:1911.03905.\nNouha Dziri, Ehsan Kamalloo, Sivan Milton, Os-\nmar Zaiane, Mo Yu, Edoardo M Ponti, and Siva\nReddy. 2022a. FaithDial: A faithful benchmark\nfor information-seeking dialogue. arXiv preprint\narXiv:2204.10757.\nNouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and\nSiva Reddy. 2022b. On the origin of hallucinations\nin conversational models: Is it the datasets or the\nmodels? arXiv preprint arXiv:2204.07931.\nAngela Fan, Claire Gardent, Chloé Braud, and Antoine\nBordes. 2019. Using local knowledge graph con-\nstruction to scale seq2seq models to multi-document\ninputs. arXiv preprint arXiv:1910.08435.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The WebNLG\nchallenge: Generating text from RDF data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 124–133.\nAlbert Gatt and Emiel Krahmer. 2018. Survey of the\nstate of the art in natural language generation: Core\ntasks, applications and evaluation. Journal of Artifi-\ncial Intelligence Research, 61:65–170.\n547\nHeng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin,\nWei Bi, Xiaojiang Liu, and Ting Liu. 2020. Tablegpt:\nFew-shot table-to-text generation with table structure\nreconstruction and content matching. In Proceedings\nof the 28th International Conference on Computa-\ntional Linguistics, pages 1978–1988.\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\nHave your text and use it too! End-to-end neural\ndata-to-text generation with semantic fidelity. arXiv\npreprint arXiv:2004.06577.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\narXiv preprint arXiv:1801.06146.\nMihir Kale and Abhinav Rastogi. 2020. Text-to-text\npre-training for data-to-text tasks. In Proceedings of\nthe 13th International Conference on Natural Lan-\nguage Generation, pages 97–102, Dublin, Ireland.\nAssociation for Computational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nLeo Leppänen, Myriam Munezero, Mark Granroth-\nWilding, and Hannu Toivonen. 2017. Data-driven\nnews generation for automated journalism. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 188–197.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2015. A diversity-promoting objec-\ntive function for neural conversation models. arXiv\npreprint arXiv:1510.03055.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. A\nsimple, fast diverse decoding algorithm for neural\ngeneration. arXiv preprint arXiv:1611.08562.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021a. What\nmakes good in-context examples for GPT-3? arXiv\npreprint arXiv:2101.06804.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nKaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg,\nand Jianfeng Gao. 2021. Open domain question an-\nswering over virtual documents: A unified approach\nfor data and text. arXiv preprint arXiv:2110.08417.\nManuel Mager, Ramón Fernandez Astudillo, Tahira\nNaseem, Md Arafat Sultan, Young-Suk Lee, Radu\nFlorian, and Salim Roukos. 2020. GPT-too: A\nlanguage-model-first approach for amr-to-text gener-\nation. arXiv preprint arXiv:2005.09123.\nHongyuan Mei, Mohit Bansal, and Matthew R Walter.\n2015. What to talk about and how? selective genera-\ntion using lstms with coarse-to-fine alignment. arXiv\npreprint arXiv:1509.00838.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nEnglish. Communications of the ACM, 38(11):39–\n41.\nMike Mintz, Steven Bills, Rion Snow, and Dan Juraf-\nsky. 2009. Distant supervision for relation extraction\nwithout labeled data. In Proceedings of the Joint Con-\nference of the 47th Annual Meeting of the ACL and\nthe 4th International Joint Conference on Natural\nLanguage Processing of the AFNLP , pages 1003–\n1011.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. Opendialkg: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845–854.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru\nTang, Aadit Vyas, Neha Verma, Pranav Krishna, et al.\n2020. Dart: Open-domain structured data record to\ntext generation. arXiv preprint arXiv:2007.02871.\nJekaterina Novikova, Ondˇrej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-to-\nend generation. arXiv preprint arXiv:1706.09254.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nVassilis Plachouras, Charese Smiley, Hiroko Bretz, Ola\nTaylor, Jochen L Leidner, Dezhao Song, and Frank\nSchilder. 2016. Interacting with financial data using\nnatural language. In Proceedings of the 39th Inter-\nnational ACM SIGIR conference on Research and\nDevelopment in Information Retrieval, pages 1121–\n1124.\n548\nMaja Popovi´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395.\nFrançois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,\nSomayajulu Sripada, Yvonne Freer, and Cindy Sykes.\n2009. Automatic generation of textual summaries\nfrom neonatal intensive care data. Artificial Intelli-\ngence, 173(7-8):789–816.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nAlejandro Ramos-Soto, Alberto Jose Bugarin, Senén\nBarro, and Juan Taboada. 2014. Linguistic descrip-\ntions for automatic generation of textual short-term\nweather forecasts on real prediction data. IEEE\nTransactions on Fuzzy Systems, 23(1):44–57.\nLeonardo FR Ribeiro, Martin Schmitt, Hinrich Schütze,\nand Iryna Gurevych. 2020. Investigating pretrained\nlanguage models for graph-to-text generation. arXiv\npreprint arXiv:2007.08426.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D Manning. 2019. Do\nmassively pretrained language models make better\nstorytellers? arXiv preprint arXiv:1909.10705.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\n2020. BLEURT: Learning robust metrics for text\ngeneration. arXiv preprint arXiv:2004.04696.\nHuajie Shao, Jun Wang, Haohong Lin, Xuezhou\nZhang, Aston Zhang, Heng Ji, and Tarek Abdelzaher.\n2021. Controllable and diverse text generation in\ne-commerce. In Proceedings of the Web Conference\n2021, pages 2392–2401.\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A study\nof translation edit rate with targeted human annota-\ntion. In Proceedings of the 7th Conference of the\nAssociation for Machine Translation in the Americas:\nTechnical Papers, pages 223–231.\nChris van der Lee, Emiel Krahmer, and Sander Wubben.\n2017. PASS: A dutch data-to-text system for soccer,\ntargeted towards specific audiences. In Proceedings\nof the 10th International Conference on Natural Lan-\nguage Generation, pages 95–104.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nSam Wiseman, Stuart M Shieber, and Alexander M\nRush. 2017. Challenges in data-to-document genera-\ntion. arXiv preprint arXiv:1707.08052.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\n549\n0 100 200 300 400 500\n# Training Examples\n0\n5\n10\n15\n20\n25\n30\n35\n40BLEU\n2.73.1\n10.8\n21.7\n27.7\n33.1\nT5\ncopy baseline \nGPT (0-shot)\nGPT (3-shot)\nGPT-PT\nFigure 3: Impact of fine-tuning data size on performance\nof T5. Numbers reflect average performance over 5\ndifferent data samples, with standard error of the mean\nindicated by bars.\nAppendices\nA Data Splits\nExamples from the EASY and HARD partitions are\nshown in Figure 4. The copy baseline achieves\ngood results on the EASY examples. On the other\nhand, the examples from the HARD partition are\nmore abtractive – generating descriptions for these\nexamples requires substantial rewriting. In several\ncases, the reference text has a low fidelity with\nrespect to the input record. For example, when one\nor more triples in the input are not described in the\nreference text. This is a data quality issue and is a\ncommon occurrence in DART.\nB Results\nExperimental results on SEEN and UNSEEN parti-\ntions are presented in Table 5. As reported in § 6,\nT5 performs well on this task (row 6). The 0-shot\nGPT2-XL outperforms the copy baseline in terms\nof all metrics except for chrF++ (row 2). GPT2-XL\nwith a 3-shot prompt does much better than the 0-\nshot case. Prompt tuning improves the results both\nin terms of BertScore and BLEURT (row 4). We\nsee another gain in the performance by adding re-\nranking (row 5). These trends are consistent with\nwhat we observed for BLEU, METEOR, and TER\nin Table 2.\nWe do not see a consistent performance drop\ngoing from SEEN to the UNSEEN partition when\nlooking at chrF++, BertScore, and BLEURT. This\nis somewhat surprising, but also hard to interpret\ngiven that chrF++ relies on character n-gram and\nBertScore and BLEU rely in contextualized embed-\ndings.\nTraining Curves In this experiment, we seek to\nanswer that how much data does T5 require to do\nwell on this task? Specifically, how many exam-\nples are required for T5 to exceed the performance\nof the few-shot GPT2-XL? We fine-tune T5 on\nincreasingly larger amounts of training data. We\nstart off with an off-the-shelf T5 model with no\nadditional training. We then vary the number of\ntraining examples in {10, 20, 50, 100, 200, 500}.12\nWe repeat each setting five times by resampling a\ntraining set and fine-tuning T5, and report results\nfor each training set size averaged cross all test\npartitions. Figure 3 shows the BLEU performance\n(y-axis) of T5 as a function of number of training\nexamples (x-axis). Performance of the copy base-\nline, 0-shot, 3-shot, and prompt tuned GPT2-XL\nare indicated by horizontal lines. Without any task-\nspecific fine-tuning, T5 does slightly worse than\nthe copy baseline, easily outperformed by 0-shot\nGPT2-XL. In settings without training data, GPT2-\nXL is the clear choice. T5 continues to lag behind\nGPT2-XL 3-shot until trained on at least 200 ex-\namples, and meets the performance of GPT2-XL\nprompt tuned after training on 500.\nC Sample Model Output\nIn this section, we share a few samples from the\nDART test set as well as outputs generated by dif-\nferent models. We qualitatively compare different\nmodels and highlight a few of their common errors.\nTask Prompting As seen in Examples 1 and 2,\nGPT2-XL in the 0-shot setting often copies the\ninput. GPT2-XL with a 3-shot prompt generates a\nmuch more fluent text than the 0-shot case. This\ncan be seen in Examples 2, 4, and 5. Although\nGPT2-XL with few-shot prompting generates more\nfluent text, it often generates hallucinated content\n(see Example 3).\nWe see that prompt tuning further boosts our\nperformance and generates a more coherent text in\ncomparison to few-shot GPT2-XL (see Example 1\nand 3). Moreover, it hallucinates much less than\nthe few-shot setting (e.g. see Example 3). We also\nsaw this previously in Table 2, as the prompt tuned\nGPT2-XL achieved lower TER score. In contrast\nto T5 training, in which all model parameters are\nupdated, prompt tuning adapts only a small fraction\n12We use the same hyper-parameters as before except for\nthe number of training epochs and batch size. To avoid over-\nfitting on small data, we only fine-tune for 1 epoch. We use\nbatch size of 2.\n550\nID Model chrF++ ↑ BERTScore(F1)↑ BLEURT↑\nSEEN UNSEEN ALL SEEN UNSEEN ALL SEEN UNSEEN ALL\n1 copy baseline 0.33 0.34 0.33 0.83 0.85 0.83 -0.59 -0.29 -0.58\n2 GPT2-XL (0-shot) 0.34 0.34 0.34 0.88 0.87 0.88 -0.46 -0.30 -0.46\n3 GPT2-XL (3-shot) 0.48 0.44 0.48 0.91 0.91 0.91 -0.19 -0.17 -0.19\n4 GPT2-XL-PT 0.40 0.44 0.40 0.92 0.92 0.92 -0.11 0.06 -0.10\n5 GPT2-XL-PT + Reranking 0.46 0.47 0.46 0.92 0.92 0.92 -0.01 0.12 0.00\n6 T5 large 0.64 0.64 0.64 0.95 0.95 0.95 0.38 0.44 0.39\n+ Description\n7 GPT2-XL (0-shot) 0.31 0.23 0.30 0.88 0.86 0.88 -0.46 -0.54 -0.46\n8 GPT2-XL (3-shot) 0.47 0.42 0.46 0.91 0.90 0.91 -0.19 -0.16 -0.19\n9 GPT2-XL-PT 0.39 0.45 0.39 0.91 0.92 0.91 -0.14 0.09 -0.13\n10 T5 large 0.64 0.63 0.64 0.95 0.95 0.95 0.38 0.43 0.38\nTable 5: Performance on the DART test set, partitioned by whether predicates areSEEN , UNSEEN , and overall. ↑:\nHigher is better.\nof the model parameters. However, in many cases\nthe generated text is as good as the benchmark T5\n(see Example 2). Despite generating very fluent\ntext, prompt tuned GPT2-XL often misses infor-\nmation from one or more relations (Examples 1, 3,\nand 4).\nRe-ranking Re-ranking based on entity cover-\nage solves the missing information issue in several\ncases. For example, in Example 3, the entity Alvis\nSpeed 25 which is missed by the prompt tuned\nGPT2-XL, is covered after re-ranking. The ben-\nefit of re-ranking also can be seen in Example 4.\nOn the other hand, in Example 2, ranking does\nnot solve the missing information issue. This is\nbecause argument \"yes\" of \"family-friendly\" prob-\nably would not naturally appear in generated text\n(e.g., \"Yes, this is a family-friendly restaurant\").\nFor such cases, the re-ranking heuristic will not\nprovide useful feedback.\nPredicate Descriptions As mentioned in Sec-\ntion 6.1, in several cases, the description extracted\nfrom WordNet and WikiData are trivial. In Exam-\nple 2, the definition of relations food, area, and\nnear add no information beyond the word itself,\nand therefore not helpful for the model. On the\nother hand, it seems like defining relation MAN-\nUFACTURERin Example 3 has improved genera-\ntions of GPT2-XL in both the few-shot and prompt-\ntuned settings. In some cases, while the predicate\ndescription can be potentially useful, the model ig-\nnores the augmented description. For example, in 4,\nthe definition of relation GENRE is not covered in\nthe generated text of any of models.\n551\nEASY Examples\nInput: <H> Adolfo Suárez Madrid–Barajas Airport <R> LOCATION <T> Madrid, Paracuellos de Jarama, San Sebastián de\nlos Reyes and Alcobendas\nReference: Adolfo Suárez Madrid–Barajas Airport can be found in Madrid, Paracuellos de Jarama, San Sebastián de los\nReyes and Alcobendas.’\n###\nInput: <H> Alaa Abdul-Zahra <R> CLUB <T> Sanat Mes Kerman F.C.\nReference: Alaa Abdul-Zahra’s club is Sanat Mes Kerman F.C.\n###\nInput: <H> Alderney Airport <R> RUNW AY_NAME <T> \"14/32\"\nReference: Alderney Airport runway name is 14/32\n###\nInput: <H> Asunción <R> IS_PART_OF <T> Gran Asunción\nReference: Asunción is a part of Gran Asunción.\n###\nInput: <H> Airey Neave <R> AW ARD <T> Military Cross\nReference: Airey Neave was awarded the Military Cross.\nHARD Examples\nInput: <H> 2004 <R> MOVEMENTS <T> Promotion Playoffs - Promoted <H> 2004 <R> POSITION <T> 1st\nReference: Sports stats for Ljungskile SK\n###\nInput: <H> Khokhan Sen <R> MATCHES <T> 14 <H> Khokhan Sen <R> INNINGS <T> 21 <H> Khokhan Sen <R>\nRANK <T> 9 <H> Khokhan Sen <R> CAUGHT <T> 20 <H> Khokhan Sen <R> STUMPED <T> 11 <H> Khokhan Sen\n<R> DISMISSALS <T> 31\nReference: The innings when caught was 20 was 21\n###\nInput: <H> thierry morin <R> POSITION <T> defender <H> [TABLECONTEXT] <R> NAME <T> thierry morin <H>\n[TABLECONTEXT] <R> [TITLE] <T> Players\nReference: Thierry Morin was a defender for Paris Saint-Germain.\n###\nInput: <H> ALV X-1 <R> COUNTRY_ORIGIN <T> United States <H> United States <R> ETHNIC_GROUP <T>\nAfrican Americans <H> United States <R> DEMONYM <T> Americans\nReference: Originating in the United States and by Americans, some of African decent is the ALVX-1.’, ’ALVX-1 comes\nfrom the US where Americans live and African Americans are an ethnic group\n###\nInput: <H> past tense <R> SEASON_# <T> 4 <H> past tense <R> ORIGINAL_AIR_DATE <T> october29,2008 <H> past\ntense <R> NO._IN_SERIES <T> 13 <H> past tense <R> U.S._VIEWERS_(MILLIONS) <T> 7.93 <H> past tense <R>\nDIRECTED_BY <T> michael pressman <H> past tense <R> WRITTEN_BY <T> craig turk\nReference: Past Tense was the 13th episode in the series.\nFigure 4: Examples from the EASY and HARD partition\n552\nExample #1\nInput: <H> Aleksey Chirikov (icebreaker) <R> SHIP_BEAM <T> 21.2 <H> Aleksey Chirikov (icebreaker) <R> STATUS\n<T> \"\"In service\"\" <H> Aleksey Chirikov (icebreaker) <R> BUILDER <T> Helsinki <H> Aleksey Chirikov (icebreaker)\n<R> BUILDER <T> Arctech Helsinki Shipyard\nReference: The icebreaker Aleksey Chirikov was built at the Arctech Helsinki Shipyard in Helsinki. The Aleksey Chirikov\n(icebreaker) is in service and its ship beam is 21.2\nDescription: BUILDER means a person who constructs something\nGPT2-XL (0-shot): <H> Aleksey Chirikov (icebreaker) <R> SHIP_BEAM <T> 21.2 <H> Aleksey Chirikov (icebreaker)\n<R> STATUS <T> \"In service\" <H> Aleksey Chirikov (icebreaker) <R> BUILDER <T> Helsinki <H> Aleksey Chirikov\n(icebreaker) <R> BUILDER <T>\nGPT2-XL (3-shot): Aleksey Chirikov (icebreaker) is of type 21.2.\nGPT2-XL (3-shot) + Description: Aleksey Chirikov (icebreaker) was a Russian icebreaker.\nGPT2-XL-PT: Aleksey Chirikov (icebreaker) was built by Arctech Helsinki Shipyard.\nGPT2-XL-PT + Description: Aleksey Chirikov (icebreaker) was built by Arctech Helsinki Shipyard.\nGPT2-XL-PT + Reranking: Aleksey Chirikov (icebreaker) is in service at Arctech Helsinki Shipyard.\nT5large: The ship Aleksey Chirikov was built by Arctech, Helsinki Shipyard and has a ship beam of 21.2 metres. It is currently\nin service.\nExample #2\nInput: <H> The Mill <R> eatType <T> pub <H> The Mill <R> food <T> Fast food <H> The Mill <R> area <T> city centre\n<H> The Mill <R> familyFriendly <T> no <H> The Mill <R> near <T> Café Sicilia\nReference: The Mill pub is near Café Sicilia in the city centre that isn’t family friendly, is average and has Fast food.\nDescription: eatType means type of food served by a restaurant or restaurant chain. food means any substance that can be\nmetabolized by an animal to give energy and build tissue. area means area occupied by an object. near means move towards.\nGPT2-XL (0-shot): Graph: <H> The Mill <R> eatType <T> pub <H> The Mill <R> food <T> Fast food <H> The Mill <R>\narea <T> city centre <H> The Mill <R> familyFriendly <T> no <H> The Mill <R> near <T> Café Sicilia\nEnglish:\nGPT2-XL (3-shot): The Mill is located in the city centre near Café Sicilia. It serves fast food in the price range of £20-25\nwith a moderate customer rating\nGPT2-XL (3-shot) + Description: The fast food restaurant, The Mill, has an average rating of 3 out of 5 perhaps as it is not\nfriendly to kids. The Mill is located in the city centre area near Café Sicilia.\nGPT2-XL-PT: The Mill is a fast food pub in the city centre near Café Sicilia.\nGPT2-XL-PT + Description: The Mill is a family friendly pub near Cafe Sicilia.\nGPT2-XL-PT + Reranking: The Mill is a fast food pub in the city centre near Café Sicilia.\nT5large: The Mill is a fast food pub in the city centre near Café Sicilia.\nExample #3\nInput: <H> Alvis Speed 25 <R> MANUFACTURER <T> Alvis Car and Engineering Company <H> Alvis Car and\nEngineering Company <R> LOCATION_CITY <T> Coventry\nReference: Alvis Speed 25 was made by the Alvis Car and Engineering Company of Coventry.\nDescription: MANUFACTURER means manufacturer or producer of this product.\nGPT2-XL (0-shot): <H> Alvis Speed 25 <R> MANUFACTURER <T> Alvis Car and Engineering Company <H> Alvis\nCar and Engineering Company <R> LOCATION_CITY <T> Coventry\nGPT2-XL (3-shot): Alvis Speed 25 won the race in Coventry on April 25, 1996.\nGPT2-XL (3-shot) + Description: Alvis Speed 25 is a car manufactured by Alvis Car and Engineering Company in Coventry,\nEngland\nGPT2-XL-PT: Alvis Car and Engineering Company is located in Coventry.\nGPT2-XL-PT + Description: Alvis Speed 25 is manufactured in Coventry.\nGPT2-XL-PT + Reranking: Alvis Speed 25 is manufactured by Alvis Car and Engineering Company in Coventry.\nT5large: Alvis Car and Engineering Company is located in Coventry and manufactures the Alvis Speed 25.\n553\nExample #4\nInput: <H> Anders Osborne <R> ASSOCIATED_BAND_ASSOCIATED_MUSICAL_ARTIST <T> Galactic <H> Anders\nOsborne <R> ASSOCIATED_BAND_ASSOCIATED_MUSICAL_ARTIST <T> Tab Benoit <H> Anders Osborne <R>\nGENRE <T> Blues <H> Anders Osborne <R> ASSOCIATED_BAND_ASSOCIATED_MUSICAL_ARTIST <T> Billy Iuso\n<H> Anders Osborne <R> ACTIVE_YEARS_START_YEAR <T> 1989\nReference: Anders Osborne musical career began in 1989. His career began in the blue’s genre. He has been associated\nwith the band, Galactic as well as Tab Benoit. Osborne also has ties to fellow musician Billy Iuso.\nDescription: GENRE means creative work’s genre or an artist’s field of work\nGPT2-XL (0-shot): <H> Anders Osborne <R> ASSOCIATED_BAND_ASSOCIATED_MUSICAL_ARTIST <T> Galactic\n<H> Anders Osborne <R> ASSOCIATED_BAND_ASSOCIATED_MUSICAL_ARTIST <T> Tab Benoit <H> Anders\nOsborne <R> GENRE <T> Blues <H> Anders Osborne <R> ASSOCIATED_BAND_ASSOCIATED_M\nGPT2-XL (3-shot): Anders Osborne is an English singer, songwriter and guitarist.\nGPT2-XL (3-shot) + Description: Anders Osborne is an active musician and composer.\nGPT2-XL-PT: Tab Benoit and Anders Osborne are associated with Galactic.\nGPT2-XL-PT + Description: Tab Benoit and Anders Osborne are associated with Galactic.\nGPT2-XL-PT + Reranking: Tab Benoit is associated with Galactic and Anders Osborne is associated with Billy Iuso.\nT5large: Anders Osborne is associated with the band Galactic, Billy Iuso, Tab Benoit and the genre of Blues. His active years\nstart in 1989.\nExample #5\nInput: <H> Nurburgring <R> FASTEST_LAP <T> PSV Eindhoven <H> Nurburgring <R> WINNING_CLUB <T> A.C.\nMilan\nReference: PSV Eindhoven had the fastest lab on the Nurburgring, and A.C. Milan was the winning club at that race.\nDescription: FASTEST_LAP means person who does the fastest lap during a motorsport race.\nGPT2-XL (0-shot): <H> Nurburgring <R> FASTEST_LAP <T> PSV Eindhoven <H> Nurburgring <R> WINNING_CLUB\n<T> A.C. Milan\nGPT2-XL (3-shot): A.C. Milan had the fastest lap in the race won by PSV Eindhoven.\nGPT2-XL (3-shot) + Description: A.C. Milan set the fastest lap at the Nurburgring when the winning team was PSV\nEindhoven.\nGPT2-XL-PT: Nurburgring is the fastest lap at A.C. Milan.\nGPT2-XL-PT + Description: PSV Eindhoven is the winning club at the Nurburgring.\nGPT2-XL-PT + Reranking: Nurburgring is the fastest lap at A.C. Milan\nT5large: A.C. Milan won the race where PSV Eindhoven had the fastest lap.\n554"
}