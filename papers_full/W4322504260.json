{
    "title": "CCTCOVID: COVID-19 detection from chest X-ray images using Compact Convolutional Transformers",
    "url": "https://openalex.org/W4322504260",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4322564125",
            "name": "Abdolreza Marefat",
            "affiliations": [
                "Islamic Azad University South Tehran Branch"
            ]
        },
        {
            "id": "https://openalex.org/A4322564126",
            "name": "Mahdieh Marefat",
            "affiliations": [
                "Islamic Azad University, Science and Research Branch"
            ]
        },
        {
            "id": "https://openalex.org/A2937883819",
            "name": "Javad Hassannataj Joloudari",
            "affiliations": [
                "University of Birjand"
            ]
        },
        {
            "id": "https://openalex.org/A2109905072",
            "name": "Mohammad Ali Nematollahi",
            "affiliations": [
                "Fasa University of Medical Sciences",
                "Fasa University"
            ]
        },
        {
            "id": "https://openalex.org/A1097874669",
            "name": "Reza Lashgari",
            "affiliations": [
                "Shahid Beheshti University"
            ]
        },
        {
            "id": "https://openalex.org/A4322564125",
            "name": "Abdolreza Marefat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4322564126",
            "name": "Mahdieh Marefat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2937883819",
            "name": "Javad Hassannataj Joloudari",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109905072",
            "name": "Mohammad Ali Nematollahi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1097874669",
            "name": "Reza Lashgari",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3082209631",
        "https://openalex.org/W6803502595",
        "https://openalex.org/W3043216367",
        "https://openalex.org/W3013334176",
        "https://openalex.org/W3089168916",
        "https://openalex.org/W3195997474",
        "https://openalex.org/W6748314208",
        "https://openalex.org/W2273396394",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2809254203",
        "https://openalex.org/W2565516711",
        "https://openalex.org/W2910603373",
        "https://openalex.org/W2999711171",
        "https://openalex.org/W4223899585",
        "https://openalex.org/W2804860796",
        "https://openalex.org/W6769243733",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W3165304031",
        "https://openalex.org/W2997775613",
        "https://openalex.org/W3019623663",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W2592929672",
        "https://openalex.org/W3105081694",
        "https://openalex.org/W3080406710",
        "https://openalex.org/W3125413540",
        "https://openalex.org/W3048749423",
        "https://openalex.org/W3013507463",
        "https://openalex.org/W3087300877",
        "https://openalex.org/W4214756141",
        "https://openalex.org/W4226275002",
        "https://openalex.org/W6801855556",
        "https://openalex.org/W6803754060",
        "https://openalex.org/W3117052584",
        "https://openalex.org/W3032017599",
        "https://openalex.org/W3017855299",
        "https://openalex.org/W3030621456",
        "https://openalex.org/W3028427008",
        "https://openalex.org/W3155545305",
        "https://openalex.org/W3185654650",
        "https://openalex.org/W3193713564",
        "https://openalex.org/W4313492145",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3204349696",
        "https://openalex.org/W3028070348",
        "https://openalex.org/W3019336217",
        "https://openalex.org/W2940616741",
        "https://openalex.org/W6780086851",
        "https://openalex.org/W3041463877",
        "https://openalex.org/W4284958810",
        "https://openalex.org/W4282924627",
        "https://openalex.org/W3170900500",
        "https://openalex.org/W4220807233",
        "https://openalex.org/W3083972167",
        "https://openalex.org/W3129482290",
        "https://openalex.org/W3017644243",
        "https://openalex.org/W3207610089",
        "https://openalex.org/W3037666819",
        "https://openalex.org/W4200556933",
        "https://openalex.org/W6746693533",
        "https://openalex.org/W2789876780",
        "https://openalex.org/W3216045725",
        "https://openalex.org/W3200177155",
        "https://openalex.org/W3162351260",
        "https://openalex.org/W4226325015",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3035618017",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3211135382",
        "https://openalex.org/W4300485340",
        "https://openalex.org/W3184998487"
    ],
    "abstract": "COVID-19 is a novel virus that attacks the upper respiratory tract and the lungs. Its person-to-person transmissibility is considerably rapid and this has caused serious problems in approximately every facet of individuals' lives. While some infected individuals may remain completely asymptomatic, others have been frequently witnessed to have mild to severe symptoms. In addition to this, thousands of death cases around the globe indicated that detecting COVID-19 is an urgent demand in the communities. Practically, this is prominently done with the help of screening medical images such as Computed Tomography (CT) and X-ray images. However, the cumbersome clinical procedures and a large number of daily cases have imposed great challenges on medical practitioners. Deep Learning-based approaches have demonstrated a profound potential in a wide range of medical tasks. As a result, we introduce a transformer-based method for automatically detecting COVID-19 from X-ray images using Compact Convolutional Transformers (CCT). Our extensive experiments prove the efficacy of the proposed method with an accuracy of 99.22% which outperforms the previous works.",
    "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/seven.tnum February /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nOPEN ACCESS\nEDITED BY\nWellington Pinheiro dos Santos,\nFederal University of Pernambuco, Brazil\nREVIEWED BY\nMaira Santana,\nUniversidade de Pernambuco, Brazil\nJuliana Gomes,\nFederal University of Pernambuco, Brazil\n*CORRESPONDENCE\nMohammad Ali Nematollahi\nma.nematollahi@fasau.ac.ir\nSPECIALTY SECTION\nThis article was submitted to\nDigital Public Health,\na section of the journal\nFrontiers in Public Health\nRECEIVED /one.tnum/five.tnum October /two.tnum/zero.tnum/two.tnum/two.tnum\nACCEPTED /zero.tnum/seven.tnum February /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /two.tnum/seven.tnum February /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nMarefat A, Marefat M, Hassannataj Joloudari J,\nNematollahi MA and Lashgari R (/two.tnum/zero.tnum/two.tnum/three.tnum)\nCCTCOVID: COVID-/one.tnum/nine.tnum detection from chest\nX-ray images using Compact Convolutional\nTransformers. Front. Public Health/one.tnum/one.tnum:/one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Marefat, Marefat, Hassannataj\nJoloudari, Nematollahi and Lashgari. This is an\nopen-access article distributed under the terms\nof the\nCreative Commons Attribution License\n(CC BY) . The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nCCTCOVID: COVID-/one.tnum/nine.tnum detection\nfrom chest X-ray images using\nCompact Convolutional\nTransformers\nAbdolreza Marefat/one.tnum, Mahdieh Marefat /two.tnum,\nJavad Hassannataj Joloudari /three.tnum, Mohammad Ali Nematollahi /four.tnum* and\nReza Lashgari /five.tnum\n/one.tnumDepartment of Computer Engineering, South Tehran Branch, Islam ic Azad University, Tehran, Iran,\n/two.tnumDepartment of Cellular and Molecular Biology, Science and Resea rch Branch, Islamic Azad University,\nTehran, Iran, /three.tnumDepartment of Computer Engineering, Faculty of Engineering, Univ ersity of Birjand,\nBirjand, Iran, /four.tnumDepartment of Computer Sciences, Fasa University, Fasa, Iran , /five.tnumInstitute of Medical\nScience and Technology, Shahid Beheshti University, Tehran, Ir an\nCOVID-/one.tnum/nine.tnum is a novel virus that attacks the upper respiratory tract and the lungs.\nIts person-to-person transmissibility is considerably rapid and this has caused\nserious problems in approximately every facet of individual s’ lives. While some\ninfected individuals may remain completely asymptomatic, othe rs have been\nfrequently witnessed to have mild to severe symptoms. In additi on to this,\nthousands of death cases around the globe indicated that detect ing COVID-/one.tnum/nine.tnum\nis an urgent demand in the communities. Practically, this is prom inently done\nwith the help of screening medical images such as Computed Tomograp hy (CT)\nand X-ray images. However, the cumbersome clinical procedures and a large\nnumber of daily cases have imposed great challenges on medical pract itioners.\nDeep Learning-based approaches have demonstrated a profoun d potential in\na wide range of medical tasks. As a result, we introduce a transform er-based\nmethod for automatically detecting COVID-/one.tnum/nine.tnum from X-ray images using Compact\nConvolutional Transformers (CCT). Our extensive experime nts prove the eﬃcacy\nof the proposed method with an accuracy of /nine.tnum/nine.tnum./two.tnum/two.tnum% which outperformsthe\nprevious works.\nKEYWORDS\nCOVID-/one.tnum/nine.tnum, deep learning, Convolutional Neural Networks, vision transformers, Compact\nConvolutional Transformers\n/one.tnum. Introduction\nThe virus, named Severe Acute Respiratory Syndrome Corona-Virus 2 SARS-CoV-2, also\nknown by the name COVID-19, is the source of a severe disease that started in Wuhan, China\nduring the last months of 2019 (\n1). It soon spread to other parts of the globe and caused one\nof the most devastating pandemics, in that millions of people became abruptly aﬀected or\ndead. According to the World Health Organization (WHO), the number of death cases in\nthe ﬁrst half of 2022 stood at more than 6,200,000 and the number of diagnosed people\nreached more than 516,000,000 in the same year worldwide. This virus belongs to the same\ngroup as Severe Acute Respiratory Syndrome (SARS) and Middle East Respiratory Syndrome\n(MERS) (\n2). Its commonly recognized symptoms are coughing, shortness of breath, fever,\npneumonia, and respiratory distress ( 3).\nFrontiers in Public Health /zero.tnum/one.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nThe negative ramiﬁcations, imposed on the communities by\nthis virus, and also its rapid transmission from one person\nto another, prove the necessity of tackling this disease with\nprohibitive measures. Approximately all countries included a\nvariety of safety protocols, such as social distancing, with the\nobject of controlling the outbreak of this pandemic. Accurately\nand rapidly detecting COVID-19 is an essential step that should\nbe taken to control the widespread disease (\n4). Screening\nand monitoring of Computed Tomography (CT) and X-ray\nimages have demonstrated great potential in providing a reliable\nmodality for experts to examine diﬀerent lung diseases such\nas tuberculosis, inﬁltration, atelectasis, pneumonia, and COVID-\n19 (\n5). However, the lack of specialized human resources in\nmany regions, especially poor and underdeveloped countries\nacts as an impediment to taking advantage of such imaging\ntechnologies. This motivated the scientiﬁc community to utilize\ncomputer-aided intelligent decision-making systems to automate\nthe required process.\nDeep Learning (DL) is a powerful tool that can provide\nus with such systems. Among various architectures, designed\nfor processing diﬀerent types of data, Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) are\nspeciﬁcally invented for visual data. Especially, in medical\nimage analysis, these architectures have proven to be remarkably\neﬀective for diagnosing a wide variety of medical conditions.\nIn the following, a brief explanation of CNNs and ViTs\nis given.\n/one.tnum./one.tnum. Convolutional Neural Network\nConvolutional Neural Network (CNN) is one of the most\nfavored types of architectures in deep learning, especially in\ncomputer vision (\n6). The main component of CNN-based\narchitectures is convolution, which is a mathematical linear\noperation between matrices (\n7). CNNs’ most notable success is\nin the ﬁeld of pattern recognition applied to imagery, that is,\nvisual data (\n8). In fact, the introduction of CNNs by Krizhevsky\net al. ( 9), has revolutionized a wide variety of challenges in the\ndomain of computer vision such as medical image analysis, face\nrecognition, image classiﬁcation, object detection, and semantic\nsegmentation (\n10–15).\nIn general, CNN-based models comprise three types of\nlayers, namely convolutional layers, pooling layers, and\nfully-connected layers. These three are depicted in\nFigure 1,\nwhere you can see a formation of a CNN-based model for\nclassifying the input lung X-ray image into healthy or unhealthy\nsamples. As is shown in this ﬁgure, the convolution layer\noperates by sliding a kernel on the input data. Each kernel\nresults in a feature map, to which the pooling operation\nis applied.\nFurthermore, translation equivariance and translational\ninvariance, which is inherent to CNNs, enable them to learn the\nnatural statistics of the input image. In addition to this, sparse\ninteraction, weight sharing, and equivariant representations\nmake CNN-based models more eﬃcient and less computationally\nexpensive (\n16).\n/one.tnum./two.tnum. Vision transformer\nTransformer-based models in deep neural networks have\nbeen originally introduced in the domain of Natural Language\nProcessing (NLP) (\n17). The astounding performance of these\nmodels in a variety of tasks in NLP , i.e., machine translation ( 18),\nquestion answering ( 19), text classiﬁcation ( 20), and sentiment\nanalysis (20, 21), has sparked the interest of a considerable number\nof researchers in computer vision to attune these models to the ﬁeld\nof computer vision (\n22, 23) was the ﬁrst research paper, in which\nthe authors creatively invented a way to apply transformers to the\nvisual data and introduced ViTs for image classiﬁcation.\nFigure 2\ndemonstrates a general procedure in ViT-based models. Based on\nthis ﬁgure, it can be witnessed that an image is converted to a set\nof patches, each representing a locality of a region in the image.\nThis procedure enables us to look upon an image as sequential\ndata; the type of data that is prevalent in NLP and is tailored\nfor transformers.\nFirstly, ViT ﬂattens these patches and then passes them through\na trainable linear projection layer, making the projections the\nsame with regard to their dimensionality. Then, since the ViT is\nthoroughly agnostic to the hierarchy of the input image, meaning\nthat it does not take into consideration where each patch is located\nin the original image, the position embeddings are integrated\ninto these projections to eradicate this problem. After that, the\ntransformer encoder block takes these patches, alongside their\nposition, and an extra classiﬁcation token named CLS token. The\ntransformer encoder includes multi-head attention layers, capable\nof learning a variety of self-attention states. Lastly, the outputs\nof all existent heads are amalgamated and fed to the Multi-Layer\nPerceptron (MLP).\n/two.tnum. Related works\nIn this section, we present a brief review of the previous\nworks for detecting COVID-19 from CT or X-Ray images. Due to\nthe successful performance of deep learning-based approaches in\nmedical image analysis (\n24), researchers have focused on proposing\ndiﬀerent CNN or ViT-based architectures in order to automatically\nrecognize the presence of the infection (\n5).\nTo begin with, Wang et al. ( 25) were one of the ﬁrst groups\nwho designed a deep neural network for detecting COVID-19. In\naddition to this, they provided a relatively large dataset of chest X-\nray images. They achieved 93.3% accuracy. In (\n26), Marques et al.\nproposed a pipeline based on EﬃcientNet and followed the 10-\nfold cross-validation paradigm to evaluate their approach to chest\nx-ray images. They have achieved an average accuracy of 99.62\nand 97.54% in binary and multi-class classiﬁcation, respectively.\nSingh et al. (\n27) utilized a famous neural network, named VGG16,\nand transfer learning in order to detect COVID-19 from CT\nscans. In their approach, the extracted features were chosen by\nusing Principal Component Analysis (PCA) and later classiﬁed\nby diﬀerent classiﬁers. At most, they achieved 95.7% accuracy.\nIn (\n28), Islam et al. made a neural network that was a hybrid\nof CNNs and Long Short-Term Memory (LSTM) networks. They\ntrained their model on 3 classes, namely COVID-19, pneumonia,\nand normal, and achieved 99.2, 99.2, and 99.8% accuracy for\nFrontiers in Public Health /zero.tnum/two.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nFIGURE /one.tnum\nThe general architecture of a CNN-based model.\nFIGURE /two.tnum\nThe general architecture of a ViT-based model.\neach class, respectively. In Narin et al. ( 29), have thoroughly\ninvestigated the impact of transfer learning on the analysis of chest\nX-ray radiographs. Five pre-trained models, namely ResNet50,\nResnet101, ResNet152, InceptionV3, and Inception-ResNetV2\nwere the models examined by them and they achieved accuracies\nof 96.1, 99.5, 99.7% in three diﬀerent datasets. In addition, Goel\net al. (\n30) have proposed OptCoNet; an optimized Convolutional\nNeural Network for detecting COVID-19 from X-ray images.\nThey employed the gray wolf optimization algorithm with the\naim of tuning the hyperparameters of the classiﬁer and achieved\n97.78% accuracy.\nFurthermore, more recently ViT-based models have been put\nforward for COVID-19 detection. In Al Rahhal et al. (\n31), a\nnovel model with two branches has been proposed. In this work,\na ViT architecture is utilized as a backbone, integrated with a\nSiamese network for processing an augmented version of the input\nX-ray image. They could obtain 99.13% for their accuracy in\nthe 80:20 distribution of train and test. Further, Mondal et al.\nFrontiers in Public Health /zero.tnum/three.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nTABLE /one.tnum An overview of the existing works.\nReferences Dataset\n(CT/X-ray)\nClassiﬁcation Approach Train/test/validation Performance\n(accuracy)\nKonar et al. ( 35) X-Ray Binary Proposed Semi-Supervised\nClassiﬁer\nRandom sampling 70% for\ntraining and 30% for testing\n98.40%\nVaid et al. ( 36) X-Ray Binary VGG-19 Random sampling 80:20:20 for\ntrain, validation and testing.\n96.30%\nOzturk et al. ( 37) X-Ray Binary and\nmulti-class\nDarkNet 5-fold cross-validation 98.08%\nPanwar et al. ( 38) X-Ray Binary Proposed nCOVnet Random sampling 70% for\ntraining and 30% for testing\n97.62%\nAhuja et al. ( 39) X-Ray Binary ResNet-18 Random sampling. 70% for\ntraining and 30% for testing.\n99.40%\nSharifrazi et al. ( 40) X-Ray Binary Sobel+Support Vector\nMachine +CNN\n10-fold cross-validation 99.02%\nKhozeimeh et al. ( 41) Numerical Binary CNN-AE 10-fold cross-validation 96.05%\nAl Rahhal et al. ( 31) CT/X-Ray Multi-class Proposed Siamese+ViT\nClassiﬁer\n60:40\n80:20\n20:80\n99.13 ± 0.23\nMondal et al. ( 32) CT/X-Ray Multi-class Proposed xViTCOS +\nMultistage Transfer Learning\n80:20 0.981\nKrishnan and Krishnan ( 2) X-Ray Binary Pretrained ViT 73:3:24 97.61\nKumar et al. ( 42) X-Ray Multi-class SARS-Net CNN 90:10 97.60\nEsmi et al. ( 43) X-Ray Multi-class Fuzzy ﬁne-tuned Xception 80:20 96.60\nFIGURE /three.tnum\nWorkﬂow of the proposed pipeline for detecting COVID-/one.tnum/nine.tnum.\n(32) proposed a ViT-based model and employed a multi-stage\ntransfer learning technique to address the scarcity of data. They\nobtained an overall accuracy of 96.00%. Furthermore, Liu et al.\n(\n33) have applied a transformer-like classiﬁer model. By employing\ntransfer learning techniques in their approach, they improved\nTABLE /two.tnum The dataset distribution.\nNo. train\nsamples\nNo. test\nsamples\nPositive (COVID-19) 16,490 200\nNegative (NO COVID-19) 13,992 200\nTotal 30,482 400\nthe results to outperform CNN-based models, achieving 99.7%\naccuracy. Additionally, in (\n34), the authors applied a ViT-based\nalgorithm based on the Swin transformer for feature learning and\naggregation in two stages segmentation and classiﬁcation. In their\npaper, they further validated the superiority of their algorithm by\ncomparing their results with well-known visual feature extractors,\ni.e., EﬃcientNetV2. The accuracy of 94.3% was obtained by\ntheir approach.\nFurthermore, we have provided\nTable 1, which details an\noverview of the existing research works on the diagnosis of COVID-\n19 from CT or X-ray images.\nIn contrast to the eﬃciency of previous works, the related\nliterature lacks employ ViT-based deep models with less hunger for\ndata. Although such models lack inductive biases like translation\nequivariance and locality, which are inherent to CNN-based\nmodels, they are not eﬃcient in generalizing in the procedure\nof training on small datasets and this shows their data-driven\nnature which is not feasible, especially in the medical area, where\nit is less likely to have access to the huge amount of data. As a\nresult, ViTs do not seem to be a better choice when dealing with\nFrontiers in Public Health /zero.tnum/four.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nFIGURE /four.tnum\nNegative and positive samples of X-ray images from the dataset.\nsmall datasets because they have more requirements both in terms\nof computation and memory, preventing many researchers from\nadopting such models in diﬀerent areas. The above-mentioned\nchallenges motivated us to propose a more performant solution that\nutilizes both the CNN and ViT-based models simultaneously with\nthe object of boosting COVID-19 detection from visual data.\n/three.tnum. Methodology\nThis section includes our methodology for detecting COVID-\n19 from X-ray images. The workﬂow of the adopted pipeline is\nshown in\nFigure 3.\nMoreover, in this section, after describing the details of the\nused dataset, all the main components of the proposed method will\nbe elaborated.\n/three.tnum./one.tnum. Dataset description\nIn this paper, a publicly available dataset\n/one.tnumis used for training\nand evaluating our proposed method. Table 2 shows the oﬃcial\ndistribution of this dataset.\nMoreover, Figure 4 demonstrates some samples from both\npositive and negative classes.\n/three.tnum./two.tnum. The used architecture\nThis section introduces our proposed algorithm, including\ndiﬀerent stages in Compact Convolutional Transformers (CCT)\n(\n16). The overview of CCT architecture is illustrated in Figure 5.\n/one.tnumhttps://www.kaggle.com/datasets/andyczhao/covidx-cxr/two.tnum?select=\ncompetition_test\nFIGURE /five.tnum\nAn overview of the proposed architecture.\nCompact Convolutional Transformer (CCT) is one of the most\nrecent compact transformer-based models for image processing.\nThe biggest advantage of CCT is its ability to learn from the\nsmall amount of data in comparison with the base ViT models\nwhich are considerably data-hungry. In situations where we do\nnot have access to huge number of samples, ViT models cannot\nachieve better performance than convolutional-based models and\nthus such models are not considered suitable. However, although\nsome state-of-the-art models, namely DeiT, ConViT, and Compact\nVision Transformers have addressed the issue of the low volume\nof data in ViTs, CCT has managed to outperform all other pre-\nexisting approaches. CCT’s performance has been studied through\nFrontiers in Public Health /zero.tnum/five.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nFIGURE /six.tnum\nThe architecture of transformer encoder.\nFIGURE /seven.tnum\nConfusion matrix.\na variety of low to high-resolution images in benchmark datasets\nsuch as FashionMNIST, MNIST, CIFAR-10, CIFAR-100, ImageNet,\nand Flowers-102.\nMoreover, CCT is developed over Compact Vision\nTransformers (CVT) and takes advantage of a convolutional\ntokenizer leading to the preservation of local information and\nthe production of richer tokens. Compared to the primitive ViT,\nthe convolutional tokenizer is more eﬀective in encoding the\nconnection between patches. In the sequel, we go into further detail\non the components of compact transformers.\nTABLE /three.tnum The metrics used for evaluation.\nMetric name Equation\nAccuracy TP + TN\nFP + FN + TP + TN (2)\nPrecision TP\nTP + FP (3)\nRecall TP\nTP + FN (4)\nF1-score (2∗precision∗recall)\nprecision+recall (5)\nAUC-ROC Area under curve of receiver operator characteristic (6)\nFalse positive rate\n(FPR)\nFP/(FP + TN) (7)\nFalse negative rate\n(FNR)\nFN/(TP + FN) (8)\nTrue negative rate\n(TNR)\nTN/(TN + FP) (9)\nTABLE /four.tnum Experimental setup.\nProgramming\nlanguage\nPython /three.tnum./seven.tnum\nDeep learning library Pytorch 1.9\nCPU Intel R⃝ CoreTM i7-10700 CPU @ 2.90 GHz × 16\nGPU GeForce GTX 1060\nTABLE /five.tnum Hyperparameter settings.\nParameter name Detail\nImage size (256, 256)\nEmbedding dimension 512\nNumber of convolution layers 4\nPooling kernel size 5\nPooling padding 1\nPooling stride 2\nKernel size 5\nStride 2\nPadding 1\nNumber of heads 8\nNumber of classes 2\nPositional embedding Sine function\n/three.tnum./two.tnum./one.tnum. Transformer-based backbone\nAs for CCT model design, the original Vision Transformer ( 23),\nand original Transformer ( 44) are proposed. The encoder is made\nup of transformer blocks, each of which has an MLP block and a\nMulti-Head Self-Attention (MHSA) layer. Based on\nFigure 6, the\ninput image is patchiﬁed, after which each patch becomes ﬂattened\nand projected linearly. Then, the positional embeddings are added\nto these patch embeddings. These embeddings are fed to multiple\ntransformer encoders, whose architecture is shown in\nFigure 6\nin detail.\nFigure 6 demonstrates that the embedded patches are fed to\na normalization layer and after that MHSA module is applied to\nFrontiers in Public Health /zero.tnum/six.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nFIGURE /eight.tnum\nPolicies for train and test split.\nTABLE /six.tnum Results of classiﬁcation on the oﬃcial test data (all metrics are reported on a /zero.tnum–/one.tnum/zero.tnum/zero.tnum scale).\nAccuracy Precision Recall F/one.tnum-scoreAUC-ROC FPR FNR TNR\n99.00 99.00 99.00 99.00 99.67 1.00 1.00 99.00\nthe output. Also, the residual connections exist before and after\neach block of MLP. The encoder additionally employs dropout,\nGELU activation, and Layer Normalization. It is considered that the\nvision transformers are more compact and simpler. Variants with\n(the minimum number of) 2 layers, 2 heads, and 128-dimensional\nhidden layers are implemented. Based on the image resolution of\nthe training dataset, the tokenizers are modiﬁed. These variations\nare referred to as ViT-Lite, and although they diﬀer in size, they are\nlargely comparable to ViT in terms of architecture.\n/three.tnum./two.tnum./two.tnum. Sequence pooling\nThe ViT and almost all general transformer-based classiﬁers\nfollow BERT (\n45), which sends a learnable class or query token\nacross the network before feeding it to the classiﬁer leading\nto the conversion of the sequential outputs to a single class\nindex. However, in CCT, an attention-based technique that pools\nover the output token sequence are leveraged, and hence, unlike\nthe learnable token, the output sequence contains substantial\ninformation that includes several parts of the input image, resulting\nin a more eﬃcient performance. Moreover, the network can\ncorrelate data across the input data and weigh the sequential\nembedding of the transformer encoder’s latent space. Finally,\nCompact Vision Transformer (CVT) is made by substituting\nSeqPool for the ordinary class token in ViT-Lite.\n/three.tnum./two.tnum./three.tnum. Convolutional tokenizer\nAs for the last steps in designing CCT, a straightforward\nconvolutional block is substituted for the patch and embedding\nin ViT-Lite and CVT to induce an inductive bias into the\nmodel. A single convolution, ReLU activation, and a max pool\nmake up the standard and customary design of this block by\nwhich the models have more ﬂexibility than models like ViT\nsince they are no longer restricted to input resolutions that are\nstrictly divisible by the predetermined patch size. The CCT is\nproduced via this convolutional tokenizer, whose mathematical\nrepresentation is shown in Equation (1), Sequence Pooling, and the\ntransformer encoder.\nX0 = MaxPool(ReLU\n(\nconv2d(x)\n)\n) (1)\nThe feature map is extracted to be the representation of local\nfeatures. Based on Equation 1, we can deduce that CCT does\nnot depend on image resolution, since it preserves locality in\ninformation gained from the data due to its convolutional blocks.\nFrontiers in Public Health /zero.tnum/seven.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nFIGURE /nine.tnum\nConfusion matrix (CM) for oﬃcial test data.\nFIGURE /one.tnum/zero.tnum\nAccuracy diagram vs. epochs.\n/three.tnum./three.tnum. Evaluation metrics\nThe measures used for evaluating the performance of the\nproposed classiﬁer are estimated against the following metrics:\nConfusion Matrix (CM): A matrix, containing four main\nelements, namely True Positive (TP), True Negative (TN), False\nPositive (FP), and False Negative (FN). For a binary classiﬁer, CM\nis as\nFigure 7.\nTrue Positive (TP):the number of infected samples correctly\nclassiﬁed as infected.\nTrue Negative (TN): the number of uninfected samples\ncorrectly classiﬁed as uninfected.\nFalse Positive (FP):the number of infected samples correctly\nclassiﬁed as infected.\nFalse Negative (FN):the number of infected samples correctly\nclassiﬁed as infected.\nFIGURE /one.tnum/one.tnum\nLoss diagram vs. epochs.\nFIGURE /one.tnum/two.tnum\nReceiver operating characteristic (ROC) curve for oﬃcial tes t data.Based on the metrics mentioned above, the metrics detailed\nin Table 3 can be deduced and used for evaluating a classiﬁer.\nOther metrics used for evaluating the proposed approach are\naccuracy, precision, recall, F1-Score, AUC-ROC, False Positive\nRate (FPR), False Negative Rate (FNR), and True Negative Rate\n(TNR) (\n40, 46–48).\n/four.tnum. Results\nThis section includes the results of classiﬁcation by our\nproposed approach.\n/four.tnum./one.tnum. Experimental setup\nTable 4 details the software and hardware used for\nimplementing our proposed method.\nFrontiers in Public Health /zero.tnum/eight.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nTABLE /seven.tnum Results of classiﬁcation using /one.tnum/zero.tnum-fold cross-validation (all metrics are reported in /zero.tnum–/one.tnum/zero.tnum/zero.tnum scale).\nNo. fold Accuracy Precision Recall F/one.tnum-scoreAUC-ROC FPR FNR TNR\n1 99.16 98.90 99.42 99.16 99.23 1.10 0.58 98.90\n2 98.90 98.21 99.61 98.91 99.15 1.81 0.39 98.19\n3 99.71 99.68 99.74 99.71 99.20 0.32 0.26 99.68\n4 99.38 99.10 99.68 99.39 99.61 0.91 0.32 99.09\n5 98.96 98.40 99.55 98.97 99.03 1.62 0.45 98.38\n6 99.61 99.42 99.81 99.61 99.27 0.58 0.19 99.42\n7 99.03 98.84 99.22 99.03 99.19 1.17 0.78 98.83\n8 98.80 97.90 99.74 98.81 99.02 2.14 0.26 97.86\n9 99.64 99.68 99.61 99.64 99.64 0.32 0.39 99.68\n10 99.03 98.65 99.42 99.03 99.31 1.36 0.58 98.64\nAverage 99.22 98.88 99.58 99.23 99.27 1.13 0.42 98.87\nThe bold values demonstrate that the best values is placed in the last r ecord.\nFIGURE /one.tnum/three.tnum\nAccuracy diagram vs. epochs.\n/four.tnum./two.tnum. Hyperparameter settings\nTable 5 details the Hyperparameter settings applied for\nimplementing our proposed method.\n/four.tnum./three.tnum. Dataset split\nNote that we opted for three main policies for evaluating the\nclassiﬁer. These three are:\n1) Policy #1: We used the oﬃcial training data for training and\nvalidating the model and the oﬃcial test data for testing it.\n2) Policy #2: We amalgamated oﬃcial train and test data with\neach other; then randomly shuﬄed the data multiple times.\nNext, we used 10-fold cross-validation method for the training\nand evaluation process.\nFIGURE /one.tnum/four.tnum\nLoss diagram vs. epochs.\n3) Policy #3: We randomly shuﬄed the training data multiple\ntimes and then chose a speciﬁc number of training data\n(randomly chosen), removed them from the training set, and\nadded them to the testing set. The number of replaced samples\nwas set in a way to make the test size 0.1 of the remaining\ntraining data.\nThe main reason for pursuing these policies is the small size\nof the oﬃcial test chunk, which makes the evaluation results\nunreliable. This process is depicted in\nFigure 8.\n/four.tnum./three.tnum./one.tnum. Results for policy #/one.tnum\nTable 6 contains the results of classiﬁcation by the proposed\nmodel on the oﬃcial distribution of the used dataset. Additionally,\nFigure 9 shows the obtained CM for the same distribution.\nFigures 10, 11 show the accuracy and loss curves vs. epochs,\nrespectively. Figure 12 demonstrates the ROC curve for the\nclassiﬁer.\nFrontiers in Public Health /zero.tnum/nine.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nFIGURE /one.tnum/five.tnum\nThe CMs based on /one.tnum/zero.tnum-folds(A–J).\nBased on Table 6, it can be observed that our approach can\nachieve a high value of 99.00% for accuracy, precision, recall, and\nF1-Score. The stability of the proposed model in terms of detecting\nTABLE /eight.tnum Train and test distribution in policy #/three.tnum.\nNo. train samples No. test samples\nPositive (COVID-19) 26142 4740\nFIGURE /one.tnum/six.tnum\nAccuracy diagram vs. epochs.\nFIGURE /one.tnum/seven.tnum\nLoss diagram vs. epochs.both negative and positive samples can be proved by the fact that a\nhigh value of 99.67 is achieved for AUC-ROC.\n/four.tnum./three.tnum./two.tnum. Results for policy #/two.tnum\nTable 7 details the achieved results for each fold based on the\nintroduced metrics. Figures 13, 14 demonstrate the accuracy and\nvalidation curves achieved in the training process. Figures 15A–J\nshows the CMs obtained in the second policy.\nTable 7 shows the results achieved in all folds as well as the\naverage. The achieved accuracy, on average, is 99.22, the precision\nis 98.88, the recall is 99.58, and the F1-Score is 99.23. The value for\nAUC-ROC, on average, is 99.27 which shows the strong conﬁdence\nFrontiers in Public Health /one.tnum/zero.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nTABLE /nine.tnum Results of classiﬁcation using policy three (all metrics are reported on a /zero.tnum–/one.tnum/zero.tnum/zero.tnum scale).\nAccuracy Precision Recall F/one.tnum-scoreAUC-ROC FPR FNR TNR\n99.09 98.74 99.45 99.09 99.73 1.27 0.55 98.73\nFIGURE /one.tnum/eight.tnum\nConfusion matrix.\nof the proposed classiﬁer in classifying both negative and positive\nsamples.\n/four.tnum./three.tnum./three.tnum. Results for policy #/three.tnum\nThis subsection includes our results based on the third\nevaluation policy. Train and test distribution in policy #3 is\nindicated in\nTable 8. Figure 16 demonstrates training and validation\naccuracy in each epoch. Also, Figure 17 illustrates training and\nvalidation loss in the training procedure. Table 9 shows the results\nachieved by the classiﬁer when we adopt policy 3 for the evaluation.\nAlso, the obtained CM and ROC, in this policy, is shown in\nFigures 18, 19, respectively.\nBased on Table 9, we can see that in policy 3, we have achieved\n99.09 as accuracy, 98.74 as precision, 99.45 as recall, and 99.09\nas F1-Score. 99.73 is achieved as the AUC-ROC of the classiﬁer\nin policy 3 and proves the eﬃcient performance of the model in\ndistinguishing positive and negative samples correctly.\n/five.tnum. Discussion\nThe COVID-19 serious illness that began in the ﬁnal months\nof 2019 and quickly spread to other regions of the world, led to\none of the most destructive pandemics. The WHO estimates that\nas of August 2022, there have been more than 6.4 million deaths\nand 570 million conﬁrmed cases. According to the research and\nexperiences obtained up to now, CT scans and X-ray images are\nhighly eﬀective tools in diagnosing COVID-19. The absence of\nspecialized human resources in many areas makes it diﬃcult to\nFIGURE /one.tnum/nine.tnum\nReceiver operating characteristic curve.beneﬁt from such imaging technologies that are encouraged. The\nscientiﬁc community uses computer-aided intelligent systems to\nautomate the desired procedure.\nIn this study, we proposed deep learning methods for the\ndetection of COVID-19, based on X-ray images of both conﬁrmed\n(positive) and negative COVID-19 cases that were gathered in\na dataset with 30,882 samples. The main architecture that we\nproposed was CCT. Because of its compactness, CCT can be\nimplemented in low-resource environments, which is its primary\nadvantage, and therefore, is considered to be among mobile-\nfriendly models. In addition, because CCT is a hybrid model\nbased on CNN and ViT, it combines the beneﬁts of both\nwhile avoiding their drawbacks. For instance, CCT experiences\nsubstantial performance improvements, resulting in a top-1%\naccuracy of 98% on CIFAR-10.\nThe CCT model is the only transformer-based model among\nthe top 25 models in terms of performance and eﬃciency on\nCIFAR-10, despite having no pre-training and being rather small\ncompared to the majority. Moreover, CCT surpasses the majority\nof comparable CNN-based models in this ﬁeld, except for some\nNeural Architectural Search techniques (\n49). Furthermore, CCT\ncan be lightweight, using only 0:28 million parameters, while still\nachieving accuracy within 1% of the top 90% of similar models\non CIFAR-10. CCT obtains 80.67% accuracy on ImageNet with\nfewer parameters and less computational work, and it outperforms\nmore recent, comparably sized models like DeiT (\n50) [for more\ninformation, see ( 16)].\nIn order to achieve better performance in our study, we chose to\nevaluate the classiﬁer according to three primary policies. In policy\n1, we merely trained and validated the model through the oﬃcial\ntraining data, and we examined the classiﬁer using the oﬃcial test\ndata. Afterward, to obtain more reliable and robust outcomes, the\nFrontiers in Public Health /one.tnum/one.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nTABLE /one.tnum/zero.tnum Comparison between the current study and related studies based on binary classiﬁcation for COVID-/one.tnum/nine.tnum diagnosis.\nReferences Cases/normal/COVID Method Performance (accuracy, precision,\nrecall, F/one.tnum-score, AUC, NC: Not\nconsidered)\nAlakus and Turkoglu ( 51) 600/520/80 CNN-LSTM Accuracy: 92.30%\nPrecision: 92.35%\nRecall:93.68%\nF1-Score: 93.00%\nAUC:90.00%\nOguz and Yaganoglu ( 52) 1,345/738/607 ResNet-50+SVM Accuracy: 96.29%\nPrecision: 96.66%\nRecall: 95.08%\nF1-Score: 95.86%\nAUC:98.21%\nSrivastava et al. ( 53) 4,551/3,270/1,281 lightweight CNN (CoviXNet) Accuracy: 99.56%\nPrecision:100%\nRecall: 99.70%\nF1-Score: 100%\nAUC: 99.00%\nChen (54) 60,000/59,600/400 CNN + Histogram-oriented gradients Accuracy: 92.95%\nPrecision: 91.5%\nRecall: 85%\nF1-Score: N/C\nAUC: N/C\nNasiri and Hasani ( 55) 625/500/125 DenseNet-169 + XGBoost Accuracy: 98.23%\nPrecision: 98.54%\nRecall: 92.08%\nF1-Score: 97.00%\nSpeciﬁcity: 99.78%\nAUC: N/C\nJain et al. ( 56) 1,832/1,372/460 ResNet-101 Accuracy: 98.93%\nPrecision: 96.39%\nRecall:98.93%\nF1-Score: 98.15%\nAUC: 98.20%\nAl-Waisy et al. ( 57) 800/400/400 A hybrid deep learning detection\nSystem (deep belief network +\nconvolutional deep belief network)\nAccuracy: 99.93%\nPrecision: 100%\nRecall: 99.90%\nF1-Score:99.93%\nAUC: N/C\nArdakani et al. ( 58) 1,020/510/510 ResNet-101 Accuracy: 99.51%\nPrecision: 99.03%\nRecall: 100%\nF1-Score: N/C\nAUC: 99.40%\nZhao et al. ( 59) 16,351/14,704/1,647 Big transfer-M Accuracy: 96.50%\nPrecision:100%\nRecall: 93.00%\nF1-Score: N/C\nAUC: 99.40%\nHaghanifar et al. ( 60) 3,628/3,200/428 A 121-layer dense\nConvolutional network\nAccuracy: 99.04%\nPrecision: N/C\nRecall: N/C\nF1-Score: 96.00%\nAUC: N/C\nIn this study (2023) 30,882/14,192/16,690 Compact Convolutional Transformers Accuracy: 99.22%\nPrecision:98.88%\nRecall: 99.58%\nF1-Score: 99.23%\nAUC: 99.27%\nFrontiers in Public Health /one.tnum/two.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\noﬃcial test and train data were combined, after which they were\nrepeatedly shuﬄed at random. The training and evaluation process\nwas then conducted using the 10-fold cross-validation method\nwhich altogether constitutes our second policy. Finally, to provide\nthe third (and the last) policy, we shuﬄed the training data at\nrandom several times followed by randomly selecting a group of\ntraining data, taking them out of the train set, and adding them to\nthe testing set. It is important to note that the oﬃcial test chunk’s\nsmall size, which renders the evaluation results untrustworthy, was\nour main motivation for considering these three diﬀerent policies\nand approaches.\nTable 10 lists the comparison between the current study and\nseveral related studies on the topic of COVID-19 diagnosis\nbased on binary classiﬁcation, and the performance of each\nstudy is mentioned due to the evaluation metrics used by the\nauthors.\nFollowing is a brief description of the methodology and\nresults of the articles listed in the table above. In Alakus and\nTurkoglu’s study (\n51), six diﬀerent deep-learning model types\nwere developed and the outcomes were compared. With an\naccuracy of 92.30%, CNN-LSTM produced the best results out of\nthe group.\nIn (\n52), 1,345 CT scans were subjected to deep feature\nextraction using deep learning models like ResNet-50, ResNet-\n101, AlexNet, etc. Following that, classiﬁcation methods\nwere given the deep features, and test images were used for\nmodel evaluation. The results showed that ResNet-50 and\nthe SVM together provided the best performance. The F1-\nscore was 95.86%, the accuracy was 96.29%, and the AUC\nwas 98.21%.\nSrivastava et al. in (\n53) proposed CoviXNet, a lightweight CNN-\nbased model, over a dataset of three classes: COVID-19, normal\nX-rays, and viral-pneumonia-infected chest X-ray images, with an\naccuracy of 99.56% for binary classiﬁcation (i.e., normal Chest\nX-ray image and COVID-19 infected).\nThe literature study (\n54) suggested a CNN-based plus\nhistogram-oriented gradients (HOG) model on a public dataset\nof 60,000 X-ray images with 59,600 negative and 400 positive\nCOVID-19 samples and a 92.95% accuracy was attained.\nIn (\n55), features from 1,125 X-ray images, including 125 images\nidentiﬁed as COVID-19 were extracted using DenseNet-169. The\nXGBoost classiﬁer was then fed the derived features and the average\naccuracy was 98.23%.\nA deep learning ResNet-50 network was utilized as a classiﬁer\nin the study (\n56) to identify viral/bacterial pneumonia and\nnormal cases among 1,832 X-ray chest images. Additionally, the\nResNet-101 was employed to determine COVID-19 in patients\nwith positive viral-induced pneumonia and the overall accuracy\nwas 98.93%.\nA parallel design (COVID-DeepNet) that combines a deep\nbelief network with a convolutional deep belief network trained\nfrom scratch on a large dataset was proposed by Al-Waisy et al.\n(\n57). With a 99.93% detection accuracy rate, the method properly\nidentiﬁed COVID-19 in patients.\nTen well-known deep learning-based techniques for\ndistinguishing COVID-19 from non-COVID-19 in CT scan\nimages were proposed by Ardakani et al. (\n58), and the results\nshowed that the ResNet-101 model achieved 99.51% accuracy.\nTo detect COVID-19 infections from chest X-ray images,\nMahajan et al. ( 61) developed a single-shot MultiBox detector\n(SSD) in conjunction with deep transfer learning models and\nachieved high precision (i.e., 93.01%).\nThe authors of (\n60) used transfer learning to diagnose COVID-\n19 over 1,326 chest X-ray images, and the ﬁnal method, the\n121-layer Dense Convolutional Network (COVID-CXNet), was\ndeveloped using the well-known CheXNet model (\n62). They\nachieved 99.04% accuracy using the COVID-CXNet method.\nIn ( 59), the authors conducted in-depth convolutional\nneural network (CNN) ﬁne-tuning experiments and showed that\nmodels pre-trained on larger out-of-domain datasets demonstrate\nenhanced performance. Also, higher-quality images include more\nclinical information when the hyperparameters are chosen\nproperly, and using mixups during training enhanced the\nmodel’s performance.\nAccording to the related works, to evaluate the performance\nof our proposed compact convolutional Transformer method, we\ntook into account almost all of the standard and most important\nevaluation metrics, including accuracy (99.22%), precision\n(98.88%), recall (99.58%), F1-score (99.23%), AUC-ROC (99.27%),\nFPR 1.13, FNR (0.42%), and TNR (98.87%), which is outstanding\nin this regard. The results of our study show that this research is\nsuperior to many similar and state-of-the-art works in general\nand also when each of the evaluation metrics is considered or\nis completely comparable with them, and\nTable 10 conﬁrms\nthis claim.\n/six.tnum. Conclusion and future works\nIn this paper, a transformer-based model is proposed for\nscreening chest X-ray images to detect COVID-19 disease. The\nproposed model is based on Compact Convolutional Transformers,\nwhose main advantage over the other transformer-based models\nis its less need for data. This is important since in most medical\ndomains data scarcity is ubiquitous. Using diﬀerent metrics, we\nhave demonstrated the eﬃcacy of the proposed model for COVID-\n19 diagnosis. In future work, we tend to evaluate our proposed\napproach to other diseases related to human beings’ lungs. That\nis to say, instead of classifying in a binary fashion positive and\nnegative COVID-19, the approach should detect more classes of\nlung disorders.\nData availability statement\nThe dataset presented in this study can be found at https://ww\nw.kaggle.com/datasets/andyczhao/covidx-cxr2?select=competition\n_test\n.\nAuthor contributions\nJHJ, AM, and RL designed the study. AM performed the\nimplementation of the approach. MM performed the literature\nreview. AM and MAN wrote the methodology. JHJ, MAN, and\nRL did the discussion. JHJ, AM, and MAN edited the ﬁnal version\nFrontiers in Public Health /one.tnum/three.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\nof the article. MAN supervised the project. RL co-supervised the\nstudy. All authors have read and approved the ﬁnal manuscript.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those\nof the authors and do not necessarily represent those of\ntheir aﬃliated organizations, or those of the publisher,\nthe editors and the reviewers. Any product that may be\nevaluated in this article, or claim that may be made by\nits manufacturer, is not guaranteed or endorsed by the\npublisher.\nReferences\n1. Elibol E. Otolaryngological symptoms in COVID-19. Eur Arch Otorhinolaryngol.\n(2021) 278:1233–6. doi: 10.1007/s00405-020-06319-7\n2. Krishnan KS, Krishnan KS. Vision transformer based COVID -19 detection using\nchest X-rays. In: 2021 6th International Conference on Signal Processing, Computing\nControl (ISPCC)(2021). IEEE. p. 644–8.\n3. Chen X, Laurent S, Onur OA, Kleineberg NN, Fink GR, Schweitze r F, et al. A\nsystematic review of neurological symptoms and complications o f COVID-19. J Neurol.\n(2021) 268:392–402. doi: 10.1007/s00415-020-10067-3\n4. Jin Y, Yang H, Ji W, Wu W, Chen S, Zhang W, et al. Virology, epidem iology,\npathogenesis, and contr of COVID-19. Viruses. (2020) 12:372. doi: 10.3390/v12040372\n5. Ismael AM, Sengür A. Deep learning approaches for COVID-\n19 detection based on chest X-ray images. Expert Syst Appl. (2021)\n164:114054. doi: 10.1016/j.eswa.2020.114054\n6. Chai J, Zeng H, Li A, Ngai EW. Deep learning in computer vision : a critical\nreview of emerging techniques and application scenarios. Mach Learn Appl.(2021)\n6:100134. doi: 10.1016/j.mlwa.2021.100134\n7. Albawi S, Mohammed TA, Al-Zawi S. Understanding of a convolut ional neural\nnetwork. In: 2017 International Conference on Engineering Technology (ICET)(2017).\nIEEE. p. 1–6.\n8. O’Shea K, Nash R. An introduction to convolutional neural n etworks. arXiv\npreprint. (2015).\n9. Krizhevsky A, Sutskever I, Hinton GE. Imagenet classiﬁcati on with deep\nconvolutional neural networks. Commun ACM.(2017) 60:84–90. doi: 10.1145/3065386\n10. Yamashita R, Nishio M, Do RKG, Togashi K. Convolutional neu ral networks:\nan overview and application in radiology. Insights Imaging. (2018) 9:611–\n29. doi: 10.1007/s13244-018-0639-9\n11. Liu W, Wang Z, Liu X, Zeng N, Liu Y, Alsaadi FE. A survey of dee p\nneural network architectures and their applications. Neurocomputing. (2017) 234:11–\n26. doi: 10.1016/j.neucom.2016.12.038\n12. Masi I, Wu Y, Hassner T, Natarajan P. Deep face recognitio n: A survey. In:\n2018 31st SIBGRAPI conference on graphics, patterns images (SIBGRAPI) (2018). IEEE.\np. 471–8.\n13. Kortli Y, Jridi M, Al Falou A, Atri M. Face recognition syste ms: a survey. Sensors.\n(2020) 20:342. doi: 10.3390/s20020342\n14. Kim HE, Cosa-Linan A, Santhanam N, Jannesari M, Maros ME, Ganslandt\nT. Transfer learning for medical image classiﬁcation: a litera ture review. BMC Med\nImaging. (2022) 22:1–13. doi: 10.1186/s12880-022-00793-7\n15. Garcia-Garcia A, Orts-Escolano S, Oprea S, Villena-Martine z V , Martinez-\nGonzalez P , Garcia-Rodriguez J. A survey on deep learning techn iques for\nimage and video semantic segmentation. Appl Soft Comput. (2018) 70:41–\n65. doi: 10.1016/j.asoc.2018.05.018\n16. Wolf T, Debut L, Sanh V , Chaumond J, Delangue C, Moi A, et al. Tr ansformers:\nState-of-the-art natural language processing. In: Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations. Association\nfor Computational Linguistics (2020). p. 38–45.\n17. Wolf T, Debut L, Sanh V , Chaumond J, Delangue C, Moi A, et al.\nHuggingface’s transformers: state-of-the-art natural lan guage processing. arXiv\npreprint. (2019). doi: 10.18653/v1/2020.emnlp-demos.6\n18. Edunov S, Ott M, Auli M, Grangier D. Understanding back-tra nslation at scale.\narXiv preprint.(2018). doi: 10.18653/v1/D18-1045\n19. Abbasiantaeb Z, Momtazi S. Text-based question answerin g from information\nretrieval and deep neural network perspectives: a survey. Wiley Interdiscip Rev Data\nMining Knowledge Discov.(2021) 11:e1412. doi: 10.1002/widm.1412\n20. Habimana O, Li Y, Li R, Gu X, Yu G. Sentiment analysis using\ndeep learning approaches: an overview. Sci China Inform Sci. (2020)\n63:1–36. doi: 10.1007/s11432-018-9941-6\n21. Mathew L, Bindu V. A review of natural language processing t echniques for\nsentiment analysis using pre-trained models. In: 2020 Fourth International Conference\non Computing Methodologies Communication (ICCMC)(2020). IEEE. p. 340–5.\n22. Khan S, Naseer M, Hayat M, Zamir SW, Khan FS, Shah M. Transf ormers in\nvision: a survey. ACM Computing Surveys.(2021) 54:1–41. doi: 10.1145/3505244\n23. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T,\net al. An image is worth 16x16 words: transformers for image re cognition at scale. arXiv\nPreprint. (2020).\n24. Litjens G, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoor ian M, et al. A\nsurvey on deep learning in medical image analysis. Med Image Anal.(2017) 42:60–\n88. doi: 10.1016/j.media.2017.07.005\n25. Wang L, Lin ZQ, Wong A. Covid-net: a tailored deep convoluti onal neural\nnetwork design for detection of covid-19 cases from chest x- ray images. Sci Rep.(2020)\n10:1–12. doi: 10.1038/s41598-020-76550-z\n26. Marques G, Agarwal D, de la Torre Díez I. Automated medical di agnosis of\nCOVID-19 through EﬃcientNet convolutional neural network. Appl Soft Comput.\n(2020) 96:106691. doi: 10.1016/j.asoc.2020.106691\n27. Singh M, Bansal S, Ahuja S, Dubey RK, Panigrahi BK, Dey N.\nTransfer learning–based ensemble support vector machine model for\nautomated COVID-19 detection using lung computerized tomogr aphy scan\ndata. Med Biol Eng Comput. (2021) 59:825–39. doi: 10.1007/s11517-020-\n02299-2\n28. Islam MZ, Islam MM, Asraf A. A combined deep CNN-LSTM networ k for the\ndetection of novel coronavirus (COVID-19) using X-ray imag es. Inform Med Unlocked.\n(2020) 20:100412. doi: 10.1016/j.imu.2020.100412\n29. Narin A, Kaya C, Pamuk Z. Automatic detection of coronavi rus\ndisease (covid-19) using x-ray images and deep convolutiona l neural\nnetworks. Pattern Anal Appl. (2021) 24:1207–20. doi: 10.1007/s10044-021-\n00984-y\n30. Goel T, Murugan R, Mirjalili S, Chakrabartty DK. OptCoNet: an optimized\nconvolutional neural network for an automatic diagnosis of C OVID-19. Appl Intell.\n(2021) 51:1351–66. doi: 10.1007/s10489-020-01904-z\n31. Al Rahhal MM, Bazi Y, Jomaa RM, AlShibli A, Alajlan N, Mekhalﬁ ML , et al.\nCovid-19 detection in ct/x-ray imagery using vision transf ormers. J Pers Med.(2022)\n12:310. doi: 10.3390/jpm12020310\n32. Mondal AK, Bhattacharjee A, Singla P , Prathosh A. xViTCOS : explainable vision\ntransformer based COVID-19 screening using radiography. IEEE J Transl Eng Health\nMed. (2021) 10:1–10. doi: 10.1109/JTEHM.2021.3134096\n33. Liu C, Yin Q. Automatic diagnosis of covid-19 using a tailo red transformer-like\nnetwork. In: Journal of Physics: Conference Series, Vol. 2010 (2021). IOP Publishing.\np. 012175.\n34. Zhang L, Wen Y. A transformer-based framework for automa tic COVID19\ndiagnosis in chest CTs. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision(2021). p. 513–8.\n35. Konar D, Panigrahi BK, Bhattacharyya S, Dey N, Jiang R. Au to-diagnosis of\nCOVID-19 using lung CT images with semi-supervised shallow learn ing network. IEEE\nAccess. (2021) 9:28716–28. doi: 10.1109/ACCESS.2021.3058854\n36. Vaid S, Kalantar R, Bhandari M. Deep learning COVID-19\ndetection bias: accuracy through artiﬁcial intelligence. Int Orthop. (2020)\n44:1539–42. doi: 10.1007/s00264-020-04609-7\n37. Ozturk T, Talo M, Yildirim EA, Baloglu UB, Yildirim O, Acharya U R.\nAutomated detection of COVID-19 cases using deep neural net works with X-\nray images. Comput Biol Med.(2020) 121:103792. doi: 10.1016/j.compbiomed.2020.\n103792\n38. Panwar H, Gupta P , Siddiqui MK, Morales-Menendez R, Singh V. A pplication\nof deep learning for fast detection of COVID-19 in X-Rays usin g nCOVnet. Chaos\nSolitons Fractals.(2020) 138:109944. doi: 10.1016/j.chaos.2020.109944\nFrontiers in Public Health /one.tnum/four.tnum frontiersin.org\nMarefat et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpubh./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/five.tnum/seven.tnum/four.tnum/six.tnum\n39. Ahuja S, Panigrahi BK, Dey N, Rajinikanth V , Gandhi TK. De ep transfer\nlearning-based automated detection of COVID-19 from lung CT s can slices. Appl\nIntell. (2021) 51:571–85. doi: 10.1007/s10489-020-01826-w\n40. Sharifrazi D, Alizadehsani R, Roshanzamir M, Joloudari JH , Shoeibi A, Jafari M,\net al. Fusion of convolution neural network, support vector mach ine and Sobel ﬁlter\nfor accurate detection of COVID-19 patients using X-ray imag es. Biomed Signal Process\nControl. (2021) 68:102622. doi: 10.1016/j.bspc.2021.102622\n41. Khozeimeh F, Sharifrazi D, Izadi NH, Joloudari JH, Shoeib i A, Alizadehsani\nR, et al. Combining a convolutional neural network with autoen coders to\npredict the survival chance of COVID-19 patients. Sci Rep. (2021) 11:1–\n18. doi: 10.1038/s41598-021-93543-8\n42. Kumar A, Tripathi AR, Satapathy SC, Zhang Y-D. SARS-Net:\nCOVID-19 detection from chest x-rays by combining graph conv olutional\nnetwork and convolutional neural network. Pattern Recognit. (2022)\n122:108255. doi: 10.1016/j.patcog.2021.108255\n43. Esmi N, Golshan Y, Asadi S, Shahbahrami A, Gaydadjiev G. A\nfuzzy ﬁne-tuned model for COVID-19 diagnosis. Comput Biol Med. (2023)\n2023:106483. doi: 10.1016/j.compbiomed.2022.106483\n44. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.\nAttention is all you need. Adv Neural Inf Process Syst. (2017) 30:1–11.\n45. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint . (2018).\ndoi: 10.5555/3295222.3295349\n46. Hassannataj Joloudari J, Azizi F, Nematollahi MA, Alizadehs ani R,\nHassannatajjeloudari E, Nodehi I, et al. GSVMA: a genetic support vector\nmachine ANOV A method for CAD diagnosis. Front Cardiovasc Med. (2022)\n8:2178. doi: 10.3389/fcvm.2021.760178\n47. Wang X, Deng X, Fu Q, Zhou Q, Feng J, Ma H, et al. A weakly-superv ised\nframework for COVID-19 classiﬁcation and lesion localization f rom chest CT. IEEE\nTrans Med Imaging.(2020) 39:2615–2625. doi: 10.1109/TMI.2020.2995965\n48. Prokop M, Van Everdingen W, van Rees Vellinga T, Quarles van U ﬀord\nH, Stöger L, Beenen L, et al. CO-RADS: a categorical CT assessm ent scheme for\npatients suspected of having COVID-19—deﬁnition and evaluati on. Radiology. (2020)\n296:E97–104. doi: 10.1148/radiol.2020201473\n49. Cai H, Zhu L, Han S. Proxylessnas: direct neural architect ure search on target\ntask and hardware. arXiv preprint. (2018).\n50. Huang XS, Perez F, Ba J, Volkovs M. Improving transformer opt imization\nthrough better initialization. In: International Conference on Machine Learning(2020).\nPMLR. p. 4475–83.\n51. Alakus TB, Turkoglu I. Comparison of deep learning approaches\nto predict COVID-19 infection. Chaos Solitons Fractals. (2020)\n140:110120. doi: 10.1016/j.chaos.2020.110120\n52. Oguz Ç, Yaganoglu M. Detection of COVID-19 using deep\nlearning techniques and classiﬁcation methods. Inf Process Manag. (2022)\n59:103025. doi: 10.1016/j.ipm.2022.103025\n53. Srivastava G, Chauhan A, Jangid M, Chaurasia S. CoviXNet: A novel and\neﬃcient deep learning model for detection of COVID-19 using c hest X-Ray\nimages. Biomed Signal Process Control.(2022) 2022:103848. doi: 10.1016/j.bspc.2022.\n103848\n54. Chen JI-Z. Design of accurate classiﬁcation of COVID-19 d isease\nin X-ray images using deep learning approach. J ISMAC. (2021)\n3:132–48. doi: 10.36548/jismac.2021.2.006\n55. Nasiri H, Hasani S. Automated detection of COVID-19 case s from chest X-\nray images using deep neural network and XGBoost. Radiography. (2022) 28:732–\n8. doi: 10.1016/j.radi.2022.03.011\n56. Jain G, Mittal D, Thakur D, Mittal MK. A deep learning approach to detect\nCovid-19 coronavirus with X-ray images. Biocybernetics Biomed Eng.(2020) 40:1391–\n405. doi: 10.1016/j.bbe.2020.08.008\n57. Al-Waisy A, Mohammed MA, Al-Fahdawi S, Maashi M, Garcia-Za pirain B,\nAbdulkareem KH, et al. COVID-DeepNet: hybrid multimodal deep lea rning system\nfor improving COVID-19 pneumonia detection in chest X-ray ima ges. Comput Mater\nContinua. (2021) 67:2409–29. doi: 10.32604/cmc.2021.012955\n58. Ardakani AA, Kanaﬁ AR, Acharya UR, Khadem N, Mohammadi A.\nApplication of deep learning technique to manage COVID-19 in rout ine\nclinical practice using CT images: results of 10 convolutional n eural networks.\nComput Biol Med. (2020) 121:103795. doi: 10.1016/j.compbiomed.2020.\n103795\n59. Zhao W, Jiang W, Qiu X. Fine-tuning convolutional neural n etworks\nfor COVID-19 detection from chest X-ray images. Diagnostics. (2021)\n11:1887. doi: 10.3390/diagnostics11101887\n60. Haghanifar A, Majdabadi MM, Choi Y, Deivalakshmi S, Ko S. C ovid-cxnet:\ndetecting covid-19 in frontal chest x-ray images using deep learning. Multimed Tools\nAppl. (2022) 81:30615–45. doi: 10.1007/s11042-022-12156-z\n61. Mahajan S, Raina A, Gao XZ, Pandit AK. COVID-19 detection using hybrid\ndeep learning model in chest x-rays images. Concurr Comput Pract Exp. (2022)\n34:e6747. doi: 10.1002/cpe.6747\n62. Rajpurkar P , Irvin J, Zhu K, Yang B, Mehta H, Duan T, et al. Che xnet: radiologist-\nlevel pneumonia detection on chest x-rays with deep learning. arXiv preprint. (2017).\nFrontiers in Public Health /one.tnum/five.tnum frontiersin.org"
}