{
  "title": "Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning",
  "url": "https://openalex.org/W4385876383",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2015423992",
      "name": "Georgia Chalvatzaki",
      "affiliations": [
        "Technical University of Darmstadt",
        "Hessisches Landesmuseum Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2112712228",
      "name": "Ali Younes",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A3138958729",
      "name": "Daljeet Nandha",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": null,
      "name": "An Thai Le",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2951364960",
      "name": "Leonardo F. R. Ribeiro",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A40109512",
      "name": "Iryna Gurevych",
      "affiliations": [
        "Hessisches Landesmuseum Darmstadt",
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2015423992",
      "name": "Georgia Chalvatzaki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112712228",
      "name": "Ali Younes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3138958729",
      "name": "Daljeet Nandha",
      "affiliations": []
    },
    {
      "id": null,
      "name": "An Thai Le",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2951364960",
      "name": "Leonardo F. R. Ribeiro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A40109512",
      "name": "Iryna Gurevych",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6680532216",
    "https://openalex.org/W1969472392",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W6802171045",
    "https://openalex.org/W2161414194",
    "https://openalex.org/W6803455561",
    "https://openalex.org/W6846242362",
    "https://openalex.org/W6680464289",
    "https://openalex.org/W2945600613",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6810156098",
    "https://openalex.org/W6796126471",
    "https://openalex.org/W6713184877",
    "https://openalex.org/W6762392948",
    "https://openalex.org/W2914044489",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2135639338",
    "https://openalex.org/W4214700710",
    "https://openalex.org/W4303648971",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2403069916",
    "https://openalex.org/W4221159132",
    "https://openalex.org/W4286857134",
    "https://openalex.org/W3200650887"
  ],
  "abstract": "Long-horizon task planning is essential for the development of intelligent assistive and service robots. In this work, we investigate the applicability of a smaller class of large language models (LLMs), specifically GPT-2, in robotic task planning by learning to decompose tasks into subgoal specifications for a planner to execute sequentially. Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark. We compare our approach with classical planning and baseline methods to examine the applicability and generalizability of LLM-based planners. Our findings suggest that the knowledge stored in an LLM can be effectively grounded to perform long-horizon task planning, demonstrating the promising potential for the future application of neuro-symbolic planning methods in robotics.",
  "full_text": "TYPE Original Research\nPUBLISHED 15 August 2023\nDOI 10.3389/frobt.2023.1221739\nOPEN ACCESS\nEDITED BY\nDimitrios Kanoulas,\nUniversity College London,\nUnited Kingdom\nREVIEWED BY\nMaria Koskinopoulou,\nHeriot-Watt University, United Kingdom\nDario Albani,\nT echnology Innovation Institute (TII),\nUnited Arab Emirates\n*CORRESPONDENCE\nGeorgia Chalvatzaki,\ngeorgia.chalvatzaki@tu-darmstadt.de\n†PRESENT ADDRESS\nLeonardo F. R. Ribeiro,\nTU Darmstadt, Darmstadt, Germany\nRECEIVED 12 May 2023\nACCEPTED 03 July 2023\nPUBLISHED 15 August 2023\nCITATION\nChalvatzaki G, Younes A, Nandha D,\nLe AT, Ribeiro LFR and Gurevych I (2023),\nLearning to reason over scene graphs: a\ncase study of finetuning GPT-2 into a\nrobot language model for grounded task\nplanning.\nFront. Robot. AI 10:1221739.\ndoi: 10.3389/frobt.2023.1221739\nCOPYRIGHT\n© 2023 Chalvatzaki, Younes, Nandha, Le,\nRibeiro and Gurevych. This is an\nopen-access article distributed under\nthe terms of the Creative Commons\nAttribution License (CC BY) . The use,\ndistribution or reproduction in other\nforums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution\nor reproduction is permitted which does\nnot comply with these terms.\nLearning to reason over scene\ngraphs: a case study of finetuning\nGPT-2 into a robot language\nmodel for grounded task\nplanning\nGeorgia Chalvatzaki 1,2,3*, Ali Younes 1, Daljeet Nandha 1,\nAn Thai Le 1, Leonardo F. R. Ribeiro 4† and Iryna Gurevych 1,2\n1Computer Science Department, Technische Universität Darmstadt, Darmstadt, Germany, 2Hessian.AI,\nDarmstadt, Germany, 3Center for Mind, Brain and Behavior, University Marburg and JLU Giessen,\nMarburg, Germany, 4Amazon Alexa, Seattle, WA, United States\nLong-horizon task planning is essential for the development of intelligent\nassistive and service robots. In this work, we investigate the applicability of a\nsmaller class of large language models (LLMs), specifically GPT-2, in robotic\ntask planning by learning to decompose tasks into subgoal specifications for\na planner to execute sequentially. Our method grounds the input of the LLM\non the domain that is represented as a scene graph, enabling it to translate\nhuman requests into executable robot plans, thereby learning to reason over\nlong-horizon tasks, as encountered in the ALFRED benchmark. We compare\nour approach with classical planning and baseline methods to examine the\napplicability and generalizability of LLM-based planners. Our findings suggest\nthat the knowledge stored in an LLM can be effectively grounded to perform\nlong-horizon task planning, demonstrating the promising potential for the future\napplication of neuro-symbolic planning methods in robotics.\nKEYWORDS\nrobot learning, task planning, grounding, language models (LMs), pretrained models,\nscene graphs\n1 Introduction\nThe autonomous execution of long-horizon tasks is of utmost importance for future\nassistive and service robots. An intelligent robot should reason about its surroundings, e.g.,\nregarding the included objects and their spatial-semantic relations, and abstract an action\nplan for achieving a goal that will purposefully alter the perceived environment. Such an\nelaborate course of robot actions requires scene understanding, semantic reasoning, and\nplanning over symbols and geometries. The advent of Deep Learning led many researchers\nto faithfully follow end-to-end approaches due to the representation power of differentiable\ndeep neural networks (LeCun et al., 2015).\nThe problem of sequential decision-making has been addressed both with search-\nbased and optimization approaches (Kaelbling and Lozano-Pérez, 2011; Toussaint, 2015;\nDriess and Toussaint, 2019; Garrett et al., 2021; 2020), as well as learning-based (Nair and\nFinn, 2019; Funk et al., 2021; Hoang et al., 2021) and hybrid methods (Kim et al., 2019;\nDriess et al., 2020;Ren et al., 2021;Funk et al., 2022). While the first ones enjoy probabilistic\nFrontiers in Robotics and AI 01 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\ncompleteness, they require full domain specification and have high\ncomputational demands.The learning-based methods require broad\nexploration to learn from experience, but they have shown better\ngeneralization capabilities in similar domains to those experienced\nduring training.\nLarge Language Models (LLMs) have exhibited an\nunprecedented generative ability (Bommasani et al., 2021), thanks\ntothetransformerarchitecture( Vaswani et al.,2017)combinedwith\nmassive datasets distilled from the internet. Naturally, in the quest\nfor general artificial intelligence, researchers try to benchmark such\nmodels in reasoning tasks, among others (Wang et al., 2018; 2019).\nRobotic embodied intelligence requires both logical and geometric\nreasoning; hence, it is a holy grail of AI. Several researchers saw a\nbenefit in LLMs, and it was not long before several works explored\ntheir application to robotics for endowing robots with reasoning\nabilities in the scope of autonomous task planning and interaction\n(Brohan et al., 2022; Wei et al., 2022b). However, most works have\nfocused on the prompting (Brown et al., 2020) and the subsequent\nprompt engineering (White et al., 2023), in which engineers provide\nappropriate inputs to LLMs for extracting outputs that can be\nrealizable by a robotic agent, either for human-instruction following\n(Ouyang et al., 2022) or for planning (Singh et al., 2022; Zeng et al.,\n2022).\nIn this work, we study a finetuning process for grounding a\nsmall LLM for robotics, i.e., GPT-2 (Radford et al., 2021), to be used\nas a high-level abstraction in a task planning pipeline. Particularly,\nwe propose a method that decomposes a long-horizon task into\nsubgoals in the form of goal specifications for a robotic task planner\nto execute, and we investigate whether such a method can reach the\nperformance levels of an oracle task planning baseline.\nOur contribution is twofold: (i) we propose a novel method for\nlinearizing the relations in a scene-graph structure representing the\ndomain (world) to provide it as grounding context when finetuning\na pretrained language model (e.g., GPT-2) for learning to draw\nassociations between possible actions (goto, pick, etc.) and objects\nin the scene (e.g., kitchen, apple, etc.). Importantly, in our context,\nwe encode the relative position of objects (far, close, right, left, etc.),\nallowing our model to account for the scene’s geometrical structure\nwhen learning to plan. The proper structure of the input context is\nnecessary for enabling the model to reason about the combinatorics\nof actions with affordable objects and their logical sequence (e.g., to\ncook something, one must first go to the kitchen). (ii) We showed\nthat larger pretrained models do not necessarily possess grounded\nreasoning abilities, while it is possible to finetune smaller models\non various tasks to use them as parts of a broader neuro-symbolic\nplanningarchitecture.Contrarilytoworksthatdirectlyapplyactions\nsuggested by the GPTs to robots, we use language models at a\nhigher level of abstraction, effectively suggesting sub-goals as PDDL\nproblems to be solved by a Fast Downward task plannerHelmert\n(2006),effectivelydecomposingthewholeproblemintosmallerones\nof lower complexity.\nOur thorough experimental evaluation shows that finetuning\nGPT-2 by additionally grounding its input on the domain can\nhelp translate human requests (tasks) to executable robot plans, to\nlearn to reason over long-horizon tasks, as those encountered in\nthe ALFRED benchmark (Shridhar et al., 2020). We compare our\nproposed approach with classical planning methods to investigate\nthe applicability and generalizability of the Pre-trained Language\nModel (PLM)-based planners compared to classical task planners\noperating on a limited computational budget for a fair comparison.\nWe conclude that the knowledge stored in a PLM can be grounded\non different domains to perform long-horizon task planning,\nshowing encouraging results for the future application of neuro-\nsymbolic planning methods in robotics.\n2 State of the art\n2.1 Reasoning with large language models\nLLMs have attracted much attention for understanding\nthe commonsense and reasoning patterns in their latent space\n(Zhou et al.,2020;Li et al.,2021;Bian et al.,2023).Ithasbeenshown\nthat some abilities in logical and mathematical reasoning seem to\nemerge when LLMs are prompted appropriately (Wei et al., 2022b;\na). However, the engineering effort, as well as the lack of robustness,\nis a key issue in prompting massive models ( Ruis et al., 2022;\nValmeekam et al., 2022). While great effort seems to be consumed\non few-shot prompting of huge parametric models, it has also been\nshown by other lines of work show that efficient finetuning of much\nsmaller models (Tay et al., 2022), or the use of small adaptation\nmodules (Adapters) ( Houlsby et al., 2019 ; Pfeiffer et al., 2021 )\ncan lead to methods that perform more robustly than large-scale\ngeneralist few-shot prompters. In the same direction, the chatbot\nversions of those huge models raised several points of criticism\nrecently, showing that much more is needed than just prompting a\nblind human-preference alignment\n1\n.\n2.2 Robot behavior planning\nLong-horizon robot behavior planning is an NP-hard problem\n(Wells et al., 2019). Current advances in ML and perception led\nresearcherstorevisitthisfundamentalproblem,i.e.,theexecutionof\nmulti-stage tasks, whose completion requires many sequential goals\nto be achieved, considering learning-based heuristics (Driess et al.,\n2020). Researchers consider such problems as Task And Motion\nPlanning (TAMP) problems (Garrett et al., 2021; Ren et al., 2021;\nXu et al., 2022), with a symbolic plan over entities and predicate\nwith respective action operators with preconditions and effects\nin the environment. In contrast, a motion plan tries to find a\nfeasible path to the goal. Nevertheless, most TAMP methods rely\non manually specified rules; they do not integrate perception, and\nthe combinatorial explosion when searching over symbolic and\ncontinuousparametersprohibitsscalingthemethodstochallenging,\nrealistic problems (Kim et al., 2019; Garrett et al., 2020).\nTransformer models (Vaswani et al., 2017) that revolutionized\nthe field of Natural Language Processing (NLP) opened the way\nfor multiple new applications, in particular for robotics, e.g., visual-\nlanguage instruction following (Pashevich et al., 2021), 3D scene\nunderstanding and grounding ( Chen W. et al., 2022; Mees et al.,\n2022), language-based navigation (Huang C. et al., 2022; Shah et al.,\n2023). Due to their training on extensive databases, several\n1 7 problems facing Bing, Bard, and the future of AI search, from The Verge.\nFrontiers in Robotics and AI 02 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nworks explored the use of LLMs for task planning and long-\nhorizon manipulation ( Huang et al., 2022b), mainly employing\nclever prompting ( Raman et al., 2022; Singh et al., 2022), using\nmultimodal information ( Jiang et al., 2022 ; Zeng et al., 2022 ),\ngrounding with value-functions (Chen B. et al., 2022; Brohan et al.,\n2022; Huang et al., 2022c ), and deploying advances in code\ngeneration to extract executable robot plans ( Liang et al., 2022).\n(Li et al., 2022) propose to use a PLM as a scaffold for decision-\nmakingpoliciesininteractiveenvironments,demonstratingbenefits\ninthegeneralizationabilitiesforpolicylearningevenwhenlanguage\nis not provided as input or output. Recently, PALM-e (Driess et al.,\n2020) has integrated a vision transformer with the PALM language\nmodel and has encoded some robotic state data to propose a\nmultimodal embodied model, which showed the potential of\nintegrating geometric information of the robot state but achieved\nlimited performance in robotic tasks.\n3 Language models for grounded\nrobot task planning\n3.1 Problem statement\nLet us assume an agent that is able to move and manipulate\nobjects in an environment, e.g., a mobile manipulator robot in a\nhousehold environment. Let the environment be composed of a\ncombination ofrooms, such as ‘bathroom,’ ‘living room,’ or ‘kitchen.’\nEach room containsobjects and receptacles, i.e., objects that are able\nto receive other objects, such as ‘table,’ ‘drawer,’ or ‘sink.’ Each object\nhas (household-specific) properties (affordances) associated with it\nthat define whether it can be picked up, cleaned, heated, cooled, cut,\netc. These properties can change, meaning that the objects have a\nstate. The agent can pick up only one object at a time, meaning the\nagentalsohasastate,e.g.,‘objectinhand.’Giventhefactthatthestate\nis preserved over time and future actions depend on past actions,\nthe environment can be characterized as sequential. Therefore, a\nseries of actions has to be reasoned upon for an agent to be able to\nexecute a series of actions for solving a long-horizon task, i.e., a task\nthat requires the completion of several subtasks and potentially the\nmanipulation of various objects to achieve the end-goal.\n3.2 The ALFRED benchmark\nThe ALFRED benchmark ( Shridhar et al., 2020 ) contains\nhuman-annotated training samples and image-based recordings of\neverydayhouseholdtasks;thisis“25,743Englishlanguagedirectives\ndescribing 8,055 expert demonstrations averaging 50 steps each,\nresulting in 428,322 image-action pair”. In addition to that, the\ndataset provides a PDDL domain of the overall task and a PDDL\nproblem for each sample ( Aeronautiques et al., 1998). ALFRED\nheavily depends on AI2-THOR (Kolve et al., 2017), which acts as\nthe underlying controller and simulation environment (based on\nthe Unity game engine): trajectories for each sample of the ALFRED\ndataset were generated with AI2-THOR, and the validation of user-\ngeneratedactionsrequirestheAI2-THORcontroller. Figure 1shows\nasamplesceneloadedintotheAI2-THORsimulator.Eachsamplein\nthe dataset consists of a high-level plan in PDDL and the trajectory\nof the agent’s actions which lead to successful task completion,\ntogether with a description of the task goal and each plan step in\nNatural Language (NL).\n3.3 State and action space\nThe state space defines the feedback provided by the\nenvironment, while the action space defines the available actions\nto interact with the environment.\nState space. The environments we consider in this work\nare fully observable, having access to the complete simulator\nstate representing the domain, as commonly considered for task\nplanning tasks. AI2-THOR, the underlying simulator, eliminates\nmost physics-related aspects (e.g., objects are automatically picked\nup and placed by a single action), which makes the highly\ndynamic and stochastic household environment almost static\nand deterministic—almost because some physics still exists. This\nsimplifies the core TAMP problem along with the discrete agent\nactions defined in the ALFRED dataset. Therefore, the ALFRED\nbenchmark represents an appropriate choice for studying the\nproblemoflearningforrobotictaskplanning,wheremotionfailures\nare minimized by the underlying AI2-THOR controllers. Hence, we\ncanfocusonthereasoningaspectsoftheproblem,whichisthefocus\nof this study. In the following, the state of the domain is transformed\nto NL context (§3.6) and not used directly as model input.\nAction space. ALFRED has an action space of eight\ndiscrete high-level actions:GotoLocation, PickupObject, PutObject,\nCoolObject, HeatObject, CleanObject, SliceObject and T oggleObject.\nThe underlying AI2-THOR navigation controller also has a discrete\naction space; the agent can move forward, backward, left or right\nand rotate clockwise or counter-clockwise in fixed steps.\n3.4 Task categories\nThe ALFRED dataset encompasses seven categories of\nhousehold tasks: “Look at object,” “Pick and place,” “Pick two\nand place,” “Pick and place with movable receptacle,” “Pick, clean\nthen place,” “Pick, cool then place,” “Pick, heat then place.” Because\nobjects can be placed in different corners of a room, each of these\ntasks includes the sub-problem of navigation. For the ‘pick’ or ‘place’\nsubtasks executing the respectivePickupObject or PutObject action\nis sufficient. But, the subtasks “clean”, “cool” and “heat” must be\nseen as planning problems on their own, because the corresponding\nactions are a composition of high-level state-dependent actions.\nRegarding the household environment, the subtask “cool” requires\na fridge, “heat” requires a microwave (or oven), and “clean” requires\na sink as anreceptacle. The ALFRED simulator tracks the state of\neach object, and the subtask is only considered successful when the\nfinal object state is correct. For example, if the task category is ‘Pick,\nclean then place’, the task goal is only completed when the placed\nobject is marked as ‘clean.’ The implementation aspects of these task\ncategories are discussed inSection 4.\nFrontiers in Robotics and AI 03 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nFIGURE 1\nAI2-THOR simulator rendering a sample rollout from the ALFRED ( Shridhar et al., 2020). The scenes show a room with household objects and the\nrobot executing a task. Note that the robot does not have an arm, and the object automatically floats in front of the camera; it interacts with the\nenvironment through discrete actions. The discrete actions are shown underneath each frame in the form of PDDL commands.\n3.5 RobLM: robot language model for task\nplan generation\nJust like images can be represented by discretizing color space,\nNL can be expressed as a sequence of tokens x = [x1, x2, … ,xn],\nwhere each token is mapped to an embedding (lookup table). The\nLanguage Model (LM) places a probability distributionp(x) over the\noutput token sequence.p(x) can be decomposed into a conditional\nprobability distribution p (xi+1|xi), where the probability of each\ntoken depends on all previous tokens. This results in the following\njoint distributionp(x) for a sequence of tokensx:\np (x) = ∏\ni\np (xi|x0, x1, …,xi−1) (1)\nInregardstoNeuralNetwork(NN), p(x) iscommonlyestimated\nwith the Softmax function (Bengio et al., 2000)2\np (x) = So ftmax (WhT + b) =\nexp(WhT + b)\n∑ exp(WhT + b)\n, (2)\nwhere W is the learned weight matrix, b the bias and hT the\noutput vector of the NN. For text generation, the joint probability\n2 The Softmax function applies the exponential function to each element of\nthe input vector and normalizes the values, by dividing by the sum of the\nexponentials.\ndistribution p(x) (see Eq. 1) can be formulated as a maximum-\nlikelihood objective, where the objective is to maximize the\nlikelihood of the next token occurrence for the given data.\nOur goal is to finetune a LM to get a Robot Language Model\n(RobLM) that can generate a complete high-level task plan in one\nshot, given the domain information and a task goal. Because LMs\nare unsupervised learners, a single training sample contains both\ngiven and desired information as NL text. A restriction to the text\nformat (a string of characters) comes with challenges: structural\ninformation needs to be condensed into a single linear dimension,\nand conceptually different aspects of the input need to be annotated\nin the text. This text format, including the syntax, has to be designed\nin such a way that information can be fed to and extracted from the\nLM reliably.\nIn RobLM, the format definition for a NL task description\nmust comply with the following syntactic rule (spaces added for\nreadability):\nGoal [<SEP> Context] <BOS> Plan <EOS>\n[...] := optional\n<SEP> := separator token\n<BOS> := begin-of-sequence token\n<EOS> := end-of-sequence token\nGoal is the task goal in NL. Context is any additional, yet\noptional information provided to the LM. The task might have\nFrontiers in Robotics and AI 04 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nambiguous solutions, and the inherent assumption is that the LM\nwill better “understand” the task if given a context. Examples of a\ncontext are the name of theroom, the name of the target object, or a\nNL description of the environment (see 3.6).\nPlan is the sequence of high-level task actions and their\nrespective arguments, such as objective name or location. Because\nPLM have been trained on a diverse corpus of NL, including\nprogram code, the format for plans follows syntactical rules similar\nto that of a generic programming language:\nAction0(arg0[,arg1]); Action1(arg0[,arg1]);\n...\nThe sequence between the special tokens<BOS > and <EOS >\ncan be extracted to retrieve the plan from the LM-generated output.\n3.5.1 Data augmentation\nEach sample in the ALFRED dataset can be replayed in the AI2-\nTHOR simulator to collect additional information not contained\nin the original dataset. ALFRED provides a script that has been\nmodified for that purpose. Data augmentation is necessary for\nGraph2NL (c.f. §3.6) to generate a graph representation from the\nenvironment state. For each replayed sample, the complete list of\nobjects in the scene, with their respective name, position, and\nrotation, and the agent position is saved to a separate file next to the\ntrajectory data. This file is later loaded and turned into a processable\ngraph.\n3.6 Mapping scene graphs to natural\nlanguage: Graph2NL\nPLMs are trained on NL. Because of this, NL is a natural\nmodality for finetuning a PLM. When a context is provided to\nthe LM, this context must be presented in NL just like the input\nsequence. If the context should encapsulate the environment state,\nthis means that the state has to be transformed into NL before being\nsupplied to the PLM.\nGraph2NL is a novel method that “translates” the object-\ncentric scene graph representation of the environment state to\nNL. Optionally, domain knowledge about the environment3 can\nbe infused into this graph. The following steps describe the core\nGraph2NL process.\n1) Generate an object-scene graph G with a node for the agent\nand nodes corresponding to objects, node attributes being the\nposition and rotation of the object in Euclidean space and their\nrespective distance and orientation vectors as edge attributes.\n2) (Optional) Infuse domain knowledge about the environment by\nconnecting all dependent nodes and all nodes reachable by the\nagent.\n3) Connect the agent (node) to all reachable nodes, if given domain\nknowledge, or to all nodes, if not given domain knowledge.\n3 Domain knowledge entails every possible room, object, and receptacle name\nand their allowed relations, as described in the respective documentation:\nhttps://ai2thor.allenai.org/ithor/documentation/objects/object-types.\nTABLE 1 Graph2NL mapping table. Distances are mapped to NL vocabulary\n(or a symbol) in a one-to-one relation. Y awdescribes the orientation along\nthe surface normal when viewed from a top-down perspective, and Pitch\ndescribes the z-planar offset (altitude) in relation to the origin.\nDistance [m]\nValue NL Symbol\n>5 distant a\n>4 far b\n>3 reachable c\n>2 near d\n>1 close e\n>0.5 closer f\n>0.1 next g\n<0.1 in h\nYaw [°]\nValue NL Symbol\n45 to 135 right i\n135 to 225 back j\n225 to 315 left k\n315 to 45 front l\nPitch [°]\nValue NL Symbol\n≥0 above m\n<0 below n\n4) Given a task and the identified target object, find all paths in the\ngraphs leading from the agent (node) to the target object (node).\n5) Use edge attributes in the found paths to describe the task-\ncentric environment state, by mapping geometric relations to NL\ntokens.\n3.6.1 NL mapping\nTo translate geometric relations attributed by the graph edges\ninto a NL description, a mapping function is designed. In human\nspeech, distances are expressed by a vocabulary of words such as\n“close” or “far” and orientations are expressed by words such as “in\nfront” or “behind”. Graph2NL adapts this vocabulary to describe\nthe (numeric) distance and orientation from one node relative to\nanother in NL.\nTable 1 summarizes the mapping used in Graph2NL. The\ndistance between nodes is expressed in Cartesian space and\norientation in polar coordinates, whereY awis the azimuth angle\n(rotation along the surface normal) and Pitch is the zenith angle\n(altitude). With this mapping, the geometric relation between\ntwo nodes can be explained by three words (one for each:\ndistance, pitch, and yaw). The vocabulary contains 8 words\nto express the distance, 4 words to express the vertical, and\n2 words to express the horizontal orientation. Combinatorially,\nthis gives 64 possible geometric configurations. The geometric\nrelationship is expressed in a condensed form by treating each\nof these configurations as a relation and assigning a special\nFrontiers in Robotics and AI 05 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nFIGURE 2\nGraph2NL example graph. After locating the root (“agent”) and target node (“soapbar”), the shortest paths connecting those nodes are found and\nsummarized in NL by mapping all edge attributes along the path.\nsymbol (token) for each relation. A simple approach, referring\nto the Symbol column in Table 1, is by assigning a symbol to\neach word. Combining the symbols for distance, pitch, and yaw\ncreates the condensed (three-letter) representation of the geometric\nrelationship. These symbolic representations can optionally be\nadded to the LM tokenizer as special tokens. Shorter token\nsequences generally decrease both the training and inference\ntime.\nExample Let the task be: “Put the soap into the\ndrawer”. The input query to Graph2NL consists of the\ntarget object “soap”. Figure 2 shows the graph constructed by\nGraph2NL from augmented data (§3.5.1), including domain-\nspecific knowledge.After finding the shortest paths between\nthe root (‘agent’) and target node (‘soapbar’), Graph2NL\nproduces an output in the following form (cut-off at search\ndepth 2):\n[Bathroom=\n- closer below left sink near below back\nsoapbar\n- closer below left cabinet near above\nback soapbar\n- closer above left countertop next above\nback soapbar\n- close below back toilet closer below\nback soapbar\n- closer below back garbagecan close below\nback soapbar]\nThe NL context by Graph2NL starts with the name of the\nroom extracted from the scene graph, followed by the geometric\ndescription of each node connected to the target node on the path\nfrom the agent. “-” indicates the root note, i.e., the agent. For the\nFrontiers in Robotics and AI 06 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nprevious example, Graph2NL produces the following condensed\nform:\n[Bathroom=\n- fnk sink dnj soapbar\n- fnj cabinet dmj soapbar\n- fmk countertop gmj soapbar\n- enj toilet fnj soapbar\n- fnj garbagecan enj soapbar]\nThis form of state representation is unique for each problem\nconfiguration and forms the context that grounds RobLM.\n3.7 Training\nRobLM generates a plan as text given the goal and the context,\nwhich involves causal language modeling for text generation.\nDecoder-only autoregressive language models ( Figure 3) are\nfrequently used for the problem of text generation; we chose\nGPT-2 as the base model for RobLM. RobLM uses the base\nversion of the GPT-2 PLM (‘gpt-2’) (Radford et al., 2021), loaded\nand initialized with pre-trained weights from the Huggingface\n(Wolf et al., 2019) Transformer library. Finetuning GPT-2 for causal\nlanguage generation has a self-supervised setup, where the labels\nare the inputs shifted to the right, which entitles learning to predict\nthe next token in a sequence.We finetune the GPT-2 model using\nthe pre-processed training data of the ALFRED dataset, which\nhas around 20.000 samples, with three sets of NL descriptions\nfor each sample. The ADAM (Kingma and Ba, 2014) optimizer\nis used with a learning rate of 5 e−5, and the LM is trained for\ntwo epochs. Finetuning a GPT-2 LM to the ALFRED training data\nwith a single GPU-accelerated computer takes around 30 min (27\niterations/s - measurement not representative due to hardware\ndependence).\n3.8 Generation pipeline\nFor inference, RobLM takes only the NL task goal together with\nan optional context and outputs the complete step-by-step plan for\ncompletingthegoal.Thisplaniscomposedofhigh-levelinstructions\nrather than low-level controller commands.\nExample. Giventhetask“Putthesoapintothedrawer:”,RobLM\n(no context) generates the plan:\nPut the soap into the drawer:\n0.GotoLocation(countertop)\n1.PickupObject(soap)\n2.GotoLocation(drawer)\n3.PutObject(soap,drawer)\nThe plan is generated by consecutive forward passes through\nthe Transformer model. For a vocabulary size of k and a token\nsequence of length l (with l ≤ 1, 024 for GPT-2), the forward pass\nof the Transformer yields an output vector of sizek × l with values\nin the interval[0, 1]. The Transformer outputsscores, i.e.,logits, for\neach token in the input sequence. These scores are converted to\nFIGURE 3\nDecoder-only T ransformer architecture. The input to the decoder is tokenized text, and the output is probabilities over the tokens in the tokenizer\nvocabulary. The positional encoding is added to the embedded input to account for the order. T ransformer’s decoder can have multiple transformer\nblocks, each of which contains multi-head attention with linear layers and layer normalization.\nFrontiers in Robotics and AI 07 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nFIGURE 4\nIllustration of a forward pass through RobLM for text generation with a greedy next-token selection strategy. The forward passes are repeated in a\nrecursive manner until an end-of-text token is encountered or the defined sequence limit is reached.\na probability distribution p(x) by using the Softmax function, as\ndescribed in Eq. 2.\nTwo possible generation strategies for selecting the next\ntoken from p(x) are: greedy search and top-k/top-p sampling\n(Holtzman et al., 2020). In the greedy strategy, the token xsel\nwith the highest likelihood is picked with xsel = argmax p(x). In\nthe top-k sampling strategy, as the name suggests, the scores\nare sorted, and one of the first k candidate tokens is randomly\nsampled. By extending the top-k sampling with an additional top-\np strategy, the sum of the k candidates must be equal to or\ngreater thanp ∈ [0, 1]. Simply put, top-k widens the choice over the\nnext tokens and top-p filters out low-probability tokens.Figure 4\nillustrates, on the basis of an example, a forward pass through\nthe Transformer with a greedy selection strategy. These steps are\nrepeated recursively until an end-of-text token is encountered\nor the defined sequence limit is reached to generate the full\nplan.\nThis LM model was finetuned to generate a structured output,\nomitting special tokens, characterized by numbered actions and\ntheir arguments in parenthesis. The input is always part of the\noutput, due to the generation function utilized by RobLM. Note\nthat it is not guaranteed that the ‘soap’ can be found inside\nthe ‘drawer’ on the ‘countertop’. In fact, it could be at any\npossible location permitted by the environment. However, given\na greedy search strategy, for the given task goal, the likelihood\nfor the ‘soap’ being on the ‘countertop’ is the highest in this\ncase.\n3.8.1 Hardware setup\nFor finetuning LMs and evaluating each model, we used the\nLichtenberg Cluster of TU Darmstadt, which contains stacks of\nNVIDIA\nR⃝ A100 and V100 GPUs. Internal tests have shown that\na single GPU can decrease the training time by a factor of 10\n(these tests are not representative because performance depends on\nevery hardware component). To run experiments in the AI2-THOR\nsimulation, we used a PC with an NVIDIA\nR⃝ RTX 3080Ti GPU.\n4 Experiments\n4.1 Preliminary analysis for task plan\ngeneration with GPT-2 and GPT-3\nLLMs can represent knowledge from the data they have been\ntrained on. However, the question remains, whether this knowledge\ncan be leveraged to solve planning tasks, i.e., can LLMs reason? This\nis investigated by comparing the text-generation results of GPT-2\n(Zero-Shot Learning (ZSL)) and GPT-3 (Few-Shot Learning (FSL))\nfor a planning task.\nGiven an instruction to a household robot,\nformulate the steps to complete the\ninstruction.\nThe instruction is: ‘‘Put a washed slice\nof apple on the table.’’\nFrontiers in Robotics and AI 08 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nThe results for this task vary greatly between the two LLMs.\nGPT-2 (‘gpt-2’model,1.5Bparameters)completelyfailsthistask\nand produces an output that resembles a repetition of phrases in the\ninput sentence:\nPut a washed slice of apple on the table.\nPut a washed slice of apple on the table.\nPut a washed slice of apple on the table.\n...\nSimilar behavior has been observed for other LMs falling into\nthe ZSL category. The input sentence is not an open-ended question\nand requires reasoning.\nGPT-3 (‘text-davinci-002’model,175Bparameters),whengiven\nthe same instruction as input, is able to make assumptions and\nformulate a whole plan based on these assumptions:\nAssuming the apple is not cut and not\nwashed.\n1. Pick up the apple from the counter.\n2. Cut the apple into a slice.\n3. Wash the apple slice.\n4. Place the apple slice on the table.\nThe FSL paradigm allows GPT-3 to be very sensitive to context\nchanges and seemingly understand the request at hand. However,\nsmaller GPT-3 PLMs (GPT-3 curie, GPT-3 babbage, GPT3-ada)\nshow a degraded quality in the produced plan (Floridi and Chiriatti,\n2020). have shown that GPT-3 would not pass the Turing test, as\nto having “no understanding of the semantics and contexts of the\nrequest, but only a syntactic (statistical) capacity to associate words\n[…]”.\nThesetestshaveshownthatplangenerationcapabilitiesofLLMs\nvary dramatically depending on the underlying learning paradigm,\nmodel architecture, and parameter size. GPT-2, out of the box,\nis completely unsuited for solving planning tasks that require a\nminimum level of text understanding. However, as later (§3.5)\nshown, GPT-2 can successfully generate plans when finetuned to a\ntraining dataset (§3.2). The question of whether a finetuned GPT-\n2 model can leverage knowledge for planning is addressed in the\nfollowing section. GPT-3, unfortunately, is only accessible through\na paid service by OpenAI, and finetuning of own GPT-3 models\nis possible through the provided service. Practical applications,\nhowever, are limited because each query has to be sent to and\nprocessed by the OpenAI service. Even if a PLM was made available,\nthehardwarerequirementsforrunningGPT-3modelsareimmense,\neven for today’s standards, due to the sheer parameter count. It is for\nthese reasons that GPT-3 and its newest versions are not considered\nas a basis for finetuning to RobLM.\n4.2 Evaluation of RobLM\nThis section presents the main experiments conducted for\nevaluation of RobLM. We first define the appropriate metrics and\na baseline method required to make the evaluations measurable and\ncomparable.The grounding problemisexplainedinaccordancewith\nthe practical aspects of integrating the available methods into the\nsimulator. For the experimentation part, a set of finetuned LLMss is\ncompared with the baseline performance.\n4.2.1 Metrics\nTo validate a finetuned LM, only the NL task goal of each\nvalidation sample and optionally, the context is fed to the RobLM\ngeneration pipeline (see Figure 4). Validation is performed over\neach task category rather than all the validation data. This enables\nthe analysis of a task-dependent performance: some task categories\nare more complex than others leading to a longer trajectory of\nactions and hence an increased difficulty. Two metrics are defined\nfor validation: LM accuracy and plan success rate.\nDefinition—Accuracy. Accuracy measures how accurately the\nLM is able to predict the following parts of the plan.\n• the correct count and names of all actions in the plan (action\naccuracy)\n• the correct count and names of all arguments in the plan\n(argument accuracy)\n• the correct count and names of all actionsand arguments in the\nplan (“full plan” accuracy)\nFor a found plan, the accuracy of actions and arguments counts if\nall actions or arguments are correct. With this metric, it is possible\nto anchor the cause of plan failure to either the actions or the\narguments, or both.\nHaving an accurate LM does not necessarily mean that the\ngenerated plan leads to success—at least, as long the “full plan”\naccuracy is below 1.0, i.e., the trajectory is not replicated perfectly. A\nsecond metric is required that measures the actualsuccess rate of the\nfinetuned LM in simulation. There are two possible scenarios that\njustify this additional metric. First, the plan could fail in simulation,\neven if it seems accurate. And second, the plan could succeed in\nsimulation, even if the plan is not completely accurate.\nDefinition—Success rate . The success rate is a measure of\nthe successful completion of individual sub-tasks of a validation\ntask. After loading the trajectory, environment state, and goal from\nthe validation sample into the AI2-THOR simulator, the actions\npredicted by the LM are translated into low-level controller actions\nvia task and geometric grounding (§4.2.3), which are then passed\nto the AI2-THOR controller and executed in the simulator. After\nevery simulator step, a check is performed to determine whether the\ntargetconditionsforsub-taskcompletionhavebeenmet.Ifthetarget\nconditions are kept unsatisfied after execution of the last low-level\naction, it counts as a success towards the sub-task, or otherwise, as a\nfailure.\n4.2.2 Baseline\nA baseline is an oracle, or upper bound, that serves as a\nmeasurement reference. Fast Downward (FD) (Helmert, 2006) is\nused as the baseline for evaluation. We consider a classical task\nplannerlikeFDappropriatesinceitalsohasaccesstothefulldomain\nand is a complete algorithm (Helmert, 2006). Therefore, the ability\nof a RobLM to match or outperform FD (for a given time budget)\nwould reveal whether LMs can be helpful towards learning task\nplanning. Every ALFRED validation sample comes with a PDDL\nproblem file, while the PDDL domain is shared by all tasks; this\nallows the PDDL planner to generate a plan for each sample. To\ngenerate a plan using FD, the PDDL problem files provided by\nALFRED have to be pre-processed. FD is able to handle Action\nDescription Language (ADL) instructions, as found in the PDDL\nFrontiers in Robotics and AI 09 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nFIGURE 5\nPrediction accuracy of actions and arguments for previously unseen data across a set of tasks. Neither RobLM model is able to outperform the baseline\n(blue) but shows high accuracy in the prediction of plan actions. Context-driven models (green, red, and purple) perform better than the model\nwithout any scene-related context (orange).\nproblem, but is not able to process optimization-related additional\ninformation present in the files.\n4.2.3 Instruction grounding\nGrounding can be defined as mapping a high-level, abstract,\nor symbolic representation to a low-level grounded representation.\nGroundingofanabstractplantoobjectsiscalledobjectorgeometric\ngrounding (or “world grounding”), and grounding of NL to robot\ntasks is called task grounding. In this case, instructions generated by\nthe LM are made up of actions that require atask grounding, and\narguments, which require ageometric grounding.\n4.2.4 Task grounding\nPlans generated by RobLM consist of high-level actions and are\nnot directly executable by the AI2-THOR controller. Each possible\naction predicted by the LM has to be grounded to a task, which\nthen translates to a sequence of low-level controller actions. For\ntask grounding, three possible types of tasks are defined: navigation,\nmanipulation and composite. In a navigation task, the agent is\nrequired to move from one to another location. In a manipulation\ntask, the agent performs an action affecting the environment state.\nComposite tasks are a composition of manipulation tasks that need\nto be completed in a specific order.\nTask grounding is performed as follows.\n• The actionGotoLocation is grounded to the navigation task and\ndelegated to a trajectory planner for navigation (see below).\n• The actions PickupObject, PutObject, T oggleObject and\nSliceObject are grounded to the manipulation task, the actions\ncan be directly executed by the low-level controller.\n• The actions HeatObject, CoolObject and CleanObject are\ngrounded to the composite task, which is translated to\nthis sequence of low-level actions: T oggleObject →PutObject\n→T oggleObject→T oggleObject→PickupObject →T oggleObject\n(example given below).\n4.2.5 Geometric grounding\nAn argument can be either a location or an object name. An\nargument produced by the LM might be ambiguous or non-existing\nin the environment. In order to be understood by the controller,\nthese arguments have to be grounded on a geometric level. For\ngrounding arguments, first, all available objects are retrieved from\nthe simulation. Then, the world coordinates of all objects matching\nthe predicted symbol (target object) are gathered. E.g., if the\npredicted target object is ‘soap’, the position of all ‘soap’-type objects\ncan be queried and retrieved from the simulator. The low-level\ncontrol commands arefinally generated with the help ofthe ground-\ntruth navigation graph of the scene.\n4.2.6 Navigation\nBy overlaying the world with a grid, every position in the\nworld is given a discrete coordinate. A navigation graph (not to be\nconfused with a scene graph or Graph2NL graph) creates a node\nfor each coordinate and connects all the nodes that areaccessible\none from another. Similar to the procedure of Graph2NL (§3.6),\nthe navigation graph is traversed after locating the agent and target\nnode by the object name. A search algorithm is used to find the\nshortest path in the graph from the agent to the target object - in\nthis case, it is the A* algorithm (Duchoň et al., 2014). The search\nreturns a sequence of nodes, which corresponds to a sequence\nof coordinates (a trajectory). Lastly, a motion planner takes the\ntrajectory as an input and outputs a sequence of low-level controller\nactions (AI2-THOR conveniently provides a motion planner for\nnavigation).\nFrontiers in Robotics and AI 10 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\n4.2.7 Experimental results\nA set of finetuned RobLM models are evaluated against the\nbaseline. The finetuned models differ in the amount of context\nprovided during training time.\n1) ‘No context’ — Only task goal\n2) ‘Scene knowledge’ — List of all available objects in the\nenvironment, found in the PDDL problem\n3) ‘Scene graph’ — Description of geometric relations to the target\nobject, generated by Graph2NL\n4) ‘Full context’ — Description of geometric relations of all objects,\ngenerated by Graph2NL\nGiven a PDDL problem file, Graph2NL automatically generates the\ncontext in the specified text format. This context is provided to the\nLM for training and inference.\nAccuracy. Figure 5 summarizes the evaluation of the finetuned\nRobLM models compared to the FD baseline for previously unseen\n(validation) data. It can be observed that none of the finetuned\nRobLM models is able to outperform the baseline. Going through\neach of the models and starting with the ‘No context’ model\nit is surprising that this model, even without any contextual\ninformation, is able to generate the correct plan actions with high\naccuracy. The ‘Scene knowledge’ and ‘Scene graph’ models have a\nsimilar performance, the ‘Scene graph’ generally being slightly more\naccurate in both actions and argument prediction. Both of these\nmodelsoveralloutperformthe‘Nocontext’model,withasignificant\nimprovement of the context models in the arguments prediction.\nGiven these results, the following conclusions about the\nexamined models can be made.\n1) Failed plans are mostly caused by wrong arguments (objects or\nlocations) and only in some cases by wrong actions.\n2) The LM is able to learn the structure of tasks, but not scene-\ndependent components.\nRobLM is able to distinguish between the task categories and\nprovide a correct task action plan. However, where this model fails\nis in finding all correct action arguments, i.e., locations and object\nnames. This can be explained by the fact that the task goal alone\ndoes not reveal the actual location of the target object. Because the\ntarget can be in any accessible location in the environment, or in any\naccessible receptacle, the produced argument is the result of the LM\nimitating themost-likely cases observed in the training data.\nOverall, these results are consistent with the point made on\ncontextualinformationandpredictionaccuracy:givingtothemodel\ninformation about the environment, i.e., finetuning a model to be\ngrounded to the scene,does improve performance.\nSuccess rate. A plan is successful if each of the sub-tasks for\na stated task is completed.Table 2 summarizes the success rate of\nRobLM compared to the baseline for actions of the navigation task\n(GotoLocation) and manipulation task ( PickupObject, PutObject,\netc.). Composite tasks have been omitted from this evaluation\nbecause of high failure rates caused by their task grounding\ncomplexity.\nRegarding geometric grounding, arguments predicted by\nRobLM are grounded to all matching objects in the world, and\nRobLM is allowed to “try” all possibilities. E.g., when the objective\nis to “Get soap”, multiple ‘soap’-type objects could exist in the scene.\nEach possibility given by the geometric grounding is simulated\nTABLE 2 Success rates of sub-task completion in simulation—RobLM (‘No\ncontext’) compared to the baseline on seen and unseen validation data.\nSuccess rate Baseline RobLM (‘no context’)\nTask seen unseen seen unseen\nGotoLocation 0.318 0.393 0.422 0.499\nPickupObject 0.466 0.474 0.776 0.749\nPutObject 0.385 0.331 0.116 0.092\nSliceObject 0.629 0.5 0.94 0.98\nToggleObject 0 0 0.84 0.864\nBold represent maximum values.\nTABLE 3 Top-k and top-p sampling ( k =10 and p = 0.9) — tokens are sampled\nthree times for the ‘Pick Simple’ task, giving only slight deviations in the final\naccuracy.\nRobLM ‘no context’ model, ‘pick simple’ task\nAccuracy of 1st sample 2nd sample 3rd sample\nActions 0.7746 0.8169 0.7817\nArguments 0.3803 0.4085 0.3944\nGotoLocation 0.8772 0.9123 0.8904\nPickupObject 0.8380 0.8662 0.8451\nPutObject 0.7971 0.8227 0.7986\nGotoLocation_Args 0.5658 0.5877 0.5833\nPickupObject_Args 0.7535 0.8028 0.7746\nPutObject_Args 0.6449 0.6667 0.6331\nby storing and restoring the simulator state. While this is a clear\nadvantage for RobLM over the baseline, the evaluation still holds\nbecause the LM is required to predict the correct location or\nobject names. Based on the presented results, the LM-based system\nperforms well on sub-tasks requiring the actionPickupObject, while\nthe actionPutObject does not succeed equally well, being far from\nthe baseline performance.\nOverall, the success rate of the baseline method is not\nnearly as high as expected, hinting at potential implementation-\nspecific failures in the task grounding and in the low-level\ncontroller interaction with objects. In the low-level controller,\nvisual information is not included. This means that the robot is\ncontrolled in a “blind flight” mode. The AI2-THOR simulation\nrequires the target object to be inview. If the object is not visible,\ne.g., because the agent is looking in the wrong direction, the\ninteraction fails and with it, the sub-task. Because of the fact that\nboth systems have been evaluated within the same framework,\nthese results do not dismiss a potential use-case for LM in\nplanning.\n4.2.8 Additional results\nWe provide additional experiments for a deeper analysis of\npotential points of failure of RobLM. These experiments entail a\ndifferent sampling strategy and context refinement.\nFrontiers in Robotics and AI 11 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nFIGURE 6\nPrediction accuracy of actions and arguments for unseen tasks of RobLM with a refined context. This experiment compares a model finetuned to the\ntask goal and a context consisting of a NL description of the first plan instruction (green) with the baseline (blue) and a RobLM with ‘No context’ model\n(orange).\n4.2.8.1 T op-k/top-p sampling\nSo far, every experiment conducted has used a greedy next-\ntoken selection strategy. In order to be able to tell with certainty\nthat a found plan is the “best possible” plan, a comparison with\nanother sampling strategy is required. This additional experiment\nrepeats the previous one, but this time with a top-k and top-p\nsampling strategy. The comparison is done with the ‘No context’\nRobLM model for all tasks withk = 10 and p = 0.9, i.e., tokens are\nsampled from the top-10 predictions and sum up to a probability\n≥0.9. Since a similar pattern was observed in the individual task\nevaluations,Table 3reportstheresultsforthe‘PickSimple’taskonly.\nEach token is sampled three times, giving three possible solutions\nto be evaluated. Slight—uniform and hence dismissable—variations\nin the prediction accuracy exist between these three runs. The\nsampling-based method performs slightly worse than the greedy\nstrategy.\n4.2.8.2 Refined context\nIn a deeper analysis of RobLM failure cases, it has been\nfound that the first argument in the generated plan is the\nhardest to predict correctly by the LM. The LM is not able to\ndraw enough conclusions about the first instruction from the\nsupplied context of any form. This causality becomes obvious\nafter the following experiment: Given the task goal and a NL\ndescription of the first instruction as context, how does the overall\naccuracy of the LM change? The following text is an example\nof an instruction description in NL, as found in the ALFRED\ndataset:\nTurn left and walk across the room towards\nthe shelves on the wall.\nThe results inFigure 6 show that, given this extra information,\nRobLMisalmostabletoreachtheperformancelevelsofthebaseline\nmeasurement across all tasks; it shows very high accuracy on “full\nplan” actions and arguments. The conclusion of this experiment is\nthat the more precisely the supplied context is tailored towards the\nkey issue of LM generation task, the more accurate the generated\nplan becomes. For this specific problem, finding the correct first\nargumentiskeytoasuccessfulplan,andwithaNLdescriptionofthe\nfirst instruction, the LM is able to draw the necessary connections\nfrom context to plan.\nThe overall conclusion of this observation is that the LM are\nadaptive; the LM is able to adapt new information into the plan\ngeneration, towards a more accurate sequence of instructions.\n4.3 Run-time analysis\nInference frequency is an important factor when it comes to\nreal-life applications. This is especially true for industrial robotics,\nwhere cycle times are important. But not every robotic application\nis time-critical, e.g., a household robot is not expected to respond\nin a sub-second time. However, if task planning is seen as a\nprogramming problem, a fast execution time greatly enhances the\noperator experience Table 4 shows a comparison of the inference\nspeeds of RobLM against the baseline (FD). RobLM, in all cases,\nis slower compared to the baseline, which is likely due to the\nreliance on the full GPT-2 vocabulary size for the LM tokenizer\nand the usage of a LM-internal, implementation-specific generation\nfunction4. Such an issue can be mitigated by training a new\n4 Huggingface generation function: ‘transformers/src/transformers/generation_\nutils.py’.\nFrontiers in Robotics and AI 12 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nTABLE 4 Comparison of inference speeds—RobLM against baseline. GPU\nacceleration used for LM (NVIDIA R⃝GeForce RTX 2080 SUPER). The timer\nstarts only after the program or model has been loaded into memory, i.e.,\nonly computation (inference) time is measured. “No context” has a maximum\ntoken sequence length of 200 and “Full context” has a maximum length of\n1,024 tokens for generation.\nIterations per second (average over 800 samples)\nBaseline RobLM ‘No context’ RobLM ‘Full context’\n2.9 1.0 0.2\ntokenizer on the task-specific vocabulary, but this comes at the\ncost of not utilizing the stored knowledge in the PLM. However,\ncurrent progress in language models allows faster inferences\nin more advanced hardware than the one used in this work;\ntherefore, we believe that the frequency limitations can be easily\novercome.\nRemarks. Overall, our analysis has shown that finetuning\nPLMs toward robotic task planning is possible when providing\nan appropriate grounding context. However, we have shown that\nsuch models cannot yet reach the planning abilities of classical\ntask planners. A combination of finetuning with proper scene\nrepresentation and a more elaborate sampling strategy, as well\nas the addition of more sophisticated prompts, can boost the\nperformance of RobLM, leading them to performances that are\ncloser to the oracle task planners. Still, the benefit of providing goal\nspecifications as natural language commands alleviate the burden\nof engineering, while advances in scene graph generation can make\nthe extraction of domain specifications autonomous. Therefore, we\nbelieve that using RobLMs at a higher level of abstraction for\nneuro-symbolic task planning is valuable but is still in its infancy.\nAdditional challenges have been recently summarized by Weng\n(2023), where some of the listed points are in accordance with our\nfindings.\n5 Conclusion\nWe presented a framework for finetuning grounded Large\nLanguage Models (LLMs) and investigated the applicability of\nsuch models combined with planning in solving ling-horizon\nrobot reasoning tasks. This paper has shown that LLMs can\nextract commonsense knowledge through precise queries and\nadjust their behavior based on available information or context.\nAmong our contributions are the development of RobLM, a\ngrounded finetuned LLM that generates plans directly from\nnatural language commands, and Graph2NL, which creates natural\nlanguagetextdescribinggraph-baseddata,torepresentscenegraphs\nas inputs into RobLM. Our extensive experimental results have\nrevealed, nevertheless, the challenges in representing structured\nand geometric data in natural language. However, LLMs still\nneed to demonstrate a consistent ability to perform long-horizon\nplanning tasks and cannot yet replace classical planners. Despite\ntheir limitations, LLMs possess powerful features such as efficient\nstorage and retrieval of commonsense knowledge, which can be\nuseful in planning tasks when presented with partially observable\nenvironments.\nFor future work, exploring larger models like GPT-3 or GPT-\nNeoX could increase the accuracy and success rate of RobLM.\nProviding structured context to the Transformer model and\nexploring multi-modal inputs, such as visual information, may\nalso improve the planning capabilities of LLMs. Further research\nin the field of applied natural language processing in robotics\ncould help unlock the full potential of LLMs and contribute\nto the development of more advanced neuro-symbolic planning\nsystems.\nData availability statement\nThe datasets presented in this study can be found in online\nrepositories. The names of the repository/repositories and accession\nnumber(s) can be found below: https://github.com/dnandha/\nRobLM.\nAuthor contributions\nGC, AY, and DN contributed to the conception and design\nof the method. AL assisted in the setup of the baseline method.\nLR provided insights for the training and fine-tuning of language\nmodels and advice on the linearization of the graph. IG gave advice\non the overall method and the idea of using language models\nfor planning. GC wrote the current version of the manuscript,\nbuilding on the initial write-up by DN. AY assisted in writing and\nvisualizations. All authors contributed to the article and approved\nthe submitted version.\nFunding\nThis research has been supported by the German Research\nFoundation (DFG) through the Emmy Noether Programme (CH\n2676/1-1) and by the Hessian. AI Connectom Fund “Robot\nLearning of Long-Horizon Manipulation bridging Object-centric\nRepresentations to Knowledge Graphs”. We acknowledge support\nby the Deutsche Forschungsgemeinschaft (DFG, German Research\nFoundation) and the Open Access Publishing Fund of Technical\nUniversity of Darmstadt.\nAcknowledgments\nThe authors would like to acknowledge Snehal Jauhri and Haau-\nSing Li for the fruitful discussions and suggestions.\nConflict of interest\nAuthor LR was employed by Amazon Alexa.\nThe remaining authors declare that the research was conducted\nin the absence of any commercial or financial relationships that\ncould be construed as a potential conflict of interest.\nFrontiers in Robotics and AI 13 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their affiliated\norganizations, or those of the publisher, the editors and the\nreviewers.Anyproductthatmaybeevaluatedinthisarticle,orclaim\nthatmaybemadebyitsmanufacturer,isnotguaranteedorendorsed\nby the publisher.\nReferences\nAeronautiques, C., Howe, A., Knoblock, C., McDermott, I. D., Ram, A., Veloso,\nM., et al. (1998). Pddl– the planning domain definition language. T ech. Rep. T ech.\nRep.\nBengio, Y., Ducharme, R., and Vincent, P. (2000). A neural probabilistic language\nmodel. Adv. neural Inf. Process. Syst.13.\nBian, N., Han, X., Sun, L., Lin, H., Lu, Y., and He, B. (2023).\nChatgpt is a knowledgeable but inexperienced solver: an investigation\nof commonsense problem in large language models. Available at:\nhttps://arxiv.org/abs/2303.16421.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., et al.\n(2021). On the opportunities and risks of foundation models. Available at: https://\narxiv.org/abs/2108.07258.\nBrohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho, D., et al. (2022). Do\nas i can, not as i say: grounding language in robotic affordances. Available at:https://\narxiv.org/abs/2204.01691.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., et al.\n(2020). Language models are few-shot learners. Available at:https://arxiv.org/abs/2005.\n14165.\nChen, B., Xia, F., Ichter, B., Rao, K., Gopalakrishnan, K., Ryoo, M. S., et al. (2022a).\nOpen-vocabulary queryable scene representations for real world planning. Available at:\nhttps://arxiv.org/abs/2209.09874.\nChen, W., Hu, S., Talak, R., and Carlone, L. (2022b). Leveraging large language\nmodels for robot 3d scene understanding. Available at: https://arxiv.org/abs/2209.\n05629.\nDriess, D., Ha, J.-S., and Toussaint, M. (2020). Deep visual reasoning: learning to\npredict action sequences for task and motion planning from an initial scene image.\nAvailable at:https://arxiv.org/abs/2006.05398.\nDriess, D., and Toussaint, M. (2019).Hierarchical task and motion planning using\nlogic-geometric programming (hlgp).\nDuchoň, F., Babinec, A., Kajan, M., Beňo, P., Florek, M., Fico, T., et al. (2014). Path\nplanning with modified a star algorithm for a mobile robot.Procedia Eng. 96, 59–69.\ndoi:10.1016/j.proeng.2014.12.098\nFloridi,L.,andChiriatti,M.(2020).Gpt-3:itsnature,scope,limits,andconsequences.\nMinds Mach. 30, 681–694. doi:10.1007/s11023-020-09548-1\nFunk, N., Chalvatzaki, G., Belousov, B., and Peters, J. (2021). Learn2assemble with\nstructured representations and search for robotic architectural construction. Conf.\nRobot Learn. (CoRL).\nFunk, N., Menzenbach, S., Chalvatzaki, G., and Peters, J. (2022). Graph-\nbased reinforcement learning meets mixed integer programs: an application\nto 3d robot assembly discovery. Available at: https://arxiv.org/abs/2203.\n04120.\nGarrett, C. R., Chitnis, R., Holladay, R., Kim, B., Silver, T., Kaelbling, L. P., et al.\n(2021). Integrated task and motion planning. Available at:https://arxiv.org/abs/2010.\n01083.\nGarrett, C. R., Lozano-Pérez, T., and Kaelbling, L. P. (2020). Pddlstream: integrating\nsymbolic planners and blackbox samplers via optimistic adaptive planning. Available\nat: https://arxiv.org/abs/1802.08705.\nHelmert, M. (2006). The fast downward planning system. J. Artif. Intell. Res. 26,\n191–246. doi:10.1613/jair.1705\nHoang, C., Sohn, S., Choi, J., Carvalho, W., and Lee, H. (2021). Successor feature\nlandmarks for long-horizon goal-conditioned reinforcement learning.Adv. Neural Inf.\nProcess. Syst.34, 26963–26975.\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. (2020). The curious case of\nneural text degeneration. Available at:https://arxiv.org/abs/1904.09751.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo,\nA., et al. (2019). Parameter-efficient transfer learning for nlp. Available at: https://\narxiv.org/abs/1902.00751.\nHuang, C., Mees, O., Zeng, A., and Burgard, W. (2022a). Visual language maps for\nrobot navigation. Available at:https://arxiv.org/abs/2210.05714.\nHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. (2022b). Language models as\nzero-shot planners: extracting actionable knowledge for embodied agents. Available at:\nhttps://arxiv.org/abs/2201.07207.\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., et al. (2022c). Inner\nmonologue: embodied reasoning through planning with language models. Available at:\nhttps://arxiv.org/abs/2207.05608.\nJiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., et al. (2022). Vima:\ngeneral robot manipulation with multimodal prompts. In Proceedings of the NeurIPS\nFoundation Models for Decision Making Workshop, New Orleans, USA, December\n2022\nKaelbling, L. P., and Lozano-Pérez, T. (2011). Hierarchical task and motion planning\nin the now. In Proceedings of the 2011 IEEE International Conference on Robotics and\nAutomation, 1470–1477. Shanghai, China, May 2011\nKim, B., Wang, Z., Kaelbling, L. P., and Lozano-Pérez, T. (2019). Learning to\nguide task and motion planning using score-space representation.IJRR38, 793–812.\ndoi:10.1177/0278364919848837\nKingma,D.P.,andBa,J.(2014).Adam:Amethodforstochasticoptimization. https://\narxiv.org/abs/1412.6980.\nKolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., et al. (2017).\nAi2-thor: an interactive 3d environment for visual ai. Available at:https://arxiv.org/abs/\n1712.05474.\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning.Nature521, 436–444.\ndoi:10.1038/nature14539\nLi,S.,Puig,X.,Paxton,C.,Du,Y.,Wang,C.,Fan,L.,etal.(2022).Pre-trainedlanguage\nmodels for interactive decision-making. Adv. Neural Inf. Process. Syst. 35, 31199–\n31212.\nLi, X. L., Kuncoro, A., d’Autume, C. d. M., Blunsom, P., and Nematzadeh, A. (2021).\nDolanguagemodelslearncommonsenseknowledge?Availableat: https://arxiv.org/abs/\n2111.00607.\nLiang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., et al. (2022). Code as\npolicies:languagemodelprogramsforembodiedcontrol.Availableat: https://arxiv.org/\nabs/2209.07753.\nMees, O., Borja-Diaz, J., and Burgard, W. (2022). Grounding language with\nvisual affordances over unstructured data. Available at: https://arxiv.org/abs/2210.\n01911.\nNair, S., and Finn, C. (2019). Hierarchical foresight: self-supervised learning of\nlong-horizon tasks via visual subgoal generation. Available at: https://arxiv.org/abs/\n1909.05829.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., et al.\n(2022). Training language models to follow instructions with human feedback.https://\narxiv.org/abs/2203.02155.\nPashevich, A., Schmid, C., and Sun, C. (2021). Episodic transformer for vision-and-\nlanguage navigation. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision. Montreal, BC, Canada, October 2021, 15942–15952.\nPfeiffer, J., Kamath, A., Rücklé, A., Cho, K., and Gurevych, I. (2021). Adapterfusion:\nnon-destructive task composition for transfer learning. Available at:https://arxiv.org/\nabs/2005.00247.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., et al.\n(2021). “Learning transferable visual models from natural language supervision,” in\nInternational conference on machine learning (PMLR), 8748–8763.\nRaman, S. S., Cohen, V., Rosen, E., Idrees, I., Paulius, D., and Tellex, S. (2022).\nPlanning with largelanguage models via correctivere-prompting.https://arxiv.org/abs/\n2211.09935.\nRen, T., Chalvatzaki, G., and Peters, J. (2021). Extended task and motion\nplanning of long-horizon robot manipulation. https://arxiv.org/abs/2103.\n05456.\nRuis, L., Khan, A., Biderman, S., Hooker, S., Rocktäschel, T., and Grefenstette, E.\n(2022). Large language models are not zero-shot communicators. Available at:https://\narxiv.org/abs/2210.14986.\nShah, D., Osiński, B., and Levine, S. (2023). “Lm-nav: robotic navigation with large\npre-trained models of language, vision, and action,” in Conference on robot learning\n(PMLR), 492–504.\nShridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., et al.\n(2020). Alfred: A benchmark for interpreting grounded instructions for everyday tasks.\nAvailable at:https://arxiv.org/abs/1912.01734.\nFrontiers in Robotics and AI 14 frontiersin.org\nChalvatzaki et al. 10.3389/frobt.2023.1221739\nSingh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., et al. (2022).\nProgprompt:generatingsituatedrobottaskplansusinglargelanguagemodels.Available\nat: https://arxiv.org/abs/2209.11302.\nTay, Y., Wei, J., Chung, H. W., Tran, V. Q., So, D. R., Shakeri, S., et al. (2022).\nTranscending scaling laws with 0.1% extra compute. Available at:https://arxiv.org/abs/\n2210.11399.\nToussaint, M. (2015). Logic-geometric programming: an optimization-based\napproach to combined task and motion planning.Proceedings of the Twenty-Fourth\nInternational Joint Conference on Artificial Intelligence (IJCAI 2015), Buenos Aires,\nArgentina, July 2015.\nValmeekam, K., Olmo, A., Sreedharan, S., and Kambhampati, S. (2022). Large\nlanguage models still can’t plan (a benchmark for llms on planning and reasoning about\nchange). Available at:https://arxiv.org/abs/2206.10498.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Available at:https://arxiv.org/abs/1706.03762.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., et al. (2019).\nSuperglue: A stickier benchmark for general-purpose language understanding systems.\nAdv. neural Inf. Process. Syst.32.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). Glue:\nA multi-task benchmark and analysis platform for natural language understanding.\nAvailable at:https://arxiv.org/abs/1804.07461.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., et al. (2022a).\nEmergent abilities of large language models. Available at: https://arxiv.org/abs/\n2206.07682.\nWei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,Xia,F.,Chi,E.H.,etal.(2022b).Chain-\nof-thought prompting elicits reasoning in large language models. Available at:https://\narxiv.org/abs/2201.11903.\nWells, A. M., Dantam, N. T., Shrivastava, A., and Kavraki,\nL. E. (2019). Learning feasibility for task and motion planning\nin tabletop environments. IEEE Ral. 4, 1255–1262. doi: 10.1109/\nlra.2019.2894861\nWeng, L. (2023). Ll-powered autonomous agents.lilianweng.github.Io.\nWhite, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., et al. (2023). A prompt\npattern catalog to enhance prompt engineering with chatgpt. Available at: https://\narxiv.org/abs/2302.11382.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., et al. (2019). Huggingface’s transformers: state-of-the-art\nnatural language processing. Available at: https://arxiv.org/abs/1910.\n03771.\nXu, L., Ren, T., Chalvatzaki, G., and Peters, J. (2022). Accelerating integrated task\nand motion planning with neural feasibility checking. Available at:https://arxiv.org/\nabs/2203.10568.\nZeng, A., Wong, A., Welker, S., Choromanski, K., Tombari, F., Purohit, A., et al.\n(2022). Socratic models: composing zero-shot multimodal reasoning with language.\nAvailable at:https://arxiv.org/abs/2204.00598.\nZhou, X., Zhang, Y., Cui, L., and Huang, D. (2020). Evaluating commonsense\nin pre-trained language models. Proc. AAAI Conf. Artif. Intell. 34, 9733–9740.\ndoi:10.1609/aaai.v34i05.6523\nFrontiers in Robotics and AI 15 frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8186506032943726
    },
    {
      "name": "Executable",
      "score": 0.7632623910903931
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6392496824264526
    },
    {
      "name": "Planner",
      "score": 0.5956181883811951
    },
    {
      "name": "Robot",
      "score": 0.580477237701416
    },
    {
      "name": "Task (project management)",
      "score": 0.5739197731018066
    },
    {
      "name": "Automated planning and scheduling",
      "score": 0.5598106384277344
    },
    {
      "name": "Human–computer interaction",
      "score": 0.499392032623291
    },
    {
      "name": "Robotics",
      "score": 0.4846256673336029
    },
    {
      "name": "Machine learning",
      "score": 0.4354243874549866
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.41537538170814514
    },
    {
      "name": "Programming language",
      "score": 0.13268700242042542
    },
    {
      "name": "Systems engineering",
      "score": 0.12127599120140076
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}