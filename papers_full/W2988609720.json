{
    "title": "Zero-Shot Paraphrase Generation with Multilingual Language Models",
    "url": "https://openalex.org/W2988609720",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4283225341",
            "name": "Guo, Yinpeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101329899",
            "name": "Liao Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2030371270",
            "name": "Jiang Xin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1269586548",
            "name": "Zhang Qing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1440858485",
            "name": "Zhang Yi-bo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1952241286",
            "name": "Liu Qun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2921280978",
        "https://openalex.org/W2419539795",
        "https://openalex.org/W2804232614",
        "https://openalex.org/W2123678043",
        "https://openalex.org/W1787338159",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2962768052",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W2531908596",
        "https://openalex.org/W2963558220",
        "https://openalex.org/W2964212550",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W2890731353",
        "https://openalex.org/W11511616",
        "https://openalex.org/W174630521",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962953307",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2741049976",
        "https://openalex.org/W2949832505",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963903950",
        "https://openalex.org/W2025768430",
        "https://openalex.org/W2402144811",
        "https://openalex.org/W2517773427",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2952153923",
        "https://openalex.org/W2963456134",
        "https://openalex.org/W2963463583"
    ],
    "abstract": "Leveraging multilingual parallel texts to automatically generate paraphrases has drawn much attention as size of high-quality paraphrase corpus is limited. Round-trip translation, also known as the pivoting method, is a typical approach to this end. However, we notice that the pivoting process involves multiple machine translation models and is likely to incur semantic drift during the two-step translations. In this paper, inspired by the Transformer-based language models, we propose a simple and unified paraphrasing model, which is purely trained on multilingual parallel data and can conduct zero-shot paraphrase generation in one step. Compared with the pivoting approach, paraphrases generated by our model is more semantically similar to the input sentence. Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences. In addition, we introduce the mechanism of denoising auto-encoder (DAE) to improve diversity and robustness of the model. Experimental results show that our model surpasses the pivoting method in terms of relevance, diversity, fluency and efficiency.",
    "full_text": "Zero-Shot Paraphrase Generation with Multilingual Language Models\nYinpeng Guo1, Yi Liao1, Xin Jiang1, Qing Zhang2, Yibo Zhang2, Qun Liu1\n1Huawei Noah’s Ark Lab\n2Intelligence Engineering Department, Huawei Consumer Business Group\n{guo.yinpeng, liao.yi, jiang.xin, zhangqing49,\nyibo.cheung, qun.liu}@huawei.com\nAbstract\nLeveraging multilingual parallel texts to au-\ntomatically generate paraphrases has drawn\nmuch attention as size of high-quality para-\nphrase corpus is limited. Round-trip transla-\ntion, also known as the pivoting method, is\na typical approach to this end. However, we\nnotice that the pivoting process involves mul-\ntiple machine translation models and is likely\nto incur semantic drift during the two-step\ntranslations. In this paper, inspired by the\nTransformer-based language models, we pro-\npose a simple and uniﬁed paraphrasing model,\nwhich is purely trained on multilingual paral-\nlel data and can conduct zero-shot paraphrase\ngeneration in one step. Compared with the piv-\noting approach, paraphrases generated by our\nmodel is more semantically similar to the input\nsentence. Moreover, since our model shares\nthe same architecture as GPT (Radford and\nSutskever, 2018), we are able to pre-train the\nmodel on large-scale unparallel corpus, which\nfurther improves the ﬂuency of the output sen-\ntences. In addition, we introduce the mecha-\nnism of denoising auto-encoder (DAE) to im-\nprove diversity and robustness of the model.\nExperimental results show that our model sur-\npasses the pivoting method in terms of rele-\nvance, diversity, ﬂuency and efﬁciency.\n1 Introduction\nParaphrasing is to express the same meaning us-\ning different expressions. Paraphrase generation\nplays an important role in various natural language\nprocessing (NLP) tasks such as response diversi-\nﬁcation in dialogue system, query reformulation\nin information retrieval, and data augmentation\nin machine translation. Recently, models based\non Seq2Seq learning (Ilya Sutskever, 2014) have\nachieved the state-of-the-art results on paraphrase\ngeneration. Most of these models (Prakash et al .,\n2016; Ziqiang Cao, 2017; Ankush Gupta, 2018;\nFigure 1: Paraphrase generation via round-trip transla-\ntion.\nZichao Li, 2018, 2019) focus on training the para-\nphrasing models based on a paraphrase corpus,\nwhich contains a number of pairs of paraphrases.\nHowever, high-quality paraphrases are usually dif-\nﬁcult to acquire in practice, which becomes the\nmajor limitation of these methods. Therefore, we\nfocus on zero-shot paraphrase generationapproach\nin this paper, which aims to generate paraphrases\nwithout requiring a paraphrase corpus.\nA natural choice is to leverage the bilingual or\nmultilingual parallel data used in machine transla-\ntion, which are of great quantity and quality. The\nbasic assumption is that if two sentences in one\nlanguage (e.g., English) have the same translation\nin another language (e.g., French), they are as-\nsumed to have the same meaning, i.e., they are\nparaphrases of each other. Therefore, one typical\nsolution for paraphrasing in one language is topivot\nover a translation in another language. Speciﬁcally,\nit is implemented as the round-trip translation,\nwhere the input sentence is translated into a for-\neign sentence, then back-translated into a sentence\nin the same language as input (Jonathan Mallinson\nand Lapata, 2017). The process is shown in Fig-\nure 1. Apparently, two machine translation sys-\ntems (English→French and French←English) are\nneeded to conduct the generation of a paraphrase.\nAlthough the pivoting approach works in gen-\neral, there are several intrinsic defects. First,\narXiv:1911.03597v1  [cs.CL]  9 Nov 2019\nthe round-trip system can hardly explore all the\npaths of paraphrasing, since it is pivoted through\nthe ﬁnite intermedia outputs of a translation sys-\ntem. More formally, let Z denote the meaning\nrepresentation of a sentence X, and ﬁnding para-\nphrases of X can be treated as sampling another\nsentence Y conditioning on the representation Z.\nIdeally, paraphrases should be generated by follow-\ning P(Y|X) =\n∫\nZ P(Y|Z)P(Z|X)dZ, which is\nmarginalized over all possible values of Z. How-\never, in the round-trip translation, only one or sev-\neral Zs are sampled from the machine translation\nsystem P(Z|X), which can lead to an inaccurate\napproximation of the whole distribution and is\nprone to the problem of semantic drift due to the\nsampling variances. Second, the results are deter-\nmined by the pre-existing translation systems, and\nit is difﬁcult to optimize the pipeline end-to-end.\nLast, the system is not efﬁcient especially at the\ninference stage, because it needs two rounds of\ntranslation decoding.\nTo address these issues, we propose a single-step\nzero-shot paraphrase generation model, which can\nbe trained on machine translation corpora in an\nend-to-end fashion. Unlike the pivoting approach,\nour proposed model does not involve explicit trans-\nlation between multiple languages. Instead, it di-\nrectly learns the paraphrasing distribution P(Y|X)\nfrom the parallel data sampled from P(Z|X) and\nP(Y|Z). Speciﬁcally, we build a Transformer-\nbased (Ashish Vaswani, 2017) language model,\nwhich is trained on the concatenated bilingual par-\nallel sentences with language indicators. At infer-\nence stage, given a input sentence in a particular\nlanguage, the model is guided to generate sentences\nin the same language, which are deemed as para-\nphrases of the input. Our model is simple and\ncompact, and can empirically reduce the risk of\nsemantic drift to a large extent. Moreover, we can\ninitialize our model with generative pre-training\n(GPT) (Radford and Sutskever, 2018) on mono-\nlingual data, which can beneﬁt the generation in\nlow-resource languages. Finally, we borrow the\nidea of denoising auto-encoder (DAE) to further\nenhance robustness in paraphrase generation.\nWe conduct experiments on zero-shot paraphrase\ngeneration task, and ﬁnd that the proposed model\nsigniﬁcantly outperforms the pivoting approach in\nterms of both automatic and human evaluations.\nMeanwhile, the training and inference cost are\nlargely reduced compared to the pivot-based meth-\nods which involves multiple systems.\n2 Methodology\n2.1 Transformer-based Language Model\nTransformer-based language model (TLM) is a\nneural language model constructed with a stack\nof Transformer decoder layers (Ashish Vaswani,\n2017). Given a sequence of tokens, TLM is trained\nwith maximizing the likelihood:\nL(X) =\nn∑\ni=1\nlog P(xi|x1,...,i−1; θ) (1)\nwhere X = [x1,x2,...,x n] is a sentence in a lan-\nguage (e.g., English), and θdenotes the parameters\nof the model. Each Transformer layer is composed\nof multi-head self-attention, layer normalization\nand a feed-forward network. We refer reader to\nthe original paper for details of each component.\nFormally, the decoding probability is given by\n[e1,...,e i−1] = [Wex1 + p1,...,W exi−1 + pi−1],\n[h1,...,h i−1] = Transformer([e1,...,e i−1]),\nP(xi|x1,...,i−1; θ) = Softmax(Wohi−1), (2)\nwhere xi denotes the token embedding, pi denote\nthe positional embedding and hi denotes the output\nstates of the i-th token, and We and Wo are the\ninput and output embedding matrices.\nAlthough TLM is normally employed to model\nmonolingual sequences, there is no barrier to utilize\nTLM to model sequences in multiple languages. In\nthis paper, inspired by Lample and Conneau (2019),\nwe concatenate pairs of sentences from bilingual\nparallel corpora (e.g., English→French) as training\ninstances to the model. Let X and Y denote the\nparallel sentences in two different languages, the\ntraining objective becomes\nL(X,Y) =\nn∑\ni=1\nlog P(xi|x1,...,i−1; θ)\n+\nm∑\nj=1\nlog P(yj|x1,...,n,y1,...,j−1; θ). (3)\nThis bilingual language model can be regarded as\nthe decoder-only model compared to the traditional\nencoder-decoder model. It has been proved to work\neffectively on monolingual text-to-text generation\ntasks such as summarization (Peter J. Liu, 2018).\nThe advantages of such architecture include less\n(a) Multilingual Language Model Training.\n (b) Zero-Shot Paraphrase Generation.\nFigure 2: Paraphrase generation via multilingual language model training.\nmodel parameters, easier optimization and poten-\ntial better performance for longer sequences. Fur-\nthermore, it naturally integrates with language mod-\nels pre-training on monolingual corpus.\nFor each input sequence of concatenated sen-\ntences, we add special tokens ⟨bos⟩and ⟨eos⟩at\nthe beginning and the end, and ⟨delim⟩in between\nthe sentences. Moreover, at the beginning of each\nsentence, we add a special token as its language\nidentiﬁer, for instance, ⟨en⟩for English, ⟨fr⟩for\nFrench. One example of English→French training\nsequence is “⟨bos⟩⟨en⟩cat sat on the mat ⟨delim⟩\n⟨fr⟩chat assis sur le tapis ⟨eos⟩”.\nAt inference stage, the model predicts the next\nword as the conventional auto-regressive model:\nˆyj ∼P(yj|x1,...,n,y1,...,j−1; θ) (4)\n2.2 Zero-shot Paraphrase Generation\nWe train the bilingual language model on multiple\nbilingual corpora, for example, English ↔French\nand German↔Chinese. Once the language model\nhas been trained, we can conduct zero-shot para-\nphrase generation based on the model. Speciﬁcally,\ngiven an input sentence that is fed into the language\nmodel, we set the output language identiﬁer the\nsame as input, and then simply conduct decoding\nto generate paraphrases of the input sentence.\nFigure 2 illustrates the training and decoding\nprocess of our model. In the training stage, the\nmodel is trained to sequentially generate the input\nsentence and its translation in a speciﬁc language.\nTraining is conducted in the way of teacher-forcing.\nIn the decoding stage, after an English sentence\n“⟨bos⟩⟨en⟩cat sat on the mat ⟨delim⟩” is fed to\nthe model, we intentionally set the output language\nidentiﬁer as “⟨en⟩”, in order to guide the model to\ncontinue to generate English words. At the same\ntime, since the model has been trained on transla-\ntion corpus, it implicitly learns to keep the semantic\nmeaning of the output sentence the same as the in-\nput. Accordingly, the model will probably generate\nthe paraphrases of the input sentence, such as “the\ncat sitting on the carpet ⟨eos⟩”.\nIt should be noted our model can obviously be\ntrained on parallel paraphrase data without any\nmodiﬁcation. But in this paper, we will mainly\nfocus on the research and evaluation in the zero-\nshot learning setting.\nIn the preliminary experiments of zero-shot para-\nphrasing, we ﬁnd the model does not perform con-\nsistently well and sometimes fails to generate the\nwords in the correct language as indicated by the\nlanguage identiﬁer. Similar phenomenon has been\nobserved in the research of zero-shot neural ma-\nchine translation (Sestorain et al., 2018; Arivazha-\ngan et al., 2019; Jiatao Gu, 2019), which is referred\nas the degeneracy problem by Jiatao Gu (2019). To\naddress these problems in zero-shot paraphrase gen-\neration, we propose several techniques to improve\nthe quality and diversity of the model as follows.\n2.2.1 Language Embeddings\nThe language identiﬁer prior to the sentence does\nnot always guarantee the language of the sequences\ngenerated by the model. In order to keep the lan-\nguage consistency, we introduce language embed-\ndings, where each language is assigned a speciﬁc\nvector representation. Supposing that the language\nembedding for the i-th token in a sentence is ai,\nwe concatenate the language embedding with the\nTransformer output states and feed it to the softmax\nlayer for predicting each token:\nP(yj|x1,...,n,y1,...,j−1; θ) = Softmax(Wo[hj,aj])\n(5)\nWe empirically demonstrate that the language em-\nbedding added to each tokens can effectively guide\nthe model to generate sentences in the required lan-\nguage. Note that we still let the model to learn the\noutput distribution for each language rather than\nsimply restricting the vocabularies of output space.\nThis offers ﬂexibility to handle coding switching\ncases commonly seen in real-world data, e.g., En-\nglish words could also appear in French sentences.\n2.2.2 Pre-Training on Monolingual Corpora\nLanguage model pre-training has shown its effec-\ntiveness in language generation tasks such as ma-\nchine translation, text summarization and genera-\ntive question answering (Radford et al., 2019; Dong\net al., 2019; Song et al ., 2019). It is particularly\nhelpful to the low/zero-resource tasks since the\nknowledge learned from large-scale monolingual\ncorpus can be transferred to downstream tasks via\nthe pre-training-then-ﬁne-tuning approach. Since\nour model for paraphrase generation shares the\nsame architecture as the language model, we are\nable to pre-train the model on massive monolingual\ndata.\nPre-training on monolingual data is conducted\nin the same way as training on parallel data, except\nthat each training example contains only one sen-\ntence with the beginning/end of sequence tokens\nand the language identiﬁer. The language embed-\ndings are also employed. The pre-training objective\nis the same as Equation (1).\nIn our experiments, we ﬁrst pre-train the model\non monolingual corpora of multiple languages re-\nspectively, and then ﬁne-tune the model on parallel\ncorpora.\n2.2.3 Denoising Auto-Encoder\nWe adopt the idea of denoising auto-encoder (DAE)\nto further improve the robustness of our para-\nphrasing model. DAE is originally proposed to\nlearn intermediate representations that are robust\nto partial corruption of the inputs in training auto-\nencoders (Pascal Vincent, 2008). Speciﬁcally, the\ninitial input X is ﬁrst partially corrupted as ˜X,\nwhich can be treated as sampling from a noise dis-\ntribution ˜X ∼q( ˜X|X). Then, an auto-encoder is\ntrained to recover the original X from the noisy\ninput ˜X by minimizing the reconstruction error.\nIn the applications of text generation (Freitag and\nRoy, 2018) and machine translation (Yunsu Kim,\n2018), DAE has shown to be able to learn represen-\ntations that are more robust to input noises and also\ngeneralize to unseen examples.\nInspired by (Yunsu Kim, 2018), we directly in-\nject three different types of noises into input sen-\ntence that are commonly encountered in real appli-\ncations.\n1) Deletion: We randomly delete 1% tokens from\nsource sentences, for example, “cat sat on the mat\n↦→cat on the mat.”\n2) Insertion: We insert a random token into source\nsentences in 1% random positions, for example,\n“cat sat on the mat ↦→cat sat on red the mat.”\n3) Reordering: We randomly swap 1% tokens in\nsource sentences, and keep the distance between\ntokens being swapped within 5. “cat sat on the mat\n↦→mat sat on the cat.”\nBy introducing such noises into the input sen-\ntences while keeping the target sentences clean in\ntraining, our model can be more stable in generat-\ning paraphrases and generalisable to unseen sen-\ntences in the training corpus. The training objective\nwith DAE becomes\nL(X,Y) =L(X) +L(Y|˜X)q( ˜X|X)\n=\nn∑\ni=1\nlog P(xi|x1,...,i−1; θ)\n+\nm∑\nj=1\nlog P(yj|˜x1,...,n,y1,...,j−1; θ). (6)\nOnce the model is trained, we generate para-\nphrases of a given sentence based on P(Y|X; θ).\n3 Experiments\n3.1 Datasets\nWe adopt the mixture of two multilingual trans-\nlation corpus as our training data: MultiUN (An-\ndreas Eisele, 2010) and OpenSubtitles (Pierre Li-\nson, 2016). MultiUN consists of 463,406 ofﬁcial\ndocuments in six languages, containing around\n300M words for each language. OpenSubtitles\nis a corpus consisting of movie and TV subtitles,\nwhich contains 2.6B sentences over 60 languages.\nWe select four shared languages of the two corpora:\nEnglish, Spanish, Russian and Chinese. Statistics\nof the training corpus are shown in Table 1. Sen-\ntences are tokenized by Wordpiece as in BERT.\nA multilingual vocabulary of 50K tokens is used.\nFor validation and testing, we randomly sample\n10000 sentences respectively from each language\npair. The rest data are used for training. For mono-\nTable 1: Statistics of training data (#sentences).\nEn↔Es En ↔Ru En ↔Zh Es ↔Ru Es ↔Zh Ru ↔Zh\nOpenSubtitles 11.7M 11.7M 11.2M 10.5M 8.5M 9.6M\nMultiUN 11.4M 11.7M 9.6M 10.6M 9.8M 9.6M\nTotal 23.1M 23.4M 20.8M 21.1M 18.3M 19.2M\n(a) (b)\n(c) (d)\nFigure 3: Automatic evaluation: (a)(c) Distinct-2 versus Relevance; (b)(d) Inverse Self-BLEU versus Relevance.\nlingual pre-training, we use English Wikipedia 1\ncorpus, which contains 2,500M words.\n3.2 Experimental Settings\nWe implement our model in Tensorﬂow (Abadi et\nal., 2016). The size of our Transformer model is\nidentical to BERT-base (Jacob Devlin, 2019). The\nmodel is constituted by 12 layers of Transformer\nblocks. Number of dimension of token embed-\nding, position embedding and transformer hidden\n1https://dumps.wikimedia.org/enwiki/\nlatest/\nstate are 768, while that of states in position-wise\nfeed-forward networks are 3072. The number of\nattention heads is 12. Models are train using Adam\noptimization (Diederik P. Kingma) with a learn-\ning rate up to 1e−4, β1 = 0.9, β2 = 0.999 and\nL2 weight decay of 0.01. We use top-k truncated\nrandom sampling strategy for inference that only\nsample from k candidate words with highest proba-\nbilities.\nThroughout our experiments, we train and\nevaluate two models for paraphrase generation:\nthe bilingual model and the multilingual model.\nThe bilingual models are trained only with\nEnglish↔Chinese, while the multilingual models\nare trained with all the data between the four lan-\nguages. The round-trip translation baseline is based\non the Transformer-based neural translation model.\n3.3 Automatic Evaluation\nWe evaluate the relevance between input and gen-\nerated paraphrase as well as the diversity among\nmultiple generated paraphrases from the same in-\nput. For relevance, we use the cosine similarity be-\ntween the sentential representations (Chia-Wei Liu,\n2016). Speciﬁcally, we use the Glove-840B embed-\ndings (Jeffrey Pennington, 2014) for word repre-\nsentation and Vector Extrema (Chia-Wei Liu, 2016)\nfor sentential representation. For generation diver-\nsity, we employ two evaluation metrics: Distinct-\n22 and inverse Self-BLEU (deﬁned as: 1−Self-\nBLEU) (Yaoming Zhu, 2018). Larger values of\nDistinct-2 and inverse Self-BLEU indicate higher\ndiversity of the generation.\nFor each model, we draw curves in Figure 3 with\nthe aforementioned metrics as coordinates, and\neach data-point is obtained at a speciﬁc sampling\ntemperature. Since a good paraphrasing model\nshould generate both relevant and diverse para-\nphrases, the model with curve lying towards the\nup-right corner is regarded as with good perfor-\nmance.\n3.3.1 Comparison with Baseline\nFirst we compare our models with the conventional\npivoting method, i.e., round-trip translation. As\nshown in Figure 3 (a)(b), either the bilingual or\nthe multilingual model is better than the baseline\nin terms of relevance and diversity in most cases.\nIn other words, with the same generation diversity\n(measured by both Distinct-2 and Self-BLEU), our\nmodels can generate paraphrase with more seman-\ntically similarity to the input sentence.\nNote that in Figure 3 (a), there is a cross point\nbetween the curve of the bilingual model and\nthe baseline curve when relevance is around 0.71.\nWe particularly investigate generated paraphrases\naround this point and ﬁnd that the baseline actually\nachieves better relevance when Distinct-2 is at a\nhigh level (>0.3). It means our bilingual model is\nsemantically drifting faster than the baseline model\nas the Distinct-2 diversity increases. The round-trip\n2https://github.com/\nneural-dialogue-metrics/Distinct-N\ntranslation performs two-round of supervised trans-\nlations, while the zero-shot paraphrasing performs\nsingle-round unsupervised ‘translation’ (paraphras-\ning). We suspect that the unsupervised paraphras-\ning can be more sensitive to the decoding strategy.\nIt also implies the latent, language-agnostic repre-\nsentation may be not well learned in our bilingual\nmodel. While on the other hand, our multilingual\nmodel alleviate this insufﬁciency. We further verify\nand analyze it as follows.\n3.3.2 Multilingual Models\nAs mentioned above, our bilingual model can be\nunstable in some cases due to the lack of a well-\nlearned language-agnostic semantic representation.\nA natural method is to introduce multilingual cor-\npus, which consists of various translation direc-\ntions. Training over multilingual corpus forces the\nmodel to decouple the language type and semantic\nrepresentation.\nEmpirical results shows that our multilingual\nmodel performs signiﬁcantly better than the bilin-\ngual model. The red and blue curves in Figure 3\n(a)(b) demonstrates a great improvement of our\nmultilingual model over the bilingual model. In\naddition, the multilingual model also signiﬁcantly\noutperforms the baseline in the setting with the\nreasonable relevance scores.\n3.3.3 Denoising Auto-Encoder\nTo verify the effectiveness of DAE in our\nmodel, various experiments with different hyper-\nparameters were conducted. We ﬁnd that DAE\nworks the best when uniformly perturbing input\nsentences with probability 0.01, using only Dele-\ntion and Reordering operations. We investigate\nDAE over both bilingual and multilingual models\nas plotted in Figure 3 (c)(d). Curves with the yellow\ncircles represent models with DAE training.\nResults in the Figure 3 (c)(d) demonstrate posi-\ntive effects of DAE in either bilingual or multilin-\ngual models. It is worth to note that, while DAE\nhave marginal impact on multilingual model, it im-\nproves bilingual model signiﬁcantly. This is an evi-\ndence indicating that DAE can improve the model\nin learning a more robust representation.\nMore speciﬁcally, since Deletion forces model\nto focus on sentence-level semantics rather than\nword-level meaning whileReordering forces model\nto focus more on meaning rather than their posi-\ntions, it would be more difﬁcult for a model to learn\nshortcuts (e.g. copy words). In other words, DAE\nimproves models’ capability in extracting deep se-\nmantic representation, which has a similar effect to\nintroducing multilingual data.\n3.3.4 Monolingual Pre-Training\nTable 2: Log-probabilities of the generated sentences.√and ×symbols denote learning with or without pre-\ntraining respectively, bold font denotes greater values.\nModel Sampling Pre-TrainingLog-Prob\nMultilingual\ngreedy, temp=1\n√ -0.1427\n× -0.1428\ntop-3, temp=1\n√ -0.1425\n× -0.1448\ntop-3, temp=1.5\n√ -0.1420\n× -0.1425\nBilingual\ngreedy, temp=1\n√ -0.1472\n× -0.1484\ntop-3, temp=1\n√ -0.1487\n× -0.1502\ntop-3, temp=1.5\n√ -0.1461\n× -0.1506\nAs shown in Figure 3 (a)(b), the model with lan-\nguage model pre-training almost performs equally\nto its contemporary without pre-training. However,\nevaluations on ﬂuency uncover the value of pre-\ntraining. We evaluate a group of models over our\ntest set in terms of ﬂuency, using a n-grams lan-\nguage model3 trained on 14k public domain books.\nAs depicted in Table 2, models with language\nmodel pre-training stably achieves greater log-\nprobabilities than the model without pre-training.\nNamely, language model pre-training brings better\nﬂuency.\n3.4 Human Evaluation\n200 sentences are sampled from our test set for\nhuman evaluation. The human evaluation guid-\nance generally follows that of (Zichao Li, 2018)\nbut with a compressed scoring range from [1, 5] to\n[1, 4]. We recruit ﬁve human annotators to evalu-\nate models in semantic relevance and ﬂuency. A\ntest example consists of one input sentence, one\ngenerated sentence from baseline model and one\ngenerated sentence from our model. We randomly\npermute a pair of generated sentences to reduce\nannotators’ bias on a certain model. Each example\nis evaluated by two annotators.\nAs shown in Table 3, our method outperforms\nthe baseline in both relevance and ﬂuency signif-\nicantly. We further calculate agreement (Cohen’s\nkappa) between two annotators.\n3http://www.openslr.org/11/\nTable 3: Human evaluation results.\nModel Relevance Fluency Agreement\nRound-trip 2.72 3.61 0.36\nMultilingual (ours) 3.43 3.75 0.35\nBoth round-trip translation and our method per-\nforms well as to ﬂuency. But the huge gap of rele-\nvance between the two systems draw much atten-\ntion of us. We investigate the test set in details and\nﬁnd that round-trip approach indeed generate more\nnoise as shown in case studies.\n3.5 Case Studies\nWe further study some generated cases from dif-\nferent models. All results in Table 4 are generated\nover our test set using randomly sampling. For both\nbaseline and multilingual model, we tune their sam-\npling temperatures to control the Distinct-2 and the\ninverse Self-BLEU at 0.31 and 0.47 respectively.\nIn the case studies, we ﬁnd that our method\nusually generates sentences with better relevance\nto source inputs, while the round-trip translation\nmethod can sometimes run into serious semantic\ndrift. In the second case, our model demonstrates a\ngood feature that it maintains the meaning and even\na proper noun guideunchanged while modiﬁes the\nsource sentence by both changing and reordering\nwords. This feature may be introduced by DAE\nperturbation strategies which improves model’s ro-\nbustness and diversity simultaneously. These re-\nsults evidence that our methods outperforms the\nbaseline in both relevance and diversity.\n4 Related Work\nGenerating paraphrases based on deep neural net-\nworks, especially Seq2Seq models, has become\nthe mainstream approach. A majority of neu-\nral paraphrasing models tried to improve gener-\nation quality and diversity with high-quality para-\nphrase corpora. Prakash et al. (2016) starts a deep\nlearning line of paraphrase generation through in-\ntroducing stacked residual LSTM network. A\nword constraint model proposed by Ziqiang Cao\n(2017) improves both generation quality and diver-\nsity. Ankush Gupta (2018) adopts variational auto-\nencoder to further improve generation diversity.\nZichao Li (2018) utilize neural reinforcement learn-\ning and adversarial training to promote generation\nquality. Zichao Li (2019) decompose paraphrase\ngeneration into phrase-level and sentence-level.\nTable 4: Case studies. For each input source, we randomly sample three paraphrases for comparison.\nSource I guess I kinda felt insigniﬁcant.\nRound-trip\nI think I just don’t feel right about that.\nI guess I’m a little uncomfortable.\nI think I’m a little bit of a problem right now.\nMultilingual\n(ours)\nI guess I was feeling a bit unsigniﬁcant.\nI guess I felt some sorts of insigniﬁcant.\nI guess I kind of felt insigniﬁcant.\nSource This site will make better use of the guide and will increase its distribution.\nRound-trip\nThe site would make better use of the guidelines and would be expanded.\nThe site will make the best use of guides and expand them.\nThis site would have made use more of the guidelines and could be expanded to its distribution.\nMultilingual\n(ours)\nThis web site will make better use of the guide and will increase its dissemination.\nThis site will better utilize the guide, and will improve its distribution.\nThe web site is going to make the guide’s use more efﬁcient and its distribution will grow.\nSource That’s how eric got the passcodes.\nRound-trip\nThen eric has a code.\nThen eric has the codes.\nThen erik’ll have the codes.\nMultilingual\n(ours)\nThat’s the way eric got the password codes.\nThat’s how eric got passwords.\nThat’s where eric gets the passcodes.\nSeveral works tried to generate paraphrases\nfrom monolingual non-parallel or translation cor-\npora. Lilin Zhang (2016) exploits Markov Net-\nwork model to extract paraphrase tables from\nmonolingual corpus. Quirk, Brockett, and Dolan\n(2004), Wubben, van den Bosch, and Krahmer\n(2010) and Wubben, van den Bosch, and Krah-\nmer (2014) create paraphrase corpus through clus-\ntering and aligning paraphrases from crawled ar-\nticles or headlines. With parallel translation cor-\npora, pivoting approaches such round-trip trans-\nlation (Jonathan Mallinson and Lapata, 2017) and\nback-translation (John Wieting, 2018) are explored.\nHowever, to the best knowledge of us, none\nof these paraphrase generation models has been\ntrained directly from parallel translation corpora as\na single-round end-to-end model.\n5 Conclusions\nIn this work, we have proposed a Transformer-\nbased model for zero-shot paraphrase generation,\nwhich can leverage huge amount of off-the-shelf\ntranslation corpora. Moreover, we improve gener-\nation ﬂuency of our model with language model\npre-training. Empirical results from both automatic\nand human evaluation demonstrate that our model\nsurpasses the conventional pivoting approaches in\nterms of relevance, diversity, ﬂuency and efﬁciency.\nNevertheless, there are some interesting directions\nto be explored. For instance, how to obtain a bet-\nter latent semantic representation with multi-modal\ndata and how to further improve the generation di-\nversity without sacriﬁcing relevance. We plan to\nstrike these challenging yet valuable problems in\nthe future.\nReferences\nAbadi, M.; Barham, P.; Chen, J.; Chen, Z.; Davis, A.;\nDean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard,\nM.; Kudlur, M.; Levenberg, J.; Monga, R.; Moore,\nS.; Murray, D. G.; Steiner, B.; Tucker, P.; Vasude-\nvan, V .; Warden, P.; Wicke, M.; Yu, Y .; and Zheng,\nX. 2016. Tensorﬂow: A system for large-scale ma-\nchine learning. In 12th USENIX Symposium on Op-\nerating Systems Design and Implementation (OSDI\n16), 265–283.\nAndreas Eisele, Y . C. 2010. Multiun: A multilingual\ncorpus from united nation documents. In LREC.\nAnkush Gupta, Arvind Agarwal, P. S. P. R. 2018. A\ndeep generative framework for paraphrase genera-\ntion. In AAAI.\nArivazhagan, N.; Bapna, A.; Firat, O.; Aharoni, R.;\nJohnson, M.; and Macherey, W. 2019. The miss-\ning ingredient in zero-shot neural machine transla-\ntion. arXiv preprint arXiv:1903.07091.\nAshish Vaswani, Noam Shazeer, N. P. J. U. L. J. A. N.\nG. L. K. I. P. 2017. Attention is all you need. In\nNIPS.\nChia-Wei Liu, Ryan Lowe, I. V . S. M. N. L. C. J. P.\n2016. How not to evaluate your dialogue system:\nAn empirical study of unsupervised evaluation met-\nrics for dialogue response generation. In EMNLP.\nDiederik P. Kingma, J. B. Adam: A method for\nstochastic optimization. In ICLR.\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.;\nWang, Y .; Gao, J.; Zhou, M.; and Hon, H.-W. 2019.\nUniﬁed language model pre-training for natural lan-\nguage understanding and generation. arXiv preprint\narXiv:1905.03197.\nFreitag, M., and Roy, S. 2018. Unsupervised natural\nlanguage generation with denoising autoencoders.\nIn EMNLP.\nIlya Sutskever, Oriol Vinyals, Q. V . L. 2014. Sequence\nto sequence learning with neural networks. In NIPS.\nJacob Devlin, Ming-Wei Chang, K. L. K. T. 2019.\nBert: Pre-training of deep bidirectional transformers\nfor language understanding. In NAACL.\nJeffrey Pennington, Richard Socher, C. D. M. 2014.\nGlove: Global vectors for word representation. In\nEMNLP.\nJiatao Gu, Yong Wang, K. C. V . O. L. 2019. Im-\nproved zero-shot neural machine translation via ig-\nnoring spurious correlations. In ACL.\nJohn Wieting, K. G. 2018. Paranmt-50m - pushing\nthe limits of paraphrastic sentence embeddings with\nmillions of machine translations. In ACL.\nJonathan Mallinson, R. S., and Lapata, M. 2017. Para-\nphrasing revisited with neural machine translation.\nIn EACL.\nLample, G., and Conneau, A. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nLilin Zhang, Zhen Weng, W. X. J. W. Z. C. Y . T. M.\nL. M. W. 2016. Extract domain-speciﬁc paraphrase\nfrom monolingual corpus for automatic evaluation\nof machine translation. In MT.\nPascal Vincent, Hugo Larochelle, Y . B. P.-A. M. 2008.\nExtracting and composing robust features with de-\nnoising autoencoders. In ICML.\nPeter J. Liu, Mohammad Saleh, E. P.-B. G. R. S. L. K.\nN. S. 2018. Generating wikipedia by summarizing\nlong sequences. In ICLR.\nPierre Lison, J. T. 2016. Opensubtitles2016: Extract-\ning large parallel corpora from movie and tv subti-\ntles. In LREC.\nPrakash, A.; Hasan, S. A.; Lee, K.; Datla, V .; Qadir, A.;\nLiu, J.; and Farri, O. 2016. Neural paraphrase gener-\nation with stacked residual LSTM networks. In Pro-\nceedings of COLING 2016, the 26th International\nConference on Computational Linguistics: Techni-\ncal Papers, 2923–2934. Osaka, Japan: The COL-\nING 2016 Organizing Committee.\nQuirk, C.; Brockett, C.; and Dolan, W. 2004. Monolin-\ngual machine translation for paraphrase generation.\nIn EMNLP, 142–149. Barcelona, Spain: Associa-\ntion for Computational Linguistics.\nRadford, A., and Sutskever, I. 2018. Improving lan-\nguage understanding by generative pre-training. In\narxiv.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nand Sutskever, I. 2019. Language models are unsu-\npervised multitask learners. OpenAI Blog 1(8).\nSestorain, L.; Ciaramita, M.; Buck, C.; and Hofmann,\nT. 2018. Zero-shot dual machine translation. arXiv\npreprint arXiv:1805.10338.\nSong, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y .\n2019. Mass: Masked sequence to sequence pre-\ntraining for language generation. arXiv preprint\narXiv:1905.02450.\nWubben, S.; van den Bosch, A.; and Krahmer, E.\n2010. Paraphrase generation as monolingual trans-\nlation: Data and evaluation. In INLG, INLG ’10,\n203–207. Stroudsburg, PA, USA: Association for\nComputational Linguistics.\nWubben, S.; van den Bosch, A.; and Krahmer, E. 2014.\nCreating and using large monolingual parallel cor-\npora for sentential paraphrase generation. In LREC,\n4292–4299. Reykjavik, Iceland: European Lan-\nguages Resources Association (ELRA).\nYaoming Zhu, Sidi Lu, L. Z. J. G. W. Z. J. W. Y . Y .\n2018. Texygen: A benchmarking platform for text\ngeneration models. In SIGIR.\nYunsu Kim, Jiahui Geng, H. N. 2018. Improving un-\nsupervised word-by-word translation with language\nmodel and denoising autoencoder. In EMNLP.\nZichao Li, Xin Jiang, L. S. H. L. 2018. Paraphrase\ngeneration with deep reinforcement learning. In\nEMNLP.\nZichao Li, Xin Jiang, L. S. Q. L. 2019. Decomposable\nneural paraphrase generation. In ACL.\nZiqiang Cao, Chuwei Luo, W. L. S. L. 2017. Joint\ncopying and restricted generation for paraphrase. In\nAAAI."
}